
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/21/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_10_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/12/cs.CV_2023_10_12/" class="article-date">
  <time datetime="2023-10-12T13:00:00.000Z" itemprop="datePublished">2023-10-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/12/cs.CV_2023_10_12/">cs.CV - 2023-10-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Investigating-the-Robustness-and-Properties-of-Detection-Transformers-DETR-Toward-Difficult-Images"><a href="#Investigating-the-Robustness-and-Properties-of-Detection-Transformers-DETR-Toward-Difficult-Images" class="headerlink" title="Investigating the Robustness and Properties of Detection Transformers (DETR) Toward Difficult Images"></a>Investigating the Robustness and Properties of Detection Transformers (DETR) Toward Difficult Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08772">http://arxiv.org/abs/2310.08772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao Ning Zou, Yuhang Zhang, Robert Wijaya</li>
<li>for: 本研究探讨了基于Transformer的对象检测器（DETR）如何处理不同的图像干扰因素，如 occlusion 和 adversarial 杂乱。</li>
<li>methods: 我们使用了不同的实验和测试基准来评估 DETR 的性能，并对其与基于 convolutional neural network（CNN）的检测器如 YOLO 和 Faster-RCNN 进行了比较。</li>
<li>results: 我们发现 DETR 在遇到 occlusion 图像时表现良好，但在遇到 adversarial 杂乱时表现较差，并且依赖于主要查询来做预测，导致查询的贡献不均匀。<details>
<summary>Abstract</summary>
Transformer-based object detectors (DETR) have shown significant performance across machine vision tasks, ultimately in object detection. This detector is based on a self-attention mechanism along with the transformer encoder-decoder architecture to capture the global context in the image. The critical issue to be addressed is how this model architecture can handle different image nuisances, such as occlusion and adversarial perturbations. We studied this issue by measuring the performance of DETR with different experiments and benchmarking the network with convolutional neural network (CNN) based detectors like YOLO and Faster-RCNN. We found that DETR performs well when it comes to resistance to interference from information loss in occlusion images. Despite that, we found that the adversarial stickers put on the image require the network to produce a new unnecessary set of keys, queries, and values, which in most cases, results in a misdirection of the network. DETR also performed poorer than YOLOv5 in the image corruption benchmark. Furthermore, we found that DETR depends heavily on the main query when making a prediction, which leads to imbalanced contributions between queries since the main query receives most of the gradient flow.
</details>
<details>
<summary>摘要</summary>
带有变换器的对象检测器（DETR）在机器视觉任务中表现出色，特别是对象检测任务。这种检测器基于自我注意机制以及变换器编码解码架构，以捕捉图像中的全局上下文。然而，需要解决的问题是如何让这种模型架构在不同的图像噪音（如 occlusion 和 adversarial 扰动）下表现良好。我们通过不同的实验和比较 DE TR 与基于 convolutional neural network（CNN）的检测器如 YOLO 和 Faster-RCNN 进行了比较。我们发现 DE TR 在干扰图像中具有较好的抗干扰性能。然而，我们发现对图像添加 adversarial 贴图会导致网络生成新的无用的键、问题和值，这通常会导致网络的误导。此外，我们发现 DE TR 在图像损害benchmark中表现较差，而且 DE TR 依赖于主要的查询来进行预测，导致查询的贡献不均。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Scoliosis-Screening-and-Diagnosis-A-Survey"><a href="#Intelligent-Scoliosis-Screening-and-Diagnosis-A-Survey" class="headerlink" title="Intelligent Scoliosis Screening and Diagnosis: A Survey"></a>Intelligent Scoliosis Screening and Diagnosis: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08756">http://arxiv.org/abs/2310.08756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhang Zhenlin, Pu Lixin, Li Ang, Zhang Jun, Li Xianjie, Fan Jipeng</li>
<li>for: 这个论文是为了探讨计算机助手诊断和评估脊梁 curvature 的现状和发展趋势。</li>
<li>methods: 论文使用了不同算法模型来描述脊梁 curvature的计算机助手诊断和评估，并对这些模型的优缺点进行分析。</li>
<li>results: 论文分析了现有算法模型的优缺点，并预测未来发展趋势。I hope that helps!<details>
<summary>Abstract</summary>
Scoliosis is a three-dimensional spinal deformity, which may lead to abnormal morphologies, such as thoracic deformity, and pelvic tilt. Severe patients may suffer from nerve damage and urinary abnormalities. At present, the number of scoliosis patients in primary and secondary schools has exceeded five million in China, the incidence rate is about 3% to 5% which is growing every year. The research on scoliosis, therefore, has important clinical value. This paper systematically introduces computer-assisted scoliosis screening and diagnosis as well as analyzes the advantages and limitations of different algorithm models in the current issue field. Moreover, the paper also discusses the current development bottlenecks in this field and looks forward to future development trends.
</details>
<details>
<summary>摘要</summary>
斯科利病是三维脊梁弯曲的疾病，可能会导致异常的体征，如胸部弯曲和臀部倾斜。严重的患者可能会uffer from nerve damage和尿液异常。目前，中国primary和secondary学校的斯科利病患者人数已经超过500万，发生率大约为3%-5%，每年都在增长。因此，斯科利病的研究具有重要的临床价值。本文系统介绍了计算机助け的斯科利病检测和诊断，分析了不同算法模型在当前领域的优劣点。此外，本文还讨论了当前领域的发展瓶颈和未来发展趋势。
</details></li>
</ul>
<hr>
<h2 id="PU-Ray-Point-Cloud-Upsampling-via-Ray-Marching-on-Implicit-Surface"><a href="#PU-Ray-Point-Cloud-Upsampling-via-Ray-Marching-on-Implicit-Surface" class="headerlink" title="PU-Ray: Point Cloud Upsampling via Ray Marching on Implicit Surface"></a>PU-Ray: Point Cloud Upsampling via Ray Marching on Implicit Surface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08755">http://arxiv.org/abs/2310.08755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sum1lim/PU-Ray">https://github.com/sum1lim/PU-Ray</a></li>
<li>paper_authors: Sangwon Lim, Karim El-Basyouny, Yee Hong Yang</li>
<li>For:	+ The paper addresses the problems of domain dependency and computational redundancy in deep-learning-based point cloud upsampling methods, and proposes a ray-based upsampling approach with an arbitrary rate for more precise and stable results.* Methods:	+ The method uses a ray-based approach to simulate the ray marching algorithm for implicit surface learning, and employs a rule-based mid-point query sampling method to achieve a uniform output point distribution without requiring model training.* Results:	+ The results demonstrate the method’s versatility across different domains and training scenarios with limited computational resources and training data, allowing the upsampling task to transition from academic research to real-world applications.Here’s the simplified Chinese text for the three key information points:* 用:	+ 本文解决了深度学习基于点云upsampling方法中的域dependency和计算过程的重复性问题，并提出了一种基于探针的upsampling方法，以实现更精确和稳定的结果。* 方法:	+ 方法使用探针基本来模拟探针迭代算法，实现了隐藏表面学习，并采用规则基于中点查询法来实现输出点云的均匀分布，不需要模型训练。* 结果:	+ 结果表明该方法在不同的域和训练场景下具有限制的计算资源和训练数据，可以将upsampling任务从学术研究转移到实际应用中。<details>
<summary>Abstract</summary>
While the recent advancements in deep-learning-based point cloud upsampling methods improve the input to autonomous driving systems, they still suffer from the uncertainty of denser point generation resulting from end-to-end learning. For example, due to the vague training objectives of the models, their performance depends on the point distributions of the input and the ground truth. This causes problems of domain dependency between synthetic and real-scanned point clouds and issues with substantial model sizes and dataset requirements. Additionally, many existing methods upsample point clouds with a fixed scaling rate, making them inflexible and computationally redundant. This paper addresses the above problems by proposing a ray-based upsampling approach with an arbitrary rate, where a depth prediction is made for each query ray. The method simulates the ray marching algorithm to achieve more precise and stable ray-depth predictions through implicit surface learning. The rule-based mid-point query sampling method enables a uniform output point distribution without requiring model training using the Chamfer distance loss function, which can exhibit bias towards the training dataset. Self-supervised learning becomes possible with accurate ground truths within the input point cloud. The results demonstrate the method's versatility across different domains and training scenarios with limited computational resources and training data. This allows the upsampling task to transition from academic research to real-world applications.
</details>
<details>
<summary>摘要</summary>
“Recent advancements in deep learning-based point cloud upsampling methods have improved the input for autonomous driving systems, but they still suffer from the uncertainty of denser point generation due to end-to-end learning. For example, the models' performance depends on the point distributions of the input and ground truth, causing domain dependency between synthetic and real-scanned point clouds, as well as issues with large model sizes and dataset requirements. Existing methods upsample point clouds with a fixed scaling rate, making them inflexible and computationally redundant. This paper addresses these problems by proposing a ray-based upsampling approach with an arbitrary rate, where a depth prediction is made for each query ray. The method simulates the ray marching algorithm to achieve more precise and stable ray-depth predictions through implicit surface learning. The rule-based mid-point query sampling method enables a uniform output point distribution without requiring model training using the Chamfer distance loss function, which can exhibit bias towards the training dataset. Self-supervised learning becomes possible with accurate ground truths within the input point cloud. The results demonstrate the method's versatility across different domains and training scenarios with limited computational resources and training data, allowing the upsampling task to transition from academic research to real-world applications.”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="AcTExplore-Active-Tactile-Exploration-on-Unknown-Objects"><a href="#AcTExplore-Active-Tactile-Exploration-on-Unknown-Objects" class="headerlink" title="AcTExplore: Active Tactile Exploration on Unknown Objects"></a>AcTExplore: Active Tactile Exploration on Unknown Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08745">http://arxiv.org/abs/2310.08745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir-Hossein Shahidzadeh, Seong Jong Yoo, Pavan Mantripragada, Chahat Deep Singh, Cornelia Fermüller, Yiannis Aloimonos</li>
<li>for: 本研究旨在提出一种基于奖励学习的活动戚触探测方法，以便高效地探索物体结构，从而提高机器人抓取和操作等基础任务的能力。</li>
<li>methods: 本方法使用奖励学习驱动的活动戚触探测策略，通过尝试不同的探测动作，逐渐探索物体表面，并通过缓存的策略来减少探测次数。</li>
<li>results: 本研究在未看过的 YCB 对象上达到了95.97% IoU 覆盖率，而且只需要在基本形状上进行训练。项目网站：<a target="_blank" rel="noopener" href="https://prg.cs.umd$.$edu/AcTExplore">https://prg.cs.umd$.$edu/AcTExplore</a><details>
<summary>Abstract</summary>
Tactile exploration plays a crucial role in understanding object structures for fundamental robotics tasks such as grasping and manipulation. However, efficiently exploring such objects using tactile sensors is challenging, primarily due to the large-scale unknown environments and limited sensing coverage of these sensors. To this end, we present AcTExplore, an active tactile exploration method driven by reinforcement learning for object reconstruction at scales that automatically explores the object surfaces in a limited number of steps. Through sufficient exploration, our algorithm incrementally collects tactile data and reconstructs 3D shapes of the objects as well, which can serve as a representation for higher-level downstream tasks. Our method achieves an average of 95.97% IoU coverage on unseen YCB objects while just being trained on primitive shapes. Project Webpage: https://prg.cs.umd$.$edu/AcTExplore
</details>
<details>
<summary>摘要</summary>
“感觉探索”在基本机器人任务中，如抓取和操作，具有重要的作用。然而，使用感觉传感器效率地探索物体是具有挑战性，主要是因为物体环境规模很大，感觉传感器的感知范围也有限。为此，我们提出了AcTExplore，一种基于奖励学习的活动感觉探索方法，可以自动在有限步数内探索物体表面。通过适当的探索，我们的算法逐步收集感觉数据，并将其用于重建物体的3D形状，这可以作为下游任务的表示。我们的方法在未经训练的YCB物体上实现了95.97%的IOU覆盖率，请参阅项目网页：https://prg.cs.umd$.$edu/AcTExplore。
</details></li>
</ul>
<hr>
<h2 id="A-Benchmarking-Protocol-for-SAR-Colorization-From-Regression-to-Deep-Learning-Approaches"><a href="#A-Benchmarking-Protocol-for-SAR-Colorization-From-Regression-to-Deep-Learning-Approaches" class="headerlink" title="A Benchmarking Protocol for SAR Colorization: From Regression to Deep Learning Approaches"></a>A Benchmarking Protocol for SAR Colorization: From Regression to Deep Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08705">http://arxiv.org/abs/2310.08705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kangqing Shen, Gemine Vivone, Xiaoyuan Yang, Simone Lolli, Michael Schmitt</li>
<li>for: 这篇论文旨在提出一种基于supervised learning的SAR颜色化方法，以帮助解决Remote sensing中SAR图像的雾涂问题。</li>
<li>methods: 该方法包括一种协议 для生成合成的SAR颜色图像、多种基线和一种基于conditional generative adversarial network（cGAN）的有效SAR颜色化方法。</li>
<li>results: EXTENSIVE TESTS表明我们提出的cGAN基本网络对SAR颜色化问题具有高效性。代码将公开发布。<details>
<summary>Abstract</summary>
Synthetic aperture radar (SAR) images are widely used in remote sensing. Interpreting SAR images can be challenging due to their intrinsic speckle noise and grayscale nature. To address this issue, SAR colorization has emerged as a research direction to colorize gray scale SAR images while preserving the original spatial information and radiometric information. However, this research field is still in its early stages, and many limitations can be highlighted. In this paper, we propose a full research line for supervised learning-based approaches to SAR colorization. Our approach includes a protocol for generating synthetic color SAR images, several baselines, and an effective method based on the conditional generative adversarial network (cGAN) for SAR colorization. We also propose numerical assessment metrics for the problem at hand. To our knowledge, this is the first attempt to propose a research line for SAR colorization that includes a protocol, a benchmark, and a complete performance evaluation. Our extensive tests demonstrate the effectiveness of our proposed cGAN-based network for SAR colorization. The code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
雷达图像（SAR）广泛用于远程感知。解释SAR图像可以是困难的，因为它们具有内生的点粒度噪声和灰度特征。为解决这一问题，SAR彩色化在研究中得到了广泛关注，以彩色化灰度SAR图像，保留原始空间信息和雷达信息。但这一研究领域仍处于早期阶段，有很多限制。在这篇论文中，我们提出了一个完整的超级vised学习基础的研究线路，用于SAR彩色化。我们的方法包括一种协议，一些基elines，以及基于条件生成 adversarial network（cGAN）的有效方法。我们还提出了评估问题的数字量表。根据我们所知，这是第一次为SAR彩色化提出了一个完整的研究线路，包括协议、标准和完整的性能评估。我们的广泛测试表明，我们提出的cGAN基于网络可以有效地进行SAR彩色化。代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Fed-Safe-Securing-Federated-Learning-in-Healthcare-Against-Adversarial-Attacks"><a href="#Fed-Safe-Securing-Federated-Learning-in-Healthcare-Against-Adversarial-Attacks" class="headerlink" title="Fed-Safe: Securing Federated Learning in Healthcare Against Adversarial Attacks"></a>Fed-Safe: Securing Federated Learning in Healthcare Against Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08681">http://arxiv.org/abs/2310.08681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erfan Darzi, Nanna M. Sijtsema, P. M. A van Ooijen</li>
<li>for: 本研究探讨了受防御性训练和加密技术保护的 Federated Learning 医学图像分析应用的安全性。</li>
<li>methods: 本研究使用了分布式噪声来防御模型免受攻击，并且通过对不同攻击场景、参数和使用场景进行广泛的评估来证明其效果。</li>
<li>results: 研究结果表明，通过分布式噪声可以实现与传统防御训练相同的安全水平，而且需要更少的重训样本来建立一个可靠的模型。<details>
<summary>Abstract</summary>
This paper explores the security aspects of federated learning applications in medical image analysis. Current robustness-oriented methods like adversarial training, secure aggregation, and homomorphic encryption often risk privacy compromises. The central aim is to defend the network against potential privacy breaches while maintaining model robustness against adversarial manipulations. We show that incorporating distributed noise, grounded in the privacy guarantees in federated settings, enables the development of a adversarially robust model that also meets federated privacy standards. We conducted comprehensive evaluations across diverse attack scenarios, parameters, and use cases in cancer imaging, concentrating on pathology, meningioma, and glioma. The results reveal that the incorporation of distributed noise allows for the attainment of security levels comparable to those of conventional adversarial training while requiring fewer retraining samples to establish a robust model.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SSG2-A-new-modelling-paradigm-for-semantic-segmentation"><a href="#SSG2-A-new-modelling-paradigm-for-semantic-segmentation" class="headerlink" title="SSG2: A new modelling paradigm for semantic segmentation"></a>SSG2: A new modelling paradigm for semantic segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08671">http://arxiv.org/abs/2310.08671</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feevos/ssg2">https://github.com/feevos/ssg2</a></li>
<li>paper_authors: Foivos I. Diakogiannis, Suzanne Furby, Peter Caccetta, Xiaoliang Wu, Rodrigo Ibata, Ondrej Hlinka, John Taylor</li>
<li>for: 这个论文主要是为了解决semantic segmentation中的一个问题，即模型只能处理单个静止图像，导致无法进行误差修正。</li>
<li>methods: 这篇论文提出了一种方法，即使用序列可观测的方式来提高semantic segmentation的准确率。具体来说，该方法使用了一个双encoder、单decoder的基网络，以及一个序列模型。</li>
<li>results: 在三个不同的数据集上进行测试，SSG2模型表现出色，与UNet类基线模型相比，它在同样的数量的梯度更新中显著地提高了准确率。然而，添加时间维度会增加内存占用量。<details>
<summary>Abstract</summary>
State-of-the-art models in semantic segmentation primarily operate on single, static images, generating corresponding segmentation masks. This one-shot approach leaves little room for error correction, as the models lack the capability to integrate multiple observations for enhanced accuracy. Inspired by work on semantic change detection, we address this limitation by introducing a methodology that leverages a sequence of observables generated for each static input image. By adding this "temporal" dimension, we exploit strong signal correlations between successive observations in the sequence to reduce error rates. Our framework, dubbed SSG2 (Semantic Segmentation Generation 2), employs a dual-encoder, single-decoder base network augmented with a sequence model. The base model learns to predict the set intersection, union, and difference of labels from dual-input images. Given a fixed target input image and a set of support images, the sequence model builds the predicted mask of the target by synthesizing the partial views from each sequence step and filtering out noise. We evaluate SSG2 across three diverse datasets: UrbanMonitor, featuring orthoimage tiles from Darwin, Australia with five spectral bands and 0.2m spatial resolution; ISPRS Potsdam, which includes true orthophoto images with multiple spectral bands and a 5cm ground sampling distance; and ISIC2018, a medical dataset focused on skin lesion segmentation, particularly melanoma. The SSG2 model demonstrates rapid convergence within the first few tens of epochs and significantly outperforms UNet-like baseline models with the same number of gradient updates. However, the addition of the temporal dimension results in an increased memory footprint. While this could be a limitation, it is offset by the advent of higher-memory GPUs and coding optimizations.
</details>
<details>
<summary>摘要</summary>
现代 semantic segmentation 模型主要在单个静止图像上运行，生成相应的分割标签。这种一枚投入方法留下了Errata的Room for error correction，因为模型缺乏集成多个观察到的能力。我们由 semantic change detection 的工作 inspirited，我们提出了一种方法，利用每个静止输入图像的序列可观测。通过添加这个“时间”维度，我们利用序列观察中强相关的信号强度来减少错误率。我们的框架，称为 SSG2 (Semantic Segmentation Generation 2)，使用了 dual-encoder，single-decoder 基础网络，并添加了一个序列模型。基础模型可以预测 dual-input 图像上的标签集 intersection， union 和 difference。给定一个固定的目标输入图像和一组支持图像，序列模型可以使用每个序列步骤中的Synthesizing partial views，并将噪声滤除，生成目标图像的预测掩码。我们在 UrbanMonitor，ISPRS Potsdam 和 ISIC2018 三个多样化的数据集上评估了 SSG2 模型，它在First few tens of epochs 内快速聚合，并与同样多个梯度更新的 UNet-like 基eline模型相比，显著提高性能。然而，添加时间维度会增加内存占用量。尽管这可能是一个限制，但是随着更高内存 GPU 和编程优化，这个问题可以被解决。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Large-Language-Model-for-Visual-Navigation"><a href="#Multimodal-Large-Language-Model-for-Visual-Navigation" class="headerlink" title="Multimodal Large Language Model for Visual Navigation"></a>Multimodal Large Language Model for Visual Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08669">http://arxiv.org/abs/2310.08669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao-Hung Hubert Tsai, Vansh Dhar, Jialu Li, Bowen Zhang, Jian Zhang</li>
<li>for: 本研究旨在开发一种可以通过语言模型来实现视觉导航的方法，不需要复杂的提示系统。</li>
<li>methods: 我们的方法使用了简单的文本提示、当前观察和历史收集器模型，将输入为视觉导航。输出为可能的行为选择的概率分布。</li>
<li>results: 我们的方法在使用人类示例和碰撞信号从Habitat-Matterport 3D Dataset（HM3D）进行训练后，与现有的行为快照方法相比，表现出来的结果更好，并有效降低碰撞率。<details>
<summary>Abstract</summary>
Recent efforts to enable visual navigation using large language models have mainly focused on developing complex prompt systems. These systems incorporate instructions, observations, and history into massive text prompts, which are then combined with pre-trained large language models to facilitate visual navigation. In contrast, our approach aims to fine-tune large language models for visual navigation without extensive prompt engineering. Our design involves a simple text prompt, current observations, and a history collector model that gathers information from previous observations as input. For output, our design provides a probability distribution of possible actions that the agent can take during navigation. We train our model using human demonstrations and collision signals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results demonstrate that our method outperforms state-of-the-art behavior cloning methods and effectively reduces collision rates.
</details>
<details>
<summary>摘要</summary>
Recent efforts to enable visual navigation using large language models have mainly focused on developing complex prompt systems. These systems incorporate instructions, observations, and history into massive text prompts, which are then combined with pre-trained large language models to facilitate visual navigation. In contrast, our approach aims to fine-tune large language models for visual navigation without extensive prompt engineering. Our design involves a simple text prompt, current observations, and a history collector model that gathers information from previous observations as input. For output, our design provides a probability distribution of possible actions that the agent can take during navigation. We train our model using human demonstrations and collision signals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results demonstrate that our method outperforms state-of-the-art behavior cloning methods and effectively reduces collision rates.Here's the text in Traditional Chinese:近期对使用大型自然语言模型进行视觉NAVIIGATION的努力主要集中在开发复杂的提示系统上。这些系统将 instrucions、观察和历史合并到巨量文本提示中，然后与预训的大型自然语言模型结合以便视觉NAVIIGATION。相比之下，我们的方法则是调整大型自然语言模型以便视觉NAVIIGATION，不需要广泛的提示工程。我们的设计包括简单的文本提示、当前观察和历史收集器模型，这些模型将前一次观察的信息作为输入，并将输出为可能的行动选择的概率分布。我们使用人类示范和Habitat-Matterport 3D Dataset（HM3D）中的碰撞信号进行训练。实验结果显示，我们的方法比预设的行为复制方法更高效，并有效地降低碰撞率。
</details></li>
</ul>
<hr>
<h2 id="Histogram-and-Diffusion-Based-Medical-Out-of-Distribution-Detection"><a href="#Histogram-and-Diffusion-Based-Medical-Out-of-Distribution-Detection" class="headerlink" title="Histogram- and Diffusion-Based Medical Out-of-Distribution Detection"></a>Histogram- and Diffusion-Based Medical Out-of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08654">http://arxiv.org/abs/2310.08654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evi M. C. Huijben, Sina Amirrajab, Josien P. W. Pluim</li>
<li>for: 本研究旨在提高医学领域人工智能算法的安全性和可靠性，通过检测异常输入数据（Out-of-distribution，OOD）。</li>
<li>methods: 本研究提出了一个组合使用 histogram-based 方法和 diffusion-based 方法的检测管道，以检测医学领域中的异常数据。 histogram-based 方法用于检测医学领域中的同型异常（homogeneous anomalies），而 diffusion-based 方法基于最新的无监督异常检测方法（DDPM-OOD）。</li>
<li>results: 研究发现，提出的 DDPM 方法敏感于卷积和偏置场示例，但面临着解剖变形、黑色slice和交换 patches 等挑战。这些发现表明，进一步研究可以提高 DDPM 的性能，以便更好地检测医学领域中的异常数据。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection is crucial for the safety and reliability of artificial intelligence algorithms, especially in the medical domain. In the context of the Medical OOD (MOOD) detection challenge 2023, we propose a pipeline that combines a histogram-based method and a diffusion-based method. The histogram-based method is designed to accurately detect homogeneous anomalies in the toy examples of the challenge, such as blobs with constant intensity values. The diffusion-based method is based on one of the latest methods for unsupervised anomaly detection, called DDPM-OOD. We explore this method and propose extensive post-processing steps for pixel-level and sample-level anomaly detection on brain MRI and abdominal CT data provided by the challenge. Our results show that the proposed DDPM method is sensitive to blur and bias field samples, but faces challenges with anatomical deformation, black slice, and swapped patches. These findings suggest that further research is needed to improve the performance of DDPM for OOD detection in medical images.
</details>
<details>
<summary>摘要</summary>
外部分布 (OOD) 检测是人工智能算法的安全性和可靠性关键，特别在医疗领域。在2023年医疗外部分布检测挑战中，我们提议一个管道， combinates  histogram-based 方法和扩散-based 方法。 histogram-based 方法用于准确检测医疗示例中的同质异常，如具有常数Intensity值的blob。扩散-based 方法基于最新的无监督异常检测方法DDPM-OOD。我们探索这个方法，并提出了广泛的后处理步骤，用于像素级和样本级异常检测在脑MRI和腹部CT数据中。我们的结果表明，我们提议的 DDPM 方法对于锐化和偏置场景敏感，但面临着解剖变形、黑色slice和交换 patches 等挑战。这些发现表明，进一步的研究可以提高 DDPM 的外部分布检测性能在医疗图像中。
</details></li>
</ul>
<hr>
<h2 id="Defect-Analysis-of-3D-Printed-Cylinder-Object-Using-Transfer-Learning-Approaches"><a href="#Defect-Analysis-of-3D-Printed-Cylinder-Object-Using-Transfer-Learning-Approaches" class="headerlink" title="Defect Analysis of 3D Printed Cylinder Object Using Transfer Learning Approaches"></a>Defect Analysis of 3D Printed Cylinder Object Using Transfer Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08645">http://arxiv.org/abs/2310.08645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Manjurul Ahsan, Shivakumar Raman, Zahed Siddique</li>
<li>for: 这个研究旨在测试机器学习（ML）方法，特别是转移学习（TL）模型，以检测3D印造中的缺陷。</li>
<li>methods: 研究使用了多种ML模型，包括VGG16、VGG19、ResNet50、ResNet101、InceptionResNetV2和MobileNetV2，对3D印造中的图像进行分析。</li>
<li>results: 研究发现，MobileNetV2、InceptionResNetV2和VGG16等TL模型在第一个研究中均取得了完美的分数，而ResNet50则表现不佳，其平均F1分数为0.32。在第二个研究中，MobileNetV2正确地显示了所有的实例，而ResNet50则因为更多的假阳性和 fewer true positives，其F1分数为0.75。总的来说，研究发现了一些TL模型，如MobileNetV2，可以为3D印造中的缺陷分类提供高精度。<details>
<summary>Abstract</summary>
Additive manufacturing (AM) is gaining attention across various industries like healthcare, aerospace, and automotive. However, identifying defects early in the AM process can reduce production costs and improve productivity - a key challenge. This study explored the effectiveness of machine learning (ML) approaches, specifically transfer learning (TL) models, for defect detection in 3D-printed cylinders. Images of cylinders were analyzed using models including VGG16, VGG19, ResNet50, ResNet101, InceptionResNetV2, and MobileNetV2. Performance was compared across two datasets using accuracy, precision, recall, and F1-score metrics. In the first study, VGG16, InceptionResNetV2, and MobileNetV2 achieved perfect scores. In contrast, ResNet50 had the lowest performance, with an average F1-score of 0.32. Similarly, in the second study, MobileNetV2 correctly classified all instances, while ResNet50 struggled with more false positives and fewer true positives, resulting in an F1-score of 0.75. Overall, the findings suggest certain TL models like MobileNetV2 can deliver high accuracy for AM defect classification, although performance varies across algorithms. The results provide insights into model optimization and integration needs for reliable automated defect analysis during 3D printing. By identifying the top-performing TL techniques, this study aims to enhance AM product quality through robust image-based monitoring and inspection.
</details>
<details>
<summary>摘要</summary>
三维打印（AM）在医疗、航空和汽车等领域得到了广泛关注，但是早期发现AM制造过程中的缺陷可以降低生产成本和提高生产效率，这是一个关键挑战。本研究通过机器学习（ML）方法，具体来说是传输学习（TL）模型，研究了3D打印的缺陷检测。研究使用了多种模型，包括VGG16、VGG19、ResNet50、ResNet101、InceptionResNetV2和MobileNetV2。通过精度、准确率、回归率和F1得分来评估模型的性能。在第一个研究中，VGG16、InceptionResNetV2和MobileNetV2均取得了完美的分数。相比之下，ResNet50表现最差，其平均F1分数为0.32。在第二个研究中，MobileNetV2正确地分类了所有实例，而ResNet50则有更多的假阳性和 fewer true positive，其F1分数为0.75。总的来说，研究发现一些TL模型，如MobileNetV2，可以在AM缺陷分类中达到高精度。然而，不同的算法之间存在性能差异。这些结果为自动化3D打印图像基于监测和检测中的模型优化和集成提供了信息。通过确定最佳TL技术，本研究旨在通过图像基于的可靠自动检测，提高AM产品质量。
</details></li>
</ul>
<hr>
<h2 id="Is-Generalized-Dynamic-Novel-View-Synthesis-from-Monocular-Videos-Possible-Today"><a href="#Is-Generalized-Dynamic-Novel-View-Synthesis-from-Monocular-Videos-Possible-Today" class="headerlink" title="Is Generalized Dynamic Novel View Synthesis from Monocular Videos Possible Today?"></a>Is Generalized Dynamic Novel View Synthesis from Monocular Videos Possible Today?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08587">http://arxiv.org/abs/2310.08587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoming Zhao, Alex Colburn, Fangchang Ma, Miguel Angel Bautista, Joshua M. Susskind, Alexander G. Schwing</li>
<li>for: 动态新视角Synthesizing from monocular videos</li>
<li>methods: 基于现有技术的分析框架和 pseudo-generalized 方法</li>
<li>results:  despite lacking scene-specific appearance optimization, the pseudo-generalized approach improves upon some scene-specific methods and achieves geometrically and temporally consistent depth estimates.<details>
<summary>Abstract</summary>
Rendering scenes observed in a monocular video from novel viewpoints is a challenging problem. For static scenes the community has studied both scene-specific optimization techniques, which optimize on every test scene, and generalized techniques, which only run a deep net forward pass on a test scene. In contrast, for dynamic scenes, scene-specific optimization techniques exist, but, to our best knowledge, there is currently no generalized method for dynamic novel view synthesis from a given monocular video. To answer whether generalized dynamic novel view synthesis from monocular videos is possible today, we establish an analysis framework based on existing techniques and work toward the generalized approach. We find a pseudo-generalized process without scene-specific appearance optimization is possible, but geometrically and temporally consistent depth estimates are needed. Despite no scene-specific appearance optimization, the pseudo-generalized approach improves upon some scene-specific methods.
</details>
<details>
<summary>摘要</summary>
<SYS> translate("Rendering scenes observed in a monocular video from novel viewpoints is a challenging problem.")</SYS>对于单目视频中观察到的场景，社区已经研究了两种类型的技术：一是场景特定优化技术，这些技术在每个测试场景上进行优化；另一种是通用技术，只需在测试场景上运行深度网络的前进 pass。然而，对于动态场景，只有场景特定优化技术存在，而没有通用的方法 для动态新视角synthesis from monocular videos。为了回答这个问题，我们建立了一个分析框架，基于现有的技术和工作 toward a generalized approach。我们发现可以使用 Pseudo-generalized process without scene-specific appearance optimization，但需要ogeometrically和temporally consistent depth estimates。尽管没有场景特定的外观优化， Pseudo-generalized approach仍然可以超越一些场景特定的方法。Note: "Pseudo-generalized" is a term used in the original text, and it refers to a process that is not entirely generalized, but rather a simplified version of a generalized process.
</details></li>
</ul>
<hr>
<h2 id="Im4D-High-Fidelity-and-Real-Time-Novel-View-Synthesis-for-Dynamic-Scenes"><a href="#Im4D-High-Fidelity-and-Real-Time-Novel-View-Synthesis-for-Dynamic-Scenes" class="headerlink" title="Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic Scenes"></a>Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08585">http://arxiv.org/abs/2310.08585</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zju3dv/im4d">https://github.com/zju3dv/im4d</a></li>
<li>paper_authors: Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, Xiaowei Zhou</li>
<li>For: 该 paper targets 动态视角合成问题，即从多视角视频中生成高质量的动态视图图像。* Methods: 该 paper 提出了 Im4D  Hybrid Scene Representation，即将格子基 geometry 与多视角图像基于的 appearance 结合在一起，以捕捉复杂动态场景中的earance detail。* Results: 该 paper 的方法在 five 个动态视角合成数据集上进行了评估，并表现出了state-of-the-art的渲染质量和可教学性，同时实现了实时渲染，单个 RTX 3090 GPU 上的速度为 79.8 FPS。<details>
<summary>Abstract</summary>
This paper aims to tackle the challenge of dynamic view synthesis from multi-view videos. The key observation is that while previous grid-based methods offer consistent rendering, they fall short in capturing appearance details of a complex dynamic scene, a domain where multi-view image-based rendering methods demonstrate the opposite properties. To combine the best of two worlds, we introduce Im4D, a hybrid scene representation that consists of a grid-based geometry representation and a multi-view image-based appearance representation. Specifically, the dynamic geometry is encoded as a 4D density function composed of spatiotemporal feature planes and a small MLP network, which globally models the scene structure and facilitates the rendering consistency. We represent the scene appearance by the original multi-view videos and a network that learns to predict the color of a 3D point from image features, instead of memorizing detailed appearance totally with networks, thereby naturally making the learning of networks easier. Our method is evaluated on five dynamic view synthesis datasets including DyNeRF, ZJU-MoCap, NHR, DNA-Rendering and ENeRF-Outdoor datasets. The results show that Im4D exhibits state-of-the-art performance in rendering quality and can be trained efficiently, while realizing real-time rendering with a speed of 79.8 FPS for 512x512 images, on a single RTX 3090 GPU.
</details>
<details>
<summary>摘要</summary>
The dynamic geometry of the scene is represented as a 4D density function consisting of spatiotemporal feature planes and a small MLP network. This allows for global modeling of the scene structure and consistent rendering. The scene appearance is represented by the original multi-view videos and a network that predicts the color of a 3D point based on image features, rather than memorizing detailed appearance with networks. This approach makes it easier to learn the networks and naturally leads to more efficient training.We evaluate our method on five dynamic view synthesis datasets, including DyNeRF, ZJU-MoCap, NHR, DNA-Rendering, and ENeRF-Outdoor. The results show that Im4D achieves state-of-the-art performance in rendering quality and can be trained efficiently. Additionally, our method realizes real-time rendering with a speed of 79.8 FPS for 512x512 images on a single RTX 3090 GPU.
</details></li>
</ul>
<hr>
<h2 id="PonderV2-Pave-the-Way-for-3D-Foundation-Model-with-A-Universal-Pre-training-Paradigm"><a href="#PonderV2-Pave-the-Way-for-3D-Foundation-Model-with-A-Universal-Pre-training-Paradigm" class="headerlink" title="PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm"></a>PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08586">http://arxiv.org/abs/2310.08586</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/PonderV2">https://github.com/OpenGVLab/PonderV2</a></li>
<li>paper_authors: Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha Zhang, Xianglong He, Tong He, Hengshuang Zhao, Chunhua Shen, Yu Qiao, Wanli Ouyang</li>
<li>for: 本研究旨在开发一种robust和高度泛化的3D基础模型，以解决现有的2D计算机视觉和自然语言处理基础模型不足的问题。</li>
<li>methods: 该研究提出了一种包括点云编码器和volumetric神经渲染器的完整3D预训练框架，通过对实际图像和预测图像进行对比，以学习有用的3D表示。</li>
<li>results: 该研究首次实现了在11个室内和室外标准测试集上的state-of-the-art性能，并在不同的场景下表现了一致性。代码和模型将在<a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/PonderV2%E4%B8%AD%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/OpenGVLab/PonderV2中公开。</a><details>
<summary>Abstract</summary>
In contrast to numerous NLP and 2D computer vision foundational models, the learning of a robust and highly generalized 3D foundational model poses considerably greater challenges. This is primarily due to the inherent data variability and the diversity of downstream tasks. In this paper, we introduce a comprehensive 3D pre-training framework designed to facilitate the acquisition of efficient 3D representations, thereby establishing a pathway to 3D foundational models. Motivated by the fact that informative 3D features should be able to encode rich geometry and appearance cues that can be utilized to render realistic images, we propose a novel universal paradigm to learn point cloud representations by differentiable neural rendering, serving as a bridge between 3D and 2D worlds. We train a point cloud encoder within a devised volumetric neural renderer by comparing the rendered images with the real images. Notably, our approach demonstrates the seamless integration of the learned 3D encoder into diverse downstream tasks. These tasks encompass not only high-level challenges such as 3D detection and segmentation but also low-level objectives like 3D reconstruction and image synthesis, spanning both indoor and outdoor scenarios. Besides, we also illustrate the capability of pre-training a 2D backbone using the proposed universal methodology, surpassing conventional pre-training methods by a large margin. For the first time, PonderV2 achieves state-of-the-art performance on 11 indoor and outdoor benchmarks. The consistent improvements in various settings imply the effectiveness of the proposed method. Code and models will be made available at https://github.com/OpenGVLab/PonderV2.
</details>
<details>
<summary>摘要</summary>
相比多种自然语言处理和2D计算机视觉的基础模型，学习一个强大和高度总结的3D基础模型带来了许多更大的挑战。这主要是因为数据的自然变化和下游任务的多样性。在这篇论文中，我们介绍了一个全面的3D预训练框架，用于实现高效的3D表示的获得，从而建立3D基础模型的路径。我们被激励了由于3D特征应该能够编码丰富的几何和外观提示，以便生成真实的图像。我们提出了一种新的通用 paradigma，用于学习点云表示，作为2D和3D世界之间的桥梁。我们在定制的Volumetric Neural Renderer中训练了一个点云编码器，通过比较生成的图像与真实图像来训练。值得注意的是，我们的方法可以很好地整合学习的3D编码器到多种下游任务中。这些任务包括高级挑战 like 3D检测和分割，以及低级目标 like 3D重建和图像生成，涵盖了室内和室外场景。此外，我们还示出了使用我们所提出的通用方法来预训练2D脊梁的优势。在11个室内和室外标准测试 benchmark上，PonderV2首次实现了状态机器人的性能。这一共见的改进表明了我们的方法的有效性。代码和模型将在https://github.com/OpenGVLab/PonderV2上提供。
</details></li>
</ul>
<hr>
<h2 id="Is-ImageNet-worth-1-video-Learning-strong-image-encoders-from-1-long-unlabelled-video"><a href="#Is-ImageNet-worth-1-video-Learning-strong-image-encoders-from-1-long-unlabelled-video" class="headerlink" title="Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video"></a>Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08584">http://arxiv.org/abs/2310.08584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashanka Venkataramanan, Mamshad Nayeem Rizve, João Carreira, Yuki M. Asano, Yannis Avrithis</li>
<li>for: 这个论文是为了研究自主学习中的数据使用方法，以及如何更加经济地使用数据。</li>
<li>methods: 本论文提出了两个贡献。首先，它介绍了一个新的自助学习图像预训练方法，该方法基于时间Tracking来学习认知。其次，它提出了一个新的自助学习预训练方法，该方法使用 transformer 交叉关注来生成焦点地图，并使用这些焦点地图来学习图像和视频下游任务。</li>
<li>results: 根据论文描述，使用这两种方法可以使一个来自 Walking Tours 的视频成为 ImageNet 的强大竞争对手。<details>
<summary>Abstract</summary>
Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a "Walking Tours" dataset. These videos are high-resolution, hours-long, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning.   Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos. Existing methods typically adapt image-based pretraining approaches to incorporate more frames. Instead, we advocate a "tracking to learn to recognize" approach. Our method called DoRA, leads to attention maps that Discover and tRAck objects over time in an end-to-end manner, using transformer cross-attention. We derive multiple views from the tracks and use them in a classical self-supervised distillation loss. Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.
</details>
<details>
<summary>摘要</summary>
自我指导学习已经开放了扩大预训练到数十亿张图像的潜力，因为没有注释。但我们是否可以更经济地使用数据？在这项工作中，我们尝试回答这个问题，并提供了两项贡献。首先，我们研究了一个“步行旅游”数据集，这是高解度、数小时长、不间断拍摄的首人视频。这些视频没有标签和排序，因此与人类学习的方式相似，适合自我指导学习。其次，我们介绍了一种适合从连续视频中学习的自我指导图像预训练方法。现有方法通常是将图像预训练方法与更多帧相结合。相反，我们提倡“跟踪以学习认知”的方法。我们的方法称为DoRA，通过转换器跨层关注来发现和跟踪 объек over time，并使用классиical自我指导液化损失来生成多视图。使用我们的新方法，一个单个步行旅游视频很奇迹地变成了 ImageNet 的强大竞争对手。
</details></li>
</ul>
<hr>
<h2 id="Universal-Visual-Decomposer-Long-Horizon-Manipulation-Made-Easy"><a href="#Universal-Visual-Decomposer-Long-Horizon-Manipulation-Made-Easy" class="headerlink" title="Universal Visual Decomposer: Long-Horizon Manipulation Made Easy"></a>Universal Visual Decomposer: Long-Horizon Manipulation Made Easy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08581">http://arxiv.org/abs/2310.08581</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zcczhang/UVD">https://github.com/zcczhang/UVD</a></li>
<li>paper_authors: Zichen Zhang, Yunshuang Li, Osbert Bastani, Abhishek Gupta, Dinesh Jayaraman, Yecheng Jason Ma, Luca Weihs<br>for:本研究旨在开发一种可靠、可 reuse 的视觉任务剖分方法，以便在 robotic 控制中学习长期 manipulate 任务。methods:本研究使用 pre-trained 视觉表示，通过检测视觉 embedding 空间中的阶段变化，自动找到视觉子任务。无需额外训练，UVD 可以减少 compositional generalization 问题，并在实际任务中显著提高性能。results:与基eline 相比，UVD 在 simulation 和实际任务中均表现出色，可以快速地学习和适应新任务。UVD 可以提供更好的 compositional generalization，并且可以用于 constructing goal-based reward shaping。<details>
<summary>Abstract</summary>
Real-world robotic tasks stretch over extended horizons and encompass multiple stages. Learning long-horizon manipulation tasks, however, is a long-standing challenge, and demands decomposing the overarching task into several manageable subtasks to facilitate policy learning and generalization to unseen tasks. Prior task decomposition methods require task-specific knowledge, are computationally intensive, and cannot readily be applied to new tasks. To address these shortcomings, we propose Universal Visual Decomposer (UVD), an off-the-shelf task decomposition method for visual long horizon manipulation using pre-trained visual representations designed for robotic control. At a high level, UVD discovers subgoals by detecting phase shifts in the embedding space of the pre-trained representation. Operating purely on visual demonstrations without auxiliary information, UVD can effectively extract visual subgoals embedded in the videos, while incurring zero additional training cost on top of standard visuomotor policy training. Goal-conditioned policies learned with UVD-discovered subgoals exhibit significantly improved compositional generalization at test time to unseen tasks. Furthermore, UVD-discovered subgoals can be used to construct goal-based reward shaping that jump-starts temporally extended exploration for reinforcement learning. We extensively evaluate UVD on both simulation and real-world tasks, and in all cases, UVD substantially outperforms baselines across imitation and reinforcement learning settings on in-domain and out-of-domain task sequences alike, validating the clear advantage of automated visual task decomposition within the simple, compact UVD framework.
</details>
<details>
<summary>摘要</summary>
实际世界中的 роботи工作通常是长时间的、多个阶段的。学习长时间的抓取任务是一个长期的挑战，需要将总体任务分解成可控制的子任务，以便策略学习和对未看过的任务进行泛化。现有的任务分解方法需要任务特定的知识， computationally 成本高，并不能方便地应用于新任务。为解决这些缺点，我们提出了一种通用视觉分解器（UVD），用于视觉长时间抓取任务的偏振 decomposition。UVD 使用预训练的视觉表示进行检测预Shift 在 embedding 空间中的阶段变化，从而发现子任务。不需要辅助信息，UVD 可以有效地从视频中提取视觉子任务，并不需要额外的训练成本。与标准视听动作策略训练相同，使用 UVD 发现的子任务可以显著提高含 композиitional 泛化的表现，并且可以用于构建目标基于的奖励形式，刺激执行扩展的探索。我们广泛测试了 UVD 在 simulator 和实际世界中，并在所有情况下都显著超越基eline，证明了自动视觉任务分解在简单、 компакт的 UVD 框架中的明显优势。
</details></li>
</ul>
<hr>
<h2 id="OmniControl-Control-Any-Joint-at-Any-Time-for-Human-Motion-Generation"><a href="#OmniControl-Control-Any-Joint-at-Any-Time-for-Human-Motion-Generation" class="headerlink" title="OmniControl: Control Any Joint at Any Time for Human Motion Generation"></a>OmniControl: Control Any Joint at Any Time for Human Motion Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08580">http://arxiv.org/abs/2310.08580</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neu-vi/OmniControl">https://github.com/neu-vi/OmniControl</a></li>
<li>paper_authors: Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, Huaizu Jiang</li>
<li>for: 用于 incorporating  flexible spatial control signals into a text-conditioned human motion generation model</li>
<li>methods: 使用 analytic spatial guidance 和 realism guidance 两种不同的指导方法</li>
<li>results: 实验结果表明，OmniControl 可以实现更加真实、协调和一致的人体动作生成，并且在不同 JOINTS 上的控制也有显著改善。<details>
<summary>Abstract</summary>
We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process. Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model. Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals. At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion. Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism. By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints. Experiments on HumanML3D and KIT-ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法 named OmniControl，用于在基于扩散过程的文本条件人体运动生成模型中 incorporating  flexible spatial control signals。与之前的方法不同，OmniControl 可以在不同的 JOINTS 和不同的时间点上使用 flexible spatial control signals，只需一个模型。我们提出了分析空间指导，以确保生成的运动能够紧跟输入控制信号。同时，我们还引入了真实性指导，以进一步让所有 JOINTS 都更加协调，生成更加合理的运动。这两种指导都是重要的，它们是彼此补做的，可以均衡控制准确性和运动真实性。通过将它们结合起来，OmniControl 可以生成更加真实、协调、遵循空间约束的运动。在 HumanML3D 和 KIT-ML 数据集上进行了实验，OmniControl 不仅在 pelvis 控制方面取得了显著改进，还在其他 JOINTS 上 incorporating 约束时表现了良好的结果。
</details></li>
</ul>
<hr>
<h2 id="HyperHuman-Hyper-Realistic-Human-Generation-with-Latent-Structural-Diffusion"><a href="#HyperHuman-Hyper-Realistic-Human-Generation-with-Latent-Structural-Diffusion" class="headerlink" title="HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion"></a>HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08579">http://arxiv.org/abs/2310.08579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snap-research/HyperHuman">https://github.com/snap-research/HyperHuman</a></li>
<li>paper_authors: Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, Sergey Tulyakov</li>
<li>for: 该论文目标是生成高度真实的人像图像，以满足在各种场景下的人像生成需求。</li>
<li>methods: 该论文提出了一种叫做HyperHuman的框架，该框架包括三个主要部分：1) 构建了一个大规模的人像数据集（名为HumanVerse），包括340万个图像和人 pose、深度和表面法向量等精心标注。2) 提出了一种叫做Latent Structural Diffusion Model的模型，该模型同时减去了深度和表面法向量以及生成的RGB图像中的噪声。3) 最后，提出了一种叫做Structure-Guided Refiner的组合方法，用于更加细腻地生成更高分辨率的人像图像。</li>
<li>results: 经过广泛的实验，该论文的框架实现了state-of-the-art的性能，可以在多种场景下生成高度真实的人像图像。<details>
<summary>Abstract</summary>
Despite significant advances in large-scale text-to-image models, achieving hyper-realistic human image generation remains a desirable yet unsolved task. Existing models like Stable Diffusion and DALL-E 2 tend to generate human images with incoherent parts or unnatural poses. To tackle these challenges, our key insight is that human image is inherently structural over multiple granularities, from the coarse-level body skeleton to fine-grained spatial geometry. Therefore, capturing such correlations between the explicit appearance and latent structure in one model is essential to generate coherent and natural human images. To this end, we propose a unified framework, HyperHuman, that generates in-the-wild human images of high realism and diverse layouts. Specifically, 1) we first build a large-scale human-centric dataset, named HumanVerse, which consists of 340M images with comprehensive annotations like human pose, depth, and surface normal. 2) Next, we propose a Latent Structural Diffusion Model that simultaneously denoises the depth and surface normal along with the synthesized RGB image. Our model enforces the joint learning of image appearance, spatial relationship, and geometry in a unified network, where each branch in the model complements to each other with both structural awareness and textural richness. 3) Finally, to further boost the visual quality, we propose a Structure-Guided Refiner to compose the predicted conditions for more detailed generation of higher resolution. Extensive experiments demonstrate that our framework yields the state-of-the-art performance, generating hyper-realistic human images under diverse scenarios. Project Page: https://snap-research.github.io/HyperHuman/
</details>
<details>
<summary>摘要</summary>
尽管大规模文本到图像模型已经取得了 significativo 进步，但 Achieving hyper-realistic human image generation 仍然是一个需要解决的任务。现有的模型如 Stable Diffusion 和 DALL-E 2 通常会生成人像图像中的部分不协调或不自然的姿势。为了解决这些挑战，我们的关键洞察是人像图像具有多个粒度的结构，从粗粒度的体姿skeleton到细粒度的空间几何。因此，捕捉这些相关性在一个模型中是关键，以生成协调的和自然的人像图像。为此，我们提出了一个统一框架，即 HyperHuman，可以生成宽泛的人像图像，高度真实和多样化的布局。specifically，我们采取以下三个步骤：1. 我们首先建立了一个大规模的人类中心的数据集，名为 HumanVerse，该集包含340万张图像，并包括人pose、深度和表面法向的全面注解。2. 接下来，我们提出了一种干扰难度和表面法向同时减震的模型，即 Latent Structural Diffusion Model。该模型同时学习图像外观、空间关系和几何结构，并在一个统一网络中进行结合学习。每个分支在模型中补做了对于彼此的结构意识和 текстуаль丰富的补做。3. 为了进一步提高视觉质量，我们还提出了一种结构指导的修正器，用于更详细地生成更高分辨率的图像。广泛的实验表明，我们的框架可以 дости到当前最佳性能，在多种enario下生成高度真实的人像图像。项目页面：https://snap-research.github.io/HyperHuman/
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Act-from-Actionless-Videos-through-Dense-Correspondences"><a href="#Learning-to-Act-from-Actionless-Videos-through-Dense-Correspondences" class="headerlink" title="Learning to Act from Actionless Videos through Dense Correspondences"></a>Learning to Act from Actionless Videos through Dense Correspondences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08576">http://arxiv.org/abs/2310.08576</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flow-diffusion/AVDC">https://github.com/flow-diffusion/AVDC</a></li>
<li>paper_authors: Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, Joshua B. Tenenbaum</li>
<li>for: 本研究旨在构建一种基于视频示例的机器人策略，可以在不同机器人和环境中可靠执行多种任务，仅从视频示例中学习而无需使用任何动作标注。</li>
<li>methods: 本方法利用图像作为任务免疑表示，同时使用文本来表示机器人目标。我们使用视频拼接技术生成机器人执行动作的视频，并利用密集对准关系来INFER机器人需要执行的具体动作。</li>
<li>results: 我们在表面 manipulate 和导航任务上证明了本方法的效果，并提供了一个开源框架，可以有效地模型视频，使得在四个GPU上进行高精度策略模型训练，可以在一天内完成。<details>
<summary>Abstract</summary>
In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that ``hallucinate'' robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four GPUs within a single day.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种方法，能够基于视频构建一个多功能机器人策略，可靠地执行多种任务在不同的机器人和环境中，只需从视频示例中学习而无需使用任何动作标注。我们的方法利用图像作为任务无关的表示，卷积 both 状态和动作信息，并使用文本作为机器人目标的通用表示。通过将视频“幻化”机器人执行动作，并在每帧之间进行紧密的对应关系，我们的方法可以从RGB视频中推理出要执行的closed-form动作，无需任何显式动作标注。这种特有的能力允许我们通过RGB视频进行训练策略，并将学习的策略部署到多种机器人任务中。我们在表ptop抓取和导航任务上证明了这种方法的效果。此外，我们还提供了一个开源的视频模型框架，可以使用四个GPU在单天内高效地训练高精度策略模型。
</details></li>
</ul>
<hr>
<h2 id="Idea2Img-Iterative-Self-Refinement-with-GPT-4V-ision-for-Automatic-Image-Design-and-Generation"><a href="#Idea2Img-Iterative-Self-Refinement-with-GPT-4V-ision-for-Automatic-Image-Design-and-Generation" class="headerlink" title="Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation"></a>Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08541">http://arxiv.org/abs/2310.08541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang</li>
<li>for:  automatic image design and generation</li>
<li>methods:  multimodal iterative self-refinement with GPT-4V(ision)</li>
<li>results:  images of better semantic and visual qualities, with the ability to process input ideas with interleaved image-text sequences and follow ideas with design instructions.<details>
<summary>Abstract</summary>
We introduce ``Idea to Image,'' a system that enables multimodal iterative self-refinement with GPT-4V(ision) for automatic image design and generation. Humans can quickly identify the characteristics of different text-to-image (T2I) models via iterative explorations. This enables them to efficiently convert their high-level generation ideas into effective T2I prompts that can produce good images. We investigate if systems based on large multimodal models (LMMs) can develop analogous multimodal self-refinement abilities that enable exploring unknown models or environments via self-refining tries. Idea2Img cyclically generates revised T2I prompts to synthesize draft images, and provides directional feedback for prompt revision, both conditioned on its memory of the probed T2I model's characteristics. The iterative self-refinement brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img can process input ideas with interleaved image-text sequences, follow ideas with design instructions, and generate images of better semantic and visual qualities. The user preference study validates the efficacy of multimodal iterative self-refinement on automatic image design and generation.
</details>
<details>
<summary>摘要</summary>
我们介绍“想法到图像”系统，该系统允许多Modal迭代自修正（GPT-4V）为自动图像设计和生成。人类可以快速认识不同的文本到图像（T2I）模型的特征，通过迭代探索来快速转化高级生成想法为有效的T2I提示，以生成好的图像。我们研究了基于大型多Modal模型（LMM）是否可以发展类似的多Modal自修复能力，以实现探索未知模型或环境 via 自修复尝试。Idea2Img 在循环生成修订T2I提示，并提供向提示修改的指导反馈，两者均基于它的记忆 probed T2I 模型的特征。多Modal迭代自修正带来了 Idea2Img 多种优势，包括可以处理交错的图像文本序列、跟随设计指令、生成更好的 semantic 和视觉质量的图像。用户偏好调查证明了自动图像设计和生成中多Modal迭代自修复的有效性。
</details></li>
</ul>
<hr>
<h2 id="Image2PCI-–-A-Multitask-Learning-Framework-for-Estimating-Pavement-Condition-Indices-Directly-from-Images"><a href="#Image2PCI-–-A-Multitask-Learning-Framework-for-Estimating-Pavement-Condition-Indices-Directly-from-Images" class="headerlink" title="Image2PCI – A Multitask Learning Framework for Estimating Pavement Condition Indices Directly from Images"></a>Image2PCI – A Multitask Learning Framework for Estimating Pavement Condition Indices Directly from Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08538">http://arxiv.org/abs/2310.08538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neema Jakisa Owor, Hang Du, Abdulateef Daud, Armstrong Aboah, Yaw Adu-Gyamfi</li>
<li>For: The paper aims to develop a unified multi-tasking model for estimating Pavement Condition Index (PCI) directly from top-down pavement images.* Methods: The proposed model is a multi-task architecture that combines feature extraction and four decoders for PCI estimation, crack detection, and segmentation. The model uses deep learning techniques and is trained on a benchmarked and open pavement distress dataset.* Results: The proposed model achieves excellent accuracy on all related tasks for crack detection and segmentation, and can estimate PCI directly from images at real-time speeds. This is the first work that can accomplish this task, to the best of the authors’ knowledge.<details>
<summary>Abstract</summary>
The Pavement Condition Index (PCI) is a widely used metric for evaluating pavement performance based on the type, extent and severity of distresses detected on a pavement surface. In recent times, significant progress has been made in utilizing deep-learning approaches to automate PCI estimation process. However, the current approaches rely on at least two separate models to estimate PCI values -- one model dedicated to determining the type and extent and another for estimating their severity. This approach presents several challenges, including complexities, high computational resource demands, and maintenance burdens that necessitate careful consideration and resolution. To overcome these challenges, the current study develops a unified multi-tasking model that predicts the PCI directly from a top-down pavement image. The proposed architecture is a multi-task model composed of one encoder for feature extraction and four decoders to handle specific tasks: two detection heads, one segmentation head and one PCI estimation head. By multitasking, we are able to extract features from the detection and segmentation heads for automatically estimating the PCI directly from the images. The model performs very well on our benchmarked and open pavement distress dataset that is annotated for multitask learning (the first of its kind). To our best knowledge, this is the first work that can estimate PCI directly from an image at real time speeds while maintaining excellent accuracy on all related tasks for crack detection and segmentation.
</details>
<details>
<summary>摘要</summary>
《路面条件指数（PCI）评估 metric 是评估路面性能的 widely 使用方法，基于路面表面上的类型、规模和严重程度的病诊。在最近的时间里，深入学习方法在自动化 PCI 评估过程中进行了显著的进步。然而，当前的方法都需要至少两个分开的模型来计算 PCI 值 -- 一个模型用于确定类型和规模，另一个用于估计严重程度。这种方法存在许多挑战，包括复杂性、高计算资源需求和维护压力，需要仔细考虑和解决。为了突破这些挑战，当前的研究开发了一种简化多任务模型，可以直接从路面图像中预测 PCI。我们的建议的架构包括一个嵌入器 для特征提取和四个解码器来处理特定任务：两个检测头、一个分割头和一个 PCI 估计头。通过多任务学习，我们能够自动从检测和分割任务中提取特征，以便直接从图像中预测 PCI。我们的模型在我们自己练制的和公开的路面裂隙数据集上表现出色，并且在所有相关任务上保持了高精度。到我们所知，这是第一个可以在实时速度下从图像中直接预测 PCI，并且保持所有相关任务的高精度的工作。》
</details></li>
</ul>
<hr>
<h2 id="XAI-Benchmark-for-Visual-Explanation"><a href="#XAI-Benchmark-for-Visual-Explanation" class="headerlink" title="XAI Benchmark for Visual Explanation"></a>XAI Benchmark for Visual Explanation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08537">http://arxiv.org/abs/2310.08537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Zhang, Siyi Gu, James Song, Bo Pan, Liang Zhao<br>for:The paper aims to provide a benchmark for evaluating the performance of visual explanation models in the context of image data.methods:The paper introduces a comprehensive visual explanation pipeline that integrates data loading, preprocessing, experimental setup, and model evaluation processes. The pipeline is designed to enable fair comparisons of various visual explanation techniques.results:The paper provides a comprehensive review of over 10 evaluation methods for visual explanation and conducts experiments on selected datasets using various model-centered and ground truth-centered evaluation metrics. The results demonstrate the effectiveness of the proposed benchmark for evaluating the performance of visual explanation models.Here is the simplified Chinese text for the three key points:for:这篇论文的目的是为了提供一个用于评估图像数据上视觉解释模型表现的标准 benchmarck。methods:这篇论文介绍了一个完整的视觉解释管道，该管道 integrates 数据加载、预处理、实验设置和模型评估过程。该管道的设计目的是允许不同的视觉解释技术进行公正的比较。results:这篇论文提供了一个涵盖 более十种评估方法的全面的视觉解释评估文献，并在选择的数据集上使用不同的模型中心和真实数据中心评估 метри来进行实验。结果表明该 benchmark 对视觉解释模型表现的评估具有效果。<details>
<summary>Abstract</summary>
The rise of deep learning algorithms has led to significant advancements in computer vision tasks, but their "black box" nature has raised concerns regarding interpretability. Explainable AI (XAI) has emerged as a critical area of research aiming to open this "black box", and shed light on the decision-making process of AI models. Visual explanations, as a subset of Explainable Artificial Intelligence (XAI), provide intuitive insights into the decision-making processes of AI models handling visual data by highlighting influential areas in an input image. Despite extensive research conducted on visual explanations, most evaluations are model-centered since the availability of corresponding real-world datasets with ground truth explanations is scarce in the context of image data. To bridge this gap, we introduce an XAI Benchmark comprising a dataset collection from diverse topics that provide both class labels and corresponding explanation annotations for images. We have processed data from diverse domains to align with our unified visual explanation framework. We introduce a comprehensive Visual Explanation pipeline, which integrates data loading, preprocessing, experimental setup, and model evaluation processes. This structure enables researchers to conduct fair comparisons of various visual explanation techniques. In addition, we provide a comprehensive review of over 10 evaluation methods for visual explanation to assist researchers in effectively utilizing our dataset collection. To further assess the performance of existing visual explanation methods, we conduct experiments on selected datasets using various model-centered and ground truth-centered evaluation metrics. We envision this benchmark could facilitate the advancement of visual explanation models. The XAI dataset collection and easy-to-use code for evaluation are publicly accessible at https://xaidataset.github.io.
</details>
<details>
<summary>摘要</summary>
“深度学习算法的出现导致计算机视觉任务得到了重大进步，但它们的“黑盒”性带来了解释性的担忧。解释人工智能（XAI）成为了一个重要的研究领域，旨在打开这“黑盒”，了解人工智能模型做出决策的过程。视觉解释，作为解释人工智能的一个子集，为处理视觉数据的人工智能模型提供了直观的决策过程解释。然而，大多数研究都是模型中心的，因为对图像数据的相关真实数据集的可用性非常scarce。为了bridging这个差距，我们介绍了一个XAI Benchmark，包括从多种主题收集的数据集，每个数据集都包括图像的类别标签和相应的解释注释。我们对这些数据进行了多种领域的处理，以适应我们的统一的视觉解释框架。我们还提供了一个完整的视觉解释管线，包括数据加载、预处理、实验设置和模型评估过程。这种结构使研究人员能够进行公正的比较多种视觉解释技术。此外，我们还提供了更 than 10 评估方法的完整审查，以帮助研究人员有效地利用我们的数据集。为了进一步评估现有的视觉解释方法，我们在选择的数据集上进行了多种模型中心和真实数据中心的评估指标。我们希望这个Benchmark能够促进视觉解释模型的进步。XAI数据集和使用方式的代码公开访问，可以在 <https://xaidataset.github.io> 中找到。”
</details></li>
</ul>
<hr>
<h2 id="Animating-Street-View"><a href="#Animating-Street-View" class="headerlink" title="Animating Street View"></a>Animating Street View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08534">http://arxiv.org/abs/2310.08534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jblsmith/street-view-movie-maker">https://github.com/jblsmith/street-view-movie-maker</a></li>
<li>paper_authors: Mengyi Shan, Brian Curless, Ira Kemelmacher-Shlizerman, Steve Seitz</li>
<li>for: 这个系统可以自动将街景图像带到生命中，通过插入自然行为的行人和车辆，并且规划路径和交通行为，同时还会模拟遮盖和阴影效果。</li>
<li>methods: 这个系统使用了去除原有的人和车辆、插入运动对象、规划路径和交通行为、模拟人群行为、并且使用一致的照明、可见度、遮盖和阴影效果来实现。</li>
<li>results: 这个系统在各种街景图像中得到了丰富的生命化效果，包括正常的拍摄图像和扫描图像。<details>
<summary>Abstract</summary>
We present a system that automatically brings street view imagery to life by populating it with naturally behaving, animated pedestrians and vehicles. Our approach is to remove existing people and vehicles from the input image, insert moving objects with proper scale, angle, motion, and appearance, plan paths and traffic behavior, as well as render the scene with plausible occlusion and shadowing effects. The system achieves these by reconstructing the still image street scene, simulating crowd behavior, and rendering with consistent lighting, visibility, occlusions, and shadows. We demonstrate results on a diverse range of street scenes including regular still images and panoramas.
</details>
<details>
<summary>摘要</summary>
我们提出了一种系统，可以自动将街景图像带到生命中，通过插入自然行为的步行者和交通工具，让图像具有更加生动的效果。我们的方法是从输入图像中移除现有的人员和交通工具，插入正确的规模、角度、运动和外观的运动对象，规划路径和交通行为，同时进行透明度和阴影效果的渲染。该系统通过重建静止图像街景、模拟人群行为、渲染透明度和阴影效果来实现这一目标。我们在多种不同的街景图像中进行了证明，包括普通的静止图像和拍摄的Panorama。
</details></li>
</ul>
<hr>
<h2 id="UniPose-Detecting-Any-Keypoints"><a href="#UniPose-Detecting-Any-Keypoints" class="headerlink" title="UniPose: Detecting Any Keypoints"></a>UniPose: Detecting Any Keypoints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08530">http://arxiv.org/abs/2310.08530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IDEA-Research/UniPose">https://github.com/IDEA-Research/UniPose</a></li>
<li>paper_authors: Jie Yang, Ailing Zeng, Ruimao Zhang, Lei Zhang</li>
<li>for: 这个研究旨在探索一个统一的框架，叫做UniPose，以探测任何骨骼结构的人类或动物体姿的关键点，包括眼睛、脚、爪子等细部信息，以便进一步掌握和操作细部物品的视觉理解。</li>
<li>methods: 这个研究使用了一个统一的框架，叫做UniPose，让探测关键点的任何类型的物品，包括人类和动物体姿，并且使用了文本或图像提示来进行探测。</li>
<li>results: 研究结果显示UniPose能够具有优秀的细部定位和普遍化能力，可以在不同的图像样式、类别和姿势下进行精确的关键点探测。<details>
<summary>Abstract</summary>
This work proposes a unified framework called UniPose to detect keypoints of any articulated (e.g., human and animal), rigid, and soft objects via visual or textual prompts for fine-grained vision understanding and manipulation. Keypoint is a structure-aware, pixel-level, and compact representation of any object, especially articulated objects. Existing fine-grained promptable tasks mainly focus on object instance detection and segmentation but often fail to identify fine-grained granularity and structured information of image and instance, such as eyes, leg, paw, etc. Meanwhile, prompt-based keypoint detection is still under-explored. To bridge the gap, we make the first attempt to develop an end-to-end prompt-based keypoint detection framework called UniPose to detect keypoints of any objects. As keypoint detection tasks are unified in this framework, we can leverage 13 keypoint detection datasets with 338 keypoints across 1,237 categories over 400K instances to train a generic keypoint detection model. UniPose can effectively align text-to-keypoint and image-to-keypoint due to the mutual enhancement of textual and visual prompts based on the cross-modality contrastive learning optimization objectives. Our experimental results show that UniPose has strong fine-grained localization and generalization abilities across image styles, categories, and poses. Based on UniPose as a generalist keypoint detector, we hope it could serve fine-grained visual perception, understanding, and generation.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一个统一框架called UniPose，用于检测任何骨Structured object（如人类和动物）的关键点，通过视觉或文本提示进行细腻视觉理解和操作。关键点是一种结构意识、像素级别、紧凑的对象表示，特别是复杂的对象。现有的细腻提示任务主要集中在对象实例检测和分割，但经常无法识别图像和实例的细腻特征，如眼睛、脚、爪子等。同时，基于提示的关键点检测还是下不足探索的领域。为了填补这个空白，我们首次尝试开发了一个端到端基于提示的关键点检测框架，可以检测任何对象的关键点。由于这个框架中的关键点检测任务被统一，我们可以使用13个关键点检测数据集，包含338个关键点，涵盖1,237个类型，共400,000个实例来训练一个通用的关键点检测模型。UniPose可以有效地将文本到关键点和图像到关键点进行对应，基于跨Modalities的对比学习优化目标，从而实现文本和图像之间的协调。我们的实验结果表明，UniPose具有强大的细腻地方化和泛化能力，可以在不同的图像风格、类型和姿势下进行高精度的关键点检测。基于UniPose作为一个通用的关键点检测器，我们希望它可以为细腻视觉理解、理解和生成提供服务。
</details></li>
</ul>
<hr>
<h2 id="GaussianDreamer-Fast-Generation-from-Text-to-3D-Gaussian-Splatting-with-Point-Cloud-Priors"><a href="#GaussianDreamer-Fast-Generation-from-Text-to-3D-Gaussian-Splatting-with-Point-Cloud-Priors" class="headerlink" title="GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors"></a>GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08529">http://arxiv.org/abs/2310.08529</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hustvl/GaussianDreamer">https://github.com/hustvl/GaussianDreamer</a></li>
<li>paper_authors: Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, Xinggang Wang</li>
<li>for: 本研究旨在bridging 2D和3D扩散模型之间，通过使用 latest 3D Gaussian splatting representation，实现高质量和高效的3D生成。</li>
<li>methods: 本研究提出了一种fast 3D生成框架，named as \name，其中2D扩散模型提供初始化点云约束，而3D扩散模型为 initialization提供点云质量约束。操作包括噪点增长和颜色干扰，以提高 initialized Gaussians。</li>
<li>results: 根据实验结果，我们的 \name 可以在一个GPU上生成高质量的3D实例，耗时只有25分钟，比之前的方法更快。生成的实例可以 direct rendering in real time。示例和代码可以在<a target="_blank" rel="noopener" href="https://taoranyi.com/gaussiandreamer/">https://taoranyi.com/gaussiandreamer/</a> 中找到。<details>
<summary>Abstract</summary>
In recent times, the generation of 3D assets from text prompts has shown impressive results. Both 2D and 3D diffusion models can generate decent 3D objects based on prompts. 3D diffusion models have good 3D consistency, but their quality and generalization are limited as trainable 3D data is expensive and hard to obtain. 2D diffusion models enjoy strong abilities of generalization and fine generation, but the 3D consistency is hard to guarantee. This paper attempts to bridge the power from the two types of diffusion models via the recent explicit and efficient 3D Gaussian splatting representation. A fast 3D generation framework, named as \name, is proposed, where the 3D diffusion model provides point cloud priors for initialization and the 2D diffusion model enriches the geometry and appearance. Operations of noisy point growing and color perturbation are introduced to enhance the initialized Gaussians. Our \name can generate a high-quality 3D instance within 25 minutes on one GPU, much faster than previous methods, while the generated instances can be directly rendered in real time. Demos and code are available at https://taoranyi.com/gaussiandreamer/.
</details>
<details>
<summary>摘要</summary>
现今，从文本提示生成3D资产已经获得了优秀的结果。两种类型的扩散模型都能够生成可以接受的3D物件，包括2D扩散模型和3D扩散模型。3D扩散模型具有良好的3D一致性，但是其质量和应用范围受到可读性和实际应用的限制。2D扩散模型具有强大的一般化和细节生成能力，但是3D一致性很难保证。这篇论文尝试将2D和3D扩散模型的力量融合起来，通过最近的明确和高效的3D Gaussian抛物表示。我们提出了一个快速的3D生成框架，名为\name，其中3D扩散模型提供初始化的点云偏好，而2D扩散模型则丰富了几何和外观。我们引入随机点增长和颜色干扰的操作来改善初始化的Gaussian。我们的\name可以在一个GPU上生成高品质的3D实例，比前方法更快，且生成的实例可以实时显示。 demo 和代码可以在 <https://taoranyi.com/gaussiandreamer/> 上获取。
</details></li>
</ul>
<hr>
<h2 id="4D-Gaussian-Splatting-for-Real-Time-Dynamic-Scene-Rendering"><a href="#4D-Gaussian-Splatting-for-Real-Time-Dynamic-Scene-Rendering" class="headerlink" title="4D Gaussian Splatting for Real-Time Dynamic Scene Rendering"></a>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08528">http://arxiv.org/abs/2310.08528</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hustvl/4DGaussians">https://github.com/hustvl/4DGaussians</a></li>
<li>paper_authors: Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang</li>
<li>for: 实现高效的动态场景渲染，包括模拟复杂运动和高分辨率渲染。</li>
<li>methods: 提出了4D Gaussian Splatting（4D-GS）方法，通过构建高效的凝固场和 hexPlane 连接来实现高效的形态和 Gaussian 运动模拟。</li>
<li>results: 实现了70帧&#x2F;秒的实时渲染，在800x800分辨率的RTX 3090 GPU上，并且与之前的状态OF艺术方法相当或更高的质量。更多 demo 和代码可以在 <a target="_blank" rel="noopener" href="https://guanjunwu.github.io/4dgs/">https://guanjunwu.github.io/4dgs/</a> 上找到。<details>
<summary>Abstract</summary>
Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to maintain. We introduce the 4D Gaussian Splatting (4D-GS) to achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency. An efficient deformation field is constructed to model both Gaussian motions and shape deformations. Different adjacent Gaussians are connected via a HexPlane to produce more accurate position and shape deformations. Our 4D-GS method achieves real-time rendering under high resolutions, 70 FPS at a 800$\times$800 resolution on an RTX 3090 GPU, while maintaining comparable or higher quality than previous state-of-the-art methods. More demos and code are available at https://guanjunwu.github.io/4dgs/.
</details>
<details>
<summary>摘要</summary>
Dynamic scene representation and rendering has been an important but challenging task. Especially, accurately modeling complex motions is often difficult to achieve while maintaining high efficiency. We propose the 4D Gaussian Splatting (4D-GS) method to achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency. An efficient deformation field is constructed to model both Gaussian motions and shape deformations. Different adjacent Gaussians are connected via a HexPlane to produce more accurate position and shape deformations. Our 4D-GS method achieves real-time rendering under high resolutions, with 70 FPS at an 800x800 resolution on an RTX 3090 GPU, while maintaining comparable or higher quality than previous state-of-the-art methods. More demos and code are available at https://guanjunwu.github.io/4dgs/.Here's the breakdown of the translation:* "dynamic scene" becomes "动态场景" (dòngtài chǎngjìng)* "representation" becomes "表示" (biǎozhì)* "rendering" becomes "渲染" (chūjiān)* "challenging task" becomes "difficult task" ( Zhèngshì zhèngdào)* "Gaussian motions" becomes "高斯运动" (gāosī yùndòng)* "shape deformations" becomes "形态变形" (xíngtài biànxiàng)* "HexPlane" becomes "六面体" (liùmiàn tǐ)* "real-time" becomes "实时" (shíshí)* "high resolutions" becomes "高分辨率" (gāo fēnbiàn zhù)* "70 FPS" becomes "70帧每秒" (qīshí fēn fēi shí)* "RTX 3090 GPU" becomes "RTX 3090 GPU" (RTX 3090 GPU)* "while maintaining" becomes "保持" (bǎojìn)* "comparable or higher quality" becomes "相当或更高质量" (xiāngdàng huí gèng qiàngyù)* "previous state-of-the-art methods" becomes "前一代方法" (qián yīdài fāngchéng)* "More demos and code" becomes "更多示例和代码" (gèng duō shìjì yǔ gōngcháng)Note that the translation is done in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I can provide that instead.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Learning-of-Object-Centric-Embeddings-for-Cell-Instance-Segmentation-in-Microscopy-Images"><a href="#Unsupervised-Learning-of-Object-Centric-Embeddings-for-Cell-Instance-Segmentation-in-Microscopy-Images" class="headerlink" title="Unsupervised Learning of Object-Centric Embeddings for Cell Instance Segmentation in Microscopy Images"></a>Unsupervised Learning of Object-Centric Embeddings for Cell Instance Segmentation in Microscopy Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08501">http://arxiv.org/abs/2310.08501</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/funkelab/cellulus">https://github.com/funkelab/cellulus</a></li>
<li>paper_authors: Steffen Wolf, Manan Lalit, Henry Westmacott, Katie McDole, Jan Funke</li>
<li>for: This paper is written for the task of segmenting objects in microscopy images, which is an important task in biomedical applications.</li>
<li>methods: The paper introduces a new method called object-centric embeddings (OCEs) that learns to embed image patches in a way that preserves spatial offsets between patches from the same object.</li>
<li>results: The paper shows that the OCE method can be used to delineate individual objects and obtain instance segmentations, and evaluates the method on nine diverse large-scale microscopy datasets. The results show that the method leads to substantially improved results compared to state-of-the-art baselines on six out of nine datasets, and performs on par on the remaining three datasets. If ground-truth annotations are available, the method can serve as an excellent starting point for supervised training, reducing the required amount of ground-truth needed by one order of magnitude.<details>
<summary>Abstract</summary>
Segmentation of objects in microscopy images is required for many biomedical applications. We introduce object-centric embeddings (OCEs), which embed image patches such that the spatial offsets between patches cropped from the same object are preserved. Those learnt embeddings can be used to delineate individual objects and thus obtain instance segmentations. Here, we show theoretically that, under assumptions commonly found in microscopy images, OCEs can be learnt through a self-supervised task that predicts the spatial offset between image patches. Together, this forms an unsupervised cell instance segmentation method which we evaluate on nine diverse large-scale microscopy datasets. Segmentations obtained with our method lead to substantially improved results, compared to state-of-the-art baselines on six out of nine datasets, and perform on par on the remaining three datasets. If ground-truth annotations are available, our method serves as an excellent starting point for supervised training, reducing the required amount of ground-truth needed by one order of magnitude, thus substantially increasing the practical applicability of our method. Source code is available at https://github.com/funkelab/cellulus.
</details>
<details>
<summary>摘要</summary>
分割微scopic图像中的对象是生物医学应用中的重要任务。我们介绍了对象中心嵌入（OCE），它将图像块嵌入以保留归一化的空间偏移。这些学习的嵌入可以用来划分个体对象并获取实例分割。我们证明了，在微scopic图像中常见的假设下，OCE可以通过自动学习任务预测图像块之间的空间偏移来学习。这两个组件共同形成了无监督细胞实例分割方法，我们在九个大规模微scopic图像集合上进行了评估。 segmentation结果表现出色，相比于状态函数基eline，在六个 dataset 上表现出了明显的改善，并在剩下三个 dataset 上表现在eline。如果有ground truth标注，我们的方法可以作为supervised学习的初始点， thereby reducing the amount of ground truth needed by one order of magnitude, thereby significantly increasing the practical applicability of our method。可以在https://github.com/funkelab/cellulus上获取源代码。
</details></li>
</ul>
<hr>
<h2 id="MotionDirector-Motion-Customization-of-Text-to-Video-Diffusion-Models"><a href="#MotionDirector-Motion-Customization-of-Text-to-Video-Diffusion-Models" class="headerlink" title="MotionDirector: Motion Customization of Text-to-Video Diffusion Models"></a>MotionDirector: Motion Customization of Text-to-Video Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08465">http://arxiv.org/abs/2310.08465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/showlab/MotionDirector">https://github.com/showlab/MotionDirector</a></li>
<li>paper_authors: Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, Mike Zheng Shou</li>
<li>for: 这种研究的目的是为了使用扩展的涂抹模型生成具有自定义动作的视频。</li>
<li>methods: 这种方法使用了全模型调整、附加层参数精度调整以及低级适应（LoRAs）等方法进行自定义动作的调整。</li>
<li>results: 实验结果表明，提议的方法可以生成具有多样化外观的自定义动作视频。此外，该方法还可以支持多种下游应用，如混合不同视频的外观和动作，以及将单个图像涂抹到自定义动作中。<details>
<summary>Abstract</summary>
Large-scale pre-trained diffusion models have exhibited remarkable capabilities in diverse video generations. Given a set of video clips of the same motion concept, the task of Motion Customization is to adapt existing text-to-video diffusion models to generate videos with this motion. For example, generating a video with a car moving in a prescribed manner under specific camera movements to make a movie, or a video illustrating how a bear would lift weights to inspire creators. Adaptation methods have been developed for customizing appearance like subject or style, yet unexplored for motion. It is straightforward to extend mainstream adaption methods for motion customization, including full model tuning, parameter-efficient tuning of additional layers, and Low-Rank Adaptions (LoRAs). However, the motion concept learned by these methods is often coupled with the limited appearances in the training videos, making it difficult to generalize the customized motion to other appearances. To overcome this challenge, we propose MotionDirector, with a dual-path LoRAs architecture to decouple the learning of appearance and motion. Further, we design a novel appearance-debiased temporal loss to mitigate the influence of appearance on the temporal training objective. Experimental results show the proposed method can generate videos of diverse appearances for the customized motions. Our method also supports various downstream applications, such as the mixing of different videos with their appearance and motion respectively, and animating a single image with customized motions. Our code and model weights will be released.
</details>
<details>
<summary>摘要</summary>
大规模预训 diffusion 模型在多种视频生成任务中表现出色，例如：根据给定的视频clip生成具有指定动作的视频。为了解决这个问题，我们提出了 MotionDirector，它使用了双路LoRAs架构来解耦动作和外观的学习。此外，我们还设计了一种新的外观偏好的时间损失来减轻外观对时间目标的影响。实验结果表明，我们的方法可以生成具有多样化外观的动作视频。此外，我们的方法还支持多种下游应用程序，例如：将不同视频的外观和动作分别混合，并将一张图片动画为自定义动作。我们将发布代码和模型参数。
</details></li>
</ul>
<hr>
<h2 id="Proving-the-Potential-of-Skeleton-Based-Action-Recognition-to-Automate-the-Analysis-of-Manual-Processes"><a href="#Proving-the-Potential-of-Skeleton-Based-Action-Recognition-to-Automate-the-Analysis-of-Manual-Processes" class="headerlink" title="Proving the Potential of Skeleton Based Action Recognition to Automate the Analysis of Manual Processes"></a>Proving the Potential of Skeleton Based Action Recognition to Automate the Analysis of Manual Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08451">http://arxiv.org/abs/2310.08451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marlin Berger, Frederik Cloppenburg, Jens Eufinger, Thomas Gries</li>
<li>For: The paper aims to improve the analysis and monitoring of manual processes in manufacturing sectors such as textiles and electronics by using machine learning (ML) methods.* Methods: The paper uses a skeleton-based action recognition approach, which is a recent successful method in machine vision tasks, to detect the current motion performed by an operator in manual assembly. The authors also develop a ML pipeline to enable extensive research on different pre-processing methods and neural nets.* Results: The authors find that ML methods can provide higher flexibility, self-sufficiency, and lower costs compared to traditional methods such as Methods-Time-Measurement (MTM). They also demonstrate that their approach can be applied to all kinds of manual processes, not just manual assembly.<details>
<summary>Abstract</summary>
In manufacturing sectors such as textiles and electronics, manual processes are a fundamental part of production. The analysis and monitoring of the processes is necessary for efficient production design. Traditional methods for analyzing manual processes are complex, expensive, and inflexible. Compared to established approaches such as Methods-Time-Measurement (MTM), machine learning (ML) methods promise: Higher flexibility, self-sufficient & permanent use, lower costs. In this work, based on a video stream, the current motion class in a manual assembly process is detected. With information on the current motion, Key-Performance-Indicators (KPIs) can be derived easily. A skeleton-based action recognition approach is taken, as this field recently shows major success in machine vision tasks. For skeleton-based action recognition in manual assembly, no sufficient pre-work could be found. Therefore, a ML pipeline is developed, to enable extensive research on different (pre-) processing methods and neural nets. Suitable well generalizing approaches are found, proving the potential of ML to enhance analyzation of manual processes. Models detect the current motion, performed by an operator in manual assembly, but the results can be transferred to all kinds of manual processes.
</details>
<details>
<summary>摘要</summary>
在制造业务中，如纺织和电子等，手动过程是生产的基本组成部分。分析和监测这些过程是生产设计的必要条件。传统方法分析手动过程相对复杂、昂贵和不灵活。相比已有的方法，如方法时间测量（MTM），机器学习（ML）方法承诺：更高的灵活性、自主和常规使用、更低的成本。在这项工作中，通过视频流，检测当前手动 Assembly 过程中的动作类别。通过动作类别信息，可以轻松地 derivation Key-Performance-Indicators（KPIs）。我们采用skeleton基于动作识别方法，因为这个领域最近几年在机器视觉任务中占据了主导地位。对于手动 Assembly 中skeleton基于动作识别，没有充分的前置工作。因此，我们开发了一个ML管道，以便进行广泛的不同（前）处理方法和神经网络的研究。适合通用的方法被发现，证明了机器学习可以增强手动过程的分析。模型可以检测手动 Assembly 过程中操作员现在执行的动作，但结果可以应用于所有的手动过程。
</details></li>
</ul>
<hr>
<h2 id="Assessing-of-Soil-Erosion-Risk-Through-Geoinformation-Sciences-and-Remote-Sensing-–-A-Review"><a href="#Assessing-of-Soil-Erosion-Risk-Through-Geoinformation-Sciences-and-Remote-Sensing-–-A-Review" class="headerlink" title="Assessing of Soil Erosion Risk Through Geoinformation Sciences and Remote Sensing – A Review"></a>Assessing of Soil Erosion Risk Through Geoinformation Sciences and Remote Sensing – A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08430">http://arxiv.org/abs/2310.08430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lachezar Filchev, Vasil Kolev</li>
<li>for: 这篇论文主要是为了评估不同类型和结构的风化模型，以及它们在全球各地的应用。</li>
<li>methods: 这篇论文使用了空间分析技术，如地理信息系统（GIS），以进行风化风险评估，包括美国和世界各地的常用USLE和RUSLE方法，以及更进一步的实验室方法和人工智能技术。</li>
<li>results: 这篇论文提出了一种可能的未来发展方向，即采用人工智能技术进行风化风险评估。<details>
<summary>Abstract</summary>
During past decades a marked manifestation of widespread erosion phenomena was studied worldwide. Global conservation community has launched campaigns at local, regional and continental level in developing countries for preservation of soil resources in order not only to stop or mitigate human impact on nature but also to improve life in rural areas introducing new approaches for soil cultivation. After the adoption of Sustainable Development Goals of UNs and launching several world initiatives such as the Land Degradation Neutrality (LDN) the world came to realize the very importance of the soil resources on which the biosphere relies for its existence. The main goal of the chapter is to review different types and structures erosion models as well as their applications. Several methods using spatial analysis capabilities of geographic information systems (GIS) are in operation for soil erosion risk assessment, such as Universal Soil Loss Equation (USLE), Revised Universal Soil Loss Equation (RUSLE) in operation worldwide and in the USA and MESALES model. These and more models are being discussed in the present work alongside more experimental models and methods for assessing soil erosion risk such as Artificial Intelligence (AI), Machine and Deep Learning, etc. At the end of this work, a prospectus for the future development of soil erosion risk assessment is drawn.
</details>
<details>
<summary>摘要</summary>
The main goal of this chapter is to review different types and structures of erosion models, as well as their applications. Various methods using spatial analysis capabilities of geographic information systems (GIS) are currently in operation for soil erosion risk assessment, such as the Universal Soil Loss Equation (USLE), the Revised Universal Soil Loss Equation (RUSLE), and the MESALES model. These and other models, as well as more experimental models and methods for assessing soil erosion risk, such as Artificial Intelligence (AI), Machine Learning, and Deep Learning, are discussed in this work. Finally, a prospectus for the future development of soil erosion risk assessment is drawn.
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Data-Augmentation-for-Rotational-Invariance-in-Convolutional-Neural-Networks"><a href="#Revisiting-Data-Augmentation-for-Rotational-Invariance-in-Convolutional-Neural-Networks" class="headerlink" title="Revisiting Data Augmentation for Rotational Invariance in Convolutional Neural Networks"></a>Revisiting Data Augmentation for Rotational Invariance in Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08429">http://arxiv.org/abs/2310.08429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facundoq/rotational_invariance_data_augmentation">https://github.com/facundoq/rotational_invariance_data_augmentation</a></li>
<li>paper_authors: Facundo Manuel Quiroga, Franco Ronchetti, Laura Lanzarini, Aurelio Fernandez-Bariviera</li>
<li>for: 这个论文是为了研究如何在图像分类任务中实现旋转不变性。</li>
<li>methods: 该论文使用了数据增强法和两种特殊的Convolutional Neural Networks（Spatial Transformer Networks和Group Equivariant CNNs）来实现旋转不变性。</li>
<li>results: 研究发现，通过数据增强法可以让网络在旋转图像上准确分类，但是这需要更多的训练时间。此外，研究还发现了哪些层在网络中帮助网络编码旋转不变性。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNN) offer state of the art performance in various computer vision tasks. Many of those tasks require different subtypes of affine invariances (scale, rotational, translational) to image transformations. Convolutional layers are translation equivariant by design, but in their basic form lack invariances. In this work we investigate how best to include rotational invariance in a CNN for image classification. Our experiments show that networks trained with data augmentation alone can classify rotated images nearly as well as in the normal unrotated case; this increase in representational power comes only at the cost of training time. We also compare data augmentation versus two modified CNN models for achieving rotational invariance or equivariance, Spatial Transformer Networks and Group Equivariant CNNs, finding no significant accuracy increase with these specialized methods. In the case of data augmented networks, we also analyze which layers help the network to encode the rotational invariance, which is important for understanding its limitations and how to best retrain a network with data augmentation to achieve invariance to rotation.
</details>
<details>
<summary>摘要</summary>
convolutional neural networks (CNN) 提供了计算机视觉任务中的状态机器人表现。这些任务中有许多不同的Affine invariance（比例、旋转、平移）图像变换需求。 convolutional layers 是设计为 equivariant 的，但在其基本形式中缺乏抗变征性。在这项工作中，我们调查了如何在 CNN 中包含旋转不变性，以提高图像分类的表现。我们的实验结果表明，使用数据增强alone 可以将旋转图像分类到 nearly 与 Normal 无旋转情况下分类的程度相似; 这种增强的表现力只需要增加训练时间的代价。我们还比较了数据增强与两种修改后 CNN 模型来实现旋转不变性或 equivariance，Spatial Transformer Networks 和 Group Equivariant CNNs，发现这些特殊化方法并没有提高准确率。在数据增强网络中，我们还分析了哪些层 помо助网络编码旋转不变性，这是重要的，因为它可以理解其限制和如何最好地重新训练数据增强网络以实现旋转不变性。
</details></li>
</ul>
<hr>
<h2 id="Visual-Attention-Prompted-Prediction-and-Learning"><a href="#Visual-Attention-Prompted-Prediction-and-Learning" class="headerlink" title="Visual Attention-Prompted Prediction and Learning"></a>Visual Attention-Prompted Prediction and Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08420">http://arxiv.org/abs/2310.08420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Zhang, Siyi Gu, Bo Pan, Guangji Bai, Xiaofeng Yang, Liang Zhao</li>
<li>for: 提高模型预测力，解决注意力引导学习中的时间和计算成本问题</li>
<li>methods: 提出了注意力提前预测技术，不需要模型重新训练，并解决了视觉注意力提示的不完整信息问题</li>
<li>results: 通过实验表明，提出的框架可以在两个 dataset 上增强预测结果，并且可以在注意力提示和无注意力提示的情况下进行预测，提高了模型的预测能力<details>
<summary>Abstract</summary>
Explanation(attention)-guided learning is a method that enhances a model's predictive power by incorporating human understanding during the training phase. While attention-guided learning has shown promising results, it often involves time-consuming and computationally expensive model retraining. To address this issue, we introduce the attention-prompted prediction technique, which enables direct prediction guided by the attention prompt without the need for model retraining. However, this approach presents several challenges, including: 1) How to incorporate the visual attention prompt into the model's decision-making process and leverage it for future predictions even in the absence of a prompt? and 2) How to handle the incomplete information from the visual attention prompt? To tackle these challenges, we propose a novel framework called Visual Attention-Prompted Prediction and Learning, which seamlessly integrates visual attention prompts into the model's decision-making process and adapts to images both with and without attention prompts for prediction. To address the incomplete information of the visual attention prompt, we introduce a perturbation-based attention map modification method. Additionally, we propose an optimization-based mask aggregation method with a new weight learning function for adaptive perturbed annotation aggregation in the attention map modification process. Our overall framework is designed to learn in an attention-prompt guided multi-task manner to enhance future predictions even for samples without attention prompts and trained in an alternating manner for better convergence. Extensive experiments conducted on two datasets demonstrate the effectiveness of our proposed framework in enhancing predictions for samples, both with and without provided prompts.
</details>
<details>
<summary>摘要</summary>
针对解释（注意）导学习方法，我们提出了一种新的框架，即视觉注意点引导预测和学习框架（Visual Attention-Prompted Prediction and Learning，简称VAPPL）。这种框架可以让模型在训练过程中通过注意点引导来增强预测力，而无需进行复杂和计算昂贵的模型重新训练。然而，这种方法存在一些挑战，包括如何在模型决策过程中 incorporate 视觉注意点，以及如何处理视觉注意点中的不完整信息。为解决这些挑战，我们提出了以下两点方法：1. 将视觉注意点integrated 到模型决策过程中，并在未提供注意点时进行预测。2. 使用杂化基于注意点的修正方法，以处理视觉注意点中的不完整信息。我们的框架包括以下几个组成部分：1. 注意点引导预测：使用提供的注意点来直接预测图像中的特征。2. 注意点修正：使用杂化基于注意点的修正方法，以处理视觉注意点中的不完整信息。3. 杂化基于注意点的mask aggregation：使用一种新的杂化基于注意点的mask aggregation方法，以便更好地处理不完整的注意点信息。4. 优化基于注意点的weight learning：使用一种新的优化基于注意点的weight learning方法，以便更好地适应不同的注意点信息。我们的框架采用了一种带有注意点的多任务学习方法，通过在不同任务之间进行交互学习，以提高未提供注意点时的预测性能。此外，我们还采用了一种分段的训练策略，以便更好地适应不同的注意点信息。我们对两个数据集进行了广泛的实验，并证明了我们的提出的框架可以提高未提供注意点时的预测性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Design-and-Development-of-an-ArUco-Markers-Based-Quantitative-Surface-Tactile-Sensor"><a href="#Towards-Design-and-Development-of-an-ArUco-Markers-Based-Quantitative-Surface-Tactile-Sensor" class="headerlink" title="Towards Design and Development of an ArUco Markers-Based Quantitative Surface Tactile Sensor"></a>Towards Design and Development of an ArUco Markers-Based Quantitative Surface Tactile Sensor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08398">http://arxiv.org/abs/2310.08398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ozdemir Can Kara, Charles Everson, Farshid Alambeigi</li>
<li>for: 这项研究的目标是量化视觉基于感知器的图像输出。</li>
<li>methods: 该研究提出了一种新的量化表面感知器（QS-TS），使得机器人抓取机械臂安全和自主地操作细腻物品。QS-TS通过在实时中测量感知器的gel层变形来实现这一目标。</li>
<li>results: 实验结果表明，QS-TS可以准确地测量感知器的gel层变形，相对误差低于5%。<details>
<summary>Abstract</summary>
In this paper, with the goal of quantifying the qualitative image outputs of a Vision-based Tactile Sensor (VTS), we present the design, fabrication, and characterization of a novel Quantitative Surface Tactile Sensor (called QS-TS). QS-TS directly estimates the sensor's gel layer deformation in real-time enabling safe and autonomous tactile manipulation and servoing of delicate objects using robotic manipulators. The core of the proposed sensor is the utilization of miniature 1.5 mm x 1.5 mm synthetic square markers with inner binary patterns and a broad black border, called ArUco Markers. Each ArUco marker can provide real-time camera pose estimation that, in our design, is used as a quantitative measure for obtaining deformation of the QS-TS gel layer. Moreover, thanks to the use of ArUco markers, we propose a unique fabrication procedure that mitigates various challenges associated with the fabrication of the existing marker-based VTSs and offers an intuitive and less-arduous method for the construction of the VTS. Remarkably, the proposed fabrication facilitates the integration and adherence of markers with the gel layer to robustly and reliably obtain a quantitative measure of deformation in real-time regardless of the orientation of ArUco Markers. The performance and efficacy of the proposed QS-TS in estimating the deformation of the sensor's gel layer were experimentally evaluated and verified. Results demonstrate the phenomenal performance of the QS-TS in estimating the deformation of the gel layer with a relative error of <5%.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们目标是量化视觉基于感测器（VTS）的 качеitative图像输出。我们介绍了一种新的量化表面感测器（QS-TS）的设计、制造和性能Characterization。QS-TS直接测量感测器的gel层塑料的变形，并在实时中提供了安全和自主的柔软物体把握和控制。核心思想是利用微型1.5毫米 x 1.5毫米的合成方块 marker，内部具有内 binar pattern和宽黑边框，称为 ArUco 标记。每个 ArUco 标记可以提供实时相机pose estimation，在我们的设计中用作量化测量gel层的变形。此外，我们提出了一种独特的制造过程，解决了现有 marker-based VTS 的制造问题，并提供了一种直观和不困难的方法 для VTS 的建造。另外，我们的制造过程可以强制性地和可靠地在不同的 ArUco 标记orientation下获取量化测量gel层的变形。我们对提出的 QS-TS 的性能和可靠性进行了实验性评估和验证。结果表明，QS-TS 可以准确地测量gel层的变形，Relative error <5%。
</details></li>
</ul>
<hr>
<h2 id="Hyp-UML-Hyperbolic-Image-Retrieval-with-Uncertainty-aware-Metric-Learning"><a href="#Hyp-UML-Hyperbolic-Image-Retrieval-with-Uncertainty-aware-Metric-Learning" class="headerlink" title="Hyp-UML: Hyperbolic Image Retrieval with Uncertainty-aware Metric Learning"></a>Hyp-UML: Hyperbolic Image Retrieval with Uncertainty-aware Metric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08390">http://arxiv.org/abs/2310.08390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyang Yan, Zongxuan Liu, Lin Xu</li>
<li>for: 这篇论文主要应用于图像搜寻和分类，并且是一种代表学习的关键算法，例如特征学习和它们在度量空间的对齐。</li>
<li>methods: 本论文提出了一种基于希腊圆形空间的图像嵌入，并且还包括两种不同的不确定度测量方法，一种是基于对比学习，另一种是基于margin-based度量学习。</li>
<li>results: 实验验证确认了提出的方法可以实现相关方法中的最佳结果，并且进行了广泛的ablation研究，验证每个方法的有效性。<details>
<summary>Abstract</summary>
Metric learning plays a critical role in training image retrieval and classification. It is also a key algorithm in representation learning, e.g., for feature learning and its alignment in metric space. Hyperbolic embedding has been recently developed. Compared to the conventional Euclidean embedding in most of the previously developed models, Hyperbolic embedding can be more effective in representing the hierarchical data structure. Second, uncertainty estimation/measurement is a long-lasting challenge in artificial intelligence. Successful uncertainty estimation can improve a machine learning model's performance, robustness, and security. In Hyperbolic space, uncertainty measurement is at least with equivalent, if not more, critical importance. In this paper, we develop a Hyperbolic image embedding with uncertainty-aware metric learning for image retrieval. We call our method Hyp-UML: Hyperbolic Uncertainty-aware Metric Learning. Our contribution are threefold: we propose an image embedding algorithm based on Hyperbolic space, with their corresponding uncertainty value; we propose two types of uncertainty-aware metric learning, for the popular Contrastive learning and conventional margin-based metric learning, respectively. We perform extensive experimental validations to prove that the proposed algorithm can achieve state-of-the-art results among related methods. The comprehensive ablation study validates the effectiveness of each component of the proposed algorithm.
</details>
<details>
<summary>摘要</summary>
metric 学习在图像检索和分类训练中扮演了关键角色，它还是表示学习中的关键算法，例如特征学习和其在度量空间的对齐。 reciently， Hyperbolic embedding 已经被开发出来。相比传统的欧几何 embedding ，Hyperbolic embedding 可以更好地表示层次结构的数据。第二，人工智能中的不确定性估计是一个长期的挑战。成功的不确定性估计可以提高机器学习模型的性能、Robustness 和安全性。在 Hyperbolic 空间中，不确定性测量是至少与欧几何空间相当重要，可能更重要。在这篇论文中，我们开发了一种基于 Hyperbolic 空间的图像嵌入，并与不确定性值相对。我们称之为 Hyp-UML：Hyperbolic Uncertainty-aware Metric Learning。我们的贡献有三个方面：1. 我们提出了基于 Hyperbolic 空间的图像嵌入算法，并附带不确定性值。2. 我们提出了两种不确定性意识度量学习方法，一种是基于对比学习，另一种是基于折衔学习。3. 我们进行了广泛的实验验证，证明我们的方法可以在相关的方法中 achieve 状态的较好Result。另外，我们进行了全面的减少学习来验证每个方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="MeanAP-Guided-Reinforced-Active-Learning-for-Object-Detection"><a href="#MeanAP-Guided-Reinforced-Active-Learning-for-Object-Detection" class="headerlink" title="MeanAP-Guided Reinforced Active Learning for Object Detection"></a>MeanAP-Guided Reinforced Active Learning for Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08387">http://arxiv.org/abs/2310.08387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhixuan Liang, Xingyu Zeng, Rui Zhao, Ping Luo</li>
<li>for: 本研究旨在提高对象检测模型的训练效果，使用最少的标注数据，通过选择最有用的示例进行标注并将其包含到任务学习器中。</li>
<li>methods: 本研究使用了 MeanAP  metric来作为查找数据的信息吸引度，并采用了一种基于 reinforcement learning 的抽象代理来选择后续训练示例。</li>
<li>results: 实验结果表明，MAGRAL 在 PASCAL VOC 和 MS COCO 上比最新的状态艺术方法表现出色，显示了substantial的性能提升。MAGRAL 为激活学习对象检测提供了一个坚实的基线，这表明它在这个领域可能会取得进一步的进步。<details>
<summary>Abstract</summary>
Active learning presents a promising avenue for training high-performance models with minimal labeled data, achieved by judiciously selecting the most informative instances to label and incorporating them into the task learner. Despite notable advancements in active learning for image recognition, metrics devised or learned to gauge the information gain of data, crucial for query strategy design, do not consistently align with task model performance metrics, such as Mean Average Precision (MeanAP) in object detection tasks. This paper introduces MeanAP-Guided Reinforced Active Learning for Object Detection (MAGRAL), a novel approach that directly utilizes the MeanAP metric of the task model to devise a sampling strategy employing a reinforcement learning-based sampling agent. Built upon LSTM architecture, the agent efficiently explores and selects subsequent training instances, and optimizes the process through policy gradient with MeanAP serving as reward. Recognizing the time-intensive nature of MeanAP computation at each step, we propose fast look-up tables to expedite agent training. We assess MAGRAL's efficacy across popular benchmarks, PASCAL VOC and MS COCO, utilizing different backbone architectures. Empirical findings substantiate MAGRAL's superiority over recent state-of-the-art methods, showcasing substantial performance gains. MAGRAL establishes a robust baseline for reinforced active object detection, signifying its potential in advancing the field.
</details>
<details>
<summary>摘要</summary>
active learning可能是训练高性能模型的有望途径，通过选择最有信息的实例进行标注并将其添加到任务学习器中，以实现最小的标注数据量。然而，关键指标选择和任务模型性能指标之间存在一定的差异，这些指标通常是图像识别任务中的 Mean Average Precision（MeanAP）。这篇论文介绍了 MeanAP-Guided Reinforced Active Learning for Object Detection（MAGRAL），一种新的方法，它直接使用任务模型的 MeanAP 指标来设计查询策略，并使用长短期记忆（LSTM）架构建立一个强化学习 Agent。通过策略梯度下降，Agent 可以快速探索和选择后续训练实例，并且可以通过 MeanAP 作为奖励来优化过程。由于 MeanAP 的计算在每步都是时间开销的，我们提出了快速查找表来加速 Agent 的训练。我们在 PASCAL VOC 和 MS COCO 等 популяр的 benchmark 上进行了实验，并使用不同的底层架构。实验结果证明 MAGRAL 在最新的方法中表现出色，显示了大幅性能提升。MAGRAL 建立了一个强大的底线 для强化活动对象检测，这表明它在该领域的发展潜力很大。
</details></li>
</ul>
<hr>
<h2 id="AutoVP-An-Automated-Visual-Prompting-Framework-and-Benchmark"><a href="#AutoVP-An-Automated-Visual-Prompting-Framework-and-Benchmark" class="headerlink" title="AutoVP: An Automated Visual Prompting Framework and Benchmark"></a>AutoVP: An Automated Visual Prompting Framework and Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08381">http://arxiv.org/abs/2310.08381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IBM/AutoVP">https://github.com/IBM/AutoVP</a></li>
<li>paper_authors: Hsi-Ai Tsao, Lei Hsiung, Pin-Yu Chen, Sijia Liu, Tsung-Yi Ho<br>for:这篇论文的目的是提出一个叫做AutoVP的扩展性框架，用于自动化Visual Prompting（VP）设计选择，以及提供12个下游图像分类任务，用于全面评估VP性能。methods:论文使用了一个名为AutoVP的框架，包括了三个设计空间：1）对于Prompt的共同优化; 2）适用于预训练模型的选择，包括图像分类器和文本图像Encoder; 3）模型输出映射策略，包括非 Parametric 和可训练的标签映射。results:实验结果显示，AutoVP比现有最佳VP方法有着重大的提升，具体而言，可以提高精度的最大提升为27.5%，并且在12个下游图像分类任务中实现了6.7%的提升。<details>
<summary>Abstract</summary>
Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach to adapting pre-trained vision models to solve various downstream image-classification tasks. However, there has hitherto been little systematic study of the design space of VP and no clear benchmark for evaluating its performance. To bridge this gap, we propose AutoVP, an end-to-end expandable framework for automating VP design choices, along with 12 downstream image-classification tasks that can serve as a holistic VP-performance benchmark. Our design space covers 1) the joint optimization of the prompts; 2) the selection of pre-trained models, including image classifiers and text-image encoders; and 3) model output mapping strategies, including nonparametric and trainable label mapping. Our extensive experimental results show that AutoVP outperforms the best-known current VP methods by a substantial margin, having up to 6.7% improvement in accuracy; and attains a maximum performance increase of 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold contribution: serving both as an efficient tool for hyperparameter tuning on VP design choices, and as a comprehensive benchmark that can reasonably be expected to accelerate VP's development. The source code is available at https://github.com/IBM/AutoVP.
</details>
<details>
<summary>摘要</summary>
“幻像提示（VP）是一种emerging的参数高效调整方法，用于适应预训练的视觉模型解决各种下游图像分类任务。然而，有很少的系统性研究VP的设计空间，也没有明确的性能标准。为bridge这个差距，我们提议AutoVP，一个可扩展的框架，用于自动化VP设计选择，以及12个下游图像分类任务，可以作为VP性能标准。我们的设计空间包括：1）提示的共同优化; 2）采用预训练模型，包括图像分类器和文本图像编码器; 3）模型输出映射策略，包括非 Parametric 和可训练标签映射。我们的广泛实验结果表明，AutoVP比现有最佳VP方法有substantial的提升，具有最高27.5%的性能提升比基准线性探测（LP）方法。AutoVP因此作出了两重贡献：作为一个高效的 hyperparameter 调整工具，以及一个全面的标准，可以加速VP的发展。代码可以在https://github.com/IBM/AutoVP 中获取。”
</details></li>
</ul>
<hr>
<h2 id="Worst-Case-Morphs-using-Wasserstein-ALI-and-Improved-MIPGAN"><a href="#Worst-Case-Morphs-using-Wasserstein-ALI-and-Improved-MIPGAN" class="headerlink" title="Worst-Case Morphs using Wasserstein ALI and Improved MIPGAN"></a>Worst-Case Morphs using Wasserstein ALI and Improved MIPGAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08371">http://arxiv.org/abs/2310.08371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Una M. Kelly, Meike Nauta, Lu Liu, Luuk J. Spreeuwers, Raymond N. J. Veldhuis</li>
<li>for: 这 paper 的目的是提出一种能够生成 worst-case 模糊图像的方法，以挑战 face recognition 系统（FR）的安全性。</li>
<li>methods: 这 paper 使用了 Adversarially Learned Inference（ALI）和 Wasserstein GANs  trains with Gradient Penalty（WGAN-GP）等方法来生成模糊图像，并通过特定的损失函数来提高模糊图像中的人脸信息 manipulate 的能力。</li>
<li>results: 这 paper 的结果表明，使用 WALI 方法可以生成更加挑战 FR 系统的模糊图像，并且可以提高 MIPGAN 等现有的 StyleGAN-based morph generator 的性能。<details>
<summary>Abstract</summary>
A morph is a combination of two separate facial images and contains identity information of two different people. When used in an identity document, both people can be authenticated by a biometric Face Recognition (FR) system. Morphs can be generated using either a landmark-based approach or approaches based on deep learning such as Generative Adversarial Networks (GAN). In a recent paper, we introduced a \emph{worst-case} upper bound on how challenging morphing attacks can be for an FR system. The closer morphs are to this upper bound, the bigger the challenge they pose to FR. We introduced an approach with which it was possible to generate morphs that approximate this upper bound for a known FR system (white box), but not for unknown (black box) FR systems.   In this paper, we introduce a morph generation method that can approximate worst-case morphs even when the FR system is not known. A key contribution is that we include the goal of generating difficult morphs \emph{during} training. Our method is based on Adversarially Learned Inference (ALI) and uses concepts from Wasserstein GANs trained with Gradient Penalty, which were introduced to stabilise the training of GANs. We include these concepts to achieve similar improvement in training stability and call the resulting method Wasserstein ALI (WALI). We finetune WALI using loss functions designed specifically to improve the ability to manipulate identity information in facial images and show how it can generate morphs that are more challenging for FR systems than landmark- or GAN-based morphs. We also show how our findings can be used to improve MIPGAN, an existing StyleGAN-based morph generator.
</details>
<details>
<summary>摘要</summary>
文本：A morph is a combination of two separate facial images and contains identity information of two different people. When used in an identity document, both people can be authenticated by a biometric Face Recognition (FR) system. Morphs can be generated using either a landmark-based approach or approaches based on deep learning such as Generative Adversarial Networks (GAN). In a recent paper, we introduced a worst-case upper bound on how challenging morphing attacks can be for an FR system. The closer morphs are to this upper bound, the bigger the challenge they pose to FR. We introduced an approach with which it was possible to generate morphs that approximate this upper bound for a known FR system (white box), but not for unknown (black box) FR systems. In this paper, we introduce a morph generation method that can approximate worst-case morphs even when the FR system is not known. A key contribution is that we include the goal of generating difficult morphs during training. Our method is based on Adversarially Learned Inference (ALI) and uses concepts from Wasserstein GANs trained with Gradient Penalty, which were introduced to stabilize the training of GANs. We include these concepts to achieve similar improvement in training stability and call the resulting method Wasserstein ALI (WALI). We finetune WALI using loss functions designed specifically to improve the ability to manipulate identity information in facial images and show how it can generate morphs that are more challenging for FR systems than landmark- or GAN-based morphs. We also show how our findings can be used to improve MIPGAN, an existing StyleGAN-based morph generator.翻译：一个 morph 是两个不同人的面部图像的组合，它包含这两个人的身份信息。在身份文件中使用时，这两个人可以通过面部识别系统进行验证。 morphs 可以使用 landmark-based 方法或深度学习方法如生成敌对学习网络 (GAN) 来生成。在一篇最近的论文中，我们引入了一个 worst-case 上限，用于描述 morphing 攻击的复杂程度。这个上限更近的 morphs 对于 Face Recognition (FR) 系统来说更加具有挑战性。我们引入了一种可以在知道 FR 系统 (白盒) 上生成 Approximate worst-case morphs 的方法，但不能在不知道 FR 系统 (黑盒) 上生成。在这篇论文中，我们介绍了一种可以在不知道 FR 系统上生成 worst-case morphs 的方法。这个方法基于 Adversarially Learned Inference (ALI)，并使用 Wasserstein GANs  trained with Gradient Penalty 的概念，这些概念可以帮助稳定 GANs 的训练。我们在这些概念基础上进行了类似的改进，并将其称为 Wasserstein ALI (WALI)。我们使用特定设计来提高 facial image 中的身份信息 manipulate 能力的损失函数，并通过这些损失函数来训练 WALI。我们还显示了我们的发现可以用来改进 MIPGAN，一个基于 StyleGAN 的 morph generator。
</details></li>
</ul>
<hr>
<h2 id="UniPAD-A-Universal-Pre-training-Paradigm-for-Autonomous-Driving"><a href="#UniPAD-A-Universal-Pre-training-Paradigm-for-Autonomous-Driving" class="headerlink" title="UniPAD: A Universal Pre-training Paradigm for Autonomous Driving"></a>UniPAD: A Universal Pre-training Paradigm for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08370">http://arxiv.org/abs/2310.08370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Nightmare-n/UniPAD">https://github.com/Nightmare-n/UniPAD</a></li>
<li>paper_authors: Honghui Yang, Sha Zhang, Di Huang, Xiaoyang Wu, Haoyi Zhu, Tong He, Shixiang Tang, Hengshuang Zhao, Qibo Qiu, Binbin Lin, Xiaofei He, Wanli Ouyang</li>
<li>for: This paper is written for the purpose of proposing a novel self-supervised learning paradigm called UniPAD, which is designed to improve the effectiveness of feature learning for autonomous driving.</li>
<li>methods: The paper uses a 3D volumetric differentiable rendering technique to implicitly encode 3D space and facilitate the reconstruction of continuous 3D shape structures and intricate appearance characteristics of their 2D projections.</li>
<li>results: The paper demonstrates the feasibility and effectiveness of UniPAD through extensive experiments on various downstream 3D tasks, achieving significant improvements over lidar-, camera-, and lidar-camera-based baselines, and achieving state-of-the-art results in 3D object detection and 3D semantic segmentation on the nuScenes validation set.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文是为了介绍一种新的自我超vised学习方法UniPAD，该方法是用于提高自驾护护的特征学习效果。</li>
<li>methods: 该论文使用了3D可微分渲染技术，以隐式地编码3D空间，并且促进了连续3D形状结构和2D投影中的细腻特征的重建。</li>
<li>results: 论文通过对多个下游3D任务进行广泛的实验，证明了UniPAD的可行性和效果，并在nuScenes验证集上 achieved state-of-the-art  Results in 3D物体检测和3D semantics排序。<details>
<summary>Abstract</summary>
In the context of autonomous driving, the significance of effective feature learning is widely acknowledged. While conventional 3D self-supervised pre-training methods have shown widespread success, most methods follow the ideas originally designed for 2D images. In this paper, we present UniPAD, a novel self-supervised learning paradigm applying 3D volumetric differentiable rendering. UniPAD implicitly encodes 3D space, facilitating the reconstruction of continuous 3D shape structures and the intricate appearance characteristics of their 2D projections. The flexibility of our method enables seamless integration into both 2D and 3D frameworks, enabling a more holistic comprehension of the scenes. We manifest the feasibility and effectiveness of UniPAD by conducting extensive experiments on various downstream 3D tasks. Our method significantly improves lidar-, camera-, and lidar-camera-based baseline by 9.1, 7.7, and 6.9 NDS, respectively. Notably, our pre-training pipeline achieves 73.2 NDS for 3D object detection and 79.4 mIoU for 3D semantic segmentation on the nuScenes validation set, achieving state-of-the-art results in comparison with previous methods. The code will be available at https://github.com/Nightmare-n/UniPAD.
</details>
<details>
<summary>摘要</summary>
在自动驾驶中，有效特征学习的重要性广泛得到了认可。传统的3D自我超vised预训练方法已经在各种应用中得到了广泛的成功，但大多数方法都是基于2D图像的想法。在这篇论文中，我们提出了UniPAD，一种新的自我超vised学习方法，通过3D分割可 differentiable rendering来隐式地编码3D空间，使得可以重建连续的3D形状结构和其2D投影图像的细节特征。我们的方法具有灵活性，可以轻松地与2D和3D框架集成，从而更好地理解场景。我们通过对多种下游3D任务进行广泛的实验证明了UniPAD的可行性和效果。与基eline相比，我们的预训练管道可以提高lidar-, camera-和lidar-camera-based基eline的NDS分数，分别提高9.1、7.7和6.9个NDS。特别是，我们的预训练管道在3D物体检测和3Dsemantic segmentation任务上实现了73.2个NDS和79.4个mIoU的最佳成绩，与前一代方法相比，达到了状态的艺术水平。代码将在https://github.com/Nightmare-n/UniPAD中提供。
</details></li>
</ul>
<hr>
<h2 id="Mapping-Memes-to-Words-for-Multimodal-Hateful-Meme-Classification"><a href="#Mapping-Memes-to-Words-for-Multimodal-Hateful-Meme-Classification" class="headerlink" title="Mapping Memes to Words for Multimodal Hateful Meme Classification"></a>Mapping Memes to Words for Multimodal Hateful Meme Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08368">http://arxiv.org/abs/2310.08368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miccunifi/issues">https://github.com/miccunifi/issues</a></li>
<li>paper_authors: Giovanni Burbi, Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, Alberto Del Bimbo</li>
<li>for: 本研究旨在探讨Multimodal图文投稿中的仇恨内容检测，以提高网络上的仇恨内容识别和防控。</li>
<li>methods: 本研究提出了一种名为ISSUES的新方法，利用预训练的CLIP视觉语言模型和文本倒转技术，有效地捕捉 Multimodal图文投稿的semantic内容。</li>
<li>results: 实验表明，ISSUES方法在Hateful Memes Challenge和HarMeme数据集上达到了状态之前的最佳结果。代码和预训练模型公开在<a target="_blank" rel="noopener" href="https://github.com/miccunifi/ISSUES%E3%80%82">https://github.com/miccunifi/ISSUES。</a><details>
<summary>Abstract</summary>
Multimodal image-text memes are prevalent on the internet, serving as a unique form of communication that combines visual and textual elements to convey humor, ideas, or emotions. However, some memes take a malicious turn, promoting hateful content and perpetuating discrimination. Detecting hateful memes within this multimodal context is a challenging task that requires understanding the intertwined meaning of text and images. In this work, we address this issue by proposing a novel approach named ISSUES for multimodal hateful meme classification. ISSUES leverages a pre-trained CLIP vision-language model and the textual inversion technique to effectively capture the multimodal semantic content of the memes. The experiments show that our method achieves state-of-the-art results on the Hateful Memes Challenge and HarMeme datasets. The code and the pre-trained models are publicly available at https://github.com/miccunifi/ISSUES.
</details>
<details>
<summary>摘要</summary>
多模态图文投稿在互联网上广泛存在，作为一种混合视觉和文本元素的特殊形式的沟通，用于传达幽默、想法或情感。然而，一些投稿会发展为恶意的，推广仇恨内容并推动歧视。在这种多模态上下文中探测恶意投稿是一项复杂的任务，需要理解图文中的含义相互作用。在这种情况下，我们提出了一种名为ISSUES的新方法，用于多模态恶意投稿分类。ISSUES利用预训练的CLIP视觉语言模型和文本倒转技术，有效地捕捉投稿的多模态含义。实验结果表明，我们的方法在Hateful Memes Challenge和HarMeme数据集上达到了状态码的最佳结果。代码和预训练模型可以在https://github.com/miccunifi/ISSUES上下载。
</details></li>
</ul>
<hr>
<h2 id="A-Generic-Software-Framework-for-Distributed-Topological-Analysis-Pipelines"><a href="#A-Generic-Software-Framework-for-Distributed-Topological-Analysis-Pipelines" class="headerlink" title="A Generic Software Framework for Distributed Topological Analysis Pipelines"></a>A Generic Software Framework for Distributed Topological Analysis Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08339">http://arxiv.org/abs/2310.08339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eve Le Guillou, Michael Will, Pierre Guillou, Jonas Lukasczyk, Pierre Fortin, Christoph Garth, Julien Tierny</li>
<li>for: 本文提出了一个软件框架，用于支持分布式内存中的拓扑分析管道。相比之下，一些最近的论文已经在分布式内存环境中实现了基于拓扑的方法，但是这些方法都是专门为单一算法而实现的。本文则描述了一个通用的、Generic框架，可以支持多种拓扑算法的交互，可能在不同的进程上运行。</li>
<li>methods: 我们在本文中使用了MPI模型，并在Topology ToolKit（TTK）中实现了这个框架。在开发这个框架时，我们遇到了许多算法和软件工程困难，并在文中 документирова了这些困难。我们还提供了分布式内存中的拓扑算法的分类，根据它们的通信需求，以及一些Hybrid MPI+线程并行的示例。</li>
<li>results: 我们对这个框架的性能进行了详细的分析，发现并行效率可以在20%到80%之间，具体取决于算法。此外，我们在我们的框架中引入的MPI特定的预处理对计算时间 overhead是可以忽略的。 finally，我们使用了TTK在一个大规模的数据集上进行了一个高级的分析管道示例，演示了这个框架的新的分布式内存能力。<details>
<summary>Abstract</summary>
This system paper presents a software framework for the support of topological analysis pipelines in a distributed-memory model. While several recent papers introduced topology-based approaches for distributed-memory environments, these were reporting experiments obtained with tailored, mono-algorithm implementations. In contrast, we describe in this paper a general-purpose, generic framework for topological analysis pipelines, i.e. a sequence of topological algorithms interacting together, possibly on distinct numbers of processes. Specifically, we instantiated our framework with the MPI model, within the Topology ToolKit (TTK). While developing this framework, we faced several algorithmic and software engineering challenges, which we document in this paper. We provide a taxonomy for the distributed-memory topological algorithms supported by TTK, depending on their communication needs and provide examples of hybrid MPI+thread parallelizations. Detailed performance analyses show that parallel efficiencies range from $20\%$ to $80\%$ (depending on the algorithms), and that the MPI-specific preconditioning introduced by our framework induces a negligible computation time overhead. We illustrate the new distributed-memory capabilities of TTK with an example of advanced analysis pipeline, combining multiple algorithms, run on the largest publicly available dataset we have found (120 billion vertices) on a standard cluster with 64 nodes (for a total of 1,536 cores). Finally, we provide a roadmap for the completion of TTK's MPI extension, along with generic recommendations for each algorithm communication category.
</details>
<details>
<summary>摘要</summary>
We provide a taxonomy for the distributed-memory topological algorithms supported by TTK, depending on their communication needs, and examples of hybrid MPI+thread parallelizations. Our performance analyses show that parallel efficiencies range from 20% to 80% (depending on the algorithms), and that the MPI-specific preconditioning introduced by our framework has negligible computation time overhead.We illustrate the new distributed-memory capabilities of TTK with an example of an advanced analysis pipeline combining multiple algorithms on the largest publicly available dataset (120 billion vertices) on a standard cluster with 64 nodes (for a total of 1,536 cores). Finally, we provide a roadmap for the completion of TTK's MPI extension, along with generic recommendations for each algorithm communication category.
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Neural-BRDF-with-Spherically-Distributed-Primitives"><a href="#Real-Time-Neural-BRDF-with-Spherically-Distributed-Primitives" class="headerlink" title="Real-Time Neural BRDF with Spherically Distributed Primitives"></a>Real-Time Neural BRDF with Spherically Distributed Primitives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08332">http://arxiv.org/abs/2310.08332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yishun Dou, Zhong Zheng, Qiaoqiao Jin, Bingbing Ni, Yugang Chen, Junxiang Ke</li>
<li>for: 提供一种高效简洁的神经网络 BRDF，用于实现实时渲染。</li>
<li>methods: 提议使用两个低维度的方向特征网格（一个是入射方向网格，另一个是出射方向网格），以及一个小型的神经网络来学习反射特征。</li>
<li>results: 实验结果表明，提议的方法可以在高解度下实现实时渲染，并且可以模型各种材料的各种表现。<details>
<summary>Abstract</summary>
We propose a novel compact and efficient neural BRDF offering highly versatile material representation, yet with very-light memory and neural computation consumption towards achieving real-time rendering. The results in Figure 1, rendered at full HD resolution on a current desktop machine, show that our system achieves real-time rendering with a wide variety of appearances, which is approached by the following two designs. On the one hand, noting that bidirectional reflectance is distributed in a very sparse high-dimensional subspace, we propose to project the BRDF into two low-dimensional components, i.e., two hemisphere feature-grids for incoming and outgoing directions, respectively. On the other hand, learnable neural reflectance primitives are distributed on our highly-tailored spherical surface grid, which offer informative features for each component and alleviate the conventional heavy feature learning network to a much smaller one, leading to very fast evaluation. These primitives are centrally stored in a codebook and can be shared across multiple grids and even across materials, based on the low-cost indices stored in material-specific spherical surface grids. Our neural BRDF, which is agnostic to the material, provides a unified framework that can represent a variety of materials in consistent manner. Comprehensive experimental results on measured BRDF compression, Monte Carlo simulated BRDF acceleration, and extension to spatially varying effect demonstrate the superior quality and generalizability achieved by the proposed scheme.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的紧凑型高效神经BRDF，可以高效地表示各种材料的各种表现，且具有很低的内存和神经计算占用率，以实现实时渲染。图1所示的结果，在全高清解算器上的当前桌面机器上进行渲染，显示了我们的系统可以实现实时渲染，并且可以表示各种不同的外观。在一种方法上，我们注意到了反射率在高维度下的极其稀畴分布，我们将BRDF投影到了两个低维度组件中，即进行和出行方向的两个半球特征网格。另一方面，我们使用学习神经反射元素，分布在我们特制的球面网格上，这些元素提供了每个组件中的有用特征，从而使得传统的重量级特征学习网络可以减少到非常小，从而实现非常快的评估。这些元素被中心存储在一个编码表中，可以在多个网格和材料之间共享，基于材料特有的球面网格中的低成本索引。我们的神经BRDF是材料无关的，它提供了一种统一的框架，可以一致地表示各种材料。我们的实验结果表明，我们的方法可以高效地压缩BRDF，使用MCV simulated BRDF加速，并在空间变化的效果上进行扩展。
</details></li>
</ul>
<hr>
<h2 id="NSM4D-Neural-Scene-Model-Based-Online-4D-Point-Cloud-Sequence-Understanding"><a href="#NSM4D-Neural-Scene-Model-Based-Online-4D-Point-Cloud-Sequence-Understanding" class="headerlink" title="NSM4D: Neural Scene Model Based Online 4D Point Cloud Sequence Understanding"></a>NSM4D: Neural Scene Model Based Online 4D Point Cloud Sequence Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08326">http://arxiv.org/abs/2310.08326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Dong, Zhuoyang Zhang, Yunze Liu, Li Yi</li>
<li>for: 本研究旨在提高现有4D背bone的在线感知能力，包括VR&#x2F;AR、机器人和自动驾驶等场景。</li>
<li>methods: 我们提出了一种名为NSM4D的通用在线4D感知方法，可以与现有的4D背bone结合使用，以提高其在线感知能力。NSM4D使用神经场景模型来分解空间和运动信息，并通过token表示来提高鲁棒性和可缩放性。</li>
<li>results: 我们在各种在线感知测试 benchmark 上达到了显著的改善，包括HOI4D在线动作 segmentation 的9.6%精度提高和SemanticKITTI在线 semantics segmentation 的3.4% mIoU 提高。此外，NSM4D表现出了优秀的扩展性，可以适应更长的序列。<details>
<summary>Abstract</summary>
Understanding 4D point cloud sequences online is of significant practical value in various scenarios such as VR/AR, robotics, and autonomous driving. The key goal is to continuously analyze the geometry and dynamics of a 3D scene as unstructured and redundant point cloud sequences arrive. And the main challenge is to effectively model the long-term history while keeping computational costs manageable. To tackle these challenges, we introduce a generic online 4D perception paradigm called NSM4D. NSM4D serves as a plug-and-play strategy that can be adapted to existing 4D backbones, significantly enhancing their online perception capabilities for both indoor and outdoor scenarios. To efficiently capture the redundant 4D history, we propose a neural scene model that factorizes geometry and motion information by constructing geometry tokens separately storing geometry and motion features. Exploiting the history becomes as straightforward as querying the neural scene model. As the sequence progresses, the neural scene model dynamically deforms to align with new observations, effectively providing the historical context and updating itself with the new observations. By employing token representation, NSM4D also exhibits robustness to low-level sensor noise and maintains a compact size through a geometric sampling scheme. We integrate NSM4D with state-of-the-art 4D perception backbones, demonstrating significant improvements on various online perception benchmarks in indoor and outdoor settings. Notably, we achieve a 9.6% accuracy improvement for HOI4D online action segmentation and a 3.4% mIoU improvement for SemanticKITTI online semantic segmentation. Furthermore, we show that NSM4D inherently offers excellent scalability to longer sequences beyond the training set, which is crucial for real-world applications.
</details>
<details>
<summary>摘要</summary>
理解4D点云序列在线是实际场景中的重要任务，如VR/AR、 робо太器和自动驾驶。主要挑战是在新观察到的数据流入时，有效地模型长期历史，同时保持计算成本可控。为解决这些挑战，我们介绍了一种通用的在线4D感知方法 called NSM4D。NSM4D是一种插件化策略，可以适应现有4D脊梁，明显提高在线感知能力，包括室内和室外场景。为了有效地捕捉重复的4D历史，我们提议一种神经场景模型，该模型将geometry和动作信息分解为两个分量，并将geometry特征存储在geometry tokens中。利用历史变得如查询神经场景模型。随着序列的扩展，神经场景模型会逐渐对新观察到的数据进行匹配，以提供历史上的 контекст和更新。通过使用Token表示，NSM4D也能够对低级别的感知器骤动具有抗性，并保持紧凑的大小通过地理学取样方式。我们将NSM4D与现有的4D感知脊梁集成，在室内和室外场景中展示了显著改进。特别是，我们实现了HOI4D在线动作分割 tasks中的9.6%精度提高和SemanticKITTI在线semantic segmentation tasks中的3.4%mIoU提高。此外，我们还证明NSM4D自然地具有优秀的扩展性，可以处理更长的序列，这在实际应用中是非常重要的。
</details></li>
</ul>
<hr>
<h2 id="Extended-target-tracking-utilizing-machine-learning-software-–-with-applications-to-animal-classification"><a href="#Extended-target-tracking-utilizing-machine-learning-software-–-with-applications-to-animal-classification" class="headerlink" title="Extended target tracking utilizing machine-learning software – with applications to animal classification"></a>Extended target tracking utilizing machine-learning software – with applications to animal classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08316">http://arxiv.org/abs/2310.08316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Magnus Malmström, Anton Kullberg, Isaac Skog, Daniel Axehill, Fredrik Gustafsson</li>
<li>for: 检测和跟踪图像序列中的对象</li>
<li>methods: 使用对象检测算法输出为检测结果，并利用前一帧的类信息强化分类，以鲁棒化分类结果</li>
<li>results: 在使用camera trap图像进行测试后，实现了更加鲁检的分类结果<details>
<summary>Abstract</summary>
This paper considers the problem of detecting and tracking objects in a sequence of images. The problem is formulated in a filtering framework, using the output of object-detection algorithms as measurements. An extension to the filtering formulation is proposed that incorporates class information from the previous frame to robustify the classification, even if the object-detection algorithm outputs an incorrect prediction. Further, the properties of the object-detection algorithm are exploited to quantify the uncertainty of the bounding box detection in each frame. The complete filtering method is evaluated on camera trap images of the four large Swedish carnivores, bear, lynx, wolf, and wolverine. The experiments show that the class tracking formulation leads to a more robust classification.
</details>
<details>
<summary>摘要</summary>
这篇论文考虑了图像序列中对象检测和跟踪的问题。问题是使用滤波框架来解决，使用对象检测算法的输出作为测量。另外，一种增强的滤波形式是提出，该形式包括上一帧的类信息来强化分类，即使对象检测算法输出错误预测也能够强化分类。此外，利用对象检测算法的性质来评估每帧 bounding box 检测结果的uncertainty。完整的滤波方法在摄像头捕捉的瑞典四大哺乳动物摄像头上进行了评估。实验结果表明，类跟踪形式导致更加稳定的分类。
</details></li>
</ul>
<hr>
<h2 id="GePSAn-Generative-Procedure-Step-Anticipation-in-Cooking-Videos"><a href="#GePSAn-Generative-Procedure-Step-Anticipation-in-Cooking-Videos" class="headerlink" title="GePSAn: Generative Procedure Step Anticipation in Cooking Videos"></a>GePSAn: Generative Procedure Step Anticipation in Cooking Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08312">http://arxiv.org/abs/2310.08312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Ashraf Abdelsalam, Samrudhdhi B. Rangrej, Isma Hadji, Nikita Dvornik, Konstantinos G. Derpanis, Afsaneh Fazly</li>
<li>for: 预测未来步骤在进程视频中</li>
<li>methods: 使用生成模型，通过模型学习多个可能的下一步选择</li>
<li>results: 在 YouCookII 上实现新的状态态-of-the-art 结果，并在没有调整或适应的情况下在视频频道上进行预测。Here’s a breakdown of each point:</li>
<li>for: The paper is focused on the problem of future step anticipation in procedural videos.</li>
<li>methods: The authors use a generative model to predict multiple plausible candidates for the next step in a procedural video. They pretrain the model on a large text-based corpus of procedural activities and then transfer it to the video domain.</li>
<li>results: The authors achieve new state-of-the-art results on the YouCookII dataset, and demonstrate that their model can successfully transfer from text to the video domain without fine-tuning or adaptation.<details>
<summary>Abstract</summary>
We study the problem of future step anticipation in procedural videos. Given a video of an ongoing procedural activity, we predict a plausible next procedure step described in rich natural language. While most previous work focus on the problem of data scarcity in procedural video datasets, another core challenge of future anticipation is how to account for multiple plausible future realizations in natural settings. This problem has been largely overlooked in previous work. To address this challenge, we frame future step prediction as modelling the distribution of all possible candidates for the next step. Specifically, we design a generative model that takes a series of video clips as input, and generates multiple plausible and diverse candidates (in natural language) for the next step. Following previous work, we side-step the video annotation scarcity by pretraining our model on a large text-based corpus of procedural activities, and then transfer the model to the video domain. Our experiments, both in textual and video domains, show that our model captures diversity in the next step prediction and generates multiple plausible future predictions. Moreover, our model establishes new state-of-the-art results on YouCookII, where it outperforms existing baselines on the next step anticipation. Finally, we also show that our model can successfully transfer from text to the video domain zero-shot, ie, without fine-tuning or adaptation, and produces good-quality future step predictions from video.
</details>
<details>
<summary>摘要</summary>
我们研究未来步骤预测在进程视频中的问题。给定一个正在进行的进程活动视频，我们预测下一步的可能性描述在丰富的自然语言中。而前一个工作主要关注的问题是数据缺乏在进程视频数据集上，另一个核心挑战是如何考虑多个可能的未来实现在自然 Setting中。这个问题在前一个工作中得到了广泛忽略。为了解决这个挑战，我们将未来步骤预测定义为模型所有可能候选人的分布。具体来说，我们设计了一种生成模型，接受一系列视频剪辑作为输入，并生成多个可能和多样的候选人（在自然语言中）的下一步。根据之前的工作，我们训练我们的模型在大量的文本基础数据集上，然后将模型转移到视频领域。我们的实验表明，我们的模型能够捕捉多个下一步预测的多样性，并生成多个可能的未来预测。此外，我们的模型在YouCookII上新做出了状态的报表结果，比现有的基elines superior。最后，我们还证明了我们的模型可以成功地在视频领域中转移到零例情况下，即无需调整或适应，并生成良质的未来步骤预测。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Variational-Auto-encoder-based-Audio-Visual-Segmentation"><a href="#Multimodal-Variational-Auto-encoder-based-Audio-Visual-Segmentation" class="headerlink" title="Multimodal Variational Auto-encoder based Audio-Visual Segmentation"></a>Multimodal Variational Auto-encoder based Audio-Visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08303">http://arxiv.org/abs/2310.08303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opennlplab/mmvae-avs">https://github.com/opennlplab/mmvae-avs</a></li>
<li>paper_authors: Yuxin Mao, Jing Zhang, Mochu Xiang, Yiran Zhong, Yuchao Dai</li>
<li>for: 为 audio-visual segmentation (AVS) 任务，提出了Explicit Conditional Multimodal Variational Auto-Encoder (ECMVAE) 模型，用于音频视频序列中的音源分割。</li>
<li>methods: 我们使用了模式特征学习的视角，强调明确地捕捉每个模式的特征。具体来说，我们发现音频中含有音源生产者的关键分类信息，而视频数据则提供了可能的声音生产者。这两种数据的共同信息与视频中显示的声音生产者相对应。因此，跨modal共享表示学习是AVS中非常重要的。为了实现这一目标，我们的ECMVAE模型使用了共享表示和特定表示的因子化。在这种情况下，我们应用了modalities之间的正交性约束，以保持因子化的独特性。此外，我们还引入了广泛探索的强制正则化，以便对每个模式进行详细的探索。</li>
<li>results: 我们在AVSBench上进行了量化和质量评估，并证明了我们的方法的效iveness。相比之前的AVS方法，我们的ECMVAE模型在多个声音源分割任务中达到了新的州OF-THE-ART Waterloo，升级了3.84 mIOU的性能。<details>
<summary>Abstract</summary>
We propose an Explicit Conditional Multimodal Variational Auto-Encoder (ECMVAE) for audio-visual segmentation (AVS), aiming to segment sound sources in the video sequence. Existing AVS methods focus on implicit feature fusion strategies, where models are trained to fit the discrete samples in the dataset. With a limited and less diverse dataset, the resulting performance is usually unsatisfactory. In contrast, we address this problem from an effective representation learning perspective, aiming to model the contribution of each modality explicitly. Specifically, we find that audio contains critical category information of the sound producers, and visual data provides candidate sound producer(s). Their shared information corresponds to the target sound producer(s) shown in the visual data. In this case, cross-modal shared representation learning is especially important for AVS. To achieve this, our ECMVAE factorizes the representations of each modality with a modality-shared representation and a modality-specific representation. An orthogonality constraint is applied between the shared and specific representations to maintain the exclusive attribute of the factorized latent code. Further, a mutual information maximization regularizer is introduced to achieve extensive exploration of each modality. Quantitative and qualitative evaluations on the AVSBench demonstrate the effectiveness of our approach, leading to a new state-of-the-art for AVS, with a 3.84 mIOU performance leap on the challenging MS3 subset for multiple sound source segmentation.
</details>
<details>
<summary>摘要</summary>
我们提出了一种显式条件多模态变分自动编码器（ECMVAE），用于音频视频分割（AVS），目的是在视频序列中分割声音源。现有的AVS方法主要采用隐式特征融合策略，其中模型通常是根据数据集中的精确样本进行训练。由于数据集规模有限，模型的性能通常不满足要求。我们则从表示学习的视角来解决这个问题，即模型需要明确地表示每个modalities的贡献。具体来说，我们发现音频中含有重要的声音生产者类别信息，而视觉数据则提供了声音生产者候选人。他们共享的信息与视频中显示的声音生产者相对应。在这种情况下，跨Modalities的共享表示学习特别重要。为此，我们的ECMVAE使用一个共享表示和一个特定表示来分解每个modalities的表示。我们还应用一个共享和特定表示之间的正交约束，以保持各个modalities的独特性。此外，我们还引入了一个最大化对抗信息 regularizer，以实现每个modalities的广泛探索。量化和质量评估表明，我们的方法有效地解决AVS问题，在AVSBench上达到了新的状态机器，其中MS3子集上多个声音源分割的IOU性能提高3.84米。
</details></li>
</ul>
<hr>
<h2 id="GraphAlign-Enhancing-Accurate-Feature-Alignment-by-Graph-matching-for-Multi-Modal-3D-Object-Detection"><a href="#GraphAlign-Enhancing-Accurate-Feature-Alignment-by-Graph-matching-for-Multi-Modal-3D-Object-Detection" class="headerlink" title="GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for Multi-Modal 3D Object Detection"></a>GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for Multi-Modal 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08261">http://arxiv.org/abs/2310.08261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziying Song, Haiyue Wei, Lin Bai, Lei Yang, Caiyan Jia</li>
<li>for: 3D object detection in autonomous driving</li>
<li>methods: graph matching, feature alignment, projection calibration, self-attention module</li>
<li>results: more accurate feature alignment, improved performance in 3D object detection<details>
<summary>Abstract</summary>
LiDAR and cameras are complementary sensors for 3D object detection in autonomous driving. However, it is challenging to explore the unnatural interaction between point clouds and images, and the critical factor is how to conduct feature alignment of heterogeneous modalities. Currently, many methods achieve feature alignment by projection calibration only, without considering the problem of coordinate conversion accuracy errors between sensors, leading to sub-optimal performance. In this paper, we present GraphAlign, a more accurate feature alignment strategy for 3D object detection by graph matching. Specifically, we fuse image features from a semantic segmentation encoder in the image branch and point cloud features from a 3D Sparse CNN in the LiDAR branch. To save computation, we construct the nearest neighbor relationship by calculating Euclidean distance within the subspaces that are divided into the point cloud features. Through the projection calibration between the image and point cloud, we project the nearest neighbors of point cloud features onto the image features. Then by matching the nearest neighbors with a single point cloud to multiple images, we search for a more appropriate feature alignment. In addition, we provide a self-attention module to enhance the weights of significant relations to fine-tune the feature alignment between heterogeneous modalities. Extensive experiments on nuScenes benchmark demonstrate the effectiveness and efficiency of our GraphAlign.
</details>
<details>
<summary>摘要</summary>
李达朗和摄像头是自动驾驶中3D对象检测的补充传感器。然而，在点云和图像之间的不自然交互问题具有挑战性，而且关键因素是如何进行多模态特征对齐。目前，许多方法通过投影准备 alone，不考虑投影准备精度错误之间传感器的坐标转换问题，导致优化性不佳。在这篇论文中，我们提出了图像对齐策略，通过图像特征和点云特征的图像对齐来提高3D对象检测的精度。具体来说，我们将图像分支中的semantic segmentation编码器输出的图像特征与LiDAR分支中的3D稀畴CNN输出的点云特征进行融合。为了降低计算量，我们将点云特征分解成子空间，并在这些子空间内计算最近邻关系。然后，通过点云特征与图像特征的投影准备，将点云特征的最近邻映射到图像特征上。最后，我们通过将多个点云特征对应到同一张图像上的多个特征进行匹配，以找到更加适合的特征对齐。此外，我们还提供了一个自注意模块，以增强不同模态之间的特征对齐关系的权重，以进一步细调特征对齐。我们在nuScenes标准测试集上进行了广泛的实验，并证明了我们的图像对齐策略的有效性和高效性。
</details></li>
</ul>
<hr>
<h2 id="Invisible-Threats-Backdoor-Attack-in-OCR-Systems"><a href="#Invisible-Threats-Backdoor-Attack-in-OCR-Systems" class="headerlink" title="Invisible Threats: Backdoor Attack in OCR Systems"></a>Invisible Threats: Backdoor Attack in OCR Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08259">http://arxiv.org/abs/2310.08259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mauro Conti, Nicola Farronato, Stefanos Koffas, Luca Pajola, Stjepan Picek</li>
<li>for: 这个论文的目的是描述一种针对 Optical Character Recognition (OCR) 的后门攻击，使得 extracted text 不可读用于自然语言处理应用程序中。</li>
<li>methods: 该论文使用了深度神经网络来实现后门攻击，并通过插入特定的图像模式来让 OCR 模型在测试阶段输出不可读的字符。</li>
<li>results: 实验结果表明，攻击后 OCR 模型可以成功输出不可读的字符约 90% 的恶意输入图像，而不会对其他输入图像产生影响。<details>
<summary>Abstract</summary>
Optical Character Recognition (OCR) is a widely used tool to extract text from scanned documents. Today, the state-of-the-art is achieved by exploiting deep neural networks. However, the cost of this performance is paid at the price of system vulnerability. For instance, in backdoor attacks, attackers compromise the training phase by inserting a backdoor in the victim's model that will be activated at testing time by specific patterns while leaving the overall model performance intact. This work proposes a backdoor attack for OCR resulting in the injection of non-readable characters from malicious input images. This simple but effective attack exposes the state-of-the-art OCR weakness, making the extracted text correct to human eyes but simultaneously unusable for the NLP application that uses OCR as a preprocessing step. Experimental results show that the attacked models successfully output non-readable characters for around 90% of the poisoned instances without harming their performance for the remaining instances.
</details>
<details>
<summary>摘要</summary>
“光学字符识别（OCR）是一个广泛使用的工具来提取扫描文档中的文字。今天，技术的前进是通过启用深度神经网络来实现的。然而，这些性能的代价是系统的易受攻击性。例如，在后门攻击中，攻击者将在受害者的模型中植入后门，使特定的模式在试验阶段 Activate 时会导致模型产生非法的字符。这个简单而有效的攻击可以让OCR模型对逻辑不正确的输入图像进行处理，从而导致提取的文字 Correct 到人眼看来，但同时也使得这些文字无法用于基于OCR的自然语言处理应用程序中。实验结果显示，攻击模型可以在90%的毒品实验中产生非法的字符，而不会对其他实验中的模型造成影响。”
</details></li>
</ul>
<hr>
<h2 id="Distilling-from-Vision-Language-Models-for-Improved-OOD-Generalization-in-Vision-Tasks"><a href="#Distilling-from-Vision-Language-Models-for-Improved-OOD-Generalization-in-Vision-Tasks" class="headerlink" title="Distilling from Vision-Language Models for Improved OOD Generalization in Vision Tasks"></a>Distilling from Vision-Language Models for Improved OOD Generalization in Vision Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08255">http://arxiv.org/abs/2310.08255</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/val-iisc/VL2V-ADiP">https://github.com/val-iisc/VL2V-ADiP</a></li>
<li>paper_authors: Sravanti Addepalli, Ashish Ramayee Asokan, Lakshay Sharma, R. Venkatesh Babu</li>
<li>for: 这种 исследование的目的是提高在黑盒 Setting中的vision-language模型（VLM）的使用效果，使其在不同数据分布下进行推理，并且可以在有限的任务特定数据上进行减少推理成本。</li>
<li>methods: 该研究提出了一种名为Vision-Language to Vision-Align, Distill, Predict（VL2V-ADiP）的方法，该方法首先对教师模型的视觉语言模式进行对齐，然后将对齐后的VLM嵌入托管到学生模型中，进行减少。</li>
<li>results: 该研究在标准的领域普适化benchmark上达到了黑盒教师设置下的state-of-the-art结果，并且当VLM的权重可用时，也可以达到更高的性能。<details>
<summary>Abstract</summary>
Vision-Language Models (VLMs) such as CLIP are trained on large amounts of image-text pairs, resulting in remarkable generalization across several data distributions. The prohibitively expensive training and data collection/curation costs of these models make them valuable Intellectual Property (IP) for organizations. This motivates a vendor-client paradigm, where a vendor trains a large-scale VLM and grants only input-output access to clients on a pay-per-query basis in a black-box setting. The client aims to minimize inference cost by distilling the VLM to a student model using the limited available task-specific data, and further deploying this student model in the downstream application. While naive distillation largely improves the In-Domain (ID) accuracy of the student, it fails to transfer the superior out-of-distribution (OOD) generalization of the VLM teacher using the limited available labeled images. To mitigate this, we propose Vision-Language to Vision-Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and language modalities of the teacher model with the vision modality of a pre-trained student model, and further distills the aligned VLM embeddings to the student. This maximally retains the pre-trained features of the student, while also incorporating the rich representations of the VLM image encoder and the superior generalization of the text embeddings. The proposed approach achieves state-of-the-art results on the standard Domain Generalization benchmarks in a black-box teacher setting, and also when weights of the VLM are accessible.
</details>
<details>
<summary>摘要</summary>
vision-language模型（VLM）如CLIP在大量图像文本对的训练中显示出惊人的总结能力。这些模型的训练和数据收集/筛选成本高昂，使其成为组织的价值财产。这种厂商-客户模式中，厂商将大规模的VLM训练成功，并只提供输入-输出访问权限给客户，并在黑盒模式下收取访问成本。客户希望通过简化VLM来减少推理成本，并将其部署到下游应用程序中。虽然简化大幅提高了学生模型的区域性（ID）准确率，但是它无法传递VLM教师模型在有限可用标注图像上的出色的跨类泛化性。为解决这个问题，我们提议vision-language到vision-align、distill、predict（VL2V-ADiP），它首先将视语模式的教师模型与先验学生模型的视模式对齐，然后将对齐后的VLM嵌入简化到学生模型中，以保留先验学生模型的特征，同时也包含VLM图像编码器的丰富表示和文本嵌入的出色泛化性。该方法在标准领域普遍化benchmark上达到了黑盒教师模式下的状态艺术成绩，以及可以访问VLM权重时的成绩。
</details></li>
</ul>
<hr>
<h2 id="Fast-Discrete-Optimisation-for-Geometrically-Consistent-3D-Shape-Matching"><a href="#Fast-Discrete-Optimisation-for-Geometrically-Consistent-3D-Shape-Matching" class="headerlink" title="Fast Discrete Optimisation for Geometrically Consistent 3D Shape Matching"></a>Fast Discrete Optimisation for Geometrically Consistent 3D Shape Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08230">http://arxiv.org/abs/2310.08230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Roetzer, Ahmed Abbas, Dongliang Cao, Florian Bernard, Paul Swoboda</li>
<li>for: 提高3D形状匹配的精度和效率。</li>
<li>methods: 结合学习基于和AXIOmatic方法，实现地面一个有效的匹配方案。</li>
<li>results: 提供了一种初始化自由、大量并行化、提供优化差值、运行时间减少和全球最优的匹配方案。<details>
<summary>Abstract</summary>
In this work we propose to combine the advantages of learning-based and combinatorial formalisms for 3D shape matching. While learning-based shape matching solutions lead to state-of-the-art matching performance, they do not ensure geometric consistency, so that obtained matchings are locally unsmooth. On the contrary, axiomatic methods allow to take geometric consistency into account by explicitly constraining the space of valid matchings. However, existing axiomatic formalisms are impractical since they do not scale to practically relevant problem sizes, or they require user input for the initialisation of non-convex optimisation problems. In this work we aim to close this gap by proposing a novel combinatorial solver that combines a unique set of favourable properties: our approach is (i) initialisation free, (ii) massively parallelisable powered by a quasi-Newton method, (iii) provides optimality gaps, and (iv) delivers decreased runtime and globally optimal results for many instances.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提议结合学习基于和组合形式的3D形状匹配方法。学习基于的匹配解决方案可以达到状态最佳的匹配性，但是它们不能保证几何一致性，因此所获得的匹配是地方不稳定。相反，AXIOmatic方法可以直接考虑几何一致性，通过明确限制有效匹配空间。然而，现有的AXIOmatic形式不scalable，或者需要用户输入来初始化非拟合优化问题。在这项工作中，我们希望通过提出一种新的 combinatorial solver，并且这种 solver具有以下优点：我们的方法是（i）无需初始化，（ii）可以大规模并行化，通过 quasi-Newton 方法，（iii）提供优化差，并（iv）在许多实例中具有减少的时间和全球最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Structural-analysis-of-Hindi-online-handwritten-characters-for-character-recognition"><a href="#Structural-analysis-of-Hindi-online-handwritten-characters-for-character-recognition" class="headerlink" title="Structural analysis of Hindi online handwritten characters for character recognition"></a>Structural analysis of Hindi online handwritten characters for character recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08222">http://arxiv.org/abs/2310.08222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anand Sharma, A. G. Ramakrishnan</li>
<li>for: 这个论文的目的是分析在线手写文字的方向性特性，并将其分解成具有共同几何特性的子单元（sub-units）。</li>
<li>methods: 该论文使用了一种方法，即提取点笔、顺时针弧形笔、逆时针弧形笔和循环笔段作为子单元。这些提取的子单元与相应的在线理想文字的子单元具有相似的结构。</li>
<li>results: 该论文的结果表明，使用了本论文提出的子单元提取方法和基于子单元的字符分类器，可以提高在线手写文字识别率。Specifically, the recognition accuracy of the classifier trained with sub-unit level local and character level global features is 93.5%, which is the highest compared with other classifiers trained only with global features.<details>
<summary>Abstract</summary>
Direction properties of online strokes are used to analyze them in terms of homogeneous regions or sub-strokes with points satisfying common geometric properties. Such sub-strokes are called sub-units. These properties are used to extract sub-units from Hindi ideal online characters. These properties along with some heuristics are used to extract sub-units from Hindi online handwritten characters.\\ A method is developed to extract point stroke, clockwise curve stroke, counter-clockwise curve stroke and loop stroke segments as sub-units from Hindi online handwritten characters. These extracted sub-units are close in structure to the sub-units of the corresponding Hindi online ideal characters.\\ Importance of local representation of online handwritten characters in terms of sub-units is assessed by training a classifier with sub-unit level local and character level global features extracted from characters for character recognition. The classifier has the recognition accuracy of 93.5\% on the testing set. This accuracy is the highest when compared with that of the classifiers trained only with global features extracted from characters in the same training set and evaluated on the same testing set.\\ Sub-unit extraction algorithm and the sub-unit based character classifier are tested on Hindi online handwritten character dataset. This dataset consists of samples from 96 different characters. There are 12832 and 2821 samples in the training and testing sets, respectively.
</details>
<details>
<summary>摘要</summary>
irection Properties of Online Strokes are Used to Analyze Them in Terms of Homogeneous Regions or Sub-strokes with Points Satisfying Common Geometric Properties. Such Sub-strokes are Called Sub-units. These Properties are Used to Extract Sub-units from Hindi Ideal Online Characters.\\ A Method is Developed to Extract Point Stroke, Clockwise Curve Stroke, Counter-clockwise Curve Stroke, and Loop Stroke Segments as Sub-units from Hindi Online Handwritten Characters. These Extracted Sub-units are Close in Structure to the Sub-units of the Corresponding Hindi Online Ideal Characters.\\ Importance of Local Representation of Online Handwritten Characters in Terms of Sub-units is Assessed by Training a Classifier with Sub-unit Level Local and Character Level Global Features Extracted from Characters for Character Recognition. The Classifier has the Recognition Accuracy of 93.5% on the Testing Set. This Accuracy is the Highest When Compared with That of the Classifiers Trained Only with Global Features Extracted from Characters in the Same Training Set and Evaluated on the Same Testing Set.\\ Sub-unit Extraction Algorithm and the Sub-unit Based Character Classifier are Tested on Hindi Online Handwritten Character Dataset. This Dataset Consists of Samples from 96 Different Characters. There are 12832 and 2821 Samples in the Training and Testing Sets, Respectively.
</details></li>
</ul>
<hr>
<h2 id="Lifelong-Audio-video-Masked-Autoencoder-with-Forget-robust-Localized-Alignments"><a href="#Lifelong-Audio-video-Masked-Autoencoder-with-Forget-robust-Localized-Alignments" class="headerlink" title="Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments"></a>Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08204">http://arxiv.org/abs/2310.08204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaewoo Lee, Jaehong Yoon, Wonjae Kim, Yunji Kim, Sung Ju Hwang</li>
<li>for: 本研究旨在应对 continuous audio-video 流的学习，即时学习多媒体表示。</li>
<li>methods: 我们提出了两个新想法来解决这个问题：(1) 本地对过程：我们引入了一个小型可训练的多媒体编码器，它预测 audio 和 video 词汇的对顺掌握。这使得模型仅学习高度相关的 audiovisual 矩阵。(2) 忘却Robust多媒体矩阵选择：我们比较了每对 audio-video 矩阵的相对重要性，以mitigate 过去学习的 audiovisual 表示的忘却。</li>
<li>results: 我们的实验显示，FLAVA 比顶对应的 continual learning 方法在多个 bencmark 数据集上表现出色。<details>
<summary>Abstract</summary>
We present a lifelong audio-video masked autoencoder that continually learns the multimodal representations from a video stream containing audio-video pairs, while its distribution continually shifts over time. Specifically, we propose two novel ideas to tackle the problem: (1) Localized Alignment: We introduce a small trainable multimodal encoder that predicts the audio and video tokens that are well-aligned with each other. This allows the model to learn only the highly correlated audiovisual patches with accurate multimodal relationships. (2) Forget-robust multimodal patch selection: We compare the relative importance of each audio-video patch between the current and past data pair to mitigate unintended drift of the previously learned audio-video representations. Our proposed method, FLAVA (Forget-robust Localized Audio-Video Alignment), therefore, captures the complex relationships between the audio and video modalities during training on a sequence of pre-training tasks while alleviating the forgetting of learned audiovisual correlations. Our experiments validate that FLAVA outperforms the state-of-the-art continual learning methods on several benchmark datasets under continual audio-video representation learning scenarios.
</details>
<details>
<summary>摘要</summary>
我们提出了一种持续学习的音频视频匿名自动编码器，该模型从包含音频视频对的视频流中不断学习多modal表示，而其分布也在时间上不断变化。我们提出了两个新的想法来解决这个问题：（1）本地对齐：我们引入了一个可学习的小型多modal编码器，该编码器预测了audio和视频标记的匹配。这使得模型只学习了高度相关的音频视频 patches，并且保持了准确的多modal关系。（2）忘记抗性多modal patch选择：我们比较了当前和过去数据对的相对重要性，以mitigate不必要的演变。我们的提议方法FLAVA（忘记抗性本地音频视频对齐）因此在训练一系列预训练任务时，捕捉了音频视频modal之间的复杂关系，并减轻了已经学习的audiovisual相关性的忘记。我们的实验证明，FLAVA在多个 benchmark 数据集上比state-of-the-art continual learning方法表现出色。
</details></li>
</ul>
<hr>
<h2 id="XIMAGENET-12-An-Explainable-AI-Benchmark-Dataset-for-Model-Robustness-Evaluation"><a href="#XIMAGENET-12-An-Explainable-AI-Benchmark-Dataset-for-Model-Robustness-Evaluation" class="headerlink" title="XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness Evaluation"></a>XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08182">http://arxiv.org/abs/2310.08182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaohai12/explainable-ai-imagenet-12">https://github.com/xiaohai12/explainable-ai-imagenet-12</a></li>
<li>paper_authors: Qiang Li, Dan Zhang, Shengzhao Lei, Xun Zhao, Shuyan Li, Porawit Kamnoedboon, WeiWei Li</li>
<li>for: 本研究旨在提供一个可解释的图像标注数据集，以评估计算机视觉模型在实际应用中的Robustness。</li>
<li>methods: 本研究使用了XIMAGENET-12数据集，该数据集包含200,000张图像和15,600个手动semantic标注。数据集 simulates six diverse scenarios，包括过度曝光、模糊、颜色变化等。</li>
<li>results: 本研究提出了一种新的Robustness criterion，可以评估计算机视觉模型在实际应用中的Robustness。这个数据集， along with related code，是可以用于评估计算机视觉模型的Robustness的重要资源。<details>
<summary>Abstract</summary>
The lack of standardized robustness metrics and the widespread reliance on numerous unrelated benchmark datasets for testing have created a gap between academically validated robust models and their often problematic practical adoption. To address this, we introduce XIMAGENET-12, an explainable benchmark dataset with over 200K images and 15,600 manual semantic annotations. Covering 12 categories from ImageNet to represent objects commonly encountered in practical life and simulating six diverse scenarios, including overexposure, blurring, color changing, etc., we further propose a novel robustness criterion that extends beyond model generation ability assessment. This benchmark dataset, along with related code, is available at https://sites.google.com/view/ximagenet-12/home. Researchers and practitioners can leverage this resource to evaluate the robustness of their visual models under challenging conditions and ultimately benefit from the demands of practical computer vision systems.
</details>
<details>
<summary>摘要</summary>
因为缺乏标准化的稳定性指标和各种不相关的benchmark dataset的广泛依赖，这导致了学术验证的模型和其在实际应用中的问题aticadoptation之间的一个差距。为解决这个问题，我们介绍了ximagenet-12，一个可解释的benchmark dataset，包含超过20万个图像和15600个手动semantic annotations。这些dataset covers 12个类从imageNet中选择了通常在实际生活中遇到的 объекcs，并模拟了6种多样化的enario，包括过度曝光、模糊、颜色变化等。此外，我们还提出了一个新的稳定性标准，超过了模型生成能力评价。这个benchmark dataset， along with related code，可以在https://sites.google.com/view/ximagenet-12/home上获取。研究人员和实践者可以利用这个资源来评估他们的视觉模型在具有挑战性的条件下的稳定性，从而 ultimately benefit from the demands of practical computer vision systems。
</details></li>
</ul>
<hr>
<h2 id="Improving-Fast-Minimum-Norm-Attacks-with-Hyperparameter-Optimization"><a href="#Improving-Fast-Minimum-Norm-Attacks-with-Hyperparameter-Optimization" class="headerlink" title="Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization"></a>Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08177">http://arxiv.org/abs/2310.08177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pralab/HO-FMN">https://github.com/pralab/HO-FMN</a></li>
<li>paper_authors: Giuseppe Floris, Raffaele Mura, Luca Scionis, Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio</li>
<li>for: 提高机器学习模型的敌对 robustness 使用Gradient-based攻击是困难的。</li>
<li>methods: 通过自动选择损失函数、优化器和步长调节器以及它们相关的超参数进行超参数优化，以提高快速最小 нор攻击的效果。</li>
<li>results: 我们在多种Robust模型的广泛评估中发现，通过超参数优化可以提高快速最小 нор攻击的效果。我们发布了相关的开源代码在<a target="_blank" rel="noopener" href="https://github.com/pralab/HO-FMN%E3%80%82">https://github.com/pralab/HO-FMN。</a><details>
<summary>Abstract</summary>
Evaluating the adversarial robustness of machine learning models using gradient-based attacks is challenging. In this work, we show that hyperparameter optimization can improve fast minimum-norm attacks by automating the selection of the loss function, the optimizer and the step-size scheduler, along with the corresponding hyperparameters. Our extensive evaluation involving several robust models demonstrates the improved efficacy of fast minimum-norm attacks when hyper-up with hyperparameter optimization. We release our open-source code at https://github.com/pralab/HO-FMN.
</details>
<details>
<summary>摘要</summary>
evaluating the adversarial robustness of machine learning models using gradient-based attacks is challenging. In this work, we show that hyperparameter optimization can improve fast minimum-norm attacks by automating the selection of the loss function, the optimizer, and the step-size scheduler, along with the corresponding hyperparameters. Our extensive evaluation involving several robust models demonstrates the improved efficacy of fast minimum-norm attacks when hyper-up with hyperparameter optimization. We release our open-source code at https://github.com/pralab/HO-FMN.Here's the translation in Traditional Chinese:评估机器学习模型的敌方性防护效果使用Gradient-based攻击是具有挑战性的。在这个工作中，我们显示出hyperparameter优化可以提高快速最小范数攻击的效率，通过自动选择损失函数、优化器和步长调节器，以及它们所对应的超参数。我们的广泛评估，包括多个预防型模型，显示了快速最小范数攻击的改善效果，当hyper-up with hyperparameter优化时。我们在https://github.com/pralab/HO-FMN上发布了我们的开源代码。
</details></li>
</ul>
<hr>
<h2 id="COVID-19-Detection-Using-Swin-Transformer-Approach-from-Computed-Tomography-Images"><a href="#COVID-19-Detection-Using-Swin-Transformer-Approach-from-Computed-Tomography-Images" class="headerlink" title="COVID-19 Detection Using Swin Transformer Approach from Computed Tomography Images"></a>COVID-19 Detection Using Swin Transformer Approach from Computed Tomography Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08165">http://arxiv.org/abs/2310.08165</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idu-cvlab/cov19d_4th">https://github.com/idu-cvlab/cov19d_4th</a></li>
<li>paper_authors: Kenan Morani</li>
<li>for: 针对大规模医学成像数据集，提出一种新的 COVID-19 诊断方法使用 CT 图像，利用 Swin Transformer 模型的力量，为计算机视觉任务提供现代解决方案。</li>
<li>methods: 方法包括一种系统化的病人级预测方法，即将个别 CT 片分类为 COVID-19 或非 COVID-19，并通过多数投票决定病人的总诊断结果。</li>
<li>results: 对比基准和竞争方法，我们的方法在评价指标中表现出色，具有 Exceptional 的诊断精度。 macro F1 分数达到了基准和竞争方法的高点，提供了一个可靠的 COVID-19 诊断解决方案。<details>
<summary>Abstract</summary>
The accurate and efficient diagnosis of COVID-19 is of paramount importance, particularly in the context of large-scale medical imaging datasets. In this preprint paper, we propose a novel approach for COVID-19 diagnosis using CT images that leverages the power of Swin Transformer models, state-of-the-art solutions in computer vision tasks. Our method includes a systematic approach for patient-level predictions, where individual CT slices are classified as COVID-19 or non-COVID, and the patient's overall diagnosis is determined through majority voting. The application of the Swin Transformer in this context results in patient-level predictions that demonstrate exceptional diagnostic accuracy. In terms of evaluation metrics, our approach consistently outperforms the baseline, as well as numerous competing methods, showcasing its effectiveness in COVID-19 diagnosis. The macro F1 score achieved by our model exceeds the baseline and offers a robust solution for accurate diagnosis.
</details>
<details>
<summary>摘要</summary>
“ covid-19 诊断的精确性和效率非常重要，特别在大规模医疗影像数据中。在这个预印稿中，我们提出了一种新的 covid-19 诊断方法使用 CT 影像，利用了 Swin Transformer 模型，现今的计算机见解应用。我们的方法包括对每个 CT 层进行分类，将每个 CT 层分为 covid-19 或非 covid-19，并通过多数决进行病人级别预测。Swin Transformer 在这个上下文中的应用导致了病人级别预测的非常高精度。在评估指标方面，我们的方法比基准和多个竞争方法表现出色，展示了它在 covid-19 诊断中的有效性。 macro F1 分数由我们的模型实现，超过基准，提供了一个可靠的准确诊断解决方案。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Framework-for-Spatiotemporal-Ultrasound-Localization-Microscopy"><a href="#A-Deep-Learning-Framework-for-Spatiotemporal-Ultrasound-Localization-Microscopy" class="headerlink" title="A Deep Learning Framework for Spatiotemporal Ultrasound Localization Microscopy"></a>A Deep Learning Framework for Spatiotemporal Ultrasound Localization Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08143">http://arxiv.org/abs/2310.08143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Léo Milecki, Jonathan Porée, Hatim Belgharbi, Chloé Bourquin, Rafat Damseh, Patrick Delafontaine-Martel, Frédéric Lesage, Maxime Gasse, Jean Provost</li>
<li>for: 本研究旨在使用深度学习方法重建微vascular网络，以提高ultrasound localization microscopy（ULM）的分辨率。</li>
<li>methods: 本研究使用了三维卷积神经网络（3D-CNN），基于V-net架构，来重建微vascular网络。采用了实际的mouse brain microvascular网络，从2氪微scopy中提取的数据来训练3D-CNN。</li>
<li>results: 本研究的结果表明，使用3D-CNN方法可以提高ULM的分辨率，在silico中的 precisión为81%，与传统ULM框架相比下降。在生物体中，3D-CNN方法可以分解出微vascular网络中的小血管，分辨率高于传统方法。<details>
<summary>Abstract</summary>
Ultrasound Localization Microscopy can resolve the microvascular bed down to a few micrometers. To achieve such performance microbubble contrast agents must perfuse the entire microvascular network. Microbubbles are then located individually and tracked over time to sample individual vessels, typically over hundreds of thousands of images. To overcome the fundamental limit of diffraction and achieve a dense reconstruction of the network, low microbubble concentrations must be used, which lead to acquisitions lasting several minutes. Conventional processing pipelines are currently unable to deal with interference from multiple nearby microbubbles, further reducing achievable concentrations. This work overcomes this problem by proposing a Deep Learning approach to recover dense vascular networks from ultrasound acquisitions with high microbubble concentrations. A realistic mouse brain microvascular network, segmented from 2-photon microscopy, was used to train a three-dimensional convolutional neural network based on a V-net architecture. Ultrasound data sets from multiple microbubbles flowing through the microvascular network were simulated and used as ground truth to train the 3D CNN to track microbubbles. The 3D-CNN approach was validated in silico using a subset of the data and in vivo on a rat brain acquisition. In silico, the CNN reconstructed vascular networks with higher precision (81%) than a conventional ULM framework (70%). In vivo, the CNN could resolve micro vessels as small as 10 $\mu$m with an increase in resolution when compared against a conventional approach.
</details>
<details>
<summary>摘要</summary>
超声本地化微scopic imaging可以达到几微米级别的分辨率。为了实现这一表现，微ubble contrast agents必须在整个微血管网络中流动。然后，微ubble会被 individuated 和跟踪时间，以采样个体血管，通常是数十万张图像。为了超越干扰的基本限制，使用低微ubble浓度，需要持续数分钟的获取。现有的处理管道无法处理多个附近微ubble的干扰，从而降低实现的浓度。这项工作解决了这个问题，提出了基于深度学习的方法，从ultrasound获取 dense vascular network 的重建。使用真实的mouse brain microvascular network，从2气相icroscopy中 segments，并用3维 convolutional neural network (CNN) 基于V-net架构进行训练。ultrasound数据集从多个微ubble流经 microvascular network 进行模拟，并用作真实数据来训练3D CNN。在silico中，CNN可以比 convential ULM framework (70%) 提高分辨率（81%）。在 vivo中，CNN可以分解到10微米级别的微血管，并与 convential approach 比较，显示了增加的分辨率。
</details></li>
</ul>
<hr>
<h2 id="Fine-Grained-Annotation-for-Face-Anti-Spoofing"><a href="#Fine-Grained-Annotation-for-Face-Anti-Spoofing" class="headerlink" title="Fine-Grained Annotation for Face Anti-Spoofing"></a>Fine-Grained Annotation for Face Anti-Spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08142">http://arxiv.org/abs/2310.08142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Chen, Yunde Jia, Yuwei Wu</li>
<li>for: 防止面部验证系统受到攻击，提高面部验证系统的安全性。</li>
<li>methods: 提出了一种细化注释方法，通过使用面部特征点作为提示，获取面部区域的分割 маSK。然后，将这些区域分割成三个分割地图：骗ubble、生物和背景地图。最后，将这三个地图组合成一个三通道地图，用于模型训练。此外，我们还引入了多通道区域交换增强，以增加训练数据的多样性和减少过拟合。</li>
<li>results: 实验结果表明，我们的方法比现有状态的方法在内部和跨 dataset 评估中表现出色，得到了更高的识别率。<details>
<summary>Abstract</summary>
Face anti-spoofing plays a critical role in safeguarding facial recognition systems against presentation attacks. While existing deep learning methods show promising results, they still suffer from the lack of fine-grained annotations, which lead models to learn task-irrelevant or unfaithful features. In this paper, we propose a fine-grained annotation method for face anti-spoofing. Specifically, we first leverage the Segment Anything Model (SAM) to obtain pixel-wise segmentation masks by utilizing face landmarks as point prompts. The face landmarks provide segmentation semantics, which segments the face into regions. We then adopt these regions as masks and assemble them into three separate annotation maps: spoof, living, and background maps. Finally, we combine three separate maps into a three-channel map as annotations for model training. Furthermore, we introduce the Multi-Channel Region Exchange Augmentation (MCREA) to diversify training data and reduce overfitting. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches in both intra-dataset and cross-dataset evaluations.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_text=" Face anti-spoofing plays a critical role in safeguarding facial recognition systems against presentation attacks. While existing deep learning methods show promising results, they still suffer from the lack of fine-grained annotations, which lead models to learn task-irrelevant or unfaithful features. In this paper, we propose a fine-grained annotation method for face anti-spoofing. Specifically, we first leverage the Segment Anything Model (SAM) to obtain pixel-wise segmentation masks by utilizing face landmarks as point prompts. The face landmarks provide segmentation semantics, which segments the face into regions. We then adopt these regions as masks and assemble them into three separate annotation maps: spoof, living, and background maps. Finally, we combine three separate maps into a three-channel map as annotations for model training. Furthermore, we introduce the Multi-Channel Region Exchange Augmentation (MCREA) to diversify training data and reduce overfitting. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches in both intra-dataset and cross-dataset evaluations. "translate_text_simplified = "面部防质备措施对于面部识别系统的保护起到关键作用。现有的深度学习方法虽显示出了扎实的结果，但仍然受到精细注解的缺乏，这导致模型学习到无关任务或不准确的特征。在这篇论文中，我们提议一种精细注解方法 для面部防质备。具体来说，我们首先利用Segment Anything Model（SAM）获取面部像素级分割masks，通过面部特征点作为点提示来使用。这些面部特征点提供分割 semantics，将面部分成不同区域。我们然后采用这些区域作为masks，并将其组装成三个分割图：骗球、生物和背景图。最后，我们将三个分割图合并成三通道的映射，用于模型训练。此外，我们还引入多通道区域交换增强技术（MCREA），以增加训练数据的多样性，降低过拟合。实验结果表明，我们的方法在内部和交叉 dataset 评估中都超过了现有状态码的方法。
</details></li>
</ul>
<hr>
<h2 id="DualAug-Exploiting-Additional-Heavy-Augmentation-with-OOD-Data-Rejection"><a href="#DualAug-Exploiting-Additional-Heavy-Augmentation-with-OOD-Data-Rejection" class="headerlink" title="DualAug: Exploiting Additional Heavy Augmentation with OOD Data Rejection"></a>DualAug: Exploiting Additional Heavy Augmentation with OOD Data Rejection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08139">http://arxiv.org/abs/2310.08139</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuguang99/DualAug">https://github.com/shuguang99/DualAug</a></li>
<li>paper_authors: Zehao Wang, Yiwen Guo, Qizhang Li, Guanglei Yang, Wangmeng Zuo</li>
<li>for: 提高模型泛化和鲁棒性，避免模型适应性问题</li>
<li>methods: 提出了一种新的数据扩充方法，即双重扩充（DualAug），通过混合基本扩充和重大扩充分支来保持扩充在适度上，并且可以适应不同的训练样本</li>
<li>results: 在图像分类Benchmark上进行了广泛的实验，并证明了DualAug可以提高自动数据扩充方法，同时在 semi-supervised learning 和自我监督学习中也有良好的效果<details>
<summary>Abstract</summary>
Data augmentation is a dominant method for reducing model overfitting and improving generalization. Most existing data augmentation methods tend to find a compromise in augmenting the data, \textit{i.e.}, increasing the amplitude of augmentation carefully to avoid degrading some data too much and doing harm to the model performance. We delve into the relationship between data augmentation and model performance, revealing that the performance drop with heavy augmentation comes from the presence of out-of-distribution (OOD) data. Nonetheless, as the same data transformation has different effects for different training samples, even for heavy augmentation, there remains part of in-distribution data which is beneficial to model training. Based on the observation, we propose a novel data augmentation method, named \textbf{DualAug}, to keep the augmentation in distribution as much as possible at a reasonable time and computational cost. We design a data mixing strategy to fuse augmented data from both the basic- and the heavy-augmentation branches. Extensive experiments on supervised image classification benchmarks show that DualAug improve various automated data augmentation method. Moreover, the experiments on semi-supervised learning and contrastive self-supervised learning demonstrate that our DualAug can also improve related method. Code is available at \href{https://github.com/shuguang99/DualAug}{https://github.com/shuguang99/DualAug}.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本转换成简化中文。<</SYS>>数据增强是现有方法中最主要的方法，用于降低模型适应度和提高泛化能力。大多数现有的数据增强方法都是找到一个妥协，即缓和增强数据的方式，以避免一些数据被增强得太多，对模型表现产生负面影响。我们深入研究数据增强和模型性能之间的关系，发现增强后模型表现下降的原因是存在外部数据（OOD）。然而，即使使用同一种数据变换，不同的训练样本会受到不同的影响，甚至在增强得 Very Heavy 时，还有一部分内部数据会对模型训练有利。基于这个观察，我们提出了一种新的数据增强方法，名为 DualAug，可以保持增强在distribution中的可能性最大，同时在合理的时间和计算成本下进行增强。我们设计了一种混合策略，将基本增强和重增强分支中的增强数据混合在一起。广泛的实验表明，我们的 DualAug 可以提高不同的自动数据增强方法，并且在 semi-supervised 学习和对比自动数据增强方法中也有优化效果。代码可以在 <https://github.com/shuguang99/DualAug> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Tailored-Visions-Enhancing-Text-to-Image-Generation-with-Personalized-Prompt-Rewriting"><a href="#Tailored-Visions-Enhancing-Text-to-Image-Generation-with-Personalized-Prompt-Rewriting" class="headerlink" title="Tailored Visions: Enhancing Text-to-Image Generation with Personalized Prompt Rewriting"></a>Tailored Visions: Enhancing Text-to-Image Generation with Personalized Prompt Rewriting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08129">http://arxiv.org/abs/2310.08129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Chen, Lichao Zhang, Fangsheng Weng, Lili Pan, Zhenzhong Lan</li>
<li>for: 提高文本到图像生成的个性化性和用户体验</li>
<li>methods: 利用历史用户与系统交互增强用户提示，并使用大规模文本到图像数据集进行提示重写</li>
<li>results: 比基eline方法有显著提高，在新的离线评估方法和在线测试中得到较高的效果<details>
<summary>Abstract</summary>
We propose a novel perspective of viewing large pretrained models as search engines, thereby enabling the repurposing of techniques previously used to enhance search engine performance. As an illustration, we employ a personalized query rewriting technique in the realm of text-to-image generation. Despite significant progress in the field, it is still challenging to create personalized visual representations that align closely with the desires and preferences of individual users. This process requires users to articulate their ideas in words that are both comprehensible to the models and accurately capture their vision, posing difficulties for many users. In this paper, we tackle this challenge by leveraging historical user interactions with the system to enhance user prompts. We propose a novel approach that involves rewriting user prompts based a new large-scale text-to-image dataset with over 300k prompts from 3115 users. Our rewriting model enhances the expressiveness and alignment of user prompts with their intended visual outputs. Experimental results demonstrate the superiority of our methods over baseline approaches, as evidenced in our new offline evaluation method and online tests. Our approach opens up exciting possibilities of applying more search engine techniques to build truly personalized large pretrained models.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的视角，即将大型预训练模型视为搜索引擎，从而使得可以复用以前用于提高搜索引擎性能的技术。作为一个示例，我们在文本到图生成领域使用了个性化查询 rewrite 技术。虽然在这个领域已经做出了很大的进步，但是仍然很难创造个性化的视觉表示，使得用户需要用语言来表达他们的想法，这会对用户提出很大的挑战。在这篇论文中，我们解决了这个问题，通过利用系统历史用户交互记录来增强用户提示。我们提出了一种新的方法，即基于大规模文本到图数据集（包含超过 300k 提示，来自 3115 名用户）进行用户提示 rewrite。我们的 rewrite 模型可以提高用户提示的表达力和与愿景的匹配度。实验结果表明我们的方法在基准方法上有superiority，可见于我们新的离线评估方法和在线测试中。我们的方法开 up了应用更多搜索引擎技术来建立真正个性化的大型预训练模型的可能性。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Active-Measurement-for-Human-Mesh-Recovery-in-Close-Proximity"><a href="#Multimodal-Active-Measurement-for-Human-Mesh-Recovery-in-Close-Proximity" class="headerlink" title="Multimodal Active Measurement for Human Mesh Recovery in Close Proximity"></a>Multimodal Active Measurement for Human Mesh Recovery in Close Proximity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08116">http://arxiv.org/abs/2310.08116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takahiro Maeda, Keisuke Takeshita, Kazuhito Tanaka<br>for: 这个研究旨在提高人机交互中机器人的人体位姿估计精度，以实现安全和复杂的人机交互。methods: 本研究提出了一个活动测量和感应融合框架，使用equipped镜头和其他感应器，如触摸感应器和2D LiDAR，在人机交互中获取稀疏但可靠的感应讯号，并融合这些感应讯号和镜头测量估计的人体位姿，以提高人体位姿估计精度。results: 实验结果显示， compared to existing methods, 本研究的方法能够更好地估计人体位姿，尤其是在实际情况下，如人被覆盖物品 occluded 和人机交互中。<details>
<summary>Abstract</summary>
For safe and sophisticated physical human-robot interactions (pHRI), a robot needs to estimate the accurate body pose or mesh of the target person. However, in these pHRI scenarios, the robot cannot fully observe the target person's body with equipped cameras because the target person is usually close to the robot. This leads to severe truncation and occlusions, and results in poor accuracy of human pose estimation. For better accuracy of human pose estimation or mesh recovery on this limited information from cameras, we propose an active measurement and sensor fusion framework of the equipped cameras and other sensors such as touch sensors and 2D LiDAR. These touch and LiDAR sensing are obtained attendantly through pHRI without additional costs. These sensor measurements are sparse but reliable and informative cues for human mesh recovery. In our active measurement process, camera viewpoints and sensor placements are optimized based on the uncertainty of the estimated pose, which is closely related to the truncated or occluded areas. In our sensor fusion process, we fuse the sensor measurements to the camera-based estimated pose by minimizing the distance between the estimated mesh and measured positions. Our method is agnostic to robot configurations. Experiments were conducted using the Toyota Human Support Robot, which has a camera, 2D LiDAR, and a touch sensor on the robot arm. Our proposed method demonstrated the superiority in the human pose estimation accuracy on the quantitative comparison. Furthermore, our proposed method reliably estimated the pose of the target person in practical settings such as target people occluded by a blanket and standing aid with the robot arm.
</details>
<details>
<summary>摘要</summary>
为实现安全和复杂的人机 робо交互（pHRI）， робо需要估算target人体姿或网格的准确位置。然而，在这些pHRI场景中，робо不能完全观察target人体的全部部分，因此会出现严重的截断和遮挡，导致人体姿势估算的精度低。为了提高人体姿势估算或网格恢复的精度，我们提议使用配备了摄像头和其他感知器的活动测量和感知融合框架。这些触感和LiDAR感知通过pHRI获得，无需额外成本。这些感知测量 sparse yet reliable and informative cues for human mesh recovery。在我们的活动测量过程中，摄像头视点和感知器位置被优化基于估算 pose 的uncertainty，这与 truncated或 occluded areas 有关。在我们的感知融合过程中，我们将感知测量与摄像头基于 estimated pose 进行融合，以iminize the distance between the estimated mesh and measured positions。我们的方法不受机器人配置的限制。我们在使用 Toyota Human Support Robot，它配备了摄像头、2D LiDAR和触感器，进行实验。我们的提议方法在量化比较中表现出了superiority。此外，我们的方法可靠地估算target人体姿势在实际场景中，如target人被布料 occluded 和robot臂上的standing aid。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Logit-Adjustment-Calibrating-Fine-tuned-Models-by-Removing-Label-Bias-in-Foundation-Models"><a href="#Generalized-Logit-Adjustment-Calibrating-Fine-tuned-Models-by-Removing-Label-Bias-in-Foundation-Models" class="headerlink" title="Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models"></a>Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08106">http://arxiv.org/abs/2310.08106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BeierZhu/GLA">https://github.com/BeierZhu/GLA</a></li>
<li>paper_authors: Beier Zhu, Kaihua Tang, Qianru Sun, Hanwang Zhang</li>
<li>for: 提高预训练模型的表现，尤其是在零shot任务上。</li>
<li>methods: 研究基础模型中的内在偏见问题，并提出一种通过优化来减少这种偏见的方法（Generalized Logit Adjustment，GLA）。</li>
<li>results: 在多个任务上达到了显著的改善，包括在ImageNet上的1.5 pp精度提升，以及在11个少量数据集上的大均值改善（1.4-4.6 pp）和长尾分类任务上的2.4 pp提升。<details>
<summary>Abstract</summary>
Foundation models like CLIP allow zero-shot transfer on various tasks without additional training data. Yet, the zero-shot performance is less competitive than a fully supervised one. Thus, to enhance the performance, fine-tuning and ensembling are also commonly adopted to better fit the downstream tasks. However, we argue that such prior work has overlooked the inherent biases in foundation models. Due to the highly imbalanced Web-scale training set, these foundation models are inevitably skewed toward frequent semantics, and thus the subsequent fine-tuning or ensembling is still biased. In this study, we systematically examine the biases in foundation models and demonstrate the efficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that bias estimation in foundation models is challenging, as most pre-train data cannot be explicitly accessed like in traditional long-tailed classification tasks. To this end, GLA has an optimization-based bias estimation approach for debiasing foundation models. As our work resolves a fundamental flaw in the pre-training, the proposed GLA demonstrates significant improvements across a diverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large average improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on long-tailed classification. Codes are in \url{https://github.com/BeierZhu/GLA}.
</details>
<details>
<summary>摘要</summary>
基于CLIP等基础模型的零shot传输能力在多种任务上表现不佳，但是通过精度调整和组合来进一步适应下游任务的性能。然而，我们认为这些前工作忽略了基础模型内置的偏见。由于Web规模训练集的高度偏袋性，这些基础模型无法快速识别少见的 semantics，因此后续的精度调整或组合仍然偏袋。在本研究中，我们系统地检查基础模型中的偏见，并示出我们的提议的通用Logit调整（GLA）方法的效果。尽管对基础模型的偏见估计是一项挑战，因为大多数预训练数据无法直接访问如传统长尾分类任务一样。为此，GLA使用优化基本偏见估计方法来减少基础模型的偏见。我们的工作解决了预训练的基本漏洞，因此我们的GLA方法在多种任务上表现出了显著改进：在ImageNet上达到1.5 pp的精度提升，在11个少量样本任务上平均提高1.4-4.6 pp，在长尾分类任务上提高2.4 pp。代码在\url{https://github.com/BeierZhu/GLA}。
</details></li>
</ul>
<hr>
<h2 id="SingleInsert-Inserting-New-Concepts-from-a-Single-Image-into-Text-to-Image-Models-for-Flexible-Editing"><a href="#SingleInsert-Inserting-New-Concepts-from-a-Single-Image-into-Text-to-Image-Models-for-Flexible-Editing" class="headerlink" title="SingleInsert: Inserting New Concepts from a Single Image into Text-to-Image Models for Flexible Editing"></a>SingleInsert: Inserting New Concepts from a Single Image into Text-to-Image Models for Flexible Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08094">http://arxiv.org/abs/2310.08094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Wu, Chaohui Yu, Zhen Zhu, Fan Wang, Xiang Bai</li>
<li>for: 这个研究的目的是提出一个简单且有效的单一图像转文本（I2T）倒排基eline，实现高品质的图像生成和自由的文本控制。</li>
<li>methods: 这个基eline使用了两个阶段的方案，第一阶段是调整学习的对象 embedding，使其专注在对话领域而不与无关的背景相关。第二阶段是精微调整T2I模型，以提高图像的可观性和避免语言漂移问题。</li>
<li>results: 这个基eline可以实现高品质的单一概念生成，同时允许自由的编辑。此外，这个基eline也可以实现单一图像新视角生成和多概念合成，不需要共同训练。我们设计了一个编辑提示列表和一个名为Editing Success Rate（ESR）的评估指标，以便评估编辑的灵活性。<details>
<summary>Abstract</summary>
Recent progress in text-to-image (T2I) models enables high-quality image generation with flexible textual control. To utilize the abundant visual priors in the off-the-shelf T2I models, a series of methods try to invert an image to proper embedding that aligns with the semantic space of the T2I model. However, these image-to-text (I2T) inversion methods typically need multiple source images containing the same concept or struggle with the imbalance between editing flexibility and visual fidelity. In this work, we point out that the critical problem lies in the foreground-background entanglement when learning an intended concept, and propose a simple and effective baseline for single-image I2T inversion, named SingleInsert. SingleInsert adopts a two-stage scheme. In the first stage, we regulate the learned embedding to concentrate on the foreground area without being associated with the irrelevant background. In the second stage, we finetune the T2I model for better visual resemblance and devise a semantic loss to prevent the language drift problem. With the proposed techniques, SingleInsert excels in single concept generation with high visual fidelity while allowing flexible editing. Additionally, SingleInsert can perform single-image novel view synthesis and multiple concepts composition without requiring joint training. To facilitate evaluation, we design an editing prompt list and introduce a metric named Editing Success Rate (ESR) for quantitative assessment of editing flexibility. Our project page is: https://jarrentwu1031.github.io/SingleInsert-web/
</details>
<details>
<summary>摘要</summary>
最近的文本到图像（T2I）模型进步，使得高质量图像生成变得可控。为了利用存在的图像Visual prior，一些方法尝试将图像转换为与T2I模型的semantic空间匹配的嵌入。然而，这些图像到文本（I2T）反向方法通常需要多个包含同一概念的源图像，或者面临着编辑灵活性和视觉准确性之间的矛盾。在这种情况下，我们指出了带前景背景杂化的问题是学习某一概念的关键问题。为了解决这问题，我们提出了一种简单而有效的基线方法，名为SingleInsert。SingleInsert采用两个阶段方案。在第一阶段，我们规定学习的嵌入向量集中注意力集中在前景区域，而不与无关的背景相关。在第二阶段，我们进一步训练T2I模型，以更好地保持视觉准确性，并设置了semantic损失，以避免语言迁移问题。与传统方法相比，SingleInsert在单个概念生成中实现高视觉准确性，同时允许高灵活度编辑。此外，SingleInsert还可以完成单图像新视图生成和多个概念组合，无需共同训练。为方便评估，我们设计了编辑提示列表，并引入了一个名为Editing Success Rate（ESR）的评价指标，用于评估编辑flexibility的量化评价。我们的项目页面是：https://jarrentwu1031.github.io/SingleInsert-web/
</details></li>
</ul>
<hr>
<h2 id="Consistent123-Improve-Consistency-for-One-Image-to-3D-Object-Synthesis"><a href="#Consistent123-Improve-Consistency-for-One-Image-to-3D-Object-Synthesis" class="headerlink" title="Consistent123: Improve Consistency for One Image to 3D Object Synthesis"></a>Consistent123: Improve Consistency for One Image to 3D Object Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08092">http://arxiv.org/abs/2310.08092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong Zhang, C. L. Philip Chen, Lei Zhang</li>
<li>for: 提高视图一致性和三维重建性</li>
<li>methods:  incorporating additional cross-view attention layers and shared self-attention mechanism</li>
<li>results:  outperforms baselines in view consistency and shows great potential in 3D generation field<details>
<summary>Abstract</summary>
Large image diffusion models enable novel view synthesis with high quality and excellent zero-shot capability. However, such models based on image-to-image translation have no guarantee of view consistency, limiting the performance for downstream tasks like 3D reconstruction and image-to-3D generation. To empower consistency, we propose Consistent123 to synthesize novel views simultaneously by incorporating additional cross-view attention layers and the shared self-attention mechanism. The proposed attention mechanism improves the interaction across all synthesized views, as well as the alignment between the condition view and novel views. In the sampling stage, such architecture supports simultaneously generating an arbitrary number of views while training at a fixed length. We also introduce a progressive classifier-free guidance strategy to achieve the trade-off between texture and geometry for synthesized object views. Qualitative and quantitative experiments show that Consistent123 outperforms baselines in view consistency by a large margin. Furthermore, we demonstrate a significant improvement of Consistent123 on varying downstream tasks, showing its great potential in the 3D generation field. The project page is available at consistent-123.github.io.
</details>
<details>
<summary>摘要</summary>
大型图像扩散模型可以实现高质量的新视图合成，但这些模型基于图像到图像翻译没有保证视图一致性，这限制了下游任务如3D重建和图像到3D转换的性能。为了强化一致性，我们提议Consistent123同时生成新视图，通过添加跨视图注意力层和共享自注意机制来实现。该注意力机制提高了所生成视图之间的交互，以及condition视图和新视图之间的对齐。在抽取阶段，这种建筑支持同时生成任意数量的视图，并在固定长度进行训练。我们还提出了不需要分类器的进度导航策略，以实现Texture和Geometry之间的融合。Qualitative和量化实验显示，Consistent123在视图一致性方面大幅超过基eline。此外，我们还证明Consistent123在不同的下游任务上表现出了很大的提升，这表明它在3D生成领域的潜力非常大。项目页面可以在consistent-123.github.io上找到。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Shape-and-Appearance-Priors-for-Few-Shot-Full-Head-Reconstruction"><a href="#Implicit-Shape-and-Appearance-Priors-for-Few-Shot-Full-Head-Reconstruction" class="headerlink" title="Implicit Shape and Appearance Priors for Few-Shot Full Head Reconstruction"></a>Implicit Shape and Appearance Priors for Few-Shot Full Head Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08784">http://arxiv.org/abs/2310.08784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pol Caselles, Eduard Ramon, Jaime Garcia, Gil Triginer, Francesc Moreno-Noguer</li>
<li>for: 这篇论文主要targets few-shot full 3D head reconstruction, aiming to improve the efficiency and accuracy of coordinate-based neural representations.</li>
<li>methods: 该方法具有以下三个特点：1)  incorporating a probabilistic shape and appearance prior into coordinate-based representations, 2) leveraging a differentiable renderer for fitting a signed distance function, and 3) employing parallelizable ray tracing and dynamic caching strategies.</li>
<li>results: 该方法可以在只使用几张输入图像（甚至只有一张）的情况下实现高精度的3D头部重建，并且比前一代方法快速多了一个数量级。此外，该方法还可以在测试阶段使用H3DS数据集进行评估，并达到了当前最佳的结果。<details>
<summary>Abstract</summary>
Recent advancements in learning techniques that employ coordinate-based neural representations have yielded remarkable results in multi-view 3D reconstruction tasks. However, these approaches often require a substantial number of input views (typically several tens) and computationally intensive optimization procedures to achieve their effectiveness. In this paper, we address these limitations specifically for the problem of few-shot full 3D head reconstruction. We accomplish this by incorporating a probabilistic shape and appearance prior into coordinate-based representations, enabling faster convergence and improved generalization when working with only a few input images (even as low as a single image). During testing, we leverage this prior to guide the fitting process of a signed distance function using a differentiable renderer. By incorporating the statistical prior alongside parallelizable ray tracing and dynamic caching strategies, we achieve an efficient and accurate approach to few-shot full 3D head reconstruction. Moreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D full head scans and their corresponding posed images and masks, which we use for evaluation purposes. By leveraging this dataset, we demonstrate the remarkable capabilities of our approach in achieving state-of-the-art results in geometry reconstruction while being an order of magnitude faster than previous approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Volumetric-Medical-Image-Segmentation-via-Scribble-Annotations-and-Shape-Priors"><a href="#Volumetric-Medical-Image-Segmentation-via-Scribble-Annotations-and-Shape-Priors" class="headerlink" title="Volumetric Medical Image Segmentation via Scribble Annotations and Shape Priors"></a>Volumetric Medical Image Segmentation via Scribble Annotations and Shape Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08084">http://arxiv.org/abs/2310.08084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiuhui Chen, Haiying Lyu, Xinyue Hu, Yong Lu, Yi Hong</li>
<li>for: 这个论文目的是提出一种基于scribble的三维图像分割方法，以提高边界预测和ROI的形态regularization。</li>
<li>methods: 该方法使用了一种2.5D注意力UNet，加上一个提议的标签传播模块，以扩展scribble中的semantic信息，并使用了static和active边界预测来学习ROI的边界和形态regulation。</li>
<li>results: 对于三个公共数据集和一个私有数据集， experiments demonstrate that our Scribble2D5方法可以在基于scribble的volumetric图像分割 task中 achieve state-of-the-art performance，并且可以利用shape prior信息来进一步提高模型准确性。<details>
<summary>Abstract</summary>
Recently, weakly-supervised image segmentation using weak annotations like scribbles has gained great attention in computer vision and medical image analysis, since such annotations are much easier to obtain compared to time-consuming and labor-intensive labeling at the pixel/voxel level. However, due to a lack of structure supervision on regions of interest (ROIs), existing scribble-based methods suffer from poor boundary localization. Furthermore, most current methods are designed for 2D image segmentation, which do not fully leverage the volumetric information if directly applied to each image slice. In this paper, we propose a scribble-based volumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image segmentation and aims to its improve boundary prediction. To achieve this, we augment a 2.5D attention UNet with a proposed label propagation module to extend semantic information from scribbles and use a combination of static and active boundary prediction to learn ROI's boundary and regularize its shape. Also, we propose an optional add-on component, which incorporates the shape prior information from unpaired segmentation masks to further improve model accuracy. Extensive experiments on three public datasets and one private dataset demonstrate our Scribble2D5 achieves state-of-the-art performance on volumetric image segmentation using scribbles and shape prior if available.
</details>
<details>
<summary>摘要</summary>
Translation:近期，受到scribble（简要标注）的关注强化了计算机视觉和医学影像分析领域，因为这些标注比 pixel/voxel 级别的时间consuming和劳动 INTENSIVE 标注更加容易获得。然而，由于ROI（区域关注点）的结构监督缺乏，现有的scribble-based方法受到边界预测的差。此外，大多数当前方法是为2D图像分割而设计，这些方法直接应用于每个图像片不会充分利用图像堆叠中的三维信息。在这篇论文中，我们提出了一种基于scribble的三维图像分割方法，即Scribble2D5，该方法旨在改进边界预测。为了实现这一点，我们将2.5D注意力UNet（2.5D注意力网络）与一个提议的标签传播模块相结合，以延伸scribble中的semantic信息，并使用组合动态和活动边界预测来学习ROI的边界和正则化其形状。此外，我们还提出了一个可选的组件，即将不对应分割mask中的形状优先信息 integrate到模型中，以进一步提高模型精度。广泛的实验表明，我们的Scribble2D5在使用scribble和形状优先信息时取得了state-of-the-art的性能。
</details></li>
</ul>
<hr>
<h2 id="Jointly-Optimized-Global-Local-Visual-Localization-of-UAVs"><a href="#Jointly-Optimized-Global-Local-Visual-Localization-of-UAVs" class="headerlink" title="Jointly Optimized Global-Local Visual Localization of UAVs"></a>Jointly Optimized Global-Local Visual Localization of UAVs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08082">http://arxiv.org/abs/2310.08082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoling Li, Jiuniu Wang, Zhiwei Wei, Wenjia Xu</li>
<li>For: 本研究旨在解决无人机在GNSS干扰和不可靠情况下的导航和定位问题，特别是解决传统方法（如同时地图和视差估计）的缺陷，如错误积累和实时性不足。* Methods: 我们提出了一种新的全球-地方视觉定位网络（GLVL），该网络是一种两个阶段的视觉定位方法，其首先使用大规模检索模块找到与无人机飞行场景中相似的区域，然后使用细腻匹配模块确定精确的无人机坐标，实现实时和精确的定位。* Results: 我们在六个无人机飞行场景中进行了实验，包括了Texture-rich和Texture-sparse两类场景。结果表明，我们的方法可以实现实时精确的定位要求，特别是在村庄场景中，我们的方法可以在0.48秒内达到2.39米的定位错误。<details>
<summary>Abstract</summary>
Navigation and localization of UAVs present a challenge when global navigation satellite systems (GNSS) are disrupted and unreliable. Traditional techniques, such as simultaneous localization and mapping (SLAM) and visual odometry (VO), exhibit certain limitations in furnishing absolute coordinates and mitigating error accumulation. Existing visual localization methods achieve autonomous visual localization without error accumulation by matching with ortho satellite images. However, doing so cannot guarantee real-time performance due to the complex matching process. To address these challenges, we propose a novel Global-Local Visual Localization (GLVL) network. Our GLVL network is a two-stage visual localization approach, combining a large-scale retrieval module that finds similar regions with the UAV flight scene, and a fine-grained matching module that localizes the precise UAV coordinate, enabling real-time and precise localization. The training process is jointly optimized in an end-to-end manner to further enhance the model capability. Experiments on six UAV flight scenes encompassing both texture-rich and texture-sparse regions demonstrate the ability of our model to achieve the real-time precise localization requirements of UAVs. Particularly, our method achieves a localization error of only 2.39 meters in 0.48 seconds in a village scene with sparse texture features.
</details>
<details>
<summary>摘要</summary>
Navigation and localization of UAVs present a challenge when global navigation satellite systems (GNSS) are disrupted and unreliable. Traditional techniques, such as simultaneous localization and mapping (SLAM) and visual odometry (VO), have certain limitations in providing absolute coordinates and mitigating error accumulation. Existing visual localization methods can achieve autonomous visual localization without error accumulation by matching with ortho satellite images, but this cannot guarantee real-time performance due to the complex matching process. To address these challenges, we propose a novel Global-Local Visual Localization (GLVL) network. Our GLVL network is a two-stage visual localization approach, combining a large-scale retrieval module that finds similar regions with the UAV flight scene, and a fine-grained matching module that localizes the precise UAV coordinate, enabling real-time and precise localization. The training process is jointly optimized in an end-to-end manner to further enhance the model capability. Experiments on six UAV flight scenes encompassing both texture-rich and texture-sparse regions demonstrate the ability of our model to achieve the real-time precise localization requirements of UAVs. Particularly, our method achieves a localization error of only 2.39 meters in 0.48 seconds in a village scene with sparse texture features.Here's the word-for-word translation of the text into Simplified Chinese:导航和地理位置系统（GNSS）在受到干扰和不可靠时，UAV的导航和地理位置问题具有挑战性。传统技术，如同时地理位置和地图（SLAM）和视觉速度（VO），在提供绝对坐标和减少错误偏差方面存在一定的局限性。现有的视觉定位方法可以通过与正交卫星图像匹配来实现无错误的自主视觉定位，但这无法保证实时性。为解决这些挑战，我们提出了一种新的全球视觉定位网络（GLVL）。我们的 GLVL 网络是一种两stage的视觉定位方法，包括一个大规模检索模块，找到与 UAV 飞行场景相似的区域，以及一个细化匹配模块，在 UAV 坐标上进行精度定位，实现实时和准确的定位。训练过程是在端到端方式进行并行优化，以进一步提高模型能力。实验结果表明，我们的方法可以在包括Texture-rich和Texture-sparse区域的六个 UAV 飞行场景中实现实时精度定位要求。特别是，我们的方法在村庄场景中，具有稀疏特征的Texture-sparse区域，可以实现只有2.39米的地理位置错误，在0.48秒内完成。
</details></li>
</ul>
<hr>
<h2 id="RT-SRTS-Angle-Agnostic-Real-Time-Simultaneous-3D-Reconstruction-and-Tumor-Segmentation-from-Single-X-Ray-Projection"><a href="#RT-SRTS-Angle-Agnostic-Real-Time-Simultaneous-3D-Reconstruction-and-Tumor-Segmentation-from-Single-X-Ray-Projection" class="headerlink" title="RT-SRTS: Angle-Agnostic Real-Time Simultaneous 3D Reconstruction and Tumor Segmentation from Single X-Ray Projection"></a>RT-SRTS: Angle-Agnostic Real-Time Simultaneous 3D Reconstruction and Tumor Segmentation from Single X-Ray Projection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08080">http://arxiv.org/abs/2310.08080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miao Zhu, Qiming Fu, Bo Liu, Mengxi Zhang, Bojian Li, Xiaoyan Luo, Fugen Zhou</li>
<li>for: 这篇论文的目的是提出一种新的医疗影像重建方法，以帮助肿瘤治疗中的放射线治疗过程。</li>
<li>methods: 这篇论文使用的方法是基于多任务学习（MTL）的一种综合三维图像重建和肿瘤分类的网络，可以实现单据X射线像面的实时三维重建和肿瘤分类。此外，还提出了注意力增强calibrator（AEC）和不确定区域详细（URE）模组，以帮助特征提取和提高分类精度。</li>
<li>results: 这篇论文的结果显示，提出的方法可以实现实时三维重建和肿瘤分类，并且与两种现有方法比较，表现更加出色。实际上，这篇论文可以实现单据X射线像面的实时三维重建和肿瘤分类，并且可以在约70ms内完成这个过程，远远超过了实时肿瘤追踪所需的时间点。此外，还进一步验证了AEC和URE模组的有效性。<details>
<summary>Abstract</summary>
Radiotherapy is one of the primary treatment methods for tumors, but the organ movement caused by respiratory motion limits its accuracy. Recently, 3D imaging from single X-ray projection receives extensive attentions as a promising way to address this issue. However, current methods can only reconstruct 3D image without direct location of the tumor and are only validated for fixed-angle imaging, which fails to fully meet the requirement of motion control in radiotherapy. In this study, we propose a novel imaging method RT-SRTS which integrates 3D imaging and tumor segmentation into one network based on the multi-task learning (MTL) and achieves real-time simultaneous 3D reconstruction and tumor segmentation from single X-ray projection at any angle. Futhermore, we propose the attention enhanced calibrator (AEC) and uncertain-region elaboration (URE) modules to aid feature extraction and improve segmentation accuracy. We evaluated the proposed method on ten patient cases and compared it with two state-of-the-art methods. Our approach not only delivered superior 3D reconstruction but also demonstrated commendable tumor segmentation results. The simultaneous reconstruction and segmentation could be completed in approximately 70 ms, significantly faster than the required time threshold for real-time tumor tracking. The efficacy of both AEC and URE was also validated through ablation studies.
</details>
<details>
<summary>摘要</summary>
医学中，辐射疗法是肿瘤的主要治疗方法，但是呼吸运动引起的器官运动限制了它的精度。最近，3D成像从单个X射线投影所receives extensive attention为一种有前途的方法来解决这个问题。然而，当前的方法只能重建3D图像而不是直接定位肿瘤，并且只适用于固定角度的成像，这些方法无法充分满足肿瘤跟踪的需求。在本研究中，我们提出了一种新的成像方法，即RT-SRTS，它将3D成像和肿瘤分割 integrate into one network based on multi-task learning (MTL)，并在单个X射线投影任意角度下实现实时同步3D重建和肿瘤分割。此外，我们还提出了注意力增强calibrator (AEC)和uncertain-region elaboration (URE)模块，以帮助特征提取和提高分割精度。我们对十个患者案例进行了评估，并与两种当前最佳方法进行比较。我们的方法不仅提供了superior 3D重建，还demonstrated commendable tumor segmentation results。同时，我们的方法可以在约70ms内完成同步重建和分割，这比较于实时肿瘤跟踪的时间要求更快。此外，我们还 validate了AEC和URE模块的效果通过ablation study。
</details></li>
</ul>
<hr>
<h2 id="Samples-on-Thin-Ice-Re-Evaluating-Adversarial-Pruning-of-Neural-Networks"><a href="#Samples-on-Thin-Ice-Re-Evaluating-Adversarial-Pruning-of-Neural-Networks" class="headerlink" title="Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural Networks"></a>Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08073">http://arxiv.org/abs/2310.08073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio</li>
<li>for: 这篇论文的目的是重新评估三种最新的对抗式范例遗传方法，并评估这些方法的稳定性和抗衰变性。</li>
<li>methods: 这篇论文使用了三种最新的对抗式范例遗传方法，分别是 adversarial training、input preprocessing 和 output preprocessing。</li>
<li>results: 研究发现，这三种方法的 robustness 被过度估计，而且对于较具有挑战性的测试数据集，这些方法的表现相对较差。此外，研究发现这些方法遗传后的模型通常会对于较接近原始模型的决策界面的样本进行错误分类。<details>
<summary>Abstract</summary>
Neural network pruning has shown to be an effective technique for reducing the network size, trading desirable properties like generalization and robustness to adversarial attacks for higher sparsity. Recent work has claimed that adversarial pruning methods can produce sparse networks while also preserving robustness to adversarial examples. In this work, we first re-evaluate three state-of-the-art adversarial pruning methods, showing that their robustness was indeed overestimated. We then compare pruned and dense versions of the same models, discovering that samples on thin ice, i.e., closer to the unpruned model's decision boundary, are typically misclassified after pruning. We conclude by discussing how this intuition may lead to designing more effective adversarial pruning methods in future work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Transferable-Conceptual-Prototypes-for-Interpretable-Unsupervised-Domain-Adaptation"><a href="#Learning-Transferable-Conceptual-Prototypes-for-Interpretable-Unsupervised-Domain-Adaptation" class="headerlink" title="Learning Transferable Conceptual Prototypes for Interpretable Unsupervised Domain Adaptation"></a>Learning Transferable Conceptual Prototypes for Interpretable Unsupervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08071">http://arxiv.org/abs/2310.08071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Gao, Xinhong Ma, Changsheng Xu</li>
<li>for: 本研究旨在提出一种可解释的频繁领域适应（UDA）方法，以提高模型的安全性和可控性。</li>
<li>methods: 本方法基于层次分类模型，设计了一个层次概念模型（TCPL），通过将来源频繁领域的基本概念传递到目标频繁领域，学习了频繁领域共享的原型。同时，设计了一种自适应的自我预测稳定潜在标签策略，以选择适合 Pseudo 注解的目标样本，逐渐缩小频繁领域的差距。</li>
<li>results: 实验表明，提出的方法可以不仅提供有效和直观的解释，还能够超越之前的状态。<details>
<summary>Abstract</summary>
Despite the great progress of unsupervised domain adaptation (UDA) with the deep neural networks, current UDA models are opaque and cannot provide promising explanations, limiting their applications in the scenarios that require safe and controllable model decisions. At present, a surge of work focuses on designing deep interpretable methods with adequate data annotations and only a few methods consider the distributional shift problem. Most existing interpretable UDA methods are post-hoc ones, which cannot facilitate the model learning process for performance enhancement. In this paper, we propose an inherently interpretable method, named Transferable Conceptual Prototype Learning (TCPL), which could simultaneously interpret and improve the processes of knowledge transfer and decision-making in UDA. To achieve this goal, we design a hierarchically prototypical module that transfers categorical basic concepts from the source domain to the target domain and learns domain-shared prototypes for explaining the underlying reasoning process. With the learned transferable prototypes, a self-predictive consistent pseudo-label strategy that fuses confidence, predictions, and prototype information, is designed for selecting suitable target samples for pseudo annotations and gradually narrowing down the domain gap. Comprehensive experiments show that the proposed method can not only provide effective and intuitive explanations but also outperform previous state-of-the-arts.
</details>
<details>
<summary>摘要</summary>
尽管深度神经网络在无监督领域适应（UDA）中做出了很大的进步，但目前的UDA模型仍然不透明，无法提供有前途的解释，限制其在需要安全和可控的模型决策的场景中的应用。目前，大量的研究集中在设计深度可解释方法上，但大多数这些方法仅考虑了数据注解的问题，而很少考虑分布shift问题。现有的可解释UDA方法都是后续的方法，无法促进模型性能的提高。在这篇论文中，我们提出了内置可解释的方法，即传递可读 prototype 学习（TCPL），可同时解释和改进知识传递和决策过程。为 достичь这个目标，我们设计了层次prototype模块，将来源领域中的基本概念传递到目标领域，并在不同领域之间学习共享的概念示例。通过学习传递的示例，我们设计了一种自预测一致的 pseudo-label 策略，将信任度、预测值和示例信息 fusion 以选择适合 pseudo 标注的目标样本，逐渐缩小领域差距。经过完整的实验表明，我们的方法不仅可以提供有效和直观的解释，还可以超越先前的状态 искус。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Aware-Re-Parameterization-for-Over-Fitting-Based-Image-Compression"><a href="#Frequency-Aware-Re-Parameterization-for-Over-Fitting-Based-Image-Compression" class="headerlink" title="Frequency-Aware Re-Parameterization for Over-Fitting Based Image Compression"></a>Frequency-Aware Re-Parameterization for Over-Fitting Based Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08068">http://arxiv.org/abs/2310.08068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Ye, Yanjie Pan, Qually Jiang, Ming Lu, Xiaoran Fang, Beryl Xu</li>
<li>for: 压缩图像过滤需要图像压缩和实时调整，对于深度卷积神经网 (CNN) 的方法而言，这会带来储存类型和快速调整的挑战。</li>
<li>methods: 这篇 paper 提出了一个简单的重构化方法，用于训练 CNNs 的储存类型和快速调整。卷积核心被重构化为一个权重总和的离散弹道变换 (DCT) 核心，允许直接优化频域中。combined with L1 正规化，提出的方法可以超过普通的卷积，在短时间内 achieve 较好的比特率-调整。</li>
<li>results: 实验结果显示，这篇 paper 的方法可以在不同的数据集上进行压缩图像的过滤，并且可以实现 -46.12% BD-rate 的提升，仅需要 200 迭代。<details>
<summary>Abstract</summary>
Over-fitting-based image compression requires weights compactness for compression and fast convergence for practical use, posing challenges for deep convolutional neural networks (CNNs) based methods. This paper presents a simple re-parameterization method to train CNNs with reduced weights storage and accelerated convergence. The convolution kernels are re-parameterized as a weighted sum of discrete cosine transform (DCT) kernels enabling direct optimization in the frequency domain. Combined with L1 regularization, the proposed method surpasses vanilla convolutions by achieving a significantly improved rate-distortion with low computational cost. The proposed method is verified with extensive experiments of over-fitting-based image restoration on various datasets, achieving up to -46.12% BD-rate on top of HEIF with only 200 iterations.
</details>
<details>
<summary>摘要</summary>
适应过拟合的图像压缩需要权重紧密度 для压缩和快速收敛，对深度卷积神经网络（CNN）基本方法带来挑战。这篇论文提出了一种简单的重parameter化方法，以减少权重存储和加速收敛。核心卷积被重parameterized为一个权重加权的积分幂函数，允许直接优化频率频谱中。与L1正则化结合使用，提议方法在环境成本低下实现了明显提高的比特率-误差率。试验表明，在多种适应过拟合图像修复 task 上，提议方法可以达到最高 -46.12% BD-rate，只需200个迭代。
</details></li>
</ul>
<hr>
<h2 id="Age-Estimation-Based-on-Graph-Convolutional-Networks-and-Multi-head-Attention-Mechanisms"><a href="#Age-Estimation-Based-on-Graph-Convolutional-Networks-and-Multi-head-Attention-Mechanisms" class="headerlink" title="Age Estimation Based on Graph Convolutional Networks and Multi-head Attention Mechanisms"></a>Age Estimation Based on Graph Convolutional Networks and Multi-head Attention Mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08064">http://arxiv.org/abs/2310.08064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaomiao Yang, Changwei Yao, Shijin Yan</li>
<li>For: 本研究开发了一个端正游戏用户识别系统，使用腔边卷网络和多头注意力机制来提高年龄估测的精度。* Methods: 本研究使用了卷网络和多头注意力机制，实现了不 Regular 面部图像特征的抽象和模型，以减少背景信息的影响和提高年龄估测的精度。* Results: 本研究获得了较高的年龄估测精度，MAE错误值降至约3.64，比今天的年龄估测模型更好，从而提高了面部识别和身份验证的精度。<details>
<summary>Abstract</summary>
Age estimation technology is a part of facial recognition and has been applied to identity authentication. This technology achieves the development and application of a juvenile anti-addiction system by authenticating users in the game. Convolutional Neural Network (CNN) and Transformer algorithms are widely used in this application scenario. However, these two models cannot flexibly extract and model features of faces with irregular shapes, and they are ineffective in capturing key information. Furthermore, the above methods will contain a lot of background information while extracting features, which will interfere with the model. In consequence, it is easy to extract redundant information from images. In this paper, a new modeling idea is proposed to solve this problem, which can flexibly model irregular objects. The Graph Convolutional Network (GCN) is used to extract features from irregular face images effectively, and multi-head attention mechanisms are added to avoid redundant features and capture key region information in the image. This model can effectively improve the accuracy of age estimation and reduce the MAE error value to about 3.64, which is better than the effect of today's age estimation model, to improve the accuracy of face recognition and identity authentication.
</details>
<details>
<summary>摘要</summary>
现代年龄估计技术是人脸识别的一部分，已经应用于身份验证。这种技术通过验证用户在游戏中的身份来实现青少年反加ict系统的发展和应用。卷积神经网络（CNN）和变换器算法广泛应用于这个应用场景中。然而，这两种模型无法flexibly提取和模型面呈扁桃形的特征，并且不能 Capture关键信息。此外，上述方法会从图像中提取背景信息，这会干扰模型。因此，容易提取图像中的废弃信息。在本文中，一种新的模型化想法被提出来解决这个问题，即使用图像特征提取GCN网络，并添加多头注意机制以避免废弃特征和Capture图像关键区域信息。这种模型可以有效提高年龄估计的准确性，并将MAE错误值降到约3.64，比现有的年龄估计模型更好。这将有助于提高人脸识别和身份验证的准确性。
</details></li>
</ul>
<hr>
<h2 id="EC-Depth-Exploring-the-consistency-of-self-supervised-monocular-depth-estimation-under-challenging-scenes"><a href="#EC-Depth-Exploring-the-consistency-of-self-supervised-monocular-depth-estimation-under-challenging-scenes" class="headerlink" title="EC-Depth: Exploring the consistency of self-supervised monocular depth estimation under challenging scenes"></a>EC-Depth: Exploring the consistency of self-supervised monocular depth estimation under challenging scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08044">http://arxiv.org/abs/2310.08044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RuijieZhu94/EC-Depth">https://github.com/RuijieZhu94/EC-Depth</a></li>
<li>paper_authors: Ruijie Zhu, Ziyang Song, Chuxin Wang, Jianfeng He, Tianzhu Zhang<br>for:EC-Depth is designed to improve the robustness of self-supervised monocular depth estimation models in real-world applications, where adverse conditions are prevalent.methods:The proposed method utilizes a two-stage training framework with a perturbation-invariant depth consistency constraint module and a consistency-based pseudo-label selection module to achieve accurate and consistent depth predictions.results:EC-Depth surpasses existing state-of-the-art methods on KITTI, KITTI-C, and DrivingStereo benchmarks, demonstrating its effectiveness in challenging scenarios.<details>
<summary>Abstract</summary>
Self-supervised monocular depth estimation holds significant importance in the fields of autonomous driving and robotics. However, existing methods are typically designed to train and test on clear and pristine datasets, overlooking the impact of various adverse conditions prevalent in real-world scenarios. As a result, it is commonly observed that most self-supervised monocular depth estimation methods struggle to perform adequately under challenging conditions. To address this issue, we present EC-Depth, a novel self-supervised two-stage training framework to achieve a robust depth estimation, starting from the foundation of depth prediction consistency under different perturbations. Leveraging the proposed perturbation-invariant depth consistency constraint module and the consistency-based pseudo-label selection module, our model attains accurate and consistent depth predictions in both standard and challenging scenarios. Extensive experiments substantiate the effectiveness of the proposed method. Moreover, our method surpasses existing state-of-the-art methods on KITTI, KITTI-C and DrivingStereo benchmarks, demonstrating its potential for enhancing the reliability of self-supervised monocular depth estimation models in real-world applications.
</details>
<details>
<summary>摘要</summary>
自我监督单目深度估计在自动驾驶和 робо械学中具有重要意义，但现有方法通常是在清晰和完整的数据集上训练和测试，忽视了实际场景中的多种不利条件。因此，大多数自我监督单目深度估计方法在实际场景中表现不佳。为解决这个问题，我们提出了 EC-Depth，一种新的自我监督两 stage 训练框架，以实现robust的深度估计。我们利用了提议的扰动不敏感深度一致性约束模块和一致性基于pseudo标签选择模块，从而使我们的模型在标准和复杂场景中都能够获得准确和一致的深度预测。广泛的实验证明了我们的方法的有效性。此外，我们的方法在 KITTI、KITTI-C 和 DrivingStereo 标准吗chmark上超过了现有状态的艺术方法，这表明了我们的方法在实际应用中提高了自我监督单目深度估计模型的可靠性。
</details></li>
</ul>
<hr>
<h2 id="X-HRNet-Towards-Lightweight-Human-Pose-Estimation-with-Spatially-Unidimensional-Self-Attention"><a href="#X-HRNet-Towards-Lightweight-Human-Pose-Estimation-with-Spatially-Unidimensional-Self-Attention" class="headerlink" title="X-HRNet: Towards Lightweight Human Pose Estimation with Spatially Unidimensional Self-Attention"></a>X-HRNet: Towards Lightweight Human Pose Estimation with Spatially Unidimensional Self-Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08042">http://arxiv.org/abs/2310.08042</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cool-xuan/x-hrnet">https://github.com/cool-xuan/x-hrnet</a></li>
<li>paper_authors: Yixuan Zhou, Xuanhan Wang, Xing Xu, Lei Zhao, Jingkuan Song</li>
<li>for: 提高人 pose 估计精度，降低计算复杂性</li>
<li>methods: 引入空间单 dimensional 自注意力 (SUSA)，取代点 wise (1x1) 卷积</li>
<li>results: 实现高精度人 pose 估计，降低计算复杂性96%，并提供了可重复使用的代码Here’s a breakdown of each sentence:* “for”: 该文章是为了提高人 pose 估计精度和降低计算复杂性。* “methods”: 文章提出了一种新的方法，即引入空间单 dimensional 自注意力 (SUSA)，以取代点 wise (1x1) 卷积。* “results”: 实验结果表明，使用 SUSA 可以实现高精度人 pose 估计，并降低计算复杂性96%。此外，文章还提供了可重复使用的代码。<details>
<summary>Abstract</summary>
High-resolution representation is necessary for human pose estimation to achieve high performance, and the ensuing problem is high computational complexity. In particular, predominant pose estimation methods estimate human joints by 2D single-peak heatmaps. Each 2D heatmap can be horizontally and vertically projected to and reconstructed by a pair of 1D heat vectors. Inspired by this observation, we introduce a lightweight and powerful alternative, Spatially Unidimensional Self-Attention (SUSA), to the pointwise (1x1) convolution that is the main computational bottleneck in the depthwise separable 3c3 convolution. Our SUSA reduces the computational complexity of the pointwise (1x1) convolution by 96% without sacrificing accuracy. Furthermore, we use the SUSA as the main module to build our lightweight pose estimation backbone X-HRNet, where `X' represents the estimated cross-shape attention vectors. Extensive experiments on the COCO benchmark demonstrate the superiority of our X-HRNet, and comprehensive ablation studies show the effectiveness of the SUSA modules. The code is publicly available at https://github.com/cool-xuan/x-hrnet.
</details>
<details>
<summary>摘要</summary>
高分辨率表示是人体姿势估计高性能所需的，但是随之而来的问题是高计算复杂性。特别是，主流的姿势估计方法都是通过2D单峰热图来估计人体关节。每个2D热图可以被水平和垂直投影，并通过一对1D热向量重建。从这个观察中，我们提出了一种轻量级、强大的替代方案——空间单维自注意（SUSA），以减少点 wise（1x1）卷积的计算复杂性。我们的SUSA可以将点 wise（1x1）卷积的计算复杂性减少96%，而不会失去精度。另外，我们使用SUSA作为主模块，建立了我们的轻量级姿势估计后缘X-HRNet，其中`X'表示估计的交叉形注意 vector。EXTENSIVE EXPERIMENTS ON THE COCO BENCHMARK DEMONSTRATE THE SUPERIORITY OF OUR X-HRNet， AND COMPREHENSIVE ABLAATION STUDIES SHOW THE EFFECTIVENESS OF THE SUSA MODULES。代码可以在https://github.com/cool-xuan/x-hrnet中获得。
</details></li>
</ul>
<hr>
<h2 id="Continual-Learning-via-Manifold-Expansion-Replay"><a href="#Continual-Learning-via-Manifold-Expansion-Replay" class="headerlink" title="Continual Learning via Manifold Expansion Replay"></a>Continual Learning via Manifold Expansion Replay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08038">http://arxiv.org/abs/2310.08038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Xu, Xuan Tang, Yufei Shi, Jianfeng Zhang, Jian Yang, Mingsong Chen, Xian Wei</li>
<li>for: 本研究旨在提高连续学习中的稳定性和表达力，透过扩大知识表示的含义槽的几何尺度。</li>
<li>methods: 本研究提出了一种新的播放策略called Manifold Expansion Replay (MaER)，通过在知识缓存中扩大隐式几何的缺失来提高模型的稳定性和表达力。</li>
<li>results: 通过对MNIST、CIFAR10、CIFAR100和TinyImageNet等数据集进行了广泛的实验验证，提出的方法在连续学习设置下显著提高了精度，比对状态前的表现更高。<details>
<summary>Abstract</summary>
In continual learning, the learner learns multiple tasks in sequence, with data being acquired only once for each task. Catastrophic forgetting is a major challenge to continual learning. To reduce forgetting, some existing rehearsal-based methods use episodic memory to replay samples of previous tasks. However, in the process of knowledge integration when learning a new task, this strategy also suffers from catastrophic forgetting due to an imbalance between old and new knowledge. To address this problem, we propose a novel replay strategy called Manifold Expansion Replay (MaER). We argue that expanding the implicit manifold of the knowledge representation in the episodic memory helps to improve the robustness and expressiveness of the model. To this end, we propose a greedy strategy to keep increasing the diameter of the implicit manifold represented by the knowledge in the buffer during memory management. In addition, we introduce Wasserstein distance instead of cross entropy as distillation loss to preserve previous knowledge. With extensive experimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show that the proposed method significantly improves the accuracy in continual learning setup, outperforming the state of the arts.
</details>
<details>
<summary>摘要</summary>
在连续学习中，学习者需要在序列中学习多个任务，并且每个任务只有一次数据采集。然而，这会导致忘记问题，特别是在知识集成过程中学习新任务时。为解决这问题，我们提出了一种新的回忆策略，即扩展隐式抽象的 manifold 扩展回忆（MaER）策略。我们认为，通过扩展知识表示的隐式抽象 manifold 可以提高模型的稳定性和表达力。为此，我们提出了一种满足策略，在内存管理中不断增加知识在缓存中的径距。此外，我们引入 Wasserstein 距离 instead of cross entropy 作为练习损失，以保持之前的知识。经验 validate 在 MNIST、CIFAR10、CIFAR100 和 TinyImageNet 上，我们发现提出的方法可以在连续学习设置中显著提高准确率，超过当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="BaSAL-Size-Balanced-Warm-Start-Active-Learning-for-LiDAR-Semantic-Segmentation"><a href="#BaSAL-Size-Balanced-Warm-Start-Active-Learning-for-LiDAR-Semantic-Segmentation" class="headerlink" title="BaSAL: Size Balanced Warm Start Active Learning for LiDAR Semantic Segmentation"></a>BaSAL: Size Balanced Warm Start Active Learning for LiDAR Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08035">http://arxiv.org/abs/2310.08035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiarong Wei, Yancong Lin, Holger Caesar</li>
<li>for: 降低成本的数据标注，通过重复询问 annotator 标注pool中的无标签数据中最有用的样本，并将其用于重新训练模型。</li>
<li>methods: 使用size-balanced warm start active learning模型，根据对象类别的特征大小进行对象群集 sampling，以创建更加均衡的数据集。</li>
<li>results: 能够大幅提高初始模型的性能，并且与使用整个SemanticKITTI dataset进行训练相当，使用只有5%的标注数据，而且与现有的活动学习方法相当。<details>
<summary>Abstract</summary>
Active learning strives to reduce the need for costly data annotation, by repeatedly querying an annotator to label the most informative samples from a pool of unlabeled data and retraining a model from these samples. We identify two problems with existing active learning methods for LiDAR semantic segmentation. First, they ignore the severe class imbalance inherent in LiDAR semantic segmentation datasets. Second, to bootstrap the active learning loop, they train their initial model from randomly selected data samples, which leads to low performance and is referred to as the cold start problem. To address these problems we propose BaSAL, a size-balanced warm start active learning model, based on the observation that each object class has a characteristic size. By sampling object clusters according to their size, we can thus create a size-balanced dataset that is also more class-balanced. Furthermore, in contrast to existing information measures like entropy or CoreSet, size-based sampling does not require an already trained model and thus can be used to address the cold start problem. Results show that we are able to improve the performance of the initial model by a large margin. Combining size-balanced sampling and warm start with established information measures, our approach achieves a comparable performance to training on the entire SemanticKITTI dataset, despite using only 5% of the annotations, which outperforms existing active learning methods. We also match the existing state-of-the-art in active learning on nuScenes. Our code will be made available upon paper acceptance.
</details>
<details>
<summary>摘要</summary>
aktive learning实践旨在减少成本的标注资料，通过重复询问标注者 labelpool中的不标注资料中的最有用样本，并从这些样本中重训模型。我们发现了现有的 aktive learning方法对于LiDAR semantic segmentation有两个问题。首先，它们忽略了LiDAR semantic segmentationdataset中的严重类别不均衡。其次，为了启动活动学习循环，它们从Random选择的资料样本中训练初始模型，这个问题被称为冷启动问题。为了解决这些问题，我们提出了Basal，一个size-balanced warm start aktive learning模型，基于每个物类的特征大小。通过根据物类的大小排序物类对，我们可以创建一个size-balanceddataset，并且更好地对类别进行均衡。此外，不同于现有的信息度量like entropy或CoreSet，size-based sampling不需要已经训练的模型，因此可以用来解决冷启动问题。我们的结果显示，我们能够从初始模型中大幅提高性能。通过结合size-balanced sampling和暖启动，我们的方法可以与使用整个SemanticKITTI dataset的性能相匹配，即使只使用5%的标注资料，而且超越现有的aktive learning方法。我们还与nuScenes中的active learning方法匹配。我们将代码公开发布一并发表论文。
</details></li>
</ul>
<hr>
<h2 id="Dual-Stream-Knowledge-Preserving-Hashing-for-Unsupervised-Video-Retrieval"><a href="#Dual-Stream-Knowledge-Preserving-Hashing-for-Unsupervised-Video-Retrieval" class="headerlink" title="Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video Retrieval"></a>Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08009">http://arxiv.org/abs/2310.08009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IMCCretrieval/DKPH">https://github.com/IMCCretrieval/DKPH</a></li>
<li>paper_authors: Pandeng Li, Hongtao Xie, Jiannan Ge, Lei Zhang, Shaobo Min, Yongdong Zhang</li>
<li>for: 本研究旨在提高无监督视频哈希的性能，通过分解视频信息为重建依赖的信息和Semantic依赖的信息，从而隔离 semantic extraction 从重建约束。</li>
<li>methods: 我们采用了一种简单的 dual-stream 结构，包括一个时间层和一个哈希层。在这种结构中，哈希层通过自我监督获得的含义相似知识，学习捕捉 binary codes 中的 semantics，而时间层则学习重建视频信息。</li>
<li>results: 我们的方法在三个视频benchmark上进行了广泛的实验 validate，与之前的状态场景比较，我们的方法一直表现出优于其他方法。<details>
<summary>Abstract</summary>
Unsupervised video hashing usually optimizes binary codes by learning to reconstruct input videos. Such reconstruction constraint spends much effort on frame-level temporal context changes without focusing on video-level global semantics that are more useful for retrieval. Hence, we address this problem by decomposing video information into reconstruction-dependent and semantic-dependent information, which disentangles the semantic extraction from reconstruction constraint. Specifically, we first design a simple dual-stream structure, including a temporal layer and a hash layer. Then, with the help of semantic similarity knowledge obtained from self-supervision, the hash layer learns to capture information for semantic retrieval, while the temporal layer learns to capture the information for reconstruction. In this way, the model naturally preserves the disentangled semantics into binary codes. Validated by comprehensive experiments, our method consistently outperforms the state-of-the-arts on three video benchmarks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translates the given text into Simplified Chinese.Unsupervised video hashing usually optimizes binary codes by learning to reconstruct input videos. Such reconstruction constraint spends much effort on frame-level temporal context changes without focusing on video-level global semantics that are more useful for retrieval. Hence, we address this problem by decomposing video information into reconstruction-dependent and semantic-dependent information, which disentangles the semantic extraction from reconstruction constraint. Specifically, we first design a simple dual-stream structure, including a temporal layer and a hash layer. Then, with the help of semantic similarity knowledge obtained from self-supervision, the hash layer learns to capture information for semantic retrieval, while the temporal layer learns to capture the information for reconstruction. In this way, the model naturally preserves the disentangled semantics into binary codes. Validated by comprehensive experiments, our method consistently outperforms the state-of-the-arts on three video benchmarks.中文翻译：通常情况下，无监督视频哈希优化二进制代码通过学习重建输入视频的方式进行优化。这种重建约束会占用帧级时间上下文变化的大量精力，而不是关注视频级全局 semantics 更有用于检索。因此，我们解决这个问题，通过分解视频信息为重建依赖的信息和semantic依赖的信息来分离semantic抽取。具体来说，我们首先设计了一个简单的双流结构，包括一个时间层和一个哈希层。然后，通过自我监督获得的semantic相似性知识，哈希层学习捕捉Semantic检索中的信息，而时间层学习捕捉重建中的信息。这样，模型会自然地储存分离的semantics到二进制代码中。经过了广泛的实验 validate，我们的方法在三个视频 benchmark 上 consistently 超越了状态的艺术。
</details></li>
</ul>
<hr>
<h2 id="MLP-AMDC-An-MLP-Architecture-for-Adaptive-Mask-based-Dual-Camera-snapshot-hyperspectral-imaging"><a href="#MLP-AMDC-An-MLP-Architecture-for-Adaptive-Mask-based-Dual-Camera-snapshot-hyperspectral-imaging" class="headerlink" title="MLP-AMDC: An MLP Architecture for Adaptive-Mask-based Dual-Camera snapshot hyperspectral imaging"></a>MLP-AMDC: An MLP Architecture for Adaptive-Mask-based Dual-Camera snapshot hyperspectral imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08002">http://arxiv.org/abs/2310.08002</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caizeyu1992/MLP-AMDC">https://github.com/caizeyu1992/MLP-AMDC</a></li>
<li>paper_authors: Zeyu Cai, Can Zhang, Xunhao Chen, Shanghuan Liu, Chengqian Jin, Feipeng Da</li>
<li>for:  This paper aims to improve the performance and speed of Coded Aperture Snapshot Spectral Imaging (CASSI) systems, which are used to acquire Hyper-Spectral Images (HSI).</li>
<li>methods:  The paper proposes an AMDC-CASSI system that uses an RGB camera with CASSI and Adaptive-Mask to improve the reconstruction quality of HSI. The proposed method replaces the transformer structure of the network with an MLP architecture to improve the inference speed of the reconstruction network.</li>
<li>results:  The paper shows that the proposed MLP-AMDC method achieves an 8 dB improvement over the state-of-the-art (SOTA) and at least a 5-fold improvement in reconstruction speed, while maintaining competitive reconstruction quality.<details>
<summary>Abstract</summary>
Coded Aperture Snapshot Spectral Imaging (CASSI) system has great advantages over traditional methods in dynamically acquiring Hyper-Spectral Image (HSI), but there are the following problems. 1) Traditional mask relies on random patterns or analytical design, both of which limit the performance improvement of CASSI. 2) Existing high-quality reconstruction algorithms are slow in reconstruction and can only reconstruct scene information offline. To address the above two problems, this paper designs the AMDC-CASSI system, introducing RGB camera with CASSI based on Adaptive-Mask as multimodal input to improve the reconstruction quality. The existing SOTA reconstruction schemes are based on transformer, but the operation of self-attention pulls down the operation efficiency of the network. In order to improve the inference speed of the reconstruction network, this paper proposes An MLP Architecture for Adaptive-Mask-based Dual-Camera (MLP-AMDC) to replace the transformer structure of the network. Numerous experiments have shown that MLP performs no less well than transformer-based structures for HSI reconstruction, while MLP greatly improves the network inference speed and has less number of parameters and operations, our method has a 8 db improvement over SOTA and at least a 5-fold improvement in reconstruction speed. (https://github.com/caizeyu1992/MLP-AMDC.)
</details>
<details>
<summary>摘要</summary>
CASSI（coded aperture snapshot spectral imaging）系统在获取高spectral resolution的图像方面有优势，但存在以下问题：1）传统的面Mask rely on random patterns或分析设计，两者都限制了CASSI的性能提升。2）现有的高质量重建算法慢于重建和只能在离线重建场景信息。为了解决上述两个问题，本文提出了RGB camera与CASSI基于Adaptive-Mask的多模态输入，以提高重建质量。现有的SOTA重建方案基于transformer，但自我注意operation pulls down网络的运算效率。为了提高重建网络的吞吐量，本文提议使用An MLP Architecture for Adaptive-Mask-based Dual-Camera（MLP-AMDC）来替换网络的transformer结构。多个实验表明，MLP与transformer-based结构相当，而MLP可以大幅提高网络的吞吐量和参数数量，我们的方法与SOTA差距8db，并至少提高5倍的重建速度。（https://github.com/caizeyu1992/MLP-AMDC。）
</details></li>
</ul>
<hr>
<h2 id="Reset-It-and-Forget-It-Relearning-Last-Layer-Weights-Improves-Continual-and-Transfer-Learning"><a href="#Reset-It-and-Forget-It-Relearning-Last-Layer-Weights-Improves-Continual-and-Transfer-Learning" class="headerlink" title="Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning"></a>Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07996">http://arxiv.org/abs/2310.07996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lapo Frati, Neil Traft, Jeff Clune, Nick Cheney</li>
<li>for: 这个论文旨在提出一种简单的预训练机制，以便 representations 能够更好地进行 continual 学习和转移学习。</li>
<li>methods: 这个机制是在最后一层的权重重新设置，我们昵称其为 “zapping”。这种机制原本是为 meta-continual-learning 过程设计的，但我们表明它可以在许多其他场景中应用。</li>
<li>results: 在我们的实验中，我们想要将预训练的图像分类器转移到新的类别中，并在几个极少的试验中达到了更高的转移精度和&#x2F;或更快的适应速度，而无需使用昂贵的高阶导数。这种 zapping 机制可以考虑为 computationally 更便宜的、或者是 meta-learning 快速适应特征的代替方案。<details>
<summary>Abstract</summary>
This work identifies a simple pre-training mechanism that leads to representations exhibiting better continual and transfer learning. This mechanism -- the repeated resetting of weights in the last layer, which we nickname "zapping" -- was originally designed for a meta-continual-learning procedure, yet we show it is surprisingly applicable in many settings beyond both meta-learning and continual learning. In our experiments, we wish to transfer a pre-trained image classifier to a new set of classes, in a few shots. We show that our zapping procedure results in improved transfer accuracy and/or more rapid adaptation in both standard fine-tuning and continual learning settings, while being simple to implement and computationally efficient. In many cases, we achieve performance on par with state of the art meta-learning without needing the expensive higher-order gradients, by using a combination of zapping and sequential learning. An intuitive explanation for the effectiveness of this zapping procedure is that representations trained with repeated zapping learn features that are capable of rapidly adapting to newly initialized classifiers. Such an approach may be considered a computationally cheaper type of, or alternative to, meta-learning rapidly adaptable features with higher-order gradients. This adds to recent work on the usefulness of resetting neural network parameters during training, and invites further investigation of this mechanism.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CleftGAN-Adapting-A-Style-Based-Generative-Adversarial-Network-To-Create-Images-Depicting-Cleft-Lip-Deformity"><a href="#CleftGAN-Adapting-A-Style-Based-Generative-Adversarial-Network-To-Create-Images-Depicting-Cleft-Lip-Deformity" class="headerlink" title="CleftGAN: Adapting A Style-Based Generative Adversarial Network To Create Images Depicting Cleft Lip Deformity"></a>CleftGAN: Adapting A Style-Based Generative Adversarial Network To Create Images Depicting Cleft Lip Deformity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07969">http://arxiv.org/abs/2310.07969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdullah Hayajneh, Erchin Serpedin, Mohammad Shaqfeh, Graeme Glass, Mitchell A. Stotland</li>
<li>for: This paper aims to address the challenge of training a machine learning system to evaluate facial clefts by generating a large dataset of high-quality, ethics board-approved patient images using a deep learning-based cleft lip generator.</li>
<li>methods: The authors use a transfer learning protocol with a deep learning-based generative adversarial network image generator incorporating adaptive data augmentation (ADA) to generate a large dataset of artificial images exhibiting high-fidelity facsimiles of cleft lip with wide variation.</li>
<li>results: The authors found that StyleGAN3 with translation invariance (StyleGAN3-t) performed optimally as a base model, and the generated images achieved a low Frechet Inception Distance (FID) reflecting a close similarity to the training input dataset of genuine cleft images. The PPL and DISH measures also showed a smooth and semantically valid interpolation of images through the transfer learning process, and a similar distribution of severity in the training and generated images.<details>
<summary>Abstract</summary>
A major obstacle when attempting to train a machine learning system to evaluate facial clefts is the scarcity of large datasets of high-quality, ethics board-approved patient images. In response, we have built a deep learning-based cleft lip generator designed to produce an almost unlimited number of artificial images exhibiting high-fidelity facsimiles of cleft lip with wide variation. We undertook a transfer learning protocol testing different versions of StyleGAN-ADA (a generative adversarial network image generator incorporating adaptive data augmentation (ADA)) as the base model. Training images depicting a variety of cleft deformities were pre-processed to adjust for rotation, scaling, color adjustment and background blurring. The ADA modification of the primary algorithm permitted construction of our new generative model while requiring input of a relatively small number of training images. Adversarial training was carried out using 514 unique frontal photographs of cleft-affected faces to adapt a pre-trained model based on 70,000 normal faces. The Frechet Inception Distance (FID) was used to measure the similarity of the newly generated facial images to the cleft training dataset, while Perceptual Path Length (PPL) and the novel Divergence Index of Severity Histograms (DISH) measures were also used to assess the performance of the image generator that we dub CleftGAN. We found that StyleGAN3 with translation invariance (StyleGAN3-t) performed optimally as a base model. Generated images achieved a low FID reflecting a close similarity to our training input dataset of genuine cleft images. Low PPL and DISH measures reflected a smooth and semantically valid interpolation of images through the transfer learning process and a similar distribution of severity in the training and generated images, respectively.
</details>
<details>
<summary>摘要</summary>
很多时候，在尝试使机器学习系统评估面部缺陷时，面临着大量高质量、伦理委员会批准的患者图像的缺乏问题。为此，我们构建了一个基于深度学习的面部缺陷生成器，可以生成具有广泛变化的人工图像，以便模拟面部缺陷的多种形式。我们采用了一种转移学习协议，测试不同版本的StyleGAN-ADA（一种基于生成 adversarial network的图像生成器，其中ADA表示适应性数据增强）作为基本模型。我们使用了不同的扭转、缩放、颜色调整和背景模糊等方法来预处理训练图像，以适应不同的缺陷形式。ADA修改后的主要算法允许我们建立我们新的生成模型，只需输入相对较少的训练图像。我们使用了514个特定的rontal相机拍摄了缺陷面部图像来适应一个预训练模型，基于70000个正常面部图像。我们使用了Frechet Inception Distance（FID）、Perceptual Path Length（PPL）和 novel Divergence Index of Severity Histograms（DISH）等方法来评估我们所建立的图像生成器，我们称之为CleftGAN。我们发现，StyleGAN3 with translation invariance（StyleGAN3-t）在基本模型中表现最佳。生成的图像得到了低的FID，表示它们与我们的训练输入图像的真实缺陷图像很相似。PPL和DISH值均较低，表示通过转移学习过程中的满意 interpolate 和生成图像的分布相似。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/12/cs.CV_2023_10_12/" data-id="clorjzl7800k2f188akwncywq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/12/cs.AI_2023_10_12/" class="article-date">
  <time datetime="2023-10-12T12:00:00.000Z" itemprop="datePublished">2023-10-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/12/cs.AI_2023_10_12/">cs.AI - 2023-10-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Examining-the-Potential-and-Pitfalls-of-ChatGPT-in-Science-and-Engineering-Problem-Solving"><a href="#Examining-the-Potential-and-Pitfalls-of-ChatGPT-in-Science-and-Engineering-Problem-Solving" class="headerlink" title="Examining the Potential and Pitfalls of ChatGPT in Science and Engineering Problem-Solving"></a>Examining the Potential and Pitfalls of ChatGPT in Science and Engineering Problem-Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08773">http://arxiv.org/abs/2310.08773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karen D. Wang, Eric Burkholder, Carl Wieman, Shima Salehi, Nick Haber</li>
<li>for: 本研究探讨OpenAI的ChatGPT在解决不同类型物理问题的能力。</li>
<li>methods: 本研究使用ChatGPT（GPT-4）解决了一共40个大学物理课程中的问题，这些问题包括具有完整数据的准确问题以及缺乏数据的实际问题。</li>
<li>results: 研究发现ChatGPT可以成功解决62.5%的准确问题，但对于缺乏数据的问题，准确率只有8.3%。分析模型的错误解决方法发现有三种失败模式：1）建立不准确的物理世界模型，2）缺乏数据的假设，3）计算错误。<details>
<summary>Abstract</summary>
The study explores the capabilities of OpenAI's ChatGPT in solving different types of physics problems. ChatGPT (with GPT-4) was queried to solve a total of 40 problems from a college-level engineering physics course. These problems ranged from well-specified problems, where all data required for solving the problem was provided, to under-specified, real-world problems where not all necessary data were given. Our findings show that ChatGPT could successfully solve 62.5% of the well-specified problems, but its accuracy drops to 8.3% for under-specified problems. Analysis of the model's incorrect solutions revealed three distinct failure modes: 1) failure to construct accurate models of the physical world, 2) failure to make reasonable assumptions about missing data, and 3) calculation errors. The study offers implications for how to leverage LLM-augmented instructional materials to enhance STEM education. The insights also contribute to the broader discourse on AI's strengths and limitations, serving both educators aiming to leverage the technology and researchers investigating human-AI collaboration frameworks for problem-solving and decision-making.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>失败 construct accurate models of the physical world2. 失败 make reasonable assumptions about missing data3. calculation errorsThe study offers implications for how to leverage LLM-augmented instructional materials to enhance STEM education. The insights also contribute to the broader discourse on AI’s strengths and limitations, serving both educators aiming to leverage the technology and researchers investigating human-AI collaboration frameworks for problem-solving and decision-making.</details></li>
</ol>
<hr>
<h2 id="Stabilizing-Subject-Transfer-in-EEG-Classification-with-Divergence-Estimation"><a href="#Stabilizing-Subject-Transfer-in-EEG-Classification-with-Divergence-Estimation" class="headerlink" title="Stabilizing Subject Transfer in EEG Classification with Divergence Estimation"></a>Stabilizing Subject Transfer in EEG Classification with Divergence Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08762">http://arxiv.org/abs/2310.08762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niklas Smedemark-Margulies, Ye Wang, Toshiaki Koike-Akino, Jing Liu, Kieran Parsons, Yunus Bicer, Deniz Erdogmus</li>
<li>for: 这篇论文的目的是提高电enzephalogram（EEG）数据的分类模型性能。</li>
<li>methods: 作者使用了新的调整技术来减少分类模型在未见到的测试主题上的性能下降。他们提出了几个图形模型来描述EEG分类任务，并从每个模型中提取了一些关于理想训练enario中的统计关系。他们设计了一些调整 penalty来保持这些关系在实际训练中。</li>
<li>results: 作者的提案方法可以对EEG数据进行分类，并且可以增加测试主题上的均衡精度和减少过滤。这些方法在不同的参数下展现出更大的优化效果，并且仅对训练时间进行小量的computational cost。<details>
<summary>Abstract</summary>
Classification models for electroencephalogram (EEG) data show a large decrease in performance when evaluated on unseen test sub jects. We reduce this performance decrease using new regularization techniques during model training. We propose several graphical models to describe an EEG classification task. From each model, we identify statistical relationships that should hold true in an idealized training scenario (with infinite data and a globally-optimal model) but that may not hold in practice. We design regularization penalties to enforce these relationships in two stages. First, we identify suitable proxy quantities (divergences such as Mutual Information and Wasserstein-1) that can be used to measure statistical independence and dependence relationships. Second, we provide algorithms to efficiently estimate these quantities during training using secondary neural network models. We conduct extensive computational experiments using a large benchmark EEG dataset, comparing our proposed techniques with a baseline method that uses an adversarial classifier. We find our proposed methods significantly increase balanced accuracy on test subjects and decrease overfitting. The proposed methods exhibit a larger benefit over a greater range of hyperparameters than the baseline method, with only a small computational cost at training time. These benefits are largest when used for a fixed training period, though there is still a significant benefit for a subset of hyperparameters when our techniques are used in conjunction with early stopping regularization.
</details>
<details>
<summary>摘要</summary>
“电击脑波（EEG）标本分类模型表现出现大量的减少性能，当被评估在未见到的测试主题时。我们使用新的调整技术来减少这种性能下降。我们提出了一些图形模型来描述EEG标本分类任务。从每个模型中，我们识别出理想情况下（即无穷数据和全球最佳模型）不会出现的统计关系。我们设计了调整罚则来强制这些关系在两个阶段中。首先，我们选择适合的代理量（如共识信息和沃瑟敏-1）来量度弹性和依赖关系。其次，我们提供了高效的训练 Algorithm 来计算这些量。我们使用大量的benchmark EEG数据进行了广泛的计算实验，比较我们的提议方法与基eline方法（使用对抗网络）。我们发现，我们的提议方法在测试主题上具有更高的平衡率和更低的过滤。我们的提议方法在多个参数的范围中表现出更大的优势，仅仅需要在训练过程中进行小量的计算成本。这些优势在固定训练时间下最大化，但是还存在一些参数的子集中，使用我们的技术和早期停止调整时仍然具有重要的优势。”
</details></li>
</ul>
<hr>
<h2 id="CompA-Addressing-the-Gap-in-Compositional-Reasoning-in-Audio-Language-Models"><a href="#CompA-Addressing-the-Gap-in-Compositional-Reasoning-in-Audio-Language-Models" class="headerlink" title="CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models"></a>CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08753">http://arxiv.org/abs/2310.08753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sreyan Ghosh, Ashish Seth, Sonal Kumar, Utkarsh Tyagi, Chandra Kiran Evuru, S. Ramaneswaran, S. Sakshi, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha</li>
<li>for: The paper is written to explore the ability of audio-language models (ALMs) to perform compositional reasoning, and to propose a new benchmark (CompA) to evaluate this ability.</li>
<li>methods: The paper uses a contrastive approach (e.g., CLAP) to train the ALMs, and proposes a novel learning method to improve the model’s compositional reasoning abilities. The method includes improvements to contrastive training with composition-aware hard negatives, and a novel modular contrastive loss.</li>
<li>results: The paper shows that current ALMs perform only marginally better than random chance on the CompA benchmark, and proposes a new model (CompA-CLAP) that significantly improves over all baseline models on the benchmark. The results indicate that the proposed method has superior compositional reasoning capabilities.Here’s the Chinese translation of the three key information points:</li>
<li>for: 这篇论文是为了探究语音语言模型（ALM）的 композиitional 理解能力而写的，并提出了一个新的 benchmark（CompA）来评估这种能力。</li>
<li>methods: 这篇论文使用了对比方法（e.g., CLAP）来训练 ALM，并提出了一种新的学习方法来提高模型的compositional 理解能力。这种方法包括对比训练中的组合感知强制性进行改进，以及一种新的模块对比损失。</li>
<li>results: 这篇论文显示了现有的 ALM 只能marginally  better than随机的概率来 Perform 在 CompA  bencmark 上，并提出了一种新的模型（CompA-CLAP）来解决这个问题。这种模型在 CompA  bencmark 上显示出了明显的改进， indicating 它的 compositional 理解能力更强。<details>
<summary>Abstract</summary>
A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. An ALM is evaluated on how well it matches the right audio to the right caption. Using this benchmark, we first show that current ALMs perform only marginally better than random chance, thereby struggling with compositional reasoning. Next, we propose CompA-CLAP, where we fine-tune CLAP using a novel learning method to improve its compositional reasoning abilities. To train CompA-CLAP, we first propose improvements to contrastive training with composition-aware hard negatives, allowing for more focused training. Next, we propose a novel modular contrastive loss that helps the model learn fine-grained compositional understanding and overcomes the acute scarcity of openly available compositional audios. CompA-CLAP significantly improves over all our baseline models on the CompA benchmark, indicating its superior compositional reasoning capabilities.
</details>
<details>
<summary>摘要</summary>
音频的基本特点之一是其 Compositional nature。使用对比方法（例如CLAP）训练的音频语言模型（ALM）在许多下游应用程序中表现得更好，包括零shot音频分类、音频检索等。然而，这些模型对于实际进行compositional reasoning的能力尚未得到足够的探索，需要进一步的研究。在这篇论文中，我们提出了CompA，一个由专家标注的benchmark集合，用于评估ALM的compositional reasoning能力。我们的CompA-order评估了ALM是否能够正确地理解音频中的事件顺序或发生频度，而CompA-attribute评估了事件绑定的能力。每个benchmark实例都包括两对音频-标签对，其中两个音频具有相同的听觉事件，但具有不同的组合。ALM被评估是否能够匹配正确的音频和标签。使用这个benchmark，我们首先发现现有ALM的表现只是marginally better than random chance，因此它们在compositional reasoning方面几乎没有表现出来。然后，我们提出了CompA-CLAP，其中我们使用一种新的学习方法来改进CLAP的compositional reasoning能力。为了训练CompA-CLAP，我们首先提出了对比训练中的组合感知强制对手，以便更加专注的训练。然后，我们提出了一种新的模块化对比损失，帮助模型学习细致的compositional理解，并且解决了公开available的compositional音频的缺乏问题。CompA-CLAP在CompA benchmark上显著超越了所有基线模型， indicating its superior compositional reasoning capabilities.
</details></li>
</ul>
<hr>
<h2 id="Constrained-Bayesian-Optimization-with-Adaptive-Active-Learning-of-Unknown-Constraints"><a href="#Constrained-Bayesian-Optimization-with-Adaptive-Active-Learning-of-Unknown-Constraints" class="headerlink" title="Constrained Bayesian Optimization with Adaptive Active Learning of Unknown Constraints"></a>Constrained Bayesian Optimization with Adaptive Active Learning of Unknown Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08751">http://arxiv.org/abs/2310.08751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengxue Zhang, Zejie Zhu, Yuxin Chen</li>
<li>for: 这 paper 是关于 constrained Bayesian optimization (CBO) 的研究，用于处理具有黑盒函数目标和约束的复杂应用场景。</li>
<li>methods: 该 paper 提出了一种基于 ROI 的 CBO 框架，利用了目标和约束可以帮助确定高可信区域的想法。</li>
<li>results: 该 paper 提供了一种有理 теорем 的 CBO 框架，并通过实验证明了其效率和稳定性。 In English:</li>
<li>for: This paper is about research on constrained Bayesian optimization (CBO) for handling complex application scenarios with black-box objective and constraint functions.</li>
<li>methods: The paper proposes an CBO framework based on the idea of identifying high-confidence regions of interest (ROI) using both the objective and constraint functions.</li>
<li>results: The paper provides a theoretically grounded CBO framework and demonstrates its efficiency and robustness through empirical evidence.<details>
<summary>Abstract</summary>
Optimizing objectives under constraints, where both the objectives and constraints are black box functions, is a common scenario in real-world applications such as scientific experimental design, design of medical therapies, and industrial process optimization. One popular approach to handling these complex scenarios is Bayesian Optimization (BO). In terms of theoretical behavior, BO is relatively well understood in the unconstrained setting, where its principles have been well explored and validated. However, when it comes to constrained Bayesian optimization (CBO), the existing framework often relies on heuristics or approximations without the same level of theoretical guarantees.   In this paper, we delve into the theoretical and practical aspects of constrained Bayesian optimization, where the objective and constraints can be independently evaluated and are subject to noise. By recognizing that both the objective and constraints can help identify high-confidence regions of interest (ROI), we propose an efficient CBO framework that intersects the ROIs identified from each aspect to determine the general ROI. The ROI, coupled with a novel acquisition function that adaptively balances the optimization of the objective and the identification of feasible regions, enables us to derive rigorous theoretical justifications for its performance. We showcase the efficiency and robustness of our proposed CBO framework through empirical evidence and discuss the fundamental challenge of deriving practical regret bounds for CBO algorithms.
</details>
<details>
<summary>摘要</summary>
In this paper, we delve into the theoretical and practical aspects of constrained Bayesian optimization, where the objective and constraints can be independently evaluated and are subject to noise. By recognizing that both the objective and constraints can help identify high-confidence regions of interest (ROI), we propose an efficient CBO framework that intersects the ROIs identified from each aspect to determine the general ROI. The ROI, coupled with a novel acquisition function that adaptively balances the optimization of the objective and the identification of feasible regions, enables us to derive rigorous theoretical justifications for its performance. We showcase the efficiency and robustness of our proposed CBO framework through empirical evidence and discuss the fundamental challenge of deriving practical regret bounds for CBO algorithms.
</details></li>
</ul>
<hr>
<h2 id="Development-and-Validation-of-a-Deep-Learning-Based-Microsatellite-Instability-Predictor-from-Prostate-Cancer-Whole-Slide-Images"><a href="#Development-and-Validation-of-a-Deep-Learning-Based-Microsatellite-Instability-Predictor-from-Prostate-Cancer-Whole-Slide-Images" class="headerlink" title="Development and Validation of a Deep Learning-Based Microsatellite Instability Predictor from Prostate Cancer Whole-Slide Images"></a>Development and Validation of a Deep Learning-Based Microsatellite Instability Predictor from Prostate Cancer Whole-Slide Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08743">http://arxiv.org/abs/2310.08743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiyuan Hu, Abbas A. Rizvi, Geoffery Schau, Kshitij Ingale, Yoni Muller, Rachel Baits, Sebastian Pretzer, Aïcha BenTaieb, Abigail Gordhamer, Roberto Nussenzveig, Adam Cole, Matthew O. Leavitt, Rohan P. Joshi, Nike Beaubier, Martin C. Stumpe, Kunal Nagpal</li>
<li>for: 这个研究的目的是为了开发一个基于人工智能的肉眼染色图像（H&amp;E）的微isatellite不稳定（MSI）诊断模型，以便将这些模型应用到肝癌患者身上，以促进免疫抑制剂治疗的适应率。</li>
<li>methods: 这个研究使用了一种名为“注意力型多个例学习（Multiple Instance Learning，MIL）”的人工智能模型，并使用了4015名肝癌患者的肝癌标本，其中173名患者的标本是在实验室内进行了染色和扫描。这个模型使用了一个叫做“注意力”的技术，将标本中的细胞扫描到图像中，以便更好地识别细胞的特征。</li>
<li>results: 这个研究发现了一个新的AI-based MSI诊断模型，可以从H&amp;E标本中预测肝癌患者是否有高度微isatellite不稳定（MSI-H）。这个模型在3个不同的验证集中都达到了高度的准确率，分别为0.78、0.72和0.72。此外，这个模型还发现了与 gleason 分子数值相关的MSI-H诊断。<details>
<summary>Abstract</summary>
Microsatellite instability-high (MSI-H) is a tumor agnostic biomarker for immune checkpoint inhibitor therapy. However, MSI status is not routinely tested in prostate cancer, in part due to low prevalence and assay cost. As such, prediction of MSI status from hematoxylin and eosin (H&E) stained whole-slide images (WSIs) could identify prostate cancer patients most likely to benefit from confirmatory testing and becoming eligible for immunotherapy. Prostate biopsies and surgical resections from de-identified records of consecutive prostate cancer patients referred to our institution were analyzed. Their MSI status was determined by next generation sequencing. Patients before a cutoff date were split into an algorithm development set (n=4015, MSI-H 1.8%) and a paired validation set (n=173, MSI-H 19.7%) that consisted of two serial sections from each sample, one stained and scanned internally and the other at an external site. Patients after the cutoff date formed the temporal validation set (n=1350, MSI-H 2.3%). Attention-based multiple instance learning models were trained to predict MSI-H from H&E WSIs. The MSI-H predictor achieved area under the receiver operating characteristic curve values of 0.78 (95% CI [0.69-0.86]), 0.72 (95% CI [0.63-0.81]), and 0.72 (95% CI [0.62-0.82]) on the internally prepared, externally prepared, and temporal validation sets, respectively. While MSI-H status is significantly correlated with Gleason score, the model remained predictive within each Gleason score subgroup. In summary, we developed and validated an AI-based MSI-H diagnostic model on a large real-world cohort of routine H&E slides, which effectively generalized to externally stained and scanned samples and a temporally independent validation cohort. This algorithm has the potential to direct prostate cancer patients toward immunotherapy and to identify MSI-H cases secondary to Lynch syndrome.
</details>
<details>
<summary>摘要</summary>
微卫星稳定性高 (MSI-H) 是一种肿瘤不吝啬的生物标志物，可以用于免疫检查点剂治疗。然而，MSI 状态在前列腺癌中并不是常见的测试项，一些原因是诊断成本高和预测率低。因此，可以通过从 Hematoxylin 和 Eosin (H&E) 染色整个扫描图像 (WSIs) 预测 prostate cancer 患者可能会从 confirmatory testing 中受益，并成为免疫治疗的 кандидат。我们分析了来自 consecutive prostate cancer 患者的杯尿和手术摘取记录，并确定了他们的 MSI 状态通过次世代测序。在割Date 之前，患者被分为了一个算法开发集 (n=4015, MSI-H 1.8%) 和一个验证集 (n=173, MSI-H 19.7%)，其中每个样本都有两个并行的 serial section，一个在内部染色和扫描，另一个在外部Site 染色。割Date 之后的患者组成了 temporal 验证集 (n=1350, MSI-H 2.3%)。我们使用了注意力基本多实例学习模型来预测 MSI-H 从 H&E WSIs。预测模型在 internally prepared、 externally prepared 和 temporal 验证集上的 area under the receiver operating characteristic curve 值分别为 0.78 (95% CI [0.69-0.86]), 0.72 (95% CI [0.63-0.81]), 0.72 (95% CI [0.62-0.82])。尽管 MSI-H 状态与 gleason 分型显著相关，但模型在每个 gleason 分型 subgroup 中保持预测性。综上所述，我们开发了一种基于 AI 的 MSI-H 诊断模型，并在大量实际患者数据上验证了其效果。这种算法有可能导引 prostate cancer 患者进行免疫治疗，并确定 MSI-H  случа例是否与 Lynch 综合征相关。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Event-Detection-with-Random-Forests-and-Temporal-Convolutional-Networks-for-More-Sustainable-Petroleum-Industry"><a href="#Real-Time-Event-Detection-with-Random-Forests-and-Temporal-Convolutional-Networks-for-More-Sustainable-Petroleum-Industry" class="headerlink" title="Real-Time Event Detection with Random Forests and Temporal Convolutional Networks for More Sustainable Petroleum Industry"></a>Real-Time Event Detection with Random Forests and Temporal Convolutional Networks for More Sustainable Petroleum Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08737">http://arxiv.org/abs/2310.08737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanwei Qu, Baifan Zhou, Arild Waaler, David Cameron</li>
<li>for: 本研究旨在提供更有效的生产过程中不愿意事件探测方法，以避免环境和经济损害。</li>
<li>methods: 本研究使用机器学习方法，包括Random Forest和时间卷积网络，实时探测不愿意事件。</li>
<li>results: 研究结果表明，我们的方法可以有效地类型化事件并预测事件出现概率，从而解决过去研究中存在的挑战，并为生产过程中的事件管理提供更有效的解决方案。<details>
<summary>Abstract</summary>
The petroleum industry is crucial for modern society, but the production process is complex and risky. During the production, accidents or failures, resulting from undesired production events, can cause severe environmental and economic damage. Previous studies have investigated machine learning (ML) methods for undesired event detection. However, the prediction of event probability in real-time was insufficiently addressed, which is essential since it is important to undertake early intervention when an event is expected to happen. This paper proposes two ML approaches, random forests and temporal convolutional networks, to detect undesired events in real-time. Results show that our approaches can effectively classify event types and predict the probability of their appearance, addressing the challenges uncovered in previous studies and providing a more effective solution for failure event management during the production.
</details>
<details>
<summary>摘要</summary>
现代社会中，石油工业具有重要的地位，但生产过程具有复杂和危险的特点。生产过程中的意外或失败可能会导致严重的环境和经济损害。先前的研究已经调查了机器学习（ML）方法用于不愿意事件检测。然而，实时预测事件概率的问题尚未得到充分解决，这是因为在事件预计将发生时，早期干预是非常重要的。本文提出了两种ML方法，随机森林和时间卷积网络，用于实时检测不愿意事件。结果表明，我们的方法可以有效地分类事件类型并预测事件出现的概率，解决先前研究中存在的挑战，并为生产过程中的失败事件管理提供更有效的解决方案。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Way-to-Incorporate-Novelty-Detection-in-World-Models"><a href="#A-Simple-Way-to-Incorporate-Novelty-Detection-in-World-Models" class="headerlink" title="A Simple Way to Incorporate Novelty Detection in World Models"></a>A Simple Way to Incorporate Novelty Detection in World Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08731">http://arxiv.org/abs/2310.08731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geigh Zollicoffer, Kenneth Eaton, Jonathan Balloch, Julia Kim, Mark O. Riedl, Robert Wright</li>
<li>for: 保护RL Agent在突然改变世界机制或属性时的性能和可靠性。</li>
<li>methods: 利用生成的世界模型框架中的假象状态与真实观察状态的偏差来检测新鲜事物。</li>
<li>results: 在一个新环境中，比传统机器学习新鲜事物检测方法和当前RL关注的新鲜事物检测算法更有优势。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) using world models has found significant recent successes. However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline. We refer to the sudden change in visual properties or state transitions as {\em novelties}. Implementing novelty detection within generated world model frameworks is a crucial task for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents, by utilizing the misalignment of the world model's hallucinated states and the true observed states as an anomaly score. We first provide an ontology of novelty detection relevant to sequential decision making, then we provide effective approaches to detecting novelties in a distribution of transitions learned by an agent in a world model. Finally, we show the advantage of our work in a novel environment compared to traditional machine learning novelty detection methods as well as currently accepted RL focused novelty detection algorithms.
</details>
<details>
<summary>摘要</summary>
现代控制学（RL）使用世界模型已经取得了显著成功。然而，当世界机制或属性突然发生变化时，智能体性能和可靠性可能很快减退。我们称这种突然变化为“新奇”（novelties）。在生成世界模型框架中实现新奇探测是保护智能体部署的关键任务。在这篇论文中，我们提出了简单的绝对方法，通过利用世界模型生成的幻觉状态和实际观察状态之间的偏差作为异常分数来检测新奇。我们首先提供了对于顺序决策的新奇检测 Ontology，然后我们提供了有效的检测新奇在智能体在世界模型中学习的转移分布中的方法。最后，我们展示了我们的工作在一个新环境中的优势，比传统机器学习新奇检测方法和当前广泛accepted RL专注的新奇检测算法。
</details></li>
</ul>
<hr>
<h2 id="Transformer-Choice-Net-A-Transformer-Neural-Network-for-Choice-Prediction"><a href="#Transformer-Choice-Net-A-Transformer-Neural-Network-for-Choice-Prediction" class="headerlink" title="Transformer Choice Net: A Transformer Neural Network for Choice Prediction"></a>Transformer Choice Net: A Transformer Neural Network for Choice Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08716">http://arxiv.org/abs/2310.08716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanzhao Wang, Xiaocheng Li, Kalyan Talluri</li>
<li>for: 这篇论文旨在提出一种能够预测客户选择多个 item 的Transformer neural network architecture，即 Transformer Choice Net。</li>
<li>methods: 该论文使用 transformer 网络，考虑客户和物品特征以及上下文（如购物礼品和客户之前选择）来预测客户选择。</li>
<li>results: 在多个 benchmark 数据集上，该 Architecture 表现出比 Literature 中主流模型更好的out-of-sample 预测性能，无需特定模型定制或调整。<details>
<summary>Abstract</summary>
Discrete-choice models, such as Multinomial Logit, Probit, or Mixed-Logit, are widely used in Marketing, Economics, and Operations Research: given a set of alternatives, the customer is modeled as choosing one of the alternatives to maximize a (latent) utility function. However, extending such models to situations where the customer chooses more than one item (such as in e-commerce shopping) has proven problematic. While one can construct reasonable models of the customer's behavior, estimating such models becomes very challenging because of the combinatorial explosion in the number of possible subsets of items. In this paper we develop a transformer neural network architecture, the Transformer Choice Net, that is suitable for predicting multiple choices. Transformer networks turn out to be especially suitable for this task as they take into account not only the features of the customer and the items but also the context, which in this case could be the assortment as well as the customer's past choices. On a range of benchmark datasets, our architecture shows uniformly superior out-of-sample prediction performance compared to the leading models in the literature, without requiring any custom modeling or tuning for each instance.
</details>
<details>
<summary>摘要</summary>
偏函数模型，如多项逻辑或混合逻辑，在市场学、经济学和运筹学中广泛应用：给定一组选项，客户会选择一个选项以最大化隐藏的凝聚函数。然而，将这些模型扩展到客户选择多个Item（如在电子商务上的购物）是有困难的。尽管可以构建合理的客户行为模型，但估计这些模型变得非常困难，因为选择的可能性的 combinatorial 爆炸。在这篇文章中，我们开发了一种变换神经网络架构，名为Transformer Choice Net，适用于预测多个选择。Transformer网络在这种任务中特别适用，因为它们考虑客户和Item的特征以及上下文，上下文可能是商品组合以及客户的过去选择。在一系列的标准数据集上，我们的架构在无需任何定制化或调整的情况下显示了对比标准模型的uniformly 出色的尝试预测性能。
</details></li>
</ul>
<hr>
<h2 id="Toward-Joint-Language-Modeling-for-Speech-Units-and-Text"><a href="#Toward-Joint-Language-Modeling-for-Speech-Units-and-Text" class="headerlink" title="Toward Joint Language Modeling for Speech Units and Text"></a>Toward Joint Language Modeling for Speech Units and Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08715">http://arxiv.org/abs/2310.08715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju-Chieh Chou, Chung-Ming Chien, Wei-Ning Hsu, Karen Livescu, Arun Babu, Alexis Conneau, Alexei Baevski, Michael Auli</li>
<li>for: 本研究旨在模型 speech 和 text 之间的共同表达。</li>
<li>methods: 我们使用不同的 speech tokenizer 将连续的 speech 信号转换成 discrete 单元，并使用不同的方法构建混合 speech-text 数据。我们还引入自动评价指标，以评估模型是否能够共同学习 speech 和 text。</li>
<li>results: 我们的结果表明，通过我们的混合技术，混合 speech 单元和 text，joint LM 可以在 SLU 任务上超过 speech-only 基线，并且在不同的模式（speech 或 text）下进行 Zero-shot 跨模态传递。<details>
<summary>Abstract</summary>
Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model's learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability.
</details>
<details>
<summary>摘要</summary>
文本和语音是人类语言的两大形式。研究者们在映射语音到文本或反之方面努力了很多年。然而，在语言模型化领域，很少努力用于同时模型语音和文本。为了解决这个问题，我们在语音单元和文本之间进行同时语言模型化。我们比较不同的语音切分器将连续的语音信号转换成分解单元，并使用不同的方法构建混合语音-文本数据。我们引入自动评估 metric来评估混合LM如何混合语音和文本。此外，我们在不同Modalitites（语音或文本）下进行了精细调整，并测试模型在下游语言理解任务上的性能，以评估模型是否学习了共享表示。我们的结果表明，通过我们提议的混合技术，混合语音单元和文本的混合LM在SLU任务上超过了基准点的语音Only模型，并表现出零 shot cross-modal可转移性。
</details></li>
</ul>
<hr>
<h2 id="ELDEN-Exploration-via-Local-Dependencies"><a href="#ELDEN-Exploration-via-Local-Dependencies" class="headerlink" title="ELDEN: Exploration via Local Dependencies"></a>ELDEN: Exploration via Local Dependencies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08702">http://arxiv.org/abs/2310.08702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaheng Hu, Zizhao Wang, Peter Stone, Roberto Martin-Martin</li>
<li>for: 这篇论文是为了解决复杂的任务和奖励不够的问题，提出了一种新的自适应奖励方法。</li>
<li>methods: 该方法基于当前环境中实体之间的异常依赖关系，通过计算部分导数来准确地捕捉实体之间的依赖关系，并使用这些依赖关系来鼓励探索新的交互方式。</li>
<li>results: 在四个不同的领域中，ELDEN方法在许多复杂的任务上表现出色，比前一个状态的探索方法更加成功，并且能够准确地捕捉实体之间的依赖关系。<details>
<summary>Abstract</summary>
Tasks with large state space and sparse rewards present a longstanding challenge to reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. To deal with this problem, the community has proposed to augment the reward function with intrinsic reward, a bonus signal that encourages the agent to visit interesting states. In this work, we propose a new way of defining interesting states for environments with factored state spaces and complex chained dependencies, where an agent's actions may change the value of one entity that, in order, may affect the value of another entity. Our insight is that, in these environments, interesting states for exploration are states where the agent is uncertain whether (as opposed to how) entities such as the agent or objects have some influence on each other. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward that encourages the discovery of new interactions between entities. ELDEN utilizes a novel scheme -- the partial derivative of the learned dynamics to model the local dependencies between entities accurately and computationally efficiently. The uncertainty of the predicted dependencies is then used as an intrinsic reward to encourage exploration toward new interactions. We evaluate the performance of ELDEN on four different domains with complex dependencies, ranging from 2D grid worlds to 3D robotic tasks. In all domains, ELDEN correctly identifies local dependencies and learns successful policies, significantly outperforming previous state-of-the-art exploration methods.
</details>
<details>
<summary>摘要</summary>
Tasks with large state space and sparse rewards have long been a challenge for reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. To address this problem, the community has proposed augmenting the reward function with an intrinsic reward, a bonus signal that encourages the agent to visit interesting states. In this work, we propose a new way of defining interesting states for environments with factored state spaces and complex chained dependencies, where an agent's actions may change the value of one entity that, in turn, may affect the value of another entity. Our insight is that, in these environments, interesting states for exploration are states where the agent is uncertain whether (as opposed to how) entities such as the agent or objects have some influence on each other. We present ELDEN, Exploration via Local Dependencies, a novel intrinsic reward that encourages the discovery of new interactions between entities. ELDEN utilizes a novel scheme -- the partial derivative of the learned dynamics to model the local dependencies between entities accurately and computationally efficiently. The uncertainty of the predicted dependencies is then used as an intrinsic reward to encourage exploration toward new interactions. We evaluate the performance of ELDEN on four different domains with complex dependencies, ranging from 2D grid worlds to 3D robotic tasks. In all domains, ELDEN correctly identifies local dependencies and learns successful policies, significantly outperforming previous state-of-the-art exploration methods.
</details></li>
</ul>
<hr>
<h2 id="Virtual-Augmented-Reality-for-Atari-Reinforcement-Learning"><a href="#Virtual-Augmented-Reality-for-Atari-Reinforcement-Learning" class="headerlink" title="Virtual Augmented Reality for Atari Reinforcement Learning"></a>Virtual Augmented Reality for Atari Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08683">http://arxiv.org/abs/2310.08683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/c-a-schiller/var4arl">https://github.com/c-a-schiller/var4arl</a></li>
<li>paper_authors: Christian A. Schiller</li>
<li>for: 研究是使RL代理人在Atari游戏中表现更好的途径，以及是否可以通过现有的图像分割模型提高RL代理人的游戏表现。</li>
<li>methods: 使用现有的图像分割模型（如Segment Anything Model）对RL代理人的游戏环境进行修饰，以提高其游戏表现。</li>
<li>results: 研究发现，对RL代理人的游戏环境进行修饰可以提高其游戏表现，但是需要满足certain condition。 Comparing RL agent performance results from raw and augmented pixel inputs provides insight into these conditions.<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) has achieved significant milestones in the gaming domain, most notably Google DeepMind's AlphaGo defeating human Go champion Ken Jie. This victory was also made possible through the Atari Learning Environment (ALE): The ALE has been foundational in RL research, facilitating significant RL algorithm developments such as AlphaGo and others. In current Atari video game RL research, RL agents' perceptions of its environment is based on raw pixel data from the Atari video game screen with minimal image preprocessing. Contrarily, cutting-edge ML research, external to the Atari video game RL research domain, is focusing on enhancing image perception. A notable example is Meta Research's "Segment Anything Model" (SAM), a foundation model capable of segmenting images without prior training (zero-shot). This paper addresses a novel methodical question: Can state-of-the-art image segmentation models such as SAM improve the performance of RL agents playing Atari video games? The results suggest that SAM can serve as a "virtual augmented reality" for the RL agent, boosting its Atari video game playing performance under certain conditions. Comparing RL agent performance results from raw and augmented pixel inputs provides insight into these conditions. Although this paper was limited by computational constraints, the findings show improved RL agent performance for augmented pixel inputs and can inform broader research agendas in the domain of "virtual augmented reality for video game playing RL agents".
</details>
<details>
<summary>摘要</summary>
reinforcement learning (RL) 在游戏领域取得了重要的成就，最 Notable example 是 Google DeepMind 的 AlphaGo 击败人类Go冠军 Ken Jie。这胜利也得到了 ALE 的支持：ALE 是RL研究中基础的平台，促进了一系列RL算法的发展，如 AlphaGo 等。现在的 Atari 游戏 RL 研究中，RL 代理的环境感知基于 raw pixel 数据从 Atari 游戏屏幕， minimal 图像预处理。然而，当前的 ML 研究，外部于 Atari 游戏 RL 研究领域，正在强调图像感知的提高。一个 notable example 是 Meta Research 的 "Segment Anything Model" (SAM)，这是一个无需先期训练的基本模型，可以 segmenting 图像。本文提出了一个新的问题：可以使用 state-of-the-art 图像 segmentation 模型来提高 Atari 游戏 RL 代理的性能吗？结果表明，SAM 可以作为 RL 代理的 "虚拟增强 reality"，在某些条件下提高其 Atari 游戏性能。通过比较 raw 和增强 pixel 输入的 RL 代理性能结果，可以了解这些条件。虽然这篇文章受限于计算力，但结果表明在某些情况下，使用 state-of-the-art 图像 segmentation 模型可以提高 RL 代理的性能，这些结果可以推导到更广泛的 "虚拟增强 reality  для video game 游戏 RL 代理" 的研究论题。
</details></li>
</ul>
<hr>
<h2 id="Can-GPT-models-be-Financial-Analysts-An-Evaluation-of-ChatGPT-and-GPT-4-on-mock-CFA-Exams"><a href="#Can-GPT-models-be-Financial-Analysts-An-Evaluation-of-ChatGPT-and-GPT-4-on-mock-CFA-Exams" class="headerlink" title="Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams"></a>Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08678">http://arxiv.org/abs/2310.08678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan Callanan, Amarachi Mbakwe, Antony Papadimitriou, Yulong Pei, Mathieu Sibue, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu, Sameena Shah</li>
<li>For: This study aims to assess the financial reasoning capabilities of Large Language Models (LLMs) using mock exam questions from the Chartered Financial Analyst (CFA) Program.* Methods: The study uses ChatGPT and GPT-4 in financial analysis, considering Zero-Shot (ZS), Chain-of-Thought (CoT), and Few-Shot (FS) scenarios.* Results: The study presents an in-depth analysis of the models’ performance and limitations, and estimates whether they would have a chance at passing the CFA exams. Additionally, it outlines insights into potential strategies and improvements to enhance the applicability of LLMs in finance.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable performance on a wide range of Natural Language Processing (NLP) tasks, often matching or even beating state-of-the-art task-specific models. This study aims at assessing the financial reasoning capabilities of LLMs. We leverage mock exam questions of the Chartered Financial Analyst (CFA) Program to conduct a comprehensive evaluation of ChatGPT and GPT-4 in financial analysis, considering Zero-Shot (ZS), Chain-of-Thought (CoT), and Few-Shot (FS) scenarios. We present an in-depth analysis of the models' performance and limitations, and estimate whether they would have a chance at passing the CFA exams. Finally, we outline insights into potential strategies and improvements to enhance the applicability of LLMs in finance. In this perspective, we hope this work paves the way for future studies to continue enhancing LLMs for financial reasoning through rigorous evaluation.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经在各种自然语言处理任务上显示出极具表现力，经常与状态流的任务特定模型匹配或甚至超越。这项研究的目的是评估LLM在金融分析中的理解能力。我们利用Chartered Financial Analyst（CFA）考试Mock问题来进行全面的GPT和GPT-4在金融分析中的评估，包括零极（ZS）、链条（CoT）和几极（FS）场景。我们提供了深入的分析和限制，并估算这些模型是否会在CFA考试中通过。最后，我们总结了可能的策略和改进，以提高LLM在金融领域的应用性。希望这项研究能够为未来的研究提供依据，继续提高LLM在金融分析中的表现。
</details></li>
</ul>
<hr>
<h2 id="GDL-DS-A-Benchmark-for-Geometric-Deep-Learning-under-Distribution-Shifts"><a href="#GDL-DS-A-Benchmark-for-Geometric-Deep-Learning-under-Distribution-Shifts" class="headerlink" title="GDL-DS: A Benchmark for Geometric Deep Learning under Distribution Shifts"></a>GDL-DS: A Benchmark for Geometric Deep Learning under Distribution Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08677">http://arxiv.org/abs/2310.08677</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graph-com/gdl_ds">https://github.com/graph-com/gdl_ds</a></li>
<li>paper_authors: Deyu Zou, Shikun Liu, Siqi Miao, Victor Fung, Shiyu Chang, Pan Li</li>
<li>for: 本研究旨在评估深度学习模型在数据分布变化的情况下的性能。</li>
<li>methods: 本研究使用的方法包括提出了一个全面的benchmark，用于评估深度学习模型在不同的数据分布变化情况下的性能。</li>
<li>results: 研究结果显示，在30个不同的实验设置中，3种深度学习基础模型和11种学习算法在不同的数据分布变化情况下的性能有所差异。<details>
<summary>Abstract</summary>
Geometric deep learning (GDL) has gained significant attention in various scientific fields, chiefly for its proficiency in modeling data with intricate geometric structures. Yet, very few works have delved into its capability of tackling the distribution shift problem, a prevalent challenge in many relevant applications. To bridge this gap, we propose GDL-DS, a comprehensive benchmark designed for evaluating the performance of GDL models in scenarios with distribution shifts. Our evaluation datasets cover diverse scientific domains from particle physics and materials science to biochemistry, and encapsulate a broad spectrum of distribution shifts including conditional, covariate, and concept shifts. Furthermore, we study three levels of information access from the out-of-distribution (OOD) testing data, including no OOD information, only OOD features without labels, and OOD features with a few labels. Overall, our benchmark results in 30 different experiment settings, and evaluates 3 GDL backbones and 11 learning algorithms in each setting. A thorough analysis of the evaluation results is provided, poised to illuminate insights for DGL researchers and domain practitioners who are to use DGL in their applications.
</details>
<details>
<summary>摘要</summary>
几何深度学习（GDL）已经受到了不同领域的科学家的重视，主要是因为它能够有效地模型复杂的几何结构数据。然而，只有一些研究探讨了GDL模型在分布类型错误（distribution shift）的情况下的能力。为了补充这个空白，我们提出了GDL-DS，一个全面的对照测试框架，用于评估GDL模型在分布类型错误的情况下的表现。我们的评估数据集覆盖了物理学和材料科学等多个科学领域，并包含了各种分布类型错误，包括增量、偏好和概念类型错误。此外，我们还研究了从 OUT-OF-Distribution（OOD）测试数据中获取信息的三种水平，包括没有OOD信息、只有OOD特征而无 labels，以及OOD特征和一些labels。总的来说，我们的对照测试得出了30个不同的实验设定，并评估了3个GDL核心和11种学习算法在每个设定中。我们进行了详细的分析结果，以便为DGL研究者和领域实践者提供启发。
</details></li>
</ul>
<hr>
<h2 id="Learning-RL-Policies-for-Joint-Beamforming-Without-Exploration-A-Batch-Constrained-Off-Policy-Approach"><a href="#Learning-RL-Policies-for-Joint-Beamforming-Without-Exploration-A-Batch-Constrained-Off-Policy-Approach" class="headerlink" title="Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach"></a>Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08660">http://arxiv.org/abs/2310.08660</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/heasung-kim/safe-rl-deployment-for-5g">https://github.com/heasung-kim/safe-rl-deployment-for-5g</a></li>
<li>paper_authors: Heasung Kim, Sravan Ankireddy</li>
<li>For: The paper is written for optimizing network parameters for rate maximization in 5G communication systems.* Methods: The paper proposes using deep reinforcement learning (RL) techniques, specifically discrete batch constrained deep Q-learning (BCQ), to solve the non-convex optimization problem of power control, beam forming, and interference cancellation.* Results: The paper shows that the proposed BCQ algorithm can achieve performance similar to deep Q-network (DQN) based control with only a fraction of the data and without the need for exploration, resulting in maximized sample efficiency and minimized risk in the deployment of a new algorithm to commercial networks.Here are the three key information points in Simplified Chinese text:* For: 本文是为了优化5G通信系统中的网络参数以实现速率最大化。* Methods: 本文提议使用深度学习 Reinforcement Learning (RL) 技术，特别是粗粒度约束的深度 Q-学习 (BCQ)，解决非对称优化问题。* Results: 本文显示，提议的 BCQ 算法可以与 DQN 基于控制 дости到类似性，只需要一小部分数据和不需要探索，从而最大化样本效率和风险的避免。<details>
<summary>Abstract</summary>
In this project, we consider the problem of network parameter optimization for rate maximization. We frame this as a joint optimization problem of power control, beam forming, and interference cancellation. We consider the setting where multiple Base Stations (BSs) are communicating with multiple user equipments (UEs). Because of the exponential computational complexity of brute force search, we instead solve this non-convex optimization problem using deep reinforcement learning (RL) techniques. The modern communication systems are notorious for their difficulty in exactly modeling their behaviour. This limits us in using RL based algorithms as interaction with the environment is needed for the agent to explore and learn efficiently. Further, it is ill advised to deploy the algorithm in real world for exploration and learning because of the high cost of failure. In contrast to the previous RL-based solutions proposed, such as deep-Q network (DQN) based control, we propose taking an offline model based approach. We specifically consider discrete batch constrained deep Q-learning (BCQ) and show that performance similar to DQN can be acheived with only a fraction of the data and without the need for exploration. This results in maximizing sample efficiency and minimizing risk in the deployment of a new algorithm to commercial networks. We provide the entire resource of the project, including code and data, at the following link: https://github.com/Heasung-Kim/ safe-rl-deployment-for-5g.
</details>
<details>
<summary>摘要</summary>
在这个项目中，我们考虑了网络参数优化问题，以maximize rate。我们将这个问题划为多个基站（BS）与多个用户设备（UE）之间的共同优化问题，包括功率控制、扫描形成和干扰抑制。由于条件矩阵的计算复杂性，我们不能采用条件矩阵搜索法。相反，我们使用深度学习束缚学习（RL）技术来解决这个非连续优化问题。现代通信系统的行为难以准确模拟，这限制了我们使用RL基于算法。此外，由于实际部署中的失败成本高，我们不建议在实际环境中进行探索和学习。相比之前的RL基于解决方案，我们提出了离线模型基于的BCQ算法。我们表明，BCQ算法可以在只需一部分数据和不需探索的情况下，达到与DQN算法相同的性能。这使得我们可以最大化样本效率，最小化部署新算法到商业网络中的风险。我们提供了该项目的所有资源，包括代码和数据，请参考以下链接：https://github.com/Heasung-Kim/safe-rl-deployment-for-5g。
</details></li>
</ul>
<hr>
<h2 id="LoftQ-LoRA-Fine-Tuning-Aware-Quantization-for-Large-Language-Models"><a href="#LoftQ-LoRA-Fine-Tuning-Aware-Quantization-for-Large-Language-Models" class="headerlink" title="LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"></a>LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08659">http://arxiv.org/abs/2310.08659</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yxli2123/loftq">https://github.com/yxli2123/loftq</a></li>
<li>paper_authors: Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, Tuo Zhao</li>
<li>for: 本研究旨在探讨在预训练模型上同时应用量化和LoRA精度调整的场景下，量化和LoRA精度调整可以共同提高下游任务的性能。</li>
<li>methods: 我们提出了LoftQ（LoRA-Fine-Tuning-aware Quantization）量化框架，该框架同时对LLM进行量化，并在LoRA精度调整中找到适当的低级别初始化，以解决量化模型和全精度模型之间的性能差距。</li>
<li>results: 我们在自然语言理解、问答、概要、自然语言生成等任务上进行了实验，结果表明，我们的方法在2比特和2&#x2F;4比特混合精度 режиmes中具有显著的优势，与现有的量化方法相比，尤其是在更加具有挑战性的场景下表现出色。<details>
<summary>Abstract</summary>
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms existing quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision regimes. We will release our code.
</details>
<details>
<summary>摘要</summary>
“量化”是现代大语言模型（LLM）的不可或缺技巧，最近它在LoRA精细调整中找到了应用。在这种场景下，我们发现在预训练模型上应用量化和LoRA精细调整时，下游任务表现存在一个一致的差距。为了解决这个问题，我们提出了LoftQ（LoRA-Fine-Tuning-aware Quantization），一种新的量化框架，同时对大语言模型进行量化，并在LoRA精细调整中找到合适的低级初始化。这种初始化可以减轻量化模型和整数模型之间的差异，并在下游任务中提高通用性。我们在自然语言理解、问答、概要、自然语言生成等任务上进行了实验，结果显示，我们的方法非常有效，特别在2位和2/4位混合精度 régime中表现出色。我们将发布我们的代码。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Textual-Data-for-Fatality-Classification-in-Afghanistan’s-Armed-Conflicts-A-BERT-Approach"><a href="#Analyzing-Textual-Data-for-Fatality-Classification-in-Afghanistan’s-Armed-Conflicts-A-BERT-Approach" class="headerlink" title="Analyzing Textual Data for Fatality Classification in Afghanistan’s Armed Conflicts: A BERT Approach"></a>Analyzing Textual Data for Fatality Classification in Afghanistan’s Armed Conflicts: A BERT Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08653">http://arxiv.org/abs/2310.08653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hikmatullah Mohammadi, Ziaullah Momand, Parwin Habibi, Nazifa Ramaki, Bibi Storay Fazli, Sayed Zobair Rohany, Iqbal Samsoor</li>
<li>For: The paper aims to classify the outcomes of armed conflicts in Afghanistan as either fatal or non-fatal based on textual descriptions provided by the ACLED dataset.* Methods: The paper uses the BERT model, a cutting-edge language representation model in natural language processing, to classify the events based on their raw textual descriptions.* Results: The model achieved impressive performance on the test set with an accuracy of 98.8%, recall of 98.05%, precision of 99.6%, and an F1 score of 98.82%. These results highlight the model’s robustness and indicate its potential impact in various areas such as resource allocation, policymaking, and humanitarian aid efforts in Afghanistan.Here are the three points in Simplified Chinese text:</li>
<li>for: 这个研究目标是使用 ACLED 数据集的文本描述来分类阿富汗武装冲突的结果为非致死或致死。</li>
<li>methods: 这个研究使用 BERT 模型，一种现代自然语言处理的语言表示模型，来基于事件的原始文本描述来分类。</li>
<li>results: 模型在测试集上表现出色，准确率为 98.8%，回归率为 98.05%，准确率为 99.6%， F1 分数为 98.82%。这些结果表明模型的稳定性，并指示其在阿富汗资源分配、政策制定和人道主义援助等领域的潜在影响。<details>
<summary>Abstract</summary>
Afghanistan has witnessed many armed conflicts throughout history, especially in the past 20 years; these events have had a significant impact on human lives, including military and civilians, with potential fatalities. In this research, we aim to leverage state-of-the-art machine learning techniques to classify the outcomes of Afghanistan armed conflicts to either fatal or non-fatal based on their textual descriptions provided by the Armed Conflict Location & Event Data Project (ACLED) dataset. The dataset contains comprehensive descriptions of armed conflicts in Afghanistan that took place from August 2021 to March 2023. The proposed approach leverages the power of BERT (Bidirectional Encoder Representations from Transformers), a cutting-edge language representation model in natural language processing. The classifier utilizes the raw textual description of an event to estimate the likelihood of the event resulting in a fatality. The model achieved impressive performance on the test set with an accuracy of 98.8%, recall of 98.05%, precision of 99.6%, and an F1 score of 98.82%. These results highlight the model's robustness and indicate its potential impact in various areas such as resource allocation, policymaking, and humanitarian aid efforts in Afghanistan. The model indicates a machine learning-based text classification approach using the ACLED dataset to accurately classify fatality in Afghanistan armed conflicts, achieving robust performance with the BERT model and paving the way for future endeavors in predicting event severity in Afghanistan.
</details>
<details>
<summary>摘要</summary>
阿富汗历史上有很多武装冲突，特别是过去20年，这些事件对人类生命产生了深远的影响，包括军事人员和平民，可能导致致命性伤亡。在这项研究中，我们想利用当今最先进的机器学习技术来分类阿富汗武装冲突的结果为致命或非致命，基于ACLED数据集（武装冲突位置和事件数据项目）提供的文本描述。ACLED数据集包含了阿富汗2021年8月至2023年3月期间的武装冲突描述。我们提出的方法利用BERT（irectional Encoder Representations from Transformers）模型，这是当今自然语言处理领域最先进的语言表示模型。分类器使用事件描述的Raw文本来估计事件是否会导致致命性伤亡。测试集上，模型实现了惊人的表现，准确率为98.8%，回归率为98.05%，精度为99.6%，F1分数为98.82%。这些结果显示模型的强健性，并指示其在各种领域，如资源分配、政策制定和人道主义援助等，有可能产生深远的影响。模型表明，使用ACLED数据集和BERT模型进行文本分类可以准确地 классифици阿富汗武装冲突的致命性，实现了robust性表现，开创了预测阿富汗事件严重性的先河。
</details></li>
</ul>
<hr>
<h2 id="Electrical-Grid-Anomaly-Detection-via-Tensor-Decomposition"><a href="#Electrical-Grid-Anomaly-Detection-via-Tensor-Decomposition" class="headerlink" title="Electrical Grid Anomaly Detection via Tensor Decomposition"></a>Electrical Grid Anomaly Detection via Tensor Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08650">http://arxiv.org/abs/2310.08650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Most, Maksim Eren, Nigel Lawrence, Boian Alexandrov</li>
<li>For:  This paper aims to improve the accuracy and specificity of anomaly detection in Supervisory Control and Data Acquisition (SCADA) systems for electrical grid systems.* Methods: The paper applies a non-negative tensor decomposition method called Canonical Polyadic Alternating Poisson Regression (CP-APR) with a probabilistic framework to identify anomalies in SCADA systems.* Results: The use of statistical behavior analysis of SCADA communication with tensor decomposition improves the specificity and accuracy of identifying anomalies in electrical grid systems, as demonstrated through experiments using real-world SCADA system data collected from the Los Alamos National Laboratory (LANL).<details>
<summary>Abstract</summary>
Supervisory Control and Data Acquisition (SCADA) systems often serve as the nervous system for substations within power grids. These systems facilitate real-time monitoring, data acquisition, control of equipment, and ensure smooth and efficient operation of the substation and its connected devices. Previous work has shown that dimensionality reduction-based approaches, such as Principal Component Analysis (PCA), can be used for accurate identification of anomalies in SCADA systems. While not specifically applied to SCADA, non-negative matrix factorization (NMF) has shown strong results at detecting anomalies in wireless sensor networks. These unsupervised approaches model the normal or expected behavior and detect the unseen types of attacks or anomalies by identifying the events that deviate from the expected behavior. These approaches; however, do not model the complex and multi-dimensional interactions that are naturally present in SCADA systems. Differently, non-negative tensor decomposition is a powerful unsupervised machine learning (ML) method that can model the complex and multi-faceted activity details of SCADA events. In this work, we novelly apply the tensor decomposition method Canonical Polyadic Alternating Poisson Regression (CP-APR) with a probabilistic framework, which has previously shown state-of-the-art anomaly detection results on cyber network data, to identify anomalies in SCADA systems. We showcase that the use of statistical behavior analysis of SCADA communication with tensor decomposition improves the specificity and accuracy of identifying anomalies in electrical grid systems. In our experiments, we model real-world SCADA system data collected from the electrical grid operated by Los Alamos National Laboratory (LANL) which provides transmission and distribution service through a partnership with Los Alamos County, and detect synthetically generated anomalies.
</details>
<details>
<summary>摘要</summary>
超visory控制和数据获取（SCADA）系统 часто作为电网互网络的神经系统。这些系统实时监控、数据获取、控制设备，以确保电网和相关设备的运行平滑和高效。以前的研究表明，维度减少基本方法，如主成分分析（PCA），可以准确地检测SCADA系统中的异常。尽管不直接应用于SCADA，非负矩阵分解（NMF）在无人报表网络中检测异常表现出色。这些不监管的方法模拟正常或预期的行为，并检测不可见的攻击或异常情况，并且可以快速地响应变化。然而，这些方法不能模拟SCADA系统中自然存在的复杂多维度交互。相反，非负矩阵分解是一种强大的无监管机器学习方法，可以模拟SCADA事件的复杂多方面活动详细情况。在这种工作中，我们首次应用tensor decompositions方法Canonical Polyadic Alternating Poisson Regression（CP-APR）的概率框架，以前已经在网络数据上达到了状态之绩异常检测结果。我们显示，通过统计行为分析SCADA通信和tensor decompositions，可以提高异常检测在电力网络系统中的特点和准确率。在我们的实验中，我们使用实际的SCADA系统数据，从洛斯阿拉莫斯国家实验室（LANL）电力网络提供的传输和分布服务，并检测生成的异常。
</details></li>
</ul>
<hr>
<h2 id="A-Mass-Conserving-Perceptron-for-Machine-Learning-Based-Modeling-of-Geoscientific-Systems"><a href="#A-Mass-Conserving-Perceptron-for-Machine-Learning-Based-Modeling-of-Geoscientific-Systems" class="headerlink" title="A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems"></a>A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08644">http://arxiv.org/abs/2310.08644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan-Heng Wang, Hoshin V. Gupta</li>
<li>for: 该文章是为了开发一种能够更准确地预测地球科学系统时间序列进程的 физи学基础模型。</li>
<li>methods: 该文章使用机器学习（ML）技术，开发了一种基于权重链网络（GRNN）的 физи学基础模型。</li>
<li>results: 该文章的实验结果表明，该模型可以更好地预测地球科学系统的时间序列进程，并且可以帮助科学家更好地理解系统的结构和功能。<details>
<summary>Abstract</summary>
Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As a proof of concept, we investigate the functional expressivity (capacity) of the MCP, explore its ability to parsimoniously represent the rainfall-runoff (RR) dynamics of the Leaf River Basin, and demonstrate its utility for scientific hypothesis testing. To conclude, we discuss extensions of the concept to enable ML-based physical-conceptual representation of the coupled nature of mass-energy-information flows through geoscientific systems.
</details>
<details>
<summary>摘要</summary>
尽管多年的努力已经投入到建立物理概念（PC）模型以预测地球科学系统的时间序列演化，但最近的研究表明，机器学习（ML）基于闭合循环神经网络技术可以建立更高度准确的模型。然而，提取物理理解从ML基于模型中带来了问题，使其在提高科学知识系统结构和功能方面具有限制。为了bridging这个鸿沟，我们提议一种可解释的质量保持嵌入（MCP），该模型利用PC模型和GRNNs的直接对应关系来显式表示物理过程中的质量保持性，同时允许功能性过程直接从数据中学习（可解释的方式）。作为证明，我们研究MCP的功能表达能力，探讨它在哥伦比亚河流水系中表达简洁性，并示出其在科学假设测试中的实用性。最后，我们讨论了扩展该概念，以实现ML基于物理概念的表示地球科学系统的结合性。
</details></li>
</ul>
<hr>
<h2 id="Octopus-Embodied-Vision-Language-Programmer-from-Environmental-Feedback"><a href="#Octopus-Embodied-Vision-Language-Programmer-from-Environmental-Feedback" class="headerlink" title="Octopus: Embodied Vision-Language Programmer from Environmental Feedback"></a>Octopus: Embodied Vision-Language Programmer from Environmental Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08588">http://arxiv.org/abs/2310.08588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongyh20/octopus">https://github.com/dongyh20/octopus</a></li>
<li>paper_authors: Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu</li>
<li>for: 本研究旨在开发一种能够高效地理解智能代理人的视觉和文本任务目标，并生成复杂的行动序列和可执行代码的新型视觉语言模型（VLM）。</li>
<li>methods: 本研究使用GPT-4来控制一个探索性的代理人生成训练数据，包括行动蓝图和相应的可执行代码，并采用反馈学习环境反馈（RLEF）来进一步优化决策。</li>
<li>results: 经过一系列实验，我们证明 Octopus 的功能和取得了吸引人的结果，并且 RLEF 提高了代理人的决策。<details>
<summary>Abstract</summary>
Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.
</details>
<details>
<summary>摘要</summary>
大型视力语言模型（VLM）已经取得了多样化感知和理解的重要进步。更重要的是，当这些模型与embody agent结合使用时，表示自主和上下文感知系统的创造。在这篇论文中，我们介绍了Octopus，一种新的VLM，可以高效地理解机器人的视觉和文本任务目标，并生成复杂的动作序列和执行代码。我们的设计允许机器人在各种任务中灵活处理，从日常 simulate 中的杂乱任务到复杂的 виде游戏中的互动。Octopus 通过利用 GPT-4 控制一个探索性的机器人生成训练数据，即动作蓝图和相应的执行代码，在我们的实验环境 OctoVerse 中。我们还收集了反馈，用于改进强化学习环境反馈（RLEF）的训练方案。通过一系列实验，我们表明 Octopus 的功能和结果，并发现 RLEF 对机器人做出了更好的决策。我们通过开源我们的模型结构、模拟器和数据集，希望能够点燃更多的创新和在更广泛的embody AI社区中的合作应用。
</details></li>
</ul>
<hr>
<h2 id="Tree-Planner-Efficient-Close-loop-Task-Planning-with-Large-Language-Models"><a href="#Tree-Planner-Efficient-Close-loop-Task-Planning-with-Large-Language-Models" class="headerlink" title="Tree-Planner: Efficient Close-loop Task Planning with Large Language Models"></a>Tree-Planner: Efficient Close-loop Task Planning with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08582">http://arxiv.org/abs/2310.08582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, Ping Luo</li>
<li>for: 该论文研究了一种名为close-loop任务规划的技术，它是一种根据实时观察而逐步生成任务计划的过程。</li>
<li>methods: 该论文使用了大语言模型（LLM）来生成动作，并将其分为三个阶段：计划抽样、动作树构建和基于实际环境信息的决策。</li>
<li>results: 该论文通过将LLM查询分解为多个基于实际环境信息的决策，可以大幅减少token消耗量，同时提高了效率。实验显示，该方法可以达到状态的术语表现，而且可以减少92.2%的token消耗量和40.5%的错误纠正量。<details>
<summary>Abstract</summary>
This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5% decrease in error corrections. Project page: https://tree-planner.github.io/
</details>
<details>
<summary>摘要</summary>
这份论文研究了闭环任务规划，即通过生成一个序列的技能（计划）来完成特定目标，并在实时观察基础上修改计划。最近，通过让大型自然语言模型（LLM）逐步生成动作来实现这种方法，已成为流行的方法，因为它的性能和用户友好性。然而，这种方法受到两种不足：高度的token消耗和重复的错误修正，两者都阻碍了其扩展性，特别是对大规模测试和应用。为了解决这些问题，我们提出了Tree-Planner，它将任务规划转化为三个不同阶段：计划抽样、动作树构建和基于现场信息的决策。Tree-Planner开始使用LLM生成一组可能的计划，然后将它们聚合成动作树。最后，LLM在树上进行顶部决策过程，考虑实时环境信息。实验结果表明，Tree-Planner可以 дости得状态足以性，同时保持高效。通过将LLM查询分解成单个计划抽样调用和多个基于现场信息的决策调用，可以减少提示的92.2%。此外，通过允许在动作树上进行弹回 correction， correction过程更加灵活，导致错误修正减少40.5%。项目页面：https://tree-planner.github.io/
</details></li>
</ul>
<hr>
<h2 id="Jigsaw-Supporting-Designers-in-Prototyping-Multimodal-Applications-by-Assembling-AI-Foundation-Models"><a href="#Jigsaw-Supporting-Designers-in-Prototyping-Multimodal-Applications-by-Assembling-AI-Foundation-Models" class="headerlink" title="Jigsaw: Supporting Designers in Prototyping Multimodal Applications by Assembling AI Foundation Models"></a>Jigsaw: Supporting Designers in Prototyping Multimodal Applications by Assembling AI Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08574">http://arxiv.org/abs/2310.08574</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Chuan-En Lin, Nikolas Martelaro</li>
<li>for: 本研究旨在帮助设计师在创作过程中更好地利用基础模型，提高设计效率和质量。</li>
<li>methods: 本研究使用维度模型作为基础模型，并通过将这些模型转化为独特的盘点模式来帮助设计师更好地组合不同的模式和任务。</li>
<li>results: 在用户研究中，Jigsaw系统有助于设计师更好地理解可用基础模型的功能，提供了不同模式和任务之间的组合指南，并且可以作为设计探索、原型制作和文档支持的画布。<details>
<summary>Abstract</summary>
Recent advancements in AI foundation models have made it possible for them to be utilized off-the-shelf for creative tasks, including ideating design concepts or generating visual prototypes. However, integrating these models into the creative process can be challenging as they often exist as standalone applications tailored to specific tasks. To address this challenge, we introduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to represent foundation models. Jigsaw allows designers to combine different foundation model capabilities across various modalities by assembling compatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten designers and distilled design goals. In a user study, we showed that Jigsaw enhanced designers' understanding of available foundation model capabilities, provided guidance on combining capabilities across different modalities and tasks, and served as a canvas to support design exploration, prototyping, and documentation.
</details>
<details>
<summary>摘要</summary>
Recent advancements in AI基础模型have made it possible to use them for creative tasks such as generating design concepts or visual prototypes. However, integrating these models into the creative process can be challenging because they often exist as standalone applications tailored to specific tasks. To address this challenge, we introduce Jigsaw, a prototype system that uses puzzle pieces as metaphors to represent foundation models. Jigsaw allows designers to combine different foundation model capabilities across various modalities by assembling compatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten designers and distilled their design goals. In a user study, we found that Jigsaw enhanced designers' understanding of available foundation model capabilities, provided guidance on combining capabilities across different modalities and tasks, and served as a canvas to support design exploration, prototyping, and documentation.
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Calibrated-Simulation-Enabling-Efficient-Offline-Learning-for-Optimal-Control-of-Real-Buildings"><a href="#A-Lightweight-Calibrated-Simulation-Enabling-Efficient-Offline-Learning-for-Optimal-Control-of-Real-Buildings" class="headerlink" title="A Lightweight Calibrated Simulation Enabling Efficient Offline Learning for Optimal Control of Real Buildings"></a>A Lightweight Calibrated Simulation Enabling Efficient Offline Learning for Optimal Control of Real Buildings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08569">http://arxiv.org/abs/2310.08569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Judah Goldfeder, John Sipple</li>
<li>for: 这篇论文的目的是提出一种基于强化学习的空调系统控制方法，以减少能源消耗和碳排放。</li>
<li>methods: 这篇论文使用了一个自订的模拟器来训练代理人，并使用了现有的建筑和天气资料来实现更高的精度。</li>
<li>results: 在一个68,000平方英尺的二层建筑物上，使用这种方法可以实现仅仅半度的偏差值和现实世界之间的调整，这显示了这种方法在减少能源消耗和碳排放方面的重要性。<details>
<summary>Abstract</summary>
Modern commercial Heating, Ventilation, and Air Conditioning (HVAC) devices form a complex and interconnected thermodynamic system with the building and outside weather conditions, and current setpoint control policies are not fully optimized for minimizing energy use and carbon emission. Given a suitable training environment, a Reinforcement Learning (RL) model is able to improve upon these policies, but training such a model, especially in a way that scales to thousands of buildings, presents many real world challenges. We propose a novel simulation-based approach, where a customized simulator is used to train the agent for each building. Our open-source simulator (available online: https://github.com/google/sbsim) is lightweight and calibrated via telemetry from the building to reach a higher level of fidelity. On a two-story, 68,000 square foot building, with 127 devices, we were able to calibrate our simulator to have just over half a degree of drift from the real world over a six-hour interval. This approach is an important step toward having a real-world RL control system that can be scaled to many buildings, allowing for greater efficiency and resulting in reduced energy consumption and carbon emissions.
</details>
<details>
<summary>摘要</summary>
现代商业冷却、通风、空调设备形成了复杂且相互连接的 термодинамиче系统，与建筑物和外部天气条件相关。目前的设点控制策略并没有充分优化能源使用和二氧化碳排放。一个适当的训练环境下，一个强化学习（RL）模型可以改进这些策略，但是训练这样一个模型，特别是在千量级建筑物上，存在许多现实世界挑战。我们提议一种新的模拟基本方法，其中每座建筑物都有自己的特定的模拟器。我们开源的模拟器（可以在线访问：https://github.com/google/sbsim）轻量级，通过建筑物的测验数据进行准确调整。在一座两层、68,000平方米的建筑物上，拥有127个设备时，我们可以在六个小时内将模拟器与实际世界之间的偏差降低到了超过一半度。这种方法是有效地帮助实现大规模化RL控制系统，从而提高能源使用效率，并减少能源消耗和二氧化碳排放。
</details></li>
</ul>
<hr>
<h2 id="Transformers-as-Decision-Makers-Provable-In-Context-Reinforcement-Learning-via-Supervised-Pretraining"><a href="#Transformers-as-Decision-Makers-Provable-In-Context-Reinforcement-Learning-via-Supervised-Pretraining" class="headerlink" title="Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining"></a>Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08566">http://arxiv.org/abs/2310.08566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Licong Lin, Yu Bai, Song Mei</li>
<li>for: 这paper的目的是理解可以在offline启动的大型变换器模型中进行ICRL。</li>
<li>methods: 这paper使用了两种近期提出的训练方法：算法涵化和决策预训练变换器。</li>
<li>results: 这paper表明，supervised预训练的变换器可以很好地复制条件预期的专家算法，并且可以有效地近似在线学习算法。<details>
<summary>Abstract</summary>
Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories.
</details>
<details>
<summary>摘要</summary>
大型转换器模型在线上强化学习数据上预训练后表现出了非常出色的在场景强化学习（ICRL）能力，它们可以在未看过环境中接受交互轨迹时作出良好的决策。然而，transformer在ICRL中被训练的时候和怎样做到ICRL都没有有 teorтичеamente好的理解。具体来说，transformer可以执行哪些强化学习算法在场景中，以及在线上训练数据中的分布差异如何影响学习的算法。这篇论文提供了一个理论框架，用于分析监督预训练的ICRL。这包括两种最近提出的训练方法：算法采样和决策预训练转换器。首先，我们假设模型可行，我们证明监督预训练的转换器将在观察轨迹时效果地复制出 conditional expectation 的专家算法。总的来说，泛化误差将与模型容量和一个分布分化因子 между专家和线上算法相关。其次，我们表明 transformer  WITH ReLU 注意力可以高效地近似在线强化学习算法 like LinUCB 和 Thompson sampling  для随机线性奖励，以及 UCB-VI  для表格 Markov 决策过程。这是 ICRL 能力的首次量化分析。
</details></li>
</ul>
<hr>
<h2 id="Security-Considerations-in-AI-Robotics-A-Survey-of-Current-Methods-Challenges-and-Opportunities"><a href="#Security-Considerations-in-AI-Robotics-A-Survey-of-Current-Methods-Challenges-and-Opportunities" class="headerlink" title="Security Considerations in AI-Robotics: A Survey of Current Methods, Challenges, and Opportunities"></a>Security Considerations in AI-Robotics: A Survey of Current Methods, Challenges, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08565">http://arxiv.org/abs/2310.08565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subash Neupane, Shaswata Mitra, Ivan A. Fernandez, Swayamjit Saha, Sudip Mittal, Jingdao Chen, Nisha Pillai, Shahram Rahimi</li>
<li>for: 这篇论文的目的是为了探讨人工智能机器人系统的安全问题。</li>
<li>methods: 这篇论文使用了三维ensional的攻击表面、伦理和法律问题、人机交互安全等方面进行概括和分类。</li>
<li>results: 这篇论文提供了一个总结性的对话，包括攻击表面、伦理和法律问题、人机交互安全等方面的概括和分类，以帮助用户、开发者和其他关注者更好地理解这些领域，并提高整体系统安全性。<details>
<summary>Abstract</summary>
Robotics and Artificial Intelligence (AI) have been inextricably intertwined since their inception. Today, AI-Robotics systems have become an integral part of our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These systems are built upon three fundamental architectural elements: perception, navigation and planning, and control. However, while the integration of AI-Robotics systems has enhanced the quality our lives, it has also presented a serious problem - these systems are vulnerable to security attacks. The physical components, algorithms, and data that make up AI-Robotics systems can be exploited by malicious actors, potentially leading to dire consequences. Motivated by the need to address the security concerns in AI-Robotics systems, this paper presents a comprehensive survey and taxonomy across three dimensions: attack surfaces, ethical and legal concerns, and Human-Robot Interaction (HRI) security. Our goal is to provide users, developers and other stakeholders with a holistic understanding of these areas to enhance the overall AI-Robotics system security. We begin by surveying potential attack surfaces and provide mitigating defensive strategies. We then delve into ethical issues, such as dependency and psychological impact, as well as the legal concerns regarding accountability for these systems. Besides, emerging trends such as HRI are discussed, considering privacy, integrity, safety, trustworthiness, and explainability concerns. Finally, we present our vision for future research directions in this dynamic and promising field.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）和机器人技术自出发以来一直是不可分割的。今天，AI-机器人系统已成为我们日常生活的一部分，从吸尘器到半自动汽车。这些系统建立在三个基本建筑元素之上：感知、导航和规划，以及控制。然而，AI-机器人系统的集成也导致了一个严重的问题——这些系统容易受到安全攻击。物理组件、算法和数据，这些组成AI-机器人系统的元素可以被恶意攻击者滥用，可能导致严重的后果。为了解决AI-机器人系统的安全问题，本文提供了全面的调查和分类，涵盖三个维度：攻击表面、伦理和法律问题，以及人机交互安全。我们的目标是为用户、开发者和其他参与者提供一个整体的理解，以增强AI-机器人系统的安全性。我们开始是检查潜在的攻击表面，并提供防御策略。然后，我们详细讨论了伦理问题，如依赖和心理影响，以及法律问题，包括负责任的问题。此外，我们还讨论了新趋势，如人机交互，考虑隐私、完整性、安全、可靠性、可 explainer 的问题。最后，我们提出了未来研究方向的视野。
</details></li>
</ul>
<hr>
<h2 id="MemGPT-Towards-LLMs-as-Operating-Systems"><a href="#MemGPT-Towards-LLMs-as-Operating-Systems" class="headerlink" title="MemGPT: Towards LLMs as Operating Systems"></a>MemGPT: Towards LLMs as Operating Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08560">http://arxiv.org/abs/2310.08560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, Joseph E. Gonzalez</li>
<li>for: 该论文旨在解决现代大语言模型（LLM）受限于局部上下文窗口的问题，提高LLM在长 conversations 和文档分析等任务中的实用性。</li>
<li>methods: 该论文提出了虚拟上下文管理技术， drawing inspiration from hierarchical memory systems in traditional operating systems，以提供较大的上下文资源，并使用中断来管理控制流。</li>
<li>results: 在文档分析和多会话聊天两个领域中，MemGPT能够有效地提供extended context，超过了基于LLM的局部上下文窗口的性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Phenomenal-Yet-Puzzling-Testing-Inductive-Reasoning-Capabilities-of-Language-Models-with-Hypothesis-Refinement"><a href="#Phenomenal-Yet-Puzzling-Testing-Inductive-Reasoning-Capabilities-of-Language-Models-with-Hypothesis-Refinement" class="headerlink" title="Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement"></a>Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08559">http://arxiv.org/abs/2310.08559</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linlu-qiu/lm-inductive-reasoning">https://github.com/linlu-qiu/lm-inductive-reasoning</a></li>
<li>paper_authors: Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren</li>
<li>for: 这个研究旨在探讨语言模型（LM）在推理中的 inductive reasoning 能力，以及LM与人类 inductive reasoning 过程的差异。</li>
<li>methods: 研究使用了迭代假设细化（iterative hypothesis refinement）技术，包括提出、选择和细化假设的三个步骤，以模拟人类 inductive reasoning 过程。</li>
<li>results: 研究发现，LM 在 inductive reasoning 任务中表现出色，但也存在一些问题，如规则推理和应用等方面的表现下降，这表明LM 可能只是提出了假设而无法实际应用规则。此外，研究还发现了LM 和人类 inductive reasoning 过程之间的几个差异。<details>
<summary>Abstract</summary>
The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps in rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.
</details>
<details>
<summary>摘要</summary>
人类智能中的一个重要特点是从少量观察结果中推导出基本原则，然后将其应用到新的情况下。这种推导能力被称为推理，是人类智能的核心能力。尽管语言模型（LM）在研究 benchmark上表现出色，但在推理能力方面 frequently falls short。在这项工作中，我们通过迭代假设细化来系统地研究LM的推理能力，这种方法更加像人类的推理过程。迭代假设细化包括提出、选择和细化假设的三个步骤，通过分析中间规则，我们发现LM是出色的假设提出者（即生成候选规则），当与任务特定的符号化 интерпрета器相结合，这种混合方法在induction reasoning benchmarks上表现出强劲。然而，LMs也表现出了吸引人的推理行为，包括规则生成和规则应用的性能差距，这表明LMs在提出假设时不能实际应用规则。通过实验和人类分析，我们进一步揭示了LMs和人类在推理过程中的差异，这有助于理解LMs在推理任务中的潜在能力和局限性。
</details></li>
</ul>
<hr>
<h2 id="Offline-Retraining-for-Online-RL-Decoupled-Policy-Learning-to-Mitigate-Exploration-Bias"><a href="#Offline-Retraining-for-Online-RL-Decoupled-Policy-Learning-to-Mitigate-Exploration-Bias" class="headerlink" title="Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate Exploration Bias"></a>Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate Exploration Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08558">http://arxiv.org/abs/2310.08558</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MaxSobolMark/OOO">https://github.com/MaxSobolMark/OOO</a></li>
<li>paper_authors: Max Sobol Mark, Archit Sharma, Fahim Tajwar, Rafael Rafailov, Sergey Levine, Chelsea Finn</li>
<li>for: 该论文主要目标是提高在在线学习 Reinforcement Learning (RL) 中的策略训练效果，特别是在缺乏足够状态覆盖的情况下。</li>
<li>methods: 该论文提出了一种 Offline-to-Online-to-Offline (OOO) 框架，其中在在线finetuning过程中使用了一个optimistic（探索）策略和一个pessimistic（利用）策略。在这个框架中， optimistic策略用于与环境交互，而pessimistic策略则是根据所有观察到的数据进行训练。</li>
<li>results: 该论文的实验结果显示，OOO框架可以提高在线RL的性能，并且可以在缺乏足够状态覆盖的情况下进行策略训练。实验结果还表明，OOO框架可以与其他在线RL和离线RL方法相结合，并且可以在一些OpenAI gym环境上提高在线RL性能 by 165%。<details>
<summary>Abstract</summary>
It is desirable for policies to optimistically explore new states and behaviors during online reinforcement learning (RL) or fine-tuning, especially when prior offline data does not provide enough state coverage. However, exploration bonuses can bias the learned policy, and our experiments find that naive, yet standard use of such bonuses can fail to recover a performant policy. Concurrently, pessimistic training in offline RL has enabled recovery of performant policies from static datasets. Can we leverage offline RL to recover better policies from online interaction? We make a simple observation that a policy can be trained from scratch on all interaction data with pessimistic objectives, thereby decoupling the policies used for data collection and for evaluation. Specifically, we propose offline retraining, a policy extraction step at the end of online fine-tuning in our Offline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL). An optimistic (exploration) policy is used to interact with the environment, and a separate pessimistic (exploitation) policy is trained on all the observed data for evaluation. Such decoupling can reduce any bias from online interaction (intrinsic rewards, primacy bias) in the evaluation policy, and can allow more exploratory behaviors during online interaction which in turn can generate better data for exploitation. OOO is complementary to several offline-to-online RL and online RL methods, and improves their average performance by 14% to 26% in our fine-tuning experiments, achieves state-of-the-art performance on several environments in the D4RL benchmarks, and improves online RL performance by 165% on two OpenAI gym environments. Further, OOO can enable fine-tuning from incomplete offline datasets where prior methods can fail to recover a performant policy. Implementation: https://github.com/MaxSobolMark/OOO
</details>
<details>
<summary>摘要</summary>
<<SYS>>translation into Simplified Chinese<</SYS>>政策应该在在线强化学习（RL）或精度调整时，积极探索新状态和行为。特别是当前在线数据不够覆盖状态时，这对于政策的学习非常有利。然而，探索奖励可能会偏移学习的政策，我们的实验发现，标准使用探索奖励可能会失败回归高性能政策。同时，在线RL中的积极训练已经使得从静态数据中回归高性能政策成为可能。我们可以利用在线RL来回归更好的政策从在线互动中？我们提出了一个简单的观察：一个政策可以从所有互动数据中准备零，并使用消极目标进行训练。这可以减少在线互动中的偏见（内在奖励、优先级偏见），并允许在线互动中更多的探索行为，从而生成更好的数据进行利用。我们提出了在线重新训练（OOO）框架，它在在线 Fine-tuning 过程中使用一个积极（探索）政策和一个独立的消极（利用）政策进行训练。这种分离可以减少在线互动中的偏见，并允许更多的探索行为，从而提高在线RL的性能。OOO 与多种在线-to-Offline RL 和在线RL 方法相结合，可以提高均衡性能。我们的实验表明，OOO 可以在 D4RL  benchmark 上达到状态-of-the-art 性能，并在 OpenAI gym 中的两个环境上提高在线RL 性能 by 165%。此外，OOO 可以在无法回归高性能政策的情况下，从不完整的 Offline 数据进行 fine-tuning。实现：https://github.com/MaxSobolMark/OOO。
</details></li>
</ul>
<hr>
<h2 id="Cross-Episodic-Curriculum-for-Transformer-Agents"><a href="#Cross-Episodic-Curriculum-for-Transformer-Agents" class="headerlink" title="Cross-Episodic Curriculum for Transformer Agents"></a>Cross-Episodic Curriculum for Transformer Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08549">http://arxiv.org/abs/2310.08549</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CEC-Agent/CEC">https://github.com/CEC-Agent/CEC</a></li>
<li>paper_authors: Lucy Xiaoyang Shi, Yunfan Jiang, Jake Grigsby, Linxi “Jim” Fan, Yuke Zhu</li>
<li>for: 提高 transformer 代理的学习效率和通用性</li>
<li>methods: 跨话Context curriculum 方法</li>
<li>results: 在多任务 reinforcement learning 和模仿学习中表现出色，政策表现出超过比较者的优势和强大的通用性<details>
<summary>Abstract</summary>
We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents. Central to CEC is the placement of cross-episodic experiences into a Transformer's context, which forms the basis of a curriculum. By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes. Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism. The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings; and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators' expertise. In all instances, policies resulting from CEC exhibit superior performance and strong generalization. Code is open-sourced at https://cec-agent.github.io/ to facilitate research on Transformer agent learning.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的算法 named Cross-Episodic Curriculum (CEC), 用于提高 transformer 代理的学习效率和通用性。 CEC 的核心思想是在 transformer 的上下文中放置 cross-episodic 经验，这些经验组成了一个 curriculum。通过将在线学习课程和杂质示例进行顺序排序，CEC 构建了包含学习进程和能力提升的 curricula。这种同时利用 transformer 模型强大的模式识别能力和 curriculum 结构的 synergy，实现了一种强大的 cross-episodic 注意力机制。在 two 个代表性的场景中，CEC 的效果得到了证明：一个是在 DeepMind Lab 中进行多任务强化学习，其中 curriculum 捕捉了学习过程中的个体和逐渐复杂的设置；另一个是在 RoboMimic 中进行模仿学习，其中 curriculum 捕捉了示例师的专业水平提高。在所有情况下，由 CEC 生成的策略均显示出superior performance和强大的泛化能力。code 可以在 <https://cec-agent.github.io/> 上下载，以便研究 transformer 代理学习。
</details></li>
</ul>
<hr>
<h2 id="Do-pretrained-Transformers-Really-Learn-In-context-by-Gradient-Descent"><a href="#Do-pretrained-Transformers-Really-Learn-In-context-by-Gradient-Descent" class="headerlink" title="Do pretrained Transformers Really Learn In-context by Gradient Descent?"></a>Do pretrained Transformers Really Learn In-context by Gradient Descent?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08540">http://arxiv.org/abs/2310.08540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingfeng Shen, Aayush Mishra, Daniel Khashabi</li>
<li>for: 这种研究旨在检验是否存在某种潜在的相似性 между大语言模型中的增量学习（ICL）和梯度下降（GD）。</li>
<li>methods: 该研究使用了一种新的方法，即在大语言模型中使用Transformer网络进行学习，并通过对ICL和GD进行比较，检验它们之间的关系。</li>
<li>results: 研究发现，ICL和GD在不同的数据集、模型和示例数下 exhibit 不同的行为，表明它们之间并不是完全相同的。这些结果 Suggests 存在一些假设不符的问题，需要进一步的研究以确认它们的等价性。<details>
<summary>Abstract</summary>
Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)? Several recent works draw analogies between the dynamics of GD and the emergent behavior of ICL in large language models. However, these works make assumptions far from the realistic natural language setting in which language models are trained. Such discrepancies between theory and practice, therefore, necessitate further investigation to validate their applicability.   We start by highlighting the weaknesses in prior works that construct Transformer weights to simulate gradient descent. Their experiments with training Transformers on ICL objective, inconsistencies in the order sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity to parameter changes are some examples of a mismatch from the real-world setting.   Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pretrained on natural data (LLaMa-7B). Our comparisons on various performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and number of demonstrations. We observe that ICL and GD adapt the output distribution of language models differently. These results indicate that the equivalence between ICL and GD is an open hypothesis, requires nuanced considerations and calls for further studies.
</details>
<details>
<summary>摘要</summary>
是否存在卷积下降（GD）与Context Learning（ICL）的隐式等价？一些最近的研究将GD和ICL的动力学比作，但这些研究假设了不realistic的自然语言训练环境，导致了与实际情况之间的差异。因此，进一步的调查是必要的以验证其可靠性。我们开始于 highlighting priors works的缺陷，它们通过构建Transformer weights来模拟GD。他们在训练Transformers时使用ICL目标，但存在一些不一致的问题，如ICL和GD的敏感性顺序、稀疏的构建矩阵、和参数变化的敏感性。此外，我们进行了ICL vs. GD的比较，并在自然 Setting中进行了广泛的实验分析。我们在使用自然数据（LLaMa-7B）预训练的语言模型上进行了多种表现指标的比较。我们发现，ICL和GD在不同的数据集、模型和示例数目上 exhibit不一致的行为。这些结果表明，ICL和GD的等价性是一个开放的假设，需要细致的考虑和进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="Formally-Specifying-the-High-Level-Behavior-of-LLM-Based-Agents"><a href="#Formally-Specifying-the-High-Level-Behavior-of-LLM-Based-Agents" class="headerlink" title="Formally Specifying the High-Level Behavior of LLM-Based Agents"></a>Formally Specifying the High-Level Behavior of LLM-Based Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08535">http://arxiv.org/abs/2310.08535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxwell Crouse, Ibrahim Abdelaziz, Kinjal Basu, Soham Dan, Sadhana Kumaravel, Achille Fokoue, Pavan Kapanipathi, Luis Lastras<br>for:LLM-based agents are promising tools for solving challenging problems without the need for task-specific finetuned models.methods:The proposed framework uses Linear Temporal Logic (LTL) to specify desired agent behaviors, and a constrained decoder to guarantee the LLM will produce an output exhibiting the desired behavior.results:The framework enables rapid design, implementation, and experimentation with different LLM-based agents, and provides benefits such as the ability to enforce complex agent behavior, formally validate prompt examples, and incorporate content-focused logical constraints into generation. The approach leads to improvements in agent performance, and the code is released for general use.Here is the text in Simplified Chinese:for: LLM-based agents 是一种可以解决复杂问题的有前途的工具，无需特定任务的精心适应模型。methods: 提议的框架使用线性时间逻辑（LTL）来指定代理行为，并使用受限的解码器来保证 LLM 生成输出符合所需的行为。results: 该框架可以快速设计、实现和测试不同的 LLM-based agents，并提供了一些优点，如强制执行复杂的代理行为、正式验证提示示例、内容专注的逻辑约束的 incorporation into generation。该方法可以提高代理性能，并公开发布代码。<details>
<summary>Abstract</summary>
LLM-based agents have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic, high-level generation framework that simplifies the process of building agents. The framework we introduce allows the user to specify desired agent behaviors in Linear Temporal Logic (LTL). The declarative LTL specification is then used to construct a constrained decoder that guarantees the LLM will produce an output exhibiting the desired behavior. By designing our framework in this way, we obtain several benefits, including the ability to enforce complex agent behavior, the ability to formally validate prompt examples, and the ability to seamlessly incorporate content-focused logical constraints into generation. In particular, our declarative approach, in which the desired behavior is simply described without concern for how it should be implemented or enforced, enables rapid design, implementation and experimentation with different LLM-based agents. We demonstrate how the proposed framework can be used to implement recent LLM-based agents, and show how the guardrails our approach provides can lead to improvements in agent performance. In addition, we release our code for general use.
</details>
<details>
<summary>摘要</summary>
The framework we introduce allows the user to specify desired agent behaviors in Linear Temporal Logic (LTL). The declarative LTL specification is then used to construct a constrained decoder that guarantees the LLM will produce an output exhibiting the desired behavior. By designing our framework in this way, we obtain several benefits, including the ability to enforce complex agent behavior, the ability to formally validate prompt examples, and the ability to seamlessly incorporate content-focused logical constraints into generation.In particular, our declarative approach, in which the desired behavior is simply described without concern for how it should be implemented or enforced, enables rapid design, implementation, and experimentation with different LLM-based agents. We demonstrate how the proposed framework can be used to implement recent LLM-based agents, and show how the guardrails our approach provides can lead to improvements in agent performance. Additionally, we release our code for general use.
</details></li>
</ul>
<hr>
<h2 id="How-connectivity-structure-shapes-rich-and-lazy-learning-in-neural-circuits"><a href="#How-connectivity-structure-shapes-rich-and-lazy-learning-in-neural-circuits" class="headerlink" title="How connectivity structure shapes rich and lazy learning in neural circuits"></a>How connectivity structure shapes rich and lazy learning in neural circuits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08513">http://arxiv.org/abs/2310.08513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Helena Liu, Aristide Baratin, Jonathan Cornford, Stefan Mihalas, Eric Shea-Brown, Guillaume Lajoie</li>
<li>for: 这个论文探讨了深度学习工具如何用于研究神经网络学习动态。</li>
<li>methods: 这篇论文使用了实验和理论分析来研究初始积分特性如何影响神经网络的学习 режим。</li>
<li>results: 研究发现，高级别初始积分通常导致小变化的网络学习 режим，而低级别初始积分则导致更加丰富的学习 режим。<details>
<summary>Abstract</summary>
In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics. Notably, initial weight distributions with small (resp. large) variance may yield a rich (resp. lazy) regime, where significant (resp. minor) changes to network states and representation are observed over the course of learning. However, in biology, neural circuit connectivity generally has a low-rank structure and therefore differs markedly from the random initializations generally used for these studies. As such, here we investigate how the structure of the initial weights, in particular their effective rank, influences the network learning regime. Through both empirical and theoretical analyses, we discover that high-rank initializations typically yield smaller network changes indicative of lazier learning, a finding we also confirm with experimentally-driven initial connectivity in recurrent neural networks. Conversely, low-rank initialization biases learning towards richer learning. Importantly, however, as an exception to this rule, we find lazier learning can still occur with a low-rank initialization that aligns with task and data statistics. Our research highlights the pivotal role of initial weight structures in shaping learning regimes, with implications for metabolic costs of plasticity and risks of catastrophic forgetting.
</details>
<details>
<summary>摘要</summary>
在理论神经科学中，最近的工作利用深度学习工具来探索如何某些网络特性影响其学习动态。特别是，初始 веса分布有小（resp. 大）方差可能导致一个丰富（resp. 懒散）的学习模式，其中网络状态和表示有 significiant（resp. 微不足）的变化。然而，生物中神经Circuit连接通常具有低维结构，因此与通常用于这些研究的随机初始化不同。因此，我们 investigate how the structure of the initial weights, particularly their effective rank, influences the network learning regime.通过实验和理论分析，我们发现高维初始化通常导致小网络变化，表示懒散学习，而低维初始化启动学习更加丰富。然而，我们发现在任务和数据统计相align的低维初始化下，可以occurrence lazier learning。我们的研究强调初始 веса结构在形成学习模式的作用，有关 метаболиic cost of plasticity和忘记风险。
</details></li>
</ul>
<hr>
<h2 id="HoneyBee-Progressive-Instruction-Finetuning-of-Large-Language-Models-for-Materials-Science"><a href="#HoneyBee-Progressive-Instruction-Finetuning-of-Large-Language-Models-for-Materials-Science" class="headerlink" title="HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science"></a>HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08511">http://arxiv.org/abs/2310.08511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee">https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee</a></li>
<li>paper_authors: Yu Song, Santiago Miret, Huan Zhang, Bang Liu</li>
<li>for: 本研究的目的是提出一种信任worthy数据准备过程（MatSci-Instruct），并应用其在语言模型中进行迭代优化（HoneyBee），以解决物理科学领域的数据准备问题。</li>
<li>methods: 本研究使用了多个商业可用的大语言模型（如Chat-GPT和Claude），通过Instructor模块和Verifier模块的合作，提高生成的数据的可靠性和相关性。</li>
<li>results: 本研究通过MatSci-Instruct来构建多个任务的数据集，并评估了数据集的质量从多个维度，包括准确性、相关性、完整性和合理性。此外，本研究还通过迭代生成更加定向的指令和指令数据来进行迭代优化，以达到进一步改进HoneyBee模型的性能。<details>
<summary>Abstract</summary>
We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee). MatSci-Instruct helps alleviate the scarcity of relevant, high-quality materials science textual data available in the open literature, and HoneyBee is the first billion-parameter language model specialized to materials science. In MatSci-Instruct we improve the trustworthiness of generated data by prompting multiple commercially available large language models for generation with an Instructor module (e.g. Chat-GPT) and verification from an independent Verifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of multiple tasks and measure the quality of our dataset along multiple dimensions, including accuracy against known facts, relevance to materials science, as well as completeness and reasonableness of the data. Moreover, we iteratively generate more targeted instructions and instruction-data in a finetuning-evaluation-feedback loop leading to progressively better performance for our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark shows HoneyBee's outperformance of existing language models on materials science tasks and iterative improvement in successive stages of instruction-data refinement. We study the quality of HoneyBee's language modeling through automatic evaluation and analyze case studies to further understand the model's capabilities and limitations. Our code and relevant datasets are publicly available at \url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee}.
</details>
<details>
<summary>摘要</summary>
我们提出一种基于 instrucion 的数据纯化 процесс，称为 MatSci-Instruct，用于提高材料科学领域的数据质量。我们 THEN 使用这种 processto 训练一个基于 LLaMa 语言模型，称为 HoneyBee，以提高材料科学领域的语言模型性能。MatSci-Instruct 可以帮助解决开 literature 中材料科学领域的相关、高质量文本数据的缺乏问题，HoneyBee 是首个专门针对材料科学的一千亿参数语言模型。在 MatSci-Instruct 中，我们通过多个商业可用的大语言模型（例如 Chat-GPT 和 Claude）的干预和独立验证模块的验证来提高生成数据的可靠性。我们使用 MatSci-Instruct 构建多个任务的数据集，并对数据集进行多维度评估，包括准确性、 relevance、完整性和合理性。此外，我们在 finetuning-evaluation-feedback 循环中不断生成更加定向的 instructon-data，导致我们的 fine-tuned HoneyBee 模型的表现不断改善。我们在 MatSci-NLP benchmark 上进行评估，发现 HoneyBee 对材料科学任务的表现优于现有语言模型，并在 successive stages of instruction-data refinement 中进行Iterative improvement。我们通过自动评估和案例研究来深入了解 HoneyBee 模型的能力和局限性。我们的代码和相关数据集可以在 \url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-time-and-note-duration-tokenizations-on-deep-learning-symbolic-music-modeling"><a href="#Impact-of-time-and-note-duration-tokenizations-on-deep-learning-symbolic-music-modeling" class="headerlink" title="Impact of time and note duration tokenizations on deep learning symbolic music modeling"></a>Impact of time and note duration tokenizations on deep learning symbolic music modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08497">http://arxiv.org/abs/2310.08497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Natooz/music-modeling-time-duration">https://github.com/Natooz/music-modeling-time-duration</a></li>
<li>paper_authors: Nathan Fradet, Nicolas Gutowski, Fabien Chhel, Jean-Pierre Briot</li>
<li>for: 本研究旨在研究Symbolic music在深度学习任务中的应用，包括生成、识别、合成和Music Information Retrieval（MIR）等。</li>
<li>methods: 本研究使用了不同的tokenization方法，包括时间和音长表示方法，以研究这些方法对Transformer模型的表现的影响。</li>
<li>results: 研究发现，виси于任务，explicit信息可以提高表现，而time和音长表示方法在不同任务中的表现有所不同。<details>
<summary>Abstract</summary>
Symbolic music is widely used in various deep learning tasks, including generation, transcription, synthesis, and Music Information Retrieval (MIR). It is mostly employed with discrete models like Transformers, which require music to be tokenized, i.e., formatted into sequences of distinct elements called tokens. Tokenization can be performed in different ways. As Transformer can struggle at reasoning, but capture more easily explicit information, it is important to study how the way the information is represented for such model impact their performances. In this work, we analyze the common tokenization methods and experiment with time and note duration representations. We compare the performances of these two impactful criteria on several tasks, including composer and emotion classification, music generation, and sequence representation learning. We demonstrate that explicit information leads to better results depending on the task.
</details>
<details>
<summary>摘要</summary>
Symbolic music 广泛应用于深度学习任务中，包括生成、识别、合成和音乐信息检索（MIR）。它通常与分割模型如转换器结合使用，这些模型需要音乐被格式化为序列中的固定元素，即token。格式化可以通过不同的方式进行，而转换器可能会很难理解，但可以较容易捕捉明确的信息。因此，我们需要研究不同的表示方式对这种模型的性能产生何种影响。在这项工作中，我们分析了常见的tokenization方法，并对时间和音符持续时间的表示进行实验。我们比较了这两个重要的标准准则在不同任务中的表现，包括作曲和情感分类、音乐生成和序列表示学习。我们发现，明确的信息会带来更好的结果，具体取决于任务。
</details></li>
</ul>
<hr>
<h2 id="Can-We-Edit-Multimodal-Large-Language-Models"><a href="#Can-We-Edit-Multimodal-Large-Language-Models" class="headerlink" title="Can We Edit Multimodal Large Language Models?"></a>Can We Edit Multimodal Large Language Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08475">http://arxiv.org/abs/2310.08475</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/easyedit">https://github.com/zjunlp/easyedit</a></li>
<li>paper_authors: Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, Ningyu Zhang</li>
<li>for: 这个论文主要关注于编辑多Modal大型自然语言模型（MLLMs）。与单Modal模型编辑相比，多Modal模型编辑更加具有挑战性，需要更高的级别的精检和谨慎的考虑。为促进这一领域的研究，我们建立了一个新的标准 benchmark，名为MMEdit，并开发了一组创新的评价指标。</li>
<li>methods: 我们在这个论文中采用了多种模型编辑基线和评价指标，并进行了广泛的实验。我们发现，之前的基线可以在一定程度上实现编辑多Modal LLMs，但效果仍然很有限，这表明这个任务可能比较困难。</li>
<li>results: 我们的实验结果表明，之前的基线可以在一定程度上实现编辑多Modal LLMs，但效果仍然很有限。我们希望通过这个研究，为NLP社区提供一些新的想法和灵感。代码和数据集可以在<a target="_blank" rel="noopener" href="https://github.com/zjunlp/EasyEdit%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/zjunlp/EasyEdit中下载。</a><details>
<summary>Abstract</summary>
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注编辑多Modal Large Language Models（MLLMs）。与单modal LLMs 编辑相比，多modal 模型编辑更加具有挑战性，需要更高的审核和谨慎评估。为促进这一领域的研究，我们构建了一个新的标准测试集，名为MMEdit，并开发了一套创新的评价指标。我们进行了对多modal LLMs 编辑不同组件的全面实验，并分析了不同组件的编辑对多modal LLMs 的影响。实验结果表明，前一代基eline可以部分地编辑多modal LLMs，但效果仍然很有限，表明这是一项具有挑战性的任务。我们希望通过这项工作，为NLP社区提供新的想法。代码和数据集可以在https://github.com/zjunlp/EasyEdit 上找到。
</details></li>
</ul>
<hr>
<h2 id="Belief-formation-and-the-persistence-of-biased-beliefs"><a href="#Belief-formation-and-the-persistence-of-biased-beliefs" class="headerlink" title="Belief formation and the persistence of biased beliefs"></a>Belief formation and the persistence of biased beliefs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08466">http://arxiv.org/abs/2310.08466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Compte</li>
<li>for: 本研究旨在描述智能代理如何在决策过程中处理信息，以及如何偏袋证据导致的偏见。</li>
<li>methods: 本研究使用了一种假设形成模型，其中代理尝试将两个理论区分开，并且因为证据的强度差异，倾向于接受具有强（可能罕见）证据的理论。</li>
<li>results: 研究发现，由于信息处理限制，代理可能会剪辑弱证据，导致一些歧义问题中的证据变得一面。更加聪明的代理不会受到这些偏袋证据的影响，但是一些不那么聪明的代理可能会偏袋其信念。<details>
<summary>Abstract</summary>
We propose a belief-formation model where agents attempt to discriminate between two theories, and where the asymmetry in strength between confirming and disconfirming evidence tilts beliefs in favor of theories that generate strong (and possibly rare) confirming evidence and weak (and frequent) disconfirming evidence. In our model, limitations on information processing provide incentives to censor weak evidence, with the consequence that for some discrimination problems, evidence may become mostly one-sided, independently of the true underlying theory. Sophisticated agents who know the characteristics of the censored data-generating process are not lured by this accumulation of ``evidence'', but less sophisticated ones end up with biased beliefs.
</details>
<details>
<summary>摘要</summary>
我们提出了一种信仰形成模型，在这个模型中，代理人尝试区分两个理论，而差异强度 между证实和驳斥证据使得信仰倾向于强大（可能罕见）的证实证据和弱（常见）的驳斥证据。在我们的模型中，信息处理的限制提供了奖励自我ensorcement的机会，导致一些推理问题上的证据变得一面，独立于真实下面理论。更加了解的代理人不会受到这些偏见的影响，但是不那么了解的代理人则会受到偏见。
</details></li>
</ul>
<hr>
<h2 id="DistillSpec-Improving-Speculative-Decoding-via-Knowledge-Distillation"><a href="#DistillSpec-Improving-Speculative-Decoding-via-Knowledge-Distillation" class="headerlink" title="DistillSpec: Improving Speculative Decoding via Knowledge Distillation"></a>DistillSpec: Improving Speculative Decoding via Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08461">http://arxiv.org/abs/2310.08461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, Rishabh Agarwal</li>
<li>for: 这个论文旨在提高大型语言模型的推导速度，使用快速的范本模型生成多个 tokens，然后在平行验证过程中运用更大的目标模型来生成文本，根据目标模型的分布。</li>
<li>methods: 这个方法使用知识传递来更好地调整范本模型和目标模型之间的对齐性，然后通过快速推导来实现文本生成。</li>
<li>results: 这个方法可以在多个标准参数上获得很好的速度提升，从10%到45%不等，并且可以在不同的标准参数和推导策略下进行精确的调整。此外，这个方法可以与丧失SD结合，以控制推导延误和任务性能的贸易。最后，这个方法可以在实际的实验中，使用对齐模型来实现6-10倍的延误缩减，而且几乎没有性能下降。<details>
<summary>Abstract</summary>
Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by 6-10x with minimal performance drop, compared to standard decoding without distillation.
</details>
<details>
<summary>摘要</summary>
假设解oding（SD）可以加速大型语言模型的推断，通过使用更快的稿本模型来生成多个字元，然后在平行验证这些字元的准确性，以生成根据目标模型分布的文本。但是，找到一个具有单位大小的稿本模型，与目标模型相互Alignment是一个挑战。为了解决这个问题，我们提出了DistillSpec，它使用知识传播来更好地对稿本模型和目标模型进行Alignment。DistillSpec做出了两项重要的设计决策，我们通过系统性的研究证明这些设计决策是关键的提高稿本和目标模型的Alignment：使用稿本模型生成的在policy数据来验证稿本模型，并调整差异函数以适应任务和推断策略。特别是，DistillSpec可以在标准 benchmark 上获得了10-45%的提高，使用了 both greedy 和 non-greedy 推断。此外，我们可以将DistillSpec与lossy SD 结合，以获得精确的任务性能和时延调整。最后，在实际应用中，首先使用对target模型进行增强，然后使用DistillSpec对稿本模型进行训练，可以将推断时间降低6-10倍，而且几乎没有性能下降。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Heterogeneous-Transfer-Learning"><a href="#A-Survey-of-Heterogeneous-Transfer-Learning" class="headerlink" title="A Survey of Heterogeneous Transfer Learning"></a>A Survey of Heterogeneous Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08459">http://arxiv.org/abs/2310.08459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ymsun99/Heterogeneous-Transfer-Learning">https://github.com/ymsun99/Heterogeneous-Transfer-Learning</a></li>
<li>paper_authors: Runxue Bao, Yiming Sun, Yuhe Gao, Jindong Wang, Qiang Yang, Haifeng Chen, Zhi-Hong Mao, Ye Ye</li>
<li>for: 本研究旨在提供一份彻悟的综述，涵盖最新的非同一致学习方法的发展，以帮助未来的研究。</li>
<li>methods: 本文总结了不同学习场景下的多样化学习方法，包括自适应学习、卷积神经网络、隐藏状态模型、等方法，以及它们在不同应用场景中的应用。</li>
<li>results: 本文综述了不同领域中的实验结果，包括自然语言处理、计算机视觉、多模式识别、生物医学等领域，以及它们的应用场景和限制。<details>
<summary>Abstract</summary>
The application of transfer learning, an approach utilizing knowledge from a source domain to enhance model performance in a target domain, has seen a tremendous rise in recent years, underpinning many real-world scenarios. The key to its success lies in the shared common knowledge between the domains, a prerequisite in most transfer learning methodologies. These methods typically presuppose identical feature spaces and label spaces in both domains, known as homogeneous transfer learning, which, however, is not always a practical assumption. Oftentimes, the source and target domains vary in feature spaces, data distributions, and label spaces, making it challenging or costly to secure source domain data with identical feature and label spaces as the target domain. Arbitrary elimination of these differences is not always feasible or optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with such disparities, has emerged as a promising approach for a variety of tasks. Despite the existence of a survey in 2017 on this topic, the fast-paced advances post-2017 necessitate an updated, in-depth review. We therefore present a comprehensive survey of recent developments in heterogeneous transfer learning methods, offering a systematic guide for future research. Our paper reviews methodologies for diverse learning scenarios, discusses the limitations of current studies, and covers various application contexts, including Natural Language Processing, Computer Vision, Multimodality, and Biomedicine, to foster a deeper understanding and spur future research.
</details>
<details>
<summary>摘要</summary>
“将学习传播技术应用到目标领域，以优化模型表现，在过去几年中获得了巨大的发展，支撑了许多实际应用场景。这些方法通常假设源领域和目标领域之间存在共同知识，这是传统的传播学习方法的前提。然而，这些方法通常假设源领域和目标领域之间存在同样的特征空间和标签空间，这称为同样的传播学习。然而，这种假设不一定是实际可行或优化的。因此，不同领域之间的传播学习，承认和处理这些差异，已经成为一种有前途的方法。尽管在2017年已经有一篇关于这个主题的调查，但随着时间的推移，这些领域的发展速度很快，因此我们需要一份更新、更深入的评论。我们因此提出了一份综观最近几年传播学习方法的综观，实现了系统化的引导。我们的评论涵盖了多种学习enario，讨论了现有研究的限制，并涵盖了不同应用场景，包括自然语言处理、computer vision、多模式和生医，以促进更深入的理解和未来研究。”
</details></li>
</ul>
<hr>
<h2 id="Metrics-for-popularity-bias-in-dynamic-recommender-systems"><a href="#Metrics-for-popularity-bias-in-dynamic-recommender-systems" class="headerlink" title="Metrics for popularity bias in dynamic recommender systems"></a>Metrics for popularity bias in dynamic recommender systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08455">http://arxiv.org/abs/2310.08455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentijn Braun, Debarati Bhaumik, Diptish Dey</li>
<li>for: 这篇论文主要目标是量化推荐系统中的不公正和偏见。</li>
<li>methods: 论文提出了四种度量推荐系统中受欢迎性偏见的指标，并在两个常用的 benchmark 数据集上测试了四种 collaborative filtering 算法。</li>
<li>results: 测试结果表明，提出的度量指标可以为推荐系统的不公正和偏见提供全面的理解，并且在不同的敏感用户群体中存在增长的差距。<details>
<summary>Abstract</summary>
Albeit the widespread application of recommender systems (RecSys) in our daily lives, rather limited research has been done on quantifying unfairness and biases present in such systems. Prior work largely focuses on determining whether a RecSys is discriminating or not but does not compute the amount of bias present in these systems. Biased recommendations may lead to decisions that can potentially have adverse effects on individuals, sensitive user groups, and society. Hence, it is important to quantify these biases for fair and safe commercial applications of these systems. This paper focuses on quantifying popularity bias that stems directly from the output of RecSys models, leading to over recommendation of popular items that are likely to be misaligned with user preferences. Four metrics to quantify popularity bias in RescSys over time in dynamic setting across different sensitive user groups have been proposed. These metrics have been demonstrated for four collaborative filtering based RecSys algorithms trained on two commonly used benchmark datasets in the literature. Results obtained show that the metrics proposed provide a comprehensive understanding of growing disparities in treatment between sensitive groups over time when used conjointly.
</details>
<details>
<summary>摘要</summary>
This paper aims to address this issue by quantifying popularity bias in RecSys, which stems directly from the output of the models and leads to the over-recommendation of popular items that may be misaligned with user preferences. To achieve this, four metrics have been proposed to quantify popularity bias in RecSys over time in a dynamic setting across different sensitive user groups. These metrics have been demonstrated for four collaborative filtering-based RecSys algorithms trained on two commonly used benchmark datasets in the literature.The results obtained show that the proposed metrics provide a comprehensive understanding of the growing disparities in treatment between sensitive groups over time when used conjointly. This study contributes to the development of fair and safe RecSys by providing a quantitative approach to identify and mitigate popularity bias.
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-Multi-Modal-Reasoning-via-Model-Selection"><a href="#Towards-Robust-Multi-Modal-Reasoning-via-Model-Selection" class="headerlink" title="Towards Robust Multi-Modal Reasoning via Model Selection"></a>Towards Robust Multi-Modal Reasoning via Model Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08446">http://arxiv.org/abs/2310.08446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyan Liu, Rongxue Li, Wei Ji, Tao Lin</li>
<li>For: This paper aims to improve the robustness of multi-modal agents in multi-step reasoning by addressing the challenge of model selection.* Methods: The paper proposes the $\textit{M}^3$ framework, a plug-in with negligible runtime overhead at test-time, to improve model selection and bolster the robustness of multi-modal agents.* Results: The paper creates a new dataset, MS-GQA, to investigate the model selection challenge in multi-modal agents and shows that the proposed framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process.<details>
<summary>Abstract</summary>
The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the "brain" of agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning.   To this end, we identify the key challenges therein and propose the $\textit{M}^3$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https://github.com/LINs-lab/M3.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的智能能力在最新的研究中得到了广泛认可，激发了工具学习和自主代理研究。LLM作为代理的“脑”，整合多种工具进行合作多步任务解决。与传统的方法不同，现在的多模态代理忽略了模型选择的重要性：它们主要关注计划和执行阶段，只在每个子任务中预先定义任务特定的模型，使执行过程脆弱。此外，传统的模型选择方法在多模态代理场景中不兼容或优化不够，因为忽略了由多步逻辑导致的任务依赖关系。为了解决这些挑战，我们认为需要一个可插入的框架，具有较少的运行时开销。我们称之为$\textit{M}^3$框架，它可以在测试时进行插入。这个框架改进了模型选择，使多模态代理在多步逻辑中更加稳定。由于缺乏适当的benchmark，我们创建了MS-GQA数据集，用于调查多模态代理中模型选择挑战的问题。我们的实验表明，我们的框架可以动态选择模型，考虑用户输入和子任务依赖关系，从而强化整体逻辑过程的稳定性。我们的代码和benchmark可以在GitHub上找到：https://github.com/LINs-lab/M3。
</details></li>
</ul>
<hr>
<h2 id="Debias-the-Training-of-Diffusion-Models"><a href="#Debias-the-Training-of-Diffusion-Models" class="headerlink" title="Debias the Training of Diffusion Models"></a>Debias the Training of Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08442">http://arxiv.org/abs/2310.08442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hu Yu, Li Shen, Jie Huang, Man Zhou, Hongsheng Li, Feng Zhao</li>
<li>for: 提高Diffusion模型的生成质量</li>
<li>methods: 提出了一种有效的权重调整策略，以解决常用的损失函数策略带来的偏见问题</li>
<li>results: 通过理论分析和实验评估，证明了该策略可以减少偏见问题，并提高样本质量和生成效率<details>
<summary>Abstract</summary>
Diffusion models have demonstrated compelling generation quality by optimizing the variational lower bound through a simple denoising score matching loss. In this paper, we provide theoretical evidence that the prevailing practice of using a constant loss weight strategy in diffusion models leads to biased estimation during the training phase. Simply optimizing the denoising network to predict Gaussian noise with constant weighting may hinder precise estimations of original images. To address the issue, we propose an elegant and effective weighting strategy grounded in the theoretically unbiased principle. Moreover, we conduct a comprehensive and systematic exploration to dissect the inherent bias problem deriving from constant weighting loss from the perspectives of its existence, impact and reasons. These analyses are expected to advance our understanding and demystify the inner workings of diffusion models. Through empirical evaluation, we demonstrate that our proposed debiased estimation method significantly enhances sample quality without the reliance on complex techniques, and exhibits improved efficiency compared to the baseline method both in training and sampling processes.
</details>
<details>
<summary>摘要</summary>
Diffusion models 已经展示了吸引人的生成质量，通过简单的降噪对应loss来优化variational lower bound。在这篇论文中，我们提供了理论证明，表明常用的常数损失重量策略在Diffusion models中导致训练阶段的估计偏见。只是优化降噪网络以预测 Gaussian noise 的常数权重，可能会妨碍精准估计原始图像。为解决这个问题，我们提议一种精美和有效的权重策略，基于理论上的无偏估计原理。此外，我们进行了系统性的探索，析分了常数损失重量导致的内在偏见问题的存在、影响和原因。这些分析将有助于我们更深入理解Diffusion models的内部工作机制。通过实验评估，我们示出了我们提议的减偏估计方法可以大幅提高样本质量，不需要复杂的技术，并且在训练和采样过程中比基eline方法更高效。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Explanations-on-Fairness-in-Human-AI-Decision-Making-Protected-vs-Proxy-Features"><a href="#The-Impact-of-Explanations-on-Fairness-in-Human-AI-Decision-Making-Protected-vs-Proxy-Features" class="headerlink" title="The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features"></a>The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08617">http://arxiv.org/abs/2310.08617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navita Goyal, Connor Baumler, Tin Nguyen, Hal Daumé III</li>
<li>for: 本研究旨在 investigating the effect of protected and proxy features on participants’ perception of model fairness and their ability to improve demographic parity over an AI alone.</li>
<li>methods: 本研究使用了不同的treatments，包括解释、模型偏见披露和代理相关性披露，以影响人们对模型公平性的识别和决策公平性。</li>
<li>results: 研究发现，解释可以帮助人们检测直接偏见，但不能帮助人们检测间接偏见。此外，无论偏见类型如何，解释都会增加对模型偏见的同意。披露可以减轻这种效果，提高不公正认知和决策公平性。<details>
<summary>Abstract</summary>
AI systems have been known to amplify biases in real world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants' perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments -- explanations, model bias disclosure and proxy correlation disclosure -- affect fairness perception and parity. We find that explanations help people detect direct biases but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model biases. Disclosures can help mitigate this effect for indirect biases, improving both unfairness recognition and the decision-making fairness. We hope that our findings can help guide further research into advancing explanations in support of fair human-AI decision-making.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neural-Sampling-in-Hierarchical-Exponential-family-Energy-based-Models"><a href="#Neural-Sampling-in-Hierarchical-Exponential-family-Energy-based-Models" class="headerlink" title="Neural Sampling in Hierarchical Exponential-family Energy-based Models"></a>Neural Sampling in Hierarchical Exponential-family Energy-based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08431">http://arxiv.org/abs/2310.08431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingsi Dong, Si Wu</li>
<li>for: 这个论文旨在探讨脑海的推理和学习方法。</li>
<li>methods: 该论文提出了 Hierarchical Exponential-family Energy-based（HEE）模型，该模型可以同时进行推理和学习，并且可以通过采样神经元响应的梯度来估计归一化函数。</li>
<li>results: 该模型可以快速地进行推理和学习，并且可以在自然图像 datasets 上显示出类似于生物视觉系统中的表示。此外，该模型还可以通过 marginal generation 或 joint generation 生成观察结果，并且 marginal generation 可以达到与其他 EBMs 相同的性能。<details>
<summary>Abstract</summary>
Bayesian brain theory suggests that the brain employs generative models to understand the external world. The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses. Additionally, the brain continually updates its generative model to approach the true distribution of the external world. In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning. In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term. This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs). As a result, the learning process is localized both in time and space, and the model is easy to converge. To match the brain's rapid computation, we demonstrate that neural adaptation can serve as a momentum term, significantly accelerating the inference process. On natural image datasets, our model exhibits representations akin to those observed in the biological visual system. Furthermore, for the machine learning community, our model can generate observations through joint or marginal generation. We show that marginal generation outperforms joint generation and achieves performance on par with other EBMs.
</details>
<details>
<summary>摘要</summary>
bayesian 脑理论 suggets that the brain 使用生成模型来理解外部世界。 sampling-based 观点认为脑内部INFERS posterior distribution 通过抽样 Stochastic neuronal responses。 此外，脑 continually 更新其生成模型，以 approaching true distribution 外部世界。 在这种研究中，我们引入 Hierarchical Exponential-family Energy-based (HEE) 模型，该模型捕捉了推理和学习的动力学。在 HEE 模型中，我们将 partition function 分解成各层，并利用一组具有 shorter time constants 的 neurons 来抽样分解 normalization term 的梯度。 这 permit our model 可以估算 partition function 并同时进行推理，而不是在 conventional energy-based models (EBMs) 中遇到的负相位。 因此，学习过程是在时间和空间上局部化的，模型易于收敛。 为了匹配脑的快速计算，我们示出 neural adaptation 可以作为推理过程中的推进量，帮助加速推理过程。 在自然图像数据集上，我们的模型表现出类似于生物视觉系统中的表征。 此外，为机器学习社区，我们的模型可以通过 joint 或 marginal generation 生成观测。 我们表明 marginal generation 超过 joint generation，并达到与其他 EBMs 相同的性能。
</details></li>
</ul>
<hr>
<h2 id="DeltaSpace-A-Semantic-aligned-Feature-Space-for-Flexible-Text-guided-Image-Editing"><a href="#DeltaSpace-A-Semantic-aligned-Feature-Space-for-Flexible-Text-guided-Image-Editing" class="headerlink" title="DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided Image Editing"></a>DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided Image Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08785">http://arxiv.org/abs/2310.08785</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yueming6568/deltaedit">https://github.com/yueming6568/deltaedit</a></li>
<li>paper_authors: Yueming Lyu, Kang Zhao, Bo Peng, Yue Jiang, Yingya Zhang, Jing Dong</li>
<li>for: 文章主要旨在提高文本引导图像修改的训练和推理灵活性。</li>
<li>methods: 文章提出了一种基于 CLIP  delta 空间的新方法，称为 deltaedit，它可以在训练阶段将 CLIP 视觉特征差映射到生成模型的幂空间方向上，并在推理阶段通过 CLIP 文本特征差来预测幂空间方向。</li>
<li>results: 实验证明， deltaedit 可以在不同的生成模型（包括 GAN 模型和扩散模型）上实现文本引导图像修改的灵活性，并且可以在不同的文本描述下进行零shot推理。<details>
<summary>Abstract</summary>
Text-guided image editing faces significant challenges to training and inference flexibility. Much literature collects large amounts of annotated image-text pairs to train text-conditioned generative models from scratch, which is expensive and not efficient. After that, some approaches that leverage pre-trained vision-language models are put forward to avoid data collection, but they are also limited by either per text-prompt optimization or inference-time hyper-parameters tuning. To address these issues, we investigate and identify a specific space, referred to as CLIP DeltaSpace, where the CLIP visual feature difference of two images is semantically aligned with the CLIP textual feature difference of their corresponding text descriptions. Based on DeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP visual feature differences to the latent space directions of a generative model during the training phase, and predicts the latent space directions from the CLIP textual feature differences during the inference phase. And this design endows DeltaEdit with two advantages: (1) text-free training; (2) generalization to various text prompts for zero-shot inference. Extensive experiments validate the effectiveness and versatility of DeltaEdit with different generative models, including both the GAN model and the diffusion model, in achieving flexible text-guided image editing. Code is available at https://github.com/Yueming6568/DeltaEdit.
</details>
<details>
<summary>摘要</summary>
文本导向图像编辑面临训练和推理灵活性的重大挑战。大量文本描述和图像对应的 annotated image-text pairs 收集是贵重的并不是效率的。后来，一些利用预训练的视觉语言模型的方法被提出，以避免数据收集，但它们也受到文本提示优化或推理时的参数调整的限制。为解决这些问题，我们调查并发现了一个特定的空间，称为 CLIP DeltaSpace，其中 CLIP 视觉特征差异与 CLIP 文本特征差异相semantically 对齐。基于 DeltaSpace，我们提议一种新的框架 called DeltaEdit，它在训练阶段将 CLIP 视觉特征差异映射到生成模型的幂值空间方向上，并在推理阶段从 CLIP 文本特征差异预测幂值空间方向。这种设计具有两个优势：（1）无需文本训练；（2）对不同文本提示进行零件推理。广泛的实验证明了 DeltaEdit 与不同的生成模型，包括 GAN 模型和扩散模型，在实现文本导向图像编辑的灵活性方面的有效和多样化。代码可以在 https://github.com/Yueming6568/DeltaEdit 上获取。
</details></li>
</ul>
<hr>
<h2 id="SegLoc-Visual-Self-supervised-Learning-Scheme-for-Dense-Prediction-Tasks-of-Security-Inspection-X-ray-Images"><a href="#SegLoc-Visual-Self-supervised-Learning-Scheme-for-Dense-Prediction-Tasks-of-Security-Inspection-X-ray-Images" class="headerlink" title="SegLoc: Visual Self-supervised Learning Scheme for Dense Prediction Tasks of Security Inspection X-ray Images"></a>SegLoc: Visual Self-supervised Learning Scheme for Dense Prediction Tasks of Security Inspection X-ray Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08421">http://arxiv.org/abs/2310.08421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shervin Halat, Mohammad Rahmati, Ehsan Nazerfard</li>
<li>for: 本研究旨在提高对验安全检查X射线图像进行密集预测的能力。</li>
<li>methods: 本研究使用了增强的自然语言处理（NLP）技术，并将对比学习策略应用于现有的视觉自我超级学习（SSL）模型。</li>
<li>results: 对比于随机初始化方法，本研究的方法在AR和AP指标下，在不同的IOU值下表现出3%至6%的提高，但在不同的预训练纪元下，被超越了指导初始化方法。<details>
<summary>Abstract</summary>
Lately, remarkable advancements of artificial intelligence have been attributed to the integration of self-supervised learning (SSL) scheme. Despite impressive achievements within natural language processing (NLP), SSL in computer vision has not been able to stay on track comparatively. Recently, integration of contrastive learning on top of existing visual SSL models has established considerable progress, thereby being able to outperform supervised counterparts. Nevertheless, the improvements were mostly limited to classification tasks; moreover, few studies have evaluated visual SSL models in real-world scenarios, while the majority considered datasets containing class-wise portrait images, notably ImageNet. Thus, here, we have considered dense prediction tasks on security inspection x-ray images to evaluate our proposed model Segmentation Localization (SegLoc). Based upon the model Instance Localization (InsLoc), our model has managed to address one of the most challenging downsides of contrastive learning, i.e., false negative pairs of query embeddings. To do so, our pre-training dataset is synthesized by cutting, transforming, then pasting labeled segments, as foregrounds, from an already existing labeled dataset (PIDray) onto instances, as backgrounds, of an unlabeled dataset (SIXray;) further, we fully harness the labels through integration of the notion, one queue per class, into MoCo-v2 memory bank, avoiding false negative pairs. Regarding the task in question, our approach has outperformed random initialization method by 3% to 6%, while having underperformed supervised initialization, in AR and AP metrics at different IoU values for 20 to 30 pre-training epochs.
</details>
<details>
<summary>摘要</summary>
近期，人工智能的发展受到了自我指导学习（SSL）的整合的影响。尽管在自然语言处理（NLP）领域内的成果很出色，但在计算机视觉领域，SSL并没有很好地保持同步。最近，在现有的视觉SSL模型之上添加了对比学习，已经实现了较好的进步，并且能够超越指导学习的对比。然而，这些进步主要集中在分类任务上，而且很少的研究对视觉SSL模型进行了实际场景的评估，大多数研究都是使用类别图像 datasets，特别是ImageNet。因此，我们在安全检查式x射线图像上进行了粒度预测任务来评估我们的提议模型Segmentation Localization（SegLoc）。基于Instance Localization（InsLoc）模型，我们解决了对比学习中一个最大的挑战，即查询embedding false negative对。为此，我们使用了将已有的标注dataset（PIDray）中的标注段落切割、变换并贴上无标注dataset（SIXray）中的图像作为背景，并通过 integrate notion one queue per class into MoCo-v2 memory bank来完全利用标签。在问题上，我们的方法在与随机初始化方法的比较中出现了3%到6%的提升，而与指导初始化方法相比，在不同的IoU值下的AR和AP metric上出现了20到30个预训练纪元内的下降。
</details></li>
</ul>
<hr>
<h2 id="Jailbreaking-Black-Box-Large-Language-Models-in-Twenty-Queries"><a href="#Jailbreaking-Black-Box-Large-Language-Models-in-Twenty-Queries" class="headerlink" title="Jailbreaking Black Box Large Language Models in Twenty Queries"></a>Jailbreaking Black Box Large Language Models in Twenty Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08419">http://arxiv.org/abs/2310.08419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/patrickrchao/jailbreakingllms">https://github.com/patrickrchao/jailbreakingllms</a></li>
<li>paper_authors: Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong</li>
<li>for: 保障大型自然语言模型（LLM）与人类价值观Alignment。</li>
<li>methods: 使用攻击者LLM自动生成 semantic jailbreaks，只需黑盒访问目标LLM。</li>
<li>results: PAIR algorithm可以很快生成jailbreak，需要 fewer than twenty queries，并且在不同的LLM上 achieve competitive jailbreaking success rates and transferability.<details>
<summary>Abstract</summary>
There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.
</details>
<details>
<summary>摘要</summary>
有越来越多的关注是确保大语言模型（LLM）与人类价值观念相对应。然而， LLM 的启用是易受到黑客攻击的威胁，这些攻击可以让 LLM 绕过安全拦束。因此，可以通过确定这些漏洞来理解 LLM 的内在弱点，并防止未来的滥用。为此目的，我们提议 Prompt Automatic Iterative Refinement（PAIR）算法，该算法可以使用黑盒访问 LLM 生成 semantic jailbreak，而无需人类干预。PAIR 灵感来自社会工程攻击，使用攻击者 LLM 自动生成针对目标 LLM 的 jailbreak。这样，攻击者 LLM 可以针对目标 LLM 进行无数次询问，以更新和精细化候选 jailbreak。我们的实验表明，PAIR 通常需要 fewer than twenty 个询问来生成 jailbreak，这是现有算法的整个数量级别效率。PAIR 还实现了对 open 和 closed-source LLM 的突破和传输性。包括 GPT-3.5/4、Vicuna 和 PaLM-2 等。
</details></li>
</ul>
<hr>
<h2 id="Tightening-Bounds-on-Probabilities-of-Causation-By-Merging-Datasets"><a href="#Tightening-Bounds-on-Probabilities-of-Causation-By-Merging-Datasets" class="headerlink" title="Tightening Bounds on Probabilities of Causation By Merging Datasets"></a>Tightening Bounds on Probabilities of Causation By Merging Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08406">http://arxiv.org/abs/2310.08406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Numair Sani, Atalanti A. Mastakouri</li>
<li>For: The paper aims to provide symbolic bounds on the Probabilities of Causation (PoC) for a challenging scenario where multiple datasets with different treatment assignment mechanisms are available.* Methods: The paper uses causal sufficiency and combines two randomized experiments or a randomized experiment and an observational study to derive symbolic bounds on the PoC.* Results: The paper provides bounds on the PoC that work for arbitrary dimensionality of covariates and treatment, and discusses the conditions under which these bounds are tighter than existing bounds in literature. Additionally, the paper allows for the possibility of different treatment assignment mechanisms across datasets, enabling the transfer of causal information from the external dataset to the target dataset.<details>
<summary>Abstract</summary>
Probabilities of Causation (PoC) play a fundamental role in decision-making in law, health care and public policy. Nevertheless, their point identification is challenging, requiring strong assumptions, in the absence of which only bounds can be derived. Existing work to further tighten these bounds by leveraging extra information either provides numerical bounds, symbolic bounds for fixed dimensionality, or requires access to multiple datasets that contain the same treatment and outcome variables. However, in many clinical, epidemiological and public policy applications, there exist external datasets that examine the effect of different treatments on the same outcome variable, or study the association between covariates and the outcome variable. These external datasets cannot be used in conjunction with the aforementioned bounds, since the former may entail different treatment assignment mechanisms, or even obey different causal structures. Here, we provide symbolic bounds on the PoC for this challenging scenario. We focus on combining either two randomized experiments studying different treatments, or a randomized experiment and an observational study, assuming causal sufficiency. Our symbolic bounds work for arbitrary dimensionality of covariates and treatment, and we discuss the conditions under which these bounds are tighter than existing bounds in literature. Finally, our bounds parameterize the difference in treatment assignment mechanism across datasets, allowing the mechanisms to vary across datasets while still allowing causal information to be transferred from the external dataset to the target dataset.
</details>
<details>
<summary>摘要</summary>
“ causal sufficiency ”在法律、医疗和公共政策中的决策中发挥基本作用。然而，它们的点标识具有挑战性，需要强大的假设，在缺乏这些假设的情况下只能 derivation 出界。现有的工作是通过利用额外信息来进一步紧紧这些界。然而，在许多临床、EPIDEMIOLOGY 和公共政策应用中，存在外部数据集，其研究不同的治疗方法对同一个结果变量的影响，或者研究 covariates 和结果变量之间的关系。这些外部数据集不能与上述界一起使用，因为前者可能具有不同的治疗分配机制，或者甚至遵循不同的 causal 结构。在这里，我们提供了符号约束，用于评估 PoC。我们集中于组合两个随机化实验，其中一个研究不同的治疗方法，另一个是随机化实验和观察研究，假设 causal sufficiency。我们的符号约束适用于任意维度的 covariates 和治疗，并讨论了这些约束在文献中是否更紧的。最后，我们的约束可以 Parametrize 治疗分配机制的差异，让机制在数据集之间差异，同时仍然允许 causal 信息从外部数据集传递到目标数据集。”
</details></li>
</ul>
<hr>
<h2 id="Performance-power-assessment-of-CNN-packages-on-embedded-automotive-platforms"><a href="#Performance-power-assessment-of-CNN-packages-on-embedded-automotive-platforms" class="headerlink" title="Performance&#x2F;power assessment of CNN packages on embedded automotive platforms"></a>Performance&#x2F;power assessment of CNN packages on embedded automotive platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08401">http://arxiv.org/abs/2310.08401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paolo Burgio, Gianluca Brilli<br>for:This paper aims to support engineers in choosing the most appropriate deep neural network (CNN) package and computing system for their autonomous driving designs, while also deriving guidelines for adequately sizing their systems.methods:The paper will validate the effectiveness and efficiency of recent CNN networks on state-of-the-art platforms with embedded commercial-off-the-shelf System-on-Chips (SoCs), including Xavier AGX, Tegra X2, Nano for NVIDIA, and XCZU9EG and XCZU3EG of the Zynq UltraScale+ family for the Xilinx counterpart.results:The paper will provide guidelines for engineers to choose the most appropriate CNN package and computing system for their designs, based on the performance and power consumption of the SoCs.<details>
<summary>Abstract</summary>
The rise of power-efficient embedded computers based on highly-parallel accelerators opens a number of opportunities and challenges for researchers and engineers, and paved the way to the era of edge computing. At the same time, advances in embedded AI for object detection and categorization such as YOLO, GoogleNet and AlexNet reached an unprecedented level of accuracy (mean-Average Precision - mAP) and performance (Frames-Per-Second - FPS). Today, edge computers based on heterogeneous many-core systems are a predominant choice to deploy such systems in industry 4.0, wearable devices, and - our focus - autonomous driving systems. In these latter systems, engineers struggle to make reduced automotive power and size budgets co-exist with the accuracy and performance targets requested by autonomous driving. We aim at validating the effectiveness and efficiency of most recent networks on state-of-the-art platforms with embedded commercial-off-the-shelf System-on-Chips, such as Xavier AGX, Tegra X2 and Nano for NVIDIA and XCZU9EG and XCZU3EG of the Zynq UltraScale+ family, for the Xilinx counterpart. Our work aims at supporting engineers in choosing the most appropriate CNN package and computing system for their designs, and deriving guidelines for adequately sizing their systems.
</details>
<details>
<summary>摘要</summary>
随着高效的嵌入式计算机的兴起，基于高并行加速器的技术开创了许多机遇和挑战，并导致了边缘计算的时代。同时，嵌入式AI的对象检测和分类技术，如YOLO、GoogleNet和AlexNet，在准确率（mean-Average Precision - mAP）和性能（Frame-Per-Second - FPS）方面达到了历史性的水平。在现代工业4.0、穿梭设备和自动驾驶系统等领域，基于多核心多处理器系统的边缘计算机已成为主流选择。在这些系统中，工程师面临着减少汽车功率和尺寸预算的挑战，同时需要保持自动驾驶系统的准确率和性能标准。我们的研究旨在验证最新的网络在现有的商业半导体SoC上的效果和效率，如Xavier AGX、Tegra X2和Nano等NVIDIA SoC，以及XCZU9EG和XCZU3EG等Xilinx SoC。我们的工作旨在支持工程师选择最适合的Convolutional Neural Network（CNN）套件和计算系统，并 derive出适用于适应系统的指南。
</details></li>
</ul>
<hr>
<h2 id="Prompting-Large-Language-Models-with-Chain-of-Thought-for-Few-Shot-Knowledge-Base-Question-Generation"><a href="#Prompting-Large-Language-Models-with-Chain-of-Thought-for-Few-Shot-Knowledge-Base-Question-Generation" class="headerlink" title="Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation"></a>Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08395">http://arxiv.org/abs/2310.08395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Liang, Jianing Wang, Hanlun Zhu, Lei Wang, Weining Qian, Yunshi Lan</li>
<li>for: 本研究旨在提出一种基于大语言模型的几何问题生成方法，以解决现有KBQG方法对于几何数据的依赖性。</li>
<li>methods: 我们提出了一种基于链条思想的几何问题生成方法（KQG-CoT），首先从无标注数据池中选择支持的逻辑形式，然后根据选择的示例进行链条式启发，并通过扩展KQG-CoT+来确保提问质量。</li>
<li>results: 我们在三个公共KBQG数据集上进行了广泛的实验，结果表明，我们的提问方法在评估数据集上一直表现出优于其他提问基线。特别是，我们的KQG-CoT+方法在PathQuestions数据集上超越了现有的几何数据集的SoTA结果，提高了BLEU-4、METEOR和ROUGE-L的评估指标的相对提升率。<details>
<summary>Abstract</summary>
The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.
</details>
<details>
<summary>摘要</summary>
KBQG任务的目的是将逻辑形式转换为自然语言问题。由于大规模问题标注的昂贵成本，KBQG在低资源场景下的方法urgently需要开发。然而，现有方法均重视 annotated data 的微调，这不适用于少量问题生成。大型自然语言模型（LLMs）的出现表明它们在少量任务中表现出色。受链条思维（CoT）提问策略启发，我们将KBQG任务定义为reasoning问题，其中问题生成的完整过程被拆分为多个子问题生成。我们提出的KQG-CoT提问方法首先从无标注数据池中选择符合特征的逻辑形式，然后写出一个提示，以显示生成复杂问题的逻辑链。为了进一步保证提示质量，我们将KQG-CoT+进一步推广，对逻辑形式进行排序，以确保提示的复杂度适中。我们在三个公共KBQG数据集上进行了广泛的实验。结果表明，我们的提示方法在评估数据集上一直表现出色，并且可以与其他提示基eline比肩。特别是，我们的KQG-CoT+方法可以在PathQuestions数据集上超越现有的几个shot SoTA结果，在BLEU-4、METEOR和ROUGE-L三个指标上提高相对评价18.25、10.72和10.18分。
</details></li>
</ul>
<hr>
<h2 id="Towards-Better-Evaluation-of-Instruction-Following-A-Case-Study-in-Summarization"><a href="#Towards-Better-Evaluation-of-Instruction-Following-A-Case-Study-in-Summarization" class="headerlink" title="Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization"></a>Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08394">http://arxiv.org/abs/2310.08394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ondrej Skopek, Rahul Aralikatte, Sian Gooding, Victor Carbune<br>for: 这个论文的目的是评估大型自然语言模型（LLM）如何遵循用户的指令。methods: 这篇论文使用了多种评估方法来量化LLM的指令遵循能力，包括Prompt-based方法。results: 研究发现，新的LLM-based reference-free评估方法可以提高评估精度，并与高品质的参照基础metric相当。<details>
<summary>Abstract</summary>
Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing 300 document-instruction pairs with 3 answers each. All 900 answers are rated by 3 human annotators. Using riSum, we analyze the agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on par with costly reference-based metrics that require high-quality summaries.
</details>
<details>
<summary>摘要</summary>
尽管最近有所进步，评估大语言模型（LLM）遵循用户指令仍然是一个开放的问题。评估语言模型的方法有很多，但对这些方法的正确性进行了有限的研究。在这种情况下，我们进行了一项meta评估，用于量化 LLM 遵循用户指令的能力。我们的调查是基于文本摘要的基础，收集了300份文档指令对，每个对有3个答案。所有900个答案都被3名人类标注员评分。使用riSum，我们分析了评估方法与人类判断的一致性。最后，我们提出了一些新的 LLM 基于参照free评估方法，超越了已有的基线，并与高质量参照基础的评估方法相当。
</details></li>
</ul>
<hr>
<h2 id="Do-Not-Marginalize-Mechanisms-Rather-Consolidate"><a href="#Do-Not-Marginalize-Mechanisms-Rather-Consolidate" class="headerlink" title="Do Not Marginalize Mechanisms, Rather Consolidate!"></a>Do Not Marginalize Mechanisms, Rather Consolidate!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08377">http://arxiv.org/abs/2310.08377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Willig, Matej Zečević, Devendra Singh Dhami, Kristian Kersting</li>
<li>for: 本研究旨在开发一种能够简化大规模结构 causal model（SCM）的方法，以便更好地理解这些系统的复杂 causal 关系。</li>
<li>methods: 本研究提出了一种基于 consolidating causal mechanisms 的方法，可以将大规模 SCM 转换为更加简单的模型，保持了可变量的 causal 行为。</li>
<li>results: 研究表明，通过 consolidation 可以大幅减少计算复杂性，同时保持 SCM 的可变量性和 causal 行为的一致性。此外，研究还提供了一种泛化 SCM 的思路，以增强其应用范围。<details>
<summary>Abstract</summary>
Structural causal models (SCMs) are a powerful tool for understanding the complex causal relationships that underlie many real-world systems. As these systems grow in size, the number of variables and complexity of interactions between them does, too. Thus, becoming convoluted and difficult to analyze. This is particularly true in the context of machine learning and artificial intelligence, where an ever increasing amount of data demands for new methods to simplify and compress large scale SCM. While methods for marginalizing and abstracting SCM already exist today, they may destroy the causality of the marginalized model. To alleviate this, we introduce the concept of consolidating causal mechanisms to transform large-scale SCM while preserving consistent interventional behaviour. We show consolidation is a powerful method for simplifying SCM, discuss reduction of computational complexity and give a perspective on generalizing abilities of consolidated SCM.
</details>
<details>
<summary>摘要</summary>
Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well. The translation is based on the standard grammar and vocabulary of Simplified Chinese, and may differ from Traditional Chinese, which is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="MCU-A-Task-centric-Framework-for-Open-ended-Agent-Evaluation-in-Minecraft"><a href="#MCU-A-Task-centric-Framework-for-Open-ended-Agent-Evaluation-in-Minecraft" class="headerlink" title="MCU: A Task-centric Framework for Open-ended Agent Evaluation in Minecraft"></a>MCU: A Task-centric Framework for Open-ended Agent Evaluation in Minecraft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08367">http://arxiv.org/abs/2310.08367</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/craftjarvis/mcu">https://github.com/craftjarvis/mcu</a></li>
<li>paper_authors: Haowei Lin, Zihao Wang, Jianzhu Ma, Yitao Liang</li>
<li>for: 本研究旨在开发一个开放式 Minecraft 代理人， therefore 提出了一个任务中心框架（MCU）用于评估 Minecraft 代理人。</li>
<li>methods: 本研究使用了MCU框架，其基于atom任务作为基本建构件，可以生成多种多样的任务。每个任务都有六个不同的困难度分数（时间消耗、运作努力、规划复杂度、细节、创新、新颖），这些分数可以从不同的角度评估代理人的能力。</li>
<li>results: 研究发现MCU框架具有高表达力，能够覆盖所有在latest literature中使用的 Minecraft 代理人任务。此外，研究还发现了代理人开发中的一些挑战，如创新、精准控制和out-of-distribution总结。<details>
<summary>Abstract</summary>
To pursue the goal of creating an open-ended agent in Minecraft, an open-ended game environment with unlimited possibilities, this paper introduces a task-centric framework named MCU for Minecraft agent evaluation. The MCU framework leverages the concept of atom tasks as fundamental building blocks, enabling the generation of diverse or even arbitrary tasks. Within the MCU framework, each task is measured with six distinct difficulty scores (time consumption, operational effort, planning complexity, intricacy, creativity, novelty). These scores offer a multi-dimensional assessment of a task from different angles, and thus can reveal an agent's capability on specific facets. The difficulty scores also serve as the feature of each task, which creates a meaningful task space and unveils the relationship between tasks. For efficient evaluation of Minecraft agents employing the MCU framework, we maintain a unified benchmark, namely SkillForge, which comprises representative tasks with diverse categories and difficulty distribution. We also provide convenient filters for users to select tasks to assess specific capabilities of agents. We show that MCU has the high expressivity to cover all tasks used in recent literature on Minecraft agent, and underscores the need for advancements in areas such as creativity, precise control, and out-of-distribution generalization under the goal of open-ended Minecraft agent development.
</details>
<details>
<summary>摘要</summary>
为了实现在 Minecraft 中创造开放式的代理人，这篇论文提出了一个任务中心框架 named MCU，用于评估 Minecraft 代理人的能力。MCU 框架利用了原子任务作为基本建构件，可以生成多种多样的任务。在 MCU 框架中，每个任务都有六种不同的难度分数（时间消耗、操作努力、计划复杂度、细节、创造力、新颖性）。这些分数可以从不同的角度评估一个任务的难度，从而揭示代理人的特定能力。难度分数还成为每个任务的特征，创造了一个有意义的任务空间，揭示了任务之间的关系。为了有效评估 Minecraft 代理人使用 MCU 框架，我们维护了一个统一的标准套件，称为 SkillForge，该套件包含了多种类型的代表任务，并且有多样化的难度分布。我们还提供了用户友好的筛选工具，以便用户选择评估特定能力的代理人。我们发现 MCU 框架可以覆盖所有在最近的 Minecraft 代理人研究中使用的任务，并且强调了在开放式 Minecraft 代理人发展中的创新、精准控制和非标型泛化等领域的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="2SFGL-A-Simple-And-Robust-Protocol-For-Graph-Based-Fraud-Detection"><a href="#2SFGL-A-Simple-And-Robust-Protocol-For-Graph-Based-Fraud-Detection" class="headerlink" title="2SFGL: A Simple And Robust Protocol For Graph-Based Fraud Detection"></a>2SFGL: A Simple And Robust Protocol For Graph-Based Fraud Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08335">http://arxiv.org/abs/2310.08335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhirui Pan, Guangzhong Wang, Zhaoning Li, Lifeng Chen, Yang Bian, Zhongyuan Lai</li>
<li>for: 提高金融安全性和效率，避免金融犯罪者逃脱检测</li>
<li>methods: 联邦学习（FL）和虚拟图谱融合</li>
<li>results: 在一种常见诈骗检测任务上，与 FedAvg 相比， integrating GCN 和 2SFGL 协同检测方法可以提高性能 indicator 17.6%-30.2%，而 integrating GraphSAGE 和 2SFGL 协同检测方法可以提高性能 indicator 6%-16.2%。<details>
<summary>Abstract</summary>
Financial crime detection using graph learning improves financial safety and efficiency. However, criminals may commit financial crimes across different institutions to avoid detection, which increases the difficulty of detection for financial institutions which use local data for graph learning. As most financial institutions are subject to strict regulations in regards to data privacy protection, the training data is often isolated and conventional learning technology cannot handle the problem. Federated learning (FL) allows multiple institutions to train a model without revealing their datasets to each other, hence ensuring data privacy protection. In this paper, we proposes a novel two-stage approach to federated graph learning (2SFGL): The first stage of 2SFGL involves the virtual fusion of multiparty graphs, and the second involves model training and inference on the virtual graph. We evaluate our framework on a conventional fraud detection task based on the FraudAmazonDataset and FraudYelpDataset. Experimental results show that integrating and applying a GCN (Graph Convolutional Network) with our 2SFGL framework to the same task results in a 17.6\%-30.2\% increase in performance on several typical metrics compared to the case only using FedAvg, while integrating GraphSAGE with 2SFGL results in a 6\%-16.2\% increase in performance compared to the case only using FedAvg. We conclude that our proposed framework is a robust and simple protocol which can be simply integrated to pre-existing graph-based fraud detection methods.
</details>
<details>
<summary>摘要</summary>
金融犯罪检测使用图学学习提高金融安全和效率。然而，犯罪者可能会在不同机构中犯罪，以避免检测，这会增加金融机构使用本地数据进行图学学习时的检测难度。由于大多数金融机构受到严格的数据隐私保护法规，训练数据通常孤立，传统的学习技术无法处理这个问题。联邦学习（FL）allow multiple institutions to train a model without revealing their datasets to each other, thereby ensuring data privacy protection.在这篇论文中，我们提出了一种新的两stage方法： federated graph learning（2SFGL）。第一stage of 2SFGL involves the virtual fusion of multiparty graphs, and the second stage involves model training and inference on the virtual graph. We evaluate our framework on a conventional fraud detection task based on the FraudAmazonDataset and FraudYelpDataset. Experimental results show that integrating and applying a GCN (Graph Convolutional Network) with our 2SFGL framework to the same task results in a 17.6%-30.2% increase in performance on several typical metrics compared to the case only using FedAvg, while integrating GraphSAGE with 2SFGL results in a 6%-16.2% increase in performance compared to the case only using FedAvg. We conclude that our proposed framework is a robust and simple protocol which can be simply integrated to pre-existing graph-based fraud detection methods.
</details></li>
</ul>
<hr>
<h2 id="Transport-Hub-Aware-Spatial-Temporal-Adaptive-Graph-Transformer-for-Traffic-Flow-Prediction"><a href="#Transport-Hub-Aware-Spatial-Temporal-Adaptive-Graph-Transformer-for-Traffic-Flow-Prediction" class="headerlink" title="Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for Traffic Flow Prediction"></a>Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for Traffic Flow Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08328">http://arxiv.org/abs/2310.08328</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fantasy-shaw/h-stformer">https://github.com/fantasy-shaw/h-stformer</a></li>
<li>paper_authors: Xiao Xu, Lei Zhang, Bailong Liu, Zhizhen Liang, Xuefei Zhang</li>
<li>for: 这篇论文的目的是提出一种基于交通运输系统核心技术的交通流量预测方法，以解决现有方法不充分利用交通流量数据的特性和增量学习问题。</li>
<li>methods: 该方法基于Transport-Hub-aware Spatial-Temporal adaptive graph transFormer (H-STFormer)，包括了一个新的空间自注意机制，三个图集合矩阵和一个时间自注意机制，以及一个额外的空间-时间知识塑造模块。</li>
<li>results: 经过广泛的实验，该方法在正常和增量交通流量预测任务中表现出色，能够更好地利用交通流量数据的特性和增量学习知识。<details>
<summary>Abstract</summary>
As a core technology of Intelligent Transportation System (ITS), traffic flow prediction has a wide range of applications. Traffic flow data are spatial-temporal, which are not only correlated to spatial locations in road networks, but also vary with temporal time indices. Existing methods have solved the challenges in traffic flow prediction partly, focusing on modeling spatial-temporal dependencies effectively, while not all intrinsic properties of traffic flow data are utilized fully. Besides, there are very few attempts at incremental learning of spatial-temporal data mining, and few previous works can be easily transferred to the traffic flow prediction task. Motivated by the challenge of incremental learning methods for traffic flow prediction and the underutilization of intrinsic properties of road networks, we propose a Transport-Hub-aware Spatial-Temporal adaptive graph transFormer (H-STFormer) for traffic flow prediction. Specifically, we first design a novel spatial self-attention module to capture the dynamic spatial dependencies. Three graph masking matrices are integrated into spatial self-attentions to highlight both short- and long-term dependences. Additionally, we employ a temporal self-attention module to detect dynamic temporal patterns in the traffic flow data. Finally, we design an extra spatial-temporal knowledge distillation module for incremental learning of traffic flow prediction tasks. Through extensive experiments, we show the effectiveness of H-STFormer in normal and incremental traffic flow prediction tasks. The code is available at https://github.com/Fantasy-Shaw/H-STFormer.
</details>
<details>
<summary>摘要</summary>
为智能交通系统（ITS）核心技术之一，交通流量预测具有广泛的应用。交通流量数据具有空间-时间相关性，不仅与路网中的空间位置相关，还随着时间索引而变化。现有方法已经解决了交通流量预测中的一些挑战，主要是有效地模型空间-时间相关性，但是没有 completelly 利用交通流量数据的内在特性。此外，有很少的尝试在增量学习空间-时间数据挖掘中，而且前期工作很难 direct 应用于交通流量预测任务。驱动 by 交通流量预测任务的增量学习挑战和路网内在特性的 unterutilization，我们提出了一种基于交通枢纽的空间-时间 adaptive graph transformer（H-STFormer）。具体来说，我们首先设计了一种新的空间自注意模块，以捕捉流动的空间相关性。在空间自注意模块中，我们采用了三个图 masking 矩阵，以强调短期和长期相关性。此外，我们采用了一种时间自注意模块，以检测交通流量数据中的动态时间模式。最后，我们设计了一个额外的空间-时间知识继承模块，以进行增量学习交通流量预测任务。通过广泛的实验，我们证明了 H-STFormer 在正常和增量交通流量预测任务中的效果。代码可以在 GitHub 上找到：https://github.com/Fantasy-Shaw/H-STFormer。
</details></li>
</ul>
<hr>
<h2 id="CHIP-Contrastive-Hierarchical-Image-Pretraining"><a href="#CHIP-Contrastive-Hierarchical-Image-Pretraining" class="headerlink" title="CHIP: Contrastive Hierarchical Image Pretraining"></a>CHIP: Contrastive Hierarchical Image Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08304">http://arxiv.org/abs/2310.08304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arpit Mittal, Harshil Jhaveri, Swapnil Mallick, Abhishek Ajmera</li>
<li>for: 这个论文主要目的是提出一种几 shot 对象分类模型，用于将未seen类对象分类到一个相对普遍的类别中。</li>
<li>methods: 该模型使用了三级层次的对比损失函数基于 ResNet152 分类器，用于基于图像嵌入特征进行对象分类。</li>
<li>results: 经过训练后，模型可以准确地将未seen类对象分类到一个相对普遍的类别中，并且对这些结果进行了详细的讨论。<details>
<summary>Abstract</summary>
Few-shot object classification is the task of classifying objects in an image with limited number of examples as supervision. We propose a one-shot/few-shot classification model that can classify an object of any unseen class into a relatively general category in an hierarchically based classification. Our model uses a three-level hierarchical contrastive loss based ResNet152 classifier for classifying an object based on its features extracted from Image embedding, not used during the training phase. For our experimentation, we have used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal classes for training our model and created our own dataset of unseen classes for evaluating our trained model. Our model provides satisfactory results in classifying the unknown objects into a generic category which has been later discussed in greater detail.
</details>
<details>
<summary>摘要</summary>
几个示例物类分类是指将图像中的对象分类到有限多个示例的超级类别中。我们提出了一种一批/几批分类模型，可以将未看到的对象分类到一个层次结构基于的总体类别中。我们的模型使用了基于对比损失的ResNet152分类器，通过图像嵌入特征来分类对象。在训练阶段，我们使用了ILSVRC-12 dataset的动物类 subsets来训练我们的模型，并创建了一个包含未看到类的自定义数据集来评估我们的训练后模型。我们的模型在不知道对象的情况下提供了满意的结果，这些结果在后续详细介绍。
</details></li>
</ul>
<hr>
<h2 id="If-our-aim-is-to-build-morality-into-an-artificial-agent-how-might-we-begin-to-go-about-doing-so"><a href="#If-our-aim-is-to-build-morality-into-an-artificial-agent-how-might-we-begin-to-go-about-doing-so" class="headerlink" title="If our aim is to build morality into an artificial agent, how might we begin to go about doing so?"></a>If our aim is to build morality into an artificial agent, how might we begin to go about doing so?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08295">http://arxiv.org/abs/2310.08295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reneira Seeamber, Cosmin Badea</li>
<li>for: 本研究旨在强调在AI中建立道德机器人的重要性，以及关键考虑的道德方法和挑战。</li>
<li>methods: 本文提出了一种混合方法和层次结合方法，以实现建立道德机器人。</li>
<li>results: 本研究提出了一些解决方案，包括 hybrid 方法和层次结合方法，以确保 AI 的道德行为和政策的实施。<details>
<summary>Abstract</summary>
As Artificial Intelligence (AI) becomes pervasive in most fields, from healthcare to autonomous driving, it is essential that we find successful ways of building morality into our machines, especially for decision-making. However, the question of what it means to be moral is still debated, particularly in the context of AI. In this paper, we highlight the different aspects that should be considered when building moral agents, including the most relevant moral paradigms and challenges. We also discuss the top-down and bottom-up approaches to design and the role of emotion and sentience in morality. We then propose solutions including a hybrid approach to design and a hierarchical approach to combining moral paradigms. We emphasize how governance and policy are becoming ever more critical in AI Ethics and in ensuring that the tasks we set for moral agents are attainable, that ethical behavior is achieved, and that we obtain good AI.
</details>
<details>
<summary>摘要</summary>
随着人工智能（AI）在各个领域的普及，从医疗到自动驾驶，建立机器内置的道德是非常重要的。然而，我们 Still debating what it means to be moral, especially in the context of AI. In this paper, we highlight the different aspects that should be considered when building moral agents, including the most relevant moral paradigms and challenges. We also discuss the top-down and bottom-up approaches to design and the role of emotion and sentience in morality. We then propose solutions including a hybrid approach to design and a hierarchical approach to combining moral paradigms. We emphasize how governance and policy are becoming ever more critical in AI Ethics and in ensuring that the tasks we set for moral agents are attainable, that ethical behavior is achieved, and that we obtain good AI.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Concealed-Electronic-Countermeasures-of-Radar-Signal-with-Adversarial-Examples"><a href="#Concealed-Electronic-Countermeasures-of-Radar-Signal-with-Adversarial-Examples" class="headerlink" title="Concealed Electronic Countermeasures of Radar Signal with Adversarial Examples"></a>Concealed Electronic Countermeasures of Radar Signal with Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08292">http://arxiv.org/abs/2310.08292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruinan Ma, Canjie Zhu, Mingfeng Lu, Yunjie Li, Yu-an Tan, Ruibin Zhang, Ran Tao</li>
<li>for: 本研究旨在探讨基于AI技术的雷达信号电子干扰技术，以解决传统干扰技术的缺点，即干扰信号过于明显。</li>
<li>methods: 我们提出了一个基于时域频域图像的雷达信号分类攻击管道，并使用DITIMI-FGSM攻击算法，实现了高度可移植性。此外，我们还提出了基于STFT算法的时域信号攻击方法（STDS），以解决时域分析中的非逆问题。</li>
<li>results: 我们通过大量实验发现，我们的攻击管道是可行的，并且提出的攻击方法具有高度成功率。<details>
<summary>Abstract</summary>
Electronic countermeasures involving radar signals are an important aspect of modern warfare. Traditional electronic countermeasures techniques typically add large-scale interference signals to ensure interference effects, which can lead to attacks being too obvious. In recent years, AI-based attack methods have emerged that can effectively solve this problem, but the attack scenarios are currently limited to time domain radar signal classification. In this paper, we focus on the time-frequency images classification scenario of radar signals. We first propose an attack pipeline under the time-frequency images scenario and DITIMI-FGSM attack algorithm with high transferability. Then, we propose STFT-based time domain signal attack(STDS) algorithm to solve the problem of non-invertibility in time-frequency analysis, thus obtaining the time-domain representation of the interference signal. A large number of experiments show that our attack pipeline is feasible and the proposed attack method has a high success rate.
</details>
<details>
<summary>摘要</summary>
现代战争中电子干扰技术是非常重要的。传统的电子干扰技术通常添加大规模干扰信号以确保干扰效果，这可能导致攻击变得太明显。在最近几年，基于人工智能的攻击方法出现了，可以有效解决这个问题，但攻击场景目前仅限于时域雷达信号分类。在这篇论文中，我们关注时频图像分类场景中的雷达信号。我们首先提出了基于时频图像的攻击管道和DITIMI-FGSM攻击算法，该算法具有高传输性。然后，我们提出了STFT基于时域信号攻击算法（STDS）以解决时频分析中的非可逆性问题，从而获得了干扰信号的时域表示。大量实验表明，我们的攻击管道是可行的，并且提posed攻击方法具有高成功率。
</details></li>
</ul>
<hr>
<h2 id="Expanding-the-Vocabulary-of-BERT-for-Knowledge-Base-Construction"><a href="#Expanding-the-Vocabulary-of-BERT-for-Knowledge-Base-Construction" class="headerlink" title="Expanding the Vocabulary of BERT for Knowledge Base Construction"></a>Expanding the Vocabulary of BERT for Knowledge Base Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08291">http://arxiv.org/abs/2310.08291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MaastrichtU-IDS/LMKBC-2023">https://github.com/MaastrichtU-IDS/LMKBC-2023</a></li>
<li>paper_authors: Dong Yang, Xu Wang, Remzi Celebi</li>
<li>for: 本研究旨在constructing knowledge base using language model, specifically tackling the task of knowledge base construction from pre-trained language models at International Semantic Web Conference 2023.</li>
<li>methods: 我们提出了Vocabulary Expandable BERT，一种可以扩展语言模型词汇表的方法，同时保持语义嵌入的新增词语。我们采用了任务特有的重新预训练方法来进一步提高语言模型。</li>
<li>results: 我们的方法在实验中表现出色，F1分数达0.323和0.362分别在隐藏测试集和验证集上，两者均由挑战提供。我们的框架使用了轻量级语言模型（BERT-base，0.13亿参数），超过使用直接在大语言模型上预训练（Chatgpt-3，175亿参数）。此外，Token-Recode achieve相当的表现与Re-pretrain。本研究提升了语言理解模型，使得直接嵌入多token实体，标志着知识图和数据管理中的链接预测任务做出了重大进步。<details>
<summary>Abstract</summary>
Knowledge base construction entails acquiring structured information to create a knowledge base of factual and relational data, facilitating question answering, information retrieval, and semantic understanding. The challenge called "Knowledge Base Construction from Pretrained Language Models" at International Semantic Web Conference 2023 defines tasks focused on constructing knowledge base using language model. Our focus was on Track 1 of the challenge, where the parameters are constrained to a maximum of 1 billion, and the inclusion of entity descriptions within the prompt is prohibited.   Although the masked language model offers sufficient flexibility to extend its vocabulary, it is not inherently designed for multi-token prediction. To address this, we present Vocabulary Expandable BERT for knowledge base construction, which expand the language model's vocabulary while preserving semantic embeddings for newly added words. We adopt task-specific re-pre-training on masked language model to further enhance the language model.   Through experimentation, the results show the effectiveness of our approaches. Our framework achieves F1 score of 0.323 on the hidden test set and 0.362 on the validation set, both data set is provided by the challenge. Notably, our framework adopts a lightweight language model (BERT-base, 0.13 billion parameters) and surpasses the model using prompts directly on large language model (Chatgpt-3, 175 billion parameters). Besides, Token-Recode achieves comparable performances as Re-pretrain. This research advances language understanding models by enabling the direct embedding of multi-token entities, signifying a substantial step forward in link prediction task in knowledge graph and metadata completion in data management.
</details>
<details>
<summary>摘要</summary>
知识库建设需要获取结构化信息，以创建一个包含事实和关系数据的知识库，以便问题回答、信息检索和Semantic理解。国际semantic Web会议2023年度挑战定义了一系列任务，用于使用语言模型构建知识库。我们的关注点是第一轨道的挑战，其中语言模型的参数不得超过10亿，并且在提示中禁止包含实体描述。虽然遮盲语言模型具有较好的灵活性，但它并不自然地适应多个单词预测。为解决这个问题，我们提出了用于知识库建设的词汇扩展BERT（Vocabulary Expandable BERT），可以扩展语言模型的词汇，同时保持新增的单词含义表示。我们采用了特定任务的再预训练masked语言模型，以进一步提高语言模型。经过实验，我们的方法得到了显著的效果。我们的框架在隐藏测试集上达到了F1分数0.323，并在验证集上达到了F1分数0.362。值得注意的是，我们的框架使用了轻量级语言模型（BERT-base，0.13亿参数），并在使用大型语言模型（Chatgpt-3，175亿参数）上超过了模型。此外，Token-Recode获得了与Re-pretrain相当的性能。这项研究提高了语言理解模型，使其能直接嵌入多个单词实体，标志着链接预测任务在知 graphs和数据管理中的一个重要进步。
</details></li>
</ul>
<hr>
<h2 id="CP-KGC-Constrained-Prompt-Knowledge-Graph-Completion-with-Large-Language-Models"><a href="#CP-KGC-Constrained-Prompt-Knowledge-Graph-Completion-with-Large-Language-Models" class="headerlink" title="CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models"></a>CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08279">http://arxiv.org/abs/2310.08279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sjlmg/CP-KGC">https://github.com/sjlmg/CP-KGC</a></li>
<li>paper_authors: Rui Yang, Li Fang, Yi Zhou</li>
<li>For: 这篇论文的目的是利用现有的知识来推理和推测知识图中缺失的连接。* Methods: 这篇论文使用了文本基本方法，如SimKGC，以提高知识图补充的效果。但是，文本基本方法的效果受到实体文本描述的质量的限制。本文提出了使用约束基于的提示来减少LLM生成文本中的幻化问题。* Results: 本文的Constraint-Prompt Knowledge Graph Completion（CP-KGC）方法在低资源计算条件下表现出了有效的推理能力，并在WN18RR和FB15K237数据集上超过了之前的结果。这表明了LLMs在KGC任务中的整合和未来研究的新方向。<details>
<summary>Abstract</summary>
Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce and infer missing connections within knowledge graphs. Text-based approaches, like SimKGC, have outperformed graph embedding methods, showcasing the promise of inductive KGC. However, the efficacy of text-based methods hinges on the quality of entity textual descriptions. In this paper, we identify the key issue of whether large language models (LLMs) can generate effective text. To mitigate hallucination in LLM-generated text in this paper, we introduce a constraint-based prompt that utilizes the entity and its textual description as contextual constraints to enhance data quality. Our Constrained-Prompt Knowledge Graph Completion (CP-KGC) method demonstrates effective inference under low resource computing conditions and surpasses prior results on the WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC tasks and provides new directions for future research.
</details>
<details>
<summary>摘要</summary>
知识图完成（KGC）目标是利用现有知识来推理和推断知识图中缺失的连接。文本基本方法，如SimKGC，在完成KGC任务中表现出色，超越了图集 embedding 方法。然而，文本基本方法的效果归结于实体文本描述的质量。在这篇论文中，我们发现了大语言模型（LLM）是否能生成有效的文本是关键问题。为了消除LLM生成文本中的幻觉，我们引入了一种基于约束的提问方法，使用实体和其文本描述作为 Contextual 约束来提高数据质量。我们的受约束知识图完成（CP-KGC）方法在低资源计算条件下表现出了有效的推理能力，并在WN18RR和FB15K237数据集上超越了先前的结果。这表明了LLM在KGC任务中的整合，并为未来的研究提供了新的方向。
</details></li>
</ul>
<hr>
<h2 id="Lag-Llama-Towards-Foundation-Models-for-Time-Series-Forecasting"><a href="#Lag-Llama-Towards-Foundation-Models-for-Time-Series-Forecasting" class="headerlink" title="Lag-Llama: Towards Foundation Models for Time Series Forecasting"></a>Lag-Llama: Towards Foundation Models for Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08278">http://arxiv.org/abs/2310.08278</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kashif/pytorch-transformer-ts">https://github.com/kashif/pytorch-transformer-ts</a></li>
<li>paper_authors: Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Biloš, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, Sahil Garg, Alexandre Drouin, Nicolas Chapados, Yuriy Nevmyvaka, Irina Rish</li>
<li>for: 这个论文的目的是建立基础模型，用于时间序列预测，以及研究这些模型的扩展性。</li>
<li>methods: 这个模型使用了一种通用的 probabilistic 时间序列预测模型，并使用了大量的时间序列数据进行训练。</li>
<li>results: 模型在未看过的 “out-of-distribution” 时间序列数据上表现出色，超过了supervised baselines的预测性能。模型使用了光滑破碎的power-laws来预测模型的扩展性。Here’s the English version for reference:</li>
<li>for: The purpose of this paper is to build foundational models for time-series forecasting and study their scalability.</li>
<li>methods: The model uses a general-purpose probabilistic time-series forecasting model trained on a large collection of time-series data.</li>
<li>results: The model shows good zero-shot prediction capabilities on unseen “out-of-distribution” time-series datasets, outperforming supervised baselines. The model uses smoothly broken power-laws to fit and predict model scaling behavior.<details>
<summary>Abstract</summary>
Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama, a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen "out-of-distribution" time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws to fit and predict model scaling behavior. The open source code is made available at https://github.com/kashif/pytorch-transformer-ts.
</details>
<details>
<summary>摘要</summary>
目标建立时间序列预测基础模型，我们现在发表我们的工作进度， Lag-Llama 是一种通用单变量时间序列预测模型，通过大量时间序列数据进行训练。该模型在未看过的 "out-of-distribution" 时间序列数据上表现出良好的预测能力，超过了指导基eline。我们使用缓和的力普洛斯来预测模型缩放行为。开源代码可以在 https://github.com/kashif/pytorch-transformer-ts 上获取。
</details></li>
</ul>
<hr>
<h2 id="Direction-Oriented-Visual-semantic-Embedding-Model-for-Remote-Sensing-Image-text-Retrieval"><a href="#Direction-Oriented-Visual-semantic-Embedding-Model-for-Remote-Sensing-Image-text-Retrieval" class="headerlink" title="Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval"></a>Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08276">http://arxiv.org/abs/2310.08276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qing Ma, Jiancheng Pan, Cong Bai</li>
<li>for: 提高Remote Sensing中的图像文本检索精度，解决视觉语义不匹配问题</li>
<li>methods: 提出一种新的方向偏置视 semantic Embedding Model (DOVE)，利用 Regional-Oriented Attention Module (ROAM) 和 lightweight Digging Text Genome Assistant (DTGA) 实现视觉语义关系的挖掘</li>
<li>results: 通过广泛的实验，包括参数评估、量化比较、拆除研究和视觉分析，证明方法的效果和优越性，在RSICD和RSITMD两个标准测试集上<details>
<summary>Abstract</summary>
Image-text retrieval has developed rapidly in recent years. However, it is still a challenge in remote sensing due to visual-semantic imbalance, which leads to incorrect matching of non-semantic visual and textual features. To solve this problem, we propose a novel Direction-Oriented Visual-semantic Embedding Model (DOVE) to mine the relationship between vision and language. Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the distance between the final visual and textual embeddings in the latent semantic space, oriented by regional visual features. Meanwhile, a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the range of tractable textual representation and enhance global word-level semantic connections using less attention operations. Ultimately, we exploit a global visual-semantic constraint to reduce single visual dependency and serve as an external constraint for the final visual and textual representations. The effectiveness and superiority of our method are verified by extensive experiments including parameter evaluation, quantitative comparison, ablation studies and visual analysis, on two benchmark datasets, RSICD and RSITMD.
</details>
<details>
<summary>摘要</summary>
<SYS>图文检索在最近几年内得到了快速发展，但在遥感领域仍然存在视 semantic 不匹配问题，这会导致非 semantic 的视觉和文本特征的不正确匹配。为解决这问题，我们提议一种新的方向围绕视 semantic 嵌入模型（DOVE），以挖掘视 Semantic 关系。具体来说，一个地域围绕注意模块（ROAM）可以在最终的视觉和文本嵌入空间中进行 adaptive 距离调整，以便根据地域视觉特征进行方向引导。同时，我们设计了一种轻量级的挖掘文本遗传助手（DTGA），以扩大可处理文本表示范围和增强全局单词 Semantic 连接使用 fewer attention 操作。最后，我们利用全局视 Semantic 约束来减少单个视觉依赖和作为外部约束 для final 视觉和文本表示。我们的方法的效果和优越性在两个 benchmark 数据集上，RSICD 和 RSITMD 进行了广泛的实验，包括参数评估、量化比较、剥削学习和视觉分析。</SYS>Note that Simplified Chinese is used in the translation, as it is more widely used in mainland China and is the standard writing system used in the country. Traditional Chinese is used in Hong Kong, Macau, and Taiwan, and it has some differences in spelling and grammar compared to Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Co-occurrence-on-Factual-Knowledge-of-Large-Language-Models"><a href="#Impact-of-Co-occurrence-on-Factual-Knowledge-of-Large-Language-Models" class="headerlink" title="Impact of Co-occurrence on Factual Knowledge of Large Language Models"></a>Impact of Co-occurrence on Factual Knowledge of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08256">http://arxiv.org/abs/2310.08256</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cheongwoong/impact_of_cooccurrence">https://github.com/cheongwoong/impact_of_cooccurrence</a></li>
<li>paper_authors: Cheongwoong Kang, Jaesik Choi</li>
<li>for: 本研究旨在探讨大语言模型（LLM）常常返回错误的原因，以及如何提高LLM的可靠性。</li>
<li>methods: 本研究使用了一种定量方法，通过分析LLM在不同预训练集中的表现，探讨LLM在回答问题时是否受到预训练数据中的偏见影响。</li>
<li>results: 研究发现，LLM受到预训练数据中的偏见影响，导致它们偏好频繁共occurrence的词语，而不是正确的答案。这使得LLM在回答有关的问题时难以讲述事实，尤其是当问题中的主题和 объек  hardly co-occur在预训练数据中时。研究还发现，不 matter how large the model size or how much finetuning is done, co-occurrence bias still exists. 因此，研究建议使用减偏数据集进行finetuning，以避免这种偏见。<details>
<summary>Abstract</summary>
Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further research in mitigation will help build reliable language models by preventing potential errors. The code is available at \url{https://github.com/CheongWoong/impact_of_cooccurrence}.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）经常会给出错误的回答，即使在不同的应用中具有成功。在这篇文章中，我们提出了假设，认为将重点放在预训料中的简单共occurrence统计上是 LLM 产生错误的主要原因。我们的结果显示 LLM 受到共occurrence偏见，即偏爱常见的词语，而不是正确的答案。因此， LLM 对于 rarely 共occurrence 的主题和物件没有记忆，即使它们在调整中看到过。我们发现，共occurrence偏见不受模型大小或调整的影响，因此我们建议使用删除偏见样本的删除调整，以降低这种偏见。虽然删除调整可以帮助 LLM 记忆预训料中罕见的事实，但是它不能帮助 LLM 在调整中发现过去未见的罕见事实。进一步的研究将有助于建立可靠的语言模型，以避免潜在的错误。代码可以在 \url{https://github.com/CheongWoong/impact_of_cooccurrence} 获取。
</details></li>
</ul>
<hr>
<h2 id="MetaBox-A-Benchmark-Platform-for-Meta-Black-Box-Optimization-with-Reinforcement-Learning"><a href="#MetaBox-A-Benchmark-Platform-for-Meta-Black-Box-Optimization-with-Reinforcement-Learning" class="headerlink" title="MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning"></a>MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08252">http://arxiv.org/abs/2310.08252</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GMC-DRL/MetaBox">https://github.com/GMC-DRL/MetaBox</a></li>
<li>paper_authors: Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Zhenrui Li, Guojun Peng, Yue-Jiao Gong, Yining Ma, Zhiguang Cao</li>
<li>for: 这个研究旨在探讨Meta-Black-Box Optimization with Reinforcement Learning（MetaBBO-RL）的可能性，并提供一个全面的 benchmark 平台 для开发和评估MetaBBO-RL 方法。</li>
<li>methods: 这个研究使用了一个可调的 algorithmic template，让使用者可以轻松地实现自己的设计 within the platform。另外，它还提供了300多个问题实例，涵盖了从 sintetic 到 realistic 的情况，以及19个基eline 方法，包括传统的 black-box optimizer 和最近的 MetaBBO-RL 方法。</li>
<li>results: 这个研究为了证明 MetaBox 的用途，对现有的 MetaBBO-RL 方法进行了广泛的 benchmarking 研究。<details>
<summary>Abstract</summary>
Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-depth analysis, we carry out a wide-ranging benchmarking study on existing MetaBBO-RL methods. Our MetaBox is open-source and accessible at: https://github.com/GMC-DRL/MetaBox.
</details>
<details>
<summary>摘要</summary>
近期，Meta-Black-Box优化器与强化学习（MetaBBO-RL）已经展示了通过在meta层使用RL来减少人工细化低级黑盒优化器的问题。然而，这个领域受到互联网缺乏一个统一的 benchmark 的限制。为了填补这个空白，我们介绍了 MetaBox，第一个专门为开发和评估 MetaBBO-RL 方法而设计的 benchmark 平台。MetaBox 提供了灵活的算法模板，allowing users 可以轻松地实现他们的独特设计在平台上。此外，它还提供了来自 sintetic 到 realistic 的问题集，以及一个广泛的库，包括传统的黑盒优化器和最新的 MetaBBO-RL 方法。此外，MetaBox 引入了三种标准性能指标，以便更加全面地评估方法。为了证明 MetaBox 的用于促进严格评估和深入分析的能力，我们进行了广泛的 benchmarking 研究，覆盖了现有的 MetaBBO-RL 方法。我们的 MetaBox 是开源的，可以在以下地址下载：https://github.com/GMC-DRL/MetaBox。
</details></li>
</ul>
<hr>
<h2 id="GROOT-Learning-to-Follow-Instructions-by-Watching-Gameplay-Videos"><a href="#GROOT-Learning-to-Follow-Instructions-by-Watching-Gameplay-Videos" class="headerlink" title="GROOT: Learning to Follow Instructions by Watching Gameplay Videos"></a>GROOT: Learning to Follow Instructions by Watching Gameplay Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08235">http://arxiv.org/abs/2310.08235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CraftJarvis/GROOT">https://github.com/CraftJarvis/GROOT</a></li>
<li>paper_authors: Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, Yitao Liang</li>
<li>for: 这个论文目标是建立一个可以遵循开放式指令的控制器，用于在开放世界环境中进行游戏play。</li>
<li>methods: 该论文提出了以参考视频作为指令，从游戏媒体中学习控制器的方法，并使用 causal transformers 实现了一个简单 yet effective  encoder-decoder 架构。</li>
<li>results: 在一个基于 Minecraft 的 SkillForge  benchamark 上，对于开放世界的对手和人类玩家进行评测，GROOT 表现出了70% 的赢利率，并且与人类机器同等水平。 代码和视频可以在<a target="_blank" rel="noopener" href="https://craftjarvis-groot.github.io/">https://craftjarvis-groot.github.io</a> 上找到。<details>
<summary>Abstract</summary>
We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers. We evaluate GROOT against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap as well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis of the induced goal space further demonstrates some interesting emergent properties, including the goal composition and complex gameplay behavior synthesis. Code and video can be found on the website https://craftjarvis-groot.github.io.
</details>
<details>
<summary>摘要</summary>
我们研究如何建立一个可以遵循开放式指令的控制器，在开放世界环境中进行游戏游戏。我们提议以参考视频作为指令，这些指令提供了表达力强的目标规范，同时消除了高昂的文本游戏注释。我们 derivates a new learning framework，使得可以从游戏视频中学习这种指令遵循控制器，并生成一个视频指令编码器，该编码器在游戏中生成结构化的目标空间。我们实现了我们的代理GROOT，使用了 causal transformers 基于 encoder-decoder 架构。我们对 Minecraft SkillForge benchmark 进行了评估，并与人类玩家和其他开放世界控制器进行比较。很明显，GROOT 在人机之间减少了差距，并在最佳通用代理基eline上达到 70% 的赢利率。另外，对于引导空间的分析也表明了一些有趣的 emergent 性，包括目标组合和复杂的游戏行为合成。代码和视频可以在 website https://craftjarvis-groot.github.io 找到。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Time-Step-Frequency-on-the-Realism-of-Robotic-Manipulation-Simulation-for-Objects-of-Different-Scales"><a href="#The-Impact-of-Time-Step-Frequency-on-the-Realism-of-Robotic-Manipulation-Simulation-for-Objects-of-Different-Scales" class="headerlink" title="The Impact of Time Step Frequency on the Realism of Robotic Manipulation Simulation for Objects of Different Scales"></a>The Impact of Time Step Frequency on the Realism of Robotic Manipulation Simulation for Objects of Different Scales</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08233">http://arxiv.org/abs/2310.08233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minh Q. Ta, Holly Dinkel, Hameed Abdul-Rashid, Yangfei Dai, Jessica Myers, Tan Chen, Junyi Geng, Timothy Bretl</li>
<li>for: 本研究探讨了机器人操作模拟精度中的时间步频和组件尺度的影响。</li>
<li>methods: 研究使用了不同的时间步频和组件尺度，对小规模 объек的机器人操作模拟精度进行了评估。</li>
<li>results: 结果显示，逐步增加时间步频可以提高小规模 объек的机器人操作模拟精度。<details>
<summary>Abstract</summary>
This work evaluates the impact of time step frequency and component scale on robotic manipulation simulation accuracy. Increasing the time step frequency for small-scale objects is shown to improve simulation accuracy. This simulation, demonstrating pre-assembly part picking for two object geometries, serves as a starting point for discussing how to improve Sim2Real transfer in robotic assembly processes.
</details>
<details>
<summary>摘要</summary>
这个研究evaluates the impact of时间步频和组件尺度在机器人操作 simulated accuracy. 增加小规模对象的时间步频可以提高simulation accuracy. 这个simulation, demonstrating pre-assembly part picking for two object geometries, serves as a starting point for discussing how to improve Sim2Real transfer in robotic assembly processes.Note: "Sim2Real" refers to the transfer of skills learned in simulation to the real world.
</details></li>
</ul>
<hr>
<h2 id="Large-language-models-can-replicate-cross-cultural-differences-in-personality"><a href="#Large-language-models-can-replicate-cross-cultural-differences-in-personality" class="headerlink" title="Large language models can replicate cross-cultural differences in personality"></a>Large language models can replicate cross-cultural differences in personality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10679">http://arxiv.org/abs/2310.10679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paweł Niszczota, Mateusz Janczak</li>
<li>for: 本研究用于测试GPT-4是否能够复制不同文化之间的五大人性特质差异，使用美国和韩国作为文化对比。</li>
<li>methods: 研究使用了大规模实验（N&#x3D;8000），使用GPT-4和GPT-3.5两种语言模型，对英语和韩语版本的十项人性测试表进行了 manipulate。</li>
<li>results: 研究发现GPT-4能够复制不同文化之间的每个因素差异，但是 сред值有上升偏好，表现出来的结构适应性较低。<details>
<summary>Abstract</summary>
We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. Overall, we provide preliminary evidence that LLMs can aid cross-cultural psychological research.
</details>
<details>
<summary>摘要</summary>
我们使用大规模实验（N=8000）来确定GPT-4是否可以复制不同文化之间的五大人格 trait，使用美国和韩国作为文化对，因为先前的研究表明这两个国家之间存在重要的人格差异。我们在目标 simulate（美国 vs. 韩国）、语言测量 инструment（英语 vs. 韩语）和语言模型（GPT-4 vs. GPT-3.5）上进行了 manipulate。我们的结果表明，GPT-4能够复制不同文化之间的每个因素。然而，平均评价显示有上升偏好，并且表现出较低的多样性和结构有效性。总的来说，我们提供了初步的证据，表明LLMs可以助助cross-cultural psychological research。Note: "韩语" (Korean language) is used in the text to refer to the language spoken in South Korea.
</details></li>
</ul>
<hr>
<h2 id="SimCKP-Simple-Contrastive-Learning-of-Keyphrase-Representations"><a href="#SimCKP-Simple-Contrastive-Learning-of-Keyphrase-Representations" class="headerlink" title="SimCKP: Simple Contrastive Learning of Keyphrase Representations"></a>SimCKP: Simple Contrastive Learning of Keyphrase Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08221">http://arxiv.org/abs/2310.08221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brightjade/SimCKP">https://github.com/brightjade/SimCKP</a></li>
<li>paper_authors: Minseok Choi, Chaeheon Gwak, Seho Kim, Si Hyeong Kim, Jaegul Choo</li>
<li>for: 本文目的是提出一种简单的对比学习框架，以提高键短语生成和键短语提取的效果。</li>
<li>methods: 本文使用了一个抽象generator和一个reranker来实现对比学习框架。抽象generator通过学习上下文感知词语表示来提取键短语，同时也生成不在文档中的键短语。reranker则是对生成的每个词语进行适应性分配，使其与文档的表示相似。</li>
<li>results: 实验结果表明，提出的方法在多个benchmark数据集上表现出色，与现有状态的模型相比，具有显著的超越性。<details>
<summary>Abstract</summary>
Keyphrase generation (KG) aims to generate a set of summarizing words or phrases given a source document, while keyphrase extraction (KE) aims to identify them from the text. Because the search space is much smaller in KE, it is often combined with KG to predict keyphrases that may or may not exist in the corresponding document. However, current unified approaches adopt sequence labeling and maximization-based generation that primarily operate at a token level, falling short in observing and scoring keyphrases as a whole. In this work, we propose SimCKP, a simple contrastive learning framework that consists of two stages: 1) An extractor-generator that extracts keyphrases by learning context-aware phrase-level representations in a contrastive manner while also generating keyphrases that do not appear in the document; 2) A reranker that adapts scores for each generated phrase by likewise aligning their representations with the corresponding document. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed approach, which outperforms the state-of-the-art models by a significant margin.
</details>
<details>
<summary>摘要</summary>
“键签生成（KG）的目标是从来源文档中生成一系列概要的词汇或短语，而键签提取（KE）则是从文档中直接找到这些键签。由于搜寻空间较小的KE，因此通常与KG结合以预测文档中可能存在的键签。然而，现有的统一方法通常运用序列标记和最大化生成，主要在字元水平运作，忽略了评估和评分键签的整体性。在这个工作中，我们提出了简单的对照学习框架SimCKP，它包括以下两个阶段：1）抽取生成器，通过学习上下文感知词汇水平表示来提取键签，同时生成不存在文档中的键签；2）改进器，将每个生成的词汇排名更新，根据该词汇与文档的表示相互适合。实验结果显示，我们提出的方法可以对多个标准 benchmark dataset 进行优化，并与现有模型相比，具有较高的效果。”
</details></li>
</ul>
<hr>
<h2 id="TriRE-A-Multi-Mechanism-Learning-Paradigm-for-Continual-Knowledge-Retention-and-Promotion"><a href="#TriRE-A-Multi-Mechanism-Learning-Paradigm-for-Continual-Knowledge-Retention-and-Promotion" class="headerlink" title="TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion"></a>TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08217">http://arxiv.org/abs/2310.08217</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NeurAI-Lab/TriRE">https://github.com/NeurAI-Lab/TriRE</a></li>
<li>paper_authors: Preetha Vijayan, Prashant Bhat, Elahe Arani, Bahram Zonooz</li>
<li>For: The paper aims to address the challenge of continual learning (CL) in deep neural networks, specifically catastrophic forgetting (CF) of previously learned tasks.* Methods: The proposed method, called TriRE, combines several neurophysiological processes, including neurogenesis, active forgetting, neuromodulation, metaplasticity, experience rehearsal, and context-dependent gating, to mitigate CF and improve CL performance.* Results: TriRE significantly reduces task interference and outperforms other CL approaches considered in isolation across various CL settings.<details>
<summary>Abstract</summary>
Continual learning (CL) has remained a persistent challenge for deep neural networks due to catastrophic forgetting (CF) of previously learned tasks. Several techniques such as weight regularization, experience rehearsal, and parameter isolation have been proposed to alleviate CF. Despite their relative success, these research directions have predominantly remained orthogonal and suffer from several shortcomings, while missing out on the advantages of competing strategies. On the contrary, the brain continually learns, accommodates, and transfers knowledge across tasks by simultaneously leveraging several neurophysiological processes, including neurogenesis, active forgetting, neuromodulation, metaplasticity, experience rehearsal, and context-dependent gating, rarely resulting in CF. Inspired by how the brain exploits multiple mechanisms concurrently, we propose TriRE, a novel CL paradigm that encompasses retaining the most prominent neurons for each task, revising and solidifying the extracted knowledge of current and past tasks, and actively promoting less active neurons for subsequent tasks through rewinding and relearning. Across CL settings, TriRE significantly reduces task interference and surpasses different CL approaches considered in isolation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Trustworthy-Machine-Learning"><a href="#Trustworthy-Machine-Learning" class="headerlink" title="Trustworthy Machine Learning"></a>Trustworthy Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08215">http://arxiv.org/abs/2310.08215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matthew-mcateer/practicing_trustworthy_machine_learning">https://github.com/matthew-mcateer/practicing_trustworthy_machine_learning</a></li>
<li>paper_authors: Bálint Mucsányi, Michael Kirchhof, Elisa Nguyen, Alexander Rubinstein, Seong Joon Oh</li>
<li>for: This paper is written for researchers and practitioners who want to build trustworthy machine learning models that can generalize to small changes in the distribution, provide explainability, and quantify uncertainty.</li>
<li>methods: The paper covers four key topics in trustworthy machine learning: out-of-distribution generalization, explainability, uncertainty quantification, and evaluation of trustworthiness. It discusses classical and contemporary research papers in these fields and uncovers their underlying intuitions.</li>
<li>results: The book provides a theoretical and technical background in trustworthy machine learning, including code snippets and pointers to further sources on topics of TML. It is meant to be a stand-alone product and has evolved from a course offered at the University of Tübingen.<details>
<summary>Abstract</summary>
As machine learning technology gets applied to actual products and solutions, new challenges have emerged. Models unexpectedly fail to generalize to small changes in the distribution, tend to be confident on novel data they have never seen, or cannot communicate the rationale behind their decisions effectively with the end users. Collectively, we face a trustworthiness issue with the current machine learning technology. This textbook on Trustworthy Machine Learning (TML) covers a theoretical and technical background of four key topics in TML: Out-of-Distribution Generalization, Explainability, Uncertainty Quantification, and Evaluation of Trustworthiness. We discuss important classical and contemporary research papers of the aforementioned fields and uncover and connect their underlying intuitions. The book evolved from the homonymous course at the University of T\"ubingen, first offered in the Winter Semester of 2022/23. It is meant to be a stand-alone product accompanied by code snippets and various pointers to further sources on topics of TML. The dedicated website of the book is https://trustworthyml.io/.
</details>
<details>
<summary>摘要</summary>
machine learning技术应用到实际产品和解决方案时，新的挑战出现了。模型往往无法泛化到小的分布变化，对新数据感到非常自信，或者无法有效地通过决策的理由与用户交流。总之，我们面临着当前机器学习技术的信任问题。这本《信任worthy机器学习》（TML）教程涵盖了四个关键话题的理论和技术背景：离distribution泛化、解释性、不确定度量和评估信任worthiness。我们讨论了重要的经典和当代研究论文，探索了它们的基本感知。这本书源于同名课程，在2022/23学年冬季学期首次举行。它是一个独立的产品，附有代码示例和各种关于TML主题的资源。相关网站是<https://trustworthyml.io/>。
</details></li>
</ul>
<hr>
<h2 id="Long-Tailed-Classification-Based-on-Coarse-Grained-Leading-Forest-and-Multi-Center-Loss"><a href="#Long-Tailed-Classification-Based-on-Coarse-Grained-Leading-Forest-and-Multi-Center-Loss" class="headerlink" title="Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss"></a>Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08206">http://arxiv.org/abs/2310.08206</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinyery/cognisance">https://github.com/jinyery/cognisance</a></li>
<li>paper_authors: Jinye Yang, Ji Xu</li>
<li>For: This paper aims to address the long-tailed classification problem by proposing a new framework called \textbf{\textsc{Cognisance}, which uses a combination of Coarse-Grained Leading Forest (CLF) and Multi-Center Loss (MCL) to learn invariant features and improve the performance of long-tailed classification.* Methods: The proposed method uses an unsupervised learning method, CLF, to better characterize the distribution of attributes within a class, and introduces a new metric learning loss, MCL, to gradually eliminate confusing attributes during the feature learning process.* Results: The proposed method has state-of-the-art performance in both existing benchmarks ImageNet-GLT and MSCOCO-GLT, and can improve the performance of existing LT methods. The codes are available on GitHub: \url{<a target="_blank" rel="noopener" href="https://github.com/jinyery/cognisance%7D">https://github.com/jinyery/cognisance}</a>.<details>
<summary>Abstract</summary>
Long-tailed(LT) classification is an unavoidable and challenging problem in the real world. Most of the existing long-tailed classification methods focus only on solving the inter-class imbalance in which there are more samples in the head class than in the tail class, while ignoring the intra-lass imbalance in which the number of samples of the head attribute within the same class is much larger than the number of samples of the tail attribute. The deviation in the model is caused by both of these factors, and due to the fact that attributes are implicit in most datasets and the combination of attributes is very complex, the intra-class imbalance is more difficult to handle. For this purpose, we proposed a long-tailed classification framework, known as \textbf{\textsc{Cognisance}, which is founded on Coarse-Grained Leading Forest (CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint solution model by means of invariant feature learning. In this method, we designed an unsupervised learning method, i.e., CLF, to better characterize the distribution of attributes within a class. Depending on the distribution of attributes, we can flexibly construct sampling strategies suitable for different environments. In addition, we introduce a new metric learning loss (MCL), which aims to gradually eliminate confusing attributes during the feature learning process. More importantly, this approach does not depend on a specific model structure and can be integrated with existing LT methods as an independent component. We have conducted extensive experiments and our approach has state-of-the-art performance in both existing benchmarks ImageNet-GLT and MSCOCO-GLT, and can improve the performance of existing LT methods. Our codes are available on GitHub: \url{https://github.com/jinyery/cognisance}
</details>
<details>
<summary>摘要</summary>
Traditional long-tailed classification methods only focus on solving the inter-class imbalance issue, where there are more samples in the head class than in the tail class, while ignoring the intra-class imbalance issue where the number of samples of the head attribute within the same class is much larger than the number of samples of the tail attribute. This leads to deviation in the model. Moreover, attributes are implicit in most datasets and the combination of attributes is very complex, making the intra-class imbalance more difficult to handle.To address these issues, we proposed a long-tailed classification framework called \textbf{\textsc{Cognisance} which is founded on Coarse-Grained Leading Forest (CLF) and Multi-Center Loss (MCL). The goal is to build a multi-granularity joint solution model through invariant feature learning.Our approach includes an unsupervised learning method, CLF, to better characterize the distribution of attributes within a class. Depending on the distribution of attributes, we can flexibly construct sampling strategies suitable for different environments. Additionally, we introduce a new metric learning loss, MCL, which aims to gradually eliminate confusing attributes during the feature learning process.The key advantage of our approach is that it does not depend on a specific model structure and can be integrated with existing LT methods as an independent component. We have conducted extensive experiments and our approach has achieved state-of-the-art performance in both existing benchmarks ImageNet-GLT and MSCOCO-GLT, and can improve the performance of existing LT methods. Our codes are available on GitHub: \url{https://github.com/jinyery/cognisance}.
</details></li>
</ul>
<hr>
<h2 id="Beyond-Traditional-DoE-Deep-Reinforcement-Learning-for-Optimizing-Experiments-in-Model-Identification-of-Battery-Dynamics"><a href="#Beyond-Traditional-DoE-Deep-Reinforcement-Learning-for-Optimizing-Experiments-in-Model-Identification-of-Battery-Dynamics" class="headerlink" title="Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing Experiments in Model Identification of Battery Dynamics"></a>Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing Experiments in Model Identification of Battery Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08198">http://arxiv.org/abs/2310.08198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gokhan Budan, Francesca Damiani, Can Kurtulus, N. Kemal Ure</li>
<li>For: 该研究旨在提高电池模型的建模效率，以便更好地优化能源管理系统和设计过程。* Methods: 该研究使用深度强化学习来改进传统设计实验（DoE）方法，以避免手动配置多个电流配置，并通过更新过去实验统计信息来动态调整当前实验。* Results: 实验和仿真结果表明，提案的方法可以与传统DoE方法相比，使用85% menos资源获得同样准确的电池模型。<details>
<summary>Abstract</summary>
Model identification of battery dynamics is a central problem in energy research; many energy management systems and design processes rely on accurate battery models for efficiency optimization. The standard methodology for battery modelling is traditional design of experiments (DoE), where the battery dynamics are excited with many different current profiles and the measured outputs are used to estimate the system dynamics. However, although it is possible to obtain useful models with the traditional approach, the process is time consuming and expensive because of the need to sweep many different current-profile configurations. In the present work, a novel DoE approach is developed based on deep reinforcement learning, which alters the configuration of the experiments on the fly based on the statistics of past experiments. Instead of sticking to a library of predefined current profiles, the proposed approach modifies the current profiles dynamically by updating the output space covered by past measurements, hence only the current profiles that are informative for future experiments are applied. Simulations and real experiments are used to show that the proposed approach gives models that are as accurate as those obtained with traditional DoE but by using 85\% less resources.
</details>
<details>
<summary>摘要</summary>
模型识别电池动态是能源研究的中心问题，许多能源管理系统和设计过程都依赖于准确的电池模型以优化效率。现行的方法是传统的设计实验（DoE），通过刺激电池动态多种不同的电流 Profiling 并根据测量输出来估算系统动态。然而，尽管可以通过传统方法获得有用的模型，但这个过程占用时间和成本很大，因为需要探索许多不同的电流配置。在 presente 工作中，一种新的DoE方法基于深度强化学习被发展出来，该方法在实验过程中基于过去测量的统计参数来修改配置。相比传统方法，该方法不再仅仅依赖于静态的电流配置库，而是在实验过程中动态地修改电流配置，只有在过去测量中有用的电流配置才会被应用。通过实验和真实实验，我们显示了该方法可以提供与传统DoE相同的准确性，但是使用85% fewer resources。
</details></li>
</ul>
<hr>
<h2 id="EIPE-text-Evaluation-Guided-Iterative-Plan-Extraction-for-Long-Form-Narrative-Text-Generation"><a href="#EIPE-text-Evaluation-Guided-Iterative-Plan-Extraction-for-Long-Form-Narrative-Text-Generation" class="headerlink" title="EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation"></a>EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08185">http://arxiv.org/abs/2310.08185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wang You, Wenshan Wu, Yaobo Liang, Shaoguang Mao, Chenfei Wu, Maosong Cao, Yuzhe Cai, Yiduo Guo, Yan Xia, Furu Wei, Nan Duan</li>
<li>for: 这个论文主要目标是提高长篇文章生成的质量，使其更加 coherent 和 relevante。</li>
<li>methods: 这个论文提出了一种新的框架 called Evaluation-guided Iterative Plan Extraction (EIPE-text)，它从 narrative 干员中提取计划，并使用这些计划来构建更好的规划器。这个框架包括三个阶段：计划提取、学习和推理。在计划提取阶段，它通过迭代提取和改进计划来构建一个计划库。</li>
<li>results: 这个论文的实验结果表明，使用 EIPE-text 可以生成更加 coherent 和 relevante 的长篇文章，比如小说和故事。两个 GPT-4 基于的评估和人类评估都表明了这一点。<details>
<summary>Abstract</summary>
Plan-and-Write is a common hierarchical approach in long-form narrative text generation, which first creates a plan to guide the narrative writing. Following this approach, several studies rely on simply prompting large language models for planning, which often yields suboptimal results. In this paper, we propose a new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner. EIPE-text has three stages: plan extraction, learning, and inference. In the plan extraction stage, it iteratively extracts and improves plans from the narrative corpus and constructs a plan corpus. We propose a question answer (QA) based evaluation mechanism to automatically evaluate the plans and generate detailed plan refinement instructions to guide the iterative improvement. In the learning stage, we build a better planner by fine-tuning with the plan corpus or in-context learning with examples in the plan corpus. Finally, we leverage a hierarchical approach to generate long-form narratives. We evaluate the effectiveness of EIPE-text in the domains of novels and storytelling. Both GPT-4-based evaluations and human evaluations demonstrate that our method can generate more coherent and relevant long-form narratives. Our code will be released in the future.
</details>
<details>
<summary>摘要</summary>
Plan-and-Write 是一种常见的幂等方法，用于长篇叙述文本生成。在这种方法中，首先创建一个指导叙述的计划。然而，许多研究仅通过简单地请求大语言模型生成计划，这经常会得到不佳的结果。在这篇论文中，我们提出了一个新的框架，即评估指导逐步提取计划（EIPE-text）。这个框架包括三个阶段：计划提取、学习和推理。在计划提取阶段，我们使用迭代提取和改进计划的方法，从叙述资源中提取计划，并构建计划库。我们提出了一种问答（QA）基于的评估机制，自动评估计划，并生成详细的计划细化指导，以帮助迭代改进。在学习阶段，我们使用计划库或在计划库中进行Contextual learning 进行训练，以建立更好的规划器。最后，我们采用层次结构来生成长篇叙述。我们在小说和故事领域进行了评估，并得到了人类和 GPT-4 基于的评估结果，表明我们的方法可以生成更 coherent 和 relevante 的长篇叙述。我们将在未来发布代码。
</details></li>
</ul>
<hr>
<h2 id="Learn-From-Model-Beyond-Fine-Tuning-A-Survey"><a href="#Learn-From-Model-Beyond-Fine-Tuning-A-Survey" class="headerlink" title="Learn From Model Beyond Fine-Tuning: A Survey"></a>Learn From Model Beyond Fine-Tuning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08184">http://arxiv.org/abs/2310.08184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ruthless-man/awesome-learn-from-model">https://github.com/ruthless-man/awesome-learn-from-model</a></li>
<li>paper_authors: Hongling Zheng, Li Shen, Anke Tang, Yong Luo, Han Hu, Bo Du, Dacheng Tao</li>
<li>for: 这篇论文主要研究的是基于模型接口的学习从模型（Learn From Model，LFM）技术，以提高模型的性能和普适性。</li>
<li>methods: 该论文主要介绍了五个主要领域的研究方法，包括模型调整、模型萃取、模型重用、元学习和模型编辑。每个领域都包括了多种方法和策略，用于提高模型的表现和普适性。</li>
<li>results: 该论文对现有的研究进行了一个全面的回顾，并提出了未来研究的一些关键领域和需要更多的关注的问题。<details>
<summary>Abstract</summary>
Foundation models (FM) have demonstrated remarkable performance across a wide range of tasks (especially in the fields of natural language processing and computer vision), primarily attributed to their ability to comprehend instructions and access extensive, high-quality data. This not only showcases their current effectiveness but also sets a promising trajectory towards the development of artificial general intelligence. Unfortunately, due to multiple constraints, the raw data of the model used for large model training are often inaccessible, so the use of end-to-end models for downstream tasks has become a new research trend, which we call Learn From Model (LFM) in this article. LFM focuses on the research, modification, and design of FM based on the model interface, so as to better understand the model structure and weights (in a black box environment), and to generalize the model to downstream tasks. The study of LFM techniques can be broadly categorized into five major areas: model tuning, model distillation, model reuse, meta learning and model editing. Each category encompasses a repertoire of methods and strategies that aim to enhance the capabilities and performance of FM. This paper gives a comprehensive review of the current methods based on FM from the perspective of LFM, in order to help readers better understand the current research status and ideas. To conclude, we summarize the survey by highlighting several critical areas for future exploration and addressing open issues that require further attention from the research community. The relevant papers we investigated in this article can be accessed at <https://github.com/ruthless-man/Awesome-Learn-from-Model>.
</details>
<details>
<summary>摘要</summary>
基于模型（FM）在各种任务上表现出色，特别是自然语言处理和计算机视觉领域，这主要归功于它们对指令的理解和访问高质量数据的能力。这不仅表明当前的效果，还预示了人工智能发展的美好趋势。然而，由于多种限制，FM的原始数据通常不可 accessible，因此使用端到端模型进行下游任务的研究成为了新的研究趋势，我们在这篇文章中称之为“学习从模型”（LFM）。LFM的研究重点在于 FM 的模型接口上进行研究、修改和设计，以更好地理解模型结构和权重（在黑盒环境中），并将模型扩展到下游任务。LFM 的研究领域可以分为五大类：模型调整、模型蒸馏、模型复用、元学习和模型编辑。每个类别包括一系列方法和策略，旨在提高 FM 的能力和性能。本文通过 FM 的角度，对当前的 LFM 方法进行了全面的审视，以帮助读者更好地了解当前的研究状况和想法。以下是本文的结论：我们将来的探索细分为五个重要领域，并提出了一些需要进一步关注的问题。相关的研究论文可以在 <https://github.com/ruthless-man/Awesome-Learn-from-Model> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Multi-Scale-Spatial-Temporal-Recurrent-Networks-for-Traffic-Flow-Prediction"><a href="#Multi-Scale-Spatial-Temporal-Recurrent-Networks-for-Traffic-Flow-Prediction" class="headerlink" title="Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow Prediction"></a>Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08138">http://arxiv.org/abs/2310.08138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haiyang Liu, Chunjiang Zhu, Detian Zhang, Qing Li<br>for:  traffic flow predictionmethods:  Multi-Scale Spatial-Temporal Recurrent Network (MSSTRN) with single-step gate recurrent unit and multi-step gate recurrent unit, and spatial-temporal synchronous attention mechanismresults:  best prediction accuracy with non-trivial margins compared to all twenty baseline methods.Here is the simplified Chinese text:for: 交通流量预测methods: 多尺度空间-时间径向网络（MSSTRN），包括单步门闭合径向单元和多步门闭合径向单元，以及空间-时间同步注意力机制results: 与所有基线方法相比，实现了最佳预测精度。<details>
<summary>Abstract</summary>
Traffic flow prediction is one of the most fundamental tasks of intelligent transportation systems. The complex and dynamic spatial-temporal dependencies make the traffic flow prediction quite challenging. Although existing spatial-temporal graph neural networks hold prominent, they often encounter challenges such as (1) ignoring the fixed graph that limits the predictive performance of the model, (2) insufficiently capturing complex spatial-temporal dependencies simultaneously, and (3) lacking attention to spatial-temporal information at different time lengths. In this paper, we propose a Multi-Scale Spatial-Temporal Recurrent Network for traffic flow prediction, namely MSSTRN, which consists of two different recurrent neural networks: the single-step gate recurrent unit and the multi-step gate recurrent unit to fully capture the complex spatial-temporal information in the traffic data under different time windows. Moreover, we propose a spatial-temporal synchronous attention mechanism that integrates adaptive position graph convolutions into the self-attention mechanism to achieve synchronous capture of spatial-temporal dependencies. We conducted extensive experiments on four real traffic datasets and demonstrated that our model achieves the best prediction accuracy with non-trivial margins compared to all the twenty baseline methods.
</details>
<details>
<summary>摘要</summary>
做为智能交通系统的基本任务之一，流行预测是非常复杂和动态的。虽然现有的空间-时间图神经网络具有显著的优势，但它们经常遇到以下困难：（1）忽略固定图，这限制了预测模型的性能；（2）不够同时捕捉复杂的空间-时间依赖关系；（3）缺乏对不同时间长度的空间-时间信息的注意力。在这篇论文中，我们提出了一种多级空间-时间循环网络（MSSTRN），它包括单步门阻循环单元和多步门阻循环单元，以全面捕捉不同时间窗口下的复杂空间-时间信息。此外，我们提出了一种空间-时间同步注意机制，它将适应性位图 convolution integrated into the self-attention mechanism，以同步捕捉空间-时间依赖关系。我们对四个实际交通数据集进行了广泛的实验，并证明了我们的模型在所有二十个基eline方法的比较下具有最好的预测精度。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Really-Improve-by-Self-critiquing-Their-Own-Plans"><a href="#Can-Large-Language-Models-Really-Improve-by-Self-critiquing-Their-Own-Plans" class="headerlink" title="Can Large Language Models Really Improve by Self-critiquing Their Own Plans?"></a>Can Large Language Models Really Improve by Self-critiquing Their Own Plans?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08118">http://arxiv.org/abs/2310.08118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karthik Valmeekam, Matthew Marquez, Subbarao Kambhampati</li>
<li>for:  investigate the verification&#x2F;self-critiquing abilities of large language models in the context of planning</li>
<li>methods:  employ LLMs for both plan generation and verification, assess the verifier LLM’s performance against ground-truth verification, and evaluate the impact of self-critiquing and feedback levels on system performance</li>
<li>results:  self-critiquing appears to diminish plan generation performance, LLM verifiers produce a notable number of false positives, and the nature of feedback has minimal impact on plan generation.<details>
<summary>Abstract</summary>
There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation. Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.
</details>
<details>
<summary>摘要</summary>
有很多人提出了大型自然语言模型（LLM）可以成功验证或自我批判其候选解决方案的宣传。为了调查这些宣传，我们在这篇论文中进行了大语言模型在规划中的验证/自我批判能力的调查。我们评估了使用LLM进行生成和验证的规划系统。我们评估了验证LLM的性能与基准验证、自我批判对计划生成的影响以及不同反馈水平对系统性能的影响。使用GPT-4，当前的状态顶尖LLM，进行生成和验证，我们发现自我批判对计划生成性能产生了负面影响，尤其是与外部、有效的验证器和LLM验证器相比。此外，我们发现验证LLM生成的许多假阳性，这使得系统的可靠性受到了损害。此外，反馈的性质，无论是binary还是详细，对计划生成没有显著影响。总之，我们的结果表明LLM在自我批判、迭代模式下的规划任务效果不足。
</details></li>
</ul>
<hr>
<h2 id="DUSA-Decoupled-Unsupervised-Sim2Real-Adaptation-for-Vehicle-to-Everything-Collaborative-Perception"><a href="#DUSA-Decoupled-Unsupervised-Sim2Real-Adaptation-for-Vehicle-to-Everything-Collaborative-Perception" class="headerlink" title="DUSA: Decoupled Unsupervised Sim2Real Adaptation for Vehicle-to-Everything Collaborative Perception"></a>DUSA: Decoupled Unsupervised Sim2Real Adaptation for Vehicle-to-Everything Collaborative Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08117">http://arxiv.org/abs/2310.08117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/refkxh/DUSA">https://github.com/refkxh/DUSA</a></li>
<li>paper_authors: Xianghao Kong, Wentao Jiang, Jinrang Jia, Yifeng Shi, Runsheng Xu, Si Liu</li>
<li>for: 这个研究是为了解决自动驾驶需要高精度的车辆到所有事物（V2X）的共同感知问题，但是获得大量真实世界数据可能是costly和difficult的。因此，实验数据获得了更多的注意，因为它们可以在非常低的成本下生成大量的数据。但是，实验和真实世界之间的领域差强度常常导致从实验数据训练的模型在真实世界数据上表现不佳。</li>
<li>methods: 这个研究使用了一种名为Decoupled Unsupervised Sim2Real Adaptation（DUSA）的新方法，它将V2X共同感知领域的 sim2real 领域对应问题分解为两个互相独立的子问题： sim2real 适应和间 agent 适应。在 sim2real 适应方面，我们设计了一个位置适应的 LSA（Location-adaptive Sim2Real Adapter）模组，将从critical locations of the feature map中提取的特征进行适应，并通过一个 sim&#x2F;real 检测器来调整这些特征与实验数据之间的对应。在间 agent 适应方面，我们还提出了一个 Confidence-aware Inter-agent Adapter（CIA）模组，将Agent-wise confidence maps的指导下进行细部特征的对应。</li>
<li>results: 实验结果显示，提案的 DUSA 方法在无supervision的 sim2real 适应上具有优秀的效果，从 simulated V2XSet 数据集中获得了高精度的 V2X 共同感知结果，并且在真实世界 DAIR-V2X-C 数据集上进行验证。<details>
<summary>Abstract</summary>
Vehicle-to-Everything (V2X) collaborative perception is crucial for autonomous driving. However, achieving high-precision V2X perception requires a significant amount of annotated real-world data, which can always be expensive and hard to acquire. Simulated data have raised much attention since they can be massively produced at an extremely low cost. Nevertheless, the significant domain gap between simulated and real-world data, including differences in sensor type, reflectance patterns, and road surroundings, often leads to poor performance of models trained on simulated data when evaluated on real-world data. In addition, there remains a domain gap between real-world collaborative agents, e.g. different types of sensors may be installed on autonomous vehicles and roadside infrastructures with different extrinsics, further increasing the difficulty of sim2real generalization. To take full advantage of simulated data, we present a new unsupervised sim2real domain adaptation method for V2X collaborative detection named Decoupled Unsupervised Sim2Real Adaptation (DUSA). Our new method decouples the V2X collaborative sim2real domain adaptation problem into two sub-problems: sim2real adaptation and inter-agent adaptation. For sim2real adaptation, we design a Location-adaptive Sim2Real Adapter (LSA) module to adaptively aggregate features from critical locations of the feature map and align the features between simulated data and real-world data via a sim/real discriminator on the aggregated global feature. For inter-agent adaptation, we further devise a Confidence-aware Inter-agent Adapter (CIA) module to align the fine-grained features from heterogeneous agents under the guidance of agent-wise confidence maps. Experiments demonstrate the effectiveness of the proposed DUSA approach on unsupervised sim2real adaptation from the simulated V2XSet dataset to the real-world DAIR-V2X-C dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Promptor-A-Conversational-and-Autonomous-Prompt-Generation-Agent-for-Intelligent-Text-Entry-Techniques"><a href="#Promptor-A-Conversational-and-Autonomous-Prompt-Generation-Agent-for-Intelligent-Text-Entry-Techniques" class="headerlink" title="Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques"></a>Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08101">http://arxiv.org/abs/2310.08101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junxiao Shen, John J. Dudley, Jingyao Zheng, Bill Byrne, Per Ola Kristensson</li>
<li>for: 这篇研究旨在提高文本输入的效率和流畅性，并且应对深度学习模型在文本输入中的应用。</li>
<li>methods: 这篇研究使用了大型语言模型GPT-3.5的内置学习能力，将其训练为不同的文本预测技术。另外，还引入了一个对话式提示生成器Promptor，以帮助设计师创建适当的提示。</li>
<li>results: 研究结果显示，使用Promptor生成的提示可以提高文本预测的相似度和 coherence 比设计师自己创建的提示高出35%和22%。<details>
<summary>Abstract</summary>
Text entry is an essential task in our day-to-day digital interactions. Numerous intelligent features have been developed to streamline this process, making text entry more effective, efficient, and fluid. These improvements include sentence prediction and user personalization. However, as deep learning-based language models become the norm for these advanced features, the necessity for data collection and model fine-tuning increases. These challenges can be mitigated by harnessing the in-context learning capability of large language models such as GPT-3.5. This unique feature allows the language model to acquire new skills through prompts, eliminating the need for data collection and fine-tuning. Consequently, large language models can learn various text prediction techniques. We initially showed that, for a sentence prediction task, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is comparable with a fine-tuned GPT-3.5 model, with the latter two methods requiring costly data collection, fine-tuning and post-processing. However, the task of prompting large language models to specialize in specific text prediction tasks can be challenging, particularly for designers without expertise in prompt engineering. To address this, we introduce Promptor, a conversational prompt generation agent designed to engage proactively with designers. Promptor can automatically generate complex prompts tailored to meet specific needs, thus offering a solution to this challenge. We conducted a user study involving 24 participants creating prompts for three intelligent text entry tasks, half of the participants used Promptor while the other half designed prompts themselves. The results show that Promptor-designed prompts result in a 35% increase in similarity and 22% in coherence over those by designers.
</details>
<details>
<summary>摘要</summary>
文本输入是我们日常数字互动中的基本任务。许多智能功能已经被开发出来，以减少这个过程的复杂性、效率和流畅性。这些改进包括句子预测和用户个性化。然而，随着深度学习基于语言模型成为标准，数据采集和模型细化的必要性增加。这些挑战可以通过大语言模型的上下文学习能力来解决，例如GPT-3.5。这种特有的功能允许语言模型通过提示来获得新的技能，从而消除数据采集和细化的需要。因此，大语言模型可以学习多种文本预测技术。我们的初步研究表明，对于句子预测任务，只需提示GPT-3.5，其性能比GPT-2 backing system和精心细化GPT-3.5模型高，但需要大量数据采集、细化和后处理。然而，让大语言模型专注于特定文本预测任务的任务可能是挑战，特别是没有提示工程学习的设计师。为解决这个问题，我们介绍了Promptor，一个用于生成对话提示的对话引擎，旨在与设计师进行激活engage。Promptor可以自动生成特定需求的复杂提示，因此为这个挑战提供了解决方案。我们对24名参与者进行了用户研究，其中一半使用Promptor，另一半设计自己的提示。结果表明，Promptor-设计的提示与设计师自己设计的提示相比，同样的任务上的相似性提高35%， coherence提高22%。
</details></li>
</ul>
<hr>
<h2 id="Sentinel-An-Aggregation-Function-to-Secure-Decentralized-Federated-Learning"><a href="#Sentinel-An-Aggregation-Function-to-Secure-Decentralized-Federated-Learning" class="headerlink" title="Sentinel: An Aggregation Function to Secure Decentralized Federated Learning"></a>Sentinel: An Aggregation Function to Secure Decentralized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08097">http://arxiv.org/abs/2310.08097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Feng, Alberto Huertas Celdran, Janosch Baltensperger, Enrique Tomas Martinez Beltran, Gerome Bovet, Burkhard Stiller</li>
<li>for: 本研究旨在提出一种防御策略，以counteract poisoning attacks在分布式学习（DFL）中。</li>
<li>methods: 该策略基于本地数据的可访问性，定义了三步集成协议：相似性筛选、bootstrap验证和 нормализация，以保护 Against malicious model updates。</li>
<li>results: 对于多种数据集和攻击类型和威胁水平，Sentinel可以提高防御性能，超越当前领域的状态之作。<details>
<summary>Abstract</summary>
The rapid integration of Federated Learning (FL) into networking encompasses various aspects such as network management, quality of service, and cybersecurity while preserving data privacy. In this context, Decentralized Federated Learning (DFL) emerges as an innovative paradigm to train collaborative models, addressing the single point of failure limitation. However, the security and trustworthiness of FL and DFL are compromised by poisoning attacks, negatively impacting its performance. Existing defense mechanisms have been designed for centralized FL and they do not adequately exploit the particularities of DFL. Thus, this work introduces Sentinel, a defense strategy to counteract poisoning attacks in DFL. Sentinel leverages the accessibility of local data and defines a three-step aggregation protocol consisting of similarity filtering, bootstrap validation, and normalization to safeguard against malicious model updates. Sentinel has been evaluated with diverse datasets and various poisoning attack types and threat levels, improving the state-of-the-art performance against both untargeted and targeted poisoning attacks.
</details>
<details>
<summary>摘要</summary>
随着联邦学习（FL）在网络中的快速整合，包括网络管理、质量服务和网络安全，同时保护数据隐私。在这个 контексте，分布式联邦学习（DFL） emerges as an innovative paradigm to train collaborative models, addressing the single point of failure limitation。然而，FL和DFL的安全性和可靠性受到毒素攻击的威胁，这会 negatively impact its performance。现有的防御机制是为中央化FL设计的，它们不充分利用了 DFL 的特点。因此，这个工作介绍了 Sentinel，一种防御策略，用于对抗毒素攻击在 DFL 中。Sentinel 利用了本地数据的可 accessible 性，并定义了三步集成协议，包括相似性筛选、 bootstrap 验证和归一化，以保护 против 恶意模型更新。Sentinel 在多种数据集和不同类型和威胁水平的攻击下进行了评估，提高了对于不argeted和targeted毒素攻击的状态前艺性表现。
</details></li>
</ul>
<hr>
<h2 id="Discerning-Temporal-Difference-Learning"><a href="#Discerning-Temporal-Difference-Learning" class="headerlink" title="Discerning Temporal Difference Learning"></a>Discerning Temporal Difference Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08091">http://arxiv.org/abs/2310.08091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianfei Ma</li>
<li>for: 提高 reinforcement learning 中值函数的效率评估</li>
<li>methods: 使用 temporal difference learning ($\lambda$) 和 flexible emphasis functions</li>
<li>results: 提高 value estimation 和 学习速度，适用于多种情况<details>
<summary>Abstract</summary>
Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites learning across diverse scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Temporal difference learning（TD）是RL中的基本概念，旨在效率地评估策略的价值函数。TD($\lambda$)是一种强大的变体，它在历史上追溯记忆中分配预测错误。然而，这种方法经常忽略历史状态的重要性和对TD错误的相对重要性，这可能导致探索偏误或结果噪音的问题。为解决这个问题，我们提出了一种新的TD算法，名为分化TD学习（DTD），它允许在训练过程中预先或适应定制强调函数，以有效地分配努力 across状态。我们证明了我们的方法在特定类型的强调函数下的收敛性质，并在深度RLContext中展示了其扬名的潜力。实验结果表明，采用合适的强调函数不仅改善价值估计，而且加快学习过程中的探索。Translation notes:* TD($\lambda$) is translated as TD($\lambda$)，where $\lambda$ is a memory trace.* 历史状态 (lìshǐ zhèngjī) is translated as "historical states".* 探索偏误 (tànsuǒ biànpò) is translated as "exploration bias".* 结果噪音 (jiéguǒ zhāoxīn) is translated as "outcome noise".* 强调函数 (qiángdǎo fungs) is translated as "emphasis functions".* 深度RLContext (shēngrán yījīng) is translated as "deep RL contexts".
</details></li>
</ul>
<hr>
<h2 id="Low-Resource-Clickbait-Spoiling-for-Indonesian-via-Question-Answering"><a href="#Low-Resource-Clickbait-Spoiling-for-Indonesian-via-Question-Answering" class="headerlink" title="Low-Resource Clickbait Spoiling for Indonesian via Question Answering"></a>Low-Resource Clickbait Spoiling for Indonesian via Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08085">http://arxiv.org/abs/2310.08085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ni Putu Intan Maharani, Ayu Purwarianti, Alham Fikri Aji</li>
<li>for: 这个论文的目的是怎样解决Clickbait spoiling问题，即通过生成短文满足Clickbait文章中引起的Curiosity。</li>
<li>methods: 这个论文使用了跨语言零shot问答模型，以及多语言模型，来解决Clickbait spoiling问题。</li>
<li>results: 实验结果表明，XLM-RoBERTa（大）模型在短语和段落 spoilers 中表现最佳，而 mDeBERTa（基础）模型在多部 spoilers 中表现最佳。<details>
<summary>Abstract</summary>
Clickbait spoiling aims to generate a short text to satisfy the curiosity induced by a clickbait post. As it is a newly introduced task, the dataset is only available in English so far. Our contributions include the construction of manually labeled clickbait spoiling corpus in Indonesian and an evaluation on using cross-lingual zero-shot question answering-based models to tackle clikcbait spoiling for low-resource language like Indonesian. We utilize selection of multilingual language models. The experimental results suggest that XLM-RoBERTa (large) model outperforms other models for phrase and passage spoilers, meanwhile, mDeBERTa (base) model outperforms other models for multipart spoilers.
</details>
<details>
<summary>摘要</summary>
Clickbait 恶作戏目的是生成一篇短文以满足 clickbait 帖子所引起的好奇心。现在这个任务刚刚引入，数据集只有英语版本。我们的贡献包括手动标注的 Indonesian  clickbait 恶作戏训练集，以及使用 cross-lingual zero-shot 问答模型来解决 low-resource 语言 like Indonesian 的 clickbait 恶作戏。我们利用多语言语言模型的选择。实验结果表明， XLM-RoBERTa (大) 模型在短语和段落 spoilers 方面表现出色，而 mDeBERTa (基础) 模型在多部 spoilers 方面表现更佳。
</details></li>
</ul>
<hr>
<h2 id="GameGPT-Multi-agent-Collaborative-Framework-for-Game-Development"><a href="#GameGPT-Multi-agent-Collaborative-Framework-for-Game-Development" class="headerlink" title="GameGPT: Multi-agent Collaborative Framework for Game Development"></a>GameGPT: Multi-agent Collaborative Framework for Game Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08067">http://arxiv.org/abs/2310.08067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, Haoyang Zhang</li>
<li>for:  automatize and expedite game development processes</li>
<li>methods:  dual collaboration, layered approaches with several in-house lexicons, and decoupling approach</li>
<li>results:  mitigate hallucination and redundancy in planning, task identification, and implementation phases, and achieve code generation with better precision.Here’s the full translation of the paper’s abstract in Simplified Chinese:</li>
<li>for: 这个论文主要是为了自动化和加速游戏开发过程而写的。</li>
<li>methods: 该框架使用了双合作、层次分解和多个内部词典的方法来 Mitigate hallucination和重复性在规划、任务标识和实现阶段。</li>
<li>results: 这些方法可以减少hallucination和重复性，并实现代码生成更加精准。<details>
<summary>Abstract</summary>
The large language model (LLM) based agents have demonstrated their capacity to automate and expedite software development processes. In this paper, we focus on game development and propose a multi-agent collaborative framework, dubbed GameGPT, to automate game development. While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern: redundancy. Our framework presents a series of methods to mitigate both concerns. These methods include dual collaboration and layered approaches with several in-house lexicons, to mitigate the hallucination and redundancy in the planning, task identification, and implementation phases. Furthermore, a decoupling approach is also introduced to achieve code generation with better precision.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）基于代理的代理系统已经展示了自动化和加速软件开发过程的能力。在这篇文章中，我们专注于游戏开发，并提出了一个多代理协同框架，名为GameGPT，以自动化游戏开发。许多研究都指出了推几成为 LLM 在生产环境中应用时的主要障碍。我们则识别了另一个问题：重复。我们的框架提出了一系列方法来减轻这两个问题。这些方法包括双投递和层次方法，以减少在规划、任务识别和实现阶段中的重复和推几。此外，我们还引入了解离方法，以实现代码生成的更高精度。
</details></li>
</ul>
<hr>
<h2 id="The-Search-and-Mix-Paradigm-in-Approximate-Nash-Equilibrium-Algorithms"><a href="#The-Search-and-Mix-Paradigm-in-Approximate-Nash-Equilibrium-Algorithms" class="headerlink" title="The Search-and-Mix Paradigm in Approximate Nash Equilibrium Algorithms"></a>The Search-and-Mix Paradigm in Approximate Nash Equilibrium Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08066">http://arxiv.org/abs/2310.08066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaotie Deng, Dongchen Li, Hanyu Li</li>
<li>for: 本文旨在提供一种自动化筛选和混合方法，用于计算 approximate Nash equilibria 在两个玩家的游戏中。</li>
<li>methods: 本文使用了一种搜索和混合方法，其中包括一个搜索阶段和一个混合阶段。通过这种方法，我们可以自动化筛选和混合过程，并不需要手写证明。</li>
<li>results: 本文通过自动化筛选和混合方法，可以计算出所有文献中的算法精度下界。同时，我们还可以使用一个程序来分析这些下界，而不需要手写证明。这种方法可以扩展到其他算法中，以自动化其分析。<details>
<summary>Abstract</summary>
AI in Math deals with mathematics in a constructive manner so that reasoning becomes automated, less laborious, and less error-prone. For algorithms, the question becomes how to automate analyses for specific problems. For the first time, this work provides an automatic method for approximation analysis on a well-studied problem in theoretical computer science: computing approximate Nash equilibria in two-player games. We observe that such algorithms can be reformulated into a search-and-mix paradigm, which involves a search phase followed by a mixing phase. By doing so, we are able to fully automate the procedure of designing and analyzing the mixing phase. For example, we illustrate how to perform our method with a program to analyze the approximation bounds of all the algorithms in the literature. Same approximation bounds are computed without any hand-written proof. Our automatic method heavily relies on the LP-relaxation structure in approximate Nash equilibria. Since many approximation algorithms and online algorithms adopt the LP relaxation, our approach may be extended to automate the analysis of other algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-from-Label-Proportions-Bootstrapping-Supervised-Learners-via-Belief-Propagation"><a href="#Learning-from-Label-Proportions-Bootstrapping-Supervised-Learners-via-Belief-Propagation" class="headerlink" title="Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation"></a>Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08056">http://arxiv.org/abs/2310.08056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyas Havaldar, Navodita Sharma, Shubhi Sareen, Karthikeyan Shanmugam, Aravindan Raghuveer</li>
<li>for: 本文targets the Learning from Label Proportions (LLP) problem, where only aggregate level labels are available during training, and the aim is to achieve the best performance at the instance-level on test data.</li>
<li>methods: 本文提出了一个新的算法框架，包括两个主要步骤：Pseudo Labeling和Embedding Refinement。在Pseudo Labeling阶段，我们使用Gibbs分布 incorporate covariate information和bag level aggregated label，然后使用Belief Propagation marginalize Gibbs distribution获得pseudo labels。在Embedding Refinement阶段，我们使用pseudo labels提供supervision for a learner to obtain a better embedding。</li>
<li>results: 本文的算法在LLPBinary Classification问题上 display strong gains against several SOTA baselines (up to 15%) on various dataset types - tabular and Image. 更重要的是，我们的算法可以在大袋子样本数量下 достичь这些提高，并且具有较少的计算负担。<details>
<summary>Abstract</summary>
Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps again by using the second step's embeddings as new covariates for the next iteration. In the final iteration, a classifier is trained using the pseudo labels. Our algorithm displays strong gains against several SOTA baselines (up to 15%) for the LLP Binary Classification problem on various dataset types - tabular and Image. We achieve these improvements with minimal computational overhead above standard supervised learning due to Belief Propagation, for large bag sizes, even for a million samples.
</details>
<details>
<summary>摘要</summary>
学习从标签分布（LLP）是一个学习问题，在训练时只有袋子级别标签可用，并且目标是在测试数据上达到最佳实例级别性能。这种设定出现在广告和医疗等领域 due to privacy considerations。我们提出了一种新的算法框架，它在每次迭代中执行两个主要步骤。在第一步（假标签生成）中，我们定义了一个 Gibbs 分布 над二进制实例标签，该分布包含 a) covariate 信息通过要求同 covariate 的实例有同样的标签，以及 b) 袋子级别归一化标签。然后，我们使用信念传播（BP）来抽象 Gibbs 分布，从而获得假标签。在第二步（嵌入级修正）中，我们使用假标签来提供对一个更好的嵌入的超vision。然后，我们在下一轮迭代中使用上一轮的嵌入作为新的covariate。在最后一轮迭代中，我们使用假标签来训练一个分类器。我们的算法在LLP binary classification问题中 Display strong gains against several SOTA baselines (up to 15%) on various dataset types - tabular and Image. We achieve these improvements with minimal computational overhead above standard supervised learning due to Belief Propagation, even for large bag sizes, even for a million samples.
</details></li>
</ul>
<hr>
<h2 id="Understanding-and-Controlling-a-Maze-Solving-Policy-Network"><a href="#Understanding-and-Controlling-a-Maze-Solving-Policy-Network" class="headerlink" title="Understanding and Controlling a Maze-Solving Policy Network"></a>Understanding and Controlling a Maze-Solving Policy Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08043">http://arxiv.org/abs/2310.08043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ulisse Mini, Peli Grietzer, Mrinank Sharma, Austin Meek, Monte MacDiarmid, Alexander Matt Turner</li>
<li>for: 研究AI系统的目标和目标表现方式，通过实际测试一个预训练的套件学习策略，实现迷宫 Navigation 的多个上下文依赖性目标。</li>
<li>methods: 使用实验研究方法，精确地研究这个策略所解决的迷宫 Navigation 问题，并对策略中的不同部分进行修改和测试，以探索策略中的多个目标表现方式。</li>
<li>results: 研究发现，这个策略包含多个重复、分散和可重新目标表现方式，并且可以通过修改策略中的不同部分来控制策略的行为。这些结果提供了关于对AI系统的目标和目标表现方式的更深入理解。<details>
<summary>Abstract</summary>
To understand the goals and goal representations of AI systems, we carefully study a pretrained reinforcement learning policy that solves mazes by navigating to a range of target squares. We find this network pursues multiple context-dependent goals, and we further identify circuits within the network that correspond to one of these goals. In particular, we identified eleven channels that track the location of the goal. By modifying these channels, either with hand-designed interventions or by combining forward passes, we can partially control the policy. We show that this network contains redundant, distributed, and retargetable goal representations, shedding light on the nature of goal-direction in trained policy networks.
</details>
<details>
<summary>摘要</summary>
要了解人工智能系统的目标和目标表达，我们仔细研究了一个预训练的奖励学习策略，该策略在迷宫中穿梭到多种目标方块。我们发现该网络追求多个Context-dependent目标，并且我们进一步确定了网络中的一些Circuits与这些目标相关。例如，我们发现了11个跟踪目标的通道。通过修改这些通道，可以在一定程度上控制策略。我们显示，这个网络包含多余的、分布式的和可重定向的目标表达，这 shed light on the nature of goal-direction in trained policy networks。
</details></li>
</ul>
<hr>
<h2 id="QLLM-Accurate-and-Efficient-Low-Bitwidth-Quantization-for-Large-Language-Models"><a href="#QLLM-Accurate-and-Efficient-Low-Bitwidth-Quantization-for-Large-Language-Models" class="headerlink" title="QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models"></a>QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08041">http://arxiv.org/abs/2310.08041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, Bohan Zhuang</li>
<li>for: 提高大型语言模型（LLMs）的广泛部署，因为它们的需求很高。</li>
<li>methods: 提议使用Quantization-Aware Training（QAT）来解决这个问题，但QAT的训练成本很高。因此，提议使用Post-Training Quantization（PTQ）来实现LLMs的低位数部署。</li>
<li>results: 提出了一种名为QLLM的准确和高效的低位数PTQ方法，可以快速地量化LLMs。例如，在LLaMA-2上，QLLM可以在10个小时内量化4位数LLaMA-2-70B模型，比前一个state-of-the-art方法提高了7.89%的均值准确率。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a more balanced distribution of activation magnitudes. Then similar channels are merged to maintain the original channel number for efficiency. Additionally, an adaptive strategy is designed to autonomously determine the optimal number of sub-channels for channel disassembly. To further compensate for the performance loss caused by quantization, we propose an efficient tuning method that only learns a small number of low-rank weights while freezing the pre-trained quantized model. After training, these low-rank parameters can be fused into the frozen weights without affecting inference. Extensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate quantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B within 10 hours on a single A100-80G GPU, outperforming the previous state-of-the-art method by 7.89% on the average accuracy across five zero-shot tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在自然语言处理（NLP）领域表现出色，但它们的需求限制了它们的广泛部署。量化意识训练（QAT）提供了一种解决方案，但它的训练成本非常高，因此在LLM中使用Post-Training Quantization（PTQ）成为了更实际的方法。在现有的研究中，活动异常值在特定通道被识别为PTQ准确率的瓶颈。它们提议将活动的大小从权重转移到 weights，但这些方法具有有限的缓解或因为不稳定的梯度而导致性能下降。在这篇论文中，我们提出了QLLM，一种高效和准确的低位宽PTQ方法，适用于LLM。QLLM引入了适应通道重新组装技术，通过将异常通道的大小分配到其他通道来缓解其影响量化范围。这是通过通道分解和通道组装来实现的，先将异常通道分解成多个子通道，以确保更加平衡的活动大小分布。然后，类似通道被合并以维持原始通道数量的效率。此外，我们还提出了一种自适应的整数截取策略，以确定最佳的子通道数量。为了进一步补偿由量化所带来的性能损失，我们还提出了一种高效的调整方法，只需学习一小部分的低维度参数，而不影响推理。经验表明，QLLM可以高效地生成准确的量化模型，比如在4位LLAMA-2-70B上，QLLM在10个小时内在单个A100-80G GPU上量化，并在五个零shot任务上平均提高了7.89%的准确率。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Large-scale-Pre-ranking-System-Entire-chain-Cross-domain-Models"><a href="#Rethinking-Large-scale-Pre-ranking-System-Entire-chain-Cross-domain-Models" class="headerlink" title="Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain Models"></a>Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08039">http://arxiv.org/abs/2310.08039</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songjinbo/ECMM">https://github.com/songjinbo/ECMM</a></li>
<li>paper_authors: Jinbo Song, Ruoran Huang, Xinyang Wang, Wei Huang, Qian Yu, Mingming Chen, Yafei Yao, Chaosheng Fan, Changping Peng, Zhangang Lin, Jinghe Hu, Jingping Shao</li>
<li>for: 提高推荐系统和在线广告中的多Stage架构的性能，减少样本选择偏见问题。</li>
<li>methods: 提出基于全样本空间的整体链模型（ECM），并设计了细化神经网络结构ECMM以提高预选精度。</li>
<li>results: 对实际大规模交通日志数据进行评估，ECM模型比状态艺术法准确率高，时间消耗在可接受水平之下，实现了更好的效率和效果之间的交易。<details>
<summary>Abstract</summary>
Industrial systems such as recommender systems and online advertising, have been widely equipped with multi-stage architectures, which are divided into several cascaded modules, including matching, pre-ranking, ranking and re-ranking. As a critical bridge between matching and ranking, existing pre-ranking approaches mainly endure sample selection bias (SSB) problem owing to ignoring the entire-chain data dependence, resulting in sub-optimal performances. In this paper, we rethink pre-ranking system from the perspective of the entire sample space, and propose Entire-chain Cross-domain Models (ECM), which leverage samples from the whole cascaded stages to effectively alleviate SSB problem. Besides, we design a fine-grained neural structure named ECMM to further improve the pre-ranking accuracy. Specifically, we propose a cross-domain multi-tower neural network to comprehensively predict for each stage result, and introduce the sub-networking routing strategy with $L0$ regularization to reduce computational costs. Evaluations on real-world large-scale traffic logs demonstrate that our pre-ranking models outperform SOTA methods while time consumption is maintained within an acceptable level, which achieves better trade-off between efficiency and effectiveness.
</details>
<details>
<summary>摘要</summary>
Note:* 推荐系统 (recommender systems) is translated as 推荐系统* online advertising is translated as 在线广告* SOTA (state-of-the-art) is translated as 当前最佳方案* SSB (sample selection bias) is translated as 样本选择偏见
</details></li>
</ul>
<hr>
<h2 id="Receive-Reason-and-React-Drive-as-You-Say-with-Large-Language-Models-in-Autonomous-Vehicles"><a href="#Receive-Reason-and-React-Drive-as-You-Say-with-Large-Language-Models-in-Autonomous-Vehicles" class="headerlink" title="Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles"></a>Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08034">http://arxiv.org/abs/2310.08034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Ziran Wang</li>
<li>for: 提高自动驾驶车辆的安全性和效率，通过语言模型增强决策过程</li>
<li>methods: 利用语言模型的语言和上下文理解能力，与专门的工具集成在自动驾驶车辆中</li>
<li>results: 实验表明，使用链条提示可以提高驾驶决策，并实现实时个性化驾驶，语言模型可以 influencing driving behaviors based on verbal commands, leading to improved driving decisions and personalized driving experiences.<details>
<summary>Abstract</summary>
The fusion of human-centric design and artificial intelligence (AI) capabilities has opened up new possibilities for next-generation autonomous vehicles that go beyond transportation. These vehicles can dynamically interact with passengers and adapt to their preferences. This paper proposes a novel framework that leverages Large Language Models (LLMs) to enhance the decision-making process in autonomous vehicles. By utilizing LLMs' linguistic and contextual understanding abilities with specialized tools, we aim to integrate the language and reasoning capabilities of LLMs into autonomous vehicles. Our research includes experiments in HighwayEnv, a collection of environments for autonomous driving and tactical decision-making tasks, to explore LLMs' interpretation, interaction, and reasoning in various scenarios. We also examine real-time personalization, demonstrating how LLMs can influence driving behaviors based on verbal commands. Our empirical results highlight the substantial advantages of utilizing chain-of-thought prompting, leading to improved driving decisions, and showing the potential for LLMs to enhance personalized driving experiences through ongoing verbal feedback. The proposed framework aims to transform autonomous vehicle operations, offering personalized support, transparent decision-making, and continuous learning to enhance safety and effectiveness. We achieve user-centric, transparent, and adaptive autonomous driving ecosystems supported by the integration of LLMs into autonomous vehicles.
</details>
<details>
<summary>摘要</summary>
人 centered 设计和人工智能（AI）技术的融合已经开启了下一代自动驾驶车的新可能性，这些车辆可以在交通过程中动态与乘客交互，并根据他们的偏好进行适应。本文提出了一种新的框架，利用大型自然语言模型（LLM）来增强自动驾驶车的决策过程。通过利用 LLM 的语言和上下文理解能力，我们希望将语言和思维能力 integrate into autonomous vehicles。我们的研究包括在 HighwayEnv 环境中进行了一系列的实验，以探索 LLM 在不同场景中的解释、交互和思维能力。我们还考虑了实时个性化，以示如何 LLM 可以根据 verbalemands 的指令来影响驾驶行为。我们的实验结果表明，使用 chain-of-thought prompting 可以提高驾驶决策的质量，并显示 LLM 可以在不断的语言反馈中提供个性化驾驶体验。我们的提案框架 aimsto transform autonomous vehicle operations, offering personalized support, transparent decision-making, and continuous learning to enhance safety and effectiveness。我们实现了用户中心、透明和 adaptive 的自动驾驶生态系统，通过 LLM 的 integrate into autonomous vehicles。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Domain-Knowledge-Graph-into-Multimodal-Movie-Genre-Classification-with-Self-Supervised-Attention-and-Contrastive-Learning"><a href="#Incorporating-Domain-Knowledge-Graph-into-Multimodal-Movie-Genre-Classification-with-Self-Supervised-Attention-and-Contrastive-Learning" class="headerlink" title="Incorporating Domain Knowledge Graph into Multimodal Movie Genre Classification with Self-Supervised Attention and Contrastive Learning"></a>Incorporating Domain Knowledge Graph into Multimodal Movie Genre Classification with Self-Supervised Attention and Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08032">http://arxiv.org/abs/2310.08032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aoluming/IDKG">https://github.com/aoluming/IDKG</a></li>
<li>paper_authors: Jiaqi Li, Guilin Qi, Chuanyi Zhang, Yongrui Chen, Yiming Tan, Chenlong Xia, Ye Tian</li>
<li>for: 这篇论文旨在提高多模态电影类别分类的性能，解决了 metadata 中的群体关系未利用、自动注意分配和综合特征混合等问题。</li>
<li>methods: 该方法利用知识图从多种角度来解决这些问题，首先将 metadata 转化为域知识图，然后使用 translate model 获取知识图中的关系。接着，引入自动注意分配模块，使用自我超vised学习学习知识图的分布，生成合理的注意重量。最后，提出一种 Genre-Centroid Anchored Contrastive Learning 模块，增强综合特征的抑制能力。</li>
<li>results: 实验结果表明，我们的方法在 MM-IMDb 2.0 数据集上比现有方法高效，并且在 MM-IMDb 数据集上也达到了比较好的效果。<details>
<summary>Abstract</summary>
Multimodal movie genre classification has always been regarded as a demanding multi-label classification task due to the diversity of multimodal data such as posters, plot summaries, trailers and metadata. Although existing works have made great progress in modeling and combining each modality, they still face three issues: 1) unutilized group relations in metadata, 2) unreliable attention allocation, and 3) indiscriminative fused features. Given that the knowledge graph has been proven to contain rich information, we present a novel framework that exploits the knowledge graph from various perspectives to address the above problems. As a preparation, the metadata is processed into a domain knowledge graph. A translate model for knowledge graph embedding is adopted to capture the relations between entities. Firstly we retrieve the relevant embedding from the knowledge graph by utilizing group relations in metadata and then integrate it with other modalities. Next, we introduce an Attention Teacher module for reliable attention allocation based on self-supervised learning. It learns the distribution of the knowledge graph and produces rational attention weights. Finally, a Genre-Centroid Anchored Contrastive Learning module is proposed to strengthen the discriminative ability of fused features. The embedding space of anchors is initialized from the genre entities in the knowledge graph. To verify the effectiveness of our framework, we collect a larger and more challenging dataset named MM-IMDb 2.0 compared with the MM-IMDb dataset. The experimental results on two datasets demonstrate that our model is superior to the state-of-the-art methods. We will release the code in the near future.
</details>
<details>
<summary>摘要</summary>
多Modal电影类别分类一直被视为一项具有多个标签的复杂分类任务，这是因为多modal数据，如海报、剧情简介、预告片和元数据的多样性。 existing works have made great progress in modeling and combining each modality, but they still face three issues: 1) unutilized group relations in metadata, 2) unreliable attention allocation, and 3) indiscriminative fused features. Given that the knowledge graph has been proven to contain rich information, we present a novel framework that exploits the knowledge graph from various perspectives to address the above problems. As a preparation, the metadata is processed into a domain knowledge graph. A translate model for knowledge graph embedding is adopted to capture the relations between entities. Firstly we retrieve the relevant embedding from the knowledge graph by utilizing group relations in metadata and then integrate it with other modalities. Next, we introduce an Attention Teacher module for reliable attention allocation based on self-supervised learning. It learns the distribution of the knowledge graph and produces rational attention weights. Finally, a Genre-Centroid Anchored Contrastive Learning module is proposed to strengthen the discriminative ability of fused features. The embedding space of anchors is initialized from the genre entities in the knowledge graph. To verify the effectiveness of our framework, we collect a larger and more challenging dataset named MM-IMDb 2.0 compared with the MM-IMDb dataset. The experimental results on two datasets demonstrate that our model is superior to the state-of-the-art methods. We will release the code in the near future.
</details></li>
</ul>
<hr>
<h2 id="Beyond-Sharing-Weights-in-Decoupling-Feature-Learning-Network-for-UAV-RGB-Infrared-Vehicle-Re-Identification"><a href="#Beyond-Sharing-Weights-in-Decoupling-Feature-Learning-Network-for-UAV-RGB-Infrared-Vehicle-Re-Identification" class="headerlink" title="Beyond Sharing Weights in Decoupling Feature Learning Network for UAV RGB-Infrared Vehicle Re-Identification"></a>Beyond Sharing Weights in Decoupling Feature Learning Network for UAV RGB-Infrared Vehicle Re-Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08026">http://arxiv.org/abs/2310.08026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyue Liu, Jiahao Qi, Chen Chen, Kangcheng Bin, Ping Zhong</li>
<li>for: 该论文旨在解决无人机视觉检索中的跨模态车辆识别问题，提高视觉监测和公共安全领域的应用。</li>
<li>methods: 该论文提出了一个跨模态车辆识别 benchmark 名为 UAV Cross-Modality Vehicle Re-ID (UCM-VeID)，包含 753 个标识性的车辆图像，以及一种 hybrid weights decoupling network (HWDNet) 来解决跨模态差异和方向差异挑战。</li>
<li>results: 实验结果表明，UCM-VeID 可以有效地解决跨模态车辆识别问题，并且 HWDNet 可以学习共享的 orientation-invariant 特征。<details>
<summary>Abstract</summary>
Owing to the capacity of performing full-time target search, cross-modality vehicle re-identification (Re-ID) based on unmanned aerial vehicle (UAV) is gaining more attention in both video surveillance and public security. However, this promising and innovative research has not been studied sufficiently due to the data inadequacy issue. Meanwhile, the cross-modality discrepancy and orientation discrepancy challenges further aggravate the difficulty of this task. To this end, we pioneer a cross-modality vehicle Re-ID benchmark named UAV Cross-Modality Vehicle Re-ID (UCM-VeID), containing 753 identities with 16015 RGB and 13913 infrared images. Moreover, to meet cross-modality discrepancy and orientation discrepancy challenges, we present a hybrid weights decoupling network (HWDNet) to learn the shared discriminative orientation-invariant features. For the first challenge, we proposed a hybrid weights siamese network with a well-designed weight restrainer and its corresponding objective function to learn both modality-specific and modality shared information. In terms of the second challenge, three effective decoupling structures with two pretext tasks are investigated to learn orientation-invariant feature. Comprehensive experiments are carried out to validate the effectiveness of the proposed method. The dataset and codes will be released at https://github.com/moonstarL/UAV-CM-VeID.
</details>
<details>
<summary>摘要</summary>
由于全时目标搜索的能力，基于无人机（UAV）的跨模态汽车重新认识（Re-ID）在视频监测和公共安全领域获得更多的注意力。然而，这项有前瞻性和创新的研究尚未得到充分的研究，主要是因为数据不足问题。此外，跨模态差异和Orientation差挑战更加增加了这个任务的难度。为此，我们开创了一个跨模态汽车Re-ID标准 bencmark named UAV Cross-Modality Vehicle Re-ID (UCM-VeID),包含753个标识性、16015个RGB和13913个 инфракра们图像。此外，为了解决跨模态差异和Orientation差挑战，我们提出了一种hybrid weights decoupling network (HWDNet)，以学习共享的Discriminative orientation-invariant特征。首先，我们提出了一种hybrid weights siamese network，其中包括一个Well-designed weight restrainer和其对应的目标函数，以学习两个模态Specific和共享信息。在第二个挑战中，我们investigated three effective decoupling structures with two pretext tasks，以学习Orientation-invariant特征。为了证明方法的有效性，我们进行了广泛的实验。UCM-VeID数据集和代码将在https://github.com/moonstarL/UAV-CM-VeID上发布。
</details></li>
</ul>
<hr>
<h2 id="Effects-of-Human-Adversarial-and-Affable-Samples-on-BERT-Generalizability"><a href="#Effects-of-Human-Adversarial-and-Affable-Samples-on-BERT-Generalizability" class="headerlink" title="Effects of Human Adversarial and Affable Samples on BERT Generalizability"></a>Effects of Human Adversarial and Affable Samples on BERT Generalizability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08008">http://arxiv.org/abs/2310.08008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aparna Elangovan, Jiayuan He, Yuan Li, Karin Verspoor</li>
<li>for: 这篇论文的目的是探讨训练数据质量对模型的泛化性的影响，而不是训练数据量。</li>
<li>methods: 这篇论文使用了BERT模型，并对训练数据进行分类和关系抽取任务。</li>
<li>results: 研究发现，固定训练样本数量下，有10-30%的人工挑战（h-adversarial）样本可以提高精度和F1值，但是超过这个范围可能会导致性能普遍下降。同时，h-affable样本可能没有提高模型的泛化性，甚至会导致模型的泛化性下降。<details>
<summary>Abstract</summary>
BERT-based models have had strong performance on leaderboards, yet have been demonstrably worse in real-world settings requiring generalization. Limited quantities of training data is considered a key impediment to achieving generalizability in machine learning. In this paper, we examine the impact of training data quality, not quantity, on a model's generalizability. We consider two characteristics of training data: the portion of human-adversarial (h-adversarial), i.e., sample pairs with seemingly minor differences but different ground-truth labels, and human-affable (h-affable) training samples, i.e., sample pairs with minor differences but the same ground-truth label. We find that for a fixed size of training samples, as a rule of thumb, having 10-30% h-adversarial instances improves the precision, and therefore F1, by up to 20 points in the tasks of text classification and relation extraction. Increasing h-adversarials beyond this range can result in performance plateaus or even degradation. In contrast, h-affables may not contribute to a model's generalizability and may even degrade generalization performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Novel-Statistical-Measure-for-Out-of-Distribution-Detection-in-Data-Quality-Assurance"><a href="#A-Novel-Statistical-Measure-for-Out-of-Distribution-Detection-in-Data-Quality-Assurance" class="headerlink" title="A Novel Statistical Measure for Out-of-Distribution Detection in Data Quality Assurance"></a>A Novel Statistical Measure for Out-of-Distribution Detection in Data Quality Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07998">http://arxiv.org/abs/2310.07998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tinghui Ouyang, Isao Echizen, Yoshiki Seo</li>
<li>for: 本研究旨在 investigate AIQualityManagement (AIQM) 领域中数据领域和out-of-distribution (OOD) 数据的问题。</li>
<li>methods: 本研究使用深度学习技术来实现特征表示，并开发了一种新的统计量来检测OOD数据。</li>
<li>results: 经过实验和评估于图像 benchmark  datasets 和工业 dataset，提出的方法被证明为可靠和有效的OOD检测方法。<details>
<summary>Abstract</summary>
Data outside the problem domain poses significant threats to the security of AI-based intelligent systems. Aiming to investigate the data domain and out-of-distribution (OOD) data in AI quality management (AIQM) study, this paper proposes to use deep learning techniques for feature representation and develop a novel statistical measure for OOD detection. First, to extract low-dimensional representative features distinguishing normal and OOD data, the proposed research combines the deep auto-encoder (AE) architecture and neuron activation status for feature engineering. Then, using local conditional probability (LCP) in data reconstruction, a novel and superior statistical measure is developed to calculate the score of OOD detection. Experiments and evaluations are conducted on image benchmark datasets and an industrial dataset. Through comparative analysis with other common statistical measures in OOD detection, the proposed research is validated as feasible and effective in OOD and AIQM studies.
</details>
<details>
<summary>摘要</summary>
□ Text ①人工智能系统的安全性受到数据外部威胁。本研究旨在通过深度学习技术实现特征表示和外部数据检测的AI质量管理（AIQM）研究。首先，通过结合自动encoder（AE）架构和神经元活动状态进行特征工程，提取出Normal和外部数据之间的低维度表示特征。然后，通过局部概率（LCP）进行数据重建，提出了一种新的和优秀的统计度量，用于评估外部数据检测得分。经对图像 bench mark 数据集和工业数据集进行实验和评估，与其他常见的统计度量进行比较分析，本研究被证明可行和有效。
</details></li>
</ul>
<hr>
<h2 id="Point-NeuS-Point-Guided-Neural-Implicit-Surface-Reconstruction-by-Volume-Rendering"><a href="#Point-NeuS-Point-Guided-Neural-Implicit-Surface-Reconstruction-by-Volume-Rendering" class="headerlink" title="Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by Volume Rendering"></a>Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by Volume Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07997">http://arxiv.org/abs/2310.07997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhang, Wanjuan Su, Wenbing Tao</li>
<li>for: 本研究旨在提高多视图重建的精度和效率，提出一种基于点导航机制的新方法Point-NeuS。</li>
<li>methods: 该方法利用点模型进行几何约束，将点云的 aleatoric 不确定性模型为捕捉噪声和估计点的可靠性。另外，引入图像 проек示模块，将点和图像连接到signed distance function中，以增强 geometric constraint。</li>
<li>results: 经过效果的点导航，使用轻量级网络实现了11倍的速度提升，并且在多个实验中表现出高质量表面，尤其是细腻的细节和平滑区域。此外，它还具有强大的鲁棒性，可以抗 resist 噪声和缺失数据。<details>
<summary>Abstract</summary>
Recently, learning neural implicit surface by volume rendering has been a promising way for multi-view reconstruction. However, limited accuracy and excessive time complexity remain bottlenecks that current methods urgently need to overcome. To address these challenges, we propose a new method called Point-NeuS, utilizing point-guided mechanisms to achieve accurate and efficient reconstruction. Point modeling is organically embedded into the volume rendering to enhance and regularize the representation of implicit surface. Specifically, to achieve precise point guidance and noise robustness, aleatoric uncertainty of the point cloud is modeled to capture the distribution of noise and estimate the reliability of points. Additionally, a Neural Projection module connecting points and images is introduced to add geometric constraints to the Signed Distance Function (SDF). To better compensate for geometric bias between volume rendering and point modeling, high-fidelity points are filtered into an Implicit Displacement Network to improve the representation of SDF. Benefiting from our effective point guidance, lightweight networks are employed to achieve an impressive 11x speedup compared to NeuS. Extensive experiments show that our method yields high-quality surfaces, especially for fine-grained details and smooth regions. Moreover, it exhibits strong robustness to both noisy and sparse data.
</details>
<details>
<summary>摘要</summary>
近期，通过量rendering学习神经隐 superficie的方法在多视图重建方面表现出了抢眼的承诺。然而，当前方法仍面临着准确性和时间复杂度的瓶颈。为了解决这些挑战，我们提出了一种新的方法 called Point-NeuS，该方法利用点导向机制来实现准确和高效的重建。在量rendering中，点模型被天然地嵌入到隐 superficie中，以增强和规范隐 superficie的表示。具体来说，为了实现精准的点导航和随机变量的鲁棒性，点云的 aleatoric 不确定性被模型来捕捉随机变量的分布和计算点的可靠性。此外，我们引入了一种神经投影模块，将点和图像连接起来，以加入 геометрические约束到signed Distance Function (SDF)。为了更好地补做几何偏见 между量rendering和点模型，高精度的点被筛选到一个Implicit Displacement Network中，以提高SDF的表示。由于我们的有效点导航，我们采用了轻量级的网络，实现了与NeuS的11倍的速度提升。我们的实验表明，我们的方法可以生成高质量的表面，特别是细节和平滑的区域。此外，它还具有强大的鲁棒性，能够抗抗噪和缺失数据。
</details></li>
</ul>
<hr>
<h2 id="HeightFormer-A-Multilevel-Interaction-and-Image-adaptive-Classification-regression-Network-for-Monocular-Height-Estimation-with-Aerial-Images"><a href="#HeightFormer-A-Multilevel-Interaction-and-Image-adaptive-Classification-regression-Network-for-Monocular-Height-Estimation-with-Aerial-Images" class="headerlink" title="HeightFormer: A Multilevel Interaction and Image-adaptive Classification-regression Network for Monocular Height Estimation with Aerial Images"></a>HeightFormer: A Multilevel Interaction and Image-adaptive Classification-regression Network for Monocular Height Estimation with Aerial Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07995">http://arxiv.org/abs/2310.07995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhan Chen, Yidan Zhang, Xiyu Qi, Yongqiang Mao, Xin Zhou, Lulu Niu, Hui Wu, Lei Wang, Yunping Ge</li>
<li>for: 这篇论文是针对单一图像高度估测在远程测量领域中提出了全面的解决方案，以提高现有方法的精度和效能。</li>
<li>methods: 这篇论文使用了一种叫做HeightFormer的全新方法，其结合了多个层次互动和适应性分类回归，以解决单一图像高度估测中的常见问题。</li>
<li>results: 这篇论文的结果显示，使用HeightFormer方法可以实现高度估测的精度和效能，并且可以提高实际应用中的对象边缘深度估测精度。<details>
<summary>Abstract</summary>
Height estimation has long been a pivotal topic within measurement and remote sensing disciplines, proving critical for endeavours such as 3D urban modelling, MR and autonomous driving. Traditional methods utilise stereo matching or multisensor fusion, both well-established techniques that typically necessitate multiple images from varying perspectives and adjunct sensors like SAR, leading to substantial deployment costs. Single image height estimation has emerged as an attractive alternative, boasting a larger data source variety and simpler deployment. However, current methods suffer from limitations such as fixed receptive fields, a lack of global information interaction, leading to noticeable instance-level height deviations. The inherent complexity of height prediction can result in a blurry estimation of object edge depth when using mainstream regression methods based on fixed height division. This paper presents a comprehensive solution for monocular height estimation in remote sensing, termed HeightFormer, combining multilevel interactions and image-adaptive classification-regression. It features the Multilevel Interaction Backbone (MIB) and Image-adaptive Classification-regression Height Generator (ICG). MIB supplements the fixed sample grid in CNN of the conventional backbone network with tokens of different interaction ranges. It is complemented by a pixel-, patch-, and feature map-level hierarchical interaction mechanism, designed to relay spatial geometry information across different scales and introducing a global receptive field to enhance the quality of instance-level height estimation. The ICG dynamically generates height partition for each image and reframes the traditional regression task, using a refinement from coarse to fine classification-regression that significantly mitigates the innate ill-posedness issue and drastically improves edge sharpness.
</details>
<details>
<summary>摘要</summary>
Height estimation 已经是测量和远程感知领域中长期焦点问题，对3D城市模型、MR和无人驾驶等项目具有重要意义。传统方法通常使用ステレオ匹配或多感器融合，这些技术需要多张视角不同的图像和附加感知器 like SAR，这导致了巨大的投入成本。单张图像高度估计已经成为一种有吸引力的alternative，具有更多的数据源和更简单的投入。然而，当前方法受到Fixed receptive fields和缺乏全球信息互动的限制，导致了明显的实例级高度偏差。图像高度预测的自然复杂性可能导致使用主流回归方法的对象边缘深度估计变得模糊。这篇论文提出了一种干扰HeightFormer，用于远程感知单张图像高度估计。该方法结合多层互动和图像适应分类回归，具有多层互动背bone和图像适应分类回归高度生成器（ICG）。多层互动背bone将传统的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的固定样本网络中CNN的
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-Scientific-Synthesis-Inference-and-Explanation"><a href="#Large-Language-Models-for-Scientific-Synthesis-Inference-and-Explanation" class="headerlink" title="Large Language Models for Scientific Synthesis, Inference and Explanation"></a>Large Language Models for Scientific Synthesis, Inference and Explanation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07984">http://arxiv.org/abs/2310.07984</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zyzisastudyreallyhardguy/llm4sd">https://github.com/zyzisastudyreallyhardguy/llm4sd</a></li>
<li>paper_authors: Yizhen Zheng, Huan Yee Koh, Jiaxin Ju, Anh T. N. Nguyen, Lauren T. May, Geoffrey I. Webb, Shirui Pan</li>
<li>for: 这个论文的目的是用大语言模型来执行科学合成、推理和解释。</li>
<li>methods: 论文使用了通用的大语言模型来从科学数据集中进行推理，并将这些推理结果与专门用于机器学习的数据集相结合，以提高预测分子性质的性能。</li>
<li>results: 研究表明，当将大语言模型的推理和合成结果与专门用于机器学习的数据集相结合时，可以超过当前的状态艺术水平。此外，大语言模型还可以解释机器学习系统的预测结果。<details>
<summary>Abstract</summary>
Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language1. Despite their limited forms of "knowledge", these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation. However, they have yet to demonstrate advanced applications in natural science. Here we show how large language models can perform scientific synthesis, inference, and explanation. We present a method for using general-purpose large language models to make inferences from scientific datasets of the form usually associated with special-purpose machine learning algorithms. We show that the large language model can augment this "knowledge" by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge it can outperform the current state of the art across a range of benchmark tasks for predicting molecular properties. This approach has the further advantage that the large language model can explain the machine learning system's predictions. We anticipate that our framework will open new avenues for AI to accelerate the pace of scientific discovery.
</details>
<details>
<summary>摘要</summary>
We present a method for using general-purpose large language models to make inferences from scientific datasets, which are usually used for special-purpose machine learning algorithms. We found that the large language model can augment this "knowledge" by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge, it can outperform the current state of the art in predicting molecular properties. This approach also has the advantage that the large language model can explain the machine learning system's predictions. We believe that our framework will open up new opportunities for AI to accelerate scientific discovery.
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-visual-learning-for-analyzing-firearms-trafficking-activities-on-the-Web"><a href="#Self-supervised-visual-learning-for-analyzing-firearms-trafficking-activities-on-the-Web" class="headerlink" title="Self-supervised visual learning for analyzing firearms trafficking activities on the Web"></a>Self-supervised visual learning for analyzing firearms trafficking activities on the Web</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07975">http://arxiv.org/abs/2310.07975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sotirios Konstantakos, Despina Ioanna Chalkiadaki, Ioannis Mademlis, Adamantia Anna Rebolledo Chrysochoou, Georgios Th. Papadopoulos</li>
<li>for: 这篇论文的目的是对RGB图像中的自动化火器分类进行研究，以应对公共空间安全、情报收集和刑事调查等实际应用。</li>
<li>methods: 这篇论文使用的方法是深度神经网络（DNN），特别是卷积神经网络（CNN），并且使用了转移学习和自我超vised learning（SSL）。</li>
<li>results: 这篇论文的结果显示，使用SSL和转移学习可以实现更好的火器分类效果，并且可以在较小的 annotated 数据集上进行实现。<details>
<summary>Abstract</summary>
Automated visual firearms classification from RGB images is an important real-world task with applications in public space security, intelligence gathering and law enforcement investigations. When applied to images massively crawled from the World Wide Web (including social media and dark Web sites), it can serve as an important component of systems that attempt to identify criminal firearms trafficking networks, by analyzing Big Data from open-source intelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology for achieving this, with Convolutional Neural Networks (CNN) being typically employed. The common transfer learning approach consists of pretraining on a large-scale, generic annotated dataset for whole-image classification, such as ImageNet-1k, and then finetuning the DNN on a smaller, annotated, task-specific, downstream dataset for visual firearms classification. Neither Visual Transformer (ViT) neural architectures nor Self-Supervised Learning (SSL) approaches have been so far evaluated on this critical task. SSL essentially consists of replacing the traditional supervised pretraining objective with an unsupervised pretext task that does not require ground-truth labels..
</details>
<details>
<summary>摘要</summary>
自动化视觉枪支分类从RGB图像是一项重要的现实世界任务，有应用于公共空间安全、情报收集和刑事调查调查。当应用于互联网上搜索大量图像时，它可以作为系统的一部分，用于识别刑事枪支贩卖网络，通过分析开源情报大数据。深度神经网络（DNN）是现状最佳方法，常用的是卷积神经网络（CNN）。常见的传输学习方法是先在大规模、通用注解 dataset 上预训练 DNN，然后在下游任务特定的注解 dataset 上精度调整 DNN。而Visual Transformer（ViT）神经网络 architectures 和Self-Supervised Learning（SSL）方法尚未在这一关键任务上进行评估。 SSL 基本上是将传统的超级vised预训练目标取代为无labels的自主预TEXT task。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Diffusion-via-Information-Decomposition"><a href="#Interpretable-Diffusion-via-Information-Decomposition" class="headerlink" title="Interpretable Diffusion via Information Decomposition"></a>Interpretable Diffusion via Information Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07972">http://arxiv.org/abs/2310.07972</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kxh001/info-decomp">https://github.com/kxh001/info-decomp</a></li>
<li>paper_authors: Xianghao Kong, Ollie Liu, Han Li, Dani Yogatama, Greg Ver Steeg</li>
<li>for: This paper is written for understanding the fine-grained relationships learned by diffusion models, and for developing methods to quantify and manipulate these relationships.</li>
<li>methods: The paper uses denoising diffusion models and exact expressions for mutual information and conditional mutual information to illuminate the relationships between words and parts of an image.</li>
<li>results: The paper shows that a natural non-negative decomposition of mutual information emerges, allowing for the quantification of informative relationships between words and pixels in an image, and enabling unsupervised localization of objects in images and measurement of effects through selective editing.<details>
<summary>Abstract</summary>
Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutual information emerges, allowing us to quantify informative relationships between words and pixels in an image. We exploit these new relations to measure the compositional understanding of diffusion models, to do unsupervised localization of objects in images, and to measure effects when selectively editing images through prompt interventions.
</details>
<details>
<summary>摘要</summary>
Diffusion models 可以实现Conditional generation和density模型复杂关系，如图像和文本之间的关系。然而，学习的关系性是不透明的，难以理解 diffusion 模型中 capture 的细腻关系是什么，或者预测干预的效果。我们通过发现 diffusion 和信息分解的精确关系来照明 diffusion 模型学习的细腻关系。我们可以通过 conditional 和 mutual information 的准确表达来理解这些关系，并且可以轻松地估算点wise 的关系。这些新关系allow us 可以问题关于特定图像和caption 之间的关系，并且可以进一步分解信息，以便理解 diffusion 模型中各变量的信息含量。我们展示了一种自然的非负分解方法，以便量化 diffusion 模型中各变量之间的信息关系。我们利用这些新关系来衡量 diffusion 模型的 compositional understanding，进行无监督的对象本地化，并且measure 干预后图像的效果。
</details></li>
</ul>
<hr>
<h2 id="A-New-Approach-Towards-Autoformalization"><a href="#A-New-Approach-Towards-Autoformalization" class="headerlink" title="A New Approach Towards Autoformalization"></a>A New Approach Towards Autoformalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07957">http://arxiv.org/abs/2310.07957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nilay Patel, Rahul Saha, Jeffrey Flanigan</li>
<li>for: 本文提出了一种方法来自动化推理证明的验证，即通过自动将自然语言数学转化为可验证的正式语言。</li>
<li>methods: 本文提出了一种分解任务的方法，即将自然语言数学分解成三个更容易实现的子任务：不连接化形式化（即使用不连接的定义和证明）、实体链接（将证明和定义链接到正确的位置）和类型调整（使类型检查器通过）。</li>
<li>results: 本文提出了一个名为 arXiv2Formal 的 benchmark 数据集，包含 50 个证明，来验证自然语言数学的自动化验证能力。<details>
<summary>Abstract</summary>
Verifying mathematical proofs is difficult, but can be automated with the assistance of a computer. Autoformalization is the task of automatically translating natural language mathematics into a formal language that can be verified by a program. This is a challenging task, and especially for higher-level mathematics found in research papers. Research paper mathematics requires large amounts of background and context. In this paper, we propose an avenue towards tackling autoformalization for research-level mathematics, by breaking the task into easier and more approachable subtasks: unlinked formalization (formalization with unlinked definitions and theorems), entity linking (linking to the proper theorems and definitions), and finally adjusting types so it passes the type checker. In addition, we present arXiv2Formal, a benchmark dataset for unlinked formalization consisting of 50 theorems formalized for the Lean theorem prover sampled from papers on arXiv.org. We welcome any contributions from the community to future versions of this dataset.
</details>
<details>
<summary>摘要</summary>
自动化验证数学证明是具有挑战性的任务，但可以通过计算机的协助进行自动化。自动化形式化是将自然语言数学转换为可以由计算机验证的形式语言的任务。这是一项复杂的任务，特别是在研究论文中出现的更高水平的数学。在这篇论文中，我们提出了一种方法来解决研究级数学自动化问题，即将任务分解为更容易实现的子任务：无关定义（定义和证明分开）、实体链接（将证明和定义链接到正确的位置）和最后调整类型，以便通过类型检查器进行验证。此外，我们还提供了arXiv2Formal数据集，这是一个由arXiv.org上的50个论文中所选择的50个证明，用于测试Lean证明引擎。我们欢迎社区的贡献，以便未来版本的数据集。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/12/cs.AI_2023_10_12/" data-id="clorjzl2q005lf188hfnu7cyq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/12/cs.CL_2023_10_12/" class="article-date">
  <time datetime="2023-10-12T11:00:00.000Z" itemprop="datePublished">2023-10-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/12/cs.CL_2023_10_12/">cs.CL - 2023-10-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Calibrating-Likelihoods-towards-Consistency-in-Summarization-Models"><a href="#Calibrating-Likelihoods-towards-Consistency-in-Summarization-Models" class="headerlink" title="Calibrating Likelihoods towards Consistency in Summarization Models"></a>Calibrating Likelihoods towards Consistency in Summarization Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08764">http://arxiv.org/abs/2310.08764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Polina Zablotskaia, Misha Khalman, Rishabh Joshi, Livio Baldini Soares, Shoshana Jakobovits, Joshua Maynez, Shashi Narayan</li>
<li>for: 提高抽象文本概要生成模型的可靠性，以便应用于实际场景。</li>
<li>methods: 使用自然语言判断（NLI）模型来衡量模型生成的文本的一致性，并对模型进行均衡化，使其更好地评估文本的一致性。</li>
<li>results: 通过人工评估和自动指标，显示了使用我们的方法生成的概要更加一致、质量更高，同时模型返回的概率也更加吻合NLI分数，提高了抽象文本概要生成模型的可靠性。<details>
<summary>Abstract</summary>
Despite the recent advances in abstractive text summarization, current summarization models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. We argue that the main reason for such behavior is that the summarization models trained with maximum likelihood objective assign high probability to plausible sequences given the context, but they often do not accurately rank sequences by their consistency. In this work, we solve this problem by calibrating the likelihood of model generated sequences to better align with a consistency metric measured by natural language inference (NLI) models. The human evaluation study and automatic metrics show that the calibrated models generate more consistent and higher-quality summaries. We also show that the models trained using our method return probabilities that are better aligned with the NLI scores, which significantly increase reliability of summarization models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Circuit-Component-Reuse-Across-Tasks-in-Transformer-Language-Models"><a href="#Circuit-Component-Reuse-Across-Tasks-in-Transformer-Language-Models" class="headerlink" title="Circuit Component Reuse Across Tasks in Transformer Language Models"></a>Circuit Component Reuse Across Tasks in Transformer Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08744">http://arxiv.org/abs/2310.08744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Merullo, Carsten Eickhoff, Ellie Pavlick</li>
<li>for: 这个研究的目的是为了解释大型语言模型的行为，以及它们是如何在不同任务上实现的。</li>
<li>methods: 这个研究使用了循环分析来reverse工程语言模型，并通过这种方法发现了一个名为IOI任务的电路。</li>
<li>results: 研究发现，这个IOI任务的电路可以在一个更大的GPT2模型上重现，并且可以用于解决一个看起来很不同的任务—颜色物品任务。此外，研究还发现了这两个任务之间的函数相似性，具体来说，它们的电路中的听力头数量相似，并且它们的处理逻辑也很相似。此外，研究还进行了一个观察性实验，通过调整中间层的四个听力头来使颜色物品任务的电路更像IOI任务的电路，从而提高了任务的准确率从49.6%提高到93.7%。这些结果表明，可能有一些可解释的任务通用算法构建块和计算组件，可以用于解释大型语言模型的行为。<details>
<summary>Abstract</summary>
Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to 'repair' the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models' behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Zero-Shot-Language-Agent-for-Computer-Control-with-Structured-Reflection"><a href="#A-Zero-Shot-Language-Agent-for-Computer-Control-with-Structured-Reflection" class="headerlink" title="A Zero-Shot Language Agent for Computer Control with Structured Reflection"></a>A Zero-Shot Language Agent for Computer Control with Structured Reflection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08740">http://arxiv.org/abs/2310.08740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, Yang Li</li>
<li>for: 本研究的目的是开发一个不需要专家示例的零批量机器人，能够自主学习并完成计算机上的任务。</li>
<li>methods: 本研究使用了规划和自我反思的技术，让机器人通过自己的错误分析和结构化思维管理来学习和改进控制。</li>
<li>results: 研究发现，在MiniWoB++中的易于完成任务上，我们的零批量机器人可以超越现有的最佳实践，并且更加高效地进行了分析。而在更复杂的任务上，我们的反思机器人与之前有特权的模型相当，即使这些模型有访问专家示例或额外的屏幕信息的优势。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot prompting. Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task. We approach this problem with a zero-shot agent that requires no given expert traces. Our agent plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management. On the easy tasks of MiniWoB++, we show that our zero-shot agent often outperforms recent SoTAs, with more efficient reasoning. For tasks with more complexity, our reflective agent performs on par with prior best models, even though previous works had the advantages of accessing expert traces or additional screen information.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Visual-Data-Type-Understanding-does-not-emerge-from-Scaling-Vision-Language-Models"><a href="#Visual-Data-Type-Understanding-does-not-emerge-from-Scaling-Vision-Language-Models" class="headerlink" title="Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models"></a>Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08577">http://arxiv.org/abs/2310.08577</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bethgelab/DataTypeIdentification">https://github.com/bethgelab/DataTypeIdentification</a></li>
<li>paper_authors: Vishaal Udandarao, Max F. Burg, Samuel Albanie, Matthias Bethge</li>
<li>for: 本研究旨在提出一个新的任务——视觉数据类型识别，以探索现代视觉语言模型（VLM）在识别视觉内容的能力。</li>
<li>methods: 本研究开发了两个类型数据集，包括动物图像被修改成27种不同的视觉数据类型，分成四个主要类别。对于39个VLM，包括从100M到80B个参数的模型，进行了广泛的零基础评估。</li>
<li>results: 研究结果显示，VLMs在某些类型的视觉数据类型识别方面表现不俗，如动画和素描，但对于较简单的视觉数据类型，如图像旋转或加法噪声，表现不佳。研究显示，视觉数据类型识别需要更进一步的训练和模型设计。<details>
<summary>Abstract</summary>
Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \textit{data-types}, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise. Our findings reveal that (i) model scaling alone yields marginal gains for contrastively-trained models like CLIP, and (ii) there is a pronounced drop in performance for the largest auto-regressively trained VLMs like OpenFlamingo. This finding points to a blind spot in current frontier VLMs: they excel in recognizing semantic content but fail to acquire an understanding of visual data-types through scaling. By analyzing the pre-training distributions of these models and incorporating data-type information into the captions during fine-tuning, we achieve a significant enhancement in performance. By exploring this previously uncharted task, we aim to set the stage for further advancing VLMs to equip them with visual data-type understanding. Code and datasets are released at https://github.com/bethgelab/DataTypeIdentification.
</details>
<details>
<summary>摘要</summary>
最近的视力语言模型（VLM）的发展进展得到了非常出色的成果，包括惊人的图像Semantic content认知。在这里，我们介绍了一个新的任务：视图数据类型标识，这是一种基本的感知技能，它在数据筛选（例如，去除大量数据中的噪音）和自主视觉方面具有重要的意义。我们制作了两个包含动物图像被修改的数据集，其中包括27种不同的视图数据类型，分为四个大类。我们进行了39种VLM的无参数测试，其中参数量从100M到80B不等。我们发现，虽然VLM在某些风格的数据类型方面表现reasonably well，但对于基本的修改，如图像旋转或添加噪音，它们表现不佳。我们的发现表明，（i）升级模型 alone 并不能提供明显的提升，而（ii）最大化自动回归式VLMs like OpenFlamingo 在大型数据集上表现下降。这种发现表明当前前沿VLMs 在扩大scale 时，它们并不能通过升级来学习视图数据类型。通过分析这些模型的预训练分布 ribbon 和在 fine-tuning 过程中包含数据类型信息，我们实现了显著的性能提升。通过这个以前未探索的任务，我们希望能够为 VLM 带来更好的视图数据类型理解。代码和数据集可以在 <https://github.com/bethgelab/DataTypeIdentification> 上下载。
</details></li>
</ul>
<hr>
<h2 id="LLM-augmented-Preference-Learning-from-Natural-Language"><a href="#LLM-augmented-Preference-Learning-from-Natural-Language" class="headerlink" title="LLM-augmented Preference Learning from Natural Language"></a>LLM-augmented Preference Learning from Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08523">http://arxiv.org/abs/2310.08523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inwon Kang, Sikai Ruan, Tyler Ho, Jui-Chien Lin, Farhad Mohsin, Oshani Seneviratne, Lirong Xia</li>
<li>for: 本研究旨在使用大型自然语言模型（LLM）进行比较文本分类任务。</li>
<li>methods: 本研究采用了 transformer-based 模型和 graph neural architecture，并对 LLM 进行了直接 Classification 任务的设计和实验。</li>
<li>results: 研究发现，预训练的 LLM 能够在不需要精度调整的情况下，超越现有的 State-of-the-art 模型，特别是在多句text中的情况下。此外，一些几拟学习也能够提高表现。<details>
<summary>Abstract</summary>
Finding preferences expressed in natural language is an important but challenging task. State-of-the-art(SotA) methods leverage transformer-based models such as BERT, RoBERTa, etc. and graph neural architectures such as graph attention networks. Since Large Language Models (LLMs) are equipped to deal with larger context lengths and have much larger model sizes than the transformer-based model, we investigate their ability to classify comparative text directly. This work aims to serve as a first step towards using LLMs for the CPC task. We design and conduct a set of experiments that format the classification task into an input prompt for the LLM and a methodology to get a fixed-format response that can be automatically evaluated. Comparing performances with existing methods, we see that pre-trained LLMs are able to outperform the previous SotA models with no fine-tuning involved. Our results show that the LLMs can consistently outperform the SotA when the target text is large -- i.e. composed of multiple sentences --, and are still comparable to the SotA performance in shorter text. We also find that few-shot learning yields better performance than zero-shot learning.
</details>
<details>
<summary>摘要</summary>
找到用户喜好表达在自然语言中是一项重要 yet challenging task. 现有的State-of-the-art (SotA) 方法利用 transformer-based 模型如 BERT、RoBERTa 等，以及图 neural 架构如图注意力网络。由于 Large Language Models (LLMs) 可以处理更长的上下文长度和有更大的模型大小于 transformer-based 模型，我们调查其能否直接类型比较文本。这项工作的目的是使用 LLMs 进行 CPC 任务。我们设计并进行了一系列实验，将类型任务转换为 LLM 的输入提示和一种自动评估的方法。与现有方法进行比较，我们发现预训练的 LLMs 能够在无需 fine-tuning 的情况下超越前一个 SotA 模型。我们的结果表明，LLMs 在 longer 的目标文本（即多个句子）上能够顺利地超越 SotA，并且在 shorter 的文本上仍然与 SotA 性能相当。我们还发现，几个 shot 学习比零 shot 学习更好。
</details></li>
</ul>
<hr>
<h2 id="The-Uncertainty-based-Retrieval-Framework-for-Ancient-Chinese-CWS-and-POS"><a href="#The-Uncertainty-based-Retrieval-Framework-for-Ancient-Chinese-CWS-and-POS" class="headerlink" title="The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and POS"></a>The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and POS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08496">http://arxiv.org/abs/2310.08496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jihuai-wpy/bert-ancient-chinese">https://github.com/Jihuai-wpy/bert-ancient-chinese</a></li>
<li>paper_authors: Pengyu Wang, Zhichen Ren</li>
<li>for: 这 paper 是为了提高古代中文文本分 segmentation 和 parts-of-speech 标注的框架。</li>
<li>methods: 该 framework 使用了两种方法：一方面是capture 词 semantics; 另一方面是通过引入外部知识来重新预测基线模型的不确定样本。</li>
<li>results: 该框架的性能超过了预先训练的 BERT 与 CRF 以及现有的工具 such as Jiayan。<details>
<summary>Abstract</summary>
Automatic analysis for modern Chinese has greatly improved the accuracy of text mining in related fields, but the study of ancient Chinese is still relatively rare. Ancient text division and lexical annotation are important parts of classical literature comprehension, and previous studies have tried to construct auxiliary dictionary and other fused knowledge to improve the performance. In this paper, we propose a framework for ancient Chinese Word Segmentation and Part-of-Speech Tagging that makes a twofold effort: on the one hand, we try to capture the wordhood semantics; on the other hand, we re-predict the uncertain samples of baseline model by introducing external knowledge. The performance of our architecture outperforms pre-trained BERT with CRF and existing tools such as Jiayan.
</details>
<details>
<summary>摘要</summary>
自动分析现代中文已经大幅提高了相关领域的文本挖掘精度，但古代中文的研究仍然相对罕见。古代文本分区和词性标注是古典文学理解的重要组成部分，先前的研究已经尝试了构建辅助词典和其他融合知识以提高性能。在这篇论文中，我们提出了古代中文单词分 segmentation和部分标注框架，该框架做出了两重努力：一方面，我们尝试捕捉词 semantics；另一方面，我们重新预测基eline模型的不确定样本，通过引入外部知识。我们的架构的性能超过了预训练BERT与CRF以及现有工具such as Jiayan。
</details></li>
</ul>
<hr>
<h2 id="Prometheus-Inducing-Fine-grained-Evaluation-Capability-in-Language-Models"><a href="#Prometheus-Inducing-Fine-grained-Evaluation-Capability-in-Language-Models" class="headerlink" title="Prometheus: Inducing Fine-grained Evaluation Capability in Language Models"></a>Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08491">http://arxiv.org/abs/2310.08491</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaistAI/Prometheus">https://github.com/kaistAI/Prometheus</a></li>
<li>paper_authors: Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo</li>
<li>for: 这篇论文的目的是提出一个可以用于长篇回答评估的完全开源语言模型（Prometheus），以取代 propriety LLM（如GPT-4），并且可以根据用户提供的自定义评估标准（customized score rubric）进行评估。</li>
<li>methods: 这篇论文使用了一个新的数据集——Feedback Collection，包含1000个细化的评估标准、20000个指令和100000个由GPT-4生成的语言反馈。然后，通过使用Feedback Collection，提出了一个13亿参数的评估语言模型（Prometheus），可以根据用户提供的自定义评估标准进行评估。</li>
<li>results: 实验结果显示，Prometheus与人工评估人员的相关性为0.897，与GPT-4相关性为0.882，并且大大超过ChatGPT的相关性（0.392）。此外，通过对1222个自定义评估标准进行测试，Prometheus在四个 benchmark 上的相关性都是GPT-4的。最后，Prometheus在两个人类偏好benchmark上的准确率最高，比开源奖励模型在人类偏好数据集上的训练结果更好。<details>
<summary>Abstract</summary>
Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model at https://github.com/kaistAI/Prometheus.
</details>
<details>
<summary>摘要</summary>
近些时候，使用强大的专有大语言模型（LLM）（例如GPT-4）作为评价长篇回答的标准已成为了现实。然而，对于具有大规模评估任务和自定义评价标准的实践者来说，使用专有LLM作为评价器是不可靠的，因为它们的关闭源代码、不可控的版本和昂贵的成本。在这种情况下，我们提出了Prometheus，一个完全开源的LLM，它与GPT-4的评价能力相当，只要附带合适的参考答案和评价标准。我们首先构建了Feedback Collection，一个新的数据集，包括1000个细化的评价标准、20000个说明和100000个由GPT-4生成的语言反馈。使用Feedback Collection，我们训练了Prometheus，一个13亿 evaluator LLM，可以根据用户提供的自定义评价标准评估任何长篇文本。实验结果显示，Prometheus与人类评估器相关性为0.897，与GPT-4相关性为0.882，并大幅超过ChatGPT（0.392）。此外，使用1222个自定义评价标准在四个 bench 上测试Prometheus和GPT-4的相关性也显示了类似的趋势，这ebolsters Prometheus的评价器LLM能力。最后，Prometheus在两个人类偏好bench（HHH Alignment & MT Bench Human Judgment）上达到了其他开源奖励模型explicitly trained on human preference datasets的最高准确率， highlighting its potential as an universal reward model。我们将我们的代码、数据集和模型公开于https://github.com/kaistAI/Prometheus。
</details></li>
</ul>
<hr>
<h2 id="GraphextQA-A-Benchmark-for-Evaluating-Graph-Enhanced-Large-Language-Models"><a href="#GraphextQA-A-Benchmark-for-Evaluating-Graph-Enhanced-Large-Language-Models" class="headerlink" title="GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language Models"></a>GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08487">http://arxiv.org/abs/2310.08487</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/happen2me/cross-gnn">https://github.com/happen2me/cross-gnn</a></li>
<li>paper_authors: Yuanchun Shen, Ruotong Liao, Zhen Han, Yunpu Ma, Volker Tresp</li>
<li>for:  This paper aims to evaluate and develop graph-language models that can integrate graph knowledge into language generation.</li>
<li>methods:  The proposed method uses a question answering dataset called GraphextQA, which includes paired subgraphs retrieved from Wikidata, to condition answer generation on the paired graphs through cross-attention.</li>
<li>results:  The proposed method demonstrates the usefulness of paired graphs for answer generation and shows the difficulty of the task by comparing language-only models and the proposed graph-language model.Here’s the Chinese version:</li>
<li>for: 这篇论文目的是评估和发展基于图像知识的语言模型。</li>
<li>methods: 提议的方法使用了一个名为GraphextQA的问答集，其包含从Wikidata中检索的匹配子图，以便在解码时通过跨模型的注意力来使用问题相关的图像特征。</li>
<li>results: 提议的方法证明了匹配图像的用处性，并表明了这个任务的困难性，通过比较语言只模型和提议的图像语言模型。<details>
<summary>Abstract</summary>
While multi-modal models have successfully integrated information from image, video, and audio modalities, integrating graph modality into large language models (LLMs) remains unexplored. This discrepancy largely stems from the inherent divergence between structured graph data and unstructured text data. Incorporating graph knowledge provides a reliable source of information, enabling potential solutions to address issues in text generation, e.g., hallucination, and lack of domain knowledge. To evaluate the integration of graph knowledge into language models, a dedicated dataset is needed. However, there is currently no benchmark dataset specifically designed for multimodal graph-language models. To address this gap, we propose GraphextQA, a question answering dataset with paired subgraphs, retrieved from Wikidata, to facilitate the evaluation and future development of graph-language models. Additionally, we introduce a baseline model called CrossGNN, which conditions answer generation on the paired graphs by cross-attending question-aware graph features at decoding. The proposed dataset is designed to evaluate graph-language models' ability to understand graphs and make use of it for answer generation. We perform experiments with language-only models and the proposed graph-language model to validate the usefulness of the paired graphs and to demonstrate the difficulty of the task.
</details>
<details>
<summary>摘要</summary>
While multi-modal models have successfully integrated information from image, video, and audio modalities, integrating graph modality into large language models (LLMs) remains unexplored. This discrepancy largely stems from the inherent divergence between structured graph data and unstructured text data. Incorporating graph knowledge provides a reliable source of information, enabling potential solutions to address issues in text generation, e.g., hallucination, and lack of domain knowledge. To evaluate the integration of graph knowledge into language models, a dedicated dataset is needed. However, there is currently no benchmark dataset specifically designed for multimodal graph-language models. To address this gap, we propose GraphextQA, a question answering dataset with paired subgraphs, retrieved from Wikidata, to facilitate the evaluation and future development of graph-language models. Additionally, we introduce a baseline model called CrossGNN, which conditions answer generation on the paired graphs by cross-attending question-aware graph features at decoding. The proposed dataset is designed to evaluate graph-language models' ability to understand graphs and make use of it for answer generation. We perform experiments with language-only models and the proposed graph-language model to validate the usefulness of the paired graphs and to demonstrate the difficulty of the task.
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Humans-Behind-Online-Misinformation-An-Observational-Study-Through-the-Lens-of-the-COVID-19-Pandemic"><a href="#Understanding-the-Humans-Behind-Online-Misinformation-An-Observational-Study-Through-the-Lens-of-the-COVID-19-Pandemic" class="headerlink" title="Understanding the Humans Behind Online Misinformation: An Observational Study Through the Lens of the COVID-19 Pandemic"></a>Understanding the Humans Behind Online Misinformation: An Observational Study Through the Lens of the COVID-19 Pandemic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08483">http://arxiv.org/abs/2310.08483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohit Chandra, Anush Mattapalli, Munmun De Choudhury</li>
<li>for: 本研究旨在理解在COVID-19疫情期间用户如何传播谣言，以及这种行为与过去在非COVID话题上的谣言传播倾向之间的关系。</li>
<li>methods: 该研究采用时序分析技术和robust causal inference-based设计，分析了超过32万个COVID-19推文和16万个历史时间推文。</li>
<li>results: 分析发现，用户在COVID-19疫情期间的谣言传播行为和过去在非COVID话题上的谣言传播倾向之间存在正相关关系，表明用户的历史倾向对当前谣言传播行为产生了影响。这些结果可能为设计用户中心的免疫策略和生态系统基于的迅速干预策略提供了价值的基础。<details>
<summary>Abstract</summary>
The proliferation of online misinformation has emerged as one of the biggest threats to society. Considerable efforts have focused on building misinformation detection models, still the perils of misinformation remain abound. Mitigating online misinformation and its ramifications requires a holistic approach that encompasses not only an understanding of its intricate landscape in relation to the complex issue and topic-rich information ecosystem online, but also the psychological drivers of individuals behind it. Adopting a time series analytic technique and robust causal inference-based design, we conduct a large-scale observational study analyzing over 32 million COVID-19 tweets and 16 million historical timeline tweets. We focus on understanding the behavior and psychology of users disseminating misinformation during COVID-19 and its relationship with the historical inclinations towards sharing misinformation on Non-COVID topics before the pandemic. Our analysis underscores the intricacies inherent to cross-topic misinformation, and highlights that users' historical inclination toward sharing misinformation is positively associated with their present behavior pertaining to misinformation sharing on emergent topics and beyond. This work may serve as a valuable foundation for designing user-centric inoculation strategies and ecologically-grounded agile interventions for effectively tackling online misinformation.
</details>
<details>
<summary>摘要</summary>
“在线资讯的滥读问题已经成为现代社会面临的一大挑战。各方努力建立误信探测模型，但误信的危害仍然存在。为了对抗网络误信和其后果，我们需要一个整体的方法，不仅要理解网络资讯的复杂领域，也要理解个人在网络上传播误信的心理驱动。我们运用时间序列分析技术和强健的 causal inference-based 设计，对 COVID-19  tweets 和历史时间轴 tweets 进行大规模观察分析，焦点在探索传播误信的用户行为和心理。我们发现跨主题误信的复杂性，并发现用户在过去传播误信的倾向与今天传播误信的行为之间存在正相关。这项研究可能成为设计用户中心的传染策略和生态系考虑的基础。”
</details></li>
</ul>
<hr>
<h2 id="A-Confederacy-of-Models-a-Comprehensive-Evaluation-of-LLMs-on-Creative-Writing"><a href="#A-Confederacy-of-Models-a-Comprehensive-Evaluation-of-LLMs-on-Creative-Writing" class="headerlink" title="A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing"></a>A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08433">http://arxiv.org/abs/2310.08433</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/komoku/confederacy-of-models">https://github.com/komoku/confederacy-of-models</a></li>
<li>paper_authors: Carlos Gómez-Rodríguez, Paul Williams</li>
<li>for: 我们用这篇论文来评估一些最新的自然语言处理技术（LLMs）在英语创作写作中的表现。</li>
<li>methods: 我们使用一个具有挑战性和复杂性的enario， chosen to avoid training data reuse，要求 Ignatius J. Reilly，一位著名小说《奴隶制度》（1980）中的主人公和一个恐龙进行一场决斗。我们向几个LLMs和人类写作者请求作品，并进行了人类评价，包括流畅性、一致性、创新力、幽默和风格等方面的评价。</li>
<li>results: 我们的结果表明，一些现代商业LLMs在大多数维度上与我们的写作者匹配或甚至超越了。然而，开源LLMs落后于其他LLMs。人类在创作方面仍保留了一定的优势，而幽默方面则存在 binary 的分化，一些LLMs可以与人类相匹配，而其他LLMs则完全失败。我们讨论了这些研究的限制和意义，并提出了未来研究的方向。<details>
<summary>Abstract</summary>
We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile. We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style. Our results show that some state-of-the-art commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it. We discuss the implications and limitations of our study and suggest directions for future research.
</details>
<details>
<summary>摘要</summary>
我们评估了一些最新的自然语言处理模型（LLM）在英语创作写作方面的表现，这是一项复杂和挑战性的任务，需要想象力、一致性和风格。我们使用了一个具有挑战性和开放性的enario，避免了训练数据的重复使用：一场 Ignatius J. Reilly，《一个奴隶共和国》（1980）中的主角，与恐龙相打的漫长战役。我们征求了一些LLM和人类作者写出这种故事，并进行了人类评估，包括流畅性、一致性、创新性、幽默和风格等多个指标。我们的结果显示，一些当前的商业LLM几乎与人类作者相当或略高于其他维度中的大多数维度，而开源LLM则落后于人类。人类在创意方面仍保持优势，而幽默方面则存在binary分化，一些LLM可以与人类相比肯定地处理，而另一些则完全失败。我们讨论了我们的研究的限制和意义，并建议未来研究的方向。
</details></li>
</ul>
<hr>
<h2 id="Reconstructing-Materials-Tetrahedron-Challenges-in-Materials-Information-Extraction"><a href="#Reconstructing-Materials-Tetrahedron-Challenges-in-Materials-Information-Extraction" class="headerlink" title="Reconstructing Materials Tetrahedron: Challenges in Materials Information Extraction"></a>Reconstructing Materials Tetrahedron: Challenges in Materials Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08383">http://arxiv.org/abs/2310.08383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kausik Hira, Mohd Zaki, Dhruvil Sheth, Mausam, N M Anoop Krishnan</li>
<li>for: 本研究旨在探讨自然语言处理和深度学习技术在材料科学文献中自动信息提取中存在的挑战，以创建大规模的材料科学知识库。</li>
<li>methods: 本研究使用了深度学习和自然语言处理技术来检测和提取材料科学文献中的信息，特别是从文本和表格中提取信息。</li>
<li>results: 本研究发现了许多自动信息提取中的挑战，包括表格和文本中的信息提取、不同报告风格和不具有一致性的报告方式等。<details>
<summary>Abstract</summary>
Discovery of new materials has a documented history of propelling human progress for centuries and more. The behaviour of a material is a function of its composition, structure, and properties, which further depend on its processing and testing conditions. Recent developments in deep learning and natural language processing have enabled information extraction at scale from published literature such as peer-reviewed publications, books, and patents. However, this information is spread in multiple formats, such as tables, text, and images, and with little or no uniformity in reporting style giving rise to several machine learning challenges. Here, we discuss, quantify, and document these outstanding challenges in automated information extraction (IE) from materials science literature towards the creation of a large materials science knowledge base. Specifically, we focus on IE from text and tables and outline several challenges with examples. We hope the present work inspires researchers to address the challenges in a coherent fashion, providing to fillip to IE for the materials knowledge base.
</details>
<details>
<summary>摘要</summary>
人类进步史上发现新材料有记录，对人类进步产生了深远的影响。材料的行为取决于其组成、结构和性能，这些因素又取决于材料的处理和测试条件。现代深度学习和自然语言处理技术已经允许大规模提取出版文献中的信息，如同行 peer-reviewed 论文、书籍和专利。然而，这些信息分散在多种格式中，如表格、文本和图片，并且无一统一的报告风格，从而带来了许多机器学习挑战。我们在这里讨论、量化和记录了自动信息提取（IE）在材料科学文献中的一些挑战，以创建大规模的材料科学知识库。我们专注于IE文本和表格中的挑战，并提供了一些例子。我们希望现在的工作能够激励研究人员解决这些挑战，以提供填充材料知识库的动力。
</details></li>
</ul>
<hr>
<h2 id="Improving-Factual-Consistency-for-Knowledge-Grounded-Dialogue-Systems-via-Knowledge-Enhancement-and-Alignment"><a href="#Improving-Factual-Consistency-for-Knowledge-Grounded-Dialogue-Systems-via-Knowledge-Enhancement-and-Alignment" class="headerlink" title="Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment"></a>Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08372">http://arxiv.org/abs/2310.08372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amourwaltz/factdial">https://github.com/amourwaltz/factdial</a></li>
<li>paper_authors: Boyang Xue, Weichao Wang, Hongru Wang, Fei Mi, Rui Wang, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong</li>
<li>for: 提高知识grounded对话系统中FFN模块的准确表达能力</li>
<li>methods:  investigate two methods to improve the factual expression capability of FFNs, including explicit knowledge enhancement and implicit alignment through reinforcement learning</li>
<li>results:  experimental results on WoW and CMU_DoG datasets show that our methods efficiently enhance the ability of the FFN module to convey factual knowledge, validating the effectiveness of improving factual consistency for knowledge-grounded dialogue systems.<details>
<summary>Abstract</summary>
Pretrained language models (PLMs) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source. In such inconsistent responses, the dialogue models fail to accurately express the external knowledge they rely upon. Inspired by previous work which identified that feed-forward networks (FFNs) within Transformers are responsible for factual knowledge expressions, we investigate two methods to efficiently improve the factual expression capability {of FFNs} by knowledge enhancement and alignment respectively. We first propose \textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers to enhance factual knowledge expressions} given the specific patterns of knowledge-grounded dialogue inputs. Additionally, we apply the reinforcement learning for factual consistency (RLFC) method to implicitly adjust FFNs' expressions in responses by aligning with gold knowledge for the factual consistency preference. To comprehensively assess the factual consistency and dialogue quality of responses, we employ extensive automatic measures and human evaluations including sophisticated fine-grained NLI-based metrics. Experimental results on WoW and CMU\_DoG datasets demonstrate that our methods efficiently enhance the ability of the FFN module to convey factual knowledge, validating the efficacy of improving factual consistency for knowledge-grounded dialogue systems.
</details>
<details>
<summary>摘要</summary>
预训言语模型（PLM）基于的知识围绕对话系统有可能生成不符合知识源的回答。在这些不符合回答中，对话模型失去了正确表达外部知识的能力。根据前期研究，发现Feed-Forward Networks（FFNs）在Transformers中负责表达事实知识。我们调查了两种方法可以有效提高FFNs的事实表达能力，即知识增强和对齐方法。我们首先提出了\textsc{K-Dial}，该方法在Transformers中引入扩展的FFNs以提高基于知识围绕对话输入的事实表达。此外，我们应用了对齐抽象金 знание的方法来隐式地调整FFNs的表达，以确保回答的事实一致性。为了全面评估回答的事实一致性和对话质量，我们采用了广泛的自动度量和人工评估，包括复杂的细致的NLI基metric。实验结果表明，我们的方法可以有效提高FFN module的事实表达能力，证明了提高知识围绕对话系统的事实一致性的效果。
</details></li>
</ul>
<hr>
<h2 id="From-Large-Language-Models-to-Knowledge-Graphs-for-Biomarker-Discovery-in-Cancer"><a href="#From-Large-Language-Models-to-Knowledge-Graphs-for-Biomarker-Discovery-in-Cancer" class="headerlink" title="From Large Language Models to Knowledge Graphs for Biomarker Discovery in Cancer"></a>From Large Language Models to Knowledge Graphs for Biomarker Discovery in Cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08365">http://arxiv.org/abs/2310.08365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Rezaul Karim, Lina Molinas Comet, Md Shajalal, Oya Beyan, Dietrich Rebholz-Schuhmann, Stefan Decker</li>
<li>for: 这个论文是为了提供一个封装了生物医学知识的域 Specific Knowledge Graph（KG），以便用于抑肿癌病的诊断和治疗建议。</li>
<li>methods: 这个论文使用了生物医学文献中的数据（例如文章、图像、ómics数据和临床数据），通过构建一个大规模的知识图（KG），提取了与生物医学知识相关的实体和关系。然后，使用生物语义技术（例如 BioBERT 和 SciBERT）进行信息提取（IE），以提高 KG 的质量。</li>
<li>results: 这个论文通过构建域 Specific KG，使得域专家可以通过查询和探索 KG 来获得更多的生物医学知识，并且可以通过Semantic reasoning来验证基因与疾病关系。此外，通过使用大语言模型（LLMs）进行 KG 的迭代更新，使得 AI 系统可以更好地适应生物医学领域的演变。<details>
<summary>Abstract</summary>
Domain experts often rely on up-to-date knowledge for apprehending and disseminating specific biological processes that help them design strategies to develop prevention and therapeutic decision-making. A challenging scenario for artificial intelligence (AI) is using biomedical data (e.g., texts, imaging, omics, and clinical) to provide diagnosis and treatment recommendations for cancerous conditions. Data and knowledge about cancer, drugs, genes, proteins, and their mechanism is spread across structured (knowledge bases (KBs)) and unstructured (e.g., scientific articles) sources. A large-scale knowledge graph (KG) can be constructed by integrating these data, followed by extracting facts about semantically interrelated entities and relations. Such KGs not only allow exploration and question answering (QA) but also allow domain experts to deduce new knowledge. However, exploring and querying large-scale KGs is tedious for non-domain users due to a lack of understanding of the underlying data assets and semantic technologies. In this paper, we develop a domain KG to leverage cancer-specific biomarker discovery and interactive QA. For this, a domain ontology called OncoNet Ontology (ONO) is developed to enable semantic reasoning for validating gene-disease relations. The KG is then enriched by harmonizing the ONO, controlled vocabularies, and additional biomedical concepts from scientific articles by employing BioBERT- and SciBERT-based information extraction (IE) methods. Further, since the biomedical domain is evolving, where new findings often replace old ones, without employing up-to-date findings, there is a high chance an AI system exhibits concept drift while providing diagnosis and treatment. Therefore, we finetuned the KG using large language models (LLMs) based on more recent articles and KBs that might not have been seen by the named entity recognition models.
</details>
<details>
<summary>摘要</summary>
域内专家常靠最新的知识来理解和传达特定生物过程，以设计预防和治疗决策的策略。人工智能（AI）面临着使用生物医学数据（如文本、成像、ómics和临床）提供诊断和治疗建议的挑战。生物医学数据和知识分布在结构化（知识库）和不结构化（如科学文章）来源中。我们可以将这些数据集成成大规模知识图（KG），然后提取关键的生物医学实体和关系信息。这些KG不仅允许探索和问答（QA），还允许域专家推理出新的知识。然而，探索和查询大规模KG可以是非域用户的繁琐和困难，因为他们缺乏对下面数据资产和semantic技术的理解。在这篇论文中，我们开发了域知识图（KG），以便利用抗癌特异性生物标志物发现和互动问答。为此，我们开发了一个域 ontology（ONO），以启用semantic推理，验证蛋白质与疾病关系。然后，我们将KG丰富化，通过融合ONO、控制词汇和生物医学概念，使用BioBERT和SciBERT基于文本提取技术。此外，由于医学领域在不断发展，新的发现经常取代老的发现，如果不使用最新的发现，AI系统可能会出现概念漂移，从而影响诊断和治疗的准确性。因此，我们在LLMs（大语言模型）基于更新的文章和知识库进行训练和finetuning。
</details></li>
</ul>
<hr>
<h2 id="Defending-Our-Privacy-With-Backdoors"><a href="#Defending-Our-Privacy-With-Backdoors" class="headerlink" title="Defending Our Privacy With Backdoors"></a>Defending Our Privacy With Backdoors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08320">http://arxiv.org/abs/2310.08320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/D0miH/Defending-Our-Privacy-With-Backdoors">https://github.com/D0miH/Defending-Our-Privacy-With-Backdoors</a></li>
<li>paper_authors: Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting</li>
<li>for: 保护个人隐私，防止敌对者通过隐私攻击提取模型中的敏感信息。</li>
<li>methods: 利用后门攻击方法，将敏感词的嵌入与中性词的嵌入进行对应，使得模型不会受到隐私攻击的影响。</li>
<li>results: 通过对CLIP模型进行特殊隐私攻击评估，证明了我们的后门防御策略的有效性。<details>
<summary>Abstract</summary>
The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspective on backdoor attacks, but also presents a promising avenue to enhance the privacy of individuals within models trained on uncurated web-scraped data.
</details>
<details>
<summary>摘要</summary>
大量的AI模型通过未经整理、有时敏感的网络抓取数据进行训练，带来了一些隐私问题。其中一个问题是，敌对者可以通过隐私攻击提取模型中的信息。然而，从模型中删除特定信息而不影响性能是一项不容易的任务，并且已经证明是具有挑战性的。我们提出了一种简单又有效的防御方法，基于后门攻击来移除模型中的隐私信息，并且在本文中专注于文本编码器。具体来说，通过策略性的插入后门，我们将敏感词的嵌入与中性词的嵌入相对应，例如将个人名称替换为“一个人”。我们的实验结果表明，我们的后门基于防御方法在CLIP上表现出色，并且提供了一种新的“双用”视角，以及一个可能的方式来增强模型中人类隐私的保护。
</details></li>
</ul>
<hr>
<h2 id="Not-All-Demonstration-Examples-are-Equally-Beneficial-Reweighting-Demonstration-Examples-for-In-Context-Learning"><a href="#Not-All-Demonstration-Examples-are-Equally-Beneficial-Reweighting-Demonstration-Examples-for-In-Context-Learning" class="headerlink" title="Not All Demonstration Examples are Equally Beneficial: Reweighting Demonstration Examples for In-Context Learning"></a>Not All Demonstration Examples are Equally Beneficial: Reweighting Demonstration Examples for In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08309">http://arxiv.org/abs/2310.08309</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Zhe-Young/WICL">https://github.com/Zhe-Young/WICL</a></li>
<li>paper_authors: Zhe Yang, Damai Dai, Peiyi Wang, Zhifang Sui</li>
<li>for: 这 paper 的目的是研究如何在 In-Context Learning (ICL) 中确定示例的权重，以及如何在不同的模型位置上应用这些权重。</li>
<li>methods: 这 paper 使用了 masked self-prediction (MSP) Score 来评估示例的质量，并采用了粒子搜索和粒子搜索来寻找approximately optimal weights。</li>
<li>results: 这 paper 的实验结果显示，使用该方法可以大幅提高 IC L 性能，并且比 convential ICL 要好得多。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have recently gained the In-Context Learning (ICL) ability with the models scaling up, allowing them to quickly adapt to downstream tasks with only a few demonstration examples prepended in the input sequence. Nonetheless, the current practice of ICL treats all demonstration examples equally, which still warrants improvement, as the quality of examples is usually uneven. In this paper, we investigate how to determine approximately optimal weights for demonstration examples and how to apply them during ICL. To assess the quality of weights in the absence of additional validation data, we design a masked self-prediction (MSP) score that exhibits a strong correlation with the final ICL performance. To expedite the weight-searching process, we discretize the continuous weight space and adopt beam search. With approximately optimal weights obtained, we further propose two strategies to apply them to demonstrations at different model positions. Experimental results on 8 text classification tasks show that our approach outperforms conventional ICL by a large margin. Our code are publicly available at https:github.com/Zhe-Young/WICL.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MProto-Multi-Prototype-Network-with-Denoised-Optimal-Transport-for-Distantly-Supervised-Named-Entity-Recognition"><a href="#MProto-Multi-Prototype-Network-with-Denoised-Optimal-Transport-for-Distantly-Supervised-Named-Entity-Recognition" class="headerlink" title="MProto: Multi-Prototype Network with Denoised Optimal Transport for Distantly Supervised Named Entity Recognition"></a>MProto: Multi-Prototype Network with Denoised Optimal Transport for Distantly Supervised Named Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08298">http://arxiv.org/abs/2310.08298</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XiPotatonium/mproto">https://github.com/XiPotatonium/mproto</a></li>
<li>paper_authors: Shuhui Wu, Yongliang Shen, Zeqi Tan, Wenqi Ren, Jietian Guo, Shiliang Pu, Weiming Lu</li>
<li>for: 这篇论文targets distantly supervised named entity recognition (NER) task, aiming to locate entity mentions and classify their types with only knowledge bases or gazetteers and unlabeled corpus.</li>
<li>methods: 这篇论文提出了一个具有抗杂读能力的原型网络（MProto）来解决DS-NER任务。不同于先前的原型基本的NER方法，MProto将每个entity type represented by multiple prototypes，以具体化entity表现的内部变化。</li>
<li>results:  experiments on several DS-NER benchmarks show that our MProto achieves state-of-the-art performance.Here’s the format you requested:</li>
<li>for: &lt;这篇论文targets distantly supervised named entity recognition (NER) task, aiming to locate entity mentions and classify their types with only knowledge bases or gazetteers and unlabeled corpus.&gt;</li>
<li>methods: &lt;这篇论文提出了一个具有抗杂读能力的原型网络（MProto）来解决DS-NER任务。不同于先前的原型基本的NER方法，MProto将每个entity type represented by multiple prototypes，以具体化entity表现的内部变化。&gt;</li>
<li>results: <experiments on several DS-NER benchmarks show that our MProto achieves state-of-the-art performance.><details>
<summary>Abstract</summary>
Distantly supervised named entity recognition (DS-NER) aims to locate entity mentions and classify their types with only knowledge bases or gazetteers and unlabeled corpus. However, distant annotations are noisy and degrade the performance of NER models. In this paper, we propose a noise-robust prototype network named MProto for the DS-NER task. Different from previous prototype-based NER methods, MProto represents each entity type with multiple prototypes to characterize the intra-class variance among entity representations. To optimize the classifier, each token should be assigned an appropriate ground-truth prototype and we consider such token-prototype assignment as an optimal transport (OT) problem. Furthermore, to mitigate the noise from incomplete labeling, we propose a novel denoised optimal transport (DOT) algorithm. Specifically, we utilize the assignment result between Other class tokens and all prototypes to distinguish unlabeled entity tokens from true negatives. Experiments on several DS-NER benchmarks demonstrate that our MProto achieves state-of-the-art performance. The source code is now available on Github.
</details>
<details>
<summary>摘要</summary>
distant supervised named entity recognition (DS-NER) targets to locate entity mentions and classify their types with only knowledge bases or gazetteers and unlabeled corpus. However, distant annotations are noisy and degrade the performance of NER models. In this paper, we propose a noise-robust prototype network named MProto for the DS-NER task. Different from previous prototype-based NER methods, MProto represents each entity type with multiple prototypes to characterize the intra-class variance among entity representations. To optimize the classifier, each token should be assigned an appropriate ground-truth prototype, and we consider such token-prototype assignment as an optimal transport (OT) problem. Furthermore, to mitigate the noise from incomplete labeling, we propose a novel denoised optimal transport (DOT) algorithm. Specifically, we utilize the assignment result between Other class tokens and all prototypes to distinguish unlabeled entity tokens from true negatives. Experiments on several DS-NER benchmarks demonstrate that our MProto achieves state-of-the-art performance. The source code is now available on Github.Here's the word-for-word translation of the text into Simplified Chinese:远程监督名entity recognition (DS-NER) 目标是在只有知识库或地图的情况下，找到实体提及和其类型的标注，但远程标注具有噪音性，这会降低NER模型的性能。本文提出一种鲁棒的prototype网络，名为MProto，用于DS-NER任务。与前一些prototype-based NER方法不同，MProto对每个实体类型使用多个проtotypes来描述实体表示的内部变异。为优化分类器，每个token应该被分配到合适的真实prototype，我们认为这是一个optimal transport (OT)问题。此外，为了 mitigate incomplete labeling的噪音，我们提出了一种新的denoised optimal transport (DOT)算法。具体来说，我们使用其他类型token和所有prototypes之间的匹配结果，来 отличиtrue negatives from unlabeled entity tokens。实验表明，我们的MProto在多个DS-NER benchmark上达到了状态的性能。源代码现已经在Github上可用。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Odia-Braille-Literacy-The-Influence-of-Speed-on-Error-Reduction-and-Enhanced-Comprehension"><a href="#Optimizing-Odia-Braille-Literacy-The-Influence-of-Speed-on-Error-Reduction-and-Enhanced-Comprehension" class="headerlink" title="Optimizing Odia Braille Literacy: The Influence of Speed on Error Reduction and Enhanced Comprehension"></a>Optimizing Odia Braille Literacy: The Influence of Speed on Error Reduction and Enhanced Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08280">http://arxiv.org/abs/2310.08280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monnie Parida, Manjira Sinha, Anupam Basu, Pabitra Mitra</li>
<li>for: 这项研究的目的是对学生们的奥地利Braille阅读理解进行详细的分析，尤其是针对视障学生的阅读速度和手或指运动。</li>
<li>methods: 本研究使用观察参与者的手运动来理解阅读错误与手运动之间的关系，以及参与者的奥地利Braille阅读技能、阅读速度、错误和理解水平。</li>
<li>results: 研究发现阅读速度和阅读错误之间存在显著的相关性，即阅读速度下降时，阅读错误的数量往往增加。此外，研究还发现，改善Braille阅读错误可以提高阅读理解水平，而不同的Braille阅读模式可能存在不同的理论、发展和方法ологи的意义。<details>
<summary>Abstract</summary>
This study aims to conduct an extensive detailed analysis of the Odia Braille reading comprehension among students with visual disability. Specifically, the study explores their reading speed and hand or finger movements. The study also aims to investigate any comprehension difficulties and reading errors they may encounter. Six students from the 9th and 10th grades, aged between 14 and 16, participated in the study. We observed participants hand movements to understand how reading errors were connected to hand movement and identify the students reading difficulties. We also evaluated the participants Odia Braille reading skills, including their reading speed (in words per minute), errors, and comprehension. The average speed of Odia Braille reader is 17.64wpm. According to the study, there was a noticeable correlation between reading speed and reading errors. As reading speed decreased, the number of reading errors tended to increase. Moreover, the study established a link between reduced Braille reading errors and improved reading comprehension. In contrast, the study found that better comprehension was associated with increased reading speed. The researchers concluded with some interesting findings about preferred Braille reading patterns. These findings have important theoretical, developmental, and methodological implications for instruction.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Odia" is the Braille script used for the Odia language.* "wpm" stands for "words per minute".* "reading errors" refer to mistakes made while reading, such as misidentifying letters or words.* "comprehension" refers to the ability to understand the meaning of the text being read.* "preferred Braille reading patterns" refer to the specific ways in which students with visual disabilities tend to read Braille text.
</details></li>
</ul>
<hr>
<h2 id="Who-Said-That-Benchmarking-Social-Media-AI-Detection"><a href="#Who-Said-That-Benchmarking-Social-Media-AI-Detection" class="headerlink" title="Who Said That? Benchmarking Social Media AI Detection"></a>Who Said That? Benchmarking Social Media AI Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08240">http://arxiv.org/abs/2310.08240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyun Cui, Linqiu Zhang, Qianle Wang, Shuyang Cai</li>
<li>for: 本研究旨在评估社交媒体平台上AI文本检测模型的能力，并提出了一个新的用户参与型AI文本检测挑战。</li>
<li>methods: 本研究使用了SAID（社交媒体AI检测） benchmark，该benchmark基于真实的社交媒体平台上的AI生成文本，如Zhihu和Quora。与现有的benchmark不同，SAID更加注重实际上的AI用户在互联网上使用的策略，以提供更加真实和挑战性的评估环境。</li>
<li>results: 研究发现，使用Zhihu数据集，人工标注者可以将AI生成文本和人类生成文本正确分类的平均准确率为96.5%。这一结果表明，在今天广泛使用AI的环境中，人类可能需要重新评估AI生成文本的识别能力。此外，研究还提出了一个新的用户参与型AI文本检测挑战，该挑战的实验结果表明，在实际社交媒体平台上进行检测任务比传统的模拟AI文本检测更加具有挑战性，并且用户参与型AI文本检测可以提高检测精度。<details>
<summary>Abstract</summary>
AI-generated text has proliferated across various online platforms, offering both transformative prospects and posing significant risks related to misinformation and manipulation. Addressing these challenges, this paper introduces SAID (Social media AI Detection), a novel benchmark developed to assess AI-text detection models' capabilities in real social media platforms. It incorporates real AI-generate text from popular social media platforms like Zhihu and Quora. Unlike existing benchmarks, SAID deals with content that reflects the sophisticated strategies employed by real AI users on the Internet which may evade detection or gain visibility, providing a more realistic and challenging evaluation landscape. A notable finding of our study, based on the Zhihu dataset, reveals that annotators can distinguish between AI-generated and human-generated texts with an average accuracy rate of 96.5%. This finding necessitates a re-evaluation of human capability in recognizing AI-generated text in today's widely AI-influenced environment. Furthermore, we present a new user-oriented AI-text detection challenge focusing on the practicality and effectiveness of identifying AI-generated text based on user information and multiple responses. The experimental results demonstrate that conducting detection tasks on actual social media platforms proves to be more challenging compared to traditional simulated AI-text detection, resulting in a decreased accuracy. On the other hand, user-oriented AI-generated text detection significantly improve the accuracy of detection.
</details>
<details>
<summary>摘要</summary>
人工智能生成的文本已经渗透到了各种在线平台，带来了重大的可能性和风险，其中包括误information和操纵。为了解决这些挑战，本文提出了SAID（社交媒体AI检测），一个新的benchmark，用于评估AI文本检测模型在实际社交媒体平台上的能力。它包括来自popular社交媒体平台 like Zhihu和Quora的真实AI生成的文本。与现有benchmark不同，SAID处理了实际上的AI用户在互联网上采用的复杂策略，这些策略可能会逃避检测或获得可见性，提供一个更真实和挑战的评估景象。我们的研究发现，基于Zhihu数据集，标注员可以在96.5%的情况下 correctly distinguish between AI-generated and human-generated texts。这一发现需要我们重新评估现代社交媒体环境中人类对AI生成文本的识别能力。此外，我们还提出了一个新的用户 oriented AI文本检测挑战，该挑战的目的是评估检测模型在实际社交媒体平台上的实用性和效果。实验结果表明，在实际社交媒体平台上进行检测任务比传统的模拟AI文本检测更加具有挑战性，导致检测精度下降。然而，用户 oriented AI文本检测显示出明显的改善，提高检测精度。
</details></li>
</ul>
<hr>
<h2 id="Language-Models-are-Universal-Embedders"><a href="#Language-Models-are-Universal-Embedders" class="headerlink" title="Language Models are Universal Embedders"></a>Language Models are Universal Embedders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08232">http://arxiv.org/abs/2310.08232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/izhx/uni-rep">https://github.com/izhx/uni-rep</a></li>
<li>paper_authors: Xin Zhang, Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Min Zhang</li>
<li>for:  This paper aims to build a unified embedding model that can be applied across tasks and languages, rather than dedicated models for each scenario.</li>
<li>methods: The authors use pre-trained transformer decoders for multiple languages and fine-tune them on limited English data to demonstrate universal embedding.</li>
<li>results: The models achieve competitive performance on different embedding tasks with minimal training data, and perform comparably to heavily supervised baselines and&#x2F;or APIs on other benchmarks such as multilingual classification and code search.<details>
<summary>Abstract</summary>
In the large language model (LLM) revolution, embedding is a key component of various systems. For example, it is used to retrieve knowledge or memories for LLMs, to build content moderation filters, etc. As such cases span from English to other natural or programming languages, from retrieval to classification and beyond, it is desirable to build a unified embedding model rather than dedicated ones for each scenario. In this work, we make an initial step towards this goal, demonstrating that multiple languages (both natural and programming) pre-trained transformer decoders can embed universally when finetuned on limited English data. We provide a comprehensive practice with thorough evaluations. On English MTEB, our models achieve competitive performance on different embedding tasks by minimal training data. On other benchmarks, such as multilingual classification and code search, our models (without any supervision) perform comparably to, or even surpass heavily supervised baselines and/or APIs. These results provide evidence of a promising path towards building powerful unified embedders that can be applied across tasks and languages.
</details>
<details>
<summary>摘要</summary>
在大语言模型（LLM）革命中，嵌入是一个关键组件，用于多种系统。例如，用于检索知识或记忆，建立内容审核筛选器等。由于这些案例跨越英语到其他自然语言或编程语言，从检索到分类和更多的应用场景，因此建立一个统一的嵌入模型比较愿意。在这项工作中，我们做出了初步的尝试，证明多种自然语言和编程语言预训练转换器可以在有限的英语数据上进行共同嵌入。我们提供了全面的实践和详细的评估。在英语MTEB上，我们的模型在不同的嵌入任务上达到了竞争性的性能，只需要训练数据的最小量。在其他benchmark上，如多语言分类和代码搜索，我们的模型（无任何监督）与大量监督的基线和/或API进行了相当的比较，甚至超越了它们。这些结果表明了建立强大的统一嵌入器的可能性，可以在任务和语言之间应用。
</details></li>
</ul>
<hr>
<h2 id="Fast-Word-Error-Rate-Estimation-Using-Self-Supervised-Representations-For-Speech-And-Text"><a href="#Fast-Word-Error-Rate-Estimation-Using-Self-Supervised-Representations-For-Speech-And-Text" class="headerlink" title="Fast Word Error Rate Estimation Using Self-Supervised Representations For Speech And Text"></a>Fast Word Error Rate Estimation Using Self-Supervised Representations For Speech And Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08225">http://arxiv.org/abs/2310.08225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chanho Park, Chengsong Lu, Mingjie Chen, Thomas Hain</li>
<li>for: 这篇论文是为了提出一种快速的word error rate（WER）估计方法，以提高计算效率。</li>
<li>methods: 该方法使用了自主学习表示（SSLR），通过均值抽象来组合SSLR，并实现了快速的计算。</li>
<li>results: 实验结果表明，该方法（Fe-WER）相比基eline（e-WER3）在Ted-Lium3上提高了19.69%和7.16%，并且Weighted by duration是10.43%。同时，实时因子大约是4倍。<details>
<summary>Abstract</summary>
The quality of automatic speech recognition (ASR) is typically measured by word error rate (WER). WER estimation is a task aiming to predict the WER of an ASR system, given a speech utterance and a transcription. This task has gained increasing attention while advanced ASR systems are trained on large amounts of data. In this case, WER estimation becomes necessary in many scenarios, for example, selecting training data with unknown transcription quality or estimating the testing performance of an ASR system without ground truth transcriptions. Facing large amounts of data, the computation efficiency of a WER estimator becomes essential in practical applications. However, previous works usually did not consider it as a priority. In this paper, a Fast WER estimator (Fe-WER) using self-supervised learning representation (SSLR) is introduced. The estimator is built upon SSLR aggregated by average pooling. The results show that Fe-WER outperformed the e-WER3 baseline relatively by 19.69% and 7.16% on Ted-Lium3 in both evaluation metrics of root mean square error and Pearson correlation coefficient, respectively. Moreover, the estimation weighted by duration was 10.43% when the target was 10.88%. Lastly, the inference speed was about 4x in terms of a real-time factor.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）的质量通常由单词错误率（WER）来度量。WER估计是一个目标，它是估计给定一个语音utterance和一个转写的ASR系统的WER。这个任务在高级ASR系统被训练在大量数据后得到了越来越多的关注。在这种情况下，WER估计在许多场景中变得必要，例如选择 unknown 转写质量的训练数据或者测试ASR系统的性能 без 真实的转写。面临大量数据的情况下，WER估计的计算效率在实际应用中变得非常重要。然而，之前的工作通常不会将其作为优先级考虑。本文提出了一种快速的WER估计器（Fe-WER），使用自然学习表示（SSLR）进行自我监督学习。 Fe-WER 基于 SSLR 的均值聚合。结果表明，Fe-WER 相比 e-WER3 基线比例提高了19.69%和7.16% 在 Ted-Lium3 上的两个评价指标中的根mean square error 和 Pearson 相关系数，分别。此外，Weighted by duration 的估计为10.43%，目标值为10.88%。最后，实时因子约为4倍。
</details></li>
</ul>
<hr>
<h2 id="Visual-Question-Generation-in-Bengali"><a href="#Visual-Question-Generation-in-Bengali" class="headerlink" title="Visual Question Generation in Bengali"></a>Visual Question Generation in Bengali</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08187">http://arxiv.org/abs/2310.08187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mahmudhasankhan/vqg-in-bengali">https://github.com/mahmudhasankhan/vqg-in-bengali</a></li>
<li>paper_authors: Mahmud Hasan, Labiba Islam, Jannatul Ferdous Ruma, Tasmiah Tahsin Mayeesha, Rashedur M. Rahman</li>
<li>for: The paper is written for the task of Visual Question Generation (VQG) in Bengali, with the goal of generating human-like questions relevant to given images.</li>
<li>methods: The paper proposes a novel transformer-based encoder-decoder architecture for VQG in Bengali, with multiple variants including image-only, image-category, and image-answer-category.</li>
<li>results: The paper achieves state-of-the-art results on the translated VQAv2.0 dataset, with the image-cat model achieving the highest BLUE-1 and BLEU-3 scores. The human evaluation suggests that the image-cat model is capable of generating goal-driven and attribute-specific questions that are relevant to the corresponding images.Here is the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文是为视觉问题生成（VQG）任务中的孟加语言（Bengali）而写的。</li>
<li>methods: 这篇论文提出了一种基于转换器的编码器-解码体系，用于VQG任务中的孟加语言。</li>
<li>results: 这篇论文在转换VQAv2.0数据集上实现了状态的最佳Result，image-cat模型在BLUE-1和BLEU-3分数上达到了最高分数。人工评估表明，image-cat模型能够生成具有目标和特征的问题，与对应的图像相关。<details>
<summary>Abstract</summary>
The task of Visual Question Generation (VQG) is to generate human-like questions relevant to the given image. As VQG is an emerging research field, existing works tend to focus only on resource-rich language such as English due to the availability of datasets. In this paper, we propose the first Bengali Visual Question Generation task and develop a novel transformer-based encoder-decoder architecture that generates questions in Bengali when given an image. We propose multiple variants of models - (i) image-only: baseline model of generating questions from images without additional information, (ii) image-category and image-answer-category: guided VQG where we condition the model to generate questions based on the answer and the category of expected question. These models are trained and evaluated on the translated VQAv2.0 dataset. Our quantitative and qualitative results establish the first state of the art models for VQG task in Bengali and demonstrate that our models are capable of generating grammatically correct and relevant questions. Our quantitative results show that our image-cat model achieves a BLUE-1 score of 33.12 and BLEU-3 score of 7.56 which is the highest of the other two variants. We also perform a human evaluation to assess the quality of the generation tasks. Human evaluation suggests that image-cat model is capable of generating goal-driven and attribute-specific questions and also stays relevant to the corresponding image.
</details>
<details>
<summary>摘要</summary>
文本内容：视觉问题生成（VQG）的任务是生成与给定图像相关的人类化问题。由于现有的数据集主要集中在英语等资源丰富的语言上，现有研究主要集中在这些语言上。在这篇论文中，我们提出了第一个孟加拉语视觉问题生成任务，并开发了一种基于转换器的编码器-解码体系，该系统可以从图像中生成孟加拉语问题。我们提出了多种变体的模型，包括（i）图像只：基线模型，不受任何额外信息影响而生成问题（ii）图像类别和图像答案类别：指导VQG的模型，conditioning模型以图像答案和预期的问题类别来生成问题。这些模型在翻译的VQAv2.0数据集上进行训练和评估。我们的量化和质量结果表明，我们的模型在孟加拉语VQG任务中创造了第一个状态的艺术模型，并且能够生成正确的 grammatical 和相关的问题。我们的量化结果表明，我们的图像类别模型在BLUE-1和BLEU-3指标上达到了33.12和7.56的最高分，并且在人类评价中也表现出了优异。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Cognitive-Knowledge-Structure-of-Large-Language-Models-An-Educational-Diagnostic-Assessment-Approach"><a href="#Exploring-the-Cognitive-Knowledge-Structure-of-Large-Language-Models-An-Educational-Diagnostic-Assessment-Approach" class="headerlink" title="Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach"></a>Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08172">http://arxiv.org/abs/2310.08172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Zhang, Jifan Yu, Juanzi Li, Lei Hou</li>
<li>for: 本研究旨在evaluate Large Language Models (LLMs)的知识结构，以便更好地理解LLMs的认知能力和知识表达方式。</li>
<li>methods: 本研究使用了eduational diagnostic assessment method和MoocRadar dataset，这是一个基于Bloom Taxonomy的人工测试数据集。</li>
<li>results: 研究发现LLMs具有了丰富的知识结构，并且其认知能力在不同领域中具有强大的表达能力。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities. This research emphasizes the significance of investigating LLMs' knowledge and understanding the disparate cognitive patterns of LLMs. By shedding light on models' knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）不仅在不同任务中表现出色，而且也表现出了智能的启示。最近的研究主要集中在评估这些模型在人类考试中的能力，并发现它们在不同领域中表现出了惊人的能力。然而，对于LLM的总体知识结构的认知研究仍然缺乏。在这篇论文中，我们通过基于教育诊断评估方法的MoocRadar，一个精心注释的人类测试数据集基于Bloom分类法，进行评估。我们希望通过这些研究来揭示LLM的知识结构，并了解它们的不同认知模式。这些研究可以帮助研究人员更好地发展和利用LLM，以更加了解和有效地使用它们。
</details></li>
</ul>
<hr>
<h2 id="Simplicity-Level-Estimate-SLE-A-Learned-Reference-Less-Metric-for-Sentence-Simplification"><a href="#Simplicity-Level-Estimate-SLE-A-Learned-Reference-Less-Metric-for-Sentence-Simplification" class="headerlink" title="Simplicity Level Estimate (SLE): A Learned Reference-Less Metric for Sentence Simplification"></a>Simplicity Level Estimate (SLE): A Learned Reference-Less Metric for Sentence Simplification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08170">http://arxiv.org/abs/2310.08170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamcripwell/sle">https://github.com/liamcripwell/sle</a></li>
<li>paper_authors: Liam Cripwell, Joël Legrand, Claire Gardent</li>
<li>for: 这个论文是为了提出一种新的自动评估方法，以解决自动 sentence simplification 中的评估问题。</li>
<li>methods: 该论文使用了一种新的学习型评估 metric，称为 SLE，它专注于简洁性，并且与人类评估更高度相关。</li>
<li>results: 论文表明，SLE metric 可以准确地评估自动 sentence simplification 的性能，并且与人类评估更高度相关，比大多数现有的评估 metric 更高。<details>
<summary>Abstract</summary>
Automatic evaluation for sentence simplification remains a challenging problem. Most popular evaluation metrics require multiple high-quality references -- something not readily available for simplification -- which makes it difficult to test performance on unseen domains. Furthermore, most existing metrics conflate simplicity with correlated attributes such as fluency or meaning preservation. We propose a new learned evaluation metric (SLE) which focuses on simplicity, outperforming almost all existing metrics in terms of correlation with human judgements.
</details>
<details>
<summary>摘要</summary>
自动评估句子简化仍然是一个挑战性的问题。大多数流行的评估指标需要多个高质量的参考文本，但这些参考文本对简化不 readily available，这使得测试性能在未看到的领域变得困难。另外，大多数现有的指标会混同简化的特征与相关的属性，如流利度或意义保持。我们提出了一个新的学习based的评估指标（SLE），它专注于简化，超越了大多数现有指标在人工判断上的相关性。
</details></li>
</ul>
<hr>
<h2 id="Multiclass-Classification-of-Policy-Documents-with-Large-Language-Models"><a href="#Multiclass-Classification-of-Policy-Documents-with-Large-Language-Models" class="headerlink" title="Multiclass Classification of Policy Documents with Large Language Models"></a>Multiclass Classification of Policy Documents with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08167">http://arxiv.org/abs/2310.08167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erkan Gunes, Christoffer Koch Florczak</li>
<li>for:  automate text classification processes for social science research purposes</li>
<li>methods:  use GPT 3.5 and GPT 4 models of OpenAI, pre-trained instruction-tuned Large Language Models (LLM)</li>
<li>results:  overall accuracies ranging from 58-83% depending on scenario and GPT model employed, with the most humanly demanding use-case achieving 83% accuracy on 65% of the data.<details>
<summary>Abstract</summary>
Classifying policy documents into policy issue topics has been a long-time effort in political science and communication disciplines. Efforts to automate text classification processes for social science research purposes have so far achieved remarkable results, but there is still a large room for progress. In this work, we test the prediction performance of an alternative strategy, which requires human involvement much less than full manual coding. We use the GPT 3.5 and GPT 4 models of the OpenAI, which are pre-trained instruction-tuned Large Language Models (LLM), to classify congressional bills and congressional hearings into Comparative Agendas Project's 21 major policy issue topics. We propose three use-case scenarios and estimate overall accuracies ranging from %58-83 depending on scenario and GPT model employed. The three scenarios aims at minimal, moderate, and major human interference, respectively. Overall, our results point towards the insufficiency of complete reliance on GPT with minimal human intervention, an increasing accuracy along with the human effort exerted, and a surprisingly high accuracy achieved in the most humanly demanding use-case. However, the superior use-case achieved the %83 accuracy on the %65 of the data in which the two models agreed, suggesting that a similar approach to ours can be relatively easily implemented and allow for mostly automated coding of a majority of a given dataset. This could free up resources allowing manual human coding of the remaining %35 of the data to achieve an overall higher level of accuracy while reducing costs significantly.
</details>
<details>
<summary>摘要</summary>
政策文档的分类into policy issue topics已经是政治科学和communication disciplines的长期努力。为了自动化文本分类过程，以便社会科学研究purposes，已经取得了很好的结果，但还有很大的进步空间。在这个工作中，我们测试了一种 alternativestrategy，需要人类参与度 much less than full manual coding。我们使用OpenAI提供的GPT 3.5和GPT 4模型，这些模型是预训练的 instruction-tuned Large Language Models (LLM)，来分类国会法案和国会听证会into Comparative Agendas Project的21个主要政策问题。我们提出了三种使用enario和 estimate了准确率，从58%到83%，具体取决于scenario和GPT模型。我们的结果表明，完全依赖GPT的自动编码是不够的，随着人类努力的增加，准确率也逐渐提高。 Surprisingly, the most humanly demanding use-case achieved an accuracy of 83% on 65% of the data, suggesting that a similar approach to ours can be relatively easily implemented and allow for mostly automated coding of a majority of a given dataset. This could free up resources, allowing manual human coding of the remaining 35% of the data to achieve an overall higher level of accuracy while reducing costs significantly.
</details></li>
</ul>
<hr>
<h2 id="Ziya-Visual-Bilingual-Large-Vision-Language-Model-via-Multi-Task-Instruction-Tuning"><a href="#Ziya-Visual-Bilingual-Large-Vision-Language-Model-via-Multi-Task-Instruction-Tuning" class="headerlink" title="Ziya-Visual: Bilingual Large Vision-Language Model via Multi-Task Instruction Tuning"></a>Ziya-Visual: Bilingual Large Vision-Language Model via Multi-Task Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08166">http://arxiv.org/abs/2310.08166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Lu, Dixiang Zhang, Xiaojun Wu, Xinyu Gao, Ruyi Gan, Jiaxing Zhang, Yan Song, Pingjian Zhang</li>
<li>for: 这paper aimed to improve the ability of large language models (LLMs) in zero-shot image-to-text generation and understanding by integrating multi-modal inputs, specifically in non-English scenarios.</li>
<li>methods: The paper introduces the Ziya-Visual series of bilingual large-scale vision-language models (LVLMs) that incorporate visual semantics into LLMs for multi-modal dialogue. The models use the Querying Transformer from BLIP-2 and explore optimization schemes such as instruction tuning, multi-stage training, and low-rank adaptation module for visual-language alignment.</li>
<li>results: The paper shows that compared to existing LVLMs, Ziya-Visual achieves competitive performance across a wide range of English-only tasks including zero-shot image-text retrieval, image captioning, and visual question answering. The evaluation leaderboard accessed by GPT-4 also indicates that the models possess satisfactory image-text understanding and generation capabilities in Chinese multi-modal scenario dialogues.<details>
<summary>Abstract</summary>
Recent advancements enlarge the capabilities of large language models (LLMs) in zero-shot image-to-text generation and understanding by integrating multi-modal inputs. However, such success is typically limited to English scenarios due to the lack of large-scale and high-quality non-English multi-modal resources, making it extremely difficult to establish competitive counterparts in other languages. In this paper, we introduce the Ziya-Visual series, a set of bilingual large-scale vision-language models (LVLMs) designed to incorporate visual semantics into LLM for multi-modal dialogue. Composed of Ziya-Visual-Base and Ziya-Visual-Chat, our models adopt the Querying Transformer from BLIP-2, further exploring the assistance of optimization schemes such as instruction tuning, multi-stage training and low-rank adaptation module for visual-language alignment. In addition, we stimulate the understanding ability of GPT-4 in multi-modal scenarios, translating our gathered English image-text datasets into Chinese and generating instruction-response through the in-context learning method. The experiment results demonstrate that compared to the existing LVLMs, Ziya-Visual achieves competitive performance across a wide range of English-only tasks including zero-shot image-text retrieval, image captioning, and visual question answering. The evaluation leaderboard accessed by GPT-4 also indicates that our models possess satisfactory image-text understanding and generation capabilities in Chinese multi-modal scenario dialogues. Code, demo and models are available at ~\url{https://huggingface.co/IDEA-CCNL/Ziya-BLIP2-14B-Visual-v1}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Context-Compression-for-Auto-regressive-Transformers-with-Sentinel-Tokens"><a href="#Context-Compression-for-Auto-regressive-Transformers-with-Sentinel-Tokens" class="headerlink" title="Context Compression for Auto-regressive Transformers with Sentinel Tokens"></a>Context Compression for Auto-regressive Transformers with Sentinel Tokens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08152">http://arxiv.org/abs/2310.08152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DRSY/KV_Compression">https://github.com/DRSY/KV_Compression</a></li>
<li>paper_authors: Siyu Ren, Qi Jia, Kenny Q. Zhu</li>
<li>for: 这个研究旨在提高Transformer-based LLMs中的发话范围，以减少computational cost和memory footprint。</li>
<li>methods:  authors proposed a plug-and-play approach to incrementally compress the intermediate activation of a specified span of tokens into compact ones, reducing both memory and computational cost.</li>
<li>results:  experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of the proposed approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity.<details>
<summary>Abstract</summary>
The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression.
</details>
<details>
<summary>摘要</summary>
“对于Transformer基于模型中的注意模块，其二次性复杂性使得它在生成过程中逐渐成为计算的主要部分。此外，对长输入的处理也会导致严重的内存占用和执行时间问题。在这种情况下，我们提出了一种插件化方法，可以逐步压缩指定的Token之间的中间活动，从而降低内存和计算成本。实验表明，我们的方法在语料处理和零基础文档生成中表现出优于稀采baseline，在流畅性、n-gram匹配和semantic相似性等方面具有优势。最后，我们对系统性能进行了全面的评估。代码可以在https://github.com/DRSY/KV_Compression中找到。”
</details></li>
</ul>
<hr>
<h2 id="On-the-Relevance-of-Phoneme-Duration-Variability-of-Synthesized-Training-Data-for-Automatic-Speech-Recognition"><a href="#On-the-Relevance-of-Phoneme-Duration-Variability-of-Synthesized-Training-Data-for-Automatic-Speech-Recognition" class="headerlink" title="On the Relevance of Phoneme Duration Variability of Synthesized Training Data for Automatic Speech Recognition"></a>On the Relevance of Phoneme Duration Variability of Synthesized Training Data for Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08132">http://arxiv.org/abs/2310.08132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nick Rossenbach, Benedikt Hilmes, Ralf Schlüter</li>
<li>for: 提高自动语音识别（ASR）系统的表现，特别是在low-resource或频率域不符合任务下。</li>
<li>methods: 使用新的oracle设置，研究 tekst-to-speech（TTS）系统生成的数据质量如何影响ASR训练。使用两种常见的对齐方法：隐藏马尔可夫混合模型（HMM-GMM）对齐器和神经网络Connectionist Temporal Classification（CTC）对齐器。使用一种简单的随机步骤算法，将TTS系统生成的音频帧duration分布靠拟真实duration分布，从而提高ASR系统使用synthetic数据的表现。</li>
<li>results: 使用这种方法可以提高ASR系统在semi-supervised Setting下的表现，使得它能够更好地识别来自TTS系统生成的语音。<details>
<summary>Abstract</summary>
Synthetic data generated by text-to-speech (TTS) systems can be used to improve automatic speech recognition (ASR) systems in low-resource or domain mismatch tasks. It has been shown that TTS-generated outputs still do not have the same qualities as real data. In this work we focus on the temporal structure of synthetic data and its relation to ASR training. By using a novel oracle setup we show how much the degradation of synthetic data quality is influenced by duration modeling in non-autoregressive (NAR) TTS. To get reference phoneme durations we use two common alignment methods, a hidden Markov Gaussian-mixture model (HMM-GMM) aligner and a neural connectionist temporal classification (CTC) aligner. Using a simple algorithm based on random walks we shift phoneme duration distributions of the TTS system closer to real durations, resulting in an improvement of an ASR system using synthetic data in a semi-supervised setting.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用文本到语音（TTS）系统生成的 sintetic 数据可以提高自动语音识别（ASR）系统在low-resource或频率匹配任务中的性能。然而，TTS生成的输出还没有与实际数据相同的质量。在这项工作中，我们关注了synthetic数据的时间结构和ASR训练之间的关系。我们使用一种新的oracle设置，以证明duration模型在non-autoregressive（NAR）TTS中对数据质量的影响。为获取参考音频duration，我们使用两种常见的对接方法：隐藏马尔可夫混合模型（HMM-GMM）对接器和神经网络时间分类（CTC）对接器。使用一种基于随机漫步的简单算法，我们将TTS系统中phoneme duration的分布shift到更近于实际duration，从而提高使用synthetic数据的ASR系统在半supervised Setting中的性能。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Fine-grained-Conversational-Decoding-via-Isotropic-and-Proximal-Search"><a href="#Fine-grained-Conversational-Decoding-via-Isotropic-and-Proximal-Search" class="headerlink" title="Fine-grained Conversational Decoding via Isotropic and Proximal Search"></a>Fine-grained Conversational Decoding via Isotropic and Proximal Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08130">http://arxiv.org/abs/2310.08130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Yao, Han Wu, Qiling Xu, Linqi Song</li>
<li>for: 提高对话Response的质量，提出了一种细化的对话解码方法。</li>
<li>methods: 基于\citet{wu2023learning}的思想，提出了一种名为\textit{isotropic and proximal search (IPS)}的方法，以实现对话具有本地和均匀性的特征空间。</li>
<li>results: 对比其他对话解码策略，我们的方法在对话领域中表现出色，在自动和人类评价指标上都有显著的优势。<details>
<summary>Abstract</summary>
General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by \citet{wu2023learning} that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed \textit{isotropic and proximal search (IPS)}. Our method is designed to generate the semantic-concentrated response, while still maintaining informativeness and discrimination against the context. Experiments show that our approach outperforms existing decoding strategies in the dialogue field across both automatic and human evaluation metrics. More in-depth analyses further confirm the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
通用文本解码方法通常用于对话回复生成。虽然使用对话特定编码方法可以提高生成的回复质量，但对话解码方法仍然受欢迎。以\citet{wu2023learning}为例，我们认为一个好的对话特征空间应该遵循地方性和射线性的规则。基于这些原则，我们提出了细化的对话解码方法，称为iso tropic and proximal search（IPS）。我们的方法旨在生成具有semantic concentrate的回复，同时仍保持对上下文的信息性和分化性。实验表明，我们的方法在对话领域中胜过现有的解码策略，在自动和人类评估指标上都显示出优异表现。更详细的分析还证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Who-Wrote-it-and-Why-Prompting-Large-Language-Models-for-Authorship-Verification"><a href="#Who-Wrote-it-and-Why-Prompting-Large-Language-Models-for-Authorship-Verification" class="headerlink" title="Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification"></a>Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08123">http://arxiv.org/abs/2310.08123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chia-Yu Hung, Zhiqiang Hu, Yujia Hu, Roy Ka-Wei Lee</li>
<li>for: 本研究的目的是提出一种基于大型自然语言模型（LLM）的作者鉴定（AV）技术，以提高AV的数据要求和解释性。</li>
<li>methods: 本研究使用了LLMs提供步骤性的 стилиometric解释提示，以解决现有AV技术的数据限制和解释性不足问题。</li>
<li>results: 实验结果显示，PromptAV比 estado-of-the-art基elines高效，可以有效地使用有限的培训数据，并提供了Intuitive的解释，表明PromptAV可能成为一种有效和可解释的AV解决方案。<details>
<summary>Abstract</summary>
Authorship verification (AV) is a fundamental task in natural language processing (NLP) and computational linguistics, with applications in forensic analysis, plagiarism detection, and identification of deceptive content. Existing AV techniques, including traditional stylometric and deep learning approaches, face limitations in terms of data requirements and lack of explainability. To address these limitations, this paper proposes PromptAV, a novel technique that leverages Large-Language Models (LLMs) for AV by providing step-by-step stylometric explanation prompts. PromptAV outperforms state-of-the-art baselines, operates effectively with limited training data, and enhances interpretability through intuitive explanations, showcasing its potential as an effective and interpretable solution for the AV task.
</details>
<details>
<summary>摘要</summary>
<<SYS Translate="1">作者识别（AV）是自然语言处理（NLP）和计算语言学的基本任务，具有潜在的应用于医学分析、 плаги依抄检测和识别偏执性内容。现有的AV技术，包括传统的风格统计和深度学习方法，受到数据需求的限制和解释性的不足。为了解决这些限制，本文提出了PromptAV，一种新的技术，利用大型自然语言模型（LLMs）进行AV，并提供了步骤性的风格解释提示。PromptAV在比较州的基elines上表现出色，可以有效地使用有限的训练数据，并提高了解释性通过直观的解释，这显示了PromptAV作为一种有效和可解释的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Voice-Conversion-for-Stuttered-Speech-Instruments-Unseen-Languages-and-Textually-Described-Voices"><a href="#Voice-Conversion-for-Stuttered-Speech-Instruments-Unseen-Languages-and-Textually-Described-Voices" class="headerlink" title="Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices"></a>Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08104">http://arxiv.org/abs/2310.08104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Baas, Herman Kamper</li>
<li>for:  investigate how a recent voice conversion model performs on non-standard downstream voice conversion tasks</li>
<li>methods: 使用 k-nearest neighbors voice conversion (kNN-VC) 方法</li>
<li>results:  compared to an established baseline, kNN-VC retains high performance in stuttered and cross-lingual voice conversion, but results are more mixed for musical instrument and text-to-voice conversion tasks.<details>
<summary>Abstract</summary>
Voice conversion aims to convert source speech into a target voice using recordings of the target speaker as a reference. Newer models are producing increasingly realistic output. But what happens when models are fed with non-standard data, such as speech from a user with a speech impairment? We investigate how a recent voice conversion model performs on non-standard downstream voice conversion tasks. We use a simple but robust approach called k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard applications: stuttered voice conversion, cross-lingual voice conversion, musical instrument conversion, and text-to-voice conversion. The latter involves converting to a target voice specified through a text description, e.g. "a young man with a high-pitched voice". Compared to an established baseline, we find that kNN-VC retains high performance in stuttered and cross-lingual voice conversion. Results are more mixed for the musical instrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some instruments like drums but not on others. Nevertheless, this shows that voice conversion models - and kNN-VC in particular - are increasingly applicable in a range of non-standard downstream tasks. But there are still limitations when samples are very far from the training distribution. Code, samples, trained models: https://rf5.github.io/sacair2023-knnvc-demo/.
</details>
<details>
<summary>摘要</summary>
声音转换目标是将源语音转换为目标声音，使用目标说话人的录音作为参考。 newer模型在生成声音结果时的实现得更加真实。但当模型接受非标准数据，如受残疾的说话人的语音时，会发生什么？我们调查了一种最近的声音转换模型在非标准下沟通任务中的性能。我们使用了一种简单 yet robust的方法，即k-nearest neighbors声音转换（kNN-VC）。我们分析了四种非标准应用：偏残声音转换、cross-lingual声音转换、乐器转换和文本到声音转换。后一个任务是将文本描述转换为目标声音，例如"一个年轻的男孩 WITH high-pitched 的声音"。相比已成熟的基线，我们发现kNN-VC在偏残和cross-lingual声音转换任务中保持高性能。结果在乐器和文本到声音转换任务中是更加杂乱，例如kNN-VC在鼓乐器上工作良好，但不是在其他乐器上。这表明声音转换模型 - 和kNN-VC在特定情况下的情况 - 在一些非标准下沟通任务中越来越可靠。然而，当样本很遥か于训练分布时，还存在一些限制。代码、样本、训练模型可以在https://rf5.github.io/sacair2023-knnvc-demo/中找到。
</details></li>
</ul>
<hr>
<h2 id="QASiNa-Religious-Domain-Question-Answering-using-Sirah-Nabawiyah"><a href="#QASiNa-Religious-Domain-Question-Answering-using-Sirah-Nabawiyah" class="headerlink" title="QASiNa: Religious Domain Question Answering using Sirah Nabawiyah"></a>QASiNa: Religious Domain Question Answering using Sirah Nabawiyah</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08102">http://arxiv.org/abs/2310.08102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rizquuula/QASiNa">https://github.com/rizquuula/QASiNa</a></li>
<li>paper_authors: Muhammad Razif Rizqullah, Ayu Purwarianti, Alham Fikri Aji</li>
<li>for: 这个论文目的是为了评估大语言模型（LLM）在宗教领域中的性能，特别是在伊斯兰教中。</li>
<li>methods: 本论文使用了几种大语言模型（mBERT、XLM-R和IndoBERT），对于这些模型进行了精度调整，并使用了印度尼西亚翻译的SQuAD v2.0作为数据集。</li>
<li>results: 研究发现，XLM-R模型在Question Answering Sirah Nabawiyah（QASiNa）数据集上返回了最好的表现，EM为61.20，F1-Score为75.94，和字符串匹配为70.00。与Chat GPT-3.5和GPT-4进行比较后，发现Chat GPT版本返回了较低的EM和F1-Score，同时字符串匹配得分更高，这表明Chat GPT倾向于提供过多的解释，尤其是在宗教领域。<details>
<summary>Abstract</summary>
Nowadays, Question Answering (QA) tasks receive significant research focus, particularly with the development of Large Language Model (LLM) such as Chat GPT [1]. LLM can be applied to various domains, but it contradicts the principles of information transmission when applied to the Islamic domain. In Islam we strictly regulates the sources of information and who can give interpretations or tafseer for that sources [2]. The approach used by LLM to generate answers based on its own interpretation is similar to the concept of tafseer, LLM is neither an Islamic expert nor a human which is not permitted in Islam. Indonesia is the country with the largest Islamic believer population in the world [3]. With the high influence of LLM, we need to make evaluation of LLM in religious domain. Currently, there is only few religious QA dataset available and none of them using Sirah Nabawiyah especially in Indonesian Language. In this paper, we propose the Question Answering Sirah Nabawiyah (QASiNa) dataset, a novel dataset compiled from Sirah Nabawiyah literatures in Indonesian language. We demonstrate our dataset by using mBERT [4], XLM-R [5], and IndoBERT [6] which fine-tuned with Indonesian translation of SQuAD v2.0 [7]. XLM-R model returned the best performance on QASiNa with EM of 61.20, F1-Score of 75.94, and Substring Match of 70.00. We compare XLM-R performance with Chat GPT-3.5 and GPT-4 [1]. Both Chat GPT version returned lower EM and F1-Score with higher Substring Match, the gap of EM and Substring Match get wider in GPT-4. The experiment indicate that Chat GPT tends to give excessive interpretations as evidenced by its higher Substring Match scores compared to EM and F1-Score, even after providing instruction and context. This concludes Chat GPT is unsuitable for question answering task in religious domain especially for Islamic religion.
</details>
<details>
<summary>摘要</summary>
现在，问答任务（QA） receiving significant research focus, particularly with the development of Large Language Model (LLM) such as Chat GPT [1]. LLM can be applied to various domains, but it contradicts the principles of information transmission when applied to the Islamic domain. In Islam, we strictly regulate the sources of information and who can give interpretations or tafseer for that sources [2]. The approach used by LLM to generate answers based on its own interpretation is similar to the concept of tafseer, LLM is neither an Islamic expert nor a human which is not permitted in Islam. Indonesia has the largest Islamic believer population in the world [3]. With the high influence of LLM, we need to evaluate LLM in the religious domain. Currently, there are only a few religious QA datasets available, and none of them use Sirah Nabawiyah, especially in Indonesian. In this paper, we propose the Question Answering Sirah Nabawiyah (QASiNa) dataset, a novel dataset compiled from Sirah Nabawiyah literatures in Indonesian language. We demonstrate our dataset by using mBERT [4], XLM-R [5], and IndoBERT [6], which were fine-tuned with Indonesian translation of SQuAD v2.0 [7]. XLM-R model returned the best performance on QASiNa with EM of 61.20, F1-Score of 75.94, and Substring Match of 70.00. We compare XLM-R performance with Chat GPT-3.5 and GPT-4 [1]. Both Chat GPT versions returned lower EM and F1-Score with higher Substring Match, the gap of EM and Substring Match gets wider in GPT-4. The experiment indicates that Chat GPT tends to give excessive interpretations as evidenced by its higher Substring Match scores compared to EM and F1-Score, even after providing instruction and context. This concludes that Chat GPT is unsuitable for question answering tasks in the religious domain, especially for Islamic religion.
</details></li>
</ul>
<hr>
<h2 id="ClimateNLP-Analyzing-Public-Sentiment-Towards-Climate-Change-Using-Natural-Language-Processing"><a href="#ClimateNLP-Analyzing-Public-Sentiment-Towards-Climate-Change-Using-Natural-Language-Processing" class="headerlink" title="ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using Natural Language Processing"></a>ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08099">http://arxiv.org/abs/2310.08099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Krishnan, V. S. Anoop<br>for: 这篇论文旨在分析社交媒体上关于气候变化的讨论，了解公众对这种全球挑战的看法和情感。methods: 本文使用自然语言处理技术（NLP）分析社交媒体上的气候变化讨论，并使用气候BERT模型进行精度的分类。results: 研究发现，公众对气候变化的看法和情感具有诸多特征，包括担忧、抗拒和争议等。这些发现可以帮助政策制定者、研究人员和组织更好地理解公众的看法，制定有效的策略以应对气候变化挑战。<details>
<summary>Abstract</summary>
Climate change's impact on human health poses unprecedented and diverse challenges. Unless proactive measures based on solid evidence are implemented, these threats will likely escalate and continue to endanger human well-being. The escalating advancements in information and communication technologies have facilitated the widespread availability and utilization of social media platforms. Individuals utilize platforms such as Twitter and Facebook to express their opinions, thoughts, and critiques on diverse subjects, encompassing the pressing issue of climate change. The proliferation of climate change-related content on social media necessitates comprehensive analysis to glean meaningful insights. This paper employs natural language processing (NLP) techniques to analyze climate change discourse and quantify the sentiment of climate change-related tweets. We use ClimateBERT, a pretrained model fine-tuned specifically for the climate change domain. The objective is to discern the sentiment individuals express and uncover patterns in public opinion concerning climate change. Analyzing tweet sentiments allows a deeper comprehension of public perceptions, concerns, and emotions about this critical global challenge. The findings from this experiment unearth valuable insights into public sentiment and the entities associated with climate change discourse. Policymakers, researchers, and organizations can leverage such analyses to understand public perceptions, identify influential actors, and devise informed strategies to address climate change challenges.
</details>
<details>
<summary>摘要</summary>
人类健康受气候变化影响面临历史上无 precedent 和多样化的挑战。 Unless 采取有据且有效的措施，这些威胁将持续升级，继续威胁人类生存。随着信息和通信技术的不断发展，社交媒体平台的普及和使用已成为现实。人们通过平台如Twitter和Facebook表达自己的看法、思想和评论，其中包括气候变化问题。气候变化相关内容的快速普及需要系统性的分析，以便从中提取有价值的洞察。本文使用自然语言处理（NLP）技术分析气候变化讨论，并利用ClimateBERT预训练模型，特意为气候变化领域进行了精细调整。我们的目标是探索人们表达的情感，找到气候变化话题相关的公众情况和感受。分析微博情感可以帮助我们更深入了解公众对这个全球挑战的看法、担忧和情感。本研究的发现可以为政策制定者、研究人员和组织提供有价值的情感分析和影响力actor的报告，以便更好地理解公众情况，制定有效的策略，解决气候变化挑战。
</details></li>
</ul>
<hr>
<h2 id="To-token-or-not-to-token-A-Comparative-Study-of-Text-Representations-for-Cross-Lingual-Transfer"><a href="#To-token-or-not-to-token-A-Comparative-Study-of-Text-Representations-for-Cross-Lingual-Transfer" class="headerlink" title="To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer"></a>To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08078">http://arxiv.org/abs/2310.08078</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mushfiqur11/tokenfreetransfer">https://github.com/mushfiqur11/tokenfreetransfer</a></li>
<li>paper_authors: Md Mushfiqur Rahman, Fardin Ahsan Sakib, Fahim Faisal, Antonios Anastasopoulos</li>
<li>For: The paper aims to understand the downstream implications of text representation choices in low-resource cross-lingual transfer, and to provide a recommendation scheme for model selection based on task and language requirements.* Methods: The paper compares language models with diverse text representation modalities, including segmentation-based models (BERT, mBERT), image-based models (PIXEL), and character-level models (CANINE), on three NLP tasks (POS tagging, Dependency parsing, and NER) in 19 source languages and 133 target languages.* Results: The paper finds that image-based models excel in cross-lingual transfer for closely related languages with visually similar scripts, while segmentation-based models are superior for tasks that rely on word meaning (POS, NER). Character-level models perform best in dependency parsing tasks that require an understanding of word relationships.<details>
<summary>Abstract</summary>
Choosing an appropriate tokenization scheme is often a bottleneck in low-resource cross-lingual transfer. To understand the downstream implications of text representation choices, we perform a comparative analysis on language models having diverse text representation modalities including 2 segmentation-based models (\texttt{BERT}, \texttt{mBERT}), 1 image-based model (\texttt{PIXEL}), and 1 character-level model (\texttt{CANINE}). First, we propose a scoring Language Quotient (LQ) metric capable of providing a weighted representation of both zero-shot and few-shot evaluation combined. Utilizing this metric, we perform experiments comprising 19 source languages and 133 target languages on three tasks (POS tagging, Dependency parsing, and NER). Our analysis reveals that image-based models excel in cross-lingual transfer when languages are closely related and share visually similar scripts. However, for tasks biased toward word meaning (POS, NER), segmentation-based models prove to be superior. Furthermore, in dependency parsing tasks where word relationships play a crucial role, models with their character-level focus, outperform others. Finally, we propose a recommendation scheme based on our findings to guide model selection according to task and language requirements.
</details>
<details>
<summary>摘要</summary>
选择合适的减少方案是跨语言转移中的一大瓶颈。为了理解文本表示方式选择的下游影响，我们进行了包括2个分 segmentation-based模型（BERT、mBERT）、1个图像基于模型（PIXEL）和1个字符级模型（CANINE）的比较分析。首先，我们提出了一个语言指数（LQ） metric，可以提供零shot和几shot评估的权重表示。使用这个 metric，我们进行了包括19种源语言和133种目标语言的三个任务（POS标签、依赖分析和NER）的实验。我们的分析发现，图像基于模型在语言相似度高和字形相似的语言间的跨语言转移中表现出色。然而，对于受word意义倾斜的任务（POS、NER），分 segmentation-based模型表现更出色。此外，在依赖分析任务中，字符级模型因其专注于字符级别的表示，而表现出了优异。最后，我们提出了根据我们的发现进行模型选择的建议方案，以便根据任务和语言要求进行指导。
</details></li>
</ul>
<hr>
<h2 id="Training-Generative-Question-Answering-on-Synthetic-Data-Obtained-from-an-Instruct-tuned-Model"><a href="#Training-Generative-Question-Answering-on-Synthetic-Data-Obtained-from-an-Instruct-tuned-Model" class="headerlink" title="Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model"></a>Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08072">http://arxiv.org/abs/2310.08072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kosuke Takahashi, Takahiro Omi, Kosuke Arima, Tatsuya Ishigaki</li>
<li>for: 这 paper 是为了开发一种可靠且Cost-effective的问答系统训练数据生成方法。</li>
<li>methods: 这 paper 使用一种名为 instruct-tuned 模型，通过自动生成问题和答案对来训练问答系统。</li>
<li>results: 实验结果表明，使用我们提议的合成数据可以达到与 manually 批注数据相同的性能水平，而无需人工成本。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This paper presents a simple and cost-effective method for synthesizing data to train question-answering systems. For training, fine-tuning GPT models is a common practice in resource-rich languages like English, however, it becomes challenging for non-English languages due to the scarcity of sufficient question-answer (QA) pairs. Existing approaches use question and answer generators trained on human-authored QA pairs, which involves substantial human expenses. In contrast, we use an instruct-tuned model to generate QA pairs in a zero-shot or few-shot manner. We conduct experiments to compare various strategies for obtaining QA pairs from the instruct-tuned model. The results demonstrate that a model trained on our proposed synthetic data achieves comparable performance to a model trained on manually curated datasets, without incurring human costs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Rethinking-Negative-Pairs-in-Code-Search"><a href="#Rethinking-Negative-Pairs-in-Code-Search" class="headerlink" title="Rethinking Negative Pairs in Code Search"></a>Rethinking Negative Pairs in Code Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08069">http://arxiv.org/abs/2310.08069</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Alex-HaochenLi/Soft-InfoNCE">https://github.com/Alex-HaochenLi/Soft-InfoNCE</a></li>
<li>paper_authors: Haochen Li, Xin Zhou, Luu Anh Tuan, Chunyan Miao</li>
<li>for: 提高代码搜索模型的软件开发效率和效果，通过对搜索查询返回的正例和负例进行对比学习。</li>
<li>methods: 提议使用Soft-InfoNCE损失函数，该损失函数在 InfoNCE 损失函数基础上增加了权重项来处理负例中的假阳性样本和不同负例之间的可能相互关系。</li>
<li>results: 经过广泛的实验，提出的 Soft-InfoNCE 损失函数和权重估计方法在现有的代码搜索模型中显示出了更高的效果和精度，并且可以更好地控制学习的代码表示分布。<details>
<summary>Abstract</summary>
Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less ``negative'' than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of negative pairs and show that the vanilla InfoNCE loss is a special case of Soft-InfoNCE. Theoretically, we analyze the effects of Soft-InfoNCE on controlling the distribution of learnt code representations and on deducing a more precise mutual information estimation. We furthermore discuss the superiority of proposed loss functions with other design alternatives. Extensive experiments demonstrate the effectiveness of Soft-InfoNCE and weights estimation methods under state-of-the-art code search models on a large-scale public dataset consisting of six programming languages. Source code is available at \url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploring-Large-Language-Models-for-Multi-Modal-Out-of-Distribution-Detection"><a href="#Exploring-Large-Language-Models-for-Multi-Modal-Out-of-Distribution-Detection" class="headerlink" title="Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection"></a>Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08027">http://arxiv.org/abs/2310.08027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Dai, Hao Lang, Kaisheng Zeng, Fei Huang, Yongbin Li</li>
<li>for:  This paper focuses on improving out-of-distribution (OOD) detection for reliable and trustworthy machine learning by leveraging world knowledge from large language models (LLMs).</li>
<li>methods: The proposed method uses a consistency-based uncertainty calibration approach to estimate the confidence score of each generation, and extracts visual objects from each image to fully capitalize on the world knowledge.</li>
<li>results: The proposed method consistently outperforms the state-of-the-art in OOD detection tasks, demonstrating its effectiveness in leveraging world knowledge for improved performance.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文目的是提高机器学习中的外围样本检测，以确保可靠和可信worthy的机器学习模型。</li>
<li>methods: 该方法使用一种兼容性基于的不确定性准备方法来估计每一代的信任度，并从每个图像中提取完整的视觉对象，以全面利用世界知识。</li>
<li>results: 该方法在OOD检测任务中 consistently outperform了现有的状态则，示出其在利用世界知识方面的效果。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistently outperforms the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
非常重要的 OUT-OF-DISTRIBUTION（OOD）检测是可靠和可信认的机器学习的一部分。现代多Modal OOD检测利用了内部分布（ID）类名的文本信息进行视觉OOD检测，但是它目前忽视了内部分布类的丰富 Contextual information。大型语言模型（LLMs）包含了大量的世界知识，可以通过提示来生成每个类的描述性特征。不经过选择性地使用这些知识会导致OOD检测的毁灭性损害，这可以通过我们的分析所观察到。在这篇论文中，我们提议通过选择性生成来增强OOD检测性能。具体来说，我们引入了一种归一化uncertainty calibration方法来估计每个生成的可信度。我们还EXTRACT visual object from each image，以便全面利用上述世界知识。我们的方法在EXTENSIVE EXPERIMENTS中经常超越了现状的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Large-Language-Models’-Empathetic-Response-Generation-Capabilities-for-Online-Mental-Health-Counselling-Support"><a href="#Harnessing-Large-Language-Models’-Empathetic-Response-Generation-Capabilities-for-Online-Mental-Health-Counselling-Support" class="headerlink" title="Harnessing Large Language Models’ Empathetic Response Generation Capabilities for Online Mental Health Counselling Support"></a>Harnessing Large Language Models’ Empathetic Response Generation Capabilities for Online Mental Health Counselling Support</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08017">http://arxiv.org/abs/2310.08017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Brandon Loh, Aravind Sesagiri Raamkumar</li>
<li>for: 本研究旨在探讨 LLM 是否能够生成同情响应，以满足心理健康护理的需求。</li>
<li>methods: 研究使用 five 种 LLM：GPT 版本 3.5 和版本 4，Vicuna FastChat-T5，PaLM 版本 2，以及 Falcon-7B-Instruct。通过简单的指令提示，这些模型对 EmpatheticDialogues 数据集中的词语进行回应。</li>
<li>results: 研究发现， LLMs 的回应比传统的回应生成对话系统和人类生成的回应更加同情。这些结果位于创造同情对话系统的创新进步中。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable performance across various information-seeking and reasoning tasks. These computational systems drive state-of-the-art dialogue systems, such as ChatGPT and Bard. They also carry substantial promise in meeting the growing demands of mental health care, albeit relatively unexplored. As such, this study sought to examine LLMs' capability to generate empathetic responses in conversations that emulate those in a mental health counselling setting. We selected five LLMs: version 3.5 and version 4 of the Generative Pre-training (GPT), Vicuna FastChat-T5, Pathways Language Model (PaLM) version 2, and Falcon-7B-Instruct. Based on a simple instructional prompt, these models responded to utterances derived from the EmpatheticDialogues (ED) dataset. Using three empathy-related metrics, we compared their responses to those from traditional response generation dialogue systems, which were fine-tuned on the ED dataset, along with human-generated responses. Notably, we discovered that responses from the LLMs were remarkably more empathetic in most scenarios. We position our findings in light of catapulting advancements in creating empathetic conversational systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Think-Act-and-Ask-Open-World-Interactive-Personalized-Robot-Navigation"><a href="#Think-Act-and-Ask-Open-World-Interactive-Personalized-Robot-Navigation" class="headerlink" title="Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation"></a>Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07968">http://arxiv.org/abs/2310.07968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinpei Dai, Run Peng, Sikai Li, Joyce Chai</li>
<li>for: 本研究旨在开发一种能够在未知环境中根据用户指令前往开放词汇对象的自适应智能代理人。</li>
<li>methods: 该研究提出了一种新的框架，称为Open-woRld Interactive persOnalized Navigation（ORION），该框架使用大语言模型（LLMs）来采取顺序决策，以控制不同模块的感知、导航和通信。</li>
<li>results: 实验结果表明，可以通过使用用户反馈来提高交互代理人的性能，但是在完成任务和导航导入交互中保持 equilibrio是一个挑战。此外，研究还发现了不同用户反馈形式对代理人性能的影响。<details>
<summary>Abstract</summary>
Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION), which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion and the efficiency of navigation and interaction remains challenging for all methods. We further provide more findings on the impact of diverse user feedback forms on the agents' performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>> Zero-Shot Object Navigation (ZSON) 允许代理人在未知环境中寻找开放词汇对象。现有的 ZSON 工作主要集中于遵循个人指令来找到通用对象类，忽视了自然语言互动和用户特定对象的复杂性。为解决这些局限性，我们引入 Zero-shot Interactive Personalized Object Navigation (ZIPON)， robots 需要在与用户交流的过程中前往个性化目标对象。为解决 ZIPON，我们提出一个新的框架，称为 Open-woRld Interactive persOnalized Navigation (ORION)，使用大语言模型 (LLM) 进行顺序决策，以控制不同模块的感知、导航和通信。实验结果表明，可以使用用户反馈来改进交互代理人的性能。然而，在任务完成和导航和交互的效率之间寻找良好的平衡仍然是一个挑战。我们还提供了更多关于不同用户反馈形式对代理人性能的影响的发现。
</details></li>
</ul>
<hr>
<h2 id="Clustering-of-Spell-Variations-for-Proper-Nouns-Transliterated-from-the-other-languages"><a href="#Clustering-of-Spell-Variations-for-Proper-Nouns-Transliterated-from-the-other-languages" class="headerlink" title="Clustering of Spell Variations for Proper Nouns Transliterated from the other languages"></a>Clustering of Spell Variations for Proper Nouns Transliterated from the other languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07962">http://arxiv.org/abs/2310.07962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prathamesh Pawar</li>
<li>for: 这篇论文是为了解决文本数据处理中的非统一性问题，即因为语言和方言的变化，导致翻译质量低下，从而使得NLP技术在处理文本数据时遇到的问题。</li>
<li>methods: 该论文提出了一种使用机器学习技术和数学相似性方程来归一化不同语言和方言中的专名的方法。具体来说，使用Affinity Propagation算法确定专名Token之间的相似性，并通过对Token-变化对的筛选而减少了专名的变体数量。</li>
<li>results: 该方法可以减少专名的变体数量，从而降低了人工注释的努力。这种应用可以大幅减少数据整理和格式化的人工努力。<details>
<summary>Abstract</summary>
One of the prominent problems with processing and operating on text data is the non uniformity of it. Due to the change in the dialects and languages, the caliber of translation is low. This creates a unique problem while using NLP in text data; which is the spell variation arising from the inconsistent translations and transliterations. This problem can also be further aggravated by the human error arising from the various ways to write a Proper Noun from an Indian language into its English equivalent. Translating proper nouns originating from Indian languages can be complicated as some proper nouns are also used as common nouns which might be taken literally. Applications of NLP that require addresses, names and other proper nouns face this problem frequently. We propose a method to cluster these spell variations for proper nouns using ML techniques and mathematical similarity equations. We aimed to use Affinity Propagation to determine relative similarity between the tokens. The results are augmented by filtering the token-variation pair by a similarity threshold. We were able to reduce the spell variations by a considerable amount. This application can significantly reduce the amount of human annotation efforts needed for data cleansing and formatting.
</details>
<details>
<summary>摘要</summary>
一个常见的文本处理和操作问题是文本不具有固定格式和标准，这导致翻译质量低下。这种问题在使用自然语言处理（NLP）时特别明显，其中一个问题是缺乏一致性的翻译和转写，从而导致的拼写差异。这种问题可以通过人类错误进一步加剧，特别是在印地语言中的专名译成英文的情况下。将印地语言中的专名翻译到英文中可以是复杂的，因为一些专名也可以作为通用名称使用，并且可能会被 Literal 解释。NLP 应用程序需要识别地址、名称和其他专名时，这种问题经常出现。我们提出了使用机器学习（ML）技术和数学相似性方程来归类拼写差异的方法。我们使用 Affinity Propagation 确定token之间的相似性，并将其Filter 为相似性阈值。我们成功地减少了拼写差异的数量。这种应用可以减少数据整理和格式化所需的人工注解工作量。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/12/cs.CL_2023_10_12/" data-id="clorjzl5100cpf1883cwjbpy9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/12/cs.LG_2023_10_12/" class="article-date">
  <time datetime="2023-10-12T10:00:00.000Z" itemprop="datePublished">2023-10-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/12/cs.LG_2023_10_12/">cs.LG - 2023-10-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="When-Machine-Learning-Models-Leak-An-Exploration-of-Synthetic-Training-Data"><a href="#When-Machine-Learning-Models-Leak-An-Exploration-of-Synthetic-Training-Data" class="headerlink" title="When Machine Learning Models Leak: An Exploration of Synthetic Training Data"></a>When Machine Learning Models Leak: An Exploration of Synthetic Training Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08775">http://arxiv.org/abs/2310.08775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manel Slokom, Peter-Paul de Wolf, Martha Larson</li>
<li>for: 这个研究旨在攻击一种用于预测个人或户口在下一两年内是否重新迁徙的机器学习模型，即潜在迁徙分类器。</li>
<li>methods: 攻击者可以访问模型并获取预测结果，并且假设公开训练数据的边缘分布。攻击者还假设他们已经获取了一些目标个体的非敏感属性的值。攻击者的目标是推断目标个体的敏感属性值。</li>
<li>results: 我们研究了在使用synthetic数据更新模型训练时，攻击者是否可以更successfully推断目标个体的敏感属性值。<details>
<summary>Abstract</summary>
We investigate an attack on a machine learning model that predicts whether a person or household will relocate in the next two years, i.e., a propensity-to-move classifier. The attack assumes that the attacker can query the model to obtain predictions and that the marginal distribution of the data on which the model was trained is publicly available. The attack also assumes that the attacker has obtained the values of non-sensitive attributes for a certain number of target individuals. The objective of the attack is to infer the values of sensitive attributes for these target individuals. We explore how replacing the original data with synthetic data when training the model impacts how successfully the attacker can infer sensitive attributes.\footnote{Original paper published at PSD 2022. The paper was subsequently updated.}
</details>
<details>
<summary>摘要</summary>
我们研究了一种攻击机器学习模型，该模型预测 individu 或家庭在下一两年内是否将移居，即潜在移居分类器。该攻击假设攻击者可以访问模型并获得预测结果，同时公共数据分布也可以公开。攻击者还假设他们已经获得了目标个体的非敏感特征值。攻击者的目标是推断目标个体的敏感特征值。我们研究了在训练模型时使用synthetic数据取代原始数据的影响，以及如何防止攻击者成功推断敏感特征值。Note: "敏感特征值" (sensitive attributes) in the text refers to information that is private and confidential, such as medical information or financial information.
</details></li>
</ul>
<hr>
<h2 id="PhyloGFN-Phylogenetic-inference-with-generative-flow-networks"><a href="#PhyloGFN-Phylogenetic-inference-with-generative-flow-networks" class="headerlink" title="PhyloGFN: Phylogenetic inference with generative flow networks"></a>PhyloGFN: Phylogenetic inference with generative flow networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08774">http://arxiv.org/abs/2310.08774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, Yoshua Bengio</li>
<li>For: The paper is written for researchers and practitioners in the field of phylogenetics, particularly those interested in computational methods for inferring evolutionary relationships.* Methods: The paper uses a framework called generative flow networks (GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. The proposed method, PhyloGFN, is an amortized posterior sampler that uses GFlowNets to explore and sample from the multimodal posterior distribution over tree topologies and evolutionary distances.* Results: The paper demonstrates that PhyloGFN produces diverse and high-quality evolutionary hypotheses on real benchmark datasets, and achieves a closer fit to the target distribution than state-of-the-art variational inference methods.Here’s the same information in Simplified Chinese text:* For: 本文是为phylogenetics研究人员和实践者所写的，尤其是关注计算方法来推断生物体之间的演化关系。* Methods: 本文使用generative flow networks（GFlowNets）解决phylogenetics中的两个核心问题：简洁基于和 bayesianphylogenetic inference。提议的方法是PhyloGFN，它是一种amortized posterior sampler，使用GFlowNets来探索和采样多模态 posterior distribution中的树构型和进化距离。* Results: 本文示出PhyloGFN在真实的benchmark数据上produces多样和高质量的演化假设，并超过现有variational inference方法的fit度。<details>
<summary>Abstract</summary>
Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the high complexity of tree space poses a significant obstacle for the current combinatorial and probabilistic techniques. In this paper, we adopt the framework of generative flow networks (GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling complex combinatorial structures, they are a natural choice for exploring and sampling from the multimodal posterior distribution over tree topologies and evolutionary distances. We demonstrate that our amortized posterior sampler, PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real benchmark datasets. PhyloGFN is competitive with prior works in marginal likelihood estimation and achieves a closer fit to the target distribution than state-of-the-art variational inference methods.
</details>
<details>
<summary>摘要</summary>
生物学计算分支——phylogenetics，研究生物体之间进化关系。尽管它们历史悠久，应用广泛，但从序列数据推导演化树仍然是一项挑战：高复杂的树空间对当前的组合学和概率方法 pose significant obstacles。在这篇论文中，我们采用泵流网络（GFlowNets）框架来解决演化树假设的两个核心问题：简便性基于的和 bayesian 演化树推导。由于GFlowNets 适合探索和采样复杂的组合结构，因此它们是探索和采样 posterior 分布中树 topologies 和演化距离的自然选择。我们示出了 PhyloGFN 可以生成多质量和多样性的进化假设，并且与优化 posterior 分布中的目标分布相似。在实际 benchmark 数据上，PhyloGFN 与 priors 在 marginal likelihood estimation 方面竞争，并且在 state-of-the-art variational inference methods 中实现了更加紧密的适应。
</details></li>
</ul>
<hr>
<h2 id="Modeling-Fission-Gas-Release-at-the-Mesoscale-using-Multiscale-DenseNet-Regression-with-Attention-Mechanism-and-Inception-Blocks"><a href="#Modeling-Fission-Gas-Release-at-the-Mesoscale-using-Multiscale-DenseNet-Regression-with-Attention-Mechanism-and-Inception-Blocks" class="headerlink" title="Modeling Fission Gas Release at the Mesoscale using Multiscale DenseNet Regression with Attention Mechanism and Inception Blocks"></a>Modeling Fission Gas Release at the Mesoscale using Multiscale DenseNet Regression with Attention Mechanism and Inception Blocks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08767">http://arxiv.org/abs/2310.08767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Toma, Md Ali Muntaha, Joel B. Harley, Michael R. Tonks</li>
<li>for: 研究微structure影响裂变气体释放 (FGR) 的mesoscale仿真方法，但这些方法具有计算成本高的问题。</li>
<li>methods: 本研究使用深度学习方法，使用2D核料微structure图像预测裂变气体释放流速。</li>
<li>results: 四种 convolutional neural network (CNN) 架构在 simulate FGR 数据上进行了训练和评估，其中最佳performing网络 combin CBAM 和 InceptionNet 机制，可以提供高精度（ mean absolute percentage error of 4.4%）、良好的训练稳定性和鲁棒性，特别是在 very low instantaneous FGR flux values 下。<details>
<summary>Abstract</summary>
Mesoscale simulations of fission gas release (FGR) in nuclear fuel provide a powerful tool for understanding how microstructure evolution impacts FGR, but they are computationally intensive. In this study, we present an alternate, data-driven approach, using deep learning to predict instantaneous FGR flux from 2D nuclear fuel microstructure images. Four convolutional neural network (CNN) architectures with multiscale regression are trained and evaluated on simulated FGR data generated using a hybrid phase field/cluster dynamics model. All four networks show high predictive power, with $R^{2}$ values above 98%. The best performing network combine a Convolutional Block Attention Module (CBAM) and InceptionNet mechanisms to provide superior accuracy (mean absolute percentage error of 4.4%), training stability, and robustness on very low instantaneous FGR flux values.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Mesoscale simulations of fission gas release (FGR) in nuclear fuel provide a powerful tool for understanding how microstructure evolution impacts FGR, but they are computationally intensive. In this study, we present an alternate, data-driven approach, using deep learning to predict instantaneous FGR flux from 2D nuclear fuel microstructure images. Four convolutional neural network (CNN) architectures with multiscale regression are trained and evaluated on simulated FGR data generated using a hybrid phase field/cluster dynamics model. All four networks show high predictive power, with $R^{2}$ values above 98%. The best performing network combine a Convolutional Block Attention Module (CBAM) and InceptionNet mechanisms to provide superior accuracy (mean absolute percentage error of 4.4%), training stability, and robustness on very low instantaneous FGR flux values." into Simplified Chinese.习惯的方式是使用深度学习来预测核燃料中的发生气体释放（FGR）实时流量，从2D核燃料微струк影像中提取特征。这篇研究使用了四种卷积神经网络架构，每个架构都包含多尺度调整。这些架构都显示了高预测力，$R^{2}$值高于98%。最佳performing网络是一个结合卷积块注意模组（CBAM）和inception Net机制的网络，具有最高的准确性（统计误差的平均相对误差为4.4%）、训练稳定性和对very low实时FGR流量的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Question-Answering-for-Electronic-Health-Records-A-Scoping-Review-of-datasets-and-models"><a href="#Question-Answering-for-Electronic-Health-Records-A-Scoping-Review-of-datasets-and-models" class="headerlink" title="Question Answering for Electronic Health Records: A Scoping Review of datasets and models"></a>Question Answering for Electronic Health Records: A Scoping Review of datasets and models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08759">http://arxiv.org/abs/2310.08759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayetri Bardhan, Kirk Roberts, Daisy Zhe Wang</li>
<li>For: This paper is focused on providing a methodological review of existing works on question answering (QA) over electronic health records (EHRs).* Methods: The authors searched four digital sources (Google Scholar, ACL Anthology, ACM Digital Library, and PubMed) to collect relevant publications on EHR QA, and identified 47 papers for further study. They found that most of the works are fairly recent and that emrQA is the most popular EHR QA dataset.* Results: The authors identified the different models used in EHR QA and the evaluation metrics used to assess these models. They also observed that QA on EHRs is a relatively new and unexplored area, and that there is a need for further research in this area.Here is the same information in Simplified Chinese text:* For: 这篇论文是关于电子医疗记录（EHR）上问答（QA）的方法学评估。* Methods: 作者通过四个数字源（Google Scholar、ACL Anthology、ACM Digital Library、PubMed）收集了相关的EHR QA论文，并从4111篇论文中选择了47篇进行进一步研究。他们发现大多数工作是非常新的，并且emrQA是EHR QA数据集中最受引用和使用的。* Results: 作者发现了EHR QA中不同的模型和评价指标，并评估了这些模型。他们还发现了EHR QA是一个相对新的和未经探索的领域，需要进一步的研究。<details>
<summary>Abstract</summary>
Question Answering (QA) systems on patient-related data can assist both clinicians and patients. They can, for example, assist clinicians in decision-making and enable patients to have a better understanding of their medical history. Significant amounts of patient data are stored in Electronic Health Records (EHRs), making EHR QA an important research area. In EHR QA, the answer is obtained from the medical record of the patient. Because of the differences in data format and modality, this differs greatly from other medical QA tasks that employ medical websites or scientific papers to retrieve answers, making it critical to research EHR question answering. This study aimed to provide a methodological review of existing works on QA over EHRs. We searched for articles from January 1st, 2005 to September 30th, 2023 in four digital sources including Google Scholar, ACL Anthology, ACM Digital Library, and PubMed to collect relevant publications on EHR QA. 4111 papers were identified for our study, and after screening based on our inclusion criteria, we obtained a total of 47 papers for further study. Out of the 47 papers, 25 papers were about EHR QA datasets, and 37 papers were about EHR QA models. It was observed that QA on EHRs is relatively new and unexplored. Most of the works are fairly recent. Also, it was observed that emrQA is by far the most popular EHR QA dataset, both in terms of citations and usage in other papers. Furthermore, we identified the different models used in EHR QA along with the evaluation metrics used for these models.
</details>
<details>
<summary>摘要</summary>
问答系统（QA）在患者相关数据上可以帮助临床医生和患者。它们可以帮助临床医生决策，并让患者更好地理解自己的医疗记录。大量患者数据被存储在电子医疗记录（EHR）中，因此EHR QA成为了重要的研究领域。在EHR QA中，答案来自患者的医疗记录。由于数据格式和模式的不同，这与其他医学问答任务不同，这使得研究EHR问答非常重要。本研究的目的是对现有的EHR QA工作进行方法学性评估。我们在2005年1月1日至2023年9月30日之间在Google学术、ACL Anthology、ACM数字图书馆和PubMed等四个数字源中搜索相关的文献，并从这些源中收集了4111篇文献。经过屏选 Based on our inclusion criteria，我们获得了47篇文献进行进一步研究。其中25篇文献关于EHR QA数据集，37篇文献关于EHR QA模型。我们发现，EHR QA相对较新和未探索。大多数工作是非常新的。此外，我们发现，emrQA是EHR QA数据集中最受欢迎的，同时也是其他文献中最多被引用和使用的。此外，我们还识别了EHR QA中使用的不同模型和评价指标。
</details></li>
</ul>
<hr>
<h2 id="Detection-and-prediction-of-clopidogrel-treatment-failures-using-longitudinal-structured-electronic-health-records"><a href="#Detection-and-prediction-of-clopidogrel-treatment-failures-using-longitudinal-structured-electronic-health-records" class="headerlink" title="Detection and prediction of clopidogrel treatment failures using longitudinal structured electronic health records"></a>Detection and prediction of clopidogrel treatment failures using longitudinal structured electronic health records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08757">http://arxiv.org/abs/2310.08757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Kim, In Gu Sean Lee, Mijeong Irene Ban, Jane Chiang</li>
<li>for: 这个研究旨在使用机器学习算法自动检测和预测lopidogrel治疗失败，以及将自然语言处理（NLP）技术应用于医疗记录（EHR）中。</li>
<li>methods: 我们使用了不同的机器学习算法，包括Transformer和时间序列模型，来建立模型来检测和预测lopidogrel治疗失败。</li>
<li>results: 我们从uk Biobank数据集中获得了502,527名病人中的1,824名治疗失败病例和6,859名控制病例。我们组织了每名病人的诊断、处方和手术记录，并将其分为同一天的访问。实验结果显示，时间序列模型在检测和预测任务中都能够超越bag-of-words方法。尤其是BERT模型，其在检测任务中可以达到0.928AUC，而在预测任务中可以达到0.729AUC。BERT模型还在缺乏训练数据时表现出色，因为它可以利用大量的预先训练数据。<details>
<summary>Abstract</summary>
We propose machine learning algorithms to automatically detect and predict clopidogrel treatment failure using longitudinal structured electronic health records (EHR). By drawing analogies between natural language and structured EHR, we introduce various machine learning algorithms used in natural language processing (NLP) applications to build models for treatment failure detection and prediction. In this regard, we generated a cohort of patients with clopidogrel prescriptions from UK Biobank and annotated if the patients had treatment failure events within one year of the first clopidogrel prescription; out of 502,527 patients, 1,824 patients were identified as treatment failure cases, and 6,859 patients were considered as control cases. From the dataset, we gathered diagnoses, prescriptions, and procedure records together per patient and organized them into visits with the same date to build models. The models were built for two different tasks, i.e., detection and prediction, and the experimental results showed that time series models outperform bag-of-words approaches in both tasks. In particular, a Transformer-based model, namely BERT, could reach 0.928 AUC in detection tasks and 0.729 AUC in prediction tasks. BERT also showed competence over other time series models when there is not enough training data, because it leverages the pre-training procedure using large unlabeled data.
</details>
<details>
<summary>摘要</summary>
我们提议使用机器学习算法自动检测和预测托剂治疗失败，使用长期结构化电子医疗记录（EHR）。我们通过对自然语言和结构化EHR之间的相似性进行Drawing analogies，引入了多种机器学习算法，用于建立治疗失败检测和预测模型。在这种情况下，我们从UK Biobank中提取了托剂订金的患者群体，并将其分为失败和控制两类。其中，1,824名患者被诊断为治疗失败 случа，6,859名患者被诊断为控制 caso。从数据集中，我们收集了诊断、订金和手术记录，并将其分组为同一个日期下的访问。然后，我们建立了两种不同任务的模型，即检测和预测模型，实验结果表明，时间序列模型在两个任务中都高于bag-of-words方法。特别是，一种基于Transformer的模型，即BERT，在检测任务中可达0.928AUC，在预测任务中可达0.729AUC。此外，BERT还在缺乏训练数据时表现出了优势，因为它可以利用大量未标注数据进行预处理。
</details></li>
</ul>
<hr>
<h2 id="Tokenizer-Choice-For-LLM-Training-Negligible-or-Crucial"><a href="#Tokenizer-Choice-For-LLM-Training-Negligible-or-Crucial" class="headerlink" title="Tokenizer Choice For LLM Training: Negligible or Crucial?"></a>Tokenizer Choice For LLM Training: Negligible or Crucial?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08754">http://arxiv.org/abs/2310.08754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max Lübbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper Schulze Buschhoff, Charvi Jain, Alexander Arno Weber, Lena Jurkschat, Hammam Abdelwahab, Chelsea John, Pedro Ortiz Suarez, Malte Ostendorff, Samuel Weinbach, Rafet Sifa, Stefan Kesselheim, Nicolas Flores-Herr</li>
<li>for: 这 paper 的目的是研究 tokenizer 对 LLM 的下游性能的影响，并提出了一些可能的解决方案。</li>
<li>methods: 作者使用了 24 个 mono-和多语言 LLM，在 2.6B 参数级别进行了训练，并对不同的 tokenizer 算法和参数进行了ablation 研究。</li>
<li>results: 研究发现，选择的 tokenizer 可以对 LLM 的下游性能产生显著的影响，同时 Training 和执行成本也受到了影响。特别是，通用的 tokenizer 评价指标 fertility 和 parity 并不总是预测模型的下游性能，因此这些指标可能是一个不可靠的代理。此外，作者发现，使用英语单语言 tokenizer 进行多语言 LLM 的训练会导致下游性能下降和额外的训练成本增加（最高达 68%），因为英语单语言 tokenizer 的词汇表大小不足。<details>
<summary>Abstract</summary>
The recent success of LLMs has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance, training and inference costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable proxy for the model's downstream performance. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary size increases of factor three in comparison to English. While English-only tokenizers have been applied to the training of multi-lingual LLMs, we find that this approach results in a severe downstream performance degradation and additional training costs of up to 68%, due to an inefficient tokenization vocabulary.
</details>
<details>
<summary>摘要</summary>
We trained 24 mono- and multilingual LLMs with 2.6 billion parameters and compared different tokenizer algorithms and parameterizations. Our findings reveal that the tokenizer choice can significantly affect the model's downstream performance, training and inference costs. In particular, we observed that commonly used tokenizer evaluation metrics, such as fertility and parity, are not always indicative of the model's downstream performance, making these metrics a questionable proxy for assessing the model's performance.Moreover, we discovered that multilingual tokenizers trained on the five most frequent European languages require a vocabulary size increase of a factor of three compared to English. While English-only tokenizers have been used for training multi-lingual LLMs, our results show that this approach leads to a significant downstream performance degradation and additional training costs of up to 68%, due to an inefficient tokenization vocabulary.
</details></li>
</ul>
<hr>
<h2 id="Search-Adaptor-Text-Embedding-Customization-for-Information-Retrieval"><a href="#Search-Adaptor-Text-Embedding-Customization-for-Information-Retrieval" class="headerlink" title="Search-Adaptor: Text Embedding Customization for Information Retrieval"></a>Search-Adaptor: Text Embedding Customization for Information Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08750">http://arxiv.org/abs/2310.08750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinsung Yoon, Sercan O Arik, Yanfei Chen, Tomas Pfister</li>
<li>for: 这篇论文旨在提高信息检索和搜索的效果，通过利用相关的查询-文档对应数据来改进大语言模型（LLM）的能力。</li>
<li>methods: 该论文提出了一种新的方法，即Search-Adaptor，可以快速和可靠地适应LLM进行信息检索。Search-Adaptor会修改原始的文本嵌入，并可以与任何LLM集成，包括通过API访问的LLM。</li>
<li>results: 在多个实际的英语和多语言检索数据集上，我们显示了Search-Adaptor的性能优势，例如在13个BEIR数据集上，与Google嵌入API相比，Search-Adaptor可以提高nDCG@10的平均提升超过5.2%。<details>
<summary>Abstract</summary>
Text embeddings extracted by pre-trained Large Language Models (LLMs) have significant potential to improve information retrieval and search. Beyond the zero-shot setup in which they are being conventionally used, being able to take advantage of the information from the relevant query-corpus paired data has the power to further boost the LLM capabilities. In this paper, we propose a novel method, Search-Adaptor, for customizing LLMs for information retrieval in an efficient and robust way. Search-Adaptor modifies the original text embedding generated by pre-trained LLMs, and can be integrated with any LLM, including those only available via APIs. On multiple real-world English and multilingual retrieval datasets, we show consistent and significant performance benefits for Search-Adaptor -- e.g., more than 5.2% improvements over the Google Embedding APIs in nDCG@10 averaged over 13 BEIR datasets.
</details>
<details>
<summary>摘要</summary>
文本嵌入EXTracted by pre-trained Large Language Models (LLMs) has significant potential to improve information retrieval and search. Beyond the zero-shot setup in which they are being conventionally used, being able to take advantage of the information from the relevant query-corpus paired data has the power to further boost the LLM capabilities. In this paper, we propose a novel method, Search-Adaptor, for customizing LLMs for information retrieval in an efficient and robust way. Search-Adaptor modifies the original text embedding generated by pre-trained LLMs, and can be integrated with any LLM, including those only available via APIs. On multiple real-world English and multilingual retrieval datasets, we show consistent and significant performance benefits for Search-Adaptor -- e.g., more than 5.2% improvements over the Google Embedding APIs in nDCG@10 averaged over 13 BEIR datasets.
</details></li>
</ul>
<hr>
<h2 id="Evolutionary-Dynamic-Optimization-and-Machine-Learning"><a href="#Evolutionary-Dynamic-Optimization-and-Machine-Learning" class="headerlink" title="Evolutionary Dynamic Optimization and Machine Learning"></a>Evolutionary Dynamic Optimization and Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08748">http://arxiv.org/abs/2310.08748</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Abdennour Boulesnane</li>
<li>for: 本研究旨在探讨EVOLUTIONARY DYNAMIC OPTIMIZATION (EDO)和 MACHINE LEARNING (ML)的相互 интеграción，以便在ML任务中实现动态优化。</li>
<li>methods: 本研究使用了EVOLUTIONARY ALGORITHMS (EA)和ML ALGORITHMS (MLA)的相互 интеграción，以便在不同的ML任务中实现更好的优化。</li>
<li>results: 研究发现，通过在EA和MLA之间的相互 интеграción，可以在ML任务中实现更好的动态优化，并且可以提高模型的性能。<details>
<summary>Abstract</summary>
Evolutionary Computation (EC) has emerged as a powerful field of Artificial Intelligence, inspired by nature's mechanisms of gradual development. However, EC approaches often face challenges such as stagnation, diversity loss, computational complexity, population initialization, and premature convergence. To overcome these limitations, researchers have integrated learning algorithms with evolutionary techniques. This integration harnesses the valuable data generated by EC algorithms during iterative searches, providing insights into the search space and population dynamics. Similarly, the relationship between evolutionary algorithms and Machine Learning (ML) is reciprocal, as EC methods offer exceptional opportunities for optimizing complex ML tasks characterized by noisy, inaccurate, and dynamic objective functions. These hybrid techniques, known as Evolutionary Machine Learning (EML), have been applied at various stages of the ML process. EC techniques play a vital role in tasks such as data balancing, feature selection, and model training optimization. Moreover, ML tasks often require dynamic optimization, for which Evolutionary Dynamic Optimization (EDO) is valuable. This paper presents the first comprehensive exploration of reciprocal integration between EDO and ML. The study aims to stimulate interest in the evolutionary learning community and inspire innovative contributions in this domain.
</details>
<details>
<summary>摘要</summary>
适应进化 Computation (EC) 已经成为人工智能中的一个强大领域， Drawing inspiration from nature's gradual development mechanisms. However, EC approaches often face challenges such as stagnation, diversity loss, computational complexity, population initialization, and premature convergence. To overcome these limitations, researchers have integrated learning algorithms with evolutionary techniques. This integration harnesses the valuable data generated by EC algorithms during iterative searches, providing insights into the search space and population dynamics. Similarly, the relationship between evolutionary algorithms and Machine Learning (ML) is reciprocal, as EC methods offer exceptional opportunities for optimizing complex ML tasks characterized by noisy, inaccurate, and dynamic objective functions. These hybrid techniques, known as Evolutionary Machine Learning (EML), have been applied at various stages of the ML process. EC techniques play a vital role in tasks such as data balancing, feature selection, and model training optimization. Moreover, ML tasks often require dynamic optimization, for which Evolutionary Dynamic Optimization (EDO) is valuable. This paper presents the first comprehensive exploration of reciprocal integration between EDO and ML. The study aims to stimulate interest in the evolutionary learning community and inspire innovative contributions in this domain.Translated into Simplified Chinese:适应进化计算 (EC) 已经成为人工智能中的一个强大领域，灵感自然的慢慢发展机制。然而，EC方法经常面临困难，如停滞、多样性损失、计算复杂性、人口初始化和快速 converges。为了突破这些局限性，研究人员已经将学习算法与进化技术相结合。这种结合使得EC算法在迭代搜索中生成的有价值数据，提供搜索空间和人口动态的启示。同时，机器学习 (ML) 和 EC 之间的关系是相互的，EC方法在复杂的 ML 任务中提供了优秀的优化机会。这些混合技术，称为进化机器学习 (EML)，在 ML 过程中的不同阶段应用。EC 技术在数据均衡、特征选择和模型训练优化等任务中扮演着重要的角色。此外， ML 任务经常需要动态优化，为了这些任务，进化动态优化 (EDO) 是非常有价值的。这篇文章提供了首次对 EDO 和 ML 之间的相互 интеграции的全面探讨。这项研究的目标是在进化学习社区中启发兴趣，激发创新的贡献。
</details></li>
</ul>
<hr>
<h2 id="Robustness-to-Multi-Modal-Environment-Uncertainty-in-MARL-using-Curriculum-Learning"><a href="#Robustness-to-Multi-Modal-Environment-Uncertainty-in-MARL-using-Curriculum-Learning" class="headerlink" title="Robustness to Multi-Modal Environment Uncertainty in MARL using Curriculum Learning"></a>Robustness to Multi-Modal Environment Uncertainty in MARL using Curriculum Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08746">http://arxiv.org/abs/2310.08746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aakriti05/Robust-Multimodal-MARL">https://github.com/Aakriti05/Robust-Multimodal-MARL</a></li>
<li>paper_authors: Aakriti Agrawal, Rohith Aralikatti, Yanchao Sun, Furong Huang</li>
<li>for: 这篇论文的目的是解决多智能体强化学习中环境不确定性的问题。</li>
<li>methods: 本文提出了一种基于课程学习技术的通用响应方法来面对多种环境不确定性。</li>
<li>results: 本文的实验结果显示，这种方法可以在多智能体强化学习环境中提高稳定性，并在协力和竞争环境中都达到了现有最佳性能。<details>
<summary>Abstract</summary>
Multi-agent reinforcement learning (MARL) plays a pivotal role in tackling real-world challenges. However, the seamless transition of trained policies from simulations to real-world requires it to be robust to various environmental uncertainties. Existing works focus on finding Nash Equilibrium or the optimal policy under uncertainty in one environment variable (i.e. action, state or reward). This is because a multi-agent system itself is highly complex and unstationary. However, in real-world situation uncertainty can occur in multiple environment variables simultaneously. This work is the first to formulate the generalised problem of robustness to multi-modal environment uncertainty in MARL. To this end, we propose a general robust training approach for multi-modal uncertainty based on curriculum learning techniques. We handle two distinct environmental uncertainty simultaneously and present extensive results across both cooperative and competitive MARL environments, demonstrating that our approach achieves state-of-the-art levels of robustness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Splicing-Up-Your-Predictions-with-RNA-Contrastive-Learning"><a href="#Splicing-Up-Your-Predictions-with-RNA-Contrastive-Learning" class="headerlink" title="Splicing Up Your Predictions with RNA Contrastive Learning"></a>Splicing Up Your Predictions with RNA Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08738">http://arxiv.org/abs/2310.08738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phil-fradkin/contrastive_rna">https://github.com/phil-fradkin/contrastive_rna</a></li>
<li>paper_authors: Philip Fradkin, Ruian Shi, Bo Wang, Brendan Frey, Leo J. Lee</li>
<li>for: 该研究旨在填补 genomic 数据的理解 gap，通过自动学习法在其他领域所示出的可能性。</li>
<li>methods: 该研究使用了对比学习技术，利用功能相似性 междуalternative splicing和基因复制生成的序列。</li>
<li>results: 研究证明了通过这种策略可以学习通用RNAisoform表示，并在下游任务中达到竞争性的结果，包括RNA半衰期和平均核酶荷载预测。<details>
<summary>Abstract</summary>
In the face of rapidly accumulating genomic data, our understanding of the RNA regulatory code remains incomplete. Recent self-supervised methods in other domains have demonstrated the ability to learn rules underlying the data-generating process such as sentence structure in language. Inspired by this, we extend contrastive learning techniques to genomic data by utilizing functional similarities between sequences generated through alternative splicing and gene duplication. Our novel dataset and contrastive objective enable the learning of generalized RNA isoform representations. We validate their utility on downstream tasks such as RNA half-life and mean ribosome load prediction. Our pre-training strategy yields competitive results using linear probing on both tasks, along with up to a two-fold increase in Pearson correlation in low-data conditions. Importantly, our exploration of the learned latent space reveals that our contrastive objective yields semantically meaningful representations, underscoring its potential as a valuable initialization technique for RNA property prediction.
</details>
<details>
<summary>摘要</summary>
在大量基因数据面前，我们对树落语言中的RNA规则 Code仍然不够完善。其他领域的自动学习方法已经表明了数据生成过程中的规则，例如语言 sentence 结构。受这种启发，我们扩展了对 genomic 数据的冲突学习技术，利用功能相似性 между通过alternative splicing和 gene duplication 生成的序列。我们的新数据集和对比目标可以学习通用 RNA isoform 表示。我们验证了它们在下游任务中的有用性，包括 RNA 半衰期和平均ribosome 荷载预测。我们的预训练策略在线性检测中获得了竞争力和在低数据情况下的一两倍增加的皮尔逊相关性。进一步探索学习的latent空间表明，我们的对比目标实际上得到了Semantically meaningful的表示，这 highlights its potential as a valuable initialization technique for RNA property prediction。
</details></li>
</ul>
<hr>
<h2 id="Provably-Robust-Cost-Sensitive-Learning-via-Randomized-Smoothing"><a href="#Provably-Robust-Cost-Sensitive-Learning-via-Randomized-Smoothing" class="headerlink" title="Provably Robust Cost-Sensitive Learning via Randomized Smoothing"></a>Provably Robust Cost-Sensitive Learning via Randomized Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08732">http://arxiv.org/abs/2310.08732</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/trustmlrg/cs-rs">https://github.com/trustmlrg/cs-rs</a></li>
<li>paper_authors: Yuan Xin, Michael Backes, Xiao Zhang</li>
<li>for: 本研究旨在开发一种可靠的鲁棒性证明框架，以适应成本敏感的情景下学习抵抗攻击的分类器。</li>
<li>methods: 本研究使用随机抑制的灵活矩阵来证明鲁棒性，并为不同的数据子集设计细化的证明范围优化策略。</li>
<li>results: 实验结果表明， compared to existing methods, 本方法可以在成本敏感的情景下实现显著改善的鲁棒性证明性，而无需增加训练时间或计算复杂度。<details>
<summary>Abstract</summary>
We focus on learning adversarially robust classifiers under a cost-sensitive scenario, where the potential harm of different classwise adversarial transformations is encoded in a binary cost matrix. Existing methods are either empirical that cannot certify robustness or suffer from inherent scalability issues. In this work, we study whether randomized smoothing, a more scalable robustness certification framework, can be leveraged to certify cost-sensitive robustness. Built upon a notion of cost-sensitive certified radius, we show how to adapt the standard randomized smoothing certification pipeline to produce tight robustness guarantees for any cost matrix. In addition, with fine-grained certified radius optimization schemes specifically designed for different data subgroups, we propose an algorithm to train smoothed classifiers that are optimized for cost-sensitive robustness. Extensive experiments on image benchmarks and a real-world medical dataset demonstrate the superiority of our method in achieving significantly improved performance of certified cost-sensitive robustness while having a negligible impact on overall accuracy.
</details>
<details>
<summary>摘要</summary>
我们关注在一个成本敏感的情况下学习对抗性robust classifier，其中不同类型的对抗性变换的潜在危害是通过二进制成本矩阵编码。现有方法有两种缺点：一是经验的，无法证明Robustness；二是存在内在的扩展性问题。在这个工作中，我们研究了randomized smoothing是否可以用来证明成本敏感Robustness。基于成本敏感证明半径，我们展示了如何适应任何成本矩阵，并提出了一种特定数据 subgroup fine-grained证明半径优化方案来训练优化成本敏感Robustness的滑动类ifier。实验表明，我们的方法可以在图像benchmark和一个真实的医疗数据集上实现显著提高证明成本敏感Robustness的性能，而无需对总准确率造成重要影响。
</details></li>
</ul>
<hr>
<h2 id="Heterophily-Based-Graph-Neural-Network-for-Imbalanced-Classification"><a href="#Heterophily-Based-Graph-Neural-Network-for-Imbalanced-Classification" class="headerlink" title="Heterophily-Based Graph Neural Network for Imbalanced Classification"></a>Heterophily-Based Graph Neural Network for Imbalanced Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08725">http://arxiv.org/abs/2310.08725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zirui Liang, Yuntao Li, Tianjin Huang, Akrati Saxena, Yulong Pei, Mykola Pechenizkiy</li>
<li>for:  Addressing class imbalance in graph-related problems, particularly in node classification tasks.</li>
<li>methods:  Proposes a unique approach that considers graph heterophily to tackle imbalanced classification, integrating an imbalance classification strategy with heterophily-aware GNNs to improve performance and efficiency.</li>
<li>results:  Demonstrates superiority in classification performance and efficiency compared to existing baselines through experiments on real-world graphs.<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have shown promise in addressing graph-related problems, including node classification. However, conventional GNNs assume an even distribution of data across classes, which is often not the case in real-world scenarios, where certain classes are severely underrepresented. This leads to suboptimal performance of standard GNNs on imbalanced graphs. In this paper, we introduce a unique approach that tackles imbalanced classification on graphs by considering graph heterophily. We investigate the intricate relationship between class imbalance and graph heterophily, revealing that minority classes not only exhibit a scarcity of samples but also manifest lower levels of homophily, facilitating the propagation of erroneous information among neighboring nodes. Drawing upon this insight, we propose an efficient method, called Fast Im-GBK, which integrates an imbalance classification strategy with heterophily-aware GNNs to effectively address the class imbalance problem while significantly reducing training time. Our experiments on real-world graphs demonstrate our model's superiority in classification performance and efficiency for node classification tasks compared to existing baselines.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 已经显示了解决图像问题的抢势，包括节点分类。然而， conventioal GNNs 假设图像上的数据具有均匀分布，而实际场景中的类别经常受到不均匀的影响，导致标准 GNNs 在不均匀图像上表现下降。在这篇论文中，我们介绍了一种独特的方法，该方法解决了不均匀分类问题，通过考虑图像的异质。我们发现了少数类不仅具有样本稀缺，而且也表现出较低的同类关系度，从而使得邻居节点之间的误差信息更易传播。基于这一点，我们提出了一种高效的方法，即 Fast Im-GBK，该方法结合了不均匀分类策略和异质意识 GNNs，以有效地解决不均匀分类问题，同时显著降低训练时间。我们在实际图像上进行了Node classification任务的实验，并证明了我们的模型在性能和效率方面与现有的基elines相比较优。
</details></li>
</ul>
<hr>
<h2 id="Designing-Observables-for-Measurements-with-Deep-Learning"><a href="#Designing-Observables-for-Measurements-with-Deep-Learning" class="headerlink" title="Designing Observables for Measurements with Deep Learning"></a>Designing Observables for Measurements with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08717">http://arxiv.org/abs/2310.08717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/owen234/designer-obs-paper">https://github.com/owen234/designer-obs-paper</a></li>
<li>paper_authors: Owen Long, Benjamin Nachman</li>
<li>for: 这篇论文的目的是用机器学习设计优化的观察量来推导基础物理模型的参数。</li>
<li>methods: 这篇论文使用机器学习来设计优化的观察量，并使用神经网络输出不平衡的凝聚section来包含最多有关参数的信息。</li>
<li>results: 作者使用两种物理模型进行深层受体散射的包含测量，并通过比较传统的physics intuition和机器学习设计的观察量来证明机器学习设计的优势。<details>
<summary>Abstract</summary>
Many analyses in particle and nuclear physics use simulations to infer fundamental, effective, or phenomenological parameters of the underlying physics models. When the inference is performed with unfolded cross sections, the observables are designed using physics intuition and heuristics. We propose to design optimal observables with machine learning. Unfolded, differential cross sections in a neural network output contain the most information about parameters of interest and can be well-measured by construction. We demonstrate this idea using two physics models for inclusive measurements in deep inelastic scattering.
</details>
<details>
<summary>摘要</summary>
很多分析在粒子和核物理中使用模拟来推导基础、有效或现象学 Parameters of interest. When the inference is performed with unfolded cross sections, the observables are designed using physics intuition and heuristics. We propose to design optimal observables with machine learning. Unfolded, differential cross sections in a neural network output contain the most information about parameters of interest and can be well-measured by construction. We demonstrate this idea using two physics models for inclusive measurements in deep inelastic scattering.Here's the word-for-word translation of the text into Simplified Chinese:很多分析在粒子和核物理中使用模拟来推导基础、有效或现象学参数。当推导使用 unfolded 跨sections 时，观察器是通过物理直觉和规则来设计的。我们提议使用机器学习设计优化观察器。 unfolded，差分跨sections 在神经网络输出中含有最多关于参数的信息，并可以通过构建好地测量。我们使用 two 物理模型 для inclusive 测量在深刻射撃中示例。
</details></li>
</ul>
<hr>
<h2 id="Waymax-An-Accelerated-Data-Driven-Simulator-for-Large-Scale-Autonomous-Driving-Research"><a href="#Waymax-An-Accelerated-Data-Driven-Simulator-for-Large-Scale-Autonomous-Driving-Research" class="headerlink" title="Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research"></a>Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08710">http://arxiv.org/abs/2310.08710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cole Gulino, Justin Fu, Wenjie Luo, George Tucker, Eli Bronstein, Yiren Lu, Jean Harb, Xinlei Pan, Yan Wang, Xiangyu Chen, John D. Co-Reyes, Rishabh Agarwal, Rebecca Roelofs, Yao Lu, Nico Montali, Paul Mougin, Zoey Yang, Brandyn White, Aleksandra Faust, Rowan McAllister, Dragomir Anguelov, Benjamin Sapp</li>
<li>for: 本研究旨在开发一个可靠、成本效果的 autonomous driving  simulate 工具，以便在安全和可靠的环境中进行大规模的测试和训练。</li>
<li>methods: 本研究使用了公共发布的实际驾驶数据（如 Waymo Open Motion Dataset）来初始化或播放多个自适应者的 simulated 场景。它利用硬件加速器（如 TPUs&#x2F;GPUs）进行加速，支持在图像上进行训练，适用于现代大规模分布式机器学习工作流程。</li>
<li>results: 本研究通过对各种启发式学习和奖励学习算法进行比较，以及不同设计决策的ablation 研究，证明了路径作为规划代理的有效性，以及RL 可能受到 simulated 代理的过拟合。<details>
<summary>Abstract</summary>
Simulation is an essential tool to develop and benchmark autonomous vehicle planning software in a safe and cost-effective manner. However, realistic simulation requires accurate modeling of nuanced and complex multi-agent interactive behaviors. To address these challenges, we introduce Waymax, a new data-driven simulator for autonomous driving in multi-agent scenes, designed for large-scale simulation and testing. Waymax uses publicly-released, real-world driving data (e.g., the Waymo Open Motion Dataset) to initialize or play back a diverse set of multi-agent simulated scenarios. It runs entirely on hardware accelerators such as TPUs/GPUs and supports in-graph simulation for training, making it suitable for modern large-scale, distributed machine learning workflows. To support online training and evaluation, Waymax includes several learned and hard-coded behavior models that allow for realistic interaction within simulation. To supplement Waymax, we benchmark a suite of popular imitation and reinforcement learning algorithms with ablation studies on different design decisions, where we highlight the effectiveness of routes as guidance for planning agents and the ability of RL to overfit against simulated agents.
</details>
<details>
<summary>摘要</summary>
模拟是自动驾驶车辆规划软件的开发和测试的重要工具，可以在安全和经济的方式下进行模拟。然而，实际的模拟需要准确地模拟复杂多代理交互行为。为解决这些挑战，我们介绍了 Waymax，一个新的数据驱动的自动驾驶多代理场景模拟器，适用于大规模的模拟和测试。Waymax 使用公共发布的实际驾驶数据（例如 Waymo 开放运动数据集）来初始化或播放多种多代理模拟enario。它完全采用硬件加速器 such as TPUs/GPUs 运行，适合现代大规模分布式机器学习工作流程。为支持在线培训和评估，Waymax 包含了一些学习和硬编码的行为模型， allowing for realistic interaction within simulation。为了补充 Waymax，我们对一些流行的仿效学习和奖励学习算法进行了ablation study，其中我们强调路径作为规划代理的导航和RL 可以做到对模拟代理的适应。
</details></li>
</ul>
<hr>
<h2 id="Polynomial-Time-Cryptanalytic-Extraction-of-Neural-Network-Models"><a href="#Polynomial-Time-Cryptanalytic-Extraction-of-Neural-Network-Models" class="headerlink" title="Polynomial Time Cryptanalytic Extraction of Neural Network Models"></a>Polynomial Time Cryptanalytic Extraction of Neural Network Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08708">http://arxiv.org/abs/2310.08708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adi Shamir, Isaac Canales-Martinez, Anna Hambitzer, Jorge Chavez-Saab, Francisco Rodrigez-Henriquez, Nitin Satpute</li>
<li>for: 本研究旨在提高现有的攻击方法，以EXTRACT ALL PARAMETERS OF RELU-based deep neural networks WITH ARBITRARILY HIGH PRECISION AND POLYNOMIAL COMPLEXITY。</li>
<li>methods: 本研究使用了新的技术，包括一种新的搜索算法和一种新的筛选算法，以提高攻击效率和精度。</li>
<li>results: 研究发现，使用新的技术可以在30分钟内EXTRACT ALL PARAMETERS OF A FULL-SIZED NEURAL NETWORK FOR CLASSIFYING CIFAR10 DATASET，这个任务原来需要了2^256的搜索空间。<details>
<summary>Abstract</summary>
Billions of dollars and countless GPU hours are currently spent on training Deep Neural Networks (DNNs) for a variety of tasks. Thus, it is essential to determine the difficulty of extracting all the parameters of such neural networks when given access to their black-box implementations. Many versions of this problem have been studied over the last 30 years, and the best current attack on ReLU-based deep neural networks was presented at Crypto 2020 by Carlini, Jagielski, and Mironov. It resembles a differential chosen plaintext attack on a cryptosystem, which has a secret key embedded in its black-box implementation and requires a polynomial number of queries but an exponential amount of time (as a function of the number of neurons). In this paper, we improve this attack by developing several new techniques that enable us to extract with arbitrarily high precision all the real-valued parameters of a ReLU-based DNN using a polynomial number of queries and a polynomial amount of time. We demonstrate its practical efficiency by applying it to a full-sized neural network for classifying the CIFAR10 dataset, which has 3072 inputs, 8 hidden layers with 256 neurons each, and over million neuronal parameters. An attack following the approach by Carlini et al. requires an exhaustive search over 2 to the power 256 possibilities. Our attack replaces this with our new techniques, which require only 30 minutes on a 256-core computer.
</details>
<details>
<summary>摘要</summary>
估计 billions of dollars 和 countless GPU 小时是为了训练深度神经网络（DNN）而被花费。因此， Determining the difficulty of extracting all the parameters of such neural networks when given access to their black-box implementations is essential. Over the last 30 years, many versions of this problem have been studied, and the best current attack on ReLU-based deep neural networks was presented at Crypto 2020 by Carlini, Jagielski, and Mironov. This attack resembles a differential chosen plaintext attack on a cryptosystem, which has a secret key embedded in its black-box implementation and requires a polynomial number of queries but an exponential amount of time (as a function of the number of neurons).在这篇论文中，我们提高了这种攻击，发展了several new techniques，使得我们可以使用一个 полиномиаль数量的查询和一个 полиномиаль时间来EXTRACT WITH ARBITRARILY HIGH PRECISION 所有实数参数 OF A ReLU-based DNN。我们示出了这个攻击的实践效率，通过应用它到一个用于分类 CIFAR10 数据集的全大小神经网络，该神经网络有 3072 个输入、8 个隐藏层，每个隐藏层有 256 个神经元，以及数百万个神经元。一个如Carlini et al. 所提出的攻击需要枚举 2 的 256 个可能性。我们的攻击则使用我们新提出的技术，只需要 30 分钟的时间在 256 核心计算机上完成。
</details></li>
</ul>
<hr>
<h2 id="Eliciting-Model-Steering-Interactions-from-Users-via-Data-and-Visual-Design-Probes"><a href="#Eliciting-Model-Steering-Interactions-from-Users-via-Data-and-Visual-Design-Probes" class="headerlink" title="Eliciting Model Steering Interactions from Users via Data and Visual Design Probes"></a>Eliciting Model Steering Interactions from Users via Data and Visual Design Probes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09314">http://arxiv.org/abs/2310.09314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anamaria Crisan, Maddie Shang, Eric Brochu</li>
<li>for: 该研究旨在探讨采用自动化数据科学工具的领域专家如何使用语义交互来更新简单的分类模型。</li>
<li>methods: 该研究使用数据和视觉设计探索法来检查专家们在不同程度的机器学习知识下如何使用语义交互来更新机器学习模型。</li>
<li>results: 研究发现，许多Semantic interactions的目标不直接映射到机器学习模型参数，而是增强训练数据集。研究还发现参与者们有几种不同的需求，这些需求与机器学习专家的不同程度相关。此外，参与者们也发现使用语义交互可以帮助团队成员之间合作工作，特别是对于没有机器学习背景的成员。<details>
<summary>Abstract</summary>
Domain experts increasingly use automated data science tools to incorporate machine learning (ML) models in their work but struggle to "debug" these models when they are incorrect. For these experts, semantic interactions can provide an accessible avenue to guide and refine ML models without having to programmatically dive into its technical details. In this research, we conduct an elicitation study using data and visual design probes to examine if and how experts with a spectrum of ML expertise use semantic interactions to update a simple classification model. We use our design probes to facilitate an interactive dialogue with 20 participants and codify their interactions as a set of target-interaction pairs. Interestingly, our findings revealed that many targets of semantic interactions do not directly map to ML model parameters, but instead aim to augment the data a model uses for training. We also identify reasons that participants would hesitate to interact with ML models, including burdens of cognitive load and concerns of injecting bias. Unexpectedly participants also saw the value of using semantic interactions to work collaboratively with members of their team. Participants with less ML expertise found this to be a useful mechanism for communicating their concerns to ML experts. This was an especially important observation, as our study also shows the different needs that correspond to diverse ML expertise. Collectively, we demonstrate that design probes are effective tools for proactively gathering the affordances that should be offered in an interactive machine learning system.
</details>
<details>
<summary>摘要</summary>
域专业人员 increasingly 使用自动化数 science工具来 incorporate 机器学习（ML）模型到他们的工作中，但是在模型错误时很难“调试”这些模型。为这些专业人员，语义互动可以提供一条可 accessible 的通道，以便指导并细化 ML 模型，不需要深入了解技术细节。在这项研究中，我们通过数据和视觉设计探索来评估专业人员spectrum 的 ML 技能水平上是否使用语义互动来更新一个简单的分类模型。我们使用我们的设计探索来促进参与者和20名参与者之间的交互对话，并将其 codified 为一组目标互动对。我们发现了许多语义互动目标并不直接映射到 ML 模型参数，而是用于增强训练数据集。我们还发现了参与者与 ML 模型交互时的一些障碍，包括认知负担和担心插入偏见。但是，参与者也看到了使用语义互动工作协作团队成员的价值。参与者具有较低的 ML 专业知识水平发现这是一种有用的机制，用于与 ML 专家进行沟通他们的关注。这是特别重要的，因为我们的研究也显示了不同的 ML 专业知识水平对应的不同需求。总的来说，我们示示了设计探索是可以有效地收集互动机器学习系统的可用性的工具。
</details></li>
</ul>
<hr>
<h2 id="Kernel-Elastic-Autoencoder-for-Molecular-Design"><a href="#Kernel-Elastic-Autoencoder-for-Molecular-Design" class="headerlink" title="Kernel-Elastic Autoencoder for Molecular Design"></a>Kernel-Elastic Autoencoder for Molecular Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08685">http://arxiv.org/abs/2310.08685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haote Li, Yu Shee, Brandon Allen, Federica Maschietto, Victor Batista</li>
<li>for: 这篇论文是为了描述一种基于 transformer 架构的自然语言生成模型，即 Kernel-Elastic Autoencoder (KAE)，以及 KAE 在分子设计方面的表现。</li>
<li>methods: KAE 使用了两个新的损失函数：修改后最大平均差和权重重建。这两个损失函数使得 KAE 可以同时实现有效的生成和准确的重建。</li>
<li>results: KAE 在独立测试集上实现了很好的多样性，同时保持了near-perfect的重建性。此外，KAE 还可以进行 conditional 生成和基于搜索的解码，从而实现了 state-of-the-art 的性能在受限优化中。最后，KAE 还可以根据偏好的粘合能力进行 Conditional 生成，并且在 AutoDock Vina 和 Glide 分数上超过了所有从训练集中的候选者。<details>
<summary>Abstract</summary>
We introduce the Kernel-Elastic Autoencoder (KAE), a self-supervised generative model based on the transformer architecture with enhanced performance for molecular design. KAE is formulated based on two novel loss functions: modified maximum mean discrepancy and weighted reconstruction. KAE addresses the long-standing challenge of achieving valid generation and accurate reconstruction at the same time. KAE achieves remarkable diversity in molecule generation while maintaining near-perfect reconstructions on the independent testing dataset, surpassing previous molecule-generating models. KAE enables conditional generation and allows for decoding based on beam search resulting in state-of-the-art performance in constrained optimizations. Furthermore, KAE can generate molecules conditional to favorable binding affinities in docking applications as confirmed by AutoDock Vina and Glide scores, outperforming all existing candidates from the training dataset. Beyond molecular design, we anticipate KAE could be applied to solve problems by generation in a wide range of applications.
</details>
<details>
<summary>摘要</summary>
我们介绍了核心弹性自适应器（KAE），一种基于传播架构造的无监督生成模型，具有优化的表现力 для分子设计。KAE 是基于两个新的损失函数：修改后最大平均差异和负载重合成。KAE 解决了实现有效生成和精准重建的问题，同时维持了独立测试集上的近乎完美重建。KAE 可以实现 conditional generation 和基于排序搜寻的解oding，实现了顶尖的性能在受限制的优化中。此外，KAE 可以根据偏好的缘Points generates molecules with favorable binding affinities in docking applications, as confirmed by AutoDock Vina and Glide scores, outperforming all existing candidates from the training dataset. 在分子设计之外，我们预期 KAE 可以应用到各种生成问题中。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Who-to-Nudge-Causal-vs-Predictive-Targeting-in-a-Field-Experiment-on-Student-Financial-Aid-Renewal"><a href="#Machine-Learning-Who-to-Nudge-Causal-vs-Predictive-Targeting-in-a-Field-Experiment-on-Student-Financial-Aid-Renewal" class="headerlink" title="Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal"></a>Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08672">http://arxiv.org/abs/2310.08672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Susan Athey, Niall Keleher, Jann Spiess</li>
<li>for: 该研究目的是分析在大规模实验中对学生进行”激励”以提高学生在过期前续续金融援助申请的效果。</li>
<li>methods: 该研究使用了” causal forest” 来估计各个学生对待治疗的不同效果，然后根据这些估计分配学生到待治疗组和控制组中。然后评估了两种不同的目标策略，一种是targeting学生的低预测结果，另一种是targeting学生的高预测结果。</li>
<li>results: 研究发现，targeting中间预测结果最有效，而targeting低预测结果实际上是有害的。在一年的实验中，对所有学生进行激励提高了37%的学生早期申请率的平均值6.4个百分点，并估计使用我们选择的策略可以达到约75%的这个效果。<details>
<summary>Abstract</summary>
In many settings, interventions may be more effective for some individuals than others, so that targeting interventions may be beneficial. We analyze the value of targeting in the context of a large-scale field experiment with over 53,000 college students, where the goal was to use "nudges" to encourage students to renew their financial-aid applications before a non-binding deadline. We begin with baseline approaches to targeting. First, we target based on a causal forest that estimates heterogeneous treatment effects and then assigns students to treatment according to those estimated to have the highest treatment effects. Next, we evaluate two alternative targeting policies, one targeting students with low predicted probability of renewing financial aid in the absence of the treatment, the other targeting those with high probability. The predicted baseline outcome is not the ideal criterion for targeting, nor is it a priori clear whether to prioritize low, high, or intermediate predicted probability. Nonetheless, targeting on low baseline outcomes is common in practice, for example because the relationship between individual characteristics and treatment effects is often difficult or impossible to estimate with historical data. We propose hybrid approaches that incorporate the strengths of both predictive approaches (accurate estimation) and causal approaches (correct criterion); we show that targeting intermediate baseline outcomes is most effective, while targeting based on low baseline outcomes is detrimental. In one year of the experiment, nudging all students improved early filing by an average of 6.4 percentage points over a baseline average of 37% filing, and we estimate that targeting half of the students using our preferred policy attains around 75% of this benefit.
</details>
<details>
<summary>摘要</summary>
在许多场景下，干预可能对某些个人更有效，因此定向干预可能有利。我们在一个大规模的场景实验中分析了定向的价值，该实验包括53,000名大学生，并使用“拐弯”来吸引学生在不紧迫的截止日期前续写金融援助申请。我们开始于基本的定向方法。首先，我们使用 causal forest 来估计各个人的不同待遇效果，然后将学生按照这些估计的最高待遇效果进行干预。接着，我们评估了两种替代的定向策略，一种targeting学生的低预测报道续写金融援助的可能性，另一种targeting学生的高预测报道续写金融援助的可能性。理想的基准结果不是定向的理想准则，也没有先前确定是否优先级低、高或中等预测报道。然而，定向低基准结果很普遍，例如因为对个人特征和干预效果的关系 often difficult or impossible to estimate with historical data。我们提议的混合方法可以结合predictive approach的准确估计和causal approach的正确准则，我们发现定向中间基准结果最有效，而定向低基准结果是不利的。在一年的实验中，对所有学生进行拐弯提高了37%的早期申请率的平均值6.4个百分点，我们估计使用我们的首选策略定向一半的学生可以达到约75%的这个效果。
</details></li>
</ul>
<hr>
<h2 id="Every-Parameter-Matters-Ensuring-the-Convergence-of-Federated-Learning-with-Dynamic-Heterogeneous-Models-Reduction"><a href="#Every-Parameter-Matters-Ensuring-the-Convergence-of-Federated-Learning-with-Dynamic-Heterogeneous-Models-Reduction" class="headerlink" title="Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction"></a>Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08670">http://arxiv.org/abs/2310.08670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanhan Zhou, Tian Lan, Guru Venkataramani, Wenbo Ding</li>
<li>for: This paper focuses on addressing the challenges of cross-device Federated Learning (FL) by developing a unifying framework for heterogeneous FL algorithms with online model extraction, and providing a general convergence analysis for the first time.</li>
<li>methods: The paper proposes a holistic approach that considers both model reduction noise and minimum coverage index to enhance the efficiency of heterogeneous FL.</li>
<li>results: The authors prove that under certain sufficient conditions, the proposed algorithms converge to a stationary point of standard FL for general smooth cost functions, both for IID and non-IID data.<details>
<summary>Abstract</summary>
Cross-device Federated Learning (FL) faces significant challenges where low-end clients that could potentially make unique contributions are excluded from training large models due to their resource bottlenecks. Recent research efforts have focused on model-heterogeneous FL, by extracting reduced-size models from the global model and applying them to local clients accordingly. Despite the empirical success, general theoretical guarantees of convergence on this method remain an open question. This paper presents a unifying framework for heterogeneous FL algorithms with online model extraction and provides a general convergence analysis for the first time. In particular, we prove that under certain sufficient conditions and for both IID and non-IID data, these algorithms converge to a stationary point of standard FL for general smooth cost functions. Moreover, we introduce the concept of minimum coverage index, together with model reduction noise, which will determine the convergence of heterogeneous federated learning, and therefore we advocate for a holistic approach that considers both factors to enhance the efficiency of heterogeneous federated learning.
</details>
<details>
<summary>摘要</summary>
跨设备联合学习（FL）面临着严重的挑战，low-end客户端因资源瓶颈而无法参与大型模型的训练，这些客户端具有唯一的贡献 potential。 recent research efforts have focused on model-heterogeneous FL, by extracting reduced-size models from the global model and applying them to local clients accordingly. Despite the empirical success, general theoretical guarantees of convergence on this method remain an open question. This paper presents a unifying framework for heterogeneous FL algorithms with online model extraction and provides a general convergence analysis for the first time. In particular, we prove that under certain sufficient conditions and for both IID and non-IID data, these algorithms converge to a stationary point of standard FL for general smooth cost functions. Moreover, we introduce the concept of minimum coverage index, together with model reduction noise, which will determine the convergence of heterogeneous federated learning, and therefore we advocate for a holistic approach that considers both factors to enhance the efficiency of heterogeneous federated learning.Here's the translation in Traditional Chinese:跨设备联合学习（FL）面临着严重的挑战，low-end客户端因为资源瓶颈而无法参与大型模型的训练，这些客户端具有唯一的贡献 potential。 recent research efforts have focused on model-heterogeneous FL, by extracting reduced-size models from the global model and applying them to local clients accordingly. Despite the empirical success, general theoretical guarantees of convergence on this method remain an open question. This paper presents a unifying framework for heterogeneous FL algorithms with online model extraction and provides a general convergence analysis for the first time. In particular, we prove that under certain sufficient conditions and for both IID and non-IID data, these algorithms converge to a stationary point of standard FL for general smooth cost functions. Moreover, we introduce the concept of minimum coverage index, together with model reduction noise, which will determine the convergence of heterogeneous federated learning, and therefore we advocate for a holistic approach that considers both factors to enhance the efficiency of heterogeneous federated learning.
</details></li>
</ul>
<hr>
<h2 id="Counting-and-Algorithmic-Generalization-with-Transformers"><a href="#Counting-and-Algorithmic-Generalization-with-Transformers" class="headerlink" title="Counting and Algorithmic Generalization with Transformers"></a>Counting and Algorithmic Generalization with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08661">http://arxiv.org/abs/2310.08661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simonouellette35/countingwithtransformers">https://github.com/simonouellette35/countingwithtransformers</a></li>
<li>paper_authors: Simon Ouellette, Rolf Pfister, Hansueli Jud</li>
<li>for: 该论文旨在研究机器学习算法的泛化能力，即能够学习数据生成算法的下发性。</li>
<li>methods: 该论文使用了标准的Transformers架构，并进行了一些修改来改善其对异常数据的性能。</li>
<li>results: 该论文显示了一种使用了修改后的Transformers架构可以在计数任务上实现良好的泛化能力，而且使用的是非常轻量级的架构。<details>
<summary>Abstract</summary>
Algorithmic generalization in machine learning refers to the ability to learn the underlying algorithm that generates data in a way that generalizes out-of-distribution. This is generally considered a difficult task for most machine learning algorithms. Here, we analyze algorithmic generalization when counting is required, either implicitly or explicitly. We show that standard Transformers are based on architectural decisions that hinder out-of-distribution performance for such tasks. In particular, we discuss the consequences of using layer normalization and of normalizing the attention weights via softmax. With ablation of the problematic operations, we demonstrate that a modified transformer can exhibit a good algorithmic generalization performance on counting while using a very lightweight architecture.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SplitBeam-Effective-and-Efficient-Beamforming-in-Wi-Fi-Networks-Through-Split-Computing"><a href="#SplitBeam-Effective-and-Efficient-Beamforming-in-Wi-Fi-Networks-Through-Split-Computing" class="headerlink" title="SplitBeam: Effective and Efficient Beamforming in Wi-Fi Networks Through Split Computing"></a>SplitBeam: Effective and Efficient Beamforming in Wi-Fi Networks Through Split Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08656">http://arxiv.org/abs/2310.08656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yoshitomo-matsubara/split-beam">https://github.com/yoshitomo-matsubara/split-beam</a></li>
<li>paper_authors: Niloofar Bahadori, Yoshitomo Matsubara, Marco Levorato, Francesco Restuccia<br>for: 提高 Wi-Fi 网络吞吐量methods: 使用分解深度神经网络 (SplitBeam) 直接生成扫描矩阵 (BM)，并解决瓶颈优化问题 (BOP)results: 比标准 IEEE 802.11 算法和 LB-SciFi state-of-the-art DNN-based approach 减少扫描矩阵返回大小和计算复杂度，保持 bit error rate (BER) 在 about 10^-3 以下，并实现FPGA硬件实现以下结果。<details>
<summary>Abstract</summary>
Modern IEEE 802.11 (Wi-Fi) networks extensively rely on multiple-input multiple-output (MIMO) to significantly improve throughput. To correctly beamform MIMO transmissions, the access point needs to frequently acquire a beamforming matrix (BM) from each connected station. However, the size of the matrix grows with the number of antennas and subcarriers, resulting in an increasing amount of airtime overhead and computational load at the station. Conventional approaches come with either excessive computational load or loss of beamforming precision. For this reason, we propose SplitBeam, a new framework where we train a split deep neural network (DNN) to directly output the BM given the channel state information (CSI) matrix as input. We formulate and solve a bottleneck optimization problem (BOP) to keep computation, airtime overhead, and bit error rate (BER) below application requirements. We perform extensive experimental CSI collection with off-the-shelf Wi-Fi devices in two distinct environments and compare the performance of SplitBeam with the standard IEEE 802.11 algorithm for BM feedback and the state-of-the-art DNN-based approach LB-SciFi. Our experimental results show that SplitBeam reduces the beamforming feedback size and computational complexity by respectively up to 81% and 84% while maintaining BER within about 10^-3 of existing approaches. We also implement the SplitBeam DNNs on FPGA hardware to estimate the end-to-end BM reporting delay, and show that the latter is less than 10 milliseconds in the most complex scenario, which is the target channel sounding frequency in realistic multi-user MIMO scenarios.
</details>
<details>
<summary>摘要</summary>
现代IEEE 802.11（Wi-Fi）网络广泛采用多输入多输出（MIMO）技术，以大幅提高吞吐量。为正确扫扫MIMO传输，接入点需要从每个连接的站点得到扫扫矩阵（BM）频繁。然而，矩阵的大小与天线和子频点数量成正比，导致了在站点处的空中时间开销和计算负担的增加。传统方法会导致过高的计算负担或扫扫矩阵精度的损失。为此，我们提出了SplitBeam，一个新的框架，其中我们使用分解深度神经网络（DNN）直接输出BM， given the channel state information（CSI）矩阵作为输入。我们解决了瓶颈优化问题（BOP），以保证计算、空中时间开销和比特错误率（BER）都在应用要求之下。我们进行了广泛的实验CSI收集，使用现成的Wi-Fi设备，在两种不同的环境中进行了测试，并与IEEE 802.11标准Feedback算法和LB-SciFi状态的艺术隐藏法相比较。我们的实验结果表明，SplitBeam可以将扫扫矩阵反馈大小和计算复杂性分别减少到81%和84%，保持BER在约10^-3的范围内。我们还将SplitBeam DNN硬件实现在FPGA上，并估算了终端到终端BM报告延迟，发现其在最复杂的场景下不超过10毫秒，这与实际多用户MIMO场景中的目标频率匹配。
</details></li>
</ul>
<hr>
<h2 id="Time-vectorized-numerical-integration-for-systems-of-ODEs"><a href="#Time-vectorized-numerical-integration-for-systems-of-ODEs" class="headerlink" title="Time-vectorized numerical integration for systems of ODEs"></a>Time-vectorized numerical integration for systems of ODEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08649">http://arxiv.org/abs/2310.08649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark C. Messner, Tianchen Hu, Tianju Chen</li>
<li>for: 该文章描述了一种高效、隐式、矩阵化的方法，用于解决科学问题中的紧张系统ordinary differential equations（ODEs）和稀缺训练数据。</li>
<li>methods: 该方法使用隐式法则和矩阵化技术，将问题vector化在独立时间序列和批处理时间步骤之间，从而提高计算设备的带宽。</li>
<li>results: 该方法可以实现 greater than 100x的速度提升， compared to标准、顺序时间integation方法，并且可以完全利用现代GPU的性能。文章还提供了一些示例问题，来说明方法的优势。<details>
<summary>Abstract</summary>
Stiff systems of ordinary differential equations (ODEs) and sparse training data are common in scientific problems. This paper describes efficient, implicit, vectorized methods for integrating stiff systems of ordinary differential equations through time and calculating parameter gradients with the adjoint method. The main innovation is to vectorize the problem both over the number of independent times series and over a batch or "chunk" of sequential time steps, effectively vectorizing the assembly of the implicit system of ODEs. The block-bidiagonal structure of the linearized implicit system for the backward Euler method allows for further vectorization using parallel cyclic reduction (PCR). Vectorizing over both axes of the input data provides a higher bandwidth of calculations to the computing device, allowing even problems with comparatively sparse data to fully utilize modern GPUs and achieving speed ups of greater than 100x, compared to standard, sequential time integration. We demonstrate the advantages of implicit, vectorized time integration with several example problems, drawn from both analytical stiff and non-stiff ODE models as well as neural ODE models. We also describe and provide a freely available open-source implementation of the methods developed here.
</details>
<details>
<summary>摘要</summary>
常见的科学问题中有刚性系统（ODE）和稀疏的训练数据。这篇论文描述了高效、隐式、矩阵化方法，用于在时间上集成刚性系统的常微分方程，并计算参数偏导数量方法。主要创新在于将问题矩阵化在独立时间序列上，以及在批处理（chunk）或连续时间步上。这使得将问题矩阵化在输入数据上，从而提高计算设备的带宽，使得 Even problems with relatively sparse data can fully utilize modern GPUs, achieving speedups of greater than 100x compared to standard, sequential time integration. We demonstrate the advantages of implicit, vectorized time integration with several example problems, drawn from both analytical stiff and non-stiff ODE models as well as neural ODE models. We also describe and provide a freely available open-source implementation of the methods developed here.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Bucks-for-Buckets-B4B-Active-Defenses-Against-Stealing-Encoders"><a href="#Bucks-for-Buckets-B4B-Active-Defenses-Against-Stealing-Encoders" class="headerlink" title="Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders"></a>Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08571">http://arxiv.org/abs/2310.08571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Dubiński, Stanisław Pawlak, Franziska Boenisch, Tomasz Trzciński, Adam Dziedzic</li>
<li>For: 本研究旨在保护Machine Learning as a Service（MLaaS）APIs中的高性能编码器，防止对编码器的模型盗取攻击。* Methods: 本研究提出了一种名为“Bucks for Buckets”（B4B）的活动防御策略，利用了对 adversary 的观察和适应，以防止盗取编码器的功能。* Results: B4B 能够防止盗取编码器的功能，同时不会影响合法用户的表单质量。此外，B4B 还可以防止 adaptive adversaries 通过创建多个帐户（sybils）来绕过防御。<details>
<summary>Abstract</summary>
Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose Bucks for Buckets (B4B), the first active defense that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task.vB4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding space. To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user's representations. This prevents the adversary from directly aggregating representations over multiple accounts to create their stolen encoder copy. Our active defense opens a new path towards securely sharing and democratizing encoders over public APIs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Stronger-Coreset-Bounds-for-Kernel-Density-Estimators-via-Chaining"><a href="#Stronger-Coreset-Bounds-for-Kernel-Density-Estimators-via-Chaining" class="headerlink" title="Stronger Coreset Bounds for Kernel Density Estimators via Chaining"></a>Stronger Coreset Bounds for Kernel Density Estimators via Chaining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08548">http://arxiv.org/abs/2310.08548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rainie Bozzai, Thomas Rothvoss</li>
<li>for: 这个论文的目的是提供一种改进了核函数核心集的复杂性的方法，以便在广泛的kernel函数中实现随机多阶时间算法。</li>
<li>methods: 这个论文使用了不同的方法，包括误差方法和链接方法，以便实现核函数核心集的生成。</li>
<li>results: 这个论文的结果是提供了一种可随机多阶时间算法来生成核函数核心集，其大小为$O\left(\frac{\sqrt{d}{\varepsilon}\sqrt{\log\log \frac{1}{\varepsilon}\right)$，其中$d$是数据集的维度，$\varepsilon$是误差的大小。此外，这个论文还提供了一种可随机多阶时间算法来生成核函数核心集，其大小为$O\left(\frac{1}{\varepsilon}\sqrt{\log\log \frac{1}{\varepsilon}\right)$，但是只适用于 laplacian kernel。最后，这个论文还提供了对 exponential、Hellinger 和 JS kernel 的最佳known bounds，即 $O\left(\frac{\sqrt{d}{\varepsilon}\sqrt{\log(2\max{1,\alpha})}\right)$。<details>
<summary>Abstract</summary>
We apply the discrepancy method and a chaining approach to give improved bounds on the coreset complexity of a wide class of kernel functions. Our results give randomized polynomial time algorithms to produce coresets of size $O\big(\frac{\sqrt{d}{\varepsilon}\sqrt{\log\log \frac{1}{\varepsilon}\big)$ for the Gaussian and Laplacian kernels in the case that the data set is uniformly bounded, an improvement that was not possible with previous techniques. We also obtain coresets of size $O\big(\frac{1}{\varepsilon}\sqrt{\log\log \frac{1}{\varepsilon}\big)$ for the Laplacian kernel for $d$ constant. Finally, we give the best known bounds of $O\big(\frac{\sqrt{d}{\varepsilon}\sqrt{\log(2\max\{1,\alpha\})}\big)$ on the coreset complexity of the exponential, Hellinger, and JS Kernels, where $1/\alpha$ is the bandwidth parameter of the kernel.
</details>
<details>
<summary>摘要</summary>
我们使用差异方法和链接方法来提供增强的核心复杂度下界，并且给出随机算法来生成核心集的大小为$O\left(\frac{\sqrt{d}{\varepsilon}\sqrt{\log\log \frac{1}{\varepsilon}\right)$的 Gaussian 和 Laplacian 核函数的情况下，其中数据集是均匀分布的。此外，我们还得到了 Laplacian 核函数的核心集大小为$O\left(\frac{1}{\varepsilon}\sqrt{\log\log \frac{1}{\varepsilon}\right)$的情况。最后，我们给出了最好的下界为$O\left(\frac{\sqrt{d}{\varepsilon}\sqrt{\log(2\max\{1,\alpha\})}\right)$的核心复杂度，其中 $\frac{1}{\alpha}$ 是核函数的宽度参数。
</details></li>
</ul>
<hr>
<h2 id="Divorce-Prediction-with-Machine-Learning-Insights-and-LIME-Interpretability"><a href="#Divorce-Prediction-with-Machine-Learning-Insights-and-LIME-Interpretability" class="headerlink" title="Divorce Prediction with Machine Learning: Insights and LIME Interpretability"></a>Divorce Prediction with Machine Learning: Insights and LIME Interpretability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08620">http://arxiv.org/abs/2310.08620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Manjurul Ahsan</li>
<li>For: The paper aims to predict whether a couple will divorce or not using machine learning algorithms and interpretability techniques.* Methods: The authors use six machine learning algorithms (Logistic Regression, Linear Discriminant Analysis, K-Nearest Neighbors, Classification and Regression Trees, Gaussian Na&quot;ive Bayes, and Support Vector Machines) to classify married and divorced individuals based on a dataset. They also use Local Interpretable Model-Agnostic Explanations (LIME) to provide interpretable results.* Results: The authors achieve an accuracy of 98.57% in predicting divorce using the SVM, KNN, and LDA algorithms. They also use LIME to explain the prediction probabilities and identify the most important features that differentiate divorced and married couples. Additionally, they develop a divorce predictor app that considers the ten most important features to help couples make decisions about their relationship.<details>
<summary>Abstract</summary>
Divorce is one of the most common social issues in developed countries like in the United States. Almost 50% of the recent marriages turn into an involuntary divorce or separation. While it is evident that people vary to a different extent, and even over time, an incident like Divorce does not interrupt the individual's daily activities; still, Divorce has a severe effect on the individual's mental health, and personal life. Within the scope of this research, the divorce prediction was carried out by evaluating a dataset named by the 'divorce predictor dataset' to correctly classify between married and Divorce people using six different machine learning algorithms- Logistic Regression (LR), Linear Discriminant Analysis (LDA), K-Nearest Neighbors (KNN), Classification and Regression Trees (CART), Gaussian Na\"ive Bayes (NB), and, Support Vector Machines (SVM). Preliminary computational results show that algorithms such as SVM, KNN, and LDA, can perform that task with an accuracy of 98.57%. This work's additional novel contribution is the detailed and comprehensive explanation of prediction probabilities using Local Interpretable Model-Agnostic Explanations (LIME). Utilizing LIME to analyze test results illustrates the possibility of differentiating between divorced and married couples. Finally, we have developed a divorce predictor app considering ten most important features that potentially affect couples in making decisions in their divorce, such tools can be used by any one in order to identify their relationship condition.
</details>
<details>
<summary>摘要</summary>
各种社会问题中，离婚是最常见的一种，美国和其他发达国家的现代社会中约50%的新婚夫妻会经历不可避免的离婚或分居。虽然人们在不同程度和时间上有所不同，但离婚仍然会对个人的心理健康和生活产生严重的影响。在这个研究中，我们使用了名为“离婚预测数据集”的数据集，使用六种机器学习算法（LR、LDA、KNN、CART、NB和SVM）对已婚和离婚人进行分类，并实现了准确率达98.57%。此外，我们还提供了详细的解释，使用本地可解释性模型无关性（LIME）来分析测试结果，并证明了可以在已婚和离婚夫妻之间进行区分。最后，我们开发了一款离婚预测应用，考虑了十大最重要的因素，这些因素可能影响夫妻在离婚决策中的选择。这些工具可以由任何人使用，以了解他们的关系状况。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-climate-pathways-using-feature-importance-on-echo-state-networks"><a href="#Characterizing-climate-pathways-using-feature-importance-on-echo-state-networks" class="headerlink" title="Characterizing climate pathways using feature importance on echo state networks"></a>Characterizing climate pathways using feature importance on echo state networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08495">http://arxiv.org/abs/2310.08495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katherine Goode, Daniel Ries, Kellie McClernon</li>
<li>for: 这份研究旨在探讨使用echostate网络（ESN）来 caracterize气候路径（climate pathway）的变量关系。</li>
<li>methods: 研究使用了echostate网络（ESN）来模拟气候数据，并开发了一种基于特征重要性的方法来评估变量关系。</li>
<li>results: 研究发现，使用echostate网络（ESN）可以准确地捕捉气候变量之间的关系，并且可以用特征重要性来评估这些关系的重要性。<details>
<summary>Abstract</summary>
The 2022 National Defense Strategy of the United States listed climate change as a serious threat to national security. Climate intervention methods, such as stratospheric aerosol injection, have been proposed as mitigation strategies, but the downstream effects of such actions on a complex climate system are not well understood. The development of algorithmic techniques for quantifying relationships between source and impact variables related to a climate event (i.e., a climate pathway) would help inform policy decisions. Data-driven deep learning models have become powerful tools for modeling highly nonlinear relationships and may provide a route to characterize climate variable relationships. In this paper, we explore the use of an echo state network (ESN) for characterizing climate pathways. ESNs are a computationally efficient neural network variation designed for temporal data, and recent work proposes ESNs as a useful tool for forecasting spatio-temporal climate data. Like other neural networks, ESNs are non-interpretable black-box models, which poses a hurdle for understanding variable relationships. We address this issue by developing feature importance methods for ESNs in the context of spatio-temporal data to quantify variable relationships captured by the model. We conduct a simulation study to assess and compare the feature importance techniques, and we demonstrate the approach on reanalysis climate data. In the climate application, we select a time period that includes the 1991 volcanic eruption of Mount Pinatubo. This event was a significant stratospheric aerosol injection, which we use as a proxy for an artificial stratospheric aerosol injection. Using the proposed approach, we are able to characterize relationships between pathway variables associated with this event.
</details>
<details>
<summary>摘要</summary>
美国2022年国防策略列出了气候变化为国家安全的严重威胁。气候干预方法，如大气粉末注射，已经被提出来 mitigation 策略，但气候系统下流效果不很清楚。开发算法技术来衡量气候事件（即气候路径）变量之间的关系可以帮助决策。数据驱动的深度学习模型已成为模拟非线性关系的强大工具，可能提供了 caracterize 气候变量关系的方法。在这篇文章中，我们探讨使用echostate network（ESN）来 caracterize 气候路径。ESN是用于时间数据的计算效率高的神经网络变体，而最近的研究还提出了使用ESN来预测空间时间气候数据。just like other neural networks, ESNs are non-interpretable black-box models, which poses a hurdle for understanding variable relationships。我们解决这个问题 by developing feature importance methods for ESNs in the context of spatio-temporal data to quantify variable relationships captured by the model。我们在模拟研究中评估和比较特征重要性技术，并在气候数据上进行了应用。在气候应用中，我们选择了1991年Mount Pinatubo火山喷发事件作为人工大气粉末注射的代表。使用我们的方法，我们能够 caracterize 事件相关变量之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Strategies-and-impact-of-learning-curve-estimation-for-CNN-based-image-classification"><a href="#Strategies-and-impact-of-learning-curve-estimation-for-CNN-based-image-classification" class="headerlink" title="Strategies and impact of learning curve estimation for CNN-based image classification"></a>Strategies and impact of learning curve estimation for CNN-based image classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08470">http://arxiv.org/abs/2310.08470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura Didyk, Brayden Yarish, Michael A. Beck, Christopher P. Bidinosti, Christopher J. Henry</li>
<li>for: 这篇论文主要用于提出一些简化启发式学习模型训练时间的策略，以便更好地优化模型性能。</li>
<li>methods: 这篇论文使用了一些不同的采样策略来估计学习曲线，并评估了这些策略在模拟学习曲线和实际图像分类任务中的性能。</li>
<li>results: 根据实验结果，提出的采样策略可以减少模型训练时间，同时仍能准确地估计学习曲线。<details>
<summary>Abstract</summary>
Learning curves are a measure for how the performance of machine learning models improves given a certain volume of training data. Over a wide variety of applications and models it was observed that learning curves follow -- to a large extent -- a power law behavior. This makes the performance of different models for a given task somewhat predictable and opens the opportunity to reduce the training time for practitioners, who are exploring the space of possible models and hyperparameters for the problem at hand. By estimating the learning curve of a model from training on small subsets of data only the best models need to be considered for training on the full dataset. How to choose subset sizes and how often to sample models on these to obtain estimates is however not researched. Given that the goal is to reduce overall training time strategies are needed that sample the performance in a time-efficient way and yet leads to accurate learning curve estimates. In this paper we formulate the framework for these strategies and propose several strategies. Further we evaluate the strategies for simulated learning curves and in experiments with popular datasets and models for image classification tasks.
</details>
<details>
<summary>摘要</summary>
学习曲线是机器学习模型的性能改进度量，随着训练数据量的增加而变化。在各种应用和模型中，学习曲线大多数情况下遵循几何规律行为。这使得不同任务的模型性能之间有一定的预测性，从而为实际应用提供了减少训练时间的机会。通过在小样本数据上训练模型来估算学习曲线，只需考虑最佳的模型进行全数据集训练。然而，选择样本大小和如何采样模型以获得准确的学习曲线估计并不是研究的焦点。为了减少总训练时间，需要采用时效的采样策略，同时能够准确地估计学习曲线。在这篇论文中，我们提出了这些策略的框架，并提出了一些策略。此外，我们还对模拟学习曲线和实际数据集和模型进行了实验评估。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Non-convex-Learning-for-Multi-layer-Neural-Networks"><a href="#Differentially-Private-Non-convex-Learning-for-Multi-layer-Neural-Networks" class="headerlink" title="Differentially Private Non-convex Learning for Multi-layer Neural Networks"></a>Differentially Private Non-convex Learning for Multi-layer Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08425">http://arxiv.org/abs/2310.08425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanpu Shen, Cheng-Long Wang, Zihang Xiang, Yiming Ying, Di Wang</li>
<li>for: 本研究探讨了多层fully connected神经网络中的差分隐私权限化优化问题。</li>
<li>methods: 我们提出了一些算法，并进行了分析，证明可以实现数据维度不受影响的过分布风险。</li>
<li>results: 我们的研究表明，在一些特定的情况下，DP-SGD可以提供优化的过分布风险保证。<details>
<summary>Abstract</summary>
This paper focuses on the problem of Differentially Private Stochastic Optimization for (multi-layer) fully connected neural networks with a single output node. In the first part, we examine cases with no hidden nodes, specifically focusing on Generalized Linear Models (GLMs). We investigate the well-specific model where the random noise possesses a zero mean, and the link function is both bounded and Lipschitz continuous. We propose several algorithms and our analysis demonstrates the feasibility of achieving an excess population risk that remains invariant to the data dimension. We also delve into the scenario involving the ReLU link function, and our findings mirror those of the bounded link function. We conclude this section by contrasting well-specified and misspecified models, using ReLU regression as a representative example.   In the second part of the paper, we extend our ideas to two-layer neural networks with sigmoid or ReLU activation functions in the well-specified model. In the third part, we study the theoretical guarantees of DP-SGD in Abadi et al. (2016) for fully connected multi-layer neural networks. By utilizing recent advances in Neural Tangent Kernel theory, we provide the first excess population risk when both the sample size and the width of the network are sufficiently large. Additionally, we discuss the role of some parameters in DP-SGD regarding their utility, both theoretically and empirically.
</details>
<details>
<summary>摘要</summary>
In the second part of the paper, we extend our ideas to two-layer neural networks with sigmoid or ReLU activation functions in the well-specified model. In the third part, we study the theoretical guarantees of DP-SGD in Abadi et al. (2016) for fully connected multi-layer neural networks. By utilizing recent advances in Neural Tangent Kernel theory, we provide the first excess population risk when both the sample size and the width of the network are sufficiently large. Additionally, we discuss the role of some parameters in DP-SGD regarding their utility, both theoretically and empirically.
</details></li>
</ul>
<hr>
<h2 id="Introducing-a-Deep-Neural-Network-based-Model-Predictive-Control-Framework-for-Rapid-Controller-Implementation"><a href="#Introducing-a-Deep-Neural-Network-based-Model-Predictive-Control-Framework-for-Rapid-Controller-Implementation" class="headerlink" title="Introducing a Deep Neural Network-based Model Predictive Control Framework for Rapid Controller Implementation"></a>Introducing a Deep Neural Network-based Model Predictive Control Framework for Rapid Controller Implementation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08392">http://arxiv.org/abs/2310.08392</a></li>
<li>repo_url: None</li>
<li>paper_authors: David C. Gordon, Alexander Winkler, Julian Bedei, Patrick Schaber, Jakob Andert, Charles R. Koch</li>
<li>For: This paper presents an experimental implementation of a deep neural network (DNN) based nonlinear model predictive control (MPC) for Homogeneous Charge Compression Ignition (HCCI) combustion control.* Methods: The MPC uses a Long Short-Term Memory (LSTM) network surrounded by fully connected layers, which was trained using experimental engine data and showed acceptable prediction performance with under 5% error for all outputs.* Results: The developed controller was able to track the Indicated Mean Effective Pressure (IMEP) and combustion phasing trajectories, while minimizing several parameters. The IMEP trajectory following was excellent, with a root-mean-square error of 0.133 bar, and process constraints were observed.Here is the same information in Simplified Chinese text:* For: 这篇论文提出了一种基于深度神经网络（DNN）的非线性模块预测控制（MPC）方法，用于控制Homogeneous Charge Compression Ignition（HCCI）燃燃过程。* Methods: MPC使用了Long Short-Term Memory（LSTM）网络和全连接层，通过使用实验引擎数据训练，并达到了所有输出的下限Error的Acceptable prediction performance。* Results: 开发的控制器能够跟踪Indicated Mean Effective Pressure（IMEP）和燃燃阶段的轨迹，同时最小化一些参数。IMEP轨迹跟踪非常出色，root-mean-square error为0.133 bar，并且观察到了过程约束。<details>
<summary>Abstract</summary>
Model Predictive Control (MPC) provides an optimal control solution based on a cost function while allowing for the implementation of process constraints. As a model-based optimal control technique, the performance of MPC strongly depends on the model used where a trade-off between model computation time and prediction performance exists. One solution is the integration of MPC with a machine learning (ML) based process model which are quick to evaluate online. This work presents the experimental implementation of a deep neural network (DNN) based nonlinear MPC for Homogeneous Charge Compression Ignition (HCCI) combustion control. The DNN model consists of a Long Short-Term Memory (LSTM) network surrounded by fully connected layers which was trained using experimental engine data and showed acceptable prediction performance with under 5% error for all outputs. Using this model, the MPC is designed to track the Indicated Mean Effective Pressure (IMEP) and combustion phasing trajectories, while minimizing several parameters. Using the acados software package to enable the real-time implementation of the MPC on an ARM Cortex A72, the optimization calculations are completed within 1.4 ms. The external A72 processor is integrated with the prototyping engine controller using a UDP connection allowing for rapid experimental deployment of the NMPC. The IMEP trajectory following of the developed controller was excellent, with a root-mean-square error of 0.133 bar, in addition to observing process constraints.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="How-Many-Pretraining-Tasks-Are-Needed-for-In-Context-Learning-of-Linear-Regression"><a href="#How-Many-Pretraining-Tasks-Are-Needed-for-In-Context-Learning-of-Linear-Regression" class="headerlink" title="How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?"></a>How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08391">http://arxiv.org/abs/2310.08391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, Peter L. Bartlett</li>
<li>for: 本研究探讨了一种简单的内容学习（ICL）设置，即预训练一个线性参数化的单层线性注意力模型，用于解决未看过的任务。</li>
<li>methods: 本研究使用了一个线性参数化的单层线性注意力模型，并采用了一些独立的任务进行预训练。</li>
<li>results: 研究发现，只需要一个小数量的独立任务，可以有效地预训练模型，并且预训练后的模型可以与最佳折叠 regression 匹配。<details>
<summary>Abstract</summary>
Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a linearly parameterized single-layer linear attention model for linear regression with a Gaussian prior. We establish a statistical task complexity bound for the attention model pretraining, showing that effective pretraining only requires a small number of independent tasks. Furthermore, we prove that the pretrained model closely matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by achieving nearly Bayes optimal risk on unseen tasks under a fixed context length. These theoretical findings complement prior experimental research and shed light on the statistical foundations of ICL.
</details>
<details>
<summary>摘要</summary>
启发器预训练在多个任务上表现出了惊人的境TEXTcontext学习（ICL）能力，使其可以基于输入上下文解决未经调整模型参数的未看过任务。在这篇论文中，我们研究了ICL的一个最简设置：预训练一个线性参数化的单层线性注意力模型，用 Gaussian 假设进行线性回归。我们确定了预训练模型的任务复杂度下界，表明有效的预训练只需要一小数量独立任务。此外，我们证明了预训练模型与最优调整ridge回归算法几乎相同，即在未看过任务下，模型可以达到最优的风险，而且这一结论与先前的实验研究相符。这些理论发现补充了ICL的统计基础，并为其进一步的研究提供了新的思路。
</details></li>
</ul>
<hr>
<h2 id="Towards-Demystifying-the-Generalization-Behaviors-When-Neural-Collapse-Emerges"><a href="#Towards-Demystifying-the-Generalization-Behaviors-When-Neural-Collapse-Emerges" class="headerlink" title="Towards Demystifying the Generalization Behaviors When Neural Collapse Emerges"></a>Towards Demystifying the Generalization Behaviors When Neural Collapse Emerges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08358">http://arxiv.org/abs/2310.08358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peifeng Gao, Qianqian Xu, Yibo Yang, Peisong Wen, Huiyang Shao, Zhiyong Yang, Bernard Ghanem, Qingming Huang</li>
<li>for: 这个论文探讨了深度神经网络在训练过程中的终端阶段（TPT）中的神经塌陷（NC）现象，以及在这个过程中的泛化行为。</li>
<li>methods: 这篇论文使用了泛化学习理论来解释在TPT阶段内部件的泛化行为，并提出了一种多类margin泛化约束，用于解释为何在终端训练阶段仍然可以 дости到测试集上的准确率提高。</li>
<li>results: 论文的研究结果表明，在TPT阶段内，不同的标签和特征之间的对齐程度可以导致不同的泛化水平，即”非保守泛化”现象。此外，论文还提供了实验证据来支持其理论结论。<details>
<summary>Abstract</summary>
Neural Collapse (NC) is a well-known phenomenon of deep neural networks in the terminal phase of training (TPT). It is characterized by the collapse of features and classifier into a symmetrical structure, known as simplex equiangular tight frame (ETF). While there have been extensive studies on optimization characteristics showing the global optimality of neural collapse, little research has been done on the generalization behaviors during the occurrence of NC. Particularly, the important phenomenon of generalization improvement during TPT has been remaining in an empirical observation and lacking rigorous theoretical explanation. In this paper, we establish the connection between the minimization of CE and a multi-class SVM during TPT, and then derive a multi-class margin generalization bound, which provides a theoretical explanation for why continuing training can still lead to accuracy improvement on test set, even after the train accuracy has reached 100%. Additionally, our further theoretical results indicate that different alignment between labels and features in a simplex ETF can result in varying degrees of generalization improvement, despite all models reaching NC and demonstrating similar optimization performance on train set. We refer to this newly discovered property as "non-conservative generalization". In experiments, we also provide empirical observations to verify the indications suggested by our theoretical results.
</details>
<details>
<summary>摘要</summary>
neural collapse (NC) 是深度神经网络在终端训练阶段的一个常见现象，特征是特征和分类器归一化到一个对称结构，称为简单等距离框架（ETF）。 虽然有了广泛的优化特性研究，表明NC的全球优化性，但对于NC发生时的泛化行为进行了少量的研究。特别是在训练阶段达到100%的时候，继续训练可以导致测试集上的准确率改善，这种现象在理论上没有得到充分的解释。在这篇论文中，我们连接了CE的最小化和多类SVM的训练过程，并 derivated一种多类边界泛化 bound，这提供了一个理论上的解释，为什么在TPT阶段继续训练可以导致准确率改善。此外，我们的进一步理论结果表明，不同的标签和特征在简单ETF中的对应关系可能会导致不同的泛化提升，即使所有模型都达到NC并在训练集上达到相同的优化性。我们称之为“非保守泛化”。在实验中，我们也提供了一些实证证明我们的理论结果的指示。
</details></li>
</ul>
<hr>
<h2 id="LightZero-A-Unified-Benchmark-for-Monte-Carlo-Tree-Search-in-General-Sequential-Decision-Scenarios"><a href="#LightZero-A-Unified-Benchmark-for-Monte-Carlo-Tree-Search-in-General-Sequential-Decision-Scenarios" class="headerlink" title="LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios"></a>LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08348">http://arxiv.org/abs/2310.08348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opendilab/LightZero">https://github.com/opendilab/LightZero</a></li>
<li>paper_authors: Yazhe Niu, Yuan Pu, Zhenjie Yang, Xueyan Li, Tong Zhou, Jiyuan Ren, Shuai Hu, Hongsheng Li, Yu Liu</li>
<li>for: 这篇论文的目的是为了开发一种通用的 Monte Carlo Tree Search（MCTS）&#x2F;MuZero 基于的决策制定方法，以便在多种不同的应用中使用。</li>
<li>methods: 这篇论文使用了一种名为 LightZero 的统一测试基准，以便在多种不同的环境中评估 MCTS&#x2F;MuZero 的性能。具体来说，论文将 MCTS 的算法拆分成多个子模块，然后通过适当的探索和优化策略来提高这些子模块的性能。</li>
<li>results: 论文通过对多种任务和环境进行测试，表明了这种方法的潜在强大性和可扩展性。详细的测试结果表明，通过采用这种方法，可以在多种不同的应用中建立可扩展和高效的决策智能。<details>
<summary>Abstract</summary>
Building agents based on tree-search planning capabilities with learned models has achieved remarkable success in classic decision-making problems, such as Go and Atari. However, it has been deemed challenging or even infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse real-world applications, especially when these environments involve complex action spaces and significant simulation costs, or inherent stochasticity. In this work, we introduce LightZero, the first unified benchmark for deploying MCTS/MuZero in general sequential decision scenarios. Specificially, we summarize the most critical challenges in designing a general MCTS-style decision-making solver, then decompose the tightly-coupled algorithm and system design of tree-search RL methods into distinct sub-modules. By incorporating more appropriate exploration and optimization strategies, we can significantly enhance these sub-modules and construct powerful LightZero agents to tackle tasks across a wide range of domains, such as board games, Atari, MuJoCo, MiniGrid and GoBigger. Detailed benchmark results reveal the significant potential of such methods in building scalable and efficient decision intelligence. The code is available as part of OpenDILab at https://github.com/opendilab/LightZero.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>>建立基于树搜索规划能力的智能代理，如Go和Atari等 класси型决策问题中 achieved remarkable success。然而，扩展 Monte Carlo Tree Search（MCTS）基于算法到实际应用中，特别是当这些环境包含复杂的行动空间和重要的仿真成本，或者内在的随机性时，被评估为困难或不可能。在这种情况下，我们引入了 LightZero，第一个通用的 MCTS/MuZero Benchmark，用于在通用sequential决策场景中部署 MCTS/MuZero 算法。 Specifically, we summarize the most critical challenges in designing a general MCTS-style decision-making solver, then decompose the tightly-coupled algorithm and system design of tree-search RL methods into distinct sub-modules。 By incorporating more appropriate exploration and optimization strategies, we can significantly enhance these sub-modules and construct powerful LightZero agents to tackle tasks across a wide range of domains, such as board games, Atari, MuJoCo, MiniGrid and GoBigger。详细的 benchmark 结果表明这种方法在构建可扩展和高效的决策智能方面具有潜在的潜力。代码可以在 OpenDILab 中下载，https://github.com/opendilab/LightZero。Note: Simplified Chinese is used in this translation, which is a version of Chinese that uses simpler grammar and vocabulary than Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Neural-Diffusion-Models"><a href="#Neural-Diffusion-Models" class="headerlink" title="Neural Diffusion Models"></a>Neural Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08337">http://arxiv.org/abs/2310.08337</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maum-ai/nuwave">https://github.com/maum-ai/nuwave</a></li>
<li>paper_authors: Grigory Bartosh, Dmitry Vetrov, Christian A. Naesseth<br>for:NDMs are designed to train generative distributions more efficiently and simplify the reverse process by allowing time-dependent non-linear transformations of data.methods:NDMs use a variational bound to optimize the model in a simulation-free setting, and a time-continuous formulation allows for fast and reliable inference using off-the-shelf numerical ODE and SDE solvers.results:NDMs outperform conventional diffusion models in terms of likelihood and produce high-quality samples, as demonstrated through experiments on standard image generation benchmarks such as CIFAR-10, downsampled versions of ImageNet, and CelebA-HQ.<details>
<summary>Abstract</summary>
Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image generation benchmarks, including CIFAR-10, downsampled versions of ImageNet and CelebA-HQ. NDMs outperform conventional diffusion models in terms of likelihood and produce high-quality samples.
</details>
<details>
<summary>摘要</summary>
Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, a broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimize NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image generation benchmarks, including CIFAR-10, downsampled versions of ImageNet, and CelebA-HQ. NDMs outperform conventional diffusion models in terms of likelihood and produce high-quality samples.Here's the translation in Traditional Chinese:Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, a broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimize NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image generation benchmarks, including CIFAR-10, downsampled versions of ImageNet, and CelebA-HQ. NDMs outperform conventional diffusion models in terms of likelihood and produce high-quality samples.
</details></li>
</ul>
<hr>
<h2 id="Impact-of-multi-armed-bandit-strategies-on-deep-recurrent-reinforcement-learning"><a href="#Impact-of-multi-armed-bandit-strategies-on-deep-recurrent-reinforcement-learning" class="headerlink" title="Impact of multi-armed bandit strategies on deep recurrent reinforcement learning"></a>Impact of multi-armed bandit strategies on deep recurrent reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08331">http://arxiv.org/abs/2310.08331</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ValentinaZangirolami/DRL">https://github.com/ValentinaZangirolami/DRL</a></li>
<li>paper_authors: Valentina Zangirolami, Matteo Borrotti</li>
<li>for: 这个论文是研究如何在半 observable 系统中均衡探索和利用的trade-off，以便在自动驾驶enario中预测车辊。</li>
<li>methods: 该论文使用了多种技术来均衡探索和利用trade-off，包括随机和决定性多 armed bandit策略以及深度循环神经网络。</li>
<li>results: 研究表明，使用适应性随机方法可以更好地 approximates the trade-off between exploration and exploitation，而且在总体来说，Softmax和Max-Boltzmann策略可以超越epsilon-greedy策略。<details>
<summary>Abstract</summary>
Incomplete knowledge of the environment leads an agent to make decisions under uncertainty. One of the major dilemmas in Reinforcement Learning (RL) where an autonomous agent has to balance two contrasting needs in making its decisions is: exploiting the current knowledge of the environment to maximize the cumulative reward as well as exploring actions that allow improving the knowledge of the environment, hopefully leading to higher reward values (exploration-exploitation trade-off). Concurrently, another relevant issue regards the full observability of the states, which may not be assumed in all applications. Such as when only 2D images are considered as input in a RL approach used for finding the optimal action within a 3D simulation environment. In this work, we address these issues by deploying and testing several techniques to balance exploration and exploitation trade-off on partially observable systems for predicting steering wheels in autonomous driving scenario. More precisely, the final aim is to investigate the effects of using both stochastic and deterministic multi-armed bandit strategies coupled with a Deep Recurrent Q-Network. Additionally, we adapted and evaluated the impact of an innovative method to improve the learning phase of the underlying Convolutional Recurrent Neural Network. We aim to show that adaptive stochastic methods for exploration better approximate the trade-off between exploration and exploitation as, in general, Softmax and Max-Boltzmann strategies are able to outperform epsilon-greedy techniques.
</details>
<details>
<summary>摘要</summary>
agent在缺乏环境知识的情况下做出决策，是Reinforcement Learning（RL）中一个主要的挑战。在RL中，自动代理人需要均衡两种冲突的决策：一方面是利用当前环境知识来最大化总奖励，另一方面是探索可以提高环境知识的动作，希望能够获得更高的奖励值（探索-利用贸易offs）。同时，另一个相关的问题是环境完全可见性，这可能不是所有应用中都能假设。例如，在使用RL方法控制汽车自动驾驶时，只有2D图像作为输入。在这种情况下，我们采用了多种技术来均衡探索和利用贸易offs，并在部分可见性下进行预测车辊。具体来说，我们的目标是研究使用杂合精度和杂合概率方法来均衡探索和利用贸易offs，同时采用深度循环网络来改进学习阶段。我们希望能够证明，适应性的随机方法可以更好地均衡探索和利用贸易offs，而且在总体来说，Softmax和Max-Boltzmann策略可以超越epsilon-greedy策略。
</details></li>
</ul>
<hr>
<h2 id="A-Symmetry-Aware-Exploration-of-Bayesian-Neural-Network-Posteriors"><a href="#A-Symmetry-Aware-Exploration-of-Bayesian-Neural-Network-Posteriors" class="headerlink" title="A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors"></a>A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08287">http://arxiv.org/abs/2310.08287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Laurent, Emanuel Aldea, Gianni Franchi</li>
<li>for: 这篇论文旨在探讨现代深度神经网络（DNNs）的权重分布，以便量化不确定性和robustness。</li>
<li>methods: 该论文提出了一种大规模探讨深度 bayesian neural network（BNNs）的 posterior distribution，并推广到实际视觉任务和结构。特别是，我们研究了 posterior 的最佳approximation方法，分析了 posterior 质量和不确定性量化之间的关系，探讨了模式对 posterior 的影响，并探索了可视化 posterior 的方法。</li>
<li>results: 我们发现 weight-space symmetries 是理解 posterior 的关键方面，并开发了对这种 symmetries 的深入分析。特别是，我们发现 permutation 和 scaling symmetries 对 Bayesian posterior 有重要影响，并探讨了这些 symmetries 与 L2 正则化之间的关系。此外，我们将 shortly release 一个大规模的 checkpoint 数据集，包括了千个实际模型和我们的代码，以帮助社区更好地理解 Bayesian posterior。<details>
<summary>Abstract</summary>
The distribution of the weights of modern deep neural networks (DNNs) - crucial for uncertainty quantification and robustness - is an eminently complex object due to its extremely high dimensionality. This paper proposes one of the first large-scale explorations of the posterior distribution of deep Bayesian Neural Networks (BNNs), expanding its study to real-world vision tasks and architectures. Specifically, we investigate the optimal approach for approximating the posterior, analyze the connection between posterior quality and uncertainty quantification, delve into the impact of modes on the posterior, and explore methods for visualizing the posterior. Moreover, we uncover weight-space symmetries as a critical aspect for understanding the posterior. To this extent, we develop an in-depth assessment of the impact of both permutation and scaling symmetries that tend to obfuscate the Bayesian posterior. While the first type of transformation is known for duplicating modes, we explore the relationship between the latter and L2 regularization, challenging previous misconceptions. Finally, to help the community improve our understanding of the Bayesian posterior, we will shortly release the first large-scale checkpoint dataset, including thousands of real-world models and our codes.
</details>
<details>
<summary>摘要</summary>
现代深度神经网络（DNN）的权重分布 - 对于不确定性评估和稳定性的评估非常重要 - 是一个极其复杂的对象，因为其维度非常高。这篇论文提出了一个大规模的 posterior distribution 的探索，扩展到了真实世界视觉任务和架构。我们详细研究 posterior 的优化方法，分析 posterior 质量和不确定性评估之间的连接，探讨模式对 posterior 的影响，以及如何可视化 posterior。此外，我们还发现权重空间的Symmetry 是理解 posterior 的关键因素。为了更好地理解 posterior，我们开发了一种权重空间Symmetry 的深入分析，包括 permutation 和缩放Symmetry。而 permutation 类型的转换可以复制模式，我们探索了 L2 正则化和这种转换之间的关系，推翻了一些以前的错误假设。最后，为了帮助社区更好地理解 Bayesian posterior，我们将 shortly 发布大规模的 checkpoint 数据集，包括 тысячи个真实世界模型和我们的代码。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-modeling-of-self-similar-dynamics"><a href="#Data-driven-modeling-of-self-similar-dynamics" class="headerlink" title="Data driven modeling of self-similar dynamics"></a>Data driven modeling of self-similar dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08282">http://arxiv.org/abs/2310.08282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ru-yi Tao, Ning-ning Tao, Yi-zhuang You, Jiang Zhang</li>
<li>for: 本研究旨在提出一种基于多尺度模型的数据驱动模型，用于理解复杂系统的内含特性。</li>
<li>methods: 该模型基于自similarity prior knowledge，可以用来模型自similar dynamical systems。</li>
<li>results: 研究人员通过对Isng模型进行测试，发现模型可以提取出scale-invariant kernel，并且可以确定非平衡系统的极限阶段转移。<details>
<summary>Abstract</summary>
Multiscale modeling of complex systems is crucial for understanding their intricacies. Data-driven multiscale modeling has emerged as a promising approach to tackle challenges associated with complex systems. On the other hand, self-similarity is prevalent in complex systems, hinting that large-scale complex systems can be modeled at a reduced cost. In this paper, we introduce a multiscale neural network framework that incorporates self-similarity as prior knowledge, facilitating the modeling of self-similar dynamical systems. For deterministic dynamics, our framework can discern whether the dynamics are self-similar. For uncertain dynamics, it can compare and determine which parameter set is closer to self-similarity. The framework allows us to extract scale-invariant kernels from the dynamics for modeling at any scale. Moreover, our method can identify the power law exponents in self-similar systems. Preliminary tests on the Ising model yielded critical exponents consistent with theoretical expectations, providing valuable insights for addressing critical phase transitions in non-equilibrium systems.
</details>
<details>
<summary>摘要</summary>
多尺度模型复杂系统的重要性，是为了理解它们的复杂性。数据驱动多尺度模型已成为规模系统的研究挑战的有效方法。同时，复杂系统中的自相似性很普遍，这表示大规模复杂系统可以在更低的成本下进行模型化。在本文中，我们提出了一种多尺度神经网络框架，该框架利用自相似性作为先验知识，以便模型自相似动力系统。对于确定性动力学，我们的框架可以判断动力系统是否为自相似的。对于不确定性动力学，它可以比较和确定最接近自相似性的参数集。我们的方法可以从动力系统中提取任意尺度的扁平函数，并且可以识别自相似系统中的指数律指数。我们的方法可以在不同尺度下进行模型化，并且可以提供非平衡系统的普遍性。在预测期望值下，我们的方法对牛顿模型进行了初步测试，并得到了相关的扩展 exponent。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Unified-Analysis-of-Kernel-based-Methods-Under-Covariate-Shift"><a href="#Towards-a-Unified-Analysis-of-Kernel-based-Methods-Under-Covariate-Shift" class="headerlink" title="Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift"></a>Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08237">http://arxiv.org/abs/2310.08237</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WangCaixing-96/Kernel_CS">https://github.com/WangCaixing-96/Kernel_CS</a></li>
<li>paper_authors: Xingdong Feng, Xin He, Caixing Wang, Chao Wang, Jingnan Zhang</li>
<li>for: 异常输出预测问题中的covariate shift问题</li>
<li>methods: 基于可重构kernel空间的非 Parametric方法</li>
<li>results: 提出了一种统一分析方法，并在一个 ricloss函数家族中得到了正确的理论和数值结果，并且在synthetic和实际示例中进行了广泛的数值研究，证明了方法的有效性。<details>
<summary>Abstract</summary>
Covariate shift occurs prevalently in practice, where the input distributions of the source and target data are substantially different. Despite its practical importance in various learning problems, most of the existing methods only focus on some specific learning tasks and are not well validated theoretically and numerically. To tackle this problem, we propose a unified analysis of general nonparametric methods in a reproducing kernel Hilbert space (RKHS) under covariate shift. Our theoretical results are established for a general loss belonging to a rich loss function family, which includes many commonly used methods as special cases, such as mean regression, quantile regression, likelihood-based classification, and margin-based classification. Two types of covariate shift problems are the focus of this paper and the sharp convergence rates are established for a general loss function to provide a unified theoretical analysis, which concurs with the optimal results in literature where the squared loss is used. Extensive numerical studies on synthetic and real examples confirm our theoretical findings and further illustrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
covariate shift 是在实践中非常普遍存在的问题，source和target数据的输入分布substantially different。Despite its practical importance in various learning problems, most existing methods only focus on some specific learning tasks and are not well validated theoretically and numerically. To tackle this problem, we propose a unified analysis of general nonparametric methods in a reproducing kernel Hilbert space (RKHS) under covariate shift. Our theoretical results are established for a general loss belonging to a rich loss function family, which includes many commonly used methods as special cases, such as mean regression, quantile regression, likelihood-based classification, and margin-based classification. Two types of covariate shift problems are the focus of this paper and the sharp convergence rates are established for a general loss function to provide a unified theoretical analysis, which concurs with the optimal results in literature where the squared loss is used. Extensive numerical studies on synthetic and real examples confirm our theoretical findings and further illustrate the effectiveness of our proposed method.
</details></li>
</ul>
<hr>
<h2 id="Emergence-of-Latent-Binary-Encoding-in-Deep-Neural-Network-Classifiers"><a href="#Emergence-of-Latent-Binary-Encoding-in-Deep-Neural-Network-Classifiers" class="headerlink" title="Emergence of Latent Binary Encoding in Deep Neural Network Classifiers"></a>Emergence of Latent Binary Encoding in Deep Neural Network Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08224">http://arxiv.org/abs/2310.08224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luigi Sbailò, Luca Ghiringhelli</li>
<li>for: 这个论文是为了研究深度神经网络分类器中的二进制编码。</li>
<li>methods: 作者使用了在训练过程中引入线性末级层，并使用一个增长为$\exp(\vec{x}^2)$的损失函数来训练。</li>
<li>results: 研究发现，在训练的末 stages，深度神经网络的latent space中会出现二进制编码，这会加速训练过程中的折衔和提高分类精度。<details>
<summary>Abstract</summary>
We observe the emergence of binary encoding within the latent space of deep-neural-network classifiers. Such binary encoding is induced by introducing a linear penultimate layer, which is equipped during training with a loss function that grows as $\exp(\vec{x}^2)$, where $\vec{x}$ are the coordinates in the latent space. The phenomenon we describe represents a specific instance of a well-documented occurrence known as \textit{neural collapse}, which arises in the terminal phase of training and entails the collapse of latent class means to the vertices of a simplex equiangular tight frame (ETF). We show that binary encoding accelerates convergence toward the simplex ETF and enhances classification accuracy.
</details>
<details>
<summary>摘要</summary>
Note:* "latent space" is translated as "latent space" (干净空间)* "deep-neural-network classifiers" is translated as "深度神经网络分类器" (shēn dào shén zhī wǎng wǎng)* "binary encoding" is translated as "二进制编码" (èr jì bìn yì)* "linear penultimate layer" is translated as "线性末层" (xiào xìng zhì yù)* "loss function" is translated as "损失函数" (shū shī fún)* "neural collapse" is translated as "神经塌陷" (shén xiān zhù)* "simplex equiangular tight frame" is translated as "等角紧缩框架" (děng jiàng jǐn zhù kōng jì)* "accelerates convergence" is translated as "加速收敛" (jiā sù shōu jí)* "enhances classification accuracy" is translated as "提高分类准确性" (tí gāo fēn xiǎng yì yì)
</details></li>
</ul>
<hr>
<h2 id="Conformal-inference-for-regression-on-Riemannian-Manifolds"><a href="#Conformal-inference-for-regression-on-Riemannian-Manifolds" class="headerlink" title="Conformal inference for regression on Riemannian Manifolds"></a>Conformal inference for regression on Riemannian Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08209">http://arxiv.org/abs/2310.08209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alejandro Cholaquidis, Fabrice Gamboa, Leonardo Moreno</li>
<li>for: 这篇论文探讨了在拟合空间中进行回归的问题，具体来说是在拟合空间中预测变量Y的问题。</li>
<li>methods: 该论文使用了传统的协形推断方法，不假设任何关于$(X, Y)$的联合分布模型，而是基于拟合空间上的抽象结构来建立预测集。</li>
<li>results: 该论文提出了一种基于拟合空间的预测集方法，并证明了该方法的 asymptotic almost sure convergence 性和效率。 通过实验和实际数据分析，论文还证明了该方法的可行性和适用性。<details>
<summary>Abstract</summary>
Regression on manifolds, and, more broadly, statistics on manifolds, has garnered significant importance in recent years due to the vast number of applications for this type of data. Circular data is a classic example, but so is data in the space of covariance matrices, data on the Grassmannian manifold obtained as a result of principal component analysis, among many others. In this work we investigate prediction sets for regression scenarios when the response variable, denoted by $Y$, resides in a manifold, and the covariable, denoted by X, lies in Euclidean space. This extends the concepts delineated in [Lei and Wasserman, 2014] to this novel context. Aligning with traditional principles in conformal inference, these prediction sets are distribution-free, indicating that no specific assumptions are imposed on the joint distribution of $(X, Y)$, and they maintain a non-parametric character. We prove the asymptotic almost sure convergence of the empirical version of these regions on the manifold to their population counterparts. The efficiency of this method is shown through a comprehensive simulation study and an analysis involving real-world data.
</details>
<details>
<summary>摘要</summary>
“投 regression on manifolds 和更广泛的 statistics on manifolds 在最近几年内受到了广泛关注，因为这类数据在各个领域中有着广泛的应用。Circular data 和 data on the Grassmannian manifold 都是 класси的例子，以及由 principal component analysis 获得的数据在 manifold 上。在这项工作中，我们调查了 regression enario 中回归变量 $Y$ 的 manifold 上的预测集，而 covariable $X$ 则在欧几何空间中。这个扩展了 [Lei and Wasserman, 2014] 中的概念，并遵循了传统的准确推断原则。这些预测集是无结构的，即没有对 $(X, Y)$ 的共同分布假设，并且具有非Parametric 的特点。我们证明了这些预测集在 manifold 上的empirical版本在极限上对应于人口版本的 asymptotic almost sure convergence。我们通过了广泛的 simulate 研究和一个使用实际数据的分析，证明了这种方法的效率。”Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Infinite-Width-Graph-Neural-Networks-for-Node-Regression-Classification"><a href="#Infinite-Width-Graph-Neural-Networks-for-Node-Regression-Classification" class="headerlink" title="Infinite Width Graph Neural Networks for Node Regression&#x2F; Classification"></a>Infinite Width Graph Neural Networks for Node Regression&#x2F; Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08176">http://arxiv.org/abs/2310.08176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yCobanoglu/infinite-width-gnns">https://github.com/yCobanoglu/infinite-width-gnns</a></li>
<li>paper_authors: Yunus Cobanoglu</li>
<li>for: 这paper探讨了图神经网络（GNN）的普通化，当其宽度（即每层全连接节点数）增加到无穷大时。</li>
<li>methods: 这paper使用了各种架构，包括标准图神经网络、跳过 concatenate 连接的图神经网络和Graph Attention神经网络，并 derivied了它们的kernel和 Gaussian Process 的闭式。</li>
<li>results: 这paper在多种dataset上进行了透传节点预测和分类任务，并使用了spectral sparsification方法来提高运行时间和内存需求。此外， inductive graph learning任务（图 regression&#x2F;classification）的扩展也 briefley discussed。<details>
<summary>Abstract</summary>
This work analyzes Graph Neural Networks, a generalization of Fully-Connected Deep Neural Nets on Graph structured data, when their width, that is the number of nodes in each fullyconnected layer is increasing to infinity. Infinite Width Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels, both Machine Learning Frameworks with long traditions and extensive theoretical foundations. Gaussian Processes and Kernels have much less hyperparameters then Neural Networks and can be used for uncertainty estimation, making them more user friendly for applications. This works extends the increasing amount of research connecting Gaussian Processes and Kernels to Neural Networks. The Kernel and Gaussian Process closed forms are derived for a variety of architectures, namely the standard Graph Neural Network, the Graph Neural Network with Skip-Concatenate Connections and the Graph Attention Neural Network. All architectures are evaluated on a variety of datasets on the task of transductive Node Regression and Classification. Additionally, a Spectral Sparsification method known as Effective Resistance is used to improve runtime and memory requirements. Extending the setting to inductive graph learning tasks (Graph Regression/ Classification) is straightforward and is briefly discussed in 3.5.
</details>
<details>
<summary>摘要</summary>
这个研究 analyze Graph Neural Networks (GNNs), a generalization of Fully-Connected Deep Neural Nets on graph-structured data, when the width (i.e., the number of nodes in each fully connected layer) increases to infinity. 无穷宽神经网络可以将深度学习与泊松过程和kernel相连接，这两者都是机器学习框架，具有较少的Hyperparameter，可以用于uncertainty estimation，使其更适用于应用。这项研究将Gaussian Processes和Kernels相关的研究扩展到神经网络领域。这些架构包括标准的Graph Neural Network (GNN)、Skip-Concatenate Connection GNN和Graph Attention Neural Network (GAT)。这些架构在逻辑回归和分类任务上进行了多种数据集的评估。此外，我们还使用了一种spectral sparsification方法，即Effective Resistance，以改善运行时间和内存需求。将这些设置推广到 inductive graph learning任务 (graph regression/classification) 也是可行的，并在3.5中简要介绍。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-Reward-Models-in-RLHF-Tuned-Language-Models-Using-Sparse-Autoencoders"><a href="#Interpreting-Reward-Models-in-RLHF-Tuned-Language-Models-Using-Sparse-Autoencoders" class="headerlink" title="Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders"></a>Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08164">http://arxiv.org/abs/2310.08164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luke Marks, Amir Abdullah, Luna Mendez, Rauno Arike, Philip Torr, Fazl Barez</li>
<li>for: This paper aims to provide a method for interpreting the learned reward functions in reinforcement learning-tuned large language models (LLMs), in order to ensure alignment between the model’s behaviors and the specified objectives.</li>
<li>methods: The proposed method uses sparse autoencoders to compare the activations of a base LLM and its RLHF-tuned version, and identifies unique features that reflect the accuracy of the learned reward model.</li>
<li>results: The method provides an abstract approximation of reward integrity, and is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs.<details>
<summary>Abstract</summary>
Large language models (LLMs) aligned to human preferences via reinforcement learning from human feedback (RLHF) underpin many commercial applications. However, how RLHF impacts LLM internals remains opaque. We propose a novel method to interpret learned reward functions in RLHF-tuned LLMs using sparse autoencoders. Our approach trains autoencoder sets on activations from a base LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. To quantify this, we construct a scenario where the tuned LLM learns token-reward mappings to maximize reward. This is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs. Our method provides an abstract approximation of reward integrity. This presents a promising technique for ensuring alignment between specified objectives and model behaviors.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese:大型语言模型（LLM）通过人类反馈学习（RLHF）实现了许多商业应用程序。然而，RLHF如何影响 LLM 的内部结构仍然不清晰。我们提出了一种新的方法来解释 RLHF 调教的 LLM 学习的奖励函数使用稀疏自动编码器。我们的方法通过将自动编码器集 trains 在基础 LLM 和其RLHF 调教版本的活动上，并比较自动编码器隐藏空间，以确定RLHF 调教后 LLM 学习的奖励模型的准确性。为了量化这一点，我们构建了一种情况，在RLHF 调教后 LLM 学习token-奖励映射以最大化奖励。这是 sparse autoencoders 用于解释学习奖励的第一个应用，并广泛检查RLHF 学习在 LLM 中的奖励学习。我们的方法提供了一种抽象的奖励完整性评估方法，可以确保模型的行为与指定目标之间的一致性。
</details></li>
</ul>
<hr>
<h2 id="On-Extreme-Value-Asymptotics-of-Projected-Sample-Covariances-in-High-Dimensions-with-Applications-in-Finance-and-Convolutional-Networks"><a href="#On-Extreme-Value-Asymptotics-of-Projected-Sample-Covariances-in-High-Dimensions-with-Applications-in-Finance-and-Convolutional-Networks" class="headerlink" title="On Extreme Value Asymptotics of Projected Sample Covariances in High Dimensions with Applications in Finance and Convolutional Networks"></a>On Extreme Value Asymptotics of Projected Sample Covariances in High Dimensions with Applications in Finance and Convolutional Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08150">http://arxiv.org/abs/2310.08150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ansgar Steland</li>
<li>for: 研究高维时间序列数据集是否遵循正常分布条件下收集的Maximum-type统计方法。</li>
<li>methods: 使用样本协variance矩阵的最大值统计方法，并应用Gumbel-type极值 asymptotics 来确定数据集的分布条件。</li>
<li>results: 研究应用于最小协variance股票组合优化和对偏好风险的分析，ETF指数追踪 by sparse tracking portfolios， convolutional deep learners for image analysis 和 array-of-sensors数据分析。<details>
<summary>Abstract</summary>
Maximum-type statistics of certain functions of the sample covariance matrix of high-dimensional vector time series are studied to statistically confirm or reject the null hypothesis that a data set has been collected under normal conditions. The approach generalizes the case of the maximal deviation of the sample autocovariances function from its assumed values. Within a linear time series framework it is shown that Gumbel-type extreme value asymptotics holds true. As applications we discuss long-only mimimal-variance portfolio optimization and subportfolio analysis with respect to idiosyncratic risks, ETF index tracking by sparse tracking portfolios, convolutional deep learners for image analysis and the analysis of array-of-sensors data.
</details>
<details>
<summary>摘要</summary>
“ maximum-type 统计学的certain function of the sample covariance matrix of high-dimensional vector time series 被研究以确认或拒绝 null hypothesis 是否在正常情况下收集数据。这种方法总结了最大偏差sample autocovariances函数的假设值。在线性时间序列框架下，证明Gumbel-type极值几何 asymptotics 成立。应用包括long-only minimal-variance portfolio optimization和对唯一风险的subportfolio分析，ETF指数追踪 porfolio by sparse tracking， convolutional deep learners for image analysis和 array-of-sensors data 分析。”Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Knowledge-Based-Visual-Question-Answering-with-Inference-Paths"><a href="#Open-Set-Knowledge-Based-Visual-Question-Answering-with-Inference-Paths" class="headerlink" title="Open-Set Knowledge-Based Visual Question Answering with Inference Paths"></a>Open-Set Knowledge-Based Visual Question Answering with Inference Paths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08148">http://arxiv.org/abs/2310.08148</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JingruG/GATHER">https://github.com/JingruG/GATHER</a></li>
<li>paper_authors: Jingru Gan, Xinzhe Han, Shuhui Wang, Qingming Huang</li>
<li>for: Answering open-set questions with explicit reasoning paths in a knowledge-based visual question answering system.</li>
<li>methods: The proposed Graph pATH rankER (GATHER) framework, which includes graph constructing, pruning, and path-level ranking.</li>
<li>results: The model is able to perform open-set question answering across the whole knowledge base and provide explicit reasoning paths, as demonstrated through extensive experiments on real-world questions.<details>
<summary>Abstract</summary>
Given an image and an associated textual question, the purpose of Knowledge-Based Visual Question Answering (KB-VQA) is to provide a correct answer to the question with the aid of external knowledge bases. Prior KB-VQA models are usually formulated as a retriever-classifier framework, where a pre-trained retriever extracts textual or visual information from knowledge graphs and then makes a prediction among the candidates. Despite promising progress, there are two drawbacks with existing models. Firstly, modeling question-answering as multi-class classification limits the answer space to a preset corpus and lacks the ability of flexible reasoning. Secondly, the classifier merely consider "what is the answer" without "how to get the answer", which cannot ground the answer to explicit reasoning paths. In this paper, we confront the challenge of \emph{explainable open-set} KB-VQA, where the system is required to answer questions with entities at wild and retain an explainable reasoning path. To resolve the aforementioned issues, we propose a new retriever-ranker paradigm of KB-VQA, Graph pATH rankER (GATHER for brevity). Specifically, it contains graph constructing, pruning, and path-level ranking, which not only retrieves accurate answers but also provides inference paths that explain the reasoning process. To comprehensively evaluate our model, we reformulate the benchmark dataset OK-VQA with manually corrected entity-level annotations and release it as ConceptVQA. Extensive experiments on real-world questions demonstrate that our framework is not only able to perform open-set question answering across the whole knowledge base but provide explicit reasoning path.
</details>
<details>
<summary>摘要</summary>
KB-VQA的目的是使用外部知识库提供问题的正确答案，而且可以提供解释的推理路径。现有的KB-VQA模型通常采用 Retriever-Classifier 框架，其中 Pre-trained Retriever 从知识图中提取文本或视觉信息，然后进行多类分类预测。然而，现有模型存在两个缺陷：首先，问题答案模型为多类分类，限制答案空间为预设的词汇库，缺乏灵活的推理能力。其次，分类器仅考虑“问题的答案”而不考虑“如何得到答案”，无法固定推理路径。在这篇文章中，我们面临KB-VQA中的解释开放集问题，需要在未知知识库中回答问题并提供可解释的推理路径。为解决这些问题，我们提出了一种新的 Retriever-Ranker 模型，即 Graph pATH rankER（GATHER）。GATHER模型包含图构建、剪辑和路径级别排名，不仅可以 Retrieves 精准答案，还可以提供推理路径。为了全面评估我们的模型，我们对 OK-VQA  benchmark dataset进行了手动修改Entity-level的注释，并将其发布为 ConceptVQA。广泛的实验表明，我们的框架不仅可以在整个知识库中开放式回答问题，还可以提供可解释的推理路径。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanations-for-Time-Series-Forecasting"><a href="#Counterfactual-Explanations-for-Time-Series-Forecasting" class="headerlink" title="Counterfactual Explanations for Time Series Forecasting"></a>Counterfactual Explanations for Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08137">http://arxiv.org/abs/2310.08137</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhendong3wang/counterfactual-explanations-for-forecasting">https://github.com/zhendong3wang/counterfactual-explanations-for-forecasting</a></li>
<li>paper_authors: Zhendong Wang, Ioanna Miliou, Isak Samsten, Panagiotis Papapetrou</li>
<li>for: 本研究旨在提出一种新的时间序列预测方法，可以生成有意义的对照性解释。</li>
<li>methods: 本研究使用了深度预测模型，并提出了一种基于梯度的方法来生成对照性解释。</li>
<li>results: 实验结果表明，本方法可以在不同的深度预测模型上达到更高的对照性和数据抽象度。<details>
<summary>Abstract</summary>
Among recent developments in time series forecasting methods, deep forecasting models have gained popularity as they can utilize hidden feature patterns in time series to improve forecasting performance. Nevertheless, the majority of current deep forecasting models are opaque, hence making it challenging to interpret the results. While counterfactual explanations have been extensively employed as a post-hoc approach for explaining classification models, their application to forecasting models still remains underexplored. In this paper, we formulate the novel problem of counterfactual generation for time series forecasting, and propose an algorithm, called ForecastCF, that solves the problem by applying gradient-based perturbations to the original time series. ForecastCF guides the perturbations by applying constraints to the forecasted values to obtain desired prediction outcomes. We experimentally evaluate ForecastCF using four state-of-the-art deep model architectures and compare to two baselines. Our results show that ForecastCF outperforms the baseline in terms of counterfactual validity and data manifold closeness. Overall, our findings suggest that ForecastCF can generate meaningful and relevant counterfactual explanations for various forecasting tasks.
</details>
<details>
<summary>摘要</summary>
现代时间序列预测方法中，深度预测模型在使用隐藏特征模式来提高预测性能方面具有广泛的应用。然而，大多数当前的深度预测模型是不透明的，因此很难解释结果。在分类模型中，对比方面的解释方法已经广泛应用，但是对预测模型的应用仍然尚未得到充分发展。在本文中，我们提出了时间序列预测中的对比方面问题，并提出了一种名为 ForecastCF 的算法，该算法通过对原始时间序列应用梯度基于的干扰来解决这个问题。ForecastCF 使用约束来导引干扰，以实现想要的预测结果。我们通过四种当今最佳的深度模型架构进行实验测试，并与两个基准值进行比较。我们的结果表明，ForecastCF 在对比验证中高于基准值，以至于counterfactual正确性和数据折衔离的方面。总的来说，我们的发现表明，ForecastCF 可以生成有意义和相关的对比解释，用于各种预测任务。
</details></li>
</ul>
<hr>
<h2 id="Core-sets-for-Fair-and-Diverse-Data-Summarization"><a href="#Core-sets-for-Fair-and-Diverse-Data-Summarization" class="headerlink" title="Core-sets for Fair and Diverse Data Summarization"></a>Core-sets for Fair and Diverse Data Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08122">http://arxiv.org/abs/2310.08122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/coresets-fair-diverse">https://github.com/microsoft/coresets-fair-diverse</a></li>
<li>paper_authors: Sepideh Mahabadi, Stojan Trajanovski</li>
<li>for: 本文研究核心集构造算法，用于实现多元化最大化并且满足公平&#x2F;分区约束。给定一个点集 $P$ 在 метри空间中，分成 $m$ 个组，并给定 $k_1, \ldots, k_m$，目标是从每个组中选择 $k_i$ 个点，使得总体多元化最大化。</li>
<li>methods: 本文考虑了两种自然多元度度量：点对之间距离和最近邻居距离，并提供了改进的核心集构造算法。具体来说，我们提供了对sum-of-pairwise distances的首个常量因子核心集，其大小独立于数据集大小和尺度比。其次，我们提供了对sum-of-nearest-neighbor distances的首个核心集。</li>
<li>results: 我们运行了多个实验，证明我们的核心集方法的效果。具体来说，我们应用了这种核心集方法，对一个实际应用中的时间消息汇总 зада题进行了解决。Specifically, the summary should include more recent messages compared to older ones. 我们实现了100倍的速度提升，而多元化损失只减少了很少。此外，我们的方法可以在流式设定中提高算法的空间使用率。<details>
<summary>Abstract</summary>
We study core-set construction algorithms for the task of Diversity Maximization under fairness/partition constraint. Given a set of points $P$ in a metric space partitioned into $m$ groups, and given $k_1,\ldots,k_m$, the goal of this problem is to pick $k_i$ points from each group $i$ such that the overall diversity of the $k=\sum_i k_i$ picked points is maximized. We consider two natural diversity measures: sum-of-pairwise distances and sum-of-nearest-neighbor distances, and show improved core-set construction algorithms with respect to these measures. More precisely, we show the first constant factor core-set w.r.t. sum-of-pairwise distances whose size is independent of the size of the dataset and the aspect ratio. Second, we show the first core-set w.r.t. the sum-of-nearest-neighbor distances. Finally, we run several experiments showing the effectiveness of our core-set approach. In particular, we apply constrained diversity maximization to summarize a set of timed messages that takes into account the messages' recency. Specifically, the summary should include more recent messages compared to older ones. This is a real task in one of the largest communication platforms, affecting the experience of hundreds of millions daily active users. By utilizing our core-set method for this task, we achieve a 100x speed-up while losing the diversity by only a few percent. Moreover, our approach allows us to improve the space usage of the algorithm in the streaming setting.
</details>
<details>
<summary>摘要</summary>
我们研究核心集建构算法，以实现多样性最大化 unter fairness/分区约束。给定一个点集 $P$ 在一个度量空间中分成 $m$ 个组，并给定 $k_1, \ldots, k_m$, 则这个问题的目标是从每个组 $i$ 中选择 $k_i$ 个点，使得总的多样性最大化。我们考虑了两种自然的多样性度量：点对的总距离和最近邻居距离，并提供了改进的核心集建构算法。更加准确地说，我们提供了第一个常量因子核心集，其大小独立于数据集大小和尺度比。其次，我们提供了第一个对最近邻居距离进行多样性最大化的核心集。最后，我们运行了多个实验，证明了我们的核心集方法的有效性。特别是，我们应用了多样性最大化来摘要一组时间消息，考虑到消息的新鲜度。具体来说，摘要应包含更新的消息比较新的消息。这是一个现实中的大型通信平台上的实际任务，影响了每天活跃用户数百万。通过利用我们的核心集方法来解决这个任务，我们得到了100倍的速度提升，而多样性损失只减少了很少。此外，我们的方法可以改进流式设定下的算法空间使用情况。
</details></li>
</ul>
<hr>
<h2 id="Overview-of-Physics-Informed-Machine-Learning-Inversion-of-Geophysical-Data"><a href="#Overview-of-Physics-Informed-Machine-Learning-Inversion-of-Geophysical-Data" class="headerlink" title="Overview of Physics-Informed Machine Learning Inversion of Geophysical Data"></a>Overview of Physics-Informed Machine Learning Inversion of Geophysical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08109">http://arxiv.org/abs/2310.08109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gerard T. Schuster, Shihang Feng</li>
<li>For: The paper discusses the use of physics-informed machine learning (PIML) algorithms for geophysical data inversion.* Methods: The paper proposes four different PIML algorithms, each with a unique combination of weights and neural network operations, and uses a joint objective function (Equation \ref{PIML.eq120}) to minimize the difference between observed and predicted data.* Results: The paper highlights the potential advantages of PIML over standard full-waveform inversion (FWI), including the ability to avoid local minima and the option to locally train the inversion operator, but notes that the effectiveness of PIML relies on the similarity between the test and trained data.Here is the information in Simplified Chinese:* For: 这篇论文介绍了物理学 Informed 机器学习（PIML）算法的应用于地球物理数据逆向。* Methods: 这篇论文提出了四种不同的 PIML 算法，每种都有特定的量身定制的量和神经网络操作，并使用共同目标函数（方程 \ref{PIML.eq120}）来最小化观测数据和预测数据之间的差异。* Results: 这篇论文指出了 PIML 相比标准全波形逆向（FWI）具有避免地点最小值和地方训练逆向器的选择，但是其效iveness 受观测数据和训练数据之间的相似性影响。<details>
<summary>Abstract</summary>
We review four types of algorithms for physics-informed machine learning (PIML) inversion of geophysical data. The unifying equation is given by the joint objective function $\epsilon$:   \begin{eqnarray} \epsilon^{||-PIML}&=&\lambda_1 \overbrace{||{\bf W}^{ML}({\bf H}_{\bf w} {\bf d}^{obs}-{\bf m})||^2}^{NN} + \lambda_2 \overbrace{||{\bf W}^{FWI}({\bf L} {\bf m}-{\bf d}^{obs})||^2}^{FWI} ~+ \nonumber\\ \nonumber\\ && + ~~Regularizer, \label{PIML.eq120} \end{eqnarray}where the optimal model ${\bf m}^*$ and weights $\bf w^*$ minimize $\epsilon$. Here, The matrix weights are given by the boldface symbol $\bf W$, and full waveform inversion (FWI) is typically computed using a finite-difference solution of the wave equation, where $\bf L$ represents the forward modeling operation of the wave equation as a function of the model $\bf m$. Also, a fully-connected neural network (NN) is used to compute the model ${\bf H_w}{\bf d}^{obs} \approx \bf m$ from the observed input data ${\bf d}^{obs}$. The selection of weights $\lambda_i$ and the NN operations determine one of four different PIML algorithms.   PIML offers potential advantages over standard FWI through its enhanced ability to avoid local minima and the option to locally train the inversion operator, minimizing the requirement for extensive training data for global applicability. However, the effectiveness of PIML relies on the similarity between the test and trained data. Nevertheless, a possible strategy to overcome this limitation involves initial pretraining of a PIML architecture with data from a broader region, followed by fine-tuning for specific data-a method reminiscent of the way large language models are pretrained and adapted for various tasks.
</details>
<details>
<summary>摘要</summary>
我们评审了四种物理学 Informed Machine Learning（PIML）逆转 geophysical 数据的算法。共同的目标函数为 $\epsilon$：$$\epsilon^{||-PIML} = \lambda_1 \left\lVert{\bf W}^{ML}({\bf H}_{\bf w} {\bf d}^{obs} - {\bf m})\right\rVert^2 + \lambda_2 \left\lVert{\bf W}^{FWI}({\bf L} {\bf m} - {\bf d}^{obs})\right\rVert^2 + \text{Regularizer}$$其中，最佳模型 ${\bf m}^*$ 和权重 $\bf w^*$ 将 minimize $\epsilon$。其中，粗体字 $\bf W$ 表示矩阵权重，全波形推导（FWI）通常使用finite-difference方法解析波动方程，其中 $\bf L$ 表示模型 $\bf m$ 的前向模型化操作。此外，完全连接神经网络（NN）用于计算模型 ${\bf H_w}{\bf d}^{obs} \approx \bf m$ 从观察数据 ${\bf d}^{obs}$。选择权重 $\lambda_i$ 和 NN 操作确定了一种中的四种 PIML 算法。PIML 具有增强的避免本地最小值的能力，以及可以地方式训练逆转操作，以降低训练数据的需求。然而，PIML 的有效性取决于测试和训练数据之间的相似性。然而，一种可能的策略是先初始化 PIML 架构使用更广泛的地区数据进行预训练，然后进行细化适应特定数据。这种策略类似于大型自然语言模型的预训练和适应不同任务的方法。
</details></li>
</ul>
<hr>
<h2 id="Generative-Intrinsic-Optimization-Intrisic-Control-with-Model-Learning"><a href="#Generative-Intrinsic-Optimization-Intrisic-Control-with-Model-Learning" class="headerlink" title="Generative Intrinsic Optimization: Intrisic Control with Model Learning"></a>Generative Intrinsic Optimization: Intrisic Control with Model Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08100">http://arxiv.org/abs/2310.08100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianfei Ma</li>
<li>for: 这个论文主要为了研究如何通过 incorporating intrinsic motivation with reward maximization来提高机器人的决策效率和适应能力。</li>
<li>methods: 这个论文使用了变量方法来联合学习必要的量和动力学模型，并将其纳入政策迭代算法，以确保优化策略。</li>
<li>results: 这个论文通过 теоретиче分析和实验 validate了其方法的可行性和效果，并开启了在机器人决策中利用内生控制和模型学习来提高样本效率和环境不确定性的潜在应用前景。<details>
<summary>Abstract</summary>
Future sequence represents the outcome after executing the action into the environment. When driven by the information-theoretic concept of mutual information, it seeks maximally informative consequences. Explicit outcomes may vary across state, return, or trajectory serving different purposes such as credit assignment or imitation learning. However, the inherent nature of incorporating intrinsic motivation with reward maximization is often neglected. In this work, we propose a variational approach to jointly learn the necessary quantity for estimating the mutual information and the dynamics model, providing a general framework for incorporating different forms of outcomes of interest. Integrated into a policy iteration scheme, our approach guarantees convergence to the optimal policy. While we mainly focus on theoretical analysis, our approach opens the possibilities of leveraging intrinsic control with model learning to enhance sample efficiency and incorporate uncertainty of the environment into decision-making.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:未来序列表示在环境中执行动的结果。当被动力学概念的共同信息概念驱动时，它寻求最大化的信息后果。显式结果可能因状态、返回或轨迹而变化，以服务不同的目的，如归报学习或模仿学习。然而，在把内在动机与奖励最大化结合的情况下，通常会被忽略。在这项工作中，我们提出了一种变量方法，同时学习必要的量来估计共同信息和动力学模型，提供一个通用的框架，以包括不同形式的结果。将这种方法集成到策略迭代程序中，我们的方法保证 converges to the optimal policy。虽主要关注理论分析，但我们的方法开创了在使用内在控制与模型学习来提高样本效率和在决策中包含环境不确定性的可能性。
</details></li>
</ul>
<hr>
<h2 id="ClimateBERT-NetZero-Detecting-and-Assessing-Net-Zero-and-Reduction-Targets"><a href="#ClimateBERT-NetZero-Detecting-and-Assessing-Net-Zero-and-Reduction-Targets" class="headerlink" title="ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction Targets"></a>ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08096">http://arxiv.org/abs/2310.08096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Schimanski, Julia Bingler, Camilla Hyslop, Mathias Kraus, Markus Leippold</li>
<li>for: 本研究的目的是帮助公共和私人行为者评估各种机构的可持续发展承诺。</li>
<li>methods: 本研究使用了一种新的自动检测方法，包括三个步骤：首先，我们提供了一个专家标注的数据集，包含3.5K个文本样本；其次，我们训练并发布了一种基于自然语言分类器的气候BERT-NetZero模型，用于检测文本中是否包含网零或减少目标；最后，我们使用这种模型分析了两个使用 случа例，包括与传统问答（Q&amp;A）模型结合分析网零或减少目标的批处，以及基于四季报告会讲话稿的 коммуникаtion 模式的演化。</li>
<li>results: 我们的实验结果表明，使用 ClimateBERT-NetZero 模型可以帮助自动检测和分析网零或减少目标，并且可以在大规模数据中提取有用的信息。<details>
<summary>Abstract</summary>
Public and private actors struggle to assess the vast amounts of information about sustainability commitments made by various institutions. To address this problem, we create a novel tool for automatically detecting corporate, national, and regional net zero and reduction targets in three steps. First, we introduce an expert-annotated data set with 3.5K text samples. Second, we train and release ClimateBERT-NetZero, a natural language classifier to detect whether a text contains a net zero or reduction target. Third, we showcase its analysis potential with two use cases: We first demonstrate how ClimateBERT-NetZero can be combined with conventional question-answering (Q&A) models to analyze the ambitions displayed in net zero and reduction targets. Furthermore, we employ the ClimateBERT-NetZero model on quarterly earning call transcripts and outline how communication patterns evolve over time. Our experiments demonstrate promising pathways for extracting and analyzing net zero and emission reduction targets at scale.
</details>
<details>
<summary>摘要</summary>
公共和私人行业努力评估各种机构的可持续发展承诺，但面临巨大的信息量和识别挑战。为解决这问题，我们创建了一种自动检测公司、国家和地区的减少和零排放目标的新工具。我们的方法包括以下三步：第一步，我们提供了一个专家标注的数据集，包含3.5K个文本样本。第二步，我们训练并发布了一个基于自然语言的分类器，用于判断文本是否包含减少或零排放目标。第三步，我们展示了这种分类器的分析潜力，通过两个使用情况：首先，我们将 ClimateBERT-NetZero 与传统的问答（Q&A）模型结合，分析减少和零排放目标中表达的目标。其次，我们使用 ClimateBERT-NetZero 模型对每季财务会议笔记进行分析，并详细描述了时间序列中的沟通趋势。我们的实验结果表明，这种方法可以有效地检测和分析减少和零排放目标。
</details></li>
</ul>
<hr>
<h2 id="Dealing-with-zero-inflated-data-achieving-SOTA-with-a-two-fold-machine-learning-approach"><a href="#Dealing-with-zero-inflated-data-achieving-SOTA-with-a-two-fold-machine-learning-approach" class="headerlink" title="Dealing with zero-inflated data: achieving SOTA with a two-fold machine learning approach"></a>Dealing with zero-inflated data: achieving SOTA with a two-fold machine learning approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08088">http://arxiv.org/abs/2310.08088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jože M. Rožanec, Gašper Petelin, João Costa, Blaž Bertalanič, Gregor Cerar, Marko Guček, Gregor Papa, Dunja Mladenić</li>
<li>for: 本研究旨在应对 zero-inflated 数据，提高模型的预测性能。</li>
<li>methods: 本研究使用了层次模型，将 zero-inflated 数据分解为两个部分，并通过统计学方法来捕捉 zero 的影响。</li>
<li>results: 本研究在实际应用中，包括家用电器分类和机场接驳车需求预测，均得到了优秀的结果。例如，在家用电器分类中，与传统方法相比，提高了精度、回传率、内容积分和 ROC 曲线的平均值。而在机场接驳车需求预测中，使用了 two-fold 模型，与其他模型相比，得到了 statistically significant 的结果。<details>
<summary>Abstract</summary>
In many cases, a machine learning model must learn to correctly predict a few data points with particular values of interest in a broader range of data where many target values are zero. Zero-inflated data can be found in diverse scenarios, such as lumpy and intermittent demands, power consumption for home appliances being turned on and off, impurities measurement in distillation processes, and even airport shuttle demand prediction. The presence of zeroes affects the models' learning and may result in poor performance. Furthermore, zeroes also distort the metrics used to compute the model's prediction quality. This paper showcases two real-world use cases (home appliances classification and airport shuttle demand prediction) where a hierarchical model applied in the context of zero-inflated data leads to excellent results. In particular, for home appliances classification, the weighted average of Precision, Recall, F1, and AUC ROC was increased by 27%, 34%, 49%, and 27%, respectively. Furthermore, it is estimated that the proposed approach is also four times more energy efficient than the SOTA approach against which it was compared to. Two-fold models performed best in all cases when predicting airport shuttle demand, and the difference against other models has been proven to be statistically significant.
</details>
<details>
<summary>摘要</summary>
在许多情况下，机器学习模型需要正确预测一些特定值的数据点，而这些数据点中有许多目标值为零。零扩散数据可以在多种场景中找到，如峰值和间歇性的需求，家用电器的功耗频率是否打开或关闭，蒸馏过程中的杂质测量，以及机场接驳车需求预测。 zeros的存在会影响模型的学习，并可能导致模型的性能差。此外， zeros还会扭曲用于计算模型预测质量的指标。这篇文章介绍了两个实际应用场景（家用电器分类和机场接驳车需求预测），在零扩散数据的上下文中使用层次模型，并取得了优秀的结果。具体来说，对于家用电器分类，使用权重平均的精度、回归率、F1指标和AUC ROC指标的提高分别为27%、34%、49%和27%。此外，还估计该方法比最佳实践（State of the Art，SOTA）方法四倍更能效率。在预测机场接驳车需求方面，两重模型都表现最佳，与其他模型之间的差异被证明为 statistically significant。
</details></li>
</ul>
<hr>
<h2 id="A-Carbon-Tracking-Model-for-Federated-Learning-Impact-of-Quantization-and-Sparsification"><a href="#A-Carbon-Tracking-Model-for-Federated-Learning-Impact-of-Quantization-and-Sparsification" class="headerlink" title="A Carbon Tracking Model for Federated Learning: Impact of Quantization and Sparsification"></a>A Carbon Tracking Model for Federated Learning: Impact of Quantization and Sparsification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08087">http://arxiv.org/abs/2310.08087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Barbieri, Stefano Savazzi, Sanaz Kianoush, Monica Nicoli, Luigi Serio</li>
<li>for: 这篇论文是为了研究 Federated Learning (FL) 方法如何减少数据存储和计算复杂性，以提高环境可持续性。</li>
<li>methods: 这篇论文提出了一个框架，用于实时监测 FL 系统的能源和碳足迹影响。这个框架包括一个碳追踪工具，用于评估不同的 FL 策略。</li>
<li>results: 研究发现，在低能量通信 Situations (i.e., &lt; 25 Kbit&#x2F;Joule) 下，使用 consensus-driven FL 实现可以最大限度减少碳排放。此外，研究还发现，量化和简化操作可以寻求一个平衡点，使得 FL 设计变得可持续。<details>
<summary>Abstract</summary>
Federated Learning (FL) methods adopt efficient communication technologies to distribute machine learning tasks across edge devices, reducing the overhead in terms of data storage and computational complexity compared to centralized solutions. Rather than moving large data volumes from producers (sensors, machines) to energy-hungry data centers, raising environmental concerns due to resource demands, FL provides an alternative solution to mitigate the energy demands of several learning tasks while enabling new Artificial Intelligence of Things (AIoT) applications. This paper proposes a framework for real-time monitoring of the energy and carbon footprint impacts of FL systems. The carbon tracking tool is evaluated for consensus (fully decentralized) and classical FL policies. For the first time, we present a quantitative evaluation of different computationally and communication efficient FL methods from the perspectives of energy consumption and carbon equivalent emissions, suggesting also general guidelines for energy-efficient design. Results indicate that consensus-driven FL implementations should be preferred for limiting carbon emissions when the energy efficiency of the communication is low (i.e., < 25 Kbit/Joule). Besides, quantization and sparsification operations are shown to strike a balance between learning performances and energy consumption, leading to sustainable FL designs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Tight-Time-Space-Lower-Bounds-for-Constant-Pass-Learning"><a href="#Tight-Time-Space-Lower-Bounds-for-Constant-Pass-Learning" class="headerlink" title="Tight Time-Space Lower Bounds for Constant-Pass Learning"></a>Tight Time-Space Lower Bounds for Constant-Pass Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08070">http://arxiv.org/abs/2310.08070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Lyu, Avishay Tal, Hongxun Wu, Junzhao Yang</li>
<li>for: 这个论文的目的是为了证明任何偏好学习算法都需要 quadratic memory 或者 exponential number of samples。</li>
<li>methods: 这个论文使用了多 passes 模型，允许学习者在流中多次访问样本。</li>
<li>results: 这个论文证明了任何偏好学习算法在多 passes 模型下都需要 either $\Omega(n^2)$ 内存大小或者至少 $2^{\sqrt{n}}$ 样本。<details>
<summary>Abstract</summary>
In his breakthrough paper, Raz showed that any parity learning algorithm requires either quadratic memory or an exponential number of samples [FOCS'16, JACM'19]. A line of work that followed extended this result to a large class of learning problems. Until recently, all these results considered learning in the streaming model, where each sample is drawn independently, and the learner is allowed a single pass over the stream of samples. Garg, Raz, and Tal [CCC'19] considered a stronger model, allowing multiple passes over the stream. In the $2$-pass model, they showed that learning parities of size $n$ requires either a memory of size $n^{1.5}$ or at least $2^{\sqrt{n}$ samples. (Their result also generalizes to other learning problems.)   In this work, for any constant $q$, we prove tight memory-sample lower bounds for any parity learning algorithm that makes $q$ passes over the stream of samples. We show that such a learner requires either $\Omega(n^{2})$ memory size or at least $2^{\Omega(n)}$ samples. Beyond establishing a tight lower bound, this is the first non-trivial lower bound for $q$-pass learning for any $q\ge 3$. Similar to prior work, our results extend to any learning problem with many nearly-orthogonal concepts.   We complement the lower bound with an upper bound, showing that parity learning with $q$ passes can be done efficiently with $O(n^2/\log q)$ memory.
</details>
<details>
<summary>摘要</summary>
在他的突破论文中，拉兹（Raz）证明了任何偏置学习算法都需要 Either quadratic memory or an exponential number of samples （FOCS'16, JACM'19）。一条随后的工作扩展了这个结论到了一大类学习问题。直到最近，所有的结果都考虑了学习在流式模型下，每个样本独立地被采样，学习者被允许单次遍历样本流。Garg、拉兹和塔尔（Garg, Raz, and Tal）在 $2$-pass模型中显示了学习parity的大小为 $n$ 需要 Either a memory of size $n^{1.5}$ or at least $2^{\sqrt{n}$ samples （他们的结果也总结于其他学习问题）。在这项工作中，我们证明了任何偏置学习算法在 $q$ 次遍历样本流时，需要 Either $\Omega(n^{2})$ 内存大小或至少 $2^{\Omega(n)}$ 样本。这不仅是首次给出了非微小的下界，还是首次给出了 $q$-pass 学习的非微小下界，其中 $q\ge 3$。我们的结果同样适用于任何具有多个几乎正交概念的学习问题。我们在这篇论文中还提供了一个上界，证明了在 $q$ 次遍历样本流时，可以使用 $O(n^2/\log q)$ 内存来有效地学习 parity。
</details></li>
</ul>
<hr>
<h2 id="ETDock-A-Novel-Equivariant-Transformer-for-Protein-Ligand-Docking"><a href="#ETDock-A-Novel-Equivariant-Transformer-for-Protein-Ligand-Docking" class="headerlink" title="ETDock: A Novel Equivariant Transformer for Protein-Ligand Docking"></a>ETDock: A Novel Equivariant Transformer for Protein-Ligand Docking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08061">http://arxiv.org/abs/2310.08061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqiang Yi, Xu Wan, Yatao Bian, Le Ou-Yang, Peilin Zhao</li>
<li>for: 预测蛋白质和抗体之间的吸附是药物发现中的关键和挑战之一，但传统的吸附方法主要依靠分数函数，深度学习基于的吸附方法通常忽略蛋白质和抗体的3D空间信息以及抗体的图形级特征，这限制了其性能。</li>
<li>methods: 我们提出了一种对称变换神经网络模型 для蛋白质-抗体吸附pose预测，该方法包括精细 Ligand 的特征处理、TAMformer模块来学习蛋白质和抗体的表达，以及基于预测距离矩阵的迭代优化来生成高精度的抗体pose。</li>
<li>results: 我们在实际数据集上进行了实验，结果表明，我们的模型可以达到状态畅的性能。<details>
<summary>Abstract</summary>
Predicting the docking between proteins and ligands is a crucial and challenging task for drug discovery. However, traditional docking methods mainly rely on scoring functions, and deep learning-based docking approaches usually neglect the 3D spatial information of proteins and ligands, as well as the graph-level features of ligands, which limits their performance. To address these limitations, we propose an equivariant transformer neural network for protein-ligand docking pose prediction. Our approach involves the fusion of ligand graph-level features by feature processing, followed by the learning of ligand and protein representations using our proposed TAMformer module. Additionally, we employ an iterative optimization approach based on the predicted distance matrix to generate refined ligand poses. The experimental results on real datasets show that our model can achieve state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
预测蛋白和小分子的吸附是药物发现中的关键和挑战。传统的吸附方法主要基于得分函数，而深度学习基于吸附方法通常忽略蛋白和小分子的3D空间信息以及ligand的图形级特征，这限制了其性能。为解决这些限制，我们提议一种具有等变性的变换神经网络 для蛋白-小分子吸附姿态预测。我们的方法包括ligand图形级特征的融合以及使用我们提出的TAMformer模块来学习蛋白和小分子表示。此外，我们采用基于预测距离矩阵的迭代优化方法来生成高精度的ligand姿态。实验结果表明，我们的模型可以在真实数据上达到顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="LGL-BCI-A-Lightweight-Geometric-Learning-Framework-for-Motor-Imagery-Based-Brain-Computer-Interfaces"><a href="#LGL-BCI-A-Lightweight-Geometric-Learning-Framework-for-Motor-Imagery-Based-Brain-Computer-Interfaces" class="headerlink" title="LGL-BCI: A Lightweight Geometric Learning Framework for Motor Imagery-Based Brain-Computer Interfaces"></a>LGL-BCI: A Lightweight Geometric Learning Framework for Motor Imagery-Based Brain-Computer Interfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08051">http://arxiv.org/abs/2310.08051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianchao Lu, Yuzhe Tian, Yang Zhang, Jiaqi Ge, Quan Z. Sheng, Xi Zheng</li>
<li>for: 这个研究旨在提高电enzephalogram (EEG)-based Motor Imagery (MI) 任务的精度和效率，并且探索geometry deep learning的应用在脑机器接口 (BCI) 领域。</li>
<li>methods: 本研究使用Geometric Deep Learning Framework for EEG processing in non-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD) Manifold space, 并且提出了一个EEG通道选择解决方案，通过将SPD矩阵维度减少，以提高推断速度。</li>
<li>results: 实验结果显示LGL-BCI的精度和效率明显超过现有解决方案（$82.54%$ vs. $62.22%$），并且具有较少的参数（64.9M）。<details>
<summary>Abstract</summary>
Brain-Computer Interfaces (BCIs) are a groundbreaking technology for interacting with external devices using brain signals. Despite advancements, electroencephalogram (EEG)-based Motor Imagery (MI) tasks face challenges like amplitude and phase variability, and complex spatial correlations, with a need for smaller model size and faster inference. This study introduces the LGL-BCI framework, employing a Geometric Deep Learning Framework for EEG processing in non-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD) Manifold space. LGL-BCI offers robust EEG data representation and captures spatial correlations. We propose an EEG channel selection solution via a feature decomposition algorithm to reduce SPD matrix dimensionality, with a lossless transformation boosting inference speed. Extensive experiments show LGL-BCI's superior accuracy and efficiency compared to current solutions, highlighting geometric deep learning's potential in MI-BCI applications. The efficiency, assessed on two public EEG datasets and two real-world EEG devices, significantly outperforms the state-of-the-art solution in accuracy ($82.54\%$ versus $62.22\%$) with fewer parameters (64.9M compared to 183.7M).
</details>
<details>
<summary>摘要</summary>
Brain-Computer Interfaces (BCIs) 是一种创新的技术，通过脑电响应与外部设备交互。尽管有进步，但EEG基于 Motor Imagery (MI) 任务还面临着幅度和相位变化，复杂的空间相关性等问题，需要更小的模型大小和更快的推理。本研究介绍了LGL-BCI框架，利用几何深度学习框架对EEG数据进行处理，特别是在非欧几何度量空间中的Symmetric Positive Definite（SPD） manifesto空间。LGL-BCI可以准确地表示EEG数据，并捕捉空间相关性。我们提出了基于特征分解算法的EEG通道选择解决方案，以减少SPD矩阵维度，并通过无损变换提高推理速度。广泛的实验表明LGL-BCI的精度和效率都高于当前解决方案，这highlights几何深度学习在MI-BCI应用中的潜力。LGL-BCI的效率，在两个公共EEG数据集和两个实际世界EEG设备上进行评估，明显超过了当前解决方案的精度($82.54\%$ VS $62.22\%$)，同时具有 fewer parameters（64.9M VS 183.7M）。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Relationship-Between-Model-Architecture-and-In-Context-Learning-Ability"><a href="#Exploring-the-Relationship-Between-Model-Architecture-and-In-Context-Learning-Ability" class="headerlink" title="Exploring the Relationship Between Model Architecture and In-Context Learning Ability"></a>Exploring the Relationship Between Model Architecture and In-Context Learning Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08049">http://arxiv.org/abs/2310.08049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ivnle/synth-icl">https://github.com/ivnle/synth-icl</a></li>
<li>paper_authors: Ivan Lee, Nan Jiang, Taylor Berg-Kirkpatrick</li>
<li>for: 本研究探讨模型架构与 Context-aware 学习能力之间的关系。</li>
<li>methods: 我们对十五种模型架构进行了一系列的synthetic Context-aware 学习任务测试。这些选择的架构包括Recurrent Neural Networks、Convolutional Neural Networks、Transformers以及emerging attention的替代品。我们发现所有考虑的架构都可以在某些条件下进行Context-aware 学习，但是当任务复杂度增加时，当代架构表现最佳。</li>
<li>results: 我们的追加实验表明，不同架构对于 Hyperparameter 设置和训练动态的影响不同。另外，我们发现一些 emerging attention 替代品在 Context-aware 学习中表现更加稳定和 Robust，这可能开启了将Context-aware 学习扩展到更多的 Context-aware 示例的未来可能性。<details>
<summary>Abstract</summary>
What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps towards answering this question. In particular, we evaluate fifteen model architectures across a suite of synthetic in-context learning tasks. The selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, and emerging attention alternatives. We discover that all considered architectures can perform in-context learning under certain conditions. However, contemporary architectures are found to be the best performing, especially as task complexity grows. Additionally, our follow-up experiments delve into various factors that influence in-context learning. We observe varied sensitivities among architectures with respect to hyperparameter settings. Our study of training dynamics reveals that certain architectures exhibit a smooth, progressive learning trajectory, while others demonstrate periods of stagnation followed by abrupt mastery of the task. Finally, and somewhat surprisingly, we find that several emerging attention alternatives are more robust in-context learners than transformers; since such approaches have constant-sized memory footprints at inference time, this result opens the future possibility of scaling up in-context learning to vastly larger numbers of in-context examples.
</details>
<details>
<summary>摘要</summary>
What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps towards answering this question. In particular, we evaluate fifteen model architectures across a suite of synthetic in-context learning tasks. The selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, and emerging attention alternatives. We discover that all considered architectures can perform in-context learning under certain conditions. However, contemporary architectures are found to be the best performing, especially as task complexity grows. Additionally, our follow-up experiments delve into various factors that influence in-context learning. We observe varied sensitivities among architectures with respect to hyperparameter settings. Our study of training dynamics reveals that certain architectures exhibit a smooth, progressive learning trajectory, while others demonstrate periods of stagnation followed by abrupt mastery of the task. Finally, and somewhat surprisingly, we find that several emerging attention alternatives are more robust in-context learners than transformers; since such approaches have constant-sized memory footprints at inference time, this result opens the future possibility of scaling up in-context learning to vastly larger numbers of in-context examples.Here is the translation in Traditional Chinese:这篇研究对于模型架构和内容学习的关系进行了初步的探索。我们评估了15种模型架构，包括循环神经网络、卷积神经网络、转移器和新兴注意力方法。我们发现所有考虑的架构都可以在某些情况下进行内容学习，但是现代架构在任务复杂度增加时表现最佳。我们还进行了多种因素影响内容学习的实验，发现不同的架构对于内容学习有不同的敏感性。我们的训练动态研究发现一些架构在进行学习时会展示平滑、进步的学习曲线，而其他架构则会在进行学习时出现停滞期后快速掌握任务。最后，我们发现一些新兴注意力方法在内容学习中表现更稳定和更强，这可能是因为这些方法在测试时具有常量大小的内存占用量，这开启了未来可以扩展内容学习至更大量的内容示例的可能性。
</details></li>
</ul>
<hr>
<h2 id="SEE-OoD-Supervised-Exploration-For-Enhanced-Out-of-Distribution-Detection"><a href="#SEE-OoD-Supervised-Exploration-For-Enhanced-Out-of-Distribution-Detection" class="headerlink" title="SEE-OoD: Supervised Exploration For Enhanced Out-of-Distribution Detection"></a>SEE-OoD: Supervised Exploration For Enhanced Out-of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08040">http://arxiv.org/abs/2310.08040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyang Song, Wenbo Sun, Maher Nouiehed, Raed Al Kontar, Judy Jin</li>
<li>for: 提高Out-of-Distribution（OoD）检测精度</li>
<li>methods: 基于 Wasserstein 分数的生成对抗训练方案</li>
<li>results: 在多个计算机视觉数据集上表现出超过现有技术，并且在未看到OoD数据时表现出更好的普适性<details>
<summary>Abstract</summary>
Current techniques for Out-of-Distribution (OoD) detection predominantly rely on quantifying predictive uncertainty and incorporating model regularization during the training phase, using either real or synthetic OoD samples. However, methods that utilize real OoD samples lack exploration and are prone to overfit the OoD samples at hand. Whereas synthetic samples are often generated based on features extracted from training data, rendering them less effective when the training and OoD data are highly overlapped in the feature space. In this work, we propose a Wasserstein-score-based generative adversarial training scheme to enhance OoD detection accuracy, which, for the first time, performs data augmentation and exploration simultaneously under the supervision of limited OoD samples. Specifically, the generator explores OoD spaces and generates synthetic OoD samples using feedback from the discriminator, while the discriminator exploits both the observed and synthesized samples for OoD detection using a predefined Wasserstein score. We provide theoretical guarantees that the optimal solutions of our generative scheme are statistically achievable through adversarial training in empirical settings. We then demonstrate that the proposed method outperforms state-of-the-art techniques on various computer vision datasets and exhibits superior generalizability to unseen OoD data.
</details>
<details>
<summary>摘要</summary>
现有的Out-of-Distribution（OoD）检测技术主要基于计量预测不确定性和在训练阶段添加模型规则，使用实际或 sintetic OoD 样本。然而，使用实际 OoD 样本的方法缺乏探索，容易过拟合当前 OoD 样本。而使用 sintetic 样本则是基于训练数据中提取的特征，当训练和 OoD 数据在特征空间高度重叠时，这些 sintetic 样本可能变得不效果。在这种情况下，我们提出一种基于 Wasserstein 分数的生成敌对训练方案，以提高 OoD 检测精度。这种方法在supervise limited OoD 样本下同时进行数据扩充和探索。具体来说，生成器通过对 Discriminator 提供反馈，在 OoD 空间中探索并生成 sintetic OoD 样本，而 Discriminator 则是使用观察到的和 sintetic 样本来进行 OoD 检测，使用预定的 Wasserstein 分数。我们提供了理论保证，表明我们的生成方案在实际情况下通过对抗训练来实现最优解。我们然后通过在多种计算机视觉数据集上进行比较，证明了我们的方法在不同的 OoD 数据集上具有优于现状技术的普适性和检测精度。
</details></li>
</ul>
<hr>
<h2 id="ZEST-Attention-based-Zero-Shot-Learning-for-Unseen-IoT-Device-Classification"><a href="#ZEST-Attention-based-Zero-Shot-Learning-for-Unseen-IoT-Device-Classification" class="headerlink" title="ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device Classification"></a>ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08036">http://arxiv.org/abs/2310.08036</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Binghui99/ZEST">https://github.com/Binghui99/ZEST</a></li>
<li>paper_authors: Binghui Wu, Philipp Gysel, Dinil Mon Divakaran, Mohan Gurusamy</li>
<li>for: 本研究旨在提出一种基于自注意力的零例学习（ZEST）框架，用于分类未经训练过的 IoT 设备。</li>
<li>methods: ZEST 框架包括 i) 基于自注意力的网络特征提取器（SANE），用于提取 IoT 流量的隐藏空间表示；ii) 使用这些隐藏特征生成 pseudo 数据，并 iii) 使用这些生成的 pseudo 数据进行预测设备分类。</li>
<li>results: 我们在实际 IoT 流量数据上进行了广泛的实验，结果表明：i) ZEST 在基础模型上显著提高了准确率；ii) ZEST 能够更好地提取有意义的表示，比 LSTM 更常用于网络流量模型。<details>
<summary>Abstract</summary>
Recent research works have proposed machine learning models for classifying IoT devices connected to a network. However, there is still a practical challenge of not having all devices (and hence their traffic) available during the training of a model. This essentially means, during the operational phase, we need to classify new devices not seen during the training phase. To address this challenge, we propose ZEST -- a ZSL (zero-shot learning) framework based on self-attention for classifying both seen and unseen devices. ZEST consists of i) a self-attention based network feature extractor, termed SANE, for extracting latent space representations of IoT traffic, ii) a generative model that trains a decoder using latent features to generate pseudo data, and iii) a supervised model that is trained on the generated pseudo data for classifying devices. We carry out extensive experiments on real IoT traffic data; our experiments demonstrate i) ZEST achieves significant improvement (in terms of accuracy) over the baselines; ii) ZEST is able to better extract meaningful representations than LSTM which has been commonly used for modeling network traffic.
</details>
<details>
<summary>摘要</summary>
现代研究工作已经提出了基于机器学习的互联网设备分类模型。然而，实际上还存在一个实际挑战，即在训练阶段没有所有设备（以及其交换流量）可用。这意味着在运维阶段需要将新设备分类，这些设备未在训练阶段看到。为解决这个挑战，我们提出了ZEST——基于自注意力的零shot学习（ZSL）框架，用于分类已知和未知的设备。ZEST包括以下三个部分：1. 基于自注意力的网络特征提取器（SANE），用于提取互联网流量的隐藏空间表示。2. 使用隐藏特征进行生成模型，用于生成 Pseudo 数据。3. 使用生成的 Pseudo 数据进行超参数学习，用于分类设备。我们对实际的互联网流量数据进行了广泛的实验，我们的实验结果表明：1. ZEST 相比基elines提供了显著改进（准确率）。2. ZEST 能够更好地提取有意义的表示，比如 LSTM 通常用于网络流量模型。
</details></li>
</ul>
<hr>
<h2 id="Local-Graph-Clustering-with-Noisy-Labels"><a href="#Local-Graph-Clustering-with-Noisy-Labels" class="headerlink" title="Local Graph Clustering with Noisy Labels"></a>Local Graph Clustering with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08031">http://arxiv.org/abs/2310.08031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Artur Back de Luca, Kimon Fountoulakis, Shenghao Yang<br>for:这个论文主要研究的是如何使用噪声节点标签来提高本地图表 clustering 性能。methods:该论文提出了一种基于扩展图的本地 clustering 方法，使用噪声节点标签来提高 clustering 性能。此外，论文还提出了一种基于权重图的方法，通过在噪声标签上进行扩散来提高 clustering 性能。results:实验结果表明，使用噪声节点标签可以获得可靠的节点标签，并且通过在权重图上进行扩散可以提高本地 clustering 性能。论文还进行了多个实验，并证明了这种方法可以在多种实际应用中提高 clustering 性能。<details>
<summary>Abstract</summary>
The growing interest in machine learning problems over graphs with additional node information such as texts, images, or labels has popularized methods that require the costly operation of processing the entire graph. Yet, little effort has been made to the development of fast local methods (i.e. without accessing the entire graph) that extract useful information from such data. To that end, we propose a study of local graph clustering using noisy node labels as a proxy for additional node information. In this setting, nodes receive initial binary labels based on cluster affiliation: 1 if they belong to the target cluster and 0 otherwise. Subsequently, a fraction of these labels is flipped. We investigate the benefits of incorporating noisy labels for local graph clustering. By constructing a weighted graph with such labels, we study the performance of graph diffusion-based local clustering method on both the original and the weighted graphs. From a theoretical perspective, we consider recovering an unknown target cluster with a single seed node in a random graph with independent noisy node labels. We provide sufficient conditions on the label noise under which, with high probability, using diffusion in the weighted graph yields a more accurate recovery of the target cluster. This approach proves more effective than using the given labels alone or using diffusion in the label-free original graph. Empirically, we show that reliable node labels can be obtained with just a few samples from an attributed graph. Moreover, utilizing these labels via diffusion in the weighted graph leads to significantly better local clustering performance across several real-world datasets, improving F1 scores by up to 13%.
</details>
<details>
<summary>摘要</summary>
“对于具有附加节点信息的图像进行学习问题的兴趣不断增长，然而现有的方法通常需要处理整个图像，即使这些方法可能会带来成本高昂的计算成本。为了开发更快速的本地方法，我们提出了一种基于不纯净节点标签的本地图像归类方法。在这种设定下，每个节点都会Initially receive binary labels based on cluster affiliation：1 if they belong to the target cluster and 0 otherwise。然后，一部分这些标签会被翻转。我们研究了在Weighted graph上使用这些标签来进行图像归类的效果。从理论上看，我们考虑了在Random graph上recover an unknown target cluster with a single seed node。我们提供了对标签噪声的condition under which, with high probability, using diffusion in the weighted graph yields a more accurate recovery of the target cluster。这种方法比使用给定标签alone或使用 diffusion in the label-free original graph更有效。Empirically, we show that reliable node labels can be obtained with just a few samples from an attributed graph。此外，通过使用这些标签via diffusion in the weighted graph，可以在多个实际数据集上获得显著提高的本地归类性能，提高F1分数达13%。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Robust-1-bit-Compressed-Sensing-with-Iterative-Hard-Thresholding"><a href="#Robust-1-bit-Compressed-Sensing-with-Iterative-Hard-Thresholding" class="headerlink" title="Robust 1-bit Compressed Sensing with Iterative Hard Thresholding"></a>Robust 1-bit Compressed Sensing with Iterative Hard Thresholding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08019">http://arxiv.org/abs/2310.08019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Namiko Matsumoto, Arya Mazumdar</li>
<li>for: 本文研究了一种噪声损坏的1比特压缩感知问题，即在一些量化为签字的噪声损坏中估计$k$-次稀畴向量$x\in S^{n-1}$。</li>
<li>methods: 本文使用了一种名为BIHT的 proximal 梯度下降算法，该算法在一个特定的损失函数上进行 iterative 硬阈值处理，以估计$x$。</li>
<li>results: 本文表明，BIHT算法在噪声损坏情况下可以提供更好的结果，并且可以在 $\tilde{O}(\epsilon+\tau)$ 误差范围内估计$x$，其中 $\epsilon$ 是约束误差，$\tau$ 是噪声损坏率。这个结果表明了iterative 硬阈值处理在噪声损坏情况下的稳定性。<details>
<summary>Abstract</summary>
In 1-bit compressed sensing, the aim is to estimate a $k$-sparse unit vector $x\in S^{n-1}$ within an $\epsilon$ error (in $\ell_2$) from minimal number of linear measurements that are quantized to just their signs, i.e., from measurements of the form $y = \mathrm{Sign}(\langle a, x\rangle).$ In this paper, we study a noisy version where a fraction of the measurements can be flipped, potentially by an adversary. In particular, we analyze the Binary Iterative Hard Thresholding (BIHT) algorithm, a proximal gradient descent on a properly defined loss function used for 1-bit compressed sensing, in this noisy setting. It is known from recent results that, with $\tilde{O}(\frac{k}{\epsilon})$ noiseless measurements, BIHT provides an estimate within $\epsilon$ error. This result is optimal and universal, meaning one set of measurements work for all sparse vectors. In this paper, we show that BIHT also provides better results than all known methods for the noisy setting. We show that when up to $\tau$-fraction of the sign measurements are incorrect (adversarial error), with the same number of measurements as before, BIHT agnostically provides an estimate of $x$ within an $\tilde{O}(\epsilon+\tau)$ error, maintaining the universality of measurements. This establishes stability of iterative hard thresholding in the presence of measurement error. To obtain the result, we use the restricted approximate invertibility of Gaussian matrices, as well as a tight analysis of the high-dimensional geometry of the adversarially corrupted measurements.
</details>
<details>
<summary>摘要</summary>
在1比特压缩感知中，目标是从最小的线性测量中估计一个$k$-简单向量$x\in S^{n-1}$，保证错误在$\ell_2$范围内偏差不超过$\epsilon$。在这篇论文中，我们研究了一种噪声损失的情况，其中一部分测量可能会被反转，可能由一个对手引起。特别是，我们分析了使用 proximal 梯度下降的损失函数来实现1比特压缩感知的Binary Iterative Hard Thresholding（BIHT）算法在这种噪声设定下的性能。已知于最近的结果是，只需要$\tilde{O}\left(\frac{k}{\epsilon}\right)$个噪声无关的测量，BIHT可以提供 $\epsilon$ 误差内的估计。这个结果是最佳的和通用的，意味着一组测量可以适用于所有简单向量。在这篇论文中，我们证明BIHT在噪声设定下也比所有已知方法更好。我们证明，当最多 $\tau$ 部分签字测量错误（对手错误）时，BIHT可以在 $\tilde{O}(\epsilon+\tau)$ 误差内提供 $x$ 的估计，保持测量的 universality。这个结果证明了迭代坚固resholding在测量误差存在的情况下的稳定性。为了获得这个结果，我们使用了受限的近似逆Matrix，以及高维ensional的对抗性测量的紧张分析。
</details></li>
</ul>
<hr>
<h2 id="Why-Train-More-Effective-and-Efficient-Membership-Inference-via-Memorization"><a href="#Why-Train-More-Effective-and-Efficient-Membership-Inference-via-Memorization" class="headerlink" title="Why Train More? Effective and Efficient Membership Inference via Memorization"></a>Why Train More? Effective and Efficient Membership Inference via Memorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08015">http://arxiv.org/abs/2310.08015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihye Choi, Shruti Tople, Varun Chandrasekaran, Somesh Jha</li>
<li>for: 本研究旨在攻击机器学习模型的私人训练数据，以便破坏个人隐私和其他高级威胁。</li>
<li>methods: 本研究使用了黑盒攻击方法，通过访问数据分布来训练陌生模型。</li>
<li>results: 研究表明，通过策略选择敏感样本，攻击者可以最大化攻击成功的可能性，同时减少陌生模型的数量。<details>
<summary>Abstract</summary>
Membership Inference Attacks (MIAs) aim to identify specific data samples within the private training dataset of machine learning models, leading to serious privacy violations and other sophisticated threats. Many practical black-box MIAs require query access to the data distribution (the same distribution where the private data is drawn) to train shadow models. By doing so, the adversary obtains models trained "with" or "without" samples drawn from the distribution, and analyzes the characteristics of the samples under consideration. The adversary is often required to train more than hundreds of shadow models to extract the signals needed for MIAs; this becomes the computational overhead of MIAs. In this paper, we propose that by strategically choosing the samples, MI adversaries can maximize their attack success while minimizing the number of shadow models. First, our motivational experiments suggest memorization as the key property explaining disparate sample vulnerability to MIAs. We formalize this through a theoretical bound that connects MI advantage with memorization. Second, we show sample complexity bounds that connect the number of shadow models needed for MIAs with memorization. Lastly, we confirm our theoretical arguments with comprehensive experiments; by utilizing samples with high memorization scores, the adversary can (a) significantly improve its efficacy regardless of the MIA used, and (b) reduce the number of shadow models by nearly two orders of magnitude compared to state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AutoFHE-Automated-Adaption-of-CNNs-for-Efficient-Evaluation-over-FHE"><a href="#AutoFHE-Automated-Adaption-of-CNNs-for-Efficient-Evaluation-over-FHE" class="headerlink" title="AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE"></a>AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08012">http://arxiv.org/abs/2310.08012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Ao, Vishnu Naresh Boddeti</li>
<li>for: 这 paper 的目的是提出一种自动化深度 convolutional neural networks (CNNs) 的安全执行方法，以便在 RNS-CKKS 上进行加密数据处理。</li>
<li>methods: 这 paper 使用了一种叫做 AutoFHE 的方法，它利用层次混合度 polynomial activation functions，并通过多目标优化来调整 homomorphic evaluation 架构以适应不同的 CNN 架构。</li>
<li>results: 实验结果表明，AutoFHE 可以在 RNS-CKKS 加密 CIFAR 数据集上提高安全执行的速度，比较传统方法快得多，同时也可以提高准确率。相比TFHE，AutoFHE 可以提高执行速度和准确率的同时，达到 $103\times$ 和 3.46% 的提升。<details>
<summary>Abstract</summary>
Secure inference of deep convolutional neural networks (CNNs) under RNS-CKKS involves polynomial approximation of unsupported non-linear activation functions. However, existing approaches have three main limitations: 1) Inflexibility: The polynomial approximation and associated homomorphic evaluation architecture are customized manually for each CNN architecture and do not generalize to other networks. 2) Suboptimal Approximation: Each activation function is approximated instead of the function represented by the CNN. 3) Restricted Design: Either high-degree or low-degree polynomial approximations are used. The former retains high accuracy but slows down inference due to bootstrapping operations, while the latter accelerates ciphertext inference but compromises accuracy. To address these limitations, we present AutoFHE, which automatically adapts standard CNNs for secure inference under RNS-CKKS. The key idea is to adopt layerwise mixed-degree polynomial activation functions, which are optimized jointly with the homomorphic evaluation architecture in terms of the placement of bootstrapping operations. The problem is modeled within a multi-objective optimization framework to maximize accuracy and minimize the number of bootstrapping operations. AutoFHE can be applied flexibly on any CNN architecture, and it provides diverse solutions that span the trade-off between accuracy and latency. Experimental evaluation over RNS-CKKS encrypted CIFAR datasets shows that AutoFHE accelerates secure inference by $1.32\times$ to $1.8\times$ compared to methods employing high-degree polynomials. It also improves accuracy by up to 2.56% compared to methods using low-degree polynomials. Lastly, AutoFHE accelerates inference and improves accuracy by $103\times$ and 3.46%, respectively, compared to CNNs under TFHE.
</details>
<details>
<summary>摘要</summary>
安全的深度卷积神经网络（CNN）在RNS-CKKS中进行推理需要多项式近似未支持的非线性活动函数。然而，现有的方法具有以下三个主要的限制：1. 不灵活：用于CNN的多项式近似和相关的卷积评估架构是手动定制的，不能泛化到其他网络。2. 不优化的近似：对于每个CNN，都使用多项式近似，而不是网络中的功能表示。3. 局限的设计：使用高度或低度的多项式近似，前者保持高精度，但增加了卷积评估的速度，后者快速了ciphertext推理，但牺牲了精度。为了解决这些限制，我们提出了AutoFHE，它自动将标准的CNN适应安全地进行RNS-CKKS中的推理。AutoFHE的关键思想是采用层次混合度多项式活动函数，并在卷积评估架构中对它们进行优化，以最大化精度和减少卷积评估操作数量。问题被模型为多目标优化框架，以优化精度和响应时间之间的负荷。AutoFHE可以灵活应用于任何CNN架构，并提供了多种解决方案，横跨精度和响应时间之间的负荷Trade-off。实验表明，AutoFHE比使用高度多项式快速了1.32倍至1.8倍的安全推理，并提高了精度。同时，AutoFHE也提高了精度和响应时间之间的负荷Trade-off，相比TFHE， acceleration和精度提高分别为103倍和3.46%。
</details></li>
</ul>
<hr>
<h2 id="LEMON-Lossless-model-expansion"><a href="#LEMON-Lossless-model-expansion" class="headerlink" title="LEMON: Lossless model expansion"></a>LEMON: Lossless model expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07999">http://arxiv.org/abs/2310.07999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yite Wang, Jiahao Su, Hanlin Lu, Cong Xie, Tianyi Liu, Jianbo Yuan, Haibin Lin, Ruoyu Sun, Hongxia Yang</li>
<li>for: 这篇论文的目的是提高深度神经网络的扩展和优化，以提高其表现和智能推理能力。</li>
<li>methods: 本文使用的方法是将小型神经网络的发现获得到大型神经网络的初始化，并且运用特定的学习率调询器来训练大型神经网络。</li>
<li>results: 实验结果显示，使用LEMON可以降低对于Vision Transformers和BERT等模型的训练时间和计算成本，相比训练 desde scratch，可以节省56.7%的计算成本和33.2%的训练时间。<details>
<summary>Abstract</summary>
Scaling of deep neural networks, especially Transformers, is pivotal for their surging performance and has further led to the emergence of sophisticated reasoning capabilities in foundation models. Such scaling generally requires training large models from scratch with random initialization, failing to leverage the knowledge acquired by their smaller counterparts, which are already resource-intensive to obtain. To tackle this inefficiency, we present $\textbf{L}$ossl$\textbf{E}$ss $\textbf{MO}$del Expansio$\textbf{N}$ (LEMON), a recipe to initialize scaled models using the weights of their smaller but pre-trained counterparts. This is followed by model training with an optimized learning rate scheduler tailored explicitly for the scaled models, substantially reducing the training time compared to training from scratch. Notably, LEMON is versatile, ensuring compatibility with various network structures, including models like Vision Transformers and BERT. Our empirical results demonstrate that LEMON reduces computational costs by 56.7% for Vision Transformers and 33.2% for BERT when compared to training from scratch.
</details>
<details>
<summary>摘要</summary>
深度神经网络的扩展，特别是转换器，对其表现的增长和引入了更加复杂的理解能力而言是非常重要的。这种扩展通常需要从零开始训练大型模型，不能利用已经训练过的小型模型所获得的知识，这些模型已经是资源占用的了。为解决这种不效率，我们提出了$\textbf{L}$ossl$\textbf{E}$ss $\textbf{MO}$del Expansio$\textbf{N}$（LEMON），一种初始化扩展模型的方法，使用小型模型已经训练过的权重。然后，通过对扩展模型进行优化的学习率调整器，减少了训练时间与从零开始训练的计算成本。值得注意的是，LEMON具有兼容性，可以与不同的网络结构相结合，包括视觉转换器和BERT等模型。我们的实验结果表明，LEMON可以在视觉转换器和BERT等模型上减少计算成本，相比于从零开始训练，减少了56.7%和33.2%。
</details></li>
</ul>
<hr>
<h2 id="Multi-View-Variational-Autoencoder-for-Missing-Value-Imputation-in-Untargeted-Metabolomics"><a href="#Multi-View-Variational-Autoencoder-for-Missing-Value-Imputation-in-Untargeted-Metabolomics" class="headerlink" title="Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics"></a>Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07990">http://arxiv.org/abs/2310.07990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhao, Kuan-Jui Su, Chong Wu, Xuewei Cao, Qiuying Sha, Wu Li, Zhe Luo, Tian Qin, Chuan Qiu, Lan Juan Zhao, Anqi Liu, Lindong Jiang, Xiao Zhang, Hui Shen, Weihua Zhou, Hong-Wen Deng</li>
<li>For: The paper aims to improve the accuracy of metabolomics data imputation by integrating whole-genome sequencing (WGS) data with metabolomics data.* Methods: The proposed method uses a multi-view variational autoencoder to jointly model the burden score, polygenetic risk score (PGS), and linkage disequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature extraction and missing metabolomics data imputation.* Results: The proposed method achieved r2-scores &gt; 0.01 for 71.55% of metabolites, demonstrating its superiority compared to conventional imputation techniques.Here are the three points in Simplified Chinese:* For: 这个研究想要使用整个基因组序列（WGS）数据来改进大规模精确的 метабо树据报告。* Methods: 这种方法使用多视图自适应变换器来同时模型负担分数、多因素风险分数（PGS）和链接不均衡（LD）剔除单树谱分割（SNPs），以提取特征和缺失的 метабо树据。* Results: 这种方法在实验数据上显示了与传统填充方法相比的超越性，其中71.55%的代表物质的r2-分数大于0.01。<details>
<summary>Abstract</summary>
Background: Missing data is a common challenge in mass spectrometry-based metabolomics, which can lead to biased and incomplete analyses. The integration of whole-genome sequencing (WGS) data with metabolomics data has emerged as a promising approach to enhance the accuracy of data imputation in metabolomics studies. Method: In this study, we propose a novel method that leverages the information from WGS data and reference metabolites to impute unknown metabolites. Our approach utilizes a multi-view variational autoencoder to jointly model the burden score, polygenetic risk score (PGS), and linkage disequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature extraction and missing metabolomics data imputation. By learning the latent representations of both omics data, our method can effectively impute missing metabolomics values based on genomic information. Results: We evaluate the performance of our method on empirical metabolomics datasets with missing values and demonstrate its superiority compared to conventional imputation techniques. Using 35 template metabolites derived burden scores, PGS and LD-pruned SNPs, the proposed methods achieved r2-scores > 0.01 for 71.55% of metabolites. Conclusion: The integration of WGS data in metabolomics imputation not only improves data completeness but also enhances downstream analyses, paving the way for more comprehensive and accurate investigations of metabolic pathways and disease associations. Our findings offer valuable insights into the potential benefits of utilizing WGS data for metabolomics data imputation and underscore the importance of leveraging multi-modal data integration in precision medicine research.
</details>
<details>
<summary>摘要</summary>
背景：大量数据缺失是生物 массспектрометрии中的常见挑战，可能导致偏执和缺失的分析。整合整个基因组序列数据（WGS）与生物 массспектрометria数据已经出现为提高生物 массспектрометria数据缺失的精度的有力的方法。方法：在这种研究中，我们提出了一种新的方法，利用WGS数据和参照物质的信息来填充未知的代谢物质。我们的方法使用多视图变换自适应器来同时模型负担分数、多型风险分数（PGS）和遗传关系不均匀变异（LD）排序单核苷变异（SNPs），以提取特征和缺失的生物 массспектрометria数据。通过学习两种各自的隐藏表示，我们的方法可以有效地填充缺失的生物 массспектрометria数据。结果：我们对实验室中的生物 массспектрометria数据进行评估，并证明了我们的方法在缺失数据的情况下的表现较为优于传统填充技术。使用35个模板代谢物质的负担分数、PGS和LD排序SNPs，我们的方法在71.55%的代谢物质上达到了r2分数>0.01。结论：整合WGS数据在生物 массспектрометria填充中不仅提高了数据完teness，还提高了下游分析，开创了更全面和准确的代谢 PATHway和疾病相关性的研究。我们的发现对于利用WGS数据进行生物 массспектрометria填充的潜在利益和多Modal数据集成在精度医学研究中的重要性提供了有价值的意见。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Forward-Relaying-A-Novel-Framework-Towards-6G-Cooperative-Communications"><a href="#Semantic-Forward-Relaying-A-Novel-Framework-Towards-6G-Cooperative-Communications" class="headerlink" title="Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative Communications"></a>Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07987">http://arxiv.org/abs/2310.07987</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linwest/Semantic_Forward">https://github.com/linwest/Semantic_Forward</a></li>
<li>paper_authors: Wensheng Lin, Yuna Yan, Lixin Li, Zhu Han, Tad Matsumoto</li>
<li>for: 该文章提出了一种新的 relaying 框架，即 semantic-forward (SF)，用于 sixth-generation (6G) 无线网络中的合作通信。</li>
<li>methods: 该文章使用了semantic feature extraction和传输，以减少前进 payload，并提高网络对内链错误的Robustness。基于合作通信与侧信息的理论基础以及 turbo 原理，文章设计了一种 joint source-channel coding 算法，用于在 destination 中循环交换 extrinsic information，以提高解码 gain。</li>
<li>results:  simulation results 表明，即使在坏通道条件下，SF relaying 仍然可以有效地提高 recovered information 质量。<details>
<summary>Abstract</summary>
This letter proposes a novel relaying framework, semantic-forward (SF), for cooperative communications towards the sixth-generation (6G) wireless networks. The SF relay extracts and transmits the semantic features, which reduces forwarding payload, and also improves the network robustness against intra-link errors. Based on the theoretical basis for cooperative communications with side information and the turbo principle, we design a joint source-channel coding algorithm to iteratively exchange the extrinsic information for enhancing the decoding gains at the destination. Surprisingly, simulation results indicate that even in bad channel conditions, SF relaying can still effectively improve the recovered information quality.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neural-Combinatorial-Optimization-with-Heavy-Decoder-Toward-Large-Scale-Generalization"><a href="#Neural-Combinatorial-Optimization-with-Heavy-Decoder-Toward-Large-Scale-Generalization" class="headerlink" title="Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization"></a>Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07985">http://arxiv.org/abs/2310.07985</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ciam-group/nco_code">https://github.com/ciam-group/nco_code</a></li>
<li>paper_authors: Fu Luo, Xi Lin, Fei Liu, Qingfu Zhang, Zhenkun Wang</li>
<li>for: 解决复杂的 combinatorial 优化问题，无需专家设计专门的算法。</li>
<li>methods: 提出了一种新的 Light Encoder and Heavy Decoder (LEHD) 模型，可以动态捕捉所有节点的关系，提高模型通用性。 并开发了一种数据效率高的训练方案和灵活的解决方案构建机制。</li>
<li>results: 通过训练小规模问题实例，LEHD 模型可以在 TSP 和 CVRP 问题上生成近似优解，并在实际 TSPLib 和 CVRPLib 问题上也达到了优秀表现。这些结果证明了我们提出的 LEHD 模型可以显著提高现有的 NCO 性能。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/LEHD">https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/LEHD</a> 上下载。<details>
<summary>Abstract</summary>
Neural combinatorial optimization (NCO) is a promising learning-based approach for solving challenging combinatorial optimization problems without specialized algorithm design by experts. However, most constructive NCO methods cannot solve problems with large-scale instance sizes, which significantly diminishes their usefulness for real-world applications. In this work, we propose a novel Light Encoder and Heavy Decoder (LEHD) model with a strong generalization ability to address this critical issue. The LEHD model can learn to dynamically capture the relationships between all available nodes of varying sizes, which is beneficial for model generalization to problems of various scales. Moreover, we develop a data-efficient training scheme and a flexible solution construction mechanism for the proposed LEHD model. By training on small-scale problem instances, the LEHD model can generate nearly optimal solutions for the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to 1000 nodes, and also generalizes well to solve real-world TSPLib and CVRPLib problems. These results confirm our proposed LEHD model can significantly improve the state-of-the-art performance for constructive NCO. The code is available at https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/LEHD.
</details>
<details>
<summary>摘要</summary>
神经组合优化（NCO）是一种有前途的学习基于方法，用于解决复杂的组合优化问题，不需要专家设计专门的算法。然而，大多数构造性NCO方法无法解决大规模实例问题，这会对实际应用中的使用有很大的限制。在这种情况下，我们提出了一种新的轻量级编码器和重量级解码器（LEHD）模型，具有强大的总结能力。LEHD模型可以学习动态捕捉所有可用节点的关系，这对模型总结有利，可以在不同的缩放比例下解决问题。此外，我们还开发了一种数据效率的训练方案和灵活的解决方案构建机制。通过训练小规模问题，LEHD模型可以生成近似优质解决方案，并在实际TSPLib和CVRPLib问题上具有良好的总结性。这些结果证明，我们提出的LEHD模型可以对构造性NCO进行显著改进，从而提高实际应用中的性能。代码可以在https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/LEHD中下载。
</details></li>
</ul>
<hr>
<h2 id="RandCom-Random-Communication-Skipping-Method-for-Decentralized-Stochastic-Optimization"><a href="#RandCom-Random-Communication-Skipping-Method-for-Decentralized-Stochastic-Optimization" class="headerlink" title="RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization"></a>RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07983">http://arxiv.org/abs/2310.07983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luyao Guo, Sulaiman A. Alghunaim, Kun Yuan, Laurent Condat, Jinde Cao</li>
<li>for: 这个论文主要针对的是分布式优化方法，具体来说是随机沟通跳过的分布式优化方法，以提高通信复杂性的加速。</li>
<li>methods: 这个论文提出了一种名为RandCom的分布式优化方法，该方法包含抽象的地方更新。作者对RandCom的性能进行了分析，并证明了在随机非 convex、 convex 和强 convex 设定下，RandCom可以减少通信开销。此外，作者还证明了 RandCom 可以在网络独立步长下实现线性增速。</li>
<li>results: 作者通过实验和分析表明，RandCom 可以在 federated learning 中实现线性增速，并且在非 convex 设定下，RandCom 可以实现网络独立步长下的线性增速。<details>
<summary>Abstract</summary>
Distributed optimization methods with random communication skips are gaining increasing attention due to their proven benefits in accelerating communication complexity. Nevertheless, existing research mainly focuses on centralized communication protocols for strongly convex deterministic settings. In this work, we provide a decentralized optimization method called RandCom, which incorporates probabilistic local updates. We analyze the performance of RandCom in stochastic non-convex, convex, and strongly convex settings and demonstrate its ability to asymptotically reduce communication overhead by the probability of communication. Additionally, we prove that RandCom achieves linear speedup as the number of nodes increases. In stochastic strongly convex settings, we further prove that RandCom can achieve linear speedup with network-independent stepsizes. Moreover, we apply RandCom to federated learning and provide positive results concerning the potential for achieving linear speedup and the suitability of the probabilistic local update approach for non-convex settings.
</details>
<details>
<summary>摘要</summary>
<font face="宋体">分布式优化方法 WITH random communication skips 在加速通信复杂性方面受到越来越多的关注，因为它们在强 convex  deterministic 设定下证明了其 benefits。然而，现有研究主要集中在中央通信协议上，即使在非 convex 和 strongly convex 设定下也进行了研究。在这项工作中，我们提出了一种分布式优化方法，称为 RandCom，该方法包含 probabilistic local updates。我们分析了 RandCom 在随机非 convex、 convex 和 strongly convex 设定下的性能，并证明了它可以减少通信开销的概率。此外，我们证明了 RandCom 随着节点数量增加而 achieve linear speedup。在随机 strongly convex 设定下，我们进一步证明了 RandCom 可以 achieve linear speedup WITH network-independent stepsizes。此外，我们将 RandCom 应用于联合学习，并提供了关于 achievable linear speedup 和非 convex 设定下 probablistic local update 方法的正面结果。</font>
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-of-Display-Transfer-Robots-in-Glass-Flow-Control-Systems-A-Physical-Simulation-Based-Approach"><a href="#Reinforcement-Learning-of-Display-Transfer-Robots-in-Glass-Flow-Control-Systems-A-Physical-Simulation-Based-Approach" class="headerlink" title="Reinforcement Learning of Display Transfer Robots in Glass Flow Control Systems: A Physical Simulation-Based Approach"></a>Reinforcement Learning of Display Transfer Robots in Glass Flow Control Systems: A Physical Simulation-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07981">http://arxiv.org/abs/2310.07981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hwajong Lee, Chan Kim, Seong-Woo Kim</li>
<li>for: 解决制造系统生产能力提高中的流控系统优化问题，提高生产率。</li>
<li>methods: 使用深度反馈学习解决制造过程中的调度优化问题，实现可靠的流控系统设计。</li>
<li>results: 通过实验验证，使用反馈学习对显示制造过程中的玻璃流控系统进行优化，可以获得更高的生产率和更好的制造质量。<details>
<summary>Abstract</summary>
A flow control system is a critical concept for increasing the production capacity of manufacturing systems. To solve the scheduling optimization problem related to the flow control with the aim of improving productivity, existing methods depend on a heuristic design by domain human experts. Therefore, the methods require correction, monitoring, and verification by using real equipment. As system designs increase in complexity, the monitoring time increases, which decreases the probability of arriving at the optimal design. As an alternative approach to the heuristic design of flow control systems, the use of deep reinforcement learning to solve the scheduling optimization problem has been considered. Although the existing research on reinforcement learning has yielded excellent performance in some areas, the applicability of the results to actual FAB such as display and semiconductor manufacturing processes is not evident so far. To this end, we propose a method to implement a physical simulation environment and devise a feasible flow control system design using a transfer robot in display manufacturing through reinforcement learning. We present a model and parameter setting to build a virtual environment for different display transfer robots, and training methods of reinforcement learning on the environment to obtain an optimal scheduling of glass flow control systems. Its feasibility was verified by using different types of robots used in the actual process.
</details>
<details>
<summary>摘要</summary>
一种流控系统是生产系统的关键概念，可以提高生产能力。为解决流控系统的调度优化问题，目前的方法通过域内人工设计而实现，但这些方法需要纠正、监测和验证，而且随着系统设计的增加，监测时间增加，优化设计的概率降低。为了避免人工设计的限制，我们提出了使用深度强化学习解决调度优化问题的方法。虽然现有研究中的深度学习得到了一些领域的优秀表现，但是其应用于实际的FAB，如显示和半导体生产过程，还未得到证明。因此，我们提出了一种在物理模拟环境中实现流控系统设计的方法，并在显示生产过程中使用转移机器人进行实际应用。我们提出了建立不同类型机器人的虚拟环境，并在这些环境中进行强化学习训练，以获得最佳的玻璃流控系统调度。我们验证了这种方法的可行性，并在实际过程中使用不同类型机器人进行了验证。
</details></li>
</ul>
<hr>
<h2 id="GRASP-Accelerating-Shortest-Path-Attacks-via-Graph-Attention"><a href="#GRASP-Accelerating-Shortest-Path-Attacks-via-Graph-Attention" class="headerlink" title="GRASP: Accelerating Shortest Path Attacks via Graph Attention"></a>GRASP: Accelerating Shortest Path Attacks via Graph Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07980">http://arxiv.org/abs/2310.07980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zohair Shafi, Benjamin A. Miller, Ayan Chatterjee, Tina Eliassi-Rad, Rajmonda S. Caceres</li>
<li>for: 本研究旨在提高现有的 комбиниatorial优化算法的速度，使用机器学习技术来帮助和加速这些算法。</li>
<li>methods: 本研究提出了GRASP算法（图注意力加速最短路攻击），它使用图注意力网络来缩小输入问题的大小，从而提高运行速度。</li>
<li>results: 对于APX-硬度的问题，GRASP算法可以在10倍的运行时间内维持解的质量。此外，通过精心设计输入图的表示，包括节点特征与优化任务之间的相关性，可以更好地高亮优化解的重要结构。<details>
<summary>Abstract</summary>
Recent advances in machine learning (ML) have shown promise in aiding and accelerating classical combinatorial optimization algorithms. ML-based speed ups that aim to learn in an end to end manner (i.e., directly output the solution) tend to trade off run time with solution quality. Therefore, solutions that are able to accelerate existing solvers while maintaining their performance guarantees, are of great interest. We consider an APX-hard problem, where an adversary aims to attack shortest paths in a graph by removing the minimum number of edges. We propose the GRASP algorithm: Graph Attention Accelerated Shortest Path Attack, an ML aided optimization algorithm that achieves run times up to 10x faster, while maintaining the quality of solution generated. GRASP uses a graph attention network to identify a smaller subgraph containing the combinatorial solution, thus effectively reducing the input problem size. Additionally, we demonstrate how careful representation of the input graph, including node features that correlate well with the optimization task, can highlight important structure in the optimization solution.
</details>
<details>
<summary>摘要</summary>
We consider an APX-hard problem where an adversary aims to attack shortest paths in a graph by removing the minimum number of edges. To address this, we propose the GRASP algorithm: Graph Attention Accelerated Shortest Path Attack, an ML-aided optimization algorithm that achieves run times up to 10 times faster while maintaining the quality of the solution generated.GRASP utilizes a graph attention network to identify a smaller subgraph containing the combinatorial solution, effectively reducing the input problem size. Additionally, we demonstrate how carefully representing the input graph, including node features that strongly correlate with the optimization task, can highlight important structure in the optimization solution.
</details></li>
</ul>
<hr>
<h2 id="Graph-SCP-Accelerating-Set-Cover-Problems-with-Graph-Neural-Networks"><a href="#Graph-SCP-Accelerating-Set-Cover-Problems-with-Graph-Neural-Networks" class="headerlink" title="Graph-SCP: Accelerating Set Cover Problems with Graph Neural Networks"></a>Graph-SCP: Accelerating Set Cover Problems with Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07979">http://arxiv.org/abs/2310.07979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zohair Shafi, Benjamin A. Miller, Tina Eliassi-Rad, Rajmonda S. Caceres</li>
<li>for:  solves the Set Cover Problem (SCP) using graph neural networks to accelerate combinatorial optimization.</li>
<li>methods:  uses a graph neural network method called Graph-SCP to identify a smaller sub-problem that contains the solution space, and can be used with other optimization solvers to achieve run time improvement.</li>
<li>results:  reduces the problem size by 30-70% and achieves run time speedups up to~25x compared to commercial solvers, and can achieve 100% optimality given a desired threshold.<details>
<summary>Abstract</summary>
Machine learning (ML) approaches are increasingly being used to accelerate combinatorial optimization (CO) problems. We look specifically at the Set Cover Problem (SCP) and propose Graph-SCP, a graph neural network method that can augment existing optimization solvers by learning to identify a much smaller sub-problem that contains the solution space. We evaluate the performance of Graph-SCP on synthetic weighted and unweighted SCP instances with diverse problem characteristics and complexities, and on instances from the OR Library, a canonical benchmark for SCP. We show that Graph-SCP reduces the problem size by 30-70% and achieves run time speedups up to~25x when compared to commercial solvers (Gurobi). Given a desired optimality threshold, Graph-SCP will improve upon it or even achieve 100% optimality. This is in contrast to fast greedy solutions that significantly compromise solution quality to achieve guaranteed polynomial run time. Graph-SCP can generalize to larger problem sizes and can be used with other conventional or ML-augmented CO solvers to lead to potential additional run time improvement.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Hyperparameter-Adaptive-Search-for-Surrogate-Optimization-A-Self-Adjusting-Approach"><a href="#Hyperparameter-Adaptive-Search-for-Surrogate-Optimization-A-Self-Adjusting-Approach" class="headerlink" title="Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting Approach"></a>Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07970">http://arxiv.org/abs/2310.07970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazanin Nezami, Hadis Anahideh</li>
<li>for: 提高Expensive Black-box函数优化算法的可用性、效果和 converges speed</li>
<li>methods:  Hyperparameter Adaptive Search for SO (HASSO)方法，一种自适应的SO算法，动态调整自己的超参数，不需要额外评估</li>
<li>results: 实验结果表明，HASSO可以提高多种SO算法在不同全球优化问题的性能<details>
<summary>Abstract</summary>
Surrogate Optimization (SO) algorithms have shown promise for optimizing expensive black-box functions. However, their performance is heavily influenced by hyperparameters related to sampling and surrogate fitting, which poses a challenge to their widespread adoption. We investigate the impact of hyperparameters on various SO algorithms and propose a Hyperparameter Adaptive Search for SO (HASSO) approach. HASSO is not a hyperparameter tuning algorithm, but a generic self-adjusting SO algorithm that dynamically tunes its own hyperparameters while concurrently optimizing the primary objective function, without requiring additional evaluations. The aim is to improve the accessibility, effectiveness, and convergence speed of SO algorithms for practitioners. Our approach identifies and modifies the most influential hyperparameters specific to each problem and SO approach, reducing the need for manual tuning without significantly increasing the computational burden. Experimental results demonstrate the effectiveness of HASSO in enhancing the performance of various SO algorithms across different global optimization test problems.
</details>
<details>
<summary>摘要</summary>
供质 Optimization（SO）算法已经显示出优化成本高black-box函数的承诺。然而，其性能受到采样和代理适应参数的影响，这对其普及化带来挑战。我们调查了不同SO算法中的 гипер参数对各种问题的影响，并提出了自适应搜索SO方法（HASSO）。HASSO不是一个hyperparameter tuning算法，而是一种通用的自适应SO算法，可以同时进行主要目标函数优化和自动调整参数，不需要额外的评估。目标是使SO算法更加容易使用、有效和快速收敛，为实践者提供更好的解决方案。我们的方法可以根据具体的问题和SO方法自动确定和修改最有影响的参数，从而减少手动调整的需求，不会提高计算负担。实验结果表明，HASSO可以在不同的全球优化测试问题中提高SO算法的性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Causal-Deep-Learning-for-Vulnerability-Detection"><a href="#Towards-Causal-Deep-Learning-for-Vulnerability-Detection" class="headerlink" title="Towards Causal Deep Learning for Vulnerability Detection"></a>Towards Causal Deep Learning for Vulnerability Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07958">http://arxiv.org/abs/2310.07958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Mahbubur Rahman, Ira Ceka, Chengzhi Mao, Saikat Chakraborty, Baishakhi Ray, Wei Le<br>for:这个研究旨在提高深度学习漏洞探测的可靠性和一致性，以便在实际应用中使用。methods:我们提出了一种基于 causality 的方法，包括两个阶段。第一个阶段是设计 novel perturbations，以发现模型可能使用的伪实际特征。第二个阶段是将 causal learning 算法，特别是 do-calculus，应用到现有的深度学习模型上，以系统地移除使用伪实际特征，并且将 causal 基于预测。results:我们的结果显示，我们的方法 CausalVul 可以适当地提高模型的准确性、可靠性和 OOD 性能，并且适用于所有 state-of-the-art 模型和数据集。此外，我们的研究是首个将 do-calculus 基于 causal learning 应用到软件工程模型上，并证明其实际用途。我们的重复套件可以在 <a target="_blank" rel="noopener" href="https://figshare.com/s/0ffda320dcb96c249ef2">https://figshare.com/s/0ffda320dcb96c249ef2</a> 找到。<details>
<summary>Abstract</summary>
Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to systematically remove the use of spurious features and thus promote causal based prediction. Our results show that CausalVul consistently improved the model accuracy, robustness and OOD performance for all the state-of-the-art models and datasets we experimented. To the best of our knowledge, this is the first work that introduces do calculus based causal learning to software engineering models and shows it's indeed useful for improving the model accuracy, robustness and generalization. Our replication package is located at https://figshare.com/s/0ffda320dcb96c249ef2.
</details>
<details>
<summary>摘要</summary>
深度学习漏洞检测在最近几年内已经展示了有 promise的结果。然而，一个重要的挑战仍然阻碍它在实际中变得非常有用是，模型不具有对扰动和不同数据集（OOD）的抗锋性和扩展性。我们认为这是因为模型学习了不稳定的特征，例如变量名称，这些特征与标签之间存在假 correlations。当扰动和OOD数据集不再具有这些假特征时，模型预测失败。为了解决这个挑战，在这篇论文中，我们引入了 causality 到深度学习漏洞检测中。我们的方法 CausalVul 包括两个阶段。第一个阶段，我们设计了新的扰动，以便发现模型可能使用的假特征。第二个阶段，我们应用了 causal 学习算法，具体来说是 do-calculus，在现有的深度学习模型之上进行系统性的减少假特征的使用，以便促进 causal 基于预测。我们的结果显示， CausalVul 在所有state-of-the-art 模型和数据集上都有提高模型精度、抗锋性和 OOD 性能。到目前为止，这是首次引入 do-calculus 基于 causal 学习到软件工程模型中，并证明其实际上有用于提高模型精度、抗锋性和扩展性。我们的复制包可以在 https://figshare.com/s/0ffda320dcb96c249ef2 找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/12/cs.LG_2023_10_12/" data-id="clorjzl9r00rbf1884qx005ps" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/12/eess.IV_2023_10_12/" class="article-date">
  <time datetime="2023-10-12T09:00:00.000Z" itemprop="datePublished">2023-10-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/12/eess.IV_2023_10_12/">eess.IV - 2023-10-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Cross-correlation-image-analysis-for-real-time-particle-tracking"><a href="#Cross-correlation-image-analysis-for-real-time-particle-tracking" class="headerlink" title="Cross-correlation image analysis for real-time particle tracking"></a>Cross-correlation image analysis for real-time particle tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08770">http://arxiv.org/abs/2310.08770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo R. Werneck, Cody Jessup, Austin Brandenberger, Tyler Knowles, Charles W. Lewandowski, Megan Nolan, Ken Sible, Zachariah B. Etienne, Brian D’Urso</li>
<li>for: 用于实时图像分析，尤其是在反馈控制系统中。</li>
<li>methods: 使用新的算法，可以实时检测图像中的微小移动，并且可以抗干扰。</li>
<li>results: 实现了实时图像分析，并且可以达到干扰限制的精度。<details>
<summary>Abstract</summary>
Accurately measuring translations between images is essential in many fields, including biology, medicine, geography, and physics. Existing methods, including the popular FFT-based cross-correlation, are not suitable for real-time analysis, which is especially vital in feedback control systems. To fill this gap, we introduce a new algorithm which approaches shot-noise limited displacement detection and a GPU-based implementation for real-time image analysis.
</details>
<details>
<summary>摘要</summary>
精准测量图像之间的翻译是许多领域的关键，包括生物学、医学、地理学和物理学。现有的方法，包括受欢迎的FFT基于的横距矩阵相关，不适合实时分析，尤其是在反馈控制系统中。为填补这一漏洞，我们介绍了一种新的算法，可以实现射频噪声限制的位移检测，以及基于GPU的实时图像分析实现。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-the-capabilities-of-explainable-fewshot-learning-in-remote-sensing"><a href="#Unlocking-the-capabilities-of-explainable-fewshot-learning-in-remote-sensing" class="headerlink" title="Unlocking the capabilities of explainable fewshot learning in remote sensing"></a>Unlocking the capabilities of explainable fewshot learning in remote sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08619">http://arxiv.org/abs/2310.08619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, Vu N Duong</li>
<li>for: 这个评论旨在提供一个最新的概述，探讨深度学习方法在基于遥感图像的任务上的效率和可效性。</li>
<li>methods: 这篇评论提出了一些新的几何学学习方法，以及现有的数据集的应用。</li>
<li>results: 评论表明，几何学学习方法可以有效地适应基于遥感图像的更广泛和多样化的视角。同时，评论也评估了一些最新的state-of-the-art几何学学习方法在隐私场景中的性能。<details>
<summary>Abstract</summary>
Recent advancements have significantly improved the efficiency and effectiveness of deep learning methods for imagebased remote sensing tasks. However, the requirement for large amounts of labeled data can limit the applicability of deep neural networks to existing remote sensing datasets. To overcome this challenge, fewshot learning has emerged as a valuable approach for enabling learning with limited data. While previous research has evaluated the effectiveness of fewshot learning methods on satellite based datasets, little attention has been paid to exploring the applications of these methods to datasets obtained from UAVs, which are increasingly used in remote sensing studies. In this review, we provide an up to date overview of both existing and newly proposed fewshot classification techniques, along with appropriate datasets that are used for both satellite based and UAV based data. Our systematic approach demonstrates that fewshot learning can effectively adapt to the broader and more diverse perspectives that UAVbased platforms can provide. We also evaluate some SOTA fewshot approaches on a UAV disaster scene classification dataset, yielding promising results. We emphasize the importance of integrating XAI techniques like attention maps and prototype analysis to increase the transparency, accountability, and trustworthiness of fewshot models for remote sensing. Key challenges and future research directions are identified, including tailored fewshot methods for UAVs, extending to unseen tasks like segmentation, and developing optimized XAI techniques suited for fewshot remote sensing problems. This review aims to provide researchers and practitioners with an improved understanding of fewshot learnings capabilities and limitations in remote sensing, while highlighting open problems to guide future progress in efficient, reliable, and interpretable fewshot methods.
</details>
<details>
<summary>摘要</summary>
现代技术的进步有效地提高了深度学习方法对于图像基于远程感知任务的效率和效果。然而，需要大量标注数据可能限制深度神经网络对现有远程感知数据集的应用。为解决这个挑战，几何学习（fewshot learning）作为一种有价值的方法来启用学习。 although previous research has evaluated the effectiveness of fewshot learning methods on satellite-based datasets, little attention has been paid to exploring the applications of these methods to datasets obtained from UAVs, which are increasingly used in remote sensing studies. In this review, we provide an up-to-date overview of both existing and newly proposed fewshot classification techniques, along with appropriate datasets that are used for both satellite-based and UAV-based data. Our systematic approach demonstrates that fewshot learning can effectively adapt to the broader and more diverse perspectives that UAV-based platforms can provide. We also evaluate some state-of-the-art fewshot approaches on a UAV disaster scene classification dataset, yielding promising results. We emphasize the importance of integrating XAI techniques like attention maps and prototype analysis to increase the transparency, accountability, and trustworthiness of fewshot models for remote sensing. Key challenges and future research directions are identified, including tailored fewshot methods for UAVs, extending to unseen tasks like segmentation, and developing optimized XAI techniques suited for fewshot remote sensing problems. This review aims to provide researchers and practitioners with an improved understanding of fewshot learning's capabilities and limitations in remote sensing, while highlighting open problems to guide future progress in efficient, reliable, and interpretable fewshot methods.
</details></li>
</ul>
<hr>
<h2 id="MUN-FRL-A-Visual-Inertial-LiDAR-Dataset-for-Aerial-Autonomous-Navigation-and-Mapping"><a href="#MUN-FRL-A-Visual-Inertial-LiDAR-Dataset-for-Aerial-Autonomous-Navigation-and-Mapping" class="headerlink" title="MUN-FRL: A Visual Inertial LiDAR Dataset for Aerial Autonomous Navigation and Mapping"></a>MUN-FRL: A Visual Inertial LiDAR Dataset for Aerial Autonomous Navigation and Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08435">http://arxiv.org/abs/2310.08435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ravindu G. Thalagala, Sahan M. Gunawardena, Oscar De Silva, Awantha Jayasiri, Arthur Gubbels, George K. I Mann, Raymond G. Gosine<br>for:This paper aims to promote GNSS-denied navigation research by providing a unique outdoor aerial dataset captured using a multi-sensor payload.methods:The dataset includes hardware synchronized monocular images, IMU measurements, 3D LiDAR point-clouds, and high-precision RTK-GNSS based ground truth.results:The paper provides a performance summary of state-of-the-art methods applied on the datasets.Here is the same information in Simplified Chinese text:for:这篇论文目的是促进GNSS denied navigation研究，提供一个独特的外部飞行数据集，通过多感器 payload 所捕获。methods:数据集包括硬件同步化的单镜像，IMU测量，3D LiDAR 点云，以及高精度 RTK-GNSS 基准数据。results:论文提供了现有方法在数据集上的性能摘要。<details>
<summary>Abstract</summary>
This paper presents a unique outdoor aerial visual-inertial-LiDAR dataset captured using a multi-sensor payload to promote the global navigation satellite system (GNSS)-denied navigation research. The dataset features flight distances ranging from 300m to 5km, collected using a DJI M600 hexacopter drone and the National Research Council (NRC) Bell 412 Advanced Systems Research Aircraft (ASRA). The dataset consists of hardware synchronized monocular images, IMU measurements, 3D LiDAR point-clouds, and high-precision real-time kinematic (RTK)-GNSS based ground truth. Ten datasets were collected as ROS bags over 100 mins of outdoor environment footage ranging from urban areas, highways, hillsides, prairies, and waterfronts. The datasets were collected to facilitate the development of visual-inertial-LiDAR odometry and mapping algorithms, visual-inertial navigation algorithms, object detection, segmentation, and landing zone detection algorithms based upon real-world drone and full-scale helicopter data. All the datasets contain raw sensor measurements, hardware timestamps, and spatio-temporally aligned ground truth. The intrinsic and extrinsic calibrations of the sensors are also provided along with raw calibration datasets. A performance summary of state-of-the-art methods applied on the datasets is also provided.
</details>
<details>
<summary>摘要</summary>
Note: "Simplified Chinese" is used to refer to the written form of Chinese that uses simpler characters and grammar than Traditional Chinese. However, it is important to note that there is no single standard for Simplified Chinese, and different regions may use different forms of Simplified Chinese. The translation provided above is based on the common usage of Simplified Chinese in Mainland China.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/12/eess.IV_2023_10_12/" data-id="clorjzlgr018af188h4713733" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/12/eess.SP_2023_10_12/" class="article-date">
  <time datetime="2023-10-12T08:00:00.000Z" itemprop="datePublished">2023-10-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/12/eess.SP_2023_10_12/">eess.SP - 2023-10-12</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Cell-free-Massive-MIMO-and-SWIPT-Access-Point-Operation-Mode-Selection-and-Power-Control"><a href="#Cell-free-Massive-MIMO-and-SWIPT-Access-Point-Operation-Mode-Selection-and-Power-Control" class="headerlink" title="Cell-free Massive MIMO and SWIPT: Access Point Operation Mode Selection and Power Control"></a>Cell-free Massive MIMO and SWIPT: Access Point Operation Mode Selection and Power Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08752">http://arxiv.org/abs/2310.08752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadali Mohammadi, Le-Nam Tran, Zahra Mobini, Hien Quoc Ngo, Michail Matthaiou</li>
<li>for: 这 paper 研究了无机体 massive multiple-input multiple-output (CF-mMIMO) 系统，它们包括同时无线信息和能量传输 (SWIPT) for 分开的信息用户 (IUs) 和能量用户 (EUs) in Internet of Things (IoT) 网络。</li>
<li>methods: 作者提出了一种联合Access Point (AP) 操作模式选择和功率控制设计，其中一些 APs 专门用于向 EUs 传输能量，而其他 APs 专门用于向 IUs 传输信息。</li>
<li>results: 作者的数字结果表明，提出的 AP 操作模式选择算法可以提供高达 76% 和 130% 的性能提升 compared to random AP 操作模式选择，具体来说是 maximizing the total harvested energy (HE) for EUs, while satisfying constraints on spectral efficiency (SE) for individual IUs and minimum HE for individual EUs.<details>
<summary>Abstract</summary>
This paper studies cell-free massive multiple-input multiple-output (CF-mMIMO) systems incorporating simultaneous wireless information and power transfer (SWIPT) for separate information users (IUs) and energy users (EUs) in Internet of Things (IoT) networks. To optimize both the spectral efficiency (SE) of IUs and harvested energy (HE) of EUs, we propose a joint access point (AP) operation mode selection and power control design, wherein certain APs are designated for energy transmission to EUs, while others are dedicated to information transmission to IUs. We investigate the problem of maximizing the total HE for EUs, considering constraints on SE for individual IUs and minimum HE for individual EUs. Our numerical results showcase that the proposed AP operation mode selection algorithm can provide up to $76\%$ and $130\%$ performance gains over random AP operation mode selection with and without power control, respectively.
</details>
<details>
<summary>摘要</summary>
We investigate the problem of maximizing the total HE for EUs, while ensuring constraints on SE for individual IUs and minimum HE for individual EUs. Our numerical results show that the proposed AP operation mode selection algorithm can provide up to 76% and 130% performance gains over random AP operation mode selection with and without power control, respectively.
</details></li>
</ul>
<hr>
<h2 id="A-Framework-for-Developing-and-Evaluating-Algorithms-for-Estimating-Multipath-Propagation-Parameters-from-Channel-Sounder-Measurements"><a href="#A-Framework-for-Developing-and-Evaluating-Algorithms-for-Estimating-Multipath-Propagation-Parameters-from-Channel-Sounder-Measurements" class="headerlink" title="A Framework for Developing and Evaluating Algorithms for Estimating Multipath Propagation Parameters from Channel Sounder Measurements"></a>A Framework for Developing and Evaluating Algorithms for Estimating Multipath Propagation Parameters from Channel Sounder Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08718">http://arxiv.org/abs/2310.08718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akbar Sayeed, Damla Guven, Michael Doebereiner, Sebastian Semper, Camillo Gentile, Anuraag Bodi, Zihang Cheng</li>
<li>for: 提出了一个框架，用于开发和评估毫米波频率上的多path干扰分布（MPCs）测量数据中的算法。</li>
<li>methods: 使用了一种具有普通平面阵列（UPA）的接收器，并对 channel sounder 进行了精度的数学模型建立，包括非理想的横截波特性。</li>
<li>results: 结果表明，使用 CLEAN 算法可以获得比较可靠的估计，SAGE 和 RiMAX 算法可以进一步改进估计结果，但是 RiMAX 还可以捕捉到干扰的散射效应。<details>
<summary>Abstract</summary>
A framework is proposed for developing and evaluating algorithms for extracting multipath propagation components (MPCs) from measurements collected by channel sounders at millimeter-wave frequencies. Sounders equipped with an omnidirectional transmitter and a receiver with a uniform planar array (UPA) are considered. An accurate mathematical model is developed for the spatial frequency response of the sounder that incorporates the non-ideal cross-polar beampatterns for the UPA elements. Due to the limited Field-of-View (FoV) of each element, the model is extended to accommodate multi-FoV measurements in distinct azimuth directions. A beamspace representation of the spatial frequency response is leveraged to develop three progressively complex algorithms aimed at solving the singlesnapshot maximum likelihood estimation problem: greedy matching pursuit (CLEAN), space-alternative generalized expectationmaximization (SAGE), and RiMAX. The first two are based on purely specular MPCs whereas RiMAX also accommodates diffuse MPCs. Two approaches for performance evaluation are proposed, one with knowledge of ground truth parameters, and one based on reconstruction mean-squared error. The three algorithms are compared through a demanding channel model with hundreds of MPCs and through real measurements. The results demonstrate that CLEAN gives quite reasonable estimates which are improved by SAGE and RiMAX. Lessons learned and directions for future research are discussed.
</details>
<details>
<summary>摘要</summary>
一种框架被提议用于开发和评估抽取多路干扰组件（MPC）的算法，该算法使用频率上的通道测量仪器进行 millimeter-wave 频率收集数据。这些测量仪器包括一个全irectional 发射器和一个具有均匀平面阵列（UPA）的接收器。为了更好地模型测量仪器的空间频率响应，一个精确的数学模型被开发，该模型考虑了 UPA 元素的非理想交叉束波响应。由于每个元素的视场有限，模型进一步扩展以处理多个视场的测量。通过使用 beamspace 表示法，开发了三种不同的算法，以解决单个快照最大可能性估计问题：排序匹配缓解（CLEAN）、空间替换总体预期最大化（SAGE）和 RiMAX。前两个算法基于纯specular MPC，而 RiMAX 还处理 diffuse MPC。两种方法被提议用于性能评估：一种基于地面参数的知情预测，另一种基于重建平均方差。这三种算法在一种复杂的通道模型和实际测量中进行比较，结果表明，CLEAN 提供了相对较好的估计，SAGE 和 RiMAX 则提供了更好的估计。文章结尾还讨论了学习的教训和未来研究的方向。
</details></li>
</ul>
<hr>
<h2 id="Free-space-optics-communication-system-design-using-iterative-optimization"><a href="#Free-space-optics-communication-system-design-using-iterative-optimization" class="headerlink" title="Free space optics communication system design using iterative optimization"></a>Free space optics communication system design using iterative optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13633">http://arxiv.org/abs/2310.13633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gebrehiwet Gebrekrstos Lema</li>
<li>for: 提高无线光通信系统的链路容量和可用性，并mitigate各种天气和地理因素的影响</li>
<li>methods: 使用迭代优化算法，最大化可见距离，保证可靠性，并最小化Bit Error Rate（BER）</li>
<li>results: 比对文献，实现10Gbps的数据传输速率，在不同天气条件下，visibility距离、质量因子、BER和眼图都有较好的表现<details>
<summary>Abstract</summary>
Free Space Optics (FSO) communication provides attractive bandwidth enhancement with unlicensed bands worldwide spectrum. However, the link capacity and availability are the major concern in the different atmospheric conditions. The reliability of the link is highly dependent on weather conditions that attenuate the signal strength. Hence, this study focuses to mitigate the weather and geographic effects using iterative optimization on FSO communication. The optimization maximizes the visibility distance while guaranteeing the reliability by minimizing the Bit Error Rate (BER). The wireless optical communication system is designed for the data rate of 10 Gbps. The performance of the proposed wireless optical communication is compared against the literature in terms of visibility distance, quality factor, BER, and Eye diagram at different atmospheric conditions. The simulation results have shown that the proposed work has achieved better performance.
</details>
<details>
<summary>摘要</summary>
自由空间光学（FSO）通信提供了广阔的带宽提升，但链接容量和可用性在不同的天气条件下是主要的问题。链接可靠性高度依赖于天气条件的吸收强度，因此这个研究旨在通过迭代优化 mitigate 天气和地理效应，以提高FSO通信的可靠性。优化寻味距离，并保证可靠性，最小化Bit Error Rate（BER）。这个无线光学通信系统设计了10Gbps的数据速率。与文献比较，这个提案的性能在不同的天气条件下表现较好，visibility distance、质量因子、BER和eye diagram都有所提高。
</details></li>
</ul>
<hr>
<h2 id="How-secure-is-the-time-modulated-array-enabled-ofdm-directional-modulation"><a href="#How-secure-is-the-time-modulated-array-enabled-ofdm-directional-modulation" class="headerlink" title="How secure is the time-modulated array-enabled ofdm directional modulation?"></a>How secure is the time-modulated array-enabled ofdm directional modulation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08551">http://arxiv.org/abs/2310.08551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihao Tao, Zhaoyi Xu, Athina Petropulu</li>
<li>for: 研究了时间模ulated arrays（TMA）发送的 ortogonal frequency division multiplexing（OFDM）波形的物理层安全性，并表明了攻击者可以破坏scrambling。</li>
<li>methods: 使用独立组分分析（ICA）技术来分离数据符号和TMA参数，并利用扩展和Permutation ambiguity resolved by exploiting the Toeplitz structure of the mixing matrix and knowledge of data constellation, OFDM specifics, and TMA parameter selection rules。</li>
<li>results: 表明了在ICA技术的帮助下，攻击者可以从混乱的信号中提取数据符号和TMA参数，并且引入了一种新的TMA实现方式来防止攻击。<details>
<summary>Abstract</summary>
Time-modulated arrays (TMA) transmitting orthogonal frequency division multiplexing (OFDM) waveforms achieve physical layer security by allowing the signal to reach the legitimate destination undistorted, while making the signal appear scrambled in all other directions. In this paper, we examine how secure the TMA OFDM system is, and show that it is possible for the eavesdropper to defy the scrambling. In particular, we show that, based on the scrambled signal, the eavesdropper can formulate a blind source separation problem and recover data symbols and TMA parameters via independent component analysis (ICA) techniques. We show how the scaling and permutation ambiguities arising in ICA can be resolved by exploiting the Toeplitz structure of the corresponding mixing matrix, and knowledge of data constellation, OFDM specifics, and the rules for choosing TMA parameters. We also introduce a novel TMA implementation to defend the scrambling against the eavesdropper.
</details>
<details>
<summary>摘要</summary>
时间模拟数组（TMA）发送 orthogonal frequency division multiplexing（OFDM）波形可以实现物理层安全性，使信号只能正确地达到合法目标，而在其他方向都显示混乱。在这篇论文中，我们研究了TMA OFDM系统的安全性，并显示了可以由侦测者违规破坏混乱。特别是，我们表明，基于混乱的信号，侦测者可以将问题转化为盲源分离问题，并通过独立元分析（ICA）技术来恢复数据符号和TMA参数。我们解决了涉及到ICA的缩放和排序歧义，通过利用混合矩阵的托勒茨结构和数据集、OFDM特点、TMA参数选择规则。此外，我们还介绍了一种新的TMA实现方式，以防止混乱被侦测者破坏。
</details></li>
</ul>
<hr>
<h2 id="Map2Schedule-An-End-to-End-Link-Scheduling-Method-for-Urban-V2V-Communications"><a href="#Map2Schedule-An-End-to-End-Link-Scheduling-Method-for-Urban-V2V-Communications" class="headerlink" title="Map2Schedule: An End-to-End Link Scheduling Method for Urban V2V Communications"></a>Map2Schedule: An End-to-End Link Scheduling Method for Urban V2V Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08364">http://arxiv.org/abs/2310.08364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lihao Zhang, Haijian Sun, Jin Sun, Ramviyas Parasuraman, Yinghui Ye, Rose Qingyang Hu</li>
<li>For: 本研究的目的是设计一个可以在城市环境中实现高性能的车辆间通讯链路选择方法。* Methods: 本研究使用了机器学习技术，包括卷积神经网络和 гра embedding 模型，从城市地图和车辆位置中估计频率state information，并对最佳链路选择策略进行优化。* Results: 本研究的结果显示，提案的方法可以在城市环境中实现高精度的频率state estimation，并且具有较低的过程复杂度和延迟。<details>
<summary>Abstract</summary>
Urban vehicle-to-vehicle (V2V) link scheduling with shared spectrum is a challenging problem. Its main goal is to find the scheduling policy that can maximize system performance (usually the sum capacity of each link or their energy efficiency). Given that each link can experience interference from all other active links, the scheduling becomes a combinatorial integer programming problem and generally does not scale well with the number of V2V pairs. Moreover, link scheduling requires accurate channel state information (CSI), which is very difficult to estimate with good accuracy under high vehicle mobility. In this paper, we propose an end-to-end urban V2V link scheduling method called Map2Schedule, which can directly generate V2V scheduling policy from the city map and vehicle locations. Map2Schedule delivers comparable performance to the physical-model-based methods in urban settings while maintaining low computation complexity. This enhanced performance is achieved by machine learning (ML) technologies. Specifically, we first deploy the convolutional neural network (CNN) model to estimate the CSI from street layout and vehicle locations and then apply the graph embedding model for optimal scheduling policy. The results show that the proposed method can achieve high accuracy with much lower overhead and latency.
</details>
<details>
<summary>摘要</summary>
城市往返自动车（V2V）链接调度问题是一个具有挑战性的问题。其主要目标是找到调度策略，以最大化系统性能（通常是每个链接的总容量或能效性）。由于每个链接可以受到所有活跃链接的干扰，调度问题变成了一个 combinatorial 整数编程问题，通常不会随着 V2V 对的数量很好地扩展。此外，链接调度需要准确的通道状态信息（CSI），这是在高速移动的情况下很难以估计准确。在这篇论文中，我们提出了一种名为 Map2Schedule 的综合urban V2V 链接调度方法，可以直接从城市地图和车辆位置生成 V2V 调度策略。Map2Schedule 可以在城市环境下达到与物理模型基于方法相当的性能，同时保持低的计算复杂度。这种提高的性能是基于机器学习（ML）技术。具体来说，我们首先部署了卷积神经网络（CNN）模型，以估计从街道布局和车辆位置中的 CSI，然后应用图像嵌入模型进行优化调度策略。结果显示，我们的方法可以达到高精度，并且带有远低的开销和延迟。
</details></li>
</ul>
<hr>
<h2 id="Fusion-framework-and-multimodality-for-the-Laplacian-approximation-of-Bayesian-neural-networks"><a href="#Fusion-framework-and-multimodality-for-the-Laplacian-approximation-of-Bayesian-neural-networks" class="headerlink" title="Fusion framework and multimodality for the Laplacian approximation of Bayesian neural networks"></a>Fusion framework and multimodality for the Laplacian approximation of Bayesian neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08315">http://arxiv.org/abs/2310.08315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Magnus Malmström, Isaac Skog, Daniel Axehill, Fredrik Gustafsson</li>
<li>for: 本研究考虑了神经网络（NN）的顺序融合和多个NN的融合策略，以增加鲁棒性，即减少错误分类对结果的影响和检测外部异常。</li>
<li>methods: 本研究使用巴YES（BNN）的勒拉cean approximation来衡量必要的不确定性，以便进行融合。此外，提出了一种扩展，即使NN的预测可以表示多Modal的分布。</li>
<li>results: 在使用两个经典的图像分类任务（MNIST和CFAR10）和摄像头拍摄的瑞典森林中的狮子摄像头拍摄图像序列进行示例，这种融合策略和提出的扩展都能够提高误差calibration的性能。<details>
<summary>Abstract</summary>
This paper considers the problem of sequential fusion of predictions from neural networks (NN) and fusion of predictions from multiple NN. This fusion strategy increases the robustness, i.e., reduces the impact of one incorrect classification and detection of outliers the \nn has not seen during training. This paper uses Laplacian approximation of Bayesian NNs (BNNs) to quantify the uncertainty necessary for fusion. Here, an extension is proposed such that the prediction of the NN can be represented by multimodal distributions. Regarding calibration of the estimated uncertainty in the prediction, the performance is significantly improved by having the flexibility to represent a multimodal distribution. Two class classical image classification tasks, i.e., MNIST and CFAR10, and image sequences from camera traps of carnivores in Swedish forests have been used to demonstrate the fusion strategies and proposed extension to the Laplacian approximation.
</details>
<details>
<summary>摘要</summary>
这篇论文考虑了神经网络（NN）的顺序融合和多个NN的融合策略，这种融合策略可以提高鲁棒性，即减少一个错误分类的影响和探测器在训练过程中没有看到的异常值。这篇论文使用朗尼均方（Laplacian approximation）来评估神经网络（BNN）的不确定性。在这种情况下，一种扩展是提出的，即预测神经网络可以表示多模态分布。对于预测 uncertainty 的准确性的调整，表现得非常改善，这是因为可以表示多模态分布的灵活性。在两个分类任务中，即MNIST和CFAR10，以及从瑞典雨林中的摄像头拍摄的车辆猎食行为图像序列中，使用了这种融合策略和提出的扩展来示示。
</details></li>
</ul>
<hr>
<h2 id="Low-Complexity-Algorithms-for-Mission-Completion-Time-Minimization-in-UAV-Based-ISAC-Systems"><a href="#Low-Complexity-Algorithms-for-Mission-Completion-Time-Minimization-in-UAV-Based-ISAC-Systems" class="headerlink" title="Low Complexity Algorithms for Mission Completion Time Minimization in UAV-Based ISAC Systems"></a>Low Complexity Algorithms for Mission Completion Time Minimization in UAV-Based ISAC Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08311">http://arxiv.org/abs/2310.08311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateen Ashraf, Anna Gaydamaka, Bo Tan, Dmitri Moltchanov, Yevgeni Koucheryavy</li>
<li>for: 本研究旨在提高智能交通系统（ITS）应用范围，通过 sixth-generation（6G）系统启用 интеGRATED sensing and communications（ISAC） paradigm。</li>
<li>methods: 本研究使用了一种大规模地区多个点对 интерес的优化问题，通过优化无人机速度和访问点顺序，以最小化任务时间。</li>
<li>results: 研究发现，通过实际仿真参数，第一种算法可以保持至少20%的时间提升，而第二种算法可以将总完成时间减少至少7倍。<details>
<summary>Abstract</summary>
The inherent support of sixth-generation (6G) systems enabling integrated sensing and communications (ISAC) paradigm greatly enhances the application area of intelligent transportation systems (ITS). One of the mission-critical applications enabled by these systems is disaster management, where ISAC functionality may not only provide localization but also provide users with supplementary information such as escape routes, time to rescue, etc. In this paper, by considering a large area with several locations of interest, we formulate and solve the optimization problem of delivering task parameters of the ISAC system by optimizing the UAV speed and the order of visits to the locations of interest such that the mission time is minimized. The formulated problem is a mixed integer non-linear program which is quite challenging to solve. To reduce the complexity of the solution algorithms, we propose two circular trajectory designs. The first algorithm finds the optimal UAV velocity and radius of the circular trajectories. The second algorithm finds the optimal connecting points for joining the individual circular trajectories. Our numerical results reveal that, with practical simulation parameters, the first algorithm provides a time saving of at least $20\%$, while the second algorithm cuts down the total completion time by at least $7$ times.
</details>
<details>
<summary>摘要</summary>
sixth-generation (6G) 系统内置的集成感知通信（ISAC）模式可以大幅扩展智能交通系统（ITS）的应用范围。 ISAC 功能可以不仅提供地理位置信息，还可以为用户提供补充信息，如避险 Routes、救援时间等。 在这篇论文中，我们通过考虑一个大面积的多个关注点来形式化和解决 ISAC 系统交由任务参数的优化问题，以最小化任务时间。 我们提出了两种圆形轨迹设计来降低解题算法的复杂性。 首先，我们找到了最优的 UAV 速度和圆形轨迹半径。 其次，我们找到了连接各个圆形轨迹的最优连接点。 我们的数据分析表明，使用实际参数进行模拟，首 algorithm 可以保证在至少20%的时间上减少任务时间，而第二 algorithm 可以将总完成时间减少至少7倍。
</details></li>
</ul>
<hr>
<h2 id="Maximization-of-minimum-rate-in-MIMO-OFDM-RIS-assisted-Broadcast-Channels"><a href="#Maximization-of-minimum-rate-in-MIMO-OFDM-RIS-assisted-Broadcast-Channels" class="headerlink" title="Maximization of minimum rate in MIMO OFDM RIS-assisted Broadcast Channels"></a>Maximization of minimum rate in MIMO OFDM RIS-assisted Broadcast Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08289">http://arxiv.org/abs/2310.08289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Soleymani, Ignacio Santamaria, Aydin Sezgin, Eduard Jorswieck</li>
<li>for: 提高无线通信系统的频率效率</li>
<li>methods: 优化RIS元素，并joint precoding和RIS优化问题</li>
<li>results: RIS可以在多输入多出力OFDM广播频道中提高系统性能，即使每个子带中RIS元素很少。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surface (RIS) is a promising technology to enhance the spectral efficiency of wireless communication systems. By optimizing the RIS elements, the performance of the overall system can be improved. Yet, in contrast to single-carrier systems, in multi-carrier systems, it is not possible to independently optimize RIS elements at each sub-carrier, which may reduce the benefits of RIS in multi-user orthogonal frequency division multiplexing (OFDM) systems. To this end, we investigate the effectiveness of RIS in multiple-input, multiple-output (MIMO) OFDM broadcast channels (BC). We formulate and solve a joint precoding and RIS optimization problem. We show that RIS can significantly improve the system performance even when the number of RIS elements per sub-band is very low.
</details>
<details>
<summary>摘要</summary>
可重配置智能表面（RIS）是一种扩展无线通信系统的spectral efficiency的技术。通过优化RIS元素，整体系统的性能可以得到改善。然而，在多个卡通系统中，不能独立地优化RIS元素每个子带，这可能减少RIS在多用户orthogonal frequency division multiplexing（OFDM）系统中的 beneficial effects。为此，我们研究了RIS在多输入多出力（MIMO）OFDM广播频道（BC）中的效果。我们建立了一个共同预编和RIS优化问题，并证明RIS可以在每个子带中具有很少的RIS元素时 Still significantly improve the system performance。
</details></li>
</ul>
<hr>
<h2 id="Underwater-Sound-Speed-Profile-Construction-A-Review"><a href="#Underwater-Sound-Speed-Profile-Construction-A-Review" class="headerlink" title="Underwater Sound Speed Profile Construction: A Review"></a>Underwater Sound Speed Profile Construction: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08251">http://arxiv.org/abs/2310.08251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Huang, Jixuan Zhou, Fan Gao, Jiajun Lu, Sijia Li, Pengfei Wu, Junting Wang, Hao Zhang, Tianhe Xu</li>
<li>for: 这篇论文主要关注于建立水下声速profile（SSP），以提高水下定位、导航和时间（PNT）系统的精度。</li>
<li>methods: 主流方法包括直接测量SSP和SSP反推。 direct measurement方法比较精度高，但通常需要很长时间。而反推方法可以提高实时性，但精度不及直接测量方法。</li>
<li>results: 当前主流方法仅能在各种水下观测系统覆盖区域内进行SSP构建，无法预测未来时间内声速分布。未来研究将注重多源数据的共同利用，提供不同精度和实时要求的声速分布估计服务，而无需水下观测系统。<details>
<summary>Abstract</summary>
Real--time and accurate construction of regional sound speed profiles (SSP) is important for building underwater positioning, navigation, and timing (PNT) systems as it greatly affect the signal propagation modes such as trajectory. In this paper, we summarizes and analyzes the current research status in the field of underwater SSP construction, and the mainstream methods include direct SSP measurement and SSP inversion. In the direct measurement method, we compare the performance of popular international commercial temperature, conductivity, and depth profilers (CTD). While for the inversion methods, the framework and basic principles of matched field processing (MFP), compressive sensing (CS), and deep learning (DL) for constructing SSP are introduced, and their advantages and disadvantages are compared. The traditional direct measurement method has good accuracy performance, but it usually takes a long time. The proposal of SSP inversion method greatly improves the convenience and real--time performance, but the accuracy is not as good as the direct measurement method. Currently, the SSP inversion relies on sonar observation data, making it difficult to apply to areas that couldn't be covered by underwater observation systems, and these methods are unable to predict the distribution of sound velocity at future times. How to comprehensively utilize multi-source data and provide elastic sound velocity distribution estimation services with different accuracy and real-time requirements for underwater users without sonar observation data is the mainstream trend in future research on SSP construction.
</details>
<details>
<summary>摘要</summary>
实时准确地建立地区声速 Profil (SSP) 非常重要，因为它会影响信号传播模式，如轨迹。在这篇论文中，我们总结了现有领域的研究状况，主流方法包括直接测量SSP和SSP反推。直接测量方法中，我们比较了流行的国际商业温度、电导和深度测量仪器（CTD）的性能。而反推方法中，我们介绍了匹配场处理（MFP）、压缩感知（CS）和深度学习（DL）的框架和基本原则，并比较了它们的优劣点。直接测量方法具有良好的精度表现，但通常需要很长时间。反推方法可以大幅提高实时性，但精度不如直接测量方法。当前，SSP反推方法仅仅基于声波观测数据，因此在没有声波观测系统覆盖的区域无法应用，并且无法预测未来时间的声速分布。未来研究应该尝试通过多源数据的共同利用，为无声波观测数据的水下用户提供不同精度和实时需求的声速分布估计服务。
</details></li>
</ul>
<hr>
<h2 id="Analysing-of-3D-MIMO-Communication-Beamforming-in-Linear-and-Planar-Arrays"><a href="#Analysing-of-3D-MIMO-Communication-Beamforming-in-Linear-and-Planar-Arrays" class="headerlink" title="Analysing of 3D MIMO Communication Beamforming in Linear and Planar Arrays"></a>Analysing of 3D MIMO Communication Beamforming in Linear and Planar Arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08614">http://arxiv.org/abs/2310.08614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirsadegh Roshanzamir</li>
<li>for:  investigate the performance of different beamforming techniques for MIMO communication systems with planar arrays.</li>
<li>methods:  covariance-based MIMO communication waveform method, MATLAB simulations.</li>
<li>results: 3D beam patterns generated by these constellations.Here’s the full text in Simplified Chinese:</li>
<li>for: 研究不同干扰方法的MIMO通信系统平面阵列性能.</li>
<li>methods: 协方差基于MIMO通信波形方法, MATLAB仿真.</li>
<li>results: 平面阵列Generated的3D扩散模式.<details>
<summary>Abstract</summary>
Massive multiple-input multiple-output (MIMO) systems are expected to play a crucial role in the 5G wireless communication systems. These advanced systems, which are being deployed since 2021, offer significant advantages over conventional communications generations. Unlike previous versions of communication, MIMO systems can transmit various probing signals through their antennas, which may or may not be correlated with each other. This waveform diversity provided by MIMO communication enables enhanced capabilities and improved performance. Numerous research papers have proposed different approaches for beamforming in MIMO communication. We anticipate that our research will provide valuable insights into the performance of different beamforming techniques for MIMO communication systems with planar arrays. We will investigate the 3D beam patterns generated by these constellations using the covariance-based MIMO communication waveform method. MATLAB simulations will be utilized to analyze and evaluate the performance of these methods.
</details>
<details>
<summary>摘要</summary>
巨大多输入多输出（MIMO）系统在5G无线通信系统中将扮演关键角色。这些高级系统自2021年起已经被部署，与过去的通信生成器相比，它们提供了重要的优势。与过去的通信不同，MIMO系统可以通过其天线传输多种探测信号，这些信号可能或可能不是相关的。这种波形多样性提供了MIMO通信的增强功能和性能提高。许多研究论文已经提出了不同的束形策略方法。我们预计，我们的研究将为MIMO通信系统 WITH PLANAR ARRAYS 的不同束形策略的性能提供有价值的发现。我们将使用基于协方差的MIMO通信波形方法来研究这些星座的3D束 Pattern。使用MATLAB仿真来分析和评估这些方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Fast-Ray-Tracing-Based-Precise-Underwater-Acoustic-Localization-without-Prior-Acknowledgment-of-Target-Depth"><a href="#Fast-Ray-Tracing-Based-Precise-Underwater-Acoustic-Localization-without-Prior-Acknowledgment-of-Target-Depth" class="headerlink" title="Fast Ray-Tracing-Based Precise Underwater Acoustic Localization without Prior Acknowledgment of Target Depth"></a>Fast Ray-Tracing-Based Precise Underwater Acoustic Localization without Prior Acknowledgment of Target Depth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08201">http://arxiv.org/abs/2310.08201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Huang, Hao Zhang, Kaitao Meng, Fan Gao, Wenzhou Sun, Jianxu Shu, Tianhe Xu, Deshi Li</li>
<li>for: 海上localization技术在海洋观测和建筑PNT系统中具有重要意义，特别是在灾害预警、海上搜救和资源探测等领域。</li>
<li>methods: 我们提出了一种Iterative Ray Tracing 3D Underwater Localization（IRTUL）方法，用于补偿层次质量的影响。我们首先 derivate了信号路径为гляancing角度函数，然后证明信号传播时间和水平传播距离是初始折射角度的 monotonic 函数，从而实现快速的射线跟踪。此外，我们还提出了一种音速profile（SVP）简化方法，以降低射线跟踪的计算成本。</li>
<li>results: 实验结果表明，IRTUL方法可以减少深度方向的距离偏差，并提高了平均精度约3米compared to地理位置模型。此外，简化的SVP方法可以在实时性方面减少精度损失，即使在使用时间平均损失低于0.2米。<details>
<summary>Abstract</summary>
Underwater localization is of great importance for marine observation and building positioning, navigation, timing (PNT) systems that could be widely applied in disaster warning, underwater rescues and resources exploration. The uneven distribution of underwater sound velocity poses great challenge for precise underwater positioning. The current soundline correction positioning method mainly aims at scenarios with known target depth. However, for nodes that are non-cooperative nodes or lack of depth information, soundline tracking strategies cannot work well due to nonunique positional solutions. To tackle this issue, we propose an iterative ray tracing 3D underwater localization (IRTUL) method for stratification compensation. To demonstrate the feasibility of fast stratification compensation, we first derive the signal path as a function of glancing angle, and then prove that the signal propagation time and horizontal propagation distance are monotonic functions of the initial grazing angle, so that fast ray tracing can be achieved. Then, we propose an sound velocity profile (SVP) simplification method, which reduces the computational cost of ray tracing. Experimental results show that the IRTUL has the most significant distance correction in the depth direction, and the average accuracy of IRTUL has been improved by about 3 meters compared to localization model with constant sound velocity. Also, the simplified SVP can significantly improve real-time performance with average accuracy loss less than 0.2 m when used for positioning.
</details>
<details>
<summary>摘要</summary>
水下Localization对marine observation和建筑位置、导航、时间（PNT）系统的应用非常重要，特别是在紧急警示、水下搜救和资源探索等领域。水下声速的不均分布对精确水下定位 pose 大 Challenge。目前的声线修正定位方法主要针对已知目标深度的场景，但是对于不合作节点或lack of depth information的情况，声线跟踪策略难以实现Unique positional solution。为解决这个问题，我们提出了一种迭代射线跟踪3D水下定位（IRTUL）方法，用于层次补做。首先，我们 derive 声波路径作为 glance angle 函数，并证明声波传播时间和水平传播距离是初始触角 angle 的唯一 monotonic function，从而实现快速射线跟踪。然后，我们提出了一种声速profile（SVP）简化方法，可以减少射线跟踪的计算成本。实验结果表明，IRTUL在深度方向上具有最大的距离 corrections，并且localization模型中的常数声速的精度提高了约3米。此外，简化的SVP可以在实时性方面提供significant improvement， average accuracy loss less than 0.2 m。
</details></li>
</ul>
<hr>
<h2 id="Sensing-assisted-Accurate-and-Fast-Beam-Management-for-Cellular-connected-mmWave-UAV-Network"><a href="#Sensing-assisted-Accurate-and-Fast-Beam-Management-for-Cellular-connected-mmWave-UAV-Network" class="headerlink" title="Sensing-assisted Accurate and Fast Beam Management for Cellular-connected mmWave UAV Network"></a>Sensing-assisted Accurate and Fast Beam Management for Cellular-connected mmWave UAV Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08134">http://arxiv.org/abs/2310.08134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanpeng Cui, Qixun Zhang, Zhiyong Feng, Qin Wen, Ying Zhou, Zhiqing Wei, Ping Zhang</li>
<li>for: 提高 millimeter-wave UAV 网络中的比较高延迟和低精度的矢量管理，包括初始访问（IA）和矢量跟踪。</li>
<li>methods: 使用整合感知和通信技术，并采用图像处理技术和扩展卡尔曼筛法进行矢量跟踪和预测。</li>
<li>results: 比较 conventional 方法，提出的解决方案在IA延迟、关联精度、跟踪误差和通信性能方面表现出色。<details>
<summary>Abstract</summary>
Beam management, including initial access (IA) and beam tracking, is essential to the millimeter-wave Unmanned Aerial Vehicle (UAV) network. However, conventional communication-only and feedback-based schemes suffer a high delay and low accuracy of beam alignment since they only enable the receiver to passively hear the information of the transmitter from the radio domain. This paper presents a novel sensing-assisted beam management approach, the first solution that fully utilizes the information from the visual domain to improve communication performance. We employ both integrated sensing and communication and computer vision techniques and design an extended Kalman filtering method for beam tracking and prediction. Besides, we also propose a novel dual identity association solution to distinguish multiple UAVs in dynamic environments. Real-world experiments and numerical results show that the proposed solution outperforms the conventional methods in IA delay, association accuracy, tracking error, and communication performance.
</details>
<details>
<summary>摘要</summary>
beam 管理，包括初始访问（IA）和扫描 beam，对 millimeter 波无人机网络是必需的。然而，传统的通信只和反馈方案受到高延迟和低精度的扫描Alignment的限制，因为它们只允许接收器在电波频谱中接受发送器的信息。本文提出了一种新的感知协助 beam 管理方法，是首个完全利用视觉频谱中的信息提高通信性能的解决方案。我们利用了集成感知和通信技术，并设计了一种扩展 kalman 筛法 для beam 跟踪和预测。此外，我们还提出了一种新的双重标识关系解决方案，以在动态环境中分辨多个无人机。实际实验和数学结果表明，我们的解决方案在IA延迟、关联精度、跟踪错误和通信性能方面都超过了传统方法。
</details></li>
</ul>
<hr>
<h2 id="3D-terrain-mapping-and-filtering-from-coarse-resolution-data-cubes-extracted-from-real-aperture-94-GHz-radar"><a href="#3D-terrain-mapping-and-filtering-from-coarse-resolution-data-cubes-extracted-from-real-aperture-94-GHz-radar" class="headerlink" title="3D terrain mapping and filtering from coarse resolution data cubes extracted from real-aperture 94 GHz radar"></a>3D terrain mapping and filtering from coarse resolution data cubes extracted from real-aperture 94 GHz radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08120">http://arxiv.org/abs/2310.08120</a></li>
<li>repo_url: None</li>
<li>paper_authors: William D. Harcourt, David G. Macfarlane, Duncan A. Robertson</li>
<li>for: 精确高分辨率环境三维地形映射是许多领域的关键，本研究提出了一种新的技术，即PCFilts-94算法，用于从粗糙度毫米波雷达数据立方体中提取3D点云和相关的不确定性评估。</li>
<li>methods: 本研究使用了一种新的点云提取和筛选技术，包括非相关波形均值和Voronoi基于点云异常点除法，以减少点云不确定性。此外，还使用了最优的地面控制点数量（GCP）来地理参考点云，并估算了点云不确定性。</li>
<li>results: 研究结果表明，新的处理方法可以生成稳定的点云，即可重复使用不同的点云提取和筛选参数值进行预处理，并且 less sensitive to over-filtering through the point cloud processing workflow。此外，点云不确定性也减少到了约1.5米至3米之间，比其他靠近范围的雷达系统更小。这些结果可以作为未来使用毫米波雷达系统进行地形映射的标准。<details>
<summary>Abstract</summary>
Accurate, high-resolution 3D mapping of environmental terrain is critical in a range of disciplines. In this study, we develop a new technique, called the PCFilt-94 algorithm, to extract 3D point clouds from coarse resolution millimetre-wave radar data cubes and quantify their associated uncertainties. A technique to non-coherently average neighbouring waveforms surrounding each AVTIS2 range profile was developed in order to reduce speckle and was found to reduce point cloud uncertainty by 13% at long range and 20% at short range. Further, a Voronoi-based point cloud outlier removal algorithm was implemented which iteratively removes outliers in a point cloud until the process converges to the removal of 0 points. Taken together, the new processing methodology produces a stable point cloud, which means that: 1) it is repeatable even when using different point cloud extraction and filtering parameter values during pre-processing, and 2) is less sensitive to over-filtering through the point cloud processing workflow. Using an optimal number of Ground Control Points (GCPs) for georeferencing, which was determined to be 3 at close range (<1.5 km) and 5 at long range (>3 km), point cloud uncertainty was estimated to be approximately 1.5 m at 1.5 km to 3 m at 3 km and followed a Lorentzian distribution. These uncertainties are smaller than those reported for other close-range radar systems used for terrain mapping. The results of this study should be used as a benchmark for future application of millimetre-wave radar systems for 3D terrain mapping.
</details>
<details>
<summary>摘要</summary>
精确高分辨度三维地形映射是多个领域的关键。本研究中，我们开发了一种新的算法，即PCFilts-94算法，以提取毫米波雷达数据立方体点云和相关的不确定性。为了减少雷达棱镜的影响，我们开发了一种非协调平均邻近波形式的方法，并发现可以降低点云不确定性13%在远距离和20%在近距离。此外，我们实现了基于Voronoi区域的点云异常点除法，这个过程可以逐步除除点云中的异常点，直到 converge to the removal of 0 points。综上所述，新的处理方法可以生成稳定的点云，其特点是：1)可重复性，即在不同的点云提取和筛选参数值时，可以重复获得相同的结果，2) menos sensitive to over-filtering，即点云处理流程中的过滤效果更加稳定。使用最佳的地面控制点数（GCPs）进行地理参考，其值为1.5 km处3个和3 km处5个，点云不确定性约为1.5 m在1.5 km处到3 m在3 km处，遵循lorentzian分布。这些不确定性较小，比其他靠近范围的雷达系统用于地形映射报告的不确定性更小。本研究的结果应该成为未来毫米波雷达系统的3D地形映射应用的标准。
</details></li>
</ul>
<hr>
<h2 id="Multi-Satellite-Cooperative-Networks-Joint-Hybrid-Beamforming-and-User-Scheduling-Design"><a href="#Multi-Satellite-Cooperative-Networks-Joint-Hybrid-Beamforming-and-User-Scheduling-Design" class="headerlink" title="Multi-Satellite Cooperative Networks: Joint Hybrid Beamforming and User Scheduling Design"></a>Multi-Satellite Cooperative Networks: Joint Hybrid Beamforming and User Scheduling Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08095">http://arxiv.org/abs/2310.08095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Zhang, Shu Sun, Meixia Tao, Qin Huang, Xiaohu Tang</li>
<li>for: The paper is written for a cooperative communication network where multiple low-Earth-orbit satellites provide services for ground users (GUs) at the same time and on the same frequency.</li>
<li>methods: The paper proposes a hybrid beamforming method consisting of analog beamforming for beam alignment and digital beamforming for interference mitigation, as well as a low-complexity heuristic user scheduling algorithm to establish appropriate connections between the satellites and GUs.</li>
<li>results: The proposed joint hybrid beamforming and user scheduling (JHU) scheme is expected to dramatically improve the performance of the multi-satellite cooperative network, and simulations are conducted to compare the proposed schemes with representative baselines and to analyze the key factors influencing the performance of the network.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了考虑一个多个低地球轨天体卫星协同通信网络，这些卫星同时提供服务给地面用户（GUs）。</li>
<li>methods: 论文提出了一种混合 beamforming 方法，包括分析 beamforming  для射频轨道的平衡和数字 beamforming  для干扰 Mitigation，以及一种低复杂度的决策算法来确定卫星和 GUs 之间的连接。</li>
<li>results: 提出的 JHU 方案预计可以帮助提高多卫星协同网络的性能，并通过与代表性基线相比进行 simulations，分析了网络性能的关键因素。<details>
<summary>Abstract</summary>
In this paper, we consider a cooperative communication network where multiple low-Earth-orbit satellites provide services for ground users (GUs) (at the same time and on the same frequency). The multi-satellite cooperative network has great potential for satellite communications due to its dense configuration, extensive coverage, and large spectral efficiency. However, the communication and computational resources on satellites are usually restricted. Therefore, considering the limitation of the on-board radio-frequency chains of satellites, we first propose a hybrid beamforming method consisting of analog beamforming for beam alignment and digital beamforming for interference mitigation. Then, to establish appropriate connections between the satellites and GUs, we propose a low-complexity heuristic user scheduling algorithm which determines the connections according to the total spectral efficiency increment of the multi-satellite cooperative network. Next, considering the intrinsic connection between beamforming and user scheduling, a joint hybrid beamforming and user scheduling (JHU) scheme is proposed to dramatically improve the performance of the multi-satellite cooperative network. In addition to the single-connection scenario, we also consider the multi-connection case using the JHU scheme. Moreover, simulations are conducted to compare the proposed schemes with representative baselines and to analyze the key factors influencing the performance of the multi-satellite cooperative network.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了多颗低地球轨道卫星协作网络，这些卫星为地面用户（GU）提供服务（同时，在同一频率上）。这种多卫星协作网络具有密集配置、广泛覆盖和大 spectral efficiency，但卫星上的通信和计算资源受限。因此，我们首先提出了一种混合扫描方法，其中analog扫描用于杆位定位，而数字扫描用于干扰降低。然后，为建立多卫星协作网络中的合适连接，我们提出了一种低复杂度的冒险用户调度算法，该算法根据多卫星协作网络的总spectral efficiency增量确定连接。接着，我们考虑了扫描和用户调度之间的内在关系，并提出了一种结合扫描和用户调度的共同方案（JHU），以显著提高多卫星协作网络的性能。此外，我们还考虑了多连接情况下的JHU方案。此外，我们进行了对基eline的比较和分析，以分析多卫星协作网络的性能关键因素。
</details></li>
</ul>
<hr>
<h2 id="Channel-robust-Automatic-Modulation-Classification-Using-Spectral-Quotient-Cumulants"><a href="#Channel-robust-Automatic-Modulation-Classification-Using-Spectral-Quotient-Cumulants" class="headerlink" title="Channel-robust Automatic Modulation Classification Using Spectral Quotient Cumulants"></a>Channel-robust Automatic Modulation Classification Using Spectral Quotient Cumulants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08021">http://arxiv.org/abs/2310.08021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sai Huang, Yuting Chen, Jiashuo He, Shuo Chang, Zhiyong Feng<br>for:The paper is written for proposing a channel-robust modulation classification framework for orthogonal frequency division multiplexing (OFDM) systems, which can mitigate the adverse effects of multipath channel and improve the classification accuracy.methods:The proposed method uses spectral quotient cumulants (SQCs) extracted from the filtered spectral quotient (SQ) sequence as the inputs to train an artificial neural network (ANN) classifier. The method also employs an outlier detector to filter the outliers in the SQ sequence.results:The simulation results show that the proposed SQCC method exhibits classification robustness and superiority under various unknown Rician multipath fading channels, with nearly 90% classification accuracy at the signal to noise ratio (SNR) of 4dB when testing under multiple channels but training under AWGN channel.<details>
<summary>Abstract</summary>
Automatic modulation classification (AMC) is to identify the modulation format of the received signal corrupted by the channel effects and noise. Most existing works focus on the impact of noise while relatively little attention has been paid to the impact of channel effects. However, the instability posed by multipath fading channels leads to significant performance degradation. To mitigate the adverse effects of the multipath channel, we propose a channel-robust modulation classification framework named spectral quotient cumulant classification (SQCC) for orthogonal frequency division multiplexing (OFDM) systems. Specifically, we first transform the received signal to the spectral quotient (SQ) sequence by spectral circular shift division operations. Secondly, an outlier detector is proposed to filter the outliers in the SQ sequence. At last, we extract spectral quotient cumulants (SQCs) from the filtered SQ sequence as the inputs to train the artificial neural network (ANN) classifier and use the trained ANN to make the final decisions. Simulation results show that our proposed SQCC method exhibits classification robustness and superiority under various unknown Rician multipath fading channels compared with other existing methods. Specifically, the SQCC method achieves nearly 90% classification accuracy at the signal to noise ratio (SNR) of 4dB when testing under multiple channels but training under AWGN channel.
</details>
<details>
<summary>摘要</summary>
自动模式分类（AMC）是确定接收信号中的模式格式，它受到通道效果和噪声的影响。现有大多数研究强调噪声的影响，而忽略了通道效果的影响。然而，多路干扰通道会导致性能下降。为了 mitigate 多路干扰通道的影响，我们提出了一种鲁棒的通道robust模式分类框架，名为спектральquotientcumulant分类（SQCC），用于orthogonal frequency division multiplexing（OFDM）系统。specifically，我们首先将接收信号转换为spectral quotient（SQ）序列，然后提出了一种outlier检测器来过滤SQ序列中的异常值。最后，我们从过滤后的SQ序列提取spectral quotientcumulants（SQCs）作为人工神经网络（ANN）分类器的输入，并使用已经训练的ANN来做最终的决定。 simulation results show that our proposed SQCC method exhibits robustness and superiority under various unknown Rician multipath fading channels compared with other existing methods. Specifically, the SQCC method achieves nearly 90% classification accuracy at the signal to noise ratio（SNR）of 4dB when testing under multiple channels but training under AWGN channel.
</details></li>
</ul>
<hr>
<h2 id="On-the-Capacity-of-Reconfigurable-Intelligence-Surface-the-Sparse-Channel-Case"><a href="#On-the-Capacity-of-Reconfigurable-Intelligence-Surface-the-Sparse-Channel-Case" class="headerlink" title="On the Capacity of Reconfigurable Intelligence Surface: the Sparse Channel Case"></a>On the Capacity of Reconfigurable Intelligence Surface: the Sparse Channel Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07994">http://arxiv.org/abs/2310.07994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxi Zhu</li>
<li>for: 这篇论文是为了分析利用智能表面协助的MIMO通信在稀畴频谱中提高容量而写的。</li>
<li>methods: 这篇论文使用了exploring稀畴性 Property来提高MIMO通信的容量，并开发了SU-MIMO或DL MU-MIMO的高效算法。</li>
<li>results: 论文表明，在RIS反射频谱中支持高级传输更加困难，并且提出了一种基于稀畴性的MIMO通信方法来解决这个问题。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surface (RIS) is an important candidate technology for 6G. We provide an analysis of RIS-assisted MIMO communication in sparse channel typically found in the mmW or THz range. By exploring the sparse property, we maximize the capacity in the singular space of the channel and developed efficient algorithms for SU-MIMO or DL MU-MIMO. We also proved it is more difficult to support high rank transmission in the RIS reflection channel than in the traditional MIMO channel.
</details>
<details>
<summary>摘要</summary>
<<SYS>>重配置智能表面（RIS）是6G技术的重要候选人。我们对RIS协助MIMO通信在罕见频谱中进行分析，通常发生在mmWave或THz范围内。通过探索罕见性，我们最大化了频谱空间中的容量，并开发了高效的SU-MIMO或DL MU-MIMO算法。此外，我们证明了在RIS反射频谱中支持高级传输比traditional MIMO频谱更加困难。[/INST0]  Here's the translation of the text into Traditional Chinese:<<SYS>>重配置智能表面（RIS）是6G技术的重要候选人。我们对RIS协助MIMO通信在罕见频范围中进行分析，通常发生在mmWave或THz范围内。通过探索罕见性，我们最大化了频范围空间中的容量，并开发了高效的SU-MIMO或DL MU-MIMO算法。此外，我们证明了在RIS反射频范围中支持高级传输比traditional MIMO频范围更加困难。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/12/eess.SP_2023_10_12/" data-id="clorjzlia01byf188csezdg8p" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/cs.SD_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T15:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/cs.SD_2023_10_11/">cs.SD - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Vec-Tok-Speech-speech-vectorization-and-tokenization-for-neural-speech-generation"><a href="#Vec-Tok-Speech-speech-vectorization-and-tokenization-for-neural-speech-generation" class="headerlink" title="Vec-Tok Speech: speech vectorization and tokenization for neural speech generation"></a>Vec-Tok Speech: speech vectorization and tokenization for neural speech generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07246">http://arxiv.org/abs/2310.07246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bakerbunker/vectok">https://github.com/bakerbunker/vectok</a></li>
<li>paper_authors: Xinfa Zhu, Yuanjun Lv, Yi Lei, Tao Li, Wendi He, Hongbin Zhou, Heng Lu, Lei Xie</li>
<li>for: 这篇论文是为了提出一种可以实现多种演讲任务的扩展框架，包括无需预训练的语言模型（LM）。</li>
<li>methods: 论文提出了一种新的演讲编码方法，基于演讲 vectors 和 semantic tokens。演讲 vectors 包含杂音细节，可以保证高质量的演讲重建，而 semantic tokens 则是关注演讲语言内容，以便语言模型化。此外，论文还使用了 Byte-Pair Encoding（BPE）来减少表达长度和比特率，提高LM的性能。</li>
<li>results: 根据实验结果，Vec-Tok Speech 在使用 50 万小时演讲数据时表现出色，比其他最佳模型更好。<details>
<summary>Abstract</summary>
Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .
</details>
<details>
<summary>摘要</summary>
语言模型（LM）在自然语言处理和计算机视觉领域最近得到了广泛应用，生成了高精度的文本或图像。然而，当前的语音生成模型仍然在语音质量和任务泛化方面存在困难。这篇论文提出了Vec-Tok Speech框架，这是一个可扩展的框架，可以实现多种语音生成任务，生成高质量和高精度的语音。具体来说，我们提出了一种基于语音向量和semantic token的语音编码方法。语音向量包含语音中的音响细节，以便实现高精度的语音重建；semantic token则关注语音中的语言内容，使得语言模型可以更好地模型语音。基于该语音编码方法，Vec-Tok Speech可以通过使用LM进行核心语音生成。此外，我们还引入了字节对编码（BPE），以降低токен长度和比特率，从而提高LM的性能。Vec-Tok Speech可以用于 между语言和cross-lingual零Shift语音转换（VC）、零Shift发音风格转换（TTS）、语音到语音翻译（S2ST）、语音干扰除和speaker隐藏和匿名化。实验结果表明，Vec-Tok Speech，基于50000小时的语音数据，与其他SOTA模型相比，表现更好。代码将在GitHub上提供，地址为https://github.com/BakerBunker/VecTok。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/cs.SD_2023_10_11/" data-id="clorjzlcb00y2f1882gzr21kx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/eess.AS_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T14:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/eess.AS_2023_10_11/">eess.AS - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Damping-Density-of-an-Absorptive-Shoebox-Room-Derived-from-the-Image-Source-Method"><a href="#Damping-Density-of-an-Absorptive-Shoebox-Room-Derived-from-the-Image-Source-Method" class="headerlink" title="Damping Density of an Absorptive Shoebox Room Derived from the Image-Source Method"></a>Damping Density of an Absorptive Shoebox Room Derived from the Image-Source Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07363">http://arxiv.org/abs/2310.07363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian J. Schlecht, Karolina Prawda, Rudolf Rabenstein, Maximilian Schäfer</li>
<li>for: 这篇论文主要探讨了如何快速计算带有任意吸收的射镜房（shoebox room）的吸收响应（RIR）。</li>
<li>methods: 该论文使用了图像源方法计算射镜房的RIR，并 derive了一个关闭式表达式来描述全部多坡衰减率（damping density）。</li>
<li>results: 该论文通过对墙面吸收率的变化来研究射镜房的吸收响应，并提出了一种快速随机生成晚反射的方法。该方法可以准确地预测射镜房的吸收响应，并且在不同的墙面吸收率下都具有高精度。<details>
<summary>Abstract</summary>
The image-source method is widely applied to compute room impulse responses (RIRs) of shoebox rooms with arbitrary absorption. However, with increasing RIR lengths, the number of image sources grows rapidly, leading to slow computation. In this paper, we derive a closed-form expression for the damping density, which characterizes the overall multi-slope energy decay. The omnidirectional energy decay over time is directly derived from the damping density. The resulting energy decay model accurately matches the late reverberation simulated via the image-source method. The proposed model allows the fast stochastic synthesis of late reverberation by shaping noise with the energy envelope. Simulations of various wall damping coefficients demonstrate the model's accuracy. The proposed model consistently outperforms the energy decay prediction accuracy compared to a state-of-the-art approximation method. The paper elaborates on the proposed damping density's applicability to modeling multi-sloped sound energy decay, predicting reverberation time in non-diffuse sound fields, and fast frequency-dependent RIR synthesis.
</details>
<details>
<summary>摘要</summary>
“图像源方法广泛应用于计算封闭室内响应（RIR）的射频响应。然而，随着 RIR 的增长，图像源的数量增长得非常快，导致计算变得慢。在这篇论文中，我们 derive 一个闭式表达式，用于描述全体多坡衰减率。通过这个表达式，我们直接 deriv 出各个方向的能量衰减。这种能量衰减模型可以准确地与图像源方法 simulate 的晚期响应相匹配。我们的模型允许通过修形噪声的形式来快速生成晚期响应。我们通过不同墙面减噪系数的 simulations 表明了我们的模型的准确性。我们的模型在与现有的approximation方法相比之下表现出了更高的能量衰减预测精度。论文还详细介绍了我们提出的凝固density 的应用性，包括模型多坡衰减、预测射频响应时间以及快速frequency-dependent RIR synthesis。”
</details></li>
</ul>
<hr>
<h2 id="Magnitude-and-phase-aware-Speech-Enhancement-with-Parallel-Sequence-Modeling"><a href="#Magnitude-and-phase-aware-Speech-Enhancement-with-Parallel-Sequence-Modeling" class="headerlink" title="Magnitude-and-phase-aware Speech Enhancement with Parallel Sequence Modeling"></a>Magnitude-and-phase-aware Speech Enhancement with Parallel Sequence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07316">http://arxiv.org/abs/2310.07316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuewei Zhang, Huanbin Zou, Jie Zhu</li>
<li>for: 本研究是关于喷水声提高（SE）领域的一篇论文，旨在提高喷水声的语音质量。</li>
<li>methods: 本研究使用了一种新的预测方法，即使用实数网络来预测干扰声的大小和正规化cIRM（Complex Ideal Ratio Mask）。此外，研究者还提出了一种平行序列模型（PSM）块，用于改进传统的循环回归网络（CRN）模型。</li>
<li>results: 实验结果表明，使用的MPCRN方法可以在喷水声提高中实现更高的性能。<details>
<summary>Abstract</summary>
In speech enhancement (SE), phase estimation is important for perceptual quality, so many methods take clean speech's complex short-time Fourier transform (STFT) spectrum or the complex ideal ratio mask (cIRM) as the learning target. To predict these complex targets, the common solution is to design a complex neural network, or use a real network to separately predict the real and imaginary parts of the target. But in this paper, we propose to use a real network to estimate the magnitude mask and normalized cIRM, which not only avoids the significant increase of the model complexity caused by complex networks, but also shows better performance than previous phase estimation methods. Meanwhile, we devise a parallel sequence modeling (PSM) block to improve the RNN block in the convolutional recurrent network (CRN)-based SE model. We name our method as magnitude-and-phase-aware and PSM-based CRN (MPCRN). The experimental results illustrate that our MPCRN has superior SE performance.
</details>
<details>
<summary>摘要</summary>
在speech enhancement（SE）中，频谱估计是重要的，因此许多方法使用清晰speech的复杂短时傅立叶变换（STFT）谱或理想的复杂比例面纱（cIRM）作为学习目标。为预测这些复杂目标，常见的解决方案是设计复杂的神经网络，或者使用实际网络分开预测实部和虚部。但在本文中，我们提议使用实网络来估计魔方面和 нормализаzed cIRM，不仅可以避免由复杂网络引起的模型复杂度增加，而且也比前期预测方法表现更好。此外，我们设计了并行序列模型（PSM）块来改进CRN基于的卷积隐藏状态机制（CRN）模型。我们称我们的方法为魔方-和频谱-意识的PSM-CRN（MPCRN）。实验结果表明，我们的MPCRN具有更高的SE性能。
</details></li>
</ul>
<hr>
<h2 id="VSANet-Real-time-Speech-Enhancement-Based-on-Voice-Activity-Detection-and-Causal-Spatial-Attention"><a href="#VSANet-Real-time-Speech-Enhancement-Based-on-Voice-Activity-Detection-and-Causal-Spatial-Attention" class="headerlink" title="VSANet: Real-time Speech Enhancement Based on Voice Activity Detection and Causal Spatial Attention"></a>VSANet: Real-time Speech Enhancement Based on Voice Activity Detection and Causal Spatial Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07295">http://arxiv.org/abs/2310.07295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuewei Zhang, Huanbin Zou, Jie Zhu</li>
<li>for: 提高speech干扰改进（SE）性能</li>
<li>methods: 使用多任务学习框架和 causal spatial attention（CSA）块</li>
<li>results: 实验结果表明，VSANet具有出色的SE性能，其中多任务学习框架和CSA块都有益于SE性能的提高。<details>
<summary>Abstract</summary>
The deep learning-based speech enhancement (SE) methods always take the clean speech's waveform or time-frequency spectrum feature as the learning target, and train the deep neural network (DNN) by reducing the error loss between the DNN's output and the target. This is a conventional single-task learning paradigm, which has been proven to be effective, but we find that the multi-task learning framework can improve SE performance. Specifically, we design a framework containing a SE module and a voice activity detection (VAD) module, both of which share the same encoder, and the whole network is optimized by the weighted loss of the two modules. Moreover, we design a causal spatial attention (CSA) block to promote the representation capability of DNN. Combining the VAD aided multi-task learning framework and CSA block, our SE network is named VSANet. The experimental results prove the benefits of multi-task learning and the CSA block, which give VSANet an excellent SE performance.
</details>
<details>
<summary>摘要</summary>
deep learning 基于 speech enhancement（SE）方法总是使用干净speech的波形或时域频谱特征作为学习目标，并使用深度神经网络（DNN）来减少错误损失之间的差异。这是一种常见的单任务学习模式，已经证明有效，但我们发现多任务学习框架可以提高 SE 性能。specifically，我们设计了一个包含 SE 模块和voice activity detection（VAD）模块的框架，两者都共享同一个编码器，整个网络通过两个模块的权重损失来优化。此外，我们还设计了一个 causal spatial attention（CSA）块，以提高 DNN 的表达能力。将 VAD 帮助多任务学习框架和 CSA 块结合在一起，我们称之为 VSANet。实验结果表明多任务学习和 CSA 块对 VSANet 的 SE 性能产生了积极的影响。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/eess.AS_2023_10_11/" data-id="clorjzle5011zf188gbqyga1s" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/cs.CV_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T13:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/cs.CV_2023_10_11/">cs.CV - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dynamic-Appearance-Particle-Neural-Radiance-Field"><a href="#Dynamic-Appearance-Particle-Neural-Radiance-Field" class="headerlink" title="Dynamic Appearance Particle Neural Radiance Field"></a>Dynamic Appearance Particle Neural Radiance Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07916">http://arxiv.org/abs/2310.07916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ancheng Lin, Jun Li</li>
<li>for: 模elling 3D 动态场景的 Dynamic NeRFs 扩展了这种模型，但是现有的动态 NeRFs 使用类似的欧拉rian representation，导致外观和运动之间存在紧密的关联，缺乏物理解释。</li>
<li>methods: 我们提出了Dynamic Appearance Particle Neural Radiance Field (DAP-NeRF)，它通过粒子基示的方式来模拟动态场景中视觉元素的运动。DAP-NeRF 由静态场景和动态场景组成，其中动态场景是由多个{\em appearance particles} 组成，每个粒子都携带了小元素的视觉信息和运动模型。所有组件，包括静态场景、视觉特征和运动模型，都是通过单摄影视频学习而不需要任何先前的场景知识。</li>
<li>results: 我们构建了一个新的数据集来评估运动模型，并开发了一种高效的计算框架。实验结果表明，DAP-NeRF 是一种有效的技术，可以捕捉动态场景中不仅外观，还有物理意义的运动。<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D scenes. Dynamic NeRFs extend this model by capturing time-varying elements, typically using deformation fields. The existing dynamic NeRFs employ a similar Eulerian representation for both light radiance and deformation fields. This leads to a close coupling of appearance and motion and lacks a physical interpretation. In this work, we propose Dynamic Appearance Particle Neural Radiance Field (DAP-NeRF), which introduces particle-based representation to model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists of superposition of a static field and a dynamic field. The dynamic field is quantised as a collection of {\em appearance particles}, which carries the visual information of a small dynamic element in the scene and is equipped with a motion model. All components, including the static field, the visual features and motion models of the particles, are learned from monocular videos without any prior geometric knowledge of the scene. We develop an efficient computational framework for the particle-based model. We also construct a new dataset to evaluate motion modelling. Experimental results show that DAP-NeRF is an effective technique to capture not only the appearance but also the physically meaningful motions in a 3D dynamic scene.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (NeRFs) 有大量的潜力用于模拟3D场景。动态NeRFs进一步发展了这个模型，通常使用形变场来捕捉时间变化的元素。现有的动态NeRFs都使用相似的尤里安表示法来表示光辐射场和形变场，这会导致视觉和运动之间的强相关性和缺乏物理解释。在这项工作中，我们提出了动态外观粒子神经辐射场(DAP-NeRF)，它通过使用粒子基于表示来模拟动态3D场景中的视觉元素运动。DAP-NeRF包括一个静止场和一个动态场的积和。动态场被量化为一个集合的“外观粒子”，这些粒子携带了场景中小元素的视觉信息，并拥有运动模型。所有组件，包括静止场、视觉特征和运动模型，都由单投影视频无关 geometric knowledge of the scene 进行学习。我们开发了一个高效的计算框架，并构建了一个新的数据集来评估运动模型。实验结果表明，DAP-NeRF 是一种有效的方法，可以捕捉3D动态场景中的视觉特征和物理意义的运动。
</details></li>
</ul>
<hr>
<h2 id="NoMaD-Goal-Masked-Diffusion-Policies-for-Navigation-and-Exploration"><a href="#NoMaD-Goal-Masked-Diffusion-Policies-for-Navigation-and-Exploration" class="headerlink" title="NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration"></a>NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07896">http://arxiv.org/abs/2310.07896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Sridhar, Dhruv Shah, Catherine Glossop, Sergey Levine</li>
<li>for: 本研究的目的是提供一种能够执行任务导航和无目标探索的单一扩散策略，以提高在未知环境中的机器人导航性能。</li>
<li>methods: 本研究使用了一种基于Transformer的大规模策略，与一种扩散模型解码器相结合，以适应两类不同的导航需求。</li>
<li>results: 实验结果表明，使用本研究的方法可以在实际的移动机器人平台上实现更好的总性性能，并与五种相关方法进行比较，显示了显著的改进和减少碰撞的情况。<details>
<summary>Abstract</summary>
Robotic learning for navigation in unfamiliar environments needs to provide policies for both task-oriented navigation (i.e., reaching a goal that the robot has located), and task-agnostic exploration (i.e., searching for a goal in a novel setting). Typically, these roles are handled by separate models, for example by using subgoal proposals, planning, or separate navigation strategies. In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the latter providing the ability to search novel environments, and the former providing the ability to reach a user-specified goal once it has been located. We show that this unified policy results in better overall performance when navigating to visually indicated goals in novel environments, as compared to approaches that use subgoal proposals from generative models, or prior methods based on latent variable models. We instantiate our method by using a large-scale Transformer-based policy trained on data from multiple ground robots, with a diffusion model decoder to flexibly handle both goal-conditioned and goal-agnostic navigation. Our experiments, conducted on a real-world mobile robot platform, show effective navigation in unseen environments in comparison with five alternative methods, and demonstrate significant improvements in performance and lower collision rates, despite utilizing smaller models than state-of-the-art approaches. For more videos, code, and pre-trained model checkpoints, see https://general-navigation-models.github.io/nomad/
</details>
<details>
<summary>摘要</summary>
机器人学习 navigation 在未知环境中需要提供任务域导航（即达到机器人已经发现的目标）和任务无关探索（即在新环境中寻找目标）的策略。通常，这些角色由不同的模型来处理，例如使用互助提案、规划或不同的导航策略。在这篇论文中，我们描述了如何通过培训单一个混合扩散策略来处理任务域导航和任务无关探索，其中前者提供了达到用户指定的目标的能力，而后者提供了在新环境中搜索目标的能力。我们表明这种混合策略在视觉指定目标下在新环境中导航时比使用生成模型的互助提案或先前的射频变量模型方法更好的总性表现。我们在实现方面使用了大规模的Transformer基于策略，并使用扩散模型解码器来灵活地处理任务域导航和任务无关探索。我们的实验在真实世界移动机器人平台上进行，与五种alternative方法进行比较，并显示了在未 seen环境中的有效导航，并且demonstrate了与之前的方法相比更高的性能和更低的碰撞率，即使使用了更小的模型。更多视频、代码和预训练模型检查点，请访问https://general-navigation-models.github.io/nomad/。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Structured-Noise-Removal-with-Variational-Lossy-Autoencoder"><a href="#Unsupervised-Structured-Noise-Removal-with-Variational-Lossy-Autoencoder" class="headerlink" title="Unsupervised Structured Noise Removal with Variational Lossy Autoencoder"></a>Unsupervised Structured Noise Removal with Variational Lossy Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07887">http://arxiv.org/abs/2310.07887</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krulllab/DVLAE">https://github.com/krulllab/DVLAE</a></li>
<li>paper_authors: Benjamin Salmon, Alexander Krull</li>
<li>for: 该研究旨在提出一种无监督的深度学习方法，可以去除微scopic中的图像噪声，而无需任何净图像或噪声模型。</li>
<li>methods: 该方法基于变分自动编码器（VAE），并使用特制的 autoregressive 解码器，可以模拟图像噪声的分布，但不能独立地模拟净图像的分布。</li>
<li>results: 实验结果表明，该方法可以超越现有的自监和无监督图像去噪方法，并且具有对拟合核心区域大小的 Robustness。 代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/krulllab/DVLAE%E3%80%82">https://github.com/krulllab/DVLAE。</a><details>
<summary>Abstract</summary>
Most unsupervised denoising methods are based on the assumption that imaging noise is either pixel-independent, i.e., spatially uncorrelated, or signal-independent, i.e., purely additive. However, in practice many imaging setups, especially in microscopy, suffer from a combination of signal-dependent noise (e.g. Poisson shot noise) and axis-aligned correlated noise (e.g. stripe shaped scanning or readout artifacts). In this paper, we present the first unsupervised deep learning-based denoiser that can remove this type of noise without access to any clean images or a noise model. Unlike self-supervised techniques, our method does not rely on removing pixels by masking or subsampling so can utilize all available information. We implement a Variational Autoencoder (VAE) with a specially designed autoregressive decoder capable of modelling the noise component of an image but incapable of independently modelling the underlying clean signal component. As a consequence, our VAE's encoder learns to encode only underlying clean signal content and to discard imaging noise. We also propose an additional decoder for mapping the encoder's latent variables back into image space, thereby sampling denoised images. Experimental results demonstrate that our approach surpasses existing methods for self- and unsupervised image denoising while being robust with respect to the size of the autoregressive receptive field. Code for this project can be found at https://github.com/krulllab/DVLAE.
</details>
<details>
<summary>摘要</summary>
大多数无监督干涉方法假设图像噪声是像素独立的，即空间不相关，或信号独立的，即纯加性的。然而，在实际应用中，许多图像设置，特别是微scopy，受到信号相关的噪声（例如摄件衰减噪声）和排序相关的噪声（例如扫描条形或读取 artifacts）的混合噪声。在这篇论文中，我们介绍了首个无监督深度学习基于的干涉方法，可以在没有干涉图像或噪声模型的情况下，去除这种类型的噪声。与自我监督技术不同，我们的方法不需要通过屏蔽或抽样来消除像素，因此可以利用所有可用的信息。我们实现了一种变量自动机（VAE），其拥有特制的 autoregressive 解码器，可以模型图像噪声的组成部分，但不能独立模型图像的净信号部分。因此，VAE 的编码器将只编码净信号内容，并且抛弃图像噪声。我们还提出了一种将编码器的缺省变量映射回图像空间的额外解码器，从而采样干涉后的净化图像。实验结果表明，我们的方法超越了现有的自我监督和无监督图像干涉方法，并且对 autoregressive 感知场的大小具有较好的稳定性。代码可以在 <https://github.com/krulllab/DVLAE> 找到。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Feature-Types-and-Their-Contributions-for-Camera-Tampering-Detection"><a href="#A-Survey-of-Feature-Types-and-Their-Contributions-for-Camera-Tampering-Detection" class="headerlink" title="A Survey of Feature Types and Their Contributions for Camera Tampering Detection"></a>A Survey of Feature Types and Their Contributions for Camera Tampering Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07886">http://arxiv.org/abs/2310.07886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Mantini, Shishir K. Shah</li>
<li>for: 这篇论文是关于摄像头妨碍检测的，它旨在检测摄像头中的非法和不可预期的修改。</li>
<li>methods: 这篇论文使用了时间序列分析方法来检测摄像头妨碍，并对不同特征类型进行了分析和实验研究。</li>
<li>results: 研究发现，使用不同特征类型可以提高摄像头妨碍检测的精度和可靠性。同时，研究还发现了不同特征类型在不同的妨碍情况下的表现不同。<details>
<summary>Abstract</summary>
Camera tamper detection is the ability to detect unauthorized and unintentional alterations in surveillance cameras by analyzing the video. Camera tampering can occur due to natural events or it can be caused intentionally to disrupt surveillance. We cast tampering detection as a change detection problem, and perform a review of the existing literature with emphasis on feature types. We formulate tampering detection as a time series analysis problem, and design experiments to study the robustness and capability of various feature types. We compute ten features on real-world surveillance video and apply time series analysis to ascertain their predictability, and their capability to detect tampering. Finally, we quantify the performance of various time series models using each feature type to detect tampering.
</details>
<details>
<summary>摘要</summary>
surveillance camera تammer detection 是指通过分析视频来检测非法和无意之变化。 camera tampering 可能由自然事件引起，也可能是故意的干扰Surveillance。 we cast tampering detection as a change detection problem，并对现有文献进行了审查，强调特征类型。 we formulate tampering detection as a time series analysis problem，并设计了许多实验来评估不同特征类型的可靠性和检测能力。 we compute ten features on real-world surveillance video and apply time series analysis to determine their predictability and ability to detect tampering. finally, we quantify the performance of various time series models using each feature type to detect tampering.Here's the text with some additional information about the features used in the study:surveillance camera تammer detection 是指通过分析视频来检测非法和无意之变化。 camera tampering 可能由自然事件引起，也可能是故意的干扰Surveillance。 we cast tampering detection as a change detection problem，并对现有文献进行了审查，强调特征类型。 we formulate tampering detection as a time series analysis problem，并设计了许多实验来评估不同特征类型的可靠性和检测能力。 we compute ten features on real-world surveillance video and apply time series analysis to determine their predictability and ability to detect tampering. these features include:1. color histogram2. color moments3. color co-occurrence matrices4. texture features5. edge features6. corner features7. motion features8. optical flow features9. histogram of oriented gradients (HOG)10. scale-invariant feature transform (SIFT)we quantify the performance of various time series models using each feature type to detect tampering. the time series models include:1. autoregressive (AR) models2. moving average (MA) models3. ARIMA models4. seasonal ARIMA (SARIMA) models5. long short-term memory (LSTM) modelswe evaluate the performance of each model using metrics such as detection accuracy, false alarm rate, and area under the receiver operating characteristic (ROC) curve.
</details></li>
</ul>
<hr>
<h2 id="BrainVoxGen-Deep-learning-framework-for-synthesis-of-Ultrasound-to-MRI"><a href="#BrainVoxGen-Deep-learning-framework-for-synthesis-of-Ultrasound-to-MRI" class="headerlink" title="BrainVoxGen: Deep learning framework for synthesis of Ultrasound to MRI"></a>BrainVoxGen: Deep learning framework for synthesis of Ultrasound to MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08608">http://arxiv.org/abs/2310.08608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubham Singh, Dr. Mrunal Bewoor, Ammar Ranapurwala, Satyam Rai, Sheetal Patil</li>
<li>for: 这个研究旨在使用深度学习框架将三维ultrasound图像转换为三维MRI图像。</li>
<li>methods: 这个方法使用Pix2Pix GAN模型，将三维ultrasound图像输入到UNET生成器和patch检测器中，生成对应的三维MRI图像。</li>
<li>results: 研究发现，这个方法可以成功地生成三维MRI图像，并且与预期结果 exhibit 一定的相似性。<details>
<summary>Abstract</summary>
The study presents a deep learning framework aimed at synthesizing 3D MRI volumes from three-dimensional ultrasound images of the brain utilizing the Pix2Pix GAN model. The process involves inputting a 3D volume of ultrasounds into a UNET generator and patch discriminator, generating a corresponding 3D volume of MRI. Model performance was evaluated using losses on the discriminator and generator applied to a dataset of 3D ultrasound and MRI images. The results indicate that the synthesized MRI images exhibit some similarity to the expected outcomes. Despite challenges related to dataset size, computational resources, and technical complexities, the method successfully generated MRI volume with a satisfactory similarity score meant to serve as a baseline for further research. It underscores the potential of deep learning-based volume synthesis techniques for ultrasound to MRI conversion, showcasing their viability for medical applications. Further refinement and exploration are warranted for enhanced clinical relevance.
</details>
<details>
<summary>摘要</summary>
这种研究描述了一种深度学习框架，用于从三维超声图像转换为三维MRI图像，使用Pix2Pix GAN模型。该过程中，将三维超声图像输入到UNET生成器和补做分类器中，生成相应的三维MRI图像。模型性能被评估使用生成器和分类器对三维超声和MRI图像集合的损失。结果表明，生成的MRI图像具有一定的相似性。虽然数据集的大小、计算资源和技术复杂度带来了挑战，但方法仍然成功地生成了MRI图像，并且得到了一个可接受的相似性分数。这种方法的成功表明了深度学习基于超声到MRI转换的卷积synthesis技术的可能性，并且为医疗应用提供了一个可靠的基础。进一步的改进和探索是需要的，以提高临床 relevance。
</details></li>
</ul>
<hr>
<h2 id="CrIBo-Self-Supervised-Learning-via-Cross-Image-Object-Level-Bootstrapping"><a href="#CrIBo-Self-Supervised-Learning-via-Cross-Image-Object-Level-Bootstrapping" class="headerlink" title="CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping"></a>CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07855">http://arxiv.org/abs/2310.07855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Lebailly, Thomas Stegmüller, Behzad Bozorgtabar, Jean-Philippe Thiran, Tinne Tuytelaars</li>
<li>for: 提高 dense visual representation learning 的精度和效能</li>
<li>methods: 使用 object-level nearest neighbor bootstrapping 方法，在训练过程中对每个对象进行精细化的 Bootstrapping，以提高模型在具有多个对象的场景下的表现</li>
<li>results: 在具有多个对象的场景下，CrIBo 表现出色，在具有 nearest neighbor retrieval 的测试任务上达到了领先的性能水平，并在标准下游任务中保持竞争力In English, this means:</li>
<li>for: Improving the accuracy and efficiency of dense visual representation learning</li>
<li>methods: Using object-level nearest neighbor bootstrapping method, fine-grained bootstrapping is performed for each object during training to enhance the model’s performance in scenes with multiple objects</li>
<li>results: CrIBo achieves state-of-the-art performance on tasks with nearest neighbor retrieval in scenes with multiple objects, and is highly competitive in standard downstream segmentation tasks.<details>
<summary>Abstract</summary>
Leveraging nearest neighbor retrieval for self-supervised representation learning has proven beneficial with object-centric images. However, this approach faces limitations when applied to scene-centric datasets, where multiple objects within an image are only implicitly captured in the global representation. Such global bootstrapping can lead to undesirable entanglement of object representations. Furthermore, even object-centric datasets stand to benefit from a finer-grained bootstrapping approach. In response to these challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method tailored to enhance dense visual representation learning. By employing object-level nearest neighbor bootstrapping throughout the training, CrIBo emerges as a notably strong and adequate candidate for in-context learning, leveraging nearest neighbor retrieval at test time. CrIBo shows state-of-the-art performance on the latter task while being highly competitive in more standard downstream segmentation tasks. Our code and pretrained models will be publicly available upon acceptance.
</details>
<details>
<summary>摘要</summary>
利用最近邻居检索来进行自我超vised表示学习已经证明有利于对象中心图像。然而，这种方法在场景中心数据集上遇到限制，因为图像中多个对象仅在全局表示中被间接捕捉。这可能导致对象表示的不良杂化。此外，即使对象中心数据集也可以从更细化的杂化方法中受益。为回应这些挑战，我们提出了一种新的跨图像对象级Bootstrapping方法，可以提高粗细视觉表示学习。我们在训练中使用对象级最近邻居检索，并在测试时使用最近邻居检索。我们称之为CrIBo。CrIBo在具有状态的下游分 segmentation任务中表现出色，而且在标准下游任务中也具有高竞争力。我们将代码和预训练模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="Explorable-Mesh-Deformation-Subspaces-from-Unstructured-Generative-Models"><a href="#Explorable-Mesh-Deformation-Subspaces-from-Unstructured-Generative-Models" class="headerlink" title="Explorable Mesh Deformation Subspaces from Unstructured Generative Models"></a>Explorable Mesh Deformation Subspaces from Unstructured Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07814">http://arxiv.org/abs/2310.07814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arman Maesumi, Paul Guerrero, Vladimir G. Kim, Matthew Fisher, Siddhartha Chaudhuri, Noam Aigerman, Daniel Ritchie</li>
<li>for: 实现3D形状的变化探索，从传入的指标形状中找到可视化的2D探索空间，并将该空间转换为训练过的生成模型中的子空间，以实现高质量的这些形状之间的变化探索。</li>
<li>methods: 使用生成模型的高维度 latent space 进行探索，并寻找一个将这些 latent space 转换为可视化的2D探索空间的映射。然后，使用这个映射将变化在2D空间中，并将其转换为高质量这些形状的塑形场。</li>
<li>results: 可以实现视觉上可见且易于探索的2D探索空间，并将该空间转换为高质量这些形状之间的变化。对于一些形状 category 进行了评估，结果显示了与先前的学习塑形空间的比较，其可以实现更多的变化和更好的可视化。<details>
<summary>Abstract</summary>
Exploring variations of 3D shapes is a time-consuming process in traditional 3D modeling tools. Deep generative models of 3D shapes often feature continuous latent spaces that can, in principle, be used to explore potential variations starting from a set of input shapes. In practice, doing so can be problematic: latent spaces are high dimensional and hard to visualize, contain shapes that are not relevant to the input shapes, and linear paths through them often lead to sub-optimal shape transitions. Furthermore, one would ideally be able to explore variations in the original high-quality meshes used to train the generative model, not its lower-quality output geometry. In this paper, we present a method to explore variations among a given set of landmark shapes by constructing a mapping from an easily-navigable 2D exploration space to a subspace of a pre-trained generative model. We first describe how to find a mapping that spans the set of input landmark shapes and exhibits smooth variations between them. We then show how to turn the variations in this subspace into deformation fields, to transfer those variations to high-quality meshes for the landmark shapes. Our results show that our method can produce visually-pleasing and easily-navigable 2D exploration spaces for several different shape categories, especially as compared to prior work on learning deformation spaces for 3D shapes.
</details>
<details>
<summary>摘要</summary>
将文本翻译成简化中文。 traditional 3D 模型化工具中查找3D 形状的变化是一个时间消耗的过程。深度生成模型中的3D 形状通常具有连续的潜在空间，可以从输入形状开始探索潜在的变化。然而，在实践中，这可以是一个问题：潜在空间的维度很高，Difficult to visualize, contains shapes that are not relevant to the input shapes, and linear paths through them often lead to sub-optimal shape transitions. In this paper, we present a method to explore variations among a given set of landmark shapes by constructing a mapping from an easily-navigable 2D exploration space to a subspace of a pre-trained generative model. We first describe how to find a mapping that spans the set of input landmark shapes and exhibits smooth variations between them. We then show how to turn the variations in this subspace into deformation fields, to transfer those variations to high-quality meshes for the landmark shapes. Our results show that our method can produce visually-pleasing and easily-navigable 2D exploration spaces for several different shape categories, especially as compared to prior work on learning deformation spaces for 3D shapes.</SYS>以下是翻译结果：传统的3D模型化工具中查找3D形状的变化是一个时间消耗的过程。深度生成模型中的3D形状通常具有连续的潜在空间，可以从输入形状开始探索潜在的变化。然而，在实践中，这可以是一个问题：潜在空间的维度很高，Difficult to visualize，contains shapes that are not relevant to the input shapes，和Linear paths through them often lead to sub-optimal shape transitions。在这篇论文中，我们提出了一种方法，通过从可探索的2D探索空间到预训练的生成模型的子空间中构建映射，以探索给定的标记形状中的变化。我们首先描述了如何找到一个涵盖输入标记形状的映射，并且在这个映射中展示了平滑的变化。然后，我们显示了如何将这个映射中的变化转换为塑形场，以传递这些变化到高质量的矩阵 для标记形状。我们的结果表明，我们的方法可以生成可观赏和易探索的2D探索空间，特别是与先前学习3D形状的塑形空间相比。
</details></li>
</ul>
<hr>
<h2 id="CRITERIA-a-New-Benchmarking-Paradigm-for-Evaluating-Trajectory-Prediction-Models-for-Autonomous-Driving"><a href="#CRITERIA-a-New-Benchmarking-Paradigm-for-Evaluating-Trajectory-Prediction-Models-for-Autonomous-Driving" class="headerlink" title="CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving"></a>CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07794">http://arxiv.org/abs/2310.07794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changhe Chen, Mozhgan Pourkeshavarz, Amir Rasouli</li>
<li>for: 本文提出了一个新的评估巡回预测方法的评估方程（CRITERIA），用于评估自动驾驶巡回预测模型的性能。</li>
<li>methods: 本文提出了以下方法：1）通过根据道路结构、模型性能和数据特性提取驾驶场景，实现细化的评估模型表现；2）基于实际驾驶约束，开发了不偏度的多元指标来衡量巡回预测模型的多样性和合法性。</li>
<li>results: 经过广泛的实验， authors发现提出的评估方程可以更准确地评估巡回预测模型的性能，并且可以作为评估模型行为的方式。 authors还进行了缺省研究，以阐明不同元素在计算提出的指标中的贡献。<details>
<summary>Abstract</summary>
Benchmarking is a common method for evaluating trajectory prediction models for autonomous driving. Existing benchmarks rely on datasets, which are biased towards more common scenarios, such as cruising, and distance-based metrics that are computed by averaging over all scenarios. Following such a regiment provides a little insight into the properties of the models both in terms of how well they can handle different scenarios and how admissible and diverse their outputs are. There exist a number of complementary metrics designed to measure the admissibility and diversity of trajectories, however, they suffer from biases, such as length of trajectories.   In this paper, we propose a new benChmarking paRadIgm for evaluaTing trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a method for extracting driving scenarios at varying levels of specificity according to the structure of the roads, models' performance, and data properties for fine-grained ranking of prediction models; 2) A set of new bias-free metrics for measuring diversity, by incorporating the characteristics of a given scenario, and admissibility, by considering the structure of roads and kinematic compliancy, motivated by real-world driving constraints. 3) Using the proposed benchmark, we conduct extensive experimentation on a representative set of the prediction models using the large scale Argoverse dataset. We show that the proposed benchmark can produce a more accurate ranking of the models and serve as a means of characterizing their behavior. We further present ablation studies to highlight contributions of different elements that are used to compute the proposed metrics.
</details>
<details>
<summary>摘要</summary>
《用于评估自动驾驶路径预测模型的benchmarking方法》是一个常见的评估方法。现有的benchmark围绕着常见的enario，如维持速度，集成了距离基于的metric，这些metric通过平均所有scenario来计算。然而，这些benchmark存在偏见，如scenario的长度。在这篇论文中，我们提出了一个新的benchmarking方法（CRITERIA），具有以下特点：1. 提取了不同级别的驾驶scenario，根据道路结构、模型性能和数据特性进行细化的排名预测模型。2. 提出了一组新的偏见自由的多元指标，通过考虑场景特点和道路结构来衡量多样性和合法性。3. 使用大规模的Argoverse dataset进行了广泛的实验，并证明了提出的benchmark可以更准确地排名模型，并且可以用来描述模型的行为。我们还进行了减少实验来 highlight提出的元素的贡献。
</details></li>
</ul>
<hr>
<h2 id="An-automated-approach-for-improving-the-inference-latency-and-energy-efficiency-of-pretrained-CNNs-by-removing-irrelevant-pixels-with-focused-convolutions"><a href="#An-automated-approach-for-improving-the-inference-latency-and-energy-efficiency-of-pretrained-CNNs-by-removing-irrelevant-pixels-with-focused-convolutions" class="headerlink" title="An automated approach for improving the inference latency and energy efficiency of pretrained CNNs by removing irrelevant pixels with focused convolutions"></a>An automated approach for improving the inference latency and energy efficiency of pretrained CNNs by removing irrelevant pixels with focused convolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07782">http://arxiv.org/abs/2310.07782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PurdueCAM2Project/focused-convolutions">https://github.com/PurdueCAM2Project/focused-convolutions</a></li>
<li>paper_authors: Caleb Tung, Nicholas Eliopoulos, Purvish Jajal, Gowri Ramshankar, Chen-Yun Yang, Nicholas Synovic, Xuecen Zhang, Vipin Chaudhary, George K. Thiruvathukal, Yung-Hsiang Lu</li>
<li>for: 提高 Convolutional Neural Networks (CNNs) 的能效性，降低 computation 和能源成本。</li>
<li>methods: 提出一种自动化方法，通过插入一个阈值层来筛选之前层的活动，以实现更加精准地忽略图像中无关部分，从而降低推理延迟和能源成本，保持准确性。</li>
<li>results: 对多种流行的预训练 CNNs 进行了实验，发现该方法可以降低推理延迟（最多下降25%）和能源成本（最多下降22%），而且准确性几乎不受影响。<details>
<summary>Abstract</summary>
Computer vision often uses highly accurate Convolutional Neural Networks (CNNs), but these deep learning models are associated with ever-increasing energy and computation requirements. Producing more energy-efficient CNNs often requires model training which can be cost-prohibitive. We propose a novel, automated method to make a pretrained CNN more energy-efficient without re-training. Given a pretrained CNN, we insert a threshold layer that filters activations from the preceding layers to identify regions of the image that are irrelevant, i.e. can be ignored by the following layers while maintaining accuracy. Our modified focused convolution operation saves inference latency (by up to 25%) and energy costs (by up to 22%) on various popular pretrained CNNs, with little to no loss in accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="3D-TransUNet-Advancing-Medical-Image-Segmentation-through-Vision-Transformers"><a href="#3D-TransUNet-Advancing-Medical-Image-Segmentation-through-Vision-Transformers" class="headerlink" title="3D TransUNet: Advancing Medical Image Segmentation through Vision Transformers"></a>3D TransUNet: Advancing Medical Image Segmentation through Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07781">http://arxiv.org/abs/2310.07781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Beckschen/3D-TransUNet">https://github.com/Beckschen/3D-TransUNet</a></li>
<li>paper_authors: Jieneng Chen, Jieru Mei, Xianhang Li, Yongyi Lu, Qihang Yu, Qingyue Wei, Xiangde Luo, Yutong Xie, Ehsan Adeli, Yan Wang, Matthew Lungren, Lei Xing, Le Lu, Alan Yuille, Yuyin Zhou</li>
<li>for: 这篇论文的目的是提出一个基于Transformer的医疗影像分类网络，以提高医疗影像分类的精度和效率。</li>
<li>methods: 这篇论文使用了Transformer的自注意力机制来补充U-Net的本地信息，以提高医疗影像分类的能力。具体来说，论文提出了两个关键的 ком成分：1）使用Transformer嵌入器将影像片段转换为Token，以EXTRACT全局背景信息；2）使用Transformer解oder适应地对候选区域进行修饰，通过跨andidate proposal和U-Net特征之间的相互注意力。</li>
<li>results: 论文的实验结果显示，不同的医疗任务可以从不同的架构设计中获得更好的效果。Transformer嵌入器在多器官分类任务中表现出色，而Transformer解oder则在较小且具有挑战性的分类目标，如肿瘤分类任务中表现更好。总的来说，结合Transformer-based嵌入器和解oder into U-Net架构可以提高医疗影像分类的精度和效率。<details>
<summary>Abstract</summary>
Medical image segmentation plays a crucial role in advancing healthcare systems for disease diagnosis and treatment planning. The u-shaped architecture, popularly known as U-Net, has proven highly successful for various medical image segmentation tasks. However, U-Net's convolution-based operations inherently limit its ability to model long-range dependencies effectively. To address these limitations, researchers have turned to Transformers, renowned for their global self-attention mechanisms, as alternative architectures. One popular network is our previous TransUNet, which leverages Transformers' self-attention to complement U-Net's localized information with the global context. In this paper, we extend the 2D TransUNet architecture to a 3D network by building upon the state-of-the-art nnU-Net architecture, and fully exploring Transformers' potential in both the encoder and decoder design. We introduce two key components: 1) A Transformer encoder that tokenizes image patches from a convolution neural network (CNN) feature map, enabling the extraction of global contexts, and 2) A Transformer decoder that adaptively refines candidate regions by utilizing cross-attention between candidate proposals and U-Net features. Our investigations reveal that different medical tasks benefit from distinct architectural designs. The Transformer encoder excels in multi-organ segmentation, where the relationship among organs is crucial. On the other hand, the Transformer decoder proves more beneficial for dealing with small and challenging segmented targets such as tumor segmentation. Extensive experiments showcase the significant potential of integrating a Transformer-based encoder and decoder into the u-shaped medical image segmentation architecture. TransUNet outperforms competitors in various medical applications.
</details>
<details>
<summary>摘要</summary>
医疗影像分割对于提高医疗系统的疾病诊断和治疗规划具有关键作用。U-Net建筑，也称为U-shaped architecture，在各种医疗影像分割任务中表现出了极高的成功率。然而，U-Net的卷积操作自然地限制了它的长距离依赖性模型能力。为了解决这些限制，研究人员转向了Transformers，这种global self-attention机制的知名网络。在这篇论文中，我们扩展了2D TransUNet架构到3D网络，基于state-of-the-art nnU-Net架构，并充分发挥Transformers的潜力在encoder和decoder设计中。我们介绍了两个关键组件：1）使用CNN特征图像块 Tokenizer，从CNN特征图像中提取全局上下文，2）使用交叉关注来适应候选区域的精度调整。我们的调查表明，不同的医疗任务需要不同的架构设计。Transformer encoder在多器官分割任务中表现出色，因为器官之间的关系非常重要。然而，Transformer decoder在处理小型和复杂分割目标，如肿瘤分割任务中表现更出色。我们的实验结果表明，将Transformer-based encoder和decoder与U-shaped医疗影像分割架构结合使用，可以提高TransUNet的性能，并在各种医疗应用中超越竞争对手。
</details></li>
</ul>
<hr>
<h2 id="OpenLEAF-Open-Domain-Interleaved-Image-Text-Generation-and-Evaluation"><a href="#OpenLEAF-Open-Domain-Interleaved-Image-Text-Generation-and-Evaluation" class="headerlink" title="OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation"></a>OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07749">http://arxiv.org/abs/2310.07749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie An, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Lijuan Wang, Jiebo Luo</li>
<li>for: 该论文探讨了一个开放领域图文生成任务，该任务通过输入查询生成杂合的图文内容。</li>
<li>methods: 该论文提出了一个基于大型自然语言模型（LLM）和预训练文本到图像（T2I）模型的新杂合生成框架，名为OpenLEAF。该框架中的LLM生成文本描述，协调T2I模型，创建视觉提示图像生成，并将全局上下文 integrate 到T2I模型中。</li>
<li>results: 根据我们构建的评估集，使用大型多modal模型（LMM）评估Entity和Style一致性，我们的提出的杂合生成框架可以在不同领域和应用中生成高质量的图文内容，如问答、故事、图文重新写作、宣传品等。此外，我们验证了我们提出的LMM评估技术的有效性。<details>
<summary>Abstract</summary>
This work investigates a challenging task named open-domain interleaved image-text generation, which generates interleaved texts and images following an input query. We propose a new interleaved generation framework based on prompting large-language models (LLMs) and pre-trained text-to-image (T2I) models, namely OpenLEAF. In OpenLEAF, the LLM generates textual descriptions, coordinates T2I models, creates visual prompts for generating images, and incorporates global contexts into the T2I models. This global context improves the entity and style consistencies of images in the interleaved generation. For model assessment, we first propose to use large multi-modal models (LMMs) to evaluate the entity and style consistencies of open-domain interleaved image-text sequences. According to the LMM evaluation on our constructed evaluation set, the proposed interleaved generation framework can generate high-quality image-text content for various domains and applications, such as how-to question answering, storytelling, graphical story rewriting, and webpage/poster generation tasks. Moreover, we validate the effectiveness of the proposed LMM evaluation technique with human assessment. We hope our proposed framework, benchmark, and LMM evaluation could help establish the intriguing interleaved image-text generation task.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "open-domain" is translated as "开放领域" (kāifàng lǐngyè)* "interleaved" is translated as "交错" (jiāo chá)* "image-text generation" is translated as "图文生成" (túwén shēngchǎng)* "LLMs" is translated as "大型语言模型" (dàxìng yǔyán módelì)* "T2I models" is translated as "文本到图像模型" (wén tiě dào tú xiàng módelì)* "global context" is translated as "全局上下文" (quán jí shàng xiàng)* "entity and style consistencies" is translated as "实体和风格一致性" (shíwù hé fēng gé yīchāngxìng)* "LMMs" is translated as "大型多模态模型" (dàxìng duō móduō módelì)* "evaluation set" is translated as "评估集" (píngjǐ zhù)* "how-to question answering" is translated as "如何问答" (rúhěn wèn dá)* "storytelling" is translated as "故事告诉" (gùshì gào shuō)* "graphical story rewriting" is translated as "图文故事重写" (túwén gùshì zhòngxī)* "webpage/poster generation" is translated as "网页/海报生成" (wǎng jiāng/hǎi bào shēngchǎng)
</details></li>
</ul>
<hr>
<h2 id="ScaleCrafter-Tuning-free-Higher-Resolution-Visual-Generation-with-Diffusion-Models"><a href="#ScaleCrafter-Tuning-free-Higher-Resolution-Visual-Generation-with-Diffusion-Models" class="headerlink" title="ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models"></a>ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07702">http://arxiv.org/abs/2310.07702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yingqinghe/scalecrafter">https://github.com/yingqinghe/scalecrafter</a></li>
<li>paper_authors: Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, Ying Shan</li>
<li>for: 本研究探讨了使用预训练的扩散模型在更高的分辨率上生成图像，并且图像的方向比例可以是任意的。</li>
<li>methods: 我们提出了一种简单 yet有效的重尺刻法，可以在推理时动态调整卷积核心的见觉范围，以解决预训练模型中的问题。我们还提出了分散卷积和噪声抑制的自由导向方法，可以实现超高分辨率图像生成（例如4096 x 4096）。</li>
<li>results: 我们的方法可以很好地解决重复问题，并且在高分辨率图像生成中达到了状态的表现。我们的方法不需要任何训练或优化。广泛的实验表明，我们的方法可以在高分辨率图像生成中很好地保持细节Texture。<details>
<summary>Abstract</summary>
In this work, we investigate the capability of generating images from pre-trained diffusion models at much higher resolutions than the training image sizes. In addition, the generated images should have arbitrary image aspect ratios. When generating images directly at a higher resolution, 1024 x 1024, with the pre-trained Stable Diffusion using training images of resolution 512 x 512, we observe persistent problems of object repetition and unreasonable object structures. Existing works for higher-resolution generation, such as attention-based and joint-diffusion approaches, cannot well address these issues. As a new perspective, we examine the structural components of the U-Net in diffusion models and identify the crucial cause as the limited perception field of convolutional kernels. Based on this key observation, we propose a simple yet effective re-dilation that can dynamically adjust the convolutional perception field during inference. We further propose the dispersed convolution and noise-damped classifier-free guidance, which can enable ultra-high-resolution image generation (e.g., 4096 x 4096). Notably, our approach does not require any training or optimization. Extensive experiments demonstrate that our approach can address the repetition issue well and achieve state-of-the-art performance on higher-resolution image synthesis, especially in texture details. Our work also suggests that a pre-trained diffusion model trained on low-resolution images can be directly used for high-resolution visual generation without further tuning, which may provide insights for future research on ultra-high-resolution image and video synthesis.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们 investigate了使用预训练的扩散模型生成高分辨率图像。此外，生成的图像应该有任意的图像方向比。当直接在更高的分辨率1024x1024上生成图像，使用预训练的稳定扩散模型，我们观察到了持续的 объек repeating和不合理的对象结构问题。现有的高分辨率生成方法，如注意力基的方法和联合扩散方法，无法好解这些问题。作为一个新的视角，我们分析了扩散模型中的结构组件，并确定了归一化核心的局限性为主要原因。基于这一关键观察，我们提出了一种简单又有效的重定义，可以在推理过程中动态调整归一化核心的见觉范围。我们还提出了散布核心和噪声抑制的类别器-自由导航，可以实现ultra-高分辨率图像生成（例如4096x4096）。需要注意的是，我们的方法不需要任何的训练或优化。广泛的实验表明，我们的方法可以很好地解决重复问题，并在高分辨率图像生成中 achieved state-of-the-art表现，特别是在тексту册细节方面。我们的工作还表明了一个预训练的扩散模型可以直接在高分辨率图像生成中使用，无需进一步调整，这可能提供了未来研究高分辨率图像和视频生成的灵感。
</details></li>
</ul>
<hr>
<h2 id="ConditionVideo-Training-Free-Condition-Guided-Text-to-Video-Generation"><a href="#ConditionVideo-Training-Free-Condition-Guided-Text-to-Video-Generation" class="headerlink" title="ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation"></a>ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07697">http://arxiv.org/abs/2310.07697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Peng, Xinyuan Chen, Yaohui Wang, Chaochao Lu, Yu Qiao</li>
<li>for: 文章主要针对的问题是如何通过提供条件、视频和输入文本，生成高质量的动态视频。</li>
<li>methods: 本文提出了一种无需训练的文本到视频生成方法，基于现有的文本到图像生成方法（如稳定扩散），并通过分解动态场景的运动表示来提高生成的 temporal coherence。</li>
<li>results: 对比其他方法，本文的方法在Frame consistency、clip score和 conditional accuracy等指标上表现出色，得到了更高的性能。<details>
<summary>Abstract</summary>
Recent works have successfully extended large-scale text-to-image models to the video domain, producing promising results but at a high computational cost and requiring a large amount of video data. In this work, we introduce ConditionVideo, a training-free approach to text-to-video generation based on the provided condition, video, and input text, by leveraging the power of off-the-shelf text-to-image generation methods (e.g., Stable Diffusion). ConditionVideo generates realistic dynamic videos from random noise or given scene videos. Our method explicitly disentangles the motion representation into condition-guided and scenery motion components. To this end, the ConditionVideo model is designed with a UNet branch and a control branch. To improve temporal coherence, we introduce sparse bi-directional spatial-temporal attention (sBiST-Attn). The 3D control network extends the conventional 2D controlnet model, aiming to strengthen conditional generation accuracy by additionally leveraging the bi-directional frames in the temporal domain. Our method exhibits superior performance in terms of frame consistency, clip score, and conditional accuracy, outperforming other compared methods.
</details>
<details>
<summary>摘要</summary>
近期研究已成功扩展大规模文本到视频领域，但计算成本高并需大量视频数据。在这项工作中，我们介绍ConditionVideo，一种无需训练的文本到视频生成方法，基于给定的条件、视频和输入文本，通过利用市场上已有的文本到图像生成方法（如稳定扩散）。ConditionVideo可生成真实的动态视频从随机噪声或给定的场景视频。我们的方法明确分离动作表示为条件导向的动作组件和场景动作组件。为此，ConditionVideo模型采用了UNet分支和控制分支。为了改进时间准确性，我们引入罕见的双向时空注意力（sBiST-Attn）。三维控制网络将传统的二维控制网络模型扩展到三维空间，以更好地利用时间领域的双向帧。我们的方法在帧一致性、clip分数和条件准确性方面表现出色，超越其他比较方法。
</details></li>
</ul>
<hr>
<h2 id="Orbital-Polarimetric-Tomography-of-a-Flare-Near-the-Sagittarius-A-Supermassive-Black-Hole"><a href="#Orbital-Polarimetric-Tomography-of-a-Flare-Near-the-Sagittarius-A-Supermassive-Black-Hole" class="headerlink" title="Orbital Polarimetric Tomography of a Flare Near the Sagittarius A* Supermassive Black Hole"></a>Orbital Polarimetric Tomography of a Flare Near the Sagittarius A* Supermassive Black Hole</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07687">http://arxiv.org/abs/2310.07687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviad Levis, Andrew A. Chael, Katherine L. Bouman, Maciek Wielgus, Pratul P. Srinivasan</li>
<li>for: 这个研究的目的是理解黑洞吸收过程中的高能射线飞亮现象。</li>
<li>methods: 这个研究使用了人工神经网络三维表示（一种emergent artificial intelligence方法）和黑洞重力模型来解决高度不定Posing的Tomography问题，以恢复黑洞中央的辐射强度。</li>
<li>results: 研究结果表明，在2017年4月11日ALMA数据中存在一个位于黑洞事件 horizonto的紧凑的明亮区域，距离黑洞6倍，并且在低倾斜的orbital平面上进行clockwise旋转。这些结果与以往的EHT和GRAVITY合作的研究结果一致。<details>
<summary>Abstract</summary>
The interaction between the supermassive black hole at the center of the Milky Way, Sagittarius A$^*$, and its accretion disk, occasionally produces high energy flares seen in X-ray, infrared and radio. One mechanism for observed flares is the formation of compact bright regions that appear within the accretion disk and close to the event horizon. Understanding these flares can provide a window into black hole accretion processes. Although sophisticated simulations predict the formation of these flares, their structure has yet to be recovered by observations. Here we show the first three-dimensional (3D) reconstruction of an emission flare in orbit recovered from ALMA light curves observed on April 11, 2017. Our recovery results show compact bright regions at a distance of roughly 6 times the event horizon. Moreover, our recovery suggests a clockwise rotation in a low-inclination orbital plane, a result consistent with prior studies by EHT and GRAVITY collaborations. To recover this emission structure we solve a highly ill-posed tomography problem by integrating a neural 3D representation (an emergent artificial intelligence approach for 3D reconstruction) with a gravitational model for black holes. Although the recovered 3D structure is subject, and sometimes sensitive, to the model assumptions, under physically motivated choices we find that our results are stable and our approach is successful on simulated data. We anticipate that in the future, this approach could be used to analyze a richer collection of time-series data that could shed light on the mechanisms governing black hole and plasma dynamics.
</details>
<details>
<summary>摘要</summary>
黑洞的中心星系银河系的超大质量黑洞，Sagittarius A*，与其吸积盘 occasionally produce high energy flares visible in X-ray, infrared and radio。一种导致这些闪光的机制是在吸积盘中形成 Compact bright regions，这些区域位于黑洞事件 horizons 附近。理解这些闪光可以提供黑洞吸积过程的窗口。虽然先进的仿真 simulation 预测了这些闪光的形成，但它们的结构还没有由观测所回收。在这里，我们展示了由 ALMA 光谱 curves 于2017年4月11日观测到的首次三维 (3D) 重建的发射闪光。我们的重建结果表明 Compact bright regions 的距离约为黑洞事件 horizon 的6倍。此外，我们的重建结果还表明这些闪光在低倾斜的 орбиталь平面上以 clockwise 方向旋转，这与 prior studies by EHT 和 GRAVITY collaborations 的结果相符。为了重建这些发射结构，我们解决了一个高度不定 проблеma tomography 问题，通过结合人工智能 Representation （一种 emergent artificial intelligence 方法）和 gravitational model for black holes 来做。虽然recovered 3D structure 受到模型假设的影响，但在物理上有理由的选择下，我们发现结果是稳定的，并且在 simulated data 上成功。未来，我们预计这种方法可以用来分析更加丰富的时间序列数据，以了解黑洞和气体动力学的机制。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-MET-Overexpression-in-Non-Small-Cell-Lung-Adenocarcinomas-from-Hematoxylin-and-Eosin-Images"><a href="#Prediction-of-MET-Overexpression-in-Non-Small-Cell-Lung-Adenocarcinomas-from-Hematoxylin-and-Eosin-Images" class="headerlink" title="Prediction of MET Overexpression in Non-Small Cell Lung Adenocarcinomas from Hematoxylin and Eosin Images"></a>Prediction of MET Overexpression in Non-Small Cell Lung Adenocarcinomas from Hematoxylin and Eosin Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07682">http://arxiv.org/abs/2310.07682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshitij Ingale, Sun Hae Hong, Josh S. K. Bell, Abbas Rizvi, Amy Welch, Lingdao Sha, Irvin Ho, Kunal Nagpal, Aicha BenTaieb, Rohan P Joshi, Martin C Stumpe<br>for: 这个研究旨在开发一种使用 routinely available digitized H&amp;E 染色片预测 MET 蛋白过表达的算法，以便为 NSCLC 患者预测是否有可能获得 ME 蛋白或 ME 基因表达状态的诊断。methods: 这个研究使用了一个大型的匹配 H&amp;E 染色片和 RNA 表达数据库来训练一种弱级超vised 模型，以直接从 H&amp;E 图像中预测 MET RNA 过表达。results: 这个模型在一个独立的占据测试集上进行了评估，并达到了 ROC-AUC 0.70（95% 信息interval：0.66-0.74）的稳定性特征，并且在不同的患者临床变量下表现稳定，并且对于synthetic 噪声进行了Robust性测试。这些结果表明，H&amp;E 基本的预测模型可能是一种有用的工具，以便为 NSCLC 患者预测 ME 蛋白或 ME 基因表达状态的可能性。<details>
<summary>Abstract</summary>
MET protein overexpression is a targetable event in non-small cell lung cancer (NSCLC) and is the subject of active drug development. Challenges in identifying patients for these therapies include lack of access to validated testing, such as standardized immunohistochemistry (IHC) assessment, and consumption of valuable tissue for a single gene/protein assay. Development of pre-screening algorithms using routinely available digitized hematoxylin and eosin (H&E)-stained slides to predict MET overexpression could promote testing for those who will benefit most. While assessment of MET expression using IHC is currently not routinely performed in NSCLC, next-generation sequencing is common and in some cases includes RNA expression panel testing. In this work, we leveraged a large database of matched H&E slides and RNA expression data to train a weakly supervised model to predict MET RNA overexpression directly from H&E images. This model was evaluated on an independent holdout test set of 300 over-expressed and 289 normal patients, demonstrating an ROC-AUC of 0.70 (95th percentile interval: 0.66 - 0.74) with stable performance characteristics across different patient clinical variables and robust to synthetic noise on the test set. These results suggest that H&E-based predictive models could be useful to prioritize patients for confirmatory testing of MET protein or MET gene expression status.
</details>
<details>
<summary>摘要</summary>
MET蛋白过表达是非小细胞肺癌（NSCLC）中可target的事件，目前有活跃的药物开发。检测patient中MET蛋白过表达的挑战包括lack of access to validated testing, such as standardized immunohistochemistry (IHC) assessment, and consumption of valuable tissue for a single gene/protein assay。Development of pre-screening algorithms using routinely available digitized hematoxylin and eosin (H&E)-stained slides to predict MET overexpression could promote testing for those who will benefit most. Although assessment of MET expression using IHC is currently not routinely performed in NSCLC, next-generation sequencing is common and in some cases includes RNA expression panel testing. In this work, we leveraged a large database of matched H&E slides and RNA expression data to train a weakly supervised model to predict MET RNA overexpression directly from H&E images. This model was evaluated on an independent holdout test set of 300 over-expressed and 289 normal patients, demonstrating an ROC-AUC of 0.70 (95th percentile interval: 0.66 - 0.74) with stable performance characteristics across different patient clinical variables and robust to synthetic noise on the test set. These results suggest that H&E-based predictive models could be useful to prioritize patients for confirmatory testing of MET protein or MET gene expression status.
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Vision-Transformers-Based-on-Heterogeneous-Attention-Patterns"><a href="#Accelerating-Vision-Transformers-Based-on-Heterogeneous-Attention-Patterns" class="headerlink" title="Accelerating Vision Transformers Based on Heterogeneous Attention Patterns"></a>Accelerating Vision Transformers Based on Heterogeneous Attention Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07664">http://arxiv.org/abs/2310.07664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deli Yu, Teng Xi, Jianwei Li, Baopu Li, Gang Zhang, Haocheng Feng, Junyu Han, Jingtuo Liu, Errui Ding, Jingdong Wang</li>
<li>for: 提高 ViT 的运行速度，以便在计算复杂的自注意 Mechanism 上进行减少计算量。</li>
<li>methods: 提出了一个集成压缩管道，包括动态指导静止自注意 (DGSSA) 方法和全球聚合 pyramid (GLAD) 方法，以减少 ViT 的计算量。</li>
<li>results: 实验表明，该集成压缩管道可以提高 ViT 的运行速度，相比 DeiT 提高了121% 的运行throughput，超过了所有 SOTA 方法。<details>
<summary>Abstract</summary>
Recently, Vision Transformers (ViTs) have attracted a lot of attention in the field of computer vision. Generally, the powerful representative capacity of ViTs mainly benefits from the self-attention mechanism, which has a high computation complexity. To accelerate ViTs, we propose an integrated compression pipeline based on observed heterogeneous attention patterns across layers. On one hand, different images share more similar attention patterns in early layers than later layers, indicating that the dynamic query-by-key self-attention matrix may be replaced with a static self-attention matrix in early layers. Then, we propose a dynamic-guided static self-attention (DGSSA) method where the matrix inherits self-attention information from the replaced dynamic self-attention to effectively improve the feature representation ability of ViTs. On the other hand, the attention maps have more low-rank patterns, which reflect token redundancy, in later layers than early layers. In a view of linear dimension reduction, we further propose a method of global aggregation pyramid (GLAD) to reduce the number of tokens in later layers of ViTs, such as Deit. Experimentally, the integrated compression pipeline of DGSSA and GLAD can accelerate up to 121% run-time throughput compared with DeiT, which surpasses all SOTA approaches.
</details>
<details>
<summary>摘要</summary>
近期，视transformer（ViTs）在计算机视觉领域引起了很多关注。通常，ViTs的强大表现capacity mainly benefits from the self-attention mechanism, which has high computation complexity. To accelerate ViTs, we propose an integrated compression pipeline based on observed heterogeneous attention patterns across layers. On one hand, different images share more similar attention patterns in early layers than later layers, indicating that the dynamic query-by-key self-attention matrix may be replaced with a static self-attention matrix in early layers. Then, we propose a dynamic-guided static self-attention (DGSSA) method where the matrix inherits self-attention information from the replaced dynamic self-attention to effectively improve the feature representation ability of ViTs. On the other hand, the attention maps have more low-rank patterns, which reflect token redundancy, in later layers than early layers. In a view of linear dimension reduction, we further propose a method of global aggregation pyramid (GLAD) to reduce the number of tokens in later layers of ViTs, such as Deit. Experimentally, the integrated compression pipeline of DGSSA and GLAD can accelerate up to 121% run-time throughput compared with DeiT, which surpasses all SOTA approaches.
</details></li>
</ul>
<hr>
<h2 id="Deep-Video-Inpainting-Guided-by-Audio-Visual-Self-Supervision"><a href="#Deep-Video-Inpainting-Guided-by-Audio-Visual-Self-Supervision" class="headerlink" title="Deep Video Inpainting Guided by Audio-Visual Self-Supervision"></a>Deep Video Inpainting Guided by Audio-Visual Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07663">http://arxiv.org/abs/2310.07663</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyuyeonpooh/Audio-Visual-Deep-Video-Inpainting">https://github.com/kyuyeonpooh/Audio-Visual-Deep-Video-Inpainting</a></li>
<li>paper_authors: Kyuyeon Kim, Junsik Jung, Woo Jae Kim, Sung-Eui Yoon</li>
<li>for: 提高视频填充质量</li>
<li>methods: 使用深度学习模型借鉴人类的听觉视觉对应知识，实现视频填充</li>
<li>results: 实验结果表明，提出的方法可以更广泛地恢复视频场景，特别是在听觉对象在场景中部分遮盖时表现出色。<details>
<summary>Abstract</summary>
Humans can easily imagine a scene from auditory information based on their prior knowledge of audio-visual events. In this paper, we mimic this innate human ability in deep learning models to improve the quality of video inpainting. To implement the prior knowledge, we first train the audio-visual network, which learns the correspondence between auditory and visual information. Then, the audio-visual network is employed as a guider that conveys the prior knowledge of audio-visual correspondence to the video inpainting network. This prior knowledge is transferred through our proposed two novel losses: audio-visual attention loss and audio-visual pseudo-class consistency loss. These two losses further improve the performance of the video inpainting by encouraging the inpainting result to have a high correspondence to its synchronized audio. Experimental results demonstrate that our proposed method can restore a wider domain of video scenes and is particularly effective when the sounding object in the scene is partially blinded.
</details>
<details>
<summary>摘要</summary>
人类可以轻松地从听音信息中想象场景，在这篇论文中，我们模仿人类的 innate 能力，在深度学习模型中提高视频填充质量。为了实现先前知识，我们首先训练了 audio-visual 网络，该网络学习听音和视觉信息之间的对应关系。然后，我们使用我们提出的两种新的损失函数：听音视觉注意力损失和听音视觉假类一致损失。这两种损失函数进一步提高了视频填充的性能，使得填充结果具有高度对应于其同步的听音。实验结果表明，我们提出的方法可以恢复更广泛的视频场景，并在听音对象在场景中部分遮盲时特别有效。
</details></li>
</ul>
<hr>
<h2 id="Context-Enhanced-Detector-For-Building-Detection-From-Remote-Sensing-Images"><a href="#Context-Enhanced-Detector-For-Building-Detection-From-Remote-Sensing-Images" class="headerlink" title="Context-Enhanced Detector For Building Detection From Remote Sensing Images"></a>Context-Enhanced Detector For Building Detection From Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07638">http://arxiv.org/abs/2310.07638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyue Huang, Mingming Zhang, Qingjie Liu, Wei Wang, Zhe Dong, Yunhong Wang</li>
<li>for: 提高遥感图像中建筑物检测精度， Addressing the challenges of building detection in remote sensing images.</li>
<li>methods: 提出一种新的Context-Enhanced Detector（CEDet）方法，包括三个阶段堆式结构，使用Semantic Guided Contextual Mining（SGCM）模块和Instance Context Mining Module（ICMM）两个模块，并使用Semantic segmentation loss基于pseudo-masks来导引上下文信息抽取。</li>
<li>results: 在三个建筑物检测标准测试集上达到了状态 искусственный智能性能，包括CNBuilding-9P、CNBuilding-23P和SpaceNet。<details>
<summary>Abstract</summary>
The field of building detection from remote sensing images has made significant progress, but faces challenges in achieving high-accuracy detection due to the diversity in building appearances and the complexity of vast scenes. To address these challenges, we propose a novel approach called Context-Enhanced Detector (CEDet). Our approach utilizes a three-stage cascade structure to enhance the extraction of contextual information and improve building detection accuracy. Specifically, we introduce two modules: the Semantic Guided Contextual Mining (SGCM) module, which aggregates multi-scale contexts and incorporates an attention mechanism to capture long-range interactions, and the Instance Context Mining Module (ICMM), which captures instance-level relationship context by constructing a spatial relationship graph and aggregating instance features. Additionally, we introduce a semantic segmentation loss based on pseudo-masks to guide contextual information extraction. Our method achieves state-of-the-art performance on three building detection benchmarks, including CNBuilding-9P, CNBuilding-23P, and SpaceNet.
</details>
<details>
<summary>摘要</summary>
隐身图像建筑检测领域已经做出了重要进展，但是因为建筑物的多样性和场景的复杂性，高精度检测仍然面临挑战。为解决这些挑战，我们提出了一种新的方法 called Context-Enhanced Detector (CEDet)。我们的方法采用三stage阶段结构来增强 Contextual information的提取和改善建筑物检测精度。特别是，我们引入了两个模块：Semantic Guided Contextual Mining (SGCM)模块和 Instance Context Mining Module (ICMM)。SGCM模块通过不同级别的上下文进行聚合和使用注意力机制来捕捉长距离相互作用，而ICMM模块则通过构建空间关系图和聚合实例特征来捕捉实例水平的关系上下文。此外，我们还引入了基于 Pseudo-masks的 semantic segmentation loss来引导上下文信息提取。我们的方法在三个建筑物检测标准 bencmarks，包括 CNBuilding-9P、CNBuilding-23P 和 SpaceNet 上达到了当前领域的状态态��表现。
</details></li>
</ul>
<hr>
<h2 id="Attention-Map-Augmentation-for-Hypercomplex-Breast-Cancer-Classification"><a href="#Attention-Map-Augmentation-for-Hypercomplex-Breast-Cancer-Classification" class="headerlink" title="Attention-Map Augmentation for Hypercomplex Breast Cancer Classification"></a>Attention-Map Augmentation for Hypercomplex Breast Cancer Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07633">http://arxiv.org/abs/2310.07633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Lopez, Filippo Betello, Federico Carmignani, Eleonora Grassucci, Danilo Comminiello</li>
<li>for: 提高乳腺癌早期诊断性能</li>
<li>methods: 使用 Parameterized Hypercomplex Attention Maps (PHAM) 框架，包括图像增强步骤和扩展步骤，以及使用Parameterized Hypercomplex Neural Network (PHNN) 进行乳腺癌分类</li>
<li>results: 在环境中覆盖率高，超过了关注基于网络的现状和实数值对应的方法的表现，并在医疗图像和 histopathological 图像上进行了证明<details>
<summary>Abstract</summary>
Breast cancer is the most widespread neoplasm among women and early detection of this disease is critical. Deep learning techniques have become of great interest to improve diagnostic performance. Nonetheless, discriminating between malignant and benign masses from whole mammograms remains challenging due to them being almost identical to an untrained eye and the region of interest (ROI) occupying a minuscule portion of the entire image. In this paper, we propose a framework, parameterized hypercomplex attention maps (PHAM), to overcome these problems. Specifically, we deploy an augmentation step based on computing attention maps. Then, the attention maps are used to condition the classification step by constructing a multi-dimensional input comprised of the original breast cancer image and the corresponding attention map. In this step, a parameterized hypercomplex neural network (PHNN) is employed to perform breast cancer classification. The framework offers two main advantages. First, attention maps provide critical information regarding the ROI and allow the neural model to concentrate on it. Second, the hypercomplex architecture has the ability to model local relations between input dimensions thanks to hypercomplex algebra rules, thus properly exploiting the information provided by the attention map. We demonstrate the efficacy of the proposed framework on both mammography images as well as histopathological ones, surpassing attention-based state-of-the-art networks and the real-valued counterpart of our method. The code of our work is available at https://github.com/elelo22/AttentionBCS.
</details>
<details>
<summary>摘要</summary>
乳癌是女性最常见的肿瘤，早期发现这种疾病非常重要。深度学习技术在提高诊断性能方面表现出了极大的兴趣。然而，从整个照片中分别识别癌变和良性肿瘤仍然是一项极其困难的任务，因为它们在无经验的眼里看起来几乎一样，而识别区域（ROI）占整个照片的一个非常小的部分。在这篇论文中，我们提出了一个框架，即卷积注意地图（PHAM）。我们在这个框架中使用了一个增强步骤，基于计算注意地图。然后，我们使用了这些注意地图来控制分类步骤，通过构建一个多维输入，其中包括原始乳癌照片和相应的注意地图。在这个步骤中，我们使用了一个参数化的高维复杂神经网络（PHNN）来进行乳癌分类。我们的框架具有两个主要优点。首先，注意地图提供了ROI的重要信息，使神经网络能够专注于它。其次，高维复杂架构可以通过高维复杂代数规则，正确利用注意地图提供的信息。我们在照片和 histopathological 图像上进行了实验，超过了注意力基于状态最佳网络和实值对应的我们方法。我们的代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Backdoors-in-Visual-Prompt-Learning"><a href="#Prompt-Backdoors-in-Visual-Prompt-Learning" class="headerlink" title="Prompt Backdoors in Visual Prompt Learning"></a>Prompt Backdoors in Visual Prompt Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07632">http://arxiv.org/abs/2310.07632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang</li>
<li>for: 这篇论文目的是探讨大型预训模型的精细化是否可行，以及Visual Prompt Learning（VPL）是一种可能的替代方案。</li>
<li>methods: 这篇论文使用了Visual Prompt as a Service（VPPTaaS），即提供者可以供给用户一个可读的显示图像，并让用户使用这个图像和大型预训模型进行预测。</li>
<li>results: 这篇论文发现了VPL中的一个新的安全风险，即BadVisualPrompt，这是一种可以透过攻击提供者提供的显示图像来控制模型的攻击。 Specifically, 这篇论文发现了一个新的技术挑战，即显示图像触发器和显示图像之间的互动，这不同于传统的模型水平的后门攻击。<details>
<summary>Abstract</summary>
Fine-tuning large pre-trained computer vision models is infeasible for resource-limited users. Visual prompt learning (VPL) has thus emerged to provide an efficient and flexible alternative to model fine-tuning through Visual Prompt as a Service (VPPTaaS). Specifically, the VPPTaaS provider optimizes a visual prompt given downstream data, and downstream users can use this prompt together with the large pre-trained model for prediction. However, this new learning paradigm may also pose security risks when the VPPTaaS provider instead provides a malicious visual prompt. In this paper, we take the first step to explore such risks through the lens of backdoor attacks. Specifically, we propose BadVisualPrompt, a simple yet effective backdoor attack against VPL. For example, poisoning $5\%$ CIFAR10 training data leads to above $99\%$ attack success rates with only negligible model accuracy drop by $1.5\%$. In particular, we identify and then address a new technical challenge related to interactions between the backdoor trigger and visual prompt, which does not exist in conventional, model-level backdoors. Moreover, we provide in-depth analyses of seven backdoor defenses from model, prompt, and input levels. Overall, all these defenses are either ineffective or impractical to mitigate our BadVisualPrompt, implying the critical vulnerability of VPL.
</details>
<details>
<summary>摘要</summary>
大型预训计算机视觉模型的细调是对资源有限的用户来说不可能进行。因此，视觉提示学习（VPL）已经出现以提供一种高效和灵活的替代方案。具体来说，VPL提供者将Visual Prompt as a Service（VPPTaaS）优化一个视觉提示，并且用户可以使用这个提示和大型预训模型进行预测。然而，这种新的学习模式可能也会带来安全风险，当VPPTAaaS提供商而不是提供正确的视觉提示。在这篇论文中，我们通过镜头攻击的视角来探讨这些风险。具体来说，我们提出了BadVisualPrompt，一种简单 yet effective的镜头攻击方法，可以让攻击者在只需要负担5%的CIFAR10训练数据上成功率高于99%，而且只带来模型准确率的1.5%下降。我们还发现了一个新的技术挑战，即镜头触发器和视觉提示之间的交互问题，这个问题不存在于传统的模型级镜头攻击中。此外，我们还提供了七种防御策略的深入分析，包括模型、提示和输入级防御策略。总之，这些防御策略都是无效或不实际的，表明VPL具有极高的漏洞。
</details></li>
</ul>
<hr>
<h2 id="Dual-Radar-A-Multi-modal-Dataset-with-Dual-4D-Radar-for-Autonomous-Driving"><a href="#Dual-Radar-A-Multi-modal-Dataset-with-Dual-4D-Radar-for-Autonomous-Driving" class="headerlink" title="Dual Radar: A Multi-modal Dataset with Dual 4D Radar for Autonomous Driving"></a>Dual Radar: A Multi-modal Dataset with Dual 4D Radar for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07602">http://arxiv.org/abs/2310.07602</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adept-thu/dual-radar">https://github.com/adept-thu/dual-radar</a></li>
<li>paper_authors: Xinyu Zhang, Li Wang, Jian Chen, Cheng Fang, Lei Yang, Ziying Song, Guangqi Yang, Yichen Wang, Xiaofei Zhang, Qingshan Yang, Jun Li</li>
<li>For: The paper is written for the purpose of introducing a novel large-scale multi-modal dataset for studying effective 4D radar perception algorithms in autonomous driving.* Methods: The paper uses two types of 4D radars captured simultaneously to create a novel dataset, which consists of 151 consecutive series, most of which last 20 seconds and contain 10,007 meticulously synchronized and annotated frames.* Results: The paper experimentally validates the dataset and provides valuable results for studying different types of 4D radars.Here is the information in Simplified Chinese text:* For: 本文是为了介绍一个新的大规模多模式数据集，用于研究自动驾驶4D радиar感知算法的有效性。* Methods: 本文使用了两种同时捕获的4D радиar来创建一个新的数据集，该数据集包括151个连续的系列，每个系列持续20秒钟，共计10,007幅 preciselly同步和标注的帧。* Results: 本文对数据集进行了实验 validate，并提供了对不同类型4D радиar的有价值的结果。<details>
<summary>Abstract</summary>
Radar has stronger adaptability in adverse scenarios for autonomous driving environmental perception compared to widely adopted cameras and LiDARs. Compared with commonly used 3D radars, the latest 4D radars have precise vertical resolution and higher point cloud density, making it a highly promising sensor for autonomous driving in complex environmental perception. However, due to the much higher noise than LiDAR, manufacturers choose different filtering strategies, resulting in an inverse ratio between noise level and point cloud density. There is still a lack of comparative analysis on which method is beneficial for deep learning-based perception algorithms in autonomous driving. One of the main reasons is that current datasets only adopt one type of 4D radar, making it difficult to compare different 4D radars in the same scene. Therefore, in this paper, we introduce a novel large-scale multi-modal dataset featuring, for the first time, two types of 4D radars captured simultaneously. This dataset enables further research into effective 4D radar perception algorithms.Our dataset consists of 151 consecutive series, most of which last 20 seconds and contain 10,007 meticulously synchronized and annotated frames. Moreover, our dataset captures a variety of challenging driving scenarios, including many road conditions, weather conditions, nighttime and daytime with different lighting intensities and periods. Our dataset annotates consecutive frames, which can be applied to 3D object detection and tracking, and also supports the study of multi-modal tasks. We experimentally validate our dataset, providing valuable results for studying different types of 4D radars. This dataset is released on https://github.com/adept-thu/Dual-Radar.
</details>
<details>
<summary>摘要</summary>
雷达在自动驾驶环境感知方面具有更强的适应能力，比较广泛使用的相机和激光雷达更具有优势。与常见的3D雷达相比，最新的4D雷达具有高精度的垂直分辨率和更高的点云密度，使其成为自动驾驶复杂环境感知的非常有前途的传感器。然而，由于雷达的噪声比激光更高，制造商们采用不同的筛选策略，导致点云密度与噪声水平之间存在反比关系。到目前为止，没有对不同筛选策略对深度学习基于感知算法的比较分析。这是因为当前数据集只采用了一种类型的4D雷达，使其不可以在同一场景中比较不同的4D雷达。因此，在本文中，我们引入了一个新的大规模多模态数据集，其中包含了两种类型的4D雷达同时捕获的数据。这个数据集允许进一步研究4D雷达感知算法。我们的数据集包含151个连续时间序列，大多数时间长达20秒，包含10,007幅高精度同步 annotated frames。此外，我们的数据集涵盖了许多挑战性的驾驶场景，包括不同的路面条件、天气条件、夜间和白天不同的照明强度和时间。我们的数据集将 consecutively frames annotated，可以应用于3D объек体检测和跟踪，同时也支持多模态任务的研究。我们在实验 validate了我们的数据集，提供了价值的结果，用于研究不同类型的4D雷达。这个数据集在https://github.com/adept-thu/Dual-Radar上发布。
</details></li>
</ul>
<hr>
<h2 id="PeP-a-Point-enhanced-Painting-method-for-unified-point-cloud-tasks"><a href="#PeP-a-Point-enhanced-Painting-method-for-unified-point-cloud-tasks" class="headerlink" title="PeP: a Point enhanced Painting method for unified point cloud tasks"></a>PeP: a Point enhanced Painting method for unified point cloud tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07591">http://arxiv.org/abs/2310.07591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichao Dong, Hang Ji, Xufeng Huang, Weikun Zhang, Xin Zhan, Junbo Chen</li>
<li>for: 提高点云识别的性能，提供更好的输入参数 для下游模块。</li>
<li>methods: 提出了一种新的PeP模块，包括改进点绘制方法和LM基于的点编码器。</li>
<li>results: 在nuScenes和KITTI数据集上进行了实验，并证明了我们的PeP模块在semantic segmentation和物体检测方面具有强大表现，包括单点云和多模态设置下的情况。<details>
<summary>Abstract</summary>
Point encoder is of vital importance for point cloud recognition. As the very beginning step of whole model pipeline, adding features from diverse sources and providing stronger feature encoding mechanism would provide better input for downstream modules. In our work, we proposed a novel PeP module to tackle above issue. PeP contains two main parts, a refined point painting method and a LM-based point encoder. Experiments results on the nuScenes and KITTI datasets validate the superior performance of our PeP. The advantages leads to strong performance on both semantic segmentation and object detection, in both lidar and multi-modal settings. Notably, our PeP module is model agnostic and plug-and-play. Our code will be publicly available soon.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>点编码器对点云识别具有重要的重要性。作为整个模型管道的开头步骤，从多种来源添加特征并提供更强的特征编码机制可以为下游模块提供更好的输入。在我们的工作中，我们提出了一种新的PeP模块来解决上述问题。PeP包括两个主要部分：一种精度调整的点绘制方法和一种LM基于的点编码器。在nuScenes和KITTI数据集上进行的实验结果表明了我们的PeP模块在 semantic segmentation和对象检测中具有出色的表现，包括雷达和多模式下的情况。尤其是，我们的PeP模块是模型无关的和可插入的。我们的代码将很快地公开。
</details></li>
</ul>
<hr>
<h2 id="A-Discrepancy-Aware-Framework-for-Robust-Anomaly-Detection"><a href="#A-Discrepancy-Aware-Framework-for-Robust-Anomaly-Detection" class="headerlink" title="A Discrepancy Aware Framework for Robust Anomaly Detection"></a>A Discrepancy Aware Framework for Robust Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07585">http://arxiv.org/abs/2310.07585</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caiyuxuan1120/daf">https://github.com/caiyuxuan1120/daf</a></li>
<li>paper_authors: Yuxuan Cai, Dingkang Liang, Dongliang Luo, Xinwei He, Xin Yang, Xiang Bai</li>
<li>for: 本研究旨在探讨自然语言处理中的异常检测问题，尤其是使用自然语言生成的数据进行自监学习。</li>
<li>methods: 我们提出了一种异常检测方法，即异常检测框架（DAF），它可以在不同的异常检测任务中表现出较好的 Robustness。</li>
<li>results: 我们在两个复杂的数据集上进行了广泛的实验，并证明了我们的方法可以在使用简单的生成策略时表现出较好的性能，并且在异常检测任务中实现了state-of-the-art的本地化性能。<details>
<summary>Abstract</summary>
Defect detection is a critical research area in artificial intelligence. Recently, synthetic data-based self-supervised learning has shown great potential on this task. Although many sophisticated synthesizing strategies exist, little research has been done to investigate the robustness of models when faced with different strategies. In this paper, we focus on this issue and find that existing methods are highly sensitive to them. To alleviate this issue, we present a Discrepancy Aware Framework (DAF), which demonstrates robust performance consistently with simple and cheap strategies across different anomaly detection benchmarks. We hypothesize that the high sensitivity to synthetic data of existing self-supervised methods arises from their heavy reliance on the visual appearance of synthetic data during decoding. In contrast, our method leverages an appearance-agnostic cue to guide the decoder in identifying defects, thereby alleviating its reliance on synthetic appearance. To this end, inspired by existing knowledge distillation methods, we employ a teacher-student network, which is trained based on synthesized outliers, to compute the discrepancy map as the cue. Extensive experiments on two challenging datasets prove the robustness of our method. Under the simple synthesis strategies, it outperforms existing methods by a large margin. Furthermore, it also achieves the state-of-the-art localization performance. Code is available at: https://github.com/caiyuxuan1120/DAF.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>人工智能中的缺陷检测是一个关键的研究领域。最近，基于合成数据的自主学习已经表现出了很大的潜力。虽然有很多复杂的合成策略，但是对模型对不同策略的Robustness还需要更多的研究。在这篇论文中，我们将关注这个问题，并发现现有的方法对不同策略非常敏感。为了解决这个问题，我们提出了一种不同策略意识框架（DAF），它在不同的缺陷检测 bencmarks 中表现出了稳定性和简单性。我们认为现有方法对合成数据的视觉出现非常依赖，而我们的方法借鉴了现有的知识填充方法，通过计算不同策略下的异常映射来避免依赖于合成数据的视觉。为此，我们采用了一种教师生网络，通过合成异常数据来计算异常映射。我们在两个复杂的数据集上进行了广泛的实验，结果表明我们的方法在简单的合成策略下表现出了明显的优势，并且也达到了当前的最佳本地化性能。代码可以在 GitHub 上找到：https://github.com/caiyuxuan1120/DAF。
</details></li>
</ul>
<hr>
<h2 id="Centrality-of-the-Fingerprint-Core-Location"><a href="#Centrality-of-the-Fingerprint-Core-Location" class="headerlink" title="Centrality of the Fingerprint Core Location"></a>Centrality of the Fingerprint Core Location</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07584">http://arxiv.org/abs/2310.07584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laurenz Ruzicka, Bernhard Strobl, Bernhard Kohn, Clemens Heitzinger</li>
<li>for: 这个研究的目的是分析和提高指纹识别的方法，特别是研究指纹核心的分布。</li>
<li>methods: 该研究使用了rolled fingerprint recordings和plain fingerprint recordings的大量数据集，并使用了empirical distribution的方法来分析指纹核心的分布。</li>
<li>results: 研究发现，rolling fingerprint recordings中核心的位置与指纹中心的偏差为5.7% $\pm$ 5.2%到7.6% $\pm$ 6.9%，而plain fingerprint recordings中核心的位置遵循正态分布。此外，研究还发现，NFIQ 2 预测器偏爱rolled fingerprint recordings中核心处于指纹中心下方的位置。<details>
<summary>Abstract</summary>
Fingerprints have long been recognized as a unique and reliable means of personal identification. Central to the analysis and enhancement of fingerprints is the concept of the fingerprint core. Although the location of the core is used in many applications, to the best of our knowledge, this study is the first to investigate the empirical distribution of the core over a large, combined dataset of rolled, as well as plain fingerprint recordings. We identify and investigate the extent of incomplete rolling during the rolled fingerprint acquisition and investigate the centrality of the core. After correcting for the incomplete rolling, we find that the core deviates from the fingerprint center by 5.7% $\pm$ 5.2% to 7.6% $\pm$ 6.9%, depending on the finger. Additionally, we find that the assumption of normal distribution of the core position of plain fingerprint recordings cannot be rejected, but for rolled ones it can. Therefore, we use a multi-step process to find the distribution of the rolled fingerprint recordings. The process consists of an Anderson-Darling normality test, the Bayesian Information Criterion to reduce the number of possible candidate distributions and finally a Generalized Monte Carlo goodness-of-fit procedure to find the best fitting distribution. We find the non-central Fischer distribution best describes the cores' horizontal positions. Finally, we investigate the correlation between mean core position offset and the NFIQ 2 score and find that the NFIQ 2 prefers rolled fingerprint recordings where the core sits slightly below the fingerprint center.
</details>
<details>
<summary>摘要</summary>
人体指纹长期被认为是一种独特和可靠的个人认可方法。中心于指纹核心的分析和加强是识别人体指纹的关键。尽管核心位置在许多应用中被使用，但 according to our knowledge, this study is the first to investigate the empirical distribution of the core over a large, combined dataset of rolled and plain fingerprint recordings. We identify and investigate the extent of incomplete rolling during the rolled fingerprint acquisition and investigate the centrality of the core. After correcting for the incomplete rolling, we find that the core deviates from the fingerprint center by 5.7% ± 5.2% to 7.6% ± 6.9%, depending on the finger. Additionally, we find that the assumption of normal distribution of the core position of plain fingerprint recordings cannot be rejected, but for rolled ones it can. Therefore, we use a multi-step process to find the distribution of the rolled fingerprint recordings. The process consists of an Anderson-Darling normality test, the Bayesian Information Criterion to reduce the number of possible candidate distributions, and finally a Generalized Monte Carlo goodness-of-fit procedure to find the best fitting distribution. We find the non-central Fischer distribution best describes the cores' horizontal positions. Finally, we investigate the correlation between mean core position offset and the NFIQ 2 score and find that the NFIQ 2 prefers rolled fingerprint recordings where the core sits slightly below the fingerprint center.
</details></li>
</ul>
<hr>
<h2 id="Relational-Prior-Knowledge-Graphs-for-Detection-and-Instance-Segmentation"><a href="#Relational-Prior-Knowledge-Graphs-for-Detection-and-Instance-Segmentation" class="headerlink" title="Relational Prior Knowledge Graphs for Detection and Instance Segmentation"></a>Relational Prior Knowledge Graphs for Detection and Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07573">http://arxiv.org/abs/2310.07573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Osman Ülger, Yu Wang, Ysbrand Galama, Sezer Karaoglu, Theo Gevers, Martin R. Oswald</li>
<li>for: 这 paper 的目的是调查使用对象之间关系来进行物体探测和实例分割是否有效。</li>
<li>methods: 该 paper 提出了一种基于关系优先的特征增强模型（RP-FEM），该模型在Scene graph上运行，并同时学习对象探测和实例分割中的关系上下文模型。</li>
<li>results: 实验表明，在 COCO 数据集上，使用Scene graph和关系优先，可以提高物体探测和实例分割的性能，RP-FEM 可以减少图像中不可能的类别预测，同时避免生成重复预测，与基础模型相比呈现提高。<details>
<summary>Abstract</summary>
Humans have a remarkable ability to perceive and reason about the world around them by understanding the relationships between objects. In this paper, we investigate the effectiveness of using such relationships for object detection and instance segmentation. To this end, we propose a Relational Prior-based Feature Enhancement Model (RP-FEM), a graph transformer that enhances object proposal features using relational priors. The proposed architecture operates on top of scene graphs obtained from initial proposals and aims to concurrently learn relational context modeling for object detection and instance segmentation. Experimental evaluations on COCO show that the utilization of scene graphs, augmented with relational priors, offer benefits for object detection and instance segmentation. RP-FEM demonstrates its capacity to suppress improbable class predictions within the image while also preventing the model from generating duplicate predictions, leading to improvements over the baseline model on which it is built.
</details>
<details>
<summary>摘要</summary>
人类具有非常出色的能力，可以通过理解对象之间的关系来理解世界。在这篇论文中，我们研究了使用这些关系来进行对象检测和实例分割。为此，我们提出了一种基于关系优先的特征增强模型（RP-FEM），这是一种图 transformer 模型，可以通过在Scene Graph中增强对象提案特征来提高对象检测和实例分割的性能。这种建议的架构在Scene Graph中进行同时学习对象检测和实例分割的关系上下文模型。在COCO数据集上进行实验评估，我们发现使用Scene Graph和关系优先可以提高对象检测和实例分割的性能。RP-FEM可以降低图像中不可能的类别预测，同时避免模型生成重复预测，从而超越基础模型。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Label-Types-on-Training-SWIN-Models-with-Overhead-Imagery"><a href="#Impact-of-Label-Types-on-Training-SWIN-Models-with-Overhead-Imagery" class="headerlink" title="Impact of Label Types on Training SWIN Models with Overhead Imagery"></a>Impact of Label Types on Training SWIN Models with Overhead Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07572">http://arxiv.org/abs/2310.07572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Ford, Kenneth Hutchison, Nicholas Felts, Benjamin Cheng, Jesse Lew, Kyle Jackson</li>
<li>for: 这个论文的目的是研究数据集设计对模型训练和性能的影响，以帮助降低生成遥感和过空标注数据的成本。</li>
<li>methods: 这篇论文使用了固定窗口变换器的训练，使用了 bounding boxes 和 segmentation 标签，其中后者更加昂贵。作者比较了使用不同类型的标签进行训练，并研究了这些模型在不同任务上的性能。</li>
<li>results: 作者发现，使用只有目标像素的训练不会提高分类任务的性能，而且会混淆评估集中的背景像素。对于对象检测模型，使用不同类型的标签没有影响测试集的性能。作者发现，使用 bounding boxes 可以为一些不需要更复杂的标签的任务提供足够的性能。<details>
<summary>Abstract</summary>
Understanding the impact of data set design on model training and performance can help alleviate the costs associated with generating remote sensing and overhead labeled data. This work examined the impact of training shifted window transformers using bounding boxes and segmentation labels, where the latter are more expensive to produce. We examined classification tasks by comparing models trained with both target and backgrounds against models trained with only target pixels, extracted by segmentation labels. For object detection models, we compared performance using either label type when training. We found that the models trained on only target pixels do not show performance improvement for classification tasks, appearing to conflate background pixels in the evaluation set with target pixels. For object detection, we found that models trained with either label type showed equivalent performance across testing. We found that bounding boxes appeared to be sufficient for tasks that did not require more complex labels, such as object segmentation. Continuing work to determine consistency of this result across data types and model architectures could potentially result in substantial savings in generating remote sensing data sets for deep learning.
</details>
<details>
<summary>摘要</summary>
Understanding the impact of data set design on model training and performance can help reduce the costs associated with generating remote sensing and overhead labeled data. This work examined the impact of training shifted window transformers using bounding boxes and segmentation labels, where the latter are more expensive to produce. We compared models trained with both target and backgrounds against models trained with only target pixels, extracted by segmentation labels. For object detection models, we compared performance using either label type when training. We found that the models trained on only target pixels did not show performance improvement for classification tasks, appearing to conflate background pixels in the evaluation set with target pixels. For object detection, we found that models trained with either label type showed equivalent performance across testing. We found that bounding boxes were sufficient for tasks that did not require more complex labels, such as object segmentation. Continuing work to determine consistency of this result across data types and model architectures could potentially result in substantial savings in generating remote sensing data sets for deep learning.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Does-resistance-to-Style-Transfer-equal-Shape-Bias-Evaluating-Shape-Bias-by-Distorted-Shape"><a href="#Does-resistance-to-Style-Transfer-equal-Shape-Bias-Evaluating-Shape-Bias-by-Distorted-Shape" class="headerlink" title="Does resistance to Style-Transfer equal Shape Bias? Evaluating Shape Bias by Distorted Shape"></a>Does resistance to Style-Transfer equal Shape Bias? Evaluating Shape Bias by Distorted Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07555">http://arxiv.org/abs/2310.07555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Wen, Tianqin Li, Tai Sing Lee</li>
<li>for: 这项研究旨在评估深度学习模型对形状的敏感性，并提供一个新的测试工具箱（Distorted Shape Testbench，DiST）来评估模型的全局形状敏感性。</li>
<li>methods: 这项研究使用了样式传递图像来训练深度学习模型，并对模型的性能进行评估。</li>
<li>results: 研究发现，已有的Shape bias评估方法不能准确评估模型的全局形状敏感性，而DiST测试工具箱可以准确地评估模型的全局形状敏感性，并且训练使用DiST图像可以bridge人类和现有SOTA模型之间的性能差距，同时保持模型的标准图像分类任务的准确率。<details>
<summary>Abstract</summary>
Deep learning models are known to exhibit a strong texture bias, while human tends to rely heavily on global shape for object recognition. The current benchmark for evaluating a model's shape bias is a set of style-transferred images with the assumption that resistance to the attack of style transfer is related to the development of shape sensitivity in the model. In this work, we show that networks trained with style-transfer images indeed learn to ignore style, but its shape bias arises primarily from local shapes. We provide a Distorted Shape Testbench (DiST) as an alternative measurement of global shape sensitivity. Our test includes 2400 original images from ImageNet-1K, each of which is accompanied by two images with the global shapes of the original image distorted while preserving its texture via the texture synthesis program. We found that (1) models that performed well on the previous shape bias evaluation do not fare well in the proposed DiST; (2) the widely adopted ViT models do not show significant advantages over Convolutional Neural Networks (CNNs) on this benchmark despite that ViTs rank higher on the previous shape bias tests. (3) training with DiST images bridges the significant gap between human and existing SOTA models' performance while preserving the models' accuracy on standard image classification tasks; training with DiST images and style-transferred images are complementary, and can be combined to train network together to enhance both the global and local shape sensitivity of the network. Our code will be host at: https://github.com/leelabcnbc/DiST
</details>
<details>
<summary>摘要</summary>
深度学习模型通常会表现出强的文本擅长，而人类则更倾向于依靠全局形状来识别对象。现有的测试方法之一是使用样式传输图像，假设图像的鲜明度攻击性能与模型内部形状敏感度之间存在相关性。在这项工作中，我们证明了通过样式传输图像训练的网络实际上会忽略样式，但是其形状偏好主要来自本地形状。我们提供了一个扭曲形状测试台（DiST），作为评估模型全局形状敏感度的代理测试方法。我们的测试包括2400个原始图像，每个图像都有两个基于图像的全局形状扭曲的图像，保持图像的文本via texture synthesis程序。我们发现：1. 在之前的形状偏好评估中高分的模型不如在我们提出的DiST测试中表现出色。2. 广泛采用的ViT模型与传统 convolutional neural network（CNN）在该测试中并没有显著的优势，即使ViT模型在之前的形状偏好测试中得分高。3. 使用DiST图像进行训练可以bridges人类和现有最佳实际模型之间的巨大差距，同时保持模型在标准图像分类任务中的准确率。使用DiST图像和样式传输图像进行训练可以将全局和本地形状敏感度融合到同一个模型中，这种融合训练可以提高模型的总体性和特征性能。我们的代码将会hosts在：https://github.com/leelabcnbc/DiST
</details></li>
</ul>
<hr>
<h2 id="Attribute-Localization-and-Revision-Network-for-Zero-Shot-Learning"><a href="#Attribute-Localization-and-Revision-Network-for-Zero-Shot-Learning" class="headerlink" title="Attribute Localization and Revision Network for Zero-Shot Learning"></a>Attribute Localization and Revision Network for Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07548">http://arxiv.org/abs/2310.07548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junzhe Xu, Suling Duan, Chenwei Tang, Zhenan He, Jiancheng Lv</li>
<li>for: 本文旨在提出一种基于 Attribute Localization and Revision Network 的零shot学习模型，以便在无需训练数据的情况下，模型能够识别未经训练的类别。</li>
<li>methods: 本文使用了 Attribute Localization Module (ALM) 和 Attribute Revision Module (ARM) 两种模块来捕捉图像区域的本地和全局特征，并通过修改涂抹图像的每个特征值来补做忽略内类 attribute 变化的缺陷。</li>
<li>results: 经过广泛的实验测试，本文的模型在三个常用的 benchmark 上表现出色，证明了本文提出的方法的有效性。<details>
<summary>Abstract</summary>
Zero-shot learning enables the model to recognize unseen categories with the aid of auxiliary semantic information such as attributes. Current works proposed to detect attributes from local image regions and align extracted features with class-level semantics. In this paper, we find that the choice between local and global features is not a zero-sum game, global features can also contribute to the understanding of attributes. In addition, aligning attribute features with class-level semantics ignores potential intra-class attribute variation. To mitigate these disadvantages, we present Attribute Localization and Revision Network in this paper. First, we design Attribute Localization Module (ALM) to capture both local and global features from image regions, a novel module called Scale Control Unit is incorporated to fuse global and local representations. Second, we propose Attribute Revision Module (ARM), which generates image-level semantics by revising the ground-truth value of each attribute, compensating for performance degradation caused by ignoring intra-class variation. Finally, the output of ALM will be aligned with revised semantics produced by ARM to achieve the training process. Comprehensive experimental results on three widely used benchmarks demonstrate the effectiveness of our model in the zero-shot prediction task.
</details>
<details>
<summary>摘要</summary>
zero-shot 学习可以让模型认识未经见过的类别，通过 auxiliary Semantic information  such as 特征。现有工作提出了从本地图像区域检测特征并将抽取的特征与类别 semantics 对齐。在这篇论文中，我们发现选择本地和全局特征并不是零和游戏，全局特征也可以帮助理解特征。此外，对属性特征与类别 semantics 对齐忽略了内部类 attribute 的变化。为了解决这些缺点，我们在这篇论文中提出了 Attribute Localization and Revision Network。首先，我们设计了 Attribute Localization Module (ALM)，它可以从图像区域中捕捉 both local 和 global 特征。为了融合全局和本地表示，我们采用了一个新的 Scale Control Unit。其次，我们提出了 Attribute Revision Module (ARM)，它可以根据图像的实际情况修改每个特征的真实值，以补偿因为忽略内部类 attribute 的变化而导致的性能下降。最后，ALM 的输出将被与 ARM 生成的修订后的 semantics 进行对齐，以实现训练过程。我们在三个常用的 benchmark 上进行了广泛的实验， demonstarting 我们的模型在 zero-shot 预测任务中的有效性。
</details></li>
</ul>
<hr>
<h2 id="S4C-Self-Supervised-Semantic-Scene-Completion-with-Neural-Fields"><a href="#S4C-Self-Supervised-Semantic-Scene-Completion-with-Neural-Fields" class="headerlink" title="S4C: Self-Supervised Semantic Scene Completion with Neural Fields"></a>S4C: Self-Supervised Semantic Scene Completion with Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07522">http://arxiv.org/abs/2310.07522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Hayler, Felix Wimbauer, Dominik Muhle, Christian Rupprecht, Daniel Cremers</li>
<li>for: 本研究旨在解决计算机视觉中的3Dsemantic场景理解挑战，帮助移动代理人自动规划和探索不确定环境。</li>
<li>methods: 我们提出了首个不需要3D实际数据的自我超级visedapproach，通过单张图像重建场景，并只使用视频和pseudo分割数据进行训练。</li>
<li>results: 我们的方法可以准确地推断场景的填充和semantic类别，并且可以 synthesize precisemap for far away viewpoints。<details>
<summary>Abstract</summary>
3D semantic scene understanding is a fundamental challenge in computer vision. It enables mobile agents to autonomously plan and navigate arbitrary environments. SSC formalizes this challenge as jointly estimating dense geometry and semantic information from sparse observations of a scene. Current methods for SSC are generally trained on 3D ground truth based on aggregated LiDAR scans. This process relies on special sensors and annotation by hand which are costly and do not scale well. To overcome this issue, our work presents the first self-supervised approach to SSC called S4C that does not rely on 3D ground truth data. Our proposed method can reconstruct a scene from a single image and only relies on videos and pseudo segmentation ground truth generated from off-the-shelf image segmentation network during training. Unlike existing methods, which use discrete voxel grids, we represent scenes as implicit semantic fields. This formulation allows querying any point within the camera frustum for occupancy and semantic class. Our architecture is trained through rendering-based self-supervised losses. Nonetheless, our method achieves performance close to fully supervised state-of-the-art methods. Additionally, our method demonstrates strong generalization capabilities and can synthesize accurate segmentation maps for far away viewpoints.
</details>
<details>
<summary>摘要</summary>
三维semantic场景理解是计算机视觉领域的基本挑战。它使移动代理能够自主规划和探索不确定环境。SSC将这个挑战形式化为同时估算场景的密度和semantic信息从稀疏观察数据中。现有的SSC方法通常通过3D实际数据进行训练，这个过程需要特殊的感知器和手动标注，这些成本高并不具扩展性。为了解决这个问题，我们的工作提出了第一个不需要3D实际数据的自主supervised SSC方法，称为S4C。我们的提议的方法可以从单张图像中重construct场景，只需要视频和pseudo segmentationground truth生成于市场上的图像分割网络进行训练。与现有方法不同，我们将场景表示为半 implicit semantic场景。这种表示方式允许在相机投影范围内查询任何点的占据和semantic类别。我们的架构通过渲染基于的自我超vised损失来训练。尽管如此，我们的方法可以与完全supervised方法相比，并且显示出强大的泛化能力，可以生成正确的分割图像 для远距离视角。
</details></li>
</ul>
<hr>
<h2 id="CM-PIE-Cross-modal-perception-for-interactive-enhanced-audio-visual-video-parsing"><a href="#CM-PIE-Cross-modal-perception-for-interactive-enhanced-audio-visual-video-parsing" class="headerlink" title="CM-PIE: Cross-modal perception for interactive-enhanced audio-visual video parsing"></a>CM-PIE: Cross-modal perception for interactive-enhanced audio-visual video parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07517">http://arxiv.org/abs/2310.07517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaru Chen, Ruohao Guo, Xubo Liu, Peipei Wu, Guangyao Li, Zhenbo Li, Wenwu Wang</li>
<li>for: 这篇论文旨在提高视频分割任务的精度，通过利用注意力机制 capture 视频中各个段落之间的语义相关性。</li>
<li>methods: 我们提出了一种新的交互增强十Modal 感知方法（CM-PIE），通过应用段基 attention 模块来学习细腻的特征。此外，我们还引入了一个交叉模态聚合块，以协同优化视频和声音信号的 semantic表示。</li>
<li>results: 我们的模型在 Look, Listen, and Parse 数据集上的 parsing 性能比其他方法高。<details>
<summary>Abstract</summary>
Audio-visual video parsing is the task of categorizing a video at the segment level with weak labels, and predicting them as audible or visible events. Recent methods for this task leverage the attention mechanism to capture the semantic correlations among the whole video across the audio-visual modalities. However, these approaches have overlooked the importance of individual segments within a video and the relationship among them, and tend to rely on a single modality when learning features. In this paper, we propose a novel interactive-enhanced cross-modal perception method~(CM-PIE), which can learn fine-grained features by applying a segment-based attention module. Furthermore, a cross-modal aggregation block is introduced to jointly optimize the semantic representation of audio and visual signals by enhancing inter-modal interactions. The experimental results show that our model offers improved parsing performance on the Look, Listen, and Parse dataset compared to other methods.
</details>
<details>
<summary>摘要</summary>
Audio-visual视频分解是将视频分为每个段的任务，并将它们分类为听ible或可见事件。现代方法在这个任务中利用注意力机制来捕捉全视频的语义相关性。然而，这些方法通常忽略视频中每个段的重要性以及它们之间的关系，而且通常仅仅使用单一的感知modalities来学习特征。在本文中，我们提出了一种新的交互增强的交叉模态感知方法（CM-PIE），可以通过应用段级注意力模块来学习细化的特征。此外，我们还引入了交叉模态聚合块，以便在语音和视频信号之间强化交互。实验结果表明，我们的模型在Look, Listen, and Parse数据集上的分解性能比其他方法更高。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Remote-Sensing-Anomaly-Detector-Across-Modalities-and-Scenes-via-Deviation-Relationship-Learning"><a href="#A-Unified-Remote-Sensing-Anomaly-Detector-Across-Modalities-and-Scenes-via-Deviation-Relationship-Learning" class="headerlink" title="A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes via Deviation Relationship Learning"></a>A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes via Deviation Relationship Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07511">http://arxiv.org/abs/2310.07511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingtao-li-cver/uniadrs">https://github.com/jingtao-li-cver/uniadrs</a></li>
<li>paper_authors: Jingtao Li, Xinyu Wang, Hengwei Zhao, Liangpei Zhang, Yanfei Zhong<br>for:这个研究旨在建立一个横跨多 modalities 和 scene 的潜在适应率高的问题检测器，以搜寻 earth 上的多种问题。methods:本研究使用了一个基于偏差关系的 bilayer 图表 reformulation，将问题检测任务转换为一个 conditional probability 模型，并透过条件预测Problem 来训练模型。results:研究发现，在五种不同的模式（包括 Hyperspectral、可见光、Synthetic Aperture Radar （SAR）、infrared 和 low light）中，提出的模型具有横跨多 modalities 和 scene 的问题检测能力。<details>
<summary>Abstract</summary>
Remote sensing anomaly detector can find the objects deviating from the background as potential targets. Given the diversity in earth anomaly types, a unified anomaly detector across modalities and scenes should be cost-effective and flexible to new earth observation sources and anomaly types. However, the current anomaly detectors are limited to a single modality and single scene, since they aim to learn the varying background distribution. Motivated by the universal anomaly deviation pattern, in that anomalies exhibit deviations from their local context, we exploit this characteristic to build a unified anomaly detector. Firstly, we reformulate the anomaly detection task as an undirected bilayer graph based on the deviation relationship, where the anomaly score is modeled as the conditional probability, given the pattern of the background and normal objects. The learning objective is then expressed as a conditional probability ranking problem. Furthermore, we design an instantiation of the reformulation in the data, architecture, and optimization aspects. Simulated spectral and spatial anomalies drive the instantiated architecture. The model is optimized directly for the conditional probability ranking. The proposed model was validated in five modalities including the hyperspectral, visible light, synthetic aperture radar (SAR), infrared and low light to show its unified detection ability.
</details>
<details>
<summary>摘要</summary>
<<SYS>>这里使用简化字体写入中文。</SYS>> Remote sensing异常探测器可以找到背景上异常的物体作为潜在目标。由于地球上异常的多样性，一个通用的异常探测器适合多modalities和场景将是cost-effective和flexible。然而，现有的异常探测器仅对单一modalities和单一场景进行学习，因为它们专注于变化的背景分布。我们受到地球上异常的通用偏差模式所驱动，我们利用这个特征来建立一个通用的异常探测器。首先，我们将异常探测任务 reformulate为一个不同irectional bilayer graph，基于偏差关系，异常得分被model为背景和正常物体的模式下的条件 probabilities。学习目标则表示为conditional probability ranking问题。此外，我们还设计了实体化的构造、架构和优化方面。实验 spectral和spatial异常驱动了实体化架构。我们直接对条件 probabilities进行优化。我们的模型在五种modalities中，包括光谱、可见光、Synthetic Aperture Radar（SAR）、infrared和low light中显示了通用的探测能力。
</details></li>
</ul>
<hr>
<h2 id="Heuristic-Vision-Pre-Training-with-Self-Supervised-and-Supervised-Multi-Task-Learning"><a href="#Heuristic-Vision-Pre-Training-with-Self-Supervised-and-Supervised-Multi-Task-Learning" class="headerlink" title="Heuristic Vision Pre-Training with Self-Supervised and Supervised Multi-Task Learning"></a>Heuristic Vision Pre-Training with Self-Supervised and Supervised Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07510">http://arxiv.org/abs/2310.07510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiming Qian</li>
<li>for: 该 paper 的目的是提出一种基于自然语言的视觉基础模型，以便更好地模仿人类视觉的方式来认识多样化和开放的世界。</li>
<li>methods: 该 paper 使用了自然语言上的多种预言任务，包括自然语言模型和图像分类等，以便更好地学习视觉表示学习。</li>
<li>results: 该 paper 的实验结果表明，使用该基础模型可以在多种视觉任务中达到或超过现有最佳Result，包括 ImageNet-1K 分类、COCO 物体检测和 ADE-20K Semantic Segmentation 等。<details>
<summary>Abstract</summary>
To mimic human vision with the way of recognizing the diverse and open world, foundation vision models are much critical. While recent techniques of self-supervised learning show the promising potentiality of this mission, we argue that signals from labelled data are also important for common-sense recognition, and properly chosen pre-text tasks can facilitate the efficiency of vision representation learning. To this end, we propose a novel pre-training framework by adopting both self-supervised and supervised visual pre-text tasks in a multi-task manner. Specifically, given an image, we take a heuristic way by considering its intrinsic style properties, inside objects with their locations and correlations, and how it looks like in 3D space for basic visual understanding. However, large-scale object bounding boxes and correlations are usually hard to achieve. Alternatively, we develop a hybrid method by leveraging both multi-label classification and self-supervised learning. On the one hand, under the multi-label supervision, the pre-trained model can explore the detailed information of an image, e.g., image types, objects, and part of semantic relations. On the other hand, self-supervised learning tasks, with respect to Masked Image Modeling (MIM) and contrastive learning, can help the model learn pixel details and patch correlations. Results show that our pre-trained models can deliver results on par with or better than state-of-the-art (SOTA) results on multiple visual tasks. For example, with a vanilla Swin-B backbone, we achieve 85.3\% top-1 accuracy on ImageNet-1K classification, 47.9 box AP on COCO object detection for Mask R-CNN, and 50.6 mIoU on ADE-20K semantic segmentation when using Upernet. The performance shows the ability of our vision foundation model to serve general purpose vision tasks.
</details>
<details>
<summary>摘要</summary>
On one hand, under multi-label supervision, the pre-trained model can explore detailed image information such as image types, objects, and semantic relations. On the other hand, self-supervised learning tasks like Masked Image Modeling (MIM) and contrastive learning help the model learn pixel details and patch correlations. Our pre-trained models achieve results on par with or better than state-of-the-art (SOTA) on multiple visual tasks. For example, with a vanilla Swin-B backbone, we achieve 85.3% top-1 accuracy on ImageNet-1K classification, 47.9 box AP on COCO object detection for Mask R-CNN, and 50.6 mIoU on ADE-20K semantic segmentation when using Upernet. These results demonstrate the ability of our vision foundation model to handle various general-purpose vision tasks.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Hierarchical-Feature-Sharing-for-Efficient-Dataset-Condensation"><a href="#Leveraging-Hierarchical-Feature-Sharing-for-Efficient-Dataset-Condensation" class="headerlink" title="Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation"></a>Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07506">http://arxiv.org/abs/2310.07506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haizhong Zheng, Jiachen Sun, Shutong Wu, Bhavya Kailkhura, Zhuoqing Mao, Chaowei Xiao, Atul Prakash</li>
<li>for: 本文提出了一种新的数据压缩方法，以提高模型训练的性能。</li>
<li>methods: 本文使用了一种新的数据参数化方法，将数据压缩成参数化数据容器而不是像素空间。</li>
<li>results: 比较baseline方法，本文的提posed方法在四个公共数据集（SVHN、CIFAR10、CIFAR100和Tiny-ImageNet）上表现出了更高的性能，甚至在使用批处理损失函数并使用 less GPU内存。<details>
<summary>Abstract</summary>
Given a real-world dataset, data condensation (DC) aims to synthesize a significantly smaller dataset that captures the knowledge of this dataset for model training with high performance. Recent works propose to enhance DC with data parameterization, which condenses data into parameterized data containers rather than pixel space. The intuition behind data parameterization is to encode shared features of images to avoid additional storage costs. In this paper, we recognize that images share common features in a hierarchical way due to the inherent hierarchical structure of the classification system, which is overlooked by current data parameterization methods. To better align DC with this hierarchical nature and encourage more efficient information sharing inside data containers, we propose a novel data parameterization architecture, Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier structure, representing the dataset-level, class-level, and instance-level features. Another helpful property of the hierarchical architecture is that HMN naturally ensures good independence among images despite achieving information sharing. This enables instance-level pruning for HMN to reduce redundant information, thereby further minimizing redundancy and enhancing performance. We evaluate HMN on four public datasets (SVHN, CIFAR10, CIFAR100, and Tiny-ImageNet) and compare HMN with eight DC baselines. The evaluation results show that our proposed method outperforms all baselines, even when trained with a batch-based loss consuming less GPU memory.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:给一个实际世界数据集，数据缩写（DC）目标是生成一个较小的数据集，捕捉这个数据集的知识，以便模型训练高性能。现有的方法提议通过数据参数化来增强DC，将数据压缩到参数化数据容器中，而不是像素空间。我们认为图像共享特征的理念是基于图像分类系统的层次结构，这一点被当前的数据参数化方法忽略。为了更好地将DC与这个层次结构相匹配，并促进数据容器内的信息共享，我们提出了一种新的数据参数化架构——层次记忆网络（HMN）。HMN将压缩数据存储在三级结构中，表示数据集级、类级和实例级特征。另外，层次架构的帮助特性是，HMN自然地保证图像之间的独立性，同时实现信息共享。这使得HMN可以进行实例级别的剪辑，从而进一步减少重复信息，提高性能。我们在四个公共数据集（SVHN、CIFAR10、CIFAR100和Tiny-ImageNet）上评估HMN，并与八个DC基准方法进行比较。评估结果表明，我们的提出方法在所有基准方法中表现出色，即使在使用更少的GPU内存的批处理损失函数训练时。
</details></li>
</ul>
<hr>
<h2 id="PtychoDV-Vision-Transformer-Based-Deep-Unrolling-Network-for-Ptychographic-Image-Reconstruction"><a href="#PtychoDV-Vision-Transformer-Based-Deep-Unrolling-Network-for-Ptychographic-Image-Reconstruction" class="headerlink" title="PtychoDV: Vision Transformer-Based Deep Unrolling Network for Ptychographic Image Reconstruction"></a>PtychoDV: Vision Transformer-Based Deep Unrolling Network for Ptychographic Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07504">http://arxiv.org/abs/2310.07504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Gan, Qiuchen Zhai, Michael Thompson McCann, Cristina Garcia Cardona, Ulugbek S. Kamilov, Brendt Wohlberg</li>
<li>for: 本研究旨在提出一种高效、高品质ptychography图像重建方法，以解决现有深度学习方法的缺陷和费时问题。</li>
<li>methods: 本研究使用了一种新的深度学习模型，称为PtychoDV，来实现高效、高品质ptychography图像重建。PtychoDV包括一个视觉变换器，用于从 raw measurement 中生成初始图像，并考虑到这些测量的相互关系。然后，使用一个深度 unfolding 网络来修正初始图像，使用学习的卷积推荐和ptychography测量模型。</li>
<li>results: 实验结果表明，PtychoDV 可以在 simulated data 上比现有的深度学习方法表现更好，并且可以Significantly reduce 计算成本，与迭代方法相比，而且维持竞争力性。<details>
<summary>Abstract</summary>
Ptychography is an imaging technique that captures multiple overlapping snapshots of a sample, illuminated coherently by a moving localized probe. The image recovery from ptychographic data is generally achieved via an iterative algorithm that solves a nonlinear phase-field problem derived from measured diffraction patterns. However, these approaches have high computational cost. In this paper, we introduce PtychoDV, a novel deep model-based network designed for efficient, high-quality ptychographic image reconstruction. PtychoDV comprises a vision transformer that generates an initial image from the set of raw measurements, taking into consideration their mutual correlations. This is followed by a deep unrolling network that refines the initial image using learnable convolutional priors and the ptychography measurement model. Experimental results on simulated data demonstrate that PtychoDV is capable of outperforming existing deep learning methods for this problem, and significantly reduces computational cost compared to iterative methodologies, while maintaining competitive performance.
</details>
<details>
<summary>摘要</summary>
ptychography 是一种成像技术，通过多次重叠的探针探测样品，以征服干扰的方式获得样品的图像。然而，现有的方法通常需要高度计算成本。在这篇论文中，我们介绍了ptychoDV，一种新的深度学习模型基网络，用于高效、高质量ptychographic图像重建。ptychoDV包括一个视觉转换器，通过对 Raw 测量数据进行处理，生成初始图像，考虑到测量数据之间的相互关系。然后，一个深度拓展网络会使用学习的卷积约束和ptychography测量模型，来细化初始图像。我们对模拟数据进行了实验，结果表明，ptychoDV 可以在深度学习方法中出色表现，并且在计算成本方面具有显著的改善，同时维持竞争力。
</details></li>
</ul>
<hr>
<h2 id="FGPrompt-Fine-grained-Goal-Prompting-for-Image-goal-Navigation"><a href="#FGPrompt-Fine-grained-Goal-Prompting-for-Image-goal-Navigation" class="headerlink" title="FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation"></a>FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07473">http://arxiv.org/abs/2310.07473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XinyuSun/FGPrompt">https://github.com/XinyuSun/FGPrompt</a></li>
<li>paper_authors: Xinyu Sun, Peihao Chen, Jugang Fan, Thomas H. Li, Jian Chen, Mingkui Tan</li>
<li>for: 本研究旨在解决自主系统导航到图像指定目标的重要 yet challenging task，agent需要从拍摄图像中理解目标位置。</li>
<li>methods: 我们采用 Fine-grained Goal Prompting (FGPrompt) 方法，利用高级别和高分辨率特征图作为启发，通过conditioned embedding来保留目标图像中细节信息，并使观察Encoder更加关注目标相关区域。</li>
<li>results: 与existMethods比较，我们在3个图像目标导航 benchmark datasets（i.e., Gibson, MP3D, HM3D）上显著提高了性能，特别是在Gibson上，我们超过了状态之前最佳成功率by 8%，只用1&#x2F;50的模型大小。<details>
<summary>Abstract</summary>
Learning to navigate to an image-specified goal is an important but challenging task for autonomous systems. The agent is required to reason the goal location from where a picture is shot. Existing methods try to solve this problem by learning a navigation policy, which captures semantic features of the goal image and observation image independently and lastly fuses them for predicting a sequence of navigation actions. However, these methods suffer from two major limitations. 1) They may miss detailed information in the goal image, and thus fail to reason the goal location. 2) More critically, it is hard to focus on the goal-relevant regions in the observation image, because they attempt to understand observation without goal conditioning. In this paper, we aim to overcome these limitations by designing a Fine-grained Goal Prompting (FGPrompt) method for image-goal navigation. In particular, we leverage fine-grained and high-resolution feature maps in the goal image as prompts to perform conditioned embedding, which preserves detailed information in the goal image and guides the observation encoder to pay attention to goal-relevant regions. Compared with existing methods on the image-goal navigation benchmark, our method brings significant performance improvement on 3 benchmark datasets (i.e., Gibson, MP3D, and HM3D). Especially on Gibson, we surpass the state-of-the-art success rate by 8% with only 1/50 model size. Project page: https://xinyusun.github.io/fgprompt-pages
</details>
<details>
<summary>摘要</summary>
学习到具体目标位置 Navigation 是自主系统中一项重要但具有挑战性的任务。 Agent 需要从拍摄图像中理解目标位置。现有方法通过学习导航策略，以独立地捕捉目标图像和观察图像的semantic特征，并最终将其混合以预测导航动作序列。然而，这些方法受到两个主要限制：1）它们可能会遗漏目标图像中的细节信息，因此无法理解目标位置。2）更加重要的是，它们难以在观察图像中注意目标相关区域，因为它们不会在目标条件下理解观察。在这篇论文中，我们寻求超越这些限制，通过设计 Fine-grained Goal Prompting（FGPrompt）方法，以便在图像目标导航中提高性能。具体来说，我们利用目标图像中细化和高分辨率的特征图作为引导，通过conditioned embedding来实现conditioned embedding，以保持目标图像中细节信息，并导引观察编码器注意目标相关区域。相比之前的方法，我们在三个图像目标导航 benchmark 上实现了显著的性能提升。尤其是在 Gibson 上，我们超过了状态 искусternal 的成功率，并且只需要1/50 的模型大小。项目页面：https://xinyusun.github.io/fgprompt-pages
</details></li>
</ul>
<hr>
<h2 id="PoRF-Pose-Residual-Field-for-Accurate-Neural-Surface-Reconstruction"><a href="#PoRF-Pose-Residual-Field-for-Accurate-Neural-Surface-Reconstruction" class="headerlink" title="PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction"></a>PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07449">http://arxiv.org/abs/2310.07449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Wang Bian, Wenjing Bian, Victor Adrian Prisacariu, Philip Torr<br>for:* The paper aims to improve the accuracy of neural surface reconstruction in real-world scenarios by refining camera poses and reducing the impact of camera pose noise.methods:* The proposed method uses a novel implicit representation called the pose residual field (PoRF), which leverages global information over the entire sequence to improve pose accuracy.* The method also introduces an epipolar geometry loss to enhance supervision and improve pose accuracy.results:* The proposed method achieves promising results on two datasets: the DTU dataset and the MobileBrick dataset.* On the DTU dataset, the method reduces rotation error by 78% for COLMAP poses and decreases the reconstruction Chamfer distance from 3.48mm to 0.85mm.* On the MobileBrick dataset, the method improves the reconstruction F1 score from 69.18 to 75.67, outperforming the dataset provided ground-truth pose.<details>
<summary>Abstract</summary>
Neural surface reconstruction is sensitive to the camera pose noise, even if state-of-the-art pose estimators like COLMAP or ARKit are used. More importantly, existing Pose-NeRF joint optimisation methods have struggled to improve pose accuracy in challenging real-world scenarios. To overcome the challenges, we introduce the pose residual field (\textbf{PoRF}), a novel implicit representation that uses an MLP for regressing pose updates. This is more robust than the conventional pose parameter optimisation due to parameter sharing that leverages global information over the entire sequence. Furthermore, we propose an epipolar geometry loss to enhance the supervision that leverages the correspondences exported from COLMAP results without the extra computational overhead. Our method yields promising results. On the DTU dataset, we reduce the rotation error by 78\% for COLMAP poses, leading to the decreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the MobileBrick dataset that contains casually captured unbounded 360-degree videos, our method refines ARKit poses and improves the reconstruction F1 score from 69.18 to 75.67, outperforming that with the dataset provided ground-truth pose (75.14). These achievements demonstrate the efficacy of our approach in refining camera poses and improving the accuracy of neural surface reconstruction in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
“神经表面重建敏感于摄像头姿态噪声，即使使用最先进的姿态估计器如COLMAP或ARKit。更重要的是，现有的姿态-NeRF共产化优化方法在实际世界场景中困难寻求高精度姿态。为了解决这些挑战，我们引入姿态差估场（PoRF），一种新的隐式表示方法，使用多层感知（MLP）来回归姿态更新。这种方法比传统的姿态参数优化更加稳定，因为它可以共享参数，利用全序列的全局信息。此外，我们提出了视觉几何损失来增强监督，利用COLMAP结果中出口的对准关系，而无需额外计算开销。我们的方法实现了可靠的成果。在DTU数据集上，我们将COLMAP姿态Error降低至78%，导致重建Chamfer距离从3.48mm降低至0.85mm。在包含抓拍 captured 360度视频的MobileBrick数据集上，我们的方法改善了ARKit姿态，提高了重建F1分数从69.18提高至75.67，超过了数据集提供的基准姿态（75.14）。这些成果表明我们的方法在实际世界场景中有效地改善摄像头姿态和神经表面重建精度。”
</details></li>
</ul>
<hr>
<h2 id="Distance-Weighted-Trans-Network-for-Image-Completion"><a href="#Distance-Weighted-Trans-Network-for-Image-Completion" class="headerlink" title="Distance Weighted Trans Network for Image Completion"></a>Distance Weighted Trans Network for Image Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07440">http://arxiv.org/abs/2310.07440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pourya Shamsolmoali, Masoumeh Zareapoor, Huiyu Zhou, Xuelong Li, Yue Lu</li>
<li>for: 本文提出了一种新的图像生成模型，用于更好地理解图像的结构和关系。</li>
<li>methods: 该模型基于距离Weighted Transformer (DWT)和卷积神经网络 (CNN) 两种不同的模型，以优化图像完成过程。</li>
<li>results: 对三个复杂的图像 dataset 进行了广泛的量化和质量测试，并证明了该模型的超越性。<details>
<summary>Abstract</summary>
The challenge of image generation has been effectively modeled as a problem of structure priors or transformation. However, existing models have unsatisfactory performance in understanding the global input image structures because of particular inherent features (for example, local inductive prior). Recent studies have shown that self-attention is an efficient modeling technique for image completion problems. In this paper, we propose a new architecture that relies on Distance-based Weighted Transformer (DWT) to better understand the relationships between an image's components. In our model, we leverage the strengths of both Convolutional Neural Networks (CNNs) and DWT blocks to enhance the image completion process. Specifically, CNNs are used to augment the local texture information of coarse priors and DWT blocks are used to recover certain coarse textures and coherent visual structures. Unlike current approaches that generally use CNNs to create feature maps, we use the DWT to encode global dependencies and compute distance-based weighted feature maps, which substantially minimizes the problem of visual ambiguities. Meanwhile, to better produce repeated textures, we introduce Residual Fast Fourier Convolution (Res-FFC) blocks to combine the encoder's skip features with the coarse features provided by our generator. Furthermore, a simple yet effective technique is proposed to normalize the non-zero values of convolutions, and fine-tune the network layers for regularization of the gradient norms to provide an efficient training stabiliser. Extensive quantitative and qualitative experiments on three challenging datasets demonstrate the superiority of our proposed model compared to existing approaches.
</details>
<details>
<summary>摘要</summary>
描述文本：图像生成挑战已经被模型为结构先验或变换问题。然而，现有模型在理解全局输入图像结构方面表现不满足，主要因为特定的遗传特征（例如本地推导先验）。最近的研究表明，自注意是一种高效的模型技术 для图像完成问题。在这篇论文中，我们提议一种新的架构，即距离基于变换器（DWT），以更好地理解图像组件之间的关系。我们在模型中结合了卷积神经网络（CNN）和DWT块来提高图像完成过程。具体来说，CNNs用于增强粗略先验中的本地特征信息，而DWT块用于恢复一些粗略的文本特征和一致视觉结构。与现有方法一样，我们使用DWT来编码全局依赖关系并计算距离基于权重的特征地图，从而减少视觉混乱的问题。此外，我们还提出了一种简单 yet有效的技术，即 residual Fast Fourier Convolution（Res-FFC）块，以结合生成器提供的粗略特征和编码器的跳过特征。此外，我们还提出了一种简单 yet有效的正则化技术，即Normalize the non-zero values of convolutions，并在网络层次进行 fine-tune 来稳定训练过程。广泛的量化和质量测试表明，我们的提议模型在三个复杂的数据集上表现出色，比现有方法更高效。
</details></li>
</ul>
<hr>
<h2 id="DESTINE-Dynamic-Goal-Queries-with-Temporal-Transductive-Alignment-for-Trajectory-Prediction"><a href="#DESTINE-Dynamic-Goal-Queries-with-Temporal-Transductive-Alignment-for-Trajectory-Prediction" class="headerlink" title="DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for Trajectory Prediction"></a>DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07438">http://arxiv.org/abs/2310.07438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rezaul Karim, Soheil Mohamad Alizadeh Shabestary, Amir Rasouli</li>
<li>for: 预测路用户的时间相关路径，以便更好地预测路用户的行为。</li>
<li>methods: 使用semantic map信息和模型交互，并建立一种能够理解不同粒度的行为的机制。</li>
<li>results: 使用DESTINE方法，在Argoverse数据集上实现了状态领先的性能，并通过精细的ablation研究，证明了不同模块的贡献。<details>
<summary>Abstract</summary>
Predicting temporally consistent road users' trajectories in a multi-agent setting is a challenging task due to unknown characteristics of agents and their varying intentions. Besides using semantic map information and modeling interactions, it is important to build an effective mechanism capable of reasoning about behaviors at different levels of granularity. To this end, we propose Dynamic goal quErieS with temporal Transductive alIgNmEnt (DESTINE) method. Unlike past arts, our approach 1) dynamically predicts agents' goals irrespective of particular road structures, such as lanes, allowing the method to produce a more accurate estimation of destinations; 2) achieves map compliant predictions by generating future trajectories in a coarse-to-fine fashion, where the coarser predictions at a lower frame rate serve as intermediate goals; and 3) uses an attention module designed to temporally align predicted trajectories via masked attention. Using the common Argoverse benchmark dataset, we show that our method achieves state-of-the-art performance on various metrics, and further investigate the contributions of proposed modules via comprehensive ablation studies.
</details>
<details>
<summary>摘要</summary>
预测 temporally consistent road users' trajectories 在多代理人设定下是一项具有挑战性的任务，因为代理人的特性和意图都是未知的。除了使用semantic map信息和模型交互外，我们还需要建立一种有效的机制，以便对代理人的行为进行不同级别的推理。为此，我们提出了动态目标QuErieS with temporal Transductive alIgNmEnt（DESTINE）方法。与过去的艺术不同，我们的方法具有以下特点：1. 动态预测代理人的目标，不受特定的道路结构，如车道，影响预测的精度;2. 实现地图符合的预测，通过在粗粒度和细粒度之间进行多层次预测，使得中间的粗粒度预测服为间接目标;3. 使用面向时间的注意模块，通过做Masked attention来进行时间启配。使用常用的 Argoverse 数据集，我们表明了我们的方法在不同的维度上达到了领先的性能，并通过完整的减少研究来调查提posed模块的贡献。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Voronoi-based-Convolutional-Neural-Network-Framework-for-Pushing-Person-Detection-in-Crowd-Videos"><a href="#A-Novel-Voronoi-based-Convolutional-Neural-Network-Framework-for-Pushing-Person-Detection-in-Crowd-Videos" class="headerlink" title="A Novel Voronoi-based Convolutional Neural Network Framework for Pushing Person Detection in Crowd Videos"></a>A Novel Voronoi-based Convolutional Neural Network Framework for Pushing Person Detection in Crowd Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07416">http://arxiv.org/abs/2310.07416</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pedestriandynamics/vcnn4pude">https://github.com/pedestriandynamics/vcnn4pude</a></li>
<li>paper_authors: Ahmed Alia, Mohammed Maree, Mohcine Chraibi, Armin Seyfried</li>
<li>for: 本研究旨在通过分析人群中微观动态行为，提供更深刻的人群模式和互动理解，以便制定更有效的人群管理策略、优化人群流动和提高总体人群体验。</li>
<li>methods: 本研究提出了一种新的自动化推拿框架，包括两个主要组成部分：i）特征提取和ii）视频标注。在特征提取部分，采用了基于Voronoi方法的新方法来确定输入视频中每个人的本地区域。然后，这些区域被 fed into EfficientNetV1B0卷积神经网络以提取每个人在时间上的深度特征。在第二个组成部分，使用了一个完全连接层与sigmoid激活函数来分析这些深度特征，并将其与视频中推拿行为相关的个体标注。</li>
<li>results: 实验结果表明，提议的框架在比较分析中超过了7个基准方法。<details>
<summary>Abstract</summary>
Analyzing the microscopic dynamics of pushing behavior within crowds can offer valuable insights into crowd patterns and interactions. By identifying instances of pushing in crowd videos, a deeper understanding of when, where, and why such behavior occurs can be achieved. This knowledge is crucial to creating more effective crowd management strategies, optimizing crowd flow, and enhancing overall crowd experiences. However, manually identifying pushing behavior at the microscopic level is challenging, and the existing automatic approaches cannot detect such microscopic behavior. Thus, this article introduces a novel automatic framework for identifying pushing in videos of crowds on a microscopic level. The framework comprises two main components: i) Feature extraction and ii) Video labeling. In the feature extraction component, a new Voronoi-based method is developed for determining the local regions associated with each person in the input video. Subsequently, these regions are fed into EfficientNetV1B0 Convolutional Neural Network to extract the deep features of each person over time. In the second component, a combination of a fully connected layer with a Sigmoid activation function is employed to analyze these deep features and annotate the individuals involved in pushing within the video. The framework is trained and evaluated on a new dataset created using six real-world experiments, including their corresponding ground truths. The experimental findings indicate that the suggested framework outperforms seven baseline methods that are employed for comparative analysis purposes.
</details>
<details>
<summary>摘要</summary>
可以通过分析人群中微型动态行为来获得有价值的人群模式和互动知识。通过在人群视频中标识推担行为，可以更深入地理解推担行为何时、何处、何 raison。这些知识是创建更有效的人群管理策略、优化人群流动和提高总体人群体验的关键。然而，手动地在微型水平上识别推担行为很困难，而现有的自动方法无法检测微型行为。因此，本文提出了一种新的自动框架，用于在人群视频中识别推担行为。该框架包括两个主要组成部分：一是特征提取，二是视频标注。在特征提取部分，我们开发了一种基于Voronoi区域的新方法，用于每个输入视频中确定每个人的本地区域。然后，这些区域将被传递给EfficientNetV1B0卷积神经网络，以提取每个人的深度特征。在第二个组成部分，我们使用一个全连接层和sigmoid激活函数，以分析这些深度特征并将其与视频中的推担行为关联。我们对新的实验 dataset 进行了训练和评估，该dataset 包括六个实际实验，以及其对应的地面真实值。实验结果表明，我们的方法在七个基准方法的比较分析中表现出色，并且在识别推担行为方面达到了更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="CLIP-for-Lightweight-Semantic-Segmentation"><a href="#CLIP-for-Lightweight-Semantic-Segmentation" class="headerlink" title="CLIP for Lightweight Semantic Segmentation"></a>CLIP for Lightweight Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07394">http://arxiv.org/abs/2310.07394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Jin, Wankou Yang</li>
<li>for: 该文章目的是提出一种新的语言引导的Semantic Segmentation方法，使得语言引导的方法可以应用于轻量级网络。</li>
<li>methods: 该方法使用了一种新的特征融合模块，该模块通过并行的CNN和Transformer网络实现语言引导的特征融合，从而提高Semantic Segmentation的性能。</li>
<li>results: 实验结果表明，该方法可以在不同的视觉背景下 achieve better performance than previous SOTA work，例如DenseCLIP，并且可以全面利用预训练的语言优先知识来提高Semantic Segmentation的性能。<details>
<summary>Abstract</summary>
The large-scale pretrained model CLIP, trained on 400 million image-text pairs, offers a promising paradigm for tackling vision tasks, albeit at the image level. Later works, such as DenseCLIP and LSeg, extend this paradigm to dense prediction, including semantic segmentation, and have achieved excellent results. However, the above methods either rely on CLIP-pretrained visual backbones or use none-pretrained but heavy backbones such as Swin, while falling ineffective when applied to lightweight backbones. The reason for this is that the lightweitht networks, feature extraction ability of which are relatively limited, meet difficulty embedding the image feature aligned with text embeddings perfectly. In this work, we present a new feature fusion module which tackles this problem and enables language-guided paradigm to be applied to lightweight networks. Specifically, the module is a parallel design of CNN and transformer with a two-way bridge in between, where CNN extracts spatial information and visual context of the feature map from the image encoder, and the transformer propagates text embeddings from the text encoder forward. The core of the module is the bidirectional fusion of visual and text feature across the bridge which prompts their proximity and alignment in embedding space. The module is model-agnostic, which can not only make language-guided lightweight semantic segmentation practical, but also fully exploit the pretrained knowledge of language priors and achieve better performance than previous SOTA work, such as DenseCLIP, whatever the vision backbone is. Extensive experiments have been conducted to demonstrate the superiority of our method.
</details>
<details>
<summary>摘要</summary>
大规模预训练模型CLIP，训练了400万张图像和文本对应对，提供了解决视觉任务的有前途的方法，即图像水平。后续的工作，如 denseclip 和 lseg，在 dense prediction 方面扩展了这种方法，并取得了出色的结果。然而，以上方法都是基于 CLIP 预训练的视觉脊梁或使用不预训练的但是重量级别的脊梁，如 swin，而不是使用轻量级别的脊梁。这是因为轻量级别的网络，其特征提取能力相对较弱，难以将图像特征与文本嵌入完全匹配。在这种情况下，我们提出了一种新的特征融合模块，该模块可以解决这个问题，并使得语言导向的方法可以应用于轻量级别的网络。具体来说，该模块是一种并行的 CNN 和 transformer 的设计，其中 CNN 提取图像中的空间信息和视觉上下文，而 transformer 将文本嵌入从文本编码器传递进来。模块的核心是两个方向的特征融合，使得视觉和文本特征在嵌入空间中相互吸引。这种模块是模型无关的，可以不仅使得语言导向的轻量级别semantic segmentation 成为现实，还可以全面利用预训练的语言优先知识，并在不同的视觉脊梁上达到更高的性能，比如 denseclip 等。我们进行了广泛的实验，以证明我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-Guided-by-Gradient-Signal-to-Noise-Ratio-of-Parameters"><a href="#Domain-Generalization-Guided-by-Gradient-Signal-to-Noise-Ratio-of-Parameters" class="headerlink" title="Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters"></a>Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07361">http://arxiv.org/abs/2310.07361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateusz Michalkiewicz, Masoud Faraki, Xiang Yu, Manmohan Chandraker, Mahsa Baktashmotlagh</li>
<li>for: 防止深度神经网络过拟合源频道数据</li>
<li>methods: 基于梯度信号噪声比（GSNR）选择dropout掩码，利用元学习法寻找优化的dropout比率</li>
<li>results: 在标准频道总结chmark上达到竞争性的结果，包括分类和人脸反掩膜问题<details>
<summary>Abstract</summary>
Overfitting to the source domain is a common issue in gradient-based training of deep neural networks. To compensate for the over-parameterized models, numerous regularization techniques have been introduced such as those based on dropout. While these methods achieve significant improvements on classical benchmarks such as ImageNet, their performance diminishes with the introduction of domain shift in the test set i.e. when the unseen data comes from a significantly different distribution. In this paper, we move away from the classical approach of Bernoulli sampled dropout mask construction and propose to base the selection on gradient-signal-to-noise ratio (GSNR) of network's parameters. Specifically, at each training step, parameters with high GSNR will be discarded. Furthermore, we alleviate the burden of manually searching for the optimal dropout ratio by leveraging a meta-learning approach. We evaluate our method on standard domain generalization benchmarks and achieve competitive results on classification and face anti-spoofing problems.
</details>
<details>
<summary>摘要</summary>
常见的过拟合问题在深度神经网络的梯度基本训练中出现。为了缓解过参数的模型，许多正则化技术被引入，如基于dropout的方法。这些方法在ImageNet等 классических测试集上实现了显著改善，但是在测试集中的领域变化时，其性能减退。在这篇论文中，我们偏离了传统的bernoulli抽样dropout面积建构方法，并基于网络参数的梯度噪声比率（GSNR）来选择参数。具体来说，在每个训练步骤中，具有高GSNR的参数将被排除。此外，我们利用元学习方法，以避免手动搜索dropout率的优化。我们在标准的领域普适性测试上评估了我们的方法，并在分类和人脸防伪检测问题上实现了竞争性的结果。
</details></li>
</ul>
<hr>
<h2 id="Diagnosing-Bipolar-Disorder-from-3-D-Structural-Magnetic-Resonance-Images-Using-a-Hybrid-GAN-CNN-Method"><a href="#Diagnosing-Bipolar-Disorder-from-3-D-Structural-Magnetic-Resonance-Images-Using-a-Hybrid-GAN-CNN-Method" class="headerlink" title="Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance Images Using a Hybrid GAN-CNN Method"></a>Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance Images Using a Hybrid GAN-CNN Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07359">http://arxiv.org/abs/2310.07359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masood Hamed Saghayan, Mohammad Hossein Zolfagharnasab, Ali Khadem, Farzam Matinfar, Hassan Rashidi</li>
<li>for: 这个研究旨在开发一个基于三维显微镜像（sMRI）的潜在鉴别BD患者的方法，以提供更可靠的诊断支持系统，帮助医生更加准确地诊断BD患者。</li>
<li>methods: 这个研究使用了一种混合式GAN-CNN模型，通过对sMRI样本进行扩展，提高了BD的诊断精度。研究还使用了5-fold检查分割，以评估不同扩展比例的影响。</li>
<li>results: 根据结果，这个研究获得了75.8%的准确率、60.3%的感染率和82.5%的特异率，较以往研究高出3-5%，并使用了少于6%的样本数。此外，研究还证明了一个2D层基本的GAN生成器可以有效地重现复杂的3D脑样本，并且使用了50%的扩展阈值。<details>
<summary>Abstract</summary>
Bipolar Disorder (BD) is a psychiatric condition diagnosed by repetitive cycles of hypomania and depression. Since diagnosing BD relies on subjective behavioral assessments over a long period, a solid diagnosis based on objective criteria is not straightforward. The current study responded to the described obstacle by proposing a hybrid GAN-CNN model to diagnose BD from 3-D structural MRI Images (sMRI). The novelty of this study stems from diagnosing BD from sMRI samples rather than conventional datasets such as functional MRI (fMRI), electroencephalography (EEG), and behavioral symptoms while removing the data insufficiency usually encountered when dealing with sMRI samples. The impact of various augmentation ratios is also tested using 5-fold cross-validation. Based on the results, this study obtains an accuracy rate of 75.8%, a sensitivity of 60.3%, and a specificity of 82.5%, which are 3-5% higher than prior work while utilizing less than 6% sample counts. Next, it is demonstrated that a 2- D layer-based GAN generator can effectively reproduce complex 3D brain samples, a more straightforward technique than manual image processing. Lastly, the optimum augmentation threshold for the current study using 172 sMRI samples is 50%, showing the applicability of the described method for larger sMRI datasets. In conclusion, it is established that data augmentation using GAN improves the accuracy of the CNN classifier using sMRI samples, thus developing more reliable decision support systems to assist practitioners in identifying BD patients more reliably and in a shorter period
</details>
<details>
<summary>摘要</summary>
《抑郁症（BD）是一种心理疾病，通过反复的假mania和抑郁而诊断。由于诊断BD需要长期的主观行为评估，因此不可靠的诊断方法不是很 straightforward。本研究回应了这个挑战，提出了一种将GAN和CNN结合使用的模型，用于从三维结构MRI图像（sMRI）上诊断BD。本研究的创新之处在于，不同于以往的fMRI、EEG和行为症状数据，从sMRI样本中诊断BD，同时解决了对sMRI样本的数据缺乏问题。此外，本研究还测试了不同的扩展比例，并使用5次分割验证。根据结果，本研究获得了75.8%的准确率，60.3%的敏感度和82.5%的特异性，与先前的工作相比高出3-5%，使用的样本数量少于6%。然后，示出了一种使用2维GAN生成器可以有效地重现复杂的3D脑样本，比手动图像处理更加简单。最后，本研究发现了使用172个sMRI样本的最佳扩展阈值为50%。总之，本研究证明了通过GAN数据扩展提高CNN分类器的准确率，从而开发更可靠的决策支持系统，帮助医生更准确地诊断BD患者，更快速地进行诊断。
</details></li>
</ul>
<hr>
<h2 id="IMITATE-Clinical-Prior-Guided-Hierarchical-Vision-Language-Pre-training"><a href="#IMITATE-Clinical-Prior-Guided-Hierarchical-Vision-Language-Pre-training" class="headerlink" title="IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training"></a>IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07355">http://arxiv.org/abs/2310.07355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Che Liu, Sibo Cheng, Miaojing Shi, Anand Shah, Wenjia Bai, Rossella Arcucci</li>
<li>for: 这 paper 是为了提高医学视语预训练（VLP）中对医疗报告和相关医疗图像的特征提取方法。</li>
<li>methods: 这 paper 提出了一种新的临床指导 VLP 框架，named IMITATE，通过在医疗报告中捕捉结构信息来学习视语对Alignment。该框架使用多级视觉特征来对医疗报告中的描述性和结论性文本进行分层对应。此外，这 paper 还提出了一种新的临床知识引入对医疗报告进行对比学习的矫正损失。</li>
<li>results: 根据实验结果，IMITATE 模型在六个不同的数据集上比基eline VLP 方法表现出色，在五种医疗影像下游任务中均显示出优于基eline方法。这些结果表明，通过 integrate 医疗报告中的结构信息可以提高视语预训练中的特征提取效果。<details>
<summary>Abstract</summary>
In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furthermore, a new clinical-informed contrastive loss is introduced for cross-modal learning, which accounts for clinical prior knowledge in formulating sample correlations in contrastive learning. The proposed model, IMITATE, outperforms baseline VLP methods across six different datasets, spanning five medical imaging downstream tasks. Comprehensive experimental results highlight the advantages of integrating the hierarchical structure of medical reports for vision-language alignment.
</details>
<details>
<summary>摘要</summary>
在医疗领域的医学视语预训练（VLP）领域，有大量努力投入到从医疗报告和关联的医学图像中提取文本和图像特征。然而，大多数现有方法可能忽略了利用医疗报告的自然层次结构，这些报告通常分为描述性内容的“发现”和结论的“印象”。相反，现有的医学VLP方法通常将报告简化为单一实体或分解成多个符号。在这种情况下，我们提出了一种新的临床导向的VLP框架，名为IMITATE，用于从医疗报告中学习层次结构信息。该框架在胸部X射线图像中提取多级视觉特征，并将这些特征与描述性和结论文本进行层次视语对应。此外，我们还引入了一种新的临床知识 Informed Contrastive Loss，用于在跨模态学习中考虑临床知识。我们的模型IMITATE在六个不同的数据集上比基eline VLP方法表现出色，在五种医学影像下沉淀任务中取得了最高的表现。广泛的实验结果表明，将医疗报告的层次结构 интеグ吧到视语对应可以提高视语对应的性能。
</details></li>
</ul>
<hr>
<h2 id="PointHR-Exploring-High-Resolution-Architectures-for-3D-Point-Cloud-Segmentation"><a href="#PointHR-Exploring-High-Resolution-Architectures-for-3D-Point-Cloud-Segmentation" class="headerlink" title="PointHR: Exploring High-Resolution Architectures for 3D Point Cloud Segmentation"></a>PointHR: Exploring High-Resolution Architectures for 3D Point Cloud Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07743">http://arxiv.org/abs/2310.07743</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haibo-qiu/PointHR">https://github.com/haibo-qiu/PointHR</a></li>
<li>paper_authors: Haibo Qiu, Baosheng Yu, Yixin Chen, Dacheng Tao</li>
<li>for: 高精度3D点云分割，即使没有额外的优化和修饰。</li>
<li>methods: 提出了一种高精度架构，named PointHR，包括knn顺序运算符和差分扩散运算符，以及预计算序列和扩散运算符的索引。</li>
<li>results: 在S3DIS和ScanNetV2数据集上进行了广泛的实验，并表明PointHR可以高度竞争与当前最佳方法无需额外优化和修饰。<details>
<summary>Abstract</summary>
Significant progress has been made recently in point cloud segmentation utilizing an encoder-decoder framework, which initially encodes point clouds into low-resolution representations and subsequently decodes high-resolution predictions. Inspired by the success of high-resolution architectures in image dense prediction, which always maintains a high-resolution representation throughout the entire learning process, we consider it also highly important for 3D dense point cloud analysis. Therefore, in this paper, we explore high-resolution architectures for 3D point cloud segmentation. Specifically, we generalize high-resolution architectures using a unified pipeline named PointHR, which includes a knn-based sequence operator for feature extraction and a differential resampling operator to efficiently communicate different resolutions. Additionally, we propose to avoid numerous on-the-fly computations of high-resolution architectures by pre-computing the indices for both sequence and resampling operators. By doing so, we deliver highly competitive high-resolution architectures while capitalizing on the benefits of well-designed point cloud blocks without additional effort. To evaluate these architectures for dense point cloud analysis, we conduct thorough experiments using S3DIS and ScanNetV2 datasets, where the proposed PointHR outperforms recent state-of-the-art methods without any bells and whistles. The source code is available at \url{https://github.com/haibo-qiu/PointHR}.
</details>
<details>
<summary>摘要</summary>
Recently, there have been significant advances in point cloud segmentation using an encoder-decoder framework, where point clouds are first encoded into low-resolution representations and then decoded into high-resolution predictions. Inspired by the success of high-resolution architectures in image dense prediction, we believe it is also crucial for 3D dense point cloud analysis. Therefore, in this paper, we explore high-resolution architectures for 3D point cloud segmentation. Specifically, we propose a unified pipeline named PointHR, which includes a knn-based sequence operator for feature extraction and a differential resampling operator to efficiently communicate different resolutions. Additionally, we pre-compute the indices for both sequence and resampling operators to avoid on-the-fly computations. By doing so, we achieve highly competitive high-resolution architectures without additional effort.To evaluate these architectures for dense point cloud analysis, we conduct thorough experiments using S3DIS and ScanNetV2 datasets. The results show that our proposed PointHR outperforms recent state-of-the-art methods without any additional bells and whistles. The source code is available at \url{https://github.com/haibo-qiu/PointHR}.
</details></li>
</ul>
<hr>
<h2 id="Guided-Attention-for-Interpretable-Motion-Captioning"><a href="#Guided-Attention-for-Interpretable-Motion-Captioning" class="headerlink" title="Guided Attention for Interpretable Motion Captioning"></a>Guided Attention for Interpretable Motion Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07324">http://arxiv.org/abs/2310.07324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rd20karim/m2t-interpretable">https://github.com/rd20karim/m2t-interpretable</a></li>
<li>paper_authors: Karim Radouane, Andon Tchechmedjiev, Sylvie Ranwez, Julien Lagarde</li>
<li>for: 本研究旨在生成文本从动作中，并提出了一种使用运动编码器和时空注意模型的方法，以及一种在训练中引导注意力的策略，以提高文本生成的可读性。</li>
<li>methods: 本研究使用运动编码器和时空注意模型，并提出了一种在训练中引导注意力的策略，以提高文本生成的可读性。</li>
<li>results: 本研究在KIT MLD dataset和HumanML3D dataset上取得了比基eline高的性能，包括BLEU@4、ROUGE-L、CIDEr和Bertscore等指标。<details>
<summary>Abstract</summary>
While much effort has been invested in generating human motion from text, relatively few studies have been dedicated to the reverse direction, that is, generating text from motion. Much of the research focuses on maximizing generation quality without any regard for the interpretability of the architectures, particularly regarding the influence of particular body parts in the generation and the temporal synchronization of words with specific movements and actions. This study explores the combination of movement encoders with spatio-temporal attention models and proposes strategies to guide the attention during training to highlight perceptually pertinent areas of the skeleton in time. We show that adding guided attention with adaptive gate leads to interpretable captioning while improving performance compared to higher parameter-count non-interpretable SOTA systems. On the KIT MLD dataset, we obtain a BLEU@4 of 24.4% (SOTA+6%), a ROUGE-L of 58.30% (SOTA +14.1%), a CIDEr of 112.10 (SOTA +32.6) and a Bertscore of 41.20% (SOTA +18.20%). On HumanML3D, we obtain a BLEU@4 of 25.00 (SOTA +2.7%), a ROUGE-L score of 55.4% (SOTA +6.1%), a CIDEr of 61.6 (SOTA -10.9%), a Bertscore of 40.3% (SOTA +2.5%). Our code implementation and reproduction details will be soon available at https://github.com/rd20karim/M2T-Interpretable/tree/main.
</details>
<details>
<summary>摘要</summary>
“尽管有很多研究投入到人体动作从文本生成中，但相对少数研究关注反向方向，即从动作生成文本。大多数研究集中于提高生成质量而忽略建模 interpretability，特别是关于特定身体部分在生成中的影响和时间同步词语与动作之间的关系。本研究拟合运动编码器和空间时间注意模型，并提出了引导注意力的策略，以提高文本生成的可读性。我们的实验表明，在 KIT MLD 数据集上，通过添加引导注意力和适应门户，可以实现可读性的文本生成，同时提高性能，与高参数计数的非可读性 SOTA 系统相比。在 HumanML3D 数据集上，我们获得了 BLEU@4 的 25.00% (SOTA +2.7%), ROUGE-L 的 55.4% (SOTA +6.1%), CIDEr 的 61.6 (SOTA -10.9%), Bertscore 的 40.3% (SOTA +2.5%).我们的代码实现和复现细节将在 GitHub 上公开。”
</details></li>
</ul>
<hr>
<h2 id="A-webcam-based-machine-learning-approach-for-three-dimensional-range-of-motion-evaluation"><a href="#A-webcam-based-machine-learning-approach-for-three-dimensional-range-of-motion-evaluation" class="headerlink" title="A webcam-based machine learning approach for three-dimensional range of motion evaluation"></a>A webcam-based machine learning approach for three-dimensional range of motion evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07322">http://arxiv.org/abs/2310.07322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoye Michael Wang, Derek T. Smith, Qin Zhu</li>
<li>for: 这个研究旨在提供一种可靠且可以远端存取的肢体范围动作评估方法，并评估这种方法的可靠性。</li>
<li>methods: 这个研究使用机器学习方法来评估肢体范围动作，并使用 webcam 进行评估。</li>
<li>results: 研究结果显示，这种 webcam 方法具有高的内排重复性和其他评估方法的相互重复性，并且可以实现远端存取physical therapy 和rehabilitation。<details>
<summary>Abstract</summary>
Background. Joint range of motion (ROM) is an important quantitative measure for physical therapy. Commonly relying on a goniometer, accurate and reliable ROM measurement requires extensive training and practice. This, in turn, imposes a significant barrier for those who have limited in-person access to healthcare.   Objective. The current study presents and evaluates an alternative machine learning-based ROM evaluation method that could be remotely accessed via a webcam.   Methods. To evaluate its reliability, the ROM measurements for a diverse set of joints (neck, spine, and upper and lower extremities) derived using this method were compared to those obtained from a marker-based optical motion capture system.   Results. Data collected from 25 healthy adults demonstrated that the webcam solution exhibited high test-retest reliability, with substantial to almost perfect intraclass correlation coefficients for most joints. Compared with the marker-based system, the webcam-based system demonstrated substantial to almost perfect inter-rater reliability for some joints, and lower inter-rater reliability for other joints (e.g., shoulder flexion and elbow flexion), which could be attributed to the reduced sensitivity to joint locations at the apex of the movement.   Conclusions. The proposed webcam-based method exhibited high test-retest and inter-rater reliability, making it a versatile alternative for existing ROM evaluation methods in clinical practice and the tele-implementation of physical therapy and rehabilitation.
</details>
<details>
<summary>摘要</summary>
背景： JOINT 范围动作（ROM）是物理治疗中非常重要的量化测量。通常使用指南仪，准确和可靠的 ROM 测量需要广泛的训练和实践。这种情况限制了那些具有有限的面对面医疗访问的人们。目标：本研究提出了一种基于机器学习的 ROM 评估方法，可以通过网络摄像头访问。方法：为评估其可靠性，使用这种方法测量的 JOINT 范围动作数据与使用标记器基于光学运动跟踪系统获取的数据进行比较。结果：从25名健康成人收集的数据表明，网络摄像头解决方案具有高测试再测试可靠性和大多数关节的substantial到几乎完美的同准类相关系数。与标记器基于系统相比，网络摄像头解决方案在一些关节（如肩部和肘部）上具有substantial到几乎完美的同准类相关系数，而在其他关节（如肩部和肘部）上具有较低的同准类相关系数，这可能是因为网络摄像头的感度减退。结论：提出的网络摄像头解决方案具有高测试再测试和同准类相关性，使其成为现有的 ROM 评估方法的多样化选择，并且可以在临床实践和远程物理治疗中应用。
</details></li>
</ul>
<hr>
<h2 id="Deep-Aramaic-Towards-a-Synthetic-Data-Paradigm-Enabling-Machine-Learning-in-Epigraphy"><a href="#Deep-Aramaic-Towards-a-Synthetic-Data-Paradigm-Enabling-Machine-Learning-in-Epigraphy" class="headerlink" title="Deep Aramaic: Towards a Synthetic Data Paradigm Enabling Machine Learning in Epigraphy"></a>Deep Aramaic: Towards a Synthetic Data Paradigm Enabling Machine Learning in Epigraphy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07310">http://arxiv.org/abs/2310.07310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrei C. Aioanei, Regine Hunziker-Rodewald, Konstantin Klein, Dominik L. Michels</li>
<li>for: 这个研究旨在提高古代文献中文字识别率，使用现代人工智能技术，如机器学习（ML），从古代文献中提取知识。</li>
<li>methods: 该研究使用创新的数据生成技术，synthesize photo-realistic Aramaic letter datasets，包括文本特征、照明、损害和扩展等，以模拟实际铭文多样性。</li>
<li>results: 该研究成功创建了250,000个训练图像和25,000个验证图像，涵盖了古代阿拉伯字母的22个类别。这个全面的数据集为训练一个剩余神经网络（ResNet）提供了高度的识别率，并在不同材料和风格下进行了成功的检验，证明了模型的可重复性。<details>
<summary>Abstract</summary>
Epigraphy increasingly turns to modern artificial intelligence (AI) technologies such as machine learning (ML) for extracting insights from ancient inscriptions. However, scarce labeled data for training ML algorithms severely limits current techniques, especially for ancient scripts like Old Aramaic. Our research pioneers an innovative methodology for generating synthetic training data tailored to Old Aramaic letters. Our pipeline synthesizes photo-realistic Aramaic letter datasets, incorporating textural features, lighting, damage, and augmentations to mimic real-world inscription diversity. Despite minimal real examples, we engineer a dataset of 250,000 training and 25,000 validation images covering the 22 letter classes in the Aramaic alphabet. This comprehensive corpus provides a robust volume of data for training a residual neural network (ResNet) to classify highly degraded Aramaic letters. The ResNet model demonstrates high accuracy in classifying real images from the 8th century BCE Hadad statue inscription. Additional experiments validate performance on varying materials and styles, proving effective generalization. Our results validate the model's capabilities in handling diverse real-world scenarios, proving the viability of our synthetic data approach and avoiding the dependence on scarce training data that has constrained epigraphic analysis. Our innovative framework elevates interpretation accuracy on damaged inscriptions, thus enhancing knowledge extraction from these historical resources.
</details>
<details>
<summary>摘要</summary>
随着epigraphy越来越向现代人工智能(AI)技术，如机器学习(ML)，提取古代铭文中的信息。然而，古代文字如Old Aramaic的有限的标注数据对当前技术带来了严重的限制。我们的研究创新了一种适用于Old Aramaic字母的创新方法。我们的管道Synthesize了高度真实的Aramaic字母数据集，包括文字特征、照明、损害和扩展等，以模拟实际铭文多样性。尽管具有 minimal real examples，我们Enginered一个包括250,000个训练图像和25,000个验证图像的数据集，覆盖了Aramaic字母的22个类型。这个全面的数据集提供了一个强大的训练ResNet神经网络来分类高度损害的Aramaic字母。ResNet模型在实际图像中的8世纪前BCE Hadad雕塑铭文中显示出高精度的分类能力。此外，我们还进行了多种材质和风格的实验，证明了模型的普适性和可行性。我们的结果证明了我们的合成数据方法的可行性，并且不再依赖于稀缺的训练数据，从而提高了铭文解读的准确性，并扩展了历史资源的知识抽取。
</details></li>
</ul>
<hr>
<h2 id="Distilling-Efficient-Vision-Transformers-from-CNNs-for-Semantic-Segmentation"><a href="#Distilling-Efficient-Vision-Transformers-from-CNNs-for-Semantic-Segmentation" class="headerlink" title="Distilling Efficient Vision Transformers from CNNs for Semantic Segmentation"></a>Distilling Efficient Vision Transformers from CNNs for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07265">http://arxiv.org/abs/2310.07265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Zheng, Yunhao Luo, Pengyuan Zhou, Lin Wang</li>
<li>for: 本研究目的是如何将预训练的 CNN 模型知识转移到学习 Compact Vision Transformer (ViT) 模型，而保持其学习能力？</li>
<li>methods: 本研究提出了一个 CNN 到 ViT 知识传递框架（C2VKD），并提出了一个独特的视语相容特征（VLFD）模组和一个像素对像分离分配（PDD）模组。</li>
<li>results: 实验结果显示，与 SoTA KD 方法相比，我们的方法可以提高 mIoU 的提升量超过 200%。<details>
<summary>Abstract</summary>
In this paper, we tackle a new problem: how to transfer knowledge from the pre-trained cumbersome yet well-performed CNN-based model to learn a compact Vision Transformer (ViT)-based model while maintaining its learning capacity? Due to the completely different characteristics of ViT and CNN and the long-existing capacity gap between teacher and student models in Knowledge Distillation (KD), directly transferring the cross-model knowledge is non-trivial. To this end, we subtly leverage the visual and linguistic-compatible feature character of ViT (i.e., student), and its capacity gap with the CNN (i.e., teacher) and propose a novel CNN-to-ViT KD framework, dubbed C2VKD. Importantly, as the teacher's features are heterogeneous to those of the student, we first propose a novel visual-linguistic feature distillation (VLFD) module that explores efficient KD among the aligned visual and linguistic-compatible representations. Moreover, due to the large capacity gap between the teacher and student and the inevitable prediction errors of the teacher, we then propose a pixel-wise decoupled distillation (PDD) module to supervise the student under the combination of labels and teacher's predictions from the decoupled target and non-target classes. Experiments on three semantic segmentation benchmark datasets consistently show that the increment of mIoU of our method is over 200% of the SoTA KD methods
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们面临一个新的问题：如何将预训练的庞然一物的 CNN 模型的知识传递到学习 compact Vision Transformer (ViT) 模型，而保持其学习能力？由于 CNN 和 ViT 模型之间的完全不同特征和长期存在的容量差异，直接传递模型之间的知识是非常困难的。为此，我们偏好利用 ViT 模型（学生）的视觉和语言相容特征，以及它们与 CNN 模型（教师）之间的容量差，并提出了一种新的 CNN 到 ViT KD 框架，称为 C2VKD。特别是，由于教师模型的特征与学生模型的特征不同，我们首先提出了一种视觉语言相容特征采样（VLFD）模块，以实现有效的 KD。此外，由于教师模型和学生模型之间的容量差较大，以及不可避免的预测错误，我们则提出了一种像素独立分离采样（PDD）模块，以便在教师的预测和非预测类划分中监督学生。我们在三个semantic segmentation benchmarkdataset上进行了实验，结果表明，我们的方法可以提高 mIoU 的增量超过 200% 的 SoTA KD 方法。
</details></li>
</ul>
<hr>
<h2 id="ADASR-An-Adversarial-Auto-Augmentation-Framework-for-Hyperspectral-and-Multispectral-Data-Fusion"><a href="#ADASR-An-Adversarial-Auto-Augmentation-Framework-for-Hyperspectral-and-Multispectral-Data-Fusion" class="headerlink" title="ADASR: An Adversarial Auto-Augmentation Framework for Hyperspectral and Multispectral Data Fusion"></a>ADASR: An Adversarial Auto-Augmentation Framework for Hyperspectral and Multispectral Data Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07255">http://arxiv.org/abs/2310.07255</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fangfang11-plog/adasr">https://github.com/fangfang11-plog/adasr</a></li>
<li>paper_authors: Jinghui Qin, Lihuang Fang, Ruitao Lu, Liang Lin, Yukai Shi</li>
<li>for: 提高深度学习驱动的干涉光спектраль图像（HSI）超分辨率，通过融合干涉光спектраль图像（HSI）和多spectral图像（MSI），使用深度神经网络（DNNs）来生成高空间分辨率HSI（HR-HSI）。</li>
<li>methods: 我们提出了一种新的反对抗整形自动数据增强框架ADASR，可以自动优化和增强HSI-MSI示例对的多样性，以便在实际场景中应用。我们的框架是示例意识的，并通过对扩展网络和两个下采样网络进行对抗学习来优化它们，以便学习更加稳定的下采样网络用于训练扩展网络。</li>
<li>results: 我们的ADASR在两个公共的古典干涉光спектраль数据集上进行了广泛的实验，并证明了我们的ADASR与当前方法相比更加有效。<details>
<summary>Abstract</summary>
Deep learning-based hyperspectral image (HSI) super-resolution, which aims to generate high spatial resolution HSI (HR-HSI) by fusing hyperspectral image (HSI) and multispectral image (MSI) with deep neural networks (DNNs), has attracted lots of attention. However, neural networks require large amounts of training data, hindering their application in real-world scenarios. In this letter, we propose a novel adversarial automatic data augmentation framework ADASR that automatically optimizes and augments HSI-MSI sample pairs to enrich data diversity for HSI-MSI fusion. Our framework is sample-aware and optimizes an augmentor network and two downsampling networks jointly by adversarial learning so that we can learn more robust downsampling networks for training the upsampling network. Extensive experiments on two public classical hyperspectral datasets demonstrate the effectiveness of our ADASR compared to the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
深度学习基于卷积神经网络（DNN）的卷积谱图像（HSI）超分辨率，旨在通过将卷积谱图像（HSI）和多спектраль图像（MSI）融合，生成高空间分辨率卷积谱图像（HR-HSI）。然而，DNN需要大量的训练数据，使其在实际场景中应用受限。在这封信中，我们提出了一种新的反对抗自动数据增强框架ADASR，可以自动优化和增强HSI-MSI样本对的多样性，以便为HSI-MSI融合提供更多的数据。我们的框架是样本意识的，并通过反对学习来优化一个增强器网络和两个下采样网络，以便我们可以更好地学习更有效的下采样网络，用于训练上采样网络。我们在两个公共古典卷积谱数据集上进行了广泛的实验，并证明了我们的ADASR比 estado-of-the-art方法更有效。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Pre-trained-CNNs-and-GRU-Based-Attention-for-Image-Caption-Generation"><a href="#A-Comparative-Study-of-Pre-trained-CNNs-and-GRU-Based-Attention-for-Image-Caption-Generation" class="headerlink" title="A Comparative Study of Pre-trained CNNs and GRU-Based Attention for Image Caption Generation"></a>A Comparative Study of Pre-trained CNNs and GRU-Based Attention for Image Caption Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07252">http://arxiv.org/abs/2310.07252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rashid Khan, Bingding Huang, Haseeb Hassan, Asim Zaman, Zhongfu Ye</li>
<li>for: 这个论文是为了提出一种深度神经网络框架，用于图像描述文本生成。</li>
<li>methods: 该方法使用了多个预训练的卷积神经网络作为Encoder来提取图像特征，并使用GRU语言模型作为Decoder来生成描述文本。它还使用了Bahdanau注意机制与GRU解码器相结合，以便学习专注于特定图像部分。</li>
<li>results: 该方法在MSCOCO和Flickr30k datasets上进行了评估，并取得了与当前方法相当的成绩。<details>
<summary>Abstract</summary>
Image captioning is a challenging task involving generating a textual description for an image using computer vision and natural language processing techniques. This paper proposes a deep neural framework for image caption generation using a GRU-based attention mechanism. Our approach employs multiple pre-trained convolutional neural networks as the encoder to extract features from the image and a GRU-based language model as the decoder to generate descriptive sentences. To improve performance, we integrate the Bahdanau attention model with the GRU decoder to enable learning to focus on specific image parts. We evaluate our approach using the MSCOCO and Flickr30k datasets and show that it achieves competitive scores compared to state-of-the-art methods. Our proposed framework can bridge the gap between computer vision and natural language and can be extended to specific domains.
</details>
<details>
<summary>摘要</summary>
Image captioning是一个复杂的任务，即使用计算机视觉和自然语言处理技术生成图像的文本描述。这篇论文提出了一种深度神经网络框架，用于图像描述生成。我们的方法使用多个预训练的卷积神经网络作为Encoder提取图像特征，并使用GRU基于注意机制来生成描述性句子。为了提高性能，我们将巴登瑙注意模型与GRU解码器结合，让学习关注特定图像部分。我们使用MSCOCO和Flickr30k数据集进行评估，并显示了与状态之前方法相当的分数。我们的提议的框架可以将计算机视觉和自然语言相连，并可以扩展到特定领域。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Missing-MRI-Sequences-from-Available-Modalities-using-Generative-Adversarial-Networks-in-BraTS-Dataset"><a href="#Synthesizing-Missing-MRI-Sequences-from-Available-Modalities-using-Generative-Adversarial-Networks-in-BraTS-Dataset" class="headerlink" title="Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset"></a>Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07250">http://arxiv.org/abs/2310.07250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibrahim Ethem Hamamci</li>
<li>for: 这篇论文的目的是为了提高脑癌MRI检查和诊断的精度和效率，并且将AI技术应用到脑癌MRI量化中。</li>
<li>methods: 这篇论文使用了生成对抗网络（GAN）技术，将三个MRI序列作为输入，生成缺失的第四个MRI序列。</li>
<li>results: 这篇论文的结果显示，使用GAN技术可以实现高品质和现实的MRI序列生成，帮助临床医生提高诊断能力，并且支持AI技术应用到脑癌MRI量化中。<details>
<summary>Abstract</summary>
Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic resonance imaging (MRI) plays a significant role in the diagnosis, treatment planning, and follow-up of glioblastoma patients due to its non-invasive and radiation-free nature. The International Brain Tumor Segmentation (BraTS) challenge has contributed to generating numerous AI algorithms to accurately and efficiently segment glioblastoma sub-compartments using four structural (T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not always be available. To address this issue, Generative Adversarial Networks (GANs) can be used to synthesize the missing MRI sequences. In this paper, we implement and utilize an open-source GAN approach that takes any three MRI sequences as input to generate the missing fourth structural sequence. Our proposed approach is contributed to the community-driven generally nuanced deep learning framework (GaNDLF) and demonstrates promising results in synthesizing high-quality and realistic MRI sequences, enabling clinicians to improve their diagnostic capabilities and support the application of AI methods to brain tumor MRI quantification.
</details>
<details>
<summary>摘要</summary>
高级肿瘤癌是脑肿瘤的一种高度致命的形式。核磁共振成像（MRI）在诊断、治疗规划和跟踪高级肿瘤癌患者中扮演着非常重要的角色，因为它不侵入性和无辐射性。国际脑肿瘤分 segmentation（BraTS）挑战已经促成了许多人工智能算法，以准确和高效地分 segment glioblastoma 子组件使用四种结构（T1、T1Gd、T2、T2-FLAIR）MRI扫描。然而，这四种 MRI 序列可能不总是可用。为解决这个问题，生成对抗网络（GANs）可以用于生成缺失的 MRI 序列。在这篇论文中，我们实现了一种开源 GAN 方法，该方法使用任意三种 MRI 序列作为输入，以生成缺失的第四种结构序列。我们的提议方法被添加到了社区驱动的普遍精细深度学习框架（GaNDLF）中，并在生成高质量和实实际的 MRI 序列方面显示出了扎实的成果，使临床医生可以提高诊断能力，并支持人工智能方法的脑肿瘤 MRI 量化应用。
</details></li>
</ul>
<hr>
<h2 id="IBoxCLA-Towards-Robust-Box-supervised-Segmentation-of-Polyp-via-Improved-Box-dice-and-Contrastive-Latent-anchors"><a href="#IBoxCLA-Towards-Robust-Box-supervised-Segmentation-of-Polyp-via-Improved-Box-dice-and-Contrastive-Latent-anchors" class="headerlink" title="IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via Improved Box-dice and Contrastive Latent-anchors"></a>IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via Improved Box-dice and Contrastive Latent-anchors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07248">http://arxiv.org/abs/2310.07248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwei Wang, Qiang Hu, Hongkuan Shi, Li He, Man He, Wenxuan Dai, Ting Li, Yitong Zhang, Dun Li, Mei Liu, Qiang Li</li>
<li>for: 这篇论文主要应用于泌脓腺部分分类，以提高医疗影像分类的精度和效率。</li>
<li>methods: 这篇论文提出了两种创新的学习方法：Improved Box-dice（IBox）和Contrastive Latent-Anchors（CLA），并将它们融合使用以训练一个坚固的泌脓腺部分分类模型IBoxCLA。这两种学习方法可以将形状学习和位置&#x2F;大小学习分开，从而增强模型的准确性和稳定性。</li>
<li>results: 这篇论文的实验结果显示IBoxCLA在五个公共的泌脓腺数据集上的比较表现，与最近的完全监督泌脓腺部分分类方法相比，有至少6.5%和7.5%的提升。此外，IBoxCLA也比其他的泌脓腺部分分类方法有更好的稳定性和精度。<details>
<summary>Abstract</summary>
Box-supervised polyp segmentation attracts increasing attention for its cost-effective potential. Existing solutions often rely on learning-free methods or pretrained models to laboriously generate pseudo masks, triggering Dice constraint subsequently. In this paper, we found that a model guided by the simplest box-filled masks can accurately predict polyp locations/sizes, but suffers from shape collapsing. In response, we propose two innovative learning fashions, Improved Box-dice (IBox) and Contrastive Latent-Anchors (CLA), and combine them to train a robust box-supervised model IBoxCLA. The core idea behind IBoxCLA is to decouple the learning of location/size and shape, allowing for focused constraints on each of them. Specifically, IBox transforms the segmentation map into a proxy map using shape decoupling and confusion-region swapping sequentially. Within the proxy map, shapes are disentangled, while locations/sizes are encoded as box-like responses. By constraining the proxy map instead of the raw prediction, the box-filled mask can well supervise IBoxCLA without misleading its shape learning. Furthermore, CLA contributes to shape learning by generating two types of latent anchors, which are learned and updated using momentum and segmented polyps to steadily represent polyp and background features. The latent anchors facilitate IBoxCLA to capture discriminative features within and outside boxes in a contrastive manner, yielding clearer boundaries. We benchmark IBoxCLA on five public polyp datasets. The experimental results demonstrate the competitive performance of IBoxCLA compared to recent fully-supervised polyp segmentation methods, and its superiority over other box-supervised state-of-the-arts with a relative increase of overall mDice and mIoU by at least 6.5% and 7.5%, respectively.
</details>
<details>
<summary>摘要</summary>
《Box-supervised胞分割吸引了增加的关注，因为它的成本效果很高。现有的解决方案frequently使用学习无关的方法或预训练模型生成 Pseudo masks，从而触发 dice约束。在这篇论文中，我们发现一个以最简单的方块填充mask为导向的模型可以准确预测胞位置/大小，但是受到形态压缩的影响。为了解决这个问题，我们提出了两种创新的学习方法：Improved Box-dice（IBox）和Contrastive Latent-Anchors（CLA），并将它们相结合以训练一个可靠的box-supervised模型IBoxCLA。IBoxCLA的核心想法是解耦位置/大小和形态的学习，以便对每个特征进行专注的约束。specifically, IBox将 segmentation map转换为一个代理 map，先后进行形态解耦和干扰区域交换。在代理 map中，形态被解耦，而位置/大小被编码为方块样式的回应。通过约束代理 map而不是直接约束raw prediction，方块填充mask可以良好地指导IBoxCLA不mislead its shape learning。此外，CLA通过生成两种类型的latent anchors，通过涨动和分割胞和背景特征来逐渐代表胞和背景特征。这些latent anchors使IBoxCLA能够在对和外部box中捕捉特征，并通过对比式学习捕捉更清晰的边界。我们对IBoxCLA进行五个公共胞数据集的测试。实验结果表明IBoxCLA与最近的完全监督胞分割方法相比，有着竞争性的性能，并且与其他box-supervised state-of-the-arts相比，在全部mDice和mIoU上增加至少6.5%和7.5%。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-the-Placement-of-Roadside-LiDARs-for-Autonomous-Driving"><a href="#Optimizing-the-Placement-of-Roadside-LiDARs-for-Autonomous-Driving" class="headerlink" title="Optimizing the Placement of Roadside LiDARs for Autonomous Driving"></a>Optimizing the Placement of Roadside LiDARs for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07247">http://arxiv.org/abs/2310.07247</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PJLab-ADG/PCSim">https://github.com/PJLab-ADG/PCSim</a></li>
<li>paper_authors: Wentao Jiang, Hao Xiang, Xinyu Cai, Runsheng Xu, Jiaqi Ma, Yikang Li, Gim Hee Lee, Si Liu</li>
<li>for: 提高自动驾驶roadside LiDAR的探测性能</li>
<li>methods: 基于探测功能的greedy算法和单个点云帧学习的感知预测器</li>
<li>results: 提出了一种基于探测功能的LiDAR位置优化方法，并创建了Roadside-Opt数据集以便进一步研究roadside LiDAR placement问题。<details>
<summary>Abstract</summary>
Multi-agent cooperative perception is an increasingly popular topic in the field of autonomous driving, where roadside LiDARs play an essential role. However, how to optimize the placement of roadside LiDARs is a crucial but often overlooked problem. This paper proposes an approach to optimize the placement of roadside LiDARs by selecting optimized positions within the scene for better perception performance. To efficiently obtain the best combination of locations, a greedy algorithm based on perceptual gain is proposed, which selects the location that can maximize the perceptual gain sequentially. We define perceptual gain as the increased perceptual capability when a new LiDAR is placed. To obtain the perception capability, we propose a perception predictor that learns to evaluate LiDAR placement using only a single point cloud frame. A dataset named Roadside-Opt is created using the CARLA simulator to facilitate research on the roadside LiDAR placement problem.
</details>
<details>
<summary>摘要</summary>
Multi-agent合作感知是自驾车领域中越来越受欢迎的话题，路边LiDAR扮演着关键性的角色。然而，如何优化路边LiDAR的布局是一个关键 yet often overlooked的问题。这篇论文提出了一种方法来优化路边LiDAR的布局，通过选择场景中最佳位置来提高感知性能。为了高效地获得最佳组合位置，我们提出了一种基于偏好度的贪婪算法，该算法在每次选择新LiDAR位置时，选择能够最大化偏好度的位置。我们定义偏好度为新LiDAR的布局增加的感知能力。为了获得感知能力，我们提出了一种感知预测器，该预测器通过只使用单个点云帧来评估LiDAR布局。为了促进路边LiDAR布局问题的研究，我们创建了名为Roadside-Opt的数据集，该数据集使用CARLA模拟器生成。
</details></li>
</ul>
<hr>
<h2 id="Crowd-Counting-in-Harsh-Weather-using-Image-Denoising-with-Pix2Pix-GANs"><a href="#Crowd-Counting-in-Harsh-Weather-using-Image-Denoising-with-Pix2Pix-GANs" class="headerlink" title="Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs"></a>Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07245">http://arxiv.org/abs/2310.07245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Asif Khan, Hamid Menouar, Ridha Hamila</li>
<li>For: 本研究旨在提高人群计数模型在恶劣天气下的性能，特别是在fog、尘埃和低光照等不良条件下。* Methods: 本研究提出使用Pix2Pix生成器对人群图像进行预处理，以提高计数模型的推理性能。* Results: 研究测试了JHU-Crowd数据集，并证明了提出的方法可以在不良天气下提高人群计数模型的准确率和可靠性。<details>
<summary>Abstract</summary>
Visual crowd counting estimates the density of the crowd using deep learning models such as convolution neural networks (CNNs). The performance of the model heavily relies on the quality of the training data that constitutes crowd images. In harsh weather such as fog, dust, and low light conditions, the inference performance may severely degrade on the noisy and blur images. In this paper, we propose the use of Pix2Pix generative adversarial network (GAN) to first denoise the crowd images prior to passing them to the counting model. A Pix2Pix network is trained using synthetic noisy images generated from original crowd images and then the pretrained generator is then used in the inference engine to estimate the crowd density in unseen, noisy crowd images. The performance is tested on JHU-Crowd dataset to validate the significance of the proposed method particularly when high reliability and accuracy are required.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传感技术可以估算人群的密度使用深度学习模型，如 convolutional neural networks (CNNs)。模型的性能强度取决于训练数据中的人群图像质量。在恶劣天气条件下，如雾、尘埃和低光照条件下，推理性能可能会受到图像噪声和模糊的影响，使得推理结果不准确。在这篇论文中，我们提议使用 Pix2Pix 生成 adversarial network (GAN) 首先将人群图像去噪，然后将已经训练的生成器用于实际推理引擎中，以估算人群密度。我们在 JHU-Crowd 数据集上测试了方法，以验证该方法在需要高可靠性和准确性时的效果。Note: "Pix2Pix" is translated as "生成 adversarial network" (生成对抗网络) in Simplified Chinese, which is a combination of "Pix2Pix" and "adversarial network" (对抗网络).
</details></li>
</ul>
<hr>
<h2 id="SAGE-ICP-Semantic-Information-Assisted-ICP"><a href="#SAGE-ICP-Semantic-Information-Assisted-ICP" class="headerlink" title="SAGE-ICP: Semantic Information-Assisted ICP"></a>SAGE-ICP: Semantic Information-Assisted ICP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07237">http://arxiv.org/abs/2310.07237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaming Cui, Jiming Chen, Liang Li<br>for: 本研究旨在提高遥感器加载 unknown environment中的姿态估计精度和稳定性。methods: 本文提出了一种基于LiDAR的点对点ICP算法，并利用有效的semantic信息。results: 实验表明，相比基eline方法，本方法可以在大规模场景中提高姿态估计精度，并且可以保持实时性。<details>
<summary>Abstract</summary>
Robust and accurate pose estimation in unknown environments is an essential part of robotic applications. We focus on LiDAR-based point-to-point ICP combined with effective semantic information. This paper proposes a novel semantic information-assisted ICP method named SAGE-ICP, which leverages semantics in odometry. The semantic information for the whole scan is timely and efficiently extracted by a 3D convolution network, and these point-wise labels are deeply involved in every part of the registration, including semantic voxel downsampling, data association, adaptive local map, and dynamic vehicle removal. Unlike previous semantic-aided approaches, the proposed method can improve localization accuracy in large-scale scenes even if the semantic information has certain errors. Experimental evaluations on KITTI and KITTI-360 show that our method outperforms the baseline methods, and improves accuracy while maintaining real-time performance, i.e., runs faster than the sensor frame rate.
</details>
<details>
<summary>摘要</summary>
Robust和准确的姿态估计在未知环境中是机器人应用的关键部分。我们关注LiDAR基于点对点ICP的方法，并且使用有效的semantic信息。这篇论文提出了一种基于semantic信息的ICP方法，名为SAGE-ICP，它在odometry中利用semantic信息。整个扫描的semantic信息在时间上是有效的和高效的提取，并且这些点级标签在注册过程中深入参与了每一部分，包括semantic尺度下采样、数据关联、自适应地图和动态车辆除除。与过去的semantic援助方法不同，我们的方法可以在大规模场景中提高姿态精度，即使semantic信息有一定错误。实验评估在KITTI和KITTI-360上显示，我们的方法在精度和实时性之间协调，比基准方法快速，即在感知框架帧率以上运行。
</details></li>
</ul>
<hr>
<h2 id="AdaMesh-Personalized-Facial-Expressions-and-Head-Poses-for-Speech-Driven-3D-Facial-Animation"><a href="#AdaMesh-Personalized-Facial-Expressions-and-Head-Poses-for-Speech-Driven-3D-Facial-Animation" class="headerlink" title="AdaMesh: Personalized Facial Expressions and Head Poses for Speech-Driven 3D Facial Animation"></a>AdaMesh: Personalized Facial Expressions and Head Poses for Speech-Driven 3D Facial Animation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07236">http://arxiv.org/abs/2310.07236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyang Chen, Weihong Bao, Shun Lei, Boshi Tang, Zhiyong Wu, Shiyin Kang, Haozhi Huang</li>
<li>for: 这个论文旨在提出一种基于 speech-driven 3D 人脸动画的个性化方法，以实现与驱动语音同步的自然人脸表达和头pose。</li>
<li>methods: 该方法使用 mixture-of-low-rank adaptation (MoLoRA) 技术来精准地捕捉Reference video中的 talking style，并通过一个具有语义意识的 pose style matrix 来自动调整头pose。</li>
<li>results: 对比于现有方法，该方法能够更好地保持 Reference video 中的 talking style，并生成更加生动的人脸动画。<details>
<summary>Abstract</summary>
Speech-driven 3D facial animation aims at generating facial movements that are synchronized with the driving speech, which has been widely explored recently. Existing works mostly neglect the person-specific talking style in generation, including facial expression and head pose styles. Several works intend to capture the personalities by fine-tuning modules. However, limited training data leads to the lack of vividness. In this work, we propose AdaMesh, a novel adaptive speech-driven facial animation approach, which learns the personalized talking style from a reference video of about 10 seconds and generates vivid facial expressions and head poses. Specifically, we propose mixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter, which efficiently captures the facial expression style. For the personalized pose style, we propose a pose adapter by building a discrete pose prior and retrieving the appropriate style embedding with a semantic-aware pose style matrix without fine-tuning. Extensive experimental results show that our approach outperforms state-of-the-art methods, preserves the talking style in the reference video, and generates vivid facial animation. The supplementary video and code will be available at https://adamesh.github.io.
</details>
<details>
<summary>摘要</summary>
《语音驱动3D面部动画》是一项相对新的领域，目标是在语音驱动下生成同步的面部动作，现有的工作大多忽略了个人特有的说话风格，包括表情和头部姿态风格。一些工作尝试通过细化模块来捕捉个人性格，但由于训练数据的限制，生成的表情和头部姿态具有浓郁的缺乏生命力。在这种情况下，我们提出了 AdaMesh，一种新的适应语音驱动面部动画方法，可以从约10秒的参考视频中学习个人化说话风格，并生成有生命力的表情和头部姿态。具体来说，我们提出了一种混合低级 adaptation（MoLoRA）方法，用于精细地捕捉表情风格。为个性化姿态风格，我们提出了一种姿态适应器，通过建立精确的姿态前providing和使用语义意识 pose style matrix 来无需 Fine-tuning retrieve相应的风格嵌入。EXTensive experimental results show that our approach outperforms state-of-the-art methods, preserves the talking style in the reference video, and generates vivid facial animation. Supplementary video and code will be available at https://adamesh.github.io.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-blind-spectral-unmixing-of-LULC-classes-with-MODIS-multispectral-time-series-and-ancillary-data"><a href="#Deep-Learning-for-blind-spectral-unmixing-of-LULC-classes-with-MODIS-multispectral-time-series-and-ancillary-data" class="headerlink" title="Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data"></a>Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07223">http://arxiv.org/abs/2310.07223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jrodriguezortega/msmtu">https://github.com/jrodriguezortega/msmtu</a></li>
<li>paper_authors: José Rodríguez-Ortega, Rohaifa Khaldi, Domingo Alcaraz-Segura, Siham Tabik</li>
<li>for: 这个论文的目的是提出一种基于深度学习模型的多spectral时间序列数据中的纯化方法，用于提取杂合的土地用途和地形特征信息。</li>
<li>methods: 该方法使用了深度学习模型，包括长短期记忆网络（LSTM）模型，将 spectral-temporal 输入数据与地ографи和气候参数结合使用，以提高杂合像素中的物类含量估计。</li>
<li>results: 实验表明，结合spectral-temporal输入数据与地ографи和气候参数可以substantially improve the abundance estimation of LULC classes in mixed pixels。<details>
<summary>Abstract</summary>
Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC) types. Spectral unmixing is a technique to extract information from mixed pixels into their constituent LULC types and corresponding abundance fractions. Traditionally, solving this task has relied on either classical methods that require prior knowledge of endmembers or machine learning methods that avoid explicit endmembers calculation, also known as blind spectral unmixing (BSU). Most BSU studies based on Deep Learning (DL) focus on one time-step hyperspectral data, yet its acquisition remains quite costly compared with multispectral data. To our knowledge, here we provide the first study on BSU of LULC classes using multispectral time series data with DL models. We further boost the performance of a Long-Short Term Memory (LSTM)-based model by incorporating geographic plus topographic (geo-topographic) and climatic ancillary information. Our experiments show that combining spectral-temporal input data together with geo-topographic and climatic information substantially improves the abundance estimation of LULC classes in mixed pixels. To carry out this study, we built a new labeled dataset of the region of Andalusia (Spain) with monthly multispectral time series of pixels for the year 2013 from MODIS at 460m resolution, for two hierarchical levels of LULC classes, named Andalusia MultiSpectral MultiTemporal Unmixing (Andalusia-MSMTU). This dataset provides, at the pixel level, a multispectral time series plus ancillary information annotated with the abundance of each LULC class inside each pixel. The dataset and code are available to the public.
</details>
<details>
<summary>摘要</summary>
Remotely sensed data受混合土地用途和土地覆盖(LULC)类型的控制。spectral unmixing是一种技术，以提取混合像素中的各种LULC类型和相应的含量分数。传统上，解决这个任务通常需要知识先驱练或机器学习方法，而不需要显式计算终端成员。大多数无知终端混合(BSU)研究基于深度学习(DL)方法，但这些研究通常只关注单个时间步的干涉光谱数据。在我们所知道的情况下，我们在这里提供了首个BSU的LULC类型使用多spectral时间序数据与DL模型的研究。我们还使用地理加topographic(geo-topographic)和气候附加信息来提高LSTM模型的性能。我们的实验表明，将spectral-temporal输入数据与geo-topographic和气候信息结合在一起可以显著提高混合像素中LULC类型的含量估计。为了进行这项研究，我们建立了一个新的标注集，名为Andalusia MultiSpectral MultiTemporal Unmixing(Andalusia-MSMTU)。这个集包括2013年MODIS在460米分辨率上每月多spectral时间序数据，以及两个层次的LULC类型。每个像素级别具有多spectral时间序数据和附加信息，并标注每个像素中每种LULC类型的含量。这个集和代码现在对公众开放。
</details></li>
</ul>
<hr>
<h2 id="Uni-paint-A-Unified-Framework-for-Multimodal-Image-Inpainting-with-Pretrained-Diffusion-Model"><a href="#Uni-paint-A-Unified-Framework-for-Multimodal-Image-Inpainting-with-Pretrained-Diffusion-Model" class="headerlink" title="Uni-paint: A Unified Framework for Multimodal Image Inpainting with Pretrained Diffusion Model"></a>Uni-paint: A Unified Framework for Multimodal Image Inpainting with Pretrained Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07222">http://arxiv.org/abs/2310.07222</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ysy31415/unipaint">https://github.com/ysy31415/unipaint</a></li>
<li>paper_authors: Shiyuan Yang, Xiaodong Chen, Jing Liao</li>
<li>for: 这篇论文的目的是提出一种多modal填充方法，以提供多种导向方式，包括无条件、文本驱动、橡皮擦驱动和示例驱动填充，以及这些模式的组合。</li>
<li>methods: 该方法基于预训练的稳定扩散模型（Stable Diffusion），不需要特定任务的训练，可以在少量数据下实现一元化。</li>
<li>results: 论文通过对多种数据集进行广泛的质量和量化评估，表明其方法可以与单modal方法匹配的效果，同时具有多modal填充的能力。<details>
<summary>Abstract</summary>
Recently, text-to-image denoising diffusion probabilistic models (DDPMs) have demonstrated impressive image generation capabilities and have also been successfully applied to image inpainting. However, in practice, users often require more control over the inpainting process beyond textual guidance, especially when they want to composite objects with customized appearance, color, shape, and layout. Unfortunately, existing diffusion-based inpainting methods are limited to single-modal guidance and require task-specific training, hindering their cross-modal scalability. To address these limitations, we propose Uni-paint, a unified framework for multimodal inpainting that offers various modes of guidance, including unconditional, text-driven, stroke-driven, exemplar-driven inpainting, as well as a combination of these modes. Furthermore, our Uni-paint is based on pretrained Stable Diffusion and does not require task-specific training on specific datasets, enabling few-shot generalizability to customized images. We have conducted extensive qualitative and quantitative evaluations that show our approach achieves comparable results to existing single-modal methods while offering multimodal inpainting capabilities not available in other methods. Code will be available at https://github.com/ysy31415/unipaint.
</details>
<details>
<summary>摘要</summary>
最近，文本到图像去噪扩散概率模型（DDPM）已经表现出了惊人的图像生成能力，并且在图像填充方面也有成功应用。然而，在实践中，用户经常需要更多的控制力在填充过程中，特别是当他们想要搭配自定义外观、颜色、形状和布局时。可惜，现有的扩散基于的填充方法都受到单modal导航的限制，需要任务特定的训练，这会阻碍其跨modal扩展性。为了解决这些限制，我们提出了Uni-paint，一个多modal填充框架，它提供了多种导航模式，包括随机、文本驱动、笔划驱动、示例驱动填充，以及这些模式的组合。此外，我们的Uni-paint基于预训练的稳定扩散，不需要任务特定的训练，可以在特定图像上进行几步扩展，实现了自定义图像的填充。我们进行了广泛的质量和量测试，结果表明，我们的方法可以与单modal方法匹配，同时提供了多modal填充的能力，不在其他方法中可以实现。代码将在https://github.com/ysy31415/unipaint上公开。
</details></li>
</ul>
<hr>
<h2 id="Multi-task-Explainable-Skin-Lesion-Classification"><a href="#Multi-task-Explainable-Skin-Lesion-Classification" class="headerlink" title="Multi-task Explainable Skin Lesion Classification"></a>Multi-task Explainable Skin Lesion Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07209">http://arxiv.org/abs/2310.07209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahapara Khurshid, Mayank Vatsa, Richa Singh</li>
<li>for: 这篇论文旨在提出一种基于几个扩展练习的多任务几据学习方法，以帮助早期识别皮肤癌。</li>
<li>methods: 本文提出了一种多任务几据学习方法，包括一个统一维度的条件统计学习（Segmentation Network）和一个分类维度的条件统计学习（Classification Network），并将它们融合为一个整体的条件统计学习系统。</li>
<li>results: 实验结果显示，提出的方法可以对皮肤癌进行早期识别，并且在不同的数据集上具有很好的一致性和稳定性。<details>
<summary>Abstract</summary>
Skin cancer is one of the deadliest diseases and has a high mortality rate if left untreated. The diagnosis generally starts with visual screening and is followed by a biopsy or histopathological examination. Early detection can aid in lowering mortality rates. Visual screening can be limited by the experience of the doctor. Due to the long tail distribution of dermatological datasets and significant intra-variability between classes, automatic classification utilizing computer-aided methods becomes challenging. In this work, we propose a multitask few-shot-based approach for skin lesions that generalizes well with few labelled data to address the small sample space challenge. The proposed approach comprises a fusion of a segmentation network that acts as an attention module and classification network. The output of the segmentation network helps to focus on the most discriminatory features while making a decision by the classification network. To further enhance the classification performance, we have combined segmentation and classification loss in a weighted manner. We have also included the visualization results that explain the decisions made by the algorithm. Three dermatological datasets are used to evaluate the proposed method thoroughly. We also conducted cross-database experiments to ensure that the proposed approach is generalizable across similar datasets. Experimental results demonstrate the efficacy of the proposed work.
</details>
<details>
<summary>摘要</summary>
皮肤癌是一种非常危险的疾病，如果不得到治疗，则可能会有很高的死亡率。诊断通常由视觉检查开始，然后是比采或历史检查。早期发现可以降低死亡率。然而，视觉检查可能受医生的经验限制。由于皮肤癌数据集的长尾分布和类型之间的显著内部变化，使用计算机助手的自动分类变得困难。在这种情况下，我们提出了一种多任务几 shot 基于的方法，用于处理皮肤癌的小样本空间问题。我们的方法包括一个 fusion 的 segmentation 网络，它作为注意模块，并一个分类网络。分类网络输出的结果可以帮助注意模块决策，同时，注意模块可以帮助分类网络做出更加准确的决策。为了进一步提高分类性能，我们将 segmentation 和分类损失权重加以权重，并包括了可视化结果，以便更好地解释算法的决策。我们使用了三个皮肤癌数据集进行了全面的评估。我们还进行了跨数据集的试验，以确保我们的方法是可重复性的。实验结果表明，我们的方法具有效果。
</details></li>
</ul>
<hr>
<h2 id="DeepSimHO-Stable-Pose-Estimation-for-Hand-Object-Interaction-via-Physics-Simulation"><a href="#DeepSimHO-Stable-Pose-Estimation-for-Hand-Object-Interaction-via-Physics-Simulation" class="headerlink" title="DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation"></a>DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07206">http://arxiv.org/abs/2310.07206</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rongakowang/DeepSimHO">https://github.com/rongakowang/DeepSimHO</a></li>
<li>paper_authors: Rong Wang, Wei Mao, Hongdong Li</li>
<li>for: 这个研究目的是从单一图像观察中进行3D姿势估测，并且处理手与物体之间的互动。</li>
<li>methods: 这个研究使用了深度学习架构，其中包括前向物理 simulations 和反向梯度推断。</li>
<li>results: 实验结果显示，这个方法可以提高估测的稳定性，并且比测时优化更高效。<details>
<summary>Abstract</summary>
This paper addresses the task of 3D pose estimation for a hand interacting with an object from a single image observation. When modeling hand-object interaction, previous works mainly exploit proximity cues, while overlooking the dynamical nature that the hand must stably grasp the object to counteract gravity and thus preventing the object from slipping or falling. These works fail to leverage dynamical constraints in the estimation and consequently often produce unstable results. Meanwhile, refining unstable configurations with physics-based reasoning remains challenging, both by the complexity of contact dynamics and by the lack of effective and efficient physics inference in the data-driven learning framework. To address both issues, we present DeepSimHO: a novel deep-learning pipeline that combines forward physics simulation and backward gradient approximation with a neural network. Specifically, for an initial hand-object pose estimated by a base network, we forward it to a physics simulator to evaluate its stability. However, due to non-smooth contact geometry and penetration, existing differentiable simulators can not provide reliable state gradient. To remedy this, we further introduce a deep network to learn the stability evaluation process from the simulator, while smoothly approximating its gradient and thus enabling effective back-propagation. Extensive experiments show that our method noticeably improves the stability of the estimation and achieves superior efficiency over test-time optimization. The code is available at https://github.com/rongakowang/DeepSimHO.
</details>
<details>
<summary>摘要</summary>
To address these issues, we propose DeepSimHO, a novel deep-learning pipeline that combines forward physics simulation and backward gradient approximation with a neural network. Given an initial hand-object pose estimated by a base network, we forward it to a physics simulator to evaluate its stability. However, existing differentiable simulators cannot provide reliable state gradients due to non-smooth contact geometry and penetration. To address this, we introduce a deep network to learn the stability evaluation process from the simulator, while smoothly approximating its gradient and enabling effective back-propagation.Our method significantly improves the stability of the estimation and achieves superior efficiency over test-time optimization. The code is available at https://github.com/rongakowang/DeepSimHO.
</details></li>
</ul>
<hr>
<h2 id="SpikePoint-An-Efficient-Point-based-Spiking-Neural-Network-for-Event-Cameras-Action-Recognition"><a href="#SpikePoint-An-Efficient-Point-based-Spiking-Neural-Network-for-Event-Cameras-Action-Recognition" class="headerlink" title="SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition"></a>SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07189">http://arxiv.org/abs/2310.07189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongwei Ren, Yue Zhou, Yulong Huang, Haotian Fu, Xiaopeng Lin, Jie Song, Bojun Cheng</li>
<li>for: 本研究旨在开发一种能够实现低功耗和高精度的事件摄像头应用场景，通过将事件摄像头与脉冲神经网络（SNN）相结合。</li>
<li>methods: 本研究提出了一种名为SpikePoint的新的端到端点 wise SNN架构，可以高效处理 sparse event cloud 数据，并提取全局和局部特征。</li>
<li>results: 对于四个事件基因 recognize 数据集，SpikePoint 达到了状态机器人（SOTA）性能，只需要使用 16 个时间步骤，超过了其他 SNN 方法。此外，它还在三个数据集上达到了 SOTA 性能，使用了约 0.3% 的参数和 0.5% 的能耗，相比较于人工神经网络（ANNs）的参数和能耗。这些结果证明 Point Cloud 的重要性，并开启了许多低功耗事件基因数据处理应用场景。<details>
<summary>Abstract</summary>
Event cameras are bio-inspired sensors that respond to local changes in light intensity and feature low latency, high energy efficiency, and high dynamic range. Meanwhile, Spiking Neural Networks (SNNs) have gained significant attention due to their remarkable efficiency and fault tolerance. By synergistically harnessing the energy efficiency inherent in event cameras and the spike-based processing capabilities of SNNs, their integration could enable ultra-low-power application scenarios, such as action recognition tasks. However, existing approaches often entail converting asynchronous events into conventional frames, leading to additional data mapping efforts and a loss of sparsity, contradicting the design concept of SNNs and event cameras. To address this challenge, we propose SpikePoint, a novel end-to-end point-based SNN architecture. SpikePoint excels at processing sparse event cloud data, effectively extracting both global and local features through a singular-stage structure. Leveraging the surrogate training method, SpikePoint achieves high accuracy with few parameters and maintains low power consumption, specifically employing the identity mapping feature extractor on diverse datasets. SpikePoint achieves state-of-the-art (SOTA) performance on four event-based action recognition datasets using only 16 timesteps, surpassing other SNN methods. Moreover, it also achieves SOTA performance across all methods on three datasets, utilizing approximately 0.3\% of the parameters and 0.5\% of power consumption employed by artificial neural networks (ANNs). These results emphasize the significance of Point Cloud and pave the way for many ultra-low-power event-based data processing applications.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:事件摄像机是生物体外的感知器，响应当地光度变化，具有低延迟、高能效率和高 dinamic范围。同时，使得神经网络（SNN）在处理数据时具有很好的性能和容错性。通过将事件摄像机的能效性与 SNN 的射频处理机制相结合，可以实现低功耗应用场景，如行动认知任务。然而，现有的方法通常会将异步事件转换成常规帧，从而需要额外的数据映射努力，并且会导致数据精度下降，与 SNN 和事件摄像机的设计原则相抵触。为解决这个挑战，我们提出了 PointCloud，一种新的端到端点基 SNN 架构。PointCloud  excells 在处理稀疏事件云数据，通过单个阶段结构，提取全局和局部特征。通过使用代理训练方法，PointCloud 可以在多个数据集上实现高精度，并且具有低功耗。在四个事件基于动作认知数据集上，PointCloud 使用 16 个时间步骤，已经超越了其他 SNN 方法。此外，PointCloud 还在三个数据集上实现了全方法的最佳性能，使用了约 0.3% 的参数和 0.5% 的电力耗用，相比于人工神经网络 (ANN) 的参数和电力耗用。这些结果强调了 PointCloud 的重要性，并开创了许多低功耗事件基数据处理应用场景。
</details></li>
</ul>
<hr>
<h2 id="NeuroInspect-Interpretable-Neuron-based-Debugging-Framework-through-Class-conditional-Visualizations"><a href="#NeuroInspect-Interpretable-Neuron-based-Debugging-Framework-through-Class-conditional-Visualizations" class="headerlink" title="NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations"></a>NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07184">http://arxiv.org/abs/2310.07184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yeongjoonju/neuroinspect">https://github.com/yeongjoonju/neuroinspect</a></li>
<li>paper_authors: Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee</li>
<li>for: This paper aims to provide an interpretable neuron-based debugging framework for deep learning (DL) models, to help DL practitioners understand and fix mistakes made by the models.</li>
<li>methods: The paper proposes a three-stage debugging framework called NeuroInspect, which includes counterfactual explanations, feature visualizations, and false correlation mitigation. The framework uses a novel feature visualization method called CLIP-Illusion to generate human-interpretable explanations for model errors.</li>
<li>results: The paper demonstrates the effectiveness of NeuroInspect in addressing false correlations and improving inferences for classes with the worst performance in real-world settings. The results show that NeuroInspect helps debug the mistakes of DL models and improves human understanding of the decision-making process within the networks.<details>
<summary>Abstract</summary>
Despite deep learning (DL) has achieved remarkable progress in various domains, the DL models are still prone to making mistakes. This issue necessitates effective debugging tools for DL practitioners to interpret the decision-making process within the networks. However, existing debugging methods often demand extra data or adjustments to the decision process, limiting their applicability. To tackle this problem, we present NeuroInspect, an interpretable neuron-based debugging framework with three key stages: counterfactual explanations, feature visualizations, and false correlation mitigation. Our debugging framework first pinpoints neurons responsible for mistakes in the network and then visualizes features embedded in the neurons to be human-interpretable. To provide these explanations, we introduce CLIP-Illusion, a novel feature visualization method that generates images representing features conditioned on classes to examine the connection between neurons and the decision layer. We alleviate convoluted explanations of the conventional visualization approach by employing class information, thereby isolating mixed properties. This process offers more human-interpretable explanations for model errors without altering the trained network or requiring additional data. Furthermore, our framework mitigates false correlations learned from a dataset under a stochastic perspective, modifying decisions for the neurons considered as the main causes. We validate the effectiveness of our framework by addressing false correlations and improving inferences for classes with the worst performance in real-world settings. Moreover, we demonstrate that NeuroInspect helps debug the mistakes of DL models through evaluation for human understanding. The code is openly available at https://github.com/yeongjoonJu/NeuroInspect.
</details>
<details>
<summary>摘要</summary>
尽管深度学习（DL）已经取得了各种领域的显著进步，但DL模型仍然容易出错。这个问题需要有效的调试工具，以便DL实践者可以解释网络的决策过程。然而，现有的调试方法经常需要额外的数据或调整决策过程，限制其应用。为解决这个问题，我们提出了NeuroInspect，一个可解释的 neuron-based 调试框架。我们的调试框架首先在网络中标识出负责出错的 neuron，然后使用CLIP-Illusion，一种新的特征视图方法，生成表示特征类别的图像，以便人类可以理解。我们通过使用类信息，缩小混杂的解释，从而提供更人类可理解的错误解释，而无需更改已训练的网络或需要额外的数据。此外，我们的框架还解决了基于数据的false correlation问题，通过修改考虑到的neuron的决策，从而提高网络的准确率。我们验证了NeuroInspect的效果，通过对实际场景中的false correlation和各类错误进行修复，提高了网络的推理能力。此外，我们还证明了NeuroInspect可以帮助调试DL模型的错误。代码可以在https://github.com/yeongjoonJu/NeuroInspect中下载。
</details></li>
</ul>
<hr>
<h2 id="Improving-mitosis-detection-on-histopathology-images-using-large-vision-language-models"><a href="#Improving-mitosis-detection-on-histopathology-images-using-large-vision-language-models" class="headerlink" title="Improving mitosis detection on histopathology images using large vision-language models"></a>Improving mitosis detection on histopathology images using large vision-language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07176">http://arxiv.org/abs/2310.07176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiwen Ding, James Hall, Neil Tenenholtz, Kristen Severson</li>
<li>for: 这个论文的目的是提高悉尼肿瘤检测的准确率，使用混沌神经网络和自然语言进行辅助。</li>
<li>methods: 这个论文使用了混沌神经网络，并利用了图像描述和自然语言来提高悉尼肿瘤检测的准确率。</li>
<li>results: 研究表明，使用这种方法可以提高悉尼肿瘤检测的准确率，并且比较于各种基线模型。<details>
<summary>Abstract</summary>
In certain types of cancerous tissue, mitotic count has been shown to be associated with tumor proliferation, poor prognosis, and therapeutic resistance. Due to the high inter-rater variability of mitotic counting by pathologists, convolutional neural networks (CNNs) have been employed to reduce the subjectivity of mitosis detection in hematoxylin and eosin (H&E)-stained whole slide images. However, most existing models have performance that lags behind expert panel review and only incorporate visual information. In this work, we demonstrate that pre-trained large-scale vision-language models that leverage both visual features and natural language improve mitosis detection accuracy. We formulate the mitosis detection task as an image captioning task and a visual question answering (VQA) task by including metadata such as tumor and scanner types as context. The effectiveness of our pipeline is demonstrated via comparison with various baseline models using 9,501 mitotic figures and 11,051 hard negatives (non-mitotic figures that are difficult to characterize) from the publicly available Mitosis Domain Generalization Challenge (MIDOG22) dataset.
</details>
<details>
<summary>摘要</summary>
某些癌细胞组织中， Mitotic 数量与肿瘤增殖、较差的预后和治疗耐荷强相关。由于病理医生在 Mitotic 计数中存在高度的交互变性，因此人工神经网络（CNNs）已被应用以减少病理分析中的主观性。然而，大多数现有模型只使用视觉信息，其性能落后于专家审查anel。在这项工作中，我们示出了使用预训练的大规模视力语言模型，可以提高 Mitosis 检测精度。我们将 Mitosis 检测任务定义为一个图像描述任务和一个视觉问答（VQA）任务，并通过包括肿瘤类型和扫描仪类型等metadata来提供背景信息。我们的管道的效果通过与多种基准模型进行比较，使用 MIDOG22 公共可用的 Mitosis 领域泛化挑战 dataset 中的 9,501 个 Mitotic 图像和 11,051 个困难的非 Mitotic 图像进行证明。
</details></li>
</ul>
<hr>
<h2 id="Anchor-based-Multi-view-Subspace-Clustering-with-Hierarchical-Feature-Descent"><a href="#Anchor-based-Multi-view-Subspace-Clustering-with-Hierarchical-Feature-Descent" class="headerlink" title="Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent"></a>Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07166">http://arxiv.org/abs/2310.07166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiyuan Ou, Siwei Wang, Pei Zhang, Sihang Zhou, En Zhu</li>
<li>for: 这个论文主要目标是提出一种基于多视图的含义下降 clustering 算法，以解决现有多视图 clustering 算法的时间复杂度问题。</li>
<li>methods: 该论文使用了一种基于层次特征下降的方法，通过在不同视图之间建立相互关系来实现视图之间的数据拟合。然后，通过一种统一采样策略在含义下降空间中进行采样，并使用子空间 clustering 算法来学习共同表示。</li>
<li>results: 实验结果表明，提出的 MVSC-HFD 模型在公共评估数据集上经常超越当前状态艺技。<details>
<summary>Abstract</summary>
Multi-view clustering has attracted growing attention owing to its capabilities of aggregating information from various sources and its promising horizons in public affairs. Up till now, many advanced approaches have been proposed in recent literature. However, there are several ongoing difficulties to be tackled. One common dilemma occurs while attempting to align the features of different views. We dig out as well as deploy the dependency amongst views through hierarchical feature descent, which leads to a common latent space( STAGE 1). This latent space, for the first time of its kind, is regarded as a 'resemblance space', as it reveals certain correlations and dependencies of different views. To be exact, the one-hot encoding of a category can also be referred to as a resemblance space in its terminal phase. Moreover, due to the intrinsic fact that most of the existing multi-view clustering algorithms stem from k-means clustering and spectral clustering, this results in cubic time complexity w.r.t. the number of the objects. However, we propose Anchor-based Multi-view Subspace Clustering with Hierarchical Feature Descent(MVSC-HFD) to further reduce the computing complexity to linear time cost through a unified sampling strategy in resemblance space( STAGE 2), followed by subspace clustering to learn the representation collectively( STAGE 3). Extensive experimental results on public benchmark datasets demonstrate that our proposed model consistently outperforms the state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
多视图聚类在最近几年来得到了越来越多的关注，这是因为它可以聚合来自不同源泉的信息，并且在公共事务中具有承诺的前途。到目前为止，文献中已经提出了许多高级方法。然而，还有许多在进行的困难，其中一个最常见的困难是对不同视图之间的特征进行Alignment。我们通过层次特征降低来解决这个问题，并在多视图聚类中提出了一个新的Latent space（Stage 1）。这个Latent space被认为是一种'相似空间'，因为它揭示了不同视图之间的相似性和依赖关系。此外，由于大多数现有的多视图聚类算法来自k-means clustering和spectral clustering，这会导致 cubic time complexity 对于对象的数量。然而，我们提出了基于 anchor的多视图子空间聚类 Algorithm with Hierarchical Feature Descent(MVSC-HFD)，可以在resemblance space中进行统一采样策略（Stage 2），然后使用子空间聚类来学习表示（Stage 3）。我们在公共测试数据集上进行了广泛的实验，结果显示，我们提出的模型在与现有技术相比 consistently outperform。
</details></li>
</ul>
<hr>
<h2 id="Robust-Unsupervised-Domain-Adaptation-by-Retaining-Confident-Entropy-via-Edge-Concatenation"><a href="#Robust-Unsupervised-Domain-Adaptation-by-Retaining-Confident-Entropy-via-Edge-Concatenation" class="headerlink" title="Robust Unsupervised Domain Adaptation by Retaining Confident Entropy via Edge Concatenation"></a>Robust Unsupervised Domain Adaptation by Retaining Confident Entropy via Edge Concatenation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07149">http://arxiv.org/abs/2310.07149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hye-Seong Hong, Abhishek Kumar, Dong-Gyu Lee</li>
<li>for: 提高不监督领域适应的Semantic segmentation网络训练效果，使用计算机生成的标注数据作为源数据。</li>
<li>methods: 利用内部和外部信息的共同作用，在Entropy-based adversarial networks中增强预测源领域的能力。增加权重 Edge-predicted probability values，以提高分类边界的清晰度。设计了一种概率分享网络，将多种信息集成到更有效的分类中。</li>
<li>results: 在不监督领域适应 benchmark上进行了严格的评估，包括 SYNTHIA $\rightarrow$ Cityscapes和 SYNTHIA $\rightarrow$ Mapillary。实验结果表明，提出的方法在不同的无监督领域适应场景中具有优于当前方法的性能。<details>
<summary>Abstract</summary>
The generalization capability of unsupervised domain adaptation can mitigate the need for extensive pixel-level annotations to train semantic segmentation networks by training models on synthetic data as a source with computer-generated annotations. Entropy-based adversarial networks are proposed to improve source domain prediction; however, they disregard significant external information, such as edges, which have the potential to identify and distinguish various objects within an image accurately. To address this issue, we introduce a novel approach to domain adaptation, leveraging the synergy of internal and external information within entropy-based adversarial networks. In this approach, we enrich the discriminator network with edge-predicted probability values within this innovative framework to enhance the clarity of class boundaries. Furthermore, we devised a probability-sharing network that integrates diverse information for more effective segmentation. Incorporating object edges addresses a pivotal aspect of unsupervised domain adaptation that has frequently been neglected in the past -- the precise delineation of object boundaries. Conventional unsupervised domain adaptation methods usually center around aligning feature distributions and may not explicitly model object boundaries. Our approach effectively bridges this gap by offering clear guidance on object boundaries, thereby elevating the quality of domain adaptation. Our approach undergoes rigorous evaluation on the established unsupervised domain adaptation benchmarks, specifically in adapting SYNTHIA $\rightarrow$ Cityscapes and SYNTHIA $\rightarrow$ Mapillary. Experimental results show that the proposed model attains better performance than state-of-the-art methods. The superior performance across different unsupervised domain adaptation scenarios highlights the versatility and robustness of the proposed method.
</details>
<details>
<summary>摘要</summary>
通过不监督领域适应，可以减轻 semantic segmentation 网络训练所需的广泛像素级注释。通过训练模型使用生成的数据作为源，并使用计算机生成的注释。不过，基于Entropy的反对抗网络可能会忽略一些重要的外部信息，如图像中的边缘，这些信息可能能够准确地识别和分类不同的对象。为解决这个问题，我们提出了一种新的适应领域方法，具有内部和外部信息的共同作用。在这种方法中，我们增强了推定网络的边缘预测概率值，以提高类划界的清晰度。此外，我们设计了一种概率共享网络，以更有效地进行分类。在这种方法中，我们利用了对象边缘的信息，解决了过去常见的适应领域方法不具备的对象边界的精确分类问题。我们的方法在已知的适应领域 benchmark 上进行了严格的评估，包括 SYNTHIA $\rightarrow$ Cityscapes 和 SYNTHIA $\rightarrow$ Mapillary 等。实验结果表明，我们的方法在不同的适应领域场景中表现出色，超过了现有的state-of-the-art方法。这些结果表明了我们的方法的多样性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Echocardiography-video-synthesis-from-end-diastolic-semantic-map-via-diffusion-model"><a href="#Echocardiography-video-synthesis-from-end-diastolic-semantic-map-via-diffusion-model" class="headerlink" title="Echocardiography video synthesis from end diastolic semantic map via diffusion model"></a>Echocardiography video synthesis from end diastolic semantic map via diffusion model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07131">http://arxiv.org/abs/2310.07131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phi Nguyen Van, Duc Tran Minh, Hieu Pham Huy, Long Tran Quoc</li>
<li>for: 这篇论文的目的是为echocardiography视频生成任务提供一种新的方法，使用semantic映射来指导生成过程，以提高生成的视频的真实感和一致性。</li>
<li>methods: 这篇论文使用了Denoising Diffusion Probabilistic Models (DDPMs)，并在其基础上进行了扩展和改进，以适应echocardiography视频生成任务。具体来说，我们使用了semantic映射来指导生成过程，并在多尺度特征图中进行了空间适应 нормализа。</li>
<li>results: 经过实验，我们发现OUR模型在CAMUS数据集上的表现比标准扩散技术更好，包括多个纪录指标，如FID、FVD和SSMI。这表明OUR模型可以更好地生成echocardiography视频序列，具有更高的真实感和一致性。<details>
<summary>Abstract</summary>
Denoising Diffusion Probabilistic Models (DDPMs) have demonstrated significant achievements in various image and video generation tasks, including the domain of medical imaging. However, generating echocardiography videos based on semantic anatomical information remains an unexplored area of research. This is mostly due to the constraints imposed by the currently available datasets, which lack sufficient scale and comprehensive frame-wise annotations for every cardiac cycle. This paper aims to tackle the aforementioned challenges by expanding upon existing video diffusion models for the purpose of cardiac video synthesis. More specifically, our focus lies in generating video using semantic maps of the initial frame during the cardiac cycle, commonly referred to as end diastole. To further improve the synthesis process, we integrate spatial adaptive normalization into multiscale feature maps. This enables the inclusion of semantic guidance during synthesis, resulting in enhanced realism and coherence of the resultant video sequences. Experiments are conducted on the CAMUS dataset, which is a highly used dataset in the field of echocardiography. Our model exhibits better performance compared to the standard diffusion technique in terms of multiple metrics, including FID, FVD, and SSMI.
</details>
<details>
<summary>摘要</summary>
德 numerically 噪声扩散模型（DDPM）在不同的图像和视频生成任务中表现出了显著的成就，其中包括医学影像领域。然而，基于semantic anatomical information的echocardiography视频生成仍然是一个未经探索的领域，这主要是因为当前available的数据集缺乏 sufficient scale和每个cardiac cycle的完整框架级别的注释。本文旨在解决上述挑战，扩展现有的视频扩散模型用于卡ди亚视频生成。更 specifically，我们的重点是使用initial frame during cardiac cycle的semantic maps来生成视频。为了进一步改进生成过程，我们将spatial adaptive normalization integrate into multiscale feature maps，以使用semantic guidance during synthesis，从而提高生成的视频序列的真实性和一致性。我们在CAMUS dataset上进行了实验，这是医学影像领域中的一个非常流行的数据集。我们的模型在多个纪录metric上表现出了更好的性能，包括FID、FVD和SSMI。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/cs.CV_2023_10_11/" data-id="clorjzl7400jqf1884fsr1dk7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/11/cs.AI_2023_10_11/" class="article-date">
  <time datetime="2023-10-11T12:00:00.000Z" itemprop="datePublished">2023-10-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/11/cs.AI_2023_10_11/">cs.AI - 2023-10-11</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AutoRepo-A-general-framework-for-multi-modal-LLM-based-automated-construction-reporting"><a href="#AutoRepo-A-general-framework-for-multi-modal-LLM-based-automated-construction-reporting" class="headerlink" title="AutoRepo: A general framework for multi-modal LLM-based automated construction reporting"></a>AutoRepo: A general framework for multi-modal LLM-based automated construction reporting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07944">http://arxiv.org/abs/2310.07944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongxu Pu, Xincong Yang, Jing Li, Runhao Guo, Heng Li</li>
<li>for: 提高建筑项目安全、质量和时间完成性，使用自动生成建筑检查报告的新框架AutoRepo。</li>
<li>methods: 利用无人车进行建筑检查，收集场景信息，并使用多模态大语言模型生成检查报告。</li>
<li>results: 在实际建筑项目中应用并测试了AutoRepo框架，显示它可以减少检查过程的时间和资源配置，并生成符合法规标准的高质量检查报告。<details>
<summary>Abstract</summary>
Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.
</details>
<details>
<summary>摘要</summary>
Ensuring the safety, quality, and timely completion of construction projects is crucial, with construction inspections serving as a vital tool towards these goals. However, the predominantly manual approach of present-day inspections frequently leads to inefficiencies and inadequate information management. Such methods often fall short of providing comprehensive, exhaustive assessments, resulting in regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Co-NavGPT-Multi-Robot-Cooperative-Visual-Semantic-Navigation-using-Large-Language-Models"><a href="#Co-NavGPT-Multi-Robot-Cooperative-Visual-Semantic-Navigation-using-Large-Language-Models" class="headerlink" title="Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models"></a>Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07937">http://arxiv.org/abs/2310.07937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bangguo Yu, Hamidreza Kasaei, Ming Cao</li>
<li>for: 这篇论文旨在解决多机器人合作时的可视目标导航问题，以实现高效率和可靠性。</li>
<li>methods: 本文提出了一个创新的框架，即Co-NavGPT，它使用大型自然语言模型（LLM）作为多机器人合作的全球观察者，将环境资料转换为提示，进而提高 LLM 的景象理解能力。</li>
<li>results: 实验结果显示，Co-NavGPT 在 HM3D 环境中的成功率和效率都高于现有模型，而且不需要任何学习过程，这表明 LLM 在多机器人合作领域的应用潜力非常大。<details>
<summary>Abstract</summary>
In advanced human-robot interaction tasks, visual target navigation is crucial for autonomous robots navigating unknown environments. While numerous approaches have been developed in the past, most are designed for single-robot operations, which often suffer from reduced efficiency and robustness due to environmental complexities. Furthermore, learning policies for multi-robot collaboration are resource-intensive. To address these challenges, we propose Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs) as a global planner for multi-robot cooperative visual target navigation. Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs' scene comprehension. It then assigns exploration frontiers to each robot for efficient target search. Experimental results on Habitat-Matterport 3D (HM3D) demonstrate that Co-NavGPT surpasses existing models in success rates and efficiency without any learning process, demonstrating the vast potential of LLMs in multi-robot collaboration domains. The supplementary video, prompts, and code can be accessed via the following link: \href{https://sites.google.com/view/co-navgpt}{https://sites.google.com/view/co-navgpt}.
</details>
<details>
<summary>摘要</summary>
在高级人机交互任务中，视觉目标导航是关键，以便自主机器人在未知环境中进行自主导航。过去有许多方法被开发出来，但大多数是单机器人操作的，它们往往因环境复杂性而减少效率和可靠性。此外，学习策略 для多机器人合作也是费时费力的。为解决这些挑战，我们提出了Co-NavGPT框架，它将大型自然语言模型（LLM）作为多机器人合作的全球规划器。Co-NavGPT将探索环境数据编码成提示，从而提高 LLM 的景象理解能力。然后，它将每个机器人分配出探索前沿，以实现高效的目标搜索。在Habitat-Matterport 3D（HM3D）上进行的实验结果表明，Co-NavGPT比既有模型在成功率和效率方面具有更高的潜力，而且无需进行学习过程，这表明 LLM 在多机器人合作领域的潜力是非常大。补充视频、提示和代码可以通过以下链接获取：\href{https://sites.google.com/view/co-navgpt}{https://sites.google.com/view/co-navgpt}.
</details></li>
</ul>
<hr>
<h2 id="What-Matters-to-You-Towards-Visual-Representation-Alignment-for-Robot-Learning"><a href="#What-Matters-to-You-Towards-Visual-Representation-Alignment-for-Robot-Learning" class="headerlink" title="What Matters to You? Towards Visual Representation Alignment for Robot Learning"></a>What Matters to You? Towards Visual Representation Alignment for Robot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07932">http://arxiv.org/abs/2310.07932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Tian, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, Andrea Bajcsy</li>
<li>for: 本研究旨在帮助机器人优化与人类 preference 相关的奖励，以便机器人可以根据人类的需求和选择进行行为。</li>
<li>methods: 本研究使用了 Representation-Aligned Preference-based Learning (RAPL) 方法，该方法通过人类反馈来调整机器人的视觉表示，以便更好地满足人类的需求。</li>
<li>results: 实验结果表明，RAPL 的奖励可以生成人类喜欢的机器人行为，并且具有高样本效率和零样本泛化性。<details>
<summary>Abstract</summary>
When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs like RGB images, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycle-consistency. However, all these proxy tasks bypass the human's input on what matters to them, exacerbating spurious correlations and ultimately leading to robot behaviors that are misaligned with user preferences. In this work, we propose that robots should leverage human feedback to align their visual representations with the end-user and disentangle what matters for the task. We propose Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward learning problem through the lens of preference-based learning and optimal transport. Across experiments in X-MAGICAL and in robotic manipulation, we find that RAPL's reward consistently generates preferred robot behaviors with high sample efficiency, and shows strong zero-shot generalization when the visual representation is learned from a different embodiment than the robot's.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="D2-Pruning-Message-Passing-for-Balancing-Diversity-and-Difficulty-in-Data-Pruning"><a href="#D2-Pruning-Message-Passing-for-Balancing-Diversity-and-Difficulty-in-Data-Pruning" class="headerlink" title="D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning"></a>D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07931">http://arxiv.org/abs/2310.07931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adymaharana/d2pruning">https://github.com/adymaharana/d2pruning</a></li>
<li>paper_authors: Adyasha Maharana, Prateek Yadav, Mohit Bansal</li>
<li>for: 提高模型训练数据质量可以降低模型测试错误率，同时可以采用数据减少方法来降低计算成本。</li>
<li>methods: 提出了一种基于图structure的数据选择算法， named D2 Pruning， 使用前向和反向消息传递来更新数据集中每个示例的difficulty scores，然后使用图Structured sampling方法选择最佳的核心集。</li>
<li>results: 对于视觉和语言 datasets，D2 Pruning比前一代方法更好地选择核心集，可以达到70%的减少率，同时发现使用D2 Pruning来筛选大量多模态数据集可以提高数据集的多样性和预训练模型的泛化性。<details>
<summary>Abstract</summary>
Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection. We represent a dataset as an undirected graph and propose a novel pruning algorithm, D2 Pruning, that uses forward and reverse message passing over this dataset graph for coreset selection. D2 Pruning updates the difficulty scores of each example by incorporating the difficulty of its neighboring examples in the dataset graph. Then, these updated difficulty scores direct a graph-based sampling method to select a coreset that encapsulates both diverse and difficult regions of the dataset space. We evaluate supervised and self-supervised versions of our method on various vision and language datasets. Results show that D2 Pruning improves coreset selection over previous state-of-the-art methods for up to 70% pruning rates. Additionally, we find that using D2 Pruning for filtering large multimodal datasets leads to increased diversity in the dataset and improved generalization of pretrained models.
</details>
<details>
<summary>摘要</summary>
高品质数据可能导致模型在固定数据预算下的测试错误下降。此外，一个模型可以在固定计算预算下训练无需妥协性能，只要可以从数据集中剔除重复项。核心集选择（数据剔除）目的是选择训练数据集中的子集，以最大化模型在这个子集上的性能。现有两种主导方法：（1）基于geometry的数据选择，以提高数据多样性在核心集中，和（2）基于训练动态函数来评分样本的难度。最佳化数据多样性会导致核心集偏向容易样本，而选择难度排名则会忽略容易样本，这些样本是深度学习模型训练所必需的。这表明数据多样性和重要性分数是两种完全相关的因素，需要在核心集选择中同时考虑。我们将数据集表示为无向图，并提出一种新的减少算法，D2 Pruning，它使用数据集图上的前向和反向消息传递来进行核心集选择。D2 Pruning将数据集图上的每个例子的难度分数更新，基于该例子的邻居例子在数据集图上的难度。然后，这些更新后的难度分数将导航一种基于图的采样方法，选择包含多样和困难区域的核心集。我们对各种视觉和语言数据集进行了超过70%的减少率。此外，我们发现使用D2 Pruning进行大量多模态数据集的筛选可以提高数据集的多样性和预训练模型的泛化性。
</details></li>
</ul>
<hr>
<h2 id="Contextualized-Policy-Recovery-Modeling-and-Interpreting-Medical-Decisions-with-Adaptive-Imitation-Learning"><a href="#Contextualized-Policy-Recovery-Modeling-and-Interpreting-Medical-Decisions-with-Adaptive-Imitation-Learning" class="headerlink" title="Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning"></a>Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07918">http://arxiv.org/abs/2310.07918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannik Deuschel, Caleb N. Ellington, Benjamin J. Lengerich, Yingtao Luo, Pascal Friederich, Eric P. Xing</li>
<li>for: 该论文目的是提出一种 Contextualized Policy Recovery（CPR）方法，以解决现有的政策学习模型存在准确性和可读性之间的负面选择问题。</li>
<li>methods: CPR方法将问题定义为多任务学习问题，将复杂的决策过程分解为不同的上下文特定策略。每个上下文特定策略都是一个线性观察到行动映射。CPR方法可以在完全无线上和部分可见的决策环境中运行，并可以与任何循环黑盒模型或可读的决策模型结合使用。</li>
<li>results: 研究人员通过在 simulate 和实际数据上测试 CPR 方法，实现了在静脉抗生素干扰 ICU 中预测抗生素药物的 (+22% AUROC vs. 前一代 SOTA) 和预测 Alzheimer 病人 MRI 药物的 (+7.7% AUROC vs. 前一代 SOTA) 任务上的状元表现。与此同时，CPR 方法 closing 了可读性和黑盒方法之间的准确性差距，允许高分辨率探索和分析上下文特定的决策模型。<details>
<summary>Abstract</summary>
Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapping, and generates new decision models $\textit{on-demand}$ as contexts are updated with new observations. CPR is compatible with fully offline and partially observable decision environments, and can be tailored to incorporate any recurrent black-box model or interpretable decision model. We assess CPR through studies on simulated and real data, achieving state-of-the-art performance on the canonical tasks of predicting antibiotic prescription in intensive care units ($+22\%$ AUROC vs. previous SOTA) and predicting MRI prescription for Alzheimer's patients ($+7.7\%$ AUROC vs. previous SOTA). With this improvement in predictive performance, CPR closes the accuracy gap between interpretable and black-box methods for policy learning, allowing high-resolution exploration and analysis of context-specific decision models.
</details>
<details>
<summary>摘要</summary>
“干预推理”政策学习目标在于从观察行为中估算出可理解的决策政策，但现有模型却存在精度和可理解之间的对抗。这个对抗限制了基于数据的人类决策过程的解释。例如，为了审核医疗决策中的偏见和不良实践，我们需要一些可解释的决策过程模型，以提供简洁的行为描述。实际上，现有的方法受到这个对抗，因为它们将基于决策过程的底层模型设计为一个通用政策，但人类决策实际上是动态的，可以随着上下文资讯而改变。因此，我们提出了“上下文化政策恢复”（CPR），它将决策过程模型化为多任务学习问题，并将复杂的决策政策拆分为具有上下文特定的政策。CPR模型每个上下文特定的政策为一个线性观察到动作的映射，并在上下文更新时产生新的决策模型。CPR适用于完全离线和部分可观察的决策环境，并可以与任何可读黑盒模型或可解释的决策模型整合。我们通过在模拟和实际数据上进行了研究， CP 的表现比前一代最佳化� Архивная копия от 20 августа 2020 на Wayback Machine
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Machine-Learning-Techniques-in-Imbalanced-Data-and-Future-Trends"><a href="#A-Review-of-Machine-Learning-Techniques-in-Imbalanced-Data-and-Future-Trends" class="headerlink" title="A Review of Machine Learning Techniques in Imbalanced Data and Future Trends"></a>A Review of Machine Learning Techniques in Imbalanced Data and Future Trends</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07917">http://arxiv.org/abs/2310.07917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elaheh Jafarigol, Theodore Trafalis</li>
<li>for: 本研究旨在提供大规模不均衡数据中机器学习领域各种方法的概述和总结，以便在不同领域中应用大规模不均衡数据。</li>
<li>methods: 本研究涉及了各种手段，包括各种数据处理技术和机器学习算法，以 Addressing the problem of imbalanced data in various domains。</li>
<li>results: 本研究通过收集和评审258篇同行评审文章，提供了对各种方法的审视和总结，以及在不同领域中机器学习的应用。<details>
<summary>Abstract</summary>
For over two decades, detecting rare events has been a challenging task among researchers in the data mining and machine learning domain. Real-life problems inspire researchers to navigate and further improve data processing and algorithmic approaches to achieve effective and computationally efficient methods for imbalanced learning. In this paper, we have collected and reviewed 258 peer-reviewed papers from archival journals and conference papers in an attempt to provide an in-depth review of various approaches in imbalanced learning from technical and application perspectives. This work aims to provide a structured review of methods used to address the problem of imbalanced data in various domains and create a general guideline for researchers in academia or industry who want to dive into the broad field of machine learning using large-scale imbalanced data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Recurrent-networks-recognize-patterns-with-low-dimensional-oscillations"><a href="#Recurrent-networks-recognize-patterns-with-low-dimensional-oscillations" class="headerlink" title="Recurrent networks recognize patterns with low-dimensional oscillations"></a>Recurrent networks recognize patterns with low-dimensional oscillations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07908">http://arxiv.org/abs/2310.07908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ktmurray1999/neural-rules">https://github.com/ktmurray1999/neural-rules</a></li>
<li>paper_authors: Keith T. Murray</li>
<li>for: 这个研究探讨了一种新的动力学机制，用于识别模式，通过解释一个基于SET卡游戏的简单任务中的回归神经网络（RNN）的含义。</li>
<li>methods: 这个研究使用了解释RNN的方法，并将其视为在低维度限径谱中发生相位变换的模式识别。此外，研究者还手工制作了一个气泡模型，以重现RNN的动力学。</li>
<li>results: 这个研究发现，RNN可以通过相位变换来识别模式，并且这种机制与金字塔自动机（FSA）的转移有相似之处。此外，研究还发现了一种可能的神经网络实现FSA的机制。这项研究不仅提供了一种可能的模式识别机制，还为深度学习模型解释提供了一个新的视角。<details>
<summary>Abstract</summary>
This study proposes a novel dynamical mechanism for pattern recognition discovered by interpreting a recurrent neural network (RNN) trained on a simple task inspired by the SET card game. We interpreted the trained RNN as recognizing patterns via phase shifts in a low-dimensional limit cycle in a manner analogous to transitions in a finite state automaton (FSA). We further validated this interpretation by handcrafting a simple oscillatory model that reproduces the dynamics of the trained RNN. Our findings not only suggest of a potential dynamical mechanism capable of pattern recognition, but also suggest of a potential neural implementation of FSA. Above all, this work contributes to the growing discourse on deep learning model interpretability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RoboCLIP-One-Demonstration-is-Enough-to-Learn-Robot-Policies"><a href="#RoboCLIP-One-Demonstration-is-Enough-to-Learn-Robot-Policies" class="headerlink" title="RoboCLIP: One Demonstration is Enough to Learn Robot Policies"></a>RoboCLIP: One Demonstration is Enough to Learn Robot Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07899">http://arxiv.org/abs/2310.07899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumedh A Sontakke, Jesse Zhang, Sébastien M. R. Arnold, Karl Pertsch, Erdem Bıyık, Dorsa Sadigh, Chelsea Finn, Laurent Itti<br>for: RoboCLIP is designed to address the difficulty of reward specification in reinforcement learning, particularly the need for extensive expert supervision to design robust reward functions.methods: RoboCLIP uses a single video demonstration or textual description of the task to generate rewards without manual reward function design, leveraging pretrained Video-and-Language Models (VLMs) without any finetuning.results: Reinforcement learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher zero-shot performance than competing imitation learning methods on downstream robot manipulation tasks, using only one video&#x2F;text demonstration.<details>
<summary>Abstract</summary>
Reward specification is a notoriously difficult problem in reinforcement learning, requiring extensive expert supervision to design robust reward functions. Imitation learning (IL) methods attempt to circumvent these problems by utilizing expert demonstrations but typically require a large number of in-domain expert demonstrations. Inspired by advances in the field of Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation learning method that uses a single demonstration (overcoming the large data requirement) in the form of a video demonstration or a textual description of the task to generate rewards without manual reward function design. Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like videos of humans solving the task for reward generation, circumventing the need to have the same demonstration and deployment domains. RoboCLIP utilizes pretrained VLMs without any finetuning for reward generation. Reinforcement learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher zero-shot performance than competing imitation learning methods on downstream robot manipulation tasks, doing so using only one video/text demonstration.
</details>
<details>
<summary>摘要</summary>
<<SYS>>请注意，以下文本将使用简化中文表示。</SYS>> reward specification 是 reinforcement learning 中的一个不orious难题，需要广泛的专家指导以设计可靠的奖励函数。 imitation learning（IL）方法尝试通过使用专家示范来绕过这些问题，但通常需要大量的域内专家示范。 我们受到 Video-and-Language Models（VLMs）领域的进步 inspirited，我们提出了 RoboCLIP，一种在线的 imitation learning 方法，使用单个示范（超越大量数据要求），可以通过视频示范或文本描述来生成奖励，无需手动设计奖励函数。 RoboCLIP 还可以使用不同域的示范，如人类解决任务的视频示范，以便不需要同一个示范和部署域。 RoboCLIP 使用预训练的 VLMs，无需任何finetuning，可以生成奖励。 reinforcement learning 代理人使用 RoboCLIP 奖励表现出2-3倍于竞争的 imitation learning 方法在下游机器人处理任务上的零件表现，使用单个视频/文本示范。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Integrators-for-Diffusion-Generative-Models"><a href="#Efficient-Integrators-for-Diffusion-Generative-Models" class="headerlink" title="Efficient Integrators for Diffusion Generative Models"></a>Efficient Integrators for Diffusion Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07894">http://arxiv.org/abs/2310.07894</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mandt-lab/PSLD">https://github.com/mandt-lab/PSLD</a></li>
<li>paper_authors: Kushagra Pandey, Maja Rudolph, Stephan Mandt</li>
<li>for: 本研究旨在提高扩展Diffusion模型的采样速度，以便更快地生成样本。</li>
<li>methods: 我们提出了两种扩展Diffusion模型的采样方法： conjugate integrators和splitting integrators。 conjugate integrators通过将反射 diffusion 动力学映射到更易于采样的空间，而 splitting-based integrators通过在数据和占 auxiliary 变量之间进行交替更新来减少数值计算误差。</li>
<li>results: 我们对这两种方法进行了广泛的实验和理论研究，并提出了一种hybrid方法，这种方法可以在扩展空间中对Diffusion模型进行最佳性能的采样。在应用于[Pandey &amp; Mandt, 2023]中的CIFAR-10上，我们的deterministic和stochastic采样器可以在100个网络功能评估（NFE）后达到FID分数为2.11和2.36，比最佳基eline的2.57和2.63更低。<details>
<summary>Abstract</summary>
Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively. Our code and model checkpoints will be made publicly available at \url{https://github.com/mandt-lab/PSLD}.
</details>
<details>
<summary>摘要</summary>
Diffusion models在推理时会受到慢的样本生成问题。因此，开发一个原则性的框架来加速预训练模型的干扰/随机抽取是一个有前途的方向。我们提议两种补充性框架来加速预训练模型的样本生成：协调 интеграторы和分割 интеграторы。协调 интеграторы扩展了逆扩散动力学，将逆扩散动力学映射到更适合采样的空间。相比之下，分割基于分子动力学通常使用数值方法更新数据和辅助变量，从而减少数值计算误差。我们经验和理论上深入研究这些方法，并提出了一种混合方法，导致预训练模型在扩展空间中的最佳表现。应用于[Pandey & Mandt, 2023]中的phas Space Langevin Diffusion（PSLD）在CIFAR-10上，我们的干扰和随机抽取器在100个网络函数评估（NFE）内达到了FID分数为2.11和2.36，与最佳基线相比下升2.57和2.63。我们将代码和模型Checkpoint公开在 GitHub上，请参考\url{https://github.com/mandt-lab/PSLD}.
</details></li>
</ul>
<hr>
<h2 id="LangNav-Language-as-a-Perceptual-Representation-for-Navigation"><a href="#LangNav-Language-as-a-Perceptual-Representation-for-Navigation" class="headerlink" title="LangNav: Language as a Perceptual Representation for Navigation"></a>LangNav: Language as a Perceptual Representation for Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07889">http://arxiv.org/abs/2310.07889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva, Phillip Isola, Yoon Kim</li>
<li>for: 本文探讨了使用语言作为视觉 Navigation 的感知表示。</li>
<li>methods: 我们的方法使用了市场上可得的视觉系统（图像描述和对象检测）将当前时间步的自我中心Panoramic View转换成自然语言描述。然后，我们将预训练的语言模型进行训练，以选择基于当前视图和轨迹历史的最佳行为。与标准设置不同的是，我们的方法使用（粗粒度）语言作为感知表示，而不是直接使用预训练的视觉特征。</li>
<li>results: 我们的方法在R2R视觉语言导航标准 benchmark上实现了比预先学习的视觉特征更好的性能，特别是在只有10-100个黄金轨迹available的情况下。这表明使用语言作为感知表示可以在导航任务中提供更好的性能。<details>
<summary>Abstract</summary>
We explore the use of language as a perceptual representation for vision-and-language navigation. Our approach uses off-the-shelf vision systems (for image captioning and object detection) to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore two use cases of our language-based navigation (LangNav) approach on the R2R vision-and-language navigation benchmark: generating synthetic trajectories from a prompted large language model (GPT-4) with which to finetune a smaller language model; and sim-to-real transfer where we transfer a policy learned on a simulated environment (ALFRED) to a real-world environment (R2R). Our approach is found to improve upon strong baselines that rely on visual features in settings where only a few gold trajectories (10-100) are available, demonstrating the potential of using language as a perceptual representation for navigation tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Leader-Follower-Neural-Networks-with-Local-Error-Signals-Inspired-by-Complex-Collectives"><a href="#Leader-Follower-Neural-Networks-with-Local-Error-Signals-Inspired-by-Complex-Collectives" class="headerlink" title="Leader-Follower Neural Networks with Local Error Signals Inspired by Complex Collectives"></a>Leader-Follower Neural Networks with Local Error Signals Inspired by Complex Collectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07885">http://arxiv.org/abs/2310.07885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenzhong Yin, Mingxi Cheng, Xiongye Xiao, Xinghe Chen, Shahin Nazarian, Andrei Irimia, Paul Bogdan</li>
<li>For: The paper is written to propose a neural network architecture inspired by the rules observed in nature’s collective ensembles, and to investigate the behavior of workers in the network.* Methods: The paper uses a leader-follower neural network (LFNN) structure, and trains the network using local error signals and optionally incorporating backpropagation (BP) and global loss.* Results: The paper achieves significantly lower error rates than previous BP-free algorithms on MNIST and CIFAR-10, and outperforms previous BP-free algorithms by a significant margin on ImageNet.Here is the information in Simplified Chinese text, as requested:* For: 这篇论文是为了提出基于自然集合中规则的神经网络架构，并研究工作者在网络中的行为。* Methods: 这篇论文使用了领导者-追随者神经网络（LFNN）结构，并通过本地错误信号和可选的反propagation（BP）和全局损失来训练网络。* Results: 这篇论文在MNIST和CIFAR-10上实现了比前一代BP-free算法更低的错误率，并在ImageNet上超过了前一代BP-free算法的性能。<details>
<summary>Abstract</summary>
The collective behavior of a network with heterogeneous, resource-limited information processing units (e.g., group of fish, flock of birds, or network of neurons) demonstrates high self-organization and complexity. These emergent properties arise from simple interaction rules where certain individuals can exhibit leadership-like behavior and influence the collective activity of the group. Motivated by the intricacy of these collectives, we propose a neural network (NN) architecture inspired by the rules observed in nature's collective ensembles. This NN structure contains workers that encompass one or more information processing units (e.g., neurons, filters, layers, or blocks of layers). Workers are either leaders or followers, and we train a leader-follower neural network (LFNN) by leveraging local error signals and optionally incorporating backpropagation (BP) and global loss. We investigate worker behavior and evaluate LFNNs through extensive experimentation. Our LFNNs trained with local error signals achieve significantly lower error rates than previous BP-free algorithms on MNIST and CIFAR-10 and even surpass BP-enabled baselines. In the case of ImageNet, our LFNN-l demonstrates superior scalability and outperforms previous BP-free algorithms by a significant margin.
</details>
<details>
<summary>摘要</summary>
集体行为的网络（例如鱼群、鸟群或神经网络）表现出高度自组织和复杂性。这些emergent特性来自简单的互动规则，其中某些个体可以展示领导性行为，影响集体活动的总体表现。受自然集体ensemble的复杂性启发，我们提出一种基于自然集体的神经网络（NN）结构。这个NN结构包含有一个或多个信息处理单元（例如神经元、滤波器、层或层组）的工作者。工作者可以是领导者或追随者，我们使用本地错误信号和可选地包括反向传播（BP）和全局损失来训练领导者-追随者神经网络（LFNN）。我们对工作者行为进行了广泛的实验研究，并评估了LFNN的性能。我们的LFNN使用本地错误信号进行训练，在MNIST和CIFAR-10上达到了较低的错误率，并在ImageNet上超越了前一代BP-free算法。
</details></li>
</ul>
<hr>
<h2 id="The-Thousand-Faces-of-Explainable-AI-Along-the-Machine-Learning-Life-Cycle-Industrial-Reality-and-Current-State-of-Research"><a href="#The-Thousand-Faces-of-Explainable-AI-Along-the-Machine-Learning-Life-Cycle-Industrial-Reality-and-Current-State-of-Research" class="headerlink" title="The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research"></a>The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07882">http://arxiv.org/abs/2310.07882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Decker, Ralf Gross, Alexander Koebler, Michael Lebacher, Ronald Schnitzer, Stefan H. Weber</li>
<li>for: 本研究探讨了可解释人工智能（XAI）在生产业中的实际应用 relevance，并对当前学术研究进行了比较。</li>
<li>methods: 本研究采用了广泛的采访方法，包括各种角色和关键参与者从不同的行业部门进行了访问。此外，我们还提供了一个简短的文献回顾，以提供一个涵盖所调查人员的意见以及当前学术研究的总览。</li>
<li>results: 我们的调查结果表明，虽然存在许多不同的XAI方法，但大多数都集中在模型评估阶段和数据科学家之间。此外，我们还发现了一些不足，例如现有方法和框架不足以帮助非专家用户理解和解释透明度不高的人工智能模型。<details>
<summary>Abstract</summary>
In this paper, we investigate the practical relevance of explainable artificial intelligence (XAI) with a special focus on the producing industries and relate them to the current state of academic XAI research. Our findings are based on an extensive series of interviews regarding the role and applicability of XAI along the Machine Learning (ML) lifecycle in current industrial practice and its expected relevance in the future. The interviews were conducted among a great variety of roles and key stakeholders from different industry sectors. On top of that, we outline the state of XAI research by providing a concise review of the relevant literature. This enables us to provide an encompassing overview covering the opinions of the surveyed persons as well as the current state of academic research. By comparing our interview results with the current research approaches we reveal several discrepancies. While a multitude of different XAI approaches exists, most of them are centered around the model evaluation phase and data scientists. Their versatile capabilities for other stages are currently either not sufficiently explored or not popular among practitioners. In line with existing work, our findings also confirm that more efforts are needed to enable also non-expert users' interpretation and understanding of opaque AI models with existing methods and frameworks.
</details>
<details>
<summary>摘要</summary>
The study finds that while there are many different XAI approaches, most are centered around the model evaluation phase and data scientists, with limited exploration of their versatility in other stages. Additionally, the study confirms that more efforts are needed to enable non-expert users to interpret and understand opaque AI models using existing methods and frameworks.The review of the relevant literature provides an encompassing overview of the opinions of the surveyed persons as well as the current state of academic research. By comparing the interview results with the current research approaches, the study reveals several discrepancies between the two, highlighting the need for further research and development in XAI.
</details></li>
</ul>
<hr>
<h2 id="DeePref-Deep-Reinforcement-Learning-For-Video-Prefetching-In-Content-Delivery-Networks"><a href="#DeePref-Deep-Reinforcement-Learning-For-Video-Prefetching-In-Content-Delivery-Networks" class="headerlink" title="DeePref: Deep Reinforcement Learning For Video Prefetching In Content Delivery Networks"></a>DeePref: Deep Reinforcement Learning For Video Prefetching In Content Delivery Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07881">http://arxiv.org/abs/2310.07881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nawras Alkassab, Chin-Tser Huang, Tania Lorido Botran</li>
<li>for: 这篇论文旨在提高内容传输网络（Content Delivery Networks，CDN）中视频内容的缓存和预取优化算法，以提高用户体验质量。</li>
<li>methods: 这篇论文提出了一种基于深度学习的预取优化算法，named DeePref，可以在CDN边缘网络中自动适应用户访问模式的变化，提高预取精度和覆盖率。</li>
<li>results: 实验结果表明，使用DeePref DRQN在实际世界数据集上，可以提高预取精度和覆盖率，相比基eline方法，提高28%和17%，同时也研究了将统计模型从一个边缘网络传输到另一个边缘网络，可以提高预取精度和覆盖率，相比基eline方法，提高30%和10%。<details>
<summary>Abstract</summary>
Content Delivery Networks carry the majority of Internet traffic, and the increasing demand for video content as a major IP traffic across the Internet highlights the importance of caching and prefetching optimization algorithms. Prefetching aims to make data available in the cache before the requester places its request to reduce access time and improve the Quality of Experience on the user side. Prefetching is well investigated in operating systems, compiler instructions, in-memory cache, local storage systems, high-speed networks, and cloud systems. Traditional prefetching techniques are well adapted to a particular access pattern, but fail to adapt to sudden variations or randomization in workloads. This paper explores the use of reinforcement learning to tackle the changes in user access patterns and automatically adapt over time. To this end, we propose, DeePref, a Deep Reinforcement Learning agent for online video content prefetching in Content Delivery Networks. DeePref is a prefetcher implemented on edge networks and is agnostic to hardware design, operating systems, and applications. Our results show that DeePref DRQN, using a real-world dataset, achieves a 17% increase in prefetching accuracy and a 28% increase in prefetching coverage on average compared to baseline approaches that use video content popularity as a building block to statically or dynamically make prefetching decisions. We also study the possibility of transfer learning of statistical models from one edge network into another, where unseen user requests from unknown distribution are observed. In terms of transfer learning, the increase in prefetching accuracy and prefetching coverage are [$30%$, $10%$], respectively. Our source code will be available on Github.
</details>
<details>
<summary>摘要</summary>
content delivery networks 承载了互联网上大量流量，而在互联网上占主要的流量中，视频内容的增长也使得缓存和预取优化算法变得越来越重要。预取的目的是在用户请求之前将数据提取到缓存中，以降低访问时间和提高用户体验质量。预取技术已经在操作系统、编译器指令、内存缓存、本地存储系统、高速网络和云系统中得到了广泛的研究。传统的预取技术通常是根据特定的访问模式进行适应，但是它们无法适应用户访问模式的快速变化或随机化。本文探讨了使用强化学习来解决这些变化的问题。为此，我们提出了DeePref，一种基于深度强化学习的在线视频内容预取代理。DeePref在边缘网络上实现，不依赖于硬件设计、操作系统或应用程序。我们的结果表明，DeePref DRQN使用实际数据集时，平均提高预取精度28%和预取覆盖率17% Compared to基eline方法，使用视频内容的流行度作为预取决策的基础或动态决策。我们还研究了将统计模型从一个边缘网络传播到另一个边缘网络中，以处理未经见过的用户请求和未知分布。在传播学习中，预取精度和预取覆盖率增加了[30%, 10%]。我们的源代码将在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="TabLib-A-Dataset-of-627M-Tables-with-Context"><a href="#TabLib-A-Dataset-of-627M-Tables-with-Context" class="headerlink" title="TabLib: A Dataset of 627M Tables with Context"></a>TabLib: A Dataset of 627M Tables with Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07875">http://arxiv.org/abs/2310.07875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gus Eggert, Kevin Huo, Mike Biven, Justin Waugh</li>
<li>for: 这篇论文是为了提供一个大型、多样化的表格数据集，以便进行现代AI系统的研究和发展。</li>
<li>methods: 该论文使用了多种数据抽取方法，从GitHub和Common Crawl等来源中提取了627万个表格，总量达69 TiB，并且包含了867亿个上下文符号。</li>
<li>results: 该论文提出了一个名为”TabLib”的新的表格数据集，其包含了627万个表格，总量达69 TiB，并且具有多样化的表格结构和上下文。这个数据集的大小和多样性都提供了许多探索和研究的机会，类似于text和图像模态的基础数据集。<details>
<summary>Abstract</summary>
It is well-established that large, diverse datasets play a pivotal role in the performance of modern AI systems for text and image modalities. However, there are no datasets for tabular data of comparable size and diversity to those available for text and images. Thus we present "TabLib'', a compilation of 627 million tables totaling 69 TiB, along with 867B tokens of context. TabLib was extracted from numerous file formats, including CSV, HTML, SQLite, PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and diversity of TabLib offer considerable promise in the table modality, reminiscent of the original promise of foundational datasets for text and images, such as The Pile and LAION.
</details>
<details>
<summary>摘要</summary>
“已经确立了现代人工智能系统中大量多样数据的重要作用。然而，对于表格数据，没有相关的大量多样数据集的存在，与文本和图像模式相似。因此，我们提出了“TabLib”，包含627万个表格，总量为69 TiB，以及867亿个上下文token。TabLib从多种文件格式中提取，包括CSV、HTML、SQLite、PDF、Excel等，来自GitHub和Common Crawl。TabLib的大小和多样性表现出了很大的抢救潜力，类似于文本和图像领域的基础数据集，如The Pile和LAION。”
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Pretraining-on-Multimodal-Electronic-Health-Records"><a href="#Hierarchical-Pretraining-on-Multimodal-Electronic-Health-Records" class="headerlink" title="Hierarchical Pretraining on Multimodal Electronic Health Records"></a>Hierarchical Pretraining on Multimodal Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07871">http://arxiv.org/abs/2310.07871</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaochenwang-psu/medhmp">https://github.com/xiaochenwang-psu/medhmp</a></li>
<li>paper_authors: Xiaochen Wang, Junyu Luo, Jiaqi Wang, Ziyi Yin, Suhan Cui, Yuan Zhong, Yaqing Wang, Fenglong Ma</li>
<li>for: 这篇论文是为了解决医疗领域中电子健康记录（EHR）资料的层次结构问题，以提高现有预训模型在多种下游任务中的通用能力。</li>
<li>methods: 本文提出了一个新的、通用的、多modal EHR预训框架（MEDHMP），专门针对医疗领域中的层次结构资料进行预训。</li>
<li>results:  authors通过实验结果显示了MEDHMP的效果，在八个下游任务中三个层次上展示了佳绩，与十八个基准相比，更加强调了我们的方法的可行性。<details>
<summary>Abstract</summary>
Pretraining has proven to be a powerful technique in natural language processing (NLP), exhibiting remarkable success in various NLP downstream tasks. However, in the medical domain, existing pretrained models on electronic health records (EHR) fail to capture the hierarchical nature of EHR data, limiting their generalization capability across diverse downstream tasks using a single pretrained model. To tackle this challenge, this paper introduces a novel, general, and unified pretraining framework called MEDHMP, specifically designed for hierarchically multimodal EHR data. The effectiveness of the proposed MEDHMP is demonstrated through experimental results on eight downstream tasks spanning three levels. Comparisons against eighteen baselines further highlight the efficacy of our approach.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>预训练技术在自然语言处理（NLP）中已经证明是一种强大的技术，在不同的NLP下游任务中表现出了很好的成绩。然而，在医疗领域，现有的预训练模型在电子医疗记录（EHR）数据上失去了层次结构的特点，因此限制了单个预训练模型在多种下游任务中的通用化能力。为解决这个挑战，本文提出了一种新的、通用、多模式预训练框架called MEDHMP，专门针对层次多模式EHR数据。我们通过对八个下游任务进行实验，证明了我们的方法的有效性。与 eighteen个基准值进行比较，我们的方法的成绩更加出色。
</details></li>
</ul>
<hr>
<h2 id="Cheap-Talking-Algorithms"><a href="#Cheap-Talking-Algorithms" class="headerlink" title="Cheap Talking Algorithms"></a>Cheap Talking Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07867">http://arxiv.org/abs/2310.07867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Condorelli, Massimiliano Furlan</li>
<li>for: 研究独立强化学习算法在 crawford 和 sobel (1982) 游戏中的信息传输行为。</li>
<li>methods:  sender 和 receiver 在训练中共同 converges to 离散的 Nash 均衡。</li>
<li>results: 通信占据预期的最大程度，即根据交战利益冲突的程度。结论是对不同参数和游戏 especification  robust。I hope this helps!<details>
<summary>Abstract</summary>
We simulate behaviour of independent reinforcement learning algorithms playing the Crawford and Sobel (1982) game of strategic information transmission. We show that a sender and a receiver training together converge to strategies close to the exante optimal equilibrium of the game. Hence, communication takes place to the largest extent predicted by Nash equilibrium given the degree of conflict of interest between agents. The conclusion is shown to be robust to alternative specifications of the hyperparameters and of the game. We discuss implications for theories of equilibrium selection in information transmission games, for work on emerging communication among algorithms in computer science and for the economics of collusions in markets populated by artificially intelligent agents.
</details>
<details>
<summary>摘要</summary>
我们模拟独立强化学习算法在克劳福德和索贝尔（1982）游戏中的行为。我们显示，发送者和接收者一起培训时，会 converges到靠近预先优化的均衡点。因此，通信发生在预先优化的均衡点所预测的范围内。结论是对于不同的具体化参数和游戏参数，都是Robust的。我们讨论这些结论对信息传输游戏理论选择、计算机科学中算法之间的emerging通信以及人工智能代理人市场中的经济合作等方面的影响。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Data-Generation-with-Large-Language-Models-for-Text-Classification-Potential-and-Limitations"><a href="#Synthetic-Data-Generation-with-Large-Language-Models-for-Text-Classification-Potential-and-Limitations" class="headerlink" title="Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations"></a>Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07849">http://arxiv.org/abs/2310.07849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ming Yin</li>
<li>for:  investigate the effectiveness of using large language models (LLMs) to generate synthetic datasets for text classification</li>
<li>methods:  use LLMs to generate synthetic data, and evaluate the performance of models trained on these synthetic data</li>
<li>results:  find that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic dataHere’s the full text in Simplified Chinese:</li>
<li>for: 这个研究是为了investigate大语言模型（LLMs）是否可以生成高质量的synthetic datasets，以便提高文本分类模型的性能。</li>
<li>methods: 研究者使用LLMs生成synthetic data，并评估这些synthetic data上模型的性能。</li>
<li>results: 发现任务级别和实例级别的主观性均对模型在synthetic data上的性能产生负面影响。<details>
<summary>Abstract</summary>
The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.
</details>
<details>
<summary>摘要</summary>
collection and curation of high-quality training data 高质量训练数据的收集和整理是发展文本分类模型的表现优秀的关键因素，但它们frequently associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks.To better understand the factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we investigate how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data.We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.Here's the translation in Traditional Chinese:集成和整理高质量训练数据的收集是发展文本分类模型的表现优秀的关键因素，但它们经常与大量的成本和时间投入相关。研究人员最近尝试使用大语言模型（LLMs）生成 sintetic数据作为代替方案。然而，LLM生成的 sintetic数据在不同的分类任务中的效果是不一致的。为了更好地理解LLM生成 sintetic数据的效果的moderating因素，在这个研究中，我们investigate how the performance of models trained on these sintetic data may vary with the subjectivity of classification.我们的结果表明，在任务水平和实例水平都存在负相关性 между模型在 sintetic数据上的性能和分类的主观性。我们 conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Fundamental-Limits-of-Knowledge-Transfer-over-Finite-Domains"><a href="#Towards-the-Fundamental-Limits-of-Knowledge-Transfer-over-Finite-Domains" class="headerlink" title="Towards the Fundamental Limits of Knowledge Transfer over Finite Domains"></a>Towards the Fundamental Limits of Knowledge Transfer over Finite Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07838">http://arxiv.org/abs/2310.07838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sw-packages/ae0783895ca52d793929d6e5e57c365320dc5864c41ab9a7d5f64b2310c2fd59">https://github.com/sw-packages/ae0783895ca52d793929d6e5e57c365320dc5864c41ab9a7d5f64b2310c2fd59</a></li>
<li>paper_authors: Qingyue Zhao, Banghua Zhu</li>
<li>for: 本文研究了知识传递的统计效率，具体来说是通过$n$个教师样本传递到一个 probabilistic 学习器中，以便在输入空间$\mathcal{S}$上预测标签$\mathcal{A}$。</li>
<li>methods: 本文使用了三个进行知识传递的水平，它们分别是：只有硬标签信息（first level）、教师概率分布信息+硬标签信息（second level）和完整的soft labels信息（third level）。</li>
<li>results: 本文证明了，在第一个水平上，只有硬标签信息时，最优的最大 LIKElihood estimator 的渐近速率为 $\sqrt{|{\mathcal S}||{\mathcal A}|}&#x2F;{n}$。在第二个水平上，增加教师概率分布信息可以提高渐近速率的下界至 ${|{\mathcal S}||{\mathcal A}|}&#x2F;{n}$。在第三个水平上，使用完整的soft labels信息可以实现渐近速率 ${|{\mathcal S}|}&#x2F;{n}$ ，而且任何 Kullback-Leibler divergence 最小化器都是优选的。numerical simulations 验证了这些理论结论。<details>
<summary>Abstract</summary>
We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{|{\mathcal S}||{\mathcal A}|}/{n}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${|{\mathcal S}||{\mathcal A}|}/{n}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (complete logits) on ${\mathcal A}$ given every sampled input, thereby provably enables the student to enjoy a rate ${|{\mathcal S}|}/{n}$ free of $|{\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be optimal in the last case. Numerical simulations distinguish the four learners and corroborate our theory.
</details>
<details>
<summary>摘要</summary>
我们Characterize了知识传输的统计效率通过$n$个教师到一个概率学生分类器的输入空间$\mathcal S$上的标签$\mathcal A$.我们显示了隐私信息的三个进步级别可以加速传输。在第一个级别上，只有硬标签是知道的，由最大likelihood估计达到最小最大值$\sqrt{|{\mathcal S}||{\mathcal A}|}/{n}$.在第二个级别上，教师标签的概率也可以获得，这会降低下界到${|{\mathcal S}||{\mathcal A}|}/{n}$.然而，在这个第二个数据收集协议下，直接适应cross-entropy损失会导致漫游学生。我们解决了这个局限性，并达到基本限制，使用一种新的实际变体的平方差logit损失。在第三个级别上，学生还获得了每个输入的完整logits(${\mathcal A}$)，这使得学生可以在${|{\mathcal S}|}/{n}$的时间内达到基本限制。我们发现任何Kullback-Leibler divergence最小化者是最佳的。数字实验证明了我们的理论。
</details></li>
</ul>
<hr>
<h2 id="When-Why-and-How-Much-Adaptive-Learning-Rate-Scheduling-by-Refinement"><a href="#When-Why-and-How-Much-Adaptive-Learning-Rate-Scheduling-by-Refinement" class="headerlink" title="When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement"></a>When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07831">http://arxiv.org/abs/2310.07831</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/adaptive_scheduling">https://github.com/facebookresearch/adaptive_scheduling</a></li>
<li>paper_authors: Aaron Defazio, Ashok Cutkosky, Harsh Mehta, Konstantin Mishchenko</li>
<li>for: 这个论文的目的是为了bridging theory和实践之间的差距，并为一类优化算法（包括SGD）提供新的问题适应学习率计划。</li>
<li>methods: 这篇论文使用了对一类优化算法的推论分析，并使用实际中的观测梯度 norm来Derive更加精细的学习率计划。</li>
<li>results: 论文的实验结果表明，linear decay schedule在10种深度学习问题中具有最好的性能，而且在一系列LLMs和一组логистиック回归问题中也有出色的表现。此外，论文还提供了一种自动实现学习率计划的系统方法，可以实现学习率温化和快速学习率降低。<details>
<summary>Abstract</summary>
Learning rate schedules used in practice bear little resemblance to those recommended by theory. We close much of this theory/practice gap, and as a consequence are able to derive new problem-adaptive learning rate schedules. Our key technical contribution is a refined analysis of learning rate schedules for a wide class of optimization algorithms (including SGD). In contrast to most prior works that study the convergence of the average iterate, we study the last iterate, which is what most people use in practice. When considering only worst-case analysis, our theory predicts that the best choice is the linear decay schedule: a popular choice in practice that sets the stepsize proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the total number of steps. To go beyond this worst-case analysis, we use the observed gradient norms to derive schedules refined for any particular task. These refined schedules exhibit learning rate warm-up and rapid learning rate annealing near the end of training. Ours is the first systematic approach to automatically yield both of these properties. We perform the most comprehensive evaluation of learning rate schedules to date, evaluating across 10 diverse deep learning problems, a series of LLMs, and a suite of logistic regression problems. We validate that overall, the linear-decay schedule matches or outperforms all commonly used default schedules including cosine annealing, and that our schedule refinement method gives further improvements.
</details>
<details>
<summary>摘要</summary>
theory和实践中的学习率调度不符合，我们通过提出新的问题适应型学习率调度来减少这一差距。我们的关键技术贡献是对一类优化算法（包括SGD）的学习率调度进行了精细分析。与大多数前一些工作一样，我们研究了平均迭代点的渐进，但是我们强调研究最后一个迭代点，这是实际应用中人们常用的。在假设最坏情况下，我们的理论预测，最佳选择是线性衰减调度：一种广泛使用的做法，其中每个迭代点的步长与当前迭代次数/$T$ 成正比。为了超越这个最坏情况分析，我们使用观察到的梯度 norm 来 derive 更加细化的调度。这些细化调度具有学习率暖启和快速学习率缓和两个性能。我们是首个系统地自动实现这两个特性的系统。我们对10种深度学习问题、一系列LLMs以及一组logistic regression问题进行了最全面的学习率调度评估。我们证明，总的来说，线性衰减调度与所有通用的默认调度（包括cosine annealing）相当或超越。此外，我们的调度修正方法可以提供进一步的改进。
</details></li>
</ul>
<hr>
<h2 id="Does-Synthetic-Data-Make-Large-Language-Models-More-Efficient"><a href="#Does-Synthetic-Data-Make-Large-Language-Models-More-Efficient" class="headerlink" title="Does Synthetic Data Make Large Language Models More Efficient?"></a>Does Synthetic Data Make Large Language Models More Efficient?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07830">http://arxiv.org/abs/2310.07830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sia Gholami, Marwan Omar</li>
<li>for: 本研究旨在探讨深度学习方法在自然语言处理（NLP）领域中的应用，尤其是关于生成模板化问题的数据生成。</li>
<li>methods: 本研究使用模板基于的问题生成方法，以增加数据的多样性和数据量，并对现代变换器模型的性能进行评估。</li>
<li>results: 研究发现，使用模板基于的数据生成可以提高变换器模型的性能，但同时也存在风险的过拟合和模板限制的问题。<details>
<summary>Abstract</summary>
Natural Language Processing (NLP) has undergone transformative changes with the advent of deep learning methodologies. One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation. By assessing its advantages, including data augmentation potential and the introduction of structured variety, we juxtapose these benefits against inherent limitations, such as the risk of overfitting and the constraints posed by pre-defined templates. Drawing from empirical evaluations, we demonstrate the impact of template-based synthetic data on the performance of modern transformer models. We conclude by emphasizing the delicate balance required between synthetic and real-world data, and the future trajectories of integrating synthetic data in model training pipelines. The findings aim to guide NLP practitioners in harnessing synthetic data's potential, ensuring optimal model performance in diverse applications.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）在深度学习方法出现后已经经历了转变性变化。一个持续挑战研究人员的问题是数据质量的缺乏，这些模型驱动。这篇论文探讨了NLP中 sintetic数据生成的细节，特点是模板基于的问题生成。我们评估了这些优点，如数据增强潜力和结构化多样性的引入，并与内置的限制进行对比，如过拟合风险和预定模板所带来的限制。通过实验评估，我们证明了模板基于的 sintetic数据对现代转换器模型的性能产生了影响。我们结论，将synthetic数据和真实世界数据进行 equilibrio是NLP实践者需要注意的。将来，我们预计将在模型训练管道中集成synthetic数据，以便优化模型在多样化应用中的性能。这些发现希望能够引导NLP实践者在使用synthetic数据的潜力，以确保模型在多样化应用中的优秀性能。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Relationship-between-Analogy-Identification-and-Sentence-Structure-Encoding-in-Large-Language-Models"><a href="#Exploring-the-Relationship-between-Analogy-Identification-and-Sentence-Structure-Encoding-in-Large-Language-Models" class="headerlink" title="Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models"></a>Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07818">http://arxiv.org/abs/2310.07818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thilini Wijesiriwardene, Ruwan Wickramarachchi, Aishwarya Naresh Reganti, Vinija Jain, Aman Chadha, Amit Sheth, Amitava Das</li>
<li>for: 本研究旨在探讨现代自然语言处理（NLP）技术是如何识别句子对应关系，以及这种能力与语言模型（LLM）的句子结构编码能力之间的关系。</li>
<li>methods: 本研究使用多种大语言模型（LLM）来识别句子对应关系，并分析这些模型对句子结构的编码能力。</li>
<li>results: 研究发现，LLMs的句子对应关系识别能力与它们对句子结构的编码能力之间存在正相关性。具体来说， LLMS 更好地捕捉句子结构，它们也更具句子对应关系识别能力。<details>
<summary>Abstract</summary>
Identifying analogies plays a pivotal role in human cognition and language proficiency. In the last decade, there has been extensive research on word analogies in the form of ``A is to B as C is to D.'' However, there is a growing interest in analogies that involve longer text, such as sentences and collections of sentences, which convey analogous meanings. While the current NLP research community evaluates the ability of Large Language Models (LLMs) to identify such analogies, the underlying reasons behind these abilities warrant deeper investigation. Furthermore, the capability of LLMs to encode both syntactic and semantic structures of language within their embeddings has garnered significant attention with the surge in their utilization. In this work, we examine the relationship between the abilities of multiple LLMs to identify sentence analogies, and their capacity to encode syntactic and semantic structures. Through our analysis, we find that analogy identification ability of LLMs is positively correlated with their ability to encode syntactic and semantic structures of sentences. Specifically, we find that the LLMs which capture syntactic structures better, also have higher abilities in identifying sentence analogies.
</details>
<details>
<summary>摘要</summary>
理解对比在人类认知和语言能力中发挥着重要作用。过去十年，关于单词对比的研究得到了广泛的关注，但是现在越来越多的研究者关注 sentence对比，即 sentence A 和 sentence B 之间的对比。然而，当前的自然语言处理（NLP）研究社区正在评估大型自然语言模型（LLM）能否识别这类对比。尽管 LLM 的能力在识别对比方面的研究得到了广泛的关注，但是这些能力的下面原因仍然需要更深入的探究。此外， LLM 能够内嵌语言结构的能力也在不断受到关注，特别是它们可以内嵌 sentence 的语法和 semantics 结构。在这篇文章中，我们研究了多个 LLM 的对比 Identification 能力和它们内嵌语言结构的关系。我们发现，LLM 的对比 Identification 能力和它们内嵌语言结构的能力是正相关的。具体来说，我们发现 LLM 可以更好地捕捉语法结构的那些也有更高的对比 Identification 能力。
</details></li>
</ul>
<hr>
<h2 id="Generative-Modeling-with-Phase-Stochastic-Bridges"><a href="#Generative-Modeling-with-Phase-Stochastic-Bridges" class="headerlink" title="Generative Modeling with Phase Stochastic Bridges"></a>Generative Modeling with Phase Stochastic Bridges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07805">http://arxiv.org/abs/2310.07805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianrong Chen, Jiatao Gu, Laurent Dinh, Evangelos A. Theodorou, Josh Susskind, Shuangfei Zhai</li>
<li>for: 这篇论文的目的是提出一种基于阶段空间动力学的生成模型框架，以便更好地生成连续输入数据。</li>
<li>methods: 该模型使用了Stochastic Differential Equation（SDE）和神经网络来实现逆向生成。具体来说，它首先在输入空间中定义了一个阶段空间，然后使用Stochastic Optimal Control的理念来建立一个路径度量，以便高效地采样数据。</li>
<li>results: 在标准图像生成测试 benchmark 上，该模型在小范围内的Number of Function Evaluations（NFEs）下表现出色，并且与使用有效采样技术的 diffusion models 的性能相当，这说明该模型有potential作为一种新的生成模型工具。<details>
<summary>Abstract</summary>
Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluations (NFEs). Furthermore, our approach rivals the performance of diffusion models equipped with efficient sampling techniques, underscoring its potential as a new tool generative modeling.
</details>
<details>
<summary>摘要</summary>
干扰模型（DM）表示现代生成模型的极限，它们在维度输入空间中构建了随机差分方程（SDE），并使用神经网络来逆向解决。在这项工作中，我们介绍了一种新的生成模型框架，基于phaspace动力学，phaspace是包括位置和速度的扩展空间。利用Stochastic Optimal Control的洞察，我们在phaspace中建立了一个路径度量，以便高效采样。与DMs不同，我们的框架在动力学征标的早期阶段就能生成真实数据点，这使得可以通过使用速度信息来加速数据生成。在标准图像生成标准benchmark上，我们的模型在小范围内的Number of Function Evaluations（NFEs）下表现优秀，并且与Diffusion Models配备高效采样技术相比，我们的方法在生成模型方面具有潜在的潜力。
</details></li>
</ul>
<hr>
<h2 id="A-general-mechanism-of-humor-reformulating-the-semantic-overlap"><a href="#A-general-mechanism-of-humor-reformulating-the-semantic-overlap" class="headerlink" title="A general mechanism of humor: reformulating the semantic overlap"></a>A general mechanism of humor: reformulating the semantic overlap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07803">http://arxiv.org/abs/2310.07803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Martínez</li>
<li>for: 提出一种通用的幽默机制，不限于语言交流。</li>
<li>methods: 基于礼物的概念，包括脱困和冲突的解决方法。</li>
<li>results: 提出了一种可以应用于非语言交流的幽默机制，并且认为这种机制是人类思维的核心。<details>
<summary>Abstract</summary>
This article proposes a cognitive mechanism of humour of general applicability, not restricted to verbal communication. It is indebted to Raskin's concept of script overlap, and conforms to the incongruity-resolution theoretical framework, but it is built on the notion of constraint, an abstract correspondence between sets of data. Under this view, script overlap is an outcome of a more abstractly described phenomenon, constraint overlap. The important concept of the overlooked argument is introduced to characterise the two overlapping constraints -- overt and covert. Their inputs and outputs are not directly encoded in utterances, but implicated by them, and their overlap results in another overlap at the level of the communicated utterances, that the incongruity reveals. Our hypothesis assumes as a given that the evocation of such constraints is a cognitive effect of the inferential process by which a hearer interprets utterances. We base this assumption on Hofstadter's theory of analogy-making as the essence of human thought. By substituting "stimuli" of any kind for "utterances" in this model, we obtain a mechanism as easily applicable to non-verbal communication -- slapstick, cartoons -- and we propose it describes the necessary and sufficient conditions for a communicative act in any modality to carry humour.
</details>
<details>
<summary>摘要</summary>
The authors propose that the evocation of these constraints is a cognitive effect of the inferential process by which a hearer interprets utterances. This idea is based on Hofstadter's theory of analogy-making as the essence of human thought. By applying this mechanism to non-verbal communication, such as slapstick or cartoons, the authors suggest that it provides a necessary and sufficient condition for a communicative act in any modality to carry humor.
</details></li>
</ul>
<hr>
<h2 id="An-Information-Bottleneck-Characterization-of-the-Understanding-Workload-Tradeoff"><a href="#An-Information-Bottleneck-Characterization-of-the-Understanding-Workload-Tradeoff" class="headerlink" title="An Information Bottleneck Characterization of the Understanding-Workload Tradeoff"></a>An Information Bottleneck Characterization of the Understanding-Workload Tradeoff</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07802">http://arxiv.org/abs/2310.07802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mycal-tucker/ib-explanations">https://github.com/mycal-tucker/ib-explanations</a></li>
<li>paper_authors: Lindsay Sanneman, Mycal Tucker, Julie Shah</li>
<li>for: 这篇论文旨在探讨人工智能系统的可解释性（XAI），以支持人类理解AI系统。</li>
<li>methods: 论文使用信息瓶颈方法（Information Bottleneck method）来自动生成抽象（hand-crafted groupings of related problem features），以平衡工作负荷和理解之间的 contradistinction。</li>
<li>results: 实验表明，通过抽象来解释复杂概念可以有效地Address和平衡工作负荷和理解之间的 contradistinction。<details>
<summary>Abstract</summary>
Recent advances in artificial intelligence (AI) have underscored the need for explainable AI (XAI) to support human understanding of AI systems. Consideration of human factors that impact explanation efficacy, such as mental workload and human understanding, is central to effective XAI design. Existing work in XAI has demonstrated a tradeoff between understanding and workload induced by different types of explanations. Explaining complex concepts through abstractions (hand-crafted groupings of related problem features) has been shown to effectively address and balance this workload-understanding tradeoff. In this work, we characterize the workload-understanding balance via the Information Bottleneck method: an information-theoretic approach which automatically generates abstractions that maximize informativeness and minimize complexity. In particular, we establish empirical connections between workload and complexity and between understanding and informativeness through human-subject experiments. This empirical link between human factors and information-theoretic concepts provides an important mathematical characterization of the workload-understanding tradeoff which enables user-tailored XAI design.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "artificial intelligence" is translated as "人工智能" (réngōng zhìnéng)* "explainable AI" is translated as "可解释人工智能" (kějìjiě xiǎng réngōng zhìnéng)* "human understanding" is translated as "人类理解" (réngrì lǐjiě)* "mental workload" is translated as "心理劳动" (xīn lǐ gōngzuò)* "information-theoretic approach" is translated as "信息理论方法" (xìnwù lǐlùn fāngfa)* "abstractions" is translated as "抽象" (chōuxiàng)* "hand-crafted groupings" is translated as "手工组合" (shǒu gōng zǔyì)* "problem features" is translated as "问题特征" (wèn tí tèchēng)* "workload-understanding tradeoff" is translated as "劳动-理解交换" (gōngzuò-lǐjiě gòuhuan)* "user-tailored XAI design" is translated as "用户定制XAI设计" (yònghòu dìngxì XAI jièdì)
</details></li>
</ul>
<hr>
<h2 id="Explainable-Attention-for-Few-shot-Learning-and-Beyond"><a href="#Explainable-Attention-for-Few-shot-Learning-and-Beyond" class="headerlink" title="Explainable Attention for Few-shot Learning and Beyond"></a>Explainable Attention for Few-shot Learning and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07800">http://arxiv.org/abs/2310.07800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bahareh Nikpour, Narges Armanfard</li>
<li>for: 提高几何学模型的准确率和可靠性，特别在数据采集和标注过程中面临限制的情况下。</li>
<li>methods: 利用深度强化学习实现硬注意力找到，直接影响原始输入数据，使其可解释性提高。</li>
<li>results: 通过对多个 benchmark 数据集进行广泛的实验，证明我们提出的方法的效果。<details>
<summary>Abstract</summary>
Attention mechanisms have exhibited promising potential in enhancing learning models by identifying salient portions of input data. This is particularly valuable in scenarios where limited training samples are accessible due to challenges in data collection and labeling. Drawing inspiration from human recognition processes, we posit that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, akin to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically tailored for few-shot learning scenarios, called FewXAT. Our approach employs deep reinforcement learning to implement the concept of hard attention, directly impacting raw input data and thus rendering the process interpretable for human understanding. Through extensive experimentation across various benchmark datasets, we demonstrate the efficacy of our proposed method.
</details>
<details>
<summary>摘要</summary>
注意机制在增强学习模型方面表现出了扎实的潜力，特别是在数据采集和标注过程中存在困难时。 Drawing inspiration from human recognition processes, we argue that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, similar to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically tailored for few-shot learning scenarios, called FewXAT. Our approach employs deep reinforcement learning to implement the concept of hard attention, directly impacting raw input data and thus rendering the process interpretable for human understanding. Through extensive experimentation across various benchmark datasets, we demonstrate the efficacy of our proposed method.
</details></li>
</ul>
<hr>
<h2 id="A-Transfer-Learning-Based-Prognosis-Prediction-Paradigm-that-Bridges-Data-Distribution-Shift-across-EMR-Datasets"><a href="#A-Transfer-Learning-Based-Prognosis-Prediction-Paradigm-that-Bridges-Data-Distribution-Shift-across-EMR-Datasets" class="headerlink" title="A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets"></a>A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07799">http://arxiv.org/abs/2310.07799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongji Zhang, Yuhang Wang, Yinghao Zhu, Xinyu Ma, Tianlong Wang, Chaohe Zhang, Yasha Wang, Liantao Ma</li>
<li>for: 预测新疆突病和其他疾病的准确性</li>
<li>methods: 使用传输学习方法建立源数据集和目标数据集之间的转换模型，以适应不同任务域下的特征分布偏移问题</li>
<li>results: 比基eline方法高效，特别是在处理有限数据量时 display(“我们的方法可以更好地预测新疆突病和其他疾病。”)<details>
<summary>Abstract</summary>
Due to the limited information about emerging diseases, symptoms are hard to be noticed and recognized, so that the window for clinical intervention could be ignored. An effective prognostic model is expected to assist doctors in making right diagnosis and designing personalized treatment plan, so to promptly prevent unfavorable outcomes. However, in the early stage of a disease, limited data collection and clinical experiences, plus the concern out of privacy and ethics, may result in restricted data availability for reference, to the extent that even data labels are difficult to mark correctly. In addition, Electronic Medical Record (EMR) data of different diseases or of different sources of the same disease can prove to be having serious cross-dataset feature misalignment problems, greatly mutilating the efficiency of deep learning models. This article introduces a transfer learning method to build a transition model from source dataset to target dataset. By way of constraining the distribution shift of features generated in disparate domains, domain-invariant features that are exclusively relative to downstream tasks are captured, so to cultivate a unified domain-invariant encoder across various task domains to achieve better feature representation. Experimental results of several target tasks demonstrate that our proposed model outperforms competing baseline methods and has higher rate of training convergence, especially in dealing with limited data amount. A multitude of experiences have proven the efficacy of our method to provide more accurate predictions concerning newly emergent pandemics and other diseases.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)由于疾病出现的信息有限，症状难以注意和识别，因此临床 intervención的窗口可能被忽略。一个有效的预测模型可以帮助医生确定病种和制定个性化的治疗方案，以便更快地预防不利的结果。然而，疾病的早期阶段，有限的数据收集和临床经验，加上隐私和伦理的担忧，可能导致参考数据的有限性，甚至数据标签难以正确地标注。此外，不同疾病或同一种疾病的不同来源的电子医疗记录（EMR）数据可能会导致严重的跨数据集特征不一致问题，大大降低深度学习模型的效率。这篇文章介绍了一种传输学习方法，用于从源数据集转移到目标数据集。通过限制不同领域中特征生成的分布shift，捕捉固有的领域特征，以便在不同任务领域中培养一个统一的领域特征不变的编码器，以达到更好的特征表示。实验结果表明，我们提出的方法在多个目标任务上表现出色，特别是在处理有限数据量时。多种经验证明了我们的方法在新出现的流行病和其他疾病预测方面的可靠性。
</details></li>
</ul>
<hr>
<h2 id="GenTKG-Generative-Forecasting-on-Temporal-Knowledge-Graph"><a href="#GenTKG-Generative-Forecasting-on-Temporal-Knowledge-Graph" class="headerlink" title="GenTKG: Generative Forecasting on Temporal Knowledge Graph"></a>GenTKG: Generative Forecasting on Temporal Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07793">http://arxiv.org/abs/2310.07793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruotong Liao, Xu Jia, Yunpu Ma, Volker Tresp</li>
<li>for: 用于替代传统的 embedding-based 和 rule-based 模型，并在 temporal knowledge graph 领域实现生成式预测。</li>
<li>methods: 提出了一种基于 retrieval 策略和 lightweight 参数efficient  instruciton tuning的生成式预测方法，named GenTKG。</li>
<li>results: 在低计算资源下，GenTKG 比传统方法有更高的预测性能，并且在未经重新训练的情况下，在未看到的数据集上也表现出了很好的转移性。<details>
<summary>Abstract</summary>
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments have shown that GenTKG outperforms conventional methods of temporal relational forecasting under low computation resources. GenTKG also highlights remarkable transferability with exceeding performance on unseen datasets without re-training. Our work reveals the huge potential of LLMs in the tKG domain and opens a new frontier for generative forecasting on tKGs.
</details>
<details>
<summary>摘要</summary>
大Language Model (LLM)的快速进步使得temporal knowledge graph (tKG)领域受到了关注，而在这个领域，传统的经过设计的嵌入式和规则基本模型仍然主导。问题是， pré-trained LLMs能否理解结构化的时间关系数据，并取代它们作为时间关系预测的基本模型？因此，我们将 temporal knowledge forecasting 引入到生成设定中。然而，在复杂的时间图数据结构和Sequential natural expressions LLMs处理的大� Fischer  gap 和 tKGs的庞大数据量和轻量级 fine-tuning LLMs 的计算成本之间存在挑战。为解决这些挑战，我们提出了一种新的检索增强生成框架，名为 GenTKG，它结合了时间逻辑规则基本的检索策略和轻量级参数高效调整。我们的实验表明，GenTKG 在低计算资源下超过了传统的时间关系预测方法。GenTKG 还表现出了很好的转移性，在未看过的数据集上达到了 excel 的表现。我们的工作揭示了 LLMs 在 tKG 领域的巨大潜力，并开启了一个新的前ier  для generative forecasting on tKGs。
</details></li>
</ul>
<hr>
<h2 id="DrivingDiffusion-Layout-Guided-multi-view-driving-scene-video-generation-with-latent-diffusion-model"><a href="#DrivingDiffusion-Layout-Guided-multi-view-driving-scene-video-generation-with-latent-diffusion-model" class="headerlink" title="DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model"></a>DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07771">http://arxiv.org/abs/2310.07771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaofan Li, Yifu Zhang, Xiaoqing Ye</li>
<li>for: 提供高质量、大规模多视图视频数据，用于自动驾驶研究。</li>
<li>methods: 提出了一种基于 Bird’s-Eye-View（BEV）表示的协调扩散框架DrivingDiffusion，用于生成真实多视图视频。</li>
<li>results: 通过该框架，可以免费生成大规模、高质量多视图视频，用于驱动任务研究。<details>
<summary>Abstract</summary>
With the increasing popularity of autonomous driving based on the powerful and unified bird's-eye-view (BEV) representation, a demand for high-quality and large-scale multi-view video data with accurate annotation is urgently required. However, such large-scale multi-view data is hard to obtain due to expensive collection and annotation costs. To alleviate the problem, we propose a spatial-temporal consistent diffusion framework DrivingDiffusion, to generate realistic multi-view videos controlled by 3D layout. There are three challenges when synthesizing multi-view videos given a 3D layout: How to keep 1) cross-view consistency and 2) cross-frame consistency? 3) How to guarantee the quality of the generated instances? Our DrivingDiffusion solves the problem by cascading the multi-view single-frame image generation step, the single-view video generation step shared by multiple cameras, and post-processing that can handle long video generation. In the multi-view model, the consistency of multi-view images is ensured by information exchange between adjacent cameras. In the temporal model, we mainly query the information that needs attention in subsequent frame generation from the multi-view images of the first frame. We also introduce the local prompt to effectively improve the quality of generated instances. In post-processing, we further enhance the cross-view consistency of subsequent frames and extend the video length by employing temporal sliding window algorithm. Without any extra cost, our model can generate large-scale realistic multi-camera driving videos in complex urban scenes, fueling the downstream driving tasks. The code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
随着自动驾驶基于强大和统一的 bird's-eye-view (BEV) 表示的 популяр度增长，需要大量高质量多视图视频数据和准确的标注，但这些数据很难获得因为收集和标注成本高昂。为解决这个问题，我们提出了一个空间-时间一致的扩散框架 DrivingDiffusion，用于生成真实的多视图视频，控制了3D 布局。在生成多视图视频时，存在以下三个挑战：1）保持多视图视频之间的一致性和2）保持多帧视频之间的一致性？3）如何保证生成的实例质量？我们的 DrivingDiffusion 解决这些问题，通过将多视图单帧图像生成步骤、多camera共享的单视图视频生成步骤和后处理步骤，来生成真实的多视图视频。在多视图模型中，保证多视图图像之间的一致性，通过邻接相机之间的信息交换。在时间模型中，我们主要从多视图图像的首帧中提取需要注意的信息，并通过引入本地提示来提高生成的实例质量。在后处理步骤中，我们进一步提高了后续帧之间的一致性，并通过使用时间滑动窗口算法，延长视频的长度。无需额外成本，我们的模型可以生成大量高质量的多相机城市驾驶视频，为下游驾驶任务提供燃料。代码将公开。
</details></li>
</ul>
<hr>
<h2 id="PAD-A-Dataset-and-Benchmark-for-Pose-agnostic-Anomaly-Detection"><a href="#PAD-A-Dataset-and-Benchmark-for-Pose-agnostic-Anomaly-Detection" class="headerlink" title="PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection"></a>PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07716">http://arxiv.org/abs/2310.07716</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ericlee0224/pad">https://github.com/ericlee0224/pad</a></li>
<li>paper_authors: Qiang Zhou, Weize Li, Lihan Jiang, Guoliang Wang, Guyue Zhou, Shanghang Zhang, Hao Zhao</li>
<li>for: 这个论文的目的是解决对象异常检测中的两个主要挑战：第一个是现有数据集缺乏完整的视觉信息，其中数据集通常假设训练和测试样本具有相同的pose angle，但在实际应用中，异常可能存在任何对象区域，需要研究无关于pose的异常检测。第二个挑战是对于无关于pose的异常检测的实验协议的缺乏一致性，这使得不同方法之间的比较不公平，阻碍了无关于pose的异常检测的研究。</li>
<li>methods: 作者们开发了一个名为Multi-pose Anomaly Detection（MAD）数据集和Pose-agnostic Anomaly Detection（PAD）benchmark，以解决上述两个挑战。Specifically，他们使用了20种复杂形状的LEGO玩具，包括4K视图，以及高质量和多样化的3D异常在 both simulated和real environments中。此外，作者们还提出了一种名为OmniposeAD的新方法，通过使用MAD进行训练，专门设计用于无关于pose的异常检测。</li>
<li>results: 作者们通过了全面的评估，证明了他们的数据集和方法的相关性。此外，他们还提供了一个开源的benchmark库，包括数据集和基eline方法，以便未来的研究和应用。代码、数据和模型都公开可用于<a target="_blank" rel="noopener" href="https://github.com/EricLee0224/PAD%E3%80%82">https://github.com/EricLee0224/PAD。</a><details>
<summary>Abstract</summary>
Object anomaly detection is an important problem in the field of machine vision and has seen remarkable progress recently. However, two significant challenges hinder its research and application. First, existing datasets lack comprehensive visual information from various pose angles. They usually have an unrealistic assumption that the anomaly-free training dataset is pose-aligned, and the testing samples have the same pose as the training data. However, in practice, anomaly may exist in any regions on a object, the training and query samples may have different poses, calling for the study on pose-agnostic anomaly detection. Second, the absence of a consensus on experimental protocols for pose-agnostic anomaly detection leads to unfair comparisons of different methods, hindering the research on pose-agnostic anomaly detection. To address these issues, we develop Multi-pose Anomaly Detection (MAD) dataset and Pose-agnostic Anomaly Detection (PAD) benchmark, which takes the first step to address the pose-agnostic anomaly detection problem. Specifically, we build MAD using 20 complex-shaped LEGO toys including 4K views with various poses, and high-quality and diverse 3D anomalies in both simulated and real environments. Additionally, we propose a novel method OmniposeAD, trained using MAD, specifically designed for pose-agnostic anomaly detection. Through comprehensive evaluations, we demonstrate the relevance of our dataset and method. Furthermore, we provide an open-source benchmark library, including dataset and baseline methods that cover 8 anomaly detection paradigms, to facilitate future research and application in this domain. Code, data, and models are publicly available at https://github.com/EricLee0224/PAD.
</details>
<details>
<summary>摘要</summary>
“物体异常检测是机器视觉领域的重要问题，最近有很大的进步。然而，两个主要挑战是阻碍其研究和应用。第一个是现有数据集缺乏全面的视觉信息，通常假设 anomaly-free 训练数据集是同一个 pose 的，测试样本也是同一个 pose。然而，在实际情况下，异常可能存在于对象任意区域，训练和查询样本可能有不同的 pose，需要研究无 pose 的异常检测。第二个是对pose-agnostic异常检测的实验室协议缺乏共识，导致不公正的比较，阻碍研究pose-agnostic异常检测。为解决这些问题，我们开发了Multi-pose Anomaly Detection（MAD）数据集和Pose-agnostic Anomaly Detection（PAD） benchmar，这是解决pose-agnostic异常检测问题的第一步。 Specifically，我们使用了20种复杂形状的 LEGO 玩具，包括4K 视图和各种 pose，以及高质量和多样化的3D 异常在 both simulated 和实际环境中。此外，我们提出了一种新的 OmniposeAD 方法，通过 MAD 训练而得，专门针对无 pose 异常检测。通过全面的评估，我们证明了我们的数据集和方法的相关性。此外，我们还提供了一个开源的benchmark库，包括dataset和基准方法，覆盖8种异常检测思想，以便未来的研究和应用。代码、数据和模型都公开可用于https://github.com/EricLee0224/PAD。”
</details></li>
</ul>
<hr>
<h2 id="InstructRetro-Instruction-Tuning-post-Retrieval-Augmented-Pretraining"><a href="#InstructRetro-Instruction-Tuning-post-Retrieval-Augmented-Pretraining" class="headerlink" title="InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining"></a>InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07713">http://arxiv.org/abs/2310.07713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, Bryan Catanzaro</li>
<li>for: 这个论文是为了研究预训练自然语言模型（LLM）的可靠性和精度，以及如何通过外部数据库来提高这些模型的性能。</li>
<li>methods: 这个论文使用了Retrieval方法来预训练LLM，并在这个基础模型上进行了更多的预训练和调教。</li>
<li>results: 论文的实验结果表明，使用Retrieval方法预训练LLM后，可以大幅提高模型的精度和可靠性，并且可以在零基础情况下进行成功的问答 tasks。<details>
<summary>Abstract</summary>
Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval before instruction tuning. Specifically, we continue to pretrain the 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. The obtained foundation model, Retro 48B, largely outperforms the original 43B GPT in terms of perplexity. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on zero-shot question answering (QA) tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA tasks, and 10% over GPT across 4 challenging long-form QA tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. We hypothesize that pretraining with retrieval makes its decoder good at incorporating context for QA. Our results highlights the promising direction to obtain a better GPT decoder for QA through continued pretraining with retrieval before instruction tuning.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval before instruction tuning. Specifically, we continue to pretrain the 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. The obtained foundation model, Retro 48B, largely outperforms the original 43B GPT in terms of perplexity. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction tuned GPT on zero-shot question answering (QA) tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA tasks, and 10% over GPT across 4 challenging long-form QA tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. We hypothesize that pretraining with retrieval makes its decoder good at incorporating context for QA. Our results highlights the promising direction to obtain a better GPT decoder for QA through continued pretraining with retrieval before instruction tuning." into Simplified Chinese.Here's the translation:<<SYS>>预训自动循环大语言模型（LLM）与检索结合可以提高混淆率和事实准确率，通过利用外部数据库。然而，现有的预训检索增强LLM的大小仍然有限（例如，Retro有7.5亿参数），这限制了指令调整和零基数泛化的效iveness。在这种工作中，我们引入Retro 48B，最大化LLM预训检索后的指令调整。具体来说，我们继续预训43B GPT模型在additional 100亿个字符上，使用Retro增强方法，通过检索1.2万亿个字符来进行预训。获得的基础模型，Retro 48B，与原始43B GPT在混淆率上显著提高。在Retro上进行指令调整后，InstructRetro在零基数问答任务上表现出了显著提高。具体来说，InstructRetro在8个短形问答任务中平均提高7%，在4个挑战长形问答任务中提高10%。 surprisely，我们发现可以从InstructRetro架构中除去encoder，直接使用其decoder backbone，而 achieve comparable results。我们 hypothesize that预训检索使得其decoder可以好好地包含上下文。我们的结果显示了预训检索后GPT decoder的提高的可能性，并且 highlights the promising direction to obtain a better GPT decoder for QA through continued pretraining with retrieval before instruction tuning。
</details></li>
</ul>
<hr>
<h2 id="Growing-Brains-Co-emergence-of-Anatomical-and-Functional-Modularity-in-Recurrent-Neural-Networks"><a href="#Growing-Brains-Co-emergence-of-Anatomical-and-Functional-Modularity-in-Recurrent-Neural-Networks" class="headerlink" title="Growing Brains: Co-emergence of Anatomical and Functional Modularity in Recurrent Neural Networks"></a>Growing Brains: Co-emergence of Anatomical and Functional Modularity in Recurrent Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07711">http://arxiv.org/abs/2310.07711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziming Liu, Mikail Khona, Ila R. Fiete, Max Tegmark</li>
<li>for: 这个论文的目的是研究如何使用机器学习方法来实现脑模式下的神经网络结构。</li>
<li>methods: 这个论文使用的方法是一种名为“脑灵感模块化训练”（BIMT），它可以让神经网络中的神经元组织成功参与到同一些计算任务中，同时也可以使神经网络的性能更高。</li>
<li>results: 研究发现，通过使用BIMT训练神经网络，可以同时实现功能和结构的模块化，并且这些模块化的神经元也可以在不同的计算任务中保持一定的稳定性。此外，相比标准的$L_1$或无regularization设置，BIMT可以使神经网络的性能更高。<details>
<summary>Abstract</summary>
Recurrent neural networks (RNNs) trained on compositional tasks can exhibit functional modularity, in which neurons can be clustered by activity similarity and participation in shared computational subtasks. Unlike brains, these RNNs do not exhibit anatomical modularity, in which functional clustering is correlated with strong recurrent coupling and spatial localization of functional clusters. Contrasting with functional modularity, which can be ephemerally dependent on the input, anatomically modular networks form a robust substrate for solving the same subtasks in the future. To examine whether it is possible to grow brain-like anatomical modularity, we apply a recent machine learning method, brain-inspired modular training (BIMT), to a network being trained to solve a set of compositional cognitive tasks. We find that functional and anatomical clustering emerge together, such that functionally similar neurons also become spatially localized and interconnected. Moreover, compared to standard $L_1$ or no regularization settings, the model exhibits superior performance by optimally balancing task performance and network sparsity. In addition to achieving brain-like organization in RNNs, our findings also suggest that BIMT holds promise for applications in neuromorphic computing and enhancing the interpretability of neural network architectures.
</details>
<details>
<summary>摘要</summary>
Recurrent neural networks (RNNs) 训练在compositional tasks上可以显示函数含量，在这些 neurons 可以被分为活动相似性和共享计算子任务中的集群。与大脑不同，这些 RNNs 不会显示解剖学含量，解剖学含量与强回路互连和功能集群的空间地域化强相关。与函数含量不同，解剖学含量可以在输入的影响下短暂地存在。为了检查是否可以培养大脑类似的解剖学含量，我们在一个解剖学含量训练（BIMT）中训练一个解剖学含量的网络，以解决一组compositional cognitive tasks。我们发现，功能相似的 neurons 不仅在活动上相似，还在空间上受到相似的归一化和连接。此外，相比标准 $L_1$ 或无规则化设置，模型在任务性能和网络稀缺性之间取得了优质平衡，并且表现出色。除了在 RNNs 中实现大脑类似的组织结构外，我们的发现还表明BIMT在 neuromorphic computing 和增强神经网络架构的解释性方面具有潜在的潜力。
</details></li>
</ul>
<hr>
<h2 id="Pixel-State-Value-Network-for-Combined-Prediction-and-Planning-in-Interactive-Environments"><a href="#Pixel-State-Value-Network-for-Combined-Prediction-and-Planning-in-Interactive-Environments" class="headerlink" title="Pixel State Value Network for Combined Prediction and Planning in Interactive Environments"></a>Pixel State Value Network for Combined Prediction and Planning in Interactive Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07706">http://arxiv.org/abs/2310.07706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sascha Rosbach, Stefan M. Leupold, Simon Großjohann, Stefan Roth</li>
<li>for: 本研究旨在提高自动驾驶车辆在城市环境中的交通互动能力。</li>
<li>methods: 该研究提出了一种基于深度学习的方法，将预测和规划分别作为两个独立模块。 conditional GAN with U-Net architecture 是用于预测高分辨率图像序列的。</li>
<li>results: 研究结果表明，该方法可以在复杂的情况下，如车道变换 amidst conflicting objectives 中展现出直观的行为。<details>
<summary>Abstract</summary>
Automated vehicles operating in urban environments have to reliably interact with other traffic participants. Planning algorithms often utilize separate prediction modules forecasting probabilistic, multi-modal, and interactive behaviors of objects. Designing prediction and planning as two separate modules introduces significant challenges, particularly due to the interdependence of these modules. This work proposes a deep learning methodology to combine prediction and planning. A conditional GAN with the U-Net architecture is trained to predict two high-resolution image sequences. The sequences represent explicit motion predictions, mainly used to train context understanding, and pixel state values suitable for planning encoding kinematic reachability, object dynamics, safety, and driving comfort. The model can be trained offline on target images rendered by a sampling-based model-predictive planner, leveraging real-world driving data. Our results demonstrate intuitive behavior in complex situations, such as lane changes amidst conflicting objectives.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆在城市环境中必须可靠地与其他交通参与者交互。规划算法经常利用分离的预测模块预测 probabilistic、多模式和互动行为。将预测和规划分为两个模块会导致很多挑战，尤其是由于这两个模块之间的互相关系。这项工作提出了基于深度学习的方法，将预测和规划合并起来。使用 conditional GAN  WITH U-Net 架构，训练预测两个高分辨率图像序列。这两个序列表示明确的运动预测，主要用于训练上下文理解，以及适用于规划编码减速可能性、物体动力学、安全和驾驶舒适。模型可以在 target 图像上进行训练，使用采样基本的模拟预测规划器生成的图像，利用实际驾驶数据。我们的结果表明在复杂的情况下，如lane change amidst conflicting objectives， exhibit intuitive behavior。
</details></li>
</ul>
<hr>
<h2 id="From-Scarcity-to-Efficiency-Improving-CLIP-Training-via-Visual-enriched-Captions"><a href="#From-Scarcity-to-Efficiency-Improving-CLIP-Training-via-Visual-enriched-Captions" class="headerlink" title="From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched Captions"></a>From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07699">http://arxiv.org/abs/2310.07699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengfeng Lai, Haotian Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, Meng Cao</li>
<li>for: 提高CLIP模型的训练效果和数据效率</li>
<li>methods: 利用视觉概念和新生成的视觉增强caption（VeC）进行拓展和改进 caption，并提出了一种混合训练方案</li>
<li>results: 对于不同规模的原始数据进行了全面的评估，显示 VeCLIP 在图像-文本对齐和总体模型性能方面具有显著优势，例如在 COCO 和 Flickr30k 检索任务中的 Retrieval 任务中达到了20%以上的提升，同时在数据效率方面也达到了3%以上的提升。<details>
<summary>Abstract</summary>
Web-crawled datasets are pivotal to the success of pre-training vision-language models, exemplified by CLIP. However, web-crawled AltTexts can be noisy and potentially irrelevant to images, thereby undermining the crucial image-text alignment. Existing methods for rewriting captions using large language models (LLMs) have shown promise on small, curated datasets like CC3M and CC12M. Nevertheless, their efficacy on massive web-captured captions is constrained by the inherent noise and randomness in such data. In this study, we address this limitation by focusing on two key aspects: data quality and data variety. Unlike recent LLM rewriting techniques, we emphasize exploiting visual concepts and their integration into the captions to improve data quality. For data variety, we propose a novel mixed training scheme that optimally leverages AltTexts alongside newly generated Visual-enriched Captions (VeC). We use CLIP as one example and adapt the method for CLIP training on large-scale web-crawled datasets, named VeCLIP. We conduct a comprehensive evaluation of VeCLIP across small, medium, and large scales of raw data. Our results show significant advantages in image-text alignment and overall model performance, underscoring the effectiveness of VeCLIP in improving CLIP training. For example, VeCLIP achieves a remarkable over 20% improvement in COCO and Flickr30k retrieval tasks under the 12M setting. For data efficiency, we also achieve a notable over 3% improvement while using only 14% of the data employed in the vanilla CLIP and 11% in ALIGN.
</details>
<details>
<summary>摘要</summary>
网络爬取数据集是CLIP成功的关键因素，但网络爬取AltText可能是不稳定和无关的图像，从而损害图像和文本的对齐。现有的使用大型自然语言模型（LLM）重写caption的方法有所成就在小型 cura dataset上，但它们在大量网络抓取caption上的效果受限于数据的噪音和随机性。在这项研究中，我们解决这一问题，重点关注数据质量和数据多样性两个方面。与之前的LLM重写技术不同，我们强调利用视觉概念并将其 интегриinto caption中以提高数据质量。为了提高数据多样性，我们提议一种新的混合训练方案，利用AltText alongside新生成的视觉增强caption（VeC）进行优化。我们使用CLIP作为一个例子，并适应CLIP在大规模网络抓取数据上进行训练，称之为VeCLIP。我们对VeCLIP进行了广泛的评估，包括小、中和大规模的 raw data 评估。我们的结果表明，VeCLIP在图像和文本对齐方面存在显著改善，并且在COCO和Flickr30k检索任务中 achieved 辉煌的提升，尤其在12M设定下。此外，我们还在数据效率方面取得了明显的提升，只使用了14%的数据，相比于vanilla CLIP和ALIGN使用的11%和14%。
</details></li>
</ul>
<hr>
<h2 id="SurroCBM-Concept-Bottleneck-Surrogate-Models-for-Generative-Post-hoc-Explanation"><a href="#SurroCBM-Concept-Bottleneck-Surrogate-Models-for-Generative-Post-hoc-Explanation" class="headerlink" title="SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation"></a>SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07698">http://arxiv.org/abs/2310.07698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Pan, Zhenke Liu, Yifei Zhang, Liang Zhao</li>
<li>For: 这 paper 的目的是解释黑盒模型的决策过程，以提高模型的可解释性。* Methods: 这 paper 使用了 Concept Activation Vectors (CAVs) 和 Concept Bottleneck Models (CBMs) 等新的技术，以提供基于概念的解释。但是，这些技术需要人工定义的概念，可能是成本高的。因此，这 paper 提出了一种新的框架，即 Concept Bottleneck Surrogate Models (SurroCBM)，可以自动发现黑盒模型中的概念，并提供可解释的模型。* Results: 经过广泛的实验，这 paper 证明了 SurroCBM 的可行性和有效性，并且可以不断提高解释质量。这表明 SurroCBM 有可能成为黑盒模型可解释性的新途径。<details>
<summary>Abstract</summary>
Explainable AI seeks to bring light to the decision-making processes of black-box models. Traditional saliency-based methods, while highlighting influential data segments, often lack semantic understanding. Recent advancements, such as Concept Activation Vectors (CAVs) and Concept Bottleneck Models (CBMs), offer concept-based explanations but necessitate human-defined concepts. However, human-annotated concepts are expensive to attain. This paper introduces the Concept Bottleneck Surrogate Models (SurroCBM), a novel framework that aims to explain the black-box models with automatically discovered concepts. SurroCBM identifies shared and unique concepts across various black-box models and employs an explainable surrogate model for post-hoc explanations. An effective training strategy using self-generated data is proposed to enhance explanation quality continuously. Through extensive experiments, we demonstrate the efficacy of SurroCBM in concept discovery and explanation, underscoring its potential in advancing the field of explainable AI.
</details>
<details>
<summary>摘要</summary>
<<SYS>>用�ayer 的 Explainable AI 技术来揭示黑盒型模型的决策过程。传统的焦点方法可以高亮影响数据段的数据，但缺乏含义理解。最近的进步包括概念活化 вектор (CAV) 和概念瓶颈模型 (CBM)，可以提供基于概念的解释，但需要人类定义的概念。然而，人类标注的概念是有成本的获得的。这篇论文介绍了概念瓶颈代理模型 (SurroCBM)，一种新的框架，用于解释黑盒型模型的决策过程。SurroCBM 可以自动发现黑盒型模型中共享和特有的概念，并使用可解释的代理模型进行后期解释。通过自动生成的数据进行高质量的训练，可以不断提高解释质量。经过广泛的实验，我们证明 SurroCBM 在概念发现和解释方面具有极高的效果，这有助于进一步发展透明 AI 技术。
</details></li>
</ul>
<hr>
<h2 id="Hypergraph-Neural-Networks-through-the-Lens-of-Message-Passing-A-Common-Perspective-to-Homophily-and-Architecture-Design"><a href="#Hypergraph-Neural-Networks-through-the-Lens-of-Message-Passing-A-Common-Perspective-to-Homophily-and-Architecture-Design" class="headerlink" title="Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design"></a>Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07684">http://arxiv.org/abs/2310.07684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lev Telyatnikov, Maria Sofia Bucarelli, Guillermo Bernardez, Olga Zaghen, Simone Scardapane, Pietro Lio</li>
<li>for: 本文探讨了hypergraph学习领域中存在的一些问题，包括homophily在高阶网络中的作用、现有的hypergraph架构和方法的可能性，以及现有的数据集是否能够为高阶网络学习提供有意义的比较标准。</li>
<li>methods: 本文提出了一种基于消息传递方式的高阶网络内部homophily概念，并提出了一种新的消息传递框架MultiSet，以及一种基于新的超链抽样策略的新架构MultiSetMixer。</li>
<li>results: 经过广泛的实验，本文得出了许多有价值的发现，包括homophily在高阶网络中的作用、现有的hypergraph架构和方法的局限性，以及一些改进的方法和架构的可能性。<details>
<summary>Abstract</summary>
Most of the current hypergraph learning methodologies and benchmarking datasets in the hypergraph realm are obtained by lifting procedures from their graph analogs, simultaneously leading to overshadowing hypergraph network foundations. This paper attempts to confront some pending questions in that regard: Can the concept of homophily play a crucial role in Hypergraph Neural Networks (HGNNs), similar to its significance in graph-based research? Is there room for improving current hypergraph architectures and methodologies? (e.g. by carefully addressing the specific characteristics of higher-order networks) Do existing datasets provide a meaningful benchmark for HGNNs? Diving into the details, this paper proposes a novel conceptualization of homophily in higher-order networks based on a message passing scheme; this approach harmonizes the analytical frameworks of datasets and architectures, offering a unified perspective for exploring and interpreting complex, higher-order network structures and dynamics. Further, we propose MultiSet, a novel message passing framework that redefines HGNNs by allowing hyperedge-dependent node representations, as well as introduce a novel architecture MultiSetMixer that leverages a new hyperedge sampling strategy. Finally, we provide an extensive set of experiments that contextualize our proposals and lead to valuable insights in hypergraph representation learning.
</details>
<details>
<summary>摘要</summary>
Currently, most hypergraph learning methodologies and benchmarking datasets in the hypergraph realm are derived from lifting procedures from their graph analogs, which can lead to overshadowing hypergraph network foundations. This paper aims to address some outstanding questions in this regard: Can the concept of homophily play a crucial role in Hypergraph Neural Networks (HGNNs), similar to its significance in graph-based research? Is there room for improving current hypergraph architectures and methodologies? (e.g., by carefully addressing the specific characteristics of higher-order networks) Do existing datasets provide a meaningful benchmark for HGNNs?In detail, this paper proposes a novel conceptualization of homophily in higher-order networks based on a message passing scheme, which harmonizes the analytical frameworks of datasets and architectures, offering a unified perspective for exploring and interpreting complex, higher-order network structures and dynamics. Furthermore, we propose MultiSet, a novel message passing framework that redefines HGNNs by allowing hyperedge-dependent node representations, as well as introduce a novel architecture MultiSetMixer that leverages a new hyperedge sampling strategy. Finally, we provide an extensive set of experiments that contextualize our proposals and lead to valuable insights in hypergraph representation learning.Here's the translation in Traditional Chinese:现在，大多数的超гра网学方法和测试数据集在超гра网领域都是从它们的几何网领域中提取出来的，这可能会导致超гра网网络基础建筑的陌生。本文尝试回答一些尚未得到解答的问题：在超гра网神经网络（HGNN）中，认可性（homophily）是否能够扮演重要的角色，跟graph-based研究中的认可性一样？现有的数据集是否能够提供有意义的参考 benchmark для HGNN？进一步详细地说，本文提出了一个新的高阶网络中认可性的概念化方法，基于讯息传递方案，这种方法可以融合数据集和架构的分析框架，提供一个统一的见解来探索和解释高阶网络结构和动态的复杂性。此外，我们还提出了 MultiSet，一个新的讯息传递框架，它可以让超边依赖的节点表现，以及引入一个新的超边抽样策略。最后，我们提供了一系列实验，以背景和评估我们的提议，并带来有价值的见解在超гра网表示学习中。
</details></li>
</ul>
<hr>
<h2 id="Controllable-Data-Generation-Via-Iterative-Data-Property-Mutual-Mappings"><a href="#Controllable-Data-Generation-Via-Iterative-Data-Property-Mutual-Mappings" class="headerlink" title="Controllable Data Generation Via Iterative Data-Property Mutual Mappings"></a>Controllable Data Generation Via Iterative Data-Property Mutual Mappings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07683">http://arxiv.org/abs/2310.07683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Pan, Muran Qin, Shiyu Wang, Yifei Zhang, Liang Zhao</li>
<li>for: 这个论文的目的是提高基于VAE的数据生成器的控制性和分离性。</li>
<li>methods: 这个论文提出了一种普适的框架，通过在数据和属性之间进行互相映射，来增强VAE基于的数据生成器的控制性和分离性。</li>
<li>results: 实验结果表明，该框架可以在短时间内准确地控制生成样本的属性，同时保持生成样本的有效性和分离性。<details>
<summary>Abstract</summary>
Deep generative models have been widely used for their ability to generate realistic data samples in various areas, such as images, molecules, text, and speech. One major goal of data generation is controllability, namely to generate new data with desired properties. Despite growing interest in the area of controllable generation, significant challenges still remain, including 1) disentangling desired properties with unrelated latent variables, 2) out-of-distribution property control, and 3) objective optimization for out-of-distribution property control. To address these challenges, in this paper, we propose a general framework to enhance VAE-based data generators with property controllability and ensure disentanglement. Our proposed objective can be optimized on both data seen and unseen in the training set. We propose a training procedure to train the objective in a semi-supervised manner by iteratively conducting mutual mappings between the data and properties. The proposed framework is implemented on four VAE-based controllable generators to evaluate its performance on property error, disentanglement, generation quality, and training time. The results indicate that our proposed framework enables more precise control over the properties of generated samples in a short training time, ensuring the disentanglement and keeping the validity of the generated samples.
</details>
<details>
<summary>摘要</summary>
Translation notes:* 离干分离 (lìgǎn fēnhū) refers to the problem of disentangling desired properties from unrelated latent variables.* OUT-OF-DISTRIBUTION (OUT-OF-DISTRIBUTION) refers to the challenge of controlling properties that are not present in the training data.* 目标优化 (mèngtǎo yòujiā) refers to the process of optimizing the objective function to achieve the desired properties.* VAE-based controllable generators (VAE-based kòngzhì yǐngchǎng) refers to the generative models that use Variational Autoencoders (VAEs) to generate data with desired properties.
</details></li>
</ul>
<hr>
<h2 id="Explainable-Image-Similarity-Integrating-Siamese-Networks-and-Grad-CAM"><a href="#Explainable-Image-Similarity-Integrating-Siamese-Networks-and-Grad-CAM" class="headerlink" title="Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM"></a>Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07678">http://arxiv.org/abs/2310.07678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis E. Livieris, Emmanuel Pintelas, Niki Kiriakidou, Panagiotis Pintelas</li>
<li>for: 提高图像相似性评估的可解释性，以便更好地理解图像之间的相似性原因。</li>
<li>methods: 提出了一种基于Siamese网络和Grad-CAM的图像相似性评估方法，并提供了可视化的实际和假设性解释。</li>
<li>results: 提出了一种新的图像相似性评估框架，可以提供可解释的图像相似性分数以及实际和假设性解释，并且有可能提高图像基于系统的解释性、可靠性和用户接受度。<details>
<summary>Abstract</summary>
With the proliferation of image-based applications in various domains, the need for accurate and interpretable image similarity measures has become increasingly critical. Existing image similarity models often lack transparency, making it challenging to understand the reasons why two images are considered similar. In this paper, we propose the concept of explainable image similarity, where the goal is the development of an approach, which is capable of providing similarity scores along with visual factual and counterfactual explanations. Along this line, we present a new framework, which integrates Siamese Networks and Grad-CAM for providing explainable image similarity and discuss the potential benefits and challenges of adopting this approach. In addition, we provide a comprehensive discussion about factual and counterfactual explanations provided by the proposed framework for assisting decision making. The proposed approach has the potential to enhance the interpretability, trustworthiness and user acceptance of image-based systems in real-world image similarity applications. The implementation code can be found in https://github.com/ioannislivieris/Grad_CAM_Siamese.git.
</details>
<details>
<summary>摘要</summary>
We present a new framework that integrates Siamese Networks and Grad-CAM for providing explainable image similarity. This approach has the potential to enhance the interpretability, trustworthiness, and user acceptance of image-based systems in real-world image similarity applications.In addition, we provide a comprehensive discussion of the factual and counterfactual explanations provided by the proposed framework, which can assist decision-making. The proposed approach has the potential to improve the interpretability and trustworthiness of image-based systems, and the implementation code can be found at https://github.com/ioannislivieris/Grad_CAM_Siamese.git.
</details></li>
</ul>
<hr>
<h2 id="Accountability-in-Offline-Reinforcement-Learning-Explaining-Decisions-with-a-Corpus-of-Examples"><a href="#Accountability-in-Offline-Reinforcement-Learning-Explaining-Decisions-with-a-Corpus-of-Examples" class="headerlink" title="Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples"></a>Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07747">http://arxiv.org/abs/2310.07747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Sun, Alihan Hüyük, Daniel Jarrett, Mihaela van der Schaar</li>
<li>for: 本研究旨在提出一种负责任控制器，以便在决策系统中减少实际应用中的风险。</li>
<li>methods: 本研究使用了离线数据集作为决策 cuerpo，并根据特定的例子选择（称为 Corpus Subset）进行负责任控制。</li>
<li>results: 研究表明，AOC可以在低数据量情况下运行，并且可以在具有离线imitating设定的情况下进行扩展。AOC在模拟和实际医疗应用中表现出了高水平的性能，同时保持负责任。<details>
<summary>Abstract</summary>
Learning controllers with offline data in decision-making systems is an essential area of research due to its potential to reduce the risk of applications in real-world systems. However, in responsibility-sensitive settings such as healthcare, decision accountability is of paramount importance, yet has not been adequately addressed by the literature. This paper introduces the Accountable Offline Controller (AOC) that employs the offline dataset as the Decision Corpus and performs accountable control based on a tailored selection of examples, referred to as the Corpus Subset. AOC operates effectively in low-data scenarios, can be extended to the strictly offline imitation setting, and displays qualities of both conservation and adaptability. We assess AOC's performance in both simulated and real-world healthcare scenarios, emphasizing its capability to manage offline control tasks with high levels of performance while maintaining accountability.
</details>
<details>
<summary>摘要</summary>
学习控制器使用停机数据进行决策系统的研究是一个非常重要的领域，因为它可以减少实际系统中的风险。然而，在责任感知的设置中，决策责任并没有得到文献充分考虑。这篇论文介绍了负责任控制器（AOC），它使用停机dataset作为决策体系，并基于定制的示例选择（称为Corpus Subset）进行负责任控制。AOC在低数据情况下运行得非常有效，可以扩展到严格的停机模仿环境，并表现出保守和适应的特点。我们在模拟和实际医疗场景中评估了AOC的性能，强调它在处理停机控制任务时能够达到高水平的性能，同时保持责任。
</details></li>
</ul>
<hr>
<h2 id="HaarNet-Large-scale-Linear-Morphological-Hybrid-Network-for-RGB-D-Semantic-Segmentation"><a href="#HaarNet-Large-scale-Linear-Morphological-Hybrid-Network-for-RGB-D-Semantic-Segmentation" class="headerlink" title="HaarNet: Large-scale Linear-Morphological Hybrid Network for RGB-D Semantic Segmentation"></a>HaarNet: Large-scale Linear-Morphological Hybrid Network for RGB-D Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07669">http://arxiv.org/abs/2310.07669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rick Groenendijk, Leo Dorst, Theo Gevers</li>
<li>for: 本研究旨在开探使用多样性模式的约束来提高RGB-D数据的处理和分析效能。</li>
<li>methods: 本文提出了一种混合线性- morphological 网络，称为 HaarNet，使用了 morphological 元素和常见的线性模块。</li>
<li>results: 实验表明， HaarNet 与当前最佳 CNN 相当竞争，表明 morphological 网络是 geometry-based 学习任务的可能的研究方向。<details>
<summary>Abstract</summary>
Signals from different modalities each have their own combination algebra which affects their sampling processing. RGB is mostly linear; depth is a geometric signal following the operations of mathematical morphology. If a network obtaining RGB-D input has both kinds of operators available in its layers, it should be able to give effective output with fewer parameters. In this paper, morphological elements in conjunction with more familiar linear modules are used to construct a mixed linear-morphological network called HaarNet. This is the first large-scale linear-morphological hybrid, evaluated on a set of sizeable real-world datasets. In the network, morphological Haar sampling is applied to both feature channels in several layers, which splits extreme values and high-frequency information such that both can be processed to improve both modalities. Moreover, morphologically parameterised ReLU is used, and morphologically-sound up-sampling is applied to obtain a full-resolution output. Experiments show that HaarNet is competitive with a state-of-the-art CNN, implying that morphological networks are a promising research direction for geometry-based learning tasks.
</details>
<details>
<summary>摘要</summary>
文本中的不同modalities每个都有自己的组合代数，这些组合代数会影响它们的采样处理。RGB主要是线性的；深度是一种几何信号，按照数学形态学的操作进行处理。如果一个网络获得RGB-D输入，这个网络中有多种操作可以在其层次结构中使用，那么它应该能够在 fewer parameters 下提供有效的输出。在这篇论文中，作者使用了 conjunction 的 linear-morphological 网络，称之为 HaarNet。这是第一个大规模的线性-几何混合网络，在一些实际世界的数据集上进行了评估。在网络中，morphological Haar sampling 是在多个层次应用于特征通道上，将极端值和高频信息分割，以便进一步处理这两个模式。此外，使用了 morphologically parameterized ReLU 和 morphologically-sound up-sampling，以获得全分辨率输出。实验表明，HaarNet 与state-of-the-art CNN 相当竞争，implying that morphological networks 是一个有前途的研究方向 для基于几何学的学习任务。
</details></li>
</ul>
<hr>
<h2 id="GRaMuFeN-Graph-based-Multi-modal-Fake-News-Detection-in-Social-Media"><a href="#GRaMuFeN-Graph-based-Multi-modal-Fake-News-Detection-in-Social-Media" class="headerlink" title="GRaMuFeN: Graph-based Multi-modal Fake News Detection in Social Media"></a>GRaMuFeN: Graph-based Multi-modal Fake News Detection in Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07668">http://arxiv.org/abs/2310.07668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Makan Kananian, Fatima Badiei, S. AmirAli Gh. Ghahramani</li>
<li>for: 检测假信息在社交媒体平台上的扩散，提高公众意见形成的真实性。</li>
<li>methods: 提议使用文本encoder和图像encoder组合，文本encoder使用图Converter网络（GCN）进行文本分析，图像encoder使用预训练的ResNet-152 convolutional neural network（CNN）进行图像分析，并实现对比相似度损失函数，以提高检测假信息的精度。</li>
<li>results: 对两个公共可用的社交媒体新闻数据集进行了广泛的评估，相比现有的状态之artifact，提高了10%的微 F1-Score，表明GCN和CNN模型的组合可以有效地检测社交媒体上的假信息。<details>
<summary>Abstract</summary>
The proliferation of social media platforms such as Twitter, Instagram, and Weibo has significantly enhanced the dissemination of false information. This phenomenon grants both individuals and governmental entities the ability to shape public opinions, highlighting the need for deploying effective detection methods. In this paper, we propose GraMuFeN, a model designed to detect fake content by analyzing both the textual and image content of news. GraMuFeN comprises two primary components: a text encoder and an image encoder. For textual analysis, GraMuFeN treats each text as a graph and employs a Graph Convolutional Neural Network (GCN) as the text encoder. Additionally, the pre-trained ResNet-152, as a Convolutional Neural Network (CNN), has been utilized as the image encoder. By integrating the outputs from these two encoders and implementing a contrastive similarity loss function, GraMuFeN achieves remarkable results. Extensive evaluations conducted on two publicly available benchmark datasets for social media news indicate a 10 % increase in micro F1-Score, signifying improvement over existing state-of-the-art models. These findings underscore the effectiveness of combining GCN and CNN models for detecting fake news in multi-modal data, all while minimizing the additional computational burden imposed by model parameters.
</details>
<details>
<summary>摘要</summary>
“社交媒体平台的普及，如Twitter、Instagram和微博，已经提高了假信息的传播。这种现象让个人和政府机构都可以影响公众意见，高亮了需要部署有效的检测方法。本文提出了GraMuFeN模型，用于检测假新闻。GraMuFeN包括两个主要组成部分：文本编码器和图像编码器。对文本分析，GraMuFeN将每个文本视为一个图，并使用图 convolutional neural network (GCN) 作为文本编码器。此外，预训练的 ResNet-152 也被用作图像编码器。通过将这两个编码器的输出集成并实现对比相似性损失函数，GraMuFeN实现了显著的效果。对社交媒体新闻两个公共可用的 benchmark 数据集进行了广泛的评估，GraMuFeN 在 micro F1-Score 方面提高了10%，表明与现有状态码模型相比有显著的提高。这些发现表明了将 GCN 和 CNN 模型结合使用可以在多模式数据中检测假新闻，同时减少模型参数所增加的计算负担。”
</details></li>
</ul>
<hr>
<h2 id="Global-Minima-Recoverability-Thresholds-and-Higher-Order-Structure-in-GNNS"><a href="#Global-Minima-Recoverability-Thresholds-and-Higher-Order-Structure-in-GNNS" class="headerlink" title="Global Minima, Recoverability Thresholds, and Higher-Order Structure in GNNS"></a>Global Minima, Recoverability Thresholds, and Higher-Order Structure in GNNS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07667">http://arxiv.org/abs/2310.07667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Drake Brown, Trevor Garrity, Kaden Parker, Jason Oliphant, Stone Carson, Cole Hanson, Zachary Boyd</li>
<li>for: 这个论文探讨了图 neural network（GNN）架构的性能从Random Graph Theory的角度。</li>
<li>methods: 作者使用了理论和数值方法来分析GNN的性能，包括对一层和二层GCNs的nodewise准确率的理论分析，以及对四种不同的GNN架构（GCN、GAT、SAGE和Graph Transformer）在不同假设下的数值分析。</li>
<li>results: 作者发现了一些关键的结论，包括：重 tailed degree distribution可以提高GNN性能，GNN可以在强烈不同结构上工作，SAGE和Graph Transformer可以在无isy edge数据上工作，但是没有架构能够处理足够噪音特征数据。此外，作者发现了一些特定的高阶结构在 sintetic data中和实际数据中的杂合效果通常是负面的。<details>
<summary>Abstract</summary>
We analyze the performance of graph neural network (GNN) architectures from the perspective of random graph theory. Our approach promises to complement existing lenses on GNN analysis, such as combinatorial expressive power and worst-case adversarial analysis, by connecting the performance of GNNs to typical-case properties of the training data. First, we theoretically characterize the nodewise accuracy of one- and two-layer GCNs relative to the contextual stochastic block model (cSBM) and related models. We additionally prove that GCNs cannot beat linear models under certain circumstances. Second, we numerically map the recoverability thresholds, in terms of accuracy, of four diverse GNN architectures (GCN, GAT, SAGE, and Graph Transformer) under a variety of assumptions about the data. Sample results of this second analysis include: heavy-tailed degree distributions enhance GNN performance, GNNs can work well on strongly heterophilous graphs, and SAGE and Graph Transformer can perform well on arbitrarily noisy edge data, but no architecture handled sufficiently noisy feature data well. Finally, we show how both specific higher-order structures in synthetic data and the mix of empirical structures in real data have dramatic effects (usually negative) on GNN performance.
</details>
<details>
<summary>摘要</summary>
我们从Random graph theory的角度分析图 neural network（GNN）的性能。我们的方法可以补充现有的GNN分析方法，如 combinatorial expressive power和最坏情况攻击分析，以连接GNN的性能和训练数据的典型特性。首先，我们理论上Characterize the node accuracy of one- and two-layer GCNs relative to the contextual stochastic block model (cSBM) and related models。我们还证明GCNs不能在某些情况下超过线性模型。其次，我们 numerically map the recoverability thresholds, in terms of accuracy, of four diverse GNN architectures (GCN, GAT, SAGE, and Graph Transformer) under a variety of assumptions about the data。 Sample results of this second analysis include: heavy-tailed degree distributions enhance GNN performance, GNNs can work well on strongly heterophilous graphs, and SAGE and Graph Transformer can perform well on arbitrarily noisy edge data, but no architecture handled sufficiently noisy feature data well。最后，我们示出了 especific higher-order structures in synthetic data and the mix of empirical structures in real data have dramatic effects (usually negative) on GNN performance。
</details></li>
</ul>
<hr>
<h2 id="Deep-Backtracking-Counterfactuals-for-Causally-Compliant-Explanations"><a href="#Deep-Backtracking-Counterfactuals-for-Causally-Compliant-Explanations" class="headerlink" title="Deep Backtracking Counterfactuals for Causally Compliant Explanations"></a>Deep Backtracking Counterfactuals for Causally Compliant Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07665">http://arxiv.org/abs/2310.07665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Klaus-Rudolf Kladny, Julius von Kügelgen, Bernhard Schölkopf, Michael Muehlebach</li>
<li>for: 本文研究了Counterfactuals的一种新方法，即backtracking方法，可以在结构 causal models 中计算出 conditional counterfactuals。</li>
<li>methods: 本文提出了一种实用的方法，通过在结构 latent space 中做 tractable constrained optimization 问题，来生成 backtracking counterfactuals。</li>
<li>results: 实验表明， compared to existing methods of counterfactual explanations, 本文的方法更加 versatile, modular, and causally compliant。<details>
<summary>Abstract</summary>
Counterfactuals can offer valuable insights by answering what would have been observed under altered circumstances, conditional on a factual observation. Whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative the backtracking principle has emerged as an alternative philosophy where all causal laws are kept intact. In the present work, we introduce a practical method for computing backtracking counterfactuals in structural causal models that consist of deep generative components. To this end, we impose conditions on the structural assignments that enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of a causal model. Our formulation also facilitates a comparison with methods in the field of counterfactual explanations. Compared to these, our method represents a versatile, modular and causally compliant alternative. We demonstrate these properties experimentally on a modified version of MNIST and CelebA.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译成简化中文。<</SYS>> counterfactuals 可以提供有价值的洞察，回答在修改条件下所观察到的结果。 classical interventional interpretation of counterfactuals 已经得到了广泛的研究，而 backtracking 则是一种 less studied alternative 。 backtracking principle 是一种具有保持所有 causal laws 的哲学原则。 在当前的工作中，我们介绍了一种实用的计算 backtracking counterfactuals 的方法，该方法在 structural causal models 中包含深度生成组件。 为此，我们在 causal model 中做出了特定的结构分配，以便通过解决一个可解决的封闭优化问题在 structured latent space 中生成 counterfactuals。 我们的формаulation 还可以与 counterfactual explanations 方法进行比较，相比之下，我们的方法表现出了 versatile、modular 和 causally compliant 的性能。 我们在一个修改后的 MNIST 和 CelebA 上进行了实验来证明这一点。
</details></li>
</ul>
<hr>
<h2 id="Mini-DALLE3-Interactive-Text-to-Image-by-Prompting-Large-Language-Models"><a href="#Mini-DALLE3-Interactive-Text-to-Image-by-Prompting-Large-Language-Models" class="headerlink" title="Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models"></a>Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07653">http://arxiv.org/abs/2310.07653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Zeqiang-Lai/MiniDALLE-3">https://github.com/Zeqiang-Lai/MiniDALLE-3</a></li>
<li>paper_authors: Zeqiang Lai, Xizhou Zhu, Jifeng Dai, Yu Qiao, Wenhai Wang</li>
<li>for: 这个研究旨在探讨如何使用自然语言描述来与高品质的文字至图模型（T2I）进行有效的沟通，以及如何将这种技术应用于实际的人机交互中。</li>
<li>methods: 本研究使用了调整提示的技术和现有的T2I模型来解决问题，并评估了这种方法在不同的语言模型（LLM）和T2I模型下的效果。</li>
<li>results: 研究发现，这种方法可以让LLMs拥有更好的图像质量和更强的文字与图像相互对应，并且可以让任何现有的LLMs和T2I模型都具备这种能力，而且不需要进行任何训练。<details>
<summary>Abstract</summary>
The revolution of artificial intelligence content generation has been rapidly accelerated with the booming text-to-image (T2I) diffusion models. Within just two years of development, it was unprecedentedly of high-quality, diversity, and creativity that the state-of-the-art models could generate. However, a prevalent limitation persists in the effective communication with these popular T2I models, such as Stable Diffusion, using natural language descriptions. This typically makes an engaging image hard to obtain without expertise in prompt engineering with complex word compositions, magic tags, and annotations. Inspired by the recently released DALLE3 - a T2I model directly built-in ChatGPT that talks human language, we revisit the existing T2I systems endeavoring to align human intent and introduce a new task - interactive text to image (iT2I), where people can interact with LLM for interleaved high-quality image generation/edit/refinement and question answering with stronger images and text correspondences using natural language. In addressing the iT2I problem, we present a simple approach that augments LLMs for iT2I with prompting techniques and off-the-shelf T2I models. We evaluate our approach for iT2I in a variety of common-used scenarios under different LLMs, e.g., ChatGPT, LLAMA, Baichuan, and InternLM. We demonstrate that our approach could be a convenient and low-cost way to introduce the iT2I ability for any existing LLMs and any text-to-image models without any training while bringing little degradation on LLMs' inherent capabilities in, e.g., question answering and code generation. We hope this work could draw broader attention and provide inspiration for boosting user experience in human-machine interactions alongside the image quality of the next-generation T2I systems.
</details>
<details>
<summary>摘要</summary>
人工智能内容生成革命已经快速加速，特别是在文本到图像（T2I）扩散模型方面。只用两年的时间，这些模型的质量、多样性和创造力已经到了历史的新高度。然而，在使用自然语言描述与这些流行的T2I模型进行有效交流仍然存在一定的限制，这通常需要专业的提示工程师、魔法标签和注释。 drawing inspiration from the recently released DALLE3 - a T2I model directly built-in ChatGPT that talks human language, we revisit the existing T2I systems and introduce a new task - interactive text to image (iT2I), where people can interact with LLM for interleaved high-quality image generation/edit/refinement and question answering with stronger images and text correspondences using natural language.在解决iT2I问题时，我们提出了一种简单的方法，即在LLMs中进行iT2I的扩展，使用提示技术和现有的T2I模型。我们在不同的LLMs（如ChatGPT、LLAMA、Baichuan和InternLM）下进行了多种常见的场景的评估。我们的方法可以让任何现有的LLMs和任何文本到图像模型都具备iT2I能力，而无需训练，同时带来对LLMs的内置能力的影响很小。我们希望这种工作能吸引更广泛的关注，并为下一代T2I系统的图像质量和人机交互的进步提供灵感。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-the-BERT-like-Pretraining-for-DNA-Sequences"><a href="#Rethinking-the-BERT-like-Pretraining-for-DNA-Sequences" class="headerlink" title="Rethinking the BERT-like Pretraining for DNA Sequences"></a>Rethinking the BERT-like Pretraining for DNA Sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07644">http://arxiv.org/abs/2310.07644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoqi Liang, Weiqiang Bai, Lifeng Qiao, Yuchen Ren, Jianle Sun, Peng Ye, Hongliang Yan, Xinzhu Ma, Wangmeng Zuo, Wanli Ouyang</li>
<li>for: 这份研究旨在探讨如何将大规模预训应用到生物科学领域，特别是基于DNA序列的预训方法。</li>
<li>methods: 研究人员首先执行了一系列的探索性实验，获得了许多有益的观察，包括：在下游任务 fine-tuning 阶段，使用 K-mer 重叠tokenization 而不是 K-mer 非重叠tokenization，两者都可以在下游任务中获得显著的性能改进。</li>
<li>results: 研究人员发现，使用 K-mer 重叠tokenization 在预训过程中可以迅速生成明确的 K-mer 嵌入，并降低损失到非常低水平，但使用 K-mer 非重叠tokenization 则会导致嵌入变得更模糊，并持续降低损失。此外，使用重叠tokenization 会导致预训模型的自我注意力在中间层中过度集中在某些字串上，显示这些层未能得到适当的优化。总之，重叠tokenization 可以帮助下游任务的 fine-tuning，但它会导致预训过程中的快速收敛。为了解开预训的潜力，研究人员提出了一种新的方法called RandomMask，它通过不断扩展隐藏界限来增加BERT-like预训的任务难度，并成功取得了26个数据集中的28个数据集上的7个下游任务的Top-tier表现。<details>
<summary>Abstract</summary>
With the success of large-scale pretraining in NLP, there is an increasing trend of applying it to the domain of life sciences. In particular, pretraining methods based on DNA sequences have garnered growing attention due to their potential to capture generic information about genes. However, existing pretraining methods for DNA sequences largely rely on direct adoptions of BERT pretraining from NLP, lacking a comprehensive understanding and a specifically tailored approach. To address this research gap, we first conducted a series of exploratory experiments and gained several insightful observations: 1) In the fine-tuning phase of downstream tasks, when using K-mer overlapping tokenization instead of K-mer non-overlapping tokenization, both overlapping and non-overlapping pretraining weights show consistent performance improvement.2) During the pre-training process, using K-mer overlapping tokenization quickly produces clear K-mer embeddings and reduces the loss to a very low level, while using K-mer non-overlapping tokenization results in less distinct embeddings and continuously decreases the loss. 3) Using overlapping tokenization causes the self-attention in the intermediate layers of pre-trained models to tend to overly focus on certain tokens, reflecting that these layers are not adequately optimized. In summary, overlapping tokenization can benefit the fine-tuning of downstream tasks but leads to inadequate pretraining with fast convergence. To unleash the pretraining potential, we introduce a novel approach called RandomMask, which gradually increases the task difficulty of BERT-like pretraining by continuously expanding its mask boundary, forcing the model to learn more knowledge. RandomMask is simple but effective, achieving top-tier performance across 26 datasets of 28 datasets spanning 7 downstream tasks.
</details>
<details>
<summary>摘要</summary>
随着人工智能的应用在生命科学领域的扩大，对于基因序列的预训练方法也在吸引越来越多的关注。特别是基因序列预训练方法，因为它们可能会捕捉到生物体中 generic 信息。然而，现有的基因序列预训练方法大多基于 NLP 中的 BERT 预训练方法，lacking a comprehensive understanding and a specifically tailored approach。为了填补这个研究漏洞，我们首先进行了一系列的探索性实验，获得了一些有价值的观察：1）在下游任务 fine-tuning 阶段，使用 K-mer  overlap 的tokenization而不是 K-mer non-overlapping 的tokenization， both overlapping 和 non-overlapping 预训练权重都显示了一致的性能提升。2）在预训练过程中，使用 K-mer overlap 的tokenization快速生成了明确的 K-mer 嵌入，并将损失降到了非常低的水平，而使用 K-mer non-overlapping 的tokenization则导致了较为模糊的嵌入，并持续降低损失。3）使用 overlap 的tokenization会让预训练模型中的自注意力倾向于过度关注某些符号，表明这些层并未充分优化。总之，overlapping 的tokenization可以优化下游任务的 fine-tuning，但是会导致预训练快速收敛。为了解锁预训练的潜力，我们提出了一种新的方法RandomMask，它通过不断扩展BERT-like 预训练模型的mask边界来增加任务难度，让模型学习更多的知识。RandomMask 简单 yet effective，在 28 个数据集上的 26 个任务上实现了顶尖表现。
</details></li>
</ul>
<hr>
<h2 id="OpsEval-A-Comprehensive-Task-Oriented-AIOps-Benchmark-for-Large-Language-Models"><a href="#OpsEval-A-Comprehensive-Task-Oriented-AIOps-Benchmark-for-Large-Language-Models" class="headerlink" title="OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models"></a>OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07637">http://arxiv.org/abs/2310.07637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhe Liu, Changhua Pei, Longlong Xu, Bohan Chen, Mingze Sun, Zhirui Zhang, Yongqian Sun, Shenglin Zhang, Kun Wang, Haiming Zhang, Jianhui Li, Gaogang Xie, Xidao Wen, Xiaohui Nie, Dan Pei<br>for:The paper is written to evaluate the performance of large language models (LLMs) in Artificial Intelligence for IT Operations (AIOps) tasks.methods:The paper presents a comprehensive task-oriented AIOps benchmark called OpsEval, which includes 7,200 questions in both multiple-choice and question-answer formats to assess LLMs’ proficiency in three crucial scenarios (Wired Network Operation, 5G Communication Operation, and Database Operation) at various ability levels.results:The paper shows that GPT4-score is more consistent with experts than widely used Bleu and Rouge, and that various LLM tricks can affect the performance of AIOps, including zero-shot, chain-of-thought, and few-shot in-context learning. The paper also provides quantitative and qualitative results that demonstrate the effectiveness of OpsEval in evaluating LLMs’ performance in AIOps tasks.<details>
<summary>Abstract</summary>
Large language models (LLMs) have exhibited remarkable capabilities in NLP-related tasks such as translation, summarizing, and generation. The application of LLMs in specific areas, notably AIOps (Artificial Intelligence for IT Operations), holds great potential due to their advanced abilities in information summarizing, report analyzing, and ability of API calling. Nevertheless, the performance of current LLMs in AIOps tasks is yet to be determined. Furthermore, a comprehensive benchmark is required to steer the optimization of LLMs tailored for AIOps. Compared with existing benchmarks that focus on evaluating specific fields like network configuration, in this paper, we present \textbf{OpsEval}, a comprehensive task-oriented AIOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs' proficiency in three crucial scenarios (Wired Network Operation, 5G Communication Operation, and Database Operation) at various ability levels (knowledge recall, analytical thinking, and practical application). The benchmark includes 7,200 questions in both multiple-choice and question-answer (QA) formats, available in English and Chinese. With quantitative and qualitative results, we show how various LLM tricks can affect the performance of AIOps, including zero-shot, chain-of-thought, and few-shot in-context learning. We find that GPT4-score is more consistent with experts than widely used Bleu and Rouge, which can be used to replace automatic metrics for large-scale qualitative evaluations.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在自然语言处理（NLP）相关任务中表现出了非常出色的能力，包括翻译、摘要和生成等。在特定领域中应用LLM的潜在性非常大，尤其是在人工智能操作（AIOps）中，因为它们在资讯摘要、报告分析和API调用等方面有出色的能力。然而，目前LLM在AIOps任务中的表现仍未被评估。此外，为了适当地优化LLM，需要一个全面的标准参考。相比于现有的标准，这篇文章提出了一个名为“OpsEval”的全面的AIOps标准参考，用于评估LLM的能力。OpsEval包括三个重要的操作场景（有线网络操作、5G通信操作和数据库操作），并且在不同的能力水平（知识回传、分析思维和实践应用）进行评估。标准包括7,200个问题，分为多选和问题回答（QA）格式，英文和中文两种语言。我们通过量化和质量的结果显示出不同的LLM技巧可以如何影响AIOps的表现，包括零式、串行和少数内容学习。我们发现GPT4-score与专家的表现更一致，而Bleu和Rouge的自动评分可以用来取代大规模的质量评分。
</details></li>
</ul>
<hr>
<h2 id="Dual-Quaternion-Rotational-and-Translational-Equivariance-in-3D-Rigid-Motion-Modelling"><a href="#Dual-Quaternion-Rotational-and-Translational-Equivariance-in-3D-Rigid-Motion-Modelling" class="headerlink" title="Dual Quaternion Rotational and Translational Equivariance in 3D Rigid Motion Modelling"></a>Dual Quaternion Rotational and Translational Equivariance in 3D Rigid Motion Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07623">http://arxiv.org/abs/2310.07623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guilherme Vieira, Eleonora Grassucci, Marcos Eduardo Valle, Danilo Comminiello</li>
<li>for: 本 paper 的目的是提出一种基于 dual quaternion 表示的3D空间中对象的刚性运动模型，以便更好地处理3D学习任务。</li>
<li>methods: 本 paper 使用 dual quaternion 表示法来模型3D空间中对象的刚性运动，并且通过对每个点进行同时旋转和平移的表示，保留了点集的相关性。</li>
<li>results: 实验证明，使用本 paper 提出的 dual quaternion 表示法可以在人姿预测任务中超越前一些方法，表明该方法在3D学习任务中的效果。<details>
<summary>Abstract</summary>
Objects' rigid motions in 3D space are described by rotations and translations of a highly-correlated set of points, each with associated $x,y,z$ coordinates that real-valued networks consider as separate entities, losing information. Previous works exploit quaternion algebra and their ability to model rotations in 3D space. However, these algebras do not properly encode translations, leading to sub-optimal performance in 3D learning tasks. To overcome these limitations, we employ a dual quaternion representation of rigid motions in the 3D space that jointly describes rotations and translations of point sets, processing each of the points as a single entity. Our approach is translation and rotation equivariant, so it does not suffer from shifts in the data and better learns object trajectories, as we validate in the experimental evaluations. Models endowed with this formulation outperform previous approaches in a human pose forecasting application, attesting to the effectiveness of the proposed dual quaternion formulation for rigid motions in 3D space.
</details>
<details>
<summary>摘要</summary>
三维空间中物体的刚性运动被描述为旋转和平移的高相关点集，每个点有相应的 $x,y,z$ 坐标，但是实值网络视为每个点为独立实体，导致信息损失。先前的工作利用四元数代数来模型旋转运动，但这些代数不能正确地表示平移，从而导致三维学习任务中的下降性能。为了解决这些限制，我们使用三元数表示方式来描述点集的刚性运动，对每个点进行单一处理。我们的方法具有平移和旋转对称性，因此不会受到数据的偏移，更好地学习物体的运动轨迹，如我们在实验评估中所证明。使用这种形式的模型，与先前的方法相比，在人姿预测应用中表现出色，证明了我们的双元数表示方法在三维空间中的刚性运动的有效性。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-based-Knowledge-Graph-Reasoning-for-Explainable-Fact-checking"><a href="#Reinforcement-Learning-based-Knowledge-Graph-Reasoning-for-Explainable-Fact-checking" class="headerlink" title="Reinforcement Learning-based Knowledge Graph Reasoning for Explainable Fact-checking"></a>Reinforcement Learning-based Knowledge Graph Reasoning for Explainable Fact-checking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07613">http://arxiv.org/abs/2310.07613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gustav Nikopensius, Mohit Mayank, Orchid Chetia Phukan, Rajesh Sharma<br>for:The paper aims to improve the trustworthiness of automated fact-checking systems by incorporating reinforcement learning (RL) and knowledge graph (KG) reasoning for explainable fact-checking.methods:The proposed approach uses RL to train an agent to compute paths that prove or disprove factual claims, and a voting mechanism to reach a verdict based on the paths produced by the agent. The KG is used to represent knowledge for explanations.results:Extensive experiments on two datasets (FB15K-277 and NELL-995) show that the proposed approach is effective in producing human-readable explanations in the form of paths and classifications for fact claims, and can increase trustworthiness by providing a human-in-the-loop approach.<details>
<summary>Abstract</summary>
Fact-checking is a crucial task as it ensures the prevention of misinformation. However, manual fact-checking cannot keep up with the rate at which false information is generated and disseminated online. Automated fact-checking by machines is significantly quicker than by humans. But for better trust and transparency of these automated systems, explainability in the fact-checking process is necessary. Fact-checking often entails contrasting a factual assertion with a body of knowledge for such explanations. An effective way of representing knowledge is the Knowledge Graph (KG). There have been sufficient works proposed related to fact-checking with the usage of KG but not much focus is given to the application of reinforcement learning (RL) in such cases. To mitigate this gap, we propose an RL-based KG reasoning approach for explainable fact-checking. Extensive experiments on FB15K-277 and NELL-995 datasets reveal that reasoning over a KG is an effective way of producing human-readable explanations in the form of paths and classifications for fact claims. The RL reasoning agent computes a path that either proves or disproves a factual claim, but does not provide a verdict itself. A verdict is reached by a voting mechanism that utilizes paths produced by the agent. These paths can be presented to human readers so that they themselves can decide whether or not the provided evidence is convincing or not. This work will encourage works in this direction for incorporating RL for explainable fact-checking as it increases trustworthiness by providing a human-in-the-loop approach.
</details>
<details>
<summary>摘要</summary>
fact-checking是一项非常重要的任务，因为它可以防止谣言的扩散。然而，手动fact-checking无法与在线false信息的速度保持 pace。自动化fact-checking机器比人类更快。但为了提高自动化系统的信任和透明度，需要解释性在fact-checking过程中。fact-checking通常包括对一个真实声明与一个知识库进行对比，以便提供这些解释。知识图（KG）是一种有效的知识表示方式。虽然有很多关于fact-checking的提议，但没有很多关于RL的应用。为了填补这一差，我们提出了一种基于RL的KG逻辑应用，用于可读性的fact-checking。我们对FB15K-277和NELL-995 datasets进行了广泛的实验，结果表明，使用KG进行逻辑 reasoning可以生成人类可读的解释，包括路径和分类。RL逻辑代理 computes一个路径，以证明或驳斥一个真实声明，但不提供自己的判断。一个决策是通过RL逻辑代理生成的路径进行投票，以确定声明的真实性。这些路径可以向人类读者展示，让他们自己决定提供的证据是否有力。这种工作将鼓励更多关于RL的fact-checking应用，因为它提高了系统的可信度，并提供了人类在循环的方式。
</details></li>
</ul>
<hr>
<h2 id="PHYDI-Initializing-Parameterized-Hypercomplex-Neural-Networks-as-Identity-Functions"><a href="#PHYDI-Initializing-Parameterized-Hypercomplex-Neural-Networks-as-Identity-Functions" class="headerlink" title="PHYDI: Initializing Parameterized Hypercomplex Neural Networks as Identity Functions"></a>PHYDI: Initializing Parameterized Hypercomplex Neural Networks as Identity Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07612">http://arxiv.org/abs/2310.07612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ispamm/phydi">https://github.com/ispamm/phydi</a></li>
<li>paper_authors: Matteo Mancanelli, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello</li>
<li>for: 这篇论文主要用于研究 parameterized hypercomplex neural networks（PHNNs）的收敛性和提高其性能。</li>
<li>methods: 本文提出了 parameterized hypercomplex identity initialization（PHYDI）方法，用于控制PHNNs的收敛性，并在不同的缩放量下实现更好的性能。</li>
<li>results: 研究发现，PHYDI方法可以在不同的benchmark中提高PHNNs的性能，并且可以在减少迭代次数的情况下达到相同的性能水平。<details>
<summary>Abstract</summary>
Neural models based on hypercomplex algebra systems are growing and prolificating for a plethora of applications, ranging from computer vision to natural language processing. Hand in hand with their adoption, parameterized hypercomplex neural networks (PHNNs) are growing in size and no techniques have been adopted so far to control their convergence at a large scale. In this paper, we study PHNNs convergence and propose parameterized hypercomplex identity initialization (PHYDI), a method to improve their convergence at different scales, leading to more robust performance when the number of layers scales up, while also reaching the same performance with fewer iterations. We show the effectiveness of this approach in different benchmarks and with common PHNNs with ResNets- and Transformer-based architecture. The code is available at https://github.com/ispamm/PHYDI.
</details>
<details>
<summary>摘要</summary>
神经网络基于高维复杂代数系统在各种应用中增长和普遍，从计算机视觉到自然语言处理。随着其采用，具有参数化的高维复杂神经网络（PHNNs）的大小不断增长，而没有任何控制其归一化的技术。本文研究PHNNs的归一化并提出具有参数化高维复杂标识初始化（PHYDI）方法，以提高它们在不同级别上的归一化性能，以达到更加稳定的性能，同时也可以在更少的迭代次数下达到相同的性能。我们在不同的标准吨数据集上测试了这种方法，并与常见的PHNNs结构（ResNets和Transformers）结合使用。代码可以在https://github.com/ispamm/PHYDI中找到。
</details></li>
</ul>
<hr>
<h2 id="Democratizing-LLMs-An-Exploration-of-Cost-Performance-Trade-offs-in-Self-Refined-Open-Source-Models"><a href="#Democratizing-LLMs-An-Exploration-of-Cost-Performance-Trade-offs-in-Self-Refined-Open-Source-Models" class="headerlink" title="Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models"></a>Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07611">http://arxiv.org/abs/2310.07611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumuk Shashidhar, Abhinav Chinta, Vaibhav Sahai, Zhenhailong Wang, Heng Ji</li>
<li>For: The paper aims to address the issue of restricted access and information privacy concerns due to the dominance of proprietary large language models (LLMs). It seeks to provide high-performing open-source alternatives that can compete with proprietary models in performance and cost.* Methods: The paper proposes a novel ranking metric called Performance, Refinement, and Inference Cost Score (PeRFICS) to evaluate and select the optimal open-source model for a given task. The authors also propose a domain-agnostic self-refinement process to improve the performance of open-source models.* Results: The authors’ experiments show that open-source models of varying sizes, on average, improve 8.2% from their baseline performance. The smallest model, Vicuna-7B, achieves a 11.74% improvement overall and up to a 25.39% improvement in high-creativity tasks. The best-performing model, Vicuna-13B, outperforms ChatGPT post-refinement, demonstrating the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
The dominance of proprietary LLMs has led to restricted access and raised information privacy concerns. High-performing open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance. To address this gap, we propose (1) A untargeted variant of iterative self-critique and self-refinement devoid of external influence. (2) A novel ranking metric - Performance, Refinement, and Inference Cost Score (PeRFICS) - to find the optimal model for a given task considering refined performance and cost. Our experiments show that SoTA open source models of varying sizes from 7B - 65B, on average, improve 8.2% from their baseline performance. Strikingly, even models with extremely small memory footprints, such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39% improvement in high-creativity, open ended tasks on the Vicuna benchmark. Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement. This work has profound implications for resource-constrained and information-sensitive environments seeking to leverage LLMs without incurring prohibitive costs, compromising on performance and privacy. The domain-agnostic self-refinement process coupled with our novel ranking metric facilitates informed decision-making in model selection, thereby reducing costs and democratizing access to high-performing language models, as evidenced by case studies.
</details>
<details>
<summary>摘要</summary>
due to the dominance of proprietary LLMs, there are concerns about restricted access and information privacy. High-performing open-source alternatives are crucial for information-sensitive and high-volume applications, but they often lag behind in performance. To address this gap, we propose:1. A untargeted variant of iterative self-critique and self-refinement that is not influenced by external factors.2. A new ranking metric called Performance, Refinement, and Inference Cost Score (PeRFICS) to find the best model for a given task based on refined performance and cost.Our experiments show that the SoTA open-source models of varying sizes from 7B to 65B, on average, improve 8.2% from their baseline performance. Surprisingly, even models with extremely small memory footprints, such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39% improvement in high-creativity, open-ended tasks on the Vicuna benchmark. Vicuna-13B even outperforms ChatGPT post-refinement.This work has significant implications for resource-constrained and information-sensitive environments seeking to leverage LLMs without incurring prohibitive costs, compromising on performance, and privacy. The domain-agnostic self-refinement process coupled with our novel ranking metric facilitates informed decision-making in model selection, thereby reducing costs and democratizing access to high-performing language models, as shown by case studies.
</details></li>
</ul>
<hr>
<h2 id="Survey-on-Imbalanced-Data-Representation-Learning-and-SEP-Forecasting"><a href="#Survey-on-Imbalanced-Data-Representation-Learning-and-SEP-Forecasting" class="headerlink" title="Survey on Imbalanced Data, Representation Learning and SEP Forecasting"></a>Survey on Imbalanced Data, Representation Learning and SEP Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07598">http://arxiv.org/abs/2310.07598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josias Moukpe</li>
<li>for: 这篇论文旨在探讨深度学习方法如何在实际应用中进行调整，以减少因数据不均衡而导致的影响。</li>
<li>methods: 这篇论文使用了表示学习方法，将注意力集中在具有丰富特征的数据空间中，以更好地捕捉资料特征和泛化到少数类别。</li>
<li>results: 这篇论文发现，这些新的深度学习方法可以更好地处理实际世界中的数据不均衡问题，并且在SEP预测任务中获得了更好的结果。<details>
<summary>Abstract</summary>
Deep Learning methods have significantly advanced various data-driven tasks such as regression, classification, and forecasting. However, much of this progress has been predicated on the strong but often unrealistic assumption that training datasets are balanced with respect to the targets they contain. This misalignment with real-world conditions, where data is frequently imbalanced, hampers the effectiveness of such models in practical applications. Methods that reconsider that assumption and tackle real-world imbalances have begun to emerge and explore avenues to address this challenge. One such promising avenue is representation learning, which enables models to capture complex data characteristics and generalize better to minority classes. By focusing on a richer representation of the feature space, these techniques hold the potential to mitigate the impact of data imbalance. In this survey, we present deep learning works that step away from the balanced-data assumption, employing strategies like representation learning to better approximate real-world imbalances. We also highlight a critical application in SEP forecasting where addressing data imbalance is paramount for success.
</details>
<details>
<summary>摘要</summary>
深度学习方法在许多数据驱动任务中取得了重大进步，包括回归、分类和预测等。然而，大多数这些进步假设了训练集与目标之间的均衡，这种假设在实际应用中并不是真实的。因此，许多模型在实际应用中效果不佳。为了解决这个问题，有些新的方法开始出现，它们尝试重新考虑训练集与实际应用中的偏衡。一种这样的有前途的方向是表示学习，它允许模型捕捉复杂的数据特征，并更好地泛化到小类。通过强调richer的表示空间，这些技术可能会减轻数据偏衡的影响。在这篇评论中，我们介绍了脱离平衡数据的深度学习工作，使用表示学习等策略来更好地 aproximate实际应用中的偏衡。我们还高亮了应用在SEP预测中，Addressing data imbalance是成功的关键应用。
</details></li>
</ul>
<hr>
<h2 id="Goodtriever-Adaptive-Toxicity-Mitigation-with-Retrieval-augmented-Models"><a href="#Goodtriever-Adaptive-Toxicity-Mitigation-with-Retrieval-augmented-Models" class="headerlink" title="Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models"></a>Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07589">http://arxiv.org/abs/2310.07589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luiza Pozzobon, Beyza Ermis, Patrick Lewis, Sara Hooker</li>
<li>for: 这篇论文的目的是提出一种适应语言演化的恶意抑制方法，以提高模型在实际应用中的性能和可靠性。</li>
<li>methods: 该方法基于Retrieval-based Approach，通过在decode时使用检索来实现恶意控制文本生成。</li>
<li>results: 论文通过对多种语言模型进行实验，证明了Goodtriever方法可以减少43%的延迟时间和提高计算效率，同时保持与当前状态艺术的水平。<details>
<summary>Abstract</summary>
Considerable effort has been dedicated to mitigating toxicity, but existing methods often require drastic modifications to model parameters or the use of computationally intensive auxiliary models. Furthermore, previous approaches have often neglected the crucial factor of language's evolving nature over time. In this work, we present a comprehensive perspective on toxicity mitigation that takes into account its changing nature. We introduce Goodtriever, a flexible methodology that matches the current state-of-the-art toxicity mitigation while achieving 43% relative latency reduction during inference and being more computationally efficient. By incorporating a retrieval-based approach at decoding time, Goodtriever enables toxicity-controlled text generation. Our research advocates for an increased focus on adaptable mitigation techniques, which better reflect the data drift models face when deployed in the wild. Code and data are available at https://github.com/for-ai/goodtriever.
</details>
<details>
<summary>摘要</summary>
很大的努力已经投入到抑制毒性方面，但现有的方法frequently需要对模型参数进行极大的修改或使用 computationally intensive的auxiliary models。此外，前一代的方法经常忽视了语言的不断发展的特点。在这项工作中，我们提出了一个完整的抑制毒性视角，考虑到其变化的特点。我们介绍了Goodtriever，一种灵活的方法，与当前状态的抑制毒性方法相当，实现了43%的相对延迟减少 durante la inferencia y es más eficiente en términos de computación。通过在 decode 时 incorporating a retrieval-based approach，Goodtriever 实现了基于文本生成的毒性控制。我们强调了适应性的技术的重要性，以更好地反映数据模型在野外部署时所面临的数据漂移模型。代码和数据可以在 <https://github.com/for-ai/goodtriever> 获取。
</details></li>
</ul>
<hr>
<h2 id="Accurate-Use-of-Label-Dependency-in-Multi-Label-Text-Classification-Through-the-Lens-of-Causality"><a href="#Accurate-Use-of-Label-Dependency-in-Multi-Label-Text-Classification-Through-the-Lens-of-Causality" class="headerlink" title="Accurate Use of Label Dependency in Multi-Label Text Classification Through the Lens of Causality"></a>Accurate Use of Label Dependency in Multi-Label Text Classification Through the Lens of Causality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07588">http://arxiv.org/abs/2310.07588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caoyun Fan, Wenqing Chen, Jidong Tian, Yitian Li, Hao He, Yaohui Jin</li>
<li>for: 本研究旨在提高多标签文本分类（MLTC）模型的性能，通过引入标签相关性来增强模型的预测能力。</li>
<li>methods: 本研究提出了一种Counterfactual Text Classifier（CFTC），它首先使用预测后修改的基础体系来提取标签相关性中嵌入的精准标签信息，然后通过对标签相关性的反向矩阵来阻断相关性快捷的偏好。</li>
<li>results: 实验结果表明，CFTC在三个数据集上显著超过基eline，并有效地消除了标签相关性偏好。<details>
<summary>Abstract</summary>
Multi-Label Text Classification (MLTC) aims to assign the most relevant labels to each given text. Existing methods demonstrate that label dependency can help to improve the model's performance. However, the introduction of label dependency may cause the model to suffer from unwanted prediction bias. In this study, we attribute the bias to the model's misuse of label dependency, i.e., the model tends to utilize the correlation shortcut in label dependency rather than fusing text information and label dependency for prediction. Motivated by causal inference, we propose a CounterFactual Text Classifier (CFTC) to eliminate the correlation bias, and make causality-based predictions. Specifically, our CFTC first adopts the predict-then-modify backbone to extract precise label information embedded in label dependency, then blocks the correlation shortcut through the counterfactual de-bias technique with the help of the human causal graph. Experimental results on three datasets demonstrate that our CFTC significantly outperforms the baselines and effectively eliminates the correlation bias in datasets.
</details>
<details>
<summary>摘要</summary>
多 Label 文本分类 (MLTC) 目标是为每个给定的文本分配最 relevante 标签。现有方法表明，标签依赖可以帮助提高模型的性能。然而，标签依赖的引入可能会导致模型受到不良预测偏见。在本研究中，我们归因偏见于模型对标签依赖的误用，即模型倾向于利用标签依赖的相关短cut 而不是将文本信息和标签依赖 fusion 用于预测。 motivated by causal inference，我们提出了Counterfactual Text Classifier (CFTC)，以消除相关偏见，并基于 causality 进行预测。specifically，我们的 CFTC 首先采用 predict-then-modify 基干来提取标签依赖中嵌入的精确标签信息，然后通过 counterfactual de-bias 技术和人类 causal graph 屏蔽相关短cut，以消除偏见。实验结果在三个数据集上表明，我们的 CFTC significantly outperforms 基elines，并有效地消除数据集中的偏见。
</details></li>
</ul>
<hr>
<h2 id="Fed-GraB-Federated-Long-tailed-Learning-with-Self-Adjusting-Gradient-Balancer"><a href="#Fed-GraB-Federated-Long-tailed-Learning-with-Self-Adjusting-Gradient-Balancer" class="headerlink" title="Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer"></a>Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07587">http://arxiv.org/abs/2310.07587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zackzikaixiao/fedgrab">https://github.com/zackzikaixiao/fedgrab</a></li>
<li>paper_authors: Zikai Xiao, Zihan Chen, Songshang Liu, Hualiang Wang, Yang Feng, Jin Hao, Joey Tianyi Zhou, Jian Wu, Howard Hao Yang, Zuozhu Liu</li>
<li>for: 这篇论文研究了一种 federated long-tailed learning（Fed-LT）任务，在每个客户端上存在本地不同的数据集，如果这些数据集可以全局聚合，它们就会共同出现长尾分布。在这种设置下，现有的联邦优化和&#x2F;或中央长尾学习方法很难应用，因为在隐私约束下不能准确地特征长尾分布的全局性。</li>
<li>methods: 该论文提出了一种名为 $\texttt{Fed-GraB}$ 的方法，包括一个自适应权重调整器（SGB）模块，该模块在关闭loop的方式下，根据客户端的反馈，重新权重客户端的梯度。此外，该方法还包括一个直接先验分析器（DPA）模块，用于评估客户端数据集的全局长尾分布。</li>
<li>results: EXTENSIVE EXPERIMENTS DEMONSTRATE THAT $\texttt{Fed-GraB}$ ACHIEVES STATE-OF-THE-ART PERFORMANCE ON REPRESENTATIVE DATASETS SUCH AS CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, AND iNaturalist。<details>
<summary>Abstract</summary>
Data privacy and long-tailed distribution are the norms rather than the exception in many real-world tasks. This paper investigates a federated long-tailed learning (Fed-LT) task in which each client holds a locally heterogeneous dataset; if the datasets can be globally aggregated, they jointly exhibit a long-tailed distribution. Under such a setting, existing federated optimization and/or centralized long-tailed learning methods hardly apply due to challenges in (a) characterizing the global long-tailed distribution under privacy constraints and (b) adjusting the local learning strategy to cope with the head-tail imbalance. In response, we propose a method termed $\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB) module that re-weights clients' gradients in a closed-loop manner, based on the feedback of global long-tailed distribution evaluated by a Direct Prior Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively alleviate the distribution drift caused by data heterogeneity during the model training process and obtain a global model with better performance on the minority classes while maintaining the performance of the majority classes. Extensive experiments demonstrate that $\texttt{Fed-GraB}$ achieves state-of-the-art performance on representative datasets such as CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist.
</details>
<details>
<summary>摘要</summary>
“数据隐私和长尾分布是现实任务中的常见现象，而不是特例。本文研究了一种联合长尾学习（Fed-LT）任务，每个客户端持有本地不同 datasets，如果这些数据集可以全球聚合，它们就会共同表现出长尾分布。在这种设置下，现有的联合优化和/或中央长尾学习方法几乎无法应用，因为（a）全局长尾分布的特征难以在隐私约束下表征，（b）本地学习策略难以适应头部和尾部差异。为应对这些挑战，我们提出了一种方法，称为 $\texttt{Fed-GraB}$，它包括一个自适应权重重新分配（SGB）模块，根据全球长尾分布的反馈，在关闭循环方式下重新分配客户端的梯度。使用 $\texttt{Fed-GraB}$，客户端可以在模型训练过程中有效地缓解由数据不同性引起的分布漂移，并在少数类上获得更好的性能，同时保持多数类的性能。广泛的实验表明， $\texttt{Fed-GraB}$ 达到了代表性数据集的领先性能，包括 CIFAR-10-LT、CIFAR-100-LT、ImageNet-LT 和 iNaturalist。”
</details></li>
</ul>
<hr>
<h2 id="Linear-Latent-World-Models-in-Simple-Transformers-A-Case-Study-on-Othello-GPT"><a href="#Linear-Latent-World-Models-in-Simple-Transformers-A-Case-Study-on-Othello-GPT" class="headerlink" title="Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT"></a>Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07582">http://arxiv.org/abs/2310.07582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deanhazineh/emergent-world-representations-othello">https://github.com/deanhazineh/emergent-world-representations-othello</a></li>
<li>paper_authors: Dean S. Hazineh, Zechen Zhang, Jeffery Chiu</li>
<li>for: 这篇论文是为了研究基于Othello的Transformer模型是否真正理解世界，而不仅仅是随机模仿。</li>
<li>methods: 这篇论文使用了一个简单的Transformer模型，并对其进行了扩展，以提高对Othello-GPT模型的理解。</li>
<li>results: 研究发现，Othello-GPT模型具有一个线性的对抗方面表示，这个表示导致了它的决策过程。此外，研究还发现了这个线性世界表示和 causal 决策之间的互动，以及层数和模型复杂度对这种互动的影响。<details>
<summary>Abstract</summary>
Foundation models exhibit significant capabilities in decision-making and logical deductions. Nonetheless, a continuing discourse persists regarding their genuine understanding of the world as opposed to mere stochastic mimicry. This paper meticulously examines a simple transformer trained for Othello, extending prior research to enhance comprehension of the emergent world model of Othello-GPT. The investigation reveals that Othello-GPT encapsulates a linear representation of opposing pieces, a factor that causally steers its decision-making process. This paper further elucidates the interplay between the linear world representation and causal decision-making, and their dependence on layer depth and model complexity. We have made the code public.
</details>
<details>
<summary>摘要</summary>
基本模型在决策和逻辑推理方面表现出了显著的能力。然而，关于它们真正理解世界的问题仍然存在着不断的讨论，一些人认为它们只是Random mimicry。本文仔细检查了一个简单的 transformer 被训练 для奥菲洛，扩展了先前的研究，以更好地了解奥菲洛-GPT 的 emergent 世界模型。调查发现，奥菲洛-GPT 包含了对对抗的 Piece 的线性表示，这种因素 causally 导向它的决策过程。本文还详细解释了线性世界表示和 causal 决策之间的交互，以及它们与模型复杂度和层数之间的依赖关系。我们已经公开了代码。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Unlearning-Language-Models-as-Few-Shot-Unlearners"><a href="#In-Context-Unlearning-Language-Models-as-Few-Shot-Unlearners" class="headerlink" title="In-Context Unlearning: Language Models as Few Shot Unlearners"></a>In-Context Unlearning: Language Models as Few Shot Unlearners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07579">http://arxiv.org/abs/2310.07579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MartinPawel/In-Context-Unlearning">https://github.com/MartinPawel/In-Context-Unlearning</a></li>
<li>paper_authors: Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju</li>
<li>for: 研究高效地从特定训练点中除去对训练模型的影响，以遵守隐私法规 like Right to be Forgotten.</li>
<li>methods: 提出了一些用于 approximate 训练数据除去而不需要重新训练模型的算法，这些算法需要对模型参数进行访问。</li>
<li>results: 实验结果表明，在提供输入Context和颠倒标签 alongside correctly labelled instances 时，可以高效地除去特定训练点的影响，并保持与状态体系方法的竞争力。<details>
<summary>Abstract</summary>
Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the instance alongside a flipped label and additional correctly labelled instances which are prepended as inputs to the LLM at inference time. Our experimental results demonstrate that these contexts effectively remove specific information from the training set while maintaining performance levels that are competitive with (or in some cases exceed) state-of-the-art unlearning methods that require access to the LLM parameters.
</details>
<details>
<summary>摘要</summary>
机器学习无学习（Machine Unlearning），即在已经训练过的模型上efficiently removing特定训练点的影响，在最近几年来收到了更多的关注，即由隐私法规如“Right to be Forgotten”所驱动。特别是在LLM中，由于复杂的版权问题，无学习变得非常重要。然而，在非常大的模型上，准确的无学习是计算不可能的。为此，最近的研究提出了一些使用模型参数更新的算法，以便精确地除掉训练数据。这些算法具有访问模型参数的假设，但在实践中这种假设可能不成立，例如因为计算限制或者LLM通过API访问。在这项工作中，我们提出了一种新的LLM无学习方法，我们称之为“在Context中的无学习”（In-Context Unlearning）。在这种方法中，我们在推理时提供特定训练实例，并将其旋转后的标签和正确标签的其他实例预处理为LLM的输入。我们的实验结果表明，这些上下文可以有效地从训练集中除掉特定信息，而且与无需更新模型参数的状态之前的方法竞争。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-for-Computational-Topology"><a href="#ChatGPT-for-Computational-Topology" class="headerlink" title="ChatGPT for Computational Topology"></a>ChatGPT for Computational Topology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07570">http://arxiv.org/abs/2310.07570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joybearliu/chatgpt-for-computational-topology">https://github.com/joybearliu/chatgpt-for-computational-topology</a></li>
<li>paper_authors: Jian Liu, Li Shen, Guo-Wei Wei</li>
<li>for:  bridging the gap between theoretical topological concepts and their practical implementation in computational topology</li>
<li>methods:  utilizing ChatGPT to transform mathematical formulations and concepts into functional code for computational topology</li>
<li>results:  demonstrating the effectiveness of ChatGPT in computing Betti numbers, Laplacian matrices, and Dirac matrices for simplicial complexes, as well as the persistence of various homologies and Laplacians, and exploring its application in computing recently developed topological theories for hypergraphs and digraphs.<details>
<summary>Abstract</summary>
ChatGPT represents a significant milestone in the field of artificial intelligence (AI), finding widespread applications across diverse domains. However, its effectiveness in mathematical contexts has been somewhat constrained by its susceptibility to conceptual errors. Concurrently, topological data analysis (TDA), a relatively new discipline, has garnered substantial interest in recent years. Nonetheless, the advancement of TDA is impeded by the limited understanding of computational algorithms and coding proficiency among theoreticians. This work endeavors to bridge the gap between theoretical topological concepts and their practical implementation in computational topology through the utilization of ChatGPT. We showcase how a pure theoretician, devoid of computational experience and coding skills, can effectively transform mathematical formulations and concepts into functional code for computational topology with the assistance of ChatGPT. Our strategy outlines a productive process wherein a mathematician trains ChatGPT on pure mathematical concepts, steers ChatGPT towards generating computational topology code, and subsequently validates the generated code using established examples. Our specific case studies encompass the computation of Betti numbers, Laplacian matrices, and Dirac matrices for simplicial complexes, as well as the persistence of various homologies and Laplacians. Furthermore, we explore the application of ChatGPT in computing recently developed topological theories for hypergraphs and digraphs. This work serves as an initial step towards effectively transforming pure mathematical theories into practical computational tools, with the ultimate goal of enabling real applications across diverse fields.
</details>
<details>
<summary>摘要</summary>
chatGPT 代表了人工智能（AI）领域的一个重要里程碑，在多个领域中发现了广泛的应用。然而，它在数学上的效iveness受到了概念错误的限制。同时，数据 topology 分析（TDA）在最近几年内得到了广泛的关注。然而，TDA 的发展受到了计算算法和编程技能的限制，特别是在理论家中。这项工作的目的是通过使用 chatGPT 将数学概念与计算 topology 的实现相连接。我们展示了如何让纯粹的数学家，没有计算经验和编程技能，通过 chatGPT 将数学表述和概念转化为功能的计算 topology 代码。我们的策略是让数学家通过 chatGPT 训练数学概念，然后通过 chatGPT 生成计算 topology 代码，并 finally 验证生成的代码使用已知的例子进行验证。我们的具体案例包括计算 simplicial 复杂体上的 Betti 数、Laplacian 矩阵和 Dirac 矩阵，以及不同 homology 和 Laplacian 的 persistency。此外，我们还探讨了 chatGPT 在计算最新发展的图 theoretically 的应用。这项工作作为将纯粹的数学理论转化为实用计算工具的第一步，最终目标是在多个领域应用。
</details></li>
</ul>
<hr>
<h2 id="ROMO-Retrieval-enhanced-Offline-Model-based-Optimization"><a href="#ROMO-Retrieval-enhanced-Offline-Model-based-Optimization" class="headerlink" title="ROMO: Retrieval-enhanced Offline Model-based Optimization"></a>ROMO: Retrieval-enhanced Offline Model-based Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07560">http://arxiv.org/abs/2310.07560</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cmciris/romo">https://github.com/cmciris/romo</a></li>
<li>paper_authors: Mingcheng Chen, Haoran Zhao, Yuxiang Zhao, Hulei Fan, Hongqiao Gao, Yong Yu, Zheng Tian</li>
<li>for: 在各种实际应用场景中，数据驱动黑盒模型基于优化（MBO）问题广泛存在，旨在在整个空间中找到一个最优设计，以最大化黑盒目标函数，基于静止的离线数据集。</li>
<li>methods: 我们在这篇论文中考虑了一种更加普遍而具有挑战性的MBO设定，即受限MBO（CoMBO），其中只有一部分的设计空间可以优化，而另一部分则被环境所限制。我们提出了一种新的挑战，即大多数在离线数据集中满足约束的设计都是中等的评价。因此，我们将注意力集中在优化这些中等的设计，而不是进一步提高传统MBO设定中的最优设计。</li>
<li>results: 我们提出了一种新的forward方法，名为回 retrieve-enhanced offline model-based optimization（ROMO），它可以在离线数据集中检索和聚合相关样本，以提供可靠的预测，并用其进行梯度下降优化。ROMO简单易行，并在CoMBO设定中超过了现有方法的表现。我们在一个 sintetic Hartmann（3D）函数数据集、一个工业CIO数据集以及一个Modified Tasks中进行了实验，结果表明，ROMO在各种受限优化任务中表现良好。<details>
<summary>Abstract</summary>
Data-driven black-box model-based optimization (MBO) problems arise in a great number of practical application scenarios, where the goal is to find a design over the whole space maximizing a black-box target function based on a static offline dataset. In this work, we consider a more general but challenging MBO setting, named constrained MBO (CoMBO), where only part of the design space can be optimized while the rest is constrained by the environment. A new challenge arising from CoMBO is that most observed designs that satisfy the constraints are mediocre in evaluation. Therefore, we focus on optimizing these mediocre designs in the offline dataset while maintaining the given constraints rather than further boosting the best observed design in the traditional MBO setting. We propose retrieval-enhanced offline model-based optimization (ROMO), a new derivable forward approach that retrieves the offline dataset and aggregates relevant samples to provide a trusted prediction, and use it for gradient-based optimization. ROMO is simple to implement and outperforms state-of-the-art approaches in the CoMBO setting. Empirically, we conduct experiments on a synthetic Hartmann (3D) function dataset, an industrial CIO dataset, and a suite of modified tasks in the Design-Bench benchmark. Results show that ROMO performs well in a wide range of constrained optimization tasks.
</details>
<details>
<summary>摘要</summary>
“数据驱动黑盒模型基于优化（MBO）问题在许多实际应用场景中出现，目标是在整个空间上找到最优化黑盒目标函数的设计，使用静态离线数据。在这项工作中，我们考虑了更一般 yet 更加挑战性的 MBO 设定，即受限 MBO（CoMBO），其中只有部分设计空间可以优化，而另外的部分受到环境的限制。这种新的挑战是，大多数满足约束的观察到的设计都是较差的评价。因此，我们将注意力转向了这些较差的评价设计，而不是在传统 MBO 设定中进一步提高最佳观察到的设计。我们提出了 reuse-based offline model-based optimization（ROMO），一种新的可求导的前向方法，它在拥有离线数据时重新检索和聚合相关样本，以提供可靠的预测，并用其进行梯度下降优化。ROMO 简单实现，并在 CoMBO 设定中超过了当前状态的方法。我们在一个 Synthetic Hartmann（3D）函数数据集、一个industrial CIO数据集以及一个 Design-Bench  benchmark 中进行了实验，结果表明，ROMO 在受限优化任务中表现良好。”
</details></li>
</ul>
<hr>
<h2 id="ProtoHPE-Prototype-guided-High-frequency-Patch-Enhancement-for-Visible-Infrared-Person-Re-identification"><a href="#ProtoHPE-Prototype-guided-High-frequency-Patch-Enhancement-for-Visible-Infrared-Person-Re-identification" class="headerlink" title="ProtoHPE: Prototype-guided High-frequency Patch Enhancement for Visible-Infrared Person Re-identification"></a>ProtoHPE: Prototype-guided High-frequency Patch Enhancement for Visible-Infrared Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07552">http://arxiv.org/abs/2310.07552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guiwei Zhang, Yongfei Zhang, Zichang Tan</li>
<li>for:  bridging the modality gap in visible-infrared person re-identification</li>
<li>methods:  using high-frequency components and ProtoHPE with two core designs: split patches and multimodal prototypical contrast</li>
<li>results:  effective enhancement of representation ability and capture of key high-frequency components without extra complexity, validated by extensive experiments<details>
<summary>Abstract</summary>
Visible-infrared person re-identification is challenging due to the large modality gap. To bridge the gap, most studies heavily rely on the correlation of visible-infrared holistic person images, which may perform poorly under severe distribution shifts. In contrast, we find that some cross-modal correlated high-frequency components contain discriminative visual patterns and are less affected by variations such as wavelength, pose, and background clutter than holistic images. Therefore, we are motivated to bridge the modality gap based on such high-frequency components, and propose \textbf{Proto}type-guided \textbf{H}igh-frequency \textbf{P}atch \textbf{E}nhancement (ProtoHPE) with two core designs. \textbf{First}, to enhance the representation ability of cross-modal correlated high-frequency components, we split patches with such components by Wavelet Transform and exponential moving average Vision Transformer (ViT), then empower ViT to take the split patches as auxiliary input. \textbf{Second}, to obtain semantically compact and discriminative high-frequency representations of the same identity, we propose Multimodal Prototypical Contrast. To be specific, it hierarchically captures the comprehensive semantics of different modal instances, facilitating the aggregation of high-frequency representations belonging to the same identity. With it, ViT can capture key high-frequency components during inference without relying on ProtoHPE, thus bringing no extra complexity. Extensive experiments validate the effectiveness of ProtoHPE.
</details>
<details>
<summary>摘要</summary>
visible-infrared人识别具有大的模态差异，因此大多数研究都会仰仗可 correlate的可见-红外人像，可能会在严重的分布转换下表现不佳。然而，我们发现一些跨模态相关的高频成分含有可 дискriminative的视觉特征，并且比可见人像更不受波长、姿势和背景噪声的影响。因此，我们决心基于这些高频成分来bridging模态差异，并提出了ProtoHPE方法，具有两个核心设计。首先，为了提高跨模态相关高频成分的表征能力，我们使用波峰变换和抽象迭代ViT来拆分patches，然后使ViT作为辅助输入使用。其次，为了获得不同感知模式下的同一个人的semantically Compact和дискriminative高频表示，我们提出了多模态原型冲突。具体来说，它可以 hierarchically capture不同感知模式下的同一个人的全面semantics，使得ViT能在推理过程中捕捉到关键的高频成分，无需依赖于ProtoHPE，从而不增加额外复杂性。我们的实验证明了ProtoHPE的效果。
</details></li>
</ul>
<hr>
<h2 id="Improving-Fairness-Accuracy-tradeoff-with-few-Test-Samples-under-Covariate-Shift"><a href="#Improving-Fairness-Accuracy-tradeoff-with-few-Test-Samples-under-Covariate-Shift" class="headerlink" title="Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift"></a>Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07535">http://arxiv.org/abs/2310.07535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyas Havaldar, Jatin Chauhan, Karthikeyan Shanmugam, Jay Nandy, Aravindan Raghuveer</li>
<li>for: 本研究旨在提高模型的准确性和公平性，在covariate shift问题下，确保不同敏感群体之间的公平性具有社会意义，如刑事正义。</li>
<li>methods: 本文提出三种贡献：首先，我们提出一种新的复合权重 entropy 基于目标函数，用于提高预测准确性和公平性。其次，我们提出了一种新的不对称 covariate shift Setting，这种设定在许多现有基eline上表现出EXTREMELY CHALLENGING的特点。第三，我们提出了一种理论基础，表明我们的方法可以准确地预测测试数据中的loss。</li>
<li>results: 我们的实验和理论分析表明，我们的方法可以在几个标准数据集上出perform state-of-the-art baselines in the Pareto sense，并且不受重要样本偏置的影响。此外，我们还证明了我们的方法可以在新的不对称 covariate shift Setting中提高公平性和准确性。<details>
<summary>Abstract</summary>
Covariate shift in the test data can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups in such settings is of paramount importance due to societal implications like criminal justice. We operate under the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards this problem, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift occurs when distribution of covariates of one group shifts significantly compared to the other groups and this happens when a dominant group is over-represented. While this setting is extremely challenging for current baselines, We show that our proposed method significantly outperforms them. Our third contribution is theoretical, where we show that our weighted entropy term along with prediction loss on the training set approximates test loss under covariate shift. Empirically and through formal sample complexity bounds, we show that this approximation to the unseen test loss does not depend on importance sampling variance which affects many other baselines.
</details>
<details>
<summary>摘要</summary>
科Variate shift在测试数据中可能会导致模型的准确性和公平性表现下降。在社会中，保持不同敏感群体之间的公平性非常重要，特别是在刑事司法方面。我们在无监督情况下运行，只有一小组无标记测试样本和一个标记训练集可用。为解决这问题，我们提出了三个贡献：第一个贡献是一种新的复合权重Entropy基于目标函数，可以同时保证预测准确性和公平性。我们通过实验表明，使用我们的损失函数可以在 pareto折衔中超过一些状态方法的基elines，并且在多个标准数据集上达到更好的性能。第二个贡献是一种新的设定，我们称之为不对称科Variate shift。这种情况发生在一个群体中covariate的分布发生了显著变化，而另一个群体的分布则相对稳定。这种情况非常困难，但我们展示了我们提出的方法可以在这种情况下表现出色。第三个贡献是理论上的，我们表明了我们的复合权重Entropy терμ和预测损失函数可以近似测试损失函数。我们通过实验和正式样本复杂度下界来证明，这种近似不依赖于重要样本变量，因此不同于许多其他基elines。
</details></li>
</ul>
<hr>
<h2 id="Human-Centered-Evaluation-of-XAI-Methods"><a href="#Human-Centered-Evaluation-of-XAI-Methods" class="headerlink" title="Human-Centered Evaluation of XAI Methods"></a>Human-Centered Evaluation of XAI Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07534">http://arxiv.org/abs/2310.07534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karam Dawoud, Wojciech Samek, Peter Eisert, Sebastian Lapuschkin, Sebastian Bosse</li>
<li>for: 本研究旨在探讨深度学习中的决策过程，以提高人们对AI的理解和信任。</li>
<li>methods: 本研究使用三种主流解释方法：Prototypical Part Network、Occlusion和Layer-wise Relevance Propagation，以评估这些方法的可解释性。</li>
<li>results: 研究发现，这三种方法可以帮助人们快速理解和分类图像，并且它们之间的解释结果相对一致，从而提高了AI的透明度。<details>
<summary>Abstract</summary>
In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called "black boxes" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salient features or aspects. Capitalizing on this parallel, our research embarked on a user-centric study. We sought to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Intriguingly, our results highlight that while the regions spotlighted by these methods can vary widely, they all offer humans a nearly equivalent depth of understanding. This enables users to discern and categorize images efficiently, reinforcing the value of these methods in enhancing AI transparency.
</details>
<details>
<summary>摘要</summary>
在人工智能领域中，一个关键挑战是解释深度学习中的决策过程。在过去几年，许多方法出现了，旨在对各种任务中的决策进行解释。特别是在图像分类任务中，这些方法通常可以识别并强调影响分类器预测的关键像素。这种方法与人类行为的差异不大：当被问到分类图像的理由时，我们通常会指出最引人注目的特征或方面。 builds on this parallel， our research conducted a user-centered study to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Our results show that while the regions highlighted by these methods can vary widely, they all provide humans with a nearly equivalent depth of understanding. This enables users to efficiently categorize and discern images, which reinforces the value of these methods in enhancing AI transparency.
</details></li>
</ul>
<hr>
<h2 id="Energy-Estimates-Across-Layers-of-Computing-From-Devices-to-Large-Scale-Applications-in-Machine-Learning-for-Natural-Language-Processing-Scientific-Computing-and-Cryptocurrency-Mining"><a href="#Energy-Estimates-Across-Layers-of-Computing-From-Devices-to-Large-Scale-Applications-in-Machine-Learning-for-Natural-Language-Processing-Scientific-Computing-and-Cryptocurrency-Mining" class="headerlink" title="Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining"></a>Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07516">http://arxiv.org/abs/2310.07516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadasivan Shankar</li>
<li>for: 这 paper 旨在确定和分析计算机系统中的能源使用量。</li>
<li>methods: 这 paper 使用了以前分析 [3] 的基础，对单个设备和系统，包括三种大规模计算应用程序（人工智能&#x2F;机器学习自然语言处理、科学仿真和加密货币矿 Pool）进行了能源估计。与以前的比较 bit-level switching 中，由于几何缩放，通过逻辑门阵列来实现的能效性，现在在应用程序层次的指令和仿真层次中都需要更多的能源。</li>
<li>results: 这 paper 的分析表明，使用older 半导体技术节点的架构改变可以与使用 newer 技术节点的架构相比，在 AI&#x2F;ML 加速器中实现相同的能效性。此外，对计算系统中的能量和热动力学限制进行了比较，显示计算应用程序的总模拟需要27-36个数量级更高的能量需求。这些能量估计表明计算系统中的能效性需要被考虑，包括能量作为设计参数，以满足计算应用程序的增长需求在数字世界中。<details>
<summary>Abstract</summary>
Estimates of energy usage in layers of computing from devices to algorithms have been determined and analyzed. Building on the previous analysis [3], energy needed from single devices and systems including three large-scale computing applications such as Artificial Intelligence (AI)/Machine Learning for Natural Language Processing, Scientific Simulations, and Cryptocurrency Mining have been estimated. In contrast to the bit-level switching, in which transistors achieved energy efficiency due to geometrical scaling, higher energy is expended both at the at the instructions and simulations levels of an application. Additionally, the analysis based on AI/ML Accelerators indicate that changes in architectures using an older semiconductor technology node have comparable energy efficiency with a different architecture using a newer technology. Further comparisons of the energy in computing systems with the thermodynamic and biological limits, indicate that there is a 27-36 orders of magnitude higher energy requirements for total simulation of an application. These energy estimates underscore the need for serious considerations of energy efficiency in computing by including energy as a design parameter, enabling growing needs of compute-intensive applications in a digital world.
</details>
<details>
<summary>摘要</summary>
计算层从设备到算法的能源使用估算和分析已经确定。基于之前分析（3），包括人工智能（AI）/机器学习自然语言处理、科学仿真和加密货币开采等三大规模计算应用程序的能源需求已经估算。与比特级 switching相比，在应用程序的指令和仿真层级上都需要更多的能源。此外，使用older半导体技术节点的架构改变表明，与不同架构使用更新的技术节点相比，能效性没有很大差异。此外，对计算系统的能源需求和热动力学和生物学限制进行比较，显示计算应用程序的总模拟需求高达27-36个数量级。这些能源估算表明，在计算设计中包含能源为重要参数是必要的，以满足计算密集应用程序在数字世界中的增长需求。
</details></li>
</ul>
<hr>
<h2 id="Sample-Driven-Federated-Learning-for-Energy-Efficient-and-Real-Time-IoT-Sensing"><a href="#Sample-Driven-Federated-Learning-for-Energy-Efficient-and-Real-Time-IoT-Sensing" class="headerlink" title="Sample-Driven Federated Learning for Energy-Efficient and Real-Time IoT Sensing"></a>Sample-Driven Federated Learning for Energy-Efficient and Real-Time IoT Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07497">http://arxiv.org/abs/2310.07497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skyd-fl/scfl">https://github.com/skyd-fl/scfl</a></li>
<li>paper_authors: Minh Ngoc Luu, Minh-Duong Nguyen, Ebrahim Bedeer, Van Duc Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Quoc-Viet Pham</li>
<li>for: 这篇论文主要针对 Federated Learning (FL) 系统中的最新潮流方法，它们假设训练集在 IoT 设备上的数据具有全球数据分布的相似性。但这种方法无法捕捉现实时感数据的全面特点。这篇论文的目的是为 IoT 网络实现现实时感数据的 Federated Learning 系统。</li>
<li>methods: 这篇论文提出了一个新的方法，即 Sample-driven Control for Federated Learning (SCFL)，该方法可以在 IoT 网络中实现现实时感数据的 Federated Learning。SCFL 方法首先将数据采样过程视为一个优化问题，并通过实现数据采样过程中的范例控制来减少过滤问题并提高准确性。</li>
<li>results: 这篇论文的结果显示，SCFL 方法可以有效地控制数据采样过程中的过滤问题，并提高 Federated Learning 系统的准确性。实验结果显示，SCFL 方法在不同的数据分布下均可以获得高准确性。此外，SCFL 方法还可以在变化的环境下获得佳效果，因为它可以在不同的数据分布下进行自适应调整。<details>
<summary>Abstract</summary>
In the domain of Federated Learning (FL) systems, recent cutting-edge methods heavily rely on ideal conditions convergence analysis. Specifically, these approaches assume that the training datasets on IoT devices possess similar attributes to the global data distribution. However, this approach fails to capture the full spectrum of data characteristics in real-time sensing FL systems. In order to overcome this limitation, we suggest a new approach system specifically designed for IoT networks with real-time sensing capabilities. Our approach takes into account the generalization gap due to the user's data sampling process. By effectively controlling this sampling process, we can mitigate the overfitting issue and improve overall accuracy. In particular, We first formulate an optimization problem that harnesses the sampling process to concurrently reduce overfitting while maximizing accuracy. In pursuit of this objective, our surrogate optimization problem is adept at handling energy efficiency while optimizing the accuracy with high generalization. To solve the optimization problem with high complexity, we introduce an online reinforcement learning algorithm, named Sample-driven Control for Federated Learning (SCFL) built on the Soft Actor-Critic (A2C) framework. This enables the agent to dynamically adapt and find the global optima even in changing environments. By leveraging the capabilities of SCFL, our system offers a promising solution for resource allocation in FL systems with real-time sensing capabilities.
</details>
<details>
<summary>摘要</summary>
在 Federated Learning（FL）系统领域，现代方法倚靠理想的条件进行归一化分析。specifically, these approaches assume that the training datasets on IoT devices possess similar attributes to the global data distribution. However, this approach fails to capture the full spectrum of data characteristics in real-time sensing FL systems. In order to overcome this limitation, we propose a new approach system specifically designed for IoT networks with real-time sensing capabilities. Our approach takes into account the generalization gap due to the user's data sampling process. By effectively controlling this sampling process, we can mitigate the overfitting issue and improve overall accuracy. In particular, We first formulate an optimization problem that harnesses the sampling process to concurrently reduce overfitting while maximizing accuracy. In pursuit of this objective, our surrogate optimization problem is adept at handling energy efficiency while optimizing the accuracy with high generalization. To solve the optimization problem with high complexity, we introduce an online reinforcement learning algorithm, named Sample-driven Control for Federated Learning (SCFL) built on the Soft Actor-Critic (A2C) framework. This enables the agent to dynamically adapt and find the global optima even in changing environments. By leveraging the capabilities of SCFL, our system offers a promising solution for resource allocation in FL systems with real-time sensing capabilities.Here's the word-for-word translation of the text into Simplified Chinese:在 Federated Learning（FL）系统领域，现代方法倚靠理想的条件进行归一化分析。specifically, these approaches assume that the training datasets on IoT devices possess similar attributes to the global data distribution. However, this approach fails to capture the full spectrum of data characteristics in real-time sensing FL systems. In order to overcome this limitation, we propose a new approach system specifically designed for IoT networks with real-time sensing capabilities. Our approach takes into account the generalization gap due to the user's data sampling process. By effectively controlling this sampling process, we can mitigate the overfitting issue and improve overall accuracy. In particular, We first formulate an optimization problem that harnesses the sampling process to concurrently reduce overfitting while maximizing accuracy. In pursuit of this objective, our surrogate optimization problem is adept at handling energy efficiency while optimizing the accuracy with high generalization. To solve the optimization problem with high complexity, we introduce an online reinforcement learning algorithm, named Sample-driven Control for Federated Learning (SCFL) built on the Soft Actor-Critic (A2C) framework. This enables the agent to dynamically adapt and find the global optima even in changing environments. By leveraging the capabilities of SCFL, our system offers a promising solution for resource allocation in FL systems with real-time sensing capabilities.
</details></li>
</ul>
<hr>
<h2 id="Diversity-for-Contingency-Learning-Diverse-Behaviors-for-Efficient-Adaptation-and-Transfer"><a href="#Diversity-for-Contingency-Learning-Diverse-Behaviors-for-Efficient-Adaptation-and-Transfer" class="headerlink" title="Diversity for Contingency: Learning Diverse Behaviors for Efficient Adaptation and Transfer"></a>Diversity for Contingency: Learning Diverse Behaviors for Efficient Adaptation and Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07493">http://arxiv.org/abs/2310.07493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Finn Rietz, Johannes Andreas Stork</li>
<li>for: 本文旨在提出一种简单的方法，以寻找任务中所有可能的解决方案，以提高转移RL代理的性能和适应能力。</li>
<li>methods: 本文使用迭代学习策略，每一个策略都要求在所有前一个策略下 unlikely 的解决方案。不需要学习额外的模型，也不需要调整任务和新鲜度奖励信号。</li>
<li>results: 本文的方法可以快速适应任务和转移动力学变化，并且可以提高转移RL代理的性能。<details>
<summary>Abstract</summary>
Discovering all useful solutions for a given task is crucial for transferable RL agents, to account for changes in the task or transition dynamics. This is not considered by classical RL algorithms that are only concerned with finding the optimal policy, given the current task and dynamics. We propose a simple method for discovering all possible solutions of a given task, to obtain an agent that performs well in the transfer setting and adapts quickly to changes in the task or transition dynamics. Our method iteratively learns a set of policies, while each subsequent policy is constrained to yield a solution that is unlikely under all previous policies. Unlike prior methods, our approach does not require learning additional models for novelty detection and avoids balancing task and novelty reward signals, by directly incorporating the constraint into the action selection and optimization steps.
</details>
<details>
<summary>摘要</summary>
发现所有有用的解决方案是让转移RL代理工作的关键，以适应任务或过程动态变化。这并不是классиical RL算法所考虑的，它们只关心当前任务和动态下找到优化策略。我们提出了一种简单的方法，可以帮助代理人在转移设置下快速适应任务或动态变化，并且 Perform well。我们的方法每次迭代学习一组策略，而每一个后续策略都需要在所有前一个策略下 unlikely to yield a solution。不同于先前的方法，我们的方法不需要学习额外的模型来检测新事物，也不需要平衡任务和新事物的奖励信号，直接将约束 incorporated into action selection and optimization steps。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Black-box-Attack-to-Deep-Neural-Networks-with-Conditional-Diffusion-Models"><a href="#Boosting-Black-box-Attack-to-Deep-Neural-Networks-with-Conditional-Diffusion-Models" class="headerlink" title="Boosting Black-box Attack to Deep Neural Networks with Conditional Diffusion Models"></a>Boosting Black-box Attack to Deep Neural Networks with Conditional Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07492">http://arxiv.org/abs/2310.07492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renyang Liu, Wei Zhou, Tianwei Zhang, Kangjie Chen, Jun Zhao, Kwok-Yan Lam<br>for:This paper proposes a novel black-box attack strategy to improve the query efficiency of generating adversarial examples (AE) under query-limited situations.methods:The proposed method, called Conditional Diffusion Model Attack (CDMA), formulates the task of AE synthesis as a distribution transformation problem and uses the conditional Denoising Diffusion Probabilistic Model as the converter to learn the transformation from clean samples to AEs.results:CDMA significantly reduces the number of queries needed compared to nine state-of-the-art black-box attacks, with an average reduction of the query count to a handful of times. The attack success rate is high, with $&gt;99%$ success rate for untargeted attacks over all datasets and targeted attack over CIFAR-10 with a noise budget of $\epsilon&#x3D;16$.<details>
<summary>Abstract</summary>
Existing black-box attacks have demonstrated promising potential in creating adversarial examples (AE) to deceive deep learning models. Most of these attacks need to handle a vast optimization space and require a large number of queries, hence exhibiting limited practical impacts in real-world scenarios. In this paper, we propose a novel black-box attack strategy, Conditional Diffusion Model Attack (CDMA), to improve the query efficiency of generating AEs under query-limited situations. The key insight of CDMA is to formulate the task of AE synthesis as a distribution transformation problem, i.e., benign examples and their corresponding AEs can be regarded as coming from two distinctive distributions and can transform from each other with a particular converter. Unlike the conventional \textit{query-and-optimization} approach, we generate eligible AEs with direct conditional transform using the aforementioned data converter, which can significantly reduce the number of queries needed. CDMA adopts the conditional Denoising Diffusion Probabilistic Model as the converter, which can learn the transformation from clean samples to AEs, and ensure the smooth development of perturbed noise resistant to various defense strategies. We demonstrate the effectiveness and efficiency of CDMA by comparing it with nine state-of-the-art black-box attacks across three benchmark datasets. On average, CDMA can reduce the query count to a handful of times; in most cases, the query count is only ONE. We also show that CDMA can obtain $>99\%$ attack success rate for untarget attacks over all datasets and targeted attack over CIFAR-10 with the noise budget of $\epsilon=16$.
</details>
<details>
<summary>摘要</summary>
现有的黑盒攻击已经显示了吸引深度学习模型的可能性（AE）的潜在攻击性。大多数这些攻击需要处理広大的优化空间，需要大量的询问，因此在实际情况下显示有限的实际影响。在这篇论文中，我们提出了一种新的黑盒攻击策略，即条件扩散模型攻击（CDMA），以提高受限制的询问量生成AE的能力。我们的关键见解是将AE生成视为两种不同分布之间的转换问题，即正常示例和其相应的AE可以被视为来自两个不同的分布，并可以使用特定的数据转换器将其转换为另一种分布。不同于传统的询问和优化方法，我们可以直接使用条件扩散probabilistic模型来生成适合的AE，这可以将询问数量大大减少。CDMA使用条件排除扩散模型来实现转换，这种模型可以从清洁示例到AE的转换，并确保受到不同防御策略的抗压。我们透过与九种现有的黑盒攻击进行比较，证明CDMA可以将询问数量大大减少，平均只需要几回询问。此外，我们还证明CDMA可以在CIFAR-10上 obtaint>99%的攻击成功率，并且在其他两个测试集上也有出色的表现。
</details></li>
</ul>
<hr>
<h2 id="KwaiYiiMath-Technical-Report"><a href="#KwaiYiiMath-Technical-Report" class="headerlink" title="KwaiYiiMath: Technical Report"></a>KwaiYiiMath: Technical Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07488">http://arxiv.org/abs/2310.07488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong Chen, Zhirui Yang, Shengnan Zhang, Xue Zheng, Yan Li, Yuliang Liu, Xucheng Ye, Yiqiao Liao, Chao Liao, Bin Chen, Chengru Song, Junchen Wan, Zijia Lin, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, Kun Gai</li>
<li>for: The paper is written to enhance the mathematical reasoning abilities of KwaiYiiBase1, a large language model, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF) on both English and Chinese mathematical tasks.</li>
<li>methods: The paper uses Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF) to enhance the mathematical reasoning abilities of KwaiYiiBase1.</li>
<li>results: The paper achieves state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with similar size models, respectively.Here are the three key information points in Simplified Chinese text:</li>
<li>for: 本文是为了增强kwaiyiibase1的数学逻辑能力，通过supervised fine-tuning (SFT)和人工反馈学习 (RLHF)，在英文和中文数学任务上进行了应用。</li>
<li>methods: 本文使用supervised fine-tuning (SFT)和人工反馈学习 (RLHF)来增强kwaiyiibase1的数学逻辑能力。</li>
<li>results: 本文在GSM8k、CMath和KMath上 achievements state-of-the-art (SOTA)性能，与相似大小的模型相比。<details>
<summary>Abstract</summary>
Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
</details>
<details>
<summary>摘要</summary>
近期大语言模型（LLM）的进步已经表现出对各种自然语言处理（NLP）下游任务的出色能力，甚至包括需要多步逻辑的数学任务。在这份报告中，我们介绍了废弃YiiMath，通过精度练习（SFT）和人工反馈学习（RLHF）进行数学逻辑能力的提升，包括英语和中文数学任务。同时，我们还建立了一个小规模的中文小学数学测试集（名为KMath），包含188个例子，用于评估模型生成的问题解决过程的正确性。实验表明，废弃YiiMath可以在GSM8k、CMath和KMath上达到相同规模模型的SOTA性能。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Graph-Learning-for-Generative-Tasks"><a href="#Multimodal-Graph-Learning-for-Generative-Tasks" class="headerlink" title="Multimodal Graph Learning for Generative Tasks"></a>Multimodal Graph Learning for Generative Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07478">http://arxiv.org/abs/2310.07478</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minjiyoon/mmgl">https://github.com/minjiyoon/mmgl</a></li>
<li>paper_authors: Minji Yoon, Jing Yu Koh, Bryan Hooi, Ruslan Salakhutdinov</li>
<li>for: 本研究旨在扩展现有的文本生成模型，以使其能够利用多模态数据进行生成。</li>
<li>methods: 我们提出了一种名为多模态图学习（MMGL）的框架，可以捕捉多模态数据之间的复杂关系。我们基于预训练语言模型（LM），并通过infuse多个邻居信息和图структура信息来增强其文本生成能力。</li>
<li>results: 我们通过了三个研究问题，即如何兼容多个邻居信息，如何兼容图структура信息，以及如何 Parametric-efficiently 训练预训练LM。我们的实验结果表明，MMGL可以增强文本生成能力，并且可以在兼容多个邻居信息和图структура信息的情况下进行Parameter-efficient 训练。<details>
<summary>Abstract</summary>
Multimodal learning combines multiple data modalities, broadening the types and complexity of data our models can utilize: for example, from plain text to image-caption pairs. Most multimodal learning algorithms focus on modeling simple one-to-one pairs of data from two modalities, such as image-caption pairs, or audio-text pairs. However, in most real-world settings, entities of different modalities interact with each other in more complex and multifaceted ways, going beyond one-to-one mappings. We propose to represent these complex relationships as graphs, allowing us to capture data with any number of modalities, and with complex relationships between modalities that can flexibly vary from one sample to another. Toward this goal, we propose Multimodal Graph Learning (MMGL), a general and systematic framework for capturing information from multiple multimodal neighbors with relational structures among them. In particular, we focus on MMGL for generative tasks, building upon pretrained Language Models (LMs), aiming to augment their text generation with multimodal neighbor contexts. We study three research questions raised by MMGL: (1) how can we infuse multiple neighbor information into the pretrained LMs, while avoiding scalability issues? (2) how can we infuse the graph structure information among multimodal neighbors into the LMs? and (3) how can we finetune the pretrained LMs to learn from the neighbor context in a parameter-efficient manner? We conduct extensive experiments to answer these three questions on MMGL and analyze the empirical results to pave the way for future MMGL research.
</details>
<details>
<summary>摘要</summary>
多模态学习结合多种数据模式，扩大我们模型使用的数据类型和复杂度：例如，从纯文本到图像描述对。大多数多模态学习算法都是模型简单的一对一对数据，如图像描述对或音频文本对。然而，在实际世界中，不同模式之间的实体往往存在更复杂和多方面的交互，超出一对一映射。我们建议表示这些复杂关系为图，以便捕捉多模式数据，并且在不同样本之间可以有不同的关系。为了实现这个目标，我们提出了多模态图学习（MMGL），一种通用和系统的框架，用于从多个多模式邻居中捕捉信息。具体来说，我们将关注MMGL的生成任务，基于预训练语言模型（LM），以增强其文本生成功能。我们提出了三个研究问题：（1）如何将多个邻居信息融入预训练LM中，而不会面临扩展性问题？（2）如何将多模式邻居之间的图结构信息融入LM中？（3）如何在LM中 parameter-efficient 地学习邻居 контекст？我们进行了广泛的实验和分析，以回答这三个问题，并为未来MMGL研究做出了基础。
</details></li>
</ul>
<hr>
<h2 id="An-Ontology-of-Co-Creative-AI-Systems"><a href="#An-Ontology-of-Co-Creative-AI-Systems" class="headerlink" title="An Ontology of Co-Creative AI Systems"></a>An Ontology of Co-Creative AI Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07472">http://arxiv.org/abs/2310.07472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyu Lin, Mark Riedl</li>
<li>for: 这个论文旨在为研究人员提供一种 Ontology of co-creative systems，以帮助解决人工智能和人类在创造过程中的分工和信息交换问题。</li>
<li>methods: 这篇论文使用了 Lubart 的原始 Ontology of creativity support tools，并新增了三个类别，专注于人工智能：计算机作为合作伙伴、计算机作为评论人和计算机作为合作者，其中一些有子类别。</li>
<li>results: 论文提出了一个 Ontology of co-creative systems，可以帮助研究人员更好地理解和分析人工智能和人类在创造过程中的相互作用，并且可以帮助设计更加有效的人工智能和人类合作系统。<details>
<summary>Abstract</summary>
The term co-creativity has been used to describe a wide variety of human-AI assemblages in which human and AI are both involved in a creative endeavor. In order to assist with disambiguating research efforts, we present an ontology of co-creative systems, focusing on how responsibilities are divided between human and AI system and the information exchanged between them. We extend Lubart's original ontology of creativity support tools with three new categories emphasizing artificial intelligence: computer-as-subcontractor, computer-as-critic, and computer-as-teammate, some of which have sub-categorizations.
</details>
<details>
<summary>摘要</summary>
《合作创造力》一词汇集了许多人工智能融合体，在创造活动中，人类和AI系统都参与了创新。为了帮助研究工作，我们提出了一个协创系统 ontology，关注人类和 AI 系统之间的责任分配和信息交换。我们对 Lubart 的原始创造支持工具 ontology 进行扩展，添加了三个新类型，强调人工智能：计算机作为合同人，计算机作为评论人，计算机作为团队成员，其中有一些子分类。
</details></li>
</ul>
<hr>
<h2 id="The-Implications-of-Decentralization-in-Blockchained-Federated-Learning-Evaluating-the-Impact-of-Model-Staleness-and-Inconsistencies"><a href="#The-Implications-of-Decentralization-in-Blockchained-Federated-Learning-Evaluating-the-Impact-of-Model-Staleness-and-Inconsistencies" class="headerlink" title="The Implications of Decentralization in Blockchained Federated Learning: Evaluating the Impact of Model Staleness and Inconsistencies"></a>The Implications of Decentralization in Blockchained Federated Learning: Evaluating the Impact of Model Staleness and Inconsistencies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07471">http://arxiv.org/abs/2310.07471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesc Wilhelmi, Nima Afraz, Elia Guerra, Paolo Dini</li>
<li>for: 这篇论文探讨了在区块链上实现分布式机器学习（DL）的可能性，尤其是在分布式学习（FL）中，区块链可以提供更多的分布式、安全、不可变和信任等特性，这些特性可以激发下一代应用中的共同智能。</li>
<li>methods: 论文使用了预训练模型，并在块链上进行了模型协调和更新。</li>
<li>results: 研究发现，在使用块链进行FL时，模型不一致和延迟会导致预测精度下降（下降约35%），这说明在设计块链系统时，需要考虑FL应用的特点。<details>
<summary>Abstract</summary>
Blockchain promises to enhance distributed machine learning (ML) approaches such as federated learning (FL) by providing further decentralization, security, immutability, and trust, which are key properties for enabling collaborative intelligence in next-generation applications. Nonetheless, the intrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads to an uncharted setting for FL, whereby the concepts of FL round and global model become meaningless, as devices' synchronization is lost without the figure of a central orchestrating server. In this paper, we study the practical implications of outsourcing the orchestration of FL to a democratic network such as in a blockchain. In particular, we focus on the effects that model staleness and inconsistencies, endorsed by blockchains' modus operandi, have on the training procedure held by FL devices asynchronously. Using simulation, we evaluate the blockchained FL operation on the well-known CIFAR-10 dataset and focus on the accuracy and timeliness of the solutions. Our results show the high impact of model inconsistencies on the accuracy of the models (up to a ~35% decrease in prediction accuracy), which underscores the importance of properly designing blockchain systems based on the characteristics of the underlying FL application.
</details>
<details>
<summary>摘要</summary>
blockchain 承诺增强分布式机器学习（ML）方法，如联合学习（FL），通过提供更多的分布式、安全、不可变和信任性，这些属性是实现共同智能在下一代应用程序的关键。然而，干预式P2P区块链节点的自然分布式运行环境导致FL中的概念，例如轮次和全局模型，失去意义，因为设备的同步失去了中央调度服务器的引导。在这篇论文中，我们研究了在块链上进行FL的实际影响。具体来说，我们关注FL设备在异步情况下进行训练过程中的模型落后和不一致性问题，这些问题由块链的操作方式推动。通过实验，我们评估了使用块链进行FL操作在CIFAR-10数据集上的性能。我们发现，模型不一致性可以导致预测精度下降，最高下降约35%，这说明了如何设计基于FL应用程序的块链系统的重要性。
</details></li>
</ul>
<hr>
<h2 id="AI-ML-based-Load-Prediction-in-IEEE-802-11-Enterprise-Networks"><a href="#AI-ML-based-Load-Prediction-in-IEEE-802-11-Enterprise-Networks" class="headerlink" title="AI&#x2F;ML-based Load Prediction in IEEE 802.11 Enterprise Networks"></a>AI&#x2F;ML-based Load Prediction in IEEE 802.11 Enterprise Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07467">http://arxiv.org/abs/2310.07467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesc Wilhelmi, Dariush Salami, Gianluca Fontanesi, Lorenzo Galati-Giordano, Mika Kasslin</li>
<li>for: 该论文旨在探讨在实际企业 Wi-Fi 网络中采用基于人工智能和机器学习（AI&#x2F;ML）的负荷预测是否可行和有效。</li>
<li>methods: 该论文采用了基于 AI&#x2F;ML 的负荷预测方法，并对其适用性和可行性进行了研究。</li>
<li>results: 研究发现，使用硬件受限的 AI&#x2F;ML 模型可以预测网络负荷，误差在20%以下，85%分位数在3%以下，这可以作为 Wi-Fi 网络优化的输入。<details>
<summary>Abstract</summary>
Enterprise Wi-Fi networks can greatly benefit from Artificial Intelligence and Machine Learning (AI/ML) thanks to their well-developed management and operation capabilities. At the same time, AI/ML-based traffic/load prediction is one of the most appealing data-driven solutions to improve the Wi-Fi experience, either through the enablement of autonomous operation or by boosting troubleshooting with forecasted network utilization. In this paper, we study the suitability and feasibility of adopting AI/ML-based load prediction in practical enterprise Wi-Fi networks. While leveraging AI/ML solutions can potentially contribute to optimizing Wi-Fi networks in terms of energy efficiency, performance, and reliability, their effective adoption is constrained to aspects like data availability and quality, computational capabilities, and energy consumption. Our results show that hardware-constrained AI/ML models can potentially predict network load with less than 20% average error and 3% 85th-percentile error, which constitutes a suitable input for proactively driving Wi-Fi network optimization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficient-machine-learning-surrogates-for-large-scale-geological-carbon-and-energy-storage"><a href="#Efficient-machine-learning-surrogates-for-large-scale-geological-carbon-and-energy-storage" class="headerlink" title="Efficient machine-learning surrogates for large-scale geological carbon and energy storage"></a>Efficient machine-learning surrogates for large-scale geological carbon and energy storage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07461">http://arxiv.org/abs/2310.07461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teeratorn Kadeethum, Stephen J. Verzi, Hongkyu Yoon</li>
<li>for: 这篇论文是为了探讨地质碳和能源储存在实现零碳排放和气候变化管理方面所面临的不确定性，并提出一个特殊化的机器学习（ML）模型来有效管理大规模的油气储存模型。</li>
<li>methods: 这篇论文使用了一种特殊的机器学习模型，具有预测精度和训练成本之间的平衡，以解决大规模地质碳储存应用中的计算资源限制问题。</li>
<li>results: 这篇论文的结果显示，这种特殊的机器学习模型可以实现高精度的预测，并且可以对大规模的油气储存应用进行有效管理，协助解决地质碳储存中的不确定性和操作限制问题。<details>
<summary>Abstract</summary>
Geological carbon and energy storage are pivotal for achieving net-zero carbon emissions and addressing climate change. However, they face uncertainties due to geological factors and operational limitations, resulting in possibilities of induced seismic events or groundwater contamination. To overcome these challenges, we propose a specialized machine-learning (ML) model to manage extensive reservoir models efficiently.   While ML approaches hold promise for geological carbon storage, the substantial computational resources required for large-scale analysis are the obstacle. We've developed a method to reduce the training cost for deep neural operator models, using domain decomposition and a topology embedder to link spatio-temporal points. This approach allows accurate predictions within the model's domain, even for untrained data, enhancing ML efficiency for large-scale geological storage applications.
</details>
<details>
<summary>摘要</summary>
地质碳和能源储存对于实现零碳排放和气候变化做出重要贡献，但它们面临地质因素和运营限制，可能导致人工地震或地下水污染。为了解决这些挑战，我们提议使用专门的机器学习（ML）模型来有效管理大规模的沉存模型。While ML approaches hold promise for geological carbon storage, the substantial computational resources required for large-scale analysis are the obstacle. We've developed a method to reduce the training cost for deep neural operator models, using domain decomposition and a topology embedder to link spatio-temporal points. This approach allows accurate predictions within the model's domain, even for untrained data, enhancing ML efficiency for large-scale geological storage applications.Translation notes:* "地质碳" (dì qiè diān) means "geological carbon"* "能源储存" (néngyuan jīcè) means "energy storage"* "零碳排放" (líng diān fāshuā) means "net-zero carbon emissions"* "气候变化" (qìngkēng biànhàng) means "climate change"* "地质因素" (dì qiè yīnxiàng) means "geological factors"* "运营限制" (yùn yì jìzhèng) means "operational limitations"* "人工地震" (réngōng dìzhèn) means "induced seismic events"* "地下水污染" (dì xià shuǐwù) means "groundwater contamination"* "ML" (M L) stands for "machine learning"* "沉存模型" (chéncèng módel) means "reservoir models"* "域 decomposition" (dì zhāng) means "domain decomposition"* "トポлоги embedder" (tuōpōlógì yìbiāo) means "topology embedder"* "链接点" (liánjié diǎn) means "linking points"* "模型的域" (módel de dì) means "the model's domain"* "untrained data" (wutaining tiào xiàng) means "untrained data"
</details></li>
</ul>
<hr>
<h2 id="HealthWalk-Promoting-Health-and-Mobility-through-Sensor-Based-Rollator-Walker-Assistance"><a href="#HealthWalk-Promoting-Health-and-Mobility-through-Sensor-Based-Rollator-Walker-Assistance" class="headerlink" title="HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator Walker Assistance"></a>HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator Walker Assistance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07434">http://arxiv.org/abs/2310.07434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivanna Kramer, Kevin Weirauch, Sabine Bauer, Mark Oliver Mints, Peer Neubert</li>
<li>for: 增强physically limited人群 mobilty和独立参与社会</li>
<li>methods:  интегриyo sensors into rollator walker designs</li>
<li>results: 数据收集和其他 interessin use casesHere’s the information in Simplified Chinese text:</li>
<li>for: 增强physically limited人群 mobilty和独立参与社会</li>
<li>methods:  интегриyo sensors into rollator walker designs</li>
<li>results: 数据收集和其他 interessin use casesI hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Rollator walkers allow people with physical limitations to increase their mobility and give them the confidence and independence to participate in society for longer. However, rollator walker users often have poor posture, leading to further health problems and, in the worst case, falls. Integrating sensors into rollator walker designs can help to address this problem and results in a platform that allows several other interesting use cases. This paper briefly overviews existing systems and the current research directions and challenges in this field. We also present our early HealthWalk rollator walker prototype for data collection with older people, rheumatism, multiple sclerosis and Parkinson patients, and individuals with visual impairments.
</details>
<details>
<summary>摘要</summary>
轮椅步行器让人们 WITH 身体限制能够提高 mobilility 并给他们带来自信和独立，以便 longer 参与社会。但是轮椅步行器用户 часто有坏 posture，导致更多的健康问题，甚至最坏情况下掉进了 falls。把感应器 integrate 到轮椅步行器设计中可以解决这个问题，并且可以实现多种有趣的用case。本文 briefly 概述了现有系统和这个领域的当前研究方向和挑战。我们也 Present 我们的早期 HealthWalk 轮椅步行器原型，用于收集数据 FROM older people, rheumatism, multiple sclerosis 和 Parkinson 病人，以及Visual impairments 患者。
</details></li>
</ul>
<hr>
<h2 id="Imitation-Learning-from-Observation-with-Automatic-Discount-Scheduling"><a href="#Imitation-Learning-from-Observation-with-Automatic-Discount-Scheduling" class="headerlink" title="Imitation Learning from Observation with Automatic Discount Scheduling"></a>Imitation Learning from Observation with Automatic Discount Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07433">http://arxiv.org/abs/2310.07433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyang Liu, Weijun Dong, Yingdong Hu, Chuan Wen, Zhao-Heng Yin, Chongjie Zhang, Yang Gao<br>for:本研究旨在解决机器人学习从视频示例数据中学习的问题，即“imitating the expert without access to its action”。methods:本研究使用了一种 inverse reinforcement learning 方法，将 ILfO 问题转化为 reinforcement learning 问题，使用代理奖金计算从 agent 和专家的观察中。results:实验结果显示，我们的方法在 nine Meta-World 任务上表现出色，与现有方法比较，在所有任务上均有显著改善，包括一些不可解决的任务。<details>
<summary>Abstract</summary>
Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earlier behaviors before advancing to later ones. We introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively alters the discount factor in reinforcement learning during the training phase, prioritizing earlier rewards initially and gradually engaging later rewards only when the earlier behaviors have been mastered. Our experiments, conducted on nine Meta-World tasks, demonstrate that our method significantly outperforms state-of-the-art methods across all tasks, including those that are unsolvable by them.
</details>
<details>
<summary>摘要</summary>
人类常通过观察和模仿获得新技能。对于机器人代理，从互联网上的大量未标注视频示例数据中学习，却面临了无法访问行为者的行为的挑战，称为观察学习从抽象（ILfO）。常见的解决ILfO问题的方法是将其转化为反杂化学习问题，使用代理人和行为者的观察得到的代理奖励。然而，我们发现任务具有进步依赖性属性时，这些方法会遇到很大的挑战，因为代理人需要在学习早期的行为之前，才能学习专家的后续行为。我们的研究发现，主要的问题在于奖励信号赋给后续步骤会阻碍代理人学习初期的行为。为解决这个挑战，我们提出了一种新的ILfO框架，使得代理人可以在训练期间，先学习初期的行为，然后才能进行后续的学习。我们在训练过程中引入自适应折扣因子调整机制（ADS），根据训练过程中的步骤，自动调整折扣因子，在初期偏好早期奖励，逐渐地只有在初期行为已经熟悉时，才能够参与后续奖励。我们在Meta-World任务上进行了九项实验，结果表明，我们的方法在所有任务上都有显著的优势，包括一些不可能由现有方法解决的任务。
</details></li>
</ul>
<hr>
<h2 id="Multi-Concept-T2I-Zero-Tweaking-Only-The-Text-Embeddings-and-Nothing-Else"><a href="#Multi-Concept-T2I-Zero-Tweaking-Only-The-Text-Embeddings-and-Nothing-Else" class="headerlink" title="Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing Else"></a>Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing Else</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07419">http://arxiv.org/abs/2310.07419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hazarapet Tunanyan, Dejia Xu, Shant Navasardyan, Zhangyang Wang, Humphrey Shi</li>
<li>for: 提高文本描述逻辑概率生成图像的自然多元概念能力，不需要额外训练或执行时指导。</li>
<li>methods: 通过修正预训练文本扩展模型中的文本嵌入，解决概念占据和非本地贡献问题，提高多元概念图像生成性能。</li>
<li>results: 在文本描述逻辑概率生成图像、图像修改和个性化任务中，比前方法高效，且不需要额外训练或执行时指导。<details>
<summary>Abstract</summary>
Recent advances in text-to-image diffusion models have enabled the photorealistic generation of images from text prompts. Despite the great progress, existing models still struggle to generate compositional multi-concept images naturally, limiting their ability to visualize human imagination. While several recent works have attempted to address this issue, they either introduce additional training or adopt guidance at inference time. In this work, we consider a more ambitious goal: natural multi-concept generation using a pre-trained diffusion model, and with almost no extra cost. To achieve this goal, we identify the limitations in the text embeddings used for the pre-trained text-to-image diffusion models. Specifically, we observe concept dominance and non-localized contribution that severely degrade multi-concept generation performance. We further design a minimal low-cost solution that overcomes the above issues by tweaking (not re-training) the text embeddings for more realistic multi-concept text-to-image generation. Our Correction by Similarities method tweaks the embedding of concepts by collecting semantic features from most similar tokens to localize the contribution. To avoid mixing features of concepts, we also apply Cross-Token Non-Maximum Suppression, which excludes the overlap of contributions from different concepts. Experiments show that our approach outperforms previous methods in text-to-image, image manipulation, and personalization tasks, despite not introducing additional training or inference costs to the diffusion steps.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Revisiting-Plasticity-in-Visual-Reinforcement-Learning-Data-Modules-and-Training-Stages"><a href="#Revisiting-Plasticity-in-Visual-Reinforcement-Learning-Data-Modules-and-Training-Stages" class="headerlink" title="Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages"></a>Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07418">http://arxiv.org/abs/2310.07418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Guozheng-Ma/Adaptive-Replay-Ratio">https://github.com/Guozheng-Ma/Adaptive-Replay-Ratio</a></li>
<li>paper_authors: Guozheng Ma, Lu Li, Sen Zhang, Zixuan Liu, Zhen Wang, Yixin Chen, Li Shen, Xueqian Wang, Dacheng Tao</li>
<li>for: 这篇论文的目的是研究强化学习中的塑性（plasticity）如何影响高性能和效率的视觉强化学习（VRL）。</li>
<li>methods: 该论文使用了系统的实验尝试，探讨了三个未曾充分研究的方面，并得出了以下有益的结论：（1）数据扩展是维持塑性的关键因素；（2）评价器的塑性损失是干扰高效培训的主要障碍；（3）在早期阶段不及时 intervene 可能会导致评价器的塑性损失变成致命的问题。</li>
<li>results: 该论文的研究结果表明，适应式 RR 可以避免早期阶段的评价器塑性损失，并在后期阶段更频 reuse，从而提高样本效率。<details>
<summary>Abstract</summary>
Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased reuse frequency. Rather than setting a static RR for the entire training process, we propose Adaptive RR, which dynamically adjusts the RR based on the critic's plasticity level. Extensive evaluations indicate that Adaptive RR not only avoids catastrophic plasticity loss in the early stages but also benefits from more frequent reuse in later phases, resulting in superior sample efficiency.
</details>
<details>
<summary>摘要</summary>
neural network 的可变性(plasticity) 是视觉强化学习(VRL) 的关键因素，它可以使 agent 在新数据上进行学习和改进。 although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. 在这项工作中，我们进行了系统性的实验研究，关注三个次要未经探索的方面，得到以下有价值的结论：（1）数据扩展是维护可变性的关键因素；（2）批评者的可变性损失是训练的主要瓶颈；（3）在早期阶段没有及时干预可以使批评者的可变性恢复，这种损失会变成灾难性的。这些结论建议了一种新的策略，即适应性的 reuse frequency（RR），可以 dynamically 根据批评者的可变性水平进行调整。我们的实验表明，适应性的 RR 不仅可以避免批评者的可变性损失在早期阶段，还可以在后期阶段更频 reuse，从而提高样本效率。
</details></li>
</ul>
<hr>
<h2 id="What-can-knowledge-graph-alignment-gain-with-Neuro-Symbolic-learning-approaches"><a href="#What-can-knowledge-graph-alignment-gain-with-Neuro-Symbolic-learning-approaches" class="headerlink" title="What can knowledge graph alignment gain with Neuro-Symbolic learning approaches?"></a>What can knowledge graph alignment gain with Neuro-Symbolic learning approaches?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07417">http://arxiv.org/abs/2310.07417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Giesteira Cotovio, Ernesto Jimenez-Ruiz, Catia Pesquita</li>
<li>for: 本研究旨在探讨现有的知识图гра（KG）对应算法的局限性，以及可以通过结合符号学习和数字学习的гибри德学习模型来改进KGA的性能和可解性。</li>
<li>methods: 本研究使用了现有的KGA算法和深度学习模型，以及一些相关的数字学习和符号学习方法进行比较和分析。</li>
<li>results: 研究发现，结合符号学习和数字学习的гибри德学习模型可以提高KGA的性能和可解性，并且可以支持人类中心的验证和验证方法。<details>
<summary>Abstract</summary>
Knowledge Graphs (KG) are the backbone of many data-intensive applications since they can represent data coupled with its meaning and context. Aligning KGs across different domains and providers is necessary to afford a fuller and integrated representation. A severe limitation of current KG alignment (KGA) algorithms is that they fail to articulate logical thinking and reasoning with lexical, structural, and semantic data learning. Deep learning models are increasingly popular for KGA inspired by their good performance in other tasks, but they suffer from limitations in explainability, reasoning, and data efficiency. Hybrid neurosymbolic learning models hold the promise of integrating logical and data perspectives to produce high-quality alignments that are explainable and support validation through human-centric approaches. This paper examines the current state of the art in KGA and explores the potential for neurosymbolic integration, highlighting promising research directions for combining these fields.
</details>
<details>
<summary>摘要</summary>
知识 graphs (KG) 是许多数据敏感应用的重要组成部分，因为它们可以表示数据以及其意义和上下文。对不同领域和提供商的 KG 进行对接是必要的，以便获得更加全面和集成的表示。当前的 KG 对应 (KGA) 算法有一定的限制，它们无法体现逻辑思维和语言、结构和 semantics 数据学习的相互作用。深入学习模型在 KGA 方面具有良好的表现，但它们受到解释性、逻辑和数据效率的限制。混合 neuralsymbolic 学习模型可以结合逻辑和数据视角，生成高质量的对接，同时可以提供可解释的结果和人类中心的验证方法。本文将对当前 KGA 领域的状况进行检查，并探讨将这两个领域结合在一起的潜在研究方向。
</details></li>
</ul>
<hr>
<h2 id="DASpeech-Directed-Acyclic-Transformer-for-Fast-and-High-quality-Speech-to-Speech-Translation"><a href="#DASpeech-Directed-Acyclic-Transformer-for-Fast-and-High-quality-Speech-to-Speech-Translation" class="headerlink" title="DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation"></a>DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07403">http://arxiv.org/abs/2310.07403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ictnlp/daspeech">https://github.com/ictnlp/daspeech</a></li>
<li>paper_authors: Qingkai Fang, Yan Zhou, Yang Feng<br>for:* DASpeech is a non-autoregressive direct speech-to-speech translation model that aims to achieve both high-quality translations and fast decoding speeds.methods:* DASpeech uses a two-pass architecture to decompose the generation process into two steps: a linguistic decoder first generates the target text, and an acoustic decoder then generates the target speech based on the hidden states of the linguistic decoder.* DA-Transformer models translations with a directed acyclic graph (DAG), and dynamic programming is used to calculate the expected hidden states for each target token during training.results:* DASpeech achieves comparable or even better performance than the state-of-the-art S2ST model Translatotron 2, while preserving up to 18.53x speedup compared to the autoregressive baseline.* DASpeech shows significant improvements in both translation quality and decoding speed compared to previous non-autoregressive S2ST models, without relying on knowledge distillation or iterative decoding.* DASpeech preserves the speaker’s voice of the source speech during translation.Here is the format you requested:for:  direkt S2ST模型可以实现高质量翻译和快速解码methods:  two-pass架构，首先使用语言解码器生成目标文本，然后使用听音解码器基于语言解码器的隐藏状态生成目标语音results:  comparable或更好的性能，与当前状态OF-the-art S2ST模型Translatotron 2相当，同时保持18.53倍的速度提升相比于核心基eline。<details>
<summary>Abstract</summary>
Direct speech-to-speech translation (S2ST) translates speech from one language into another using a single model. However, due to the presence of linguistic and acoustic diversity, the target speech follows a complex multimodal distribution, posing challenges to achieving both high-quality translations and fast decoding speeds for S2ST models. In this paper, we propose DASpeech, a non-autoregressive direct S2ST model which realizes both fast and high-quality S2ST. To better capture the complex distribution of the target speech, DASpeech adopts the two-pass architecture to decompose the generation process into two steps, where a linguistic decoder first generates the target text, and an acoustic decoder then generates the target speech based on the hidden states of the linguistic decoder. Specifically, we use the decoder of DA-Transformer as the linguistic decoder, and use FastSpeech 2 as the acoustic decoder. DA-Transformer models translations with a directed acyclic graph (DAG). To consider all potential paths in the DAG during training, we calculate the expected hidden states for each target token via dynamic programming, and feed them into the acoustic decoder to predict the target mel-spectrogram. During inference, we select the most probable path and take hidden states on that path as input to the acoustic decoder. Experiments on the CVSS Fr-En benchmark demonstrate that DASpeech can achieve comparable or even better performance than the state-of-the-art S2ST model Translatotron 2, while preserving up to 18.53x speedup compared to the autoregressive baseline. Compared with the previous non-autoregressive S2ST model, DASpeech does not rely on knowledge distillation and iterative decoding, achieving significant improvements in both translation quality and decoding speed. Furthermore, DASpeech shows the ability to preserve the speaker's voice of the source speech during translation.
</details>
<details>
<summary>摘要</summary>
直接Speech-to-Speech翻译（S2ST）模型可以将一种语言的语音翻译成另一种语言。然而，由于语言和听音多样性，目标语音表现出复杂的多Modal分布，这会对S2ST模型的高质量翻译和快速解码速度带来挑战。在这篇论文中，我们提出了DASpeech模型，这是一种非autoregressive的直接S2ST模型，可以同时实现高质量翻译和快速解码速度。为了更好地捕捉目标语音的复杂分布，DASpeech采用了两个过程来分解生成过程，其中一个是语言解码器，它首先生成目标文本；另一个是听音解码器，它根据语言解码器生成的隐藏状态来生成目标听音spectrogram。我们使用DA-Transformer模型的解码器作为语言解码器，并使用FastSpeech 2模型作为听音解码器。在训练时，我们使用DAG模型来表示翻译，并通过动态计算隐藏状态来考虑所有可能的路径。在推理时，我们选择最有可能性的路径，并将隐藏状态feed into听音解码器来预测目标听音spectrogram。实验结果表明，DASpeech可以与状态艺术S2ST模型Translatotron 2进行比较，同时保持18.53倍的速度提升。与之前的非autoregressive S2ST模型相比，DASpeech不需要知识储存和迭代解码，它可以实现显著的改进 both translation quality和解码速度。此外，DASpeech还可以保持源语音的 speaker voice。
</details></li>
</ul>
<hr>
<h2 id="NuTime-Numerically-Multi-Scaled-Embedding-for-Large-Scale-Time-Series-Pretraining"><a href="#NuTime-Numerically-Multi-Scaled-Embedding-for-Large-Scale-Time-Series-Pretraining" class="headerlink" title="NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining"></a>NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07402">http://arxiv.org/abs/2310.07402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenguo Lin, Xumeng Wen, Wei Cao, Congrui Huang, Jiang Bian, Stephen Lin, Zhirong Wu</li>
<li>for: 学习时序数据的 semantic 表示。</li>
<li>methods: 采用 Transformer 架构，首先将输入分割成不重叠的窗口，然后对每个窗口进行 numerically multi-scaled embedding。</li>
<li>results: 在多个uniivariate和multivariate classification benchmark上显示出惊人的改进，并在非学习基础方法中Establish新的状态码。<details>
<summary>Abstract</summary>
Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales to high-dimensional vectors, we propose a numerically multi-scaled embedding module enumerating all possible scales for the scalar values. The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset containing over a million sequences. We study its transfer performance on a number of univariate and multivariate classification benchmarks. Our method exhibits remarkable improvement against previous representation learning approaches and establishes the new state of the art, even compared with domain-specific non-learning-based methods.
</details>
<details>
<summary>摘要</summary>
We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales into high-dimensional vectors, we propose a numerically multi-scaled embedding module that enumerates all possible scales for the scalar values.The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset containing over a million sequences. We study its transfer performance on a number of univariate and multivariate classification benchmarks. Our method exhibits remarkable improvement compared to previous representation learning approaches and establishes a new state of the art, even compared with domain-specific non-learning-based methods.
</details></li>
</ul>
<hr>
<h2 id="Target-oriented-Proactive-Dialogue-Systems-with-Personalization-Problem-Formulation-and-Dataset-Curation"><a href="#Target-oriented-Proactive-Dialogue-Systems-with-Personalization-Problem-Formulation-and-Dataset-Curation" class="headerlink" title="Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation"></a>Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07397">http://arxiv.org/abs/2310.07397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iwangjian/topdial">https://github.com/iwangjian/topdial</a></li>
<li>paper_authors: Jian Wang, Yi Cheng, Dongding Lin, Chak Tou Leong, Wenjie Li<br>for: 这 paper 是研究 targets-oriented dialogue systems 的，它们可以帮助对话系统更好地驱动对话向 predetermined targets 或达到系统方面的目标。methods: 这 paper 使用了一种新的方法，即在 &lt;dialogue act, topic&gt; 对应的对话中进行个性化目标途径。它还提出了一种自动 dataset curation 框架，使用 role-playing 方法来生成大规模的个性化目标对话数据集。results: 这 paper 通过实验表明，这些个性化目标对话数据集具有高质量，可以用于探索个性化目标对话。<details>
<summary>Abstract</summary>
Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a <dialogue act, topic> pair as the conversation target, we explore a novel problem of personalized target-oriented dialogue by considering personalization during the target accomplishment process. However, there remains an emergent need for high-quality datasets, and building one from scratch requires tremendous human effort. To address this, we propose an automatic dataset curation framework using a role-playing approach. Based on this framework, we construct a large-scale personalized target-oriented dialogue dataset, TopDial, which comprises about 18K multi-turn dialogues. The experimental results show that this dataset is of high quality and could contribute to exploring personalized target-oriented dialogue.
</details>
<details>
<summary>摘要</summary>
目标导向对话系统，旨在主动导引对话向预定的目标或完成系统侧的目标，是现代对话智能领域的一个有趣领域。在这种工作中，我们通过формализова对话行为和话题的对应关系，探索一种个性化目标导向对话的问题。然而，有一定的需求升级高质量的数据集，建立自身的数据集需要巨大的人工劳动。为解决这个问题，我们提出了一种自动数据集筛选框架，基于角色扮演的方法。通过这种框架，我们建立了一个大规模的个性化目标导向对话数据集，TopDial，包含约18K多轮对话。实验结果表明，这个数据集具有高质量，可以探索个性化目标导向对话。
</details></li>
</ul>
<hr>
<h2 id="Learning-a-Reward-Function-for-User-Preferred-Appliance-Scheduling"><a href="#Learning-a-Reward-Function-for-User-Preferred-Appliance-Scheduling" class="headerlink" title="Learning a Reward Function for User-Preferred Appliance Scheduling"></a>Learning a Reward Function for User-Preferred Appliance Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07389">http://arxiv.org/abs/2310.07389</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikskiks/learning-reward-function-demand-response">https://github.com/nikskiks/learning-reward-function-demand-response</a></li>
<li>paper_authors: Nikolina Čović, Jochen Cremer, Hrvoje Pandžić</li>
<li>for: 降低电力 секто�的碳排放，需要加快住宅部门的需给回应服务提供的推展。</li>
<li>methods: 使用反馈学习算法，将住宅用户的过往消耗数据变数为创建每天家用电器运行时间表。</li>
<li>results: 透过不询问住宅用户的需求和愿望，将住宅用户内在地参与服务设计和决策过程，并透过金钱或环保动机来鼓励住宅用户继续参与需给回应服务。<details>
<summary>Abstract</summary>
Accelerated development of demand response service provision by the residential sector is crucial for reducing carbon-emissions in the power sector. Along with the infrastructure advancement, encouraging the end users to participate is crucial. End users highly value their privacy and control, and want to be included in the service design and decision-making process when creating the daily appliance operation schedules. Furthermore, unless they are financially or environmentally motivated, they are generally not prepared to sacrifice their comfort to help balance the power system. In this paper, we present an inverse-reinforcement-learning-based model that helps create the end users' daily appliance schedules without asking them to explicitly state their needs and wishes. By using their past consumption data, the end consumers will implicitly participate in the creation of those decisions and will thus be motivated to continue participating in the provision of demand response services.
</details>
<details>
<summary>摘要</summary>
加速了住宅部分的需求应答服务提供的发展是减少能源部门碳排放的关键。同时，激励终端用户参与是关键。终端用户强烈关注隐私和控制，希望在日常家用电器运行时间的设计和决策过程中被包括。此外，如果他们没有经济或环境上的驱动力，他们通常不愿意为了帮助平衡能源系统而做出牺牲。本文提出了一种基于逆激励学习的模型，可以无需直接询问终端用户需求和愿望，通过使用其过去的消耗数据，使终端用户在创造这些决策过程中implicitly参与，从而被激励继续参与提供需求应答服务。
</details></li>
</ul>
<hr>
<h2 id="Histopathological-Image-Classification-and-Vulnerability-Analysis-using-Federated-Learning"><a href="#Histopathological-Image-Classification-and-Vulnerability-Analysis-using-Federated-Learning" class="headerlink" title="Histopathological Image Classification and Vulnerability Analysis using Federated Learning"></a>Histopathological Image Classification and Vulnerability Analysis using Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07380">http://arxiv.org/abs/2310.07380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sankalp Vyas, Amar Nath Patra, Raj Mani Shukla</li>
<li>for: 革新健康预测技术，保护用户隐私</li>
<li>methods: 联邦学习（Federated Learning）技术，对于肉眼皮癌资料集进行预测</li>
<li>results: 发现联邦学习容易受到数据毒素攻击，影响模型的精度。透过测试实验，发现当数据毒素比例增加时，模型的精度会显著下降。<details>
<summary>Abstract</summary>
Healthcare is one of the foremost applications of machine learning (ML). Traditionally, ML models are trained by central servers, which aggregate data from various distributed devices to forecast the results for newly generated data. This is a major concern as models can access sensitive user information, which raises privacy concerns. A federated learning (FL) approach can help address this issue: A global model sends its copy to all clients who train these copies, and the clients send the updates (weights) back to it. Over time, the global model improves and becomes more accurate. Data privacy is protected during training, as it is conducted locally on the clients' devices.   However, the global model is susceptible to data poisoning. We develop a privacy-preserving FL technique for a skin cancer dataset and show that the model is prone to data poisoning attacks. Ten clients train the model, but one of them intentionally introduces flipped labels as an attack. This reduces the accuracy of the global model. As the percentage of label flipping increases, there is a noticeable decrease in accuracy. We use a stochastic gradient descent optimization algorithm to find the most optimal accuracy for the model. Although FL can protect user privacy for healthcare diagnostics, it is also vulnerable to data poisoning, which must be addressed.
</details>
<details>
<summary>摘要</summary>
医疗是机器学习（ML）的一个重要应用领域。传统上，ML模型通常由中央服务器进行训练，该服务器将来自不同分布式设备的数据集成以预测新生成的数据结果。这会导致隐私披露问题，因为模型可以访问敏感用户信息。一种联邦学习（FL）方法可以解决这个问题：全球模型将其 копию传递给所有客户端，客户端将其更新（ weights）回传给它。随着全球模型的改进，其精度会逐渐提高。在训练过程中，数据隐私得到保护，因为训练在客户端上进行。然而，全球模型受到数据毒品攻击的威胁。我们开发了一种隐私保护的FL技术，并在皮肤癌数据集上进行了实验。发现，当一个客户端意外地将标签反转为攻击时，全球模型的准确率会下降。随着标签反转的百分比增加，全球模型的准确率会显著下降。我们使用某种随机梯度下降优化算法来找到最佳准确率。虽然FL可以保护用户隐私 для医疗诊断，但也容易受到数据毒品攻击，这些问题需要解决。
</details></li>
</ul>
<hr>
<h2 id="Causal-Unsupervised-Semantic-Segmentation"><a href="#Causal-Unsupervised-Semantic-Segmentation" class="headerlink" title="Causal Unsupervised Semantic Segmentation"></a>Causal Unsupervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07379">http://arxiv.org/abs/2310.07379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/byungkwanlee/causal-unsupervised-segmentation">https://github.com/byungkwanlee/causal-unsupervised-segmentation</a></li>
<li>paper_authors: Junho Kim, Byung-Kwan Lee, Yong Man Ro</li>
<li>for: 这个论文的目的是提出一种新的无监督 semantic segmentation 方法，以实现高质量的semantic grouping无需人工标注。</li>
<li>methods: 该方法利用自动预训练的特征进行train prediction heads，并采用 causal inference 的思想来定义适当的 clustering 级别。</li>
<li>results: 通过大量实验和分析，该方法在不同的数据集上达到了无监督 semantic segmentation 的州OF-THE-ART表现。<details>
<summary>Abstract</summary>
Unsupervised semantic segmentation aims to achieve high-quality semantic grouping without human-labeled annotations. With the advent of self-supervised pre-training, various frameworks utilize the pre-trained features to train prediction heads for unsupervised dense prediction. However, a significant challenge in this unsupervised setup is determining the appropriate level of clustering required for segmenting concepts. To address it, we propose a novel framework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages insights from causal inference. Specifically, we bridge intervention-oriented approach (i.e., frontdoor adjustment) to define suitable two-step tasks for unsupervised prediction. The first step involves constructing a concept clusterbook as a mediator, which represents possible concept prototypes at different levels of granularity in a discretized form. Then, the mediator establishes an explicit link to the subsequent concept-wise self-supervised learning for pixel-level grouping. Through extensive experiments and analyses on various datasets, we corroborate the effectiveness of CAUSE and achieve state-of-the-art performance in unsupervised semantic segmentation.
</details>
<details>
<summary>摘要</summary>
自动 semantic segmentation 的目标是实现高质量的 semantic grouping 无需人工标注。随着自我超级预训练的出现，各种框架通过预训练特征来训练预测头 для无监督的稠密预测。然而，在这种无监督设置中，确定适当的归类水平是一个重要挑战。为解决这个问题，我们提出了一种新的框架，名为 CAusal Unsupervised Semantic sEgmentation (CAUSE)，它利用 causal inference 的启示。具体来说，我们将 intervention-oriented approach (i.e., frontdoor adjustment) 引入，定义适当的两步任务 для无监督预测。第一步是构建 concept clusterbook 作为中间变量，它表示不同粒度的概念原型的可能性。然后， mediator 建立了Explicit链接，使得后续的概念wise self-supervised learning 可以帮助像素级别的归类。通过对多个数据集的广泛实验和分析，我们证明 CAUSE 的效果，并实现了无监督 semantic segmentation 的州际性能。
</details></li>
</ul>
<hr>
<h2 id="Point-Cloud-Denoising-and-Outlier-Detection-with-Local-Geometric-Structure-by-Dynamic-Graph-CNN"><a href="#Point-Cloud-Denoising-and-Outlier-Detection-with-Local-Geometric-Structure-by-Dynamic-Graph-CNN" class="headerlink" title="Point Cloud Denoising and Outlier Detection with Local Geometric Structure by Dynamic Graph CNN"></a>Point Cloud Denoising and Outlier Detection with Local Geometric Structure by Dynamic Graph CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07376">http://arxiv.org/abs/2310.07376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kosuke Nakayama, Hiroto Fukuta, Hiroshi Watanabe</li>
<li>for: 点云数据清洗和异常检测</li>
<li>methods: 应用两种基于动态图 convolutional layer的方法</li>
<li>results: 提出的方法在AUPR和Chamfer Distance上表现出色，比传统方法更高的异常检测精度和清洗精度<details>
<summary>Abstract</summary>
The digitalization of society is rapidly developing toward the realization of the digital twin and metaverse. In particular, point clouds are attracting attention as a media format for 3D space. Point cloud data is contaminated with noise and outliers due to measurement errors. Therefore, denoising and outlier detection are necessary for point cloud processing. Among them, PointCleanNet is an effective method for point cloud denoising and outlier detection. However, it does not consider the local geometric structure of the patch. We solve this problem by applying two types of graph convolutional layer designed based on the Dynamic Graph CNN. Experimental results show that the proposed methods outperform the conventional method in AUPR, which indicates outlier detection accuracy, and Chamfer Distance, which indicates denoising accuracy.
</details>
<details>
<summary>摘要</summary>
社会的数字化发展 rapidly towards the realization of digital twin和metaverse。特别是3D空间中的点云数据受到关注。由于测量错误导致的噪声和异常值，因此需要对点云数据进行噪声除除和异常值检测。其中，PointCleanNet是一种有效的点云噪声除除和异常值检测方法。但它不考虑点云的本地 геометрическую结构。我们解决这个问题，通过基于动态图 convolutional layer的两种类型应用。实验结果显示，我们提出的方法在AUPR和Chamfer Distance中表现出色，即异常检测精度和噪声除除精度。Here's the breakdown of the translation:* 社会的数字化发展 (society's digital development) -> 社会的数字化发展 (society's digital development)* rapidly towards the realization of digital twin和metaverse (rapidly towards the realization of digital twin and metaverse) ->  rapid towards the realization of digital twin和metaverse (rapid towards the realization of digital twin and metaverse)* 特别是3D空间中的点云数据 (point cloud data in 3D space) -> 特别是3D空间中的点云数据 (point cloud data in 3D space)* 受到关注 (attracting attention) -> 受到关注 (attracting attention)* 由于测量错误导致的噪声和异常值 (due to measurement errors and outliers) -> 由于测量错误导致的噪声和异常值 (due to measurement errors and outliers)* 因此需要对点云数据进行噪声除除和异常值检测 (therefore, need to denoise and detect outliers for point cloud data) -> 因此需要对点云数据进行噪声除除和异常值检测 (therefore, need to denoise and detect outliers for point cloud data)* 其中，PointCleanNet是一种有效的点云噪声除除和异常值检测方法 (PointCleanNet is an effective method for point cloud denoising and outlier detection) -> 其中，PointCleanNet是一种有效的点云噪声除除和异常值检测方法 (PointCleanNet is an effective method for point cloud denoising and outlier detection)* 但它不考虑点云的本地 геометрическую结构 (but it does not consider the local geometric structure of the point cloud) -> 但它不考虑点云的本地 геометрическую结构 (but it does not consider the local geometric structure of the point cloud)* 我们解决这个问题，通过基于动态图 convolutional layer的两种类型应用 (we solve this problem by applying two types of graph convolutional layers based on dynamic graphs) -> 我们解决这个问题，通过基于动态图 convolutional layer的两种类型应用 (we solve this problem by applying two types of graph convolutional layers based on dynamic graphs)* 实验结果显示，我们提出的方法在AUPR和Chamfer Distance中表现出色 (experimental results show that our method outperforms the conventional method in AUPR and Chamfer Distance) -> 实验结果显示，我们提出的方法在AUPR和Chamfer Distance中表现出色 (experimental results show that our method outperforms the conventional method in AUPR and Chamfer Distance)
</details></li>
</ul>
<hr>
<h2 id="Give-and-Take-Federated-Transfer-Learning-for-Industrial-IoT-Network-Intrusion-Detection"><a href="#Give-and-Take-Federated-Transfer-Learning-for-Industrial-IoT-Network-Intrusion-Detection" class="headerlink" title="Give and Take: Federated Transfer Learning for Industrial IoT Network Intrusion Detection"></a>Give and Take: Federated Transfer Learning for Industrial IoT Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07354">http://arxiv.org/abs/2310.07354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lochana Telugu Rajesh, Tapadhir Das, Raj Mani Shukla, Shamik Sengupta</li>
<li>for: 这篇论文旨在提出一种联边学习（Federated Transfer Learning，FTL）方法，用于防护互联网络（IIoT）中的网络入侵攻击。</li>
<li>methods: 本论文提出了一个搭配式神经网络（Combinational Neural Network，CNN），用于实现FTL的中心部分。在这篇论文中，我们将IIoT数据分成客户端和服务器端两部分，然后使用客户端模型的weight更新服务器模型。</li>
<li>results: 本论文的实验结果显示，FTL设置在IIoT客户端和服务器之间的 Iterations 具有高性能，并且比现有的机器学习算法在网络入侵攻击预测方面表现更好。<details>
<summary>Abstract</summary>
The rapid growth in Internet of Things (IoT) technology has become an integral part of today's industries forming the Industrial IoT (IIoT) initiative, where industries are leveraging IoT to improve communication and connectivity via emerging solutions like data analytics and cloud computing. Unfortunately, the rapid use of IoT has made it an attractive target for cybercriminals. Therefore, protecting these systems is of utmost importance. In this paper, we propose a federated transfer learning (FTL) approach to perform IIoT network intrusion detection. As part of the research, we also propose a combinational neural network as the centerpiece for performing FTL. The proposed technique splits IoT data between the client and server devices to generate corresponding models, and the weights of the client models are combined to update the server model. Results showcase high performance for the FTL setup between iterations on both the IIoT clients and the server. Additionally, the proposed FTL setup achieves better overall performance than contemporary machine learning algorithms at performing network intrusion detection.
</details>
<details>
<summary>摘要</summary>
“现代互联网络设备（IoT）技术的快速发展已成为今天的行业核心，组成了企业级互联网络（IIoT）计划，where industries are leveraging IoT to improve communication and connectivity via emerging solutions like data analytics and cloud computing. Unfortunately, the rapid use of IoT has made it an attractive target for cybercriminals. Therefore, protecting these systems is of utmost importance. In this paper, we propose a federated transfer learning (FTL) approach to perform IIoT network intrusion detection. As part of the research, we also propose a combinational neural network as the centerpiece for performing FTL. The proposed technique splits IoT data between the client and server devices to generate corresponding models, and the weights of the client models are combined to update the server model. Results showcase high performance for the FTL setup between iterations on both the IIoT clients and the server. Additionally, the proposed FTL setup achieves better overall performance than contemporary machine learning algorithms at performing network intrusion detection.”Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese writing systems.
</details></li>
</ul>
<hr>
<h2 id="Semantic-Association-Rule-Learning-from-Time-Series-Data-and-Knowledge-Graphs"><a href="#Semantic-Association-Rule-Learning-from-Time-Series-Data-and-Knowledge-Graphs" class="headerlink" title="Semantic Association Rule Learning from Time Series Data and Knowledge Graphs"></a>Semantic Association Rule Learning from Time Series Data and Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07348">http://arxiv.org/abs/2310.07348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erkan Karabulut, Victoria Degeler, Paul Groth</li>
<li>for: 这篇论文的目的是为了提出一个基于知识 graphs 和时间序列数据的 semantic association rule 学习管道，以及一个新的 semantic association rule 标准。</li>
<li>methods: 这篇论文使用了知识 graphs 和时间序列数据来学习 semantic association rules，并提出了一种新的 semantic association rule 标准。</li>
<li>results: 实验结果表明，提出的方法可以学习出具有 semantic information 的大量 association rules，这些规则更加普适。<details>
<summary>Abstract</summary>
Digital Twins (DT) are a promising concept in cyber-physical systems research due to their advanced features including monitoring and automated reasoning. Semantic technologies such as Knowledge Graphs (KG) are recently being utilized in DTs especially for information modelling. Building on this move, this paper proposes a pipeline for semantic association rule learning in DTs using KGs and time series data. In addition to this initial pipeline, we also propose new semantic association rule criterion. The approach is evaluated on an industrial water network scenario. Initial evaluation shows that the proposed approach is able to learn a high number of association rules with semantic information which are more generalizable. The paper aims to set a foundation for further work on using semantic association rule learning especially in the context of industrial applications.
</details>
<details>
<summary>摘要</summary>
数字双胞（DT）是现代物联网系统研究中的一个抢险概念，它具有监测和自动推理等高级功能。 semantic技术如知识图（KG）在DT中特别地应用于信息模型化。基于这种趋势，本文提出了基于知识图和时间序列数据的semantic association rule学习管道。此外，我们还提出了一新的semantic association rule标准。我们通过对工业水网enario进行初步评估，发现提出的方法能够学习大量具有Semantic信息的相关规则，这些规则更加通用。本文的目的是为将来的Semantic association rule学习研究提供基础，特别是在工业应用中。
</details></li>
</ul>
<hr>
<h2 id="Fast-ELECTRA-for-Efficient-Pre-training"><a href="#Fast-ELECTRA-for-Efficient-Pre-training" class="headerlink" title="Fast-ELECTRA for Efficient Pre-training"></a>Fast-ELECTRA for Efficient Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07347">http://arxiv.org/abs/2310.07347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengyu Dong, Liyuan Liu, Hao Cheng, Jingbo Shang, Jianfeng Gao, Xiaodong Liu</li>
<li>for: 本文针对ELECTRA预训方法进行改进，以提高效率和稳定性。</li>
<li>methods: 本文使用现有语言模型作为助动模型，并透过温度调整的渐减 schedule 建立学习课程，以帮助主模型提高表现。</li>
<li>results: 本研究显示，使用现有语言模型作为助动模型可以帮助提高ELECTRA的效率和稳定性，并且与现有state-of-the-art ELECTRA-style预训方法相当。<details>
<summary>Abstract</summary>
ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity to hyper-parameters and enhances the pre-training stability.
</details>
<details>
<summary>摘要</summary>
ELECTRA 使用替换token的模型进行预训练，以提高效率。然而，ELECTRA 的潜力受到助手模型的训练成本的限制。具体来说，这个模型只是用于帮助主模型训练，并且在训练结束后被抛弃。这会导致大量的训练成本浪费。为了解决这个问题，我们提出了 Fast-ELECTRA，它利用现有的语言模型作为助手模型。我们使用温度层 scaling 将主模型的输出分布平滑化，以建立一个学习课程 для主模型。我们的方法可以与现有的状态态-of-the-art ELECTRA 预训练方法相比，同时减少计算和内存成本，以及敏感度到hyperparameter和预训练稳定性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Social-Motion-Latent-Space-and-Human-Awareness-for-Effective-Robot-Navigation-in-Crowded-Environments"><a href="#Exploring-Social-Motion-Latent-Space-and-Human-Awareness-for-Effective-Robot-Navigation-in-Crowded-Environments" class="headerlink" title="Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation in Crowded Environments"></a>Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation in Crowded Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07335">http://arxiv.org/abs/2310.07335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junaid Ahmed Ansari, Satyajit Tourani, Gourav Kumar, Brojeshwar Bhowmick</li>
<li>for: 本研究提出了一种基于社交动作潜在空间学习的社交机器人导航方法，以提高社交导航指标（如成功率、导航时间、轨迹长度），并生成更平滑、更预测性的轨迹。</li>
<li>methods: 该方法利用社交动作潜在空间来生成机器人控制，并通过对比基线模型而证明其超越性。</li>
<li>results: 研究表明，包含人类意识的社交机器人导航框架可以生成更短、更平滑的轨迹，是因为人类可以正面与机器人互动。<details>
<summary>Abstract</summary>
This work proposes a novel approach to social robot navigation by learning to generate robot controls from a social motion latent space. By leveraging this social motion latent space, the proposed method achieves significant improvements in social navigation metrics such as success rate, navigation time, and trajectory length while producing smoother (less jerk and angular deviations) and more anticipatory trajectories. The superiority of the proposed method is demonstrated through comparison with baseline models in various scenarios. Additionally, the concept of humans' awareness towards the robot is introduced into the social robot navigation framework, showing that incorporating human awareness leads to shorter and smoother trajectories owing to humans' ability to positively interact with the robot.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种新的社交机器人导航方法，通过学习生成机器人控制从社交动作潜在空间中生成机器人控制。通过利用这个社交动作潜在空间，提议方法可以获得 significatively 提高社交导航指标，如成功率、导航时间和轨迹长度，同时生成更平滑（具有 menos jerk 和 angular deviation）的轨迹。在不同的场景下，相比基eline 模型，提议方法的优势得到了证明。此外，在社交机器人导航框架中引入人类意识的概念，表明在机器人与人类之间的交互中，机器人可以更短更平滑的轨迹。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Instruction-tuning-Large-Language-Models-in-Chinese"><a href="#An-Empirical-Study-of-Instruction-tuning-Large-Language-Models-in-Chinese" class="headerlink" title="An Empirical Study of Instruction-tuning Large Language Models in Chinese"></a>An Empirical Study of Instruction-tuning Large Language Models in Chinese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07328">http://arxiv.org/abs/2310.07328</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phoebussi/alpaca-cot">https://github.com/phoebussi/alpaca-cot</a></li>
<li>paper_authors: Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, Weiping Wang</li>
<li>for: 这 paper 旨在对中文大语言模型（LLMs）进行深入的实验研究，以便更好地适应中文指令。</li>
<li>methods: 本 paper 使用了三个关键元素进行实验研究：LLM bases、参数效率方法和指令数据类型。此外，它还研究了其他因素，如 chain-of-thought 数据和人类价值对Alignment。</li>
<li>results: 本 paper 的实验结果表明，通过对 LLM bases、参数效率方法和指令数据类型进行调整，可以实现更好地适应中文指令的中文 LLMS。 Code 和数据可以在 <a target="_blank" rel="noopener" href="https://github.com/PhoebusSi/Alpaca-CoT">https://github.com/PhoebusSi/Alpaca-CoT</a> 上获取。<details>
<summary>Abstract</summary>
The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community's interest in instruction-tuning, which is deemed to accelerate ChatGPT's replication process. However, research on instruction-tuning LLMs in Chinese, the world's most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can make a modest contribution to the open Chinese version of ChatGPT. This paper will release a powerful Chinese LLMs that is comparable to ChatGLM. The code and data are available at https://github.com/PhoebusSi/Alpaca-CoT.
</details>
<details>
<summary>摘要</summary>
成功的ChatGPT证明大语言模型（LLM）在人工通用智能（AGI）中的潜力。随后，LLM的发布激发了开源社区对于教程调整的兴趣，这被认为可以加速ChatGPT的复制过程。然而，对于中文的LLM教程调整研究仍处于初期阶段。因此，本文进行了深入的实验研究，以提供有价值的发现，可以帮助更好地调整LLM，以便更好地回应中文指令。具体来说，我们系统地探讨LLM基础、参数效率方法和指令数据类型等三个关键元素的影响。此外，我们还进行了实验研究其他因素，如链条数据和人类价值对alignment。我们希望这个实验研究可以为开放的中文版ChatGPT提供一个有价值的参考，并释放一个与ChatGPT相当的中文LLM。代码和数据可以在https://github.com/PhoebusSi/Alpaca-CoT中找到。
</details></li>
</ul>
<hr>
<h2 id="An-Adversarial-Example-for-Direct-Logit-Attribution-Memory-Management-in-gelu-4l"><a href="#An-Adversarial-Example-for-Direct-Logit-Attribution-Memory-Management-in-gelu-4l" class="headerlink" title="An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l"></a>An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07325">http://arxiv.org/abs/2310.07325</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Dao, Yeu-Tong Lau, Can Rager, Jett Janiak</li>
<li>for: 该研究探讨了一种4层转换器的内存管理问题，并提供了具体的证据。</li>
<li>methods: 研究使用了直观逻辑权重分析技术来分析模型的输出。</li>
<li>results: 研究发现，直观逻辑权重分析技术可能提供不准确的结果，因为它不考虑模型中的干净行为。<details>
<summary>Abstract</summary>
We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
</details>
<details>
<summary>摘要</summary>
我们提供具体证据对 transformer Memory Management 的研究。具体来说，我们发现了“清理”行为，即模型组件在前进通道中一致地 removes 前一个组件的输出。我们的发现表明，使用 Direct Logit Attribution  interpretability 技术可能会得到错误的结果。我们提供了明确的例子，证明这种技术无法考虑清理行为。
</details></li>
</ul>
<hr>
<h2 id="On-the-Impact-of-Cross-Domain-Data-on-German-Language-Models"><a href="#On-the-Impact-of-Cross-Domain-Data-on-German-Language-Models" class="headerlink" title="On the Impact of Cross-Domain Data on German Language Models"></a>On the Impact of Cross-Domain Data on German Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07321">http://arxiv.org/abs/2310.07321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Dada, Aokun Chen, Cheng Peng, Kaleb E Smith, Ahmad Idrissi-Yaghir, Constantin Marc Seibold, Jianning Li, Lars Heiliger, Xi Yang, Christoph M. Friedrich, Daniel Truhn, Jan Egger, Jiang Bian, Jens Kleesiek, Yonghui Wu</li>
<li>for: 本研究目的是探讨数据多样性对大语言模型的影响，以及高质量数据是否能够超越多样性的效果。</li>
<li>methods: 研究者使用了五个不同领域的文本数据集，并对这些数据集进行了归一化和分词处理。然后，他们在这些数据集上训练了一系列大语言模型，并对这些模型进行了多个下游任务的benchmark测试。</li>
<li>results: 研究者发现，训练在多样性数据集上的模型可以与高质量数据集上的模型相比，在多个下游任务上表现出较好的性能，并且可以提高过去最佳性能的4.45%。<details>
<summary>Abstract</summary>
Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
</details>
<details>
<summary>摘要</summary>
传统上，大型语言模型通常是通过全网爬虫或域专业数据进行训练。然而，最近的生成大型语言模型的成功，抛光了跨领域数据的优势。为了评估数据多样性的重要性，我们提供了一个德国 dataset，包含五个领域的文本，以及另一个具有高质量数据的 dataset。通过在两个 dataset 上训练一系列模型，从 122M 到 750M 参数的模型，我们进行了多个下游任务的完整性测试。我们的发现表明，跨领域 dataset 上训练的模型比单一质量数据alone 训练的模型提高了 $4.45\%$，超过了之前的状态调。模型可以在 https://huggingface.co/ikim-uk-essen 上获取。
</details></li>
</ul>
<hr>
<h2 id="WiGenAI-The-Symphony-of-Wireless-and-Generative-AI-via-Diffusion-Models"><a href="#WiGenAI-The-Symphony-of-Wireless-and-Generative-AI-via-Diffusion-Models" class="headerlink" title="WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models"></a>WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07312">http://arxiv.org/abs/2310.07312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Letafati, Samad Ali, Matti Latva-aho</li>
<li>for: 这 paper 旨在探讨生成AI在无线通信系统中的应用，以铺垫未来的研究。</li>
<li>methods: 这 paper 使用了Diffusion-based生成模型，这种新的状态态模型在生成模型中具有最新的状态。</li>
<li>results: 这 paper 通过两个案例研究，提出了一种使用Diffusion模型提高无线通信系统的Bit Error Rate的方法，并且在不理想的接收器情况下实现了30%的提高。<details>
<summary>Abstract</summary>
Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose denoising diffusion probabilistic models (DDPM) for a wireless communication scheme with non-ideal transceivers, where 30% improvement is achieved in terms of bit error rate. As the second application, DDPMs are employed at the transmitter to shape the constellation symbols, highlighting a robust out-of-distribution performance. Finally, future directions and open issues for the development of generative AI-based wireless systems are discussed to promote future research endeavors towards wireless generative AI (WiGenAI).
</details>
<details>
<summary>摘要</summary>
创新基础模型，如GPT-3和稳定扩散模型，在人工智能（AI）领域引入了一个新的思维方式，推动了基于生成AI的系统的发展。在数据通信和网络方面，AI/ML算法将在未来的无线通信系统中普遍应用，需要新的AINative解决方案来应对新兴的通信场景。本文介绍了生成AI在无线通信系统中的应用，为这一领域的研究奠基。 diffusion基础生成模型被介绍为新的生成模型的状态艺术，其应用在无线通信系统中被讨论。两个案例研究如何使用扩散模型来开发鲁棒的AINative通信系统。首先，我们提出了噪声扩散概率模型（DDPM），用于一种无线通信方案中的非理想接收器，其中提高了比特错误率30%。其次，DDPM被用于发送器来修饰 konstellation 符号，并证明了在异常输出情况下的稳定性。最后，我们讨论了未来发展和开放问题，以促进未来的无线生成AI（WiGenAI）研究。
</details></li>
</ul>
<hr>
<h2 id="RobustGEC-Robust-Grammatical-Error-Correction-Against-Subtle-Context-Perturbation"><a href="#RobustGEC-Robust-Grammatical-Error-Correction-Against-Subtle-Context-Perturbation" class="headerlink" title="RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation"></a>RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07299">http://arxiv.org/abs/2310.07299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hillzhang1999/robustgec">https://github.com/hillzhang1999/robustgec</a></li>
<li>paper_authors: Yue Zhang, Leyang Cui, Enbo Zhao, Wei Bi, Shuming Shi<br>for: 这种论文的目的是为了评估语言修正系统的上下文稳定性。methods: 该论文使用了5000个语言修正例子，每个例子包括一个原始错误语句和五个人工标注的变体。results: 研究发现当前的语言修正系统仍然无法快速响应上下文变化，而提议的简单 yet effective 方法可以有效解决这个问题。<details>
<summary>Abstract</summary>
Grammatical Error Correction (GEC) systems play a vital role in assisting people with their daily writing tasks. However, users may sometimes come across a GEC system that initially performs well but fails to correct errors when the inputs are slightly modified. To ensure an ideal user experience, a reliable GEC system should have the ability to provide consistent and accurate suggestions when encountering irrelevant context perturbations, which we refer to as context robustness. In this paper, we introduce RobustGEC, a benchmark designed to evaluate the context robustness of GEC systems. RobustGEC comprises 5,000 GEC cases, each with one original error-correct sentence pair and five variants carefully devised by human annotators. Utilizing RobustGEC, we reveal that state-of-the-art GEC systems still lack sufficient robustness against context perturbations. In addition, we propose a simple yet effective method for remitting this issue.
</details>
<details>
<summary>摘要</summary>
grammatical error correction (GEC) 系统在日常写作任务中扮演着重要的角色。然而，用户可能会在使用 GEC 系统时发现，当输入有所修改时，GEC 系统可能会在初始化时表现良好，但在修改后仍然无法正确地更正错误。为确保理想的用户体验，一个可靠的 GEC 系统应该有能力在不相关的上下文干扰下提供一致和准确的建议。在这篇论文中，我们介绍了 RobustGEC，一个用于评估 GEC 系统的上下文稳定性的库。RobustGEC 包含 5,000 个 GEC 案例，每个案例包含一对原始错误 corrected 句子 pair 和五个由人类标注员所设计的修改案例。利用 RobustGEC，我们发现现有的 GEC 系统仍然缺乏对上下文干扰的抗衡能力。此外，我们也提出了一个简单 yet 有效的方法来解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Memorization-Violating-Privacy-Via-Inference-with-Large-Language-Models"><a href="#Beyond-Memorization-Violating-Privacy-Via-Inference-with-Large-Language-Models" class="headerlink" title="Beyond Memorization: Violating Privacy Via Inference with Large Language Models"></a>Beyond Memorization: Violating Privacy Via Inference with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07298">http://arxiv.org/abs/2310.07298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Staab, Mark Vero, Mislav Balunović, Martin Vechev</li>
<li>for: 这个研究的目的是研究现有大语言模型（LLM）是否可以通过文本内容来推断个人特征。</li>
<li>methods: 研究使用了现有的LLM，并构建了一个基于Reddit Profilese的数据集，以测试LLM的推断能力。</li>
<li>results: 研究发现，现有的LLM可以准确地推断个人特征，例如地点、收入和性别，并且可以在人工智能技术的一个分之一的成本和时间下达到人类水平。此外，研究还探讨了隐私泄露的风险，并发现现有的防御措施无法保护用户隐私。<details>
<summary>Abstract</summary>
Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection.
</details>
<details>
<summary>摘要</summary>
现有大量语言模型（LLM）隐私研究主要关注模型在训练数据中储存的问题。同时，模型的推理能力有了很大的提高。这提出了关键问题：现有LLM是否可以通过文本来推断个人特征？在这个工作中，我们提供了首次LLM在文本中推断个人特征的全面研究。我们构建了基于真实的Reddit Profilestext dataset，并显示了当前LLM可以通过文本推断各种个人特征（如地点、收入、性别），达到了人类的$85\%$ top-1和$95.8\%$ top-3准确率，而且只需要人类的$100\times$ 时间和$240\times$ 成本。随着人们在所有方面的生活中与LLM驱动的chatbot进行交互，我们也探讨了隐私泄露的emerging threat，即通过看起来普通的问题来提取个人信息。最后，我们发现现有的mitigationstrategies，如文本匿名和模型对齐，目前无法保护用户隐私。我们的发现表明当前LLM可以在前所未有的规模上进行个人数据推断。在没有工作的防御措施时，我们呼吁更广泛的隐私问题讨论，努力为更多的隐私保护。
</details></li>
</ul>
<hr>
<h2 id="Automated-Verification-of-Equivalence-Properties-in-Advanced-Logic-Programs-–-Bachelor-Thesis"><a href="#Automated-Verification-of-Equivalence-Properties-in-Advanced-Logic-Programs-–-Bachelor-Thesis" class="headerlink" title="Automated Verification of Equivalence Properties in Advanced Logic Programs – Bachelor Thesis"></a>Automated Verification of Equivalence Properties in Advanced Logic Programs – Bachelor Thesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19806">http://arxiv.org/abs/2310.19806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Heuer</li>
<li>for: 这个论文的目的是提供一种自动化正式验证工具，用于替换原始子程序。</li>
<li>methods: 这个论文使用了 Anthem 翻译工具，以及一个自动证明工具 для классической逻辑，来验证两个程序的强等价性。</li>
<li>results: 该论文扩展了 Anthem 工具，使其能够验证包含卷积、否定和简单选择规则的逻辑程序的强等价性。新版本的 Anthem 还能够翻译这些程序到 классической逻辑中。<details>
<summary>Abstract</summary>
With the increase in industrial applications using Answer Set Programming, the need for formal verification tools, particularly for critical applications, has also increased. During the program optimisation process, it would be desirable to have a tool which can automatically verify whether an optimised subprogram can replace the original subprogram. Formally this corresponds to the problem of verifying the strong equivalence of two programs. In order to do so, the translation tool anthem was developed. It can be used in conjunction with an automated theorem prover for classical logic to verify that two programs are strongly equivalent. With the current version of anthem, only the strong equivalence of positive programs with a restricted input language can be verified. This is a result of the translation $\tau^*$ implemented in anthem that produces formulas in the logic of here-and-there, which coincides with classical logic only for positive programs. This thesis extends anthem in order to overcome these limitations. First, the transformation $\sigma^*$ is presented, which transforms formulas from the logic of here-and-there to classical logic. A theorem formalises how $\sigma^*$ can be used to express equivalence in the logic of here-and-there in classical logic. Second, the translation $\tau^*$ is extended to programs containing pools. Another theorem shows how $\sigma^*$ can be combined with $\tau^*$ to express the strong equivalence of two programs in classical logic. With $\sigma^*$ and the extended $\tau^*$, it is possible to express the strong equivalence of logic programs containing negation, simple choices, and pools. Both the extended $\tau^*$ and $\sigma^*$ are implemented in a new version of anthem. Several examples of logic programs containing pools, negation, and simple choice rules, which the new version of anthem can translate to classical logic, are presented. Some a...
</details>
<details>
<summary>摘要</summary>
随着应用 Answer Set Programming 的增加，对于重要应用程序的正式验证工具的需求也增加了。在程序优化过程中，您希望有一个工具可以自动验证优化后的子程序是否可以替换原始子程序。正式来说，这对应于两个程序的强等价性问题的验证。为此，anthem 工具被开发出来。它可以与自动证明工具结合，以验证两个程序的强等价性。anthem 的当前版本只能验证正确的程序的强等价性，这是因为 anthem 中的翻译 $\tau^*$ 仅能处理正确的程序。这个论文扩展 anthem，以解决这些限制。首先，我们提出了一种变换 $\sigma^*$，可以将 formulas 从 here-and-there 逻辑转换为类型逻辑。一个定理证明了 $\sigma^*$ 如何用于表达 here-and-there 逻辑中的等价性。其次，我们扩展了 $\tau^*$，以便处理包含池的程序。另一个定理证明了如何将 $\sigma^*$ 与 $\tau^*$ 结合使用，以表达两个程序的强等价性。通过 $\sigma^*$ 和扩展后的 $\tau^*$，我们可以表达包含否定、简单选择规则的逻辑程序的强等价性。这些扩展后的 $\sigma^*$ 和 $\tau^*$ 都被实现在新版本的 anthem 中。我们还提供了一些逻辑程序示例，包括包含池、否定和简单选择规则的程序，这些程序可以通过新版本的 anthem 转换为类型逻辑。
</details></li>
</ul>
<hr>
<h2 id="An-Analysis-on-Large-Language-Models-in-Healthcare-A-Case-Study-of-BioBERT"><a href="#An-Analysis-on-Large-Language-Models-in-Healthcare-A-Case-Study-of-BioBERT" class="headerlink" title="An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT"></a>An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07282">http://arxiv.org/abs/2310.07282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shyni Sharaf, V. S. Anoop</li>
<li>for: This paper explores the application of large language models, specifically BioBERT, in healthcare and its potential benefits for clinical decision support and information retrieval.</li>
<li>methods: The paper proposes a systematic methodology for fine-tuning BioBERT to meet the unique needs of the healthcare domain, including data gathering, annotation, and specialized preprocessing techniques.</li>
<li>results: The paper evaluates the performance of BioBERT in various healthcare-related tasks, such as medical entity recognition and question-answering, and explores techniques to improve the model’s interpretability. It also acknowledges the ethical considerations and challenges of integrating BioBERT into healthcare contexts.<details>
<summary>Abstract</summary>
This paper conducts a comprehensive investigation into applying large language models, particularly on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing (NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face. Following that, this research explores the path that led to the incorporation of BioBERT into healthcare applications, highlighting its suitability for addressing the specific requirements of tasks related to biomedical text mining. The analysis outlines a systematic methodology for fine-tuning BioBERT to meet the unique needs of the healthcare domain. This approach includes various components, including the gathering of data from a wide range of healthcare sources, data annotation for tasks like identifying medical entities and categorizing them, and the application of specialized preprocessing techniques tailored to handle the complexities found in biomedical texts. Additionally, the paper covers aspects related to model evaluation, with a focus on healthcare benchmarks and functions like processing of natural language in biomedical, question-answering, clinical document classification, and medical entity recognition. It explores techniques to improve the model's interpretability and validates its performance compared to existing healthcare-focused language models. The paper thoroughly examines ethical considerations, particularly patient privacy and data security. It highlights the benefits of incorporating BioBERT into healthcare contexts, including enhanced clinical decision support and more efficient information retrieval. Nevertheless, it acknowledges the impediments and complexities of this integration, encompassing concerns regarding data privacy, transparency, resource-intensive requirements, and the necessity for model customization to align with diverse healthcare domains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BioT5-Enriching-Cross-modal-Integration-in-Biology-with-Chemical-Knowledge-and-Natural-Language-Associations"><a href="#BioT5-Enriching-Cross-modal-Integration-in-Biology-with-Chemical-Knowledge-and-Natural-Language-Associations" class="headerlink" title="BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations"></a>BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07276">http://arxiv.org/abs/2310.07276</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/QizhiPei/BioT5">https://github.com/QizhiPei/BioT5</a></li>
<li>paper_authors: Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan</li>
<li>For: The paper aims to enhance drug discovery by integrating molecules, proteins, and natural language.* Methods: The proposed method, BioT5, uses SELFIES to generate robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. It also distinguishes between structured and unstructured knowledge.* Results: BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability of capturing underlying relations and properties of bio-entities.<details>
<summary>Abstract</summary>
Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability of capturing underlying relations and properties of bio-entities. Our code is available at $\href{https://github.com/QizhiPei/BioT5}{Github}$.
</details>
<details>
<summary>摘要</summary>
最近的生物研究进步利用分子、蛋白质和自然语言的集成来提高药物发现。然而，当前的模型具有许多限制，如生成无效的分子SMILES、Contextual information的过度利用和结构化知识和未结构化知识的平等对待。为解决这些问题，我们提出了 $\mathbf{BioT5}$，一个全面预训练框架，用于增强生物学中的分子知识和自然语言关系。 $\mathbf{BioT5}$ 使用 SELFIES 确保 $100%$ 可靠的分子表示，从生物文献中的生物实体周围的上下文中提取知识，并在结构化和未结构化知识之间进行区分。这使得 $\mathbf{BioT5}$ 在许多任务上表现出色，表明它能够捕捉生物实体下面的关系和性质。我们的代码可以在 $\href{https://github.com/QizhiPei/BioT5}{Github}$ 上找到。
</details></li>
</ul>
<hr>
<h2 id="CoPAL-Corrective-Planning-of-Robot-Actions-with-Large-Language-Models"><a href="#CoPAL-Corrective-Planning-of-Robot-Actions-with-Large-Language-Models" class="headerlink" title="CoPAL: Corrective Planning of Robot Actions with Large Language Models"></a>CoPAL: Corrective Planning of Robot Actions with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07263">http://arxiv.org/abs/2310.07263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Joublin, Antonello Ceravola, Pavel Smirnov, Felix Ocker, Joerg Deigmoeller, Anna Belardinelli, Chao Wang, Stephan Hasler, Daniel Tanneberg, Michael Gienger</li>
<li>for: 这种研究旨在提高机器人完全自主系统的可行性，以替代人类执行任务。</li>
<li>methods: 这种研究使用了大语言模型应用于机器人任务和动作规划，提出了一种新的重启策略来处理物理、逻辑和 semantics 错误。</li>
<li>results: 经验证明，提出的反馈体系可以提高执行可能性、正确性和时间复杂度。<details>
<summary>Abstract</summary>
In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation.
</details>
<details>
<summary>摘要</summary>
“在实现完全自主机器人系统的挑战中，开放世界环境的复杂性对任务和动作观念规划产生了很大的挑战。这项研究对于大型自然语言模型（LLM）应用在机器人任务和动作规划方面做出了贡献。我们提出了一个系统架构，它在多个认知水平之间进行了联系，包括理解、规划和动作生成。这个架构的核心是一种新的重新规划策略，可以处理物理基础、逻辑和semantic错误在生成的计划中。我们透过实际评估在模拟和两个实际世界情况下（即积木世界和制作啤酒和制作izza），证明了我们提出的反馈架构的有效性，特别是对于执行可能性、正确性和时间复杂度的影响。”
</details></li>
</ul>
<hr>
<h2 id="Uncovering-Hidden-Connections-Iterative-Tracking-and-Reasoning-for-Video-grounded-Dialog"><a href="#Uncovering-Hidden-Connections-Iterative-Tracking-and-Reasoning-for-Video-grounded-Dialog" class="headerlink" title="Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog"></a>Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07259">http://arxiv.org/abs/2310.07259</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hyu-zhang/itr">https://github.com/hyu-zhang/itr</a></li>
<li>paper_authors: Haoyu Zhang, Meng Liu, Yaowei Wang, Da Cao, Weili Guan, Liqiang Nie</li>
<li>for: 这篇论文的目的是提出一种新的视频对话方法，可以快速和准确地回答视频内容相关的问题。</li>
<li>methods: 这篇论文使用了一种迭代跟踪和理解策略，将文本编码器、视觉编码器和生成器相结合。文本编码器使用了一种路径跟踪和汇总机制，能够从对话历史中提取关键信息，解决问题。视觉编码器使用了一种迭代理解网络，精心挑选和强调视频中重要的视觉特征，提高视觉理解的深度。</li>
<li>results: 作者通过在两个知名的数据集上进行实验，证明了他们提出的方法的可靠性和适应性。<details>
<summary>Abstract</summary>
In contrast to conventional visual question answering, video-grounded dialog necessitates a profound understanding of both dialog history and video content for accurate response generation. Despite commendable strides made by existing methodologies, they often grapple with the challenges of incrementally understanding intricate dialog histories and assimilating video information. In response to this gap, we present an iterative tracking and reasoning strategy that amalgamates a textual encoder, a visual encoder, and a generator. At its core, our textual encoder is fortified with a path tracking and aggregation mechanism, adept at gleaning nuances from dialog history that are pivotal to deciphering the posed questions. Concurrently, our visual encoder harnesses an iterative reasoning network, meticulously crafted to distill and emphasize critical visual markers from videos, enhancing the depth of visual comprehension. Culminating this enriched information, we employ the pre-trained GPT-2 model as our response generator, stitching together coherent and contextually apt answers. Our empirical assessments, conducted on two renowned datasets, testify to the prowess and adaptability of our proposed design.
</details>
<details>
<summary>摘要</summary>
contrast to traditional visual question answering, video-grounded dialog requires a profound understanding of both dialog history and video content for accurate response generation. Despite notable advances made by existing methodologies, they often struggle with incrementally understanding complex dialog histories and integrating video information. In response to this gap, we propose an iterative tracking and reasoning strategy that combines a textual encoder, a visual encoder, and a generator. At its core, our textual encoder is reinforced with a path tracking and aggregation mechanism, skilled at extracting subtleties from dialog history that are crucial to deciphering the posed questions. Meanwhile, our visual encoder utilizes an iterative reasoning network, carefully crafted to distill and emphasize essential visual cues from videos, enhancing the depth of visual understanding. Combining this enriched information, we employ the pre-trained GPT-2 model as our response generator, seamlessly stitching together coherent and contextually appropriate answers. Our empirical evaluations, conducted on two well-known datasets, demonstrate the effectiveness and adaptability of our proposed design.
</details></li>
</ul>
<hr>
<h2 id="ADMEOOD-Out-of-Distribution-Benchmark-for-Drug-Property-Prediction"><a href="#ADMEOOD-Out-of-Distribution-Benchmark-for-Drug-Property-Prediction" class="headerlink" title="ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction"></a>ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07253">http://arxiv.org/abs/2310.07253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qweasdzxc-wsy/ADMEOOD">https://github.com/qweasdzxc-wsy/ADMEOOD</a></li>
<li>paper_authors: Shuoying Wei, Xinlong Wen, Lida Zhu, Songquan Li, Rongbo Zhu</li>
<li>For: This paper aims to address the out-of-distribution (OOD) problem in drug property prediction by proposing a novel benchmark dataset and evaluating the performance of different domain generalization models.* Methods: The proposed benchmark, called ADMEOOD, includes a systematic OOD dataset curator and benchmark specifically designed for drug property prediction. It includes two types of OOD data shifts: Noise Shift and Concept Conflict Drift (CCD).* Results: The experimental results demonstrate the effectiveness of the proposed partition method in ADMEOOD, showing a significant difference in performance between in-distribution and out-of-distribution data. Additionally, the paper shows that Empirical Risk Minimization (ERM) and other models exhibit distinct trends in performance across different domains and measurement types.<details>
<summary>Abstract</summary>
Obtaining accurate and valid information for drug molecules is a crucial and challenging task. However, chemical knowledge and information have been accumulated over the past 100 years from various regions, laboratories, and experimental purposes. Little has been explored in terms of the out-of-distribution (OOD) problem with noise and inconsistency, which may lead to weak robustness and unsatisfied performance. This study proposes a novel benchmark ADMEOOD, a systematic OOD dataset curator and benchmark specifically designed for drug property prediction. ADMEOOD obtained 27 ADME (Absorption, Distribution, Metabolism, Excretion) drug properties from Chembl and relevant literature. Additionally, it includes two kinds of OOD data shifts: Noise Shift and Concept Conflict Drift (CCD). Noise Shift responds to the noise level by categorizing the environment into different confidence levels. On the other hand, CCD describes the data which has inconsistent label among the original data. Finally, it tested on a variety of domain generalization models, and the experimental results demonstrate the effectiveness of the proposed partition method in ADMEOOD: ADMEOOD demonstrates a significant difference performance between in-distribution and out-of-distribution data. Moreover, ERM (Empirical Risk Minimization) and other models exhibit distinct trends in performance across different domains and measurement types.
</details>
<details>
<summary>摘要</summary>
得到正确和有效的药分子信息是一项关键和挑战性的任务。然而，化学知识和信息在过去100年间在不同的地方、实验室和实验目的下积累了大量的经验。虽然有很多研究对药物性能进行了预测，但对于异常情况（OOD）的问题还有很少的探索。本研究提出了一个新的标准测试集ADMEOOD，该集包含27个ADME（吸收、分布、代谢、排除）药物性能 Parameters from Chembl和相关文献。此外，它还包括两种类型的OOD数据推移：噪声推移和概念冲突推移（CCD）。噪声推移根据噪声水平进行分类环境，而CCD则描述了原始数据中存在冲突的标签。最后，它在多种领域通用化模型上进行了测试，实验结果表明了提案的分区方法在ADMEOOD中的效果：ADMEOOD在含有和不含的数据之间显示了明显的性能差异。此外，ERM（Empirical Risk Minimization）模型和其他模型在不同的领域和测量类型上表现出了不同的趋势。
</details></li>
</ul>
<hr>
<h2 id="Ethical-Reasoning-over-Moral-Alignment-A-Case-and-Framework-for-In-Context-Ethical-Policies-in-LLMs"><a href="#Ethical-Reasoning-over-Moral-Alignment-A-Case-and-Framework-for-In-Context-Ethical-Policies-in-LLMs" class="headerlink" title="Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs"></a>Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07251">http://arxiv.org/abs/2310.07251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhinav Rao, Aditi Khandelwal, Kumar Tanmay, Utkarsh Agarwal, Monojit Choudhury</li>
<li>for:  argue that LLMs should be infused with generic ethical reasoning capabilities to handle value pluralism at a global scale, rather than aligning them to specific ethical principles.</li>
<li>methods: develop a framework that integrates moral dilemmas with moral principles from different formalisms of normative ethics and at different levels of abstractions.</li>
<li>results: initial experiments with GPT-x models show that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies.<details>
<summary>Abstract</summary>
In this position paper, we argue that instead of morally aligning LLMs to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. When provided with an ethical policy, an LLM should be capable of making decisions that are ethically consistent to the policy. We develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and at different levels of abstractions. Initial experiments with GPT-x models shows that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies.
</details>
<details>
<summary>摘要</summary>
在这份Position paper中，我们认为，而不是将人工智能语言模型（LLMs） morally align到特定的道德原则上，我们应该通过嵌入基于道德理解的能力来让它们能够处理全球范围内的价值多元性。当提供了一个道德政策时，一个LLM应该能够作出道德一致的决策。我们开发了一个整合道德困境和不同形式的normative ethics的道德原则的框架。初步实验表明，使用GPT-x模型时，GPT-4是一个几乎完美的道德思考者，但模型仍然偏向西方和英语社会的道德价值观。
</details></li>
</ul>
<hr>
<h2 id="Surrogate-modeling-for-stochastic-crack-growth-processes-in-structural-health-monitoring-applications"><a href="#Surrogate-modeling-for-stochastic-crack-growth-processes-in-structural-health-monitoring-applications" class="headerlink" title="Surrogate modeling for stochastic crack growth processes in structural health monitoring applications"></a>Surrogate modeling for stochastic crack growth processes in structural health monitoring applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07241">http://arxiv.org/abs/2310.07241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas E. Silionis, Konstantinos N. Anyfantis</li>
<li>for: 这个论文的目的是用structural health monitoring（SHM）技术预测金属结构中裂缝增长的未来趋势，以便实现预测维护。</li>
<li>methods: 该论文使用了物理基础的裂缝增长模型，以及对这些模型的不确定性进行了 reprehension。具体来说，这个论文使用了 Gaussian Process（GP）回归模型，以生成不同类型的不确定性的先验分布。</li>
<li>results: 该论文通过在数值上实现了这种方法，并对两个基本的裂缝监测问题进行了评估，即裂缝长度监测（损害评估）和裂缝增长监测（损害预测）。<details>
<summary>Abstract</summary>
Fatigue crack growth is one of the most common types of deterioration in metal structures with significant implications on their reliability. Recent advances in Structural Health Monitoring (SHM) have motivated the use of structural response data to predict future crack growth under uncertainty, in order to enable a transition towards predictive maintenance. Accurately representing different sources of uncertainty in stochastic crack growth (SCG) processes is a non-trivial task. The present work builds on previous research on physics-based SCG modeling under both material and load-related uncertainty. The aim here is to construct computationally efficient, probabilistic surrogate models for SCG processes that successfully encode these different sources of uncertainty. An approach inspired by latent variable modeling is employed that utilizes Gaussian Process (GP) regression models to enable the surrogates to be used to generate prior distributions for different Bayesian SHM tasks as the application of interest. Implementation is carried out in a numerical setting and model performance is assessed for two fundamental crack SHM problems; namely crack length monitoring (damage quantification) and crack growth monitoring (damage prognosis).
</details>
<details>
<summary>摘要</summary>
轻度疲劳裂隙是金属结构衰弱的一种最常见的类型，它对结构可靠性有着重要的影响。现代结构健康监测（SHM）技术的发展，使得可以通过结构响应数据预测未来裂隙增长，以便实现预测维护。正确表达不同类型的不确定性在杂音裂隙（SCG）过程中是一项非常困难的任务。本工作基于之前的物理基础SCG模型下的不确定性研究，旨在构建高效、probabilistic替身模型，以成功地编码不同类型的不确定性。我们采用了含隐变量模型的方法，使用 Gaussian Process（GP）回归模型，以便替身模型可以用来生成不同bayesian SHM任务的先验分布。实施在数字化环境中，并对两个基本的裂隙 SHM问题进行了评估，即裂隙长度监测（损害评估）和裂隙增长监测（损害预测）。
</details></li>
</ul>
<hr>
<h2 id="Using-Learnable-Physics-for-Real-Time-Exercise-Form-Recommendations"><a href="#Using-Learnable-Physics-for-Real-Time-Exercise-Form-Recommendations" class="headerlink" title="Using Learnable Physics for Real-Time Exercise Form Recommendations"></a>Using Learnable Physics for Real-Time Exercise Form Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07221">http://arxiv.org/abs/2310.07221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Jaiswal, Gautam Chauhan, Nisheeth Srivastava</li>
<li>for: 这篇论文是为了提供一个用于训练和重abilitation的推荐系统，以实时评估和给出修正建议，以提高安全性和生产力。</li>
<li>methods: 这个推荐系统使用MediaPipe进行姿势识别，使用峰值振荡检测缩数量，并使用一个可学习的物理模拟器追踪每个运动动作的动作演进。</li>
<li>results: 这个系统在六种全身和上半身运动动作中进行了实时评估和修正建议，以提高自修练的可能性和降低运动伤害的风险。<details>
<summary>Abstract</summary>
Good posture and form are essential for safe and productive exercising. Even in gym settings, trainers may not be readily available for feedback. Rehabilitation therapies and fitness workouts can thus benefit from recommender systems that provide real-time evaluation. In this paper, we present an algorithmic pipeline that can diagnose problems in exercise techniques and offer corrective recommendations, with high sensitivity and specificity in real-time. We use MediaPipe for pose recognition, count repetitions using peak-prominence detection, and use a learnable physics simulator to track motion evolution for each exercise. A test video is diagnosed based on deviations from the prototypical learned motion using statistical learning. The system is evaluated on six full and upper body exercises. These real-time recommendations, counseled via low-cost equipment like smartphones, will allow exercisers to rectify potential mistakes making self-practice feasible while reducing the risk of workout injuries.
</details>
<details>
<summary>摘要</summary>
好的姿势和形态是健身安全和生产力的关键。即使在健身房 Setting中，教练可能不总是可以提供反馈。rehabilitation therapy和健身训练可以从推荐系统中受益，该系统可以提供实时的评估。在这篇论文中，我们提出了一个算法管道，可以诊断运动技巧中的问题并提供修正建议，具有高度敏感和特异性。我们使用MediaPipe进行姿势识别，使用峰值特征检测计数 repetitions，并使用可学习的物理模拟器跟踪运动的动态变化。一个测试视频根据异常分析 deviations from the learned prototypical motion。该系统被评估在六种全身和上半身运动中。这些实时建议，通过低成本的设备如智能手机，将让运动员可以Rectify potential mistakes，使自修成为可能，同时降低运动伤害的风险。
</details></li>
</ul>
<hr>
<h2 id="Improved-Membership-Inference-Attacks-Against-Language-Classification-Models"><a href="#Improved-Membership-Inference-Attacks-Against-Language-Classification-Models" class="headerlink" title="Improved Membership Inference Attacks Against Language Classification Models"></a>Improved Membership Inference Attacks Against Language Classification Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07219">http://arxiv.org/abs/2310.07219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shlomit Shachor, Natalia Razinkov, Abigail Goldsteen</li>
<li>for: 这篇论文旨在评估机器学习模型中的隐私风险，以帮助决策使用、部署或共享模型。</li>
<li>methods: 该论文提出了一种新的整合方法，通过生成多个专门的攻击模型来对分类模型进行会员推理攻击。</li>
<li>results: 该研究表明，使用该整合方法可以实现更高的攻击精度，比单个攻击模型或每个分类标签的攻击模型都高。<details>
<summary>Abstract</summary>
Artificial intelligence systems are prevalent in everyday life, with use cases in retail, manufacturing, health, and many other fields. With the rise in AI adoption, associated risks have been identified, including privacy risks to the people whose data was used to train models. Assessing the privacy risks of machine learning models is crucial to enabling knowledgeable decisions on whether to use, deploy, or share a model. A common approach to privacy risk assessment is to run one or more known attacks against the model and measure their success rate. We present a novel framework for running membership inference attacks against classification models. Our framework takes advantage of the ensemble method, generating many specialized attack models for different subsets of the data. We show that this approach achieves higher accuracy than either a single attack model or an attack model per class label, both on classical and language classification tasks.
</details>
<details>
<summary>摘要</summary>
人工智能系统在日常生活中广泛应用，包括零售、制造、医疗等领域。随着人工智能的普及，相关的风险也被识别出来，包括使用人工智能模型训练数据的隐私风险。评估机器学习模型的隐私风险是必要的，以便做出了知情的决策是否使用、部署或共享模型。我们提出了一种新的散 membership 攻击框架，该框架利用 ensemble 方法，生成多个特化的攻击模型，用于不同的数据子集。我们展示了，这种方法可以在类别和语言分类任务上达到更高的准确率，比单个攻击模型或每个类别标签的攻击模型都高。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Agent-Interaction-in-Multi-agent-Reinforcement-Learning-for-Cost-efficient-Generalization"><a href="#Quantifying-Agent-Interaction-in-Multi-agent-Reinforcement-Learning-for-Cost-efficient-Generalization" class="headerlink" title="Quantifying Agent Interaction in Multi-agent Reinforcement Learning for Cost-efficient Generalization"></a>Quantifying Agent Interaction in Multi-agent Reinforcement Learning for Cost-efficient Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07218">http://arxiv.org/abs/2310.07218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Chen, Chen Tang, Ran Tian, Chenran Li, Jinning Li, Masayoshi Tomizuka, Wei Zhan</li>
<li>for: 这 paper 是 investigate 多智能体强化学习（MARL）中的泛化问题。</li>
<li>methods: 这 paper 使用 Level of Influence（LoI）metric 量化多智能体之间的互动程度，并在不同的enario和环境中评估 LoI 对泛化性能的影响。</li>
<li>results: 研究发现，在许多enario中，多个合作者的多样化训练可以提高 eg agent 的泛化性能，但是这种提高的程度因enario和环境而异。LoI 能够预测这种差异性。此外，基于 LoI 的资源分配策略可以在受限的 computation budget 下提高泛化性能。<details>
<summary>Abstract</summary>
Generalization poses a significant challenge in Multi-agent Reinforcement Learning (MARL). The extent to which an agent is influenced by unseen co-players depends on the agent's policy and the specific scenario. A quantitative examination of this relationship sheds light on effectively training agents for diverse scenarios. In this study, we present the Level of Influence (LoI), a metric quantifying the interaction intensity among agents within a given scenario and environment. We observe that, generally, a more diverse set of co-play agents during training enhances the generalization performance of the ego agent; however, this improvement varies across distinct scenarios and environments. LoI proves effective in predicting these improvement disparities within specific scenarios. Furthermore, we introduce a LoI-guided resource allocation method tailored to train a set of policies for diverse scenarios under a constrained budget. Our results demonstrate that strategic resource allocation based on LoI can achieve higher performance than uniform allocation under the same computation budget.
</details>
<details>
<summary>摘要</summary>
<<SYS>>文本翻译成简化中文。<</SYS>>多 Agent Reinforcement Learning（MARL）中，泛化带来了重要的挑战。代表者在未经见过的合作者的影响程度取决于代表者的策略和特定情况。一个量化的分析这个关系，可以帮助有效地训练代表者。在这个研究中，我们提出了影响度指数（LoI），用于衡量在给定enario和环境中代表者之间的交互强度。我们发现，通常情况下，在训练过程中采用更加多样化的合作者集合，可以提高代表者的泛化性能；然而，这种改善的程度随着不同的情况和环境而异。LoI有效地预测这些改善的差异。此外，我们还提出了基于LoI的资源分配策略，用于在有限的计算预算下训练多种情况下的策略。我们的结果表明，根据LoI进行策略资源的有计划分配，可以在同一个计算预算下达到更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Learning-Enabled-Automatic-Vessel-Draft-Reading-for-Intelligent-Maritime-Surveillance"><a href="#Multi-Task-Learning-Enabled-Automatic-Vessel-Draft-Reading-for-Intelligent-Maritime-Surveillance" class="headerlink" title="Multi-Task Learning-Enabled Automatic Vessel Draft Reading for Intelligent Maritime Surveillance"></a>Multi-Task Learning-Enabled Automatic Vessel Draft Reading for Intelligent Maritime Surveillance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07212">http://arxiv.org/abs/2310.07212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingxiang Qu, Ryan Wen Liu, Chenjie Zhao, Yu Guo, Sendren Sheng-Dong Xu, Fenghua Zhu, Yisheng Lv</li>
<li>For:  This paper proposes a multi-task learning-enabled computational method (MTL-VDR) for generating highly reliable vessel draft depth readings.* Methods: The MTL-VDR method consists of four components: draft mark detection, draft scale recognition, vessel&#x2F;water segmentation, and final draft depth estimation. The method uses a powerful and efficient convolutional neural network for draft mark detection and employs a multi-task learning method for simultaneous draft scale recognition and vessel&#x2F;water segmentation.* Results: The method demonstrated superior performance in terms of accuracy, robustness, and efficiency, with an adaptive computational method used to yield an accurate and robust draft depth. The computational speed exceeds 40 FPS, satisfying the requirements of real-time maritime surveillance to guarantee vessel traffic safety.<details>
<summary>Abstract</summary>
The accurate and efficient vessel draft reading (VDR) is an important component of intelligent maritime surveillance, which could be exploited to assist in judging whether the vessel is normally loaded or overloaded. The computer vision technique with an excellent price-to-performance ratio has become a popular medium to estimate vessel draft depth. However, the traditional estimation methods easily suffer from several limitations, such as sensitivity to low-quality images, high computational cost, etc. In this work, we propose a multi-task learning-enabled computational method (termed MTL-VDR) for generating highly reliable VDR. In particular, our MTL-VDR mainly consists of four components, i.e., draft mark detection, draft scale recognition, vessel/water segmentation, and final draft depth estimation. We first construct a benchmark dataset related to draft mark detection and employ a powerful and efficient convolutional neural network to accurately perform the detection task. The multi-task learning method is then proposed for simultaneous draft scale recognition and vessel/water segmentation. To obtain more robust VDR under complex conditions (e.g., damaged and stained scales, etc.), the accurate draft scales are generated by an automatic correction method, which is presented based on the spatial distribution rules of draft scales. Finally, an adaptive computational method is exploited to yield an accurate and robust draft depth. Extensive experiments have been implemented on the realistic dataset to compare our MTL-VDR with state-of-the-art methods. The results have demonstrated its superior performance in terms of accuracy, robustness, and efficiency. The computational speed exceeds 40 FPS, which satisfies the requirements of real-time maritime surveillance to guarantee vessel traffic safety.
</details>
<details>
<summary>摘要</summary>
“精准和高效的船舶吃水深度读取（VDR）是智能海上监测中重要的一部分，可以帮助判断船舶是否超载。计算机视觉技术具有出色的价格-性能比，成为船舶吃水深度估算的受欢迎媒体。然而，传统估算方法容易受到低质量图像、高计算成本等限制。在这种情况下，我们提出了一种基于多任务学习的计算方法（简称MTL-VDR），用于生成高可靠性的VDR。具体来说，我们的MTL-VDR包括四个组件：船舶吃水深度检测、船舶/水域分割、船舶吃水深度估算和自适应计算方法。我们首先构建了相关的船舶吃水深度检测数据集，并使用高效和强大的卷积神经网络进行检测任务的准确实施。然后，我们提出了多任务学习方法，用于同时进行船舶吃水深度估算和船舶/水域分割。为了在复杂情况下（如损坏和污染等）获得更加稳定的VDR，我们提出了一种自动更正方法，基于船舶吃水深度的精度规则。最后，我们运用了一种适应计算方法，以确保高准确性和稳定性。我们对实际数据进行了广泛的实验，与现有方法进行比较。结果显示，我们的MTL-VDR在精度、稳定性和效率方面具有显著的优势。计算速度超过40帧每秒，满足了实时海上监测的需求，以保障船舶交通安全。”
</details></li>
</ul>
<hr>
<h2 id="State-of-the-Art-on-Diffusion-Models-for-Visual-Computing"><a href="#State-of-the-Art-on-Diffusion-Models-for-Visual-Computing" class="headerlink" title="State of the Art on Diffusion Models for Visual Computing"></a>State of the Art on Diffusion Models for Visual Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07204">http://arxiv.org/abs/2310.07204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T. Barron, Amit H. Bermano, Eric Ryan Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, C. Karen Liu, Lingjie Liu, Ben Mildenhall, Matthias Nießner, Björn Ommer, Christian Theobalt, Peter Wonka, Gordon Wetzstein</li>
<li>for: 提供一个入门性的状态报告，帮助研究者、艺术家和实践者了解扩散模型在视觉计算领域的基本数学概念、实现细节和设计选择，以及扩散基于生成AI工具的各种应用和扩展。</li>
<li>methods: 涵盖了扩散模型的基本数学概念、Stable Diffusion模型的实现细节和设计选择，以及扩散基于生成AI工具的各种应用和扩展。</li>
<li>results: 提供了一个全面的Literature综述，概述了扩散基于生成AI工具的各种应用和扩展，包括2D图像、视频、3D物体、动作和4D场景等。同时也讨论了可用的数据集、评价指标、开放的挑战和社会影响等。<details>
<summary>Abstract</summary>
The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion-based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.
</details>
<details>
<summary>摘要</summary>
领域的视觉计算在迅速发展，启动了基于生成人工智能（AI）的扩展，这些技术在图像、视频和3D场景的生成、编辑和重建方面提供了无前例的能力。在这些领域中，扩散模型是生成AI架构的首选。过去一年内，有关扩散工具和应用的学术论文数量在计算机图形、计算机视觉和人工智能领域呈指数增长，每天在arXiv上出现新的论文。这种快速发展的领域使得保持最新的发展变得困难。本state-of-the-art报告（STAR）的目的是介绍扩散模型的基本数学概念，扩散模型的实现细节和设计选择，以及生成AI工具的重要方面，包括个性化、条件、反向等。此外，我们还给出了生成和编辑扩散模型的快速增长的评价，分类为生成媒介的类型，包括2D图像、视频、3D物体、移动和4D场景。最后，我们讨论了可用的数据集、评价指标、开放的挑战和社会影响。这个STAR为研究者、艺术家和实践者提供了直观的入门点，以便更好地探索这个激动人心的主题。
</details></li>
</ul>
<hr>
<h2 id="MatChat-A-Large-Language-Model-and-Application-Service-Platform-for-Materials-Science"><a href="#MatChat-A-Large-Language-Model-and-Application-Service-Platform-for-Materials-Science" class="headerlink" title="MatChat: A Large Language Model and Application Service Platform for Materials Science"></a>MatChat: A Large Language Model and Application Service Platform for Materials Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07197">http://arxiv.org/abs/2310.07197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Chen, Fankai Xie, Meng Wan, Yang Yuan, Miao Liu, Zongguo Wang, Sheng Meng, Yangang Wang</li>
<li>for: 预测化学合成路径，以满足材料科学研究中的需求。</li>
<li>methods: 利用自动生成文本和问答系统，以及精细调整技术，实现大规模AI模型在特定领域中的部署。</li>
<li>results: 研究人员通过使用LLaMA2-7B模型，并将其特化为材料科学领域，创造出了名为MatChat的特殊AI模型，可以预测无机材料合成路径。MatChat表现出了丰富的知识掌握和逻辑能力，但还需要进一步改进，以满足不同材料设计需求。<details>
<summary>Abstract</summary>
The prediction of chemical synthesis pathways plays a pivotal role in materials science research. Challenges, such as the complexity of synthesis pathways and the lack of comprehensive datasets, currently hinder our ability to predict these chemical processes accurately. However, recent advancements in generative artificial intelligence (GAI), including automated text generation and question-answering systems, coupled with fine-tuning techniques, have facilitated the deployment of large-scale AI models tailored to specific domains. In this study, we harness the power of the LLaMA2-7B model and enhance it through a learning process that incorporates 13,878 pieces of structured material knowledge data. This specialized AI model, named MatChat, focuses on predicting inorganic material synthesis pathways. MatChat exhibits remarkable proficiency in generating and reasoning with knowledge in materials science. Although MatChat requires further refinement to meet the diverse material design needs, this research undeniably highlights its impressive reasoning capabilities and innovative potential in the field of materials science. MatChat is now accessible online and open for use, with both the model and its application framework available as open source. This study establishes a robust foundation for collaborative innovation in the integration of generative AI in materials science.
</details>
<details>
<summary>摘要</summary>
文本预测在材料科学研究中扮演着重要的角色。现在，化学合成路径的复杂性和缺乏全面数据库等问题正在阻碍我们准确预测这些化学过程。然而，最近的生成人工智能（GAI）技术，包括自动生成文本和问答系统，以及精细调整技术，已经使得大规模AI模型在特定领域中进行部署。在本研究中，我们利用LLaMA2-7B模型的力量，并通过包括13,878个结构化物理知识数据的学习过程，开发了一个专门用于预测无机材料合成路径的AI模型，称为MatChat。MatChat在材料科学领域中表现出了惊人的知识生成和理解能力。虽然MatChat还需要进一步的优化，以满足多样化的材料设计需求，但这项研究无疑地高亮了MatChat在材料科学领域的创新潜力。MatChat现在在线开放，可以免费使用，模型和应用框架都是开源的。本研究建立了对生成AI在材料科学领域的集成创新的坚实基础。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Gating-in-Mixture-of-Experts-based-Language-Models"><a href="#Adaptive-Gating-in-Mixture-of-Experts-based-Language-Models" class="headerlink" title="Adaptive Gating in Mixture-of-Experts based Language Models"></a>Adaptive Gating in Mixture-of-Experts based Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07188">http://arxiv.org/abs/2310.07188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiamin Li, Qiang Su, Yitao Yang, Yimin Jiang, Cong Wang, Hong Xu</li>
<li>for: 这篇论文主要是关于如何使用适应性网络来提高语言模型的训练效率和性能。</li>
<li>methods: 该论文提出了一种适应性网络（MoE）模型，其中每个token可以通过不同的专家来进行计算，以适应不同的语言复杂度。此外，论文还使用了课程学习来进一步降低训练时间。</li>
<li>results: 实验结果显示，适应性网络可以减少最多22.5%的训练时间，同时保持推理质量。此外，论文还进行了路由决策的分析，并提供了相关的分析结论。<details>
<summary>Abstract</summary>
Large language models, such as OpenAI's ChatGPT, have demonstrated exceptional language understanding capabilities in various NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE model adopts a fixed gating network where each token is computed by the same number of experts. However, this approach contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. The proposed framework preserves sparsity while improving training efficiency. Additionally, curriculum learning is leveraged to further reduce training time. Extensive experiments on diverse NLP tasks show that adaptive gating reduces at most 22.5% training time while maintaining inference quality. Moreover, we conduct a comprehensive analysis of the routing decisions and present our insights when adaptive gating is used.
</details>
<details>
<summary>摘要</summary>
大型语言模型，如OpenAI的ChatGPT，在不同的自然语言处理任务中表现出了非常出色的语言理解能力。零启动权重的混合专家（MoE）已经成为了可扩展模型的一个有 promise的解决方案，以保持计算操作数量的常数。现有的MoE模型采用固定的闭包网络，每个字符都由相同数量的专家计算。然而，这种方法与我们的语言理解 intuition 相抵触，即每个序列中的字符具有不同的语言复杂度，因此需要不同的计算成本。在优化过程中，对计算每个字符的时间和模型性能之间的负面ffekt little 的研究。本文提出了适应性闭包（Adaptive Gating），一种灵活的训练策略，使得字符可以根据专家概率分布来处理不同数量的专家。该提案保持了稀疏性，同时改善了训练效率。此外，我们还利用了课程学习，以进一步减少训练时间。广泛的实验表明，适应性闭包可以在不同的自然语言处理任务中减少训练时间最多22.5%，保持推理质量。此外，我们还进行了路由决策的全面分析，并对适应性闭包使用的时候提供了我们的思路。
</details></li>
</ul>
<hr>
<h2 id="Multiview-Transformer-Rethinking-Spatial-Information-in-Hyperspectral-Image-Classification"><a href="#Multiview-Transformer-Rethinking-Spatial-Information-in-Hyperspectral-Image-Classification" class="headerlink" title="Multiview Transformer: Rethinking Spatial Information in Hyperspectral Image Classification"></a>Multiview Transformer: Rethinking Spatial Information in Hyperspectral Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07186">http://arxiv.org/abs/2310.07186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Zhang, Yongshan Zhang, Yicong Zhou</li>
<li>for: 本文是为了提高涉及谱图像（HSI）的地形分类准确性而研究的。</li>
<li>methods: 本文使用多视图变换器（MPCA、SED、SPTT）来提取HSI中的空间-spectral特征表示。MPCA通过构建多视图观察数据，并在每个视图数据上应用PCA来提取低维度视图表示。SED使用全连接卷积神经网络来提取多视图特征图。SPTT使用空间poolingtokenization策略将多视图特征转换为tokens，学习稳定和分类的空间-spectral特征。</li>
<li>results: 实验结果表明，提出的多视图变换器在三个HSI数据集上表现出色，超过了现有方法的性能。<details>
<summary>Abstract</summary>
Identifying the land cover category for each pixel in a hyperspectral image (HSI) relies on spectral and spatial information. An HSI cuboid with a specific patch size is utilized to extract spatial-spectral feature representation for the central pixel. In this article, we investigate that scene-specific but not essential correlations may be recorded in an HSI cuboid. This additional information improves the model performance on existing HSI datasets and makes it hard to properly evaluate the ability of a model. We refer to this problem as the spatial overfitting issue and utilize strict experimental settings to avoid it. We further propose a multiview transformer for HSI classification, which consists of multiview principal component analysis (MPCA), spectral encoder-decoder (SED), and spatial-pooling tokenization transformer (SPTT). MPCA performs dimension reduction on an HSI via constructing spectral multiview observations and applying PCA on each view data to extract low-dimensional view representation. The combination of view representations, named multiview representation, is the dimension reduction output of the MPCA. To aggregate the multiview information, a fully-convolutional SED with a U-shape in spectral dimension is introduced to extract a multiview feature map. SPTT transforms the multiview features into tokens using the spatial-pooling tokenization strategy and learns robust and discriminative spatial-spectral features for land cover identification. Classification is conducted with a linear classifier. Experiments on three HSI datasets with rigid settings demonstrate the superiority of the proposed multiview transformer over the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Identifying the land cover category for each pixel in a hyperspectral image (HSI) relies on both spectral and spatial information. We use an HSI cuboid with a specific patch size to extract spatial-spectral feature representations for the central pixel. However, we find that scene-specific but not essential correlations may be recorded in the HSI cuboid, which can improve model performance on existing HSI datasets but also make it difficult to evaluate the model's ability. We refer to this problem as the spatial overfitting issue and use strict experimental settings to avoid it.To address this issue, we propose a multiview transformer for HSI classification, which consists of multiview principal component analysis (MPCA), spectral encoder-decoder (SED), and spatial-pooling tokenization transformer (SPTT). MPCA performs dimension reduction on the HSI by constructing spectral multiview observations and applying PCA on each view data to extract low-dimensional view representations. The combination of view representations, named multiview representation, is the dimension reduction output of the MPCA. To aggregate the multiview information, a fully-convolutional SED with a U-shape in spectral dimension is introduced to extract a multiview feature map. SPTT transforms the multiview features into tokens using the spatial-pooling tokenization strategy and learns robust and discriminative spatial-spectral features for land cover identification. Classification is conducted with a linear classifier.Experiments on three HSI datasets with rigid settings demonstrate the superiority of the proposed multiview transformer over the state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="rpcPRF-Generalizable-MPI-Neural-Radiance-Field-for-Satellite-Camera"><a href="#rpcPRF-Generalizable-MPI-Neural-Radiance-Field-for-Satellite-Camera" class="headerlink" title="rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera"></a>rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07179">http://arxiv.org/abs/2310.07179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongtong Zhang, Yuanxiang Li</li>
<li>for: 这个论文targets the task of novel view synthesis of satellite images, with a focus on Rational Polynomial Camera (RPC) models.</li>
<li>methods: The proposed method, called rpcPRF, uses a Multiplane Images (MPI) based Planar neural Radiance Field (PRF) to synthesize novel views of satellite images. The model leverages reprojection supervision to generalize to unseen scenes and removes the need for dense depth supervision.</li>
<li>results: The paper reports that rpcPRF outperforms state-of-the-art NERF-based methods in terms of image fidelity, reconstruction accuracy, and efficiency on two datasets (TLC and SatMVS3D) with urban scenes from WV-3 and ZY-3 satellites.<details>
<summary>Abstract</summary>
Novel view synthesis of satellite images holds a wide range of practical applications. While recent advances in the Neural Radiance Field have predominantly targeted pin-hole cameras, and models for satellite cameras often demand sufficient input views. This paper presents rpcPRF, a Multiplane Images (MPI) based Planar neural Radiance Field for Rational Polynomial Camera (RPC). Unlike coordinate-based neural radiance fields in need of sufficient views of one scene, our model is applicable to single or few inputs and performs well on images from unseen scenes. To enable generalization across scenes, we propose to use reprojection supervision to induce the predicted MPI to learn the correct geometry between the 3D coordinates and the images. Moreover, we remove the stringent requirement of dense depth supervision from deep multiview-stereo-based methods by introducing rendering techniques of radiance fields. rpcPRF combines the superiority of implicit representations and the advantages of the RPC model, to capture the continuous altitude space while learning the 3D structure. Given an RGB image and its corresponding RPC, the end-to-end model learns to synthesize the novel view with a new RPC and reconstruct the altitude of the scene. When multiple views are provided as inputs, rpcPRF exerts extra supervision provided by the extra views. On the TLC dataset from ZY-3, and the SatMVS3D dataset with urban scenes from WV-3, rpcPRF outperforms state-of-the-art nerf-based methods by a significant margin in terms of image fidelity, reconstruction accuracy, and efficiency, for both single-view and multiview task.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:Novel view synthesis of satellite images has a wide range of practical applications. While recent advances in the Neural Radiance Field have predominantly targeted pin-hole cameras, and models for satellite cameras often demand sufficient input views. This paper presents rpcPRF, a Multiplane Images (MPI) based Planar neural Radiance Field for Rational Polynomial Camera (RPC). Unlike coordinate-based neural radiance fields in need of sufficient views of one scene, our model is applicable to single or few inputs and performs well on images from unseen scenes. To enable generalization across scenes, we propose to use reprojection supervision to induce the predicted MPI to learn the correct geometry between the 3D coordinates and the images. Moreover, we remove the stringent requirement of dense depth supervision from deep multiview-stereo-based methods by introducing rendering techniques of radiance fields. rpcPRF combines the superiority of implicit representations and the advantages of the RPC model, to capture the continuous altitude space while learning the 3D structure. Given an RGB image and its corresponding RPC, the end-to-end model learns to synthesize the novel view with a new RPC and reconstruct the altitude of the scene. When multiple views are provided as inputs, rpcPRF exerts extra supervision provided by the extra views. On the TLC dataset from ZY-3, and the SatMVS3D dataset with urban scenes from WV-3, rpcPRF outperforms state-of-the-art nerf-based methods by a significant margin in terms of image fidelity, reconstruction accuracy, and efficiency, for both single-view and multiview task.Translation:新视图合成卫星图像应用广泛。Recent Advances in Neural Radiance Field 主要针对平面镜头，而卫星相机模型通常需要足够的输入视图。本文提出了rpcPRF，基于多平面图像（MPI）的平面神经频率场（RPC）模型。与坐标基于神经频率场的模型不同，我们的模型适用于单个或几个输入，并在未见场景中表现良好。为实现场景总结，我们提议使用重投映监督，使预测的MPI学习正确的场景坐标和图像之间的几何关系。此外，我们从深度多视图雷达方法中移除了密集深度监督的需求，通过引入投映技术来实现雷达场景的渲染。rpcPRF结合了神经频率场的优势和RPC模型的优点，可以捕捉不间断的高度空间，同时学习3D结构。给定一个RGB图像和其相应的RPC，结束到终点模型可以使用新的RPCsynthesize Novel View和重建场景的高度。当提供多个视图输入时，rpcPRF可以提供Extra supervision。在ZY-3的TLC数据集和WV-3的SatMVS3D数据集上，rpcPRF与状态aru的nerf-based方法比之，在图像准确度、重建精度和效率等方面具有显著的优势，包括单视图和多视图任务。
</details></li>
</ul>
<hr>
<h2 id="Online-Speculative-Decoding"><a href="#Online-Speculative-Decoding" class="headerlink" title="Online Speculative Decoding"></a>Online Speculative Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07177">http://arxiv.org/abs/2310.07177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, Hao Zhang</li>
<li>for: 加速大语言模型（LLM）的推理过程，使用较小的稿本模型预测目标模型的输出。</li>
<li>methods: 在线预测推理（OSD）技术，通过在用户查询数据观察到的过程中不断更新（多个）稿本模型（），使用LLM服务器集群的剩余计算能力进行在线重新训练稿本模型，以提高预测精度。</li>
<li>results: 根据实验结果，在线预测推理可以提高稿本模型的准确率，从而提高LLM的推理效率，并且可以降低延迟时间，实现1.22倍至3.06倍的提升。<details>
<summary>Abstract</summary>
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target model's outputs, particularly on data originating from query distributions. As the draft model evolves online, it aligns with the query distribution in real time, mitigating distribution shifts. We develop a prototype of online speculative decoding based on online knowledge distillation and evaluate it using both synthetic and real query data on several popular LLMs. The results show a substantial increase in the token acceptance rate by 0.1 to 0.65, which translates into 1.22x to 3.06x latency reduction.
</details>
<details>
<summary>摘要</summary>
推测解码是一种重要的技术，可以加速大型语言模型（LLM）的推断过程，通过使用一个更小的签名模型来预测目标模型的输出。然而，其效果可能受到签名模型预测精度的限制，特别是面对多样化的文本输入和目标模型的能力差距。我们提出在线推测解码（OSD）技术来解决这个挑战。主要想法是在 LLM 服务中继续更新（多个）签名模型（多个），使用 LLM 的剩余计算能力来在线 retrained 签名模型，从而使得 retrained 成本neutral。由于 LLM 的查询分布相对简单，因此在线 retrained 签名模型可以更好地预测目标模型的输出，特别是来自查询分布的数据。在签名模型在线演化的过程中，它与查询分布相对应，使得分布shift Mitigated。我们开发了基于在线知识填充的 OSD  прототип，并对其进行了Synthetic和实际查询数据的评估。结果表明，通过使用 OSD 技术，可以提高 токен接受率 BY 0.1 TO 0.65，相当于减少延迟时间 BY 1.22x TO 3.06x。
</details></li>
</ul>
<hr>
<h2 id="Solving-Travelling-Thief-Problems-using-Coordination-Based-Methods"><a href="#Solving-Travelling-Thief-Problems-using-Coordination-Based-Methods" class="headerlink" title="Solving Travelling Thief Problems using Coordination Based Methods"></a>Solving Travelling Thief Problems using Coordination Based Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07156">http://arxiv.org/abs/2310.07156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Majid Namazi, M. A. Hakim Newton, Conrad Sanderson, Abdul Sattar</li>
<li>for:  solves the Travelling Thief Problem (TTP) by proposing a coordination-based approach that integrates human-designed and machine learning-based heuristics to improve solution quality.</li>
<li>methods:  uses a combination of local search, human-designed coordination heuristics, and machine learning to explore cyclic tours and make item selections during collection plan exploration.</li>
<li>results:  significantly outperforms existing state-of-the-art TTP solvers on a set of benchmark problems, demonstrating the effectiveness of the proposed coordination-based approach.<details>
<summary>Abstract</summary>
A travelling thief problem (TTP) is a proxy to real-life problems such as postal collection. TTP comprises an entanglement of a travelling salesman problem (TSP) and a knapsack problem (KP) since items of KP are scattered over cities of TSP, and a thief has to visit cities to collect items. In TTP, city selection and item selection decisions need close coordination since the thief's travelling speed depends on the knapsack's weight and the order of visiting cities affects the order of item collection. Existing TTP solvers deal with city selection and item selection separately, keeping decisions for one type unchanged while dealing with the other type. This separation essentially means very poor coordination between two types of decision. In this paper, we first show that a simple local search based coordination approach does not work in TTP. Then, to address the aforementioned problems, we propose a human designed coordination heuristic that makes changes to collection plans during exploration of cyclic tours. We further propose another human designed coordination heuristic that explicitly exploits the cyclic tours in item selections during collection plan exploration. Lastly, we propose a machine learning based coordination heuristic that captures characteristics of the two human designed coordination heuristics. Our proposed coordination based approaches help our TTP solver significantly outperform existing state-of-the-art TTP solvers on a set of benchmark problems. Our solver is named Cooperation Coordination (CoCo) and its source code is available from https://github.com/majid75/CoCo
</details>
<details>
<summary>摘要</summary>
“旅行盗僧问题”（TTP）是一个代表现实生活中的问题，如邮政收集。TTP包括一个旅行销售问题（TSP）和一个袋包问题（KP）的挂钮，因为KP中的物品是在TSP中的城市分散的，盗僧需要到城市集取物品。在TTP中，城市选择和物品选择的决策需要密切协调，因为盗僧的旅行速度取决于袋包的重量，并且城市顺序影响物品顺序收集。现有的TTP解决方案是分开处理城市选择和物品选择，对一种类型的决策不会改变，而对另一种类型的决策进行处理。这种分离实际上意味着两种决策之间的很 poor 的协调。在这篇论文中，我们首先显示了一个简单的本地搜索基于协调方法在TTP中不会工作。然后，为了解决上述问题，我们提出了一个人工设计的协调规则，在探索游历的过程中对收集计划进行修改。我们还提出了另一个人工设计的协调规则，在收集计划探索过程中明确地利用游历。 finally, we propose a machine learning based coordination heuristic that captures the characteristics of the two human-designed coordination heuristics. our proposed coordination-based approaches significantly outperform existing state-of-the-art TTP solvers on a set of benchmark problems. our solver is named Cooperation Coordination (CoCo), and its source code is available from <https://github.com/majid75/CoCo>.
</details></li>
</ul>
<hr>
<h2 id="No-Privacy-Left-Outside-On-the-In-Security-of-TEE-Shielded-DNN-Partition-for-On-Device-ML"><a href="#No-Privacy-Left-Outside-On-the-In-Security-of-TEE-Shielded-DNN-Partition-for-On-Device-ML" class="headerlink" title="No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML"></a>No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07152">http://arxiv.org/abs/2310.07152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ziqi-zhang/teeslice-artifact">https://github.com/ziqi-zhang/teeslice-artifact</a></li>
<li>paper_authors: Ziqi Zhang, Chen Gong, Yifeng Cai, Yuanyuan Yuan, Bingyan Liu, Ding Li, Yao Guo, Xiangqun Chen</li>
<li>For: The paper is focused on addressing the security challenges of on-device machine learning (ML) models, specifically the threats of model stealing (MS) and membership inference attack (MIA).* Methods: The paper proposes a novel technique called TEESlice, which partitions the DNN model into two parts before training to defend against MS and MIA during inference. TEESlice uses a partition-before-training strategy to accurately separate privacy-related weights from public weights.* Results: The paper presents experimental results that show TEESlice delivers the same security protection as shielding the entire DNN model inside a Trusted Execution Environment (TEE), but with over 10X less overhead and no accuracy loss compared to prior TSDP solutions. The paper also highlights the inherent difficulty in deciding optimal DNN partition configurations for present TSDP solutions and the variability of such configurations across datasets and models.<details>
<summary>Abstract</summary>
On-device ML introduces new security challenges: DNN models become white-box accessible to device users. Based on white-box information, adversaries can conduct effective model stealing (MS) and membership inference attack (MIA). Using Trusted Execution Environments (TEEs) to shield on-device DNN models aims to downgrade (easy) white-box attacks to (harder) black-box attacks. However, one major shortcoming is the sharply increased latency (up to 50X). To accelerate TEE-shield DNN computation with GPUs, researchers proposed several model partition techniques. These solutions, referred to as TEE-Shielded DNN Partition (TSDP), partition a DNN model into two parts, offloading the privacy-insensitive part to the GPU while shielding the privacy-sensitive part within the TEE. This paper benchmarks existing TSDP solutions using both MS and MIA across a variety of DNN models, datasets, and metrics. We show important findings that existing TSDP solutions are vulnerable to privacy-stealing attacks and are not as safe as commonly believed. We also unveil the inherent difficulty in deciding optimal DNN partition configurations (i.e., the highest security with minimal utility cost) for present TSDP solutions. The experiments show that such ``sweet spot'' configurations vary across datasets and models. Based on lessons harvested from the experiments, we present TEESlice, a novel TSDP method that defends against MS and MIA during DNN inference. TEESlice follows a partition-before-training strategy, which allows for accurate separation between privacy-related weights from public weights. TEESlice delivers the same security protection as shielding the entire DNN model inside TEE (the ``upper-bound'' security guarantees) with over 10X less overhead (in both experimental and real-world environments) than prior TSDP solutions and no accuracy loss.
</details>
<details>
<summary>摘要</summary>
ondevice ML引入新的安全挑战：DNN模型变成了设备用户可见的白盒模型。基于白盒信息，攻击者可以进行有效的模型窃取（MS）和会员推理攻击（MIA）。使用Trusted Execution Environments（TEEs）保护在设备上的DNN模型，以降低（容易）白盒攻击到（更加困难）黑盒攻击。然而，一个主要缺点是增加了响应时间（最多50倍）。为了加速TEE保护的DNN计算，研究人员提出了多种模型分割技术。这些解决方案被称为TEE-Shielded DNN Partition（TSDP），它将DNN模型分成两部分，将隐私敏感部分卷入TEE中，而隐私不敏感部分将被卷入GPU上。这篇论文对现有TSDP解决方案进行了MS和MIA的分别测试，并对多个DNN模型、数据集和指标进行了测试。我们发现现有TSDP解决方案容易受到隐私窃取攻击，并不如常被认为的安全。我们还发现决定最佳DNN分割配置（即最高安全性和最小实用成本）对现有TSDP解决方案是困难的。实验表明，这些“甜点”配置在不同的数据集和模型上具有差异。基于实验所获的经验，我们提出了TEESlice，一种新的TSDP方法。TEESlice采用分配before training的策略，允许准确地分化隐私相关的权重与公共权重。TEESlice提供了完全保护MS和MIA During DNN推理的安全保障，并且在实验和实际环境中具有10倍以上的性能优化，无损失 accuracy。
</details></li>
</ul>
<hr>
<h2 id="Determining-Winners-in-Elections-with-Absent-Votes"><a href="#Determining-Winners-in-Elections-with-Absent-Votes" class="headerlink" title="Determining Winners in Elections with Absent Votes"></a>Determining Winners in Elections with Absent Votes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07150">http://arxiv.org/abs/2310.07150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qishen Han, Amélie Marian, Lirong Xia</li>
<li>for: 这个论文研究了选举中缺失选票的情况下，决定赢家的问题。</li>
<li>methods: 这篇论文使用了NP-完全理论和特殊的位置得分规则，以计算缺失选票情况下的赢家问题。</li>
<li>results: 论文表明，在投票 truncated 情况下，赢家问题是NP-完全的，而在特定的位置得分规则下，问题可以在多阶段时间内解决。<details>
<summary>Abstract</summary>
An important question in elections is the determine whether a candidate can be a winner when some votes are absent. We study this determining winner with the absent votes (WAV) problem when the votes are top-truncated. We show that the WAV problem is NP-complete for the single transferable vote, Maximin, and Copeland, and propose a special case of positional scoring rule such that the problem can be computed in polynomial time. Our results in top-truncated rankings differ from the results in full rankings as their hardness results still hold when the number of candidates or the number of missing votes are bounded, while we show that the problem can be solved in polynomial time in either case.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>选举中一个重要问题是确定缺失票的候选人是否可以赢得选举。我们研究缺失票确定赢家问题（WAV问题），当投票是top-truncated时。我们显示WAV问题是NP-完全的 для单轮转移投票、最大最小值和冠军得分方式，并提出一种特殊情况的 pozitional scoring rule，使得问题可以在多阶段时间内解决。我们的结果在top-truncated排名中与全排名的结果不同，因为他们的困难结果仍然在缺失票或候选人数量是有限的情况下仍然成立，而我们则显示问题可以在任一情况下解决在多阶段时间内。Note: "简化中文" (Simplified Chinese) is a standardized form of Chinese used in mainland China and Singapore, while " tradicional中文" (Traditional Chinese) is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Denoising-Task-Routing-for-Diffusion-Models"><a href="#Denoising-Task-Routing-for-Diffusion-Models" class="headerlink" title="Denoising Task Routing for Diffusion Models"></a>Denoising Task Routing for Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07138">http://arxiv.org/abs/2310.07138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, Changick Kim</li>
<li>for: 这篇论文的目的是提出一种简单的添加策略，以提高 diffusion 模型中的多任务学习（MTL）性能。</li>
<li>methods: 该策略基于 selectively 活跃 diffusion 模型中的多个核心通道，以实现不同任务之间的信息路径分离。</li>
<li>results:  experiments 表明，该策略可以提高 diffusion 模型的性能，并且不需要添加额外参数。此外，该策略还可以加速训练过程的收敛。<details>
<summary>Abstract</summary>
Diffusion models generate highly realistic images through learning a multi-step denoising process, naturally embodying the principles of multi-task learning (MTL). Despite the inherent connection between diffusion models and MTL, there remains an unexplored area in designing neural architectures that explicitly incorporate MTL into the framework of diffusion models. In this paper, we present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. What makes DTR particularly compelling is its seamless integration of prior knowledge of denoising tasks into the framework: (1) Task Affinity: DTR activates similar channels for tasks at adjacent timesteps and shifts activated channels as sliding windows through timesteps, capitalizing on the inherent strong affinity between tasks at adjacent timesteps. (2) Task Weights: During the early stages (higher timesteps) of the denoising process, DTR assigns a greater number of task-specific channels, leveraging the insight that diffusion models prioritize reconstructing global structure and perceptually rich contents in earlier stages, and focus on simple noise removal in later stages. Our experiments demonstrate that DTR consistently enhances the performance of diffusion models across various evaluation protocols, all without introducing additional parameters. Furthermore, DTR contributes to accelerating convergence during training. Finally, we show the complementarity between our architectural approach and existing MTL optimization techniques, providing a more complete view of MTL within the context of diffusion training.
</details>
<details>
<summary>摘要</summary>
Diffusion models可以生成非常真实的图像，通过学习多步降噪过程，自然地启用多任务学习（MTL）的原理。尽管涉及到的连接在 diffusion models 和 MTL 之间，仍然有一个未探索的领域，那就是设计神经网络架构，以显式地包含 MTL 在 diffusion models 中。在这篇论文中，我们提出了一种简单的扩展策略，即 Denoising Task Routing（DTR），可以让现有的 diffusion model 架构中设置独特的信息通路，以便每个任务在单一架构中有自己的信息通路。DTR 的实现方式很有趣，它通过在模型中选择性地启用多个通道来实现这一点。具体来说，DTR 在不同的时间步骤中启用不同的通道，使得模型可以在不同的时间步骤中完成不同的任务。此外，DTR 还可以根据任务之间的相互关系来启用相应的通道，从而使得模型可以更好地利用多任务的相互关系。我们的实验结果表明，DTR 可以一直提高 diffusion models 的性能，无需添加额外参数。此外，DTR 还可以加速训练过程的收敛。最后，我们还证明了 DTR 和现有的 MTL 优化技术之间的相互关系，从而提供了更全面的 MTL 视角，以便更好地理解 diffusion 训练中的多任务学习。
</details></li>
</ul>
<hr>
<h2 id="Off-Policy-Evaluation-for-Human-Feedback"><a href="#Off-Policy-Evaluation-for-Human-Feedback" class="headerlink" title="Off-Policy Evaluation for Human Feedback"></a>Off-Policy Evaluation for Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07123">http://arxiv.org/abs/2310.07123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qitong Gao, Ge Gao, Juncheng Dong, Vahid Tarokh, Min Chi, Miroslav Pajic</li>
<li>for: 用于评估人工奖励信号（HF）的精度，提高RL在医疗等领域的安全性和效率。</li>
<li>methods: 基于IHR恢复方法和环境知识储存的幂等空间，对HF信号进行精度评估。</li>
<li>results: 在实验中，比直接使用现有OPE方法而言，我们的方法显著提高了HF信号的精度评估表现。<details>
<summary>Abstract</summary>
Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and is only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we develop an immediate human reward (IHR) reconstruction approach, regularized by environmental knowledge distilled in a latent space that captures the underlying dynamics of state transitions as well as issuing HF signals. Our approach has been tested over two real-world experiments, adaptive in-vivo neurostimulation and intelligent tutoring, as well as in a simulation environment (visual Q&A). Results show that our approach significantly improves the performance toward estimating HF signals accurately, compared to directly applying (variants of) existing OPE methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Off-policy evaluation (OPE) 是关键性的，可以减少在训练和评估强化学习（RL）中线上部署的成本，通过使用停滞轨迹来估计目标（评估）策略的性能和/或排名。它可以提高数据采集和策略测试的安全性和效率，特别是在医疗领域，因为在线部署是昂贵的。然而，现有的 OPE 方法无法准确地估计人类反馈（HF）信号，因为 HF 可能会受到多个下面因素的影响，并且只有稀缺的可用；相比之下，agent-defined 环境奖励（用于策略优化）通常是基于参数函数或分布来定义的。因此，HF 信号的自然特点使得 extrapolating 准确的 OPE 估计变得挑战。为解决这一问题，我们介绍了一个 OPEHF 框架，该框架可以重新使用现有的 OPE 方法，以准确地评估 HF 信号。specifically，我们开发了一种立即人类奖励（IHR）重构方法，该方法在环境知识捕获层中进行Regularization，以捕捉状态转移的下面动态和发出 HF 信号。我们的方法在两个实际实验（adaptive in-vivo neurostimulation和智能教学）以及一个模拟环境（视觉问答）中进行测试，结果表明，我们的方法可以准确地估计 HF 信号，相比直接应用（变种的）现有 OPE 方法。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="The-Temporal-Structure-of-Language-Processing-in-the-Human-Brain-Corresponds-to-The-Layered-Hierarchy-of-Deep-Language-Models"><a href="#The-Temporal-Structure-of-Language-Processing-in-the-Human-Brain-Corresponds-to-The-Layered-Hierarchy-of-Deep-Language-Models" class="headerlink" title="The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models"></a>The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07106">http://arxiv.org/abs/2310.07106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ariel Goldstein, Eric Ham, Mariano Schain, Samuel Nastase, Zaid Zada, Avigail Dabush, Bobbi Aubrey, Harshvardhan Gazula, Amir Feder, Werner K Doyle, Sasha Devore, Patricia Dugan, Daniel Friedman, Roi Reichart, Michael Brenner, Avinatan Hassidim, Orrin Devinsky, Adeen Flinker, Omer Levy, Uri Hasson</li>
<li>for: 这paper的目的是用Deep Language Models（DLMs）来理解人类大脑中自然语言处理的机制。</li>
<li>methods: 这paper使用了层次序列的连续数值向量来表示单词和上下文，从而开拓了一系列的应用，如人类化文本生成。</li>
<li>results: 这paper表明了DLM层次结构可以模型人类大脑中语言理解的时间动力学，并通过对ECoG数据的使用，实现了更高的时间分辨率。结果表明DLM和人类大脑的语言处理有关系，DLM层次结构的层次积累信息与人类大脑高级语言区域的神经活动相吻合。<details>
<summary>Abstract</summary>
Deep Language Models (DLMs) provide a novel computational paradigm for understanding the mechanisms of natural language processing in the human brain. Unlike traditional psycholinguistic models, DLMs use layered sequences of continuous numerical vectors to represent words and context, allowing a plethora of emerging applications such as human-like text generation. In this paper we show evidence that the layered hierarchy of DLMs may be used to model the temporal dynamics of language comprehension in the brain by demonstrating a strong correlation between DLM layer depth and the time at which layers are most predictive of the human brain. Our ability to temporally resolve individual layers benefits from our use of electrocorticography (ECoG) data, which has a much higher temporal resolution than noninvasive methods like fMRI. Using ECoG, we record neural activity from participants listening to a 30-minute narrative while also feeding the same narrative to a high-performing DLM (GPT2-XL). We then extract contextual embeddings from the different layers of the DLM and use linear encoding models to predict neural activity. We first focus on the Inferior Frontal Gyrus (IFG, or Broca's area) and then extend our model to track the increasing temporal receptive window along the linguistic processing hierarchy from auditory to syntactic and semantic areas. Our results reveal a connection between human language processing and DLMs, with the DLM's layer-by-layer accumulation of contextual information mirroring the timing of neural activity in high-order language areas.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ClausewitzGPT-Framework-A-New-Frontier-in-Theoretical-Large-Language-Model-Enhanced-Information-Operations"><a href="#ClausewitzGPT-Framework-A-New-Frontier-in-Theoretical-Large-Language-Model-Enhanced-Information-Operations" class="headerlink" title="ClausewitzGPT Framework: A New Frontier in Theoretical Large Language Model Enhanced Information Operations"></a>ClausewitzGPT Framework: A New Frontier in Theoretical Large Language Model Enhanced Information Operations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07099">http://arxiv.org/abs/2310.07099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Kereopa-Yorke</li>
<li>for: This paper aims to provide a framework for navigating the risks and challenges of Large Language Models (LLMs) and autonomous AI agents in the context of information operations.</li>
<li>methods: The paper uses a novel formulation called the “ClausewitzGPT” equation to quantify the risks of LLM-augmented operations and emphasizes the importance of ethical considerations and autonomous AI agents in ensuring a moral compass and societal imperatives.</li>
<li>results: The paper highlights the staggering year-on-year growth of AI information campaigns and emphasizes the urgency of addressing the challenges and risks of LLMs and autonomous AI agents in the context of information operations.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文目标是为了提供LLM和自动化AI代理在信息操作中 navigating 技术飞速带来的风险和挑战的框架。</li>
<li>methods: 论文使用一种新的方法，称为“ClausewitzGPT”方程，以量化LLM增强操作的风险，并强调了伦理考虑和自动化AI代理在保持道德 компас和社会要求方面的重要性。</li>
<li>results: 论文强调了年度增长的AI信息运动，并强调了现在的杰uncje点，需要我们通过加强LLM和自动化AI代理的风险和挑战来应对。<details>
<summary>Abstract</summary>
In a digital epoch where cyberspace is the emerging nexus of geopolitical contention, the melding of information operations and Large Language Models (LLMs) heralds a paradigm shift, replete with immense opportunities and intricate challenges. As tools like the Mistral 7B LLM (Mistral, 2023) democratise access to LLM capabilities (Jin et al., 2023), a vast spectrum of actors, from sovereign nations to rogue entities (Howard et al., 2023), find themselves equipped with potent narrative-shaping instruments (Goldstein et al., 2023). This paper puts forth a framework for navigating this brave new world in the "ClausewitzGPT" equation. This novel formulation not only seeks to quantify the risks inherent in machine-speed LLM-augmented operations but also underscores the vital role of autonomous AI agents (Wang, Xie, et al., 2023). These agents, embodying ethical considerations (Hendrycks et al., 2021), emerge as indispensable components (Wang, Ma, et al., 2023), ensuring that as we race forward, we do not lose sight of moral compasses and societal imperatives.   Mathematically underpinned and inspired by the timeless tenets of Clausewitz's military strategy (Clausewitz, 1832), this thesis delves into the intricate dynamics of AI-augmented information operations. With references to recent findings and research (Department of State, 2023), it highlights the staggering year-on-year growth of AI information campaigns (Evgeny Pashentsev, 2023), stressing the urgency of our current juncture. The synthesis of Enlightenment thinking, and Clausewitz's principles provides a foundational lens, emphasising the imperative of clear strategic vision, ethical considerations, and holistic understanding in the face of rapid technological advancement.
</details>
<details>
<summary>摘要</summary>
在数字时代，虚拟空间成为地opolitical竞争的emerging nexus，information操作和大型自然语言模型（LLM）的融合标志着一种新的 paradigm shift，具有巨大的机遇和复杂的挑战。Tools like Mistral 7B LLM（Mistral，2023）通过 démocratising access to LLM capabilities（Jin et al., 2023），让各种actor，从主权国家到黑帮（Howard et al., 2023），拥有高效的叙述形成工具。这篇论文提出了一种用于 navigate这个勇敢的新世界的“ClausewitzGPT”方程。这种新的方程不仅试图量化机器速度下LLM-加速的风险，而且强调了自主AI代理（Wang, Xie, et al., 2023）的重要性。这些代理，具有伦理考虑（Hendrycks et al., 2021），在我们前进的过程中变得不可或缺。这篇论文受数学基础和Clausewitz的 воен略思想（Clausewitz, 1832）的激发，探讨了人工智能加速的信息操作动态。通过参考最新的发现和研究（Department of State, 2023），这篇论文强调了艺术智能信息活动的年度增长（Evgeny Pashentsev, 2023），强调当前的战略危机性。通过融合Enlightenment思想和Clausewitz的原则，这篇论文提供了一种基本的镜像，强调了在快速技术进步的面前，我们必须具备清晰的战略视野、伦理考虑和整体理解。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Universal-Transformer"><a href="#Sparse-Universal-Transformer" class="headerlink" title="Sparse Universal Transformer"></a>Sparse Universal Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07096">http://arxiv.org/abs/2310.07096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shawntan/SUT">https://github.com/shawntan/SUT</a></li>
<li>paper_authors: Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron Courville, Chuang Gan</li>
<li>for: 本研究旨在提出一种叫做 sparse universal transformer（SUT），用于提高 universal transformer（UT）的计算复杂度和参数效率，同时保持其基本特性和泛化能力。</li>
<li>methods: 本研究使用了 sparse mixture of experts（SMoE）和一种新的棒拌分解方式来降低 UT 的计算复杂度，并且提出了一种新的停止机制来控制计算复杂度。</li>
<li>results: 实验表明，SUT 可以与强基线模型相当的性能，仅使用了半个计算和参数量，并且在正式语言任务（Logical inference和CFQ）上显示了强大的泛化能力。此外，新的停止机制还可以在推理过程中减少计算量约 50%，而无须减少性能。<details>
<summary>Abstract</summary>
The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\% reduction in computation during inference with very little performance decrease on formal language tasks.
</details>
<details>
<summary>摘要</summary>
《 universal transformer（UT）是一种变体的transformer，它在层之间共享参数。实验证明，UT在正式语言任务上有更好的compositional generalizationthan Vanilla Transformers（VT）。此外，UT的参数共享还使其的参数使用效率更高than VT。尽管它有许多优点，但是扩展UT参数的计算复杂度会比VT的计算复杂度更高。这篇论文提出了Sparse Universal Transformer（SUT），它利用Sparse Mixture of Experts（SMoE）和一种基于扔投的新动态停止机制来降低UT的计算复杂度，保持UT的参数效率和泛化能力。实验表明，SUT可以与强基eline模型相当的性能，仅使用半个计算和参数来处理WMT'14和正式语言任务（逻辑推理和CFQ）。新的停止机制还可以在推理过程中降低计算量约50%，而无需减少正式语言任务中的性能。
</details></li>
</ul>
<hr>
<h2 id="Jaeger-A-Concatenation-Based-Multi-Transformer-VQA-Model"><a href="#Jaeger-A-Concatenation-Based-Multi-Transformer-VQA-Model" class="headerlink" title="Jaeger: A Concatenation-Based Multi-Transformer VQA Model"></a>Jaeger: A Concatenation-Based Multi-Transformer VQA Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07091">http://arxiv.org/abs/2310.07091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jieting Long, Zewei Shi, Penghao Jiang, Yidong Gan</li>
<li>for: 提高文档视觉问答的表现，增强语义含义的准确性和细化的multimodal检索。</li>
<li>methods: 利用大语言和开放世界的先前模型，如RoBERTa大型和GPT2-xl，作为特征提取器，并对其输出进行 concatenation 操作，以实现多源信息的同时考虑。</li>
<li>results: 在 Task C 的 PDF-VQA 数据集上实现竞争性表现。<details>
<summary>Abstract</summary>
Document-based Visual Question Answering poses a challenging task between linguistic sense disambiguation and fine-grained multimodal retrieval. Although there has been encouraging progress in document-based question answering due to the utilization of large language and open-world prior models\cite{1}, several challenges persist, including prolonged response times, extended inference durations, and imprecision in matching. In order to overcome these challenges, we propose Jaegar, a concatenation-based multi-transformer VQA model. To derive question features, we leverage the exceptional capabilities of RoBERTa large\cite{2} and GPT2-xl\cite{3} as feature extractors. Subsequently, we subject the outputs from both models to a concatenation process. This operation allows the model to consider information from diverse sources concurrently, strengthening its representational capability. By leveraging pre-trained models for feature extraction, our approach has the potential to amplify the performance of these models through concatenation. After concatenation, we apply dimensionality reduction to the output features, reducing the model's computational effectiveness and inference time. Empirical results demonstrate that our proposed model achieves competitive performance on Task C of the PDF-VQA Dataset. If the user adds any new data, they should make sure to style it as per the instructions provided in previous sections.
</details>
<details>
<summary>摘要</summary>
文档视觉问答 зада题存在语义含义涉及和细腻多媒体检索的挑战。虽然因使用大语言和开放世界先进模型\cite{1}而取得了鼓舞人的进步，但还存在许多挑战，包括长时间响应、延长推理时间和匹配不准确。为了解决这些挑战，我们提议Jaegar，一种 concatenation-based 多变换 VQA 模型。为了 derivate 问题特征，我们利用 RoBERTa 大\cite{2} 和 GPT2-xl\cite{3} 作为特征提取器。然后，我们将两个模型的输出经 concatenation 操作，以便同时考虑不同来源的信息，提高模型的表达能力。通过利用预训练模型来提取特征，我们的方法可能会强化这些模型的表现。然后，我们对输出特征进行维度缩放，以降低模型的计算效率和推理时间。实验结果表明，我们提议的模型在 PDF-VQA 数据集的 Task C 上达到了竞争性的性能。如果用户添加任何新数据，他们应该按照以前提供的指导方针进行风格化处理。
</details></li>
</ul>
<hr>
<h2 id="Diversity-of-Thought-Improves-Reasoning-Abilities-of-Large-Language-Models"><a href="#Diversity-of-Thought-Improves-Reasoning-Abilities-of-Large-Language-Models" class="headerlink" title="Diversity of Thought Improves Reasoning Abilities of Large Language Models"></a>Diversity of Thought Improves Reasoning Abilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07088">http://arxiv.org/abs/2310.07088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ranjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi, Besmira Nushi</li>
<li>for: 提高 LLM 在需要复杂逻辑的设置中的表现</li>
<li>methods: 使用多种生成和修改解码步骤来 ensemble 多个生成，以提高模型表现</li>
<li>results: 在固定生成预算下，DIV-SE 和 IDIV-SE 在多个逻辑测试上表现出色，超过了之前的基线值，而且在最Difficult Blocksworld任务上达到了最高的29.6%的提升率。<details>
<summary>Abstract</summary>
Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps (Wei et al., 2022), or ensembling various generations through modifying decoding steps (Wang et al., 2023) boosts performance. Current methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we relax this assumption and discuss how one can create and leverage variations of the input prompt as a means to diversity of thought to improve model performance. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that fit for the problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse reasoning path Self-Ensemble) across multiple inference calls. We also propose a cost-effective alternative where diverse prompts are used within a single inference call; we call this IDIV-SE (In-call DIVerse reasoning path Self-Ensemble). Under a fixed generation budget, DIV-SE and IDIV-SE outperform the previously discussed baselines using both GPT-3.5 and GPT-4 on several reasoning benchmarks, without modifying the decoding process. Additionally, DIV-SE advances state-of-the-art performance on recent planning benchmarks (Valmeekam et al., 2023), exceeding the highest previously reported accuracy by at least 29.6 percentage points on the most challenging 4/5 Blocksworld task. Our results shed light on how to enforce prompt diversity toward LLM reasoning and thereby improve the pareto frontier of the accuracy-cost trade-off.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在需要复杂推理的设定下 frequently struggle （Wei et al., 2022）。然而，将问题拆分成小推理步骤（Wei et al., 2022）或者聚合不同代表（Wang et al., 2023）可以提高表现。现有的方法假设输入提示是固定的，并且预期解码策略可以引入充分的多样性。在这个工作中，我们实际松动这个假设，并讨论如何将输入提示的多样性引入到模型中，以提高模型表现。我们提出了一个方法，可以自动提高提示多样性，通过从LLM获取反馈，以便发展适合问题的方法。我们然后聚合这些多样的提示，在多个推理步骤中进行ensemble。我们还提出了一个成本效益高的代替方案，使用多个不同的提示在单一的推理步骤中进行ensemble。在固定的生成预算下，DIV-SE和IDIV-SE在多个推理benchmark上表现出色，不需要修改解码过程。此外，DIV-SE在最近的规划benchmark上进一步提高了州时的表现，至少比前一代最高的accuracy报告提高29.6个百分点。我们的结果 shed light on how to enforce提示多样性在LLM推理中，并因此提高精度-成本费用的 pareto frontier。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Twitter-Data-for-Sentiment-Analysis-of-Transit-User-Feedback-An-NLP-Framework"><a href="#Leveraging-Twitter-Data-for-Sentiment-Analysis-of-Transit-User-Feedback-An-NLP-Framework" class="headerlink" title="Leveraging Twitter Data for Sentiment Analysis of Transit User Feedback: An NLP Framework"></a>Leveraging Twitter Data for Sentiment Analysis of Transit User Feedback: An NLP Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07086">http://arxiv.org/abs/2310.07086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adway Das, Abhishek Kumar Prajapati, Pengxiang Zhang, Mukund Srinath, Andisheh Ranjbari</li>
<li>for: 本研究は、便宜なソーシャルメディアデータを利用して、ユーザーフィードバックを収集するための新しいNLP基盘フレームワークを提案しています。</li>
<li>methods: このフレームワークでは、ツイートをクラスタリングするためのフィーウェイ学习を使用し、ツイートの感情を评価するためのレキシコンベースの感情分析モデルを适用します。</li>
<li>results: この研究では、ニューヨーク市の地下铁システムを例として、このフレームワークを适用して、ユーザーの感想を捉えることができました。结果は、安全性、信赖性、および保守の3つのカテゴリーに分类されたツイートを正确に识别することができました。また、感情の强さと方向性を评価することができました。この结果は、便宜なソーシャルメディアデータを使用してユーザーフィードバックを収集することが有效であることを证明しています。<details>
<summary>Abstract</summary>
Traditional methods of collecting user feedback through transit surveys are often time-consuming, resource intensive, and costly. In this paper, we propose a novel NLP-based framework that harnesses the vast, abundant, and inexpensive data available on social media platforms like Twitter to understand users' perceptions of various service issues. Twitter, being a microblogging platform, hosts a wealth of real-time user-generated content that often includes valuable feedback and opinions on various products, services, and experiences. The proposed framework streamlines the process of gathering and analyzing user feedback without the need for costly and time-consuming user feedback surveys using two techniques. First, it utilizes few-shot learning for tweet classification within predefined categories, allowing effective identification of the issues described in tweets. It then employs a lexicon-based sentiment analysis model to assess the intensity and polarity of the tweet sentiments, distinguishing between positive, negative, and neutral tweets. The effectiveness of the framework was validated on a subset of manually labeled Twitter data and was applied to the NYC subway system as a case study. The framework accurately classifies tweets into predefined categories related to safety, reliability, and maintenance of the subway system and effectively measured sentiment intensities within each category. The general findings were corroborated through a comparison with an agency-run customer survey conducted in the same year. The findings highlight the effectiveness of the proposed framework in gauging user feedback through inexpensive social media data to understand the pain points of the transit system and plan for targeted improvements.
</details>
<details>
<summary>摘要</summary>
传统的公共交通User feedback收集方法经常是时间consuming、资源占用和成本高的。在这篇论文中，我们提出了一种基于自然语言处理（NLP）的框架，利用社交媒体平台 like Twitter 上的丰富、便宜的用户生成内容来了解用户对不同服务问题的看法。Twitter 是一个 Microblogging 平台，它上有大量的实时用户生成内容，这些内容经常包含有价值的反馈和意见。我们的框架可以快速地收集和分析用户反馈，不需要费时和费力的用户反馈调查。我们使用了 few-shot learning 来分类 tweet，并使用 sentiment analysis 模型来评估 tweet 的情感 INTENSITY和方向。我们验证了该框架的有效性，并将其应用到纽约市地铁系统作为案例研究。结果表明，框架可以有效地将 tweet 分类到预定义的安全、可靠性和维护等类别中，并准确地评估每个类别中的情感 INTENSITY。我们的发现得到了公共交通机构所运行的客户调查的支持，这些发现反映了该框架在估计用户反馈的有效性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/11/cs.AI_2023_10_11/" data-id="clorjzl2o005df188gpscbllb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/20/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><a class="page-number" href="/page/20/">20</a><span class="page-number current">21</span><a class="page-number" href="/page/22/">22</a><a class="page-number" href="/page/23/">23</a><span class="space">&hellip;</span><a class="page-number" href="/page/89/">89</a><a class="extend next" rel="next" href="/page/22/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
