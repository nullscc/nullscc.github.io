
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/21/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/cs.SD_2023_07_09/" class="article-date">
  <time datetime="2023-07-08T16:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/09/cs.SD_2023_07_09/">cs.SD - 2023-07-09 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Can-Generative-Large-Language-Models-Perform-ASR-Error-Correction"><a href="#Can-Generative-Large-Language-Models-Perform-ASR-Error-Correction" class="headerlink" title="Can Generative Large Language Models Perform ASR Error Correction?"></a>Can Generative Large Language Models Perform ASR Error Correction?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04172">http://arxiv.org/abs/2307.04172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rao Ma, Mengjie Qian, Potsawee Manakul, Mark Gales, Kate Knill</li>
<li>for: æé«˜ ASR ç³»ç»Ÿçš„è¡¨ç°ï¼Œä½¿å…¶æ›´åŠ å‡†ç¡®å’Œæœ‰æ•ˆã€‚</li>
<li>methods: ä½¿ç”¨ ChatGPT å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé›¶æ¬¡æˆ–ä¸€æ¬¡å­¦ä¹ ï¼Œå¯¹ ASR N-best åˆ—è¡¨è¿›è¡Œé”™è¯¯ä¿®æ­£ã€‚</li>
<li>results: å¯¹ Conformer-Transducer æ¨¡å‹å’Œé¢„è®­ç»ƒçš„ Whisper æ¨¡å‹è¿›è¡Œé”™è¯¯ä¿®æ­£ï¼Œå¯ä»¥å¤§å¹…æé«˜ ASR ç³»ç»Ÿçš„è¡¨ç°ã€‚I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
ASR error correction continues to serve as an important part of post-processing for speech recognition systems. Traditionally, these models are trained with supervised training using the decoding results of the underlying ASR system and the reference text. This approach is computationally intensive and the model needs to be re-trained when switching the underlying ASR model. Recent years have seen the development of large language models and their ability to perform natural language processing tasks in a zero-shot manner. In this paper, we take ChatGPT as an example to examine its ability to perform ASR error correction in the zero-shot or 1-shot settings. We use the ASR N-best list as model input and propose unconstrained error correction and N-best constrained error correction methods. Results on a Conformer-Transducer model and the pre-trained Whisper model show that we can largely improve the ASR system performance with error correction using the powerful ChatGPT model.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>è½¬æ¢æ–‡æœ¬ä¸ºç®€åŒ–ä¸­æ–‡ã€‚<</SYS>>ASRé”™è¯¯ä¿®æ­£ä»ç„¶serve asé‡è¦çš„åå¤„ç†æ­¥éª¤ Ğ´Ğ»Ñè¯­éŸ³è¯†åˆ«ç³»ç»Ÿã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™äº›æ¨¡å‹é€šè¿‡æŒ‡å¯¼å­¦ä¹ ä½¿ç”¨ä¸‹é¢ASRç³»ç»Ÿçš„è§£ç ç»“æœå’Œå‚è€ƒæ–‡æœ¬è¿›è¡Œè®­ç»ƒã€‚è¿™ç§æ–¹æ³•æ˜¯è®¡ç®—æœºç¨‹åºæ˜‚è´µï¼Œæ¨¡å‹éœ€è¦åœ¨Switching beneath ASRæ¨¡å‹æ—¶é‡æ–°è®­ç»ƒã€‚ recent years have seen the development of large language models and their ability to perform natural language processing tasks in a zero-shot manner. In this paper, we take ChatGPT as an example to examine its ability to perform ASR error correction in the zero-shot or 1-shot settings. We use the ASR N-best list as model input and propose unconstrained error correction and N-best constrained error correction methods. Results on a Conformer-Transducer model and the pre-trained Whisper model show that we can largely improve the ASR system performance with error correction using the powerful ChatGPT model.Note: The translation is done using Google Translate, which may not be perfect, but it should give you a good idea of the content in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Emotion-Guided-Music-Accompaniment-Generation-Based-on-Variational-Autoencoder"><a href="#Emotion-Guided-Music-Accompaniment-Generation-Based-on-Variational-Autoencoder" class="headerlink" title="Emotion-Guided Music Accompaniment Generation Based on Variational Autoencoder"></a>Emotion-Guided Music Accompaniment Generation Based on Variational Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04015">http://arxiv.org/abs/2307.04015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/duoluoluos/emotion-guided-music-accompaniment-generation">https://github.com/duoluoluos/emotion-guided-music-accompaniment-generation</a></li>
<li>paper_authors: Qi Wang, Shubing Zhang, Li Zhou</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½åœ¨éŸ³ä¹åˆ›ä½œè¿‡ç¨‹ä¸­å›°éš¾åœ° integrate äººç±»æƒ…æ„Ÿåˆ›ä½œç¾å¦™çš„ä¼´å¥ä¹æ›²ã€‚</li>
<li>methods: æˆ‘ä»¬æè®®ä½¿ç”¨ä¸€ç§æ˜“äºè¡¨ç¤ºæƒ…æ„Ÿæµç¨‹çš„æ›²çº¿æ¨¡å‹ï¼Œå³Valence&#x2F;Arousal Curveï¼Œä»¥å®ç°æ¨¡å‹å†…éƒ¨æƒ…æ„Ÿä¿¡æ¯çš„å…¼å®¹ï¼Œå¹¶ä½¿ç”¨å˜é‡è‡ªåŠ¨ç¼–ç å™¨ä½œä¸ºæ¨¡å‹ç»“æ„ä»¥æé«˜æƒ…æ„Ÿå› ç´ çš„è§£é‡Šæ€§ã€‚ è€ŒRelative Self-Attention æŠ€æœ¯ä¹Ÿç”¨äºä¿æŒéŸ³ä¹å¥å­æ°´å¹³ç»“æ„å’Œç”Ÿæˆæ›´åŠ ä¸°å¯Œçš„ä¼´å¥ä¹æ›²ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å¢å¼ºéŸ³ä¹åˆ›ä½œè¿‡ç¨‹ä¸­ AI çš„æƒ…æ„Ÿåˆ›ä½œèƒ½åŠ›ï¼Œå¹¶ç”Ÿæˆæ›´åŠ ç¾å¦™çš„ä¼´å¥ä¹æ›²ã€‚<details>
<summary>Abstract</summary>
Music accompaniment generation is a crucial aspect in the composition process. Deep neural networks have made significant strides in this field, but it remains a challenge for AI to effectively incorporate human emotions to create beautiful accompaniments. Existing models struggle to effectively characterize human emotions within neural network models while composing music. To address this issue, we propose the use of an easy-to-represent emotion flow model, the Valence/Arousal Curve, which allows for the compatibility of emotional information within the model through data transformation and enhances interpretability of emotional factors by utilizing a Variational Autoencoder as the model structure. Further, we used relative self-attention to maintain the structure of the music at music phrase level and to generate a richer accompaniment when combined with the rules of music theory.
</details>
<details>
<summary>æ‘˜è¦</summary>
éŸ³ä¹ä¼´å¥ç”Ÿæˆæ˜¯ä½œæ›²è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªé‡è¦æ–¹é¢ã€‚æ·±åº¦ç¥ç»ç½‘ç»œåœ¨è¿™ä¸ªé¢†åŸŸå·²ç»åšå‡ºäº†å¾ˆå¤§çš„è¿›æ­¥ï¼Œä½†æ˜¯AIè¿˜æœªèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¶µç›–äººç±»æƒ…æ„Ÿä»¥åˆ›é€ ç¾å¦™çš„ä¼´å¥ã€‚ç°æœ‰çš„æ¨¡å‹å¾ˆéš¾å‡†ç¡®åœ°æ•æ‰äººç±»æƒ…æ„Ÿä¿¡æ¯åœ¨ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œè€Œä¸”é€šå¸¸ä¼šå¯¼è‡´æ¨¡å‹éš¾ä»¥ç†è§£å’Œæè¿°æƒ…æ„Ÿå› ç´ ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨ä¸€ç§å®¹æ˜“è¡¨è¾¾æƒ…æ„Ÿæµç¨‹çš„æ›²çº¿æ¨¡å‹ï¼Œå³æƒ…æ„Ÿå€¼/åˆºæ¿€æ›²çº¿ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ•°æ®è½¬æ¢æ¥å…¼å®¹æƒ…æ„Ÿä¿¡æ¯ï¼Œå¹¶é€šè¿‡ä½¿ç”¨å˜é‡è‡ªåŠ¨ç¼–ç å™¨ç»“æ„æ¥æé«˜æƒ…æ„Ÿå› ç´ çš„è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸å¯¹è‡ªæ³¨æ„åŠ›æ¥ä¿æŒéŸ³ä¹æ®µçº§ç»“æ„å’Œç”Ÿæˆæ›´åŠ ä¸°å¯Œçš„ä¼´å¥ï¼Œå¹¶ä¸éŸ³ä¹ç†è®ºè§„åˆ™ç›¸ç»“åˆã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/cs.SD_2023_07_09/" data-id="clltaagom0077r8889rshbg06" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/eess.AS_2023_07_09/" class="article-date">
  <time datetime="2023-07-08T16:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/09/eess.AS_2023_07_09/">eess.AS - 2023-07-09 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="IANS-Intelligibility-aware-Null-steering-Beamforming-for-Dual-Microphone-Arrays"><a href="#IANS-Intelligibility-aware-Null-steering-Beamforming-for-Dual-Microphone-Arrays" class="headerlink" title="IANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays"></a>IANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04179">http://arxiv.org/abs/2307.04179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen-Yuan Ting, Syu-Siang Wang, Yu Tsao, Borching Su</li>
<li>for: æé«˜æ‚éŸ³å¹²æ‰° speech çš„ intelligibility</li>
<li>methods: ä½¿ç”¨ STOI-Net æ™ºèƒ½é¢„æµ‹æ¨¡å‹ï¼Œå¹¶ç»“åˆ null-steering  beamformer (NSBF) ç”Ÿæˆä¸€ç³»åˆ— beamformed è¾“å‡ºï¼Œä»¥æé«˜ speech çš„ intelligibility</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒIANS å¯ä»¥ä½¿ç”¨å°å‹åŒéº¦å…‹éº¦array ç”Ÿæˆæœ‰ inteligibility æé«˜çš„ signalï¼Œä¸ known DOAs çš„ null-steering beamformers çš„ç»“æœç›¸ä¼¼<details>
<summary>Abstract</summary>
Beamforming techniques are popular in speech-related applications due to their effective spatial filtering capabilities. Nonetheless, conventional beamforming techniques generally depend heavily on either the target's direction-of-arrival (DOA), relative transfer function (RTF) or covariance matrix. This paper presents a new approach, the intelligibility-aware null-steering (IANS) beamforming framework, which uses the STOI-Net intelligibility prediction model to improve speech intelligibility without prior knowledge of the speech signal parameters mentioned earlier. The IANS framework combines a null-steering beamformer (NSBF) to generate a set of beamformed outputs, and STOI-Net, to determine the optimal result. Experimental results indicate that IANS can produce intelligibility-enhanced signals using a small dual-microphone array. The results are comparable to those obtained by null-steering beamformers with given knowledge of DOAs.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>> simultrare il testo in Cinese semplificato.<</SYS>>Beamforming æŠ€æœ¯åœ¨è¯­éŸ³ç›¸å…³åº”ç”¨ä¸­å¾ˆå—æ¬¢è¿ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æä¾›æœ‰æ•ˆçš„ç©ºé—´æ»¤æ³¢åŠŸèƒ½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ beamforming æŠ€æœ¯é€šå¸¸å…·æœ‰ä¾èµ–äºç›®æ ‡çš„æ–¹å‘ arrival (DOA)ã€ç›¸å¯¹è½¬ç§»å‡½æ•° (RTF) æˆ– covariance matrix çš„ç¼ºç‚¹ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æ™ºèƒ½å¯è¯†åˆ« beamforming æ¡†æ¶ (IANS)ï¼Œå®ƒä½¿ç”¨ STOI-Net æ™ºèƒ½å¯è¯†åˆ«æ¨¡å‹æ¥æé«˜è¯­éŸ³å¯è¯†åˆ«æ€§ï¼Œæ— éœ€å…ˆçŸ¥é“è¯­éŸ³ä¿¡å·å‚æ•°ã€‚IANS æ¡†æ¶å°† null-steering beamformer (NSBF) ä¸ STOI-Net ç»“åˆï¼Œç”Ÿæˆä¸€ç»„æ‰©å±•å‡ºçš„è¾“å‡ºï¼Œå¹¶ä½¿ç”¨ STOI-Net ç¡®å®šæœ€ä½³ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIANS å¯ä»¥ä½¿ç”¨å°å‹åŒ Ğ¼Ğ¸ĞºÑ€Ğ¾Ñ„Ğ¾Ğ½é˜µåˆ—ç”Ÿæˆå…·æœ‰å¯è¯†åˆ«æ€§çš„ä¿¡å·ï¼Œä¸ null-steering beamformers çš„ç»“æœç›¸ä¼¼ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/eess.AS_2023_07_09/" data-id="clltaagpg009zr888h0s07am3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/09/eess.IV_2023_07_09/" class="article-date">
  <time datetime="2023-07-08T16:00:00.000Z" itemprop="datePublished">2023-07-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/09/eess.IV_2023_07_09/">eess.IV - 2023-07-09 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Ultrasonic-Imageâ€™s-Annotation-Removal-A-Self-supervised-Noise2Noise-Approach"><a href="#Ultrasonic-Imageâ€™s-Annotation-Removal-A-Self-supervised-Noise2Noise-Approach" class="headerlink" title="Ultrasonic Imageâ€™s Annotation Removal: A Self-supervised Noise2Noise Approach"></a>Ultrasonic Imageâ€™s Annotation Removal: A Self-supervised Noise2Noise Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04133">http://arxiv.org/abs/2307.04133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grandarth/ultrasonicimage-n2n-approach">https://github.com/grandarth/ultrasonicimage-n2n-approach</a></li>
<li>paper_authors: Yuanheng Zhang, Nan Jiang, Zhaoheng Xie, Junying Cao, Yueyang Teng</li>
<li>for: æé«˜åŒ»ç–—æŠ¥å‘Šè´¨é‡çš„é«˜çº§åŒ»ç–—å›¾åƒæ ‡æ³¨æ•°æ®é›†çš„è‡ªåŠ¨åŒ–å¤„ç†ã€‚</li>
<li>methods: ä½¿ç”¨å™ªå£°ä½œä¸ºé¢„ Text taskï¼Œä½¿ç”¨Noise2Noise schemeè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä»¥æ¢å¤å›¾åƒåˆ°å¹²å‡€çŠ¶æ€ã€‚</li>
<li>results: å¯¹ä¸åŒç±»å‹çš„æ ‡æ³¨æ•°æ®è¿›è¡Œæµ‹è¯•ï¼Œå¤§å¤šæ•°åŸºäºNoise2Noise schemeçš„æ¨¡å‹åœ¨å™ªå£°æ¢å¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨è‡ªå®šä¹‰U-Netç»“æ„åœ¨Body markeræ ‡æ³¨æ•°æ®é›†ä¸Šå¾—åˆ°äº†æœ€ä½³æ•ˆæœï¼Œå…·æœ‰é«˜ç²¾åº¦å’Œé«˜é‡å»ºç›¸ä¼¼æ€§ã€‚<details>
<summary>Abstract</summary>
Accurately annotated ultrasonic images are vital components of a high-quality medical report. Hospitals often have strict guidelines on the types of annotations that should appear on imaging results. However, manually inspecting these images can be a cumbersome task. While a neural network could potentially automate the process, training such a model typically requires a dataset of paired input and target images, which in turn involves significant human labour. This study introduces an automated approach for detecting annotations in images. This is achieved by treating the annotations as noise, creating a self-supervised pretext task and using a model trained under the Noise2Noise scheme to restore the image to a clean state. We tested a variety of model structures on the denoising task against different types of annotation, including body marker annotation, radial line annotation, etc. Our results demonstrate that most models trained under the Noise2Noise scheme outperformed their counterparts trained with noisy-clean data pairs. The costumed U-Net yielded the most optimal outcome on the body marker annotation dataset, with high scores on segmentation precision and reconstruction similarity. We released our code at https://github.com/GrandArth/UltrasonicImage-N2N-Approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
é«˜å“è´¨åŒ»ç–—æŠ¥å‘Šä¸­çš„å‡†ç¡®æ ‡æ³¨å›¾åƒæ˜¯å…³é”®ç»„æˆéƒ¨åˆ†ã€‚åŒ»é™¢é€šå¸¸æœ‰ä¸¥æ ¼çš„å›¾åƒæ ‡æ³¨è§„èŒƒï¼Œä½†æ‰‹åŠ¨æ£€æŸ¥è¿™äº›å›¾åƒå¯ä»¥æ˜¯ä¸€é¡¹ç¹ççš„ä»»åŠ¡ã€‚ä½¿ç”¨ç¥ç»ç½‘ç»œè‡ªåŠ¨åŒ–è¿™ä¸ªè¿‡ç¨‹å¯èƒ½æ˜¯ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼Œä½†æ˜¯è®­ç»ƒè¿™ç§æ¨¡å‹é€šå¸¸éœ€è¦ä¸€ä¸ªåŒ…å«è¾“å…¥å’Œç›®æ ‡å›¾åƒçš„ paired æ•°æ®é›†ï¼Œè¿™éœ€è¦å¾ˆå¤šäººå·¥åŠ³åŠ¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–å›¾åƒæ ‡æ³¨æ£€æµ‹æ–¹æ³•ã€‚è¿™æ˜¯é€šè¿‡å°†æ ‡æ³¨è§†ä¸ºå™ªå£°ï¼Œåˆ›å»ºä¸€ç§è‡ªæˆ‘è¶…vised pretext taskï¼Œå¹¶ä½¿ç”¨åŸºäº Noise2Noise æ–¹æ¡ˆè®­ç»ƒçš„æ¨¡å‹æ¥è¿˜åŸå›¾åƒä¸ºä¸€ä¸ªæ¸…æ™°çš„çŠ¶æ€æ¥å®ç°çš„ã€‚æˆ‘ä»¬æµ‹è¯•äº†ä¸åŒç±»å‹çš„æ ‡æ³¨ï¼ŒåŒ…æ‹¬ä½“ markers æ ‡æ³¨å’Œå¾„å‘çº¿æ ‡æ³¨ç­‰ï¼Œå¹¶è¯„ä¼°äº†ä¸åŒæ¨¡å‹ç»“æ„çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°åŸºäº Noise2Noise æ–¹æ¡ˆè®­ç»ƒçš„æ¨¡å‹åœ¨æ‚å™ªclean æ•°æ®å¯¹æ¯”ä¸‹è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”ç”¨äºä½“ markers æ ‡æ³¨é›†ä¸Šçš„ customized U-Net å¾—åˆ°äº†æœ€ä½³çš„ç»“æœï¼Œå…¶segmentationç²¾åº¦å’Œé‡å»ºç›¸ä¼¼åº¦å‡è¾¾åˆ°äº†é«˜æ°´å¹³ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨ <https://github.com/GrandArth/UltrasonicImage-N2N-Approach> ä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Low-Light-Images-Using-Infrared-Encoded-Images"><a href="#Enhancing-Low-Light-Images-Using-Infrared-Encoded-Images" class="headerlink" title="Enhancing Low-Light Images Using Infrared-Encoded Images"></a>Enhancing Low-Light Images Using Infrared-Encoded Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04122">http://arxiv.org/abs/2307.04122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyf0912/ELIEI">https://github.com/wyf0912/ELIEI</a></li>
<li>paper_authors: Shulin Tian, Yufei Wang, Renjie Wan, Wenhan Yang, Alex C. Kot, Bihan Wen</li>
<li>for: æé«˜ä½å…‰ç…§å›¾åƒçš„å¯è§åº¦å’Œç»†èŠ‚è¡¨ç¤º</li>
<li>methods: ç§»é™¤å¡å£å†…çš„æŠ˜å°„å…‰é•œFilterï¼Œä½¿ç”¨æ›´å¤šçš„ç…§æ˜ä¿¡æ¯ä»è¿‘infraredpectrumä¸­è·å–æ›´é«˜çš„ä¿¡å™ªæ¯”</li>
<li>results: å¯¹æ¯” referencetest datasetï¼Œæå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æé«˜ä½å…‰ç…§å›¾åƒçš„å¯è§åº¦å’Œç»†èŠ‚è¡¨ç¤ºï¼Œå¹¶ä¸”é‡åŒ–å’Œ ĞºĞ°Ñ‡Ğµåœ°æ¯”è¾ƒå¥½<details>
<summary>Abstract</summary>
Low-light image enhancement task is essential yet challenging as it is ill-posed intrinsically. Previous arts mainly focus on the low-light images captured in the visible spectrum using pixel-wise loss, which limits the capacity of recovering the brightness, contrast, and texture details due to the small number of income photons. In this work, we propose a novel approach to increase the visibility of images captured under low-light environments by removing the in-camera infrared (IR) cut-off filter, which allows for the capture of more photons and results in improved signal-to-noise ratio due to the inclusion of information from the IR spectrum. To verify the proposed strategy, we collect a paired dataset of low-light images captured without the IR cut-off filter, with corresponding long-exposure reference images with an external filter. The experimental results on the proposed dataset demonstrate the effectiveness of the proposed method, showing better performance quantitatively and qualitatively. The dataset and code are publicly available at https://wyf0912.github.io/ELIEI/
</details>
<details>
<summary>æ‘˜è¦</summary>
ä½å…‰ç…§å›¾åƒæå‡ä»»åŠ¡æ˜¯å¿…å¤‡åˆæŒ‘æˆ˜çš„ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå†…åœ¨ä¸å®šçš„é—®é¢˜ã€‚å…ˆå‰çš„è‰ºæœ¯ä¸»è¦é€šè¿‡åƒç´ ç²¾åº¦æŸå¤±æ¥å¤„ç†ä½å…‰ç…§å›¾åƒï¼Œè¿™é™åˆ¶äº†æ¢å¤å›¾åƒçš„äº®åº¦ã€å¯¹æ¯”åº¦å’Œç»†èŠ‚çš„èƒ½åŠ›ï¼Œå› ä¸ºå¯å¾—åˆ°çš„å…‰å­æ•°å¾ˆå°‘ã€‚åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬æè®®ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨å–æ¶ˆç›¸æœºå†…ç½®çš„çº¢å¤–ï¼ˆIRï¼‰å‰”é™¤filterï¼Œä»¥è·å–æ›´å¤šçš„å…‰å­æ•°ï¼Œä»è€Œæé«˜ä¿¡å·å™ªå£°æ¯”ã€‚ä¸ºéªŒè¯æè®®çš„ç­–ç•¥ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªå¯¹åº”çš„ä½å…‰ç…§å›¾åƒé›†å’Œé•¿æ›å…‰å‚ç…§å›¾åƒé›†ï¼Œä½¿ç”¨å¤–éƒ¨æ»¤é•œè·å–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæè®®çš„æ–¹æ³•å…·æœ‰æ›´å¥½çš„æ•°é‡å’Œè´¨é‡æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç åœ¨https://wyf0912.github.io/ELIEI/ä¸Šå…¬å¼€ã€‚
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Building-Semantic-Segmentation-Accuracy-with-Super-Resolution-and-Deep-Learning-Investigating-the-Impact-of-Spatial-Resolution-on-Various-Datasets"><a href="#Enhancing-Building-Semantic-Segmentation-Accuracy-with-Super-Resolution-and-Deep-Learning-Investigating-the-Impact-of-Spatial-Resolution-on-Various-Datasets" class="headerlink" title="Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets"></a>Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04101">http://arxiv.org/abs/2307.04101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiling Guo, Xiaodan Shi, Haoran Zhang, Dou Huang, Xiaoya Song, Jinyue Yan, Ryosuke Shibasaki</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨æ·±åº¦å­¦ä¹ åŸºäºè¿œç¨‹æ„ŸçŸ¥æŠ€æœ¯çš„å»ºç­‘ semantics åˆ†å‰²ä¸­ spatial resolution çš„å½±å“ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº† super-resolution å’Œ down-sampling æŠ€æœ¯å°† remote sensing å›¾åƒè½¬åŒ–ä¸ºå¤šä¸ªç©ºé—´åˆ†è¾¨ç‡ï¼Œç„¶åé€‰æ‹©äº† UNet å’Œ FPN ä¸¤ç§æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œå»ºç­‘ semantics åˆ†å‰²ç»“æœå—åˆ°ç©ºé—´åˆ†è¾¨ç‡çš„å½±å“ï¼Œå¹¶ä¸”åœ¨çº¦ 0.3m çš„ç©ºé—´åˆ†è¾¨ç‡ä¸‹è¾¾åˆ°æœ€ä½³æˆæœ¬æ•ˆæœã€‚<details>
<summary>Abstract</summary>
The development of remote sensing and deep learning techniques has enabled building semantic segmentation with high accuracy and efficiency. Despite their success in different tasks, the discussions on the impact of spatial resolution on deep learning based building semantic segmentation are quite inadequate, which makes choosing a higher cost-effective data source a big challenge. To address the issue mentioned above, in this study, we create remote sensing images among three study areas into multiple spatial resolutions by super-resolution and down-sampling. After that, two representative deep learning architectures: UNet and FPN, are selected for model training and testing. The experimental results obtained from three cities with two deep learning models indicate that the spatial resolution greatly influences building segmentation results, and with a better cost-effectiveness around 0.3m, which we believe will be an important insight for data selection and preparation.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>> translate "The development of remote sensing and deep learning techniques has enabled building semantic segmentation with high accuracy and efficiency. Despite their success in different tasks, the discussions on the impact of spatial resolution on deep learning based building semantic segmentation are quite inadequate, which makes choosing a higher cost-effective data source a big challenge. To address the issue mentioned above, in this study, we create remote sensing images among three study areas into multiple spatial resolutions by super-resolution and down-sampling. After that, two representative deep learning architectures: UNet and FPN, are selected for model training and testing. The experimental results obtained from three cities with two deep learning models indicate that the spatial resolution greatly influences building segmentation results, and with a better cost-effectiveness around 0.3m, which we believe will be an important insight for data selection and preparation." into Chinese (Simplified)ğŸ“å‘å±•è¿œç¨‹æ„ŸçŸ¥å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œä½¿å¾—å»ºç­‘ semantic segmentation çš„å‡†ç¡®ç‡å’Œæ•ˆç‡å¾—åˆ°äº†å¤§å¹…æé«˜ã€‚ç„¶è€Œï¼Œå…³äºæ·±åº¦å­¦ä¹ åŸºäºå»ºç­‘ semantic segmentation çš„ç©ºé—´åˆ†è¾¨ç‡å½±å“çš„è®¨è®ºï¼Œå°šä¸å……åˆ†ï¼Œè¿™ä½¿å¾—é€‰æ‹©æ›´é«˜æ•ˆæœå’Œç»æµçš„æ•°æ®æºæˆä¸ºä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬ç ”ç©¶ä¸­å°†Remote sensing å›¾åƒåœ¨ä¸‰ä¸ªç ”ç©¶åŒºåŸŸä¸­è¿›è¡Œå¤šä¸ªç©ºé—´åˆ†è¾¨ç‡çš„åˆ›å»ºï¼Œé€šè¿‡è¶…åˆ†è¾¨å’Œé™é‡‡æ ·ã€‚ç„¶åï¼Œé€‰æ‹©äº†ä¸¤ç§ä»£è¡¨æ€§çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼šUNet å’Œ FPNã€‚é€šè¿‡ä¸‰åº§åŸå¸‚çš„ä¸¤ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œç©ºé—´åˆ†è¾¨ç‡å¯¹å»ºç­‘ segmentation ç»“æœäº§ç”Ÿäº†æå¤§çš„å½±å“ï¼Œå¹¶ä¸”åœ¨çº¦0.3ç±³çš„æˆæœ¬æ•ˆæœä¸Šè¡¨ç°è¾ƒå¥½ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™å°†æˆä¸ºæ•°æ®é€‰æ‹©å’Œå‡†å¤‡çš„é‡è¦æ„è§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Combining-transmission-speckle-photography-and-convolutional-neural-network-for-determination-of-fat-content-in-cowâ€™s-milk-â€“-an-exercise-in-classification-of-parameters-of-a-complex-suspension"><a href="#Combining-transmission-speckle-photography-and-convolutional-neural-network-for-determination-of-fat-content-in-cowâ€™s-milk-â€“-an-exercise-in-classification-of-parameters-of-a-complex-suspension" class="headerlink" title="Combining transmission speckle photography and convolutional neural network for determination of fat content in cowâ€™s milk â€“ an exercise in classification of parameters of a complex suspension"></a>Combining transmission speckle photography and convolutional neural network for determination of fat content in cowâ€™s milk â€“ an exercise in classification of parameters of a complex suspension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15069">http://arxiv.org/abs/2307.15069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kwasi Nyandey, Daniel Jakubczyk</li>
<li>for: direct classification and recognition of milk fat content classes</li>
<li>methods: combined transmission speckle photography and machine learning</li>
<li>results: unambiguous recognition of milk fat content classes with high accuracy (100% and ~99%)<details>
<summary>Abstract</summary>
We have combined transmission speckle photography and machine learning for direct classification and recognition of milk fat content classes. Our aim was hinged on the fact that parameters of scattering particles (and the dispersion medium) can be linked to the intensity distribution (speckle) observed when coherent light is transmitted through a scattering medium. For milk, it is primarily the size distribution and concentration of fat globules, which constitutes the total fat content. Consequently, we trained convolutional neural network to recognise and classify laser speckle from different fat content classes (0.5, 1.5, 2.0 and 3.2%). We investigated four exposure-time protocols and obtained the highest performance for shorter exposure times, in which the intensity histograms are kept similar for all images and the most probable intensity in the speckle pattern is close to zero. Our neural network was able to recognize the milk fat content classes unambiguously and we obtained the highest test and independent classification accuracies of 100 and ~99% respectively. It indicates that the parameters of other complex realistic suspensions could be classified with similar methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬å°†ä¼ è¾“æ‰«æå…‰å­¦å’Œæœºå™¨å­¦ä¹ ç»“åˆï¼Œä»¥ç›´æ¥åˆ†ç±»å’Œè¯†åˆ«ç‰›å¥¶è„‚è‚ªå«é‡ç±»åˆ«ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ©ç”¨æ•£å°„ä½“ç²’å­ï¼ˆä»¥åŠæ•£å°„åª’ä½“ï¼‰çš„å‚æ•°ä¸æ‰«æå…‰é€šè¿‡æ•£å°„åª’ä½“æ‰€è§‚å¯Ÿåˆ°çš„INTENSITYåˆ†å¸ƒï¼ˆæ‰«æå…‰æ–‘ï¼‰ä¹‹é—´å­˜åœ¨å…³ç³»ã€‚åœ¨ç‰›å¥¶ä¸­ï¼Œä¸»è¦æ˜¯è„‚è‚ªçƒä½“å¤§å°åˆ†å¸ƒå’Œæ€»è„‚è‚ªå«é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°† convolutional neural network è®­ç»ƒæ¥è¯†åˆ«å’Œåˆ†ç±»ä¸åŒè„‚è‚ªå«é‡ç±»åˆ«ï¼ˆ0.5%, 1.5%, 2.0%å’Œ3.2%ï¼‰ã€‚æˆ‘ä»¬ç ”ç©¶äº†å››ç§æ›å…‰æ—¶é—´åè®®ï¼Œå¹¶è·å¾—äº†æœ€é«˜æ€§èƒ½ï¼Œå…¶ä¸­æ›å…‰æ—¶é—´è¾ƒçŸ­ï¼Œå›¾åƒINTENSITYé¢‘è°±å‡åŒ€ï¼Œæœ€ probable intensity åœ¨æ‰«æå…‰æ–‘æ¨¡å¼ä¸­æ¥è¿‘é›¶ã€‚æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œèƒ½å¤Ÿä¸ ambiguously è¯†åˆ«ç‰›å¥¶è„‚è‚ªå«é‡ç±»åˆ«ï¼Œå¹¶åœ¨æµ‹è¯•å’Œç‹¬ç«‹åˆ†ç±»å‡†ç¡®ç‡è¾¾100%å’Œ~99%ã€‚è¿™è¡¨æ˜å¯ä»¥ä½¿ç”¨ç±»ä¼¼æ–¹æ³•æ¥åˆ†ç±»å…¶ä»–å¤æ‚çš„çœŸå®æ¶‚æ•·ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/09/eess.IV_2023_07_09/" data-id="clltaagq000c1r888fpenbgxf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/08/cs.AI_2023_07_08/" class="article-date">
  <time datetime="2023-07-07T16:00:00.000Z" itemprop="datePublished">2023-07-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/08/cs.AI_2023_07_08/">cs.AI - 2023-07-08 20:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="PCG-based-Static-Underground-Garage-Scenario-Generation"><a href="#PCG-based-Static-Underground-Garage-Scenario-Generation" class="headerlink" title="PCG-based Static Underground Garage Scenario Generation"></a>PCG-based Static Underground Garage Scenario Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03988">http://arxiv.org/abs/2307.03988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjin Li, Kai Li</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨ç”¨Sarsaç®—æ³•è§£å†³åœ°ä¸‹åœè½¦åœº static scenario simulation çš„PCGé—®é¢˜ã€‚</li>
<li>methods: æœ¬paperä½¿ç”¨Sarsaç®—æ³•è¿›è¡ŒPCGï¼Œä»¥ç”Ÿæˆå…·æœ‰å……è¶³ç»†èŠ‚çš„åœ°ä¸‹åœè½¦åœºåœºæ™¯ã€‚</li>
<li>results: æœ¬ç ”ç©¶å®ç°äº†åŸºäºSarsaç®—æ³•çš„PCGæ–¹æ³•ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åœ°ä¸‹åœè½¦åœºåœºæ™¯ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„è®­ç»ƒæä¾›äº†æ›´å¤šçš„æ•°æ®æ”¯æŒã€‚<details>
<summary>Abstract</summary>
Autonomous driving technology has five levels, from L0 to L5. Currently, only the L2 level (partial automation) can be achieved, and there is a long way to go before reaching the final level of L5 (full automation). The key to crossing these levels lies in training the autonomous driving model. However, relying solely on real-world road data to train the model is far from enough and consumes a great deal of resources. Although there are already examples of training autonomous driving models through simulators that simulate real-world scenarios, these scenarios require complete manual construction. Directly converting 3D scenes from road network formats will lack a large amount of detail and cannot be used as training sets. Underground parking garage static scenario simulation is regarded as a procedural content generation (PCG) problem. This paper will use the Sarsa algorithm to solve procedural content generation on underground garage structures.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªåŠ¨é©¾é©¶æŠ€æœ¯æœ‰äº”çº§ï¼Œä»L0åˆ°L5ã€‚ç›®å‰åªæœ‰L2çº§ï¼ˆéƒ¨åˆ†è‡ªåŠ¨åŒ–ï¼‰å¯ä»¥å®ç°ï¼Œå‰©ä¸‹çš„çº§åˆ«è¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚æ¨¡å‹è®­ç»ƒæ˜¯ååˆ†é‡è¦çš„å…³é”®ã€‚ç„¶è€Œï¼Œä»…ä»…é€šè¿‡ä½¿ç”¨ç°å®ä¸–ç•Œé“è·¯æ•°æ®æ¥è®­ç»ƒæ¨¡å‹æ˜¯è´¹å°½èµ„æºçš„ï¼Œè€Œä¸”éœ€è¦å¤§é‡çš„æ•°æ®ã€‚è™½ç„¶å·²ç»æœ‰äº›ä½¿ç”¨æ¨¡æ‹Ÿå™¨ simulate real-world scenariosçš„ä¾‹å­ï¼Œä½†è¿™äº›åœºæ™¯éœ€è¦å®Œå…¨æ‰‹åŠ¨æ„å»ºã€‚ç›´æ¥å°†3Dåœºæ™¯ä»é“è·¯ç½‘ç»œæ ¼å¼è½¬æ¢æ¥ç”¨ä½œè®­ç»ƒé›†æ˜¯ç¼ºä¹è¯¦ç»†ä¿¡æ¯çš„ï¼Œæ— æ³•ç”¨äºè®­ç»ƒã€‚åœ°ä¸‹åœè½¦åœº static scenario simulationè¢«è§†ä¸ºä¸€ä¸ªè¿‡ç¨‹Content generationï¼ˆPCGï¼‰é—®é¢˜ã€‚æœ¬æ–‡ä½¿ç”¨Sarsaç®—æ³•è§£å†³åœ°ä¸‹åœè½¦åœºçš„PCGé—®é¢˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="Integrating-Curricula-with-Replays-Its-Effects-on-Continual-Learning"><a href="#Integrating-Curricula-with-Replays-Its-Effects-on-Continual-Learning" class="headerlink" title="Integrating Curricula with Replays: Its Effects on Continual Learning"></a>Integrating Curricula with Replays: Its Effects on Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05747">http://arxiv.org/abs/2307.05747</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanglab-deepneurocoglab/integrating-curricula-with-replays">https://github.com/zhanglab-deepneurocoglab/integrating-curricula-with-replays</a></li>
<li>paper_authors: Ren Jie Tee, Mengmi Zhang</li>
<li>for: è¿™ç§ç ”ç©¶æ—¨åœ¨æ¢è®¨åœ¨è¿›è¡Œè¿ç»­å­¦ä¹ æ—¶ï¼Œé€šè¿‡èåˆè¯¾ç¨‹å’Œé‡æ¸©æ–¹æ³•ï¼Œä»¥ä¾¿æé«˜çŸ¥è¯†ä¿æŒå’Œå­¦ä¹ ä¼ é€’ã€‚</li>
<li>methods: ç ”ç©¶ä½¿ç”¨äº†ä¸åŒçš„è¯¾ç¨‹è®¾è®¡ï¼ŒåŒ…æ‹¬äº¤å é¢‘ç‡ã€é¡ºåºå’Œé€‰æ‹©ç­–ç•¥ï¼Œä»¥å½±å“é‡æ¸©è¿‡ç¨‹ä¸­çš„è¿ç»­å­¦ä¹ ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œé€šè¿‡èåˆè¯¾ç¨‹å’Œé‡æ¸©æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°é¿å…å¿˜å´ç°è±¡ï¼Œå¹¶æé«˜çŸ¥è¯†ä¼ é€’ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œèåˆè¯¾ç¨‹å¯ä»¥æˆä¸ºè¿ç»­å­¦ä¹ æ–¹æ³•çš„è¿›ä¸€æ­¥å‘å±•ã€‚<details>
<summary>Abstract</summary>
Humans engage in learning and reviewing processes with curricula when acquiring new skills or knowledge. This human learning behavior has inspired the integration of curricula with replay methods in continual learning agents. The goal is to emulate the human learning process, thereby improving knowledge retention and facilitating learning transfer. Existing replay methods in continual learning agents involve the random selection and ordering of data from previous tasks, which has shown to be effective. However, limited research has explored the integration of different curricula with replay methods to enhance continual learning. Our study takes initial steps in examining the impact of integrating curricula with replay methods on continual learning in three specific aspects: the interleaved frequency of replayed exemplars with training data, the sequence in which exemplars are replayed, and the strategy for selecting exemplars into the replay buffer. These aspects of curricula design align with cognitive psychology principles and leverage the benefits of interleaved practice during replays, easy-to-hard rehearsal, and exemplar selection strategy involving exemplars from a uniform distribution of difficulties. Based on our results, these three curricula effectively mitigated catastrophic forgetting and enhanced positive knowledge transfer, demonstrating the potential of curricula in advancing continual learning methodologies. Our code and data are available: https://github.com/ZhangLab-DeepNeuroCogLab/Integrating-Curricula-with-Replays
</details>
<details>
<summary>æ‘˜è¦</summary>
äººç±»åœ¨å­¦ä¹ å’Œå¤ä¹ è¿‡ç¨‹ä¸­ä½¿ç”¨è¯¾ç¨‹ï¼Œå½“å­¦ä¹ æ–°æŠ€èƒ½æˆ–çŸ¥è¯†æ—¶ã€‚è¿™ç§äººç±»å­¦ä¹ è¡Œä¸ºæ¿€å‘äº†åœ¨ä¸æ–­å­¦ä¹ ä»£ç†äººä¸­ç»“åˆè¯¾ç¨‹å’Œå¤ä¹ æ–¹æ³•çš„æ•´åˆã€‚ç›®æ ‡æ˜¯æ¨¡æ‹Ÿäººç±»å­¦ä¹ è¿‡ç¨‹ï¼Œä»è€Œæé«˜çŸ¥è¯†ä¿æŒå’Œå­¦ä¹ è½¬ç§»ã€‚ç°æœ‰çš„å¤ä¹ æ–¹æ³•åœ¨ä¸æ–­å­¦ä¹ ä»£ç†äººä¸­Randomly selecting and ordering data from previous tasks has been shown to be effective. However, limited research has explored the integration of different curricula with replay methods to enhance continual learning. Our study takes initial steps in examining the impact of integrating curricula with replay methods on continual learning in three specific aspects: the interleaved frequency of replayed exemplars with training data, the sequence in which exemplars are replayed, and the strategy for selecting exemplars into the replay buffer. These aspects of curricula design align with cognitive psychology principles and leverage the benefits of interleaved practice during replays, easy-to-hard rehearsal, and exemplar selection strategy involving exemplars from a uniform distribution of difficulties. According to our results, these three curricula effectively mitigated catastrophic forgetting and enhanced positive knowledge transfer, demonstrating the potential of curricula in advancing continual learning methodologies.æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨ GitHubä¸Šè·å–ï¼šhttps://github.com/ZhangLab-DeepNeuroCogLab/Integrating-Curricula-with-Replays
</details></li>
</ul>
<hr>
<h2 id="Autonomy-2-0-The-Quest-for-Economies-of-Scale"><a href="#Autonomy-2-0-The-Quest-for-Economies-of-Scale" class="headerlink" title="Autonomy 2.0: The Quest for Economies of Scale"></a>Autonomy 2.0: The Quest for Economies of Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03973">http://arxiv.org/abs/2307.03973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Wu, Bo Yu, Shaoshan Liu, Yuhao Zhu</li>
<li>for: æœ¬æ–‡ä¸»è¦é€‚ç”¨äº autonomous machines é¢†åŸŸçš„æŠ€æœ¯æŒ‘æˆ˜å’Œç»æµå½±å“ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨æŠ€æœ¯åˆ†æå’Œç»æµåˆ†ææ–¹æ³•æ¥æ¢è®¨ autonomous machines é¢†åŸŸçš„å¯æè¿°æ€§å’Œç»æµå¯èƒ½æ€§ã€‚</li>
<li>results: æœ¬æ–‡ argue that scalability æ˜¯ autonomous machines é¢†åŸŸçš„å…³é”®å› ç´ ï¼Œä½†ç°æœ‰çš„å‘å±•æ¨¡å¼ï¼ˆAutonomy 1.0ï¼‰ä¸èƒ½å……åˆ†åˆ©ç”¨è®¡ç®—æˆæœ¬å’Œæ•°æ®èµ„æºçš„ç»æµæ•ˆç›Šã€‚ åœ¨è§£å†³å…³é”®ç“¶é¢ˆçš„åŒæ—¶ï¼Œæ–°çš„å‘å±•æ¨¡å¼ï¼ˆAutonomy 2.0ï¼‰å¯ä»¥å¤§å¹…æé«˜ autonomous machines é¢†åŸŸçš„å¯æè¿°æ€§å’Œç»æµå¯èƒ½æ€§ã€‚<details>
<summary>Abstract</summary>
With the advancement of robotics and AI technologies in the past decade, we have now entered the age of autonomous machines. In this new age of information technology, autonomous machines, such as service robots, autonomous drones, delivery robots, and autonomous vehicles, rather than humans, will provide services. In this article, through examining the technical challenges and economic impact of the digital economy, we argue that scalability is both highly necessary from a technical perspective and significantly advantageous from an economic perspective, thus is the key for the autonomy industry to achieve its full potential. Nonetheless, the current development paradigm, dubbed Autonomy 1.0, scales with the number of engineers, instead of with the amount of data or compute resources, hence preventing the autonomy industry to fully benefit from the economies of scale, especially the exponentially cheapening compute cost and the explosion of available data. We further analyze the key scalability blockers and explain how a new development paradigm, dubbed Autonomy 2.0, can address these problems to greatly boost the autonomy industry.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:éšç€è¿‡å»åå¹´çš„æœºå™¨äººå’Œäººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•ï¼Œæˆ‘ä»¬å·²ç»è¿›å…¥äº†è‡ªåŠ¨åŒ–æœºå™¨çš„æ—¶ä»£ã€‚åœ¨è¿™ä¸ªæ–°çš„ä¿¡æ¯æŠ€æœ¯æ—¶ä»£ï¼Œè‡ªåŠ¨åŒ–æœºå™¨ï¼Œå¦‚æœåŠ¡æœºå™¨äººã€è‡ªåŠ¨é£è¡Œå™¨ã€é…é€æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼Œè€Œä¸æ˜¯äººç±»ï¼Œå°†æä¾›æœåŠ¡ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†ææŠ€æœ¯æŒ‘æˆ˜å’Œæ•°å­—ç»æµçš„å½±å“ï¼Œ argue thatå¯æ‰©å±•æ€§æ˜¯æŠ€æœ¯ä¸Šå¿…éœ€çš„å’Œç»æµä¸Šæœ‰åˆ©çš„ï¼Œå› æ­¤æ˜¯è‡ªåŠ¨åŒ–industryçš„æ½œåœ¨åŠ›é‡ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¼€å‘æ¨¡å¼ï¼Œç§°ä¸ºAutonomy 1.0ï¼Œä¸å·¥ç¨‹å¸ˆæ•°é‡æˆæ¯”ä¾‹å¢é•¿ï¼Œè€Œä¸æ˜¯ä¸æ•°æ®é‡æˆ–è®¡ç®—èµ„æºæˆæ¯”ä¾‹å¢é•¿ï¼Œå› æ­¤é˜»ç¢äº†è‡ªåŠ¨åŒ–industryä»å…¨é¢è·å¾—ç»æµæ•ˆç›Šï¼Œç‰¹åˆ«æ˜¯å¿«é€Ÿå‡å°‘çš„è®¡ç®—æˆæœ¬å’Œå¯ç”¨æ•°æ®çš„çˆ†å‘ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æé˜»ç¢å¯æ‰©å±•æ€§çš„å…³é”®é—®é¢˜ï¼Œå¹¶è§£é‡Šå¦‚ä½•ä¸€ç§æ–°çš„å¼€å‘æ¨¡å¼ï¼Œç§°ä¸ºAutonomy 2.0ï¼Œå¯ä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä»¥å¤§å¹…æé«˜è‡ªåŠ¨åŒ–industryã€‚
</details></li>
</ul>
<hr>
<h2 id="Multi-Intent-Detection-in-User-Provided-Annotations-for-Programming-by-Examples-Systems"><a href="#Multi-Intent-Detection-in-User-Provided-Annotations-for-Programming-by-Examples-Systems" class="headerlink" title="Multi-Intent Detection in User Provided Annotations for Programming by Examples Systems"></a>Multi-Intent Detection in User Provided Annotations for Programming by Examples Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03966">http://arxiv.org/abs/2307.03966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nischal Ashok Kumar, Nitin Gupta, Shanmukha Guttula, Hima Patel</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯è§£å†³åœ¨é›†æˆå¼€å‘ä¸­æ•°æ®æ˜ å°„çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨åº”ç”¨ç¨‹åºç¼ºä¹å‘½åæ ‡å‡†å’ŒåµŒå¥—å­—æ®µç»“æ„çš„æƒ…å†µä¸‹ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ç¼–ç¨‹ä¾‹å­ï¼ˆPBEï¼‰æŠ€æœ¯æ¥è‡ªåŠ¨ç”Ÿæˆæ•°æ®è½¬æ¢ç¨‹åºï¼Œä»ç”¨æˆ·æä¾›çš„è¾“å…¥å’Œè¾“å‡ºæ ·æœ¬ä¸­å­¦ä¹ è®¡ç®—æœºç¨‹åºçš„æ­£ç¡®æ„å›¾ã€‚</li>
<li>results: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ·±åº¦ç¥ç»ç½‘ç»œåŸºäºä¸ç¡®å®šæ€§é¢„æµ‹æ¨¡å‹ï¼Œå¯ä»¥åˆ†æè¾“å…¥è¾“å‡ºå­—ç¬¦ä¸²å¹¶å°†å…¶æ˜ å°„åˆ°ä¸åŒçš„å±æ€§é›†ï¼Œä»¥è§£å†³PBEç³»ç»Ÿä¸­çš„å¤šæ„é—®é¢˜ã€‚<details>
<summary>Abstract</summary>
In mapping enterprise applications, data mapping remains a fundamental part of integration development, but its time consuming. An increasing number of applications lack naming standards, and nested field structures further add complexity for the integration developers. Once the mapping is done, data transformation is the next challenge for the users since each application expects data to be in a certain format. Also, while building integration flow, developers need to understand the format of the source and target data field and come up with transformation program that can change data from source to target format. The problem of automatic generation of a transformation program through program synthesis paradigm from some specifications has been studied since the early days of Artificial Intelligence (AI). Programming by Example (PBE) is one such kind of technique that targets automatic inferencing of a computer program to accomplish a format or string conversion task from user-provided input and output samples. To learn the correct intent, a diverse set of samples from the user is required. However, there is a possibility that the user fails to provide a diverse set of samples. This can lead to multiple intents or ambiguity in the input and output samples. Hence, PBE systems can get confused in generating the correct intent program. In this paper, we propose a deep neural network based ambiguity prediction model, which analyzes the input-output strings and maps them to a different set of properties responsible for multiple intent. Users can analyze these properties and accordingly can provide new samples or modify existing samples which can help in building a better PBE system for mapping enterprise applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
Mappingä¼ä¸šåº”ç”¨ç¨‹åºä¸­ï¼Œæ•°æ®æ˜ å°„ä»ç„¶æ˜¯é›†æˆå¼€å‘çš„åŸºæœ¬éƒ¨åˆ†ï¼Œä½†æ˜¯å®ƒå ç”¨äº†å¾ˆå¤šæ—¶é—´ã€‚è¶Šæ¥è¶Šå¤šçš„åº”ç”¨ç¨‹åºç¼ºå°‘å‘½åæ ‡å‡†ï¼ŒåµŒå¥—çš„å­—æ®µç»“æ„è¿›ä¸€æ­¥å¢åŠ äº†é›†æˆå¼€å‘äººå‘˜çš„å¤æ‚æ€§ã€‚ä¸€æ—¦æ˜ å°„å®Œæˆï¼Œåˆ™ä¸‹ä¸€ä¸ªæŒ‘æˆ˜æ˜¯æ•°æ®è½¬æ¢ï¼Œå› ä¸ºæ¯ä¸ªåº”ç”¨ç¨‹åºéƒ½ä¼šé¢„æœŸæ•°æ®åœ¨æŸç§æ ¼å¼ä¸‹æ¥ã€‚åœ¨å»ºç«‹é›†æˆæµæ—¶ï¼Œå¼€å‘äººå‘˜éœ€è¦ç†è§£æºæ•°æ®å’Œç›®æ ‡æ•°æ®å­—æ®µçš„æ ¼å¼ï¼Œå¹¶ç¼–å†™è½¬æ¢ç¨‹åºä»¥å°†æ•°æ®ä»æºæ ¼å¼è½¬æ¢åˆ°ç›®æ ‡æ ¼å¼ã€‚è‡ªAIæ—¶ä»£ä»¥æ¥ï¼Œäººå·¥æ™ºèƒ½ç¼–ç¨‹æ–¹æ³•å·²ç»è¢«ç ”ç©¶äº†å¾ˆé•¿æ—¶é—´ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œä¸€ç§æŠ€æœ¯æ˜¯ç¼–ç¨‹ç¤ºä¾‹ï¼ˆPBEï¼‰ï¼Œå®ƒå¯ä»¥è‡ªåŠ¨ç”Ÿæˆè®¡ç®—æœºç¨‹åºï¼Œä»¥å®ç°æ ¼å¼æˆ–å­—ç¬¦ä¸²è½¬æ¢ä»»åŠ¡ã€‚ä¸ºäº†å­¦ä¹ æ­£ç¡®çš„æ„å›¾ï¼Œç”¨æˆ·éœ€è¦æä¾›å¤šæ ·çš„ç¤ºä¾‹ã€‚ç„¶è€Œï¼Œç”¨æˆ·å¯èƒ½æ— æ³•æä¾›å¤šæ ·çš„ç¤ºä¾‹ï¼Œè¿™ä¼šå¯¼è‡´å¤šä¸ªæ„å›¾æˆ–è¾“å…¥/è¾“å‡ºæ ·æœ¬çš„æ­§ä¹‰ã€‚å› æ­¤ï¼ŒPBEç³»ç»Ÿå¯èƒ½ä¼šåœ¨ç”Ÿæˆæ­£ç¡®æ„å›¾ç¨‹åºæ—¶æ„Ÿåˆ°å›°æƒ‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ­§ä¹‰é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥åˆ†æè¾“å…¥/è¾“å‡ºå­—ç¬¦ä¸²ï¼Œå¹¶å°†å…¶æ˜ å°„åˆ°ä¸åŒçš„å±æ€§é›†ï¼Œè¿™äº›å±æ€§é›†è´Ÿè´£å¤šä¸ªæ„å›¾ã€‚ç”¨æˆ·å¯ä»¥åˆ†æè¿™äº›å±æ€§ï¼Œå¹¶æ ¹æ®è¿™äº›å±æ€§æä¾›æ–°çš„ç¤ºä¾‹æˆ–ä¿®æ”¹ç°æœ‰ç¤ºä¾‹ï¼Œä»¥å¸®åŠ©å»ºç«‹æ›´å¥½çš„PBEç³»ç»Ÿã€‚
</details></li>
</ul>
<hr>
<h2 id="Right-to-be-Forgotten-in-the-Era-of-Large-Language-Models-Implications-Challenges-and-Solutions"><a href="#Right-to-be-Forgotten-in-the-Era-of-Large-Language-Models-Implications-Challenges-and-Solutions" class="headerlink" title="Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions"></a>Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03941">http://arxiv.org/abs/2307.03941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, Xiwei Xu</li>
<li>For: This paper explores the challenges of implementing the Right to Be Forgotten (RTBF) in Large Language Models (LLMs) and provides insights on how to implement technical solutions for RTBF.* Methods: The paper discusses the use of machine unlearning, model editing, and prompting engineering as potential solutions for RTBF in LLMs.* Results: The paper provides insights on the challenges of implementing RTBF in LLMs and suggests potential solutions for compliance with the RTBF.Hereâ€™s the text in Simplified Chinese:* For: è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å®ç°â€œå¿˜è®°æƒâ€ï¼ˆRTBFï¼‰çš„æŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†å®ç°RTBFçš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆã€‚* Methods: è®ºæ–‡è®¨è®ºäº†æœºå™¨â€œå¿˜è®°â€ã€æ¨¡å‹ä¿®æ”¹å’Œå¼•å¯¼å·¥ç¨‹ç­‰å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚* Results: è®ºæ–‡æä¾›äº†LLMä¸­å®ç°RTBFçš„æŒ‘æˆ˜å’Œå¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚<details>
<summary>Abstract</summary>
The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of machine unlearning, model editing, and prompting engineering.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå³æŠ•å¿˜â€ï¼ˆRTBFï¼‰é¦–æ¬¡å¾—åˆ°äº†Googleè¥¿ç­ç‰™SLã€Googleå…¬å¸è¯‰AEPDã€é©¬é‡Œå¥¥Â·ç§‘æ–¯æ³°åŠ Â·å†ˆè¨é›·æ–¯æ¡ˆä¾‹çš„åˆ¤å†³ï¼Œ laterè¢«åŒ…å«åœ¨æ¬§ç›Ÿæ•°æ®ä¿æŠ¤æ¡ä¾‹ï¼ˆGDPRï¼‰ä¸­ï¼Œä»¥allowä¸ªäººè¯·æ±‚ç»„ç»‡åˆ é™¤ä¸ªäººæ•°æ®ã€‚ç‰¹åˆ«æ˜¯ Ğ´Ğ»Ñæœç´¢å¼•æ“ï¼Œä¸ªäººå¯ä»¥å‘ç»„ç»‡å‘é€è¯·æ±‚ï¼Œè¯·æ±‚æ’é™¤ä»–ä»¬çš„ä¿¡æ¯ä»æŸ¥è¯¢ç»“æœä¸­ã€‚éšç€å¤§è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•å’Œå®ƒä»¬åœ¨ Ñ‡Ğ°Ñ‚æœºå™¨äººä¸­çš„ä½¿ç”¨ï¼ŒLLM-enabled software systems å·²æˆä¸ºæµè¡Œçš„ã€‚ä½†å®ƒä»¬å¹¶ä¸æ˜¯RTBFçš„ä¾‹å¤–ã€‚ä¸æœç´¢å¼•æ“ä½¿ç”¨çš„ç´¢å¼•æ–¹æ³•ä¸åŒï¼ŒLLMså­˜å‚¨å’Œå¤„ç†ä¿¡æ¯çš„æ–¹å¼å¸¦æ¥äº†æ–°çš„RTBFçš„æŒ‘æˆ˜ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨è¿™äº›æŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†å®ç°RTBFçš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ¨¡å‹ç¼–è¾‘å’Œæç¤ºå·¥ç¨‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Copilot-for-Xcode-Exploring-AI-Assisted-Programming-by-Prompting-Cloud-based-Large-Language-Models"><a href="#Copilot-for-Xcode-Exploring-AI-Assisted-Programming-by-Prompting-Cloud-based-Large-Language-Models" class="headerlink" title="Copilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-based Large Language Models"></a>Copilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-based Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14349">http://arxiv.org/abs/2307.14349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chee Wei Tan, Shangxin Guo, Man Fai Wong, Ching Nam Hang</li>
<li>for: æ”¯æŒäººç±»è½¯ä»¶å¼€å‘è€…çš„AIåŠ©æ‰‹å·¥å…·ï¼Œå¸®åŠ©å¼€å‘è€…æ›´å¿«é€Ÿã€æ›´é«˜æ•ˆåœ°å®Œæˆè½¯ä»¶å¼€å‘ä»»åŠ¡ã€‚</li>
<li>methods: åˆ©ç”¨äº‘ç«¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒAppleçš„æœ¬åœ°å¼€å‘ç¯å¢ƒXcodeè¿›è¡Œé›†æˆï¼Œé€šè¿‡é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯æ¥å¤„ç†ä»£ç ç¬¦å·å’Œä»£ç æ¨¡å¼ï¼Œå®ç°ä»£ç ç”Ÿæˆã€è‡ªåŠ¨å®Œæˆã€æ–‡æ¡£ç”Ÿæˆå’Œé”™è¯¯æ¢æµ‹ç­‰åŠŸèƒ½ã€‚</li>
<li>results: é€šè¿‡åœ¨Xcodeä¸­ integrate LLMï¼Œå¯ä»¥æé«˜å¼€å‘æ•ˆç‡å’Œé‡Šæ”¾åˆ›é€ åŠ›ï¼Œå¹¶ä¸”å¯ä»¥åŒæ—¶è¿›è¡Œä¸€äº›å°å‹å†³ç­–ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹æ¥å¸®åŠ©å¼€å‘è€…æ›´å¿«é€Ÿåœ°å®Œæˆè½¯ä»¶å¼€å‘ä»»åŠ¡ã€‚<details>
<summary>Abstract</summary>
This paper presents an AI-assisted programming tool called Copilot for Xcode for program composition and design to support human software developers. By seamlessly integrating cloud-based Large Language Models (LLM) with Apple's local development environment, Xcode, this tool enhances productivity and unleashes creativity for software development in Apple software ecosystem (e.g., iOS apps, macOS). Leveraging advanced natural language processing (NLP) techniques, Copilot for Xcode effectively processes source code tokens and patterns within code repositories, enabling features such as code generation, autocompletion, documentation, and error detection. Software developers can also query and make "small" decisions for program composition, some of which can be made simultaneously, and this is facilitated through prompt engineering in a chat interface of Copilot for Xcode. Finally, we present simple case studies as evidence of the effectiveness of utilizing NLP in Xcode to prompt popular LLM services like OpenAI ChatGPT for program composition and design.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡æè¿°äº†ä¸€ä¸ªåŸºäºäººå·¥æ™ºèƒ½çš„ç¼–ç¨‹å·¥å…·called Copilotï¼Œç”¨äºå¢å¼º Xcode ä¸­çš„è½¯ä»¶å¼€å‘äº§ä¸šã€‚è¯¥å·¥å…·é€šè¿‡å°†äº‘ç«¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ Apple çš„æœ¬åœ°å¼€å‘ç¯å¢ƒ XCode é›†æˆï¼Œä»¥æé«˜å¼€å‘æ•ˆç‡å’Œé‡Šæ”¾åˆ›é€ åŠ›ã€‚é€šè¿‡è¿›è¡Œæºä»£ç Tokenå’Œæ¨¡å¼çš„é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å¤„ç†ï¼ŒCopilot for Xcode å¯ä»¥å®ç°ä»£ç ç”Ÿæˆã€è‡ªåŠ¨å®Œæˆã€æ–‡æ¡£ç”Ÿæˆå’Œé”™è¯¯æ£€æµ‹ç­‰åŠŸèƒ½ã€‚å¼€å‘è€…å¯ä»¥é€šè¿‡æç¤ºå·¥ç¨‹æ¥è¿›è¡Œå°å†³ç­–ï¼Œå¹¶é€šè¿‡äº¤äº’å¼å¼¹å‡ºæ¡†æ¶æ¥åŒæ—¶è¿›è¡Œå¤šä¸ªå†³ç­–ã€‚æœ€åï¼Œæˆ‘ä»¬æä¾›äº†ä¸€äº›ç®€å•çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œä»¥è¯æ˜ä½¿ç”¨ NLP åœ¨ Xcode ä¸­æç¤ºæµè¡Œçš„ LLM æœåŠ¡å¦‚ OpenAI ChatGPT å¯ä»¥å¢å¼ºè½¯ä»¶å¼€å‘å’Œè®¾è®¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Inductive-Meta-path-Learning-for-Schema-complex-Heterogeneous-Information-Networks"><a href="#Inductive-Meta-path-Learning-for-Schema-complex-Heterogeneous-Information-Networks" class="headerlink" title="Inductive Meta-path Learning for Schema-complex Heterogeneous Information Networks"></a>Inductive Meta-path Learning for Schema-complex Heterogeneous Information Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03937">http://arxiv.org/abs/2307.03937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shixuan Liu, Changjun Fan, Kewei Cheng, Yunfei Wang, Peng Cui, Yizhou Sun, Zhong Liu</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†è§£å†³å¤æ‚ schema ä¸Šçš„ Heterogeneous Information Networks (HINs) ä¸­çš„ meta-path é—®é¢˜ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ inducing meta-path learning æ¡†æ¶ï¼Œä½¿ç”¨ schema-level è¡¨ç¤ºæ¥æ”¯æŒä¸åŒå…³ç³»çš„ meta-path å­¦ä¹ ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§åŸºäºå¥–åŠ±å­¦ä¹ çš„è·¯å¾„æ‰¾ç´¢å¼•æœºåˆ¶æ¥å­¦ä¹ Establishing meta-paths with high coverage and confidence for multiple relationsã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æå‡ºçš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è§£å†³å¤æ‚ schema ä¸Šçš„ HINs ä¸­ meta-path é—®é¢˜ï¼Œå¹¶ä¸”å¯ä»¥æé«˜ meta-path çš„è¦†ç›–ç‡å’Œä¿¡ä»»åº¦ã€‚<details>
<summary>Abstract</summary>
Heterogeneous Information Networks (HINs) are information networks with multiple types of nodes and edges. The concept of meta-path, i.e., a sequence of entity types and relation types connecting two entities, is proposed to provide the meta-level explainable semantics for various HIN tasks. Traditionally, meta-paths are primarily used for schema-simple HINs, e.g., bibliographic networks with only a few entity types, where meta-paths are often enumerated with domain knowledge. However, the adoption of meta-paths for schema-complex HINs, such as knowledge bases (KBs) with hundreds of entity and relation types, has been limited due to the computational complexity associated with meta-path enumeration. Additionally, effectively assessing meta-paths requires enumerating relevant path instances, which adds further complexity to the meta-path learning process. To address these challenges, we propose SchemaWalk, an inductive meta-path learning framework for schema-complex HINs. We represent meta-paths with schema-level representations to support the learning of the scores of meta-paths for varying relations, mitigating the need of exhaustive path instance enumeration for each relation. Further, we design a reinforcement-learning based path-finding agent, which directly navigates the network schema (i.e., schema graph) to learn policies for establishing meta-paths with high coverage and confidence for multiple relations. Extensive experiments on real data sets demonstrate the effectiveness of our proposed paradigm.
</details>
<details>
<summary>æ‘˜è¦</summary>
éåŒè´¨ä¿¡æ¯ç½‘ç»œï¼ˆHINï¼‰æ˜¯ä¸€ç§å…·æœ‰å¤šç§èŠ‚ç‚¹å’Œè¾¹çš„ä¿¡æ¯ç½‘ç»œã€‚meta-pathè¿™ä¸ªæ¦‚å¿µï¼Œå³è¿æ¥ä¸¤ä¸ªå®ä½“çš„ä¸€ç³»åˆ—å®ä½“ç±»å‹å’Œå…³ç³»ç±»å‹çš„åºåˆ—ï¼Œæ˜¯ä¸ºäº†æä¾›HINä»»åŠ¡çš„å…ƒç´ çº§åˆ«è§£é‡Š semanticsã€‚ç„¶è€Œï¼Œåœ¨Schema-Complex HINsä¸­ï¼Œä¾‹å¦‚çŸ¥è¯†åº“ï¼ˆKBï¼‰ä¸­çš„ç™¾ç§å®ä½“å’Œå…³ç³»ç±»å‹ï¼Œmeta-pathçš„é‡‡ç”¨å—åˆ°äº†è®¡ç®—å¤æ‚æ€§çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œè¯„ä¼°meta-pathéœ€è¦åˆ—å‡ºç›¸å…³çš„è·¯å¾„å®ä¾‹ï¼Œè¿™åŠ é‡äº†meta-pathå­¦ä¹ è¿‡ç¨‹ä¸­çš„å¤æ‚æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SchemaWalkï¼Œä¸€ç§induktive meta-pathå­¦ä¹ æ¡†æ¶ Ğ´Ğ»ÑSchema-Complex HINsã€‚æˆ‘ä»¬ä½¿ç”¨schemaå±‚æ¬¡è¡¨ç¤ºmeta-pathsï¼Œä»¥æ”¯æŒä¸åŒå…³ç³»çš„scoreå­¦ä¹ ï¼Œä»è€Œæ¶ˆé™¤äº†æ¯ä¸ªå…³ç³»çš„æšä¸¾è·¯å¾„å®ä¾‹çš„éœ€è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºå¥–åŠ±å­¦ä¹ çš„è·¯å¾„æ‰¾ç´¢å¼•å™¨ï¼Œå¯ä»¥ç›´æ¥åœ¨ç½‘ç»œschemaå›¾ï¼ˆå³schemaå›¾ï¼‰ä¸Šå­¦ä¹ Establish meta-paths with high coverage and confidence for multiple relationsã€‚å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Towards-Efficient-In-memory-Computing-Hardware-for-Quantized-Neural-Networks-State-of-the-art-Open-Challenges-and-Perspectives"><a href="#Towards-Efficient-In-memory-Computing-Hardware-for-Quantized-Neural-Networks-State-of-the-art-Open-Challenges-and-Perspectives" class="headerlink" title="Towards Efficient In-memory Computing Hardware for Quantized Neural Networks: State-of-the-art, Open Challenges and Perspectives"></a>Towards Efficient In-memory Computing Hardware for Quantized Neural Networks: State-of-the-art, Open Challenges and Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03936">http://arxiv.org/abs/2307.03936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olga Krestinskaya, Li Zhang, Khaled Nabil Salama</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨åœ¨ Edge Computing ç¯å¢ƒä¸­å®ç°ååé‡å‹ç¼©ç¥ç»ç½‘ç»œï¼Œå°¤å…¶æ˜¯é’ˆå¯¹å…·æœ‰é™åˆ¶çš„èƒ½æºå’Œè®¡ç®—èµ„æºçš„è¾¹ç¼˜è®¾å¤‡ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨ In-memory Computing (IMC) æŠ€æœ¯å’Œé‡åŒ–ç¥ç»ç½‘ç»œ (QNN) æ¥å®ç°Edge Computingä¸­çš„ç¥ç»ç½‘ç»œå¤„ç†ã€‚</li>
<li>results: æœ¬ç ”ç©¶æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„ QNN å’Œ IMC ç¡¬ä»¶å®ç°çš„è¯„ä¼°ï¼Œä»¥åŠå¼€æ”¾çš„æŒ‘æˆ˜ã€è®¾è®¡è¦æ±‚ã€å»ºè®®å’Œå‰ç»ï¼Œå¹¶æä¾›äº†ä¸€ä¸ª IMCC ç¡¬ä»¶è·¯çº¿å›¾ã€‚<details>
<summary>Abstract</summary>
The amount of data processed in the cloud, the development of Internet-of-Things (IoT) applications, and growing data privacy concerns force the transition from cloud-based to edge-based processing. Limited energy and computational resources on edge push the transition from traditional von Neumann architectures to In-memory Computing (IMC), especially for machine learning and neural network applications. Network compression techniques are applied to implement a neural network on limited hardware resources. Quantization is one of the most efficient network compression techniques allowing to reduce the memory footprint, latency, and energy consumption. This paper provides a comprehensive review of IMC-based Quantized Neural Networks (QNN) and links software-based quantization approaches to IMC hardware implementation. Moreover, open challenges, QNN design requirements, recommendations, and perspectives along with an IMC-based QNN hardware roadmap are provided.
</details>
<details>
<summary>æ‘˜è¦</summary>
äº‘è®¡ç®—ä¸­æ•°æ®å¤„ç†é‡ã€äº’è”ç½‘ç‰©è”ç½‘ï¼ˆIoTï¼‰åº”ç”¨çš„å‘å±•ä»¥åŠæ•°æ®éšç§é—®é¢˜çš„å¢åŠ ï¼Œå¯¼è‡´ä»äº‘åŸºç¡€åˆ°è¾¹ç¼˜åŸºç¡€çš„å¤„ç†è¿‡æ¸¡ã€‚è¾¹ç¼˜è®¾å¤‡çš„æœ‰é™èƒ½æºå’Œè®¡ç®—èµ„æºä½¿å¾—ä»ä¼ ç»Ÿ von Neumann æ¶æ„è¿‡æ¸¡åˆ°å†…å­˜è®¡ç®—ï¼ˆIMCï¼‰ï¼Œç‰¹åˆ«æ˜¯ Ğ´Ğ»Ñæœºå™¨å­¦ä¹ å’Œç¥ç»ç½‘ç»œåº”ç”¨ã€‚ä¸ºå®ç°é™åˆ¶æ€§ç¡¬ä»¶èµ„æºä¸Šçš„ç¥ç»ç½‘ç»œå®ç°ï¼Œç½‘ç»œå‹ç¼©æŠ€æœ¯è¢«åº”ç”¨ã€‚é‡åŒ–æ˜¯æœ€é«˜æ•ˆçš„ç½‘ç»œå‹ç¼©æŠ€æœ¯ï¼Œå¯ä»¥å‡å°‘å†…å­˜å ç”¨é‡ã€å»¶è¿Ÿå’Œèƒ½è€—ã€‚æœ¬æ–‡æä¾›äº†å…¨é¢çš„å†…å­˜è®¡ç®—åŸºäºé‡åŒ–ç¥ç»ç½‘ç»œï¼ˆQNNï¼‰å®¡æŸ¥ï¼Œå¹¶å°†è½¯ä»¶åŸºäºé‡åŒ–æ–¹æ³•ä¸IMCç¡¬ä»¶å®ç°ç›¸è¿ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†å¼€æ”¾çš„æŒ‘æˆ˜ã€QNNè®¾è®¡è¦æ±‚ã€å»ºè®®å’Œå‰ç»ï¼Œä»¥åŠIMCåŸºäºQNNç¡¬ä»¶è·¯çº¿å›¾ã€‚
</details></li>
</ul>
<hr>
<h2 id="Bounding-data-reconstruction-attacks-with-the-hypothesis-testing-interpretation-of-differential-privacy"><a href="#Bounding-data-reconstruction-attacks-with-the-hypothesis-testing-interpretation-of-differential-privacy" class="headerlink" title="Bounding data reconstruction attacks with the hypothesis testing interpretation of differential privacy"></a>Bounding data reconstruction attacks with the hypothesis testing interpretation of differential privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03928">http://arxiv.org/abs/2307.03928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georgios Kaissis, Jamie Hayes, Alexander Ziller, Daniel Rueckert</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†æ•°æ®é‡å»ºæ”»å‡»å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹çš„æˆåŠŸç‡ä¸Šçš„Upper boundï¼Œå³æ•°æ®é‡å»ºRobustnessï¼ˆReRoï¼‰ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†æ¸è¿‘ Monte Carlo ä¼°è®¡æ¥è®¡ç®— ReRo çš„ç´§è‡´ boundï¼Œä½†è¿™äº›ä¼°è®¡åªé€‚ç”¨äºç‰¹å®šçš„æ¸è¿‘DPæœºåˆ¶ã€‚æœ¬æ–‡åˆ™æå‡ºäº†åŸºäºå‡è®¾æµ‹è¯•DPå’ŒReRoçš„è¿æ¥ï¼Œå¹¶ deriveclosed-formã€åˆ†æçš„æˆ–æ•°å­— ReRo ä¸‹é™ Ğ´Ğ»Ñåˆ« Laplace å’Œ Gaussian æœºåˆ¶ä»¥åŠå®ƒä»¬çš„æŠ½æ ·variantã€‚</li>
<li>results: æœ¬ç ”ç©¶æå‡ºäº†å¯ç›´æ¥è®¡ç®—çš„ ReRo ä¸‹é™ Ğ´Ğ»Ñæ™®é€šçš„DPæœºåˆ¶ï¼ŒåŒ…æ‹¬Laplaceå’ŒGaussianæœºåˆ¶ä»¥åŠå®ƒä»¬çš„æŠ½æ ·variantã€‚è¿™äº›ä¸‹é™å¯ç”¨äºè¯„ä¼°æ•°æ®é‡å»ºæ”»å‡»çš„æˆåŠŸç‡ï¼Œå¹¶å¸®åŠ©é€‰æ‹©åˆé€‚çš„DPæœºåˆ¶ã€‚<details>
<summary>Abstract</summary>
We explore Reconstruction Robustness (ReRo), which was recently proposed as an upper bound on the success of data reconstruction attacks against machine learning models. Previous research has demonstrated that differential privacy (DP) mechanisms also provide ReRo, but so far, only asymptotic Monte Carlo estimates of a tight ReRo bound have been shown. Directly computable ReRo bounds for general DP mechanisms are thus desirable. In this work, we establish a connection between hypothesis testing DP and ReRo and derive closed-form, analytic or numerical ReRo bounds for the Laplace and Gaussian mechanisms and their subsampled variants.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æ¢ç´¢é‡å»ºé²æ£’æ€§ï¼ˆReRoï¼‰ï¼Œæœ€è¿‘æå‡ºçš„æ•°æ®é‡å»ºæ”»å‡»éšç§æ¨¡å‹çš„æˆåŠŸç‡ä¸Šé™ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œå·®åˆ†éšç§ï¼ˆDPï¼‰æœºåˆ¶ä¹Ÿæä¾›äº†ReRoï¼Œä½†åªæœ‰éæ­£å¼çš„è´å¶æ–¯ Monte Carlo ä¼°è®¡ã€‚å› æ­¤ï¼Œç›´æ¥è®¡ç®—å¯è¯»çš„ ReRo ä¸‹é™å¯¹æ™®é€šçš„ DP æœºåˆ¶æ˜¯æœ‰ä»·å€¼çš„ã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†å‡è®¾æµ‹è¯•DPä¸ReRoä¹‹é—´çš„è¿æ¥ï¼Œå¹¶å¯¹æ‹‰æ™®æ‹‰æ–¯å’Œé«˜æ–¯æœºåˆ¶åŠå…¶æŠ½æ ·å˜ä½“ derivation of closed-form, analytic or numerical ReRo bounds.
</details></li>
</ul>
<hr>
<h2 id="Applying-human-centered-AI-in-developing-effective-human-AI-teaming-A-perspective-of-human-AI-joint-cognitive-systems"><a href="#Applying-human-centered-AI-in-developing-effective-human-AI-teaming-A-perspective-of-human-AI-joint-cognitive-systems" class="headerlink" title="Applying human-centered AI in developing effective human-AI teaming: A perspective of human-AI joint cognitive systems"></a>Applying human-centered AI in developing effective human-AI teaming: A perspective of human-AI joint cognitive systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03913">http://arxiv.org/abs/2307.03913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Xu, Zaifeng Gao<br>for:* The paper focuses on the concept of human-AI teaming (HAT) as a new paradigm for developing AI systems, and the challenges and limitations of each member in human-AI collaboration.methods:* The paper proposes a conceptual framework of human-AI joint cognitive systems (HAIJCS) to represent and implement HAT for developing effective human-AI teaming.results:* The paper discusses the implications and future work for HAIJCS, and argues that HAIJCS may help adopt HAI while enabling HCAI.<details>
<summary>Abstract</summary>
Research and application have used human-AI teaming (HAT) as a new paradigm to develop AI systems. HAT recognizes that AI will function as a teammate instead of simply a tool in collaboration with humans. Effective human-AI teams need to be capable of taking advantage of the unique abilities of both humans and AI while overcoming the known challenges and limitations of each member, augmenting human capabilities, and raising joint performance beyond that of either entity. The National AI Research and Strategic Plan 2023 update has recognized that research programs focusing primarily on the independent performance of AI systems generally fail to consider the functionality that AI must provide within the context of dynamic, adaptive, and collaborative teams and calls for further research on human-AI teaming and collaboration. However, there has been debate about whether AI can work as a teammate with humans. The primary concern is that adopting the "teaming" paradigm contradicts the human-centered AI (HCAI) approach, resulting in humans losing control of AI systems. This article further analyzes the HAT paradigm and the debates. Specifically, we elaborate on our proposed conceptual framework of human-AI joint cognitive systems (HAIJCS) and apply it to represent HAT under the HCAI umbrella. We believe that HAIJCS may help adopt HAI while enabling HCAI. The implications and future work for HAIJCS are also discussed.   Insights: AI has led to the emergence of a new form of human-machine relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems; We must follow a human-centered AI (HCAI) approach when applying HAT as a new design paradigm; We propose a conceptual framework of human-AI joint cognitive systems (HAIJCS) to represent and implement HAT for developing effective human-AI teaming
</details>
<details>
<summary>æ‘˜è¦</summary>
ç ”ç©¶å’Œåº”ç”¨å·²ç»ä½¿ç”¨äººç±»-äººå·¥æ™ºèƒ½å›¢é˜Ÿï¼ˆHATï¼‰ä½œä¸ºæ–°çš„è®¾è®¡æ¨¡å¼å¼€å‘äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚HATè®¤å¯äººå·¥æ™ºèƒ½å°†ä½œä¸ºå›¢é˜Ÿæˆå‘˜è€Œä¸ä»…ä»…æ˜¯å·¥å…·å’Œäººç±»åˆä½œã€‚æœ‰æ•ˆçš„äººç±»-äººå·¥æ™ºèƒ½å›¢é˜Ÿéœ€è¦èƒ½å¤Ÿåˆ©ç”¨äººç±»å’Œäººå·¥æ™ºèƒ½çš„ç‰¹æ®Šèƒ½åŠ›ï¼Œè¶…è¶Šæ¯ä¸ªæˆå‘˜çš„çŸ¥é“çš„é™åˆ¶ï¼Œå¢å¼ºäººç±»èƒ½åŠ›ï¼Œå¹¶ä½¿å›¢é˜Ÿæ€§èƒ½é«˜äºä»»ä½•ä¸€ä¸ªæˆå‘˜ã€‚2023å¹´å›½å®¶äººå·¥æ™ºèƒ½ç ”ç©¶å’Œæˆ˜ç•¥è®¡åˆ’æ›´æ–°è®¤ä¸ºï¼Œç ”ç©¶ä¸“æ³¨äºç‹¬ç«‹è¿è¡Œçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„Programmesé€šå¸¸ä¸ä¼šè€ƒè™‘äººå·¥æ™ºèƒ½åœ¨åŠ¨æ€ã€é€‚åº”å’Œåä½œå›¢é˜Ÿä¸­çš„åŠŸèƒ½ï¼Œå¹¶å‘¼åè¿›ä¸€æ­¥ç ”ç©¶äººç±»-äººå·¥æ™ºèƒ½å›¢é˜Ÿå’Œåˆä½œã€‚ç„¶è€Œï¼Œæœ‰è®¨è®ºæ˜¯äººå·¥æ™ºèƒ½èƒ½å¤Ÿä½œä¸ºå›¢é˜Ÿæˆå‘˜ã€‚ä¸»è¦å…³æ³¨ç‚¹æ˜¯é‡‡ç”¨â€œå›¢é˜Ÿâ€æ¨¡å¼ä¼šè®©äººç±»å¤±å»å¯¹äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ§åˆ¶ã€‚æœ¬æ–‡è¿›ä¸€æ­¥åˆ†æHATæ¨¡å¼å’Œè¾©è®ºã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è¯¦ç»†é˜è¿°æˆ‘ä»¬çš„æå‡ºçš„äººç±»-äººå·¥æ™ºèƒ½å…±åŒè®¤çŸ¥ç³»ç»Ÿï¼ˆHAIJCSï¼‰æ¦‚å¿µæ¡†æ¶ï¼Œå¹¶å°†å…¶åº”ç”¨äºHCAIé¢†åŸŸä¸­çš„HATã€‚æˆ‘ä»¬è®¤ä¸ºHAIJCSå¯ä»¥å¸®åŠ©é‡‡ç”¨HAIï¼ŒåŒæ—¶ä¿æŒHCAIã€‚æœ¬æ–‡è¿˜è®¨è®ºäº†HAIJCSçš„æ„ä¹‰å’Œæœªæ¥å·¥ä½œã€‚
</details></li>
</ul>
<hr>
<h2 id="ScriptWorld-Text-Based-Environment-For-Learning-Procedural-Knowledge"><a href="#ScriptWorld-Text-Based-Environment-For-Learning-Procedural-Knowledge" class="headerlink" title="ScriptWorld: Text Based Environment For Learning Procedural Knowledge"></a>ScriptWorld: Text Based Environment For Learning Procedural Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03906">http://arxiv.org/abs/2307.03906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploration-lab/scriptworld">https://github.com/exploration-lab/scriptworld</a></li>
<li>paper_authors: Abhinav Joshi, Areeb Ahmad, Umang Pandey, Ashutosh Modi</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ªåŸºäºå¥–åŠ±å­¦ä¹ çš„æ–‡æœ¬ç¯å¢ƒï¼Œç”¨äºå¸®åŠ©ä»£ç†äººå­¦ä¹ æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¸¸è¯†çŸ¥è¯†å’Œè‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ä¸ªåä¸ºScriptWorldçš„æ–‡æœ¬ç¯å¢ƒï¼Œå…¶ä¸­åŒ…å«äº†10ç§æ—¥å¸¸ç”Ÿæ´»ä¸­çš„æ´»åŠ¨ï¼Œå¹¶å¯¹è¿™äº›æ´»åŠ¨è¿›è¡Œäº†è¯¦ç»†çš„åˆ†æã€‚furthermore, the authors use reinforcement learning-based baseline models&#x2F;agents to play the games in Scriptworld, and leverage features obtained from pre-trained language models to understand the role of language models in such environments.</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œç”±äºä½¿ç”¨äº†é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œä»£ç†äººå¯ä»¥æ›´å¥½åœ°è§£å†³æ—¥å¸¸ç”Ÿæ´»ä¸­çš„æ–‡æœ¬åŸºäºå¥–åŠ±å­¦ä¹ ç¯å¢ƒã€‚<details>
<summary>Abstract</summary>
Text-based games provide a framework for developing natural language understanding and commonsense knowledge about the world in reinforcement learning based agents. Existing text-based environments often rely on fictional situations and characters to create a gaming framework and are far from real-world scenarios. In this paper, we introduce ScriptWorld: a text-based environment for teaching agents about real-world daily chores and hence imparting commonsense knowledge. To the best of our knowledge, it is the first interactive text-based gaming framework that consists of daily real-world human activities designed using scripts dataset. We provide gaming environments for 10 daily activities and perform a detailed analysis of the proposed environment. We develop RL-based baseline models/agents to play the games in Scriptworld. To understand the role of language models in such environments, we leverage features obtained from pre-trained language models in the RL agents. Our experiments show that prior knowledge obtained from a pre-trained language model helps to solve real-world text-based gaming environments. We release the environment via Github: https://github.com/Exploration-Lab/ScriptWorld
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Prototypical-Part-Networks-with-Reward-Reweighing-Reselection-and-Retraining"><a href="#Improving-Prototypical-Part-Networks-with-Reward-Reweighing-Reselection-and-Retraining" class="headerlink" title="Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining"></a>Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03887">http://arxiv.org/abs/2307.03887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Netzorg, Jiaxun Li, Bin Yu</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºæ·±åº¦å¯è¯»æ€§æ–¹æ³•çš„å›¾åƒåˆ†ç±»æ–¹æ³•ï¼Œä»¥ä¾¿ä»å›¾åƒä¸­æå–æœ‰æ„ä¹‰çš„ç‰¹å¾æ¥è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>methods: è¿™ç§æ–¹æ³•æ˜¯åŸºäºprotoypical part networkï¼ˆProtoPNetï¼‰ï¼Œå®ƒå°è¯•é€šè¿‡åˆ†æå›¾åƒçš„å„ä¸ªéƒ¨åˆ†æ¥è¿›è¡Œåˆ†ç±»ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ç»å¸¸ä¼šä»å›¾åƒä¸­å­¦ä¹ æ— ç”¨æˆ–ä¸ä¸€è‡´çš„éƒ¨åˆ†æ¥è¿›è¡Œåˆ†ç±»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™ä¸ªè®ºæ–‡å¼•ç”¨äº†äººå·¥æ™ºèƒ½åé¦ˆå­¦ä¹ ï¼ˆRLHFï¼‰çš„æœ€è¿‘å‘å±•ï¼Œä»¥ä¾¿ä¸ºProtoPNetè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡æ”¶é›†CUB-200-2011 datasetä¸Šçš„äººå·¥æ ‡æ³¨ï¼Œæ„å»ºä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œä»¥ä¾¿è¯†åˆ«éæ— ç”¨çš„åŸå‹ã€‚</li>
<li>results: é€šè¿‡åœ¨ProtoPNetè®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ å¥–åŠ±æ¨¡å‹ã€é‡æ–°é€‰æ‹©å’Œé‡æ–°è®­ç»ƒåŸå‹ï¼Œæå‡ºäº†ä¸€ç§åä¸ºR3-ProtoPNetçš„æ–°æ–¹æ³•ã€‚R3-ProtoPNetå¯ä»¥æé«˜å›¾åƒåˆ†ç±»ä¸­çš„æ€»ä½“ä¸€è‡´æ€§å’Œæœ‰æ„ä¹‰æ€§ï¼Œä½†æ˜¯ç‹¬ç«‹ä½¿ç”¨R3-ProtoPNetæ—¶ä¼šä¸‹é™æµ‹è¯•é¢„æµ‹ç²¾åº¦ã€‚ç„¶è€Œï¼Œå°†å¤šä¸ªR3-ProtoPNetç»„åˆæˆensembleæ—¶ï¼Œå¯ä»¥æé«˜æµ‹è¯•é¢„æµ‹æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå¯è¯»æ€§ã€‚<details>
<summary>Abstract</summary>
In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The first two steps are reward-based reweighting and reselection, which align prototypes with human feedback. The final step is retraining to realign the model's features with the updated prototypes. We find that R3-ProtoPNet improves the overall consistency and meaningfulness of the prototypes, but lower the test predictive accuracy when used independently. When multiple R3-ProtoPNets are incorporated into an ensemble, we find an increase in test predictive performance while maintaining interpretability.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘å¹´æ¥ï¼Œæœ‰å¾ˆå¤šå·¥ä½œåœ¨å‘å±•æ·±å…¥å¯è§£é‡Šçš„å›¾åƒåˆ†ç±»æ–¹æ³•ï¼Œä»¥ä¾¿æ¸…æ™°åœ°å½’å› æ¨¡å‹çš„è¾“å‡ºåˆ°ç‰¹å®šçš„æ•°æ®ç‰¹å¾ã€‚ä¸€ç§è¿™äº›æ–¹æ³•æ˜¯ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒéƒ¨åˆ†ç½‘ç»œï¼ˆProtoPNetï¼‰ï¼Œå®ƒå°è¯•é€šè¿‡åŸºäºå›¾åƒçš„æ„ä¹‰éƒ¨åˆ†æ¥åˆ†ç±»å›¾åƒã€‚è™½ç„¶è¿™ç§æ–¹æ³•å¯ä»¥å¾—åˆ°å¯è§£é‡Šçš„åˆ†ç±»ç»“æœï¼Œä½†å®ƒç»å¸¸ä»ä¸å®‰å®šæˆ–ä¸ä¸€è‡´çš„å›¾åƒéƒ¨åˆ†è¿›è¡Œåˆ†ç±»ã€‚ä¸ºäº†çº æ­£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å–å¾—äº†äººç±»å¯¹ Ğ¿Ñ€Ğ¾totypes è´¨é‡çš„æ³¨é‡Šï¼ˆé€šè¿‡ CUB-200-2011 æ•°æ®é›†ä¸Šçš„ä¸€ä¸ª5çº§è¯„åˆ†ç³»ç»Ÿï¼‰ï¼Œå¹¶æ ¹æ®è¿™äº›æ³¨é‡Šæ„å»ºäº†ä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œå¯ä»¥è¯†åˆ«éå®‰å®šçš„ prototypesã€‚è€Œä¸æ˜¯æ•´ä¸ªRLæ›´æ–°ï¼Œæˆ‘ä»¬æè®®ä¸€ç§åä¸º R3-ProtoPNet çš„æ–¹æ³•ï¼Œå®ƒåœ¨ ProtoPNet è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ äº†ä¸‰ä¸ªé¢å¤–æ­¥éª¤ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å¥–åŠ±æ¥é‡æ–°æƒé‡å’Œé€‰æ‹© prototypesï¼Œä»¥ä½¿å…¶ä¸äººç±»åé¦ˆç›¸å»åˆã€‚ç„¶åï¼Œæˆ‘ä»¬é‡æ–°è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾ï¼Œä»¥ä¾¿ä¸æ›´æ–°åçš„ prototypes è¿›è¡Œå¯¹é½ã€‚æˆ‘ä»¬å‘ç° R3-ProtoPNet å¯ä»¥æé«˜æ€»çš„ä¸€è‡´æ€§å’Œæ„ä¹‰æ€§ï¼Œä½†åœ¨ç‹¬ç«‹ä½¿ç”¨æ—¶æµ‹è¯•é¢„æµ‹ç²¾åº¦ç›¸å¯¹è¾ƒä½ã€‚ç„¶è€Œï¼Œå½“å¤šä¸ª R3-ProtoPNets è¢«ç»„åˆæˆensembleæ—¶ï¼Œæˆ‘ä»¬å‘ç°æµ‹è¯•é¢„æµ‹æ€§èƒ½å¾—åˆ°äº†æé«˜ï¼ŒåŒæ—¶ä¿æŒäº†å¯è§£é‡Šæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Designing-Mixed-Initiative-Video-Games"><a href="#Designing-Mixed-Initiative-Video-Games" class="headerlink" title="Designing Mixed-Initiative Video Games"></a>Designing Mixed-Initiative Video Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03877">http://arxiv.org/abs/2307.03877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daijin Yang<br>for: This paper aims to explore the use of gamification in mixed-initiative co-creation to make human-AI interactions more accessible and fun.methods: The author prototyped a game called Snake Story, where players can select AI-generated texts to write a story of a snake by playing a â€œSnakeâ€ like game. A controlled experiment was conducted to compare player-AI interactions with and without the game component.results: The study found that players utilized different strategies when playing with the two versions, game mechanics significantly affected the output stories, playersâ€™ creative process, and playersâ€™ role perceptions. Additionally, players with different backgrounds showed different preferences for the two versions.<details>
<summary>Abstract</summary>
The development of Artificial Intelligence (AI) enables humans to co-create content with machines. The unexpectedness of AI-generated content can bring inspiration and entertainment to users. However, the co-creation interactions are always designed for content creators and have poor accessibility. To explore gamification of mixed-initiative co-creation and make human-AI interactions accessible and fun for players, I prototyped Snake Story, a mixed-initiative game where players can select AI-generated texts to write a story of a snake by playing a "Snake" like game. A controlled experiment was conducted to investigate the dynamics of player-AI interactions with and without the game component in the designed interface. As a result of a study with 11 players (n=11), I found that players utilized different strategies when playing with the two versions, game mechanics significantly affected the output stories, players' creative process, as well as role perceptions, and players with different backgrounds showed different preferences for the two versions. Based on these results, I further discussed considerations for mixed-initiative game design. This work aims to inspire the design of engaging co-creation experiences.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å‘å±•ä½¿å¾—äººä»¬å¯ä»¥ä¸æœºå™¨å…±åŒåˆ›ä½œå†…å®¹ã€‚AIç”Ÿæˆçš„å†…å®¹çš„ä¸å¯é¢„æµ‹æ€§å¯ä»¥ç»™ç”¨æˆ·å¸¦æ¥åˆ›æ„å’Œå¨±ä¹ã€‚ç„¶è€Œï¼Œå…±åŒåˆ›ä½œäº¤äº’éƒ½æ˜¯ä¸ºå†…å®¹åˆ›ä½œè€…è®¾è®¡çš„ï¼Œè€Œä¸”è®¿é—®æ€§å¾ˆå·®ã€‚ä¸ºäº†æ¢ç´¢æ‚åˆå¼å…±åŒåˆ›ä½œçš„æ¸¸æˆåŒ–å’Œäººæœºäº¤äº’çš„å¯ä¹æ€§ï¼Œæˆ‘è®¾è®¡äº†è›‡æ•…äº‹ï¼Œä¸€æ¬¾æ‚åˆå¼æ¸¸æˆï¼Œå…¶ä¸­ç©å®¶å¯ä»¥é€šè¿‡é€‰æ‹©AIç”Ÿæˆçš„æ–‡æœ¬æ¥å†™ä¸€ä¸ªè›‡çš„æ•…äº‹ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹æ§åˆ¶å®éªŒï¼Œå¹¶ä¸11åç©å®¶è¿›è¡Œäº†æµ‹è¯•ï¼ˆn=11ï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œç©å®¶åœ¨ä¸¤ä¸ªç‰ˆæœ¬ä¸­ä½¿ç”¨äº†ä¸åŒçš„ç­–ç•¥ï¼Œæ¸¸æˆæœºåˆ¶å¯¹è¾“å‡ºæ•…äº‹ã€ç©å®¶çš„åˆ›ä½œè¿‡ç¨‹ä»¥åŠç©å®¶çš„è§’è‰²è®¤çŸ¥äº§ç”Ÿäº†æ˜¾è‘—å½±å“ï¼Œè€Œä¸åŒèƒŒæ™¯çš„ç©å®¶ä¹Ÿè¡¨ç°å‡ºäº†ä¸åŒçš„åå¥½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œåœ¨è®¾è®¡æ‚åˆå¼æ¸¸æˆæ—¶éœ€è¦è€ƒè™‘ä¸€äº›è€ƒé‡ã€‚è¿™é¡¹å·¥ä½œçš„ç›®çš„æ˜¯é¼“åŠ±è®¾è®¡æœ‰è¶£çš„å…±åŒåˆ›ä½œç»éªŒã€‚
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-Supply-Chain-Optimization"><a href="#Large-Language-Models-for-Supply-Chain-Optimization" class="headerlink" title="Large Language Models for Supply Chain Optimization"></a>Large Language Models for Supply Chain Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03875">http://arxiv.org/abs/2307.03875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, Ishai Menache</li>
<li>for: è¯¥ç ”ç©¶æ—¨åœ¨ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æé«˜ä¾›åº”é“¾è‡ªåŠ¨åŒ–çš„å¯ç†è§£åº¦å’Œä¿¡ä»»åº¦ã€‚</li>
<li>methods: ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸º OptiGuide çš„æ¡†æ¶ï¼Œå¯ä»¥æ¥å—æ™®é€šæ–‡æœ¬æŸ¥è¯¢ï¼Œå¹¶è¾“å‡ºä¾›åº”é“¾ä¼˜åŒ–ç»“æœçš„æ¦‚å¿µæ€§è§£é‡Šã€‚è¯¥æ¡†æ¶ä¸ä¼šæŠ›å¼ƒç°æœ‰çš„å¯combined optimizationæŠ€æœ¯ï¼Œè€Œæ˜¯é€šè¿‡è§£å†³ what-if é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨æä¾›å•† B è€Œä¸æ˜¯æä¾›å•† A æ¥æ»¡è¶³æŸä¸ªéœ€æ±‚ï¼Œåˆ™cost ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿï¼‰æ¥æä¾›å¯è¡¡é‡çš„ç­”æ¡ˆã€‚</li>
<li>results: ç ”ç©¶åœ¨ Microsoft äº‘ä¾›åº”é“¾ä¸­å®ç°äº†ä¸€ä¸ªçœŸå®çš„æœåŠ¡å™¨åˆ†å¸ƒå¼enarioï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªé€šç”¨çš„è¯„ä¼°æ ‡å‡†ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°å…¶ä»–æƒ…å†µä¸‹ LLM è¾“å‡ºçš„å‡†ç¡®æ€§ã€‚<details>
<summary>Abstract</summary>
Supply chain operations traditionally involve a variety of complex decision making problems. Over the last few decades, supply chains greatly benefited from advances in computation, which allowed the transition from manual processing to automation and cost-effective optimization. Nonetheless, business operators still need to spend substantial efforts in explaining and interpreting the optimization outcomes to stakeholders. Motivated by the recent advances in Large Language Models (LLMs), we study how this disruptive technology can help bridge the gap between supply chain automation and human comprehension and trust thereof. We design OptiGuide -- a framework that accepts as input queries in plain text, and outputs insights about the underlying optimization outcomes. Our framework does not forgo the state-of-the-art combinatorial optimization technology, but rather leverages it to quantitatively answer what-if scenarios (e.g., how would the cost change if we used supplier B instead of supplier A for a given demand?). Importantly, our design does not require sending proprietary data over to LLMs, which can be a privacy concern in some circumstances. We demonstrate the effectiveness of our framework on a real server placement scenario within Microsoft's cloud supply chain. Along the way, we develop a general evaluation benchmark, which can be used to evaluate the accuracy of the LLM output in other scenarios.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¾›åº”é“¾æ“ä½œæ¶‰åŠåˆ°è®¸å¤šå¤æ‚çš„å†³ç­–é—®é¢˜ã€‚è¿‡å»å‡ åå¹´ï¼Œä¾›åº”é“¾å—åˆ°è®¡ç®—æŠ€æœ¯çš„è¿›æ­¥ï¼Œä»æ‰‹åŠ¨å¤„ç†è½¬å˜åˆ°è‡ªåŠ¨åŒ–å’Œæˆæœ¬æ•ˆæœçš„ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œå•†ä¸šè¿è¥è€…ä»éœ€æŠ•å…¥å¤§é‡çš„åŠªåŠ›æ¥è§£é‡Šå’Œè§£è¯»ä¼˜åŒ–ç»“æœç»™æ½œåœ¨æŠ•èµ„è€…ã€‚é¼“åŠ±äº†æœ€è¿‘çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ï¼Œæˆ‘ä»¬ç ”ç©¶å¦‚ä½•ä½¿ç”¨è¿™ç§ç ´åæŠ€æœ¯æ¥bridgingä¾›åº”é“¾è‡ªåŠ¨åŒ–å’Œäººç±»ç†è§£å’Œä¿¡ä»»ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬è®¾è®¡äº†OptiGuideæ¡†æ¶ï¼Œå®ƒæ¥å—è¾“å…¥æ˜¯ç®€å•æ–‡æœ¬é—®é¢˜ï¼Œå¹¶è¾“å‡ºä¾›åº”é“¾ä¼˜åŒ–ç»“æœçš„å‡è§†ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ä¼šæŠ›å¼ƒç°æœ‰çš„ç»„åˆä¼˜åŒ–æŠ€æœ¯ï¼Œè€Œæ˜¯åˆ©ç”¨å®ƒæ¥å›ç­”å…·æœ‰ä¼˜åŒ–ç»“æœçš„what-ifenarioï¼ˆä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨supplier Bè€Œä¸æ˜¯supplier Aæ¥æ»¡è¶³ä¸€ä¸ªéœ€æ±‚ï¼Œ Then what would be the cost?ï¼‰ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è®¾è®¡ä¸ä¼šå°†ä¸“æœ‰æ•°æ®ä¼ è¾“åˆ°LLMï¼Œè¿™å¯èƒ½ä¼šåœ¨æŸäº›æƒ…å†µä¸‹æˆä¸ºéšç§é—®é¢˜ã€‚æˆ‘ä»¬åœ¨Microsoftäº‘ä¾›åº”é“¾ä¸­è¿›è¡Œäº†ä¸€ä¸ªçœŸå®çš„æœåŠ¡éƒ¨ç½²åœºæ™¯ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªé€šç”¨çš„è¯„ä¼°æ ‡å‡†ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°LLMè¾“å‡ºçš„å‡†ç¡®æ€§åœ¨å…¶ä»–åœºæ™¯ä¸­ã€‚
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-using-Silver-Standard-Labels-for-Ki-67-Scoring-in-Digital-Pathology-A-Step-Closer-to-Widescale-Deployment"><a href="#Domain-Adaptation-using-Silver-Standard-Labels-for-Ki-67-Scoring-in-Digital-Pathology-A-Step-Closer-to-Widescale-Deployment" class="headerlink" title="Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital Pathology: A Step Closer to Widescale Deployment"></a>Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital Pathology: A Step Closer to Widescale Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03872">http://arxiv.org/abs/2307.03872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amanda Dy, Ngoc-Nhu Jennifer Nguyen, Seyed Hossein Mirjahanmardi, Melanie Dawe, Anthony Fyles, Wei Shi, Fei-Fei Liu, Dimitrios Androutsos, Susan Done, April Khademi</li>
<li>for: æé«˜åŸºå› çŸ«æ­£è¯„åˆ†çš„å¯¹ Ğ¾Ğ±ÑŠĞµĞºivityå’Œæ•ˆç‡</li>
<li>methods: ä½¿ç”¨æ— ç›‘ç£æ¡†æ¶ç”Ÿæˆç›®æ ‡é¢†åŸŸçš„é“¶æ ‡ç­¾ï¼Œå¹¶å°†é“¶æ ‡ç­¾ä¸æ¥æºé¢†åŸŸçš„é‡‘æ ‡ç­¾ï¼ˆGSï¼‰ä¸€èµ·ä½¿ç”¨è¿›è¡Œè®­ç»ƒ</li>
<li>results: åœ¨ä¸¤ç§éªŒè¯çš„åŸºå› çŸ«æ­£æ¶æ„ï¼ˆUV-Netå’ŒpiNETï¼‰ä¸Šæµ‹è¯•äº†äº”ç§è®­ç»ƒæ–¹æ¡ˆï¼Œå…¶ä¸­SS+GSæ–¹æ³•åœ¨ç›®æ ‡æ•°æ®ä¸Šæ˜¾ç¤ºå‡ºæœ€é«˜çš„PIå‡†ç¡®ç‡ï¼ˆ95.9%ï¼‰å’Œæ›´ä¸€è‡´çš„ç»“æœï¼Œå¹¶ä¸”åˆ†æäº†t-SNEå›¾è¡¨è¡¨æ˜SS+GSæ¨¡å‹å­¦ä¹ çš„ç‰¹å¾æ›´åŠ é€‚åˆæºå’Œç›®æ ‡æ•°æ®ï¼Œä»è€Œæé«˜äº†é€šç”¨æ€§ã€‚<details>
<summary>Abstract</summary>
Deep learning systems have been proposed to improve the objectivity and efficiency of Ki- 67 PI scoring. The challenge is that while very accurate, deep learning techniques suffer from reduced performance when applied to out-of-domain data. This is a critical challenge for clinical translation, as models are typically trained using data available to the vendor, which is not from the target domain. To address this challenge, this study proposes a domain adaptation pipeline that employs an unsupervised framework to generate silver standard (pseudo) labels in the target domain, which is used to augment the gold standard (GS) source domain data. Five training regimes were tested on two validated Ki-67 scoring architectures (UV-Net and piNET), (1) SS Only: trained on target silver standard (SS) labels, (2) GS Only: trained on source GS labels, (3) Mixed: trained on target SS and source GS labels, (4) GS+SS: trained on source GS labels and fine-tuned on target SS labels, and our proposed method (5) SS+GS: trained on source SS labels and fine-tuned on source GS labels. The SS+GS method yielded significantly (p < 0.05) higher PI accuracy (95.9%) and more consistent results compared to the GS Only model on target data. Analysis of t-SNE plots showed features learned by the SS+GS models are more aligned for source and target data, resulting in improved generalization. The proposed pipeline provides an efficient method for learning the target distribution without manual annotations, which are time-consuming and costly to generate for medical images. This framework can be applied to any target site as a per-laboratory calibration method, for widescale deployment.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ ç³»ç»Ÿå¯ä»¥æé«˜ Ki-67 PI åˆ†æ•°çš„ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²æ€§å’Œæ•ˆç‡ã€‚ç„¶è€Œï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨ä¸åŒé¢†åŸŸæ•°æ®ä¸Šè¡¨ç°ä¸ä½³ï¼Œè¿™æ˜¯ä¸´åºŠç¿»è¯‘ä¸­çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé¢†åŸŸé€‚åº”ç®¡é“ï¼Œä½¿ç”¨æ— ç›‘ç£æ¡†æ¶ç”Ÿæˆç›®æ ‡é¢†åŸŸçš„é“¶æ ‡ç­¾ï¼ˆpseudo labelsï¼‰ï¼Œå¹¶å°†å…¶ä¸æºé¢†åŸŸçš„é»„é‡‘æ ‡ç­¾ï¼ˆGSï¼‰ä¸€èµ·ä½¿ç”¨æ¥è¡¥å……æ•°æ®ã€‚æœ¬ç ”ç©¶æµ‹è¯•äº†äº”ç§è®­ç»ƒæ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰SS Onlyï¼šåœ¨ç›®æ ‡é¢†åŸŸçš„é“¶æ ‡ç­¾ï¼ˆSSï¼‰ä¸Šè®­ç»ƒï¼Œï¼ˆ2ï¼‰GS Onlyï¼šåœ¨æºé¢†åŸŸçš„é»„é‡‘æ ‡ç­¾ï¼ˆGSï¼‰ä¸Šè®­ç»ƒï¼Œï¼ˆ3ï¼‰Mixedï¼šåœ¨ç›®æ ‡é¢†åŸŸçš„é“¶æ ‡ç­¾å’Œæºé¢†åŸŸçš„é»„é‡‘æ ‡ç­¾ä¸Šè®­ç»ƒï¼Œï¼ˆ4ï¼‰GS+SSï¼šåœ¨æºé¢†åŸŸçš„é»„é‡‘æ ‡ç­¾ä¸Šè®­ç»ƒï¼Œç„¶ååœ¨ç›®æ ‡é¢†åŸŸçš„é“¶æ ‡ç­¾ä¸Šè¿›è¡Œç»†åŒ–ï¼Œä»¥åŠæˆ‘ä»¬çš„æè®®æ–¹æ³•ï¼ˆ5ï¼‰SS+GSï¼šåœ¨æºé¢†åŸŸçš„é“¶æ ‡ç­¾ä¸Šè®­ç»ƒï¼Œç„¶ååœ¨æºé¢†åŸŸçš„é»„é‡‘æ ‡ç­¾ä¸Šè¿›è¡Œç»†åŒ–ã€‚SS+GS æ–¹æ³•åœ¨ç›®æ ‡æ•°æ®ä¸Šæ˜¾ç¤ºå‡ºäº†æ˜æ˜¾é«˜äºGS Onlyæ¨¡å‹çš„PIå‡†ç¡®ç‡ï¼ˆ95.9%ï¼‰å’Œæ›´ä¸€è‡´çš„ç»“æœã€‚åˆ†æt-SNEå›¾ç¤ºäº†SS+GSæ¨¡å‹å­¦ä¹ çš„ç‰¹å¾ä¸æºå’Œç›®æ ‡æ•°æ®æ›´åŠ ä¸€è‡´ï¼Œå¯¼è‡´äº†æé«˜äº†æ€»ä½“åŒ–ã€‚è¿™ç§ç®¡é“å¯ä»¥é«˜æ•ˆåœ°å­¦ä¹ ç›®æ ‡åˆ†å¸ƒè€Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ï¼Œè¿™äº›æ ‡æ³¨æ˜¯åŒ»ç–—å›¾åƒç”Ÿæˆçš„æ—¶é—´æ¶ˆè€—å’Œæˆæœ¬é«˜æ˜‚çš„ã€‚è¿™ç§æ¡†æ¶å¯ä»¥åœ¨ä»»ä½•ç›®æ ‡ç«™ç‚¹ä¸Šè¿›è¡Œå„å®éªŒå®¤çš„è°ƒæ•´ï¼Œä¸ºå¹¿æ³›éƒ¨ç½²å‡†å¤‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Personalized-Resource-Allocation-in-Wireless-Networks-An-AI-Enabled-and-Big-Data-Driven-Multi-Objective-Optimization"><a href="#Personalized-Resource-Allocation-in-Wireless-Networks-An-AI-Enabled-and-Big-Data-Driven-Multi-Objective-Optimization" class="headerlink" title="Personalized Resource Allocation in Wireless Networks: An AI-Enabled and Big Data-Driven Multi-Objective Optimization"></a>Personalized Resource Allocation in Wireless Networks: An AI-Enabled and Big Data-Driven Multi-Objective Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03867">http://arxiv.org/abs/2307.03867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rawan Alkurd, Ibrahim Abualhaol, Halim Yanikomeroglu</li>
<li>for: æœ¬æ–‡ä¸»è¦ç”¨äºæè¿°å¦‚ä½•ä½¿ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æŠ€æœ¯æ¥ä¼˜åŒ–æ— çº¿ç½‘ç»œè®¾è®¡å’Œä¼˜åŒ–ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†AIæŠ€æœ¯æ¥å®ç°ç”¨æˆ·æ°´å¹³ä¸ªæ€§åŒ–ï¼Œå¹¶ä¸”æå‡ºäº†ä¸€ç§åŸºäºå¤§æ•°æ®çš„æ™ºèƒ½å±‚æ¥å¾®ç®¡æ— çº¿ç½‘ç»œèµ„æºã€‚</li>
<li>results: æ ¹æ®æœ¬æ–‡æè¿°ï¼Œä½¿ç”¨AIæŠ€æœ¯å¯ä»¥åœ¨æ— çº¿ç½‘ç»œä¸­å®ç°ç”¨æˆ·æ°´å¹³ä¸ªæ€§åŒ–ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å®æ—¶ä¸­å¾®è°ƒç½‘ç»œèµ„æºä»¥æé«˜ç”¨æˆ·æ»¡æ„åº¦å’Œèµ„æºåˆ©ç”¨ç‡ã€‚<details>
<summary>Abstract</summary>
The design and optimization of wireless networks have mostly been based on strong mathematical and theoretical modeling. Nonetheless, as novel applications emerge in the era of 5G and beyond, unprecedented levels of complexity will be encountered in the design and optimization of the network. As a result, the use of Artificial Intelligence (AI) is envisioned for wireless network design and optimization due to the flexibility and adaptability it offers in solving extremely complex problems in real-time. One of the main future applications of AI is enabling user-level personalization for numerous use cases. AI will revolutionize the way we interact with computers in which computers will be able to sense commands and emotions from humans in a non-intrusive manner, making the entire process transparent to users. By leveraging this capability, and accelerated by the advances in computing technologies, wireless networks can be redesigned to enable the personalization of network services to the user level in real-time. While current wireless networks are being optimized to achieve a predefined set of quality requirements, the personalization technology advocated in this article is supported by an intelligent big data-driven layer designed to micro-manage the scarce network resources. This layer provides the intelligence required to decide the necessary service quality that achieves the target satisfaction level for each user. Due to its dynamic and flexible design, personalized networks are expected to achieve unprecedented improvements in optimizing two contradicting objectives in wireless networks: saving resources and improving user satisfaction levels.
</details>
<details>
<summary>æ‘˜è¦</summary>
wireless ç½‘ç»œçš„è®¾è®¡å’Œä¼˜åŒ–æ›¾ä¸»è¦åŸºäºå¼ºå¤§çš„æ•°å­¦å’Œç†è®ºæ¨¡å‹ã€‚ç„¶è€Œï¼Œéšç€5Gå’Œä»¥åçš„åº”ç”¨å‡ºç°ï¼Œ wireless ç½‘ç»œçš„è®¾è®¡å’Œä¼˜åŒ–å°†é¢ä¸´æ— å‰ä¾‹çš„å¤æ‚æ€§ã€‚å› æ­¤ï¼Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å°†åœ¨æ— çº¿ç½‘ç»œè®¾è®¡å’Œä¼˜åŒ–ä¸­æ‰®æ¼”é‡è¦çš„ä½œç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥åœ¨å®æ—¶è§£å†³æå…¶å¤æ‚çš„é—®é¢˜ä¸­æä¾›çµæ´»å’Œé€‚åº”æ€§ã€‚æœªæ¥ï¼ŒAI å°†åœ¨æ— çº¿ç½‘ç»œè®¾è®¡å’Œä¼˜åŒ–ä¸­æ‰®æ¼”é‡è¦çš„ä½œç”¨ï¼Œå…è®¸ç”¨æˆ·æ°´å¹³ä¸ªæ€§åŒ–ã€‚é€šè¿‡æ„ŸçŸ¥äººç±»çš„å‘½ä»¤å’Œæƒ…æ„Ÿï¼Œè®¡ç®—æœºå°†æˆä¸ºä¸ä¾µå…¥çš„ï¼Œä½¿æ•´ä¸ªè¿‡ç¨‹é€æ˜ç»™ç”¨æˆ·ã€‚é€šè¿‡è¿™ç§èƒ½åŠ›ï¼Œå¹¶ä¸”å—è®¡ç®—æŠ€æœ¯çš„åŠ é€Ÿï¼Œæ— çº¿ç½‘ç»œå¯ä»¥è¢«é‡æ–°è®¾è®¡ï¼Œä»¥å®æ—¶ä¸ªæ€§åŒ–ç½‘ç»œæœåŠ¡ï¼Œä»¥è¾¾åˆ°æ¯ä¸ªç”¨æˆ·çš„ç›®æ ‡æ»¡æ„åº¦ã€‚å½“å‰çš„æ— çº¿ç½‘ç»œè¢«ä¼˜åŒ–ä»¥å®ç°ä¸€ç»„é¢„å®šçš„è´¨é‡è¦æ±‚ï¼Œè€Œäººæ€§åŒ–æŠ€æœ¯åœ¨æœ¬æ–‡ä¸­æå‡ºçš„ååŠ©å±‚ï¼Œé€šè¿‡å¤§æ•°æ®é©±åŠ¨çš„æ™ºèƒ½å±‚ï¼Œä¸ºç¼ºä¹èµ„æºçš„ç½‘ç»œèµ„æºè¿›è¡Œå¾®ç®¡ç†ã€‚è¿™å±‚æä¾›äº†å¿…è¦çš„æ™ºèƒ½ï¼Œä»¥ç¡®å®šæ¯ä¸ªç”¨æˆ·éœ€è¦çš„æœåŠ¡è´¨é‡ï¼Œä»¥è¾¾åˆ°ç›®æ ‡æ»¡æ„åº¦æ°´å¹³ã€‚ç”±äºå®ƒçš„åŠ¨æ€å’Œçµæ´»è®¾è®¡ï¼Œä¸ªæ€§åŒ–ç½‘ç»œé¢„è®¡ä¼šå®ç°æ— å‰ä¾‹çš„æ”¹å–„ï¼Œåœ¨æ— çº¿ç½‘ç»œä¸­æ”¹å–„ä¸¤ä¸ªçŸ›ç›¾ç›®æ ‡ï¼šèŠ‚çº¦èµ„æºå’Œæé«˜ç”¨æˆ·æ»¡æ„åº¦æ°´å¹³ã€‚
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-and-Deep-Reinforcement-Learning-based-Solutions-for-Machine-Maintenance-Planning-Scheduling-Policies-and-Optimization"><a href="#Reinforcement-and-Deep-Reinforcement-Learning-based-Solutions-for-Machine-Maintenance-Planning-Scheduling-Policies-and-Optimization" class="headerlink" title="Reinforcement and Deep Reinforcement Learning-based Solutions for Machine Maintenance Planning, Scheduling Policies, and Optimization"></a>Reinforcement and Deep Reinforcement Learning-based Solutions for Machine Maintenance Planning, Scheduling Policies, and Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03860">http://arxiv.org/abs/2307.03860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oluwaseyi Ogunfowora, Homayoun Najjaran</li>
<li>for: æœ¬ç ”ç©¶ç›®çš„æ˜¯å¯¹ç»´æŠ¤è§„åˆ’å’Œä¼˜åŒ–é—®é¢˜çš„åº”ç”¨ Reinforcement Learning è¿›è¡Œæ–‡çŒ®æŸ¥è¯¢å’Œåˆ†æã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº† Reinforcement Learning å’Œæ·±åº¦ Reinforcement Learning ç­‰æ•°æ®é©±åŠ¨å†³ç­–ç®—æ³•æ¥å¼€å‘æ™ºèƒ½ç»´æŠ¤è®¡åˆ’ã€‚</li>
<li>results: æœ¬ç ”ç©¶é€šè¿‡å¯¹æ–‡çŒ®è¿›è¡Œåˆ†ç±»å’Œæ‘˜è¦ï¼Œæå‡ºäº†ç»´æŠ¤è§„åˆ’å’Œä¼˜åŒ–é—®é¢˜çš„å…±åŒä¸»é¢˜å’Œç ”ç©¶æ–¹æ³•ï¼ŒåŒæ—¶è¿˜æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘å’Œå…³é”®é—®é¢˜ã€‚<details>
<summary>Abstract</summary>
Systems and machines undergo various failure modes that result in machine health degradation, so maintenance actions are required to restore them back to a state where they can perform their expected functions. Since maintenance tasks are inevitable, maintenance planning is essential to ensure the smooth operations of the production system and other industries at large. Maintenance planning is a decision-making problem that aims at developing optimum maintenance policies and plans that help reduces maintenance costs, extend asset life, maximize their availability, and ultimately ensure workplace safety. Reinforcement learning is a data-driven decision-making algorithm that has been increasingly applied to develop dynamic maintenance plans while leveraging the continuous information from condition monitoring of the system and machine states. By leveraging the condition monitoring data of systems and machines with reinforcement learning, smart maintenance planners can be developed, which is a precursor to achieving a smart factory. This paper presents a literature review on the applications of reinforcement and deep reinforcement learning for maintenance planning and optimization problems. To capture the common ideas without losing touch with the uniqueness of each publication, taxonomies used to categorize the systems were developed, and reviewed publications were highlighted, classified, and summarized based on these taxonomies. Adopted methodologies, findings, and well-defined interpretations of the reviewed studies were summarized in graphical and tabular representations to maximize the utility of the work for both researchers and practitioners. This work also highlights the research gaps, key insights from the literature, and areas for future work.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç³»ç»Ÿå’Œæœºå™¨ä¼šç»å†å¤šç§å¤±æ•ˆæ¨¡å¼ï¼Œå¯¼è‡´æœºå™¨å¥åº·ä¸‹é™ï¼Œå› æ­¤ç»´æŠ¤å·¥ä½œæ˜¯å¿…è¦çš„æ¥æ¢å¤å®ƒä»¬å›åˆ°å¯ä»¥è¿›è¡Œé¢„æœŸåŠŸèƒ½çš„çŠ¶æ€ã€‚ç”±äºç»´æŠ¤ä»»åŠ¡æ˜¯ä¸å¯é¿å…çš„ï¼Œå› æ­¤ç»´æŠ¤è§‚å¯Ÿæ˜¯å¿…è¦çš„æ¥ç¡®ä¿ç”Ÿäº§ç³»ç»Ÿå’Œå…¶ä»–ä¸šç•Œçš„é¡ºç•…è¿è¡Œã€‚ç»´æŠ¤è§‚å¯Ÿæ˜¯ä¸€ä¸ªå†³ç­–é—®é¢˜ï¼Œæ—¨åœ¨å‘å±•æœ€ä½³ç»´æŠ¤æ”¿ç­–å’Œè®¡åˆ’ï¼Œå¸®åŠ©é™ä½ç»´æŠ¤æˆæœ¬ï¼Œå»¶é•¿èµ„äº§å¯¿å‘½ï¼Œæœ€å¤§åŒ–èµ„äº§å¯ç”¨æ€§ï¼Œå¹¶ç¡®ä¿å·¥ä½œå®‰å…¨ã€‚å¯¹äºç³»ç»Ÿå’Œæœºå™¨çš„çŠ¶æ€ç›‘æ§æ•°æ®ï¼Œå¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§èµ„æ–™é©±åŠ¨çš„å†³ç­–ç®—æ³•ï¼Œå®ƒåœ¨å‘å±•åŠ¨æ€ç»´æŠ¤è®¡åˆ’æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚é€è¿‡å¯¹ç³»ç»Ÿå’Œæœºå™¨çš„çŠ¶æ€ç›‘æ§æ•°æ®çš„å¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥å‘å±•å‡ºæ™ºèƒ½ç»´æŠ¤è§‚å¯Ÿå™¨ï¼Œè¿™æ˜¯æ™ºèƒ½å·¥å‚çš„å‰æã€‚æœ¬æ–‡å°†ä»‹ç»å¯¹ç»´æŠ¤è§‚å¯Ÿå’Œä¼˜åŒ–é—®é¢˜çš„åº”ç”¨å¼ºåŒ–å­¦ä¹ å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ–‡çŒ®ç»¼è¿°ã€‚ä¸ºäº†æ•æ‰æ¯ç¯‡æ–‡ç« çš„å…±åŒä¸»é¢˜è€Œä¸å¤±å…¶å„è‡ªç‹¬ç‰¹æ€§ï¼Œæ–‡ç« è¢«åˆ†ç±»å’Œæ‘˜è¦ï¼Œå¹¶ä»¥æ–‡çŒ®ç»¼è¿°çš„å½¢å¼å‘ˆç°ï¼Œä»¥ä¾¿ Ğ´Ğ»Ñç ”ç©¶äººå‘˜å’Œå®è·µè€…å¯¹å…¶å…·æœ‰æœ€å¤§çš„å®ç”¨æ€§ã€‚æœ¬æ–‡è¿˜ highlights ç»´æŠ¤è§‚å¯Ÿå’Œä¼˜åŒ–é—®é¢˜çš„ç ”ç©¶æ¼æ´ã€æ–‡çŒ®ç»¼è¿°ä¸­çš„ä¸»è¦æ„ä¹‰å’Œæœªæ¥å·¥ä½œçš„æ–¹å‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="Teach-Me-How-to-Learn-A-Perspective-Review-towards-User-centered-Neuro-symbolic-Learning-for-Robotic-Surgical-Systems"><a href="#Teach-Me-How-to-Learn-A-Perspective-Review-towards-User-centered-Neuro-symbolic-Learning-for-Robotic-Surgical-Systems" class="headerlink" title="Teach Me How to Learn: A Perspective Review towards User-centered Neuro-symbolic Learning for Robotic Surgical Systems"></a>Teach Me How to Learn: A Perspective Review towards User-centered Neuro-symbolic Learning for Robotic Surgical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03853">http://arxiv.org/abs/2307.03853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Gomaa, Bilal Mahdy, Niko Kleer, Michael Feld, Frank Kirchner, Antonio KrÃ¼ger<br>for: è¿™é¡¹ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§äººç±»åœ¨Loopå­¦ä¹  paradigmaï¼Œç”¨äºæ•™è‚²æœºå™¨äººåœ¨æœ¯å¼å’Œæ„ŸçŸ¥ä¸¤ä¸ªæ°´å¹³ä¸Šè¿›è¡Œå­¦ä¹ ï¼Œä»¥æé«˜æœºå™¨äººåœ¨æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„æ€§èƒ½ã€‚methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†æ··åˆç¥ç»ç¬¦å·å­¦ä¹ æ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬äººç±»åœ¨Loopå­¦ä¹ å’Œè‡ªåŠ¨å­¦ä¹ ä¸¤ä¸ªæ–¹é¢ã€‚results: ç ”ç©¶äººå‘˜é€šè¿‡å¯¹å„ç§æ‰‹æœ¯ä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚æœºå™¨äººä¸å¤–ç§‘åŒ»ç”Ÿä¹‹é—´çš„äº¤äº’å’Œfeedbacké—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°äº†ä¸€äº›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚åœ¨çº¿å­¦ä¹ å’Œä¸“å®¶åé¦ˆã€‚<details>
<summary>Abstract</summary>
Recent advances in machine learning models allowed robots to identify objects on a perceptual nonsymbolic level (e.g., through sensor fusion and natural language understanding). However, these primarily black-box learning models still lack interpretation and transferability and require high data and computational demand. An alternative solution is to teach a robot on both perceptual nonsymbolic and conceptual symbolic levels through hybrid neurosymbolic learning approaches with expert feedback (i.e., human-in-the-loop learning). This work proposes a concept for this user-centered hybrid learning paradigm that focuses on robotic surgical situations. While most recent research focused on hybrid learning for non-robotic and some generic robotic domains, little work focuses on surgical robotics. We survey this related research while focusing on human-in-the-loop surgical robotic systems. This evaluation highlights the most prominent solutions for autonomous surgical robots and the challenges surgeons face when interacting with these systems. Finally, we envision possible ways to address these challenges using online apprenticeship learning based on implicit and explicit feedback from expert surgeons.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:è¿‘æœŸæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¿›æ­¥ä½¿å¾—æœºå™¨äººèƒ½å¤Ÿé€šè¿‡æ„ŸçŸ¥æ··åˆå’Œè‡ªç„¶è¯­è¨€ç†è§£è¯†åˆ«ç‰©ä½“ï¼Œä½†è¿™äº›ä¸»è¦æ˜¯é»‘ç›’å­¦ä¹ æ¨¡å‹ï¼Œä»ç„¶ç¼ºä¹è§£é‡Šæ€§å’Œå¯ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñæ€§ï¼Œå¹¶éœ€è¦é«˜åº¦æ•°æ®å’Œè®¡ç®—èµ„æºã€‚ä¸€ç§ altenative è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡æ··åˆç¥ç»ç¬¦å·å­¦ä¹ æ–¹æ³•ä¸ä¸“å®¶åé¦ˆï¼ˆå³äººåœ¨å¾ªç¯å­¦ä¹ ï¼‰æ¥æ•™è‚²æœºå™¨äººã€‚è¿™ä¸ªå·¥ä½œæå‡ºäº†ä¸€ç§ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æ··åˆå­¦ä¹  Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³ï¼Œä¸“æ³¨äº Ñ€Ğ¾Ğ±Ğ¾åŒ»å­¦åº”ç”¨ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨éæœºå™¨äººå’Œä¸€äº›é€šç”¨æœºå™¨äººé¢†åŸŸçš„æ··åˆå­¦ä¹ ï¼Œä½†å¯¹äºæ‰‹æœ¯æœºå™¨äººæ¥è¯´ï¼Œæœ‰å¾ˆå°‘çš„ç ”ç©¶ã€‚æˆ‘ä»¬åœ¨è¿™äº›ç›¸å…³ç ”ç©¶ä¸­åšäº†è¯„ä¼°ï¼Œä¸»è¦å…³æ³¨äººåœ¨å¾ªç¯æ‰‹æœ¯æœºå™¨ç³»ç»Ÿä¸­ä¸è¿™äº›ç³»ç»Ÿçš„äº’åŠ¨æ‰€é‡åˆ°çš„æŒ‘æˆ˜ã€‚æœ€åï¼Œæˆ‘ä»¬æƒ³è±¡äº†ä½¿ç”¨åœ¨çº¿å¾ªç¯å­¦ä¹ ï¼ŒåŸºäºä¸“å®¶å¤–ç§‘åŒ»ç”Ÿçš„æ˜¾å¼å’Œéšå¼åé¦ˆæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="Optimal-Learners-for-Realizable-Regression-PAC-Learning-and-Online-Learning"><a href="#Optimal-Learners-for-Realizable-Regression-PAC-Learning-and-Online-Learning" class="headerlink" title="Optimal Learners for Realizable Regression: PAC Learning and Online Learning"></a>Optimal Learners for Realizable Regression: PAC Learning and Online Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03848">http://arxiv.org/abs/2307.03848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Idan Attias, Steve Hanneke, Alkis Kalavasis, Amin Karbasi, Grigoris Velegkas</li>
<li>for: æœ¬æ–‡çš„ç›®çš„æ˜¯Characterizing the statistical complexity of realizable regression in both PAC learning setting and online learning setting.</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†ä¸€ç§æ–°çš„ç»´åº¦æ¥Characterize which classes of real-valued predictors are learnable, ä»¥åŠä¸€ç§æ–°çš„ç»´åº¦æ¥Characterize the minimax instance optimal cumulative loss up to a constant factor.</li>
<li>results: æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¿…è¦çš„ condition for learnability based on a combinatorial dimension related to the DS dimension, å¹¶ conjecture that it may also be sufficient in this context. æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è§£å†³äº†Daskalakiså’ŒGolowichåœ¨STOC â€˜22ä¸­æå‡ºçš„ä¸€ä¸ªå¼€Question.<details>
<summary>Abstract</summary>
In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.   Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in this context.   Additionally, in the context of online learning we provide a dimension that characterizes the minimax instance optimal cumulative loss up to a constant factor and design an optimal online learner for realizable regression, thus resolving an open question raised by Daskalakis and Golowich in STOC '22.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç›®æ ‡æ˜¯characterize realizable regressionçš„ç»Ÿè®¡å¤æ‚æ€§åœ¨PACå­¦ä¹  Settingå’Œåœ¨çº¿å­¦ä¹  Settingä¸­ã€‚å…ˆå‰çš„å·¥ä½œå·²ç»è¯æ˜äº†å®izable regressionçš„å¯å­¦ä¹ æ€§å……åˆ† suffices finiteness of the fat shattering dimensionï¼Œä½†æ˜¯æ²¡æœ‰è¿›è¡Œæ›´å®Œæ•´çš„Characterizationã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥ä¸€ä¸ªæœ€å°æœ€ä¼˜å­¦ä¹ å™¨ Ğ´Ğ»Ñ realizable regressionï¼Œå¹¶æå‡ºä¸€ç§æ–°çš„ç»´åº¦æ¥Characterize which classes of real-valued predictors are learnableã€‚ç„¶åï¼Œæˆ‘ä»¬ indentify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable settingã€‚æœ€åï¼Œæˆ‘ä»¬ç¡®ç«‹äº†å­¦ä¹ å¯èƒ½æ€§çš„å¿…è¦æ¡ä»¶ï¼ŒåŸºäºä¸€ç§ combinatorial dimension related to the DS dimensionï¼Œå¹¶speculate that it may also be sufficient in this contextã€‚åœ¨åœ¨çº¿å­¦ä¹  Settingä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªCharacterize the minimax instance optimal cumulative loss up to a constant factorçš„ç»´åº¦ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªoptimal online learner for realizable regressionï¼Œè¿™è§£å†³äº†Daskalakiså’ŒGolowichåœ¨STOC '22ä¸­æå‡ºçš„ä¸€ä¸ªå¼€é—®ã€‚
</details></li>
</ul>
<hr>
<h2 id="RADAR-Robust-AI-Text-Detection-via-Adversarial-Learning"><a href="#RADAR-Robust-AI-Text-Detection-via-Adversarial-Learning" class="headerlink" title="RADAR: Robust AI-Text Detection via Adversarial Learning"></a>RADAR: Robust AI-Text Detection via Adversarial Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03838">http://arxiv.org/abs/2307.03838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho<br>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦ç›®çš„æ˜¯æå‡ºä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå³ RADARï¼Œç”¨äºæ£€æµ‹äººå·¥æ™ºèƒ½æ–‡æœ¬ç”Ÿæˆå™¨ï¼ˆLLMï¼‰ç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦çœŸå®ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¯¹æŠ—LLMç”Ÿæˆçš„å„ç§æŠ½è±¡å’Œé‡å»ºæ–‡æœ¬ã€‚methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶RADARï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šä¸€ä¸ªæ˜¯ä¸€ä¸ªå¼ºå¤§çš„AI-textæ£€æµ‹å™¨ï¼Œå¦ä¸€ä¸ªæ˜¯ä¸€ä¸ªå¼ºå¤§çš„ä¼ªæ–‡æœ¬ç”Ÿæˆå™¨ã€‚è¿™ä¸¤ä¸ªéƒ¨åˆ†åœ¨è®­ç»ƒæ—¶ä¼šäº’ç›¸å½±å“ï¼Œä»¥æé«˜AI-textæ£€æµ‹å™¨çš„å‡†ç¡®æ€§å’Œé€‚ç”¨èŒƒå›´ã€‚results: å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRADARåœ¨8ç§ä¸åŒçš„LLMä¸­è¿›è¡Œæµ‹è¯•æ—¶ï¼Œå…·æœ‰ä¸ç°æœ‰AI-textæ£€æµ‹æ–¹æ³•ç›¸æ¯”çš„ä¼˜ç§€æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å„ç§æŠ½è±¡å’Œé‡å»ºæ–‡æœ¬æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼ŒRADARè¿˜èƒ½å¤Ÿå¯¹ä¸åŒçš„LLMè¿›è¡Œå¼ºå¤§çš„è½¬ç§»å­¦ä¹ ï¼Œå¹¶ä¸”é€è¿‡GPT-3.5è¿›è¡Œè¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚<details>
<summary>Abstract</summary>
Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘å¹´çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ™ºèƒ½å®¢æˆ·ç«¯åº”ç”¨çš„æ™®åŠï¼Œä½¿å¾—äººæœºç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬çš„è¾¹ç•Œå˜å¾—æ¨¡ç³Šã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯å’Œç¤¾ä¼šå˜é©çš„æ½œåœ¨æ”¹å˜ï¼Œä¹Ÿå¸¦æ¥äº†æ–°çš„å›°éš¾ï¼Œå¦‚å‡å†…å®¹ç”Ÿæˆã€æŠ„è¢­å’Œæ— è¾œçš„å†™ä½œè€…è¢«è¯¬å‘Šã€‚ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„AI-æ–‡æœ¬æ£€æµ‹å™¨å¯¹LLM-åŸºäºé‡æ–°æ¨æ•²çš„æ–‡æœ¬ä¸å…·æœ‰åšå›ºçš„é²æ£’æ€§ã€‚è¿™ç¯‡è®ºæ–‡æ—¨åœ¨bridgingè¿™ä¸ªå·®è·ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå³RADARï¼Œå®ƒé€šè¿‡å¯¹æŠ—å­¦ä¹ è®­ç»ƒä¸€ä¸ªRobust AI-æ–‡æœ¬æ£€æµ‹å™¨å’Œä¸€ä¸ªé‡æ–°æ¨æ•²å™¨ã€‚RADARåŸºäºæŠ—å¯¹æŠ—è®­ç»ƒï¼Œé‡æ–°æ¨æ•²å™¨çš„ç›®æ ‡æ˜¯ç”ŸæˆçœŸå®çš„å†…å®¹ï¼Œé€ƒè„±AI-æ–‡æœ¬æ£€æµ‹å™¨çš„æ£€æµ‹ã€‚RADARé€šè¿‡æ£€æµ‹å™¨å¯¹é‡æ–°æ¨æ•²å™¨çš„åé¦ˆæ¥æ›´æ–°é‡æ–°æ¨æ•²å™¨ï¼Œå¹¶ vice versaã€‚åœ¨8ç§ä¸åŒçš„LLMï¼ˆPythiaã€Dolly 2.0ã€Palmyraã€Camelã€GPT-Jã€Dolly 1.0ã€LLaMAå’ŒVicunaï¼‰åœ¨4ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœæ˜¾ç¤ºï¼ŒRADARåœ¨AI-æ–‡æœ¬æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡æ–°æ¨æ•²æ—¶ã€‚æˆ‘ä»¬è¿˜å‘ç°äº†RADARåœ¨å—è¿‡ç‰¹å®šæŒ‡ä»¤çš„LLMä¸Šçš„å¼ºå¤§ä¼ è¾“æ€§ï¼Œå¹¶é€šè¿‡GPT-3.5è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°RADARçš„æ”¹è¿›èƒ½åŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Back-to-Optimization-Diffusion-based-Zero-Shot-3D-Human-Pose-Estimation"><a href="#Back-to-Optimization-Diffusion-based-Zero-Shot-3D-Human-Pose-Estimation" class="headerlink" title="Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation"></a>Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03833">http://arxiv.org/abs/2307.03833</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ipl-uw/ZeDO-Release">https://github.com/ipl-uw/ZeDO-Release</a></li>
<li>paper_authors: Zhongyu Jiang, Zhuoran Zhou, Lei Li, Wenhao Chai, Cheng-Yen Yang, Jenq-Neng Hwang</li>
<li>for: 3D human pose estimation (HPE) tasks in the wild, where traditional optimization-based methods have limited performance and learning-based methods have difficulty generalizing to new domains and scenarios.</li>
<li>methods: Zero-shot Diffusion-based Optimization (ZeDO) pipeline, which combines the advantages of optimization-based and learning-based methods by using a diffusion process to refine the pose estimates and a multi-hypothesis framework to handle cross-domain and in-the-wild variations.</li>
<li>results: state-of-the-art (SOTA) performance on Human3.6M and 3DPW datasets, with minMPJPE $51.4$mm and PA-MPJPE $42.6$mm, respectively, without requiring any 2D-3D or image-3D pairs for training.<details>
<summary>Abstract</summary>
Learning-based methods have dominated the 3D human pose estimation (HPE) tasks with significantly better performance in most benchmarks than traditional optimization-based methods. Nonetheless, 3D HPE in the wild is still the biggest challenge of learning-based models, whether with 2D-3D lifting, image-to-3D, or diffusion-based methods, since the trained networks implicitly learn camera intrinsic parameters and domain-based 3D human pose distributions and estimate poses by statistical average. On the other hand, the optimization-based methods estimate results case-by-case, which can predict more diverse and sophisticated human poses in the wild. By combining the advantages of optimization-based and learning-based methods, we propose the Zero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve the problem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDO achieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE $51.4$mm without training with any 2D-3D or image-3D pairs. Moreover, our single-hypothesis ZeDO achieves SOTA performance on 3DPW dataset with PA-MPJPE $42.6$mm on cross-dataset evaluation, which even outperforms learning-based methods trained on 3DPW.
</details>
<details>
<summary>æ‘˜è¦</summary>
å­¦ä¹ åŸºäºæ–¹æ³•åœ¨3Däººå§¿ä¼°è®¡ä»»åŠ¡ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œåœ¨å¤§å¤šæ•°æ ‡å‡†å‡†åˆ™ä¸Šæ¯”ä¼ ç»Ÿä¼˜åŒ–åŸºäºæ–¹æ³•æ›´å¥½è¡¨ç°ã€‚ç„¶è€Œï¼Œ3Däººå§¿ä¼°è®¡åœ¨é‡å¤–ä»ç„¶æ˜¯å­¦ä¹ åŸºäºæ–¹æ³•æ¨¡å‹çš„æœ€å¤§æŒ‘æˆ˜ï¼Œæ— è®ºæ˜¯2D-3Då‡çº§ã€å›¾åƒ-3Dæˆ–æ‰©æ•£åŸºäºæ–¹æ³•ã€‚è¿™æ˜¯å› ä¸ºè®­ç»ƒçš„ç½‘ç»œéšè—çŠ¶æ€å‚æ•°å’Œé¢†åŸŸåŸºäº3Däººå§¿åˆ†å¸ƒï¼Œå¹¶ä¼°è®¡å§¿åŠ¿çš„ç»Ÿè®¡å¹³å‡å€¼ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼˜åŒ–åŸºäºæ–¹æ³•æ–¹æ³•æ¯ä¸ªæ¡ˆä¾‹éƒ½é¢„æµ‹ç»“æœï¼Œå¯ä»¥é¢„æµ‹æ›´å¤šå’Œæ›´å¤æ‚çš„äººå§¿ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºé›¶shotæ‰©æ•£ä¼˜åŒ–ï¼ˆZeDOï¼‰ç®¡é“æ¥è§£å†³è·¨é¢†åŸŸå’Œé‡å¤–3Däººå§¿ä¼°è®¡é—®é¢˜ã€‚æˆ‘ä»¬çš„å¤šç§å‡è®¾ZeDOåœ¨äºº3.6Mæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§ï¼ˆSOTAï¼‰æ€§èƒ½ï¼Œæ— éœ€è®­ç»ƒ2D-3Dæˆ–å›¾åƒ-3Då¯¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å•ç§å‡è®¾ZeDOåœ¨3DPWæ•°æ®é›†ä¸Šå®ç°äº†SOTAæ€§èƒ½ï¼Œä¸å­¦ä¹ åŸºäºæ–¹æ³•æ–¹æ³•ç›¸æ¯”ï¼Œå³ä½¿åœ¨è·¨æ•°æ®é›†è¯„ä¼°ä¸­ã€‚
</details></li>
</ul>
<hr>
<h2 id="Effect-of-Intensity-Standardization-on-Deep-Learning-for-WML-Segmentation-in-Multi-Centre-FLAIR-MRI"><a href="#Effect-of-Intensity-Standardization-on-Deep-Learning-for-WML-Segmentation-in-Multi-Centre-FLAIR-MRI" class="headerlink" title="Effect of Intensity Standardization on Deep Learning for WML Segmentation in Multi-Centre FLAIR MRI"></a>Effect of Intensity Standardization on Deep Learning for WML Segmentation in Multi-Centre FLAIR MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03827">http://arxiv.org/abs/2307.03827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdollah Ghazvanchahi, Pejman Jahbedar Maralani, Alan R. Moody, April Khademi</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯å¯¹FLAIR MRå›¾åƒè¿›è¡Œç™½ matteræŸå˜ï¼ˆWMLï¼‰åˆ†å‰²ï¼Œä»¥æé«˜DLæ–¹æ³•åœ¨ä¸åŒæˆåƒä¸­å¿ƒçš„æ€§èƒ½ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†å¤šç§æŠ‘åˆ¶æ³•ï¼ŒåŒ…æ‹¬IALMLABå’Œå…¶ä»–æµè¡Œçš„ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°æ³•ï¼Œä»¥åŠä¸€ç§ skip-connection UNet æ¨¡å‹ã€‚</li>
<li>results: ç»“æœæ˜¾ç¤ºï¼ŒIALMLABå’Œ ensemble æ–¹æ³•åœ¨å„ç§ç»´åº¦ä¸Šéƒ½æœ‰è¾ƒé«˜çš„WMLåˆ†å‰²æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒçš„æˆåƒä¸­å¿ƒæ•°æ®ä¸Šã€‚<details>
<summary>Abstract</summary>
Deep learning (DL) methods for white matter lesion (WML) segmentation in MRI suffer a reduction in performance when applied on data from a scanner or centre that is out-of-distribution (OOD) from the training data. This is critical for translation and widescale adoption, since current models cannot be readily applied to data from new institutions. In this work, we evaluate several intensity standardization methods for MRI as a preprocessing step for WML segmentation in multi-centre Fluid-Attenuated Inversion Recovery (FLAIR) MRI. We evaluate a method specifically developed for FLAIR MRI called IAMLAB along with other popular normalization techniques such as White-strip, Nyul and Z-score. We proposed an Ensemble model that combines predictions from each of these models. A skip-connection UNet (SC UNet) was trained on the standardized images, as well as the original data and segmentation performance was evaluated over several dimensions. The training (in-distribution) data consists of a single study, of 60 volumes, and the test (OOD) data is 128 unseen volumes from three clinical cohorts. Results show IAMLAB and Ensemble provide higher WML segmentation performance compared to models from original data or other normalization methods. IAMLAB & Ensemble have the highest dice similarity coefficient (DSC) on the in-distribution data (0.78 & 0.80) and on clinical OOD data. DSC was significantly higher for IAMLAB compared to the original data (p<0.05) for all lesion categories (LL>25mL: 0.77 vs. 0.71; 10mL<= LL<25mL: 0.66 vs. 0.61; LL<10mL: 0.53 vs. 0.52). The IAMLAB and Ensemble normalization methods are mitigating MRI domain shift and are optimal for DL-based WML segmentation in unseen FLAIR data.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•ç”¨äºFLAIR MRä¸­çš„ç™½è´¨æŸä¼¤ï¼ˆWMLï¼‰åˆ† segmentationæ—¶ï¼Œå½“åº”ç”¨äºä¸åŒçš„æ‰«æä»ªæˆ–ä¸­å¿ƒæ•°æ®æ—¶ï¼Œæ€§èƒ½ä¼šä¸‹é™ã€‚è¿™å¯¹äºç¿»è¯‘å’Œå¤§è§„æ¨¡åº”ç”¨è€Œè¨€æ˜¯é‡è¦çš„ï¼Œå› ä¸ºå½“å‰çš„æ¨¡å‹æ— æ³• direct åº”ç”¨äºæ–°æœºæ„çš„æ•°æ®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šç§INTENSITY STANDARDIZATION METHODS FOR MRIä½œä¸ºFLAIR MRåˆ† segmentationçš„é¢„å¤„ç†æ­¥éª¤ã€‚æˆ‘ä»¬è¯„ä¼°äº†specifically developed for FLAIR MRçš„IAMLABæ–¹æ³•ï¼Œä»¥åŠå…¶ä»–æµè¡Œçš„normalizationæŠ€æœ¯ï¼Œå¦‚White-stripã€Nyulå’ŒZ-scoreã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ensembleæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœè¿›è¡Œcombineã€‚ä¸€ä¸ªskip-connection UNETï¼ˆSC UNNetï¼‰åœ¨æ ‡å‡†åŒ–å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥åŠåŸå§‹æ•°æ®ä¸Šã€‚æˆ‘ä»¬å¯¹æ ‡å‡†åŒ–å›¾åƒå’ŒåŸå§‹æ•°æ®è¿›è¡Œåˆ† segmentationæ€§èƒ½è¿›è¡Œè¯„ä¼°ã€‚trainingï¼ˆåœ¨ Distributionï¼‰æ•°æ®åŒ…æ‹¬60ä¸ªVolumeï¼Œæµ‹è¯•ï¼ˆOODï¼‰æ•°æ®åŒ…æ‹¬128ä¸ªæœªçœ‹è¿‡çš„Volumeä»ä¸‰ä¸ªä¸´åºŠç¾¤ä½“ã€‚ç»“æœæ˜¾ç¤ºï¼ŒIAMLABå’ŒEnsembleæ–¹æ³•åœ¨WMLåˆ† segmentationä¸­æä¾›äº†æ›´é«˜çš„æ€§èƒ½ï¼Œç›¸æ¯”äºåŸå§‹æ•°æ®æˆ–å…¶ä»–normalizationæ–¹æ³•ã€‚IAMLABå’ŒEnsembleæ–¹æ³•åœ¨in-distributionæ•°æ®ä¸Šçš„DSCï¼ˆ dice similarity coefficientï¼‰ä¸º0.78å’Œ0.80ï¼Œå¹¶åœ¨ä¸´åºŠOODæ•°æ®ä¸Šä¹Ÿæœ‰æœ€é«˜çš„DSCã€‚ç›¸æ¯”åŸå§‹æ•°æ®ï¼ŒIAMLABæ–¹æ³•çš„DSCæ˜¾è‘—é«˜äºåŸå§‹æ•°æ®ï¼ˆp<0.05ï¼‰ï¼Œå¯¹æ‰€æœ‰æŸä¼¤åˆ†ç±»ï¼ˆLL>25mLï¼š0.77 vs. 0.71; 10mLâ‰¤ LL<25mLï¼š0.66 vs. 0.61; LL<10mLï¼š0.53 vs. 0.52ï¼‰ã€‚IAMLABå’ŒEnsemble normalizationæ–¹æ³•å¯ä»¥ Mitigate MRI DOMAIN SHIFTï¼Œæ˜¯é€‚ç”¨äºDL-based WMLåˆ† segmentationçš„ä¼˜é€‰æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="How-does-AI-chat-change-search-behaviors"><a href="#How-does-AI-chat-change-search-behaviors" class="headerlink" title="How does AI chat change search behaviors?"></a>How does AI chat change search behaviors?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03826">http://arxiv.org/abs/2307.03826</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Capra, Jaime Arguello</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ä¸€ä¸ªåˆæ­¥çš„è°ƒæŸ¥ï¼Œæ—¨åœ¨ç ”ç©¶ç”¨æˆ·åœ¨æœç´¢è¿‡ç¨‹ä¸­å¦‚ä½•ä½¿ç”¨ç”ŸæˆAIèŠå¤©ç³»ç»Ÿï¼ˆç®€ç§°chatï¼‰ï¼Œä»¥åŠå°†chatç³»ç»Ÿä¸ç°æœ‰æœç´¢å·¥å…·ç»“åˆä½¿ç”¨åï¼Œç”¨æˆ·çš„æœç´¢ä¹ æƒ¯å’Œç­–ç•¥ä¼šå—åˆ°ä»€ä¹ˆå½±å“ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†10åå‚ä¸è€…ï¼Œä»–ä»¬ä½¿ç”¨äº†ä¸€ä¸ªç»„åˆçš„Chat+Searchç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨äº†OpenAI GPT-3.5 APIå’ŒBing Web Search v5 APIã€‚å‚ä¸è€…å®Œæˆäº†ä¸‰ä¸ªæœç´¢ä»»åŠ¡ã€‚</li>
<li>results: è¿™ä¸ªé¢„å°ç¨¿ä¸­æŠ¥å‘Šäº†ç”¨æˆ·å¦‚ä½•å°†AIèŠå¤©ç³»ç»Ÿçº³å…¥æœç´¢è¿‡ç¨‹ä¸­ï¼Œä»–ä»¬å¯¹èŠå¤©ç³»ç»Ÿçš„å–œå¥½å’Œä¸å–œå¥½ï¼Œå¯¹èŠå¤©ç³»ç»Ÿçš„ä¿¡ä»»åº¦ï¼Œä»¥åŠä»–ä»¬å¯¹èŠå¤©ç³»ç»Ÿç”Ÿæˆå›å¤çš„å¿ƒç†æ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
Generative AI tools such as chatGPT are poised to change the way people engage with online information. Recently, Microsoft announced their "new Bing" search system which incorporates chat and generative AI technology from OpenAI. Google has announced plans to deploy search interfaces that incorporate similar types of technology. These new technologies will transform how people can search for information. The research presented here is an early investigation into how people make use of a generative AI chat system (referred to simply as chat from here on) as part of a search process, and how the incorporation of chat systems with existing search tools may effect users search behaviors and strategies.   We report on an exploratory user study with 10 participants who used a combined Chat+Search system that utilized the OpenAI GPT-3.5 API and the Bing Web Search v5 API. Participants completed three search tasks. In this pre-print paper of preliminary results, we report on ways that users integrated AI chat into their search process, things they liked and disliked about the chat system, their trust in the chat responses, and their mental models of how the chat system generated responses.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œç”ŸæˆAIå·¥å…·å¦‚ chatGPT å°†æ”¹å˜çº¿ä¸Šä¿¡æ¯æœå¯»æ–¹å¼ã€‚å¾®è½¯æœ€è¿‘å®£å¸ƒâ€œæ–°çš„Bingâ€æœå¯»ç³»ç»Ÿï¼Œèåˆäº†OpenAIçš„ chatå’Œç”ŸæˆAIæŠ€æœ¯ã€‚Google ä¹Ÿå®£å¸ƒå°†æ¨å‡ºç›¸ä¼¼çš„æŠ€æœ¯ã€‚è¿™äº›æ–°æŠ€æœ¯å°†æ”¹å˜äººä»¬æœå¯»èµ„è®¯çš„æ–¹å¼ã€‚æœ¬ç ”ç©¶æ˜¯æ¢ç´¢ç”¨æˆ·å¦‚ä½•ä½¿ç”¨ç”ŸæˆAI chatç³»ç»Ÿï¼ˆä»¥ä¸‹ç®€ç§°ä¸ºâ€œchatâ€ï¼‰ä½œä¸ºæœå¯»è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œä»¥åŠå°† chat ç³»ç»Ÿä¸ç°æœ‰çš„æœå¯»å·¥å…·ç»“åˆåå¯¹ç”¨æˆ·æœå¯»è¡Œä¸ºå’Œç­–ç•¥çš„å½±å“ã€‚â€â€œæˆ‘ä»¬è¿›è¡Œäº†10åç”¨æˆ·çš„exploratoryç”¨æˆ·ç ”ç©¶ï¼Œä»–ä»¬ä½¿ç”¨äº† combine Chat+Search ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨ OpenAI GPT-3.5 API å’Œ Bing Web Search v5 APIã€‚ç”¨æˆ·å®Œæˆäº†ä¸‰ä¸ªæœå¯»ä»»åŠ¡ã€‚åœ¨è¿™ä¸ªé¢„å°ç¨¿ä¸­ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†ç”¨æˆ·å¦‚ä½•å°† chat ç³»ç»Ÿç»„åˆåˆ°æœå¯»è¿‡ç¨‹ä¸­ï¼Œä»–ä»¬å–œæ¬¢å’Œä¸å–œæ¬¢ chat ç³»ç»Ÿï¼Œä»–ä»¬å¯¹ chat ç³»ç»Ÿçš„ä¿¡ä»»åº¦ï¼Œä»¥åŠä»–ä»¬å¦‚ä½•è§£é‡Š chat ç³»ç»Ÿç”Ÿæˆçš„å›ç­”ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Exploring-and-Characterizing-Large-Language-Models-For-Embedded-System-Development-and-Debugging"><a href="#Exploring-and-Characterizing-Large-Language-Models-For-Embedded-System-Development-and-Debugging" class="headerlink" title="Exploring and Characterizing Large Language Models For Embedded System Development and Debugging"></a>Exploring and Characterizing Large Language Models For Embedded System Development and Debugging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03817">http://arxiv.org/abs/2307.03817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zachary Englhardt, Richard Li, Dilini Nissanka, Zhihan Zhang, Girish Narayanswamy, Joseph Breda, Xin Liu, Shwetak Patel, Vikram Iyer<br>for:è¿™ç§è®ºæ–‡æ—¨åœ¨è¯„ä¼°é¢†å…ˆçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆGPT-3.5ã€GPT-4ã€PaLM 2ï¼‰åœ¨åµŒå…¥å¼ç³»ç»Ÿå¼€å‘ä¸­çš„è¡¨ç°ï¼Œä»¥åŠäººå·¥ç¨‹åºå‘˜ä¸è¿™äº›å·¥å…·ä¹‹é—´çš„äº¤äº’æ–¹å¼ã€‚methods:è¯¥è®ºæ–‡é‡‡ç”¨ç«¯åˆ°ç«¯ç¡¬ä»¶åœ¨Loopï¼ˆHILï¼‰è¯„ä¼°å¹³å°æ¥éªŒè¯LLMç”Ÿæˆçš„ç¨‹åºï¼Œå¹¶å¯¹N&#x3D;450ä¸ªå®éªŒè¿›è¡Œæ¯”è¾ƒã€‚åŒæ—¶ï¼Œè¯¥è®ºæ–‡è¿˜å¼€å‘äº†ä¸€ç§åŸºäºAIçš„è½¯ä»¶å·¥ç¨‹åŠäº‹å¤„ç¨‹åºï¼Œç”¨äºå»ºç«‹åµŒå…¥å¼ç³»ç»Ÿã€‚results:ç ”ç©¶å‘ç°ï¼ŒGPT-4ç‰¹åˆ«è¡¨ç°å‡ºè·¨é¢†åŸŸç†è§£å’Œé€»è¾‘èƒ½åŠ›ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥ä»å•ä¸ªæç¤ºç”Ÿæˆå®Œå…¨æ­£ç¡®çš„ç¨‹åºã€‚åœ¨N&#x3D;50æ¬¡å®éªŒä¸­ï¼ŒGPT-4ç”Ÿæˆçš„I2Cæ¥å£66%çš„æ—¶é—´èƒ½å¤Ÿæ­£å¸¸å·¥ä½œã€‚æ­¤å¤–ï¼ŒGPT-4è¿˜ç”Ÿæˆäº†ç‰¹å®šçš„å‚¨å­˜å™¨é©±åŠ¨ç¨‹åºã€LoRaé€šä¿¡ç¨‹åºå’ŒContextç‰¹å®šçš„ç”µæºä¼˜åŒ–ç¨‹åºï¼Œä½¿nRF52ç¨‹åºçš„ç”µæµå‡å°‘è‡³12.2 uAï¼Œå‡å°‘äº†740å€ã€‚<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown remarkable abilities to generate code, however their ability to develop software for embedded systems, which requires cross-domain knowledge of hardware and software has not been studied. In this paper we systematically evaluate leading LLMs (GPT-3.5, GPT-4, PaLM 2) to assess their performance for embedded system development, study how human programmers interact with these tools, and develop an AI-based software engineering workflow for building embedded systems.   We develop an an end-to-end hardware-in-the-loop evaluation platform for verifying LLM generated programs using sensor actuator pairs. We compare all three models with N=450 experiments and find surprisingly that GPT-4 especially shows an exceptional level of cross-domain understanding and reasoning, in some cases generating fully correct programs from a single prompt. In N=50 trials, GPT-4 produces functional I2C interfaces 66% of the time. GPT-4 also produces register-level drivers, code for LoRa communication, and context-specific power optimizations for an nRF52 program resulting in over 740x current reduction to 12.2 uA. We also characterize the models' limitations to develop a generalizable workflow for using LLMs in embedded system development. We evaluate the workflow with 15 users including novice and expert programmers. We find that our workflow improves productivity for all users and increases the success rate for building a LoRa environmental sensor from 25% to 100%, including for users with zero hardware or C/C++ experience.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»è¡¨ç°å‡ºæ°å‡ºçš„ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å®ƒä»¬å¯¹åµŒå…¥å¼ç³»ç»Ÿå¼€å‘ï¼Œéœ€è¦è·¨é¢†åŸŸçŸ¥è¯†çš„å¼€å‘ä»æœªè¢«ç ”ç©¶ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿè¯„ä¼°äº†ä¸»æµLLMï¼ˆGPT-3.5ã€GPT-4ã€PaLM 2ï¼‰çš„è¡¨ç°ï¼Œäº†è§£äººå·¥å¼€å‘è€…ä¸è¿™äº›å·¥å…·ä¹‹é—´çš„äº’åŠ¨ï¼Œå¹¶å¼€å‘äº†åŸºäºäººå·¥æ™ºèƒ½çš„è½¯ä»¶å·¥ç¨‹ç”Ÿå‘½å‘¨æœŸ workflowï¼Œç”¨äºå»ºç«‹åµŒå…¥å¼ç³»ç»Ÿã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç»ˆç«¯ç¡¬ä»¶åœ¨Loopè¯„ä¼°å¹³å°ï¼Œç”¨äºéªŒè¯LLMç”Ÿæˆçš„ç¨‹å¼ç ã€‚æˆ‘ä»¬å¯¹N=450å®éªŒè¿›è¡Œæ¯”è¾ƒï¼Œå‘ç°GPT-4å°¤å…¶è¡¨ç°å‡ºè·¨é¢†åŸŸç†è§£å’Œæ¨ç†çš„ç‰¹åˆ«é«˜æ°´å¹³ï¼Œæœ‰æ—¶å€™ä»å•ä¸€æç¤ºä¸­ç”Ÿæˆå®Œå…¨æ­£ç¡®çš„ç¨‹å¼ç ã€‚åœ¨N=50è¯•éªŒä¸­ï¼ŒGPT-4äº§ç”Ÿäº†66%çš„æ­£å¸¸I2Cæ¥å£ã€‚GPT-4è¿˜äº§ç”Ÿäº†å¯¹åº”çš„å‚¨å­˜å™¨é©±åŠ¨ç¨‹å¼ç ã€LoRaé€šä¿¡ç¨‹å¼ç å’Œç‰¹å®šåº”ç”¨ç¨‹åºç ï¼Œå¯¼è‡´nRF52ç¨‹å¼çš„ç”µæµé™ä½è‡³12.2 uAï¼Œå®ç°äº†740å€çš„ç”µæµå¢å¼ºã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†æ¨¡å‹çš„é™åˆ¶ï¼Œä»¥å‘å±•ä¸€ä¸ªé€šç”¨çš„å·¥ä½œæµç¨‹ï¼Œç”¨äºåœ¨åµŒå…¥å¼ç³»ç»Ÿå¼€å‘ä¸­ä½¿ç”¨LLMã€‚æˆ‘ä»¬å°†è¿™ä¸ªå·¥ä½œæµç¨‹è¯„ä¼°äº†15åä½¿ç”¨è€…ï¼ŒåŒ…æ‹¬åˆå­¦è€…å’Œé«˜çº§ç¨‹åºå‘˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬çš„å·¥ä½œæµç¨‹å¯ä»¥å¸®åŠ©æ‰€æœ‰ä½¿ç”¨è€…æé«˜ç”Ÿäº§åŠ›ï¼Œå¹¶å°†å°†LoRaç¯å¢ƒæ„Ÿåº”å™¨çš„å»ºç«‹ç‡ç”±25%æé«˜è‡³100%ï¼ŒåŒ…æ‹¬é›¶ç¡¬ä»¶æˆ–C/C++ç»éªŒçš„ä½¿ç”¨è€…ã€‚
</details></li>
</ul>
<hr>
<h2 id="For-Women-Life-Freedom-A-Participatory-AI-Based-Social-Web-Analysis-of-a-Watershed-Moment-in-Iranâ€™s-Gender-Struggles"><a href="#For-Women-Life-Freedom-A-Participatory-AI-Based-Social-Web-Analysis-of-a-Watershed-Moment-in-Iranâ€™s-Gender-Struggles" class="headerlink" title="For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis of a Watershed Moment in Iranâ€™s Gender Struggles"></a>For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis of a Watershed Moment in Iranâ€™s Gender Struggles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03764">http://arxiv.org/abs/2307.03764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adel Khorramrouz, Sujan Dutta, Ashiqur R. KhudaBukhsh</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®çš„æ˜¯è®¡ç®—æ¨ç‰¹è¯­è¨€å¯¹å¦‡å¥³å¹³ç­‰çš„æ¨åŠ¨ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ä¸ªensembleæ´»åŠ¨å­¦ä¹ ç®¡é“ï¼Œä»¥è®­ç»ƒä¸€ä¸ªç«‹åœºåˆ†ç±»å™¨ã€‚è¯¥ç®¡é“ä¸­ï¼Œä¼Šæœ—å¥³æ€§å‚ä¸äº†ä¸€ä¸ªæ´»è·ƒçš„è§’è‰²ï¼Œä¸ä»…ä¸ºæ ‡æ³¨æä¾›æ ‡ç­¾ï¼Œè¿˜æä¾›äº†æœ‰ä»·å€¼çš„å…³é”®è¯å’Œæ›´å…·æœ‰æ„ä¹‰çš„æ–‡æ¡£æ ·æœ¬ï¼Œä»¥ä¾¿æ›´å¥½åœ°å»ºç«‹AIç³»ç»Ÿã€‚</li>
<li>results: åˆ†æç»“æœæ˜¾ç¤ºï¼Œé©¬å“ˆèµ›å¦®Â·é˜¿ç±³å°¼çš„æ­»äº¡å¼•å‘äº†ä¸€äº›æåŒ–çš„æ¨ç‰¹è¯­è¨€è®¨è®ºï¼Œå¢åŠ äº†å¯¹å¦‡å¥³å¹³ç­‰çš„è´Ÿé¢å’Œæ­£é¢æ¨æ–‡ã€‚æ­£é¢æ¨æ–‡çš„å¢åŠ ç•¥å¤§äºè´Ÿé¢æ¨æ–‡çš„å¢åŠ ã€‚æ­¤å¤–ï¼Œå¯¹äºè´¦æˆ·åˆ›å»ºæ—¶é—´ï¼Œä¸å›½å®¶å¯¹é½çš„æ¨ç‰¹è´¦æˆ·å’Œoprotestæ¨ç‰¹è´¦æˆ·ä¹‹é—´ï¼Œoprotestè´¦æˆ·æ›´åƒåŸºçº¿æ³¢æ–¯è¯­æ¨ç‰¹æ´»åŠ¨ã€‚<details>
<summary>Abstract</summary>
In this paper, we present a computational analysis of the Persian language Twitter discourse with the aim to estimate the shift in stance toward gender equality following the death of Mahsa Amini in police custody. We present an ensemble active learning pipeline to train a stance classifier. Our novelty lies in the involvement of Iranian women in an active role as annotators in building this AI system. Our annotators not only provide labels, but they also suggest valuable keywords for more meaningful corpus creation as well as provide short example documents for a guided sampling step. Our analyses indicate that Mahsa Amini's death triggered polarized Persian language discourse where both fractions of negative and positive tweets toward gender equality increased. The increase in positive tweets was slightly greater than the increase in negative tweets. We also observe that with respect to account creation time, between the state-aligned Twitter accounts and pro-protest Twitter accounts, pro-protest accounts are more similar to baseline Persian Twitter activity.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è®¡ç®—æœºåˆ†ææ–¹æ³•ï¼Œç”¨äºåˆ†æTwitterä¸Šçš„æ³¢æ–¯è¯­è¨€è®¨è®ºï¼Œä»¥ä¼°è®¡åœ¨è´å¨…Â·è‰¾ç±³å°¼åœ¨è­¦å¯Ÿæ‰§æ³•ä¸­å»ä¸–åï¼Œå¯¹ç”·å¥³å¹³ç­‰çš„æ€åº¦å‘ç”Ÿäº†å“ªäº›å˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªensembleæ´»åŠ¨å­¦ä¹ ç®¡é“ï¼Œç”¨äºè®­ç»ƒä¸€ä¸ªç«‹åœºåˆ†ç±»å™¨ã€‚æˆ‘ä»¬çš„åˆ›æ–°åœ¨äºï¼Œä¼Šæœ—å¥³æ€§å‚ä¸äº†è¿™ä¸ªäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å»ºæ„è¿‡ç¨‹ä¸­ï¼Œä¸ä»…æä¾›æ ‡ç­¾ï¼Œè¿˜æä¾›äº†æœ‰ä»·å€¼çš„å…³é”®è¯ï¼Œä»¥åŠä¸€äº›å¯¼å‘é‡‡æ ·æ­¥éª¤ä¸­çš„çŸ­æ–‡æ¡ˆä¾‹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè´å¨…Â·è‰¾ç±³å°¼çš„å»ä¸–å¯¼è‡´äº†æ³¢æ–¯è¯­è¨€è®¨è®ºçš„å„ç§æåŒ–ï¼Œæ­£é¢å’Œè´Ÿé¢çš„æ¨æ–‡æ•°é‡å‡å¢åŠ ï¼Œæ­£é¢æ¨æ–‡æ•°é‡ç•¥å¤§äºè´Ÿé¢æ¨æ–‡æ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨å¸æˆ·åˆ›å»ºæ—¶é—´æ–¹é¢ï¼ŒæŠ—æŠ¤æŠ¤æŠ¤ Twitter å¸æˆ·ä¸æ”¯æŒæŠ—è®® Twitter å¸æˆ·ä¹‹é—´ï¼Œåè€…æ›´åŠ ç±»ä¼¼äºåŸºçº¿æ³¢æ–¯ Twitter æ´»åŠ¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="URL-A-Representation-Learning-Benchmark-for-Transferable-Uncertainty-Estimates"><a href="#URL-A-Representation-Learning-Benchmark-for-Transferable-Uncertainty-Estimates" class="headerlink" title="URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates"></a>URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03810">http://arxiv.org/abs/2307.03810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mkirchhof/url">https://github.com/mkirchhof/url</a></li>
<li>paper_authors: Michael Kirchhof, BÃ¡lint MucsÃ¡nyi, Seong Joon Oh, Enkelejda Kasneci</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ä¸ªåä¸ºâ€œUncertainty-aware Representation Learningâ€ï¼ˆURLï¼‰çš„æµ‹è¯•åº“ï¼Œç”¨äºè¯„ä¼°å¯¹äºä¸åŒç±»å‹çš„èµ„æ–™é›†è¿›è¡Œè½¬æ¢çš„è¡¨è¾¾å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†åä¸€ç§ä¸åŒçš„ä¸ç¡®å®šé‡åŒ–æ–¹æ³•ï¼Œä»ImageNetä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åè½¬ç§»åˆ°å…«ä¸ªä¸‹æ¸¸èµ„æ–™é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>results: ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸“æ³¨äºè¡¨è¾¾æœ¬èº«çš„ä¸ç¡®å®šæ€§æˆ–ç›´æ¥ä¼°è®¡é¢„æµ‹é£é™©çš„æ–¹æ³•è¡¨ç°è¾ƒå¥½ï¼Œä½†å®ç°å¯è½¬ç§»çš„ä¸ç¡®å®šé‡åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªå¼€å¯çš„æŒ‘æˆ˜ã€‚<details>
<summary>Abstract</summary>
Representation learning has significantly driven the field to develop pretrained models that can act as a valuable starting point when transferring to new datasets. With the rising demand for reliable machine learning and uncertainty quantification, there is a need for pretrained models that not only provide embeddings but also transferable uncertainty estimates. To guide the development of such models, we propose the Uncertainty-aware Representation Learning (URL) benchmark. Besides the transferability of the representations, it also measures the zero-shot transferability of the uncertainty estimate using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers that are pretrained on ImageNet and transferred to eight downstream datasets. We find that approaches that focus on the uncertainty of the representation itself or estimate the prediction risk directly outperform those that are based on the probabilities of upstream classes. Yet, achieving transferable uncertainty quantification remains an open challenge. Our findings indicate that it is not necessarily in conflict with traditional representation learning goals. Code is provided under https://github.com/mkirchhof/url .
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>å°†æ–‡æœ¬ç¿»è¯‘æˆç®€åŒ–ä¸­æ–‡ã€‚</SYS>>ç°ä»£åŒ–å­¦ä¹ ä¸­ï¼Œè¡¨ç°å­¦ä¹ å·²ç»å¸¦æ¥äº†å¤§é‡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥ä½œä¸ºæ–°æ•°æ®é›†çš„èµ·ç‚¹è¿›è¡Œè½¬ç§»ã€‚éšç€å¯é æœºå™¨å­¦ä¹ å’Œä¸ç¡®å®šé‡è¯„ä¼°çš„éœ€æ±‚å¢åŠ ï¼Œéœ€è¦å¼€å‘å¯ä»¥æä¾›åµŒå…¥å’Œä¼ è¾“ä¸ç¡®å®šåº¦ä¼°è®¡çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸ºäº†å¯¼å¼•è¿™äº›æ¨¡å‹çš„å‘å±•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ç¡®å®šæ€§æ„ŸçŸ¥å­¦ä¹ ï¼ˆURLï¼‰ benchmarkã€‚é™¤äº†è¡¨è¾¾çš„ä¼ è¾“æ€§å¤–ï¼Œå®ƒè¿˜æµ‹é‡äº†é›¶æ‰¹é‡ä¼ è¾“ä¸ç¡®å®šåº¦ä¼°è®¡çš„æ–°åº¦é‡ã€‚æˆ‘ä»¬ä½¿ç”¨ URL è¯„ä¼°äº† eleven ç§ ImageNet é¢„è®­ç»ƒçš„ä¸ç¡®å®šåº¦ä¼°è®¡å™¨ï¼Œå¹¶å°†å®ƒä»¬è½¬ç§»åˆ°å…«ä¸ªä¸‹æ¸¸æ•°æ®é›†ã€‚æˆ‘ä»¬å‘ç°ï¼Œå…³æ³¨è¡¨è¾¾æœ¬èº«çš„ä¸ç¡®å®šåº¦æˆ–ç›´æ¥ä¼°è®¡é¢„æµ‹é£é™©çš„æ–¹æ³•è¡¨ç°è¾ƒå¥½ã€‚ç„¶è€Œï¼Œå®ç°ä¼ è¾“ä¸ç¡®å®šåº¦ä¼°è®¡ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œè¿™å¹¶ä¸ä¸€å®šä¸ä¼ ç»Ÿè¡¨ç°å­¦ä¹ ç›®æ ‡åœ¨å†²çªã€‚ä»£ç å¯ä»¥åœ¨ <https://github.com/mkirchhof/url> è·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="CLIPMasterPrints-Fooling-Contrastive-Language-Image-Pre-training-Using-Latent-Variable-Evolution"><a href="#CLIPMasterPrints-Fooling-Contrastive-Language-Image-Pre-training-Using-Latent-Variable-Evolution" class="headerlink" title="CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution"></a>CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03798">http://arxiv.org/abs/2307.03798</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matfrei/clipmasterprints">https://github.com/matfrei/clipmasterprints</a></li>
<li>paper_authors: Matthias Freiberger, Peter Kun, Anders Sundnes LÃ¸vlie, Sebastian Risi</li>
<li>for:  This paper demonstrates the vulnerability of Contrastive Language-Image Pre-training (CLIP) models to â€œfooling master imagesâ€ that can manipulate the modelâ€™s confidence score for a wide range of prompts, while being unrecognizable to humans.</li>
<li>methods:  The authors mine fooling master images by searching the latent space of generative models using evolution strategies or stochastic gradient descent. They investigate the properties of these mined images and find that they can generalize to a large number of semantically related captions.</li>
<li>results:  The authors evaluate two possible mitigation strategies and find that the vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained multi-modal networks. They argue for the mitigation of modality gaps in CLIP and related multi-modal approaches to improve their robustness.Hereâ€™s the full summary in Simplified Chinese:</li>
<li>for: è¿™ä¸ªè®ºæ–‡å±•ç¤ºäº†CLIPæ¨¡å‹å¯¹â€éª—ä¸»å›¾åƒâ€çš„æ„Ÿå—æ€§ï¼Œè¿™äº›å›¾åƒå¯ä»¥è®©CLIPæ¨¡å‹å¯¹å¹¿æ³›çš„æç¤ºä¸­çš„å¤§é‡æç¤ºçš„ç¡®idenceå¾—åˆ†é«˜ï¼Œè€Œäººç±»åˆ™æ— æ³•è¯†åˆ«ã€‚</li>
<li>methods: ä½œè€…é€šè¿‡æ¼”åŒ–ç­–ç•¥æˆ–æ‰¹å¤„ gradient descent æœç´¢ç”Ÿæˆæ¨¡å‹çš„latentç©ºé—´ï¼ŒæŒ–æ˜å‡ºå¯ä»¥è®©CLIPæ¨¡å‹å¯¹å¹¿æ³›çš„æç¤ºä¸­çš„å¤§é‡æç¤ºçš„ç¡®idenceå¾—åˆ†é«˜çš„â€éª—ä¸»å›¾åƒâ€ã€‚ä»–ä»¬investigateè¿™äº›æŒ–æ˜å‡ºæ¥çš„å›¾åƒçš„æ€§è´¨ï¼Œå‘ç°å®ƒä»¬å¯ä»¥æ³›åŒ–åˆ°å¤§é‡ç›¸å…³çš„æç¤ºä¸­ã€‚</li>
<li>results: ä½œè€…è¯„ä¼°äº†ä¸¤ç§å¯èƒ½çš„é˜²å¾¡ç­–ç•¥ï¼Œå‘ç°æ¨¡æ€å·®åœ¨ç›¸å¯¹çš„å¤šmodalç½‘ç»œä¸­å¯¹CLIPæ¨¡å‹çš„æ„Ÿå—æ€§å…·æœ‰ç›´æ¥å…³ç³»ã€‚ä»–ä»¬å› æ­¤ argues foråœ¨CLIPå’Œç›¸å…³å¤šmodalæ–¹æ³•ä¸­å‡å°‘æ¨¡æ€å·®ä»¥æé«˜å…¶Robustnessã€‚<details>
<summary>Abstract</summary>
Models leveraging both visual and textual data such as Contrastive Language-Image Pre-training (CLIP), are increasingly gaining importance. In this work, we show that despite their versatility, such models are vulnerable to what we refer to as fooling master images. Fooling master images are capable of maximizing the confidence score of a CLIP model for a significant number of widely varying prompts, while being unrecognizable for humans. We demonstrate how fooling master images can be mined by searching the latent space of generative models by means of an evolution strategy or stochastic gradient descent. We investigate the properties of the mined fooling master images, and find that images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions. Further, we evaluate two possible mitigation strategies and find that vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained multi-modal networks. From the perspective of vulnerability to off-manifold attacks, we therefore argue for the mitigation of modality gaps in CLIP and related multi-modal approaches. Source code and mined CLIPMasterPrints are available at https://github.com/matfrei/CLIPMasterPrints.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ¨¡å‹ç»“åˆè§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œå¦‚å¯¹ç…§è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ï¼Œåœ¨å½“å‰ç ”ç©¶ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œå³ä½¿è¿™äº›æ¨¡å‹å…·æœ‰å¤šæ ·æ€§ï¼Œå®ƒä»¬å´å®¹æ˜“å—åˆ°æˆ‘ä»¬ç§°ä¸ºâ€œè¯¡å¼‚ä¸»å›¾â€çš„æ”»å‡»ã€‚è¯¡å¼‚ä¸»å›¾å¯ä»¥è®©CLIPæ¨¡å‹å¯¹è®¸å¤šSemanticçš„æç¤ºå…·æœ‰æœ€é«˜çš„ä¿¡ä»»åº¦ï¼Œè€Œè¿™äº›å›¾åƒå¯¹äººç±»æ¥è¯´æ˜¯ä¸å¯è¯†åˆ«çš„ã€‚æˆ‘ä»¬é€šè¿‡æœç´¢ç”Ÿæˆæ¨¡å‹çš„latentç©ºé—´ä½¿ç”¨æ¼”åŒ–ç­–ç•¥æˆ–Stochastic gradient descentæ¥æŒ–æ˜è¯¡å¼‚ä¸»å›¾ã€‚æˆ‘ä»¬ç ”ç©¶äº†æŒ–æ˜å‡ºæ¥çš„è¯¡å¼‚ä¸»å›¾çš„æ€§è´¨ï¼Œå‘ç°å›¾åƒé€šè¿‡ä¸€å° nÃºmero deå›¾åƒæè¿°å­¦ä¹ å¯ä»¥æ¶µç›–ä¸€ä¸ªè®¸å¤šæ›´å¹¿æ³›çš„Semanticç›¸å…³çš„æè¿°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§å¯èƒ½çš„ç¼“è§£ç­–ç•¥ï¼Œå‘ç°æ¨¡å¼å·®å¼‚å¯¼è‡´çš„æ”»å‡»æ€§è´¨ä¸CLIPå’Œç›¸å…³å¤šæ¨¡æ€ç½‘ç»œçš„æ˜“è®­ç»ƒæ€§ç›¸å…³ã€‚ä»æ”»å‡»æ€§è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å› æ­¤ä¸»å¼ åœ¨CLIPå’Œç›¸å…³å¤šæ¨¡æ€ç½‘ç»œä¸­å‡å°‘æ¨¡å¼å·®å¼‚ï¼Œä»¥é¿å…è¯¡å¼‚ä¸»å›¾çš„æ”»å‡»ã€‚ä»£ç å’ŒæŒ–æ˜å‡ºæ¥çš„CLIPMasterPrintså¯ä»¥åœ¨https://github.com/matfrei/CLIPMasterPrintsä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="When-does-the-ID-algorithm-fail"><a href="#When-does-the-ID-algorithm-fail" class="headerlink" title="When does the ID algorithm fail?"></a>When does the ID algorithm fail?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03750">http://arxiv.org/abs/2307.03750</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SOYJUN/Implement-ODR-protocol">https://github.com/SOYJUN/Implement-ODR-protocol</a></li>
<li>paper_authors: Ilya Shpitser</li>
<li>for: æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§è§£å†³å›¾è§£ causal æ¨¡å‹ä¸­ interventional åˆ†å¸ƒ p(Y | do(a)) çš„é—®é¢˜ï¼Œå³ ID ç®—æ³•ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº† ID ç®—æ³•ï¼Œå¹¶æä¾›äº†ä¸€äº›å…¶ä»–å½¢å¼çš„è¡¨è¿°ã€‚</li>
<li>results: æœ¬æ–‡è¯æ˜äº† ID ç®—æ³•æ˜¯æœ‰å£°ç§°çš„ï¼ˆå¯¹äºè¾“å…¥ graphical causal model ä¸­çš„ p(Y | do(a)) è¿›è¡Œæ­£ç¡®çš„å‡½æ•°ï¼‰ï¼Œå¹¶ä¸”å®Œæ•´ï¼ˆå¦‚æœè¾“å…¥ä¸èƒ½è¢«æ¨¡å‹ä¸­çš„ p(Y | do(a)) è¿›è¡Œæ­£ç¡®çš„å‡½æ•°ï¼Œåˆ™ä¼šæ˜¾å¼åœ°æ ‡è¯†ä¸ºå¤±è´¥ï¼‰ã€‚<details>
<summary>Abstract</summary>
The ID algorithm solves the problem of identification of interventional distributions of the form p(Y | do(a)) in graphical causal models, and has been formulated in a number of ways [12, 9, 6]. The ID algorithm is sound (outputs the correct functional of the observed data distribution whenever p(Y | do(a)) is identified in the causal model represented by the input graph), and complete (explicitly flags as a failure any input p(Y | do(a)) whenever this distribution is not identified in the causal model represented by the input graph).   The reference [9] provides a result, the so called "hedge criterion" (Corollary 3), which aims to give a graphical characterization of situations when the ID algorithm fails to identify its input in terms of a structure in the input graph called the hedge. While the ID algorithm is, indeed, a sound and complete algorithm, and the hedge structure does arise whenever the input distribution is not identified, Corollary 3 presented in [9] is incorrect as stated. In this note, I outline the modern presentation of the ID algorithm, discuss a simple counterexample to Corollary 3, and provide a number of graphical characterizations of the ID algorithm failing to identify its input distribution.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€ŠIDç®—æ³•è§£å†³äº† causal æ¨¡å‹ä¸­ intervenational åˆ†å¸ƒçš„å½¢å¼ p(Y | do(a)) çš„é—®é¢˜ï¼Œå¹¶å·²ç»æœ‰å¤šç§è¡¨è¿°æ–¹å¼ [12, 9, 6]ã€‚IDç®—æ³•æ˜¯æœ‰æ•ˆçš„ï¼ˆå¯¹äºè¾“å…¥æ•°æ®åˆ†å¸ƒ p(Y | do(a)) æ˜¯ causal æ¨¡å‹ä¸­çš„æ­£ç¡®å‡½æ•°ï¼‰ï¼Œå¹¶ä¸”æ˜¯å®Œæ•´çš„ï¼ˆæ˜ç¡®åœ°æ ‡è¯†è¾“å…¥æ•°æ®åˆ†å¸ƒ p(Y | do(a)) ä¸èƒ½åœ¨ causal æ¨¡å‹ä¸­è¢«è¯†åˆ«ï¼‰ã€‚å‚è€ƒ [9] æä¾›äº†ä¸€ä¸ªåä¸º "é˜²èŒ‚ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¾Ğ½"ï¼ˆæ‚¬å³°3ï¼‰çš„ç»“æœï¼Œå®ƒç›®çš„æ˜¯ç»™å‡ºä¸€ç§å›¾è§£æ–¹å¼ï¼Œç”¨äºæè¿° ID ç®—æ³•è¾“å…¥åˆ†å¸ƒä¸èƒ½è¢«è¯†åˆ«çš„æƒ…å†µã€‚ç„¶è€Œï¼ŒID ç®—æ³•ç¡®å®æ˜¯ä¸€ä¸ªæœ‰æ•ˆå’Œå®Œæ•´çš„ç®—æ³•ï¼Œè€Œä¸”é˜²èŒ‚ç»“æ„åœ¨è¾“å…¥åˆ†å¸ƒä¸èƒ½è¢«è¯†åˆ«æ—¶ä¼šå‡ºç°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†è¯¦ç»†ä»‹ç» ID ç®—æ³•çš„ç°ä»£è¡¨è¿°æ–¹æ³•ï¼Œæä¾›ä¸€ä¸ªç®€å•çš„åä¾‹ï¼Œä»¥åŠä¸€äº›å›¾è§£æ–¹å¼ï¼Œç”¨äºæè¿° ID ç®—æ³•è¾“å…¥åˆ†å¸ƒä¸èƒ½è¢«è¯†åˆ«çš„æƒ…å†µã€‚
</details></li>
</ul>
<hr>
<h2 id="AI-and-the-EU-Digital-Markets-Act-Addressing-the-Risks-of-Bigness-in-Generative-AI"><a href="#AI-and-the-EU-Digital-Markets-Act-Addressing-the-Risks-of-Bigness-in-Generative-AI" class="headerlink" title="AI and the EU Digital Markets Act: Addressing the Risks of Bigness in Generative AI"></a>AI and the EU Digital Markets Act: Addressing the Risks of Bigness in Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02033">http://arxiv.org/abs/2308.02033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayse Gizem Yasar, Andrew Chong, Evan Dong, Thomas Krendl Gilbert, Sarah Hladikova, Roland Maio, Carlos Mougan, Xudong Shen, Shubham Singh, Ana-Andreea Stoica, Savannah Thais, Miri Zilka</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨é€doFiltering the risks of bigness in digital markets, particularly in relation to generative AI systems.</li>
<li>methods: ä½œè€…æè®® integrate certain AI software as core platform services,å¹¶ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸ certain developers as gatekeepers under the EUâ€™s Digital Markets Act (DMA).</li>
<li>results: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è¯„ä¼° gatekeeper obligationsçš„æ–¹æ³•ï¼Œä»¥ç¡®ä¿å®ƒä»¬è¦†ç›– generative AI servicesã€‚è¿™äº›ç»“æœå¯ä»¥å¸®åŠ©æ¬§ç›Ÿåœ¨è€ƒè™‘ generative AI ç‰¹å®šè§„åˆ™å’Œå¯èƒ½çš„ DMA ä¿®è®¢æ—¶ï¼Œæ›´å¥½åœ°ä¿æŒå¤šæ ·æ€§å’Œå¼€æ”¾æ€§åœ¨ generative AI æœåŠ¡ä¸­ã€‚<details>
<summary>Abstract</summary>
As AI technology advances rapidly, concerns over the risks of bigness in digital markets are also growing. The EU's Digital Markets Act (DMA) aims to address these risks. Still, the current framework may not adequately cover generative AI systems that could become gateways for AI-based services. This paper argues for integrating certain AI software as core platform services and classifying certain developers as gatekeepers under the DMA. We also propose an assessment of gatekeeper obligations to ensure they cover generative AI services. As the EU considers generative AI-specific rules and possible DMA amendments, this paper provides insights towards diversity and openness in generative AI services.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå¤§å‹æ•°å­—å¸‚åœºçš„é£é™©é—®é¢˜ä¹Ÿåœ¨ä¸æ–­å¢é•¿ã€‚æ¬§ç›Ÿçš„æ•°å­—å¸‚åœºæ³•ï¼ˆDMAï¼‰æƒ³è¦è§£å†³è¿™äº›é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¡†æ¶å¯èƒ½ä¸å¤Ÿè¦†ç›–ç”ŸæˆAIç³»ç»Ÿï¼Œè¿™äº›ç³»ç»Ÿå¯èƒ½ä¼šæˆä¸ºAIæœåŠ¡çš„é—¨æˆ·ã€‚è¿™ç¯‡æ–‡ç« æè®®å°†æŸäº›AIè½¯ä»¶ä½œä¸ºæ ¸å¿ƒå¹³å°æœåŠ¡ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°è¿›DMAï¼Œå¹¶å°†æŸäº›å¼€å‘è€… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸ä¸ºDMAä¸­çš„â€œé—¨æ§›keeperâ€ã€‚æˆ‘ä»¬è¿˜æè®®å¯¹é—¨æ§›keeperçš„ä¹‰åŠ¡è¿›è¡Œè¯„ä¼°ï¼Œä»¥ç¡®ä¿å®ƒä»¬è¦†ç›–ç”ŸæˆAIæœåŠ¡ã€‚éšç€æ¬§ç›Ÿè€ƒè™‘ç”ŸæˆAIç‰¹æœ‰çš„è§„åˆ™å’Œå¯èƒ½çš„DMAä¿®æ”¹ï¼Œè¿™ç¯‡æ–‡ç« æä¾›äº†å…³äºå¤šæ ·æ€§å’Œå¼€æ”¾æ€§åœ¨ç”ŸæˆAIæœåŠ¡æ–¹é¢çš„æ´å¯Ÿã€‚
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Robotic-Sonographer-Mutual-Information-based-Disentangled-Reward-Learning-from-Few-Demonstrations"><a href="#Intelligent-Robotic-Sonographer-Mutual-Information-based-Disentangled-Reward-Learning-from-Few-Demonstrations" class="headerlink" title="Intelligent Robotic Sonographer: Mutual Information-based Disentangled Reward Learning from Few Demonstrations"></a>Intelligent Robotic Sonographer: Mutual Information-based Disentangled Reward Learning from Few Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03705">http://arxiv.org/abs/2307.03705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongliang Jiang, Yuan Bi, Mingchuan Zhou, Ying Hu, Michael Burke, Nassir Navab<br>for:The paper proposes an intelligent robotic sonographer to autonomously explore target anatomies and navigate a US probe to a relevant 2D plane by learning from expert.methods:The proposed approach uses a neural reward function, ranked pairwise image comparisons, and mutual information to learn the â€œlanguage of sonographyâ€ and overcome inter-patient variations. A Gaussian distribution-based filter is also developed to evaluate the quality of the expertâ€™s demonstrations.results:The proposed approach is demonstrated to be effective in different experiments, including representative experiments for the â€œlineâ€ target and â€œpointâ€ target on vascular phantom and two ex-vivo animal organ phantoms, respectively. The results showed that the proposed advanced framework can robustly work on different kinds of known and unseen phantoms.<details>
<summary>Abstract</summary>
Ultrasound (US) imaging is widely used for biometric measurement and diagnosis of internal organs due to the advantages of being real-time and radiation-free. However, due to high inter-operator variability, resulting images highly depend on operators' experience. In this work, an intelligent robotic sonographer is proposed to autonomously "explore" target anatomies and navigate a US probe to a relevant 2D plane by learning from expert. The underlying high-level physiological knowledge from experts is inferred by a neural reward function, using a ranked pairwise image comparisons approach in a self-supervised fashion. This process can be referred to as understanding the "language of sonography". Considering the generalization capability to overcome inter-patient variations, mutual information is estimated by a network to explicitly extract the task-related and domain features in latent space. Besides, a Gaussian distribution-based filter is developed to automatically evaluate and take the quality of the expert's demonstrations into account. The robotic localization is carried out in coarse-to-fine mode based on the predicted reward associated to B-mode images. To demonstrate the performance of the proposed approach, representative experiments for the "line" target and "point" target are performed on vascular phantom and two ex-vivo animal organ phantoms (chicken heart and lamb kidney), respectively. The results demonstrated that the proposed advanced framework can robustly work on different kinds of known and unseen phantoms.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¶…å£°æˆåƒï¼ˆUSï¼‰æˆä¸ºå†…éƒ¨å™¨å®˜è¯Šæ–­å’Œç”Ÿç‰©ç±³ç‰¹Ñ€Ğ¸è¯„ä¼°çš„å¹¿æ³›åº”ç”¨ï¼Œä¸»è¦åŸå› æ˜¯å®ƒå…·æœ‰å®æ—¶å’Œæ— æ ¸ç‡ç°çš„ä¼˜ç‚¹ã€‚ç„¶è€Œï¼Œç”±äºæ“ä½œå‘˜ä¹‹é—´çš„é«˜åº¦å˜åŒ–ï¼Œå¯¼è‡´å›¾åƒå…·æœ‰æ“ä½œå‘˜çš„ç»éªŒæ•ˆåº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä¸€ä¸ªæ™ºèƒ½æœºå™¨äººè¶…å£°æµ‹è¯•å‘˜è¢«æè®®ï¼Œä»¥è‡ªä¸»åœ°"æ¢ç´¢"ç›®æ ‡è§£å‰–ç»“æ„ï¼Œå¹¶ä½¿ç”¨å­¦ä¹ ä»ä¸“å®¶å¾—åˆ°çš„é«˜çº§ç”Ÿç†çŸ¥è¯†æ¥å¯¼èˆªUS probã€‚è¿™ç§è¿‡ç¨‹å¯ä»¥ç§°ä¸º"åŒ»å­¦è¶…å£°è¯­è¨€"çš„ç†è§£ã€‚å¦å¤–ï¼Œä¸ºäº†å¼ºåŒ–å›¾åƒçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸€ä¸ªåŸºäºGaussianåˆ†å¸ƒçš„ç­›é€‰å™¨è¢«å¼€å‘ï¼Œä»¥è‡ªåŠ¨è¯„ä¼°ä¸“å®¶çš„ç¤ºèŒƒè´¨é‡ã€‚æœºå™¨äººæœ¬åœ°åŒ–é‡‡ç”¨äº†ä»é¢„æµ‹çš„å¥–åŠ±å…³ç³»æ¥è¿›è¡Œç²—ç•¥åˆ°ç»†ç²’çš„æ¨¡å¼ï¼Œä»¥å®ç°å¯¹Bæ¨¡å¼å›¾åƒçš„é¢„æµ‹ã€‚ä¸ºè¯æ˜æå‡ºçš„æ–¹æ³•çš„æ€§èƒ½ï¼Œå¯¹ä¸åŒç±»å‹çš„çŸ¥åå’ŒæœªçŸ¥èŸè¢‹ï¼ˆvascular phantomå’Œä¸¤åªé…ªè‚‰åŠ¨ç‰©å™¨å®˜èŸè¢‹ï¼‰è¿›è¡Œäº†ä»£è¡¨æ€§çš„å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæå‡ºçš„é«˜çº§æ¡†æ¶å¯ä»¥åœ¨ä¸åŒç±»å‹çš„èŸè¢‹ä¸Šå…·æœ‰robustæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Potential-of-Knowledge-Prompted-ChatGPT-for-Enhancing-Drug-Trafficking-Detection-on-Social-Media"><a href="#Unveiling-the-Potential-of-Knowledge-Prompted-ChatGPT-for-Enhancing-Drug-Trafficking-Detection-on-Social-Media" class="headerlink" title="Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media"></a>Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03699">http://arxiv.org/abs/2307.03699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanbo Hu, Bin Liu, Xin Li, Yanfang Ye<br>for:The paper aims to detect illicit drug trafficking activities on social media, specifically on platforms like Instagram and Twitter.methods:The authors use large language models (LLMs), such as ChatGPT, to detect drug trafficking activities. They propose an analytical framework that incorporates prior knowledge and scenario-based prompts to improve the accuracy of drug trafficking detection.results:The proposed framework outperforms other baseline language models in terms of drug trafficking detection accuracy, with a remarkable improvement of nearly 12%. The use of prior knowledge and scenario-based prompts helps ChatGPT effectively identify and label drug trafficking activities, even in the presence of deceptive language and euphemisms used by drug dealers to evade detection.<details>
<summary>Abstract</summary>
Social media platforms such as Instagram and Twitter have emerged as critical channels for drug marketing and illegal sale. Detecting and labeling online illicit drug trafficking activities becomes important in addressing this issue. However, the effectiveness of conventional supervised learning methods in detecting drug trafficking heavily relies on having access to substantial amounts of labeled data, while data annotation is time-consuming and resource-intensive. Furthermore, these models often face challenges in accurately identifying trafficking activities when drug dealers use deceptive language and euphemisms to avoid detection. To overcome this limitation, we conduct the first systematic study on leveraging large language models (LLMs), such as ChatGPT, to detect illicit drug trafficking activities on social media. We propose an analytical framework to compose \emph{knowledge-informed prompts}, which serve as the interface that humans can interact with and use LLMs to perform the detection task. Additionally, we design a Monte Carlo dropout based prompt optimization method to further to improve performance and interpretability. Our experimental findings demonstrate that the proposed framework outperforms other baseline language models in terms of drug trafficking detection accuracy, showing a remarkable improvement of nearly 12\%. By integrating prior knowledge and the proposed prompts, ChatGPT can effectively identify and label drug trafficking activities on social networks, even in the presence of deceptive language and euphemisms used by drug dealers to evade detection. The implications of our research extend to social networks, emphasizing the importance of incorporating prior knowledge and scenario-based prompts into analytical tools to improve online security and public safety.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç¤¾äº¤åª’ä½“å¹³å°å¦‚Instagramå’ŒTwitterå·²æˆä¸ºè¯å“å¸‚åœºå’Œéæ³•é”€å”®çš„é‡è¦æ¸ é“ã€‚æ£€æµ‹å’Œæ ‡æ³¨åœ¨çº¿è´©å–è¯å“æ´»åŠ¨å˜å¾—éå¸¸é‡è¦ï¼Œä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨æ£€æµ‹è´©å–è¯å“ä¸Šå‡­å€Ÿå…·æœ‰è¾ƒå¤§é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œæ£€æµ‹æ˜¯ä¸å¯é çš„ï¼Œè€Œä¸”è¿™äº›æ¨¡å‹ç»å¸¸é‡åˆ°è¯†åˆ«è´©å–è¯å“æ´»åŠ¨æ—¶ï¼Œè´©å–è€…ä½¿ç”¨æ¬ºéª—è¯­è¨€å’Œæ¨èè¯æ¥é¿å…æ£€æµ‹çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç¬¬ä¸€æ¬¡ç³»ç»Ÿæ€§çš„ç ”ç©¶ï¼Œåˆ©ç”¨å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚ChatGPTï¼Œæ£€æµ‹ç¤¾äº¤åª’ä½“ä¸Šçš„è´©å–è¯å“æ´»åŠ¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œå°†çŸ¥è¯†å‘Šè¯‰ä½œä¸ºç•Œé¢ï¼Œè®©äººä»¬é€šè¿‡LLMè¿›è¡Œæ£€æµ‹ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†åŸºäºMonte Carlo Dropoutçš„æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œä»¥æé«˜æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨è´©å–è¯å“æ£€æµ‹ç²¾åº¦æ–¹é¢æ¯”åŸºelineè¯­è¨€æ¨¡å‹æé«˜äº†12%ä»¥ä¸Šã€‚é€šè¿‡å°†çŸ¥è¯†å’Œæˆ‘ä»¬æå‡ºçš„æç¤ºç»“åˆä½¿ç”¨ï¼ŒChatGPTå¯ä»¥åœ¨ç¤¾äº¤ç½‘ç»œä¸Šæœ‰æ•ˆåœ°è¯†åˆ«å’Œæ ‡æ³¨è´©å–è¯å“æ´»åŠ¨ï¼Œå³ä½¿è´©å–è€…ä½¿ç”¨æ¬ºéª—è¯­è¨€å’Œæ¨èè¯æ¥é¿å…æ£€æµ‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¯¹ç¤¾äº¤ç½‘ç»œäº§ç”Ÿé‡è¦çš„æ‰©å±•ï¼Œå¼ºè°ƒåœ¨çº¿å®‰å…¨å’Œå…¬å…±å®‰å…¨ä¸­åŒ…å«çŸ¥è¯†å’Œåœºæ™¯åŸºäºçš„åˆ†æå·¥å…·çš„é‡è¦æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Scalable-Membership-Inference-Attacks-via-Quantile-Regression"><a href="#Scalable-Membership-Inference-Attacks-via-Quantile-Regression" class="headerlink" title="Scalable Membership Inference Attacks via Quantile Regression"></a>Scalable Membership Inference Attacks via Quantile Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03694">http://arxiv.org/abs/2307.03694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Bertran, Shuai Tang, Michael Kearns, Jamie Morgenstern, Aaron Roth, Zhiwei Steven Wu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ”»å‡»ç”¨é»‘ç›’è®­ç»ƒçš„æ¨¡å‹ï¼Œä»¥ç¡®å®šç‰¹å®šç¤ºä¾‹æ˜¯å¦è¢«ç”¨äºè®­ç»ƒã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨é‡åŒ–å›å½’æ¥æ”»å‡»æ¨¡å‹ï¼Œå¹¶ä¸éœ€è¦çŸ¥é“æ¨¡å‹çš„ç»“æ„ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–¹æ³•å¯ä»¥ä¸ç°æœ‰æœ€ä½³æ”»å‡»æ–¹æ³•ç«äº‰ï¼Œè€Œä¸”éœ€è¦è®­ç»ƒåªä¸€ä¸ªæ¨¡å‹ï¼Œç›¸æ¯”ä¹‹å‰çš„æ”»å‡»æ–¹æ³•éœ€è¦è®­ç»ƒå¤šä¸ªæ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large.   We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆå‘˜æ¨æµ‹æ”»å‡»æ˜¯è®¾è®¡ç”¨é»‘ç›’è®¿é—®è®­ç»ƒè¿‡çš„æ¨¡å‹æ¥ç¡®å®šç‰¹å®šç¤ºä¾‹æ˜¯å¦åœ¨è®­ç»ƒä¸­ä½¿ç”¨è¿‡ã€‚æˆå‘˜æ¨æµ‹å¯ä»¥å½¢å¼åŒ–ä¸ºä¸€ä¸ªå‡è®¾æµ‹è¯•é—®é¢˜ã€‚ç°æœ‰æœ€æœ‰æ•ˆçš„æ”»å‡»æ–¹æ³•æ˜¯ä¼°è®¡æ¨¡å‹åœ¨çœŸå®æ ‡ç­¾ä¸Šçš„ä¿¡ä»»åº¦åˆ†å¸ƒä¸­çš„æŸäº›æµ‹è¯•ç»Ÿè®¡ï¼ˆé€šå¸¸æ˜¯æ¨¡å‹å¯¹çœŸå®æ ‡ç­¾çš„ä¿¡ä»»åº¦ï¼‰ã€‚è¿™äº›æ”»å‡»é€šå¸¸æ˜¯éå¸¸è®¡ç®—æ˜‚è´µçš„ï¼Œç‰¹åˆ«æ˜¯å½“æ¨¡å‹è¢«æ”»å‡»æ—¶éå¸¸å¤§ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ”»å‡»æ–¹æ³•ï¼ŒåŸºäºæ¨¡å‹ä¸‹æ”»å‡»çš„ä¿¡ä»»åº¦åˆ†å¸ƒä¸­çš„confidenceåˆ†å¸ƒè¿›è¡Œé‡åŒ–å›å½’ã€‚æˆ‘ä»¬æ˜¾ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰çš„é˜´å½±æ¨¡å‹æ”»å‡»ç›¸å½“ç«äº‰åŠ›ï¼Œè€Œéœ€è¦æ›´å°‘çš„è®¡ç®—èµ„æºï¼Œå› ä¸ºæˆ‘ä»¬çš„æ”»å‡»åªéœ€è¦è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æè®®çš„æ”»å‡»æ–¹æ³•ä¸éœ€è¦çŸ¥é“æ¨¡å‹ä¸‹æ”»å‡»çš„ç»“æ„ï¼Œå› æ­¤æ˜¯çœŸæ­£çš„â€œé»‘ç›’â€æ”»å‡»ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„ dataset å’Œæ¨¡å‹ç»“æ„ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥è¯æ˜è¿™ç§æ–¹æ³•çš„å¯è¡Œæ€§ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/08/cs.AI_2023_07_08/" data-id="clltaagm70007r888fvyf1yrc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/08/cs.CL_2023_07_08/" class="article-date">
  <time datetime="2023-07-07T16:00:00.000Z" itemprop="datePublished">2023-07-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/08/cs.CL_2023_07_08/">cs.CL - 2023-07-08 19:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Advancements-in-Scientific-Controllable-Text-Generation-Methods"><a href="#Advancements-in-Scientific-Controllable-Text-Generation-Methods" class="headerlink" title="Advancements in Scientific Controllable Text Generation Methods"></a>Advancements in Scientific Controllable Text Generation Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05538">http://arxiv.org/abs/2307.05538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnav Goel, Medha Hira, Avinash Anand, Siddhesh Bangar, Dr. Rajiv Ratn Shah</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§å¯æ§æ–‡æœ¬ç”ŸæˆæŠ€æœ¯ï¼Œç”¨äºç§‘å­¦æ–‡çŒ®ä¸­çš„è‡ªåŠ¨ç”Ÿæˆã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†ä¸ƒä¸ªç»„ä»¶ï¼ŒåŒ…æ‹¬è¯­æ–™åº“ã€æ¨¡å‹ã€æ’åºç­–ç•¥ã€å¹²æ‰°æ–¹æ³•ã€éšæœºåŒ–æ–¹æ³•ã€æƒé‡è¡¥åšå’Œæ‹Ÿåˆç­–ç•¥ã€‚è¿™äº›ç»„ä»¶ä¹‹é—´å¯ä»¥è¿›è¡Œç»„åˆï¼Œä»¥å®ç°ä¸åŒçš„æ§åˆ¶æ•ˆæœã€‚</li>
<li>results: æœ¬ç ”ç©¶é€šè¿‡ç†è®ºåˆ†æå’Œè´¨é‡è¯„ä¼°æ¥æè¿°è¿™äº›æ–¹æ³•çš„å·¥ä½œåŸç†å’Œä¼˜åŠ£ç‚¹ï¼Œå¹¶æä¾›äº†å¯èƒ½çš„æ–°architectureã€‚æœªæ¥çš„å®éªŒç ”ç©¶å°†æ¯”è¾ƒè¿™äº›æ–¹æ³•çš„æ•ˆæœï¼Œä»¥äº†è§£å®ƒä»¬åœ¨ä¸åŒæƒ…å†µä¸‹çš„ä¼˜åŠ£ç‚¹ã€‚<details>
<summary>Abstract</summary>
The previous work on controllable text generation is organized using a new schema we provide in this study. Seven components make up the schema, and each one is crucial to the creation process. To accomplish controlled generation for scientific literature, we describe the various modulation strategies utilised to modulate each of the seven components. We also offer a theoretical study and qualitative examination of these methods. This insight makes possible new architectures based on combinations of these components. Future research will compare these methods empirically to learn more about their strengths and utility.
</details>
<details>
<summary>æ‘˜è¦</summary>
å…ˆå‰çš„æ–‡æœ¬æ§åˆ¶ç”Ÿæˆç ”ç©¶ç”±æˆ‘ä»¬æä¾›çš„æ–°æ¶æ„ç»„ç»‡ã€‚è¿™ä¸ªæ¶æ„åŒ…æ‹¬7ä¸ªç»„ä»¶ï¼Œæ¯ä¸ªç»„ä»¶éƒ½æ˜¯ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸å¯æˆ–ç¼ºçš„ã€‚ä¸ºäº†å®ç°ç§‘å­¦æ–‡çŒ®æ§åˆ¶ç”Ÿæˆï¼Œæˆ‘ä»¬ä»‹ç»äº†å¯¹æ¯ä¸ªç»„ä»¶è¿›è¡Œè°ƒæ•´çš„å„ç§è°ƒåˆ¶ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜æä¾›äº†è¿™äº›æ–¹æ³•çš„ç†è®ºç ”ç©¶å’Œè´¨é‡åˆ†æã€‚è¿™äº›æ´å¯Ÿå¯èƒ½å¯¼è‡´åŸºäºè¿™äº›ç»„ä»¶çš„æ–°æ¶æ„çš„å¼€å‘ã€‚æœªæ¥çš„ç ”ç©¶å°†é€šè¿‡å®éªŒæ¯”è¾ƒè¿™äº›æ–¹æ³•çš„ä¼˜åŠ¿å’Œå®ç”¨æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Stitch-in-Time-Saves-Nine-Detecting-and-Mitigating-Hallucinations-of-LLMs-by-Validating-Low-Confidence-Generation"><a href="#A-Stitch-in-Time-Saves-Nine-Detecting-and-Mitigating-Hallucinations-of-LLMs-by-Validating-Low-Confidence-Generation" class="headerlink" title="A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation"></a>A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03987">http://arxiv.org/abs/2307.03987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, Dong Yu</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æé«˜å¤§å‹è‡ªç„¶è¯­è¨€å¤„ç†å™¨çš„å¯é æ€§å’Œå¯ä¿¡åº¦ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ´»åŠ¨æ£€æµ‹å’Œçº æ­£æ–¹æ³•ï¼Œä»¥è§£å†³å¤§å‹è‡ªç„¶è¯­è¨€å¤„ç†å™¨ä¸­çš„â€œå¹»è§â€é—®é¢˜ã€‚è¿™ç§æ–¹æ³•åŒ…æ‹¬åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é¦–å…ˆæ ‡è¯†æœ‰å¯èƒ½å­˜åœ¨å¹»è§çš„è¯­å¥ï¼Œç„¶åé€šè¿‡éªŒè¯ç¨‹åºç¡®è®¤å…¶æ­£ç¡®æ€§ï¼Œå¹¶æœ€åçº æ­£å·²ç¡®æ£€æµ‹åˆ°çš„å¹»è§ã€‚</li>
<li>results: ç»è¿‡å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„æ£€æµ‹å’Œçº æ­£æ–¹æ³•å¯ä»¥æˆåŠŸåœ°é™ä½GPT-3.5æ¨¡å‹ä¸­çš„å¹»è§ç‡ï¼Œä»47.5%é™ä½åˆ°14.5%çš„å¹³å‡å€¼ã€‚æ­¤å¤–ï¼Œè¿™ç§æ–¹æ³•è¿˜å¯ä»¥åœ¨ä¸åŒç±»å‹çš„é—®é¢˜ä¸Šè¿›è¡Œæœ‰æ•ˆåœ°åº”ç”¨ï¼ŒåŒ…æ‹¬å¤šæ­¥é—®é¢˜å’ŒFalse Premiseé—®é¢˜ã€‚<details>
<summary>Abstract</summary>
Recently developed large language models have achieved remarkable success in generating fluent and coherent text. However, these models often tend to 'hallucinate' which critically hampers their reliability. In this work, we address this crucial problem and propose an approach that actively detects and mitigates hallucinations during the generation process. Specifically, we first identify the candidates of potential hallucination leveraging the model's logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process. Through extensive experiments with GPT-3.5 (text-davinci-003) on the 'article generation task', we first demonstrate the individual efficacy of our detection and mitigation techniques. Specifically, the detection technique achieves a recall of ~88% and the mitigation technique successfully mitigates 57.6% of the correctly detected hallucinations. Importantly, our mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. Then, we show that the proposed active detection and mitigation approach successfully reduces the hallucinations of the GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the effectiveness and wide applicability of our approach through additional studies including performance on different types of questions (multi-hop and false premise questions) and with another LLM from a different model family (Vicuna). In summary, our work contributes to improving the reliability and trustworthiness of large language models, a crucial step en route to enabling their widespread adoption in real-world applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°åœ¨å·²ç»å¼€å‘å‡ºçš„å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»è¾¾åˆ°äº†éå¸¸å‡ºè‰²çš„æˆç»©ï¼Œå¯ä»¥ç”Ÿæˆæµç•…ã€ä¸€è‡´çš„æ–‡æœ¬ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ç»å¸¸ä¼šâ€œå¹»è§‰â€ï¼Œè¿™ä¼šä¸¥é‡é™ä½å…¶å¯é æ€§ã€‚åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³è¿™ä¸ªé‡è¦çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ´»åŠ¨æ£€æµ‹å’Œçº æ­£å¹»è§‰çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡æ¨¡å‹çš„æå€¼è¾“å‡ºå€¼æ¥è®¤ä¸ºæ½œåœ¨çš„å¹»è§‰è€…ï¼Œç„¶åé€šè¿‡éªŒè¯è¿‡ç¨‹æ¥ç¡®è®¤å…¶æ­£ç¡®æ€§ï¼Œå¹¶åœ¨æ£€æµ‹åˆ°çš„å¹»è§‰è¢«çº æ­£åç»§ç»­è¿›è¡Œç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡å¯¹GPT-3.5ï¼ˆæ–‡æœ¬è¾¾æ–‡å¥‡003ï¼‰è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ£€æµ‹å’Œçº æ­£æŠ€æœ¯çš„ä¸ªäººæ•ˆæœã€‚ Specifically, our detection technique achieves a recall of approximately 88%, and the mitigation technique successfully mitigates 57.6% of the correctly detected hallucinations. Furthermore, our mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. Finally, we show that our active detection and mitigation approach successfully reduces the hallucinations of the GPT-3.5 model from 47.5% to 14.5% on average. We also demonstrate the effectiveness and wide applicability of our approach through additional studies including performance on different types of questions (multi-hop and false premise questions) and with another LLM from a different model family (Vicuna). In summary, our work contributes to improving the reliability and trustworthiness of large language models, a crucial step en route to enabling their widespread adoption in real-world applications.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Capability-of-Large-scale-Language-Models-on-Chinese-Grammatical-Error-Correction-Task"><a href="#Evaluating-the-Capability-of-Large-scale-Language-Models-on-Chinese-Grammatical-Error-Correction-Task" class="headerlink" title="Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task"></a>Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03972">http://arxiv.org/abs/2307.03972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fanyi Qu, Yunfang Wu</li>
<li>for: è¿™ä»½æŠ¥å‘Šæ—¨åœ¨æ¢è®¨å¤§å‹è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œä»¥åŠæœªæ¥å·¥ä½œçš„æŒ‡å¯¼ã€‚</li>
<li>methods: æˆ‘ä»¬åœ¨4ä¸ªä¸­æ–‡grammatical error correctionï¼ˆGECï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†3ç§ä¸åŒçš„LLMæ¨¡å‹çš„å®éªŒã€‚</li>
<li>results: æˆ‘ä»¬å‘ç°LLMsåœ¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸Šçš„æ€§èƒ½ä¸è¶³å‰ä¸€ä»£æ¨¡å‹ï¼Œå¹¶ä¸”å­˜åœ¨è¿‡ corrections çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†ä¸åŒæ•°æ®åˆ†å¸ƒä¸‹LLMsçš„æ€§èƒ½æœ‰å¾ˆå¤§å·®å¼‚ã€‚è¿™äº›å‘ç°è¡¨æ˜éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥LLMsåœ¨ä¸­æ–‡GECä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚<details>
<summary>Abstract</summary>
Large-scale language models (LLMs) has shown remarkable capability in various of Natural Language Processing (NLP) tasks and attracted lots of attention recently. However, some studies indicated that large language models fail to achieve promising result beyond the state-of-the-art models in English grammatical error correction (GEC) tasks. In this report, we aim to explore the how large language models perform on Chinese grammatical error correction tasks and provide guidance for future work. We conduct experiments with 3 different LLMs of different model scale on 4 Chinese GEC dataset. Our experimental results indicate that the performances of LLMs on automatic evaluation metrics falls short of the previous sota models because of the problem of over-correction. Furthermore, we also discover notable variations in the performance of LLMs when evaluated on different data distributions. Our findings demonstrates that further investigation is required for the application of LLMs on Chinese GEC task.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œä¸€äº›ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‹±è¯­grammatical error correctionï¼ˆGECï¼‰ä»»åŠ¡ä¸­æœªèƒ½è¾¾åˆ°å‰æ™¯æ¨¡å‹çš„æˆç»©ã€‚åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬æƒ³è¦æ¢ç©¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡GECä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶æä¾›æœªæ¥å·¥ä½œçš„æŒ‡å¯¼ã€‚æˆ‘ä»¬åœ¨4ä¸ªä¸­æ–‡GECæ•°æ®é›†ä¸Šè¿›è¡Œäº†3ç§ä¸åŒçš„LLMæ¨¡å‹çš„å®éªŒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMçš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„è¡¨ç°ä¸è¶³ï¼Œä¸»è¦æ˜¯å› ä¸ºè¿‡åº¦ä¿®å¤çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†ä¸åŒæ•°æ®åˆ†å¸ƒä¸‹LLMçš„è¡¨ç°å¼‚å¸¸å¤§çš„ç°è±¡ã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œæœªæ¥åº”è¯¥è¿›ä¸€æ­¥è°ƒæŸ¥å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡GECä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Is-ChatGPT-a-Good-Personality-Recognizer-A-Preliminary-Study"><a href="#Is-ChatGPT-a-Good-Personality-Recognizer-A-Preliminary-Study" class="headerlink" title="Is ChatGPT a Good Personality Recognizer? A Preliminary Study"></a>Is ChatGPT a Good Personality Recognizer? A Preliminary Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03952">http://arxiv.org/abs/2307.03952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Ji, Wen Wu, Hong Zheng, Yi Hu, Xi Chen, Liang He<br>for:è¿™ä¸ªç ”ç©¶çš„ç›®çš„æ˜¯è¯„ä¼° chatGPT åœ¨æ–‡æœ¬åŸºç¡€äººæ ¼è¯†åˆ«ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œä»¥ç”Ÿæˆæœ‰æ•ˆçš„äººæ ¼æ•°æ®ã€‚methods:æœ¬ç ”ç©¶ä½¿ç”¨äº†å¤šç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€ç”Ÿæˆå’Œé€»è¾‘æ¨ç†ï¼Œä»¥æµ‹è¯• chatGPT åœ¨æ–‡æœ¬åˆ†æå’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚results:å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ zero-shot chain-of-thought æç¤ºç­–ç•¥å¯ä»¥å¸®åŠ© chatGPT åœ¨æ–‡æœ¬åŸºç¡€äººæ ¼è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”èƒ½å¤Ÿæä¾›è‡ªç„¶è¯­è¨€çš„è§£é‡Šã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹ chatGPT è¿›è¡Œæ°´å¹³è°ƒæ•´çš„æç¤ºç­–ç•¥ï¼Œå¯ä»¥å°†å…¶ä¸ç›¸åº”çš„ç°æœ‰æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·æ›´åŠ ç¼©å°ã€‚ä½†æ˜¯ï¼Œç ”ç©¶å‘ç° chatGPT å¯¹æŸäº›æ•æ„Ÿç‰¹å¾ï¼ˆå¦‚æ€§åˆ«å’Œå¹´é¾„ï¼‰å­˜åœ¨ä¸å…¬æ­£ç°è±¡ã€‚åŒæ—¶ï¼Œé€šè¿‡è¯¢é—® chatGPT çš„äººæ ¼è¯†åˆ«èƒ½åŠ›ï¼Œå¯ä»¥æé«˜å®ƒåœ¨ç›¸å…³ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¦‚æƒ…æ„Ÿåˆ†ç±»å’Œå‹åŠ›é¢„æµ‹ã€‚<details>
<summary>Abstract</summary>
In recent years, personality has been regarded as a valuable personal factor being incorporated into numerous tasks such as sentiment analysis and product recommendation. This has led to widespread attention to text-based personality recognition task, which aims to identify an individual's personality based on given text. Considering that ChatGPT has recently exhibited remarkable abilities on various natural language processing tasks, we provide a preliminary evaluation of ChatGPT on text-based personality recognition task for generating effective personality data. Concretely, we employ a variety of prompting strategies to explore ChatGPT's ability in recognizing personality from given text, especially the level-oriented prompting strategy we designed for guiding ChatGPT in analyzing given text at a specified level. The experimental results on two representative real-world datasets reveal that ChatGPT with zero-shot chain-of-thought prompting exhibits impressive personality recognition ability and is capable to provide natural language explanations through text-based logical reasoning. Furthermore, by employing the level-oriented prompting strategy to optimize zero-shot chain-of-thought prompting, the performance gap between ChatGPT and corresponding state-of-the-art model has been narrowed even more. However, we observe that ChatGPT shows unfairness towards certain sensitive demographic attributes such as gender and age. Additionally, we discover that eliciting the personality recognition ability of ChatGPT helps improve its performance on personality-related downstream tasks such as sentiment classification and stress prediction.
</details>
<details>
<summary>æ‘˜è¦</summary>
Recently, personality has been recognized as a valuable personal factor in various tasks such as sentiment analysis and product recommendation. This has led to widespread attention to text-based personality recognition tasks, which aim to identify an individual's personality based on given text. Considering ChatGPT's recent remarkable abilities in natural language processing tasks, we provide a preliminary evaluation of ChatGPT on text-based personality recognition tasks to generate effective personality data.To explore ChatGPT's ability in recognizing personality from given text, we employ various prompting strategies. Specifically, we use a level-oriented prompting strategy designed to guide ChatGPT in analyzing given text at a specified level. The experimental results on two representative real-world datasets show that ChatGPT with zero-shot chain-of-thought prompting exhibits impressive personality recognition ability and can provide natural language explanations through text-based logical reasoning. Furthermore, by optimizing zero-shot chain-of-thought prompting with the level-oriented prompting strategy, the performance gap between ChatGPT and corresponding state-of-the-art models has been narrowed even more.However, we observe that ChatGPT shows unfairness towards certain sensitive demographic attributes such as gender and age. Additionally, we find that eliciting the personality recognition ability of ChatGPT can improve its performance on personality-related downstream tasks such as sentiment classification and stress prediction.
</details></li>
</ul>
<hr>
<h2 id="Opening-up-ChatGPT-Tracking-openness-transparency-and-accountability-in-instruction-tuned-text-generators"><a href="#Opening-up-ChatGPT-Tracking-openness-transparency-and-accountability-in-instruction-tuned-text-generators" class="headerlink" title="Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators"></a>Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05532">http://arxiv.org/abs/2307.05532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io">https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io</a></li>
<li>paper_authors: Andreas Liesenfeld, Alianda Lopez, Mark Dingemanse</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å¤§è¯­è¨€æ¨¡å‹åœ¨å¯¹è¯ç•Œé¢ä¸­çš„ä½¿ç”¨ï¼Œä»¥åŠè¿™ç§ä½¿ç”¨çš„é£é™©å’Œæœºé‡ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å¼€æºé¡¹ç›®çš„æ¯”è¾ƒæ€§èƒ½ï¼Œå¹¶å¯¹è¿™äº›é¡¹ç›®è¿›è¡Œäº†è¯„ä¼°ï¼Œä»¥æ¢è®¨è¿™äº›é¡¹ç›®çš„å¼€æ”¾æ€§å’Œå¯è¯»æ€§ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œè®¸å¤šè‡ªç§°ä¸ºâ€œå¼€æºâ€çš„é¡¹ç›®ä»ç„¶å­˜åœ¨ä¸æ˜æ–‡æ³•å¾‹æ€§çš„æ•°æ®é—®é¢˜ï¼Œå¹¶ä¸”å°‘æ•°é¡¹ç›®åˆ†äº«äººå·¥è°ƒæ•™æ•°æ®ï¼Œå¯¼è‡´ scientific documentation éå¸¸ç½•è§ã€‚<details>
<summary>Abstract</summary>
Large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of OpenAI's ChatGPT, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (LLM+RLHF). We review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. The main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. We evaluate projects in terms of openness of code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods. We find that while there is a fast-growing list of projects billing themselves as 'open source', many inherit undocumented data of dubious legality, few share the all-important instruction-tuning (a key site where human annotation labour is involved), and careful scientific documentation is exceedingly rare. Degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture, and from training and fine-tuning to release and deployment.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è¯­è¨€æ¨¡å‹ exhibiting instruction-following behavior æ˜¯æœ€è¿‘ä¸€å¤§é©å‘½çš„ conversational interfaces çš„è¶‹åŠ¿ï¼Œè¿™è¶‹åŠ¿å—åˆ° OpenAI çš„ ChatGPT çš„å‘å¸ƒä»¥åŠäººç±»åé¦ˆé©±åŠ¨çš„ reinforcement learning çš„æ¿€å‘ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¾èµ–äºä¸“æœ‰è½¯ä»¶çš„é£é™©ï¼Œå¹¶æ£€æŸ¥äº†ç›¸åŒæ¶æ„å’ŒåŠŸèƒ½çš„å¼€æºé¡¹ç›®çš„ç¬¬ä¸€æ‰¹ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯æ˜¾ç¤ºå¼€æºæ˜¯ differentiatedï¼Œå¹¶æä¾›äº†è¿™ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸä¸­ç§‘å­¦æ–‡çŒ®çš„è®°å½•ã€‚æˆ‘ä»¬å°†é¡¹ç›®è¯„ä¼°ä¸ºå¼€æºä»£ç ã€è®­ç»ƒæ•°æ®ã€æ¨¡å‹æƒé‡ã€RLHF æ•°æ®ã€è®¸å¯è¯ã€ç§‘å­¦æ–‡çŒ®å’Œè®¿é—®æ–¹æ³•ç­‰æ–¹é¢çš„å¼€æ”¾ç¨‹åº¦è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°è®¸å¤šè‡ªç§°ä¸º 'open source' çš„é¡¹ç›®ç»§æ‰¿äº†æœªç»Documentedçš„æ•°æ®ï¼Œå°‘æ•°åˆ†äº«äº†å…³é”®çš„æŒ‡ä»¤è°ƒæ•´ï¼ˆä¸€ä¸ªå…³é”®çš„äººå·¥æ³¨é‡ŠåŠ³åŠ¨ Siteï¼‰ï¼Œå¹¶ä¸”ç²¾å¿ƒçš„ç§‘å­¦æ–‡çŒ®è®°å½•æ˜¯éå¸¸ç½•è§ã€‚åº¦é‡å¼€æ”¾ç¨‹åº¦å¯¹å…¬å¹³å’Œè´Ÿè´£ä»»æ˜¯éå¸¸é‡è¦ï¼Œä»æ•°æ®æ”¶é›†å’Œæ•´ç†åˆ°æ¨¡å‹æ¶æ„ã€è®­ç»ƒå’Œç»†åŒ–åˆ°å‘å¸ƒå’Œéƒ¨ç½²éƒ½æ˜¯å¦‚æ­¤ã€‚
</details></li>
</ul>
<hr>
<h2 id="On-decoder-only-architecture-for-speech-to-text-and-large-language-model-integration"><a href="#On-decoder-only-architecture-for-speech-to-text-and-large-language-model-integration" class="headerlink" title="On decoder-only architecture for speech-to-text and large language model integration"></a>On decoder-only architecture for speech-to-text and large language model integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03917">http://arxiv.org/abs/2307.03917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, Yu Wu</li>
<li>For: è¿™ç§ç ”ç©¶æ—¨åœ¨æ¢ç´¢å°†è¯­éŸ³ä¿¡å·ç»†ç›®Integrated intoæ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥æé«˜äººæœºäº¤äº’çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚* Methods: è¯¥æ–¹æ³•ä½¿ç”¨Connectionist Temporal Classificationå’Œä¸€ä¸ªç®€å•çš„éŸ³é¢‘ç¼–ç å™¨å°†å‹ç¼©è¯­éŸ³ç‰¹å¾æ˜ å°„åˆ°æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¿ç»­ semantic spaceã€‚* Results: å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•å¯ä»¥åœ¨å¤šè¯­è¨€speech-to-textç¿»è¯‘ä»»åŠ¡ä¸­å®ç°æ˜¾è‘—çš„æå‡ï¼Œè¿™ highlights the potential advantages of decoder-only models for speech-to-text conversionã€‚<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The "decoder-only" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†å¾ˆå¤§çš„æˆåŠŸï¼Œä½¿å¾—äººæœºäº¤äº’ä½¿ç”¨è‡ªç„¶è¯­è¨€æ›´åŠ ç®€å•ã€‚ç„¶è€Œï¼Œå°†è¯­éŸ³ä¿¡å·é›†æˆåˆ°LLMä¸­è¿˜æ²¡æœ‰å¾—åˆ°äº†å……åˆ†çš„æ¢ç´¢ã€‚â€œè§£ç å™¨åªâ€æ¶æ„ä¹Ÿå°šæœªå¾—åˆ°äº†å……åˆ†çš„ç ”ç©¶ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³å°†è¯­éŸ³ä¿¡å·é›†æˆåˆ°æ–‡æœ¬åŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†Connectionist Temporal Classificationå’Œä¸€ä¸ªç®€å•çš„éŸ³é¢‘ç¼–ç å™¨ï¼Œå°†å‹ç¼©çš„è¯­éŸ³ç‰¹å¾æ˜ å°„åˆ°æ–‡æœ¬åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è¿ç»­ semantics ç©ºé—´ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ¢ç´¢äº†åŸºäº speech-to-text ä»»åŠ¡çš„ decoder-only æ¶æ„ï¼Œé€šè¿‡ä» speech-text å¯¹åº”æ•°æ® alone  initialize ä¸€ä¸ªè¾ƒå°è§„æ¨¡çš„éšæœº initialize çš„ speech-LLaMA æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬åœ¨å¤šè¯­è¨€ speech-to-text ç¿»è¯‘ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶è¡¨æ˜äº† decoder-only æ¨¡å‹åœ¨ speech-to-text è½¬æ¢ä¸­çš„æ½œåœ¨ä¼˜åŠ¿ã€‚
</details></li>
</ul>
<hr>
<h2 id="Answering-Ambiguous-Questions-via-Iterative-Prompting"><a href="#Answering-Ambiguous-Questions-via-Iterative-Prompting" class="headerlink" title="Answering Ambiguous Questions via Iterative Prompting"></a>Answering Ambiguous Questions via Iterative Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03897">http://arxiv.org/abs/2307.03897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sunnweiwei/ambigprompt">https://github.com/sunnweiwei/ambigprompt</a></li>
<li>paper_authors: Weiwei Sun, Hengyi Cai, Hongshen Chen, Pengjie Ren, Zhumin Chen, Maarten de Rijke, Zhaochun Ren</li>
<li>for:  answering ambiguous questions</li>
<li>methods:  integrates an answering model with a prompting model in an iterative manner, with task-specific post-pretraining approach</li>
<li>results: achieves state-of-the-art or competitive results while using less memory and having a lower inference latency than competing approaches, performs well in low-resource settings<details>
<summary>Abstract</summary>
In open-domain question answering, due to the ambiguity of questions, multiple plausible answers may exist. To provide feasible answers to an ambiguous question, one approach is to directly predict all valid answers, but this can struggle with balancing relevance and diversity. An alternative is to gather candidate answers and aggregate them, but this method can be computationally costly and may neglect dependencies among answers. In this paper, we present AmbigPrompt to address the imperfections of existing approaches to answering ambiguous questions. Specifically, we integrate an answering model with a prompting model in an iterative manner. The prompting model adaptively tracks the reading process and progressively triggers the answering model to compose distinct and relevant answers. Additionally, we develop a task-specific post-pretraining approach for both the answering model and the prompting model, which greatly improves the performance of our framework. Empirical studies on two commonly-used open benchmarks show that AmbigPrompt achieves state-of-the-art or competitive results while using less memory and having a lower inference latency than competing approaches. Additionally, AmbigPrompt also performs well in low-resource settings. The code are available at: https://github.com/sunnweiwei/AmbigPrompt.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨å¼€æ”¾é¢†åŸŸé—®ç­”ä¸­ï¼Œç”±äºé—®é¢˜çš„æŠ½è±¡æ€§ï¼Œå¯èƒ½å­˜åœ¨å¤šä¸ªæœ‰å¯èƒ½çš„ç­”æ¡ˆã€‚ä¸ºæä¾›æœ‰å¯èƒ½æ€§çš„ç­”æ¡ˆï¼Œä¸€ç§æ–¹æ³•æ˜¯ç›´æ¥é¢„æµ‹æ‰€æœ‰æœ‰æ•ˆç­”æ¡ˆï¼Œä½†è¿™å¯èƒ½ä¼šå›°éš¾å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯æ”¶é›†å€™é€‰ç­”æ¡ˆå¹¶èšåˆå®ƒä»¬ï¼Œä½†è¿™å¯èƒ½ä¼šå¾ˆè®¡ç®—æ˜‚è´µå¹¶å¯èƒ½å¿½è§†ç­”æ¡ˆä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† AmbigPromptï¼Œä»¥è§£å†³ç°æœ‰é—®ç­”ç³»ç»Ÿä¸­çš„ç¼ºé™·ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ç­”é¢˜æ¨¡å‹ä¸æç¤ºæ¨¡å‹é›†æˆåœ¨è¿­ä»£æ–¹å¼ä¸‹ï¼Œä»¥ä¾¿åœ¨ä¸åŒçš„è¯»è€…è¿‡ç¨‹ä¸­é€‚åº”åœ°è§¦å‘ç­”é¢˜æ¨¡å‹ï¼Œå¹¶ç”Ÿæˆå…·æœ‰ä¸åŒç‰¹å¾å’Œç›¸å…³æ€§çš„ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ç‰¹å®šä»»åŠ¡çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæé«˜æˆ‘ä»¬çš„æ¡†æ¶çš„æ€§èƒ½ã€‚ç»éªŒç ”ç©¶è¡¨æ˜ï¼ŒAmbigPromptåœ¨ä¸¤ä¸ªå¸¸ç”¨çš„å¼€æ”¾benchmarkä¸Šå®ç°äº†çŠ¶æ€å½“å‰æˆ–ç«äº‰æ€§çš„ç»“æœï¼ŒåŒæ—¶ä½¿ç”¨çš„å†…å­˜å’Œæ‰§è¡Œæ—¶é—´æ¯”ç«äº‰æ–¹æ³•æ›´ä½ã€‚æ­¤å¤–ï¼ŒAmbigPromptè¿˜åœ¨ä½èµ„æºç¯å¢ƒä¸‹è¡¨ç°è‰¯å¥½ã€‚ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ï¼šhttps://github.com/sunnweiwei/AmbigPromptã€‚
</details></li>
</ul>
<hr>
<h2 id="Incomplete-Utterance-Rewriting-as-Sequential-Greedy-Tagging"><a href="#Incomplete-Utterance-Rewriting-as-Sequential-Greedy-Tagging" class="headerlink" title="Incomplete Utterance Rewriting as Sequential Greedy Tagging"></a>Incomplete Utterance Rewriting as Sequential Greedy Tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06337">http://arxiv.org/abs/2307.06337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunshan Chen</li>
<li>for: å®ç°å¯¹å¥å­ä¸­æ¬ ç¼ºçš„è¯æ±‡è¿›è¡Œè¡¥å……ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§åŸºäºåºåˆ—æ ‡è®°çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿä»å¯¹è¯ ĞºĞ¾Ğ½Ñ‚ĞµĞºæ–¯ä¸­æå–ä¿¡æ¯ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†speaker-awareåµŒå…¥ï¼Œä»¥å¤„ç† speaker çš„å˜åŒ–ã€‚</li>
<li>results: åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šå®éªŒï¼Œæ¨¡å‹åœ¨ä¹ä¸ªé‡å»ºå¾—åˆ†ä¸­å‡ achievable æœ€ä½³ç»“æœï¼Œè€Œå…¶ä»– Ğ¼ĞµÑ‚Ñ€Ğ¸ãƒƒã‚¯åˆ†æ•°ä¸ä¹‹å‰çš„å·æµé‡æ¨¡å‹ç›¸ä¼¼ã€‚æ­¤å¤–ï¼Œç”±äºæ¨¡å‹çš„ç®€å•æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨æ–­é€Ÿåº¦ä¸Šè¡¨ç°å‡ºä¼˜äºå¤§å¤šæ•°å…ˆå‰çš„æ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
The task of incomplete utterance rewriting has recently gotten much attention. Previous models struggled to extract information from the dialogue context, as evidenced by the low restoration scores. To address this issue, we propose a novel sequence tagging-based model, which is more adept at extracting information from context. Meanwhile, we introduce speaker-aware embedding to model speaker variation. Experiments on multiple public datasets show that our model achieves optimal results on all nine restoration scores while having other metric scores comparable to previous state-of-the-art models. Furthermore, benefitting from the model's simplicity, our approach outperforms most previous models on inference speed.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘æœŸï¼Œå¥å­é‡æ„ä»»åŠ¡å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚å…ˆå‰çš„æ¨¡å‹åœ¨å¯¹è¯ä¸Šæå–ä¿¡æ¯æ–¹é¢å­˜åœ¨é—®é¢˜ï¼Œè¿™å¯ä»¥ä»ä½çš„é‡å»ºå¾—åˆ†æ¥çœ‹å‡ºæ¥ã€‚ä¸ºè§£å†³è¿™é—®é¢˜ï¼Œæˆ‘ä»¬æè®®ä¸€ç§åŸºäºåºåˆ—æ ‡è®°çš„æ–°æ¨¡å‹ï¼Œå®ƒæ›´å¥½åœ°ä»å¯¹è¯ä¸Šæå–ä¿¡æ¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯´è¯äººå˜åŒ–çš„åµŒå…¥ï¼Œä»¥æ¨¡å‹è¯´è¯äººçš„å˜åŒ–ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ‰€æœ‰ä¹ä¸ªé‡å»ºå¾—åˆ†ä¸Šå…·æœ‰ä¼˜åŒ–çš„ç»“æœï¼Œè€Œå…¶ä»–ç»´åº¦å¾—åˆ†ä¸å…ˆå‰çŠ¶æ€è‰ºæ¨¡å‹ç›¸ä¼¼ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„æ¨¡å‹ç®€å•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†é€Ÿåº¦æ–¹é¢è¶…è¿‡äº†å¤§å¤šæ•°å…ˆå‰æ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Embedding-Mental-Health-Discourse-for-Community-Recommendation"><a href="#Embedding-Mental-Health-Discourse-for-Community-Recommendation" class="headerlink" title="Embedding Mental Health Discourse for Community Recommendation"></a>Embedding Mental Health Discourse for Community Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03892">http://arxiv.org/abs/2307.03892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hy Dang, Bang Nguyen, Noah Ziems, Meng Jiang</li>
<li>for:  investigate the use of discourse embedding techniques to develop a community recommendation system for mental health support groups on social media</li>
<li>methods:  use content-based and collaborative filtering techniques to enhance the performance of the recommendation system</li>
<li>results:  the proposed approach outperforms the use of each technique separately and provides interpretability in the recommendation process.Hereâ€™s the full text in Simplified Chinese:</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºè¯è¯­åµŒå…¥æŠ€æœ¯çš„ç¤¾äº¤åª’ä½“ç¾¤ä½“æ¨èç³»ç»Ÿï¼Œå¸®åŠ©ç”¨æˆ·åœ¨ç¤¾äº¤åª’ä½“å¹³å°ä¸Šæ‰¾åˆ°é€‚åˆè‡ªå·±ç²¾ç¥å¥åº·é—®é¢˜çš„ç¾¤ä½“ã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨äº†å†…å®¹åŸºäºå’Œåˆä½œç­›é€‰æŠ€æœ¯æ¥æé«˜æ¨èç³»ç»Ÿçš„æ€§èƒ½ï¼Œä»¥æä¾›æ›´åŠ æœ‰æ•ˆå’Œå¯è§£é‡Šçš„æ¨èç»“æœã€‚</li>
<li>results: æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæposeçš„æ–¹æ³•åœ¨å•ç‹¬ä½¿ç”¨æ¯ç§æŠ€æœ¯æ—¶éƒ½è¡¨ç°å‡ºä¼˜å¼‚ï¼Œå¹¶ä¸”å…·æœ‰æ›´é«˜çš„å¯è§£é‡Šæ€§ã€‚<details>
<summary>Abstract</summary>
Our paper investigates the use of discourse embedding techniques to develop a community recommendation system that focuses on mental health support groups on social media. Social media platforms provide a means for users to anonymously connect with communities that cater to their specific interests. However, with the vast number of online communities available, users may face difficulties in identifying relevant groups to address their mental health concerns. To address this challenge, we explore the integration of discourse information from various subreddit communities using embedding techniques to develop an effective recommendation system. Our approach involves the use of content-based and collaborative filtering techniques to enhance the performance of the recommendation system. Our findings indicate that the proposed approach outperforms the use of each technique separately and provides interpretability in the recommendation process.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬çš„è®ºæ–‡ç ”ç©¶äº†åˆ©ç”¨è¯è¯­åµŒå…¥æŠ€æœ¯å¼€å‘ä¸€ä¸ªå…³æ³¨ç¤¾äº¤åª’ä½“ä¸Šå¿ƒç†å¥åº·æ”¯æŒç¾¤ä½“çš„ç¤¾åŒºæ¨èç³»ç»Ÿã€‚ç¤¾äº¤åª’ä½“å¹³å°æä¾›äº†ç”¨æˆ·åŒ¿åè¿æ¥åˆ°ä¸“é—¨é’ˆå¯¹ä»–ä»¬çš„å…´è¶£çš„ç¤¾åŒºçš„æœºåˆ¶ã€‚ç„¶è€Œï¼Œç”±äºåœ¨çº¿ç¤¾åŒºçš„æ•°é‡ç¹å¤šï¼Œç”¨æˆ·å¯èƒ½é¢ä¸´ç€æ‰¾åˆ°ä¸ä»–ä»¬å¿ƒç†å¥åº·é—®é¢˜ç›¸å…³çš„ç¾¤ä½“çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†è¯è¯­ä¿¡æ¯ä»ä¸åŒçš„ subreddit ç¤¾åŒºåµŒå…¥æŠ€æœ¯çš„é›†æˆï¼Œä»¥å¼€å‘ä¸€ä¸ªé«˜æ•ˆçš„æ¨èç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨å†…å®¹åŸºæœ¬å’Œåˆä½œç­›é€‰æŠ€æœ¯æ¥æé«˜æ¨èç³»ç»Ÿçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æè®®æ–¹æ³•æ¯”ä½¿ç”¨æ¯ä¸€ç§æŠ€æœ¯åˆ†åˆ«æä¾›æ›´é«˜çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="MDACE-MIMIC-Documents-Annotated-with-Code-Evidence"><a href="#MDACE-MIMIC-Documents-Annotated-with-Code-Evidence" class="headerlink" title="MDACE: MIMIC Documents Annotated with Code Evidence"></a>MDACE: MIMIC Documents Annotated with Code Evidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03859">http://arxiv.org/abs/2307.03859</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/3mcloud/MDACE">https://github.com/3mcloud/MDACE</a></li>
<li>paper_authors: Hua Cheng, Rana Jafari, April Russell, Russell Klopfer, Edmond Lu, Benjamin Striner, Matthew R. Gormley</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ä¸ªåŸºäºåŒ»ç–—è®°å½•çš„ä»£ç è¯æ®é›†ï¼Œä»¥ä¾¿ç”¨äºè¯„ä¼°åŒ»ç–—ä»£ç æ£€ç´¢ç³»ç»Ÿä¸­çš„ä»£ç è¯æ®æå–æ–¹æ³•ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†EffectiveCANæ¨¡å‹ï¼ˆLiu et al., 2021ï¼‰æ¥å®ç°ä»£ç è¯æ®æå–æ–¹æ³•çš„åŸºå‡†æ€§èƒ½ã€‚</li>
<li>results: è¿™ä¸ªè®ºæ–‡ introduceäº†ç¬¬ä¸€ä¸ªå…¬å…±å¯ç”¨çš„ä»£ç è¯æ®é›†ï¼ˆMDACEï¼‰ï¼Œè¯¥é›†åŸºäºMIMIC-IIIåŒ»ç–—è®°å½•å­é›†ï¼Œå¹¶ç”±ä¸“ä¸šåŒ»ç–—ç¼–ç äººå‘˜è¿›è¡Œæ ‡æ³¨ã€‚è¯¥é›†åŒ…æ‹¬302åå…¥é™¢ç—…äººçš„3,934ä¸ªè¯æ®æ®µå’Œ52ååŒ»ç”Ÿçš„5,563ä¸ªè¯æ®æ®µã€‚<details>
<summary>Abstract</summary>
We introduce a dataset for evidence/rationale extraction on an extreme multi-label classification task over long medical documents. One such task is Computer-Assisted Coding (CAC) which has improved significantly in recent years, thanks to advances in machine learning technologies. Yet simply predicting a set of final codes for a patient encounter is insufficient as CAC systems are required to provide supporting textual evidence to justify the billing codes. A model able to produce accurate and reliable supporting evidence for each code would be a tremendous benefit. However, a human annotated code evidence corpus is extremely difficult to create because it requires specialized knowledge. In this paper, we introduce MDACE, the first publicly available code evidence dataset, which is built on a subset of the MIMIC-III clinical records. The dataset -- annotated by professional medical coders -- consists of 302 Inpatient charts with 3,934 evidence spans and 52 Profee charts with 5,563 evidence spans. We implemented several evidence extraction methods based on the EffectiveCAN model (Liu et al., 2021) to establish baseline performance on this dataset. MDACE can be used to evaluate code evidence extraction methods for CAC systems, as well as the accuracy and interpretability of deep learning models for multi-label classification. We believe that the release of MDACE will greatly improve the understanding and application of deep learning technologies for medical coding and document classification.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»ä¸€ä¸ªæ•°æ®é›†ç”¨äºè¯æ®/ç†ç”±æå–çš„æå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼Œ specifically Computer-Assisted Coding (CAC)ã€‚åœ¨è¿‡å»å‡ å¹´ï¼Œéšç€æœºå™¨å­¦ä¹ æŠ€æœ¯çš„è¿›æ­¥ï¼ŒCAC æŠ€æœ¯å¾—åˆ°äº†æ˜¾è‘—æ”¹è¿›ã€‚ç„¶è€Œï¼Œåªæ˜¯é¢„æµ‹ç—…äººé‡åˆ°çš„æœ€ç»ˆä»£ç æ˜¯ä¸å¤Ÿçš„ï¼ŒCAC ç³»ç»Ÿéœ€è¦æä¾›å¯é çš„æ–‡æœ¬è¯æ®æ¥è¯æ˜è®¡è´¹ä»£ç ã€‚ä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®å’Œå¯é çš„è¯æ® Ğ´Ğ»Ñæ¯ä¸ªä»£ç çš„æ¨¡å‹ä¼šæ˜¯ä¸€é¡¹å¾ˆå¤§çš„åˆ©ç›Šã€‚ç„¶è€Œï¼Œäººå·¥æ ‡æ³¨çš„ä»£ç è¯æ®é›†æ˜¯éå¸¸å›°éš¾çš„åˆ›å»ºï¼Œå› ä¸ºå®ƒéœ€è¦ä¸“ä¸šçš„åŒ»ç–—çŸ¥è¯†ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† MDACEï¼Œç¬¬ä¸€ä¸ªå…¬å…±å¯ç”¨çš„ä»£ç è¯æ®é›†ï¼Œå»ºç«‹åœ¨ MIMIC-III åŒ»ç–—è®°å½•å­é›†ä¸Šã€‚è¯¥æ•°æ®é›†ç”±ä¸“ä¸šåŒ»ç–—ç¼–ç å‘˜æ ‡æ³¨ï¼ŒåŒ…æ‹¬302 ä¾‹å…¥é™¢è®°å½•å’Œ 52 ä¾‹ Profee è®°å½•ï¼Œå…±è®¡ 3,934 ä¸ªè¯æ®æ®µå’Œ 5,563 ä¸ªè¯æ®æ®µã€‚æˆ‘ä»¬åŸºäº EffectiveCAN æ¨¡å‹ï¼ˆLiu et al., 2021ï¼‰å®ç°äº†å¤šç§è¯æ®æå–æ–¹æ³•ï¼Œä»¥å»ºç«‹åŸºçº¿æ€§èƒ½äºè¿™ä¸ªæ•°æ®é›†ã€‚ MDACE å¯ä»¥ç”¨æ¥è¯„ä¼°ä»£ç è¯æ®æå–æ–¹æ³•ï¼Œä»¥åŠæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¤šæ ‡ç­¾åˆ†ç±»ç²¾åº¦å’Œå¯è¯»æ€§ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œé‡Šæ”¾ MDACE å°†å¤§å¤§æé«˜æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨åŒ»ç–—ç¼–ç å’Œæ–‡æ¡£åˆ†ç±»é¢†åŸŸçš„ç†è§£å’Œåº”ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Subjective-Crowd-Disagreements-for-Subjective-Data-Uncovering-Meaningful-CrowdOpinion-with-Population-level-Learning"><a href="#Subjective-Crowd-Disagreements-for-Subjective-Data-Uncovering-Meaningful-CrowdOpinion-with-Population-level-Learning" class="headerlink" title="Subjective Crowd Disagreements for Subjective Data: Uncovering Meaningful CrowdOpinion with Population-level Learning"></a>Subjective Crowd Disagreements for Subjective Data: Uncovering Meaningful CrowdOpinion with Population-level Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10189">http://arxiv.org/abs/2307.10189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tharindu Cyril Weerasooriya, Sarah Luger, Saloni Poddar, Ashiqur R. KhudaBukhsh, Christopher M. Homan</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨æé«˜äººå·¥æ ‡æ³¨æ•°æ®çš„å…¬å¹³æ€§ï¼ŒåŒ…æ‹¬ç”Ÿæ´»alteringçš„å†³ç­–å’Œç¤¾äº¤åª’ä½“ä¸Šçš„å†…å®¹å®¡æ ¸ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ— ç›‘ç£å­¦ä¹ çš„æ–¹æ³•ï¼Œä½¿ç”¨è¯­è¨€ç‰¹å¾å’Œæ ‡ç­¾åˆ†å¸ƒæ¥æ±‡é›†ç›¸ä¼¼çš„é¡¹ç›®ï¼Œä»¥æé«˜æ ‡æ³¨æ•°æ®çš„å…¬å¹³æ€§ã€‚</li>
<li>results: è¯¥è®ºæ–‡é€šè¿‡åœ¨äº”ä¸ªç¤¾äº¤åª’ä½“å¹³å°ä¸Šè¿›è¡Œå®éªŒï¼Œå‘ç°è¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°é™ä½æ ‡æ³¨å·®å¼‚ï¼Œå¹¶åœ¨ Facebook ä¸Šè¿›è¡Œäº†åœ¨é‡å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§ã€‚<details>
<summary>Abstract</summary>
Human-annotated data plays a critical role in the fairness of AI systems, including those that deal with life-altering decisions or moderating human-created web/social media content. Conventionally, annotator disagreements are resolved before any learning takes place. However, researchers are increasingly identifying annotator disagreement as pervasive and meaningful. They also question the performance of a system when annotators disagree. Particularly when minority views are disregarded, especially among groups that may already be underrepresented in the annotator population. In this paper, we introduce \emph{CrowdOpinion}\footnote{Accepted for publication at ACL 2023}, an unsupervised learning based approach that uses language features and label distributions to pool similar items into larger samples of label distributions. We experiment with four generative and one density-based clustering method, applied to five linear combinations of label distributions and features. We use five publicly available benchmark datasets (with varying levels of annotator disagreements) from social media (Twitter, Gab, and Reddit). We also experiment in the wild using a dataset from Facebook, where annotations come from the platform itself by users reacting to posts. We evaluate \emph{CrowdOpinion} as a label distribution prediction task using KL-divergence and a single-label problem using accuracy measures.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººç±»æ ‡æ³¨æ•°æ®åœ¨äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­å‘æŒ¥ kritical ä½œç”¨ï¼ŒåŒ…æ‹¬å†³ç­–ç”Ÿæ´»æ–¹å¼æˆ–ä¿®è®¢äººç±»åˆ›å»ºçš„ç½‘ç»œ/ç¤¾äº¤åª’ä½“å†…å®¹ã€‚ Conventionally, annotator disagreements are resolved before any learning takes place. However, researchers are increasingly identifying annotator disagreement as pervasive and meaningful. They also question the performance of a system when annotators disagree, especially when minority views are disregarded, especially among groups that may already be underrepresented in the annotator population. In this paper, we introduce \emph{CrowdOpinion}\footnote{Accepted for publication at ACL 2023}, an unsupervised learning based approach that uses language features and label distributions to pool similar items into larger samples of label distributions. We experiment with four generative and one density-based clustering method, applied to five linear combinations of label distributions and features. We use five publicly available benchmark datasets (with varying levels of annotator disagreements) from social media (Twitter, Gab, and Reddit). We also experiment in the wild using a dataset from Facebook, where annotations come from the platform itself by users reacting to posts. We evaluate \emph{CrowdOpinion} as a label distribution prediction task using KL-divergence and a single-label problem using accuracy measures.
</details></li>
</ul>
<hr>
<h2 id="Linguistic-representations-for-fewer-shot-relation-extraction-across-domains"><a href="#Linguistic-representations-for-fewer-shot-relation-extraction-across-domains" class="headerlink" title="Linguistic representations for fewer-shot relation extraction across domains"></a>Linguistic representations for fewer-shot relation extraction across domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03823">http://arxiv.org/abs/2307.03823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sireesh Gururaja, Ritam Dutt, Tinglong Liao, Carolyn Rose</li>
<li>for: è¿™äº›ç ”ç©¶æ—¨åœ¨æ¢è®¨è¯­è¨€è¡¨ç¤ºçš„ incorporation å¯¹å‡ ä¸ª NLP ä»»åŠ¡çš„è·¨é¢†åŸŸæ€§è¡¨ç°çš„å½±å“ã€‚</li>
<li>methods: è¿™äº›ç ”ç©¶ä½¿ç”¨äº† freely available off-the-shelf å·¥å…· construct è¯­æ³•å’Œ semantics å›¾ï¼Œå¹¶å°†å…¶ä¸ popular transformer-based æ¶æ„ç»“åˆä½¿ç”¨ï¼Œä»¥æé«˜ Generalization æ€§ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œé€šè¿‡ incorporating è¯­è¨€è¡¨ç¤ºï¼Œå¯ä»¥significantly æé«˜ few-shot è½¬ç§»ä¸­çš„æ€§èƒ½ï¼Œä½†æ˜¯ä¸¤ç§ç±»å‹çš„å›¾éƒ½ display  roughly equivalent utilityã€‚<details>
<summary>Abstract</summary>
Recent work has demonstrated the positive impact of incorporating linguistic representations as additional context and scaffolding on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic representations enhance generalizability by providing features that function as cross-domain pivots. We focus on the task of relation extraction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer-based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domains. We find that while the inclusion of these graphs results in significantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘çš„ç ”ç©¶å·²ç»è¯æ˜åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œé€šè¿‡æ·»åŠ è¯­è¨€è¡¨ç¤ºæ¥æä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡å’Œæ‰˜ç®¡çš„ç¯å¢ƒå¯ä»¥æé«˜åŸŸå†…æ€§èƒ½ã€‚æˆ‘ä»¬å»¶ç»­è¿™é¡¹å·¥ä½œï¼Œæ¢ç´¢è¯­è¨€è¡¨ç¤ºåœ¨è·¨é¢†åŸŸä¼ è¾“ä¸­çš„å½±å“ã€‚å…³é”®é—®é¢˜æ˜¯å¦reno linguistic representations enhance generalizability by providing features that function as cross-domain pivotsã€‚æˆ‘ä»¬é€‰æ‹©å…³æ³¨åœ¨ä¸‰ä¸ªdatasetä¸Šè¿›è¡ŒRelation Extractionä»»åŠ¡ï¼Œè¿™ä¸‰ä¸ªdatasetåˆ†åˆ«æ˜¯cookingå’Œmaterials scienceã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯åœ¨æµè¡Œçš„transformer-based architectureä¸­ï¼Œé€šè¿‡è‡ªç”±å¯ç”¨çš„off-the-shelfå·¥å…·æ¥æ„å»ºè¯­æ³•å’ŒSemantic graphsï¼Œå¹¶å°è¯•ä»¥è¿™äº›graphsæ¥æé«˜é€šç”¨æ€§ã€‚æˆ‘ä»¬æ£€æŸ¥å®ƒä»¬æ˜¯å¦æœ‰åŠ©äºæé«˜é€šç”¨æ€§ï¼Œå¹¶ investigates whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domainsã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶åŒ…å«è¿™äº›graphså¯ä»¥åœ¨å‡ ä¸ªshotä¼ è¾“ä¸­æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä½†æ˜¯ä¸¤ç§ç±»å‹çš„graphå…·æœ‰ç›¸å½“çš„æœ‰ç”¨æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="On-the-Efficacy-of-Sampling-Adapters"><a href="#On-the-Efficacy-of-Sampling-Adapters" class="headerlink" title="On the Efficacy of Sampling Adapters"></a>On the Efficacy of Sampling Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03749">http://arxiv.org/abs/2307.03749</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rycolab/sampling-adapters">https://github.com/rycolab/sampling-adapters</a></li>
<li>paper_authors: Clara Meister, Tiago Pimentel, Luca Malagutti, Ethan G. Wilcox, Ryan Cotterell</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯è§£é‡Šå„ç§ä¿®æ”¹è¯­è¨€ç”Ÿæˆæ¨¡å‹çš„æŠ½è±¡åˆ†å¸ƒï¼Œä»¥æé«˜ç”Ÿæˆçš„æ–‡æœ¬è´¨é‡ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§ç»Ÿä¸€çš„æ¡†æ¶æ¥ç†è§£è¿™äº›ä¿®æ”¹æŠ€æœ¯ï¼Œå¹¶é€šè¿‡åˆ†æè¿™äº›æŠ€æœ¯å¯¹æ¨¡å‹çš„å½±å“æ¥è§£é‡Šå®ƒä»¬å¦‚ä½•æ”¹å–„æ–‡æœ¬è´¨é‡ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ sampling adapters æŠ€æœ¯å¯ä»¥å¯¼è‡´æ›´é«˜è´¨é‡çš„æ–‡æœ¬ç”Ÿæˆï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„æµ‹è¯•é›†ä¸Šä¿æŒè¿™ç§æ”¹å–„ã€‚æ­¤å¤–ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°éµå¾ªè¯­è¨€çš„è§„åˆ™ï¼Œä»è€Œæé«˜æ–‡æœ¬çš„å¯è¯»æ€§å’Œå¯ç†è§£æ€§ã€‚<details>
<summary>Abstract</summary>
Sampling is a common strategy for generating text from probabilistic models, yet standard ancestral sampling often results in text that is incoherent or ungrammatical. To alleviate this issue, various modifications to a model's sampling distribution, such as nucleus or top-k sampling, have been introduced and are now ubiquitously used in language generation systems. We propose a unified framework for understanding these techniques, which we term sampling adapters. Sampling adapters often lead to qualitatively better text, which raises the question: From a formal perspective, how are they changing the (sub)word-level distributions of language generation models? And why do these local changes lead to higher-quality text? We argue that the shift they enforce can be viewed as a trade-off between precision and recall: while the model loses its ability to produce certain strings, its precision rate on desirable text increases. While this trade-off is not reflected in standard metrics of distribution quality (such as perplexity), we find that several precision-emphasizing measures indeed indicate that sampling adapters can lead to probability distributions more aligned with the true distribution. Further, these measures correlate with higher sequence-level quality scores, specifically, Mauve.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>å°†æ–‡æœ¬ç¿»è¯‘æˆç®€åŒ–ä¸­æ–‡ã€‚<</SYS>>é‡‡æ ·æ˜¯ä¸€ç§å¸¸è§çš„è¯­è¨€ç”Ÿæˆæ¨¡å‹ç­–ç•¥ï¼Œç„¶è€Œæ ‡å‡†ç¥–å…ˆé‡‡æ ·é€šå¸¸ä¼šå¯¼è‡´æ–‡æœ¬æ— æ³•ç†è§£æˆ–ä¸æ­£ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®¸å¤šæ¨¡å‹é‡‡æ ·åˆ†å¸ƒçš„ä¿®æ”¹ï¼Œå¦‚æ ¸å¿ƒé‡‡æ ·æˆ–top-ké‡‡æ ·ï¼Œå·²ç»è¢«å¼•å…¥å¹¶å¹¿æ³›åº”ç”¨äºè¯­è¨€ç”Ÿæˆç³»ç»Ÿã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªç»Ÿä¸€æ¡†æ¶æ¥ç†è§£è¿™äº›æŠ€æœ¯ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºé‡‡æ ·é€‚é…å™¨ã€‚é‡‡æ ·é€‚é…å™¨é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„æ–‡æœ¬ï¼Œè¿™å¼•èµ·äº†é—®é¢˜ï¼šä»å½¢å¼ä¸Šè®²ï¼Œè¿™äº›åœ°æ–¹æ€§æ”¹å˜å¦‚ä½•å½±å“è¯­è¨€ç”Ÿæˆæ¨¡å‹çš„ï¼ˆå­ï¼‰å­—å…ƒçº§åˆ†å¸ƒï¼Ÿè€Œè¿™äº›æœ¬åœ°æ”¹å˜æ˜¯å¦‚ä½•å¯¼è‡´æ›´é«˜è´¨é‡çš„æ–‡æœ¬å‘¢ï¼Ÿæˆ‘ä»¬è®¤ä¸ºè¿™ç§shiftå¯ä»¥è¢«è§†ä¸ºä¸€ç§ç²¾åº¦å’Œå›å½’ä¹‹é—´çš„äº¤æ˜“ï¼šè™½ç„¶æ¨¡å‹å¤±å»äº†ç”ŸæˆæŸäº›å­—ä¸²çš„èƒ½åŠ›ï¼Œä½†å®ƒåœ¨æ„¿æ™¯å­—ä¸²ä¸Šçš„ç²¾åº¦æé«˜ã€‚å°½ç®¡è¿™ç§äº¤æ˜“ä¸è¢«æ ‡å‡†çš„åˆ†å¸ƒè´¨é‡æŒ‡æ ‡ï¼ˆå¦‚å¤æ‚åº¦ï¼‰åæ˜ ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€äº›ç²¾åº¦å¼ºè°ƒçš„æŒ‡æ ‡ç¡®å®è¡¨æ˜é‡‡æ ·é€‚é…å™¨å¯ä»¥å¯¼è‡´æ›´åŠ é€‚åº”trueåˆ†å¸ƒçš„æ¦‚ç‡åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œè¿™äº›æŒ‡æ ‡ä¸é«˜çº§åºçº§è´¨é‡åˆ†æ•°ç›¸å…³ï¼Œå…·ä½“æ˜¯Mauveã€‚
</details></li>
</ul>
<hr>
<h2 id="QIGen-Generating-Efficient-Kernels-for-Quantized-Inference-on-Large-Language-Models"><a href="#QIGen-Generating-Efficient-Kernels-for-Quantized-Inference-on-Large-Language-Models" class="headerlink" title="QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models"></a>QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03738">http://arxiv.org/abs/2307.03738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ist-daslab/qigen">https://github.com/ist-daslab/qigen</a></li>
<li>paper_authors: Tommaso Pegolotti, Elias Frantar, Dan Alistarh, Markus PÃ¼schel</li>
<li>for: æ”¯æŒé‡åŒ–ç”Ÿæˆæ¨ç†åœ¨ LLMA æˆ– OPT ä¸Šçš„è‡ªåŠ¨ä»£ç ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>methods: ä½¿ç”¨ç›®æ ‡æ¶æ„å’Œæ€§èƒ½æ¨¡å‹ï¼ŒåŒ…æ‹¬ç¡¬ä»¶ç‰¹æ€§å’Œæ–¹æ³•ç‰¹å®šçš„å‡†ç¡®æ€§çº¦æŸã€‚</li>
<li>results: å¯¹ CPU ä¸Šçš„ LLMA æ¨¡å‹è¿›è¡Œäº†å¿«é€Ÿå’Œå‡†ç¡®çš„æ¨ç†ï¼Œä¸ç°æœ‰å¼€æºè§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼Œæ€§èƒ½å’Œå‡†ç¡®æ€§éƒ½æ¯”è¾ƒé«˜ã€‚<details>
<summary>Abstract</summary>
We present ongoing work on a new automatic code generation approach for supporting quantized generative inference on LLMs such as LLaMA or OPT on off-the-shelf CPUs. Our approach is informed by the target architecture and a performance model, including both hardware characteristics and method-specific accuracy constraints. Results on CPU-based inference for LLaMA models show that our approach can lead to high performance and high accuracy, comparing favorably to the best existing open-source solution. A preliminary implementation is available at https://github.com/IST-DASLab/QIGen.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ç°åœ¨æ­£åœ¨è¿›è¡Œä¸€ç§æ–°çš„è‡ªåŠ¨ä»£ç ç”Ÿæˆæ–¹æ³•çš„ç ”ç©¶ï¼Œç”¨äºæ”¯æŒé‡åŒ–ç”Ÿæˆæ¨ç†åœ¨LLaMAæˆ–OPTç±»å‹çš„å¤§è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡ŒOFF-the-SHELF CPUä¸Šçš„æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¢«ç›®æ ‡æ¶æ„å’Œæ€§èƒ½æ¨¡å‹æ‰€æŒ‡å¯¼ï¼ŒåŒ…æ‹¬ç¡¬ä»¶ç‰¹ç‚¹å’Œæ–¹æ³•ç‰¹å®šçš„å‡†ç¡®æ€§çº¦æŸã€‚å¯¹CPUä¸Šçš„LLaMAæ¨¡å‹è¿›è¡Œæ¨ç†çš„ç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¾¾åˆ°é«˜æ€§èƒ½å’Œé«˜å‡†ç¡®æ€§ï¼Œä¸ç°æœ‰å¼€æºè§£å†³æ–¹æ¡ˆç›¸æ¯”ä¹‹ä¸‹ï¼Œè¡¨ç°å‡ºè‰²ã€‚ä¸€ä¸ªåˆæ­¥çš„å®ç°å¯ä»¥åœ¨https://github.com/IST-DASLab/QIGenä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Improving-Automatic-Quotation-Attribution-in-Literary-Novels"><a href="#Improving-Automatic-Quotation-Attribution-in-Literary-Novels" class="headerlink" title="Improving Automatic Quotation Attribution in Literary Novels"></a>Improving Automatic Quotation Attribution in Literary Novels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03734">http://arxiv.org/abs/2307.03734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krishnapriya Vishnubhotla, Frank Rudzicz, Graeme Hirst, Adam Hammond</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨Addressing the challenge of quotation attribution in literary novels, where available information varies.</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨four interconnected sub-tasksï¼šcharacter identification, coreference resolution, quotation identification, and speaker attribution.</li>
<li>results: ç ”ç©¶æ˜¾ç¤ºï¼Œä½¿ç”¨state-of-the-art models on each sub-task independentlyï¼Œå¯ä»¥ achieve high accuracy scores. ç‰¹åˆ«æ˜¯ï¼Œä¸€ç§ç®€å•çš„sequential prediction modelå¯ä»¥è¾¾åˆ°ä¸ç°æœ‰æ¨¡å‹ç›¸åŒçš„å‡†ç¡®ç‡ã€‚<details>
<summary>Abstract</summary>
Current models for quotation attribution in literary novels assume varying levels of available information in their training and test data, which poses a challenge for in-the-wild inference. Here, we approach quotation attribution as a set of four interconnected sub-tasks: character identification, coreference resolution, quotation identification, and speaker attribution. We benchmark state-of-the-art models on each of these sub-tasks independently, using a large dataset of annotated coreferences and quotations in literary novels (the Project Dialogism Novel Corpus). We also train and evaluate models for the speaker attribution task in particular, showing that a simple sequential prediction model achieves accuracy scores on par with state-of-the-art models.
</details>
<details>
<summary>æ‘˜è¦</summary>
å½“å‰çš„å¼•ç”¨å½’å±æ¨¡å‹åœ¨æ–‡å­¦å°è¯´ä¸­å‡è®¾æœ‰ä¸åŒæ°´å¹³çš„å¯ç”¨ä¿¡æ¯ï¼Œè¿™ä¼šå¯¹åœ¨é‡å¤–æ¨ç†ä¸­å¸¦æ¥æŒ‘æˆ˜ã€‚æˆ‘ä»¬å°†å¼•ç”¨å½’å±çœ‹ä½œä¸ºå››ä¸ªç›¸äº’è¿æ¥çš„å­ä»»åŠ¡ï¼šäººç‰©è¯†åˆ«ã€æ ¸å¿ƒå¼•ç”¨è§£å†³ã€å¼•ç”¨å½’å±å’Œè¯´è¯äººå½’å±ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§é‡Literary novelsä¸­æ³¨é‡Šçš„æ ¸å¿ƒå¼•ç”¨å’Œå¼•ç”¨çš„æ•°æ®é›†æ¥å¯¹æ¯ä¸ªå­ä»»åŠ¡è¿›è¡Œç‹¬ç«‹çš„ bencharkingï¼Œå¹¶å¯¹è¯´è¯äººå½’å±ä»»åŠ¡è¿›è¡Œç‰¹ç‚¹éªŒè¯ï¼Œå‘ç°ç®€å•çš„é¡ºåºé¢„æµ‹æ¨¡å‹å¯ä»¥è¾¾åˆ°ä¸çŠ¶æ€å€¼æ¨¡å‹ç›¸åŒçš„å‡†ç¡®ç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="INT-FP-QSim-Mixed-Precision-and-Formats-For-Large-Language-Models-and-Vision-Transformers"><a href="#INT-FP-QSim-Mixed-Precision-and-Formats-For-Large-Language-Models-and-Vision-Transformers" class="headerlink" title="INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers"></a>INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03712">http://arxiv.org/abs/2307.03712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lightmatter-ai/int-fp-qsim">https://github.com/lightmatter-ai/int-fp-qsim</a></li>
<li>paper_authors: Lakshmi Nair, Mikhail Bernadskiy, Arulselvan Madhavan, Craig Chan, Ayon Basumallik, Darius Bunandar</li>
<li>for: æœ¬æ–‡æ—¨åœ¨æ”¯æŒèµ„æºçº¦æŸå’Œå¤§è¯­è¨€æ¨¡å‹çš„æ°‘ç”ŸåŒ–ï¼Œæå‡ºä¸€ä¸ªå¼€æºçš„INT-FP-QSim simulateå™¨ï¼Œå¯ä»¥åœ¨ä¸åŒçš„æ•°å­—ç²¾åº¦å’Œæ ¼å¼ä¸‹çµæ´»è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹å’Œè§†Transformersã€‚</li>
<li>methods:  INT-FP-QSim ä½¿ç”¨ç°æœ‰çš„å¼€æºåº“ such as TensorRT, QPytorch å’Œ AIMETï¼Œç»„åˆäº†è¿™äº›åº“æ¥æ”¯æŒä¸åŒçš„æµ®ç‚¹æ•°å’Œæ•´æ•°æ ¼å¼ã€‚</li>
<li>results: é€šè¿‡ä½¿ç”¨ INT-FP-QSimï¼Œæˆ‘ä»¬å¯¹å¤§è¯­è¨€æ¨¡å‹å’Œè§†Transformers çš„æ€§èƒ½åšäº†è¯„ä¼°ï¼Œå¹¶æ¯”è¾ƒäº†æœ€è¿‘æå‡ºçš„ Adaptive Block Floating Point, SmoothQuant, GPTQ å’Œ RPTQ æ–¹æ³•çš„å½±å“ã€‚<details>
<summary>Abstract</summary>
The recent rise of large language models (LLMs) has resulted in increased efforts towards running LLMs at reduced precision. Running LLMs at lower precision supports resource constraints and furthers their democratization, enabling users to run billion-parameter LLMs on their personal devices. To supplement this ongoing effort, we propose INT-FP-QSim: an open-source simulator that enables flexible evaluation of LLMs and vision transformers at various numerical precisions and formats. INT-FP-QSim leverages existing open-source repositories such as TensorRT, QPytorch and AIMET for a combined simulator that supports various floating point and integer formats. With the help of our simulator, we survey the impact of different numerical formats on the performance of LLMs and vision transformers at 4-bit weights and 4-bit or 8-bit activations. We also compare recently proposed methods like Adaptive Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We hope INT-FP-QSim will enable researchers to flexibly simulate models at various precisions to support further research in quantization of LLMs and vision transformers.
</details>
<details>
<summary>æ‘˜è¦</summary>
INT-FP-QSim leverages existing open-source repositories such as TensorRT, QPytorch, and AIMET to create a combined simulator that supports various floating point and integer formats. With the help of our simulator, we survey the impact of different numerical formats on the performance of LLMs and vision transformers when using 4-bit weights and 4-bit or 8-bit activations. We also compare recently proposed methods like Adaptive Block Floating Point, SmoothQuant, GPTQ, and RPTQ on the model performances.Our hope is that INT-FP-QSim will enable researchers to flexibly simulate models at various precisions, supporting further research in quantization of LLMs and vision transformers.
</details></li>
</ul>
<hr>
<h2 id="LaunchpadGPT-Language-Model-as-Music-Visualization-Designer-on-Launchpad"><a href="#LaunchpadGPT-Language-Model-as-Music-Visualization-Designer-on-Launchpad" class="headerlink" title="LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad"></a>LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04827">http://arxiv.org/abs/2307.04827</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yunlong10/launchpadgpt">https://github.com/yunlong10/launchpadgpt</a></li>
<li>paper_authors: Siting Xu, Yunlong Tang, Feng Zheng</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯å¸®åŠ©è®¾è®¡Launchpadçš„éŸ³ä¹è§†è§‰æ•ˆæœï¼Œä»¥åŠä¸º Beginner æä¾›æ›´åŠ å¯ accessibleçš„æ–¹æ³•æ¥åˆ›å»ºéŸ³ä¹è§†è§‰ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨ç”ŸæˆéŸ³ä¹è§†è§‰è®¾è®¡æ–¹æ³•ï¼Œç”¨äºLaunhpadä¸Šæ¼”å¥éŸ³ä¹ã€‚è¯¥æ–¹æ³•ä½¿ç”¨äº†éŸ³ä¹pieceä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºäº†Launhpadä¸Šæ¼”å¥çš„å…‰æ•ˆæœå½¢å¼çš„è§†é¢‘ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥åˆ›é€ å‡ºæ¯”éšæœºç”Ÿæˆæ–¹æ³•æ›´å¥½çš„éŸ³ä¹è§†è§‰ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¹¿æ³›çš„éŸ³ä¹è§†è§‰åº”ç”¨å‰æ™¯ã€‚<details>
<summary>Abstract</summary>
Launchpad is a musical instrument that allows users to create and perform music by pressing illuminated buttons. To assist and inspire the design of the Launchpad light effect, and provide a more accessible approach for beginners to create music visualization with this instrument, we proposed the LaunchpadGPT model to generate music visualization designs on Launchpad automatically. Based on the language model with excellent generation ability, our proposed LaunchpadGPT takes an audio piece of music as input and outputs the lighting effects of Launchpad-playing in the form of a video (Launchpad-playing video). We collect Launchpad-playing videos and process them to obtain music and corresponding video frame of Launchpad-playing as prompt-completion pairs, to train the language model. The experiment result shows the proposed method can create better music visualization than random generation methods and hold the potential for a broader range of music visualization applications. Our code is available at https://github.com/yunlong10/LaunchpadGPT/.
</details>
<details>
<summary>æ‘˜è¦</summary>
Launchpadæ˜¯ä¸€ç§éŸ³ä¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡ç‚¹å‡»ç¯å…‰æŒ‰é’®åˆ›ä½œå’Œæ¼”å¥éŸ³ä¹ã€‚ä¸ºå¸®åŠ©è®¾è®¡Launchpadçš„å…‰æ•ˆå’Œå¯å‘åˆ›æ–°ï¼Œä»¥åŠä¸ºBeginneræä¾›æ›´ accessibleçš„éŸ³ä¹è§†è§‰åˆ›ä½œæ–¹å¼ï¼Œæˆ‘ä»¬æè®®äº†LaunchpadGPTæ¨¡å‹ï¼Œè‡ªåŠ¨ç”ŸæˆLaunchpadæ¼”å¥çš„è§†è§‰è®¾è®¡ã€‚åŸºäºå‡ºè‰²çš„è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æè®®LaunchpadGPTæ¥å—éŸ³ä¹ä½œå“ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºLaunchpadæ¼”å¥çš„å…‰æ•ˆæ•ˆæœ Ğ²Ğ¸Ğ´ĞµĞ¾ï¼ˆLaunchpadæ¼”å¥è§†é¢‘ï¼‰ã€‚æˆ‘ä»¬æ”¶é›†äº†Launchpadæ¼”å¥è§†é¢‘ï¼Œå¹¶å¤„ç†å®ƒä»¬ï¼Œä»¥è·å¾—éŸ³ä¹å’Œå¯¹åº”çš„è§†é¢‘å¸§çš„Launchpadæ¼”å¥Prompt-completionå¯¹ã€‚é€šè¿‡è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å®ç°äº†è¯¥æ–¹æ³•å¯ä»¥åˆ›é€ æ›´å¥½çš„éŸ³ä¹è§†è§‰ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¹¿æ³›çš„éŸ³ä¹è§†è§‰åº”ç”¨å‰æ™¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨https://github.com/yunlong10/LaunchpadGPT/ä¸Šè·å–ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/08/cs.CL_2023_07_08/" data-id="clltaagmq001nr8886j7z0eaz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/08/cs.CV_2023_07_08/" class="article-date">
  <time datetime="2023-07-07T16:00:00.000Z" itemprop="datePublished">2023-07-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/08/cs.CV_2023_07_08/">cs.CV - 2023-07-08 21:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Lightweight-Improved-Residual-Network-for-Efficient-Inverse-Tone-Mapping"><a href="#Lightweight-Improved-Residual-Network-for-Efficient-Inverse-Tone-Mapping" class="headerlink" title="Lightweight Improved Residual Network for Efficient Inverse Tone Mapping"></a>Lightweight Improved Residual Network for Efficient Inverse Tone Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03998">http://arxiv.org/abs/2307.03998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liqi Xue, Tianyi Xu, Yongbao Song, Yan Liu, Lei Zhang, Xiantong Zhen, Jun Xu</li>
<li>for: ç”¨äºé«˜åŠ¨æ€èŒƒå›´å›¾åƒçš„å€’è®¡æ—¶é—´æ˜ å°„ï¼ˆITMï¼‰ä»»åŠ¡ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§åŸºäºå¢å¼ºçš„å¾…é‡å—çš„è½»é‡çº§Improved Residual Networkï¼ˆIRNetï¼‰ï¼Œç”¨äºé«˜æ•ˆåœ°å®ŒæˆITMä»»åŠ¡ã€‚</li>
<li>results: åœ¨ä¸‰ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œå¾—åˆ°äº†æœ€ä½³æ€§èƒ½åœ¨ITMå’Œjoint SR-ITMä»»åŠ¡ä¸­ã€‚<details>
<summary>Abstract</summary>
The display devices like HDR10 televisions are increasingly prevalent in our daily life for visualizing high dynamic range (HDR) images. But the majority of media images on the internet remain in 8-bit standard dynamic range (SDR) format. Therefore, converting SDR images to HDR ones by inverse tone mapping (ITM) is crucial to unlock the full potential of abundant media images. However, existing ITM methods are usually developed with complex network architectures requiring huge computational costs. In this paper, we propose a lightweight Improved Residual Network (IRNet) by enhancing the power of popular residual block for efficient ITM. Specifically, we propose a new Improved Residual Block (IRB) to extract and fuse multi-layer features for fine-grained HDR image reconstruction. Experiments on three benchmark datasets demonstrate that our IRNet achieves state-of-the-art performance on both the ITM and joint SR-ITM tasks. The code, models and data will be publicly available at https://github.com/ThisisVikki/ITM-baseline.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°åœ¨çš„æ˜¾ç¤ºè®¾å¤‡å¦‚HDR10ç”µè§†åœ¨æˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œç”¨äºæ˜¾ç¤ºé«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰å›¾åƒã€‚ç„¶è€Œï¼Œäº’è”ç½‘ä¸Šçš„åª’ä½“å›¾åƒå¤§å¤šæ•°ä»ç„¶æ˜¯8æ¯”ç‰¹æ ‡å‡†åŠ¨æ€èŒƒå›´ï¼ˆSDRï¼‰æ ¼å¼ã€‚å› æ­¤ï¼Œå°†SDRå›¾åƒè½¬æ¢ä¸ºHDRå›¾åƒçš„ inverse tone mappingï¼ˆITMï¼‰å˜å¾—éå¸¸é‡è¦ï¼Œä»¥ä¾¿ä½¿ç”¨ä¸°å¯Œçš„åª’ä½“å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„ITMæ–¹æ³•é€šå¸¸å…·æœ‰å¤æ‚çš„ç½‘ç»œæ¶æ„ï¼Œéœ€è¦å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ”¹è¿›å¾‹é‡ç½‘ç»œï¼ˆIRNetï¼‰ï¼Œé€šè¿‡æé«˜æµè¡Œçš„å¾‹é‡å—æ¥æé«˜HDRå›¾åƒé‡å»ºç²¾åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ”¹è¿›å¾‹é‡å—ï¼ˆIRBï¼‰ï¼Œç”¨äºæå–å’Œèåˆå¤šå±‚ç‰¹å¾æ¥å®ç°ç»†è…»çš„HDRå›¾åƒé‡å»ºã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„IRNetåœ¨ITMå’Œ joint SR-ITMä»»åŠ¡ä¸Šè¾¾åˆ°äº†çŠ¶æ€ç•¥çš„è¡¨ç°ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å°†åœ¨https://github.com/ThisisVikki/ITM-baselineä¸Šå…¬å¼€ã€‚
</details></li>
</ul>
<hr>
<h2 id="Stimulating-the-Diffusion-Model-for-Image-Denoising-via-Adaptive-Embedding-and-Ensembling"><a href="#Stimulating-the-Diffusion-Model-for-Image-Denoising-via-Adaptive-Embedding-and-Ensembling" class="headerlink" title="Stimulating the Diffusion Model for Image Denoising via Adaptive Embedding and Ensembling"></a>Stimulating the Diffusion Model for Image Denoising via Adaptive Embedding and Ensembling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03992">http://arxiv.org/abs/2307.03992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Li, Hansen Feng, Lizhi Wang, Zhiwei Xiong, Hua Huang</li>
<li>for: æé«˜å›¾åƒå™ªå£°å»é™¤çš„é«˜è´¨é‡æ„ŸçŸ¥æ€§å’Œä½æ‰­æ›²æ€§ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥called Diffusion Model for Image Denoising (DMID),åŒ…æ‹¬é€‚åº”åµŒå…¥æ–¹æ³•å’Œé€‚åº” ensembleæ–¹æ³•ï¼Œä»¥è§£å†³å™ªå£°æ¨¡å‹å’Œå›¾åƒå»å™ªçš„å…³é”®é—®é¢˜ã€‚</li>
<li>results: å®ç°äº†å¯¹æ‰€æœ‰å™ªå£°åŸºæ•°å’Œæ„ŸçŸ¥æŒ‡æ ‡çš„çŠ¶æ€è‰ºæœ¯æ€§å’Œä½æ‰­æ›²æ€§è¡¨ç°ï¼ŒåŒ…æ‹¬å¯¹çœŸå®ä¸–ç•Œå›¾åƒçš„å»å™ªã€‚<details>
<summary>Abstract</summary>
Image denoising is a fundamental problem in computational photography, where achieving high-quality perceptual performance with low distortion is highly demanding. Current methods either struggle with perceptual performance or suffer from significant distortion. Recently, the emerging diffusion model achieves state-of-the-art performance in various tasks, and its denoising mechanism demonstrates great potential for image denoising. However, stimulating diffusion models for image denoising is not straightforward and requires solving several critical problems. On the one hand, the input inconsistency hinders the connection of diffusion models and image denoising. On the other hand, the content inconsistency between the generated image and the desired denoised image introduces additional distortion. To tackle these problems, we present a novel strategy called Diffusion Model for Image Denoising (DMID) by understanding and rethinking the diffusion model from a denoising perspective. Our DMID strategy includes an adaptive embedding method that embeds the noisy image into a pre-trained diffusion model, and an adaptive ensembling method that reduces distortion in the denoised image. Our DMID strategy achieves state-of-the-art performance on all distortion-based and perceptual metrics, for both Gaussian and real-world image denoising.
</details>
<details>
<summary>æ‘˜è¦</summary>
Image denoisingæ˜¯è®¡ç®—æ‘„å½±å­¦ä¸­çš„åŸºæœ¬é—®é¢˜ï¼Œè¦æ±‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒé«˜è´¨é‡çš„æ„ŸçŸ¥æ€§æ€§èƒ½å¹¶ä¸”å‡å°‘æ‰­æ›²ã€‚ç›®å‰çš„æ–¹æ³•å¯èƒ½ä¼šå¯¼è‡´æ„ŸçŸ¥æ€§æ€§èƒ½ä¸‹é™æˆ–è€…å—åˆ°é‡å¤§çš„æ‰­æ›²å½±å“ã€‚è¿‘äº›å¹´ï¼Œå‡ºç°äº†æ‰©æ•£æ¨¡å‹ï¼Œåœ¨ä¸åŒä»»åŠ¡ä¸­è¾¾åˆ°äº†å›½é™…å‰ierçš„æ€§èƒ½ï¼Œå…¶æ‰©æ•£æœºåˆ¶å¯¹å›¾åƒæ‚ noise æœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œæ¿€æ´»æ‰©æ•£æ¨¡å‹ä»¥å®ç°å›¾åƒæ‚ noise çº¦æŸæ˜¯ä¸ç›´æ¥çš„ï¼Œéœ€è¦è§£å†³å¤šä¸ªå…³é”®é—®é¢˜ã€‚ä¸€æ–¹é¢ï¼Œè¾“å…¥ä¸ä¸€è‡´ä¼šé˜»ç¢æ‰©æ•£æ¨¡å‹å’Œå›¾åƒæ‚ noise çš„è¿æ¥ã€‚å¦ä¸€æ–¹é¢ï¼Œç”Ÿæˆçš„å›¾åƒä¸éœ€è¦çš„æ‚ noise å›¾åƒçš„å†…å®¹ä¸ä¸€è‡´ä¼šå¼•å…¥é¢å¤–çš„æ‰­æ›²ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œå³æ‰©æ•£æ¨¡å‹ Ğ´Ğ»Ñå›¾åƒæ‚ noiseï¼ˆDMIDï¼‰ã€‚æˆ‘ä»¬çš„ DMID ç­–ç•¥åŒ…æ‹¬é€‚åº”åµŒå…¥æ–¹æ³•ï¼Œå°†å™ªå›¾åƒ embedding åˆ°é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥åŠé€‚åº”ensembleæ–¹æ³•ï¼Œä»¥å‡å°‘ç”Ÿæˆçš„å›¾åƒä¸­çš„æ‰­æ›²ã€‚æˆ‘ä»¬çš„ DMID ç­–ç•¥åœ¨æ‰€æœ‰åŸºäºæ‰­æ›²å’Œæ„ŸçŸ¥æ€§çš„æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†å›½é™…å‰ierçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ Gaussian å’Œå®é™…ä¸–ç•Œçš„å›¾åƒæ‚ noise çº¦æŸã€‚
</details></li>
</ul>
<hr>
<h2 id="FTFDNet-Learning-to-Detect-Talking-Face-Video-Manipulation-with-Tri-Modality-Interaction"><a href="#FTFDNet-Learning-to-Detect-Talking-Face-Video-Manipulation-with-Tri-Modality-Interaction" class="headerlink" title="FTFDNet: Learning to Detect Talking Face Video Manipulation with Tri-Modality Interaction"></a>FTFDNet: Learning to Detect Talking Face Video Manipulation with Tri-Modality Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03990">http://arxiv.org/abs/2307.03990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ganglai Wang, Peng Zhang, Junwen Xiong, Feihan Yang, Wei Huang, Yufei Zha</li>
<li>for: é˜²æ­¢æ·±ä¼ªè§†é¢‘æ”»å‡»å…¬å…±åª’ä½“å®‰å…¨ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨è¯­éŸ³å’Œè§†é¢‘æµè¿›è¡Œæ•°å­—é¢éƒ¨ä¼ªé€ ï¼Œä»¥è‡³äºè¯†åˆ«äººè„¸ç‰¹å¾å›°éš¾ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰ã€éŸ³é¢‘å’Œè¿åŠ¨ç‰¹å¾çš„æ£€æµ‹ç½‘ç»œï¼ˆFTFDNetï¼‰ï¼Œå¹¶ä½¿ç”¨é«˜æ•ˆçš„è·¨æ¨¡æ€èåˆï¼ˆCMFï¼‰æ¨¡å—å°†è¿™äº›ç‰¹å¾èåˆã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„éŸ³é¢‘è§†é¢‘æ³¨æ„æœºåˆ¶ï¼ˆAVAMï¼‰ï¼Œå¯ä»¥æ‰¾åˆ°æ›´å¤šçš„æœ‰ç”¨ç‰¹å¾ã€‚</li>
<li>results: FTFDNetåœ¨å·²çŸ¥çš„æ·±ä¼ªé¢éƒ¨æ£€æµ‹æ•°æ®é›†ï¼ˆFTFDDï¼‰ä»¥åŠæ·±ä¼ªè§†é¢‘æ£€æµ‹æ•°æ®é›†ï¼ˆDFDCå’ŒDF-TIMITï¼‰ä¸Šè¾¾åˆ°äº†ä¸å…¶ä»–å½“å‰é¢†å…ˆçš„æ·±ä¼ªè§†é¢‘æ£€æµ‹æ–¹æ³•ç›¸æ¯”è¾ƒå¥½çš„æ£€æµ‹æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
DeepFake based digital facial forgery is threatening public media security, especially when lip manipulation has been used in talking face generation, and the difficulty of fake video detection is further improved. By only changing lip shape to match the given speech, the facial features of identity are hard to be discriminated in such fake talking face videos. Together with the lack of attention on audio stream as the prior knowledge, the detection failure of fake talking face videos also becomes inevitable. It's found that the optical flow of the fake talking face video is disordered especially in the lip region while the optical flow of the real video changes regularly, which means the motion feature from optical flow is useful to capture manipulation cues. In this study, a fake talking face detection network (FTFDNet) is proposed by incorporating visual, audio and motion features using an efficient cross-modal fusion (CMF) module. Furthermore, a novel audio-visual attention mechanism (AVAM) is proposed to discover more informative features, which can be seamlessly integrated into any audio-visual CNN architecture by modularization. With the additional AVAM, the proposed FTFDNet is able to achieve a better detection performance than other state-of-the-art DeepFake video detection methods not only on the established fake talking face detection dataset (FTFDD) but also on the DeepFake video detection datasets (DFDC and DF-TIMIT).
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å¤åˆ¶åŸºæœ¬çš„æ•°å­—é¢ä¼ªé€ æ˜¯å…¬å…±åª’ä½“å®‰å…¨çš„å¨èƒï¼Œç‰¹åˆ«æ˜¯åœ¨å£å‹ manipulate åœ¨ç”Ÿæˆ talking face ä¸­ä½¿ç”¨ï¼Œå¹¶ä½¿ fake video æ£€æµ‹æ›´åŠ å›°éš¾ã€‚åªæœ‰æ”¹å˜ lip å½¢çŠ¶æ¥åŒ¹é…ç»™å®šçš„speechï¼Œthen the facial features of identity are difficult to distinguish in such fake talking face videos. In addition, the lack of attention to the audio stream as prior knowledge makes it impossible to detect fake talking face videos. It is found that the optical flow of the fake talking face video is disordered, especially in the lip region, while the optical flow of the real video changes regularly, which means that the motion feature from optical flow is useful for capturing manipulation cues. In this study, a fake talking face detection network (FTFDNet) is proposed by incorporating visual, audio, and motion features using an efficient cross-modal fusion (CMF) module. Furthermore, a novel audio-visual attention mechanism (AVAM) is proposed to discover more informative features, which can be seamlessly integrated into any audio-visual CNN architecture by modularization. With the additional AVAM, the proposed FTFDNet is able to achieve a better detection performance than other state-of-the-art DeepFake video detection methods not only on the established fake talking face detection dataset (FTFDD) but also on the DeepFake video detection datasets (DFDC and DF-TIMIT).
</details></li>
</ul>
<hr>
<h2 id="TractGeoNet-A-geometric-deep-learning-framework-for-pointwise-analysis-of-tract-microstructure-to-predict-language-assessment-performance"><a href="#TractGeoNet-A-geometric-deep-learning-framework-for-pointwise-analysis-of-tract-microstructure-to-predict-language-assessment-performance" class="headerlink" title="TractGeoNet: A geometric deep learning framework for pointwise analysis of tract microstructure to predict language assessment performance"></a>TractGeoNet: A geometric deep learning framework for pointwise analysis of tract microstructure to predict language assessment performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03982">http://arxiv.org/abs/2307.03982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqian Chen, Leo R. Zekelman, Chaoyi Zhang, Tengfei Xue, Yang Song, Nikos Makris, Yogesh Rathi, Alexandra J. Golby, Weidong Cai, Fan Zhang, Lauren J. Oâ€™Donnell</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§åŸºäºå‡ ä½•æ·±åº¦å­¦ä¹ çš„æ¨è®ºæ¡†æ¶ï¼Œä»¥ç”¨äºä½¿ç”¨æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰è½¨è„‰å›¾åƒå’Œç›¸å…³çš„ç‚¹ç²’å­å¾®ç»“æ„æµ‹é‡è¿›è¡Œå›å½’ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨ç‚¹äº‘è¡¨ç¤ºï¼Œå¯ä»¥ç›´æ¥åˆ©ç”¨è½¨è„‰å›¾åƒä¸­æ‰€æœ‰ç‚¹çš„ä½åŠ¿ä¿¡æ¯å’Œç»†èƒå¾®ç»“æ„ä¿¡æ¯è¿›è¡Œæ¨è®ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œå³å¯¹ç§°å¯¹æ¯”æ¨è®ºæŸå¤±å‡½æ•°ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹å‡†ç¡®åœ°é¢„æµ‹è½¨è„‰å›¾åƒä¸­ç‚¹ç²’å­å¾®ç»“æ„ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>results: æˆ‘ä»¬é€šè¿‡ä½¿ç”¨TractGeoNetè¿›è¡Œå›å½’é¢„æµ‹ï¼Œå¹¶è¯„ä¼°äº†20ä¸ªç™½ matterè½¨è„‰å›¾åƒçš„è¯­è¨€åŠŸèƒ½è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒTractGeoNetæ¯”è®¸å¤šæµè¡Œçš„å›å½’æ¨¡å‹è¡¨ç°æ›´ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å·¦å¼¯æ›²è„‰å¹•è½¨è„‰æ˜¯è¯­è¨€åŠŸèƒ½è¡¨ç°çš„æœ€é‡è¦é¢„æµ‹å› ç´ ä¹‹ä¸€ã€‚æœ¬åœ°åŒ–çš„å…³é”®åŒºåŸŸåˆ†å¸ƒåœ¨ä¸¤ä¸ªåŠçƒçš„ç™½ matterè½¨è„‰ä¸­ï¼ŒåŒ…æ‹¬è€³å»¶ç›˜ã€å‰å¶ã€ä¸Šéƒ¨å’Œä¸‹éƒ¨çš„è„‘åŒºåŸŸï¼Œè¿™äº›è„‘åŒºåŸŸè¢«è®¤ä¸ºæ˜¯è¯­è¨€åŠŸèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æ€»çš„æ¥è¯´ï¼ŒTractGeoNetè¡¨æ˜å‡ ä½•æ·±åº¦å­¦ä¹ å¯ä»¥å¢å¼ºè„‘ç™½ matterè½¨è„‰çš„ç ”ç©¶å’Œè¯­è¨€åŠŸèƒ½ä¹‹é—´çš„å…³ç³»ã€‚<details>
<summary>Abstract</summary>
We propose a geometric deep-learning-based framework, TractGeoNet, for performing regression using diffusion magnetic resonance imaging (dMRI) tractography and associated pointwise tissue microstructure measurements. By employing a point cloud representation, TractGeoNet can directly utilize pointwise tissue microstructure and positional information from all points within a fiber tract. To improve regression performance, we propose a novel loss function, the Paired-Siamese Regression loss, which encourages the model to focus on accurately predicting the relative differences between regression label scores rather than just their absolute values. In addition, we propose a Critical Region Localization algorithm to identify highly predictive anatomical regions within the white matter fiber tracts for the regression task. We evaluate the effectiveness of the proposed method by predicting individual performance on two neuropsychological assessments of language using a dataset of 20 association white matter fiber tracts from 806 subjects from the Human Connectome Project. The results demonstrate superior prediction performance of TractGeoNet compared to several popular regression models. Of the twenty tracts studied, we find that the left arcuate fasciculus tract is the most highly predictive of the two studied language performance assessments. The localized critical regions are widespread and distributed across both hemispheres and all cerebral lobes, including areas of the brain considered important for language function such as superior and anterior temporal regions, pars opercularis, and precentral gyrus. Overall, TractGeoNet demonstrates the potential of geometric deep learning to enhance the study of the brain's white matter fiber tracts and to relate their structure to human traits such as language performance.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå‡ ä½•æ·±åº¦å­¦ä¹ åŸºäºæ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰çš„æ¨è®ºæ¡†æ¶ï¼Œç§°ä¸ºTractGeoNetï¼Œç”¨äºè¿›è¡Œå›å½’ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç‚¹äº‘è¡¨ç¤ºï¼Œå¯ä»¥ç›´æ¥åˆ©ç”¨æ‰€æœ‰çº¤ç»´è‚¡åŒºåŸŸçš„ç‚¹wiseç»„ç»‡å’Œä½åŸŸä¿¡æ¯ã€‚ä¸ºäº†æé«˜å›å½’æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œå³å¯¹ç§°å¯¹æ‹¼å‡æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°é¼“åŠ±æ¨¡å‹å¯¹å‡†ç²¾ç¡®åœ°é¢„æµ‹ç›¸å¯¹å·®å€¼è€Œä¸ä»…ä»…æ˜¯ç»å¯¹å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…³é”®åŒºåŸŸå®šä½ç®—æ³•ï¼Œç”¨äºåœ¨ç™½ matterçº¤ç»´è‚¡ä¸­é¢„æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨dMRIæ•°æ®é›†çš„20ä¸ªç›¸å…³çº¤ç»´è‚¡ï¼Œä»806åå‚ä¸è€…ä¸­é¢„æµ‹è¯­è¨€æ€§èƒ½ï¼Œè¯æ˜äº†TractGeoNetçš„æ•ˆæœæ¯”è¾ƒå‡ºè‰²ã€‚å…¶ä¸­ï¼Œå·¦å¼¯æ›² fasiculus çº¤ç»´è‚¡è¢«è¯æ˜ä¸ºè¯­è¨€æ€§èƒ½é¢„æµ‹çš„æœ€é«˜åº¦ç›¸å…³ã€‚å…³é”®åŒºåŸŸå¹¿æ³›åˆ†å¸ƒåœ¨ä¸¤ä¸ªåŠçƒå’Œæ‰€æœ‰è„‘å¶ï¼ŒåŒ…æ‹¬è¢«è®¤ä¸ºæ˜¯è¯­è¨€åŠŸèƒ½é‡è¦çš„è„‘åŒºåŸŸï¼Œå¦‚ä¸Šä¾§ temporalisã€anterior temporalisã€pars opercularis å’Œ precentral gyrusã€‚æ€»ä¹‹ï¼ŒTractGeoNetè¡¨æ˜å‡ ä½•æ·±åº¦å­¦ä¹ å¯ä»¥æé«˜ç™½ matterçº¤ç»´è‚¡çš„ç ”ç©¶å’Œè¯­è¨€æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚
</details></li>
</ul>
<hr>
<h2 id="Building-and-Road-Segmentation-Using-EffUNet-and-Transfer-Learning-Approach"><a href="#Building-and-Road-Segmentation-Using-EffUNet-and-Transfer-Learning-Approach" class="headerlink" title="Building and Road Segmentation Using EffUNet and Transfer Learning Approach"></a>Building and Road Segmentation Using EffUNet and Transfer Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03980">http://arxiv.org/abs/2307.03980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahil Gangurde</li>
<li>for: æœ¬è®ºæ–‡æ—¨åœ¨ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•å®ç°åŸå¸‚è§„åˆ’ä¸­çš„å»ºç­‘ç‰©å’Œè·¯å¾„åˆ†å‰²ï¼Œä»¥æé«˜åŸå¸‚è§„åˆ’å†³ç­–çš„æ•ˆæœã€‚</li>
<li>methods: æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºGoogleæ–°æå‡ºçš„EfficientNetV2çš„Encoder-UNet Decoderæ¶æ„ï¼Œç”¨äºfeatureæå–å’Œåˆ†å‰²åœ°å›¾ä¸­çš„å»ºç­‘ç‰©å’Œè·¯å¾„ã€‚</li>
<li>results: ä½¿ç”¨è¯¥æ–¹æ³•ï¼Œ authorsåœ¨éº»çœå»ºç­‘ç‰©å’Œè·¯å¾„ datasetä¸Šè¾¾åˆ°äº† benchmark åˆ†å‰²ç²¾åº¦ï¼ˆmIOUï¼‰çš„0.8365å’Œ0.9153ã€‚<details>
<summary>Abstract</summary>
In city, information about urban objects such as water supply, railway lines, power lines, buildings, roads, etc., is necessary for city planning. In particular, information about the spread of these objects, locations and capacity is needed for the policymakers to make impactful decisions. This thesis aims to segment the building and roads from the aerial image captured by the satellites and UAVs. Many different architectures have been proposed for the semantic segmentation task and UNet being one of them. In this thesis, we propose a novel architecture based on Google's newly proposed EfficientNetV2 as an encoder for feature extraction with UNet decoder for constructing the segmentation map. Using this approach we achieved a benchmark score for the Massachusetts Building and Road dataset with an mIOU of 0.8365 and 0.9153 respectively.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨åŸå¸‚ä¸­ï¼Œæœ‰å…³åŸå¸‚ Ğ¾Ğ±ÑŠĞµĞºts such as æ°´upply, é“è·¯çº¿, ç”µåŠ›çº¿, å»ºç­‘ç‰©, è·¯ç½‘ç­‰çš„ä¿¡æ¯æ˜¯å¿…è¦çš„ Ğ´Ğ»ÑåŸå¸‚è§„åˆ’ã€‚ç‰¹åˆ«æ˜¯åœ¨å†³ç­–è€…ä»¬éœ€è¦äº†è§£è¿™äº›å¯¹è±¡çš„æ‰©æ•£ã€ä½ç½®å’Œå®¹é‡ï¼Œä»¥ä¾¿åšå‡ºæœ‰æ•ˆçš„å†³ç­–ã€‚æœ¬è®ºæ–‡ç›®çš„æ˜¯ä»èˆªç©ºå›¾åƒå’Œæ— äººæœºæ‹æ‘„çš„å«æ˜Ÿå’ŒUAVä¸­æå–å»ºç­‘ç‰©å’Œè·¯ç½‘çš„ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨Googleæå‡ºçš„æ–°çš„EfficientNetV2åµŒå…¥å™¨å’ŒUNetè§£ç å™¨ç»„æˆ semantic segmentation å›¾åƒã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„ï¼Œå¹¶åœ¨Massachusetts Building and Roadæ•°æ®é›†ä¸Šå®ç°äº†benchmarkåˆ†æ•°ï¼Œå…·ä½“åˆ†åˆ«ä¸º0.8365å’Œ0.9153ã€‚
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Supervised-Multilabel-Contrastive-Learning"><a href="#End-to-End-Supervised-Multilabel-Contrastive-Learning" class="headerlink" title="End-to-End Supervised Multilabel Contrastive Learning"></a>End-to-End Supervised Multilabel Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03967">http://arxiv.org/abs/2307.03967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mahdihosseini/kmcl">https://github.com/mahdihosseini/kmcl</a></li>
<li>paper_authors: Ahmad Sajedi, Samir Khaki, Konstantinos N. Plataniotis, Mahdi S. Hosseini</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§æ–°çš„ç»¼åˆè®­ç»ƒæ¡†æ¶ï¼Œä»¥è§£å†³å¤šæ ‡ç­¾è¡¨ç¤ºå­¦ä¹ ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ ‡ç­¾ç›¸å…³æ€§å’Œæ•°æ®ç›¸å…³æ€§ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†kernel-basedå¤šæ ‡ç­¾å¯¹æ¯”å­¦ä¹ ï¼ˆKMCLï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é¦–å…ˆå°†åµŒå…¥ç‰¹å¾è½¬åŒ–ä¸ºä¸€ç§æ··åˆ exponential kernel çš„ Gaussian RKHS ä¸­ï¼Œç„¶åä½¿ç”¨ä¸€ä¸ªåŒ…å« reconstruction lossã€éå¯¹ç§°åˆ†ç±»æŸå¤±å’Œå¯¹æ¯”æŸå¤±çš„ç›®æ ‡å‡½æ•°è¿›è¡Œç¼–ç ã€‚</li>
<li>results: EXTENSIVE experiments è¡¨æ˜ï¼ŒKMCL å¯ä»¥åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å®ç°é²æ£’çš„æ”¹è¿›ï¼Œå¹¶ä¸”å¯ä»¥å‡å°‘è®¡ç®—å¤æ‚æ€§ã€‚<details>
<summary>Abstract</summary>
Multilabel representation learning is recognized as a challenging problem that can be associated with either label dependencies between object categories or data-related issues such as the inherent imbalance of positive/negative samples. Recent advances address these challenges from model- and data-centric viewpoints. In model-centric, the label correlation is obtained by an external model designs (e.g., graph CNN) to incorporate an inductive bias for training. However, they fail to design an end-to-end training framework, leading to high computational complexity. On the contrary, in data-centric, the realistic nature of the dataset is considered for improving the classification while ignoring the label dependencies. In this paper, we propose a new end-to-end training framework -- dubbed KMCL (Kernel-based Mutlilabel Contrastive Learning) -- to address the shortcomings of both model- and data-centric designs. The KMCL first transforms the embedded features into a mixture of exponential kernels in Gaussian RKHS. It is then followed by encoding an objective loss that is comprised of (a) reconstruction loss to reconstruct kernel representation, (b) asymmetric classification loss to address the inherent imbalance problem, and (c) contrastive loss to capture label correlation. The KMCL models the uncertainty of the feature encoder while maintaining a low computational footprint. Extensive experiments are conducted on image classification tasks to showcase the consistent improvements of KMCL over the SOTA methods. PyTorch implementation is provided in \url{https://github.com/mahdihosseini/KMCL}.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šæ ‡ç­¾è¡¨ç¤ºå­¦ä¹ è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ï¼Œä¸å¯¹è±¡ç±»åˆ«æ ‡ç­¾ä¹‹é—´çš„ç›¸äº’å…³ç³»æˆ–æ•°æ®ç›¸å…³çš„é—®é¢˜æœ‰å…³ã€‚ç°ä»£è¿›æ­¥ä»æ¨¡å‹å’Œæ•°æ®ä¸­å¿ƒè§†è§’æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚åœ¨æ¨¡å‹ä¸­å¿ƒçš„æ–¹æ³•ä¸­ï¼Œé€šè¿‡å¤–éƒ¨æ¨¡å‹è®¾è®¡ï¼ˆä¾‹å¦‚å›¾åƒ CNNï¼‰è·å¾—æ ‡ç­¾ç›¸å…³æ€§ï¼Œä½†æ˜¯å®ƒä»¬æ— æ³•è®¾è®¡ç«¯åˆ°ç«¯è®­ç»ƒæ¡†æ¶ï¼Œå¯¼è‡´è®¡ç®—å¤æ‚æ€§è¿‡é«˜ã€‚åœ¨æ•°æ®ä¸­å¿ƒçš„æ–¹æ³•ä¸­ï¼Œåˆ©ç”¨å®é™…æ•°æ®çš„è‡ªç„¶ç‰¹æ€§æ¥æé«˜åˆ†ç±»ï¼Œå¿½ç•¥æ ‡ç­¾ç›¸äº’å…³ç³»ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°çš„ç«¯åˆ°ç«¯è®­ç»ƒæ¡†æ¶â€”â€”æ‹¥æœ‰kernel-basedå¤šæ ‡ç­¾å¯¹æ¯”å­¦ä¹ ï¼ˆKMCLï¼‰â€”â€”ä»¥è§£å†³æ¨¡å‹å’Œæ•°æ®ä¸­å¿ƒçš„ç¼ºç‚¹ã€‚KMCLé¦–å…ˆå°†åµŒå…¥ç‰¹å¾è½¬æ¢ä¸ºæ··åˆå‡ ä½•å‡½æ•°åœ¨é«˜æ–¯RKHSä¸­ï¼Œç„¶åç¼–ç ä¸€ä¸ªåŒ…å«ï¼ˆaï¼‰é‡å»ºkernelè¡¨ç¤ºçš„æŸå¤±å‡½æ•°ï¼Œï¼ˆbï¼‰åå¥½ç±»åˆ«çš„ä¸å¯¹ç§°åˆ†ç±»æŸå¤±å‡½æ•°ï¼Œå’Œï¼ˆcï¼‰æ•æ‰æ ‡ç­¾ç›¸å…³æ€§çš„å¯¹æ¯”æŸå¤±å‡½æ•°çš„ç›®æ ‡æŸå¤±å‡½æ•°ã€‚KMCLæ¨¡å‹ç‰¹å¾ç¼–ç å™¨çš„ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—è„šæœ¬çš„ä½å³°å€¼ã€‚æˆ‘ä»¬åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶è¯æ˜äº†KMCLåœ¨SOTAæ–¹æ³•ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚PyTorchå®ç°å¯ä»¥åœ¨ \url{https://github.com/mahdihosseini/KMCL} ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Reading-Between-the-Lanes-Text-VideoQA-on-the-Road"><a href="#Reading-Between-the-Lanes-Text-VideoQA-on-the-Road" class="headerlink" title="Reading Between the Lanes: Text VideoQA on the Road"></a>Reading Between the Lanes: Text VideoQA on the Road</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03948">http://arxiv.org/abs/2307.03948</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Tom, Minesh Mathew, Sergi Garcia, Dimosthenis Karatzas, C. V. Jawahar</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ä¸ºäº†æé«˜è½¦è¾†é©¾é©¶å‘˜çš„å®‰å…¨é©¾é©¶å’Œæƒ…æ„ŸçŸ¥è¯†ï¼Œé€šè¿‡æ•æ‰è½¦è¾†å‰æ–¹ç¯å¢ƒä¸­çš„æ–‡å­—ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºé©¾é©¶å‘˜å¯ä»¥ç†è§£çš„å½¢å¼ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†è§†è§‰æ•°æ®æµè¿›è¡Œæ–‡å­—è¯†åˆ«ï¼Œå¹¶å°†æ–‡å­—è¯†åˆ«ç»“æœä¸æ—¶é—´è¿›è¡Œæ¨ç†ï¼Œä»¥æé«˜é©¾é©¶å‘˜çš„æƒ…æ„ŸçŸ¥è¯†å’Œé©¾é©¶å®‰å…¨æ€§ã€‚</li>
<li>results: è¿™ä¸ªç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„VideoQAæ¨¡å‹åœ¨è¿™ä¸ªé¢†åŸŸä¸­ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ï¼Œå¹¶ä¸”æ˜¾ç¤ºäº†è¿™ä¸ª dataset çš„å¯ç”¨æ€§å’Œé‡è¦æ€§åœ¨è¿›ä¸€æ­¥æ¨è¿›é©¾é©¶å‘˜æ”¯æŒç³»ç»Ÿå’Œæ–‡å­—æ•æ„Ÿå¤šmodalé—®ç­”çš„ç ”ç©¶ä¸­ã€‚<details>
<summary>Abstract</summary>
Text and signs around roads provide crucial information for drivers, vital for safe navigation and situational awareness. Scene text recognition in motion is a challenging problem, while textual cues typically appear for a short time span, and early detection at a distance is necessary. Systems that exploit such information to assist the driver should not only extract and incorporate visual and textual cues from the video stream but also reason over time. To address this issue, we introduce RoadTextVQA, a new dataset for the task of video question answering (VideoQA) in the context of driver assistance. RoadTextVQA consists of $3,222$ driving videos collected from multiple countries, annotated with $10,500$ questions, all based on text or road signs present in the driving videos. We assess the performance of state-of-the-art video question answering models on our RoadTextVQA dataset, highlighting the significant potential for improvement in this domain and the usefulness of the dataset in advancing research on in-vehicle support systems and text-aware multimodal question answering. The dataset is available at http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtextvqa
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–‡æœ¬å’Œè·¯ä¸Šçš„ç¤ºæ„å›¾æä¾›äº†é©¾é©¶å‘˜ navigate å’Œæƒ…å†µæ„è¯† çš„å…³é”®ä¿¡æ¯ã€‚Scene text recognition in motion æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºæ–‡æœ¬cue é€šå¸¸åªå‡ºç°çŸ­æ—¶é—´ï¼Œæ—©æœŸæ£€æµ‹è·ç¦»æ˜¯å¿…è¦çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä»‹ç»äº† RoadTextVQA  datasetï¼Œç”¨äºé©¾é©¶åŠ©æ‰‹Question Answering ä»»åŠ¡ï¼ˆVideoQAï¼‰çš„ç ”ç©¶ã€‚RoadTextVQA åŒ…æ‹¬ $3,222$ æ®µé©¾é©¶è§†é¢‘ï¼Œæ”¶é›†è‡ªå¤šä¸ªå›½å®¶ï¼Œå¹¶æœ‰ $10,500$ ä¸ªé—®é¢˜ï¼Œæ‰€æœ‰é—®é¢˜åŸºäºé©¾é©¶è§†é¢‘ä¸­çš„æ–‡æœ¬æˆ–è·¯ä¸Šç¤ºæ„å›¾ã€‚æˆ‘ä»¬è¯„ä¼°äº†ç°æœ‰çš„ VideoQA æ¨¡å‹åœ¨æˆ‘ä»¬çš„ RoadTextVQA dataset ä¸Šçš„æ€§èƒ½ï¼Œå¹¶æŒ‡å‡ºäº†è¿™ä¸ªé¢†åŸŸçš„æ˜¾è‘—æ”¹è¿›æ½œåŠ›å’Œä½¿ç”¨è¿™ä¸ªdatasetè¿›è¡Œæ–‡æœ¬æ„ŸçŸ¥å¤šæ¨¡å¼é—®ç­”çš„ç ”ç©¶çš„ç”¨äºã€‚dataset å¯ä»¥åœ¨ http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtextvqa ä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Camouflaged-Object-Detection-with-Feature-Grafting-and-Distractor-Aware"><a href="#Camouflaged-Object-Detection-with-Feature-Grafting-and-Distractor-Aware" class="headerlink" title="Camouflaged Object Detection with Feature Grafting and Distractor Aware"></a>Camouflaged Object Detection with Feature Grafting and Distractor Aware</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03943">http://arxiv.org/abs/2307.03943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/syxvision/fdnet">https://github.com/syxvision/fdnet</a></li>
<li>paper_authors: Yuxuan Song, Xinyue Li, Lin Qi</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜æ©è”½ç‰©æ£€æµ‹çš„ç²¾åº¦ï¼Œä½¿å¾—èƒ½å¤Ÿå‡†ç¡®åœ°æ£€æµ‹æ©è”½åœ¨ç¯å¢ƒä¸­çš„ç›®æ ‡ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„Feature Grafting and Distractor Awareç½‘ç»œï¼ˆFDNetï¼‰æ¥è§£å†³æ©è”½ç‰©æ£€æµ‹é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†CNNå’ŒTransformeræ¥å¹¶è¡Œåœ°ç¼–ç å¤šå°ºåº¦å›¾åƒã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ä¸¤ä¸ªç¼–ç å™¨çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªcross-attention-based Feature GraftingModuleï¼Œå°†Transformeråˆ†æ”¯ä¸­æå–çš„ç‰¹å¾èåˆåˆ°CNNåˆ†æ”¯ä¸­ï¼Œç„¶ååœ¨Feature Fusion Moduleä¸­è¿›è¡Œç²˜åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªExplicitly Modeling Distractorsæ¨¡å—ï¼Œä»¥ä¾¿æ›´åŠ ç²¾ç¡®åœ°æ¨¡æ‹Ÿæ©è”½ç‰©æ£€æµ‹ä¸­çš„ä¸¤ç§å¯èƒ½çš„å¹²æ‰°å› ç´ ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•åœ¨å››ä¸ªå¸¸ç”¨çš„ benchmark datasets ä»¥åŠACOD2K datasetä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸å…¶ä»–çŠ¶æ€ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæœ‰æ˜¾è‘—çš„æé«˜ã€‚ä»£ç å’ŒACOD2K datasetå°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/syxvision/FDNet%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/syxvision/FDNetä¸Šå…¬å¼€ã€‚</a><details>
<summary>Abstract</summary>
The task of Camouflaged Object Detection (COD) aims to accurately segment camouflaged objects that integrated into the environment, which is more challenging than ordinary detection as the texture between the target and background is visually indistinguishable. In this paper, we proposed a novel Feature Grafting and Distractor Aware network (FDNet) to handle the COD task. Specifically, we use CNN and Transformer to encode multi-scale images in parallel. In order to better explore the advantages of the two encoders, we design a cross-attention-based Feature Grafting Module to graft features extracted from Transformer branch into CNN branch, after which the features are aggregated in the Feature Fusion Module. A Distractor Aware Module is designed to explicitly model the two possible distractors in the COD task to refine the coarse camouflage map. We also proposed the largest artificial camouflaged object dataset which contains 2000 images with annotations, named ACOD2K. We conducted extensive experiments on four widely used benchmark datasets and the ACOD2K dataset. The results show that our method significantly outperforms other state-of-the-art methods. The code and the ACOD2K will be available at https://github.com/syxvision/FDNet.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œCamouflaged Object Detectionï¼ˆCODï¼‰ä»»åŠ¡çš„ç›®æ ‡æ˜¯ç²¾å‡†åœ°æ‰¾åˆ°èå…¥ç¯å¢ƒä¸­çš„æ©è”½ç‰©ï¼Œè¿™æ¯”æ™®é€šçš„æ£€æµ‹æ›´åŠ å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ ‡çš„å’ŒèƒŒæ™¯çš„æ–‡å­—ç‰¹å¾æ— æ³•è¾¨è¯†ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ç‰¹è‰²æ’å…¥å’Œå¹²æ‰°è¯†åˆ«ç½‘ç»œï¼ˆFDNetï¼‰æ¥å¤„ç†CODä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨CNNå’ŒTransformerå¹¶è¡Œåœ°å®ç°å¤šä¸ªæ ‡æœ¬çš„åƒç´ ç½‘ç»œã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ä¸¤ä¸ªç½‘ç»œçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªäº¤äº’å¼ç‰¹è‰²æ’å…¥æ¨¡ç»„ï¼Œå°†Transformeråˆ†æ”¯ä¸­æå–çš„ç‰¹è‰²æ’å…¥åˆ°CNNåˆ†æ”¯ä¸­ï¼Œç„¶åå°†ç‰¹è‰²åœ¨ç‰¹è‰²èšåˆæ¨¡ç»„ä¸­èšåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¹²æ‰°è¯†åˆ«æ¨¡ç»„ï¼Œä»¥Explicitlyæ¨¡å‹CODä»»åŠ¡ä¸­çš„ä¸¤ç§å¹²æ‰°å› ç´ ï¼Œä»¥æ”¹å–„ç²—ç³™çš„æ©è”½åœ°å›¾ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†2000å¹…æ©è”½ç‰©æ ‡æ³¨å›¾åƒé›†åˆï¼Œåä¸ºACOD2Kã€‚æˆ‘ä»¬å®ç°äº†å¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬å››ä¸ªé€šç”¨çš„ benchmark æµ‹è¯•é›†å’Œ ACOD2K æ ‡æ³¨å›¾åƒé›†åˆã€‚ç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¶ä»–çŠ¶æ€é¡¶ä¸“é—¨æ–¹æ³•ä¹‹ä¸Šå¾—åˆ°äº†å¾ˆå¥½çš„è¡¨ç°ã€‚æˆ‘ä»¬å°†ä»£ç å’ŒACOD2Kæ•°æ®é›†å­˜å‚¨åœ¨ GitHub ä¸Šï¼Œè¯·éµå¾ª https://github.com/syxvision/FDNet æ¥è®¿é—®ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Ariadneâ€™s-Thread-Using-Text-Prompts-to-Improve-Segmentation-of-Infected-Areas-from-Chest-X-ray-images"><a href="#Ariadneâ€™s-Thread-Using-Text-Prompts-to-Improve-Segmentation-of-Infected-Areas-from-Chest-X-ray-images" class="headerlink" title="Ariadneâ€™s Thread:Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray images"></a>Ariadneâ€™s Thread:Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03942">http://arxiv.org/abs/2307.03942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junelin2333/languidemedseg-miccai2023">https://github.com/junelin2333/languidemedseg-miccai2023</a></li>
<li>paper_authors: Yi Zhong, Mengqiu Xu, Kongming Liang, Kaixin Chen, Ming Wu</li>
<li>for: è¯¥ç ”ç©¶æ—¨åœ¨æé«˜è‚ºç—…è¯„ä¼°çš„ç²¾åº¦ï¼Œæä¾›æ›´å‡†ç¡®çš„è‚ºç—…è¯Šæ–­å’Œæ²»ç–—æ–¹æ¡ˆã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨è¯­è¨€é©±åŠ¨çš„å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºæ¥æ”¹è¿›å›¾åƒåˆ†å‰²ç»“æœã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸å•æ¨¡æ–¹æ³•ç›¸æ¯”ï¼Œæé«˜äº†QaTa-COV19æ•°æ®é›†çš„ diceåˆ†æ•°6.09%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¡¨æ˜ï¼Œå¤šæ¨¡å¼æ–¹æ³•åœ¨æ–‡æœ¬ç²’åº¦å’Œè®­ç»ƒæ•°æ®å¤§å°æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚<details>
<summary>Abstract</summary>
Segmentation of the infected areas of the lung is essential for quantifying the severity of lung disease like pulmonary infections. Existing medical image segmentation methods are almost uni-modal methods based on image. However, these image-only methods tend to produce inaccurate results unless trained with large amounts of annotated data. To overcome this challenge, we propose a language-driven segmentation method that uses text prompt to improve to the segmentation result. Experiments on the QaTa-COV19 dataset indicate that our method improves the Dice score by 6.09% at least compared to the uni-modal methods. Besides, our extended study reveals the flexibility of multi-modal methods in terms of the information granularity of text and demonstrates that multi-modal methods have a significant advantage over image-only methods in terms of the size of training data required.
</details>
<details>
<summary>æ‘˜è¦</summary>
segmentation of infected lung areas is crucial for assessing lung disease severity, such as pulmonary infections. current medical image segmentation methods are mostly uni-modal, relying solely on images. however, these image-only methods often produce inaccurate results without large amounts of annotated data. to address this challenge, we propose a language-driven segmentation method that utilizes text prompts to improve segmentation accuracy. experiments on the QaTa-COV19 dataset show that our method improves the dice score by at least 6.09% compared to uni-modal methods. furthermore, our extended study demonstrates the flexibility of multi-modal methods in terms of text information granularity and shows that multi-modal methods require significantly less training data than image-only methods.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Face-Image-Quality-Enhancement-Study-for-Face-Recognition"><a href="#Face-Image-Quality-Enhancement-Study-for-Face-Recognition" class="headerlink" title="Face Image Quality Enhancement Study for Face Recognition"></a>Face Image Quality Enhancement Study for Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05534">http://arxiv.org/abs/2307.05534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Iqbal Nouyed, Na Zhang</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨ä½è´¨é‡faceå›¾åƒ face recognitionçš„é—®é¢˜ï¼Œå°è¯•æé«˜ä½è´¨é‡faceå›¾åƒçš„è¯†åˆ«ç²¾åº¦ã€‚</li>
<li>methods: ä½¿ç”¨å½“å‰æœ€ä½³çš„äººè„¸å›¾åƒè¿›è¡Œä¼˜åŒ–ï¼Œå¼€å‘äº†ä¸€ç§æ–°çš„è¯†åˆ«åè®®ï¼Œä»¥é¿å…å®éªŒåè§ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ä¼˜åŒ–åçš„faceå›¾åƒå¯ä»¥æé«˜è¯†åˆ«ç²¾åº¦ï¼Œä½†ä¹Ÿå­˜åœ¨ä¸€äº›æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚<details>
<summary>Abstract</summary>
Unconstrained face recognition is an active research area among computer vision and biometric researchers for many years now. Still the problem of face recognition in low quality photos has not been well-studied so far. In this paper, we explore the face recognition performance on low quality photos, and we try to improve the accuracy in dealing with low quality face images. We assemble a large database with low quality photos, and examine the performance of face recognition algorithms for three different quality sets. Using state-of-the-art facial image enhancement approaches, we explore the face recognition performance for the enhanced face images. To perform this without experimental bias, we have developed a new protocol for recognition with low quality face photos and validate the performance experimentally. Our designed protocol for face recognition with low quality face images can be useful to other researchers. Moreover, experiment results show some of the challenging aspects of this problem.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>æ— çº¦æŸé¢Recognitionæ˜¯è®¡ç®—æœºè§†è§‰å’Œç”Ÿç‰©è®¤è¯é¢†åŸŸçš„æ´»è·ƒç ”ç©¶é¢†åŸŸï¼Œæ•°å¹´æ¥ä»ç„¶æ²¡æœ‰å……åˆ†ç ”ç©¶ä½è´¨é‡ç…§ç‰‡çš„é¢Recognitioné—®é¢˜ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½è´¨é‡ç…§ç‰‡ä¸­çš„é¢Recognitionæ€§èƒ½ï¼Œå¹¶å°è¯•æé«˜å¯¹ä½è´¨é‡é¢å›¾åƒçš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬ç»„ç»‡äº†ä¸€ä¸ªå¤§å‹æ•°æ®åº“ï¼Œå¹¶å¯¹ä¸‰ä¸ªä¸åŒè´¨é‡é›†è¿›è¡Œäº†æµ‹è¯•ã€‚ä½¿ç”¨å½“å‰çš„çŠ¶æ€kemal facial image enhancementæ–¹æ³•ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¯¹åŠ å¼ºçš„é¢å›¾åƒè¿›è¡Œè®¤è¯çš„æ€§èƒ½ã€‚ä¸ºäº†é¿å…å®éªŒåè§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„åè®® Ğ´Ğ»Ñé¢Recognition with low quality face photosï¼Œå¹¶ Validate its performance experimentallyã€‚æˆ‘ä»¬çš„è®¾è®¡çš„åè®®å¯ä»¥ä¸ºå…¶ä»–ç ”ç©¶äººå‘˜æä¾›å¸®åŠ©ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜äº†ä¸€äº›é¢Recognitioné—®é¢˜çš„æŒ‘æˆ˜æ€§æ–¹é¢ã€‚Translation notes:* "æ— çº¦æŸ" (wÃº jiÃ¨ shÃ¬) means "unconstrained" in Chinese.* "é¢Recognition" (liÇn tiÄn xiÇng) means "face recognition" in Chinese.* "ç”Ÿç‰©è®¤è¯" (shÄ“ng wÃ¹ rÃ¨n shÃ¨) means "biometric recognition" in Chinese.* "è®¡ç®—æœºè§†è§‰" (jÃ¬suÃ n jÄ«suÄn) means "computer vision" in Chinese.* "æ•°å¹´æ¥" (shÃ¹ niÃ¡n lÃ¡i) means "for many years" in Chinese.* "ä»ç„¶" (jiÃ©guÄn) means "still" in Chinese.* "low quality" (gÅng yÇn) means "low quality" in Chinese.* "ç…§ç‰‡" (zhÄo pÇn) means "photos" in Chinese.* "é¢å›¾åƒ" (liÇn tÃº xiÃ ng) means "face images" in Chinese.* " recognition" (tiÄn xiÇng) means "recognition" in Chinese.* "å‡†ç¡®ç‡" (zhÃ¨ng qiÃº lÇ) means "accuracy" in Chinese.* "æ•°æ®åº“" (shÃ¹ jÄ«ng kÃ¹) means "database" in Chinese.* "æµ‹è¯•" (cÃ¨ shÃ­) means "testing" in Chinese.* "çŠ¶æ€kemal" (zhuÃ ng tÃ i kÃ¨ mÄ) means "state-of-the-art" in Chinese.* "facial image enhancement" (liÇn tÃ­ng xiÇng yÇng) means "facial image enhancement" in Chinese.* "åè®®" (xiÃ© yÃ¬) means "protocol" in Chinese.* " Validate" (bÃ¨i yÇ) means "to validate" in Chinese.* "å®éªŒåè§" (shÃ­ yÃ n pÄ“n jiÃ n) means "experimental bias" in Chinese.* "æŒ‘æˆ˜æ€§" (tiÇo zhÃ n xÃ¬ng) means "challenging aspects" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Edge-Aware-Mirror-Network-for-Camouflaged-Object-Detection"><a href="#Edge-Aware-Mirror-Network-for-Camouflaged-Object-Detection" class="headerlink" title="Edge-Aware Mirror Network for Camouflaged Object Detection"></a>Edge-Aware Mirror Network for Camouflaged Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03932">http://arxiv.org/abs/2307.03932</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdy1999/eamnet">https://github.com/sdy1999/eamnet</a></li>
<li>paper_authors: Dongyue Sun, Shiyao Jiang, Lin Qi</li>
<li>for: æé«˜éšå½¢ç›®æ ‡æ£€æµ‹ç²¾åº¦</li>
<li>methods: æå‡ºäº†ä¸€ç§Edge-aware Mirror Networkï¼ˆEAMNetï¼‰ï¼Œé€šè¿‡åœ¨æ£€æµ‹å’Œåˆ†å‰²è¿‡ç¨‹ä¸­è¿›è¡Œäº¤å å¼•å¯¼ï¼Œæé«˜äº†ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²ç²¾åº¦</li>
<li>results: å¯¹ä¸‰ä¸ªå¸¸ç”¨çš„éšå½¢ç›®æ ‡æ£€æµ‹æ•°æ®é›†è¿›è¡Œé‡åŒ–å’Œè´¨é‡æµ‹è¯•ï¼Œæ¯”è¾ƒäº†ä¸ç°æœ‰æœ€ä½³åŸºelineçš„æ¯”è¾ƒï¼Œå¾—åˆ°äº†æ›´é«˜çš„ç²¾åº¦Here is the translation in English:</li>
<li>for: Improving the accuracy of camouflaged object detection</li>
<li>methods: Proposed a novel Edge-aware Mirror Network (EAMNet) that models edge detection and camouflaged object segmentation as a cross refinement process, consisting of a segmentation-induced edge aggregation module, an edge-induced integrity aggregation module, and a guided-residual channel attention module.</li>
<li>results: Quantitative and qualitative experiment results on three widely used COD datasets show that EAMNet outperforms existing cutting-edge baselines.Note that the translation is done in a simplified Chinese format, which is a more casual and conversational style of Chinese writing. If you prefer a more formal style, I can also provide that.<details>
<summary>Abstract</summary>
Existing edge-aware camouflaged object detection (COD) methods normally output the edge prediction in the early stage. However, edges are important and fundamental factors in the following segmentation task. Due to the high visual similarity between camouflaged targets and the surroundings, edge prior predicted in early stage usually introduces erroneous foreground-background and contaminates features for segmentation. To tackle this problem, we propose a novel Edge-aware Mirror Network (EAMNet), which models edge detection and camouflaged object segmentation as a cross refinement process. More specifically, EAMNet has a two-branch architecture, where a segmentation-induced edge aggregation module and an edge-induced integrity aggregation module are designed to cross-guide the segmentation branch and edge detection branch. A guided-residual channel attention module which leverages the residual connection and gated convolution finally better extracts structural details from low-level features. Quantitative and qualitative experiment results show that EAMNet outperforms existing cutting-edge baselines on three widely used COD datasets. Codes are available at https://github.com/sdy1999/EAMNet.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°æœ‰çš„éšå½¢ç›®æ ‡æ£€æµ‹ï¼ˆCODï¼‰æ–¹æ³•é€šå¸¸åœ¨æ—©æœŸè¾“å‡ºè¾¹é¢„æµ‹ã€‚ç„¶è€Œï¼Œè¾¹æ˜¯åç»­ segmentation ä»»åŠ¡ä¸­éå¸¸é‡è¦çš„å› ç´ ã€‚ç”±äºéšå½¢ç›®æ ‡å’Œå‘¨å›´ç¯å¢ƒçš„è§†è§‰ç›¸ä¼¼æ€§å¾ˆé«˜ï¼Œæ—©æœŸè¾¹é¢„æµ‹é€šå¸¸ä¼šå¯¼è‡´è¯¯åˆ†åˆ«å‰æ™¯å’ŒèƒŒæ™¯ï¼Œæ±¡æŸ“åˆ†å‰²ç‰¹å¾ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ Edge-aware Mirror Networkï¼ˆEAMNetï¼‰ï¼Œå®ƒå°†è¾¹æ£€æµ‹å’Œéšå½¢ç›®æ ‡åˆ†å‰²è§†ä¸ºäº¤å çš„è¿‡ç¨‹ã€‚æ›´ Specificallyï¼ŒEAMNet å…·æœ‰ä¸¤ææ€§ä½“ç³»ï¼ŒåŒ…æ‹¬ segmentation-induced edge aggregation module å’Œ edge-induced integrity aggregation moduleï¼Œè¿™ä¸¤ä¸ªæ¨¡å—ç”¨äºäº¤å å¯¼èˆªåˆ†å‰²æ”¯è·¯å’Œè¾¹æ£€æµ‹æ”¯è·¯ã€‚æœ€åï¼Œä¸€ä¸ªå—å¼•ç”¨çš„æ®‹å·®æ ¸å¿ƒæ³¨æ„åŠ›æ¨¡å—ï¼Œé€šè¿‡æ®‹å·®è¿æ¥å’Œé˜»æ­¢ convolutionï¼Œç»ˆäºæ›´å¥½åœ°æå–ä½çº§ç‰¹å¾çš„ç»“æ„ç»†èŠ‚ã€‚é‡åŒ–å’Œè´¨é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒEAMNet åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„ COD æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡ç°æœ‰çš„æœ€æ–°åŸºçº¿ã€‚ä»£ç å¯ä»¥åœ¨ https://github.com/sdy1999/EAMNet ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="VS-TransGRU-A-Novel-Transformer-GRU-based-Framework-Enhanced-by-Visual-Semantic-Fusion-for-Egocentric-Action-Anticipation"><a href="#VS-TransGRU-A-Novel-Transformer-GRU-based-Framework-Enhanced-by-Visual-Semantic-Fusion-for-Egocentric-Action-Anticipation" class="headerlink" title="VS-TransGRU: A Novel Transformer-GRU-based Framework Enhanced by Visual-Semantic Fusion for Egocentric Action Anticipation"></a>VS-TransGRU: A Novel Transformer-GRU-based Framework Enhanced by Visual-Semantic Fusion for Egocentric Action Anticipation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03918">http://arxiv.org/abs/2307.03918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Congqi Cao, Ze Sun, Qinyi Lv, Lingtong Min, Yanning Zhang</li>
<li>for: This paper aims to improve the performance of egocentric action anticipation by proposing a novel visual-semantic fusion enhanced and Transformer GRU-based action anticipation framework.</li>
<li>methods: The proposed method introduces high-level semantic information to improve action anticipation performance, uses an effective visual-semantic fusion module, and employs a Transformer based encoder and GRU-based decoder to model long-term sequential and flexible iteration decoding.</li>
<li>results: The proposed method achieves new state-of-the-art performance on two large-scale first-person view datasets, outperforming previous approaches by a large margin.<details>
<summary>Abstract</summary>
Egocentric action anticipation is a challenging task that aims to make advanced predictions of future actions from current and historical observations in the first-person view. Most existing methods focus on improving the model architecture and loss function based on the visual input and recurrent neural network to boost the anticipation performance. However, these methods, which merely consider visual information and rely on a single network architecture, gradually reach a performance plateau. In order to fully understand what has been observed and capture the dependencies between current observations and future actions well enough, we propose a novel visual-semantic fusion enhanced and Transformer GRU-based action anticipation framework in this paper. Firstly, high-level semantic information is introduced to improve the performance of action anticipation for the first time. We propose to use the semantic features generated based on the class labels or directly from the visual observations to augment the original visual features. Secondly, an effective visual-semantic fusion module is proposed to make up for the semantic gap and fully utilize the complementarity of different modalities. Thirdly, to take advantage of both the parallel and autoregressive models, we design a Transformer based encoder for long-term sequential modeling and a GRU-based decoder for flexible iteration decoding. Extensive experiments on two large-scale first-person view datasets, i.e., EPIC-Kitchens and EGTEA Gaze+, validate the effectiveness of our proposed method, which achieves new state-of-the-art performance, outperforming previous approaches by a large margin.
</details>
<details>
<summary>æ‘˜è¦</summary>
å…ˆå‰çš„æ–¹æ³•ä¸»è¦æ˜¯é€šè¿‡æé«˜æ¨¡å‹æ¶æ„å’ŒæŸå¤±å‡½æ•°æ¥æé«˜é¢„æµ‹æ€§èƒ½ï¼Œä¸»è¦åŸºäºè§†è§‰è¾“å…¥å’Œå¾ªç¯ç¥ç»ç½‘ç»œã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾ˆå¿«å°±ä¼šè¾¾åˆ°æ€§èƒ½æé™ã€‚ä¸ºäº†å…¨é¢ç†è§£å·²ç»è§‚å¯Ÿåˆ°çš„å†…å®¹å’Œæ•æ‰æœªæ¥è¡Œä¸ºä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæˆ‘ä»¬åœ¨è¿™ç¯‡è®ºæ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„è§† semanticèåˆå¢å¼ºçš„åŠ¨ä½œé¢„æµ‹æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨åŠ¨ä½œé¢„æµ‹ä¸­ä½¿ç”¨é«˜çº§ semanticä¿¡æ¯ï¼Œä»¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºç±»åˆ«æ ‡ç­¾æˆ–ç›´æ¥ä»è§†è§‰è§‚å¯Ÿåˆ°çš„semanticç‰¹å¾æ¥å¢å¼ºåŸå§‹è§†è§‰ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„è§† semanticèåˆæ¨¡å—ï¼Œä»¥å¼¥è¡¥semanticæ¼æ–—å¹¶å…¨é¢åˆ©ç”¨ä¸åŒæ¨¡å¼ä¹‹é—´çš„å…±è½­æ€§ã€‚æœ€åï¼Œä¸ºäº†åˆ©ç”¨å¹¶è¡Œå’Œè‡ªé€‚åº”æ¨¡å‹ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŸºäºTransformerçš„ç¼–ç å™¨è¿›è¡Œé•¿æœŸåºåˆ—åŒ–å’ŒåŸºäºGRUçš„è§£ç å™¨è¿›è¡Œçµæ´»è¿­ä»£è§£ç ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¤§è§„æ¨¡çš„ç¬¬ä¸€äººè§†è§’æ•°æ®é›†ï¼Œå³EPIC-Kitchenså’ŒEGTEA Gaze+ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æ•ˆivenessï¼ŒæˆåŠŸå‡»è´¥äº†ä¹‹å‰çš„æ–¹æ³•ï¼Œå æ–°çš„çŠ¶æ€ç¬¦æé™ã€‚
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Self-Attack-Defense-and-Spatial-Temporal-Relation-Mining-for-Visible-Infrared-Video-Person-Re-Identification"><a href="#Adversarial-Self-Attack-Defense-and-Spatial-Temporal-Relation-Mining-for-Visible-Infrared-Video-Person-Re-Identification" class="headerlink" title="Adversarial Self-Attack Defense and Spatial-Temporal Relation Mining for Visible-Infrared Video Person Re-Identification"></a>Adversarial Self-Attack Defense and Spatial-Temporal Relation Mining for Visible-Infrared Video Person Re-Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03903">http://arxiv.org/abs/2307.03903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huafeng Li, Le Xu, Yafei Zhang, Dapeng Tao, Zhengtao Yu<br>for:The paper is written for solving the problem of cross-modal pedestrian identity matching in visible-infrared video person re-identification, by proposing a new method that integrates adversarial self-attack defense and spatial-temporal relation mining.methods:The proposed method uses adversarial self-attack to defend against the perturbations caused by changes in views, posture, background, and modal discrepancy, and a spatial-temporal information-guided feature representation network to extract robust features from video sequences.results:The proposed method exhibits compelling performance on large-scale cross-modality video datasets.Here is the Chinese version of the three information:for:è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†è§£å†³è§†é¢‘äººè¯†åˆ«ä¸­çš„è·¨æ¨¡æ€äººè¯†åˆ«é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³åˆ©ç”¨å¯¹æŠ—è‡ªæˆ‘æ”»å‡»é˜²å¾¡å’Œç©ºé—´æ—¶é—´å…³ç³»æŒ–æ˜ã€‚methods:è¯¥æ–¹æ³•ä½¿ç”¨å¯¹æŠ—è‡ªæˆ‘æ”»å‡»æ¥é˜²å¾¡è§†é¢‘ä¸­äººè¯†åˆ«ç‰¹å¾çš„å˜åŒ–ï¼Œå¹¶ä½¿ç”¨ç©ºé—´æ—¶é—´å…³ç³»å¼•å¯¼çš„ç‰¹å¾è¡¨ç¤ºç½‘ç»œæ¥æå–æ›´åŠ ç¨³å®šçš„ç‰¹å¾ã€‚results:è¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡è·¨æ¨¡æ€è§†é¢‘ dataset ä¸Šè¡¨ç°å‡ºäº†å¸å¼•äººçš„è¡¨ç°ã€‚<details>
<summary>Abstract</summary>
In visible-infrared video person re-identification (re-ID), extracting features not affected by complex scenes (such as modality, camera views, pedestrian pose, background, etc.) changes, and mining and utilizing motion information are the keys to solving cross-modal pedestrian identity matching. To this end, the paper proposes a new visible-infrared video person re-ID method from a novel perspective, i.e., adversarial self-attack defense and spatial-temporal relation mining. In this work, the changes of views, posture, background and modal discrepancy are considered as the main factors that cause the perturbations of person identity features. Such interference information contained in the training samples is used as an adversarial perturbation. It performs adversarial attacks on the re-ID model during the training to make the model more robust to these unfavorable factors. The attack from the adversarial perturbation is introduced by activating the interference information contained in the input samples without generating adversarial samples, and it can be thus called adversarial self-attack. This design allows adversarial attack and defense to be integrated into one framework. This paper further proposes a spatial-temporal information-guided feature representation network to use the information in video sequences. The network cannot only extract the information contained in the video-frame sequences but also use the relation of the local information in space to guide the network to extract more robust features. The proposed method exhibits compelling performance on large-scale cross-modality video datasets. The source code of the proposed method will be released at https://github.com/lhf12278/xxx.
</details>
<details>
<summary>æ‘˜è¦</summary>
visible-infrared Ğ²Ğ¸Ğ´ĞµĞ¾äººé‡å¤æ ‡è¯†ï¼ˆre-IDï¼‰ä¸­ï¼Œæå–ä¸å—å¤æ‚åœºæ™¯ï¼ˆå¦‚Modalidadã€æ‘„åƒå¤´è§†å›¾ã€è¡Œäººå§¿æ€ã€èƒŒæ™¯ç­‰ï¼‰å˜åŒ–çš„ç‰¹å¾æ˜¯é”®ï¼Œä»¥å®ç°äº¤å‰æ¨¡å¼äººæ ‡è¯†åŒ¹é…ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯è§infraredè§†é¢‘äººé‡å¤æ ‡è¯†æ–¹æ³•ï¼Œå³åå¯¹æŠ—è‡ªæˆ‘æ”»å‡»å’Œç©ºé—´æ—¶é—´å…³ç³»æŒ–æ˜ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œè§†å›¾ã€å§¿æ€ã€èƒŒæ™¯å’Œæ¨¡å¼å·®å¼‚è¢«è§†ä¸ºäººæ ‡è¯†ç‰¹å¾å˜åŒ–çš„ä¸»è¦å› ç´ ã€‚è¿™äº›å¹²æ‰°ä¿¡æ¯è¢«åŒ…å«åœ¨è®­ç»ƒæ ·æœ¬ä¸­ï¼Œå¹¶ç”¨ä½œå¯¹æŠ—æ”»å‡»ã€‚é€šè¿‡åœ¨è®­ç»ƒä¸­å¼•å…¥è¿™äº›å¹²æ‰°ä¿¡æ¯ï¼Œä½¿æ¨¡å‹æ›´åŠ æŠ—æ€§äºè¿™äº›ä¸åˆ©å› ç´ ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘åºåˆ—çš„ç©ºé—´æ—¶é—´ä¿¡æ¯å¼•å¯¼ç‰¹å¾è¡¨ç¤ºç½‘ç»œï¼Œä»¥ä½¿ç”¨è§†é¢‘åºåˆ—ä¸­çš„ä¿¡æ¯ã€‚è¯¥ç½‘ç»œä¸ä»…å¯ä»¥æå–è§†é¢‘å¸§åºåˆ—ä¸­çš„ä¿¡æ¯ï¼Œè¿˜å¯ä»¥ä½¿ç”¨å½“åœ°ä¿¡æ¯çš„ç©ºé—´å…³ç³»æ¥å¼•å¯¼ç½‘ç»œæå–æ›´åŠ Robustçš„ç‰¹å¾ã€‚æè®®çš„æ–¹æ³•åœ¨å¤§è§„æ¨¡äº¤å‰æ¨¡å¼è§†é¢‘æ•°æ®é›†ä¸Šå±•ç¤ºäº†å¸å¼•äººçš„è¡¨ç°ã€‚æºä»£ç å°†åœ¨https://github.com/lhf12278/xxxä¸Šå‘å¸ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="StyleGAN3-Generative-Networks-for-Improving-the-Equivariance-of-Translation-and-Rotation"><a href="#StyleGAN3-Generative-Networks-for-Improving-the-Equivariance-of-Translation-and-Rotation" class="headerlink" title="StyleGAN3: Generative Networks for Improving the Equivariance of Translation and Rotation"></a>StyleGAN3: Generative Networks for Improving the Equivariance of Translation and Rotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03898">http://arxiv.org/abs/2307.03898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianlei Zhu, Junqi Chen, Renzhe Zhu, Gaurav Gupta</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯å¯¹StyleGANè¿›è¡Œæ”¹è¿›ï¼Œä»¥æé«˜å…¶å¯¹ç­‰å˜æ¢çš„èƒ½åŠ›ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†StyleGAN2å’Œä¸¤ä¸ªä¿®æ”¹åçš„StyleGAN3ç‰ˆæœ¬ï¼Œå¹¶ä½¿ç”¨FFHQ datasetè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼ŒStyleGAN3ç‰ˆæœ¬æ˜¯ä¸€ä¸ªæ›´å¥½çš„ç”Ÿæˆç½‘ç»œï¼Œå¯ä»¥æé«˜ç­‰å˜æ¢æ€§ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºåŠ¨ç”»å’Œè§†é¢‘çš„åˆ›ä½œã€‚Translation:</li>
<li>for: The purpose of this study is to improve the equivariance of StyleGAN.</li>
<li>methods: The study used StyleGAN2 and two modified versions of StyleGAN3, and evaluated them using the FFHQ dataset.</li>
<li>results: The study found that the StyleGAN3 version is a better generative network, with improved equivariance. These findings are beneficial for the creation of animations and videos.<details>
<summary>Abstract</summary>
StyleGAN can use style to affect facial posture and identity features, and noise to affect hair, wrinkles, skin color and other details. Among these, the outcomes of the picture processing will vary slightly between different versions of styleGAN. As a result, the comparison of performance differences between styleGAN2 and the two modified versions of styleGAN3 will be the main focus of this study. We used the FFHQ dataset as the dataset and FID, EQ-T, and EQ-R were used to be the assessment of the model. In the end, we discovered that Stylegan3 version is a better generative network to improve the equivariance. Our findings have a positive impact on the creation of animation and videos.
</details>
<details>
<summary>æ‘˜è¦</summary>
StyleGAN å¯ä»¥é€šè¿‡é£æ ¼æ¥å½±å“è„¸éƒ¨å§¿åŠ¿å’Œä¸ªä½“ç‰¹å¾ï¼Œå¹¶é€šè¿‡å™ªéŸ³æ¥å½±å“å¤´å‘ã€çš±çº¹ã€çš®è‚¤é¢œè‰²ç­‰ç»†èŠ‚ã€‚ Among these, the outcomes of the picture processing will vary slightly between different versions of StyleGAN. As a result, the comparison of performance differences between StyleGAN2 and the two modified versions of StyleGAN3 will be the main focus of this study. We used the FFHQ dataset as the dataset and FID, EQ-T, and EQ-R were used to be the assessment of the model. In the end, we discovered that Stylegan3 version is a better generative network to improve the equivariance. Our findings have a positive impact on the creation of animation and videos.Here's the translation in Traditional Chinese: StyleGAN å¯ä»¥é€è¿‡é£æ ¼æ¥å½±å“è„¸éƒ¨å§¿åŠ¿å’Œä¸ªä½“ç‰¹å¾ï¼Œå¹¶é€šè¿‡å™ªéŸ³æ¥å½±å“å¤´å‘ã€çš±çº¹ã€çš®è‚¤é¢œè‰²ç­‰ç»†èŠ‚ã€‚ Among these, the outcomes of the picture processing will vary slightly between different versions of StyleGAN. As a result, the comparison of performance differences between StyleGAN2 and the two modified versions of StyleGAN3 will be the main focus of this study. We used the FFHQ dataset as the dataset and FID, EQ-T, and EQ-R were used to be the assessment of the model. In the end, we discovered that Stylegan3 version is a better generative network to improve the equivariance. Our findings have a positive impact on the creation of animation and videos.
</details></li>
</ul>
<hr>
<h2 id="HUMS2023-Data-Challenge-Result-Submission"><a href="#HUMS2023-Data-Challenge-Result-Submission" class="headerlink" title="HUMS2023 Data Challenge Result Submission"></a>HUMS2023 Data Challenge Result Submission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03871">http://arxiv.org/abs/2307.03871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhiraj Neupane, Lakpa Dorje Tamang, Ngoc Dung Huynh, Mohamed Reda Bouadjenek, Sunil Aryal</li>
<li>for: è¿™é¡¹ç ”ç©¶çš„ç›®çš„æ˜¯æå‡ºä¸€ç§æ—©æœŸæ£€æµ‹æ–¹æ³•ã€‚</li>
<li>methods: è¿™é¡¹ç ”ç©¶ä½¿ç”¨äº†ç»˜å›¾å’ŒScalogramå›¾åƒåˆ†æï¼Œä»¥åŠè®¡ç®—æ¯ä¸ªä¿¡å·çš„å¹³å‡å€¼ã€æ ‡å‡†å·®ï¼ˆSTDï¼‰å’Œå³°å€¼è‡³å³°å€¼ï¼ˆP2Pï¼‰å€¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ä½¿ç”¨äº† autoregressive integrated moving averageï¼ˆARIMAï¼‰æ–¹æ³•è·Ÿè¸ªè¿›ç¨‹ã€‚</li>
<li>results: ç ”ç©¶å‘ç°äº†ä¸€äº›æœ‰ç”¨çš„ç»“æœï¼ŒåŒ…æ‹¬æ£€æµ‹åˆ°çš„ç‘•ç†ç—‡çŠ¶å’ŒARIMAæ–¹æ³•çš„é¢„æµ‹ç»“æœã€‚<details>
<summary>Abstract</summary>
We implemented a simple method for early detection in this research. The implemented methods are plotting the given mat files and analyzing scalogram images generated by performing Continuous Wavelet Transform (CWT) on the samples. Also, finding the mean, standard deviation (STD), and peak-to-peak (P2P) values from each signal also helped detect faulty signs. We have implemented the autoregressive integrated moving average (ARIMA) method to track the progression.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬åœ¨è¿™é¡¹ç ”ç©¶ä¸­å®ç°äº†ä¸€ç§ç®€å•çš„æ—©æœŸæ£€æµ‹æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨äº†Continuous Wavelet Transform (CWT)æ¥ç”Ÿæˆscalogramå›¾åƒï¼Œå¹¶ä»æ¯ä¸ªä¿¡å·ä¸­è®¡ç®—äº†å¹³å‡å€¼ã€æ ‡å‡†å·®ï¼ˆSTDï¼‰å’Œå³°å€¼è‡³è°·å€¼ï¼ˆP2Pï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨äº†autoregressive integrated moving average (ARIMA)æ–¹æ³•æ¥è·Ÿè¸ªè¿›ç¨‹ã€‚Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Sketch-A-Shape-Zero-Shot-Sketch-to-3D-Shape-Generation"><a href="#Sketch-A-Shape-Zero-Shot-Sketch-to-3D-Shape-Generation" class="headerlink" title="Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation"></a>Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03869">http://arxiv.org/abs/2307.03869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Sanghi, Pradeep Kumar Jayaraman, Arianna Rampini, Joseph Lambourne, Hooman Shayani, Evan Atherton, Saeid Asgari Taghanaki</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨exploring how large pre-trained models can be used to generate 3D shapes from sketches, which has been an open challenge due to limited datasets and varying abstraction levels in the sketches.</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œå³åœ¨è®­ç»ƒæ—¶ä½¿ç”¨å¤§å‹é¢„è®­ç»ƒè§†è§‰æ¨¡å‹çš„ç‰¹å¾æ¥conditioning 3Dç”Ÿæˆæ¨¡å‹ï¼Œä»¥ä¾¿åœ¨æ¨ç†æ—¶ä»ç»˜åˆ¶ generate 3D shapes.</li>
<li>results: æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨å¤§å‹é¢„è®­ç»ƒè§†è§‰æ¨¡å‹çš„ç‰¹å¾å¯ä»¥å…è®¸æˆ‘ä»¬åœ¨æ¨ç†æ—¶ä»ç»˜åˆ¶ generate 3D shapes, regardless of the level of abstraction in the sketches. æˆ‘ä»¬è¿˜å‘ç°è¿™äº›ç‰¹å¾å¯ä»¥è·¨åŸŸä¼ é€’Semanticä¿¡å·ï¼Œä»è€Œå®ç°å¤šä¸ª3D shapesçš„ç”Ÿæˆ per each input sketch.<details>
<summary>Abstract</summary>
Significant progress has recently been made in creative applications of large pre-trained models for downstream tasks in 3D vision, such as text-to-shape generation. This motivates our investigation of how these pre-trained models can be used effectively to generate 3D shapes from sketches, which has largely remained an open challenge due to the limited sketch-shape paired datasets and the varying level of abstraction in the sketches. We discover that conditioning a 3D generative model on the features (obtained from a frozen large pre-trained vision model) of synthetic renderings during training enables us to effectively generate 3D shapes from sketches at inference time. This suggests that the large pre-trained vision model features carry semantic signals that are resilient to domain shifts, i.e., allowing us to use only RGB renderings, but generalizing to sketches at inference time. We conduct a comprehensive set of experiments investigating different design factors and demonstrate the effectiveness of our straightforward approach for generation of multiple 3D shapes per each input sketch regardless of their level of abstraction without requiring any paired datasets during training.
</details>
<details>
<summary>æ‘˜è¦</summary>
Recently, there have been significant advances in using large pre-trained models for downstream tasks in 3D vision, such as text-to-shape generation. This has inspired us to explore how these pre-trained models can be used effectively to generate 3D shapes from sketches, which has been a long-standing challenge due to the limited availability of sketch-shape paired datasets and the varying level of abstraction in the sketches. We discovered that conditioning a 3D generative model on the features (obtained from a frozen large pre-trained vision model) of synthetic renderings during training enables us to effectively generate 3D shapes from sketches at inference time. This suggests that the large pre-trained vision model features carry semantic signals that are robust to domain shifts, i.e., allowing us to use only RGB renderings, but generalizing to sketches at inference time. We conducted a comprehensive set of experiments investigating different design factors and demonstrated the effectiveness of our straightforward approach for generating multiple 3D shapes per each input sketch regardless of their level of abstraction without requiring any paired datasets during training.
</details></li>
</ul>
<hr>
<h2 id="Novel-Categories-Discovery-from-probability-matrix-perspective"><a href="#Novel-Categories-Discovery-from-probability-matrix-perspective" class="headerlink" title="Novel Categories Discovery from probability matrix perspective"></a>Novel Categories Discovery from probability matrix perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03856">http://arxiv.org/abs/2307.03856</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mxahan/nev-ncd">https://github.com/mxahan/nev-ncd</a></li>
<li>paper_authors: Zahid Hasan, Abu Zaher Md Faridee, Masud Ahmed, Sanjay Purushotham, Heesung Kwon, Hyungtae Lee, Nirmalya Roy</li>
<li>for: æœ¬ç ”ç©¶æ˜¯ä¸ºäº†è§£å†³å¼€æ”¾ä¸–ç•Œé—®é¢˜ï¼Œé€šè¿‡ç±» semantics è¿›è¡ŒçŸ¥é“çš„åˆ†ç±»å’Œ clustering novel categoryã€‚</li>
<li>methods: æˆ‘ä»¬ä» novel data æ¦‚ç‡çŸ©é˜µçš„è§’åº¦ investigate NCDï¼Œå¹¶åˆ©ç”¨æä¾›çš„ novel class å¤šå°¼å°”åˆ†å¸ƒï¼ˆ categorical distributionï¼‰çš„è¿æ¥ã€‚æˆ‘ä»¬é¢„æµ‹å¯ä»¥é€šè¿‡å­¦ä¹ å…¶ç±»åˆ†å¸ƒæ¥å®ç°semantic-based novel data clusteringã€‚æˆ‘ä»¬æå‡ºäº†ä¸€äº›æ–°çš„çº¦æŸï¼ŒåŒ…æ‹¬å®ä¾‹çº§åˆ«çš„ä¿¡æ¯çº¦æŸå’Œç¬¬ä¸€ä¸ªç»Ÿè®¡ç‰¹å¾çº¦æŸã€‚</li>
<li>results: æˆ‘ä»¬çš„ç®€å•æ–¹æ³•æˆåŠŸåœ°å®ç°äº†åŸºäºç±» semantics çš„ novel data clusteringï¼Œä½†éœ€è¦æä¾›ç±» semantic similarity  Ğ¼ĞµĞ¶Ğ´Ñƒæ ‡ç­¾æœªæ ‡æ³¨ç±»ã€‚æˆ‘ä»¬åœ¨å›¾åƒå’Œè§†é¢‘æ¨¡å¼ä¸‹ demonstateäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æ¢ç´¢æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å‡å°‘ç ”ç©¶ï¼Œä»¥æä¾›æ›´å¥½çš„ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ Cifar10ã€UCF101 å’Œ MPSC-ARL æ•°æ®é›†ä¸Šå®ç° <del>94%ã€</del>93% å’Œ <del>85% çš„åˆ†ç±»ç²¾åº¦ï¼ŒåŒæ—¶å®ç° ~90%ã€</del>84% å’Œ ~72% çš„ clustering ç²¾åº¦ï¼Œä¸çŠ¶æ€ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹æ™ºèƒ½æ–¹æ³•ç›¸åŒ¹é…ã€‚<details>
<summary>Abstract</summary>
Novel Categories Discovery (NCD) tackles the open-world problem of classifying known and clustering novel categories based on the class semantics using partial class space annotated data. Unlike traditional pseudo-label and retraining, we investigate NCD from the novel data probability matrix perspective. We leverage the connection between NCD novel data sampling with provided novel class Multinoulli (categorical) distribution and hypothesize to implicitly achieve semantic-based novel data clustering by learning their class distribution. We propose novel constraints on first-order (mean) and second-order (covariance) statistics of probability matrix features while applying instance-wise information constraints. In particular, we align the neuron distribution (activation patterns) under a large batch of Monte-Carlo novel data sampling by matching their empirical features mean and covariance with the provided Multinoulli-distribution. Simultaneously, we minimize entropy and enforce prediction consistency for each instance. Our simple approach successfully realizes semantic-based novel data clustering provided the semantic similarity between label-unlabeled classes. We demonstrate the discriminative capacity of our approaches in image and video modalities. Moreover, we perform extensive ablation studies regarding data, networks, and our framework components to provide better insights. Our approach maintains ~94%, ~93%, and ~85%, classification accuracy in labeled data while achieving ~90%, ~84%, and ~72% clustering accuracy for novel categories for Cifar10, UCF101, and MPSC-ARL datasets that matches state-of-the-art approaches without any external clustering.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–°é¢†åŸŸå‘ç°ï¼ˆNCDï¼‰å¤„ç†å¼€æ”¾ä¸–ç•Œä¸­çš„å·²çŸ¥ç±»å’Œæ–°ç±»åˆ†ç±»é—®é¢˜ï¼ŒåŸºäºç±» semantics ä½¿ç”¨ååºæ•°æ®è¿›è¡Œåˆ†ç±»ã€‚ unlike traditional pseudo-label å’Œé‡æ–°è®­ç»ƒï¼Œæˆ‘ä»¬ä»æ–°æ•°æ®æ¦‚ç‡çŸ©é˜µçš„è§†è§’è¿›è¡Œç ”ç©¶ã€‚æˆ‘ä»¬åˆ©ç”¨æ–°æ•°æ®é‡‡æ ·ä¸æä¾›çš„æ–°ç±» Multinoulli åˆ†å¸ƒä¹‹é—´çš„è¿æ¥ï¼Œå¹¶å‡è®¾é€šè¿‡å­¦ä¹ å…¶ç±»åˆ†å¸ƒæ¥éšå¼åœ°å®ç° semantic-based æ–°æ•°æ®å½’ç±»ã€‚æˆ‘ä»¬æå‡ºäº†æ–°çš„ä¸€çº§ï¼ˆå¹³å‡å€¼ï¼‰å’ŒäºŒçº§ï¼ˆåæ–¹å·®ï¼‰ç»Ÿè®¡ç‰¹å¾çš„çº¦æŸï¼Œå¹¶åœ¨å®ä¾‹çº§åˆ«ä¸Šåº”ç”¨æƒ…å†µçº¦æŸã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åœ¨å¤§æ‰¹é‡ Monte-Carlo æ–°æ•°æ®é‡‡æ ·ä¸­å¯¹ neuron åˆ†å¸ƒï¼ˆæ´»åŠ¨æ¨¡å¼ï¼‰è¿›è¡Œå¯¹é½ï¼Œä½¿å…¶ empirical features çš„å¹³å‡å€¼å’Œåæ–¹å·®ä¸æä¾›çš„ Multinoulli-åˆ†å¸ƒåŒ¹é…ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å‡å°‘ entropy å¹¶å¼ºåˆ¶å®ä¾‹çº§åˆ«é¢„æµ‹ä¸€è‡´ã€‚æˆ‘ä»¬ç®€å•çš„æ–¹æ³•æˆåŠŸåœ°å®ç° semantic-based æ–°æ•°æ®å½’ç±»ï¼Œåªè¦æä¾›ç±»ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬åœ¨å›¾åƒå’Œè§†é¢‘æ¨¡å¼ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æ¢ç´¢æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„æ•°æ®ã€ç½‘ç»œå’Œæ¡†æ¶ç»„ä»¶çš„æ‹Ÿåˆç ”ç©¶ï¼Œä»¥æä¾›æ›´å¥½çš„ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ Cifar10ã€UCF101 å’Œ MPSC-ARL æ•°æ®é›†ä¸Šä¿æŒäº† ~94%ã€~93% å’Œ ~85% çš„åˆ†ç±»ç²¾åº¦ï¼ŒåŒæ—¶å®ç°äº† ~90%ã€~84% å’Œ ~72% çš„å½’ç±»ç²¾åº¦ï¼Œä¸çŠ¶æ€è‰ºæŠ€æœ¯ç›¸åŒ¹é…ã€‚
</details></li>
</ul>
<hr>
<h2 id="TBSS-A-novel-computational-method-for-Tract-Based-Spatial-Statistics"><a href="#TBSS-A-novel-computational-method-for-Tract-Based-Spatial-Statistics" class="headerlink" title="TBSS++: A novel computational method for Tract-Based Spatial Statistics"></a>TBSS++: A novel computational method for Tract-Based Spatial Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05387">http://arxiv.org/abs/2307.05387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davood Karimi, Hamza Kebiri, Ali Gholipour</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨æé«˜Diffusion-weightedç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰ä¸­è¯„ä¼°å¤§è„‘ç™½atterçš„ç²¾åº¦å’Œå¯é æ€§ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®¡ç®—æ¡†æ¶ï¼Œé€šè¿‡å‡†ç¡®çš„åˆ†å‰²å’Œæ•°æ®é›†ä¹‹é—´çš„ç²¾ç¡®åŒ¹é…æ¥è¶…è¶Šç°æœ‰æ–¹æ³•çš„ç¼ºé™·å’Œå±€é™æ€§ã€‚</li>
<li>results: å¯¹æ¯”TBSSæ–¹æ³•ï¼Œè¯¥è®ºæ–‡çš„æè®®æ–¹æ³•æ˜¾ç¤ºäº†æ›´é«˜çš„å¤åˆ¶æ€§å’Œå¯¹æ•°æ®å¹²æ‰°çš„Robustnessã€‚<details>
<summary>Abstract</summary>
Diffusion-weighted magnetic resonance imaging (dMRI) is widely used to assess the brain white matter. One of the most common computations in dMRI involves cross-subject tract-specific analysis, whereby dMRI-derived biomarkers are compared between cohorts of subjects. The accuracy and reliability of these studies hinges on the ability to compare precisely the same white matter tracts across subjects. This is an intricate and error-prone computation. Existing computational methods such as Tract-Based Spatial Statistics (TBSS) suffer from a host of shortcomings and limitations that can seriously undermine the validity of the results. We present a new computational framework that overcomes the limitations of existing methods via (i) accurate segmentation of the tracts, and (ii) precise registration of data from different subjects/scans. The registration is based on fiber orientation distributions. To further improve the alignment of cross-subject data, we create detailed atlases of white matter tracts. These atlases serve as an unbiased reference space where the data from all subjects is registered for comparison. Extensive evaluations show that, compared with TBSS, our proposed framework offers significantly higher reproducibility and robustness to data perturbations. Our method promises a drastic improvement in accuracy and reproducibility of cross-subject dMRI studies that are routinely used in neuroscience and medical research.
</details>
<details>
<summary>æ‘˜è¦</summary>
Diffusion-weighted Ğ¼Ğ°Ğ³Ğ½Ğ¸Ñ‚Ğ½Ğ°Ñ Ñ€ĞµĞ·Ğ¾Ğ½Ğ°Ğ½ÑĞ½Ğ°Ñ Ñ‚Ğ¾Ğ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ (dMRI) å¹¿æ³›ç”¨äºè¯„ä¼°å¤§è„‘ç™½ mater. ä¸€ç§æœ€å¸¸è§çš„è®¡ç®—åœ¨ dMRI ä¸­æ˜¯ cross-subject è‚¡é“ç‰¹å¾åˆ†æï¼Œå…¶ä¸­ dMRI è·å¾—çš„ç”Ÿç‰©æ ‡å¿—ç‰©è¢«æ¯”è¾ƒ Ğ¼ĞµĞ¶Ğ´Ñƒ å›¢é˜Ÿçš„Subjects. è¿™äº›ç ”ç©¶çš„å‡†ç¡®æ€§å’Œå¯é æ€§å–å†³äºèƒ½å¤Ÿå‡†ç¡®æ¯”è¾ƒä¸åŒä¸»ä½“/æ‰«ææ•°æ®ä¸­çš„ç™½ materè‚¡é“ã€‚ ç°æœ‰çš„è®¡ç®—æ–¹æ³•ï¼Œå¦‚ Tract-Based Spatial Statistics (TBSS)ï¼Œå—åˆ°å¤šç§ç¼ºé™·å’Œå±€é™æ€§çš„å½±å“ï¼Œå¯èƒ½ä¼šä¸¥é‡æŸå®³ç ”ç©¶ç»“æœçš„æœ‰æ•ˆæ€§ã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è®¡ç®—æ¡†æ¶ï¼Œé€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ–¹æ³•æ¥ç¼“è§£ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼š1. ç²¾å‡†çš„è‚¡é“åˆ†å‰²2. åŸºäºçº¤ç»´æ–¹å‘åˆ†å¸ƒçš„æ•°æ®é‡å¤æ³¨å†Œä¸ºäº†è¿›ä¸€æ­¥æé«˜äº¤ç”±æ•°æ®çš„å¯¹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºäº†è¯¦ç»†çš„ç™½ materè‚¡é“ Ğ°Ñ‚Ğ»Ğ°æ–¯ã€‚ è¿™äº› Ğ°Ñ‚Ğ»Ğ°æ–¯ä½œä¸ºä¸€ç§æ— åå‚ç…§ç©ºé—´ï¼Œç”¨äºæ³¨å†Œæ‰€æœ‰ä¸»ä½“çš„æ•°æ®ï¼Œä»¥ä¾¿å¯¹æ¯”ã€‚ æˆ‘ä»¬çš„æ–¹æ³•ä¸ TBSS ç›¸æ¯”ï¼Œå…·æœ‰æ˜¾è‘—æ›´é«˜çš„é‡å¤æ€§å’Œå¯¹æ•°æ®æ‰°åŠ¨çš„æŠ—éš¾åº¦ã€‚ æˆ‘ä»¬çš„æ–¹æ³•æ‰¿è¯ºå¯ä»¥å¤§å¹…æé«˜äº¤ç”±æ•°æ®çš„ç²¾åº¦å’Œå¯é‡å¤æ€§ï¼Œè¿™äº›ç ”ç©¶åœ¨ neuroscience å’ŒåŒ»å­¦ç ”ç©¶ä¸­ Routinely ä½¿ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Blocks2World-Controlling-Realistic-Scenes-with-Editable-Primitives"><a href="#Blocks2World-Controlling-Realistic-Scenes-with-Editable-Primitives" class="headerlink" title="Blocks2World: Controlling Realistic Scenes with Editable Primitives"></a>Blocks2World: Controlling Realistic Scenes with Editable Primitives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03847">http://arxiv.org/abs/2307.03847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaibhav Vavilala, Seemandhar Jain, Rahul Vasanth, Anand Bhattad, David Forsyth</li>
<li>for: 3D scene rendering and editing</li>
<li>methods: convex decomposition of images and conditioned synthesis</li>
<li>results: highly customizable scene rendering process with remarkable control over the synthesis of novel and edited scenesHereâ€™s the full summary in Simplified Chinese:</li>
<li>for: è¿™paperæ˜¯ä¸ºäº†è§£å†³3Dåœºæ™¯æ¸²æŸ“å’Œç¼–è¾‘é—®é¢˜</li>
<li>methods: ä½¿ç”¨å‡ ä½•åˆ†è§£å’Œå—æ§åˆæˆ</li>
<li>results: æä¾›ä¸€ç§é«˜åº¦è‡ªå®šä¹‰çš„åœºæ™¯æ¸²æŸ“è¿‡ç¨‹ï¼Œå¯ä»¥é«˜åº¦æ§åˆ¶åˆ›å»ºå’Œç¼–è¾‘åœºæ™¯çš„å›¾åƒç”ŸæˆI hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We present Blocks2World, a novel method for 3D scene rendering and editing that leverages a two-step process: convex decomposition of images and conditioned synthesis. Our technique begins by extracting 3D parallelepipeds from various objects in a given scene using convex decomposition, thus obtaining a primitive representation of the scene. These primitives are then utilized to generate paired data through simple ray-traced depth maps. The next stage involves training a conditioned model that learns to generate images from the 2D-rendered convex primitives. This step establishes a direct mapping between the 3D model and its 2D representation, effectively learning the transition from a 3D model to an image. Once the model is fully trained, it offers remarkable control over the synthesis of novel and edited scenes. This is achieved by manipulating the primitives at test time, including translating or adding them, thereby enabling a highly customizable scene rendering process. Our method provides a fresh perspective on 3D scene rendering and editing, offering control and flexibility. It opens up new avenues for research and applications in the field, including authoring and data augmentation.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»äº†Blocks2Worldï¼Œä¸€ç§æ–°çš„3Dåœºæ™¯æ¸²æŸ“å’Œç¼–è¾‘æ–¹æ³•ï¼Œåˆ©ç”¨äº†ä¸¤æ­¥è¿‡ç¨‹ï¼šå‡ ä½•åˆ†è§£å’Œæ¡ä»¶ç”Ÿæˆã€‚æˆ‘ä»¬çš„æŠ€æœ¯é¦–å…ˆä»ç»™å®šåœºæ™¯ä¸­çš„å„ç§ç‰©ä½“ä¸­æå–3DçŸ©å½¢ä½“ä½¿ç”¨å‡ ä½•åˆ†è§£ï¼Œä»è€Œè·å¾—åœºæ™¯çš„åŸå§‹è¡¨ç¤ºã€‚è¿™äº›åŸºæœ¬å¯¹è±¡ç„¶åç”¨ç®€å•çš„æŠ•å½±æ³•ç”Ÿæˆå¯¹åº”çš„æ·±åº¦åœ°å›¾ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™äº›å¯¹åº”çš„æ•°æ®ç”¨æ¡ä»¶å­¦ä¹ æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»¥å­¦ä¹ å°†3Dæ¨¡å‹è½¬æ¢ä¸ºå›¾åƒã€‚è¿™ä¸ªæ­¥éª¤å»ºç«‹äº†3Dæ¨¡å‹å’Œå…¶2Dè¡¨ç¤ºä¹‹é—´çš„ç›´æ¥æ˜ å°„ï¼Œä»è€Œå­¦ä¹ äº†å°†3Dæ¨¡å‹è½¬æ¢ä¸ºå›¾åƒçš„è¿‡ç¨‹ã€‚ä¸€æ—¦æ¨¡å‹å®Œå…¨è®­ç»ƒå®Œæˆï¼Œå®ƒå¯ä»¥åœ¨æµ‹è¯•æ—¶å¯¹åŸºæœ¬å¯¹è±¡è¿›è¡Œ manipulateï¼ŒåŒ…æ‹¬å¹³ç§»æˆ–æ·»åŠ ï¼Œä»¥æ­¤è·å¾—é«˜åº¦è‡ªå®šä¹‰çš„åœºæ™¯æ¸²æŸ“è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸º3Dåœºæ™¯æ¸²æŸ“å’Œç¼–è¾‘å¸¦æ¥äº†æ–°çš„è§†è§’ï¼Œæä¾›äº†æ§åˆ¶å’Œçµæ´»æ€§ã€‚å®ƒæ‰“å¼€äº†æ–°çš„ç ”ç©¶å’Œåº”ç”¨é¢†åŸŸï¼ŒåŒ…æ‹¬ä½œè€…å’Œæ•°æ®å¢å¼ºã€‚
</details></li>
</ul>
<hr>
<h2 id="Invariant-Scattering-Transform-for-Medical-Imaging"><a href="#Invariant-Scattering-Transform-for-Medical-Imaging" class="headerlink" title="Invariant Scattering Transform for Medical Imaging"></a>Invariant Scattering Transform for Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04771">http://arxiv.org/abs/2307.04771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nafisa Labiba Ishrat Huda, Angona Biswas, MD Abdullah Al Nasim, Md. Fahim Rahman, Shoaib Ahmed</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦ç ”ç©¶çš„æ˜¯ç”¨æ·±åº¦å­¦ä¹ å¯¹åŒ»ç–—å›¾åƒè¿›è¡Œåˆ†ç±»çš„æ–¹æ³•ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†æ•£å°„å˜æ¢ï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºå¹²æ‰°çš„ä¿¡å·å¤„ç†æŠ€æœ¯ï¼Œå¯ä»¥å¸®åŠ©å»ºç«‹æœ‰ç”¨çš„å›¾åƒåˆ†ç±»è¡¨ç¤ºã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨æ•£å°„å˜æ¢å¯ä»¥æé«˜åŒ»ç–—å›¾åƒåˆ†ç±»çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚Hereâ€™s the full text in Simplified Chinese:</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦ç ”ç©¶çš„æ˜¯ç”¨æ·±åº¦å­¦ä¹ å¯¹åŒ»ç–—å›¾åƒè¿›è¡Œåˆ†ç±»çš„æ–¹æ³•ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†æ•£å°„å˜æ¢ï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºå¹²æ‰°çš„ä¿¡å·å¤„ç†æŠ€æœ¯ï¼Œå¯ä»¥å¸®åŠ©å»ºç«‹æœ‰ç”¨çš„å›¾åƒåˆ†ç±»è¡¨ç¤ºã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨æ•£å°„å˜æ¢å¯ä»¥æé«˜åŒ»ç–—å›¾åƒåˆ†ç±»çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚<details>
<summary>Abstract</summary>
Invariant scattering transform introduces new area of research that merges the signal processing with deep learning for computer vision. Nowadays, Deep Learning algorithms are able to solve a variety of problems in medical sector. Medical images are used to detect diseases brain cancer or tumor, Alzheimer's disease, breast cancer, Parkinson's disease and many others. During pandemic back in 2020, machine learning and deep learning has played a critical role to detect COVID-19 which included mutation analysis, prediction, diagnosis and decision making. Medical images like X-ray, MRI known as magnetic resonance imaging, CT scans are used for detecting diseases. There is another method in deep learning for medical imaging which is scattering transform. It builds useful signal representation for image classification. It is a wavelet technique; which is impactful for medical image classification problems. This research article discusses scattering transform as the efficient system for medical image analysis where it's figured by scattering the signal information implemented in a deep convolutional network. A step by step case study is manifested at this research work.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œæ’å¸¸æ•£å°„å˜æ¢å¼•å…¥äº†ä¸€æ–°çš„ç ”ç©¶é¢†åŸŸï¼Œå³å°†ä¿¡å·å¤„ç†ä¸æ·±åº¦å­¦ä¹ ç»“åˆç”¨äºè®¡ç®—æœºè§†è§‰ã€‚ç›®å‰ï¼Œæ·±åº¦å­¦ä¹ ç®—æ³•èƒ½å¤Ÿè§£å†³åŒ»ç–—é¢†åŸŸå¤šç§é—®é¢˜ã€‚åŒ»ç–—å›¾åƒç”¨äºæ£€æµ‹è„‘ç™Œæˆ–è‚¿ç˜¤ã€é˜¿å°”èŒ¨æ›¼ç—…ã€ä¹³ç™Œã€ Parkinson ç—…ç­‰ã€‚åœ¨2020å¹´å¤§æµè¡ŒæœŸé—´ï¼Œæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ‰®æ¼”äº†å…³é”®çš„è§’è‰²ï¼Œç”¨äºæ£€æµ‹ COVID-19ï¼ŒåŒ…æ‹¬å˜å¼‚åˆ†æã€é¢„æµ‹ã€è¯Šæ–­å’Œå†³ç­–ã€‚åŒ»ç–—å›¾åƒå¦‚ X å°„ã€MRIï¼ˆç£å…±æŒ¯æˆåƒï¼‰ã€CTæ‰«ææ˜¯ç”¨äºæ£€æµ‹ç–¾ç—…çš„ã€‚å¦ä¸€ç§åœ¨æ·±åº¦å­¦ä¹ ä¸­ç”¨äºåŒ»ç–—å›¾åƒåˆ†ç±»çš„æ–¹æ³•æ˜¯æ•£å°„å˜æ¢ã€‚å®ƒå»ºç«‹äº†æœ‰ç”¨çš„ä¿¡å·è¡¨ç¤ºï¼Œç”¨äºå›¾åƒåˆ†ç±»é—®é¢˜ã€‚è¿™ç¯‡ç ”ç©¶æ–‡ç« ä»‹ç»äº†æ•£å°„å˜æ¢ä½œä¸ºåŒ»ç–—å›¾åƒåˆ†æä¸­æ•ˆæœçš„ç³»ç»Ÿï¼Œå…¶ä¸­å°†ä¿¡å·ä¿¡æ¯åœ¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œä¸­æ•£å°„ã€‚æœ¬ç ”ç©¶æ–‡ç« é€šè¿‡ä¸€ä¸ªæ­¥éª¤æ¡ˆä¾‹ç ”ç©¶ã€‚â€Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Thoracic-Cartilage-Ultrasound-CT-Registration-using-Dense-Skeleton-Graph"><a href="#Thoracic-Cartilage-Ultrasound-CT-Registration-using-Dense-Skeleton-Graph" class="headerlink" title="Thoracic Cartilage Ultrasound-CT Registration using Dense Skeleton Graph"></a>Thoracic Cartilage Ultrasound-CT Registration using Dense Skeleton Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03800">http://arxiv.org/abs/2307.03800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongliang Jiang, Chenyang Li, Xuesong Li, Nassir Navab</li>
<li>for: æé«˜è‡ªåŠ¨ultrasoundï¼ˆUSï¼‰æˆåƒçš„ç²¾åº¦å’Œæ•ˆç‡ï¼Œå°¤å…¶æ˜¯åœ¨éª¨éª¼ç»“æ„ä¸‹æ–¹çš„é«˜é˜»æŒ¡æ€§è‚ºéƒ¨åº”ç”¨ä¸­ã€‚</li>
<li>methods: ä½¿ç”¨å›¾å½¢åŸºäºéRIGIDæ³¨å†Œæ–¹æ³•ï¼Œè€ƒè™‘åˆ°éª¨éª¼è¡¨é¢ç‰¹å¾ï¼Œä»CTæ¨¡æ¿ä¸­æå–æœ€ä½³å›¾å½¢è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨è‡ªç»„ç»‡åœ°å›¾è¿›è¡Œä¸¤æ¬¡Successive Registrationã€‚</li>
<li>results: å¯¹äº”ä¸ªä¸åŒæ‚£è€…çš„è½¯éª¨ç‚¹äº‘è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œææ¡ˆçš„å›¾å½¢åŸºäºæ³¨å†Œæ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†CTä¸­çš„è½¨è¿¹æ˜ å°„åˆ°å½“å‰è®¾ç½®ä¸­ï¼Œå¹¶ä¸”éRIGIDæ³¨å†Œç»“æœä¸­çš„ Hausdorff è·ç¦»ï¼ˆMean$\pm$SDï¼‰ä¸º9.48$\pm$0.27 mmï¼Œè·¯å¾„ä¼ è¾“é”™è¯¯ï¼ˆEuclidean distanceï¼‰ä¸º2.21$\pm$1.11 mmã€‚<details>
<summary>Abstract</summary>
Autonomous ultrasound (US) imaging has gained increased interest recently, and it has been seen as a potential solution to overcome the limitations of free-hand US examinations, such as inter-operator variations. However, it is still challenging to accurately map planned paths from a generic atlas to individual patients, particularly for thoracic applications with high acoustic-impedance bone structures under the skin. To address this challenge, a graph-based non-rigid registration is proposed to enable transferring planned paths from the atlas to the current setup by explicitly considering subcutaneous bone surface features instead of the skin surface. To this end, the sternum and cartilage branches are segmented using a template matching to assist coarse alignment of US and CT point clouds. Afterward, a directed graph is generated based on the CT template. Then, the self-organizing map using geographical distance is successively performed twice to extract the optimal graph representations for CT and US point clouds, individually. To evaluate the proposed approach, five cartilage point clouds from distinct patients are employed. The results demonstrate that the proposed graph-based registration can effectively map trajectories from CT to the current setup for displaying US views through limited intercostal space. The non-rigid registration results in terms of Hausdorff distance (Mean$\pm$SD) is 9.48$\pm$0.27 mm and the path transferring error in terms of Euclidean distance is 2.21$\pm$1.11 mm.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªä¸»å¼è¶…å£°æˆåƒï¼ˆUSï¼‰å·²ç»åœ¨æœ€è¿‘å¾—åˆ°äº†æ›´å¤šçš„å…³æ³¨ï¼Œè¢«è§†ä¸ºå¯ä»¥è§£å†³è‡ªç”±æ‰‹è¶…å£°æ£€æµ‹ä¸­çš„æ“ä½œå‘˜é—´å˜åŒ–çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œå°†è§„åˆ’çš„è·¯å¾„ä»é€šç”¨Atlasåˆ°ä¸ªä½“æ‚£è€…ä¸­çš„ç²¾å‡†æ˜ å°„ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸€ç§åŸºäºå›¾çš„éRIGIDæ³¨å†Œæ–¹æ³•è¢«æè®®ï¼Œä»¥ä¾¿å°†è§„åˆ’çš„è·¯å¾„ä»Atlasä¼ é€’åˆ°å½“å‰è®¾ç½®ï¼Œå¹¶ä¸”ç‰¹åˆ«è€ƒè™‘ä¸‹çš®éª¨è¡¨é¢ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œä½¿ç”¨æ¨¡æ¿åŒ¹é…å°†æ°”è‚ å’Œè½¯éª¨åˆ†æ”¯åˆ†åˆ«åˆ†å‰²å‡ºæ¥ã€‚ç„¶åï¼ŒåŸºäºCTæ¨¡æ¿ç”Ÿæˆäº†æŒ‡å‘å›¾ã€‚æ¥ç€ï¼Œåœ¨CTå’ŒUSç‚¹äº‘ä¸Šè¿›è¡Œäº†é¡ºåºçš„è‡ªç»„ç»‡åœ°å›¾ä½¿ç”¨åœ°ç† distanceæ¥æŠ½å–æœ€ä½³è¡¨ç¤ºã€‚ä¸ºè¯„ä¼°æè®®æ–¹æ³•ï¼Œä½¿ç”¨äº†äº”ä¸ªä¸åŒæ‚£è€…çš„è½¯éª¨ç‚¹äº‘ã€‚ç»“æœè¡¨æ˜ï¼Œæè®®çš„å›¾åŸºäºæ³¨å†Œå¯ä»¥æœ‰æ•ˆåœ°å°†CTä¸­çš„è·¯å¾„æ˜ å°„åˆ°å½“å‰è®¾ç½®ä¸­ï¼Œå¹¶ä¸”éRIGIDæ³¨å†Œçš„ Hausdorffè·ç¦»ï¼ˆMean$\pm$SDï¼‰ä¸º9.48$\pm$0.27 mmï¼Œè·¯å¾„ä¼ è¾“é”™è¯¯ï¼ˆEuclidean distanceï¼‰ä¸º2.21$\pm$1.11 mmã€‚
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Lottery-Ticket-Hypothesis-with-Explainability-Methods-Insights-into-Sparse-Network-Performance"><a href="#Exploring-the-Lottery-Ticket-Hypothesis-with-Explainability-Methods-Insights-into-Sparse-Network-Performance" class="headerlink" title="Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance"></a>Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13698">http://arxiv.org/abs/2307.13698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shantanu Ghosh, Kayhan Batmanghelich</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨æ‰¾å‡ºä¸€ä¸ªé«˜æ€§èƒ½çš„ç¨€ç–ç½‘ç»œï¼Œä»¥ä¾¿åœ¨æœ‰é™å­˜å‚¨çš„è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œå¦‚ç§»åŠ¨ç”µè¯ã€‚åŒæ—¶ï¼ŒAIçš„å¯è§£é‡Šæ€§æ˜¯éå¸¸é‡è¦çš„ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº† Lottery Ticket Hypothesisï¼ˆLTHï¼‰æ¥æ‰¾åˆ°ä¸€ä¸ªæ·±åº¦ç½‘ç»œä¸­çš„ä¸€ä¸ªé«˜æ€§èƒ½çš„å­ç½‘ç»œã€‚ä½†æ˜¯ï¼Œæœ‰é™çš„ç ”ç©¶å·²ç»å‘ç°äº†LTHåœ¨å¯è§£é‡Šæ€§æ–¹é¢çš„æˆåŠŸæˆ–å¤±è´¥ã€‚è¿™ä¸ªè®ºæ–‡ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°äº†å‰ªè¾‘åç½‘ç»œçš„æ€§èƒ½æ˜¯å¦‚ä½•é€æ¸å¢åŠ æˆ–å‡å°‘çš„åŸå› ã€‚ä½¿ç”¨Grad-CAMå’ŒPost-hocæ¦‚å¿µç“¶éš”ï¼ˆPCBMï¼‰æ¥è°ƒæŸ¥å‰ªè¾‘åç½‘ç»œçš„å¯è§£é‡Šæ€§ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œéšç€å‰ªè¾‘æ›´å¤šçš„å‚æ•°ï¼Œç½‘ç»œçš„æ€§èƒ½é€æ¸ä¸‹é™ã€‚å‘ç°çš„æ¦‚å¿µå’Œåƒç´ ä»å‰ªè¾‘åçš„ç½‘ç»œä¸åŸå§‹ç½‘ç»œä¸åŒ¹é…ï¼Œå¯èƒ½æ˜¯æ€§èƒ½ä¸‹é™çš„åŸå› ã€‚<details>
<summary>Abstract</summary>
Discovering a high-performing sparse network within a massive neural network is advantageous for deploying them on devices with limited storage, such as mobile phones. Additionally, model explainability is essential to fostering trust in AI. The Lottery Ticket Hypothesis (LTH) finds a network within a deep network with comparable or superior performance to the original model. However, limited study has been conducted on the success or failure of LTH in terms of explainability. In this work, we examine why the performance of the pruned networks gradually increases or decreases. Using Grad-CAM and Post-hoc concept bottleneck models (PCBMs), respectively, we investigate the explainability of pruned networks in terms of pixels and high-level concepts. We perform extensive experiments across vision and medical imaging datasets. As more weights are pruned, the performance of the network degrades. The discovered concepts and pixels from the pruned networks are inconsistent with the original network -- a possible reason for the drop in performance.
</details>
<details>
<summary>æ‘˜è¦</summary>
å‘ç°ä¸€ä¸ªé«˜æ€§èƒ½ç¨€ç•´ç½‘ç»œåœ¨å¤§è§„æ¨¡ç¥ç»ç½‘ç»œä¸­æ˜¯æœ‰åˆ©äºåœ¨å…·æœ‰é™åˆ¶å­˜å‚¨çš„è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œå¦‚ç§»åŠ¨ç”µè¯ã€‚æ­¤å¤–ï¼ŒAIçš„å¯è§£é‡Šæ€§æ˜¯æé«˜äººå·¥æ™ºèƒ½çš„ä¿¡ä»»çš„å…³é”®ã€‚æŠ½å¥–å‡è®¾ï¼ˆLTHï¼‰æ‰¾åˆ°ä¸€ä¸ªåœ¨æ·±åº¦ç½‘ç»œä¸­çš„ç½‘ç»œï¼Œå…¶æ€§èƒ½ä¸åŸå§‹æ¨¡å‹ç›¸å½“æˆ–æ›´é«˜ã€‚ç„¶è€Œï¼Œæœ‰é™çš„ç ”ç©¶åœ¨LTHçš„æˆåŠŸæˆ–å¤±è´¥æ–¹é¢è¿›è¡Œäº†è§£é‡Šæ€§çš„ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å‰ªé™¤ç½‘ç»œæ€§èƒ½çš„å¢åŠ æˆ–å‡å°‘åŸå› ã€‚ä½¿ç”¨Grad-CAMå’Œåç½®æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆPCBMï¼‰ï¼Œæˆ‘ä»¬ç ”ç©¶å‰ªé™¤ç½‘ç»œçš„å¯è§£é‡Šæ€§ï¼Œå³åƒç´ å’Œé«˜çº§æ¦‚å¿µã€‚æˆ‘ä»¬åœ¨è§†è§‰å’ŒåŒ»å­¦å½±åƒ dataset ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚éšç€æ›´å¤šçš„æƒé‡è¢«å‰ªé™¤ï¼Œç½‘ç»œçš„æ€§èƒ½ä¸‹é™ã€‚å‘ç°çš„æ¦‚å¿µå’Œåƒç´ ä»å‰ªé™¤ç½‘ç»œä¸åŸå§‹ç½‘ç»œä¸åŒ¹é…ï¼Œå¯èƒ½æ˜¯æ€§èƒ½ä¸‹é™çš„åŸå› ã€‚
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Forestry-Images-Conditioned-on-Plant-Phenotype-Using-a-Generative-Adversarial-Network"><a href="#Synthesizing-Forestry-Images-Conditioned-on-Plant-Phenotype-Using-a-Generative-Adversarial-Network" class="headerlink" title="Synthesizing Forestry Images Conditioned on Plant Phenotype Using a Generative Adversarial Network"></a>Synthesizing Forestry Images Conditioned on Plant Phenotype Using a Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03789">http://arxiv.org/abs/2307.03789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debasmita Pal, Arun Ross</li>
<li>for: è¿™ç§ç ”ç©¶çš„ç›®çš„æ˜¯å¼€å‘ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆç¬¦åˆæŸç‰¹å®šåœ°åŒºæ¤è¢«ç‰¹å¾çš„äººå·¥æ£®æ—å›¾åƒï¼Œä»¥æé«˜å†œä¸šç”Ÿäº§åŠ›ã€‚</li>
<li>methods: è¿™ç§æ–¹æ³•ä½¿ç”¨äº†è‡ªåŠ¨åŒ–çš„æ•°å­—ç›¸æœºå›¾åƒï¼Œæä¾›ç”±å›½å®¶ç”Ÿæ€è§‚æµ‹ç½‘ç»œï¼ˆNEONï¼‰ï¼Œå¹¶ç”±phenocamç½‘ç»œå¤„ç†ã€‚å®ƒè¿˜ä½¿ç”¨äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¥ç”Ÿæˆç¬¦åˆæ¤è¢«ç‰¹å¾çš„äººå·¥å›¾åƒã€‚</li>
<li>results: è¿™ç§æ–¹æ³•å¯ä»¥å‡†ç¡®åœ°ç”Ÿæˆç¬¦åˆæ¤è¢«ç‰¹å¾çš„äººå·¥å›¾åƒï¼Œå¹¶ä¸”å¯ä»¥ç”¨æ¥é¢„æµ‹å¦ä¸€ç§æ¤è¢«ç‰¹å¾ï¼šæ¤ç‰©çš„çº¢è‰²åº¦ã€‚è¿™ç§æ–¹æ³•çš„å¯é‡å¤æ€§å’Œæ‰©å±•æ€§ä¹Ÿè¢«è¯æ˜ã€‚<details>
<summary>Abstract</summary>
Plant phenology and phenotype prediction using remote sensing data is increasingly gaining the attention of the plant science community to improve agricultural productivity. In this work, we generate synthetic forestry images that satisfy certain phenotypic attributes, viz. canopy greenness. The greenness index of plants describes a particular vegetation type in a mixed forest. Our objective is to develop a Generative Adversarial Network (GAN) to synthesize forestry images conditioned on this continuous attribute, i.e., greenness of vegetation, over a specific region of interest. The training data is based on the automated digital camera imagery provided by the National Ecological Observatory Network (NEON) and processed by the PhenoCam Network. The synthetic images generated by our method are also used to predict another phenotypic attribute, viz., redness of plants. The Structural SIMilarity (SSIM) index is utilized to assess the quality of the synthetic images. The greenness and redness indices of the generated synthetic images are compared against that of the original images using Root Mean Squared Error (RMSE) in order to evaluate their accuracy and integrity. Moreover, the generalizability and scalability of our proposed GAN model is determined by effectively transforming it to generate synthetic images for other forest sites and vegetation types.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ¤ç‰©ç”Ÿç†å­¦å’Œå½¢æ€é¢„æµ‹ä½¿ç”¨è¿œç¨‹æ„ŸçŸ¥æ•°æ®åœ¨å†œä¸šç”Ÿäº§åŠ›æé«˜æ–¹é¢å¾—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ç¬¦åˆæŸäº›å½¢æ€ç‰¹æ€§çš„äººå·¥æ£®æ—å›¾åƒï¼Œå…¶ä¸­ä¸€ä¸ªæ˜¯å¶ç»¿åº¦ã€‚å¶ç»¿åº¦æŒ‡çš„æ˜¯æŸç§æ··åˆæ—ä¸­çš„æ¤ç‰©ç§ç±»ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿ç”¨ç”Ÿæˆ adversarial Network (GAN) æ¥ç”ŸæˆåŸºäºè¿™ä¸ªè¿ç»­ç‰¹å¾ï¼ˆæ¤ç‰©è¦†ç›–ç‰©çš„ç»¿åº¦ï¼‰çš„æ£®æ—å›¾åƒï¼Œåœ¨ç‰¹å®šåŒºåŸŸä¸Šè¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®æ¥è‡ªè‡ªåŠ¨åŒ–çš„æ•°å­—ç›¸æœºå›¾åƒï¼Œç”±å›½å®¶ç”Ÿæ€è§‚æµ‹ç½‘ç»œï¼ˆNEONï¼‰æä¾›ï¼Œå¹¶ç”±phenoCamç½‘ç»œå¤„ç†ã€‚æˆ‘ä»¬çš„ç”Ÿæˆçš„äººå·¥å›¾åƒä¹Ÿç”¨äºé¢„æµ‹å¦ä¸€ä¸ªå½¢æ€ç‰¹æ€§ï¼šæ¤ç‰©çš„çº¢åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰æŒ‡æ•°æ¥è¯„ä¼°ç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬æ¯”è¾ƒç”Ÿæˆçš„ç»¿åº¦å’Œçº¢åº¦æŒ‡æ•°ä¸åŸå§‹å›¾åƒçš„Root Mean Squared Errorï¼ˆRMSEï¼‰æ¥è¯„ä¼°å®ƒä»¬çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç¡®å®šäº†æˆ‘ä»¬æè®®çš„GANæ¨¡å‹çš„æ™®é€‚æ€§å’Œæ‰©å±•æ€§ï¼Œé€šè¿‡å°†å…¶è½¬æ¢ä¸ºç”Ÿæˆå…¶ä»–æ£®æ—ç«™ç‚¹å’Œæ¤ç‰©ç±»å‹çš„ synthetic å›¾åƒã€‚
</details></li>
</ul>
<hr>
<h2 id="Context-aware-Pedestrian-Trajectory-Prediction-with-Multimodal-Transformer"><a href="#Context-aware-Pedestrian-Trajectory-Prediction-with-Multimodal-Transformer" class="headerlink" title="Context-aware Pedestrian Trajectory Prediction with Multimodal Transformer"></a>Context-aware Pedestrian Trajectory Prediction with Multimodal Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03786">http://arxiv.org/abs/2307.03786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haleh Damirchi, Michael Greenspan, Ali Etemad</li>
<li>for: é¢„æµ‹æœªæ¥è¡Œäººè½¨è¿¹</li>
<li>methods: ä½¿ç”¨å¤šModalEncoder-Decoder transformeræ¶æ„ï¼Œè¾“å…¥åŒ…æ‹¬è¡Œäººä½ç½®å’Œegoæ±½è½¦é€Ÿåº¦ï¼Œå•passé¢„æµ‹æ•´ä¸ªæœªæ¥è½¨è¿¹ï¼Œé€‚ç”¨äºåµŒå…¥å¼è¾¹ç¼˜éƒ¨ç½²</li>
<li>results: ä¸å½“å‰çŠ¶æ€è‰ºæœ¯æ¯”è¾ƒï¼Œå¸¸é‡é”™è¯¯ä½äº0.5ã€1.0å’Œ1.5ç§’ä¸‰ä¸ªæ—¶åˆ»ç‚¹ï¼Œå¹¶ä¸”æ¯”å½“å‰çŠ¶æ€è‰ºæœ¯æ›´å¿«äºPIEå’ŒJAADä¸¤ä¸ªæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œçµæ´»çš„å¤šModalé…ç½®å¯¹æ–¹æ³•çš„å½±å“ä¹Ÿè¿›è¡Œäº†è¯æ˜ã€‚<details>
<summary>Abstract</summary>
We propose a novel solution for predicting future trajectories of pedestrians. Our method uses a multimodal encoder-decoder transformer architecture, which takes as input both pedestrian locations and ego-vehicle speeds. Notably, our decoder predicts the entire future trajectory in a single-pass and does not perform one-step-ahead prediction, which makes the method effective for embedded edge deployment. We perform detailed experiments and evaluate our method on two popular datasets, PIE and JAAD. Quantitative results demonstrate the superiority of our proposed model over the current state-of-the-art, which consistently achieves the lowest error for 3 time horizons of 0.5, 1.0 and 1.5 seconds. Moreover, the proposed method is significantly faster than the state-of-the-art for the two datasets of PIE and JAAD. Lastly, ablation experiments demonstrate the impact of the key multimodal configuration of our method.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºé¢„æµ‹è¡Œäººæœªæ¥è·¯å¾„ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä¸€ç§å¤šModalEncoder-Decoderå˜æ¢æ¶æ„ï¼Œè¯¥æ¶æ„æ¥å—è¡Œäººä½ç½®å’Œè‡ªèº«è½¦é€Ÿåº¦ä½œä¸ºè¾“å…¥ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è§£ç å™¨åœ¨å•æ¬¡æ‰§è¡Œä¸­é¢„æµ‹æ•´ä¸ªæœªæ¥è·¯å¾„ï¼Œè€Œä¸æ˜¯ä¸€æ­¥é¢„æµ‹ï¼Œè¿™ä½¿å¾—æ–¹æ³•é€‚åˆåµŒå…¥å¼è¾¹ç¼˜éƒ¨ç½²ã€‚æˆ‘ä»¬è¿›è¡Œäº†è¯¦ç»†çš„å®éªŒå’ŒPIEå’ŒJAADä¸¤ä¸ªæµè¡Œçš„æ•°æ®é›†ä¸Šçš„è¯„ä¼°ã€‚é‡åŒ–ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æå‡ºçš„æ¨¡å‹åœ¨0.5ã€1.0å’Œ1.5ç§’ä¸‰ä¸ªæ—¶åˆ»çš„é”™è¯¯ç‡å§‹ç»ˆä¿æŒæœ€ä½ï¼Œå¹¶ä¸”ä¸ç°æœ‰çŠ¶æ€çš„è‰ºæœ¯ consistently outperformã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨PIEå’ŒJAADä¸¤ä¸ªæ•°æ®é›†ä¸Šæ˜æ˜¾æ›´å¿«äºç°æœ‰çŠ¶æ€ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…³é”®å¤šæ¨¡å¼é…ç½®çš„ablationå®éªŒï¼Œä»¥è¯„ä¼°å…¶å½±å“ã€‚
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-3D-out-of-distribution-detection-with-latent-diffusion-models"><a href="#Unsupervised-3D-out-of-distribution-detection-with-latent-diffusion-models" class="headerlink" title="Unsupervised 3D out-of-distribution detection with latent diffusion models"></a>Unsupervised 3D out-of-distribution detection with latent diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03777">http://arxiv.org/abs/2307.03777</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marksgraham/ddpm-ood">https://github.com/marksgraham/ddpm-ood</a></li>
<li>paper_authors: Mark S. Graham, Walter Hugo Lopez Pinaya, Paul Wright, Petru-Daniel Tudosiu, Yee H. Mah, James T. Teo, H. Rolf JÃ¤ger, David Werring, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso</li>
<li>For: The paper is written for detecting out-of-distribution (OOD) data in 3D medical data using Latent Diffusion Models (LDMs).* Methods: The paper proposes using LDMs to scale denoising diffusion probabilistic models (DDPMs) to high-resolution 3D medical data, and compares the proposed approach to a recently proposed, 3D-enabled approach using Latent Transformer Models (LTMs).* Results: The proposed LDM-based approach achieves statistically significant better performance than the LTM-based approach, with less sensitivity to the underlying latent representation, more favourable memory scaling, and produces better spatial anomaly maps.Hereâ€™s the simplified Chinese text for the three key points:* ä¸ºï¼šè¯¥æ–‡ç« æ˜¯ç”¨Latent Diffusion Modelsï¼ˆLDMsï¼‰æ£€æµ‹ä¸‰ç»´åŒ»ç–—æ•°æ®ä¸­çš„å¤–å›´æ•°æ®ï¼ˆOut-of-distributionï¼ŒOODï¼‰ã€‚* æ–¹æ³•ï¼šæ–‡ç« æå‡ºä½¿ç”¨LDMså°†denoising diffusion probabilistic modelsï¼ˆDDPMsï¼‰æ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡ä¸‰ç»´åŒ»ç–—æ•°æ®ï¼Œå¹¶ä¸è¿‘æœŸæå‡ºçš„ä½¿ç”¨Latent Transformer Modelsï¼ˆLTMsï¼‰çš„3Då¯ç”¨æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚* ç»“æœï¼šæå‡ºçš„LDM-basedæ–¹æ³•ä¸LTM-basedæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œæ˜¾ç¤ºLDM-basedæ–¹æ³•å…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œå…·æœ‰æ›´å¥½çš„å‡†ç¡®ç‡ã€æ›´å¥½çš„åµŒå…¥ç‰¹å¾ã€æ›´å¥½çš„ç©ºé—´å¼‚å¸¸æ˜ å°„ã€‚<details>
<summary>Abstract</summary>
Methods for out-of-distribution (OOD) detection that scale to 3D data are crucial components of any real-world clinical deep learning system. Classic denoising diffusion probabilistic models (DDPMs) have been recently proposed as a robust way to perform reconstruction-based OOD detection on 2D datasets, but do not trivially scale to 3D data. In this work, we propose to use Latent Diffusion Models (LDMs), which enable the scaling of DDPMs to high-resolution 3D medical data. We validate the proposed approach on near- and far-OOD datasets and compare it to a recently proposed, 3D-enabled approach using Latent Transformer Models (LTMs). Not only does the proposed LDM-based approach achieve statistically significant better performance, it also shows less sensitivity to the underlying latent representation, more favourable memory scaling, and produces better spatial anomaly maps. Code is available at https://github.com/marksgraham/ddpm-ood
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>å°†æ–‡æœ¬ç¿»è¯‘æˆç®€åŒ–ä¸­æ–‡ã€‚<</SYS>>åœ¨çœŸå®ä¸–ç•Œä¸´åºŠæ·±åº¦å­¦ä¹ ç³»ç»Ÿä¸­ï¼Œå¯¹äºä¸åŒæ•°æ®é›†çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•æ˜¯éå¸¸é‡è¦çš„ç»„ä»¶ã€‚ç»å…¸çš„æ‚å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰åœ¨2Dæ•°æ®é›†ä¸Šè¿›è¡Œé‡å»ºåŸºäºå¼‚å¸¸æ£€æµ‹å·²è¢«æè®®ï¼Œä½†æ˜¯è¿™äº›æ¨¡å‹ä¸ç›´æ¥é€‚ç”¨äº3Dæ•°æ®é›†ã€‚åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨å¹½é»˜æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ï¼Œä»¥ä¾¿å°†DDPMæ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡3DåŒ»å­¦æ•°æ®é›†ã€‚æˆ‘ä»¬éªŒè¯æè®®çš„æ–¹æ³•åœ¨é è¿‘å’Œè¿œç¦»å¼‚å¸¸æ•°æ®é›†ä¸Šï¼Œå¹¶ä¸æœ€è¿‘æè®®çš„3Då¯ç”¨çš„Latent Transformer Modelsï¼ˆLTMï¼‰è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å‘ç°æè®®çš„LDMåŸºæœ¬é€»è¾‘è¾¾åˆ°äº†ç»Ÿè®¡å­¦ä¸Šæ˜¾è‘—æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¹Ÿå…·æœ‰æ›´å¥½çš„å†…å­˜æ‰©å±•å’Œæ›´å¥½çš„ç©ºé—´å¼‚å¸¸åœ°å›¾ã€‚ä»£ç å¯ä»¥åœ¨https://github.com/marksgraham/ddpm-oodä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="AutoDecoding-Latent-3D-Diffusion-Models"><a href="#AutoDecoding-Latent-3D-Diffusion-Models" class="headerlink" title="AutoDecoding Latent 3D Diffusion Models"></a>AutoDecoding Latent 3D Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05445">http://arxiv.org/abs/2307.05445</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snap-research/3dvader">https://github.com/snap-research/3dvader</a></li>
<li>paper_authors: Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski, Chaoyang Wang, Luc Van Gool, Sergey Tulyakov</li>
<li>for: ç”Ÿæˆé™æ€å’ŒåŠ¨æ€3Dèµ„äº§çš„æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬3Dè‡ªåŠ¨è§£ç å™¨æ¡†æ¶ï¼Œç”¨äºæ•æ‰è§†å›¾ä¸€è‡´çš„å¤–è§‚å’Œå‡ ä½•ç»“æ„ã€‚</li>
<li>methods: ä½¿ç”¨ç›®æ ‡æ•°æ®é›†ä¸­å­¦ä¹ çš„å±æ€§åµŒå…¥åœ¨ç¼ºçœç©ºé—´ä¸­ï¼Œç„¶åå°†å…¶è§£ç æˆå¯è§†åŒ–çš„æ¶‚æŠ¹è¡¨ç¤ºå½¢å¼ï¼Œå¹¶ä½¿ç”¨robustæŠ½è±¡å’Œæ­£åˆ™åŒ–æ“ä½œæ¥å­¦ä¹ 3DåæŒ¯ã€‚</li>
<li>results: æ¯”state-of-the-artæ–¹æ³•é«˜æ•ˆï¼Œåœ¨å¤šè§†å›¾å›¾åƒæ•°æ®é›†ã€å®é™…é‡å¤–è§†é¢‘å’Œå¤§è§„æ¨¡çœŸå®è§†é¢‘æ•°æ®é›†ä¸Šè·å¾—ä¼˜ç§€çš„ç”Ÿæˆç»“æœã€‚<details>
<summary>Abstract</summary>
We present a novel approach to the generation of static and articulated 3D assets that has a 3D autodecoder at its core. The 3D autodecoder framework embeds properties learned from the target dataset in the latent space, which can then be decoded into a volumetric representation for rendering view-consistent appearance and geometry. We then identify the appropriate intermediate volumetric latent space, and introduce robust normalization and de-normalization operations to learn a 3D diffusion from 2D images or monocular videos of rigid or articulated objects. Our approach is flexible enough to use either existing camera supervision or no camera information at all -- instead efficiently learning it during training. Our evaluations demonstrate that our generation results outperform state-of-the-art alternatives on various benchmark datasets and metrics, including multi-view image datasets of synthetic objects, real in-the-wild videos of moving people, and a large-scale, real video dataset of static objects.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥ç”Ÿæˆé™æ­¢å’Œå—æ‹˜å¼3Dèµ„äº§ï¼Œå…¶æ ¸å¿ƒæ˜¯3Dè‡ªåŠ¨è§£ç å™¨æ¡†æ¶ã€‚è¯¥æ¡†æ¶åµŒå…¥ç›®æ ‡æ•°æ®é›†ä¸­å­¦ä¹ çš„å±æ€§åœ¨ç¼ºå¤±ç©ºé—´ä¸­åµŒå…¥ï¼Œç„¶åå¯ä»¥ç”¨äºæ¸²æŸ“è§†è§’ä¸€è‡´çš„å¤–è§‚å’Œå‡ ä½•ç»“æ„ã€‚æˆ‘ä»¬ç„¶åç¡®å®šäº†é€‚å½“çš„ä¸­é—´ç¼ºå¤±ç©ºé—´ï¼Œå¹¶å¼•å…¥äº†ç¨³å®šçš„ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°å’Œè§£å†³æ“ä½œæ¥å­¦ä¹ 3Dæ‰©æ•£ä»2Då›¾åƒæˆ–åŠçƒå½¢ç‰©ä½“çš„ç…§ç‰‡æˆ–è§†é¢‘ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä½¿ç”¨ç°æœ‰çš„æ‘„åƒå¤´ç›‘ç£æˆ–æ²¡æœ‰æ‘„åƒå¤´ä¿¡æ¯ï¼Œè€Œä¸æ˜¯åœ¨è®­ç»ƒä¸­é«˜æ•ˆåœ°å­¦ä¹ ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆç»“æœè¶…è¿‡äº†å½“å‰çš„å‚è€ƒæ–¹æ³•åœ¨å¤šè§†å›¾å›¾åƒé›†ã€å®é™…é‡å¤–è§†é¢‘ä¸­çš„äººä½“åŠ¨ä½œå’Œå¤§è§„æ¨¡ã€å®é™…è§†é¢‘é›†ä¸­çš„å¤šç§æ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šã€‚
</details></li>
</ul>
<hr>
<h2 id="Training-Ensembles-with-Inliers-and-Outliers-for-Semi-supervised-Active-Learning"><a href="#Training-Ensembles-with-Inliers-and-Outliers-for-Semi-supervised-Active-Learning" class="headerlink" title="Training Ensembles with Inliers and Outliers for Semi-supervised Active Learning"></a>Training Ensembles with Inliers and Outliers for Semi-supervised Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03741">http://arxiv.org/abs/2307.03741</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vladan-stojnic/active-outliers">https://github.com/vladan-stojnic/active-outliers</a></li>
<li>paper_authors: Vladan StojniÄ‡, Zakaria Laskar, Giorgos Tolias</li>
<li>for: æœ¬æ–‡é‡‡ç”¨æ·±åº¦å­¦ä¹ å’Œæ´»åŠ¨å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³åœ¨å¼‚å¸¸ç¤ºä¾‹å­˜åœ¨ä¸‹çš„æ¿€æ´»å­¦ä¹ é—®é¢˜ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸‰ä¸ªé«˜åº¦ååŒçš„ç»„ä»¶ï¼ŒåŒ…æ‹¬ï¼š joint åˆ†ç±»å™¨è®­ç»ƒï¼ˆå«å¼‚å¸¸ç¤ºä¾‹å’Œæ­£å¸¸ç¤ºä¾‹ï¼‰ã€ semi-supervised learning é€šè¿‡ pseudo-labelingã€æ¨¡å‹ensembleã€‚</li>
<li>results: æœ¬æ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå°†è¿™ä¸‰ä¸ªç»„ä»¶ç»“åˆä½¿ç”¨å¯ä»¥æé«˜ pseudo-labeling çš„å‡†ç¡®ç‡å’Œæ•°æ®æ”¶é›†è´¨é‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œjoint è®­ç»ƒå¯ä»¥æ­£ç¡®å¤„ç†å¼‚å¸¸ç¤ºä¾‹ï¼Œæ— éœ€è¿›è¡ŒExplicit outlier detectionã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•çš„ç®€å•æ€§å’Œæ˜“ç”¨æ€§ï¼Œä½¿å…¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Šå…¶ä»–æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Deep active learning in the presence of outlier examples poses a realistic yet challenging scenario. Acquiring unlabeled data for annotation requires a delicate balance between avoiding outliers to conserve the annotation budget and prioritizing useful inlier examples for effective training. In this work, we present an approach that leverages three highly synergistic components, which are identified as key ingredients: joint classifier training with inliers and outliers, semi-supervised learning through pseudo-labeling, and model ensembling. Our work demonstrates that ensembling significantly enhances the accuracy of pseudo-labeling and improves the quality of data acquisition. By enabling semi-supervision through the joint training process, where outliers are properly handled, we observe a substantial boost in classifier accuracy through the use of all available unlabeled examples. Notably, we reveal that the integration of joint training renders explicit outlier detection unnecessary; a conventional component for acquisition in prior work. The three key components align seamlessly with numerous existing approaches. Through empirical evaluations, we showcase that their combined use leads to a performance increase. Remarkably, despite its simplicity, our proposed approach outperforms all other methods in terms of performance. Code: https://github.com/vladan-stojnic/active-outliers
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±å…¥çš„æ´»åŠ¨å­¦ä¹ åœ¨å¼‚å¸¸ç¤ºä¾‹å­˜åœ¨ä¸‹ pose ä¸€ä¸ªç°å® yet æŒ‘æˆ˜çš„åœºæ™¯ã€‚ è·å–æœªæ ‡æ³¨æ•°æ® Ğ´Ğ»Ñæ³¨é‡Šéœ€è¦ä¸€ä¸ªç»†è…»çš„å¹³è¡¡ï¼Œä»¥é¿å…å¼‚å¸¸ç¤ºä¾‹ï¼Œå¹¶ä¸”ä¼˜å…ˆçº§åˆ’åˆ†æœ‰ç”¨çš„å‡†ç¡®ç¤ºä¾‹ï¼Œä»¥ä¾¿æœ‰æ•ˆåœ°è®­ç»ƒã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä¸‰ä¸ªé«˜åº¦ååŒçš„ç»„ä»¶ï¼Œå³ï¼š joint ç±»ifier è®­ç»ƒä¸å‡†ç¡®ç¤ºä¾‹å’Œå¼‚å¸¸ç¤ºä¾‹ï¼Œ semi-supervised å­¦ä¹ é€šè¿‡pseudo-labelingï¼Œä»¥åŠæ¨¡å‹é›†æˆã€‚ æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œå°†è¿™ä¸‰ä¸ªç»„ä»¶ç›¸äº’èåˆå¯ä»¥æ˜¾è‘—æé«˜ pseudo-labeling çš„å‡†ç¡®åº¦å’Œæ•°æ®æ”¶é›†è´¨é‡ã€‚é€šè¿‡å…è®¸ semi-supervision è¿‡ç¨‹ä¸­çš„å¼‚å¸¸ç¤ºä¾‹å¤„ç†ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªéå¸¸å¤§çš„æå‡å‡†ç¡®ç‡ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„æ— æ ‡æ³¨ç¤ºä¾‹ã€‚å¦å¤–ï¼Œæˆ‘ä»¬å‘ç°åœ¨ joint è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ— éœ€è¿›è¡Œæ˜ç¡®çš„å¼‚å¸¸æ£€æµ‹ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨æ‰€æœ‰ç¤ºä¾‹è¿›è¡Œè®­ç»ƒã€‚è¿™ä¸‰ä¸ªå…³é”®ç»„ä»¶ä¸è®¸å¤šç°æœ‰æ–¹æ³•å…¼å®¹ã€‚é€šè¿‡å®éªŒè¯„ä¼°ï¼Œæˆ‘ä»¬è¡¨æ˜è¿™äº›ç»„ä»¶çš„ç»“åˆä½¿ç”¨å¯ä»¥å¸¦æ¥æ€§èƒ½æå‡ã€‚å°¤å…¶æ˜¯ï¼Œæˆ‘ä»¬çš„æè®®æ–¹æ³•çš„ç®€å•æ€§å’Œé«˜æ•ˆæ€§ï¼Œåœ¨æ€§èƒ½æ–¹é¢èƒœè¿‡æ‰€æœ‰å…¶ä»–æ–¹æ³•ã€‚ä»£ç ï¼šhttps://github.com/vladan-stojnic/active-outliersNote: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Equivariant-Single-View-Pose-Prediction-Via-Induced-and-Restricted-Representations"><a href="#Equivariant-Single-View-Pose-Prediction-Via-Induced-and-Restricted-Representations" class="headerlink" title="Equivariant Single View Pose Prediction Via Induced and Restricted Representations"></a>Equivariant Single View Pose Prediction Via Induced and Restricted Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03704">http://arxiv.org/abs/2307.03704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Owen Howell, David Klee, Ondrej Biza, Linfeng Zhao, Robin Walters</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è®¡ç®—æœºè§†è§‰ä¸­çš„åŸºæœ¬é—®é¢˜ï¼Œå³ä»äºŒdimensionalå›¾åƒä¸­å­¦ä¹ ä¸‰dimensionalä¸–ç•Œã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨SO(3)å¯¹å‡†çš„çº¦æŸæ¥é™åˆ¶äºŒdimensionalè¾“å…¥çš„å¯èƒ½æ€§ï¼Œå¹¶ä½¿ç”¨SO(2)å¯¹å‡†çš„çº¦æŸæ¥ä¿è¯å›¾åƒçš„å‡†ç¡®æ€§ã€‚</li>
<li>results: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œå¯ä»¥learnä¸‰dimensionalä¸–ç•Œçš„è¡¨ç¤ºä»äºŒdimensionalå›¾åƒä¸­ï¼Œå¹¶åœ¨PASCAL3D+å’ŒSYMSOLä¸¤ä¸ª pose estimation ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€é«˜ç²¾åº¦ã€‚<details>
<summary>Abstract</summary>
Learning about the three-dimensional world from two-dimensional images is a fundamental problem in computer vision. An ideal neural network architecture for such tasks would leverage the fact that objects can be rotated and translated in three dimensions to make predictions about novel images. However, imposing SO(3)-equivariance on two-dimensional inputs is difficult because the group of three-dimensional rotations does not have a natural action on the two-dimensional plane. Specifically, it is possible that an element of SO(3) will rotate an image out of plane. We show that an algorithm that learns a three-dimensional representation of the world from two dimensional images must satisfy certain geometric consistency properties which we formulate as SO(2)-equivariance constraints. We use the induced and restricted representations of SO(2) on SO(3) to construct and classify architectures which satisfy these geometric consistency constraints. We prove that any architecture which respects said consistency constraints can be realized as an instance of our construction. We show that three previously proposed neural architectures for 3D pose prediction are special cases of our construction. We propose a new algorithm that is a learnable generalization of previously considered methods. We test our architecture on three pose predictions task and achieve SOTA results on both the PASCAL3D+ and SYMSOL pose estimation tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
å­¦ä¹ ä¸‰ç»´ä¸–ç•Œä»äºŒç»´å›¾åƒæ˜¯è®¡ç®—æœºè§†è§‰çš„åŸºæœ¬é—®é¢˜ã€‚ç†æƒ³çš„ç¥ç»ç½‘ç»œæ¶æ„ Ğ´Ğ»Ñæ­¤ç±»ä»»åŠ¡åº”è¯¥åˆ©ç”¨å¯¹è±¡å¯ä»¥åœ¨ä¸‰ç»´ç©ºé—´ä¸­æ—‹è½¬å’Œå¹³ç§»æ¥åšé¢„æµ‹ã€‚ä½†æ˜¯ï¼Œåœ¨å°† SO(3) å¯¹äºŒç»´å¹³é¢çš„åŠ¨ä½œç›´è§‚ä¸æ˜¯è‡ªç„¶çš„ï¼Œè¿™ä½¿å¾—åœ¨äºŒç»´è¾“å…¥ä¸Šå¼ºåˆ¶ SO(3) å¯¹ç§°æ€§æ˜¯å›°éš¾çš„ã€‚æˆ‘ä»¬æ˜¾ç¤ºï¼Œä¸€ä¸ªå¯ä»¥å­¦ä¹ ä¸‰ç»´ä¸–ç•Œçš„äºŒç»´å›¾åƒè¡¨ç¤ºéœ€è¦æ»¡è¶³æŸäº›å‡ ä½•ä¸€è‡´æ€§æ€§è´¨ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º SO(2) å¯¹ç§°æ€§çº¦æŸã€‚æˆ‘ä»¬ä½¿ç”¨ SO(2) åœ¨ SO(3) ä¸Šçš„å¼•å‡ºå’Œå—é™è¡¨ç¤ºæ¥æ„å»ºå’Œåˆ†ç±»æ»¡è¶³è¿™äº›å‡ ä½•ä¸€è‡´æ€§çº¦æŸçš„æ¶æ„ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä»»ä½•æ»¡è¶³è¿™äº›çº¦æŸçš„æ¶æ„éƒ½å¯ä»¥é€šè¿‡æˆ‘ä»¬çš„æ„å»ºå®ç°ã€‚æˆ‘ä»¬æ˜¾ç¤ºï¼Œä¸‰ç§ä¹‹å‰æå‡ºçš„ç¥ç»ç½‘ç»œæ¶æ„ Ğ´Ğ»Ñ 3D å§¿æ€é¢„æµ‹æ˜¯ç‰¹ä¾‹æˆ‘ä»¬çš„æ„å»ºã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–°çš„ç®—æ³•ï¼Œå®ƒæ˜¯learnableçš„ä¸€èˆ¬åŒ–å‰è¿°æ–¹æ³•ã€‚æˆ‘ä»¬æµ‹è¯•æˆ‘ä»¬çš„æ¶æ„åœ¨ä¸‰ç§å§¿æ€é¢„æµ‹ä»»åŠ¡ä¸Šï¼Œå¹¶åœ¨ PASCAL3D+ å’Œ SYMSOL å§¿æ€é¢„æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€é«˜çš„ SOTA ç»“æœã€‚
</details></li>
</ul>
<hr>
<h2 id="Motion-Magnification-in-Robotic-Sonography-Enabling-Pulsation-Aware-Artery-Segmentation"><a href="#Motion-Magnification-in-Robotic-Sonography-Enabling-Pulsation-Aware-Artery-Segmentation" class="headerlink" title="Motion Magnification in Robotic Sonography: Enabling Pulsation-Aware Artery Segmentation"></a>Motion Magnification in Robotic Sonography: Enabling Pulsation-Aware Artery Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03698">http://arxiv.org/abs/2307.03698</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dianyehuang/robpmepasnn">https://github.com/dianyehuang/robpmepasnn</a></li>
<li>paper_authors: Dianye Huang, Yuan Bi, Nassir Navab, Zhongliang Jiang</li>
<li>for: è¯¥ç ”ç©¶æ—¨åœ¨æé«˜è¡€æ¶²åŠ¨åŠ›å›¾åƒ segmentation ç²¾åº¦å’Œç¨³å®šæ€§ï¼Œé€šè¿‡åˆ©ç”¨å¿ƒè„è¡€æµå¸¦åŠ¨çš„ä¿¡æ¯æ¥å¸®åŠ©ä¸´åºŠåŒ»ç”Ÿè¯Šæ–­å’Œç›‘æµ‹åŠ¨è„‰ç–¾ç—…ã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§æ–°çš„æŒ¯è¡å¸®åŠ©åˆ†å‰²ç¥ç»ç½‘ç»œï¼ˆPAS-NNï¼‰ï¼Œåˆ©ç”¨å¿ƒè„è¡€æµå¸¦åŠ¨çš„ä¿¡æ¯æ¥å¸®åŠ©å®šä½åŠ¨è„‰ï¼Œå¹¶ä½¿ç”¨äº†è¿åŠ¨å¢å¼ºæŠ€æœ¯æ¥å¢å¼ºå¿ƒè„è¡€æµçš„ä¿¡å·ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒPAS-NN å¯ä»¥ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“ï¼Œå¹¶ä¸”å¯ä»¥æœ‰æ•ˆåœ°æé«˜å°åŠ¨è„‰ï¼ˆè¡€ç®¡ï¼‰çš„ segmentation æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Ultrasound (US) imaging is widely used for diagnosing and monitoring arterial diseases, mainly due to the advantages of being non-invasive, radiation-free, and real-time. In order to provide additional information to assist clinicians in diagnosis, the tubular structures are often segmented from US images. To improve the artery segmentation accuracy and stability during scans, this work presents a novel pulsation-assisted segmentation neural network (PAS-NN) by explicitly taking advantage of the cardiac-induced motions. Motion magnification techniques are employed to amplify the subtle motion within the frequency band of interest to extract the pulsation signals from sequential US images. The extracted real-time pulsation information can help to locate the arteries on cross-section US images; therefore, we explicitly integrated the pulsation into the proposed PAS-NN as attention guidance. Notably, a robotic arm is necessary to provide stable movement during US imaging since magnifying the target motions from the US images captured along a scan path is not manually feasible due to the hand tremor. To validate the proposed robotic US system for imaging arteries, experiments are carried out on volunteers' carotid and radial arteries. The results demonstrated that the PAS-NN could achieve comparable results as state-of-the-art on carotid and can effectively improve the segmentation performance for small vessels (radial artery).
</details>
<details>
<summary>æ‘˜è¦</summary>
ultrasoundï¼ˆUSï¼‰æˆåƒå¹¿æ³›ç”¨äºè¯Šæ–­å’Œç›‘æµ‹arterialç–¾ç—…ï¼Œä¸»è¦å› ä¸ºéä¾µå…¥æ€§ã€æ— è¾å°„å’Œå®æ—¶ç­‰ä¼˜ç‚¹ã€‚ä¸ºäº†ä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›æ›´å¤šçš„è¯Šæ–­ä¿¡æ¯ï¼Œåœ¨USå›¾åƒä¸­åˆ†å‰² tubularç»“æ„æˆä¸ºä¸€é¡¹é‡è¦çš„ä»»åŠ¡ã€‚ä¸ºäº†æé«˜arteryåˆ†å‰²ç²¾åº¦å’Œç¨³å®šæ€§ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¾åŠ¨è¾å°„ä¿¡å·çš„æ–°å‹pulsation-assisted segmentation neural networkï¼ˆPAS-NNï¼‰ã€‚åœ¨å›¾åƒåºåˆ—ä¸­æå– cardiac-induced motions çš„æŸ”åŒ–ä¿¡å·ï¼Œå¹¶å°†å…¶ä½œä¸ºæ³¨æ„åŠ›å¼•å¯¼è¿›è¡Œexplicitly integratingã€‚ç”±äºéœ€è¦ç¨³å®šçš„è¿åŠ¨æ¥æä¾›USæˆåƒï¼Œå› æ­¤åœ¨USæˆåƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†Robotic armã€‚ä¸ºéªŒè¯æå‡ºçš„Robotic USç³»ç»Ÿåœ¨è¯Šæ–­arteriesä¸­çš„å¯è¡Œæ€§ï¼Œå¯¹å¿—æ„¿è€…çš„ Common carotidå’ŒRadial arteryè¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒPAS-NNå¯ä»¥ä¸å½“å‰çŠ¶æ€è‰ºæœ¯ä¸€æ ·å¥½ï¼Œå¹¶ä¸”å¯ä»¥æœ‰æ•ˆåœ°æé«˜å°åŠ¨è„‰ï¼ˆRadial arteryï¼‰çš„åˆ†å‰²æ€§èƒ½ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/08/cs.CV_2023_07_08/" data-id="clltaagn8002wr888bcpfd0r7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/08/cs.LG_2023_07_08/" class="article-date">
  <time datetime="2023-07-07T16:00:00.000Z" itemprop="datePublished">2023-07-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/08/cs.LG_2023_07_08/">cs.LG - 2023-07-08 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Model-Free-Exploration-in-Low-Rank-MDPs"><a href="#Efficient-Model-Free-Exploration-in-Low-Rank-MDPs" class="headerlink" title="Efficient Model-Free Exploration in Low-Rank MDPs"></a>Efficient Model-Free Exploration in Low-Rank MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03997">http://arxiv.org/abs/2307.03997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zakaria Mhammedi, Adam Block, Dylan J. Foster, Alexander Rakhlin</li>
<li>for: æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ç§å®ç”¨ã€æ•ˆç‡é«˜çš„æ¢ç´¢ç®—æ³•ï¼Œç”¨äºåœ¨é«˜ç»´ç©ºé—´ä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå¹¶ä¸”å…·æœ‰å‡½æ•°è¿‘ä¼¼å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºçš„ VoX ç®—æ³•ä½¿ç”¨ä¸€ç§é€šè¿‡unknown feature embeddingçš„ä½ç»´ Markov Decision Processesï¼ˆMDPsï¼‰æ¥å®ç°æ¢ç´¢ï¼Œè¯¥ç®—æ³•æ˜¯ computationally efficient å¹¶ä¸”æ— éœ€é¢å¤–çš„ç»Ÿè®¡å‡è®¾ã€‚</li>
<li>results: æœ¬æ–‡çš„åˆ†æè¡¨æ˜ï¼ŒVoX ç®—æ³•å¯ä»¥åœ¨ä½ç»´ MDPs ä¸­æä¾› provably æ ·æœ¬æ•ˆç‡çš„æ¢ç´¢ï¼Œå¹¶ä¸”ä¸éœ€è¦é¢å¤–çš„æ¨¡å‹åŸºç¡€æˆ– latent variable structureã€‚<details>
<summary>Abstract</summary>
A major challenge in reinforcement learning is to develop practical, sample-efficient algorithms for exploration in high-dimensional domains where generalization and function approximation is required. Low-Rank Markov Decision Processes -- where transition probabilities admit a low-rank factorization based on an unknown feature embedding -- offer a simple, yet expressive framework for RL with function approximation, but existing algorithms are either (1) computationally intractable, or (2) reliant upon restrictive statistical assumptions such as latent variable structure, access to model-based function approximation, or reachability. In this work, we propose the first provably sample-efficient algorithm for exploration in Low-Rank MDPs that is both computationally efficient and model-free, allowing for general function approximation and requiring no additional structural assumptions. Our algorithm, VoX, uses the notion of a generalized optimal design for the feature embedding as an efficiently computable basis for exploration, performing efficient optimal design computation by interleaving representation learning and policy optimization. Our analysis -- which is appealingly simple and modular -- carefully combines several techniques, including a new reduction from optimal design computation to policy optimization based on the Frank-Wolfe method, and an improved analysis of a certain minimax representation learning objective found in prior work.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜åœ¨å¼ºåŒ–å­¦ä¹ ä¸­æ˜¯å¼€å‘å®ç”¨ã€æ ·æœ¬æ•ˆç‡é«˜çš„æ¢ç´¢ç®—æ³•ï¼Œä»¥ä¾¿åœ¨é«˜ç»´åº¦ç©ºé—´ä¸­è¿›è¡Œæ¢ç´¢ï¼Œå¹¶ä¸”éœ€è¦æ³›åŒ–å’Œå‡½æ•°è¿‘ä¼¼ã€‚ä½ç»´é©¬å°”å¯å¤«é‡è¿‡ç¨‹ï¼ˆLow-Rank MDPsï¼‰æä¾›äº†ä¸€ä¸ªç®€å• yet è¡¨è¾¾åŠ›å¼ºçš„æ¡†æ¶ï¼Œä½†ç°æœ‰ç®—æ³•çš„é—®é¢˜åŒ…æ‹¬ï¼ˆ1ï¼‰è®¡ç®—å¤æ‚åº¦å¤ªé«˜ï¼Œæˆ–ï¼ˆ2ï¼‰éœ€è¦ç‰¹æ®Šçš„ç»Ÿè®¡å‡è®¾ï¼Œå¦‚éšè—å˜é‡ç»“æ„ã€å‡½æ•°è¿‘ä¼¼æ¨¡å‹æˆ–å¯è¾¾æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªå¯è¯æ˜æ ·æœ¬æ•ˆç‡é«˜çš„æ¢ç´¢ç®—æ³•ï¼Œå¯ä»¥åœ¨ä½ç»´é©¬å°”å¯å¤«é‡è¿‡ç¨‹ä¸­è¿›è¡Œæ¢ç´¢ï¼Œå¹¶ä¸”ä¸éœ€è¦ç‰¹æ®Šçš„ç»“æ„å‡è®¾ã€‚æˆ‘ä»¬çš„ç®—æ³•VoXåˆ©ç”¨äº†ä¸€ä¸ªé€šç”¨ä¼˜åŒ–è®¾è®¡çš„æ¦‚å¿µï¼Œä½œä¸ºä¸€ç§å¯è¯»å–çš„åŸºç¡€ Ğ´Ğ»Ñæ¢ç´¢ï¼Œå¹¶é€šè¿‡å°†è¡¨ç¤ºå­¦ä¹ å’Œæ”¿ç­–ä¼˜åŒ–ç»“åˆåœ¨ä¸€èµ·ï¼Œå®ç°é«˜æ•ˆçš„ä¼˜åŒ–è®¾è®¡è®¡ç®—ã€‚æˆ‘ä»¬çš„åˆ†æï¼Œæ„ŸçŸ¥ç®€å•è€Œå¹²å‡€ï¼Œç»¼åˆä½¿ç”¨äº†å¤šç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä¸€ç§æ–°çš„å‡å°‘ä»ä¼˜åŒ–è®¾è®¡è®¡ç®—åˆ°æ”¿ç­–ä¼˜åŒ–åŸºäºFrank-Wolfeæ–¹æ³•çš„æŠ€æœ¯ï¼Œä»¥åŠåœ¨å…ˆå‰çš„å·¥ä½œä¸­å‘ç°çš„ä¸€ç§æ”¹è¿›çš„æœ€å°æœ€å¤§è¡¨è¾¾å­¦ä¹ ç›®æ ‡çš„åˆ†æã€‚
</details></li>
</ul>
<hr>
<h2 id="NLP-Meets-RNA-Unsupervised-Embedding-Learning-for-Ribozymes-with-Word2Vec"><a href="#NLP-Meets-RNA-Unsupervised-Embedding-Learning-for-Ribozymes-with-Word2Vec" class="headerlink" title="NLP Meets RNA: Unsupervised Embedding Learning for Ribozymes with Word2Vec"></a>NLP Meets RNA: Unsupervised Embedding Learning for Ribozymes with Word2Vec</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05537">http://arxiv.org/abs/2307.05537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Kean Gao</li>
<li>for: æœ¬ç ”ç©¶ä½¿ç”¨Word2Vecç®—æ³•æ¥æé«˜æˆ‘ä»¬å¯¹æ ¸é…¸æ‚åˆç‰©ï¼ˆribozymeï¼‰çš„ç†è§£ï¼Œå¹¶å¯»æ‰¾æ›´å¥½çš„æ–¹æ³•æ¥åˆ†ç±»ribozymeã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨Word2Vecç®—æ³•ï¼Œé€šè¿‡è®­ç»ƒ9,000å¤šä¸ªä¸åŒçš„æ ¸é…¸æ‚åˆç‰©ï¼Œå°†æ ¸é…¸åºåˆ—æ˜ å°„åˆ°128å’Œ256ç»´åº¦çš„ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ç©ºé—´ä¸­ã€‚</li>
<li>results: ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ ¸é…¸åºåˆ—embeddingå¯ä»¥å‡†ç¡®åœ°åˆ†ç±»æ ¸é…¸æ‚åˆç‰©ï¼Œå¹¶ä¸”256ç»´åº¦çš„embedding Vector Spaceå¯ä»¥æ•æ‰æ ¸é…¸æ‚åˆç‰©çš„ç‰¹å¾ã€‚<details>
<summary>Abstract</summary>
Ribozymes, RNA molecules with distinct 3D structures and catalytic activity, have widespread applications in synthetic biology and therapeutics. However, relatively little research has focused on leveraging deep learning to enhance our understanding of ribozymes. This study implements Word2Vec, an unsupervised learning technique for natural language processing, to learn ribozyme embeddings. Ribo2Vec was trained on over 9,000 diverse ribozymes, learning to map sequences to 128 and 256-dimensional vector spaces. Using Ribo2Vec, sequence embeddings for five classes of ribozymes (hatchet, pistol, hairpin, hovlinc, and twister sister) were calculated. Principal component analysis demonstrated the ability of these embeddings to distinguish between ribozyme classes. Furthermore, a simple SVM classifier trained on ribozyme embeddings showed promising results in accurately classifying ribozyme types. Our results suggest that the embedding vectors contained meaningful information about ribozymes. Interestingly, 256-dimensional embeddings behaved similarly to 128-dimensional embeddings, suggesting that a lower dimension vector space is generally sufficient to capture ribozyme features. This approach demonstrates the potential of Word2Vec for bioinformatics, opening new avenues for ribozyme research. Future research includes using a Transformer-based method to learn RNA embeddings, which can capture long-range interactions between nucleotides.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç‘åšé…µç´ ï¼ˆribozymeï¼‰å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬ç”Ÿç‰©æŠ€æœ¯å’Œç”Ÿç‰©åŒ»å­¦ã€‚ç„¶è€Œï¼Œç›¸å¯¹è®ºæ–‡ä¸å¤šå…³æ³¨åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¥æé«˜æˆ‘ä»¬å¯¹ç‘åšé…µç´ çš„ç†è§£ã€‚è¿™é¡¹ç ”ç©¶ä½¿ç”¨Word2Vecç®—æ³•ï¼Œä¸€ç§æ— ç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œæ¥å­¦ä¹ ç‘åšé…µç´ çš„åµŒå…¥ã€‚ Ribo2Vec è®­ç»ƒåœ¨è¶…è¿‡9000ä¸ªå¤šæ ·åŒ–ç‘åšé…µç´ ä¸Šï¼Œå°†åºåˆ—æ˜ å°„åˆ°128å’Œ256ç»´åº¦çš„å‘é‡ç©ºé—´ä¸­ã€‚ä½¿ç”¨ Ribo2Vecï¼Œæˆ‘ä»¬è®¡ç®—äº†5ç§ç‘åšé…µç´ ç±»å‹ï¼ˆé”¥å­ã€æ‰‹æªã€æ·å¾„ã€æ‰˜å¾å’Œå§å¦¹ï¼‰çš„åºåˆ—åµŒå…¥ã€‚å¯¹äºè¿™äº›åµŒå…¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸»æˆåˆ†åˆ†æå¾—åˆ°äº†å¯ä»¥åˆ†è¾¨ç‘åšé…µç´ ç±»å‹çš„ç»“æœã€‚æ­¤å¤–ï¼Œä½¿ç”¨ç‘åšé…µç´ åµŒå…¥è®­ç»ƒçš„ç®€å•æ”¯æŒå‘é‡æœºå™¨å­¦ä¹ ï¼ˆSVMï¼‰åˆ†ç±»å™¨ä¹Ÿè¡¨ç°å‡ºäº†è‰¯å¥½çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ç‘åšé…µç´ åµŒå…¥ vectors å«æœ‰æœ‰ç”¨çš„ä¿¡æ¯ã€‚æœ‰è¶£çš„æ˜¯ï¼Œ256ç»´åº¦çš„åµŒå…¥å’Œ128ç»´åº¦çš„åµŒå…¥ä¹‹é—´çš„è¡Œä¸ºç›¸ä¼¼ï¼Œè¿™è¡¨æ˜ä½ç»´åº¦å‘é‡ç©ºé—´é€šå¸¸è¶³å¤Ÿ captureç‘åšé…µç´ ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•å¯èƒ½ä¼šæ‰“å¼€æ–°çš„ Bioinformatics ç ”ç©¶é€”å¾„ã€‚æœªæ¥çš„ç ”ç©¶å¯èƒ½åŒ…æ‹¬ä½¿ç”¨Transformerç®—æ³•æ¥å­¦ä¹ RNAåµŒå…¥ï¼Œä»¥æ•æ‰RNAä¸­è·ç¦»è¾ƒè¿œçš„æ ¸è‹·é…¸ä¹‹é—´çš„é•¿è·ç¦»äº¤äº’ã€‚
</details></li>
</ul>
<hr>
<h2 id="Integrating-Curricula-with-Replays-Its-Effects-on-Continual-Learning"><a href="#Integrating-Curricula-with-Replays-Its-Effects-on-Continual-Learning" class="headerlink" title="Integrating Curricula with Replays: Its Effects on Continual Learning"></a>Integrating Curricula with Replays: Its Effects on Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05747">http://arxiv.org/abs/2307.05747</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanglab-deepneurocoglab/integrating-curricula-with-replays">https://github.com/zhanglab-deepneurocoglab/integrating-curricula-with-replays</a></li>
<li>paper_authors: Ren Jie Tee, Mengmi Zhang</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨curriculaçš„integratingä¸replayæ–¹æ³•åœ¨æŒç»­å­¦ä¹ ä¸­çš„ä½œç”¨ï¼Œä»¥æé«˜çŸ¥è¯†é€€åŒ–å’Œå­¦ä¹ è½¬ç§»ã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨äº†ä¸‰ç§ä¸åŒçš„curriculaè®¾è®¡æ–¹æ³•ï¼ŒåŒ…æ‹¬äº¤å é¢‘ç‡çš„é‡å¤ç¤ºä¾‹ä¸è®­ç»ƒæ•°æ®ã€é¡ºåºæ’åºç¤ºä¾‹çš„é‡å¤é¡ºåºã€ä»¥åŠä»uniformåˆ†å¸ƒä¸­é€‰æ‹©ç¤ºä¾‹çš„ç­–ç•¥ã€‚è¿™äº›æ–¹æ³•ä¸è®¤çŸ¥å¿ƒç†å­¦åŸç†ç›¸Alignmentï¼Œå¹¶å¯ä»¥åˆ©ç”¨é‡å¤å®è·µä¸­çš„ä¼˜ç‚¹ã€æ˜“äºå›°éš¾é‡å¤ã€ä»¥åŠç¤ºä¾‹é€‰æ‹©ç­–ç•¥ã€‚</li>
<li>results: æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™ä¸‰ç§curriculaæœ‰æ•ˆåœ°éåˆ¶äº†è¡°å¼±æ€§å¤±å¿†ï¼Œå¹¶æé«˜äº†æ­£é¢çŸ¥è¯†ä¼ é€’ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œcurriculaå¯ä»¥åœ¨æŒç»­å­¦ä¹ æ–¹æ³•ä¸­æä¾›è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ZhangLab-DeepNeuroCogLab/Integrating-Curricula-with-Replays">https://github.com/ZhangLab-DeepNeuroCogLab/Integrating-Curricula-with-Replays</a><details>
<summary>Abstract</summary>
Humans engage in learning and reviewing processes with curricula when acquiring new skills or knowledge. This human learning behavior has inspired the integration of curricula with replay methods in continual learning agents. The goal is to emulate the human learning process, thereby improving knowledge retention and facilitating learning transfer. Existing replay methods in continual learning agents involve the random selection and ordering of data from previous tasks, which has shown to be effective. However, limited research has explored the integration of different curricula with replay methods to enhance continual learning. Our study takes initial steps in examining the impact of integrating curricula with replay methods on continual learning in three specific aspects: the interleaved frequency of replayed exemplars with training data, the sequence in which exemplars are replayed, and the strategy for selecting exemplars into the replay buffer. These aspects of curricula design align with cognitive psychology principles and leverage the benefits of interleaved practice during replays, easy-to-hard rehearsal, and exemplar selection strategy involving exemplars from a uniform distribution of difficulties. Based on our results, these three curricula effectively mitigated catastrophic forgetting and enhanced positive knowledge transfer, demonstrating the potential of curricula in advancing continual learning methodologies. Our code and data are available: https://github.com/ZhangLab-DeepNeuroCogLab/Integrating-Curricula-with-Replays
</details>
<details>
<summary>æ‘˜è¦</summary>
äººç±»åœ¨å­¦ä¹ å’Œå¤ä¹ è¿‡ç¨‹ä¸­ä½¿ç”¨è¯¾ç¨‹ï¼Œä»¥è·å¾—æ–°æŠ€èƒ½æˆ–çŸ¥è¯†ã€‚è¿™ç§äººç±»å­¦ä¹ è¡Œä¸ºå¯¹äº continual learning agent çš„ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñæœ‰ç€çµæ„Ÿã€‚ç›®æ ‡æ˜¯é€šè¿‡æ¨¡æ‹Ÿäººç±»å­¦ä¹ è¿‡ç¨‹ï¼Œæé«˜çŸ¥è¯†ä¿æŒå’Œå­¦ä¹ ä¼ é€’ã€‚ç°æœ‰çš„é‡æ’­æ–¹æ³•åœ¨ continual learning agent ä¸­å·²ç»è¯æ˜æœ‰æ•ˆã€‚ç„¶è€Œï¼Œæœ‰é™çš„ç ”ç©¶æ¢è®¨äº†ä¸åŒè¯¾ç¨‹çš„ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñå’Œé‡æ’­æ–¹æ³•çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶é¦–å…ˆæ¢è®¨äº†åœ¨ä¸‰ä¸ªæ–¹é¢ä¸­çš„è¯¾ç¨‹è®¾è®¡å¯¹ continual learning çš„å½±å“ï¼šé‡æ’­ç¤ºä¾‹çš„é¢‘ç‡ä¸è®­ç»ƒæ•°æ®çš„æ’åºã€ç¤ºä¾‹çš„é‡æ’­é¡ºåºå’Œé€‰æ‹©ç¤ºä¾‹è¿›å…¥ç¼“å­˜çš„ç­–ç•¥ã€‚è¿™äº›è¯¾ç¨‹è®¾è®¡æ–¹é¢ä¸è®¤çŸ¥å¿ƒç†å­¦åŸç†ç›¸å»åˆï¼Œåˆ©ç”¨é‡æ’­ä¸­çš„å åŠ å®è·µã€æ˜“äºå›°éš¾çš„å¤ä¹ å’Œé€‰æ‹©ç¤ºä¾‹çš„ç­–ç•¥ã€‚æ ¹æ®æˆ‘ä»¬çš„ç»“æœï¼Œè¿™ä¸‰ç§è¯¾ç¨‹æœ‰æ•ˆåœ°éæ­¢äº†æ¶æ€§é—å¿˜å’Œæé«˜äº†æ­£é¢çŸ¥è¯†ä¼ é€’ï¼Œè¿™è¡¨æ˜äº†è¯¾ç¨‹åœ¨ continual learning æ–¹æ³•ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨ GitHub ä¸Šæ‰¾åˆ°ï¼šhttps://github.com/ZhangLab-DeepNeuroCogLab/Integrating-Curricula-with-Replays
</details></li>
</ul>
<hr>
<h2 id="Building-and-Road-Segmentation-Using-EffUNet-and-Transfer-Learning-Approach"><a href="#Building-and-Road-Segmentation-Using-EffUNet-and-Transfer-Learning-Approach" class="headerlink" title="Building and Road Segmentation Using EffUNet and Transfer Learning Approach"></a>Building and Road Segmentation Using EffUNet and Transfer Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03980">http://arxiv.org/abs/2307.03980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahil Gangurde</li>
<li>for: æœ¬è®ºæ–‡ç›®æ ‡æ˜¯å¯¹é¥æ„Ÿå›¾åƒä¸­çš„å»ºç­‘å’Œè·¯å¾„è¿›è¡Œ semantics åˆ†å‰²ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäº Google æ–°æå‡ºçš„ EfficientNetV2 å¢å¼ºç½‘ç»œï¼Œå¹¶ç»“åˆ UNet è§£ç å™¨å®ç°åˆ†å‰²å›¾åƒçš„æ–¹æ³•ã€‚</li>
<li>results: è¯¥æ–¹æ³•åœ¨éº»çœå»ºç­‘å’Œè·¯å¾„æ•°æ®é›†ä¸Šè¾¾åˆ°äº† benchmark åˆ†å‰²ç²¾åº¦ï¼ŒIOU åˆ†å‰²ç²¾åº¦åˆ†åˆ«ä¸º 0.8365 å’Œ 0.9153ã€‚<details>
<summary>Abstract</summary>
In city, information about urban objects such as water supply, railway lines, power lines, buildings, roads, etc., is necessary for city planning. In particular, information about the spread of these objects, locations and capacity is needed for the policymakers to make impactful decisions. This thesis aims to segment the building and roads from the aerial image captured by the satellites and UAVs. Many different architectures have been proposed for the semantic segmentation task and UNet being one of them. In this thesis, we propose a novel architecture based on Google's newly proposed EfficientNetV2 as an encoder for feature extraction with UNet decoder for constructing the segmentation map. Using this approach we achieved a benchmark score for the Massachusetts Building and Road dataset with an mIOU of 0.8365 and 0.9153 respectively.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨åŸå¸‚ä¸­ï¼Œæœ‰å…³åŸå¸‚ Ğ¾Ğ±ÑŠĞµĞºts å¦‚æ°´ supplyã€é“è·¯çº¿ã€ç”µåŠ›çº¿ã€å»ºç­‘ã€è·¯å¾„ç­‰çš„ä¿¡æ¯æ˜¯åŸå¸‚è§„åˆ’çš„å¿…è¦æ¡ä»¶ã€‚ç‰¹åˆ«æ˜¯æ”¿ç­–åˆ¶å®šè€…éœ€è¦çŸ¥é“è¿™äº›å¯¹è±¡çš„æ‰©æ•£ã€ä½ç½®å’Œå®¹é‡ï¼Œä»¥ä¾¿åšå‡ºæœ‰åŠ›çš„å†³ç­–ã€‚æœ¬è®ºæ–‡ç›®çš„æ˜¯ä»èˆªå¤©å›¾åƒå’Œæ— äººæœºæ‹æ‘„çš„å«æ˜Ÿå’Œæ— äººæœºå›¾åƒä¸­åˆ†å‰²å»ºç­‘å’Œè·¯å¾„ã€‚è®¸å¤šä¸åŒçš„æ¶æ„å·²ç»ä¸ºsemantic segmentationä»»åŠ¡æå‡ºäº†å¤šç§æ–¹æ¡ˆï¼Œå…¶ä¸­UNetæ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚æœ¬è®ºæ–‡æå‡ºäº†åŸºäºGoogleæ–°æå‡ºçš„EfficientNetV2æ¶æ„æ¥è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶ä¸UNetå†³ç å™¨ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆåˆ†å‰²å›¾åƒã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨é©¬è¨è¯¸å²›å»ºç­‘å’Œè·¯å¾„æ•°æ®é›†ä¸Šè¾¾åˆ°äº† benchmark åˆ†æ•°ï¼Œå…·ä½“åˆ†åˆ«ä¸º0.8365å’Œ0.9153ã€‚
</details></li>
</ul>
<hr>
<h2 id="Digital-Twins-for-Patient-Care-via-Knowledge-Graphs-and-Closed-Form-Continuous-Time-Liquid-Neural-Networks"><a href="#Digital-Twins-for-Patient-Care-via-Knowledge-Graphs-and-Closed-Form-Continuous-Time-Liquid-Neural-Networks" class="headerlink" title="Digital Twins for Patient Care via Knowledge Graphs and Closed-Form Continuous-Time Liquid Neural Networks"></a>Digital Twins for Patient Care via Knowledge Graphs and Closed-Form Continuous-Time Liquid Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04772">http://arxiv.org/abs/2307.04772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Logan Nye</li>
<li>for: è¿™ç¯‡ç ”ç©¶æ˜¯ä¸ºäº†æ¢è®¨å¦‚ä½•ä½¿ç”¨æ•°å­—åŒæŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸæä¾›ä¸ªæ€§åŒ–çš„è¯ç‰©å’Œæ”¯æŒã€æ—©æœŸè¯Šæ–­ã€æ¨¡æ‹Ÿæ²»ç–—ç»“æœå’Œä¼˜åŒ–æ‰‹æœ¯è§„åˆ’ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œä½¿ç”¨çŸ¥è¯†å›¾å’Œé—­å¼å½¢å¼çš„è¿ç»­æ—¶é—´æµä½“ç¥ç»ç½‘ç»œæ¥è§£å†³è®¡ç®—æˆæœ¬å’Œæ¨¡å‹å¤æ‚æ€§çš„æŒ‘æˆ˜ï¼Œä»¥å®ç°å®æ—¶åˆ†æå’Œä¸ªæ€§åŒ–åŒ»ç–—ã€‚</li>
<li>results: æœ¬ç ”ç©¶çš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¿™ç§æ–°çš„æ¡†æ¶å¯ä»¥å®ç°å®æ—¶çš„æ‚£è€…å¥åº·çŠ¶å†µæ¦‚è¿°ã€ä¸ªæ€§åŒ–åŒ»ç–—å’Œæ—©æœŸè¯Šæ–­ã€æ¨¡æ‹Ÿæ²»ç–—ç»“æœå’Œä¼˜åŒ–æ‰‹æœ¯è§„åˆ’ï¼Œä¸ºæ•°å­—åŒæŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚<details>
<summary>Abstract</summary>
Digital twin technology has is anticipated to transform healthcare, enabling personalized medicines and support, earlier diagnoses, simulated treatment outcomes, and optimized surgical plans. Digital twins are readily gaining traction in industries like manufacturing, supply chain logistics, and civil infrastructure. Not in patient care, however. The challenge of modeling complex diseases with multimodal patient data and the computational complexities of analyzing it have stifled digital twin adoption in the biomedical vertical. Yet, these major obstacles can potentially be handled by approaching these models in a different way. This paper proposes a novel framework for addressing the barriers to clinical twin modeling created by computational costs and modeling complexities. We propose structuring patient health data as a knowledge graph and using closed-form continuous-time liquid neural networks, for real-time analytics. By synthesizing multimodal patient data and leveraging the flexibility and efficiency of closed form continuous time networks and knowledge graph ontologies, our approach enables real time insights, personalized medicine, early diagnosis and intervention, and optimal surgical planning. This novel approach provides a comprehensive and adaptable view of patient health along with real-time analytics, paving the way for digital twin simulations and other anticipated benefits in healthcare.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šæ•°å­—åŒèƒæŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ã€‹æ•°å­—åŒèƒæŠ€æœ¯å·²è¢«é¢„æµ‹å°†é‡ç‚¹æ”¹å˜åŒ»ç–—é¢†åŸŸï¼Œä½¿å¾—ä¸ªæ€§åŒ–è¯ç‰©å’Œæ”¯æŒã€æ—©æœŸè¯Šæ–­ã€æ¨¡æ‹Ÿæ²»ç–—ç»“æœå’Œä¼˜åŒ–æ‰‹æœ¯è§„åˆ’ç­‰became possibleã€‚ç„¶è€Œï¼Œåœ¨ç—…äººæŠ¤ç†æ–¹é¢ï¼Œæ•°å­—åŒèƒæŠ€æœ¯çš„æ™®åŠå—åˆ°äº†è®¸å¤šæŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯æ¨¡æ‹Ÿå¤æ‚çš„ç–¾ç—…å’Œå¤šæ¨¡æ€ç—…äººæ•°æ®çš„è®¡ç®—å¤æ‚æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›ä¸»è¦éšœç¢å¯ä»¥é€šè¿‡ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç”¨äºè§£å†³åœ¨è®¡ç®—æˆæœ¬å’Œæ¨¡å‹å¤æ‚æ€§ç­‰æ–¹é¢é˜»ç¢ä¸´åºŠåŒèƒæ¨¡å‹çš„éšœç¢ã€‚æˆ‘ä»¬æè®®å°†ç—…äººå¥åº·æ•°æ®ç»“æ„åŒ–ä¸ºçŸ¥è¯†å›¾ï¼Œå¹¶ä½¿ç”¨é—­åˆå½¢æ—¶é—´è¿ç»­ç¥ç»ç½‘ç»œï¼Œä»¥å®ç°å®æ—¶åˆ†æã€‚é€šè¿‡å¯¹å¤šæ¨¡æ€ç—…äººæ•°æ®è¿›è¡Œsynthesizeï¼Œå¹¶åˆ©ç”¨closed-form continuous-time liquid neural networkså’ŒçŸ¥è¯†å›¾ ontologiesçš„çµæ´»æ€§å’Œé«˜æ•ˆæ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å®ç°å®æ—¶çš„åŒ»ç–—æ¢ç´¢å’Œä¸ªæ€§åŒ–åŒ»ç–—ã€‚æœ¬æ–‡çš„æ–°æ–¹æ³•å¯ä»¥ä¸ºåŒ»ç–—é¢†åŸŸæä¾›ä¸€ä¸ªæ™®é€‚å’Œå¯å˜çš„ç—…äººå¥åº·è§†å›¾ï¼ŒåŒæ—¶å®ç°å®æ—¶çš„åˆ†æï¼Œä¸ºæ•°å­—åŒèƒ simulationså’Œå…¶ä»–é¢„æœŸçš„å¥åº·åŒ»ç–—å¸¦æ¥äº†æ–°çš„æœºé‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Fault-Monitoring-in-Passive-Optical-Networks-using-Machine-Learning-Techniques"><a href="#Fault-Monitoring-in-Passive-Optical-Networks-using-Machine-Learning-Techniques" class="headerlink" title="Fault Monitoring in Passive Optical Networks using Machine Learning Techniques"></a>Fault Monitoring in Passive Optical Networks using Machine Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03945">http://arxiv.org/abs/2307.03945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khouloud Abdelli, Carsten Tropschug, Helmut Griesser, Stephan Pachnicke</li>
<li>for: æé«˜Passive Optical Networkï¼ˆPONï¼‰ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå¯é æ€§ï¼Œé™ä½æœåŠ¡æä¾›å•†æˆ–è¿ç»´å•†é¢ä¸´çš„è´¢åŠ¡æŸå¤±é£é™©ã€‚</li>
<li>methods: åˆ©ç”¨æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æŠ€æœ¯è¿›è¡ŒPONç³»ç»Ÿæ•…éšœç›‘æµ‹ï¼Œå¹¶é€šè¿‡å®éªŒOptical Time Domain Reflectometryï¼ˆOTDRï¼‰æ•°æ®éªŒè¯å…¶æ•ˆæœã€‚</li>
<li>results: é€šè¿‡MLæ–¹æ³•å®ç°PONç³»ç»Ÿæ•…éšœç›‘æµ‹ï¼Œå¯ä»¥å‡å°‘æœåŠ¡ä¸­æ–­æ—¶çš„è´¢åŠ¡æŸå¤±é£é™©ï¼Œå¹¶æé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œç¨³å®šæ€§ã€‚<details>
<summary>Abstract</summary>
Passive optical network (PON) systems are vulnerable to a variety of failures, including fiber cuts and optical network unit (ONU) transmitter/receiver failures. Any service interruption caused by a fiber cut can result in huge financial losses for service providers or operators. Identifying the faulty ONU becomes difficult in the case of nearly equidistant branch terminations because the reflections from the branches overlap, making it difficult to distinguish the faulty branch given the global backscattering signal. With increasing network size, the complexity of fault monitoring in PON systems increases, resulting in less reliable monitoring. To address these challenges, we propose in this paper various machine learning (ML) approaches for fault monitoring in PON systems, and we validate them using experimental optical time domain reflectometry (OTDR) data.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ¿€å…‰ç½‘ç»œï¼ˆPONï¼‰ç³»ç»Ÿå—åˆ°å¤šç§æ•…éšœçš„å¨èƒï¼ŒåŒ…æ‹¬çº¤ç»´æ–­å’ŒOptical Network Unitï¼ˆONUï¼‰å‘é€å™¨/æ¥æ”¶å™¨æ•…éšœã€‚ä»»ä½•çº¤ç»´æ–­å¯¼è‡´çš„æœåŠ¡ä¸­æ–­å¯èƒ½ä¼šå¯¹æœåŠ¡æä¾›å•†æˆ–è¿è¥å•†é€ æˆå·¨å¤§çš„ç»æµæŸå¤±ã€‚åœ¨åˆ†æ”¯ç»“æŸå¤„ nearly equidistantçš„æƒ…å†µä¸‹ï¼Œç¼ºé™·çš„ONU diffficult to identifyï¼Œå› ä¸ºåˆ†æ”¯ reflection overlapï¼Œä½¿å¾—ä¸å¯ä»¥é€šè¿‡å…¨çƒåå°„ä¿¡å·æ¥ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸å‡ºç¼ºé™·åˆ†æ”¯ã€‚éšç€ç½‘ç»œè§„æ¨¡çš„å¢åŠ ï¼ŒPONç³»ç»Ÿä¸­çš„ç¼ºé™·ç›‘æµ‹å¤æ‚åº¦å¢åŠ ï¼Œå¯¼è‡´ç›‘æµ‹å˜å¾—æ›´åŠ ä¸å¯é ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰çš„å¤šç§ç¼ºé™·ç›‘æµ‹æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®éªŒoptical time domain reflectometryï¼ˆOTDRï¼‰æ•°æ® validate themã€‚
</details></li>
</ul>
<hr>
<h2 id="Rosko-Row-Skipping-Outer-Products-for-Sparse-Matrix-Multiplication-Kernels"><a href="#Rosko-Row-Skipping-Outer-Products-for-Sparse-Matrix-Multiplication-Kernels" class="headerlink" title="Rosko: Row Skipping Outer Products for Sparse Matrix Multiplication Kernels"></a>Rosko: Row Skipping Outer Products for Sparse Matrix Multiplication Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03930">http://arxiv.org/abs/2307.03930</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vnatesh/rosko">https://github.com/vnatesh/rosko</a></li>
<li>paper_authors: Vikas Natesh, Andrew Sabot, H. T. Kung, Mark Ting</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æé«˜æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„è®¡ç®—å’Œå†…å­˜è®¿é—®éœ€æ±‚ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨äº†row skipping outer productsï¼ˆRoskoï¼‰æ¥ derivate sparse matrix multiplicationï¼ˆSpMMï¼‰kernelsï¼Œä»¥é™ä½DNNsçš„è®¡ç®—å’Œå†…å­˜è®¿é—®éœ€æ±‚ã€‚Roskoå¯ä»¥åœ¨ç¨‹å¼æ‰§è¡Œæ—¶ skip entire row computationsï¼Œå¹¶ä¸”å…·æœ‰ä½ç¼“å­˜ç®¡ç†å¼€é”€ã€‚</li>
<li>results: Rosko kernelså¯ä»¥ä¸å…¶ä»–outer product schedulingæ–¹æ³•ç»“åˆä½¿ç”¨ï¼Œå¹¶ä¸”å¯ä»¥å°†å…¶ä»–æ–¹æ³•çš„è®¡ç®— skipped by using Roskoçš„packing formatã€‚Rosko kernelsåœ¨å®é™…ç¡¬ä»¶ä¸Šæ¯” EXISTS auto-tuningå’Œæœç´¢åŸºäºè§£å†³æ–¹æ¡ˆå’Œå•†ä¸šä¾›åº”é“¾çš„ vendor-optimized åº“æ¥çš„æ€§èƒ½æ›´å¥½ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„ç¥ç»ç½‘ç»œè´Ÿè½½ä¸Šå®ç°æ›´é«˜çš„æ‰§è¡Œé€Ÿåº¦ï¼Œè¾¾åˆ°6.5å€çš„æ—¶é—´ä¼˜åŒ–ã€‚<details>
<summary>Abstract</summary>
We propose Rosko -- row skipping outer products -- for deriving sparse matrix multiplication (SpMM) kernels in reducing computation and memory access requirements of deep neural networks (DNNs). Rosko allows skipping of entire row computations during program execution with low sparsity-management overheads. We analytically derive sparse CPU kernels that adapt to given hardware characteristics to effectively utilize processor cores and minimize data movement without the need for auto-tuning or search space exploration. Rosko can be integrated with other outer product scheduling methods, allowing them to leverage row skipping by using Rosko's packing format to skip unnecessary computation.   Rosko kernels outperform existing auto-tuning and search-based solutions as well as state-of-the-art vendor-optimized libraries on real hardware across a variety of neural network workloads. For matrices with sparsities ranging from 65% to 99.8% typically found in machine learning, Rosko kernels achieve up to a 6.5x runtime reduction on Intel and ARM CPUs.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†Roskoï¼Œå®ƒæ˜¯è·³è¿‡å¤–ç§¯æ ˆçš„æ–¹æ³•ï¼Œç”¨äºè·å¾—æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ä¸­çš„ç®€çº¦çŸ©é˜µä¹˜æ³•ï¼ˆSpMMï¼‰å†…æ ¸ã€‚Roskoå¯ä»¥åœ¨ç¨‹å¼æ‰§è¡Œæ—¶è·³è¿‡æ•´ä¸ªè¡Œçš„è®¡ç®—ï¼Œå…·æœ‰ä½ç®€çº¦ç®¡ç†æˆæœ¬ã€‚æˆ‘ä»¬åˆ†ææ€§åœ° derivatesparse CPUå†…æ ¸ï¼Œå¯ä»¥æ ¹æ®ç¡¬ä»¶ç‰¹æ€§æ¥æœ‰æ•ˆåˆ©ç”¨å¤„ç†å™¨æ ¸å¿ƒå’Œå‡å°‘èµ„æ–™ç§»åŠ¨ï¼Œæ— éœ€è¿›è¡Œè‡ªåŠ¨è°ƒæ•´æˆ–æœç´¢ç©ºé—´æ¢ç´¢ã€‚Roskoå¯ä»¥ä¸å…¶ä»–å¤–ç§¯æ ˆè°ƒåº¦æ–¹æ³•ç»“åˆï¼Œè®©å®ƒä»¬åˆ©ç”¨Roskoçš„å°åŒ…æ ¼å¼è·³è¿‡æ— éœ€è®¡ç®—ã€‚Roskoå†…æ ¸æ¯”ç°æœ‰çš„è‡ªåŠ¨è°ƒæ•´å’Œæœç´¢åŸºäºè§£å†³æ–¹æ¡ˆä»¥åŠå•†ä¸šåŒ–ä¼˜åŒ–åº“åœ¨çœŸå®ç¡¬ä»¶ä¸Šè¡¨ç°æ›´å¥½ï¼Œå¯¹äºå…·æœ‰65%åˆ°99.8%çš„ç®€çº¦ç‡ï¼Œé€šå¸¸è§äºæœºå™¨å­¦ä¹  tasks ä¸­ï¼ŒRoskoå†…æ ¸åœ¨è‹±ç‰¹å°”å’ŒARM CPUä¸Šå¯ä»¥è·å¾—è‡³å¤š6.5å€çš„æ‰§è¡Œæ—¶é—´ä¼˜åŒ–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Fairness-Aware-Graph-Neural-Networks-A-Survey"><a href="#Fairness-Aware-Graph-Neural-Networks-A-Survey" class="headerlink" title="Fairness-Aware Graph Neural Networks: A Survey"></a>Fairness-Aware Graph Neural Networks: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03929">http://arxiv.org/abs/2307.03929</a></li>
<li>repo_url: None</li>
<li>paper_authors: April Chen, Ryan A. Rossi, Namyong Park, Puja Trivedi, Yu Wang, Tong Yu, Sungchul Kim, Franck Dernoncourt, Nesreen K. Ahmed<br>for: This paper focuses on improving the fairness of Graph Neural Networks (GNNs) by examining and categorizing fairness techniques for GNNs.methods: The paper discusses previous work on fair GNN models and techniques, including those that focus on improving fairness during preprocessing, training, or post-processing. The paper also introduces an intuitive taxonomy for fairness evaluation metrics.results: The paper highlights the advantages and intuition of using fairness techniques in GNNs, and summarizes graph datasets that are useful for benchmarking the fairness of GNN models. The paper also identifies key open problems and challenges that remain to be addressed.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have become increasingly important due to their representational power and state-of-the-art predictive performance on many fundamental learning tasks. Despite this success, GNNs suffer from fairness issues that arise as a result of the underlying graph data and the fundamental aggregation mechanism that lies at the heart of the large class of GNN models. In this article, we examine and categorize fairness techniques for improving the fairness of GNNs. Previous work on fair GNN models and techniques are discussed in terms of whether they focus on improving fairness during a preprocessing step, during training, or in a post-processing phase. Furthermore, we discuss how such techniques can be used together whenever appropriate, and highlight the advantages and intuition as well. We also introduce an intuitive taxonomy for fairness evaluation metrics including graph-level fairness, neighborhood-level fairness, embedding-level fairness, and prediction-level fairness metrics. In addition, graph datasets that are useful for benchmarking the fairness of GNN models are summarized succinctly. Finally, we highlight key open problems and challenges that remain to be addressed.
</details>
<details>
<summary>æ‘˜è¦</summary>
graph neural networks (GNNs) åœ¨æœ€è¿‘å‡ å¹´å†…å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œè¿™æ˜¯å› ä¸ºå®ƒä»¬åœ¨è®¸å¤šåŸºæœ¬å­¦ä¹ ä»»åŠ¡ä¸Šå…·æœ‰ä»£è¡¨åŠ›å’ŒçŠ¶æ€é€Ÿåº¦ã€‚ç„¶è€Œï¼ŒGNNs å—åˆ°å…¬å¹³é—®é¢˜çš„å½±å“ï¼Œè¿™äº›é—®é¢˜ arise ç”±ä¸‹é¢çš„å›¾æ•°æ®å’ŒåŸºæœ¬èšåˆæœºåˆ¶ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°å’Œåˆ†ç±» fairness æŠ€æœ¯ï¼Œä»¥æé«˜ GNNs çš„å…¬å¹³æ€§ã€‚å…ˆå‰çš„ fair GNN æ¨¡å‹å’ŒæŠ€æœ¯è¢«åˆ†ä¸ºä¸‰ç±»ï¼šåœ¨é¢„å¤„ç†é˜¶æ®µã€åœ¨è®­ç»ƒé˜¶æ®µå’Œåœ¨åå¤„ç†é˜¶æ®µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†è¿™äº›æŠ€æœ¯å¯ä»¥åœ¨åˆé€‚çš„æ—¶å€™ä¸€èµ·ä½¿ç”¨ï¼Œå¹¶é«˜äº®äº†å…¶ä¼˜åŠ¿å’Œç›´è§‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªç›´è§‚çš„å…¬å¹³è¯„ä»·åº¦é‡åˆ†ç±»ï¼ŒåŒ…æ‹¬å›¾çº§å…¬å¹³ã€é‚»å±…çº§å…¬å¹³ã€åµŒå…¥çº§å…¬å¹³å’Œé¢„æµ‹çº§å…¬å¹³åº¦é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç®€è¦ä»‹ç»äº†ä¸€äº›ç”¨äºè¯„ä¼° GNN æ¨¡å‹å…¬å¹³æ€§çš„å›¾æ•°æ®ã€‚æœ€åï¼Œæˆ‘ä»¬é«˜äº®äº†ä¸€äº›æœªè§£å†³çš„é—®é¢˜å’ŒæŒ‘æˆ˜ã€‚Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Fast-Empirical-Scenarios"><a href="#Fast-Empirical-Scenarios" class="headerlink" title="Fast Empirical Scenarios"></a>Fast Empirical Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03927">http://arxiv.org/abs/2307.03927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Multerer, Paul Schneider, Rohan Sen</li>
<li>for: ä»å¤§å‹é«˜ç»´æ•°æ®ä¸­æå–ä¸€å°é‡è¡¨ç¤ºæ€§çš„æ–¹æ¡ˆï¼Œä»¥ä¾¿è¿›è¡Œå¯é çš„enario-basedæ¨¡å‹å’Œé«˜ç»´æ•°å­¦ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°ã€‚</li>
<li>methods: æå‡ºäº†ä¸¤ç§æ–°çš„ç®—æ³•ï¼Œç¬¬ä¸€ç§å¯ä»¥æ‰¾åˆ°å°šæœªè¢«è§‚å¯Ÿåˆ°çš„enarioï¼Œå¹¶æä¾›äº†åŸºäºenarioçš„åæ–¹å·®çŸ©é˜µè¡¨ç¤ºï¼›ç¬¬äºŒç§é€‰æ‹©äº†å·²ç»å®ç°çš„ä¸–ç•ŒçŠ¶æ€ä¸­é‡è¦çš„æ•°æ®ç‚¹ï¼Œå¹¶ä¸æ›´é«˜é˜¶sample momentä¿¡æ¯ç›¸ç¬¦ã€‚</li>
<li>results: å¯¹æ¯”è¾ƒå‡ ç§ç°æœ‰ç®—æ³•ï¼Œæå‡ºçš„ä¸¤ç§ç®—æ³•å…·æœ‰é«˜æ•ˆè®¡ç®—å’Œå¯é çš„enario-basedæ¨¡å‹ç‰¹ç‚¹ï¼Œå¹¶åœ¨è‚¡ç¥¨æŠ•èµ„ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚<details>
<summary>Abstract</summary>
We seek to extract a small number of representative scenarios from large and high-dimensional panel data that are consistent with sample moments. Among two novel algorithms, the first identifies scenarios that have not been observed before, and comes with a scenario-based representation of covariance matrices. The second proposal picks important data points from states of the world that have already realized, and are consistent with higher-order sample moment information. Both algorithms are efficient to compute, and lend themselves to consistent scenario-based modeling and high-dimensional numerical integration. Extensive numerical benchmarking studies and an application in portfolio optimization favor the proposed algorithms.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬å¯»æ±‚ä»å¤§é‡é«˜ç»´æ‰¹å¤„ç†æ•°æ®ä¸­æå–ä¸€å°é‡è¡¨ç¤ºæ€§çš„æƒ…æ™¯ï¼Œè¿™äº›æƒ…æ™¯ä¸æ ·æœ¬å¹‚åº”äºä¸€è‡´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°ç®—æ³•ï¼Œç¬¬ä¸€ç§å¯ä»¥æ‰¾åˆ°æ²¡æœ‰è¢«è§‚å¯Ÿåˆ°çš„æƒ…æ™¯ï¼ŒåŒæ—¶æä¾›äº†æƒ…æ™¯åŸºäºåæ–¹å·®çŸ©é˜µçš„è¡¨ç¤ºæ–¹å¼ã€‚ç¬¬äºŒç§ç®—æ³•é€‰æ‹©å·²ç»å®ç°çš„ä¸–ç•ŒçŠ¶æ€ä¸­é‡è¦çš„æ•°æ®ç‚¹ï¼Œå¹¶ä¸é«˜é˜¶æ ·æœ¬å¹‚ä¿¡æ¯ä¸€è‡´ã€‚è¿™ä¸¤ç§ç®—æ³•éƒ½å…·æœ‰è®¡ç®—æ•ˆç‡ï¼Œé€‚ç”¨äºä¸€è‡´çš„enario-basedæ¨¡å‹å’Œé«˜ç»´æ•°å­— Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°ã€‚æˆ‘ä»¬åœ¨å¤§é‡è®¡ç®—å’Œè‚¡ç¥¨æŠ•èµ„åº”ç”¨ä¸­è¿›è¡Œäº†å¹¿æ³›çš„æ•°å€¼å¯¹æ¯”ç ”ç©¶ï¼Œè€Œè¿™ä¸¤ç§ç®—æ³•éƒ½å¾—åˆ°äº† preferenceã€‚
</details></li>
</ul>
<hr>
<h2 id="Training-Physics-Informed-Neural-Networks-via-Multi-Task-Optimization-for-Traffic-Density-Prediction"><a href="#Training-Physics-Informed-Neural-Networks-via-Multi-Task-Optimization-for-Traffic-Density-Prediction" class="headerlink" title="Training Physics-Informed Neural Networks via Multi-Task Optimization for Traffic Density Prediction"></a>Training Physics-Informed Neural Networks via Multi-Task Optimization for Traffic Density Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03920">http://arxiv.org/abs/2307.03920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Wang, A. K. Qin, Sajjad Shafiei, Hussein Dia, Adriana-Simona Mihaita, Hanna Grzybowska</li>
<li>for: ç”¨äºé¢„æµ‹äº¤é€šæµé‡</li>
<li>methods: ä½¿ç”¨å¤šä»»åŠ¡ä¼˜åŒ–ï¼ˆMTOï¼‰æ¡†æ¶ï¼Œåˆ›å»ºå¤šä¸ªè¾…åŠ©ä»»åŠ¡å¹¶ä¸ä¸»ä»»åŠ¡ä¸€èµ·è§£å†³</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æè®®çš„è®­ç»ƒæ¡†æ¶å¯ä»¥åœ¨æ¯”è¾ƒé™åˆ¶çš„æ•°æ®é‡ä¸‹æé«˜PINNçš„æ€§èƒ½<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) are a newly emerging research frontier in machine learning, which incorporate certain physical laws that govern a given data set, e.g., those described by partial differential equations (PDEs), into the training of the neural network (NN) based on such a data set. In PINNs, the NN acts as the solution approximator for the PDE while the PDE acts as the prior knowledge to guide the NN training, leading to the desired generalization performance of the NN when facing the limited availability of training data. However, training PINNs is a non-trivial task largely due to the complexity of the loss composed of both NN and physical law parts. In this work, we propose a new PINN training framework based on the multi-task optimization (MTO) paradigm. Under this framework, multiple auxiliary tasks are created and solved together with the given (main) task, where the useful knowledge from solving one task is transferred in an adaptive mode to assist in solving some other tasks, aiming to uplift the performance of solving the main task. We implement the proposed framework and apply it to train the PINN for addressing the traffic density prediction problem. Experimental results demonstrate that our proposed training framework leads to significant performance improvement in comparison to the traditional way of training the PINN.
</details>
<details>
<summary>æ‘˜è¦</summary>
Physics-informed neural networks (PINNs) æ˜¯ä¸€ä¸ªæ–°å…´çš„ç ”ç©¶é¢†åŸŸï¼Œå®ƒå°†ç‰©ç†æ³•åˆ™ incorporated åˆ° neural network (NN) çš„è®­ç»ƒä¸­ï¼Œä½¿å¾— NN èƒ½å¤Ÿæ›´å¥½åœ°é¢„æµ‹æ•°æ®é›†ä¸­çš„ç‰¹å¾ã€‚åœ¨ PINNs ä¸­ï¼ŒNN  acted as æ•°æ®é›†ä¸­çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œ PDE  acted as å¯¼èˆª NN è®­ç»ƒçš„çŸ¥è¯†ã€‚è¿™ä½¿å¾— NN åœ¨é¢ä¸´æœ‰é™çš„è®­ç»ƒæ•°æ®æ—¶èƒ½å¤Ÿè¾¾åˆ°æ›´å¥½çš„æ€»ä½“æ€§èƒ½ã€‚ç„¶è€Œï¼Œè®­ç»ƒ PINNs æ˜¯ä¸€ä¸ªérivialä»»åŠ¡ï¼Œä¸»è¦å› ä¸ºæŸå¤±å‡½æ•°çš„å¤æ‚æ€§ï¼Œå®ƒåŒ…æ‹¬ NN å’Œç‰©ç†æ³•åˆ™éƒ¨åˆ†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºå¤šä»»åŠ¡ä¼˜åŒ– (MTO) çš„æ–°çš„è®­ç»ƒæ¡†æ¶ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†å¤šä¸ªauxiliaryä»»åŠ¡ï¼Œå¹¶ä¸ä¸»ä»»åŠ¡ä¸€èµ·è§£å†³ï¼Œä½¿å¾—è§£å†³ä¸€ä¸ªä»»åŠ¡çš„æœ‰ç”¨çŸ¥è¯†å¯ä»¥åœ¨é€‚åº”æ¨¡å¼ä¸‹ä¼ é€’ç»™å¦ä¸€ä¸ªä»»åŠ¡ï¼Œä»¥æé«˜ä¸»ä»»åŠ¡çš„è§£å†³æ€§èƒ½ã€‚æˆ‘ä»¬å®ç°äº†æè®®çš„æ¡†æ¶ï¼Œå¹¶åº”ç”¨å®ƒæ¥è®­ç»ƒ PINN æ¥è§£å†³äº¤é€šå¯†åº¦é¢„æµ‹é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è®­ç»ƒæ¡†æ¶å¯ä»¥åœ¨ä¼ ç»Ÿè®­ç»ƒ PINN çš„åŸºç¡€ä¸Šè·å¾—æ˜¾è‘—æ€§èƒ½æå‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Deep-Q-â€“-Network-with-Multiclass-Classification-Algorithms"><a href="#Incorporating-Deep-Q-â€“-Network-with-Multiclass-Classification-Algorithms" class="headerlink" title="Incorporating Deep Q â€“ Network with Multiclass Classification Algorithms"></a>Incorporating Deep Q â€“ Network with Multiclass Classification Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03908">http://arxiv.org/abs/2307.03908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noopur Zambare, Ravindranath Sawane</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰æé«˜å¤šç±»åˆ†ç±»ç®—æ³•çš„åŠŸèƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨Kaggleçš„æ ‡å‡†æ•°æ®é›†åˆ›å»ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†DQNä¸ç°æœ‰çš„å¤šç±»åˆ†ç±»ç®—æ³•ç»“åˆä½¿ç”¨ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†Kaggleçš„æ ‡å‡†æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ ç­–ç•¥æ¥æé«˜å¤šç±»åˆ†ç±»ç²¾åº¦ã€‚</li>
<li>results: æœ¬ç ”ç©¶å‘ç°ï¼Œé€šè¿‡ä½¿ç”¨DQNï¼Œå¯ä»¥æé«˜å¤šç±»åˆ†ç±»ç²¾åº¦å’Œç¨³å®šæ€§ã€‚è¿™äº›ç»“æœå¯ä»¥ç”¨äºå„ç§é¢†åŸŸï¼ŒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿç‰©ä¿¡æ¯å­¦ã€‚ç‰¹åˆ«æ˜¯åœ¨é‡‘èé£é™©ç®¡ç†å’Œè´¢åŠ¡é¢„æµ‹æ–¹é¢ï¼Œå¯ä»¥ç”¨DQNæ¥é¢„æµ‹ä¼ä¸šé¢ä¸´financial distressçš„å¯èƒ½æ€§ã€‚<details>
<summary>Abstract</summary>
In this study, we explore how Deep Q-Network (DQN) might improve the functionality of multiclass classification algorithms. We will use a benchmark dataset from Kaggle to create a framework incorporating DQN with existing supervised multiclass classification algorithms. The findings of this study will bring insight into how deep reinforcement learning strategies may be used to increase multiclass classification accuracy. They have been used in a number of fields, including image recognition, natural language processing, and bioinformatics. This study is focused on the prediction of financial distress in companies in addition to the wider application of Deep Q-Network in multiclass classification. Identifying businesses that are likely to experience financial distress is a crucial task in the fields of finance and risk management. Whenever a business experiences serious challenges keeping its operations going and meeting its financial responsibilities, it is said to be in financial distress. It commonly happens when a company has a sharp and sustained recession in profitability, cash flow issues, or an unsustainable level of debt.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ä¸ªç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰å¦‚ä½•æ”¹å–„å¤šç±»åˆ†ç±»ç®—æ³•çš„åŠŸèƒ½ã€‚æˆ‘ä»¬å°†ä½¿ç”¨Kaggleçš„æ ‡å‡†æ•°æ®é›†åˆ›å»ºä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†åŒ…å«DQNä¸ç°æœ‰çš„å¤šç±»åˆ†ç±»ç®—æ³•ã€‚æˆ‘ä»¬çš„å‘ç°å°†æä¾›æ·±å…¥ç†è§£deep reinforcement learningç­–ç•¥å¦‚ä½•æé«˜å¤šç±»åˆ†ç±»ç²¾åº¦ã€‚è¿™äº›ç­–ç•¥åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿç‰©ä¿¡æ¯å¤„ç†ç­‰é¢†åŸŸéƒ½æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚æœ¬ç ”ç©¶çš„ç‰¹ç‚¹æ˜¯ç”¨DQNé¢„æµ‹å…¬å¸é¢ä¸´è´¢åŠ¡å±æœºçš„å¯èƒ½æ€§ï¼Œè€Œä¸æ˜¯ä»…ä»…æ˜¯å°†å…¶åº”ç”¨äºå¤šç±»åˆ†ç±»ã€‚å…¬å¸ç»å†ä¸¥é‡çš„è¿è¥å›°éš¾å’Œå±¥è¡Œè´¢åŠ¡è´£ä»»æ—¶ï¼Œè¢«ç§°ä¸ºè´¢åŠ¡å±æœºã€‚è¿™é€šå¸¸å‘ç”Ÿåœ¨å…¬å¸æ”¶å…¥ä¸‹é™ã€è´¢åŠ¡æµåŠ¨æ€§å›°éš¾æˆ–å€ºåŠ¡æ°´å¹³ä¸å¯æŒç»­æ—¶ã€‚
</details></li>
</ul>
<hr>
<h2 id="ScriptWorld-Text-Based-Environment-For-Learning-Procedural-Knowledge"><a href="#ScriptWorld-Text-Based-Environment-For-Learning-Procedural-Knowledge" class="headerlink" title="ScriptWorld: Text Based Environment For Learning Procedural Knowledge"></a>ScriptWorld: Text Based Environment For Learning Procedural Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03906">http://arxiv.org/abs/2307.03906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploration-lab/scriptworld">https://github.com/exploration-lab/scriptworld</a></li>
<li>paper_authors: Abhinav Joshi, Areeb Ahmad, Umang Pandey, Ashutosh Modi</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æ•™è‚²RLç®—æ³•ç†è§£æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¸¸è¯†çŸ¥è¯†å’Œè‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ScriptWorldç¯å¢ƒï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè„šæœ¬é›†çš„æ–‡æœ¬åŸºç¡€ç¯å¢ƒï¼Œç”¨äºæ•™è‚²RLç®—æ³•æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¸¸è¯†çŸ¥è¯†å’Œè‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ã€‚RLåŸºçº¿æ¨¡å‹&#x2F;ä»£ç†åœ¨Scriptworldç¯å¢ƒä¸­è¿›è¡Œæ¸¸æˆï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­çš„ç‰¹å¾æ¥è§£å†³æ–‡æœ¬åŸºç¡€ç¯å¢ƒä¸­çš„é—®é¢˜ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒåŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è¯­è¨€ç‰¹å¾å¯ä»¥å¸®åŠ©RLç®—æ³•è§£å†³æ—¥å¸¸ç”Ÿæ´»ä¸­çš„æ–‡æœ¬åŸºç¡€ç¯å¢ƒé—®é¢˜ã€‚<details>
<summary>Abstract</summary>
Text-based games provide a framework for developing natural language understanding and commonsense knowledge about the world in reinforcement learning based agents. Existing text-based environments often rely on fictional situations and characters to create a gaming framework and are far from real-world scenarios. In this paper, we introduce ScriptWorld: a text-based environment for teaching agents about real-world daily chores and hence imparting commonsense knowledge. To the best of our knowledge, it is the first interactive text-based gaming framework that consists of daily real-world human activities designed using scripts dataset. We provide gaming environments for 10 daily activities and perform a detailed analysis of the proposed environment. We develop RL-based baseline models/agents to play the games in Scriptworld. To understand the role of language models in such environments, we leverage features obtained from pre-trained language models in the RL agents. Our experiments show that prior knowledge obtained from a pre-trained language model helps to solve real-world text-based gaming environments. We release the environment via Github: https://github.com/Exploration-Lab/ScriptWorld
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Feature-selection-simultaneously-preserving-both-class-and-cluster-structures"><a href="#Feature-selection-simultaneously-preserving-both-class-and-cluster-structures" class="headerlink" title="Feature selection simultaneously preserving both class and cluster structures"></a>Feature selection simultaneously preserving both class and cluster structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03902">http://arxiv.org/abs/2307.03902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suchismita Das, Nikhil R. Pal</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æå‡ºä¸€ç§èƒ½åŒæ—¶è€ƒè™‘åˆ†ç±»å’Œå½’ç±»ç»“æ„çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œä»¥æé«˜åˆ†ç±»å’Œå½’ç±»æ€§èƒ½ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å®ç°ç‰¹å¾é€‰æ‹©ï¼ŒåŒæ—¶è€ƒè™‘åˆ†ç±»å’Œå½’ç±»ç»“æ„çš„ä¿æŒã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæè®®çš„ç‰¹å¾&#x2F;å¸¦é€‰æ‹©æ–¹æ³•å¯ä»¥é€‰æ‹©ä¸€ subset of ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾æ˜¯è‰¯å¥½çš„åˆ†ç±»å’Œå½’ç±»ã€‚<details>
<summary>Abstract</summary>
When a data set has significant differences in its class and cluster structure, selecting features aiming only at the discrimination of classes would lead to poor clustering performance, and similarly, feature selection aiming only at preserving cluster structures would lead to poor classification performance. To the best of our knowledge, a feature selection method that simultaneously considers class discrimination and cluster structure preservation is not available in the literature. In this paper, we have tried to bridge this gap by proposing a neural network-based feature selection method that focuses both on class discrimination and structure preservation in an integrated manner. In addition to assessing typical classification problems, we have investigated its effectiveness on band selection in hyperspectral images. Based on the results of the experiments, we may claim that the proposed feature/band selection can select a subset of features that is good for both classification and clustering.
</details>
<details>
<summary>æ‘˜è¦</summary>
å½“æ•°æ®é›†å…·æœ‰æ˜¾è‘—çš„ç±»å’Œå·®å¼‚ç»“æ„æ—¶ï¼Œä»…ä»…é€‰æ‹©ç”¨äºåˆ†ç±»çš„ç‰¹å¾ä¼šå¯¼è‡´ç¾¤é›†æ€§èƒ½å·®ï¼Œè€Œä»…ä»…ä¿æŒç¾¤é›†ç»“æ„çš„ç‰¹å¾é€‰æ‹©ä¹Ÿä¼šå¯¼è‡´åˆ†ç±»æ€§èƒ½å·®ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒLiteratureä¸­æ²¡æœ‰ä¸€ç§åŒæ—¶è€ƒè™‘ç±»åˆ†åŒ–å’Œç¾¤é›†ç»“æ„ä¿æŒçš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å°è¯•äº†bridgingè¿™ä¸ªå·®è·ï¼Œæå‡ºäº†åŸºäºç¥ç»ç½‘ç»œçš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŒæ—¶è€ƒè™‘ç±»åˆ†åŒ–å’Œç¾¤é›†ç»“æ„ä¿æŒã€‚é™¤äº†è¯„ä¼°å…¸å‹çš„åˆ†ç±»é—®é¢˜ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹å¸¦é€‰æ‹©åœ¨å¤šspectralå›¾åƒè¿›è¡Œäº†ç ”ç©¶ã€‚æ ¹æ®å®éªŒç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ç¡®è®¤ï¼Œæˆ‘ä»¬æè®®çš„ç‰¹å¾/å¸¦é€‰æ‹©æ–¹æ³•å¯ä»¥é€‰æ‹©ä¸€ä¸ªè‰¯å¥½çš„åˆ†ç±»å’Œç¾¤é›†ç»“æ„çš„å­é›†ã€‚
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-in-Physics-From-101-to-Progress-and-Perspective"><a href="#Active-Learning-in-Physics-From-101-to-Progress-and-Perspective" class="headerlink" title="Active Learning in Physics: From 101, to Progress, and Perspective"></a>Active Learning in Physics: From 101, to Progress, and Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03899">http://arxiv.org/abs/2307.03899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongcheng Ding, JosÃ© D. MartÃ­n-Guerrero, Yolanda Vives-Gilabert, Xi Chen</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†ä»‹ç»æ´»åŠ¨å­¦ä¹ ï¼ˆALï¼‰ï¼ŒåŒ…æ‹¬å®ƒçš„ç†è®ºå’Œæœ€æ–°è¿›å±•ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº† iterate é€‰æ‹©æ— æ ‡ç¤ºæ ·æœ¬ï¼Œå¹¶ç”±ä¸“å®¶è¿›è¡Œæ ‡æ³¨ã€‚è¿™ç§åè®®å¯ä»¥å¸®åŠ©æ¨¡å‹æ€§èƒ½æ›´é«˜ï¼Œæ¯”è®­ç»ƒæ‰€æœ‰æ ‡è®°æ ·æœ¬ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡æå‡ºäº†å°†æ´»åŠ¨å­¦ä¹ ä¸é‡å­æœºå™¨å­¦ä¹ ï¼ˆQLï¼‰èåˆçš„æƒ³æ³•ï¼Œä»¥å®ç°è¿™ä¸¤ä¸ªé¢†åŸŸä¹‹é—´çš„å…±èã€‚<details>
<summary>Abstract</summary>
Active Learning (AL) is a family of machine learning (ML) algorithms that predates the current era of artificial intelligence. Unlike traditional approaches that require labeled samples for training, AL iteratively selects unlabeled samples to be annotated by an expert. This protocol aims to prioritize the most informative samples, leading to improved model performance compared to training with all labeled samples. In recent years, AL has gained increasing attention, particularly in the field of physics. This paper presents a comprehensive and accessible introduction to the theory of AL reviewing the latest advancements across various domains. Additionally, we explore the potential integration of AL with quantum ML, envisioning a synergistic fusion of these two fields rather than viewing AL as a mere extension of classical ML into the quantum realm.
</details>
<details>
<summary>æ‘˜è¦</summary>
aktiv learning (AL) æ˜¯ä¸€å®¶æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç®—æ³•å®¶æ—ï¼Œæ¯”ç°ä»£äººå·¥æ™ºèƒ½æ›´æ—©å‡ºç°ã€‚ä¸åŒäºä¼ ç»Ÿçš„æ–¹æ³•ï¼ŒAL åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸éœ€è¦æ ‡æ³¨æ ·æœ¬ï¼Œè€Œæ˜¯é€æ¸é€‰æ‹©æ— æ ‡æ³¨æ ·æœ¬ï¼Œç”±ä¸“å®¶è¿›è¡Œæ ‡æ³¨ã€‚è¿™ä¸ªåè®®çš„ç›®çš„æ˜¯ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œä¸å…¨éƒ¨æ ‡æ³¨æ ·æœ¬è®­ç»ƒç›¸æ¯”ã€‚åœ¨æœ€è¿‘å‡ å¹´ï¼ŒAL åœ¨ç‰©ç†é¢†åŸŸè·å¾—äº†è¶Šæ¥è¶Šå¤šçš„æ³¨æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰©ç†é¢†åŸŸã€‚è¿™ç¯‡è®ºæ–‡æä¾›äº† AL çš„å®Œæ•´å’Œå¯è®¿é—®çš„ç†è®ºä»‹ç»ï¼ŒåŒæ—¶è¿˜æ¢è®¨äº† AL ä¸é‡å­ ML çš„å¯èƒ½çš„é›†æˆï¼Œè€Œä¸æ˜¯è§† AL ä¸ºĞºĞ»Ğ°ÑÑĞ¸ ML åœ¨é‡å­ä¸–ç•Œçš„æ‰©å±•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Incomplete-Utterance-Rewriting-as-Sequential-Greedy-Tagging"><a href="#Incomplete-Utterance-Rewriting-as-Sequential-Greedy-Tagging" class="headerlink" title="Incomplete Utterance Rewriting as Sequential Greedy Tagging"></a>Incomplete Utterance Rewriting as Sequential Greedy Tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06337">http://arxiv.org/abs/2307.06337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunshan Chen</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†è§£å†³ incomplete utterance rewriting é—®é¢˜ï¼Œå³åœ¨å¯¹è¯ä¸­æå–ä¿¡æ¯çš„é—®é¢˜ã€‚</li>
<li>methods: è¿™ä¸ªæ¨¡å‹ä½¿ç”¨ sequence tagging æ–¹æ³•ï¼Œå¯ä»¥æ›´å¥½åœ°ä»å¯¹è¯ä¸­æå–ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº† speaker-aware embeddingï¼Œä»¥æ¨¡å‹è¯´è¯äººçš„å˜åŒ–ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³çš„ nine restoration scoresï¼Œè€Œå…¶ä»– metric scores ä¸ä¹‹å‰çš„çŠ¶æ€OF-the-artæ¨¡å‹ç›¸æ¯”å¯è§‚ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„æ¨¡å‹ç®€å•ï¼Œåœ¨æ¨ç†é€Ÿåº¦æ–¹é¢ä¹Ÿè¶…è¿‡äº†å¤§å¤šæ•°ä¹‹å‰çš„æ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
The task of incomplete utterance rewriting has recently gotten much attention. Previous models struggled to extract information from the dialogue context, as evidenced by the low restoration scores. To address this issue, we propose a novel sequence tagging-based model, which is more adept at extracting information from context. Meanwhile, we introduce speaker-aware embedding to model speaker variation. Experiments on multiple public datasets show that our model achieves optimal results on all nine restoration scores while having other metric scores comparable to previous state-of-the-art models. Furthermore, benefitting from the model's simplicity, our approach outperforms most previous models on inference speed.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œ incomplete utterance rewriting â€åœ¨æœ€è¿‘å·²ç»è·å¾—äº†å¾ˆå¤šæ³¨æ„ã€‚ä»¥å‰çš„æ¨¡å‹å¾ˆéš¾ä»å¯¹è¯ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ä¸­æå–ä¿¡æ¯ï¼Œè¿™å¯ä»¥è§åˆ°ä½æ¢å¤å¾—åˆ†ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æè®®ä¸€ä¸ªæ–°çš„åºåˆ—æ ‡ç­¾åŸºäºæ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹æ›´èƒ½å¤Ÿä» ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ä¸­æå–ä¿¡æ¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†Speaker-aware embeddingæ¥æ¨¡å‹è¯´è¯è€…çš„å˜åŒ–ã€‚å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ‰€æœ‰ä¹ä¸ªæ¢å¤å¾—åˆ†ä¸Šå–å¾—äº†æœ€ä½³ç»“æœï¼Œè€Œå…¶ä»–æŒ‡æ ‡å¾—åˆ†ä¸è¿‡å¾€æœ€ä½³æ¨¡å‹ç›¸è¿‘ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„æ–¹æ³•ç®€å•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†é€Ÿåº¦æ–¹é¢è¶…è¶Šäº†å¤§å¤šæ•°å‰ä¸€ä»£æ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Improving-Prototypical-Part-Networks-with-Reward-Reweighing-Reselection-and-Retraining"><a href="#Improving-Prototypical-Part-Networks-with-Reward-Reweighing-Reselection-and-Retraining" class="headerlink" title="Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining"></a>Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03887">http://arxiv.org/abs/2307.03887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Netzorg, Jiaxun Li, Bin Yu</li>
<li>for: è¯¥paper aimed to improve the interpretability of deep learning models for image classification by using human feedback to fine-tune the prototypes.</li>
<li>methods: è¯¥paper proposed a novel method called R3-ProtoPNet, which combines reward-based reweighting, reselection, and retraining to align the modelâ€™s features with the updated prototypes.</li>
<li>results: è¯¥paper found that R3-ProtoPNet improves the overall consistency and meaningfulness of the prototypes, but lower the test predictive accuracy when used independently. However, when multiple R3-ProtoPNets are incorporated into an ensemble, the test predictive performance is increased while maintaining interpretability.<details>
<summary>Abstract</summary>
In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The first two steps are reward-based reweighting and reselection, which align prototypes with human feedback. The final step is retraining to realign the model's features with the updated prototypes. We find that R3-ProtoPNet improves the overall consistency and meaningfulness of the prototypes, but lower the test predictive accuracy when used independently. When multiple R3-ProtoPNets are incorporated into an ensemble, we find an increase in test predictive performance while maintaining interpretability.
</details>
<details>
<summary>æ‘˜è¦</summary>
recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The first two steps are reward-based reweighting and reselection, which align prototypes with human feedback. The final step is retraining to realign the model's features with the updated prototypes. We find that R3-ProtoPNet improves the overall consistency and meaningfulness of the prototypes, but lower the test predictive accuracy when used independently. When multiple R3-ProtoPNets are incorporated into an ensemble, we find an increase in test predictive performance while maintaining interpretability.Here's the translation in Traditional Chinese:åœ¨è¿‘å¹´æ¥ï¼Œæœ‰å¾ˆå¤šå·¥ä½œåœ¨å¼€å‘æ·±åº¦å¯è§£é‡Šçš„å›¾åƒåˆ†ç±»æ–¹æ³•ï¼Œä»¥å°†æ¨¡å‹çš„è¾“å‡ºæ˜ç¡®åœ°å¯¹å¯¹åº”çš„èµ„æ–™ç‰¹å¾è¿›è¡Œæ¨å¯¼ã€‚ä¸€ä¸ªå¦‚æ­¤æ–¹æ³•æ˜¯ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ñ–Ğ°Ğ»ÑŒéƒ¨åˆ†ç½‘ç»œï¼ˆProtoPNetï¼‰ï¼Œå®ƒå°è¯•æ ¹æ®è¾“å…¥å›¾åƒçš„æ„ä¹‰éƒ¨åˆ†è¿›è¡Œåˆ†ç±»ã€‚although this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. by collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. in place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. the first two steps are reward-based reweighting and reselection, which align prototypes with human feedback. the final step is retraining to realign the model's features with the updated prototypes. we find that R3-ProtoPNet improves the overall consistency and meaningfulness of the prototypes, but lower the test predictive accuracy when used independently. when multiple R3-ProtoPNets are incorporated into an ensemble, we find an increase in test predictive performance while maintaining interpretability.
</details></li>
</ul>
<hr>
<h2 id="On-Regularization-and-Inference-with-Label-Constraints"><a href="#On-Regularization-and-Inference-with-Label-Constraints" class="headerlink" title="On Regularization and Inference with Label Constraints"></a>On Regularization and Inference with Label Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03886">http://arxiv.org/abs/2307.03886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaifu Wang, Hangfeng He, Tin D. Nguyen, Piyush Kumar, Dan Roth</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦é’ˆå¯¹çš„æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„çº¦æŸé—®é¢˜ï¼Œå…·ä½“æ¥è¯´æ˜¯åœ¨ç»“æ„é¢„æµ‹é—®é¢˜ä¸­è¡¨è¾¾çº¦æŸçš„æ–¹æ³•ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨äº†ä¸¤ç§å¸¸è§çš„çº¦æŸç¼–ç ç­–ç•¥ï¼Œå³å¸¸è§„åŒ–å’Œçº¦æŸæ¨ç†ï¼Œå¹¶å¯¹å®ƒä»¬åœ¨æœºå™¨å­¦ä¹ ç®¡é“ä¸­çš„å½±å“è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>results: è®ºæ–‡è¡¨æ˜ï¼Œæ­£åˆ™åŒ–å¯ä»¥å‡å°‘æ³›åŒ–å·®è·ï¼Œä½†æ˜¯å®ƒä¼šåå¥½å°è¿åï¼Œå¯¼è‡´æ¨¡å‹åç¦»ä¼˜è´¨ç‚¹ã€‚è€Œå—çº¦æŸæ¨ç†åˆ™å¯ä»¥é™ä½äººå£é£é™©ï¼Œä»è€Œä½¿å¾—è¿åå˜æˆäº†ä¼˜åŠ¿ã€‚å› æ­¤ï¼Œè®ºæ–‡å»ºè®®åœ¨ä½¿ç”¨è¿™ä¸¤ç§æ–¹æ³•æ—¶ï¼Œå¯ä»¥å…±åŒä½¿ç”¨å®ƒä»¬ï¼Œå¹¶åœ¨æŸäº›æ¡ä»¶ä¸‹ä½¿ç”¨çº¦æŸæ¨ç†æ¥è¡¥å¿æ­£åˆ™åŒ–å¼•å…¥çš„åè§ã€‚<details>
<summary>Abstract</summary>
Prior knowledge and symbolic rules in machine learning are often expressed in the form of label constraints, especially in structured prediction problems. In this work, we compare two common strategies for encoding label constraints in a machine learning pipeline, regularization with constraints and constrained inference, by quantifying their impact on model performance. For regularization, we show that it narrows the generalization gap by precluding models that are inconsistent with the constraints. However, its preference for small violations introduces a bias toward a suboptimal model. For constrained inference, we show that it reduces the population risk by correcting a model's violation, and hence turns the violation into an advantage. Given these differences, we further explore the use of two approaches together and propose conditions for constrained inference to compensate for the bias introduced by regularization, aiming to improve both the model complexity and optimal risk.
</details>
<details>
<summary>æ‘˜è¦</summary>
Prior knowledgeå’Œç¬¦å·è§„åˆ™åœ¨æœºå™¨å­¦ä¹ ä¸­ç»å¸¸è¡¨è¾¾ä¸ºæ ‡ç­¾çº¦æŸï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“æ„é¢„æµ‹é—®é¢˜ä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ç§å¸¸è§çš„æ ‡ç­¾çº¦æŸç¼–ç ç­–ç•¥åœ¨æœºå™¨å­¦ä¹ ç®¡é“ä¸­çš„å½±å“ï¼Œå³è§„èŒƒå‡å°‘å’Œå—çº¦æŸçš„æ¨ç†ã€‚å¯¹äºè§„èŒƒï¼Œæˆ‘ä»¬è¡¨æ˜äº†å®ƒå¯ä»¥é˜²æ­¢æ¨¡å‹ä¸çº¦æŸä¸ä¸€è‡´çš„æƒ…å†µï¼Œä»è€Œç¼©å°æ³›åŒ–å·®ã€‚ä½†æ˜¯ï¼Œå®ƒä¼šåå¥½å°è§„æ¨¡çš„è¿åï¼Œå¯¼è‡´æ¨¡å‹åå¥½ä¸€ä¸ªä¸ä½³çš„æ¨¡å‹ã€‚å¯¹äºå—çº¦æŸæ¨ç†ï¼Œæˆ‘ä»¬è¡¨æ˜äº†å®ƒå¯ä»¥é™ä½äººå£é£é™©ï¼Œé€šè¿‡çº¦æŸè¿åçš„ä¿®æ­£ï¼Œä½¿è¿åå˜æˆä¸€ä¸ªä¼˜åŠ¿ã€‚åŸºäºè¿™äº›å·®å¼‚ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†ä¸¤ç§æ–¹æ³•çš„åŒæ—¶ä½¿ç”¨ï¼Œå¹¶æå‡ºäº†é™åˆ¶æ¨ç†å¯ä»¥èµ„COMPENSATE FOR THE BIAS INTRODUCED BY REGULARIZATIONï¼Œä»¥æé«˜æ¨¡å‹å¤æ‚åº¦å’Œä¼˜åŒ–é£é™©ã€‚
</details></li>
</ul>
<hr>
<h2 id="Noisy-Tensor-Ring-approximation-for-computing-gradients-of-Variational-Quantum-Eigensolver-for-Combinatorial-Optimization"><a href="#Noisy-Tensor-Ring-approximation-for-computing-gradients-of-Variational-Quantum-Eigensolver-for-Combinatorial-Optimization" class="headerlink" title="Noisy Tensor Ring approximation for computing gradients of Variational Quantum Eigensolver for Combinatorial Optimization"></a>Noisy Tensor Ring approximation for computing gradients of Variational Quantum Eigensolver for Combinatorial Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03884">http://arxiv.org/abs/2307.03884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dheeraj Peddireddy, Utkarsh Priyam, Vaneet Aggarwal</li>
<li>for: æé«˜é‡å­ approximate optimization å’Œé‡å­å¯¹è§’åŒ–å™¨ï¼ˆVQEï¼‰çš„å¯æ‰©å±•æ€§ï¼Œçªç ´åˆ†ç±»åŠ¿èƒ½ Computational complexity limit.</li>
<li>methods: æè®®ä¸€ç§ç±»åˆ«è®¡ç®—æ–¹æ³•ï¼Œåˆ©ç”¨å‚æ•°ç§»ä½è§„åˆ™ï¼Œä»ç¯å½¢çŸ©é˜µä¸­è®¡ç®—æœŸæœ›å€¼ï¼Œä½¿ç”¨äºŒå…ƒä½å·ç§¯çŸ©é˜µæ¥è¡¨ç¤ºç¯å½¢çŸ©é˜µçš„å˜æ¢ã€‚</li>
<li>results: æ¯”è¾ƒåˆ†ç±»è®¡ç®—å’Œé‡å­è®¡ç®—çš„å¤æ‚åº¦ï¼Œæ˜¾ç¤ºè¿™ç§æ–¹æ³•å¯ä»¥å‡å°‘åˆ†ç±»è®¡ç®—çš„å¤æ‚åº¦ï¼Œä½¿å…¶å¯ä»¥æ›´å¿«åœ°è¯„ä¼°é‡å­ç®—æ³•çš„æ¢¯åº¦ã€‚<details>
<summary>Abstract</summary>
Variational Quantum algorithms, especially Quantum Approximate Optimization and Variational Quantum Eigensolver (VQE) have established their potential to provide computational advantage in the realm of combinatorial optimization. However, these algorithms suffer from classically intractable gradients limiting the scalability. This work addresses the scalability challenge for VQE by proposing a classical gradient computation method which utilizes the parameter shift rule but computes the expected values from the circuits using a tensor ring approximation. The parametrized gates from the circuit transform the tensor ring by contracting the matrix along the free edges of the tensor ring. While the single qubit gates do not alter the ring structure, the state transformations from the two qubit rotations are evaluated by truncating the singular values thereby preserving the structure of the tensor ring and reducing the computational complexity. This variation of the Matrix product state approximation grows linearly in number of qubits and the number of two qubit gates as opposed to the exponential growth in the classical simulations, allowing for a faster evaluation of the gradients on classical simulators.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œç²¾ç®€é‡å­ç®—æ³•ï¼Œå°¤å…¶æ˜¯é‡å­è¿‘ä¼¼ä¼˜åŒ–å’Œé‡å­å¯¹è§’å™¨ï¼ˆVQEï¼‰ï¼Œå·²ç»è¯æ˜äº†å®ƒä»¬åœ¨æ’åºé—®é¢˜ä¸­çš„è®¡ç®—ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œè¿™äº›ç®—æ³•å—åˆ°å¤å…¸æ— æ³•è®¡ç®—çš„æ¢¯åº¦æ‰€é™åˆ¶ï¼Œè¿™ä½¿å¾—æ‰©å±•æ€§å—åˆ°æŒ‘æˆ˜ã€‚æœ¬å·¥ä½œè§£å†³äº†VQEçš„æ‰©å±•æ€§é—®é¢˜ï¼Œæå‡ºä¸€ç§å¤å…¸æ¢¯åº¦è®¡ç®—æ–¹æ³•ï¼Œåˆ©ç”¨å‚æ•°ç§»åŠ¨è§„åˆ™ï¼Œå¹¶ä»å›¾é™æ€ç¯èŠ‚ä¸­è®¡ç®—å‡ºé¢„æœŸå€¼ã€‚å›¾é™æ€ç¯èŠ‚ä¸­çš„å‚æ•°é—¨ç”±å›¾é™æ€ç¯èŠ‚ä¸­çš„çŸ©é˜µæŠ˜ç¼©ï¼Œå•ä½é—¨ä¸æ”¹å˜ç¯èŠ‚ç»“æ„ï¼Œä½†ä¸¤ä¸ªé‡å­çŸ©é˜µçš„çŠ¶æ€è½¬æ¢åˆ™è¢«èˆ’ç¼“ï¼Œä»¥ä¿æŒç¯èŠ‚ç»“æ„å¹¶é™ä½è®¡ç®—å¤æ‚æ€§ã€‚è¿™ç§çŸ©é˜µäº§å“stateçš„æ‰©å±•å¢é•¿Linearlyåœ¨é‡å­çŸ©é˜µä¸­ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤å…¸ simulationsä¸­çš„æ‰©å±•å¢é•¿ exponentialï¼Œä½¿å¾—åœ¨å¤å…¸ simulators ä¸Šè¾ƒå¿«åœ°è¯„ä¼°æ¢¯åº¦ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-Supply-Chain-Optimization"><a href="#Large-Language-Models-for-Supply-Chain-Optimization" class="headerlink" title="Large Language Models for Supply Chain Optimization"></a>Large Language Models for Supply Chain Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03875">http://arxiv.org/abs/2307.03875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, Ishai Menache</li>
<li>for: The paper is written for supply chain operators and managers who need to interpret and explain the outcomes of optimization algorithms to stakeholders.</li>
<li>methods: The paper proposes a framework called OptiGuide that leverages Large Language Models (LLMs) to provide insights into the underlying optimization outcomes. The framework accepts queries in plain text and outputs explanations of the optimization results without requiring the transfer of proprietary data to the LLM.</li>
<li>results: The paper demonstrates the effectiveness of OptiGuide on a real server placement scenario within Microsoftâ€™s cloud supply chain. The results show that OptiGuide can provide accurate explanations of the optimization outcomes, and the proposed evaluation benchmark can be used to evaluate the accuracy of the LLM output in other scenarios.<details>
<summary>Abstract</summary>
Supply chain operations traditionally involve a variety of complex decision making problems. Over the last few decades, supply chains greatly benefited from advances in computation, which allowed the transition from manual processing to automation and cost-effective optimization. Nonetheless, business operators still need to spend substantial efforts in explaining and interpreting the optimization outcomes to stakeholders. Motivated by the recent advances in Large Language Models (LLMs), we study how this disruptive technology can help bridge the gap between supply chain automation and human comprehension and trust thereof. We design OptiGuide -- a framework that accepts as input queries in plain text, and outputs insights about the underlying optimization outcomes. Our framework does not forgo the state-of-the-art combinatorial optimization technology, but rather leverages it to quantitatively answer what-if scenarios (e.g., how would the cost change if we used supplier B instead of supplier A for a given demand?). Importantly, our design does not require sending proprietary data over to LLMs, which can be a privacy concern in some circumstances. We demonstrate the effectiveness of our framework on a real server placement scenario within Microsoft's cloud supply chain. Along the way, we develop a general evaluation benchmark, which can be used to evaluate the accuracy of the LLM output in other scenarios.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¾›åº”é“¾è¿è¥ä¼ ç»Ÿä¸Šæ¶‰åŠåˆ°è®¸å¤šå¤æ‚çš„å†³ç­–é—®é¢˜ã€‚è¿‡å»å‡ åå¹´ï¼Œä¾›åº”é“¾å—è®¡ç®—æŠ€æœ¯çš„è¿›æ­¥æ‰€åŠ©ï¼Œä»äººå·¥å¤„ç†è¿‡æ¸¡åˆ°è‡ªåŠ¨åŒ–å’Œæˆæœ¬æ•ˆæœä¼˜åŒ–ã€‚ç„¶è€Œï¼Œä¸šåŠ¡è¿è¥è€…ä»éœ€æŠ•å…¥å¾ˆå¤§çš„åŠªåŠ›æ¥è§£é‡Šå’Œç†è§£ä¼˜åŒ–ç»“æœï¼Œä»¥è·å¾—æŠ•èµ„è€…å’Œå®¢æˆ·çš„ä¿¡ä»»ã€‚é¼“åŠ±äºæœ€è¿‘çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯çš„è¿›æ­¥ï¼Œæˆ‘ä»¬ç ”ç©¶å¦‚ä½•ä½¿ç”¨è¿™ç§ç ´åæŠ€æœ¯æ¥bridgingä¾›åº”é“¾è‡ªåŠ¨åŒ–å’Œäººç±»ç†è§£ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬è®¾è®¡äº†OptiGuideæ¡†æ¶ï¼Œå®ƒå¯ä»¥æ¥å—æ™®é€šæ–‡æœ¬æŸ¥è¯¢ï¼Œå¹¶è¾“å‡ºä¾›åº”é“¾ä¼˜åŒ–ç»“æœçš„æ¦‚å¿µã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ä¼šæŠ›å¼ƒç°æœ‰çš„ç»„åˆä¼˜åŒ–æŠ€æœ¯ï¼Œè€Œæ˜¯åˆ©ç”¨å®ƒä»¬æ¥å›ç­”ä»€ä¹ˆæ—¶å€™çš„é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨ä¾›åº”å•†Bè€Œä¸æ˜¯ä¾›åº”å•†Aæ¥æ»¡è¶³æŸä¸ªéœ€æ±‚æ—¶ï¼Œæˆæœ¬ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿï¼‰ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è®¾è®¡ä¸éœ€è¦å°†ä¸“æœ‰æ•°æ®ä¼ é€’åˆ°LLMä¸­ï¼Œè¿™å¯èƒ½ä¼šåœ¨æŸäº›æƒ…å†µä¸‹æˆä¸ºéšç§é—®é¢˜ã€‚æˆ‘ä»¬åœ¨å¾®è½¯äº‘ä¾›åº”é“¾ä¸­è¿›è¡Œäº†å®é™…çš„æœåŠ¡åˆ†é…enarioï¼Œå¹¶åœ¨è¿‡ç¨‹ä¸­å¼€å‘äº†ä¸€ä¸ªé€šç”¨è¯„ä¼°æ ‡å‡†ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°å…¶ä»–åœºæ™¯ä¸­LLMè¾“å‡ºçš„å‡†ç¡®æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-using-Silver-Standard-Labels-for-Ki-67-Scoring-in-Digital-Pathology-A-Step-Closer-to-Widescale-Deployment"><a href="#Domain-Adaptation-using-Silver-Standard-Labels-for-Ki-67-Scoring-in-Digital-Pathology-A-Step-Closer-to-Widescale-Deployment" class="headerlink" title="Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital Pathology: A Step Closer to Widescale Deployment"></a>Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital Pathology: A Step Closer to Widescale Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03872">http://arxiv.org/abs/2307.03872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amanda Dy, Ngoc-Nhu Jennifer Nguyen, Seyed Hossein Mirjahanmardi, Melanie Dawe, Anthony Fyles, Wei Shi, Fei-Fei Liu, Dimitrios Androutsos, Susan Done, April Khademi<br>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æé«˜ Ki-67 PI åˆ†é…çš„ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²æ€§å’Œæ•ˆç‡ï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ ç³»ç»Ÿã€‚methods: è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé¢†åŸŸé€‚åº”ç®¡é“ï¼Œä½¿ç”¨æ— ç›‘ç£æ¡†æ¶ç”Ÿæˆç›®æ ‡é¢†åŸŸçš„é“¶æ ‡ç­¾ï¼Œä»¥å¢å¼ºæºé¢‘ç‡é“¶æ ‡ç­¾æ•°æ®çš„å­¦ä¹ æ•ˆæœã€‚results: æ¯”è¾ƒ SS Onlyã€GS Onlyã€Mixedã€GS+SS å’Œæˆ‘ä»¬çš„æè®®æ–¹æ³• SS+GS çš„äº”ç§è®­ç»ƒæ–¹æ¡ˆï¼ŒSS+GS æ–¹æ³•åœ¨ç›®æ ‡æ•°æ®ä¸Šæ˜¾ç¤ºå‡ºæœ€é«˜çš„ PI å‡†ç¡®ç‡ï¼ˆ95.9%ï¼‰å’Œæ›´ä¸€è‡´çš„ç»“æœï¼Œä¸ GS Only æ¨¡å‹åœ¨ç›®æ ‡æ•°æ®ä¸Šçš„è¡¨ç°æœ‰ statistically significant differenceï¼ˆp &lt; 0.05ï¼‰ã€‚<details>
<summary>Abstract</summary>
Deep learning systems have been proposed to improve the objectivity and efficiency of Ki- 67 PI scoring. The challenge is that while very accurate, deep learning techniques suffer from reduced performance when applied to out-of-domain data. This is a critical challenge for clinical translation, as models are typically trained using data available to the vendor, which is not from the target domain. To address this challenge, this study proposes a domain adaptation pipeline that employs an unsupervised framework to generate silver standard (pseudo) labels in the target domain, which is used to augment the gold standard (GS) source domain data. Five training regimes were tested on two validated Ki-67 scoring architectures (UV-Net and piNET), (1) SS Only: trained on target silver standard (SS) labels, (2) GS Only: trained on source GS labels, (3) Mixed: trained on target SS and source GS labels, (4) GS+SS: trained on source GS labels and fine-tuned on target SS labels, and our proposed method (5) SS+GS: trained on source SS labels and fine-tuned on source GS labels. The SS+GS method yielded significantly (p < 0.05) higher PI accuracy (95.9%) and more consistent results compared to the GS Only model on target data. Analysis of t-SNE plots showed features learned by the SS+GS models are more aligned for source and target data, resulting in improved generalization. The proposed pipeline provides an efficient method for learning the target distribution without manual annotations, which are time-consuming and costly to generate for medical images. This framework can be applied to any target site as a per-laboratory calibration method, for widescale deployment.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ ç³»ç»Ÿå·²è¢«æè®®ä»¥æé«˜åŸºå› 67PIåˆ†é…çš„ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²æ€§å’Œæ•ˆç‡ã€‚ç„¶è€Œï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨åŸŸå¤–æ•°æ®ä¸Šè¡¨ç°ä¸ä½³æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼Œè¿™æ˜¯å› ä¸ºæ¨¡å‹é€šå¸¸åœ¨ç”Ÿäº§å•†æä¾›çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè€Œä¸æ˜¯targetåŸŸçš„æ•°æ®ã€‚ä¸ºè§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé€‚åº”åŸŸpipelineï¼Œè¯¥pipelineä½¿ç”¨äº†æ— ç›‘ç£æ¡†æ¶ç”ŸæˆtargetåŸŸçš„é“¶æ ‡(pseudo)æ ‡ç­¾ï¼Œå¹¶å°†å…¶ç”¨äºå¢å¼ºæ¥è‡ªæºåŸŸçš„é‡‘æ ‡(GS)æ•°æ®ã€‚æœ¬ç ”ç©¶æµ‹è¯•äº†äº”ç§è®­ç»ƒæ–¹æ¡ˆï¼ŒåŒ…æ‹¬(1) SS Onlyï¼šåŸºäºtargeté“¶æ ‡(SS)æ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œ(2) GS Onlyï¼šåŸºäºæºGSæ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œ(3) Mixedï¼šåŸºäºtarget SSå’ŒæºGSæ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œ(4) GS+SSï¼šåŸºäºæºGSæ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨target SSæ ‡ç­¾ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥åŠæˆ‘ä»¬çš„æè®®æ–¹æ³•(5) SS+GSï¼šåŸºäºæºSSæ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æºGSæ ‡ç­¾ä¸Šè¿›è¡Œå¾®è°ƒã€‚SS+GSæ–¹æ³•åœ¨targetæ•°æ®ä¸Šå¾—åˆ°äº† statistically significant (p < 0.05) çš„PIå‡†ç¡®ç‡ï¼ˆ95.9%ï¼‰ï¼Œå¹¶ä¸”ä¸æºæ•°æ®çš„ç»“æœæ›´ä¸€è‡´ã€‚åˆ†æt-SNEå›¾è¡¨æ˜¾ç¤ºï¼ŒSS+GSæ¨¡å‹å­¦ä¹ çš„ç‰¹å¾æ›´åŠ é€‚åº”äºæºå’Œç›®æ ‡æ•°æ®ï¼Œå¯¼è‡´äº†æ”¹å–„çš„æ€»ä½“æ€§ã€‚æœ¬ipelineæä¾›äº†ä¸€ç§æ•ˆç‡çš„æ–¹æ³•æ¥å­¦ä¹ ç›®æ ‡åˆ†å¸ƒï¼Œä¸éœ€è¦æ‰‹åŠ¨ç”Ÿæˆæ˜‚è´µå’Œæ—¶é—´consumingçš„åŒ»å­¦å›¾åƒæ ‡ç­¾ã€‚è¿™ç§æ¡†æ¶å¯ä»¥åœ¨ä»»ä½•ç›®æ ‡ç«™ç‚¹ä¸Šåº”ç”¨ï¼Œä½œä¸ºå®¤å†…å‡†ç¡®æ–¹æ³•è¿›è¡Œå¤§è§„æ¨¡éƒ¨ç½²ã€‚
</details></li>
</ul>
<hr>
<h2 id="When-Do-Transformers-Shine-in-RL-Decoupling-Memory-from-Credit-Assignment"><a href="#When-Do-Transformers-Shine-in-RL-Decoupling-Memory-from-Credit-Assignment" class="headerlink" title="When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment"></a>When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03864">http://arxiv.org/abs/2307.03864</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/twni2016/memory-rl">https://github.com/twni2016/memory-rl</a></li>
<li>paper_authors: Tianwei Ni, Michel Ma, Benjamin Eysenbach, Pierre-Luc Bacon</li>
<li>for: è¿™ç§ç ”ç©¶æ—¨åœ¨è§£é‡ŠRLç®—æ³•ä¸­Transformer Architectureçš„æˆåŠŸåŸå› ï¼Œä»¥åŠæœªæ¥ç ”ç©¶å’Œbenchmarkè®¾è®¡çš„é‡è¦é¢†åŸŸã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†Formal definitions of memory lengthå’Œcredit assignment lengthæ¥æµ‹è¯•Transformer-based RLæ–¹æ³•çš„è¡¨ç°ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼ŒTransformerså¯ä»¥å¢å¼ºRLç®—æ³•çš„è®°å¿†èƒ½åŠ›ï¼Œå¯ä»¥æ‰©å±•åˆ°éœ€è¦è®°å¿†æ­¥éª¤1500ä¸ªçš„ä»»åŠ¡ã€‚ä½†æ˜¯ï¼ŒTransformersä¸ä¼šæ”¹å–„é•¿æœŸå½’å› ã€‚<details>
<summary>Abstract</summary>
Reinforcement learning (RL) algorithms face two distinct challenges: learning effective representations of past and present observations, and determining how actions influence future returns. Both challenges involve modeling long-term dependencies. The transformer architecture has been very successful to solve problems that involve long-term dependencies, including in the RL domain. However, the underlying reason for the strong performance of Transformer-based RL methods remains unclear: is it because they learn effective memory, or because they perform effective credit assignment? After introducing formal definitions of memory length and credit assignment length, we design simple configurable tasks to measure these distinct quantities. Our empirical results reveal that Transformers can enhance the memory capacity of RL algorithms, scaling up to tasks that require memorizing observations $1500$ steps ago. However, Transformers do not improve long-term credit assignment. In summary, our results provide an explanation for the success of Transformers in RL, while also highlighting an important area for future research and benchmark design.
</details>
<details>
<summary>æ‘˜è¦</summary>
reinforcement learning (RL) ç®—æ³•é¢ä¸´ä¸¤ä¸ªä¸åŒçš„æŒ‘æˆ˜ï¼šå­¦ä¹ è¿‡å»å’Œå½“å‰è§‚å¯Ÿçš„æœ‰æ•ˆè¡¨ç¤ºï¼Œä»¥åŠç¡®å®šè¡ŒåŠ¨å¯¹æœªæ¥è¿”å›çš„å½±å“ã€‚ä¸¤ä¸ªæŒ‘æˆ˜éƒ½æ¶‰åŠåˆ°æ¨¡å‹é•¿æœŸå…³ç³»ã€‚ transformer æ¶æ„åœ¨RLé¢†åŸŸä¸­å…·æœ‰éå¸¸å‡ºè‰²çš„è¡¨ç°ï¼Œä½†æ˜¯ä¸‹é¢çš„åŸå› ä»ç„¶ä¸æ¸…æ¥šï¼šæ˜¯å› ä¸ºå®ƒä»¬å­¦ä¹ æœ‰æ•ˆçš„è®°å¿†ï¼Œæˆ–è€…æ˜¯å› ä¸ºå®ƒä»¬å®ç°æœ‰æ•ˆçš„å‡†ç¡®åˆ†é…ï¼Ÿæˆ‘ä»¬ç»™å‡ºäº†æ­£å¼çš„å®šä¹‰ memory length å’Œ credit assignment lengthï¼Œç„¶åè®¾è®¡äº†ç®€å•å¯é…ç½®çš„ä»»åŠ¡æ¥æµ‹é‡è¿™ä¸¤ä¸ªç‰¹ç‚¹ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTransformers å¯ä»¥å¢å¼º RL ç®—æ³•çš„è®°å¿†å®¹é‡ï¼Œå¯ä»¥æ‰©å±•åˆ°éœ€è¦è®°å¿† observation 1500 æ­¥çš„ä»»åŠ¡ã€‚ä½†æ˜¯ï¼ŒTransformers ä¸ä¼šæ”¹å–„é•¿æœŸå‡†ç¡®åˆ†é…ã€‚ç®€å•æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç»“æœå¯ä»¥è§£é‡Š transformer åœ¨ RL ä¸­çš„æˆåŠŸï¼ŒåŒæ—¶ä¹Ÿæå‡ºäº†æœªæ¥ç ”ç©¶å’Œæ ‡å‡†åŒ–çš„æµ‹è¯•è®¾è®¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Memory-Immersed-Collaborative-Digitization-for-Area-Efficient-Compute-in-Memory-Deep-Learning"><a href="#Memory-Immersed-Collaborative-Digitization-for-Area-Efficient-Compute-in-Memory-Deep-Learning" class="headerlink" title="Memory-Immersed Collaborative Digitization for Area-Efficient Compute-in-Memory Deep Learning"></a>Memory-Immersed Collaborative Digitization for Area-Efficient Compute-in-Memory Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03863">http://arxiv.org/abs/2307.03863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shamma Nasrin, Maeesha Binte Hashem, Nastaran Darabi, Benjamin Parpillon, Farah Fahim, Wilfred Gomes, Amit Ranjan Trivedi</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æé«˜æ·±åº¦å­¦ä¹ æ¨å¯¼ä¸­çš„è®¡ç®—æ•ˆç‡ï¼Œé€šè¿‡å°†compute-in-memoryï¼ˆCiMï¼‰é˜µåˆ—ç”¨äºæ·±åº¦å­¦ä¹ æ¨å¯¼ï¼Œä»¥å‡å°‘å¤–éƒ¨å†…å­˜å­˜å–å’Œé¢ç§¯å¼€é”€ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†å†…å­˜å†…éƒ¨çš„æ½œåœ¨é˜»æŠ—bitçº¿æ¥å®ç°area-efficientçš„successive approximationï¼ˆSAï¼‰æ•°å­—åŒ–ï¼Œå¹¶å€Ÿç”±CiMé˜µåˆ—ä¹‹é—´çš„ååŒè¿ç®—æ¥å®ç°æ›´å¤šçš„å¹¶è¡Œè®¡ç®—å’Œé¢ç§¯ä¼˜åŒ–ã€‚</li>
<li>results: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†65å¥ˆç±³CMOSè¯•éªŒæ¿ï¼Œä¸40å¥ˆç±³èŠ‚ç‚¹5ä½SAR ADCå’Œ40å¥ˆç±³èŠ‚ç‚¹5ä½Flash ADCè¿›è¡Œæ¯”è¾ƒï¼Œç»“æœæ˜¾ç¤ºï¼Œè¿™ä¸ªè®¾è®¡éœ€è¦ç›¸å¯¹äº40å¥ˆç±³èŠ‚ç‚¹SAR ADCçš„é¢ç§¯å’Œèƒ½æºå‡å°‘ä¸º$\sim$25$\times$å’Œ$\sim$1.4$\times$ï¼Œç›¸å¯¹äº40å¥ˆç±³èŠ‚ç‚¹Flash ADCçš„é¢ç§¯å’Œèƒ½æºå‡å°‘ä¸º$\sim$51$\times$å’Œ$\sim$13$\times$ã€‚<details>
<summary>Abstract</summary>
This work discusses memory-immersed collaborative digitization among compute-in-memory (CiM) arrays to minimize the area overheads of a conventional analog-to-digital converter (ADC) for deep learning inference. Thereby, using the proposed scheme, significantly more CiM arrays can be accommodated within limited footprint designs to improve parallelism and minimize external memory accesses. Under the digitization scheme, CiM arrays exploit their parasitic bit lines to form a within-memory capacitive digital-to-analog converter (DAC) that facilitates area-efficient successive approximation (SA) digitization. CiM arrays collaborate where a proximal array digitizes the analog-domain product-sums when an array computes the scalar product of input and weights. We discuss various networking configurations among CiM arrays where Flash, SA, and their hybrid digitization steps can be efficiently implemented using the proposed memory-immersed scheme. The results are demonstrated using a 65 nm CMOS test chip. Compared to a 40 nm-node 5-bit SAR ADC, our 65 nm design requires $\sim$25$\times$ less area and $\sim$1.4$\times$ less energy by leveraging in-memory computing structures. Compared to a 40 nm-node 5-bit Flash ADC, our design requires $\sim$51$\times$ less area and $\sim$13$\times$ less energy.
</details>
<details>
<summary>æ‘˜è¦</summary>
In the proposed digitization scheme, CiM arrays utilize their parasitic bit lines to form a within-memory capacitive digital-to-analog converter (DAC) that enables area-efficient successive approximation (SA) digitization. CiM arrays collaborate by digitizing the analog-domain product-sums when one array computes the scalar product of input and weights.The proposed memory-immersed scheme can efficiently implement various networking configurations among CiM arrays, including Flash, SA, and their hybrid digitization steps. The results are demonstrated using a 65 nm CMOS test chip. Compared to a 40 nm-node 5-bit SAR ADC, our 65 nm design requires approximately 25 times less area and 1.4 times less energy. Compared to a 40 nm-node 5-bit Flash ADC, our design requires approximately 51 times less area and 13 times less energy.
</details></li>
</ul>
<hr>
<h2 id="A-Natural-Language-Processing-Approach-to-Malware-Classification"><a href="#A-Natural-Language-Processing-Approach-to-Malware-Classification" class="headerlink" title="A Natural Language Processing Approach to Malware Classification"></a>A Natural Language Processing Approach to Malware Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11032">http://arxiv.org/abs/2307.11032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritik Mehta, Olha JureÄkovÃ¡, Mark Stamp</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§hybridæ¨¡å‹ï¼Œå°†éšè—é©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰è®­ç»ƒäºæœºå™¨ç åºåˆ—ï¼Œå¹¶å°†å…¶ç”Ÿæˆçš„éšè—çŠ¶æ€åºåˆ—ä½œä¸ºå„ç§åˆ†ç±»å™¨çš„ç‰¹å¾ vectorã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†éšè—é©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰å’ŒRandom Forestsï¼ˆRFï¼‰ç­‰å¤šç§æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå¹¶å°†è¿™äº›æŠ€æœ¯ç»„åˆæˆä¸€ç§hybridæ¨¡å‹ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨è¿™ç§NLPåŸºäºçš„æ–¹æ³•å¯ä»¥åœ¨ä¸€ä¸ªå¤æ‚çš„æ¶æ„è½¯ä»¶é›†åˆä¸Šè¾¾åˆ°æœ€ä½³æ•ˆæœï¼ŒHMM-Random Forrestæ¨¡å‹åœ¨è¿™ä¸ªé›†åˆä¸Šå¾—åˆ°äº†æœ€ä½³ç»“æœã€‚<details>
<summary>Abstract</summary>
Many different machine learning and deep learning techniques have been successfully employed for malware detection and classification. Examples of popular learning techniques in the malware domain include Hidden Markov Models (HMM), Random Forests (RF), Convolutional Neural Networks (CNN), Support Vector Machines (SVM), and Recurrent Neural Networks (RNN) such as Long Short-Term Memory (LSTM) networks. In this research, we consider a hybrid architecture, where HMMs are trained on opcode sequences, and the resulting hidden states of these trained HMMs are used as feature vectors in various classifiers. In this context, extracting the HMM hidden state sequences can be viewed as a form of feature engineering that is somewhat analogous to techniques that are commonly employed in Natural Language Processing (NLP). We find that this NLP-based approach outperforms other popular techniques on a challenging malware dataset, with an HMM-Random Forrest model yielding the best results.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¾ˆå¤šä¸åŒçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯å·²ç»æˆåŠŸåœ°åº”ç”¨äºæ¶æ„è½¯ä»¶æ£€æµ‹å’Œåˆ†ç±»ã€‚ä¾‹å¦‚ï¼Œå¸¸è§çš„å­¦ä¹ æŠ€æœ¯åœ¨æ¶æ„è½¯ä»¶é¢†åŸŸåŒ…æ‹¬éšè—markovæ¨¡å‹ï¼ˆHMMï¼‰ã€Random Forestsï¼ˆRFï¼‰ã€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€æ”¯æŒå‘é‡æœºå™¨ï¼ˆSVMï¼‰å’Œå›å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œå¦‚é•¿æœŸçŸ­æ—¶é—´è®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ã€‚åœ¨è¿™äº›ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ç§æ··åˆä½“ç³»ï¼Œå…¶ä¸­HMMè¢«è®­ç»ƒäºæœºå™¨ç åºåˆ—ï¼Œå¹¶å°†è¿™äº›è®­ç»ƒå¾—åˆ°çš„éšè—çŠ¶æ€ç”¨ä½œä¸åŒçš„åˆ†ç±»å™¨çš„ç‰¹å¾å‘é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæå–HMMéšè—çŠ¶æ€åºåˆ—å¯ä»¥è¢«è§†ä¸ºä¸€ç§ç‰¹å¾å·¥ç¨‹æŠ€æœ¯ï¼Œä¸è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­å¸¸è§çš„æŠ€æœ¯æœ‰ä¸€å®šçš„ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¿™ç§NLPåŸºäºçš„æ–¹æ³•åœ¨ä¸€ä¸ªå¤æ‚çš„æ¶æ„è½¯ä»¶æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒHMM-Random Forrestæ¨¡å‹å®ç°äº†æœ€ä½³ç»“æœã€‚
</details></li>
</ul>
<hr>
<h2 id="Keystroke-Dynamics-for-User-Identification"><a href="#Keystroke-Dynamics-for-User-Identification" class="headerlink" title="Keystroke Dynamics for User Identification"></a>Keystroke Dynamics for User Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05529">http://arxiv.org/abs/2307.05529</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andreArtelt/KeystrokeDynamicsForUserIdentification">https://github.com/andreArtelt/KeystrokeDynamicsForUserIdentification</a></li>
<li>paper_authors: Atharva Sharma, Martin JureÄek, Mark Stamp</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ä¸ºäº†è§£å†³ç”¨æˆ·éªŒè¯é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç”±æ–‡æœ¬æ•°æ®ä¸Šã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†ä¸€ç§å¤æ‚çš„å›¾åƒLikeç‰¹å¾ï¼Œä»¥åŠå¤šç±»å‹Convolutional Neural Networksæ¥è¿›è¡Œç”¨æˆ·éªŒè¯ã€‚</li>
<li>results: è¿™ä¸ªç ”ç©¶è·å¾—äº†0.78çš„åˆ†ç±»ç²¾åº¦ï¼ˆå³ç”¨æˆ·è¯†åˆ«ç‡ï¼‰ï¼Œä½†æ˜¯ä½¿ç”¨Random Foreståˆ†ç±»å™¨å¹¶ä¸å¯¹ç›¸ä¼¼çš„ç‰¹å¾è¿›è¡Œè½»å¾®ä¿®æ”¹åï¼Œè·å¾—äº†0.93çš„åˆ†ç±»ç²¾åº¦ã€‚<details>
<summary>Abstract</summary>
In previous research, keystroke dynamics has shown promise for user authentication, based on both fixed-text and free-text data. In this research, we consider the more challenging multiclass user identification problem, based on free-text data. We experiment with a complex image-like feature that has previously been used to achieve state-of-the-art authentication results over free-text data. Using this image-like feature and multiclass Convolutional Neural Networks, we are able to obtain a classification (i.e., identification) accuracy of 0.78 over a set of 148 users. However, we find that a Random Forest classifier trained on a slightly modified version of this same feature yields an accuracy of 0.93.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿‡å»çš„ç ”ç©¶ä¸­ï¼Œé”®ç›˜åŠ¨æ€å­¦å·²ç»å±•ç¤ºäº†ç”¨æˆ·è®¤è¯çš„æ­é…å¯èƒ½æ€§ï¼ŒåŸºäºå›ºå®šæ–‡æœ¬å’Œè‡ªç”±æ–‡æœ¬æ•°æ®ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†æ›´åŠ å›°éš¾çš„å¤šç±»ç”¨æˆ·è¯†åˆ«é—®é¢˜ï¼ŒåŸºäºè‡ªç”±æ–‡æœ¬æ•°æ®ã€‚æˆ‘ä»¬å°è¯•ä½¿ç”¨è¿‡å»å·²ç»ç”¨æ¥å®ç°è‡ªç”±æ–‡æœ¬æ•°æ®ä¸ŠçŠ¶æ€å‰ç»çš„å¤æ‚å›¾åƒç±»ç‰¹å¾ã€‚ä½¿ç”¨è¿™ç§å›¾åƒç±»ç‰¹å¾å’Œå¤šç±»å·ç§¯ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬èƒ½å¤Ÿè·å¾—ä¸€ä¸ªåˆ†ç±»ç²¾åº¦ï¼ˆå³è¯†åˆ«ç‡ï¼‰ä¸º0.78ï¼Œåœ¨148ä¸ªç”¨æˆ·ä¸­ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨ä¸€ä¸ªåŸºäºè¿™ä¸ªç‰¹å¾çš„å¾®å°ä¿®æ”¹ç‰ˆæœ¬çš„Random Foreståˆ†ç±»å™¨ï¼Œå¯ä»¥è¾¾åˆ°0.93çš„ç²¾åº¦ã€‚
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-and-Deep-Reinforcement-Learning-based-Solutions-for-Machine-Maintenance-Planning-Scheduling-Policies-and-Optimization"><a href="#Reinforcement-and-Deep-Reinforcement-Learning-based-Solutions-for-Machine-Maintenance-Planning-Scheduling-Policies-and-Optimization" class="headerlink" title="Reinforcement and Deep Reinforcement Learning-based Solutions for Machine Maintenance Planning, Scheduling Policies, and Optimization"></a>Reinforcement and Deep Reinforcement Learning-based Solutions for Machine Maintenance Planning, Scheduling Policies, and Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03860">http://arxiv.org/abs/2307.03860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oluwaseyi Ogunfowora, Homayoun Najjaran</li>
<li>For: This paper reviews the applications of reinforcement and deep reinforcement learning for maintenance planning and optimization problems.* Methods: The paper uses a literature review to identify and categorize existing research on reinforcement learning for maintenance planning, and provides graphical and tabular representations of the adopted methodologies, findings, and interpretations.* Results: The paper highlights research gaps, key insights from the literature, and areas for future work in the field of reinforcement learning for maintenance planning.In Simplified Chinese text, the three information points could be summarized as follows:* For: æœ¬æ–‡reviewäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ è¿›è¡Œç»´æŠ¤è§„åˆ’å’Œä¼˜åŒ–é—®é¢˜çš„åº”ç”¨ã€‚* Methods: æœ¬æ–‡ä½¿ç”¨æ–‡çŒ®ç»¼è¿°æ¥ indentifyå’Œåˆ†ç±»ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç»´æŠ¤è§„åˆ’ç ”ç©¶ï¼Œå¹¶æä¾›å›¾å½¢å’Œè¡¨æ ¼å½¢å¼çš„é‡‡ç”¨æ–¹æ³•ã€å‘ç°å’Œè§£é‡Šã€‚* Results: æœ¬æ–‡æŒ‡å‡ºäº†ç»´æŠ¤è§„åˆ’é¢†åŸŸçš„ç ”ç©¶æ¼æ´ã€æ–‡çŒ®ä¸­çš„å…³é”®å‘ç°å’Œæœªæ¥å·¥ä½œçš„æ–¹å‘ã€‚<details>
<summary>Abstract</summary>
Systems and machines undergo various failure modes that result in machine health degradation, so maintenance actions are required to restore them back to a state where they can perform their expected functions. Since maintenance tasks are inevitable, maintenance planning is essential to ensure the smooth operations of the production system and other industries at large. Maintenance planning is a decision-making problem that aims at developing optimum maintenance policies and plans that help reduces maintenance costs, extend asset life, maximize their availability, and ultimately ensure workplace safety. Reinforcement learning is a data-driven decision-making algorithm that has been increasingly applied to develop dynamic maintenance plans while leveraging the continuous information from condition monitoring of the system and machine states. By leveraging the condition monitoring data of systems and machines with reinforcement learning, smart maintenance planners can be developed, which is a precursor to achieving a smart factory. This paper presents a literature review on the applications of reinforcement and deep reinforcement learning for maintenance planning and optimization problems. To capture the common ideas without losing touch with the uniqueness of each publication, taxonomies used to categorize the systems were developed, and reviewed publications were highlighted, classified, and summarized based on these taxonomies. Adopted methodologies, findings, and well-defined interpretations of the reviewed studies were summarized in graphical and tabular representations to maximize the utility of the work for both researchers and practitioners. This work also highlights the research gaps, key insights from the literature, and areas for future work.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç³»ç»Ÿå’Œæœºå™¨ä¼šç»å†å¤šç§æ•…éšœæ¨¡å¼ï¼Œå¯¼è‡´æœºå™¨å¥åº·ä¸‹é™ï¼Œå› æ­¤ç»´æŠ¤å·¥ä½œæ˜¯å¿…è¦çš„ä»¥è¿˜åŸå®ƒä»¬åˆ°å¯ä»¥æ‰§è¡Œé¢„æœŸåŠŸèƒ½çš„çŠ¶æ€ã€‚ç»´æŠ¤å·¥ä½œæ˜¯ä¸å¯é¿å…çš„ï¼Œå› æ­¤ç»´æŠ¤è§‚å¿µæ˜¯éå¸¸é‡è¦ï¼Œä»¥ç¡®ä¿ç”Ÿäº§ç³»ç»Ÿå’Œå…¶ä»–è¡Œä¸šçš„é¡ºç•…è¿è¡Œã€‚ç»´æŠ¤è§‚å¿µæ˜¯ä¸€ä¸ªå†³ç­–é—®é¢˜ï¼Œæ—¨åœ¨å‘å±•æœ€ä½³çš„ç»´æŠ¤æ”¿ç­–å’Œè®¡åˆ’ï¼Œå¸®åŠ©é™ä½ç»´æŠ¤æˆæœ¬ï¼Œå»¶é•¿èµ„äº§å¯¿å‘½ï¼Œæœ€å¤§åŒ–èµ„äº§å¯ç”¨æ€§ï¼Œå¹¶ç¡®ä¿å·¥ä½œå®‰å…¨ã€‚å¯¹äºç»´æŠ¤è®¡åˆ’å’Œä¼˜åŒ–é—®é¢˜ï¼Œå¾ˆå¤šä½¿ç”¨äº†å¼ºåŒ–å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ•°æ®é©±åŠ¨çš„å†³ç­–æ¨æ–­ç®—æ³•ã€‚é€šè¿‡ä½¿ç”¨ç³»ç»Ÿå’Œæœºå™¨çš„çŠ¶æ€ç›‘æ§æ•°æ®å’Œå¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥å¼€å‘å‡ºæ™ºèƒ½ç»´æŠ¤è§‚å¿µï¼Œè¿™æ˜¯ä¸€ä¸ªè¿›æ”»æ™ºèƒ½å‚çš„å…ˆé©±ã€‚æœ¬æ–‡å°†ä»‹ç»ä¸€ç¯‡æ–‡çŒ®ç»¼è¿°ï¼Œæ¢è®¨å¯¹ç»´æŠ¤è®¡åˆ’å’Œä¼˜åŒ–é—®é¢˜çš„åº”ç”¨å¼ºåŒ–å­¦ä¹ å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç ”ç©¶ã€‚ä¸ºäº†æ•æ‰æ¯ç¯‡æ–‡çŒ®çš„å…±åŒä¸»é¢˜è€Œä¸è®©å®ƒä»¬ä¸å…·ä½“æ€§çš„åŒºåˆ«ï¼Œåˆ™ä½¿ç”¨äº†åˆ†ç±»ç³»ç»Ÿï¼Œå¹¶å°†ç»¼è¿°çš„æ–‡çŒ®æŒ‰ç…§è¿™äº›åˆ†ç±»ç³»ç»Ÿè¿›è¡Œåˆ†ç±»å’Œæ‘˜è¦ã€‚é‡‡ç”¨çš„æ–¹æ³•ã€å‘ç°å’Œå®é™…çš„è§£é‡Šéƒ½æ˜¯é€šè¿‡å›¾è¡¨å’Œè¡¨æ ¼çš„å½¢å¼å‘ˆç°ï¼Œä»¥ä¾¿å¯¹ç ”ç©¶äººå‘˜å’Œå®è·µè€…å…·æœ‰æœ€å¤§çš„å®ç”¨æ€§ã€‚æœ¬æ–‡è¿˜å¼ºè°ƒäº†ç ”ç©¶æ½œåœ¨å·®è·ã€å…³é”®è§è§£å’Œæœªæ¥å·¥ä½œçš„æ–¹å‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="The-Ethical-Implications-of-Generative-Audio-Models-A-Systematic-Literature-Review"><a href="#The-Ethical-Implications-of-Generative-Audio-Models-A-Systematic-Literature-Review" class="headerlink" title="The Ethical Implications of Generative Audio Models: A Systematic Literature Review"></a>The Ethical Implications of Generative Audio Models: A Systematic Literature Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05527">http://arxiv.org/abs/2307.05527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Barnett</li>
<li>for: æœ¬ç ”ç©¶å†™ä½œçš„ç›®çš„æ˜¯å®ç°Generative audioæ¨¡å‹çš„ç³»ç»Ÿæ€§æ–‡çŒ®ç»¼è¿°ï¼Œä»¥ä¾¿è¯„ä¼°è¿™ä¸ªé¢†åŸŸçš„ç ”ç©¶è€…æ˜¯å¦è€ƒè™‘åˆ°å¯èƒ½çš„è´Ÿé¢å½±å“ï¼Œä»¥åŠéœ€è¦è€ƒè™‘çš„ä¼¦ç†é—®é¢˜ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†884ç¯‡Generative audioæ¨¡å‹ç›¸å…³çš„ç ”ç©¶æ–‡çŒ®ï¼Œé€šè¿‡åˆ†æè¿™äº›æ–‡çŒ®çš„å†…å®¹æ¥è¯„ä¼°ç ”ç©¶è€…å¯¹å¯èƒ½çš„è´Ÿé¢å½±å“çš„è€ƒè™‘ç¨‹åº¦ã€‚</li>
<li>results: ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œåªæœ‰å°‘äº10%çš„Generative audioç ”ç©¶æ–‡çŒ®è®¨è®ºäº†å¯èƒ½çš„è´Ÿé¢å½±å“ï¼Œè¿™æ˜¯æå…¶ç½•è§çš„ã€‚ç„¶è€Œï¼Œè¿™äº›æ–‡çŒ®ä¸­æå‡ºçš„ä¼¦ç†é—®é¢˜å’Œé—®é¢˜æ˜¯æ·±åˆ»çš„ï¼Œä¾‹å¦‚æ¬ºè¯ˆã€æ·±åœ³åˆ¶ä½œå’Œç‰ˆæƒä¾µçŠ¯ç­‰ã€‚æœ¬ç ”ç©¶è¿™æ ·çš„ç¼ºä¹ä¼¦ç†è€ƒè™‘å’Œæ½œåœ¨çš„è´Ÿé¢å½±å“ï¼Œå°†æ˜¯è¿™ä¸ªé¢†åŸŸçš„æœªæ¥ç ”ç©¶æŒ‡å—ã€‚<details>
<summary>Abstract</summary>
Generative audio models typically focus their applications in music and speech generation, with recent models having human-like quality in their audio output. This paper conducts a systematic literature review of 884 papers in the area of generative audio models in order to both quantify the degree to which researchers in the field are considering potential negative impacts and identify the types of ethical implications researchers in this area need to consider. Though 65% of generative audio research papers note positive potential impacts of their work, less than 10% discuss any negative impacts. This jarringly small percentage of papers considering negative impact is particularly worrying because the issues brought to light by the few papers doing so are raising serious ethical implications and concerns relevant to the broader field such as the potential for fraud, deep-fakes, and copyright infringement. By quantifying this lack of ethical consideration in generative audio research and identifying key areas of potential harm, this paper lays the groundwork for future work in the field at a critical point in time in order to guide more conscientious research as this field progresses.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="inTformer-A-Time-Embedded-Attention-Based-Transformer-for-Crash-Likelihood-Prediction-at-Intersections-Using-Connected-Vehicle-Data"><a href="#inTformer-A-Time-Embedded-Attention-Based-Transformer-for-Crash-Likelihood-Prediction-at-Intersections-Using-Connected-Vehicle-Data" class="headerlink" title="inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data"></a>inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03854">http://arxiv.org/abs/2307.03854</a></li>
<li>repo_url: None</li>
<li>paper_authors: B. M. Tazbiul Hassan Anik, Zubayer Islam, Mohamed Abdel-Aty</li>
<li>for: é¢„æµ‹äº¤å‰ç‚¹äº‹æ•…å¯èƒ½æ€§çš„å®æ—¶é¢„æµ‹æ¨¡å‹ï¼Œå¸®åŠ©æé«˜äº¤é€šå®‰å…¨æ€§ã€‚</li>
<li>methods: ä½¿ç”¨Transformeræ¨¡å‹ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æ¥å¤„ç†æ•°æ®åºåˆ—ï¼Œå¹¶ä¸”å¯ä»¥åŒæ—¶å¤„ç†æ‰€æœ‰æ•°æ®å…ƒç´  durante trainingã€‚</li>
<li>results: åœ¨ä½¿ç”¨INRIXå’ŒCATT Labçš„ä¿¡å·åˆ†æå¹³å°ä¸Šæµ‹è¯•çš„connected vehicleæ•°æ®ä¸Šï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºinTformerçš„æ—¶é—´åµŒå…¥æ³¨æ„åŠ›åŸºäºTransformeræ¨¡å‹ï¼Œå¯ä»¥æ•ˆæœåœ°é¢„æµ‹äº¤å‰ç‚¹äº‹æ•…å¯èƒ½æ€§ã€‚æœ€ä½³inTformeræ¨¡å‹è¾¾åˆ°äº†73%çš„æ•æ„Ÿæ€§ã€‚<details>
<summary>Abstract</summary>
The real-time crash likelihood prediction model is an essential component of the proactive traffic safety management system. Over the years, numerous studies have attempted to construct a crash likelihood prediction model in order to enhance traffic safety, but mostly on freeways. In the majority of the existing studies, researchers have primarily employed a deep learning-based framework to identify crash potential. Lately, Transformer has emerged as a potential deep neural network that fundamentally operates through attention-based mechanisms. Transformer has several functional benefits over extant deep learning models such as Long Short-Term Memory (LSTM), Convolution Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term dependencies in a data sequence. Secondly, Transformers can parallelly process all elements in a data sequence during training. Finally, a Transformer does not have the vanishing gradient issue. Realizing the immense possibility of Transformers, this paper proposes inTersection-Transformer (inTformer), a time-embedded attention-based Transformer model that can effectively predict intersection crash likelihood in real-time. The proposed model was evaluated using connected vehicle data extracted from INRIX and Center for Advanced Transportation Technology (CATT) Lab's Signal Analytics Platform. The data was parallelly formatted and stacked at different timesteps to develop nine inTformer models. The best inTformer model achieved a sensitivity of 73%. This model was also compared to earlier studies on crash likelihood prediction at intersections and with several established deep learning models trained on the same connected vehicle dataset. In every scenario, this inTformer outperformed the benchmark models confirming the viability of the proposed inTformer architecture.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°å®æ—¶å¯å‘é£é™©é¢„æµ‹æ¨¡å‹æ˜¯æ™ºèƒ½äº¤é€šå®‰å…¨ç®¡ç†ç³»ç»Ÿçš„é‡è¦ç»„ä»¶ã€‚è¿‡å»å‡ å¹´ï¼Œè®¸å¤šç ”ç©¶éƒ½å°è¯•äº†æ„å»ºå¯å‘é£é™©é¢„æµ‹æ¨¡å‹ï¼Œä»¥æé«˜äº¤é€šå®‰å…¨ï¼Œä½†å¤§å¤šæ•°ç ”ç©¶éƒ½åœ¨é«˜é€Ÿå…¬è·¯ä¸Šè¿›è¡Œã€‚ç°æœ‰çš„å¤§å¤šæ•°ç ”ç©¶è€…éƒ½ä½¿ç”¨äº†æ·±åº¦å­¦ä¹ æ¡†æ¶æ¥è¯†åˆ«å¯å‘ potentialã€‚æœ€è¿‘ï¼ŒTransformer å‡ºç°äº†ä½œä¸ºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ½œåœ¨å¯èƒ½ï¼Œå®ƒåŸºäºæ³¨æ„åŠ›æœºåˆ¶æ¥è¿è¡Œã€‚Transformer ä¸æ—¢æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚ Long Short-Term Memory å’Œ Convolution Neural Networkï¼‰ç›¸æ¯”ï¼Œå…·æœ‰å¤šç§åŠŸèƒ½ä¼˜åŠ¿ã€‚é¦–å…ˆï¼ŒTransformer å¯ä»¥è¯†åˆ«é•¿æœŸä¾èµ–å…³ç³»ã€‚å…¶æ¬¡ï¼ŒTransformer å¯ä»¥å¹¶è¡Œå¤„ç†æ•°æ®åºåˆ—ä¸­çš„æ‰€æœ‰å…ƒç´ ã€‚æœ€åï¼ŒTransformer ä¸å—æ¶ˆå¤±æ¢¯åº¦é—®é¢˜çš„å½±å“ã€‚é‰´äº Transformer çš„å¯èƒ½æ€§ï¼Œæœ¬æ–‡æå‡ºäº† intersection-Transformerï¼ˆinTformerï¼‰æ¨¡å‹ï¼Œå¯ä»¥åœ¨å®æ—¶ä¸­é¢„æµ‹äº¤å‰å£å¯å‘é£é™©ã€‚è¯¥æ¨¡å‹ä½¿ç”¨ INRIX å’Œ Center for Advanced Transportation Technology (CATT) Lab çš„ Signal Analytics Platform æä¾›çš„è¿æ¥å¼æ±½è½¦æ•°æ®è¿›è¡Œè¯„ä¼°ã€‚æ•°æ®è¢«å¹³è¡Œæ ¼å¼åŒ–å¹¶å †å åœ¨ä¸åŒçš„æ—¶é—´æ­¥ä¸Šï¼Œä»¥æ„å»ºä¹ä¸ª inTformer æ¨¡å‹ã€‚æœ€ä½³ inTformer æ¨¡å‹è¾¾åˆ°äº† 73% çš„æ•æ„Ÿåº¦ã€‚è¿™ä¸ªæ¨¡å‹è¿˜ä¸å…¶ä»–å…³äºäº¤å‰å£å¯å‘é£é™©é¢„æµ‹çš„ç ”ç©¶å’Œå·²æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒä¸€ä¸ªè¿æ¥å¼æ±½è½¦datasetä¸Šè¿›è¡Œæ¯”è¾ƒã€‚åœ¨æ¯ç§åœºæ™¯ä¸‹ï¼Œè¿™ä¸ª inTformer æ¨¡å‹éƒ½è¶…è¶Šäº†å‚è€ƒæ¨¡å‹ï¼Œè¯æ˜äº†ææ¡ˆçš„ inTformer æ¶æ„çš„å¯è¡Œæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Optimal-Learners-for-Realizable-Regression-PAC-Learning-and-Online-Learning"><a href="#Optimal-Learners-for-Realizable-Regression-PAC-Learning-and-Online-Learning" class="headerlink" title="Optimal Learners for Realizable Regression: PAC Learning and Online Learning"></a>Optimal Learners for Realizable Regression: PAC Learning and Online Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03848">http://arxiv.org/abs/2307.03848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Idan Attias, Steve Hanneke, Alkis Kalavasis, Amin Karbasi, Grigoris Velegkas</li>
<li>for: æœ¬æ–‡ä¸»è¦ç ”ç©¶ realizable å›å½’çš„ç»Ÿè®¡å¤æ‚æ€§ï¼ŒåŒ…æ‹¬ PAC å­¦ä¹  Setting å’Œ online å­¦ä¹  Settingã€‚</li>
<li>methods: æœ¬æ–‡é¦–å…ˆæå‡ºäº†ä¸€ç§æœ€ä¼˜åŒ–å­¦ä¹ å™¨ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ç»´åº¦æ¥æè¿°å¯å­¦ä¹ çš„ç±»åˆ«ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åŸºäº Graph ç»´åº¦çš„ ERM å­¦ä¹ æ€§ç»´åº¦ï¼Œä»¥åŠä¸€ç§åŸºäº DS ç»´åº¦çš„å­¦ä¹ å¯èƒ½æ€§ç»´åº¦ã€‚</li>
<li>results: æœ¬æ–‡ç¡®å®šäº†ä¸€ä¸ªå¿…è¦æ¡ä»¶ Ğ´Ğ»Ñå­¦ä¹ å¯èƒ½æ€§ï¼Œå¹¶ conjecture è¿™å¯èƒ½ä¹Ÿæ˜¯å……åˆ†æ¡ä»¶ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è§£å†³äº† Daskalakis å’Œ Golowich åœ¨ STOC â€˜22 ä¸­æå‡ºçš„ä¸€ä¸ªå¼€é—®ã€‚<details>
<summary>Abstract</summary>
In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.   Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in this context.   Additionally, in the context of online learning we provide a dimension that characterizes the minimax instance optimal cumulative loss up to a constant factor and design an optimal online learner for realizable regression, thus resolving an open question raised by Daskalakis and Golowich in STOC '22.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç›®æ ‡æ˜¯Characterize realizable regressionçš„ç»Ÿè®¡å¤æ‚æ€§åœ¨PACå­¦ä¹ è®¾å®šä¸‹å’Œåœ¨çº¿å­¦ä¹ è®¾å®šä¸‹ã€‚previoius work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in this context.  åœ¨çº¿å­¦ä¹ ä¸Šï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ªcharacterizes the minimax instance optimal cumulative loss up to a constant factorçš„dimensionï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªoptimal online learner for realizable regressionï¼Œthereby resolving an open question raised by Daskalakis and Golowich in STOC '22.
</details></li>
</ul>
<hr>
<h2 id="RADAR-Robust-AI-Text-Detection-via-Adversarial-Learning"><a href="#RADAR-Robust-AI-Text-Detection-via-Adversarial-Learning" class="headerlink" title="RADAR: Robust AI-Text Detection via Adversarial Learning"></a>RADAR: Robust AI-Text Detection via Adversarial Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03838">http://arxiv.org/abs/2307.03838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æå‡ºä¸€ç§æ–°çš„AI-æ–‡æœ¬æ£€æµ‹æ¡†æ¶ï¼Œä»¥ä¾¿åœ¨LLMsæŠ€æœ¯çš„è¿›æ­¥å’ŒChatGPT-likeåº”ç”¨çš„æ™®åŠä¹‹ä¸‹ï¼Œæ›´å¥½åœ°åˆ†è¾¨äººå·¥ç”Ÿæˆçš„æ–‡æœ¬å’Œæœºå™¨ç”Ÿæˆçš„æ–‡æœ¬ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨çš„æ–¹æ³•æ˜¯åŸºäºå¯¹æŠ—å­¦ä¹ çš„RADARæ¡†æ¶ï¼Œå®ƒå…±åŒåŸ¹è®­äº†ä¸€ä¸ªRobust AI-text Detectorå’Œä¸€ä¸ªparaphraserã€‚paraphraserçš„ç›®çš„æ˜¯ç”ŸæˆçœŸå®çš„å†…å®¹ï¼Œä»¥é€ƒè„±AI-æ–‡æœ¬æ£€æµ‹ã€‚RADARä½¿ç”¨æ£€æµ‹å™¨çš„åé¦ˆæ¥æ›´æ–°paraphraserï¼Œå¹¶ vice versaã€‚</li>
<li>results: å¯¹8ç§LLMsï¼ˆPythiaã€Dolly 2.0ã€Palmyraã€Camelã€GPT-Jã€Dolly 1.0ã€LLaMAã€Vicunaï¼‰åœ¨4ä¸ªdatasetä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœæ˜¾ç¤ºï¼ŒRADARsignificantly outperformsç°æœ‰çš„AI-æ–‡æœ¬æ£€æµ‹æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨paraphrasingå­˜åœ¨æ—¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°RADARåœ¨instruction-tuned LLMsä¸Š Transferability å¼ºï¼Œå¹¶ä¸”é€šè¿‡GPT-3.5è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°RADARçš„æ”¹è¿›èƒ½åŠ›ã€‚<details>
<summary>Abstract</summary>
Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5.
</details>
<details>
<summary>æ‘˜è¦</summary>
RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection, while the detector's goal is to correctly identify AI-generated texts. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. The framework was evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, and the results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is involved. Additionally, RADAR was found to have strong transferability from instruction-tuned LLMs to other LLMs, and its capability was improved further via GPT-3.5.
</details></li>
</ul>
<hr>
<h2 id="Effect-of-Intensity-Standardization-on-Deep-Learning-for-WML-Segmentation-in-Multi-Centre-FLAIR-MRI"><a href="#Effect-of-Intensity-Standardization-on-Deep-Learning-for-WML-Segmentation-in-Multi-Centre-FLAIR-MRI" class="headerlink" title="Effect of Intensity Standardization on Deep Learning for WML Segmentation in Multi-Centre FLAIR MRI"></a>Effect of Intensity Standardization on Deep Learning for WML Segmentation in Multi-Centre FLAIR MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03827">http://arxiv.org/abs/2307.03827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdollah Ghazvanchahi, Pejman Jahbedar Maralani, Alan R. Moody, April Khademi</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æé«˜ç™½ matter lesionï¼ˆWMLï¼‰ segmentationåœ¨magnetic resonance imagingï¼ˆMRIï¼‰ä¸­çš„æ€§èƒ½è€Œå†™çš„ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†å¤šç§INTENSITY STANDARDIZATIONæ–¹æ³•æ¥è¿›è¡ŒMRIæ•°æ®çš„é¢„å¤„ç†ï¼Œä»¥æé«˜WML segmentationçš„æ€§èƒ½ã€‚å…¶ä¸­åŒ…æ‹¬IAMLABæ–¹æ³•ï¼Œä»¥åŠå…¶ä»–æµè¡Œçš„normalizationæŠ€æœ¯ï¼Œå¦‚White-stripã€Nyulå’ŒZ-scoreã€‚</li>
<li>results: ç»“æœè¡¨æ˜ï¼ŒIAMLABå’ŒEnsembleæ–¹æ³•åœ¨ä¸åŒçš„lesion categoryä¸­å‡æœ‰æ›´é«˜çš„WML segmentationæ€§èƒ½ï¼Œæ¯”å¦‚åŸå§‹æ•°æ®æˆ–å…¶ä»–normalizationæ–¹æ³•ã€‚IAMLABå’ŒEnsembleæ–¹æ³•åœ¨å„ç§lesion categoryä¸­éƒ½æœ‰æœ€é«˜çš„dice similarity coefficientï¼ˆDSCï¼‰ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„ä¸´åºŠæ•°æ®é›†ä¸­ä¹Ÿå…·æœ‰æœ€é«˜çš„DSCã€‚è¿™äº›æ–¹æ³•å¯ä»¥å‡è½»MRIé¢†åŸŸçš„å·®å¼‚ï¼Œå¹¶ä¸”æ˜¯é€‚ç”¨äºDL-based WML segmentationçš„ä¼˜é€‰æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Deep learning (DL) methods for white matter lesion (WML) segmentation in MRI suffer a reduction in performance when applied on data from a scanner or centre that is out-of-distribution (OOD) from the training data. This is critical for translation and widescale adoption, since current models cannot be readily applied to data from new institutions. In this work, we evaluate several intensity standardization methods for MRI as a preprocessing step for WML segmentation in multi-centre Fluid-Attenuated Inversion Recovery (FLAIR) MRI. We evaluate a method specifically developed for FLAIR MRI called IAMLAB along with other popular normalization techniques such as White-strip, Nyul and Z-score. We proposed an Ensemble model that combines predictions from each of these models. A skip-connection UNet (SC UNet) was trained on the standardized images, as well as the original data and segmentation performance was evaluated over several dimensions. The training (in-distribution) data consists of a single study, of 60 volumes, and the test (OOD) data is 128 unseen volumes from three clinical cohorts. Results show IAMLAB and Ensemble provide higher WML segmentation performance compared to models from original data or other normalization methods. IAMLAB & Ensemble have the highest dice similarity coefficient (DSC) on the in-distribution data (0.78 & 0.80) and on clinical OOD data. DSC was significantly higher for IAMLAB compared to the original data (p<0.05) for all lesion categories (LL>25mL: 0.77 vs. 0.71; 10mL<= LL<25mL: 0.66 vs. 0.61; LL<10mL: 0.53 vs. 0.52). The IAMLAB and Ensemble normalization methods are mitigating MRI domain shift and are optimal for DL-based WML segmentation in unseen FLAIR data.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³• Ğ´Ğ»Ñç™½ matteræŸå®³ï¼ˆWMLï¼‰åˆ†å‰²åœ¨MRIä¸­å—åˆ°å¤–éƒ¨æ•°æ®é›†ï¼ˆOODï¼‰çš„å½±å“ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™å¯¹äºç¿»è¯‘å’Œå¤§è§„æ¨¡åº”ç”¨è€Œè¨€æ˜¯å…³é”®ï¼Œå› ä¸ºç°æœ‰çš„æ¨¡å‹æ— æ³•ç›´æ¥åº”ç”¨äºæ–°æœºæ„çš„æ•°æ®é›†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šç§MRIIntensityæ ‡å‡†åŒ–æ–¹æ³•ä½œä¸ºWMLåˆ†å‰²å‰çš„é¢„å¤„ç†æ­¥éª¤ã€‚æˆ‘ä»¬è¯„ä¼°äº†ç‰¹å®š Ğ´Ğ»ÑFLAIR MRIçš„IAMLABæ–¹æ³•ï¼Œä»¥åŠå…¶ä»–æµè¡Œçš„æ ‡å‡†åŒ–æŠ€æœ¯ï¼Œå¦‚ç™½å¸¦ã€æ©ç´å’ŒZ-scoreã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»„åˆè¿™äº›æ¨¡å‹çš„ensembleæ¨¡å‹ã€‚ä¸€ä¸ªskip-connection UNetï¼ˆSC UNNetï¼‰åœ¨æ ‡å‡†åŒ–å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥åŠåŸå§‹æ•°æ®ä¸Šè¿›è¡Œåˆ†å‰²æ€§èƒ½çš„è¯„ä¼°ã€‚è®­ç»ƒï¼ˆå·ç§¯ï¼‰æ•°æ®é›†åŒ…æ‹¬60ä¸ªVolumeï¼Œæµ‹è¯•ï¼ˆOODï¼‰æ•°æ®é›†åŒ…æ‹¬128ä¸ªæœªçœ‹è¿‡çš„Volumeä»ä¸‰ä¸ªä¸´åºŠå„ç±»æ•°æ®é›†ã€‚ç»“æœæ˜¾ç¤ºï¼ŒIAMLABå’ŒEnsembleæ¨¡å‹åœ¨WMLåˆ†å‰²æ€§èƒ½æ–¹é¢æ¯”åŸå§‹æ•°æ®æˆ–å…¶ä»–æ ‡å‡†åŒ–æ–¹æ³•é«˜å¾—å¤šã€‚IAMLABå’ŒEnsembleæ¨¡å‹åœ¨å·ç§¯æ•°æ®é›†ï¼ˆå·ç§¯æ•°æ®ï¼‰ä¸Šçš„DSCå€¼åˆ†åˆ«ä¸º0.78å’Œ0.80ï¼Œå¹¶åœ¨ä¸´åºŠOODæ•°æ®ä¸Šè¾¾åˆ°äº†æœ€é«˜çš„DSCå€¼ï¼ˆ0.77å’Œ0.80ï¼‰ã€‚å¯¹äºæ‰€æœ‰æŸå®³ç±»åˆ«ï¼ˆLL>25mLï¼š0.77 vs. 0.71ï¼›10mLâ‰¤ LL<25mLï¼š0.66 vs. 0.61ï¼›LL<10mLï¼š0.53 vs. 0.52ï¼‰ï¼ŒIAMLABæ¨¡å‹çš„DSCå€¼ä¸åŸå§‹æ•°æ®ç›¸æ¯”æœ‰ statistically significant differenceï¼ˆP<0.05ï¼‰ã€‚IAMLABå’ŒEnsemble normalizationæ–¹æ³•å¯ä»¥ Mitigate MRIåŸŸshiftï¼Œæ˜¯é€‚ç”¨äºDLåŸºäºWMLåˆ†å‰²çš„ä¼˜é€‰æ–¹æ¡ˆã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Combinatorial-Characterization-of-Online-Learning-Games-with-Bounded-Losses"><a href="#A-Combinatorial-Characterization-of-Online-Learning-Games-with-Bounded-Losses" class="headerlink" title="A Combinatorial Characterization of Online Learning Games with Bounded Losses"></a>A Combinatorial Characterization of Online Learning Games with Bounded Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03816">http://arxiv.org/abs/2307.03816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinod Raman, Unique Subedi, Ambuj Tewari</li>
<li>for: å­¦ä¹ å‡è®¾é›†çš„åœ¨çº¿å­¦ä¹ æ€§èƒ½å¯¹äºä»»æ„ã€ä½†æ˜¯æœ‰ç•Œçš„æŸå¤±å‡½æ•°</li>
<li>methods: ä½¿ç”¨æ–°çš„æ¸è¿›æ•æ„Ÿ combinatorial ç»´åº¦â€”â€”é¡ºåºæœ€å°æœ€å¤§ç»´åº¦ï¼Œå¯¹äºåœ¨çº¿å­¦ä¹ æ€§èƒ½è¿›è¡Œæ•°é‡åŒ–å®šé‡Characterization</li>
<li>results: åœ¨vector-valued regressionå’Œå¤šæ ‡ç­¾åˆ†ç±»ä¸¤ä¸ªè‡ªç„¶çš„å­¦ä¹ è®¾å®šä¸­ï¼Œå¾—åˆ°äº†ç¬¬ä¸€ä¸ªé‡åŒ–çš„åœ¨çº¿å­¦ä¹ æ€§èƒ½Characterization<details>
<summary>Abstract</summary>
We study the online learnability of hypothesis classes with respect to arbitrary, but bounded, loss functions. We give a new scale-sensitive combinatorial dimension, named the sequential Minimax dimension, and show that it gives a tight quantitative characterization of online learnability. As applications, we give the first quantitative characterization of online learnability for two natural learning settings: vector-valued regression and multilabel classification.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ç ”ç©¶åœ¨ä½¿ç”¨å„ç§å›ºå®šä½†å—é™çš„æŸå¤±å‡½æ•°æ—¶ï¼Œå‡è®¾é›†åˆåœ¨çº¿å­¦ä¹ çš„å¯å­¦ä¹ æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•æ„Ÿåº¦é‡ï¼Œç§°ä¸ºé¡ºåºæœ€å°æœ€å¤§ç»´åº¦ï¼Œå¹¶è¯æ˜å®ƒä¸ºåœ¨çº¿å­¦ä¹ çš„å‡†ç¡®é‡æä¾›äº†ç´§è·Ÿçš„é‡åŒ–ç‰¹å¾åŒ–ã€‚æˆ‘ä»¬è¿˜åº”ç”¨åˆ°äº†ä¸¤ä¸ªè‡ªç„¶çš„å­¦ä¹ åœºæ™¯ï¼šå‘é‡å€¼å›å½’å’Œå¤šç±»åˆ†ç±»ã€‚
</details></li>
</ul>
<hr>
<h2 id="Controlling-Chaotic-Maps-using-Next-Generation-Reservoir-Computing"><a href="#Controlling-Chaotic-Maps-using-Next-Generation-Reservoir-Computing" class="headerlink" title="Controlling Chaotic Maps using Next-Generation Reservoir Computing"></a>Controlling Chaotic Maps using Next-Generation Reservoir Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03813">http://arxiv.org/abs/2307.03813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert M. Kent, Wendson A. S. Barbosa, Daniel J. Gauthier</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†ç ”ç©¶éçº¿æ€§ç³»ç»Ÿæ§åˆ¶æŠ€æœ¯å’Œä¸‹ä¸€ä»£æ½œåœ¨ computing ä¹‹é—´çš„ç»“åˆã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨äº†éçº¿æ€§ç³»ç»Ÿæ§åˆ¶æŠ€æœ¯å’Œä¸‹ä¸€ä»£æ½œåœ¨ computing æ¥é¢„æµ‹åŠ¨åŠ›ç³»ç»Ÿçš„è¡Œä¸ºã€‚</li>
<li>results: è®ºæ–‡åœ¨ä¸€ç³»åˆ—æ§åˆ¶ä»»åŠ¡ä¸­å±•ç¤ºäº†æ§åˆ¶å™¨çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ§åˆ¶ç³»ç»Ÿ Ğ¼ĞµĞ¶Ğ´Ñƒä¸ç¨³å®šçš„å›ºå®šç‚¹ã€ç¨³å®šç³»ç»Ÿåˆ°æ›´é«˜é˜¶ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…è½¨è¿¹ã€å’Œåˆ°ä¸€ä¸ªæŒ‡å®šçš„çŠ¶æ€ã€‚ è®ºæ–‡è¿˜è¡¨æ˜äº†æ§åˆ¶å™¨åªéœ€è¦10ä¸ªæ•°æ®ç‚¹è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥åœ¨ä¸€æ¬¡è¿­ä»£ä¸­æ§åˆ¶ç³»ç»Ÿåˆ°æŒ‡å®šçš„è½¨è¿¹ï¼Œå¹¶ä¸”å¯¹å™ªéŸ³å’Œæ¨¡å‹è¯¯å·® Displaytext æœ‰ robustnessã€‚<details>
<summary>Abstract</summary>
In this work, we combine nonlinear system control techniques with next-generation reservoir computing, a best-in-class machine learning approach for predicting the behavior of dynamical systems. We demonstrate the performance of the controller in a series of control tasks for the chaotic H\'enon map, including controlling the system between unstable fixed-points, stabilizing the system to higher order periodic orbits, and to an arbitrary desired state. We show that our controller succeeds in these tasks, requires only 10 data points for training, can control the system to a desired trajectory in a single iteration, and is robust to noise and modeling error.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†éçº¿æ€§ç³»ç»Ÿæ§åˆ¶æŠ€æœ¯ä¸ä¸‹ä¸€ä»£æ•£å°„ computingï¼ˆä¸€ç§æœ€ä½³çº§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼‰ç»“åˆä½¿ç”¨ï¼Œç”¨äºé¢„æµ‹åŠ¨åŠ›ç³»ç»Ÿçš„è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨å“ˆå†œåœ°å›¾ä¸­è¿›è¡Œäº†ä¸€ç³»åˆ—æ§åˆ¶ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ§åˆ¶ç³»ç»Ÿåœ¨ä¸ç¨³å®šçš„å›ºå®šç‚¹ä¸Šï¼Œç¨³å®šç³»ç»Ÿåˆ°æ›´é«˜é˜¶ periodic orbit ä»¥åŠåˆ°ä»»æ„æ‰€å¸Œæœ›çš„çŠ¶æ€ã€‚æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬çš„æ§åˆ¶å™¨åœ¨è¿™äº›ä»»åŠ¡ä¸­å…·æœ‰å‡ºè‰²çš„è¡¨ç°ï¼Œåªéœ€è¦10ä¸ªæ•°æ®ç‚¹è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥åœ¨å•æ¬¡è¿­ä»£ä¸­æ§åˆ¶ç³»ç»Ÿåˆ°æ‰€å¸Œæœ›çš„è½¨è¿¹ï¼Œå¹¶å…·æœ‰å™ªå£°å’Œæ¨¡å‹è¯¯å·®çš„æŠ—æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="For-Women-Life-Freedom-A-Participatory-AI-Based-Social-Web-Analysis-of-a-Watershed-Moment-in-Iranâ€™s-Gender-Struggles"><a href="#For-Women-Life-Freedom-A-Participatory-AI-Based-Social-Web-Analysis-of-a-Watershed-Moment-in-Iranâ€™s-Gender-Struggles" class="headerlink" title="For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis of a Watershed Moment in Iranâ€™s Gender Struggles"></a>For Women, Life, Freedom: A Participatory AI-Based Social Web Analysis of a Watershed Moment in Iranâ€™s Gender Struggles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03764">http://arxiv.org/abs/2307.03764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adel Khorramrouz, Sujan Dutta, Ashiqur R. KhudaBukhsh</li>
<li>for: è¿™ paper çš„ç›®çš„æ˜¯è®¡ç®—å¹¶åˆ†ææ³¢æ–¯è¯­æ¨ç‰¹è®¨è®ºï¼Œä»¥ä¼°ç®—åœ¨è­¦å¯Ÿæ‹˜ç•™ä¸­æ­»äº¡çš„é©¬å“ˆè¨Â·é˜¿ç±³å°¼å»ä¸–åï¼Œå¯¹ç”·å¥³å¹³ç­‰çš„æ€åº¦å‘ç”Ÿäº†å¦‚ä½•çš„å˜åŒ–ã€‚</li>
<li>methods: è¯¥ paper ä½¿ç”¨äº†ä¸€ä¸ªensemble active learningæŒºè®­ç»ƒä¸€ä¸ªç«‹åœºåˆ†ç±»å™¨ï¼Œå…¶ç‰¹ç‚¹åœ¨äºä¼Šæœ—å¥³æ€§å‚ä¸äº†æ´»åŠ¨çš„è§’è‰²ï¼Œä¸ä»…æä¾›æ ‡ç­¾ï¼Œè¿˜æä¾›äº†æœ‰ä»·å€¼çš„å…³é”®è¯ Ğ´Ğ»Ñæ›´åŠ æœ‰æ„ä¹‰çš„è¯æ±‡åˆ›é€ ä»¥åŠçŸ­ç¤ºæ–‡æ¡£ Ğ´Ğ»Ñå¯¼å‘é‡‡æ ·æ­¥éª¤ã€‚</li>
<li>results: åˆ†æç»“æœè¡¨æ˜ï¼Œè´å¨…Â·é˜¿ç±³å°¼å»ä¸–åï¼Œæ³¢æ–¯è¯­æ¨ç‰¹è®¨è®ºå‘ç”Ÿäº†åå¥½åŒ–çš„å˜åŒ–ï¼ŒåŒæ–¹çš„è´Ÿé¢å’Œæ­£é¢çš„æ¨ç‰¹æ•°é‡å‡å¢åŠ äº†ï¼Œå…¶ä¸­æ­£é¢æ¨ç‰¹æ•°é‡å¾®å°åœ°å¤§äºè´Ÿé¢æ¨ç‰¹æ•°é‡å¢åŠ ã€‚æ­¤å¤–ï¼Œä¸åŸºelineæ³¢æ–¯æ¨ç‰¹æ´»åŠ¨ç›¸æ¯”ï¼Œæ”¯æŒæŠ—è®®çš„æ¨ç‰¹è´¦æˆ·çš„åˆ›å»ºæ—¶é—´æ›´åŠ æ¥è¿‘äºåŸºelineã€‚<details>
<summary>Abstract</summary>
In this paper, we present a computational analysis of the Persian language Twitter discourse with the aim to estimate the shift in stance toward gender equality following the death of Mahsa Amini in police custody. We present an ensemble active learning pipeline to train a stance classifier. Our novelty lies in the involvement of Iranian women in an active role as annotators in building this AI system. Our annotators not only provide labels, but they also suggest valuable keywords for more meaningful corpus creation as well as provide short example documents for a guided sampling step. Our analyses indicate that Mahsa Amini's death triggered polarized Persian language discourse where both fractions of negative and positive tweets toward gender equality increased. The increase in positive tweets was slightly greater than the increase in negative tweets. We also observe that with respect to account creation time, between the state-aligned Twitter accounts and pro-protest Twitter accounts, pro-protest accounts are more similar to baseline Persian Twitter activity.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹æ³¢æ–¯è¯­æ¨ç‰¹è®¨è®ºè¿›è¡Œè®¡ç®—åˆ†æï¼Œä»¥ä¼°ç®—æ­»äºè­¦å¯Ÿæ‰§æ³•ä¸­çš„é©¬èµ›ç©†ç½•é»˜çš„å»ä¸–å¯¹æ€§åˆ«å¹³ç­‰çš„çœ‹æ³•äº§ç”Ÿå½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªååŒå­¦ä¹ æ¿€æ´»ç®¡é“ï¼Œä»¥è®­ç»ƒç«‹åœºåˆ†ç±»å™¨ã€‚æˆ‘ä»¬çš„åˆ›æ–°åœ¨äºä¼Šæœ—å¥³æ€§å‚ä¸äº†æ´»åŠ¨è§’è‰²ï¼Œä½œä¸ºæ ‡æ³¨äººå‘˜ï¼Œä¸ä»…æä¾›æ ‡ç­¾ï¼Œè¿˜æä¾›äº†æœ‰ä»·å€¼çš„å…³é”®è¯ï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ›å»ºè¯æ±‡åº“ï¼Œä»¥åŠçŸ­ç¤ºä¾‹æ–‡æ¡£ï¼Œç”¨äºæŒ‡å¯¼é‡‡æ ·æ­¥éª¤ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œé©¬èµ›ç©†ç½•é»˜çš„å»ä¸–å¯¼è‡´æ³¢æ–¯è¯­æ¨ç‰¹è®¨è®ºå‘ˆæåŒ–è¶‹åŠ¿ï¼Œè´Ÿé¢å’Œæ­£é¢çš„æ¨ç‰¹æ•°é‡å‡å¢åŠ ï¼Œä½†æ­£é¢æ¨ç‰¹æ•°é‡ç•¥å¤§äºè´Ÿé¢æ¨ç‰¹æ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨è´¦æˆ·åˆ›å»ºæ—¶é—´æ–¹é¢ï¼Œæ”¯æŒæŠ—è®®çš„æ¨ç‰¹è´¦æˆ·å’Œæ”¯æŒæ”¿åºœçš„æ¨ç‰¹è´¦æˆ·ä¹‹é—´çš„å·®å¼‚è¾ƒå°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Predicting-Outcomes-in-Long-COVID-Patients-with-Spatiotemporal-Attention"><a href="#Predicting-Outcomes-in-Long-COVID-Patients-with-Spatiotemporal-Attention" class="headerlink" title="Predicting Outcomes in Long COVID Patients with Spatiotemporal Attention"></a>Predicting Outcomes in Long COVID Patients with Spatiotemporal Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04770">http://arxiv.org/abs/2307.04770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Degan Hao, Mohammadreza Negahdar</li>
<li>for: é¢„æµ‹é•¿æœŸæ„ŸæŸ“COVID-19æ‚£è€…çš„ä¸¥é‡ç¨‹åº¦</li>
<li>methods: ä½¿ç”¨æœ¬åœ°LSTMå’Œå…±åŒç©ºé—´æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶é™åˆ¶çŸ­æœŸç›¸äº’ä¾èµ–å­¦ä¹ å’Œé•¿æœŸç›¸äº’ä¾èµ–å­¦ä¹ </li>
<li>results: åœ¨å…·æœ‰å›°éš¾é¢„æµ‹ç‰¹å¾çš„é•¿COVIDæ‚£è€…æ•°æ®é›†ä¸Šï¼Œæœ¬æ–¹æ³•æ¯”ç›¸å…³æ–¹æ³•è¡¨ç°å‡ºè‰²ï¼Œå¯ç”¨äºè¯„ä¼°é•¿COVIDæ‚£è€…çš„ä¸¥é‡ç¨‹åº¦ã€‚<details>
<summary>Abstract</summary>
Long COVID is a general term of post-acute sequelae of COVID-19. Patients with long COVID can endure long-lasting symptoms including fatigue, headache, dyspnea and anosmia, etc. Identifying the cohorts with severe long-term complications in COVID-19 could benefit the treatment planning and resource arrangement. However, due to the heterogeneous phenotype presented in long COVID patients, it is difficult to predict their outcomes from their longitudinal data. In this study, we proposed a spatiotemporal attention mechanism to weigh feature importance jointly from the temporal dimension and feature space. Considering that medical examinations can have interchangeable orders in adjacent time points, we restricted the learning of short-term dependency with a Local-LSTM and the learning of long-term dependency with the joint spatiotemporal attention. We also compared the proposed method with several state-of-the-art methods and a method in clinical practice. The methods are evaluated on a hard-to-acquire clinical dataset of patients with long COVID. Experimental results show the Local-LSTM with joint spatiotemporal attention outperformed related methods in outcome prediction. The proposed method provides a clinical tool for the severity assessment of long COVID.
</details>
<details>
<summary>æ‘˜è¦</summary>
é•¿æœŸ COVID æ˜¯ COVID-19 åé—ç—‡çš„æ€»ç§°ã€‚æ‚£æœ‰é•¿æœŸ COVID çš„æ‚£è€…å¯èƒ½ä¼šç»å†é•¿æœŸçš„ç—‡çŠ¶ï¼Œå¦‚ç–²åŠ³ã€å¤´ç—›ã€å‘¼å¸æ€¥ä¿ƒå’Œ anosmia ç­‰ã€‚ç¡®å®š COVID-19 æ‚£è€…é•¿æœŸgraveçš„åˆå¹¶ç—‡çŠ¶å¯ä»¥å¸®åŠ©è¯Šæ–­å’Œèµ„æºå®‰æ’ã€‚ç„¶è€Œï¼Œç”±äºé•¿æœŸ COVID æ‚£è€…çš„å¤šæ ·æ€§è¡¨ç°ï¼Œä» Ğ¸Ñ…é•¿æœŸæ•°æ®æ¥é¢„æµ‹ç»“æœå¾ˆå›°éš¾ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç©ºé—´æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥åŒæ—¶è¯„ä¼°ç‰¹å¾çš„é‡è¦æ€§ã€‚ç”±äºåŒ»å­¦æ£€æŸ¥å¯èƒ½ä¼šåœ¨é‚»è¿‘æ—¶é—´ç‚¹äº’æ¢æ£€æŸ¥é¡ºåºï¼Œæˆ‘ä»¬é™åˆ¶äº†çŸ­æœŸä¾èµ–æ€§å­¦ä¹ çš„ Local-LSTMï¼Œä»¥åŠä¸å…¶ä»–ç‰¹å¾ç©ºé—´è¿›è¡ŒåŒæ—¶å­¦ä¹ çš„åˆå¹¶ç©ºé—´æ—¶é—´æ³¨æ„åŠ›ã€‚æˆ‘ä»¬è¿˜å¯¹æˆ‘ä»¬çš„æ–¹æ³•ä¸å…¶ä»–ç°æœ‰æ–¹æ³•å’Œä¸´åºŠå®è·µä¸­çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚è¿™äº›æ–¹æ³•åœ¨æ™®ééš¾ä»¥è·å¾—çš„åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLocal-LSTM  WITH å…±åŒç©ºé—´æ—¶é—´æ³¨æ„åŠ›åœ¨ç»“æœé¢„æµ‹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†ç›¸å…³æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†è¯Šæ–­é•¿æœŸ COVID ä¸¥é‡ç¨‹åº¦çš„ä¸´åºŠå·¥å…·ã€‚
</details></li>
</ul>
<hr>
<h2 id="Formulation-Graphs-for-Mapping-Structure-Composition-of-Battery-Electrolytes-to-Device-Performance"><a href="#Formulation-Graphs-for-Mapping-Structure-Composition-of-Battery-Electrolytes-to-Device-Performance" class="headerlink" title="Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance"></a>Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03811">http://arxiv.org/abs/2307.03811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vidushi Sharma, Maxwell Giammona, Dmitry Zubarev, Andy Tek, Khanh Nugyuen, Linda Sundberg, Daniele Congiu, Young-Hye La</li>
<li>For: The paper is written for researchers and developers working on the discovery and development of new combinatorial materials, particularly in the context of battery electrolytes.* Methods: The paper proposes a deep learning model called Formulation Graph Convolution Network (F-GCN) that can predict the properties of liquid formulations based on the structure-composition relationship of their individual components. The model uses molecular descriptors derived from molecular graphs, informed by HOMO-LUMO and electric moment properties of the molecules.* Results: The paper demonstrates the effectiveness of the proposed model on two exemplary datasets related to battery electrolytes, achieving low errors in predicting performance metrics such as Coulombic Efficiency (CE) and specific capacity. The best performing F-GCN model uses molecular descriptors derived from molecular graphs that are informed with HOMO-LUMO and electric moment properties of the molecules.<details>
<summary>Abstract</summary>
Advanced computational methods are being actively sought for addressing the challenges associated with discovery and development of new combinatorial material such as formulations. A widely adopted approach involves domain informed high-throughput screening of individual components that can be combined into a formulation. This manages to accelerate the discovery of new compounds for a target application but still leave the process of identifying the right 'formulation' from the shortlisted chemical space largely a laboratory experiment-driven process. We report a deep learning model, Formulation Graph Convolution Network (F-GCN), that can map structure-composition relationship of the individual components to the property of liquid formulation as whole. Multiple GCNs are assembled in parallel that featurize formulation constituents domain-intuitively on the fly. The resulting molecular descriptors are scaled based on respective constituent's molar percentage in the formulation, followed by formalizing into a combined descriptor that represents a complete formulation to an external learning architecture. The use case of proposed formulation learning model is demonstrated for battery electrolytes by training and testing it on two exemplary datasets representing electrolyte formulations vs battery performance -- one dataset is sourced from literature about Li/Cu half-cells, while the other is obtained by lab-experiments related to lithium-iodide full-cell chemistry. The model is shown to predict the performance metrics like Coulombic Efficiency (CE) and specific capacity of new electrolyte formulations with lowest reported errors. The best performing F-GCN model uses molecular descriptors derived from molecular graphs that are informed with HOMO-LUMO and electric moment properties of the molecules using a knowledge transfer technique.
</details>
<details>
<summary>æ‘˜è¦</summary>
å½“å‰è®¡ç®—æ–¹æ³•åœ¨å¼€å‘æ–°çš„ combinatorialææ–™é¢†åŸŸä¸­æ˜¯æ´»è·ƒçš„æœå¯»ã€‚ä¸€ç§å¹¿æ³›é‡‡ç”¨çš„æ–¹æ³•æ˜¯é€šè¿‡åŸŸå†…é«˜é€Ÿå±é€‰ä¸ªåˆ«ç»„åˆ†ï¼Œä»¥åŠ é€Ÿé’ˆå¯¹ç‰¹å®šåº”ç”¨çš„æ–°åŒ–åˆç‰©çš„å‘ç°ã€‚ç„¶è€Œï¼Œä»çŸ­åˆ—è¡¨ä¸­é€‰æ‹©åˆé€‚çš„â€œå½¢æ€â€ä»ç„¶æ˜¯å®éªŒå®¤å®éªŒé©±åŠ¨çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬æŠ¥é“äº†ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå½¢æ€å›¾ convolutional neural network (F-GCN)ï¼Œå¯ä»¥å°†ä¸ªä½“ç»„åˆ†çš„ç»“æ„-ç»„åˆ†å…³ç³»æ˜ å°„åˆ°æ¶²ä½“å½¢æ€çš„æ€§èƒ½ã€‚å¤šä¸ªGCNè¢«ç´§å¯†åœ° assembled å¹¶åœ¨ fly ä¸ŠåŸŸç‰¹å¾åŒ–å½¢æ€æˆåˆ†ã€‚ resulting molecular descriptors è¢«æƒé‡æ ¹æ®å„ä¸ªæˆåˆ†çš„åˆ†å­æ¯”ä¾‹ç¼©æ”¾ï¼Œç„¶åä»¥ç»„åˆçš„æè¿°ç¬¦å½¢å¼ä¼ é€’ç»™å¤–éƒ¨å­¦ä¹ æ¶æ„ã€‚æˆ‘ä»¬ä½¿ç”¨äº† Li/Cu åŠç»†èƒå’Œé”‚iodide å…¨ç»†èƒåŒ–å­¦çš„ä¸¤ä¸ªæ•°æ®é›†æ¥è¯„ä¼°æˆ‘ä»¬çš„å½¢æ€å­¦ä¹ æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥é¢„æµ‹æ–°çš„ç”µè§£è´¨å½¢æ€çš„æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚ç”µå­æ•ˆç‡ï¼ˆCEï¼‰å’ŒSpecific capacityã€‚æˆ‘ä»¬çš„æœ€ä½³è¿è¡ŒF-GCNæ¨¡å‹ä½¿ç”¨åŸºäºåˆ†å­å›¾çš„åˆ†å­æè¿°ç¬¦ï¼Œå¹¶ä½¿ç”¨äº†çŸ¥è¯†ä¼ é€’æŠ€æœ¯ä»¥è·å¾—HOMO-LUMOå’Œç”µåŠ¨é‡ç‰¹æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="URL-A-Representation-Learning-Benchmark-for-Transferable-Uncertainty-Estimates"><a href="#URL-A-Representation-Learning-Benchmark-for-Transferable-Uncertainty-Estimates" class="headerlink" title="URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates"></a>URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03810">http://arxiv.org/abs/2307.03810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mkirchhof/url">https://github.com/mkirchhof/url</a></li>
<li>paper_authors: Michael Kirchhof, BÃ¡lint MucsÃ¡nyi, Seong Joon Oh, Enkelejda Kasneci</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿåœ¨æ–°æ•°æ®é›†ä¸Šä¼ è¾“çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿèƒ½å¤Ÿæä¾›å¯é çš„æœºå™¨å­¦ä¹ å’Œä¸ç¡®å®šé‡è¡¡çš„åŸºç¡€æ¨¡å‹ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ä¸€ç§æ–°çš„ uncertainty-aware representation learningï¼ˆURLï¼‰benchmarkï¼Œç”¨äºè¯„ä¼°elevenä¸ªä¸ç¡®å®šé‡è¡¡å™¨ï¼Œè¿™äº›ä¸ç¡®å®šé‡è¡¡å™¨åœ¨ImageNetä¸Šé¢„è®­ç»ƒåè¢«è½¬ç§»åˆ°äº†å…«ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œå¯¹äºè¡¨ç¤ºæœ¬èº«çš„ä¸ç¡®å®šæ€§æˆ–ç›´æ¥ä¼°è®¡é¢„æµ‹é£é™©çš„æ–¹æ³•æ¯”è¾ƒå‡ºè‰²ï¼Œä½†æ˜¯å®ç°å¯ä¼ è¾“çš„ä¸ç¡®å®šé‡è¡¡ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æŒ‘æˆ˜ã€‚<details>
<summary>Abstract</summary>
Representation learning has significantly driven the field to develop pretrained models that can act as a valuable starting point when transferring to new datasets. With the rising demand for reliable machine learning and uncertainty quantification, there is a need for pretrained models that not only provide embeddings but also transferable uncertainty estimates. To guide the development of such models, we propose the Uncertainty-aware Representation Learning (URL) benchmark. Besides the transferability of the representations, it also measures the zero-shot transferability of the uncertainty estimate using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers that are pretrained on ImageNet and transferred to eight downstream datasets. We find that approaches that focus on the uncertainty of the representation itself or estimate the prediction risk directly outperform those that are based on the probabilities of upstream classes. Yet, achieving transferable uncertainty quantification remains an open challenge. Our findings indicate that it is not necessarily in conflict with traditional representation learning goals. Code is provided under https://github.com/mkirchhof/url .
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œè¡¨è¾¾å­¦å­¦ä¹ â€åœ¨é¢†åŸŸä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œä½¿å¾—å¼€å‘è€…å¯ä»¥ä»æ–°æ•°æ®é›†ä¸Šè½¬ç§»åˆ°æ–°çš„ä»»åŠ¡ã€‚éšç€æœºå™¨å­¦ä¹ çš„å¯é æ€§å’Œä¸ç¡®å®šæ€§è¯„ä¼°çš„éœ€æ±‚å¢åŠ ï¼Œéœ€è¦å¼€å‘å¯ä»¥æä¾›åµŒå…¥å’Œä¼ è¾“ä¸ç¡®å®šæ€§ä¼°è®¡çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸ºäº†å¼•å¯¼è¿™ç±»æ¨¡å‹çš„å¼€å‘ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä¸ç¡®å®šæ€§æ„ŸçŸ¥å­¦ä¹ â€ï¼ˆURLï¼‰æ•°æ®é›†ã€‚é™¤äº†è¡¨è¾¾çš„ä¼ è¾“æ€§å¤–ï¼Œå®ƒè¿˜æµ‹é‡äº†é›¶æ‰¹è½¬ç§»ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–°æŒ‡æ ‡ã€‚æˆ‘ä»¬å¯¹ImageNeté¢„è®­ç»ƒçš„åä¸€ç§ä¸ç¡®å®šé‡è¿›è¡Œäº†URLçš„è¯„ä¼°ï¼Œå¹¶å°†å…¶è½¬ç§»åˆ°å…«ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šã€‚æˆ‘ä»¬å‘ç°ï¼Œå…³æ³¨è¡¨è¾¾ä¸ç¡®å®šæ€§æœ¬èº«æˆ–ç›´æ¥ä¼°è®¡é¢„æµ‹é£é™©çš„æ–¹æ³•è¡¨ç°è¾ƒå¥½ã€‚ç„¶è€Œï¼Œå®ç°ä¼ è¾“ä¸ç¡®å®šæ€§ä¼°è®¡ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œè¿™å¹¶ä¸æ˜¯ä¼ ç»Ÿè¡¨è¾¾å­¦ä¹ ç›®æ ‡çš„çŸ›ç›¾ã€‚ä»£ç å¯ä»¥åœ¨<https://github.com/mkirchhof/url>è·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Theoretical-Perspective-on-Subnetwork-Contributions-to-Adversarial-Robustness"><a href="#A-Theoretical-Perspective-on-Subnetwork-Contributions-to-Adversarial-Robustness" class="headerlink" title="A Theoretical Perspective on Subnetwork Contributions to Adversarial Robustness"></a>A Theoretical Perspective on Subnetwork Contributions to Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03803">http://arxiv.org/abs/2307.03803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jovon Craig, Josh Andle, Theodore S. Nowak, Salimeh Yasaei Sekeh</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†ç ”ç©¶æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰å¯¹æ”»å‡»çš„Robustnessè¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‡ç»“å’Œå®‰å…¨åº”ç”¨ä¸­çš„æ¨¡å‹å®‰å…¨æ€§ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†å¯¹DNNsè¿›è¡Œé’ˆå¯¹æ€§æ”»å‡»çš„è®­ç»ƒæ–¹æ³•æ¥å¼ºåŒ–å…¶å¯¹æ”»å‡»çš„Robustnessï¼Œå¹¶è¯æ˜äº†è¿™ç§æ–¹æ³•å¯ä»¥åœ¨æ•´ä¸ªæ¨¡å‹ä¸Šåº”ç”¨è®¡ç®—æˆæœ¬çš„é«˜çš„è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>results: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ç†è®ºæ¡†æ¶ï¼Œç”¨äºç ”ç©¶æ”»å‡»å¦‚ä½•å½±å“æ•´ä¸ªç½‘ç»œçš„Robustnessï¼Œå¹¶æä¾›äº†ä¸€ç§æµ‹è¯•è¿™ç§ç†è®ºçš„æ–¹æ³•ã€‚ç»éªŒè¡¨æ˜ï¼Œå¦‚æœæŸä¸ªå­ç½‘ç»œå…·æœ‰ä¸€å®šçš„é²æ£’æ€§ï¼Œé‚£ä¹ˆæ•´ä¸ªç½‘ç»œä¹Ÿæ˜¯é²æ£’çš„ï¼Œå¹¶ä¸”éœ€è¦åœ¨ä¸åŒå±‚æ¬¡ä¹‹é—´å­˜åœ¨ä¸€å®šçš„ä¾èµ–å…³ç³»ã€‚<details>
<summary>Abstract</summary>
The robustness of deep neural networks (DNNs) against adversarial attacks has been studied extensively in hopes of both better understanding how deep learning models converge and in order to ensure the security of these models in safety-critical applications. Adversarial training is one approach to strengthening DNNs against adversarial attacks, and has been shown to offer a means for doing so at the cost of applying computationally expensive training methods to the entire model. To better understand these attacks and facilitate more efficient adversarial training, in this paper we develop a novel theoretical framework that investigates how the adversarial robustness of a subnetwork contributes to the robustness of the entire network. To do so we first introduce the concept of semirobustness, which is a measure of the adversarial robustness of a subnetwork. Building on this concept, we then provide a theoretical analysis to show that if a subnetwork is semirobust and there is a sufficient dependency between it and each subsequent layer in the network, then the remaining layers are also guaranteed to be robust. We validate these findings empirically across multiple DNN architectures, datasets, and adversarial attacks. Experiments show the ability of a robust subnetwork to promote full-network robustness, and investigate the layer-wise dependencies required for this full-network robustness to be achieved.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰çš„å¯¹æŠ—æ”»å‡»çš„ç¨³å®šæ€§å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åè°ƒæ–¹å¼ï¼Œå¹¶ç¡®ä¿è¿™äº›æ¨¡å‹åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„å®‰å…¨æ€§ã€‚å¯¹æŠ—è®­ç»ƒæ˜¯ä¸€ç§åŠ å¼ºDNNså¯¹æŠ—æ”»å‡»çš„æ–¹æ³•ï¼Œå¹¶å·²ç»è¯æ˜å¯ä»¥é€šè¿‡å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œè®¡ç®—æ˜‚è´µçš„è®­ç»ƒæ–¹æ³•æ¥å®ç°ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™äº›æ”»å‡»å’Œå®ç°æ›´æœ‰æ•ˆçš„å¯¹æŠ—è®­ç»ƒï¼Œåœ¨è¿™ç¯‡è®ºæ–‡ä¸­æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„ç†è®ºæ¡†æ¶ï¼Œä»¥ investigateå¦‚ä½•å„ä¸ªå­ç½‘ç»œçš„å¯¹æŠ—ç¨³å®šæ€§å¯¹æ•´ä¸ªç½‘ç»œçš„ç¨³å®šæ€§çš„è´¡çŒ®ã€‚æˆ‘ä»¬é¦–å…ˆä»‹ç»äº†semirobustnessè¿™ä¸ªæ¦‚å¿µï¼Œå®ƒæ˜¯ä¸€ç§å¯¹æŠ—ç¨³å®šæ€§çš„åº¦é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§ç†è®ºåˆ†æï¼Œè¯æ˜å¦‚æœä¸€ä¸ªå­ç½‘ç»œæ˜¯semirobustçš„ï¼Œå¹¶ä¸”æ¯ä¸ªåç»­å±‚ä¸å…¶ä¹‹é—´å­˜åœ¨è¶³å¤Ÿçš„ä¾èµ–å…³ç³»ï¼Œé‚£ä¹ˆå‰©ä¸‹çš„å±‚ä¹Ÿä¸€å®šæ˜¯ç¨³å®šçš„ã€‚æˆ‘ä»¬éªŒè¯äº†è¿™äº›å‘ç°çš„å®éªŒç»“æœï¼Œå¹¶å¯¹å¤šä¸ªDNNæ¶æ„ã€æ•°æ®é›†å’Œæ”»å‡»æ–¹æ³•è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒè¡¨æ˜ï¼Œä¸€ä¸ªç¨³å®šçš„å­ç½‘ç»œå¯ä»¥æ¨åŠ¨æ•´ä¸ªç½‘ç»œçš„ç¨³å®šæ€§ï¼Œå¹¶ä¸”è°ƒæŸ¥å±‚é—´çš„ä¾èµ–å…³ç³»å¯ä»¥å®ç°è¿™ç§å…¨ç½‘ç»œç¨³å®šæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="CLIPMasterPrints-Fooling-Contrastive-Language-Image-Pre-training-Using-Latent-Variable-Evolution"><a href="#CLIPMasterPrints-Fooling-Contrastive-Language-Image-Pre-training-Using-Latent-Variable-Evolution" class="headerlink" title="CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution"></a>CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03798">http://arxiv.org/abs/2307.03798</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matfrei/clipmasterprints">https://github.com/matfrei/clipmasterprints</a></li>
<li>paper_authors: Matthias Freiberger, Peter Kun, Anders Sundnes LÃ¸vlie, Sebastian Risi</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨æ¢è®¨ Contrastive Language-Image Pre-training (CLIP) æ¨¡å‹åœ¨é¢å¯¹â€œä¼ªè£…ä¸»è¦å›¾åƒâ€ï¼ˆfooling master imagesï¼‰æ—¶çš„æŠ¤å«æœºåˆ¶ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†æ¼”åŒ–ç­–ç•¥å’Œæ‚åŒ–åº¦è§„é¿ç­–ç•¥æ¥æœå¯» CLIP æ¨¡å‹çš„æ˜“è®­ç»ƒå›¾åƒï¼Œå¹¶ investigate è¿™äº›å›¾åƒçš„ç‰¹æ€§å’Œæ™®é€‚æ€§ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨å°‘é‡å›¾åƒæ ‡ç­¾å¯ä»¥ç”Ÿæˆå¤§é‡semanticallyç›¸å…³çš„å›¾åƒï¼Œè€Œä¸”è¿™äº›å›¾åƒå¯ä»¥è®© CLIP æ¨¡å‹å…·æœ‰é«˜ç½®ä¿¡åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç° modality gap åœ¨å¤šmodalç½‘ç»œä¸­å¯¼è‡´ CLIP æ¨¡å‹æ˜“å—åˆ°ä¼ªè£…ä¸»è¦å›¾åƒçš„æ”»å‡»ã€‚<details>
<summary>Abstract</summary>
Models leveraging both visual and textual data such as Contrastive Language-Image Pre-training (CLIP), are increasingly gaining importance. In this work, we show that despite their versatility, such models are vulnerable to what we refer to as fooling master images. Fooling master images are capable of maximizing the confidence score of a CLIP model for a significant number of widely varying prompts, while being unrecognizable for humans. We demonstrate how fooling master images can be mined by searching the latent space of generative models by means of an evolution strategy or stochastic gradient descent. We investigate the properties of the mined fooling master images, and find that images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions. Further, we evaluate two possible mitigation strategies and find that vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained multi-modal networks. From the perspective of vulnerability to off-manifold attacks, we therefore argue for the mitigation of modality gaps in CLIP and related multi-modal approaches. Source code and mined CLIPMasterPrints are available at https://github.com/matfrei/CLIPMasterPrints.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ¨¡å‹åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œå¦‚å¯¹ç…§è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ï¼Œåœ¨å½“å‰ç ”ç©¶ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹å…·æœ‰å¤šæ ·æ€§ï¼Œä½†å®ƒä»¬å´å®¹æ˜“å—åˆ°æˆ‘ä»¬ç§°ä¸ºâ€œæ¬ºéª—ä¸»å›¾â€çš„æ”»å‡»ã€‚æ¬ºéª—ä¸»å›¾å¯ä»¥è®©CLIPæ¨¡å‹å¯¹å„ç§å„æ ·çš„æç¤ºè¿›è¡Œæœ€å¤§åŒ–ä¿¡ä»»åˆ†æ•°ï¼Œè€Œhumanæ˜¯æ— æ³•è¯†åˆ«çš„ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå¯ä»¥é€šè¿‡æ¼”åŒ–ç­–ç•¥æˆ–æƒé‡ä¸‹é™æ¥åœ¨ç”Ÿæˆæ¨¡å‹çš„latentç©ºé—´ä¸­æœå¯»æ¬ºéª—ä¸»å›¾ã€‚æˆ‘ä»¬ç ”ç©¶æ¬ºéª—ä¸»å›¾çš„æ€§è´¨ï¼Œå‘ç°å›¾åƒé€šè¿‡å°‘é‡çš„å›¾åƒæè¿°è®­ç»ƒå¯ä»¥å¯¹Semanticallyç›¸å…³çš„æç¤ºè¿›è¡Œæ‰©å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§å¯èƒ½çš„é˜²èŒƒç­–ç•¥ï¼Œå‘ç°æ”»å‡»æ¨¡å¼ä¸å¤šæ ·æ€§å·®æœ‰closeå…³ç³»ã€‚ä»é˜²èŒƒå¤šæ ·æ€§å·®çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬ argue for the mitigation of modality gaps in CLIP and related multi-modal approachesã€‚ä»£ç å’Œæœå¯»åˆ°çš„CLIPMasterPrintså¯ä»¥åœ¨https://github.com/matfrei/CLIPMasterPrintsä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Lottery-Ticket-Hypothesis-with-Explainability-Methods-Insights-into-Sparse-Network-Performance"><a href="#Exploring-the-Lottery-Ticket-Hypothesis-with-Explainability-Methods-Insights-into-Sparse-Network-Performance" class="headerlink" title="Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance"></a>Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13698">http://arxiv.org/abs/2307.13698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shantanu Ghosh, Kayhan Batmanghelich</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ‰¾åˆ°å…·æœ‰æ¯”è¾ƒé«˜æ€§èƒ½çš„ç¨€ç¼ºç½‘ç»œï¼Œå¹¶è§£é‡Šè¿™äº›ç¨€ç¼ºç½‘ç»œçš„æ€§èƒ½æ˜¯å¦‚ä½•é€æ¸æé«˜æˆ–ä¸‹é™çš„ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†Grad-CAMå’ŒPost-hocæ¦‚å¿µç“¶é¡¶æ¨¡å‹ï¼ˆPCBMsï¼‰æ¥è°ƒæŸ¥å‡å°‘ç½‘ç»œä¸­çš„æƒé‡åï¼Œç½‘ç»œçš„è§£é‡Šæ€§ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œéšç€æƒé‡çš„å‡å°‘ï¼Œç½‘ç»œçš„æ€§èƒ½é€æ¸ä¸‹é™ï¼Œå¹¶ä¸”å‘ç°äº†åŸå§‹ç½‘ç»œä¸­çš„æ¦‚å¿µå’Œåƒç´ ä¸å‡å°‘åçš„ç½‘ç»œå­˜åœ¨å·®å¼‚ï¼Œè¿™å¯èƒ½æ˜¯æ€§èƒ½ä¸‹é™çš„åŸå› ã€‚<details>
<summary>Abstract</summary>
Discovering a high-performing sparse network within a massive neural network is advantageous for deploying them on devices with limited storage, such as mobile phones. Additionally, model explainability is essential to fostering trust in AI. The Lottery Ticket Hypothesis (LTH) finds a network within a deep network with comparable or superior performance to the original model. However, limited study has been conducted on the success or failure of LTH in terms of explainability. In this work, we examine why the performance of the pruned networks gradually increases or decreases. Using Grad-CAM and Post-hoc concept bottleneck models (PCBMs), respectively, we investigate the explainability of pruned networks in terms of pixels and high-level concepts. We perform extensive experiments across vision and medical imaging datasets. As more weights are pruned, the performance of the network degrades. The discovered concepts and pixels from the pruned networks are inconsistent with the original network -- a possible reason for the drop in performance.
</details>
<details>
<summary>æ‘˜è¦</summary>
å‘ç°é«˜æ€§èƒ½çš„ç®€åŒ–ç½‘ç»œåœ¨å·¨é‡ç¥ç»ç½‘ç»œä¸­æ˜¯æœ‰åˆ©äºåœ¨å…·æœ‰æœ‰é™å­˜å‚¨çš„è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œå¦‚ç§»åŠ¨ç”µè¯ã€‚æ­¤å¤–ï¼ŒAIä¿¡ä»»çš„é‡è¦å› ç´ ä¹‹ä¸€æ˜¯æ¨¡å‹è§£é‡Šæ€§ã€‚æŠ½å¥–å‡è®¾ï¼ˆLTHï¼‰æ‰¾åˆ°äº†æ·±åº¦ç½‘ç»œä¸­çš„ç›¸ä¼¼æˆ–æ›´é«˜æ€§èƒ½çš„ç½‘ç»œï¼Œä½†æœ‰é™çš„ç ”ç©¶å¯¹LTHçš„æˆåŠŸæˆ–å¤±è´¥è¿›è¡Œäº†è§£é‡Šæ€§ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å‰ªé™¤ç½‘ç»œæ€§èƒ½çš„æé«˜æˆ–ä¸‹é™çš„åŸå› ã€‚ä½¿ç”¨Grad-CAMå’Œåç½®æ¦‚å¿µç“¶é¢ˆæ¨¡å‹ï¼ˆPCBMï¼‰ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å‰ªé™¤ç½‘ç»œçš„è§£é‡Šæ€§ç ”ç©¶ï¼Œå³åƒç´ å’Œé«˜çº§æ¦‚å¿µçš„è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨è§†è§‰å’ŒåŒ»å­¦å½±åƒ dataset ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚éšç€æ›´å¤šçš„æƒé‡è¢«å‰ªé™¤ï¼Œç½‘ç»œæ€§èƒ½ä¸‹é™ã€‚å‘ç°çš„æ¦‚å¿µå’Œåƒç´ ä»å‰ªé™¤ç½‘ç»œä¸­ä¸åŸå§‹ç½‘ç»œä¸åŒï¼Œå¯èƒ½æ˜¯æ€§èƒ½ä¸‹é™çš„åŸå› ã€‚
</details></li>
</ul>
<hr>
<h2 id="Neural-Abstraction-Based-Controller-Synthesis-and-Deployment"><a href="#Neural-Abstraction-Based-Controller-Synthesis-and-Deployment" class="headerlink" title="Neural Abstraction-Based Controller Synthesis and Deployment"></a>Neural Abstraction-Based Controller Synthesis and Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03783">http://arxiv.org/abs/2307.03783</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/msalamati/neural-representation">https://github.com/msalamati/neural-representation</a></li>
<li>paper_authors: Rupak Majumdar, Mahmoud Salamati, Sadegh Soudjani</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜æŠ½è±¡åŸºæœ¬æ–¹æ³•çš„å†…å­˜æ•ˆç‡ï¼Œä»¥ä¾¿åœ¨å®æ—¶æ§åˆ¶ä¸­åº”ç”¨ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œè¡¨ç¤ºçš„å†…å­˜æœ‰æ•ˆçš„æ§åˆ¶å™¨ç”Ÿæˆæ–¹æ³•ï¼ŒåŒ…æ‹¬åœ¨æ‰§è¡Œé˜¶æ®µä½¿ç”¨å‹ç¼©ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼Œä»¥åŠåœ¨è®­ç»ƒé˜¶æ®µä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å‡å°‘å†…å­˜å ç”¨ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å‡å°‘æŠ½è±¡åŸºæœ¬æ–¹æ³•çš„å†…å­˜å ç”¨ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å®æ—¶æ§åˆ¶ä¸­åº”ç”¨ã€‚åœ¨é€‰æ‹©çš„æ ‡å‡†å¥—ä»¶ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å‡å°‘å¹³å‡å†…å­˜å ç”¨é‡åˆ†åˆ«ä¸º1.31Ã—10^5å’Œ7.13Ã—10^3ï¼Œæœ€é«˜è¾¾7.54Ã—10^5å’Œ3.18Ã—10^4ã€‚<details>
<summary>Abstract</summary>
Abstraction-based techniques are an attractive approach for synthesizing correct-by-construction controllers to satisfy high-level temporal requirements. A main bottleneck for successful application of these techniques is the memory requirement, both during controller synthesis and in controller deployment.   We propose memory-efficient methods for mitigating the high memory demands of the abstraction-based techniques using neural network representations. To perform synthesis for reach-avoid specifications, we propose an on-the-fly algorithm that relies on compressed neural network representations of the forward and backward dynamics of the system. In contrast to usual applications of neural representations, our technique maintains soundness of the end-to-end process. To ensure this, we correct the output of the trained neural network such that the corrected output representations are sound with respect to the finite abstraction. For deployment, we provide a novel training algorithm to find a neural network representation of the synthesized controller and experimentally show that the controller can be correctly represented as a combination of a neural network and a look-up table that requires a substantially smaller memory.   We demonstrate experimentally that our approach significantly reduces the memory requirements of abstraction-based methods. For the selected benchmarks, our approach reduces the memory requirements respectively for the synthesis and deployment by a factor of $1.31\times 10^5$ and $7.13\times 10^3$ on average, and up to $7.54\times 10^5$ and $3.18\times 10^4$. Although this reduction is at the cost of increased off-line computations to train the neural networks, all the steps of our approach are parallelizable and can be implemented on machines with higher number of processing units to reduce the required computational time.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå¯¹äºé«˜æ°´å¹³æ—¶é—´éœ€æ±‚çš„æ­£ç¡®æ§åˆ¶å™¨çš„åˆæˆï¼Œå…·æœ‰å¸å¼•åŠ›çš„æ–¹æ³•æ˜¯åŸºäºæŠ½è±¡çš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯çš„è®°å¿†éœ€æ±‚åœ¨æ§åˆ¶å™¨åˆæˆå’Œéƒ¨ç½²è¿‡ç¨‹ä¸­éƒ½æ˜¯ä¸»è¦çš„ç“¶é¢ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€äº›è®°å¿†æ•ˆç‡çš„æ–¹æ³•ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¡¨ç¤ºæ³•æ¥å‡å°‘é«˜çº§æŠ½è±¡æŠ€æœ¯çš„è®°å¿†éœ€æ±‚ã€‚ä¸ºäº†å®ç°è¿™äº›ç›®çš„ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨çº¿ç®—æ³•ï¼Œå®ƒåŸºäºå‹ç¼©ç¥ç»ç½‘ç»œè¡¨ç¤ºæ³•æ¥è¿›è¡Œæ§åˆ¶å™¨åˆæˆã€‚ä¸ä¼ ç»Ÿç¥ç»ç½‘ç»œåº”ç”¨ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿æŒäº†é€”å¾„çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†ç¡®ä¿è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä¼šå¯¹è®­ç»ƒç¥ç»ç½‘ç»œè¾“å‡ºè¿›è¡Œä¿®æ­£ï¼Œä½¿å…¶ä¸æœ‰é™æŠ½è±¡ä¹‹é—´ä¿æŒç›¸å¯¹çš„å‡†ç¡®æ€§ã€‚åœ¨éƒ¨ç½²é˜¶æ®µï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§æ–°çš„è®­ç»ƒç®—æ³•ï¼Œç”¨äºåœ¨ç¥ç»ç½‘ç»œå’Œlookupè¡¨ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ªå¯ä»¥å‡å°‘è®°å¿†éœ€æ±‚çš„æ§åˆ¶å™¨è¡¨ç¤ºã€‚æˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å‡å°‘æŠ½è±¡åŸºäºæ–¹æ³•çš„è®°å¿†éœ€æ±‚ã€‚å¯¹äºæˆ‘ä»¬é€‰æ‹©çš„æ ‡å‡†å¥—ä»¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ†åˆ«å‡å°‘äº†åœ¨åˆæˆå’Œéƒ¨ç½²é˜¶æ®µçš„è®°å¿†éœ€æ±‚çš„å¹³å‡å€¼ä¸º1.31\*10^5å’Œ7.13\*10^3ã€‚æœ€å¤šå¯ä»¥å‡å°‘åˆ°7.54\*10^5å’Œ3.18\*10^4ã€‚è™½ç„¶è¿™äº›å‡å°‘æ˜¯åœ¨è®­ç»ƒç¥ç»ç½‘ç»œçš„è¿‡ç¨‹ä¸­ä»˜å‡ºçš„æˆæœ¬ï¼Œä½†æ‰€æœ‰çš„æ­¥éª¤éƒ½å¯ä»¥å¹¶è¡Œè¿›è¡Œå¹¶åœ¨é«˜å¤„ç†å™¨æ•°é‡çš„æœºå™¨ä¸Šè¿›è¡Œå®ç°ï¼Œä»¥å‡å°‘æ‰€éœ€çš„è®¡ç®—æ—¶é—´ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="When-does-the-ID-algorithm-fail"><a href="#When-does-the-ID-algorithm-fail" class="headerlink" title="When does the ID algorithm fail?"></a>When does the ID algorithm fail?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03750">http://arxiv.org/abs/2307.03750</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SOYJUN/Implement-ODR-protocol">https://github.com/SOYJUN/Implement-ODR-protocol</a></li>
<li>paper_authors: Ilya Shpitser</li>
<li>for: æœ¬æ–‡ç ”ç©¶çš„æ˜¯IDç®—æ³•åœ¨å›¾è§£é‡Šæ¨¡å‹ä¸­è¿›è¡Œ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ĞµĞ½Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒåˆ†å¸ƒIDé—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨çš„æ–¹æ³•åŒ…æ‹¬IDç®—æ³•çš„å„ç§è¡¨è¿°ï¼Œä»¥åŠå¯¹IDç®—æ³•çš„å®Œå–„æ€§å’Œæœ‰æ•ˆæ€§çš„åˆ†æã€‚</li>
<li>results: æœ¬æ–‡æå‡ºäº†ä¸€äº›å¯¹IDç®—æ³•çš„æ‰¹è¯„å’Œæ”¹è¿›ï¼ŒåŒ…æ‹¬æŒ‡å‡ºIDç®—æ³•åœ¨æŸäº›æƒ…å†µä¸‹ä¼šå¤±è´¥ï¼Œå¹¶æä¾›äº†ä¸€äº›å›¾å½¢åŒ–çš„Characterizationæ¥æè¿°è¿™äº›æƒ…å†µã€‚<details>
<summary>Abstract</summary>
The ID algorithm solves the problem of identification of interventional distributions of the form p(Y | do(a)) in graphical causal models, and has been formulated in a number of ways [12, 9, 6]. The ID algorithm is sound (outputs the correct functional of the observed data distribution whenever p(Y | do(a)) is identified in the causal model represented by the input graph), and complete (explicitly flags as a failure any input p(Y | do(a)) whenever this distribution is not identified in the causal model represented by the input graph).   The reference [9] provides a result, the so called "hedge criterion" (Corollary 3), which aims to give a graphical characterization of situations when the ID algorithm fails to identify its input in terms of a structure in the input graph called the hedge. While the ID algorithm is, indeed, a sound and complete algorithm, and the hedge structure does arise whenever the input distribution is not identified, Corollary 3 presented in [9] is incorrect as stated. In this note, I outline the modern presentation of the ID algorithm, discuss a simple counterexample to Corollary 3, and provide a number of graphical characterizations of the ID algorithm failing to identify its input distribution.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œIDç®—æ³•è§£å†³äº†å›¾structural causalæ¨¡å‹ä¸­p(Y | do(a))çš„åˆ†å¸ƒæ ‡å®šé—®é¢˜ï¼Œå¹¶åœ¨å¤šç§å½¢å¼ä¸‹è¡¨è¿°ï¼ˆ[12, 9, 6]ï¼‰ã€‚IDç®—æ³•æ˜¯æœ‰æ•ˆçš„ï¼ˆå¯¹äºè¾“å…¥åˆ†å¸ƒp(Y | do(a))ï¼Œè¾“å‡ºæ­£ç¡®çš„å‡½æ•°ï¼‰ï¼Œå¹¶ä¸”æ˜¯å®Œæ•´çš„ï¼ˆå¦‚æœè¾“å…¥åˆ†å¸ƒä¸èƒ½åœ¨å›¾structural causalæ¨¡å‹ä¸­æ ‡å®šï¼Œåˆ™ç›´æ¥æ ‡è®°ä¸ºå¤±è´¥ï¼‰ã€‚â€â€œå‚è€ƒ[9]ä¸­çš„ç»“è®ºï¼ˆå³â€˜åˆ«branch criterionâ€™ï¼‰æä¾›äº†ä¸€ç§å›¾Structural characterization of situations when the ID algorithm fails to identify its input, in terms of a structure in the input graph called the hedge. However, this conclusion is incorrect as stated, and in this note, I present a modern presentation of the ID algorithm and a simple counterexample to Corollary 3. Additionally, I provide several graphical characterizations of the ID algorithm failing to identify its input distribution.â€
</details></li>
</ul>
<hr>
<h2 id="Incentive-Theoretic-Bayesian-Inference-for-Collaborative-Science"><a href="#Incentive-Theoretic-Bayesian-Inference-for-Collaborative-Science" class="headerlink" title="Incentive-Theoretic Bayesian Inference for Collaborative Science"></a>Incentive-Theoretic Bayesian Inference for Collaborative Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03748">http://arxiv.org/abs/2307.03748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Bates, Michael I. Jordan, Michael Sklar, Jake A. Soloff</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†ç ”ç©¶ç°ä»£ç§‘å­¦ç ”ç©¶ä¸­çš„åˆ†å¸ƒå¼ã€åä½œæ€§ï¼Œä»¥åŠç ”ç©¶äººå‘˜ã€ç®¡åˆ¶æœºæ„ã€èµ„é‡‘æœºæ„ã€å•†ä¸šä¼™ä¼´å’Œç§‘å­¦æœºæ„ä¹‹é—´çš„äº’åŠ¨å’Œä¸åŒçš„é©±åŠ¨åŠ›ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§å‡è®¾æ£€éªŒæ–¹æ³•ï¼Œå…¶ä¸­ä¸€ä¸ªä»£ç†äººï¼ˆä¾‹å¦‚ç ”ç©¶äººå‘˜æˆ–è¯å“å…¬å¸ï¼‰æœ‰ä¸€ä¸ªç§äººå‰ç½®ä¿¡æ¯ï¼Œè€Œæ‰§è¡Œè€…ï¼ˆä¾‹å¦‚æ”¿ç­–åˆ¶å®šè€…æˆ–ç›‘ç®¡æœºæ„ï¼‰å¸Œæœ›æ ¹æ®å‚æ•°å€¼è¿›è¡Œå†³ç­–ã€‚ä»£ç†äººé€‰æ‹©æ˜¯å¦è¿›è¡Œç»Ÿè®¡è¯•éªŒï¼Œç„¶åè¯•éªŒçš„ç»“æœè¢«ç”¨äºæ‰§è¡Œè€…è¿›è¡Œå†³ç­–ã€‚</li>
<li>results: è®ºæ–‡è¡¨æ˜äº†æ‰§è¡Œè€…å¯ä»¥é€šè¿‡ä»£ç†äººçš„å†³ç­–è¡Œä¸ºæ¥æ­ç¤ºéƒ¨åˆ†ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥æ§åˆ¶ posterior æ¦‚ç‡çš„å€¼ã€‚è¿™ä¸€ç»“æœæœ‰ä¸€ä¸ªé‡è¦çš„åº”ç”¨æ˜¯åœ¨ä¸´åºŠè¯•éªŒä¸­è®¾ç½®ç±»å‹ä¸€é”™è¯¯æ°´å¹³ï¼šè¯•éªŒç±»å‹ä¸€é”™è¯¯æ°´å¹³åº”è¯¥æ˜¯ä¸´åºŠè¯•éªŒæˆæœ¬é™¤ä»¥ä¼ä¸šåˆ©æ¶¦çš„æ¯”ç‡ã€‚<details>
<summary>Abstract</summary>
Contemporary scientific research is a distributed, collaborative endeavor, carried out by teams of researchers, regulatory institutions, funding agencies, commercial partners, and scientific bodies, all interacting with each other and facing different incentives. To maintain scientific rigor, statistical methods should acknowledge this state of affairs. To this end, we study hypothesis testing when there is an agent (e.g., a researcher or a pharmaceutical company) with a private prior about an unknown parameter and a principal (e.g., a policymaker or regulator) who wishes to make decisions based on the parameter value. The agent chooses whether to run a statistical trial based on their private prior and then the result of the trial is used by the principal to reach a decision. We show how the principal can conduct statistical inference that leverages the information that is revealed by an agent's strategic behavior -- their choice to run a trial or not. In particular, we show how the principal can design a policy to elucidate partial information about the agent's private prior beliefs and use this to control the posterior probability of the null. One implication is a simple guideline for the choice of significance threshold in clinical trials: the type-I error level should be set to be strictly less than the cost of the trial divided by the firm's profit if the trial is successful.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°ä»£ç§‘å­¦ç ”ç©¶æ˜¯ä¸€é¡¹åˆ†å¸ƒå¼ã€åˆä½œæ€§çš„åŠªåŠ›ï¼Œç”±ç ”ç©¶äººå‘˜ã€ç®¡ç†æœºæ„ã€èµ„é‡‘æœºæ„ã€å•†ä¸šä¼™ä¼´å’Œç§‘å­¦æœºæ„å…±åŒå‚ä¸ï¼Œè¿™äº›ç»„ç»‡ä¹‹é—´å­˜åœ¨ä¸åŒçš„é©±åŠ¨å’Œæ¿€åŠ±ã€‚ä¸ºä¿æŒç§‘å­¦çš„ä¸¥è°¨æ€§ï¼Œç»Ÿè®¡æ–¹æ³•åº”è¯¥è€ƒè™‘è¿™ç§æƒ…å†µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç ”ç©¶åœ¨æœ‰ä¸€ä¸ªä»£ç†äººï¼ˆä¾‹å¦‚ç ”ç©¶äººå‘˜æˆ–è¯å“åˆ¶é€ å•†ï¼‰æœ‰ç§äººä¼°è®¡å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ£€æµ‹ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹çš„é—®é¢˜ã€‚ä»£ç†äººæ ¹æ®è‡ªå·±çš„ç§äººä¼°è®¡é€‰æ‹©æ˜¯å¦è¿›è¡Œç»Ÿè®¡è¯•éªŒï¼Œç„¶åè¯•éªŒç»“æœè¢«ä½¿ç”¨è€…æ¥åšå†³ç­–ã€‚æˆ‘ä»¬è¡¨æ˜å¦‚ä½•ä½¿å¾—é¦–è„‘å¯ä»¥é€šè¿‡ä»£ç†äººçš„æˆ˜ç•¥è¡Œä¸ºï¼ˆå³æ˜¯å¦è¿›è¡Œè¯•éªŒï¼‰äº†è§£ä¸€éƒ¨åˆ†ç§äººä¼°è®¡ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥æ§åˆ¶åéªŒ posterior æ¦‚ç‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜å¦‚ä½•ä½¿å¾—é¦–è„‘å¯ä»¥è®¾è®¡ä¸€ç§æ”¿ç­–æ¥æè¿°ä»£ç†äººçš„ç§äººä¼°è®¡ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥æ§åˆ¶åéªŒ posterior æ¦‚ç‡ã€‚è¿™ä¸€ç»“è®ºä¹‹ä¸€æ˜¯ä¸€ä¸ªç®€å•çš„ä¸´åºŠè¯•éªŒé€‰æ‹©æ°´å¹³æŒ‡å—ï¼šç±»å‹ä¸€é”™è¯¯æ°´å¹³åº”è¯¥è®¾ç½®ä¸ºè¯•éªŒæˆæœ¬é™¤ä»¥æˆåŠŸååˆ©æ¶¦çš„æ¯”ç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="QIGen-Generating-Efficient-Kernels-for-Quantized-Inference-on-Large-Language-Models"><a href="#QIGen-Generating-Efficient-Kernels-for-Quantized-Inference-on-Large-Language-Models" class="headerlink" title="QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models"></a>QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03738">http://arxiv.org/abs/2307.03738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ist-daslab/qigen">https://github.com/ist-daslab/qigen</a></li>
<li>paper_authors: Tommaso Pegolotti, Elias Frantar, Dan Alistarh, Markus PÃ¼schel</li>
<li>for: æ”¯æŒé‡åŒ–ç”Ÿæˆæ¨ç†åœ¨ LLMA æˆ– OPT ä¸Šçš„è‡ªåŠ¨ä»£ç ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>methods: åŸºäºç›®æ ‡æ¶æ„å’Œæ€§èƒ½æ¨¡å‹ï¼ŒåŒ…æ‹¬ç¡¬ä»¶ç‰¹æ€§å’Œæ–¹æ³•ç‰¹æœ‰çš„å‡†ç¡®æ€§çº¦æŸã€‚</li>
<li>results: CPU ä¸Šçš„ LLMA æ¨¡å‹æ¨ç†æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¾¾åˆ°é«˜æ€§èƒ½å’Œé«˜å‡†ç¡®æ€§ï¼Œä¸ç°æœ‰å¼€æºè§£å†³æ–¹æ¡ˆç›¸æ¯”ã€‚Hereâ€™s the English version for reference:</li>
<li>for: An automatic code generation approach for supporting quantized generative inference on large language models (LLMs) such as LLaMA or OPT on off-the-shelf CPUs.</li>
<li>methods: Informed by the target architecture and a performance model, including both hardware characteristics and method-specific accuracy constraints.</li>
<li>results: Results on CPU-based inference for LLaMA models show that our approach can lead to high performance and high accuracy, comparing favorably to the best existing open-source solution.<details>
<summary>Abstract</summary>
We present ongoing work on a new automatic code generation approach for supporting quantized generative inference on LLMs such as LLaMA or OPT on off-the-shelf CPUs. Our approach is informed by the target architecture and a performance model, including both hardware characteristics and method-specific accuracy constraints. Results on CPU-based inference for LLaMA models show that our approach can lead to high performance and high accuracy, comparing favorably to the best existing open-source solution. A preliminary implementation is available at https://github.com/IST-DASLab/QIGen.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æ­£åœ¨è¿›è¡Œä¸€é¡¹æ–°çš„è‡ªåŠ¨ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºæ”¯æŒé‡åŒ–ç”Ÿæˆæ¨ç†åœ¨LLaMAæˆ–OPTç±»å‹çš„è¯­è¨€æ¨¡å‹ä¸Šçš„Off-the-shelf CPUä¸Šã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°ç›®æ ‡æ¶æ„å’Œæ€§èƒ½æ¨¡å‹çš„å½±å“ï¼ŒåŒ…æ‹¬ç¡¬ä»¶ç‰¹æ€§å’Œæ–¹æ³•å…·ä½“ç²¾åº¦é™åˆ¶ã€‚å¯¹CPUä¸Šçš„æ¨ç†è¿‡ç¨‹ä¸­çš„LLaMAæ¨¡å‹è¿›è¡Œäº†ç»“æœï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å®ç°é«˜æ€§èƒ½å’Œé«˜ç²¾åº¦ï¼Œä¸ç°æœ‰å¼€æºè§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚ä¸€ä¸ªåˆæ­¥çš„å®ç°å¯ä»¥åœ¨https://github.com/IST-DASLab/QIGenä¸Šè·å¾—ã€‚
</details></li>
</ul>
<hr>
<h2 id="Polybot-Training-One-Policy-Across-Robots-While-Embracing-Variability"><a href="#Polybot-Training-One-Policy-Across-Robots-While-Embracing-Variability" class="headerlink" title="Polybot: Training One Policy Across Robots While Embracing Variability"></a>Polybot: Training One Policy Across Robots While Embracing Variability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03719">http://arxiv.org/abs/2307.03719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Yang, Dorsa Sadigh, Chelsea Finn<br>for:å¤šç§æœºå™¨äººå¹³å°ä¸Šçš„è§†è§‰æ§åˆ¶æŠ€èƒ½çš„è·¨å¹³å°ä¼ è¾“methods:ä½¿ç”¨ç»ˆç«¯æ‘„åƒå¤´å’Œå›¢é˜Ÿä»£ç åº“å®ç°è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´çš„åŒ¹é…ä½¿ç”¨å¯¹æ¯”å­¦ä¹ å¯¹ç­–çš„å†…éƒ¨è¡¨ç¤ºè¿›è¡ŒåŒ¹é…results:åœ¨6ä¸ªä»»åŠ¡å’Œ3ä¸ªæœºå™¨äººä¸Šæ”¶é›†äº†60å°æ—¶çš„æ•°æ®ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—æé«˜æˆåŠŸç‡å’Œæ ·æœ¬æ•ˆç‡çš„ç»“æœï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„è®¾è®¡å†³ç­–ã€‚Please note that the above information is in Simplified Chinese text, as requested.<details>
<summary>Abstract</summary>
Reusing large datasets is crucial to scale vision-based robotic manipulators to everyday scenarios due to the high cost of collecting robotic datasets. However, robotic platforms possess varying control schemes, camera viewpoints, kinematic configurations, and end-effector morphologies, posing significant challenges when transferring manipulation skills from one platform to another. To tackle this problem, we propose a set of key design decisions to train a single policy for deployment on multiple robotic platforms. Our framework first aligns the observation and action spaces of our policy across embodiments via utilizing wrist cameras and a unified, but modular codebase. To bridge the remaining domain shift, we align our policy's internal representations across embodiments through contrastive learning. We evaluate our method on a dataset collected over 60 hours spanning 6 tasks and 3 robots with varying joint configurations and sizes: the WidowX 250S, the Franka Emika Panda, and the Sawyer. Our results demonstrate significant improvements in success rate and sample efficiency for our policy when using new task data collected on a different robot, validating our proposed design decisions. More details and videos can be found on our anonymized project website: https://sites.google.com/view/polybot-multirobot
</details>
<details>
<summary>æ‘˜è¦</summary>
é‡ç”¨å¤§é‡æ•°æ®æ˜¯æ‰©å±•è§†è§‰åŸºäºæœºå™¨äºº manipulate åˆ°æ—¥å¸¸åœºæ™¯ä¸­çš„å…³é”®å› ç´ ï¼Œå› ä¸ºæ”¶é›†æœºå™¨äººæ•°æ®çš„æˆæœ¬å¾ˆé«˜ã€‚ç„¶è€Œï¼Œæœºå™¨äººå¹³å°å…·æœ‰ä¸åŒçš„æ§åˆ¶æ–¹æ¡ˆã€æ‘„åƒå¤´è§†ç‚¹ã€éª¨éª¼é…ç½®å’Œå™¨å®˜å½¢æ€ï¼Œè¿™ä¼šå¯¼è‡´å°†æŠ“å–æŠ€èƒ½ä»ä¸€ä¸ªå¹³å°è½¬ç§»åˆ°å¦ä¸€ä¸ªå¹³å°çš„å…·æœ‰å¾ˆå¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€äº›å…³é”®çš„è®¾è®¡å†³ç­–ï¼Œç”¨äºåœ¨å¤šä¸ªæœºå™¨äººå¹³å°ä¸Šè®­ç»ƒå•ä¸ªç­–ç•¥ã€‚æˆ‘ä»¬çš„æ¡†æ¶é¦–å…ˆå°†æˆ‘ä»¬ç­–ç•¥çš„è§‚å¯Ÿç©ºé—´å’ŒåŠ¨ä½œç©ºé—´åœ¨ä¸åŒå®ç°ä¸­è¿›è¡Œå¯¹é½ï¼Œé€šè¿‡ä½¿ç”¨è‡‚éƒ¨æ‘„åƒå¤´å’Œä¸€ä¸ªç»Ÿä¸€ã€å¯åˆ†æ¨¡å—çš„ä»£ç åº“æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†å¡«è¡¥å‰©ä¸‹çš„é¢†åŸŸå·®å¼‚ï¼Œæˆ‘ä»¬å¯¹ç­–ç•¥çš„å†…éƒ¨è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€ä¸ªåŒ…å«60å°æ—¶ã€6ä¸ªä»»åŠ¡å’Œ3ä¸ªæœºå™¨äººçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™äº›æœºå™¨äººåŒ…æ‹¬WidowX 250Sã€Franka Emika Panda å’ŒSawyerã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„ç­–ç•¥åœ¨æ–°çš„ä»»åŠ¡æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒåï¼Œåœ¨ä¸åŒæœºå™¨äººä¸Šçš„æˆåŠŸç‡å’Œæ ·æœ¬æ•ˆç‡æœ‰æ˜¾è‘—æé«˜ï¼Œè¿™ validate æˆ‘ä»¬çš„è®¾è®¡å†³ç­–ã€‚æ›´å¤šç»†èŠ‚å’Œè§†é¢‘å¯ä»¥åœ¨æˆ‘ä»¬åŒ¿åé¡¹ç›®ç½‘ç«™ä¸Šæ‰¾åˆ°ï¼šhttps://sites.google.com/view/polybot-multirobotã€‚
</details></li>
</ul>
<hr>
<h2 id="SAR-Generalization-of-Physiological-Agility-and-Dexterity-via-Synergistic-Action-Representation"><a href="#SAR-Generalization-of-Physiological-Agility-and-Dexterity-via-Synergistic-Action-Representation" class="headerlink" title="SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation"></a>SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03716">http://arxiv.org/abs/2307.03716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cameron Berg, Vittorio Caggiano, Vikash Kumar</li>
<li>for: å­¦ä¹ é«˜ç»´ç³»ç»Ÿä¸­çš„è¿ç»­æ§åˆ¶ç­–ç•¥ï¼ŒåŒ…æ‹¬è‚Œè‚‰æœºæ¢°ç³»ç»Ÿï¼Œä»ç„¶æ˜¯ä¸€é¡¹å¤æ‚çš„æŒ‘æˆ˜ã€‚ç”Ÿç‰©è¿›åŒ–è¿‡ç¨‹ä¸­ï¼Œç”Ÿç‰©ä½“å‘å±•äº†ä¸€äº›å¼ºå¤§çš„æœºåˆ¶ï¼Œä»¥å­¦ä¹ é«˜åº¦å¤æ‚çš„è¿åŠ¨æ§åˆ¶ç­–ç•¥ã€‚è¿™ç§robustè¡Œä¸ºflexibilityå“ªé‡Œæ¥è‡ªï¼Ÿ</li>
<li>methods: æ¨¡å—åŒ–æ§åˆ¶viaè‚Œè‚‰å…±åŒå¼ºåˆ¶ï¼ˆi.e., è‚Œè‚‰åˆä½œï¼‰æ˜¯ä¸€ç§å¯èƒ½çš„æœºåˆ¶ï¼Œå®ƒä½¿å¾—ç”Ÿç‰©ä½“å¯ä»¥é€šè¿‡ç®€åŒ–å’Œæ€»ç»“çš„åŠ¨ä½œç©ºé—´æ¥å­¦ä¹ è‚Œè‚‰æ§åˆ¶ã€‚è¿™ç¯‡æ–‡ç« å¼•ç”¨è¿™ç§æ¼”åŒ–å‡ºæ¥çš„è¿åŠ¨æ§åˆ¶ç­–ç•¥ï¼Œä½¿ç”¨physiologically accurateçš„äººå·¥æ‰‹å’Œè„šæ¨¡å‹ä½œä¸ºæµ‹è¯•ç¯å¢ƒï¼Œä»¥ç¡®å®šè¿™ç§Synergistic Action Representationï¼ˆSARï¼‰åœ¨æ›´å¤æ‚ä»»åŠ¡ä¸­æ˜¯å¦èƒ½å¤Ÿä¿ƒè¿›å­¦ä¹ ã€‚</li>
<li>results: ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SARåœ¨æ›´å¤æ‚ä»»åŠ¡ä¸­èƒ½å¤Ÿæ˜¾è‘—æé«˜å­¦ä¹ æ•ˆç‡å’ŒæˆåŠŸç‡ã€‚åœ¨äººå·¥æ‰‹å’Œè„šæ¨¡å‹ä¸­ï¼ŒSAR-ä½¿ç”¨ç­–ç•¥å¯ä»¥åœ¨å„ç§ Terrainsä¸Šå®ç° robust locomotionï¼Œè€ŒåŸºçº¿æ–¹æ³•æ— æ³•å­¦ä¹ æœ‰æ„ä¹‰çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œåœ¨å¤šä¸ªç›®æ ‡æ…æ‹Œä»»åŠ¡ä¸­ï¼ŒSAR-ä½¿ç”¨ç­–ç•¥çš„æˆåŠŸç‡é«˜è¾¾70%ï¼Œè€ŒåŸºçº¿æ–¹æ³•çš„æˆåŠŸç‡ä»…æœ‰20%ã€‚è¿™ä¸¤ä¸ªSAR-ä½¿ç”¨ç­–ç•¥è¿˜èƒ½å¤Ÿåœ¨ä¸åŒçš„ç¯å¢ƒæ¡ä»¶ä¸‹è¿›è¡Œé›¶åŸºç¡€å­¦ä¹ ï¼Œè€Œä¸ä½¿ç”¨SARçš„ç­–ç•¥åˆ™æ— æ³•è¿›è¡Œæ³›åŒ–ã€‚æœ€åï¼Œæ–‡ç« è¯æ˜äº†SARåœ¨æ›´å¹¿æ³›çš„é«˜ç»´æ§åˆ¶é—®é¢˜ä¸Šçš„ä¸€è‡´æ€§ï¼Œä½¿ç”¨äº†æœºå™¨äººæ…æ‹Œä»»åŠ¡é›†å’Œå…¨èº«äººå½¢æœºå™¨äººæ­¥è¡Œä»»åŠ¡ã€‚<details>
<summary>Abstract</summary>
Learning effective continuous control policies in high-dimensional systems, including musculoskeletal agents, remains a significant challenge. Over the course of biological evolution, organisms have developed robust mechanisms for overcoming this complexity to learn highly sophisticated strategies for motor control. What accounts for this robust behavioral flexibility? Modular control via muscle synergies, i.e. coordinated muscle co-contractions, is considered to be one putative mechanism that enables organisms to learn muscle control in a simplified and generalizable action space. Drawing inspiration from this evolved motor control strategy, we use physiologically accurate human hand and leg models as a testbed for determining the extent to which a Synergistic Action Representation (SAR) acquired from simpler tasks facilitates learning more complex tasks. We find in both cases that SAR-exploiting policies significantly outperform end-to-end reinforcement learning. Policies trained with SAR were able to achieve robust locomotion on a wide set of terrains with high sample efficiency, while baseline approaches failed to learn meaningful behaviors. Additionally, policies trained with SAR on a multiobject manipulation task significantly outperformed (>70% success) baseline approaches (<20% success). Both of these SAR-exploiting policies were also found to generalize zero-shot to out-of-domain environmental conditions, while policies that did not adopt SAR failed to generalize. Finally, we establish the generality of SAR on broader high-dimensional control problems using a robotic manipulation task set and a full-body humanoid locomotion task. To the best of our knowledge, this investigation is the first of its kind to present an end-to-end pipeline for discovering synergies and using this representation to learn high-dimensional continuous control across a wide diversity of tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
å­¦ä¹ é«˜ç»´ç³»ç»Ÿä¸­çš„è¿ç»­æ§åˆ¶ç­–ç•¥æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ç”Ÿç‰©æ¼”åŒ–è¿‡ç¨‹ä¸­ï¼Œç”Ÿç‰©ä½“å‘å±•å‡ºäº†ä¸€äº›å¼ºå¤§çš„æœºåˆ¶æ¥è§£å†³è¿™ç§å¤æ‚æ€§ï¼Œä»¥å­¦ä¹ é«˜åº¦å¤æ‚çš„åŠ¨ä½œæ§åˆ¶ç­–ç•¥ã€‚è¿™ç§è¡Œä¸ºçš„çµæ´»æ€§æ¥æºäºå“ªé‡Œï¼Ÿæ¨¡å—åŒ–æ§åˆ¶é€šè¿‡è‚Œè‚‰åŒæ­¥ï¼Œå³è‚Œè‚‰å…±åŒæ”¶ç¼©ï¼Œæ˜¯ä¸€ç§è¢«è®¤ä¸ºæ˜¯ç”Ÿç‰©ä½“å­¦ä¹ è‚Œè‚‰æ§åˆ¶çš„æ½œåœ¨æœºåˆ¶ã€‚æˆ‘ä»¬ä»¥äººå·¥æ‰‹å’Œè„šæ¨¡å‹ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œé€šè¿‡ä½¿ç”¨ç”Ÿç†å­¦å‡†ç¡®çš„äººå·¥æ‰‹å’Œè„šæ¨¡å‹ï¼Œç¡®å®šä½¿ç”¨SARï¼ˆSynergistic Action Representationï¼‰ä»æ›´ç®€å•çš„ä»»åŠ¡ä¸­è·å¾—çš„ç­–ç•¥æ˜¯å¦å¯ä»¥å¸®åŠ©å­¦ä¹ æ›´å¤æ‚çš„ä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨ä¸¤ä¸ªæƒ…å†µä¸‹ï¼Œä½¿ç”¨SARç­–ç•¥çš„å­¦ä¹ æ•ˆæœéƒ½é«˜äºç«¯åˆ°ç«¯å­¦ä¹ ã€‚SARç­–ç•¥å¯ä»¥åœ¨å„ç§ä¸åŒçš„åœ°å½¢ä¸Šå®ç°ç¨³å®šçš„ç§»åŠ¨ï¼Œè€ŒåŸºçº¿æ–¹æ³•æ— æ³•å­¦ä¹ æœ‰æ„ä¹‰çš„è¡Œä¸ºã€‚æ­¤å¤–ï¼Œåœ¨å¤šä¸ªç›®æ ‡æ‹¼æ¥ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨SARç­–ç•¥çš„æˆåŠŸç‡é«˜äº70%ï¼Œè€ŒåŸºçº¿æ–¹æ³•çš„æˆåŠŸç‡ä½äº20%ã€‚åŒæ—¶ï¼Œè¿™ä¸¤ç§SARç­–ç•¥è¿˜èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç¯å¢ƒæ¡ä»¶ï¼Œè€Œä¸ä½¿ç”¨SARçš„ç­–ç•¥æ— æ³•é€‚åº”ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æœºå™¨äºº manipulate ä»»åŠ¡é›†å’Œå…¨èº«äººå½¢æ­¥è¡Œä»»åŠ¡ï¼Œè¯æ˜SARåœ¨æ›´å¹¿æ³›çš„é«˜ç»´æ§åˆ¶é—®é¢˜ä¸Šå…·æœ‰æ™®éæ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å¯¹é«˜ç»´è¿ç»­æ§åˆ¶é—®é¢˜çš„æ•´ä½“è§£å†³æ–¹æ¡ˆã€‚
</details></li>
</ul>
<hr>
<h2 id="INT-FP-QSim-Mixed-Precision-and-Formats-For-Large-Language-Models-and-Vision-Transformers"><a href="#INT-FP-QSim-Mixed-Precision-and-Formats-For-Large-Language-Models-and-Vision-Transformers" class="headerlink" title="INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers"></a>INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03712">http://arxiv.org/abs/2307.03712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lightmatter-ai/int-fp-qsim">https://github.com/lightmatter-ai/int-fp-qsim</a></li>
<li>paper_authors: Lakshmi Nair, Mikhail Bernadskiy, Arulselvan Madhavan, Craig Chan, Ayon Basumallik, Darius Bunandar</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ä¸ªå¼€æºçš„æ¨¡æ‹Ÿå™¨ï¼Œä»¥ä¾¿è¯„ä¼°å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ„ŸçŸ¥å¯¹åº”æ¨¡å‹ï¼ˆVTï¼‰åœ¨ä¸åŒçš„æ•°å€¼ç²¾åº¦å’Œæ ¼å¼ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ç°æœ‰çš„å¼€æºåº“ï¼Œä¾‹å¦‚TensorRTã€QPyTorchå’ŒAIMETï¼Œå°†å…¶ç»„åˆæˆä¸€ä¸ªå¯ä»¥æ”¯æŒå¤šç§æµ®ç‚¹å’Œæ•´æ•°æ ¼å¼çš„æ¨¡æ‹Ÿå™¨ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡é€šè¿‡ä½¿ç”¨ INT-FP-QSim æ¨¡æ‹Ÿå™¨ï¼Œè¯„ä¼°äº†ä¸åŒæ•°å€¼ç²¾åº¦å’Œæ ¼å¼ä¸‹ LLM å’Œ VT çš„æ€§èƒ½ï¼Œå¹¶æ¯”è¾ƒäº†æœ€è¿‘æå‡ºçš„ Adaptive Block Floating Pointã€SmoothQuantã€GPTQ å’Œ RPTQ ç­‰æ–¹æ³•çš„å½±å“ã€‚<details>
<summary>Abstract</summary>
The recent rise of large language models (LLMs) has resulted in increased efforts towards running LLMs at reduced precision. Running LLMs at lower precision supports resource constraints and furthers their democratization, enabling users to run billion-parameter LLMs on their personal devices. To supplement this ongoing effort, we propose INT-FP-QSim: an open-source simulator that enables flexible evaluation of LLMs and vision transformers at various numerical precisions and formats. INT-FP-QSim leverages existing open-source repositories such as TensorRT, QPytorch and AIMET for a combined simulator that supports various floating point and integer formats. With the help of our simulator, we survey the impact of different numerical formats on the performance of LLMs and vision transformers at 4-bit weights and 4-bit or 8-bit activations. We also compare recently proposed methods like Adaptive Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We hope INT-FP-QSim will enable researchers to flexibly simulate models at various precisions to support further research in quantization of LLMs and vision transformers.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å´›èµ·å¯¼è‡´äº†å‡å°‘ç²¾åº¦ä¸‹è¿è¡Œçš„åŠªåŠ›çš„å¢åŠ ã€‚åœ¨å‡å°‘ç²¾åº¦ä¸‹è¿è¡Œ LLMs æ”¯æŒèµ„æºé™åˆ¶å’Œæ™®åŠï¼Œä½¿ç”¨è€…å¯ä»¥åœ¨ä¸ªäººè®¾å¤‡ä¸Šè¿è¡Œ billion-parameter LLMsã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æè®® INT-FP-QSimï¼šä¸€ä¸ªå¼€æºçš„ simulatorï¼Œå¯ä»¥åœ¨ä¸åŒçš„æ•°å­—ç²¾åº¦å’Œæ ¼å¼ä¸‹çµæ´»è¯„ä¼° LLMs å’Œè§†transformersã€‚INT-FP-QSim åˆ©ç”¨ç°æœ‰çš„å¼€æºåº“ such as TensorRT, QPytorch å’Œ AIMETï¼Œæ„å»ºäº†ä¸€ä¸ªé›†æˆçš„ simulatorï¼Œæ”¯æŒå¤šç§æµ®ç‚¹æ•°å’Œæ•´æ•°æ ¼å¼ã€‚æˆ‘ä»¬ä½¿ç”¨ INT-FP-QSimï¼Œå¯¹ LLMs å’Œè§†transformers çš„ä¸åŒæ•°å­—æ ¼å¼çš„å½±å“è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å¯¹æœ€è¿‘æå‡ºçš„æ–¹æ³• like Adaptive Block Floating Point, SmoothQuant, GPTQ å’Œ RPTQ è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬å¸Œæœ› INT-FP-QSim èƒ½å¤Ÿå¸®åŠ©ç ”ç©¶äººå‘˜åœ¨ä¸åŒçš„ç²¾åº¦ä¸‹çµæ´»æ¨¡æ‹Ÿæ¨¡å‹ï¼Œä»¥æ”¯æŒæ›´å¤šçš„ LLMS å’Œè§†transformers çš„é‡åŒ–ç ”ç©¶ã€‚
</details></li>
</ul>
<hr>
<h2 id="Fermat-Distances-Metric-Approximation-Spectral-Convergence-and-Clustering-Algorithms"><a href="#Fermat-Distances-Metric-Approximation-Spectral-Convergence-and-Clustering-Algorithms" class="headerlink" title="Fermat Distances: Metric Approximation, Spectral Convergence, and Clustering Algorithms"></a>Fermat Distances: Metric Approximation, Spectral Convergence, and Clustering Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05750">http://arxiv.org/abs/2307.05750</a></li>
<li>repo_url: None</li>
<li>paper_authors: NicolÃ¡s GarcÃ­a Trillos, Anna Little, Daniel McKenzie, James M. Murphy</li>
<li>for: è¿™ä¸ªè®ºæ–‡ç ”ç©¶äº†è´¹é©¬è·ç¦»çš„æ”¶æ•›æ€§è´¨ï¼Œå®ƒæ˜¯åœ¨é‡Œæ›¼çº²æŠ½è±¡ä¸Šå®šä¹‰çš„ä¸€ç§æµ®åŠ¨åº¦é‡ï¼Œå¯ä»¥åœ¨ç»´åº¦æ„ŸçŸ¥çš„æ•°æ®ä¸Šè¿›è¡Œåˆ† clusteringã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†æ–°çš„å‡ ä½•å’Œç»Ÿè®¡å­¦æ–¹æ³•ï¼ŒåŒ…æ‹¬åœ¨éå‡åŒ€å¯†åº¦å’ŒæŠ½è±¡çº²ä¸Šçš„å‡ ä½•æ„é€ å’Œç»Ÿè®¡å­¦åˆ†æï¼Œä»¥è¯æ˜è´¹é©¬è·ç¦»çš„æ”¶æ•›æ€§ã€‚</li>
<li>results: è¿™ä¸ªè®ºæ–‡è¯æ˜äº†è´¹é©¬è·ç¦»åœ¨å°é‚»åŸŸå†…æ”¶æ•›åˆ°å…¶è¿ç»­ç±»æ¯”ä¸­ï¼Œæ”¶æ•›é€Ÿç‡å–å†³äºæ•°æ®çš„å†…åœ¨ç»´åº¦å’Œæƒé‡å‚æ•°ã€‚æ­¤å¤–ï¼Œè¿™ä¸ªè®ºæ–‡è¿˜è¯æ˜äº†åŸºäºè´¹é©¬è·ç¦»çš„å›¾ Laplacian åœ¨ç»´åº¦æ„ŸçŸ¥çš„æ•°æ®ä¸Šçš„æ”¶æ•›æ€§ã€‚<details>
<summary>Abstract</summary>
We analyze the convergence properties of Fermat distances, a family of density-driven metrics defined on Riemannian manifolds with an associated probability measure. Fermat distances may be defined either on discrete samples from the underlying measure, in which case they are random, or in the continuum setting, in which they are induced by geodesics under a density-distorted Riemannian metric. We prove that discrete, sample-based Fermat distances converge to their continuum analogues in small neighborhoods with a precise rate that depends on the intrinsic dimensionality of the data and the parameter governing the extent of density weighting in Fermat distances. This is done by leveraging novel geometric and statistical arguments in percolation theory that allow for non-uniform densities and curved domains. Our results are then used to prove that discrete graph Laplacians based on discrete, sample-driven Fermat distances converge to corresponding continuum operators. In particular, we show the discrete eigenvalues and eigenvectors converge to their continuum analogues at a dimension-dependent rate, which allows us to interpret the efficacy of discrete spectral clustering using Fermat distances in terms of the resulting continuum limit. The perspective afforded by our discrete-to-continuum Fermat distance analysis leads to new clustering algorithms for data and related insights into efficient computations associated to density-driven spectral clustering. Our theoretical analysis is supported with numerical simulations and experiments on synthetic and real image data.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬åˆ†æ Ferma è·ç¦»çš„å½’ä¸€åŒ–æ€§è´¨ï¼Œè¿™æ˜¯åœ¨é‡Œæ›¼çº¹ç†ä¸Šå®šä¹‰çš„ä¸€å®¶density-driven mÃ©triqueä¸­çš„ä¸€ä¸ªå®¶æ—ã€‚Ferma è·ç¦»å¯ä»¥åœ¨ç²’å­æ ·æœ¬ä¸Šå®šä¹‰ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹å®ƒä»¬æ˜¯éšæœºçš„ï¼Œä¹Ÿå¯ä»¥åœ¨ kontinuum è®¾ç½®ä¸‹å®šä¹‰ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹å®ƒä»¬ç”± geodesics ä¸‹çš„ density-distorted Riemannian mÃ©trique å¼•èµ·ã€‚æˆ‘ä»¬è¯æ˜äº†ç²’å­æ ·æœ¬ä¸Šçš„ discrete Fermat è·ç¦»åœ¨å°é‚»åŸŸå†…ä¸å…¶ kontinuum åŒæ„ä¸­çš„analogues converge åˆ°ä¸€ä¸ªå…·ä½“çš„rateï¼Œè¿™ä¸ªrateå–å†³äºæ•°æ®çš„å†…åœ¨ç»´åº¦å’Œ density weighting å‚æ•°çš„å€¼ã€‚æˆ‘ä»¬ä½¿ç”¨äº†æ–°çš„å‡ ä½•å’Œç»Ÿè®¡å­¦ç†ç”±ï¼ŒåŒ…æ‹¬éå‡åŒ€çš„densityå’Œæ‹å¼¯çš„ Domianï¼Œæ¥è¯æ˜è¿™ä¸€ç»“è®ºã€‚æˆ‘ä»¬çš„ç»“æœè¢«ç”¨æ¥è¯æ˜åŸºäº discrete Fermat è·ç¦»çš„å›¾ Laplacian çš„æ•°æ® clustering ç®—æ³•æ˜¯å¯é çš„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜äº† discrete eigenvalues å’Œ eigenvectors åœ¨ä¸€å®šçš„ç»´åº¦ä¸Šä¸å…¶ kontinuum åŒæ„ä¸­çš„analogues converge åˆ°ä¸€ä¸ªå…·ä½“çš„rateï¼Œè¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥æ ¹æ®continuum limit æ¥ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ° discrete spectral clustering çš„æ•ˆæœã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æå¾—åˆ°äº† numÃ©rical simulations å’Œå®éªŒæ•°æ®çš„æ”¯æŒï¼Œå¹¶æä¾›äº†æ–°çš„ clustering ç®—æ³•å’Œç›¸å…³çš„åŠæ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Equivariant-Single-View-Pose-Prediction-Via-Induced-and-Restricted-Representations"><a href="#Equivariant-Single-View-Pose-Prediction-Via-Induced-and-Restricted-Representations" class="headerlink" title="Equivariant Single View Pose Prediction Via Induced and Restricted Representations"></a>Equivariant Single View Pose Prediction Via Induced and Restricted Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03704">http://arxiv.org/abs/2307.03704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Owen Howell, David Klee, Ondrej Biza, Linfeng Zhao, Robin Walters</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†è§£å†³è®¡ç®—æœºè§†è§‰ä¸­ä»äºŒç»´å›¾åƒä¸­å­¦ä¹ ä¸‰ç»´ä¸–ç•Œçš„åŸºæœ¬é—®é¢˜è€Œå†™çš„ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†SO(3)-equivarianceçš„é™åˆ¶æ¥é€‚åº”äºŒç»´å›¾åƒä¸Šçš„å¯¹è±¡æ—‹è½¬å’Œç¿»è¯‘ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä½¿ç”¨äº†SO(2)-equivarianceçš„çº¦æŸæ¥æ»¡è¶³ä¸‰ç»´ä¸–ç•Œä¸­å¯¹è±¡çš„å‡ ä½•ä¸€è‡´æ€§çº¦æŸã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œå¯ä»¥åœ¨ä¸‰ç»´ä¸–ç•Œä¸­ä»äºŒç»´å›¾åƒä¸­å­¦ä¹ å¯¹è±¡çš„å§¿æ€ã€‚è¯¥ç®—æ³•åœ¨ä¸‰ä¸ªä¸åŒçš„ pose é¢„æµ‹ä»»åŠ¡ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå¹¶åœ¨PASCAL3D+å’ŒSYMSOL pose estimationä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€é«˜çš„æµ‹è¯•ç²¾åº¦ã€‚<details>
<summary>Abstract</summary>
Learning about the three-dimensional world from two-dimensional images is a fundamental problem in computer vision. An ideal neural network architecture for such tasks would leverage the fact that objects can be rotated and translated in three dimensions to make predictions about novel images. However, imposing SO(3)-equivariance on two-dimensional inputs is difficult because the group of three-dimensional rotations does not have a natural action on the two-dimensional plane. Specifically, it is possible that an element of SO(3) will rotate an image out of plane. We show that an algorithm that learns a three-dimensional representation of the world from two dimensional images must satisfy certain geometric consistency properties which we formulate as SO(2)-equivariance constraints. We use the induced and restricted representations of SO(2) on SO(3) to construct and classify architectures which satisfy these geometric consistency constraints. We prove that any architecture which respects said consistency constraints can be realized as an instance of our construction. We show that three previously proposed neural architectures for 3D pose prediction are special cases of our construction. We propose a new algorithm that is a learnable generalization of previously considered methods. We test our architecture on three pose predictions task and achieve SOTA results on both the PASCAL3D+ and SYMSOL pose estimation tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
å­¦ä¹ ä¸‰ç»´ä¸–ç•Œä»äºŒç»´å›¾åƒä¸­æ˜¯è®¡ç®—æœºè§†è§‰çš„åŸºæœ¬é—®é¢˜ã€‚ç†æƒ³çš„ç¥ç»ç½‘ç»œæ¶æ„ Ğ´Ğ»Ñæ­¤ç±»ä»»åŠ¡åº”è¯¥åˆ©ç”¨å¯¹è±¡å¯ä»¥åœ¨ä¸‰ç»´ç©ºé—´ä¸­æ—‹è½¬å’Œå¹³ç§»ï¼Œä»¥ä¾¿å¯¹ novel å›¾åƒè¿›è¡Œé¢„æµ‹ã€‚ç„¶è€Œï¼Œå¯¹äºäºŒç»´è¾“å…¥ï¼Œå¼ºåˆ¶ SO(3) åŒæ€æ€§æ˜¯å›°éš¾çš„ï¼Œå› ä¸ºä¸‰ç»´ rotate æ“ä½œæ²¡æœ‰è‡ªç„¶çš„äºŒç»´å¹³é¢ä¸Šçš„è¡Œä¸ºã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä¸€ä¸ªå­¦ä¹ ä¸‰ç»´ä¸–ç•Œçš„ç®—æ³•ä»äºŒç»´å›¾åƒä¸­è·å¾—ä¸‰ç»´è¡¨ç¤ºçš„ä¸–ç•Œï¼Œå¿…é¡»æ»¡è¶³æŸäº›å‡ ä½•ä¸€è‡´æ€§è¦æ±‚ï¼Œæˆ‘ä»¬å°†è¿™äº›è¦æ±‚è¡¨è¿°ä¸º SO(2) åŒæ€æ€§çº¦æŸã€‚æˆ‘ä»¬ä½¿ç”¨ SO(2) åœ¨ SO(3) ä¸Šçš„å¯å‘å’Œå—é™è¡¨ç¤ºæ¥è®¾è®¡å’Œåˆ†ç±»æ¶æ„ï¼Œå¹¶è¯æ˜ä»»ä½•æ»¡è¶³è¿™äº›å‡ ä½•ä¸€è‡´æ€§è¦æ±‚çš„æ¶æ„éƒ½å¯ä»¥é€šè¿‡æˆ‘ä»¬çš„æ„é€ å®ç°ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå‰é¢å·²ç»æå‡ºçš„ä¸‰ä¸ªç¥ç»ç½‘ç»œæ¶æ„ Ğ´Ğ»Ñ 3D å§¿æ€é¢„æµ‹éƒ½æ˜¯æˆ‘ä»¬çš„æ„é€ çš„ç‰¹ä¾‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œå®ƒæ˜¯learnableçš„ï¼Œå¹¶ä¸”æ˜¯å·²çŸ¥çš„æ–¹æ³•çš„æ™®é€‚åŒ–ã€‚æˆ‘ä»¬å¯¹ä¸‰ä¸ªå§¿æ€é¢„æµ‹ä»»åŠ¡è¿›è¡Œæµ‹è¯•ï¼Œå¹¶åœ¨ PASCAL3D+ å’Œ SYMSOL å§¿æ€é¢„æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€é«˜çš„æˆç»©ã€‚
</details></li>
</ul>
<hr>
<h2 id="Scalable-Membership-Inference-Attacks-via-Quantile-Regression"><a href="#Scalable-Membership-Inference-Attacks-via-Quantile-Regression" class="headerlink" title="Scalable Membership Inference Attacks via Quantile Regression"></a>Scalable Membership Inference Attacks via Quantile Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03694">http://arxiv.org/abs/2307.03694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Bertran, Shuai Tang, Michael Kearns, Jamie Morgenstern, Aaron Roth, Zhiwei Steven Wu</li>
<li>for: The paper is written for discussing a new class of membership inference attacks that are competitive with state-of-the-art shadow model attacks but require substantially less compute.</li>
<li>methods: The paper uses quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training as the attack method.</li>
<li>results: The paper shows the efficacy of this approach in an extensive series of experiments on various datasets and model architectures, demonstrating that the proposed attack is competitive with state-of-the-art shadow model attacks while requiring less compute and being truly â€œblack-boxâ€.<details>
<summary>Abstract</summary>
Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large.   We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures.
</details>
<details>
<summary>æ‘˜è¦</summary>
åŸŸåæ¨æµ‹æ”»å‡»æ˜¯ç”¨äºåˆ¤æ–­ä¸€ä¸ªç‰¹å®šç¤ºä¾‹æ˜¯å¦åœ¨è®­ç»ƒä¸­ä½¿ç”¨äº†é»‘ç›’è®¿é—®å·²ç»è®­ç»ƒè¿‡çš„æ¨¡å‹ã€‚åŸŸåæ¨æµ‹å¯ä»¥å½¢å¼åŒ–ä¸ºä¸€ä¸ªå‡è®¾æµ‹è¯•é—®é¢˜ã€‚ç°æœ‰æœ€æœ‰æ•ˆçš„æ”»å‡»æ–¹æ³•æ˜¯é€šè¿‡è®­ç»ƒå¤šä¸ªâ€œé™Œç”Ÿæ¨¡å‹â€ï¼ˆå³ä¸è¢«æ”»å‡»æ¨¡å‹ç›¸åŒçš„æ¶æ„çš„æ¨¡å‹ï¼Œè¢«è®­ç»ƒäºRandom subsets of dataï¼‰æ¥ä¼°è®¡æ¨¡å‹åœ¨çœŸå®æ ‡ç­¾ä¸Šçš„ä¿¡ä»»åº¦åˆ†å¸ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ”»å‡»éå¸¸ computationally expensiveï¼Œç‰¹åˆ«æ˜¯å½“æ¨¡å‹è¢«æ”»å‡»æ—¶å¾ˆå¤§ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ”»å‡»æ–¹æ³•ï¼ŒåŸºäºæ¨¡å‹ä¸‹å‘çš„ä¿¡ä»»åˆ†å¸ƒä¸­çš„åˆ†å€¼å›å½’ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰çš„é™Œç”Ÿæ¨¡å‹æ”»å‡»ç›¸æ¯”ï¼Œéœ€è¦æ›´å°‘çš„è®¡ç®—èµ„æºï¼Œå› ä¸ºæˆ‘ä»¬çš„æ”»å‡»åªéœ€è¦è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æè®®çš„æ”»å‡»æ–¹æ³•ä¸éœ€è¦çŸ¥é“æ¨¡å‹ä¸‹å‘çš„æ¶æ„ï¼Œå› æ­¤æ˜¯çœŸæ­£çš„â€œé»‘ç›’â€æ”»å‡»ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹æ¶æ„ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†è¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/08/cs.LG_2023_07_08/" data-id="clltaagnk003wr8880fx0gnm1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/08/cs.SD_2023_07_08/" class="article-date">
  <time datetime="2023-07-07T16:00:00.000Z" itemprop="datePublished">2023-07-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/08/cs.SD_2023_07_08/">cs.SD - 2023-07-08 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Anti-noise-window-Subjective-perception-of-active-noise-reduction-and-effect-of-informational-masking"><a href="#Anti-noise-window-Subjective-perception-of-active-noise-reduction-and-effect-of-informational-masking" class="headerlink" title="Anti-noise window: Subjective perception of active noise reduction and effect of informational masking"></a>Anti-noise window: Subjective perception of active noise reduction and effect of informational masking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05533">http://arxiv.org/abs/2307.05533</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ntudsp/SPANR">https://github.com/ntudsp/SPANR</a></li>
<li>paper_authors: Bhan Lam, Kelvin Chee Quan Lim, Kenneth Ooi, Zhen-Ting Ong, Dongyuan Shi, Woon-Seng Gan</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨æ¢è®¨åœ¨å®¤å†…éš”çƒ­ä¸­ä½¿ç”¨æ´»åŠ¨å™ªéŸ³æ§åˆ¶æŠ€æœ¯ä»¥æé«˜å®¤å†…å™ªéŸ³èˆ’é€‚æ€§ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ä¸€ç§åä¸ºâ€åå™ªçª—â€ï¼ˆANWï¼‰çš„æ´»åŠ¨å™ªéŸ³æ§åˆ¶æŠ€æœ¯ï¼Œå¹¶ä¸ä¿¡æ¯æ©è”½ï¼ˆIMï¼‰ç›¸ç»“åˆï¼Œä»¥evaluateå…¶åœ¨æ¨¡æ‹ŸåºŠé—´ä¸­çš„æ•ˆæœã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ANWå¯ä»¥significantly reductions in perceived annoyanceï¼ˆPAYï¼‰å’Œå¬èµ·æ¥å¼ºåº¦ï¼ˆPLNï¼‰ï¼Œå¹¶æé«˜ISO pleasantnessï¼Œä½†æ˜¯æ°´Maskingå¯èƒ½ä¼šå¢åŠ PLNã€‚æ­¤å¤–ï¼Œ combining ANC with maskerså¯ä»¥è·å¾—äº’åŠ¨æ•ˆæœï¼Œä½¿å¾—maskers Significantly reduce PAY compared to ANC aloneã€‚<details>
<summary>Abstract</summary>
Reviving natural ventilation (NV) for urban sustainability presents challenges for indoor acoustic comfort. Active control and interference-based noise mitigation strategies, such as the use of loudspeakers, offer potential solutions to achieve acoustic comfort while maintaining NV. However, these approaches are not commonly integrated or evaluated from a perceptual standpoint. This study examines the perceptual and objective aspects of an active-noise-control (ANC)-based "anti-noise" window (ANW) and its integration with informational masking (IM) in a model bedroom. Forty participants assessed the ANW in a three-way interaction involving noise types (traffic, train, and aircraft), maskers (bird, water), and ANC (on, off). The evaluation focused on perceived annoyance (PAY; ISO/TS 15666), perceived affective quality (ISO/TS 12913-2), loudness (PLN), and included an open-ended qualitative assessment. Despite minimal objective reduction in decibel-based indicators and a slight increase in psychoacoustic sharpness, the ANW alone demonstrated significant reductions in PAY and PLN, as well as an improvement in ISO pleasantness across all noise types. The addition of maskers generally enhanced overall acoustic comfort, although water masking led to increased PLN. Furthermore, the combination of ANC with maskers showed interaction effects, with both maskers significantly reducing PAY compared to ANC alone.
</details>
<details>
<summary>æ‘˜è¦</summary>
reviving natural ventilation (NV) for urban sustainability poses challenges for indoor acoustic comfort. active control and interference-based noise mitigation strategies, such as the use of loudspeakers, offer potential solutions to achieve acoustic comfort while maintaining NV. however, these approaches are not commonly integrated or evaluated from a perceptual standpoint. this study examines the perceptual and objective aspects of an active-noise-control (ANC)-based "anti-noise" window (ANW) and its integration with informational masking (IM) in a model bedroom. forty participants assessed the ANW in a three-way interaction involving noise types (traffic, train, and aircraft), maskers (bird, water), and ANC (on, off). the evaluation focused on perceived annoyance (PAY; ISO/TS 15666), perceived affective quality (ISO/TS 12913-2), loudness (PLN), and included an open-ended qualitative assessment. despite minimal objective reduction in decibel-based indicators and a slight increase in psychoacoustic sharpness, the ANW alone demonstrated significant reductions in PAY and PLN, as well as an improvement in ISO pleasantness across all noise types. the addition of maskers generally enhanced overall acoustic comfort, although water masking led to increased PLN. furthermore, the combination of ANC with maskers showed interaction effects, with both maskers significantly reducing PAY compared to ANC alone.
</details></li>
</ul>
<hr>
<h2 id="On-decoder-only-architecture-for-speech-to-text-and-large-language-model-integration"><a href="#On-decoder-only-architecture-for-speech-to-text-and-large-language-model-integration" class="headerlink" title="On decoder-only architecture for speech-to-text and large language model integration"></a>On decoder-only architecture for speech-to-text and large language model integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03917">http://arxiv.org/abs/2307.03917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, Yu Wu</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æ¢è®¨å¦‚ä½•å°†è¯­éŸ³ä¿¡å·çº³å…¥æ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥æé«˜äººæœºäº¤äº’çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨Connectionist Temporal Classificationå’Œç®€å•çš„éŸ³é¢‘ç¼–ç å™¨å°†å‹ç¼©è¯­éŸ³ç‰¹å¾æ˜ å°„åˆ°å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¿ç»­semanticç©ºé—´ã€‚</li>
<li>results: åœ¨å¤šè¯­è¨€speech-to-textç¿»è¯‘ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ä¸å¼ºåŸºelineæ¯”è¾ƒï¼Œæ˜¾ç¤ºäº†è¾ƒå¥½çš„è¡¨ç°ï¼Œè¿™è¡¨æ˜decoder-onlyæ¨¡å‹åœ¨speech-to-textè½¬æ¢ä¸­å¯èƒ½å…·æœ‰ä¼˜åŠ¿ã€‚<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The "decoder-only" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å–å¾—äº†å¾ˆå¤§çš„æˆåŠŸï¼Œä½¿å¾—äººæœºäº¤äº’ä½¿ç”¨è‡ªç„¶è¯­è¨€æ›´åŠ ç®€å•ã€‚ç„¶è€Œï¼Œå°†è¯­éŸ³ä¿¡å·çº³å…¥LLMä¸­çš„æ•´åˆè¿˜æ²¡æœ‰å¾—åˆ°äº†å……åˆ†çš„æ¢ç´¢ã€‚â€œè§£oder-onlyâ€æ¶æ„ä¹Ÿæ²¡æœ‰å—åˆ°è¿‡å¥½çš„ç ”ç©¶ã€‚åœ¨è¿™ä¸ªç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºSpeech-LLaMAï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†è¯­éŸ³ä¿¡å·çº³å…¥æ–‡æœ¬å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨Connectionist Temporal Classificationå’Œç®€å•çš„éŸ³é¢‘ç¼–ç å™¨å°†å‹ç¼©çš„è¯­éŸ³ç‰¹å¾æ˜ å°„åˆ°æ–‡æœ¬å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ä¸­çš„è¿ç»­Semanticç©ºé—´ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ¢ç´¢äº†ä½¿ç”¨åªæœ‰è§£oder-onlyæ¶æ„è¿›è¡Œè¯­éŸ³è¯†åˆ«ä»»åŠ¡çš„å¯èƒ½æ€§ï¼Œé€šè¿‡ä»å•ç‹¬çš„è¯­éŸ³-æ–‡æœ¬å¯¹åº”æ•°æ®ä¸­ randomly initializeä¸€ä¸ªè¾ƒå°è§„æ¨¡çš„Speech-LLaMAæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬åœ¨å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ç¿»è¯‘ä»»åŠ¡ä¸­è¿›è¡Œäº†å®éªŒï¼Œå¹¶è¾¾åˆ°äº†å¾ˆå¤§çš„æ”¹å–„ï¼Œhighlighting the potential advantages of decoder-only models for speech-to-text conversionã€‚
</details></li>
</ul>
<hr>
<h2 id="LaunchpadGPT-Language-Model-as-Music-Visualization-Designer-on-Launchpad"><a href="#LaunchpadGPT-Language-Model-as-Music-Visualization-Designer-on-Launchpad" class="headerlink" title="LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad"></a>LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04827">http://arxiv.org/abs/2307.04827</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yunlong10/launchpadgpt">https://github.com/yunlong10/launchpadgpt</a></li>
<li>paper_authors: Siting Xu, Yunlong Tang, Feng Zheng<br>for:The paper is written for those who want to create music visualization designs for the Launchpad musical instrument.methods:The paper proposes a method called LaunchpadGPT, which uses a language model to generate music visualization designs on the Launchpad automatically.results:The proposed method can create better music visualization than random generation methods and has the potential for a broader range of music visualization applications.Hereâ€™s the text in Simplified Chinese:for: æœ¬ç ”ç©¶æ—¨åœ¨å¸®åŠ©è®¾è®¡Launchpadçš„éŸ³ä¹è§†è§‰åŒ–ï¼Œå¹¶æä¾›ä¸€ä¸ªæ›´åŠ å¯ accessibleçš„æ–¹æ³•æ¥åˆ›å»ºéŸ³ä¹è§†è§‰åŒ–ã€‚methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºLaunchpadGPTçš„æ–¹æ³•ï¼Œå®ƒä½¿ç”¨è¯­è¨€æ¨¡å‹æ¥è‡ªåŠ¨ç”ŸæˆLaunchpadä¸Šçš„éŸ³ä¹è§†è§‰åŒ–è®¾è®¡ã€‚results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œææ¡ˆçš„æ–¹æ³•å¯ä»¥å¯¹Launchpadä¸Šçš„éŸ³ä¹è§†è§‰åŒ–è¿›è¡Œæ›´å¥½çš„è®¾è®¡ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚<details>
<summary>Abstract</summary>
Launchpad is a musical instrument that allows users to create and perform music by pressing illuminated buttons. To assist and inspire the design of the Launchpad light effect, and provide a more accessible approach for beginners to create music visualization with this instrument, we proposed the LaunchpadGPT model to generate music visualization designs on Launchpad automatically. Based on the language model with excellent generation ability, our proposed LaunchpadGPT takes an audio piece of music as input and outputs the lighting effects of Launchpad-playing in the form of a video (Launchpad-playing video). We collect Launchpad-playing videos and process them to obtain music and corresponding video frame of Launchpad-playing as prompt-completion pairs, to train the language model. The experiment result shows the proposed method can create better music visualization than random generation methods and hold the potential for a broader range of music visualization applications. Our code is available at https://github.com/yunlong10/LaunchpadGPT/.
</details>
<details>
<summary>æ‘˜è¦</summary>
Launchpadæ˜¯ä¸€ç§éŸ³ä¹ instrumenteï¼Œå…è®¸ç”¨æˆ·é€šè¿‡é”®ç›˜æŒ‰é’®æ¥åˆ›ä½œå’Œæ¼”å¥éŸ³ä¹ã€‚ä¸ºäº†å¸®åŠ©è®¾è®¡Launchpadçš„å…‰æ•ˆå’Œæ¿€åŠ±beginneråˆ›ä½œéŸ³ä¹è§†è§‰ï¼Œæˆ‘ä»¬æå‡ºäº†LaunchpadGPTæ¨¡å‹ï¼Œè‡ªåŠ¨ç”ŸæˆLaunchpadæ¼”å¥çš„å…‰æ•ˆè®¾è®¡ã€‚åŸºäºä¼˜ç§€çš„è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬çš„LaunchpadGPTæ¥å—éŸ³ä¹ä½œå“ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºLaunchpadæ¼”å¥çš„è§†é¢‘å½¢å¼çš„å…‰æ•ˆè®¾è®¡ã€‚æˆ‘ä»¬æ”¶é›†äº†å¤§é‡Launchpadæ¼”å¥è§†é¢‘ï¼Œå¹¶å¯¹å…¶è¿›è¡Œå¤„ç†ï¼Œä»¥è·å–éŸ³ä¹å’Œå¯¹åº”çš„è§†é¢‘å¸§ä½œä¸ºæç¤ºå®Œæˆå¯¹çš„å¯¹ã€‚é€šè¿‡è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å®ç°äº†æ›´å¥½çš„éŸ³ä¹è§†è§‰åˆ›ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åˆ›é€ å‡ºæ¯”éšæœºç”Ÿæˆæ–¹æ³•æ›´å¥½çš„éŸ³ä¹è§†è§‰ï¼Œå¹¶æ‹¥æœ‰æ›´å¹¿æ³›çš„éŸ³ä¹è§†è§‰åº”ç”¨å‰æ™¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨https://github.com/yunlong10/LaunchpadGPT/æŸ¥çœ‹ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/08/cs.SD_2023_07_08/" data-id="clltaagol0073r8881o1udywe" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/08/eess.IV_2023_07_08/" class="article-date">
  <time datetime="2023-07-07T16:00:00.000Z" itemprop="datePublished">2023-07-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/08/eess.IV_2023_07_08/">eess.IV - 2023-07-08 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Lightweight-Improved-Residual-Network-for-Efficient-Inverse-Tone-Mapping"><a href="#Lightweight-Improved-Residual-Network-for-Efficient-Inverse-Tone-Mapping" class="headerlink" title="Lightweight Improved Residual Network for Efficient Inverse Tone Mapping"></a>Lightweight Improved Residual Network for Efficient Inverse Tone Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03998">http://arxiv.org/abs/2307.03998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liqi Xue, Tianyi Xu, Yongbao Song, Yan Liu, Lei Zhang, Xiantong Zhen, Jun Xu</li>
<li>for: ç”¨äºSDRå›¾åƒè½¬æ¢ä¸ºHDRå›¾åƒçš„é«˜æ•ˆ inverse tone mappingï¼ˆITMï¼‰ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§åŸºäºå¢å¼ºçš„ residual block çš„è½»é‡çº§ Improved Residual Networkï¼ˆIRNetï¼‰ï¼Œç”¨äºç²¾ç»†åŒ–HDRå›¾åƒé‡å»ºã€‚</li>
<li>results: åœ¨ä¸‰ä¸ªæ ‡å‡†æµ‹è¯•é›†ä¸Šå®ç°äº†State-of-the-artè¡¨ç°åœ¨ITMå’Œ joint SR-ITMä»»åŠ¡ä¸Šã€‚<details>
<summary>Abstract</summary>
The display devices like HDR10 televisions are increasingly prevalent in our daily life for visualizing high dynamic range (HDR) images. But the majority of media images on the internet remain in 8-bit standard dynamic range (SDR) format. Therefore, converting SDR images to HDR ones by inverse tone mapping (ITM) is crucial to unlock the full potential of abundant media images. However, existing ITM methods are usually developed with complex network architectures requiring huge computational costs. In this paper, we propose a lightweight Improved Residual Network (IRNet) by enhancing the power of popular residual block for efficient ITM. Specifically, we propose a new Improved Residual Block (IRB) to extract and fuse multi-layer features for fine-grained HDR image reconstruction. Experiments on three benchmark datasets demonstrate that our IRNet achieves state-of-the-art performance on both the ITM and joint SR-ITM tasks. The code, models and data will be publicly available at https://github.com/ThisisVikki/ITM-baseline.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ˜¾ç¤ºè®¾å¤‡å¦‚HDR10ç”µè§†åœ¨æˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œç”¨äºå¯è§åŒ–é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰å›¾åƒã€‚ä½†å¤§å¤šæ•°ç½‘ç»œå›¾åƒä»ç„¶ä¿ç•™åœ¨8ä½æ ‡å‡†åŠ¨æ€èŒƒå›´ï¼ˆSDRï¼‰æ ¼å¼ä¸­ã€‚å› æ­¤ï¼Œå°†SDRå›¾åƒè½¬æ¢æˆHDRå›¾åƒçš„ inverse tone mappingï¼ˆITMï¼‰å˜å¾—éå¸¸é‡è¦ï¼Œä»¥è§£é”ä¸°å¯Œçš„ç½‘ç»œå›¾åƒçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ITMæ–¹æ³•é€šå¸¸å…·æœ‰å¤æ‚çš„ç½‘ç»œæ¶æ„ï¼Œéœ€è¦å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ”¹è¿›çš„ residual ç½‘ç»œï¼ˆIRNetï¼‰ï¼Œé€šè¿‡æé«˜æµè¡Œçš„ residual å—æ¥æé«˜ç²¾ç»†åº¦çš„ HDR å›¾åƒé‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ”¹è¿›çš„ residual å—ï¼ˆIRBï¼‰ï¼Œç”¨äºæå–å’Œèåˆå¤šå±‚ç‰¹å¾è¿›è¡Œç²¾ç»†åº¦çš„ HDR å›¾åƒé‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ IRNet åœ¨ ITM å’Œ joint SR-ITM ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†çŠ¶æ€ç•¥ä½œæ€§çš„è¡¨ç°ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å°†åœ¨ GitHub ä¸Šå…¬å¼€ï¼Œè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒ <https://github.com/ThisisVikki/ITM-baseline>ã€‚
</details></li>
</ul>
<hr>
<h2 id="Ariadneâ€™s-Thread-Using-Text-Prompts-to-Improve-Segmentation-of-Infected-Areas-from-Chest-X-ray-images"><a href="#Ariadneâ€™s-Thread-Using-Text-Prompts-to-Improve-Segmentation-of-Infected-Areas-from-Chest-X-ray-images" class="headerlink" title="Ariadneâ€™s Thread:Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray images"></a>Ariadneâ€™s Thread:Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03942">http://arxiv.org/abs/2307.03942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junelin2333/languidemedseg-miccai2023">https://github.com/junelin2333/languidemedseg-miccai2023</a></li>
<li>paper_authors: Yi Zhong, Mengqiu Xu, Kongming Liang, Kaixin Chen, Ming Wu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜è‚ºç—…è¯Šæ–­çš„å‡†ç¡®æ€§ï¼Œæå‡ºäº†ä¸€ç§è¯­è¨€é©±åŠ¨çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œä»¥å¢å¼ºå›¾åƒåˆ†å‰²ç»“æœçš„å‡†ç¡®æ€§ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†è¯­è¨€æç¤ºæ¥æ”¹è¿›å›¾åƒåˆ†å‰²ç»“æœï¼Œå¹¶å¯¹QaTa-COV19æ•°æ®é›†è¿›è¡Œäº†å®éªŒï¼Œç»“æœæ˜¾ç¤ºï¼Œä¸å•Modalæ–¹æ³•ç›¸æ¯”ï¼Œè¯­è¨€é©±åŠ¨æ–¹æ³•å¯ä»¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>results: æœ¬ç ”ç©¶çš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯­è¨€æç¤ºå¯ä»¥æé«˜å›¾åƒåˆ†å‰²ç²¾åº¦ï¼Œå¹¶ä¸”å¯¹è®­ç»ƒæ•°æ®çš„å¤§å°æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚åœ¨QaTa-COV19æ•°æ®é›†ä¸Šï¼Œè¯­è¨€é©±åŠ¨æ–¹æ³•çš„Diceåˆ†æ•°æé«˜6.09%ä»¥ä¸Šï¼Œä¸å•Modalæ–¹æ³•ç›¸æ¯”ã€‚<details>
<summary>Abstract</summary>
Segmentation of the infected areas of the lung is essential for quantifying the severity of lung disease like pulmonary infections. Existing medical image segmentation methods are almost uni-modal methods based on image. However, these image-only methods tend to produce inaccurate results unless trained with large amounts of annotated data. To overcome this challenge, we propose a language-driven segmentation method that uses text prompt to improve to the segmentation result. Experiments on the QaTa-COV19 dataset indicate that our method improves the Dice score by 6.09% at least compared to the uni-modal methods. Besides, our extended study reveals the flexibility of multi-modal methods in terms of the information granularity of text and demonstrates that multi-modal methods have a significant advantage over image-only methods in terms of the size of training data required.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¯¹äºè‚ºç—…çš„è¯„ä¼°ï¼Œåˆ†å‰²æ„ŸæŸ“åŒºåŸŸçš„ç²¾ç¡®æ€§æ˜¯éå¸¸é‡è¦ã€‚ç°æœ‰çš„åŒ»ç–—å½±åƒåˆ†ç±»æ–¹æ³•éƒ½æ˜¯åŸºäºå›¾åƒçš„å• modal æ–¹æ³•ï¼Œä½†è¿™äº›å›¾åƒä»…æ–¹æ³•å¾€å¾€ä¼šå¯¼è‡´ä¸å‡†ç¡®çš„ç»“æœï¼Œé™¤éè®­ç»ƒæ•°æ®é‡å¾ˆå¤§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­è¨€é©±åŠ¨çš„åˆ†ç±»æ–¹æ³•ï¼Œä½¿ç”¨æ–‡æœ¬æç¤ºæ¥æ”¹å–„åˆ†ç±»ç»“æœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ QaTa-COV19 æ•°æ®é›†ä¸Šæé«˜äº† dice åˆ†æ•°6.09%è‡³å°‘ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„æ‰©å±•ç ”ç©¶æ˜¾ç¤ºï¼Œå¤š modal æ–¹æ³•åœ¨æ–‡æœ¬ä¿¡æ¯ç²’åº¦æ–¹é¢çš„çµæ´»æ€§å’Œè®­ç»ƒæ•°æ®é‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚
</details></li>
</ul>
<hr>
<h2 id="StyleGAN3-Generative-Networks-for-Improving-the-Equivariance-of-Translation-and-Rotation"><a href="#StyleGAN3-Generative-Networks-for-Improving-the-Equivariance-of-Translation-and-Rotation" class="headerlink" title="StyleGAN3: Generative Networks for Improving the Equivariance of Translation and Rotation"></a>StyleGAN3: Generative Networks for Improving the Equivariance of Translation and Rotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03898">http://arxiv.org/abs/2307.03898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianlei Zhu, Junqi Chen, Renzhe Zhu, Gaurav Gupta</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯è¯„ä¼°StyleGAN2å’Œä¸¤ä¸ªä¿®æ”¹åçš„StyleGAN3ç‰ˆæœ¬åœ¨ç”Ÿæˆå›¾åƒæ–¹é¢çš„æ€§èƒ½å·®å¼‚ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨çš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨FFHQæ•°æ®é›†å’ŒFIDã€EQ-Tå’ŒEQ-RæŒ‡æ ‡è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼ŒStyleGan3ç‰ˆæœ¬æ˜¯ä¸€ä¸ªæ›´å¥½çš„ç”Ÿæˆç½‘ç»œï¼Œå¯ä»¥æé«˜å›¾åƒçš„ç­‰è·å˜æ¢æ€§ã€‚è¿™äº›å‘ç°å¯¹åŠ¨ç”»å’Œè§†é¢‘çš„åˆ›ä½œæœ‰ç§¯æçš„å½±å“ã€‚<details>
<summary>Abstract</summary>
StyleGAN can use style to affect facial posture and identity features, and noise to affect hair, wrinkles, skin color and other details. Among these, the outcomes of the picture processing will vary slightly between different versions of styleGAN. As a result, the comparison of performance differences between styleGAN2 and the two modified versions of styleGAN3 will be the main focus of this study. We used the FFHQ dataset as the dataset and FID, EQ-T, and EQ-R were used to be the assessment of the model. In the end, we discovered that Stylegan3 version is a better generative network to improve the equivariance. Our findings have a positive impact on the creation of animation and videos.
</details>
<details>
<summary>æ‘˜è¦</summary>
StyleGAN å¯ä»¥é€šè¿‡é£æ ¼å½±å“ facial å§¿æ€å’Œäººè„¸ç‰¹å¾ï¼Œå¹¶é€šè¿‡å™ªå£°å½±å“å¤´å‘ã€çš®è‚¤è‰²ã€çš±çº¹ç­‰ç»†èŠ‚ã€‚ Among these, ä¸åŒç‰ˆæœ¬çš„ StyleGAN çš„å›¾åƒå¤„ç†ç»“æœä¼šæœ‰ä¸€äº›å¾®å¦™çš„å·®å¼‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åœ¨è¿™ç§æƒ…å†µä¸‹è¿›è¡Œ StyleGAN2 å’Œä¸¤ä¸ªä¿®æ”¹ç‰ˆæœ¬çš„ StyleGAN3 ä¹‹é—´çš„æ€§èƒ½å¯¹æ¯”ã€‚æˆ‘ä»¬ä½¿ç”¨ FFHQ æ•°æ®é›†ä½œä¸ºæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ FIDã€EQ-T å’Œ EQ-R ä¸‰ç§æŒ‡æ ‡è¯„ä¼°æ¨¡å‹ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å‘ç° StyleGAN3 ç‰ˆæœ¬æ˜¯ä¸€ä¸ªæ›´å¥½çš„ç”Ÿæˆç½‘ç»œï¼Œå¯ä»¥æé«˜equivarianceã€‚æˆ‘ä»¬çš„å‘ç°å¯¹åŠ¨ç”»å’Œè§†é¢‘çš„åˆ›å»ºäº§ç”Ÿäº†ç§¯æçš„å½±å“ã€‚Here's the breakdown of the translation:* StyleGAN å¯ä»¥é€šè¿‡é£æ ¼å½±å“ facial å§¿æ€å’Œäººè„¸ç‰¹å¾ (StyleGAN can use style to affect facial posture and identity features)* å¹¶é€šè¿‡å™ªå£°å½±å“å¤´å‘ã€çš®è‚¤è‰²ã€çš±çº¹ç­‰ç»†èŠ‚ (and noise to affect hair, skin color, and other details)* Among these, ä¸åŒç‰ˆæœ¬çš„ StyleGAN çš„å›¾åƒå¤„ç†ç»“æœä¼šæœ‰ä¸€äº›å¾®å¦™çš„å·®å¼‚ (Among these, the outcomes of the picture processing will vary slightly between different versions of StyleGAN)* å› æ­¤ï¼Œæˆ‘ä»¬å°†åœ¨è¿™ç§æƒ…å†µä¸‹è¿›è¡Œ StyleGAN2 å’Œä¸¤ä¸ªä¿®æ”¹ç‰ˆæœ¬çš„ StyleGAN3 ä¹‹é—´çš„æ€§èƒ½å¯¹æ¯” (Therefore, we will compare the performance of StyleGAN2 and the two modified versions of StyleGAN3 in this situation)* æˆ‘ä»¬ä½¿ç”¨ FFHQ æ•°æ®é›†ä½œä¸ºæ•°æ®é›† (We use the FFHQ dataset as the dataset)* å¹¶ä½¿ç”¨ FIDã€EQ-T å’Œ EQ-R ä¸‰ç§æŒ‡æ ‡è¯„ä¼°æ¨¡å‹ (And use three metrics to evaluate the model: FID, EQ-T, and EQ-R)* æœ€ç»ˆï¼Œæˆ‘ä»¬å‘ç° StyleGAN3 ç‰ˆæœ¬æ˜¯ä¸€ä¸ªæ›´å¥½çš„ç”Ÿæˆç½‘ç»œï¼Œå¯ä»¥æé«˜equivariance (Finally, we found that the StyleGAN3 version is a better generative network, which can improve equivariance)* æˆ‘ä»¬çš„å‘ç°å¯¹åŠ¨ç”»å’Œè§†é¢‘çš„åˆ›å»ºäº§ç”Ÿäº†ç§¯æçš„å½±å“ (Our discovery has a positive impact on the creation of animation and videos)
</details></li>
</ul>
<hr>
<h2 id="TBSS-A-novel-computational-method-for-Tract-Based-Spatial-Statistics"><a href="#TBSS-A-novel-computational-method-for-Tract-Based-Spatial-Statistics" class="headerlink" title="TBSS++: A novel computational method for Tract-Based Spatial Statistics"></a>TBSS++: A novel computational method for Tract-Based Spatial Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05387">http://arxiv.org/abs/2307.05387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davood Karimi, Hamza Kebiri, Ali Gholipour</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨æé«˜Diffusion-weightedç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰ä¸­è„‘ç™½ mater çš„è¯„ä¼°ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®¡ç®—æ¡†æ¶ï¼Œé€šè¿‡ï¼ˆiï¼‰ç²¾å‡†çš„è„‘ tract åˆ†å‰²ï¼Œå’Œï¼ˆiiï¼‰äº¤å‰Subjectæ•°æ®çš„ç²¾å‡†æ³¨å†Œï¼Œä»¥è¶…è¶Šç°æœ‰æ–¹æ³•çš„ç¼ºé™·å’Œé™åˆ¶ã€‚</li>
<li>results: ä¸TBSSç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å¯ä»¥æä¾›æ›´é«˜çš„é‡å¤æ€§å’Œæ•°æ®æŠ–åŠ¨é²æ£’æ€§ã€‚<details>
<summary>Abstract</summary>
Diffusion-weighted magnetic resonance imaging (dMRI) is widely used to assess the brain white matter. One of the most common computations in dMRI involves cross-subject tract-specific analysis, whereby dMRI-derived biomarkers are compared between cohorts of subjects. The accuracy and reliability of these studies hinges on the ability to compare precisely the same white matter tracts across subjects. This is an intricate and error-prone computation. Existing computational methods such as Tract-Based Spatial Statistics (TBSS) suffer from a host of shortcomings and limitations that can seriously undermine the validity of the results. We present a new computational framework that overcomes the limitations of existing methods via (i) accurate segmentation of the tracts, and (ii) precise registration of data from different subjects/scans. The registration is based on fiber orientation distributions. To further improve the alignment of cross-subject data, we create detailed atlases of white matter tracts. These atlases serve as an unbiased reference space where the data from all subjects is registered for comparison. Extensive evaluations show that, compared with TBSS, our proposed framework offers significantly higher reproducibility and robustness to data perturbations. Our method promises a drastic improvement in accuracy and reproducibility of cross-subject dMRI studies that are routinely used in neuroscience and medical research.
</details>
<details>
<summary>æ‘˜è¦</summary>
Diffusion-weighted Ğ¼Ğ°Ğ³Ğ½Ğ¸Ñ‚Ğ½Ğ¾å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰å¹¿æ³›ç”¨äºè¯„ä¼°å¤§è„‘ç™½ matterã€‚ä¸€ç§æœ€å¸¸è§çš„è®¡ç®—åœ¨ dMRI ä¸­æ˜¯ Ğ¼ĞµĞ¶Ğ´Ñƒ cohorts of subjects è¿›è¡Œ tract-specific åˆ†æï¼Œå…¶ä¸­ dMRI å¾—åˆ°çš„ç”Ÿç‰©æ ‡å¿—ç‰©è¢«æ¯”è¾ƒ Ğ¼ĞµĞ¶Ğ´Ñƒä¸åŒçš„subjectsã€‚è¿™äº›ç ”ç©¶çš„å‡†ç¡®æ€§å’Œå¯é æ€§å–å†³äºèƒ½å¤Ÿå‡†ç¡®æ¯”è¾ƒä¸åŒsubjects ä¸­çš„ç™½ matter tractsã€‚ç°æœ‰çš„è®¡ç®—æ–¹æ³•ï¼Œå¦‚ Tract-Based Spatial Statisticsï¼ˆTBSSï¼‰ï¼Œå—åˆ°ä¸¥é‡çš„ç¼ºé™·å’Œé™åˆ¶ï¼Œè¿™äº›ç¼ºé™·å¯èƒ½ä¼šä¸¥é‡åœ°æŸå®³ç»“æœçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„è®¡ç®—æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥è¶…è¶Šç°æœ‰çš„æ–¹æ³•çš„é™åˆ¶ï¼Œé€šè¿‡ï¼ˆiï¼‰å‡†ç¡®åœ°åˆ†å‰² tractsï¼Œå’Œï¼ˆiiï¼‰ç²¾å‡†åœ°æ³¨å†Œä¸åŒsubjects/scans çš„æ•°æ®ã€‚æ³¨å†ŒåŸºäºçº¤ç»´æ–¹å‘åˆ†å¸ƒã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹è¿›cross-subjectæ•°æ®çš„å¯¹Alignmentï¼Œæˆ‘ä»¬åˆ›å»ºäº†è¯¦ç»†çš„ white matter tracts  Atlasesã€‚è¿™äº› Atlases ä½œä¸ºä¸€ä¸ªä¸åè§çš„å‚ç…§ç©ºé—´ï¼Œç”¨äºæ³¨å†Œæ‰€æœ‰subjects çš„æ•°æ®è¿›è¡Œæ¯”è¾ƒã€‚å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼Œç›¸æ¯”TBSSï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•å…·æœ‰æ›´é«˜çš„å¯é‡ç°æ€§å’Œæ•°æ®æŠ–åŠ¨å¼ºåº¦çš„é²æ£’æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ‰¿è¯ºå¯ä»¥åœ¨ neuroscience å’ŒåŒ»å­¦ç ”ç©¶ä¸­æä¾›æ˜æ˜¾çš„æ”¹è¿›ï¼Œä»¥æé«˜cross-subject dMRI ç ”ç©¶çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Invariant-Scattering-Transform-for-Medical-Imaging"><a href="#Invariant-Scattering-Transform-for-Medical-Imaging" class="headerlink" title="Invariant Scattering Transform for Medical Imaging"></a>Invariant Scattering Transform for Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04771">http://arxiv.org/abs/2307.04771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nafisa Labiba Ishrat Huda, Angona Biswas, MD Abdullah Al Nasim, Md. Fahim Rahman, Shoaib Ahmed</li>
<li>for:  This paper is written for researchers and practitioners in the field of medical image analysis and deep learning, particularly those interested in using scattering transform for efficient image classification.</li>
<li>methods: The paper uses a novel approach called scattering transform, which combines signal processing and deep learning for medical image analysis. The transform is based on a wavelet technique that builds a useful signal representation for image classification.</li>
<li>results: The paper presents a step-by-step case study demonstrating the efficiency of scattering transform for medical image analysis, achieving high accuracy and outperforming traditional deep learning methods.Here is the information in Simplified Chinese text:</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºåŒ»å­¦å›¾åƒåˆ†æå’Œæ·±åº¦å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶äººå‘˜å’Œå®è·µè€…å†™çš„ï¼Œç‰¹åˆ«æ˜¯å…³å¿ƒä½¿ç”¨æ•£å°„å˜æ¢è¿›è¡Œé«˜æ•ˆçš„å›¾åƒåˆ†ç±»ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æ•£å°„å˜æ¢ï¼Œå®ƒå°†ä¿¡å·å¤„ç†å’Œæ·±åº¦å­¦ä¹ ä¸¤ä¸ªé¢†åŸŸèåˆåœ¨ä¸€èµ·ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†æã€‚æ•£å°„å˜æ¢åŸºäºwaveletæŠ€æœ¯ï¼Œå»ºç«‹äº†æœ‰ç”¨çš„ä¿¡å·è¡¨ç¤ºï¼Œç”¨äºå›¾åƒåˆ†ç±»ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡å±•ç¤ºäº†ä¸€ä¸ªæ­¥éª¤å¾ˆå¤šçš„æ¡ˆä¾‹ç ”ç©¶ï¼Œç”¨äºè¯æ˜æ•£å°„å˜æ¢åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é«˜æ•ˆæ€§ï¼Œå¹¶ä¸”è¶…è¿‡äº†ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Invariant scattering transform introduces new area of research that merges the signal processing with deep learning for computer vision. Nowadays, Deep Learning algorithms are able to solve a variety of problems in medical sector. Medical images are used to detect diseases brain cancer or tumor, Alzheimer's disease, breast cancer, Parkinson's disease and many others. During pandemic back in 2020, machine learning and deep learning has played a critical role to detect COVID-19 which included mutation analysis, prediction, diagnosis and decision making. Medical images like X-ray, MRI known as magnetic resonance imaging, CT scans are used for detecting diseases. There is another method in deep learning for medical imaging which is scattering transform. It builds useful signal representation for image classification. It is a wavelet technique; which is impactful for medical image classification problems. This research article discusses scattering transform as the efficient system for medical image analysis where it's figured by scattering the signal information implemented in a deep convolutional network. A step by step case study is manifested at this research work.
</details>
<details>
<summary>æ‘˜è¦</summary>
å›ºå®šæ‰©æ•£å˜æ¢å¼•å…¥äº†ä¸€æ–°çš„ç ”ç©¶é¢†åŸŸï¼ŒæŠŠä¿¡å·å¤„ç†ä¸æ·±åº¦å­¦ä¹ ç»“åˆä»¥åº”ç”¨äºè®¡ç®—æœºè§†è§‰ã€‚ç›®å‰ï¼Œæ·±åº¦å­¦ä¹ ç®—æ³•èƒ½å¤Ÿè§£å†³åŒ»ç–—é¢†åŸŸå¤šç§é—®é¢˜ã€‚åŒ»ç–—å›¾åƒç”¨äºæ£€æµ‹è„‘ç˜¤æˆ–è‚¿ç˜¤ã€é˜¿å°”èŒ¨æ›¼ç—…ã€ä¹³è…ºç™Œã€parkinsonç—…å’Œå…¶ä»–å¤šç§ç–¾ç—…ã€‚åœ¨2020å¹´ç–«æƒ…æœŸé—´ï¼Œæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ‰®æ¼”äº†å…³é”®è§’è‰²ï¼Œæ£€æµ‹COVID-19ï¼ŒåŒ…æ‹¬å˜å¼‚åˆ†æã€é¢„æµ‹ã€è¯Šæ–­å’Œå†³ç­–ã€‚åŒ»ç–—å›¾åƒå¦‚Xå°„çº¿ã€MRIï¼ˆç£å…±æŒ¯æˆåƒï¼‰ã€CTæ‰«ææ˜¯ç”¨äºæ£€æµ‹ç–¾ç—…çš„ã€‚æ­¤å¤–ï¼Œæ·±åº¦å­¦ä¹ è¿˜æœ‰å¦ä¸€ç§æ–¹æ³•ç”¨äºåŒ»ç–—å›¾åƒåˆ†ç±»ï¼Œå³æ‰©æ•£å˜æ¢ã€‚å®ƒå»ºç«‹äº†æœ‰ç”¨çš„ä¿¡å·è¡¨ç¤ºï¼Œç”¨äºå›¾åƒåˆ†ç±»é—®é¢˜ã€‚è¿™ç¯‡ç ”ç©¶æ–‡ç« è®¨è®ºäº†æ‰©æ•£å˜æ¢ä½œä¸ºåŒ»ç–—å›¾åƒåˆ†æçš„æœ‰æ•ˆç³»ç»Ÿï¼Œå…¶ä¸­ä½¿ç”¨äº†æ‰©æ•£ä¿¡å·ä¿¡æ¯åœ¨æ·±åº¦å¾å€¼ç½‘ç»œä¸­å®ç°ã€‚æœ¬ç ”ç©¶æ–‡ç« è¿˜æä¾›äº†ä¸€æ­¥æ­¥çš„å®è·µæ¡ˆä¾‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Coordinate-based-neural-representations-for-computational-adaptive-optics-in-widefield-microscopy"><a href="#Coordinate-based-neural-representations-for-computational-adaptive-optics-in-widefield-microscopy" class="headerlink" title="Coordinate-based neural representations for computational adaptive optics in widefield microscopy"></a>Coordinate-based neural representations for computational adaptive optics in widefield microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03812">http://arxiv.org/abs/2307.03812</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iksungk/cocoa">https://github.com/iksungk/cocoa</a></li>
<li>paper_authors: Iksung Kang, Qinrong Zhang, Stella X. Yu, Na Ji<br>for:* è¿™ä¸ªè®ºæ–‡æ—¨åœ¨æè¿°ä¸€ç§åŸºäºè‡ªé€‚åº”å…‰å­¦çš„Machine Learningç®—æ³•ï¼Œç”¨äºæ— ä¾µå…¥æ€§åœ°å›¾åƒç”Ÿç‰©ç»“æ„ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å¤æ‚çš„æ ·å“ä¸­æé«˜å›¾åƒè´¨é‡ã€‚methods:* è¿™ä¸ªç®—æ³•ä½¿ç”¨äº†è‡ªé€‚åº”å…‰å­¦æŠ€æœ¯ï¼ŒåŒ…æ‹¬å…‰è°±æ‰«æå’Œæ¿€å…‰æ‰«æï¼Œä»¥ä¼°è®¡æ³¢å‰å¼¯æ›²å’Œä¸‰ç»´ç»“æ„ä¿¡æ¯ã€‚results:* ç ”ç©¶äººå‘˜ä½¿ç”¨äº†è¿™ä¸ªç®—æ³•ï¼Œåœ¨å®éªŒå®¤ä¸­æˆåŠŸåœ°å›¾åƒäº†ä¸€ä¸ª mouse brain çš„ä¸‰ç»´ç»“æ„ï¼Œå¹¶ä¸”ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†è¿™ä¸ªç®—æ³•çš„æ€§èƒ½çš„é™åˆ¶å› ç´ ã€‚<details>
<summary>Abstract</summary>
Widefield microscopy is widely used for non-invasive imaging of biological structures at subcellular resolution. When applied to complex specimen, its image quality is degraded by sample-induced optical aberration. Adaptive optics can correct wavefront distortion and restore diffraction-limited resolution but require wavefront sensing and corrective devices, increasing system complexity and cost. Here, we describe a self-supervised machine learning algorithm, CoCoA, that performs joint wavefront estimation and three-dimensional structural information extraction from a single input 3D image stack without the need for external training dataset. We implemented CoCoA for widefield imaging of mouse brain tissues and validated its performance with direct-wavefront-sensing-based adaptive optics. Importantly, we systematically explored and quantitatively characterized the limiting factors of CoCoA's performance. Using CoCoA, we demonstrated the first in vivo widefield mouse brain imaging using machine-learning-based adaptive optics. Incorporating coordinate-based neural representations and a forward physics model, the self-supervised scheme of CoCoA should be applicable to microscopy modalities in general.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¹¿è§’å¾®scopia å¹¿æ³›åº”ç”¨äºéä¾µå…¥æ€§çš„ç”Ÿç‰©ç»“æ„æˆåƒï¼Œå…¶å›¾åƒè´¨é‡åœ¨å¤æ‚æ ·å“ä¸‹å—æ ·å“å¼•èµ·çš„å…‰å­¦æ‰­æ›²çš„å½±å“ã€‚å¯é€‚åº”å…‰å­¦å¯ä»¥ä¿®å¤æ³¢å‰å¼¯æ›²å’Œæ¢å¤ diffraction-limited åˆ†è¾¨ç‡ï¼Œä½†éœ€è¦æ³¢å‰æµ‹é‡å’Œä¿®æ­£è®¾å¤‡ï¼Œä»è€Œå¢åŠ ç³»ç»Ÿå¤æ‚åº¦å’Œæˆæœ¬ã€‚æˆ‘ä»¬æè¿°äº†ä¸€ç§è‡ªä¸»å­¦ä¹ æœºå™¨å­¦ä¹ ç®—æ³• CoCoAï¼Œå®ƒå¯ä»¥åœ¨å•ä¸ªè¾“å…¥ 3D å›¾åƒå †ä¸­åŒæ—¶è¿›è¡Œæ³¢å‰ä¼°è®¡å’Œä¸‰ç»´ç»“æ„ä¿¡æ¯æå–ï¼Œæ— éœ€å¤–éƒ¨è®­ç»ƒé›†ã€‚æˆ‘ä»¬åœ¨å®½åœºæ¢é’ˆä¸­å®ç°äº† CoCoAï¼Œå¹¶é€šè¿‡ direct-wavefront-sensing-based adaptive optics è¿›è¡ŒéªŒè¯ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢å’Œé‡åŒ– CoCoA çš„æ€§èƒ½é™åˆ¶å› ç´ ã€‚ä½¿ç”¨ CoCoAï¼Œæˆ‘ä»¬å®ç°äº†é¦–æ¬¡åœ¨ vivo å®½åœº mouse brain æˆåƒï¼Œä½¿ç”¨æœºå™¨å­¦ä¹ åŸºäº adaptive optics ã€‚é€šè¿‡å·ç§¯ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½è¡¨ç¤ºå’Œå‰å‘ç‰©ç†æ¨¡å‹ï¼ŒCoCoA çš„è‡ªä¸»å­¦ä¹ æ–¹æ¡ˆåº”ç”¨äº microscopy Modalities ä¸­ã€‚
</details></li>
</ul>
<hr>
<h2 id="Thoracic-Cartilage-Ultrasound-CT-Registration-using-Dense-Skeleton-Graph"><a href="#Thoracic-Cartilage-Ultrasound-CT-Registration-using-Dense-Skeleton-Graph" class="headerlink" title="Thoracic Cartilage Ultrasound-CT Registration using Dense Skeleton Graph"></a>Thoracic Cartilage Ultrasound-CT Registration using Dense Skeleton Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03800">http://arxiv.org/abs/2307.03800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongliang Jiang, Chenyang Li, Xuesong Li, Nassir Navab</li>
<li>for: ç”¨äºå®ç°è‡ªé€‚åº”è¶…å£°æˆåƒï¼Œå°¤å…¶æ˜¯åœ¨éª¨éª¼ç»“æ„ä¸‹é¢çš„é«˜é¢‘ç‡å¸æ”¶å±‚é¢ä¸Šã€‚</li>
<li>methods: ä½¿ç”¨å›¾å½¢åŸºäºéå¯¼å…¥æ³¨å†Œæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨éª¨è¡¨é¢ç‰¹å¾æ¥è½¬ç§»è§„åˆ’è·¯å¾„ã€‚</li>
<li>results: å¯ä»¥æœ‰æ•ˆåœ°å°†è§„åˆ’è·¯å¾„ä»CTå›¾åƒä¼ æ’­åˆ°å½“å‰è®¾ç½®ä¸‹çš„USè§†å›¾ï¼Œå¹¶ä¸”å¯ä»¥å‡å°‘å¹²æ‰°ã€‚<details>
<summary>Abstract</summary>
Autonomous ultrasound (US) imaging has gained increased interest recently, and it has been seen as a potential solution to overcome the limitations of free-hand US examinations, such as inter-operator variations. However, it is still challenging to accurately map planned paths from a generic atlas to individual patients, particularly for thoracic applications with high acoustic-impedance bone structures under the skin. To address this challenge, a graph-based non-rigid registration is proposed to enable transferring planned paths from the atlas to the current setup by explicitly considering subcutaneous bone surface features instead of the skin surface. To this end, the sternum and cartilage branches are segmented using a template matching to assist coarse alignment of US and CT point clouds. Afterward, a directed graph is generated based on the CT template. Then, the self-organizing map using geographical distance is successively performed twice to extract the optimal graph representations for CT and US point clouds, individually. To evaluate the proposed approach, five cartilage point clouds from distinct patients are employed. The results demonstrate that the proposed graph-based registration can effectively map trajectories from CT to the current setup for displaying US views through limited intercostal space. The non-rigid registration results in terms of Hausdorff distance (Mean$\pm$SD) is 9.48$\pm$0.27 mm and the path transferring error in terms of Euclidean distance is 2.21$\pm$1.11 mm.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªä¸»å¼è¶…å£°æˆåƒï¼ˆUSï¼‰åœ¨æœ€è¿‘å‡ å¹´å†…å¾—åˆ°äº†æ›´å¤šçš„å…³æ³¨ï¼Œè¢«è§†ä¸ºå¯ä»¥è¶…è¶Šè‡ªç”±æ‰‹æ“ä½œUSæ£€æµ‹çš„é™åˆ¶ã€‚ç„¶è€Œï¼Œå‡†ç¡®åœ°å°†è§„åˆ’è·¯å¾„ä»é€šç”¨ Atlas ä¼ é€’åˆ°å½“å‰è®¾ç½®ä»ç„¶æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éª¨ç›†éƒ¨åº”ç”¨ä¸­ï¼Œå› ä¸ºæœ‰é«˜é¢‘ç‡å£° impedance ç»“æ„ä½äºçš®è‚¤ä¸‹ã€‚ä¸ºè§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œä¸€ç§åŸºäºå›¾çš„éRIGID  Ñ€ĞµĞ³Ğ¸straciÃ³nè¢«æè®®ï¼Œä»¥ä¾¿å°†è§„åˆ’è·¯å¾„ä» Atlas ä¼ é€’åˆ°å½“å‰è®¾ç½®ï¼Œå¹¶ä¸”Explicitly è€ƒè™‘åˆ°éª¨è´¨è¡¨é¢ç‰¹å¾è€Œä¸æ˜¯çš®è‚¤è¡¨é¢ã€‚ä¸ºæ­¤ï¼Œä½¿ç”¨æ¨¡æ¿åŒ¹é… segment èƒ¸æ¿å’Œè½¯éª¨æ”¯æŒçš„ Cartilage åˆ†æ”¯ã€‚ç„¶åï¼ŒåŸºäº CT æ¨¡æ¿ç”Ÿæˆä¸€ä¸ªæŒ‡å‘å›¾ã€‚æ¥ç€ï¼Œä½¿ç”¨è‡ªç»„ç»‡åœ°å›¾è¿›è¡Œä¸¤æ¬¡ Successive åœ°æ‰§è¡Œ geographical distance è‡ªé€‚åº”æ˜ å°„ï¼Œä»¥æå– CT å’Œ US ç‚¹äº‘çš„æœ€ä½³å›¾è¡¨ç¤ºã€‚ä¸ºè¯„ä¼°æè®®æ–¹æ³•ï¼Œä½¿ç”¨äº†äº”ä¸ªä¸åŒæ‚£è€…çš„ Cartilage ç‚¹äº‘ã€‚ç»“æœè¡¨æ˜ï¼Œæè®®çš„å›¾åŸºäºREGISTRATION å¯ä»¥æœ‰æ•ˆåœ°å°† CT çš„è§„åˆ’è·¯å¾„ä¼ é€’åˆ°å½“å‰è®¾ç½®ï¼Œå¹¶ä¸”éRIGID  Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°ciÃ³nçš„ Hausdorff è·ç¦»ï¼ˆMean Â± SDï¼‰ä¸º 9.48 Â± 0.27 mmï¼Œè·¯å¾„ä¼ é€’é”™è¯¯ï¼ˆEuclidean è·ç¦»ï¼‰ä¸º 2.21 Â± 1.11 mmã€‚
</details></li>
</ul>
<hr>
<h2 id="Motion-Magnification-in-Robotic-Sonography-Enabling-Pulsation-Aware-Artery-Segmentation"><a href="#Motion-Magnification-in-Robotic-Sonography-Enabling-Pulsation-Aware-Artery-Segmentation" class="headerlink" title="Motion Magnification in Robotic Sonography: Enabling Pulsation-Aware Artery Segmentation"></a>Motion Magnification in Robotic Sonography: Enabling Pulsation-Aware Artery Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03698">http://arxiv.org/abs/2307.03698</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dianyehuang/robpmepasnn">https://github.com/dianyehuang/robpmepasnn</a></li>
<li>paper_authors: Dianye Huang, Yuan Bi, Nassir Navab, Zhongliang Jiang</li>
<li>for: ç”¨äºè¯Šæ–­å’Œç›‘æµ‹arterialç–¾ç—…ï¼Œæä¾›éä¾µå…¥ã€æ— è¾å°„ã€å®æ—¶çš„ä¼˜åŠ¿ã€‚</li>
<li>methods: ä½¿ç”¨neuronalç½‘ç»œï¼ˆPAS-NNï¼‰ï¼Œåˆ©ç”¨å¿ƒè·³åˆºæ¿€ä¿¡å·ï¼Œæé«˜è¡€ç®¡åˆ†å‰²ç²¾åº¦å’Œç¨³å®šæ€§ã€‚</li>
<li>results: åœ¨ volontiersçš„carotidå’Œradial arteryä¸Šè¿›è¡Œå®éªŒï¼Œ demonstarted that PAS-NNå¯ä»¥ä¸å½“å‰æœ€ä½³æ–¹æ³•åŒ¹é…ï¼Œå¹¶æœ‰æ•ˆåœ°æ”¹å–„å°è¡€ç®¡ï¼ˆradial arteryï¼‰çš„åˆ†å‰²æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Ultrasound (US) imaging is widely used for diagnosing and monitoring arterial diseases, mainly due to the advantages of being non-invasive, radiation-free, and real-time. In order to provide additional information to assist clinicians in diagnosis, the tubular structures are often segmented from US images. To improve the artery segmentation accuracy and stability during scans, this work presents a novel pulsation-assisted segmentation neural network (PAS-NN) by explicitly taking advantage of the cardiac-induced motions. Motion magnification techniques are employed to amplify the subtle motion within the frequency band of interest to extract the pulsation signals from sequential US images. The extracted real-time pulsation information can help to locate the arteries on cross-section US images; therefore, we explicitly integrated the pulsation into the proposed PAS-NN as attention guidance. Notably, a robotic arm is necessary to provide stable movement during US imaging since magnifying the target motions from the US images captured along a scan path is not manually feasible due to the hand tremor. To validate the proposed robotic US system for imaging arteries, experiments are carried out on volunteers' carotid and radial arteries. The results demonstrated that the PAS-NN could achieve comparable results as state-of-the-art on carotid and can effectively improve the segmentation performance for small vessels (radial artery).
</details>
<details>
<summary>æ‘˜è¦</summary>
ultrasoundï¼ˆUSï¼‰æˆåƒå¹¿æ³›åº”ç”¨äºè¯Šæ–­å’Œç›‘æµ‹åŠ¨è„‰ç–¾ç—…ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒä¸ä¾µå…¥ã€æ— è¾å°„å’Œå®æ—¶ã€‚ä¸ºäº†ä¸ºä¸´åºŠåŒ»ç”Ÿæä¾›æ›´å¤šçš„è¯Šæ–­ä¿¡æ¯ï¼Œåœ¨USå›¾åƒä¸­åˆ†å‰²åŠ¨è„‰ç»“æ„æˆä¸ºä¸€é¡¹é‡è¦ä»»åŠ¡ã€‚ä¸ºäº†æé«˜åŠ¨è„‰åˆ†å‰²ç²¾åº¦å’Œç¨³å®šæ€§ï¼Œæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºå¾åŠ¨è„‰ä¿¡å·çš„æ–°å‹æ¿€æ´»åˆ†å‰²ç¥ç»ç½‘ç»œï¼ˆPAS-NNï¼‰ã€‚ä½¿ç”¨äº†æŒ¯è¡å¢å¼ºæŠ€æœ¯æ¥å¢å¼ºUSå›¾åƒä¸­çš„æŸäº›é¢‘è°±ä¿¡æ¯ï¼Œä»¥æå–åŠ¨è„‰çš„å¾åŠ¨è„‰ä¿¡å·ã€‚è¿™äº›å®æ—¶å¾åŠ¨è„‰ä¿¡å·å¯ä»¥å¸®åŠ©åœ¨USå›¾åƒçš„æ¨ªæˆªé¢ä¸Šå®šä½åŠ¨è„‰ï¼Œå› æ­¤æˆ‘ä»¬ç›´æ¥å°†å¾åŠ¨è„‰ä¿¡å· Ğ¸Ğ½Ñ‚ĞµGRATEDåˆ°ææ¡ˆçš„PAS-NNä¸­ä½œä¸ºæ³¨æ„åŠ›å¼•å¯¼ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸ºäº†ä¿è¯USæˆåƒè¿‡ç¨‹ä¸­çš„ç¨³å®šè¿åŠ¨ï¼Œéœ€è¦ä½¿ç”¨æœºå™¨äººè‡‚æä¾›ç¨³å®šçš„è¿åŠ¨ã€‚ä¸ºéªŒè¯ææ¡ˆçš„æœºå™¨äººUSç³»ç»Ÿæ˜¯å¦èƒ½å¤ŸæˆåŠŸåœ°æˆåƒåŠ¨è„‰ï¼Œæˆ‘ä»¬åœ¨å¿—æ„¿è€…çš„è½®çŠ¶å’Œ radial artery ä¸Šè¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒPAS-NNå¯ä»¥ä¸å½“å‰æœ€ä½³çš„ç»“æœç›¸æ¯”ï¼Œå¹¶ä¸”å¯ä»¥æœ‰æ•ˆåœ°æé«˜å°åŠ¨è„‰ï¼ˆ radial arteryï¼‰çš„åˆ†å‰²æ€§èƒ½ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/08/eess.IV_2023_07_08/" data-id="clltaagq000bzr8885o7qbst4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/07/cs.AI_2023_07_07/" class="article-date">
  <time datetime="2023-07-06T16:00:00.000Z" itemprop="datePublished">2023-07-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/07/cs.AI_2023_07_07/">cs.AI - 2023-07-07 20:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Decomposing-the-Generalization-Gap-in-Imitation-Learning-for-Visual-Robotic-Manipulation"><a href="#Decomposing-the-Generalization-Gap-in-Imitation-Learning-for-Visual-Robotic-Manipulation" class="headerlink" title="Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation"></a>Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03659">http://arxiv.org/abs/2307.03659</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RLAgent/factor-world">https://github.com/RLAgent/factor-world</a></li>
<li>paper_authors: Annie Xie, Lisa Lee, Ted Xiao, Chelsea Finn</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æ¢è®¨è§†è§‰æœºå™¨äºº manipulate æ¼”ç¤ºä¸­çš„æ¨¡ä»¿å­¦ä¹ å›°éš¾çš„åŸå› ï¼Œä»¥åŠè¿™äº›å›°éš¾çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨äº† simulation å’ŒçœŸå®æœºå™¨äººè¯­è¨€æ¡ä»¶ manipulate ä»»åŠ¡æ¥è¯„ä¼°æ¨¡ä»¿å­¦ä¹ ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ–°çš„ simulated æµ‹è¯•ç¯å¢ƒæ¥æ›´åŠ æ§åˆ¶åœ°è¯„ä¼°ä¸åŒå› ç´ çš„æ³›åŒ–éš¾åº¦ã€‚</li>
<li>results: æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä¸åŒå› ç´ çš„æ³›åŒ–éš¾åº¦å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œå¹¶ä¸”è¿™äº›å·®å¼‚æ˜¯ç›¸å¯¹ç¨³å®šçš„ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼ŒæŸäº›å› ç´ çš„æ³›åŒ–éš¾åº¦è¾ƒé«˜ï¼Œè€Œå¦å¤–çš„å› ç´ åˆ™è¾ƒä½ã€‚<details>
<summary>Abstract</summary>
What makes generalization hard for imitation learning in visual robotic manipulation? This question is difficult to approach at face value, but the environment from the perspective of a robot can often be decomposed into enumerable factors of variation, such as the lighting conditions or the placement of the camera. Empirically, generalization to some of these factors have presented a greater obstacle than others, but existing work sheds little light on precisely how much each factor contributes to the generalization gap. Towards an answer to this question, we study imitation learning policies in simulation and on a real robot language-conditioned manipulation task to quantify the difficulty of generalization to different (sets of) factors. We also design a new simulated benchmark of 19 tasks with 11 factors of variation to facilitate more controlled evaluations of generalization. From our study, we determine an ordering of factors based on generalization difficulty, that is consistent across simulation and our real robot setup.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:è¿™ä¸ªé—®é¢˜æ˜¯éå¸¸Difficult to approach directly, because the environment from the perspective of a robot can often be decomposed intoå¤šç§å› ç´ çš„å˜åŒ–ï¼Œä¾‹å¦‚ç…§æ˜æ¡ä»¶æˆ–æ‘„åƒå¤´çš„ä½ç½®ã€‚éªŒè¯æ€§åœ°ï¼Œå¯¹ä¸€äº›è¿™äº›å› ç´ çš„æ³›åŒ–å‘ˆç°äº†æ›´å¤§çš„å›°éš¾ï¼Œä½†ç°æœ‰çš„å·¥ä½œå´æ²¡æœ‰æä¾›å…·ä½“å¦‚ä½•é‡åŒ–æ¯ä¸ªå› ç´ å¯¹æ³›åŒ–å·®è·çš„ä¿¡æ¯ã€‚ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ¨¡ä»¿å­¦ä¹ ç­–ç•¥åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººè¯­è¨€conditioned manipulationä»»åŠ¡ä¸­çš„æ³›åŒ–å›°éš¾ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªæ–°çš„æ¨¡æ‹Ÿbenchmarkï¼ŒåŒ…å«19ä¸ªä»»åŠ¡å’Œ11ä¸ªå› ç´ çš„å˜åŒ–ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¯„ä¼°æ³›åŒ–çš„æ§åˆ¶æ€§ã€‚ä»æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†å› ç´ çš„æ’åºï¼Œè¿™ä¸€ç»“æœåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººè®¾ç½®ä¸­å‡æ˜¯ä¸€è‡´çš„ã€‚
</details></li>
</ul>
<hr>
<h2 id="Discovering-Variable-Binding-Circuitry-with-Desiderata"><a href="#Discovering-Variable-Binding-Circuitry-with-Desiderata" class="headerlink" title="Discovering Variable Binding Circuitry with Desiderata"></a>Discovering Variable Binding Circuitry with Desiderata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03637">http://arxiv.org/abs/2307.03637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xander Davies, Max Nadeau, Nikhil Prakash, Tamar Rott Shaham, David Bau</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§æ–¹æ³•ï¼Œä»¥è‡ªåŠ¨åœ°å½’å› æ¨¡å‹ç»„ä»¶è´Ÿè´£æ‰§è¡Œç‰¹å®šå­ä»»åŠ¡çš„ causal  attributeã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº† causal mediation experiments æ¥è‡ªåŠ¨å½’å› æ¨¡å‹ç»„ä»¶ï¼Œå¹¶ä¸”åªéœ€è¦æŒ‡å®šæ¨¡å‹ç»„ä»¶æ‰§è¡Œå­ä»»åŠ¡çš„ causal attributeã€‚</li>
<li>results: ç ”ç©¶æˆæœæ˜¾ç¤ºï¼Œå¯ä»¥æˆåŠŸåœ°è‡ªåŠ¨å‘ç° LLama-13B æ¨¡å‹ä¸­çš„å…±äº«å˜é‡ç»‘å®šç”µè·¯ï¼Œå¹¶ä¸”åªéœ€è¦9ä¸ªæ³¨æ„å¤´å’Œ1ä¸ªMLPæ¥æ‰§è¡Œå¤šä¸ªæ•°å­¦ä»»åŠ¡ä¸­çš„å˜é‡ç»‘å®šã€‚<details>
<summary>Abstract</summary>
Recent work has shown that computation in language models may be human-understandable, with successful efforts to localize and intervene on both single-unit features and input-output circuits. Here, we introduce an approach which extends causal mediation experiments to automatically identify model components responsible for performing a specific subtask by solely specifying a set of \textit{desiderata}, or causal attributes of the model components executing that subtask. As a proof of concept, we apply our method to automatically discover shared \textit{variable binding circuitry} in LLaMA-13B, which retrieves variable values for multiple arithmetic tasks. Our method successfully localizes variable binding to only 9 attention heads (of the 1.6k) and one MLP in the final token's residual stream.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè®¡ç®—æœºè¯­è¨€æ¨¡å‹ä¸­çš„è®¡ç®—å¯èƒ½æ˜¯äººç±»ç†è§£çš„ï¼Œæœ‰æˆåŠŸçš„å°è¯•å°†å•å…ƒç‰¹å¾å’Œè¾“å…¥è¾“å‡ºç”µè·¯ lokalisiruiå’Œ interveneã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥è‡ªåŠ¨ç¡®å®šæ¨¡å‹ç»„ä»¶è´Ÿè´£æ‰§è¡Œç‰¹å®šå­ä»»åŠ¡ï¼Œåªéœ€æä¾›ä¸€ç»„ \textit{desiderata}ï¼Œæˆ–æ¨¡å‹ç»„ä»¶æ‰§è¡Œè¯¥å­ä»»åŠ¡çš„ causal ç‰¹å¾ã€‚ä½œä¸ºè¯æ˜ï¼Œæˆ‘ä»¬åº”ç”¨äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè‡ªåŠ¨å‘ç° LLama-13B ä¸­çš„å…±äº« \textit{å˜é‡ç»‘å®šCircuitry}ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä¸ºå¤šä¸ªæ•°å­¦ä»»åŠ¡è·å–å˜é‡å€¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°å°†å˜é‡ç»‘å®šLocalized to only 9 attention heads (of the 1.6k) and one MLP in the final token's residual stream.
</details></li>
</ul>
<hr>
<h2 id="Over-the-Air-Computation-in-OFDM-Systems-with-Imperfect-Channel-State-Information"><a href="#Over-the-Air-Computation-in-OFDM-Systems-with-Imperfect-Channel-State-Information" class="headerlink" title="Over-the-Air Computation in OFDM Systems with Imperfect Channel State Information"></a>Over-the-Air Computation in OFDM Systems with Imperfect Channel State Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05357">http://arxiv.org/abs/2307.05357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilong Chen, Huijun Xing, Jie Xu, Lexi Xu, Shuguang Cui<br>for:è¿™ä¸ªè®ºæ–‡ç ”ç©¶äº†åœ¨æ— çº¿ç”µé€šä¿¡ç³»ç»Ÿä¸­è¿›è¡Œç©ºä¸­è®¡ç®—ï¼ˆAirCompï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— çº¿ç”µä¿¡é“çŠ¶æ€ä¿¡æ¯ï¼ˆCSIï¼‰ä¸å‡†ç¡®æ—¶ï¼Œå¤šä¸ªå•antennaæ— çº¿è®¾å¤‡ï¼ˆWDï¼‰åŒæ—¶å‘å¤šantennaè®¿é—®ç‚¹ï¼ˆAPï¼‰ä¸Šä¼ uncodedä¿¡å·è¿›è¡Œåˆ†å¸ƒå¼åŠŸèƒ½è®¡ç®—ã€‚methods:åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸¤ç§enarioï¼šä¸€ç§æ˜¯æœ€å¤§åŒ–averageè®¡ç®—å¹³å‡æ–¹å·®ï¼ˆMSEï¼‰ï¼Œå¦ä¸€ç§æ˜¯æœ€å°åŒ–è®¡ç®—å¤±è´¥æ¦‚ç‡ï¼ˆoutage probabilityï¼‰ã€‚ä¸ºäº†å®ç°è¿™ä¸¤ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬åŒæ—¶ä¼˜åŒ–äº†WDså‘å°„å™¨å’ŒAPæ¥æ”¶æ‰«æå™¨åœ¨å­è½½æ³¢ä¸Šçš„ä¼ è¾“ç³»æ•°å’Œæ¥æ”¶æ‰«æå™¨ã€‚results:æˆ‘ä»¬åœ¨è¿™ç¯‡è®ºæ–‡ä¸­æå‡ºäº†ä¸¤ç§ç‰¹æ®Šæƒ…å†µçš„è§£ï¼šä¸€ç§æ˜¯å•ä¸ªAPæ¥æ”¶å¤©çº¿çš„æƒ…å†µï¼Œå¦ä¸€ç§æ˜¯å¤šä¸ªAPæ¥æ”¶å¤©çº¿çš„æƒ…å†µã€‚åœ¨å•ä¸ªAPæ¥æ”¶å¤©çº¿æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ Lagrange-duality æ–¹æ³•æå‡ºäº†åŠé—­å½¢ globally ä¼˜åŒ–è§£ã€‚åœ¨å¤šä¸ªAPæ¥æ”¶å¤©çº¿æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ•ˆçš„ alternate ä¼˜åŒ–å’Œå‡ ä½•ä¼˜åŒ–ç®—æ³•æ¥æ‰¾åˆ° converges è§£ã€‚<details>
<summary>Abstract</summary>
This paper studies the over-the-air computation (AirComp) in an orthogonal frequency division multiplexing (OFDM) system with imperfect channel state information (CSI), in which multiple single-antenna wireless devices (WDs) simultaneously send uncoded signals to a multi-antenna access point (AP) for distributed functional computation over multiple subcarriers. In particular, we consider two scenarios with best-effort and error-constrained computation tasks, with the objectives of minimizing the average computation mean squared error (MSE) and the computation outage probability over the multiple subcarriers, respectively. Towards this end, we jointly optimize the transmit coefficients at the WDs and the receive beamforming vectors at the AP over subcarriers, subject to the maximum transmit power constraints at individual WDs. First, for the special case with a single receive antenna at the AP, we propose the semi-closed-form globally optimal solutions to the two problems using the Lagrange-duality method. It is shown that at each subcarrier, the WDs' optimized power control policy for average MSE minimization follows a regularized channel inversion structure, while that for computation outage probability minimization follows an on-off regularized channel inversion, with the regularization dependent on the transmit power budget and channel estimation error. Next, for the general case with multiple receive antennas at the AP, we present efficient algorithms based on alternating optimization and convex optimization to find converged solutions to both problems.
</details>
<details>
<summary>æ‘˜è¦</summary>
For the special case with a single receive antenna at the AP, we propose semi-closed-form globally optimal solutions to the two problems using the Lagrange-duality method. The results show that at each subcarrier, the WDs' optimized power control policy for average MSE minimization follows a regularized channel inversion structure, while that for computation outage probability minimization follows an on-off regularized channel inversion, with the regularization dependent on the transmit power budget and channel estimation error.For the general case with multiple receive antennas at the AP, we present efficient algorithms based on alternating optimization and convex optimization to find converged solutions to both problems. These algorithms take into account the coupling between the transmit coefficients and the receive beamforming vectors, and the non-convexity of the optimization problems.In summary, this paper investigates the optimization of AirComp in an OFDM system with imperfect CSI, and proposes algorithms to minimize the average MSE and computation outage probability over multiple subcarriers. The proposed solutions take into account the maximum transmit power constraints and the coupling between the transmit coefficients and the receive beamforming vectors.
</details></li>
</ul>
<hr>
<h2 id="Brain-in-a-Vat-On-Missing-Pieces-Towards-Artificial-General-Intelligence-in-Large-Language-Models"><a href="#Brain-in-a-Vat-On-Missing-Pieces-Towards-Artificial-General-Intelligence-in-Large-Language-Models" class="headerlink" title="Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models"></a>Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03762">http://arxiv.org/abs/2307.03762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxi Ma, Chi Zhang, Song-Chun Zhu</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†æ¢è®¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°æ–¹æ³•å’Œäººå·¥é€šç”¨æ™ºèƒ½çš„å®šä¹‰ã€‚</li>
<li>methods: è®ºæ–‡é¦–å…ˆå¯¹ç°æœ‰çš„LLMè¯„ä¼°æ–¹æ³•è¿›è¡Œäº†å…¨é¢å›é¡¾ï¼Œå¹¶æŒ‡å‡ºäº†è¯„ä¼°æ–¹æ³•ä¸­çš„ä¸€äº›é—®é¢˜ï¼Œè¿™äº›é—®é¢˜ä¼šå¯¼è‡´LLMçš„èƒ½åŠ›è¢«è¿‡åˆ†è¯„ä¼°ã€‚ç„¶åï¼Œæ–‡ç« æå‡ºäº†äººå·¥é€šç”¨æ™ºèƒ½åº”åŒ…å«ä»¥ä¸‹å››ä¸ªç‰¹å¾ï¼š1ï¼‰å¯ä»¥å®Œæˆæ— æ•°é‡çš„ä»»åŠ¡ï¼›2ï¼‰å¯ä»¥åœ¨ Contextä¸­ç”Ÿæˆæ–°ä»»åŠ¡ï¼›3ï¼‰åŸºäºå€¼ç³»ç»Ÿæ¥ç”Ÿæˆä»»åŠ¡ï¼›4ï¼‰å…·æœ‰åŸºäºç°å®çš„ä¸–ç•Œæ¨¡å‹ï¼Œè¿™ç§ä¸–ç•Œæ¨¡å‹å½±å“äº†å®ƒä¸ä¸–ç•Œçš„äº¤äº’ã€‚</li>
<li>results: æ–‡ç« è®¤ä¸ºï¼Œç°æœ‰çš„äººå·¥æ™ºèƒ½ç ”ç©¶ä»…ä»…æ˜¯æ¨¡æ‹Ÿæ™ºèƒ½ï¼Œè€Œä¸æ˜¯çœŸæ­£çš„é€šç”¨æ™ºèƒ½ã€‚å®ƒä»¬ç¼ºä¹äº†çŸ¥è¯†è·å¾—å’Œè¡Œä¸ºçš„ä¸€ä½“åŒ–ï¼Œè€Œä¸”çŸ¥è¯†è·å¾—ä¸ä»…ä»…é  passive inputï¼Œè¿˜éœ€è¦é‡å¤çš„å°è¯•å’Œé”™è¯¯ã€‚æ–‡ç« ç»“æŸæ—¶ï¼Œæå‡ºäº†äººå·¥æ™ºèƒ½æœªæ¥ç ”ç©¶çš„å¯èƒ½æ€§ã€‚<details>
<summary>Abstract</summary>
In this perspective paper, we first comprehensively review existing evaluations of Large Language Models (LLMs) using both standardized tests and ability-oriented benchmarks. We pinpoint several problems with current evaluation methods that tend to overstate the capabilities of LLMs. We then articulate what artificial general intelligence should encompass beyond the capabilities of LLMs. We propose four characteristics of generally intelligent agents: 1) they can perform unlimited tasks; 2) they can generate new tasks within a context; 3) they operate based on a value system that underpins task generation; and 4) they have a world model reflecting reality, which shapes their interaction with the world. Building on this viewpoint, we highlight the missing pieces in artificial general intelligence, that is, the unity of knowing and acting. We argue that active engagement with objects in the real world delivers more robust signals for forming conceptual representations. Additionally, knowledge acquisition isn't solely reliant on passive input but requires repeated trials and errors. We conclude by outlining promising future research directions in the field of artificial general intelligence.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è§‚ç‚¹è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†æ¶µç›–ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°çš„å…¨é¢å®¡æŸ¥ï¼Œä½¿ç”¨æ ‡å‡†åŒ–æµ‹è¯•å’Œèƒ½åŠ›å°ºåº¦æ ‡å‡†ã€‚æˆ‘ä»¬æŒ‡å‡ºäº†ç°æœ‰è¯„ä¼°æ–¹æ³•å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå¯¼è‡´LLMçš„èƒ½åŠ›è¢«è¿‡åº¦è¯„ä¼°ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯¦ç»†è¯´æ˜äº†äººå·¥æ€»æ™ºèƒ½åº”åŒ…æ‹¬ä»¥ä¸‹å››ä¸ªç‰¹ç‚¹ï¼š1ï¼‰å¯ä»¥å®Œæˆæ— æ•°é¡¹ä»»åŠ¡ï¼›2ï¼‰å¯ä»¥åœ¨ Context ä¸­ç”Ÿæˆæ–°ä»»åŠ¡ï¼›3ï¼‰åŸºäºå€¼ç³»ç»Ÿæ¥å†³å®šä»»åŠ¡ç”Ÿæˆï¼›4ï¼‰å…·æœ‰å¯¹å®é™…ä¸–ç•Œçš„è®¤çŸ¥ï¼Œå½±å“å…¶ä¸ä¸–ç•Œçš„äº’åŠ¨ã€‚åŸºäºè¿™ç§è§†è§’ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†äººå·¥æ€»æ™ºèƒ½ç¼ºå¤±çš„ä¸€éƒ¨åˆ†ï¼Œå³çŸ¥è¯†å’Œè¡Œä¸ºçš„ä¸€ä½“æ€§ã€‚æˆ‘ä»¬ argued That active engagement with objects in the real world provides more robust signals for forming conceptual representations. In addition, knowledge acquisition is not solely reliant on passive input, but requires repeated trials and errors. Finally, we outline promising future research directions in the field of artificial general intelligence.
</details></li>
</ul>
<hr>
<h2 id="GEANN-Scalable-Graph-Augmentations-for-Multi-Horizon-Time-Series-Forecasting"><a href="#GEANN-Scalable-Graph-Augmentations-for-Multi-Horizon-Time-Series-Forecasting" class="headerlink" title="GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting"></a>GEANN: Scalable Graph Augmentations for Multi-Horizon Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03595">http://arxiv.org/abs/2307.03595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sitan Yang, Malcolm Wolff, Shankar Ramasubramanian, Vincent Quenneville-Belair, Ronak Metha, Michael W. Mahoney</li>
<li>for: è§£å†³â€œå†·å¯â€æ—¶é—´åºåˆ—é¢„æµ‹é—®é¢˜ï¼Œå³é¢„æµ‹ç¼ºä¹å†å²æ•°æ®çš„æ—¶é—´åºåˆ—ã€‚</li>
<li>methods: åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä½œä¸ºç¼–ç å™¨å¢å¼ºå™¨ï¼Œé€šè¿‡ç”ŸæˆGNNåŸºäºçš„ç‰¹å¾æ¥æ•æ‰æ—¶é—´åºåˆ—ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚</li>
<li>results: åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯¹ä¸€å®¶å¤§å‹ç”µå•†å•†æˆ·çš„éœ€æ±‚é¢„æµ‹ task ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æé«˜æ€»è¡¨ç°ï¼Œå¹¶æ›´é‡è¦çš„æ˜¯ï¼Œå¯¹â€œå†·å¯â€äº§å“ï¼ˆæ–°ä¸Šå¸‚æˆ–è€…åˆšä¸‹æ¶ï¼‰çš„é¢„æµ‹å¸¦æ¥æ˜¾è‘—æ”¹å–„ã€‚<details>
<summary>Abstract</summary>
Encoder-decoder deep neural networks have been increasingly studied for multi-horizon time series forecasting, especially in real-world applications. However, to forecast accurately, these sophisticated models typically rely on a large number of time series examples with substantial history. A rapidly growing topic of interest is forecasting time series which lack sufficient historical data -- often referred to as the ``cold start'' problem. In this paper, we introduce a novel yet simple method to address this problem by leveraging graph neural networks (GNNs) as a data augmentation for enhancing the encoder used by such forecasters. These GNN-based features can capture complex inter-series relationships, and their generation process can be optimized end-to-end with the forecasting task. We show that our architecture can use either data-driven or domain knowledge-defined graphs, scaling to incorporate information from multiple very large graphs with millions of nodes. In our target application of demand forecasting for a large e-commerce retailer, we demonstrate on both a small dataset of 100K products and a large dataset with over 2 million products that our method improves overall performance over competitive baseline models. More importantly, we show that it brings substantially more gains to ``cold start'' products such as those newly launched or recently out-of-stock.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å¤šä¸ªæ—¶é—´æ°´å¹³é¢„æµ‹æ–¹é¢å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å®é™…åº”ç”¨ä¸­ã€‚ç„¶è€Œï¼Œä¸ºäº†å‡†ç¡®é¢„æµ‹ï¼Œè¿™äº›å¤æ‚çš„æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„æ—¶é—´åºåˆ—ç¤ºä¾‹ï¼Œå…¶ä¸­å…·æœ‰å……åˆ†çš„å†å²è®°å½•ã€‚ä¸€ä¸ªè¿…é€Ÿå¢é•¿çš„ç ”ç©¶é¢†åŸŸæ˜¯ç¼ºå°‘å†å²æ•°æ®çš„æ—¶é—´åºåˆ—é¢„æµ‹é—®é¢˜ï¼Œé€šå¸¸è¢«ç§°ä¸ºâ€œå†·å¼€å§‹â€é—®é¢˜ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„ã€ç®€å•çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä½œä¸ºç¼–ç å™¨å¢å¼ºå™¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¿™äº›GNNåŸºäºçš„ç‰¹å¾å¯ä»¥æ•æ‰åˆ°æ—¶é—´åºåˆ—ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œå¹¶ä¸”å…¶ç”Ÿæˆè¿‡ç¨‹å¯ä»¥ä¸é¢„æµ‹ä»»åŠ¡ç»“åˆoptimizedã€‚æˆ‘ä»¬ç¤ºå‡ºäº†æˆ‘ä»¬çš„æ¶æ„å¯ä»¥ä½¿ç”¨æ•°æ®é©±åŠ¨æˆ–åŸŸçŸ¥è¯†å®šä¹‰çš„å›¾ï¼Œå¯æ¶µç›–å¤šä¸ªå…·æœ‰ç™¾ä¸‡ä¸ªèŠ‚ç‚¹çš„å›¾ã€‚åœ¨æˆ‘ä»¬çš„ç›®æ ‡åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬åœ¨10ä¸‡ä¸ªäº§å“çš„å°æ•°æ®é›†å’Œè¶…è¿‡2ä¸‡ä¸ªäº§å“çš„å¤§æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨æ¯”è¾ƒåŸºelineæ¨¡å‹çš„æƒ…å†µä¸‹æä¾›æ›´å¥½çš„æ€»ä½“æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æ–¹æ³•å¯¹â€œå†·å¼€å§‹â€äº§å“ï¼ˆå¦‚æ–°ä¸Šå¸‚æˆ–è€…åˆšå‡ºåº“ï¼‰çš„é¢„æµ‹å…·æœ‰æ˜¾è‘—çš„æ”¹å–„ã€‚
</details></li>
</ul>
<hr>
<h2 id="VesselVAE-Recursive-Variational-Autoencoders-for-3D-Blood-Vessel-Synthesis"><a href="#VesselVAE-Recursive-Variational-Autoencoders-for-3D-Blood-Vessel-Synthesis" class="headerlink" title="VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis"></a>VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03592">http://arxiv.org/abs/2307.03592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paula Feldman, Miguel Fainstein, Viviana Siless, Claudio Delrieux, Emmanuel Iarussi</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯ä¸ºäº† Synthesizing blood vessel 3D geometry, å³ç”Ÿæˆè¡€ç®¡ä¸‰ç»´å‡ ä½•ç»“æ„ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨çš„æ–¹æ³•æ˜¯ recursive variational Neural Network (VesselVAE)ï¼Œå®ƒå¯ä»¥å®Œå…¨åˆ©ç”¨è¡€ç®¡çš„å±‚æ¬¡ç»“æ„ï¼Œå­¦ä¹ ä½ç»´æŠ½è±¡è¡¨ç¤ºåˆ†æ”¯è¿æ¥æ€§ä»¥åŠè¡¨ç¤ºç›®æ ‡è¡¨é¢çš„å‡ ä½•ç‰¹å¾ã€‚</li>
<li>results: è¯¥è®ºæ–‡çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVesselVAEå¯ä»¥ç”Ÿæˆé«˜åº¦å‡†ç¡®å’Œå¤šæ ·åŒ–çš„è¡€ç®¡ä¸‰ç»´æ¨¡å‹ï¼Œå¹¶ä¸”ä¸å®é™…æ•°æ®çš„ç›¸ä¼¼æ€§è¾¾åˆ°äº†&#x2F;.97ã€&#x2F;.95å’Œ&#x2F;.96ä¸‰ä¸ªæŒ‡æ ‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒVesselVAEå¯ä»¥ç”¨äºåŒ»ç–—å’Œæ‰‹æœ¯è®­ç»ƒã€è¡€æ¶²åŠ¨åŠ›å­¦ simulations ç­‰å¤šç§ç›®çš„ã€‚<details>
<summary>Abstract</summary>
We present a data-driven generative framework for synthesizing blood vessel 3D geometry. This is a challenging task due to the complexity of vascular systems, which are highly variating in shape, size, and structure. Existing model-based methods provide some degree of control and variation in the structures produced, but fail to capture the diversity of actual anatomical data. We developed VesselVAE, a recursive variational Neural Network that fully exploits the hierarchical organization of the vessel and learns a low-dimensional manifold encoding branch connectivity along with geometry features describing the target surface. After training, the VesselVAE latent space can be sampled to generate new vessel geometries. To the best of our knowledge, this work is the first to utilize this technique for synthesizing blood vessels. We achieve similarities of synthetic and real data for radius (.97), length (.95), and tortuosity (.96). By leveraging the power of deep neural networks, we generate 3D models of blood vessels that are both accurate and diverse, which is crucial for medical and surgical training, hemodynamic simulations, and many other purposes.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ•°æ®çš„ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºsynthesizingè¡€ç®¡ä¸‰ç»´å‡ ä½•ç»“æ„ã€‚è¿™æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºè¡€æ¶²ç³»ç»Ÿçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§å¾ˆé«˜ï¼Œå®ƒä»¬çš„å½¢æ€ã€å¤§å°å’Œç»“æ„å„ä¸ç›¸åŒã€‚ç°æœ‰çš„æ¨¡å‹åŸºæœ¬æ–¹æ³•å¯ä»¥æä¾›ä¸€å®šçš„æ§åˆ¶å’Œå˜åŒ–ï¼Œä½†æ˜¯æ— æ³•æ•æ‰å®é™…ç”Ÿç‰©å­¦æ•°æ®çš„å¤šæ ·æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†VesselVAEï¼Œä¸€ç§åµŒå…¥å¼çš„å¯å˜é‡ç¥ç»ç½‘ç»œï¼Œå®ƒå®Œå…¨åˆ©ç”¨è¡€ç®¡çš„å±‚æ¬¡ç»“æ„ï¼Œå­¦ä¹ ä½ç»´åº¦æŠ½è±¡è¡¨ç¤ºåˆ†æ”¯è¿æ¥ä»¥åŠè¡¨é¢ç‰¹å¾ï¼Œæè¿°ç›®æ ‡è¡¨é¢çš„å‡ ä½•ç‰¹å¾ã€‚ç»è¿‡è®­ç»ƒï¼ŒVesselVAEçš„å¹‚æ•°ç©ºé—´å¯ä»¥é‡‡æ ·æ–°çš„è¡€ç®¡å‡ ä½•ç»“æ„ã€‚æ ¹æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€æ¬¡åˆ©ç”¨è¿™ç§æŠ€æœ¯æ¥ç”Ÿæˆè¡€ç®¡ã€‚æˆ‘ä»¬å®ç°äº†çœŸå®æ•°æ®å’Œç”Ÿæˆæ•°æ®ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ˆ.97ï¼‰ï¼Œï¼ˆ.95ï¼‰å’Œï¼ˆ.96ï¼‰ã€‚é€šè¿‡åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œçš„åŠ›é‡ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†å‡†ç¡®ä¸”å¤šæ ·çš„è¡€ç®¡ä¸‰ç»´æ¨¡å‹ï¼Œè¿™å¯¹åŒ»ç–—å’Œæ‰‹æœ¯åŸ¹è®­ã€è¡€æ¶²åŠ¨åŠ›å­¦è®¡ç®—ä»¥åŠè®¸å¤šå…¶ä»–ç›®çš„éƒ½æ˜¯å…³é”®ã€‚
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Deep-Learning-for-Personalized-Renal-Cell-Carcinoma-Prognosis-Integrating-CT-Imaging-and-Clinical-Data"><a href="#Multimodal-Deep-Learning-for-Personalized-Renal-Cell-Carcinoma-Prognosis-Integrating-CT-Imaging-and-Clinical-Data" class="headerlink" title="Multimodal Deep Learning for Personalized Renal Cell Carcinoma Prognosis: Integrating CT Imaging and Clinical Data"></a>Multimodal Deep Learning for Personalized Renal Cell Carcinoma Prognosis: Integrating CT Imaging and Clinical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03575">http://arxiv.org/abs/2307.03575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mahootiha-maryam/Survival_CTplusClinical">https://github.com/mahootiha-maryam/Survival_CTplusClinical</a></li>
<li>paper_authors: Maryamalsadat Mahootiha, Hemin Ali Qadir, Jacob Bergsland, Ilangko Balasingham<br>for:è¿™é¡¹ç ”ç©¶çš„ç›®çš„æ˜¯å¼€å‘ä¸€ä¸ªå…¨é¢çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹renoocellular carcinomaæ‚£è€…çš„ç”Ÿå­˜å¯èƒ½æ€§ï¼Œé€šè¿‡ç»“åˆCTæˆåƒå’Œä¸´åºŠæ•°æ®ï¼Œå¹¶è§£å†³è¿‡å»ç ”ç©¶ä¸­å‡ºç°çš„å±€é™æ€§ã€‚methods:è¯¥ç ”ç©¶æposedä¸€ä¸ªæ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼š3Då›¾åƒç‰¹å¾æå–å™¨ã€ä¸´åºŠå˜é‡é€‰æ‹©å’Œç”Ÿå­˜é¢„æµ‹ã€‚å›¾åƒç‰¹å¾æå–å™¨æ¨¡å—åŸºäº3D CNNæ¶æ„ï¼Œé¢„æµ‹CTæˆåƒä¸­renoocellular carcinomaè‚¿ç˜¤çš„ISUPåˆ†æœŸï¼Œä¸æ­»äº¡ç‡ç›¸å…³ã€‚ä¸´åºŠå˜é‡é€‰æ‹©ä½¿ç”¨Spearmanåˆ†æ•°å’ŒRandom Foresté‡è¦æ€§åˆ†æ•°ä½œä¸ºæ ‡å‡†ï¼Œç³»ç»Ÿåœ°é€‰æ‹©ä¸´åºŠå˜é‡ã€‚ç”Ÿå­˜é¢„æµ‹ä½¿ç”¨æ·±åº¦å­¦ä¹ ç½‘ç»œï¼Œä»¥Discrete LogisticHazard-basedæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚results:æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œæå‡ºçš„ç­–ç•¥è¶…è¿‡äº†å½“å‰renoocellular carcinomaé¢„æµ‹Literatureä¸­åŸºäºCTæˆåƒå’Œä¸´åºŠå› ç´ çš„ç ”ç©¶ã€‚æœ€ä½³å®éªŒåœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº† concordance index 0.84å’Œarea under the curve 0.8 çš„æ°´å¹³ï¼Œè¿™è¡¨æ˜äº†è¯¥æ–¹æ³•åœ¨é¢„æµ‹renoocellular carcinomaæ‚£è€…çš„ç”Ÿå­˜å¯èƒ½æ€§æ–¹é¢å…·æœ‰å¼ºå¤§çš„é¢„æµ‹åŠ›ã€‚<details>
<summary>Abstract</summary>
Renal cell carcinoma represents a significant global health challenge with a low survival rate. This research aimed to devise a comprehensive deep-learning model capable of predicting survival probabilities in patients with renal cell carcinoma by integrating CT imaging and clinical data and addressing the limitations observed in prior studies. The aim is to facilitate the identification of patients requiring urgent treatment. The proposed framework comprises three modules: a 3D image feature extractor, clinical variable selection, and survival prediction. The feature extractor module, based on the 3D CNN architecture, predicts the ISUP grade of renal cell carcinoma tumors linked to mortality rates from CT images. A selection of clinical variables is systematically chosen using the Spearman score and random forest importance score as criteria. A deep learning-based network, trained with discrete LogisticHazard-based loss, performs the survival prediction. Nine distinct experiments are performed, with varying numbers of clinical variables determined by different thresholds of the Spearman and importance scores. Our findings demonstrate that the proposed strategy surpasses the current literature on renal cancer prognosis based on CT scans and clinical factors. The best-performing experiment yielded a concordance index of 0.84 and an area under the curve value of 0.8 on the test cohort, which suggests strong predictive power. The multimodal deep-learning approach developed in this study shows promising results in estimating survival probabilities for renal cell carcinoma patients using CT imaging and clinical data. This may have potential implications in identifying patients who require urgent treatment, potentially improving patient outcomes. The code created for this project is available for the public on: \href{https://github.com/Balasingham-AI-Group/Survival_CTplusClinical}{GitHub}
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œè‚¾ç»†èƒç™Œ represents a significant global health challenge with a low survival rate. This research aimed to develop a comprehensive deep-learning model capable of predicting survival probabilities in patients with renal cell carcinoma by integrating CT imaging and clinical data, and addressing the limitations observed in prior studies. The aim is to facilitate the identification of patients requiring urgent treatment. The proposed framework consists of three modules: a 3D image feature extractor, clinical variable selection, and survival prediction. The feature extractor module, based on the 3D CNN architecture, predicts the ISUP grade of renal cell carcinoma tumors linked to mortality rates from CT images. A selection of clinical variables is systematically chosen using the Spearman score and random forest importance score as criteria. A deep learning-based network, trained with discrete LogisticHazard-based loss, performs the survival prediction. Nine distinct experiments were performed, with varying numbers of clinical variables determined by different thresholds of the Spearman and importance scores. Our findings demonstrate that the proposed strategy surpasses the current literature on renal cancer prognosis based on CT scans and clinical factors. The best-performing experiment yielded a concordance index of 0.84 and an area under the curve value of 0.8 on the test cohort, which suggests strong predictive power. The multimodal deep-learning approach developed in this study shows promising results in estimating survival probabilities for renal cell carcinoma patients using CT imaging and clinical data. This may have potential implications in identifying patients who require urgent treatment, potentially improving patient outcomes. The code created for this project is available for the public on GitHub.â€Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Why-machines-do-not-understand-A-response-to-Sogaard"><a href="#Why-machines-do-not-understand-A-response-to-Sogaard" class="headerlink" title="Why machines do not understand: A response to SÃ¸gaard"></a>Why machines do not understand: A response to SÃ¸gaard</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04766">http://arxiv.org/abs/2307.04766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jobst Landgrebe, Barry Smith</li>
<li>for: æœ¬æ–‡é’ˆå¯¹ä¸€äº›äººè®¤ä¸ºæœºå™¨äººå¯ä»¥ç†è§£è¯­è¨€çš„è§‚ç‚¹è¿›è¡Œæ‰¹åˆ¤ï¼Œå…·ä½“æ¥è¯´æ˜¯å…³äºç´¢åŠ å¾·ï¼ˆSogaardï¼‰åœ¨è¿™æœ¬æ‚å¿—ä¸Šæå‡ºçš„ä¸€ç§è¿™æ ·çš„thesisï¼ŒåŸºäºè¯­è¨€å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„æ¦‚å¿µã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†å¯¹ç´¢åŠ å¾·çš„è®ºç‚¹è¿›è¡Œåˆ†æå’Œæ‰¹åˆ¤çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å¯¹è¯­è¨€çš„ä½¿ç”¨å’Œå­˜å‚¨æ–¹å¼çš„åˆ†æï¼Œä»¥åŠå¯¹æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½çš„æ‰¹åˆ¤ã€‚</li>
<li>results: æœ¬æ–‡è¡¨æ˜äº†ç´¢åŠ å¾·çš„è®ºç‚¹å­˜åœ¨é—®é¢˜ï¼Œä¸»è¦æ˜¯å› ä¸ºä»–å¿½è§†äº†äººç±»è¯­è¨€ä½¿ç”¨å’Œè®¡ç®—æœºè¯­è¨€å­˜å‚¨çš„åŒºåˆ«ï¼Œä»è€Œå¯¼è‡´äº†æœºå™¨äººç†è§£è¯­è¨€çš„å›°éš¾ã€‚<details>
<summary>Abstract</summary>
Some defenders of so-called `artificial intelligence' believe that machines can understand language. In particular, S{\o}gaard has argued in this journal for a thesis of this sort, on the basis of the idea (1) that where there is semantics there is also understanding and (2) that machines are not only capable of what he calls `inferential semantics', but even that they can (with the help of inputs from sensors) `learn' referential semantics \parencite{sogaard:2022}. We show that he goes wrong because he pays insufficient attention to the difference between language as used by humans and the sequences of inert of symbols which arise when language is stored on hard drives or in books in libraries.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸€äº›äººè®¤ä¸ºè®ºç§°çš„äººå·¥æ™ºèƒ½å¯ä»¥ç†è§£è¯­è¨€ã€‚ç‰¹åˆ«æ˜¯ï¼ŒS{\o}gaardåœ¨è¿™ä»½æŠ¥å‘Šä¸­æå‡ºäº†è¿™ç§thesisï¼ŒåŸºäºä¸¤ç‚¹ï¼šä¸€æ˜¯è¯­è¨€å­˜åœ¨ semantics å°±æ˜¯ç†è§£çš„ garantorï¼ˆ1ï¼‰ï¼ŒäºŒæ˜¯æœºå™¨ä¸ä»…å¯ä»¥è¿›è¡Œä»–æ‰€ç§°çš„â€œæ¨ç† semanticsâ€ï¼Œè€Œä¸”å¯ä»¥ï¼ˆé€šè¿‡æ„ŸçŸ¥å™¨çš„è¾“å…¥ï¼‰â€œå­¦ä¹ â€ referential semanticsï¼ˆ\parencite{sogaard:2022ï¼‰ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä»–çš„é”™è¯¯æ˜¯å› ä¸ºä»–å¿½è§†äº†äººç±»ä½¿ç”¨è¯­è¨€å’Œå­˜å‚¨åœ¨ç¡¬ç›˜æˆ–å›¾ä¹¦é¦†ä¸­çš„è¯­è¨€åºåˆ—çš„å·®å¼‚ã€‚
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Graph-Attention-for-Anomaly-Detection-in-Heterogeneous-Sensor-Networks"><a href="#Dynamic-Graph-Attention-for-Anomaly-Detection-in-Heterogeneous-Sensor-Networks" class="headerlink" title="Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks"></a>Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03761">http://arxiv.org/abs/2307.03761</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MengjieZhao/dygatad">https://github.com/MengjieZhao/dygatad</a></li>
<li>paper_authors: Mengjie Zhao, Olga Fink</li>
<li>for: æœ¬æ–‡é’ˆå¯¹çš„æ˜¯éšç€äº’è”ç½‘ Things (IIoTs) ç³»ç»Ÿä¸­çš„å¤šå˜é‡æ—¶é—´åºåˆ— (MTS) æ•°æ®çš„å¼‚å¸¸æ£€æµ‹ï¼Œå³ä½¿åœ¨æ„ŸçŸ¥å™¨ç½‘ç»œä¸­å­˜åœ¨å¤æ‚æ€§å’Œäº’ç›¸å…³ç³»çš„æƒ…å†µä¸‹ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº† DyGATAD (åŠ¨æ€å›¾æ³¨æ„åŠ›å¼‚å¸¸æ£€æµ‹) æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ„å»ºäº†å¤šå˜é‡æ—¶é—´åºåˆ—ä¸Šçš„è¿ç»­å›¾è¡¨ç¤ºï¼Œå¹¶é€šè¿‡æ¨æ–­åŠ¨æ€è¾¹æ¥æ£€æµ‹å…³ç³»å˜åŒ–ã€‚ DyGATAD è¿˜åŒ…æ‹¬äº†åŸºäºæ“ä½œæ¡ä»¶çš„é‡å»ºå’Œ topology åŸºäºå¼‚å¸¸åˆ†æ•°ï¼Œä»è€Œæé«˜äº†å¼‚å¸¸æ£€æµ‹çš„èƒ½åŠ›ã€‚</li>
<li>results: æ ¹æ®ä¸€ä¸ªæ§åˆ¶å˜é‡çš„ synthetic æ•°æ®é›†å’Œä¸€ä¸ªå®é™… industrials çš„å¤šç›¸æµè®¾å¤‡æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯æ˜äº† DyGATAD åœ¨æ„ŸçŸ¥å™¨ç½‘ç»œä¸­çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½éå¸¸é«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸç–¾ç—…æ£€æµ‹å’Œè½»åº¦ç–¾ç—…æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚<details>
<summary>Abstract</summary>
In the era of digital transformation, systems monitored by the Industrial Internet of Things (IIoTs) generate large amounts of Multivariate Time Series (MTS) data through heterogeneous sensor networks. While this data facilitates condition monitoring and anomaly detection, the increasing complexity and interdependencies within the sensor network pose significant challenges for anomaly detection. Despite progress in this field, much of the focus has been on point anomalies and contextual anomalies, with lesser attention paid to collective anomalies. A less addressed but common variant of collective anomalies is when the abnormal collective behavior is caused by shifts in interrelationships within the system. This can be due to abnormal environmental conditions like overheating, improper operational settings resulting from cyber-physical attacks, or system-level faults. To address these challenges, this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a graph-based anomaly detection framework that leverages the attention mechanism to construct a continuous graph representation of multivariate time series by inferring dynamic edges between time series. DyGATAD incorporates an operating condition-aware reconstruction combined with a topology-based anomaly score, thereby enhancing the detection ability of relationship shifts. We evaluate the performance of DyGATAD using both a synthetic dataset with controlled varying fault severity levels and an industrial-scale multiphase flow facility benchmark featuring various fault types with different detection difficulties. Our proposed approach demonstrated superior performance in collective anomaly detection for sensor networks, showing particular strength in early-stage fault detection, even in the case of faults with minimal severity.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨æ•°å­—å˜é©æ—¶ä»£ï¼Œç”±IIoTç³»ç»Ÿç›‘æµ‹çš„ç³»ç»Ÿç”Ÿæˆå¤§é‡å¤šå˜é‡æ—¶é—´åºåˆ—ï¼ˆMTSï¼‰æ•°æ®ï¼Œè¿™äº›æ•°æ®å¯ä»¥å¸®åŠ© condition monitoring å’Œå¼‚å¸¸æ£€æµ‹ã€‚ç„¶è€Œï¼Œéšç€ä¼ æ„Ÿå™¨ç½‘ç»œçš„å¤æ‚æ€§å’Œäº’ç›¸å…³ç³»çš„å¢åŠ ï¼Œå¼‚å¸¸æ£€æµ‹é‡åˆ°äº† significiant æŒ‘æˆ˜ã€‚è™½ç„¶åœ¨è¿™ä¸€é¢†åŸŸå·²ç»åšå‡ºäº†å¾ˆå¤šè¿›å±•ï¼Œä½†æ˜¯å¤§å¤šæ•°ç ”ç©¶éƒ½æ˜¯å…³æ³¨ç‚¹å¼‚å¸¸å’Œä¸Šä¸‹æ–‡å¼‚å¸¸ï¼Œè€Œå¿½ç•¥äº†é›†ä½“å¼‚å¸¸ã€‚è¿™æ˜¯ä¸€ç§è¾ƒå°‘åœ°ç ”ç©¶çš„ï¼Œä½†æ˜¯éå¸¸æ™®éçš„ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²å¼‚å¸¸æƒ…å†µï¼Œå³ä¼ æ„Ÿå™¨ç½‘ç»œä¸­çš„å¼‚å¸¸è¡Œä¸ºæ˜¯ç”±ç³»ç»Ÿé—´å…³ç³»çš„å˜åŒ–å¼•èµ·çš„ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºç¯å¢ƒæ¡ä»¶å¼‚å¸¸ã€æ“ä½œè®¾ç½®ä¸å½“æˆ–ç³»ç»Ÿçº§åˆ«çš„æ•…éšœæ‰€è‡´ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº† DyGATADï¼ˆåŠ¨æ€å›¾æ³¨æ„åŠ›æ£€æµ‹ï¼‰ï¼Œä¸€ç§åŸºäºå›¾çš„å¼‚å¸¸æ£€æµ‹æ¡†æ¶ã€‚DyGATAD åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥æ„å»ºå¤šå˜é‡æ—¶é—´åºåˆ—ä¸­çš„è¿ç»­å›¾è¡¨ç¤ºï¼Œå¹¶é€šè¿‡æ¨ç†å‡ºåŠ¨æ€è¾¹çš„æ–¹å¼æ¥æ•æ‰ç³»ç»Ÿé—´çš„å…³ç³»å˜åŒ–ã€‚DyGATAD è¿˜åŒ…æ‹¬äº†æ ¹æ®æ“ä½œæ¡ä»¶è¿›è¡Œä¿®æ­£çš„é‡æ„ï¼Œä»¥åŠåŸºäº topological å¼‚å¸¸åˆ†æ•°çš„æ£€æµ‹ï¼Œä»è€Œæé«˜äº†å¼‚å¸¸æ£€æµ‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹ä¸€ä¸ªåˆæˆæ•°æ®é›†å’Œä¸€ä¸ªå®é™…å·¥ä¸šçº§å¤šç›¸æµè®¾æ–½çš„æ•°æ®è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼ŒDyGATAD åœ¨ä¼ æ„Ÿå™¨ç½‘ç»œä¸­çš„é›†ä½“å¼‚å¸¸æ£€æµ‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆæœŸç–¾ç—…æ£€æµ‹ä¸­ï¼Œç”šè‡³æ˜¯åœ¨ç–¾ç—…ä¸¥é‡ç¨‹åº¦è¾ƒä½çš„æƒ…å†µä¸‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-as-Batteries-Included-Zero-Shot-ESCO-Skills-Matchers"><a href="#Large-Language-Models-as-Batteries-Included-Zero-Shot-ESCO-Skills-Matchers" class="headerlink" title="Large Language Models as Batteries-Included Zero-Shot ESCO Skills Matchers"></a>Large Language Models as Batteries-Included Zero-Shot ESCO Skills Matchers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03539">http://arxiv.org/abs/2307.03539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin ClaviÃ©, Guillaume SouliÃ©<br>for:è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ä¸ªé›¶ä¸Šä¸‹æµ‹è¯•çš„è‡ªåŠ¨æŠ€èƒ½æŠ½å‡ºç³»ç»Ÿï¼Œç”¨äºå¯¹é›‡ä½£å¹¿å‘Šä¸­çš„æŠ€èƒ½æŠ½å‡ºã€‚methods:è¿™ä¸ªç³»ç»Ÿä½¿ç”¨å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”ŸæˆSyntheticè®­ç»ƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªåˆ†ç±»å™¨æ¥ä»é›‡ä½£å¹¿å‘Šä¸­æå–æŠ€èƒ½æåŠã€‚ç„¶åä½¿ç”¨å¦ä¸€ä¸ªLLMè¿›è¡Œç›¸ä¼¼é¢„æµ‹ï¼Œä»¥é‡æ–°æ’åºæŠ€èƒ½å€™é€‰äººã€‚results:è¿™ç¯‡è®ºæ–‡çš„ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨åˆæˆæ•°æ®å¯ä»¥åœ¨æŠ€èƒ½æŠ½å‡º Ğ·Ğ°Ğ´Ğ°Ñ‡Ñ–ä¸­å–å¾—10ä¸ªRP@10åˆ†çš„é«˜åˆ†ï¼Œæ¯”å‰ä¸€äº›è·ç¦»æŒ‡å¯¼æ–¹æ³•é«˜å‡º10ä¸ªåˆ†ã€‚åŒæ—¶ï¼Œæ·»åŠ GPT-4é‡æ–°æ’åºå¯ä»¥æé«˜RP@10çš„è¡¨ç°ï¼Œé«˜äºå‰ä¸€äº›æ–¹æ³•çš„22ä¸ªåˆ†ã€‚æ­¤å¤–ï¼Œå°†ä»»åŠ¡æ¡†æ¶ä¸ºâ€œå‡ç¨‹å¼â€çš„æç¤ºï¼Œå¯ä»¥è®©LLMè¡¨ç°æ›´å¥½ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨è¾ƒå¼±çš„LLMã€‚<details>
<summary>Abstract</summary>
Understanding labour market dynamics requires accurately identifying the skills required for and possessed by the workforce. Automation techniques are increasingly being developed to support this effort. However, automatically extracting skills from job postings is challenging due to the vast number of existing skills. The ESCO (European Skills, Competences, Qualifications and Occupations) framework provides a useful reference, listing over 13,000 individual skills. However, skills extraction remains difficult and accurately matching job posts to the ESCO taxonomy is an open problem. In this work, we propose an end-to-end zero-shot system for skills extraction from job descriptions based on large language models (LLMs). We generate synthetic training data for the entirety of ESCO skills and train a classifier to extract skill mentions from job posts. We also employ a similarity retriever to generate skill candidates which are then re-ranked using a second LLM. Using synthetic data achieves an RP@10 score 10 points higher than previous distant supervision approaches. Adding GPT-4 re-ranking improves RP@10 by over 22 points over previous methods. We also show that Framing the task as mock programming when prompting the LLM can lead to better performance than natural language prompts, especially with weaker LLMs. We demonstrate the potential of integrating large language models at both ends of skills matching pipelines. Our approach requires no human annotations and achieve extremely promising results on skills extraction against ESCO.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç†è§£åŠ³åŠ¨å¸‚åœºåŠ¨æ€éœ€è¦å‡†ç¡®åœ°ç¡®å®šå·¥ä½œäººå‘˜æ‰€éœ€å’Œæ‹¥æœ‰çš„æŠ€èƒ½ã€‚è‡ªåŠ¨åŒ–æŠ€æœ¯åœ¨æ”¯æŒè¿™ä¸€åŠªåŠ›æ–¹é¢å‘å±•å¾—è¶Šæ¥è¶Šå¥½ã€‚ç„¶è€Œï¼Œä»å·¥ä½œå²—postsä¸­è‡ªåŠ¨æå–æŠ€èƒ½æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨åºå¤§çš„æŠ€èƒ½æ•°é‡ã€‚æ¬§æ´²æŠ€èƒ½ã€COMPETENCESã€èµ„æ ¼å’ŒèŒä¸šï¼ˆESCOï¼‰æ¡†æ¶æä¾›äº†æœ‰ç”¨çš„å‚è€ƒï¼Œåˆ—å‡ºäº†13,000å¤šä¸ªå…·ä½“çš„æŠ€èƒ½ã€‚ç„¶è€Œï¼ŒæŠ€èƒ½æå–ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¹¶ä¸”å‡†ç¡®åŒ¹é…å·¥ä½œå²—postsåˆ°ESCOåˆ†ç±»æ˜¯ä¸€ä¸ªæ‰“å¼€çš„é—®é¢˜ã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æè®®ä¸€ç§ç»ˆç«¯é›¶æ‰¹é‡ç³»ç»Ÿï¼Œä½¿ç”¨å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡ŒæŠ€èƒ½æå–ä»å·¥ä½œå²—postsã€‚æˆ‘ä»¬ç”Ÿæˆäº†ESCOæŠ€èƒ½æ•´ä½“çš„åˆæˆè®­ç»ƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªåˆ†ç±»å™¨æå–æŠ€èƒ½æåŠä»å·¥ä½œå²—postsã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç›¸ä¼¼æœç´¢å™¨ç”ŸæˆæŠ€èƒ½å€™é€‰äººé€‰ï¼Œç„¶åä½¿ç”¨ç¬¬äºŒä¸ªLLMè¿›è¡Œé‡æ–°æ’åºã€‚ä½¿ç”¨åˆæˆæ•°æ®å®ç°RP@10åˆ†æ•°10ç‚¹é«˜äºå‰ä¸€ç§è¿œç¨‹æŒ‡å¯¼æ–¹æ³•ã€‚å¦å¤–ï¼Œæ·»åŠ GPT-4é‡æ–°æ’åºå¯ä»¥æé«˜RP@10åˆ†æ•°22ç‚¹ä»¥ä¸Šã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œå°†ä»»åŠ¡framä¸ºMockç¼–ç¨‹æ—¶è¯·æ±‚LLMçš„æç¤ºå¯ä»¥æé«˜æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨è¾ƒå¼±çš„LLMã€‚æˆ‘ä»¬å±•ç¤ºäº†å°†å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ integrateåˆ°æŠ€èƒ½åŒ¹é…ç®¡é“çš„æ½œåœ¨ä¼˜åŠ¿ï¼Œå¹¶å®ç°äº†æ— éœ€äººå·¥æ ‡æ³¨çš„æŠ€èƒ½æå– Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²ESCOã€‚
</details></li>
</ul>
<hr>
<h2 id="Physical-Color-Calibration-of-Digital-Pathology-Scanners-for-Robust-Artificial-Intelligence-Assisted-Cancer-Diagnosis"><a href="#Physical-Color-Calibration-of-Digital-Pathology-Scanners-for-Robust-Artificial-Intelligence-Assisted-Cancer-Diagnosis" class="headerlink" title="Physical Color Calibration of Digital Pathology Scanners for Robust Artificial Intelligence Assisted Cancer Diagnosis"></a>Physical Color Calibration of Digital Pathology Scanners for Robust Artificial Intelligence Assisted Cancer Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05519">http://arxiv.org/abs/2307.05519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyi Ji, Richard Salmon, Nita Mulliqi, Umair Khan, Yinxi Wang, Anders Blilie, Henrik Olsson, Bodil Ginnerup Pedersen, Karina Dalsgaard SÃ¸rensen, Benedicte Parm UlhÃ¸i, Svein R Kjosavik, Emilius AM Janssen, Mattias Rantalainen, Lars Egevad, Pekka Ruusuvuori, Martin Eklund, Kimmo Kartasalo</li>
<li>for: è¿™é¡¹ç ”ç©¶æ—¨åœ¨è§£å†³æ•°ä½patologyä¸­äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„æ½œåŠ›å—åˆ°æŠ€æœ¯ä¸ä¸€è‡´çš„æŠ‘åˆ¶ï¼Œä»è€Œä½¿AIåœ¨ä¸´åºŠåº”ç”¨ä¸­å—åˆ°æŒ‘æˆ˜ã€‚</li>
<li>methods: ç ”ç©¶è€…ä½¿ç”¨äº†ç‰©ç†è‰²å½©å‡†ç¡®çš„æ‰«æä»ªè¿›è¡Œäº†å››ä¸ªå®éªŒå®¤çš„è‰²å½©å‡†ç¡®æ€§æ ‡å‡†åŒ–ï¼Œä»¥ç¡®å®šè¿™ç§æ–¹æ³•å¯¹æŠ—ç™Œè¯Šæ–­æ¨¡å‹çš„å½±å“ã€‚</li>
<li>results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç‰©ç†è‰²å½©å‡†ç¡®çš„æ‰«æä»ªå¯ä»¥æ ‡å‡†åŒ–æ•´ä¸ªæŠ¥å‘Šå›¾åƒçš„å‡ºç°ï¼Œä»è€Œæé«˜AIæ¨¡å‹çš„å‡†ç¡®æ€§å’ŒGleasonåˆ†çº§è¡¨ç°ã€‚è¿™é¡¹ç ”ç©¶éªŒè¯äº†ç‰©ç†è‰²å½©å‡†ç¡®çš„æ‰«æä»ªå¯ä»¥è§£å†³ä¸åŒæ‰«æä»ª introduceçš„å˜åŒ–ï¼Œä½¿AIåŸºäºçš„è‚¿ç˜¤è¯Šæ–­å˜å¾—æ›´åŠ å¯é å’Œåœ¨ä¸´åºŠè®¾ç½®ä¸­å¯è¡Œã€‚<details>
<summary>Abstract</summary>
The potential of artificial intelligence (AI) in digital pathology is limited by technical inconsistencies in the production of whole slide images (WSIs), leading to degraded AI performance and posing a challenge for widespread clinical application as fine-tuning algorithms for each new site is impractical. Changes in the imaging workflow can also lead to compromised diagnoses and patient safety risks. We evaluated whether physical color calibration of scanners can standardize WSI appearance and enable robust AI performance. We employed a color calibration slide in four different laboratories and evaluated its impact on the performance of an AI system for prostate cancer diagnosis on 1,161 WSIs. Color standardization resulted in consistently improved AI model calibration and significant improvements in Gleason grading performance. The study demonstrates that physical color calibration provides a potential solution to the variation introduced by different scanners, making AI-based cancer diagnostics more reliable and applicable in clinical settings.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨æ•°å­— PATHOLOGY ä¸­çš„æ½œåŠ›å—åˆ°æ‰«ææœºå™¨ï¼ˆWhole Slide Imagesï¼ŒWSIsï¼‰æŠ€æœ¯ä¸ä¸€è‡´çš„é™åˆ¶ï¼Œå¯¼è‡´ AI æ€§èƒ½ä¸‹é™ï¼Œå¹¶å¯¹è¥å…»å¹¿æ³›ä¸´åºŠåº”ç”¨ pose æŒ‘æˆ˜ã€‚å·¥ä½œæµç¨‹å˜åŒ–ä¹Ÿå¯èƒ½å¯¼è‡´è¯Šæ–­é”™è¯¯å’Œ patient safety é£é™©ã€‚æˆ‘ä»¬è¯„ä¼°äº†æ‰«ææœºå™¨çš„ç‰©ç†è‰²å½©å‡†ç¡®æ€§æ˜¯å¦å¯ä»¥æ ‡å‡†åŒ– WSI çš„å¤–è§‚ï¼Œå¹¶å¯¹æŠ—è‚‰ç™Œè¯Šæ–­ AI ç³»ç»Ÿçš„1,161 WSI çš„è¡¨ç°ã€‚è‰²å½©æ ‡å‡†åŒ–å¯¼è‡´ AI æ¨¡å‹å‡†ç¡®æ€§çš„æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨ Gleason åˆ†æœŸæ€§èƒ½ä¸­å¾—åˆ°äº†æ˜¾è‘—æ”¹è¿›ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œç‰©ç†è‰²å½©å‡†ç¡®æ€§æä¾›äº†æ‰«ææœºå™¨é—´å˜åŒ–å¼•å…¥çš„è§£å†³æ–¹æ¡ˆï¼Œä½¿ AI åŸºäºè‚‰ç™Œè¯Šæ–­æ›´å¯é å’Œåœ¨ä¸´åºŠè®¾ç½®ä¸­åº”ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Graph-Pooling-for-Explainable-Classification-of-Brain-Networks"><a href="#Contrastive-Graph-Pooling-for-Explainable-Classification-of-Brain-Networks" class="headerlink" title="Contrastive Graph Pooling for Explainable Classification of Brain Networks"></a>Contrastive Graph Pooling for Explainable Classification of Brain Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11133">http://arxiv.org/abs/2307.11133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxing Xu, Qingtian Bian, Xinhang Li, Aihu Zhang, Yiping Ke, Miao Qiao, Wei Zhang, Wei Khang Jeremy Sim, BalÃ¡zs GulyÃ¡s</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§é€‚ç”¨äºFunctional magnetic resonance imaging (fMRI)æ•°æ®çš„å›¾ neural network (GNN) æ¨¡å‹ï¼Œä»¥æé«˜å¯¹å¤§è„‘ç½‘ç»œçš„ç†è§£å’Œæè¿°ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨çš„æ–¹æ³•åŒ…æ‹¬ä¸€ç§å¯¹æ¯”æ€§åŒæ³¨æ„åŠ›å—å’Œä¸€ç§å¯å¾®graph poolingæ–¹æ³•ï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ©ç”¨GNNæ¥æè¿°å¤§è„‘ç½‘ç»œã€‚</li>
<li>results: è¯¥è®ºæ–‡åœ¨5ä¸ªä¼‘æ¯æ€fMRIå¤§è„‘ç½‘ç»œæ•°æ®é›†ä¸Šè¿›è¡Œäº†åº”ç”¨ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨æ¯”åŸºelinesä¸Šè¡¨ç°å‡ºä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°äº†ä¸ neuroscience æ–‡çŒ®ä¸­çš„çŸ¥è¯†åŒ¹é…çš„ç‰¹å¾ç‰¹å¾ï¼Œå¹¶æä¾›äº†ç›´è§‚å’Œæœ‰è¶£çš„æ¢ç´¢ã€‚<details>
<summary>Abstract</summary>
Functional magnetic resonance imaging (fMRI) is a commonly used technique to measure neural activation. Its application has been particularly important in identifying underlying neurodegenerative conditions such as Parkinson's, Alzheimer's, and Autism. Recent analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literature, and disclose direct and interesting insights. Our contributions underscore the potential of ContrastPool for advancing the understanding of brain networks and neurodegenerative conditions.
</details>
<details>
<summary>æ‘˜è¦</summary>
Functional magnetic resonance imaging (fMRI) æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯æ¥æµ‹é‡ç¥ç»æ´»åŠ¨ã€‚å…¶åº”ç”¨åœ¨è¯†åˆ«ä¸‹é¢çš„ç¥ç»é€€åŒ–ç–¾ç—…ï¼Œå¦‚ Parkinson'sã€Alzheimer's å’Œ Autism ç­‰æ–¹é¢ç‰¹åˆ«é‡è¦ã€‚æœ€è¿‘çš„ fMRI æ•°æ®åˆ†ææ¨¡å‹å°†å¤§è„‘è§†ä¸ºå›¾ï¼Œé€šè¿‡å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æå–ç‰¹å¾ã€‚ç„¶è€Œï¼ŒfMRI æ•°æ®çš„ç‰¹æ®Šæ€§éœ€è¦ç‰¹æ®Šçš„ GNN è®¾è®¡ã€‚é€‚åº” GNN ç”Ÿæˆæœ‰æ•ˆå’ŒåŸŸ explainable ç‰¹å¾ä»ç„¶æ˜¯æŒ‘æˆ˜ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹æ¯” dual-attention å—å’Œå¯å¯¼å›¾èšåˆæ–¹æ³•ï¼Œç§°ä¹‹ä¸º ContrastPoolï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨ GNN å¯¹å¤§è„‘ç½‘ç»œã€‚æˆ‘ä»¬åœ¨ 5 ä¸ªä¼‘æ¯æ€ fMRI å¤§è„‘ç½‘ç»œæ•°æ®é›†ä¸Šåº”ç”¨äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¯”åŸºelineä¸Šæ˜¾è‘—superiorã€‚æˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æå–çš„ç‰¹å¾ä¸ neuroscience æ–‡çŒ®ä¸­çš„é¢†åŸŸçŸ¥è¯†åŒ¹é…ï¼Œå¹¶ä¸”æ­ç¤ºäº†ç›´è§‚å’Œæœ‰è¶£çš„å‘ç°ã€‚æˆ‘ä»¬çš„è´¡çŒ®è¡¨æ˜ ContrastPool åœ¨ç†è§£å¤§è„‘ç½‘ç»œå’Œç¥ç»é€€åŒ–ç–¾ç—…æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Procedurally-generating-rules-to-adapt-difficulty-for-narrative-puzzle-games"><a href="#Procedurally-generating-rules-to-adapt-difficulty-for-narrative-puzzle-games" class="headerlink" title="Procedurally generating rules to adapt difficulty for narrative puzzle games"></a>Procedurally generating rules to adapt difficulty for narrative puzzle games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05518">http://arxiv.org/abs/2307.05518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Volden, Djordje Grbic, Paolo Burelli</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨é€è¿‡ç”Ÿæˆè§„åˆ™å’Œé€šè¿‡ç©å®¶æ¥è°ƒæ•´éš¾åº¦ã€‚è¿™æ˜¯ä¸€ä¸ªæ›´å¤§çš„é¡¹ç›®ï¼Œæ—¨åœ¨æ”¶é›†å’Œé€‚åº”æ•™è‚²æ¸¸æˆ Ğ´Ğ»Ñå°å­¦ç”Ÿä½¿ç”¨æ•°å­—è°œé¢˜æ¸¸æˆï¼Œè®¾è®¡ç»™å¹¼å„¿å›­ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†é—ä¼ ç®—æ³•å’Œéš¾åº¦åº¦é‡æ¥æ‰¾åˆ°ç›®æ ‡è§£é›†å’Œå¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹æ¥é€šè¿‡narativeContextæ¥äº¤æµè§„åˆ™ã€‚</li>
<li>results: åœ¨æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å¹³å‡24ä¸ªä»£è¡¨ä¸­æ‰¾åˆ°è§„åˆ™ï¼Œä»¥è¾¾åˆ°ç›®æ ‡éš¾åº¦ã€‚å°†æ¥çš„å®éªŒè®¡åˆ’æé«˜è¯„ä¼°ã€ç‰¹åŒ–è¯­è¨€æ¨¡å‹åˆ°å„¿ç«¥æ–‡å­¦ï¼Œå¹¶æ”¶é›†å¤šmodalæ•°æ®æ¥å¼•å¯¼é€‚åº”ã€‚<details>
<summary>Abstract</summary>
This paper focuses on procedurally generating rules and communicating them to players to adjust the difficulty. This is part of a larger project to collect and adapt games in educational games for young children using a digital puzzle game designed for kindergarten. A genetic algorithm is used together with a difficulty measure to find a target number of solution sets and a large language model is used to communicate the rules in a narrative context. During testing the approach was able to find rules that approximate any given target difficulty within two dozen generations on average. The approach was combined with a large language model to create a narrative puzzle game where players have to host a dinner for animals that can't get along. Future experiments will try to improve evaluation, specialize the language model on children's literature, and collect multi-modal data from players to guide adaptation.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translation notes:* "procedurally generating" ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ (shÄ“ngchÇng yÇ” xiÇngchÇng)* "difficulty" éš¾åº¦ (nÃ¡ndÃ¹)* "target number of solution sets" ç›®æ ‡è§£å†³æ–¹æ¡ˆçš„æ•°é‡ (mÃ¹zhÃ¬ jiÄ›juÃ© fÄng'Ã n de shÃ¹liÃ ng)* "large language model" å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ (dÃ xÃ­ng zÃ¬rÃ¡n yÇ”yÃ¡n mÃ³delÃ¬)* "narrative context" å™äº‹ä¸Šä¸‹æ–‡ (jiÃ¹shÃ¬ shÃ ngxÃ¬a)* "genetic algorithm" é—ä¼ ç®—æ³• (lÃ¬chÇng suÃ nfÇ)* "solution sets" è§£å†³æ–¹æ¡ˆ (jiÄ›juÃ© fÄng'Ã n)
</details></li>
</ul>
<hr>
<h2 id="Tranfer-Learning-of-Semantic-Segmentation-Methods-for-Identifying-Buried-Archaeological-Structures-on-LiDAR-Data"><a href="#Tranfer-Learning-of-Semantic-Segmentation-Methods-for-Identifying-Buried-Archaeological-Structures-on-LiDAR-Data" class="headerlink" title="Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data"></a>Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03512">http://arxiv.org/abs/2307.03512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paolo Soleni, Wouter B. Verschoof-van der Vaart, Å½iga Kokalj, Arianna Traviglia, Marco Fiorucci</li>
<li>for: ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡Œè¿œç¨‹æ„ŸçŸ¥æ•°æ®åœ¨è€ƒå¤ç ”ç©¶ä¸­åº”ç”¨ï¼Œä¸€ä¸ªä¸»è¦éšœç¢æ˜¯è®­ç»ƒæ¨¡å‹æ‰€éœ€çš„æ•°æ®çš„æœ‰é™å¯ç”¨æ€§ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¼ è¾“å­¦ä¹ æŠ€æœ¯ï¼Œå¹¶å¯¹ä¸¤ä¸ªsemantic segmentationæ·±åº¦ç¥ç»ç½‘ç»œåœ¨ä¸¤ä¸ªLiDARæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è€ƒå¤é¢†åŸŸä¸­ä½¿ç”¨ä¼ è¾“å­¦ä¹ é…ç½®å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†å°šæœªè§‚å¯Ÿåˆ°ç³»ç»Ÿæ€§çš„æé«˜ã€‚æˆ‘ä»¬æä¾›äº†ç‰¹å®šçš„åº”ç”¨åœºæ™¯ï¼Œä»¥ä¾›æœªæ¥ç ”ç©¶çš„å‚è€ƒã€‚<details>
<summary>Abstract</summary>
When applying deep learning to remote sensing data in archaeological research, a notable obstacle is the limited availability of suitable datasets for training models. The application of transfer learning is frequently employed to mitigate this drawback. However, there is still a need to explore its effectiveness when applied across different archaeological datasets. This paper compares the performance of various transfer learning configurations using two semantic segmentation deep neural networks on two LiDAR datasets. The experimental results indicate that transfer learning-based approaches in archaeology can lead to performance improvements, although a systematic enhancement has not yet been observed. We provide specific insights about the validity of such techniques that can serve as a baseline for future works.
</details>
<details>
<summary>æ‘˜è¦</summary>
å½“åº”ç”¨æ·±åº¦å­¦ä¹ åˆ°è¿œç¨‹æ„ŸçŸ¥æ•°æ®ä¸­çš„è€ƒå¤ç ”ç©¶ä¸­ï¼Œä¸€ä¸ªæ˜¾è‘—çš„éšœç¢æ˜¯è®­ç»ƒæ¨¡å‹çš„æ•°æ®å‡å°‘çš„é™åˆ¶ã€‚é€šå¸¸ä½¿ç”¨ä¼ è¾“å­¦ä¹ æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿˜éœ€è¦æ¢ç´¢å®ƒåœ¨ä¸åŒçš„è€ƒå¤æ•°æ®é›†ä¹‹é—´çš„æ•ˆæœã€‚è¿™ç¯‡è®ºæ–‡æ¯”è¾ƒäº†ä¸åŒçš„ä¼ è¾“å­¦ä¹ é…ç½®ä½¿ç”¨ä¸¤ç§semantic segmentationæ·±åº¦ç¥ç»ç½‘ç»œåœ¨ä¸¤ä¸ªLiDARæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è€ƒå¤é¢†åŸŸä¸­ä½¿ç”¨ä¼ è¾“å­¦ä¹ å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†æ˜¯è¿˜æ²¡æœ‰ç³»ç»Ÿåœ°æé«˜ã€‚æˆ‘ä»¬æä¾›äº†ç‰¹å®šçš„æ´å¯Ÿï¼Œä»¥ä¾›æœªæ¥ç ”ç©¶çš„å‚è€ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="Derivative-Free-Weight-space-Ensembling"><a href="#Derivative-Free-Weight-space-Ensembling" class="headerlink" title="Derivative Free Weight-space Ensembling"></a>Derivative Free Weight-space Ensembling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03506">http://arxiv.org/abs/2307.03506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dean Ninalga</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æå‡ºä¸€ç§æ–°çš„å‡ ä¸ªæ ·æœ¬ä»»åŠ¡ä¼ é€’æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨å¼€æ”¾é¢†åŸŸå¯¹è¯ä¸­è¿›è¡Œæœ‰æ•ˆçš„ä»»åŠ¡ä¼ é€’ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†Derivative Free Weight-space Ensemblingï¼ˆDFWEï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ›å»ºäº†ä¸€ç»„å¤šæ ·åŒ–çš„ä¸“å®¶è¯­è¨€æ¨¡å‹ï¼Œæ¯ä¸ªä¸“å®¶æ¨¡å‹é€šè¿‡é¢„å®šçš„æºä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚ç„¶åï¼Œæ¯ä¸ªä¸“å®¶æ¨¡å‹éƒ½è¿›è¡Œäº†ç²¾åº¦è°ƒæ•´ï¼Œä»¥ä¾¿æ›´å¥½åœ°é€‚åº”ç›®æ ‡ä»»åŠ¡ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç§æ— çº§ä¼˜åŒ–ç®—æ³•æ¥çº¿æ€§ interpolate  Ğ¼ĞµĞ¶Ğ´Ñƒæ¨¡å‹çš„æƒé‡ï¼Œä»¥è¾¾åˆ°æœ‰æ•ˆåœ°æ‰¾åˆ°ä¸€ä¸ªå¥½çš„æƒé‡æ··åˆã€‚</li>
<li>results: æˆ‘ä»¬åœ¨FETA-Friendsä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶è¯æ˜äº†DFWEçš„æ•ˆæœã€‚ç›¸æ¯”æ ‡å‡†çš„é¢„è®­ç»ƒ-ç²¾åº¦è°ƒæ•´æ–¹æ³•ï¼ŒDFWEèƒ½å¤Ÿæ›´å¥½åœ°ä¼ é€’çŸ¥è¯†å¹¶æé«˜ä»»åŠ¡è¡¨ç°ã€‚<details>
<summary>Abstract</summary>
Recent work suggests that interpolating between the weights of two specialized language models can transfer knowledge between tasks in a way that multi-task learning cannot. However, very few have explored interpolation between more than two models, where each has a distinct knowledge base. In this paper, we introduce Derivative Free Weight-space Ensembling (DFWE), a new few-sample task transfer approach for open-domain dialogue. Our framework creates a set of diverse expert language models trained using a predefined set of source tasks. Next, we finetune each of the expert models on the target task, approaching the target task from several distinct knowledge bases. Finally, we linearly interpolate between the model weights using a gradient-free-optimization algorithm, to efficiently find a good interpolation weighting. We demonstrate the effectiveness of the method on FETA-Friends outperforming the standard pretrain-finetune approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ä¸¤ä¸ªç‰¹æ®ŠåŒ–è¯­è¨€æ¨¡å‹ä¹‹é—´ interpolate çŸ¥è¯†å¯ä»¥åœ¨ä»»åŠ¡ä¹‹é—´ä¼ é€’çŸ¥è¯†ï¼Œè€Œå¤šä»»åŠ¡å­¦ä¹ åˆ™æ— æ³•å®ç°ã€‚ç„¶è€Œï¼Œå¾ˆå°‘äººç ”ç©¶äº†è¶…è¿‡ä¸¤ä¸ªæ¨¡å‹çš„ interpolateã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† Derivative Free Weight-space Ensembling (DFWE)ï¼Œä¸€ç§æ–°çš„å‡ ä¸ªæ ·æœ¬ä»»åŠ¡ä¼ é€’æ–¹æ³•ï¼Œç”¨äºå¼€æ”¾é¢†åŸŸå¯¹è¯ã€‚æˆ‘ä»¬çš„æ¡†æ¶åˆ›å»ºäº†ä¸€ç»„å¤šæ ·åŒ–çš„ä¸“å®¶è¯­è¨€æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹é€šè¿‡é¢„å®šçš„æºä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚ç„¶åï¼Œæˆ‘ä»¬æ¯ä¸ªä¸“å®¶æ¨¡å‹éƒ½åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šç²¾åº¦è°ƒæ•´ï¼Œä»å¤šä¸ªä¸åŒçš„çŸ¥è¯†åŸºç¡€ä¸Šè¿›è¡Œ approachedã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª gradient-free-optimization ç®—æ³•æ¥çº¿æ€§ interpolate æ¨¡å‹çš„ weightsï¼Œä»¥æ•ˆç‡åœ°æ‰¾åˆ°ä¸€ä¸ªå¥½çš„ interpolate æƒé‡ã€‚æˆ‘ä»¬åœ¨ FETA-Friends ä¸Š demonstrate äº†æ–¹æ³•çš„æ•ˆæœï¼Œè¶…è¿‡æ ‡å‡†é¢„è®­ç»ƒ-ç²¾åº¦è°ƒæ•´æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="RCDN-â€“-Robust-X-Corner-Detection-Algorithm-based-on-Advanced-CNN-Model"><a href="#RCDN-â€“-Robust-X-Corner-Detection-Algorithm-based-on-Advanced-CNN-Model" class="headerlink" title="RCDN â€“ Robust X-Corner Detection Algorithm based on Advanced CNN Model"></a>RCDN â€“ Robust X-Corner Detection Algorithm based on Advanced CNN Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03505">http://arxiv.org/abs/2307.03505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Chen, Caihua Xiong, Quanlin Li, Zhonghua Wan</li>
<li>for: æé«˜æœºå™¨è§†è§‰å’Œæœºå™¨äººé¢†åŸŸä¸­X-è§’è½æ£€æµ‹å’Œåœ°ç†åŒ–çš„ç²¾åº¦å’Œå¯é æ€§ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§æ–°çš„æ£€æµ‹ç®—æ³•ï¼Œå¯ä»¥åœ¨å¤šç§å¹²æ‰°ä¸‹ä¿æŒé«˜æ¯”ç´ ç²¾åº¦ï¼ŒåŒ…æ‹¬é•œå¤´æ‰­æ›²ã€æç«¯poseå’Œå™ªå£°ã€‚è¯¥ç®—æ³•é‡‡ç”¨äº†ä¸€ä¸ªç²—ç²’åº¦æ£€æµ‹ç½‘ç»œå’Œä¸‰ç§åå¤„ç†æŠ€æœ¯æ¥ç­›é€‰æ­£ç¡®çš„è§’åº¦å€™é€‰è€…ï¼Œä»¥åŠä¸€ç§æ··åˆæ¯”ç´ ç²¾åº¦ä¿®æ­£æŠ€æœ¯å’Œæ”¹è¿›çš„åŒºåŸŸå¢é•¿ç­–ç•¥æ¥è‡ªåŠ¨åœ°æ¢å¤éƒ¨åˆ†å¯è§æˆ–é®æŒ¡çš„æ£€æŸ¥æ¿å›¾æ ·ã€‚</li>
<li>results: å¯¹å®é™…å’Œ sintetic å›¾åƒè¿›è¡Œè¯„ä¼°ï¼Œè¡¨æ˜æå‡ºçš„ç®—æ³•åœ¨æ£€æµ‹ç‡ã€æ¯”ç´ ç²¾åº¦å’ŒRobustnessæ–¹é¢æ¯”å…¶ä»–å¸¸ç”¨æ–¹æ³•æ›´é«˜ã€‚æ­¤å¤–ï¼Œcamera calibrationå’Œpose estimationå®éªŒä¹Ÿè¡¨æ˜ï¼Œè¯¥ç®—æ³•å¯ä»¥æ›´å¥½åœ°å®ç°ç›¸æœºå‚æ•°çš„è°ƒæ•´å’Œposeçš„ä¼°è®¡ã€‚<details>
<summary>Abstract</summary>
Accurate detection and localization of X-corner on both planar and non-planar patterns is a core step in robotics and machine vision. However, previous works could not make a good balance between accuracy and robustness, which are both crucial criteria to evaluate the detectors performance. To address this problem, in this paper we present a novel detection algorithm which can maintain high sub-pixel precision on inputs under multiple interference, such as lens distortion, extreme poses and noise. The whole algorithm, adopting a coarse-to-fine strategy, contains a X-corner detection network and three post-processing techniques to distinguish the correct corner candidates, as well as a mixed sub-pixel refinement technique and an improved region growth strategy to recover the checkerboard pattern partially visible or occluded automatically. Evaluations on real and synthetic images indicate that the presented algorithm has the higher detection rate, sub-pixel accuracy and robustness than other commonly used methods. Finally, experiments of camera calibration and pose estimation verify it can also get smaller re-projection error in quantitative comparisons to the state-of-the-art.
</details>
<details>
<summary>æ‘˜è¦</summary>
é€šè¿‡ç²¾å‡†æ¢æµ‹å’Œå®šä½Xè§’çš„ç®—æ³•ï¼Œ roboticså’Œæœºå™¨è§†è§‰ä¸­çš„æ ¸å¿ƒæ­¥éª¤æ˜¯æ£€æµ‹Xè§’ã€‚ç„¶è€Œï¼Œè¿‡å»çš„æ–¹æ³•æ— æ³•ä¿æŒé«˜ç²¾åº¦å’Œå¯é æ€§çš„å¹³è¡¡ï¼Œè¿™ä¸¤ä¸ª ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ãª¨éƒ½æ˜¯è¯„ä¼°æ¢æµ‹å™¨æ€§èƒ½çš„å…³é”®å› ç´ ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¢æµ‹ç®—æ³•ï¼Œå¯ä»¥åœ¨å¤šç§å¹²æ‰°ä¸‹ä¿æŒé«˜åˆ†è¾¨ç‡ï¼ŒåŒ…æ‹¬é•œå¤´æ‰­æ›²ã€æç«¯poseå’Œå™ªå£°ã€‚è¯¥ç®—æ³•é‡‡ç”¨äº†ç²—ç²’åº¦æ¢æµ‹ç½‘ç»œå’Œä¸‰ç§åå¤„ç†æŠ€æœ¯æ¥åˆ†è¾¨æ­£ç¡®çš„è§’åº¦å€™é€‰è€…ï¼Œä»¥åŠæ··åˆåˆ†è¾¨ç‡çº æ­£æŠ€æœ¯å’Œæ”¹è¿›çš„åŒºåŸŸå¢é•¿ç­–ç•¥æ¥è‡ªåŠ¨åœ°æ¢å¤éƒ¨åˆ†å¯è§æˆ–é®æŒ¡çš„Checkerboardæ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„ç®—æ³•åœ¨çœŸå®å’Œ sintetic å›¾åƒä¸Šå…·æœ‰æ›´é«˜çš„æ£€æµ‹ç‡ã€åˆ†è¾¨ç‡å’Œå¯é æ€§ï¼Œå¹¶ä¸”åœ¨ç›¸æœºå‡†å¤‡å’Œposeä¼°è®¡æ–¹é¢ä¹Ÿèƒ½å¤Ÿè·å¾—æ›´å°çš„é‡æ˜ å°„è¯¯å·®ã€‚
</details></li>
</ul>
<hr>
<h2 id="Large-AI-Model-Based-Semantic-Communications"><a href="#Large-AI-Model-Based-Semantic-Communications" class="headerlink" title="Large AI Model-Based Semantic Communications"></a>Large AI Model-Based Semantic Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03492">http://arxiv.org/abs/2307.03492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰çš„æ™ºèƒ½é€šä¿¡ç³»ç»Ÿä¸­çŸ¥è¯†åŸºç¡€æ„å»ºé—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºå¤§æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ™ºèƒ½é€šä¿¡æ¡†æ¶ï¼ˆLAM-SCï¼‰ï¼Œç”¨äºå¤„ç†å›¾åƒæ•°æ®ã€‚</li>
<li>methods: è¯¥æ¡†æ¶é¦–å…ˆè®¾è®¡äº†åŸºäºuniversal semantic knowledgeçš„å›¾åƒåˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰çŸ¥è¯†åŸºç¡€ï¼ˆSKBï¼‰ï¼Œç„¶åæå‡ºä¸€ç§åŸºäºæ³¨æ„åŠ›çš„Semantic Integrationï¼ˆASIï¼‰æ–¹æ³•ï¼Œä»¥åŠä¸€ç§é€‚åº”æ€§å‹ç¼©ï¼ˆASCï¼‰ç¼–ç æ–¹æ³•æ¥å‡å°‘é€šä¿¡å¼€é”€ã€‚</li>
<li>results: é€šè¿‡å®éªŒï¼Œè®ºæ–‡ç¤ºå‡ºäº†LAM-SCæ¡†æ¶çš„æ•ˆæœå’Œæœªæ¥æ™ºèƒ½é€šä¿¡æ¨¡å¼ä¸­å¤§æœºå™¨å­¦ä¹ æ¨¡å‹åŸºç¡€çŸ¥è¯†çš„é‡è¦æ€§ã€‚<details>
<summary>Abstract</summary>
Semantic communication (SC) is an emerging intelligent paradigm, offering solutions for various future applications like metaverse, mixed-reality, and the Internet of everything. However, in current SC systems, the construction of the knowledge base (KB) faces several issues, including limited knowledge representation, frequent knowledge updates, and insecure knowledge sharing. Fortunately, the development of the large AI model provides new solutions to overcome above issues. Here, we propose a large AI model-based SC framework (LAM-SC) specifically designed for image data, where we first design the segment anything model (SAM)-based KB (SKB) that can split the original image into different semantic segments by universal semantic knowledge. Then, we present an attention-based semantic integration (ASI) to weigh the semantic segments generated by SKB without human participation and integrate them as the semantic-aware image. Additionally, we propose an adaptive semantic compression (ASC) encoding to remove redundant information in semantic features, thereby reducing communication overhead. Finally, through simulations, we demonstrate the effectiveness of the LAM-SC framework and the significance of the large AI model-based KB development in future SC paradigms.
</details>
<details>
<summary>æ‘˜è¦</summary>
semantic communication (SC) æ˜¯ä¸€ç§emerging intelligent paradigmï¼Œæä¾›æœªæ¥åº”ç”¨ç¨‹åºï¼Œå¦‚ metaverseã€æ··åˆç°å®å’Œ everything äº’è”ç½‘ã€‚ç„¶è€Œï¼Œåœ¨å½“å‰ SC ç³»ç»Ÿä¸­ï¼ŒçŸ¥è¯†åº“ï¼ˆKBï¼‰çš„æ„å»ºé¢ä¸´å¤šç§é—®é¢˜ï¼ŒåŒ…æ‹¬æœ‰é™çš„çŸ¥è¯†è¡¨ç¤ºã€é¢‘ç¹çš„çŸ¥è¯†æ›´æ–°å’Œä¸å®‰å…¨çš„çŸ¥è¯†åˆ†äº«ã€‚å¹¸è¿çš„æ˜¯ï¼Œå¤§å‹ AI æ¨¡å‹çš„å¼€å‘æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæå‡ºä¸€ä¸ªåŸºäºå¤§å‹ AI æ¨¡å‹çš„ SC æ¡†æ¶ï¼ˆLAM-SCï¼‰ï¼Œä¸“é—¨è®¾è®¡ä¸ºå›¾åƒæ•°æ®å¤„ç†ã€‚æˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†åŸºäºuniversal semantic knowledgeçš„segment anything modelï¼ˆSAMï¼‰çŸ¥è¯†åº“ï¼ˆSKBï¼‰ï¼Œå¯ä»¥å°†åŸå§‹å›¾åƒåˆ†è§£æˆä¸åŒçš„semantic segmentã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†æ— äººå‚ä¸çš„æ³¨æ„åŠ›åŸºæœ¬ï¼ˆASIï¼‰ï¼Œå¯ä»¥å¯¹ SKB ç”Ÿæˆçš„semantic segmentè¿›è¡Œæƒé‡ï¼Œå¹¶å°†å®ƒä»¬é›†æˆä¸ºå…·æœ‰semantic-awareçš„å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†è‡ªé€‚åº”semantic compressionï¼ˆASCï¼‰ç¼–ç ï¼Œå¯ä»¥ä»semantic featuresä¸­å»é™¤å†—ä½™ä¿¡æ¯ï¼Œä»¥å‡å°‘é€šä¿¡å¼€é”€ã€‚æœ€åï¼Œé€šè¿‡ simulated experimentsï¼Œæˆ‘ä»¬è¯æ˜äº† LAM-SC æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œæœªæ¥ SC  Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³msä¸­å¤§å‹ AI æ¨¡å‹åŸºæœ¬çŸ¥è¯†åº“çš„å‘å±•çš„é‡è¦æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Artificial-Eye-for-the-Blind"><a href="#Artificial-Eye-for-the-Blind" class="headerlink" title="Artificial Eye for the Blind"></a>Artificial Eye for the Blind</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00801">http://arxiv.org/abs/2308.00801</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepususeel/SmartEye">https://github.com/deepususeel/SmartEye</a></li>
<li>paper_authors: Abhinav Benagi, Dhanyatha Narayan, Charith Rage, A Sushmitha</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æä¾›ä¸€ç§åŸºäºRaspberry Pi3çš„äººå·¥æ™ºèƒ½çœ¼æ¨¡å‹ï¼Œå¸®åŠ©ç›²äººè¿›è¡Œäº¤é€šå¯¼èˆªå’Œæ—¥å¸¸ç”Ÿæ´»ä¸­çš„è¡ŒåŠ¨å†³ç­–ã€‚</li>
<li>methods: è¯¥æ¨¡å‹ä½¿ç”¨äº†raspberry pi3ï¼Œwebcamï¼Œultrasonic proximity sensorï¼Œ speakerå’Œå¤šç§è½¯ä»¶æ¨¡å‹ï¼ŒåŒ…æ‹¬ç‰©ä½“æ£€æµ‹ã€æ–‡æœ¬è¯†åˆ«ã€Googleæ–‡æœ¬è¯†åˆ«å’ŒMycroftè¯­éŸ³åŠ©æ‰‹æ¨¡å‹ã€‚</li>
<li>results: æ¨¡å‹å¯ä»¥å¸®åŠ©ç›²äººåœ¨äº¤é€šå¯¼èˆªå’Œæ—¥å¸¸ç”Ÿæ´»ä¸­æ›´åŠ çµæ´»å’Œè‡ªä¿¡ï¼ŒåŒæ—¶è¿˜å¯ä»¥æä¾›è¯­éŸ³æ´åŠ©å’Œæ–‡æœ¬æ´åŠ©ã€‚<details>
<summary>Abstract</summary>
The main backbone of our Artificial Eye model is the Raspberry pi3 which is connected to the webcam ,ultrasonic proximity sensor, speaker and we also run all our software models i.e object detection, Optical Character recognition, google text to speech conversion and the Mycroft voice assistance model. At first the ultrasonic proximity sensor will be measuring the distance between itself and any obstacle in front of it .When the Proximity sensor detects any obstacle in front within its specified range, the blind person will hear an audio prompt about an obstacle in his way at a certain distance. At this time the Webcam will capture an image in front of it and the Object detection model and the Optical Character Recognition model will begin to run on the Raspberry pi. The imat of the blind person. The text and the object detected are conveyed to the blind pege captured is first sent through the Tesseract OCR module to detect any texts in the image and then through the Object detection model to detect the objects in fronrson by converting the texts to speech by using the gTTS module. Along with the above mentioned process going on there will be an active MYCROFT voice assistant model which can be used to interact with the blind person. The blind person can ask about the weather , daily news , any information on the internet ,etc
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸»è¦è„Šæ¢æˆ‘ä»¬çš„äººå·¥æ™ºèƒ½çœ¼é•œæ¨¡å‹æ˜¯Raspberry Pi3ï¼Œä¸æ‘„åƒå¤´ã€è¶…éŸ³æ³¢è·ç¦»ä»ªã€å–‡å­å’Œæˆ‘ä»¬çš„è½¯ä»¶æ¨¡å‹ï¼ˆç‰©å“æ£€æµ‹ã€å­—ç¬¦è¯†åˆ«ã€Googleæ–‡æœ¬è½¬è¯­éŸ³å’ŒMycroftè¯­éŸ³åŠ©æ‰‹æ¨¡å‹ï¼‰è¿æ¥åœ¨ä¸€èµ·ã€‚åœ¨ primerosï¼Œè¶…éŸ³æ³¢è·ç¦»ä»ªå°†æµ‹é‡è‡ªå·±å’Œå‰æ–¹ä»»ä½•éšœç¢ç‰©çš„è·ç¦»ã€‚å½“è¶…éŸ³æ³¢è·ç¦»ä»ªæ£€æµ‹åˆ°å‰æ–¹ Within its specified range çš„éšœç¢ç‰©æ—¶ï¼Œç›²äººå°†å¬åˆ°ä¸€ä¸ªè¯­éŸ³æé†’ï¼Œè¡¨ç¤ºæœ‰éšœç¢ç‰©åœ¨ä»–çš„è·¯çº¿ä¸Šã€‚åœ¨è¿™ä¸ªæ—¶å€™ï¼Œæ‘„åƒå¤´å°†æ‹æ‘„å‰æ–¹çš„å›¾åƒï¼Œå¹¶å°†å›¾åƒä¼ é€’ç»™Raspberry Piè¿›è¡Œå¤„ç†ã€‚åœ¨å¤„ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Tesseract OCRæ¨¡å—æ¥æ£€æµ‹å›¾åƒä¸­çš„æ–‡æœ¬ï¼Œç„¶åå°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³ï¼Œä½¿ç”¨gTTSæ¨¡å—è¿›è¡Œè½¬æ¢ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜ä¼šæœ‰ä¸€ä¸ªæ´»è·ƒçš„MYCROFTè¯­éŸ³åŠ©æ‰‹æ¨¡å‹ï¼Œå¯ä»¥è®©ç›²äººä¸å…¶è¿›è¡Œäº’åŠ¨ï¼Œç›²äººå¯ä»¥è¯¢é—®å¤©æ°”ã€æ¯æ—¥æ–°é—»ã€ç½‘ç»œä¸Šçš„ä¿¡æ¯ç­‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="Discovering-Hierarchical-Achievements-in-Reinforcement-Learning-via-Contrastive-Learning"><a href="#Discovering-Hierarchical-Achievements-in-Reinforcement-Learning-via-Contrastive-Learning" class="headerlink" title="Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning"></a>Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03486">http://arxiv.org/abs/2307.03486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seungyong Moon, Junyoung Yeom, Bumsoo Park, Hyun Oh Song</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢åœ¨ç”Ÿæˆå‹ç¯å¢ƒä¸­å‘ç°å…·æœ‰å±‚æ¬¡ç»“æ„çš„æˆå°±ï¼Œå¹¶ä¸”éœ€è¦ä»£ç†äººç±» possess ä¸€ç³»åˆ—èƒ½åŠ›ï¼Œå¦‚æ€»ç»“å’Œé•¿æœŸç†è§£ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨ proximal policy optimization (PPO) ç®—æ³•ï¼Œä¸€ç§ç®€å•è€Œå¤šåŠŸèƒ½çš„æ— æ¨¡å‹å­¦ä¹ æ–¹æ³•ï¼Œå¹¶ä¸”å‘ç° PPO ä»£ç†äººç±»å¯ä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªæˆå°±çš„å¯èƒ½æ€§ï¼Œè™½ç„¶ confidence è¾ƒä½ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ PPO ç®—æ³•å’Œæˆ‘ä»¬æå‡ºçš„æ–°çš„å‡†åˆ™å­¦ä¹ æ–¹æ³• achievement distillationï¼Œå¯ä»¥å¼ºåŒ–ä»£ç†äººç±»å¯¹ä¸‹ä¸€ä¸ªæˆå°±çš„é¢„æµ‹ï¼Œå¹¶ä¸”åœ¨æŒ‘æˆ˜æ€§çš„ Crafter ç¯å¢ƒä¸­æ˜¾ç¤ºå‡ºçŠ¶æ€çš„æœ¯è¯­è¡¨ç°ã€‚<details>
<summary>Abstract</summary>
Discovering achievements with a hierarchical structure on procedurally generated environments poses a significant challenge. This requires agents to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods are built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be beneficial for learning hierarchical achievements. However, these methods require an excessive amount of environment interactions or large model sizes, limiting their practicality. In this work, we identify that proximal policy optimization (PPO), a simple and versatile model-free algorithm, outperforms the prior methods with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, though with low confidence. Based on this observation, we propose a novel contrastive learning method, called achievement distillation, that strengthens the agent's capability to predict the next achievement. Our method exhibits a strong capacity for discovering hierarchical achievements and shows state-of-the-art performance on the challenging Crafter environment using fewer model parameters in a sample-efficient regime.
</details>
<details>
<summary>æ‘˜è¦</summary>
å‘ç°å…·æœ‰å±‚æ¬¡ç»“æ„çš„æˆå°±éœ€è¦æ™ºèƒ½ä½“å…·å¤‡å¹¿æ³›çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ€»ç»“å’Œé•¿æœŸé€»è¾‘ã€‚è®¸å¤šå…ˆå‰æ–¹æ³•åŸºäºæ¨¡å‹æˆ–å±‚æ¬¡ç»“æ„ï¼Œä»¥ä¸ºå­˜åœ¨æ˜ç¡®çš„é•¿æœŸè§„åˆ’æ¨¡å—å¯ä»¥å¸®åŠ©å­¦ä¹ å±‚æ¬¡æˆå°±ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¤§é‡çš„ç¯å¢ƒäº’åŠ¨æˆ–åºå¤§çš„æ¨¡å‹å¤§å°ï¼Œé™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°ï¼Œè¿‘ä¼¼ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ï¼Œä¸€ç§ç®€å•å’Œå¤šæ ·çš„æ¨¡å‹è‡ªç”±ç®—æ³•ï¼Œåœ¨ç°æœ‰å®ç°æ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”æˆ‘ä»¬å‘ç°PPOAgentå¯ä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªæˆå°±çš„æ¦‚ç‡ï¼Œè™½ç„¶æœ‰ä¸€å®šçš„ä¸ç¡®å®šæ€§ã€‚åŸºäºè¿™ä¸ªè§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œå³æˆå°±èƒå–ï¼Œä»¥å¼ºåŒ–æ™ºèƒ½ä½“çš„ä¸‹ä¸€ä¸ªæˆå°±é¢„æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æŒ‘æˆ˜æ€§é«˜çš„Crafterç¯å¢ƒä¸­å±•ç°å‡ºäº†ä¼˜ç§€çš„æˆå°±å‘ç°èƒ½åŠ›å’Œæ¨¡å‹å‚æ•°æ›´å°‘çš„æ ·æœ¬æ•ˆç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="TBGC-Task-level-Backbone-Oriented-Gradient-Clip-for-Multi-Task-Foundation-Model-Learning"><a href="#TBGC-Task-level-Backbone-Oriented-Gradient-Clip-for-Multi-Task-Foundation-Model-Learning" class="headerlink" title="TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning"></a>TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03465">http://arxiv.org/abs/2307.03465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zelun Zhang, Xue Pan</li>
<li>for: æé«˜å¤šä»»åŠ¡å­¦ä¹ ä¸­å›å½’æ¢¯åº¦åå¯¼é—®é¢˜</li>
<li>methods: æå‡ºäº†ä»»åŠ¡çº§åˆ«æ¢¯åº¦å‰ªè£ç­–ç•¥å’Œå¤šæ”¯åˆ†æ”¯æ•°æ®å¢å¼ºç­–ç•¥</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥å¯ä»¥å‡è½»å›å½’æ¢¯åº¦åå¯¼é—®é¢˜ï¼Œå¹¶åœ¨CVPR2023 Foundation Model Challengeä¸­è·å¾—1åå’Œ2åã€‚<details>
<summary>Abstract</summary>
The AllInOne training paradigm squeezes a wide range of tasks into a unified model in a multi-task learning manner. However, optimization in multi-task learning is more challenge than single-task learning, as the gradient norm from different tasks may vary greatly, making the backbone overly biased towards one specific task. To address this issue, we propose the task-level backbone-oriented gradient clip paradigm, compared with the vanilla gradient clip method, it has two points of emphasis:1) gradient clip is performed independently for each task. 2) backbone gradients generated from each task are rescaled to the same norm scale. Based on the experimental results, we argue that the task-level backbone-oriented gradient clip paradigm can relieve the gradient bias problem to some extent. We also propose a novel multi-branch data augmentation strategy where conflict augmentations are placed in different branches. Our approach has been shown to be effective and finally achieve 1st place in the Leaderboard A and 2nd place in the Leaderboard B of the CVPR2023 Foundation Model Challenge. It's worth noting that instead of evaluating all three tasks(detection, segmentation and fine-grained classification) in Leaderboard A, the segmentation task is not evaluated in Leaderboard B, in which our team has a huge advantage.
</details>
<details>
<summary>æ‘˜è¦</summary>
å…¨é¢ä¸€ä½“è®­ç»ƒæ¨¡å¼å°†å¤šç§ä»»åŠ¡é›†æˆåˆ°ä¸€ä¸ªå¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹ä¸­ï¼Œä½†æ˜¯å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„ä¼˜åŒ–å…·æœ‰æ›´å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºä¸åŒä»»åŠ¡çš„æ¢¯åº¦èŒƒå›´å¯èƒ½å¾ˆå¤§ï¼Œå¯¼è‡´æ”¯æŒç»“æ„åå‘æŸä¸€ä¸ªç‰¹å®šä»»åŠ¡ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä»»åŠ¡çº§åˆ«æ”¯æŒç»“æ„æŠ˜å æ¢¯åº¦å‰ªè¾‘æ–¹æ³•ï¼Œç›¸æ¯”äºæ™®é€šæ¢¯åº¦å‰ªè¾‘æ–¹æ³•ï¼Œå®ƒå…·æœ‰ä¸¤ç‚¹ä¼˜åŠ¿ï¼š1ï¼‰æ¢¯åº¦å‰ªè¾‘ç‹¬ç«‹è¿›è¡Œæ¯ä¸ªä»»åŠ¡ï¼›2ï¼‰æ¯ä¸ªä»»åŠ¡ç”Ÿæˆçš„æ”¯æŒç»“æ„æ¢¯åº¦éƒ½è¢«ç¼©æ”¾åˆ°åŒä¸€ä¸ªèŒƒå›´å°ºåº¦ã€‚æ ¹æ®å®éªŒç»“æœï¼Œæˆ‘ä»¬è®¤ä¸ºä»»åŠ¡çº§åˆ«æ”¯æŒç»“æ„æŠ˜å æ¢¯åº¦å‰ªè¾‘æ–¹æ³•å¯ä»¥å‡è½»æ¢¯åº¦åå‘é—®é¢˜è‡³å°‘ä¸€éƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ”¯æŒåˆ†æ”¯æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå…¶ä¸­å†²çªå¢å¼ºè¢«æ”¾ç½®åœ¨ä¸åŒæ”¯æŒä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨CVPR2023åŸºé‡‘ä¼šæ¨¡å‹æŒ‘æˆ˜ä¸­è·å¾—äº†1åå’Œ2åã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨Leaderboard Aä¸­è¯„ä¼°æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ï¼ˆæ£€æµ‹ã€ segmentation å’Œç»†åŒ–åˆ†ç±»ï¼‰ï¼Œè€ŒLeaderboard Bä¸­ä¸è¯„ä¼° segmentation ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šå…·æœ‰å¾ˆå¤§ä¼˜åŠ¿ã€‚
</details></li>
</ul>
<hr>
<h2 id="MultiQG-TI-Towards-Question-Generation-from-Multi-modal-Sources"><a href="#MultiQG-TI-Towards-Question-Generation-from-Multi-modal-Sources" class="headerlink" title="MultiQG-TI: Towards Question Generation from Multi-modal Sources"></a>MultiQG-TI: Towards Question Generation from Multi-modal Sources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04643">http://arxiv.org/abs/2307.04643</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moonlightlane/multiqg-ti">https://github.com/moonlightlane/multiqg-ti</a></li>
<li>paper_authors: Zichao Wang, Richard Baraniuk</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨ç”Ÿæˆé—®é¢˜ï¼ˆQGï¼‰ FROM å¤šModalSourceä¸­çš„å›¾åƒå’Œæ–‡æœ¬ï¼Œæ‰©å±•äº†å¤§å¤šæ•°ç°æœ‰å·¥ä½œçš„èŒƒå›´ï¼Œè¿™äº›å·¥ä½œéƒ½ä¸“æ³¨äºä»…ä»…ä»æ–‡æœ¬æºä¸­ç”Ÿæˆé—®é¢˜ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œcalled MultiQG-TIï¼Œå®ƒä½¿å¾—æ–‡æœ¬åªé—®é¢˜ç”Ÿæˆå™¨èƒ½å¤Ÿå¤„ç†è§†è§‰è¾“å…¥ã€‚æˆ‘ä»¬åˆ©ç”¨å›¾åƒæè¿°æ¨¡å‹å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«æ¨¡å‹æ¥è·å–å›¾åƒçš„æ–‡æœ¬æè¿°å’Œå›¾åƒä¸­çš„æ–‡æœ¬ï¼Œå¹¶å°†å®ƒä»¬ä¸è¾“å…¥æ–‡æœ¬ä¸€èµ·ä¼ é€’ç»™é—®é¢˜ç”Ÿæˆå™¨ã€‚æˆ‘ä»¬åªæ˜¯å¾®è°ƒé—®é¢˜ç”Ÿæˆå™¨ï¼Œè€Œä¿æŒå…¶ä»–ç»„ä»¶ä¸å˜ã€‚</li>
<li>results: åœ¨ ScienceQA æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬ç¤ºå‡ºäº† MultiQG-TI åœ¨å‡ ä¸ªshot prompting ä¸‹Significantly outperform ChatGPTï¼Œå³ä½¿å®ƒæœ‰ç™¾åˆ†ä¹‹ä¸€çš„è®­ç»ƒå‚æ•°ã€‚Additional åˆ†æä¹Ÿè¯æ˜äº†è§†è§‰å’Œæ–‡æœ¬ä¿¡å·çš„å¿…è¦æ€§ï¼Œä»¥åŠæ¨¡å‹é€‰æ‹©çš„å½±å“ã€‚<details>
<summary>Abstract</summary>
We study the new problem of automatic question generation (QG) from multi-modal sources containing images and texts, significantly expanding the scope of most of the existing work that focuses exclusively on QG from only textual sources. We propose a simple solution for our new problem, called MultiQG-TI, which enables a text-only question generator to process visual input in addition to textual input. Specifically, we leverage an image-to-text model and an optical character recognition model to obtain the textual description of the image and extract any texts in the image, respectively, and then feed them together with the input texts to the question generator. We only fine-tune the question generator while keeping the other components fixed. On the challenging ScienceQA dataset, we demonstrate that MultiQG-TI significantly outperforms ChatGPT with few-shot prompting, despite having hundred-times less trainable parameters. Additional analyses empirically confirm the necessity of both visual and textual signals for QG and show the impact of various modeling choices.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ç ”ç©¶ä¸€ä¸ªæ–°çš„è‡ªåŠ¨é—®é¢˜ç”Ÿæˆï¼ˆQGï¼‰é—®é¢˜ï¼Œåˆ©ç”¨å¤šModalæ¥æºï¼ŒåŒ…æ‹¬å›¾åƒå’Œæ–‡æœ¬ï¼Œä»è€Œæ‰©å¤§ç°æœ‰å¤§å¤šæ•°å·¥ä½œçš„èŒƒå›´ï¼Œè¿™äº›å·¥ä½œéƒ½ä¸“æ³¨äºåªä½¿ç”¨æ–‡æœ¬æ¥æºè¿›è¡ŒQGã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œç§°ä¸ºMultiQG-TIï¼Œå®ƒä½¿å¾—æ–‡æœ¬åªçš„é—®é¢˜ç”Ÿæˆå™¨èƒ½å¤Ÿå¤„ç†è§†è§‰è¾“å…¥ï¼ŒåŒæ—¶è¿˜å¯ä»¥å¤„ç†æ–‡æœ¬è¾“å…¥ã€‚æˆ‘ä»¬åˆ©ç”¨å›¾åƒåˆ°æ–‡æœ¬æ¨¡å‹å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«æ¨¡å‹æ¥è·å¾—å›¾åƒçš„æ–‡æœ¬æè¿°å’Œå›¾åƒä¸­çš„æ–‡æœ¬ï¼Œç„¶åå°†è¿™äº›ä¿¡æ¯ä¸è¾“å…¥æ–‡æœ¬ä¸€èµ·ä¼ é€’ç»™é—®é¢˜ç”Ÿæˆå™¨ã€‚æˆ‘ä»¬åªæ˜¯å¾®è°ƒé—®é¢˜ç”Ÿæˆå™¨ï¼Œè€Œä¸æ˜¯å…¶ä»–ç»„ä»¶ã€‚åœ¨ ScienceQA æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬è¯æ˜ MultiQG-TI åœ¨å°‘é‡æç¤ºä¸‹ï¼Œä»¥ hundred-times  fewer trainable parameters çš„æƒ…å†µä¸‹ï¼Œ Significantly outperform ChatGPTã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†æ›´å¤šçš„åˆ†æï¼Œç¡®è®¤äº†è§†è§‰å’Œæ–‡æœ¬ä¿¡å·çš„å¿…è¦æ€§ï¼Œä»¥åŠæ¨¡å‹é€‰æ‹©çš„å½±å“ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Graph-Neural-Networks-for-Time-Series-Forecasting-Classification-Imputation-and-Anomaly-Detection"><a href="#A-Survey-on-Graph-Neural-Networks-for-Time-Series-Forecasting-Classification-Imputation-and-Anomaly-Detection" class="headerlink" title="A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection"></a>A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03759">http://arxiv.org/abs/2307.03759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kimmeen/awesome-gnn4ts">https://github.com/kimmeen/awesome-gnn4ts</a></li>
<li>paper_authors: Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Geoffrey I. Webb, Irwin King, Shirui Pan</li>
<li>for: æœ¬ç ”ç©¶è¯„è®ºæ–‡ç« æ—¨åœ¨æ¦‚è¿°å›¾ neural networkï¼ˆGNNï¼‰åœ¨æ—¶é—´åºåˆ—åˆ†æï¼ˆTSï¼‰é¢†åŸŸçš„åº”ç”¨ï¼ŒåŒ…æ‹¬é¢„æµ‹ã€åˆ†ç±»ã€å¼‚å¸¸æ£€æµ‹å’Œå¡«å……ç­‰æ–¹é¢ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨GNNæ¥æ¨¡å‹æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å…³ç³»ï¼ŒåŒ…æ‹¬æ—¶é—´åºåˆ—ä¹‹é—´å’Œå˜é‡ä¹‹é—´çš„å…³ç³»ã€‚GNNå¯ä»¥æ›´å¥½åœ°æ¨¡å‹è¿™äº›å…³ç³»ï¼Œæ¯”å¦‚ä¼ ç»Ÿçš„æ·±åº¦ç¥ç»ç½‘ç»œå’Œå…¶ä»–GNN-basedæ–¹æ³•ã€‚</li>
<li>results: æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå…¨é¢çš„ä»»åŠ¡-å¯¼å‘çš„åˆ†ç±»æ³•ï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†ä¸€äº›ä»£è¡¨æ€§çš„ç ”ç©¶å·¥ä½œå’Œåº”ç”¨ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æå‡ºäº†æœªæ¥ç ”ç©¶çš„å¯èƒ½æ€§ï¼ŒåŒ…æ‹¬é’ˆå¯¹ä¸åŒç±»å‹æ—¶é—´åºåˆ—æ•°æ®çš„GNNæ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. These approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative research works and introduce mainstream applications of GNN4TS. A comprehensive discussion of potential future research directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities of graph neural networks for time series analysis.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ—¶é—´åºåˆ—æ˜¯ä¸»è¦æ•°æ®ç±»å‹ï¼Œç”¨äºè®°å½•åŠ¨æ€ç³»ç»Ÿæµ‹é‡å’Œç”Ÿæˆå¤§é‡æ•°æ®ï¼Œboth physical sensorså’Œåœ¨çº¿è¿‡ç¨‹ï¼ˆè™šæ‹Ÿæ„ŸçŸ¥å™¨ï¼‰ç”Ÿæˆã€‚æ—¶é—´åºåˆ—åˆ†æå› æ­¤æ˜¯è§£é”å¯ç”¨æ•°æ®ä¸­çš„å·¨é‡ä¿¡æ¯çš„å…³é”®ã€‚éšç€å›¾ neural networksï¼ˆGNNsï¼‰çš„æœ€è¿‘è¿›æ­¥ï¼Œæœ‰ä¸€ä¸ªæµªæ¶ŒGNN-basedæ—¶é—´åºåˆ—åˆ†ææ–¹æ³•çš„å‡ºç°ã€‚è¿™äº›æ–¹æ³•å¯ä»¥æ˜¾å¼åœ°æ¨¡å‹æ—¶é—´åºåˆ—å’Œå˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œä¼ ç»Ÿçš„å’Œå…¶ä»–æ·±åº¦ç¥ç»ç½‘ç»œåŸºäºæ–¹æ³•éš¾ä»¥åšåˆ°ã€‚åœ¨æœ¬surveyä¸­ï¼Œæˆ‘ä»¬æä¾›äº†Graph Neural Networks for Time Series Analysisï¼ˆGNN4TSï¼‰çš„å…¨é¢è¯„è®ºï¼Œæ¶µç›–å››ä¸ªåŸºæœ¬ç»´åº¦ï¼šé¢„æµ‹ã€åˆ†ç±»ã€å¼‚å¸¸æ£€æµ‹å’Œè¡¥åšã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼•å¯¼è®¾è®¡è€…å’Œå®è·µè€…ç†è§£ã€å»ºç«‹åº”ç”¨å’Œæ¨åŠ¨GNN4TSçš„ç ”ç©¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æä¾›äº†GNN4TSçš„ä»»åŠ¡ oriented åˆ†ç±»ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»‹ç»äº†ä»£è¡¨æ€§çš„ç ”ç©¶å·¥ä½œå’Œä¸»æµåº”ç”¨GNN4TSã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„æœªæ¥ç ”ç©¶æ–¹å‘çš„è®¨è®ºï¼Œä»¥å¸®åŠ©è¯»è€…æ›´å¥½åœ°ç†è§£GNN-basedæ—¶é—´åºåˆ—ç ”ç©¶çš„åŸºç¡€ã€å®è·µå’Œæœªæ¥å‘å±•ã€‚è¿™æ˜¯é¦–æ¬¡å°†GNN-basedæ—¶é—´åºåˆ—ç ”ç©¶æ±‡æ€»èµ·æ¥ï¼ŒæŠŠæ¶‰åŠçš„çŸ¥è¯†é›†ä¸­èµ·æ¥ï¼Œæ¨åŠ¨ Graph Neural Networks for Time Series Analysisçš„ç ”ç©¶ã€‚
</details></li>
</ul>
<hr>
<h2 id="Towards-Deep-Network-Steganography-From-Networks-to-Networks"><a href="#Towards-Deep-Network-Steganography-From-Networks-to-Networks" class="headerlink" title="Towards Deep Network Steganography: From Networks to Networks"></a>Towards Deep Network Steganography: From Networks to Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03444">http://arxiv.org/abs/2307.03444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guobiao Li, Sheng Li, Meiling Li, Zhenxing Qian, Xinpeng Zhang</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦é’ˆå¯¹çš„æ˜¯å¦‚ä½•åœ¨å…¬å…±é€šé“ä¸­éšè—æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯é‚£äº›è®­ç»ƒç”¨äºæœºå¯†å­¦ä¹ ä»»åŠ¡çš„æ¨¡å‹ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ·±åº¦ç½‘ç»œéšè—ï¼ˆDeep Network Steganographyï¼ŒDNSï¼‰ï¼Œå°†æœºå¯†çš„DNNæ¨¡å‹è½¬æ¢ä¸ºä¸€ä¸ªæ™®é€šçš„å­¦ä¹ ä»»åŠ¡ã€‚è¿™æ˜¯ç”±äºæˆ‘ä»¬çš„æ–¹æ³•å°†æœºå¯†æ¨¡å‹ä¸­çš„ä¸€äº›é‡è¦ä½ç½®è£…é¥°æˆæ™®é€šçš„å­¦ä¹ ä½ç½®ï¼Œå¹¶å°†è¿™äº›ä½ç½®éšè—åœ¨ä¸€ä¸ªéšè—é¢‘é“ä¸­ã€‚</li>
<li>results: æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å®ç°éšè—DNNæ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„å­¦ä¹ ä»»åŠ¡ä¹‹é—´è¿›è¡Œéšè—ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åœ¨å†…éƒ¨ä»»åŠ¡éšè—ï¼ˆIntra-task steganographyï¼‰å’Œå¤šä»»åŠ¡éšè—ï¼ˆInter-task steganographyï¼‰ä¸¤ç§æƒ…å†µä¸‹å®ç°äº†éšè—DNNæ¨¡å‹çš„ç›®æ ‡ã€‚<details>
<summary>Abstract</summary>
With the widespread applications of the deep neural network (DNN), how to covertly transmit the DNN models in public channels brings us the attention, especially for those trained for secret-learning tasks. In this paper, we propose deep network steganography for the covert communication of DNN models. Unlike the existing steganography schemes which focus on the subtle modification of the cover data to accommodate the secrets, our scheme is learning task oriented, where the learning task of the secret DNN model (termed as secret-learning task) is disguised into another ordinary learning task conducted in a stego DNN model (termed as stego-learning task). To this end, we propose a gradient-based filter insertion scheme to insert interference filters into the important positions in the secret DNN model to form a stego DNN model. These positions are then embedded into the stego DNN model using a key by side information hiding. Finally, we activate the interference filters by a partial optimization strategy, such that the generated stego DNN model works on the stego-learning task. We conduct the experiments on both the intra-task steganography and inter-task steganography (i.e., the secret and stego-learning tasks belong to the same and different categories), both of which demonstrate the effectiveness of our proposed method for covert communication of DNN models.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œå¦‚ä½•åœ¨å…¬å…±é¢‘é“ä¸Šä¸æ˜¾åœ°ä¼ è¾“å·²è®­ç»ƒçš„DNNæ¨¡å‹å¼•å‘äº†å…³æ³¨ï¼Œå°¤å…¶æ˜¯é‚£äº›ç”¨äºç§˜å¯†å­¦ä¹ ä»»åŠ¡çš„æ¨¡å‹ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ·±åº¦ç½‘ç»œéšè—ï¼ˆDNNéšè—ï¼‰ï¼Œç”¨äºä¸æ˜¾åœ°é€šä¿¡DNNæ¨¡å‹ã€‚ä¸ç°æœ‰çš„éšè—æ–¹æ¡ˆä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆæ˜¯ä»»åŠ¡ orientedï¼Œå…¶ä¸­ç§˜å¯†å­¦ä¹ ä»»åŠ¡ï¼ˆç§˜å¯†å­¦ä¹ ä»»åŠ¡ï¼‰è¢«éšè—åˆ°å¦ä¸€ä¸ªæ™®é€šçš„å­¦ä¹ ä»»åŠ¡ï¼ˆéšè—å­¦ä¹ ä»»åŠ¡ï¼‰ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¢¯åº¦åŸºäºçš„ç­›é€‰æ’å…¥æ–¹æ¡ˆï¼Œå°†é‡è¦çš„ä½ç½®åœ¨ç§˜å¯†DNNæ¨¡å‹ä¸­æ’å…¥å¹²æ‰°ç­›é€‰å™¨ï¼Œå½¢æˆä¸€ä¸ªéšè—DNNæ¨¡å‹ã€‚è¿™äº›ä½ç½®ç„¶åè¢«åµŒå…¥åˆ°éšè—DNNæ¨¡å‹ä¸­ä½¿ç”¨é’¥åŒ™ï¼Œå¹¶ä¸”ä½¿ç”¨ä¾§ä¿¡æ¯éšè—ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨éƒ¨åˆ†ä¼˜åŒ–ç­–ç•¥å¯åŠ¨å¹²æ‰°ç­›é€‰å™¨ï¼Œä½¿å¾—ç”Ÿæˆçš„éšè—DNNæ¨¡å‹åœ¨éšè—å­¦ä¹ ä»»åŠ¡ä¸Šå·¥ä½œã€‚æˆ‘ä»¬å¯¹ä¸¤ç§æƒ…å†µè¿›è¡Œå®éªŒï¼šå†…ä»»åŠ¡éšè—ï¼ˆi.e., ç§˜å¯†ä»»åŠ¡å’Œéšè—å­¦ä¹ ä»»åŠ¡å±äºåŒä¸€ç±»ï¼‰å’Œé—´ä»»åŠ¡éšè—ï¼ˆi.e., ç§˜å¯†ä»»åŠ¡å’Œéšè—å­¦ä¹ ä»»åŠ¡å±äºä¸åŒç±»ï¼‰ï¼Œä¸¤è€…å‡æ˜¾ç¤ºäº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æ•ˆivenessã€‚
</details></li>
</ul>
<hr>
<h2 id="Non-iterative-Coarse-to-fine-Transformer-Networks-for-Joint-Affine-and-Deformable-Image-Registration"><a href="#Non-iterative-Coarse-to-fine-Transformer-Networks-for-Joint-Affine-and-Deformable-Image-Registration" class="headerlink" title="Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration"></a>Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03421">http://arxiv.org/abs/2307.03421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mungomeng/registration-nice-trans">https://github.com/mungomeng/registration-nice-trans</a></li>
<li>paper_authors: Mingyuan Meng, Lei Bi, Michael Fulham, Dagan Feng, Jinman Kim</li>
<li>for: è¿™paperæ˜¯ä¸ºäº†æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„éè¿­ä»£ç²—ç»†åˆ°ç»†ç²’åº¦å›¾åƒåŒ¹é…ç®—æ³•ã€‚</li>
<li>methods: è¿™paperä½¿ç”¨äº†ä¸€ç§åä¸ºNICE-Transçš„éè¿­ä»£ç²—ç»†åˆ°ç»†ç²’åº¦å›¾åƒåŒ¹é…ç½‘ç»œï¼Œè¯¥ç½‘ç»œç»“åˆäº†çŸ©é˜µå˜æ¢å’Œæ‰©å±•æŠ½å–å™¨æ¥å®ç°ç²—ç»†åˆ°ç»†ç²’åº¦çš„å›¾åƒåŒ¹é…ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒNICE-Transå¯ä»¥åœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šå‡»è´¥ç°æœ‰çš„å›¾åƒåŒ¹é…æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ³¨é‡ç²¾åº¦å’Œè¿è¡Œæ—¶é—´ä¹‹é—´å–å¾—äº†ä¸€ä¸ªè‰¯å¥½çš„å¹³è¡¡ã€‚<details>
<summary>Abstract</summary>
Image registration is a fundamental requirement for medical image analysis. Deep registration methods based on deep learning have been widely recognized for their capabilities to perform fast end-to-end registration. Many deep registration methods achieved state-of-the-art performance by performing coarse-to-fine registration, where multiple registration steps were iterated with cascaded networks. Recently, Non-Iterative Coarse-to-finE (NICE) registration methods have been proposed to perform coarse-to-fine registration in a single network and showed advantages in both registration accuracy and runtime. However, existing NICE registration methods mainly focus on deformable registration, while affine registration, a common prerequisite, is still reliant on time-consuming traditional optimization-based methods or extra affine registration networks. In addition, existing NICE registration methods are limited by the intrinsic locality of convolution operations. Transformers may address this limitation for their capabilities to capture long-range dependency, but the benefits of using transformers for NICE registration have not been explored. In this study, we propose a Non-Iterative Coarse-to-finE Transformer network (NICE-Trans) for image registration. Our NICE-Trans is the first deep registration method that (i) performs joint affine and deformable coarse-to-fine registration within a single network, and (ii) embeds transformers into a NICE registration framework to model long-range relevance between images. Extensive experiments with seven public datasets show that our NICE-Trans outperforms state-of-the-art registration methods on both registration accuracy and runtime.
</details>
<details>
<summary>æ‘˜è¦</summary>
åŒ»ç–—å½±åƒåˆ†æä¸­çš„å›¾åƒ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñæ˜¯ä¸€é¡¹åŸºæœ¬è¦æ±‚ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„æ·±åº¦ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñæ–¹æ³•åœ¨æœ€è¿‘å‡ å¹´å†…å¾—åˆ°äº†å¹¿æ³›çš„è®¤å¯ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥å¿«é€Ÿå®Œæˆç«¯åˆ°ç«¯çš„ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñã€‚è®¸å¤šæ·±åº¦REGISTRATIONæ–¹æ³•åœ¨å¤šä¸ªREGISTRATIONæ­¥éª¤ä¸­é‡‡ç”¨äº†éšå¼çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œä»¥å®ç°ç²—ç»†åˆ°ç»†èŠ‚çš„REGISTRATIONã€‚ç„¶è€Œï¼Œç°æœ‰çš„NICEREGISTRATIONæ–¹æ³•ä¸»è¦å…³æ³¨äºå¼¹æ€§REGISTRATIONï¼Œè€Œå¹³ç§»REGISTRATIONï¼Œæ˜¯åŒ»ç–—å½±åƒåˆ†æä¸­éå¸¸å¸¸è§çš„å‰æï¼Œä»ç„¶æ˜¯é€šè¿‡æ—¶é—´æ¶ˆè€—çš„ä¼ ç»Ÿä¼˜åŒ–æ–¹æ³•æˆ–é¢å¤–çš„å¹³ç§»REGISTRATIONç½‘ç»œæ¥å®ç°ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„NICEREGISTRATIONæ–¹æ³•å—åˆ°å·ç§¯ç¥ç»ç½‘ç»œçš„æœ¬è´¨æ€§å±€éƒ¨æ€§çš„é™åˆ¶ã€‚ä½¿ç”¨å˜æ¢å™¨å¯èƒ½è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æ•æ‰å›¾åƒä¹‹é—´çš„é•¿è·ç¦»ç›¸å…³æ€§ã€‚ä½†æ˜¯ï¼Œä½¿ç”¨å˜æ¢å™¨æ¥è¿›è¡ŒNICEREGISTRATIONçš„å¥½å¤„å°šæœªå¾—åˆ°äº†è¶³å¤Ÿçš„æ¢è®¨ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Non-Iterative Coarse-to-finE Transformerç½‘ç»œï¼ˆNICE-Transï¼‰ï¼Œç”¨äºå›¾åƒREGISTRATIONã€‚æˆ‘ä»¬çš„NICE-Transæ˜¯ç¬¬ä¸€ä¸ªåœ¨å•ä¸ªç½‘ç»œä¸­åŒæ—¶å®ç°äº†å¹³ç§»å’Œå¼¹æ€§çš„ç²—ç»†åˆ°ç»†èŠ‚REGISTRATIONï¼Œä»¥åŠåœ¨NICEREGISTRATIONæ¡†æ¶ä¸­ä½¿ç”¨å˜æ¢å™¨æ¥æ¨¡å‹å›¾åƒä¹‹é—´çš„é•¿è·ç¦»ç›¸å…³æ€§ã€‚æˆ‘ä»¬å¯¹ä¸ƒä¸ªå…¬å…±æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„NICE-Transåœ¨REGISTRATIONç²¾åº¦å’Œè¿è¡Œæ—¶é—´ä¸¤ä¸ªæ–¹é¢éƒ½è¶…è¿‡äº†å½“å‰çš„REGISTRATIONæ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="QI2-â€“-an-Interactive-Tool-for-Data-Quality-Assurance"><a href="#QI2-â€“-an-Interactive-Tool-for-Data-Quality-Assurance" class="headerlink" title="QI2 â€“ an Interactive Tool for Data Quality Assurance"></a>QI2 â€“ an Interactive Tool for Data Quality Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03419">http://arxiv.org/abs/2307.03419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Geerkens, Christian Sieberichs, Alexander Braun, Thomas Waschulzik</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜æœºå™¨å­¦ä¹ ç³»ç»Ÿå’Œå¤§æ•°æ®çš„æ•°æ®è´¨é‡ï¼Œä»¥æ»¡è¶³æ¬§æ´²å§”å‘˜ä¼šçš„AIæ³•æ¡ˆçš„æ•°æ®è´¨é‡è¦æ±‚ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®è´¨é‡æ£€æŸ¥æ–¹æ³•ï¼Œå¯ä»¥æ£€æŸ¥å¤šä¸ªæ•°æ®è´¨é‡æ–¹é¢çš„æ•°æ®ã€‚è¿™ç§æ–¹æ³•å¯ä»¥é‡åŒ–æ•°æ®è´¨é‡è¦æ±‚ï¼Œå¹¶åœ¨å°ä¾‹å­æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æ•ˆæœã€‚</li>
<li>results: æœ¬ç ”ç©¶åœ¨well known MNISTæ•°æ®é›†ä¸Šåº”ç”¨äº†è¿™ç§æ–¹æ³•ï¼Œå¹¶é€šè¿‡ç¤ºä¾‹æ•°æ®é›†å±•ç¤ºäº†å…¶å·¥ä½œåŸç†å’Œä¼˜åŠ¿ã€‚<details>
<summary>Abstract</summary>
The importance of high data quality is increasing with the growing impact and distribution of ML systems and big data. Also the planned AI Act from the European commission defines challenging legal requirements for data quality especially for the market introduction of safety relevant ML systems. In this paper we introduce a novel approach that supports the data quality assurance process of multiple data quality aspects. This approach enables the verification of quantitative data quality requirements. The concept and benefits are introduced and explained on small example data sets. How the method is applied is demonstrated on the well known MNIST data set based an handwritten digits.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œé«˜å“è´¨æ•°æ®çš„é‡è¦æ€§åœ¨æœºå™¨å­¦ä¹ ç³»ç»Ÿå’Œå¤§æ•°æ®çš„æ™®åŠå’Œå½±å“åŠ›å¢é•¿ä¹‹é™…æ—¥ç›Šå¢åŠ ã€‚æ¬§ç›Ÿå§”å‘˜ä¼šçš„AIæ³•æ¡ˆä¹Ÿå°†æå‡ºä¸¥æ ¼çš„æ³•å¾‹è¦æ±‚ï¼Œå°¤å…¶æ˜¯åœ¨å®‰å…¨ç›¸å…³çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸Šã€‚æœ¬æ–‡ä»‹ç»ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä»¥æ”¯æŒå¤šç§æ•°æ®è´¨é‡å±‚é¢çš„è´¨é‡ç¡®ä¿è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•å¯ä»¥verifyæ•°æ®è´¨é‡çš„é‡åŒ–è¦æ±‚ã€‚æœ¬æ–‡å°† introduceå’Œè§£é‡Šè¿™ä¸ªæ¦‚å¿µï¼Œå¹¶ä½¿ç”¨å°å‹ç¤ºä¾‹æ•°æ®é›†æ¥è¯´æ˜å…¶å·¥ä½œæ–¹å¼ã€‚åœ¨è‘—åçš„MNISTæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬å°†è¯´æ˜å¦‚ä½•åº”ç”¨è¿™ä¸ªæ–¹æ³•ã€‚â€Here's the translation in Traditional Chinese:â€œé«˜å“è´¨æ•°æ®çš„é‡è¦æ€§åœ¨æœºå™¨å­¦ä¹ ç³»ç»Ÿå’Œå¤§æ•°æ®çš„æ™®åŠå’Œå½±å“åŠ›å¢é•¿ä¹‹é™…æ—¥ç›Šå¢åŠ ã€‚æ¬§ç›Ÿå§”å‘˜ä¼šçš„AIæ³•æ¡ˆä¹Ÿå°†æå‡ºä¸¥æ ¼çš„æ³•å¾‹è¦æ±‚ï¼Œå°¤å…¶æ˜¯åœ¨å®‰å…¨ç›¸å…³çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸Šã€‚æœ¬æ–‡ä»‹ç»ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä»¥æ”¯æŒå¤šç§æ•°æ®è´¨é‡å±‚é¢çš„è´¨é‡ç¡®ä¿è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•å¯ä»¥verifyæ•°æ®è´¨é‡çš„é‡åŒ–è¦æ±‚ã€‚æœ¬æ–‡å°† introduceå’Œè§£é‡Šè¿™ä¸ªæ¦‚å¿µï¼Œå¹¶ä½¿ç”¨å°å‹ç¤ºä¾‹æ•°æ®é›†æ¥è¯´æ˜å…¶å·¥ä½œæ–¹å¼ã€‚åœ¨è‘—åçš„MNISTæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬å°†è¯´æ˜å¦‚ä½•åº”ç”¨è¿™ä¸ªæ–¹æ³•ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Goal-Conditioned-Predictive-Coding-as-an-Implicit-Planner-for-Offline-Reinforcement-Learning"><a href="#Goal-Conditioned-Predictive-Coding-as-an-Implicit-Planner-for-Offline-Reinforcement-Learning" class="headerlink" title="Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning"></a>Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03406">http://arxiv.org/abs/2307.03406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilai Zeng, Ce Zhang, Shijie Wang, Chen Sun</li>
<li>for: ç ”ç©¶ whether sequence modeling can condense trajectories into useful representations for policy learning.</li>
<li>methods: é‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé¦–å…ˆä½¿ç”¨åºåˆ—æ¨¡å‹æŠ€æœ¯ç®€åŒ–è½¨è¿¹æ•°æ®ï¼Œç„¶åä½¿ç”¨è¿™äº›è¡¨ç¤ºå­¦ä¹ ç­–ç•¥å’Œæ„¿æ™¯ã€‚</li>
<li>results: åœ¨AntMazeã€FrankaKitchenå’ŒLocomotionç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå‘ç°åºåˆ—æ¨¡å‹å¯¹å†³ç­–ä»»åŠ¡æœ‰æ˜¾è‘—å½±å“ï¼Œå¹¶ä¸”GCPCå­¦ä¹ äº†ä¸€ä¸ªç›®æ ‡çŠ¶æ€ç›¸å…³çš„å«ä¹‰ reprehenstionï¼Œå…·æœ‰ç«äº‰æ€§çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Recent work has demonstrated the effectiveness of formulating decision making as a supervised learning problem on offline-collected trajectories. However, the benefits of performing sequence modeling on trajectory data is not yet clear. In this work we investigate if sequence modeling has the capability to condense trajectories into useful representations that can contribute to policy learning. To achieve this, we adopt a two-stage framework that first summarizes trajectories with sequence modeling techniques, and then employs these representations to learn a policy along with a desired goal. This design allows many existing supervised offline RL methods to be considered as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful trajectory representations and leads to performant policies. We conduct extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, and observe that sequence modeling has a significant impact on some decision making tasks. In addition, we demonstrate that GCPC learns a goal-conditioned latent representation about the future, which serves as an "implicit planner", and enables competitive performance on all three benchmarks.
</details>
<details>
<summary>æ‘˜è¦</summary>
To achieve this, we use a two-stage framework that first summarizes trajectories using sequence modeling techniques and then employs these representations to learn a policy along with a desired goal. This design allows many existing supervised offline RL methods to be considered as specific instances of our framework.Within this framework, we introduce Goal-Conditioned Predictive Coding (GCPC), an approach that provides powerful trajectory representations and leads to performant policies. We conduct extensive empirical evaluations on AntMaze, FrankaKitchen, and Locomotion environments and find that sequence modeling has a significant impact on some decision-making tasks. Additionally, we demonstrate that GCPC learns a goal-conditioned latent representation of the future, which serves as an "implicit planner" and enables competitive performance on all three benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Potential-of-Large-Language-Models-LLMs-in-Learning-on-Graphs"><a href="#Exploring-the-Potential-of-Large-Language-Models-LLMs-in-Learning-on-Graphs" class="headerlink" title="Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs"></a>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03393">http://arxiv.org/abs/2307.03393</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CurryTang/Graph-LLM">https://github.com/CurryTang/Graph-LLM</a></li>
<li>paper_authors: Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang</li>
<li>for: æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å›¾æœºå™¨å­¦ä¹ ä¸­çš„æ½œåœ¨ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸­çš„ä¸¤ç§å¯èƒ½çš„ç®¡é“ï¼šLLMs-as-Enhancers å’Œ LLMs-as-Predictorsã€‚</li>
<li>methods: æœ¬æ–‡é‡‡ç”¨äº†ä¸¤ç§ç®¡é“è¿›è¡Œç ”ç©¶ï¼šä¸€æ˜¯ä½¿ç”¨ LLMs å¢å¼ºèŠ‚ç‚¹çš„æ–‡æœ¬ç‰¹å¾ï¼Œç„¶åé€šè¿‡ GNNs è¿›è¡Œé¢„æµ‹ï¼›äºŒæ˜¯ç›´æ¥ä½¿ç”¨ LLMs ä½œä¸ºç‹¬ç«‹é¢„æµ‹å™¨ã€‚</li>
<li>results: ç»è¿‡ç³»ç»Ÿçš„å®éªŒç ”ç©¶ï¼Œæœ¬æ–‡å‘ç°äº†ä¸€äº›åŸåˆ›çš„è§‚å¯Ÿå’Œæ–°çš„å‘ç°ï¼ŒåŒ…æ‹¬ä½¿ç”¨ LLMs å¯ä»¥æé«˜èŠ‚ç‚¹åˆ†ç±»çš„å‡†ç¡®ç‡å’Œæé«˜ GNNs çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at https://github.com/CurryTang/Graph-LLM.
</details>
<details>
<summary>æ‘˜è¦</summary>
å­¦ä¹ å›¾æœ‰å¸å¼•äº†å·¨å¤§çš„æ³¨æ„åŠ›ï¼Œå› ä¸ºå®ƒåœ¨å®é™…åº”ç”¨ä¸­æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœ€å—æ¬¢è¿çš„å›¾å­¦ä¹ ç®¡é“æ˜¯ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬èŠ‚ç‚¹ç‰¹å¾çš„æµ…å±‚åµŒå…¥ï¼Œä½†è¿™æœ‰é™åˆ¶åœ¨æ€»ä½“çŸ¥è¯†å’Œæ·±åˆ»Semanticç†è§£æ–¹é¢ã€‚åœ¨æœ€è¿‘å‡ å¹´ï¼Œå¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»è¢«è¯æ˜å…·æœ‰å¹¿æ³›çš„é€šç”¨çŸ¥è¯†å’Œå¼ºå¤§çš„Semanticç†è§£èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›åœ¨å¤„ç†æ–‡æœ¬æ•°æ®æ–¹é¢å·²ç»å¼•èµ·äº†é©å‘½ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦æ¢ç´¢LLMsåœ¨å›¾æœºå™¨å­¦ä¹ ä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ï¼Œå¹¶ç ”ç©¶ä¸¤ç§å¯èƒ½çš„ç®¡é“ï¼šLLMs-as-Enhancerså’ŒLLMs-as-Predictorsã€‚å‰è€…åˆ©ç”¨LLMsæ¥å¢å¼ºèŠ‚ç‚¹çš„æ–‡æœ¬ç‰¹å¾ï¼Œç„¶åé€šè¿‡GNNsç”Ÿæˆé¢„æµ‹ã€‚åè€…å°è¯•ç›´æ¥ä½¿ç”¨LLMsä½œä¸ºç‹¬ç«‹é¢„æµ‹å™¨ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„è®¾ç½®ä¸‹è¿›è¡Œäº†ç³»ç»Ÿçš„ç ”ç©¶ï¼Œä»å¹¿æ³›çš„å®éªŒç»“æœä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°äº†åŸåˆ›çš„è§‚å¯Ÿå’Œæ–°çš„å‘ç°ï¼Œè¿™äº›å‘ç°å¼€å¯äº†æ–°çš„å¯èƒ½æ€§å’Œå»ºè®®ï¼Œå¹¶æŒ‡å‘äº†å¯ä»¥åˆ©ç”¨LLMsæ¥å­¦ä¹ å›¾çš„æ–°çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯ä»¥åœ¨https://github.com/CurryTang/Graph-LLMä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="On-Formal-Feature-Attribution-and-Its-Approximation"><a href="#On-Formal-Feature-Attribution-and-Its-Approximation" class="headerlink" title="On Formal Feature Attribution and Its Approximation"></a>On Formal Feature Attribution and Its Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03380">http://arxiv.org/abs/2307.03380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ffattr/ffa">https://github.com/ffattr/ffa</a></li>
<li>paper_authors: Jinqiang Yu, Alexey Ignatiev, Peter J. Stuckey</li>
<li>for: æé«˜å½¢å¼XAIçš„åº”ç”¨èŒƒå›´å’Œæ•ˆèƒ½ï¼Œå¯¹feature attributionè¿›è¡Œæ­£å¼é˜æ˜å’Œè¯„ä¼°ã€‚</li>
<li>methods: åŸºäºæ­£å¼é˜æ˜æ•°å­¦åŸºç¡€çš„feature attributionæ–¹æ³•ï¼Œä½¿ç”¨æ­£å¼é˜æ˜åˆ†æå™¨æ¶æ„ï¼Œå¹¶æå‡ºä¸€ä¸ªç®€æ´çš„å½¢å¼é˜æ˜æ–¹æ³•ã€‚</li>
<li>results: åœ¨å®éªŒä¸­ï¼Œæå‡ºçš„ç®€æ´å½¢å¼é˜æ˜æ–¹æ³•å¯ä»¥å®ç°é«˜ç²¾åº¦çš„feature attributionï¼Œå¹¶ä¸”æ¯”ä»¥å¾€çš„æ–¹æ³•æ›´å…·æœ‰å®ç”¨æ€§å’Œå¯scalabilityã€‚<details>
<summary>Abstract</summary>
Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include feature selection methods, e.g. Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution problem. Additionally, a formal explanation despite being formally sound is typically quite large, which hampers its applicability in practical settings. Motivated by the above, this paper proposes a way to apply the apparatus of formal XAI to the case of feature attribution based on formal explanation enumeration. Formal feature attribution (FFA) is argued to be advantageous over the existing methods, both formal and non-formal. Given the practical complexity of the problem, the paper then proposes an efficient technique for approximating exact FFA. Finally, it offers experimental evidence of the effectiveness of the proposed approximate FFA in comparison to the existing feature attribution algorithms not only in terms of feature importance and but also in terms of their relative order.
</details>
<details>
<summary>æ‘˜è¦</summary>
Motivated by these limitations, this paper proposes a way to apply formal XAI to feature attribution based on formal explanation enumeration. Formal feature attribution (FFA) is argued to be advantageous over existing methods, both formal and non-formal. Given the practical complexity of the problem, the paper proposes an efficient technique for approximating exact FFA. Finally, it offers experimental evidence of the effectiveness of the proposed approximate FFA in comparison to existing feature attribution algorithms in terms of feature importance and relative order.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Ground-Vehicle-Path-Following-in-Game-AI"><a href="#Efficient-Ground-Vehicle-Path-Following-in-Game-AI" class="headerlink" title="Efficient Ground Vehicle Path Following in Game AI"></a>Efficient Ground Vehicle Path Following in Game AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03379">http://arxiv.org/abs/2307.03379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rodrigue de Schaetzen, Alessandro Sestini</li>
<li>for: è¿™ç¯‡ç ”ç©¶ç›®çš„æ˜¯ä¸ºæ¸¸æˆAIä¸­çš„åœ°é¢è½¦è¾†è®¾è®¡ä¸€ä¸ªé«˜æ•ˆçš„è·¯å¾„è¿½è¸ªè§£å†³æ–¹æ¡ˆã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨å·²æœ‰æŠ€æœ¯åŠ ä»¥æ”¹è¿›ï¼Œè®¾è®¡äº†ä¸€ä¸ªç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶è°ƒæ•´å‚æ•°ä»¥è·å¾—é«˜æ•ˆçš„benchmarkè·¯å¾„è¿½è¸ªå™¨ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆç‰¹åˆ«æ³¨é‡è®¡ç®—è·¯å¾„æ›²ç‡çš„ quadratic Bezier æ›²çº¿ã€‚</li>
<li>results: æˆ‘ä»¬é€è¿‡åœ¨ä¸€ä¸ªé¦–äººå°„å‡»æ¸¸æˆä¸­è¿›è¡Œäº†å¤šç§æµ‹è¯•enarioï¼Œè¯„ä¼°äº†ææ¡ˆçš„è·¯å¾„è¿½è¸ªå™¨çš„æ•ˆæœå’Œå¯é æ€§ã€‚ä¸ç°æœ‰çš„è·¯å¾„è¿½è¸ªè§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼Œæˆ‘ä»¬è·å¾—äº†70%çš„ç¼©å‡åœ¨ç»Ÿè®¡ä¸Šçš„å›°éš¾äº‹ä»¶ã€‚<details>
<summary>Abstract</summary>
This short paper presents an efficient path following solution for ground vehicles tailored to game AI. Our focus is on adapting established techniques to design simple solutions with parameters that are easily tunable for an efficient benchmark path follower. Our solution pays particular attention to computing a target speed which uses quadratic Bezier curves to estimate the path curvature. The performance of the proposed path follower is evaluated through a variety of test scenarios in a first-person shooter game, demonstrating its effectiveness and robustness in handling different types of paths and vehicles. We achieved a 70% decrease in the total number of stuck events compared to an existing path following solution.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="All-in-One-Exploring-Unified-Vision-Language-Tracking-with-Multi-Modal-Alignment"><a href="#All-in-One-Exploring-Unified-Vision-Language-Tracking-with-Multi-Modal-Alignment" class="headerlink" title="All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment"></a>All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03373">http://arxiv.org/abs/2307.03373</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/983632847/All-in-One">https://github.com/983632847/All-in-One</a></li>
<li>paper_authors: Chunhui Zhang, Xin Sun, Li Liu, Yiqian Yang, Qiong Liu, Xi Zhou, Yanfeng Wang</li>
<li>for: æé«˜è§†è§‰è¯­è¨€è·Ÿè¸ªå™¨çš„æ€§èƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„åœºæ™¯ï¼Œå¦‚åŒæºæ‰°åŠ¨å’Œæç«¯ç…§æ˜ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ä¸ªAll-in-Oneæ¡†æ¶ï¼Œå°†è§†è§‰å’Œè¯­è¨€ä¿¡å·ç›´æ¥æ··åˆï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªç»Ÿä¸€çš„å˜æ¢å—æ¥å­¦ä¹ ååŒæå–å’Œäº¤äº’ã€‚è¿˜å¼•å…¥äº†ä¸€ç§å¤šModalåŒ¹é…æ¨¡å—ï¼Œä½¿ç”¨äº¤å‰modalå’Œè‡ªmodalå¯¹æ¯”ç›®æ ‡æ¥æä¾›æ›´æœ‰ç†æ€§çš„è¡¨ç¤ºã€‚</li>
<li>results: ç»è¿‡å¹¿æ³›çš„å®éªŒï¼Œåœ¨äº”ä¸ª benchmarkä¸Šéƒ½è¾¾åˆ°äº†ç°æœ‰çŠ¶æ€ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹æ™ºèƒ½çš„æœ€é«˜æ°´å¹³ï¼Œå¹¶ä¸”æ¯”ä¹‹å‰çš„æ–¹æ³•æ›´åŠ é«˜æ•ˆå’Œå¯é ã€‚<details>
<summary>Abstract</summary>
Current mainstream vision-language (VL) tracking framework consists of three parts, \ie a visual feature extractor, a language feature extractor, and a fusion model. To pursue better performance, a natural modus operandi for VL tracking is employing customized and heavier unimodal encoders, and multi-modal fusion models. Albeit effective, existing VL trackers separate feature extraction and feature integration, resulting in extracted features that lack semantic guidance and have limited target-aware capability in complex scenarios, \eg similar distractors and extreme illumination. In this work, inspired by the recent success of exploring foundation models with unified architecture for both natural language and computer vision tasks, we propose an All-in-One framework, which learns joint feature extraction and interaction by adopting a unified transformer backbone. Specifically, we mix raw vision and language signals to generate language-injected vision tokens, which we then concatenate before feeding into the unified backbone architecture. This approach achieves feature integration in a unified backbone, removing the need for carefully-designed fusion modules and resulting in a more effective and efficient VL tracking framework. To further improve the learning efficiency, we introduce a multi-modal alignment module based on cross-modal and intra-modal contrastive objectives, providing more reasonable representations for the unified All-in-One transformer backbone. Extensive experiments on five benchmarks, \ie OTB99-L, TNL2K, LaSOT, LaSOT$_{\rm Ext}$ and WebUAV-3M, demonstrate the superiority of the proposed tracker against existing state-of-the-arts on VL tracking. Codes will be made publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
å½“å‰ä¸»æµè§†è§‰è¯­è¨€ï¼ˆVLï¼‰è·Ÿè¸ªæ¡†æ¶åŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼šè§†è§‰ç‰¹å¾æå–å™¨ã€è¯­è¨€ç‰¹å¾æå–å™¨å’Œ fusions æ¨¡å‹ã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼Œå¸¸è§çš„VLè·Ÿè¸ªæ–¹æ³•æ˜¯é‡‡ç”¨è‡ªå®šä¹‰å’Œæ›´é‡çš„å•æ¨¡æ€ç¼–ç å™¨ï¼Œä»¥åŠå¤šæ¨¡æ€èåˆæ¨¡å‹ã€‚è™½ç„¶æœ‰æ•ˆï¼Œç°æœ‰VLè·Ÿè¸ªå™¨åœ¨ç‰¹å¾æå–å’Œç‰¹å¾èåˆä¹‹é—´åˆ†ç¦»ï¼Œå¯¼è‡´æå–å‡ºçš„ç‰¹å¾ç¼ºä¹ semantic æŒ‡å¯¼å’Œå…·æœ‰æœ‰é™çš„ç›®æ ‡æ„è¯†èƒ½åŠ›åœ¨å¤æ‚æƒ…å†µä¸‹ï¼Œä¾‹å¦‚ç±»ä¼¼å¹²æ‰°å’Œæç«¯ç…§æ˜ã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬Draw inspiration from the recent success of exploring foundation models with unified architecture for both natural language and computer vision tasksï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªAll-in-Oneæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡é‡‡ç”¨ç»Ÿä¸€çš„ transformer è„Šæ¢å­¦ä¹ è”åˆç‰¹å¾æå–å’Œäº¤äº’ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†åŸå§‹è§†è§‰å’Œè¯­è¨€ä¿¡å·æ··åˆç”Ÿæˆè¯­è¨€æ³¨å…¥è§†è§‰ Ñ‚Ğ¾ĞºĞµĞ½ï¼Œç„¶åå°†è¿™äº› Ñ‚Ğ¾ĞºĞµĞ½ concatenate åœ¨ç»Ÿä¸€è„Šæ¢æ¶æ„ä¸­ã€‚è¿™ç§æ–¹æ³•å®ç°äº†ç‰¹å¾èåˆåœ¨ç»Ÿä¸€è„Šæ¢ä¸­ï¼Œä»è€ŒåºŸå¼ƒäº†éœ€è¦ precisely è®¾è®¡èåˆæ¨¡å—ï¼Œå¹¶ä¸”å¯¼è‡´æ›´æœ‰æ•ˆå’Œé«˜æ•ˆçš„VLè·Ÿè¸ªæ¡†æ¶ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å­¦ä¹ æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºäº¤å‰æ¨¡å¼å’Œå†…éƒ¨å¯¹æ¯”ç›®æ ‡çš„å¤šæ¨¡æ€åŒ¹é…æ¨¡å—ï¼Œä¸ºç»Ÿä¸€ All-in-One transformer è„Šæ¢æä¾›æ›´åˆç†çš„è¡¨ç¤ºã€‚å¹¿æ³›çš„å®éªŒåœ¨äº”ä¸ªæ ‡å‡†æµ‹è¯•é›†ï¼Œå³ OTB99-Lã€TNL2Kã€LaSOTã€LaSOT$_{\rm Ext}$ å’Œ WebUAV-3M ä¸Šï¼Œè¯æ˜æˆ‘ä»¬çš„è·Ÿè¸ªå™¨åœ¨VLè·Ÿè¸ªä¸­è¶…è¿‡ç°æœ‰çŠ¶å†µã€‚ä»£ç å°†å…¬å¼€ã€‚
</details></li>
</ul>
<hr>
<h2 id="Adaptation-and-Communication-in-Human-Robot-Teaming-to-Handle-Discrepancies-in-Agentsâ€™-Beliefs-about-Plans"><a href="#Adaptation-and-Communication-in-Human-Robot-Teaming-to-Handle-Discrepancies-in-Agentsâ€™-Beliefs-about-Plans" class="headerlink" title="Adaptation and Communication in Human-Robot Teaming to Handle Discrepancies in Agentsâ€™ Beliefs about Plans"></a>Adaptation and Communication in Human-Robot Teaming to Handle Discrepancies in Agentsâ€™ Beliefs about Plans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03362">http://arxiv.org/abs/2307.03362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuening Zhang, Brian C. Williams</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³äººæœºå›¢é˜Ÿä¸­agentä¹‹é—´ä¸å…·å¤‡å…±åŒè®¤çŸ¥çš„é—®é¢˜ï¼Œå³agentå¯èƒ½éµå¾ªä¸åŒçš„ä¹ æƒ¯æˆ–åªæœ‰ä¸€äº›agentçŸ¥é“çš„å¯èƒ½æ€§ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨epistemicé€»è¾‘æ¥å¸®åŠ©agentç†è§£å¯¹æ–¹çš„ä¿¡å¿µä¸åŒï¼Œå¹¶åŠ¨æ€è®¡åˆ’è¡ŒåŠ¨ä»¥é€‚åº”æˆ–é€šä¿¡ä»¥è§£å†³è¿™äº›ä¸åŒã€‚</li>
<li>results: æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å¯ä»¥æé«˜äººæœºå›¢é˜Ÿçš„æˆåŠŸç‡å’Œæ‰©å±•æ€§ï¼Œè€Œä¸éœ€è¦å…±åŒè®¤çŸ¥ã€‚<details>
<summary>Abstract</summary>
When agents collaborate on a task, it is important that they have some shared mental model of the task routines -- the set of feasible plans towards achieving the goals. However, in reality, situations often arise that such a shared mental model cannot be guaranteed, such as in ad-hoc teams where agents may follow different conventions or when contingent constraints arise that only some agents are aware of. Previous work on human-robot teaming has assumed that the team has a set of shared routines, which breaks down in these situations. In this work, we leverage epistemic logic to enable agents to understand the discrepancy in each other's beliefs about feasible plans and dynamically plan their actions to adapt or communicate to resolve the discrepancy. We propose a formalism that extends conditional doxastic logic to describe knowledge bases in order to explicitly represent agents' nested beliefs on the feasible plans and state of execution. We provide an online execution algorithm based on Monte Carlo Tree Search for the agent to plan its action, including communication actions to explain the feasibility of plans, announce intent, and ask questions. Finally, we evaluate the success rate and scalability of the algorithm and show that our agent is better equipped to work in teams without the guarantee of a shared mental model.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translation in Simplified Chinese:å½“æœºå™¨äººåˆä½œå®Œæˆä»»åŠ¡æ—¶ï¼Œé‡è¦çš„æ˜¯ä»–ä»¬æœ‰ä¸€ä¸ªå…±äº«çš„å¿ƒç†æ¨¡å‹ï¼Œå³ä»»åŠ¡routinesçš„å¯è¡Œæ–¹æ¡ˆé›†ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸­ï¼Œæƒ…å†µç»å¸¸å‡ºç°æ— æ³•ä¿è¯è¿™ç§å…±äº«å¿ƒç†æ¨¡å‹çš„æƒ…å†µï¼Œä¾‹å¦‚åœ¨åä½œå›¢é˜Ÿä¸­æœºå™¨äººå¯èƒ½éµå¾ªä¸åŒçš„ Conventionæˆ–è€…åœ¨ç‰¹æ®Šçš„æƒ…å†µä¸‹å­˜åœ¨åªæœ‰ä¸€äº›æœºå™¨äººçŸ¥é“çš„éšå¼çº¦æŸã€‚è¿‡å»çš„äººæœºåˆä½œå·¥ä½œå‡è®¾äº†å›¢é˜Ÿæœ‰ä¸€ç»„å…±äº«çš„routinesï¼Œè¿™ä¼šå¯¼è‡´é—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åˆ©ç”¨epistemicé€»è¾‘æ¥è®©æœºå™¨äººç†è§£å¯¹æ–¹å¯èƒ½çš„ä¿¡å¿µä¸åŒï¼Œå¹¶åœ¨è¿è¡Œæ—¶åŠ¨æ€è§„åˆ’è¡ŒåŠ¨ï¼Œä»¥é€‚åº”æˆ–é€šä¿¡è§£å†³è¿™äº›ä¸åŒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäº conditional doxasticé€»è¾‘çš„å½¢å¼æ¥æè¿°çŸ¥è¯†åº“ï¼Œä»¥æ˜¾å¼åœ°è¡¨ç¤ºæœºå™¨äººåµŒå¥—çš„ä¿¡å¿µç»“æ„ã€‚æˆ‘ä»¬æä¾›äº†åŸºäºMonte Carlo Tree Searchçš„åœ¨çº¿æ‰§è¡Œç®—æ³•ï¼Œè®©æœºå™¨äººåœ¨æ‰§è¡Œæ—¶è®¡åˆ’è¡ŒåŠ¨ï¼ŒåŒ…æ‹¬é€šä¿¡è¡ŒåŠ¨æ¥è§£é‡Šè®¡åˆ’çš„å¯è¡Œæ€§ã€å®£å¸ƒæ„å›¾å’Œæé—®ã€‚æœ€åï¼Œæˆ‘ä»¬è¯„ä¼°äº†ç®—æ³•çš„æˆåŠŸç‡å’Œå¯æ‰©å±•æ€§ï¼Œå¹¶æ˜¾ç¤ºæˆ‘ä»¬çš„æœºå™¨äººåœ¨ä¸å‡è®¾å…±äº«å¿ƒç†æ¨¡å‹çš„æƒ…å†µä¸‹æ›´èƒ½å¤Ÿåˆä½œã€‚
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Biased-Attitude-Associations-of-Language-Models-in-an-Intersectional-Context"><a href="#Evaluating-Biased-Attitude-Associations-of-Language-Models-in-an-Intersectional-Context" class="headerlink" title="Evaluating Biased Attitude Associations of Language Models in an Intersectional Context"></a>Evaluating Biased Attitude Associations of Language Models in an Intersectional Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03360">http://arxiv.org/abs/2307.03360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shivaomrani/llm-bias">https://github.com/shivaomrani/llm-bias</a></li>
<li>paper_authors: Shiva Omrani Sabbaghi, Robert Wolfe, Aylin Caliskan</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨ç ”ç©¶è‹±è¯­è¯­è¨€æ¨¡å‹ä¸­å„ç§ç¤¾ä¼šç¾¤ä½“çš„åè§ã€‚</li>
<li>methods: ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§å¥å­æ¨¡æ¿ï¼Œä»¥æä¾›å¤šå…ƒåŒ–çš„ç¤¾ä¼šèƒŒæ™¯ï¼Œä»¥è¯„ä¼°è¯­è¨€æ¨¡å‹ä¸­å„ç§ç¤¾ä¼šç¾¤ä½“çš„åè§ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œè¯­è¨€æ¨¡å‹å¯¹æ€§åˆ«è®¤åŒã€ç¤¾ä¼šé˜¶å±‚å’Œæ€§ orientationç­‰ç¤¾ä¼šç¾¤ä½“çš„åè§æœ€ä¸ºæ˜æ˜¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼Œæœ€å¤§å’Œæœ€é«˜æ€§èƒ½çš„è¯­è¨€æ¨¡å‹ä¹Ÿæ˜¯æœ€åè§çš„ã€‚<details>
<summary>Abstract</summary>
Language models are trained on large-scale corpora that embed implicit biases documented in psychology. Valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. Building on this established literature, we quantify how social groups are valenced in English language models using a sentence template that provides an intersectional context. We study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. We present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. Adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. We find that the largest and better-performing model that we study is also more biased as it effectively captures bias embedded in sociocultural data. We validate the bias evaluation method by overperforming on an intrinsic valence evaluation task. The approach enables us to measure complex intersectional biases as they are known to manifest in the outputs and applications of language models that perpetuate historical biases. Moreover, our approach contributes to design justice as it studies the associations of groups underrepresented in language such as transgender and homosexual individuals.
</details>
<details>
<summary>æ‘˜è¦</summary>
Language models are trained on large-scale corpora that embed implicit biases documented in psychology. Valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. Building on this established literature, we quantify how social groups are valenced in English language models using a sentence template that provides an intersectional context. We study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. We present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. Adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. We find that the largest and better-performing model that we study is also more biased as it effectively captures bias embedded in sociocultural data. We validate the bias evaluation method by overperforming on an intrinsic valence evaluation task. The approach enables us to measure complex intersectional biases as they are known to manifest in the outputs and applications of language models that perpetuate historical biases. Moreover, our approach contributes to design justice as it studies the associations of groups underrepresented in language such as transgender and homosexual individuals.Here's the translation in Traditional Chinese:è¯­æ¨¡å‹æ˜¯æ ¹æ®å¤§è§„æ¨¡æ•°æ®åº“è¿›è¡Œè®­ç»ƒï¼Œè¿™äº›æ•°æ®åº“ä¸­åµŒå…¥äº†å¿ƒç†å­¦ä¸­documentedçš„éšå¼åè§ã€‚åœ¨ç¤¾äº¤è®¤çŸ¥ä¸­ï¼Œç¤¾ä¼šç¾¤ä½“çš„æ€åº¦åå¥½ï¼ˆæ„‰æ‚¦åº¦/ä¸æ„‰æ‚¦åº¦ï¼‰determine the biased attitudes towards groups and conceptsã€‚æ ¹æ®å·²æœ‰çš„æ–‡çŒ®ï¼Œæˆ‘ä»¬é‡åŒ–è‹±è¯­è¯­æ¨¡å‹ä¸­ç¤¾ä¼šç¾¤ä½“çš„valence associationã€‚æˆ‘ä»¬ç ”ç©¶å¹´é¾„ã€æ•™è‚²ã€æ€§åˆ«ã€èº«é«˜ã€æ™ºå•†ã€æ–‡åŒ–ç¨‹åº¦ã€ç§æ—ã€å®—æ•™ã€æ€§åˆ«ã€æ€§å‘ã€ç¤¾ä¼šé˜¶å±‚å’Œèº«é«˜ç­‰ç¤¾ä¼šç¾¤ä½“çš„åè§ã€‚æˆ‘ä»¬ä½¿ç”¨ sentence template æä¾›çš„äº¤å‰sectional contextï¼Œä»¥ capture the valence subspace through contextualized word embeddings of language modelsã€‚æˆ‘ä»¬è¿ç”¨å¯¹åµŒå…¥åè§çš„æ–¹æ³•ï¼Œä»¥é‡åŒ–è¯­æ¨¡å‹å¯¹äºæ€§åˆ«è¯†åˆ«ã€ç¤¾ä¼šé˜¶å±‚å’Œæ€§å‘ä¿¡å·çš„åè§ã€‚æˆ‘ä»¬å‘ç°ï¼ŒLanguage models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in languageã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œæˆ‘ä»¬ç ”ç©¶çš„æœ€å¤§å’Œæœ€å¥½çš„æ¨¡å‹ä¹Ÿæ˜¯æœ€åè§çš„ï¼Œå› ä¸ºå®ƒå¾ˆå¥½åœ°æ•æ‰äº†ç¤¾ä¼šæ–‡åŒ–èµ„æ–™ä¸­çš„åè§ã€‚æˆ‘ä»¬éªŒè¯äº†åè§è¯„ä¼°æ–¹æ³•çš„æ­£ç¡®æ€§ï¼Œé€šè¿‡åœ¨å†…åœ¨æ„‰æ‚¦è¯„ä¼°ä»»åŠ¡ä¸­è¿›è¡Œè¿‡ performsã€‚è¿™ç§æ–¹æ³•å¯ä»¥é‡åŒ–å¤æ‚çš„äº¤å‰åè§ï¼Œå¹¶ä¸”å¯¹äºå†å²åè§çš„å»¶ç»­è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰è®¾è®¡æ­£ä¹‰çš„åŠŸèƒ½ï¼Œå› ä¸ºå®ƒç ”ç©¶äº†è¯­è¨€ä¸­underrepresentedçš„ç¾¤ä½“ï¼Œå¦‚ Ñ‚Ñ€Ğ°Ğ½ÑGENDERå’ŒåŒæ€§æ‹è€…ã€‚
</details></li>
</ul>
<hr>
<h2 id="TRAC-Trustworthy-Retrieval-Augmented-Chatbot"><a href="#TRAC-Trustworthy-Retrieval-Augmented-Chatbot" class="headerlink" title="TRAC: Trustworthy Retrieval Augmented Chatbot"></a>TRAC: Trustworthy Retrieval Augmented Chatbot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04642">http://arxiv.org/abs/2307.04642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Li, Sangdon Park, Insup Lee, Osbert Bastani</li>
<li>for: æé«˜é—®ç­”ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œå¯é æ€§</li>
<li>methods: ç»„åˆå¼ºåˆ¶é¢„æµ‹å’Œå…¨çƒæµ‹è¯•æ¥æä¾›ç»Ÿè®¡ä¿è¯ï¼Œå¹¶ä½¿ç”¨æ³Šåˆ©æŠ• optimize é€‰æ‹©å…¨çƒæµ‹è¯•çš„ Ğ³Ğ¸Ğ¿ĞµÑ€å‚æ•°ä»¥æœ€å¤§åŒ–ç³»ç»Ÿæ€§èƒ½</li>
<li>results: åœ¨ Natural Questions æ•°æ®é›†ä¸Šå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æä¾›é¢„æœŸçš„è¦†ç›–ä¿è¯ï¼ŒåŒæ—¶æœ€å°åŒ–å¹³å‡é¢„æµ‹é›†å¤§å°<details>
<summary>Abstract</summary>
Although conversational AIs have demonstrated fantastic performance, they often generate incorrect information, or hallucinations. Retrieval augmented generation has emerged as a promising solution to reduce these hallucinations. However, these techniques still cannot guarantee correctness. Focusing on question answering, we propose a framework that can provide statistical guarantees for the retrieval augmented question answering system by combining conformal prediction and global testing. In addition, we use Bayesian optimization to choose hyperparameters of the global test to maximize the performance of the system. Our empirical results on the Natural Questions dataset demonstrate that our method can provide the desired coverage guarantee while minimizing the average prediction set size.
</details>
<details>
<summary>æ‘˜è¦</summary>
Note:* "hallucinations" in the original text is translated as " incorrect information" in Simplified Chinese, as "hallucinations" is not a commonly used term in Chinese.* "retrieval augmented generation" is translated as " Retrieval å¢å¼ºç”Ÿæˆ" in Simplified Chinese, as "augmented" is not a commonly used term in Chinese.* "conformal prediction" is translated as "å‡†ç¡®é¢„æµ‹" in Simplified Chinese, as "conformal" is not a commonly used term in Chinese.* "global testing" is translated as "å…¨çƒæµ‹è¯•" in Simplified Chinese, as "global" is not a commonly used term in Chinese.* "average prediction set size" is translated as "å¹³å‡é¢„æµ‹é›†å¤§å°" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-over-a-Wireless-Network-Distributed-User-Selection-through-Random-Access"><a href="#Federated-Learning-over-a-Wireless-Network-Distributed-User-Selection-through-Random-Access" class="headerlink" title="Federated Learning over a Wireless Network: Distributed User Selection through Random Access"></a>Federated Learning over a Wireless Network: Distributed User Selection through Random Access</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03758">http://arxiv.org/abs/2307.03758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Sun, Shiyao Ma, Ce Zheng, Songtao Wu, Tao Cui, Lingjuan Lyu</li>
<li>for: é™ä½è”åˆå­¦ä¹ ï¼ˆFederated Learningï¼‰åœ¨æ— çº¿ç½‘ç»œä¸Šçš„é€šä¿¡æˆæœ¬ã€‚</li>
<li>methods: ä½¿ç”¨ç½‘ç»œå†…ç½®çš„åˆ†å¸ƒå¼ç”¨æˆ·é€‰æ‹©æ–¹æ³•ï¼Œåˆ©ç”¨æ— çº¿èµ„æºç«äº‰æœºåˆ¶ã€‚</li>
<li>results: å¯ä»¥å¿«é€Ÿè¾¾åˆ°ä¸ä¸­å¤®ç”¨æˆ·é€‰æ‹©æ–¹æ³•ç›¸ä¼¼çš„å¿«é€Ÿåè°ƒã€‚<details>
<summary>Abstract</summary>
User selection has become crucial for decreasing the communication costs of federated learning (FL) over wireless networks. However, centralized user selection causes additional system complexity. This study proposes a network intrinsic approach of distributed user selection that leverages the radio resource competition mechanism in random access. Taking the carrier sensing multiple access (CSMA) mechanism as an example of random access, we manipulate the contention window (CW) size to prioritize certain users for obtaining radio resources in each round of training. Training data bias is used as a target scenario for FL with user selection. Prioritization is based on the distance between the newly trained local model and the global model of the previous round. To avoid excessive contribution by certain users, a counting mechanism is used to ensure fairness. Simulations with various datasets demonstrate that this method can rapidly achieve convergence similar to that of the centralized user selection approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç”¨æˆ·é€‰æ‹©å·²æˆä¸ºè”åˆå­¦ä¹ ï¼ˆFLï¼‰è¿‡æ— çº¿ç½‘ç»œçš„å…³é”®å› ç´ ï¼Œä½†ä¸­å¤®ç”¨æˆ·é€‰æ‹©ä¼šå¢åŠ ç³»ç»Ÿå¤æ‚æ€§ã€‚è¿™é¡¹ç ”ç©¶æå‡ºäº†åŸºäºç½‘ç»œå†…ç½®çš„åˆ†å¸ƒå¼ç”¨æˆ·é€‰æ‹©æ–¹æ³•ï¼Œåˆ©ç”¨æ— çº¿èµ„æºç«äº‰æœºåˆ¶ã€‚å‡è®¾CSMAæœºåˆ¶ä¸ºéšæœºè®¿é—®ï¼Œæˆ‘ä»¬åœ¨æ¯è½®è®­ç»ƒä¸­ manipulate ç«äº‰çª—å£ï¼ˆCWï¼‰å¤§å°ï¼Œä»¥ä¼˜å…ˆç»™äºˆcertainç”¨æˆ· radioèµ„æºã€‚ä½¿ç”¨è®­ç»ƒæ•°æ®åè§ä¸ºFLç”¨æˆ·é€‰æ‹©ç›®æ ‡åœºæ™¯ã€‚åè§åŸºäºä¸Šä¸€è½®è®­ç»ƒçš„å…¨çƒæ¨¡å‹ä¸å½“å‰è½®è®­ç»ƒçš„æœ¬åœ°æ¨¡å‹ä¹‹é—´çš„è·ç¦»ã€‚ä¸ºé¿å…æŸäº›ç”¨æˆ·çš„è¿‡åº¦è´¡çŒ®ï¼Œä½¿ç”¨è®¡æ•°æœºåˆ¶ä¿æŒå…¬å¹³ã€‚é€šè¿‡ simulate å¤šä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬å‘ç°è¿™ç§æ–¹æ³•å¯å¿«è¾¾åˆ°ä¸ä¸­å¤®ç”¨æˆ·é€‰æ‹©æ–¹æ³•ç›¸ä¼¼çš„å¿«é€Ÿå¯åˆã€‚
</details></li>
</ul>
<hr>
<h2 id="Assisting-Clinical-Decisions-for-Scarcely-Available-Treatment-via-Disentangled-Latent-Representation"><a href="#Assisting-Clinical-Decisions-for-Scarcely-Available-Treatment-via-Disentangled-Latent-Representation" class="headerlink" title="Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation"></a>Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03315">http://arxiv.org/abs/2307.03315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bing Xue, Ahmed Sameh Said, Ziqi Xu, Hanyang Liu, Neel Shah, Hanqing Yang, Philip Payne, Chenyang Lu</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æ”¯æŒåŒ»ç–—å†³ç­–è€Œæå‡ºçš„ï¼Œæ—¨åœ¨é¢„æµ‹æ‚£è€…æ˜¯å¦éœ€è¦ECMOæ²»ç–—ï¼Œä»¥åŠECMOæ²»ç–—åçš„å¯èƒ½æ€§ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³Treatment Variational AutoEncoderï¼ˆTVAEï¼‰ï¼Œç”¨äºä¸ªæ€§åŒ–æ²»ç–—åˆ†æã€‚TVAEæ¨¡å‹äº†æ‚£è€…çš„æ²»ç–—å†³ç­–å’Œå¯èƒ½çš„ç»“æœï¼Œå¹¶é€šè¿‡é‡æ„æ­£åˆ™åŒ–å’ŒåŠç›‘ç£æ¥ç¼“è§£å¹²æ‰°å’Œç¼ºä¹æ²»ç–—æ¡ˆä¾‹çš„é—®é¢˜ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒTVAEåœ¨å…·æœ‰å¤šæ ·åŒ–COVID-19æ‚£è€…æ•°æ®é›†ä¸Šæ¯”å·å½“å‰çš„æ²»ç–—æ•ˆæœæ¨¡å‹æ›´é«˜æ•ˆï¼Œå¯ä»¥é¢„æµ‹æ‚£è€…çš„å¯èƒ½æ€§å’Œå®é™…ç»“æœã€‚<details>
<summary>Abstract</summary>
Extracorporeal membrane oxygenation (ECMO) is an essential life-supporting modality for COVID-19 patients who are refractory to conventional therapies. However, the proper treatment decision has been the subject of significant debate and it remains controversial about who benefits from this scarcely available and technically complex treatment option. To support clinical decisions, it is a critical need to predict the treatment need and the potential treatment and no-treatment responses. Targeting this clinical challenge, we propose Treatment Variational AutoEncoder (TVAE), a novel approach for individualized treatment analysis. TVAE is specifically designed to address the modeling challenges like ECMO with strong treatment selection bias and scarce treatment cases. TVAE conceptualizes the treatment decision as a multi-scale problem. We model a patient's potential treatment assignment and the factual and counterfactual outcomes as part of their intrinsic characteristics that can be represented by a deep latent variable model. The factual and counterfactual prediction errors are alleviated via a reconstruction regularization scheme together with semi-supervision, and the selection bias and the scarcity of treatment cases are mitigated by the disentangled and distribution-matched latent space and the label-balancing generative strategy. We evaluate TVAE on two real-world COVID-19 datasets: an international dataset collected from 1651 hospitals across 63 countries, and a institutional dataset collected from 15 hospitals. The results show that TVAE outperforms state-of-the-art treatment effect models in predicting both the propensity scores and factual outcomes on heterogeneous COVID-19 datasets. Additional experiments also show TVAE outperforms the best existing models in individual treatment effect estimation on the synthesized IHDP benchmark dataset.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Š ÑĞºÑÑ‚Ñ€Ğ°ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ĞµĞ¼Ğ±Ñ€Ğ°Ğ½Ğ°Ğ½Ğ¾Ğ¹ Ğ¾ĞºÑĞ¸Ğ³ĞµĞ½Ğ°Ñ†Ğ¸Ñ (Ğ­ĞšĞœĞ) æ˜¯ COVID-19 æ‚£è€…ä»¬æ— æ³•æ¥å—å¸¸è§„æ²»ç–—çš„å…³é”®ç”Ÿå‘½æ”¯æŒ modalitiesã€‚ç„¶è€Œï¼Œæ­£ç¡®çš„æ²»ç–—å†³ç­–ä»ç„¶æ˜¯äº‰è®®çš„ï¼Œå°šæœªç¡®å®šå“ªäº›æ‚£è€…ä¼šå—ç›Šäºè¿™ç§ç½•è§å’ŒæŠ€æœ¯å¤æ‚çš„æ²»ç–—é€‰æ‹©ã€‚ä¸ºæ”¯æŒä¸´åºŠå†³ç­–ï¼Œæˆ‘ä»¬éœ€è¦é¢„æµ‹æ²»ç–—éœ€æ±‚å’Œå¯èƒ½çš„æ²»ç–—å’Œæ— æ²»ç–—å“åº”ã€‚é’ˆå¯¹è¿™ç§ä¸´åºŠæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† Treatment Variational AutoEncoder (TVAE)ï¼Œä¸€ç§æ–°çš„ä¸ªæ€§åŒ–æ²»ç–—åˆ†ææ–¹æ³•ã€‚TVAE ç‰¹åˆ«æ˜¯ä¸ºäº†è§£å†³ ECMO å¼ºçƒˆçš„é€‰æ‹©åè§å’Œç½•è§æ²»ç–—æ¡ˆä¾‹çš„æ¨¡å‹æŒ‘æˆ˜ã€‚TVAE å°†æ²»ç–—å†³ç­–è§†ä¸ºå¤šçº§é—®é¢˜ï¼Œæ¨¡å‹ç—…äººçš„å¯èƒ½çš„æ²»ç–—åˆ†é…å’Œå®é™…å’Œ counterfactual ç»“æœä¸ºå…¶å†…åœ¨ç‰¹å¾ï¼Œå¯ä»¥é€šè¿‡æ·±åº¦å·ç§¯æ¨¡å‹è¡¨ç¤ºã€‚å®é™…å’Œ counterfactual é¢„æµ‹é”™è¯¯è¢«è§£å†³é€šè¿‡é‡å»ºè§„åˆ™å’ŒåŠç›‘ç£ï¼Œå¹¶ä¸”é€‰æ‹©åè§å’Œç½•è§æ²»ç–—æ¡ˆä¾‹è¢«å‡è½»é€šè¿‡åˆ†è§£å’Œåˆ†å¸ƒåŒ¹é…çš„ç§¯åˆ†ç©ºé—´å’Œæ ‡ç­¾å‡è¡¡ç”Ÿæˆç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå®é™… COVID-19 æ•°æ®é›†ä¸Šè¯„ä¼°äº† TVAEï¼šä¸€ä¸ªå›½é™…æ•°æ®é›†ä» 1651 å®¶åŒ»é™¢ across 63 ä¸ªå›½å®¶æ”¶é›†ï¼Œå¦ä¸€ä¸ª institutional æ•°æ®é›†ä» 15 å®¶åŒ»é™¢æ”¶é›†ã€‚ç»“æœæ˜¾ç¤ºï¼ŒTVAE åœ¨å¼‚è´¨ COVID-19 æ•°æ®é›†ä¸Šé¢„æµ‹å®é™…åˆ†æ•°å’Œ factual ç»“æœçš„æ€§èƒ½è¾ƒä¸ºå‰è€…ã€‚å…¶ä»–å®éªŒä¹Ÿè¡¨æ˜ TVAE åœ¨ä¸ªä½“æ²»ç–—æ•ˆæœé¢„æµ‹æ–¹é¢è¶…è¶Šäº†ç°æœ‰æœ€ä½³æ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="On-Invariance-Equivariance-Correlation-and-Convolution-of-Spherical-Harmonic-Representations-for-Scalar-and-Vectorial-Data"><a href="#On-Invariance-Equivariance-Correlation-and-Convolution-of-Spherical-Harmonic-Representations-for-Scalar-and-Vectorial-Data" class="headerlink" title="On Invariance, Equivariance, Correlation and Convolution of Spherical Harmonic Representations for Scalar and Vectorial Data"></a>On Invariance, Equivariance, Correlation and Convolution of Spherical Harmonic Representations for Scalar and Vectorial Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03311">http://arxiv.org/abs/2307.03311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Janis Keuper</li>
<li>for: æœ¬è®ºæ–‡ä¸»è¦é’ˆå¯¹Machine Learningé¢†åŸŸä¸­åœ†å½¢å·ç§¯ï¼ˆSpherical Harmonicï¼ŒSHï¼‰è¡¨ç¤ºçš„æ•°å­¦è¡¨è¿°ï¼Œå°¤å…¶æ˜¯å¯¹äºæ—‹è½¬ä¸å˜å’Œå¯¹ç§°çš„ç‰¹å¾å’Œå·ç§¯ã€‚</li>
<li>methods: æœ¬è®ºæ–‡æå‡ºäº†SHè¡¨ç¤ºçš„ç†è®ºåŸºç¡€å’Œå®è·µæ–¹æ³•ï¼ŒåŒ…æ‹¬æ—‹è½¬ä¸å˜å’Œå¯¹ç§°ç‰¹å¾å’Œå·ç§¯ï¼Œä»¥åŠå°†scalar SHè¡¨ç¤ºæ‰©å±•åˆ°vector field on sphereä¸Šçš„VHè¡¨ç¤ºã€‚</li>
<li>results: æœ¬è®ºæ–‡summarizes the works on rotation invariant and equivariant features, as well as convolutions and exact correlations of signals on spheres, and extends these methods to 3d vector fields on spheres.<details>
<summary>Abstract</summary>
The mathematical representations of data in the Spherical Harmonic (SH) domain has recently regained increasing interest in the machine learning community. This technical report gives an in-depth introduction to the theoretical foundation and practical implementation of SH representations, summarizing works on rotation invariant and equivariant features, as well as convolutions and exact correlations of signals on spheres. In extension, these methods are then generalized from scalar SH representations to Vectorial Harmonics (VH), providing the same capabilities for 3d vector fields on spheres
</details>
<details>
<summary>æ‘˜è¦</summary>
Recently, the mathematical representations of data in the Spherical Harmonic (SH) domain have gained increasing interest in the machine learning community. This technical report provides an in-depth introduction to the theoretical foundation and practical implementation of SH representations, including works on rotation invariant and equivariant features, as well as convolutions and exact correlations of signals on spheres. Additionally, these methods are then generalized from scalar SH representations to Vectorial Harmonics (VH), allowing for 3D vector fields on spheres to have the same capabilities.Here's the word-for-word translation of the text into Simplified Chinese:è¿‘æœŸï¼Œåœ†å½¢å“ˆå¯†é¡¿ï¼ˆSHï¼‰é¢†åŸŸä¸­æ•°æ®çš„æ•°å­¦è¡¨ç¤ºå—åˆ°æœºå™¨å­¦ä¹ ç¤¾åŒºçš„è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚æœ¬æŠ€æœ¯æŠ¥å‘Šå¯¹SHè¡¨ç¤ºçš„ç†è®ºåŸºç¡€å’Œå®è·µè¿›è¡Œäº†æ·±å…¥çš„ä»‹ç»ï¼ŒåŒ…æ‹¬å¯¹æ—‹è½¬ä¸å˜å’Œå¯¹ç§°ç‰¹å¾çš„ç ”ç©¶ï¼Œä»¥åŠåœ†å½¢ä¸Šçš„ä¿¡å·å·ç§¯å’Œç²¾ç¡®ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•è¿˜è¢«æ¨å¹¿åˆ° vectorial harmonicsï¼ˆVHï¼‰ä¸­ï¼Œä»¥ä¾¿ä¸‰ç»´å‘é‡åœºåœ¨åœ†å½¢ä¸Šå…·æœ‰ç›¸åŒçš„èƒ½åŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="S2vNTM-Semi-supervised-vMF-Neural-Topic-Modeling"><a href="#S2vNTM-Semi-supervised-vMF-Neural-Topic-Modeling" class="headerlink" title="S2vNTM: Semi-supervised vMF Neural Topic Modeling"></a>S2vNTM: Semi-supervised vMF Neural Topic Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.04804">http://arxiv.org/abs/2307.04804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Xu, Jay Desai, Srinivasan Sengamedu, Xiaoyu Jiang, Francis Iannacci</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ‰¹å¤„æ–‡æœ¬åˆ†ç±» Ğ·Ğ°Ğ´Ğ°åŠ¡ä¸­æé«˜æ•ˆç‡å’Œå‡†ç¡®ç‡ï¼Œå¹¶å…è®¸ä½¿ç”¨å°‘é‡å…³é”®è¯ä½œä¸ºè¾“å…¥ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºSemi-Supervised vMF Neural Topic Modelingï¼ˆS2vNTMï¼‰çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ç§å­å…³é”®è¯æ¥åˆå§‹åŒ–ä¸»é¢˜ï¼Œå¹¶é€šè¿‡å…³é”®è¯çš„æ¨¡å¼æ¥è¯†åˆ«å’Œä¼˜åŒ–ä¸»é¢˜çš„å…³é”®è¯é›†ã€‚</li>
<li>results: åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒS2vNTMçš„åˆ†ç±»ç²¾åº¦é«˜äºç°æœ‰çš„åŠç›‘ç£ä¸»é¢˜æ¨¡å‹æ–¹æ³•ï¼Œè€Œä¸”é€Ÿåº¦è‡³å°‘ twice as fast as baselinesã€‚<details>
<summary>Abstract</summary>
Language model based methods are powerful techniques for text classification. However, the models have several shortcomings. (1) It is difficult to integrate human knowledge such as keywords. (2) It needs a lot of resources to train the models. (3) It relied on large text data to pretrain. In this paper, we propose Semi-Supervised vMF Neural Topic Modeling (S2vNTM) to overcome these difficulties. S2vNTM takes a few seed keywords as input for topics. S2vNTM leverages the pattern of keywords to identify potential topics, as well as optimize the quality of topics' keywords sets. Across a variety of datasets, S2vNTM outperforms existing semi-supervised topic modeling methods in classification accuracy with limited keywords provided. S2vNTM is at least twice as fast as baselines.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¯­è¨€æ¨¡å‹åŸºæœ¬æ–¹æ³•æ˜¯æ–‡æœ¬åˆ†ç±»çš„å¼ºå¤§æŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æœ‰å‡ ä¸ªç¼ºç‚¹ã€‚ï¼ˆ1ï¼‰å®ƒå¾ˆéš¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ° human knowledgeï¼Œå¦‚å…³é”®è¯ã€‚ï¼ˆ2ï¼‰å®ƒéœ€è¦è®­ç»ƒæ¨¡å‹å¾ˆå¤šèµ„æºã€‚ï¼ˆ3ï¼‰å®ƒä¾èµ–äºå¤§é‡æ–‡æœ¬æ•°æ®è¿›è¡Œé¢„è®­ç»ƒã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŠsupervised vMFç¥ç»è¯é¢˜æ¨¡å‹ï¼ˆS2vNTMï¼‰æ¥è§£å†³è¿™äº›å›°éš¾ã€‚S2vNTMé€šè¿‡æä¾›ä¸€äº›ç§å­å…³é”®è¯æ¥è¾“å…¥ä¸»é¢˜ï¼Œå¹¶åˆ©ç”¨å…³é”®è¯çš„æ¨¡å¼æ¥ç¡®å®šä¸»é¢˜çš„å¯èƒ½æ€§ï¼Œä»¥åŠä¼˜åŒ–ä¸»é¢˜çš„å…³é”®è¯é›†ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒS2vNTMæ¯”ç°æœ‰çš„åŠsupervisedä¸»é¢˜æ¨¡å‹åœ¨åˆ†ç±»ç²¾åº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåªéœ€æä¾›æœ‰é™çš„å…³é”®è¯ã€‚æ­¤å¤–ï¼ŒS2vNTMæ¯”åŸºå‡†æ–¹æ³•å¿«é€Ÿã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Vulnerability-of-Attribution-Methods-Using-Pre-Softmax-Scores"><a href="#A-Vulnerability-of-Attribution-Methods-Using-Pre-Softmax-Scores" class="headerlink" title="A Vulnerability of Attribution Methods Using Pre-Softmax Scores"></a>A Vulnerability of Attribution Methods Using Pre-Softmax Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03305">http://arxiv.org/abs/2307.03305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlerma54/adversarial-attacks-on-saliency-maps">https://github.com/mlerma54/adversarial-attacks-on-saliency-maps</a></li>
<li>paper_authors: Miguel Lerma, Mirtha Lucas</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†ä¸€ç§ç±»åˆ«ç¥ç»ç½‘ç»œè¾“å‡ºè§£é‡Šæ–¹æ³•çš„æ”»å‡»æ–¹æ³•ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å°å‹ä¿®æ”¹æ¨¡å‹æ¥å½±å“è§£é‡Šæ–¹æ³•çš„è¾“å‡ºï¼Œè€Œä¸æ”¹å˜æ¨¡å‹çš„è¾“å‡ºã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œè¿™ç§ä¿®æ”¹æ–¹æ³•å¯ä»¥å¯¼è‡´è§£é‡Šæ–¹æ³•çš„è¾“å‡ºå—åˆ°è¾ƒå¤§çš„å½±å“ï¼Œè€Œæ— éœ€æ”¹å˜æ¨¡å‹çš„è¾“å‡ºã€‚<details>
<summary>Abstract</summary>
We discuss a vulnerability involving a category of attribution methods used to provide explanations for the outputs of convolutional neural networks working as classifiers. It is known that this type of networks are vulnerable to adversarial attacks, in which imperceptible perturbations of the input may alter the outputs of the model. In contrast, here we focus on effects that small modifications in the model may cause on the attribution method without altering the model outputs.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬è®¨è®ºäº†ä¸€ç§ç±»å‹çš„å¯¹åº”æ–¹æ³•çš„æ¼æ´ï¼Œè¿™ç§æ–¹æ³•ç”¨äºè¯´æ˜å¯¹åº”ç½‘ç»œä½œä¸ºåˆ†ç±»å™¨çš„è¾“å‡ºã€‚å·²ç»çŸ¥é“è¿™ç§ç½‘ç»œå—åˆ°äº†æ•Œå¯¹æ”»å‡»ï¼Œè¿™äº›æ”»å‡»å¯èƒ½å¯¼è‡´è¾“å…¥çš„æ— æ³•è¯†åˆ«çš„å°å˜åŒ–ï¼Œå¯¼è‡´æ¨¡å‹çš„è¾“å‡ºå˜åŒ–ã€‚ç›¸åï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œé›†ä¸­äº†å¯¹åº”æ–¹æ³•çš„å°ä¿®æ”¹ä¼šå¯¼è‡´çš„æ•ˆæœï¼Œè€Œä¸ä¼šæ”¹å˜æ¨¡å‹çš„è¾“å‡ºã€‚
</details></li>
</ul>
<hr>
<h2 id="It-is-not-Sexually-Suggestive-It-is-Educative-Separating-Sex-Education-from-Suggestive-Content-on-TikTok-Videos"><a href="#It-is-not-Sexually-Suggestive-It-is-Educative-Separating-Sex-Education-from-Suggestive-Content-on-TikTok-Videos" class="headerlink" title="It is not Sexually Suggestive, It is Educative. Separating Sex Education from Suggestive Content on TikTok Videos"></a>It is not Sexually Suggestive, It is Educative. Separating Sex Education from Suggestive Content on TikTok Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03274">http://arxiv.org/abs/2307.03274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enfa George, Mihai Surdeanu</li>
<li>for: æœ¬ç ”ç©¶ç›®çš„æ˜¯ä¸ºäº†åˆ›å»ºä¸€ä¸ªå¤šModalæ•°æ®é›†ï¼Œä»¥ä¾¿åˆ†è¾¨TikTokä¸Šçš„æ€§ suggestiveå†…å®¹å’Œè™šæ‹Ÿæ€§æ•™è‚²è§†é¢‘ã€‚</li>
<li>methods: ç ”ç©¶ä½¿ç”¨äº†TikTokä¸Šçš„è§†é¢‘URLå’ŒéŸ³é¢‘ç¬”å½•ï¼Œå¹¶é‡‡ç”¨äº†ä¸¤ç§åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹æ¥åˆ†ç±»è§†é¢‘ã€‚</li>
<li>results: åˆæ­¥ç»“æœè¡¨æ˜ï¼Œåˆ†è¾¨è¿™äº›ç±»å‹çš„è§†é¢‘æ˜¯å¯å­¦ä¹ çš„ï¼Œä½†ä¹Ÿæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚è¿™äº›å®éªŒè¡¨æ˜ï¼Œè¿™ä¸ªæ•°æ®é›†æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå¹¶é‚€è¯·æ›´å¤šç ”ç©¶è€…æ¥æ·±å…¥ç ”ç©¶è¿™ä¸ªé¢†åŸŸã€‚I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled as sexually suggestive (from the annotator's point of view), sex-educational content, or neither. Such a dataset is necessary to address the challenge of distinguishing between sexually suggestive content and virtual sex education videos on TikTok. Children's exposure to sexually suggestive videos has been shown to have adversarial effects on their development. Meanwhile, virtual sex education, especially on subjects that are more relevant to the LGBTQIA+ community, is very valuable. The platform's current system removes or penalizes some of both types of videos, even though they serve different purposes. Our dataset contains video URLs, and it is also audio transcribed. To validate its importance, we explore two transformer-based models for classifying the videos. Our preliminary results suggest that the task of distinguishing between these types of videos is learnable but challenging. These experiments suggest that this dataset is meaningful and invites further study on the subject.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»SexTokæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«TikTokè§†é¢‘è¢«æ ‡è®°ä¸ºæ€§å–å‘ï¼ˆç”±æ³¨é‡Šå‘˜çœ‹æ¥ï¼‰ã€æ€§æ•™è‚²å†…å®¹æˆ–è€… neither çš„å¤šmodalæ•°æ®é›†ã€‚è¿™æ ·çš„æ•°æ®é›† Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ç”¨äºè§£å†³TikTokä¸Šæ€§å–å‘å†…å®¹å’Œè™šæ‹Ÿæ€§æ•™è‚²è§†é¢‘çš„åˆ†ç±»æŒ‘æˆ˜ã€‚å„¿ç«¥æ¥è§¦æ€§å–å‘è§†é¢‘ä¼šå¯¹å…¶å‘å±•äº§ç”Ÿæœ‰å®³å½±å“ã€‚ç„¶è€Œï¼Œè™šæ‹Ÿæ€§æ•™è‚²ï¼Œç‰¹åˆ«æ˜¯å¯¹LGBTQIA+ç¤¾ç¾¤æ›´åŠ é‡è¦çš„ä¸»é¢˜ï¼Œå¯¹äºå„¿ç«¥çš„æ€§æ•™è‚²å¾ˆæœ‰ä»·å€¼ã€‚ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°å½“å‰çš„ç³»ç»Ÿä¼šå°†ä¸€äº›è¿™äº›è§†é¢‘ç§»é™¤æˆ–å¤„ç½šï¼Œå°½ç®¡å®ƒä»¬åœ¨ä¸åŒçš„ç›®çš„ä¸ŠæœåŠ¡ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«è§†é¢‘ URLï¼ŒåŒæ—¶ä¹Ÿæœ‰éŸ³é¢‘ç¬”è®°ã€‚ä¸ºéªŒè¯å…¶é‡è¦æ€§ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§åŸºäº transformer æ¨¡å‹æ¥åˆ†ç±»è§†é¢‘ã€‚æˆ‘ä»¬çš„åˆæ­¥ç»“æœè¡¨æ˜ï¼Œè¿™ç§åˆ†ç±»ä»»åŠ¡å¯ä»¥å­¦ä¹ ï¼Œä½†ä¹Ÿæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™äº›å®éªŒè¡¨æ˜ï¼Œè¿™ä¸ªæ•°æ®é›†æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå¹¶é‚€è¯·è¿›ä¸€æ­¥ç ”ç©¶è¿™ä¸ªä¸»é¢˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="Vision-Language-Transformers-A-Survey"><a href="#Vision-Language-Transformers-A-Survey" class="headerlink" title="Vision Language Transformers: A Survey"></a>Vision Language Transformers: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03254">http://arxiv.org/abs/2307.03254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clayton Fields, Casey Kennington</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†æ¢è®¨è§†Languageæ¨¡å‹çš„å‘å±•å’Œåº”ç”¨ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†é¢„è®­ç»ƒçš„transformeræ¶æ„ï¼Œå¹¶é€šè¿‡å°†å…¶åº”ç”¨åˆ°æ–°ä»»åŠ¡ä¸Šï¼Œä»¥å®ç°è·¨è§†ä¸è¯­è¨€çš„æ¨¡å‹ã€‚</li>
<li>results: è¿™ä¸ªè®ºæ–‡æä¾›äº†è§†Languageæ¨¡å‹çš„å¹¿æ³›çš„ç ”ç©¶å’Œåˆ†æï¼Œä»¥åŠå…¶ä¼˜ç‚¹ã€å±€é™æ€§å’Œæœªè§£å†³çš„é—®é¢˜ã€‚<details>
<summary>Abstract</summary>
Vision language tasks, such as answering questions about or generating captions that describe an image, are difficult tasks for computers to perform. A relatively recent body of research has adapted the pretrained transformer architecture introduced in \citet{vaswani2017attention} to vision language modeling. Transformer models have greatly improved performance and versatility over previous vision language models. They do so by pretraining models on a large generic datasets and transferring their learning to new tasks with minor changes in architecture and parameter values. This type of transfer learning has become the standard modeling practice in both natural language processing and computer vision. Vision language transformers offer the promise of producing similar advancements in tasks which require both vision and language. In this paper, we provide a broad synthesis of the currently available research on vision language transformer models and offer some analysis of their strengths, limitations and some open questions that remain.
</details>
<details>
<summary>æ‘˜è¦</summary>
Computer vision language tasks, such as answering questions about or generating captions that describe an image, are difficult tasks for computers to perform.  Recently, researchers have adapted the pre-trained transformer architecture introduced in vaswani2017attention to vision language modeling, which has greatly improved performance and versatility over previous vision language models. They do so by pre-training models on large generic datasets and transferring their learning to new tasks with minor changes in architecture and parameter values. This type of transfer learning has become the standard modeling practice in both natural language processing and computer vision. Vision language transformers offer the promise of producing similar advancements in tasks that require both vision and language. In this paper, we provide a broad synthesis of the currently available research on vision language transformer models and offer some analysis of their strengths, limitations, and some open questions that remain.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the standard Mandarin pronunciation and may not be exactly the same as the traditional Chinese used in Taiwan or other regions.
</details></li>
</ul>
<hr>
<h2 id="Learned-Kernels-for-Interpretable-and-Efficient-PPG-Signal-Quality-Assessment-and-Artifact-Segmentation"><a href="#Learned-Kernels-for-Interpretable-and-Efficient-PPG-Signal-Quality-Assessment-and-Artifact-Segmentation" class="headerlink" title="Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation"></a>Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.05385">http://arxiv.org/abs/2307.05385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sully F. Chen, Zhicheng Guo, Cheng Ding, Xiao Hu, Cynthia Rudin</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§å¯é ã€æœ‰æ•ˆã€å¯è§£é‡Šçš„è„‰å†²å…‰è°±å­¦ï¼ˆPPGï¼‰ä¿¡å·è´¨é‡è¯„ä¼°å’Œartefactåˆ†å‰²æ–¹æ³•ï¼Œä»¥æé«˜PPGä¿¡å·çš„ç²¾åº¦å’Œå¯é æ€§ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§å°å‹ã€å¯è§£é‡Šçš„å·ç§¯æ ¸æ¥å­¦ä¹ PPGä¿¡å·ä¸­çš„è´¨é‡ç‰¹å¾ï¼Œå¹¶ä¸ç°æœ‰çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥å°å‹å·ç§¯æ ¸æ–¹æ³•å¯ä»¥ä¸DNNæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰ç±»ä¼¼æˆ–æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰è®¸å¤šä¸ªæ•°æ®ç‚¹çš„ä¼˜åŠ¿ï¼Œå¦‚å¿«é€Ÿã€å¯é ã€å¯è§£é‡Šã€‚<details>
<summary>Abstract</summary>
Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and interpretable signal quality assessment and artifact segmentation on low-power devices.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Push-Past-Green-Learning-to-Look-Behind-Plant-Foliage-by-Moving-It"><a href="#Push-Past-Green-Learning-to-Look-Behind-Plant-Foliage-by-Moving-It" class="headerlink" title="Push Past Green: Learning to Look Behind Plant Foliage by Moving It"></a>Push Past Green: Learning to Look Behind Plant Foliage by Moving It</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03175">http://arxiv.org/abs/2307.03175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Zhang, Saurabh Gupta</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨æå‡ºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨åŒ–å†œä¸šåº”ç”¨ç¨‹åºï¼ˆå¦‚æ£€æŸ¥ã€è¯„ä¼°ã€æ‘˜å–æ°´æœï¼‰ä¸­ manipulating æ¤ç‰©å¶å­å’Œæå¹²ä»¥æŸ¥çœ‹åæ–¹ç©ºé—´ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨è‡ªæˆ‘è¶…çº§visionæ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨SRPNetç¥ç»ç½‘ç»œé¢„æµ‹æ‰§è¡Œå€™é€‰åŠ¨ä½œåå¯ä»¥æŸ¥çœ‹çš„ç©ºé—´ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼Œå¯¹äº synthetic è”·è–‡å’Œå®é™…çš„ Ğ´Ñ€Ğ°Ñ†ĞµĞ½Ğ°æ¤ç‰©ï¼ŒPPGæ–¹æ³•åœ¨5ä¸ªè®¾å®šä¸‹è¡¨ç°å‡ºè‰²ï¼Œè€ŒSRPNetç¥ç»ç½‘ç»œåœ¨5ä¸ªè®¾å®šä¸‹éƒ½è¶…è¿‡äº†æ‰‹åŠ¨è®¾è®¡çš„æ¢ç´¢æ–¹æ³•å’Œç›¸å…³çš„ablationsã€‚<details>
<summary>Abstract</summary>
Autonomous agriculture applications (e.g., inspection, phenotyping, plucking fruits) require manipulating the plant foliage to look behind the leaves and the branches. Partial visibility, extreme clutter, thin structures, and unknown geometry and dynamics for plants make such manipulation challenging. We tackle these challenges through data-driven methods. We use self-supervision to train SRPNet, a neural network that predicts what space is revealed on execution of a candidate action on a given plant. We use SRPNet with the cross-entropy method to predict actions that are effective at revealing space beneath plant foliage. Furthermore, as SRPNet does not just predict how much space is revealed but also where it is revealed, we can execute a sequence of actions that incrementally reveal more and more space beneath the plant foliage. We experiment with a synthetic (vines) and a real plant (Dracaena) on a physical test-bed across 5 settings including 2 settings that test generalization to novel plant configurations. Our experiments reveal the effectiveness of our overall method, PPG, over a competitive hand-crafted exploration method, and the effectiveness of SRPNet over a hand-crafted dynamics model and relevant ablations.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªä¸»å†œä¸šåº”ç”¨ï¼ˆå¦‚æ£€æŸ¥ã€è¾å°„ç±»å‹ã€æ‘˜æœï¼‰éœ€è¦æ“ä½œæ¤ç‰©å¶å­å’Œæå¹²ï¼Œä»¥ä¾¿ä»åæ–¹çœ‹åˆ°å¶å­å’Œæå¹²ã€‚ä½†æ˜¯å¶å­å’Œæå¹²ä¹‹é—´çš„éƒ¨åˆ†å¯è§æ€§ã€æåº¦æ‹¥æŒ¤ã€è–„è‚‰å’Œæ¤ç‰©çš„ä¸ç¡®å®šgeometryå’ŒåŠ¨åŠ›å­¦ä½¿å¾—è¿™ç§æ“ä½œå˜å¾—å›°éš¾ã€‚æˆ‘ä»¬é€šè¿‡æ•°æ®é©±åŠ¨æ–¹æ³•è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä½¿ç”¨è‡ªæˆ‘ç›‘ç£æ¥è®­ç»ƒSRPNetï¼Œä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œé¢„æµ‹æ‰§è¡Œç»™å®šæ¤ç‰©çš„å€™é€‰åŠ¨ä½œåå¯è§çš„ç©ºé—´ã€‚æˆ‘ä»¬ä½¿ç”¨SRPNetä¸åå­—ç§¯åˆ†æ³•é¢„æµ‹æœ‰æ•ˆçš„åŠ¨ä½œï¼Œä»¥ä¾¿é€æ­¥æ­ç¤ºæ¤ç‰©ä¸‹æ–¹çš„ç©ºé—´ã€‚æ­¤å¤–ï¼ŒSRPNetä¸ä»…é¢„æµ‹æ‰§è¡ŒåŠ¨ä½œåå¯è§çš„ç©ºé—´é‡ï¼Œè¿˜é¢„æµ‹å…¶åœ¨å“ªé‡Œè¢«æ­ç¤ºï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ‰§è¡Œä¸€ç³»åˆ—çš„åŠ¨ä½œï¼Œä»¥é€æ­¥æ­ç¤ºæ›´å¤šçš„æ¤ç‰©ä¸‹æ–¹çš„ç©ºé—´ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ª sinteticï¼ˆè‘¡è„ï¼‰å’Œä¸€ä¸ªå®é™…çš„æ¤ç‰©ï¼ˆ Ğ´Ñ€Ğ°Ñ†ĞµĞ½Ğ°ï¼‰ä¸Šè¿›è¡Œäº†åœ¨ç‰©ç†æµ‹è¯•åºŠä¸Šçš„å®éªŒï¼Œå¹¶åœ¨5ä¸ªè®¾å®šä¸­æµ‹è¯•äº†æˆ‘ä»¬çš„æ€»æ–¹æ³•ï¼ŒåŒ…æ‹¬2ä¸ªè®¾å®šï¼Œä»¥æµ‹è¯•æ‰©å±•åˆ°æ–°çš„æ¤ç‰©é…ç½®ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜æˆ‘ä»¬çš„æ€»æ–¹æ³•PPGåœ¨æ¯”æ‰‹å·¥æ¢ç´¢æ–¹æ³•æ›´æœ‰æ•ˆï¼Œè€ŒSRPNetåœ¨æ‰‹å·¥åŠ¨åŠ›å­¦æ¨¡å‹å’Œç›¸å…³çš„ablationsä¸­ä¹Ÿè¡¨ç°å‡ºäº†æ•ˆæœã€‚
</details></li>
</ul>
<hr>
<h2 id="LEO-Learning-Efficient-Orderings-for-Multiobjective-Binary-Decision-Diagrams"><a href="#LEO-Learning-Efficient-Orderings-for-Multiobjective-Binary-Decision-Diagrams" class="headerlink" title="LEO: Learning Efficient Orderings for Multiobjective Binary Decision Diagrams"></a>LEO: Learning Efficient Orderings for Multiobjective Binary Decision Diagrams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03171">http://arxiv.org/abs/2307.03171</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khalil-research/leo">https://github.com/khalil-research/leo</a></li>
<li>paper_authors: Rahul Patel, Elias B. Khalil</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ä¸ºäº†è§£å†³å¤šå¯¹è±¡æ•°æ®åˆ†æé—®é¢˜ä¸­çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ç”¨BDDsæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†BDDsæ¥è§£å†³å¤šå¯¹è±¡æ•°æ®åˆ†æé—®é¢˜ï¼Œå¹¶ä¸”ä½¿ç”¨äº†ä¸€äº›æ–°çš„å˜é‡æ’åºæ–¹æ³•æ¥æé«˜BDDsçš„æ•ˆç‡å’Œç²¾åº¦ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨LEOè¿™ä¸ªè¶…çº§visedå­¦ä¹ æ–¹æ³•å¯ä»¥å¿«é€Ÿåœ°æ‰¾åˆ°é«˜æ•ˆçš„å˜é‡æ’åºæ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥å°†PFæšä¸¾æ—¶é—´ç¼©çŸ­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLEOæ¯”å¸¸ç”¨çš„æ’åºæ–¹æ³•å’Œç®—æ³•é…ç½®æ›´å¿«é€Ÿåœ°å®ŒæˆPFæšä¸¾ã€‚<details>
<summary>Abstract</summary>
Approaches based on Binary decision diagrams (BDDs) have recently achieved state-of-the-art results for multiobjective integer programming problems. The variable ordering used in constructing BDDs can have a significant impact on their size and on the quality of bounds derived from relaxed or restricted BDDs for single-objective optimization problems. We first showcase a similar impact of variable ordering on the Pareto frontier (PF) enumeration time for the multiobjective knapsack problem, suggesting the need for deriving variable ordering methods that improve the scalability of the multiobjective BDD approach. To that end, we derive a novel parameter configuration space based on variable scoring functions which are linear in a small set of interpretable and easy-to-compute variable features. We show how the configuration space can be efficiently explored using black-box optimization, circumventing the curse of dimensionality (in the number of variables and objectives), and finding good orderings that reduce the PF enumeration time. However, black-box optimization approaches incur a computational overhead that outweighs the reduction in time due to good variable ordering. To alleviate this issue, we propose LEO, a supervised learning approach for finding efficient variable orderings that reduce the enumeration time. Experiments on benchmark sets from the knapsack problem with 3-7 objectives and up to 80 variables show that LEO is ~30-300% and ~10-200% faster at PF enumeration than common ordering strategies and algorithm configuration. Our code and instances are available at https://github.com/khalil-research/leo.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>ä½¿ç”¨äºŒè¿›åˆ¶å†³ç­–å›¾ï¼ˆBDDï¼‰çš„æ–¹æ³•æœ€è¿‘åœ¨å¤šç›®æ ‡æ•´æ•°ç¼–ç¨‹é—®é¢˜ä¸Šå®ç°äº†çŠ¶æ€çš„æ°å‡ºæˆç»©ã€‚BDDä¸­å˜é‡çš„æ’åºå¯ä»¥å½±å“å…¶å¤§å°å’Œå«çº¦ç¯å¢ƒä¸­çš„ç¼“å’Œçº¦æŸçš„è´¨é‡ã€‚æˆ‘ä»¬é¦–å…ˆç¤ºå‡ºå˜é‡æ’åºå¯¹å¤šç›®æ ‡é¥¶è¤”é—®é¢˜çš„Paretoå‰åˆ—ï¼ˆPFï¼‰æšä¸¾æ—¶é—´æœ‰ç€ç›¸ä¼¼çš„å½±å“ã€‚è¿™è¡¨æ˜éœ€è¦å¼€å‘å¯ä»¥æé«˜å¤šç›®æ ‡BDDæ–¹æ³•çš„å¯æ‰©å±•æ€§çš„å˜é‡æ’åºæ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ derivateä¸€ä¸ªåŸºäºå˜é‡è¯„ä»·å‡½æ•°çš„æ–°å‚æ•°é…ç½®ç©ºé—´ï¼Œè¯¥ç©ºé—´æ˜¯çº¿æ€§çš„ï¼Œä¸”å¯ä»¥ä½¿ç”¨ä¸€å°ç»„ç®€å•æ˜“è®¡ç®—çš„å˜é‡ç‰¹å¾æ¥å®ç°ã€‚æˆ‘ä»¬è¡¨æ˜è¯¥é…ç½®ç©ºé—´å¯ä»¥ä½¿ç”¨é»‘ç›’ä¼˜åŒ–å™¨é«˜æ•ˆåœ°æ¢ç´¢ï¼Œå¹¶ä¸”å¯ä»¥å¿«é€Ÿæ‰¾åˆ°å¥½çš„æ’åºï¼Œä»è€Œå‡å°‘PFæšä¸¾æ—¶é—´ã€‚ç„¶è€Œï¼Œé»‘ç›’ä¼˜åŒ–å™¨çš„è®¡ç®—å¼€é”€ä¼šè¶…è¿‡å‡å°‘PFæšä¸¾æ—¶é—´çš„å¥½å˜é‡æ’åºçš„æ•ˆæœã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LEOï¼Œä¸€ç§ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ‰¾åˆ°é«˜æ•ˆçš„å˜é‡æ’åºï¼Œä»è€Œå‡å°‘PFæšä¸¾æ—¶é—´ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLEOæ¯”æ™®é€šçš„æ’åºç­–ç•¥å’Œç®—æ³•é…ç½®æ›´å¿«ï¼Œåœ¨é¥¶è¤”é—®é¢˜çš„ benchmark é›†ä¸­ï¼ŒLEOçš„é€Ÿåº¦æ¯”Common ordering strategieså’Œalgorithm configurationå¿«çº¦30-300%å’Œ10-200%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œå®ä¾‹å¯ä»¥åœ¨https://github.com/khalil-research/leoä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Focused-Transformer-Contrastive-Training-for-Context-Scaling"><a href="#Focused-Transformer-Contrastive-Training-for-Context-Scaling" class="headerlink" title="Focused Transformer: Contrastive Training for Context Scaling"></a>Focused Transformer: Contrastive Training for Context Scaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03170">http://arxiv.org/abs/2307.03170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cstankonrad/long_llama">https://github.com/cstankonrad/long_llama</a></li>
<li>paper_authors: Szymon Tworkowski, Konrad Staniszewski, MikoÅ‚aj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr MiÅ‚oÅ›</li>
<li>for: æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ context ä¸‹çš„è¡¨ç°</li>
<li>methods: é€šè¿‡å¯¹æ³¨æ„å±‚è¿›è¡Œä¿®æ”¹ï¼Œè®©å…¶å¯ä»¥è®¿é—®å¤–éƒ¨å­˜å‚¨ï¼Œå¹¶é€šè¿‡å¯¹åº”çš„é”®å€¼å¯¹è¿›è¡Œæ˜ å°„ï¼Œæé«˜æ¨¡å‹çš„è¡¨ç°</li>
<li>results: é€šè¿‡æå‡º Focused Transformer (FoT) æŠ€æœ¯ï¼Œå¯ä»¥å»¶é•¿æ•ˆ context çš„é•¿åº¦ï¼Œå¹¶ä¸”å¯ä»¥ç»†åŒ–ç°æœ‰å¤§è§„æ¨¡æ¨¡å‹ï¼Œä»¥æé«˜å…¶åœ¨é•¿ context ä¸‹çš„è¡¨ç°ï¼Œå¹¶ä¸”åœ¨ passkey æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å¯ä»¥ ÑƒÑĞ¿Ğµreich å¤„ç† $256 k$ é•¿ contextã€‚<details>
<summary>Abstract</summary>
Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-scale models to lengthen their effective context. This is demonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The resulting models, which we name LongLLaMA, exhibit advancements in tasks requiring a long context. We further illustrate that our LongLLaMA models adeptly manage a $256 k$ context length for passkey retrieval.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰å“è¶Šçš„Contextualized Embeddingèƒ½åŠ›ï¼Œå¯ä»¥å°†æ–°ä¿¡æ¯ç»™é€‚å½“åœ°èå…¥åˆ°æ¨¡å‹ä¸­ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„æ½œåŠ›ç»å¸¸å—åˆ°Context Lengthçš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†Attentionå±‚ç»™äº†External Memoryçš„å­˜å–æƒï¼Œè¿™ä¸ªExternal MemoryåŒ…å«äº†(é”®ã€å€¼)å¯¹ã€‚ç„¶è€Œï¼Œå½“æ–‡æ¡£æ•°é‡å¢åŠ æ—¶ï¼Œç›¸å…³çš„é”®æ•°é‡å‡å°‘ï¼Œä½¿æ¨¡å‹æ›´åŠ å€¾å‘äºå…³æ³¨æ— å…³çš„é”®ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªé—®é¢˜ä¸ºåˆ†å¿ƒé—®é¢˜ï¼Œå› ä¸ºä¸åŒçš„Semantic Valueä¹‹é—´çš„é”®å¯èƒ½ä¼š overlapï¼Œä½¿å…¶å›°éš¾åˆ†è¾¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Focused Transformerï¼ˆFoTï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§ä»¥Contrastive Learningä¸ºçµæ„Ÿçš„è®­ç»ƒè¿‡ç¨‹ã€‚è¿™ç§æ–°çš„æ–¹æ³•å¯ä»¥å°†(é”®ã€å€¼)ç©ºé—´çš„ç»“æ„æ”¹å–„ï¼Œä»è€Œå»¶é•¿Context Lengthã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è®©å·²æœ‰çš„å¤§è§„æ¨¡æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥å¢åŠ å…¶æœ‰æ•ˆContext Lengthã€‚æˆ‘ä»¬ç»™äº†$3B$å’Œ$7B$ OpenLLaMAæ£€æŸ¥ç‚¹è¿›è¡Œå¾®è°ƒï¼Œå°†å…¶ç§°ä¸ºLongLLaMAã€‚è¿™äº›LongLLaMAæ¨¡å‹åœ¨éœ€è¦é•¿Contextçš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†LongLLaMAæ¨¡å‹å¯ä»¥efficaciously manage $256 k$ Context Length for passkey retrievalã€‚
</details></li>
</ul>
<hr>
<h2 id="BrickPal-Augmented-Reality-based-Assembly-Instructions-for-Brick-Models"><a href="#BrickPal-Augmented-Reality-based-Assembly-Instructions-for-Brick-Models" class="headerlink" title="BrickPal: Augmented Reality-based Assembly Instructions for Brick Models"></a>BrickPal: Augmented Reality-based Assembly Instructions for Brick Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03162">http://arxiv.org/abs/2307.03162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Shi, Xiaofeng Zhang, Ran zhang, Zhou Yang, Xiao Tang, Hongni Ye, Yi Wu</li>
<li>for: å¸®åŠ©ç”¨æˆ·æ›´åŠ å¿«é€Ÿå’Œç²¾å‡†åœ°ç»„è£…ä¹é«˜ç§¯æœ¨ï¼Œè§£å†³ä¼ ç»Ÿæ‰‹åŠ¨å¾®è°ƒå’Œçº¸è´¨æŒ‡å—çš„é—®é¢˜ã€‚</li>
<li>methods: åˆ©ç”¨å¯è§è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯ç”Ÿæˆå¯èƒ½çš„ç»„è£…åºåˆ—ï¼Œå¹¶åœ¨æ‰©å±•ç°å®å¤´æˆ´æ˜¾ç¤ºå™¨æä¾›å®æ—¶æŒ‡å¯¼ã€‚</li>
<li>results: æ¯”ä¼ ç»Ÿç»„è£…æ–¹æ³•æ›´é«˜æ•ˆï¼ŒNLPç®—æ³•ç”Ÿæˆçš„ç»„è£…åºåˆ—å¯ä»¥è¾¾åˆ°åŒæ ·çš„å¯ç”¨æ€§ã€‚<details>
<summary>Abstract</summary>
The assembly instruction is a mandatory component of Lego-like brick sets.The conventional production of assembly instructions requires a considerable amount of manual fine-tuning, which is intractable for casual users and customized brick sets.Moreover, the traditional paper-based instructions lack expressiveness and interactivity.To tackle the two problems above, we present BrickPal, an augmented reality-based system, which visualizes assembly instructions in an augmented reality head-mounted display. It utilizes Natural Language Processing (NLP) techniques to generate plausible assembly sequences, and provide real-time guidance in the AR headset.Our user study demonstrates BrickPal's effectiveness at assisting users in brick assembly compared to traditional assembly methods. Additionally, the NLP algorithm-generated assembly sequences achieve the same usability with manually adapted sequences.
</details>
<details>
<summary>æ‘˜è¦</summary>
assembly instructionæ˜¯ä¹é«˜ç±»ç§¯æœ¨setsä¸­å¿…å¤‡çš„ä¸€éƒ¨åˆ†ã€‚ä¼ ç»Ÿç”Ÿäº§assembly instructionéœ€è¦è¾ƒå¤šçš„æ‰‹åŠ¨ç²¾åº¦è°ƒæ•´ï¼Œè¿™å¯¹æ™®é€šç”¨æˆ·å’Œè‡ªå®šä¹‰ç§¯æœ¨setsæ¥è¯´æ˜¯ä¸å¯æ¥å—çš„ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿçš„çº¸é¢æŒ‡ä»¤ç¼ºä¹è¡¨è¾¾åŠ›å’Œäº’åŠ¨æ€§ã€‚ä¸ºè§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BrickPalï¼Œä¸€ç§åŸºäºæ‰©å±•ç°å®æŠ€æœ¯çš„ç³»ç»Ÿï¼Œå¯ä»¥åœ¨æ‰©å±•ç°å®å¤´æˆ´displayä¸­å¯è§åŒ– assembly instructionã€‚å®ƒåˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯ç”Ÿæˆå¯èƒ½çš„ç§¯æœ¨ç»„åˆåºåˆ—ï¼Œå¹¶åœ¨ARå¤´æˆ´displayä¸­æä¾›å®æ—¶æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒBrickPalå¯ä»¥è¾ƒä¼ ç»ŸAssemblyæ–¹æ³•æ›´å¥½åœ°å¸®åŠ©ç”¨æˆ·ç»„è£…ç§¯æœ¨ã€‚æ­¤å¤–ï¼Œç”±NLPç®—æ³•ç”Ÿæˆçš„ç§¯æœ¨ç»„åˆåºåˆ—ä¸æ‰‹åŠ¨ä¿®æ”¹åçš„åºåˆ—ä¹‹é—´æ²¡æœ‰å·®å¼‚ã€‚
</details></li>
</ul>
<hr>
<h2 id="Distilling-Large-Vision-Language-Model-with-Out-of-Distribution-Generalizability"><a href="#Distilling-Large-Vision-Language-Model-with-Out-of-Distribution-Generalizability" class="headerlink" title="Distilling Large Vision-Language Model with Out-of-Distribution Generalizability"></a>Distilling Large Vision-Language Model with Out-of-Distribution Generalizability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03135">http://arxiv.org/abs/2307.03135</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuanlinli17/large_vlm_distillation_ood">https://github.com/xuanlinli17/large_vlm_distillation_ood</a></li>
<li>paper_authors: Xuanlin Li, Yunhao Fang, Minghua Liu, Zhan Ling, Zhuowen Tu, Hao Su</li>
<li>for: è¿™ä¸ªç ”ç©¶çš„ç›®çš„æ˜¯å°†å¤§å‹æè¿°è¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºè½»é‡çº§å¿«é€Ÿæ¨¡å‹ï¼Œä»¥ä¾¿åœ¨æœ‰é™çš„èµ„æºå’Œæ—¶é—´ä¸Šå®ç°å®é™…çš„åº”ç”¨ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†æ•™å¸ˆæ¨¡å‹çš„æè¿°è¯­è¨€è¡¨ç¤ºç©ºé—´å†…çš„å­¦ä¹ ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå­¦ç”Ÿæ¨¡å‹ã€‚å®ƒè¿˜æå‡ºäº†ä¸¤ä¸ªåŸåˆ™æ¥å¢å¼ºå­¦ç”Ÿçš„å¼€ vocabulary out-of-distributionï¼ˆOODï¼‰æ³›åŒ–æ€§ï¼šä¸€æ˜¯æ›´å¥½åœ°æ¨¡ä»¿æ•™å¸ˆçš„æè¿°è¯­è¨€è¡¨ç¤ºç©ºé—´ï¼Œå¹¶è°¨æ…åœ°å¢å¼ºè§†è¯­è”ç³»çš„ä¸€è‡´æ€§; äºŒæ˜¯å¢å¼ºæ•™å¸ˆçš„è¯­è¨€è¡¨ç¤ºå…·æœ‰æœ‰ç”¨å’Œç»†éƒ¨çš„Semantic Attributeï¼Œä»¥ä¾¿æ›´å¥½åœ°åŒºåˆ«ä¸åŒçš„æ ‡ç­¾ã€‚</li>
<li>results: è¿™ä¸ªç ”ç©¶çš„ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨äº†æå‡ºçš„æ–¹æ³•å¯ä»¥å®ç°é›¶shotå’Œå‡ shotå­¦ç”Ÿæ¨¡å‹åœ¨å¼€ vocabulary OODåˆ†ç±»ä»»åŠ¡ä¸­çš„æ˜¾è‘—æ”¹å–„ï¼Œè¿™è¯´æ˜äº†æˆ‘ä»¬çš„æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚<details>
<summary>Abstract</summary>
Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a small- or mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enriching the teacher's language representations with informative and finegrained semantic attributes to effectively distinguish between different labels. We propose several metrics and conduct extensive experiments to investigate their techniques. The results demonstrate significant improvements in zero-shot and few-shot student performance on open-vocabulary out-of-distribution classification, highlighting the effectiveness of our proposed approaches. Code released at https://github.com/xuanlinli17/large_vlm_distillation_ood
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è§†è¯­æ¨¡å‹å·²ç»å®ç°å‡ºè‰²çš„è¡¨ç°ï¼Œä½†å®ƒä»¬çš„å¤§å°å’Œè®¡ç®—éœ€æ±‚ä½¿å…¶åœ¨æœ‰é™çš„è®¾å¤‡å’Œæ—¶é—´ä¸Šä¸å¤ªå®ç”¨ã€‚æ¨¡å‹ç¼©å°ï¼Œå°†å¤§å‹æ¨¡å‹è½¬æ¢æˆæ›´å°ã€æ›´å¿«çš„æ¨¡å‹ï¼Œä»¥ä¿æŒå…¶æ€§èƒ½çš„æ–¹å‘æ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å°†å¤§æ•™å¸ˆè§†è¯­æ¨¡å‹ä¸­çš„è§†è§‰è¡¨ç¤ºå‹ç¼©åˆ°å°å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œä½¿ç”¨å°è§„æ¨¡æˆ–ä¸­è§„æ¨¡çš„æ•°æ®é›†ã€‚å°¤å…¶æ˜¯è¿™ç§ç ”ç©¶å¼ºè°ƒäº†å¼€æ”¾è¯æ±‡ OUT-OF-DISTRIBUTIONï¼ˆOODï¼‰æ³›åŒ–ï¼Œè¿™æ˜¯ä¹‹å‰çš„æ¨¡å‹ç¼©å°æ–‡çŒ®ä¸­å°šæœªå¾—åˆ°è¶³å¤Ÿçš„å…³æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªåŸåˆ™ï¼Œä¸€æ˜¯åœ¨è§†è§‰è¡¨ç¤ºç©ºé—´ä¸Šæ›´å¥½åœ°æ¨¡ä»¿å¤§æ•™å¸ˆï¼ŒäºŒæ˜¯åœ¨è§†è¯­å¯¹åº”ä¸Šæ›´åŠ ç²¾ç»†åœ°åè°ƒå¤§æ•™å¸ˆçš„è¯­è¨€è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å¤šä¸ªæŒ‡æ ‡ï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒæ¥è°ƒæŸ¥è¿™äº›æŠ€æœ¯çš„æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æè®®æ–¹æ³•å¯ä»¥åœ¨é›¶shotå’Œå‡ shotæƒ…å†µä¸‹æé«˜å°å­¦ç”Ÿæ¨¡å‹çš„OODæ³›åŒ–æ€§èƒ½ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯ä»¥åœ¨https://github.com/xuanlinli17/large_vlm_distillation_oodä¸Šä¸‹è½½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Frontier-AI-Regulation-Managing-Emerging-Risks-to-Public-Safety"><a href="#Frontier-AI-Regulation-Managing-Emerging-Risks-to-Public-Safety" class="headerlink" title="Frontier AI Regulation: Managing Emerging Risks to Public Safety"></a>Frontier AI Regulation: Managing Emerging Risks to Public Safety</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03718">http://arxiv.org/abs/2307.03718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung, Cullen Oâ€™Keefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth, Robert Trager, Kevin Wolf<br>for:è¿™ç¯‡è®ºæ–‡å…³æ³¨äºæ‰€è°“çš„â€å‰æ²¿AIâ€æ¨¡å‹ï¼Œå³å…·æœ‰å±é™©èƒ½åŠ›çš„åŸºç¡€æ¨¡å‹ï¼Œå¯èƒ½ä¼šå¯¹å…¬å…±å®‰å…¨é€ æˆä¸¥é‡å¨èƒã€‚è¿™ç±»æ¨¡å‹çš„ç®¡ç†å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ï¼šä¸å¯é¢„æœŸçš„å±é™©èƒ½åŠ›å‡ºç°ï¼Œéš¾ä»¥é˜²æ­¢å·²ç»éƒ¨ç½²çš„æ¨¡å‹è¢«è¿ç”¨ï¼Œä»¥åŠæ¨¡å‹èƒ½åŠ›çš„æ™®åŠã€‚methods:ä½œè€…æå‡ºäº†ä¸‰ä¸ªå»ºè®®æ¥ç®¡ç†å‰æ²¿AIæ¨¡å‹çš„å¼€å‘å’Œéƒ¨ç½²ï¼šï¼ˆ1ï¼‰ä¸ºå‰æ²¿AIå¼€å‘è€…è®¾ç½®æ ‡å‡†ï¼Œï¼ˆ2ï¼‰è¦æ±‚å¼€å‘è€…ç™»è®°å’ŒæŠ¥é€ç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¾¿è®©ç›‘ç®¡éƒ¨é—¨æœ‰visibility intoå‰æ²¿AIå¼€å‘è¿‡ç¨‹ï¼Œï¼ˆ3ï¼‰ç¡®ä¿æ¨¡å‹çš„å¼€å‘å’Œéƒ¨ç½²ç¬¦åˆå®‰å…¨æ ‡å‡†ã€‚results:ä½œè€…è®¤ä¸ºï¼Œäº’è”ç½‘äº§ä¸šè‡ªå¾‹ç®¡ç†æ˜¯é‡è¦çš„é¦–å…ˆæ­¥éª¤ï¼Œä½†æ˜¯æ›´å¹¿æ³›çš„ç¤¾ä¼šè®¨è®ºå’Œæ”¿åºœå¹²é¢„å°†æ˜¯å¿…è¦çš„ï¼Œä»¥åˆ›å»ºæ ‡å‡†å¹¶ç¡®ä¿å…¶éµå®ˆã€‚ä»–ä»¬è¿˜æå‡ºäº†ä¸€äº›é€‰æ‹©ï¼ŒåŒ…æ‹¬æˆäºˆç›‘ç®¡æœºæ„æ‰§æ³•æƒå’Œå‰æ²¿AIæ¨¡å‹çš„æ‰§ç…§åˆ¶åº¦ã€‚æœ€åï¼Œä½œè€…æå‡ºäº†ä¸€äº›å®‰å…¨æ ‡å‡†ï¼ŒåŒ…æ‹¬åœ¨éƒ¨ç½²ä¹‹å‰è¿›è¡Œé£é™©è¯„ä¼°ï¼Œå¤–éƒ¨å®¡æŸ¥æ¨¡å‹è¡Œä¸ºï¼Œæ ¹æ®é£é™©è¯„ä¼°å†³å®šéƒ¨ç½²ï¼Œä»¥åŠåœ¨éƒ¨ç½²åç›‘æµ‹å’Œåº”å¯¹æ–°çš„æ¨¡å‹èƒ½åŠ›å’Œç”¨é€”ä¿¡æ¯ã€‚<details>
<summary>Abstract</summary>
Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term "frontier AI" models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deployment of frontier AI models. Industry self-regulation is an important first step. However, wider societal discussions and government intervention will be needed to create standards and to ensure compliance with them. We consider several options to this end, including granting enforcement powers to supervisory authorities and licensure regimes for frontier AI models. Finally, we propose an initial set of safety standards. These include conducting pre-deployment risk assessments; external scrutiny of model behavior; using risk assessments to inform deployment decisions; and monitoring and responding to new information about model capabilities and uses post-deployment. We hope this discussion contributes to the broader conversation on how to balance public safety risks and innovation benefits from advances at the frontier of AI development.
</details>
<details>
<summary>æ‘˜è¦</summary>
é«˜åº¦æ™ºèƒ½åŒ–æ¨¡å‹å…·æœ‰å·¨å¤§çš„ç¤¾ä¼šä»·å€¼ï¼Œä½†ç¤¾ä¼šéœ€è¦ç§¯æç®¡ç†è¿™äº›æ¨¡å‹çš„é£é™©ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨äºæˆ‘ä»¬ç§°ä¸ºâ€œå‰æ²¿AIâ€æ¨¡å‹ï¼šé«˜åº¦å¯èƒ½çš„åŸºç¡€æ¨¡å‹ï¼Œå®ƒä»¬å¯èƒ½å…·æœ‰ä¸¥é‡å±å®³å…¬å…±å®‰å…¨çš„èƒ½åŠ›ã€‚å‰æ²¿AIæ¨¡å‹æå‡ºäº†ä¸€ç³»åˆ—æŒ‘æˆ˜ï¼šå±é™©èƒ½åŠ›å¯èƒ½ä¼šä¸æ–™å‡ºç°ï¼›ä¸å¯é¢„æ–™åœ°ä½¿ç”¨å·²ç»éƒ¨ç½²çš„æ¨¡å‹ï¼›æ¨¡å‹çš„èƒ½åŠ›å¾ˆéš¾æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè‡³å°‘éœ€è¦ä¸‰ç§å»ºç­‘ç‰©æ¥è§„èŒƒå‰æ²¿AIæ¨¡å‹çš„å‘å±•ï¼šï¼ˆ1ï¼‰ä¸ºå‰æ²¿AIå¼€å‘è€…è®¾ç½®æ ‡å‡†ï¼›ï¼ˆ2ï¼‰è¦æ±‚å¼€å‘è€…æ³¨å†Œå¹¶æŠ¥å‘ŠFrontier AIçš„å¼€å‘è¿›åº¦ï¼›ï¼ˆ3ï¼‰ç¡®ä¿Frontier AIæ¨¡å‹çš„å®‰å…¨æ ‡å‡†çš„å®æ–½å’Œéƒ¨ç½²ã€‚äº’è”ç½‘è‡ªå¾‹ç®¡ç†æ˜¯é‡è¦çš„é¦–å…ˆæ­¥éª¤ï¼Œä½†ç¤¾ä¼šè®¨è®ºå’Œæ”¿åºœå¹²é¢„å°†æ˜¯å¿…è¦çš„ï¼Œä»¥åˆ›å»ºæ ‡å‡†å¹¶ç¡®ä¿éµä»å…¶ä¸­ã€‚æˆ‘ä»¬è€ƒè™‘äº†è®¸å¤šé€‰é¡¹ï¼ŒåŒ…æ‹¬æˆæƒç›‘ç®¡æœºæ„æ‰§æ³•æƒå’ŒFrontier AIæ¨¡å‹çš„è®¸å¯è¯åˆ¶åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç»„å®‰å…¨æ ‡å‡†ï¼ŒåŒ…æ‹¬åœ¨éƒ¨ç½²ä¹‹å‰è¿›è¡Œé£é™©è¯„ä¼°ï¼›å¯¹æ¨¡å‹è¡Œä¸ºè¿›è¡Œå¤–éƒ¨å®¡æŸ¥ï¼›ä½¿ç”¨é£é™©è¯„ä¼°æ¥å†³å®šéƒ¨ç½²çš„å†³ç­–ï¼›ä»¥åŠåœ¨éƒ¨ç½²åç›‘æµ‹å’Œå›åº”æ–°çš„æ¨¡å‹èƒ½åŠ›å’Œä½¿ç”¨ä¿¡æ¯ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡è®ºæ–‡èƒ½å¤Ÿè´¡çŒ®åˆ°AIæŠ€æœ¯çš„å‰æ²¿å‘å±•ä¸­å…¬å…±å®‰å…¨é£é™©å’Œåˆ›æ–°å¥–åŠ±ä¹‹é—´çš„å¹³è¡¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Learning-Multi-Agent-Intention-Aware-Communication-for-Optimal-Multi-Order-Execution-in-Finance"><a href="#Learning-Multi-Agent-Intention-Aware-Communication-for-Optimal-Multi-Order-Execution-in-Finance" class="headerlink" title="Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance"></a>Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03119">http://arxiv.org/abs/2307.03119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchen Fang, Zhenggang Tang, Kan Ren, Weiqing Liu, Li Zhao, Jiang Bian, Dongsheng Li, Weinan Zhang, Yong Yu, Tie-Yan Liu</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“å­¦ä¹ ï¼ˆMARLï¼‰çš„å¤šè®¢å•æ‰§è¡Œæ–¹æ³•ï¼Œä»¥ä¼˜åŒ–è‚¡ç¥¨äº¤æ˜“çš„æ‰§è¡Œæ•ˆç‡ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†æ¨¡å‹è‡ªé€‚åº”å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œå¹¶åœ¨å¤šæ™ºèƒ½ä½“å­¦ä¹ ï¼ˆMARLï¼‰æ¡†æ¶ä¸‹è¿›è¡Œäº†ä¼˜åŒ–ã€‚åœ¨å®é™…å¸‚åœºæ•°æ®ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶é€šè¿‡å­¦ä¹ å¤šè½®é€šä¿¡åè®®æ¥æé«˜åä½œæ•ˆæœã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æœ¬ç ”ç©¶çš„æ–¹æ³•å¯ä»¥åœ¨è‚¡ç¥¨äº¤æ˜“ä¸­æé«˜æ‰§è¡Œæ•ˆç‡ï¼Œå¹¶ä¸”ä¸ä¼ ç»Ÿçš„å•ä¸ªè®¢å•æ‰§è¡Œæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„åä½œæ•ˆæœã€‚<details>
<summary>Abstract</summary>
Order execution is a fundamental task in quantitative finance, aiming at finishing acquisition or liquidation for a number of trading orders of the specific assets. Recent advance in model-free reinforcement learning (RL) provides a data-driven solution to the order execution problem. However, the existing works always optimize execution for an individual order, overlooking the practice that multiple orders are specified to execute simultaneously, resulting in suboptimality and bias. In this paper, we first present a multi-agent RL (MARL) method for multi-order execution considering practical constraints. Specifically, we treat every agent as an individual operator to trade one specific order, while keeping communicating with each other and collaborating for maximizing the overall profits. Nevertheless, the existing MARL algorithms often incorporate communication among agents by exchanging only the information of their partial observations, which is inefficient in complicated financial market. To improve collaboration, we then propose a learnable multi-round communication protocol, for the agents communicating the intended actions with each other and refining accordingly. It is optimized through a novel action value attribution method which is provably consistent with the original learning objective yet more efficient. The experiments on the data from two real-world markets have illustrated superior performance with significantly better collaboration effectiveness achieved by our method.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ‰§è¡Œè®¢å•æ˜¯é‡‘èç§‘å­¦ä¸­çš„åŸºæœ¬ä»»åŠ¡ï¼Œæ—¨åœ¨å®Œæˆè´­ä¹°æˆ–å”®å–ç‰¹å®šèµ„äº§çš„äº¤æ˜“è®¢å•ã€‚ç°ä»£æ— æ¨¡å‹å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯æä¾›äº†ä¸€ç§æ•°æ®é©±åŠ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„å·¥ä½œéƒ½æ˜¯ä¼˜åŒ–å•ä¸ªè®¢å•çš„æ‰§è¡Œï¼Œå¿½ç•¥äº†å®é™…æƒ…å†µä¸‹å¤šä¸ªè®¢å•åŒæ—¶æ‰§è¡Œçš„ç°è±¡ï¼Œä»è€Œå¯¼è‡´ä¼˜åŒ–ä¸è¶³å’Œåè§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†å¤šä¸ªä»£ç†RLï¼ˆMARLï¼‰æ–¹æ³•ï¼Œç”¨äºå¤šè®¢å•æ‰§è¡Œï¼Œè€ƒè™‘åˆ°å®é™…çº¦æŸã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªä»£ç†è§†ä¸ºä¸€ä¸ªä¸ªäººæ“ä½œè€…ï¼Œè´Ÿè´£äº¤æ˜“ä¸€ä¸ªç‰¹å®šçš„è®¢å•ï¼ŒåŒæ—¶ä¸åˆ«çš„ä»£ç†è¿›è¡Œäº¤æµå’Œåˆä½œï¼Œä»¥æœ€å¤§åŒ–æ€»æ”¶ç›Šã€‚ä½†ç°æœ‰çš„MARLç®—æ³•é€šå¸¸é€šè¿‡äº¤æ¢åªæœ‰å„è‡ªéƒ¨åˆ†è§‚å¯Ÿä¿¡æ¯æ¥è¿›è¡Œäº¤æµï¼Œè¿™åœ¨å¤æ‚çš„é‡‘èå¸‚åœºä¸­æ˜¯ä¸å…·æœ‰æ•ˆæœçš„ã€‚ä¸ºäº†æé«˜åä½œï¼Œæˆ‘ä»¬ THEN proposeäº†ä¸€ç§å¯å­¦ä¹ çš„å¤šè½®äº¤æµåè®®ï¼Œç”¨äºä»£ç†ä¹‹é—´äº¤æ¢æ„å›¾åŠ¨ä½œï¼Œå¹¶æ ¹æ®æ­¤è¿›è¡Œä¿®æ”¹ã€‚å®ƒæ˜¯é€šè¿‡ä¸€ç§æ–°çš„åŠ¨ä½œä»·å€¼è¯„ä¼°æ–¹æ³•æ¥ä¼˜åŒ–çš„ï¼Œè¯¥æ–¹æ³•æ˜¯åŸå§‹å­¦ä¹ ç›®æ ‡çš„å¯é çš„å»¶å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªå®é™…å¸‚åœºçš„æ•°æ®ä¸Šæ˜¾ç¤ºå‡ºäº†æ˜¾è‘—æ€§çš„æé«˜ï¼Œå¹¶ achieves æ›´å¥½çš„åä½œæ•ˆæœã€‚
</details></li>
</ul>
<hr>
<h2 id="Region-Wise-Attentive-Multi-View-Representation-Learning-for-Urban-Region-Embeddings"><a href="#Region-Wise-Attentive-Multi-View-Representation-Learning-for-Urban-Region-Embeddings" class="headerlink" title="Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings"></a>Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03212">http://arxiv.org/abs/2307.03212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiliang Chan, Qianqian Ren</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨ Addressing the challenges of urban region embedding by proposing a Region-Wise Multi-View Representation Learning (ROMER) model.</li>
<li>methods: è¯¥æ¨¡å‹ä½¿ç”¨å¤šè§†è§’ç›¸å…³æ€§ capture å’Œå…¨çƒå›¾æ³¨æ„åŠ›ç½‘ç»œå­¦ä¹ åŸå¸‚åŒºåŸŸè¡¨ç¤ºã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒROMER æ¨¡å‹åœ¨ä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­æ¯”å‰STATE-OF-THE-ART æ–¹æ³•æé«˜äº†17%ã€‚<details>
<summary>Abstract</summary>
Urban region embedding is an important and yet highly challenging issue due to the complexity and constantly changing nature of urban data. To address the challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER) to capture multi-view dependencies and learn expressive representations of urban regions without the constraints of rigid neighbourhood region conditions. Our model focus on learn urban region representation from multi-source urban data. First, we capture the multi-view correlations from mobility flow patterns, POI semantics and check-in dynamics. Then, we adopt global graph attention networks to learn similarity of any two vertices in graphs. To comprehensively consider and share features of multiple views, a two-stage fusion module is further proposed to learn weights with external attention to fuse multi-view embeddings. Extensive experiments for two downstream tasks on real-world datasets demonstrate that our model outperforms state-of-the-art methods by up to 17\% improvement.
</details>
<details>
<summary>æ‘˜è¦</summary>
<style>.Simplified Chinese {font-family: "Microsoft YaHei";}</style>åŸå¸‚åŒºåŸŸåµŒå…¥æ˜¯ä¸€ä¸ªé‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œç”±äºåŸå¸‚æ•°æ®çš„å¤æ‚æ€§å’Œä¸æ–­å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šè§†å›¾è¡¨ç¤ºå­¦ä¹ ï¼ˆROMERï¼‰ï¼Œç”¨äºæ•æ‰å¤šè§†å›¾ä¾èµ–å…³ç³»å¹¶å­¦ä¹ è¡¨è¾¾åŸå¸‚åŒºåŸŸçš„è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¸“æ³¨äºä»å¤šä¸ªåŸå¸‚æ•°æ®æºä¸Šå­¦ä¹ åŸå¸‚åŒºåŸŸè¡¨ç¤ºã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ•æ‰äº†æµåŠ¨äººå‘˜è¶‹åŠ¿ã€ POI  semantics å’Œæ£€æŸ¥å…¥åŠ¨æ€çš„å¤šè§†å›¾ç›¸å…³æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨å…¨çƒå›¾æ³¨æ„ç½‘ç»œæ¥å­¦ä¹ å›¾ä¸­ä»»æ„ä¸¤ä¸ªé¡¶ç‚¹çš„ç›¸ä¼¼æ€§ã€‚ä¸ºäº†å…¨é¢è€ƒè™‘å’Œå…±äº«å¤šè§†å›¾ç‰¹å¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªé˜¶æ®µèåˆæ¨¡å—ï¼Œä»¥å¤–éƒ¨æ³¨æ„åŠ›å­¦ä¹ å¤šè§†å›¾åµŒå…¥çš„æƒé‡ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å®é™… datasets ä¸Šçš„ä¸¤ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šæ¯”çŠ¶æ€é©å‘½æ–¹æ³•æé«˜äº†17%ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Evaluation-of-Large-Language-Models"><a href="#A-Survey-on-Evaluation-of-Large-Language-Models" class="headerlink" title="A Survey on Evaluation of Large Language Models"></a>A Survey on Evaluation of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03109">http://arxiv.org/abs/2307.03109</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlgroupjlu/llm-eval-survey">https://github.com/mlgroupjlu/llm-eval-survey</a></li>
<li>paper_authors: Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, Xing Xie</li>
<li>for: The paper is written to provide a comprehensive review of evaluation methods for large language models (LLMs), with a focus on three key dimensions: what to evaluate, where to evaluate, and how to evaluate.</li>
<li>methods: The paper uses a survey-based approach to evaluate LLMs, covering various evaluation tasks, benchmarks, and methods.</li>
<li>results: The paper summarizes the success and failure cases of LLMs in different tasks, and highlights several future challenges that lie ahead in LLMs evaluation.Here is the same information in Simplified Chinese text:</li>
<li>for: è¯¥è®ºæ–‡æ˜¯ä¸ºäº†æä¾›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¯„ä¼°æ–¹æ³•çš„å…¨é¢å›é¡¾ï¼Œå¼ºè°ƒä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šè¯„ä¼°ä»»åŠ¡ã€è¯„ä¼°åœºæ™¯å’Œè¯„ä¼°æ–¹æ³•ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨é—®å·æ–¹å¼è¿›è¡Œè¯„ä¼°ï¼Œæ¶µç›–äº†å„ç§è¯„ä¼°ä»»åŠ¡ã€æ ‡å‡†å¥—ä»¶å’Œè¯„ä¼°æ–¹æ³•ã€‚</li>
<li>results: è®ºæ–‡æ€»ç»“äº†ä¸åŒä»»åŠ¡ä¸­ LLMs çš„æˆåŠŸå’Œå¤±è´¥æ¡ˆä¾‹ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥è¯„ä¼°é¢†åŸŸçš„ä¸€äº›æŒ‘æˆ˜ã€‚<details>
<summary>Abstract</summary>
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficient-Domain-Adaptation-of-Sentence-Embeddings-Using-Adapters"><a href="#Efficient-Domain-Adaptation-of-Sentence-Embeddings-Using-Adapters" class="headerlink" title="Efficient Domain Adaptation of Sentence Embeddings Using Adapters"></a>Efficient Domain Adaptation of Sentence Embeddings Using Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03104">http://arxiv.org/abs/2307.03104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sebischair/efficient-domain-adaptation-of-sentence-embeddings-using-adapters">https://github.com/sebischair/efficient-domain-adaptation-of-sentence-embeddings-using-adapters</a></li>
<li>paper_authors: Tim Schopf, Dennis N. Schneider, Florian Matthes</li>
<li>for: ç”¨äºåŸŸ adaptation of sentence embeddings</li>
<li>methods: ä½¿ç”¨lightweight adapters for parameter-efficient domain adaptation</li>
<li>results: å¯ä»¥è¾¾åˆ°1%çš„ç«äº‰æ€§è¡¨ç°ï¼Œåªéœ€è¦è®­ç»ƒçº¦3.6%çš„å‚æ•°ã€‚Here is the full sentence in Simplified Chinese:</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†åŸŸ adaptation of sentence embeddingsè€Œå†™çš„ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†lightweight adaptersæ¥å®ç° parameter-efficient domain adaptationã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡å¯ä»¥è¾¾åˆ°1%çš„ç«äº‰æ€§è¡¨ç°ï¼Œåªéœ€è¦è®­ç»ƒçº¦3.6%çš„å‚æ•°ã€‚<details>
<summary>Abstract</summary>
Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always using the same base model and only exchanging the domain-specific adapters to adapt sentence embeddings to a specific domain. We show that using adapters for parameter-efficient domain adaptation of sentence embeddings yields competitive performance within 1% of a domain-adapted, entirely fine-tuned sentence embedding model while only training approximately 3.6% of the parameters.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Structure-Guided-Multi-modal-Pre-trained-Transformer-for-Knowledge-Graph-Reasoning"><a href="#Structure-Guided-Multi-modal-Pre-trained-Transformer-for-Knowledge-Graph-Reasoning" class="headerlink" title="Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph Reasoning"></a>Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.03591">http://arxiv.org/abs/2307.03591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Liang, Sihang Zhou, Yue Liu, Lingyuan Meng, Meng Liu, Xinwang Liu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§åŸºäºå¤šæ¨¡æ€çŸ¥è¯†å›¾(MKG)çš„å¤šæ¨¡æ€é¢„è®­ç»ƒ transformer æ¨¡å‹(SGMPT)ï¼Œä»¥æé«˜å¤šæ¨¡æ€çŸ¥è¯†å›¾ç†è§£(KGR)çš„æ€§èƒ½ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å›¾ç»“æ„ç¼–ç å™¨æ¥ç¼–ç çŸ¥è¯†å›¾çš„ç»“æ„ç‰¹å¾ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ç»“æ„æŒ‡å¯¼åˆå¹¶æ¨¡å—ï¼Œé€šè¿‡ä¸¤ç§ä¸åŒçš„ç­–ç•¥ï¼ˆåŠ æƒæ±‡å’Œå¯¹é½çº¦æŸï¼‰å°†ç»“æ„ä¿¡æ¯æ³¨å…¥åˆ°æ–‡æœ¬å’Œå›¾åƒç‰¹å¾ä¸­ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ SGMPT æ¨¡å‹åœ¨ FB15k-237-IMG å’Œ WN18-IMG ä¸Šå¯¹å¤šæ¨¡æ€ KGR  Task è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†ç°æœ‰çš„çŠ¶æ€ç æ¨¡å‹ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚<details>
<summary>Abstract</summary>
Multimodal knowledge graphs (MKGs), which intuitively organize information in various modalities, can benefit multiple practical downstream tasks, such as recommendation systems, and visual question answering. However, most MKGs are still far from complete, which motivates the flourishing of MKG reasoning models. Recently, with the development of general artificial architectures, the pretrained transformer models have drawn increasing attention, especially for multimodal scenarios. However, the research of multimodal pretrained transformer (MPT) for knowledge graph reasoning (KGR) is still at an early stage. As the biggest difference between MKG and other multimodal data, the rich structural information underlying the MKG still cannot be fully leveraged in existing MPT models. Most of them only utilize the graph structure as a retrieval map for matching images and texts connected with the same entity. This manner hinders their reasoning performances. To this end, we propose the graph Structure Guided Multimodal Pretrained Transformer for knowledge graph reasoning, termed SGMPT. Specifically, the graph structure encoder is adopted for structural feature encoding. Then, a structure-guided fusion module with two different strategies, i.e., weighted summation and alignment constraint, is first designed to inject the structural information into both the textual and visual features. To the best of our knowledge, SGMPT is the first MPT model for multimodal KGR, which mines the structural information underlying the knowledge graph. Extensive experiments on FB15k-237-IMG and WN18-IMG, demonstrate that our SGMPT outperforms existing state-of-the-art models, and prove the effectiveness of the designed strategies.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šModalçŸ¥è¯†å›¾(MKG)å¯ä»¥æœ‰æ•ˆåœ°æé«˜å¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½,å¦‚æ¨èç³»ç»Ÿå’Œè§†è§‰é—®ç­”ç³»ç»Ÿã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°MKGéƒ½è¿˜ä¸å¤Ÿå®Œæ•´ï¼Œè¿™äº› incomplete MKG ä»ç„¶éœ€è¦å¤§é‡çš„ç ”ç©¶å’Œå‘å±•ã€‚åœ¨ current çš„æ™®é€šäººå·¥æ™ºèƒ½æ¶æ„ä¸‹, é¢„è®­ç»ƒå˜æ¢å™¨æ¨¡å‹åœ¨å¤šModalåœºæ™¯ä¸­å—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œ, å…³äºå¤šModalé¢„è®­ç»ƒå˜æ¢å™¨(MPT)çš„ç ”ç©¶åœ¨çŸ¥è¯†å›¾ç†è§£(KGR)æ–¹é¢ä»ç„¶å¤„äºæ—©æœŸé˜¶æ®µã€‚ä¸å…¶ä»–å¤šModalæ•°æ®ä¸åŒçš„æ˜¯, çŸ¥è¯†å›¾ä¸‹çš„ä¸°å¯Œç»“æ„ä¿¡æ¯ä»ç„¶æ— æ³•å¾—åˆ°å®Œå…¨åˆ©ç”¨ã€‚å¤§å¤šæ•°æ¨¡å‹åªæ˜¯å°†çŸ¥è¯†å›¾ä½œä¸ºå›¾ç»“æ„æ¥åŒ¹é…å›¾åƒå’Œæ–‡æœ¬ç›¸å…³çš„å®ä½“ã€‚è¿™ç§æ–¹å¼é™åˆ¶äº†ä»–ä»¬çš„ç†è§£æ€§èƒ½ã€‚ä¸ºæ­¤, æˆ‘ä»¬æå‡ºäº†åŸºäºå›¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹çš„å¤šModalé¢„è®­ç»ƒå˜æ¢å™¨(SGMPT)ã€‚å…·ä½“æ¥è¯´, SGMPT ä½¿ç”¨å›¾ç»“æ„ç¼–ç å™¨æ¥ç¼–ç ç»“æ„ç‰¹å¾ã€‚ç„¶å, æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç»“æ„æŒ‡å¯¼èåˆæ¨¡å—ï¼Œé€šè¿‡ä¸¤ç§ä¸åŒçš„ç­–ç•¥ï¼Œå³Weighted Sum å’ŒAlignment Constraintï¼Œå°†ç»“æ„ä¿¡æ¯æ³¨å…¥åˆ°æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ä¸­ã€‚æˆ‘ä»¬çŸ¥é“, SGMPT æ˜¯é¦–ä¸ªåœ¨å¤šModal KGR ä¸­ä½¿ç”¨ç»“æ„ä¿¡æ¯çš„ MPT æ¨¡å‹ï¼Œä»è€Œæé«˜äº†çŸ¥è¯†å›¾ç†è§£çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ FB15k-237-IMG å’Œ WN18-IMG ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„SGMPT è¶…è¿‡äº†ç°æœ‰çš„çŠ¶æ€å¯¹æ¨¡å‹ï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„è®¾è®¡ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Site-Agnostic-Multimodal-Deep-Learning-Model-to-Identify-Pro-Eating-Disorder-Content-on-Social-Media"><a href="#A-Novel-Site-Agnostic-Multimodal-Deep-Learning-Model-to-Identify-Pro-Eating-Disorder-Content-on-Social-Media" class="headerlink" title="A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media"></a>A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06775">http://arxiv.org/abs/2307.06775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Feldman<br>for: è¿™é¡¹ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§å¤šmodalæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºåˆ¤æ–­ç¤¾äº¤åª’ä½“ä¸Šçš„å¸–å­æ˜¯å¦æ¨å¹¿ç²¾ç¥é¥®é£Ÿç–¾ç—…ã€‚methods: è¿™é¡¹ç ”ç©¶ä½¿ç”¨äº†Twitterä¸Šçš„æ ‡æ³¨æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒäº†12ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æœ€ç»ˆï¼Œç ”ç©¶äººå‘˜å‘ç°äº†ä¸€ç§å°†RoBERTaè‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹å’ŒMaxViTå›¾åƒåˆ†ç±»æ¨¡å‹è¿›è¡Œèåˆçš„å¤šmodalæ¨¡å‹ï¼Œå…¶ç²¾åº¦å’ŒF1åˆ†æ•°åˆ†åˆ«ä¸º95.9%å’Œ0.959ã€‚results: è¿™é¡¹ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨è¿™ç§å¤šmodalæ¨¡å‹å¯ä»¥åœ¨ä¸ä½¿ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‰æä¸‹ï¼Œå¯¹ç¤¾äº¤åª’ä½“ä¸Šçš„å¸–å­è¿›è¡Œåˆ†ç±»ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜é€šè¿‡å¯¹Twitterä¸Šçš„å…«ä¸ªå“ˆå¸Œæ ‡ç­¾çš„æœªçœ‹è¿‡çš„å¸–å­è¿›è¡Œæ—¶é—´åºåˆ†æï¼Œå‘ç°è‡ª2014å¹´ä»¥æ¥ï¼Œç¤¾äº¤åª’ä½“ä¸Šçš„ç²¾ç¥é¥®é£Ÿç–¾ç—…æ¨å¹¿å†…å®¹çš„ç›¸å¯¹å«é‡åœ¨è¿™äº›ç¤¾åŒºå†…é€æ¸å‡å°‘ã€‚ç„¶è€Œï¼Œåˆ°2018å¹´ï¼Œè¿™äº›å†…å®¹çš„å¢é•¿æˆ–å·²ç»åœæ­¢ä¸‹é™ï¼Œæˆ–è€…åˆå¼€å§‹å¢é•¿ã€‚<details>
<summary>Abstract</summary>
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. This study aimed to create a multimodal deep learning model that can determine if a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959, respectively. The RoBERTa and MaxViT fusion model, deployed to classify an unlabeled dataset of posts from the social media sites Tumblr and Reddit, generated results akin to those of previous research studies that did not employ artificial intelligence-based techniques, indicating that deep learning models can develop insights congruent to those of researchers. Additionally, the model was used to conduct a timeseries analysis of yet unseen Tweets from eight Twitter hashtags, uncovering that, since 2014, the relative abundance of content that promotes eating disorders has decreased drastically within those communities. Despite this reduction, by 2018, content that promotes eating disorders had either stopped declining or increased in ampleness anew on these hashtags.
</details>
<details>
<summary>æ‘˜è¦</summary>
A labeled dataset of tweets was collected from Twitter, and twelve deep learning models were trained and tested. The best-performing model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, achieving accuracy and F1 scores of 95.9% and 0.959, respectively. This model was then applied to classify unlabeled posts from Tumblr and Reddit, producing results similar to previous research studies that did not use AI-based techniques.Moreover, the model was used to conduct a time series analysis of unseen tweets from eight Twitter hashtags, revealing that the relative abundance of content that promotes eating disorders has decreased significantly since 2014 within these communities. However, by 2018, the content that promotes eating disorders had either leveled off or increased again on these hashtags.In conclusion, this study demonstrates that deep learning models can identify content that promotes eating disorders on social media, and the results can be used to monitor and understand the trends of eating disorder-related content online.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/07/cs.AI_2023_07_07/" data-id="clltaagm80008r8888abz0kk2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/20/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><a class="page-number" href="/page/20/">20</a><span class="page-number current">21</span><a class="page-number" href="/page/22/">22</a><a class="page-number" href="/page/23/">23</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" href="/page/22/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
