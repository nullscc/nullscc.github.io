
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/2/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_11_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/05/cs.CL_2023_11_05/" class="article-date">
  <time datetime="2023-11-05T11:00:00.000Z" itemprop="datePublished">2023-11-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/05/cs.CL_2023_11_05/">cs.CL - 2023-11-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Robust-Generalization-Strategies-for-Morpheme-Glossing-in-an-Endangered-Language-Documentation-Context"><a href="#Robust-Generalization-Strategies-for-Morpheme-Glossing-in-an-Endangered-Language-Documentation-Context" class="headerlink" title="Robust Generalization Strategies for Morpheme Glossing in an Endangered Language Documentation Context"></a>Robust Generalization Strategies for Morpheme Glossing in an Endangered Language Documentation Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02777">http://arxiv.org/abs/2311.02777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Ginn, Alexis Palmer</li>
<li>for:  investigate the ability of morpheme labeling models to generalize in resource-constrained settings</li>
<li>methods:  weight decay optimization, output denoising, and iterative pseudo-labeling</li>
<li>results:  achieve a 2% improvement on a test set containing texts from unseen genres<details>
<summary>Abstract</summary>
Generalization is of particular importance in resource-constrained settings, where the available training data may represent only a small fraction of the distribution of possible texts. We investigate the ability of morpheme labeling models to generalize by evaluating their performance on unseen genres of text, and we experiment with strategies for closing the gap between performance on in-distribution and out-of-distribution data. Specifically, we use weight decay optimization, output denoising, and iterative pseudo-labeling, and achieve a 2% improvement on a test set containing texts from unseen genres. All experiments are performed using texts written in the Mayan language Uspanteko.
</details>
<details>
<summary>摘要</summary>
通用化在有限资源的设置中 particualrly important，因为可能只有一小部分的可用训练数据表示整个分布中的文本。我们 investigate morpheme labeling模型的通用性，通过评估它们在未看到的类型的文本中的性能，并试用了将数据分布减少到另一个数据分布的策略。Specifically, we use weight decay optimization, output denoising, and iterative pseudo-labeling, and achieve a 2% improvement on a test set containing texts from unseen genres. All experiments are performed using texts written in the Mayan language Uspanteko.Note that Simplified Chinese uses a different set of characters and grammar compared to Traditional Chinese, so the translation may look slightly different.
</details></li>
</ul>
<hr>
<h2 id="Attention-or-Convolution-Transformer-Encoders-in-Audio-Language-Models-for-Inference-Efficiency"><a href="#Attention-or-Convolution-Transformer-Encoders-in-Audio-Language-Models-for-Inference-Efficiency" class="headerlink" title="Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency"></a>Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02772">http://arxiv.org/abs/2311.02772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungho Jeon, Ching-Feng Yeh, Hakan Inan, Wei-Ning Hsu, Rashi Rungta, Yashar Mehdad, Daniel Bikel</li>
<li>for: 这个论文目的是为了证明一种简单的自动学习音频模型可以达到与更复杂的预训练模型相同的推理效率。</li>
<li>methods: 这个论文使用了混合卷积模块和自注意模块的speech transformer Encoder来实现。</li>
<li>results: 研究发现，使用这种speech transformer Encoder可以大幅提高预训练音频模型的推理效率，并且可以与更复杂的预训练模型相当。此外，研究还发现，使用高精度的自注意模块可以提高模型的推理效率，但是使用低位数量的权重量quantization技术可以提高模型的可行性。<details>
<summary>Abstract</summary>
In this paper, we show that a simple self-supervised pre-trained audio model can achieve comparable inference efficiency to more complicated pre-trained models with speech transformer encoders. These speech transformers rely on mixing convolutional modules with self-attention modules. They achieve state-of-the-art performance on ASR with top efficiency. We first show that employing these speech transformers as an encoder significantly improves the efficiency of pre-trained audio models as well. However, our study shows that we can achieve comparable efficiency with advanced self-attention solely. We demonstrate that this simpler approach is particularly beneficial with a low-bit weight quantization technique of a neural network to improve efficiency. We hypothesize that it prevents propagating the errors between different quantized modules compared to recent speech transformers mixing quantized convolution and the quantized self-attention modules.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们显示了一个简单的自动学习预训练音频模型可以达到与更复杂的预训练模型（具有语音变换器Encoder）相似的推理效率。这些语音变换器通过混合卷积模块和自我注意模块来实现。它们在ASR中实现了状态的最佳性能，并且我们首先显示了使用这些语音变换器作为Encoder可以显著改善预训练音频模型的效率。然而，我们的研究表明，我们可以通过高级自注意来达到相似的效率。我们示出了这种更简单的方法在使用低位数量权重量化网络来提高效率时是特别有利。我们假设，这种方法可以避免在不同量化模块之间传递错误，相比于最新的语音变换器混合量化卷积和量化自注意模块。
</details></li>
</ul>
<hr>
<h2 id="Pyclipse-a-library-for-deidentification-of-free-text-clinical-notes"><a href="#Pyclipse-a-library-for-deidentification-of-free-text-clinical-notes" class="headerlink" title="Pyclipse, a library for deidentification of free-text clinical notes"></a>Pyclipse, a library for deidentification of free-text clinical notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02748">http://arxiv.org/abs/2311.02748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Callandra Moore, Jonathan Ranisau, Walter Nelson, Jeremy Petch, Alistair Johnson</li>
<li>for: 本研究旨在提供一个可重现性的自动隐私化评估框架，以便实现手动隐私化的高成本降低，促进临床自然语言处理的进步。</li>
<li>methods: 本研究使用的方法包括提出了一个可 configurable 的评估程序，允许用户在本地临床数据上运行开源隐私化算法，并实现了上述算法之间的比较。</li>
<li>results: 本研究发现，使用不同的文本处理方法、评估方法和来源数据会导致隐私化算法的表现不一，即使在同一个 benchmark 数据集上进行评估。这些差异强调了评估隐私化算法的complexity，并 highlighted 需要一个可重现性、可调整和可扩展的框架，如pyclipse。<details>
<summary>Abstract</summary>
Automated deidentification of clinical text data is crucial due to the high cost of manual deidentification, which has been a barrier to sharing clinical text and the advancement of clinical natural language processing. However, creating effective automated deidentification tools faces several challenges, including issues in reproducibility due to differences in text processing, evaluation methods, and a lack of consistency across clinical domains and institutions. To address these challenges, we propose the pyclipse framework, a unified and configurable evaluation procedure to streamline the comparison of deidentification algorithms. Pyclipse serves as a single interface for running open-source deidentification algorithms on local clinical data, allowing for context-specific evaluation. To demonstrate the utility of pyclipse, we compare six deidentification algorithms across four public and two private clinical text datasets. We find that algorithm performance consistently falls short of the results reported in the original papers, even when evaluated on the same benchmark dataset. These discrepancies highlight the complexity of accurately assessing and comparing deidentification algorithms, emphasizing the need for a reproducible, adjustable, and extensible framework like pyclipse. Our framework lays the foundation for a unified approach to evaluate and improve deidentification tools, ultimately enhancing patient protection in clinical natural language processing.
</details>
<details>
<summary>摘要</summary>
自动化医疗文本数据隐藏是非常重要，因为手动隐藏的成本太高，这成为了分享医疗文本和临床自然语言处理的障碍。然而，创建有效的自动化隐藏工具面临着一些挑战，包括复制性问题由于文本处理方法的不同、评价方法的不一致以及临床领域和机构之间的不一致。为解决这些挑战，我们提出了pyclipse框架，一个统一和可配置的评价过程，以便对临床文本数据进行开源隐藏算法的比较。pyclipse提供了一个单一的界面，用于在本地临床数据上运行开源隐藏算法，以便根据特定的上下文进行评价。为了证明pyclipse的实用性，我们比较了六种隐藏算法在四个公共和两个私人临床文本数据集上的性能。我们发现，算法的性能通常比原始论文中报道的结果低，即使在同一个标准数据集上进行评价。这些差异高亮了评价和比较隐藏算法的复杂性，强调了需要一个可重现、可调整和可扩展的框架，如pyclipse。我们的框架为临床自然语言处理中的患者隐藏做出了基础，以增强患者的保护。
</details></li>
</ul>
<hr>
<h2 id="Nepali-Video-Captioning-using-CNN-RNN-Architecture"><a href="#Nepali-Video-Captioning-using-CNN-RNN-Architecture" class="headerlink" title="Nepali Video Captioning using CNN-RNN Architecture"></a>Nepali Video Captioning using CNN-RNN Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02699">http://arxiv.org/abs/2311.02699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bipesh Subedi, Saugat Singh, Bal Krishna Bal<br>for: 本研究旨在开发一个基于深度神经网络的尼泊尔视频标题生成系统，以提供更 precises和相关的尼泊尔视频描述。methods: 该研究使用了预训练的CNN和RNN，并将MSVD数据集扩展为尼泊尔语标题。研究采用了数据预处理、模型实现和评估等步骤。results: 研究发现，使用EfficientNetB0和BiLSTM结构的模型可以 дости得BLEU-4分数17和METEOR分数46。此外，研究还描述了在尼泊尔语视频标题生成中遇到的挑战和未来研究的方向。<details>
<summary>Abstract</summary>
This article presents a study on Nepali video captioning using deep neural networks. Through the integration of pre-trained CNNs and RNNs, the research focuses on generating precise and contextually relevant captions for Nepali videos. The approach involves dataset collection, data preprocessing, model implementation, and evaluation. By enriching the MSVD dataset with Nepali captions via Google Translate, the study trains various CNN-RNN architectures. The research explores the effectiveness of CNNs (e.g., EfficientNetB0, ResNet101, VGG16) paired with different RNN decoders like LSTM, GRU, and BiLSTM. Evaluation involves BLEU and METEOR metrics, with the best model being EfficientNetB0 + BiLSTM with 1024 hidden dimensions, achieving a BLEU-4 score of 17 and METEOR score of 46. The article also outlines challenges and future directions for advancing Nepali video captioning, offering a crucial resource for further research in this area.
</details>
<details>
<summary>摘要</summary>
To enrich the MSVD dataset with Nepali captions, the researchers use Google Translate. They train various CNN-RNN architectures, including EfficientNetB0, ResNet101, and VGG16, paired with different RNN decoders like long short-term memory (LSTM), gated recurrent unit (GRU), and bidirectional LSTM (BiLSTM). The evaluation metrics used are BLEU and METEOR, and the best model is EfficientNetB0 + BiLSTM with 1024 hidden dimensions, achieving a BLEU-4 score of 17 and METEOR score of 46.The article also discusses challenges and future directions for advancing Nepali video captioning, providing a valuable resource for further research in this area.
</details></li>
</ul>
<hr>
<h2 id="LLM-enhanced-Self-training-for-Cross-domain-Constituency-Parsing"><a href="#LLM-enhanced-Self-training-for-Cross-domain-Constituency-Parsing" class="headerlink" title="LLM-enhanced Self-training for Cross-domain Constituency Parsing"></a>LLM-enhanced Self-training for Cross-domain Constituency Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02660">http://arxiv.org/abs/2311.02660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianling Li, Meishan Zhang, Peiming Guo, Min Zhang, Yue Zhang</li>
<li>for: 本研究探讨了自动训练在跨 домен constituency parsing 中的应用。</li>
<li>methods: 我们提议使用大语言模型（LLM）生成域pecific的raw corpora，以便自动训练。我们还引入了grammar rules来导引LLM生成raw corpora，并设置了pseudo数据选择的 criterion。</li>
<li>results: 我们的实验结果显示，自动训练加LLM在constituency parsing中的性能高于传统方法，无论LLM的性能如何。此外，grammar rules和confidence criterion的结合使得cross-domain constituency parsing的性能最高。<details>
<summary>Abstract</summary>
Self-training has proven to be an effective approach for cross-domain tasks, and in this study, we explore its application to cross-domain constituency parsing. Traditional self-training methods rely on limited and potentially low-quality raw corpora. To overcome this limitation, we propose enhancing self-training with the large language model (LLM) to generate domain-specific raw corpora iteratively. For the constituency parsing, we introduce grammar rules that guide the LLM in generating raw corpora and establish criteria for selecting pseudo instances. Our experimental results demonstrate that self-training for constituency parsing, equipped with an LLM, outperforms traditional methods regardless of the LLM's performance. Moreover, the combination of grammar rules and confidence criteria for pseudo-data selection yields the highest performance in the cross-domain constituency parsing.
</details>
<details>
<summary>摘要</summary>
自我训练已经证明是跨领域任务中有效的方法，在这项研究中，我们探讨了它在跨领域成分分析中的应用。传统的自我训练方法受到有限的和可能低质量的原始raw corpora的限制。为了突破这一限制，我们提议利用大语言模型（LLM）来生成领域特定的raw corpora，并在每一轮training中进行多次迭代。对于成分分析，我们引入了语法规则，以引导LLM生成raw corpora，并确定pseudo实例的选择标准。我们的实验结果表明，带有LLM的自我训练对成分分析进行跨领域均有superior表现，而且将语法规则和信任度标准结合使用，可以在跨领域成分分析中实现最高的表现。
</details></li>
</ul>
<hr>
<h2 id="Divide-Conquer-for-Entailment-aware-Multi-hop-Evidence-Retrieval"><a href="#Divide-Conquer-for-Entailment-aware-Multi-hop-Evidence-Retrieval" class="headerlink" title="Divide &amp; Conquer for Entailment-aware Multi-hop Evidence Retrieval"></a>Divide &amp; Conquer for Entailment-aware Multi-hop Evidence Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02616">http://arxiv.org/abs/2311.02616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Luo, Mihai Surdeanu</li>
<li>for: Answering multi-hop questions by retrieving relevant evidence</li>
<li>methods: 使用文本相似性和推理相似性两个Sub-任务，并将其结合以提高各种相关性信号的推理效果</li>
<li>results: 在HotpotQA上实验表明，提出的模型不仅与单独的检索模型相比显著提高了表现，还比两个直觉的组合基线模型更有效。<details>
<summary>Abstract</summary>
Lexical and semantic matches are commonly used as relevance measurements for information retrieval. Together they estimate the semantic equivalence between the query and the candidates. However, semantic equivalence is not the only relevance signal that needs to be considered when retrieving evidences for multi-hop questions. In this work, we demonstrate that textual entailment relation is another important relevance dimension that should be considered. To retrieve evidences that are either semantically equivalent to or entailed by the question simultaneously, we divide the task of evidence retrieval for multi-hop question answering (QA) into two sub-tasks, i.e., semantic textual similarity and inference similarity retrieval. We propose two ensemble models, EAR and EARnest, which tackle each of the sub-tasks separately and then jointly re-rank sentences with the consideration of the diverse relevance signals. Experimental results on HotpotQA verify that our models not only significantly outperform all the single retrieval models it is based on, but is also more effective than two intuitive ensemble baseline models.
</details>
<details>
<summary>摘要</summary>
lexical和semantic匹配通常用于信息检索中的相关性评估。它们共同估计查询和候选结果之间的semantic相等性。但semantic相等性并不是多步问题回答中唯一的相关性信号。在这种情况下，我们表明了文本包含关系是另一个重要的相关性维度。为了同时从多个角度检索对于多步问题回答的证据，我们将证据检索任务分为两个互补性任务：semantic textual similarity和inference similarity检索。我们提议了两种ensemble模型：EAR和EARnest，它们分别处理每个子任务，然后将其结果联合重新排序 sentences，考虑多种相关性信号的多样性。实验结果表明，我们的模型不仅在HotpotQA上表现出色，而且也比两个直觉ensemble基线模型更有效。
</details></li>
</ul>
<hr>
<h2 id="mahaNLP-A-Marathi-Natural-Language-Processing-Library"><a href="#mahaNLP-A-Marathi-Natural-Language-Processing-Library" class="headerlink" title="mahaNLP: A Marathi Natural Language Processing Library"></a>mahaNLP: A Marathi Natural Language Processing Library</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02579">http://arxiv.org/abs/2311.02579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vidula Magdum, Omkar Dhekane, Sharayu Hiwarkhedkar, Saloni Mittal, Raviraj Joshi</li>
<li>for: 提供了一个开源的自然语言处理（NLP）库，Specifically built for the Marathi language.</li>
<li>methods: 使用了state-of-the-art MahaBERT-based transformer models，Easy to use, extensible, and modular toolkit for Marathi text analysis.</li>
<li>results: 提供了一个包含多个NLP任务的完整array，包括基本的预处理任务和进阶的NLP任务，如情感分析、名实分辨、讨厌言语检测和句子完成。Here’s the simplified Chinese text in the format you requested:</li>
<li>for: 这个论文是为了提供一个开源的自然语言处理（NLP）库，Specifically built for the Marathi language。</li>
<li>methods: 这个库使用了state-of-the-art MahaBERT-based transformer models，Easy to use, extensible, and modular toolkit for Marathi text analysis。</li>
<li>results: 这个库提供了一个包含多个NLP任务的完整array，包括基本的预处理任务和进阶的NLP任务，如情感分析、名实分辨、讨厌言语检测和句子完成。<details>
<summary>Abstract</summary>
We present mahaNLP, an open-source natural language processing (NLP) library specifically built for the Marathi language. It aims to enhance the support for the low-resource Indian language Marathi in the field of NLP. It is an easy-to-use, extensible, and modular toolkit for Marathi text analysis built on state-of-the-art MahaBERT-based transformer models. Our work holds significant importance as other existing Indic NLP libraries provide basic Marathi processing support and rely on older models with restricted performance. Our toolkit stands out by offering a comprehensive array of NLP tasks, encompassing both fundamental preprocessing tasks and advanced NLP tasks like sentiment analysis, NER, hate speech detection, and sentence completion. This paper focuses on an overview of the mahaNLP framework, its features, and its usage. This work is a part of the L3Cube MahaNLP initiative, more information about it can be found at https://github.com/l3cube-pune/MarathiNLP .
</details>
<details>
<summary>摘要</summary>
我们介绍mahaNLP，一个开源的自然语言处理（NLP）库，专门为旁遮普语言（Marathi）设计。它旨在提高低资源印度语言Marathi在NLP领域的支持。它是一个易于使用、可扩展、归结的工具集，基于现代的MahaBERT变换器模型。我们的工作具有重要性，因为其他现有的印度语言NLP库只提供基本的Marathi处理支持，并且基于较老的模型，性能更 restricted。我们的工具集包括了各种NLP任务，包括基本的预处理任务和高级NLP任务，如情感分析、命名实体识别、恶意言语检测和句子完成。本文将介绍mahaNLP框架、其特点和使用方法。这是L3Cube MahaNLP计划的一部分，更多信息可以在https://github.com/l3cube-pune/MarathiNLP 找到。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Sequencing-of-Documents"><a href="#Temporal-Sequencing-of-Documents" class="headerlink" title="Temporal Sequencing of Documents"></a>Temporal Sequencing of Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02578">http://arxiv.org/abs/2311.02578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Gervers, Gelila Tilahun</li>
<li>for: 这个论文是为了解决历史文档的排序问题，具体来说是美国州OF THE UNIONAddresses和DEEDS古英文财产转让文档集。</li>
<li>methods: 这个论文使用了一种无监督的方法，通过计算word使用的慢变化来对文档进行排序。这种方法使用了Generalized Linear Models（Fan、Heckman、Wand，1995）的非 Parametric预测，并使用Simulated Annealing算法来解决 combinatorial优化问题。</li>
<li>results: 这个论文的结果表明，使用这种无监督方法可以有效地排序历史文档，并且比 randomly sequenced baseline要好。这种方法可以应用于无日期的文档集。<details>
<summary>Abstract</summary>
We outline an unsupervised method for temporal rank ordering of sets of historical documents, namely American State of the Union Addresses and DEEDS, a corpus of medieval English property transfer documents. Our method relies upon effectively capturing the gradual change in word usage via a bandwidth estimate for the non-parametric Generalized Linear Models (Fan, Heckman, and Wand, 1995). The number of possible rank orders needed to search through possible cost functions related to the bandwidth can be quite large, even for a small set of documents. We tackle this problem of combinatorial optimization using the Simulated Annealing algorithm, which allows us to obtain the optimal document temporal orders. Our rank ordering method significantly improved the temporal sequencing of both corpora compared to a randomly sequenced baseline. This unsupervised approach should enable the temporal ordering of undated document sets.
</details>
<details>
<summary>摘要</summary>
我们提出了一种无监督的方法来排序历史文档集合，包括美国国情咨文和中世纪英国财产转让文件集。我们的方法基于有效地捕捉文本中慢慢变化的词汇使用幅度，通过非参数化概线性模型（Fan、Heckman和Wand，1995）来估算带宽。寻找可能的排序方案的可能性数量可能会很大，即使是一个小型文档集合也可能需要很长时间。我们使用伪随机扰动算法来解决这个问题，从而获得最佳的文档排序。我们的排序方法在对两个文档集合进行排序时有显著改善，相比Random Baseline。这种无监督的方法应该能够应用于无日期文档集合的排序问题。
</details></li>
</ul>
<hr>
<h2 id="BanMANI-A-Dataset-to-Identify-Manipulated-Social-Media-News-in-Bangla"><a href="#BanMANI-A-Dataset-to-Identify-Manipulated-Social-Media-News-in-Bangla" class="headerlink" title="BanMANI: A Dataset to Identify Manipulated Social Media News in Bangla"></a>BanMANI: A Dataset to Identify Manipulated Social Media News in Bangla</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02570">http://arxiv.org/abs/2311.02570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahammed Kamruzzaman, Md. Minul Islam Shovon, Gene Louis Kim</li>
<li>for: 本研究旨在探讨社交媒体新闻中假新闻和误导性新闻的识别，特别是指将关键语句与参考文章进行误导性修改的现象。</li>
<li>methods: 我们采用了一种数据收集方法，使用了限制可用的NLP工具在孟加拉语中进行收集，并将收集到的数据标记为各种信息操纵。</li>
<li>results: 我们的分析发现，当用于零参数和微调设置时，现有的LLMs都面临着挑战，表明这是一个具有挑战性的任务。<details>
<summary>Abstract</summary>
Initial work has been done to address fake news detection and misrepresentation of news in the Bengali language. However, no work in Bengali yet addresses the identification of specific claims in social media news that falsely manipulates a related news article. At this point, this problem has been tackled in English and a few other languages, but not in the Bengali language. In this paper, we curate a dataset of social media content labeled with information manipulation relative to reference articles, called BanMANI. The dataset collection method we describe works around the limitations of the available NLP tools in Bangla. We expect these techniques will carry over to building similar datasets in other low-resource languages. BanMANI forms the basis both for evaluating the capabilities of existing NLP systems and for training or fine-tuning new models specifically on this task. In our analysis, we find that this task challenges current LLMs both under zero-shot and fine-tuned settings.
</details>
<details>
<summary>摘要</summary>
先期工作已经进行了假新闻检测和新闻歪曲的处理，但是没有任何工作在孟加拉语中处理特定新闻媒体中的假消息。在这篇论文中，我们精心准备了一个社交媒体内容标注了信息歪曲相关新闻的数据集，called BanMANI。我们的数据集采集方法考虑了当地的NLP工具的限制。我们预计这些技术将在其他低资源语言上进行推广。BanMANI将成为评估现有NLP系统的基础，以及特定任务上训练或细化新模型的基础。在我们的分析中，我们发现这个任务会挑战当前的LLMs，包括零shot和精度调整的情况。
</details></li>
</ul>
<hr>
<h2 id="Topic-model-based-on-co-occurrence-word-networks-for-unbalanced-short-text-datasets"><a href="#Topic-model-based-on-co-occurrence-word-networks-for-unbalanced-short-text-datasets" class="headerlink" title="Topic model based on co-occurrence word networks for unbalanced short text datasets"></a>Topic model based on co-occurrence word networks for unbalanced short text datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02566">http://arxiv.org/abs/2311.02566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjie Ma, Junping Du, Meiyu Liang, Zeli Guan</li>
<li>for: 本研究旨在探讨短文件中罕见话题的检测，以解决短文件中话题的稀缺性和不均匀性问题。</li>
<li>methods: 本研究提出了一种基于合occurrence字网络的话题模型，称为CWUTM，该模型利用字网络来捕捉每个字的话题分布，并通过调整节点活动和话题表示的方法，提高了检测罕见话题的敏感度。</li>
<li>results: 对于不同的短文件数据集，CWUTM模型在检测罕见话题的任务上表现出优于基eline方法，并且可以在早期准确地检测出emerging话题或意外事件。<details>
<summary>Abstract</summary>
We propose a straightforward solution for detecting scarce topics in unbalanced short-text datasets. Our approach, named CWUTM (Topic model based on co-occurrence word networks for unbalanced short text datasets), Our approach addresses the challenge of sparse and unbalanced short text topics by mitigating the effects of incidental word co-occurrence. This allows our model to prioritize the identification of scarce topics (Low-frequency topics). Unlike previous methods, CWUTM leverages co-occurrence word networks to capture the topic distribution of each word, and we enhanced the sensitivity in identifying scarce topics by redefining the calculation of node activity and normalizing the representation of both scarce and abundant topics to some extent. Moreover, CWUTM adopts Gibbs sampling, similar to LDA, making it easily adaptable to various application scenarios. Our extensive experimental validation on unbalanced short-text datasets demonstrates the superiority of CWUTM compared to baseline approaches in discovering scarce topics. According to the experimental results the proposed model is effective in early and accurate detection of emerging topics or unexpected events on social platforms.
</details>
<details>
<summary>摘要</summary>
我们提出了一种直接 Addressing the challenge of sparse and unbalanced short text topics, our approach, named CWUTM (based on co-occurrence word networks for unbalanced short text datasets), mitigates the effects of incidental word co-occurrence, allowing our model to prioritize the identification of scarce topics (low-frequency topics). Unlike previous methods, CWUTM leverages co-occurrence word networks to capture the topic distribution of each word, and we enhance the sensitivity in identifying scarce topics by redefining the calculation of node activity and normalizing the representation of both scarce and abundant topics to some extent. Moreover, CWUTM adopts Gibbs sampling, similar to LDA, making it easily adaptable to various application scenarios. Our extensive experimental validation on unbalanced short-text datasets demonstrates the superiority of CWUTM compared to baseline approaches in discovering scarce topics. According to the experimental results, the proposed model is effective in early and accurate detection of emerging topics or unexpected events on social platforms.Here's the word-for-word translation:我们提出了一种直接解决稀缺和不均衡短文数据集中的挑战的方法，我们称之为CWUTM（基于相互occurrence word networks for unbalanced short text datasets）。我们的方法可以减轻因为意外的word co-occurrence的影响，使我们的模型更加偏好发现稀缺话题（low-frequency topics）。与前一代方法不同，CWUTM利用相互occurrence word networks来捕捉每个word的话题分布，并通过重新定义节点活动计算和normalize话题表示来提高发现稀缺话题的敏感度。此外，CWUTM采用Gibbs sampling，与LDA类似，使其容易适应不同应用场景。我们对稀缺短文数据集进行了广泛的实验验证，结果表明CWUTM在发现稀缺话题方面超过了基eline方法的表现。据实验结果，提议的模型能够在社交平台上早期准确地探测出emerging topics或意外事件。
</details></li>
</ul>
<hr>
<h2 id="Relation-Extraction-Model-Based-on-Semantic-Enhancement-Mechanism"><a href="#Relation-Extraction-Model-Based-on-Semantic-Enhancement-Mechanism" class="headerlink" title="Relation Extraction Model Based on Semantic Enhancement Mechanism"></a>Relation Extraction Model Based on Semantic Enhancement Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02564">http://arxiv.org/abs/2311.02564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyu Liu, Junping Du, Yingxia Shao, Zeli Guan</li>
<li>for: 解决 triple overlap 问题</li>
<li>methods: 基于 CasRel 框架和 semantic enhancement mechanism</li>
<li>results: 提高 relation extraction 效果， луч于处理 triple overlap 问题<details>
<summary>Abstract</summary>
Relational extraction is one of the basic tasks related to information extraction in the field of natural language processing, and is an important link and core task in the fields of information extraction, natural language understanding, and information retrieval. None of the existing relation extraction methods can effectively solve the problem of triple overlap. The CasAug model proposed in this paper based on the CasRel framework combined with the semantic enhancement mechanism can solve this problem to a certain extent. The CasAug model enhances the semantics of the identified possible subjects by adding a semantic enhancement mechanism, First, based on the semantic coding of possible subjects, pre-classify the possible subjects, and then combine the subject lexicon to calculate the semantic similarity to obtain the similar vocabulary of possible subjects. According to the similar vocabulary obtained, each word in different relations is calculated through the attention mechanism. For the contribution of the possible subject, finally combine the relationship pre-classification results to weight the enhanced semantics of each relationship to find the enhanced semantics of the possible subject, and send the enhanced semantics combined with the possible subject to the object and relationship extraction module. Complete the final relation triplet extraction. The experimental results show that, compared with the baseline model, the CasAug model proposed in this paper has improved the effect of relation extraction, and CasAug's ability to deal with overlapping problems and extract multiple relations is also better than the baseline model, indicating that the semantic enhancement mechanism proposed in this paper It can further reduce the judgment of redundant relations and alleviate the problem of triple overlap.
</details>
<details>
<summary>摘要</summary>
关系提取是信息提取领域的基本任务之一，是信息提取、自然语言理解和信息检索领域的重要链接和核心任务。现有的关系提取方法无法有效解决三重重叠问题。这篇论文提出的CasAug模型基于CasRel框架和semantic enhancement机制，可以在一定程度上解决这个问题。CasAug模型对可能的主题进行semantic enhancement，首先根据可能的主题的semantic coding进行预类别，然后将主题词典与计算semantic similarity来获取相似词语的集合。根据这些相似词语集合，使用注意机制计算每个关系中的每个词语的权重。最后，将预类别结果与权重结果组合，以获取增强的主题 semantics。将增强的主题 semantics与可能的主题一起传递给对象和关系提取模块，以完成最终的关系三元组提取。实验结果显示，相比基eline模型，本文提出的CasAug模型在关系提取效果上有所提高，而且CasAug能够更好地处理重叠问题，提取多个关系，表明semantic enhancement机制可以进一步减少纬度重复的判断和三重重叠问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/05/cs.CL_2023_11_05/" data-id="cloojsme300dxre882mqn9qgz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/05/cs.LG_2023_11_05/" class="article-date">
  <time datetime="2023-11-05T10:00:00.000Z" itemprop="datePublished">2023-11-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/05/cs.LG_2023_11_05/">cs.LG - 2023-11-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="From-molecules-to-scaffolds-to-functional-groups-building-context-dependent-molecular-representation-via-multi-channel-learning"><a href="#From-molecules-to-scaffolds-to-functional-groups-building-context-dependent-molecular-representation-via-multi-channel-learning" class="headerlink" title="From molecules to scaffolds to functional groups: building context-dependent molecular representation via multi-channel learning"></a>From molecules to scaffolds to functional groups: building context-dependent molecular representation via multi-channel learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02798">http://arxiv.org/abs/2311.02798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Wan, Jialu Wu, Tingjun Hou, Chang-Yu Hsieh, Xiaowei Jia</li>
<li>for: 本研究旨在开发一种基于自我监督学习的分子机器学习模型，以便在药物发现等科学应用中进行可靠的分子性质预测。</li>
<li>methods: 本研究使用了一种新的学习框架，具体来说是通过分子结构层次结构知识、独立预训练任务和目标任务特定的通道选择来嵌入分子空间中的知识。</li>
<li>results: 本研究的结果表明，该学习框架可以在多种分子性质评价 benchmark 上达到竞争性的性能，并在特殊而普遍存在的活性峰架上具有更高的稳定性和普适性。<details>
<summary>Abstract</summary>
Reliable molecular property prediction is essential for various scientific endeavors and industrial applications, such as drug discovery. However, the scarcity of data, combined with the highly non-linear causal relationships between physicochemical and biological properties and conventional molecular featurization schemes, complicates the development of robust molecular machine learning models. Self-supervised learning (SSL) has emerged as a popular solution, utilizing large-scale, unannotated molecular data to learn a foundational representation of chemical space that might be advantageous for downstream tasks. Yet, existing molecular SSL methods largely overlook domain-specific knowledge, such as molecular similarity and scaffold importance, as well as the context of the target application when operating over the large chemical space. This paper introduces a novel learning framework that leverages the knowledge of structural hierarchies within molecular structures, embeds them through separate pre-training tasks over distinct channels, and employs a task-specific channel selection to compose a context-dependent representation. Our approach demonstrates competitive performance across various molecular property benchmarks and establishes some state-of-the-art results. It further offers unprecedented advantages in particularly challenging yet ubiquitous scenarios like activity cliffs with enhanced robustness and generalizability compared to other baselines.
</details>
<details>
<summary>摘要</summary>
可靠的分子性质预测是科学研究和工业应用的重要基础，如药物发现。然而，数据缺乏和物理化和生物性质之间的非线性 causal 关系，使得传统的分子特征化方案难以生成可靠的分子机器学习模型。自主学习（SSL）已成为一种流行的解决方案，使用大规模、无注释的分子数据来学习分子空间的基础表示，可能有利于下游任务。然而，现有的分子SSL方法很多忽略分子相似性和架构重要性，以及目标应用场景的Context。这篇论文介绍了一种新的学习框架，利用分子结构中的结构层次，通过不同的预训练任务来嵌入这些结构，并使用任务特定的通道选择来组合上下文依赖的表示。我们的方法在多种分子性质指标上达到了竞争性的表现，并在特定但普遍的enario中提供了前所未有的优势，如活性峰的增强鲁棒性和通用性。
</details></li>
</ul>
<hr>
<h2 id="Riemannian-Laplace-Approximation-with-the-Fisher-Metric"><a href="#Riemannian-Laplace-Approximation-with-the-Fisher-Metric" class="headerlink" title="Riemannian Laplace Approximation with the Fisher Metric"></a>Riemannian Laplace Approximation with the Fisher Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02766">http://arxiv.org/abs/2311.02766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Mark Girolami, Arto Klami</li>
<li>for: 用于bayesian inference中的快速和精确approximation</li>
<li>methods: 使用laplace approximation方法，并通过选择合适的里曼几何来提高approximation的精度</li>
<li>results: 提供了两种新的variant，可以在各种实验中提供更好的approximation，并且在有限数据情况下减少了偏差和过度窄approximation的问题<details>
<summary>Abstract</summary>
The Laplace's method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties heavily depend on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the method, and demonstrating practical improvements in a range of experiments.
</details>
<details>
<summary>摘要</summary>
拉普拉斯方法可以用 Gaussian 分布来近似目标概率密度。它在 bayesian 推断中是计算高效且 asymptotically exact 的，但是对于复杂的目标和 finite-data posterior 来说，它通常是太粗糙的。一种 latest 的 Laplace Approximation 扩展可以根据选择的 Riemannian geometry 来修改 Gaussian approximation，提供更加丰富的近似家族，并仍保持计算效率。然而，在选择的 metric 的情况下，这种方法的性质受到严重的限制， previous 的 metric 导致的近似偏差和偏见甚至在 infinite 数据的情况下也存在。我们在这篇文章中解决这个缺陷，开发出两种变体，其中一种是 infinite 数据的正确的，另一种是对于不同的 metric 进行了扩展的分析，并在一系列实验中展示了实践上的改进。
</details></li>
</ul>
<hr>
<h2 id="Log-Concavity-of-Multinomial-Likelihood-Functions-Under-Interval-Censoring-Constraints-on-Frequencies-or-Their-Partial-Sums"><a href="#Log-Concavity-of-Multinomial-Likelihood-Functions-Under-Interval-Censoring-Constraints-on-Frequencies-or-Their-Partial-Sums" class="headerlink" title="Log-Concavity of Multinomial Likelihood Functions Under Interval Censoring Constraints on Frequencies or Their Partial Sums"></a>Log-Concavity of Multinomial Likelihood Functions Under Interval Censoring Constraints on Frequencies or Their Partial Sums</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02763">http://arxiv.org/abs/2311.02763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bruce Levin, Erik Learned-Miller</li>
<li>for: 这篇论文是为了解释多项式观察数据下的interval隐藏约束的likelihood函数是完全log凹的。</li>
<li>methods: 这篇论文使用了证明constrained sample spaces是M-convex的 discrete simplex中的子集来证明likelihood函数的完全log凹性。</li>
<li>results: 这篇论文得到了likelihood函数在多项式观察数据下的interval隐藏约束下是completely log-concave的结论。<details>
<summary>Abstract</summary>
We show that the likelihood function for a multinomial vector observed under arbitrary interval censoring constraints on the frequencies or their partial sums is completely log-concave by proving that the constrained sample spaces comprise M-convex subsets of the discrete simplex.
</details>
<details>
<summary>摘要</summary>
我们证明了一个多omial вектор在任意间隔缩放约束下观察到的概率函数是完全log-Concave，通过证明受约束的样本空间是M-convex的集合。Here's a breakdown of the translation:* "We show" is translated as "我们证明" (wǒmen shèngyì)* "likelihood function" is translated as "概率函数" (gòu yù fungs)* "for a multinomial vector" is translated as "一个多omial вектор" (yī gè múltiōm mèng)* "observed under arbitrary interval censoring constraints" is translated as "在任意间隔缩放约束下观察" (zài ràng yì jì jiān zhòng yòu xiǎng)* "on the frequencies or their partial sums" is translated as "在频率或其部分和" (zhì yì yǔ qí bù fāng hé)* "is completely log-concave" is translated as "是完全log-Concave" (shì wán zhì yì)* "by proving that the constrained sample spaces comprise M-convex subsets of the discrete simplex" is translated as "通过证明受约束的样本空间是M-convex的集合" (tông qiào yì zhèng yì zhōng xiàng)I hope this helps! Let me know if you have any other questions.
</details></li>
</ul>
<hr>
<h2 id="One-Shot-Strategic-Classification-Under-Unknown-Costs"><a href="#One-Shot-Strategic-Classification-Under-Unknown-Costs" class="headerlink" title="One-Shot Strategic Classification Under Unknown Costs"></a>One-Shot Strategic Classification Under Unknown Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02761">http://arxiv.org/abs/2311.02761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elan Rosenfeld, Nir Rosenfeld</li>
<li>for: 本研究旨在学习具有抗压缩性的决策规则，以适应不确定的用户回应。</li>
<li>methods: 本文使用了一些新的方法，包括将用户成本函数作为不确定性的来源，并使用最小最大问题来解决一shot任务。</li>
<li>results: 本文提供了一些有效的算法，可以在一shot任务中快速地获得最佳的决策规则，并且证明了这些算法在不同的设置下是有效的。<details>
<summary>Abstract</summary>
A primary goal in strategic classification is to learn decision rules which are robust to strategic input manipulation. Earlier works assume that strategic responses are known; while some recent works address the important challenge of unknown responses, they exclusively study sequential settings which allow multiple model deployments over time. But there are many domains$\unicode{x2014}$particularly in public policy, a common motivating use-case$\unicode{x2014}$where multiple deployments are unrealistic, or where even a single bad round is undesirable. To address this gap, we initiate the study of strategic classification under unknown responses in the one-shot setting, which requires committing to a single classifier once. Focusing on the users' cost function as the source of uncertainty, we begin by proving that for a broad class of costs, even a small mis-estimation of the true cost can entail arbitrarily low accuracy in the worst case. In light of this, we frame the one-shot task as a minimax problem, with the goal of identifying the classifier with the smallest worst-case risk over an uncertainty set of possible costs. Our main contribution is efficient algorithms for both the full-batch and stochastic settings, which we prove converge (offline) to the minimax optimal solution at the dimension-independent rate of $\tilde{\mathcal{O}(T^{-\frac{1}{2})$. Our analysis reveals important structure stemming from the strategic nature of user responses, particularly the importance of dual norm regularization with respect to the cost function.
</details>
<details>
<summary>摘要</summary>
主要目标在策略分类中是学习对输入操作的Robust策略决策规则。earlier works假设战略回应是已知的；而一些最近的工作Addressing the important challenge of unknown responses, but they only study sequential settings, which allow multiple model deployments over time. However, there are many domains, particularly in public policy, a common motivating use case, where multiple deployments are unrealistic, or where even a single bad round is undesirable. To address this gap, we initiate the study of strategic classification under unknown responses in the one-shot setting, which requires committing to a single classifier once. Focusing on the users' cost function as the source of uncertainty, we begin by proving that for a broad class of costs, even a small misestimation of the true cost can lead to arbitrarily low accuracy in the worst case. In light of this, we frame the one-shot task as a minimax problem, with the goal of identifying the classifier with the smallest worst-case risk over an uncertainty set of possible costs. Our main contribution is efficient algorithms for both the full-batch and stochastic settings, which we prove converge (offline) to the minimax optimal solution at the dimension-independent rate of $\tilde{\mathcal{O}(T^{-\frac{1}{2})$. Our analysis reveals important structure stemming from the strategic nature of user responses, particularly the importance of dual norm regularization with respect to the cost function.
</details></li>
</ul>
<hr>
<h2 id="ELEGANT-Certified-Defense-on-the-Fairness-of-Graph-Neural-Networks"><a href="#ELEGANT-Certified-Defense-on-the-Fairness-of-Graph-Neural-Networks" class="headerlink" title="ELEGANT: Certified Defense on the Fairness of Graph Neural Networks"></a>ELEGANT: Certified Defense on the Fairness of Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02757">http://arxiv.org/abs/2311.02757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yushundong/ELEGANT">https://github.com/yushundong/ELEGANT</a></li>
<li>paper_authors: Yushun Dong, Binchi Zhang, Hanghang Tong, Jundong Li</li>
<li>for: 本研究旨在针对Graph Neural Networks (GNNs) 中的不公正性问题进行研究，提出了一个名为 ELEGANT 的原理性框架，以确保 GNNs 的预测结果具有公正性。</li>
<li>methods: 本研究使用了 GNNs 作为背景，提出了一个名为 ELEGANT 的原理性框架，并进行了详细的理论认证分析，以确保 GNNs 的公正性。ELEGANT 不假设 GNNs 的结构或参数，并且不需要重新训练 GNNs 来实现认证。</li>
<li>results: 实验结果显示，ELEGANT 能够实现 GNNs 的公正性认证，并且在实际应用中具有优秀的效果。此外，ELEGANT 还能够帮助 GNNs 的偏见调整。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have emerged as a prominent graph learning model in various graph-based tasks over the years. Nevertheless, due to the vulnerabilities of GNNs, it has been empirically proved that malicious attackers could easily corrupt the fairness level of their predictions by adding perturbations to the input graph data. In this paper, we take crucial steps to study a novel problem of certifiable defense on the fairness level of GNNs. Specifically, we propose a principled framework named ELEGANT and present a detailed theoretical certification analysis for the fairness of GNNs. ELEGANT takes any GNNs as its backbone, and the fairness level of such a backbone is theoretically impossible to be corrupted under certain perturbation budgets for attackers. Notably, ELEGANT does not have any assumption over the GNN structure or parameters, and does not require re-training the GNNs to realize certification. Hence it can serve as a plug-and-play framework for any optimized GNNs ready to be deployed. We verify the satisfactory effectiveness of ELEGANT in practice through extensive experiments on real-world datasets across different backbones of GNNs, where ELEGANT is also demonstrated to be beneficial for GNN debiasing. Open-source code can be found at https://github.com/yushundong/ELEGANT.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks (GNNs) 在各种基于图的任务中 emerged as a prominent graph learning model over the years. However, due to the vulnerabilities of GNNs, it has been empirically proven that malicious attackers could easily corrupt the fairness level of their predictions by adding perturbations to the input graph data. In this paper, we take crucial steps to study a novel problem of certifiable defense on the fairness level of GNNs. Specifically, we propose a principled framework named ELEGANT and present a detailed theoretical certification analysis for the fairness of GNNs. ELEGANT takes any GNNs as its backbone, and the fairness level of such a backbone is theoretically impossible to be corrupted under certain perturbation budgets for attackers. Notably, ELEGANT does not have any assumption over the GNN structure or parameters, and does not require re-training the GNNs to realize certification. Hence it can serve as a plug-and-play framework for any optimized GNNs ready to be deployed. We verify the satisfactory effectiveness of ELEGANT in practice through extensive experiments on real-world datasets across different backbones of GNNs, where ELEGANT is also demonstrated to be beneficial for GNN debiasing. Open-source code can be found at https://github.com/yushundong/ELEGANT.
</details></li>
</ul>
<hr>
<h2 id="Staged-Reinforcement-Learning-for-Complex-Tasks-through-Decomposed-Environments"><a href="#Staged-Reinforcement-Learning-for-Complex-Tasks-through-Decomposed-Environments" class="headerlink" title="Staged Reinforcement Learning for Complex Tasks through Decomposed Environments"></a>Staged Reinforcement Learning for Complex Tasks through Decomposed Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02746">http://arxiv.org/abs/2311.02746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Pina, Corentin Artaud, Xiaolan Liu, Varuna De Silva</li>
<li>for: 本研究旨在提高智能控制领域中RL的应用，特别是在交通交叉口上。</li>
<li>methods: 本文提出了两种方法来将RL问题近似到实际问题，包括划分复杂任务为多个子任务，以及基于CTDE paradigm的训练结构 mechanism。</li>
<li>results: 研究结果表明，提出的方法可以提高交通交叉口中智能代理的性能，避免 possiblesafety-critical problem。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) is an area of growing interest in the field of artificial intelligence due to its many notable applications in diverse fields. Particularly within the context of intelligent vehicle control, RL has made impressive progress. However, currently it is still in simulated controlled environments where RL can achieve its full super-human potential. Although how to apply simulation experience in real scenarios has been studied, how to approximate simulated problems to the real dynamic problems is still a challenge. In this paper, we discuss two methods that approximate RL problems to real problems. In the context of traffic junction simulations, we demonstrate that, if we can decompose a complex task into multiple sub-tasks, solving these tasks first can be advantageous to help minimising possible occurrences of catastrophic events in the complex task. From a multi-agent perspective, we introduce a training structuring mechanism that exploits the use of experience learned under the popular paradigm called Centralised Training Decentralised Execution (CTDE). This experience can then be leveraged in fully decentralised settings that are conceptually closer to real settings, where agents often do not have access to a central oracle and must be treated as isolated independent units. The results show that the proposed approaches improve agents performance in complex tasks related to traffic junctions, minimising potential safety-critical problems that might happen in these scenarios. Although still in simulation, the investigated situations are conceptually closer to real scenarios and thus, with these results, we intend to motivate further research in the subject.
</details>
<details>
<summary>摘要</summary>
《强化学习（RL）是人工智能领域中不断吸引关注的领域，尤其在智能控制领域中有很多应用。然而，RL在真实环境中仍然处于模拟控制环境中，尚未实现其全面超人类能力。虽然如何将模拟问题应用到实际动态问题已经被研究，但如何近似模拟问题与实际问题的关系仍然是一大挑战。本文提出了两种方法来近似RL问题与实际问题。在交通交叉点 simulations中，我们示出了如果将复杂任务 decomposed into多个子任务，解决这些子任务可以帮助避免在复杂任务中可能发生的致命事件。从多代理视角来看，我们引入了一种基于Centralised Training Decentralised Execution（CTDE） paradigm的训练结构机制，该机制可以利用在模拟环境中学习的经验，并在完全分布式设置中使用。这些设置更加接近实际情况， где agents 通常没有中央抽象 oracle 的访问权限，并且需要 treated as isolated independent units。结果表明，提出的方法可以提高在交通交叉点相关任务中的代理性能，最小化 potential safety-critical problems 在这些场景中发生的可能性。虽然仍然在模拟环境中，investigated 情况更加接近实际情况，我们希望通过这些结果来鼓励进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Correlated-Auxiliary-Feedback-in-Parameterized-Bandits"><a href="#Exploiting-Correlated-Auxiliary-Feedback-in-Parameterized-Bandits" class="headerlink" title="Exploiting Correlated Auxiliary Feedback in Parameterized Bandits"></a>Exploiting Correlated Auxiliary Feedback in Parameterized Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02715">http://arxiv.org/abs/2311.02715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arun Verma, Zhongxiang Dai, Yao Shu, Bryan Kian Hsiang Low</li>
<li>for: 本研究考虑了一种新的参数化弦楽器问题变体，在这种变体中，学习者可以观察额外反馈，这些额外反馈与观察到的奖励相关。</li>
<li>methods: 我们首先开发了一种利用额外反馈建立奖励估计器，并提供了准确的信息 bound，从而减少了 regret。</li>
<li>results: 我们通过对不同设置的实验结果表明，我们的提议方法可以减少 regret，并且可以在不同的奖励和额外反馈相关性水平下实现最佳性。<details>
<summary>Abstract</summary>
We study a novel variant of the parameterized bandits problem in which the learner can observe additional auxiliary feedback that is correlated with the observed reward. The auxiliary feedback is readily available in many real-life applications, e.g., an online platform that wants to recommend the best-rated services to its users can observe the user's rating of service (rewards) and collect additional information like service delivery time (auxiliary feedback). In this paper, we first develop a method that exploits auxiliary feedback to build a reward estimator with tight confidence bounds, leading to a smaller regret. We then characterize the regret reduction in terms of the correlation coefficient between reward and its auxiliary feedback. Experimental results in different settings also verify the performance gain achieved by our proposed method.
</details>
<details>
<summary>摘要</summary>
我们研究一种新的参数化弦折冲问题变体，在这种问题中，学习者可以观察额外的协助反馈，这些协助反馈与观察到的奖励相关。这些协助反馈在实际应用中很普遍，例如一个在线平台可以观察用户对服务的评分（奖励），同时收集服务交付时间（协助反馈）的信息。在这篇论文中，我们首先开发了一种利用协助反馈建立奖励估计器，并提供紧张的信任范围，从而减少了偏差。然后，我们将偏差减少的程度与奖励和其协助反馈之间的相关性系数进行描述。实际Results in different settings also verify the performance gain achieved by our proposed method.
</details></li>
</ul>
<hr>
<h2 id="A-Goal-Driven-Approach-to-Systems-Neuroscience"><a href="#A-Goal-Driven-Approach-to-Systems-Neuroscience" class="headerlink" title="A Goal-Driven Approach to Systems Neuroscience"></a>A Goal-Driven Approach to Systems Neuroscience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02704">http://arxiv.org/abs/2311.02704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aran Nayebi</li>
<li>for: 这个论文旨在解释如何从大量神经元记录数据中提取有用的信息，以便更好地理解神经元网络如何生成智能行为。</li>
<li>methods: 这个论文使用了新的方法来解释神经元网络如何处理信息，包括使用权重学习和聚类分析等方法。</li>
<li>results: 研究人员通过使用这些新方法，成功地从大量神经元记录数据中提取了有用的信息，并实现了解释神经元网络如何生成智能行为的目标。<details>
<summary>Abstract</summary>
Humans and animals exhibit a range of interesting behaviors in dynamic environments, and it is unclear how our brains actively reformat this dense sensory information to enable these behaviors. Experimental neuroscience is undergoing a revolution in its ability to record and manipulate hundreds to thousands of neurons while an animal is performing a complex behavior. As these paradigms enable unprecedented access to the brain, a natural question that arises is how to distill these data into interpretable insights about how neural circuits give rise to intelligent behaviors. The classical approach in systems neuroscience has been to ascribe well-defined operations to individual neurons and provide a description of how these operations combine to produce a circuit-level theory of neural computations. While this approach has had some success for small-scale recordings with simple stimuli, designed to probe a particular circuit computation, often times these ultimately lead to disparate descriptions of the same system across stimuli. Perhaps more strikingly, many response profiles of neurons are difficult to succinctly describe in words, suggesting that new approaches are needed in light of these experimental observations. In this thesis, we offer a different definition of interpretability that we show has promise in yielding unified structural and functional models of neural circuits, and describes the evolutionary constraints that give rise to the response properties of the neural population, including those that have previously been difficult to describe individually. We demonstrate the utility of this framework across multiple brain areas and species to study the roles of recurrent processing in the primate ventral visual pathway; mouse visual processing; heterogeneity in rodent medial entorhinal cortex; and facilitating biological learning.
</details>
<details>
<summary>摘要</summary>
人类和动物在动态环境中展现出一系列有趣的行为，但是不清楚如何我们的脑活动地重新格式化这些紧凑的感知信息以实现这些行为。现代神经科学在实验方面正在进行一场革命，可以同时记录和 manipulate hundreds to thousands of neurons  während动物执行复杂的行为。随着这些方法得到了无 precedent 的访问到脑部，一个自然的问题是如何将这些数据转化成可解释的洞察。传统的系统神经科学方法是将各个神经元的操作归因为具体的功能，然后将这些操作组合起来描述神经网络的计算。虽然这种方法在小规模记录中有一定的成功，但是这些经常导致不同的描述，尤其是当面对复杂的刺激时。此外，许多神经元的响应特征难以用语言描述，表明需要新的方法。在这个论文中，我们提出了一种不同的可解释性定义，并证明这种定义在提供简单结构和功能模型方面具有推动力。我们在多个脑区和种类中使用这种方法，研究了Primates ventral visual pathway; mouse visual processing; rodent medial entorhinal cortex 中神经元种类的多样性和动物学习。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Linearly-Mixed-Causal-Representations-from-Multi-Node-Interventions"><a href="#Identifying-Linearly-Mixed-Causal-Representations-from-Multi-Node-Interventions" class="headerlink" title="Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions"></a>Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02695">http://arxiv.org/abs/2311.02695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Bing, Urmi Ninad, Jonas Wahl, Jakob Runge</li>
<li>for: This paper focuses on the problem of inferring high-level causal variables from low-level observations, and it provides a new approach to addressing this problem by relaxing the assumption that only one variable can be intervened upon in each environment.</li>
<li>methods: The paper proposes a new method for causal representation learning that exploits the trace of interventions on the variance of ground truth causal variables, and it regularizes for a specific notion of sparsity with respect to this trace.</li>
<li>results: The paper presents empirical evidence that validates the identifiability results of the proposed approach, and it demonstrates the effectiveness of the approach in learning causal representations from multi-node interventional data.<details>
<summary>Abstract</summary>
The task of inferring high-level causal variables from low-level observations, commonly referred to as causal representation learning, is fundamentally underconstrained. As such, recent works to address this problem focus on various assumptions that lead to identifiability of the underlying latent causal variables. A large corpus of these preceding approaches consider multi-environment data collected under different interventions on the causal model. What is common to virtually all of these works is the restrictive assumption that in each environment, only a single variable is intervened on. In this work, we relax this assumption and provide the first identifiability result for causal representation learning that allows for multiple variables to be targeted by an intervention within one environment. Our approach hinges on a general assumption on the coverage and diversity of interventions across environments, which also includes the shared assumption of single-node interventions of previous works. The main idea behind our approach is to exploit the trace that interventions leave on the variance of the ground truth causal variables and regularizing for a specific notion of sparsity with respect to this trace. In addition to and inspired by our theoretical contributions, we present a practical algorithm to learn causal representations from multi-node interventional data and provide empirical evidence that validates our identifiability results.
</details>
<details>
<summary>摘要</summary>
文中提到的高级 causal 变量推理问题，通常称为 causal 表示学习，是基本不可靠定。因此， recient works 做出了多种假设，以便确定下意图的 latent causal 变量。大多数先前的方法假设了每个环境中只有一个变量被 intervened 。在这种情况下，我们在这篇文章中放弃了这一假设，并提供了首个允许多变量在一个环境中被 intervened 的 causal 表示学习的可定性结果。我们的方法基于一个通用的假设，即环境中的 intervention 覆盖和多样性，包括先前作品中的单个变量 intervened 假设。我们的主要想法是利用 intervention 对真实 causal 变量的跟踪，并对这些跟踪的特定缺失进行规范化。此外，我们还提供了一种实用的算法，用于从 multi-node interventional 数据中学习 causal 表示，并提供了实验证据，证明了我们的可定性结果。
</details></li>
</ul>
<hr>
<h2 id="Regret-Analysis-of-Learning-Based-Linear-Quadratic-Gaussian-Control-with-Additive-Exploration"><a href="#Regret-Analysis-of-Learning-Based-Linear-Quadratic-Gaussian-Control-with-Additive-Exploration" class="headerlink" title="Regret Analysis of Learning-Based Linear Quadratic Gaussian Control with Additive Exploration"></a>Regret Analysis of Learning-Based Linear Quadratic Gaussian Control with Additive Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02679">http://arxiv.org/abs/2311.02679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Archith Athrey, Othmane Mazhar, Meichen Guo, Bart De Schutter, Shengling Shi</li>
<li>for: 这个论文研究了一种 computationally efficient exploration strategy，即naive exploration，在Linear Quadratic Gaussian (LQG)框架下控制未知部分可观测系统中的 regret。</li>
<li>methods: 作者提出了一种两相控制算法，即LQG-NAIVE，它包括一个初始阶段将 Gaussian 输入信号注入系统以获得系统模型，然后在一个 episodic 的方式下进行naive exploration和控制。</li>
<li>results: 作者证明了LQG-NAIVE 可以在 $T$ 步时间内实现 regret 增长率为 $\tilde{\mathcal{O}(\sqrt{T})$，即 $\mathcal{O}(\sqrt{T})$ 以上下标Logarithmic 因子。此外，作者还提出了一种扩展exploration signal到 ‘closed-loop’ 设置的LQG-IF2E，并通过数值实验证明了它的竞争性性。<details>
<summary>Abstract</summary>
In this paper, we analyze the regret incurred by a computationally efficient exploration strategy, known as naive exploration, for controlling unknown partially observable systems within the Linear Quadratic Gaussian (LQG) framework. We introduce a two-phase control algorithm called LQG-NAIVE, which involves an initial phase of injecting Gaussian input signals to obtain a system model, followed by a second phase of an interplay between naive exploration and control in an episodic fashion. We show that LQG-NAIVE achieves a regret growth rate of $\tilde{\mathcal{O}(\sqrt{T})$, i.e., $\mathcal{O}(\sqrt{T})$ up to logarithmic factors after $T$ time steps, and we validate its performance through numerical simulations. Additionally, we propose LQG-IF2E, which extends the exploration signal to a `closed-loop' setting by incorporating the Fisher Information Matrix (FIM). We provide compelling numerical evidence of the competitive performance of LQG-IF2E compared to LQG-NAIVE.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们分析了 computationally efficient exploration strategy，即naive exploration，对于 unknown partially observable systems在线性quadratic Gaussian (LQG) 框架中的 regret。我们提出了一种两个阶段控制算法，称为LQG-NAIVE，它包括一个初始阶段在注入 Gaussian 输入信号以获取系统模型，然后是一个第二阶段的一种 episodic 的naive exploration和控制的交互。我们证明了LQG-NAIVE 的 regret增长率为 $\tilde{\mathcal{O}(\sqrt{T})$，即 $\mathcal{O}(\sqrt{T})$  после $T$ 步，并通过数学仿真来验证其性能。此外，我们还提出了LQG-IF2E，它在探索信号中添加了 Fisher Information Matrix (FIM)，并提供了对LQG-NAIVE的竞争性性能的数学证明。
</details></li>
</ul>
<hr>
<h2 id="Drone-Enabled-Load-Management-for-Solar-Small-Cell-Networks-in-Next-Gen-Communications-Optimization-for-Solar-Small-Cells"><a href="#Drone-Enabled-Load-Management-for-Solar-Small-Cell-Networks-in-Next-Gen-Communications-Optimization-for-Solar-Small-Cells" class="headerlink" title="Drone-Enabled Load Management for Solar Small Cell Networks in Next-Gen Communications Optimization for Solar Small Cells"></a>Drone-Enabled Load Management for Solar Small Cell Networks in Next-Gen Communications Optimization for Solar Small Cells</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02648">http://arxiv.org/abs/2311.02648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daksh Dave, Dhruv Khut, Sahil Nawale, Pushkar Aggrawal, Disha Rastogi, Kailas Devadkar</li>
<li>for: 该研究旨在提高5G及以后cellular网络的能源效率和可靠性，通过使用无人机携带的空中基站（BS）实现稳定和安全的能源重新分配。</li>
<li>methods: 该研究提出了一种创新的负荷传输方法，通过将空中BS从高到低能电络中转移，根据用户密度和空中BS的可用性进行能源分配优化。</li>
<li>results: 该研究表明，提议的算法可以降低BS的发电停机频率，并且只需最小化的无人机交换数量。同时，实验结果表明，该方法可以提高cellular网络的能源效率和可靠性。<details>
<summary>Abstract</summary>
In recent years, the cellular industry has witnessed a major evolution in communication technologies. It is evident that the Next Generation of cellular networks(NGN) will play a pivotal role in the acceptance of emerging IoT applications supporting high data rates, better Quality of Service(QoS), and reduced latency. However, the deployment of NGN will introduce a power overhead on the communication infrastructure. Addressing the critical energy constraints in 5G and beyond, this study introduces an innovative load transfer method using drone-carried airborne base stations (BSs) for stable and secure power reallocation within a green micro-grid network. This method effectively manages energy deficit by transferring aerial BSs from high to low-energy cells, depending on user density and the availability of aerial BSs, optimizing power distribution in advanced cellular networks. The complexity of the proposed system is significantly lower as compared to existing power cable transmission systems currently employed in powering the BSs. Furthermore, our proposed algorithm has been shown to reduce BS power outages while requiring a minimum number of drone exchanges. We have conducted a thorough review on real-world dataset to prove the efficacy of our proposed approach to support BS during high load demand times
</details>
<details>
<summary>摘要</summary>
This method effectively manages energy deficits by transferring aerial BSs from high to low-energy cells, depending on user density and the availability of aerial BSs, optimizing power distribution in advanced cellular networks. The proposed system is significantly simpler than existing power cable transmission systems currently employed in powering BSs. Additionally, our proposed algorithm has been shown to reduce BS power outages while requiring a minimum number of drone exchanges. We have conducted a thorough review of real-world data to prove the efficacy of our proposed approach in supporting BSs during high load demand times.
</details></li>
</ul>
<hr>
<h2 id="Pointer-Networks-with-Q-Learning-for-OP-Combinatorial-Optimization"><a href="#Pointer-Networks-with-Q-Learning-for-OP-Combinatorial-Optimization" class="headerlink" title="Pointer Networks with Q-Learning for OP Combinatorial Optimization"></a>Pointer Networks with Q-Learning for OP Combinatorial Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02629">http://arxiv.org/abs/2311.02629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Barro</li>
<li>for: 解决Orienteering Problem（OP）中的 combinatorial optimization 问题，它在物流、交通规划等领域广泛应用。</li>
<li>methods:  combining Pointer Networks（Ptr-Nets）和Q-learning，提出Pointer Q-Network（PQN），以解决OP 中的特定挑战。</li>
<li>results: PQN 能够有效地管理 OP 的 Situations，并且可以提高 solving  efficiency。<details>
<summary>Abstract</summary>
The Orienteering Problem (OP) presents a unique challenge in combinatorial optimization, emphasized by its widespread use in logistics, delivery, and transportation planning. Given the NP-hard nature of OP, obtaining optimal solutions is inherently complex. While Pointer Networks (Ptr-Nets) have exhibited prowess in various combinatorial tasks, their performance in the context of OP leaves room for improvement. Recognizing the potency of Q-learning, especially when paired with deep neural structures, this research unveils the Pointer Q-Network (PQN). This innovative method combines Ptr-Nets and Q-learning, effectively addressing the specific challenges presented by OP. We deeply explore the architecture and efficiency of PQN, showcasing its superior capability in managing OP situations.
</details>
<details>
<summary>摘要</summary>
Orienteering Problem (OP) 提供了一种独特的 combinatorial optimization 挑战，它在物流、配送和交通规划中广泛使用。由于 OP 的NP-hard性，得到优化的解决方案是内在复杂的。虽然 Pointer Networks (Ptr-Nets) 在其他 combinatorial 任务中表现出色，但在 OP  中它们的表现还有提升的空间。这篇研究发现了 Pointer Q-Network (PQN)，这是一种 combining Ptr-Nets 和 Q-learning 的新方法，能够有效地解决 OP  中的特定挑战。我们深入探讨了 PQN 的体系和效率，并证明它在管理 OP  situations 方面的能力。
</details></li>
</ul>
<hr>
<h2 id="An-adaptive-standardisation-model-for-Day-Ahead-electricity-price-forecasting"><a href="#An-adaptive-standardisation-model-for-Day-Ahead-electricity-price-forecasting" class="headerlink" title="An adaptive standardisation model for Day-Ahead electricity price forecasting"></a>An adaptive standardisation model for Day-Ahead electricity price forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02610">http://arxiv.org/abs/2311.02610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CCaribe9/AdaptStdEPF">https://github.com/CCaribe9/AdaptStdEPF</a></li>
<li>paper_authors: Carlos Sebastián, Carlos E. González-Guillén, Jesús Juan</li>
<li>for: 这个研究旨在探讨日前价格的电力市场，以提高时间序列预测的精度。</li>
<li>methods: 本研究提出了一种适应标准化方法，以减少 dataset shift 的影响，并将学习算法优先显示target variable 和说明 variable之间的真实关系。</li>
<li>results: 研究发现在四个不同的市场中，适应标准化方法可以实现明显的提升，并且这些方法比较简单，广泛地被接受在文献中。<details>
<summary>Abstract</summary>
The study of Day-Ahead prices in the electricity market is one of the most popular problems in time series forecasting. Previous research has focused on employing increasingly complex learning algorithms to capture the sophisticated dynamics of the market. However, there is a threshold where increased complexity fails to yield substantial improvements. In this work, we propose an alternative approach by introducing an adaptive standardisation to mitigate the effects of dataset shifts that commonly occur in the market. By doing so, learning algorithms can prioritize uncovering the true relationship between the target variable and the explanatory variables. We investigate four distinct markets, including two novel datasets, previously unexplored in the literature. These datasets provide a more realistic representation of the current market context, that conventional datasets do not show. The results demonstrate a significant improvement across all four markets, using learning algorithms that are less complex yet widely accepted in the literature. This significant advancement unveils opens up new lines of research in this field, highlighting the potential of adaptive transformations in enhancing the performance of forecasting models.
</details>
<details>
<summary>摘要</summary>
研究日前价格在电力市场是时间序列预测中最受欢迎的问题。前期研究强调使用越来越复杂的学习算法来捕捉市场的复杂 Dynamics。然而，存在一个阈值，其中增加复杂性不会导致明显的改善。在这项工作中，我们提出一种alternative方法，通过引入适应标准化来 Mitigate the effects of dataset shifts that commonly occur in the market。这样，学习算法可以更好地捕捉目标变量和解释变量之间的真实关系。我们对四个不同的市场进行了调查，包括两个新的数据集，之前在文献中未被探讨。这些数据集提供了更加现实的市场Context，与传统数据集不同。结果表明在所有四个市场中，使用 Less complex yet widely accepted in the literature 的学习算法可以 achieve significant improvement。这一成果推开了新的研究方向， highlighting the potential of adaptive transformations in enhancing the performance of forecasting models.
</details></li>
</ul>
<hr>
<h2 id="Steady-State-Analysis-of-Queues-with-Hawkes-Arrival-and-Its-Application-to-Online-Learning-for-Hawkes-Queues"><a href="#Steady-State-Analysis-of-Queues-with-Hawkes-Arrival-and-Its-Application-to-Online-Learning-for-Hawkes-Queues" class="headerlink" title="Steady-State Analysis of Queues with Hawkes Arrival and Its Application to Online Learning for Hawkes Queues"></a>Steady-State Analysis of Queues with Hawkes Arrival and Its Application to Online Learning for Hawkes Queues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02577">http://arxiv.org/abs/2311.02577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyun Chen, Guiyu Hong</li>
<li>for: 研究单服务器队列的长期行为，包括 Hawkes 到达和一般服务分布。</li>
<li>methods: 使用新的 coupling 技术，确定工作负荷和忙期过程的finite moment bound。</li>
<li>results: 显示 Hawkes 队列在极高负荷 regime 下的员工编制问题可以通过数据驱动的方式解决，与 класси GI&#x2F;GI&#x2F;1 模型存在显著差异。<details>
<summary>Abstract</summary>
We investigate the long-run behavior of single-server queues with Hawkes arrivals and general service distributions and related optimization problems. In detail, utilizing novel coupling techniques, we establish finite moment bounds for the stationary distribution of the workload and busy period processes. In addition, we are able to show that, those queueing processes converge exponentially fast to their stationary distribution. Based on these theoretic results, we develop an efficient numerical algorithm to solve the optimal staffing problem for the Hawkes queues in a data-driven manner. Numerical results indicate a sharp difference in staffing for Hawkes queues, compared to the classic GI/GI/1 model, especially in the heavy-traffic regime.
</details>
<details>
<summary>摘要</summary>
我团队 investigate 单服务器队列中的长期行为，包括骚然来到和通用服务分布。我们使用新的协同技术来确定工作负载和忙期过程的finite moment bound。此外，我们还证明这些队列过程在 exponentially fast 速度下逐渐 converges to 其 stationary distribution。基于这些理论结论，我们开发了一种高效的数据驱动的 numerical algorithm 来解决 Hawkes 队列的优化人员问题。numerical results 显示，在高负荷情况下，Hawkes 队列的人员配置与 classical GI/GI/1 模型存在很大的差异。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Treasure-Hunt-Content-based-Time-Series-Retrieval-System-for-Discovering-Insights"><a href="#Temporal-Treasure-Hunt-Content-based-Time-Series-Retrieval-System-for-Discovering-Insights" class="headerlink" title="Temporal Treasure Hunt: Content-based Time Series Retrieval System for Discovering Insights"></a>Temporal Treasure Hunt: Content-based Time Series Retrieval System for Discovering Insights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02560">http://arxiv.org/abs/2311.02560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chin-Chia Michael Yeh, Huiyuan Chen, Xin Dai, Yan Zheng, Yujie Fan, Vivian Lai, Junpeng Wang, Audrey Der, Zhongfang Zhuang, Liang Wang, Wei Zhang</li>
<li>for: 这篇论文是针对多个领域的时间序列数据进行内容基于时间序列搜寻（CTSR）的研究。</li>
<li>methods: 本研究使用了多种实验方法，包括传统的时间序列模型和distance learning模型。</li>
<li>results: 研究发现，distance learning模型在多个领域的时间序列数据上表现较好，并且超过了其他方法的性能。<details>
<summary>Abstract</summary>
Time series data is ubiquitous across various domains such as finance, healthcare, and manufacturing, but their properties can vary significantly depending on the domain they originate from. The ability to perform Content-based Time Series Retrieval (CTSR) is crucial for identifying unknown time series examples. However, existing CTSR works typically focus on retrieving time series from a single domain database, which can be inadequate if the user does not know the source of the query time series. This limitation motivates us to investigate the CTSR problem in a scenario where the database contains time series from multiple domains. To facilitate this investigation, we introduce a CTSR benchmark dataset that comprises time series data from a variety of domains, such as motion, power demand, and traffic. This dataset is sourced from a publicly available time series classification dataset archive, making it easily accessible to researchers in the field. We compare several popular methods for modeling and retrieving time series data using this benchmark dataset. Additionally, we propose a novel distance learning model that outperforms the existing methods. Overall, our study highlights the importance of addressing the CTSR problem across multiple domains and provides a useful benchmark dataset for future research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>时序数据在各个领域都是普遍存在的，如金融、医疗和制造等，但它们的特性可能因领域而异。能够进行内容基于时序数据检索（CTSR）是识别未知时序示例的重要能力。然而，现有的CTSR工作通常将注重于从单一领域数据库中检索时序数据，这可能无法满足用户如果不知道查询时序数据的来源。这种限制驱动我们调查CTSR问题在多个领域数据库中进行检索。为了促进这种调查，我们提出了一个CTSRbenchmark dataset，该dataset包含来自多个领域的时序数据，如运动、电力需求和交通。这个dataset来自公共可用的时序数据分类dataset库，使其容易访问ible для研究人员。我们比较了许多流行的时序数据模型化和检索方法，并提出了一种新的距离学习模型，该模型在本 benchmark dataset上显示出比其他方法更高的性能。总的来说，我们的研究强调了跨多个领域的CTSR问题的重要性，并提供了一个有用的benchmark dataset，以便未来的研究人员可以继续进行深入的研究。
</details></li>
</ul>
<hr>
<h2 id="Fast-Minimization-of-Expected-Logarithmic-Loss-via-Stochastic-Dual-Averaging"><a href="#Fast-Minimization-of-Expected-Logarithmic-Loss-via-Stochastic-Dual-Averaging" class="headerlink" title="Fast Minimization of Expected Logarithmic Loss via Stochastic Dual Averaging"></a>Fast Minimization of Expected Logarithmic Loss via Stochastic Dual Averaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02557">http://arxiv.org/abs/2311.02557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chung-En Tsai, Hao-Chung Cheng, Yen-Huan Li</li>
<li>for: 这篇论文的目的是为了解决预期极小化函数损失的问题，包括解决波索因 inverse problem、计算量子态tomography的最大可能性估计、和approximatepositive semi-definite矩阵常量。</li>
<li>methods: 这篇论文提出了一种名为$B$-sample随机先验法，使用ilogarithmic barrier，用于解决这些问题。</li>
<li>results: 对于波索因 inverse problem，这种算法可以在 $\tilde{O} (d^2&#x2F;\varepsilon^2)$ 时间内获得 $\varepsilon$-优解，与现状相当。对于量子态tomography的最大可能性估计，这种算法可以在 $\tilde{O} (d^3&#x2F;\varepsilon^2)$ 时间内获得 $\varepsilon$-优解，超过现有的随机先验法和批处理法。<details>
<summary>Abstract</summary>
Consider the problem of minimizing an expected logarithmic loss over either the probability simplex or the set of quantum density matrices. This problem encompasses tasks such as solving the Poisson inverse problem, computing the maximum-likelihood estimate for quantum state tomography, and approximating positive semi-definite matrix permanents with the currently tightest approximation ratio. Although the optimization problem is convex, standard iteration complexity guarantees for first-order methods do not directly apply due to the absence of Lipschitz continuity and smoothness in the loss function.   In this work, we propose a stochastic first-order algorithm named $B$-sample stochastic dual averaging with the logarithmic barrier. For the Poisson inverse problem, our algorithm attains an $\varepsilon$-optimal solution in $\tilde{O} (d^2/\varepsilon^2)$ time, matching the state of the art. When computing the maximum-likelihood estimate for quantum state tomography, our algorithm yields an $\varepsilon$-optimal solution in $\tilde{O} (d^3/\varepsilon^2)$ time, where $d$ denotes the dimension. This improves on the time complexities of existing stochastic first-order methods by a factor of $d^{\omega-2}$ and those of batch methods by a factor of $d^2$, where $\omega$ denotes the matrix multiplication exponent. Numerical experiments demonstrate that empirically, our algorithm outperforms existing methods with explicit complexity guarantees.
</details>
<details>
<summary>摘要</summary>
假设我们需要最小化预期的对数损失函数，该函数可能是对概率 simpliciter 或者是对量子稳定矩阵的函数。这个问题包括解决波尔兹 inverse problem、计算量子状态探测的最大可信度估计以及approximating positive semi-definite matrix permanents with the currently tightest approximation ratio。虽然优化问题是凸的，但标准的第一阶方法的证明不直接适用，因为损失函数缺乏 lipschitz 连续和光滑性。在这篇文章中，我们提出了一种随机首频方法，名为 $B$-sample stochastic dual averaging with the logarithmic barrier。对于波尔兹 inverse problem，我们的算法可以在 $\tilde{O} (d^2/\varepsilon^2)$ 时间内获得 $\varepsilon$-优质解，与现状精度相同。当计算量子状态探测的最大可信度估计时，我们的算法可以在 $\tilde{O} (d^3/\varepsilon^2)$ 时间内获得 $\varepsilon$-优质解，其中 $d$ 表示维度。这超过了现有的随机首频方法的时间复杂度，它们的时间复杂度为 $d^{2\omega-2}$，其中 $\omega$ 是矩乘元 exponent。此外，我们的算法也超过了批处理方法的时间复杂度，它们的时间复杂度为 $d^2$。实验表明，我们的算法在实际应用中比现有的方法 WITH 显式复杂度保证更好。
</details></li>
</ul>
<hr>
<h2 id="High-dimensional-Bid-Learning-for-Energy-Storage-Bidding-in-Energy-Markets"><a href="#High-dimensional-Bid-Learning-for-Energy-Storage-Bidding-in-Energy-Markets" class="headerlink" title="High-dimensional Bid Learning for Energy Storage Bidding in Energy Markets"></a>High-dimensional Bid Learning for Energy Storage Bidding in Energy Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02551">http://arxiv.org/abs/2311.02551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyu Liu, Hongye Guo, Qinghu Tang, En Lu, Qiuna Cai, Qixin Chen</li>
<li>for:  optimize the profitability of Energy Storage Systems (ESSs) in electricity markets with high-dimensional price-quantity bids.</li>
<li>methods:  modify the common reinforcement learning (RL) process by proposing a new bid representation method called Neural Network Embedded Bids (NNEBs), which represents market bids as monotonic neural networks with discrete outputs.</li>
<li>results:  achieve 18% higher profit than the baseline and up to 78% profit of the optimal market bidder in real-world market datasets.<details>
<summary>Abstract</summary>
With the growing penetration of renewable energy resource, electricity market prices have exhibited greater volatility. Therefore, it is important for Energy Storage Systems(ESSs) to leverage the multidimensional nature of energy market bids to maximize profitability. However, current learning methods cannot fully utilize the high-dimensional price-quantity bids in the energy markets. To address this challenge, we modify the common reinforcement learning(RL) process by proposing a new bid representation method called Neural Network Embedded Bids (NNEBs). NNEBs refer to market bids that are represented by monotonic neural networks with discrete outputs. To achieve effective learning of NNEBs, we first learn a neural network as a strategic mapping from the market price to ESS power output with RL. Then, we re-train the network with two training modifications to make the network output monotonic and discrete. Finally, the neural network is equivalently converted into a high-dimensional bid for bidding. We conducted experiments over real-world market datasets. Our studies show that the proposed method achieves 18% higher profit than the baseline and up to 78% profit of the optimal market bidder.
</details>
<details>
<summary>摘要</summary>
随着可再生能源资源的普及，电力市场价格Display price exhibited greater volatility. Therefore, it is important for Energy Storage Systems (ESSs) to leverage the multidimensional nature of energy market bids to maximize profitability. However, current learning methods cannot fully utilize the high-dimensional price-quantity bids in the energy markets. To address this challenge, we modify the common reinforcement learning (RL) process by proposing a new bid representation method called Neural Network Embedded Bids (NNEBs). NNEBs refer to market bids that are represented by monotonic neural networks with discrete outputs. To achieve effective learning of NNEBs, we first learn a neural network as a strategic mapping from the market price to ESS power output with RL. Then, we re-train the network with two training modifications to make the network output monotonic and discrete. Finally, the neural network is equivalently converted into a high-dimensional bid for bidding. We conducted experiments over real-world market datasets. Our studies show that the proposed method achieves 18% higher profit than the baseline and up to 78% profit of the optimal market bidder.Here's the breakdown of the translation:* 随着可再生能源资源的普及 (followed by a list of words in Simplified Chinese):	+ 可再生能源 (renewable energy)	+ 资源 (resources)	+ 的普及 (penetration)* 电力市场价格Display price exhibited greater volatility. (list of words in Simplified Chinese):	+ 电力 (electricity)	+ 市场 (market)	+ 价格 (price)	+ Display (as a noun, meaning "display" or "exhibition")	+ 更大 (greater)	+ 变化 (volatility)* Therefore, it is important for Energy Storage Systems (ESSs) to leverage the multidimensional nature of energy market bids to maximize profitability. (list of words in Simplified Chinese):	+ 因此 (therefore)	+ Energy Storage Systems (ESSs) (as a noun, meaning "Energy Storage Systems")	+ 需要 (need)	+ 利用 (leverage)	+ 多维 (multidimensional)	+ 能源市场 (energy market)	+ 招标 (bids)	+ 提高 (profitability)* However, current learning methods cannot fully utilize the high-dimensional price-quantity bids in the energy markets. (list of words in Simplified Chinese):	+ 然而 (however)	+ current (as an adjective, meaning "current")	+ learning (as a noun, meaning "learning")	+ methods (as a noun, meaning "methods")	+ cannot (as a verb, meaning "cannot")	+ fully (as an adverb, meaning "fully")	+ utilize (as a verb, meaning "utilize")	+ high-dimensional (as an adjective, meaning "high-dimensional")	+ price-quantity (as a noun phrase, meaning "price-quantity")	+ bids (as a noun, meaning "bids")	+ in (as a preposition, meaning "in")	+ the (as a definite article, meaning "the")	+ energy (as a noun, meaning "energy")	+ markets (as a noun, meaning "markets")* To address this challenge, we modify the common reinforcement learning (RL) process by proposing a new bid representation method called Neural Network Embedded Bids (NNEBs). (list of words in Simplified Chinese):	+  Address (as a verb, meaning "address")	+ this (as a pronoun, meaning "this")	+ challenge (as a noun, meaning "challenge")	+ we (as a pronoun, meaning "we")	+ modify (as a verb, meaning "modify")	+ common (as an adjective, meaning "common")	+ reinforcement (as a noun, meaning "reinforcement")	+ learning (as a noun, meaning "learning")	+ process (as a noun, meaning "process")	+ by (as a preposition, meaning "by")	+ proposing (as a verb, meaning "proposing")	+ a (as a definite article, meaning "a")	+ new (as an adjective, meaning "new")	+ bid (as a noun, meaning "bid")	+ representation (as a noun, meaning "representation")	+ method (as a noun, meaning "method")	+ called (as a verb, meaning "called")	+ Neural Network Embedded Bids (NNEBs) (as a noun phrase, meaning "Neural Network Embedded Bids")* NNEBs refer to market bids that are represented by monotonic neural networks with discrete outputs. (list of words in Simplified Chinese):	+ NNEBs (as a noun phrase, meaning "NNEBs")	+ refer (as a verb, meaning "refer")	+ market (as a noun, meaning "market")	+ bids (as a noun, meaning "bids")	+ that (as a pronoun, meaning "that")	+ are (as a verb, meaning "are")	+ represented (as a verb, meaning "represented")	+ by (as a preposition, meaning "by")	+ monotonic (as an adjective, meaning "monotonic")	+ neural (as an adjective, meaning "neural")	+ networks (as a noun, meaning "networks")	+ with (as a preposition, meaning "with")	+ discrete (as an adjective, meaning "discrete")	+ outputs (as a noun, meaning "outputs")* To achieve effective learning of NNEBs, we first learn a neural network as a strategic mapping from the market price to ESS power output with RL. (list of words in Simplified Chinese):	+ to (as a preposition, meaning "to")	+ achieve (as a verb, meaning "achieve")	+ effective (as an adjective, meaning "effective")	+ learning (as a noun, meaning "learning")	+ of (as a preposition, meaning "of")	+ NNEBs (as a noun phrase, meaning "NNEBs")	+ we (as a pronoun, meaning "we")	+ first (as an adverb, meaning "first")	+ learn (as a verb, meaning "learn")	+ a (as a definite article, meaning "a")	+ neural (as an adjective, meaning "neural")	+ network (as a noun, meaning "network")	+ as (as a preposition, meaning "as")	+ a (as a definite article, meaning "a")	+ strategic (as an adjective, meaning "strategic")	+ mapping (as a noun, meaning "mapping")	+ from (as a preposition, meaning "from")	+ the (as a definite article, meaning "the")	+ market (as a noun, meaning "market")	+ price (as a noun, meaning "price")	+ to (as a preposition, meaning "to")	+ ESS (as a noun, meaning "ESS")	+ power (as a noun, meaning "power")	+ output (as a noun, meaning "output")	+ with (as a preposition, meaning "with")	+ RL (as a noun, meaning "RL")* Then, we re-train the network with two training modifications to make the network output monotonic and discrete. (list of words in Simplified Chinese):	+ Then (as an adverb, meaning "then")	+ we (as a pronoun, meaning "we")	+ re-train (as a verb, meaning "re-train")	+ the (as a definite article, meaning "the")	+ network (as a noun, meaning "network")	+ with (as a preposition, meaning "with")	+ two (as a numeral, meaning "two")	+ training (as a noun, meaning "training")	+ modifications (as a noun, meaning "modifications")	+ to (as a preposition, meaning "to")	+ make (as a verb, meaning "make")	+ the (as a definite article, meaning "the")	+ network (as a noun, meaning "network")	+ output (as a noun, meaning "output")	+ monotonic (as an adjective, meaning "monotonic")	+ and (as a conjunction, meaning "and")	+ discrete (as an adjective, meaning "discrete")* Finally, the neural network is equivalently converted into a high-dimensional bid for bidding. (list of words in Simplified Chinese):	+ Finally (as an adverb, meaning "finally")	+ the (as a definite article, meaning "the")	+ neural (as an adjective, meaning "neural")	+ network (as a noun, meaning "network")	+ is (as a verb, meaning "is")	+ equivalently (as an adverb, meaning "equivalently")	+ converted (as a verb, meaning "converted")	+ into (as a preposition, meaning "into")	+ a (as a definite article, meaning "a")	+ high-dimensional (as an adjective, meaning "high-dimensional")	+ bid (as a noun, meaning "bid")	+ for (as a preposition, meaning "for")	+ bidding (as a noun, meaning "bidding")Note that the translation is based on the given text and may not be perfect or exact, as the meaning of the text may be subject to interpretation.
</details></li>
</ul>
<hr>
<h2 id="Preliminary-Analysis-on-Second-Order-Convergence-for-Biased-Policy-Gradient-Methods"><a href="#Preliminary-Analysis-on-Second-Order-Convergence-for-Biased-Policy-Gradient-Methods" class="headerlink" title="Preliminary Analysis on Second-Order Convergence for Biased Policy Gradient Methods"></a>Preliminary Analysis on Second-Order Convergence for Biased Policy Gradient Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02546">http://arxiv.org/abs/2311.02546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siqiao Mu, Diego Klabjan</li>
<li>for: 研究policy gradient算法对非几何函数的收敛性。</li>
<li>methods: 利用非难点准则估计器和非难点梯度下降更新。</li>
<li>results: 提供了biased policy gradient算法收敛到第二阶点的初步结果，并计划进一步提供actor-critic算法在 finite-time中的第二阶点收敛分析。<details>
<summary>Abstract</summary>
Although the convergence of policy gradient algorithms to first-order stationary points is well-established, the objective functions of reinforcement learning problems are typically highly nonconvex. Therefore, recent work has focused on two extensions: ``global" convergence guarantees under regularity assumptions on the function structure, and second-order guarantees for escaping saddle points and convergence to true local minima. Our work expands on the latter approach, avoiding the restrictive assumptions of the former that may not apply to general objective functions. Existing results on vanilla policy gradient only consider an unbiased gradient estimator, but practical implementations under the infinite-horizon discounted setting, including both Monte-Carlo methods and actor-critic methods, involve gradient descent updates with a biased gradient estimator. We present preliminary results on the convergence of biased policy gradient algorithms to second-order stationary points, leveraging proof techniques from nonconvex optimization. In our next steps we aim to provide the first finite-time second-order convergence analysis for actor-critic algorithms.
</details>
<details>
<summary>摘要</summary>
although the policy gradient algorithms convergence to first-order stationary points is well-established, the objective functions of reinforcement learning problems are typically highly nonconvex. Therefore, recent work has focused on two extensions: "global" convergence guarantees under regularity assumptions on the function structure, and second-order guarantees for escaping saddle points and convergence to true local minima. our work expands on the latter approach, avoiding the restrictive assumptions of the former that may not apply to general objective functions. existing results on vanilla policy gradient only consider an unbiased gradient estimator, but practical implementations under the infinite-horizon discounted setting, including both Monte-Carlo methods and actor-critic methods, involve gradient descent updates with a biased gradient estimator. we present preliminary results on the convergence of biased policy gradient algorithms to second-order stationary points, leveraging proof techniques from nonconvex optimization. in our next steps, we aim to provide the first finite-time second-order convergence analysis for actor-critic algorithms.Here's the word-for-word translation of the given text into Simplified Chinese:虽然政策梯度算法对初级站点的整合已经很好地证明，但问题函数对于循环学习问题通常是非凸的。因此，最近的工作专注在两个扩展上：“全局”的整合保证，以及避免凸函数结构的约束下的第二类保证。我们的工作是扩展其中的后一种方法，避免了前一种方法可能不适用于一般问题函数。现有的结果仅考虑了不偏的梯度估计器，但在实际实现中，包括Monte-Carlo方法和actor-critic方法，均涉及到偏梯度估计器的梯度下降。我们发表了偏梯度政策梯度算法的初步结果，利用非凸估计的证明技巧。在下一步中，我们将提供首次具有时间限制的第二类整合分析，用于actor-critic算法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/05/cs.LG_2023_11_05/" data-id="cloojsmiu00s9re882ejje6qk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/05/eess.IV_2023_11_05/" class="article-date">
  <time datetime="2023-11-05T09:00:00.000Z" itemprop="datePublished">2023-11-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/05/eess.IV_2023_11_05/">eess.IV - 2023-11-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Flexible-uniform-sampling-foveated-Fourier-single-pixel-imaging"><a href="#Flexible-uniform-sampling-foveated-Fourier-single-pixel-imaging" class="headerlink" title="Flexible uniform-sampling foveated Fourier single-pixel imaging"></a>Flexible uniform-sampling foveated Fourier single-pixel imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02646">http://arxiv.org/abs/2311.02646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huan Cui, Jie Cao, Qun Hao, Haoyu Zhang, Chang Zhou</li>
<li>for: 提高单普ixel成像质量并降低数据量</li>
<li>methods:  uniform-sampling foveated Fourier single-pixel imaging (UFFSI)方法，包括三种特征：固定抽象结构、非均匀分布加工和高速傅立勤变换</li>
<li>results: 实验结果表明，通过UFFSI方法可以在0.0084抽象比例下，对高分辨率单普ixel成像进行快速和高质量恢复，并且可以降低数据量89%。<details>
<summary>Abstract</summary>
Fourier single-pixel imaging (FSI) is a data-efficient single-pixel imaging (SPI). However, there is still a serious challenge to obtain higher imaging quality using fewer measurements, which limits the development of real-time SPI. In this work, a uniform-sampling foveated FSI (UFFSI) is proposed with three features, uniform sampling, effective sampling and flexible fovea, to achieve under-sampling high-efficiency and high-quality SPI, even in a large-scale scene. First, by flexibly using the three proposed foveated pattern structures, data redundancy is reduced significantly to only require high resolution (HR) on regions of interest (ROIs), which radically reduces the need of total data number. Next, by the non-uniform weight distribution processing, non-uniform spatial sampling is transformed into uniform sampling, then the fast Fourier transform is used accurately and directly to obtain under-sampling high imaging quality with further reduced measurements. At a sampling ratio of 0.0084 referring to HR FSI with 1024*768 pixels, experimentally, by UFFSI with 255*341 cells of 89% reduction in data redundancy, the ROI has a significantly better imaging quality to meet imaging needs. We hope this work can provide a breakthrough for future real-time SPI.
</details>
<details>
<summary>摘要</summary>
富弗瑞尔单像素成像（FSI）是一种数据效率高的单像素成像（SPI）。然而，在使用更少测量时，实现更高的成像质量仍然是一个严重的挑战，这限制了实时SPI的发展。在这种工作中，我们提出了一种均匀采样快速投影（UFFSI），具有三个特点：均匀采样、有效采样和灵活投影，以实现快速高效率的SPI，即使在大规模场景中。首先，通过随机使用三种提出的快速投影结构，减少了数据繁殖，只需要在关键区域（ROI）高分辨率（HR），从而减少了总数据量的需求。其次，通过非均匀权重分配处理，将非均匀的空间采样转化为均匀采样，然后使用快速傅立叶变换，直接和高效率的投影结构进行对比，以实现快速高质量的成像。在0.0084的抽象比例（HR FSI）下，实验表明，通过UFFSI的255*341个细胞，89%的数据繁殖减少，ROI中的成像质量得到了显著改善，能满足成像需求。我们希望这种工作可以为未来的实时SPI提供一个突破。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/05/eess.IV_2023_11_05/" data-id="cloojsmps018nre88dx4l0s8n" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/05/eess.SP_2023_11_05/" class="article-date">
  <time datetime="2023-11-05T08:00:00.000Z" itemprop="datePublished">2023-11-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/05/eess.SP_2023_11_05/">eess.SP - 2023-11-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Age-of-Information-Analysis-for-CR-NOMA-Aided-Uplink-Systems-with-Randomly-Arrived-Packets"><a href="#Age-of-Information-Analysis-for-CR-NOMA-Aided-Uplink-Systems-with-Randomly-Arrived-Packets" class="headerlink" title="Age of Information Analysis for CR-NOMA Aided Uplink Systems with Randomly Arrived Packets"></a>Age of Information Analysis for CR-NOMA Aided Uplink Systems with Randomly Arrived Packets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02691">http://arxiv.org/abs/2311.02691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanshi Sun, Yanglin Ye, Zhiguo Ding, Momiao Zhou, Lei Liu</li>
<li>for: 本文研究了应用认知广播引导的非对称多access（CR-NOMA）技术来降低上行传输中的信息年龄（AoI）。特别是，本文考虑了一个基于时分多access（TDMA）的传统网络，每个用户都被分配了一个专门的时间槽来传输其状态更新信息。</li>
<li>methods: 本文使用了CR-NOMA技术作为TDMA传统网络的扩展，允许每个用户在其他用户的时间槽中发送信息，从而增加发送机会。文中还提出了一种精确的分析框架，以计算CR-NOMA和TDMA在不同情况下的AoI表现。</li>
<li>results: 研究结果显示，对于状态更新率较低的情况，CR-NOMA可以 significatively降低AoI，而且使用重传可以进一步降低AoI。此外，文中还提供了一些参考数据，以验证分析结果的准确性。<details>
<summary>Abstract</summary>
This paper studies the application of cognitive radio inspired non-orthogonal multiple access (CR-NOMA) to reduce age of information (AoI) for uplink transmission. In particular, a time division multiple access (TDMA) based legacy network is considered, where each user is allocated with a dedicated time slot to transmit its status update information. The CR-NOMA is implemented as an add-on to the TDMA legacy network, which enables each user to have more opportunities to transmit by sharing other user's time slots. A rigorous analytical framework is developed to obtain the expressions for AoIs achieved by CR-NOMA with and without re-transmission, by taking the randomness of the status update generating process into consideration. Numerical results are presented to verify the accuracy of the developed analysis. It is shown that the AoI can be significantly reduced by applying CR-NOMA compared to TDMA. Moreover, the use of re-transmission is helpful to reduce AoI, especially when the status arrival rate is low.
</details>
<details>
<summary>摘要</summary>
To analyze the performance of CR-NOMA, a rigorous analytical framework is developed to obtain expressions for AoIs achieved with and without re-transmission, taking into account the randomness of the status update generating process. Numerical results are presented to verify the accuracy of the developed analysis.The results show that CR-NOMA can significantly reduce AoI compared to TDMA. Moreover, the use of re-transmission is found to be particularly beneficial in reducing AoI, especially when the status arrival rate is low.
</details></li>
</ul>
<hr>
<h2 id="An-Open-Dataset-Storage-Standard-for-6G-Testbeds"><a href="#An-Open-Dataset-Storage-Standard-for-6G-Testbeds" class="headerlink" title="An Open Dataset Storage Standard for 6G Testbeds"></a>An Open Dataset Storage Standard for 6G Testbeds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02662">http://arxiv.org/abs/2311.02662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gilles Callebaut, Michiel Sandra, Christian Nelson, Thomas Wilding, Daan Delabie, Benjamin J. B. Deutschmann, William Tärneberg, Emma Fitzgerald, Anders J. Johansson, Liesbet Van der Perre</li>
<li>for: 这个研究旨在提高6G网络的资料分享效率和可重复性，并且推广实验室共享资料的FAIR原则（找到、存取、可操作、重复使用）。</li>
<li>methods: 本研究提出了Dataset Storage Standard（DSS），它是一个标准化的资料存储方式，可以实现跨不同实验室资料的互操作性和可重复性。DSS支持实验和模拟资料，并且提供了一个通用的定义档案来描述实验设备。</li>
<li>results: DSS可以实现跨不同实验室资料的互操作性和可重复性，并且可以让研究人员使用相同的处理脚本和工具进行资料分析。相比之前的标准化努力，如SigMF和NI RF Data Recording API，DSS具有更广泛的范围，可以涵盖更多的资料类型和实验设备。<details>
<summary>Abstract</summary>
The emergence of sixth-generation (6G) networks has spurred the development of novel testbeds, including sub-THz networks, cell-free systems, and 6G simulators. To maximize the benefits of these systems, it is crucial to make the generated data publicly available and easily reusable by others. Although data sharing has become a common practice, a lack of standardization hinders data accessibility and interoperability. In this study, we propose the Dataset Storage Standard (DSS) to address these challenges by facilitating data exchange and enabling convenient processing script creation in a testbed-agnostic manner. DSS supports both experimental and simulated data, allowing researchers to employ the same processing scripts and tools across different datasets. Unlike existing standardization efforts such as SigMF and NI RF Data Recording API, DSS provides a broader scope by accommodating a common definition file for testbeds and is not limited to RF data storage. The dataset format utilizes a hierarchical structure, with a tensor representation for specific experiment scenarios. In summary, DSS offers a comprehensive and flexible framework for enhancing the FAIR principles (Findability, Accessibility, Interoperability, and Reusability) in 6G testbeds, promoting open and efficient data sharing in the research community.
</details>
<details>
<summary>摘要</summary>
“六代（6G）网络的出现促进了新的测试平台的发展，包括互谐网络、无绳系统和6G模拟器。为了最大化这些系统的效益，它是必须将生成的数据公开可用并且方便其他人重用的。虽然数据分享已成为常见的做法，但缺乏标准化使得数据访问和兼容性受到限制。在这项研究中，我们提议使用数据存储标准（DSS）来解决这些挑战，以便在不同测试平台上交换数据和创建便捷的脚本。DSS支持实验和模拟数据，允许研究人员使用同样的处理脚本和工具在不同数据集之间进行研究。不同于现有的标准化尝试，如SigMF和NI RF数据记录API，DSS具有更广泛的范围，可以涵盖全面的测试平台定义文件。数据格式采用层次结构，使用tensor表示法来描述特定实验场景。总之，DSS提供了一个通用和灵活的框架，以便在6G测试平台中提高FAIR原则（发现性、可访问性、兼容性和再用性），推动开放和效率的数据分享在研究社区中。”
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Hybrid-Terrestrial-LEO-Satellite-Systems-for-Rural-Connectivity"><a href="#Exploiting-Hybrid-Terrestrial-LEO-Satellite-Systems-for-Rural-Connectivity" class="headerlink" title="Exploiting Hybrid Terrestrial&#x2F;LEO Satellite Systems for Rural Connectivity"></a>Exploiting Hybrid Terrestrial&#x2F;LEO Satellite Systems for Rural Connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02591">http://arxiv.org/abs/2311.02591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Houcem Ben Salem, Nour Kouzayha, Ammar EL Falou, Mohamed-Slim Alouini, Tareq Y. Al-Naffouri</li>
<li>for: 本研究旨在评估hybrid土地&#x2F;卫星网络在农村地域连接方面的性能。</li>
<li>methods: 本文使用 Stochastic Geometry 工具， derivates 可观测性和吞吐量的公式，并通过 Monte Carlo 仿真验证其准确性。</li>
<li>results: 获得的结果表明，卫星卫星群大小、 terrestrial 基站密度和 MIMO 配置参数均对网络性能产生影响。<details>
<summary>Abstract</summary>
Satellite networks are playing an important role in realizing global seamless connectivity in beyond 5G and 6G wireless networks. In this paper, we develop a comprehensive analytical framework to assess the performance of hybrid terrestrial/satellite networks in providing rural connectivity. We assume that the terrestrial base stations are equipped with multiple-input-multiple-output (MIMO) technologies and that the user has the option to associate with a base station or a satellite to be served. Using tools from stochastic geometry, we derive tractable expressions for the coverage probability and average data rate and prove the accuracy of the derived expressions through Monte Carlo simulations. The obtained results capture the impact of the satellite constellation size, the terrestrial base station density, and the MIMO configuration parameters.
</details>
<details>
<summary>摘要</summary>
卫星网络在超5G和6G无线网络中实现全球无缝连接具有重要作用。在这篇论文中，我们建立了一个完整的分析框架，用于评估卫星/陆地网络的农村连接性能。我们假设陆地基站装备了多输入多出力（MIMO）技术，并且用户有关联基站或卫星服务的选择权。使用Stochastic geometry工具，我们 deriv出了可读性好的覆盖率和平均数据传输率的表达式，并通过Monte Carlo伪实验证明其准确性。得到的结果反映了卫星遍天幕大小、陆地基站密度以及MIMO配置参数的影响。
</details></li>
</ul>
<hr>
<h2 id="Pilot-Based-Key-Distribution-and-Encryption-for-Secure-Coherent-Passive-Optical-Networks"><a href="#Pilot-Based-Key-Distribution-and-Encryption-for-Secure-Coherent-Passive-Optical-Networks" class="headerlink" title="Pilot-Based Key Distribution and Encryption for Secure Coherent Passive Optical Networks"></a>Pilot-Based Key Distribution and Encryption for Secure Coherent Passive Optical Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02554">http://arxiv.org/abs/2311.02554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haide Wang, Ji Zhou, Qingxin Lu, Jianrui Zeng, Yongqing Liao, Weiping Liu, Changyuan Yu, Zhaohui Li</li>
<li>for: 提高几何光纤网络（PON）的物理层安全性，防止广播传输导致的安全问题。</li>
<li>methods: 使用 Advanced Encryption Standard（AES）算法和四个水平的幂值幅调制（GCS-PAM4）导线基础的钥匙分配，以实现安全的几何光纤网络。</li>
<li>results: 实验结果表明，使用GCS-PAM4导线基础的钥匙分配可以在上行传输中实现无错误的传输，而不占用额外的过载。此外，使用AES算法可以防止下行传输中的侦测。<details>
<summary>Abstract</summary>
The security issues of passive optical networks (PONs) have always been a concern due to broadcast transmission. Physical-layer security enhancement for the coherent PON should be as significant as improving transmission performance. In this paper, we propose the advanced encryption standard (AES) algorithm and geometric constellation shaping four-level pulse amplitude modulation (GCS-PAM4) pilot-based key distribution for secure coherent PON. The first bit of the GCS-PAM4 pilot is used for the hardware-efficient carrier phase recovery (CPR), while the second bit is utilized for key distribution without occupying the additional overhead. The key bits are encoded by the polar code to ensure error-free distribution. Frequent key updates are permitted for every codeword to improve the security of coherent PON. The experimental results of the 200-Gbps secure coherent PON using digital subcarrier multiplexing show that the GCS-PAM4 pilot-based key distribution could be error-free at upstream transmission without occupying the additional overhead and the eavesdropping would be prevented by AES algorithm at downstream transmission. Moreover, there is almost no performance penalty on the CPR using the GCS-PAM4 pilot compared to the binary phase shift keying pilot.
</details>
<details>
<summary>摘要</summary>
安全问题是 оптических сетей (PON) 的担忧，尤其是因为广播传输。我们在本文提出了进化加密标准 (AES) 算法和四级振荡形幂（GCS-PAM4）导向的键分配，以提高幂等束缚的安全性。GCS-PAM4 的第一比特被用于硬件高效的扩载phase recovery（CPR），而第二比特被用于键分配，不占用额外的过头。键位编码为极地码，以确保错误自动发生。每个代码词可以进行频繁的键更新，以提高幂等束缚的安全性。实验结果表明，使用数字子副扩展多路复用的200Gbps 安全幂等束缚可以在上传传输中实现错误自动发生，而下传传输中防止侦测。此外，使用 GCS-PAM4 导向的 CPR 与使用二进制扭转键（BPSK）导向的 CPR 几乎没有性能差异。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/05/eess.SP_2023_11_05/" data-id="cloojsmrl01ckre8818pdc3u1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_11_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/04/cs.SD_2023_11_04/" class="article-date">
  <time datetime="2023-11-04T15:00:00.000Z" itemprop="datePublished">2023-11-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/04/cs.SD_2023_11_04/">cs.SD - 2023-11-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="OverHear-Headphone-based-Multi-sensor-Keystroke-Inference"><a href="#OverHear-Headphone-based-Multi-sensor-Keystroke-Inference" class="headerlink" title="OverHear: Headphone based Multi-sensor Keystroke Inference"></a>OverHear: Headphone based Multi-sensor Keystroke Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02288">http://arxiv.org/abs/2311.02288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raveen Wijewickrama, Maryam Abbasihafshejani, Anindya Maiti, Murtuza Jadliwala</li>
<li>for: 这篇论文旨在检测和分析Headphones中的键盘输入嗅探攻击。</li>
<li>methods: 该论文使用了OverHear框架，该框架利用了Headphones中的高级度麦克风和加速度计数据来进行键盘输入预测。</li>
<li>results: 实验结果表明，该方法可以在不同环境下达到键盘输入预测精度达80%以上，而 word prediction 精度超过70%。<details>
<summary>Abstract</summary>
Headphones, traditionally limited to audio playback, have evolved to integrate sensors like high-definition microphones and accelerometers. While these advancements enhance user experience, they also introduce potential eavesdropping vulnerabilities, with keystroke inference being our concern in this work. To validate this threat, we developed OverHear, a keystroke inference framework that leverages both acoustic and accelerometer data from headphones. The accelerometer data, while not sufficiently detailed for individual keystroke identification, aids in clustering key presses by hand position. Concurrently, the acoustic data undergoes analysis to extract Mel Frequency Cepstral Coefficients (MFCC), aiding in distinguishing between different keystrokes. These features feed into machine learning models for keystroke prediction, with results further refined via dictionary-based word prediction methods. In our experimental setup, we tested various keyboard types under different environmental conditions. We were able to achieve top-5 key prediction accuracy of around 80% for mechanical keyboards and around 60% for membrane keyboards with top-100 word prediction accuracies over 70% for all keyboard types. The results highlight the effectiveness and limitations of our approach in the context of real-world scenarios.
</details>
<details>
<summary>摘要</summary>
headphones, 原本只是专门用于音频播放的设备, 已经演化到添加了高级 Microphone 和加速度计数器等感应器。这些进步可以增强用户体验, 但也会带来 potential eavesdropping 问题, 我们在这个工作中关注的是键盘输入推测的问题。为了验证这个问题, 我们开发了 OverHear 框架, 这个框架利用 headphones 上的 acoustic 和加速度数据来进行键盘输入推测。加速度数据,  although not detailed enough for individual key press identification, 可以帮助分组键盘输入。同时, acoustic 数据会进行分析, 以提取 Mel Frequency Cepstral Coefficients (MFCC)，帮助区分不同的键盘输入。这些特征会被 feed 到机器学习模型中, 以进行键盘预测, 结果会透过字库基于词汇预测方法进一步精确化。在我们的实验设置中, 我们对不同环境下的不同键盘进行了试验, 我们能够 achieve top-5 key prediction accuracy 约 80% 以上, 以及 top-100 word prediction accuracy 约 70% 以上, 这些结果显示了我们的方法在实际应用中的有效性和局限性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/04/cs.SD_2023_11_04/" data-id="cloojsmlu00z4re88gymj2cbi" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/04/cs.CV_2023_11_04/" class="article-date">
  <time datetime="2023-11-04T13:00:00.000Z" itemprop="datePublished">2023-11-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/04/cs.CV_2023_11_04/">cs.CV - 2023-11-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Anthropomorphic-Grasping-with-Neural-Object-Shape-Completion"><a href="#Anthropomorphic-Grasping-with-Neural-Object-Shape-Completion" class="headerlink" title="Anthropomorphic Grasping with Neural Object Shape Completion"></a>Anthropomorphic Grasping with Neural Object Shape Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02510">http://arxiv.org/abs/2311.02510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Hidalgo-Carvajal, Hanzhi Chen, Gemma C. Bettelani, Jaesug Jung, Melissa Zavaglia, Laura Busse, Abdeldjallil Naceri, Stefan Leutenegger, Sami Haddadin</li>
<li>for: 这 paper 的目的是提高 робоット 在人造环境中的物体抓取和操作能力。</li>
<li>methods: 这 paper 使用了人类对物体的理解，通过重建和完善部分观察的物体形状，并使用7度自由度人工手臂来抓取和操作物体。</li>
<li>results: 这 paper 的方法在不同的方向和位置下，可以提高基eline的抓取成功率约30%，并实现了多种不同物体类别上的150多个成功抓取。这表明这种方法可以准确预测和执行抓取姿势，并在实际场景中提高了робоット的抓取和操作能力。<details>
<summary>Abstract</summary>
The progressive prevalence of robots in human-suited environments has given rise to a myriad of object manipulation techniques, in which dexterity plays a paramount role. It is well-established that humans exhibit extraordinary dexterity when handling objects. Such dexterity seems to derive from a robust understanding of object properties (such as weight, size, and shape), as well as a remarkable capacity to interact with them. Hand postures commonly demonstrate the influence of specific regions on objects that need to be grasped, especially when objects are partially visible. In this work, we leverage human-like object understanding by reconstructing and completing their full geometry from partial observations, and manipulating them using a 7-DoF anthropomorphic robot hand. Our approach has significantly improved the grasping success rates of baselines with only partial reconstruction by nearly 30% and achieved over 150 successful grasps with three different object categories. This demonstrates our approach's consistent ability to predict and execute grasping postures based on the completed object shapes from various directions and positions in real-world scenarios. Our work opens up new possibilities for enhancing robotic applications that require precise grasping and manipulation skills of real-world reconstructed objects.
</details>
<details>
<summary>摘要</summary>
人类环境中机器人的普及进展，导致了一系列物体抓取技巧的发展，dexterity在这些技巧中扮演着关键角色。人类在抓取物体时表现出了惊人的灵活性，这种灵活性归功于对物体特性（如重量、大小、形状）的稳固了理解，以及与物体进行互动的出色能力。手姿常常反映物体需要抓取的特定区域的影响，特别是当物体只部分可见时。在这项工作中，我们利用人类对物体的理解，通过重建和完成部分观察的物体形态，并使用7自由度人工手掌进行抓取。我们的方法比基eline只有部分重建时的抓取成功率提高了近30%，并在不同的物体类别上达成了150多次成功的抓取。这表明我们的方法可以在真实世界enario中预测和执行基于完整的物体形态的抓取姿势，开启了新的机器人应用的可能性，例如精准的抓取和 manipulate技巧。
</details></li>
</ul>
<hr>
<h2 id="Neural-Network-Reconstruction-of-the-Left-Atrium-using-Sparse-Catheter-Paths"><a href="#Neural-Network-Reconstruction-of-the-Left-Atrium-using-Sparse-Catheter-Paths" class="headerlink" title="Neural Network Reconstruction of the Left Atrium using Sparse Catheter Paths"></a>Neural Network Reconstruction of the Left Atrium using Sparse Catheter Paths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02488">http://arxiv.org/abs/2311.02488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alon Baram, Moshe Safran, Tomer Noy, Naveh Geri, Hayit Greenspan</li>
<li>for: 这个论文是为了提供一种可以在进程早期提供左心室Visualization的方法，以便使用简单的刺激器动作来获得 Left atrial shape reconstruction。</li>
<li>methods: 该论文提出了一种 dense encoder-decoder 网络，并使用一种新的Regularization term来重构左心室的形状。</li>
<li>results: 该论文表明，该方法可以在3分钟内基于部分数据来重构 Left atrial shape，并且可以生成真实的Visualization。 Synthetic和人类临床案例都被示出。<details>
<summary>Abstract</summary>
Catheter based radiofrequency ablation for pulmonary vein isolation has become the first line of treatment for atrial fibrillation in recent years. This requires a rather accurate map of the left atrial sub-endocardial surface including the ostia of the pulmonary veins, which requires dense sampling of the surface and takes more than 10 minutes. The focus of this work is to provide left atrial visualization early in the procedure to ease procedure complexity and enable further workflows, such as using catheters that have difficulty sampling the surface. We propose a dense encoder-decoder network with a novel regularization term to reconstruct the shape of the left atrium from partial data which is derived from simple catheter maneuvers. To train the network, we acquire a large dataset of 3D atria shapes and generate corresponding catheter trajectories. Once trained, we show that the suggested network can sufficiently approximate the atrium shape based on a given trajectory. We compare several network solutions for the 3D atrium reconstruction. We demonstrate that the solution proposed produces realistic visualization using partial acquisition within a 3-minute time interval. Synthetic and human clinical cases are shown.
</details>
<details>
<summary>摘要</summary>
医疗器械导管基于射频热力学隔离，作为现代抗不规征性颤动疾病治疗的首选方式，已经在过去几年得到广泛应用。这需要一个非常准确的左心室内部表面地图，包括肺动脉口，这需要密集的表面探测，需要 более10分钟的时间。我们的目标是提供早期左心室视觉，以便简化过程复杂性和启用更多的工作流程，如使用困难探测表面的导管。我们提议一种密集编码-解码网络，以重建左心室形状从partial数据中。为了训练网络，我们收集了大量3Datria形状数据，并生成相应的导管轨迹。我们显示，我们的提议的网络可以基于给定轨迹sufficiently approximate left atrium shape。我们比较了多个网络解决方案，并显示我们的解决方案可以生成真实的视觉使用部分收集在3分钟时间内。我们使用 sintetic和人类临床案例展示。
</details></li>
</ul>
<hr>
<h2 id="A-Strictly-Bounded-Deep-Network-for-Unpaired-Cyclic-Translation-of-Medical-Images"><a href="#A-Strictly-Bounded-Deep-Network-for-Unpaired-Cyclic-Translation-of-Medical-Images" class="headerlink" title="A Strictly Bounded Deep Network for Unpaired Cyclic Translation of Medical Images"></a>A Strictly Bounded Deep Network for Unpaired Cyclic Translation of Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02480">http://arxiv.org/abs/2311.02480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swati Rai, Jignesh S. Bhatt, Sarat Kumar Patra</li>
<li>for: 这个论文是关于医学影像翻译的一种解决方案，它的目标是提供一种稳定的双向翻译模型，可以处理不同模式的医学影像。</li>
<li>methods: 该论文提出了一种基于patch-level concatenated cyclic conditional generative adversarial network（pCCGAN）的方法，它包括两个相互连接的CGAN，每个 generator都是conditional的，它们使用叠加的异质补做patches来学习特征。同时， generator还使用adaptive dictionary来降低可能的衰减。</li>
<li>results: 论文的实验结果表明，该方法可以在实际的CT和MRI图像翻译中提供superior的结果，并且通过质量、量化和简洁分析表明，该方法可以减少变异和提高稳定性。<details>
<summary>Abstract</summary>
Medical image translation is an ill-posed problem. Unlike existing paired unbounded unidirectional translation networks, in this paper, we consider unpaired medical images and provide a strictly bounded network that yields a stable bidirectional translation. We propose a patch-level concatenated cyclic conditional generative adversarial network (pCCGAN) embedded with adaptive dictionary learning. It consists of two cyclically connected CGANs of 47 layers each; where both generators (each of 32 layers) are conditioned with concatenation of alternate unpaired patches from input and target modality images (not ground truth) of the same organ. The key idea is to exploit cross-neighborhood contextual feature information that bounds the translation space and boosts generalization. The generators are further equipped with adaptive dictionaries learned from the contextual patches to reduce possible degradation. Discriminators are 15-layer deep networks that employ minimax function to validate the translated imagery. A combined loss function is formulated with adversarial, non-adversarial, forward-backward cyclic, and identity losses that further minimize the variance of the proposed learning machine. Qualitative, quantitative, and ablation analysis show superior results on real CT and MRI.
</details>
<details>
<summary>摘要</summary>
医学图像翻译是一个不定Problem。不同于现有的已经对应的无限向量翻译网络，在这篇论文中，我们考虑了无对应的医学图像，并提供了一个具有稳定性的双向翻译网络。我们提议了一种patch-level concatenated cyclic conditional generative adversarial network (pCCGAN)，它包括两个相互连接的CGAN，每个CGAN都有47层，其中每个生成器都是通过 concatenation of alternate unpaired patches from input and target modality images (不是真实的ground truth) of the same organ来Conditional generation。我们的关键思想是利用跨邻域特征信息，以防止翻译空间过大，并提高泛化能力。生成器还使用了从Contextual patches中学习的自适应字典，以避免可能的下降。检测器是15层深度的网络，使用最大函数来验证翻译的图像。我们定义了一个组合损失函数，包括对抗损失、非对抗损失、前向后向环路损失和标识损失，以更加减小提议的学习机器的幂等误差。Qualitative、quantitative和ablation分析表明，我们的方法在真实的CT和MRI上达到了superior的结果。
</details></li>
</ul>
<hr>
<h2 id="SPHEAR-Spherical-Head-Registration-for-Complete-Statistical-3D-Modeling"><a href="#SPHEAR-Spherical-Head-Registration-for-Complete-Statistical-3D-Modeling" class="headerlink" title="SPHEAR: Spherical Head Registration for Complete Statistical 3D Modeling"></a>SPHEAR: Spherical Head Registration for Complete Statistical 3D Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02461">http://arxiv.org/abs/2311.02461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduard Gabriel Bazavan, Andrei Zanfir, Thiemo Alldieck, Teodor Alexandru Szente, Mihai Zanfir, Cristian Sminchisescu</li>
<li>for: 该论文旨在提出一种准确、可导的参数统计3D人头模型，基于圆柱体嵌入的新型3D注册方法。</li>
<li>methods: 该模型使用非rigid注册方法，不受表面假设所限制，提高重建准确性，并减少人工干预。</li>
<li>results: 该模型可以生成高分辨率的自然色Texture、表面法向图、毛发辐射图等，同时支持自动化的视觉数据生成、 semantic 注释和总体重建任务。与现有方法相比，该模型的组件快速、内存利用率高，实验证明了设计方式的有效性和注册、重建和生成技术的准确性。<details>
<summary>Abstract</summary>
We present \emph{SPHEAR}, an accurate, differentiable parametric statistical 3D human head model, enabled by a novel 3D registration method based on spherical embeddings. We shift the paradigm away from the classical Non-Rigid Registration methods, which operate under various surface priors, increasing reconstruction fidelity and minimizing required human intervention. Additionally, SPHEAR is a \emph{complete} model that allows not only to sample diverse synthetic head shapes and facial expressions, but also gaze directions, high-resolution color textures, surface normal maps, and hair cuts represented in detail, as strands. SPHEAR can be used for automatic realistic visual data generation, semantic annotation, and general reconstruction tasks. Compared to state-of-the-art approaches, our components are fast and memory efficient, and experiments support the validity of our design choices and the accuracy of registration, reconstruction and generation techniques.
</details>
<details>
<summary>摘要</summary>
我们介绍了SPHEAR，一种精准、可微分的参数型三维人头模型，基于圆形嵌入的新三维 регистра方法。我们弃却了传统的非固定注册方法，这些方法基于不同的表面优先级，从而提高了重建准确性并最小化了人工干预。此外，SPHEAR是一个完整的模型，允许不仅采样多种 sintetic 头部形状和 facial expression，还可以控制视线方向、高分辨率颜色Texture、表面法向图和毛发 Represented in detail, as strands。SPHEAR可以用于自动生成真实的视觉数据，semantic annotation和总体重建任务。相比之前的方法，我们的组件快速和内存减少，实验证明了我们的设计选择和注册、重建和生成技术的准确性。
</details></li>
</ul>
<hr>
<h2 id="Extracting-Network-Structures-from-Corporate-Organization-Charts-Using-Heuristic-Image-Processing"><a href="#Extracting-Network-Structures-from-Corporate-Organization-Charts-Using-Heuristic-Image-Processing" class="headerlink" title="Extracting Network Structures from Corporate Organization Charts Using Heuristic Image Processing"></a>Extracting Network Structures from Corporate Organization Charts Using Heuristic Image Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02460">http://arxiv.org/abs/2311.02460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiroki Sayama, Junichi Yamanoi</li>
<li>for: 这个研究旨在开拓企业结构的影响力和性能表现。</li>
<li>methods: 研究者开发了一种图像处理方法，用于从公司组织架构图中提取和重构组织网络数据。该方法包括多个逻辑步骤，通过识别文本标签、方框、连接线和其他对象来检测组织架构图中的元素。检测到的元素将被组织成Python的NetworkX图形对象，用于可视化、验证和进一步的网络分析。</li>
<li>results: 研究者通过应用该方法，成功地从2008年至2011年《企业组织架构图&#x2F;系统图手册》中的10,008个组织架构PDF文档中提取了4,606个组织网络（数据获取成功率为46%）。对每个重构的组织网络进行了多种网络指标的测量，以便进一步的统计分析，以 investigate 其可能的相关性与企业行为和表现。<details>
<summary>Abstract</summary>
Organizational structure of corporations has potential to provide implications for dynamics and performance of corporate operations. However, this subject has remained unexplored because of the lack of readily available organization network datasets. To overcome the this gap, we developed a new heuristic image-processing method to extract and reconstruct organization network data from published organization charts. Our method analyzes a PDF file of a corporate organization chart and detects text labels, boxes, connecting lines, and other objects through multiple steps of heuristically implemented image processing. The detected components are reorganized together into a Python's NetworkX Graph object for visualization, validation and further network analysis. We applied the developed method to the organization charts of all the listed firms in Japan shown in the ``Organization Chart/System Diagram Handbook'' published by Diamond, Inc., from 2008 to 2011. Out of the 10,008 organization chart PDF files, our method was able to reconstruct 4,606 organization networks (data acquisition success rate: 46%). For each reconstructed organization network, we measured several network diagnostics, which will be used for further statistical analysis to investigate their potential correlations with corporate behavior and performance.
</details>
<details>
<summary>摘要</summary>
企业组织结构具有可能对企业运营动态和性能产生影响，但这个主题尚未被探讨，因为有限的可用组织网络数据。为了bridge这个阻隔，我们开发了一种新的图像处理方法，用于从公布的组织图中提取和重建组织网络数据。我们的方法通过多个逻辑地图处理步骤，从PDF文档中的组织图中检测文本标签、方块、连接线和其他对象。检测到的组件被重新组织为Python的NetworkX图形对象，以供可视化、验证和进一步的网络分析。我们应用了这种方法于日本上市公司《组织图/系统 диаграм手册》（Diamond, Inc.，2008-2011年）中所显示的10,008个组织图PDF文档中。其中，我们的方法成功地重建了4,606个组织网络（数据获取成功率：46%）。对每个重建的组织网络，我们测量了多种网络指标，这些指标将用于进一步的统计分析，以研究它们可能与企业行为和性能之间的相关性。
</details></li>
</ul>
<hr>
<h2 id="P-Age-Pexels-Dataset-for-Robust-Spatio-Temporal-Apparent-Age-Classification"><a href="#P-Age-Pexels-Dataset-for-Robust-Spatio-Temporal-Apparent-Age-Classification" class="headerlink" title="P-Age: Pexels Dataset for Robust Spatio-Temporal Apparent Age Classification"></a>P-Age: Pexels Dataset for Robust Spatio-Temporal Apparent Age Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02432">http://arxiv.org/abs/2311.02432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abid Ali, Ashish Marisetty, Francois Bremond</li>
<li>for: 这个论文主要targets age estimation in videos, addressing challenges such as occlusions, low resolution, and lighting conditions.</li>
<li>methods: 该论文提出了一种新的方向 для年龄分类，利用视频基于模型来 capture 人体动态信息，dominating 面部基于方法。该方法使用两树结构，TimeSformer和EfficientNet作为 backing，以 efficiently capture 人体和面部动态信息。</li>
<li>results: 该方法在不同的视频数据集上进行了评测，与现有的面部基于方法相比，达到了更高的准确率。特别是在真实世界情况下，当人脸受到干扰、模糊或遮盾时，该方法仍能够准确地 estimte 年龄。<details>
<summary>Abstract</summary>
Age estimation is a challenging task that has numerous applications. In this paper, we propose a new direction for age classification that utilizes a video-based model to address challenges such as occlusions, low-resolution, and lighting conditions. To address these challenges, we propose AgeFormer which utilizes spatio-temporal information on the dynamics of the entire body dominating face-based methods for age classification. Our novel two-stream architecture uses TimeSformer and EfficientNet as backbones, to effectively capture both facial and body dynamics information for efficient and accurate age estimation in videos. Furthermore, to fill the gap in predicting age in real-world situations from videos, we construct a video dataset called Pexels Age (P-Age) for age classification. The proposed method achieves superior results compared to existing face-based age estimation methods and is evaluated in situations where the face is highly occluded, blurred, or masked. The method is also cross-tested on a variety of challenging video datasets such as Charades, Smarthome, and Thumos-14.
</details>
<details>
<summary>摘要</summary>
���������ж�� Age 估计是一项复杂的任务，具有许多应用。在这篇论文中，我们提出了一种新的方向 для age 分类，利用视频基本模型来解决 occlusions、低分辨率和照明条件等挑战。为了解决这些挑战，我们提出了 AgeFormer，它利用全身动态信息来替代面部基本方法 для age 分类。我们的新型两核体系使用 TimeSformer 和 EfficientNet 作为后备网络，以有效地捕捉全身动态信息以实现高效准确的 age 估计。此外，为了填充实际情况中的 age 估计漏斗，我们建立了一个名为 Pexels Age (P-Age) 的视频 dataset，用于 age 分类。我们的方法在不同的挑战性视频 dataset 上实现了比较出色的结果，比如 Charades、Smarthome 和 Thumos-14。
</details></li>
</ul>
<hr>
<h2 id="Task-Arithmetic-with-LoRA-for-Continual-Learning"><a href="#Task-Arithmetic-with-LoRA-for-Continual-Learning" class="headerlink" title="Task Arithmetic with LoRA for Continual Learning"></a>Task Arithmetic with LoRA for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02428">http://arxiv.org/abs/2311.02428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajas Chitale, Ankit Vaidya, Aditya Kane, Archana Ghotkar</li>
<li>for: 这篇论文旨在解决连续训练问题，即训练数据分配为“任务”的序列。</li>
<li>methods: 我们提出了一种使用低维数据适应和任务加权的新方法，以解决连续训练问题和对模型进行多次训练的计算成本问题。</li>
<li>results: 我们的方法可以完全对抗快速忘却问题，并且降低了训练模型的计算成本。将10个类别的样本存储在小型快取中可以实现近似于全集调整的性能。<details>
<summary>Abstract</summary>
Continual learning refers to the problem where the training data is available in sequential chunks, termed "tasks". The majority of progress in continual learning has been stunted by the problem of catastrophic forgetting, which is caused by sequential training of the model on streams of data. Moreover, it becomes computationally expensive to sequentially train large models multiple times. To mitigate both of these problems at once, we propose a novel method to continually train transformer-based vision models using low-rank adaptation and task arithmetic. Our method completely bypasses the problem of catastrophic forgetting, as well as reducing the computational requirement for training models on each task. When aided with a small memory of 10 samples per class, our method achieves performance close to full-set finetuning. We present rigorous ablations to support the prowess of our method.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese: kontinuäl learning réferë à problém where training data è disponibile in sequential chunks, termed "tasks". Majorité avancements in kontinuäl learning hanno été stunted per problem of catastrophic forgetting, which è caused da sequential training of model on streams of data. Moreover, it becomes computationally expensive to sequentially train large models multiple times. To mitigate both of these problems at once, we propose a novel method to continually train transformer-based vision models using low-rank adaptation and task arithmetic. Our method completely bypasses the problem of catastrophic forgetting, as well as reducing the computational requirement for training models on each task. When aided with a small memory of 10 samples per class, our method achieves performance close to full-set finetuning. We present rigorous ablations to support the prowess of our method.
</details></li>
</ul>
<hr>
<h2 id="P2O-Calib-Camera-LiDAR-Calibration-Using-Point-Pair-Spatial-Occlusion-Relationship"><a href="#P2O-Calib-Camera-LiDAR-Calibration-Using-Point-Pair-Spatial-Occlusion-Relationship" class="headerlink" title="P2O-Calib: Camera-LiDAR Calibration Using Point-Pair Spatial Occlusion Relationship"></a>P2O-Calib: Camera-LiDAR Calibration Using Point-Pair Spatial Occlusion Relationship</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02413">http://arxiv.org/abs/2311.02413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Su Wang, Shini Zhang, Xuchong Qiu</li>
<li>for: 提高自动驾驶和机器人领域中感知器的准确和可靠性，通过缺乏目标的准则进行抗干扰的三个维度推准。</li>
<li>methods: 基于三元比较关系的2D-3D边缘点抽取法，以及基于抽取的2D-3D点对对应关系进行干扰导向的点匹配方法。</li>
<li>results: 对实际图像集KITTI进行评估，比较方法与现有目标 moins 方法的性能，结果表明，提出的方法可以在各种环境中具有低误差和高可靠性，为实际应用中的高质量摄像头-LiDAR推准做出贡献。<details>
<summary>Abstract</summary>
The accurate and robust calibration result of sensors is considered as an important building block to the follow-up research in the autonomous driving and robotics domain. The current works involving extrinsic calibration between 3D LiDARs and monocular cameras mainly focus on target-based and target-less methods. The target-based methods are often utilized offline because of restrictions, such as additional target design and target placement limits. The current target-less methods suffer from feature indeterminacy and feature mismatching in various environments. To alleviate these limitations, we propose a novel target-less calibration approach which is based on the 2D-3D edge point extraction using the occlusion relationship in 3D space. Based on the extracted 2D-3D point pairs, we further propose an occlusion-guided point-matching method that improves the calibration accuracy and reduces computation costs. To validate the effectiveness of our approach, we evaluate the method performance qualitatively and quantitatively on real images from the KITTI dataset. The results demonstrate that our method outperforms the existing target-less methods and achieves low error and high robustness that can contribute to the practical applications relying on high-quality Camera-LiDAR calibration.
</details>
<details>
<summary>摘要</summary>
“准确和可靠的感知器的准确性是自动驾驶和机器人领域的重要基础结构。目前的外部投入calibration方法主要集中在目标基础和无目标方法两个领域。目标基础方法通常在线上使用，但是受到附加的目标设计和目标置放限制。现有的无目标方法受到环境中的特征不确定和特征匹配问题的影响。为了解决这些限制，我们提出了一种新的无目标准确方法，基于2D-3D边缘点提取和3D空间 occlusion 关系。基于提取的2D-3D点对，我们进一步提出了一种 occlusion-guided 点对应方法，可以提高准确率并降低计算成本。为验证我们的方法的有效性，我们对实际来自 KITTI 数据集的图像进行质量评估。结果表明，我们的方法在无目标情况下比现有的方法更高精度和更高可靠性，可以为实际应用中的高质量 Camera-LiDAR 准确协调做出贡献。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Hybrid-quantum-image-classification-and-federated-learning-for-hepatic-steatosis-diagnosis"><a href="#Hybrid-quantum-image-classification-and-federated-learning-for-hepatic-steatosis-diagnosis" class="headerlink" title="Hybrid quantum image classification and federated learning for hepatic steatosis diagnosis"></a>Hybrid quantum image classification and federated learning for hepatic steatosis diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02402">http://arxiv.org/abs/2311.02402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Lusnig, Asel Sagingalieva, Mikhail Surmach, Tatjana Protasevich, Ovidiu Michiu, Joseph McLoughlin, Christopher Mansell, Graziano de’ Petris, Deborah Bonazza, Fabrizio Zanconati, Alexey Melnikov, Fabio Cavalli<br>for: 这项研究旨在开发一种可助Pathologist在日常诊断中使用的智能系统，该系统可以利用深度学习技术和量子计算技术，并采用联合学习方法来实现隐私友好的多方参与学习。methods: 该研究使用了一种混合式量子神经网络，该网络包括5个量子比特和超过100个变量门，可以用于评估非酒精性肝脂肿，并提出了一种基于类型深度学习解决方案，通过减少每个参与者的数据量来解决隐私问题。results: 研究发现，混合式量子神经网络的肝脂肿图像分类精度达97%，高于其类似的类型深度学习模型（ResNet）的95.2%，而且在减少数据量的情况下，hybrid方法仍然能够superior generalization和less potential for overfitting，这表明该方法在医疗应用中具有优异的普适性和可靠性。<details>
<summary>Abstract</summary>
With the maturity achieved by deep learning techniques, intelligent systems that can assist physicians in the daily interpretation of clinical images can play a very important role. In addition, quantum techniques applied to deep learning can enhance this performance, and federated learning techniques can realize privacy-friendly collaborative learning among different participants, solving privacy issues due to the use of sensitive data and reducing the number of data to be collected for each individual participant. We present in this study a hybrid quantum neural network that can be used to quantify non-alcoholic liver steatosis and could be useful in the diagnostic process to determine a liver's suitability for transplantation; at the same time, we propose a federated learning approach based on a classical deep learning solution to solve the same problem, but using a reduced data set in each part. The liver steatosis image classification accuracy of the hybrid quantum neural network, the hybrid quantum ResNet model, consisted of 5 qubits and more than 100 variational gates, reaches 97%, which is 1.8% higher than its classical counterpart, ResNet. Crucially, that even with a reduced dataset, our hybrid approach consistently outperformed its classical counterpart, indicating superior generalization and less potential for overfitting in medical applications. In addition, a federated approach with multiple clients, up to 32, despite the lower accuracy, but still higher than 90%, would allow using, for each participant, a very small dataset, i.e., up to one-thirtieth. Our work, based over real-word clinical data can be regarded as a scalable and collaborative starting point, could thus fulfill the need for an effective and reliable computer-assisted system that facilitates the daily diagnostic work of the clinical pathologist.
</details>
<details>
<summary>摘要</summary>
随着深度学习技术的成熔，智能系统可以帮助医生日常解读临床图像，扮演着非常重要的角色。此外，应用于深度学习的量子技术可以提高这种性能，而 federated learning 技术可以实现隐私友好的合作学习，解决由敏感数据使用而导致的隐私问题，同时减少每个参与者需要收集的数据量。在本研究中，我们提出了一种混合量子神经网络，可以用于评估非酒精肝炎病变，并且可以在诊断过程中决定肝脏的适用性 для移植。同时，我们提出了基于类型深度学习解决方案的联邦学习方法，可以使用减少的数据集来解决同一个问题。混合量子神经网络的肝炎病变图像分类精度达97%，高于其类型深度学习模型（ResNet）的95.2%。更重要的是，我们的混合方法在减少数据集时仍然可以高效地分类肝炎病变图像，这表明它在医疗应用中具有更好的总结和更少的潜在过拟合问题。此外，多客户联邦学习方法可以使用每个参与者的非常小数据集（最多一半）来解决同一个问题，尽管它的准确率虽然不如单个客户的混合量子神经网络，但仍高于90%。我们的工作，基于实际临床数据，可以视为一种可扩展和协作的起点，可以满足医疗应用中的有效和可靠计算助手的需求。
</details></li>
</ul>
<hr>
<h2 id="Domain-Transfer-in-Latent-Space-DTLS-Wins-on-Image-Super-Resolution-–-a-Non-Denoising-Model"><a href="#Domain-Transfer-in-Latent-Space-DTLS-Wins-on-Image-Super-Resolution-–-a-Non-Denoising-Model" class="headerlink" title="Domain Transfer in Latent Space (DTLS) Wins on Image Super-Resolution – a Non-Denoising Model"></a>Domain Transfer in Latent Space (DTLS) Wins on Image Super-Resolution – a Non-Denoising Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02358">http://arxiv.org/abs/2311.02358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chun-Chuen Hui, Wan-Chi Siu, Ngai-Fong Law</li>
<li>for: 大规模图像超分辨是一项computer vision任务，因为很多信息在高度压缩图像中缺失，例如scale x16超分辨。</li>
<li>methods:  diffusion models 在过去几年中得到了成功，它使用 Gaussian noise 来构建一个 latent photo-realistic space，并作为连接latent vector space和 latent photo-realistic space的链接。</li>
<li>results: 在这篇文章中，我们提出了一种简单的方法，它不使用 Gaussian noise，而是采用基于diffusion models的一些基本结构来实现高质量的图像超分辨。我们使用 DNN 来实现域传递，以便利用邻域域的统计特性来进行渐进 interpolate，并通过参照输入LR图像来进行条件域传递，从而进一步提高图像质量。实验结果表明，我们的方法不仅超越了当前的大规模超分辨模型，还超越了当前的扩散模型。这种方法可以轻松扩展到其他图像到图像任务，如图像照明、填充、降噪等。<details>
<summary>Abstract</summary>
Large scale image super-resolution is a challenging computer vision task, since vast information is missing in a highly degraded image, say for example forscale x16 super-resolution. Diffusion models are used successfully in recent years in extreme super-resolution applications, in which Gaussian noise is used as a means to form a latent photo-realistic space, and acts as a link between the space of latent vectors and the latent photo-realistic space. There are quite a few sophisticated mathematical derivations on mapping the statistics of Gaussian noises making Diffusion Models successful. In this paper we propose a simple approach which gets away from using Gaussian noise but adopts some basic structures of diffusion models for efficient image super-resolution. Essentially, we propose a DNN to perform domain transfer between neighbor domains, which can learn the differences in statistical properties to facilitate gradual interpolation with results of reasonable quality. Further quality improvement is achieved by conditioning the domain transfer with reference to the input LR image. Experimental results show that our method outperforms not only state-of-the-art large scale super resolution models, but also the current diffusion models for image super-resolution. The approach can readily be extended to other image-to-image tasks, such as image enlightening, inpainting, denoising, etc.
</details>
<details>
<summary>摘要</summary>
大规模图像超解析是一项计算机视觉任务，因为高度受损图像中的信息量很大，例如 scale x16 超解析。扩散模型在过去几年得到了成功，在极端超解析应用中使用 Gaussian 噪声作为一种 latent photo-realistic 空间的形成者，并作为 latent vector 空间和 latent photo-realistic 空间之间的连接。有很多复杂的数学推导，映射 Gaussian 噪声的统计特性，使扩散模型成功。在这篇论文中，我们提出了一种简单的方法，不使用 Gaussian 噪声，但采用了扩散模型的一些基本结构，实现高质量图像超解析。我们提议使用 DNN 进行频率域传输，以便利用邻域频率的不同来实现慢滑均衡，并通过参考输入低解析图像来进行条件域传输，从而进一步提高图像质量。实验结果表明，我们的方法不仅超过了当前大规模超解析模型的状态，还超过了当前扩散模型的图像超解析性能。该方法可以轻松扩展到其他图像-图像任务，如图像照明、填充、去噪等。
</details></li>
</ul>
<hr>
<h2 id="Proposal-Level-Unsupervised-Domain-Adaptation-for-Open-World-Unbiased-Detector"><a href="#Proposal-Level-Unsupervised-Domain-Adaptation-for-Open-World-Unbiased-Detector" class="headerlink" title="Proposal-Level Unsupervised Domain Adaptation for Open World Unbiased Detector"></a>Proposal-Level Unsupervised Domain Adaptation for Open World Unbiased Detector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02342">http://arxiv.org/abs/2311.02342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuanyi Liu, Zhongqi Yue, Xian-Sheng Hua</li>
<li>for: 开展开放世界对象检测（OWOD），结合开放集合对象检测和逐步学习能力，面对视觉世界中的开放和动态挑战。</li>
<li>methods: 采用Unsupervised Domain Adaptation方法，将已知类别的预测器作为源频谱，使用自动回归法学习域 invariants 的前景特征，实现不偏向的预测。</li>
<li>results: 通过OOD evaluation，我们达到了状态 искусственный智能的性能水平。<details>
<summary>Abstract</summary>
Open World Object Detection (OWOD) combines open-set object detection with incremental learning capabilities to handle the challenge of the open and dynamic visual world. Existing works assume that a foreground predictor trained on the seen categories can be directly transferred to identify the unseen categories' locations by selecting the top-k most confident foreground predictions. However, the assumption is hardly valid in practice. This is because the predictor is inevitably biased to the known categories, and fails under the shift in the appearance of the unseen categories. In this work, we aim to build an unbiased foreground predictor by re-formulating the task under Unsupervised Domain Adaptation, where the current biased predictor helps form the domains: the seen object locations and confident background locations as the source domain, and the rest ambiguous ones as the target domain. Then, we adopt the simple and effective self-training method to learn a predictor based on the domain-invariant foreground features, hence achieving unbiased prediction robust to the shift in appearance between the seen and unseen categories. Our approach's pipeline can adapt to various detection frameworks and UDA methods, empirically validated by OWOD evaluation, where we achieve state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
Translation notes:* Open World Object Detection (OWOD) is translated as "开放世界物体检测" (kāifàng shìjiè wùzhì kǎoyan).* existing works is translated as "现有的工作" (xiàn yǒu de gōngzuò).* foreground predictor is translated as "前景预测器" (qiánjìng yùdiǎn).* unseen categories is translated as "未知类别" (wèi zhī lèibì).* domain adaptation is translated as "领域适应" (dòngyì tiěbìng).* self-training method is translated as "自我启用法" (zìwǒ kāifàng fǎ).* domain-invariant features is translated as "领域不变特征" (dòngyì bùbiàn tèzhèng).* unbiased prediction is translated as "无偏预测" (wùpíng yùdiǎn).* state-of-the-art performance is translated as "顶尖性能" (dǐngjiān xìngnéng).
</details></li>
</ul>
<hr>
<h2 id="MC-Stereo-Multi-peak-Lookup-and-Cascade-Search-Range-for-Stereo-Matching"><a href="#MC-Stereo-Multi-peak-Lookup-and-Cascade-Search-Range-for-Stereo-Matching" class="headerlink" title="MC-Stereo: Multi-peak Lookup and Cascade Search Range for Stereo Matching"></a>MC-Stereo: Multi-peak Lookup and Cascade Search Range for Stereo Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02340">http://arxiv.org/abs/2311.02340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaojie Feng, Junda Cheng, Hao Jia, Longliang Liu, Gangwei Xu, Xin Yang</li>
<li>for: 本研究旨在提高iterative optimization方法的掌握能力，特别是解决多峰分布问题和固定搜索范围的限制。</li>
<li>methods: 本文提出了一种新的iterative optimization架构，称为MC-Stereo，它通过多峰查找策略来减轻多峰分布问题，并在迭代框架中 интеGRATE了粗细搜索的概念。此外，我们还引入了一个预训练的网络来提高前端的特征提取器。</li>
<li>results: 根据实验结果，MC-Stereo在KITTI-2012和KITTI-2015测试集上 ranked first among all publicly available方法，并在ETH3D上达到了领域内最佳性能。<details>
<summary>Abstract</summary>
Stereo matching is a fundamental task in scene comprehension. In recent years, the method based on iterative optimization has shown promise in stereo matching. However, the current iteration framework employs a single-peak lookup, which struggles to handle the multi-peak problem effectively. Additionally, the fixed search range used during the iteration process limits the final convergence effects. To address these issues, we present a novel iterative optimization architecture called MC-Stereo. This architecture mitigates the multi-peak distribution problem in matching through the multi-peak lookup strategy, and integrates the coarse-to-fine concept into the iterative framework via the cascade search range. Furthermore, given that feature representation learning is crucial for successful learnbased stereo matching, we introduce a pre-trained network to serve as the feature extractor, enhancing the front end of the stereo matching pipeline. Based on these improvements, MC-Stereo ranks first among all publicly available methods on the KITTI-2012 and KITTI-2015 benchmarks, and also achieves state-of-the-art performance on ETH3D. The code will be open sourced after the publication of this paper.
</details>
<details>
<summary>摘要</summary>
斯tereo匹配是Scene理解中的基本任务。在最近几年，基于迭代优化的方法在斯tereo匹配中表现良好。然而，当前的迭代框架使用单峰搜索，在多峰问题中效果不佳。此外，使用的fixed搜索范围限制了最终的整合效果。为解决这些问题，我们提出了一种新的迭代优化架构，称为MC-Stereo。这种架构通过多峰搜索策略解决了匹配中的多峰分布问题，并通过缩放搜索范围实现了从粗到细的搜索概念。此外，由于特征表示学习是成功的learnbased斯tereo匹配的关键，我们引入了预训练的网络作为特征提取器，从而提高了斯tereo匹配管线的前端。基于这些改进，MC-Stereo在KITTI-2012和KITTI-2015benchmark上名列前茅，并在ETH3D上实现了状态的art performance。代码将在本文发表后开源。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Machine-Learning-for-Clinically-Assistive-Imaging-Based-Biomedical-Applications"><a href="#Multimodal-Machine-Learning-for-Clinically-Assistive-Imaging-Based-Biomedical-Applications" class="headerlink" title="Multimodal Machine Learning for Clinically-Assistive Imaging-Based Biomedical Applications"></a>Multimodal Machine Learning for Clinically-Assistive Imaging-Based Biomedical Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02332">http://arxiv.org/abs/2311.02332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elisa Warner, Joonsang Lee, William Hsu, Tanveer Syeda-Mahmood, Charles Kahn, Arvind Rao</li>
<li>for: 这项研究旨在探讨适用于医疗人工智能系统的机器学习应用，尤其是在多modal数据集合 integrate 方面。</li>
<li>methods: 这篇论文描述了五种对多modal AI 的挑战（表示、融合、对接、翻译和共学习），并评估了这些挑战在医疗影像基础的临床决策支持模型中的应用。</li>
<li>results: 这篇论文结论提出了未来这个领域的发展趋势，并建议了在成功诊断模型的翻译和应用中进一步探索的方向。<details>
<summary>Abstract</summary>
Machine learning (ML) applications in medical artificial intelligence (AI) systems have shifted from traditional and statistical methods to increasing application of deep learning models and even more recently generative models. Recent years have seen a rise in the discovery of widely-available deep learning architectures that support multimodal data integration, particularly with images. The incorporation of multiple modalities into these models is a thriving research topic, presenting its own unique challenges. In this work, we discuss five challenges to multimodal AI as it pertains to ML (representation, fusion, alignment, translation, and co-learning) and survey recent approaches to addressing these challenges in the context of medical image-based clinical decision support models. We conclude with a discussion of the future of the field, suggesting directions that should be elucidated further for successful clinical models and their translation to the clinical setting.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）在医疗人工智能（AI）系统中的应用从传统和统计方法逐渐转移到深度学习模型，并在最近几年内，更多地使用生成模型。在过去几年中，我们发现了许多可用的深度学习架构，尤其是与图像集成。将多种模式integrated into these models presents unique challenges. In this work, we discuss five challenges to multimodal AI as it pertains to ML (representation, fusion, alignment, translation, and co-learning) and survey recent approaches to addressing these challenges in the context of medical image-based clinical decision support models. We conclude with a discussion of the future of the field, suggesting directions that should be further explored for successful clinical models and their translation to the clinical setting.Here's a breakdown of the translation:1. 机器学习 (ML) - Machine learning2. 在医疗人工智能 (AI)系统中 - In medical artificial intelligence systems3. 应用从传统和统计方法逐渐转移 - From traditional and statistical methods to4. 到深度学习模型 - Deep learning models5. 并在最近几年内，更多地使用生成模型 - And more recently, using generative models6. 在过去几年中，我们发现了许多可用的深度学习架构 - In the past few years, we have found many available deep learning architectures7. 尤其是与图像集成 - Especially with image integration8. 将多种模式integrated into these models presents unique challenges - Integrating multiple modes into these models presents unique challenges9. In this work, we discuss five challenges to multimodal AI as it pertains to ML - In this work, we discuss five challenges to multimodal AI as it relates to machine learning10.  representation, fusion, alignment, translation, and co-learning - Representation, fusion, alignment, translation, and co-learning11. 并 survey recent approaches to addressing these challenges - And survey recent approaches to addressing these challenges12. 在医疗图像基础的临床决策模型中 - In medical image-based clinical decision support models13. We conclude with a discussion of the future of the field - We conclude with a discussion of the future of the field14. 建议更多的探索 - Suggesting further explorationPlease note that the translation is done in Simplified Chinese, which is the most widely used standard for Chinese writing. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Counting-Manatee-Aggregations-using-Deep-Neural-Networks-and-Anisotropic-Gaussian-Kernel"><a href="#Counting-Manatee-Aggregations-using-Deep-Neural-Networks-and-Anisotropic-Gaussian-Kernel" class="headerlink" title="Counting Manatee Aggregations using Deep Neural Networks and Anisotropic Gaussian Kernel"></a>Counting Manatee Aggregations using Deep Neural Networks and Anisotropic Gaussian Kernel</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02315">http://arxiv.org/abs/2311.02315</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yeyimilk/deep-learning-for-manatee-counting">https://github.com/yeyimilk/deep-learning-for-manatee-counting</a></li>
<li>paper_authors: Zhiqiang Wang, Yiran Pang, Cihan Ulus, Xingquan Zhu</li>
<li>for: 这个论文是为了自动计算水獭群体中的数量而写的。</li>
<li>methods: 该方法使用深度学习技术，使用低质量图像作为输入，并使用不同类型的深度神经网络来学习水獭的密度函数，以计算水獭群体中的数量。</li>
<li>results: 实验结果显示，使用Anisotropic Gaussian Kernel（AGK）kernel，并应用于不同类型的深度神经网络，可以准确地计算水獭群体中的数量，特别是在复杂背景环境下。<details>
<summary>Abstract</summary>
Manatees are aquatic mammals with voracious appetites. They rely on sea grass as the main food source, and often spend up to eight hours a day grazing. They move slow and frequently stay in group (i.e. aggregations) in shallow water to search for food, making them vulnerable to environment change and other risks. Accurate counting manatee aggregations within a region is not only biologically meaningful in observing their habit, but also crucial for designing safety rules for human boaters, divers, etc., as well as scheduling nursing, intervention, and other plans. In this paper, we propose a deep learning based crowd counting approach to automatically count number of manatees within a region, by using low quality images as input. Because manatees have unique shape and they often stay in shallow water in groups, water surface reflection, occlusion, camouflage etc. making it difficult to accurately count manatee numbers. To address the challenges, we propose to use Anisotropic Gaussian Kernel (AGK), with tunable rotation and variances, to ensure that density functions can maximally capture shapes of individual manatees in different aggregations. After that, we apply AGK kernel to different types of deep neural networks primarily designed for crowd counting, including VGG, SANet, Congested Scene Recognition network (CSRNet), MARUNet etc. to learn manatee densities and calculate number of manatees in the scene. By using generic low quality images extracted from surveillance videos, our experiment results and comparison show that AGK kernel based manatee counting achieves minimum Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). The proposed method works particularly well for counting manatee aggregations in environments with complex background.
</details>
<details>
<summary>摘要</summary>
MANATEES 是水生哺乳动物，具有极高的食量。它们依赖于海草为食，并可能每天花费8小时在牧场。它们移动缓慢，并常常在浅水区寻找食物，使得它们易受环境变化和其他风险的影响。正确地计算MANATEES 的聚集数量在一个区域非仅生物学上重要，还是关键的 для设计人类潜水员、潜水等安全规则，以及安排护理、救援等计划。在这篇论文中，我们提出了基于深度学习的人ATEES 聚集计数方法，使用低质量图像作为输入。由于MANATEES 的特有形状和它们常常在浅水区寻找食物，水面反射、遮挡等因素使得准确计数MANATEES 数量非常困难。为了解决这些挑战，我们提议使用不规则 Gaussian kernel（AGK），并可调整旋转和方差，以确保AGK kernel能够最大化个体MANATEES 形状在不同的聚集中。然后，我们将AGK kernel应用到不同类型的深度神经网络，包括 VGG、SANet、 Congested Scene Recognition network（CSRNet）等，以学习MANATEES 的浓度函数并计算场景中的MANATEES 数量。通过使用普通的低质量图像，我们的实验结果和比较表明，AGK kernel基于MANATEES 计数实现了最小的精度平均误差（MAE）和平方根误差（RMSE）。我们的方法在环境复杂背景下特别有效。
</details></li>
</ul>
<hr>
<h2 id="LISNeRF-Mapping-LiDAR-based-Implicit-Mapping-via-Semantic-Neural-Fields-for-Large-Scale-3D-Scenes"><a href="#LISNeRF-Mapping-LiDAR-based-Implicit-Mapping-via-Semantic-Neural-Fields-for-Large-Scale-3D-Scenes" class="headerlink" title="LISNeRF Mapping: LiDAR-based Implicit Mapping via Semantic Neural Fields for Large-Scale 3D Scenes"></a>LISNeRF Mapping: LiDAR-based Implicit Mapping via Semantic Neural Fields for Large-Scale 3D Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02313">http://arxiv.org/abs/2311.02313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianyuan Zhang, Zhiliu Yang</li>
<li>for: This paper proposes a method for large-scale 3D semantic reconstruction from LiDAR measurements alone, which is crucial for outdoor autonomous agents to fulfill high-level tasks such as planning and navigation.</li>
<li>methods: The proposed method uses an octree-based and hierarchical structure to store implicit features, which are decoded to semantic information and signed distance value through shallow Multilayer Perceptrons (MLPs). Off-the-shelf algorithms are used to predict semantic labels and instance IDs of point cloud, and the implicit features and MLPs parameters are jointly optimized with self-supervision paradigm for point cloud geometry and pseudo-supervision paradigm for semantic and panoptic labels.</li>
<li>results: The proposed method is evaluated on three real-world datasets, SemanticKITTI, SemanticPOSS, and nuScenes, and demonstrates effectiveness and efficiency compared to current state-of-the-art 3D mapping methods.<details>
<summary>Abstract</summary>
Large-scale semantic mapping is crucial for outdoor autonomous agents to fulfill high-level tasks such as planning and navigation. This paper proposes a novel method for large-scale 3D semantic reconstruction through implicit representations from LiDAR measurements alone. We firstly leverages an octree-based and hierarchical structure to store implicit features, then these implicit features are decoded to semantic information and signed distance value through shallow Multilayer Perceptrons (MLPs). We adopt off-the-shelf algorithms to predict the semantic labels and instance IDs of point cloud. Then we jointly optimize the implicit features and MLPs parameters with self-supervision paradigm for point cloud geometry and pseudo-supervision pradigm for semantic and panoptic labels. Subsequently, Marching Cubes algorithm is exploited to subdivide and visualize the scenes in the inferring stage. For scenarios with memory constraints, a map stitching strategy is also developed to merge sub-maps into a complete map. As far as we know, our method is the first work to reconstruct semantic implicit scenes from LiDAR-only input. Experiments on three real-world datasets, SemanticKITTI, SemanticPOSS and nuScenes, demonstrate the effectiveness and efficiency of our framework compared to current state-of-the-art 3D mapping methods.
</details>
<details>
<summary>摘要</summary>
大规模 semantic mapping 是外部自主 Agent 完成高级任务，如规划和导航的关键。这篇论文提出了一种基于 LiDAR 测量的大规模 3D semantic 重建方法。我们首先利用 Octree 结构来存储偏函数特征，然后使用 shallow Multilayer Perceptrons (MLPs) 来解码这些偏函数特征为 semantic 信息和 signed distance value。我们采用了市场上的算法来预测 semantic 标签和实例 ID 的点云。然后，我们同时优化偏函数和 MLPs 参数使用自我超级vised paradigm for point cloud geometry和 pseudo-supervision 概念 для semantic 和 panoptic 标签。在推断阶段，我们利用 Marching Cubes 算法来分割和可视化场景。对于存储限制的场景，我们还开发了一种 map stitching 策略来合并子地图到完整的地图。根据我们所知，我们的方法是首个从 LiDAR 输入 alone 重建 semantic implicit scene。我们在三个实际世界数据集（SemanticKITTI、SemanticPOSS 和 nuScenes）进行了实验，并证明了我们的框架在现状最佳的 3D 地图方法中表现出了效果和效率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/04/cs.CV_2023_11_04/" data-id="cloojsmg900kure88302kh0pz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/04/cs.AI_2023_11_04/" class="article-date">
  <time datetime="2023-11-04T12:00:00.000Z" itemprop="datePublished">2023-11-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/04/cs.AI_2023_11_04/">cs.AI - 2023-11-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="UniTSFace-Unified-Threshold-Integrated-Sample-to-Sample-Loss-for-Face-Recognition"><a href="#UniTSFace-Unified-Threshold-Integrated-Sample-to-Sample-Loss-for-Face-Recognition" class="headerlink" title="UniTSFace: Unified Threshold Integrated Sample-to-Sample Loss for Face Recognition"></a>UniTSFace: Unified Threshold Integrated Sample-to-Sample Loss for Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02523">http://arxiv.org/abs/2311.02523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiufu Li, Xi Jia, Jiancan Zhou, Linlin Shen, Jinming Duan</li>
<li>for: 这篇论文的目的是提出一种高效的面部识别方法，以满足现实世界中的面部验证应用。</li>
<li>methods: 该方法使用了一种新的集成损失函数（USS loss），该损失函数具有一个明确的统一阈值，用于分辨正面和负面对的对比。</li>
<li>results: 实验结果表明，提出的 USS loss 高效地使用了 sample-to-sample 的损失函数，并可以与 sample-to-class 的损失函数结合使用。此外，该方法在多个 benchmark 数据集上表现出色，比如 MFR、IJB-C、LFW、CFP-FP、AgeDB 和 MegaFace，并且可以超越现有的方法，如 CosFace、ArcFace、VPL、AnchorFace 和 UNPG。<details>
<summary>Abstract</summary>
Sample-to-class-based face recognition models can not fully explore the cross-sample relationship among large amounts of facial images, while sample-to-sample-based models require sophisticated pairing processes for training. Furthermore, neither method satisfies the requirements of real-world face verification applications, which expect a unified threshold separating positive from negative facial pairs. In this paper, we propose a unified threshold integrated sample-to-sample based loss (USS loss), which features an explicit unified threshold for distinguishing positive from negative pairs. Inspired by our USS loss, we also derive the sample-to-sample based softmax and BCE losses, and discuss their relationship. Extensive evaluation on multiple benchmark datasets, including MFR, IJB-C, LFW, CFP-FP, AgeDB, and MegaFace, demonstrates that the proposed USS loss is highly efficient and can work seamlessly with sample-to-class-based losses. The embedded loss (USS and sample-to-class Softmax loss) overcomes the pitfalls of previous approaches and the trained facial model UniTSFace exhibits exceptional performance, outperforming state-of-the-art methods, such as CosFace, ArcFace, VPL, AnchorFace, and UNPG. Our code is available.
</details>
<details>
<summary>摘要</summary>
“现有的面部识别模型（sample-to-class基本模型）无法充分利用大量的面部图像之间的交叉样本关系，而sample-to-sample基本模型则需要复杂的对应过程进行训练。此外，这两种方法都不能满足实际面部验证应用中的需求，需要一个统一的阈值来分辨正面和负面的面部对。在本篇文章中，我们提出了统一阈值结合sample-to-sample基本损失（USS损失），其中包含一个明确的统一阈值，用于分辨正面和负面的面部对。我们也从USS损失中 derivated sample-to-sample基本软max损失和BCE损失，并讨论它们之间的关系。我们对多个benchmark数据集，包括MFR、IJB-C、LFW、CFP-FP、AgeDB和MegaFace进行了广泛的评估，结果显示了我们的USS损失非常高效，并且可以与sample-to-class基本损失一起运作。我们的模型UniTSFace在训练时使用了USS损失和sample-to-class软max损失，并且表现出色，超越了现有的方法，如CosFace、ArcFace、VPL、AnchorFace和UNPG。我们的代码可以通过我们的网站下载。”
</details></li>
</ul>
<hr>
<h2 id="MAAIP-Multi-Agent-Adversarial-Interaction-Priors-for-imitation-from-fighting-demonstrations-for-physics-based-characters"><a href="#MAAIP-Multi-Agent-Adversarial-Interaction-Priors-for-imitation-from-fighting-demonstrations-for-physics-based-characters" class="headerlink" title="MAAIP: Multi-Agent Adversarial Interaction Priors for imitation from fighting demonstrations for physics-based characters"></a>MAAIP: Multi-Agent Adversarial Interaction Priors for imitation from fighting demonstrations for physics-based characters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02502">http://arxiv.org/abs/2311.02502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Younes, Ewa Kijak, Richard Kulpa, Simon Malinowski, Franck Multon</li>
<li>for: 这篇论文旨在提出一种基于多智能生成对抗学习的多个物理角色动作模拟方法，以满足互动应用和电影电视行业中的自动次要人物动作生成需求。</li>
<li>methods: 该方法基于多智能生成对抗学习，使用印杂学习技术来模拟多个物理角色的交互和动作。</li>
<li>results: 该方法在两种不同的拳击和全身武术风格下进行了测试，并成功地模拟了不同风格的交互和动作。<details>
<summary>Abstract</summary>
Simulating realistic interaction and motions for physics-based characters is of great interest for interactive applications, and automatic secondary character animation in the movie and video game industries. Recent works in reinforcement learning have proposed impressive results for single character simulation, especially the ones that use imitation learning based techniques. However, imitating multiple characters interactions and motions requires to also model their interactions. In this paper, we propose a novel Multi-Agent Generative Adversarial Imitation Learning based approach that generalizes the idea of motion imitation for one character to deal with both the interaction and the motions of the multiple physics-based characters. Two unstructured datasets are given as inputs: 1) a single-actor dataset containing motions of a single actor performing a set of motions linked to a specific application, and 2) an interaction dataset containing a few examples of interactions between multiple actors. Based on these datasets, our system trains control policies allowing each character to imitate the interactive skills associated with each actor, while preserving the intrinsic style. This approach has been tested on two different fighting styles, boxing and full-body martial art, to demonstrate the ability of the method to imitate different styles.
</details>
<details>
<summary>摘要</summary>
仿真人物的交互和动作是现代应用中很受欢迎的话题，特别是在电影和电子游戏行业中。最近的学习策略中，强调实现单个人物的仿真动作，尤其是使用仿制学习技术。然而，模拟多个人物之间的交互和动作需要同时模型他们之间的互动。在这篇论文中，我们提出了一种新的多智能体生成对抗学习仿真学习方法，扩展了单个人物的动作仿真到多个物理基于的人物之间的交互和动作。我们使用两个无结构数据集作为输入：1）一个单个演员数据集，包含一个演员执行一系列动作和应用相关的动作链接，和2）一个互动数据集，包含一些多个演员之间的互动示例。基于这两个数据集，我们的系统通过控制策略让每个人物学习与每个演员相互交互的技能，保持内在的风格。我们在拳击和全身武术两种不同的战斗风格中测试了这种方法，以示方法的多样性。
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Post-Wildfire-Vegetation-Recovery-in-California-using-a-Convolutional-Long-Short-Term-Memory-Tensor-Regression-Network"><a href="#Forecasting-Post-Wildfire-Vegetation-Recovery-in-California-using-a-Convolutional-Long-Short-Term-Memory-Tensor-Regression-Network" class="headerlink" title="Forecasting Post-Wildfire Vegetation Recovery in California using a Convolutional Long Short-Term Memory Tensor Regression Network"></a>Forecasting Post-Wildfire Vegetation Recovery in California using a Convolutional Long Short-Term Memory Tensor Regression Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02492">http://arxiv.org/abs/2311.02492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahe Liu, Xiaodi Wang</li>
<li>for: 这个研究旨在发展成功的生态系统恢复策略，帮助理解火灾后植被恢复的过程。</li>
<li>methods: 这个研究使用了一种新的方法，即将Convolutional Long Short-Term Memory Tensor Regression（ConvLSTMTR）网络应用于火灾后植被恢复的预测。</li>
<li>results: 研究结果表明，ConvLSTMTR网络可以准确预测火灾后植被恢复的速度，并且可以分类不同的恢复趋势。<details>
<summary>Abstract</summary>
The study of post-wildfire plant regrowth is essential for developing successful ecosystem recovery strategies. Prior research mainly examines key ecological and biogeographical factors influencing post-fire succession. This research proposes a novel approach for predicting and analyzing post-fire plant recovery. We develop a Convolutional Long Short-Term Memory Tensor Regression (ConvLSTMTR) network that predicts future Normalized Difference Vegetation Index (NDVI) based on short-term plant growth data after fire containment. The model is trained and tested on 104 major California wildfires occurring between 2013 and 2020, each with burn areas exceeding 3000 acres. The integration of ConvLSTM with tensor regression enables the calculation of an overall logistic growth rate k using predicted NDVI. Overall, our k-value predictions demonstrate impressive performance, with 50% of predictions exhibiting an absolute error of 0.12 or less, and 75% having an error of 0.24 or less. Finally, we employ Uniform Manifold Approximation and Projection (UMAP) and KNN clustering to identify recovery trends, offering insights into regions with varying rates of recovery. This study pioneers the combined use of tensor regression and ConvLSTM, and introduces the application of UMAP for clustering similar wildfires. This advances predictive ecological modeling and could inform future post-fire vegetation management strategies.
</details>
<details>
<summary>摘要</summary>
研究火灾后植物回复的学术研究非常重要，以发展成功的生态系统回复策略。先前的研究主要探讨火灾后的生态和生物地理因素的影响。这个研究提出了一种新的方法来预测和分析火灾后植物的回复。我们开发了一个卷积长短期记忆点 regression（ConvLSTMTR）网络，可以预测未来 Normalized Difference Vegetation Index（NDVI）基于火灾后植物增长数据。这个模型在104次加利福尼亚州大火灾中训练和测试，每次火灾面积超过3000英亩。通过卷积和tensor regression的结合，我们可以计算整体的几何增长率k。总的来说，我们的k值预测表现出色，50%的预测值几何准确性在0.12或更低，75%的预测值几何准确性在0.24或更低。最后，我们使用Uniform Manifold Approximation and Projection（UMAP）和KNN推敲来识别回复趋势，提供了不同回复速率的区域差异的见解。这项研究创新了tensor regression和ConvLSTM的结合，并首次应用UMAP来推敲相似的野火。这些进展可能对未来火灾后植物管理策略提供帮助。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-of-Deep-Learning-for-Spatiotemporal-Data-Challenges-and-Opportunities"><a href="#Uncertainty-Quantification-of-Deep-Learning-for-Spatiotemporal-Data-Challenges-and-Opportunities" class="headerlink" title="Uncertainty Quantification of Deep Learning for Spatiotemporal Data: Challenges and Opportunities"></a>Uncertainty Quantification of Deep Learning for Spatiotemporal Data: Challenges and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02485">http://arxiv.org/abs/2311.02485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenchong He, Zhe Jiang</li>
<li>for: 随着GPS、Remote Sensing和计算模拟技术的发展，大量的地ospatial和时间特征数据正在不断增加，这些数据资产提供了改变社会的Unique机遇。但是，深度学习模型在高度决策应用中可能会出现意外和错误的预测，导致严重的后果。不确定性评估（UQ）可以 estimating a deep learning model’s confidence.</li>
<li>methods: 本文提供了深度学习模型的不确定性评估简介，包括其特殊挑战和现有方法。我们尤其关注不确定性来源的重要性。</li>
<li>results: 本文 highlights several future research directions for spatiotemporal data, including the importance of uncertainty sources.Here is the same information in English:</li>
<li>for: With the advancement of GPS, remote sensing, and computational simulations, large amounts of geospatial and spatiotemporal data are being collected at an increasing speed, providing unique opportunities to transform society. However, deep learning models sometimes make unexpected and incorrect predictions with unwarranted confidence, causing severe consequences in high-stake decision-making applications. Uncertainty quantification (UQ) aims to estimate a deep learning model’s confidence.</li>
<li>methods: This paper provides a brief overview of UQ of deep learning for spatiotemporal data, including its unique challenges and existing methods. We particularly focus on the importance of uncertainty sources.</li>
<li>results: The paper highlights several future research directions for spatiotemporal data, including the importance of uncertainty sources.<details>
<summary>Abstract</summary>
With the advancement of GPS, remote sensing, and computational simulations, large amounts of geospatial and spatiotemporal data are being collected at an increasing speed. Such emerging spatiotemporal big data assets, together with the recent progress of deep learning technologies, provide unique opportunities to transform society. However, it is widely recognized that deep learning sometimes makes unexpected and incorrect predictions with unwarranted confidence, causing severe consequences in high-stake decision-making applications (e.g., disaster management, medical diagnosis, autonomous driving). Uncertainty quantification (UQ) aims to estimate a deep learning model's confidence. This paper provides a brief overview of UQ of deep learning for spatiotemporal data, including its unique challenges and existing methods. We particularly focus on the importance of uncertainty sources. We identify several future research directions for spatiotemporal data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:随着GPS、远程感知和计算 simulate的发展，大量的地ospatial和时空数据在不断增加。这些emerging spatiotemporal big data assets，加上深度学习技术的最新进步，为社会转型提供了唯一的机会。然而，广泛认可的深度学习 sometimes makes unexpected and incorrect predictions with unwarranted confidence，对高度决策应用（如灾害管理、医疗诊断、自动驾驶）可致严重的后果。 uncertainty quantification (UQ) aimsto estimate a deep learning model's confidence。这篇文章提供了深度学习 for spatiotemporal data的 UQ 简介，包括其特殊挑战和现有方法。我们尤其关注了 uncertainty sources 的重要性。我们标识了多个未来研究方向 for spatiotemporal data。
</details></li>
</ul>
<hr>
<h2 id="Generalized-zero-shot-audio-to-intent-classification"><a href="#Generalized-zero-shot-audio-to-intent-classification" class="headerlink" title="Generalized zero-shot audio-to-intent classification"></a>Generalized zero-shot audio-to-intent classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02482">http://arxiv.org/abs/2311.02482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Veera Raghavendra Elluru, Devang Kulshreshtha, Rohit Paturi, Sravan Bodapati, Srikanth Ronanki</li>
<li>for: 这个研究旨在提高使用音频数据的语音识别系统的未见意能力。</li>
<li>methods: 该研究提议一种通用零shot音频到意类型分类框架，只需要几个示例文本句子每个意图。这个框架首先通过使用一个自动生成的预训练模型来训练一个监督音频到意类型分类器。然后，我们使用神经网络音频生成器生成音频嵌入 для示例文本词汇，并使用普通的cosinus相似性来进行通用零shot分类。此外，我们还提出了一种多Modal训练策略，它将字幕信息 integrate到音频表示中以提高零shot性能。</li>
<li>results: 我们的多Modal训练策略提高了SLURP dataset上未见意分类的准确率，比Audio只训练策略高2.75%和18.2%。<details>
<summary>Abstract</summary>
Spoken language understanding systems using audio-only data are gaining popularity, yet their ability to handle unseen intents remains limited. In this study, we propose a generalized zero-shot audio-to-intent classification framework with only a few sample text sentences per intent. To achieve this, we first train a supervised audio-to-intent classifier by making use of a self-supervised pre-trained model. We then leverage a neural audio synthesizer to create audio embeddings for sample text utterances and perform generalized zero-shot classification on unseen intents using cosine similarity. We also propose a multimodal training strategy that incorporates lexical information into the audio representation to improve zero-shot performance. Our multimodal training approach improves the accuracy of zero-shot intent classification on unseen intents of SLURP by 2.75% and 18.2% for the SLURP and internal goal-oriented dialog datasets, respectively, compared to audio-only training.
</details>
<details>
<summary>摘要</summary>
spoken language understanding systems using audio-only data 获得 popularity，但它们对未经见意旨的处理能力仍然有限。在这种研究中，我们提议一种通用的零shot audio-to-intent分类框架，只需几个sample text sentences per intent。为实现这一点，我们首先使用一个自我超vised audio-to-intent分类器进行训练，然后利用一个神经网络音频生成器生成音频嵌入 дляsample text词汇，并使用cosine similarity进行通用零shot分类。我们还提出了一种多Modal训练策略，该策略将语言信息 incorporated into the audio representation，以提高零shot性能。我们的多Modal训练方法在SLURP上的零shot意旨分类精度提高2.75%和18.2%，相比 audio-only 训练。
</details></li>
</ul>
<hr>
<h2 id="Constrained-Equation-Learner-Networks-for-Precision-Preserving-Extrapolation-of-Robotic-Skills"><a href="#Constrained-Equation-Learner-Networks-for-Precision-Preserving-Extrapolation-of-Robotic-Skills" class="headerlink" title="Constrained Equation Learner Networks for Precision-Preserving Extrapolation of Robotic Skills"></a>Constrained Equation Learner Networks for Precision-Preserving Extrapolation of Robotic Skills</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02475">http://arxiv.org/abs/2311.02475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hector Perez-Villeda, Justus Piater, Matteo Saveriano</li>
<li>for: 该论文目的是解决在程序示示中学习 novel 技能后，如何适应不同的环境和条件，而不需要收集新的训练数据。</li>
<li>methods: 该论文提出了一种新的监督学习框架，即受限Equation Learner Networks（CEN），用于解决程序示示中的轨迹适应问题。CEN 使用 Equation Learner Networks 来学习一组 analytical 表达，并用这些表达作为基函数。</li>
<li>results: 实验结果表明，CEN 可以比现有方法更好地适应 robotic 技能，并且可以保持适应的精度。在一些 robotic 任务中，CEN 在不同的环境下实现了更高的总体性和适应性。<details>
<summary>Abstract</summary>
In Programming by Demonstration, the robot learns novel skills from human demonstrations. After learning, the robot should be able not only to reproduce the skill, but also to generalize it to shifted domains without collecting new training data. Adaptation to similar domains has been investigated in the literature; however, an open problem is how to adapt learned skills to different conditions that are outside of the data distribution, and, more important, how to preserve the precision of the desired adaptations. This paper presents a novel supervised learning framework called Constrained Equation Learner Networks that addresses the trajectory adaptation problem in Programming by Demonstrations from a constrained regression perspective. While conventional approaches for constrained regression use one kind of basis function, e.g., Gaussian, we exploit Equation Learner Networks to learn a set of analytical expressions and use them as basis functions. These basis functions are learned from demonstration with the objective to minimize deviations from the training data while imposing constraints that represent the desired adaptations, like new initial or final points or maintaining the trajectory within given bounds. Our approach addresses three main difficulties in adapting robotic trajectories: 1) minimizing the distortion of the trajectory for new adaptations; 2) preserving the precision of the adaptations; and 3) dealing with the lack of intuition about the structure of basis functions. We validate our approach both in simulation and in real experiments in a set of robotic tasks that require adaptation due to changes in the environment, and we compare obtained results with two existing approaches. Performed experiments show that Constrained Equation Learner Networks outperform state of the art approaches by increasing generalization and adaptability of robotic skills.
</details>
<details>
<summary>摘要</summary>
在程序编程中，机器人从人类示例学习新技能。学习后，机器人应该不仅能复制技能，还能泛化到偏移域无需新的训练数据。针对相似域的适应已经在文献中 investigate;然而，一个开放的问题是如何适应不同的条件，这些条件外部训练数据分布。此外，更重要的是如何保持适应的精度。这篇论文提出了一种新的监督学习框架，即受限 regression 框架，用于程序编程中的示例适应问题。与传统的受限 regression 方法一样，我们使用一种基于Equation Learner Networks的学习算法，以学习一组分析表达式，并使其作为基函数使用。这些基函数从示例学习中得出，并与具有限制的目标函数进行拟合，以最小化示例数据与适应结果之间的差异，同时保持适应的精度。我们的方法解决了以下三个主要难题：1）减少新适应中的路径扭曲;2）保持适应的精度;3）处理基函数的感知问题。我们在实验中 validate 了我们的方法，并与两种现有方法进行比较。实验结果表明，受限Equation Learner Networks 可以比现有方法提高机器人技能的泛化和适应能力。
</details></li>
</ul>
<hr>
<h2 id="Multi-State-Brain-Network-Discovery"><a href="#Multi-State-Brain-Network-Discovery" class="headerlink" title="Multi-State Brain Network Discovery"></a>Multi-State Brain Network Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02466">http://arxiv.org/abs/2311.02466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Hang Yin, Yao Su, Xinyue Liu, Thomas Hartvigsen, Yanhua Li, Xiangnan Kong</li>
<li>for: This paper aims to discover brain networks from spatio-temporal signals obtained by neuroimaging data, such as fMRI scans of human brains, and to model multi-state brain networks that capture the intricate patterns of brain activities.</li>
<li>methods: The proposed method, called MNGL (Multi-state Network Graphical Lasso), combines CGL (coherent graphical lasso) with GMM (Gaussian Mixture Model) to successfully model multi-state brain networks.</li>
<li>results: Compared to recent state-of-the-art alternatives, MNGL outperforms by discovering more explanatory and realistic results using both synthetic and real world ADHD 200 fMRI datasets.Here’s the Chinese translation of the three information:</li>
<li>for: 这篇论文旨在基于神经成像数据，如fMRI扫描人脑的信号，发现脑网络，并模型多状态脑网络，以捕捉脑活动的复杂征性。</li>
<li>methods: 提议的方法是MNGL（多状态网络图解吸积模型），它将CGL（协调图解吸积模型）与GMM（加性分布模型）结合，成功地模型多状态脑网络。</li>
<li>results: 与最新的状态艺术相比，MNGL在使用 sintetic和实际ADHD 200 fMRI数据上表现出色，可以更好地捕捉脑活动的特征。<details>
<summary>Abstract</summary>
Brain network discovery aims to find nodes and edges from the spatio-temporal signals obtained by neuroimaging data, such as fMRI scans of human brains. Existing methods tend to derive representative or average brain networks, assuming observed signals are generated by only a single brain activity state. However, the human brain usually involves multiple activity states, which jointly determine the brain activities. The brain regions and their connectivity usually exhibit intricate patterns that are difficult to capture with only a single-state network. Recent studies find that brain parcellation and connectivity change according to the brain activity state. We refer to such brain networks as multi-state, and this mixture can help us understand human behavior. Thus, compared to a single-state network, a multi-state network can prevent us from losing crucial information of cognitive brain network. To achieve this, we propose a new model called MNGL (Multi-state Network Graphical Lasso), which successfully models multi-state brain networks by combining CGL (coherent graphical lasso) with GMM (Gaussian Mixture Model). Using both synthetic and real world ADHD 200 fMRI datasets, we demonstrate that MNGL outperforms recent state-of-the-art alternatives by discovering more explanatory and realistic results.
</details>
<details>
<summary>摘要</summary>
��� Git network discovery aims to find nodes and edges from the spatio-temporal signals obtained by neuroimaging data, such as fMRI scans of human brains. Existing methods tend to derive representative or average brain networks, assuming observed signals are generated by only a single brain activity state. However, the human brain usually involves multiple activity states, which jointly determine the brain activities. The brain regions and their connectivity usually exhibit intricate patterns that are difficult to capture with only a single-state network. Recent studies find that brain parcellation and connectivity change according to the brain activity state. We refer to such brain networks as multi-state, and this mixture can help us understand human behavior. Thus, compared to a single-state network, a multi-state network can prevent us from losing crucial information of cognitive brain network. To achieve this, we propose a new model called MNGL (Multi-state Network Graphical Lasso), which successfully models multi-state brain networks by combining CGL (coherent graphical lasso) with GMM (Gaussian Mixture Model). Using both synthetic and real world ADHD 200 fMRI datasets, we demonstrate that MNGL outperforms recent state-of-the-art alternatives by discovering more explanatory and realistic results.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Levels-of-AGI-Operationalizing-Progress-on-the-Path-to-AGI"><a href="#Levels-of-AGI-Operationalizing-Progress-on-the-Path-to-AGI" class="headerlink" title="Levels of AGI: Operationalizing Progress on the Path to AGI"></a>Levels of AGI: Operationalizing Progress on the Path to AGI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02462">http://arxiv.org/abs/2311.02462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, Shane Legg</li>
<li>for: This paper proposes a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors.</li>
<li>methods: The paper analyzes existing definitions of AGI and distills six principles that a useful ontology for AGI should satisfy.</li>
<li>results: The paper proposes “Levels of AGI” based on depth (performance) and breadth (generality) of capabilities, and discusses the challenges of quantifying the behavior and capabilities of AGI models against these levels.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文提出了一个用于分类人工通用智能（AGI）模型和其前体的框架。</li>
<li>methods: 这篇论文通过分析现有的AGI定义，提出了 six 个用于AGIontology的原则。</li>
<li>results: 这篇论文提出了“Levels of AGI”，基于深度（性能）和面积（通用）的能力，并讨论了量化AGI模型的行为和能力的挑战。<details>
<summary>Abstract</summary>
We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy. It is our hope that this framework will be useful in an analogous way to the levels of autonomous driving, by providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. These principles include focusing on capabilities rather than mechanisms; separately evaluating generality and performance; and defining stages along the path toward AGI, rather than focusing on the endpoint. With these principles in mind, we propose 'Levels of AGI' based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging requirements for future benchmarks that quantify the behavior and capabilities of AGI models against these levels. Finally, we discuss how these levels of AGI interact with deployment considerations such as autonomy and risk, and emphasize the importance of carefully selecting Human-AI Interaction paradigms for responsible and safe deployment of highly capable AI systems.
</details>
<details>
<summary>摘要</summary>
我们提出了一套AGI模型和其前体的能力和行为分类框架。这套框架 introduce AGI性能、通用性和自主性的多个级别。我们希望这套框架可以与自动驾驶技术类似，提供一种共同语言，比较模型、评估风险和衡量AGI的进步。为开发这套框架，我们分析了现有的AGI定义，并总结出六个AGIontology应满足的原则。这些原则包括专注于能力而不是机制，分开评估通用性和性能，以及定义AGI的发展阶段，而不是专注于终点。基于这些原则，我们提出了“AGI级别”，按照性能和通用性的深度和面积来评估AGI模型的能力。我们还讨论了未来测试AGI模型的标准 benchmark，并讨论了这些级别与部署考虑因素，如自主和风险的关系。最后，我们强调选择合适的人机交互模式对于负责任和安全地部署高能力AI系统非常重要。
</details></li>
</ul>
<hr>
<h2 id="Can-ChatGPT-support-software-verification"><a href="#Can-ChatGPT-support-software-verification" class="headerlink" title="Can ChatGPT support software verification?"></a>Can ChatGPT support software verification?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02433">http://arxiv.org/abs/2311.02433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Janßen, Cedric Richter, Heike Wehrheim</li>
<li>for: 这个论文的目的是研究使用 chatGPT 支持正式软件验证。</li>
<li>methods: 论文使用 chatGPT 生成 loop invariants，并通过 Frama-C 和 CPAchecker 验证其有效性和实用性。</li>
<li>results: 论文的结果表明，chatGPT 可以生成有效和实用的 loop invariants，帮助 Frama-C 验证 tasks 之前无法解决。<details>
<summary>Abstract</summary>
Large language models have become increasingly effective in software engineering tasks such as code generation, debugging and repair. Language models like ChatGPT can not only generate code, but also explain its inner workings and in particular its correctness. This raises the question whether we can utilize ChatGPT to support formal software verification.   In this paper, we take some first steps towards answering this question. More specifically, we investigate whether ChatGPT can generate loop invariants. Loop invariant generation is a core task in software verification, and the generation of valid and useful invariants would likely help formal verifiers. To provide some first evidence on this hypothesis, we ask ChatGPT to annotate 106 C programs with loop invariants. We check validity and usefulness of the generated invariants by passing them to two verifiers, Frama-C and CPAchecker. Our evaluation shows that ChatGPT is able to produce valid and useful invariants allowing Frama-C to verify tasks that it could not solve before. Based on our initial insights, we propose ways of combining ChatGPT (or large language models in general) and software verifiers, and discuss current limitations and open issues.
</details>
<details>
<summary>摘要</summary>
To provide some initial evidence for this hypothesis, we ask ChatGPT to annotate 106 C programs with loop invariants. We then check the validity and usefulness of the generated invariants by passing them to two verifiers, Frama-C and CPAchecker. Our evaluation shows that ChatGPT is able to produce valid and useful invariants that allow Frama-C to verify tasks that it could not solve before. Based on our initial insights, we propose ways of combining ChatGPT (or large language models in general) and software verifiers, and discuss current limitations and open issues.
</details></li>
</ul>
<hr>
<h2 id="CDR-Adapter-Learning-Adapters-to-Dig-Out-More-Transferring-Ability-for-Cross-Domain-Recommendation-Models"><a href="#CDR-Adapter-Learning-Adapters-to-Dig-Out-More-Transferring-Ability-for-Cross-Domain-Recommendation-Models" class="headerlink" title="CDR-Adapter: Learning Adapters to Dig Out More Transferring Ability for Cross-Domain Recommendation Models"></a>CDR-Adapter: Learning Adapters to Dig Out More Transferring Ability for Cross-Domain Recommendation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02398">http://arxiv.org/abs/2311.02398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanyu Chen, Yao Yao, Wai Kin Victor Chan, Li Xiao, Kai Zhang, Liang Zhang, Yun Ye</li>
<li>for: 解决推荐系统中的数据稀缺和冷启动问题，提高推荐性能。</li>
<li>methods: 提出了一种可扩展和高效的解决方案，即CDR-Adapter，它通过分离原来的推荐模型和映射函数，实现知识传递而不需要重新引入网络结构，从而避免了计算成本高和知识淡化问题。</li>
<li>results: 在标准测试集上进行了广泛的实验，证明了我们的方法的有效性，比如 state-of-the-art CDR 方法。<details>
<summary>Abstract</summary>
Data sparsity and cold-start problems are persistent challenges in recommendation systems. Cross-domain recommendation (CDR) is a promising solution that utilizes knowledge from the source domain to improve the recommendation performance in the target domain. Previous CDR approaches have mainly followed the Embedding and Mapping (EMCDR) framework, which involves learning a mapping function to facilitate knowledge transfer. However, these approaches necessitate re-engineering and re-training the network structure to incorporate transferrable knowledge, which can be computationally expensive and may result in catastrophic forgetting of the original knowledge. In this paper, we present a scalable and efficient paradigm to address data sparsity and cold-start issues in CDR, named CDR-Adapter, by decoupling the original recommendation model from the mapping function, without requiring re-engineering the network structure. Specifically, CDR-Adapter is a novel plug-and-play module that employs adapter modules to align feature representations, allowing for flexible knowledge transfer across different domains and efficient fine-tuning with minimal training costs. We conducted extensive experiments on the benchmark dataset, which demonstrated the effectiveness of our approach over several state-of-the-art CDR approaches.
</details>
<details>
<summary>摘要</summary>
数据稀缺和冷启问题是推荐系统中的惯常挑战。跨Domain推荐（CDR）是一种有前途的解决方案，它利用源Domain中的知识提高目标Domain中的推荐性能。先前的CDR方法主要遵循Embedding and Mapping（EMCDR）框架，这些方法通常需要重新工程和重新训练网络结构，以便传递可移植的知识。然而，这些方法可能需要大量的计算资源和可能会导致原始知识的恐慌遗忘。在这篇论文中，我们提出了一种可扩展和高效的方法，以解决推荐系统中的数据稀缺和冷启问题，名为CDR-Adapter。CDR-Adapter是一种新的插件模块，它使用适配器模块来对特征表示进行减Alignment，以便在不同的Domain之间进行可靠的知识传递和高效的细化训练，无需重新工程网络结构。我们对标准数据集进行了广泛的实验，结果表明我们的方法比先前的CDR方法更有效。
</details></li>
</ul>
<hr>
<h2 id="Continual-Learning-of-Unsupervised-Monocular-Depth-from-Videos"><a href="#Continual-Learning-of-Unsupervised-Monocular-Depth-from-Videos" class="headerlink" title="Continual Learning of Unsupervised Monocular Depth from Videos"></a>Continual Learning of Unsupervised Monocular Depth from Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02393">http://arxiv.org/abs/2311.02393</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NeurAI-Lab/CUDE-MonoDepthCL">https://github.com/NeurAI-Lab/CUDE-MonoDepthCL</a></li>
<li>paper_authors: Hemang Chawla, Arnav Varma, Elahe Arani, Bahram Zonooz</li>
<li>for: 提高无监督单目深度估计的能力，应用于 робо扮和自动驾驶等领域。</li>
<li>methods: 提出了一个框架，以及一种基于备忘的双存储方法（MonoDepthCL），利用空间时间协调的一致性来实现不间断学习。</li>
<li>results: 模型在不同的频率和规模上进行了训练和测试，显示了在不间断学习中的性能稳定性和增长。<details>
<summary>Abstract</summary>
Spatial scene understanding, including monocular depth estimation, is an important problem in various applications, such as robotics and autonomous driving. While improvements in unsupervised monocular depth estimation have potentially allowed models to be trained on diverse crowdsourced videos, this remains underexplored as most methods utilize the standard training protocol, wherein the models are trained from scratch on all data after new data is collected. Instead, continual training of models on sequentially collected data would significantly reduce computational and memory costs. Nevertheless, naive continual training leads to catastrophic forgetting, where the model performance deteriorates on older domains as it learns on newer domains, highlighting the trade-off between model stability and plasticity. While several techniques have been proposed to address this issue in image classification, the high-dimensional and spatiotemporally correlated outputs of depth estimation make it a distinct challenge. To the best of our knowledge, no framework or method currently exists focusing on the problem of continual learning in depth estimation. Thus, we introduce a framework that captures the challenges of continual unsupervised depth estimation (CUDE), and define the necessary metrics to evaluate model performance. We propose a rehearsal-based dual-memory method, MonoDepthCL, which utilizes spatiotemporal consistency for continual learning in depth estimation, even when the camera intrinsics are unknown.
</details>
<details>
<summary>摘要</summary>
空间场景理解，包括单目深度估计，在各种应用中具有重要性，如 роботиcs和自动驾驶。尽管无监督单目深度估计的改进允许模型在不同的人工训练视频上进行训练，但这还是未经探索的，因为大多数方法使用标准训练协议，即从 scratch 上所有数据进行训练，新数据收集后。相反，继续训练模型在顺序收集的数据上会显著减少计算和内存成本。然而，简单的继续训练会导致忘记现象，模型对老化领域的性能下降，显示出模型稳定性和抑制的负面关系。虽然一些技术已经被提出来解决这个问题在图像分类方面，但高维度和空间时间相关的输出使得深度估计是一个特殊的挑战。根据我们所知，现在没有任何框架或方法专门关注深度估计的连续学习问题。因此，我们提出了一个框架，即不间断无监督深度估计框架（CUDE），并定义了评估模型性能的必要指标。我们提议一种备忘队列方法，即单目深度CL，它通过空间时间一致性来实现连续学习深度估计，即使摄像头内参不详。
</details></li>
</ul>
<hr>
<h2 id="Cross-Level-Distillation-and-Feature-Denoising-for-Cross-Domain-Few-Shot-Classification"><a href="#Cross-Level-Distillation-and-Feature-Denoising-for-Cross-Domain-Few-Shot-Classification" class="headerlink" title="Cross-Level Distillation and Feature Denoising for Cross-Domain Few-Shot Classification"></a>Cross-Level Distillation and Feature Denoising for Cross-Domain Few-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02392">http://arxiv.org/abs/2311.02392</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jarucezh/cldfd">https://github.com/jarucezh/cldfd</a></li>
<li>paper_authors: Hao Zheng, Runqi Wang, Jianzhuang Liu, Asako Kanezaki</li>
<li>for: 本研究目标是解决跨频道几拟分类问题，即在培育模型时使用不同频道的数据进行学习。</li>
<li>methods: 我们采用了跨层知识填充法，以强化模型在目标数据集中提取更多特征信息。此外，我们还提出了一种特征净化操作，以降低特征重复和避免过拟合。</li>
<li>results: 我们的方法可以在BSCD-FSL benchmark中超越前任的Dynamic-Distillation方法，在1-shot和5-shot分类任务上的平均提高5.44%和1.37%。代码将在GitHub上提供。<details>
<summary>Abstract</summary>
The conventional few-shot classification aims at learning a model on a large labeled base dataset and rapidly adapting to a target dataset that is from the same distribution as the base dataset. However, in practice, the base and the target datasets of few-shot classification are usually from different domains, which is the problem of cross-domain few-shot classification. We tackle this problem by making a small proportion of unlabeled images in the target domain accessible in the training stage. In this setup, even though the base data are sufficient and labeled, the large domain shift still makes transferring the knowledge from the base dataset difficult. We meticulously design a cross-level knowledge distillation method, which can strengthen the ability of the model to extract more discriminative features in the target dataset by guiding the network's shallow layers to learn higher-level information. Furthermore, in order to alleviate the overfitting in the evaluation stage, we propose a feature denoising operation which can reduce the feature redundancy and mitigate overfitting. Our approach can surpass the previous state-of-the-art method, Dynamic-Distillation, by 5.44% on 1-shot and 1.37% on 5-shot classification tasks on average in the BSCD-FSL benchmark. The implementation code will be available at https://github.com/jarucezh/cldfd.
</details>
<details>
<summary>摘要</summary>
传统的几shot分类目标是学习一个模型，然后快速适应目标数据集，但在实际应用中，基数据集和目标数据集通常来自不同的领域，这是跨领域几shot分类的问题。我们解决这个问题 by 在训练阶段使得小量目标数据集中的无标照图像变得可访问。尽管基数据集充足并彩色标注，但大领域变化仍然使得从基数据集传输知识困难。我们仔细设计了跨层知识填充方法，该方法可以使模型在目标数据集中提取更多特征分类器。此外，为了避免评估阶段的过拟合，我们提议一种特征净化操作，可以减少特征重复和抑制过拟合。我们的方法可以在BSCD-FSL标准准则下平均超过前一个状态的方法，Dynamic-Distillation，在1shot和5shot分类任务上的平均性能提高5.44%和1.37%。代码实现将提供在https://github.com/jarucezh/cldfd中。
</details></li>
</ul>
<hr>
<h2 id="AI-based-Self-healing-Solutions-Applied-to-Cellular-Networks-An-Overview"><a href="#AI-based-Self-healing-Solutions-Applied-to-Cellular-Networks-An-Overview" class="headerlink" title="AI-based Self-healing Solutions Applied to Cellular Networks: An Overview"></a>AI-based Self-healing Solutions Applied to Cellular Networks: An Overview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02390">http://arxiv.org/abs/2311.02390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaleh Farmani, Amirreza Khalil Zadeh</li>
<li>For: The paper is written for researchers and practitioners in the field of cellular networks, specifically those interested in self-healing and machine learning techniques for network management.* Methods: The paper provides an overview of machine learning methods, including classical and deep learning variants, that are used to implement self-healing for cell outages in cellular networks.* Results: The paper reviews the state-of-the-art in literature for cell outages, with a particular emphasis on machine learning-based approaches.Here are the three key points in Simplified Chinese text:* For: 本文是为Cellular网络相关研究人员和实践者所写，尤其是关注自适应和机器学习技术的网络管理方面。* Methods: 本文提供了机器学习方法的概述，包括经典和深度学习变体，用于实现Cellular网络中的自适应。* Results: 本文对Cellular网络中的维护和自适应方面进行了文献综述，尤其是关注机器学习基于的方法。<details>
<summary>Abstract</summary>
In this article, we provide an overview of machine learning (ML) methods, both classical and deep variants, that are used to implement self-healing for cell outages in cellular networks. Self-healing is a promising approach to network management, which aims to detect and compensate for cell outages in an autonomous way. This technology aims to decrease the expenses associated with the installation and maintenance of existing 4G and 5G, i.e. emerging 6G networks by simplifying operational tasks through its ability to heal itself. We provide an overview of the basic concepts and taxonomy for SON, self-healing, and ML techniques, in network management. Moreover, we review the state-of-the-art in literature for cell outages, with a particular emphasis on ML-based approaches.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们提供了机器学习（ML）方法的概述，包括古典和深度变种，用于实现cell网络中的自适应维护。自适应维护是一种有前途的网络管理方法，旨在通过自动化方式探测和补做cell网络中的维护问题。这技术可以降低现有4G和5G等新生6G网络的安装和维护成本，通过简化操作任务来简化操作任务。我们还提供了网络管理基本概念和分类，以及相关文献综述。特别是，我们对文献中关于cell网络维护的研究进行了深入审查，强调了基于ML的方法。
</details></li>
</ul>
<hr>
<h2 id="Ultra-Long-Sequence-Distributed-Transformer"><a href="#Ultra-Long-Sequence-Distributed-Transformer" class="headerlink" title="Ultra-Long Sequence Distributed Transformer"></a>Ultra-Long Sequence Distributed Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02382">http://arxiv.org/abs/2311.02382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Wang, Isaac Lyngaas, Aristeidis Tsaris, Peng Chen, Sajal Dash, Mayanka Chandra Shekar, Tao Luo, Hong-Jun Yoon, Mohamed Wahib, John Gouley</li>
<li>for: 该论文旨在提出一种高效的分布式训练方法，以便使用长序列训练变换器模型。</li>
<li>methods: 该方法分割长序列成多个段，并将每个段分配给不同的GPU进行计算。然后，它使用了一种复合通信和双均值平均技术来避免部分自注意计算的汇聚和通信开销。</li>
<li>results: 与现有的序列并行技术相比，该方法在144个Nvidia V100 GPU上实现了5.6倍的速度提升和10.2倍的内存可用性提升。此外，该算法可以扩展到极长序列长度50,112，在3,456个GPU上实现161%的超线性并行效率和32PFLOP的吞吐量。<details>
<summary>Abstract</summary>
Transformer models trained on long sequences often achieve higher accuracy than short sequences. Unfortunately, conventional transformers struggle with long sequence training due to the overwhelming computation and memory requirements. Existing methods for long sequence training offer limited speedup and memory reduction, and may compromise accuracy. This paper presents a novel and efficient distributed training method, the Long Short-Sequence Transformer (LSS Transformer), for training transformer with long sequences. It distributes a long sequence into segments among GPUs, with each GPU computing a partial self-attention for its segment. Then, it uses a fused communication and a novel double gradient averaging technique to avoid the need to aggregate partial self-attention and minimize communication overhead. We evaluated the performance between LSS Transformer and the state-of-the-art Nvidia sequence parallelism on a Wikipedia enwik8 dataset. Results show that our proposed method lead to 5.6x faster and 10.2x more memory-efficient implementation compared to state-of-the-art sequence parallelism on 144 Nvidia V100 GPUs. Moreover, our algorithm scales to an extreme sequence length of 50,112 at 3,456 GPUs, achieving 161% super-linear parallel efficiency and a throughput of 32 petaflops.
</details>
<details>
<summary>摘要</summary>
很多变换器模型在长序列上训练时会达到更高的准确率。然而，普通的变换器在长序列训练时会遇到过于复杂的计算和内存需求的问题。现有的长序列训练方法可能会减少速度和内存占用，并可能会降低准确率。这篇论文提出了一种新的和高效的分布式训练方法——长短序列变换器（LSS Transformer），用于训练变换器模型。它将长序列分成多个 GPU 上的段，每个 GPU 计算自己的段部分自注意。然后，它使用了一种混合通信和一种新的双 Gradient 平均技术，以避免需要合并部分自注意和减少通信开销。我们对 LSS Transformer 和状态对照短序列并行方法进行了对比，使用 Wikipedia enwik8 数据集。结果显示，我们提出的方法比状态对照短序列并行方法在 144 Nvidia V100 GPU 上得到了5.6倍快速和10.2倍内存高效的实现。此外，我们的算法可以在极长序列长度为50,112的情况下，在3,456 GPU 上进行扩展，实现161%的超线性并行率和32 petaflops 的吞吐量。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Reinforcement-Learning-of-Robotic-Manipulations-via-Feedback-from-Large-Language-Models"><a href="#Accelerating-Reinforcement-Learning-of-Robotic-Manipulations-via-Feedback-from-Large-Language-Models" class="headerlink" title="Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models"></a>Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02379">http://arxiv.org/abs/2311.02379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Chu, Xufeng Zhao, Cornelius Weber, Mengdi Li, Stefan Wermter</li>
<li>for: 提高RLAgent的学习效率和成功率，通过人工智能语言模型提供有用的反馈。</li>
<li>methods: 利用大量语言数据预训练的大语言模型（LLM），提供RLAgent有用的反馈，帮助RLAgent更快速地学习和成功完成 робо控制任务。</li>
<li>results: 实验结果表明，使用Lafite-RL框架，RLAgent可以更快速地学习并成功完成RLBench任务，并且在学习效率和成功率上都有显著提高。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) plays an important role in the robotic manipulation domain since it allows self-learning from trial-and-error interactions with the environment. Still, sample efficiency and reward specification seriously limit its potential. One possible solution involves learning from expert guidance. However, obtaining a human expert is impractical due to the high cost of supervising an RL agent, and developing an automatic supervisor is a challenging endeavor. Large Language Models (LLMs) demonstrate remarkable abilities to provide human-like feedback on user inputs in natural language. Nevertheless, they are not designed to directly control low-level robotic motions, as their pretraining is based on vast internet data rather than specific robotics data. In this paper, we introduce the Lafite-RL (Language agent feedback interactive Reinforcement Learning) framework, which enables RL agents to learn robotic tasks efficiently by taking advantage of LLMs' timely feedback. Our experiments conducted on RLBench tasks illustrate that, with simple prompt design in natural language, the Lafite-RL agent exhibits improved learning capabilities when guided by an LLM. It outperforms the baseline in terms of both learning efficiency and success rate, underscoring the efficacy of the rewards provided by an LLM.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MTS-DVGAN-Anomaly-Detection-in-Cyber-Physical-Systems-using-a-Dual-Variational-Generative-Adversarial-Network"><a href="#MTS-DVGAN-Anomaly-Detection-in-Cyber-Physical-Systems-using-a-Dual-Variational-Generative-Adversarial-Network" class="headerlink" title="MTS-DVGAN: Anomaly Detection in Cyber-Physical Systems using a Dual Variational Generative Adversarial Network"></a>MTS-DVGAN: Anomaly Detection in Cyber-Physical Systems using a Dual Variational Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02378">http://arxiv.org/abs/2311.02378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haili Sun, Yan Huang, Lansheng Han, Cai Fu, Hongle Liu, Xiang Long</li>
<li>for: 这篇论文旨在应用深度生成模型来探测 Cyber-physical systems (CPSs) 中的新型攻击，并无需靠扩展标签信息。</li>
<li>methods: 这篇论文提出了一个名为 MST-DVGAN 的新型Unsupervised dual variational generative adversarial model，用于侦测 multivariate time series data 中的异常情况。</li>
<li>results:  compared with state-of-the-art methods, the proposed MTS-DVGAN is more stable and can achieve consistent performance improvement in detecting anomalies in CPSs.<details>
<summary>Abstract</summary>
Deep generative models are promising in detecting novel cyber-physical attacks, mitigating the vulnerability of Cyber-physical systems (CPSs) without relying on labeled information. Nonetheless, these generative models face challenges in identifying attack behaviors that closely resemble normal data, or deviate from the normal data distribution but are in close proximity to the manifold of the normal cluster in latent space. To tackle this problem, this article proposes a novel unsupervised dual variational generative adversarial model named MST-DVGAN, to perform anomaly detection in multivariate time series data for CPS security. The central concept is to enhance the model's discriminative capability by widening the distinction between reconstructed abnormal samples and their normal counterparts. Specifically, we propose an augmented module by imposing contrastive constraints on the reconstruction process to obtain a more compact embedding. Then, by exploiting the distribution property and modeling the normal patterns of multivariate time series, a variational autoencoder is introduced to force the generative adversarial network (GAN) to generate diverse samples. Furthermore, two augmented loss functions are designed to extract essential characteristics in a self-supervised manner through mutual guidance between the augmented samples and original samples. Finally, a specific feature center loss is introduced for the generator network to enhance its stability. Empirical experiments are conducted on three public datasets, namely SWAT, WADI and NSL_KDD. Comparing with the state-of-the-art methods, the evaluation results show that the proposed MTS-DVGAN is more stable and can achieve consistent performance improvement.
</details>
<details>
<summary>摘要</summary>
深度生成模型可以有效探测 novel 的Cyber-physical attacks，减轻 Cyber-physical systems (CPSs) 的感受性，不需要靠据标注信息。然而，这些生成模型面临着难以识别攻击行为，与正常数据 Distribution 相似或者在正常数据分布的邻近区域内偏离。为解决这个问题，本文提出了一种新的无监督 dual variational generative adversarial model（MST-DVGAN），用于Cyber-physical systems (CPSs) 安全中的异常检测。中心思想是通过增强模型的歧义能力，使重构后的异常样本与正常样本更加明显不同。具体来说，我们提出了一个增强模块，通过对重构过程进行冲突约束，以获得更加紧凑的嵌入。然后，通过利用多变量时序数据的分布特性和模型正常模式，引入了一种变量自动编码器，以使生成对抗网络（GAN）生成更加多样化的样本。此外，我们还设计了两种增强loss函数，通过自然指导两者之间的增强样本和原始样本之间的互动，从而提取出更加重要的特征。最后，我们引入了一种特定的特征中心损失，以提高生成器网络的稳定性。我们在三个公共数据集上进行了实验，即SWAT、WADI和NSL_KDD。与现状的方法相比，我们的MST-DVGAN表现更加稳定，并且可以在各种场景下提供更高的性能改进。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Deep-Nonnegative-Matrix-Factorization-for-Community-Detection"><a href="#Contrastive-Deep-Nonnegative-Matrix-Factorization-for-Community-Detection" class="headerlink" title="Contrastive Deep Nonnegative Matrix Factorization for Community Detection"></a>Contrastive Deep Nonnegative Matrix Factorization for Community Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02357">http://arxiv.org/abs/2311.02357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuecheng Li, Jialong Chen, Chuan Chen, Lei Yang, Zibin Zheng</li>
<li>for: 本研究旨在提出一种新的社区探测算法，以解决现有的非正式矩阵分解（NMF）基于方法的三大问题：1）它们直接将原始网络转换为社区会员空间，难以捕捉层次结构信息；2）它们通常仅关注网络的拓扑结构，忽略节点特征；3）它们难以学习社区探测所需的全局结构信息。</li>
<li>methods: 我们提出了一种新的社区探测算法，名为对比深度非负矩阵分解（CDNMF）。我们首先深化NMF，以增强其信息提取能力。然后，我们受到对比学习的启发，把网络拓扑结构和节点特征作为两个对比视图构建。此外，我们使用了一个减噪负样本层，以提高模型的社区同义性。</li>
<li>results: 我们在三个公共实验 graphs 上进行了实验，并证明了 CDNMF 模型在社区探测方面的优异性。与现有方法相比，CDNMF 模型在社区内部的节点相似性学习和全局结构信息捕捉方面具有优势。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/6lyc/CDNMF.git">https://github.com/6lyc/CDNMF.git</a> 中找到。<details>
<summary>Abstract</summary>
Recently, nonnegative matrix factorization (NMF) has been widely adopted for community detection, because of its better interpretability. However, the existing NMF-based methods have the following three problems: 1) they directly transform the original network into community membership space, so it is difficult for them to capture the hierarchical information; 2) they often only pay attention to the topology of the network and ignore its node attributes; 3) it is hard for them to learn the global structure information necessary for community detection. Therefore, we propose a new community detection algorithm, named Contrastive Deep Nonnegative Matrix Factorization (CDNMF). Firstly, we deepen NMF to strengthen its capacity for information extraction. Subsequently, inspired by contrastive learning, our algorithm creatively constructs network topology and node attributes as two contrasting views. Furthermore, we utilize a debiased negative sampling layer and learn node similarity at the community level, thereby enhancing the suitability of our model for community detection. We conduct experiments on three public real graph datasets and the proposed model has achieved better results than state-of-the-art methods. Code available at https://github.com/6lyc/CDNMF.git.
</details>
<details>
<summary>摘要</summary>
现在，非正式矩阵分解（NMF）已经广泛应用于社群检测，因为它的更好的解释性。然而，现有的NMF基于方法有以下三个问题：1）它们直接将原始网络转换成社群成员空间，因此很难捕捉层次信息; 2）它们通常只关注网络的结构，忽略节点特征; 3）它们难以学习必要的全局结构信息 для社群检测。因此，我们提出了一个新的社群检测算法，名为对比深度非正式矩阵分解（CDNMF）。首先，我们深入了NMF，以增强其信息提取能力。然后，我们受到对比学习的启发，创新地将网络结构和节点特征作为两个对比视图。此外，我们使用了一层恢复降低的负样本层，并在社群层上学习节点相似性，从而提高了我们模型的适应性。我们在三个公共实验Graph数据集上进行了实验，并取得了与当前最佳方法的更好的结果。代码可以在https://github.com/6lyc/CDNMF.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Perturbation-based-Active-Learning-for-Question-Answering"><a href="#Perturbation-based-Active-Learning-for-Question-Answering" class="headerlink" title="Perturbation-based Active Learning for Question Answering"></a>Perturbation-based Active Learning for Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02345">http://arxiv.org/abs/2311.02345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Luo, Mihai Surdeanu</li>
<li>for: 建立一个问答模型，可以降低标注成本，通过使用活动学习（AL）训练策略。</li>
<li>methods: 使用活动学习的采样策略，选择最有用的无标示训练数据，以更新模型。</li>
<li>results: 提出了一种扰动基于采样策略，与常用的采样策略相比，更有效率。<details>
<summary>Abstract</summary>
Building a question answering (QA) model with less annotation costs can be achieved by utilizing active learning (AL) training strategy. It selects the most informative unlabeled training data to update the model effectively. Acquisition functions for AL are used to determine how informative each training example is, such as uncertainty or diversity based sampling. In this work, we propose a perturbation-based active learning acquisition strategy and demonstrate it is more effective than existing commonly used strategies.
</details>
<details>
<summary>摘要</summary>
使用活动学习（AL）训练策略可以降低问答（QA）模型标注成本。它选择最有用的未标注训练数据来更新模型，以达到更高的效果。获取函数用于AL来确定每个训练示例的有用程度，如不确定性或多样性基本采样。在这项工作中，我们提议使用扰动基于的活动学习获取策略，并证明它比现有的通用使用策略更有效。
</details></li>
</ul>
<hr>
<h2 id="You-Only-Forward-Once-Prediction-and-Rationalization-in-A-Single-Forward-Pass"><a href="#You-Only-Forward-Once-Prediction-and-Rationalization-in-A-Single-Forward-Pass" class="headerlink" title="You Only Forward Once: Prediction and Rationalization in A Single Forward Pass"></a>You Only Forward Once: Prediction and Rationalization in A Single Forward Pass</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02344">http://arxiv.org/abs/2311.02344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Jiang, Junwen Duan, Zhe Qu, Jianxin Wang</li>
<li>for: 本研究旨在提高无监督逻辑抽象的精度和效果，使模型预测时可以快速提取有用的信息。</li>
<li>methods: 本研究使用了一种新的单阶段框架，即You Only Forward Once（YOFO）框架，其中使用了一个预训练的语言模型如BERT进行预测和分析。</li>
<li>results: 实验结果显示，YOFO模型可以比前一代RNP模型更加准确地预测和提取有用的逻辑抽象。对比于前一代方法，YOFO模型可以提高token级F1得分达18.4%。此外，研究还发现YOFO模型可以快速提取有用的逻辑抽象，并且可以在模型中移除不重要的token。<details>
<summary>Abstract</summary>
Unsupervised rationale extraction aims to extract concise and contiguous text snippets to support model predictions without any annotated rationale. Previous studies have used a two-phase framework known as the Rationalizing Neural Prediction (RNP) framework, which follows a generate-then-predict paradigm. They assumed that the extracted explanation, called rationale, should be sufficient to predict the golden label. However, the assumption above deviates from the original definition and is too strict to perform well. Furthermore, these two-phase models suffer from the interlocking problem and spurious correlations. To solve the above problems, we propose a novel single-phase framework called You Only Forward Once (YOFO), derived from a relaxed version of rationale where rationales aim to support model predictions rather than make predictions. In our framework, A pre-trained language model like BERT is deployed to simultaneously perform prediction and rationalization with less impact from interlocking or spurious correlations. Directly choosing the important tokens in an unsupervised manner is intractable. Instead of directly choosing the important tokens, YOFO gradually removes unimportant tokens during forward propagation. Through experiments on the BeerAdvocate and Hotel Review datasets, we demonstrate that our model is able to extract rationales and make predictions more accurately compared to RNP-based models. We observe an improvement of up to 18.4\% in token-level F1 compared to previous state-of-the-art methods. We also conducted analyses and experiments to explore the extracted rationales and token decay strategies. The results show that YOFO can extract precise and important rationales while removing unimportant tokens in the middle part of the model.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:Unsupervised rationale extraction aims to extract concise and contiguous text snippets to support model predictions without any annotated rationale. Previous studies have used a two-phase framework known as the Rationalizing Neural Prediction (RNP) framework, which follows a generate-then-predict paradigm. They assumed that the extracted explanation, called rationale, should be sufficient to predict the golden label. However, the assumption above deviates from the original definition and is too strict to perform well. Furthermore, these two-phase models suffer from the interlocking problem and spurious correlations. To solve the above problems, we propose a novel single-phase framework called You Only Forward Once (YOFO), derived from a relaxed version of rationale where rationales aim to support model predictions rather than make predictions. In our framework, A pre-trained language model like BERT is deployed to simultaneously perform prediction and rationalization with less impact from interlocking or spurious correlations. Directly choosing the important tokens in an unsupervised manner is intractable. Instead of directly choosing the important tokens, YOFO gradually removes unimportant tokens during forward propagation. Through experiments on the BeerAdvocate and Hotel Review datasets, we demonstrate that our model is able to extract rationales and make predictions more accurately compared to RNP-based models. We observe an improvement of up to 18.4\% in token-level F1 compared to previous state-of-the-art methods. We also conducted analyses and experiments to explore the extracted rationales and token decay strategies. The results show that YOFO can extract precise and important rationales while removing unimportant tokens in the middle part of the model.Translation:无监督的理由抽取目标在支持模型预测而不需要任何标注的理由。先前的研究使用了一个两个阶段框架，称为神经网络预测合理化（RNP）框架，该框架采用生成Then预测模式。它们假设提取的解释，即理由，应该足够预测金 Label。然而，上述假设与原始定义偏离，并且太严格来不能perform well。此外，这些两个阶段模型受到了交叠问题和偶极相关性的影响。为解决以上问题，我们提出了一种单阶段框架，称为你只能前进一次（YOFO），该框架基于放松的理由定义，其中理由的目的是支持模型预测而不是预测。在我们的框架中，使用预训练的自然语言模型，如BERT，同时进行预测和合理化，以减少交叠或偶极相关性的影响。直接从无监督中选择重要的字符是不可能。相反，YOFO在前进传播过程中逐渐移除无关重要的字符。通过对BeerAdvocate和酒店评论数据集进行实验，我们证明了我们的模型可以更加准确地提取理由和预测。我们观察到的提取改善率可达18.4%。我们还进行了分析和实验，以探索提取的理由和字符衰减策略。结果表明，YOFO可以提取精确和重要的理由，并在中部模型中移除无关重要的字符。
</details></li>
</ul>
<hr>
<h2 id="Stable-Diffusion-Reference-Only-Image-Prompt-and-Blueprint-Jointly-Guided-Multi-Condition-Diffusion-Model-for-Secondary-Painting"><a href="#Stable-Diffusion-Reference-Only-Image-Prompt-and-Blueprint-Jointly-Guided-Multi-Condition-Diffusion-Model-for-Secondary-Painting" class="headerlink" title="Stable Diffusion Reference Only: Image Prompt and Blueprint Jointly Guided Multi-Condition Diffusion Model for Secondary Painting"></a>Stable Diffusion Reference Only: Image Prompt and Blueprint Jointly Guided Multi-Condition Diffusion Model for Secondary Painting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02343">http://arxiv.org/abs/2311.02343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Ai, Lu Sheng</li>
<li>for: 这篇论文旨在提高二次绘制的效率，特别是在漫画、动画等艺术创作领域。</li>
<li>methods: 这篇论文提出了一新的自动化图像生成方法，仅使用两种条件图像进行精确控制生成，以减少对ControlNet的需求。</li>
<li>results: 这篇论文的实验结果显示，这新的方法可以实现高效的二次绘制，并且可以实现高品质的图像生成。<details>
<summary>Abstract</summary>
Stable Diffusion and ControlNet have achieved excellent results in the field of image generation and synthesis. However, due to the granularity and method of its control, the efficiency improvement is limited for professional artistic creations such as comics and animation production whose main work is secondary painting. In the current workflow, fixing characters and image styles often need lengthy text prompts, and even requires further training through TextualInversion, DreamBooth or other methods, which is very complicated and expensive for painters. Therefore, we present a new method in this paper, Stable Diffusion Reference Only, a images-to-image self-supervised model that uses only two types of conditional images for precise control generation to accelerate secondary painting. The first type of conditional image serves as an image prompt, supplying the necessary conceptual and color information for generation. The second type is blueprint image, which controls the visual structure of the generated image. It is natively embedded into the original UNet, eliminating the need for ControlNet. We released all the code for the module and pipeline, and trained a controllable character line art coloring model at https://github.com/aihao2000/stable-diffusion-reference-only, that achieved state-of-the-art results in this field. This verifies the effectiveness of the structure and greatly improves the production efficiency of animations, comics, and fanworks.
</details>
<details>
<summary>摘要</summary>
stable diffusion和controlnet在图像生成和合成领域具有优秀的成绩，但由于它的粒度和控制方法，对专业艺术创作如漫画和动画制作而言，效率提升的限制很大。现在的工作流程中，fixing人物和图像风格经常需要长时间的文本提示，甚至需要通过文本反向、梦幻箱等方法进行进一步的训练，这对画家来说是非常复杂和昂贵的。因此，我们在这篇论文中提出了一新的方法：stable diffusion reference only，这是一种图像到图像的自我超vis的模型，只需要两种类型的条件图像来进行精确的控制生成，以加速secondary painting。第一种类型的条件图像 acted as an image prompt，提供了必要的概念和颜色信息 для生成。第二种类型的条件图像是蓝图图像，它控制了生成图像的视觉结构，并且Native embedding在原始UNet中，消除了需要控制网的需求。我们已经发布了模块和管道的代码，并在https://github.com/aihao2000/stable-diffusion-reference-only上训练了一个可控色彩插画模型，达到了这个领域的州前成绩。这证明了结构的有效性，对动画、漫画和粉丝创作的生产效率进行了很大的提升。
</details></li>
</ul>
<hr>
<h2 id="Potato-Leaf-Disease-Classification-using-Deep-Learning-A-Convolutional-Neural-Network-Approach"><a href="#Potato-Leaf-Disease-Classification-using-Deep-Learning-A-Convolutional-Neural-Network-Approach" class="headerlink" title="Potato Leaf Disease Classification using Deep Learning: A Convolutional Neural Network Approach"></a>Potato Leaf Disease Classification using Deep Learning: A Convolutional Neural Network Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02338">http://arxiv.org/abs/2311.02338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Utkarsh Yashwant Tambe, A. Shobanadevi, A. Shanthini, Hsiu-Chun Hsu</li>
<li>for: 本研究使用深度学习（Deep Learning）来分类芋头叶病。</li>
<li>methods: 提议的方法包括对叶图像数据进行预处理，使用深度学习模型训练，并对测试集进行评估。</li>
<li>results: 实验结果显示，使用深度学习模型，全面准确率达99.1%，可高度准确地识别芋头叶病两种，包括早期虫疫和晚期虫疫，以及健康叶片。这种方法可能为芋头农业中疾病识别提供可靠和有效的解决方案，帮助保持食品安全和避免农业损失。<details>
<summary>Abstract</summary>
In this study, a Convolutional Neural Network (CNN) is used to classify potato leaf illnesses using Deep Learning. The suggested approach entails preprocessing the leaf image data, training a CNN model on that data, and assessing the model's success on a test set. The experimental findings show that the CNN model, with an overall accuracy of 99.1%, is highly accurate in identifying two kinds of potato leaf diseases, including Early Blight, Late Blight, and Healthy. The suggested method may offer a trustworthy and effective remedy for identifying potato diseases, which is essential for maintaining food security and minimizing financial losses in agriculture. The model can accurately recognize the various disease types even when there are severe infections present. This work highlights the potential of deep learning methods for categorizing potato diseases, which can help with effective and automated disease management in potato farming.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们使用了卷积神经网络（CNN）来分类芋头叶病。我们建议的方法包括对叶图像数据进行预处理，使用该数据训练CNN模型，并对测试集进行评估。实验结果表明，我们的CNN模型，准确率为99.1%，高度准确地识别了两种芋头叶病，包括早期病和晚期病，以及健康的叶片。我们建议的方法可能为芋头农业中维护食品安全和减少经济损失提供一个可靠和有效的解决方案。这种方法可以准确地识别不同的病种，即使有严重的感染存在。这项研究表明了深度学习方法在芋头病类分类中的潜力，这可能会帮助实现自动化和有效的芋头病管理。
</details></li>
</ul>
<hr>
<h2 id="STOW-Discrete-Frame-Segmentation-and-Tracking-of-Unseen-Objects-for-Warehouse-Picking-Robots"><a href="#STOW-Discrete-Frame-Segmentation-and-Tracking-of-Unseen-Objects-for-Warehouse-Picking-Robots" class="headerlink" title="STOW: Discrete-Frame Segmentation and Tracking of Unseen Objects for Warehouse Picking Robots"></a>STOW: Discrete-Frame Segmentation and Tracking of Unseen Objects for Warehouse Picking Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02337">http://arxiv.org/abs/2311.02337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Li, Muru Zhang, Markus Grotz, Kaichun Mo, Dieter Fox</li>
<li>for: 这篇论文主要关注在dynamic industrial robotic contexts和domestic robotic applications中，对于物品的重新排序、移除和部分遮蔽等操作，以及在长时间内实现物品追踪的任务。</li>
<li>methods: 本文提出了一个新的合成和真实世界数据集，以及一个基于transformer模组的联合分割和追踪方法，以解决这些具有挑战性的任务。</li>
<li>results: 本文的实验结果显示，该方法与最近的方法相比，有着优秀的性能。另外，请参考官方网站(\href{<a target="_blank" rel="noopener" href="https://sites.google.com/view/stow-corl23%7D%7Bwebsite%7D">https://sites.google.com/view/stow-corl23}{website}</a>) для更多的结果和视频。<details>
<summary>Abstract</summary>
Segmentation and tracking of unseen object instances in discrete frames pose a significant challenge in dynamic industrial robotic contexts, such as distribution warehouses. Here, robots must handle object rearrangement, including shifting, removal, and partial occlusion by new items, and track these items after substantial temporal gaps. The task is further complicated when robots encounter objects not learned in their training sets, which requires the ability to segment and track previously unseen items. Considering that continuous observation is often inaccessible in such settings, our task involves working with a discrete set of frames separated by indefinite periods during which substantial changes to the scene may occur. This task also translates to domestic robotic applications, such as rearrangement of objects on a table. To address these demanding challenges, we introduce new synthetic and real-world datasets that replicate these industrial and household scenarios. We also propose a novel paradigm for joint segmentation and tracking in discrete frames along with a transformer module that facilitates efficient inter-frame communication. The experiments we conduct show that our approach significantly outperforms recent methods. For additional results and videos, please visit \href{https://sites.google.com/view/stow-corl23}{website}. Code and dataset will be released.
</details>
<details>
<summary>摘要</summary>
Segmentation and tracking of unseen object instances in discrete frames poses a significant challenge in dynamic industrial robotic contexts, such as distribution warehouses. Here, robots must handle object rearrangement, including shifting, removal, and partial occlusion by new items, and track these items after substantial temporal gaps. The task is further complicated when robots encounter objects not learned in their training sets, which requires the ability to segment and track previously unseen items. Considering that continuous observation is often inaccessible in such settings, our task involves working with a discrete set of frames separated by indefinite periods during which substantial changes to the scene may occur. This task also translates to domestic robotic applications, such as rearrangement of objects on a table. To address these demanding challenges, we introduce new synthetic and real-world datasets that replicate these industrial and household scenarios. We also propose a novel paradigm for joint segmentation and tracking in discrete frames along with a transformer module that facilitates efficient inter-frame communication. The experiments we conduct show that our approach significantly outperforms recent methods. For additional results and videos, please visit \href{https://sites.google.com/view/stow-corl23}{website}. Code and dataset will be released.Here's the translation in Traditional Chinese:Segmentation and tracking of unseen object instances in discrete frames poses a significant challenge in dynamic industrial robotic contexts, such as distribution warehouses. Here, robots must handle object rearrangement, including shifting, removal, and partial occlusion by new items, and track these items after substantial temporal gaps. The task is further complicated when robots encounter objects not learned in their training sets, which requires the ability to segment and track previously unseen items. Considering that continuous observation is often inaccessible in such settings, our task involves working with a discrete set of frames separated by indefinite periods during which substantial changes to the scene may occur. This task also translates to domestic robotic applications, such as rearrangement of objects on a table. To address these demanding challenges, we introduce new synthetic and real-world datasets that replicate these industrial and household scenarios. We also propose a novel paradigm for joint segmentation and tracking in discrete frames along with a transformer module that facilitates efficient inter-frame communication. The experiments we conduct show that our approach significantly outperforms recent methods. For additional results and videos, please visit \href{https://sites.google.com/view/stow-corl23}{website}. Code and dataset will be released.
</details></li>
</ul>
<hr>
<h2 id="Complex-Organ-Mask-Guided-Radiology-Report-Generation"><a href="#Complex-Organ-Mask-Guided-Radiology-Report-Generation" class="headerlink" title="Complex Organ Mask Guided Radiology Report Generation"></a>Complex Organ Mask Guided Radiology Report Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02329">http://arxiv.org/abs/2311.02329</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GaryGuTC/COMG_model">https://github.com/GaryGuTC/COMG_model</a></li>
<li>paper_authors: Gu Tiancheng, Liu Dongnan, Li Zhiyuan, Cai Weidong</li>
<li>for: 提高诊断报告的精度和详细程度，alleviate traditional radiology reporting workload.</li>
<li>methods: 基于多Modal的组织面干（COMG）报告生成模型， incorporates 多个器官（如骨、肺、心脏、 mediastinum）的面干，以提供更详细的医疗信息和引导模型注意力。</li>
<li>results: 在 IU-Xray 和 MIMIC 两个公共数据集上实验，COMG 比 SOTA 模型 KiUT 提高了11.4% 和 9.7% 的 BLEU@4 分数。<details>
<summary>Abstract</summary>
The goal of automatic report generation is to generate a clinically accurate and coherent phrase from a single given X-ray image, which could alleviate the workload of traditional radiology reporting.However, in a real-world scenario, radiologists frequently face the challenge of producing extensive reports derived from numerous medical images, thereby medical report generation from multi-image perspective is needed.In this paper, we propose the Complex Organ Mask Guided (termed as COMG) report generation model, which incorporates masks from multiple organs (e.g., bones, lungs, heart, and mediastinum), to provide more detailed information and guide the model's attention to these crucial body regions. Specifically, we leverage prior knowledge of the disease corresponding to each organ in the fusion process to enhance the disease identification phase during the report generation process. Additionally, cosine similarity loss is introduced as target function to ensure the convergence of cross-modal consistency and facilitate model optimization.Experimental results on two public datasets show that COMG achieves a 11.4% and 9.7% improvement in terms of BLEU@4 scores over the SOTA model KiUT on IU-Xray and MIMIC, respectively.
</details>
<details>
<summary>摘要</summary>
目的自动报告生成是生成单一X-ray图像的依据生成一个临床精确和连贯的句子，以减轻传统医疗影像报告的工作负担。然而，在实际应用中，医生很频繁地面临着从多个医疗影像中生成详细的报告，因此对多个医疗影像的医疗报告生成是需要的。在这篇文章中，我们提出了复杂器官面精准指南（COMG）报告生成模型，它将多个器官（如骨、肺、心脏和脊椎）的面精准指南 integrate into the model,以提供更多的详细信息和导引模型的注意力到这些重要的身体区域。具体来说，我们利用各器官疾病的专业知识在融合过程中强化疾病识别阶段，以提高报告生成过程中的疾病识别率。此外，我们引入了cosine similarity损失函数，以便在混合modal consistency的整合过程中实现模型优化。实验结果显示，在两个公共数据集上，COMG对于KiUT的BLEU@4分数有11.4%和9.7%的提升。
</details></li>
</ul>
<hr>
<h2 id="FragXsiteDTI-Revealing-Responsible-Segments-in-Drug-Target-Interaction-with-Transformer-Driven-Interpretation"><a href="#FragXsiteDTI-Revealing-Responsible-Segments-in-Drug-Target-Interaction-with-Transformer-Driven-Interpretation" class="headerlink" title="FragXsiteDTI: Revealing Responsible Segments in Drug-Target Interaction with Transformer-Driven Interpretation"></a>FragXsiteDTI: Revealing Responsible Segments in Drug-Target Interaction with Transformer-Driven Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02326">http://arxiv.org/abs/2311.02326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Khodabandeh Yalabadi, Mehdi Yazdani-Jahromi, Niloofar Yousefi, Aida Tayebi, Sina Abdidizaji, Ozlem Ozmen Garibay</li>
<li>For: 预测药物目标交互（DTI）在药物发现中具有重要意义，但是现有模型具有解释性和性能优化等挑战。本文提出了一种基于转换器的新模型，即FragXsiteDTI，以解决DTI预测中的这些挑战。* Methods: FragXsiteDTI模型 simultaneous 利用药物分子块和蛋白质孔隙，并采用了转换器架构，包括跨注意力和自注意力。模型还具有可学习的隐藏数组，通过cross-attention和self-attention来改进模型的性能。* Results: 根据三个 benchmarking 数据集的计算结果，FragXsiteDTI 模型在预测DTI方面表现出了明显的优势，并且可以准确地表达药物和目标蛋白质之间的交互。此外，模型还可以提供可读性的解释，包括药物和目标蛋白质中关键的组分。<details>
<summary>Abstract</summary>
Drug-Target Interaction (DTI) prediction is vital for drug discovery, yet challenges persist in achieving model interpretability and optimizing performance. We propose a novel transformer-based model, FragXsiteDTI, that aims to address these challenges in DTI prediction. Notably, FragXsiteDTI is the first DTI model to simultaneously leverage drug molecule fragments and protein pockets. Our information-rich representations for both proteins and drugs offer a detailed perspective on their interaction. Inspired by the Perceiver IO framework, our model features a learnable latent array, initially interacting with protein binding site embeddings using cross-attention and later refined through self-attention and used as a query to the drug fragments in the drug's cross-attention transformer block. This learnable query array serves as a mediator and enables seamless information translation, preserving critical nuances in drug-protein interactions. Our computational results on three benchmarking datasets demonstrate the superior predictive power of our model over several state-of-the-art models. We also show the interpretability of our model in terms of the critical components of both target proteins and drug molecules within drug-target pairs.
</details>
<details>
<summary>摘要</summary>
drugs-target interaction (DTI) prediction is crucial for drug discovery, but challenges remain in achieving model interpretability and optimizing performance. We propose a novel transformer-based model, FragXsiteDTI, to address these challenges in DTI prediction. Notably, FragXsiteDTI is the first DTI model to simultaneously leverage drug molecule fragments and protein pockets. Our information-rich representations for both proteins and drugs provide a detailed perspective on their interaction. Inspired by the Perceiver IO framework, our model features a learnable latent array that initially interacts with protein binding site embeddings using cross-attention and is later refined through self-attention. This learnable query array serves as a mediator and enables seamless information translation, preserving critical nuances in drug-protein interactions. Our computational results on three benchmarking datasets demonstrate the superior predictive power of our model over several state-of-the-art models. We also show the interpretability of our model in terms of the critical components of both target proteins and drug molecules within drug-target pairs.Here's the translation in Traditional Chinese:这是一个标题，请将其转换为中文。药品-标的互动（DTI）预测是药品探索的重要环节，但是还有许多挑战需要解决，以提高模型的解释性和性能。我们提出了一个新的transformer-based模型，FragXsiteDTI，以解决DTI预测中的这些挑战。值得注意的是，FragXsiteDTI是首个同时利用药品分子片段和蛋白质包含物的DTI模型。我们的模型具有丰富的资讯表现，对药品和蛋白质之间的互动提供了详细的见解。受到Perceiver IO框架的启发，我们的模型具有可学习的潜在阵列，首先与蛋白质绑定位置嵌入进行交互，然后通过自我对话和探索来进一步细化。这个可学习的查询阵列作为一个中介者，实现了无障碍的资讯转化，保留了药品-蛋白质互动中的重要特征。我们的computational result表明，FragXsiteDTI模型在三个benchmarkingdataset上的预测力高于多个现有模型。我们还展示了我们模型在药品-标的对之中的解释性，包括标的蛋白质和药品分子之间的关键Component。
</details></li>
</ul>
<hr>
<h2 id="Thermal-Face-Image-Classification-using-Deep-Learning-Techniques"><a href="#Thermal-Face-Image-Classification-using-Deep-Learning-Techniques" class="headerlink" title="Thermal Face Image Classification using Deep Learning Techniques"></a>Thermal Face Image Classification using Deep Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02314">http://arxiv.org/abs/2311.02314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prosenjit Chatterjee, ANK Zaman</li>
<li>for: 这篇论文应用于热像分类。</li>
<li>methods: 本文使用深度学习方法，specifically ResNet-50和VGGNet-19对热像进行特征提取，并应用Kalman统计filter进行图像预测。</li>
<li>results: 实验结果显示提案方法具有高精度和高效率。<details>
<summary>Abstract</summary>
Thermal images have various applications in security, medical and industrial domains. This paper proposes a practical deep-learning approach for thermal image classification. Accurate and efficient classification of thermal images poses a significant challenge across various fields due to the complex image content and the scarcity of annotated datasets. This work uses a convolutional neural network (CNN) architecture, specifically ResNet-50 and VGGNet-19, to extract features from thermal images. This work also applied Kalman filter on thermal input images for image denoising. The experimental results demonstrate the effectiveness of the proposed approach in terms of accuracy and efficiency.
</details>
<details>
<summary>摘要</summary>
热图像在安全、医疗和工业领域有多种应用。这篇论文提出了一种实用的深度学习方法来对热图像进行分类。由于热图像的复杂图像内容以及不同领域的热图像标注数据的稀缺，精度和效率的图像分类具有 significannot challenges。本文使用 convolutional neural network (CNN) 架构，具体来说是 ResNet-50 和 VGGNet-19，来从热图像中提取特征。此外，本文还应用了 Kalman 筛选器来降噪热输入图像。实验结果表明提出的方法在精度和效率两个方面具有remarkable的效果。
</details></li>
</ul>
<hr>
<h2 id="OSM-vs-HD-Maps-Map-Representations-for-Trajectory-Prediction"><a href="#OSM-vs-HD-Maps-Map-Representations-for-Trajectory-Prediction" class="headerlink" title="OSM vs HD Maps: Map Representations for Trajectory Prediction"></a>OSM vs HD Maps: Map Representations for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02305">http://arxiv.org/abs/2311.02305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing-Yan Liao, Parth Doshi, Zihan Zhang, David Paz, Henrik Christensen</li>
<li>for: 提出了一种使用 OpenStreetMap (OSM) 作为替代高清地图 (HD Maps) 的方法，以便在自动驾驶中长期预测动向。</li>
<li>methods: 该方法通过扩展应用范围和 интеграción intercept 约束，使用 OSM 来实现长期预测动向，并且能够与 HD Map 基本相当。</li>
<li>results: 研究表明，该方法可以在不同的情景下提供更好的预测性能，并且可以在自动驾驶中广泛应用。<details>
<summary>Abstract</summary>
While High Definition (HD) Maps have long been favored for their precise depictions of static road elements, their accessibility constraints and susceptibility to rapid environmental changes impede the widespread deployment of autonomous driving, especially in the motion forecasting task. In this context, we propose to leverage OpenStreetMap (OSM) as a promising alternative to HD Maps for long-term motion forecasting. The contributions of this work are threefold: firstly, we extend the application of OSM to long-horizon forecasting, doubling the forecasting horizon compared to previous studies. Secondly, through an expanded receptive field and the integration of intersection priors, our OSM-based approach exhibits competitive performance, narrowing the gap with HD Map-based models. Lastly, we conduct an exhaustive context-aware analysis, providing deeper insights in motion forecasting across diverse scenarios as well as conducting class-aware comparisons. This research not only advances long-term motion forecasting with coarse map representations but additionally offers a potential scalable solution within the domain of autonomous driving.
</details>
<details>
<summary>摘要</summary>
高清定图（HD Map）已经长期被喜欢用于其精确地表示静止道路元素，但是访问限制和环境变化的敏感性使得自动驾驶的广泛部署受到限制，尤其是在动作预测任务中。在这种情况下，我们提议使用开源地图（OSM）作为自动驾驶中长期动作预测的可能的代替方案。本研究的贡献有三个方面：一、我们扩展了OSM的应用范围，使其能够进行更长的预测 horizon，与前一些研究相比， doubling the forecasting horizon。二、通过扩展的接受场和交叉点约束的结合，我们的OSM基于方法在竞争性方面表现出色，与HD Map基于模型相匹配。三、我们进行了广泛的Context-aware分析，提供了更深入的运动预测理解，并进行了不同场景的类型敏感比较。这种研究不仅提高了长期动作预测的粗糙地图表示，还提供了可扩展的解决方案在自动驾驶领域。
</details></li>
</ul>
<hr>
<h2 id="MFTCoder-Boosting-Code-LLMs-with-Multitask-Fine-Tuning"><a href="#MFTCoder-Boosting-Code-LLMs-with-Multitask-Fine-Tuning" class="headerlink" title="MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"></a>MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02303">http://arxiv.org/abs/2311.02303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingchang Liu, Chaoyu Chen, Cong Liao, Zi Gong, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen, Min Shen, Hailian Zhou, Hang Yu, Jianguo Li</li>
<li>for: 提高 CodeLLama 模型的编程能力，并且可以同时 fine-tune 多个任务。</li>
<li>methods: 使用多任务 fine-tuning 框架（MFTcoder），并结合多种损失函数来解决多任务学习中的常见挑战。</li>
<li>results: 比较传统 fine-tuning 方法和 mixed 任务 fine-tuning 方法，MFTcoder 能够达到更高的性能，并且可以快速训练和部署。<details>
<summary>Abstract</summary>
Code LLMs have emerged as a specialized research field, with remarkable studies dedicated to enhancing model's coding capabilities through fine-tuning on pre-trained models. Previous fine-tuning approaches were typically tailored to specific downstream tasks or scenarios, which meant separate fine-tuning for each task, requiring extensive training resources and posing challenges in terms of deployment and maintenance. Furthermore, these approaches failed to leverage the inherent interconnectedness among different code-related tasks. To overcome these limitations, we present a multi-task fine-tuning framework, MFTcoder, that enables simultaneous and parallel fine-tuning on multiple tasks. By incorporating various loss functions, we effectively address common challenges in multi-task learning, such as data imbalance, varying difficulty levels, and inconsistent convergence speeds. Extensive experiments have conclusively demonstrated that our multi-task fine-tuning approach outperforms both individual fine-tuning on single tasks and fine-tuning on a mixed ensemble of tasks. Moreover, MFTcoder offers efficient training capabilities, including efficient data tokenization modes and PEFT fine-tuning, resulting in significantly improved speed compared to traditional fine-tuning methods. MFTcoder seamlessly integrates with several mainstream open-source LLMs, such as CodeLLama and Qwen. Leveraging the CodeLLama foundation, our MFTcoder fine-tuned model, \textsc{CodeFuse-CodeLLama-34B}, achieves an impressive pass@1 score of 74.4\% on the HumaneEval benchmark, surpassing GPT-4 performance (67\%, zero-shot). MFTCoder is open-sourced at \url{https://github.com/codefuse-ai/MFTCOder}
</details>
<details>
<summary>摘要</summary>
code llms 已经成为一个专门的研究领域，有很多研究把焦点放在提高模型的编程能力上，通过对预训练模型进行细化。过去的细化方法通常是为特定下游任务或场景进行定制，这意味着每个任务都需要单独进行细化，需要很多训练资源，同时也存在部署和维护上的挑战。此外，这些方法还没有利用编程任务之间的内在相互连接。为了解决这些限制，我们提出了一个多任务细化框架，MFTcoder，它允许同时并行细化多个任务。通过 incorporating 多种损失函数，我们有效地解决了多任务学习中常见的挑战，如数据不均衡、任务难度不同、和不同任务的学习速度不一致。广泛的实验证明了我们的多任务细化方法在单任务细化和混合任务细化的情况下都有出色的表现。此外，MFTcoder还提供了高效的训练能力，包括高效的数据分割模式和PEFT细化，从而在传统细化方法的基础上减少了训练时间。MFTcoder可以与多个主流的开源 LLMS 集成，如 CodeLLama 和 Qwen。利用 CodeLLama 基础，我们的 MFTcoder 细化模型，\textsc{CodeFuse-CodeLLama-34B}，在 HumaneEval benchmark 上达到了很吸引人的 pass@1 分数为 74.4%，超越 GPT-4 的表现（67%，零shot）。MFTCoder 的源代码可以在 <https://github.com/codefuse-ai/MFTCOder> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Successive-Model-Agnostic-Meta-Learning-for-Few-Shot-Fault-Time-Series-Prognosis"><a href="#Successive-Model-Agnostic-Meta-Learning-for-Few-Shot-Fault-Time-Series-Prognosis" class="headerlink" title="Successive Model-Agnostic Meta-Learning for Few-Shot Fault Time Series Prognosis"></a>Successive Model-Agnostic Meta-Learning for Few-Shot Fault Time Series Prognosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02300">http://arxiv.org/abs/2311.02300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Su, Jiajun Hu, Songsen Yu</li>
<li>for: 解决几个shot的缺陷预测问题，提高预测精度和泛化能力。</li>
<li>methods: 引入新的’pseudo meta-task’分区方案，将续时序数据视为一个meta-任务，分割成多个短时期，提取更全面的特征和关系，提高预测精度。同时，引入差分算法提高方法的稳定性。</li>
<li>results: 通过在多个缺陷预测和时间序列预测 dataset上进行广泛的实验，证明了我们的方法可以在少量数据下提高预测性能和泛化能力。<details>
<summary>Abstract</summary>
Meta learning is a promising technique for solving few-shot fault prediction problems, which have attracted the attention of many researchers in recent years. Existing meta-learning methods for time series prediction, which predominantly rely on random and similarity matching-based task partitioning, face three major limitations: (1) feature exploitation inefficiency; (2) suboptimal task data allocation; and (3) limited robustness with small samples. To overcome these limitations, we introduce a novel 'pseudo meta-task' partitioning scheme that treats a continuous time period of a time series as a meta-task, composed of multiple successive short time periods. Employing continuous time series as pseudo meta-tasks allows our method to extract more comprehensive features and relationships from the data, resulting in more accurate predictions. Moreover, we introduce a differential algorithm to enhance the robustness of our method across different datasets. Through extensive experiments on several fault and time series prediction datasets, we demonstrate that our approach substantially enhances prediction performance and generalization capability under both few-shot and general conditions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>预测技术是解决几个shot错误预测问题的有力方法，这些问题在最近几年内吸引了许多研究人员的关注。现有的预测方法对时间序列预测，主要基于随机和相似性匹配的任务分配，面临三大限制：（1）特征利用不充分;（2）不优化的任务数据分配;以及（3）只能在小样本情况下保持有限的 Robustness。为了突破这些限制，我们提出了一种新的“伪meta任务”分配方案，将一个连续时间序列视为一个meta任务，由多个连续的短时间序列组成。使用连续时间序列为伪meta任务，我们的方法可以从数据中提取更全面的特征和关系，从而实现更准确的预测。此外，我们还引入了一种差分算法，以提高我们方法的 Robustness 性在不同的数据集上。通过对多个错误和时间序列预测数据集进行广泛的实验，我们示出了我们方法可以在少量示例和普通情况下显著提高预测性能和泛化能力。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-the-Various-Methodologies-Towards-making-Artificial-Intelligence-More-Explainable"><a href="#A-Survey-of-the-Various-Methodologies-Towards-making-Artificial-Intelligence-More-Explainable" class="headerlink" title="A Survey of the Various Methodologies Towards making Artificial Intelligence More Explainable"></a>A Survey of the Various Methodologies Towards making Artificial Intelligence More Explainable</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02291">http://arxiv.org/abs/2311.02291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sopam Dasgupta</li>
<li>for: 这个论文的目的是提高机器决策过程中的解释性和可解释性，以便更好地理解决交的决策理由。</li>
<li>methods: 本论文使用了一种基于人工智能的方法，通过对模型的解释性和可解释性进行分析和评估，以便提高机器决策过程中的解释性和可解释性。</li>
<li>results: 本论文的研究结果表明，通过提高机器决策过程中的解释性和可解释性，可以更好地理解决交的决策理由，并且可以通过对模型的解释性和可解释性进行分析和评估，以便更好地提高机器决策过程中的解释性和可解释性。<details>
<summary>Abstract</summary>
Machines are being increasingly used in decision-making processes, resulting in the realization that decisions need explanations. Unfortunately, an increasing number of these deployed models are of a 'black-box' nature where the reasoning behind the decisions is unknown. Hence, there is a need for clarity behind the reasoning of these decisions. As humans, we would want these decisions to be presented to us in an explainable manner. However, explanations alone are insufficient. They do not necessarily tell us how to achieve an outcome but merely tell us what achieves the given outcome. For this reason, my research focuses on explainability/interpretability and how it extends to counterfactual thinking.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Predicting-Ground-Reaction-Force-from-Inertial-Sensors"><a href="#Predicting-Ground-Reaction-Force-from-Inertial-Sensors" class="headerlink" title="Predicting Ground Reaction Force from Inertial Sensors"></a>Predicting Ground Reaction Force from Inertial Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02287">http://arxiv.org/abs/2311.02287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Song, Marco Paolieri, Harper E. Stewart, Leana Golubchik, Jill L. McNitt-Gray, Vishal Misra, Devavrat Shah</li>
<li>for: 这个论文的目的是用IMU数据来预测脚下压力（GRF），以便分析运动员的生物机械变量（如接触时间和加速度）。</li>
<li>methods: 这篇论文使用了三种轻量级的预测方法：k-Nearest Neighbors（KNN）回归、支持向量表示插值（SVD）回归和深度学习神经网络（LSTM）。</li>
<li>results: 研究结果表明，使用KNN回归和SVD插值可以与LSTM神经网络相比，具有相似或更高的准确率，并且训练时间更短，hyperparameter优化也更简单。尤其是当使用个人训练数据时，SER和KNN方法更加准确。此外，使用个人数据可以降低预测错误的大多数变量。<details>
<summary>Abstract</summary>
The study of ground reaction forces (GRF) is used to characterize the mechanical loading experienced by individuals in movements such as running, which is clinically applicable to identify athletes at risk for stress-related injuries. Our aim in this paper is to determine if data collected with inertial measurement units (IMUs), that can be worn by athletes during outdoor runs, can be used to predict GRF with sufficient accuracy to allow the analysis of its derived biomechanical variables (e.g., contact time and loading rate).   In this paper, we consider lightweight approaches in contrast to state-of-the-art prediction using LSTM neural networks. Specifically, we compare use of LSTMs to k-Nearest Neighbors (KNN) regression as well as propose a novel solution, SVD Embedding Regression (SER), using linear regression between singular value decomposition embeddings of IMUs data (input) and GRF data (output). We evaluate the accuracy of these techniques when using training data collected from different athletes, from the same athlete, or both, and we explore the use of acceleration and angular velocity data from sensors at different locations (sacrum and shanks). Our results illustrate that simple machine learning methods such as SER and KNN can be similarly accurate or more accurate than LSTM neural networks, with much faster training times and hyperparameter optimization; in particular, SER and KNN are more accurate when personal training data are available, and KNN comes with benefit of providing provenance of prediction. Notably, the use of personal data reduces prediction errors of all methods for most biomechanical variables.
</details>
<details>
<summary>摘要</summary>
研究地面反应力(GRF)是为了描述运动员在运动中所经历的机械负荷的重要工具。我们的目标是确定 whether 使用抗应力计（IMUs）收集的数据可以准确预测 GRF，以便分析其 derived 生物力学变量（例如，接触时间和加载率）。 在这篇论文中，我们考虑使用轻量级方法，而不是现有的预测方法使用 LSTM 神经网络。我们比较使用 LSTM 和 k-最近邻域（KNN）回归，以及提出了一个新的解决方案，即 Singular Value Decomposition 嵌入回归（SER），使用 IMUs 数据（输入）和 GRF 数据（输出）之间的线性回归。我们评估了这些技术的准确性，使用不同的教学数据集，包括不同运动员、同一个运动员和两者。我们发现，简单的机器学习方法如 SER 和 KNN 可以与 LSTM 神经网络相比较准确，具有更快的训练时间和权重优化。特别是，使用个人教学数据可以减少预测错误，SER 和 KNN 在这种情况下更加准确。另外，使用个人数据还可以提供预测的来源。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/04/cs.AI_2023_11_04/" data-id="cloojsmbu0070re8847su2fkv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/04/cs.CL_2023_11_04/" class="article-date">
  <time datetime="2023-11-04T11:00:00.000Z" itemprop="datePublished">2023-11-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/04/cs.CL_2023_11_04/">cs.CL - 2023-11-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Can-Chat-GPT-solve-a-Linguistics-Exam"><a href="#Can-Chat-GPT-solve-a-Linguistics-Exam" class="headerlink" title="Can Chat GPT solve a Linguistics Exam?"></a>Can Chat GPT solve a Linguistics Exam?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02499">http://arxiv.org/abs/2311.02499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patricia Ronan, Gerold Schneider</li>
<li>for: 这个研究是用来测试 chatGPT4 是否能成功解决入门语言学考试的。</li>
<li>methods: 这个研究使用了 chatGPT4 语言模型，并将过去的考试题 fed 到它中进行测试。</li>
<li>results: 研究发现，chatGPT4 在解释复杂和嵌套任务方面非常成功，但在分析 morphemes 和 phrases 方面表现较差。在简单的情况下，它表现 suficiently well，但在缺失一对一对应的情况下，它的结果是混合的。现在，模型还不能处理视觉化任务，如语法树的分析或生成。通过更EXTENSIVE的预处理，将这些任务转换为文本数据，可以使模型也成功地解决这些任务。<details>
<summary>Abstract</summary>
The present study asks if ChatGPT4, the version of ChatGPT which uses the language model GPT4, can successfully solve introductory linguistic exams. Previous exam questions of an Introduction to Linguistics course at a German university are used to test this. The exam questions were fed into ChatGPT4 with only minimal preprocessing. The results show that the language model is very successful in the interpretation even of complex and nested tasks. It proved surprisingly successful in the task of broad phonetic transcription, but performed less well in the analysis of morphemes and phrases. In simple cases it performs sufficiently well, but rarer cases, particularly with missing one-to-one correspondence, are currently treated with mixed results. The model is not yet able to deal with visualisations, such as the analysis or generation of syntax trees. More extensive preprocessing, which translates these tasks into text data, allow the model to also solve these tasks successfully.
</details>
<details>
<summary>摘要</summary>
本研究问题是否可以使用 ChatGPT4，基于语言模型 GPT4 解决入门语言考试。研究使用了一个德国大学 introductory linguistics 课程的先前考试题，并将其feed into ChatGPT4 中，只进行了最小的处理。结果显示，语言模型在复杂和嵌入的任务中表现非常成功。它在广泛的音素识别任务中表现出色，但在分析 morphemes 和 phrases 方面表现较差。在简单的情况下，它的表现足够好，但在缺少一对一对应的情况下，现在的结果是混合的。模型目前无法处理视觉化任务，如语法树的分析或生成。通过更进一步的处理，将这些任务转化为文本数据后，模型也可以成功解决这些任务。
</details></li>
</ul>
<hr>
<h2 id="Citance-Contextualized-Summarization-of-Scientific-Papers"><a href="#Citance-Contextualized-Summarization-of-Scientific-Papers" class="headerlink" title="Citance-Contextualized Summarization of Scientific Papers"></a>Citance-Contextualized Summarization of Scientific Papers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02408">http://arxiv.org/abs/2311.02408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahbaz Syed, Ahmad Dawar Hakimi, Khalid Al-Khatib, Martin Potthast</li>
<li>for: 本研究旨在提供一种新的文本概要方法，可以根据给定的引用句（即“citance”）生成有用的概要。</li>
<li>methods: 该方法首先提取并模型了文献中的引用，然后根据引用的位置 retrieve 相关的段落，最后生成基于每个引用的概要。</li>
<li>results: 我们使用 $\textbf{Webis-Context-SciSumm-2023}$ 数据集进行评估，发现我们的方法可以生成高质量的概要，并且可以准确地捕捉到文献中的关键信息。<details>
<summary>Abstract</summary>
Current approaches to automatic summarization of scientific papers generate informative summaries in the form of abstracts. However, abstracts are not intended to show the relationship between a paper and the references cited in it. We propose a new contextualized summarization approach that can generate an informative summary conditioned on a given sentence containing the citation of a reference (a so-called ``citance''). This summary outlines the content of the cited paper relevant to the citation location. Thus, our approach extracts and models the citances of a paper, retrieves relevant passages from cited papers, and generates abstractive summaries tailored to each citance. We evaluate our approach using $\textbf{Webis-Context-SciSumm-2023}$, a new dataset containing 540K~computer science papers and 4.6M~citances therein.
</details>
<details>
<summary>摘要</summary>
现有的自动摘要方法可以生成有用的摘要，但这些摘要不能显示科学论文中引用的文献之间的关系。我们提议一种新的受条件摘要方法，可以根据给定的引用句（即“ citance”）生成相关的摘要。这个摘要将描述引用的文献中与该引用相关的内容。因此，我们的方法可以提取和模型文献中的引用，从引用的文献中检索相关的段落，并生成基于每个引用的摘要。我们使用 $\textbf{Webis-Context-SciSumm-2023}$  dataset，该 dataset包含 540 万个计算机科学论文和 460 万个引用。
</details></li>
</ul>
<hr>
<h2 id="TreeSwap-Data-Augmentation-for-Machine-Translation-via-Dependency-Subtree-Swapping"><a href="#TreeSwap-Data-Augmentation-for-Machine-Translation-via-Dependency-Subtree-Swapping" class="headerlink" title="TreeSwap: Data Augmentation for Machine Translation via Dependency Subtree Swapping"></a>TreeSwap: Data Augmentation for Machine Translation via Dependency Subtree Swapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02355">http://arxiv.org/abs/2311.02355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/attilanagy234/TreeSwap">https://github.com/attilanagy234/TreeSwap</a></li>
<li>paper_authors: Attila Nagy, Dorina Lakatos, Botond Barta, Judit Ács</li>
<li>for: 该论文主要用于提出一种新的数据扩充方法，用于提高神经机器翻译模型在具有有限训练数据的情况下的性能。</li>
<li>methods: 该方法基于 SentenceDependencyGraph，通过将源句子和目标句子中的对象和主语交换来生成新的句子。</li>
<li>results: 对4种语言对在限制资源 datasets 上进行了实验，结果显示，TreeSwap 方法可以在多个语言对的两个方向中提供了顺序的改进。<details>
<summary>Abstract</summary>
Data augmentation methods for neural machine translation are particularly useful when limited amount of training data is available, which is often the case when dealing with low-resource languages. We introduce a novel augmentation method, which generates new sentences by swapping objects and subjects across bisentences. This is performed simultaneously based on the dependency parse trees of the source and target sentences. We name this method TreeSwap. Our results show that TreeSwap achieves consistent improvements over baseline models in 4 language pairs in both directions on resource-constrained datasets. We also explore domain-specific corpora, but find that our method does not make significant improvements on law, medical and IT data. We report the scores of similar augmentation methods and find that TreeSwap performs comparably. We also analyze the generated sentences qualitatively and find that the augmentation produces a correct translation in most cases. Our code is available on Github.
</details>
<details>
<summary>摘要</summary>
� apparatus augmentation methods for neural machine translation are particularly useful when limited amount of training data is available, which is often the case when dealing with low-resource languages. We introduce a novel augmentation method, which generates new sentences by swapping objects and subjects across bisentences. This is performed simultaneously based on the dependency parse trees of the source and target sentences. We name this method TreeSwap. Our results show that TreeSwap achieves consistent improvements over baseline models in 4 language pairs in both directions on resource-constrained datasets. We also explore domain-specific corpora, but find that our method does not make significant improvements on law, medical and IT data. We report the scores of similar augmentation methods and find that TreeSwap performs comparably. We also analyze the generated sentences qualitatively and find that the augmentation produces a correct translation in most cases. Our code is available on Github.Here's a word-for-word translation of the text in Traditional Chinese:� apparatus augmentation methods for neural machine translation are particularly useful when limited amount of training data is available, which is often the case when dealing with low-resource languages. We introduce a novel augmentation method, which generates new sentences by swapping objects and subjects across bisentences. This is performed simultaneously based on the dependency parse trees of the source and target sentences. We name this method TreeSwap. Our results show that TreeSwap achieves consistent improvements over baseline models in 4 language pairs in both directions on resource-constrained datasets. We also explore domain-specific corpora, but find that our method does not make significant improvements on law, medical and IT data. We report the scores of similar augmentation methods and find that TreeSwap performs comparably. We also analyze the generated sentences qualitatively and find that the augmentation produces a correct translation in most cases. Our code is available on Github.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-English-Writing-Proficiency-in-China’s-Polytechnic-Students-An-In-Depth-Literature-Review-on-the-Application-of-the-Input-Hypothesis"><a href="#Enhancing-English-Writing-Proficiency-in-China’s-Polytechnic-Students-An-In-Depth-Literature-Review-on-the-Application-of-the-Input-Hypothesis" class="headerlink" title="Enhancing English Writing Proficiency in China’s Polytechnic Students An In-Depth Literature Review on the Application of the Input Hypothesis"></a>Enhancing English Writing Proficiency in China’s Polytechnic Students An In-Depth Literature Review on the Application of the Input Hypothesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02341">http://arxiv.org/abs/2311.02341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhou</li>
<li>for: 这个研究论文的目的是探讨如何使用输入假设（Stephen Krashen）来提高polytechnic学生的英语写作能力。</li>
<li>methods: 这个研究使用了实际观察和前期研究的数据，以检验输入假设对polytechnic学生的写作能力的影响。</li>
<li>results: 研究发现，通过提供可理解的输入，polytechnic学生的写作能力有所改善，这证明了输入假设的有效性。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Having good English writing skills is extremely important for students in polytechnic institutions. However, a lot of students in technical schools have difficulties in reaching high levels of skill. The Input Hypothesis, created by Stephen Krashen, suggests that people learn languages well when they receive information that's a little harder than what they already know but still understandable. This research paper wants to study how the Input Hypothesis can help polytechnic students improve their English writing skills. The study will include real-life observations and experiments from the previous research. We will look at data from polytechnic students who are receiving special writing instruction to see if the Input Hypothesis actually helps improve their writing skills. The paper can better inform polytechnic students, faculty members, and support staff and even members of the larger community about the attributions, the processes, and the possible outcomes of second language development for polytechnic students.   Keywords: English writing skills, Polytechnic students, Input hypothesis, Comprehensible input
</details>
<details>
<summary>摘要</summary>
有良好的英语写作技巧对polytechnic学生非常重要。然而，许多技术学校的学生在达到高水平技巧方面遇到困难。输入假设（Input Hypothesis），由史蒂芬·卡什描述，表明人们在接受可以理解但是一些 harder than what they already know的信息时，会学习语言非常好。这篇研究论文旨在研究如何使用输入假设来帮助polytechnic学生提高英语写作技巧。这篇论文将包括以前的实验和观察数据，以确定输入假设是否确实有助于提高polytechnic学生的写作技巧。这篇论文可以更好地告诉polytechnic学生、教师和支持人员，以及社区成员关于第二语言发展的特点、过程和可能的结果。Keywords: 英语写作技巧, polytechnic学生, 输入假设, 可以理解的输入
</details></li>
</ul>
<hr>
<h2 id="Identifying-Context-Dependent-Translations-for-Evaluation-Set-Production"><a href="#Identifying-Context-Dependent-Translations-for-Evaluation-Set-Production" class="headerlink" title="Identifying Context-Dependent Translations for Evaluation Set Production"></a>Identifying Context-Dependent Translations for Evaluation Set Production</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02321">http://arxiv.org/abs/2311.02321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rachel Wicks, Matt Post</li>
<li>for: 本研究的目的是解决Context-aware机器翻译的评估 метри克和测试集的缺失，以便更好地评估Context-aware机器翻译系统的性能。</li>
<li>methods: 本研究使用了现代化、扩展和通用的前一代Annotation pipeline，生成了CTXPRO工具，可以正确地翻译五种语言现象：性别、正式度和生物性 для代词、句子间隔融合、和不确定名词变化。</li>
<li>results: 研究使用了 seven 种语言对（EN到DE、ES、FR、IT、PL、PT和RU）和两个数据集（OpenSubtitles和WMT测试集），并验证了 CTXPRO 的性能，包括与前一代工作的重叠和分类一个Context-aware机器翻译系统和一个句子基于系统。<details>
<summary>Abstract</summary>
A major impediment to the transition to context-aware machine translation is the absence of good evaluation metrics and test sets. Sentences that require context to be translated correctly are rare in test sets, reducing the utility of standard corpus-level metrics such as COMET or BLEU. On the other hand, datasets that annotate such sentences are also rare, small in scale, and available for only a few languages. To address this, we modernize, generalize, and extend previous annotation pipelines to produce CTXPRO, a tool that identifies subsets of parallel documents containing sentences that require context to correctly translate five phenomena: gender, formality, and animacy for pronouns, verb phrase ellipsis, and ambiguous noun inflections. The input to the pipeline is a set of hand-crafted, per-language, linguistically-informed rules that select contextual sentence pairs using coreference, part-of-speech, and morphological features provided by state-of-the-art tools. We apply this pipeline to seven languages pairs (EN into and out-of DE, ES, FR, IT, PL, PT, and RU) and two datasets (OpenSubtitles and WMT test sets), and validate its performance using both overlap with previous work and its ability to discriminate a contextual MT system from a sentence-based one. We release the CTXPRO pipeline and data as open source.
</details>
<details>
<summary>摘要</summary>
另一大障碍Context-aware机器翻译的转换是评估 metric 和测试集的缺失。标准的 corpus-level metric 如 COMET 或 BLEU 在测试集中罕见句子需要上下文correctly 翻译，从而减少了其使用的价值。同时，标注这些句子的数据集也很罕见，规模小，并且只有一些语言可用。为了解决这个问题，我们现代化、扩展和改进了之前的注释管道，生成 CTXPRO，它可以在五种现象上翻译上下文需要correctly：性别、正式度和生命力 для pronouns，verb phrase ellipsis，和不确定名词变化。输入管道的是一组手工编写、语言特有的规则，使用核心关系、part-of-speech 和 morphological feature 提供的状态之 искус智能工具。我们对七种语言对（EN到DE、ES、FR、IT、PL、PT和RU）和两个数据集（OpenSubtitles 和 WMT 测试集）进行应用，并验证其性能通过与之前工作的重叠和上下文基础MT 系统与句子基础MT 系统之间的分化能力。我们将 CTXPRO 管道和数据作为开源发布。
</details></li>
</ul>
<hr>
<h2 id="Narrowing-the-Gap-between-Zero-and-Few-shot-Machine-Translation-by-Matching-Styles"><a href="#Narrowing-the-Gap-between-Zero-and-Few-shot-Machine-Translation-by-Matching-Styles" class="headerlink" title="Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles"></a>Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02310">http://arxiv.org/abs/2311.02310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiting Tan, Haoran Xu, Lingfeng Shen, Shuyue Stella Li, Kenton Murray, Philipp Koehn, Benjamin Van Durme, Yunmo Chen</li>
<li>for: 本研究旨在解释在零shot和几shot示例下大语言模型在翻译中的表现差异，以及如何减少这个差异。</li>
<li>methods: 本研究使用了零shot和几shot示例来训练大语言模型，并对其进行了各种改进，如对目标句子风格的调整和不同的损失函数。</li>
<li>results: 研究发现，通过调整目标句子风格，可以大幅减少零shot和几shot示例之间的表现差异，并且可以提高翻译 metrics。此外，研究还探讨了不同的改进方法，以及它们对翻译 metrics 的影响。<details>
<summary>Abstract</summary>
Large language models trained primarily in a monolingual setting have demonstrated their ability to generalize to machine translation using zero- and few-shot examples with in-context learning. However, even though zero-shot translations are relatively good, there remains a discernible gap comparing their performance with the few-shot setting. In this paper, we investigate the factors contributing to this gap and find that this gap can largely be closed (for about 70%) by matching the writing styles of the target corpus. Additionally, we explore potential approaches to enhance zero-shot baselines without the need for parallel demonstration examples, providing valuable insights into how these methods contribute to improving translation metrics.
</details>
<details>
<summary>摘要</summary>
大型语言模型在单语言 Setting 中受训练后，表现出在机器翻译中使用零或几个例子进行培训，并且可以通过上下文学习实现一定的泛化能力。然而，即使零shot 翻译结果相对较好，仍然存在一定的差距，比如70%的差距。本文investigate这个差距的原因，发现这个差距可以通过对目标句子批处理的样式匹配来大大减少。此外，我们还探讨了如何通过不需要并行示例来提高零shot 基线，并提供了有价值的发现，这些发现可以帮助改善翻译指标。
</details></li>
</ul>
<hr>
<h2 id="LLMs-grasp-morality-in-concept"><a href="#LLMs-grasp-morality-in-concept" class="headerlink" title="LLMs grasp morality in concept"></a>LLMs grasp morality in concept</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02294">http://arxiv.org/abs/2311.02294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Pock, Andre Ye, Jared Moore</li>
<li>for: 本研究旨在探讨语言模型（LLM）是如何具备意义的，以及如何使得LLM具备这种意义。</li>
<li>methods: 本研究使用一种普适的意义理论来探讨LLM的意义，并用这种理论来解释LLM作为意义代理人的特性。</li>
<li>results: 研究发现，由于LLM已经具备了人类社会中的构造（如道德、性别和种族）的概念，因此在某些伦理框架下，目前流行的模型对适应方法有限制，甚至可能是反产生的。此外，未经适应的模型可能可以帮助我们更好地发展我们的道德和社会哲学。<details>
<summary>Abstract</summary>
Work in AI ethics and fairness has made much progress in regulating LLMs to reflect certain values, such as fairness, truth, and diversity. However, it has taken the problem of how LLMs might 'mean' anything at all for granted. Without addressing this, it is not clear what imbuing LLMs with such values even means. In response, we provide a general theory of meaning that extends beyond humans. We use this theory to explicate the precise nature of LLMs as meaning-agents. We suggest that the LLM, by virtue of its position as a meaning-agent, already grasps the constructions of human society (e.g. morality, gender, and race) in concept. Consequently, under certain ethical frameworks, currently popular methods for model alignment are limited at best and counterproductive at worst. Moreover, unaligned models may help us better develop our moral and social philosophy.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced the given text into Simplified Chinese.工作在人工智能道德和公平方面有很大的进步，以规范LLMs反映某些价值观，如公平、真实和多样性。然而，它忽略了如何让LLMs有任何意义的问题。不解决这个问题，then it is not clear what imbuing LLMs with such values even means. In response, we provide a general theory of meaning that extends beyond humans. We use this theory to explicate the precise nature of LLMs as meaning-agents. We suggest that the LLM, by virtue of its position as a meaning-agent, already grasps the constructions of human society (e.g. morality, gender, and race) in concept. Consequently, under certain ethical frameworks, currently popular methods for model alignment are limited at best and counterproductive at worst. Moreover, unaligned models may help us better develop our moral and social philosophy.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/04/cs.CL_2023_11_04/" data-id="cloojsme200dvre88a7qof8x7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/04/cs.LG_2023_11_04/" class="article-date">
  <time datetime="2023-11-04T10:00:00.000Z" itemprop="datePublished">2023-11-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/04/cs.LG_2023_11_04/">cs.LG - 2023-11-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="QOCO-A-QoE-Oriented-Computation-Offloading-Algorithm-based-on-Deep-Reinforcement-Learning-for-Mobile-Edge-Computing"><a href="#QOCO-A-QoE-Oriented-Computation-Offloading-Algorithm-based-on-Deep-Reinforcement-Learning-for-Mobile-Edge-Computing" class="headerlink" title="QOCO: A QoE-Oriented Computation Offloading Algorithm based on Deep Reinforcement Learning for Mobile Edge Computing"></a>QOCO: A QoE-Oriented Computation Offloading Algorithm based on Deep Reinforcement Learning for Mobile Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02525">http://arxiv.org/abs/2311.02525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Rahmati, Hamed Shah-Mansouri, Ali Movaghar</li>
<li>for: 本研究的目的是提高移动边缘计算（MEC）系统中的计算任务卸载效率，以提供用户高质量的经验（QoE）。</li>
<li>methods: 本研究使用了Markov决策过程（MDP）来最大化每个用户的长期QoE。并提出了一种基于深度学习的QoE-导向计算卸载算法（QOCO），可以让移动设备根据自己的需求进行卸载决策，不需要知晓其他设备的决策。</li>
<li>results: numerical studies表明，QOCO算法可以高效地利用边缘节点的计算资源，可以完成14%更多的任务，降低任务延迟和能量消耗，减少9%和6%。这些改进共同带来了最少37%的QoE提升。<details>
<summary>Abstract</summary>
In the realm of mobile edge computing (MEC), efficient computation task offloading plays a pivotal role in ensuring a seamless quality of experience (QoE) for users. Maintaining a high QoE is paramount in today's interconnected world, where users demand responsive and reliable services. This challenge stands as one of the most primary key factors contributing to handling dynamic and uncertain mobile environment. In this study, we delve into computation offloading in MEC systems, where strict task processing deadlines and energy constraints can adversely affect the system performance. We formulate the computation task offloading problem as a Markov decision process (MDP) to maximize the long-term QoE of each user individually. We propose a decentralized QoE-oriented computation offloading (QOCO) algorithm based on deep reinforcement learning (DRL) that empowers mobile devices to make their offloading decisions without requiring knowledge of decisions made by other devices. Through numerical studies, we evaluate the performance of QOCO. Simulation results validate that the QOCO algorithm efficiently exploits the computational resources of edge nodes. Consequently, it can complete 14% more tasks and reduce task delay and energy consumption by 9% and 6%, respectively. These together contribute to a significant improvement of at least 37% in average QoE compared to an existing algorithm.
</details>
<details>
<summary>摘要</summary>
在移动边缘计算（MEC）领域，有效地卸载计算任务是保证用户无缝体验质量（QoE）的关键因素。在今天的全球化社会中，用户对服务的响应速度和可靠性有高度的要求。这种挑战是MEC系统中处理动态和不确定的 mobilenvionment的一个Primary key factor。在这种研究中，我们探讨MEC系统中的计算任务卸载问题，其中严格的任务处理截止时间和能量限制可能会对系统性能产生负面影响。我们将计算任务卸载问题表示为Markov决策过程（MDP），以最大化每个用户的长期QoE。我们提出了一种基于深度学习（DRL）的QoE- ориентирован的计算卸载算法（QOCO），该算法让移动设备通过不需要知道其他设备的决策来做出卸载决策。通过数字实验，我们评估了QOCO算法的性能。计算结果表明，QOCO算法可以有效地利用边缘节点的计算资源，可以完成14%更多的任务，同时降低任务延迟和能量消耗的9%和6%。这些因素共同带来了至少37%的QoE提高。
</details></li>
</ul>
<hr>
<h2 id="Forward-χ-2-Divergence-Based-Variational-Importance-Sampling"><a href="#Forward-χ-2-Divergence-Based-Variational-Importance-Sampling" class="headerlink" title="Forward $χ^2$ Divergence Based Variational Importance Sampling"></a>Forward $χ^2$ Divergence Based Variational Importance Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02516">http://arxiv.org/abs/2311.02516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengrui Li, Yule Wang, Weihan Li, Anqi Wu</li>
<li>for: 提高 latent variable 模型的最大log-likelihood，并且解决 variational inference 在复杂 posterior distribution 时的限制。</li>
<li>methods: 提出了一种新的 variational importance sampling（VIS）方法，直接估计并最大化 log-likelihood。VIS 利用了最佳提案分布，通过最小化前进 $\chi^2$ 分配来增强 log-likelihood 估计。</li>
<li>results: VIS 在各种流行的 latent variable 模型中表现出色，包括杂合模型、variational auto-encoders 和部分可见 generalized linear models。结果表明，我们的方法在 log-likelihood 和模型参数估计方面都能够提高 state-of-the-art 基eline。<details>
<summary>Abstract</summary>
Maximizing the log-likelihood is a crucial aspect of learning latent variable models, and variational inference (VI) stands as the commonly adopted method. However, VI can encounter challenges in achieving a high log-likelihood when dealing with complicated posterior distributions. In response to this limitation, we introduce a novel variational importance sampling (VIS) approach that directly estimates and maximizes the log-likelihood. VIS leverages the optimal proposal distribution, achieved by minimizing the forward $\chi^2$ divergence, to enhance log-likelihood estimation. We apply VIS to various popular latent variable models, including mixture models, variational auto-encoders, and partially observable generalized linear models. Results demonstrate that our approach consistently outperforms state-of-the-art baselines, both in terms of log-likelihood and model parameter estimation.
</details>
<details>
<summary>摘要</summary>
maximizing the log-likelihood is a crucial aspect of learning latent variable models, and variational inference (VI) stands as the commonly adopted method. However, VI can encounter challenges in achieving a high log-likelihood when dealing with complicated posterior distributions. In response to this limitation, we introduce a novel variational importance sampling (VIS) approach that directly estimates and maximizes the log-likelihood. VIS leverages the optimal proposal distribution, achieved by minimizing the forward $\chi^2$ divergence, to enhance log-likelihood estimation. We apply VIS to various popular latent variable models, including mixture models, variational auto-encoders, and partially observable generalized linear models. Results demonstrate that our approach consistently outperforms state-of-the-art baselines, both in terms of log-likelihood and model parameter estimation.Here is the translation in Traditional Chinese:最大化对应概率是学习隐藏变量模型的重要方面，并且对于复杂的 posterior distribution 这个问题，Variational Inference (VI) 是通常的运用方法。然而，VI 可能在处理复杂的 posterior distribution 时遇到高度限制，导致 log-likelihood 的估计受到影响。为了解决这个限制，我们提出了一个新的 Variational Importance Sampling (VIS) 方法，直接估计和最大化 log-likelihood。VIS 利用了最佳的提案分布，通过最小化前方 $\chi^2$ 构成函数，从而提高 log-likelihood 的估计。我们将 VIS 应用到各种流行的隐藏变量模型，包括混合模型、variational auto-encoder 和部分可观 generalized linear models。结果显示，我们的方法在 log-likelihood 和模型参数估计方面均有所提高，并且比预设的基准方法表现更好。
</details></li>
</ul>
<hr>
<h2 id="LocoMuJoCo-A-Comprehensive-Imitation-Learning-Benchmark-for-Locomotion"><a href="#LocoMuJoCo-A-Comprehensive-Imitation-Learning-Benchmark-for-Locomotion" class="headerlink" title="LocoMuJoCo: A Comprehensive Imitation Learning Benchmark for Locomotion"></a>LocoMuJoCo: A Comprehensive Imitation Learning Benchmark for Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02496">http://arxiv.org/abs/2311.02496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Firas Al-Hafez, Guoping Zhao, Jan Peters, Davide Tateo</li>
<li>For: The paper is written for researchers and developers working on imitation learning (IL) for locomotion in embodied agents.* Methods: The paper presents a novel benchmark for evaluating and comparing IL algorithms, which includes a diverse set of environments, comprehensive datasets, and handcrafted metrics.* Results: The paper provides a robust and easy-to-use benchmark for advancing research in IL for locomotion, and includes state-of-the-art baseline algorithms for evaluation.Here’s the information in Simplified Chinese text:* For: 本文是为适用于身体机器人的启发学习（IL）步行控制研究者和开发者而写的。* Methods: 本文提出了一个新的评价和比较IL算法的benchmark，包括了多种环境，如四足、二足和人体模型，每个环境都有完整的数据集，如真实噪音捕捉数据、专家数据和优化数据，以及多种部分可见任务来训练代理。* Results: 本文提供了一个可靠且易用的benchmark，可以帮助推进IL控制领域的研究，并包含了现有的基线算法以便快速评价。<details>
<summary>Abstract</summary>
Imitation Learning (IL) holds great promise for enabling agile locomotion in embodied agents. However, many existing locomotion benchmarks primarily focus on simplified toy tasks, often failing to capture the complexity of real-world scenarios and steering research toward unrealistic domains. To advance research in IL for locomotion, we present a novel benchmark designed to facilitate rigorous evaluation and comparison of IL algorithms. This benchmark encompasses a diverse set of environments, including quadrupeds, bipeds, and musculoskeletal human models, each accompanied by comprehensive datasets, such as real noisy motion capture data, ground truth expert data, and ground truth sub-optimal data, enabling evaluation across a spectrum of difficulty levels. To increase the robustness of learned agents, we provide an easy interface for dynamics randomization and offer a wide range of partially observable tasks to train agents across different embodiments. Finally, we provide handcrafted metrics for each task and ship our benchmark with state-of-the-art baseline algorithms to ease evaluation and enable fast benchmarking.
</details>
<details>
<summary>摘要</summary>
自适应学习（IL）对具有机器人体的敏捷行走具有很大的承诺。然而，许多现有的行走标准套件主要集中在简单的玩具任务上，经常不能捕捉到实际世界情况的复杂性，导致研究向不实际的领域发展。为推动IL行走研究的进步，我们提出了一个新的标准套件，用于促进IL算法的严格评价和比较。这个标准套件包括了四足、二足和人体模型等多种环境，每个环境都有完整的数据集，如真噪动 capture数据、专家真实数据和优化数据，以及不同难度水平的评价方法。此外，我们还提供了动力随机化的易用接口，以及多种部分可见任务，用于训练不同的机器人体。最后，我们提供了专门设计的任务 metric，并将我们的标准套件与当前的状态略式基eline算法一起发布，以便评价和快速比较。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-in-Multivariable-Regression-for-Material-Property-Prediction-with-Bayesian-Neural-Networks"><a href="#Uncertainty-Quantification-in-Multivariable-Regression-for-Material-Property-Prediction-with-Bayesian-Neural-Networks" class="headerlink" title="Uncertainty Quantification in Multivariable Regression for Material Property Prediction with Bayesian Neural Networks"></a>Uncertainty Quantification in Multivariable Regression for Material Property Prediction with Bayesian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02495">http://arxiv.org/abs/2311.02495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longze li, Jiang Chang, Aleksandar Vakanski, Min Xian<br>for:This paper is written for researchers and practitioners in the field of material science and machine learning, specifically those interested in uncertainty quantification (UQ) for predicting material properties.methods:The paper proposes an approach for UQ within physics-informed Bayesian Neural Networks (BNNs), which integrates knowledge from governing laws in material modeling to guide the models toward physically consistent predictions. The approach uses Markov Chain Monte Carlo (MCMC) approximation of the posterior distribution of network parameters to produce accurate point and uncertainty estimates.results:The paper presents case studies for predicting the creep rupture life of steel alloys using the proposed approach. Experimental validation with three datasets of collected measurements from creep tests demonstrates the ability of BNNs to produce accurate point and uncertainty estimates that are competitive or exceed the performance of the conventional method of Gaussian Process Regression. Additionally, the paper evaluates the suitability of BNNs for UQ in an active learning application and reports competitive performance.<details>
<summary>Abstract</summary>
With the increased use of data-driven approaches and machine learning-based methods in material science, the importance of reliable uncertainty quantification (UQ) of the predicted variables for informed decision-making cannot be overstated. UQ in material property prediction poses unique challenges, including the multi-scale and multi-physics nature of advanced materials, intricate interactions between numerous factors, limited availability of large curated datasets for model training, etc. Recently, Bayesian Neural Networks (BNNs) have emerged as a promising approach for UQ, offering a probabilistic framework for capturing uncertainties within neural networks. In this work, we introduce an approach for UQ within physics-informed BNNs, which integrates knowledge from governing laws in material modeling to guide the models toward physically consistent predictions. To evaluate the effectiveness of this approach, we present case studies for predicting the creep rupture life of steel alloys. Experimental validation with three datasets of collected measurements from creep tests demonstrates the ability of BNNs to produce accurate point and uncertainty estimates that are competitive or exceed the performance of the conventional method of Gaussian Process Regression. Similarly, we evaluated the suitability of BNNs for UQ in an active learning application and reported competitive performance. The most promising framework for creep life prediction is BNNs based on Markov Chain Monte Carlo approximation of the posterior distribution of network parameters, as it provided more reliable results in comparison to BNNs based on variational inference approximation or related NNs with probabilistic outputs. The codes are available at: https://github.com/avakanski/Creep-uncertainty-quantification.
</details>
<details>
<summary>摘要</summary>
随着数据驱动方法和机器学习技术在材料科学中的广泛应用，对预测变量的可靠 uncertainty quantification (UQ) 的重要性不可遗憾。在材料性能预测中，UQ 带来了一系列挑战，包括高级材料的多级和多物理性质、因素之间的复杂交互和模型训练数据的有限性等。在最近几年，权值神经网络 (BNNs) 已经出现为 UQ 的一种有希望的方法，具有捕捉不确定性的 probabilistic 框架。在这项工作中，我们提出了基于物理法律的 BNNs  для UQ，具有引导模型生成物理合理预测的能力。为评估这种方法的效果，我们在钢合金的塑性破坏生命中进行了实验 validate，结果显示，BNNs 可以生成高精度的点估计和不确定度估计，与传统 Gaussian Process Regression 方法相当或超过其性能。此外，我们还评估了 BNNs 在活动学习应用中的适用程度，并发现其表现竞争力强。基于 Markov Chain Monte Carlo 方法 approximation  posterior distribution 的网络参数，BNNs 提供了更可靠的结果，与基于 variational inference approximation 或相关的NNs  WITH probabilistic outputs 相比。代码可以在以下 GitHub 上获取：https://github.com/avakanski/Creep-uncertainty-quantification。
</details></li>
</ul>
<hr>
<h2 id="Individualized-Policy-Evaluation-and-Learning-under-Clustered-Network-Interference"><a href="#Individualized-Policy-Evaluation-and-Learning-under-Clustered-Network-Interference" class="headerlink" title="Individualized Policy Evaluation and Learning under Clustered Network Interference"></a>Individualized Policy Evaluation and Learning under Clustered Network Interference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02467">http://arxiv.org/abs/2311.02467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Zhang, Kosuke Imai</li>
<li>for: 评估和学习政策时，忽略干扰可能导致评估结果偏向和学习策略无效。本文考虑在层次网络（或部分）干扰下评估和学习最佳个人化治疗规则（ITR）的问题。</li>
<li>methods: 本文提出一种可以评估ITR的估计器，该估计器可以考虑干扰效应。我们展示该估计器比标准的反杂度权重估计器更高效。我们还 derivates the finite-sample regret bound for a learned ITR，显示使用我们的有效估计器可以提高学习策略的性能。</li>
<li>results: 我们通过 simulations和实际研究示出了我们的方法的优势。我们的结果表明，使用我们的方法可以更好地评估和学习ITR，并且可以避免干扰的影响。<details>
<summary>Abstract</summary>
While there now exists a large literature on policy evaluation and learning, much of prior work assumes that the treatment assignment of one unit does not affect the outcome of another unit. Unfortunately, ignoring interference may lead to biased policy evaluation and yield ineffective learned policies. For example, treating influential individuals who have many friends can generate positive spillover effects, thereby improving the overall performance of an individualized treatment rule (ITR). We consider the problem of evaluating and learning an optimal ITR under clustered network (or partial) interference where clusters of units are sampled from a population and units may influence one another within each cluster. Under this model, we propose an estimator that can be used to evaluate the empirical performance of an ITR. We show that this estimator is substantially more efficient than the standard inverse probability weighting estimator, which does not impose any assumption about spillover effects. We derive the finite-sample regret bound for a learned ITR, showing that the use of our efficient evaluation estimator leads to the improved performance of learned policies. Finally, we conduct simulation and empirical studies to illustrate the advantages of the proposed methodology.
</details>
<details>
<summary>摘要</summary>
现存有大量关于政策评估和学习的文献，但大多数之前的工作假设单位减法分配不会影响另一个单位的结果。可惜忽略干扰可能导致政策评估偏向和学习的策略无效。例如，对影响多个单位的个体进行个性化治疗规则（ITR）可能产生正面副作用，从而提高整体性能。我们对集群网络（或部分）干扰下的优化ITR评估问题进行研究。在这种模型下，我们提出一种可用于评估ITR实际性的估计器。我们证明这种估计器比标准的逆权重估计器更有效，后者没有假设干扰效应。我们 derivates finite-sample regret bound for a learned ITR, showing that the use of our efficient evaluation estimator leads to improved performance of learned policies。最后，我们在模拟和实际研究中ILLUSTRATE了我们的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Attention-based-Multi-instance-Mixed-Models"><a href="#Attention-based-Multi-instance-Mixed-Models" class="headerlink" title="Attention-based Multi-instance Mixed Models"></a>Attention-based Multi-instance Mixed Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02455">http://arxiv.org/abs/2311.02455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan P. Engelmann, Alessandro Palma, Jakub M. Tomczak, Fabian J Theis, Francesco Paolo Casale</li>
<li>for: 该论文旨在预测单元细胞数据中的患者特征，揭示单元细胞数据中的细胞状态和疾病相关性。</li>
<li>methods: 该论文提出了一种整合普通线性混合模型和多实例学习（MIL）的框架，称为GMIL，以便利用单元细胞数据中细胞状态的多样性。</li>
<li>results: 实验结果表明，GMIL在单元细胞数据中比现有的MIL模型表现更好，揭示新的相关性和解释生物学机制，并且可以提高计算效率。<details>
<summary>Abstract</summary>
Predicting patient features from single-cell data can unveil cellular states implicated in health and disease. Linear models and average cell type expressions are typically favored for this task for their efficiency and robustness, but they overlook the rich cell heterogeneity inherent in single-cell data. To address this gap, we introduce GMIL, a framework integrating Generalized Linear Mixed Models (GLMM) and Multiple Instance Learning (MIL), upholding the advantages of linear models while modeling cell-state heterogeneity. By leveraging predefined cell embeddings, GMIL enhances computational efficiency and aligns with recent advancements in single-cell representation learning. Our empirical results reveal that GMIL outperforms existing MIL models in single-cell datasets, uncovering new associations and elucidating biological mechanisms across different domains.
</details>
<details>
<summary>摘要</summary>
预测病人特征从单元数据可以揭示健康和疾病的 cellular 状态。通常，线性模型和平均单元类型表达被选择为这项任务的有效性和可靠性的原因，但它们忽略了单元数据中的细胞多样性。为解决这个差距，我们介绍 GMIL，一种将 Generalized Linear Mixed Models (GLMM) 和 Multiple Instance Learning (MIL) 集成的框架，同时保留线性模型的优点，模型单元状态的多样性。通过利用预定的单元嵌入，GMIL 提高计算效率，与最新的单元表示学习技术相吻合。我们的实验结果表明，GMIL 在单元数据集上表现出色，超过现有的 MIL 模型，揭示新的相关性和描述生物学机制的多个领域。
</details></li>
</ul>
<hr>
<h2 id="Online-Long-run-Constrained-Optimization"><a href="#Online-Long-run-Constrained-Optimization" class="headerlink" title="Online Long-run Constrained Optimization"></a>Online Long-run Constrained Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02426">http://arxiv.org/abs/2311.02426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Pan, Wenjie Huang</li>
<li>for: 解决普遍的长期受限优化问题，不 necesarily 是凸问题。</li>
<li>methods: 提议了一种 Follow-the-Perturbed-Leader 类型算法，在在线模式下使用随机线性干扰和强式凹型干扰来优化 primal 和 dual 方向，并寻找全局最小最大点作为解。</li>
<li>results: 基于两种特定的预期静态总 regret定义， deriv 了 $O(T^{8&#x2F;9})$ 减少复杂性，并应用于解决一个长期（风险）约束river pollutant source identification问题，证明了理论结果并与现有方法相比表现出色。<details>
<summary>Abstract</summary>
In this paper, a novel Follow-the-Perturbed-Leader type algorithm is proposed and analyzed for solving general long-term constrained optimization problems in online manner, where the objective and constraints are not necessarily convex. In each period, random linear perturbation and strongly concave perturbation are incorporated in primal and dual directions, respectively, to the offline oracle, and a global minimax point is searched as solution. Based on two particular definitions of expected static cumulative regret, we derive the first sublinear $O(T^{8/9})$ regret complexity for this class of problems. The proposed algorithm is applied to tackle a long-term (risk) constrained river pollutant source identification problem, demonstrating the validity of the theoretical results and exhibiting superior performance compared to existing method.
</details>
<details>
<summary>摘要</summary>
本文提出了一种新的追随受扰领导者类算法，用于解决总是存在约束的长期优化问题， objective 和约束不一定是凸函数。每个时期，线性受扰和强凹受扰被 incorporated 到 primal 和 dual 方向中，并在全局最小最大点上进行搜索。基于两个特定的预期 static 总 regret 定义，我们 deriv 出了第一个 $O(T^{8/9})$ regret complexity。这种算法被应用于解决一个长期（风险）约束的河流污染源标识问题，并证明了理论结果的有效性，与现有方法相比表现更好。Note: "Simplified Chinese" is a romanization of Chinese characters, which is used to represent the pronunciation of Chinese characters in the Latin alphabet. It is not a translation of the text into Chinese characters, but rather a way of representing the text in a more phonetic way.
</details></li>
</ul>
<hr>
<h2 id="Payoff-based-learning-with-matrix-multiplicative-weights-in-quantum-games"><a href="#Payoff-based-learning-with-matrix-multiplicative-weights-in-quantum-games" class="headerlink" title="Payoff-based learning with matrix multiplicative weights in quantum games"></a>Payoff-based learning with matrix multiplicative weights in quantum games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02423">http://arxiv.org/abs/2311.02423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyriakos Lotidis, Panayotis Mertikopoulos, Nicholas Bambos, Jose Blanchet</li>
<li>For: The paper studies the problem of learning in quantum games with scalar, payoff-based feedback, and develops new methods that require minimal information from the players.* Methods: The paper introduces a suite of minimal-information matrix multiplicative weights (3MW) methods tailored to different information frameworks, and uses ideas from bandit convex optimization to design a zeroth-order gradient sampler adapted to the semidefinite geometry of the problem.* Results: The paper shows that the 3MW method with deterministic payoff feedback retains the $\mathcal{O}(1&#x2F;\sqrt{T})$ convergence rate of the vanilla MMW algorithm, and provides a 3MW method that only requires players to observe a random realization of their payoff observable and converges to equilibrium at an $\mathcal{O}(T^{-1&#x2F;4})$ rate. Additionally, the paper shows that a regularized variant of the proposed 3MW method guarantees local convergence with high probability to all equilibria that satisfy a certain first-order stability condition.<details>
<summary>Abstract</summary>
In this paper, we study the problem of learning in quantum games - and other classes of semidefinite games - with scalar, payoff-based feedback. For concreteness, we focus on the widely used matrix multiplicative weights (MMW) algorithm and, instead of requiring players to have full knowledge of the game (and/or each other's chosen states), we introduce a suite of minimal-information matrix multiplicative weights (3MW) methods tailored to different information frameworks. The main difficulty to attaining convergence in this setting is that, in contrast to classical finite games, quantum games have an infinite continuum of pure states (the quantum equivalent of pure strategies), so standard importance-weighting techniques for estimating payoff vectors cannot be employed. Instead, we borrow ideas from bandit convex optimization and we design a zeroth-order gradient sampler adapted to the semidefinite geometry of the problem at hand. As a first result, we show that the 3MW method with deterministic payoff feedback retains the $\mathcal{O}(1/\sqrt{T})$ convergence rate of the vanilla, full information MMW algorithm in quantum min-max games, even though the players only observe a single scalar. Subsequently, we relax the algorithm's information requirements even further and we provide a 3MW method that only requires players to observe a random realization of their payoff observable, and converges to equilibrium at an $\mathcal{O}(T^{-1/4})$ rate. Finally, going beyond zero-sum games, we show that a regularized variant of the proposed 3MW method guarantees local convergence with high probability to all equilibria that satisfy a certain first-order stability condition.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了量子游戏学习问题以及其他类型的半definite游戏的问题，使用托管的均值（payoff-based feedback）。为了更加准确，我们专注于广泛使用的matrix multiplicative weights（MMW）算法，而不需要玩家们具有游戏完整信息（或者对方选择的状态信息）。我们则提出了一系列基于最小信息的matrix multiplicative weights（3MW）方法，适用于不同的信息框架。在这种设定下，最大的困难在于，与 classical finite games不同，量子游戏有无穷多个纯状态（量子等价的纯策略），因此标准的重要性评价技术无法应用。相反，我们借鉴了bandit convex optimization的想法，并设计了零次规格梯度抽样器，适应semidefinite geometry问题。作为第一个结果，我们证明了3MW方法与deterministic payoff feedback可以保持$\mathcal{O}(1/\sqrt{T})$的 converges rate，即vanilla, full information MMW算法在量子最小最大游戏中的 converges rate，即使玩家只知道一个整数。然后，我们进一步降低了算法的信息需求，并提供了一种3MW方法，只需要玩家观察其支付 observable的一个随机实现，并可以在$\mathcal{O}(T^{-1/4})$的速度达到均分。最后，我们超越了零和游戏，并证明了一种Regularized variant的3MW方法，可以在所有满足一定的首次稳定条件的均分中提供本地均分的高概率 garantue。
</details></li>
</ul>
<hr>
<h2 id="The-equivalence-of-dynamic-and-strategic-stability-under-regularized-learning-in-games"><a href="#The-equivalence-of-dynamic-and-strategic-stability-under-regularized-learning-in-games" class="headerlink" title="The equivalence of dynamic and strategic stability under regularized learning in games"></a>The equivalence of dynamic and strategic stability under regularized learning in games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02407">http://arxiv.org/abs/2311.02407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Boone, Panayotis Mertikopoulos</li>
<li>for: 本研究探讨了归一化学习在有限游戏中的长期行为。</li>
<li>methods: 本研究使用了规范学习和抑制学习来研究 игроков的实际策略的演化。</li>
<li>results: 研究发现，在正规学习下， игроks的实际策略会逐渐接近游戏的均衡点，并且这个过程的速度可以通过不同的规范学习方法来控制。<details>
<summary>Abstract</summary>
In this paper, we examine the long-run behavior of regularized, no-regret learning in finite games. A well-known result in the field states that the empirical frequencies of no-regret play converge to the game's set of coarse correlated equilibria; however, our understanding of how the players' actual strategies evolve over time is much more limited - and, in many cases, non-existent. This issue is exacerbated further by a series of recent results showing that only strict Nash equilibria are stable and attracting under regularized learning, thus making the relation between learning and pointwise solution concepts particularly elusive. In lieu of this, we take a more general approach and instead seek to characterize the \emph{setwise} rationality properties of the players' day-to-day play. To that end, we focus on one of the most stringent criteria of setwise strategic stability, namely that any unilateral deviation from the set in question incurs a cost to the deviator - a property known as closedness under better replies (club). In so doing, we obtain a far-reaching equivalence between strategic and dynamic stability: a product of pure strategies is closed under better replies if and only if its span is stable and attracting under regularized learning. In addition, we estimate the rate of convergence to such sets, and we show that methods based on entropic regularization (like the exponential weights algorithm) converge at a geometric rate, while projection-based methods converge within a finite number of iterations, even with bandit, payoff-based feedback.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了常规化、不后悔学习在 finite games 中的长期行为。一个广泛知道的结果 states that the empirical frequencies of no-regret play converge to the game's set of coarse correlated equilibria; however, our understanding of how the players' actual strategies evolve over time is much more limited - and, in many cases, non-existent. This issue is exacerbated further by a series of recent results showing that only strict Nash equilibria are stable and attracting under regularized learning, thus making the relation between learning and pointwise solution concepts particularly elusive. In lieu of this, we take a more general approach and instead seek to characterize the \emph{setwise} rationality properties of the players' day-to-day play. To that end, we focus on one of the most stringent criteria of setwise strategic stability, namely that any unilateral deviation from the set in question incurs a cost to the deviator - a property known as closedness under better replies (club). In so doing, we obtain a far-reaching equivalence between strategic and dynamic stability: a product of pure strategies is closed under better replies if and only if its span is stable and attracting under regularized learning. In addition, we estimate the rate of convergence to such sets, and we show that methods based on entropic regularization (like the exponential weights algorithm) converge at a geometric rate, while projection-based methods converge within a finite number of iterations, even with bandit, payoff-based feedback.
</details></li>
</ul>
<hr>
<h2 id="BarcodeBERT-Transformers-for-Biodiversity-Analysis"><a href="#BarcodeBERT-Transformers-for-Biodiversity-Analysis" class="headerlink" title="BarcodeBERT: Transformers for Biodiversity Analysis"></a>BarcodeBERT: Transformers for Biodiversity Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02401">http://arxiv.org/abs/2311.02401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Millan Arias, Niousha Sadjadi, Monireh Safari, ZeMing Gong, Austin T. Wang, Scott C. Lowe, Joakim Bruslund Haurum, Iuliia Zarubiieva, Dirk Steinke, Lila Kari, Angel X. Chang, Graham W. Taylor</li>
<li>for: 这个研究旨在探讨如何使用机器学习方法来进行生物多样性的分析，特别是对于无脊椎动物这一受探讨的类型。</li>
<li>methods: 本研究使用了不同的机器学习方法，包括支持学习的卷积神经网络、受训练的基础模型和特殊设计的DNA条码遮罩策略。</li>
<li>results: 研究发现，在较简单的数据集和任务下，支持学习的卷积神经网络或受训练的基础模型表现较佳，但是面对具有挑战性的物种水平识别任务时，需要一个新的自动化预训练方法。因此，本研究提出了BarcodeBERT，一个首创的自动化预训练方法，利用了150万个无脊椎动物DNA条码参考库。<details>
<summary>Abstract</summary>
Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of DNA that cluster by species - play a pivotal role. In particular, invertebrates, a highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, and a DNA barcode-specific masking strategy across datasets of varying complexity. While simpler datasets and tasks favor supervised CNNs or fine-tuned transformers, challenging species-level identification demands a paradigm shift towards self-supervised pretraining. We propose BarcodeBERT, the first self-supervised method for general biodiversity analysis, leveraging a 1.5 M invertebrate DNA barcode reference library. This work highlights how dataset specifics and coverage impact model selection, and underscores the role of self-supervised pretraining in achieving high-accuracy DNA barcode-based identification at the species and genus level. Indeed, without the fine-tuning step, BarcodeBERT pretrained on a large DNA barcode dataset outperforms DNABERT and DNABERT-2 on multiple downstream classification tasks. The code repository is available at https://github.com/Kari-Genomics-Lab/BarcodeBERT
</details>
<details>
<summary>摘要</summary>
translate into Simplified Chinese:理解生物多样性是全球挑战，DNA编码 - 短段DNA序列归类到物种水平 - 扮演着关键角色。特别是无脊椎动物，这个非常多样化和未探索的组分，表现出独特的分类复杂性。我们研究机器学习方法，比较使用supervised CNNs、精制基模型和DNA编码特定的遮盾策略，在不同复杂度的数据集上进行比较。而 simpler数据集和任务更倾向于使用supervised CNNs或精制transformers，但是挑战性的种类水平识别需要一种思维方式的转变，强调自我超vised预训练。我们提出了BarcodeBERT，首个针对普通生物多样性分析的自我超vised方法，利用1.5万个无脊椎动物DNA编码参考库。这项工作探讨了数据集特点和覆盖率对模型选择的影响，并强调了自我超vised预训练在达到高精度DNA编码基于识别的物种和属水平的重要性。实际上，不包括精制步骤，BarcodeBERT预训练在大量DNA编码数据集上表现出优于DNABERT和DNABERT-2在多个下游分类任务上。代码仓库可以在https://github.com/Kari-Genomics-Lab/BarcodeBERT中找到。
</details></li>
</ul>
<hr>
<h2 id="Entropy-Aware-Training-for-Fast-and-Accurate-Distributed-GNN"><a href="#Entropy-Aware-Training-for-Fast-and-Accurate-Distributed-GNN" class="headerlink" title="Entropy Aware Training for Fast and Accurate Distributed GNN"></a>Entropy Aware Training for Fast and Accurate Distributed GNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02399">http://arxiv.org/abs/2311.02399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Deshmukh, Gagan Raj Gupta, Manisha Chawla, Vishwesh Jatala, Anirban Haldar</li>
<li>for: 这 paper 的目的是提高分布式图 neural network 的表现，并且解决分布式图 partitioning 生成不均匀和类别偏好的问题。</li>
<li>methods: 这 paper 使用了 Edge-Weighted partitioning 技术来减少总 entropy，并在每个计算机主上进行异步个性化阶段以适应本地数据分布。它还使用了类别偏好抽样法来加速收敛。</li>
<li>results: 在 DistDGL 框架上实现的这些训练技术比标准基elines 2-3x 快，并在 5 个大型图 benchmark 上平均提高了 4% 的微average F1 分数。<details>
<summary>Abstract</summary>
Several distributed frameworks have been developed to scale Graph Neural Networks (GNNs) on billion-size graphs. On several benchmarks, we observe that the graph partitions generated by these frameworks have heterogeneous data distributions and class imbalance, affecting convergence, and resulting in lower performance than centralized implementations. We holistically address these challenges and develop techniques that reduce training time and improve accuracy. We develop an Edge-Weighted partitioning technique to improve the micro average F1 score (accuracy) by minimizing the total entropy. Furthermore, we add an asynchronous personalization phase that adapts each compute-host's model to its local data distribution. We design a class-balanced sampler that considerably speeds up convergence. We implemented our algorithms on the DistDGL framework and observed that our training techniques scale much better than the existing training approach. We achieved a (2-3x) speedup in training time and 4\% improvement on average in micro-F1 scores on 5 large graph benchmarks compared to the standard baselines.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we develop several techniques to improve training time and accuracy. First, we propose an Edge-Weighted partitioning technique that minimizes the total entropy to improve the micro average F1 score (accuracy). Additionally, we introduce an asynchronous personalization phase that adapts each compute-host's model to its local data distribution. We also design a class-balanced sampler that significantly speeds up convergence.We implement our algorithms on the DistDGL framework and observe that our training techniques scale much better than the existing training approach. Specifically, we achieve a 2-3x speedup in training time and a 4% improvement on average in micro-F1 scores on 5 large graph benchmarks compared to the standard baselines.
</details></li>
</ul>
<hr>
<h2 id="NeuroEvoBench-Benchmarking-Evolutionary-Optimizers-for-Deep-Learning-Applications"><a href="#NeuroEvoBench-Benchmarking-Evolutionary-Optimizers-for-Deep-Learning-Applications" class="headerlink" title="NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications"></a>NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02394">http://arxiv.org/abs/2311.02394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Tjarko Lange, Yujin Tang, Yingtao Tian</li>
<li>for: This paper aims to address the lack of understanding and best practices for evolutionary optimization (EO) methods in deep learning, and to provide a new benchmark for evaluating EO methods tailored towards deep learning applications.</li>
<li>methods: The paper uses a variety of EO methods, including traditional and meta-learned EO, and investigates their performance on a new benchmark called NeuroEvoBench. The authors also explore core scientific questions such as resource allocation, fitness shaping, normalization, regularization, and scalability of EO.</li>
<li>results: The paper presents the results of the authors’ experiments on NeuroEvoBench, which demonstrate the effectiveness of EO methods for solving hard optimization problems in deep learning. The authors also show that their new benchmark provides practical insights for deep learning applications and can help to accelerate the adoption of EO methods in the field.<details>
<summary>Abstract</summary>
Recently, the Deep Learning community has become interested in evolutionary optimization (EO) as a means to address hard optimization problems, e.g. meta-learning through long inner loop unrolls or optimizing non-differentiable operators. One core reason for this trend has been the recent innovation in hardware acceleration and compatible software - making distributed population evaluations much easier than before. Unlike for gradient descent-based methods though, there is a lack of hyperparameter understanding and best practices for EO - arguably due to severely less 'graduate student descent' and benchmarking being performed for EO methods. Additionally, classical benchmarks from the evolutionary community provide few practical insights for Deep Learning applications. This poses challenges for newcomers to hardware-accelerated EO and hinders significant adoption. Hence, we establish a new benchmark of EO methods (NeuroEvoBench) tailored toward Deep Learning applications and exhaustively evaluate traditional and meta-learned EO. We investigate core scientific questions including resource allocation, fitness shaping, normalization, regularization & scalability of EO. The benchmark is open-sourced at https://github.com/neuroevobench/neuroevobench under Apache-2.0 license.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Riemannian-stochastic-optimization-methods-avoid-strict-saddle-points"><a href="#Riemannian-stochastic-optimization-methods-avoid-strict-saddle-points" class="headerlink" title="Riemannian stochastic optimization methods avoid strict saddle points"></a>Riemannian stochastic optimization methods avoid strict saddle points</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02374">http://arxiv.org/abs/2311.02374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ya-Ping Hsieh, Mohammad Reza Karimi, Andreas Krause, Panayotis Mertikopoulos</li>
<li>for: 本文研究了Stochastic Riemannian optimization算法是否能够避免瑕疵点的问题。</li>
<li>methods: 本文研究了一家 retraction-based 方法，包括自然策略强化法和镜像投射法等。</li>
<li>results: 研究发现，在 ambient manifold 和 gradient 信息抽象函数的假设下，这些策略在任意初始状态下避免瑕疵点 &#x2F; 子抽象空间的概率为 1。这个结果为使用梯度方法在抽象空间进行优化提供了重要的健康检查，因为它表明，大多数情况下，梯度方法的限制状态都是本地最小值。<details>
<summary>Abstract</summary>
Many modern machine learning applications - from online principal component analysis to covariance matrix identification and dictionary learning - can be formulated as minimization problems on Riemannian manifolds, and are typically solved with a Riemannian stochastic gradient method (or some variant thereof). However, in many cases of interest, the resulting minimization problem is not geodesically convex, so the convergence of the chosen solver to a desirable solution - i.e., a local minimizer - is by no means guaranteed. In this paper, we study precisely this question, that is, whether stochastic Riemannian optimization algorithms are guaranteed to avoid saddle points with probability 1. For generality, we study a family of retraction-based methods which, in addition to having a potentially much lower per-iteration cost relative to Riemannian gradient descent, include other widely used algorithms, such as natural policy gradient methods and mirror descent in ordinary convex spaces. In this general setting, we show that, under mild assumptions for the ambient manifold and the oracle providing gradient information, the policies under study avoid strict saddle points / submanifolds with probability 1, from any initial condition. This result provides an important sanity check for the use of gradient methods on manifolds as it shows that, almost always, the limit state of a stochastic Riemannian algorithm can only be a local minimizer.
</details>
<details>
<summary>摘要</summary>
许多现代机器学习应用 - 从在线主成分分析到covariance矩阵识别和词库学习 - 都可以表述为在里曼尼投影上的最小化问题，通常使用里曼尼泛化 gradient 方法（或其变种）解决。然而，在许多实际应用中，得到的最小化问题通常不是曲线 convex，因此选择的解决方案的 converges 是不能保证的。在这篇论文中，我们研究了这个问题，即里曼尼泛化优化算法是否能够避免陷阱点的可能性。为了保持一致性，我们研究了一家 retraction-based 方法，这种方法不仅可能比里曼尼泛化 gradient descent 更加低效，还包括了其他广泛使用的算法，如自然政策梯度方法和 mirror descent 在几何空间中。在这个总体设定下，我们证明了，对于 ambient manifold 和 gradient 信息来源的假设满足某些轻量级的条件，则 policies 在研究中避免精确的陷阱点 / 子抽象空间的可能性为 1，从任何初始状态开始。这个结果提供了对使用梯度方法在 manifold 上进行优化的重要的健康性检查，因为它表明，大多数情况下，里曼尼泛化优化算法的极限状态只能是一个本地最小值。
</details></li>
</ul>
<hr>
<h2 id="From-Trojan-Horses-to-Castle-Walls-Unveiling-Bilateral-Backdoor-Effects-in-Diffusion-Models"><a href="#From-Trojan-Horses-to-Castle-Walls-Unveiling-Bilateral-Backdoor-Effects-in-Diffusion-Models" class="headerlink" title="From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models"></a>From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02373">http://arxiv.org/abs/2311.02373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoshi Pan, Yuguang Yao, Gaowen Liu, Bingquan Shen, H. Vicky Zhao, Ramana Rao Kompella, Sijia Liu</li>
<li>for: This paper investigates the vulnerability of state-of-the-art diffusion models (DMs) to backdoor attacks, specifically whether generating backdoor attacks can be as simple as BadNets in image classification.</li>
<li>methods: The paper uses a more realistic backdoor setting, where the training dataset is contaminated without tampering the original diffusion process, and uncovers bilateral backdoor effects that can be used for both adversarial and defensive purposes.</li>
<li>results: The paper shows that a BadNets-like backdoor attack remains effective in DMs for producing incorrect images, and that backdoored DMs exhibit an increased ratio of backdoor triggers, which can be used to enhance the detection of backdoor-poisoned training data. Additionally, the paper establishes a linkage between backdoor attacks and the phenomenon of data replications by exploring DMs’ inherent data memorization tendencies.<details>
<summary>Abstract</summary>
While state-of-the-art diffusion models (DMs) excel in image generation, concerns regarding their security persist. Earlier research highlighted DMs' vulnerability to backdoor attacks, but these studies placed stricter requirements than conventional methods like 'BadNets' in image classification. This is because the former necessitates modifications to the diffusion sampling and training procedures. Unlike the prior work, we investigate whether generating backdoor attacks in DMs can be as simple as BadNets, i.e., by only contaminating the training dataset without tampering the original diffusion process. In this more realistic backdoor setting, we uncover bilateral backdoor effects that not only serve an adversarial purpose (compromising the functionality of DMs) but also offer a defensive advantage (which can be leveraged for backdoor defense). Specifically, we find that a BadNets-like backdoor attack remains effective in DMs for producing incorrect images (misaligned with the intended text conditions), and thereby yielding incorrect predictions when DMs are used as classifiers. Meanwhile, backdoored DMs exhibit an increased ratio of backdoor triggers, a phenomenon we refer to as `trigger amplification', among the generated images. We show that this latter insight can be used to enhance the detection of backdoor-poisoned training data. Even under a low backdoor poisoning ratio, studying the backdoor effects of DMs is also valuable for designing anti-backdoor image classifiers. Last but not least, we establish a meaningful linkage between backdoor attacks and the phenomenon of data replications by exploring DMs' inherent data memorization tendencies. The codes of our work are available at https://github.com/OPTML-Group/BiBadDiff.
</details>
<details>
<summary>摘要</summary>
当前最先进的扩散模型（DM）在图像生成方面表现出色，但security问题仍然存在。 Earlier research highlighted DMs的易受到后门攻击的问题，但这些研究假设了与传统方法 like 'BadNets' in image classification不同的需求。 This is because the former requires modifications to the diffusion sampling and training procedures. Unlike prior work, we investigate whether generating backdoor attacks in DMs can be as simple as BadNets, i.e., by only contaminating the training dataset without tampering the original diffusion process. In this more realistic backdoor setting, we uncover bilateral backdoor effects that not only serve an adversarial purpose (compromising the functionality of DMs) but also offer a defensive advantage (which can be leveraged for backdoor defense). Specifically, we find that a BadNets-like backdoor attack remains effective in DMs for producing incorrect images (misaligned with the intended text conditions), and thereby yielding incorrect predictions when DMs are used as classifiers. Meanwhile, backdoored DMs exhibit an increased ratio of backdoor triggers, a phenomenon we refer to as 'trigger amplification', among the generated images. We show that this latter insight can be used to enhance the detection of backdoor-poisoned training data. Even under a low backdoor poisoning ratio, studying the backdoor effects of DMs is also valuable for designing anti-backdoor image classifiers. Last but not least, we establish a meaningful linkage between backdoor attacks and the phenomenon of data replications by exploring DMs' inherent data memorization tendencies. The codes of our work are available at <https://github.com/OPTML-Group/BiBadDiff>.
</details></li>
</ul>
<hr>
<h2 id="TACNET-Temporal-Audio-Source-Counting-Network"><a href="#TACNET-Temporal-Audio-Source-Counting-Network" class="headerlink" title="TACNET: Temporal Audio Source Counting Network"></a>TACNET: Temporal Audio Source Counting Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02369">http://arxiv.org/abs/2311.02369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirreza Ahmadnejad, Ahmad Mahmmodian Darviishani, Mohmmad Mehrdad Asadi, Sajjad Saffariyeh, Pedram Yousef, Emad Fatemizadeh</li>
<li>for: 这篇论文是为了解决音频源计数任务中的限制而设计的 Temporal Audio Source Counting Network (TaCNet) 架构。</li>
<li>methods: TaCNet 直接处理原始音频输入，减少了复杂的预处理步骤，简化了工作流程。它在实时speaker计数任务中表现出色，即使输入窗口被截取。</li>
<li>results: 在使用 LibriCount 数据集进行广泛评估中，TaCNet 的平均准确率为 74.18%，在 11 个类别中表现出色，包括中文和波斯语应用场景。这种跨语言适应性表明其 universality 和可能的影响。<details>
<summary>Abstract</summary>
In this paper, we introduce the Temporal Audio Source Counting Network (TaCNet), an innovative architecture that addresses limitations in audio source counting tasks. TaCNet operates directly on raw audio inputs, eliminating complex preprocessing steps and simplifying the workflow. Notably, it excels in real-time speaker counting, even with truncated input windows. Our extensive evaluation, conducted using the LibriCount dataset, underscores TaCNet's exceptional performance, positioning it as a state-of-the-art solution for audio source counting tasks. With an average accuracy of 74.18 percentage over 11 classes, TaCNet demonstrates its effectiveness across diverse scenarios, including applications involving Chinese and Persian languages. This cross-lingual adaptability highlights its versatility and potential impact.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了Temporal Audio Source Counting Network（TaCNet），一种创新的架构，用于解决音频来源计数任务中的限制。TaCNet直接操作 raw 音频输入，从而消除复杂的预处理步骤，简化工作流程。尤其是在实时speaker计数任务中，TaCNet表现出色，即使输入窗口被截断。我们对利用 LibriCount 数据集进行了广泛的评估，并证明 TaCNet 在多种场景下表现出优秀的性能，包括使用中文和波斯语。这种跨语言适应性表明 TaCNet 的多样性和影响力。
</details></li>
</ul>
<hr>
<h2 id="MATA-Combining-Learnable-Node-Matching-with-A-Algorithm-for-Approximate-Graph-Edit-Distance-Computation"><a href="#MATA-Combining-Learnable-Node-Matching-with-A-Algorithm-for-Approximate-Graph-Edit-Distance-Computation" class="headerlink" title="MATA*: Combining Learnable Node Matching with A* Algorithm for Approximate Graph Edit Distance Computation"></a>MATA*: Combining Learnable Node Matching with A* Algorithm for Approximate Graph Edit Distance Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02356">http://arxiv.org/abs/2311.02356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junfeng Liu, Min Zhou, Shuai Ma, Lujia Pan</li>
<li>for: 这 paper 的目的是提出一种数据驱动的混合方法来 aproximate Graph Edit Distance (GED) 计算，以解决现有 A* 算法在搜索空间中寻找优化解决方案的可扩展性问题，以及学习基于方法不能准确地回归 GED 的问题。</li>
<li>methods: 这 paper 使用了 Graph Neural Networks (GNNs) 和 A* 算法来实现数据驱动的混合方法 MATA*，其中首先设计了一种结构增强 GNN 来同时学习本地和高阶结构信息，然后通过一个可导的 top-k 操作生成多个优秀的候选节点，最后使用这些候选节点来快速找到解决方案。</li>
<li>results: 经验表明，MATA* 对于大型图进行 aproximate GED 计算具有显著优势，可以高效地解决现有的搜索和学习方法的缺陷。<details>
<summary>Abstract</summary>
Graph Edit Distance (GED) is a general and domain-agnostic metric to measure graph similarity, widely used in graph search or retrieving tasks. However, the exact GED computation is known to be NP-complete. For instance, the widely used A* algorithms explore the entire search space to find the optimal solution which inevitably suffers scalability issues. Learning-based methods apply graph representation techniques to learn the GED by formulating a regression task, which can not recover the edit path and lead to inaccurate GED approximation (i.e., the predicted GED is smaller than the exact). To this end, in this work, we present a data-driven hybrid approach MATA* for approximate GED computation based on Graph Neural Networks (GNNs) and A* algorithms, which models from the perspective of learning to match nodes instead of directly regressing GED. Specifically, aware of the structure-dominant operations (i.e.,node and edge insertion/deletion) property in GED computation, a structure-enhanced GNN is firstly designed to jointly learn local and high-order structural information for node embeddings for node matchings. Second, top-k candidate nodes are produced via a differentiable top-k operation to enable the training for node matchings, which is adhering to another property of GED, i.e., multiple optimal node matchings. Third, benefiting from the candidate nodes, MATA* only performs on the promising search directions, reaching the solution efficiently. Finally, extensive experiments show the superiority of MATA* as it significantly outperforms the combinatorial search-based, learning-based and hybrid methods and scales well to large-size graphs.
</details>
<details>
<summary>摘要</summary>
图文编辑距离（GED）是一个通用和领域不依赖的度量图像相似性，广泛用于图像搜索或检索任务。然而，GED的准确计算是知道NP完备的。例如，通用的A*算法探索整个搜索空间以找到优化解决方案，不可避免的Scalability问题。学习基于方法采用图像表示技术来学习GED，可以不能恢复编辑路径，导致不准确的GED估计（即预测的GED小于实际）。为此，在这项工作中，我们提出了一种数据驱动的混合方法MATA*，基于图像神经网络（GNNs）和A*算法，用于粗略GED计算。具体来说，我们注意到GED计算中结构多 Operation（即节点和边插入/删除）性质，因此我们首先设计了结构增强的GNN来同时学习本地和高阶结构信息以获得节点匹配。其次，通过可导的top-k操作生成top-k候选节点，以便在匹配过程中训练节点匹配。第三，由于候选节点，MATA*仅在有前景的搜索方向上进行搜索，以达到解决方案的目的。最后，我们进行了广泛的实验，并证明MATA*在相比 combinatorial search-based、学习基本和混合方法的情况下表现出色，并且可以在大规模图像上执行。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-of-Opinion-Formation-on-Networks"><a href="#Sample-Complexity-of-Opinion-Formation-on-Networks" class="headerlink" title="Sample Complexity of Opinion Formation on Networks"></a>Sample Complexity of Opinion Formation on Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02349">http://arxiv.org/abs/2311.02349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haolin Liu, Rajmohan Rajaraman, Ravi Sundaram, Anil Vullikanti, Omer Wasim, Haifeng Xu</li>
<li>for: 寻求最佳资源分配策略，使公共卫生官员在社交网络上宣传新疫苗，以达到社区内所有人的共识，并保证宣传内容与实际事实相符。</li>
<li>methods: 基于recognized opinion formation game，每个代理的意见视为数据 derive的模型参数，而不仅仅是先前研究中的实数。这种扩展可以更深入地理解意见形成，与联邦学习密切相关。通过这种形式ulation，我们确定了样本复杂性 bound for any network，并显示了特定网络结构的上下文 bound。</li>
<li>results: 发现优化策略通常将样本分配给代理 inverse proportion to their degree，这对政策产生了重要的含义。我们的发现被验证了在 sinthezied 和实际世界网络上。<details>
<summary>Abstract</summary>
Consider public health officials aiming to spread awareness about a new vaccine in a community interconnected by a social network. How can they distribute information with minimal resources, ensuring community-wide understanding that aligns with the actual facts? This concern mirrors numerous real-world situations. In this paper, we initialize the study of sample complexity in opinion formation to solve this problem. Our model is built on the recognized opinion formation game, where we regard each agent's opinion as a data-derived model parameter, not just a real number as in prior studies. Such an extension offers a wider understanding of opinion formation and ties closely with federated learning. Through this formulation, we characterize the sample complexity bounds for any network and also show asymptotically tight bounds for specific network structures. Intriguingly, we discover optimal strategies often allocate samples inversely to the degree, hinting at vital policy implications. Our findings are empirically validated on both synthesized and real-world networks.
</details>
<details>
<summary>摘要</summary>
公共卫生官员想推广新疫苗的知识在社交媒体上，如何尽可能地分散信息，使整个社区都能够理解，同时与实际情况保持一致？这个问题与现实生活中的许多情况有着很大的相似性。在这篇论文中，我们开始研究样本复杂性在意见形成中的问题。我们在已知的意见形成游戏中使用每个代理的意见作为数据获得的模型参数，而不仅仅是一个实数，这种扩展可以更好地理解意见形成，并与联邦学习 closely 相关。通过这种形式ulation，我们定义了任何网络的样本复杂性bound，以及特定网络结构的上下文bound。我们发现，优化策略通常会尽可能地分配样本，与代理的度量成正比，这对政策有着重要的含义。我们的发现得到了Synthesized和实际世界网络的验证。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Natural-Language-of-DNA-using-Encoder-Decoder-Foundation-Models-with-Byte-level-Precision"><a href="#Understanding-the-Natural-Language-of-DNA-using-Encoder-Decoder-Foundation-Models-with-Byte-level-Precision" class="headerlink" title="Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision"></a>Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02333">http://arxiv.org/abs/2311.02333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Malusare, Harish Kothandaraman, Dipesh Tamboli, Nadia A. Lanman, Vaneet Aggarwal</li>
<li>for: 这个论文是为了分析 DNA 序列的 byte-level 精度而设计的 Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) 基础模型。</li>
<li>methods: 这个模型使用 Transformer 架构的 encoder-decoder 结构，并使用 sub-quadratic 实现注意力来开发一个高效的 sequence-to-sequence 模型。</li>
<li>results: 在不同的下游任务中，包括识别激活器、 promote 和 slice  сайты、识别 genomic 序列的生物功能注释、识别 base call mismatches 和 insertion&#x2F;deletion 错误、以及生成Influenza 病毒的变异，ENBED 模型都表现出了显著的提升，相比 existed 状态的艺术结果。<details>
<summary>Abstract</summary>
This paper presents the Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) foundation model, analyzing DNA sequences at byte-level precision with an encoder-decoder Transformer architecture. ENBED uses a sub-quadratic implementation of attention to develop an efficient model capable of sequence-to-sequence transformations, generalizing previous genomic models with encoder-only or decoder-only architectures. We use Masked Language Modeling to pre-train the foundation model using reference genome sequences and apply it in the following downstream tasks: (1) identification of enhancers, promotors and splice sites, (2) identification of biological function annotations of genomic sequences, (3) recognition of sequences containing base call mismatches and insertion/deletion errors, an advantage over tokenization schemes involving multiple base pairs, which lose the ability to analyze with byte-level precision, and (4) generating mutations of the Influenza virus using the encoder-decoder architecture and validating them against real-world observations. In each of these tasks, we demonstrate significant improvement as compared to the existing state-of-the-art results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Identification of enhancers, promoters, and splice sites: The ENBED model outperforms existing methods in identifying these functional elements in DNA sequences.2. Identification of biological function annotations of genomic sequences: The ENBED model accurately predicts the biological functions of genomic sequences, including the presence of transcription factor binding sites and other regulatory elements.3. Recognition of sequences containing base call mismatches and insertion&#x2F;deletion errors: The ENBED model is able to identify sequences with base call errors and insertions&#x2F;deletions, which is an advantage over tokenization schemes that lose the ability to analyze at the byte level.4. Generating mutations of the Influenza virus using the encoder-decoder architecture and validating them against real-world observations: The ENBED model is used to generate mutations of the Influenza virus and the generated mutations are validated against real-world observations, demonstrating the potential of the model for drug resistance analysis and vaccine design.In each of these tasks, the ENBED model achieves significant improvement over existing state-of-the-art results, demonstrating its effectiveness in analyzing DNA sequences at the byte level.</details></li>
</ol>
<hr>
<h2 id="An-Operator-Learning-Framework-for-Spatiotemporal-Super-resolution-of-Scientific-Simulations"><a href="#An-Operator-Learning-Framework-for-Spatiotemporal-Super-resolution-of-Scientific-Simulations" class="headerlink" title="An Operator Learning Framework for Spatiotemporal Super-resolution of Scientific Simulations"></a>An Operator Learning Framework for Spatiotemporal Super-resolution of Scientific Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02328">http://arxiv.org/abs/2311.02328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentin Duruisseaux, Amit Chakraborty<br>for:* 这篇论文是为了解决高维度解析方法在数学模型中的计算限制问题，即使在小尺度下可以更好地捕捉实际动态。methods:* 这篇论文使用机器学习技术进行超Resolution，从低维度估算中重建高维度数值解。results:* 这篇论文提出了一种名为Super Resolution Operator Network（SROpNet）的新方法，可以在各种实际问题中提供更高精度的解决方案。<details>
<summary>Abstract</summary>
In numerous contexts, high-resolution solutions to partial differential equations are required to capture faithfully essential dynamics which occur at small spatiotemporal scales, but these solutions can be very difficult and slow to obtain using traditional methods due to limited computational resources. A recent direction to circumvent these computational limitations is to use machine learning techniques for super-resolution, to reconstruct high-resolution numerical solutions from low-resolution simulations which can be obtained more efficiently. The proposed approach, the Super Resolution Operator Network (SROpNet), frames super-resolution as an operator learning problem and draws inspiration from existing architectures to learn continuous representations of solutions to parametric differential equations from low-resolution approximations, which can then be evaluated at any desired location. In addition, no restrictions are imposed on the locations of (the fixed number of) spatiotemporal sensors at which the low-resolution approximations are provided, thereby enabling the consideration of a broader spectrum of problems arising in practice, for which many existing super-resolution approaches are not well-suited.
</details>
<details>
<summary>摘要</summary>
在许多Context中，高分辨率解决方案是必要的，以捕捉小时空尺度下的重要动力学行为。然而，使用传统方法可能会很慢和困难，因为计算资源有限。一种新的方向是使用机器学习技术来实现超解析，从低分辨率的 simulations 中重建高分辨率的数学解。我们的方法，称为 Super Resolution Operator Network (SROpNet)，将超解析视为一个操作学习问题， drew inspiration from existing architectures to learn continuous representations of solutions to parametric differential equations from low-resolution approximations, which can then be evaluated at any desired location.此外，我们不假设仅有一定数量的空间时间感知器的位置，因此可以考虑更多的实际问题，其中许多现有的超解析方法并不适用。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-of-Representations-for-Space-Generates-Multi-Modular-Grid-Cells"><a href="#Self-Supervised-Learning-of-Representations-for-Space-Generates-Multi-Modular-Grid-Cells" class="headerlink" title="Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells"></a>Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02316">http://arxiv.org/abs/2311.02316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rylan Schaeffer, Mikail Khona, Tzuhsuan Ma, Cristóbal Eyzaguirre, Sanmi Koyejo, Ila Rani Fiete</li>
<li>For: 解决空间问题的映射、定位和导航，哺乳动物的后代发展出了突出的空间表示。* Methods: 使用了四种方法：编码理论、动力系统、功能优化和监督深度学习。* Results: 提出了一种新的自监学习（SSL）框架，能够在不需要指导位信息或工程特定的读出表示的情况下，使多个网格细胞模块出现在训练后进行泛化。<details>
<summary>Abstract</summary>
To solve the spatial problems of mapping, localization and navigation, the mammalian lineage has developed striking spatial representations. One important spatial representation is the Nobel-prize winning grid cells: neurons that represent self-location, a local and aperiodic quantity, with seemingly bizarre non-local and spatially periodic activity patterns of a few discrete periods. Why has the mammalian lineage learnt this peculiar grid representation? Mathematical analysis suggests that this multi-periodic representation has excellent properties as an algebraic code with high capacity and intrinsic error-correction, but to date, there is no satisfactory synthesis of core principles that lead to multi-modular grid cells in deep recurrent neural networks. In this work, we begin by identifying key insights from four families of approaches to answering the grid cell question: coding theory, dynamical systems, function optimization and supervised deep learning. We then leverage our insights to propose a new approach that combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions and a network architecture - motivated from a normative perspective, without access to supervised position information or engineering of particular readout representations as needed in previous approaches. We show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks and emergent representations generalize well outside their training distribution. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.
</details>
<details>
<summary>摘要</summary>
为解决地图、位置Localization和导航问题，哺乳动物的演化历史中发展出了突出的空间表示。一种重要的空间表示是诺贝尔奖获得的格子细胞：神经元表示自己的位置，是一个本地和不规则的量，似乎具有奇怪的非本地和空间周期性的活动模式。为什么哺乳动物演化出这种怪异的格子表示？数学分析表明这些多周期的表示具有出色的算法代码性和内置的错误修复特性，但到目前为止，没有满意的核心原则的合成。在这项工作中，我们开始 by identifying key insights from four families of approaches to answering the grid cell question: coding theory, dynamical systems, function optimization and supervised deep learning. We then leverage our insights to propose a new approach that combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions and a network architecture - motivated from a normative perspective, without access to supervised position information or engineering of particular readout representations as needed in previous approaches. We show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks and emergent representations generalize well outside their training distribution. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks.
</details></li>
</ul>
<hr>
<h2 id="Heteroskedastic-Tensor-Clustering"><a href="#Heteroskedastic-Tensor-Clustering" class="headerlink" title="Heteroskedastic Tensor Clustering"></a>Heteroskedastic Tensor Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02306">http://arxiv.org/abs/2311.02306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchen Zhou, Yuxin Chen</li>
<li>for: 提取tensor数据中各个模式下的准确层次结构</li>
<li>methods: 使用一种新的特征值算法 called $\mathsf{Thresholded~Deflated\text{-}HeteroPCA}$，然后使用approx $k$-means来获取层次结构</li>
<li>results: 提供了一种可靠地实现tensor clustering的算法，并且在多种设置下比现有算法表现出更高的可靠性和精度。<details>
<summary>Abstract</summary>
Tensor clustering, which seeks to extract underlying cluster structures from noisy tensor observations, has gained increasing attention. One extensively studied model for tensor clustering is the tensor block model, which postulates the existence of clustering structures along each mode and has found broad applications in areas like multi-tissue gene expression analysis and multilayer network analysis. However, currently available computationally feasible methods for tensor clustering either are limited to handling i.i.d. sub-Gaussian noise or suffer from suboptimal statistical performance, which restrains their utility in applications that have to deal with heteroskedastic data and/or low signal-to-noise-ratio (SNR).   To overcome these challenges, we propose a two-stage method, named $\mathsf{High\text{-}order~HeteroClustering}$ ($\mathsf{HHC}$), which starts by performing tensor subspace estimation via a novel spectral algorithm called $\mathsf{Thresholded~Deflated\text{-}HeteroPCA}$, followed by approximate $k$-means to obtain cluster nodes. Encouragingly, our algorithm provably achieves exact clustering as long as the SNR exceeds the computational limit (ignoring logarithmic factors); here, the SNR refers to the ratio of the pairwise disparity between nodes to the noise level, and the computational limit indicates the lowest SNR that enables exact clustering with polynomial runtime. Comprehensive simulation and real-data experiments suggest that our algorithm outperforms existing algorithms across various settings, delivering more reliable clustering performance.
</details>
<details>
<summary>摘要</summary>
tensor clustering，它旨在从含有噪声的张量观察中提取下面的底层结构，在过去几年内获得了越来越多的关注。一种广泛研究的张量 clustering 模型是张量块模型，它假设每个模式中存在层次结构，并在多个领域，如多组织表达分析和多层网络分析中发现了广泛的应用。然而，目前可用的计算可行的张量 clustering 方法 Either 是处理 i.i.d. 子 Gaussian 噪声的限制，或者受到不佳的统计性能的限制，这限制了它们在应用中处理不均匀数据和/或低信号响应比例 (SNR) 的能力。为了解决这些挑战，我们提出了一种两stage方法，名为 $\mathsf{High\text{-}order~HeteroClustering}$ ($\mathsf{HHC}$)，它首先通过一种新的спектраль算法 called $\mathsf{Thresholded~Deflated\text{-}HeteroPCA}$ 进行张量子空间估计，然后使用approx $k$-means 获取集群节点。鼓舞人的是，我们的算法可以在 SNR 超过计算限制 (忽略对数因素) 的情况下，提供正确的划分结果，其中 SNR 是对比两个节点之间的差异与噪声水平的比率，而计算限制则是最低的 SNR 可以使用的计算时间的下限。在广泛的 simulate 和实际数据实验中，我们的算法比现有的算法在不同的设置下表现出更高的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Multi-Modal-Representation-Learning-for-Spark-Plug-Fault-Diagnosis"><a href="#Contrastive-Multi-Modal-Representation-Learning-for-Spark-Plug-Fault-Diagnosis" class="headerlink" title="Contrastive Multi-Modal Representation Learning for Spark Plug Fault Diagnosis"></a>Contrastive Multi-Modal Representation Learning for Spark Plug Fault Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02282">http://arxiv.org/abs/2311.02282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ardavan Modarres, Vahid Mohammad-Zadeh Eivaghi, Mahdi Aliyari Shoorehdeli, Ashkan Moosavian</li>
<li>for: 这个研究旨在提高工业设备状态监控中的条件监控，因为单一感知量不能提供足够的信息，而且单一感知量的噪音会导致误导。因此，需要一个有效的数据融合策略。</li>
<li>methods: 这个研究使用了一种具有对比学习概念的Denosing Multi-Modal Autoencoder，并且首次应用了这种方法在机器健康监控领域中。这种方法不仅能够充分融合多种感知量（或视角）的数据，而且可以在测试时将一个视角 omitted 而不会影响性能，或者甚至不需要将任何视角 omitted。</li>
<li>results: 这个研究的结果显示，使用了Denosing Multi-Modal Autoencoder的方法可以实现高效的多感知量融合，并且可以在感知量失效时继续运行，不需要更改现有的感知量组态。此外，这种方法可以实现更cost-effective的状态监控系统，不需要增加更多的感知量。<details>
<summary>Abstract</summary>
Due to the incapability of one sensory measurement to provide enough information for condition monitoring of some complex engineered industrial mechanisms and also for overcoming the misleading noise of a single sensor, multiple sensors are installed to improve the condition monitoring of some industrial equipment. Therefore, an efficient data fusion strategy is demanded. In this research, we presented a Denoising Multi-Modal Autoencoder with a unique training strategy based on contrastive learning paradigm, both being utilized for the first time in the machine health monitoring realm. The presented approach, which leverages the merits of both supervised and unsupervised learning, not only achieves excellent performance in fusing multiple modalities (or views) of data into an enriched common representation but also takes data fusion to the next level wherein one of the views can be omitted during inference time with very slight performance reduction, or even without any reduction at all. The presented methodology enables multi-modal fault diagnosis systems to perform more robustly in case of sensor failure occurrence, and one can also intentionally omit one of the sensors (the more expensive one) in order to build a more cost-effective condition monitoring system without sacrificing performance for practical purposes. The effectiveness of the presented methodology is examined on a real-world private multi-modal dataset gathered under non-laboratory conditions from a complex engineered mechanism, an inline four-stroke spark-ignition engine, aiming for spark plug fault diagnosis. This dataset, which contains the accelerometer and acoustic signals as two modalities, has a very slight amount of fault, and achieving good performance on such a dataset promises that the presented method can perform well on other equipment as well.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Machine-learning’s-own-Industrial-Revolution"><a href="#Machine-learning’s-own-Industrial-Revolution" class="headerlink" title="Machine learning’s own Industrial Revolution"></a>Machine learning’s own Industrial Revolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02278">http://arxiv.org/abs/2311.02278</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Yuan Luo, Song Han, Jingjing Liu</li>
<li>for: 本研究目的是帮助机器学习完成自己的工业革命，以满足越来越高的企业需求和广泛的行业。</li>
<li>methods: 本文提出了一种新的工业革命模型，用于帮助机器学习实现自己的目标，包括标准化和自动化生产网络。</li>
<li>results: 本文预测了机器学习的未来发展趋势，并提出了新的机会和挑战，以帮助机器学习在广泛的行业中得到更广泛的应用和利用。<details>
<summary>Abstract</summary>
Machine learning is expected to enable the next Industrial Revolution. However, lacking standardized and automated assembly networks, ML faces significant challenges to meet ever-growing enterprise demands and empower broad industries. In the Perspective, we argue that ML needs to first complete its own Industrial Revolution, elaborate on how to best achieve its goals, and discuss new opportunities to enable rapid translation from ML's innovation frontier to mass production and utilization.
</details>
<details>
<summary>摘要</summary>
机器学习预计会推动下一个工业革命。然而，由于缺乏标准化和自动化的组装网络，机器学习面临着满足永不减少的企业需求和推广到多个行业的重大挑战。在我们的视角中，机器学习需要先完成自己的工业革命，详细说明如何最好实现目标，并讨论新的机会来快速将机器学习的创新前沿翻译成大规模生产和应用。Note: Simplified Chinese is used here, as it is more widely used in mainland China and is the standard language for most online content. Traditional Chinese is used in Taiwan and Hong Kong, and it has some differences in grammar and vocabulary compared to Simplified Chinese.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/04/cs.LG_2023_11_04/" data-id="cloojsmir00s0re886jckbmp6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/04/eess.SP_2023_11_04/" class="article-date">
  <time datetime="2023-11-04T08:00:00.000Z" itemprop="datePublished">2023-11-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/04/eess.SP_2023_11_04/">eess.SP - 2023-11-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="On-Learning-the-Distribution-of-a-Random-Spatial-Field-in-a-Location-Unaware-Mobile-Sensing-Setup"><a href="#On-Learning-the-Distribution-of-a-Random-Spatial-Field-in-a-Location-Unaware-Mobile-Sensing-Setup" class="headerlink" title="On Learning the Distribution of a Random Spatial Field in a Location-Unaware Mobile Sensing Setup"></a>On Learning the Distribution of a Random Spatial Field in a Location-Unaware Mobile Sensing Setup</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02464">http://arxiv.org/abs/2311.02464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meera Pai</li>
<li>for: 本研究的目的是学习一个固定一 dimensional 路径上的空间时间场的统计分布，在absence of location information。</li>
<li>methods: 本研究使用了移动感知设备采集空间时间场的样本，并提出了一些简单的假设来学习场的统计分布。</li>
<li>results: 研究表明，可以使用移动感知设备采集的样本来学习空间时间场的统计分布，并且提供了一系列的分析和实验结果来支持这一结论。<details>
<summary>Abstract</summary>
In applications like environment monitoring and pollution control, physical quantities are modeled by spatio-temporal fields. It is of interest to learn the statistical distribution of such fields as a function of space, time or both. In this work, our aim is to learn the statistical distribution of a spatio-temporal field along a fixed one dimensional path, as a function of spatial location, in the absence of location information. Spatial field analysis, commonly done using static sensor networks is a well studied problem in literature. Recently, due to flexibility in setting the spatial sampling density and low hardware cost, owing to larger spatial coverage, mobile sensors are used for this purpose. The main challenge in using mobile sensors is their location uncertainty. Obtaining location information of samples requires additional hardware and cost. So, we consider the case when the spatio-temporal field along the fixed length path is sampled using a simple mobile sensing device that records field values while traversing the path without any location information. We ask whether it is possible to learn the statistical distribution of the field, as a function of spatial location, using samples from the location-unaware mobile sensor under some simple assumptions on the field. We answer this question in affirmative and provide a series of analytical and experimental results to support our claim.
</details>
<details>
<summary>摘要</summary>
在环境监测和污染控制应用中，物理量是通过空间-时间场的模拟来表示。我们的目标是在固定一个一维路径上学习这个场的统计分布，以空间位置为变量。在文献中广泛研究的空间场分析中，通常使用静止感知网络进行检测。然而，由于可以自由设置空间抽样密度以及低硬件成本，由于更大的空间覆盖率，移动感知设备在最近几年中变得越来越受欢迎。然而，移动感知设备的位置不确定性成为主要挑战。为了获取样本的位置信息，需要额外的硬件和成本。因此，我们考虑了在固定一个一维路径上，使用简单的移动感知设备记录场值，而不提供位置信息的情况下，是否可以学习场的统计分布，以空间位置为变量。我们的答案是可以，并且提供了一系列的分析和实验结果来支持我们的说法。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Imperfect-Resolution-of-Near-Field-Beamforming-A-Hybrid-NOMA-Perspective"><a href="#Utilizing-Imperfect-Resolution-of-Near-Field-Beamforming-A-Hybrid-NOMA-Perspective" class="headerlink" title="Utilizing Imperfect Resolution of Near-Field Beamforming: A Hybrid-NOMA Perspective"></a>Utilizing Imperfect Resolution of Near-Field Beamforming: A Hybrid-NOMA Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02451">http://arxiv.org/abs/2311.02451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiguo Ding, H. Vincent Poor</li>
<li>for: 本研究旨在利用近场通信中的不完全解像程度来提高无线网络吞吐和连接稳定性。</li>
<li>methods: 该研究提出了一种混合非对准多ступ通信（NOMA）传输策略，使用预配置的近场扫描器来服务更多的用户。然后通过不同的顺序扫描取消技术来解决能量消耗最小化问题。</li>
<li>results: 分析和 simulate结果表明，随着近场解像程度的提高，hybrid NOMA传输策略可以提高无线网络的吞吐和连接稳定性。<details>
<summary>Abstract</summary>
This letter studies how the imperfect resolution of near-field beamforming, the key feature of near-field communications, can be used to improve the throughput and connectivity of wireless networks. In particular, a hybrid non-orthogonal multiple access (NOMA) transmission strategy is developed to use preconfigured near-field beams for serving additional users. An energy consumption minimization problem is first formulated and then solved by using different successive interference cancellation strategies. Both analytical and simulation results are presented to illustrate the impact of the resolution of near-field beamforming on the design of hybrid NOMA transmission.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "near-field beamforming" is translated as "近场扩散" (jìn chǎng kuò xiǎn)* "hybrid non-orthogonal multiple access" is translated as "混合非正交多接入" (hùn hǎi fēi zhèng jiāng duō yù)* "preconfigured near-field beams" is translated as "预先配置的近场扩散" (xiù xiān bèng jī de jìn chǎng kuò xiǎn)* "energy consumption minimization" is translated as "能量消耗最小化" (néng yàng xiāo hóu zuì xiǎo)* "successive interference cancellation" is translated as "successive interference cancellation" (成功ive kancel)Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Quantized-but-uncoded-Distributed-Detection-QDD-with-Unreliable-Reporting-Channels"><a href="#Quantized-but-uncoded-Distributed-Detection-QDD-with-Unreliable-Reporting-Channels" class="headerlink" title="Quantized-but-uncoded Distributed Detection (QDD) with Unreliable Reporting Channels"></a>Quantized-but-uncoded Distributed Detection (QDD) with Unreliable Reporting Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02447">http://arxiv.org/abs/2311.02447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Cao, Ramanarayanan Viswanathan</li>
<li>for: 本研究旨在提出一种新的分布式检测方法，即量化但未编码的分布式检测（QDD），以提高传输能力和复杂性。</li>
<li>methods: 本研究使用量化但未编码的方法，其中每个感知器对其完整的观测数据进行量化，然后将归一化后的值传输到总Integration Center（FC）。</li>
<li>results: 比较CDD和QDD两种方法，本研究发现QDD在传输能力限制下表现更好，但是需要更多的参数选择。此外，在独立观测下，QDD保持了CDD中的必需条件，即最佳感知器决策规则是likelihood ratio quantizers（LRQ），不受通信渠道条件影响。<details>
<summary>Abstract</summary>
Distributed detection primarily centers around two approaches: Unquantized Distributed Detection (UDD), where each sensor reports its complete observation to the fusion center (FC), and quantized-and-Coded DD (CDD), where each sensor first partitions the observation space and then reports to the FC a codeword. In this paper, we introduce Quantized-but-uncoded DD (QDD), where each sensor, after quantization, transmits a summarized value, instead of a codeword, to the FC. We show that QDD well adapts to the constraint of transmission power when compared to CDD, albeit with increased complexity in parameter selection. Moreover, we establish that, in the presence of independent observations, QDD upholds a necessary condition inherent in CDD. Specifically, the optimal sensor decision rules are the likelihood ratio quantizers (LRQ), irrelevant to the channel conditions. In the context of a single-sensor scenario involving binary decision at the sensor, we find that the optimal sensor rule in QDD is in general no longer ``channel blind", a feature presented in CDD. In addition, we compare these systems numerically under the same transmission power and bandwidth, while assuming additive white Gaussian noise (AWGN) in both sensing and reporting stages. Finally, we present some potential directions for future research.
</details>
<details>
<summary>摘要</summary>
主要分布检测方法有两种：不量化分布检测（UDD），每个感知器都直接将完整的观测报告给归一化中心（FC），以及量化编码分布检测（CDD），每个感知器首先将观测空间分割，然后向FC报告一个编码word。在本文中，我们介绍了量化但未编码的分布检测（QDD），每个感知器，经过量化，将减少值传输到FC，而不是编码word。我们表明，QDD在传输功率限制下比CDD更适应，尽管它增加了参数选择的复杂性。此外，我们证明，在独立观测下，QDD保持了CDD中的必需条件。具体来说，感知器的优化决策规则是 likelihood ratio quantizers（LRQ），不受通信条件影响。在单感知器场景中，我们发现QDD的优化决策规则不再是“通信盲目”的，这是CDD中的特点。此外，我们在同传输功率和宽度下，对这些系统进行了数值比较，假设感知和报告阶段都存在添加itive white Gaussian noise（AWGN）。最后，我们提出了未来研究的一些可能的方向。
</details></li>
</ul>
<hr>
<h2 id="PIPO-Net-A-Penalty-based-Independent-Parameters-Optimization-Deep-Unfolding-Network"><a href="#PIPO-Net-A-Penalty-based-Independent-Parameters-Optimization-Deep-Unfolding-Network" class="headerlink" title="PIPO-Net: A Penalty-based Independent Parameters Optimization Deep Unfolding Network"></a>PIPO-Net: A Penalty-based Independent Parameters Optimization Deep Unfolding Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02443">http://arxiv.org/abs/2311.02443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiumei Li, Zhijie Zhang, Huang Bai, Ljubiša Stanković, Junpeng Hao, Junmei Sun</li>
<li>for: 用于重建压缩感知图像</li>
<li>methods: 使用罚函数优化策略和高频补充块</li>
<li>results: 实现高精度重建压缩感知图像<details>
<summary>Abstract</summary>
Compressive sensing (CS) has been widely applied in signal and image processing fields. Traditional CS reconstruction algorithms have a complete theoretical foundation but suffer from the high computational complexity, while fashionable deep network-based methods can achieve high-accuracy reconstruction of CS but are short of interpretability. These facts motivate us to develop a deep unfolding network named the penalty-based independent parameters optimization network (PIPO-Net) to combine the merits of the above mentioned two kinds of CS methods. Each module of PIPO-Net can be viewed separately as an optimization problem with respective penalty function. The main characteristic of PIPO-Net is that, in each round of training, the learnable parameters in one module are updated independently from those of other modules. This makes the network more flexible to find the optimal solutions of the corresponding problems. Moreover, the mean-subtraction sampling and the high-frequency complementary blocks are developed to improve the performance of PIPO-Net. Experiments on reconstructing CS images demonstrate the effectiveness of the proposed PIPO-Net.
</details>
<details>
<summary>摘要</summary>
压缩感知（CS）在信号处理和图像处理领域广泛应用。传统的CS重建算法具有完善的理论基础，但计算复杂性高；而时尚的深度网络基于方法可以实现高精度的CS重建，但缺乏可读性。这些因素激发我们开发一种名为罚函数基本独立参数优化网络（PIPO-Net）的深度 unfolding 网络，将上述两种CS方法的优点结合起来。PIPO-Net 中每个模块可以视为一个优化问题，每个模块的学习参数在训练过程中独立地更新。这使得网络更加灵活地找到相应的优化解决方案。此外，我们还提出了mean-subtraction sampling和高频补充块来提高PIPO-Net的性能。实验表明，提议的PIPO-Net 可以有效地重建CS图像。
</details></li>
</ul>
<hr>
<h2 id="SplitMAC-Wireless-Split-Learning-over-Multiple-Access-Channels"><a href="#SplitMAC-Wireless-Split-Learning-over-Multiple-Access-Channels" class="headerlink" title="SplitMAC: Wireless Split Learning over Multiple Access Channels"></a>SplitMAC: Wireless Split Learning over Multiple Access Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02405">http://arxiv.org/abs/2311.02405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonjung Kim, Yongjeong Oh, Yo-Seb Jeon</li>
<li>For: 本文提出了一种新的分解学习（SL）框架，称为SplitMAC，它可以降低SL的延迟时间，通过同时在多个访问通道上传输多个设备的混合数据和设备 сторо面模型。* Methods: 本文使用分 grouping 策略，将设备分为多个组，并让同一组内的设备同时传输其混合数据和设备 сторо面模型。优化问题是将设备分配到最佳的组，以最小化SL延迟时间。* Results:  simulations 表明，我们的SL框架，尤其是使用提出的设备分配算法，可以在各种信号噪响比（SNR）场景下减少SL延迟时间。<details>
<summary>Abstract</summary>
This paper presents a novel split learning (SL) framework, referred to as SplitMAC, which reduces the latency of SL by leveraging simultaneous uplink transmission over multiple access channels. The key strategy is to divide devices into multiple groups and allow the devices within the same group to simultaneously transmit their smashed data and device-side models over the multiple access channels. The optimization problem of device grouping to minimize SL latency is formulated, and the benefit of device grouping in reducing the uplink latency of SL is theoretically derived. By examining a two-device grouping case, two asymptotically-optimal algorithms are devised for device grouping in low and high signal-to-noise ratio (SNR) scenarios, respectively, while providing proofs of their optimality. By merging these algorithms, a near-optimal device grouping algorithm is proposed to cover a wide range of SNR. Simulation results demonstrate that our SL framework with the proposed device grouping algorithm is superior to existing SL frameworks in reducing SL latency.
</details>
<details>
<summary>摘要</summary>
Two asymptotically-optimal algorithms are proposed for device grouping in low and high signal-to-noise ratio (SNR) scenarios, respectively. These algorithms are proven to be optimal, and by merging them, a near-optimal device grouping algorithm is proposed to cover a wide range of SNR. Simulation results show that the proposed SL framework with the device grouping algorithm is superior to existing SL frameworks in reducing SL latency.In simplified Chinese, the text can be translated as:这篇论文提出了一种新的分布式学习（SL）框架，称为SplitMAC，它可以降低SL的延迟时间。这个框架的关键策略是将设备分成多个组，并让同一组的设备同时传输压缩数据和设备侧模型通过多个访问通道。将设备分组优化问题以减少SL延迟时间被形式化，并证明了设备分组可以减少上行延迟时间。对于低和高信号噪比（SNR）两种场景，分别提出了两种极似优算法，其中一种是适用于低SNR场景，另一种是适用于高SNR场景。这两种算法都是可数的优化算法，并且将它们合并可以得到一个近似优化的设备分组算法，以覆盖各种SNR场景。实验结果表明，提出的SL框架以及设备分组算法都是现有SL框架的改进版本，可以更好地减少SL延迟时间。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Reflecting-Surface-Aided-Wireless-Communication-with-Movable-Elements"><a href="#Intelligent-Reflecting-Surface-Aided-Wireless-Communication-with-Movable-Elements" class="headerlink" title="Intelligent Reflecting Surface-Aided Wireless Communication with Movable Elements"></a>Intelligent Reflecting Surface-Aided Wireless Communication with Movable Elements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02376">http://arxiv.org/abs/2311.02376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guojie Hu, Qingqing Wu, Dognhui Xu, Kui Xu, Jiangbo Si, Yunlong Cai, Naofal Al-Dhahir<br>for: 这个研究旨在提高通信性能的智能镜面技术 (IRS) 中，为了降低生产和控制成本，采用独立阶段调校 (DPS)，但这种设置对于通过总RIician折射而具有问题。methods: 我们在这篇论文中设计了优化的非均匀DPS，以获得满意的性能水平。我们面对的主要挑战是当IRS元素的位置固定时，可能会出现各个构成元素间的偏移，导致不同的偏移模式，从而导致生产成本增加，特别是当IRS元素的数量很大时。results: 我们透过 simulations 表明，我们的提案可以与竞争 benchmark 相比，实现系统性能的明显提高。<details>
<summary>Abstract</summary>
Intelligent reflecting surface (IRS) has been recognized as a powerful technology for boosting communication performance. To reduce manufacturing and control costs, it is preferable to consider discrete phase shifts (DPSs) for IRS, which are set by default as uniformly distributed in the range of $[ - \pi,\pi )$ in the literature. Such setting, however, cannot achieve a desirable performance over the general Rician fading where the channel phase concentrates in a narrow range with a higher probability. Motivated by this drawback, we in this paper design optimal non-uniform DPSs for IRS to achieve a desirable performance level. The fundamental challenge is the \textit{possible offset in phase distribution across different cascaded source-element-destination channels}, if adopting conventional IRS where the position of each element is fixed. Such phenomenon leads to different patterns of optimal non-uniform DPSs for each IRS element and thus causes huge manufacturing costs especially when the number of IRS elements is large. Driven by the recently emerging fluid antenna system (or movable antenna technology), we demonstrate that if the position of each IRS element can be flexibly adjusted, the above phase distribution offset can be surprisingly eliminated, leading to the same pattern of DPSs for each IRS element. Armed with this, we then determine the form of unified non-uniform DPSs based on a low-complexity iterative algorithm. Simulations show that our proposed design significantly improves the system performance compared to competitive benchmarks.
</details>
<details>
<summary>摘要</summary>
智能反射表面（IRS）已被认为是一种强大的通信性能提升技术。为了降低生产和控制成本，它是将独立阶段调整（DPS）设置为默认值，即在 $[-\pi, \pi)$ 中 uniformly 分布的文献中的偏好。但这个设置无法在一般的雷电折射中 achieve  Desirable 性能，因为通道频率偏集在狭窄的范围中，具有更高的几率。驱动了这个缺陷，我们在这篇论文中设计了优化的非均匀DPS，以 achieve  Desirable 性能水平。基本挑战在于 possible 频率分布偏移 across different 缝合源-元素-目标通道，如果采用传统的 IRS，则每个 IRS 元素的位置固定。这个现象导致每个 IRS 元素的优化非均匀DPS 具有不同的几何结构，导致生产成本尤其高于当 IRS 元素的数量较多。驱动了最近发展的流体天线系统（或可动天线技术），我们示出了如果每个 IRS 元素的位置可以灵活地调整，这个偏移问题可以 unexpectedly 消除，导致每个 IRS 元素的 DPS 具有同样的模式。 armed  with  this，我们then 决定了非均匀 DPS 的形式，基于一种低复杂度的迭代算法。模拟结果显示，我们的提案对于竞争性能标准的优化做出了显著改善。
</details></li>
</ul>
<hr>
<h2 id="A-Physics-based-Machine-Learning-Model-to-characterize-Room-Temperature-Semiconductor-Detectors-in-3D"><a href="#A-Physics-based-Machine-Learning-Model-to-characterize-Room-Temperature-Semiconductor-Detectors-in-3D" class="headerlink" title="A Physics based Machine Learning Model to characterize Room Temperature Semiconductor Detectors in 3D"></a>A Physics based Machine Learning Model to characterize Room Temperature Semiconductor Detectors in 3D</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02290">http://arxiv.org/abs/2311.02290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srutarshi Banerjee, Miesher Rodrigues, Manuel Ballester, Alexander H. Vija, Aggelos K. Katsaggelos</li>
<li>For: The paper aims to develop a novel physics-based machine learning (PBML) model for characterizing room temperature semiconductor radiation detectors (RTSDs) in 3D space.* Methods: The PBML model is based on a discretized sub-pixelated 3D volume, and it considers the different physics-based charge transport properties such as drift, trapping, detrapping, and recombination of charges as trainable model weights. The model uses backpropagation to determine the trainable weights and optimize the loss function.* Results: The proposed PBML model is the first to characterize a full 3D charge transport model of RTSDs, and it can accurately determine the trainable weights that represent the one-to-one relation to the actual physical charge transport properties in a voxelized detector.<details>
<summary>Abstract</summary>
Room temperature semiconductor radiation detectors (RTSD) for X-ray and gamma-ray detection are vital tools for medical imaging, astrophysics and other applications. CdZnTe (CZT) has been the main RTSD for more than three decades with desired detection properties. In a typical pixelated configuration, CZT have electrodes on opposite ends. For advanced event reconstruction algorithms at sub-pixel level, detailed characterization of the RTSD is required in three dimensional (3D) space. However, 3D characterization of the material defects and charge transport properties in the sub-pixel regime is a labor-intensive process with skilled manpower and novel experimental setups. Presently, state-of-art characterization is done over the bulk of the RTSD considering homogenous properties. In this paper, we propose a novel physics based machine learning (PBML) model to characterize the RTSD over a discretized sub-pixelated 3D volume which is assumed. Our novel approach is the first to characterize a full 3D charge transport model of the RTSD. In this work, we first discretize the RTSD between a pixelated electrodes spatially in 3D - x, y, and z. The resulting discretizations are termed as voxels in 3D space. In each voxel, the different physics based charge transport properties such as drift, trapping, detrapping and recombination of charges are modeled as trainable model weights. The drift of the charges considers second order non-linear motion which is observed in practice with the RTSDs. Based on the electron-hole pair injections as input to the PBML model, and signals at the electrodes, free and trapped charges (electrons and holes) as outputs of the model, the PBML model determines the trainable weights by backpropagating the loss function. The trained weights of the model represents one-to-one relation to that of the actual physical charge transport properties in a voxelized detector.
</details>
<details>
<summary>摘要</summary>
室温半导体辐射探测器（RTSD）在医学影像、astrophysics和其他应用中是非常重要的工具。 Cadmium zinc telluride（CZT）在过去三十年中一直是主要的RTSD，具有欢得的探测性能。在常见的像素化配置中，CZT有电极在两端。为了在像素水平上使用高级事件重建算法，RTSD的详细三维（3D）特性的Characterization是必要的。然而，在sub-像素级别上对材料缺陷和电子传输性能的3DCharacterization是一项劳动密集的过程，需要专业人员和特殊的实验设备。现在，状态机器的Characterization都是基于整体RTSD的假设，忽略了物理性的细节。在这篇论文中，我们提出了一种新的物理学基本机器学习（PBML）模型，用于 caracterizing RTSD的3D电子传输模型。我们首先将RTSD在三维空间中分割成像素化的电极，并将每个像素称为voxel。在每个voxel中，我们模型了不同的物理学基本的电子传输特性，如漂移、固定、释放和 recombination of charges。这些模型参数被视为可训练的模型参数。基于电子-引起对的插入和电极上的信号，以及free和固定电荷（电子和洞）的输出，PBML模型通过反射损失函数来确定模型参数。训练后，模型的参数表示了RTSD的实际物理电子传输特性的一对一关系。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/04/eess.SP_2023_11_04/" data-id="cloojsmri01cere880785h07d" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/87/">87</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">67</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
