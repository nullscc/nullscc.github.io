
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/2/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/eess.IV_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/24/eess.IV_2023_08_24/">eess.IV - 2023-08-24 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Learned-Local-Attention-Maps-for-Synthesising-Vessel-Segmentations"><a href="#Learned-Local-Attention-Maps-for-Synthesising-Vessel-Segmentations" class="headerlink" title="Learned Local Attention Maps for Synthesising Vessel Segmentations"></a>Learned Local Attention Maps for Synthesising Vessel Segmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12861">http://arxiv.org/abs/2308.12861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Deo, Rodrigo Bonazzola, Haoran Dou, Yan Xia, Tianyou Wei, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila</li>
<li>for: 这个论文的目的是用MRA进行血管视化，但MRA不是常见的成像方法，因此需要一种方法来合成血管分割结果。</li>
<li>methods: 这个论文使用了一种编码器-解码器模型，用于从T2 MRI中提取血管分割结果。该模型使用了两个阶段的多目标学习方法，以捕捉全局和本地特征。它还使用了学习的本地注意力图，以便只从T2 MRI中提取与合成CoW血管相关的信息。</li>
<li>results: 在测试中，这个模型可以从T2 MRI中生成高质量的血管分割结果，其中的Dice分数为0.79±0.03，高于现有的分割网络，如转换器U-Net（0.71±0.04）和nnU-net（0.68±0.05）。主要区别在于生成的CoW血管段的分辨率更高，特别是后circulation。<details>
<summary>Abstract</summary>
Magnetic resonance angiography (MRA) is an imaging modality for visualising blood vessels. It is useful for several diagnostic applications and for assessing the risk of adverse events such as haemorrhagic stroke (resulting from the rupture of aneurysms in blood vessels). However, MRAs are not acquired routinely, hence, an approach to synthesise blood vessel segmentations from more routinely acquired MR contrasts such as T1 and T2, would be useful. We present an encoder-decoder model for synthesising segmentations of the main cerebral arteries in the circle of Willis (CoW) from only T2 MRI. We propose a two-phase multi-objective learning approach, which captures both global and local features. It uses learned local attention maps generated by dilating the segmentation labels, which forces the network to only extract information from the T2 MRI relevant to synthesising the CoW. Our synthetic vessel segmentations generated from only T2 MRI achieved a mean Dice score of $0.79 \pm 0.03$ in testing, compared to state-of-the-art segmentation networks such as transformer U-Net ($0.71 \pm 0.04$) and nnU-net($0.68 \pm 0.05$), while using only a fraction of the parameters. The main qualitative difference between our synthetic vessel segmentations and the comparative models was in the sharper resolution of the CoW vessel segments, especially in the posterior circulation.
</details>
<details>
<summary>摘要</summary>
磁共振成像（MRA）是一种成像血管的方法，可以用于诊断和评估血栓roke的风险。然而，MRA不是Routine获得的，因此一种能够从T1和T2磁共振图像中合成血管分割的方法会很有用。我们提出了一种encoder-decoder模型，可以从T2 MRI中生成主要脑动脉的分割。我们使用了两个阶段的多目标学习方法，包括全局和局部特征。它使用学习的本地注意力地图，从T2 MRI中提取与合成CoW的信息。我们的合成血管分割从T2 MRI中获得的Mean Dice分数为$0.79\pm0.03$,比如state-of-the-art segmentation网络（如transformer U-Net和nnU-net）高出了一些。主要的区别在于CoW血管段的分辨率更加高，特别是后 circulation。
</details></li>
</ul>
<hr>
<h2 id="Achromatic-imaging-systems-with-flat-lenses-enabled-by-deep-learning"><a href="#Achromatic-imaging-systems-with-flat-lenses-enabled-by-deep-learning" class="headerlink" title="Achromatic imaging systems with flat lenses enabled by deep learning"></a>Achromatic imaging systems with flat lenses enabled by deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12776">http://arxiv.org/abs/2308.12776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roy Maman, Eitan Mualem, Noa Mazurski, Jacob Engelberg, Uriel Levy</li>
<li>for: 这篇论文旨在探讨使用凹型镜代替传统凸型镜元件的现代光学系统中，使用凹型镜带来的颜色偏振问题的解决方案。</li>
<li>methods: 作者使用了一种基于深度学习的方法，使用凹型镜拍摄的颜色户外图像数据集来对凹型镜的颜色偏振进行修正。</li>
<li>results: 作者的实验结果表明，使用这种方法可以在整个可见光谱中实现高质量的图像捕捉，并且在量化方面也有显著的提升，PSNR和SSIM分别达到了45.5dB和0.93。这些结果开启了使用凹型镜在高级多色光学捕捉系统中的应用前景。<details>
<summary>Abstract</summary>
Motivated by their great potential to reduce the size, cost and weight, flat lenses, a category that includes diffractive lenses and metalenses, are rapidly emerging as key components with the potential to replace the traditional refractive optical elements in modern optical systems. Yet, the inherently strong chromatic aberration of these flat lenses is significantly impairing their performance in systems based on polychromatic illumination or passive ambient light illumination, stalling their widespread implementation. Hereby, we provide a promising solution and demonstrate high quality imaging based on flat lenses over the entire visible spectrum. Our approach is based on creating a novel dataset of color outdoor images taken with our flat lens and using this dataset to train a deep-learning model for chromatic aberrations correction. Based on this approach we show unprecedented imaging results not only in terms of qualitative measures but also in the quantitative terms of the PSNR and SSIM scores of the reconstructed images. The results pave the way for the implementation of flat lenses in advanced polychromatic imaging systems.
</details>
<details>
<summary>摘要</summary>
驱动了它们的巨大潜力来减少大小、成本和重量，扁镜，包括扁Diffractive镜和金属镜，在现代光学系统中逐渐emerge为关键组件，替代传统的反射光学元件。然而，扁镜的自然强烈多色偏振问题在基于多色照明或普通 ambient light照明的系统中，对其性能产生了很大的障碍，使其广泛实施受阻。我们提供了一个有优势的解决方案，通过创建一个新的彩色户外图像数据集，使用这个数据集来训练深度学习模型来修正扁镜的多色偏振。根据这种方法，我们展示了具有很高质量的图像成像结果，不仅在质量上有显著的提升，还在量化上通过PSNR和SSIM分数来评估图像重建结果，得到了前所未有的成果。这些结果为扁镜在高级多色成像系统中的实施铺平了道路。
</details></li>
</ul>
<hr>
<h2 id="A-Study-of-Age-and-Sex-Bias-in-Multiple-Instance-Learning-based-Classification-of-Acute-Myeloid-Leukemia-Subtypes"><a href="#A-Study-of-Age-and-Sex-Bias-in-Multiple-Instance-Learning-based-Classification-of-Acute-Myeloid-Leukemia-Subtypes" class="headerlink" title="A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes"></a>A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12675">http://arxiv.org/abs/2308.12675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ario Sadafi, Matthias Hehr, Nassir Navab, Carsten Marr</li>
<li>for: 这个研究旨在探讨急性白血病（AML）分型中是否存在年龄和性别偏见，以实现临床决策和患者照顾中的可靠性和公平性。</li>
<li>methods: 这个研究使用多例学习（MIL）架构，对不同的性别差异和年龄偏见进行训练。以评估性别偏见的影响，我们评估了男女试料集的表现。为了评估年龄偏见，我们对训练资料中缺乏的年龄组进行测试。</li>
<li>results: 我们发现，女性患者更容易受到性别偏见的影响，而certain age groups，例如72-86岁的患者，受到年龄偏见的影响。确保训练资料的多元性是生成可靠和公平的AML分型结果的关键。<details>
<summary>Abstract</summary>
Accurate classification of Acute Myeloid Leukemia (AML) subtypes is crucial for clinical decision-making and patient care. In this study, we investigate the potential presence of age and sex bias in AML subtype classification using Multiple Instance Learning (MIL) architectures. To that end, we train multiple MIL models using different levels of sex imbalance in the training set and excluding certain age groups. To assess the sex bias, we evaluate the performance of the models on male and female test sets. For age bias, models are tested against underrepresented age groups in the training data. We find a significant effect of sex and age bias on the performance of the model for AML subtype classification. Specifically, we observe that females are more likely to be affected by sex imbalance dataset and certain age groups, such as patients with 72 to 86 years of age with the RUNX1::RUNX1T1 genetic subtype, are significantly affected by an age bias present in the training data. Ensuring inclusivity in the training data is thus essential for generating reliable and equitable outcomes in AML genetic subtype classification, ultimately benefiting diverse patient populations.
</details>
<details>
<summary>摘要</summary>
精准分类AML分型是至关重要的临床决策和患者护理。本研究探讨AML分型分类中可能存在年龄和性别偏见的问题，使用多个实例学习（MIL）架构。为此，我们在培育不同性别倾向和不同年龄组的模型时进行训练多个MIL模型。以评估性别偏见，我们对男女测试集进行评估模型的性能。为年龄偏见，我们对培育数据中下 representations of age groups进行测试。我们发现，女性更容易受到数据性别偏见的影响，而certain age groups，如72-86岁的患者，受到培育数据中的年龄偏见的影响。因此，保证培育数据的多样性是AML分型分类中的关键，以实现多元患者群体的可靠和公平结果。
</details></li>
</ul>
<hr>
<h2 id="SCP-Spherical-Coordinate-based-Learned-Point-Cloud-Compression"><a href="#SCP-Spherical-Coordinate-based-Learned-Point-Cloud-Compression" class="headerlink" title="SCP: Spherical-Coordinate-based Learned Point Cloud Compression"></a>SCP: Spherical-Coordinate-based Learned Point Cloud Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12535">http://arxiv.org/abs/2308.12535</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luoao-kddi/SCP">https://github.com/luoao-kddi/SCP</a></li>
<li>paper_authors: Ao Luo, Linxin Song, Keisuke Nonaka, Kyohei Unno, Heming Sun, Masayuki Goto, Jiro Katto</li>
<li>for: 该论文主要探讨了一种基于圆柱坐标系的学习点云压缩方法（SCP），以便全面利用点云中各种圆形特征和方向强相关性，提高压缩率和重建质量。</li>
<li>methods: 该方法基于点云中的圆柱坐标系，并采用多层 Octree 结构来降低远区域重建误差。具有universal性，可应用于多种学习点云压缩技术。</li>
<li>results: 实验结果显示，SCP 方法可以与前期state-of-the-art方法比肩，并且在点对点 PSNR BD-Rate 指标上达到29.14%的提高。<details>
<summary>Abstract</summary>
In recent years, the task of learned point cloud compression has gained prominence. An important type of point cloud, the spinning LiDAR point cloud, is generated by spinning LiDAR on vehicles. This process results in numerous circular shapes and azimuthal angle invariance features within the point clouds. However, these two features have been largely overlooked by previous methodologies. In this paper, we introduce a model-agnostic method called Spherical-Coordinate-based learned Point cloud compression (SCP), designed to leverage the aforementioned features fully. Additionally, we propose a multi-level Octree for SCP to mitigate the reconstruction error for distant areas within the Spherical-coordinate-based Octree. SCP exhibits excellent universality, making it applicable to various learned point cloud compression techniques. Experimental results demonstrate that SCP surpasses previous state-of-the-art methods by up to 29.14% in point-to-point PSNR BD-Rate.
</details>
<details>
<summary>摘要</summary>
近年来，学习点云压缩任务已经受到重视。一种重要的点云数据类型是旋转 LiDAR 点云，通常由旋转 LiDAR 设备在车辆上生成。这个过程会生成很多圆形和方位角度不变特征，但这些特征在之前的方法中受到了忽略。在本文中，我们提出了一种无模型的方法called Spherical-Coordinate-based learned Point cloud compression (SCP)，旨在完全利用上述特征。此外，我们还提议了一种多级 Octree 来 Mitigate the reconstruction error for distant areas within the Spherical-coordinate-based Octree。SCP 具有优秀的通用性，可以应用于多种学习点云压缩技术。实验结果表明，SCP 可以与前一代方法相比提高点-to-点 PSNR BD-Rate 值达29.14%。
</details></li>
</ul>
<hr>
<h2 id="FFEINR-Flow-Feature-Enhanced-Implicit-Neural-Representation-for-Spatio-temporal-Super-Resolution"><a href="#FFEINR-Flow-Feature-Enhanced-Implicit-Neural-Representation-for-Spatio-temporal-Super-Resolution" class="headerlink" title="FFEINR: Flow Feature-Enhanced Implicit Neural Representation for Spatio-temporal Super-Resolution"></a>FFEINR: Flow Feature-Enhanced Implicit Neural Representation for Spatio-temporal Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12508">http://arxiv.org/abs/2308.12508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyue Jiao, Chongke Bi, Lu Yang</li>
<li>for: 本研究旨在提出一种Feature-Enhanced Implicit Neural Representation（FFEINR）方法，用于精度高的流体场数据超分辨。</li>
<li>methods: 该方法基于卷积神经网络（CNN）和生成敌对网络（GAN），并通过增强输入层的特征来支持流体场数据的精度高超分辨。</li>
<li>results: 实验结果表明，FFEINR方法比较常用的三次 interpolate方法得到更好的结果。<details>
<summary>Abstract</summary>
Large-scale numerical simulations are capable of generating data up to terabytes or even petabytes. As a promising method of data reduction, super-resolution (SR) has been widely studied in the scientific visualization community. However, most of them are based on deep convolutional neural networks (CNNs) or generative adversarial networks (GANs) and the scale factor needs to be determined before constructing the network. As a result, a single training session only supports a fixed factor and has poor generalization ability. To address these problems, this paper proposes a Feature-Enhanced Implicit Neural Representation (FFEINR) for spatio-temporal super-resolution of flow field data. It can take full advantage of the implicit neural representation in terms of model structure and sampling resolution. The neural representation is based on a fully connected network with periodic activation functions, which enables us to obtain lightweight models. The learned continuous representation can decode the low-resolution flow field input data to arbitrary spatial and temporal resolutions, allowing for flexible upsampling. The training process of FFEINR is facilitated by introducing feature enhancements for the input layer, which complements the contextual information of the flow field.To demonstrate the effectiveness of the proposed method, a series of experiments are conducted on different datasets by setting different hyperparameters. The results show that FFEINR achieves significantly better results than the trilinear interpolation method.
</details>
<details>
<summary>摘要</summary>
大规模数值 simulations 可以生成数据 hasta terabytes 或者 Even petabytes。作为数据压缩的承诺方法，超分辨率（SR）在科学视觉社区中得到了广泛的研究。然而，大多数都是基于深度卷积神经网络（CNN）或生成对抗网络（GAN），并且扩大因子需要在建立网络之前确定。因此，单一训练会话只支持固定因子，并且generalization能力不好。为了解决这些问题，本文提出了基于含义增强的偏微分神经表示（FFEINR），用于空间时间超分辨率流场数据。它可以在模型结构和采样分辨率上取得全面利用。神经表示基于完全连接网络，使得模型变得轻量级。学习的连续表示可以将低分辨率输入数据解码到任意空间和时间分辨率，以便灵活的上扩。训练过程中，我们引入了输入层的特征增强，以便补充流场的Contextual信息。为了证明提案的有效性，我们在不同的数据集上进行了一系列实验，并设置了不同的超参数。结果表明，FFEINR可以与三元 interpolate 方法相比，获得显著更好的结果。
</details></li>
</ul>
<hr>
<h2 id="MOFA-A-Model-Simplification-Roadmap-for-Image-Restoration-on-Mobile-Devices"><a href="#MOFA-A-Model-Simplification-Roadmap-for-Image-Restoration-on-Mobile-Devices" class="headerlink" title="MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices"></a>MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12494">http://arxiv.org/abs/2308.12494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Chen, Ruiwen Zhen, Shuai Li, Xiaotian Li, Guanghui Wang<br>for:这篇论文主要针对的是提高图像恢复模型在移动设备上的效率，以便在资源有限的情况下实现高质量的图像恢复。methods:该论文提出了一种方法，包括在不敏感层添加更多参数，然后使用部分深度卷积和分离卷积&#x2F;下采样层来加速模型速度。results:对多个图像恢复 datasets 进行了广泛的实验，发现该方法可以降低运行时间，同时提高 PSNR 和 SSIM。具体来说，运行时间可以降低到最多 13%，参数数量可以减少到最多 23%，而 PSNR 和 SSIM 则可以提高。代码源代码可以在 \href{<a target="_blank" rel="noopener" href="https://github.com/xiangyu8/MOFA%7D%7Bhttps://github.com/xiangyu8/MOFA%7D">https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA}</a> 上找到。<details>
<summary>Abstract</summary>
Image restoration aims to restore high-quality images from degraded counterparts and has seen significant advancements through deep learning techniques. The technique has been widely applied to mobile devices for tasks such as mobile photography. Given the resource limitations on mobile devices, such as memory constraints and runtime requirements, the efficiency of models during deployment becomes paramount. Nevertheless, most previous works have primarily concentrated on analyzing the efficiency of single modules and improving them individually. This paper examines the efficiency across different layers. We propose a roadmap that can be applied to further accelerate image restoration models prior to deployment while simultaneously increasing PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). The roadmap first increases the model capacity by adding more parameters to partial convolutions on FLOPs non-sensitive layers. Then, it applies partial depthwise convolution coupled with decoupling upsampling/downsampling layers to accelerate the model speed. Extensive experiments demonstrate that our approach decreases runtime by up to 13% and reduces the number of parameters by up to 23%, while increasing PSNR and SSIM on several image restoration datasets. Source Code of our method is available at \href{https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="InverseSR-3D-Brain-MRI-Super-Resolution-Using-a-Latent-Diffusion-Model"><a href="#InverseSR-3D-Brain-MRI-Super-Resolution-Using-a-Latent-Diffusion-Model" class="headerlink" title="InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model"></a>InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12465">http://arxiv.org/abs/2308.12465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biomedai-ucsc/inversesr">https://github.com/biomedai-ucsc/inversesr</a></li>
<li>paper_authors: Jueqi Wang, Jacob Levman, Walter Hugo Lopez Pinaya, Petru-Daniel Tudosiu, M. Jorge Cardoso, Razvan Marinescu</li>
<li>for: 提高低分辨率（LR）磁共振成像（MRI）图像的分辨率。</li>
<li>methods: 利用一个状态最佳的3D脑生成模型——潜在扩散模型（LDM）训练在UK BioBank上，以提高低分辨率磁共振成像图像的分辨率。</li>
<li>results: 验证了将LDM作为生成模型，可以帮助提高低分辨率磁共振成像图像的分辨率。<details>
<summary>Abstract</summary>
High-resolution (HR) MRI scans obtained from research-grade medical centers provide precise information about imaged tissues. However, routine clinical MRI scans are typically in low-resolution (LR) and vary greatly in contrast and spatial resolution due to the adjustments of the scanning parameters to the local needs of the medical center. End-to-end deep learning methods for MRI super-resolution (SR) have been proposed, but they require re-training each time there is a shift in the input distribution. To address this issue, we propose a novel approach that leverages a state-of-the-art 3D brain generative model, the latent diffusion model (LDM) trained on UK BioBank, to increase the resolution of clinical MRI scans. The LDM acts as a generative prior, which has the ability to capture the prior distribution of 3D T1-weighted brain MRI. Based on the architecture of the brain LDM, we find that different methods are suitable for different settings of MRI SR, and thus propose two novel strategies: 1) for SR with more sparsity, we invert through both the decoder of the LDM and also through a deterministic Denoising Diffusion Implicit Models (DDIM), an approach we will call InverseSR(LDM); 2) for SR with less sparsity, we invert only through the LDM decoder, an approach we will call InverseSR(Decoder). These two approaches search different latent spaces in the LDM model to find the optimal latent code to map the given LR MRI into HR. The training process of the generative model is independent of the MRI under-sampling process, ensuring the generalization of our method to many MRI SR problems with different input measurements. We validate our method on over 100 brain T1w MRIs from the IXI dataset. Our method can demonstrate that powerful priors given by LDM can be used for MRI reconstruction.
</details>
<details>
<summary>摘要</summary>
高分辨率（HR）MRI扫描从研究级医疗机构获得的数据提供了精确的组织信息。然而，日常临床MRI扫描通常是低分辨率（LR），并且因为扫描参数的调整而具有不同的对比度和空间分辨率。为解决这个问题，我们提议了一种新的方法，利用了UK BioBank上训练的状态艺术3D脑生成模型（LDM），以提高临床MRI扫描的分辨率。LDM acts as a generative prior, which has the ability to capture the prior distribution of 3D T1-weighted brain MRI. Based on the architecture of the brain LDM, we find that different methods are suitable for different settings of MRI SR, and thus propose two novel strategies: 1) for SR with more sparsity, we invert through both the decoder of the LDM and also through a deterministic Denoising Diffusion Implicit Models (DDIM), an approach we will call InverseSR(LDM); 2) for SR with less sparsity, we invert only through the LDM decoder, an approach we will call InverseSR(Decoder). These two approaches search different latent spaces in the LDM model to find the optimal latent code to map the given LR MRI into HR. The training process of the generative model is independent of the MRI under-sampling process, ensuring the generalization of our method to many MRI SR problems with different input measurements. We validate our method on over 100 brain T1w MRIs from the IXI dataset. Our method can demonstrate that powerful priors given by LDM can be used for MRI reconstruction.
</details></li>
</ul>
<hr>
<h2 id="HNAS-reg-hierarchical-neural-architecture-search-for-deformable-medical-image-registration"><a href="#HNAS-reg-hierarchical-neural-architecture-search-for-deformable-medical-image-registration" class="headerlink" title="HNAS-reg: hierarchical neural architecture search for deformable medical image registration"></a>HNAS-reg: hierarchical neural architecture search for deformable medical image registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12440">http://arxiv.org/abs/2308.12440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiong Wu, Yong Fan</li>
<li>for: 这篇论文旨在找到最佳的深度学习模型，用于静止医疗影像注册。</li>
<li>methods: 这篇论文使用了一个层次 NAS 框架（HNAS-Reg），包括了 convolutional 操作搜索和网络架构搜索，以找到最佳的网络架构。实际上，这篇论文还使用了一个 partial channel 策略，以减少计算负担和内存限制。</li>
<li>results: 实验结果显示，提案方法可以建立一个具有改善医疗影像注册精度和减少模型大小的深度学习模型，较比一个传统方法和两个无监督学习方法来得好。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have been widely used to build deep learning models for medical image registration, but manually designed network architectures are not necessarily optimal. This paper presents a hierarchical NAS framework (HNAS-Reg), consisting of both convolutional operation search and network topology search, to identify the optimal network architecture for deformable medical image registration. To mitigate the computational overhead and memory constraints, a partial channel strategy is utilized without losing optimization quality. Experiments on three datasets, consisting of 636 T1-weighted magnetic resonance images (MRIs), have demonstrated that the proposal method can build a deep learning model with improved image registration accuracy and reduced model size, compared with state-of-the-art image registration approaches, including one representative traditional approach and two unsupervised learning-based approaches.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在医疗影像注册领域广泛使用深度学习模型，但是人工设计的网络架构可能并不是最佳的。这篇论文提出了一种层次 NAS 框架（HNAS-Reg），包括卷积操作搜索和网络架构搜索，以便确定最佳的网络架构 для 可变的医疗影像注册。为了减少计算负担和内存限制，该方法使用了部分通道策略而不失去优化质量。实验在三个数据集上，包括 636 个 T1 束缚 magnetic resonance imaging（MRI）图像，表明该方法可以建立一个具有改进的图像注册精度和减少的模型大小的深度学习模型，比于现有的图像注册方法，包括一种传统方法和两种无监督学习方法。
</details></li>
</ul>
<hr>
<h2 id="Reframing-the-Brain-Age-Prediction-Problem-to-a-More-Interpretable-and-Quantitative-Approach"><a href="#Reframing-the-Brain-Age-Prediction-Problem-to-a-More-Interpretable-and-Quantitative-Approach" class="headerlink" title="Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach"></a>Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12416">http://arxiv.org/abs/2308.12416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Gianchandani, Mahsa Dibaji, Mariana Bento, Ethan MacDonald, Roberto Souza</li>
<li>for: 这研究的目的是使用深度学习模型从核磁共振成像图像中预测大脑年龄，并提供更加可解的结果。</li>
<li>methods: 该研究使用了一种image-to-image regression方法，将大脑年龄预测问题转化为每个大脑细胞voxel的预测问题，并与全局预测模型和相关的涉及度地图进行比较。</li>
<li>results: 结果表明，voxel-wise预测模型比全局预测模型更加可解，因为它们提供了脑部年龄变化的空间信息，并且具有量化的优点。<details>
<summary>Abstract</summary>
Deep learning models have achieved state-of-the-art results in estimating brain age, which is an important brain health biomarker, from magnetic resonance (MR) images. However, most of these models only provide a global age prediction, and rely on techniques, such as saliency maps to interpret their results. These saliency maps highlight regions in the input image that were significant for the model's predictions, but they are hard to be interpreted, and saliency map values are not directly comparable across different samples. In this work, we reframe the age prediction problem from MR images to an image-to-image regression problem where we estimate the brain age for each brain voxel in MR images. We compare voxel-wise age prediction models against global age prediction models and their corresponding saliency maps. The results indicate that voxel-wise age prediction models are more interpretable, since they provide spatial information about the brain aging process, and they benefit from being quantitative.
</details>
<details>
<summary>摘要</summary>
深度学习模型已经达到了评估大脑年龄的国际标准Result，这是重要的大脑健康指标，从核磁共振（MR）图像中获取。然而，大多数这些模型只提供全局年龄预测，并且使用技术，如吸引力地图来解释其结果。这些吸引力地图会高亮输入图像中对模型预测有影响的区域，但它们很难被解释，而且吸引力地图值不可比较。在这项工作中，我们将MR图像中大脑年龄预测问题重新定义为图像到图像回归问题，我们预测每个大脑磁共振 voxel 的年龄。我们比较了 voxel-wise 年龄预测模型和全局年龄预测模型以及其相应的吸引力地图。结果表明，voxel-wise 年龄预测模型更加可读性高，因为它们提供了空间信息关于大脑年龄过程，并且它们受益于量化。
</details></li>
</ul>
<hr>
<h2 id="SPPNet-A-Single-Point-Prompt-Network-for-Nuclei-Image-Segmentation"><a href="#SPPNet-A-Single-Point-Prompt-Network-for-Nuclei-Image-Segmentation" class="headerlink" title="SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation"></a>SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12231">http://arxiv.org/abs/2308.12231</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xq141839/sppnet">https://github.com/xq141839/sppnet</a></li>
<li>paper_authors: Qing Xu, Wenwei Kuang, Zeyu Zhang, Xueyao Bao, Haoran Chen, Wenting Duan</li>
<li>for: 本研究旨在提出一种单点提示网络（SPPNet），用于核体图像分割。</li>
<li>methods: 我们将原始图像Encoder被替换为轻量级视transformer，并添加了一个有效的 convolutional block，以提高图像下降的semantic信息。我们还提出了基于 Gaussian kernel的新的点抽象方法。</li>
<li>results: 我们在MoNuSeg-2018 dataset上评估了SPPNet，结果表明它在比较较快的训练速度和较低的计算成本下，可以达到比较高的性能水平。相比之下，SPPNet比segment anything模型快了约20倍，仅需要一个点集，这更加合理 для临床应用。<details>
<summary>Abstract</summary>
Image segmentation plays an essential role in nuclei image analysis. Recently, the segment anything model has made a significant breakthrough in such tasks. However, the current model exists two major issues for cell segmentation: (1) the image encoder of the segment anything model involves a large number of parameters. Retraining or even fine-tuning the model still requires expensive computational resources. (2) in point prompt mode, points are sampled from the center of the ground truth and more than one set of points is expected to achieve reliable performance, which is not efficient for practical applications. In this paper, a single-point prompt network is proposed for nuclei image segmentation, called SPPNet. We replace the original image encoder with a lightweight vision transformer. Also, an effective convolutional block is added in parallel to extract the low-level semantic information from the image and compensate for the performance degradation due to the small image encoder. We propose a new point-sampling method based on the Gaussian kernel. The proposed model is evaluated on the MoNuSeg-2018 dataset. The result demonstrated that SPPNet outperforms existing U-shape architectures and shows faster convergence in training. Compared to the segment anything model, SPPNet shows roughly 20 times faster inference, with 1/70 parameters and computational cost. Particularly, only one set of points is required in both the training and inference phases, which is more reasonable for clinical applications. The code for our work and more technical details can be found at https://github.com/xq141839/SPPNet.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/eess.IV_2023_08_24/" data-id="clltaagr200enr888gavrhjbl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.AI_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.AI_2023_08_23/">cs.AI - 2023-08-23 20:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CLIPN-for-Zero-Shot-OOD-Detection-Teaching-CLIP-to-Say-No"><a href="#CLIPN-for-Zero-Shot-OOD-Detection-Teaching-CLIP-to-Say-No" class="headerlink" title="CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No"></a>CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12213">http://arxiv.org/abs/2308.12213</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/clipn">https://github.com/xmed-lab/clipn</a></li>
<li>paper_authors: Hualiang Wang, Yi Li, Huifeng Yao, Xiaomeng Li</li>
<li>For: This paper focuses on developing a novel method for zero-shot out-of-distribution (OOD) detection using CLIP, a text-to-image model. The goal is to equip CLIP with the ability to distinguish between in-distribution (ID) and OOD samples using positive-semantic prompts and negation-semantic prompts.* Methods: The proposed method, called CLIP saying no (CLIPN), utilizes a novel learnable no prompt and a no text encoder to capture negation semantics within images. Two loss functions are introduced to teach CLIPN to associate images with no prompts, enabling it to identify unknown samples. Additionally, two threshold-free inference algorithms are proposed for OOD detection.* Results: The proposed CLIPN method, based on ViT-B-16, outperforms 7 well-used algorithms by at least 2.34% and 11.64% in terms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K. The code is available on GitHub.<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes. Considerable effort has been invested in designing various OOD detection methods based on either convolutional neural networks or transformers. However, zero-shot OOD detection methods driven by CLIP, which only require class names for ID, have received less attention. This paper presents a novel method, namely CLIP saying no (CLIPN), which empowers the logic of saying no within CLIP. Our key motivation is to equip CLIP with the capability of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts. Specifically, we design a novel learnable no prompt and a no text encoder to capture negation semantics within images. Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to associate images with no prompts, thereby enabling it to identify unknown samples. Furthermore, we propose two threshold-free inference algorithms to perform OOD detection by utilizing negation semantics from no prompts and the text encoder. Experimental results on 9 benchmark datasets (3 ID datasets and 6 OOD datasets) for the OOD detection task demonstrate that CLIPN, based on ViT-B-16, outperforms 7 well-used algorithms by at least 2.34% and 11.64% in terms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K. Our CLIPN can serve as a solid foundation for effectively leveraging CLIP in downstream OOD tasks. The code is available on https://github.com/xmed-lab/CLIPN.
</details>
<details>
<summary>摘要</summary>
OUT-OF-DISTRIBUTION (OOD) 检测指的是在 IN-DISTRIBUTION (ID) 数据集上训练模型，以判断输入图像来自未知类。针对这问题，各种 OOD 检测方法已经得到了广泛的投入，其中一些基于卷积神经网络，一些基于 transformers。然而，驱动 CLIP 的零shot OOD 检测方法却受到了更少的关注。本文提出了一种新的方法，即 CLIP 说不 (CLIPN)，该方法通过帮助 CLIP 内部的逻辑分别 ID 和 OOD 样本。我们的关键动机是让 CLIP 能够通过正面 semantics 和否定 semantics 来分辨 ID 和 OOD 样本。具体来说，我们设计了一个可学习的 no 提示和一个 no 文本编码器，以捕捉图像中的否定 semantics。然后，我们引入了两个损失函数：图像文本二进制对立损失和文本 semantics 对立损失，以教 CLIPN 将图像与 no 提示相关联，从而让它能够识别未知样本。此外，我们提出了两种无阈值的推理算法，以利用 no 提示和文本编码器来进行 OOD 检测。实验结果表明，基于 ViT-B-16 的 CLIPN 在 9 个标准数据集（3 ID 数据集和 6 OOD 数据集）上的 OOD 检测任务中，与 7 种常用算法相比，至少提高了 2.34% 和 11.64% 的 AUROC 和 FPR95。我们的 CLIPN 可以作为一个可靠的基础，用于有效地利用 CLIP 在下游 OOD 任务中。代码可以在 https://github.com/xmed-lab/CLIPN 上获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies"><a href="#Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies" class="headerlink" title="Learning to Learn Financial Networks for Optimising Momentum Strategies"></a>Learning to Learn Financial Networks for Optimising Momentum Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12212">http://arxiv.org/abs/2308.12212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyue Pu, Stefan Zohren, Stephen Roberts, Xiaowen Dong</li>
<li>for: 这篇论文旨在提供一种新型的风险豁免，利用金融网络中资产之间的连接来预测未来的回报。</li>
<li>methods: 该论文提出了一种名为L2GMOM的机器学习框架，该框架同时学习金融网络和股票投资策略，以提高股票投资的性能和风险控制。</li>
<li>results: 根据64个连续Future合约的回报测试，L2GMOM模型在20年时间段内能够显著提高股票投资的盈利率和风险控制，Sharpe比率为1.74。<details>
<summary>Abstract</summary>
Network momentum provides a novel type of risk premium, which exploits the interconnections among assets in a financial network to predict future returns. However, the current process of constructing financial networks relies heavily on expensive databases and financial expertise, limiting accessibility for small-sized and academic institutions. Furthermore, the traditional approach treats network construction and portfolio optimisation as separate tasks, potentially hindering optimal portfolio performance. To address these challenges, we propose L2GMOM, an end-to-end machine learning framework that simultaneously learns financial networks and optimises trading signals for network momentum strategies. The model of L2GMOM is a neural network with a highly interpretable forward propagation architecture, which is derived from algorithm unrolling. The L2GMOM is flexible and can be trained with diverse loss functions for portfolio performance, e.g. the negative Sharpe ratio. Backtesting on 64 continuous future contracts demonstrates a significant improvement in portfolio profitability and risk control, with a Sharpe ratio of 1.74 across a 20-year period.
</details>
<details>
<summary>摘要</summary>
网络势头提供了一种新型的风险偏好，利用财务网络中资产之间的关系预测未来的回报。然而，现有的金融网络建构过程受到高优质数据库和金融专业知识的限制，导致小型和学术机构的访问受到限制。此外，传统方法将网络建构和投资策略优化视为两个独立的任务，可能会降低投资策略的优化性。为解决这些挑战，我们提出了L2GMOM，一种结束到终点的机器学习框架，同时学习金融网络和优化交易信号。L2GMOM的模型是一种高度可解释的前进卷积神经网络，由算法抽象而来。L2GMOM是灵活的，可以使用多种损失函数来优化股票表现，例如负方均值系数。在64个连续未来合约的回测中，L2GMOM显示出了 significiant提高投资收益和风险控制，负方均值系数为20年期间1.74。
</details></li>
</ul>
<hr>
<h2 id="Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques"><a href="#Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques" class="headerlink" title="Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques"></a>Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12192">http://arxiv.org/abs/2308.12192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophie A. Neubauer, Radu Grosu</li>
<li>for: 这个论文旨在统一地present deterministic和统计 lagrange 验证技术，以量化时间连续过程中行为的稳定性。</li>
<li>methods: 这个论文使用了 LRT-NG、SLR 和 GoTube 算法来构建紧距盒，即在给定时间范围内可达的状态的上下文。这些算法提供了确定性和统计性的保证。</li>
<li>results: 实验表明，lagrange 技术在比较于 LRT、Flow* 和 CAPD 的情况下表现更优异，并用于不同的时间连续模型的稳定性分析。<details>
<summary>Abstract</summary>
This paper presents, in a unified fashion, deterministic as well as statistical Lagrangian-verification techniques. They formally quantify the behavioral robustness of any time-continuous process, formulated as a continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube, algorithms for constructing a tight reachtube, that is, an over-approximation of the set of states reachable within a given time-horizon, and provide guarantees for the reachtube bounds. We compare the usage of the variational equations, associated to the system equations, the mean value theorem, and the Lipschitz constants, in achieving deterministic and statistical guarantees. In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation, to compute the radius of an ellipsoid in an optimal metric, which over-approximates the set of reachable states. In SLR and GoTube, we get statistical guarantees, by using the Lipschitz constants to compute local balls around samples. These are needed to calculate the probability of having found an upper bound, of the true maximum perturbation at every timestep. Our experiments demonstrate the superior performance of Lagrangian techniques, when compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness analysis of various continuous-depth models.
</details>
<details>
<summary>摘要</summary>
In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation to compute the radius of an ellipsoid that over-approximates the set of reachable states. In SLR and GoTube, the Lipschitz constants are used to compute local balls around samples, which are needed to calculate the probability of finding an upper bound of the true maximum perturbation at each timestep. The authors demonstrate the superior performance of Lagrangian techniques compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness analysis of various continuous-depth models through experiments.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning"><a href="#Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning" class="headerlink" title="Unsupervised anomalies detection in IIoT edge devices networks using federated learning"></a>Unsupervised anomalies detection in IIoT edge devices networks using federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12175">http://arxiv.org/abs/2308.12175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niyomukiza Thamar, Hossam Samy Elsaid Sharara</li>
<li>for:  solves the privacy problem for IoT&#x2F; IIoT devices that held sensitive data for the owners.</li>
<li>methods:  Federated learning(FL) as a distributed machine learning approach, specifically the Fedavg algorithm.</li>
<li>results:  Almost the same as the centralized machine learning approach, but with the added benefit of addressing privacy concerns.Here’s the simplified Chinese text for the three points:</li>
<li>for: 解决 IoT&#x2F; IIoT 设备上的敏感数据所有者隐私问题。</li>
<li>methods: 分布式机器学习方法（Federated Learning，FL），特别是 Fedavg 算法。</li>
<li>results: 与中央机器学习方法相似，但具有隐私保护的优点。<details>
<summary>Abstract</summary>
In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules. However, some owners are reluctant of availing their data out of the company due to data security concerns. Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself. In this scenario, data is not share over the network for training purpose. Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session. The devices could be chosen at random, and a device can be aborted. The resulting models are sent to the coordinating server and then average models from the devices that finished training. The process is repeated until a desired model accuracy is achieved. By doing this, FL approach solves the privacy problem for IoT/ IIoT devices that held sensitive data for the owners. In this paper, we leverage the benefits of FL and implemented Fedavg algorithm on a recent dataset that represent the modern IoT/ IIoT device networks. The results were almost the same as the centralized machine learning approach. We also evaluated some shortcomings of Fedavg such as unfairness that happens during the training when struggling devices do not participate for every stage of training. This inefficient training of local or global model could lead in a high number of false alarms in intrusion detection systems for IoT/IIoT gadgets developed using Fedavg. Hence, after evaluating the FedAv deep auto encoder with centralized deep auto encoder ML, we further proposed and designed a Fair Fedavg algorithm that will be evaluated in the future work.
</details>
<details>
<summary>摘要</summary>
在许多物联网设备之间的连接中，通常需要将数据传输到中央服务器进行机器学习模型的训练，但有些 propietarios 对于数据安全问题感到担忧。 Federated learning（FL）作为分布式机器学习方法，在设备上进行机器学习模型的训练，不需要将数据传输到服务器。 Fedavg 是 FL 算法之一，允许在训练过程中将模型复制到参与设备上。这些设备可以随机选择，并且可以在训练过程中被终止。获得的模型将被发送到协调服务器，并与其他完成训练的设备的模型进行平均值。这种方法可以解决物联网/IIoT 设备持有敏感数据的所有者隐私问题。在这篇论文中，我们利用 FL 的优点，并在最新的数据集上实现 Fedavg 算法。结果与中央机器学习方法的结果几乎相同。我们还评估了 Fedavg 的一些缺点，如训练过程中不参与的设备会导致不公平性。这可能导致 IoT/IIoT 设备上开发的投入检测系统中出现高比例的假警示。因此，我们在未来工作中将提出和实现公平的 Fedavg 算法。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-Faithfulness-Using-the-Longest-Supported-Subsequence"><a href="#Evaluation-of-Faithfulness-Using-the-Longest-Supported-Subsequence" class="headerlink" title="Evaluation of Faithfulness Using the Longest Supported Subsequence"></a>Evaluation of Faithfulness Using the Longest Supported Subsequence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12157">http://arxiv.org/abs/2308.12157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Mittal, Timo Schick, Mikel Artetxe, Jane Dwivedi-Yu</li>
<li>for: evaluating the trustworthiness of machine-generated text, specifically in tasks such as summarization and question-answering</li>
<li>methods: introducing a novel approach called the Longest Supported Subsequence (LSS) to compute the faithfulness of machine-generated text, and finetuning a model to generate LSS using a new human-annotated dataset</li>
<li>results: demonstrating that the proposed metric correlates better with human ratings than prevailing state-of-the-art metrics, with an 18% enhancement in faithfulness on the dataset, and consistently outperforming other metrics on a summarization dataset across six different models, as well as comparing several popular Large Language Models (LLMs) for faithfulness using this metric.<details>
<summary>Abstract</summary>
As increasingly sophisticated language models emerge, their trustworthiness becomes a pivotal issue, especially in tasks such as summarization and question-answering. Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers. In this paper, we introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, which we refer to as the Longest Supported Subsequence (LSS). Using a new human-annotated dataset, we finetune a model to generate LSS. We introduce a new method of evaluation and demonstrate that these metrics correlate better with human ratings when LSS is employed, as opposed to when it is not. Our proposed metric demonstrates an 18% enhancement over the prevailing state-of-the-art metric for faithfulness on our dataset. Our metric consistently outperforms other metrics on a summarization dataset across six different models. Finally, we compare several popular Large Language Models (LLMs) for faithfulness using this metric. We release the human-annotated dataset built for predicting LSS and our fine-tuned model for evaluating faithfulness.
</details>
<details>
<summary>摘要</summary>
“随着越来越进步的语言模型出现，它们的可靠性成为一个关键的问题，特别是在摘要和问答中。确保它们的回答是基于上下文的，并不是单纯地根据语言模型的假设，是一个具有挑战性的任务。在这篇论文中，我们提出了一种新的方法来评估机器生成的文本的可靠性，通过计算文本中最长的不连续子串，我们称之为“最长支持子串”（LSS）。我们使用了一个新的人类验证数据集，调整了一个模型以生成LSS，并导入了一个新的评估方法。我们示示了这些指标与人类评分更加相似，而且在摘要数据集上，我们的提案的指标与现有的指标相比，有18%的提升。我们的指标在六个不同的模型上的表现都与其他指标相比较高。最后，我们使用这个指标评估了一些流行的大型语言模型的可靠性。我们发布了我们建立的人类验证数据集和调整后的模型，以便用于评估可靠性。”
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Latent-Emotion-Recognition-from-Micro-expression-and-Physiological-Signals"><a href="#Multimodal-Latent-Emotion-Recognition-from-Micro-expression-and-Physiological-Signals" class="headerlink" title="Multimodal Latent Emotion Recognition from Micro-expression and Physiological Signals"></a>Multimodal Latent Emotion Recognition from Micro-expression and Physiological Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12156">http://arxiv.org/abs/2308.12156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangfei Zhang, Yifei Qian, Ognjen Arandjelovic, Anthony Zhu</li>
<li>for: 提高隐藏情感识别精度</li>
<li>methods:  combining微表情(ME)和生理信号(PS)，使用1D可分和混合深度卷积网络，标准化分布预测权重混合法，以及深度&#x2F;生理指导注意模块</li>
<li>results: 提高比较方法的表现<details>
<summary>Abstract</summary>
This paper discusses the benefits of incorporating multimodal data for improving latent emotion recognition accuracy, focusing on micro-expression (ME) and physiological signals (PS). The proposed approach presents a novel multimodal learning framework that combines ME and PS, including a 1D separable and mixable depthwise inception network, a standardised normal distribution weighted feature fusion method, and depth/physiology guided attention modules for multimodal learning. Experimental results show that the proposed approach outperforms the benchmark method, with the weighted fusion method and guided attention modules both contributing to enhanced performance.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了通过多模式数据的汇入来提高潜在情绪识别精度，特点在微表情（ME）和生理信号（PS）之间。提议的方法框架组合了ME和PS，包括一个可分离的深度wise嵌入网络，一种标准化正态分布权重Feature合并方法，以及深度/生理学引导注意模块 для多模式学习。实验结果显示，提议的方法在比较方法上表现出色，权重合并方法和引导注意模块都对精度提高做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models"><a href="#A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models" class="headerlink" title="A Probabilistic Fluctuation based Membership Inference Attack for Generative Models"></a>A Probabilistic Fluctuation based Membership Inference Attack for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12143">http://arxiv.org/abs/2308.12143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang</li>
<li>for: 本研究探讨了基于生成模型的会员推测攻击（MIA），并提出了一种基于概率波动的会员推测方法（PFAMI）。</li>
<li>methods: PFAMI 基于生成模型中的记忆效应，通过分析生成记录的概率波动来推断会员性。</li>
<li>results: 对多种生成模型和数据集进行了广泛的实验，显示 PFAMI 可以提高攻击成功率（ASR）约27.9%  comparing with 基准值。<details>
<summary>Abstract</summary>
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting these trends via analyzing the overall probabilistic fluctuations around given records. We conduct extensive experiments across multiple generative models and datasets, which demonstrate PFAMI can improve the attack success rate (ASR) by about 27.9% when compared with the best baseline.
</details>
<details>
<summary>摘要</summary>
机制成员攻击（MIA）可以决定一个记录是否在机器学习模型的训练集中，通过询问模型。过往的研究主要集中在传统的分类模型上，而现在的研究则开始对生成模型进行应用。我们的研究显示，现有的生成模型MIA主要依赖目标模型的过滤。然而，过滤可以使用多种正规化技术来避免，而现有的MIA实际上却表现不佳。不同的过滤，记忆是深度学习模型所需的一种基本现象，它会使模型在实际应用中表现更好。记忆在生成模型中导致生成记录的概率分布增加，因此我们提出了一个概率波动评估机制成员攻击（PFAMI），这是一种黑盒子MIA，可以通过分析givens record的概率波动来决定成员。我们进行了多种生成模型和数据集的广泛实验，结果显示，PFAMI可以提高攻击成功率（ASR）约27.9%，相比最佳基eline。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Change-Detection-for-the-Romanian-Language"><a href="#Semantic-Change-Detection-for-the-Romanian-Language" class="headerlink" title="Semantic Change Detection for the Romanian Language"></a>Semantic Change Detection for the Romanian Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12131">http://arxiv.org/abs/2308.12131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ds4ai-upb/semanticchange-ro">https://github.com/ds4ai-upb/semanticchange-ro</a></li>
<li>paper_authors: Ciprian-Octavian Truică, Victor Tudose, Elena-Simona Apostol</li>
<li>for: 本研究旨在分析语言变化的自动Semantic Change Methods，以及在实际英语和罗马尼亚语 Corporas中的应用。</li>
<li>methods: 本研究使用Word2Vec和ELMo两种静态和 контекстual word embedding模型，并对这两种模型在英语dataset上进行评估。然后，对罗马尼亚语 dataset进行实验，并强调不同的semantic change aspect，如意义获得和丢失。</li>
<li>results: 实验结果显示，取决于 corpus，模型选择和评估距离是检测semantic change的重要因素。<details>
<summary>Abstract</summary>
Automatic semantic change methods try to identify the changes that appear over time in the meaning of words by analyzing their usage in diachronic corpora. In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and ELMo, on real-world English and Romanian datasets. To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an English dataset (SEMEVAL-CCOHA). Afterward, we focus our experiments on a Romanian dataset, and we underline different aspects of semantic changes in this low-resource language, such as meaning acquisition and loss. The experimental results show that, depending on the corpus, the most important factors to consider are the choice of model and the distance to calculate a score for detecting semantic change.
</details>
<details>
<summary>摘要</summary>
自动 semantic change 方法试图通过分析在时间上的使用情况来识别词语的意义变化。在这篇论文中，我们分析了不同的策略来创建静态和 контекст word embedding 模型，即 Word2Vec 和 ELMo，在实际的英语和罗马尼亚数据集上。为了测试我们的管道和确定模型的表现，我们首先评估了这两种 word embedding 模型在英语数据集（SEMEVAL-CCOHA）上。接着，我们将注意力集中在罗马尼亚数据集上，并强调不同的 semantics 变化方面，如 meaning acquisition 和 loss。实验结果表明，具体取决于 corpus，最重要的因素是选择模型和计算分数的距离。
</details></li>
</ul>
<hr>
<h2 id="Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models"><a href="#Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models" class="headerlink" title="Masking Strategies for Background Bias Removal in Computer Vision Models"></a>Masking Strategies for Background Bias Removal in Computer Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12127">http://arxiv.org/abs/2308.12127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ananthu-aniraj/masking_strategies_bias_removal">https://github.com/ananthu-aniraj/masking_strategies_bias_removal</a></li>
<li>paper_authors: Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos</li>
<li>for: 这种研究旨在探讨细化图像分类任务中背景引起的偏见问题，以及如何使用masking策略来 Mitigate这种偏见。</li>
<li>methods: 这些研究使用了标准的Convolutional Neural Network (CNN)和Vision Transformers (ViT)模型，并评估了两种masking策略来解决背景引起的偏见问题。</li>
<li>results: 研究发现，使用这两种masking策略可以提高模型对不同背景的抗干扰性能，特别是在使用GAP-Pooled Patch token-based classification和 early masking的情况下。<details>
<summary>Abstract</summary>
Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.
</details>
<details>
<summary>摘要</summary>
模型 для细化图像分类任务，其中一些类别之间的差别可能很小，而每个类别的样本数也很少，容易受到背景相关的偏见。为了更深入地理解这个重要问题，我们的研究探讨了背景引起的偏见对细化图像分类的影响，并评估了标准的背景模型，如卷积神经网络（CNN）和视Transformers（ViT）。我们研究了两种遮盾策略来减轻背景引起的偏见：早期遮盾，即在输入图像水平上移除背景信息，以及晚期遮盾，即在高级空间特征水平上选择性地遮盾背景相关的特征。我们进行了广泛的实验，评估不同遮盾策略对CNN和ViT模型的影响，尤其是对于不同的背景。结果显示，我们所提出的两种遮盾策略都能提高对于不同背景的性能，而早期遮盾一直保持最好的OOD性能。另外，一种基于GAP-Pooled Patch token的ViT变体，结合早期遮盾，达到了最高的OOD Robustness。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient"><a href="#Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient" class="headerlink" title="Quantifying degeneracy in singular models via the learning coefficient"></a>Quantifying degeneracy in singular models via the learning coefficient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12108">http://arxiv.org/abs/2308.12108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edmundlth/scalable_learning_coefficient_with_sgld">https://github.com/edmundlth/scalable_learning_coefficient_with_sgld</a></li>
<li>paper_authors: Edmund Lau, Daniel Murfet, Susan Wei</li>
<li>for: This paper is written to explore the concept of degeneracy in deep neural networks (DNN) and to develop a method for quantifying the degree of degeneracy using a quantity called the “learning coefficient”.</li>
<li>methods: The paper uses singular learning theory and stochastic gradient Langevin dynamics to develop a computationally scalable approximation of the localized learning coefficient.</li>
<li>results: The paper demonstrates the accuracy of the proposed approach in low-dimensional models with known theoretical values, and shows that the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. Additionally, the paper demonstrates the ability of the local learning coefficient to reveal the inductive bias of stochastic optimizers for more or less degenerate critical points using an experiment on the MNIST dataset.<details>
<summary>Abstract</summary>
Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Out-of-the-Cage-How-Stochastic-Parrots-Win-in-Cyber-Security-Environments"><a href="#Out-of-the-Cage-How-Stochastic-Parrots-Win-in-Cyber-Security-Environments" class="headerlink" title="Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments"></a>Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12086">http://arxiv.org/abs/2308.12086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Rigaki, Ondřej Lukáš, Carlos A. Catania, Sebastian Garcia</li>
<li>for: This paper focuses on using pre-trained language models (LLMs) as agents in cybersecurity network environments for sequential decision-making processes.</li>
<li>methods: The authors propose using pre-trained LLMs as attacking agents in two reinforcement learning environments and compare their performance to state-of-the-art agents and human testers.</li>
<li>results: The LLM agents demonstrate similar or better performance than state-of-the-art agents in most scenarios and configurations, and the best LLM agents perform similarly to human testers without any additional training. This suggests that LLMs have the potential to efficiently address complex decision-making tasks within cybersecurity.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文探讨了使用预训练语言模型（LLM）作为网络安全环境中的决策代理。</li>
<li>methods: 作者们提议使用预训练LLM作为两个强化学习环境中的攻击者，并与当前最佳代理进行比较。</li>
<li>results: LLM代理在大多数情况下和配置下表现相当或更好于当前最佳代理，并且最佳LLM代理在没有任何额外训练的情况下与人工测试人员表现相当。这表明LLM有可能高效地解决网络安全中的复杂决策问题。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks. Despite their inherent limitations, LLM-based designs have shown promising capabilities in planning and navigating open-world scenarios. This paper introduces a novel application of pre-trained LLMs as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes.   We present an approach wherein pre-trained LLMs are leveraged as attacking agents in two reinforcement learning environments. Our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations. In addition, the best LLM agents perform similarly to human testers of the environment without any additional training process. This design highlights the potential of LLMs to efficiently address complex decision-making tasks within cybersecurity.   Furthermore, we introduce a new network security environment named NetSecGame. The environment is designed to eventually support complex multi-agent scenarios within the network security domain. The proposed environment mimics real network attacks and is designed to be highly modular and adaptable for various scenarios.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）在多种自然语言处理任务中得到了广泛的推广，包括文本生成、摘要和各种自然语言处理任务。尽管它们有自然的限制，但LLM基本设计在开放世界enario中的规划和导航方面表现了扎实的能力。本文介绍了一种使用预训练LLM作为网络安全环境中的代理人，关注它们在顺序决策过程中的使用。我们提出了一种方法，其中预训练LLM被用作攻击者在两个循环学习环境中。我们的提议代理人在大多数情况下和现有EPisode数千个话的代理人之间表现相似或更好。此外，我们的最佳LLM代理人在没有任何额外训练过程的情况下与人类测试者的性能相似。这种设计高亮了LLM在网络安全中的潜在能力。此外，我们介绍了一个新的网络安全环境名为NetSecGame。该环境旨在最终支持复杂多代理人场景在网络安全领域。我们的设计模仿了实际网络攻击，并设计为高度可组合和可调整的多种场景。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-RNN-Gradients-through-Pre-training"><a href="#Stabilizing-RNN-Gradients-through-Pre-training" class="headerlink" title="Stabilizing RNN Gradients through Pre-training"></a>Stabilizing RNN Gradients through Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12075">http://arxiv.org/abs/2308.12075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Herranz-Celotti, Jean Rouat<br>for: This paper aims to improve the stability of deep neural networks during training, particularly for complex networks that are difficult to analyze analytically.methods: The authors propose a new approach called the Local Stability Condition (LSC) to stabilize deep neural networks. They extend known stability theories to encompass a broader family of deep recurrent networks and propose a new initialization scheme that gives a weight of a half to the time and depth contributions to the gradient.results: The authors confirm that pre-training both feed-forward and recurrent networks to fulfill the LSC often results in improved final performance across models. Their approach can be implemented as an additional step before pre-training on large augmented datasets, and as an alternative to finding stable initializations analytically.<details>
<summary>Abstract</summary>
Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponential explosion that emerges from counting gradient paths in a rectangular grid in depth and time. We propose a new approach to mitigate this issue, that consists on giving a weight of a half to the time and depth contributions to the gradient, instead of the classical weight of one. Our empirical results confirm that pre-training both feed-forward and recurrent networks to fulfill the LSC often results in improved final performance across models. This study contributes to the field by providing a means to stabilize networks of any complexity. Our approach can be implemented as an additional step before pre-training on large augmented datasets, and as an alternative to finding stable initializations analytically.
</details>
<details>
<summary>摘要</summary>
多种学习理论建议防止梯度变异的束缚增长，以稳定和改进训练。通常，这些分析是在具有数学 tractability 的批量化神经网络或单层循环神经网络上进行的。然而，这项研究表明，在神经网络太复杂以至于无法进行分析初始化时，可以预训练网络到地方稳定性。此外，我们扩展了已知稳定性理论，以覆盖更广泛的深度循环神经网络家族，不需要对数据和参数分布做出过多的假设。我们称之为地方稳定条件（LSC）。我们的调查表明，经典的格洛罗特、和合理初始化方案满足 LSC 当应用于批量化神经网络。然而，对深度循环神经网络进行分析，我们发现了一种新的加法式爆炸源，来自于计算梯度路径在深度和时间方向的矩阵中的计数。我们提出一种新的方法来缓解这个问题，即在计算梯度时，将时间和深度的贡献权重设为 0.5，而不是经典的 1.0。我们的实验结果表明，在 feed-forward 和循环神经网络中预训练满足 LSC 后，可以获得改进的最终性能。这项研究对深度学习领域的稳定性做出了贡献，并提供了一种可以稳定任何复杂性的神经网络的方法。我们的方法可以作为训练之前的额外步骤，或者作为在大量增强数据集上进行分析初始化的替代方案。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning"><a href="#Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning" class="headerlink" title="Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning"></a>Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12069">http://arxiv.org/abs/2308.12069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ni Dang, Tao Shi, Zengjie Zhang, Wanxin Jin, Marion Leibold, Martin Buss</li>
<li>for: 本研究旨在提供一种基于最大熵逆激励学习（ME-IRL）方法的自动驾驶车辆（AV）驾驶风格识别方法，以便在多辆AV交通系统中评估风险和做出更合理的驾驶决策。</li>
<li>methods: 本研究使用ME-IRL方法来定义AV驾驶风格，并设计了一系列新的特征来捕捉AV对附近AV的反应。</li>
<li>results: 经过验证Using MATLAB实验和一个Off-the-shelf experiment，提出的方法可以准确地识别AV的驾驶风格，并且可以在多辆AV交通系统中提高安全性。<details>
<summary>Abstract</summary>
The driving style of an Autonomous Vehicle (AV) refers to how it behaves and interacts with other AVs. In a multi-vehicle autonomous driving system, an AV capable of identifying the driving styles of its nearby AVs can reliably evaluate the risk of collisions and make more reasonable driving decisions. However, there has not been a consistent definition of driving styles for an AV in the literature, although it is considered that the driving style is encoded in the AV's trajectories and can be identified using Maximum Entropy Inverse Reinforcement Learning (ME-IRL) methods as a cost function. Nevertheless, an important indicator of the driving style, i.e., how an AV reacts to its nearby AVs, is not fully incorporated in the feature design of previous ME-IRL methods. In this paper, we describe the driving style as a cost function of a series of weighted features. We design additional novel features to capture the AV's reaction-aware characteristics. Then, we identify the driving styles from the demonstration trajectories generated by the Stochastic Model Predictive Control (SMPC) using a modified ME-IRL method with our newly proposed features. The proposed method is validated using MATLAB simulation and an off-the-shelf experiment.
</details>
<details>
<summary>摘要</summary>
自动驾驶车（AV）的驾驶方式指的是它如何行驶和与其他AV交互。在多辆自动驾驶车系统中，一个能够识别附近AV的驾驶方式的AV可以更加可靠地评估碰撞风险并做出更加合理的驾驶决策。然而，在文献中没有一个共识的自动驾驶车驾驶方式定义。尽管认为驾驶方式是在AV的轨迹中嵌入的，可以使用最大 entropy inverse reinforcement learning（ME-IRL）方法来识别它。然而，驾驶方式中一个重要指标，即AV如何 реаги于附近AV，并没有被完全包含在先前的ME-IRL方法中。在这篇论文中，我们定义了自动驾驶车的驾驶方式为一系列加权特征的成本函数。我们还设计了一些新的反应感知特征，以 capture AV的响应特性。然后，我们使用修改后的ME-IRL方法和我们新提出的特征来识别驾驶方式。我们的方法在MATLAB simulations和一个商业实验中得到了验证。
</details></li>
</ul>
<hr>
<h2 id="RemovalNet-DNN-Fingerprint-Removal-Attacks"><a href="#RemovalNet-DNN-Fingerprint-Removal-Attacks" class="headerlink" title="RemovalNet: DNN Fingerprint Removal Attacks"></a>RemovalNet: DNN Fingerprint Removal Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12319">http://arxiv.org/abs/2308.12319</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grasses/removalnet">https://github.com/grasses/removalnet</a></li>
<li>paper_authors: Hongwei Yao, Zheng Li, Kunzhe Huang, Jian Lou, Zhan Qin, Kui Ren<br>for: 这个论文主要是研究DNNS的知识抽取和模型权利保护问题。methods: 作者提出了一种基于最小最大二重优化的DNNS模型抽取攻击方法，以逃脱模型权利验证。在下面优化中，作者将攻击者模型的特定指纹知识除掉，而在上面优化中，作者通过液化模型的总semantic知识来保持代理模型的性能。results: 作者通过对四种高级防御方法进行了广泛的实验，证明了RemovalNet的效果、效率和精度。特别是，与基准攻击方法相比，RemovalNet使用的计算资源减少了约85%。同时，创造的代理模型保持了高精度 послеDNNS模型抽取过程。<details>
<summary>Abstract</summary>
With the performance of deep neural networks (DNNs) remarkably improving, DNNs have been widely used in many areas. Consequently, the DNN model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., DNN fingerprinting). However, the feasibility of the DNN fingerprint removal attack and its potential influence remains an open problem. In this paper, we perform the first comprehensive investigation of DNN fingerprint removal attacks. Generally, the knowledge contained in a DNN model can be categorized into general semantic and fingerprint-specific knowledge. To this end, we propose a min-max bilevel optimization-based DNN fingerprint removal attack named RemovalNet, to evade model ownership verification. The lower-level optimization is designed to remove fingerprint-specific knowledge. While in the upper-level optimization, we distill the victim model's general semantic knowledge to maintain the surrogate model's performance. We conduct extensive experiments to evaluate the fidelity, effectiveness, and efficiency of the RemovalNet against four advanced defense methods on six metrics. The empirical results demonstrate that (1) the RemovalNet is effective. After our DNN fingerprint removal attack, the model distance between the target and surrogate models is x100 times higher than that of the baseline attacks, (2) the RemovalNet is efficient. It uses only 0.2% (400 samples) of the substitute dataset and 1,000 iterations to conduct our attack. Besides, compared with advanced model stealing attacks, the RemovalNet saves nearly 85% of computational resources at most, (3) the RemovalNet achieves high fidelity that the created surrogate model maintains high accuracy after the DNN fingerprint removal process. Our code is available at: https://github.com/grasses/RemovalNet.
</details>
<details>
<summary>摘要</summary>
WITH 深度神经网络（DNN）性能显著提高，DNN已广泛应用于多个领域。因此，DNN模型成为了重要的财产，其知识产权得到了保护。然而，DNN指纹移除攻击的可能性和影响仍然是一个开放的问题。在这篇论文中，我们进行了首次全面的DNN指纹移除攻击调查。通常，DNN模型中的知识可以分为总Semantic和指纹特定知识。为此，我们提出了一种基于最小最大二级优化的DNN指纹移除攻击方法，名为RemovalNet，以避免模型所有权验证。lower-level优化设计移除指纹特定知识。而在upper-level优化中，我们通过液态热塑化将受害者模型的总Semantic知识萃取出来，以保持代理模型的性能。我们对四种高级防御方法进行了广泛的实验，并评估了RemovalNet的准确性、有效性和效率。实验结果显示了以下三点：1. RemovalNet是有效的。在我们的DNN指纹移除攻击后，模型之间的距离增加了100倍，比基eline攻击更高。2. RemovalNet是高效的。它只需使用400个样本和1000次迭代来进行攻击，而基eline攻击需要2000个样本和5000次迭代。此外，与高级模型盗取攻击相比，RemovalNet可以释放大约85%的计算资源。3. RemovalNet实现了高准确性，创建的代理模型在指纹移除过程后仍然保持高度准确。我们的代码可以在https://github.com/grasses/RemovalNet上下载。
</details></li>
</ul>
<hr>
<h2 id="InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4"><a href="#InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4" class="headerlink" title="InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"></a>InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12067">http://arxiv.org/abs/2308.12067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun</li>
<li>for: 这个论文主要用于探讨大语言模型在多模态场景中遵循指令的能力是如何强化的。</li>
<li>methods: 论文使用了两个阶段的训练方法：首先在图片和文本对的情况下进行预训练，然后在超参数数据上进行精度调整。</li>
<li>results: 论文通过提出一些metric来评估多模态指令数据的质量，并使用这些metric来自动选择高质量的视力语言数据，从而使用InstructionGPT-4超越了原始的MiniGPT-4在多种评估（如视觉问答、GPT-4首选）中的表现。<details>
<summary>Abstract</summary>
Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient to enable multimodal large language models to generate better output.
</details>
<details>
<summary>摘要</summary>
多模态大语言模型通过两stage训练过程获得指令遵循能力：先于插入图像文本对的预训练，然后在指导视语言数据上进行精度调整。现有研究表明，大语言模型可以通过有限量高质量指令遵循数据来达到满意的结果。在本文中，我们介绍InstructionGPT-4，它是基于只有200个例子，相当于MiniGPT-4的整合数据中的6%的指令遵循数据进行精度调整。我们首先提出了评估多模态指令数据质量的多种指标，然后基于这些指标，我们提出了一种简单有效的数据选择器，可以自动将低质量的视语言数据滤除。通过使用这种方法，InstructionGPT-4在多种评估中（如视觉问答、GPT-4偏好）都超过了原始MiniGPT-4。总之，我们的发现表明，虽然只有少量但高质量的指令循数据，可以使多模态大语言模型生成更好的输出。
</details></li>
</ul>
<hr>
<h2 id="Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference"><a href="#Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference" class="headerlink" title="Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference"></a>Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12066">http://arxiv.org/abs/2308.12066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ranggi Hwang, Jianyu Wei, Shijie Cao, Changho Hwang, Xiaohu Tang, Ting Cao, Mao Yang, Minsoo Rhu</li>
<li>for: 大型自然语言模型（LLM）基于变换器的实现，以实现高性能。</li>
<li>methods: 使用 Mixture-of-Experts（MoE）架构，以适应大规模 LLM 的计算和存储需求。</li>
<li>results: 提出了 Pre-gated MoE 系统，可以有效地解决 conventional MoE 架构中的计算和存储挑战，同时保持高性能和减少 GPU 内存占用量。<details>
<summary>Abstract</summary>
Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs our novel pre-gating function which alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE is able to improve performance, reduce GPU memory consumption, while also maintaining the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.
</details>
<details>
<summary>摘要</summary>
Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs a novel pre-gating function that alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE can improve performance, reduce GPU memory consumption, and maintain the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.In simplified Chinese, the text would be:大型语言模型（LLM） based on transformers  recent years  achieved significant progress, success driven by scaling up model size. However, the high computational and memory requirements of LLMs present unprecedented challenges. To address these challenges, Mixture-of-Experts（MoE） architecture was introduced, which can scale its model size without proportionally increasing its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts limit its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead.我们的 Pre-gated MoE 系统使用我们的算法-系统合理设计，有效地解决了传统 MoE 架构中的计算和内存挑战。Pre-gated MoE 使用我们的新的预 Gate 函数，解决了 sparse expert 动态 activation 的问题，使我们的提议的系统可以Addressing the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE can improve performance, reduce GPU memory consumption, and maintain the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.
</details></li>
</ul>
<hr>
<h2 id="Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers"><a href="#Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers" class="headerlink" title="Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers"></a>Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12065">http://arxiv.org/abs/2308.12065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Zoppi, Andrea Ceccarelli, Andrea Bondavalli</li>
<li>for: 本研究提出了一种安全包装（SPROUT），用于检测和防止机器学习（ML）算法的错误分类。</li>
<li>methods: 该方法使用多个不确定度测量来检测输入和输出的不确定性，并在检测到错误分类时阻止输出的传播。</li>
<li>results: 实验表明，SPROUT可以准确地检测大量的错误分类，并在特定情况下检测所有错误分类。 SPROUT适用于 binary 和多类分类问题，包括图像和表格数据集。<details>
<summary>Abstract</summary>
Machine Learning (ML) algorithms that perform classification may predict the wrong class, experiencing misclassifications. It is well-known that misclassifications may have cascading effects on the encompassing system, possibly resulting in critical failures. This paper proposes SPROUT, a Safety wraPper thROugh ensembles of UncertainTy measures, which suspects misclassifications by computing uncertainty measures on the inputs and outputs of a black-box classifier. If a misclassification is detected, SPROUT blocks the propagation of the output of the classifier to the encompassing system. The resulting impact on safety is that SPROUT transforms erratic outputs (misclassifications) into data omission failures, which can be easily managed at the system level. SPROUT has a broad range of applications as it fits binary and multi-class classification, comprising image and tabular datasets. We experimentally show that SPROUT always identifies a huge fraction of the misclassifications of supervised classifiers, and it is able to detect all misclassifications in specific cases. SPROUT implementation contains pre-trained wrappers, it is publicly available and ready to be deployed with minimal effort.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）算法可能会预测错误的类别，导致错误分类。这是已知的一点，错误分类可能会带来整体系统的崩溃。这篇文章提议了“护皮”（SPROUT），它是通过多个不确定度测量来怀疑错误分类的一种安全包装。如果检测到错误分类，SPROUT会阻止分类器的输出传递到包含系统。这会使安全性受到改善，因为SPROUT将异常输入（错误分类）转化为数据漏洞失败，这可以轻松地在系统层面进行管理。SPROUT适用于二分类和多分类，包括图像和表格数据集。我们实验表明，SPROUT总能够检测大量超级vised分类器中的错误分类，并且在某些情况下可以检测所有错误分类。SPROUT的实现包括预训练包装，它公共可用，ready to deploy 需要最小的努力。
</details></li>
</ul>
<hr>
<h2 id="FlexKBQA-A-Flexible-LLM-Powered-Framework-for-Few-Shot-Knowledge-Base-Question-Answering"><a href="#FlexKBQA-A-Flexible-LLM-Powered-Framework-for-Few-Shot-Knowledge-Base-Question-Answering" class="headerlink" title="FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering"></a>FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12060">http://arxiv.org/abs/2308.12060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leezythu/flexkbqa">https://github.com/leezythu/flexkbqa</a></li>
<li>paper_authors: Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, Zhichao Duan, Bowen Dong, Ning Liu, Jianyong Wang</li>
<li>for: 提高KBQA模型在实际应用中的性能，尤其是在缺乏高质量annotated数据的情况下。</li>
<li>methods: 利用自动生成的程序，如SPARQL查询，和大型自然语言模型（LLMs）来address问题。采用自动生成的程序可以减少人工标注的努力，而LLMs可以将程序转换成自然语言问题。</li>
<li>results: 在GrailQA、WebQSP和KQA Pro等 benchmark上进行了广泛的实验，发现在几个shot和零shot情况下，FlexKBQA可以达到很高的性能，比超过所有基eline和even approaching supervised模型的性能，达到93%相对于彻底supervised模型的性能。<details>
<summary>Abstract</summary>
Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executionguided self-training method to iterative leverage unlabeled user questions. Furthermore, we explore harnessing the inherent reasoning capability of LLMs to enhance the entire framework. Consequently, FlexKBQA delivers substantial flexibility, encompassing data annotation, deployment, and being domain agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we observe that under the few-shot even the more challenging zero-shot scenarios, FlexKBQA achieves impressive results with a few annotations, surpassing all previous baselines and even approaching the performance of supervised models, achieving a remarkable 93% performance relative to the fully-supervised models. We posit that FlexKBQA represents a significant advancement towards exploring better integration of large and lightweight models. The code is open-sourced.
</details>
<details>
<summary>摘要</summary>
知识库问答（KBQA）是一项关键性的 yet 挑战性的任务，由于知识库中的维度多样性和用户提交的自然语言问题的多样性。尽管大多数 KBQA 模型在实际场景中表现不佳，这主要归结于缺乏高质量标注数据的问题。为了解决这个问题，我们引入 FlexKBQA，利用大型自然语言模型（LLMs）作为知识库程序翻译器，以解决几何shot KBQA 任务中的挑战。Specifically, FlexKBQA 使用自动生成算法来采样知识库中的多样程序，例如 SPARQL 查询，并将其转化为自然语言问题。这些人工生成的数据可以用来训练特殊的轻量级模型。此外，为了减少实际问题和人工标注数据之间的分布差异，FlexKBQA 引入执行引导自动训练方法，以便逐步利用无标注的用户问题进行自动训练。此外，我们还考虑了利用 LLMs 的内在逻辑能力来增强整个框架。通过广泛的实验在 GrailQA、WebQSP 和 KQA Pro 等平台上，我们发现在几何shot 和零shot enario下，FlexKBQA 可以很好地表现，与完全监督模型相当，达到了93% 的性能相对于完全监督模型。我们认为 FlexKBQA 代表了大量和轻量级模型更好的 интеграción的一个重要进展。代码开源。
</details></li>
</ul>
<hr>
<h2 id="Layer-wise-Feedback-Propagation"><a href="#Layer-wise-Feedback-Propagation" class="headerlink" title="Layer-wise Feedback Propagation"></a>Layer-wise Feedback Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12053">http://arxiv.org/abs/2308.12053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leander Weber, Jim Berend, Alexander Binder, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</li>
<li>for: 这篇论文旨在提出层 wise feedback propagation（LFP），一种基于解释的训练方法，通过分层扩散反馈来评估神经网络中每个连接的贡献，从而实现比 tradicional Gradient Descent 更高效的训练。</li>
<li>methods: LFP 使用层 wise relevance propagation（LRP）来分层扩散反馈，不需要计算梯度，从而避免了一些基于梯度的限制。LFP 可以在不同的模型和数据集上实现相似的性能。</li>
<li>results: 在论文中， authors 提供了 LFP 的理论和实验证明，并证明了它在不同的模型和数据集上的效果。LFP 可以在不同的应用中提高模型的训练效率，例如在 Step-function activated Spiking Neural Networks（SNNs）中进行训练，或者进行知识传递学习。<details>
<summary>Abstract</summary>
In this paper, we present Layer-wise Feedback Propagation (LFP), a novel training approach for neural-network-like predictors that utilizes explainability, specifically Layer-wise Relevance Propagation(LRP), to assign rewards to individual connections based on their respective contributions to solving a given task. This differs from traditional gradient descent, which updates parameters towards anestimated loss minimum. LFP distributes a reward signal throughout the model without the need for gradient computations. It then strengthens structures that receive positive feedback while reducingthe influence of structures that receive negative feedback. We establish the convergence of LFP theoretically and empirically, and demonstrate its effectiveness in achieving comparable performance to gradient descent on various models and datasets. Notably, LFP overcomes certain limitations associated with gradient-based methods, such as reliance on meaningful derivatives. We further investigate how the different LRP-rules can be extended to LFP, what their effects are on training, as well as potential applications, such as training models with no meaningful derivatives, e.g., step-function activated Spiking Neural Networks (SNNs), or for transfer learning, to efficiently utilize existing knowledge.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出层 wise Feedback Propagation（LFP），一种基于解释的训练方法，使用层 wise Relevance Propagation（LRP）来为解决特定任务中的每个连接分配奖励。这与传统的梯度下降不同，梯度下降更新参数向估计损失最小值。LFP在模型中分配奖励信号，不需要梯度计算。它然后强化收到正面反馈的结构，而减少收到负面反馈的影响。我们 theoretically 和 empirically 证明 LFP 的 converges，并在不同模型和数据集上证明其效果。值得注意的是，LFP 可以超越一些相关的梯度基本方法的限制，如依赖于意义 derivatives。我们还 investigate 如何 extend LRP-rules 到 LFP，它们在训练中的效果，以及潜在应用，如训练无意义 derivatives 的模型，例如步函数激活的神经网络（SNNs），或者用于传输学习，以高效地利用现有的知识。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Language-Models-with-Offline-Reinforcement-Learning-from-Human-Feedback"><a href="#Aligning-Language-Models-with-Offline-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Aligning Language Models with Offline Reinforcement Learning from Human Feedback"></a>Aligning Language Models with Offline Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12050">http://arxiv.org/abs/2308.12050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Hu, Li Tao, June Yang, Chandler Zhou</li>
<li>For: This paper aims to align language models with human preferences using offline reinforcement learning from human feedback (RLHF) frameworks, without relying on online reinforcement learning techniques like Proximal Policy Optimization (PPO) that can be unstable and challenging to tune.* Methods: The authors propose using maximum likelihood estimation (MLE) with filtering, reward-weighted regression (RWR), and Decision Transformer (DT) to align language models to human preferences. They employ a loss function similar to supervised fine-tuning to ensure stable model training, and compare their methods with PPO and other Offline RLHF methods.* Results: The experimental results show that the DT alignment outperforms other Offline RLHF methods and is better than PPO, with a much lower computing resource requirement (around 12.3%) and a simpler machine learning system.<details>
<summary>Abstract</summary>
Learning from human preferences is crucial for language models (LMs) to effectively cater to human needs and societal values. Previous research has made notable progress by leveraging human feedback to follow instructions. However, these approaches rely primarily on online reinforcement learning (RL) techniques like Proximal Policy Optimization (PPO), which have been proven unstable and challenging to tune for language models. Moreover, PPO requires complex distributed system implementation, hindering the efficiency of large-scale distributed training. In this study, we propose an offline reinforcement learning from human feedback (RLHF) framework to align LMs using pre-generated samples without interacting with RL environments. Specifically, we explore maximum likelihood estimation (MLE) with filtering, reward-weighted regression (RWR), and Decision Transformer (DT) to align language models to human preferences. By employing a loss function similar to supervised fine-tuning, our methods ensure more stable model training than PPO with a simple machine learning system~(MLSys) and much fewer (around 12.3\%) computing resources. Experimental results demonstrate the DT alignment outperforms other Offline RLHF methods and is better than PPO.
</details>
<details>
<summary>摘要</summary>
学习人类偏好是语言模型（LM）效果服务的关键。过去的研究已经做出了可观的进步，通过使用人类反馈来跟进 instruction。然而，这些方法主要依赖于在线强化学习（RL）技术，如 proximal policy optimization（PPO），这些技术有unstable和难于调整的问题。另外，PPO需要复杂的分布式系统实现，这会阻碍大规模分布式训练的效率。在这种情况下，我们提出了一个偏好RLHF框架，用于不需要与RL环境交互的情况下，使语言模型与人类偏好相匹配。具体来说，我们explore maximum likelihood estimation（MLE）with filtering、reward-weighted regression（RWR）和Decision Transformer（DT）来对语言模型进行偏好调整。我们的方法使用一个类似于超vised fine-tuning的损失函数，以确保更稳定的模型训练，并且只需要相对较少的计算资源（约12.3%）。实验结果表明，DT调整超过其他Offline RLHF方法，并且比PPO更好。
</details></li>
</ul>
<hr>
<h2 id="Towards-Privacy-Supporting-Fall-Detection-via-Deep-Unsupervised-RGB2Depth-Adaptation"><a href="#Towards-Privacy-Supporting-Fall-Detection-via-Deep-Unsupervised-RGB2Depth-Adaptation" class="headerlink" title="Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation"></a>Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12049">http://arxiv.org/abs/2308.12049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/1015206533/privacy_supporting_fall_detection">https://github.com/1015206533/privacy_supporting_fall_detection</a></li>
<li>paper_authors: Hejun Xiao, Kunyu Peng, Xiangsheng Huang, Alina Roitberg1, Hao Li, Zhaohui Wang, Rainer Stiefelhagen</li>
<li>for: 预防跌倒，提高健康监测的效果</li>
<li>methods: 利用深度感知器和RGB视频数据，通过域 adaptation进行跌倒检测</li>
<li>results: 实现了在不需要细致图像数据的情况下，使用RGB视频数据进行跌倒检测，并达到了最佳效果<details>
<summary>Abstract</summary>
Fall detection is a vital task in health monitoring, as it allows the system to trigger an alert and therefore enabling faster interventions when a person experiences a fall. Although most previous approaches rely on standard RGB video data, such detailed appearance-aware monitoring poses significant privacy concerns. Depth sensors, on the other hand, are better at preserving privacy as they merely capture the distance of objects from the sensor or camera, omitting color and texture information. In this paper, we introduce a privacy-supporting solution that makes the RGB-trained model applicable in depth domain and utilizes depth data at test time for fall detection. To achieve cross-modal fall detection, we present an unsupervised RGB to Depth (RGB2Depth) cross-modal domain adaptation approach that leverages labelled RGB data and unlabelled depth data during training. Our proposed pipeline incorporates an intermediate domain module for feature bridging, modality adversarial loss for modality discrimination, classification loss for pseudo-labeled depth data and labeled source data, triplet loss that considers both source and target domains, and a novel adaptive loss weight adjustment method for improved coordination among various losses. Our approach achieves state-of-the-art results in the unsupervised RGB2Depth domain adaptation task for fall detection. Code is available at https://github.com/1015206533/privacy_supporting_fall_detection.
</details>
<details>
<summary>摘要</summary>
“fall detection是健康监控中的重要任务，可以让系统发送警示，从而更快地对人员坠落时进行应对。然而，大多数先前的方法仅使用标准的RGB影像数据，这种细节意识敏感的监控具有重要的隐私问题。深度感知器，则可以更好地保持隐私，因为它们仅capture物体对感知器或相机的距离，排除颜色和 texture信息。在本文中，我们介绍了一个关于隐私支持的解决方案，让RGB模型在深度领域中可用并在试用时使用深度数据进行坠落探测。”“实现跨模式的坠落探测，我们提出了一个不需要 labels的RGB to Depth（RGB2Depth）跨模式领域适应方法。我们的提案包括一个中继领域模组，用于Feature Bridging，模组挑战数据的类型和大小，以及一个对于模组的挑战数据的多对多挑战数据。我们还使用了一个对于source和target领域的多对多挑战数据，以及一个新的适应式损失调整方法，以改善不同损失函数之间的协调。”“我们的方法在RGB2Depth领域适应任务中得到了state-of-the-art的结果。我们的代码可以在https://github.com/1015206533/privacy_supporting_fall_detection中找到。”
</details></li>
</ul>
<hr>
<h2 id="CgT-GAN-CLIP-guided-Text-GAN-for-Image-Captioning"><a href="#CgT-GAN-CLIP-guided-Text-GAN-for-Image-Captioning" class="headerlink" title="CgT-GAN: CLIP-guided Text GAN for Image Captioning"></a>CgT-GAN: CLIP-guided Text GAN for Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12045">http://arxiv.org/abs/2308.12045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lihr747/cgtgan">https://github.com/lihr747/cgtgan</a></li>
<li>paper_authors: Jiarui Yu, Haoran Li, Yanbin Hao, Bin Zhu, Tong Xu, Xiangnan He</li>
<li>for: The paper is written for improving image captioning without human-annotated image-caption pairs, using a text-only training paradigm and incorporating images into the training process.</li>
<li>methods: The paper proposes a CLIP-guided text GAN (CgT-GAN) that uses adversarial training and a CLIP-based reward to provide semantic guidance, and introduces a novel semantic guidance reward called CLIP-agg that aligns the generated caption with a weighted text embedding.</li>
<li>results: The paper shows that CgT-GAN outperforms state-of-the-art methods significantly across all metrics on three subtasks (ZS-IC, In-UIC, and Cross-UIC).Here’s the simplified Chinese text version of the three key information points:</li>
<li>for: 文章是为了提高无人注意图像描述的image captioning，使用文本单独训练 paradigm，并在训练过程中包含图像。</li>
<li>methods: 文章提出了一种基于CLIP的文本GAN（CgT-GAN），使用对抗训练和基于CLIP的奖励来提供语义指导，并引入了一种新的语义指导奖励called CLIP-agg。</li>
<li>results: 文章表明，CgT-GAN在三个任务（ZS-IC、In-UIC和Cross-UIC）上比州前方法显著出众，包括所有指标。<details>
<summary>Abstract</summary>
The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training/inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to "see" real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN's discriminator and the semantic guidance reward computed by the CLIP-based reward module. In addition to the cosine similarity as the semantic guidance reward (i.e., CLIP-cos), we further introduce a novel semantic guidance reward called CLIP-agg, which aligns the generated caption with a weighted text embedding by attentively aggregating the entire corpus. Experimental results on three subtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms state-of-the-art methods significantly across all metrics. Code is available at https://github.com/Lihr747/CgtGAN.
</details>
<details>
<summary>摘要</summary>
大规模的视觉语言预训练模型CLIP（Contrastive Language-Image Pre-training）在没有人类标注的场景下提高了图像描述。最新的CLIP基于的图像描述方法采用文本只训练 paradigm，即在共享 embedding 空间中重建文本。然而，这些方法受到训练/推断差距或巨大的存储要求的限制。因为在实际世界中可以轻松地获得图像，我们提出了CLIP引导的文本GAN（CgT-GAN），它将图像 inclusion 到训练过程中，使模型可以"看到"实际的视觉Modal。特别是，我们使用对抗训练来教育CgT-GAN模仿外部文本聚合体和CLIP基于的奖励来提供语义指导。描述生成器被同时激励基于描述自然度计算从GAN的探测器和CLIP基于的奖励模块计算的语义指导奖励。此外，我们还引入了一种新的语义指导奖励called CLIP-agg，它将生成的描述与权重文本embedding进行协调，通过对整个聚合体进行注意力聚集来实现。实验结果在三个SUB Task（ZS-IC、In-UIC和Cross-UIC）中显示，CgT-GAN具有与状态艺术方法相比明显的优势，在所有指标上出现显著提升。代码可以在https://github.com/Lihr747/CgtGAN 上找到。
</details></li>
</ul>
<hr>
<h2 id="A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks"><a href="#A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks" class="headerlink" title="A multiobjective continuation method to compute the regularization path of deep neural networks"></a>A multiobjective continuation method to compute the regularization path of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12044">http://arxiv.org/abs/2308.12044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aamakor/continuation-method">https://github.com/aamakor/continuation-method</a></li>
<li>paper_authors: Augustina C. Amakor, Konstantin Sonntag, Sebastian Peitz</li>
<li>for: 本文旨在提出一种高效的方法，以优化深度神经网络（DNN）的稀疏性和损失函数之间的衔接。</li>
<li>methods: 本文使用了一种基于多目标优化的算法，以 aproximate Pareto front 上的整个衔接。</li>
<li>results: 数据示出了该算法的高效性和通用性，并且可以在不同的梯度下进行数据的验证。此外，本文还证明了知道衔接路径可以帮助网络 Parametrization 得到更好的泛化性。<details>
<summary>Abstract</summary>
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto front for the above-mentioned objectives in a very efficient manner. We present numerical examples using both deterministic and stochastic gradients. We furthermore demonstrate that knowledge of the regularization path allows for a well-generalizing network parametrization.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）中的稀畴性是一个非常强地需求的特性，因为它确保了数学效率、提高模型解释性（由于更少的相关特征），并且提高了模型的稳定性。在线性机器学习方法基于的模型中，已经知道存在一个连接到最稀 Solution 的梯度路径，这个梯度路径被称为规regularization path。很近期，有一个首次尝试将这个概念扩展到 DNN 中，通过对 empirical loss 和稀畴性（$\ell^1$ 范数）作为两个矛盾的目标，解决 resulting 多目标优化问题。然而，由于 $\ell^1$ 范数的非滑坡性和参数的高数量，这种方法并不很有效从计算机科学的角度。为了解决这个限制，我们提出了一个算法，可以高效地 aproximate 整个 Pareto front 上的目标。我们通过 deterministic 和 Stochastic 梯度来进行数值示例。此外，我们还证明了知道规regularization path 可以提供一个良好的网络参数化。
</details></li>
</ul>
<hr>
<h2 id="IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning"><a href="#IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning" class="headerlink" title="IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning"></a>IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12043">http://arxiv.org/abs/2308.12043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feiyuzhang98/increlora">https://github.com/feiyuzhang98/increlora</a></li>
<li>paper_authors: Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, Yiming Qian</li>
<li>for: 这篇论文主要针对于大型预训语言模型（PLMs）的精致化训练进行优化，以减少训练和储存成本，特别是在大量下游任务时。</li>
<li>methods: 这篇论文提出了一种增量化对应（IncreLoRA）方法，将预训模组中的参数转换为可变的权重矩阵，以提高模组之间的通信。此外，这篇论文还提出了一些对LoRA的修正方法，以提高其效能。</li>
<li>results: 在GLUE测试集上，这篇论文的方法与基eline相比，具有更高的参数效率，特别是在资源不足的情况下。另外，这篇论文还展示了对LoRA的修正方法可以对模组之间的通信进行更好的控制。<details>
<summary>Abstract</summary>
With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it is not limited by the initial number of training parameters, and each parameter matrix has a higher rank upper bound for the same training overhead. We conduct extensive experiments on GLUE to demonstrate the effectiveness of IncreLoRA. The results show that our method owns higher parameter efficiency, especially when under the low-resource settings where our method significantly outperforms the baselines. Our code is publicly available.
</details>
<details>
<summary>摘要</summary>
随着预训语言模型（PLM）的大小的增加，精细调整所有模型参数不是efficient，特别是当有大量下游任务时，会导致显著的训练和存储成本。许多参数精细调整（PEFT）approach已经提出，其中LoRA是一个代表性的方法，它在每个目标模块中注入可学习的排序矩阵。然而，LoRA忽略了参数在不同模块中的重要性。为解决这个问题，许多工作已经提出了对LoRA的剪枝。然而，在限制的训练条件下，剪枝后的参数矩阵的rankUpperBound仍然受到先前设置的值的影响。因此，我们提出了IncreLoRA，一种逐步分配参数的方法，它在训练过程中基于每个模块的重要性分数进行逐步添加可学习参数。这种方法与剪枝方法不同，它不受限于初始训练参数的数量，每个参数矩阵的rankUpperBound都高于同样的训练负担。我们在GLUE上进行了广泛的实验，结果表明我们的方法具有更高的参数效率，特别是在低资源设置下，我们的方法显著超过了基eline。我们的代码公开 disponibles。
</details></li>
</ul>
<hr>
<h2 id="PREFER-Prompt-Ensemble-Learning-via-Feedback-Reflect-Refine"><a href="#PREFER-Prompt-Ensemble-Learning-via-Feedback-Reflect-Refine" class="headerlink" title="PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine"></a>PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12033">http://arxiv.org/abs/2308.12033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zcrwind/prefer">https://github.com/zcrwind/prefer</a></li>
<li>paper_authors: Chenrui Zhang, Lin Liu, Jinpeng Wang, Chuyuan Wang, Xiao Sun, Hongyu Wang, Mingchen Cai</li>
<li>for: 提高 Large Language Model (LLM) 的表现，增强其能力。</li>
<li>methods: 提出了一种简单、通用、自动化的方法 named PREFER，通过反馈机制和迭代优化来提高 LLM 的表现。</li>
<li>results: 经过广泛的实验，我们的 PREFER 方法在多种任务上达到了 state-of-the-art 水平，超过了现有方法的表现。<details>
<summary>Abstract</summary>
As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Pompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement. Moreover, to enhance stability of the prompt effect evaluation, we propose a novel prompt bagging method involving forward and backward thinking, which is superior to majority voting and is beneficial for both feedback and weight calculation in boosting. Extensive experiments demonstrate that our PREFER achieves state-of-the-art performance in multiple types of tasks by a significant margin. We have made our code publicly available.
</details>
<details>
<summary>摘要</summary>
为了更好地利用大语言模型（LLM）的能力，提问最近在多种复杂任务中表现出了无 precedent 的能力。为了进一步提高性能，提问ensemble 已经吸引了很多关注，以解决 LLM 的幻觉和不稳定性。然而，现有的方法通常采用两个阶段 paradigm，需要大量的手动努力来预先准备提问集，并且无法 direktly 优化不同的弱学习者。在这篇论文中，我们提出了一种简单、通用和自动的方法 named PREFER (提问组合学习 via 反馈反思改进)，以解决所提到的限制。具体来说，我们知道弱学习者在扩大时会关注困难的示例，PREFER 建立了反馈机制，以反思现有弱学习者的不足。基于这，LLM 需要自动生成新的提问，进行迭代改进。此外，为了增强提问效果评估的稳定性，我们提出了一种新的提问袋裹法，其包括前向和后向思考，比较有利于提问评估和权重计算在扩大中。我们的 EXPERIMENT 表明，我们的 PREFER 可以在多种任务中达到 estado 的表现，与当前最佳方法相比，差距非常大。我们的代码已经公开发布。
</details></li>
</ul>
<hr>
<h2 id="CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures"><a href="#CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures" class="headerlink" title="CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures"></a>CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12031">http://arxiv.org/abs/2308.12031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Gherardini, Varun Ravi Varma, Karol Capala, Roger Woods, Jose Sousa</li>
<li>for: 本研究旨在提高安全分析的解释能力，为现代人工智能发展提供帮助。</li>
<li>methods: 本研究使用了CACTUS，一种可解释的人工智能工具，以提高安全分析的效果。CACTUS支持分类特征，保持特征的原始含义，提高内存使用率，并通过并行计算加速计算速度。</li>
<li>results: 本研究在应用于美洲矿业癌症和甲状腺癌0387数据集中展现出色的表现，并且可以显示每个类别中特征的频率和排名。<details>
<summary>Abstract</summary>
The availability of large data sets is providing an impetus for driving current artificial intelligent developments. There are, however, challenges for developing solutions with small data sets due to practical and cost-effective deployment and the opacity of deep learning models. The Comprehensive Abstraction and Classification Tool for Uncovering Structures called CACTUS is presented for improved secure analytics by effectively employing explainable artificial intelligence. It provides additional support for categorical attributes, preserving their original meaning, optimising memory usage, and speeding up the computation through parallelisation. It shows to the user the frequency of the attributes in each class and ranks them by their discriminative power. Its performance is assessed by application to the Wisconsin diagnostic breast cancer and Thyroid0387 data sets.
</details>
<details>
<summary>摘要</summary>
大量数据的可用性正为现代人工智能发展提供了推动力。然而，对小数据集的解决方案存在实用和成本效益的挑战，尤其是深度学习模型的透明性问题。本文提出了一种名为“CACTUS”的全面抽象分类工具，用于提高安全分析。它能够有效地使用可解释人工智能，并且支持 categorical 特征，保持原始含义，优化内存使用情况，并通过并行计算加速计算。它可以在用户看到每个类别 attribute 的频率和排名它们的抑制力。它的性能被评估通过应用于美国威斯康星诊断乳腺癌和 thyroid0387 数据集。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning"><a href="#Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning" class="headerlink" title="Prompt-Based Length Controlled Generation with Reinforcement Learning"></a>Prompt-Based Length Controlled Generation with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12030">http://arxiv.org/abs/2308.12030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, Qun Liu</li>
<li>for: 提高 GPT 类模型的准确性和效率，以便更好地满足不同场景中的需求。</li>
<li>methods: 采用了反馈学习，通过训练或使用规则来定义奖励模型，以便控制 GPT 类模型的生成长度。</li>
<li>results: 在 популяр的数据集 CNNDM 和 NYT 上实现了更高的描述精度和准确性。<details>
<summary>Abstract</summary>
Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. We believe this length-controllable ability can provide more potentials towards the era of LLMs.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose a prompt-based length control method using reinforcement learning with a trainable or rule-based reward model. Our method aims to achieve length-controlled generation in GPT-style LLMs, and experiments show that it significantly improves the accuracy of prompt-based length control for summarization tasks on popular datasets like CNNDM and NYT. We believe that this length-controllable ability has great potential in the era of LLMs.
</details></li>
</ul>
<hr>
<h2 id="A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning"><a href="#A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning" class="headerlink" title="A Scale-Invariant Task Balancing Approach for Multi-Task Learning"></a>A Scale-Invariant Task Balancing Approach for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12029">http://arxiv.org/abs/2308.12029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baijiong Lin, Weisen Jiang, Feiyang Ye, Yu Zhang, Pengguang Chen, Ying-Cong Chen, Shu Liu</li>
<li>for: 提高多任务学习（MTL）中任务均衡的问题，以便同时学习多个相关任务并实现优秀表现。</li>
<li>methods: 提出了一种具有整数归一化特性的多任务学习方法（SI-MTL），通过对所有任务损失进行对数变换来保证损失水平的均衡，并通过SI-G方法对所有任务导数进行归一化，使所有任务导数具有同一个 максималь去向量范围。</li>
<li>results: 经过广泛的实验表明，SI-G方法能够有效地约束任务导数，而SI-MTL方法能够在多个 benchmark 数据集上达到领先的性能水平。<details>
<summary>Abstract</summary>
Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL），一种同时学习多个相关任务的学习方法，在各个领域取得了很大成功。然而，任务均衡仍然是MTL中的主要挑战，因为任务损失/梯度的尺度差异常常导致性能下降。在这篇论文中，我们提出了一种减小任务均衡问题的扩展MTL方法（SI-MTL）。具体来说，SI-MTL包括一种对所有任务损失进行对数变换，以保证损失水平上的减小，以及一种梯度均衡方法SI-G，该方法将所有任务梯度 норmalizes到最大梯度 норма的同一个范围内。我们在多个标准数据集上进行了广泛的实验，并经常证明了SI-G的有效性和SI-MTL的状态之最性。
</details></li>
</ul>
<hr>
<h2 id="LKPNR-LLM-and-KG-for-Personalized-News-Recommendation-Framework"><a href="#LKPNR-LLM-and-KG-for-Personalized-News-Recommendation-Framework" class="headerlink" title="LKPNR: LLM and KG for Personalized News Recommendation Framework"></a>LKPNR: LLM and KG for Personalized News Recommendation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12028">http://arxiv.org/abs/2308.12028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuan-zw/lkpnr">https://github.com/xuan-zw/lkpnr</a></li>
<li>paper_authors: Chen hao, Xie Runfeng, Cui Xiangyang, Yan Zhou, Wang Xin, Xuan Zhanwei, Zhang Kai</li>
<li>for: 提高新闻推荐系统的准确率，解决传统方法对复杂新闻文本的理解困难和长尾问题。</li>
<li>methods:  combining Large Language Models (LLM) and Knowledge Graphs (KG) into semantic representations of traditional methods, using LLMs’ powerful text understanding ability to generate news representations containing rich semantic information, and combining information about news entities and mining high-order structural information through multiple hops in KG.</li>
<li>results:  compared with various traditional models, the framework significantly improves the recommendation effect, and the successful integration of LLM and KG in the framework has established a feasible path for achieving more accurate personalized recommendations in the news field.<details>
<summary>Abstract</summary>
Accurately recommending candidate news articles to users is a basic challenge faced by personalized news recommendation systems. Traditional methods are usually difficult to grasp the complex semantic information in news texts, resulting in unsatisfactory recommendation results. Besides, these traditional methods are more friendly to active users with rich historical behaviors. However, they can not effectively solve the "long tail problem" of inactive users. To address these issues, this research presents a novel general framework that combines Large Language Models (LLM) and Knowledge Graphs (KG) into semantic representations of traditional methods. In order to improve semantic understanding in complex news texts, we use LLMs' powerful text understanding ability to generate news representations containing rich semantic information. In addition, our method combines the information about news entities and mines high-order structural information through multiple hops in KG, thus alleviating the challenge of long tail distribution. Experimental results demonstrate that compared with various traditional models, the framework significantly improves the recommendation effect. The successful integration of LLM and KG in our framework has established a feasible path for achieving more accurate personalized recommendations in the news field. Our code is available at https://github.com/Xuan-ZW/LKPNR.
</details>
<details>
<summary>摘要</summary>
基于大语言模型和知识图的新闻个性化推荐系统是一个基本挑战。传统方法通常难以捕捉新闻文本中复杂的 semantic information，导致推荐结果不 satisfactory。另外，这些传统方法更适合有活跃用户行为的活跃用户。然而，它们无法有效解决“长尾问题”，即不活跃用户。为解决这些问题，本研究提出了一种新的通用框架，combines Large Language Models (LLM) and Knowledge Graphs (KG) into semantic representations of traditional methods。为了提高新闻文本中的semantic理解，我们使用 LLMs的强大文本理解能力生成新闻表示形式，具有丰富的semantic信息。此外，我们的方法结合新闻实体信息，通过多个层次结构信息在 KG 中挖掘高阶结构信息，从而缓解长尾分布的挑战。实验结果表明，与各种传统模型相比，我们的框架显著提高了推荐效果。我们成功地将 LLM 和 KG 集成到我们的框架中，建立了实现更高精度的个性化推荐在新闻领域的可行道路。我们的代码可以在 <https://github.com/Xuan-ZW/LKPNR> 中找到。
</details></li>
</ul>
<hr>
<h2 id="From-Instructions-to-Intrinsic-Human-Values-–-A-Survey-of-Alignment-Goals-for-Big-Models"><a href="#From-Instructions-to-Intrinsic-Human-Values-–-A-Survey-of-Alignment-Goals-for-Big-Models" class="headerlink" title="From Instructions to Intrinsic Human Values – A Survey of Alignment Goals for Big Models"></a>From Instructions to Intrinsic Human Values – A Survey of Alignment Goals for Big Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12014">http://arxiv.org/abs/2308.12014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, Xing Xie</li>
<li>for: 本研究旨在探讨现有工作中的各种Alignment Goals，以帮助确定最重要的目标。</li>
<li>methods: 本研究从两个角度 investigate了现有工作：一是对Alignment Goals的定义，二是对Alignment evaluation的研究。</li>
<li>results: 研究发现了三级别的Alignment Goals，并发现了目标转化从基本能力到价值观，这表明了可以利用内在人类价值作为Enhanced LLMs的Alignment goal。<details>
<summary>Abstract</summary>
Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced LLMs. Based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.
</details>
<details>
<summary>摘要</summary>
大型模型，如大语言模型（LLMs），是通常在庞大数据上预训练的模型，不仅在多种任务上显示出较好的性能，而且具有emergent功能，与更小的模型不同。然而，大型模型与人类生活的日益相互 penetration可能会带来潜在的风险，可能会对社会造成严重的危害。因此，许多努力已经被做出，以使LMMs与人类更好地配合，使其更好地遵从用户的指令和满足人类的偏好。然而，`与何进行对齐'的问题尚未得到了完全的讨论，不当的对齐目标可能会倒退。在这篇论文中，我们进行了完整的对齐目标的检查，并跟踪它们的演化路径，以帮助identify最重要的目标。特别是，我们从两个视角 investigate existing work：对齐目标的定义和对齐评估。我们的分析覆盖了三级别的对齐目标，并显示了对齐目标的变化从基本能力到价值观，这表明了内置人类价值的可能性作为LLMs的对齐目标，以提高它们的性能。基于这些结果，我们进一步讨论了实现这种内置价值对齐的挑战，并提供了未来对big models的对齐研究的可用资源。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Noise-driven-Generative-Diffusion-Models"><a href="#Quantum-Noise-driven-Generative-Diffusion-Models" class="headerlink" title="Quantum-Noise-driven Generative Diffusion Models"></a>Quantum-Noise-driven Generative Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12013">http://arxiv.org/abs/2308.12013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Parigi, Stefano Martina, Filippo Caruso</li>
<li>for: 这个论文旨在提出和讨论量子扩散模型的量子扩散模型，用于生成复杂的数据分布。</li>
<li>methods: 该论文使用机器学习技术实现生成模型，并利用量子随机过程中的偶极性、Entanglement和噪声来超越经典扩散模型的计算困难。</li>
<li>results: 该论文预计可以开拓新的量子感知或量子基于的生成扩散算法，用于解决经典任务，如数据生成&#x2F;预测，并具有广泛的实际应用，如气候预测、神经科学、交通流量分析和财务预测。<details>
<summary>Abstract</summary>
Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a very remarkably beneficial key ingredient to generate much more complex probability distributions that would be difficult or even impossible to express classically, and from which a quantum processor might sample more efficiently than a classical one. Therefore, our results are expected to pave the way for new quantum-inspired or quantum-based generative diffusion algorithms addressing more powerfully classical tasks as data generation/prediction with widespread real-world applications ranging from climate forecasting to neuroscience, from traffic flow analysis to financial forecasting.
</details>
<details>
<summary>摘要</summary>
通过机器学习技术实现的生成模型是一种 poderoso工具，可以从 finite 数据样本中推断出复杂而未知的数据分布，生成新的合成数据。扩散模型是一种emerging框架，最近已经超越了生成对抗网络在创造合成文本和高质量图像方面的性能。在这里，我们提出并讨论了量子扩散模型的普适化，即利用量子噪声驱动的三种量子扩散生成模型，可以在真正的量子系统上进行实验。我们的想法是利用量子特有的非rivial相互作用，即准确性、耦合和噪声，以超越经典扩散模型的主要计算危机。因此，我们建议利用量子噪声不作为问题，而是作为非常有利的重要组分，以生成更复杂的概率分布，这些分布可能是经典计算不能表达，而量子处理器可能可以更高效地采样这些分布。因此，我们的结果预计将为新的量子激发或量子基于的生成扩散算法开拓出新的应用领域，从气候预测到神经科学，从交通流量分析到金融预测。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-Representation-Learning-Across-Domains"><a href="#Trustworthy-Representation-Learning-Across-Domains" class="headerlink" title="Trustworthy Representation Learning Across Domains"></a>Trustworthy Representation Learning Across Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12315">http://arxiv.org/abs/2308.12315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronghang Zhu, Dongliang Guo, Daiqing Qi, Zhixuan Chu, Xiang Yu, Sheng Li</li>
<li>for: 这个论文的目的是提出一个可靠的表示学习框架，以适应实际应用场景中的跨domain问题。</li>
<li>methods: 该论文使用了四个概念，即Robustness、Privacy、Fairness和Explainability，以提供一个全面的文献复盘。</li>
<li>results: 该论文提出了一个基于这四个概念的信任worthy表示学习框架，并对现有方法进行了概括和分析。<details>
<summary>Abstract</summary>
As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.
</details>
<details>
<summary>摘要</summary>
Inspired by the principles of trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four key concepts: robustness, privacy, fairness, and explainability. This comprehensive literature review provides an overview of this research direction, including the details of the proposed trustworthy framework for representation learning across domains, a summary of existing methods that align with the four concepts, and insights and discussions on future research directions. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. We then provide a comprehensive overview of existing methods that align with the four concepts, including robustness, privacy, fairness, and explainability. Finally, we conclude this survey with insights and discussions on future research directions.The proposed trustworthy framework for representation learning across domains includes four key concepts:1. Robustness: The ability of the model to perform well in the presence of noise, outliers, or distributional shifts.2. Privacy: The protection of sensitive information and the prevention of unauthorized access or misuse.3. Fairness: The avoidance of bias and discrimination in the model's predictions, ensuring that all individuals or groups are treated equally and without prejudice.4. Explainability: The ability to provide clear and understandable explanations for the model's predictions, allowing users to understand the reasoning behind the model's decisions.Existing methods for the trustworthy framework from these four concepts include:1. Robustness: Techniques such as data augmentation, adversarial training, and ensemble methods can improve the model's robustness to noise and distributional shifts.2. Privacy: Methods such as differential privacy, secure multi-party computation, and homomorphic encryption can protect sensitive information and prevent unauthorized access.3. Fairness: Techniques such as fair batch normalization, fair representation learning, and fair evaluation metrics can help to mitigate bias and discrimination in the model's predictions.4. Explainability: Approaches such as feature importance, saliency maps, and model interpretability techniques can provide clear explanations for the model's predictions.In conclusion, this survey provides a comprehensive overview of the trustworthy representation learning across domains framework, including the proposed framework and existing methods that align with the four key concepts. We also discuss insights and future research directions in this field, highlighting the importance of trustworthy AI systems in our daily lives and human society.
</details></li>
</ul>
<hr>
<h2 id="Topical-Chat-Towards-Knowledge-Grounded-Open-Domain-Conversations"><a href="#Topical-Chat-Towards-Knowledge-Grounded-Open-Domain-Conversations" class="headerlink" title="Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations"></a>Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11995">http://arxiv.org/abs/2308.11995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexa/Topical-Chat">https://github.com/alexa/Topical-Chat</a></li>
<li>paper_authors: Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-Tur</li>
<li>for: 这个论文的目的是提供一个基于知识的人机对话集，帮助开发更加深入、有趣的人机对话AI。</li>
<li>methods: 论文使用了知识基础的人机对话集，并在这个集合中采用了无显式角色的对话方式。</li>
<li>results: 论文通过对这个知识基础的人机对话集进行自动和人工评价，提出了一些state-of-the-art的对话模型。<details>
<summary>Abstract</summary>
Building socialbots that can have deep, engaging open-domain conversations with humans is one of the grand challenges of artificial intelligence (AI). To this end, bots need to be able to leverage world knowledge spanning several domains effectively when conversing with humans who have their own world knowledge. Existing knowledge-grounded conversation datasets are primarily stylized with explicit roles for conversation partners. These datasets also do not explore depth or breadth of topical coverage with transitions in conversations. We introduce Topical-Chat, a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don't have explicitly defined roles, to help further research in open-domain conversational AI. We also train several state-of-the-art encoder-decoder conversational models on Topical-Chat and perform automated and human evaluation for benchmarking.
</details>
<details>
<summary>摘要</summary>
建立社交机器人，能够与人类进行深入有趣的开放领域对话，是人工智能（AI）的极大挑战之一。为此，机器人需要能够有效地利用多个领域的世界知识进行对话。现有的知识基础对话数据集主要是通过显式角色定义对话伙伴进行预设。这些数据集还不探讨对话的深度或广度，也没有探讨对话的转变。我们介绍Topical-Chat，一个基于知识的人类对话数据集，其下面知识覆盖8个广泛的主题，对话伙伴没有显式定义角色，以便进一步推动开放领域对话AI的研究。我们还在Topical-Chat上训练了多种当今最佳encoder-decoder对话模型，并进行自动和人类评估，以便作为参考。
</details></li>
</ul>
<hr>
<h2 id="Critical-Evaluation-of-Artificial-Intelligence-as-Digital-Twin-of-Pathologist-for-Prostate-Cancer-Pathology"><a href="#Critical-Evaluation-of-Artificial-Intelligence-as-Digital-Twin-of-Pathologist-for-Prostate-Cancer-Pathology" class="headerlink" title="Critical Evaluation of Artificial Intelligence as Digital Twin of Pathologist for Prostate Cancer Pathology"></a>Critical Evaluation of Artificial Intelligence as Digital Twin of Pathologist for Prostate Cancer Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11992">http://arxiv.org/abs/2308.11992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Okyaz Eminaga, Mahmoud Abbas, Christian Kunder, Yuri Tolkach, Ryan Han, James D. Brooks, Rosalie Nolley, Axel Semjonow, Martin Boegemann, Robert West, Jin Long, Richard Fan, Olaf Bettendorf</li>
<li>for: 这项研究旨在测试一种基于人工智能的 Digitaltwin 技术，用于检测 próstate cancer 和分类。</li>
<li>methods: 研究使用了 2,603 个 histology 图像，由 Hematoxylin 和 Eosin 染色。使用了多种因素对 prostate cancer 的诊断和分类进行了分析。</li>
<li>results: 研究发现，vPatho 可以与人类Pathologist 相比，在 prostate cancer 的检测和卷积量测量方面具有相当的表现。但是，在 tumor grading 方面，vPatho 和人类Pathologist 之间存在一定的不一致。此外，研究还发现了一些可能导致 grade 不一致的因素，如 tumor 的垂直扩展和抽屉含量。<details>
<summary>Abstract</summary>
Prostate cancer pathology plays a crucial role in clinical management but is time-consuming. Artificial intelligence (AI) shows promise in detecting prostate cancer and grading patterns. We tested an AI-based digital twin of a pathologist, vPatho, on 2,603 histology images of prostate tissue stained with hematoxylin and eosin. We analyzed various factors influencing tumor-grade disagreement between vPatho and six human pathologists. Our results demonstrated that vPatho achieved comparable performance in prostate cancer detection and tumor volume estimation, as reported in the literature. Concordance levels between vPatho and human pathologists were examined. Notably, moderate to substantial agreement was observed in identifying complementary histological features such as ductal, cribriform, nerve, blood vessels, and lymph cell infiltrations. However, concordance in tumor grading showed a decline when applied to prostatectomy specimens (kappa = 0.44) compared to biopsy cores (kappa = 0.70). Adjusting the decision threshold for the secondary Gleason pattern from 5% to 10% improved the concordance level between pathologists and vPatho for tumor grading on prostatectomy specimens (kappa from 0.44 to 0.64). Potential causes of grade discordance included the vertical extent of tumors toward the prostate boundary and the proportions of slides with prostate cancer. Gleason pattern 4 was particularly associated with discordance. Notably, grade discordance with vPatho was not specific to any of the six pathologists involved in routine clinical grading. In conclusion, our study highlights the potential utility of AI in developing a digital twin of a pathologist. This approach can help uncover limitations in AI adoption and the current grading system for prostate cancer pathology.
</details>
<details>
<summary>摘要</summary>
prostata cancer 的生理学pathology 在临床管理中发挥关键作用，但是它很时间消耗。人工智能（AI）表示可能用于检测 prostata cancer 和分化模式。我们使用了一个基于 AI 的pathologist 数字 близнеvPatho 测试了 2,603 个 prostata组织片中的 Hematoxylin 和 Eosin 染色图像。我们分析了不同因素 influencing tumor-grade 的不一致性。结果表明，vPatho 在检测 prostata cancer 和组织体积方面达到了文献报告的性能。我们对 vPatho 和六名人类病理学家之间的一致性进行了分析。注意，在识别 complementary 的 histological 特征方面，such as ductal、cribriform、nerve、血管和lymphocyte infiltration 中，moderate to substantial 的一致性被观察到。然而，在评估 tumor grading 方面，一致性下降到 prostatectomy specimens （kappa = 0.44），比 biopsy cores （kappa = 0.70）更低。通过调整 secondary Gleason 模式的决策阈值从 5% 到 10%，提高了 pathologists 和 vPatho 之间的一致性水平（kappa from 0.44 to 0.64）。可能导致 grade discordance 的原因包括 tumor 的 vertical 分布向 prostata 边界以及检测到的肿瘤组织片的比例。Gleason 模式 4 特别与不一致相关。需要注意的是，grade discordance 与 vPatho 不特别任何一名病理学家的 routine clinical grading 相关。在结论中，我们的研究表明了 AI 可能在开发一个 pathologist 数字 близне的方面具有潜在的用途。这种方法可以帮助揭露 AI 的采用 limitation 和当前的 prostata cancer 生理学pathology 评估系统的限制。
</details></li>
</ul>
<hr>
<h2 id="Relational-Concept-Based-Models"><a href="#Relational-Concept-Based-Models" class="headerlink" title="Relational Concept Based Models"></a>Relational Concept Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11991">http://arxiv.org/abs/2308.11991</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aghoreshwar/Awesome-Customer-Analytics">https://github.com/Aghoreshwar/Awesome-Customer-Analytics</a></li>
<li>paper_authors: Pietro Barbiero, Francesco Giannini, Gabriele Ciravegna, Michelangelo Diligenti, Giuseppe Marra</li>
<li>for: 这个论文的目的是解决关系领域中的深度学习模型可读性问题，这些模型不是专门设计来解决关系问题，而且关系模型不如概念基础模型（CBMs）那样可读性。</li>
<li>methods: 作者提议了一种名为关系概念基础模型（Relational CBMs）的家族关系深度学习方法，这些方法可以在关系领域中提供可读性的任务预测。</li>
<li>results: 作者的实验表明，关系CBMs可以与现有的关系黑obox（黑obox）相比，在图像分类和知识图表链接预测等问题上达到同等的泛化性能，同时支持生成量化的概念基础解释，并能够应对测试时间 intervención，在有限的训练数据 régime和罕见概念监督下也能够保持稳定性。<details>
<summary>Abstract</summary>
The design of interpretable deep learning models working in relational domains poses an open challenge: interpretable deep learning methods, such as Concept-Based Models (CBMs), are not designed to solve relational problems, while relational models are not as interpretable as CBMs. To address this problem, we propose Relational Concept-Based Models, a family of relational deep learning methods providing interpretable task predictions. Our experiments, ranging from image classification to link prediction in knowledge graphs, show that relational CBMs (i) match generalization performance of existing relational black-boxes (as opposed to non-relational CBMs), (ii) support the generation of quantified concept-based explanations, (iii) effectively respond to test-time interventions, and (iv) withstand demanding settings including out-of-distribution scenarios, limited training data regimes, and scarce concept supervisions.
</details>
<details>
<summary>摘要</summary>
<SYS>translate("The design of interpretable deep learning models working in relational domains poses an open challenge: interpretable deep learning methods, such as Concept-Based Models (CBMs), are not designed to solve relational problems, while relational models are not as interpretable as CBMs. To address this problem, we propose Relational Concept-Based Models, a family of relational deep learning methods providing interpretable task predictions. Our experiments, ranging from image classification to link prediction in knowledge graphs, show that relational CBMs (i) match generalization performance of existing relational black-boxes (as opposed to non-relational CBMs), (ii) support the generation of quantified concept-based explanations, (iii) effectively respond to test-time interventions, and (iv) withstand demanding settings including out-of-distribution scenarios, limited training data regimes, and scarce concept supervisions.")</SYS>Here's the translation in Simplified Chinese:“relational deep learning模型的设计问题对开放式挑战：可解释深度学习方法，如基于概念的模型（CBMs），不适合解决关系问题，而关系模型不如CBMs可解释。为解决这个问题，我们提议了关系基于概念模型（Relational Concept-Based Models），这是一种可解释的关系深度学习方法。我们的实验，从图像分类到知识图的链接预测，显示了关系CBMs（i）与现有关系黑盒（as opposed to non-relational CBMs）的一致性表现，（ii）支持生成量化的概念基于解释，（iii）在测试时干预有效，（iv）在具有异常场景、有限训练数据 régime和罕见概念监督的情况下坚持。”
</details></li>
</ul>
<hr>
<h2 id="Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks"><a href="#Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks" class="headerlink" title="Will More Expressive Graph Neural Networks do Better on Generative Tasks?"></a>Will More Expressive Graph Neural Networks do Better on Generative Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11978">http://arxiv.org/abs/2308.11978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiandong Zou, Xiangyu Zhao, Pietro Liò, Yiren Zhao</li>
<li>for: 本研究的目的是探讨 Graph Neural Network (GNN) 在分子图生成任务中的表达能力，并将 GNN 应用于两种不同的生成框架（GCPN 和 GraphAF）中。</li>
<li>methods: 本研究使用了六种不同的 GNN，包括 GCN、GAT、GGN、GraphSAGE、Graph Isomorphism Network (GIN) 和 Graph Attention Network (GAT)，并对这些 GNN 进行了比较。</li>
<li>results: 研究发现，使用更高级的 GNN 可以提高 GCPN 和 GraphAF 在分子图生成任务中的表现，但 GNN 表现不是必需的 condition  для一个好的 GNN-based 生成模型。此外，研究还发现，使用更高级的 GNN 可以使 GCPN 和 GraphAF 在17种非 GNN-based 图生成方法（如变量 autoencoders 和 Bayesian 优化模型）中 achieve state-of-the-art 结果。<details>
<summary>Abstract</summary>
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZINC-250k dataset. Through our extensive experiments, we demonstrate that advanced GNNs can indeed improve the performance of GCPN and GraphAF on molecular generation tasks, but GNN expressiveness is not a necessary condition for a good GNN-based generative model. Moreover, we show that GCPN and GraphAF with advanced GNNs can achieve state-of-the-art results across 17 other non-GNN-based graph generative approaches, such as variational autoencoders and Bayesian optimisation models, on the proposed molecular generative objectives (DRD2, Median1, Median2), which are important metrics for de-novo molecular design.
</details>
<details>
<summary>摘要</summary>
“图生成具有重要挑战，因为它需要预测一个完整的图像，包括多个节点和边，基于只提供的标签。这个任务对于许多实际应用都具有重要性，如新药和分子设计。在过去几年，图生成领域内出现了许多成功的方法。然而，这些方法受到两大缺点的影响：（1）用于这些方法的基本图神经网络（GNN）架构经常未得到充分的探索；（2）这些方法通常只被评估在有限的约束下。为了填补这个差距，我们在图生成任务中研究GNN的表达能力，通过将基本GNN替换为更表达能力的GNN来进行分析。我们在ZINC-250k数据集上进行了广泛的实验，并证明了高级GNN可以提高GCPN和GraphAF在分子生成任务中的表现，但GNN表达能力不是必要的condition。此外，我们还示出了GCPN和GraphAF与高级GNN的组合可以在17种非GNN基于的图生成方法（如变量自动编码器和搜索优化模型）中实现州际级结果，这些对于新药设计是重要的度量。”
</details></li>
</ul>
<hr>
<h2 id="Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression"><a href="#Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression" class="headerlink" title="Approximating Score-based Explanation Techniques Using Conformal Regression"></a>Approximating Score-based Explanation Techniques Using Conformal Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11975">http://arxiv.org/abs/2308.11975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Alkhatib, Henrik Boström, Sofiane Ennadir, Ulf Johansson</li>
<li>for: 这些 papers 是为了解释黑obox 模型的逻辑而写的。</li>
<li>methods: 这些 papers 使用了 computationally costly 的 explanation techniques, such as SHAP, 并提出了一种使用 computationally less costly  regression models 来近似 score-based explanation techniques 的方法。</li>
<li>results: 这些 papers 提出了一些 non-conformity measures 来考虑 approximating explanations 的困难度，并在大规模的 empirical investigation 中证明了其效果。 Results 表明，提出的方法可以significantly improve execution time compared to fast version of SHAP, TreeSHAP, 并且可以生成紧凑的 interval。<details>
<summary>Abstract</summary>
Score-based explainable machine-learning techniques are often used to understand the logic behind black-box models. However, such explanation techniques are often computationally expensive, which limits their application in time-critical contexts. Therefore, we propose and investigate the use of computationally less costly regression models for approximating the output of score-based explanation techniques, such as SHAP. Moreover, validity guarantees for the approximated values are provided by the employed inductive conformal prediction framework. We propose several non-conformity measures designed to take the difficulty of approximating the explanations into account while keeping the computational cost low. We present results from a large-scale empirical investigation, in which the approximate explanations generated by our proposed models are evaluated with respect to efficiency (interval size). The results indicate that the proposed method can significantly improve execution time compared to the fast version of SHAP, TreeSHAP. The results also suggest that the proposed method can produce tight intervals, while providing validity guarantees. Moreover, the proposed approach allows for comparing explanations of different approximation methods and selecting a method based on how informative (tight) are the predicted intervals.
</details>
<details>
<summary>摘要</summary>
黑obox模型的解释技术 oftentimes 使用分数基因 explainable machine-learning 技术。然而，这些解释技术通常 computationally expensive，这限制了它们在时间敏感上下文中的应用。因此，我们提出并 investigate 使用 computationally less costly 回归模型来近似 score-based explanation techniques, such as SHAP。此外，我们提供了雇佣 inductive conformal prediction 框架来提供有效性保证。我们还提出了一些非准确度度量，用于考虑近似解释的困难性，同时保持计算成本低。我们在大规模的实验中发现，我们提posed方法可以在执行时间方面取得显著改进，比如 TreeSHAP 的快速版本。此外，我们的结果还表明，我们的方法可以生成紧凑的间隔，同时提供有效性保证。此外，我们的方法允许比较不同的近似方法的解释，并选择一个基于解释 intervals 的紧凑程度（tightness）。
</details></li>
</ul>
<hr>
<h2 id="Blending-NeRF-Text-Driven-Localized-Editing-in-Neural-Radiance-Fields"><a href="#Blending-NeRF-Text-Driven-Localized-Editing-in-Neural-Radiance-Fields" class="headerlink" title="Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields"></a>Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11974">http://arxiv.org/abs/2308.11974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeonseop Song, Seokhun Choi, Hoseok Do, Chul Lee, Taehyeong Kim</li>
<li>for: 本研究旨在提出一种基于NeRF的模型，用于文本驱动地地方化编辑3D对象，以实现在文本提示中指定的本地修改。</li>
<li>methods: 该模型包含两个NeRF网络：预训练NeRF和可编辑NeRF，以及新的混合操作。使用CLIP模型进行视觉语言对Alignment，引导Blending-NeRF模型在文本提示中添加新物体、修改 текстуры和 removing部分原对象。</li>
<li>results: 我们的广泛实验表明，Blending-NeRF模型能够自然地和地方化地编辑3D对象，从多种文本提示中生成修改后的结果。<details>
<summary>Abstract</summary>
Text-driven localized editing of 3D objects is particularly difficult as locally mixing the original 3D object with the intended new object and style effects without distorting the object's form is not a straightforward process. To address this issue, we propose a novel NeRF-based model, Blending-NeRF, which consists of two NeRF networks: pretrained NeRF and editable NeRF. Additionally, we introduce new blending operations that allow Blending-NeRF to properly edit target regions which are localized by text. By using a pretrained vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects with varying colors and densities, modify textures, and remove parts of the original object. Our extensive experiments demonstrate that Blending-NeRF produces naturally and locally edited 3D objects from various text prompts.
</details>
<details>
<summary>摘要</summary>
文本驱动的3D对象编辑 particullary difficult, because mixing the original 3D object with the intended new object and style effects without distorting the object's form is not a straightforward process. To address this issue, we propose a novel NeRF-based model, Blending-NeRF, which consists of two NeRF networks: pretrained NeRF and editable NeRF. Additionally, we introduce new blending operations that allow Blending-NeRF to properly edit target regions which are localized by text. By using a pretrained vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects with varying colors and densities, modify textures, and remove parts of the original object. Our extensive experiments demonstrate that Blending-NeRF produces naturally and locally edited 3D objects from various text prompts.Here's the translation in Traditional Chinese:文本驱动的3D对象编译 particullary difficult, because mixing the original 3D object with the intended new object and style effects without distorting the object's form is not a straightforward process. To address this issue, we propose a novel NeRF-based model, Blending-NeRF, which consists of two NeRF networks: pretrained NeRF and editable NeRF. Additionally, we introduce new blending operations that allow Blending-NeRF to properly edit target regions which are localized by text. By using a pretrained vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects with varying colors and densities, modify textures, and remove parts of the original object. Our extensive experiments demonstrate that Blending-NeRF produces naturally and locally edited 3D objects from various text prompts.
</details></li>
</ul>
<hr>
<h2 id="Value-of-Assistance-for-Mobile-Agents"><a href="#Value-of-Assistance-for-Mobile-Agents" class="headerlink" title="Value of Assistance for Mobile Agents"></a>Value of Assistance for Mobile Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11961">http://arxiv.org/abs/2308.11961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clair-lab-technion/voa">https://github.com/clair-lab-technion/voa</a></li>
<li>paper_authors: Adi Amuzig, David Dovrat, Sarah Keren</li>
<li>for: 这篇论文是为了解决移动机器人agent的地理位置uncertainty问题，通过增加协助行为来减少uncertainty。</li>
<li>methods: 该论文提出了一种基于Gaussian process的Value of Assistance（VOA）计算方法，用于评估协助行为的效果。</li>
<li>results: 研究人员通过实验和实际应用 validate了VOA计算方法，并证明了VOA可以准确预测机器人的成本减少效果。<details>
<summary>Abstract</summary>
Mobile robotic agents often suffer from localization uncertainty which grows with time and with the agents' movement. This can hinder their ability to accomplish their task. In some settings, it may be possible to perform assistive actions that reduce uncertainty about a robot's location. For example, in a collaborative multi-robot system, a wheeled robot can request assistance from a drone that can fly to its estimated location and reveal its exact location on the map or accompany it to its intended location. Since assistance may be costly and limited, and may be requested by different members of a team, there is a need for principled ways to support the decision of which assistance to provide to an agent and when, as well as to decide which agent to help within a team. For this purpose, we propose Value of Assistance (VOA) to represent the expected cost reduction that assistance will yield at a given point of execution. We offer ways to compute VOA based on estimations of the robot's future uncertainty, modeled as a Gaussian process. We specify conditions under which our VOA measures are valid and empirically demonstrate the ability of our measures to predict the agent's average cost reduction when receiving assistance in both simulated and real-world robotic settings.
</details>
<details>
<summary>摘要</summary>
Mobile robotic agents often suffer from localization uncertainty, which increases over time and with the agents' movement. This can hinder their ability to complete tasks. In some cases, it may be possible to perform assistive actions that reduce uncertainty about a robot's location. For example, in a collaborative multi-robot system, a wheeled robot can request assistance from a drone that can fly to its estimated location and reveal its exact location on the map or accompany it to its intended location. Since assistance may be costly and limited, and may be requested by different team members, there is a need for principled ways to support the decision of which assistance to provide to an agent and when, as well as to decide which agent to help within a team. To address this need, we propose the Value of Assistance (VOA) to represent the expected cost reduction that assistance will yield at a given point of execution. We provide methods to compute VOA based on estimations of the robot's future uncertainty, modeled as a Gaussian process. We specify conditions under which our VOA measures are valid and empirically demonstrate the ability of our measures to predict the agent's average cost reduction when receiving assistance in both simulated and real-world robotic settings.
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-Neural-Networks-applied-to-the-description-of-wave-particle-resonance-in-kinetic-simulations-of-fusion-plasmas"><a href="#Physics-informed-Neural-Networks-applied-to-the-description-of-wave-particle-resonance-in-kinetic-simulations-of-fusion-plasmas" class="headerlink" title="Physics informed Neural Networks applied to the description of wave-particle resonance in kinetic simulations of fusion plasmas"></a>Physics informed Neural Networks applied to the description of wave-particle resonance in kinetic simulations of fusion plasmas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12312">http://arxiv.org/abs/2308.12312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jai Kumar, David Zarzoso, Virginie Grandgirard, Jan Ebert, Stefan Kesselheim</li>
<li>for: 这篇论文使用了哈曼-普朗托纳系统的减少形式版本（1D1V）作为物理信息学神经网络（PINN）的应用测试平台，以解决气体振荡和杯尖不稳定性问题。</li>
<li>methods: 这篇论文首先使用了PINN作为压缩方法来解决哈曼-普朗托纳系统的解，并与标准神经网络进行比较。其次，文章还应用了PINN来解决哈曼-普朗托纳系统，并强调了对部分权重的特殊强调，导致了一种基于自动导数和自动积分的PINN变体，称为可integrable PINN（I-PINN）。</li>
<li>results: 文章的结果表明，PINN可以成功地解决哈曼-普朗托纳系统的问题，并且可以提供更高精度的解决方案。此外，I-PINN还能够更好地处理部分权重的问题，提高了解决速度和精度。<details>
<summary>Abstract</summary>
The Vlasov-Poisson system is employed in its reduced form version (1D1V) as a test bed for the applicability of Physics Informed Neural Network (PINN) to the wave-particle resonance. Two examples are explored: the Landau damping and the bump-on-tail instability. PINN is first tested as a compression method for the solution of the Vlasov-Poisson system and compared to the standard neural networks. Second, the application of PINN to solving the Vlasov-Poisson system is also presented with the special emphasis on the integral part, which motivates the implementation of a PINN variant, called Integrable PINN (I-PINN), based on the automatic-differentiation to solve the partial differential equation and on the automatic-integration to solve the integral equation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用减 simplify 的 Vlasov-Poisson 系统作为测试床，以检验物理学 Informed Neural Network (PINN) 在波动-粒子共振中的可应用性。两个例子被探讨：兰道抑压和块在尾部不稳定。首先，PINN 作为 Vlasov-Poisson 系统解的压缩方法，与标准神经网络进行比较。其次，通过特别强调完 integral part，实现了一种基于自动极点 differentiable 和自动极点 integrate 的 PINN 变体，称为可 integrate PINN（I-PINN），以解决 partial differential equation 和 integral equation。[/INST0]  Here's the text in Traditional Chinese:<<SYS>>使用减 simplify 的 Vlasov-Poisson 系统作为测试床，以检验物理学 Informed Neural Network (PINN) 在波动-粒子共振中的可应用性。两个例子被探讨：兰道抑压和块在尾部不稳定。首先，PINN 作为 Vlasov-Poisson 系统解的压缩方法，与标准神经网络进行比较。其次，通过特别强调完 integral part，实现了一种基于自动极点 differentiable 和自动极点 integrate 的 PINN 变体，称为可 integrate PINN（I-PINN），以解决 partial differential equation 和 integral equation。
</details></li>
</ul>
<hr>
<h2 id="Maintaining-Plasticity-via-Regenerative-Regularization"><a href="#Maintaining-Plasticity-via-Regenerative-Regularization" class="headerlink" title="Maintaining Plasticity via Regenerative Regularization"></a>Maintaining Plasticity via Regenerative Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11958">http://arxiv.org/abs/2308.11958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabh Kumar, Henrik Marklund, Benjamin Van Roy</li>
<li>for: 维护权重的柔软性（plasticity）在处理非站点数据流时降低。</li>
<li>methods: 提出了L2Init方法，即在损失函数中添加L2正则项，以保持初始参数的柔软性。</li>
<li>results: 在不同类型的非站点性问题上，L2Init可以均衡权重的大小和柔软性，并在处理非站点数据流时提高模型的性能。<details>
<summary>Abstract</summary>
In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates plasticity loss. We additionally find that our regularization term reduces parameter magnitudes and maintains a high effective feature rank.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation"><a href="#When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation" class="headerlink" title="When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation"></a>When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11953">http://arxiv.org/abs/2308.11953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Huang, Geng Tian, Ming Tang</li>
<li>for: 这个论文的目的是提出一种名为MiniBatch-SFL的新的分布式学习方法，以解决在分布式学习中发生的“客户端漂移”问题。</li>
<li>methods: 这个方法利用了MiniBatch SGD和分布式学习的概念，在客户端和服务器之间分成了两部分的模型，让客户端只需要训练部分模型，以减少 computation workload。</li>
<li>results: 这个方法可以提高分布式学习的精度，尤其是在非同一的数据时。在实验中，MiniBatch-SFL比传统的分布式学习和Federated learning方法提高了精度，具体来说，可以提高24.1%和17.1%。<details>
<summary>Abstract</summary>
Federated learning (FL) enables collaborative model training across distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can be computationally expensive as the clients need to train the entire model multiple times. SplitFed learning (SFL) is a recent distributed approach that alleviates computation workload at the client device by splitting the model at a cut layer into two parts, where clients only need to train part of the model. However, SFL still suffers from the \textit{client drift} problem when clients' data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This algorithm incorporates MiniBatch SGD into SFL, where the clients train the client-side model in an FL fashion while the server trains the server-side model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and show that the bound of the expected loss can be obtained by analyzing the expected server-side and client-side model updates, respectively. The server-side updates do not depend on the non-IID degree of the clients' datasets and can potentially mitigate client drift. However, the client-side model relies on the non-IID degree and can be optimized by properly choosing the cut layer. Perhaps counter-intuitive, our empirical result shows that a latter position of the cut layer leads to a smaller average gradient divergence and a better algorithm performance. Moreover, numerical results show that MiniBatch-SFL achieves higher accuracy than conventional SFL and FL. The accuracy improvement can be up to 24.1\% and 17.1\% with highly non-IID data, respectively.
</details>
<details>
<summary>摘要</summary>
分布式学习（FL）可以在分布式客户端（例如边缘设备）上进行模型训练，而不需要将原始数据共享。然而，FL可能会很 computationally expensive，因为客户端需要训练整个模型多次。SplitFed learning（SFL）是一种最近的分布式方法，它可以减轻客户端设备上的计算工作负担，通过在一层截分模型两部分，其中客户端只需要训练模型的一部分。然而，SFL仍然会遭受客户端数据高度异步的问题，称为“客户端漂移”问题。为解决这个问题，我们提出了MiniBatch-SFL。这个算法将MiniBatch SGD integrate到SFL中，客户端在FL的方式上训练客户端模型，服务器则在MiniBatch SGD的方式上训练服务器模型。我们分析MiniBatch-SFL的整合和融合，并证明了预期的损失下界可以通过分析服务器和客户端模型更新的预期值来获得。服务器端的更新不виси于客户端数据的异步度，可能减轻客户端漂移问题。然而，客户端模型取决于异步度，可以通过合适地选择截分层来优化。奇怪的是，我们的实验结果表明，将截分层放在后者位置可以减少平均梯度差异和提高算法性能。此外，我们的数值结果表明，MiniBatch-SFL可以在异步数据上达到高度的准确率，比 conventinal SFL和FL高达24.1%和17.1%。
</details></li>
</ul>
<hr>
<h2 id="Pose-Modulated-Avatars-from-Video"><a href="#Pose-Modulated-Avatars-from-Video" class="headerlink" title="Pose Modulated Avatars from Video"></a>Pose Modulated Avatars from Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11951">http://arxiv.org/abs/2308.11951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunjin Song, Bastian Wandt, Helge Rhodin</li>
<li>for: 用于重建动态人体运动和形态，并模型人体的衣物和皮肤塑形。</li>
<li>methods: 使用神经辐射场（NeRF）驱动下方skeleton，并开发了一个两极分支神经网络，以adaptive和explcit方式在频率域中模型人体部件之间的相互关系。</li>
<li>results: 对比州方法，该方法能够更好地保留细节和总体化能力。<details>
<summary>Abstract</summary>
It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state-of-the-art methods in terms of preserving details and generalization capabilities.
</details>
<details>
<summary>摘要</summary>
现在可以使用神经辐射场（NeRF）和下面的骨架来重建动态人体运动和形状。然而，模拟人体皮肤和衣服的塑形仍然是一个挑战。现有的人物模型通常是通过隐藏的方式学习或者通过代理表面来实现。我们的方法受到不同姿势需要唯一频谱分配的观察所启发。忽略这种分配会导致缺陷的纹理和形状细节。我们开发了一个两极分支神经网络，其中第一极是一个图像神经网络，地方地模型体部之间的相关性，带入骨架姿势作为输入。第二极将这些相关特征与一组全局频率相结合，然后修饰特征编码。我们的实验表明，我们的网络在保持细节和泛化能力方面超越了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="High-quality-Image-Dehazing-with-Diffusion-Model"><a href="#High-quality-Image-Dehazing-with-Diffusion-Model" class="headerlink" title="High-quality Image Dehazing with Diffusion Model"></a>High-quality Image Dehazing with Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11949">http://arxiv.org/abs/2308.11949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hu Yu, Jie Huang, Kaiwen Zheng, Man Zhou, Feng Zhao</li>
<li>for: 解压缩雾化图像，即在浓雾场景下还原原始图像的信息。</li>
<li>methods: 本文提出了一种基于DDPM的物理学习框架，即DehazeDDPM，它首先使用物理模型（ASM）模拟雾化任务，然后使用DDPM进行补偿，以恢复雾化induced的信息损失。</li>
<li>results: 对比实验表明，DehazeDDPM在 sintetic和实际雾化数据集上达到了领先的表现。<details>
<summary>Abstract</summary>
Image dehazing is quite challenging in dense-haze scenarios, where quite less original information remains in the hazy image. Though previous methods have made marvelous progress, they still suffer from information loss in content and color in dense-haze scenarios. The recently emerged Denoising Diffusion Probabilistic Model (DDPM) exhibits strong generation ability, showing potential for solving this problem. However, DDPM fails to consider the physics property of dehazing task, limiting its information completion capacity. In this work, we propose DehazeDDPM: A DDPM-based and physics-aware image dehazing framework that applies to complex hazy scenarios. Specifically, DehazeDDPM works in two stages. The former stage physically models the dehazing task with the Atmospheric Scattering Model (ASM), pulling the distribution closer to the clear data and endowing DehazeDDPM with fog-aware ability. The latter stage exploits the strong generation ability of DDPM to compensate for the haze-induced huge information loss, by working in conjunction with the physical modelling. Extensive experiments demonstrate that our method attains state-of-the-art performance on both synthetic and real-world hazy datasets.
</details>
<details>
<summary>摘要</summary>
Image 降霾 quite challenging in dense-haze scenarios, where quite less original information remains in the hazy image. Although previous methods have made marvelous progress, they still suffer from information loss in content and color in dense-haze scenarios. The recently emerged Denoising Diffusion Probabilistic Model (DDPM) exhibits strong generation ability, showing potential for solving this problem. However, DDPM fails to consider the physics property of dehazing task, limiting its information completion capacity. In this work, we propose DehazeDDPM: A DDPM-based and physics-aware image dehazing framework that applies to complex hazy scenarios. Specifically, DehazeDDPM works in two stages. The former stage physically models the dehazing task with the Atmospheric Scattering Model (ASM), pulling the distribution closer to the clear data and endowing DehazeDDPM with fog-aware ability. The latter stage exploits the strong generation ability of DDPM to compensate for the haze-induced huge information loss, by working in conjunction with the physical modelling. Extensive experiments demonstrate that our method attains state-of-the-art performance on both synthetic and real-world hazy datasets.
</details></li>
</ul>
<hr>
<h2 id="LongDanceDiff-Long-term-Dance-Generation-with-Conditional-Diffusion-Model"><a href="#LongDanceDiff-Long-term-Dance-Generation-with-Conditional-Diffusion-Model" class="headerlink" title="LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model"></a>LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11945">http://arxiv.org/abs/2308.11945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siqi Yang, Zejun Yang, Zhisheng Wang</li>
<li>for: 这个研究旨在解决长期三维真实舞蹈生成中的静止问题，以提高舞蹈生成的可调和自然性。</li>
<li>methods: 我们运用了一个条件扩散模型，长舞蹈扩散（LongDanceDiff），并将输入组合了音乐、过去的动作和随机化的未来动作。我们还引入了一个共同信息最小化目标，以优化生成的舞蹈动作的多样性和自然性。</li>
<li>results: 我们的方法与现有的方法相比，实现了重大的改善，包括增加了舞蹈生成的可调和自然性。我们计划将我们的代码和模型发布给社区。<details>
<summary>Abstract</summary>
Dancing with music is always an essential human art form to express emotion. Due to the high temporal-spacial complexity, long-term 3D realist dance generation synchronized with music is challenging. Existing methods suffer from the freezing problem when generating long-term dances due to error accumulation and training-inference discrepancy. To address this, we design a conditional diffusion model, LongDanceDiff, for this sequence-to-sequence long-term dance generation, addressing the challenges of temporal coherency and spatial constraint. LongDanceDiff contains a transformer-based diffusion model, where the input is a concatenation of music, past motions, and noised future motions. This partial noising strategy leverages the full-attention mechanism and learns the dependencies among music and past motions. To enhance the diversity of generated dance motions and mitigate the freezing problem, we introduce a mutual information minimization objective that regularizes the dependency between past and future motions. We also address common visual quality issues in dance generation, such as foot sliding and unsmooth motion, by incorporating spatial constraints through a Global-Trajectory Modulation (GTM) layer and motion perceptual losses, thereby improving the smoothness and naturalness of motion generation. Extensive experiments demonstrate a significant improvement in our approach over the existing state-of-the-art methods. We plan to release our codes and models soon.
</details>
<details>
<summary>摘要</summary>
人类常用舞蹈作为表达情感的重要艺术形式。由于高度时空复杂性，长期3D真实舞蹈生成同音乐同步是一项挑战。现有方法受到预测-实际差异和错误积累的问题。为解决这问题，我们设计了一种 conditional diffusion 模型，长 dance diff（LongDanceDiff），用于这种序列到序列长期舞蹈生成任务，解决时间准确性和空间约束的挑战。LongDanceDiff 包括一个基于 transformer 的扩散模型，输入是音乐、过去动作和噪音未来动作的 concatenation。这种 partial noising 策略利用了全程注意机制，学习音乐和过去动作之间的依赖关系。为提高生成舞蹈动作的多样性和减少冻结问题，我们引入了一个 mutual information minimization 目标，规范过去和未来动作之间的依赖关系。我们还通过 incorporating 全球轨迹修饰（GTM）层和运动观察损失，提高生成动作的平滑性和自然性。广泛的实验表明我们的方法在现有状态的方法上显著提高了性能。我们计划 soon 发布我们的代码和模型。
</details></li>
</ul>
<hr>
<h2 id="RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching"><a href="#RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching" class="headerlink" title="RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching"></a>RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11943">http://arxiv.org/abs/2308.11943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steve Vott, Adam M. Lehavi</li>
<li>for: 本 paper 探讨了使用最佳先搜索算法和强化学习（RL）技术来找到特定 Ramsey 数字的反例。</li>
<li>methods: 本 paper 使用了图vectorization和深度神经网络（DNN）基于的优化和搜索算法，以评估图是否为反例。</li>
<li>results: 本 paper 提出了一种搜索框架，可以支持 Ramsey 反例探索使用其他heelures。<details>
<summary>Abstract</summary>
The Ramsey number is the minimum number of nodes, $n = R(s, t)$, such that all undirected simple graphs of order $n$, contain a clique of order $s$, or an independent set of order $t$. This paper explores the application of a best first search algorithm and reinforcement learning (RL) techniques to find counterexamples to specific Ramsey numbers. We incrementally improve over prior search methods such as random search by introducing a graph vectorization and deep neural network (DNN)-based heuristic, which gauge the likelihood of a graph being a counterexample. The paper also proposes algorithmic optimizations to confine a polynomial search runtime. This paper does not aim to present new counterexamples but rather introduces and evaluates a framework supporting Ramsey counterexample exploration using other heuristics. Code and methods are made available through a PyPI package and GitHub repository.
</details>
<details>
<summary>摘要</summary>
“拉姆齐数”是最小的节点数量，$n = R(s, t)$, 使得所有无向简单图的顺序为$n$，必然包含一个 clique 的顺序为$s$，或一个独立集的顺序为$t$。这篇论文探索使用最佳先搜索算法和强化学习（RL）技术来找到特定拉姆齐数的反例。我们通过引入图像化和深度神经网络（DNN）基于的优化来提高先前的搜索方法，如随机搜索。 paper 还提出了算法优化，以确保搜索时间 polynomial。这篇论文不是想要发现新的反例，而是介绍和评估一个支持拉姆齐反例探索的框架，使用其他规则。代码和方法通过 PyPI 包和 GitHub 存储库提供。
</details></li>
</ul>
<hr>
<h2 id="Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series"><a href="#Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series" class="headerlink" title="Retail Demand Forecasting: A Comparative Study for Multivariate Time Series"></a>Retail Demand Forecasting: A Comparative Study for Multivariate Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11939">http://arxiv.org/abs/2308.11939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sabbirul Haque, Md Shahedul Amin, Jonayet Miah</li>
<li>for: 预测零售需求的精度是零售业的金融性和供应链效率的关键因素。在全球市场变得越来越连接起来，企业们正在寻找更高级别的预测模型，以获得竞争优势。</li>
<li>methods: 本研究使用时间系列数据中的顾客需求和macro经济变量（如Consumer Price Index（CPI）、Index of Consumer Sentiment（ICS）和失业率）进行拓展。我们采用了不同的回归和机器学习模型，以准确预测零售需求。</li>
<li>results: 我们的研究发现，通过拓展时间系列数据中的顾客需求和macro经济变量，可以提高预测零售需求的准确度。不同的回归和机器学习模型在预测零售需求方面具有不同的表现，但是综合评价下，机器学习模型的表现较好。<details>
<summary>Abstract</summary>
Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.
</details>
<details>
<summary>摘要</summary>
Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.Here's the translation in Traditional Chinese:精准的预测是商业领域中的一个关键因素，对于财务性能和供应链效率都是决定性的。随着全球市场变得越来越联系，企业们正在转向更进步的预测模型，以获得竞争优势。然而，现有的文献主要集中在历史销售数据上，忽略了消费者支出行为中的重要影响因素。在这项研究中，我们将把历史销售数据丰富化，加入 macroeconomic 变量，例如消费者物价指数 (CPI)、消费者信心指数 (ICS) 和失业率。利用这个完整的数据集，我们将开发和比较不同的回归和机器学习模型，以精准预测零售需求。
</details></li>
</ul>
<hr>
<h2 id="Learning-Bottleneck-Transformer-for-Event-Image-Voxel-Feature-Fusion-based-Classification"><a href="#Learning-Bottleneck-Transformer-for-Event-Image-Voxel-Feature-Fusion-based-Classification" class="headerlink" title="Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification"></a>Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11937">http://arxiv.org/abs/2308.11937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/event-ahu/efv_event_classification">https://github.com/event-ahu/efv_event_classification</a></li>
<li>paper_authors: Chengguo Yuan, Yu Jin, Zongzhen Wu, Fanting Wei, Yangzirui Wang, Lan Chen, Xiao Wang</li>
<li>for: 本文提出了一种新的双流框架，用于事件表示、提取和融合，以解决现有方法的缺点，包括单一模式表达和网络结构设计。</li>
<li>methods: 本文使用了Transformer和结构化图 neural network（GNN）架构，同时学习事件图像和事件立方体信息。在这个框架中，用瓶颈Transformer来实现双流信息融合。</li>
<li>results: 经过广泛的实验表明，我们的提议的框架可以在两个常用的事件基本分类数据集上达到最新的性能水平。代码可以在：\url{<a target="_blank" rel="noopener" href="https://github.com/Event-AHU/EFV_event_classification%7D">https://github.com/Event-AHU/EFV_event_classification}</a> 中找到。<details>
<summary>Abstract</summary>
Recognizing target objects using an event-based camera draws more and more attention in recent years. Existing works usually represent the event streams into point-cloud, voxel, image, etc, and learn the feature representations using various deep neural networks. Their final results may be limited by the following factors: monotonous modal expressions and the design of the network structure. To address the aforementioned challenges, this paper proposes a novel dual-stream framework for event representation, extraction, and fusion. This framework simultaneously models two common representations: event images and event voxels. By utilizing Transformer and Structured Graph Neural Network (GNN) architectures, spatial information and three-dimensional stereo information can be learned separately. Additionally, a bottleneck Transformer is introduced to facilitate the fusion of the dual-stream information. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance on two widely used event-based classification datasets. The source code of this work is available at: \url{https://github.com/Event-AHU/EFV_event_classification}
</details>
<details>
<summary>摘要</summary>
recognizing target objects using event-based cameras has attracted increasing attention in recent years. existing works usually convert event streams into point clouds, voxels, images, etc., and learn feature representations using various deep neural networks. however, their final results may be limited by the following factors: monotonous modal expressions and the design of the network structure. to address these challenges, this paper proposes a novel dual-stream framework for event representation, extraction, and fusion. this framework simultaneously models two common representations: event images and event voxels. by utilizing transformer and structured graph neural network (gnn) architectures, spatial information and three-dimensional stereo information can be learned separately. additionally, a bottleneck transformer is introduced to facilitate the fusion of the dual-stream information. extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance on two widely used event-based classification datasets. the source code of this work is available at: \url{https://github.com/Event-AHU/EFV_event_classification}Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe"><a href="#Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe" class="headerlink" title="Diverse Policies Converge in Reward-free Markov Decision Processe"></a>Diverse Policies Converge in Reward-free Markov Decision Processe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11924">http://arxiv.org/abs/2308.11924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openrl-lab/diversepolicies">https://github.com/openrl-lab/diversepolicies</a></li>
<li>paper_authors: Fanqi Lin, Shiyu Huang, Weiwei Tu</li>
<li>for: 本文旨在提供一个统一的多种策略学习框架，并调查多种策略学习算法的训练是如何 converges 和效率如何。</li>
<li>methods: 本文提出了一种可证明高效的多种策略学习算法，并通过数学实验证明了其效果。</li>
<li>results: 经过数学实验，本文发现了多种策略学习算法的训练可以高效地 converge 到优化策略，并且可以提高策略的多样性和鲁棒性。<details>
<summary>Abstract</summary>
Reinforcement learning has achieved great success in many decision-making tasks, and traditional reinforcement learning algorithms are mainly designed for obtaining a single optimal solution. However, recent works show the importance of developing diverse policies, which makes it an emerging research topic. Despite the variety of diversity reinforcement learning algorithms that have emerged, none of them theoretically answer the question of how the algorithm converges and how efficient the algorithm is. In this paper, we provide a unified diversity reinforcement learning framework and investigate the convergence of training diverse policies. Under such a framework, we also propose a provably efficient diversity reinforcement learning algorithm. Finally, we verify the effectiveness of our method through numerical experiments.
</details>
<details>
<summary>摘要</summary>
“强化学习在很多决策任务中取得了很大成功，但传统的强化学习算法主要是为了获得单一的优化解决方案。然而，latest works表明了多种策略的重要性，使得这成为一个emerging研究话题。虽然多种多样性强化学习算法已经出现，但没有任何一个能回答强化学习算法如何 converges和效率如何。在这篇论文中，我们提出了一个统一的多样性强化学习框架，并investigate了训练多种策略的聚合。根据这种框架，我们还提出了可证明有效的多样性强化学习算法。最后，我们通过数值实验验证了我们的方法的有效性。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Concept-Bottleneck-with-Visual-Concept-Filtering-for-Explainable-Medical-Image-Classification"><a href="#Concept-Bottleneck-with-Visual-Concept-Filtering-for-Explainable-Medical-Image-Classification" class="headerlink" title="Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification"></a>Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11920">http://arxiv.org/abs/2308.11920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Injae Kim, Jongha Kim, Joonmyung Choi, Hyunwoo J. Kim</li>
<li>for: 提高医疗应用中模型可靠性的一个关键因素是可读性。概念瓶颈模型（CBM）可以使用人类理解的概念作为中间目标进行可读性图像分类。</li>
<li>methods: 在使用大型自然语言模型（LLM）生成概念的现有方法中，不考虑概念是否具有视觉特征，这是计算有意义的概念分数的重要因素。因此，我们提议使用视觉活动分数来衡量概念是否含有视觉cue，可以使用无标注图像数据来计算。</li>
<li>results: 我们的实验结果表明，采用我们提议的视觉活动分数来筛选概念可以consistently提高性能，相比基线。此外，qualitative analyses还证明了视觉相关概念被选择。<details>
<summary>Abstract</summary>
Interpretability is a crucial factor in building reliable models for various medical applications. Concept Bottleneck Models (CBMs) enable interpretable image classification by utilizing human-understandable concepts as intermediate targets. Unlike conventional methods that require extensive human labor to construct the concept set, recent works leveraging Large Language Models (LLMs) for generating concepts made automatic concept generation possible. However, those methods do not consider whether a concept is visually relevant or not, which is an important factor in computing meaningful concept scores. Therefore, we propose a visual activation score that measures whether the concept contains visual cues or not, which can be easily computed with unlabeled image data. Computed visual activation scores are then used to filter out the less visible concepts, thus resulting in a final concept set with visually meaningful concepts. Our experimental results show that adopting the proposed visual activation score for concept filtering consistently boosts performance compared to the baseline. Moreover, qualitative analyses also validate that visually relevant concepts are successfully selected with the visual activation score.
</details>
<details>
<summary>摘要</summary>
“可读性”是医疗应用中建立可靠模型的重要因素。概念瓶颈模型（CBM）可以实现可读性检查，通过使用人类可理解的概念作为中间目标。与传统方法不同的是，这些方法不需要大量的人工劳动来建立概念集。最近的工作则是利用大型自然语言模型（LLM）生成概念，并使用这些概念来生成可读性检查。但是，这些方法并不考虑概念是否具有视觉相关性，这是 Computing meaningful concept scores 中的重要因素。因此，我们提出了视觉活动 scores，它可以衡量概念是否包含视觉提示，并且可以轻松地使用无标注图像资料来计算。我们的实验结果显示，运用我们提出的视觉活动 scores 进行概念筛选可以与基准相比，实现更高的性能。此外，实验分析也显示，这些视觉相关的概念被成功选择。
</details></li>
</ul>
<hr>
<h2 id="LFS-GAN-Lifelong-Few-Shot-Image-Generation"><a href="#LFS-GAN-Lifelong-Few-Shot-Image-Generation" class="headerlink" title="LFS-GAN: Lifelong Few-Shot Image Generation"></a>LFS-GAN: Lifelong Few-Shot Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11917">http://arxiv.org/abs/2308.11917</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jjuon/lfs-gan">https://github.com/jjuon/lfs-gan</a></li>
<li>paper_authors: Juwon Seo, Ji-Su Kang, Gyeong-Moon Park</li>
<li>For: The paper addresses the challenging task of lifelong few-shot image generation, where a generative model learns a sequence of tasks using only a few samples per task, and prevents catastrophic forgetting and overfitting.* Methods: The proposed framework, called Lifelong Few-Shot GAN (LFS-GAN), uses an efficient task-specific modulator called Learnable Factorized Tensor (LeFT) to learn each task, and a novel mode seeking loss to improve diversity in low-data circumstances.* Results: The proposed LFS-GAN can generate high-quality and diverse images in various domains without any forgetting and mode collapse, achieving state-of-the-art in lifelong few-shot image generation task, and even outperforming existing few-shot GANs in the few-shot image generation task.Here is the simplified Chinese text for the three key points:* For: 这篇论文首次解决了难度较高的生命周期几个shot图像生成任务，其中一个生成模型需要使用只有几个样本来学习每个任务。* Methods: 该提议的框架called Lifelong Few-Shot GAN (LFS-GAN)使用高效的任务特定修饰器called Learnable Factorized Tensor (LeFT)来学习每个任务，并使用一种新的模式寻找损失来提高模型在低数据情况下的多样性。* Results: 提议的LFS-GAN可以在不同领域中生成高质量和多样的图像，无论是在几个shot图像生成任务中还是在生命周期中，并且可以超越现有的几个shot GANs在几个shot图像生成任务中的性能。<details>
<summary>Abstract</summary>
We address a challenging lifelong few-shot image generation task for the first time. In this situation, a generative model learns a sequence of tasks using only a few samples per task. Consequently, the learned model encounters both catastrophic forgetting and overfitting problems at a time. Existing studies on lifelong GANs have proposed modulation-based methods to prevent catastrophic forgetting. However, they require considerable additional parameters and cannot generate high-fidelity and diverse images from limited data. On the other hand, the existing few-shot GANs suffer from severe catastrophic forgetting when learning multiple tasks. To alleviate these issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can generate high-quality and diverse images in lifelong few-shot image generation task. Our proposed framework learns each task using an efficient task-specific modulator - Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and has a rich representation ability due to its unique reconstruction technique. Furthermore, we propose a novel mode seeking loss to improve the diversity of our model in low-data circumstances. Extensive experiments demonstrate that the proposed LFS-GAN can generate high-fidelity and diverse images without any forgetting and mode collapse in various domains, achieving state-of-the-art in lifelong few-shot image generation task. Surprisingly, we find that our LFS-GAN even outperforms the existing few-shot GANs in the few-shot image generation task. The code is available at Github.
</details>
<details>
<summary>摘要</summary>
我们首次Addressing a challenging lifelong few-shot image generation task. In this situation, a generative model learns a sequence of tasks using only a few samples per task. As a result, the learned model encounters both catastrophic forgetting and overfitting problems at the same time. Existing studies on lifelong GANs have proposed modulation-based methods to prevent catastrophic forgetting, but these methods require additional parameters and cannot generate high-fidelity and diverse images from limited data. On the other hand, existing few-shot GANs suffer from severe catastrophic forgetting when learning multiple tasks. To address these issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can generate high-quality and diverse images in the lifelong few-shot image generation task. Our proposed framework learns each task using an efficient task-specific modulator called Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and has a rich representation ability due to its unique reconstruction technique. Furthermore, we propose a novel mode seeking loss to improve the diversity of our model in low-data circumstances. Extensive experiments show that the proposed LFS-GAN can generate high-fidelity and diverse images without any forgetting and mode collapse in various domains, achieving state-of-the-art in the lifelong few-shot image generation task. Surprisingly, we find that our LFS-GAN even outperforms the existing few-shot GANs in the few-shot image generation task. The code is available on Github.Here's the translation in Traditional Chinese:我们首次Addressing a challenging lifelong few-shot image generation task. 在这个情况下, a generative model learns a sequence of tasks using only a few samples per task. 因此, the learned model encounters both catastrophic forgetting and overfitting problems at the same time.  existing studies on lifelong GANs have proposed modulation-based methods to prevent catastrophic forgetting, but these methods require additional parameters and cannot generate high-fidelity and diverse images from limited data. On the other hand, existing few-shot GANs suffer from severe catastrophic forgetting when learning multiple tasks. To address these issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can generate high-quality and diverse images in the lifelong few-shot image generation task. Our proposed framework learns each task using an efficient task-specific modulator called Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and has a rich representation ability due to its unique reconstruction technique. Furthermore, we propose a novel mode seeking loss to improve the diversity of our model in low-data circumstances. Extensive experiments show that the proposed LFS-GAN can generate high-fidelity and diverse images without any forgetting and mode collapse in various domains, achieving state-of-the-art in the lifelong few-shot image generation task. Surprisingly, we find that our LFS-GAN even outperforms the existing few-shot GANs in the few-shot image generation task. The code is available on Github.
</details></li>
</ul>
<hr>
<h2 id="Towards-CausalGPT-A-Multi-Agent-Approach-for-Faithful-Knowledge-Reasoning-via-Promoting-Causal-Consistency-in-LLMs"><a href="#Towards-CausalGPT-A-Multi-Agent-Approach-for-Faithful-Knowledge-Reasoning-via-Promoting-Causal-Consistency-in-LLMs" class="headerlink" title="Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs"></a>Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11914">http://arxiv.org/abs/2308.11914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Tang, Ruilin Wang, Weixing Chen, Keze Wang, Yang Liu, Tianshui Chen, Liang Lin</li>
<li>for: 提高知识基于reasoning的 faithfulness和 causality</li>
<li>methods: 多智能体协作 reasoning-and-consensus 框架</li>
<li>results: 在多种知识reasoning任务（如科学问答和常识reasoning）中，我们的框架比所有比较方法都高得多<details>
<summary>Abstract</summary>
Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference. Existing methods primarily encourage LLMs to autonomously plan and solve problems or to extensively sample reasoning chains without addressing the conceptual and inferential fallacies. Attempting to alleviate inferential fallacies and drawing inspiration from multi-agent collaboration, we present a framework to increase faithfulness and causality for knowledge-based reasoning. Specifically, we propose to employ multiple intelligent agents (i.e., reasoner and causal evaluator) to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness. The reasoners focus on providing solutions with human-like causality to solve open-domain problems. On the other hand, the causal evaluator agent scrutinizes if the answer in a solution is causally deducible from the question and vice versa, with a counterfactual answer replacing the original. According to the extensive and comprehensive evaluations on a variety of knowledge reasoning tasks (e.g., science question answering and commonsense reasoning), our framework outperforms all compared state-of-the-art approaches by large margins.
</details>
<details>
<summary>摘要</summary>
尽管LLM技术得到了进步，知识基于的理解仍然是一个长期的问题，因为知识回忆和推理的 fragility。现有方法主要是让LLM自动规划和解决问题，或者广泛采样推理链而未能解决概念和推理错误。借鉴多智能代理（i.e., 理解者和 causal评估器）的合作，我们提出了增强知识基于的理解 faithfulness 的框架。 Specifically, 我们提议使用多个智能代理（i.e., 理解者和 causal评估器）在一种理解和共识 paradigm中合作，以提高理解的准确性。理解者专注于提供人类化的 causality 来解决开放领域问题，而 causal评估器代理则检查问题和答案之间的 causal 关系是否正确，并将对应的 counterfactual 答案替换原答案。根据对多种知识理解任务（如科学问答和常识理解）的广泛和全面评估，我们的框架在比较的state-of-the-art方法之上出现大幅提升。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Admissible-Bounds-for-Heuristic-Learning"><a href="#Utilizing-Admissible-Bounds-for-Heuristic-Learning" class="headerlink" title="Utilizing Admissible Bounds for Heuristic Learning"></a>Utilizing Admissible Bounds for Heuristic Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11905">http://arxiv.org/abs/2308.11905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Núñez-Molina, Masataro Asai</li>
<li>for: 本研究的目的是提高前向搜索算法中的modern机器学习技术应用，并提供更好的理论基础 для这种应用。</li>
<li>methods: 本研究使用的方法包括使用Truncated Gaussian distribution作为参数，以及在训练过程中考虑扩展的admissible heuristics。</li>
<li>results: 研究发现，使用admissible heuristics作为参数，Truncated Gaussian distribution可以更好地适应实际问题，并且在训练过程中更快 converges。<details>
<summary>Abstract</summary>
While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and empirically show that, as a result, it yields more accurate heuristics and converges faster during training.
</details>
<details>
<summary>摘要</summary>
Recently, there has been growing interest in using modern machine learning techniques to learn heuristic functions for forward search algorithms. However, there has been little theoretical understanding of what these functions should learn, how to train them, and why we do so. This lack of understanding has led to various literature selecting datasets and optimization metrics on an ad-hoc basis, and little attention has been paid to the role of admissibility during learning.This paper focuses on the role of admissible heuristics in supervised heuristic learning, using Truncated Gaussian distributions as parameters. This approach tightens the hypothesis space compared to ordinary Gaussian distributions, and faithfully follows the principle of maximum entropy. Empirical results show that this approach yields more accurate heuristics and converges faster during training.
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Optimization-Objective-of-One-Class-Classification-for-Anomaly-Detection"><a href="#Exploring-the-Optimization-Objective-of-One-Class-Classification-for-Anomaly-Detection" class="headerlink" title="Exploring the Optimization Objective of One-Class Classification for Anomaly Detection"></a>Exploring the Optimization Objective of One-Class Classification for Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11898">http://arxiv.org/abs/2308.11898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Gao, Huiyuan Luo, Fei Shen, Zhengtao Zhang<br>for:This paper focuses on the optimization objective space within one-class classification (OCC) methods and its impact on performance.methods:The paper proposes a novel, data-agnostic deep one-class classification method that utilizes a single 1x1 convolutional layer as a trainable projector and any space with a suitable norm as the optimization objective.results:The proposed method achieves state-of-the-art performance in both one-class classification and industrial vision anomaly detection and segmentation tasks, validating the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
One-class classification (OCC) is a longstanding method for anomaly detection. With the powerful representation capability of the pre-trained backbone, OCC methods have witnessed significant performance improvements. Typically, most of these OCC methods employ transfer learning to enhance the discriminative nature of the pre-trained backbone's features, thus achieving remarkable efficacy. While most current approaches emphasize feature transfer strategies, we argue that the optimization objective space within OCC methods could also be an underlying critical factor influencing performance. In this work, we conducted a thorough investigation into the optimization objective of OCC. Through rigorous theoretical analysis and derivation, we unveil a key insights: any space with the suitable norm can serve as an equivalent substitute for the hypersphere center, without relying on the distribution assumption of training samples. Further, we provide guidelines for determining the feasible domain of norms for the OCC optimization objective. This novel insight sparks a simple and data-agnostic deep one-class classification method. Our method is straightforward, with a single 1x1 convolutional layer as a trainable projector and any space with suitable norm as the optimization objective. Extensive experiments validate the reliability and efficacy of our findings and the corresponding methodology, resulting in state-of-the-art performance in both one-class classification and industrial vision anomaly detection and segmentation tasks.
</details>
<details>
<summary>摘要</summary>
一类分类（OCC）是一种长期使用的异常检测方法。随着预训练后处理的特征表示能力的提高，OCC方法已经经历了显著性能提高。通常，大多数这些OCC方法使用传输学来强化预训练后处理的特征，从而实现了很好的效果。而我们认为，OCC方法的优化目标空间也是影响性能的关键因素。在这项工作中，我们进行了一项全面的OCC优化目标对象的调查。通过严格的理论分析和逻辑推导，我们揭示出一个关键发现：任何具有适当 нор 的空间都可以作为异常中心的等价substitute，不需要基于训练样本的分布假设。此外，我们还提供了确定OCC优化目标空间的可行范围的指南。这一新发现引出了一种简单、数据非依的深度一类分类方法。我们的方法包括一个单一的1x1卷积层作为可训练的投影器，以及任何具有适当 norm的空间作为优化目标。我们的实验证明了我们的发现和相应的方法ология的可靠性和效果，在一类分类和工业视觉异常检测和分割任务中实现了状态的末点性能。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Deciphering-Tabular-Data-Using-Large-Language-Model"><a href="#Bridging-the-Gap-Deciphering-Tabular-Data-Using-Large-Language-Model" class="headerlink" title="Bridging the Gap: Deciphering Tabular Data Using Large Language Model"></a>Bridging the Gap: Deciphering Tabular Data Using Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11891">http://arxiv.org/abs/2308.11891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengyuan Zhang, Peng Chang, Zongcheng Ji</li>
<li>for: 本研究旨在探讨大语言模型如何用于表格问答 tasks，以提高表格结构和内容的理解。</li>
<li>methods: 我们提出了一种特有的模块，用于将表格 serialized 到可以与大语言模型集成的格式。此外，我们还实施了一种纠正机制，以检查和修正模型的可能错误。</li>
<li>results: 我们的提议方法在总体指标中落后 SOTA 约 11.7%，但在特定数据集上测试时，超过 SOTA 约 1.2%。这些结果表明我们的方法可以增强大语言模型对表格结构和内容的理解。<details>
<summary>Abstract</summary>
In the realm of natural language processing, the understanding of tabular data has perpetually stood as a focal point of scholarly inquiry. The emergence of expansive language models, exemplified by the likes of ChatGPT, has ushered in a wave of endeavors wherein researchers aim to harness these models for tasks related to table-based question answering. Central to our investigative pursuits is the elucidation of methodologies that amplify the aptitude of such large language models in discerning both the structural intricacies and inherent content of tables, ultimately facilitating their capacity to provide informed responses to pertinent queries. To this end, we have architected a distinctive module dedicated to the serialization of tables for seamless integration with expansive language models. Additionally, we've instituted a corrective mechanism within the model to rectify potential inaccuracies. Experimental results indicate that, although our proposed method trails the SOTA by approximately 11.7% in overall metrics, it surpasses the SOTA by about 1.2% in tests on specific datasets. This research marks the first application of large language models to table-based question answering tasks, enhancing the model's comprehension of both table structures and content.
</details>
<details>
<summary>摘要</summary>
在自然语言处理领域中，表格数据的理解一直是学术研究的焦点。现在，大型语言模型的出现，如ChatGPT，使得研究人员尝试使用这些模型来解决表格问题。我们的探索的核心在于发展一种能够增强大型语言模型对表格结构和内容的理解，以便它们能够准确回答相关的问题。为此，我们设计了一个专门用于将表格序列化的模块，并在模型中实施了纠正机制以消除可能的错误。实验结果表明，虽然我们的提议方法相对于最佳实践（SOTA）落后约11.7%的总指标，但在特定数据集上测试时超过了SOTA约1.2%。这项研究是大型语言模型在表格问题回答上的首次应用，提高了模型对表格结构和内容的理解。
</details></li>
</ul>
<hr>
<h2 id="Integrating-the-Wikidata-Taxonomy-into-YAGO"><a href="#Integrating-the-Wikidata-Taxonomy-into-YAGO" class="headerlink" title="Integrating the Wikidata Taxonomy into YAGO"></a>Integrating the Wikidata Taxonomy into YAGO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11884">http://arxiv.org/abs/2308.11884</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yago-naga/yago-4.5">https://github.com/yago-naga/yago-4.5</a></li>
<li>paper_authors: Fabian Suchanek, Mehwish Alam, Thomas Bonald, Pierre-Henri Paris, Jules Soria</li>
<li>for: The paper aims to merge the entire Wikidata taxonomy into the YAGO KB as much as possible, while maintaining logical consistency.</li>
<li>methods: The authors combine Wikidata with the ontology from Schema.org to reduce and clean up the taxonomy, and use automated reasoners to ensure logical consistency.</li>
<li>results: The authors create YAGO 4.5, which adds a rich layer of informative classes to YAGO while keeping the KB logically consistent.<details>
<summary>Abstract</summary>
Wikidata is one of the largest public general-purpose Knowledge Bases (KBs). Yet, due to its collaborative nature, its schema and taxonomy have become convoluted. For the YAGO 4 KB, we combined Wikidata with the ontology from Schema.org, which reduced and cleaned up the taxonomy and constraints and made it possible to run automated reasoners on the data. However, it also cut away large parts of the Wikidata taxonomy. In this paper, we present our effort to merge the entire Wikidata taxonomy into the YAGO KB as much as possible. We pay particular attention to logical constraints and a careful distinction of classes and instances. Our work creates YAGO 4.5, which adds a rich layer of informative classes to YAGO, while at the same time keeping the KB logically consistent.
</details>
<details>
<summary>摘要</summary>
wikidata是一个非常大的公共通用知识库（kb）。然而由于其协作性，其架构和分类已经变得混乱。为了构建yaogo4kb，我们将wikidata与schema.org的ontology结合了起来，这有效地减少了和约束，并使得数据可以通过自动推理。但是，这也剪辑了大量wikidata分类。在这篇论文中，我们报告了我们将wikidata分类完全 merged into yaogoKB中的努力。我们特别注重逻辑约束和精心分类和实例的区分。我们的工作创造了yaogo4.5，它添加了一层有用的类到yaogoKB中，同时保持kb的逻辑一致。
</details></li>
</ul>
<hr>
<h2 id="Cabrita-closing-the-gap-for-foreign-languages"><a href="#Cabrita-closing-the-gap-for-foreign-languages" class="headerlink" title="Cabrita: closing the gap for foreign languages"></a>Cabrita: closing the gap for foreign languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11878">http://arxiv.org/abs/2308.11878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celio Larcher, Marcos Piau, Paulo Finardi, Pedro Gengo, Piero Esposito, Vinicius Caridá</li>
<li>for: 提高特定语言或领域上表现，以及有效地进行分词。</li>
<li>methods: 使用自scratch训练模型，并开发了一种名为Cabrita的方法ологи。</li>
<li>results: 在评估少量学习任务中，与传统连续预训练方法和7B英语预训练模型的结果相似，并且减少了分词的数量。<details>
<summary>Abstract</summary>
The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.   The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.   To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We believe that this methodology can be applied to any transformer-like architecture model. To validate the study, we conducted continuous pre-training exclusively using Portuguese text on a 3-billion-parameter model known as OpenLLaMA, resulting in a model named openCabrita 3B. The openCabrita 3B also features a new tokenizer that results in a significant reduction in the number of tokens required to represent the text. In our assessment, for few-shot learning tasks, we achieved similar results with this 3B model compared to a traditional continuous pre-training approach as well as to 7B models English pre-trained models.
</details>
<details>
<summary>摘要</summary>
strategy 训练模型从零开始在特定语言或领域上服务两个重要目的：一是提高特定语言或领域上表现，二是确保有效的分词。但这种方法存在一个主要的限制，即成本，可以达到6到7位数字的值，具体取决于模型大小和参数数量。为了缓解这个问题，可以依靠可用的预训练模型，尽管最近的进步，如LLaMA和LLaMA-2模型，仍然在特定领域问题上不具有效果，因为需要大量的字符表示文本。为了解决这个问题，我们提出了一种方法ологи，名为Cabrita，我们的研究表明，该方法能够成功地解决表现和有效的分词问题，并且具有可Affordable的成本。我们认为该方法可以应用于任何 transformer-like 架构模型。为了验证这种方法，我们进行了继续预训练，专门使用葡萄牙语文本，在一个3亿参数的模型OpenLLaMA上进行了不断预训练，从而获得了一个名为openCabrita 3B的模型。openCabrita 3B还使用了一种新的分词器，从而导致文本表示的字符数量减少了许多。在我们的评估中，对于几个shot学习任务，我们使用这个3B模型和传统预训练方法以及7B英语预训练模型进行比较，得到了类似的结果。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Image-and-Location-Analysis-for-Wound-Classification-A-Deep-Learning-Approach"><a href="#Integrated-Image-and-Location-Analysis-for-Wound-Classification-A-Deep-Learning-Approach" class="headerlink" title="Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach"></a>Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11877">http://arxiv.org/abs/2308.11877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Patel, Tirth Shah, Mrinal Kanti Dhar, Taiyu Zhang, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Zeyun Yu</li>
<li>for: 提高伤口分类精度，以便更好地诊断和治疗伤口。</li>
<li>methods: 基于深度卷积神经网络的多Modal网络，使用伤口图像和其相应的体部位置进行更加精确的分类。</li>
<li>results: 比 tradicional方法高，达到了74.79%到100%的ROI（区域 интерес）无位置分类精度，73.98%到100%的ROIwith位置分类精度，和78.10%到100%的全图分类精度。<details>
<summary>Abstract</summary>
The global burden of acute and chronic wounds presents a compelling case for enhancing wound classification methods, a vital step in diagnosing and determining optimal treatments. Recognizing this need, we introduce an innovative multi-modal network based on a deep convolutional neural network for categorizing wounds into four categories: diabetic, pressure, surgical, and venous ulcers. Our multi-modal network uses wound images and their corresponding body locations for more precise classification. A unique aspect of our methodology is incorporating a body map system that facilitates accurate wound location tagging, improving upon traditional wound image classification techniques. A distinctive feature of our approach is the integration of models such as VGG16, ResNet152, and EfficientNet within a novel architecture. This architecture includes elements like spatial and channel-wise Squeeze-and-Excitation modules, Axial Attention, and an Adaptive Gated Multi-Layer Perceptron, providing a robust foundation for classification. Our multi-modal network was trained and evaluated on two distinct datasets comprising relevant images and corresponding location information. Notably, our proposed network outperformed traditional methods, reaching an accuracy range of 74.79% to 100% for Region of Interest (ROI) without location classifications, 73.98% to 100% for ROI with location classifications, and 78.10% to 100% for whole image classifications. This marks a significant enhancement over previously reported performance metrics in the literature. Our results indicate the potential of our multi-modal network as an effective decision-support tool for wound image classification, paving the way for its application in various clinical contexts.
</details>
<details>
<summary>摘要</summary>
全球各类伤口的扩大问题，提出了加强伤口分类方法的需求，这是诊断和治疗伤口的重要一步。为此，我们介绍了一种创新的多模态网络，基于深度卷积神经网络，用于分类伤口为四类：糖尿病、压力、手术和血液溢出损伤。我们的多模态网络使用伤口图像和其相应的身体位置信息进行更加精确的分类。我们的方法的一个独特特点是通过身体地图系统，实现了更加准确的伤口位置标记，从传统伤口图像分类技术中的改进。我们的方法还integrates了多种模型，如VGG16、ResNet152和EfficientNet，并在一种新的架构中进行组合。这种架构包括空间和通道方向的压缩和激活模块、轴向注意力和适应阀控多层感知机制，为分类提供了坚实的基础。我们的多模态网络在两个不同的数据集上进行训练和评估，其中一个包含了相关的图像和身体位置信息，另一个只包含图像。我们的方法在这两个数据集上达到了74.79%到100%的ROI（区域 интереса）无地址分类精度范围，73.98%到100%的ROI与地址分类精度范围，以及78.10%到100%的整个图像分类精度范围。这表明我们的多模态网络在文献中已经报道的性能指标中达到了显著的提高。我们的结果表明，我们的多模态网络可以作为伤口图像分类的有效决策支持工具，为各种临床上下文应用。
</details></li>
</ul>
<hr>
<h2 id="Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0"><a href="#Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0" class="headerlink" title="Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0"></a>Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11854">http://arxiv.org/abs/2308.11854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anmol Chaure, Ashok Kumar Behera, Sudip Bhattacharya</li>
<li>for: 本研究使用数据驱动机器学习模型作为气候模拟器，以便政策制定者能够做出有知识基础的决策。</li>
<li>methods: 本研究使用机器学习模型作为计算昂贵的GCM模拟器的代理，从而降低时间和碳脚印。特别是，使用核函数特性，回归模型可以捕捉复杂关系，提高预测能力。</li>
<li>results: 在使用 клима本chmark 数据集进行评估时，我们发现， amongst three non-linear regression models, Gaussian Process Regressor 表现最佳，在标准评估指标上对气候场的模拟表现出色。然而， Gaussian Process Regression 具有空间和时间复杂度的问题。相比之下， Support Vector 和 Kernel Ridge 模型也能够达到竞争力水平，但是有一定的交易offs。此外，我们正在 актив地调查 composite kernels 和变量抽象等技术，以提高回归模型的性能，更好地模拟复杂非线性 patrerns，包括降水现象。<details>
<summary>Abstract</summary>
Climate projections using data driven machine learning models acting as emulators, is one of the prevailing areas of research to enable policy makers make informed decisions. Use of machine learning emulators as surrogates for computationally heavy GCM simulators reduces time and carbon footprints. In this direction, ClimateBench [1] is a recently curated benchmarking dataset for evaluating the performance of machine learning emulators designed for climate data. Recent studies have reported that despite being considered fundamental, regression models offer several advantages pertaining to climate emulations. In particular, by leveraging the kernel trick, regression models can capture complex relationships and improve their predictive capabilities. This study focuses on evaluating non-linear regression models using the aforementioned dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the best-in-class performance against standard evaluation metrics used for climate field emulation studies. However, Gaussian Process Regression suffers from being computational resource hungry in terms of space and time complexity. Alternatively, Support Vector and Kernel Ridge models also deliver competitive results and but there are certain trade-offs to be addressed. Additionally, we are actively investigating the performance of composite kernels and techniques such as variational inference to further enhance the performance of the regression models and effectively model complex non-linear patterns, including phenomena like precipitation.
</details>
<details>
<summary>摘要</summary>
政策制定者可以通过使用数据驱动机器学模型作为模拟器，来做出了 informed 的决策。通过使用机器学模型作为计算成本高GCM模拟器的代理，可以降低时间和碳脚印。在这个方向下，ClimateBench [1] 是最近筹集的气候模拟数据集，用于评估机器学模型的性能。据研究，尽管被视为基本的，但是回归模型在气候模拟方面具有许多优势。具体来说，通过核心技术，回归模型可以捕捉复杂的关系，提高预测能力。本研究将对非线性回归模型进行评估，并比较其表现。Specifically，我们将 comparing the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the best-in-class performance against standard evaluation metrics used for climate field emulation studies. However, Gaussian Process Regression suffers from being computationally resource-intensive in terms of space and time complexity. Alternatively, Support Vector and Kernel Ridge models also deliver competitive results, but there are certain trade-offs to be addressed. Additionally, we are actively investigating the performance of composite kernels and techniques such as variational inference to further enhance the performance of the regression models and effectively model complex non-linear patterns, including phenomena like precipitation.
</details></li>
</ul>
<hr>
<h2 id="A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data"><a href="#A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data" class="headerlink" title="A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data"></a>A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11849">http://arxiv.org/abs/2308.11849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Liu, Zhiyuan Lin, Judith Y. T. Wang, Hong Chen</li>
<li>For: This paper aims to provide a demand-responsive approach for real-time railway rescheduling during severe emergency events such as natural disasters, with a focus on a heavy-demand station upstream of the disrupted area.* Methods: The paper proposes using mobile data (MD) to infer real-world passenger mobility and avoid overcrowding at the target station, and a deep reinforcement learning (DRL) framework to determine the optimal reschededuled timetable, route stops, and rolling stock allocation.* Results: The paper addresses challenges such as station overcrowding, rolling stock shortage, open-ended disruption duration, and delays due to detours, and aims to improve the efficiency and safety of real-time railway rescheduling during emergency events.<details>
<summary>Abstract</summary>
Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accumulation of overcrowded passengers at this station, to prevent additional accidents arising from overcrowding. This research addresses the challenges associated with this scenario, including the dynamics of arriving and leaving of passengers, station overcrowding, rolling stock shortage, open-ended disruption duration, integrated rescheduling on multiple routes, and delays due to detours. A deep reinforcement learning (DRL) framework is proposed to determine the optimal rescheduled timetable, route stops, and rolling stock allocation, while considering real-time demand satisfaction, station overcrowding, train capacity utilization, and headway safety.
</details>
<details>
<summary>摘要</summary>
现实时铁路重新规划是一种时间变化的和灵活的技术，可以自动修改运营计划应对时间变化的条件。现有研究缺乏基于实时旅客流动数据的数据驱动方法，而是主要依赖于 Origin-Destination（OD）数据和模型基本方法来估算列车需求。同时，长期干扰的调度原则忽略了旅客需求的不均分布。为了填补这一漏洞，本文提出了一种需求响应的方法，通过推理实际旅客流动数据（MD）来促进实时重新规划。与传统网络水平方法不同，本文将注重一个重要的快速车站，该站位于干扰区域之上游。目标是重新规划通过该站的多个路线的列车，这些列车受到严重紧急事件（如自然灾害）的影响。特别是要避免该站堵塞的乘客堆积，以避免由过度堆积而导致的一次性事故。本研究解决了这种情况中的挑战，包括到站拥堵、车厢短缺、开放式干扰持续时间、多路线集成重新规划和延迟。一种深度鼓励学习（DRL）框架被提议，以确定最佳重新规划时间表、站点停留、车厢分配，同时考虑实时需求满足、站点拥堵、车厢容量利用和距离安全。
</details></li>
</ul>
<hr>
<h2 id="rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning"></a>${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11842">http://arxiv.org/abs/2308.11842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dchen48/e3ac">https://github.com/dchen48/e3ac</a></li>
<li>paper_authors: Dingyang Chen, Qi Zhang</li>
<li>for: 这个论文旨在利用生物世界中的对称Pattern进行多智能体强化学习（MARL）问题的研究，以提高其在多种应用中的性能。</li>
<li>methods: 该论文使用了Euclidean symmetries作为多智能体强化学习问题的一种限制，并采用了具有对称约束的神经网络架构。</li>
<li>results: 该研究发现，通过对称约束的适应，神经网络架构在多种合作MARL benchmark中表现出色，并且具有很好的泛化能力，如零shot学习和转移学习。<details>
<summary>Abstract</summary>
Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns. The code is available at: https://github.com/dchen48/E3AC.
</details>
<details>
<summary>摘要</summary>
Identification和分析自然界中的对称征象导致了各科学领域的重要发现，如物理学中的重力法律的制定和化学结构的研究的进步。在这篇论文中，我们关注利用多智能体强化学习（MARL）问题中的欧几何对称的特性，这些特性在许多应用中很普遍。我们首先正式定义了一类马尔可夫游戏，其具有一般对称性，这使得存在对称优质值和策略。这些属性激发我们在多智能体actor-critic方法中嵌入对称约束，这种约束导致在各种合作MARLbenchmark中表现出色，并且具有很好的泛化能力，如零shot学习和转移学习在未看到的对称 patrern中。代码可以在以下链接中找到：https://github.com/dchen48/E3AC。
</details></li>
</ul>
<hr>
<h2 id="A-Benchmark-Study-on-Calibration"><a href="#A-Benchmark-Study-on-Calibration" class="headerlink" title="A Benchmark Study on Calibration"></a>A Benchmark Study on Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11838">http://arxiv.org/abs/2308.11838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/history1">https://github.com/Aryia-Behroziuan/history1</a></li>
<li>paper_authors: Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, Chang Xu</li>
<li>for: 这种研究的目的是为了探讨神经网络模型的准确性和稳定性之间的关系，以及如何在神经网络模型中提高准确性和稳定性的方法。</li>
<li>methods: 这种研究使用了Neural Architecture Search（NAS）搜索空间，并创建了一个神经网络模型准确性评估集（Model Calibration Dataset），以探讨神经网络模型的准确性和稳定性问题。</li>
<li>results: 研究发现，模型准确性可以在不同任务之间进行泛化，而且可以使用Robustness作为准确性评估指标。另外，研究还发现了一些常见的准确性评估指标的不可靠性，以及在不同的搜索空间中，post-hoc准确性调整方法对所有模型是否具有同样的影响。此外，研究还发现了准确性和精度之间的关系，以及带区大小对准确性评估指标的影响。最后，研究发现了一些特定的建筑设计可以提高神经网络模型的准确性。<details>
<summary>Abstract</summary>
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can robustness be used as a calibration measurement? (iii) How reliable are calibration metrics? (iv) Does a post-hoc calibration method affect all models uniformly? (v) How does calibration interact with accuracy? (vi) What is the impact of bin size on calibration measurement? (vii) Which architectural designs are beneficial for calibration? Additionally, our study bridges an existing gap by exploring calibration within NAS. By providing this dataset, we enable further research into NAS calibration. As far as we are aware, our research represents the first large-scale investigation into calibration properties and the premier study of calibration issues within NAS.
</details>
<details>
<summary>摘要</summary>
深度神经网络在不同的机器学习任务中日益普及，但是随着模型复杂度的增加，它们经常面临调整问题，即使Predictive accuracy得到了提高。许多研究尝试通过数据预处理、特定的损失函数和训练框架来改进调整性能。然而，调整性质的研究受到了一定的忽视。我们的研究利用Neural Architecture Search（NAS）搜索空间，提供了详细的模型建筑空间，以便对调整性质进行全面的探索。我们专门创建了一个模型调整数据集。这个数据集评估了90个分割值和12个附加的调整测量，在117,702个Unique Neural Networks中进行了广泛的 NATS-Bench 搜索空间中进行了测试。我们的分析旨在回答一些在领域中长期存在的问题，使用我们提出的数据集：（i）可否将模型调整 generalized 到不同任务？（ii）可否使用Robustness作为调整测量？（iii）如何判定调整指标的可靠性？（iv）post-hoc calibration方法对所有模型是否具有相同的影响？（v）调整与准确度之间是否存在相互关系？（vi）分割值如何影响调整测量？（vii）哪些建筑设计对调整有利？我们的研究填补了现有的空白，通过调整在NAS中进行exploration。我们的研究表明，调整性质在NAS中存在一定的问题，并且我们的研究是这类研究中的第一个大规模调整性质的研究。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity"><a href="#Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity" class="headerlink" title="Characterizing normal perinatal development of the human brain structural connectivity"></a>Characterizing normal perinatal development of the human brain structural connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11836">http://arxiv.org/abs/2308.11836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Wu, Lana Vasung, Camilo Calixto, Ali Gholipour, Davood Karimi<br>for:这个研究的目的是为了研究新生儿期的脑发育中Structural connectome的发展趋势。methods:这个研究使用了基于时空平均的计算框架，以确定新生儿期的Structural connectivity的 normative baselines。results:研究发现，在33-44postmenstrual weeks期间，Structural connectivity发展了明显的趋势，包括全脑和局部效率的增加，特征路径长度的减少，以及脑叶和脑半球之间的连接强化。此外，研究还发现了一些偏好性特征，这些特征在不同的连接评估方法中都有一致性。<details>
<summary>Abstract</summary>
Early brain development is characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain's cognitive abilities and influences its response to diseases and environmental factors. Hence, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determining such baselines. We used this framework to analyze the structural connectivity between 33 and 44 postmenstrual weeks using data from 166 subjects. Our results unveiled clear and strong trends in the development of structural connectivity in perinatal stage. Connection weighting based on fractional anisotropy and neurite density produced the most consistent results. We observed increases in global and local efficiency, a decrease in characteristic path length, and widespread strengthening of the connections within and across brain lobes and hemispheres. We also observed asymmetry patterns that were consistent between different connection weighting approaches. The new computational method and results are useful for assessing normal and abnormal development of the structural connectome early in life.
</details>
<details>
<summary>摘要</summary>
早期大脑发展 caracterized by the formation of a highly organized structural connectome. 这个 connectome的交互性是大脑的认知能力的基础，也影响了它对疾病和环境因素的应对。因此，在早期生长阶段的量化评估结构连接性是研究正常和异常神经发展的有用工具。然而，从Diffusion MRI数据中计算structural connectivity的估计具有复杂的计算。在早期生长阶段，这些计算受到迅速发展的大脑和成像困难的挑战。此外，高 между个体变化性和不同年龄的数据也使得 Charting the normal development of the structural connectome 是困难的。因此，我们缺乏可靠的正常发展基线的结构连接度度量。在这项研究中，我们开发了一种基于时空平均的计算框架，以确定这些基线。我们使用这个框架分析33-44周孕期的结构连接度，使用166个主要数据。我们的结果表明，在早期生长阶段，结构连接度呈现了明确和强的趋势。基于分数方差和神经纤维密度的连接重量Produced the most consistent results。我们发现全球和局部效率增加，特征路径长度减少，并且广泛加强连接在大脑叶和半球之间和之间。我们还发现了相互 symmetries 的偏好，这些偏好在不同的连接重量方法之间呈现一致。这些新的计算方法和结果有用于评估早期生长阶段的正常和异常结构连接度发展。
</details></li>
</ul>
<hr>
<h2 id="Algorithm-assisted-discovery-of-an-intrinsic-order-among-mathematical-constants"><a href="#Algorithm-assisted-discovery-of-an-intrinsic-order-among-mathematical-constants" class="headerlink" title="Algorithm-assisted discovery of an intrinsic order among mathematical constants"></a>Algorithm-assisted discovery of an intrinsic order among mathematical constants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11829">http://arxiv.org/abs/2308.11829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rotem Elimelech, Ofir David, Carlos De la Cruz Mengual, Rotem Kalisch, Wolfgang Berndt, Michael Shalyt, Mark Silberstein, Yaron Hadad, Ido Kaminer</li>
<li>for: 这个论文的目的是探索数学领域中的新概念和关系，利用计算机算法和人类直觉的结合来发现新的数学常量。</li>
<li>methods: 这篇论文使用了大规模并行计算机算法，探索了巨量的参数空间，并发现了一种新的数学结构——保守矩阵场。</li>
<li>results: 这篇论文发现了一系列新的约束数学常量表达式，包括ζ(3)的多个整数值，并通过新的数学证明，证明了这些常量的不可数性。这些结果表明了计算机支持的数学研究策略的力量，并开启了新的可能性 для解决长期开放的数学问题。<details>
<summary>Abstract</summary>
In recent decades, a growing number of discoveries in fields of mathematics have been assisted by computer algorithms, primarily for exploring large parameter spaces that humans would take too long to investigate. As computers and algorithms become more powerful, an intriguing possibility arises - the interplay between human intuition and computer algorithms can lead to discoveries of novel mathematical concepts that would otherwise remain elusive. To realize this perspective, we have developed a massively parallel computer algorithm that discovers an unprecedented number of continued fraction formulas for fundamental mathematical constants. The sheer number of formulas discovered by the algorithm unveils a novel mathematical structure that we call the conservative matrix field. Such matrix fields (1) unify thousands of existing formulas, (2) generate infinitely many new formulas, and most importantly, (3) lead to unexpected relations between different mathematical constants, including multiple integer values of the Riemann zeta function. Conservative matrix fields also enable new mathematical proofs of irrationality. In particular, we can use them to generalize the celebrated proof by Ap\'ery for the irrationality of $\zeta(3)$. Utilizing thousands of personal computers worldwide, our computer-supported research strategy demonstrates the power of experimental mathematics, highlighting the prospects of large-scale computational approaches to tackle longstanding open problems and discover unexpected connections across diverse fields of science.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test"><a href="#Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test" class="headerlink" title="Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test"></a>Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11827">http://arxiv.org/abs/2308.11827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saba Rahimi, Tucker Balch, Manuela Veloso</li>
<li>for: 本研究旨在使用Context不在GPT模型训练数据中的信息来解决GPT模型无法回答最新发展或非公共文档中的问题。</li>
<li>methods: 该方法包括对Context信息进行处理、将Context和问题embedding，通过Context embedding的集成构建提问、使用GPT模型回答问题。</li>
<li>results: 在控制测试enario中，使用加州driver’s Handbook作为信息源，GPT-3模型在50个样本驾驶知识测试题上达到了96%的通过率，而无Context情况下的通过率为82%。然而，Model仍然无法正确回答一些问题，反映出还有改进空间。研究还研究了提问长度和Context格式对模型性能的影响。<details>
<summary>Abstract</summary>
Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver's Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model's passing score fell to 82%. However, the model still fails to answer some questions correctly even with providing library of context, highlighting room for improvement. The research also examined the impact of prompt length and context format, on the model's performance. Overall, the study provides insights into the limitations and potential improvements for GPT models in question-answering tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型如Open AI的生成预训练变换器（GPT）模型在回答问题方面表现出色，但它们的知识受训数据的限制。这个限制使得它们无法回答最新的发展或者非公共文档中的问题。我们的研究提出了一种方法，使得GPT模型可以通过使用不包含在它们训练数据中的信息源来回答问题。该方法包括Contextual information的处理、查询和Context的编码、通过 integrate context embedding和构建提问的推荐、使用GPT模型回答问题。我们在控制测试enario中应用了这种方法，使用加利福尼亚驾驶手册作为信息源。GPT-3模型在50个示例驾驶知识测试问题中取得96%的通过率，而无Context的情况下，模型的通过率下降到82%。然而，即使提供了库存Context，模型仍然无法回答一些问题正确，这 highlights 进一步的改进空间。研究还检查了提问长度和Context格式对模型性能的影响。总的来说，这项研究提供了GPT模型在问题回答任务中的局限性和可能的改进方向。
</details></li>
</ul>
<hr>
<h2 id="Expressive-probabilistic-sampling-in-recurrent-neural-networks"><a href="#Expressive-probabilistic-sampling-in-recurrent-neural-networks" class="headerlink" title="Expressive probabilistic sampling in recurrent neural networks"></a>Expressive probabilistic sampling in recurrent neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11809">http://arxiv.org/abs/2308.11809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shirui Chen, Linxin Preston Jiang, Rajesh P. N. Rao, Eric Shea-Brown</li>
<li>for: 这篇论文的目的是解决 sampling-based Bayesian 模型中神经活动的问题，即神经活动被视为是 probablistic 计算中的样本。</li>
<li>methods: 这篇论文使用了函数分析和随机差分方程来探讨 recurrent 神经网络是如何从复杂分布中采样的。</li>
<li>results: 论文表明，使用分立输出单元的 recurrent 神经网络可以采样到任意分布，并提出了一种有效的训练方法基于减噪得分匹配。Empirical 试验表明，该模型可以采样到一些复杂数据分布。<details>
<summary>Abstract</summary>
In sampling-based Bayesian models of brain function, neural activities are assumed to be samples from probability distributions that the brain uses for probabilistic computation. However, a comprehensive understanding of how mechanistic models of neural dynamics can sample from arbitrary distributions is still lacking. We use tools from functional analysis and stochastic differential equations to explore the minimum architectural requirements for $\textit{recurrent}$ neural circuits to sample from complex distributions. We first consider the traditional sampling model consisting of a network of neurons whose outputs directly represent the samples (sampler-only network). We argue that synaptic current and firing-rate dynamics in the traditional model have limited capacity to sample from a complex probability distribution. We show that the firing rate dynamics of a recurrent neural circuit with a separate set of output units can sample from an arbitrary probability distribution. We call such circuits reservoir-sampler networks (RSNs). We propose an efficient training procedure based on denoising score matching that finds recurrent and output weights such that the RSN implements Langevin sampling. We empirically demonstrate our model's ability to sample from several complex data distributions using the proposed neural dynamics and discuss its applicability to developing the next generation of sampling-based brain models.
</details>
<details>
<summary>摘要</summary>
在基于抽样的贝叶斯模型中，神经活动被假设为抽样来自神经网络中的概率分布。然而，完整理解如何使机制模型的神经动力学可以从任意分布中抽样仍然缺乏。我们使用函数分析和随机差分方程来探索神经网络的最小建筑要求，以便它们可以从复杂的分布中抽样。我们首先考虑传统抽样模型，即一个由神经元输出直接表示抽样的网络（抽样器只网络）。我们 argue that synaptic current和神经元发射速率动力学在传统模型中有限的抽样能力。我们显示，一个具有分离输出单元的循环神经网络可以从任意概率分布中抽样。我们称之为储备抽样网络（RSN）。我们提出了一种高效的训练方法，基于排除掉噪声的对准得分，以找到循环和输出参数，使得 RSN 实现朗凡 sampling。我们employmontricate了我们的模型，并证明其可以从多种复杂数据分布中抽样，并讨论了其在开发下一代抽样基于脑模型方面的应用。
</details></li>
</ul>
<hr>
<h2 id="Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings"><a href="#Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings" class="headerlink" title="Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings"></a>Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11804">http://arxiv.org/abs/2308.11804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugene Bagdasaryan, Vitaly Shmatikov</li>
<li>for: 这种研究用于检测多modal embeddings中的攻击点，以及这些攻击点如何影响下游任务。</li>
<li>methods: 研究人员使用了多modal embeddings，并通过对这些 embeddings 进行攻击来证明它们的易攻击性。</li>
<li>results: 研究发现，使用这种攻击方法可以让恶意者将任意输入与其他模式的输入相关联，从而影响多modal embeddings 的性能。<details>
<summary>Abstract</summary>
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.   Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
</details>
<details>
<summary>摘要</summary>
多模态编码器将图像、声音、文本、视频等转换到单一的嵌入空间中，使表示之间匹配（例如，将一张狗图像与一个叫声相对应）。我们表明，多模态嵌入可能敏感于我们称为“ adversarial 幻觉”的攻击。给定任意模式的输入，敌方可以对其进行扰动，使其嵌入接近另一种选择的敌方输入的嵌入。幻觉如此能让敌方将任意图像与任意文本、任意声音等相对应。这些幻觉利用嵌入空间的近似性，因此不受下游任务的限制。使用 ImageBind 嵌入，我们示例了如何使用不知道下游任务的情况，通过生成 adversarially 对齐的输入，使图像生成、文本生成和零学习分类发生幻觉。
</details></li>
</ul>
<hr>
<h2 id="WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters"><a href="#WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters" class="headerlink" title="WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters"></a>WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11776">http://arxiv.org/abs/2308.11776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ange Lou, Jack Noble</li>
<li>for: 这个论文的目的是建立一个自我超vised的深度和 egocentric 运动估计系统，以便在外科视频中提取深度图和摄像头参数。</li>
<li>methods: 该论文使用了一种基于 cost-volume 的超vision 方法来为系统提供辅助supervision，以便预测摄像头参数。</li>
<li>results: 实验结果表明，提议的方法可以改善摄像头参数、 egocentric 运动和深度估计的准确性。<details>
<summary>Abstract</summary>
Depth estimation in surgical video plays a crucial role in many image-guided surgery procedures. However, it is difficult and time consuming to create depth map ground truth datasets in surgical videos due in part to inconsistent brightness and noise in the surgical scene. Therefore, building an accurate and robust self-supervised depth and camera ego-motion estimation system is gaining more attention from the computer vision community. Although several self-supervision methods alleviate the need for ground truth depth maps and poses, they still need known camera intrinsic parameters, which are often missing or not recorded. Moreover, the camera intrinsic prediction methods in existing works depend heavily on the quality of datasets. In this work, we aimed to build a self-supervised depth and ego-motion estimation system which can predict not only accurate depth maps and camera pose, but also camera intrinsic parameters. We proposed a cost-volume-based supervision manner to give the system auxiliary supervision for camera parameters prediction. The experimental results showed that the proposed method improved the accuracy of estimated camera parameters, ego-motion, and depth estimation.
</details>
<details>
<summary>摘要</summary>
depth 估算在手术视频中发挥重要作用，但创建深度地图真实数据集在手术视频中具有不一致的亮度和噪声，因此建立精准和可靠的自我超视方法在计算机视觉社区中受到更多的关注。虽然一些自我超视方法可以减少需要深度地图和姿态的真实数据，但它们仍然需要已知的摄像头内参数，这些参数通常缺失或者不记录。此外，现有的摄像头内参数预测方法依赖于数据质量的改进。在这项工作中，我们希望建立一个可以预测不仅准确的深度地图和摄像头姿态，还可以预测摄像头内参数的自我超视系统。我们提出了基于Cost Volume的超视方式，以供系统进行摄像头参数预测的auxiliary超视。实验结果表明，我们的方法可以提高摄像头参数、ego-动作和深度估算的准确性。
</details></li>
</ul>
<hr>
<h2 id="3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network"><a href="#3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network" class="headerlink" title="3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network"></a>3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11771">http://arxiv.org/abs/2308.11771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinyu Chen, Zuowen Wang, Shih-Chii Liu, Chang Gao</li>
<li>for: 这 paper 是为了开发一种基于 Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) 模型，用于Event-based eye tracking，这是下一代可穿戴医疗技术，如 AR&#x2F;VR 头盔。</li>
<li>methods: 这 paper 使用了 retina-inspired event cameras，它们具有低延迟响应和稀疏输出事件流，而不是传统的帧基本摄像头。CB-ConvLSTM 架构 efficiently 提取了 spatial-temporal 特征，用于 pupil tracking，并且比 convential CNN 结构更高效。</li>
<li>results: 这 paper 使用 delta-encoded recurrent path 提高了 activation sparsity，从而减少了数学运算量约 4.7 $\times$，不会失去准确性。这使得它成为实时眼动跟踪的理想选择，特别是在资源有限的设备上。项目代码和数据集都公开可用于 \url{<a target="_blank" rel="noopener" href="https://github.com/qinche106/cb-convlstm-eyetracking%7D">https://github.com/qinche106/cb-convlstm-eyetracking}</a>.<details>
<summary>Abstract</summary>
This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR/VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\times$ without losing accuracy when tested on a \texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \url{https://github.com/qinche106/cb-convlstm-eyetracking}.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种稀疏变化基于卷积长短期记忆遮盾（CB-ConvLSTM）模型，用于事件基于眼动跟踪，这是下一代可穿戴医疗技术如AR/VR头戴式设备的关键。我们利用了眼睛引发的事件摄像头的优点，即快速响应和稀疏输出事件流，而不是传统的帧基本摄像头。我们的CB-ConvLSTM架构有效地提取了眼动跟踪的空间时间特征，超过了传统的CNN结构。通过使用delta编码的回归路增强活动稀疏，CB-ConvLSTM可以减少笔算操作数约4.7倍，无损loss性能，使其适用于实时眼动跟踪 resource-constrained设备。项目代码和数据集在GitHub上公开可用，请参考\url{https://github.com/qinche106/cb-convlstm-eyetracking}.
</details></li>
</ul>
<hr>
<h2 id="Halo-Estimation-and-Reduction-of-Hallucinations-in-Open-Source-Weak-Large-Language-Models"><a href="#Halo-Estimation-and-Reduction-of-Hallucinations-in-Open-Source-Weak-Large-Language-Models" class="headerlink" title="Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models"></a>Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11764">http://arxiv.org/abs/2308.11764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/engsalem/halo">https://github.com/engsalem/halo</a></li>
<li>paper_authors: Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu</li>
<li>for: 这 paper 的目的是量化和缓解强大语言模型（LLMs）中的幻觉现象。</li>
<li>methods: 这 paper 使用了一种名为 HaloCheck 的轻量级黑盒无知框架，以量化 LLMS 中幻觉的严重程度。此外，paper 还探讨了知识注入和教师学生方法来缓解 LLMS 中幻觉现象。</li>
<li>results:  experiments 表明，使用 HaloCheck 和其他技术可以有效缓解 LLMS 中幻觉现象，特别是在复杂的领域。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型自然语言处理模型（LLM）已经革命化了自然语言处理（NLP）领域。虽然对研究和实际应用来说非常方便，但公共源 LLM 的 fewer parameters  often suffer from severe hallucinations compared to their larger counterparts. 这篇论文关注测量和减少 LLM 中的 hallucinations，特别是公共源 LLM 中的 BLOOM 7B。我们介绍 HaloCheck，一种轻量级黑盒无知框架，用于评估 LLM 中 hallucinations 的严重程度。此外，我们还探讨了如何通过知识注入和教师-学生方法来缓解低参数 LLM 中的 hallucinations。我们的实验效果地示了在这些 LLM 中的挑战领域中减少 hallucinations。
</details></li>
</ul>
<hr>
<h2 id="VBMO-Voting-Based-Multi-Objective-Path-Planning"><a href="#VBMO-Voting-Based-Multi-Objective-Path-Planning" class="headerlink" title="VBMO: Voting-Based Multi-Objective Path Planning"></a>VBMO: Voting-Based Multi-Objective Path Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11755">http://arxiv.org/abs/2308.11755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raj Korpan</li>
<li>for: 本研究开发了一种VBMO算法，用于生成优化单个目标计划，并对每个目标进行评估，使用投票机制来选择最佳计划。</li>
<li>methods: VBMO算法不使用手动调整的权重，不是基于进化算法，而是根据一个计划在一个目标方面的优化程度来评估其在其他目标方面的表现。VBMO使用三种投票机制：范围、波达和组合批准。</li>
<li>results: 对于多种和复杂的环境，VBMO算法能够高效生成满足多个目标的计划。<details>
<summary>Abstract</summary>
This paper presents VBMO, the Voting-Based Multi-Objective path planning algorithm, that generates optimal single-objective plans, evaluates each of them with respect to the other objectives, and selects one with a voting mechanism. VBMO does not use hand-tuned weights, consider the multiple objectives at every step of search, or use an evolutionary algorithm. Instead, it considers how a plan that is optimal in one objective may perform well with respect to others. VBMO incorporates three voting mechanisms: range, Borda, and combined approval. Extensive evaluation in diverse and complex environments demonstrates the algorithm's ability to efficiently produce plans that satisfy multiple objectives.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Instance-Adversarial-Attack-on-GNN-Based-Malicious-Domain-Detection"><a href="#Multi-Instance-Adversarial-Attack-on-GNN-Based-Malicious-Domain-Detection" class="headerlink" title="Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection"></a>Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11754">http://arxiv.org/abs/2308.11754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Nazzal, Issa Khalil, Abdallah Khreishah, NhatHai Phan, Yao Ma</li>
<li>for: 这种研究的目的是检测互联网域名是否与网络攻击相关。</li>
<li>methods: 该方法使用图神经网络（GNN）来推断互联网域名的危险程度，并使用DNS日志来构建域名图（DMG）。</li>
<li>results: 该研究发现，现有的单个攻击者节点 manipulation 技术不具备防止多个节点同时 manipulate 的能力，并且提出了一种基于黑盒模型的多实例攻击方法（MintA），可以在无法访问模型的情况下进行攻击。该方法可以在实际数据上实现攻击成功率超过 80%。<details>
<summary>Abstract</summary>
Malicious domain detection (MDD) is an open security challenge that aims to detect if an Internet domain is associated with cyber-attacks. Among many approaches to this problem, graph neural networks (GNNs) are deemed highly effective. GNN-based MDD uses DNS logs to represent Internet domains as nodes in a maliciousness graph (DMG) and trains a GNN to infer their maliciousness by leveraging identified malicious domains. Since this method relies on accessible DNS logs to construct DMGs, it exposes a vulnerability for adversaries to manipulate their domain nodes' features and connections within DMGs. Existing research mainly concentrates on threat models that manipulate individual attacker nodes. However, adversaries commonly generate multiple domains to achieve their goals economically and avoid detection. Their objective is to evade discovery across as many domains as feasible. In this work, we call the attack that manipulates several nodes in the DMG concurrently a multi-instance evasion attack. We present theoretical and empirical evidence that the existing single-instance evasion techniques for are inadequate to launch multi-instance evasion attacks against GNN-based MDDs. Therefore, we introduce MintA, an inference-time multi-instance adversarial attack on GNN-based MDDs. MintA enhances node and neighborhood evasiveness through optimized perturbations and operates successfully with only black-box access to the target model, eliminating the need for knowledge about the model's specifics or non-adversary nodes. We formulate an optimization challenge for MintA, achieving an approximate solution. Evaluating MintA on a leading GNN-based MDD technique with real-world data showcases an attack success rate exceeding 80%. These findings act as a warning for security experts, underscoring GNN-based MDDs' susceptibility to practical attacks that can undermine their effectiveness and benefits.
</details>
<details>
<summary>摘要</summary>
“恶意域名检测（MDD）是一个开放的安全挑战，旨在检测互联网域名是否与网络攻击相关。许多方法来解决这个问题，Graph Neural Networks（GNN）被视为非常有效。GNN基于的 MDD 使用 DNS 日志来表示互联网域名为节点在恶意域名图（DMG）中，并使用 GNN 来推断它们的恶意程度，利用已知的恶意域名。由于这种方法依赖于可 accessible DNS 日志来构建 DMG，因此暴露了一个攻击者可以 manipulate 其域名节点的特征和连接在 DMG 中的漏洞。现有的研究主要集中在单个攻击者节点的威胁模型上。然而，攻击者通常会生成多个域名来实现他们的目标，以避免检测。我们称这种攻击为多实例逃脱攻击。我们提供了证明和实验证据，证明现有的单实例逃脱技术无法对 GNN 基于的 MDD 进行多实例逃脱攻击。因此，我们引入 MintA，一种在检测时进行多实例逃脱攻击的含糊逻辑攻击。MintA 通过优化的杂化和只有黑盒访问目标模型的能力，实现节点和邻居隐蔽性的提高。我们提出了 MintA 的优化挑战，并实现了一个近似解决方案。对一种主流 GNN 基于 MDD 技术进行实验，MintA 的攻击成功率超过 80%。这些发现作为一个警告，强调 GNN 基于 MDD 的抵御力和优势受到了实际攻击的威胁。”
</details></li>
</ul>
<hr>
<h2 id="Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data"><a href="#Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data" class="headerlink" title="Patient Clustering via Integrated Profiling of Clinical and Digital Data"></a>Patient Clustering via Integrated Profiling of Clinical and Digital Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11748">http://arxiv.org/abs/2308.11748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjin Choi, Andy Xiang, Ozgur Ozturk, Deep Shrestha, Barry Drake, Hamid Haidarian, Faizan Javed, Haesun Park</li>
<li>for: 这个研究是为了开发一个基于Profile的患者划分模型，用于医疗数据分析。</li>
<li>methods: 这个模型使用基于受限制低维度approximation的方法，利用患者的临床数据和数字互动数据（包括浏览和搜索）构建患者profile。这个方法生成了非负嵌入 вектор，作为患者的低维度表示。</li>
<li>results: 对于使用真实世界患者数据集进行评估，这个方法在划分准确性和推荐精度方面表现出优于其他基线方法。<details>
<summary>Abstract</summary>
We introduce a novel profile-based patient clustering model designed for clinical data in healthcare. By utilizing a method grounded on constrained low-rank approximation, our model takes advantage of patients' clinical data and digital interaction data, including browsing and search, to construct patient profiles. As a result of the method, nonnegative embedding vectors are generated, serving as a low-dimensional representation of the patients. Our model was assessed using real-world patient data from a healthcare web portal, with a comprehensive evaluation approach which considered clustering and recommendation capabilities. In comparison to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种基于专业 patient clustering 模型，适用于医疗数据。我们的模型通过使用受限制的低级数据方法，使用病人的临床数据和数字互动数据（包括浏览和搜索）构建病人 профи。这样，我们可以生成非负嵌入 вектор，作为病人的低维度表示。我们的模型在使用实际的病人数据from a healthcare web portal 进行评估，并通过了一种全面的评估方法，考虑 clustering 和推荐能力。与其他基准相比，我们的方法在 clustering 准确性和推荐精度方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Lifted-Inference-beyond-First-Order-Logic"><a href="#Lifted-Inference-beyond-First-Order-Logic" class="headerlink" title="Lifted Inference beyond First-Order Logic"></a>Lifted Inference beyond First-Order Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11738">http://arxiv.org/abs/2308.11738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagar Malhotra, Davide Bizzaro, Luciano Serafini</li>
<li>for: 这 paper 的目的是探讨Weighted First Order Model Counting (WFOMC) 的基本性和可行性，以及可以在 probabilistic inference 中使用的逻辑 фрагментов。</li>
<li>methods: 这 paper 使用了一种新的方法 called “counting by splitting”，用于解决 WFOMC 的难题。这种方法可以应用于多种不同的逻辑结构，如directed acyclic graphs, connected graphs, trees, etc。</li>
<li>results: 这 paper 的结果表明，使用 “counting by splitting” 方法可以将多种逻辑结构转化为 domain-liftable 的形式，从而实现 probabilistic inference。此外，这 paper 还推广了许多之前的结果，如 directed acyclic graphs, phylogenetic networks, etc。<details>
<summary>Abstract</summary>
Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic inference in statistical relational learning models. As WFOMC is known to be intractable in general ($\#$P-complete), logical fragments that admit polynomial time WFOMC are of significant interest. Such fragments are called domain liftable. Recent works have shown that the two-variable fragment of first order logic extended with counting quantifiers ($\mathrm{C^2}$) is domain-liftable. However, many properties of real-world data, like acyclicity in citation networks and connectivity in social networks, cannot be modeled in $\mathrm{C^2}$, or first order logic in general. In this work, we expand the domain liftability of $\mathrm{C^2}$ with multiple such properties. We show that any $\mathrm{C^2}$ sentence remains domain liftable when one of its relations is restricted to represent a directed acyclic graph, a connected graph, a tree (resp. a directed tree) or a forest (resp. a directed forest). All our results rely on a novel and general methodology of "counting by splitting". Besides their application to probabilistic inference, our results provide a general framework for counting combinatorial structures. We expand a vast array of previous results in discrete mathematics literature on directed acyclic graphs, phylogenetic networks, etc.
</details>
<details>
<summary>摘要</summary>
Weighted First Order Model Counting (WFOMC) 是统计关系学习模型的基本概念。由于 WFOMC 在总体来说是NP完全的（#P-完全），因此可以在有限时间内完成的逻辑 фрагменты具有重要的科学意义。这些 фрагменты被称为域 liftable。 recent works 表明，两变量 fragments 的第一阶alogic 加上计数量词（C^2）可以域 liftable。然而，许多实际数据的特性，如社交网络中的环状图和 citations 网络中的连接性，无法在 C^2 或首阶alogic 中表示。在这种情况下，我们扩展了 C^2 的域 liftability，使其能够模型这些特性。我们证明，任何 C^2 句子都可以域 liftable，当其中一个关系被限制为表示直接径向图、连接图、树（resp. 直接树）或森林（resp. 直接森林）时。我们的结果基于一种新的通用方法，称为“计数分解”。 beside 其应用于 probabilistic inference，我们的结果提供了一个总体的计数结构框架。我们扩展了许多过去的结果 в 直接径向图、phylogenetic 网络等数学文献中。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering"><a href="#Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering" class="headerlink" title="Knowledge Graph Prompting for Multi-Document Question Answering"></a>Knowledge Graph Prompting for Multi-Document Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11730">http://arxiv.org/abs/2308.11730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa Siu, Ruiyi Zhang, Tyler Derr<br>for: 这种方法可以帮助大语言模型在多文档问答 задании中提高表现，特别是在需要深刻理解不同文档之间的逻辑关系时。methods: 这种方法包括建立知识图和LM帮助图 traversal模块，用于导航多文档之间的Semantic&#x2F;语言相似性和结构关系。results: 广泛的实验表明，这种方法可以提高大语言模型在多文档问答 задании中的表现，并且可以减少检索延迟。<details>
<summary>Abstract</summary>
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the LM-guided traverser acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code is at https://github.com/YuWVandy/KG-LLM-MDQA.
</details>
<details>
<summary>摘要</summary>
“对多篇文档问题回答（MD-QA）任务，大型语言模型（LLM）的“预训、提示、预测”模式已经实现了杰出的成功。然而，现有的研究几乎没有探讨这种模式在MD-QA任务中的应用。为了填补这个重要的空白，我们提出了知识图表示法（KGP），用于将适当的 контекст提示 LLM 进行 MD-QA，这包括对多篇文档建立知识图和对图中的node和edge进行遍历。 для知识图建立，我们创建了多篇文档之间的知识图，其中node表示文档或文档中的章节/表格，而edge表示文档之间的semantic/lexical相似性或内部结构相关。 для图中的遍历，我们设计了LM-导向的图游击者，可以在图中穿梭，寻找支持文档，以助LLM进行MD-QA。建立的图 acted as a global ruler，对文档之间的转换空间实现了统一的规范，同时LM-导向的游击者 acted as a local navigator，寻找适当的上下文，以逐渐进行问题回答和提高检索质量。实验结果证明了KGP的可行性和有效性，这表明了可以通过图在提高LLM的预训设计中应用。我们的代码可以在https://github.com/YuWVandy/KG-LLM-MDQA中找到。”
</details></li>
</ul>
<hr>
<h2 id="Efficient-Benchmarking-of-Language-Models"><a href="#Efficient-Benchmarking-of-Language-Models" class="headerlink" title="Efficient Benchmarking (of Language Models)"></a>Efficient Benchmarking (of Language Models)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11696">http://arxiv.org/abs/2308.11696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sumankrsh/Sentiment-Analysis.ipynb">https://github.com/sumankrsh/Sentiment-Analysis.ipynb</a></li>
<li>paper_authors: Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen</li>
<li>for: 本研究旨在提高语言模型（LM）评估 benchmark的效率，不会 compromising 可靠性。</li>
<li>methods: 该研究使用 HELM  benchmark 作为测试例子， investigate 不同 benchmark 设计选择对计算成本-可靠性贸易的影响。提出一种新的度量方法 Decision Impact on Reliability（DIoR）来评估决策对可靠性的影响。</li>
<li>results: 研究发现，现有领先者在 HELM 上可能会改变，只需要移除一些低排名的模型即可；同时，一些不同的 HELM 场景选择可以导致成本计算的变化。基于这些发现，提出了一些具体的建议，可以带来计算成本的减少，而无需牺牲 benchmark 的可靠性。例如，可以通过 x100 或更多的计算成本减少来实现这一目标。<details>
<summary>Abstract</summary>
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark ranking. Conversely a slightly different choice of HELM scenarios varies ranking widely. Based on our findings we outline a set of concrete recommendations for more efficient benchmark design and utilization practices leading to dramatic cost savings with minimal loss of benchmark reliability often reducing computation by x100 or more.
</details>
<details>
<summary>摘要</summary>
LM模型的多样化化使得新一代的测试标准出现了，这些标准可以全面评估LM模型的各种能力。然而，这些评估努力的效率却得到了少量的讨论。在这篇文章中，我们提出了一个问题，即如何智能减少LM评估计算成本而不失去可靠性。使用HELM测试标准作为示例，我们研究了不同的测试标准设计选择对计算vs可靠性的负担交互的影响。我们提出了一个新的度量指标，即决策影响可靠性（DIoR），以评估这些决策的可靠性。我们发现，例如，当 removing一个低排名模型时，可以改变当前领先者的位置，并且只需几个例子即可获得正确的排名。然而，不同的HELM场景选择会导致排名差异很大。根据我们的发现，我们提出了一些具体的建议，以提高LM评估设计和使用做法，从而实现计算成本的减少，通常是x100或更多的减少，而且失去可靠性的可能性很低。
</details></li>
</ul>
<hr>
<h2 id="Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models"><a href="#Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models" class="headerlink" title="Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models"></a>Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11601">http://arxiv.org/abs/2308.11601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surya Narayanan Hari, Matt Thomson</li>
<li>for: 这个研究是为了提出一个适应环境的路由系统，Tryage，以便自动选择适当的语言模型库中的专家模型，以满足用户的多元化工作流程和数据领域的需求，同时解决了计算、安全性和新鲜度等考虑。</li>
<li>methods: Tryage使用了一个语言模型路由器来预测下游模型的表现，然后使用一个目标函数集成表现预测、用户目标和限制（例如模型大小、模型新鲜度、安全性、 verbosity 和可读性）来做路由决策。</li>
<li>results: Tryage在多元的数据集中，包括代码、文本、医疗资料和专利，超过 Gorilla 和 GPT3.5 Turbo 在动态模型选择中，可以实现50.9% 的准确率，比 GPT3.5 Turbo 的23.6% 和 Gorilla 的10.8% 高得多。<details>
<summary>Abstract</summary>
The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.
</details>
<details>
<summary>摘要</summary>
<sys><trans-type>text</trans-type><text><![CDATA[Introduction of transformer architecture and self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200,000 models in the Hugging Face ecosystem, users struggle with selecting and optimizing models for multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. We propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection, identifying the optimal model with an accuracy of 50.9%, compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.]]></text></sys>Note that Simplified Chinese is the standard form of Chinese used in mainland China, and it is different from Traditional Chinese, which is used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge"><a href="#Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge" class="headerlink" title="Practical Insights on Incremental Learning of New Human Physical Activity on the Edge"></a>Practical Insights on Incremental Learning of New Human Physical Activity on the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11691">http://arxiv.org/abs/2308.11691</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Arvanitakis, Jingwei Zuo, Mthandazo Ndhlovu, Hakim Hacid</li>
<li>for: This paper explores the challenges of Edge-based learning, particularly in the context of limited data storage, computing power, and the number of learning classes.</li>
<li>methods: The paper uses the MAGNETO system to conduct experiments and demonstrate the challenges of Edge ML, using data collected from mobile sensors to learn human activities.</li>
<li>results: The paper highlights the challenges of Edge ML and offers valuable perspectives on how to address them.<details>
<summary>Abstract</summary>
Edge Machine Learning (Edge ML), which shifts computational intelligence from cloud-based systems to edge devices, is attracting significant interest due to its evident benefits including reduced latency, enhanced data privacy, and decreased connectivity reliance. While these advantages are compelling, they introduce unique challenges absent in traditional cloud-based approaches. In this paper, we delve into the intricacies of Edge-based learning, examining the interdependencies among: (i) constrained data storage on Edge devices, (ii) limited computational power for training, and (iii) the number of learning classes. Through experiments conducted using our MAGNETO system, that focused on learning human activities via data collected from mobile sensors, we highlight these challenges and offer valuable perspectives on Edge ML.
</details>
<details>
<summary>摘要</summary>
《边缘机器学习（边缘ML）》，它将计算智能从云端系统传递到边缘设备，目前吸引了广泛关注，因为它的明显优势包括降低延迟、提高数据隐私和减少连接依赖。虽然这些优势吸引人，但它们也引入了传统云端方法中缺失的挑战。本文将探讨边缘学习的细节，探讨（i）边缘设备的受限数据存储、（ii）训练计算能力的限制和（iii）学习类数。通过我们的MAGNETO系统的实验，我们探讨了这些挑战，并提供了边缘ML的有价值见解。
</details></li>
</ul>
<hr>
<h2 id="Handling-the-inconsistency-of-systems-of-min-rightarrow-fuzzy-relational-equations"><a href="#Handling-the-inconsistency-of-systems-of-min-rightarrow-fuzzy-relational-equations" class="headerlink" title="Handling the inconsistency of systems of $\min\rightarrow$ fuzzy relational equations"></a>Handling the inconsistency of systems of $\min\rightarrow$ fuzzy relational equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12385">http://arxiv.org/abs/2308.12385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ismaïl Baaj</li>
<li>for: 这篇论文研究了系统$\min-\rightarrow$抽象关系方程的不一致性。</li>
<li>methods: 该论文使用了分析方法，计算了基于系统$\min-\rightarrow$抽象关系方程的Chebysev距离$\nabla &#x3D; \inf_{d \in \mathcal{D}} \Vert \beta - d \Vert$。</li>
<li>results: 该论文显示了$\nabla$的下界是一个vector不等式的解，无论使用了哪种剩下推导器（G&quot;odel、Goguen或Lukasiewicz）。此外，在$\min-\rightarrow_{G}$系统中，$\nabla$可能是下界，而在$\min-\rightarrow_{GG}$和$\min-\rightarrow_{L}$系统中，$\nabla$总是最小值。<details>
<summary>Abstract</summary>
In this article, we study the inconsistency of systems of $\min-\rightarrow$ fuzzy relational equations. We give analytical formulas for computing the Chebyshev distances $\nabla = \inf_{d \in \mathcal{D}} \Vert \beta - d \Vert$ associated to systems of $\min-\rightarrow$ fuzzy relational equations of the form $\Gamma \Box_{\rightarrow}^{\min} x = \beta$, where $\rightarrow$ is a residual implicator among the G\"odel implication $\rightarrow_G$, the Goguen implication $\rightarrow_{GG}$ or Lukasiewicz's implication $\rightarrow_L$ and $\mathcal{D}$ is the set of second members of consistent systems defined with the same matrix $\Gamma$. The main preliminary result that allows us to obtain these formulas is that the Chebyshev distance $\nabla$ is the lower bound of the solutions of a vector inequality, whatever the residual implicator used. Finally, we show that, in the case of the $\min-\rightarrow_{G}$ system, the Chebyshev distance $\nabla$ may be an infimum, while it is always a minimum for $\min-\rightarrow_{GG}$ and $\min-\rightarrow_{L}$ systems.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们研究了系统$\min-\rightarrow$uzzifiable relational equation的不一致性。我们给出了计算$\nabla$的分解式，其相关于系统$\Gamma \Box_{\rightarrow}^{\min} x = \beta$，其中$\rightarrow$是G\"odel逻辑$\rightarrow_G$, Goguen逻辑$\rightarrow_{GG}$或Lukasiewicz逻辑$\rightarrow_L$中的剩余逻辑，而$\mathcal{D}$是定义同一个矩阵$\Gamma$的一致系统中的第二个成员的集合。主要的前提结果是$\nabla$是一个准确的下界，无论使用哪种剩余逻辑。最后，我们表明，在$\min-\rightarrow_{G}$系统中，$\nabla$可能是下界，而在$\min-\rightarrow_{GG}$和$\min-\rightarrow_{L}$系统中，$\nabla$总是最小值。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/cs.AI_2023_08_23/" data-id="clltaagmk0012r8887ehm6igi" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.CL_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.CL_2023_08_23/">cs.CL - 2023-08-23 19:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Challenges-of-Machine-Learning-for-Trust-and-Safety-A-Case-Study-on-Misinformation-Detection"><a href="#The-Challenges-of-Machine-Learning-for-Trust-and-Safety-A-Case-Study-on-Misinformation-Detection" class="headerlink" title="The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection"></a>The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12215">http://arxiv.org/abs/2308.12215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ramybaly/News-Media-Reliability">https://github.com/ramybaly/News-Media-Reliability</a></li>
<li>paper_authors: Madelyne Xiao, Jonathan Mayer</li>
<li>for: 这个论文旨在探讨机器学习在信任和安全问题上的应用，使用假信息检测作为 caso study。</li>
<li>methods: 作者系мати化了关于自动检测假信息的文献，并对270篇最具影响力的论文进行分析。</li>
<li>results: 研究发现现有文献中存在 significiant 缺陷，包括数据和代码可用性差、设计错误、可重现性和泛化能力差。这些缺陷使得现有的模型在实际应用中效果不佳。<details>
<summary>Abstract</summary>
We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is for future work to avoid the pitfalls that we identify.
</details>
<details>
<summary>摘要</summary>
我团队研究机器学习应用于信任和安全问题上的偏误，使用假信息检测为案例研究。我们系统化了 relate to 270 篇引用论文中的自动检测假信息方法。然后，我们分析了这些论文中的数据和代码可用性、设计异常、可重现性和泛化性问题。我们发现了 significiant 的缺陷，质疑了已经宣称的性能和实用性。检测任务经常与实际场景不同，数据集和模型评估不符合实际情况，评估方法常常与模型训练无关。模型对尝试数据的泛化性也很差。根据这些结果，我们提出了评估机器学习应用于信任和安全问题的建议。我们希望未来的研究可以避免我们所identify的坑。
</details></li>
</ul>
<hr>
<h2 id="Curriculum-Learning-with-Adam-The-Devil-Is-in-the-Wrong-Details"><a href="#Curriculum-Learning-with-Adam-The-Devil-Is-in-the-Wrong-Details" class="headerlink" title="Curriculum Learning with Adam: The Devil Is in the Wrong Details"></a>Curriculum Learning with Adam: The Devil Is in the Wrong Details</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12202">http://arxiv.org/abs/2308.12202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Weber, Jaap Jumelet, Paul Michel, Elia Bruni, Dieuwke Hupkes</li>
<li>for: 这篇论文主要研究了机器学习模型在不同学习阶段上的学习效果，以及如何使机器学习模型更加高效地学习。</li>
<li>methods: 作者们使用了许多现有的CURRICULUM学习方法，包括手动设计的CL方法和自动生成的CL方法，以评估它们在自然语言处理（NLP）领域的效果。</li>
<li>results: 作者们发现，当CURRICULUM方法与流行的Adam优化算法结合使用时，它们经常会适应不合适的优化参数，导致学习效果下降。作者们通过多个实验案例来证明这一点，并发现无论使用哪种CL方法，都无法超越仅使用Adam优化器和合适的Hyperparameter的学习效果。<details>
<summary>Abstract</summary>
Curriculum learning (CL) posits that machine learning models -- similar to humans -- may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chosen hyperparameters. As such, our results contribute to understanding why CL methods work, but at the same time urge caution when claiming positive results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Instruction-Position-Matters-in-Sequence-Generation-with-Large-Language-Models"><a href="#Instruction-Position-Matters-in-Sequence-Generation-with-Large-Language-Models" class="headerlink" title="Instruction Position Matters in Sequence Generation with Large Language Models"></a>Instruction Position Matters in Sequence Generation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12097">http://arxiv.org/abs/2308.12097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adaxry/post-instruction">https://github.com/adaxry/post-instruction</a></li>
<li>paper_authors: Yijin Liu, Xianfeng Zeng, Fandong Meng, Jie Zhou</li>
<li>for: 提高大型自然语言模型（LLM）的条件序列生成能力，包括翻译和摘要等任务。</li>
<li>methods: 通过修改模型的 instruciton 排序来增强 LLM 的 instruction 遵循能力。</li>
<li>results: 对多种模型规模（1B &#x2F; 7B &#x2F; 13B）和不同的序列生成任务（翻译和摘要）进行了实验，并且在零基eline情况下显著提高了 conditional sequence generation 的性能，例如在 WMT zero-shot 翻译任务上提高了最高达 9.7 BLEU 点。<details>
<summary>Abstract</summary>
Large language models (LLMs) are capable of performing conditional sequence generation tasks, such as translation or summarization, through instruction fine-tuning. The fine-tuning data is generally sequentially concatenated from a specific task instruction, an input sentence, and the corresponding response. Considering the locality modeled by the self-attention mechanism of LLMs, these models face the risk of instruction forgetting when generating responses for long input sentences. To mitigate this issue, we propose enhancing the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences. Theoretical analysis suggests that our straightforward method can alter the model's learning focus, thereby emphasizing the training of instruction-following capabilities. Concurrently, experimental results demonstrate that our approach consistently outperforms traditional settings across various model scales (1B / 7B / 13B) and different sequence generation tasks (translation and summarization), without any additional data or annotation costs. Notably, our method significantly improves the zero-shot performance on conditional sequence generation, e.g., up to 9.7 BLEU points on WMT zero-shot translation tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Hybrid-Retrieval-and-Multi-stage-Text-Ranking-Solution-at-TREC-2022-Deep-Learning-Track"><a href="#Hybrid-Retrieval-and-Multi-stage-Text-Ranking-Solution-at-TREC-2022-Deep-Learning-Track" class="headerlink" title="Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track"></a>Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12039">http://arxiv.org/abs/2308.12039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangwei Xu, Yangzhao Zhang, Longhui Zhang, Dingkun Long, Pengjun Xie, Ruijie Guo</li>
<li>for: 本文是提交到TREC 2022 Deep Learning Track的系统描述。</li>
<li>methods: 本文采用混合文本 Retrieval和多阶段文本排名方法。 Retrieval阶段结合了传统稀疏检索和神经积累检索两种结构。 排名阶段除了基于大型预训练语言模型的全交互式排名模型之外，还提出了轻量级子排名模块以进一步提高文本排名性能。</li>
<li>results: 评估结果表明我们提出的方法有效。我们的模型在试用集上 achieved the 1st和4th rank for passage ranking and document ranking respectively。<details>
<summary>Abstract</summary>
Large-scale text retrieval technology has been widely used in various practical business scenarios. This paper presents our systems for the TREC 2022 Deep Learning Track. We explain the hybrid text retrieval and multi-stage text ranking method adopted in our solution. The retrieval stage combined the two structures of traditional sparse retrieval and neural dense retrieval. In the ranking stage, in addition to the full interaction-based ranking model built on large pre-trained language model, we also proposes a lightweight sub-ranking module to further enhance the final text ranking performance. Evaluation results demonstrate the effectiveness of our proposed approach. Our models achieve the 1st and 4th rank on the test set of passage ranking and document ranking respectively.
</details>
<details>
<summary>摘要</summary>
大规模文本检索技术在各种实际业务场景中广泛应用。本文介绍我们在TREC 2022深度学习轨道上的系统。我们解释了我们采用的混合文本检索和多stage文本排名方法。检索阶段组合了传统稀疏检索和神经 dense检索两种结构。排名阶段除了基于大型预训练语言模型构建的全面互动型排名模型外，我们还提出了轻量级副排名模块，以进一步提高文本排名性能。评估结果表明我们提出的方法效果。我们的模型在测试集上取得了文章排名和文档排名的1st和4th名。
</details></li>
</ul>
<hr>
<h2 id="Large-Multilingual-Models-Pivot-Zero-Shot-Multimodal-Learning-across-Languages"><a href="#Large-Multilingual-Models-Pivot-Zero-Shot-Multimodal-Learning-across-Languages" class="headerlink" title="Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages"></a>Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12038">http://arxiv.org/abs/2308.12038</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openbmb/viscpm">https://github.com/openbmb/viscpm</a></li>
<li>paper_authors: Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, Maosong Sun</li>
<li>for: 本研究旨在提出一种有效的训练方法，以便在低资源语言中训练大型多modal模型。</li>
<li>methods: 本研究使用的方法是基于强大的多语言大型语言模型，将英语Only的图像文本数据使用零 shot学习 transferred to other languages，并 achieved state-of-the-art performance in Chinese。</li>
<li>results: 研究表明，基于英语Only的图像文本数据进行零 shot学习 transferred to other languages，可以在多语言多modal learning中取得优秀的表现，并在中文场景中达到了开源最佳性能。<details>
<summary>Abstract</summary>
Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a zero-shot manner for both image-to-text and text-to-image generation, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in image-to-text and text-to-image generation, which achieve state-of-the-art (open-source) performance in Chinese. To facilitate future research, we open-source codes and model weights at https://github.com/OpenBMB/VisCPM.git.
</details>
<details>
<summary>摘要</summary>
近期 Multimodal 学习在图像到文本和文本到图像生成方面发生了明显的增长，但成功通常受限于英语，其他语言剩下来。建立竞争力强的对手在其他语言是非常困难，因为非英语多模态数据的资源短缺（即图像-文本数据的大规模高质量数据缺乏）。在这项工作中，我们提出了 MPM，一种有效的训练方法，用于在低资源语言中训练大型多模态模型。MPM表明，多语言语言模型可以在零shot多模态学习中作为中转站。具体来说，基于一个强大的多语言大语言模型，我们在英语只有图像-文本数据上进行预训练，然后在零shot情况下，我们的多模态模型可以很好地泛化到其他语言，包括图像-文本生成和文本-图像生成。我们选择中文作为MPM的实践，并在图像-文本和文本-图像生成方面建立了 VisCPM 大型多模态模型，其性能与开源数据集中的状态机器达到了领先水平。为便于未来的研究，我们将模型权重和代码开源在 GitHub 上，请参考 <https://github.com/OpenBMB/VisCPM.git>。
</details></li>
</ul>
<hr>
<h2 id="From-Quantity-to-Quality-Boosting-LLM-Performance-with-Self-Guided-Data-Selection-for-Instruction-Tuning"><a href="#From-Quantity-to-Quality-Boosting-LLM-Performance-with-Self-Guided-Data-Selection-for-Instruction-Tuning" class="headerlink" title="From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning"></a>From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12032">http://arxiv.org/abs/2308.12032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, Jing Xiao</li>
<li>for: 提高 Large Language Model 的优化效率和资源利用率</li>
<li>methods: 自动从开源数据集中选择 “cherry” 样本，使用 Instruction-Following Difficulty 指标对模型自动生成能力进行评估</li>
<li>results: 在 Alpaca 和 WizardLM 等著名数据集上实践 Validation 结果显示，只使用 10% 的传统数据输入，我们的策略可以 дости到更好的结果<details>
<summary>Abstract</summary>
In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both efficiency and resource-conscious advancements.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Knowledge-injected-Prompt-Learning-for-Chinese-Biomedical-Entity-Normalization"><a href="#Knowledge-injected-Prompt-Learning-for-Chinese-Biomedical-Entity-Normalization" class="headerlink" title="Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization"></a>Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12025">http://arxiv.org/abs/2308.12025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songhua Yang, Chenghao Zhang, Hongfei Xu, Yuxiang Jia<br>for:这个论文的目的是提高生物医学数据的一致性，通过将raw的医学实体规范化为标准实体，以便更好地应用医学应用程序。methods:该论文提出了一种新的知识注入推断（PL-Knowledge）方法，具体来说是一种五个阶段的方法：候选实体匹配、知识提取、知识编码、知识注入和预测输出。该方法通过有效地编码医学实体中含义的知识项并将其 integrate into我们自己的定制知识注入模板，以提高模型捕捉医学实体之间的潜在关系，从而更好地匹配标准实体。results:该论文对一个 benchmark 数据集进行了广泛的评估，并在 few-shot 和 full-scale  scenarios 中比较了现有的基eline。结果表明，我们的方法在 few-shot enario中平均提高了12.96%的准确率，而在 full-data enario中平均提高了0.94%的准确率，这都证明了我们的方法在 BEN 任务中的优秀性。<details>
<summary>Abstract</summary>
The Biomedical Entity Normalization (BEN) task aims to align raw, unstructured medical entities to standard entities, thus promoting data coherence and facilitating better downstream medical applications. Recently, prompt learning methods have shown promising results in this task. However, existing research falls short in tackling the more complex Chinese BEN task, especially in the few-shot scenario with limited medical data, and the vast potential of the external medical knowledge base has yet to be fully harnessed. To address these challenges, we propose a novel Knowledge-injected Prompt Learning (PL-Knowledge) method. Specifically, our approach consists of five stages: candidate entity matching, knowledge extraction, knowledge encoding, knowledge injection, and prediction output. By effectively encoding the knowledge items contained in medical entities and incorporating them into our tailor-made knowledge-injected templates, the additional knowledge enhances the model's ability to capture latent relationships between medical entities, thus achieving a better match with the standard entities. We extensively evaluate our model on a benchmark dataset in both few-shot and full-scale scenarios. Our method outperforms existing baselines, with an average accuracy boost of 12.96\% in few-shot and 0.94\% in full-data cases, showcasing its excellence in the BEN task.
</details>
<details>
<summary>摘要</summary>
文本翻译：生物医学实体 Normalization（BEN）任务的目标是将原始、未结构化医学实体与标准实体进行对应，从而提高数据准确性并促进下游医学应用。现在，提前学习方法在这个任务中已经显示出了promising的结果。然而，现有的研究仍然缺乏在中文BEN任务中更加复杂的挑战，特别是在有限的医学数据下的少量学习情况下，以及外部医学知识库的庞大潜力尚未得到完全利用。为了解决这些挑战，我们提出了一种新的知识注入推理（PL-Knowledge）方法。具体来说，我们的方法包括以下五个阶段：候选实体匹配、知识提取、知识编码、知识注入和预测输出。通过有效地编码医学实体中包含的知识项和将其注入到我们自定义的知识注入模板中，我们可以使得模型更好地捕捉医学实体之间的潜在关系，从而实现更好的匹配标准实体。我们在一个标准 benchmark dataset 上进行了广泛的评估，并在少量学习和全量数据两种情况下进行了比较。我们的方法在少量学习情况下平均提高了12.96％，而在全量数据情况下平均提高了0.94％，这显示了我们在BEN任务中的优秀表现。
</details></li>
</ul>
<hr>
<h2 id="Reranking-Passages-with-Coarse-to-Fine-Neural-Retriever-using-List-Context-Information"><a href="#Reranking-Passages-with-Coarse-to-Fine-Neural-Retriever-using-List-Context-Information" class="headerlink" title="Reranking Passages with Coarse-to-Fine Neural Retriever using List-Context Information"></a>Reranking Passages with Coarse-to-Fine Neural Retriever using List-Context Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12022">http://arxiv.org/abs/2308.12022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyin Zhu</li>
<li>for: 提高大规模文档中答案选取的精度</li>
<li>methods: 利用列Context注意力机制增强文段表示，并将列Context模型分解成两个子过程，以提高效率</li>
<li>results: 实验表明提出的方法有效地提高了答案选取的精度<details>
<summary>Abstract</summary>
Passage reranking is a crucial task in many applications, particularly when dealing with large-scale documents. Traditional neural architectures are limited in retrieving the best passage for a question because they usually match the question to each passage separately, seldom considering contextual information in other passages that can provide comparison and reference information. This paper presents a list-context attention mechanism to augment the passage representation by incorporating the list-context information from other candidates. The proposed coarse-to-fine (C2F) neural retriever addresses the out-of-memory limitation of the passage attention mechanism by dividing the list-context modeling process into two sub-processes, allowing for efficient encoding of context information from a large number of candidate answers. This method can be generally used to encode context information from any number of candidate answers in one pass. Different from most multi-stage information retrieval architectures, this model integrates the coarse and fine rankers into the joint optimization process, allowing for feedback between the two layers to update the model simultaneously. Experiments demonstrate the effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Graecia-capta-ferum-victorem-cepit-Detecting-Latin-Allusions-to-Ancient-Greek-Literature"><a href="#Graecia-capta-ferum-victorem-cepit-Detecting-Latin-Allusions-to-Ancient-Greek-Literature" class="headerlink" title="Graecia capta ferum victorem cepit. Detecting Latin Allusions to Ancient Greek Literature"></a>Graecia capta ferum victorem cepit. Detecting Latin Allusions to Ancient Greek Literature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12008">http://arxiv.org/abs/2308.12008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederick Riemenschneider, Anette Frank</li>
<li>for: 本研究旨在开发一种适用于古希腊和拉丁文学研究的多语言BERT模型，以便自动发现古希腊和拉丁文本之间的文本相似性。</li>
<li>methods: 本研究使用了一种多语言RoBERTa模型，并通过自动将英文文本翻译成古希腊文本来生成新的训练数据。</li>
<li>results: 研究表明，SPhilBERTa模型在跨语言语义理解和找到古希腊和拉丁文本中相同的句子方面表现出色，并可以自动检测古希腊和拉丁文本之间的文本相似性。<details>
<summary>Abstract</summary>
Intertextual allusions hold a pivotal role in Classical Philology, with Latin authors frequently referencing Ancient Greek texts. Until now, the automatic identification of these intertextual references has been constrained to monolingual approaches, seeking parallels solely within Latin or Greek texts. In this study, we introduce SPhilBERTa, a trilingual Sentence-RoBERTa model tailored for Classical Philology, which excels at cross-lingual semantic comprehension and identification of identical sentences across Ancient Greek, Latin, and English. We generate new training data by automatically translating English texts into Ancient Greek. Further, we present a case study, demonstrating SPhilBERTa's capability to facilitate automated detection of intertextual parallels. Our models and resources are available at https://github.com/Heidelberg-NLP/ancient-language-models.
</details>
<details>
<summary>摘要</summary>
古典文学中的文本相互参照占据着重要地位，拉丁作家 часто参照古希腊文本。在本研究中，我们引入SPhilBERTa，一种适用于古典文学的三语句子BERT模型，能够强大地捕捉跨语言semantic comprehension和identical sentences的同义 sentences。我们生成了新的训练数据，通过自动将英文文本翻译成古希腊语。此外，我们还提供了一个案例研究，证明SPhilBERTa能够自动检测文本相互参照。我们的模型和资源可以在https://github.com/Heidelberg-NLP/ancient-language-models中找到。
</details></li>
</ul>
<hr>
<h2 id="EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE"><a href="#EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE" class="headerlink" title="EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE"></a>EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11971">http://arxiv.org/abs/2308.11971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Chen, Longteng Guo, Jia Sun, Shuai Shao, Zehuan Yuan, Liang Lin, Dongyu Zhang</li>
<li>for: 本研究旨在开发一种可扩展的视觉语言模型，以便从多Modal的数据中学习。</li>
<li>methods: 本研究使用了一种名为EVE的高效的视觉语言基础模型，该模型使用了一个共享的Transformer网络，并将视觉和语言编码在一起。具体来说，EVE使用了一种模态感知的零噪Module，以捕捉不同的感知信息。</li>
<li>results: 本研究表明，EVE可以快速地在训练过程中进行训练，并且在多种视觉语言下沉淀任务中表现出色，包括视觉问答、视觉理解和图像文本检索等。<details>
<summary>Abstract</summary>
Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy to scale up, enabling better downstream performance with fewer resources and faster training speed. Despite its simplicity, EVE achieves state-of-the-art performance on various vision-language downstream tasks, including visual question answering, visual reasoning, and image-text retrieval.
</details>
<details>
<summary>摘要</summary>
建立可扩展的视觉语言模型，从多Modal的数据中学习，仍然是一个开放的挑战。在这篇论文中，我们介绍了一个高效的视觉语言基础模型，称为EVE，它是一个共享Transformer网络中的一个多modal Mixture-of-Experts（MoE）模块，可以同时处理视觉信息。特别是，EVE通过在不同专家中选择性地 switching来捕捉不同的modal信息。为了统一视觉和语言预训练任务，EVE在图像文本对中进行遮盖信号模型，即图像像素和文本符号的重建。这种简单 yet 有效的预训练目标可以加速训练，比Image-Text Contrastive和Image-Text Matching损失快3.5倍。由于EVE的共享架构和预训练任务的组合，它可以轻松扩展，以便在更多的资源和更快的训练速度下 достичь更好的下游性能。尽管其简单，EVE可以达到视觉语言下游任务的状态码性能。
</details></li>
</ul>
<hr>
<h2 id="Audio-Generation-with-Multiple-Conditional-Diffusion-Model"><a href="#Audio-Generation-with-Multiple-Conditional-Diffusion-Model" class="headerlink" title="Audio Generation with Multiple Conditional Diffusion Model"></a>Audio Generation with Multiple Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11940">http://arxiv.org/abs/2308.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifang Guo, Jianguo Mao, Rui Tao, Long Yan, Kazushige Ouchi, Hong Liu, Xiangdong Wang</li>
<li>for: 提高现有预训练文本到Audio模型的可控性，使其能够更好地控制音频的时间顺序、抑噪和音高。</li>
<li>methods: 提出一种新的模型，将额外的内容（时间戳）和风格（抑噪和音高）作为文本模型的补充条件，以提高音频生成的可控性。使用可调式控制条件编码器和Fusion-Net将额外条件编码并融合到文本模型中，保持预训练模型的权重冰结。</li>
<li>results: 实验结果表明，我们的模型成功实现了细致的控制，以达到可控的音频生成。<details>
<summary>Abstract</summary>
Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a series of evaluation metrics to evaluate the controllability performance. Experimental results demonstrate that our model successfully achieves fine-grained control to accomplish controllable audio generation. Audio samples and our dataset are publicly available at https://conditionaudiogen.github.io/conditionaudiogen/
</details>
<details>
<summary>摘要</summary>
文本基于的音频生成模型存在限制，因为它们无法包含所有音频信息，导致仅仅基于文本的控制性有限。为解决这个问题，我们提出了一种新的模型，它可以增强现有的预训练文本到Audio模型的控制性，通过添加内容（时间戳）和风格（折射和能量折射）等补充条件。这种方法可以实现精细的控制时间顺序、折射和能量等 audio 生成的属性。为保持生成的多样性，我们使用可训练的控制条件编码器，并使用大语言模型和可训练的融合网来编码和融合更多的条件，而不论预训练的文本到Audio模型的 weights 保持冻结。由于缺乏适合的数据集和评价指标，我们将现有数据集合并成一个新的数据集，并使用一系列的评价指标来评估控制性性能。实验结果表明，我们的模型成功实现了精细的控制，以完成可控的 audio 生成。音频样本和我们的数据集可以在 <https://conditionaudiogen.github.io/conditionaudiogen/> 上公开获取。
</details></li>
</ul>
<hr>
<h2 id="Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement"><a href="#Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement" class="headerlink" title="Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement"></a>Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11923">http://arxiv.org/abs/2308.11923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino</li>
<li>for: 本研究旨在描述输入对应的 Audio Clip 之间的semantic difference，而不是只是描述它们的同义性。</li>
<li>methods: 本研究提出了 Audio Difference Captioning (ADC) 任务，使用了 cross-attention-concentrated transformer encoder 抽取对比两个 Audio Clip 的差异，并使用了 similarity-discrepancy disentanglement 强调在latent space中提取差异。</li>
<li>results: 实验表明，提出的方法可以有效地解决 ADC 任务，并使得 transformer encoder 中的注意力权重更加集中在差异EXTRACTION 上。<details>
<summary>Abstract</summary>
We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.
</details>
<details>
<summary>摘要</summary>
我们提出了听音差异描述（ADC）作为audio描述的新扩展任务，用于描述输入对的类似 yet slightly different 听音clip之间的semantic差异。ADC解决了传统的听音描述经常生成类似的描述，而不能描述听音clip之间的内容差异。我们还提出了一种基于cross-attention的transformer编码器，用于比较一对听音clip，并且提出了一种similarity-discrepancy拟合来强调在latent space中的差异。为评估我们提出的方法，我们建立了AudioDiffCaps dataset，该dataset包含了类似 yet slightly different 的听音clip，以及人工标注了这些差异的描述。实验结果表明，我们的方法能够有效解决ADC任务，并且可以在transformer编码器中更好地强调差异。
</details></li>
</ul>
<hr>
<h2 id="Diagnosing-Infeasible-Optimization-Problems-Using-Large-Language-Models"><a href="#Diagnosing-Infeasible-Optimization-Problems-Using-Large-Language-Models" class="headerlink" title="Diagnosing Infeasible Optimization Problems Using Large Language Models"></a>Diagnosing Infeasible Optimization Problems Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12923">http://arxiv.org/abs/2308.12923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Gonzalo E. Constante-Flores, Can Li</li>
<li>for: 这篇论文是为了帮助决策问题的解决带来了帮助，通过使用自然语言对话系统来解释无法满足的优化模型。</li>
<li>methods: 这篇论文使用了GPT-4和优化解决方案来识别优化模型中的不可能性源，并提供了一些减少不可能性的建议。</li>
<li>results: 实验表明，使用OptiChat可以帮助专家和非专家用户更好地理解优化模型，快速地找到优化模型中的不可能性源。<details>
<summary>Abstract</summary>
Decision-making problems can be represented as mathematical optimization models, finding wide applications in fields such as economics, engineering and manufacturing, transportation, and health care. Optimization models are mathematical abstractions of the problem of making the best decision while satisfying a set of requirements or constraints. One of the primary barriers to deploying these models in practice is the challenge of helping practitioners understand and interpret such models, particularly when they are infeasible, meaning no decision satisfies all the constraints. Existing methods for diagnosing infeasible optimization models often rely on expert systems, necessitating significant background knowledge in optimization. In this paper, we introduce OptiChat, a first-of-its-kind natural language-based system equipped with a chatbot GUI for engaging in interactive conversations about infeasible optimization models. OptiChat can provide natural language descriptions of the optimization model itself, identify potential sources of infeasibility, and offer suggestions to make the model feasible. The implementation of OptiChat is built on GPT-4, which interfaces with an optimization solver to identify the minimal subset of constraints that render the entire optimization problem infeasible, also known as the Irreducible Infeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought, key-retrieve, and sentiment prompts to enhance OptiChat's reliability. Our experiments demonstrate that OptiChat assists both expert and non-expert users in improving their understanding of the optimization models, enabling them to quickly identify the sources of infeasibility.
</details>
<details>
<summary>摘要</summary>
决策问题可以表示为数学优化模型，找到广泛应用于经济、工程和生产、交通和医疗等领域。优化模型是决策问题的数学抽象，它的目标是找到满足一系列要求或限制的最佳决策。但是现有的方法用于诊断无法满足限制的优化模型通常需要较高的背景知识。在这篇论文中，我们介绍了OptiChat，一个新的自然语言基于系统，它通过交互对话来描述无法满足限制的优化模型，并提供可能的原因和解决方案。OptiChat的实现基于GPT-4，它与优化解除器结合以确定整个优化问题中的最小不可能集（IIS）。我们使用了少量学习、专家链条思考、关键提取和情感提示来提高OptiChat的可靠性。我们的实验表明，OptiChat可以帮助专家和非专家用户更好地理解优化模型，快速地确定ources of infeasibility。
</details></li>
</ul>
<hr>
<h2 id="Towards-an-On-device-Agent-for-Text-Rewriting"><a href="#Towards-an-On-device-Agent-for-Text-Rewriting" class="headerlink" title="Towards an On-device Agent for Text Rewriting"></a>Towards an On-device Agent for Text Rewriting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11807">http://arxiv.org/abs/2308.11807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Zhu, Yinxiao Liu, Felix Stahlberg, Shankar Kumar, Yu-hui Chen, Liangchen Luo, Lei Shu, Renjie Liu, Jindong Chen, Lei Meng</li>
<li>for: 这篇论文是为了开发一个轻量级的语言模型（LLM），用于在设备上进行文本重写。</li>
<li>methods: 作者提出了一种新的指令优化方法，以生成高质量的训练数据无需人工标注。此外，他们还提出了一种决策回归学习框架，可以大幅提高性能无需偏好数据。</li>
<li>results: 经验表明，作者的在设备上的模型超过了现有的状态艺术LLMs在文本重写任务上的表现，同时具有显著减少的模型大小。此外，他们还提出了一种有效的缓存方法，可以更好地衔接服务器端模型。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated impressive capabilities for text rewriting. Nonetheless, the large sizes of these models make them impractical for on-device inference, which would otherwise allow for enhanced privacy and economical inference. Creating a smaller yet potent language model for text rewriting presents a formidable challenge because it requires balancing the need for a small size with the need to retain the emergent capabilities of the LLM, that requires costly data collection. To address the above challenge, we introduce a new instruction tuning approach for building a mobile-centric text rewriting model. Our strategies enable the generation of high quality training data without any human labeling. In addition, we propose a heuristic reinforcement learning framework which substantially enhances performance without requiring preference data. To further bridge the performance gap with the larger server-side model, we propose an effective approach that combines the mobile rewrite agent with the server model using a cascade. To tailor the text rewriting tasks to mobile scenarios, we introduce MessageRewriteEval, a benchmark that focuses on text rewriting for messages through natural language instructions. Through empirical experiments, we demonstrate that our on-device model surpasses the current state-of-the-art LLMs in text rewriting while maintaining a significantly reduced model size. Notably, we show that our proposed cascading approach improves model performance.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经展示了抽象文本重写的卓越能力。然而，这些大型模型的大小使得在设备上进行推理变得不切实际，这会导致隐私和经济性推理的问题。为了解决这个挑战，我们提出了一种新的指令调整方法，用于在移动设备上建立一个高质量的文本重写模型。我们的策略可以生成高质量的训练数据，而无需人工标注。此外，我们提出了一种归纳学习框架，可以在不需要偏好数据的情况下，大幅提高性能。为了补偿大型服务器端模型的性能差距，我们提出了一种有效的级联方法，将移动重写代理与服务器模型结合使用。为了适应移动设备上的文本重写任务，我们介绍了MessageRewriteEval，一个专门针对文本重写的自然语言指令 benchmark。通过实验证明，我们的在设备上运行的模型可以胜过当前状态的各种LLMs在文本重写任务中，同时具有显著减少的模型大小。此外，我们还证明了我们的归纳方法可以提高模型性能。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Anomaly-Detection-in-Text-with-Deviation-Learning"><a href="#Few-shot-Anomaly-Detection-in-Text-with-Deviation-Learning" class="headerlink" title="Few-shot Anomaly Detection in Text with Deviation Learning"></a>Few-shot Anomaly Detection in Text with Deviation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11780">http://arxiv.org/abs/2308.11780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anindya Sundar Das, Aravind Ajay, Sriparna Saha, Monowar Bhuyan</li>
<li>for: 本文旨在提出一种基于深度几个示例学习的方法，以便利用有限的异常示例来直接学习异常分数，并在整个过程中使用偏移学习来学习异常行为。</li>
<li>methods: 本文使用的方法包括深度几个示例学习、偏移学习和多头自注意力层，以及多个实例学习方法。</li>
<li>results: 经过实验表明，本文提出的方法可以在多个标准 benchmark 数据集上达到新的州OF-the-art性能水平。<details>
<summary>Abstract</summary>
Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably deviate from the reference score in the upper tail of the prior. Additionally, our model is optimized to learn the distinct behavior of anomalies by utilizing a multi-head self-attention layer and multiple instance learning approaches. Comprehensive experiments on several benchmark datasets demonstrate that our proposed approach attains a new level of state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
现有的方法 для检测文本中的异常都集中在建立仅靠无标示资料的模型上。这些模型假设没有具有标示异常的例子存在，这限制了它们使用实际世界应用中通常存在的异常小量知识。此外，这些模型专注于学习特征嵌入而不是直接优化异常分数，这可能导致异常分数不佳和数据学习过程中的数据使用不燥。在这篇论文中，我们介绍了FATE，一个深度几何学习基础架构，它利用有限异常例子来直接学习异常分数，并使用偏差学习方法。在这种方法中，正常示例的异常分数被调整，以接近对待分布中的参考分数。相反，异常示例的异常分数需要与参考分数在Upper tail上有大幅度的偏差。此外，我们的模型还利用多头自我注意和多个实例学习方法来学习异常的特别行为。我们在多个benchmark dataset上进行了充分的实验，结果显示我们的提议方法可以达到新的州立顶点性能。
</details></li>
</ul>
<hr>
<h2 id="Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model"><a href="#Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model" class="headerlink" title="Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model"></a>Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11773">http://arxiv.org/abs/2308.11773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuezhou Zhang, Amos A Folarin, Judith Dineley, Pauline Conde, Valeria de Angel, Shaoxiong Sun, Yatharth Ranjan, Zulqarnain Rashid, Callum Stewart, Petroula Laiou, Heet Sankesara, Linglong Qian, Faith Matcham, Katie M White, Carolin Oetzmann, Femke Lamers, Sara Siddi, Sara Simblett, Björn W. Schuller, Srinivasan Vairavan, Til Wykes, Josep Maria Haro, Brenda WJH Penninx, Vaibhav A Narayan, Matthew Hotopf, Richard JB Dobson, Nicholas Cummins, RADAR-CNS consortium</li>
<li>for: 这个研究是为了检测听力语言与抑郁的关系，并采用了大规模验证的方法。</li>
<li>methods: 这个研究使用了自然语言处理技术，特别是BERTopic模型，对3919个手机采集的语音记录进行分析，并从中提取了29个话题。</li>
<li>results: 研究发现，患有抑郁的人更容易提到“没有期望”、“睡眠”、“心理治疗”、“头发”、“学习”和“课程”等话题，这些话题可能是抑郁的指标。此外，研究还发现了语言使用和行为特征之间的相关性，以及语言使用的变化和抑郁程度之间的相关性。<details>
<summary>Abstract</summary>
Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested the BERTopic model on a similar smaller dataset (356 speech recordings from 57 participants), obtaining some consistent results. In summary, our findings demonstrate specific speech topics may indicate depression severity. The presented data-driven workflow provides a practical approach to collecting and analyzing large-scale speech data from real-world settings for digital health research.
</details>
<details>
<summary>摘要</summary>
研究表明语言使用与抑郁有相关性，但大规模验证还需要进行。传统方法如临床研究过于昂贵。因此，人工智能技术在社交媒体上进行语言预测，但存在限制：无效验证标签、偏向用户样本和无Context。我们的研究在265名参与者的3919则语音记录中发现了29个话题，使用Whisper工具和BERTopic模型。6个话题的中值PHQ-8大于或等于10被视为抑郁风险话题：无期望、睡眠、心理治疗、剪发、学习和课程。为了详细描述话题的出现和与抑郁相关性，我们比较了语音和行为特征。我们还 investigate了话题变化和抑郁严重度的时间变化的相关性，表明重要监测语言使用的长期变化。此外，我们在相似的小数据集上测试了BERTopic模型，获得了一些一致的结果。总之，我们的发现表明特定的语音话题可能指示抑郁严重度。我们提供的数据驱动的工作流程为数字健康研究提供了实用的方法。
</details></li>
</ul>
<hr>
<h2 id="StoryBench-A-Multifaceted-Benchmark-for-Continuous-Story-Visualization"><a href="#StoryBench-A-Multifaceted-Benchmark-for-Continuous-Story-Visualization" class="headerlink" title="StoryBench: A Multifaceted Benchmark for Continuous Story Visualization"></a>StoryBench: A Multifaceted Benchmark for Continuous Story Visualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11606">http://arxiv.org/abs/2308.11606</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google/storybench">https://github.com/google/storybench</a></li>
<li>paper_authors: Emanuele Bugliarello, Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan, Vittorio Ferrari, Pieter-Jan Kindermans, Paul Voigtlaender</li>
<li>for: 这个论文的目的是提出一个新的多任务Benchmark，用于评估未来的文本到视频模型。</li>
<li>methods: 这个论文使用了三个视频生成任务，包括行动执行、续写故事和故事生成。它还使用了人类标注来评估模型的性能。</li>
<li>results: 研究人员通过使用这些任务和人类标注，证明了小 yet 强的文本到视频基eline的好处。此外，他们还提出了一种新的评估方法，以便更好地评估视频生成模型的性能。<details>
<summary>Abstract</summary>
Generating video stories from text prompts is a complex task. In addition to having high visual quality, videos need to realistically adhere to a sequence of text prompts whilst being consistent throughout the frames. Creating a benchmark for video generation requires data annotated over time, which contrasts with the single caption used often in video datasets. To fill this gap, we collect comprehensive human annotations on three existing datasets, and introduce StoryBench: a new, challenging multi-task benchmark to reliably evaluate forthcoming text-to-video models. Our benchmark includes three video generation tasks of increasing difficulty: action execution, where the next action must be generated starting from a conditioning video; story continuation, where a sequence of actions must be executed starting from a conditioning video; and story generation, where a video must be generated from only text prompts. We evaluate small yet strong text-to-video baselines, and show the benefits of training on story-like data algorithmically generated from existing video captions. Finally, we establish guidelines for human evaluation of video stories, and reaffirm the need of better automatic metrics for video generation. StoryBench aims at encouraging future research efforts in this exciting new area.
</details>
<details>
<summary>摘要</summary>
生成视频故事从文本提示是一个复杂的任务。除了具有高质量的视觉外，视频还需要在文本提示的时间序列中准确遵循，并在帧中保持一致。为了填补这个空白，我们收集了大量人类标注数据，并引入了StoryBench：一个新的、挑战性的多任务 bench mark，用于可靠地评估未来的文本到视频模型。我们的benchmark包括三个视频生成任务：行动执行、故事续写和故事生成。我们评估了一些小 yet 强大的文本到视频基线，并显示了使用 Algorithmically 生成的故事数据的好处。最后，我们确立了人类评估视频故事的指南，并重申了自动度量的改进。StoryBench 的目标是鼓励未来的研究努力在这一新领域。
</details></li>
</ul>
<hr>
<h2 id="SeamlessM4T-Massively-Multilingual-Multimodal-Machine-Translation"><a href="#SeamlessM4T-Massively-Multilingual-Multimodal-Machine-Translation" class="headerlink" title="SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation"></a>SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11596">http://arxiv.org/abs/2308.11596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/seamless_communication">https://github.com/facebookresearch/seamless_communication</a></li>
<li>paper_authors: Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-jussà, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzmán, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang</li>
<li>for: 该研究目的是创建一个能够将任何两种语言的语音翻译成另一种语言的工具，即babel fish。</li>
<li>methods: 该研究使用了100种语言的自动对时抽象的语音数据，并使用了w2v-BERT 2.0来学习自我监督的语音表示。然后，他们创建了一个多Modal的译文库，并将其与人工标注和 Pseudo标注数据进行了混合。</li>
<li>results: 该研究实现了一个可以同时支持语音译文、文本译文、语音识别和文本识别的多语言模型，可以在100种语言之间进行同时翻译。相比之前的最佳实现，该模型在FLEURS上实现了20%的BLEU提升，在直接语音译文任务上实现了1.3个BLEU点的提升，在语音译文任务上实现了2.6个ASR-BLEU点的提升。此外，该模型在干扰背景和 speaker变化的情况下也表现更加稳定。<details>
<summary>Abstract</summary>
What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages? While recent breakthroughs in text-based models have pushed machine translation coverage beyond 200 languages, unified speech-to-speech translation models have yet to achieve similar strides. More specifically, conventional speech-to-speech translation systems rely on cascaded systems that perform translation progressively, putting high-performing unified systems out of reach. To address these gaps, we introduce SeamlessM4T, a single model that supports speech-to-speech translation, speech-to-text translation, text-to-speech translation, text-to-text translation, and automatic speech recognition for up to 100 languages. To build this, we used 1 million hours of open speech audio data to learn self-supervised speech representations with w2v-BERT 2.0. Subsequently, we created a multimodal corpus of automatically aligned speech translations. Filtered and combined with human-labeled and pseudo-labeled data, we developed the first multilingual system capable of translating from and into English for both speech and text. On FLEURS, SeamlessM4T sets a new standard for translations into multiple target languages, achieving an improvement of 20% BLEU over the previous SOTA in direct speech-to-text translation. Compared to strong cascaded models, SeamlessM4T improves the quality of into-English translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in speech-to-speech. Tested for robustness, our system performs better against background noises and speaker variations in speech-to-text tasks compared to the current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and added toxicity to assess translation safety. Finally, all contributions in this work are open-sourced and accessible at https://github.com/facebookresearch/seamless_communication
</details>
<details>
<summary>摘要</summary>
To build this, we used 1 million hours of open speech audio data to learn self-supervised speech representations with w2v-BERT 2.0. Subsequently, we created a multimodal corpus of automatically aligned speech translations. Filtered and combined with human-labeled and pseudo-labeled data, we developed the first multilingual system capable of translating from and into English for both speech and text.On FLEURS, SeamlessM4T sets a new standard for translations into multiple target languages, achieving an improvement of 20% BLEU over the previous SOTA in direct speech-to-text translation. Compared to strong cascaded models, SeamlessM4T improves the quality of into-English translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in speech-to-speech. Tested for robustness, our system performs better against background noises and speaker variations in speech-to-text tasks compared to the current SOTA model.Critically, we evaluated SeamlessM4T on gender bias and added toxicity to assess translation safety. Finally, all contributions in this work are open-sourced and accessible at https://github.com/facebookresearch/seamless_communication.
</details></li>
</ul>
<hr>
<h2 id="Using-ChatGPT-as-a-CAT-tool-in-Easy-Language-translation"><a href="#Using-ChatGPT-as-a-CAT-tool-in-Easy-Language-translation" class="headerlink" title="Using ChatGPT as a CAT tool in Easy Language translation"></a>Using ChatGPT as a CAT tool in Easy Language translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11563">http://arxiv.org/abs/2308.11563</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/katjakaterina/chatgpt4easylang">https://github.com/katjakaterina/chatgpt4easylang</a></li>
<li>paper_authors: Silvana Deilen, Sergio Hernández Garrido, Ekaterina Lapshinova-Koltunski, Christiane Maaß</li>
<li>for:  investigate the feasibility of using ChatGPT to translate citizen-oriented administrative texts into German Easy Language</li>
<li>methods: use ChatGPT to translate selected texts from websites of German public authorities using two strategies, i.e. linguistic and holistic</li>
<li>results: the generated texts are easier than the standard texts, but still do not fully meet the established Easy Language standards, and the content is not always rendered correctly.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;<details>
<summary>Abstract</summary>
This study sets out to investigate the feasibility of using ChatGPT to translate citizen-oriented administrative texts into German Easy Language, a simplified, controlled language variety that is adapted to the needs of people with reading impairments. We use ChatGPT to translate selected texts from websites of German public authorities using two strategies, i.e. linguistic and holistic. We analyse the quality of the generated texts based on different criteria, such as correctness, readability, and syntactic complexity. The results indicated that the generated texts are easier than the standard texts, but that they still do not fully meet the established Easy Language standards. Additionally, the content is not always rendered correctly.
</details>
<details>
<summary>摘要</summary>
这项研究旨在探讨使用ChatGPT来翻译公民面向的行政文本into German Easy Language，一种简化、控制的语言变体，适应Allemagne人读取障碍者的需求。我们使用ChatGPT翻译选择的公共机构网站上的文本，使用两种策略，即语言和整体。我们分析生成的文本质量，包括正确性、可读性和 синтакситиче复杂性等多个标准。结果表明，生成的文本比标准文本更易读，但并不完全符合Established Easy Language标准。此外，内容并不总是正确地表达。
</details></li>
</ul>
<hr>
<h2 id="BELB-a-Biomedical-Entity-Linking-Benchmark"><a href="#BELB-a-Biomedical-Entity-Linking-Benchmark" class="headerlink" title="BELB: a Biomedical Entity Linking Benchmark"></a>BELB: a Biomedical Entity Linking Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11537">http://arxiv.org/abs/2308.11537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sg-wbi/belb-exp">https://github.com/sg-wbi/belb-exp</a></li>
<li>paper_authors: Samuele Garda, Leon Weber-Genzel, Robert Martin, Ulf Leser</li>
<li>for: 本研究旨在提供一个 Biomedical Entity Linking（BEL） benchmark，以测试不同系统在多个 corpora 上的性能。</li>
<li>methods: 本研究使用了不同的方法，包括rule-based系统和基于预训练语言模型的 neural方法。</li>
<li>results: 研究结果显示，基于预训练语言模型的 neural方法在不同的entity type上表现不一致， highlighting the need of further studies towards entity-agnostic models。<details>
<summary>Abstract</summary>
Biomedical entity linking (BEL) is the task of grounding entity mentions to a knowledge base. It plays a vital role in information extraction pipelines for the life sciences literature. We review recent work in the field and find that, as the task is absent from existing benchmarks for biomedical text mining, different studies adopt different experimental setups making comparisons based on published numbers problematic. Furthermore, neural systems are tested primarily on instances linked to the broad coverage knowledge base UMLS, leaving their performance to more specialized ones, e.g. genes or variants, understudied. We therefore developed BELB, a Biomedical Entity Linking Benchmark, providing access in a unified format to 11 corpora linked to 7 knowledge bases and spanning six entity types: gene, disease, chemical, species, cell line and variant. BELB greatly reduces preprocessing overhead in testing BEL systems on multiple corpora offering a standardized testbed for reproducible experiments. Using BELB we perform an extensive evaluation of six rule-based entity-specific systems and three recent neural approaches leveraging pre-trained language models. Our results reveal a mixed picture showing that neural approaches fail to perform consistently across entity types, highlighting the need of further studies towards entity-agnostic models.
</details>
<details>
<summary>摘要</summary>
生物医学实体链接（BEL）是将实体提及链接到知识库的任务。它在生物医学文献抽取管道中扮演着重要的角色。我们回顾了领域的最新研究，发现现有的生物医学文献检索标准不包含BEL任务，不同的研究采用不同的实验设置，使得基于已发布的数字进行比较困难。另外，神经系统主要在使用UMLS广泛覆盖知识库下进行测试，忽略了更专业的实体类型，例如基因或变异。为解决这一问题，我们开发了BELB，一个生物医学实体链接准则，提供11个 корпу和7个知识库的联合格式，涵盖6种实体类型：基因、疾病、化学物质、物种、细胞系和变异。BELB可以减少测试BEL系统的前处理开销，提供标准化的测试床，为可重复的实验提供了一个统一的格式。使用BELB，我们进行了广泛的BEL系统和神经网络方法的评估，结果表明，神经网络方法在不同的实体类型上表现不一致，需要进一步的研究以实现实体无关的模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/cs.CL_2023_08_23/" data-id="clltaagmy0029r888dlup4527" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.CV_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.CV_2023_08_23/">cs.CV - 2023-08-23 21:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CIParsing-Unifying-Causality-Properties-into-Multiple-Human-Parsing"><a href="#CIParsing-Unifying-Causality-Properties-into-Multiple-Human-Parsing" class="headerlink" title="CIParsing: Unifying Causality Properties into Multiple Human Parsing"></a>CIParsing: Unifying Causality Properties into Multiple Human Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12218">http://arxiv.org/abs/2308.12218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaojia Chen, Xuanhan Wang, Lianli Gao, Beitao Chen, Jingkuan Song, HenTao Shen</li>
<li>for: 提高多个人分割（MHP）模型的泛化能力和鲁棒性，使其能够更好地适应不同的图像样式和外部干扰。</li>
<li>methods: 基于 causality 原则，提出了一种新的人分割 paradigma 称为 CIParsing，该 paradigma 假设输入图像是由 causal 因素（人体部分的特征）和非 causal 因素（外部上下文）组成，只有 causal 因素才会导致人分割的生成过程。在这种情况下，人分割器需要构建 latent 表示 causal 因素，并学习保持这些表示符合 causality 原则。</li>
<li>results: 对两个广泛使用的 refer  benchmark 进行了广泛的实验，结果表明，提出的 CIParsing 方法可以提高 MHP 模型的泛化能力和鲁棒性，并且可以适应不同的图像样式和外部干扰。<details>
<summary>Abstract</summary>
Existing methods of multiple human parsing (MHP) apply statistical models to acquire underlying associations between images and labeled body parts. However, acquired associations often contain many spurious correlations that degrade model generalization, leading statistical models to be vulnerable to visually contextual variations in images (e.g., unseen image styles/external interventions). To tackle this, we present a causality inspired parsing paradigm termed CIParsing, which follows fundamental causal principles involving two causal properties for human parsing (i.e., the causal diversity and the causal invariance). Specifically, we assume that an input image is constructed by a mix of causal factors (the characteristics of body parts) and non-causal factors (external contexts), where only the former ones cause the generation process of human parsing.Since causal/non-causal factors are unobservable, a human parser in proposed CIParsing is required to construct latent representations of causal factors and learns to enforce representations to satisfy the causal properties. In this way, the human parser is able to rely on causal factors w.r.t relevant evidence rather than non-causal factors w.r.t spurious correlations, thus alleviating model degradation and yielding improved parsing ability. Notably, the CIParsing is designed in a plug-and-play fashion and can be integrated into any existing MHP models. Extensive experiments conducted on two widely used benchmarks demonstrate the effectiveness and generalizability of our method.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SG-Former-Self-guided-Transformer-with-Evolving-Token-Reallocation"><a href="#SG-Former-Self-guided-Transformer-with-Evolving-Token-Reallocation" class="headerlink" title="SG-Former: Self-guided Transformer with Evolving Token Reallocation"></a>SG-Former: Self-guided Transformer with Evolving Token Reallocation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12216">http://arxiv.org/abs/2308.12216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sucheng Ren, Xingyi Yang, Songhua Liu, Xinchao Wang</li>
<li>for: 提高 vision transformer 的 computation cost 和 parameter number, 以便应用于大型图像处理任务。</li>
<li>methods: 提出了一种新的模型，即 Self-guided Transformer~(SG-Former)，通过自适应的 significancy map 来实现有效的全局自注意。</li>
<li>results: 在 ImageNet-1K 和 CoCo 等任务上达到了现状之最，比如 Swin Transformer 高出 \textbf{+1.3% &#x2F; +2.7 mAP&#x2F; +3 mIoU}，同时具有较低的计算成本和参数数量。<details>
<summary>Abstract</summary>
Vision Transformer has demonstrated impressive success across various vision tasks. However, its heavy computation cost, which grows quadratically with respect to the token sequence length, largely limits its power in handling large feature maps. To alleviate the computation cost, previous works rely on either fine-grained self-attentions restricted to local small regions, or global self-attentions but to shorten the sequence length resulting in coarse granularity. In this paper, we propose a novel model, termed as Self-guided Transformer~(SG-Former), towards effective global self-attention with adaptive fine granularity. At the heart of our approach is to utilize a significance map, which is estimated through hybrid-scale self-attention and evolves itself during training, to reallocate tokens based on the significance of each region. Intuitively, we assign more tokens to the salient regions for achieving fine-grained attention, while allocating fewer tokens to the minor regions in exchange for efficiency and global receptive fields. The proposed SG-Former achieves performance superior to state of the art: our base size model achieves \textbf{84.7\%} Top-1 accuracy on ImageNet-1K, \textbf{51.2mAP} bbAP on CoCo, \textbf{52.7mIoU} on ADE20K surpassing the Swin Transformer by \textbf{+1.3\% / +2.7 mAP/ +3 mIoU}, with lower computation costs and fewer parameters. The code is available at \href{https://github.com/OliverRensu/SG-Former}{https://github.com/OliverRensu/SG-Former}
</details>
<details>
<summary>摘要</summary>
“视Transformer”已经在不同的视觉任务上表现出色。然而，它的计算成本呈 quadratic 关系，即Sequence length的长度，这限制了它在处理大的特征图时的能力。以前的工作通过 either 精细自注意或者缩短序列长度来降低计算成本，但这会导致 coarse 粒度。在这篇论文中，我们提出了一种新的模型，称为自适应Transformer（SG-Former），以实现高效的全球自注意。我们的方法是通过在训练中估算出的 Significance 地图，来调整token的分配，以达到 fine-grained 的自注意。具体来说，我们将更多的token分配给突出的区域，以实现细致的自注意，而同时减少 Token 的分配，以减少计算成本和全球接收器。我们的SG-Former 模型在 ImageNet-1K 上达到了 \textbf{84.7\%} Top-1 准确率，在 CoCo 上达到了 \textbf{51.2mAP} bbAP，在 ADE20K 上达到了 \textbf{52.7mIoU}，超过 Swin Transformer 的 \textbf{+1.3\% / +2.7 mAP / +3 mIoU}，同时具有更低的计算成本和 fewer 参数。代码可以在 \href{https://github.com/OliverRensu/SG-Former}{https://github.com/OliverRensu/SG-Former} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-Real-Time-Analysis-of-Broadcast-Badminton-Videos"><a href="#Towards-Real-Time-Analysis-of-Broadcast-Badminton-Videos" class="headerlink" title="Towards Real-Time Analysis of Broadcast Badminton Videos"></a>Towards Real-Time Analysis of Broadcast Badminton Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12199">http://arxiv.org/abs/2308.12199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nitin Nilesh, Tushar Sharma, Anurag Ghosh, C. V. Jawahar</li>
<li>for: 这个研究旨在实时分析羽毛球比赛中玩家的运动动作。</li>
<li>methods: 该方法使用直播启用的视频输入，并从视频中提取玩家的运动轨迹。</li>
<li>results: 该方法可以实时计算玩家在场上覆盖的距离和速度，以及在场上覆盖的区域。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Analysis of player movements is a crucial subset of sports analysis. Existing player movement analysis methods use recorded videos after the match is over. In this work, we propose an end-to-end framework for player movement analysis for badminton matches on live broadcast match videos. We only use the visual inputs from the match and, unlike other approaches which use multi-modal sensor data, our approach uses only visual cues. We propose a method to calculate the on-court distance covered by both the players from the video feed of a live broadcast badminton match. To perform this analysis, we focus on the gameplay by removing replays and other redundant parts of the broadcast match. We then perform player tracking to identify and track the movements of both players in each frame. Finally, we calculate the distance covered by each player and the average speed with which they move on the court. We further show a heatmap of the areas covered by the player on the court which is useful for analyzing the gameplay of the player. Our proposed framework was successfully used to analyze live broadcast matches in real-time during the Premier Badminton League 2019 (PBL 2019), with commentators and broadcasters appreciating the utility.
</details>
<details>
<summary>摘要</summary>
analysis of player movements 是体育分析中的一个重要子集。现有的玩家运动分析方法使用已经完成的比赛录像。在这项工作中，我们提议一种终端框架 дляBadminton比赛中的玩家运动分析。我们只使用比赛直播中的视觉输入，与其他方法不同，我们的方法不使用多Modal感知数据。我们提出了一种方法来计算比赛直播中Badminton比赛中每个玩家在视频中的场地覆盖距离。为了进行这种分析，我们减去了重播和其他无关的部分，然后对每帧中的玩家运动进行跟踪和识别。最后，我们计算了每个玩家在场地上的平均速度和覆盖距离。此外，我们还显示了每个玩家在场地上的热图，这有助于分析玩家的游戏风格。我们的提议的框架在Premier Badminton League 2019（PBL 2019）中实时分析了直播比赛，评论员和广播员都很欣赏了其有用性。
</details></li>
</ul>
<hr>
<h2 id="Sign-Language-Translation-with-Iterative-Prototype"><a href="#Sign-Language-Translation-with-Iterative-Prototype" class="headerlink" title="Sign Language Translation with Iterative Prototype"></a>Sign Language Translation with Iterative Prototype</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12191">http://arxiv.org/abs/2308.12191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huijie Yao, Wengang Zhou, Hao Feng, Hezhen Hu, Hao Zhou, Houqiang Li</li>
<li>for: 这个论文是为了提出一种简单 yet effective的手语翻译（SLT）框架，即IP-SLT。</li>
<li>methods: 该框架采用迭代结构，通过迭代改进输入手语视频的语义表示（prototype）来提高翻译效果。</li>
<li>results: 实验表明，IP-SLT可以提供更加流畅和合适的翻译结果，并且可以轻松地整合到现有的SLT系统中。<details>
<summary>Abstract</summary>
This paper presents IP-SLT, a simple yet effective framework for sign language translation (SLT). Our IP-SLT adopts a recurrent structure and enhances the semantic representation (prototype) of the input sign language video via an iterative refinement manner. Our idea mimics the behavior of human reading, where a sentence can be digested repeatedly, till reaching accurate understanding. Technically, IP-SLT consists of feature extraction, prototype initialization, and iterative prototype refinement. The initialization module generates the initial prototype based on the visual feature extracted by the feature extraction module. Then, the iterative refinement module leverages the cross-attention mechanism to polish the previous prototype by aggregating it with the original video feature. Through repeated refinement, the prototype finally converges to a more stable and accurate state, leading to a fluent and appropriate translation. In addition, to leverage the sequential dependence of prototypes, we further propose an iterative distillation loss to compress the knowledge of the final iteration into previous ones. As the autoregressive decoding process is executed only once in inference, our IP-SLT is ready to improve various SLT systems with acceptable overhead. Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the IP-SLT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Tumor-Centered-Patching-for-Enhanced-Medical-Image-Segmentation"><a href="#Tumor-Centered-Patching-for-Enhanced-Medical-Image-Segmentation" class="headerlink" title="Tumor-Centered Patching for Enhanced Medical Image Segmentation"></a>Tumor-Centered Patching for Enhanced Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12168">http://arxiv.org/abs/2308.12168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutyyba Asghar, Ahmad Raza Shahid, Akhtar Jamil, Kiran Aftab, Syed Ather Enam</li>
<li>for: 这个研究的目的是提高医疗影像诊断中的像素分类精度，以提高电脑助诊系统的效能。</li>
<li>methods: 这个研究使用了一种新的肿瘤中心的图像分析方法，将图像分割成肿瘤的不同部分，以解决类别不均衡和边界不充分的问题。这种方法可以增强图像特征提取的精度和减少计算负载。</li>
<li>results: 实验结果显示，这个方法可以改善类别不均衡， segmentation scores 为 0.78、0.76 和 0.71  для整体、核心和增强肿瘤，分别。这些结果显示这种方法具有潜力，可以帮助改善医疗影像诊断系统的效能。<details>
<summary>Abstract</summary>
The realm of medical image diagnosis has advanced significantly with the integration of computer-aided diagnosis and surgical systems. However, challenges persist, particularly in achieving precise image segmentation. While deep learning techniques show potential, obstacles like limited resources, slow convergence, and class imbalance impede their effectiveness. Traditional patch-based methods, though common, struggle to capture intricate tumor boundaries and often lead to redundant samples, compromising computational efficiency and feature quality. To tackle these issues, this research introduces an innovative approach centered on the tumor itself for patch-based image analysis. This novel tumor-centered patching method aims to address the class imbalance and boundary deficiencies, enabling focused and accurate tumor segmentation. By aligning patches with the tumor's anatomical context, this technique enhances feature extraction accuracy and reduces computational load. Experimental results demonstrate improved class imbalance, with segmentation scores of 0.78, 0.76, and 0.71 for whole, core, and enhancing tumors, respectively using a lightweight simple U-Net. This approach shows potential for enhancing medical image segmentation and improving computer-aided diagnosis systems.
</details>
<details>
<summary>摘要</summary>
医学影像诊断领域已经具有了深刻的改进，尤其是通过计算机助成诊断和手术系统的结合。然而，困难仍然存在，主要是准确的图像分割。深度学习技术表现出了潜在的优势，但是有限的资源、慢速的融合和类别不均衡问题使其效果受限。传统的贴图方法，虽然广泛使用，但是它们往往难以捕捉肿瘤边界的细节，并且经常产生重复的样本，从而降低计算效率和特征质量。为了解决这些问题，本研究提出了一种推新的肿瘤中心的贴图方法。这种新方法通过将贴图与肿瘤的生物学上的位置进行对应，以提高特征提取精度和减少计算负担。实验结果表明，这种方法可以更好地处理类别不均衡问题，并且 segmentation 分数为0.78、0.76和0.71 для整体、核心和增强肿瘤，分别使用一种轻量级的简单U-Net。这种方法显示出了改善医学影像分割的潜在可能性，并且可能为计算机助成诊断系统提供新的思路。
</details></li>
</ul>
<hr>
<h2 id="NPF-200-A-Multi-Modal-Eye-Fixation-Dataset-and-Method-for-Non-Photorealistic-Videos"><a href="#NPF-200-A-Multi-Modal-Eye-Fixation-Dataset-and-Method-for-Non-Photorealistic-Videos" class="headerlink" title="NPF-200: A Multi-Modal Eye Fixation Dataset and Method for Non-Photorealistic Videos"></a>NPF-200: A Multi-Modal Eye Fixation Dataset and Method for Non-Photorealistic Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12163">http://arxiv.org/abs/2308.12163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyu Yang, Sucheng Ren, Zongwei Wu, Nanxuan Zhao, Junle Wang, Jing Qin, Shengfeng He</li>
<li>for: 这个研究的目的是理解人们如何看到非拟屏视频，以便提高媒体制作、艺术设计和游戏用户体验。</li>
<li>methods: 这个研究使用了一个大规模的多模态数据集（NPF-200），以及一些现有的状态older方法进行比较。同时，研究者还提出了一种包括视觉和声音特征的多模态非拟屏焦点检测模型（NPSNet），以提高模型的性能。</li>
<li>results: 研究结果表明，NPSNet可以达到状态older的性能水平，并且在不同的频谱和多Modal中展示了优异的性能。此外，研究还发现了多模态网络设计和多频域训练的优势和缺陷，为未来的研究提供了丰富的发展方向。<details>
<summary>Abstract</summary>
Non-photorealistic videos are in demand with the wave of the metaverse, but lack of sufficient research studies. This work aims to take a step forward to understand how humans perceive non-photorealistic videos with eye fixation (\ie, saliency detection), which is critical for enhancing media production, artistic design, and game user experience. To fill in the gap of missing a suitable dataset for this research line, we present NPF-200, the first large-scale multi-modal dataset of purely non-photorealistic videos with eye fixations. Our dataset has three characteristics: 1) it contains soundtracks that are essential according to vision and psychological studies; 2) it includes diverse semantic content and videos are of high-quality; 3) it has rich motions across and within videos. We conduct a series of analyses to gain deeper insights into this task and compare several state-of-the-art methods to explore the gap between natural images and non-photorealistic data. Additionally, as the human attention system tends to extract visual and audio features with different frequencies, we propose a universal frequency-aware multi-modal non-photorealistic saliency detection model called NPSNet, demonstrating the state-of-the-art performance of our task. The results uncover strengths and weaknesses of multi-modal network design and multi-domain training, opening up promising directions for future works. {Our dataset and code can be found at \url{https://github.com/Yangziyu/NPF200}}.
</details>
<details>
<summary>摘要</summary>
非摄影实际视频在metaverse潮流中受欢迎，但是研究不充分。这项工作希望通过眼动检测（即眼动检测）来理解人类如何看待非摄影实际视频，这对媒体制作、艺术设计和游戏用户体验都是重要的。为了填补这一研究领域缺乏适用 datasets的问题，我们提供了NPF-200 dataset，这是首个含有声道和多Modal的非摄影实际视频眼动检测 dataset。我们的数据集有以下三个特点：1）它们包含视觉和心理学研究认为是必要的声道；2）它们包含多样化的 semantics 内容和高质量的视频；3）它们具有视频中的丰富运动。我们进行了一系列分析，以获得更深入的理解，并与多种现有方法进行比较，以探索自然图像和非摄影实际数据之间的差距。此外，人类注意系统通常会通过不同频率EXTRACT visual和声音特征，因此我们提出了一种适用于多Modal非摄影实际数据的频率意识非摄影眼动检测模型，称为NPSNet。结果表明NPSNet在这种任务上具有国际前沿性。我们的数据集和代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Mesh-Conflation-of-Oblique-Photogrammetric-Models-using-Virtual-Cameras-and-Truncated-Signed-Distance-Field"><a href="#Mesh-Conflation-of-Oblique-Photogrammetric-Models-using-Virtual-Cameras-and-Truncated-Signed-Distance-Field" class="headerlink" title="Mesh Conflation of Oblique Photogrammetric Models using Virtual Cameras and Truncated Signed Distance Field"></a>Mesh Conflation of Oblique Photogrammetric Models using Virtual Cameras and Truncated Signed Distance Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12139">http://arxiv.org/abs/2308.12139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Song, Rongjun Qin</li>
<li>for: 高精度地表模型的合并 (concatenation of high-resolution digital surface models)</li>
<li>methods: 基于投影相机场的TSDF折叠 (concatenation using Truncated Signed Distance Fields and panoramic cameras)</li>
<li>results: 提高了传统方法的精度和效率 (improved accuracy and efficiency compared to traditional methods)<details>
<summary>Abstract</summary>
Conflating/stitching 2.5D raster digital surface models (DSM) into a large one has been a running practice in geoscience applications, however, conflating full-3D mesh models, such as those from oblique photogrammetry, is extremely challenging. In this letter, we propose a novel approach to address this challenge by conflating multiple full-3D oblique photogrammetric models into a single, and seamless mesh for high-resolution site modeling. Given two or more individually collected and created photogrammetric meshes, we first propose to create a virtual camera field (with a panoramic field of view) to incubate virtual spaces represented by Truncated Signed Distance Field (TSDF), an implicit volumetric field friendly for linear 3D fusion; then we adaptively leverage the truncated bound of meshes in TSDF to conflate them into a single and accurate full 3D site model. With drone-based 3D meshes, we show that our approach significantly improves upon traditional methods for model conflations, to drive new potentials to create excessively large and accurate full 3D mesh models in support of geoscience and environmental applications.
</details>
<details>
<summary>摘要</summary>
合并/缝合2.5D矩阵数字地表模型(DSM)是地球科学应用中常见的做法，但是将全3D网格模型，如从斜视 photogrammetry 获得的模型，则是非常困难的。在这封信中，我们提出了一种新的方法，用于解决这个问题，即将两个或更多个 individually 收集和创建的 photogrammetric 模型进行合并，并生成一个完整、精准的full 3D站点模型。我们首先创建了一个虚拟摄像头场（具有扩展的全景视野），以便在 Truncated Signed Distance Field (TSDF) 中表示虚拟空间，然后利用TSDF中的截断缩限来合并这些模型，并生成一个准确、完整的full 3D站点模型。使用无人机生成的3D模型，我们表明了我们的方法在传统方法上显著提高了模型合并的性能，以支持地球科学和环境应用。
</details></li>
</ul>
<hr>
<h2 id="Select-and-Combine-SAC-A-Novel-Multi-Stereo-Depth-Fusion-Algorithm-for-Point-Cloud-Generation-via-Efficient-Local-Markov-Netlets"><a href="#Select-and-Combine-SAC-A-Novel-Multi-Stereo-Depth-Fusion-Algorithm-for-Point-Cloud-Generation-via-Efficient-Local-Markov-Netlets" class="headerlink" title="Select-and-Combine (SAC): A Novel Multi-Stereo Depth Fusion Algorithm for Point Cloud Generation via Efficient Local Markov Netlets"></a>Select-and-Combine (SAC): A Novel Multi-Stereo Depth Fusion Algorithm for Point Cloud Generation via Efficient Local Markov Netlets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12138">http://arxiv.org/abs/2308.12138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Elhashash, Rongjun Qin</li>
<li>for: 这个论文的目的是提出一种新的深度融合方法，以解决多视图深度匹配中的噪声和不一致问题。</li>
<li>methods: 该方法基于本地Markov Netlets来模型点级融合，通过选择最佳深度图并将其融合成一个清晰的点云。</li>
<li>results: 对比 existed方法，该方法提高了F1分数（考虑准确性和完整性）的提升2.07%，并生成了18%更加简洁的点云，同时保持高度准确性。<details>
<summary>Abstract</summary>
Many practical systems for image-based surface reconstruction employ a stereo/multi-stereo paradigm, due to its ability to scale for large scenes and its ease of implementation for out-of-core operations. In this process, multiple and abundant depth maps from stereo matching must be combined and fused into a single, consistent, and clean point cloud. However, the noises and outliers caused by stereo matching and the heterogenous geometric errors of the poses present a challenge for existing fusion algorithms, since they mostly assume Gaussian errors and predict fused results based on data from local spatial neighborhoods, which may inherit uncertainties from multiple depths resulting in lowered accuracy. In this paper, we propose a novel depth fusion paradigm, that instead of numerically fusing points from multiple depth maps, selects the best depth map per point, and combines them into a single and clean point cloud. This paradigm, called select-and-combine (SAC), is achieved through modeling the point level fusion using local Markov Netlets, a micro-network over point across neighboring views for depth/view selection, followed by a Netlets collapse process for point combination. The Markov Netlets are optimized such that they can inherently leverage spatial consistencies among depth maps of neighboring views, thus they can address errors beyond Gaussian ones. Our experiment results show that our approach outperforms existing depth fusion approaches by increasing the F1 score that considers both accuracy and completeness by 2.07% compared to the best existing method. Finally, our approach generates clearer point clouds that are 18% less redundant while with a higher accuracy before fusion
</details>
<details>
<summary>摘要</summary>
多种实用系统用stereo/多 Sterero paradigm进行图像基于表面重建，因为它可以扩展到大型场景并且实现离核操作。在这个过程中，多个和充沛的深度地图从stereo匹配得到，然后需要将它们合并和融合成一个单一、一致和干净的点云。然而，stereo匹配中的噪声和异常值以及不同视角的姿态差引入的精度问题，对现有的融合算法提出了挑战，因为它们大多数假设 Gaussian 错误并基于本地空间邻域数据预测融合结果，这可能会继承多个深度的不确定性，导致准确性下降。在这篇论文中，我们提出了一种新的深度融合方法，而不是纯数学地融合多个深度地图中的点，而是选择每个点最佳的深度地图，然后将它们融合成一个单一、一致和干净的点云。这种方法，我们称之为选择并融合（SAC）方法。我们通过模型点级融合使用本地Markov Netlets，一种微网络在不同视角的深度/视图之间的点级选择，然后进行Netlets归一化过程来融合点。Markov Netlets是通过优化，使其能够自然地利用邻近视角的深度地图之间的空间一致性，因此它们可以解决超出Gaussian错误的错误。我们的实验结果显示，我们的方法比现有的深度融合方法提高了F1分数，考虑到准确性和完整性的比较，提高了2.07%。此外，我们的方法生成的点云更清晰，同时具有18% less redundancy，而且准确率更高。
</details></li>
</ul>
<hr>
<h2 id="Lite-HRNet-Plus-Fast-and-Accurate-Facial-Landmark-Detection"><a href="#Lite-HRNet-Plus-Fast-and-Accurate-Facial-Landmark-Detection" class="headerlink" title="Lite-HRNet Plus: Fast and Accurate Facial Landmark Detection"></a>Lite-HRNet Plus: Fast and Accurate Facial Landmark Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12133">http://arxiv.org/abs/2308.12133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sota Kato, Kazuhiro Hotta, Yuhki Hatakeyama, Yoshinori Konishi</li>
<li>for: 这个研究是为了提出一个新的抽象方法，以解决现有的颜面特征点检测方法中的computational complexity问题。</li>
<li>methods: 这个方法使用了一个新的混合块，基于通道注意力，以及一个新的输出模组，使用多resolution的特征图像。</li>
<li>results: 这个方法在两个颜面特征检测 dataset 上进行了实验，并证明了它在比较于传统方法更高的精度下，并且在10M FLOPs的计算复杂度范围内实现了顶尖的性能。<details>
<summary>Abstract</summary>
Facial landmark detection is an essential technology for driver status tracking and has been in demand for real-time estimations. As a landmark coordinate prediction, heatmap-based methods are known to achieve a high accuracy, and Lite-HRNet can achieve a fast estimation. However, with Lite-HRNet, the problem of a heavy computational cost of the fusion block, which connects feature maps with different resolutions, has yet to be solved. In addition, the strong output module used in HRNetV2 is not applied to Lite-HRNet. Given these problems, we propose a novel architecture called Lite-HRNet Plus. Lite-HRNet Plus achieves two improvements: a novel fusion block based on a channel attention and a novel output module with less computational intensity using multi-resolution feature maps. Through experiments conducted on two facial landmark datasets, we confirmed that Lite-HRNet Plus further improved the accuracy in comparison with conventional methods, and achieved a state-of-the-art accuracy with a computational complexity with the range of 10M FLOPs.
</details>
<details>
<summary>摘要</summary>
Facial landmark detection 是一种重要的技术，用于车手状态追踪，需要实时估计。为了实现高精度的坐标预测，热图基方法在实时估计中具有高精度。然而，Lite-HRNet中的融合块具有重要的计算成本问题，即将不同分辨率的特征图连接。此外，HRNetV2中使用的强大输出模块没有应用于Lite-HRNet。为了解决这些问题，我们提出了一种新的架构：Lite-HRNet Plus。Lite-HRNet Plus实现了两项改进：一种新的融合块基于通道注意力，以及一种新的输出模块使用多resolution特征图，从而降低计算复杂性。通过在两个 facial landmark 数据集上进行的实验，我们证明了 Lite-HRNet Plus 在比较方法中提高了精度，并在10M FLOPs的计算复杂性范围内达到了状态之精度。
</details></li>
</ul>
<hr>
<h2 id="The-TYC-Dataset-for-Understanding-Instance-Level-Semantics-and-Motions-of-Cells-in-Microstructures"><a href="#The-TYC-Dataset-for-Understanding-Instance-Level-Semantics-and-Motions-of-Cells-in-Microstructures" class="headerlink" title="The TYC Dataset for Understanding Instance-Level Semantics and Motions of Cells in Microstructures"></a>The TYC Dataset for Understanding Instance-Level Semantics and Motions of Cells in Microstructures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12116">http://arxiv.org/abs/2308.12116</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ChristophReich1996/TYC-Dataset">https://github.com/ChristophReich1996/TYC-Dataset</a></li>
<li>paper_authors: Christoph Reich, Tim Prangemeier, Heinz Koeppl</li>
<li>for: 这篇论文的目的是为了提供一个大规模的细胞和微结构批量分割和跟踪的数据集，以便更好地理解细胞的 semantics 和运动。</li>
<li>methods: 这篇论文使用了高分辨率的快速扫描顾密镜像来获取细胞和微结构的批量分割和跟踪数据。</li>
<li>results: 这篇论文发布了105个高分辨率的快速扫描顾密镜像，包括约19000个实例的掩码。此外，还发布了261个 curaated 视频剪辑，包括1293个高分辨率镜像，以便无监督地理解细胞的运动和形态。<details>
<summary>Abstract</summary>
Segmenting cells and tracking their motion over time is a common task in biomedical applications. However, predicting accurate instance-wise segmentation and cell motions from microscopy imagery remains a challenging task. Using microstructured environments for analyzing single cells in a constant flow of media adds additional complexity. While large-scale labeled microscopy datasets are available, we are not aware of any large-scale dataset, including both cells and microstructures. In this paper, we introduce the trapped yeast cell (TYC) dataset, a novel dataset for understanding instance-level semantics and motions of cells in microstructures. We release $105$ dense annotated high-resolution brightfield microscopy images, including about $19$k instance masks. We also release $261$ curated video clips composed of $1293$ high-resolution microscopy images to facilitate unsupervised understanding of cell motions and morphology. TYC offers ten times more instance annotations than the previously largest dataset, including cells and microstructures. Our effort also exceeds previous attempts in terms of microstructure variability, resolution, complexity, and capturing device (microscopy) variability. We facilitate a unified comparison on our novel dataset by introducing a standardized evaluation strategy. TYC and evaluation code are publicly available under CC BY 4.0 license.
</details>
<details>
<summary>摘要</summary>
分 segmenting cells和跟踪其运动过时是生物医学应用中常见任务。然而，准确预测单个单元Instance-wise segmentation和细胞运动从微scopic imaging中remains a challenging task。使用微结构环境 для单元细胞分析在流动媒体中添加了额外复杂性。虽然大规模标注微scopic imaging数据集是可用的，但我们没有发现任何包括细胞和微结构的大规模数据集。在这篇论文中，我们介绍了被陷 yeast cell（TYC）数据集，一个新的数据集用于理解单元级别 semantics和细胞运动。我们发布了105个高分辨率炸einstein imaging microscopy图像，包括约19000个实例涂抹。我们还发布了261个CURATED video clip，包括1293个高分辨率微scopic imaging图像，以便无监督地理解细胞运动和形态。TYC提供了前一次最大的实例标注数量，包括细胞和微结构。我们的努力也超过了之前的尝试，териms of microstructure variability, resolution, complexity, and capturing device（微scopic imaging）variability。我们提出了一种标准化评估策略，以便对我们的新数据集进行一致的比较。TYC和评估代码在CC BY 4.0license下公开可用。
</details></li>
</ul>
<hr>
<h2 id="Less-is-More-–-Towards-parsimonious-multi-task-models-using-structured-sparsity"><a href="#Less-is-More-–-Towards-parsimonious-multi-task-models-using-structured-sparsity" class="headerlink" title="Less is More – Towards parsimonious multi-task models using structured sparsity"></a>Less is More – Towards parsimonious multi-task models using structured sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12114">http://arxiv.org/abs/2308.12114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richa Upadhyay, Ronald Phlypo, Rajkumar Saini, Marcus Liwicki</li>
<li>for: 本研究旨在把结构化群 sparse（group sparsity）引入多任务学习（Multi-Task Learning，MTL）框架中，以开发更简洁、更易理解的模型，同时可以更好地解决多个任务。</li>
<li>methods: 本研究使用了通道级l1&#x2F;l2群 sparse（channel-wise l1&#x2F;l2 group sparsity）在共享层中，这种方法不仅可以消除无关的通道（group），同时也对 weights 加以罚 penalty，从而提高所有任务的学习。</li>
<li>results:  compared with单任务和多任务实验，在group sparse情况下，模型的性能保持在相似或更高水平，而且可以降低模型的内存占用量、计算量和预测时间。此外，研究还发现，随着 sparse度的变化，模型的性能和群的稀畴度具有一定的关系。<details>
<summary>Abstract</summary>
Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups. This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model. Sparsifying the model during training helps decrease the model's memory footprint, computation requirements, and prediction time during inference. We use channel-wise l1/l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN). This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks. We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ. We also investigate how changing the sparsification degree impacts both the performance of the model and the sparsity of groups.
</details>
<details>
<summary>摘要</summary>
（简化中文）机器学习（ML）中的组 sparse 激励 simpler, more interpretable 的模型，具有 fewer active parameter groups。这项工作想要将结构化的组 sparse  integrate into 多任务学习（MTL）框架中，以开发更具有简洁性和可解释性的模型，能够更好地处理多个任务，而且具有更少的参数。在训练过程中，将模型简化可以降低模型的内存占用量、计算需求和预测时间。我们在 convolutional neural network（CNN） 中使用 channel-wise L1/L2 组 sparse，这种方法不仅可以消除无用的组（通道），还对权重进行罚款，从而提高所有任务的学习。我们在 NYU-v2 和 CelebAMask-HQ 两个公开available MTL 数据集上进行单任务和多任务实验，并 investigate 如何更改简化学习度强度对模型性能和组简洁度产生的影响。
</details></li>
</ul>
<hr>
<h2 id="Advancements-in-Point-Cloud-Data-Augmentation-for-Deep-Learning-A-Survey"><a href="#Advancements-in-Point-Cloud-Data-Augmentation-for-Deep-Learning-A-Survey" class="headerlink" title="Advancements in Point Cloud Data Augmentation for Deep Learning: A Survey"></a>Advancements in Point Cloud Data Augmentation for Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12113">http://arxiv.org/abs/2308.12113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinfeng Zhu, Lei Fan, Ningxin Weng</li>
<li>for: This paper focuses on point cloud data augmentation methods for tasks such as detection, segmentation, and classification in computer vision.</li>
<li>methods: The paper surveys and discusses various point cloud data augmentation methods, categorizing them into a taxonomy framework, and evaluates their potentials and limitations.</li>
<li>results: The paper provides a comprehensive understanding of the current status of point cloud data augmentation and suggests possible future research directions, promoting the wider application and development of point cloud processing techniques.<details>
<summary>Abstract</summary>
Point cloud has a wide range of applications in areas such as autonomous driving, mapping, navigation, scene reconstruction, and medical imaging. Due to its great potentials in these applications, point cloud processing has gained great attention in the field of computer vision. Among various point cloud processing techniques, deep learning (DL) has become one of the mainstream and effective methods for tasks such as detection, segmentation and classification. To reduce overfitting during training DL models and improve model performance especially when the amount and/or diversity of training data are limited, augmentation is often crucial. Although various point cloud data augmentation methods have been widely used in different point cloud processing tasks, there are currently no published systematic surveys or reviews of these methods. Therefore, this article surveys and discusses these methods and categorizes them into a taxonomy framework. Through the comprehensive evaluation and comparison of the augmentation methods, this article identifies their potentials and limitations and suggests possible future research directions. This work helps researchers gain a holistic understanding of the current status of point cloud data augmentation and promotes its wider application and development.
</details>
<details>
<summary>摘要</summary>
点云处理具有广泛的应用领域，如自动驾驶、地图建模、导航、场景重建和医疗影像等。由于其在这些应用领域的潜力，点云处理技术在计算机视觉领域得到了广泛的关注。深度学习（DL）已成为点云处理中一种主流和有效的方法，用于任务如检测、分割和分类。在训练DL模型时，以避免过拟合，数据扩展是非常重要。现在，点云数据扩展方法已经广泛应用在不同的点云处理任务中，但是没有发表过系统性的报告或评论。因此，本文对这些方法进行了抽查和讨论，并将其分类到一个分类框架中。通过对各种扩展方法的全面评估和比较，本文可以承认和限制这些方法，并提出可能的未来研究方向。这些研究结果可以帮助研究人员更好地了解当前点云数据扩展的状况，并推动其更广泛的应用和发展。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Continual-Category-Discovery"><a href="#Generalized-Continual-Category-Discovery" class="headerlink" title="Generalized Continual Category Discovery"></a>Generalized Continual Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12112">http://arxiv.org/abs/2308.12112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Marczak, Grzegorz Rypeść, Sebastian Cygert, Tomasz Trzciński, Bartłomiej Twardowski</li>
<li>for: 这个论文的目的是解决常见学习（Continual Learning，CL）方法在实际场景中的限制，即一个学习Agent需要在新任务上学习新的标签数据，而不是忘记之前的知识。</li>
<li>methods: 这个论文提出了一种新的框架，即通用类发现（Generalized Category Discovery，GCD）和常见学习（CL）的结合，用于解决实际场景中的学习问题。具体来说，在任务中，允许存在新的无标签样本和已知类的样本，并使用常见学习方法来发现它们。</li>
<li>results: 实验表明，现有的CL方法在新任务中不能够积累知识，而我们提出的方法可以通过 combining supervised和无标签信号，使得学习Agent可以更好地积累知识并避免忘记。此外，我们的方法也可以在实际场景中表现出优于强CL方法和GCD技术。<details>
<summary>Abstract</summary>
Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of novel classes are present. In light of these limitations, we propose a method that incorporates both supervised and unsupervised signals and mitigates the forgetting through the use of centroid adaptation. Our method surpasses strong CL methods adopted for GCD techniques and presents a superior representation learning performance.
</details>
<details>
<summary>摘要</summary>
大多数持续学习（CL）方法在supervised learning设置下进行推广，其中一个agent需要学习新的标注任务而不忘记之前的知识。然而，这些设置并不适合实际生活中的情景，where a learning agent有大量未标注数据，包括新的类和已知类的示例。 drawing inspiration from Generalized Category Discovery（GCD），我们介绍了一个新的框架，允许在任务中存在新的和已知的类，并且使用 continual version of unsupervised learning方法来发现它们。我们称这种设置为Generalized Continual Category Discovery（GCCD）。它将CL和GCD融合， bridge the gap between synthetic benchmarks和实际生活中的情景。通过一系列实验，我们发现了现有方法在后续任务中处理未标注的新类样本时存在问题，即忘记之前的知识。为了解决这些限制，我们提出了一种方法，该方法将supervised和unsupervised信号相互衔接，并通过中心点修改来减轻忘记。我们的方法超越了强大的CL方法，并在表示学习性能方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Cross-Modality-Proposal-guided-Feature-Mining-for-Unregistered-RGB-Thermal-Pedestrian-Detection"><a href="#Cross-Modality-Proposal-guided-Feature-Mining-for-Unregistered-RGB-Thermal-Pedestrian-Detection" class="headerlink" title="Cross-Modality Proposal-guided Feature Mining for Unregistered RGB-Thermal Pedestrian Detection"></a>Cross-Modality Proposal-guided Feature Mining for Unregistered RGB-Thermal Pedestrian Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12111">http://arxiv.org/abs/2308.12111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Tian, Zikun Zhou, Yuqing Huang, Gaojun Li, Zhenyu He<br>for:这篇论文的目的是提出一种新的不注册RGB-T人体检测方法，以解决实际中RGB-T图像对不匹配的问题。methods:这种方法使用了交叉模态提案导向特征挖掘（CPFM）机制，提取RGB和热图像中人体特征，并将其组合成为一个精准的人体检测结果。results:实验结果表明，这种方法可以有效地处理不匹配的RGB-T图像，并且能够提高人体检测的精度和稳定性。<details>
<summary>Abstract</summary>
RGB-Thermal (RGB-T) pedestrian detection aims to locate the pedestrians in RGB-T image pairs to exploit the complementation between the two modalities for improving detection robustness in extreme conditions. Most existing algorithms assume that the RGB-T image pairs are well registered, while in the real world they are not aligned ideally due to parallax or different field-of-view of the cameras. The pedestrians in misaligned image pairs may locate at different positions in two images, which results in two challenges: 1) how to achieve inter-modality complementation using spatially misaligned RGB-T pedestrian patches, and 2) how to recognize the unpaired pedestrians at the boundary. To deal with these issues, we propose a new paradigm for unregistered RGB-T pedestrian detection, which predicts two separate pedestrian locations in the RGB and thermal images, respectively. Specifically, we propose a cross-modality proposal-guided feature mining (CPFM) mechanism to extract the two precise fusion features for representing the pedestrian in the two modalities, even if the RGB-T image pair is unaligned. It enables us to effectively exploit the complementation between the two modalities. With the CPFM mechanism, we build a two-stream dense detector; it predicts the two pedestrian locations in the two modalities based on the corresponding fusion feature mined by the CPFM mechanism. Besides, we design a data augmentation method, named Homography, to simulate the discrepancy in scales and views between images. We also investigate two non-maximum suppression (NMS) methods for post-processing. Favorable experimental results demonstrate the effectiveness and robustness of our method in dealing with unregistered pedestrians with different shifts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DISGAN-Wavelet-informed-Discriminator-Guides-GAN-to-MRI-Super-resolution-with-Noise-Cleaning"><a href="#DISGAN-Wavelet-informed-Discriminator-Guides-GAN-to-MRI-Super-resolution-with-Noise-Cleaning" class="headerlink" title="DISGAN: Wavelet-informed Discriminator Guides GAN to MRI Super-resolution with Noise Cleaning"></a>DISGAN: Wavelet-informed Discriminator Guides GAN to MRI Super-resolution with Noise Cleaning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12084">http://arxiv.org/abs/2308.12084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Wang, Lucas Mahler, Julius Steiglechner, Florian Birk, Klaus Scheffler, Gabriele Lohmann</li>
<li>for: 这 paper 是为了解决 MRI 超分解 (SR) 和噪声纠正 зада题的基础挑战。</li>
<li>methods: 这 paper 使用了一种新的方法，即同时解决 SR 和噪声纠正两个任务的单一深度学习模型，不需要额外的噪声和清晰图像对应的训练数据。</li>
<li>results: 这 paper 的模型能够同时实现高质量的 SR 和噪声纠正效果，并且不需要额外的噪声和清晰图像对应的训练数据。模型的性能在多个 MRI 数据集上进行了评估，包括 Human Connectome Project (HCP) 数据集和患有脑肿和 эпилепси的subjects的 MRI 数据集。<details>
<summary>Abstract</summary>
MRI super-resolution (SR) and denoising tasks are fundamental challenges in the field of deep learning, which have traditionally been treated as distinct tasks with separate paired training data. In this paper, we propose an innovative method that addresses both tasks simultaneously using a single deep learning model, eliminating the need for explicitly paired noisy and clean images during training. Our proposed model is primarily trained for SR, but also exhibits remarkable noise-cleaning capabilities in the super-resolved images. Instead of conventional approaches that introduce frequency-related operations into the generative process, our novel approach involves the use of a GAN model guided by a frequency-informed discriminator. To achieve this, we harness the power of the 3D Discrete Wavelet Transform (DWT) operation as a frequency constraint within the GAN framework for the SR task on magnetic resonance imaging (MRI) data. Specifically, our contributions include: 1) a 3D generator based on residual-in-residual connected blocks; 2) the integration of the 3D DWT with $1\times 1$ convolution into a DWT+conv unit within a 3D Unet for the discriminator; 3) the use of the trained model for high-quality image SR, accompanied by an intrinsic denoising process. We dub the model "Denoising Induced Super-resolution GAN (DISGAN)" due to its dual effects of SR image generation and simultaneous denoising. Departing from the traditional approach of training SR and denoising tasks as separate models, our proposed DISGAN is trained only on the SR task, but also achieves exceptional performance in denoising. The model is trained on 3D MRI data from dozens of subjects from the Human Connectome Project (HCP) and further evaluated on previously unseen MRI data from subjects with brain tumours and epilepsy to assess its denoising and SR performance.
</details>
<details>
<summary>摘要</summary>
MRI超分解（SR）和噪声除除（denoising）是深度学习领域的基本挑战，传统上被视为独立的两个任务，需要分别培训独立的深度学习模型。在这篇论文中，我们提出了一种创新的方法， simultaneous addressing both tasks using a single deep learning model, eliminating the need for explicitly paired noisy and clean images during training. Our proposed model is primarily trained for SR, but also exhibits remarkable noise-cleaning capabilities in the super-resolved images. Instead of conventional approaches that introduce frequency-related operations into the generative process, our novel approach involves the use of a GAN模型 guided by a frequency-informed discriminator. To achieve this, we harness the power of the 3D Discrete Wavelet Transform (DWT) operation as a frequency constraint within the GAN framework for the SR task on magnetic resonance imaging (MRI) data. Specifically, our contributions include: 1) a 3D generator based on residual-in-residual connected blocks; 2) the integration of the 3D DWT with $1\times 1$ convolution into a DWT+conv unit within a 3D Unet for the discriminator; 3) the use of the trained model for high-quality image SR, accompanied by an intrinsic denoising process. We dub the model "Denoising Induced Super-resolution GAN (DISGAN)" due to its dual effects of SR image generation and simultaneous denoising. Departing from the traditional approach of training SR and denoising tasks as separate models, our proposed DISGAN is trained only on the SR task, but also achieves exceptional performance in denoising. The model is trained on 3D MRI data from dozens of subjects from the Human Connectome Project (HCP) and further evaluated on previously unseen MRI data from subjects with brain tumours and epilepsy to assess its denoising and SR performance.
</details></li>
</ul>
<hr>
<h2 id="Understanding-Dark-Scenes-by-Contrasting-Multi-Modal-Observations"><a href="#Understanding-Dark-Scenes-by-Contrasting-Multi-Modal-Observations" class="headerlink" title="Understanding Dark Scenes by Contrasting Multi-Modal Observations"></a>Understanding Dark Scenes by Contrasting Multi-Modal Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12320">http://arxiv.org/abs/2308.12320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/palmdong/smmcl">https://github.com/palmdong/smmcl</a></li>
<li>paper_authors: Xiaoyu Dong, Naoto Yokoya</li>
<li>for: 提高多Modal Image数据下黑色场景理解的精度</li>
<li>methods: 引入监督多Modal Contrastive Learning方法，通过跨Modal和同Modal对比来增强多Modal feature空间的semantic锐度</li>
<li>results: 在多种任务上，包括不同的光照条件和图像模式，实验显示我们的方法可以效果地提高基于多Modal Image的黑色场景理解，并且与之前的方法进行比较而显示我们的状态精度性能。<details>
<summary>Abstract</summary>
Understanding dark scenes based on multi-modal image data is challenging, as both the visible and auxiliary modalities provide limited semantic information for the task. Previous methods focus on fusing the two modalities but neglect the correlations among semantic classes when minimizing losses to align pixels with labels, resulting in inaccurate class predictions. To address these issues, we introduce a supervised multi-modal contrastive learning approach to increase the semantic discriminability of the learned multi-modal feature spaces by jointly performing cross-modal and intra-modal contrast under the supervision of the class correlations. The cross-modal contrast encourages same-class embeddings from across the two modalities to be closer and pushes different-class ones apart. The intra-modal contrast forces same-class or different-class embeddings within each modality to be together or apart. We validate our approach on a variety of tasks that cover diverse light conditions and image modalities. Experiments show that our approach can effectively enhance dark scene understanding based on multi-modal images with limited semantics by shaping semantic-discriminative feature spaces. Comparisons with previous methods demonstrate our state-of-the-art performance. Code and pretrained models are available at https://github.com/palmdong/SMMCL.
</details>
<details>
<summary>摘要</summary>
《理解黑色场景基于多Modal图像数据是具有挑战性的，因为可见和辅助modalities都提供有限的semantic信息 для任务。先前的方法主要关注两Modalities的融合，但忽视了semantic类别之间的相关性when minimizing losses to align pixels with labels, resulting in inaccurate class predictions。为解决这些问题，我们提出了一种监督多Modal异构学习方法，以增强学习的多Modal特征空间semantic抑制能力。我们同时在交叉Modal和内部Modal上进行对比，以便在监督class关系下同时实现跨Modal和内部Modal的匹配。交叉Modal对比使得同一个类别的embeddings从不同modalities中更加相近，而不同类别的embeddings则更加分开。内部Modal对比使得同一个类别或不同类别的embeddings在每个modalities中都是 вместе或分开。我们在多种任务上验证了我们的方法，包括不同的照明条件和图像modalities。实验结果表明，我们的方法可以有效地提高基于多Modal图像的黑色场景理解，并且可以Shape semantic-discriminative feature spaces。与先前的方法进行比较，我们的性能达到了国际水平。代码和预训练模型可以在https://github.com/palmdong/SMMCL上获取。》
</details></li>
</ul>
<hr>
<h2 id="SILT-Shadow-aware-Iterative-Label-Tuning-for-Learning-to-Detect-Shadows-from-Noisy-Labels"><a href="#SILT-Shadow-aware-Iterative-Label-Tuning-for-Learning-to-Detect-Shadows-from-Noisy-Labels" class="headerlink" title="SILT: Shadow-aware Iterative Label Tuning for Learning to Detect Shadows from Noisy Labels"></a>SILT: Shadow-aware Iterative Label Tuning for Learning to Detect Shadows from Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12064">http://arxiv.org/abs/2308.12064</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cralence/silt">https://github.com/cralence/silt</a></li>
<li>paper_authors: Han Yang, Tianyu Wang, Xiaowei Hu, Chi-Wing Fu</li>
<li>for: 提高阴影检测模型的性能， addresses the issue of missing or mislabeled shadows in existing shadow detection datasets.</li>
<li>methods: 提出了一种名为SILT的shadow-aware iterative label tuning框架， which explicitly considers noise in shadow labels and trains the deep model in a self-training manner.</li>
<li>results: 通过对SBU数据集的测试集重新标注和多种实验，our results show that even a simple U-Net trained with SILT can outperform all state-of-the-art methods by a large margin. When trained on SBU &#x2F; UCF &#x2F; ISTD, our network can successfully reduce the Balanced Error Rate by 25.2% &#x2F; 36.9% &#x2F; 21.3% over the best state-of-the-art method.<details>
<summary>Abstract</summary>
Existing shadow detection datasets often contain missing or mislabeled shadows, which can hinder the performance of deep learning models trained directly on such data. To address this issue, we propose SILT, the Shadow-aware Iterative Label Tuning framework, which explicitly considers noise in shadow labels and trains the deep model in a self-training manner. Specifically, we incorporate strong data augmentations with shadow counterfeiting to help the network better recognize non-shadow regions and alleviate overfitting. We also devise a simple yet effective label tuning strategy with global-local fusion and shadow-aware filtering to encourage the network to make significant refinements on the noisy labels. We evaluate the performance of SILT by relabeling the test set of the SBU dataset and conducting various experiments. Our results show that even a simple U-Net trained with SILT can outperform all state-of-the-art methods by a large margin. When trained on SBU / UCF / ISTD, our network can successfully reduce the Balanced Error Rate by 25.2% / 36.9% / 21.3% over the best state-of-the-art method.
</details>
<details>
<summary>摘要</summary>
现有的阴影检测 dataset  oftentimes 包含遗传或错abeled 的阴影，这可能会妨碍 deep learning 模型直接在这些数据上训练。为解决这个问题，我们提出了 SILT，即 Shadow-aware Iterative Label Tuning 框架，这个框架会明确地考虑阴影标签中的噪声，并在自适应方式下训练 deep model。具体来说，我们将强大的数据增强器与阴影伪造相结合，帮助网络更好地识别非阴影区域，并减少适应。我们还创造了一个简单 yet effective 的标签修正策略，具有全球-本地融合和阴影-aware 范围筛选，以鼓励网络对阴影标签中的噪声进行重要修正。我们通过对 SBU 测试集进行重新标签并进行多种实验评估 SILT 的性能。我们的结果显示，即使使用 SILT 训练的简单 U-Net 可以在所有状态对抗方法中表现出色，并且在 SBU / UCF / ISTD 上训练时可以成功降低平衡错误率 BY 25.2% / 36.9% / 21.3%。
</details></li>
</ul>
<hr>
<h2 id="HarvestNet-A-Dataset-for-Detecting-Smallholder-Farming-Activity-Using-Harvest-Piles-and-Remote-Sensing"><a href="#HarvestNet-A-Dataset-for-Detecting-Smallholder-Farming-Activity-Using-Harvest-Piles-and-Remote-Sensing" class="headerlink" title="HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing"></a>HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12061">http://arxiv.org/abs/2308.12061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Xu, Amna Elmustafa, Liya Weldegebriel, Emnet Negash, Richard Lee, Chenlin Meng, Stefano Ermon, David Lobell</li>
<li>for: 这研究旨在为叙利亚和阿姆拉地区的小型农场提供更准确和有时间的农田评估方法。</li>
<li>methods: 该研究使用了检测收割堆的方法，利用专家知识和卫星图像，收集了7000张手动标注图像和2000张地面采集标签。</li>
<li>results: 研究结果表明，使用最新的模型可以在手动标注数据上达到80%的分类性能，并在真实数据上达到90%、98%的准确率。此外，研究还发现了一些已有的覆盖地图中的偏差，并在提供了56,621公顷的新的农田面积。<details>
<summary>Abstract</summary>
Small farms contribute to a large share of the productive land in developing countries. In regions such as sub-Saharan Africa, where 80% of farms are small (under 2 ha in size), the task of mapping smallholder cropland is an important part of tracking sustainability measures such as crop productivity. However, the visually diverse and nuanced appearance of small farms has limited the effectiveness of traditional approaches to cropland mapping. Here we introduce a new approach based on the detection of harvest piles characteristic of many smallholder systems throughout the world. We present HarvestNet, a dataset for mapping the presence of farms in the Ethiopian regions of Tigray and Amhara during 2020-2023, collected using expert knowledge and satellite images, totaling 7k hand-labeled images and 2k ground collected labels. We also benchmark a set of baselines including SOTA models in remote sensing with our best models having around 80% classification performance on hand labelled data and 90%, 98% accuracy on ground truth data for Tigray, Amhara respectively. We also perform a visual comparison with a widely used pre-existing coverage map and show that our model detects an extra 56,621 hectares of cropland in Tigray. We conclude that remote sensing of harvest piles can contribute to more timely and accurate cropland assessments in food insecure region.
</details>
<details>
<summary>摘要</summary>
小型农场对发展中国家的生产地域占有重要的比重。如在非洲萨赫拉区，80%的农场面积在2公顷以下（小型农场），评估可持续发展的重要一环是映射小holder农场。然而，传统方法的映射农场面积受到小型农场的多样性和细节的限制。在这里，我们介绍了一种新的方法，基于农作物收割堆的检测。我们提供了在埃塞俄比亚地区特拉YES和阿姆拉地区2020-2023年度的HarvestNet数据集，包括7000个专家知识和卫星图像的手动标注，以及2000个地面采集标注。我们还对比了一些标准的准确性模型，我们的最佳模型在手动标注数据上达到80%的分类性能，在地面真实数据上达到90%、98%的准确性。我们还进行了与一个广泛使用的现有的覆盖地图进行视觉比较，并显示了我们的模型可以检测到特拉YES地区的56621公顷更多的农地。我们结论是，通过远程感知收割堆可以为食物不足地区提供更时准确的农地评估。
</details></li>
</ul>
<hr>
<h2 id="Manipulating-Embeddings-of-Stable-Diffusion-Prompts"><a href="#Manipulating-Embeddings-of-Stable-Diffusion-Prompts" class="headerlink" title="Manipulating Embeddings of Stable Diffusion Prompts"></a>Manipulating Embeddings of Stable Diffusion Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12059">http://arxiv.org/abs/2308.12059</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/webis-de/arxiv23-prompt-embedding-manipulation">https://github.com/webis-de/arxiv23-prompt-embedding-manipulation</a></li>
<li>paper_authors: Niklas Deckers, Julia Peters, Martin Potthast</li>
<li>for: 这篇论文旨在提出和分析一种通过直接修改提示 embedding 来更加细化地控制生成的图像的方法，以满足用户的愿望。</li>
<li>methods: 该论文提出了一种将生成图像模型看作连续函数，将图像空间和提示 embedding 空间之间传递梯度的方法，以实现更加细化的控制。</li>
<li>results: 实验结果表明该方法的可行性。<details>
<summary>Abstract</summary>
Generative text-to-image models such as Stable Diffusion allow users to generate images based on a textual description, the prompt. Changing the prompt is still the primary means for the user to change a generated image as desired. However, changing the image by reformulating the prompt remains a difficult process of trial and error, which has led to the emergence of prompt engineering as a new field of research. We propose and analyze methods to change the embedding of a prompt directly instead of the prompt text. It allows for more fine-grained and targeted control that takes into account user intentions. Our approach treats the generative text-to-image model as a continuous function and passes gradients between the image space and the prompt embedding space. By addressing different user interaction problems, we can apply this idea in three scenarios: (1) Optimization of a metric defined in image space that could measure, for example, image style. (2) Assistance of users in creative tasks by enabling them to navigate the image space along a selection of directions of "near" prompt embeddings. (3) Changing the embedding of the prompt to include information that the user has seen in a particular seed but finds difficult to describe in the prompt. Our experiments demonstrate the feasibility of the described methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本描述转换为生成图像的模型，如稳定扩散，允许用户根据文本描述生成图像。但是，通过修改描述文本来更改生成的图像仍然是一个困难的过程，它导致了提前工程的出现。我们提议和分析改变描述文本 embedding 的方法，以实现更细化和有target的控制，考虑用户的INTENT。我们的方法将生成文本到图像模型看作是连续函数，将 gradients 传递 между图像空间和描述文本 embedding 空间。通过解决不同的用户交互问题，我们可以应用这个想法在三个场景中：（1）优化图像空间中定义的一个指标，例如图像风格。（2）帮助用户完成创意任务，让他们可以在选择的方向上导航图像空间。（3）将描述文本 embedding 包含用户看到的信息，但是很难用描述在描述文本中。我们的实验表明这些方法的可行性。
</details></li>
</ul>
<hr>
<h2 id="DR-Tune-Improving-Fine-tuning-of-Pretrained-Visual-Models-by-Distribution-Regularization-with-Semantic-Calibration"><a href="#DR-Tune-Improving-Fine-tuning-of-Pretrained-Visual-Models-by-Distribution-Regularization-with-Semantic-Calibration" class="headerlink" title="DR-Tune: Improving Fine-tuning of Pretrained Visual Models by Distribution Regularization with Semantic Calibration"></a>DR-Tune: Improving Fine-tuning of Pretrained Visual Models by Distribution Regularization with Semantic Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12058">http://arxiv.org/abs/2308.12058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weeknan/dr-tune">https://github.com/weeknan/dr-tune</a></li>
<li>paper_authors: Nan Zhou, Jiaxin Chen, Di Huang</li>
<li>for: 这个论文主要针对的是如何使用先验知识来提高下游任务的表达力，并提出了一种新的资源调整框架。</li>
<li>methods: 该论文提出了一种名为分布调整 WITH semantic calibration（DR-Tune）的新方法，该方法通过对下游任务头部进行分布调整，来防止过拟合而允许下游Encoder sufficient training。此外，该方法还提出了一种名为semantic calibration（SC）模块，用于对先验知识和下游知识之间的差异进行填充。</li>
<li>results: 该论文通过对多个图像分类 datasets 进行广泛的实验，证明了 DR-Tune 可以在不同的预训练策略下提高表达力的性能。<details>
<summary>Abstract</summary>
The visual models pretrained on large-scale benchmarks encode general knowledge and prove effective in building more powerful representations for downstream tasks. Most existing approaches follow the fine-tuning paradigm, either by initializing or regularizing the downstream model based on the pretrained one. The former fails to retain the knowledge in the successive fine-tuning phase, thereby prone to be over-fitting, and the latter imposes strong constraints to the weights or feature maps of the downstream model without considering semantic drift, often incurring insufficient optimization. To deal with these issues, we propose a novel fine-tuning framework, namely distribution regularization with semantic calibration (DR-Tune). It employs distribution regularization by enforcing the downstream task head to decrease its classification error on the pretrained feature distribution, which prevents it from over-fitting while enabling sufficient training of downstream encoders. Furthermore, to alleviate the interference by semantic drift, we develop the semantic calibration (SC) module to align the global shape and class centers of the pretrained and downstream feature distributions. Extensive experiments on widely used image classification datasets show that DR-Tune consistently improves the performance when combing with various backbones under different pretraining strategies. Code is available at: https://github.com/weeknan/DR-Tune.
</details>
<details>
<summary>摘要</summary>
“视觉模型在大规模标准 datasets 预训练后encode general knowledge，并且在下游任务建立更强大的表示。现有的方法大多采用 fine-tuning 方法，包括在预训练模型的基础上初始化或正则化下游模型。前者容易过拟合，后者对下游模型的权重或特征图进行强制约束，而不考虑 semantics 的变化，通常导致优化不足。为解决这些问题，我们提出了一种新的 fine-tuning 框架，即 distribution regularization with semantic calibration (DR-Tune)。它通过在下游任务头中减少预训练特征分布上的类错误，防止过拟合而允许下游编码器充分训练。此外，为了缓解 semantics 的变化所带来的干扰，我们开发了 semantic calibration (SC) 模块，用于对预训练和下游特征分布的全球形态和类中心进行对齐。我们在各种图像分类数据集进行了广泛的实验，结果表明 DR-Tune 在不同的预训练策略下均能提高表现。代码可以在 GitHub 上获取：https://github.com/weeknan/DR-Tune。”
</details></li>
</ul>
<hr>
<h2 id="Head-Tail-Cooperative-Learning-Network-for-Unbiased-Scene-Graph-Generation"><a href="#Head-Tail-Cooperative-Learning-Network-for-Unbiased-Scene-Graph-Generation" class="headerlink" title="Head-Tail Cooperative Learning Network for Unbiased Scene Graph Generation"></a>Head-Tail Cooperative Learning Network for Unbiased Scene Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12048">http://arxiv.org/abs/2308.12048</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wanglei0618/htcl">https://github.com/wanglei0618/htcl</a></li>
<li>paper_authors: Lei Wang, Zejian Yuan, Yao Lu, Badong Chen</li>
<li>for:  solves the challenge of head-biased prediction in scene graph generation by proposing a model-agnostic Head-Tail Collaborative Learning (HTCL) network.</li>
<li>methods:  includes head-prefer and tail-prefer feature representation branches that collaborate to achieve accurate recognition of both head and tail predicates, and a self-supervised learning approach to enhance the prediction ability of the tail-prefer feature representation branch.</li>
<li>results:  achieves higher mean Recall with a minimal sacrifice in Recall and achieves a new state-of-the-art overall performance on various SGG models on VG150, Open Images V6 and GQA200 datasets.<details>
<summary>Abstract</summary>
Scene Graph Generation (SGG) as a critical task in image understanding, facing the challenge of head-biased prediction caused by the long-tail distribution of predicates. However, current unbiased SGG methods can easily prioritize improving the prediction of tail predicates while ignoring the substantial sacrifice in the prediction of head predicates, leading to a shift from head bias to tail bias. To address this issue, we propose a model-agnostic Head-Tail Collaborative Learning (HTCL) network that includes head-prefer and tail-prefer feature representation branches that collaborate to achieve accurate recognition of both head and tail predicates. We also propose a self-supervised learning approach to enhance the prediction ability of the tail-prefer feature representation branch by constraining tail-prefer predicate features. Specifically, self-supervised learning converges head predicate features to their class centers while dispersing tail predicate features as much as possible through contrast learning and head center loss. We demonstrate the effectiveness of our HTCL by applying it to various SGG models on VG150, Open Images V6 and GQA200 datasets. The results show that our method achieves higher mean Recall with a minimal sacrifice in Recall and achieves a new state-of-the-art overall performance. Our code is available at https://github.com/wanglei0618/HTCL.
</details>
<details>
<summary>摘要</summary>
Scene Graph Generation（SGG）是图像理解中的关键任务，面临长 хвоста分布的预测问题。然而，当前的无偏SGG方法可能会忽略改进头预测的代价，导致偏头偏尾的转换。为解决这个问题，我们提出了无关模型的Head-Tail Collaborative Learning（HTCL）网络，包括头预测和尾预测的特征表示分支。我们还提出了一种自然学习方法来增强尾预测特征表示分支的预测能力。具体来说，自然学习使得头预测特征分布到其类中心，而尾预测特征分布到最大程度可能的位置，通过对比学习和头中心损失来实现。我们在VG150、Open Images V6和GQA200 datasets上应用了我们的HTCL方法，结果显示，我们的方法可以实现更高的含涵率，同时减少预测错误的代价，达到新的领先性表现。我们的代码可以在https://github.com/wanglei0618/HTCL上获取。
</details></li>
</ul>
<hr>
<h2 id="RefEgo-Referring-Expression-Comprehension-Dataset-from-First-Person-Perception-of-Ego4D"><a href="#RefEgo-Referring-Expression-Comprehension-Dataset-from-First-Person-Perception-of-Ego4D" class="headerlink" title="RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D"></a>RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12035">http://arxiv.org/abs/2308.12035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuhei Kurita, Naoki Katsura, Eri Onami</li>
<li>for: 本研究旨在开发能够根据自己周围环境的描述文本进行场景物体的定位，以实现智能设备或自主机器人能够根据直觉文本指令进行行动。</li>
<li>methods: 本研究使用了大量 Egocentric 视频数据集 Ego4D，并在其基础上构建了视频基于referring表达理解数据集 RefEgo，包括12000个视频剪辑和41小时的视频基于referring表达理解标注数据。</li>
<li>results: 实验结果表明，通过结合现状最佳2D referring表达理解模型和对象跟踪算法，可以实现视频中referenced对象的跟踪，包括对象离屏或多个相似对象出现在视频中。<details>
<summary>Abstract</summary>
Grounding textual expressions on scene objects from first-person views is a truly demanding capability in developing agents that are aware of their surroundings and behave following intuitive text instructions. Such capability is of necessity for glass-devices or autonomous robots to localize referred objects in the real-world. In the conventional referring expression comprehension tasks of images, however, datasets are mostly constructed based on the web-crawled data and don't reflect diverse real-world structures on the task of grounding textual expressions in diverse objects in the real world. Recently, a massive-scale egocentric video dataset of Ego4D was proposed. Ego4D covers around the world diverse real-world scenes including numerous indoor and outdoor situations such as shopping, cooking, walking, talking, manufacturing, etc. Based on egocentric videos of Ego4D, we constructed a broad coverage of the video-based referring expression comprehension dataset: RefEgo. Our dataset includes more than 12k video clips and 41 hours for video-based referring expression comprehension annotation. In experiments, we combine the state-of-the-art 2D referring expression comprehension models with the object tracking algorithm, achieving the video-wise referred object tracking even in difficult conditions: the referred object becomes out-of-frame in the middle of the video or multiple similar objects are presented in the video.
</details>
<details>
<summary>摘要</summary>
文本表达grounding在场景物体上从第一人称视角是开发意识到围场景和按照直觉文本指令行为的真正挑战。这种能力是让玻璃设备或自动化机器人本地化参照对象的必要条件。在传统的图像引用表达理解任务上， however， dataset是基于网络爬虫数据构建的，而不具备多样化的实际世界结构。近期，一个大规模的 Egocentric video 数据集 Ego4D 被提出。Ego4D 覆盖了全球多样化的实际场景，包括室内和室外的 cooking、shopping、散步、谈话、制造等场景。基于 Ego4D 的 egocentric 视频，我们构建了包括 более чем 12k 视频剪辑和 41 小时的视频基于引用表达理解注解。在实验中，我们将 state-of-the-art 2D 引用表达理解模型与物体跟踪算法结合，实现视频基础referenced对象跟踪，包括对象在视频中心位置出现或多个类似对象出现在视频中的情况。
</details></li>
</ul>
<hr>
<h2 id="Distribution-Aware-Calibration-for-Object-Detection-with-Noisy-Bounding-Boxes"><a href="#Distribution-Aware-Calibration-for-Object-Detection-with-Noisy-Bounding-Boxes" class="headerlink" title="Distribution-Aware Calibration for Object Detection with Noisy Bounding Boxes"></a>Distribution-Aware Calibration for Object Detection with Noisy Bounding Boxes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12017">http://arxiv.org/abs/2308.12017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donghao Zhou, Jialin Li, Jinpeng Li, Jiancheng Huang, Qiang Nie, Yong Liu, Bin-Bin Gao, Qiong Wang, Pheng-Ann Heng, Guangyong Chen</li>
<li>for: 提高对象检测器的训练效果，避免因噪音绘制而减降检测性能。</li>
<li>methods: 基于提取物体潜在位置的空间分布模型，开发了三种分布意识技术：分布意识提档（DA-Aug）、分布意识盒体细化（DA-Ref）和分布意识信任估计（DA-Est），以提高分类、定位和解释性。</li>
<li>results: 在大规模噪音图像集（Pascal VOC和MS-COCO）上进行了广泛的实验，结果表明，DISCO可以在高噪音水平上达到顶峰性状检测性能。<details>
<summary>Abstract</summary>
Large-scale well-annotated datasets are of great importance for training an effective object detector. However, obtaining accurate bounding box annotations is laborious and demanding. Unfortunately, the resultant noisy bounding boxes could cause corrupt supervision signals and thus diminish detection performance. Motivated by the observation that the real ground-truth is usually situated in the aggregation region of the proposals assigned to a noisy ground-truth, we propose DIStribution-aware CalibratiOn (DISCO) to model the spatial distribution of proposals for calibrating supervision signals. In DISCO, spatial distribution modeling is performed to statistically extract the potential locations of objects. Based on the modeled distribution, three distribution-aware techniques, i.e., distribution-aware proposal augmentation (DA-Aug), distribution-aware box refinement (DA-Ref), and distribution-aware confidence estimation (DA-Est), are developed to improve classification, localization, and interpretability, respectively. Extensive experiments on large-scale noisy image datasets (i.e., Pascal VOC and MS-COCO) demonstrate that DISCO can achieve state-of-the-art detection performance, especially at high noise levels.
</details>
<details>
<summary>摘要</summary>
Motivated by the observation that real ground truth is usually located in the aggregation region of proposals assigned to noisy ground truth, we propose Distribution-aware Calibration (DISCO) to model the spatial distribution of proposals for calibrating supervision signals. DISCO uses spatial distribution modeling to statistically extract potential object locations. Based on the modeled distribution, we develop three distribution-aware techniques: distribution-aware proposal augmentation (DA-Aug), distribution-aware box refinement (DA-Ref), and distribution-aware confidence estimation (DA-Est) to improve classification, localization, and interpretability, respectively.Extensive experiments on large-scale noisy image datasets (Pascal VOC and MS-COCO) show that DISCO achieves state-of-the-art detection performance, especially at high noise levels.
</details></li>
</ul>
<hr>
<h2 id="StofNet-Super-resolution-Time-of-Flight-Network"><a href="#StofNet-Super-resolution-Time-of-Flight-Network" class="headerlink" title="StofNet: Super-resolution Time of Flight Network"></a>StofNet: Super-resolution Time of Flight Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12009">http://arxiv.org/abs/2308.12009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hahnec/stofnet">https://github.com/hahnec/stofnet</a></li>
<li>paper_authors: Christopher Hahne, Michel Hayoz, Raphael Sznitman</li>
<li>for: 这篇论文主要关注于解决时间探测技术（ToF）在复杂 ambient condition 下的探测困难，通过 moderne super-resolution 技术来学习不同的周围环境，以提高 ToF 探测的可靠性和准确性。</li>
<li>methods: 该论文提出了一种新的 StofNet 架构，结合 super-resolution 和高效的 residual contraction block，以平衡细详信号素和大规模上下文信息。</li>
<li>results: 对于 six state-of-the-art 方法进行了比较，结果表明 StofNet 具有较高的精度、可靠性和模型复杂度。 我们还发布了 SToF-Chirp 数据集， captured by 一架飞行式 ultrasound 探测器，并提供了相关的代码。<details>
<summary>Abstract</summary>
Time of Flight (ToF) is a prevalent depth sensing technology in the fields of robotics, medical imaging, and non-destructive testing. Yet, ToF sensing faces challenges from complex ambient conditions making an inverse modelling from the sparse temporal information intractable. This paper highlights the potential of modern super-resolution techniques to learn varying surroundings for a reliable and accurate ToF detection. Unlike existing models, we tailor an architecture for sub-sample precise semi-global signal localization by combining super-resolution with an efficient residual contraction block to balance between fine signal details and large scale contextual information. We consolidate research on ToF by conducting a benchmark comparison against six state-of-the-art methods for which we employ two publicly available datasets. This includes the release of our SToF-Chirp dataset captured by an airborne ultrasound transducer. Results showcase the superior performance of our proposed StofNet in terms of precision, reliability and model complexity. Our code is available at https://github.com/hahnec/stofnet.
</details>
<details>
<summary>摘要</summary>
时间飞行（ToF）是现代深度感知技术的广泛应用领域，包括 робо扮、医疗成像和非锋渠测试。然而，ToF感知受到了复杂的 ambient 环境的挑战，从而使得对稀疏时间信息的逆模型变得不可能。本文强调了现代超分解技术的潜在作用，以提高ToF探测的可靠性和准确性。与现有模型不同，我们专门设计了一种束缚精度和大规模信息的混合块，以平衡细信息和大规模信息的权重。我们对ToF进行了 benchmark 比较，使用了六种现有的状态之 искусственный智能方法，并使用了两个公共可用的数据集。这包括我们发布的 SToF-Chirp 数据集，由空中ultrasound 传感器记录。结果表明我们提posed StofNet 的性能比其他六种方法更高， both in terms of precision and reliability.我们的代码可以在 https://github.com/hahnec/stofnet 上获取。
</details></li>
</ul>
<hr>
<h2 id="Multi-stage-Factorized-Spatio-Temporal-Representation-for-RGB-D-Action-and-Gesture-Recognition"><a href="#Multi-stage-Factorized-Spatio-Temporal-Representation-for-RGB-D-Action-and-Gesture-Recognition" class="headerlink" title="Multi-stage Factorized Spatio-Temporal Representation for RGB-D Action and Gesture Recognition"></a>Multi-stage Factorized Spatio-Temporal Representation for RGB-D Action and Gesture Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12006">http://arxiv.org/abs/2308.12006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujun Ma, Benjia Zhou, Ruili Wang, Pichao Wang</li>
<li>for: 这篇论文主要是为了RGB-D动作和姿势识别而写的。</li>
<li>methods: 这篇论文使用了一种新的启发式架构，即多stage因子化空间时间（MFST）模型，该模型包括3D中心差减核心（CDC）模块和多个因子化空间时间stage。</li>
<li>results: 该模型在RGB-D动作和姿势识别任务上表现出色，比之前的方法更高效。<details>
<summary>Abstract</summary>
RGB-D action and gesture recognition remain an interesting topic in human-centered scene understanding, primarily due to the multiple granularities and large variation in human motion. Although many RGB-D based action and gesture recognition approaches have demonstrated remarkable results by utilizing highly integrated spatio-temporal representations across multiple modalities (i.e., RGB and depth data), they still encounter several challenges. Firstly, vanilla 3D convolution makes it hard to capture fine-grained motion differences between local clips under different modalities. Secondly, the intricate nature of highly integrated spatio-temporal modeling can lead to optimization difficulties. Thirdly, duplicate and unnecessary information can add complexity and complicate entangled spatio-temporal modeling. To address the above issues, we propose an innovative heuristic architecture called Multi-stage Factorized Spatio-Temporal (MFST) for RGB-D action and gesture recognition. The proposed MFST model comprises a 3D Central Difference Convolution Stem (CDC-Stem) module and multiple factorized spatio-temporal stages. The CDC-Stem enriches fine-grained temporal perception, and the multiple hierarchical spatio-temporal stages construct dimension-independent higher-order semantic primitives. Specifically, the CDC-Stem module captures bottom-level spatio-temporal features and passes them successively to the following spatio-temporal factored stages to capture the hierarchical spatial and temporal features through the Multi- Scale Convolution and Transformer (MSC-Trans) hybrid block and Weight-shared Multi-Scale Transformer (WMS-Trans) block. The seamless integration of these innovative designs results in a robust spatio-temporal representation that outperforms state-of-the-art approaches on RGB-D action and gesture recognition datasets.
</details>
<details>
<summary>摘要</summary>
MFST模型包括3D中心差异卷积核心（CDC-Stem）模块和多个因子化空间时间阶段。CDC-Stem模块使得细节的时间感知更加细化，而多个层次的因子化空间时间阶段通过多 scales卷积和Transformer（MSC-Trans）混合块和Weight-shared Multi-Scale Transformer（WMS-Trans）块来构建维度独立的高级semantic primitives。具体来说，CDC-Stem模块首先捕捉最低级的空间时间特征，然后将其传递给以下因子化空间时间阶段，以 capture层次的空间和时间特征。这些创新的设计结合使得RGB-D动作和姿势识别达到了state-of-the-art的表现。
</details></li>
</ul>
<hr>
<h2 id="Local-Distortion-Aware-Efficient-Transformer-Adaptation-for-Image-Quality-Assessment"><a href="#Local-Distortion-Aware-Efficient-Transformer-Adaptation-for-Image-Quality-Assessment" class="headerlink" title="Local Distortion Aware Efficient Transformer Adaptation for Image Quality Assessment"></a>Local Distortion Aware Efficient Transformer Adaptation for Image Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12001">http://arxiv.org/abs/2308.12001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kangmin Xu, Liang Liao, Jing Xiao, Chaofeng Chen, Haoning Wu, Qiong Yan, Weisi Lin<br>for:This paper focuses on improving image quality assessment (IQA) tasks, which are challenging due to diverse image contents and limited data availability.methods:The authors use a combination of pre-trained convolutional neural networks (CNNs) and a local distortion extractor&#x2F;injector to extract and inject local distortion features into a large-scale pre-trained vision transformer (ViT) model.results:The proposed method achieves state-of-the-art performance on popular IQA datasets, indicating that IQA can benefit from stronger high-level features drawn from large-scale pre-trained models.Here’s the simplified Chinese version:for:本文关注改进图像质量评估（IQA）任务，这些任务受到多样化图像内容和数据有限的限制。methods:作者使用了一种组合使用预训练的 convolutional neural networks（CNN）和一个地方扰动提取器&#x2F;注入器来提取和注入地方扰动特征到大规模预训练的视transformer（ViT）模型中。results:提出的方法在流行的IQA数据集上达到了状态机器人的性能，表明IQA不仅是一个低级别的问题，而且可以受益于更强的高级别特征，从大规模预训练模型中继承知识。<details>
<summary>Abstract</summary>
Image Quality Assessment (IQA) constitutes a fundamental task within the field of computer vision, yet it remains an unresolved challenge, owing to the intricate distortion conditions, diverse image contents, and limited availability of data. Recently, the community has witnessed the emergence of numerous large-scale pretrained foundation models, which greatly benefit from dramatically increased data and parameter capacities. However, it remains an open problem whether the scaling law in high-level tasks is also applicable to IQA task which is closely related to low-level clues. In this paper, we demonstrate that with proper injection of local distortion features, a larger pretrained and fixed foundation model performs better in IQA tasks. Specifically, for the lack of local distortion structure and inductive bias of vision transformer (ViT), alongside the large-scale pretrained ViT, we use another pretrained convolution neural network (CNN), which is well known for capturing the local structure, to extract multi-scale image features. Further, we propose a local distortion extractor to obtain local distortion features from the pretrained CNN and a local distortion injector to inject the local distortion features into ViT. By only training the extractor and injector, our method can benefit from the rich knowledge in the powerful foundation models and achieve state-of-the-art performance on popular IQA datasets, indicating that IQA is not only a low-level problem but also benefits from stronger high-level features drawn from large-scale pretrained models.
</details>
<details>
<summary>摘要</summary>
图像质量评估（IQA）是计算机视觉领域的基本任务，但它仍然是一个未解决的挑战，主要是因为图像的复杂预期、多样化内容和数据的有限性。在最近，社区所看到的是许多大规模预训练基础模型的出现，这些模型受益于数据和参数的增加。然而，是否存在一个涉及到IQA任务的扩展法则仍然是一个开放的问题。在这篇论文中，我们表明了一种将本地扭曲特征注入到大规模预训练的基础模型中，以提高IQA任务的表现。具体来说，由于视Transformer（ViT）缺乏本地扭曲结构和引导因子，我们采用另一种预训练的卷积神经网络（CNN），以提取多尺度图像特征。此外，我们提出了一种本地扭曲EXTractor，以从预训练CNN中提取本地扭曲特征。最后，我们提出了一种本地扭曲注入器，以注入本地扭曲特征到ViT中。只需训练EXTractor和注入器，我们的方法可以从强大的基础模型中继承丰富的知识，并在流行的IQA数据集上达到状态率表现，表明IQA不仅是一个低级问题，还可以受益于更强的高级特征，从大规模预训练模型中继承。
</details></li>
</ul>
<hr>
<h2 id="Progressive-Feature-Mining-and-External-Knowledge-Assisted-Text-Pedestrian-Image-Retrieval"><a href="#Progressive-Feature-Mining-and-External-Knowledge-Assisted-Text-Pedestrian-Image-Retrieval" class="headerlink" title="Progressive Feature Mining and External Knowledge-Assisted Text-Pedestrian Image Retrieval"></a>Progressive Feature Mining and External Knowledge-Assisted Text-Pedestrian Image Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11994">http://arxiv.org/abs/2308.11994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huafeng Li, Shedan Yang, Yafei Zhang, Dapeng Tao, Zhengtao Yu</li>
<li>for: 用文本描述人体外观来检索匹配的人体图像。</li>
<li>methods: 提出进步性特征挖掘和外知助持特征纯化方法，以避免杂音特征的损失和提高表达能力。</li>
<li>results: 对三个挑战性数据集进行了广泛的实验，并证明了提议方法的有效性和超越性，甚至在大规模数据集上超越大规模模型基础方法。<details>
<summary>Abstract</summary>
Text-Pedestrian Image Retrieval aims to use the text describing pedestrian appearance to retrieve the corresponding pedestrian image. This task involves not only modality discrepancy, but also the challenge of the textual diversity of pedestrians with the same identity. At present, although existing research progress has been made in text-pedestrian image retrieval, these methods do not comprehensively consider the above-mentioned problems. Considering these, this paper proposes a progressive feature mining and external knowledge-assisted feature purification method. Specifically, we use a progressive mining mode to enable the model to mine discriminative features from neglected information, thereby avoiding the loss of discriminative information and improving the expression ability of features. In addition, to further reduce the negative impact of modal discrepancy and text diversity on cross-modal matching, we propose to use other sample knowledge of the same modality, i.e., external knowledge to enhance identity-consistent features and weaken identity-inconsistent features. This process purifies features and alleviates the interference caused by textual diversity and negative sample correlation features of the same modal. Extensive experiments on three challenging datasets demonstrate the effectiveness and superiority of the proposed method, and the retrieval performance even surpasses that of the large-scale model-based method on large-scale datasets.
</details>
<details>
<summary>摘要</summary>
文本行人图像检索目标是使用描述行人外观的文本来检索对应的行人图像。这个任务面临着多样性扩散和文本多样性问题。现有研究已取得一定进展，但这些方法并不完全考虑上述问题。为此，本文提出了一种进程式特征挖掘和外知助动特征纯化方法。具体来说，我们使用进程式挖掘模式，让模型从抛弃信息中挖掘出特征，以避免损失特征表达能力和提高特征表达能力。此外，为了进一步减少模式差异和文本多样性对于跨模态匹配的负面影响，我们提出了使用同一模式其他样本的知识，即外知来增强一致性特征和弱化不一致性特征。这个过程纯化特征，减少了文本多样性和负样本相互干扰的影响。我们在三个挑战性 dataset 进行了广泛的实验，结果表明我们的方法效果和前期研究超越，甚至在大规模模型基础方法上的大规模dataset上表现出色。
</details></li>
</ul>
<hr>
<h2 id="RankMixup-Ranking-Based-Mixup-Training-for-Network-Calibration"><a href="#RankMixup-Ranking-Based-Mixup-Training-for-Network-Calibration" class="headerlink" title="RankMixup: Ranking-Based Mixup Training for Network Calibration"></a>RankMixup: Ranking-Based Mixup Training for Network Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11990">http://arxiv.org/abs/2308.11990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jongyoun Noh, Hyekang Park, Junghyup Lee, Bumsub Ham<br>for:这篇论文的目的是为了精确地估算深度神经网络的信任度，特别是在实际应用中使用深度神经网络时。methods:这篇论文使用了mixup来帮助网络进行训练，并且提出了一个新的架构，即RankMixup，以解决mixup中混合标签问题。results:实验结果显示，RankMixup可以对网络进行更好的训练，并且可以实现更好的信任度估算。<details>
<summary>Abstract</summary>
Network calibration aims to accurately estimate the level of confidences, which is particularly important for employing deep neural networks in real-world systems. Recent approaches leverage mixup to calibrate the network's predictions during training. However, they do not consider the problem that mixtures of labels in mixup may not accurately represent the actual distribution of augmented samples. In this paper, we present RankMixup, a novel mixup-based framework alleviating the problem of the mixture of labels for network calibration. To this end, we propose to use an ordinal ranking relationship between raw and mixup-augmented samples as an alternative supervisory signal to the label mixtures for network calibration. We hypothesize that the network should estimate a higher level of confidence for the raw samples than the augmented ones (Fig.1). To implement this idea, we introduce a mixup-based ranking loss (MRL) that encourages lower confidences for augmented samples compared to raw ones, maintaining the ranking relationship. We also propose to leverage the ranking relationship among multiple mixup-augmented samples to further improve the calibration capability. Augmented samples with larger mixing coefficients are expected to have higher confidences and vice versa (Fig.1). That is, the order of confidences should be aligned with that of mixing coefficients. To this end, we introduce a novel loss, M-NDCG, in order to reduce the number of misaligned pairs of the coefficients and confidences. Extensive experimental results on standard benchmarks for network calibration demonstrate the effectiveness of RankMixup.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Modal-Multi-Task-3MT-Road-Segmentation"><a href="#Multi-Modal-Multi-Task-3MT-Road-Segmentation" class="headerlink" title="Multi-Modal Multi-Task (3MT) Road Segmentation"></a>Multi-Modal Multi-Task (3MT) Road Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11983">http://arxiv.org/abs/2308.11983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/erkanmilli/3mt-roadseg">https://github.com/erkanmilli/3mt-roadseg</a></li>
<li>paper_authors: Erkan Milli, Özgür Erkent, Asım Egemen Yılmaz</li>
<li>for: 这 paper 的目的是提出一种cost-effective和高度准确的道路分割方法，使用多modal sensor数据，包括RGB和LiDAR深度图像，以及IMU&#x2F;GNSS各自导航系统。</li>
<li>methods: 该方法使用raw sensor输入，而不是 Typically done in many SOTA works，使用高预处理成本的表面法或dense depth prediction。它使用一种低成本模型，以Minimize both pre-processing and model computation costs。</li>
<li>results: 该方法在 KITTI 数据集上进行了实验，并达到了fast和高性能的解决方案。同时，对 Cityscapes 数据集进行了实验，并证明了该方法可以使用不同的感知模式。 segmentation 结果对于全分辨率和半分辨率图像均是与现有方法竞争力的。<details>
<summary>Abstract</summary>
Multi-modal systems have the capacity of producing more reliable results than systems with a single modality in road detection due to perceiving different aspects of the scene. We focus on using raw sensor inputs instead of, as it is typically done in many SOTA works, leveraging architectures that require high pre-processing costs such as surface normals or dense depth predictions. By using raw sensor inputs, we aim to utilize a low-cost model thatminimizes both the pre-processing andmodel computation costs. This study presents a cost-effective and highly accurate solution for road segmentation by integrating data from multiple sensorswithin a multi-task learning architecture.Afusion architecture is proposed in which RGB and LiDAR depth images constitute the inputs of the network. Another contribution of this study is to use IMU/GNSS (inertial measurement unit/global navigation satellite system) inertial navigation system whose data is collected synchronously and calibrated with a LiDAR-camera to compute aggregated dense LiDAR depth images. It has been demonstrated by experiments on the KITTI dataset that the proposed method offers fast and high-performance solutions. We have also shown the performance of our method on Cityscapes where raw LiDAR data is not available. The segmentation results obtained for both full and half resolution images are competitive with existing methods. Therefore, we conclude that our method is not dependent only on raw LiDAR data; rather, it can be used with different sensor modalities. The inference times obtained in all experiments are very promising for real-time experiments.
</details>
<details>
<summary>摘要</summary>
多模式系统可以生成更可靠的结果，因为它们可以感知不同方面的场景。我们集中在使用原始感知输入而不是，如多数State-of-the-Art工作一样，利用需要高预处理成本的 arquitectures，例如表面法向量或密集深度预测。通过使用原始感知输入，我们希望实现低成本模型，以降低预处理和计算成本。这个研究提出了一种可靠和高精度的解决方案，通过将多种感知器 Integration into a multi-task learning architecture。我们提议的架构包括RGB和LiDAR深度图像作为网络的输入。此外，我们还使用IMU/GNSS（普通测量单元/全球导航卫星系统）的抗 gravitational 数据，同步采集并与LiDAR-Camera进行同步准确 calibration，以计算聚合的密集LiDAR深度图像。经过实验表明，我们的方法可以在KITTI数据集上提供快速和高性能的解决方案。此外，我们还对Cityscapes数据集进行了实验，并证明我们的方法可以使用不同的感知模式。所得到的分割结果与现有方法相当，因此我们可以 concluced that our method is not dependent on raw LiDAR data; rather, it can be used with different sensor modalities。实验结果表明，在所有实验中的推理时间很有前途，适用于实时实验。
</details></li>
</ul>
<hr>
<h2 id="Rotation-Invariant-Completion-Network"><a href="#Rotation-Invariant-Completion-Network" class="headerlink" title="Rotation-Invariant Completion Network"></a>Rotation-Invariant Completion Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11979">http://arxiv.org/abs/2308.11979</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/agiachris/rotational3DCNN">https://github.com/agiachris/rotational3DCNN</a></li>
<li>paper_authors: Yu Chen, Pengcheng Shi</li>
<li>for: 提高Point cloud completion方法的稳定性和可靠性，能够处理不同pose的实际世界Point cloud</li>
<li>methods: 提出了一种基于Dual Pipeline Completion Network（DPCNet）和增强模块的Rotation-Invariant Completion Network（RICNet），通过提取不受旋转影响的特征来保证Feature extraction的稳定性和可靠性</li>
<li>results: 通过对MVP数据集上进行随机变换，对Point cloud completion方法进行比较，研究发现RICNet在不同pose下的Point cloud completion性能较高，超过了现有方法<details>
<summary>Abstract</summary>
Real-world point clouds usually suffer from incompleteness and display different poses. While current point cloud completion methods excel in reproducing complete point clouds with consistent poses as seen in the training set, their performance tends to be unsatisfactory when handling point clouds with diverse poses. We propose a network named Rotation-Invariant Completion Network (RICNet), which consists of two parts: a Dual Pipeline Completion Network (DPCNet) and an enhancing module. Firstly, DPCNet generates a coarse complete point cloud. The feature extraction module of DPCNet can extract consistent features, no matter if the input point cloud has undergone rotation or translation. Subsequently, the enhancing module refines the fine-grained details of the final generated point cloud. RICNet achieves better rotation invariance in feature extraction and incorporates structural relationships in man-made objects. To assess the performance of RICNet and existing methods on point clouds with various poses, we applied random transformations to the point clouds in the MVP dataset and conducted experiments on them. Our experiments demonstrate that RICNet exhibits superior completion performance compared to existing methods.
</details>
<details>
<summary>摘要</summary>
Translation:real-world point clouds 通常会受到不完整性和不同姿态的影响。当前的点云完成方法能够很好地重建完整的点云，但是它们在处理不同姿态的点云时表现不佳。我们提议一种名为Rotation-Invariant Completion Network（RICNet）的网络，它包括两部分：一个双管道完成网络（DPCNet）和一个优化模块。首先，DPCNet生成一个粗略的完整点云。DPCNet的特征提取模块可以无论输入点云是否经历了旋转或平移，都可以提取一致的特征。然后，优化模块进行细化细节的更新。RICNet实现了更好的旋转不变性在特征提取中，并具有结构关系在人工物体中。为了评估RICNet和现有方法在不同姿态的点云上的表现，我们对MVP数据集中的点云应用了随机变换，并在其上进行了实验。我们的实验结果表明，RICNet在不同姿态的点云上的完成性比现有方法更高。
</details></li>
</ul>
<hr>
<h2 id="Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification"><a href="#Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification" class="headerlink" title="Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification"></a>Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11969">http://arxiv.org/abs/2308.11969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Lambert, Pauline Roca, Florence Forbes, Senan Doyle, Michel Dojat</li>
<li>for:  Liver and tumor segmentation for treatment strategy guidance.</li>
<li>methods:  Two different pipelines based on anisotropic models were used for segmentation: a baseline multi-class model and two distinct binary models.</li>
<li>results:  Both pipelines exhibited different strengths and weaknesses, and an uncertainty quantification strategy was proposed to identify potential false positive tumor lesions.<details>
<summary>Abstract</summary>
The burden of liver tumors is important, ranking as the fourth leading cause of cancer mortality. In case of hepatocellular carcinoma (HCC), the delineation of liver and tumor on contrast-enhanced magnetic resonance imaging (CE-MRI) is performed to guide the treatment strategy. As this task is time-consuming, needs high expertise and could be subject to inter-observer variability there is a strong need for automatic tools. However, challenges arise from the lack of available training data, as well as the high variability in terms of image resolution and MRI sequence. In this work we propose to compare two different pipelines based on anisotropic models to obtain the segmentation of the liver and tumors. The first pipeline corresponds to a baseline multi-class model that performs the simultaneous segmentation of the liver and tumor classes. In the second approach, we train two distinct binary models, one segmenting the liver only and the other the tumors. Our results show that both pipelines exhibit different strengths and weaknesses. Moreover we propose an uncertainty quantification strategy allowing the identification of potential false positive tumor lesions. Both solutions were submitted to the MICCAI 2023 Atlas challenge regarding liver and tumor segmentation.
</details>
<details>
<summary>摘要</summary>
liver tumors 是一种重要的负担， ranks as the fourth leading cause of cancer mortality。在hepatocellular carcinoma（HCC）的情况下，通过对吸引磁共振成像（CE-MRI）图像进行定义肝脏和肿瘤的分割，以便guide the treatment strategy。然而，这项工作需要很高的专业技能，时间费用很高，并且存在 между观察员的变化，因此有强需求 для自动工具。然而，由于数据不足以及图像分辨率和MRI序列的高变化，这些挑战是非常大的。在这项工作中，我们提出了两个不同的管道，基于不规则模型来获得肝脏和肿瘤的分割。第一个管道是一个基线多类模型，同时分割肝脏和肿瘤类型。在第二个方法中，我们训练了两个不同的二进制模型，一个用于分割肝脏，另一个用于分割肿瘤。我们的结果表明，这两个管道具有不同的优劣点。此外，我们还提出了一种不确定性评估策略，以便标识潜在的假阳性肿瘤涂抹。这两个解决方案都被提交到了MICCAI 2023 Atlas challenge关于肝脏和肿瘤分割。
</details></li>
</ul>
<hr>
<h2 id="Gaze-Estimation-on-Spresense"><a href="#Gaze-Estimation-on-Spresense" class="headerlink" title="Gaze Estimation on Spresense"></a>Gaze Estimation on Spresense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12313">http://arxiv.org/abs/2308.12313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Ruegg, Pietro Bonazzi, Andrea Ronco</li>
<li>for: 这个研究旨在实现一个用于人机交互、虚拟现实和医学等领域的眼动估计系统，并评估其延迟、MAC&#x2F;周期和电力消耗。</li>
<li>methods: 这个系统使用索尼Spresense微控制器板，并使用TinyTrackerS模型，具有169K字节大小、85.8k参数和3 FPS的运行速度。</li>
<li>results: 研究发现这个系统具有低延迟和低MAC&#x2F;周期，并且能够在实时进行眼动估计。<details>
<summary>Abstract</summary>
Gaze estimation is a valuable technology with numerous applications in fields such as human-computer interaction, virtual reality, and medicine. This report presents the implementation of a gaze estimation system using the Sony Spresense microcontroller board and explores its performance in latency, MAC/cycle, and power consumption. The report also provides insights into the system's architecture, including the gaze estimation model used. Additionally, a demonstration of the system is presented, showcasing its functionality and performance. Our lightweight model TinyTrackerS is a mere 169Kb in size, using 85.8k parameters and runs on the Spresense platform at 3 FPS.
</details>
<details>
<summary>摘要</summary>
gaze estimation 是一种有价值的技术，它在人工智能、虚拟现实和医疗领域有很多应用。这份报告介绍了使用索尼 Spresense 微控器板实现的 gaze estimation 系统，并评估了它的响应时间、MAC/周期和电力消耗。报告还提供了系统的架构设计，包括使用的 gaze estimation 模型。此外，报告还提供了系统的示例，展示了它的功能和性能。我们的轻量级模型 TinyTrackerS 仅有 169 KB 大小，使用 85.8k 参数，在 Spresense 平台上运行于 3 FPS。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Transfer-Learning-in-Diffusion-Models-via-Adversarial-Noise"><a href="#Efficient-Transfer-Learning-in-Diffusion-Models-via-Adversarial-Noise" class="headerlink" title="Efficient Transfer Learning in Diffusion Models via Adversarial Noise"></a>Efficient Transfer Learning in Diffusion Models via Adversarial Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11948">http://arxiv.org/abs/2308.11948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiyu Wang, Baijiong Lin, Daochang Liu, Chang Xu</li>
<li>for: 解决有限数据问题在图像生成任务中</li>
<li>methods: 提出了一种基于DPM的传输学习方法，包括两种策略：一是 similarity-guided training，即通过分类器提高传输；二是 adversarial noise selection，即根据输入图像选择目标噪声。</li>
<li>results: 在几个数据少的图像生成任务中进行了广泛的实验，结果表明，我们的方法不仅高效，而且在图像质量和多样性方面也比现有的GAN-based和DPM-based方法出色。<details>
<summary>Abstract</summary>
Diffusion Probabilistic Models (DPMs) have demonstrated substantial promise in image generation tasks but heavily rely on the availability of large amounts of training data. Previous works, like GANs, have tackled the limited data problem by transferring pre-trained models learned with sufficient data. However, those methods are hard to be utilized in DPMs since the distinct differences between DPM-based and GAN-based methods, showing in the unique iterative denoising process integral and the need for many timesteps with no-targeted noise in DPMs. In this paper, we propose a novel DPMs-based transfer learning method, TAN, to address the limited data problem. It includes two strategies: similarity-guided training, which boosts transfer with a classifier, and adversarial noise selection which adaptive chooses targeted noise based on the input image. Extensive experiments in the context of few-shot image generation tasks demonstrate that our method is not only efficient but also excels in terms of image quality and diversity when compared to existing GAN-based and DDPM-based methods.
</details>
<details>
<summary>摘要</summary>
各种扩散概率模型（DPM）在图像生成任务中表现出了重要的承袭潜力，但它们受到充足的训练数据的限制。先前的工作，如GANs，通过将预训练的模型转移到具有足够数据的环境中来解决这个问题。然而，这些方法在DPM中很难实现，因为DPM和GAN之间存在重要的差异，即DPM中的迭代净化过程的独特特性和需要许多步骤和无目标噪声。在这篇论文中，我们提出了一种基于DPM的转移学习方法，称为TAN，以解决有限数据问题。该方法包括两个策略：相似性引导的训练和对输入图像进行适应性选择噪声。我们在少量图像生成任务中进行了广泛的实验，并证明了我们的方法不仅高效，还能够在图像质量和多样性方面超越现有的GAN基于和DPM基于的方法。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Diffusion-Models-with-an-Adaptive-Momentum-Sampler"><a href="#Boosting-Diffusion-Models-with-an-Adaptive-Momentum-Sampler" class="headerlink" title="Boosting Diffusion Models with an Adaptive Momentum Sampler"></a>Boosting Diffusion Models with an Adaptive Momentum Sampler</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11941">http://arxiv.org/abs/2308.11941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiyu Wang, Anh-Dung Dinh, Daochang Liu, Chang Xu</li>
<li>for: This paper aims to improve the sampling process in Diffusion Probabilistic Models (DPMs) to generate high-quality images.</li>
<li>methods: The proposed method is a novel reverse sampler for DPMs, inspired by the Adam optimizer, which uses momentum mechanisms and adaptive updating to smooth the reverse sampling process and ensure stable generation.</li>
<li>results: The proposed reverse sampler achieves remarkable improvements over different baselines, yielding enhanced quality outputs.Here’s the full summary in Simplified Chinese:</li>
<li>for: 本文目的是提高Diffusion Probabilistic Models (DPMs)中的抽样过程，以生成高质量的图像。</li>
<li>methods: 提议的方法是一种基于Adam优化器的reverse抽样器，利用势量机制和自适应更新来缓和反抽样过程，确保稳定的生成。</li>
<li>results: 提议的reverse抽样器在多个 benchmark 上得到了显著的改善，生成的输出质量得到了提升。<details>
<summary>Abstract</summary>
Diffusion probabilistic models (DPMs) have been shown to generate high-quality images without the need for delicate adversarial training. However, the current sampling process in DPMs is prone to violent shaking. In this paper, we present a novel reverse sampler for DPMs inspired by the widely-used Adam optimizer. Our proposed sampler can be readily applied to a pre-trained diffusion model, utilizing momentum mechanisms and adaptive updating to smooth the reverse sampling process and ensure stable generation, resulting in outputs of enhanced quality. By implicitly reusing update directions from early steps, our proposed sampler achieves a better balance between high-level semantics and low-level details. Additionally, this sampler is flexible and can be easily integrated into pre-trained DPMs regardless of the sampler used during training. Our experimental results on multiple benchmarks demonstrate that our proposed reverse sampler yields remarkable improvements over different baselines. We will make the source code available.
</details>
<details>
<summary>摘要</summary>
diffusion probabilistic models (DPMs) 有 shown 能生成高质量图像，无需精细 adversarial training。然而，当前的采样 процес在 DPMs 中存在激动难控的问题。在这篇论文中，我们提出了一种新的 reverse sampler for DPMs， Drawing inspiration from the widely-used Adam optimizer。我们的提议的抽样器可以 readily applied 到预训练的 diffusion model，使用动量机制和自适应更新来平稳抽样过程，以保证生成的输出质量。通过重用早期步骤的更新方向，我们的抽样器实现了更好的semantic balance 和细节层次结构。此外，这种抽样器 flexible 可以轻松地integrated into pre-trained DPMs，不管在training中使用的抽样器。我们的实验结果表明，我们的提议的 reverse sampler 在多个benchmark上具有remarkable improvement。我们将会公开源代码。
</details></li>
</ul>
<hr>
<h2 id="Synergistic-Multiscale-Detail-Refinement-via-Intrinsic-Supervision-for-Underwater-Image-Enhancement"><a href="#Synergistic-Multiscale-Detail-Refinement-via-Intrinsic-Supervision-for-Underwater-Image-Enhancement" class="headerlink" title="Synergistic Multiscale Detail Refinement via Intrinsic Supervision for Underwater Image Enhancement"></a>Synergistic Multiscale Detail Refinement via Intrinsic Supervision for Underwater Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11932">http://arxiv.org/abs/2308.11932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dehuan Zhang, Jingchun Zhou, Weishi Zhang, ChunLe Guo, Chongyi Li</li>
<li>for: 本文提出了一种synergistic multiscale detail refinement via intrinsic supervision（SMDR-IS）方法，用于 восстановление水下场景图像。</li>
<li>methods: 该方法包括一个low-degradation stage和多个高层次降低stage，以及一个适应性选择性内在监督特征模块（ASISF）。ASISF使用内在监督来精准地控制和引导特征传输在多个降低stage中。</li>
<li>results: 对比 estado-of-the-art方法，SMDR-IS示出出色的性能。<details>
<summary>Abstract</summary>
Visual restoration of underwater scenes is crucial for visual tasks, and avoiding interference from underwater media has become a prominent concern. In this work, we present a synergistic multiscale detail refinement via intrinsic supervision (SMDR-IS) to recover underwater scene details. The low-degradation stage provides multiscale detail for original stage, which achieves synergistic multiscale detail refinement through feature propagation via the adaptive selective intrinsic supervised feature module (ASISF), which achieves synergistic multiscale detail refinement. ASISF is developed using intrinsic supervision to precisely control and guide feature transmission in the multi-degradation stages. ASISF improves the multiscale detail refinement while reducing interference from irrelevant scene information from the low-degradation stage. Additionally, within the multi-degradation encoder-decoder of SMDR-IS, we introduce a bifocal intrinsic-context attention module (BICA). This module is designed to effectively leverage multi-scale scene information found in images, using intrinsic supervision principles as its foundation. BICA facilitates the guidance of higher-resolution spaces by leveraging lower-resolution spaces, considering the significant dependency of underwater image restoration on spatial contextual relationships. During the training process, the network gains advantages from the integration of a multi-degradation loss function. This function serves as a constraint, enabling the network to effectively exploit information across various scales. When compared with state-of-the-art methods, SMDR-IS demonstrates its outstanding performance. Code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT视觉修复水下场景是重要的视觉任务之一，避免水下媒体干扰已成为一项显著问题。在这种工作中，我们提出了一种同时多级细节重建via自身监督（SMDR-IS），用于恢复水下场景细节。低损阶段提供多级细节 для原始阶段，通过特有的自适应选择性内在监督特征模块（ASISF）实现同时多级细节重建。ASISF通过内在监督精准控制和导引特征传输在多损阶段。ASISF提高多级细节重建，同时减少不相关场景信息的干扰。此外，在SMDR-IS中的多损阶段Encoder-Decoder中，我们引入了一种多级内在上下文注意力模块（BICA）。这个模块是基于内在监督原则设计的，用于有效利用图像中的多尺度场景信息，并且可以在训练过程中为网络提供优势。在训练过程中，网络通过多损阶段损失函数的集成获得了优势。这个函数作为约束，使网络能够有效利用多个尺度的信息。与现状技术相比，SMDR-IS表现出色。代码将公开。
</details></li>
</ul>
<hr>
<h2 id="OFVL-MS-Once-for-Visual-Localization-across-Multiple-Indoor-Scenes"><a href="#OFVL-MS-Once-for-Visual-Localization-across-Multiple-Indoor-Scenes" class="headerlink" title="OFVL-MS: Once for Visual Localization across Multiple Indoor Scenes"></a>OFVL-MS: Once for Visual Localization across Multiple Indoor Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11928">http://arxiv.org/abs/2308.11928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mooncake199809/ufvl-net">https://github.com/mooncake199809/ufvl-net</a></li>
<li>paper_authors: Tao Xie, Kun Dai, Siyi Lu, Ke Wang, Zhiqiang Jiang, Jinghan Gao, Dedong Liu, Jie Xu, Lijun Zhao, Ruifeng Li</li>
<li>for: 本 paper 的目的是 Predict camera poses across scenes with a multi-task learning manner.</li>
<li>methods: 本 paper 提出了 OFVL-MS 框架，一种可以高效地存储和精确地visual localization 的框架，通过自适应分享策略和权重学习来解决多场景集合学习中的梯度冲突问题。</li>
<li>results: 在多个benchmark上和新发布的室内 dataset LIVL 上，OFVL-MS 家族的模型显著超越了现有的状态网络，并且可以在新场景中训练 fewer parameters 而 achieve superior localization performance。<details>
<summary>Abstract</summary>
In this work, we seek to predict camera poses across scenes with a multi-task learning manner, where we view the localization of each scene as a new task. We propose OFVL-MS, a unified framework that dispenses with the traditional practice of training a model for each individual scene and relieves gradient conflict induced by optimizing multiple scenes collectively, enabling efficient storage yet precise visual localization for all scenes. Technically, in the forward pass of OFVL-MS, we design a layer-adaptive sharing policy with a learnable score for each layer to automatically determine whether the layer is shared or not. Such sharing policy empowers us to acquire task-shared parameters for a reduction of storage cost and task-specific parameters for learning scene-related features to alleviate gradient conflict. In the backward pass of OFVL-MS, we introduce a gradient normalization algorithm that homogenizes the gradient magnitude of the task-shared parameters so that all tasks converge at the same pace. Furthermore, a sparse penalty loss is applied on the learnable scores to facilitate parameter sharing for all tasks without performance degradation. We conduct comprehensive experiments on multiple benchmarks and our new released indoor dataset LIVL, showing that OFVL-MS families significantly outperform the state-of-the-arts with fewer parameters. We also verify that OFVL-MS can generalize to a new scene with much few parameters while gaining superior localization performance.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们尝试预测相机pose在场景之间，使用多任务学习方式，视每个场景为一个新任务。我们提出了OFVL-MS框架，它摒弃了传统的每个场景都需要单独训练模型的做法，并避免了多场景集合优化induced的梯度冲突，从而实现高效的存储和精确的视觉地址 localization。技术上，在OFVL-MS的前向传播中，我们设计了层adaptive共享策略，通过学习得分来自动决定每层是否共享。这种共享策略使得我们可以获得任务共享参数，以降低存储成本，同时获得任务特定参数，以学习场景相关特征，解决梯度冲突。在OFVL-MS的反向传播中，我们引入了梯度 normalization算法，使得任务共享参数的梯度大小均匀，使所有任务在同一个步长进行迭代。此外，我们还应用了稀疏罚失函数来促进参数共享，确保OFVL-MS不会影响性能。我们在多个benchmark上进行了广泛的实验，并在我们新发布的室内 dataset LIVL 上进行了测试，结果显示OFVL-MS家族在参数量少的情况下significantly outperform了状态 искус技术。我们还证明OFVL-MS可以在新场景中进行参数化，并且在获得superior localization性能的情况下，具有较少的参数。
</details></li>
</ul>
<hr>
<h2 id="Recovering-a-Molecule’s-3D-Dynamics-from-Liquid-phase-Electron-Microscopy-Movies"><a href="#Recovering-a-Molecule’s-3D-Dynamics-from-Liquid-phase-Electron-Microscopy-Movies" class="headerlink" title="Recovering a Molecule’s 3D Dynamics from Liquid-phase Electron Microscopy Movies"></a>Recovering a Molecule’s 3D Dynamics from Liquid-phase Electron Microscopy Movies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11927">http://arxiv.org/abs/2308.11927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Ye, Yuhang Wang, Hong Zhang, Yiqin Gao, Huan Wang, He Sun</li>
<li>for: 这研究旨在利用流体电子镜像技术（liquid-phase EM）观察生物分子的动态变化。</li>
<li>methods: 这研究使用了一种新的Temporal Electron MicroscoPy Object Reconstruction算法（TEMPOR），其 combining implicit neural representation（INR）和动态variational auto-encoder（DVAE）来回归时间序列中的分子结构。</li>
<li>results: 研究表明，TEMPOR算法可以从流体电子镜像电影中回归不同的动态变化，并且是首次直接从流体电子镜像电影中回归3D结构。这提供了一种有前途的新方法，用于生物分子结构生物学中研究分子的3D动态。<details>
<summary>Abstract</summary>
The dynamics of biomolecules are crucial for our understanding of their functioning in living systems. However, current 3D imaging techniques, such as cryogenic electron microscopy (cryo-EM), require freezing the sample, which limits the observation of their conformational changes in real time. The innovative liquid-phase electron microscopy (liquid-phase EM) technique allows molecules to be placed in the native liquid environment, providing a unique opportunity to observe their dynamics. In this paper, we propose TEMPOR, a Temporal Electron MicroscoPy Object Reconstruction algorithm for liquid-phase EM that leverages an implicit neural representation (INR) and a dynamical variational auto-encoder (DVAE) to recover time series of molecular structures. We demonstrate its advantages in recovering different motion dynamics from two simulated datasets, 7bcq and Cas9. To our knowledge, our work is the first attempt to directly recover 3D structures of a temporally-varying particle from liquid-phase EM movies. It provides a promising new approach for studying molecules' 3D dynamics in structural biology.
</details>
<details>
<summary>摘要</summary>
生物分子的动力学是我们理解它们在生物系统中的功能的关键。然而，现有的3D成像技术，如冷气电子镜微scopy（cryo-EM），需要将样本冻结，这限制了观察分子的拓展变化的实时观察。新的液相电子镜微scopy（液相EM）技术使分子能够置于原始液体环境中，提供了一个独特的机会来观察它们的动态。在这篇论文中，我们提出了TEMPOR，一种基于偶极神经网络表示（INR）和动态变分自动编码器（DVAE）的电子镜微scopy对象重建算法。我们在两个 simulate datasets，7bcq和Cas9 中展示了它的优势。根据我们所知，我们的工作是直接从液相EM电影中恢复3D变化的首次尝试。它提供了一个有前途的新方法，用于Structural biology中研究分子的3D动力学。
</details></li>
</ul>
<hr>
<h2 id="MixNet-Toward-Accurate-Detection-of-Challenging-Scene-Text-in-the-Wild"><a href="#MixNet-Toward-Accurate-Detection-of-Challenging-Scene-Text-in-the-Wild" class="headerlink" title="MixNet: Toward Accurate Detection of Challenging Scene Text in the Wild"></a>MixNet: Toward Accurate Detection of Challenging Scene Text in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12817">http://arxiv.org/abs/2308.12817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Xiang Zeng, Jun-Wei Hsieh, Xin Li, Ming-Ching Chang</li>
<li>for: 实时检测小型Scene文字，尤其是在不 идеal的光线和不规则位置下，具有较高的准确率和效率。</li>
<li>methods: 混合 CNN 和 Transformer 架构，包括 Feature Shuffle Network (FSNet) 和 Central Transformer Block (CTBlock)，可以实现高精度的小文字检测。</li>
<li>results: 在多个Scene文字检测 dataset 上，MixNet 已经实现了State-of-the-art 的结果，并且在不 ideal 的光线和不规则位置下具有较高的准确率和效率。<details>
<summary>Abstract</summary>
Detecting small scene text instances in the wild is particularly challenging, where the influence of irregular positions and nonideal lighting often leads to detection errors. We present MixNet, a hybrid architecture that combines the strengths of CNNs and Transformers, capable of accurately detecting small text from challenging natural scenes, regardless of the orientations, styles, and lighting conditions. MixNet incorporates two key modules: (1) the Feature Shuffle Network (FSNet) to serve as the backbone and (2) the Central Transformer Block (CTBlock) to exploit the 1D manifold constraint of the scene text. We first introduce a novel feature shuffling strategy in FSNet to facilitate the exchange of features across multiple scales, generating high-resolution features superior to popular ResNet and HRNet. The FSNet backbone has achieved significant improvements over many existing text detection methods, including PAN, DB, and FAST. Then we design a complementary CTBlock to leverage center line based features similar to the medial axis of text regions and show that it can outperform contour-based approaches in challenging cases when small scene texts appear closely. Extensive experimental results show that MixNet, which mixes FSNet with CTBlock, achieves state-of-the-art results on multiple scene text detection datasets.
</details>
<details>
<summary>摘要</summary>
通过推出 MixNet 混合体系，我们可以准确地检测自然场景中小文本实例，无论文本方向、风格或照明条件如何。 MixNet 包含两个关键模块：（1）特点混淆网络（FSNet）作为基础，以及（2）中心转换块（CTBlock）来利用场景文本的1D manifold约束。我们首先介绍了一种新的特点混淆策略，以便在多个缩放级别之间互换特点，生成高分辨率的特点，超过了流行的 ResNet 和 HRNet。FSNet 后置网络已经超越了许多现有的文本检测方法，包括 PAN、DB 和 FAST。然后，我们设计了一种补充的 CTBlock，以利用文本区域的中心线基本特征，并示出它可以在挑战性较高的情况下，当小场景文本相互靠近时，超过边框基本方法。广泛的实验结果表明，将 MixNet 混合体系与多个场景文本检测数据集进行比较，可以获得最佳结果。
</details></li>
</ul>
<hr>
<h2 id="AMSP-UOD-When-Vortex-Convolution-and-Stochastic-Perturbation-Meet-Underwater-Object-Detection"><a href="#AMSP-UOD-When-Vortex-Convolution-and-Stochastic-Perturbation-Meet-Underwater-Object-Detection" class="headerlink" title="AMSP-UOD: When Vortex Convolution and Stochastic Perturbation Meet Underwater Object Detection"></a>AMSP-UOD: When Vortex Convolution and Stochastic Perturbation Meet Underwater Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11918">http://arxiv.org/abs/2308.11918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingchun Zhou, Zongxin He, Kin-Man Lam, Yudong Wang, Weishi Zhang, ChunLe Guo, Chongyi Li</li>
<li>for: 本研究提出了一种新的干扰噪谱扩展Vortex Convolutional Network（AMSP-UOD），用于水下物体检测。AMSP-UOD特点是对水下环境中物体检测精度的影响进行了优化。</li>
<li>methods: 我们提出了AMSP Vortex Convolution（AMSP-VConv）来破坏噪声分布，提高特征提取能力，减少参数，提高网络的可靠性。此外，我们设计了Feature Association Decoupling Cross Stage Partial（FAD-CSP）模块，增强了长和短距离特征之间的强相关性，提高网络在复杂水下环境中的性能。</li>
<li>results: 我们对URPC和RUOD数据集进行了广泛的实验，结果显示，我们的方法在精度和噪声抗性方面比现有的状态切入方法更高。AMSP-UOD提出了一种创新的解决方案，具有实际应用潜在性。代码将公开发布。<details>
<summary>Abstract</summary>
In this paper, we present a novel Amplitude-Modulated Stochastic Perturbation and Vortex Convolutional Network, AMSP-UOD, designed for underwater object detection. AMSP-UOD specifically addresses the impact of non-ideal imaging factors on detection accuracy in complex underwater environments. To mitigate the influence of noise on object detection performance, we propose AMSP Vortex Convolution (AMSP-VConv) to disrupt the noise distribution, enhance feature extraction capabilities, effectively reduce parameters, and improve network robustness. We design the Feature Association Decoupling Cross Stage Partial (FAD-CSP) module, which strengthens the association of long and short-range features, improving the network performance in complex underwater environments. Additionally, our sophisticated post-processing method, based on non-maximum suppression with aspect-ratio similarity thresholds, optimizes detection in dense scenes, such as waterweed and schools of fish, improving object detection accuracy. Extensive experiments on the URPC and RUOD datasets demonstrate that our method outperforms existing state-of-the-art methods in terms of accuracy and noise immunity. AMSP-UOD proposes an innovative solution with the potential for real-world applications. Code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的振荡干扰随机激活网络（AMSP-UOD），用于水下物体检测。AMSP-UOD特点是解决水下环境中物体检测精度下降的非理想捕集因素的影响。为了减少噪声对物体检测性能的影响，我们提议AMSP激活 Vortex Convolution（AMSP-VConv），以扰乱噪声分布，提高特征提取能力，降低参数，提高网络的可靠性。我们设计了Feature Association Decoupling Cross Stage Partial（FAD-CSP）模块，强化长和短距离特征之间的关联，提高网络在复杂水下环境中的性能。此外，我们提出了一种复杂的后处理方法，基于非最大值抑制器和方向相似度阈值，以优化检测在紧凑场景中，如水蕴和鱼群，提高物体检测精度。广泛的实验表明，我们的方法在URPC和RUOD数据集上比现有状态的方法更高的准确率和噪声抗性。AMSP-UOD提出了一种创新的解决方案，具有实际应用前景。代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Aware-Implicit-Template-Learning-via-Part-Deformation-Consistency"><a href="#Semantic-Aware-Implicit-Template-Learning-via-Part-Deformation-Consistency" class="headerlink" title="Semantic-Aware Implicit Template Learning via Part Deformation Consistency"></a>Semantic-Aware Implicit Template Learning via Part Deformation Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11916">http://arxiv.org/abs/2308.11916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sihyeon Kim, Minseok Joo, Jaewon Lee, Juyeon Ko, Juhan Cha, Hyunwoo J. Kim</li>
<li>for: 本研究旨在提高无监督形态匹配中的模板学习，以便在不同物体形状下实现semantically plausible的变换。</li>
<li>methods: 本文提出了一种semantic-aware implicit template学习框架，通过自动学习的Semantic feature extractor来提供semantic prior，并通过本地conditioning和新的semantic-aware减杂码来实现semantically plausible的变换。</li>
<li>results: 对baseline方法进行了广泛的实验，并证明了提议的方法在不同任务中（包括键点传输、部分标签传输和 Texture传输）具有更高的性能。此外，我们还提供了质量分析，以验证semantic-aware减杂码的效果。代码可以在<a target="_blank" rel="noopener" href="https://github.com/mlvlab/PDC%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/mlvlab/PDC上下载。</a><details>
<summary>Abstract</summary>
Learning implicit templates as neural fields has recently shown impressive performance in unsupervised shape correspondence. Despite the success, we observe current approaches, which solely rely on geometric information, often learn suboptimal deformation across generic object shapes, which have high structural variability. In this paper, we highlight the importance of part deformation consistency and propose a semantic-aware implicit template learning framework to enable semantically plausible deformation. By leveraging semantic prior from a self-supervised feature extractor, we suggest local conditioning with novel semantic-aware deformation code and deformation consistency regularizations regarding part deformation, global deformation, and global scaling. Our extensive experiments demonstrate the superiority of the proposed method over baselines in various tasks: keypoint transfer, part label transfer, and texture transfer. More interestingly, our framework shows a larger performance gain under more challenging settings. We also provide qualitative analyses to validate the effectiveness of semantic-aware deformation. The code is available at https://github.com/mlvlab/PDC.
</details>
<details>
<summary>摘要</summary>
学习隐式模板作为神经场景，最近显示了无监督形态匹配的卓越表现。尽管成功，我们发现当前的方法，即solely rely on geometric information，经常学习低效的形态变换 across 通用物体形态，这些形态具有高度结构变动。在这篇论文中，我们强调部分形态一致性的重要性，并提出了semantic-aware implicit template learning框架，以启用semantically plausible的形态变换。通过利用自动学习的semantic prior，我们建议了本地条件化和新的semantic-aware deformation code，以及deformation consistency regularization regarding part deformation、global deformation和global scaling。我们的广泛实验表明我们的提案方法比基eline在各种任务中表现出色：键点传输、部件标签传输和 Texture传输。更有趣的是，我们的框架在更加复杂的设置下表现出更大的性能提升。我们还提供了质量分析，以验证semantic-aware deformation的效果。代码可以在https://github.com/mlvlab/PDC上获取。
</details></li>
</ul>
<hr>
<h2 id="ACLS-Adaptive-and-Conditional-Label-Smoothing-for-Network-Calibration"><a href="#ACLS-Adaptive-and-Conditional-Label-Smoothing-for-Network-Calibration" class="headerlink" title="ACLS: Adaptive and Conditional Label Smoothing for Network Calibration"></a>ACLS: Adaptive and Conditional Label Smoothing for Network Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11911">http://arxiv.org/abs/2308.11911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyekang Park, Jongyoun Noh, Youngmin Oh, Donghyeon Baek, Bumsub Ham</li>
<li>for: 本研究旨在解决深度神经网络的调整误差问题，提高神经网络的报告精度。</li>
<li>methods: 该研究使用了现有的正则化基于方法，并进行了深入分析，以便更好地理解这些方法对神经网络调整的影响。</li>
<li>results: 研究人员通过实验证明，新引入的损失函数ACLS可以兼顾现有正则化方法的优点，并避免其缺点。这种损失函数在图像分类和 semantic segmentation 中具有广泛的应用前景。<details>
<summary>Abstract</summary>
We address the problem of network calibration adjusting miscalibrated confidences of deep neural networks. Many approaches to network calibration adopt a regularization-based method that exploits a regularization term to smooth the miscalibrated confidences. Although these approaches have shown the effectiveness on calibrating the networks, there is still a lack of understanding on the underlying principles of regularization in terms of network calibration. We present in this paper an in-depth analysis of existing regularization-based methods, providing a better understanding on how they affect to network calibration. Specifically, we have observed that 1) the regularization-based methods can be interpreted as variants of label smoothing, and 2) they do not always behave desirably. Based on the analysis, we introduce a novel loss function, dubbed ACLS, that unifies the merits of existing regularization methods, while avoiding the limitations. We show extensive experimental results for image classification and semantic segmentation on standard benchmarks, including CIFAR10, Tiny-ImageNet, ImageNet, and PASCAL VOC, demonstrating the effectiveness of our loss function.
</details>
<details>
<summary>摘要</summary>
我们考虑了深度神经网络的准确率调整问题，有许多使用常规化方法来调整神经网络的方法。虽然这些方法有效地调整神经网络，但是还没有很好地理解这些常规化方法在神经网络调整中的下面原理。在这篇论文中，我们提供了对现有常规化方法的深入分析，从而更好地理解它们如何影响神经网络调整。 Specifically, we have observed that 1) 常规化方法可以被视为变种的标签平滑，和 2) 它们不总是愿望的。基于分析，我们提出了一种新的损失函数，名为 ACLS，它结合了现有常规化方法的优点，而避免了其限制。我们在标准的benchmark上，包括CIFAR10、Tiny-ImageNet、ImageNet和PASCAL VOC，进行了广泛的实验，并证明了我们的损失函数的效果。
</details></li>
</ul>
<hr>
<h2 id="Edge-aware-Hard-Clustering-Graph-Pooling-for-Brain-Imaging-Data"><a href="#Edge-aware-Hard-Clustering-Graph-Pooling-for-Brain-Imaging-Data" class="headerlink" title="Edge-aware Hard Clustering Graph Pooling for Brain Imaging Data"></a>Edge-aware Hard Clustering Graph Pooling for Brain Imaging Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11909">http://arxiv.org/abs/2308.11909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Zhu, Jiayi Zhu, Lijuan Zhang, Xi Wu, Shuqi Yang, Ping Liang, Honghan Chen, Ying Tan</li>
<li>for: This paper aims to develop a deep learning method for probing different types of abnormal functional brain networks from a data-driven perspective.</li>
<li>methods: The proposed method, called Edge-aware hard clustering graph pooling (EHCPool), uses a clustering graph pooling method that supports multidimensional edge features and assesses node feature significance based on edge features. It also uses a novel Iteration n-top strategy to adaptively learn sparse hard clustering assignments for graphs, and an innovative N-E Aggregation strategy to aggregate node and edge feature information in each independent subgraph.</li>
<li>results: The proposed model was evaluated on multi-site brain imaging public datasets and yielded state-of-the-art performance.<details>
<summary>Abstract</summary>
Graph Convolutional Networks (GCNs) can capture non-Euclidean spatial dependence between different brain regions, and the graph pooling operator in GCNs is key to enhancing the representation learning capability and acquiring abnormal brain maps. However, the majority of existing research designs graph pooling operators only from the perspective of nodes while disregarding the original edge features, in a way that not only confines graph pooling application scenarios, but also diminishes its ability to capture critical substructures. In this study, a clustering graph pooling method that first supports multidimensional edge features, called Edge-aware hard clustering graph pooling (EHCPool), is developed. EHCPool proposes the first 'Edge-to-node' score evaluation criterion based on edge features to assess node feature significance. To more effectively capture the critical subgraphs, a novel Iteration n-top strategy is further designed to adaptively learn sparse hard clustering assignments for graphs. Subsequently, an innovative N-E Aggregation strategy is presented to aggregate node and edge feature information in each independent subgraph. The proposed model was evaluated on multi-site brain imaging public datasets and yielded state-of-the-art performance. We believe this method is the first deep learning tool with the potential to probe different types of abnormal functional brain networks from data-driven perspective.
</details>
<details>
<summary>摘要</summary>
格子卷积网络（GCNs）可以捕捉不同脑区之间的非欧几何空间相互关系，并且图 pooling 运算在 GCNs 中是关键来提高表示学习能力和获得异常脑图。然而，现有大多数研究只从节点的角度设计图 pooling 操作符，而忽视原始边特征，这不仅限制了图 pooling 应用场景，而且减少了其捕捉关键子结构的能力。在这种研究中，我们开发了一种集群图 pooling 方法，称为 Edge-aware hard clustering graph pooling（EHCPool）。EHCPool 首先支持多维边特征，并提出了基于边特征的 'Edge-to-node' 分数评价标准来评估节点特征重要性。为更好地捕捉关键子图，我们还设计了一种 novel Iteration n-top 策略，以适应性地学习 sparse hard clustering 分配。然后，我们提出了一种 innovative N-E Aggregation 策略，用于在每个独立子图中集成节点和边特征信息。我们的模型在多地点脑成像公共数据集上进行了评估，并实现了状态前的性能。我们认为这是深度学习工具的第一个能够从数据驱动的方式探索不同类型的异常功能脑网络的方法。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Data-Perturbation-and-Model-Stabilization-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Rethinking-Data-Perturbation-and-Model-Stabilization-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation"></a>Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11903">http://arxiv.org/abs/2308.11903</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenzhao/dpms">https://github.com/zhenzhao/dpms</a></li>
<li>paper_authors: Zhen Zhao, Ye Liu, Meng Zhao, Di Yin, Yixuan Yuan, Luping Zhou</li>
<li>for: 这篇论文目的是提高 semi-supervised medical image segmentation (SSMIS) 的性能。</li>
<li>methods: 本文提出了一个简单 yet effective 的方法，名为 DPMS，以生成大量适当的预测不同，以提高 SSMIS 的性能。DPMS 使用了教师-学生架构，并运用了标准的监督损失和无监督一致损失。</li>
<li>results: DPMS 可以实现新的顶尖性能在公共 2D ACDC 和 3D LA 数据集上，在不同的半supervised 设定下。例如，DPMS 在 ACDC 上比前一代 SOTA 提高了22.62%。<details>
<summary>Abstract</summary>
Studies on semi-supervised medical image segmentation (SSMIS) have seen fast progress recently. Due to the limited labelled data, SSMIS methods mainly focus on effectively leveraging unlabeled data to enhance the segmentation performance. However, despite their promising performance, current state-of-the-art methods often prioritize integrating complex techniques and loss terms rather than addressing the core challenges of semi-supervised scenarios directly. We argue that the key to SSMIS lies in generating substantial and appropriate prediction disagreement on unlabeled data. To this end, we emphasize the crutiality of data perturbation and model stabilization in semi-supervised segmentation, and propose a simple yet effective approach to boost SSMIS performance significantly, dubbed DPMS. Specifically, we first revisit SSMIS from three distinct perspectives: the data, the model, and the loss, and conduct a comprehensive study of corresponding strategies to examine their effectiveness. Based on these examinations, we then propose DPMS, which adopts a plain teacher-student framework with a standard supervised loss and unsupervised consistency loss. To produce appropriate prediction disagreements, DPMS perturbs the unlabeled data via strong augmentations to enlarge prediction disagreements considerably. On the other hand, using EMA teacher when strong augmentation is applied does not necessarily improve performance. DPMS further utilizes a forwarding-twice and momentum updating strategies for normalization statistics to stabilize the training on unlabeled data effectively. Despite its simplicity, DPMS can obtain new state-of-the-art performance on the public 2D ACDC and 3D LA datasets across various semi-supervised settings, e.g. obtaining a remarkable 22.62% improvement against previous SOTA on ACDC with 5% labels.
</details>
<details>
<summary>摘要</summary>
研究 semi-supervised medical image segmentation (SSMIS) 在近期已经进展很快。由于有限的标签数据，SSMIS 方法主要是利用无标注数据来提高 segmentation 性能。然而，当前状态的艺术方法frequently强调混合复杂的技术和损失函数，而不是直接面临 semi-supervised 场景的核心挑战。我们认为，SSMIS 的关键在于生成足够和适当的预测差异在无标注数据上。为此，我们强调了数据抖动和模型稳定在 semi-supervised 分割中的重要性，并提出了一种简单 yet effective 的方法，称为 DPMS。 Specifically, we first revisit SSMIS from three distinct perspectives: the data, the model, and the loss, and conduct a comprehensive study of corresponding strategies to examine their effectiveness. Based on these examinations, we then propose DPMS, which adopts a plain teacher-student framework with a standard supervised loss and unsupervised consistency loss. To produce appropriate prediction disagreements, DPMS perturbs the unlabeled data via strong augmentations to enlarge prediction disagreements considerably. On the other hand, using EMA teacher when strong augmentation is applied does not necessarily improve performance. DPMS further utilizes a forwarding-twice and momentum updating strategies for normalization statistics to stabilize the training on unlabeled data effectively. Despite its simplicity, DPMS can obtain new state-of-the-art performance on the public 2D ACDC and 3D LA datasets across various semi-supervised settings, e.g. obtaining a remarkable 22.62% improvement against previous SOTA on ACDC with 5% labels.
</details></li>
</ul>
<hr>
<h2 id="Camera-Driven-Representation-Learning-for-Unsupervised-Domain-Adaptive-Person-Re-identification"><a href="#Camera-Driven-Representation-Learning-for-Unsupervised-Domain-Adaptive-Person-Re-identification" class="headerlink" title="Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-identification"></a>Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11901">http://arxiv.org/abs/2308.11901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geon Lee, Sanghoon Lee, Dohyung Kim, Younghoon Shin, Yongsang Yoon, Bumsub Ham</li>
<li>for: 这个论文是为了解决人识别领域中的频道适应问题，即将源频道上训练的模型转移到目标频道上，而不需要目标频道的标注数据。</li>
<li>methods: 这个论文提出了一种基于摄像头标签的curriculum学习框架，通过逐渐使用不同摄像头的数据集来进行学习，从而将源频道上的知识传播到目标频道上。</li>
<li>results: 实验结果表明，该方法可以在标本频道和Synthetic-to-real场景中实现高效的人识别，并且可以减少摄像头偏见问题。<details>
<summary>Abstract</summary>
We present a novel unsupervised domain adaption method for person re-identification (reID) that generalizes a model trained on a labeled source domain to an unlabeled target domain. We introduce a camera-driven curriculum learning (CaCL) framework that leverages camera labels of person images to transfer knowledge from source to target domains progressively. To this end, we divide target domain dataset into multiple subsets based on the camera labels, and initially train our model with a single subset (i.e., images captured by a single camera). We then gradually exploit more subsets for training, according to a curriculum sequence obtained with a camera-driven scheduling rule. The scheduler considers maximum mean discrepancies (MMD) between each subset and the source domain dataset, such that the subset closer to the source domain is exploited earlier within the curriculum. For each curriculum sequence, we generate pseudo labels of person images in a target domain to train a reID model in a supervised way. We have observed that the pseudo labels are highly biased toward cameras, suggesting that person images obtained from the same camera are likely to have the same pseudo labels, even for different IDs. To address the camera bias problem, we also introduce a camera-diversity (CD) loss encouraging person images of the same pseudo label, but captured across various cameras, to involve more for discriminative feature learning, providing person representations robust to inter-camera variations. Experimental results on standard benchmarks, including real-to-real and synthetic-to-real scenarios, demonstrate the effectiveness of our framework.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的无监督领域适应方法，用于人重识别（reID），可以将源频率域上学习的模型映射到目标频率域上。我们引入了一个摄像头驱动的课程学习（CaCL）框架，利用摄像头标签来从源频率域传递知识到目标频率域。为此，我们将目标频率域数据集分成多个子集，并在每个子集上进行逐步训练，按照一个摄像头驱动的时间序列来调度。时间序列中的每一个子集都是根据摄像头标签与源频率域数据集的最大平均差（MMD）进行选择的。为了在目标频率域上训练reID模型，我们生成了一系列pseudo标签，用于在目标频率域上进行supervised学习。我们发现pseudo标签具有强烈的摄像头偏好， sugggesting that人像图像来自同一摄像头的人是可能具有相同的pseudo标签，即使这些人图像是不同的ID。为了解决摄像头偏好问题，我们还引入了一个摄像头多样性（CD）损失，强制人像图像同一pseudo标签，但来自不同摄像头的人图像，参与更多的特征学习，以提供对摄像头变化的人表示robust。我们的实验结果表明，我们的框架在标准benchmark上得到了显著的效果，包括实际到实际和 sintetic到实际的场景。
</details></li>
</ul>
<hr>
<h2 id="HashReID-Dynamic-Network-with-Binary-Codes-for-Efficient-Person-Re-identification"><a href="#HashReID-Dynamic-Network-with-Binary-Codes-for-Efficient-Person-Re-identification" class="headerlink" title="HashReID: Dynamic Network with Binary Codes for Efficient Person Re-identification"></a>HashReID: Dynamic Network with Binary Codes for Efficient Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11900">http://arxiv.org/abs/2308.11900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshitij Nikhal, Yujunrong Ma, Shuvra S. Bhattacharyya, Benjamin S. Riggan</li>
<li>for: 这个研究旨在提高人预识系统（ReID）的实际应用，特别是在能源有限的设备上。</li>
<li>methods: 我们提出了一个入口适应网络，具有多个终端封顶，可以在搜寻是简单或是噪声时中止计算，从而大幅降低计算量。我们还引入了一个基于时间的分类器，并运用了一个新的训练策略。此外，我们还采用了一个二进制对称码生成方法，从而大幅改善搜寻过程。</li>
<li>results: 我们的提案可以在Market1501数据集上降低了超过70%的样本使用对称码，从而降低了网络的计算成本80%，并与其他对称码基础的方法相比提高了60%。这些结果表明了我们的方法具有重要的改善，并且与传统ReID方法相似的精度性表现。<details>
<summary>Abstract</summary>
Biometric applications, such as person re-identification (ReID), are often deployed on energy constrained devices. While recent ReID methods prioritize high retrieval performance, they often come with large computational costs and high search time, rendering them less practical in real-world settings. In this work, we propose an input-adaptive network with multiple exit blocks, that can terminate computation early if the retrieval is straightforward or noisy, saving a lot of computation. To assess the complexity of the input, we introduce a temporal-based classifier driven by a new training strategy. Furthermore, we adopt a binary hash code generation approach instead of relying on continuous-valued features, which significantly improves the search process by a factor of 20. To ensure similarity preservation, we utilize a new ranking regularizer that bridges the gap between continuous and binary features. Extensive analysis of our proposed method is conducted on three datasets: Market1501, MSMT17 (Multi-Scene Multi-Time), and the BGC1 (BRIAR Government Collection). Using our approach, more than 70% of the samples with compact hash codes exit early on the Market1501 dataset, saving 80% of the networks computational cost and improving over other hash-based methods by 60%. These results demonstrate a significant improvement over dynamic networks and showcase comparable accuracy performance to conventional ReID methods. Code will be made available.
</details>
<details>
<summary>摘要</summary>
“生物emetric应用程序（ReID）经常在能源有限的设备上部署。而现代ReID方法则倾向于优先高回传率，但是这些方法往往具有较高的计算成本和搜寻时间，使其在实际应用中不太实际。在这个工作中，我们提议一个输入适应网络，该网络可以根据输入的复杂度进行计算节省。为了评估输入的复杂度，我们引入了一个基于时间的分类器，这个分类器驱动了一个新的训练策略。此外，我们还采用了一个二进制对应码生成方法，而不是依赖连续值的特征。这种方法可以很大幅度地改善搜寻过程，提高比例为20倍。确保相似性保持，我们利用一个新的排名调整仪，它可以跨越连续和二进制特征之间的差异。我们对 Market1501、MSMT17（多个场景多个时间）和 BGC1（BRIAR政府收集）三个数据集进行了广泛的分析。使用我们的方法，Market1501上的超过70%的样本可以在输入简短的情况下提前退出，实现80%的网络计算成本的减少，并且与其他对应码基本方法相比，提高了60%.这些结果表明我们的方法具有优秀的改善，并且与传统ReID方法相似的准确性表现。我们将会公开代码。”
</details></li>
</ul>
<hr>
<h2 id="Age-Prediction-From-Face-Images-Via-Contrastive-Learning"><a href="#Age-Prediction-From-Face-Images-Via-Contrastive-Learning" class="headerlink" title="Age Prediction From Face Images Via Contrastive Learning"></a>Age Prediction From Face Images Via Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11896">http://arxiv.org/abs/2308.11896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeongnam Chae, Poulami Raha, Mijung Kim, Bjorn Stenger</li>
<li>for:  accurately estimating age from face images</li>
<li>methods:  contrastive learning to extract age-related features, combining cosine similarity and triplet margin losses to suppress identity-related features</li>
<li>results:  achieved state-of-the-art performance on two public datasets, FG-NET and MORPH-II<details>
<summary>Abstract</summary>
This paper presents a novel approach for accurately estimating age from face images, which overcomes the challenge of collecting a large dataset of individuals with the same identity at different ages. Instead, we leverage readily available face datasets of different people at different ages and aim to extract age-related features using contrastive learning. Our method emphasizes these relevant features while suppressing identity-related features using a combination of cosine similarity and triplet margin losses. We demonstrate the effectiveness of our proposed approach by achieving state-of-the-art performance on two public datasets, FG-NET and MORPH-II.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的方法，用于准确地从面像中估算年龄，解决了收集大量同一个人的年龄不同的数据的挑战。而是利用可得到的不同人种的年龄面像数据，并尝试EXTRACT年龄相关特征使用对比学习。我们的方法强调这些相关特征，同时压制身份相关特征使用抽象相似性和三重margin损失。我们在两个公共数据集FG-NET和MORPH-II上进行了实验，并达到了状态的表现。
</details></li>
</ul>
<hr>
<h2 id="Does-Physical-Adversarial-Example-Really-Matter-to-Autonomous-Driving-Towards-System-Level-Effect-of-Adversarial-Object-Evasion-Attack"><a href="#Does-Physical-Adversarial-Example-Really-Matter-to-Autonomous-Driving-Towards-System-Level-Effect-of-Adversarial-Object-Evasion-Attack" class="headerlink" title="Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack"></a>Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11894">http://arxiv.org/abs/2308.11894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ningfei Wang, Yunpeng Luo, Takami Sato, Kaidi Xu, Qi Alfred Chen</li>
<li>for: 本研究旨在探讨physical adversarial object evasion攻击在自动驾驶（AD）中的安全性问题，特别是对于AD系统的全面性和上下文。</li>
<li>methods: 我们使用现有的AD系统和Physical Adversarial Attack（PAA）技术，并对现有的攻击方法进行了改进和扩展，以实现更高效的攻击效果。</li>
<li>results: 我们的研究结果显示，现有的攻击方法无法实现系统级别的攻击效果（如违反交通规则）在实际AD上。我们还发现了两个设计限制：1）物理模型与像素抽象不匹配，2）缺乏车辆植物模型和AD系统模型考虑。我们提出了SysAdv，一种基于系统的攻击设计，并证明了它可以显著提高攻击效果，即违反率提高约70%。<details>
<summary>Abstract</summary>
In autonomous driving (AD), accurate perception is indispensable to achieving safe and secure driving. Due to its safety-criticality, the security of AD perception has been widely studied. Among different attacks on AD perception, the physical adversarial object evasion attacks are especially severe. However, we find that all existing literature only evaluates their attack effect at the targeted AI component level but not at the system level, i.e., with the entire system semantics and context such as the full AD pipeline. Thereby, this raises a critical research question: can these existing researches effectively achieve system-level attack effects (e.g., traffic rule violations) in the real-world AD context? In this work, we conduct the first measurement study on whether and how effectively the existing designs can lead to system-level effects, especially for the STOP sign-evasion attacks due to their popularity and severity. Our evaluation results show that all the representative prior works cannot achieve any system-level effects. We observe two design limitations in the prior works: 1) physical model-inconsistent object size distribution in pixel sampling and 2) lack of vehicle plant model and AD system model consideration. Then, we propose SysAdv, a novel system-driven attack design in the AD context and our evaluation results show that the system-level effects can be significantly improved, i.e., the violation rate increases by around 70%.
</details>
<details>
<summary>摘要</summary>
自动驾驶（AD）中精准感知是安全驾驶的关键。由于其安全性的重要性，AD感知的安全性已经得到了广泛的研究。 amongst different AD感知攻击，物理对抗对象逃脱攻击最为严重。然而，我们发现所有的文献都只评估了这些攻击的目标AI组件级别的影响，而不是整个系统的 semantics和context，例如整个AD管道。这引出了一个关键的研究问题：现有的研究是否可以在实际的AD上实现系统级别的效果？在这种工作中，我们进行了首次的测量研究，以确定现有的设计是否可以在AD上实现系统级别的效果，特别是STOP标志逃脱攻击的情况。我们的评估结果表明，所有代表性的先前工作都无法实现任何系统级别的效果。我们发现了两个设计 limitation：1）物理模型不一致的对象大小分布在像素抽样中，2）缺乏车辆植物模型和AD系统模型考虑。然后，我们提出了SysAdv，一种基于系统的攻击设计在AD上。我们的评估结果表明，可以显著提高系统级别的效果，即违反率提高约70%。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Framework-for-3D-Point-Cloud-Visual-Grounding"><a href="#A-Unified-Framework-for-3D-Point-Cloud-Visual-Grounding" class="headerlink" title="A Unified Framework for 3D Point Cloud Visual Grounding"></a>A Unified Framework for 3D Point Cloud Visual Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11887">http://arxiv.org/abs/2308.11887</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leon1207/3dreftr">https://github.com/leon1207/3dreftr</a></li>
<li>paper_authors: Haojia Lin, Yongdong Luo, Xiawu Zheng, Lijiang Li, Fei Chao, Taisong Jin, Donghao Luo, Chengjie Wang, Yan Wang, Liujuan Cao</li>
<li>for: 本研究旨在提出一个统一的3D参考框架，协助3DScene理解和3D Referring Expression Comprehension (3DREC)。</li>
<li>methods: 本研究使用3D Referring Transformer (3DRefTR) 框架，具有双重功能：一是利用3DREC模型生成高分辨率的visual tokens，二是通过对superpoint进行组合，以提高3DRES的性能。</li>
<li>results: 实验结果显示，3DRefTR在ScanRefer dataset上比前一代3DRES方法提高12.43%的mIoU，并比前一代3DREC方法提高0.6%的<a href="mailto:&#65;&#99;&#99;&#64;&#x30;&#x2e;&#x32;&#53;&#73;&#111;&#x55;">&#65;&#99;&#99;&#64;&#x30;&#x2e;&#x32;&#53;&#73;&#111;&#x55;</a>。<details>
<summary>Abstract</summary>
3D point cloud visual grounding plays a critical role in 3D scene comprehension, encompassing 3D referring expression comprehension (3DREC) and segmentation (3DRES). We argue that 3DREC and 3DRES should be unified in one framework, which is also a natural progression in the community. To explain, 3DREC can help 3DRES locate the referent, while 3DRES can also facilitate 3DREC via more finegrained language-visual alignment. To achieve this, this paper takes the initiative step to integrate 3DREC and 3DRES into a unified framework, termed 3D Referring Transformer (3DRefTR). Its key idea is to build upon a mature 3DREC model and leverage ready query embeddings and visual tokens from the 3DREC model to construct a dedicated mask branch. Specially, we propose Superpoint Mask Branch, which serves a dual purpose: i) By leveraging the heterogeneous CPU-GPU parallelism, while the GPU is occupied generating visual tokens, the CPU concurrently produces superpoints, equivalently accomplishing the upsampling computation; ii) By harnessing on the inherent association between the superpoints and point cloud, it eliminates the heavy computational overhead on the high-resolution visual features for upsampling. This elegant design enables 3DRefTR to achieve both well-performing 3DRES and 3DREC capacities with only a 6% additional latency compared to the original 3DREC model. Empirical evaluations affirm the superiority of 3DRefTR. Specifically, on the ScanRefer dataset, 3DRefTR surpasses the state-of-the-art 3DRES method by 12.43% in mIoU and improves upon the SOTA 3DREC method by 0.6% Acc@0.25IoU.
</details>
<details>
<summary>摘要</summary>
三维点云视觉基础LAYER plays a critical role in 3D scene comprehension, including 3D referring expression comprehension (3DREC) and segmentation (3DRES). We argue that 3DREC and 3DRES should be unified in one framework, which is also a natural progression in the community. To explain, 3DREC can help 3DRES locate the referent, while 3DRES can also facilitate 3DREC via more finegrained language-visual alignment. To achieve this, this paper takes the initiative step to integrate 3DREC and 3DRES into a unified framework, termed 3D Referring Transformer (3DRefTR). Its key idea is to build upon a mature 3DREC model and leverage ready query embeddings and visual tokens from the 3DREC model to construct a dedicated mask branch. Specially, we propose Superpoint Mask Branch, which serves a dual purpose: i) By leveraging the heterogeneous CPU-GPU parallelism, while the GPU is occupied generating visual tokens, the CPU concurrently produces superpoints, equivalently accomplishing the upsampling computation; ii) By harnessing on the inherent association between the superpoints and point cloud, it eliminates the heavy computational overhead on the high-resolution visual features for upsampling. This elegant design enables 3DRefTR to achieve both well-performing 3DRES and 3DREC capacities with only a 6% additional latency compared to the original 3DREC model. Empirical evaluations affirm the superiority of 3DRefTR. Specifically, on the ScanRefer dataset, 3DRefTR surpasses the state-of-the-art 3DRES method by 12.43% in mIoU and improves upon the SOTA 3DREC method by 0.6% Acc@0.25IoU.
</details></li>
</ul>
<hr>
<h2 id="SUMMIT-Source-Free-Adaptation-of-Uni-Modal-Models-to-Multi-Modal-Targets"><a href="#SUMMIT-Source-Free-Adaptation-of-Uni-Modal-Models-to-Multi-Modal-Targets" class="headerlink" title="SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets"></a>SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11880">http://arxiv.org/abs/2308.11880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csimo005/summit">https://github.com/csimo005/summit</a></li>
<li>paper_authors: Cody Simons, Dripta S. Raychaudhuri, Sk Miraj Ahmed, Suya You, Konstantinos Karydis, Amit K. Roy-Chowdhury</li>
<li>for: 这篇论文的目的是提出一个可以在多种情况下进行Scene Understanding的方法，并且可以适应不同的数据分布而不需要实际的数据标注。</li>
<li>methods: 这篇论文使用了一个 switching 框架，它可以自动选择两种跨模式的 pseudo-label 融合方法（agreement filtering 和 entropy weighting），以适应目标领域中的数据分布。</li>
<li>results: 这篇论文的实验结果显示，这个方法可以在七个问题中 achieved results comparable to, 和在一些情况下甚至超过了可以接触到源数据的方法。实验结果显示，这个方法可以提高 mIoU 的表现，最高提高幅度达12%。<details>
<summary>Abstract</summary>
Scene understanding using multi-modal data is necessary in many applications, e.g., autonomous navigation. To achieve this in a variety of situations, existing models must be able to adapt to shifting data distributions without arduous data annotation. Current approaches assume that the source data is available during adaptation and that the source consists of paired multi-modal data. Both these assumptions may be problematic for many applications. Source data may not be available due to privacy, security, or economic concerns. Assuming the existence of paired multi-modal data for training also entails significant data collection costs and fails to take advantage of widely available freely distributed pre-trained uni-modal models. In this work, we relax both of these assumptions by addressing the problem of adapting a set of models trained independently on uni-modal data to a target domain consisting of unlabeled multi-modal data, without having access to the original source dataset. Our proposed approach solves this problem through a switching framework which automatically chooses between two complementary methods of cross-modal pseudo-label fusion -- agreement filtering and entropy weighting -- based on the estimated domain gap. We demonstrate our work on the semantic segmentation problem. Experiments across seven challenging adaptation scenarios verify the efficacy of our approach, achieving results comparable to, and in some cases outperforming, methods which assume access to source data. Our method achieves an improvement in mIoU of up to 12% over competing baselines. Our code is publicly available at https://github.com/csimo005/SUMMIT.
</details>
<details>
<summary>摘要</summary>
Scene理解使用多Modal数据是必需的许多应用程序中，例如自主导航。为了实现这种多种情况下，现有的模型需要能够适应数据分布的变化，而不需要费力数据注释。现有的方法假设源数据可以在适应过程中获得，并且假设源数据是paired multiModal数据。这两个假设可能会成为许多应用程序的问题。源数据可能不可用due to privacy, security, or economic concerns。假设存在paired multiModal数据进行训练也会导致数据收集成本增加，并且不使用可以获得的广泛分布的自由训练uniModal模型。在这种工作中，我们放弃了这两个假设，通过对策 Switching framework来解决在目标领域中使用独立训练在uniModal数据上的模型适应问题，不需要访问原始源数据集。我们的提出的方法通过自动选择 Agreement filtering和Entropy weighting两种跨Modal pseudo-label fusions的方法来解决这个问题，根据估算的领域漏洞。我们在Semantic segmentation问题上进行了实验，并在七个困难的适应场景中证明了我们的方法的效果，与访问源数据的方法相当，甚至在一些场景下超越了这些方法。我们的方法在mIoU上提高了12%以上 compared to竞争对手。我们的代码可以在https://github.com/csimo005/SUMMIT上获取。
</details></li>
</ul>
<hr>
<h2 id="Motion-to-Matching-A-Mixed-Paradigm-for-3D-Single-Object-Tracking"><a href="#Motion-to-Matching-A-Mixed-Paradigm-for-3D-Single-Object-Tracking" class="headerlink" title="Motion-to-Matching: A Mixed Paradigm for 3D Single Object Tracking"></a>Motion-to-Matching: A Mixed Paradigm for 3D Single Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11875">http://arxiv.org/abs/2308.11875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leozhiheng/mtm-tracker">https://github.com/leozhiheng/mtm-tracker</a></li>
<li>paper_authors: Zhiheng Li, Yu Lin, Yubo Cui, Shuo Li, Zheng Fang</li>
<li>for: 本文主要针对3D单个目标跟踪问题，提出了一种基于LiDAR点云的混合方法，即MTM-Tracker。</li>
<li>methods: 该方法包括两个阶段，首先使用历史桶的连续性来模型目标的运动，然后通过特征相互交互模块提取连续点云中的运动相关特征，并与之前的特征进行匹配以 refine 目标运动和其他目标状态。</li>
<li>results: 广泛的实验表明，MTM-Tracker 在大规模数据集（KITTI和NuScenes）上达到了竞争性表现（70.9% 和 51.70%）。<details>
<summary>Abstract</summary>
3D single object tracking with LiDAR points is an important task in the computer vision field. Previous methods usually adopt the matching-based or motion-centric paradigms to estimate the current target status. However, the former is sensitive to the similar distractors and the sparseness of point cloud due to relying on appearance matching, while the latter usually focuses on short-term motion clues (eg. two frames) and ignores the long-term motion pattern of target. To address these issues, we propose a mixed paradigm with two stages, named MTM-Tracker, which combines motion modeling with feature matching into a single network. Specifically, in the first stage, we exploit the continuous historical boxes as motion prior and propose an encoder-decoder structure to locate target coarsely. Then, in the second stage, we introduce a feature interaction module to extract motion-aware features from consecutive point clouds and match them to refine target movement as well as regress other target states. Extensive experiments validate that our paradigm achieves competitive performance on large-scale datasets (70.9% in KITTI and 51.70% in NuScenes). The code will be open soon at https://github.com/LeoZhiheng/MTM-Tracker.git.
</details>
<details>
<summary>摘要</summary>
“3D单目标追踪使用LiDAR点cloud是计算机见识领域中重要的任务。先前的方法通常运用匹配基本或动作中心派生估算目标状态。然而，前者受到相似的余杂物和点云 sparse 的影响，导致依赖于外观匹配而较为敏感；而后者通常专注于短期动作指标（如两帧），忽略了长期动作模式的目标。为了解决这些问题，我们提出了一种混合 paradigma 的两个阶段方法，名为 MTM-Tracker，它结合了运动模型和特征匹配到单一的网络中。具体来说，在第一阶段，我们利用独特的历史盒子作为运动假设，并提出了Encoder-Decoder结构来粗略定位目标。然后，在第二阶段，我们引入了运动相互作用模块，从 consecutives point clouds 中提取动作感知特征，并与其他 point clouds 进行匹配以精确地评估目标运动，同时预测其他目标状态。广泛的实验证明了我们的方法在大规模数据集（KITTI 70.9%和NuScenes 51.70%）中具有竞争性的表现。代码将将在https://github.com/LeoZhiheng/MTM-Tracker.git 中开源。”
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Learning-via-Weight-aware-Distillation-under-Class-Distribution-Mismatch"><a href="#Semi-Supervised-Learning-via-Weight-aware-Distillation-under-Class-Distribution-Mismatch" class="headerlink" title="Semi-Supervised Learning via Weight-aware Distillation under Class Distribution Mismatch"></a>Semi-Supervised Learning via Weight-aware Distillation under Class Distribution Mismatch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11874">http://arxiv.org/abs/2308.11874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pan Du, Suyun Zhao, Zisen Sheng, Cuiping Li, Hong Chen</li>
<li>for: 这个研究目的是提出一种robust semi-supervised learning（SSL）框架，以便在分布不对称的情况下提高SSL的性能。</li>
<li>methods: 本研究使用了Weight-Aware Distillation（WAD）框架，通过设置了适当的权重，将有用的知识传递到目标类别器中，以提高SSL的性能。</li>
<li>results: 实验结果显示，WAD比五种现有的SSL方法和一个基准方法在CIFAR10和CIFAR100类别Dataset上表现更好，并且在人工跨dataset上也获得了良好的结果。<details>
<summary>Abstract</summary>
Semi-Supervised Learning (SSL) under class distribution mismatch aims to tackle a challenging problem wherein unlabeled data contain lots of unknown categories unseen in the labeled ones. In such mismatch scenarios, traditional SSL suffers severe performance damage due to the harmful invasion of the instances with unknown categories into the target classifier. In this study, by strict mathematical reasoning, we reveal that the SSL error under class distribution mismatch is composed of pseudo-labeling error and invasion error, both of which jointly bound the SSL population risk. To alleviate the SSL error, we propose a robust SSL framework called Weight-Aware Distillation (WAD) that, by weights, selectively transfers knowledge beneficial to the target task from unsupervised contrastive representation to the target classifier. Specifically, WAD captures adaptive weights and high-quality pseudo labels to target instances by exploring point mutual information (PMI) in representation space to maximize the role of unlabeled data and filter unknown categories. Theoretically, we prove that WAD has a tight upper bound of population risk under class distribution mismatch. Experimentally, extensive results demonstrate that WAD outperforms five state-of-the-art SSL approaches and one standard baseline on two benchmark datasets, CIFAR10 and CIFAR100, and an artificial cross-dataset. The code is available at https://github.com/RUC-DWBI-ML/research/tree/main/WAD-master.
</details>
<details>
<summary>摘要</summary>
半监督学习（SSL）在类分布不匹配场景下面临着一个具有挑战性的问题，那就是无标签数据中含有很多未知类别，这些类别不在标签数据中出现过。在这种场景下，传统的SSL表现糟糕，这是因为未知类别的实例侵入了目标分类器，从而导致SSL错误的增加。在这项研究中，通过严格的数学推理，我们发现SSL错误在类分布不匹配情况下是由pseudo-labeling错误和入侵错误两者相互绑定的。为了减轻SSL错误，我们提出了一种可靠SSL框架calledWeight-Aware Distillation（WAD）。WAD通过权重来选择ively传输无标签数据中有助于目标任务的知识到目标分类器。具体来说，WAD捕捉适应性权重和高质量pseudo标签，以便target实例通过在表示空间中探索点对应信息（PMI）来最大化无标签数据的作用，并过滤未知类别。理论上，我们证明WAD在类分布不匹配情况下具有紧binding的人口风险。实验证明，WAD在CIFAR10和CIFAR100两个benchmarkdataset和一个人工交叉dataset上比五种现状顶峰SSL方法和一个标准基eline表现出色，并且可以减轻SSL错误。代码可以在https://github.com/RUC-DWBI-ML/research/tree/main/WAD-master中下载。
</details></li>
</ul>
<hr>
<h2 id="CoC-GAN-Employing-Context-Cluster-for-Unveiling-a-New-Pathway-in-Image-Generation"><a href="#CoC-GAN-Employing-Context-Cluster-for-Unveiling-a-New-Pathway-in-Image-Generation" class="headerlink" title="CoC-GAN: Employing Context Cluster for Unveiling a New Pathway in Image Generation"></a>CoC-GAN: Employing Context Cluster for Unveiling a New Pathway in Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11857">http://arxiv.org/abs/2308.11857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Wang, Yiming Huang, Ziyu Zhou</li>
<li>for: 这个论文的目的是提出一种新的图像生成方法，旨在将图像转化为点云集，并使用简单的聚类算法来生成图像。</li>
<li>methods: 该方法使用的是Context Clustering（CoC）聚类算法，并结合多层感知网络（MLP）来生成图像。另外，该方法还包括一个名为“点增加器”的模块，用于生成额外的点数据，以便聚类。</li>
<li>results: 实验表明，该方法无需使用核函数或注意力机制，却可以达到出色的性能。此外，该方法的可读性也使得可以在实验中进行可视化。这些结果证明了该方法的可行性，并促使未来对Context Clustering在更多的图像生成 task中进行进一步研究。<details>
<summary>Abstract</summary>
Image generation tasks are traditionally undertaken using Convolutional Neural Networks (CNN) or Transformer architectures for feature aggregating and dispatching. Despite the frequent application of convolution and attention structures, these structures are not fundamentally required to solve the problem of instability and the lack of interpretability in image generation. In this paper, we propose a unique image generation process premised on the perspective of converting images into a set of point clouds. In other words, we interpret an image as a set of points. As such, our methodology leverages simple clustering methods named Context Clustering (CoC) to generate images from unordered point sets, which defies the convention of using convolution or attention mechanisms. Hence, we exclusively depend on this clustering technique, combined with the multi-layer perceptron (MLP) in a generative model. Furthermore, we implement the integration of a module termed the 'Point Increaser' for the model. This module is just an MLP tasked with generating additional points for clustering, which are subsequently integrated within the paradigm of the Generative Adversarial Network (GAN). We introduce this model with the novel structure as the Context Clustering Generative Adversarial Network (CoC-GAN), which offers a distinctive viewpoint in the domain of feature aggregating and dispatching. Empirical evaluations affirm that our CoC-GAN, devoid of convolution and attention mechanisms, exhibits outstanding performance. Its interpretability, endowed by the CoC module, also allows for visualization in our experiments. The promising results underscore the feasibility of our method and thus warrant future investigations of applying Context Clustering to more novel and interpretable image generation.
</details>
<details>
<summary>摘要</summary>
Image 生成任务通常使用 Convolutional Neural Networks (CNN) 或 Transformer 架构来进行特征聚合和派发。尽管这些结构频繁应用，但它们并不是解决图像生成中的不稳定和不可解释性的基本要求。在这篇论文中，我们提出了一种独特的图像生成过程，基于将图像转换为一组点云的思想。即我们将图像视为一组点。因此，我们的方法ология利用简单的聚合方法名为 Context Clustering (CoC) 来生成图像，而不需要使用 convolution 或 attention 机制。因此，我们几乎完全依赖这种聚合技术，加上多层感知机制 (MLP) 在生成模型中。此外，我们还实现了一个模块，称为 'Point Increaser'，用于生成更多的点，并将其集成到生成模型中。我们称之为 Context Clustering Generative Adversarial Network (CoC-GAN)，它在特征聚合和派发领域提供了一种新的视角。我们的实验结果表明，我们的 CoC-GAN 模型，没有使用 convolution 或 attention 机制，在性能上表现出色。此外，它的可读性，受 CoC 模块的启发，也允许我们在实验中进行可见化。这些优秀的结果证明了我们的方法的可行性，因此在将来的研究中，我们可以继续探索在更多的新和可解释的图像生成中应用 Context Clustering。
</details></li>
</ul>
<hr>
<h2 id="Compressed-Models-Decompress-Race-Biases-What-Quantized-Models-Forget-for-Fair-Face-Recognition"><a href="#Compressed-Models-Decompress-Race-Biases-What-Quantized-Models-Forget-for-Fair-Face-Recognition" class="headerlink" title="Compressed Models Decompress Race Biases: What Quantized Models Forget for Fair Face Recognition"></a>Compressed Models Decompress Race Biases: What Quantized Models Forget for Fair Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11840">http://arxiv.org/abs/2308.11840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro C. Neto, Eduarda Caldeira, Jaime S. Cardoso, Ana F. Sequeira</li>
<li>for: 本研究旨在调查使用现有深度学习模型进行脸recognition的量化方法对模型性能和人类偏见的影响。</li>
<li>methods: 本研究使用了State-of-the-Art量化方法，并在真实数据和synthetic数据上进行了测试。</li>
<li>results: 研究发现，使用synthetic数据可以减少大多数测试场景中的偏见，并且对不同的种族背景进行了分析。<details>
<summary>Abstract</summary>
With the ever-growing complexity of deep learning models for face recognition, it becomes hard to deploy these systems in real life. Researchers have two options: 1) use smaller models; 2) compress their current models. Since the usage of smaller models might lead to concerning biases, compression gains relevance. However, compressing might be also responsible for an increase in the bias of the final model. We investigate the overall performance, the performance on each ethnicity subgroup and the racial bias of a State-of-the-Art quantization approach when used with synthetic and real data. This analysis provides a few more details on potential benefits of performing quantization with synthetic data, for instance, the reduction of biases on the majority of test scenarios. We tested five distinct architectures and three different training datasets. The models were evaluated on a fourth dataset which was collected to infer and compare the performance of face recognition models on different ethnicity.
</details>
<details>
<summary>摘要</summary>
随着深度学习面Recognition模型的复杂度不断增加，实际部署变得越来越Difficult.研究人员有两个选择：1）使用更小的模型；2）压缩当前模型。然而，使用更小的模型可能会导致问题的偏见，因此压缩变得更加重要。然而，压缩也可能会导致最终模型的偏见增加。我们 investigate了State-of-the-Art量化方法的总性能、每个种族 subgroup 的性能和最终模型的种族偏见。这种分析提供了一些更多的细节，例如使用synthetic数据进行量化可以减少大多数测试场景中的偏见。我们测试了五种不同的架构和三个不同的训练数据集。模型被评估在一个 fourth 数据集上，该数据集用于对不同种族的面Recognition模型的性能进行比较。
</details></li>
</ul>
<hr>
<h2 id="PatchBackdoor-Backdoor-Attack-against-Deep-Neural-Networks-without-Model-Modification"><a href="#PatchBackdoor-Backdoor-Attack-against-Deep-Neural-Networks-without-Model-Modification" class="headerlink" title="PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification"></a>PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11822">http://arxiv.org/abs/2308.11822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xaiveryuan/patchbackdoor">https://github.com/xaiveryuan/patchbackdoor</a></li>
<li>paper_authors: Yizhen Yuan, Rui Kong, Shenghao Xie, Yuanchun Li, Yunxin Liu</li>
<li>for: 这种论文旨在攻击深度学习系统，尤其是在安全关键场景下。</li>
<li>methods: 该论文提出了一种新的后门攻击方法，即通过在摄像头前面放置一个特制的贴图（称为后门贴图），使得模型在攻击者控制的条件下产生错误预测。</li>
<li>results: 实验结果显示，该攻击方法可以在常见的深度学习模型（VGG、MobileNet、ResNet）上实现攻击成功率为93%～99%。此外，作者还在实际应用中实现了该攻击方法，并证明了其仍然具有威胁性。<details>
<summary>Abstract</summary>
Backdoor attack is a major threat to deep learning systems in safety-critical scenarios, which aims to trigger misbehavior of neural network models under attacker-controlled conditions. However, most backdoor attacks have to modify the neural network models through training with poisoned data and/or direct model editing, which leads to a common but false belief that backdoor attack can be easily avoided by properly protecting the model. In this paper, we show that backdoor attacks can be achieved without any model modification. Instead of injecting backdoor logic into the training data or the model, we propose to place a carefully-designed patch (namely backdoor patch) in front of the camera, which is fed into the model together with the input images. The patch can be trained to behave normally at most of the time, while producing wrong prediction when the input image contains an attacker-controlled trigger object. Our main techniques include an effective training method to generate the backdoor patch and a digital-physical transformation modeling method to enhance the feasibility of the patch in real deployments. Extensive experiments show that PatchBackdoor can be applied to common deep learning models (VGG, MobileNet, ResNet) with an attack success rate of 93% to 99% on classification tasks. Moreover, we implement PatchBackdoor in real-world scenarios and show that the attack is still threatening.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese<</SYS>>深度学习系统中的后门攻击是一种重要的威胁，该攻击目的是在攻击者控制的条件下让神经网络模型出现异常行为。然而，大多数后门攻击需要修改神经网络模型通过受攻击者控制的数据和/或直接模型编辑，这导致了一种常见 pero false的信念，即后门攻击可以通过正确地保护模型来避免。在这篇论文中，我们展示了后门攻击可以无需修改模型。相反，我们提议在前置摄像头上放置一个特制的贴图（称为后门贴图），该贴图在与输入图像一起被传递给模型时被训练。贴图可以在大多数情况下保持正常行为，而在攻击者控制的触发对象存在时产生错误预测。我们的主要技术包括生成后门贴图的有效训练方法和提高贴图在实际部署中的可行性的数字物理变换模型。我们的实验显示，PatchBackdoor可以应用于常见的深度学习模型（VGG、MobileNet、ResNet），攻击成功率为93%到99%。此外，我们在实际场景中实现了PatchBackdoor攻击，并证明了攻击仍然具有威胁性。
</details></li>
</ul>
<hr>
<h2 id="CLIP-Multi-modal-Hashing-A-new-baseline-CLIPMH"><a href="#CLIP-Multi-modal-Hashing-A-new-baseline-CLIPMH" class="headerlink" title="CLIP Multi-modal Hashing: A new baseline CLIPMH"></a>CLIP Multi-modal Hashing: A new baseline CLIPMH</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11797">http://arxiv.org/abs/2308.11797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Zhu, Mingkai Sheng, Mingda Ke, Zhangmin Huang, Jingfei Chang</li>
<li>for: 提高多媒体检索精度</li>
<li>methods: 使用CLIP模型提取文本和图像特征，并将其拼接生成哈希码</li>
<li>results: 与状态对比，CLIPMH可以显著提高多媒体检索精度（最大提高率8.38%），CLIP也比文本和视觉基础网络更有优势。<details>
<summary>Abstract</summary>
The multi-modal hashing method is widely used in multimedia retrieval. It can fuse multi-source data to generate binary hash code. However, the current multi-modal methods have the problem of low retrieval accuracy. The reason is that the individual backbone networks have limited feature expression capabilities and are not jointly pre-trained on large-scale unsupervised multi-modal data. To solve this problem, we propose a new baseline CLIP Multi-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and image features, and then fuse to generate hash code. CLIP improves the expressiveness of each modal feature. In this way, it can greatly improve the retrieval performance of multi-modal hashing methods. In comparison to state-of-the-art unsupervised and supervised multi-modal hashing methods, experiments reveal that the proposed CLIPMH can significantly enhance performance (Maximum increase of 8.38%). CLIP also has great advantages over the text and visual backbone networks commonly used before.
</details>
<details>
<summary>摘要</summary>
多模式哈希方法广泛应用于多媒体检索。它可以将多源数据 fusion 生成 binary 哈希码。然而，现有的多模式方法受到低回aterra retrieval 精度的限制。原因在于各个后 nac 网络具有有限的特征表达能力，并未在大规模无监督多模式数据上进行联合预训练。为解决这个问题，我们提出了一个新基线CLIP多模式哈希（CLIPMH）方法。它使用CLIP模型提取文本和图像特征，然后融合生成哈希码。CLIP提高了每种Modal特征的表达能力，从而可以大幅提高多模式哈希方法的检索性能。与state-of-the-art无监督和监督多模式哈希方法进行比较，实验表明，提议的CLIPMH可以显著提高性能（最大提升8.38%）。CLIP还在文本和视觉后 nac 网络通常使用之前具有优势。
</details></li>
</ul>
<hr>
<h2 id="Time-Does-Tell-Self-Supervised-Time-Tuning-of-Dense-Image-Representations"><a href="#Time-Does-Tell-Self-Supervised-Time-Tuning-of-Dense-Image-Representations" class="headerlink" title="Time Does Tell: Self-Supervised Time-Tuning of Dense Image Representations"></a>Time Does Tell: Self-Supervised Time-Tuning of Dense Image Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11796">http://arxiv.org/abs/2308.11796</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smsd75/timetuning">https://github.com/smsd75/timetuning</a></li>
<li>paper_authors: Mohammadreza Salehi, Efstratios Gavves, Cees G. M. Snoek, Yuki M. Asano</li>
<li>for: 本研究的目的是提出一种 incorporating temporal consistency in dense self-supervised learning 的方法，以提高视频和图像表示质量。</li>
<li>methods: 该方法从图像预训练模型开始，并使用一种新的自我超vised temporal-alignment clustering loss 来练化图像表示。这种方法可以帮助图像表示中传递高级信息到视频中。</li>
<li>results: 对于无监督 semantic segmentation 任务，该方法可以提高视频表示质量8-10%，并与图像表示质量相同。这种方法可以推动更多的自我超vised scaling，因为视频的可用性很高。代码可以在这里找到：<a target="_blank" rel="noopener" href="https://github.com/SMSD75/Timetuning%E3%80%82">https://github.com/SMSD75/Timetuning。</a><details>
<summary>Abstract</summary>
Spatially dense self-supervised learning is a rapidly growing problem domain with promising applications for unsupervised segmentation and pretraining for dense downstream tasks. Despite the abundance of temporal data in the form of videos, this information-rich source has been largely overlooked. Our paper aims to address this gap by proposing a novel approach that incorporates temporal consistency in dense self-supervised learning. While methods designed solely for images face difficulties in achieving even the same performance on videos, our method improves not only the representation quality for videos-but also images. Our approach, which we call time-tuning, starts from image-pretrained models and fine-tunes them with a novel self-supervised temporal-alignment clustering loss on unlabeled videos. This effectively facilitates the transfer of high-level information from videos to image representations. Time-tuning improves the state-of-the-art by 8-10% for unsupervised semantic segmentation on videos and matches it for images. We believe this method paves the way for further self-supervised scaling by leveraging the abundant availability of videos. The implementation can be found here : https://github.com/SMSD75/Timetuning
</details>
<details>
<summary>摘要</summary>
隐式密集自监学习是一个迅速增长的问题领域，推荐潜在应用于无监督分割和预训练 dense downstream task。尽管视频充满了时间信息，但这些信息却被大量忽略。我们的论文旨在解决这个问题，我们提出了一种新的方法，即时间调整（Time-tuning）。这种方法从图像预训练模型开始，并使用一种新的自我监督时间对齐分群损失来练化图像表示。这有效地传递高级信息从视频到图像表示。时间调整可以提高无监督 semantic segmentation 的状态得到的表示质量，并与图像表示匹配。我们的方法可以提高无监督 semantic segmentation 的状态得到的表示质量，并与图像表示匹配。我们认为这种方法将开拓更多的自监督扩展空间，因为视频的可用性是充沛的。实现可以在以下链接中找到：https://github.com/SMSD75/Timetuning。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-NeRF-akin-to-Enhancing-LLMs-Generalizable-NeRF-Transformer-with-Mixture-of-View-Experts"><a href="#Enhancing-NeRF-akin-to-Enhancing-LLMs-Generalizable-NeRF-Transformer-with-Mixture-of-View-Experts" class="headerlink" title="Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts"></a>Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11793">http://arxiv.org/abs/2308.11793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/VITA-Group/GNT-MOVE">https://github.com/VITA-Group/GNT-MOVE</a></li>
<li>paper_authors: Wenyan Cong, Hanxue Liang, Peihao Wang, Zhiwen Fan, Tianlong Chen, Mukund Varma, Yi Wang, Zhangyang Wang</li>
<li>for: 本研究旨在提出一种基于 Mixture-of-Experts (MoE) 的泛化 NeRF 模型，以提高 NeRF 模型在不同场景下的泛化能力。</li>
<li>methods: 本研究使用了一种基于 transformers 的 feedforward “neuralized” 架构，并将 Mixture-of-Experts (MoE) idea应用到这个架构中，以提高模型的泛化能力。</li>
<li>results: 实验表明，使用 GNT-MOVE 模型可以在不同场景下提取出高质量的视图 synthesis 结果，并且在 zero-shot 和 few-shot  Setting 中表现出色， indicating 模型具有remarkable 的泛化能力。<details>
<summary>Abstract</summary>
Cross-scene generalizable NeRF models, which can directly synthesize novel views of unseen scenes, have become a new spotlight of the NeRF field. Several existing attempts rely on increasingly end-to-end "neuralized" architectures, i.e., replacing scene representation and/or rendering modules with performant neural networks such as transformers, and turning novel view synthesis into a feed-forward inference pipeline. While those feedforward "neuralized" architectures still do not fit diverse scenes well out of the box, we propose to bridge them with the powerful Mixture-of-Experts (MoE) idea from large language models (LLMs), which has demonstrated superior generalization ability by balancing between larger overall model capacity and flexible per-instance specialization. Starting from a recent generalizable NeRF architecture called GNT, we first demonstrate that MoE can be neatly plugged in to enhance the model. We further customize a shared permanent expert and a geometry-aware consistency loss to enforce cross-scene consistency and spatial smoothness respectively, which are essential for generalizable view synthesis. Our proposed model, dubbed GNT with Mixture-of-View-Experts (GNT-MOVE), has experimentally shown state-of-the-art results when transferring to unseen scenes, indicating remarkably better cross-scene generalization in both zero-shot and few-shot settings. Our codes are available at https://github.com/VITA-Group/GNT-MOVE.
</details>
<details>
<summary>摘要</summary>
cross-scene总体化NeRF模型，可直接生成未看过场景的新视图，成为NeRF领域的新焦点。现有尝试通过逐渐替换场景表示和渲染模块为高性能神经网络，如转换器，并将新视图生成变成feed-forward推理管道。然而，这些feedforward“神经化”体系仍然无法适应多样化场景，我们提议通过大型语言模型（LLM）的强大混合专家（MoE）想法来桥接它们，这个想法在LLM中已经证明了更好的总体化能力，通过平衡更大的总体模型容量和每个实例特定化的专家。从最近的通用NeRF架构GNT开始，我们首先示出MoE可以简单地插入以提高模型。我们然后定制了共享永久专家和geometry-aware的一致损失，以保持 across-scene一致性和空间平滑性，这些属性是通用视图生成中重要的。我们的提出的模型，命名为GNT with Mixture-of-View-Experts（GNT-MOVE），在实验中表现出了state-of-the-art的结果，在未看过场景中转移时，表现出了很好的横向一致性和少量示例设定下的优秀总体化能力。我们的代码可以在https://github.com/VITA-Group/GNT-MOVE中找到。
</details></li>
</ul>
<hr>
<h2 id="An-extensible-point-based-method-for-data-chart-value-detection"><a href="#An-extensible-point-based-method-for-data-chart-value-detection" class="headerlink" title="An extensible point-based method for data chart value detection"></a>An extensible point-based method for data chart value detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11788">http://arxiv.org/abs/2308.11788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bnlnlp/ppn_model">https://github.com/bnlnlp/ppn_model</a></li>
<li>paper_authors: Carlos Soto, Shinjae Yoo</li>
<li>for: 本研究提出了一种可扩展的方法，用于从科学文献中提取数据图表中的Semantic Points，特别是复杂的折线图。</li>
<li>methods: 该方法使用点提议网络（类似于对象检测中的区域提议网络）直接预测数据图表中的关键点的位置，并可以轻松地扩展到多种图表类型和元素。</li>
<li>results: 研究表明，在复杂的折线图上，该模型可以准确地检测出关键点，其性能为0.8705 F1（@1.5维度最大差），并在synthetically生成的图表上达到0.9810 F1的高性能。此外，通过专门训练于synthetic数据上的新的扩展，该模型在实际图表上也可以达到良好的性能（0.6621 F1）。<details>
<summary>Abstract</summary>
We present an extensible method for identifying semantic points to reverse engineer (i.e. extract the values of) data charts, particularly those in scientific articles. Our method uses a point proposal network (akin to region proposal networks for object detection) to directly predict the position of points of interest in a chart, and it is readily extensible to multiple chart types and chart elements. We focus on complex bar charts in the scientific literature, on which our model is able to detect salient points with an accuracy of 0.8705 F1 (@1.5-cell max deviation); it achieves 0.9810 F1 on synthetically-generated charts similar to those used in prior works. We also explore training exclusively on synthetic data with novel augmentations, reaching surprisingly competent performance in this way (0.6621 F1) on real charts with widely varying appearance, and we further demonstrate our unchanged method applied directly to synthetic pie charts (0.8343 F1). Datasets, trained models, and evaluation code are available at https://github.com/BNLNLP/PPN_model.
</details>
<details>
<summary>摘要</summary>
我们提出了一种可扩展的方法来识别数据图表中的 semantic point（即提取值），特别是在科学文献中的数据图表。我们的方法使用一种点提议网络（类似于物体检测中的区域提议网络）直接预测数据图表中的点位置，并且可以轻松扩展到多种图表类型和元素。我们主要关注科学文献中的复杂柱形图，我们的模型能够检测出这些图表中的突出点，准确率为0.8705 F1（@1.5细分差值）；在先前作者使用的图表上进行synthetically生成的图表上，我们的模型可以达到0.9810 F1的准确率。此外，我们还探索了专门使用生成的synthetic数据进行训练，并在实际图表上达到了意外地好的性能（0.6621 F1）。此外，我们还证明了我们的无变方法可以直接应用于synthetic pie chart（0.8343 F1）。我们的数据集、训练模型和评估代码可以在https://github.com/BNLNLP/PPN_model上获取。
</details></li>
</ul>
<hr>
<h2 id="Coarse-to-Fine-Multi-Scene-Pose-Regression-with-Transformers"><a href="#Coarse-to-Fine-Multi-Scene-Pose-Regression-with-Transformers" class="headerlink" title="Coarse-to-Fine Multi-Scene Pose Regression with Transformers"></a>Coarse-to-Fine Multi-Scene Pose Regression with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11783">http://arxiv.org/abs/2308.11783</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yolish/c2f-ms-transformer">https://github.com/yolish/c2f-ms-transformer</a></li>
<li>paper_authors: Yoli Shavit, Ron Ferens, Yosi Keller</li>
<li>for: 本文旨在提出一种基于 transformer 的多景绝对摄像头姿态 regression 方法，用于同时Localization 多个景象。</li>
<li>methods: 该方法使用 encoder 聚合活动图像，并通过 self-attention 机制将多个景象编码进行同时embedding。decoder 则将 latent features 和景象编码转换成姿态预测。</li>
<li>results: 该方法在常见的indoor和outdoor dataset上进行评估，并与多个景象和单个景象绝对摄像头姿态推导器进行比较，并表明其在Localization 精度方面具有优势。<details>
<summary>Abstract</summary>
Absolute camera pose regressors estimate the position and orientation of a camera given the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron (MLP) head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended to learn multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into pose predictions. This allows our model to focus on general features that are informative for localization, while embedding multiple scenes in parallel. We extend our previous MS-Transformer approach \cite{shavit2021learning} by introducing a mixed classification-regression architecture that improves the localization accuracy. Our method is evaluated on commonly benchmark indoor and outdoor datasets and has been shown to exceed both multi-scene and state-of-the-art single-scene absolute pose regressors.
</details>
<details>
<summary>摘要</summary>
绝对摄像头姿势回归器可以根据捕捉到的图像来估算摄像头的位置和方向。通常情况下，会使用卷积神经网络和多层感知器（MLP）来训练一个单个参考场景。在最近的扩展中，这种方案被替换为多个完全连接层，以学习多个场景。在这个工作中，我们提议使用变换器来学习多个场景绝对摄像头姿势回归，其中扩展器被用来聚合活动图像和自身注意力，而解码器则将缓存特征和场景编码转换为姿势预测。这使得我们的模型能够关注到通用特征，并同时将多个场景embedded在平行。我们在先前的MS-Transformer方法 \cite{shavit2021learning} 上进行了扩展，并引入了混合分类-回归架构，以提高地理位置准确性。我们的方法在常见的indoor和outdoor数据集上进行了评估，并已经超过了多个场景和状态的绝对摄像头姿势回归器。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Hessian-Alignment-for-Domain-Generalization"><a href="#Understanding-Hessian-Alignment-for-Domain-Generalization" class="headerlink" title="Understanding Hessian Alignment for Domain Generalization"></a>Understanding Hessian Alignment for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11778">http://arxiv.org/abs/2308.11778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/federated-learning">https://github.com/huawei-noah/federated-learning</a></li>
<li>paper_authors: Sobhan Hemati, Guojun Zhang, Amir Estiri, Xi Chen</li>
<li>for: 本文主要针对 Deep Learning 模型在各种实际场景中的 OUT-OF-distribution (OOD) 泛化问题，包括医疗和自动驾驶等。</li>
<li>methods: 本文使用了不同的技术来提高 OOD 泛化，其中 gradient-based 正则化表现最佳。然而，我们对 Hessian 和梯度在领域泛化中的角色还是有限的。本文通过使用最近的 OOD 理论来分析 Hessian 和梯度在领域泛化中的作用。</li>
<li>results: 本文的分析表明，在领域泛化中，梯度和 Hessian 的匹配可以提高 OOD 泛化的性能。此外，本文还提出了两种简单 yet effective 的方法来匹配梯度和 Hessian，不需要直接计算 Hessian。这些方法在不同的 OOD 场景中都达到了良好的性能。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenarios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regularizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier's head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spectral norm between the classifier's head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Furthermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients. Our analysis explains the success of many regularizers like CORAL, IRM, V-REx, Fish, IGA, and Fishr as they regularize part of the classifier's head Hessian and/or gradient. Finally, we propose two simple yet effective methods to match the classifier's head Hessians and gradients in an efficient way, based on the Hessian Gradient Product (HGP) and Hutchinson's method (Hutchinson), and without directly calculating Hessians. We validate the OOD generalization ability of proposed methods in different scenarios, including transferability, severe correlation shift, label shift and diversity shift. Our results show that Hessian alignment methods achieve promising performance on various OOD benchmarks. The code is available at \url{https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment}.
</details>
<details>
<summary>摘要</summary>
外部数据（OOD）泛化是深度学习模型在许多实际场景中的关键能力，包括医疗和自动驾驶等。在这些场景中，不同的技术已经被提出来改进OOD泛化。其中，梯度基的正则化方法已经显示了出色的表现，相比其他竞争对手。然而，我们对梯度和梯度对域泛化的理解仍然有限。为了解决这个问题，我们使用最近的OOD理论来分析泛化中梯度和梯度的角色。我们发现，在不同域之间的梯度头矩阵的 спектраль范数是泛化度量的上界，这是一个距离目标和源域的诱导。此外，我们分析了梯度和梯度之间的各种特征的对应关系，包括梯度对域的对应关系。我们的分析解释了许多正则化器，如CORAL、IRM、V-REx、Fish、IGA和Fishr的成功，因为它们在梯度和梯度之间进行了一定的对齐。最后，我们提出了两种简单 yet有效的方法来匹配梯度头矩阵和梯度，基于梯度和梯度产生的产品（HGP）和哈特金森的方法（Hutchinson）。我们在不同的OOD场景中进行了验证，包括传输性、严重相关变换、标签变换和多样性变换。我们的结果显示，梯度对齐方法在多种OOD场景中具有优秀的表现。代码可以在 \url{https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment} 中找到。
</details></li>
</ul>
<hr>
<h2 id="SAMSNeRF-Segment-Anything-Model-SAM-Guides-Dynamic-Surgical-Scene-Reconstruction-by-Neural-Radiance-Field-NeRF"><a href="#SAMSNeRF-Segment-Anything-Model-SAM-Guides-Dynamic-Surgical-Scene-Reconstruction-by-Neural-Radiance-Field-NeRF" class="headerlink" title="SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene Reconstruction by Neural Radiance Field (NeRF)"></a>SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene Reconstruction by Neural Radiance Field (NeRF)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11774">http://arxiv.org/abs/2308.11774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ange Lou, Yamin Li, Xing Yao, Yike Zhang, Jack Noble</li>
<li>for: 提供高精度动态外科场景重建，以增强外科Navigation和自动化。</li>
<li>methods:  combining Segment Anything Model (SAM)和Neural Radiance Field (NeRF)技术，通过生成高精度的分割mask来导引NeRF进行高精度动态场景重建。</li>
<li>results: 实验结果表明，我们的方法可以成功重建高精度的动态外科场景，并准确地反映外科工具的空间信息。<details>
<summary>Abstract</summary>
The accurate reconstruction of surgical scenes from surgical videos is critical for various applications, including intraoperative navigation and image-guided robotic surgery automation. However, previous approaches, mainly relying on depth estimation, have limited effectiveness in reconstructing surgical scenes with moving surgical tools. To address this limitation and provide accurate 3D position prediction for surgical tools in all frames, we propose a novel approach called SAMSNeRF that combines Segment Anything Model (SAM) and Neural Radiance Field (NeRF) techniques. Our approach generates accurate segmentation masks of surgical tools using SAM, which guides the refinement of the dynamic surgical scene reconstruction by NeRF. Our experimental results on public endoscopy surgical videos demonstrate that our approach successfully reconstructs high-fidelity dynamic surgical scenes and accurately reflects the spatial information of surgical tools. Our proposed approach can significantly enhance surgical navigation and automation by providing surgeons with accurate 3D position information of surgical tools during surgery.The source code will be released soon.
</details>
<details>
<summary>摘要</summary>
准确重建手术场景从手术视频中是许多应用程序的关键之一，包括实时导航和基于图像感知的 робо脑外科自动化。然而，先前的方法，主要基于深度估计，具有限制性，不能准确重建移动的手术工具。为了解决这个限制并提供所有帧中的准确3D位置预测，我们提出了一种新的方法，即SAMSNeRF，它结合了 Segment Anything Model（SAM）和 Neural Radiance Field（NeRF）技术。我们的方法生成了高精度的手术工具分割推荐，这导引了 NeRF 的动态手术场景重建的精度更高。我们的实验结果表明，我们的方法可以成功重建高精度的动态手术场景，并准确反映手术工具的空间信息。我们的提议的方法可以在手术过程中为外科医生提供准确的3D位置信息，从而明显提高手术导航和自动化。我们即将发布源代码。
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Face-and-Whole-Body-Recognition-in-Turbulent-Environments"><a href="#Weakly-Supervised-Face-and-Whole-Body-Recognition-in-Turbulent-Environments" class="headerlink" title="Weakly Supervised Face and Whole Body Recognition in Turbulent Environments"></a>Weakly Supervised Face and Whole Body Recognition in Turbulent Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11757">http://arxiv.org/abs/2308.11757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshitij Nikhal, Benjamin S. Riggan</li>
<li>for: 提高扩展距离人脸识别系统的稳定性和可靠性，尤其是在大气干扰和各种距离下。</li>
<li>methods: 提出了一种新的弱监督框架，利用自我注意机制生成域不涉及的表示，将抖擞和平静图像匹配到同一个空间中。同时，引入了一种新的倾斜地图估计器，预测大气干扰图像中的几何变形。</li>
<li>results: 对于LRFID和BGC1两个预测集，我们的方法可以提高排名一的精度，尤其是在不同的大气干扰和距离下。<details>
<summary>Abstract</summary>
Face and person recognition have recently achieved remarkable success under challenging scenarios, such as off-pose and cross-spectrum matching. However, long-range recognition systems are often hindered by atmospheric turbulence, leading to spatially and temporally varying distortions in the image. Current solutions rely on generative models to reconstruct a turbulent-free image, but often preserve photo-realism instead of discriminative features that are essential for recognition. This can be attributed to the lack of large-scale datasets of turbulent and pristine paired images, necessary for optimal reconstruction. To address this issue, we propose a new weakly supervised framework that employs a parameter-efficient self-attention module to generate domain agnostic representations, aligning turbulent and pristine images into a common subspace. Additionally, we introduce a new tilt map estimator that predicts geometric distortions observed in turbulent images. This estimate is used to re-rank gallery matches, resulting in up to 13.86\% improvement in rank-1 accuracy. Our method does not require synthesizing turbulent-free images or ground-truth paired images, and requires significantly fewer annotated samples, enabling more practical and rapid utility of increasingly large datasets. We analyze our framework using two datasets -- Long-Range Face Identification Dataset (LRFID) and BRIAR Government Collection 1 (BGC1) -- achieving enhanced discriminability under varying turbulence and standoff distance.
</details>
<details>
<summary>摘要</summary>
面部和人识别在现在的挑战性场景中已经取得了很大的成功，如偏pose和跨谱匹配。然而，长距离识别系统经常受到大气扰动的影响，导致图像中的扰动变得空间和时间变化。现有的解决方案是使用生成模型来重建扰动的图像，但经常保留着照片实际的特征而不是识别特征，这可以归结于缺乏大规模的扰动和静止图像对应的数据集，这些数据集是必要的 для优化重建。为解决这个问题，我们提出了一个新的弱监督框架，该框架使用高效的自注意模块来生成域无关的表示，将扰动和静止图像调整到共同的空间中。此外，我们还引入了一个新的倾斜地图预测器，该预测器预测了大气扰动中图像中的几何扭曲。这个预测结果用于重新排序画库匹配结果，从而提高了rank-1准确率达到13.86%。我们的方法不需要生成扰动自由图像或真实的对应图像，也不需要大量的注释样本，因此可以更实用和快速地利用越来越大的数据集。我们分析了我们的框架使用两个数据集--长距离面部识别数据集（LRFID）和BRIAR政府收集1（BGC1）--并在不同的扰动和距离 circumstance中获得了提高的分布性。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Controllable-Multi-Task-Architectures"><a href="#Efficient-Controllable-Multi-Task-Architectures" class="headerlink" title="Efficient Controllable Multi-Task Architectures"></a>Efficient Controllable Multi-Task Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11744">http://arxiv.org/abs/2308.11744</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AriGho/An-Efficient-Algorithm-for-Increasing-Modularity-in-IoT-Based-Automation-Systems">https://github.com/AriGho/An-Efficient-Algorithm-for-Increasing-Modularity-in-IoT-Based-Automation-Systems</a></li>
<li>paper_authors: Abhishek Aich, Samuel Schulter, Amit K. Roy-Chowdhury, Manmohan Chandraker, Yumin Suh</li>
<li>for: 这种方法用于训练一个多任务模型，以便在部署后由用户调整计算负载和任务性能的权重，而无需重新训练和存储不同场景的模型。</li>
<li>methods: 这种方法包括一个共享Encoder和任务特定的解码器，其中Encoder和解码器通道宽度都是可缩放的。我们的关键思想是通过变化任务特定的解码器的容量来控制任务的重要性，而不是固定的训练多任务模型。</li>
<li>results: 我们的方法可以提高总准确率，同时提供高质量的缩放下的子架构，而无需重新训练和存储不同场景的模型。在三个多任务benchmark上（PASCALContext、NYUDv2和CIFAR100-MTL），我们的方法可以提高控制性by ~33.5%，而且计算成本较低。<details>
<summary>Abstract</summary>
We aim to train a multi-task model such that users can adjust the desired compute budget and relative importance of task performances after deployment, without retraining. This enables optimizing performance for dynamically varying user needs, without heavy computational overhead to train and save models for various scenarios. To this end, we propose a multi-task model consisting of a shared encoder and task-specific decoders where both encoder and decoder channel widths are slimmable. Our key idea is to control the task importance by varying the capacities of task-specific decoders, while controlling the total computational cost by jointly adjusting the encoder capacity. This improves overall accuracy by allowing a stronger encoder for a given budget, increases control over computational cost, and delivers high-quality slimmed sub-architectures based on user's constraints. Our training strategy involves a novel 'Configuration-Invariant Knowledge Distillation' loss that enforces backbone representations to be invariant under different runtime width configurations to enhance accuracy. Further, we present a simple but effective search algorithm that translates user constraints to runtime width configurations of both the shared encoder and task decoders, for sampling the sub-architectures. The key rule for the search algorithm is to provide a larger computational budget to the higher preferred task decoder, while searching a shared encoder configuration that enhances the overall MTL performance. Various experiments on three multi-task benchmarks (PASCALContext, NYUDv2, and CIFAR100-MTL) with diverse backbone architectures demonstrate the advantage of our approach. For example, our method shows a higher controllability by ~33.5% in the NYUD-v2 dataset over prior methods, while incurring much less compute cost.
</details>
<details>
<summary>摘要</summary>
我们目标是训练一个多任务模型，以便用户在部署后可以调整计算预算和任务表现的重要性，而不需要重新训练。这样可以在动态变化的用户需求下优化性能，而不是带来大量的计算开销来训练和存储多种场景的模型。为此，我们提议一个多任务模型，其中包括共享Encoder和任务特定Decoder。Encoder和Decoder的通道宽度都是可变的。我们的关键思想是通过变化任务特定Decoder的容量来控制任务的重要性，而控制总计算成本的同时，通过共同调整Encoder的容量来提高总MTL性能。这样可以提高精度，提高计算成本控制，并提供基于用户的限制而生成高质量的纤维子模型。我们的训练策略包括一种新的“配置不变知识传播”损失函数，该函数使得后处理表示不变于不同的运行时宽度配置，以提高精度。此外，我们还提出了一种简单 yet有效的搜索算法，该算法将用户的限制翻译成运行时宽度配置的共享Encoder和任务特定Decoder，以采样子模型。搜索算法的关键规则是为高优先级任务Decoder提供更大的计算预算，而搜索共享Encoder配置，以提高总MTL性能。我们在三个多任务测试集（PASCALContext、NYUDv2和CIFAR100-MTL）上进行了多种不同后处理架构的实验，结果显示我们的方法具有更高的可控性（~33.5%），而且带来许多计算成本的减少。
</details></li>
</ul>
<hr>
<h2 id="Animal3D-A-Comprehensive-Dataset-of-3D-Animal-Pose-and-Shape"><a href="#Animal3D-A-Comprehensive-Dataset-of-3D-Animal-Pose-and-Shape" class="headerlink" title="Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape"></a>Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11737">http://arxiv.org/abs/2308.11737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, Wei Ji, Chen Wang, Xiaoding Yuan, Prakhar Kaushik, Guofeng Zhang, Jie Liu, Yushan Xie, Yawen Cui, Alan Yuille, Adam Kortylewski</li>
<li>for:  This paper aims to provide a comprehensive dataset for mammal animal 3D pose and shape estimation, which can potentially benefit many downstream applications such as wildlife conservation.</li>
<li>methods: The paper proposes a dataset called Animal3D, which consists of 3379 images collected from 40 mammal species, high-quality annotations of 26 keypoints, and the pose and shape parameters of the SMAL model.</li>
<li>results: The paper benchmarks representative shape and pose estimation models on the Animal3D dataset and demonstrates that synthetic pre-training is a viable strategy to boost the model performance. However, predicting the 3D shape and pose of animals across species remains a very challenging task.<details>
<summary>Abstract</summary>
Accurately estimating the 3D pose and shape is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. However, research in this area is held back by the lack of a comprehensive and diverse dataset with high-quality 3D pose and shape annotations. In this paper, we propose Animal3D, the first comprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D consists of 3379 images collected from 40 mammal species, high-quality annotations of 26 keypoints, and importantly the pose and shape parameters of the SMAL model. All annotations were labeled and checked manually in a multi-stage process to ensure highest quality results. Based on the Animal3D dataset, we benchmark representative shape and pose estimation models at: (1) supervised learning from only the Animal3D data, (2) synthetic to real transfer from synthetically generated images, and (3) fine-tuning human pose and shape estimation models. Our experimental results demonstrate that predicting the 3D shape and pose of animals across species remains a very challenging task, despite significant advances in human pose estimation. Our results further demonstrate that synthetic pre-training is a viable strategy to boost the model performance. Overall, Animal3D opens new directions for facilitating future research in animal 3D pose and shape estimation, and is publicly available.
</details>
<details>
<summary>摘要</summary>
accurately estimating 动物的3D姿态和形状是理解动物行为的重要步骤，可能有多个下游应用，如野生动物保护。然而，这一领域的研究受到缺乏完整和多样的数据集，高质量的3D姿态和形状注释的限制。本文提出了《动物3D》数据集，包括40种哺乳动物的3379张图像，高质量的26个关键点注释，以及SMAL模型的姿态和形状参数。所有注释都是人工标注和检查的，以确保最高质量的结果。基于《动物3D》数据集，我们对代表性的形状和姿态估计模型进行了：（1）只使用《动物3D》数据进行监督学习，（2）从 sintetically生成的图像进行synthetic to real transfer，以及（3）人体姿态和形状估计模型的精细调整。我们的实验结果表明，预测动物各种种的3D姿态和形状仍然是一个非常困难的任务，尽管人体姿态估计技术有所进步。我们的结果还表明，Synthetic pre-training是一种有效的策略，可以提高模型性能。总的来说，《动物3D》开启了未来动物3D姿态和形状估计研究的新途径，并公共可用。
</details></li>
</ul>
<hr>
<h2 id="Un-fair-Exposure-in-Deep-Face-Rankings-at-a-Distance"><a href="#Un-fair-Exposure-in-Deep-Face-Rankings-at-a-Distance" class="headerlink" title="(Un)fair Exposure in Deep Face Rankings at a Distance"></a>(Un)fair Exposure in Deep Face Rankings at a Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11732">http://arxiv.org/abs/2308.11732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Atzori, Gianni Fenu, Mirko Marras</li>
<li>for: 该论文旨在探讨法律执法人员在使用深度Face模型时面临的涉嫌人脸图像排名挑战，以及这种挑战中不同人群的偏见问题。</li>
<li>methods: 该论文提出了一种新的实验方案，包括六种现代的Face编码器和两个公共数据集，用于检测法律执法人员在面临涉嫌人脸图像排名任务中受到的偏见问题。</li>
<li>results: 经过对两个数据集的重复和识别任务的广泛实验，论文显示了这个领域中的偏见问题仍然存在，需要采取特殊的政策和 corrected measures 来解决。<details>
<summary>Abstract</summary>
Law enforcement regularly faces the challenge of ranking suspects from their facial images. Deep face models aid this process but frequently introduce biases that disproportionately affect certain demographic segments. While bias investigation is common in domains like job candidate ranking, the field of forensic face rankings remains underexplored. In this paper, we propose a novel experimental framework, encompassing six state-of-the-art face encoders and two public data sets, designed to scrutinize the extent to which demographic groups suffer from biases in exposure in the context of forensic face rankings. Through comprehensive experiments that cover both re-identification and identification tasks, we show that exposure biases within this domain are far from being countered, demanding attention towards establishing ad-hoc policies and corrective measures. The source code is available at https://github.com/atzoriandrea/ijcb2023-unfair-face-rankings
</details>
<details>
<summary>摘要</summary>
法 enforcement  régulièrement 面临涉及到嫌犯人像的排名挑战。深度脸部模型可以帮助这个过程，但经常引入偏见，这些偏见会对某些民族群体产生不公正的影响。在领域 like 聘请候选人排名中，偏见调查是常见的，但在法医脸部排名领域，这一问题仍未得到充分关注。在这篇论文中，我们提出了一个新的实验框架，包括六种最新的脸部编码器和两个公共数据集，用于检验嫌犯人脸部排名中不同民族群体是否受到偏见的影响。通过对重识别和识别任务进行全面的实验，我们显示出，在这个领域中的曝光偏见仍未得到控制，需要采取特殊的政策和纠正措施。源代码可以在 https://github.com/atzoriandrea/ijcb2023-unfair-face-rankings 中下载。
</details></li>
</ul>
<hr>
<h2 id="GRIP-Generating-Interaction-Poses-Using-Latent-Consistency-and-Spatial-Cues"><a href="#GRIP-Generating-Interaction-Poses-Using-Latent-Consistency-and-Spatial-Cues" class="headerlink" title="GRIP: Generating Interaction Poses Using Latent Consistency and Spatial Cues"></a>GRIP: Generating Interaction Poses Using Latent Consistency and Spatial Cues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11617">http://arxiv.org/abs/2308.11617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omid Taheri, Yi Zhou, Dimitrios Tzionas, Yang Zhou, Duygu Ceylan, Soren Pirk, Michael J. Black</li>
<li>for: 模型人类手部与物体的互动，包括手指的细微运动，是计算机图形、计算机视觉和混合现实等领域的关键问题。</li>
<li>methods: 我们提出了一种学习基于的方法GRIP，它可以从身体和物体的3D运动中生成真实的手部运动。首先，我们使用ANet网络进行采样，然后利用身体和物体之间的空间时间关系提取两种新的互动时间指标，并在两个阶段的推理管道中使用这些指标生成手部运动。</li>
<li>results: 我们的GRIP方法可以在不同的运动捕捉数据集上对手部运动进行升级，并且在不同的物体和运动方式下保持高度的一致性和普适性。量化实验和感知研究表明，GRIP方法在比基eline方法更高效，并且可以扩展到未看过的物体和运动。<details>
<summary>Abstract</summary>
Hands are dexterous and highly versatile manipulators that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand-object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality. Prior work on capturing and modeling humans interacting with objects in 3D focuses on the body and object motion, often ignoring hand pose. In contrast, we introduce GRIP, a learning-based method that takes, as input, the 3D motion of the body and the object, and synthesizes realistic motion for both hands before, during, and after object interaction. As a preliminary step before synthesizing the hand motion, we first use a network, ANet, to denoise the arm motion. Then, we leverage the spatio-temporal relationship between the body and the object to extract two types of novel temporal interaction cues, and use them in a two-stage inference pipeline to generate the hand motion. In the first stage, we introduce a new approach to enforce motion temporal consistency in the latent space (LTC), and generate consistent interaction motions. In the second stage, GRIP generates refined hand poses to avoid hand-object penetrations. Given sequences of noisy body and object motion, GRIP upgrades them to include hand-object interaction. Quantitative experiments and perceptual studies demonstrate that GRIP outperforms baseline methods and generalizes to unseen objects and motions from different motion-capture datasets.
</details>
<details>
<summary>摘要</summary>
手是人类与物体之间的灵活和多样化抓取器，对人类与物体交互的研究非常重要。因此，模拟人类与物体之间的真实互动，包括手指的细微运动，是计算机图形、计算机视觉和混合现实等领域的关键问题。在过去的工作中，对3D中人体和物体的互动进行捕捉和模拟，通常会忽略手姿势。然而，我们介绍了GRIP，一种基于学习的方法，它可以将人体和物体的3D运动作为输入，并生成真实的手姿势。在生成手姿势之前，我们首先使用ANet网络来减噪人体运动。然后，我们利用人体和物体之间的空间时间关系提取两种新的时间互动指示，并使其在两个阶段的推理管道中使用，以生成真实的手姿势。在第一阶段，我们引入了一种新的方法来在幂空间中保证运动的时间一致性（LTC），以生成一致的互动运动。在第二阶段，GRIP生成了不具有手object割辑的补做手姿势。给定含杂的人体和物体运动序列，GRIP可以将其升级为包括手object互动的序列。量化实验和人类感知研究表明，GRIP在不同的动作捕捉数据集上超过了基eline方法，并在不同的物体和运动中展现了普适性。
</details></li>
</ul>
<hr>
<h2 id="Delving-into-Motion-Aware-Matching-for-Monocular-3D-Object-Tracking"><a href="#Delving-into-Motion-Aware-Matching-for-Monocular-3D-Object-Tracking" class="headerlink" title="Delving into Motion-Aware Matching for Monocular 3D Object Tracking"></a>Delving into Motion-Aware Matching for Monocular 3D Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11607">http://arxiv.org/abs/2308.11607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kuanchihhuang/moma-m3t">https://github.com/kuanchihhuang/moma-m3t</a></li>
<li>paper_authors: Kuan-Chih Huang, Ming-Hsuan Yang, Yi-Hsuan Tsai</li>
<li>for: 提高低成本摄像头感知器的3D多bject跟踪任务</li>
<li>methods: 利用物体运动cue在不同时帧中的研究</li>
<li>results: 提出了一种基于运动感知的monocular 3D MOT框架，在nuScenes和KITTI datasets上进行了广泛的实验，并达到了与状态当前方法的竞争性表现。 Code和模型在<a target="_blank" rel="noopener" href="https://github.com/kuanchihhuang/MoMA-M3T%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/kuanchihhuang/MoMA-M3T上提供。</a><details>
<summary>Abstract</summary>
Recent advances of monocular 3D object detection facilitate the 3D multi-object tracking task based on low-cost camera sensors. In this paper, we find that the motion cue of objects along different time frames is critical in 3D multi-object tracking, which is less explored in existing monocular-based approaches. In this paper, we propose a motion-aware framework for monocular 3D MOT. To this end, we propose MoMA-M3T, a framework that mainly consists of three motion-aware components. First, we represent the possible movement of an object related to all object tracklets in the feature space as its motion features. Then, we further model the historical object tracklet along the time frame in a spatial-temporal perspective via a motion transformer. Finally, we propose a motion-aware matching module to associate historical object tracklets and current observations as final tracking results. We conduct extensive experiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3T achieves competitive performance against state-of-the-art methods. Moreover, the proposed tracker is flexible and can be easily plugged into existing image-based 3D object detectors without re-training. Code and models are available at https://github.com/kuanchihhuang/MoMA-M3T.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>We represent the possible movement of an object in the feature space as its motion features.2. We model the historical object tracklet in a spatial-temporal perspective using a motion transformer.3. We propose a motion-aware matching module to associate historical object tracklets and current observations as final tracking results.We conduct extensive experiments on the nuScenes and KITTI datasets and show that our MoMA-M3T achieves competitive performance against state-of-the-art methods. Additionally, our proposed tracker is flexible and can be easily integrated into existing image-based 3D object detectors without re-training. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/kuanchihhuang/MoMA-M3T">https://github.com/kuanchihhuang/MoMA-M3T</a>.</details></li>
</ol>
<hr>
<h2 id="GOPro-Generate-and-Optimize-Prompts-in-CLIP-using-Self-Supervised-Learning"><a href="#GOPro-Generate-and-Optimize-Prompts-in-CLIP-using-Self-Supervised-Learning" class="headerlink" title="GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised Learning"></a>GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11605">http://arxiv.org/abs/2308.11605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mainak Singha, Ankit Jha, Biplab Banerjee</li>
<li>for: 提高图像识别任务的性能，并解决CLIP和SSL的组合问题</li>
<li>methods: 使用提问学习方法，即使用CLIP和SSL的损失函数，并 introduce visual contrastive loss和prompt consistency loss</li>
<li>results: 在三个困难的领域通用化任务上，GOPro比前一个状态的提示技术得到了显著的提高，并且可以充分发挥CLIP和SSL的优势<details>
<summary>Abstract</summary>
Large-scale foundation models, such as CLIP, have demonstrated remarkable success in visual recognition tasks by embedding images in a semantically rich space. Self-supervised learning (SSL) has also shown promise in improving visual recognition by learning invariant features. However, the combination of CLIP with SSL is found to face challenges due to the multi-task framework that blends CLIP's contrastive loss and SSL's loss, including difficulties with loss weighting and inconsistency among different views of images in CLIP's output space. To overcome these challenges, we propose a prompt learning-based model called GOPro, which is a unified framework that ensures similarity between various augmented views of input images in a shared image-text embedding space, using a pair of learnable image and text projectors atop CLIP, to promote invariance and generalizability. To automatically learn such prompts, we leverage the visual content and style primitives extracted from pre-trained CLIP and adapt them to the target task. In addition to CLIP's cross-domain contrastive loss, we introduce a visual contrastive loss and a novel prompt consistency loss, considering the different views of the images. GOPro is trained end-to-end on all three loss objectives, combining the strengths of CLIP and SSL in a principled manner. Empirical evaluations demonstrate that GOPro outperforms the state-of-the-art prompting techniques on three challenging domain generalization tasks across multiple benchmarks by a significant margin. Our code is available at https://github.com/mainaksingha01/GOPro.
</details>
<details>
<summary>摘要</summary>
大规模基础模型，如CLIP，在视觉识别任务中表现出了惊人的成功，通过嵌入图像在semantically rich space中。自我超vised学习（SSL）也表现出了提高视觉识别的潜力，通过学习不变的特征。然而，CLIP与SSL的组合面临了多任务框架的挑战，包括权重失衡和图像不同视图的不一致性。为解决这些挑战，我们提出了一种名为GOPro的提问学习模型，它是一个统一的框架，使得输入图像的多个扩展视图在一个共享图像文本嵌入空间中保持相似性，通过使用CLIP顶部的两个可学习的图像和文本投影器。我们通过可学习的提问来自动学习这些提示，并利用预训练CLIP中的视觉内容和风格元素来适应目标任务。此外，我们还引入了视觉对比损失和新的提示一致损失，考虑到图像不同视图的差异。GOPro通过综合CLIP和SSL的优点，在三个损失目标上进行了一个整体的训练，并在三个难度大的领域总结任务上表现出了显著的超越。我们的代码可以在https://github.com/mainaksingha01/GOPro上下载。
</details></li>
</ul>
<hr>
<h2 id="G3Reg-Pyramid-Graph-based-Global-Registration-using-Gaussian-Ellipsoid-Model"><a href="#G3Reg-Pyramid-Graph-based-Global-Registration-using-Gaussian-Ellipsoid-Model" class="headerlink" title="G3Reg: Pyramid Graph-based Global Registration using Gaussian Ellipsoid Model"></a>G3Reg: Pyramid Graph-based Global Registration using Gaussian Ellipsoid Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11573">http://arxiv.org/abs/2308.11573</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-aerial-robotics/lidar-registration-benchmark">https://github.com/hkust-aerial-robotics/lidar-registration-benchmark</a></li>
<li>paper_authors: Zhijian Qiao, Zehuan Yu, Binqian Jiang, Huan Yin, Shaojie Shen</li>
<li>for: 这个研究是为了提出一种新的全球扫描LiDAR点云的框架G3Reg，以实现快速和稳定的全球 регистраción。</li>
<li>methods: 这个框架使用基本的地理 primitives，包括平面、集群和直线（PCL）从Raw点云中提取低级别的semantic分割，然后使用这些GEM来实现全球 регистраción。</li>
<li>results: 研究表明G3Reg框架在三个公共可用数据集和一个自收集的多会话数据集上展现出了superior的Robustness和实时性，并且可以将个体GEM和PAGOR组件与其他算法框架结合以提高其效果。<details>
<summary>Abstract</summary>
This study introduces a novel framework, G3Reg, for fast and robust global registration of LiDAR point clouds. In contrast to conventional complex keypoints and descriptors, we extract fundamental geometric primitives including planes, clusters, and lines (PCL) from the raw point cloud to obtain low-level semantic segments. Each segment is formulated as a unified Gaussian Ellipsoid Model (GEM) by employing a probability ellipsoid to ensure the ground truth centers are encompassed with a certain degree of probability. Utilizing these GEMs, we then present a distrust-and-verify scheme based on a Pyramid Compatibility Graph for Global Registration (PAGOR). Specifically, we establish an upper bound, which can be traversed based on the confidence level for compatibility testing to construct the pyramid graph. Gradually, we solve multiple maximum cliques (MAC) for each level of the graph, generating numerous transformation candidates. In the verification phase, we adopt a precise and efficient metric for point cloud alignment quality, founded on geometric primitives, to identify the optimal candidate. The performance of the algorithm is extensively validated on three publicly available datasets and a self-collected multi-session dataset, without changing any parameter settings in the experimental evaluation. The results exhibit superior robustness and real-time performance of the G3Reg framework compared to state-of-the-art methods. Furthermore, we demonstrate the potential for integrating individual GEM and PAGOR components into other algorithmic frameworks to enhance their efficacy. To advance further research and promote community understanding, we have publicly shared the source code.
</details>
<details>
<summary>摘要</summary>
To perform the registration, the framework uses a distrust-and-verify scheme based on a Pyramid Compatibility Graph for Global Registration (PAGOR). This involves establishing an upper bound for compatibility testing and gradually solving multiple maximum cliques (MAC) for each level of the graph. The verification phase then uses a precise and efficient metric for point cloud alignment quality, founded on geometric primitives, to identify the optimal candidate.The performance of the G3Reg framework is extensively validated on three publicly available datasets and a self-collected multi-session dataset, without changing any parameter settings in the experimental evaluation. The results show that the G3Reg framework exhibits superior robustness and real-time performance compared to state-of-the-art methods. Additionally, the framework has the potential to be integrated with other algorithmic frameworks to enhance their efficacy. To promote further research and community understanding, the source code has been publicly shared.
</details></li>
</ul>
<hr>
<h2 id="SPANet-Frequency-balancing-Token-Mixer-using-Spectral-Pooling-Aggregation-Modulation"><a href="#SPANet-Frequency-balancing-Token-Mixer-using-Spectral-Pooling-Aggregation-Modulation" class="headerlink" title="SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation"></a>SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11568">http://arxiv.org/abs/2308.11568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guhnoo Yun, Juhan Yoo, Kijung Kim, Jeongho Lee, Dong Hwan Kim</li>
<li>for: 这个论文旨在探讨现有的卷积模型中的低通滤波能力是否能提高模型性能，以及如何使用优化的卷积操作来实现这一目标。</li>
<li>methods: 作者通过spectral analysis来研究现有的卷积模型，并发现提高卷积操作中的低通滤波能力也可以提高模型性能。为了解释这一观察，作者提出了一种假设，即使用优化的卷积操作可以capture balance的表示形式，从而提高模型性能。</li>
<li>results: 作者通过对视觉特征的分解和组合来验证这一假设，并提出了一种新的卷积操作名为SPAM。通过使用SPAM操作，作者提出了一种名为SPANet的新模型，并通过实验表明了这种方法可以提高多个计算机视觉任务的性能。<details>
<summary>Abstract</summary>
Recent studies show that self-attentions behave like low-pass filters (as opposed to convolutions) and enhancing their high-pass filtering capability improves model performance. Contrary to this idea, we investigate existing convolution-based models with spectral analysis and observe that improving the low-pass filtering in convolution operations also leads to performance improvement. To account for this observation, we hypothesize that utilizing optimal token mixers that capture balanced representations of both high- and low-frequency components can enhance the performance of models. We verify this by decomposing visual features into the frequency domain and combining them in a balanced manner. To handle this, we replace the balancing problem with a mask filtering problem in the frequency domain. Then, we introduce a novel token-mixer named SPAM and leverage it to derive a MetaFormer model termed as SPANet. Experimental results show that the proposed method provides a way to achieve this balance, and the balanced representations of both high- and low-frequency components can improve the performance of models on multiple computer vision tasks. Our code is available at $\href{https://doranlyong.github.io/projects/spanet/}{\text{https://doranlyong.github.io/projects/spanet/}}$.
</details>
<details>
<summary>摘要</summary>
We verify this by decomposing visual features into the frequency domain and combining them in a balanced manner. To handle this, we replace the balancing problem with a mask filtering problem in the frequency domain. We then introduce a novel token-mixer named SPAM and leverage it to derive a MetaFormer model termed as SPANet. Experimental results show that the proposed method provides a way to achieve this balance, and the balanced representations of both high- and low-frequency components can improve the performance of models on multiple computer vision tasks. Our code is available at <https://doranlyong.github.io/projects/spanet/>.
</details></li>
</ul>
<hr>
<h2 id="EndoNet-model-for-automatic-calculation-of-H-score-on-histological-slides"><a href="#EndoNet-model-for-automatic-calculation-of-H-score-on-histological-slides" class="headerlink" title="EndoNet: model for automatic calculation of H-score on histological slides"></a>EndoNet: model for automatic calculation of H-score on histological slides</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11562">http://arxiv.org/abs/2308.11562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Egor Ushakov, Anton Naumov, Vladislav Fomberg, Polina Vishnyakova, Aleksandra Asaturova, Alina Badlaeva, Anna Tregubova, Evgeny Karpulevich, Gennady Sukhikh, Timur Fatkhudinov</li>
<li>for: 这个论文的目的是提出一种自动计算 Histology 扫描片上protein表达的方法，以帮助病理学家提高工作效率。</li>
<li>methods: 该方法使用人工神经网络，包括两个主要部分：一是一个检测模型，可以预测核心点的位置；二是一个 H-score 模块，使用 Mean 像素值来计算 H-score 的值。</li>
<li>results: 该模型在 1780 个标注的TILE中得到了 0.77 mAP 的评估结果，并且可以根据特定的专家或实验室来修改计算方法，以达到更高的准确率和速度。<details>
<summary>Abstract</summary>
H-score is a semi-quantitative method used to assess the presence and distribution of proteins in tissue samples by combining the intensity of staining and percentage of stained nuclei. It is widely used but time-consuming and can be limited in accuracy and precision. Computer-aided methods may help overcome these limitations and improve the efficiency of pathologists' workflows. In this work, we developed a model EndoNet for automatic calculation of H-score on histological slides. Our proposed method uses neural networks and consists of two main parts. The first is a detection model which predicts keypoints of centers of nuclei. The second is a H-score module which calculates the value of the H-score using mean pixel values of predicted keypoints. Our model was trained and validated on 1780 annotated tiles with a shape of 100x100 $\mu m$ and performed 0.77 mAP on a test dataset. Moreover, the model can be adjusted to a specific specialist or whole laboratory to reproduce the manner of calculating the H-score. Thus, EndoNet is effective and robust in the analysis of histology slides, which can improve and significantly accelerate the work of pathologists.
</details>
<details>
<summary>摘要</summary>
“H-score”是一种半量化方法，用于评估组织样本中蛋白质的存在和分布，通过组合染色强度和染色细胞总数的百分比。它广泛应用，但是时间consuming和精度有限。计算机助け方法可以帮助超越这些限制，提高病理医生的工作流程效率。在这个工作中，我们开发了一个名为EndoNet的自动计算H-score模型。我们的提议方法包括两个主要部分：第一部分是一个检测模型，可以预测细胞核心点的位置；第二部分是H-score模块，可以使用预测的细胞核心点的平均像素值来计算H-score的值。我们的模型在1780个注释的矩阵上训练和验证，并在测试集上达到了0.77 mAP的性能。此外，模型还可以根据特定的专家或实验室来调整H-score的计算方式，以便复制专家的计算方式。因此，EndoNet是一种有效和可靠的 Histology 板准分析方法，可以提高和加速病理医生的工作。
</details></li>
</ul>
<hr>
<h2 id="Target-Grounded-Graph-Aware-Transformer-for-Aerial-Vision-and-Dialog-Navigation"><a href="#Target-Grounded-Graph-Aware-Transformer-for-Aerial-Vision-and-Dialog-Navigation" class="headerlink" title="Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation"></a>Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11561">http://arxiv.org/abs/2308.11561</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yifeisu/avdn-challenge">https://github.com/yifeisu/avdn-challenge</a></li>
<li>paper_authors: Yifei Su, Dong An, Yuan Xu, Kehan Chen, Yan Huang</li>
<li>for: 本文描述了ICCV 2023年AVDN挑战赛中的胜利入场。比赛要求一架无人机智能代理人根据对话历史与空中观察结合寻找目的地。</li>
<li>methods: 我们提出了一个Target-Grounded Graph-Aware Transformer（TG-GAT）框架，以便更好地跨Modal 锚定能力。TG-GAT首先利用图像感知 transformer 捕捉空时间相关性，从而改善导航状态跟踪和稳定行动规划。此外，我们还设计了一个辅助视觉锚定任务，以提高代理人对参照地标的认识。</li>
<li>results: 我们的TG-GAT框架在SPL和SR指标上比基eline提高2.2%和3.0%，减少了数据缺乏的限制。代码可以在<a target="_blank" rel="noopener" href="https://github.com/yifeisu/avdn-challenge%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yifeisu/avdn-challenge中找到。</a><details>
<summary>Abstract</summary>
This report details the method of the winning entry of the AVDN Challenge in ICCV 2023. The competition addresses the Aerial Navigation from Dialog History (ANDH) task, which requires a drone agent to associate dialog history with aerial observations to reach the destination. For better cross-modal grounding abilities of the drone agent, we propose a Target-Grounded Graph-Aware Transformer (TG-GAT) framework. Concretely, TG-GAT first leverages a graph-aware transformer to capture spatiotemporal dependency, which benefits navigation state tracking and robust action planning. In addition, an auxiliary visual grounding task is devised to boost the agent's awareness of referred landmarks. Moreover, a hybrid augmentation strategy based on large language models is utilized to mitigate data scarcity limitations. Our TG-GAT framework won the AVDN Challenge 2023, with 2.2% and 3.0% absolute improvements over the baseline on SPL and SR metrics, respectively. The code is available at https://github.com/yifeisu/avdn-challenge.
</details>
<details>
<summary>摘要</summary>
这份报告详细介绍了ICCV 2023年AVDN Challenge的赢家方案。该比赛目标是通过对话历史（ANDH）任务，让机器人在机器人对话历史中 associates  aerial observations 以达到目的地。为了提高机器人的跨模态固定能力，我们提议了Target-Grounded Graph-Aware Transformer（TG-GAT）框架。具体来说，TG-GAT首先利用图像感知器来捕捉空间时间相互关系，这有利于导航状态追踪和可靠的行动规划。此外，我们还提出了一个辅助视觉定位任务，以提高机器人对被引用的地标的认知。此外，我们采用基于大型自然语言模型的混合增强策略，以降低数据缺乏的限制。我们的TG-GAT框架在SPL和SR指标上比基eline表现出2.2%和3.0%的绝对提升。代码可以在https://github.com/yifeisu/avdn-challenge中找到。
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Synthetic-Image-Source-Attribution"><a href="#Open-Set-Synthetic-Image-Source-Attribution" class="headerlink" title="Open Set Synthetic Image Source Attribution"></a>Open Set Synthetic Image Source Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11557">http://arxiv.org/abs/2308.11557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbang Fang, Tai D. Nguyen, Matthew C. Stamm</li>
<li>for: 这个论文旨在解决新型图像生成技术中的谍影响，即使图像来源不明确，也可以准确地判断图像是否来自新未经见过的生成器。</li>
<li>methods: 该论文提出了一种基于 metric learning 的图像来源归属方法，通过学习可转移的嵌入来区分不同的图像生成器，并且可以在开放集成enario中进行图像来源归属。</li>
<li>results: 经过一系列实验，论文表明该方法可以准确地归属新型图像的来源，并且可以在不同的图像生成器和数据集上实现良好的性能。<details>
<summary>Abstract</summary>
AI-generated images have become increasingly realistic and have garnered significant public attention. While synthetic images are intriguing due to their realism, they also pose an important misinformation threat. To address this new threat, researchers have developed multiple algorithms to detect synthetic images and identify their source generators. However, most existing source attribution techniques are designed to operate in a closed-set scenario, i.e. they can only be used to discriminate between known image generators. By contrast, new image-generation techniques are rapidly emerging. To contend with this, there is a great need for open-set source attribution techniques that can identify when synthetic images have originated from new, unseen generators. To address this problem, we propose a new metric learning-based approach. Our technique works by learning transferrable embeddings capable of discriminating between generators, even when they are not seen during training. An image is first assigned to a candidate generator, then is accepted or rejected based on its distance in the embedding space from known generators' learned reference points. Importantly, we identify that initializing our source attribution embedding network by pretraining it on image camera identification can improve our embeddings' transferability. Through a series of experiments, we demonstrate our approach's ability to attribute the source of synthetic images in open-set scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-event-Video-Text-Retrieval"><a href="#Multi-event-Video-Text-Retrieval" class="headerlink" title="Multi-event Video-Text Retrieval"></a>Multi-event Video-Text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11551">http://arxiv.org/abs/2308.11551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gengyuanmax/mevtr">https://github.com/gengyuanmax/mevtr</a></li>
<li>paper_authors: Gengyuan Zhang, Jisen Ren, Jindong Gu, Volker Tresp</li>
<li>for: 该论文targets the Multi-event Video-Text Retrieval (MeVTR) task, which is a niche scenario of the conventional Video-Text Retrieval Task, where each video contains multiple different events.</li>
<li>methods: 该论文提出了一种简单的模型，即Me-Retriever，which incorporates key event video representation and a new MeVTR loss for the MeVTR task.</li>
<li>results: 实验表明，该模型在Video-to-Text和Text-to-Video任务上表现出色，超越了其他模型，成为MeVTR任务的robust基础。<details>
<summary>Abstract</summary>
Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We present a simple model, Me-Retriever, which incorporates key event video representation and a new MeVTR loss for the MeVTR task. Comprehensive experiments show that this straightforward framework outperforms other models in the Video-to-Text and Text-to-Video tasks, effectively establishing a robust baseline for the MeVTR task. We believe this work serves as a strong foundation for future studies. Code is available at https://github.com/gengyuanmax/MeVTR.
</details>
<details>
<summary>摘要</summary>
视频文本检索（VTR）是当今互联网巨量视频文本数据时代的一项重要多模式任务。大量工作都是使用两栅视语模型建立 joint 表示方式来学习视频文本对的连接，这种方法在VTR任务上成为了主要方法。然而，这些模型假设视频内容和文本之间是对应的，忽略了实际应用中更加实际的情况：视频内容通常包含多个事件，而文本则是用户查询或页面元数据，它们通常对应单个事件。这种情况导致了以前的训练目标与实际应用之间的差距，从而使得早期的模型在推理时可能会出现性能下降的问题。在这种情况下，我们引入了多事件视频文本检索（MeVTR）任务，解决了视频中包含多个不同事件的场景，这是传统的VTR任务的一个特殊情况。我们提出了一个简单的模型，Me-Retriever，它包括关键事件视频表示和一个新的MeVTR损失函数。我们进行了广泛的实验，结果表明这个简单的框架可以在视频到文本和文本到视频两个任务中击败其他模型，有效地建立了MeVTR任务的坚实基础。我们认为这项工作为未来研究提供了强大的基础。代码可以在 GitHub 上找到：https://github.com/gengyuanmax/MeVTR。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/cs.CV_2023_08_23/" data-id="clltaagnf003er888gf872ikk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.LG_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.LG_2023_08_23/">cs.LG - 2023-08-23 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Challenges-of-Machine-Learning-for-Trust-and-Safety-A-Case-Study-on-Misinformation-Detection"><a href="#The-Challenges-of-Machine-Learning-for-Trust-and-Safety-A-Case-Study-on-Misinformation-Detection" class="headerlink" title="The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection"></a>The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12215">http://arxiv.org/abs/2308.12215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ramybaly/News-Media-Reliability">https://github.com/ramybaly/News-Media-Reliability</a></li>
<li>paper_authors: Madelyne Xiao, Jonathan Mayer</li>
<li>for: 本研究旨在探讨机器学习在信任和安全问题上的应用，以推识信息检测为例。</li>
<li>methods: 研究者通过系мати化文献检测自动推识谣言的方法，从270篇引用次多的论文中检测出了 significan t shortcomings。</li>
<li>results: 研究发现，现有的论文中的检测任务与实际应用中的挑战存在 significan t的差异，数据和代码的可用性差、模型评价方法不独立、模型在不同领域数据上的泛化能力差。<details>
<summary>Abstract</summary>
We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is for future work to avoid the pitfalls that we identify.
</details>
<details>
<summary>摘要</summary>
我们检查机器学习在保障和安全问题上的应用中存在的偏差，用误信息探测作为例子。我们系统化了 relate field 中270篇最具影响力的论文，然后对这些论文中的数据和代码可用性、设计异常、可重现性和泛化能力进行了检查。我们发现了 significante 缺陷，这些缺陷可能会质疑已经宣称的性能和实用性。检测任务与在线服务实际面临的挑战有很大差异，数据集和模型评估 часто不符合实际情况，评估 часто与模型训练无关。数据和代码可用性很差，模型无法在域外数据上Generalization well。基于这些结果，我们提出了评估机器学习应用到保障和安全问题的建议。我们的目标是为未来的工作避免这些偏差。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies"><a href="#Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies" class="headerlink" title="Learning to Learn Financial Networks for Optimising Momentum Strategies"></a>Learning to Learn Financial Networks for Optimising Momentum Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12212">http://arxiv.org/abs/2308.12212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyue Pu, Stefan Zohren, Stephen Roberts, Xiaowen Dong</li>
<li>for: 这篇论文旨在提供一种新的风险豁免，即基于金融网络的资产关系来预测未来的回报。</li>
<li>methods: 本论文提出了一种基于机器学习的综合学习框架，即L2GMOM，可以同时学习金融网络和优化交易信号。L2GMOM的模型是一种高度可读性的神经网络，其架构来自于算法抽象。</li>
<li>results: 根据64个连续未来合约的回报测试，L2GMOM可以显著提高股票资产的利润和风险控制，Sharpe比率为1.74，覆盖20年的时间段。<details>
<summary>Abstract</summary>
Network momentum provides a novel type of risk premium, which exploits the interconnections among assets in a financial network to predict future returns. However, the current process of constructing financial networks relies heavily on expensive databases and financial expertise, limiting accessibility for small-sized and academic institutions. Furthermore, the traditional approach treats network construction and portfolio optimisation as separate tasks, potentially hindering optimal portfolio performance. To address these challenges, we propose L2GMOM, an end-to-end machine learning framework that simultaneously learns financial networks and optimises trading signals for network momentum strategies. The model of L2GMOM is a neural network with a highly interpretable forward propagation architecture, which is derived from algorithm unrolling. The L2GMOM is flexible and can be trained with diverse loss functions for portfolio performance, e.g. the negative Sharpe ratio. Backtesting on 64 continuous future contracts demonstrates a significant improvement in portfolio profitability and risk control, with a Sharpe ratio of 1.74 across a 20-year period.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ULDP-FL-Federated-Learning-with-Across-Silo-User-Level-Differential-Privacy"><a href="#ULDP-FL-Federated-Learning-with-Across-Silo-User-Level-Differential-Privacy" class="headerlink" title="ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy"></a>ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12210">http://arxiv.org/abs/2308.12210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fumiyukikato/uldp-fl">https://github.com/fumiyukikato/uldp-fl</a></li>
<li>paper_authors: Fumiyuki Kato, Li Xiong, Shun Takagi, Yang Cao, Masatoshi Yoshikawa</li>
<li>for: This paper aims to provide user-level differential privacy (DP) in cross-silo federated learning (FL) settings, where a single user’s data may belong to multiple silos.</li>
<li>methods: The proposed algorithm, called ULDP-FL, uses per-user weighted clipping to directly ensure user-level DP, departing from group-privacy approaches. The algorithm also utilizes cryptographic building blocks to enhance its utility and provide private implementation.</li>
<li>results: The authors provide a theoretical analysis of the algorithm’s privacy and utility, and showcase substantial improvements in privacy-utility trade-offs under user-level DP compared to baseline methods through empirical experiments on real-world datasets.<details>
<summary>Abstract</summary>
Differentially Private Federated Learning (DP-FL) has garnered attention as a collaborative machine learning approach that ensures formal privacy. Most DP-FL approaches ensure DP at the record-level within each silo for cross-silo FL. However, a single user's data may extend across multiple silos, and the desired user-level DP guarantee for such a setting remains unknown. In this study, we present ULDP-FL, a novel FL framework designed to guarantee user-level DP in cross-silo FL where a single user's data may belong to multiple silos. Our proposed algorithm directly ensures user-level DP through per-user weighted clipping, departing from group-privacy approaches. We provide a theoretical analysis of the algorithm's privacy and utility. Additionally, we enhance the algorithm's utility and showcase its private implementation using cryptographic building blocks. Empirical experiments on real-world datasets show substantial improvements in our methods in privacy-utility trade-offs under user-level DP compared to baseline methods. To the best of our knowledge, our work is the first FL framework that effectively provides user-level DP in the general cross-silo FL setting.
</details>
<details>
<summary>摘要</summary>
受众级 differentially private federated learning（DP-FL）已经吸引了关注，这是一种合作机器学习方法，确保正式隐私。大多数 DP-FL 方法在每个缓存中保证了DP，但是一个用户的数据可能会分布在多个缓存中，并且未知用户级DP保证。在这种情况下，我们提出了ULDP-FL，一种新的 federated learning 框架，确保了用户级DP在跨缓存FL中。我们的提议算法直接确保用户级DP通过每个用户的质量截断，而不是GROUP-privacy方法。我们提供了算法的理论分析，包括隐私和用户性能的分析。此外，我们还提高了算法的用户性能，并使用密码学建筑块实现私有的实现。实验表明，我们的方法在保证用户级DP的情况下，与基eline方法相比，在隐私-用户性能质量上具有显著提升。我们知道，我们的工作是首次在通用跨缓存FL设置中提供了用户级DP的FL框架。
</details></li>
</ul>
<hr>
<h2 id="Curriculum-Learning-with-Adam-The-Devil-Is-in-the-Wrong-Details"><a href="#Curriculum-Learning-with-Adam-The-Devil-Is-in-the-Wrong-Details" class="headerlink" title="Curriculum Learning with Adam: The Devil Is in the Wrong Details"></a>Curriculum Learning with Adam: The Devil Is in the Wrong Details</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12202">http://arxiv.org/abs/2308.12202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Weber, Jaap Jumelet, Paul Michel, Elia Bruni, Dieuwke Hupkes</li>
<li>for: 本研究旨在探讨CURRENT LEARNING PROGRESS（CL）方法在自然语言处理（NLP）领域的应用。</li>
<li>methods: 本研究使用了许多最近的CURRENT LEARNING PROGRESS方法的复制和扩展，并发现这些方法在NLP领域的结果具有许多不稳定性。</li>
<li>results: 研究发现，当CURRENT LEARNING PROGRESS方法与受欢迎的Adam优化算法结合使用时，它们经常学习到不适合选择的优化参数，导致结果不佳。研究还发现，不同的手动和自动CL方法在不同的场景下的表现都不佳， none of them outperforms optimisation with only Adam with well-chosen hyperparameters。<details>
<summary>Abstract</summary>
Curriculum learning (CL) posits that machine learning models -- similar to humans -- may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chosen hyperparameters. As such, our results contribute to understanding why CL methods work, but at the same time urge caution when claiming positive results.
</details>
<details>
<summary>摘要</summary>
学习课程（CL）认为机器学习模型，类似于人类，可能更有效地从匹配其当前学习进程的数据中学习。然而，CL方法仍然不够了解，特别是在自然语言处理（NLP）领域，只有有限的成功。在这篇论文中，我们探究了这一点。从尝试复制和扩展一些最近的课程方法开始，我们发现它们在NLP领域的结果很有限制。我们进行了深入的分析，发现在使用Adam优化算法时，课程经常学习适应不合适的优化参数。我们通过不同的常见手动编制和自动生成CL方法的几个案例研究，发现 none of them outperforms 仅使用Adam算法和合适的超参数。因此，我们的结果对CL方法的工作 Mechanism 提供了更深入的理解，同时也警告使用CL方法时应该有谨慎。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Drug-Solubility-Using-Different-Machine-Learning-Methods-–-Linear-Regression-Model-with-Extracted-Chemical-Features-vs-Graph-Convolutional-Neural-Network"><a href="#Predicting-Drug-Solubility-Using-Different-Machine-Learning-Methods-–-Linear-Regression-Model-with-Extracted-Chemical-Features-vs-Graph-Convolutional-Neural-Network" class="headerlink" title="Predicting Drug Solubility Using Different Machine Learning Methods – Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network"></a>Predicting Drug Solubility Using Different Machine Learning Methods – Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12325">http://arxiv.org/abs/2308.12325</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Ho, Zhao-Heng Yin, Colin Zhang, Henry Overhauser, Kyle Swanson, Yang Ha</li>
<li>for: 预测药物的溶解度是药品研发领域中重要的任务，这个问题在现代计算机资源的帮助下得到了进一步研究。</li>
<li>methods: 本研究使用了两种机器学习模型：一个线性回归模型和一个图 convolutional neural network 模型，在多个实验数据集上应用了这两种方法。两种方法都可以做出合理的预测，但GCNN模型的性能最好。</li>
<li>results: 使用GCNN模型可以获得比较好的预测结果，但目前GCNN模型是一个黑盒模型，而线性回归模型的特征重要性分析可以提供更多有关化学结构下的物理性质的信息。使用线性回归模型，我们显示了各种 функциональ组在总溶解度上的影响。<details>
<summary>Abstract</summary>
Predicting the solubility of given molecules is an important task in the pharmaceutical industry, and consequently this is a well-studied topic. In this research, we revisited this problem with the advantage of modern computing resources. We applied two machine learning models, a linear regression model and a graph convolutional neural network model, on multiple experimental datasets. Both methods can make reasonable predictions while the GCNN model had the best performance. However, the current GCNN model is a black box, while feature importance analysis from the linear regression model offers more insights into the underlying chemical influences. Using the linear regression model, we show how each functional group affects the overall solubility. Ultimately, knowing how chemical structure influences chemical properties is crucial when designing new drugs. Future work should aim to combine the high performance of GCNNs with the interpretability of linear regression, unlocking new advances in next generation high throughput screening.
</details>
<details>
<summary>摘要</summary>
预测给定分子的溶解度是药物工业中重要的任务，因此这是一个广泛研究的话题。在这项研究中，我们利用现代计算资源重新探讨了这个问题。我们使用了两种机器学习模型：线性回归模型和图 convolutional neural network 模型，并在多个实验数据集上应用了它们。两种方法都可以做出合理的预测，但GCNN模型的性能最佳。然而，当前GCNN模型是黑盒模型，而线性回归模型的功能重要性分析却可以提供更多有关化学影响的启示。使用线性回归模型，我们展示了每个 функциональ组如何影响总的溶解度。最终，知道化学结构如何影响化学性质是设计新药物的关键。未来的工作应该努力将高性能的 GCNN 与可解释的线性回归结合起来，这将开启下一代高通过筛选的新进展。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Knowledge-Driven-Deep-Learning-for-3D-Magnetic-Inversion"><a href="#Self-Supervised-Knowledge-Driven-Deep-Learning-for-3D-Magnetic-Inversion" class="headerlink" title="Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion"></a>Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12193">http://arxiv.org/abs/2308.12193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinshuo Li, Zhuo Jia, Wenkai Lu, Cao Song</li>
<li>for: 这个研究的目的是提出一种基于自我监督深度学习的非破坏性地磁探测方法，以便估算地下 Distribution of susceptibility.</li>
<li>methods: 这个方法使用了自我监督深度学习，并且具有知识驱动模组，以便更好地解释模型的运作。</li>
<li>results: 实验结果显示，提出的方法可以实现高效的地磁探测，并且可以提供更好的结果。<details>
<summary>Abstract</summary>
The magnetic inversion method is one of the non-destructive geophysical methods, which aims to estimate the subsurface susceptibility distribution from surface magnetic anomaly data. Recently, supervised deep learning methods have been widely utilized in lots of geophysical fields including magnetic inversion. However, these methods rely heavily on synthetic training data, whose performance is limited since the synthetic data is not independently and identically distributed with the field data. Thus, we proposed to realize magnetic inversion by self-supervised deep learning. The proposed self-supervised knowledge-driven 3D magnetic inversion method (SSKMI) learns on the target field data by a closed loop of the inversion and forward models. Given that the parameters of the forward model are preset, SSKMI can optimize the inversion model by minimizing the mean absolute error between observed and re-estimated surface magnetic anomalies. Besides, there is a knowledge-driven module in the proposed inversion model, which makes the deep learning method more explicable. Meanwhile, comparative experiments demonstrate that the knowledge-driven module can accelerate the training of the proposed method and achieve better results. Since magnetic inversion is an ill-pose task, SSKMI proposed to constrain the inversion model by a guideline in the auxiliary loop. The experimental results demonstrate that the proposed method is a reliable magnetic inversion method with outstanding performance.
</details>
<details>
<summary>摘要</summary>
magnetische inversie-methode is een van de niet-verwoestende geofysische methodes, die wordt gebruikt om de subsurface gelijkstroomsverdeling te schatten vanaf bovenstaande magneet-anomaliedata. Recent hebben supervisionele diepe-lerenmethodes werden breed toegepast in verschillende geofysische velden, waaronder magneet-inversie. However, these methods rely heavily on synthetic training data, whose performance is limited since the synthetic data is not independently and identically distributed with the field data. Thus, we proposed to realize magneet-inversie by self-supervised deep learning.De voorgestelde self-supervised knowledge-driven 3D magneet-inversie-methode (SSKMI) leert op het doelveld data van de inversie en de voorwaardelijke modellen. Given that the parameters of the voorwaardelijke model are preset, SSKMI can optimize the inversie model by minimizing the mean absolute error between observed and re-estimated surface magneet-anomalieën. Besides, there is a knowledge-driven module in the proposed inversie model, which makes the diepe-lerenmethode more explicable. Meanwhile, comparative experiments demonstrate that the knowledge-driven module can accelerate the training of the proposed method and achieve better results.Since magneet-inversie is an ill-pose task, SSKMI proposed to constrain the inversie model by a guideline in the auxiliary loop. The experimental results demonstrate that the proposed method is a reliable magneet-inversie-methode with outstanding performance.
</details></li>
</ul>
<hr>
<h2 id="Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques"><a href="#Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques" class="headerlink" title="Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques"></a>Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12192">http://arxiv.org/abs/2308.12192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophie A. Neubauer, Radu Grosu</li>
<li>for: 这篇论文旨在统一地描述某些时间连续过程的有序和随机拉格朗日方法，以确定这些过程的行为稳定性。</li>
<li>methods: 这篇论文使用了LRT-NG、SLR和GoTube算法来构建紧凑的可达集，即在给定时间范围内可达的状态集。这些算法提供了确定性和随机性的保证。</li>
<li>results: 实验表明，使用拉格朗日方法可以比LRT、Flow*和CAPD更高效地分析不同类型的时间连续模型的稳定性。<details>
<summary>Abstract</summary>
This paper presents, in a unified fashion, deterministic as well as statistical Lagrangian-verification techniques. They formally quantify the behavioral robustness of any time-continuous process, formulated as a continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube, algorithms for constructing a tight reachtube, that is, an over-approximation of the set of states reachable within a given time-horizon, and provide guarantees for the reachtube bounds. We compare the usage of the variational equations, associated to the system equations, the mean value theorem, and the Lipschitz constants, in achieving deterministic and statistical guarantees. In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation, to compute the radius of an ellipsoid in an optimal metric, which over-approximates the set of reachable states. In SLR and GoTube, we get statistical guarantees, by using the Lipschitz constants to compute local balls around samples. These are needed to calculate the probability of having found an upper bound, of the true maximum perturbation at every timestep. Our experiments demonstrate the superior performance of Lagrangian techniques, when compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness analysis of various continuous-depth models.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文提出了一种统一的方法，即抽象的Lagrangian验证技术，可以确定时间连续过程的行为Robustness。这些技术基于构建一个紧缩的reachtube，即在给定时间范围内可达的状态的覆盖，并提供了reachtube bound的保证。文章介绍了LRT-NG、SLR和GoTube三种算法，并比较它们在使用变量方程、mean value theorem和Lipschitz常数来实现束缚和统计保证方面的表现。实验表明LAGRANGIAN技术在对LRT、Flow*和CAPD等方法的比较中表现出了superiority，并 ilustrated its use in various continuous-depth models的Robustness分析。
</details></li>
</ul>
<hr>
<h2 id="Development-and-external-validation-of-a-lung-cancer-risk-estimation-tool-using-gradient-boosting"><a href="#Development-and-external-validation-of-a-lung-cancer-risk-estimation-tool-using-gradient-boosting" class="headerlink" title="Development and external validation of a lung cancer risk estimation tool using gradient-boosting"></a>Development and external validation of a lung cancer risk estimation tool using gradient-boosting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12188">http://arxiv.org/abs/2308.12188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/plbenveniste/lungcancerrisk">https://github.com/plbenveniste/lungcancerrisk</a></li>
<li>paper_authors: Pierre-Louis Benveniste, Julie Alberge, Lei Xing, Jean-Emmanuel Bibault</li>
<li>For: The paper aims to develop a machine learning tool for estimating the likelihood of developing lung cancer within five years, which can help individuals make informed decisions regarding lung cancer screening.* Methods: The study uses two datasets, PLCO and NLST, consisting of comprehensive information on risk factors, clinical measurements, and outcomes related to lung cancer. The ML model is trained on the pre-processed PLCO dataset and tested on the NLST dataset, using XGBoost algorithm. The model incorporates features such as age, gender, smoking history, medical diagnoses, and family history of lung cancer.* Results: The model is well-calibrated (Brier score&#x3D;0.044) and shows good performance on both datasets, with ROC-AUC of 82% on the PLCO dataset and 70% on the NLST dataset. The PR-AUC is 29% and 11% respectively. The developed ML tool provides a freely available web application for estimating the likelihood of developing lung cancer within five years.<details>
<summary>Abstract</summary>
Lung cancer is a significant cause of mortality worldwide, emphasizing the importance of early detection for improved survival rates. In this study, we propose a machine learning (ML) tool trained on data from the PLCO Cancer Screening Trial and validated on the NLST to estimate the likelihood of lung cancer occurrence within five years. The study utilized two datasets, the PLCO (n=55,161) and NLST (n=48,595), consisting of comprehensive information on risk factors, clinical measurements, and outcomes related to lung cancer. Data preprocessing involved removing patients who were not current or former smokers and those who had died of causes unrelated to lung cancer. Additionally, a focus was placed on mitigating bias caused by censored data. Feature selection, hyper-parameter optimization, and model calibration were performed using XGBoost, an ensemble learning algorithm that combines gradient boosting and decision trees. The ML model was trained on the pre-processed PLCO dataset and tested on the NLST dataset. The model incorporated features such as age, gender, smoking history, medical diagnoses, and family history of lung cancer. The model was well-calibrated (Brier score=0.044). ROC-AUC was 82% on the PLCO dataset and 70% on the NLST dataset. PR-AUC was 29% and 11% respectively. When compared to the USPSTF guidelines for lung cancer screening, our model provided the same recall with a precision of 13.1% vs. 9.3% on the PLCO dataset and 3.2% vs. 3.1% on the NLST dataset. The developed ML tool provides a freely available web application for estimating the likelihood of developing lung cancer within five years. By utilizing risk factors and clinical data, individuals can assess their risk and make informed decisions regarding lung cancer screening. This research contributes to the efforts in early detection and prevention strategies, aiming to reduce lung cancer-related mortality rates.
</details>
<details>
<summary>摘要</summary>
全球范围内，肺癌是一种重要的死亡原因，因此早期检测的重要性得到了更多的注意。在本研究中，我们提出了一种基于机器学习（ML）技术的工具，用于在5年内lung cancer的发生可能性的估计。该工具基于PLCO癌症检测试验和NLST试验数据进行训练和验证。研究使用了两个数据集：PLCO（n=55,161）和NLST（n=48,595），这两个数据集包含了肺癌的风险因素、临床测量和结果的全面信息。数据处理包括移除不是当前或前任吸烟者和不相关于肺癌的死亡病人，以及减少偏见的报告数据。我们使用XGBoost算法进行特征选择、超参数优化和模型约束。模型在PLCO数据集上训练，并在NLST数据集上测试。模型包含了年龄、性别、吸烟历史、医疗诊断和家族史肺癌的特征。模型具有良好的准备（Brier分数=0.044），ROC-AUC为82%、PLCO数据集上和70%、NLST数据集上。PR-AUC分别为29%和11%。与美国预防服务委员会（USPSTF）肺癌检测指南相比，我们的模型具有同等的回快，但精度更高（9.3% vs. 13.1%、PLCO数据集上，3.1% vs. 3.2%、NLST数据集上）。我们开发的ML工具提供了一个免费的网上应用程序，用于在5年内lung cancer的发生可能性的估计。通过利用风险因素和临床数据，个人可以评估自己的风险，并做出了有知情的决策关于肺癌检测。这些研究贡献到了早期检测和预防策略的努力，以减少肺癌相关的死亡率。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning"><a href="#Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning" class="headerlink" title="Unsupervised anomalies detection in IIoT edge devices networks using federated learning"></a>Unsupervised anomalies detection in IIoT edge devices networks using federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12175">http://arxiv.org/abs/2308.12175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niyomukiza Thamar, Hossam Samy Elsaid Sharara<br>for: 这个研究是为了解决跨多个IoT设备之间的数据共享问题，以及保护数据的隐私问题。methods: 这个研究使用了联邦学习（Federated Learning）的分布式机器学习方法，并使用了FedAvg算法。results: 研究发现，使用FedAvg算法可以在IoT&#x2F;IIoT设备上进行机器学习模型训练，并且结果与中央化机器学习方法相似。但是，研究也发现了一些缺陷，例如在训练过程中不稳定的设备可能会导致伪阳性警示。为了解决这个问题，研究人员提出了一个公平的FedAvg算法，将在未来的工作中进行评估。<details>
<summary>Abstract</summary>
In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules. However, some owners are reluctant of availing their data out of the company due to data security concerns. Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself. In this scenario, data is not share over the network for training purpose. Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session. The devices could be chosen at random, and a device can be aborted. The resulting models are sent to the coordinating server and then average models from the devices that finished training. The process is repeated until a desired model accuracy is achieved. By doing this, FL approach solves the privacy problem for IoT/ IIoT devices that held sensitive data for the owners. In this paper, we leverage the benefits of FL and implemented Fedavg algorithm on a recent dataset that represent the modern IoT/ IIoT device networks. The results were almost the same as the centralized machine learning approach. We also evaluated some shortcomings of Fedavg such as unfairness that happens during the training when struggling devices do not participate for every stage of training. This inefficient training of local or global model could lead in a high number of false alarms in intrusion detection systems for IoT/IIoT gadgets developed using Fedavg. Hence, after evaluating the FedAv deep auto encoder with centralized deep auto encoder ML, we further proposed and designed a Fair Fedavg algorithm that will be evaluated in the future work.
</details>
<details>
<summary>摘要</summary>
在许多物联网设备之间的连接中，每个设备都会收集数据，通常需要将数据传输到中央服务器进行机器学习模型训练。然而，一些业主可能会拒绝将数据传输到公司外due to data security concerns。基于分布式机器学习的 Federated learning（FL）方法可以在设备上训练机器学习模型，从而解决数据隐私问题。在这种情况下，数据不会在训练过程中传输到网络。Fedavg是FL算法的一种实现，允许在训练过程中将模型复制到参与设备上。这些设备可以随机选择，并且设备可以被中止。获得的模型将被送往协调服务器，然后平均处理参与设备完成训练的模型。这个过程会重复，直到达到所需的模型精度。通过这种方法，FL方法解决了物联网/IIoT设备所拥有的敏感数据所有者的隐私问题。在这篇论文中，我们利用了FL的优点，并在当今物联网/IIoT设备网络上使用Fedavg算法进行实验。结果与中央机器学习方法的结果几乎相同。我们还评估了Fedavg的一些缺陷，如训练过程中不参与的设备会导致不公平性。这可能会导致在物联网/IIoT设备中的预测报警系统中出现高达数百个假报警。因此，我们在后续的工作中提出了一种公平的Fedavg算法，它将在未来进行评估。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-decision-focused-surrogate-modeling"><a href="#Data-driven-decision-focused-surrogate-modeling" class="headerlink" title="Data-driven decision-focused surrogate modeling"></a>Data-driven decision-focused surrogate modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12161">http://arxiv.org/abs/2308.12161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ddolab/decfocsurrmod">https://github.com/ddolab/decfocsurrmod</a></li>
<li>paper_authors: Rishabh Gupta, Qi Zhang</li>
<li>for: 这个论文是为了解决计算复杂非线性优化问题而设计的。</li>
<li>methods: 论文使用了数据驱动的框架，通过学习一个简单的、例如几何的优化模型，来减少决策预测错误。</li>
<li>results: 研究通过数字实验 validate了该框架，并与标准数据驱动模型相比较，表明该方法更加数据有效，并且生成了高决策预测精度的简单优化模型。<details>
<summary>Abstract</summary>
We introduce the concept of decision-focused surrogate modeling for solving computationally challenging nonlinear optimization problems in real-time settings. The proposed data-driven framework seeks to learn a simpler, e.g. convex, surrogate optimization model that is trained to minimize the decision prediction error, which is defined as the difference between the optimal solutions of the original and the surrogate optimization models. The learning problem, formulated as a bilevel program, can be viewed as a data-driven inverse optimization problem to which we apply a decomposition-based solution algorithm from previous work. We validate our framework through numerical experiments involving the optimization of common nonlinear chemical processes such as chemical reactors, heat exchanger networks, and material blending systems. We also present a detailed comparison of decision-focused surrogate modeling with standard data-driven surrogate modeling methods and demonstrate that our approach is significantly more data-efficient while producing simple surrogate models with high decision prediction accuracy.
</details>
<details>
<summary>摘要</summary>
我团队提出了一种决策抽象模型的概念，用于在实时设置下解决复杂非线性优化问题。我们的数据驱动框架寻求学习一个更简单的，例如几何，优化模型，以减少决策预测错误，即原始优化模型和代理优化模型的优化解的差异。我们将这种学习问题视为一个数据驱动的反向优化问题，并应用之前的研究中的分解法。我们通过数值实验，包括化学反应器、热交换网络和材料混合系统的优化问题，证明了我们的方法的有效性。此外，我们还进行了标准数据驱动模型和我们方法的比较，发现我们的方法在数据效率方面明显高于标准方法，并生成了高决策预测精度的简单模型。
</details></li>
</ul>
<hr>
<h2 id="A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models"><a href="#A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models" class="headerlink" title="A Probabilistic Fluctuation based Membership Inference Attack for Generative Models"></a>A Probabilistic Fluctuation based Membership Inference Attack for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12143">http://arxiv.org/abs/2308.12143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang</li>
<li>for: 该研究旨在开发一种黑盒式会员推断攻击（MIA），用于检测生成模型中是否存在记录。</li>
<li>methods: 该研究使用了现有的生成模型，并采用了多种正则化技术来避免过拟合。另外，该研究还使用了概率随机变量来检测生成模型中的记录分布变化。</li>
<li>results: 研究结果表明，在多个生成模型和数据集上，PFAMI可以提高攻击成功率（ASR）约27.9%，相比最佳基线。<details>
<summary>Abstract</summary>
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting these trends via analyzing the overall probabilistic fluctuations around given records. We conduct extensive experiments across multiple generative models and datasets, which demonstrate PFAMI can improve the attack success rate (ASR) by about 27.9% when compared with the best baseline.
</details>
<details>
<summary>摘要</summary>
具有简化的中文翻译如下：机器学习模型训练集中记录是否存在的问题被称为成员推断攻击（MIA）。在经典分类模型上，MIA已经得到了广泛的研究，而最近的研究又开始探索如何将MIA应用于生成模型。我们的调查表明，现有的生成模型MIA主要依赖于目标模型的过拟合。然而，过拟合可以通过多种正则化技术来避免，而现有的MIAs在实践中表现不佳。与过拟合不同的是，记忆是深度学习模型取得优化性能的关键因素，因此在生成模型中更常见。记忆导致生成记录随member记录的概率分布增长趋势，因此我们提出了一种基于概率变动的成员推断攻击方法（PFAMI）。PFAMI是一种黑盒子MIA，通过分析给定记录的总probabilistic fluctuations来推断成员。我们在多个生成模型和数据集上进行了广泛的实验，实验结果表明，PFAMI可以提高攻击成功率（ASR）约27.9%，相比最佳基准。
</details></li>
</ul>
<hr>
<h2 id="Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models"><a href="#Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models" class="headerlink" title="Masking Strategies for Background Bias Removal in Computer Vision Models"></a>Masking Strategies for Background Bias Removal in Computer Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12127">http://arxiv.org/abs/2308.12127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ananthu-aniraj/masking_strategies_bias_removal">https://github.com/ananthu-aniraj/masking_strategies_bias_removal</a></li>
<li>paper_authors: Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos</li>
<li>for: 本研究探讨了细化图像分类任务中背景引起的偏见问题，特别是当一些类别之间的差异非常微小，并且每个类别的样本数很少时，模型容易受到背景相关的偏见。</li>
<li>methods: 我们 investigate了两种遮盾策略来mitigate背景引起的偏见：早期遮盾（ removes background information at the input image level）和晚期遮盾（ selectively masks high-level spatial features corresponding to the background）。</li>
<li>results: 我们的实验结果表明，两种遮盾策略都能够提高模型对非典型背景的抗性，其中早期遮盾 consistently exhibiting the best OOD performance。另外，一种基于GAP-Pooled Patch token的ViT变体，combined with early masking, achieves the highest OOD robustness。<details>
<summary>Abstract</summary>
Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.
</details>
<details>
<summary>摘要</summary>
fine-grained 图像分类任务中，某些类别之间的差异可能非常微妙，同时每个类型的样本数往往很少，因此模型容易受到背景相关的偏见。为了更深入地理解这个重要问题，我们的研究investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT).我们探索了两种遮盖策略来 mitigate background-induced bias：早期遮盖， removes background information at the (input) image level，和晚期遮盖， selectively masks high-level spatial features corresponding to the background。我们在不同的遮盖策略下进行了广泛的实验，专注于其对于不同背景的泛化性能。结果显示，我们所提出的两种策略都能够提高对于不同背景的性能，其中 early masking  consistently exhibits the best OOD performance。宁可是，一种基于 GAP-Pooled Patch token-based classification 的 ViT 变体，结合 early masking， achieved the highest OOD robustness。
</details></li>
</ul>
<hr>
<h2 id="An-Accelerated-Block-Proximal-Framework-with-Adaptive-Momentum-for-Nonconvex-and-Nonsmooth-Optimization"><a href="#An-Accelerated-Block-Proximal-Framework-with-Adaptive-Momentum-for-Nonconvex-and-Nonsmooth-Optimization" class="headerlink" title="An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization"></a>An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12126">http://arxiv.org/abs/2308.12126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weifeng Yang, Wenwen Min</li>
<li>For: 非对称和非光滑优化问题* Methods: 加速块 proximal 线性框架（ABPL$^+$），以及随机排序更新阶段的扩展* Results: 可以 monotonically 降低函数值，并且可以在某些情况下提高步长和扩展参数，以及在多个实验中证明了其效果和灵活性。<details>
<summary>Abstract</summary>
We propose an accelerated block proximal linear framework with adaptive momentum (ABPL$^+$) for nonconvex and nonsmooth optimization. We analyze the potential causes of the extrapolation step failing in some algorithms, and resolve this issue by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm. Furthermore, we extends our algorithm to any scenario involving updating block variables with positive integers, allowing each cycle to randomly shuffle the update order of the variable blocks. Additionally, under mild assumptions, we prove that ABPL$^+$ can monotonically decrease the function value without strictly restricting the extrapolation parameters and step size, demonstrates the viability and effectiveness of updating these blocks in a random order, and we also more obviously and intuitively demonstrate that the derivative set of the sequence generated by our algorithm is a critical point set. Moreover, we demonstrate the global convergence as well as the linear and sublinear convergence rates of our algorithm by utilizing the Kurdyka-Lojasiewicz (K{\L}) condition. To enhance the effectiveness and flexibility of our algorithm, we also expand the study to the imprecise version of our algorithm and construct an adaptive extrapolation parameter strategy, which improving its overall performance. We apply our algorithm to multiple non-negative matrix factorization with the $\ell_0$ norm, nonnegative tensor decomposition with the $\ell_0$ norm, and perform extensive numerical experiments to validate its effectiveness and efficiency.
</details>
<details>
<summary>摘要</summary>
我们提出一种加速的块距离 próxima线性框架（ABPL$^+$）用于非拟合和非光滑优化。我们分析了一些算法中扩rapolation步骤失败的可能原因，并解决这个问题 by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm。此外，我们扩展了我们的算法，以便在更多的enario中更新块变量，并允许每个 цикла随机洗牌更新变量块的顺序。此外，在某些假设下，我们证明了ABPL$^+$可以 monotonically decrease the function value without strictly restricting the extrapolation parameters and step size，并且可以更加明确地示出该序列生成的 derivative set 是一个critical point set。此外，我们还证明了我们的算法的全球收敛性以及其线性和非线性收敛率，并使用Kurdyka-Lojasiewicz（K{\L））条件。为了提高我们的算法的效iveness和灵活性，我们还扩展了它的不精确版本，并构建了一种 adaptive extrapolation parameter strategy。我们应用我们的算法到多个非负矩阵因子化with the $\ell_0$ norm，非负tensor decomposition with the $\ell_0$ norm，并进行了广泛的数值实验来验证其效果和效率。
</details></li>
</ul>
<hr>
<h2 id="An-Open-Source-ML-Based-Full-Stack-Optimization-Framework-for-Machine-Learning-Accelerators"><a href="#An-Open-Source-ML-Based-Full-Stack-Optimization-Framework-for-Machine-Learning-Accelerators" class="headerlink" title="An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators"></a>An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12120">http://arxiv.org/abs/2308.12120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Esmaeilzadeh, Soroush Ghodrati, Andrew B. Kahng, Joon Kyung Kim, Sean Kinzer, Sayak Kundu, Rohan Mahapatra, Susmita Dey Manasi, Sachin Sapatnekar, Zhiang Wang, Ziqing Zeng</li>
<li>for: 这 paper 是为了探讨 físical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN machine learning (ML) algorithms。</li>
<li>methods: 这 paper 使用了 backend power, performance, and area (PPA) analysis 和 frontend performance simulation，以实现backend PPA 和系统指标如运行时和能耗的实际估计。此外，这 paper 还提出了一种自动化的设计空间探索技术，通过自动化搜索架构和后端参数，优化backend和系统指标。</li>
<li>results: 实验表明，这 paper 的方法可以准确预测backend PPA 和系统指标，平均预测错误为7%或更低，并在商业12nm进程和研究 oriented 45nm进程中实现了两个深度学习加速器平台（VTA和VeriGOOD-ML）的ASIC实现。<details>
<summary>Abstract</summary>
Parameterizable machine learning (ML) accelerators are the product of recent breakthroughs in ML. To fully enable their design space exploration (DSE), we propose a physical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN ML algorithms. It adopts a unified approach that combines backend power, performance, and area (PPA) analysis with frontend performance simulation, thereby achieving a realistic estimation of both backend PPA and system metrics such as runtime and energy. In addition, our framework includes a fully automated DSE technique, which optimizes backend and system metrics through an automated search of architectural and backend parameters. Experimental studies show that our approach consistently predicts backend PPA and system metrics with an average 7% or less prediction error for the ASIC implementation of two deep learning accelerator platforms, VTA and VeriGOOD-ML, in both a commercial 12 nm process and a research-oriented 45 nm process.
</details>
<details>
<summary>摘要</summary>
现代机器学习（ML）加速器的 Parameterizable 机制是 ML 的最新突破。为了充分利用设计空间探索（DSE），我们提议一种物理设计驱动、学习基于预测框架，用于硬件加速深度神经网络（DNN）和非 DNN ML 算法。它采用一种统一的方法，结合后端能力、性能和面积（PPA）分析与前端性能仿真，从而实现真实的 backend PPA 和系统指标（如运行时间和能耗）的估计。此外，我们的框架还包括一种完全自动化 DSE 技术，通过自动搜索设计和后端参数，实现最佳化 backend 和系统指标。实验研究显示，我们的方法可预测 backend PPA 和系统指标的平均差异为 7% 或更小，对 ASIC 实现的两种深度学习加速器平台（VTA 和 VeriGOOD-ML）在商用 12 nm 进程和研究 oriented 45 nm 进程中进行了可靠的预测。
</details></li>
</ul>
<hr>
<h2 id="Less-is-More-–-Towards-parsimonious-multi-task-models-using-structured-sparsity"><a href="#Less-is-More-–-Towards-parsimonious-multi-task-models-using-structured-sparsity" class="headerlink" title="Less is More – Towards parsimonious multi-task models using structured sparsity"></a>Less is More – Towards parsimonious multi-task models using structured sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12114">http://arxiv.org/abs/2308.12114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richa Upadhyay, Ronald Phlypo, Rajkumar Saini, Marcus Liwicki</li>
<li>for: 本研究旨在在多任务学习（MTL）框架中 incorporate 结构化群 sparse 性，以开发 fewer 参数的模型，能够有效地解决多个任务，同时保持与权重 dense 模型的性能相似或更高。</li>
<li>methods: 我们使用 channel-wise L1&#x2F;L2 群 sparse 性在共享层中，通过减少模型的内存占用量、计算需求和预测时间来降低模型的资源占用。</li>
<li>results: 我们通过对单任务和多任务实验表明，在 group sparsity 下，模型的性能与 dense 模型相似或更高，同时可以减少模型的参数数量。 我们还发现，适当减少 sparse 度可以提高模型的性能和简洁度。<details>
<summary>Abstract</summary>
Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups. This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model. Sparsifying the model during training helps decrease the model's memory footprint, computation requirements, and prediction time during inference. We use channel-wise l1/l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN). This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks. We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ. We also investigate how changing the sparsification degree impacts both the performance of the model and the sparsity of groups.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:group sparsity in machine learning (ML) 推动更加简单、更易理解的模型，具有 fewer 活动参数组。本工作想要将结构化组简陋性 incorporated 到多任务学习 (MTL) 框架中，以开发更具有经济性的模型，可以更好地解决多个任务，而且具有更好的性能。在训练中减少模型的内存占用、计算需求和预测时间，可以提高模型的效率。我们在共享层中使用通道级 L1/L2 组简陋性，这种方法不仅可以消除无用的通道，还对权重进行 penalty，从而提高所有任务的学习。我们在 NYU-v2 和 CelebAMask-HQ 两个公开的 MTL 数据集上进行了单任务和多任务的实验，并 investigate 如何改变简陋化度对模型性能和组简陋性的影响。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Continual-Category-Discovery"><a href="#Generalized-Continual-Category-Discovery" class="headerlink" title="Generalized Continual Category Discovery"></a>Generalized Continual Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12112">http://arxiv.org/abs/2308.12112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Marczak, Grzegorz Rypeść, Sebastian Cygert, Tomasz Trzciński, Bartłomiej Twardowski</li>
<li>for: 这个论文是为了探讨一种新的常规学习（Continual Learning，CL）框架，它允许在任务之间学习新的分类和保持之前的知识。</li>
<li>methods: 该论文使用了一种基于常规分类发现（Generalized Category Discovery，GCD）的方法，允许在任务中存在新的分类和已知的分类，并使用了一种新的混合指导方法来减少忘记。</li>
<li>results: 实验表明，该方法可以在存在新分类的情况下积累知识，并且表现比较好，超过了一些强大的CL方法。<details>
<summary>Abstract</summary>
Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of novel classes are present. In light of these limitations, we propose a method that incorporates both supervised and unsupervised signals and mitigates the forgetting through the use of centroid adaptation. Our method surpasses strong CL methods adopted for GCD techniques and presents a superior representation learning performance.
</details>
<details>
<summary>摘要</summary>
大多数 continual learning (CL) 方法都是在Supervised learning  Setting 中进行学习，agent 需要学习新的标注任务，而不是忘记之前的知识。然而，这些设置并不是实际生活中的enario ，因为学习 Agent 可以访问大量的无标注数据，包括未知类和已知类的示例。 drawing inspiration from Generalized Category Discovery (GCD)，我们介绍了一个新的框架，允许在任务中存在未知和已知类，并且使用 continual 版本的无标注学习方法来发现它们。我们称这种设置为 Generalized Continual Category Discovery (GCCD)。它将 CL 和 GCD 融合起来， bridge  Synthetic  benchmarks 和实际生活中的enario。我们通过一系列实验表明，现有的方法在 Subsequent 任务中不能够从无标注样本中积累知识。在这些限制下，我们提出了一种方法，该方法将 supervised 和无标注信号相互作用，以避免忘记。我们的方法超越了Strong CL 方法，并在 representation learning 中表现出优于其他方法。
</details></li>
</ul>
<hr>
<h2 id="Constrained-Stein-Variational-Trajectory-Optimization"><a href="#Constrained-Stein-Variational-Trajectory-Optimization" class="headerlink" title="Constrained Stein Variational Trajectory Optimization"></a>Constrained Stein Variational Trajectory Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12110">http://arxiv.org/abs/2308.12110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Power, Dmitry Berenson</li>
<li>for: 这篇论文的目的是提出一种具有约束的轨迹优化算法，以便在多条轨迹上同时满足约束。</li>
<li>methods: 这篇论文使用stein可变数 gradient descent（SVGD）来找到一组粒子，这组粒子可以 aproximate一个低成本轨迹分布，并且遵循约束。</li>
<li>results: 实验结果显示，这篇论文的算法可以在具有高度约束的任务中，比基eline更好地避免损坏和初始化问题，并且在7DoF夹寸推进任务中取得了20&#x2F;20次成功，比基eline的13&#x2F;20次成功率高。<details>
<summary>Abstract</summary>
We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with arbitrary equality and inequality constraints and includes a novel particle resampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constrained tasks, such as a 7DoF wrench manipulation task, where CSVTO succeeds in 20/20 trials vs 13/20 for the closest baseline. Our results demonstrate that generating diverse constraint-satisfying trajectories improves robustness to disturbances and initialization over baselines.
</details>
<details>
<summary>摘要</summary>
我们介绍了受限制 Stein 变量梯度下降（CSVTO）算法，用于并行执行具有约束的轨迹优化。我们将受限制轨迹优化视为一种新的约束函数最小化问题，这种方法可以避免对约束进行处理，并使我们可以生成一个多样化的约束满足轨迹集。我们使用 Stein 变量梯度下降（SVGD）来找到一组粒子，这些粒子可以近似一个低成本轨迹分布，同时遵循约束。CSVTO 适用于具有平等和不平等约束的问题，并包括一种新的粒子重采样步骤，以避免局部最优解。通过生成多样化的约束满足轨迹集，CSVTO 能够更好地避免初始化和干扰的影响，并且更加稳定。我们在一个高度约束的 7DoF 工具抓取任务中，证明了CSVTO 比基eline更高效，CSVTO 在 20/20 次试验中成功，而基eline 只有 13/20 次成功。我们的结果表明，通过生成多样化的约束满足轨迹集，可以提高对干扰和初始化的 robustness。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient"><a href="#Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient" class="headerlink" title="Quantifying degeneracy in singular models via the learning coefficient"></a>Quantifying degeneracy in singular models via the learning coefficient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12108">http://arxiv.org/abs/2308.12108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edmundlth/scalable_learning_coefficient_with_sgld">https://github.com/edmundlth/scalable_learning_coefficient_with_sgld</a></li>
<li>paper_authors: Edmund Lau, Daniel Murfet, Susan Wei</li>
<li>for: 这篇论文的目的是解释深度神经网络（DNN）中的复杂的异常性。</li>
<li>methods: 这篇论文使用了 singular learning theory 中的一个量称为学习系数（learning coefficient），来量化 DNN 中的异常性。 它们还提出了一种可扩展的 Approximation 方法，使用游化 gradient Langevin dynamics，以便计算 localized 版本的学习系数。</li>
<li>results: 该论文的结果表明，local learning coefficient 可以准确地回归不同参数区域的异常性的排序。在 MNIST 实验中，local learning coefficient 能够揭示随机优化器对不同异常点的吸引力。<details>
<summary>Abstract</summary>
Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）是单一的统计模型，具有复杂的多重性。在这项工作中，我们展示了singular学习理论中的一个量——学习系数，可以准确地量化深度神经网络中的多重性。重要的是，我们将证明 dass多重性在DNN中不可以通过简单地计数"平坦"方向来解释。我们提出了一种可扩展的本地化学习系数的计算方法，使用随机梯度质子泊利动力学。为验证我们的方法，我们在低维模型中展示了其准确性，并且可以正确地推断多重性的顺序。在MNIST实验中，本地学习系数可以揭示随机优化器对更或少多重极点的 inductive bias。
</details></li>
</ul>
<hr>
<h2 id="Cached-Operator-Reordering-A-Unified-View-for-Fast-GNN-Training"><a href="#Cached-Operator-Reordering-A-Unified-View-for-Fast-GNN-Training" class="headerlink" title="Cached Operator Reordering: A Unified View for Fast GNN Training"></a>Cached Operator Reordering: A Unified View for Fast GNN Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12093">http://arxiv.org/abs/2308.12093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Bazinska, Andrei Ivanov, Tal Ben-Nun, Nikoli Dryden, Maciej Besta, Siyuan Shen, Torsten Hoefler</li>
<li>for: 本文旨在提高图像网络（Graph Neural Networks，GNNs）的性能优化，以满足大规模图像模型的训练。</li>
<li>methods: 本文使用了一种统一的视角，对图像网络计算、输入&#x2F;输出和内存进行分析。基于图像 convolutional network（GCN）和图像注意力（GAT）层的计算图的分析，提出了一些 alternating computation strategies。</li>
<li>results: 提出的优化策略可以达到GCN中的速度提高（最高达2.43倍）和GAT中的速度提高（最高达1.94倍），同时减少内存占用。这些优化可以在不同的硬件平台上实现，并且有助于减轻训练大规模GNN模型的性能瓶颈。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are a powerful tool for handling structured graph data and addressing tasks such as node classification, graph classification, and clustering. However, the sparse nature of GNN computation poses new challenges for performance optimization compared to traditional deep neural networks. We address these challenges by providing a unified view of GNN computation, I/O, and memory. By analyzing the computational graphs of the Graph Convolutional Network (GCN) and Graph Attention (GAT) layers -- two widely used GNN layers -- we propose alternative computation strategies. We present adaptive operator reordering with caching, which achieves a speedup of up to 2.43x for GCN compared to the current state-of-the-art. Furthermore, an exploration of different caching schemes for GAT yields a speedup of up to 1.94x. The proposed optimizations save memory, are easily implemented across various hardware platforms, and have the potential to alleviate performance bottlenecks in training large-scale GNN models.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 是一种有力的工具，用于处理结构化图数据，并解决节点分类、图分类和聚类等任务。然而，GNN 的稀疏性导致了性能优化的新挑战，与传统深度神经网络相比。我们通过提供一种统一的视图，对 GNN 计算、输入和存储进行分析。通过分析图 convolutional network (GCN) 和 graph attention (GAT) 两种广泛使用的 GNN 层的计算图，我们提出了 alternate computation strategies。我们的提案包括 adaptive operator reordering with caching，可以达到 GCN 比现状态 искус的最大速度提升率为 2.43倍。此外，对 GAT 的缓存 schemes 的探索，可以达到最大速度提升率为 1.94倍。我们的优化措施可以降低训练大规模 GNN 模型的内存占用量，易于在不同硬件平台上实现，并有 Potential to alleviate performance bottlenecks in training large-scale GNN models。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-RNN-Gradients-through-Pre-training"><a href="#Stabilizing-RNN-Gradients-through-Pre-training" class="headerlink" title="Stabilizing RNN Gradients through Pre-training"></a>Stabilizing RNN Gradients through Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12075">http://arxiv.org/abs/2308.12075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Herranz-Celotti, Jean Rouat</li>
<li>for: 本研究旨在稳定和改进深度学习模型的训练，以避免梯度变化的扩散式增长。</li>
<li>methods: 本研究使用了先验学习稳定性的理论，并扩展了知名的稳定性条件（LSC）至更广泛的深度循环神经网络。</li>
<li>results: 研究发现，在应用классиical Glorot、He和Orthogonal初始化方案时， feed-forward fully-connected神经网络和深度循环神经网络都可以满足LSC。此外，研究还发现了一种新的权重加权问题，并提出了一种新的方法来解决这个问题。<details>
<summary>Abstract</summary>
Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponential explosion that emerges from counting gradient paths in a rectangular grid in depth and time. We propose a new approach to mitigate this issue, that consists on giving a weight of a half to the time and depth contributions to the gradient, instead of the classical weight of one. Our empirical results confirm that pre-training both feed-forward and recurrent networks to fulfill the LSC often results in improved final performance across models. This study contributes to the field by providing a means to stabilize networks of any complexity. Our approach can be implemented as an additional step before pre-training on large augmented datasets, and as an alternative to finding stable initializations analytically.
</details>
<details>
<summary>摘要</summary>
多种学习理论建议防止梯度差值的几何增长，以稳定和改进训练。通常，这些分析是对 fully-connected neural network 或 single-layer recurrent neural network 进行的，这些网络的数学性让其更易分析。然而，本研究表明，在网络太复杂，无法分析的情况下，先通过网络的本地稳定来初始化网络，可以取得良好的效果。此外，我们扩展了已知的稳定性理论，以覆盖更广泛的深度循环网络家族，这些网络的参数和数据分布假设最少。我们称之为本地稳定条件（LSC）。我们的调查发现，经典的 Glorot、He 和orthogonal 初始化方案满足 LSC 当应用于 fully-connected neural network。然而，对深度循环网络进行分析，我们发现了一种新的加法性梯度增长的问题，这种问题来自于在深度和时间方向上的 counting 梯度路径。我们提出了一种新的方法来解决这个问题，即将时间和深度方向的贡献权重设为 0.5，而不是经典的 1。我们的实验结果表明，在 feed-forward 和 recurrent 网络中，通过满足 LSC 来初始化网络，经常会导致最终性能的改进。这项研究对深度学习领域的稳定性做出了贡献，我们的方法可以作为训练之前的额外步骤，以及analytically 找到稳定初始化的替代方案。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning"><a href="#Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning" class="headerlink" title="Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning"></a>Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12069">http://arxiv.org/abs/2308.12069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ni Dang, Tao Shi, Zengjie Zhang, Wanxin Jin, Marion Leibold, Martin Buss</li>
<li>for: 这篇论文旨在提出一种基于Maximum Entropy Inverse Reinforcement Learning（ME-IRL）方法来识别自动驾驶车辆（AV）的驾驶模式。</li>
<li>methods: 该方法使用了一系列加权特征来定义驾驶模式，并提出了一些新的反应式特征来捕捉AV对附近AV的反应。</li>
<li>results: 通过使用修改后的ME-IRL方法和新提出的特征，该研究成功地识别了从权重精算控制（SMPC）生成的示范轨迹中的驾驶模式。<details>
<summary>Abstract</summary>
The driving style of an Autonomous Vehicle (AV) refers to how it behaves and interacts with other AVs. In a multi-vehicle autonomous driving system, an AV capable of identifying the driving styles of its nearby AVs can reliably evaluate the risk of collisions and make more reasonable driving decisions. However, there has not been a consistent definition of driving styles for an AV in the literature, although it is considered that the driving style is encoded in the AV's trajectories and can be identified using Maximum Entropy Inverse Reinforcement Learning (ME-IRL) methods as a cost function. Nevertheless, an important indicator of the driving style, i.e., how an AV reacts to its nearby AVs, is not fully incorporated in the feature design of previous ME-IRL methods. In this paper, we describe the driving style as a cost function of a series of weighted features. We design additional novel features to capture the AV's reaction-aware characteristics. Then, we identify the driving styles from the demonstration trajectories generated by the Stochastic Model Predictive Control (SMPC) using a modified ME-IRL method with our newly proposed features. The proposed method is validated using MATLAB simulation and an off-the-shelf experiment.
</details>
<details>
<summary>摘要</summary>
自动驾驶车（AV）的驾驶方式指的是它在行驶过程中的行为和与其他AV的交互方式。在多辆自动驾驶车系统中，能够识别附近AV的驾驶方式的AV可以更可靠地评估碰撞风险并做出更合理的驾驶决策。然而，在文献中没有一致的定义自动驾驶车的驾驶方式，尽管人们认为驾驶方式在AV的轨迹中被编码，可以使用最大 entropy inverse reinforcement learning（ME-IRL）方法来作为成本函数来识别。然而，附近AV的反应不完全包含在前一代ME-IRL方法中的特征设计中。在这篇论文中，我们定义了自动驾驶车的驾驶方式为一系列权重的特征函数。然后，我们设计了新的反应意外特征，以更好地捕捉AV的反应特征。最后，我们使用修改后的ME-IRL方法和我们新提出的特征来识别示例轨迹，并从示例轨迹中提取驾驶方式。我们的方法在MATLAB simulate和一个简易实验中得到验证。
</details></li>
</ul>
<hr>
<h2 id="InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4"><a href="#InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4" class="headerlink" title="InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"></a>InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12067">http://arxiv.org/abs/2308.12067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun</li>
<li>for: 本研究旨在探讨大型语言模型如何通过限量高质量 instrucion-following 数据进行训练，以提高其对多模态任务的执行能力。</li>
<li>methods: 本研究使用了两个阶段的训练方法：首先在图像-文本对的集合上进行预训练，然后在超过200个例子的指令数据上进行精度训练。</li>
<li>results: 研究发现，使用少量但高质量的 instrucion-following 数据可以使大型语言模型实现更好的输出。InstructionGPT-4 在视觉问答、GPT-4 偏好等多种评价中表现出色，超过了原始 MiniGPT-4。<details>
<summary>Abstract</summary>
Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient to enable multimodal large language models to generate better output.
</details>
<details>
<summary>摘要</summary>
多模式大语言模型通过两个阶段训练过程获得 instrucion-following 能力：先于敏感图文对应和精度检测数据进行预训练，然后在精度检测数据上进行微调。当前研究表明，大语言模型可以通过有限量高质量 instrucion-following 数据实现满意的结果。在本文中，我们介绍 InstructionGPT-4，它通过一个小数据集（约6%的对Alignment dataset）进行微调，并使用我们提出的多个评价指标来自动选择和筛选低质量视听数据。通过这种方法，InstructionGPT-4 在视觉问答和 GPT-4 偏好等评价中表现出色，超过原始 MiniGPT-4。总的来说，我们的发现表明，更少但高质量的 instrucion-following 准则可以使得多模式大语言模型生成更好的输出。
</details></li>
</ul>
<hr>
<h2 id="Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference"><a href="#Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference" class="headerlink" title="Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference"></a>Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12066">http://arxiv.org/abs/2308.12066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ranggi Hwang, Jianyu Wei, Shijie Cao, Changho Hwang, Xiaohu Tang, Ting Cao, Mao Yang, Minsoo Rhu</li>
<li>for: 提高大型语言模型（LLM）的性能和可扩展性。</li>
<li>methods: 使用 Mixture-of-Experts（MoE）架构，并提出了一种新的预先阻塞函数来缓解 sparse expert 的动态活动问题，从而实现高性能和低内存占用。</li>
<li>results: 对比 conventional MoE 架构，提出的 Pre-gated MoE 系统能够提高性能、降低 GPU 内存占用，同时保持模型质量不变。这些特点使得 Pre-gated MoE 系统可以低成本地部署大规模 LLM，只需要一个 GPU 实现高性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs our novel pre-gating function which alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE is able to improve performance, reduce GPU memory consumption, while also maintaining the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.
</details>
<details>
<summary>摘要</summary>
Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs a novel pre-gating function that alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE can improve performance, reduce GPU memory consumption, and maintain the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.In simplified Chinese, the text would be:大型语言模型（LLMs）基于转换器的进步在最近几年很大，成功的原因是通过增加模型的大小来增加性能。然而，LLMs的高度计算和内存需求对应到前所未有的挑战。为了解决这些挑战，混合专家（MoE）架构被引入，它可以无需与计算需求成比例增加其模型大小来增加性能。然而，MoE的高内存需求和动态专家活动限制了它的实际应用。先前的解决方案，即将MoE的内存吃力强大的专家参数异步到CPU内存，不足以因为迁移到GPU的延迟会导致高性能开销。我们提出的预级MoE系统可以有效地解决计算和内存挑战，使用我们的算法-系统合作设计。预级MoE使用我们的新预级函数，解决了动态专家活动的问题，使我们的提议的系统可以面临大 Memory Footprint 的 MoE 问题，同时实现高性能。我们示出了预级MoE可以提高性能，减少GPU内存占用量，同时保持模型质量不变。这些特点使得我们的预级MoE系统可以效率地使用单个GPU进行大规模 LLMS 的部署，并且可以高性能。
</details></li>
</ul>
<hr>
<h2 id="Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers"><a href="#Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers" class="headerlink" title="Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers"></a>Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12065">http://arxiv.org/abs/2308.12065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Zoppi, Andrea Ceccarelli, Andrea Bondavalli</li>
<li>for: 提高机器学习模型的安全性，避免因误分类而导致的 kritical failures。</li>
<li>methods: 使用 ensemble of uncertainty measures 检测误分类，并在检测到误分类时阻止输出进行系统级别的处理。</li>
<li>results: SPROUT 能够准确地检测机器学习模型的误分类，并在检测到误分类时阻止输出进行系统级别的处理，从而提高机器学习模型的安全性。<details>
<summary>Abstract</summary>
Machine Learning (ML) algorithms that perform classification may predict the wrong class, experiencing misclassifications. It is well-known that misclassifications may have cascading effects on the encompassing system, possibly resulting in critical failures. This paper proposes SPROUT, a Safety wraPper thROugh ensembles of UncertainTy measures, which suspects misclassifications by computing uncertainty measures on the inputs and outputs of a black-box classifier. If a misclassification is detected, SPROUT blocks the propagation of the output of the classifier to the encompassing system. The resulting impact on safety is that SPROUT transforms erratic outputs (misclassifications) into data omission failures, which can be easily managed at the system level. SPROUT has a broad range of applications as it fits binary and multi-class classification, comprising image and tabular datasets. We experimentally show that SPROUT always identifies a huge fraction of the misclassifications of supervised classifiers, and it is able to detect all misclassifications in specific cases. SPROUT implementation contains pre-trained wrappers, it is publicly available and ready to be deployed with minimal effort.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）算法可能会预测错误的类别，导致错分。这是已知的事实，错分可能会对包含系统产生卷积效应，可能导致重要的失败。这篇论文提议了“SPROUT”，一个安全包装 Through ensembles of UncertainTy measures，它猜测错分的方式是计算输入和输出uncertainty measures的黑色框分类器。如果检测到了错分，SPROUT会阻止输出分类器的输出对包含系统的传递。这将导致安全性的改善，因为SPROUT将异常输出（错分）转化为数据损失失败，这可以轻松地在系统层面进行管理。SPROUT适用于二分和多分类фикация，包括图像和表格数据集。我们实验表明，SPROUT总能够检测出超级vised分类器中的大部分错分，而且在特定情况下，它能够检测所有错分。SPROUT实现包括预训练的包装，现在公开可用，只需要最小的努力就可以部署。
</details></li>
</ul>
<hr>
<h2 id="HarvestNet-A-Dataset-for-Detecting-Smallholder-Farming-Activity-Using-Harvest-Piles-and-Remote-Sensing"><a href="#HarvestNet-A-Dataset-for-Detecting-Smallholder-Farming-Activity-Using-Harvest-Piles-and-Remote-Sensing" class="headerlink" title="HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing"></a>HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12061">http://arxiv.org/abs/2308.12061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Xu, Amna Elmustafa, Liya Weldegebriel, Emnet Negash, Richard Lee, Chenlin Meng, Stefano Ermon, David Lobell</li>
<li>For: The paper aims to improve the accuracy of cropland mapping in smallholder farming regions, specifically in sub-Saharan Africa.* Methods: The authors use satellite images and expert knowledge to collect a dataset called HarvestNet, which includes 7,000 hand-labeled images and 2,000 ground-collected labels. They also benchmark several state-of-the-art models in remote sensing and compare their performance with a pre-existing coverage map.* Results: The authors achieve an accuracy of around 80% on hand-labeled data and 90%, 98% accuracy on ground truth data for Tigray and Amhara, respectively. They also detect an additional 56,621 hectares of cropland in Tigray using their method, which is not captured by the pre-existing coverage map.<details>
<summary>Abstract</summary>
Small farms contribute to a large share of the productive land in developing countries. In regions such as sub-Saharan Africa, where 80% of farms are small (under 2 ha in size), the task of mapping smallholder cropland is an important part of tracking sustainability measures such as crop productivity. However, the visually diverse and nuanced appearance of small farms has limited the effectiveness of traditional approaches to cropland mapping. Here we introduce a new approach based on the detection of harvest piles characteristic of many smallholder systems throughout the world. We present HarvestNet, a dataset for mapping the presence of farms in the Ethiopian regions of Tigray and Amhara during 2020-2023, collected using expert knowledge and satellite images, totaling 7k hand-labeled images and 2k ground collected labels. We also benchmark a set of baselines including SOTA models in remote sensing with our best models having around 80% classification performance on hand labelled data and 90%, 98% accuracy on ground truth data for Tigray, Amhara respectively. We also perform a visual comparison with a widely used pre-existing coverage map and show that our model detects an extra 56,621 hectares of cropland in Tigray. We conclude that remote sensing of harvest piles can contribute to more timely and accurate cropland assessments in food insecure region.
</details>
<details>
<summary>摘要</summary>
小规模农场在发展国家占较大的生产地面积。如在 SUB-SAHARAN AFRICA 地区，80% 的农场面积在 2 ha 以下， mapping 小holder 耕地是跟踪可持续发展的标准部署之一。然而，传统方法对小holder 耕地的映射受到 visually 多样和细节的限制。我们介绍了一种新的方法，基于耕地收割堆的检测，这种特征是许多小holder 系统中的共同特征。我们提供了 HarvestNet 数据集，用于在埃塞俄比亚地区的 Tigray 和 Amhara 地区在 2020-2023 年间的耕地映射。我们收集了 7000 个专家知识和卫星图像，以及 2000 个地面收集的标签。我们还对一些先进的远程感知模型进行了比较，我们的最佳模型在手动标注数据上达到了 80% 的分类性能，并在真实数据上达到了 90%、98% 的准确率。我们还进行了一个视觉比较，发现我们的模型可以检测到传统覆盖地图中缺失的 56,621 公顷耕地。我们结论认为，远程感知耕地收割堆可以为食 insecurities 地区提供更加准确和及时的耕地评估。
</details></li>
</ul>
<hr>
<h2 id="Manipulating-Embeddings-of-Stable-Diffusion-Prompts"><a href="#Manipulating-Embeddings-of-Stable-Diffusion-Prompts" class="headerlink" title="Manipulating Embeddings of Stable Diffusion Prompts"></a>Manipulating Embeddings of Stable Diffusion Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12059">http://arxiv.org/abs/2308.12059</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/webis-de/arxiv23-prompt-embedding-manipulation">https://github.com/webis-de/arxiv23-prompt-embedding-manipulation</a></li>
<li>paper_authors: Niklas Deckers, Julia Peters, Martin Potthast</li>
<li>for: 这个研究旨在提高文本描述到图像的转换精度，通过直接修改提示语 embedding，从而实现更细化和具有目标性的控制。</li>
<li>methods: 我们提出了一种将生成图像模型看作连续函数，通过图像空间和提示语 embedding 空间之间传递梯度的方法，以实现更精细和具有目标性的控制。</li>
<li>results: 我们的实验表明，这种方法可以实现更高精度的图像转换，并且可以帮助用户更好地完成创意任务。<details>
<summary>Abstract</summary>
Generative text-to-image models such as Stable Diffusion allow users to generate images based on a textual description, the prompt. Changing the prompt is still the primary means for the user to change a generated image as desired. However, changing the image by reformulating the prompt remains a difficult process of trial and error, which has led to the emergence of prompt engineering as a new field of research. We propose and analyze methods to change the embedding of a prompt directly instead of the prompt text. It allows for more fine-grained and targeted control that takes into account user intentions. Our approach treats the generative text-to-image model as a continuous function and passes gradients between the image space and the prompt embedding space. By addressing different user interaction problems, we can apply this idea in three scenarios: (1) Optimization of a metric defined in image space that could measure, for example, image style. (2) Assistance of users in creative tasks by enabling them to navigate the image space along a selection of directions of "near" prompt embeddings. (3) Changing the embedding of the prompt to include information that the user has seen in a particular seed but finds difficult to describe in the prompt. Our experiments demonstrate the feasibility of the described methods.
</details>
<details>
<summary>摘要</summary>
“文本描述”を基于的生成图像模型，如稳定扩散，允许用户根据文本描述生成图像。但是，通过修改描述仍然是用户改变生成图像的主要方式。然而，通过修改描述来改变图像是一项困难的试验和错误过程，这导致了“提示工程”作为一种新的研究领域的出现。我们提出并分析了通过直接修改提示的embedding来改变图像的方法。这种方法允许更细化和targeted控制，考虑用户的意图。我们将生成文本到图像模型看作是连续函数，并在图像空间和提示 embedding 空间之间传递梯度。通过解决不同的用户互动问题，我们可以在以下三个场景中应用这个想法：1. 图像空间中定义的一个度量的优化，例如图像风格。2. 用户在创意任务中的帮助，通过让用户在“near”提示 embeddings 上导航图像空间来实现。3. 将提示 embedding 包含用户在种子中看到的信息，但是difficult to describe in the prompt。我们的实验表明这种方法的可行性。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-of-Robust-Learning-against-Evasion-Attacks"><a href="#Sample-Complexity-of-Robust-Learning-against-Evasion-Attacks" class="headerlink" title="Sample Complexity of Robust Learning against Evasion Attacks"></a>Sample Complexity of Robust Learning against Evasion Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12054">http://arxiv.org/abs/2308.12054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascale Gourdeau</li>
<li>for: 本研究的目的是理解机器学习模型对攻击的抵触性。</li>
<li>methods: 本文使用了学习理论的角度，研究了在攻击下学习的可行性，并考虑了样本复杂度。</li>
<li>results: 本文显示了对于随机样本only的情况，需要 distributional assumptions 来保证机器学习模型的抵触性。此外，如果攻击者只能对输入数据进行 $O(\log n)$ 位的偏移，那么可以robustly学习 conjunctions 和 decision lists w.r.t. log-Lipschitz distributions。在learner具有更多权限的情况下，本文研究了local membership queries 和 local equivalence query oracle，并提供了可行性和Upper bound的结论。<details>
<summary>Abstract</summary>
It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. One of the fundamental problems in adversarial machine learning is to quantify how much training data is needed in the presence of evasion attacks, where data is corrupted at test time. In this thesis, we work with the exact-in-the-ball notion of robustness and study the feasibility of adversarially robust learning from the perspective of learning theory, considering sample complexity.   We first explore the setting where the learner has access to random examples only, and show that distributional assumptions are essential. We then focus on learning problems with distributions on the input data that satisfy a Lipschitz condition and show that robustly learning monotone conjunctions has sample complexity at least exponential in the adversary's budget (the maximum number of bits it can perturb on each input). However, if the adversary is restricted to perturbing $O(\log n)$ bits, then one can robustly learn conjunctions and decision lists w.r.t. log-Lipschitz distributions.   We then study learning models where the learner is given more power. We first consider local membership queries, where the learner can query the label of points near the training sample. We show that, under the uniform distribution, the exponential dependence on the adversary's budget to robustly learn conjunctions remains inevitable. We then introduce a local equivalence query oracle, which returns whether the hypothesis and target concept agree in a given region around a point in the training sample, and a counterexample if it exists. We show that if the query radius is equal to the adversary's budget, we can develop robust empirical risk minimization algorithms in the distribution-free setting. We give general query complexity upper and lower bounds, as well as for concrete concept classes.
</details>
<details>
<summary>摘要</summary>
“机器学习模型对于攻击性测试的漏洞日益增加的重要性。一个基本问题在于在恶意攻击下对于训练数据的需求量。在这些thesis中，我们运用精确在球体中的不可变量来测量机器学习模型的稳定性，并从学习理论的角度来研究抗攻击学习的可行性。我们首先考虑learner仅有随机样本的存在，并证明了需要分布方程的假设。接着，我们对受到点对点的分布的学习问题进行研究，并证明了对于log-Lipschitz分布，可以在 exponential 时间内对于攻击者的预算进行抗攻击学习。然后，我们研究learner具有更多权力的情况。我们首先考虑了本地会员询问，learner可以询问训练样本附近的标签。我们证明了，在 uniform 分布下，抗攻击学习 conjugation 和决策列在 exponential 时间内是不可避免的。然后，我们引入了本地相似询问 oracle，它可以返回训练样本附近的标签，以及在这个区域附近没有对应的 counterexample。我们证明了，如果询问半径等于攻击者的预算，则可以在分布自由设定下开发抗攻击 empirical risk minimization 算法。我们还给出了一般询问量上限和下限，以及具体的概念类别。”
</details></li>
</ul>
<hr>
<h2 id="Layer-wise-Feedback-Propagation"><a href="#Layer-wise-Feedback-Propagation" class="headerlink" title="Layer-wise Feedback Propagation"></a>Layer-wise Feedback Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12053">http://arxiv.org/abs/2308.12053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leander Weber, Jim Berend, Alexander Binder, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</li>
<li>for: 这paper是为了提出一种基于解释的神经网络训练方法，即层WISEFeedbackPropagation（LFP），用于评估神经网络模型中各个连接的贡献，并将其作为权重赋予各个连接。</li>
<li>methods: 这paper使用了层WISERelevancePropagation（LRP）来计算每个连接的权重，然后将这些权重分配给各个连接。这种方法不需要计算梯度，可以在模型的训练过程中分配权重。</li>
<li>results: 这paper提出了LFP的理论和实验研究，并证明了LFP可以在不同的模型和数据集上实现相同或更好的性能，而且可以超越传统的梯度下降法。此外，paper还 investigate了不同LRP规则的扩展和应用，如训练无意义梯度的神经网络模型，或者为转移学习而高效地利用现有知识。<details>
<summary>Abstract</summary>
In this paper, we present Layer-wise Feedback Propagation (LFP), a novel training approach for neural-network-like predictors that utilizes explainability, specifically Layer-wise Relevance Propagation(LRP), to assign rewards to individual connections based on their respective contributions to solving a given task. This differs from traditional gradient descent, which updates parameters towards anestimated loss minimum. LFP distributes a reward signal throughout the model without the need for gradient computations. It then strengthens structures that receive positive feedback while reducingthe influence of structures that receive negative feedback. We establish the convergence of LFP theoretically and empirically, and demonstrate its effectiveness in achieving comparable performance to gradient descent on various models and datasets. Notably, LFP overcomes certain limitations associated with gradient-based methods, such as reliance on meaningful derivatives. We further investigate how the different LRP-rules can be extended to LFP, what their effects are on training, as well as potential applications, such as training models with no meaningful derivatives, e.g., step-function activated Spiking Neural Networks (SNNs), or for transfer learning, to efficiently utilize existing knowledge.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出层wise Feedback Propagation（LFP），一种基于解释性的训练方法，使用层wise Relevance Propagation（LRP）来为各个连接分配奖励，以便解决给定任务。这与传统的梯度下降不同，梯度下降更新参数向估计损失最小点。LFP在模型中分配奖励信号，不需要梯度计算。它会强化接收正面奖励的结构，同时减弱接收负面奖励的结构。我们证明LFP的定理和实验均可以达到预期的性能，并在不同的模型和数据集上证明其效果。另外，LFP可以超越一些相关的梯度基于方法的限制，例如依赖于有意义的导数。我们还 investigate了不同的LRP规则如何扩展到LFP，以及它们在训练中的效果和应用，例如训练无意义导数的模型，如步函数激活的神经网络（SNN），或者用于过渡学习，以高效地利用现有的知识。
</details></li>
</ul>
<hr>
<h2 id="A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks"><a href="#A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks" class="headerlink" title="A multiobjective continuation method to compute the regularization path of deep neural networks"></a>A multiobjective continuation method to compute the regularization path of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12044">http://arxiv.org/abs/2308.12044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aamakor/continuation-method">https://github.com/aamakor/continuation-method</a></li>
<li>paper_authors: Augustina C. Amakor, Konstantin Sonntag, Sebastian Peitz</li>
<li>for: 提高深度神经网络（DNNs）的数值效率、模型解释性和Robustness。</li>
<li>methods: 基于线性模型的机器学习方法，扩展了规则化路径的概念到DNNs，并通过处理经验损失和稀疏度（$\ell^1$ norm）为两个矛盾目标解决multiobjective optimization问题。</li>
<li>results: 提出了一种高效地近似Pareto前面的算法，并通过deterministic和随机梯度示例 validate了该算法的效果。此外，还证明了知道规则化路径可以为网络参数化提供好的泛化能力。<details>
<summary>Abstract</summary>
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto front for the above-mentioned objectives in a very efficient manner. We present numerical examples using both deterministic and stochastic gradients. We furthermore demonstrate that knowledge of the regularization path allows for a well-generalizing network parametrization.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）中的稀疏性是一个非常需要的特性，因为它保证了数学效率、提高模型解释性（由于更少的相关特征），并且提高了模型的稳定性。在基于线性模型的机器学习方法中，已经知道存在一个连接路径 между最稀疏的解决方案（按照$\ell^1$范数）和不Regularized解决方案，这个路径被称为Regularization路径。然而，在DNN中扩展Regularization路径的概念是非常困难，因为Empirical损失和稀疏性（$\ell^1$范数）是两个矛盾的目标。为了解决这个问题，我们提出了一种算法，可以高效地approximate整个Pareto前列。我们通过Deterministic和Stochastic梯度进行数值示例，并证明了知道Regularization路径可以获得一个很好地泛化网络参数化。
</details></li>
</ul>
<hr>
<h2 id="IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning"><a href="#IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning" class="headerlink" title="IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning"></a>IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12043">http://arxiv.org/abs/2308.12043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feiyuzhang98/increlora">https://github.com/feiyuzhang98/increlora</a></li>
<li>paper_authors: Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, Yiming Qian</li>
<li>for: 提高预训练语言模型（PLM）的 parameter efficiency，尤其是在多个下游任务时。</li>
<li>methods: 使用 Low-Rank Adaptation（LoRA）方法，并在每个目标模块中添加可学习的约数分解矩阵。</li>
<li>results: 在 GLUE 上进行了广泛的实验，显示我们的方法在低资源设置下表现更优，尤其是在 parameter efficiency 方面。<details>
<summary>Abstract</summary>
With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it is not limited by the initial number of training parameters, and each parameter matrix has a higher rank upper bound for the same training overhead. We conduct extensive experiments on GLUE to demonstrate the effectiveness of IncreLoRA. The results show that our method owns higher parameter efficiency, especially when under the low-resource settings where our method significantly outperforms the baselines. Our code is publicly available.
</details>
<details>
<summary>摘要</summary>
随着大型预训言语模型（PLM）的增加，精细调整模型中的所有参数不是高效的，特别是当有多个下游任务时，带来了训练和存储成本的增加。许多参数高效调整（PEFT）方法已经被提出，其中，低级别适应（LoRA）是一种代表性的方法，将适应矩阵注入到每个目标模块中。然而，LoRA忽略了模块中参数的重要性。为解决这个问题，许多工作已经被提出来剪裁LoRA中的参数。然而，在有限的训练条件下，剪裁后参数矩阵的最大级别仍然受到先前设置的值的限制。我们因此提出了IncreLoRA，一种逐步分配参数的方法，在训练过程中根据模块的重要性分配参数。这种方法与剪裁方法不同，不受初始训练参数的限制，每个参数矩阵的最大级别Upper bound也比剪裁方法高。我们在GLUE上进行了广泛的实验， demonstarted the effectiveness of IncreLoRA。结果显示，我们的方法在参数效率方面高于baseline，特别是在资源受限的情况下，我们的方法显著超过baseline。我们的代码公开可用。
</details></li>
</ul>
<hr>
<h2 id="CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures"><a href="#CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures" class="headerlink" title="CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures"></a>CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12031">http://arxiv.org/abs/2308.12031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Gherardini, Varun Ravi Varma, Karol Capala, Roger Woods, Jose Sousa</li>
<li>For: The paper is written for improving secure analytics using explainable artificial intelligence, specifically addressing the challenges of developing solutions with small data sets.* Methods: The paper presents a new tool called CACTUS, which employs explainable AI techniques to provide additional support for categorical attributes, optimize memory usage, and speed up computation through parallelization.* Results: The paper evaluates the performance of CACTUS on two data sets, Wisconsin diagnostic breast cancer and Thyroid0387, and shows that it can effectively rank attributes by their discriminative power and provide accurate classification results.Here’s the same information in Simplified Chinese text:* 为：本文是为了提高安全分析，使用可解释人工智能，特别是面临小数据集的挑战。* 方法：本文提出了一种新的工具called CACTUS，该工具使用可解释AI技术，为分类特征提供更多的支持，保持分类特征的原始含义，提高内存使用率，并通过并行化加速计算。* 结果：本文使用CACTUS工具对 Wisconcin诊断乳腺癌和 Thyroid0387 数据集进行评估，并显示了它可以准确地排序特征，提供高效的分类结果。<details>
<summary>Abstract</summary>
The availability of large data sets is providing an impetus for driving current artificial intelligent developments. There are, however, challenges for developing solutions with small data sets due to practical and cost-effective deployment and the opacity of deep learning models. The Comprehensive Abstraction and Classification Tool for Uncovering Structures called CACTUS is presented for improved secure analytics by effectively employing explainable artificial intelligence. It provides additional support for categorical attributes, preserving their original meaning, optimising memory usage, and speeding up the computation through parallelisation. It shows to the user the frequency of the attributes in each class and ranks them by their discriminative power. Its performance is assessed by application to the Wisconsin diagnostic breast cancer and Thyroid0387 data sets.
</details>
<details>
<summary>摘要</summary>
大量数据的可用性对当前人工智能发展提供了动力。然而，对小数据集的解决方案存在实际和成本效益的挑战，主要是深度学习模型的透明度问题。本文提出了一种名为CACTUS的全面抽象和分类工具，用于提高安全分析。它可以有效地使用可解释人工智能，并且对分类属性进行了更好的支持，保持原始含义，优化内存使用和并行计算，以提高计算速度。它还可以为用户显示每个类别的属性频率，并将其排序为权重。本文通过应用于美国威斯康星诊断乳腺癌和 thyroid0387 数据集来评估其性能。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning"><a href="#Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning" class="headerlink" title="Prompt-Based Length Controlled Generation with Reinforcement Learning"></a>Prompt-Based Length Controlled Generation with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12030">http://arxiv.org/abs/2308.12030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, Qun Liu</li>
<li>for: 这个论文旨在提出一种基于提示的Length Control方法，以实现控制长度的GPT-style语言模型生成。</li>
<li>methods: 该方法使用了 reward学习，通过给出trainable或规则型的奖励模型，对GPT-style语言模型的生成进行影响，以实现目标长度的控制。</li>
<li>results: 实验显示，该方法可以有效地提高CNNDM和NYT等 популяр的数据集上的提示基于长度控制精度。我们认为这种可控长度的能力可以为LLMs的未来带来更多的潜力。<details>
<summary>Abstract</summary>
Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. We believe this length-controllable ability can provide more potentials towards the era of LLMs.
</details>
<details>
<summary>摘要</summary>
近期，大型语言模型（LLM）如ChatGPT和GPT-4吸引了很大的注意，因其奇妙的进步和表现。控制LLM的Length emerges as an important topic，这也使得用户可以充分利用LLM的能力在更多的实际应用 scenario 中，如生成适当的答案或论文的 desired length。此外，LLM中的autoregressive generation extremely time-consuming，而控制这些生成的Length可以Randomly reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. We believe this length-controllable ability can provide more potentials towards the era of LLMs.
</details></li>
</ul>
<hr>
<h2 id="A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning"><a href="#A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning" class="headerlink" title="A Scale-Invariant Task Balancing Approach for Multi-Task Learning"></a>A Scale-Invariant Task Balancing Approach for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12029">http://arxiv.org/abs/2308.12029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baijiong Lin, Weisen Jiang, Feiyang Ye, Yu Zhang, Pengguang Chen, Ying-Cong Chen, Shu Liu</li>
<li>for: This paper is written for people who are interested in multi-task learning (MTL) and want to learn about a new method called Scale-Invariant Multi-Task Learning (SI-MTL) that can alleviate the task-balancing problem.</li>
<li>methods: The paper proposes two methods to address the task-balancing problem in MTL: a logarithm transformation on all task losses to ensure scale-invariance at the loss level, and a gradient balancing method called SI-G that normalizes all task gradients to the same magnitude as the maximum gradient norm.</li>
<li>results: The paper reports extensive experimental results on several benchmark datasets, which consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.<details>
<summary>Abstract</summary>
Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL），一种同时学习多个相关任务的学习方法，在不同领域都取得了很大成功。然而，任务均衡仍然是MTL中的主要挑战，由于任务损失/梯度的比例不同，经常会导致性能下降。在这篇论文中，我们提出了具有权重归一化和梯度归一化的缩减多任务学习方法（SI-MTL），以解决任务均衡问题从损失和梯度两个角度。具体来说，SI-MTL包括一种对所有任务损失进行对数变换，以保证损失水平上的归一化，以及一种梯度归一化方法SI-G，用于 норма化所有任务梯度，使其具有最大梯度 нор 的同样大小。我们在多个标准数据集上进行了广泛的实验，并 consistently demonstrates了SI-G的有效性和SI-MTL的状态的杰出性。
</details></li>
</ul>
<hr>
<h2 id="Bias-Aware-Minimisation-Understanding-and-Mitigating-Estimator-Bias-in-Private-SGD"><a href="#Bias-Aware-Minimisation-Understanding-and-Mitigating-Estimator-Bias-in-Private-SGD" class="headerlink" title="Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD"></a>Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12018">http://arxiv.org/abs/2308.12018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Knolle, Robert Dorfman, Alexander Ziller, Daniel Rueckert, Georgios Kaissis</li>
<li>for: 提高 differentially private SGD 的模型 Utility 和 Privacy 的融合</li>
<li>methods: 利用 per-sample gradient norms 和 private gradient oracle 的关系来减少 private gradient estimator bias</li>
<li>results: 在 CIFAR-10, CIFAR-100, 和 ImageNet-32  datasets 上提供了 empirical evidence, 显示 Bias-Aware Minimisation 不仅减少了 bias, 还有substantially improved privacy-utility trade-offs.<details>
<summary>Abstract</summary>
Differentially private SGD (DP-SGD) holds the promise of enabling the safe and responsible application of machine learning to sensitive datasets. However, DP-SGD only provides a biased, noisy estimate of a mini-batch gradient. This renders optimisation steps less effective and limits model utility as a result. With this work, we show a connection between per-sample gradient norms and the estimation bias of the private gradient oracle used in DP-SGD. Here, we propose Bias-Aware Minimisation (BAM) that allows for the provable reduction of private gradient estimator bias. We show how to efficiently compute quantities needed for BAM to scale to large neural networks and highlight similarities to closely related methods such as Sharpness-Aware Minimisation. Finally, we provide empirical evidence that BAM not only reduces bias but also substantially improves privacy-utility trade-offs on the CIFAR-10, CIFAR-100, and ImageNet-32 datasets.
</details>
<details>
<summary>摘要</summary>
differentially private SGD (DP-SGD) 可以使机器学习应用于敏感数据集而不需担心隐私泄露。然而，DP-SGD只提供偏差、噪音的小批量梯度估计。这会导致优化步骤效果减退，模型实用性受限。在这项工作中，我们显示了每个样本梯度norm和私有梯度 Oracle 的估计偏差之间的连接。我们提出了偏差意识的最小化（BAM），允许降低私有梯度估计偏差。我们证明了如何有效地计算BAM所需的量，并将其扩展到大型神经网络。最后，我们提供了实验证明BAM不仅减少偏差，还substantially改善了隐私-实用性质量比在CIFAR-10、CIFAR-100和ImageNet-32数据集上。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Stochastic-Differential-Equations"><a href="#Graph-Neural-Stochastic-Differential-Equations" class="headerlink" title="Graph Neural Stochastic Differential Equations"></a>Graph Neural Stochastic Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12316">http://arxiv.org/abs/2308.12316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard Bergna, Felix Opolka, Pietro Liò, Jose Miguel Hernandez-Lobato</li>
<li>for: 这个论文旨在提出一种新的模型Graph Neural Stochastic Differential Equations (Graph Neural SDEs)，用于评估预测不确定性。</li>
<li>methods: 该模型基于Brownian Motion嵌入随机性，从而提高了现有模型缺失的预测不确定性评估。</li>
<li>results: 经验研究表明，Latent Graph Neural SDEs可以超过常见模型 like Graph Convolutional Networks和Graph Neural ODEs，特别是在信任预测中，能够更好地处理out-of-distribution检测。<details>
<summary>Abstract</summary>
We present a novel model Graph Neural Stochastic Differential Equations (Graph Neural SDEs). This technique enhances the Graph Neural Ordinary Differential Equations (Graph Neural ODEs) by embedding randomness into data representation using Brownian motion. This inclusion allows for the assessment of prediction uncertainty, a crucial aspect frequently missed in current models. In our framework, we spotlight the \textit{Latent Graph Neural SDE} variant, demonstrating its effectiveness. Through empirical studies, we find that Latent Graph Neural SDEs surpass conventional models like Graph Convolutional Networks and Graph Neural ODEs, especially in confidence prediction, making them superior in handling out-of-distribution detection across both static and spatio-temporal contexts.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的模型——图 neural 随机 дифференциаль方程（图 neural SDE）。这种技术在图 neural ordinary differential equation（图 neural ODE）中嵌入随机性，通过游戏摆动来表示数据的不确定性。这种包含允许我们评估预测不确定性，这是当前模型中常常缺失的一个重要方面。在我们的框架中，我们强调了《隐藏图 neural SDE》的变体，并证明其效果。通过实验研究，我们发现隐藏图 neural SDE 在信任预测方面表现出色，特别是在对于静态和空间时间上的 OUT-OF-DISTRIBUTION 检测中，与传统模型如图 convolutional networks 和图 neural ODEs 相比，它们更为稳定和可靠。
</details></li>
</ul>
<hr>
<h2 id="MKL-L-0-1-SVM"><a href="#MKL-L-0-1-SVM" class="headerlink" title="MKL-$L_{0&#x2F;1}$-SVM"></a>MKL-$L_{0&#x2F;1}$-SVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12016">http://arxiv.org/abs/2308.12016</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maxis1718/simplemkl">https://github.com/maxis1718/simplemkl</a></li>
<li>paper_authors: Bin Zhu, Yijie Shi</li>
<li>for: 这篇论文提出了一个基于多kernel学习（MKL）的支持向量机（SVM）模型，使用$(0, 1)$损失函数。</li>
<li>methods: 论文提供了一些首选条件，然后利用这些条件来开发一个快速的ADMM解决方案来处理非 convex 和非 гладhloss 优化问题。</li>
<li>results: 数据库实验表明，我们的MKL-$L_{0&#x2F;1}$-SVM表现与 SimpleMKL 比较相似，SimpleMKL 是 Rakotomamonjy 等人在 Journal of Machine Learning Research 上发表的一篇论文 [vol. 9, pp. 2491-2521, 2008] 。<details>
<summary>Abstract</summary>
This paper presents a Multiple Kernel Learning (abbreviated as MKL) framework for the Support Vector Machine (SVM) with the $(0, 1)$ loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver to deal with the nonconvex and nonsmooth optimization problem. Extensive numerical experiments on synthetic and real datasets show that the performance of our MKL-$L_{0/1}$-SVM is comparable with the one of the leading approaches called SimpleMKL developed by Rakotomamonjy, Bach, Canu, and Grandvalet [Journal of Machine Learning Research, vol. 9, pp. 2491-2521, 2008].
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种基于多kernel学习（简称MKL）的支持向量机器（SVM）$(0, 1)$损失函数的框架。文中给出了一些首要的优化条件，然后利用这些条件来开发一个快速的ADMM算法来解决非对称和不连续的优化问题。实验表明，我们的MKL-$L_{0/1}$-SVM的性能与2008年 Rakotomamonjy等在Journal of Machine Learning Research上发表的SimpleMKL方法相当。Here's the breakdown of the translation:* 这篇论文 (zhè běn zhōng zhì) - This paper* 提出了 (tī qù le) - proposes* 一种 (yī zhǒng) - a kind of* 基于多kernel学习 (jī yù duō jiān xué xí) - based on multiple kernel learning* SVM $(0, 1)$ 损失函数 (SVM $0, 1$ loss function) - SVM with the $(0, 1)$ loss function* 框架 (kōng zhì) - framework* 给出了 (gěi dòu le) - gives* 一些 (yī xiē) - some* 首要的 (shǒu yào de) - primary* 优化条件 (yòu yòu tiáo yì) - optimization conditions* 然后 (rán hái) - then* 利用 (lǐ yòng) - use* 这些条件 (zhè xiē tiáo yì) - these conditions* 开发 (kāi fā) - develop* 一个 (yī ge) - a* 快速的 (kuài sù de) - fast* ADMM算法 (ADMM suān gòu) - ADMM algorithm* 来解决 (laī jiě jué) - to solve* 非对称和不连续的 (fēi duì xiǎng yǔ bù lián zhí de) - nonconvex and nonsmooth* 优化问题 (yòu yòu wèn tí) - optimization problem* 实验 (shí yàn) - experiments* 表明 (biǎo mǐng) - show* 性能 (xìng néng) - performance* 与 (yǔ) - and* 2008年 Rakotomamonjy等 (2008 nián Rakotomamonjy déng) - Rakotomamonjy et al. in 2008* 在 (zài) - in* Journal of Machine Learning Research (Journal of Machine Learning Research)* 发表 (fā bèi) - published*  SimpleMKL (SimpleMKL) - SimpleMKL* 性能 (xìng néng) - performance* 相当 (xiāng dàng) - comparableI hope this helps!
</details></li>
</ul>
<hr>
<h2 id="Quantum-Noise-driven-Generative-Diffusion-Models"><a href="#Quantum-Noise-driven-Generative-Diffusion-Models" class="headerlink" title="Quantum-Noise-driven Generative Diffusion Models"></a>Quantum-Noise-driven Generative Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12013">http://arxiv.org/abs/2308.12013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Parigi, Stefano Martina, Filippo Caruso</li>
<li>for: 这个论文的目的是提出和讨论量子扩散模型的量子扩散模型，用于生成复杂的数据分布。</li>
<li>methods: 这个论文使用了机器学习技术实现的生成模型，特别是使用量子随机过程来驱动扩散模型，以生成新的 sintetic 数据。</li>
<li>results: 这个论文预计可以通过利用量子随机过程的特点，例如干扰、Entanglement和噪声的非常规交互，超越传统的扩散模型在推断中的主要计算压力，从而实现更高效的数据生成和预测。<details>
<summary>Abstract</summary>
Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a very remarkably beneficial key ingredient to generate much more complex probability distributions that would be difficult or even impossible to express classically, and from which a quantum processor might sample more efficiently than a classical one. Therefore, our results are expected to pave the way for new quantum-inspired or quantum-based generative diffusion algorithms addressing more powerfully classical tasks as data generation/prediction with widespread real-world applications ranging from climate forecasting to neuroscience, from traffic flow analysis to financial forecasting.
</details>
<details>
<summary>摘要</summary>
生成模型利用机器学习技术可以从 finite 数量的训练样本中推理出复杂和未知的数据分布，以生成新的 sintetic 数据。扩散模型是一种出现在的框架，最近已经超越了生成对抗网络在生成 sintetic 文本和高质量图像方面的性能。在这里，我们提出了量子扩散模型的量子扩散模型，可以在实际量子系统上进行实验。我们想利用量子特有的非rium特性，即减 coherence、Entanglement和噪声之间的非rivial交互，来超越类型 diffusion 模型的主要计算卷积。因此，我们建议利用量子噪声，不是探测和解决的问题，而是作为一个非常有利的元素，以生成更复杂的概率分布，这些分布可能是类型 diffusion 模型无法表达的，而且从量子处理器中采样可能更高效于类型处理器。因此，我们的结果预计将开拓出新的量子激发或量子基于的扩散算法，用于更有力的数据生成/预测，它们在广泛的实际应用中将扮演重要的角色，包括气候预测、神经科学、交通流量分析和金融预测等。
</details></li>
</ul>
<hr>
<h2 id="Neural-oscillators-for-magnetic-hysteresis-modeling"><a href="#Neural-oscillators-for-magnetic-hysteresis-modeling" class="headerlink" title="Neural oscillators for magnetic hysteresis modeling"></a>Neural oscillators for magnetic hysteresis modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12002">http://arxiv.org/abs/2308.12002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Chandra, Taniya Kapoor, Bram Daniels, Mitrofan Curti, Koen Tiels, Daniel M. Tartakovsky, Elena A. Lomonova</li>
<li>for: 模型和诊断普遍科学和工程中的普遍现象-干扰。</li>
<li>methods: 使用 ordinary differential equation-based recurrent neural network (RNN) 方法来模型和诊断干扰。</li>
<li>results: HystRNN 能够在未经训练的区域中适应并预测复杂的干扰模式，这是传统 RNN 方法所无法做到的。<details>
<summary>Abstract</summary>
Hysteresis is a ubiquitous phenomenon in science and engineering; its modeling and identification are crucial for understanding and optimizing the behavior of various systems. We develop an ordinary differential equation-based recurrent neural network (RNN) approach to model and quantify the hysteresis, which manifests itself in sequentiality and history-dependence. Our neural oscillator, HystRNN, draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states. The performance of HystRNN is evaluated to predict generalized scenarios, involving first-order reversal curves and minor loops. The findings show the ability of HystRNN to generalize its behavior to previously untrained regions, an essential feature that hysteresis models must have. This research highlights the advantage of neural oscillators over the traditional RNN-based methods in capturing complex hysteresis patterns in magnetic materials, where traditional rate-dependent methods are inadequate to capture intrinsic nonlinearity.
</details>
<details>
<summary>摘要</summary>
《干支度量学习：一种基于偏微分方程的循环神经网络方法》Introduction:干支度量是科学和工程中的一种普遍现象，其模型化和识别是理解和优化系统的行为的关键。在本文中，我们提出了基于偏微分方程的循环神经网络方法（HystRNN），以模型和量化干支度量。HystRNN draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states.Methodology:我们的HystRNN方法基于Ordinary Differential Equation (ODE)，它可以模型干支度量的循环和历史依赖性。我们通过将循环神经网络中的每个节点更新为一个偏微分方程，来实现模型的循环和历史依赖性。这种方法可以更好地捕捉干支度量的复杂特征，比如折返曲线和小循环。Results:我们通过测试HystRNN的性能，发现它可以在未经训练的区域中预测折返曲线和小循环。这表明HystRNN具有普适性，是一种可以在不同的干支度量情况下预测行为的模型。此外，我们还发现HystRNN的性能比传统的RNN-based方法更好，这表明循环神经网络可以更好地捕捉干支度量的复杂特征。Conclusion:本文提出了一种基于偏微分方程的循环神经网络方法（HystRNN），用于模型和量化干支度量。HystRNN draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states.我们的实验表明，HystRNN具有普适性和更好的预测性，可以在不同的干支度量情况下预测行为。这些结果表明循环神经网络可以更好地捕捉干支度量的复杂特征，比如折返曲线和小循环。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-Representation-Learning-Across-Domains"><a href="#Trustworthy-Representation-Learning-Across-Domains" class="headerlink" title="Trustworthy Representation Learning Across Domains"></a>Trustworthy Representation Learning Across Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12315">http://arxiv.org/abs/2308.12315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronghang Zhu, Dongliang Guo, Daiqing Qi, Zhixuan Chu, Xiang Yu, Sheng Li</li>
<li>for: 本研究旨在提供一个可靠的表征学习框架，以满足实际应用中的跨领域应用需求。</li>
<li>methods: 本研究使用了四个基本概念， namely 类别、隐私、公平和解释性，来建立一个包含多种方法的可靠表征学习框架。</li>
<li>results: 本研究提出了一个全面的文献综述，涵盖了从四个基本概念中的多种方法，以及它们在实际应用中的应用和发展前景。<details>
<summary>Abstract</summary>
As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.
</details>
<details>
<summary>摘要</summary>
现在人工智能系统在我们日常生活和人类社会中广泛应用，人们不仅享受了这些技术的好处，也面临着由这些系统引起的许多社会问题。为了让人工智能系统足够可靠和可信，很多研究者在建立可靠人工智能系统的指南方面做出了很多努力。机器学习是人工智能系统中最重要的一部分，表示学习是机器学习的核心技术。为了让表示学习在实际应用中是可靠的，例如跨领域场景，是非常有价值和必需的。 inspirited by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four concepts, i.e., robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.Here's the translation in Traditional Chinese:现在人工智能系统在我们日常生活和人类社会中广泛应用，人们不单享受了这些技术的好处，也面临由这些系统引起的许多社会问题。为了让人工智能系统足够可靠和可信，很多研究者在建立可靠人工智能系统的指南方面做出了很多努力。机器学习是人工智能系统中最重要的一部分，表示学习是机器学习的核心技术。为了让表示学习在实际应用中是可靠的，例如跨领域场景，是非常有价值和必需的。 inspirited by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four concepts, i.e., robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.
</details></li>
</ul>
<hr>
<h2 id="On-Uniformly-Optimal-Algorithms-for-Best-Arm-Identification-in-Two-Armed-Bandits-with-Fixed-Budget"><a href="#On-Uniformly-Optimal-Algorithms-for-Best-Arm-Identification-in-Two-Armed-Bandits-with-Fixed-Budget" class="headerlink" title="On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget"></a>On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12000">http://arxiv.org/abs/2308.12000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-An Wang, Kaito Ariu, Alexandre Proutiere</li>
<li>For: 本研究考虑了固定预算下最佳臂标识问题，具体是随机两臂投注机制下的 Bernoulli 奖励。* Methods: 我们使用了一种自然的一臂投注算法，即所谓的{\it 均匀投注}算法，并证明了这种算法在所有情况下都是最佳的。此外，我们还引入了一种名为{\it 一臂投注}的自然的算法类，并证明了任何能够与{\it 均匀投注}算法相当的算法都必须属于这种类。* Results: 我们证明了，无论是在所有情况下还是在某些特定情况下，都无法找到一种能够超过{\it 均匀投注}算法的算法。具体来说，我们证明了任何能够与{\it 均匀投注}算法相当的算法都必须是一种{\it 一臂投注}算法。这个结论解决了在\cite{qin2022open}中提出的两个开放问题。<details>
<summary>Abstract</summary>
We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
</details>
<details>
<summary>摘要</summary>
我们研究了固定预算下最佳臂标识问题， Specifically, we study the problem of best-arm identification with a fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the uniform sampling algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm.To achieve this result, we introduce the natural class of consistent and stable algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.Here's the translation in Traditional Chinese:我们研究了固定预算下最佳臂标识问题， Specifically, we study the problem of best-arm identification with a fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the uniform sampling algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm.To achieve this result, we introduce the natural class of consistent and stable algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
</details></li>
</ul>
<hr>
<h2 id="Relational-Concept-Based-Models"><a href="#Relational-Concept-Based-Models" class="headerlink" title="Relational Concept Based Models"></a>Relational Concept Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11991">http://arxiv.org/abs/2308.11991</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aghoreshwar/Awesome-Customer-Analytics">https://github.com/Aghoreshwar/Awesome-Customer-Analytics</a></li>
<li>paper_authors: Pietro Barbiero, Francesco Giannini, Gabriele Ciravegna, Michelangelo Diligenti, Giuseppe Marra</li>
<li>for: 本研究的目的是解释深度学习模型在关系领域中的工作方式，以便提高模型的可解释性和可信度。</li>
<li>methods: 本研究提出了一种新的关系深度学习模型，即关系概念基模型（Relational Concept-Based Models，RCBMs），该模型结合了深度学习和概念分析技术，以提高模型的可解释性和可信度。</li>
<li>results: 实验结果表明，关系CBMs可以与现有的关系黑盒模型（relational black-box models）匹配的普适性和可解释性，同时支持生成量化的概念基模型解释。此外，关系CBMs还可以在各种具有挑战性的情况下表现出色，如out-of-distribution场景、有限的训练数据场景和罕见概念指导场景。<details>
<summary>Abstract</summary>
The design of interpretable deep learning models working in relational domains poses an open challenge: interpretable deep learning methods, such as Concept-Based Models (CBMs), are not designed to solve relational problems, while relational models are not as interpretable as CBMs. To address this problem, we propose Relational Concept-Based Models, a family of relational deep learning methods providing interpretable task predictions. Our experiments, ranging from image classification to link prediction in knowledge graphs, show that relational CBMs (i) match generalization performance of existing relational black-boxes (as opposed to non-relational CBMs), (ii) support the generation of quantified concept-based explanations, (iii) effectively respond to test-time interventions, and (iv) withstand demanding settings including out-of-distribution scenarios, limited training data regimes, and scarce concept supervisions.
</details>
<details>
<summary>摘要</summary>
文本： Deep learning 模型在关系领域的设计呈现出一个开放的挑战：可读性深度学习方法，如概念基本模型（CBMs），不是设计来解决关系问题，而关系模型则不如可读性深度学习方法。为解决这个问题，我们提出了关系概念基本模型（Relational CBMs），这是一种可读性深度学习方法，可以在关系任务上提供可读性的任务预测。我们的实验，从图像分类到知识图表链接预测，表明了关系 CBMs 具有以下特点：(i) 与现有关系黑盒相比，可以达到相同的泛化性能; (ii) 可以生成量化的概念基本解释; (iii) 在测试时间干扰中能够有效回应; (iv) 在具有异常分布、有限训练数据和罕见概念监督的情况下，也能够坚持。翻译结果：文本：深度学习模型在关系领域的设计存在一个开放的挑战，可读性深度学习方法，如概念基本模型（CBMs），不是设计来解决关系问题，而关系模型则不如可读性深度学习方法。为解决这个问题，我们提出了关系概念基本模型（Relational CBMs），这是一种可读性深度学习方法，可以在关系任务上提供可读性的任务预测。我们的实验，从图像分类到知识图表链接预测，表明了关系 CBMs 具有以下特点：(i) 与现有关系黑盒相比，可以达到相同的泛化性能; (ii) 可以生成量化的概念基本解释; (iii) 在测试时间干扰中能够有效回应; (iv) 在具有异常分布、有限训练数据和罕见概念监督的情况下，也能够坚持。
</details></li>
</ul>
<hr>
<h2 id="Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks"><a href="#Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks" class="headerlink" title="Will More Expressive Graph Neural Networks do Better on Generative Tasks?"></a>Will More Expressive Graph Neural Networks do Better on Generative Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11978">http://arxiv.org/abs/2308.11978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiandong Zou, Xiangyu Zhao, Pietro Liò, Yiren Zhao</li>
<li>For: The paper is focused on the task of graph generation, specifically in the context of molecular graph generation for de-novo drug and molecular design.* Methods: The paper investigates the expressiveness of different Graph Neural Network (GNN) architectures in two popular generative frameworks (GCPN and GraphAF) on six different molecular generative objectives using the ZINC-250k dataset.* Results: The paper demonstrates that advanced GNNs can improve the performance of GCPN and GraphAF on molecular generation tasks, but GNN expressiveness is not a necessary condition for a good GNN-based generative model. Additionally, the paper shows that GCPN and GraphAF with advanced GNNs can achieve state-of-the-art results compared to 17 other non-GNN-based graph generative approaches on important metrics such as DRD2, Median1, and Median2.Here is the information in Simplified Chinese text:* 用途：研究 graph generation 任务，特别是在分子图生成中，以满足 de-novo 药物和分子设计。* 方法： investigate  Graph Neural Network (GNN)  Architecture 在 GCPN 和 GraphAF 中，并在 ZINC-250k 数据集上进行 six 个分子生成目标的测试。* 结果：显示 advanced GNNs 可以提高 GCPN 和 GraphAF 在分子生成任务中的性能，但 GNN 表达能力不是必需的条件。此外， paper 还证明 GCPN 和 GraphAF 可以使用 advanced GNNs 在 DRD2、Median1 和 Median2 等重要指标上达到 state-of-the-art 结果，并且比 17 种非 GNN-based graph generative approach 更好。<details>
<summary>Abstract</summary>
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZINC-250k dataset. Through our extensive experiments, we demonstrate that advanced GNNs can indeed improve the performance of GCPN and GraphAF on molecular generation tasks, but GNN expressiveness is not a necessary condition for a good GNN-based generative model. Moreover, we show that GCPN and GraphAF with advanced GNNs can achieve state-of-the-art results across 17 other non-GNN-based graph generative approaches, such as variational autoencoders and Bayesian optimisation models, on the proposed molecular generative objectives (DRD2, Median1, Median2), which are important metrics for de-novo molecular design.
</details>
<details>
<summary>摘要</summary>
Graph生成具有重要挑战，因为它需要预测具有多个节点和边的完整图基于单个标签。这个任务对实际应用中的许多应用，如新药和分子设计，具有基本重要性。在过去几年，有几种成功的方法在图生成领域出现。然而，这些方法受到两个主要缺点的影响：（1）被用的图神经网络（GNN）架构经常被忽视；和（2）这些方法通常只被评估在有限数量的指标上。为了填补这个空白，我们在图生成任务上investigate GNN的表达能力，并将GNN替换为更表达力强的GNN。 Specifically，我们分析了六种GNN在两个不同的生成框架（GCPN和GraphAF）中的表现，在ZINC-250k数据集上进行六种分子生成目标。通过我们的广泛实验，我们证明了高级GNN可以提高GCPN和GraphAF在分子生成任务中的表现，但GNN表达能力不是必要的condition for a good GNN-based generative model。此外，我们显示GCPN和GraphAF使用高级GNN可以在17种非GNN-based图生成方法（如变量自动编码器和抽象优化模型）上达到状态的最佳结果，在提案的分子生成目标（DRD2、Median1、Median2）上。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression"><a href="#Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression" class="headerlink" title="Approximating Score-based Explanation Techniques Using Conformal Regression"></a>Approximating Score-based Explanation Techniques Using Conformal Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11975">http://arxiv.org/abs/2308.11975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Alkhatib, Henrik Boström, Sofiane Ennadir, Ulf Johansson</li>
<li>for: 本文旨在提出和研究一种 computationally less costly 的 regression model，用于approximating score-based explanation techniques，如 SHAP 的输出。</li>
<li>methods: 本文使用了 inductive conformal prediction 框架，提供了有效性保证，以及多种 non-conformity measures，用于考虑 approximations 的困难性，同时保持计算成本低。</li>
<li>results: 本文通过大规模的实验研究，发现提案的方法可以significantly improve execution time compared to fast SHAP version，TreeSHAP。 results also suggest that the proposed method can produce tight intervals, while providing validity guarantees. In addition, the proposed approach allows for comparing explanations of different approximation methods and selecting a method based on how informative (tight) are the predicted intervals.<details>
<summary>Abstract</summary>
Score-based explainable machine-learning techniques are often used to understand the logic behind black-box models. However, such explanation techniques are often computationally expensive, which limits their application in time-critical contexts. Therefore, we propose and investigate the use of computationally less costly regression models for approximating the output of score-based explanation techniques, such as SHAP. Moreover, validity guarantees for the approximated values are provided by the employed inductive conformal prediction framework. We propose several non-conformity measures designed to take the difficulty of approximating the explanations into account while keeping the computational cost low. We present results from a large-scale empirical investigation, in which the approximate explanations generated by our proposed models are evaluated with respect to efficiency (interval size). The results indicate that the proposed method can significantly improve execution time compared to the fast version of SHAP, TreeSHAP. The results also suggest that the proposed method can produce tight intervals, while providing validity guarantees. Moreover, the proposed approach allows for comparing explanations of different approximation methods and selecting a method based on how informative (tight) are the predicted intervals.
</details>
<details>
<summary>摘要</summary>
黑盒模型的解释技术 often 使用 Score-based 的解释技术，但这些解释技术经常具有高计算成本，这限制了它们在时间紧张的上下文中的应用。因此，我们提出了和探索使用 computationally 较低成本的回归模型来近似黑盒模型的输出。此外，我们采用了 inductive  conformal prediction 框架提供了有效性保证。我们提出了一些非准确度度量，用于考虑近似解释的困难而保持计算成本低。我们在大规模的实验中提出了这些方法，并评估了这些方法的效率（间隔大小）。结果表明，我们的提议方法可以significantly 改善执行时间，相比于快速版本的 SHAP，TreeSHAP。结果还表明，我们的方法可以生成紧凑的间隔，同时提供有效性保证。此外，我们的方法允许比较不同的近似方法的解释，并选择一种基于解释是否具有紧凑的预测间隔的方法。
</details></li>
</ul>
<hr>
<h2 id="EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE"><a href="#EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE" class="headerlink" title="EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE"></a>EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11971">http://arxiv.org/abs/2308.11971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Chen, Longteng Guo, Jia Sun, Shuai Shao, Zehuan Yuan, Liang Lin, Dongyu Zhang<br>for:这篇论文旨在开探如何建立可扩展的视觉语言模型，以学习具有多 modal 数据的多模式资料。methods:论文提出了一个效率的视觉语言基础模型，名为EVE，它是一个统一的多模式Transformer预测器，具有适应器 Mixture-of-Experts（MoE）模组，可以选择性地转换到不同的专家。results:论文表明，EVE可以在训练时间和资源更少的情况下，实现更好的下游性能，并且在多种视觉语言下游任务上实现了州流的表现。<details>
<summary>Abstract</summary>
Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy to scale up, enabling better downstream performance with fewer resources and faster training speed. Despite its simplicity, EVE achieves state-of-the-art performance on various vision-language downstream tasks, including visual question answering, visual reasoning, and image-text retrieval.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将视觉语言模型建立成可扩展的基础模型仍然是一个开放的挑战。在这篇论文中，我们介绍了一个高效的视觉语言基础模型，即EVE（高效视觉语言基础模型）。EVE是一个共享转换网络，其中视觉和语言都被编码在同一个网络中，并通过特性意识模块来捕捉不同类型的信息。这些模块可以选择性地切换到不同的专家，以捕捉不同类型的信息。为了统一视觉和语言预训练任务，EVE在图像和文本对中进行遮盲信号模型，即将图像像素和文本字符遮盲，然后使用可见信号来还原遮盲信号。这个简单而有效的预训练目标可以加速训练，比 tradicional Image-Text Contrastive和Image-Text Matching损失的3.5倍。由于EVE的整合的体系和预训练任务，它具有更好的下游性能，需要 fewer resources和更快的训练速度。尽管它的简单性，EVE仍然达到了视觉语言下游任务的州OF-THE-ART性能，包括视觉问答、视觉逻辑和图像-文本检索。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification"><a href="#Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification" class="headerlink" title="Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification"></a>Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11969">http://arxiv.org/abs/2308.11969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Lambert, Pauline Roca, Florence Forbes, Senan Doyle, Michel Dojat</li>
<li>for: 本研究旨在提出一种自动化的肝脏和肿瘤分 segmentation 方法，以帮助治疗抑制肝癌的医疗决策。</li>
<li>methods: 本研究使用了两种不同的管道，即多类分类模型和二元分类模型，以实现肝脏和肿瘤的同时分 segmentation。</li>
<li>results: 研究结果显示了两种管道具有不同的优劣点，并提出了一种不确定性评估策略，以识别可能存在的假阳性肿瘤患者。<details>
<summary>Abstract</summary>
The burden of liver tumors is important, ranking as the fourth leading cause of cancer mortality. In case of hepatocellular carcinoma (HCC), the delineation of liver and tumor on contrast-enhanced magnetic resonance imaging (CE-MRI) is performed to guide the treatment strategy. As this task is time-consuming, needs high expertise and could be subject to inter-observer variability there is a strong need for automatic tools. However, challenges arise from the lack of available training data, as well as the high variability in terms of image resolution and MRI sequence. In this work we propose to compare two different pipelines based on anisotropic models to obtain the segmentation of the liver and tumors. The first pipeline corresponds to a baseline multi-class model that performs the simultaneous segmentation of the liver and tumor classes. In the second approach, we train two distinct binary models, one segmenting the liver only and the other the tumors. Our results show that both pipelines exhibit different strengths and weaknesses. Moreover we propose an uncertainty quantification strategy allowing the identification of potential false positive tumor lesions. Both solutions were submitted to the MICCAI 2023 Atlas challenge regarding liver and tumor segmentation.
</details>
<details>
<summary>摘要</summary>
liver tumor 的负担是非常重要的， ranking as the fourth leading cause of cancer mortality。在肝细胞癌（HCC）的 случа子中，通过对增强磁共振成像（CE-MRI）进行描述，以便引导治疗策略。然而，由于这个任务需要较高的专业知识和较长的时间，并且可能会受到观察者间的差异，因此有强需求于自动工具。然而，由于数据不足以及图像分辨率和MRI序列的高变化性，这些任务具有挑战性。在这项工作中，我们提出了两种不同的管道，基于不规则模型来实现肝脏和肿瘤的分割。第一个管道是基础多类模型，同时进行肝脏和肿瘤的同时分割。第二个管道是分别训练两个不同的二进制模型，一个用于肝脏的分割，另一个用于肿瘤的分割。我们的结果显示，这两种管道具有不同的优势和劣势。此外，我们还提出了一种不确定性评估策略，以便标识潜在的假阳性肿瘤患区。这两种解决方案都被提交到了MICCAI 2023 Atlas challenge，关于肝脏和肿瘤的分割。
</details></li>
</ul>
<hr>
<h2 id="Maintaining-Plasticity-via-Regenerative-Regularization"><a href="#Maintaining-Plasticity-via-Regenerative-Regularization" class="headerlink" title="Maintaining Plasticity via Regenerative Regularization"></a>Maintaining Plasticity via Regenerative Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11958">http://arxiv.org/abs/2308.11958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabh Kumar, Henrik Marklund, Benjamin Van Roy</li>
<li>for: 维护 continual learning 中的材料塑性（plasticity），使 neural network 能够快速适应新信息。</li>
<li>methods: 提出了 L2 Init，一种简单的方法，通过在损失函数中添加 L2 正则化来维护初始参数的塑性。</li>
<li>results: 在不同类型的非站ARY数据流程上进行了简单的问题示例，证明了 L2 Init 能够有效地避免材料塑性损失。 另外，我们发现了这个正则化项可以减少参数的大小，并保持高效的特征级别。<details>
<summary>Abstract</summary>
In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates plasticity loss. We additionally find that our regularization term reduces parameter magnitudes and maintains a high effective feature rank.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation"><a href="#When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation" class="headerlink" title="When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation"></a>When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11953">http://arxiv.org/abs/2308.11953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Huang, Geng Tian, Ming Tang<br>for:这个论文旨在解决 federated learning (FL) 中的客户端偏移问题，提出了 MiniBatch-SFL 算法，它将 MiniBatch SGD integrated into SFL 中。methods:这个论文使用了 SFL 分布式算法，并在客户端和服务器之间分割模型。客户端只需要训练部分模型，而服务器则负责训练服务器端模型。此外，这个论文还使用了 MiniBatch SGD 算法来优化客户端模型。results:这个论文的实验结果表明，MiniBatch-SFL 算法可以在非Identical Independent Distributions (non-IID) 数据上提高精度，并且可以与传统的 FL 和 SFL 相比，提高精度达到 24.1% 和 17.1%。此外，这个论文还发现，将 cut layer 放置在模型的末端可以降低客户端模型的均值梯度偏移。<details>
<summary>Abstract</summary>
Federated learning (FL) enables collaborative model training across distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can be computationally expensive as the clients need to train the entire model multiple times. SplitFed learning (SFL) is a recent distributed approach that alleviates computation workload at the client device by splitting the model at a cut layer into two parts, where clients only need to train part of the model. However, SFL still suffers from the \textit{client drift} problem when clients' data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This algorithm incorporates MiniBatch SGD into SFL, where the clients train the client-side model in an FL fashion while the server trains the server-side model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and show that the bound of the expected loss can be obtained by analyzing the expected server-side and client-side model updates, respectively. The server-side updates do not depend on the non-IID degree of the clients' datasets and can potentially mitigate client drift. However, the client-side model relies on the non-IID degree and can be optimized by properly choosing the cut layer. Perhaps counter-intuitive, our empirical result shows that a latter position of the cut layer leads to a smaller average gradient divergence and a better algorithm performance. Moreover, numerical results show that MiniBatch-SFL achieves higher accuracy than conventional SFL and FL. The accuracy improvement can be up to 24.1\% and 17.1\% with highly non-IID data, respectively.
</details>
<details>
<summary>摘要</summary>
分布式学习（FL）允许分布式客户端（例如边缘设备）共同训练模型，无需分享原始数据。然而，FL可能具有 computationally expensive 的问题，因为客户端需要训练整个模型多次。SplitFed learning（SFL）是一种最近的分布式方法，它将模型在一个割层中分成两部分，客户端只需要训练一部分模型。然而，SFL仍然受到客户端数据非常不一致（non-IID）的问题困扰。为解决这个问题，我们提出了 MiniBatch-SFL。这个算法将 MiniBatch SGD  integrate into SFL，客户端在分布式方式上训练客户端模型，服务器则在服务器模型上进行类似的 MiniBatch SGD 训练。我们分析了 MiniBatch-SFL 的收敛性，并证明了 bound 的预期损失可以通过分析服务器和客户端模型更新的预期值来获得。服务器端更新不依赖于客户端数据的非一致程度，可能减轻客户端游弋问题。然而，客户端模型受到非一致度的影响，可以通过选择合适的割层来优化。 Surprisingly,我们的实验结果显示，在割层的位置越后，客户端模型的平均梯度差异越小，算法性能更好。此外，我们的数值结果显示，MiniBatch-SFL 可以达到更高的准确率，比 conventinal SFL 和 FL 高出 24.1% 和 17.1%。
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-Transformer-Pyramid-Networks-for-Multivariate-Time-Series-Forecasting"><a href="#Multi-scale-Transformer-Pyramid-Networks-for-Multivariate-Time-Series-Forecasting" class="headerlink" title="Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting"></a>Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11946">http://arxiv.org/abs/2308.11946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhang, Rui Wu, Sergiu M. Dascalu, Frederick C. Harris Jr</li>
<li>for: 这篇论文主要针对多重时间序列（MTS）预测任务进行探索，旨在模型时间序列之间的相互 dependencies。</li>
<li>methods: 本论文提出了一种维度不变的嵌入技术，可以将MTS数据转换为更高维度的空间，保持时间步骤和变数的维度，并且提出了一个多尺度 transformer  pyramid network（MTPNet），可以有效地捕捉时间序列之间的多个不同级数的相互dependencies。</li>
<li>results: 实验结果显示，提出的MTPNet方法在九个 benchmark 数据集上表现出色，较前一些现有的方法更好。<details>
<summary>Abstract</summary>
Multivariate Time Series (MTS) forecasting involves modeling temporal dependencies within historical records. Transformers have demonstrated remarkable performance in MTS forecasting due to their capability to capture long-term dependencies. However, prior work has been confined to modeling temporal dependencies at either a fixed scale or multiple scales that exponentially increase (most with base 2). This limitation hinders their effectiveness in capturing diverse seasonalities, such as hourly and daily patterns. In this paper, we introduce a dimension invariant embedding technique that captures short-term temporal dependencies and projects MTS data into a higher-dimensional space, while preserving the dimensions of time steps and variables in MTS data. Furthermore, we present a novel Multi-scale Transformer Pyramid Network (MTPNet), specifically designed to effectively capture temporal dependencies at multiple unconstrained scales. The predictions are inferred from multi-scale latent representations obtained from transformers at various scales. Extensive experiments on nine benchmark datasets demonstrate that the proposed MTPNet outperforms recent state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
多变量时间序列（MTS）预测涉及到历史记录中的时间相关性模型化。transformers已经表现出了很好的性能在MTS预测中，因为它们可以捕捉长期相关性。然而，先前的工作受限于模型时间相关性的固定尺度或多个尺度，其中尺度递增级数(大多数为2)。这种限制使得它们难以捕捉多样化的季节性，如每小时和每天的模式。在这篇论文中，我们介绍了一种维度不变的嵌入技术，该技术可以捕捉短期时间相关性，并将MTS数据 проек到高维空间中，保持时间步骤和变量的维度。此外，我们提出了一种新的多尺度transformer piramid网络（MTPNet），该网络专门设计用于有效地捕捉多个不受限制的时间尺度的时间相关性。预测来自多个尺度的秘密表示，由transformers在不同尺度上获得。经验表明，我们提出的MTPNet在九个 benchmark datasets上表现出了较高的性能。
</details></li>
</ul>
<hr>
<h2 id="RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching"><a href="#RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching" class="headerlink" title="RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching"></a>RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11943">http://arxiv.org/abs/2308.11943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steve Vott, Adam M. Lehavi</li>
<li>for: 本研究探讨了应用最佳先进搜索算法和强化学习（RL）技术来找到特定的拉曼数字（Ramsey number）的对例。</li>
<li>methods: 本研究引入了图Vectorization和深度强化网络（DNN）基于的优化方法，用于评估图是否为对例，并且提出了一些算法优化来减少搜索时间的复杂度。</li>
<li>results: 本研究不目的是发现新的对例，而是提出了一种基于RL技术的对例探讨框架，可以应用于其他评估器。<details>
<summary>Abstract</summary>
The Ramsey number is the minimum number of nodes, $n = R(s, t)$, such that all undirected simple graphs of order $n$, contain a clique of order $s$, or an independent set of order $t$. This paper explores the application of a best first search algorithm and reinforcement learning (RL) techniques to find counterexamples to specific Ramsey numbers. We incrementally improve over prior search methods such as random search by introducing a graph vectorization and deep neural network (DNN)-based heuristic, which gauge the likelihood of a graph being a counterexample. The paper also proposes algorithmic optimizations to confine a polynomial search runtime. This paper does not aim to present new counterexamples but rather introduces and evaluates a framework supporting Ramsey counterexample exploration using other heuristics. Code and methods are made available through a PyPI package and GitHub repository.
</details>
<details>
<summary>摘要</summary>
“拉姆馆数”是最少节点数量，$n = R(s, t)$, 使得所有无向简单图的顺序为$n$，都包含一个 clique 的顺序为$s$，或一个独立集的顺序为$t$。这篇论文探讨了使用最佳先搜索算法和强化学习（RL）技术来找到特定拉姆数字的对例。我们在先前搜索方法，如随机搜索，基础上进行了改进，通过引入图vector化和深度神经网络（DNN）基于的优化，来评估图是否为对例。这篇论文也提出了算法优化来限制搜索时间的多项式增长。本论文不是为提供新的对例，而是为探讨拉姆对例探索使用其他规则的框架。代码和方法通过PyPI包和GitHub存储库提供。
</details></li>
</ul>
<hr>
<h2 id="Audio-Generation-with-Multiple-Conditional-Diffusion-Model"><a href="#Audio-Generation-with-Multiple-Conditional-Diffusion-Model" class="headerlink" title="Audio Generation with Multiple Conditional Diffusion Model"></a>Audio Generation with Multiple Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11940">http://arxiv.org/abs/2308.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifang Guo, Jianguo Mao, Rui Tao, Long Yan, Kazushige Ouchi, Hong Liu, Xiangdong Wang</li>
<li>for: 这个研究的目的是提高现有的文本到audio模型的可控性，以便在仅仅基于文本的情况下实现细化的声音生成。</li>
<li>methods: 该模型使用了一种新的方法，即在已经训练过的文本到audio模型基础上加入了内容（时间戳）和风格（折射和能量折射）等附加条件，以便控制生成的声音的时间顺序、折射和能量。</li>
<li>results: 实验结果表明，该模型成功实现了细化的声音生成，并且可以控制生成的声音在不同的时间戳、折射和能量上的表现。<details>
<summary>Abstract</summary>
Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a series of evaluation metrics to evaluate the controllability performance. Experimental results demonstrate that our model successfully achieves fine-grained control to accomplish controllable audio generation. Audio samples and our dataset are publicly available at https://conditionaudiogen.github.io/conditionaudiogen/
</details>
<details>
<summary>摘要</summary>
文本基于的音频生成模型受限，因为它们无法包含所有音频中的信息，导致仅仅基于文本的控制性不够。为解决这问题，我们提出了一种新的模型，它可以增强现有的预训练文本到音频模型的控制性，通过添加内容（时间戳）和风格（折射和能量折射）等条件。这种方法可以实现细致的控制音频的时间顺序、折射和能量。为保持生成的多样性，我们使用可训练的控制条件编码器，其中包括一个大型自然语言模型和可训练的拟合网络，以编码和融合其他条件，同时保持预训练文本到音频模型的重量冰结。由于缺乏适合的数据集和评价指标，我们将现有的数据集整合成一个新的数据集，包括音频和相应的条件，并使用一系列的评价指标来评价控制性性能。实验结果表明，我们的模型成功实现了细致的控制，以实现可控音频生成。生成的音频和数据集可以在https://conditionaudiogen.github.io/conditionaudiogen/上公开获取。
</details></li>
</ul>
<hr>
<h2 id="Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series"><a href="#Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series" class="headerlink" title="Retail Demand Forecasting: A Comparative Study for Multivariate Time Series"></a>Retail Demand Forecasting: A Comparative Study for Multivariate Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11939">http://arxiv.org/abs/2308.11939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sabbirul Haque, Md Shahedul Amin, Jonayet Miah</li>
<li>for: 预测零售需求的精度预测是零售业的关键因素，对于公司的财务表现和供应链效率都是关键。</li>
<li>methods: 本研究使用了时间序列数据和 macro经济变量（如Consumer Price Index（CPI）、Index of Consumer Sentiment（ICS）和失业率）进行预测，并比较了不同的回归和机器学习模型以确定最佳预测模型。</li>
<li>results: 研究发现，通过添加macro经济变量，可以更好地预测零售需求，并且可以选择合适的预测模型以达到更高的预测精度。<details>
<summary>Abstract</summary>
Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.
</details>
<details>
<summary>摘要</summary>
<<sys.language_model.translate(text="Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.", from_language="en", to_language="zh-CN")>>Here's the translation:精准预测销售需求是零售业的关键性质因素，直接影响营业性和供应链效率。随着全球市场变得越来越相连，企业正在寻找更高级的预测模型，以获得竞争优势。然而，现有的文献主要关注历史销售数据，忽视了消费者支出行为中的重要影响因素。本研究通过把 macro经济条件纳入销售需求时间序列数据中，使用 Consumer Price Index (CPI)、Index of Consumer Sentiment (ICS) 和失业率等macro经济变量，并利用这些全面的数据来开发和比较不同的回归和机器学习模型，以准确预测零售需求。
</details></li>
</ul>
<hr>
<h2 id="System-Identification-for-Continuous-time-Linear-Dynamical-Systems"><a href="#System-Identification-for-Continuous-time-Linear-Dynamical-Systems" class="headerlink" title="System Identification for Continuous-time Linear Dynamical Systems"></a>System Identification for Continuous-time Linear Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11933">http://arxiv.org/abs/2308.11933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jonas-Nicodemus/phdmd">https://github.com/Jonas-Nicodemus/phdmd</a></li>
<li>paper_authors: Peter Halmos, Jonathan Pillow, David A. Knowles</li>
<li>for: 这篇论文旨在探讨kalman筛Filter的系统识别问题，具体来说是通过期望最大化（EM）程序来学习底层系统的参数。</li>
<li>methods: 这篇论文使用了一种新的两filter方法，其中一个是 posterior的分布，另一个是 bayesian derivation。这两个方法可以减少了前向传递的计算量。</li>
<li>results: 通过这种新的方法， authors 可以在不Regularly sampled measurements中进行系统识别，并且可以扩展kalman筛Filter的应用范围。他们还提出了一种扩展了learning的方法，可以在不Regularly sampled measurements中学习非线性系统。<details>
<summary>Abstract</summary>
The problem of system identification for the Kalman filter, relying on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has largely been studied assuming that observations are sampled at equally-spaced time points. However, in many applications this is a restrictive and unrealistic assumption. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates which do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure which estimates the parameters of the SDE, naturally incorporating irregularly sampled measurements. Generalizing the learning of latent linear dynamical systems (LDS) to continuous-time may extend the use of the hybrid Kalman filter to data which is not regularly sampled or has intermittent missing values, and can extend the power of non-linear system identification methods such as switching LDS (SLDS), which rely on EM for the linear discrete-time Kalman filter as a sub-unit for learning locally linearized behavior of a non-linear system. We apply the method by learning the parameters of a latent, multivariate Fokker-Planck SDE representing a toggle-switch genetic circuit using biologically realistic parameters, and compare the efficacy of learning relative to the discrete-time Kalman filter as the step-size irregularity and spectral-radius of the dynamics-matrix increases.
</details>
<details>
<summary>摘要</summary>
System identification for the Kalman filter, which relies on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has been largely studied assuming that observations are sampled at equally-spaced time points. However, in many applications, this assumption is unrealistic. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates that do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure that estimates the parameters of the SDE, naturally incorporating irregularly sampled measurements. Generalizing the learning of latent linear dynamical systems (LDS) to continuous-time may extend the use of the hybrid Kalman filter to data that is not regularly sampled or has intermittent missing values, and can extend the power of non-linear system identification methods such as switching LDS (SLDS), which rely on EM for the linear discrete-time Kalman filter as a sub-unit for learning locally linearized behavior of a non-linear system. We apply the method by learning the parameters of a latent, multivariate Fokker-Planck SDE representing a toggle-switch genetic circuit using biologically realistic parameters, and compare the efficacy of learning relative to the discrete-time Kalman filter as the step-size irregularity and spectral-radius of the dynamics-matrix increases.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-landslide-susceptibility-mapping-over-recent-three-decades-to-uncover-variations-in-landslide-causes-in-subtropical-urban-mountainous-areas"><a href="#Dynamic-landslide-susceptibility-mapping-over-recent-three-decades-to-uncover-variations-in-landslide-causes-in-subtropical-urban-mountainous-areas" class="headerlink" title="Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas"></a>Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11929">http://arxiv.org/abs/2308.11929</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cli-de/d_lsm">https://github.com/cli-de/d_lsm</a></li>
<li>paper_authors: Peifeng Ma, Li Chen, Chang Yu, Qing Zhu, Yulin Ding</li>
<li>for: 这项研究的目的是为了开发一种能够适应不同时间间隔的风险隐藏易受损地区风险评估方法，以便更好地预测滥覆风险。</li>
<li>methods: 这项研究使用了多种预测模型来实现年度风险评估，并使用了 SHAP 算法来解释每个模型的输入特征和预测结果。此外，研究还使用了 MT-InSAR 技术来增强和验证风险评估结果。</li>
<li>results: 研究结果显示，在香港大屿山岛的风险评估中，地形坡度和极端雨量是诱发滥覆的主要因素。此外，研究还发现，由于全球气候变化和香港政府实施的LPMitP计划，风险评估结果的变化归因于极端雨量事件。<details>
<summary>Abstract</summary>
Landslide susceptibility assessment (LSA) is of paramount importance in mitigating landslide risks. Recently, there has been a surge in the utilization of data-driven methods for predicting landslide susceptibility due to the growing availability of aerial and satellite data. Nonetheless, the rapid oscillations within the landslide-inducing environment (LIE), primarily due to significant changes in external triggers such as rainfall, pose difficulties for contemporary data-driven LSA methodologies to accommodate LIEs over diverse timespans. This study presents dynamic landslide susceptibility mapping that simply employs multiple predictive models for annual LSA. In practice, this will inevitably encounter small sample problems due to the limited number of landslide samples in certain years. Another concern arises owing to the majority of the existing LSA approaches train black-box models to fit distinct datasets, yet often failing in generalization and providing comprehensive explanations concerning the interactions between input features and predictions. Accordingly, we proposed to meta-learn representations with fast adaptation ability using a few samples and gradient updates; and apply SHAP for each model interpretation and landslide feature permutation. Additionally, we applied MT-InSAR for LSA result enhancement and validation. The chosen study area is Lantau Island, Hong Kong, where we conducted a comprehensive dynamic LSA spanning from 1992 to 2019. The model interpretation results demonstrate that the primary factors responsible for triggering landslides in Lantau Island are terrain slope and extreme rainfall. The results also indicate that the variation in landslide causes can be primarily attributed to extreme rainfall events, which result from global climate change, and the implementation of the Landslip Prevention and Mitigation Programme (LPMitP) by the Hong Kong government.
</details>
<details>
<summary>摘要</summary>
降坡风险评估 (LSA) 在减轻降坡风险方面具有重要的重要性。在最近几年，由于飞地和卫星数据的可用性的增加，数据驱动方法在预测降坡风险方面得到了广泛的应用。然而，降坡 inducing 环境 (LIE) 中的快速摆动，主要归因于外部触发因素的显著变化，如降水量，使得当今的数据驱动 LSA 方法困难于同时覆盖多年时间。本研究提出了动态降坡风险地图，使用多个预测模型来年度预测降坡风险。在实践中，这将不可避免小样本问题，因为降坡样本数在某些年仅有限制。另一个问题在于大多数现有 LSA 方法通常会训练黑盒模型适应特定数据集，而不能总结和提供降坡特征之间和预测之间的丰富解释。因此，我们提议使用元学习来学习表达能力快速适应，使用少量样本和梯度更新；并使用 SHAP 来对每个模型进行解释和降坡特征的排序。此外，我们还应用 MT-InSAR 来增强和验证 LSA 结果。选择的研究区为香港大屿山岛，我们在1992年至2019年之间进行了全面的动态 LSA。模型解释结果显示，降坡岛的主要触发降坡的因素是地形坡度和极端降水量。结果还表明，降坡的变化原因可以主要归因于全球气候变化和香港政府实施的降坡预防和控制Programme (LPMitP)。
</details></li>
</ul>
<hr>
<h2 id="Solving-Elliptic-Optimal-Control-Problems-using-Physics-Informed-Neural-Networks"><a href="#Solving-Elliptic-Optimal-Control-Problems-using-Physics-Informed-Neural-Networks" class="headerlink" title="Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks"></a>Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11925">http://arxiv.org/abs/2308.11925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bangti Jin, Ramesh Sau, Luowei Yin, Zhi Zhou</li>
<li>for: 这篇论文是关于优化控制问题的数值解决方案，包括不包括箱constraint的情况。</li>
<li>methods: 该方法基于优化控制问题的第一阶最优系统，使用物理学 informed neural networks (PINNs) 解决coupled系统。</li>
<li>results: 论文提供了深度逻辑学习网络参数（例如深度、宽度、参数范围）和样本点数的$L^2(\Omega)$ 误差 bounds，并进行了误差分析。示例包括了比较现有三种方法。<details>
<summary>Abstract</summary>
In this work, we present and analyze a numerical solver for optimal control problems (without / with box constraint) for linear and semilinear second-order elliptic problems. The approach is based on a coupled system derived from the first-order optimality system of the optimal control problem, and applies physics informed neural networks (PINNs) to solve the coupled system. We present an error analysis of the numerical scheme, and provide $L^2(\Omega)$ error bounds on the state, control and adjoint state in terms of deep neural network parameters (e.g., depth, width, and parameter bounds) and the number of sampling points in the domain and on the boundary. The main tools in the analysis include offset Rademacher complexity and boundedness and Lipschitz continuity of neural network functions. We present several numerical examples to illustrate the approach and compare it with three existing approaches.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了一种数值方法来解决优化控制问题（无框约束和框约束）的线性和半线性第二阶几何问题。我们基于优化控制问题的第一阶优化系统 derivated 一个联系系统，并使用物理学 Informed Neural Networks (PINNs) 解决这个联系系统。我们提供了数值方案的误差分析，并给出了 $L^2(\Omega)$ 误差 bound 在状态、控制和副状态上，这些 bound 取决于深度神经网络参数（例如深度、宽度和参数 bound）和采样点数量在领域和边界上。我们使用偏移Rademacher复杂度和领域和边界上的神经网络函数的稳定性和 lipschitz 连续性作为主要工具进行分析。我们在数据中提供了多个数值示例，以示出方法的应用和与三种现有方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe"><a href="#Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe" class="headerlink" title="Diverse Policies Converge in Reward-free Markov Decision Processe"></a>Diverse Policies Converge in Reward-free Markov Decision Processe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11924">http://arxiv.org/abs/2308.11924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openrl-lab/diversepolicies">https://github.com/openrl-lab/diversepolicies</a></li>
<li>paper_authors: Fanqi Lin, Shiyu Huang, Weiwei Tu</li>
<li>for: 这篇论文的目的是提供一种统一的多种政策强化学习框架，并研究多种政策强化学习的训练是如何 converges 和效率如何。</li>
<li>methods: 这篇论文使用了一种提取多种政策的框架，并提出了一种可证明有效的多种政策强化学习算法。</li>
<li>results: 经过数学实验，论文证明了其方法的有效性和效率。<details>
<summary>Abstract</summary>
Reinforcement learning has achieved great success in many decision-making tasks, and traditional reinforcement learning algorithms are mainly designed for obtaining a single optimal solution. However, recent works show the importance of developing diverse policies, which makes it an emerging research topic. Despite the variety of diversity reinforcement learning algorithms that have emerged, none of them theoretically answer the question of how the algorithm converges and how efficient the algorithm is. In this paper, we provide a unified diversity reinforcement learning framework and investigate the convergence of training diverse policies. Under such a framework, we also propose a provably efficient diversity reinforcement learning algorithm. Finally, we verify the effectiveness of our method through numerical experiments.
</details>
<details>
<summary>摘要</summary>
现在的束缚学习已经在许多决策任务中取得了很大的成功，但传统的束缚学习算法主要是为了获得单个优化解决方案。然而，最近的研究表明了多样化策略的重要性，使得这成为一个emerging研究话题。虽然多样化束缚学习算法的多种出现，但没有任何一个能够理论地回答束缚学习算法是如何收敛的和如何效率的问题。在这篇论文中，我们提供了一个统一的多样化束缚学习框架，并investigate束缚学习训练多样化策略的收敛性。根据这种框架，我们还提出了一种可证明高效的多样化束缚学习算法。最后，我们通过数学实验验证了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement"><a href="#Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement" class="headerlink" title="Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement"></a>Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11923">http://arxiv.org/abs/2308.11923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino</li>
<li>for: 这 paper 是为了解决音频描述中的 semantic difference 问题，提出了 Audio Difference Captioning (ADC) 任务。</li>
<li>methods: 该 paper 提出了一种 cross-attention-concentrated transformer encoder 和一种 similarity-discrepancy disentanglement 来提取差异。</li>
<li>results: 实验表明，提出的方法可以有效地解决 ADC 任务，并且可以提高 transformer encoder 中的 attention weights 来提取差异。<details>
<summary>Abstract</summary>
We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.
</details>
<details>
<summary>摘要</summary>
我们提出了听音差异描述（ADC）作为audio描述中的新扩展任务，用于描述输入对的相似 yet slightly different 听音clip中的semantic差异。ADC解决了 convention audio描述 Sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Addressing-Selection-Bias-in-Computerized-Adaptive-Testing-A-User-Wise-Aggregate-Influence-Function-Approach"><a href="#Addressing-Selection-Bias-in-Computerized-Adaptive-Testing-A-User-Wise-Aggregate-Influence-Function-Approach" class="headerlink" title="Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach"></a>Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11912">http://arxiv.org/abs/2308.11912</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/riiid/useraif">https://github.com/riiid/useraif</a></li>
<li>paper_authors: Soonwoo Kwon, Sojung Kim, Seunghyun Lee, Jin-Young Kim, Suyeong An, Kyuseok Kim</li>
<li>for: 这 paper 的目的是提出一种基于CAT服务中的应答数据的项目 profiling 方法，以提高CAT的效率和准确性。</li>
<li>methods: 这 paper 使用了一种叫做 user-wise aggregate influence function 方法，该方法可以纠正CAT中选择性的偏见问题，并提高CAT的性能。</li>
<li>results:  experiments 表明，使用了该方法可以减少CAT中的偏见问题，并提高CAT的准确性和效率。<details>
<summary>Abstract</summary>
Computerized Adaptive Testing (CAT) is a widely used, efficient test mode that adapts to the examinee's proficiency level in the test domain. CAT requires pre-trained item profiles, for CAT iteratively assesses the student real-time based on the registered items' profiles, and selects the next item to administer using candidate items' profiles. However, obtaining such item profiles is a costly process that involves gathering a large, dense item-response data, then training a diagnostic model on the collected data. In this paper, we explore the possibility of leveraging response data collected in the CAT service. We first show that this poses a unique challenge due to the inherent selection bias introduced by CAT, i.e., more proficient students will receive harder questions. Indeed, when naively training the diagnostic model using CAT response data, we observe that item profiles deviate significantly from the ground-truth. To tackle the selection bias issue, we propose the user-wise aggregate influence function method. Our intuition is to filter out users whose response data is heavily biased in an aggregate manner, as judged by how much perturbation the added data will introduce during parameter estimation. This way, we may enhance the performance of CAT while introducing minimal bias to the item profiles. We provide extensive experiments to demonstrate the superiority of our proposed method based on the three public datasets and one dataset that contains real-world CAT response data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diagnosing-Infeasible-Optimization-Problems-Using-Large-Language-Models"><a href="#Diagnosing-Infeasible-Optimization-Problems-Using-Large-Language-Models" class="headerlink" title="Diagnosing Infeasible Optimization Problems Using Large Language Models"></a>Diagnosing Infeasible Optimization Problems Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12923">http://arxiv.org/abs/2308.12923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Gonzalo E. Constante-Flores, Can Li</li>
<li>for: 本研究旨在帮助决策问题中的优化模型实用化，通过自然语言对话系统OptiChat来帮助用户更好地理解和解释无法满足约束的优化模型。</li>
<li>methods: 本研究使用了GPT-4和一些自然语言处理技术，如几何学习、专家链思维、关键提取和情感提示，以提高OptiChat的可靠性。</li>
<li>results: 实验表明，OptiChat可以帮助both expert和非专家用户更好地理解优化模型，快速地发现约束不可遵循的来源。<details>
<summary>Abstract</summary>
Decision-making problems can be represented as mathematical optimization models, finding wide applications in fields such as economics, engineering and manufacturing, transportation, and health care. Optimization models are mathematical abstractions of the problem of making the best decision while satisfying a set of requirements or constraints. One of the primary barriers to deploying these models in practice is the challenge of helping practitioners understand and interpret such models, particularly when they are infeasible, meaning no decision satisfies all the constraints. Existing methods for diagnosing infeasible optimization models often rely on expert systems, necessitating significant background knowledge in optimization. In this paper, we introduce OptiChat, a first-of-its-kind natural language-based system equipped with a chatbot GUI for engaging in interactive conversations about infeasible optimization models. OptiChat can provide natural language descriptions of the optimization model itself, identify potential sources of infeasibility, and offer suggestions to make the model feasible. The implementation of OptiChat is built on GPT-4, which interfaces with an optimization solver to identify the minimal subset of constraints that render the entire optimization problem infeasible, also known as the Irreducible Infeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought, key-retrieve, and sentiment prompts to enhance OptiChat's reliability. Our experiments demonstrate that OptiChat assists both expert and non-expert users in improving their understanding of the optimization models, enabling them to quickly identify the sources of infeasibility.
</details>
<details>
<summary>摘要</summary>
决策问题可以表示为数学优化模型，在经济、工程和生产、运输和医疗等领域找到广泛应用。优化模型是决策问题的数学抽象，它的目的是找到满足一组要求或限制的最佳决策。但现有的优化模型诊断方法通常需要很多背景知识，特别是当模型无法满足所有限制时。在这篇论文中，我们介绍了一种名为OptiChat的自然语言基于系统，它通过自然语言描述优化模型、找到可能导致无法满足限制的来源、并提供改进模型的建议。OptiChat的实现基于GPT-4，它通过与优化解тил器集成来确定整个优化问题的不可能满足子集（IIS）。我们使用了少量学习、专家链条思维、关键检索和情感提示来增强OptiChat的可靠性。我们的实验表明，OptiChat可以帮助专家和非专家用户更好地理解优化模型，快速地找到无法满足限制的来源。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Admissible-Bounds-for-Heuristic-Learning"><a href="#Utilizing-Admissible-Bounds-for-Heuristic-Learning" class="headerlink" title="Utilizing Admissible Bounds for Heuristic Learning"></a>Utilizing Admissible Bounds for Heuristic Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11905">http://arxiv.org/abs/2308.11905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Núñez-Molina, Masataro Asai</li>
<li>for: 这篇论文旨在解决超参论文中的问题，即使用现代机器学习技术学习前进搜索算法的准则，但是这些准则的选择和训练方法尚未得到充分的理论理解。</li>
<li>methods: 这篇论文使用了 truncated Gaussian distributions 作为参数，以便更紧的限制假设空间，从而更好地遵循最大 entropy 原则。</li>
<li>results: 论文的实验结果表明，使用 admissible heuristics 作为参数可以更准确地学习前进搜索算法的准则，并且在训练过程中更快地 converges。<details>
<summary>Abstract</summary>
While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and empirically show that, as a result, it yields more accurate heuristics and converges faster during training.
</details>
<details>
<summary>摘要</summary>
“在最近几年中，使用现代机器学习技术学习前向搜索算法的启发函数Received increasing attention。然而，对于这些启发函数的学习还没有充分的理论理解，包括它们应该学习什么、如何训练它们以及为什么这样做。这导致了一些文献选择不优化的数据集（非最优化成本或非合法启发）和优化指标（例如平方差误差）。此外，由于启发函数的不合法性，对其学习过程中的合法性得 little attention。本文详细介绍了合法启发函数在监督式学习中的角色，并使用 truncated Gaussian distribution 作为参数，这会紧紧地限制假设空间，与普通 Gaussian distribution 相比。我们认为这种数学模型遵循最大Entropy原则，并且在实验中证明这会导致更加准确的启发函数和更快的学习速度。”Note: "Truncated Gaussian distribution" in the text refers to a type of probability distribution that is similar to a Gaussian distribution, but with a truncated range of values.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Data-Perturbation-and-Model-Stabilization-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Rethinking-Data-Perturbation-and-Model-Stabilization-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation"></a>Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11903">http://arxiv.org/abs/2308.11903</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenzhao/dpms">https://github.com/zhenzhao/dpms</a></li>
<li>paper_authors: Zhen Zhao, Ye Liu, Meng Zhao, Di Yin, Yixuan Yuan, Luping Zhou<br>for: 本研究目的是提高 semi-supervised medical image segmentation（SSMIS）的性能。methods: 本研究提出了一种简单 yet effective的方法，称为DPMS，以提高 SSMIS 性能。DPMS 使用了一种 teacher-student 框架，并采用了数据杂化、模型稳定化和损失函数优化等策略来提高 SSMIS 性能。results: DPMS 在公共的 2D ACDC 和 3D LA 数据集上 across 多种 semi-supervised 设定中均获得了新的state-of-the-art 性能，比如在 ACDC 上使用 5% 标签时获得了22.62% 的提升。<details>
<summary>Abstract</summary>
Studies on semi-supervised medical image segmentation (SSMIS) have seen fast progress recently. Due to the limited labelled data, SSMIS methods mainly focus on effectively leveraging unlabeled data to enhance the segmentation performance. However, despite their promising performance, current state-of-the-art methods often prioritize integrating complex techniques and loss terms rather than addressing the core challenges of semi-supervised scenarios directly. We argue that the key to SSMIS lies in generating substantial and appropriate prediction disagreement on unlabeled data. To this end, we emphasize the crutiality of data perturbation and model stabilization in semi-supervised segmentation, and propose a simple yet effective approach to boost SSMIS performance significantly, dubbed DPMS. Specifically, we first revisit SSMIS from three distinct perspectives: the data, the model, and the loss, and conduct a comprehensive study of corresponding strategies to examine their effectiveness. Based on these examinations, we then propose DPMS, which adopts a plain teacher-student framework with a standard supervised loss and unsupervised consistency loss. To produce appropriate prediction disagreements, DPMS perturbs the unlabeled data via strong augmentations to enlarge prediction disagreements considerably. On the other hand, using EMA teacher when strong augmentation is applied does not necessarily improve performance. DPMS further utilizes a forwarding-twice and momentum updating strategies for normalization statistics to stabilize the training on unlabeled data effectively. Despite its simplicity, DPMS can obtain new state-of-the-art performance on the public 2D ACDC and 3D LA datasets across various semi-supervised settings, e.g. obtaining a remarkable 22.62% improvement against previous SOTA on ACDC with 5% labels.
</details>
<details>
<summary>摘要</summary>
研究 semi-supervised medical image segmentation (SSMIS) 在最近几年内已经进步很快。由于受限于标注数据的有限性，SSMIS 方法主要关注使用无标注数据来提高 segmentation 性能。然而，现有的状态态� Armenian 方法经常强调 интегрирование复杂的技术和损失函数而不是直接面对半标注场景的核心挑战。我们认为 semi-supervised 的关键在于生成足够和合适的预测差异。为此，我们强调数据杂化和模型稳定在半标注 segmentation 中的重要性，并提出了一种简单 yet effective 的方法，称为 DPMS。我们从数据、模型和损失三个角度重新探讨 SSMIS，并进行了全面的研究相应的策略的效果。基于这些研究，我们提出了 DPMS，它采用了一种简单的教师-学生框架，并采用标准的supervised损失函数和无标注一致损失函数。为生成足够的预测差异，DPMS 通过强大的杂化来增加预测差异较大。另一方面，在强大杂化应用时，使用 EMA 教师并不一定提高性能。DPMS 还利用了前向 twice 和积分更新策略来normal化统计，以确保在无标注数据上的训练效果。尽管简单，DPMS 可以在各种 semi-supervised 设置中取得新的状态态� Armenian 性能，例如在 ACDC 和 LA 数据集上取得了22.62%的提高。
</details></li>
</ul>
<hr>
<h2 id="Shape-conditioned-3D-Molecule-Generation-via-Equivariant-Diffusion-Models"><a href="#Shape-conditioned-3D-Molecule-Generation-via-Equivariant-Diffusion-Models" class="headerlink" title="Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models"></a>Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11890">http://arxiv.org/abs/2308.11890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Chen, Bo Peng, Srinivasan Parthasarathy, Xia Ning</li>
<li>for:  Identifying novel drug candidates with desired 3D shapes that bind to protein target pockets.</li>
<li>methods: 使用ShapeMol，一种基于ShapeMol的均衡生成模型，将分子表面形状编码成 latent 表示，然后通过这些表示生成3D分子结构。</li>
<li>results: ShapeMol 可以生成 novel, diverse, drug-like 分子，其3D分子结构与给定形状相似。这些结果表明 ShapeMol 可能在设计抗体细胞膜的药物候选者中发挥作用。<details>
<summary>Abstract</summary>
Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.
</details>
<details>
<summary>摘要</summary>
ligand-based drug design的目的是找到与已知活性分子相似的新药候选体。在这篇论文中，我们形ulated an in silico shape-conditioned molecule generation problem，以生成基于给定分子形状的3D分子结构。为解决这个问题，我们开发了一种具有转化和旋转对称的形态响应生成模型ShapeMol。ShapeMol包括一种对称形态编码器，该编码器将分子表面形状映射到缓存中的嵌入式，以及一种对称扩散模型，该模型基于这些嵌入生成3D分子。实验结果表明，ShapeMol可以生成新、多样、药理化的3D分子结构，这些结构与给定形状相似。这些结果表明ShapeMol在设计欲 Bind to protein target pocket的药物候选体中具有潜在的应用价值。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Training-Using-Feedback-Loops"><a href="#Adversarial-Training-Using-Feedback-Loops" class="headerlink" title="Adversarial Training Using Feedback Loops"></a>Adversarial Training Using Feedback Loops</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11881">http://arxiv.org/abs/2308.11881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Haisam Muhammad Rafid, Adrian Sandu</li>
<li>for: 防止深度神经网络受到攻击</li>
<li>methods: 使用控制理论和反馈控制网络</li>
<li>results: 比现状更有效地防止攻击<details>
<summary>Abstract</summary>
Deep neural networks (DNN) have found wide applicability in numerous fields due to their ability to accurately learn very complex input-output relations. Despite their accuracy and extensive use, DNNs are highly susceptible to adversarial attacks due to limited generalizability. For future progress in the field, it is essential to build DNNs that are robust to any kind of perturbations to the data points. In the past, many techniques have been proposed to robustify DNNs using first-order derivative information of the network.   This paper proposes a new robustification approach based on control theory. A neural network architecture that incorporates feedback control, named Feedback Neural Networks, is proposed. The controller is itself a neural network, which is trained using regular and adversarial data such as to stabilize the system outputs. The novel adversarial training approach based on the feedback control architecture is called Feedback Looped Adversarial Training (FLAT). Numerical results on standard test problems empirically show that our FLAT method is more effective than the state-of-the-art to guard against adversarial attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SUMMIT-Source-Free-Adaptation-of-Uni-Modal-Models-to-Multi-Modal-Targets"><a href="#SUMMIT-Source-Free-Adaptation-of-Uni-Modal-Models-to-Multi-Modal-Targets" class="headerlink" title="SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets"></a>SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11880">http://arxiv.org/abs/2308.11880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csimo005/summit">https://github.com/csimo005/summit</a></li>
<li>paper_authors: Cody Simons, Dripta S. Raychaudhuri, Sk Miraj Ahmed, Suya You, Konstantinos Karydis, Amit K. Roy-Chowdhury</li>
<li>for: 这 paper 的目的是解决多Modal 数据的Scene Understanding问题，以便在多种应用中实现自主导航等。</li>
<li>methods: 这 paper 使用了一种 switching 框架，通过自动选择 cross-modal pseudo-label 融合方法（agreement filtering 和 entropy weighting）来适应不同的目标领域。</li>
<li>results: 这 paper 的实验结果表明，该方法可以在七个复杂的适应场景中实现比较好的效果，与有源数据的方法相当或者超过。 Specifically, the improvement in mIoU was up to 12% compared to baseline methods.<details>
<summary>Abstract</summary>
Scene understanding using multi-modal data is necessary in many applications, e.g., autonomous navigation. To achieve this in a variety of situations, existing models must be able to adapt to shifting data distributions without arduous data annotation. Current approaches assume that the source data is available during adaptation and that the source consists of paired multi-modal data. Both these assumptions may be problematic for many applications. Source data may not be available due to privacy, security, or economic concerns. Assuming the existence of paired multi-modal data for training also entails significant data collection costs and fails to take advantage of widely available freely distributed pre-trained uni-modal models. In this work, we relax both of these assumptions by addressing the problem of adapting a set of models trained independently on uni-modal data to a target domain consisting of unlabeled multi-modal data, without having access to the original source dataset. Our proposed approach solves this problem through a switching framework which automatically chooses between two complementary methods of cross-modal pseudo-label fusion -- agreement filtering and entropy weighting -- based on the estimated domain gap. We demonstrate our work on the semantic segmentation problem. Experiments across seven challenging adaptation scenarios verify the efficacy of our approach, achieving results comparable to, and in some cases outperforming, methods which assume access to source data. Our method achieves an improvement in mIoU of up to 12% over competing baselines. Our code is publicly available at https://github.com/csimo005/SUMMIT.
</details>
<details>
<summary>摘要</summary>
场景理解使用多Modal数据是非常重要的，例如自主导航。要在各种情况下实现这一点，现有的模型需要能够适应数据分布的变化，而无需辛苦地注释数据。现有的方法假设源数据在适应过程中可以获得，并且假设源数据是对应的多Modal数据。这两个假设可能会对许多应用程序带来问题。源数据可能因为隐私、安全或经济问题而不可用。假设存在对应的多Modal数据 для训练也会导致极大的数据收集成本，并且不利用广泛可用的免费分布的预训练单Modal模型。在这项工作中，我们松弛了这两个假设，解决了一个独立训练在单Modal数据上的模型，并在目标领域中使用无标记多Modal数据进行适应，无需访问源数据集。我们提出的方法通过自动选择两种补充方法——协调过滤和Entropy质量——根据估计的领域差来解决这个问题。我们在semantic segmentation问题中进行了实验，并在七个困难的适应场景中证明了我们的方法的有效性，与可以访问源数据的方法相比，我们的方法可以达到和在某些情况下超过相同的结果。我们的方法可以提高mIoU指标的改进率达到12%。我们的代码公开可用于https://github.com/csimo005/SUMMIT。
</details></li>
</ul>
<hr>
<h2 id="Cabrita-closing-the-gap-for-foreign-languages"><a href="#Cabrita-closing-the-gap-for-foreign-languages" class="headerlink" title="Cabrita: closing the gap for foreign languages"></a>Cabrita: closing the gap for foreign languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11878">http://arxiv.org/abs/2308.11878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celio Larcher, Marcos Piau, Paulo Finardi, Pedro Gengo, Piero Esposito, Vinicius Caridá</li>
<li>for: 这个研究的目的是提高特定语言或领域上模型的性能，以及解决有效的tokenization问题。</li>
<li>methods: 这个研究使用了自scratch训练模型，并开发了一种名为Cabrita的方法，以解决性能和有效tokenization问题。</li>
<li>results: 研究表明，使用Cabrita方法可以在低成本下提高模型的性能，并且在少量学习任务中达到了与传统连续预训练方法和7B英语预训练模型相当的结果。<details>
<summary>Abstract</summary>
The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.   The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.   To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We believe that this methodology can be applied to any transformer-like architecture model. To validate the study, we conducted continuous pre-training exclusively using Portuguese text on a 3-billion-parameter model known as OpenLLaMA, resulting in a model named openCabrita 3B. The openCabrita 3B also features a new tokenizer that results in a significant reduction in the number of tokens required to represent the text. In our assessment, for few-shot learning tasks, we achieved similar results with this 3B model compared to a traditional continuous pre-training approach as well as to 7B models English pre-trained models.
</details>
<details>
<summary>摘要</summary>
这种训练模型从零开始在特定语言或领域中的策略有两个重要目的：一是提高特定语言或领域上表现，二是确保有效的分词。然而，这种方法的主要局限性在于相关的成本，可能达到六到七位数字的值，具体取决于模型大小和参数的数量。为了缓解这个挑战，我们可以依靠可用的预训练模型，尽管最近的进步，如LLaMA和LLaMA-2模型，仍然在特定领域问题上不够灵活，因为需要大量的 tokens 来表示文本。为了解决这个问题，我们提出了一种方法Named Cabrita，它成功地解决表现和有效的分词问题，并且具有可Affordable的成本。我们认为这种方法可以应用于任何 transformer-like 架构模型。为了验证这种方法的有效性，我们进行了不间断的预训练，只使用葡萄牙语文本，并使用一个3亿参数的模型，称为 OpenLLaMA，从而生成了一个名为 openCabrita 3B 的模型。openCabrita 3B 还有一个新的分词器，它可以减少表示文本所需的 tokens 的数量。在我们的评估中，我们在少量学习任务上达到了与传统的连续预训练方法和英语预训练模型相同的结果，而无需投入大量的时间和资源。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Large-Language-Models-into-the-Debugging-C-Compiler-for-generating-contextual-error-explanations"><a href="#Integrating-Large-Language-Models-into-the-Debugging-C-Compiler-for-generating-contextual-error-explanations" class="headerlink" title="Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations"></a>Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11873">http://arxiv.org/abs/2308.11873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comp1511unsw/dcc">https://github.com/comp1511unsw/dcc</a></li>
<li>paper_authors: Andrew Taylor, Alexandra Vassar, Jake Renzella, Hammond Pearce</li>
<li>for: 这个论文旨在使用大型自然语言模型（LLM）生成改进的编译器错误说明，以便为beginner programmer提供更好的学习体验。</li>
<li>methods: 这个论文使用了LLM生成错误说明，并在Debugging C Compiler（DCC）中实现了这种方法。</li>
<li>results: 经过专家评估，LLM生成的错误说明具有90%的编译时错误概率和75%的运行时错误概率的概念准确性。此外，DCC-help工具已经得到了学生的广泛采用，每周平均有1047次唯一运行，表明使用LLM complement编译器输出可以有助于初学者学习编程。<details>
<summary>Abstract</summary>
This paper introduces a method for Large Language Models (LLM) to produce enhanced compiler error explanations, in simple language, within our Debugging C Compiler (DCC). It is well documented that compiler error messages have been known to present a barrier for novices learning how to program. Although our initial use of DCC in introductory programming (CS1) has been instrumental in teaching C to novice programmers by providing safeguards to commonly occurring errors and translating the usually cryptic compiler error messages at both compile- and run-time, we proposed that incorporating LLM-generated explanations would further enhance the learning experience for novice programmers. Through an expert evaluation, we observed that LLM-generated explanations for compiler errors were conceptually accurate in 90% of compile-time errors, and 75% of run-time errors. Additionally, the new DCC-help tool has been increasingly adopted by students, with an average of 1047 unique runs per week, demonstrating a promising initial assessment of using LLMs to complement compiler output to enhance programming education for beginners. We release our tool as open-source to the community.
</details>
<details>
<summary>摘要</summary>
Through an expert evaluation, we observed that LLM-generated explanations for compiler errors were conceptually accurate in 90% of compile-time errors and 75% of run-time errors. Additionally, the new DCC-help tool has been increasingly adopted by students, with an average of 1047 unique runs per week, demonstrating a promising initial assessment of using LLMs to complement compiler output to enhance programming education for beginners. We release our tool as open-source to the community.
</details></li>
</ul>
<hr>
<h2 id="Fast-Exact-NPN-Classification-with-Influence-aided-Canonical-Form"><a href="#Fast-Exact-NPN-Classification-with-Influence-aided-Canonical-Form" class="headerlink" title="Fast Exact NPN Classification with Influence-aided Canonical Form"></a>Fast Exact NPN Classification with Influence-aided Canonical Form</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12311">http://arxiv.org/abs/2308.12311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhang, Liwei Ni, Jiaxi Zhang, Guojie Luo, Huawei Li, Shenggen Zheng</li>
<li>for: 本文关于NPN类别的研究，具有许多应用在数字电路的合成和验证中。</li>
<li>methods: 本文提出了一种新的 canonical form 和其算法，通过引入Boolean influence来加速NPN类别。</li>
<li>results: 实验结果表明，影响在计算 canonical form 中减少了转换枚举的数量，相比之前的算法在 ABC 中实现的最佳效果，本文的影响帮助 canoncal form 在 exact NPN 类别中获得了5.5倍的速度提升。<details>
<summary>Abstract</summary>
NPN classification has many applications in the synthesis and verification of digital circuits. The canonical-form-based method is the most common approach, designing a canonical form as representative for the NPN equivalence class first and then computing the transformation function according to the canonical form. Most works use variable symmetries and several signatures, mainly based on the cofactor, to simplify the canonical form construction and computation. This paper describes a novel canonical form and its computation algorithm by introducing Boolean influence to NPN classification, which is a basic concept in analysis of Boolean functions. We show that influence is input-negation-independent, input-permutation-dependent, and has other structural information than previous signatures for NPN classification. Therefore, it is a significant ingredient in speeding up NPN classification. Experimental results prove that influence plays an important role in reducing the transformation enumeration in computing the canonical form. Compared with the state-of-the-art algorithm implemented in ABC, our influence-aided canonical form for exact NPN classification gains up to 5.5x speedup.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods"><a href="#KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods" class="headerlink" title="KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods"></a>KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11863">http://arxiv.org/abs/2308.11863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Nzeyimana</li>
<li>for:  This paper aims to improve the robustness of Kinyarwanda speech recognition using self-supervised pre-training, a simple curriculum schedule, and semi-supervised learning.</li>
<li>methods: The approach uses public domain data only, including a new studio-quality speech dataset and a more diverse and noisy public dataset. The model is trained using a simple curriculum schedule and semi-supervised learning.</li>
<li>results: The final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on the Mozilla Common Voice benchmark, which is state-of-the-art to the best of the authors’ knowledge. Additionally, the authors find that syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda.Here is the simplified Chinese text in the format you requested:</li>
<li>for: 这篇论文目的是提高基奈语音识别的稳定性，使用自动预训练、简单的学习级排程和半监督学习。</li>
<li>methods: 方法使用公共领域数据，包括一个新的录音室质量的语音数据集和一个更多样本和噪音的公共数据集。模型使用简单的学习级排程和半监督学习。</li>
<li>results: 最终模型在新数据集上达到3.2%词错率（WER）和Mozilla Common Voice标准测试数据上达到15.9% WER，这是作者们知道的最佳成绩。此外，作者们发现，使用 syllabic 而不是字符基本的分词可以提高基奈语音识别性能。<details>
<summary>Abstract</summary>
Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art to the best of our knowledge. Our experiments also indicate that using syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda.
</details>
<details>
<summary>摘要</summary>
尽管最近有大量的采用Kinyarwanda语音训练数据，但实现Kinyarwanda语音识别仍然是一项挑战。在这项工作中，我们表明使用自我监督预训练、在练习阶段按照简单的课程表进行调整以及使用半监督学习来利用大量未标注的语音数据，可以显著提高Kinyarwanda语音识别性能。我们的方法仅使用公共领域数据。我们收集了一个新的studio质量语音数据集，然后使用这个数据集来训练一个清晰的基线模型。然后，我们使用这个基线模型来排序来自更加多样化和噪音公共数据集的示例，定义了一个简单的课程训练计划。最后，我们应用半监督学习来标注和学习大量未标注的数据，在四个成功的代代中进行四次生成。我们的最终模型在新数据集上达到3.2%的单词错误率（WER）和Mozilla Common Voicebenchmark上达到15.9%的WER，这是我们知道的状态之最。我们的实验还表明，使用音节基于的分词而不是字符基于的分词可以提高Kinyarwanda语音识别性能。
</details></li>
</ul>
<hr>
<h2 id="Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0"><a href="#Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0" class="headerlink" title="Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0"></a>Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11854">http://arxiv.org/abs/2308.11854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anmol Chaure, Ashok Kumar Behera, Sudip Bhattacharya</li>
<li>for: 该研究使用数据驱动机器学习模型作为气候侦测器，以帮助政策制定者做出了解的决策。</li>
<li>methods: 该研究使用机器学习 emulator 作为 computationally heavy GCM simulator 的代理，以降低时间和碳脚印。</li>
<li>results: 研究发现，尽管被视为基本的，回归模型在气候侦测方面具有许多优势。特别是通过内核技术，回归模型可以捕捉复杂的关系，提高预测能力。该研究比较了三种非线性回归模型，其中 Gaussian Process Regressor 表现最佳，但它在空间和时间复杂度方面具有较高的计算资源占用。相比之下，支持向量和 kernel ridge 模型也可以提供竞争力，但需要解决一些交易OFF。此外，我们正在活动调查 composite kernels 和变量推理技术的性能，以进一步提高回归模型的表现和有效地模型复杂非线性现象，包括降水现象。<details>
<summary>Abstract</summary>
Climate projections using data driven machine learning models acting as emulators, is one of the prevailing areas of research to enable policy makers make informed decisions. Use of machine learning emulators as surrogates for computationally heavy GCM simulators reduces time and carbon footprints. In this direction, ClimateBench [1] is a recently curated benchmarking dataset for evaluating the performance of machine learning emulators designed for climate data. Recent studies have reported that despite being considered fundamental, regression models offer several advantages pertaining to climate emulations. In particular, by leveraging the kernel trick, regression models can capture complex relationships and improve their predictive capabilities. This study focuses on evaluating non-linear regression models using the aforementioned dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the best-in-class performance against standard evaluation metrics used for climate field emulation studies. However, Gaussian Process Regression suffers from being computational resource hungry in terms of space and time complexity. Alternatively, Support Vector and Kernel Ridge models also deliver competitive results and but there are certain trade-offs to be addressed. Additionally, we are actively investigating the performance of composite kernels and techniques such as variational inference to further enhance the performance of the regression models and effectively model complex non-linear patterns, including phenomena like precipitation.
</details>
<details>
<summary>摘要</summary>
“气候预测使用数据驱动机器学模型作为模拟器，是当前气候政策决策支持的一个主要领域。使用机器学模型作为计算昂贵GCM模拟器的代理，可以降低时间和碳脚印。在这个方向上，气候Bench（1）是最近筛选的气候机器学模型评估 benchmark dataset。据报道，尽管被视为基本的，但是回归模型在气候模拟方面具有多种优势。具体来说，通过kernel trick，回归模型可以捕捉复杂的关系，提高预测能力。本研究将focus on evaluating non-linear regression models using the above dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the best-in-class performance against standard evaluation metrics used for climate field emulation studies. However, Gaussian Process Regression suffers from being computationally resource-intensive in terms of space and time complexity. Alternatively, Support Vector and Kernel Ridge models also deliver competitive results, but there are certain trade-offs to be addressed. Additionally, we are actively investigating the performance of composite kernels and techniques such as variational inference to further enhance the performance of the regression models and effectively model complex non-linear patterns, including phenomena like precipitation.”Note: The translation is done using Google Translate, and may not be perfect. Please let me know if you need further assistance.
</details></li>
</ul>
<hr>
<h2 id="A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data"><a href="#A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data" class="headerlink" title="A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data"></a>A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11849">http://arxiv.org/abs/2308.11849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Liu, Zhiyuan Lin, Judith Y. T. Wang, Hong Chen</li>
<li>for:  This paper aims to develop a real-time railway rescheduling approach that can automatically adjust the operation schedule in response to time-varying conditions, with a focus on a heavy-demand station upstream of a disrupted area.</li>
<li>methods:  The proposed approach uses mobile data (MD) to infer real-world passenger mobility and a deep reinforcement learning (DRL) framework to determine the optimal reschededuled timetable, route stops, and rolling stock allocation.</li>
<li>results:  The proposed approach can effectively handle challenges such as station overcrowding, rolling stock shortage, open-ended disruption duration, and delays due to detours, while ensuring real-time demand satisfaction, station overcrowding, train capacity utilization, and headway safety.<details>
<summary>Abstract</summary>
Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accumulation of overcrowded passengers at this station, to prevent additional accidents arising from overcrowding. This research addresses the challenges associated with this scenario, including the dynamics of arriving and leaving of passengers, station overcrowding, rolling stock shortage, open-ended disruption duration, integrated rescheduling on multiple routes, and delays due to detours. A deep reinforcement learning (DRL) framework is proposed to determine the optimal rescheduled timetable, route stops, and rolling stock allocation, while considering real-time demand satisfaction, station overcrowding, train capacity utilization, and headway safety.
</details>
<details>
<summary>摘要</summary>
现实时铁路重新安排是一种时间可靠和灵活的技术，可以自动根据时间变化的条件修改运营时间表。现有研究缺乏基于实时旅客流动数据的数据驱动方法，多数依赖于 Origin-Destination（OD）数据和模型基于方法来估算列车需求。同时，长期紧急情况下的时间表更新原则忽视了时间分布的不均。为了填补这一空白，这篇论文提出了一种响应式的需求方法，通过抽象实际旅客流动数据来支持实时重新安排。与传统网络水平方法不同，这篇论文将重点放在一个受到紧急事件影响的重要客运站上，该站位于紧急事件的上游。目标是重新安排经过该站的所有列车，并且在严重紧急事件such as自然灾害发生时进行重新安排。特别是要避免在该站堆积过多的旅客，以避免由过度堆积而引起的额外事故。本研究解决了这种情况中的挑战，包括到站拥堵、列车短缺、开放式紧急事件持续时间、多路线重新安排、延迟due to detours。一种深度强化学习（DRL）框架被提出，以确定最佳重新安排时间表、站点停留、列车分配，同时考虑实时需求满足、站点拥堵、列车容量利用率和队列安全。
</details></li>
</ul>
<hr>
<h2 id="SEA-Shareable-and-Explainable-Attribution-for-Query-based-Black-box-Attacks"><a href="#SEA-Shareable-and-Explainable-Attribution-for-Query-based-Black-box-Attacks" class="headerlink" title="SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks"></a>SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11845">http://arxiv.org/abs/2308.11845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Gao, Ilia Shumailov, Kassem Fawaz</li>
<li>for: 防止机器学习系统面临黑盒攻击的攻击者 profiling 和攻击信息共享。</li>
<li>methods: 基于隐藏марков模型框架，对黑盒攻击进行攻击特征分析和攻击行为描述。</li>
<li>results: 能够准确地识别黑盒攻击，包括第二次攻击，并能够抗击 adaptive 策略和库特定漏洞攻击。<details>
<summary>Abstract</summary>
Machine Learning (ML) systems are vulnerable to adversarial examples, particularly those from query-based black-box attacks. Despite various efforts to detect and prevent such attacks, there is a need for a more comprehensive approach to logging, analyzing, and sharing evidence of attacks. While classic security benefits from well-established forensics and intelligence sharing, Machine Learning is yet to find a way to profile its attackers and share information about them. In response, this paper introduces SEA, a novel ML security system to characterize black-box attacks on ML systems for forensic purposes and to facilitate human-explainable intelligence sharing. SEA leverages the Hidden Markov Models framework to attribute the observed query sequence to known attacks. It thus understands the attack's progression rather than just focusing on the final adversarial examples. Our evaluations reveal that SEA is effective at attack attribution, even on their second occurrence, and is robust to adaptive strategies designed to evade forensics analysis. Interestingly, SEA's explanations of the attack behavior allow us even to fingerprint specific minor implementation bugs in attack libraries. For example, we discover that the SignOPT and Square attacks implementation in ART v1.14 sends over 50% specific zero difference queries. We thoroughly evaluate SEA on a variety of settings and demonstrate that it can recognize the same attack's second occurrence with 90+% Top-1 and 95+% Top-3 accuracy.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）系统容易受到反面示例攻击，特别是来自查询基本黑盒攻击。尽管有各种努力检测和防止这些攻击，但是还是需要一种更全面的方法来记录、分析和分享攻击证据。 Classic security 受益于良好的遗产安全和知识共享，而 Machine Learning 尚未找到一种 profiling 攻击者并分享关于他们的信息的方法。因此，这篇论文介绍了 SEA，一种新的 ML 安全系统，用于 caracterize 黑盒攻击 ML 系统的攻击行为，以便进行审计和人类可解释的知识分享。SEA 基于隐藏markov模型框架，将观察到的查询序列归因于已知的攻击。因此，它可以理解攻击的进程，而不仅仅是关注最终的反面示例。我们的评估表明，SEA 可以有效地归因攻击，甚至在第二次出现时，并且具有鲁棒性，可以抵御适应攻击的策略。另外，SEA 的解释也可以让我们发现特定的小型实现漏洞。例如，我们发现ART v1.14中的SignOPT和方块攻击实现中发送了50%以上的特定零差查询。我们对 SEA 进行了多种设置的测试，并证明它可以在不同的设置下识别同一个攻击的第二次出现，Top-1 和 Top-3 准确率高于 90% 和 95%。
</details></li>
</ul>
<hr>
<h2 id="rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning"></a>${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11842">http://arxiv.org/abs/2308.11842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dchen48/e3ac">https://github.com/dchen48/e3ac</a></li>
<li>paper_authors: Dingyang Chen, Qi Zhang</li>
<li>for: 这篇论文的目的是解决协同多体学习（MARL）问题中的几何 symmetries问题，并通过嵌入几何约束来提高多体actor-critic方法的性能。</li>
<li>methods: 该论文使用了形式化 caracterizing a subclass of Markov games with a general notion of symmetries，并设计了具有几何约束的神经网络架构。</li>
<li>results: 该论文的实验结果表明，在各种协同MARL benchmark中，具有几何约束的神经网络架构可以获得更高的性能，并且具有很好的总结和传播学习能力。<details>
<summary>Abstract</summary>
Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns. The code is available at: https://github.com/dchen48/E3AC.
</details>
<details>
<summary>摘要</summary>
“对自然世界中的对称图案进行识别和分析，对于不同科学领域的发现做出了重要贡献，例如物理学中的重力法则和化学结构的研究。在这篇文章中，我们专注于利用多客户多代理人学习（MARL）问题中的对称性，并将其应用到许多实际应用中。我们首先正式定义了一个泛化的Markov游戏，其中包含一般的对称性，并证明了存在对称优值和策略。这些对称性导致我们设计了具有对称约束的神经网络架构，并将其作为多代理人actor-critic方法的类别偏好。这种偏好导致了在多个合作MARL测试中的出色表现，包括零shot学习和转移学习在未见过的对称图案中。代码可以在以下GitHub页面上获取：https://github.com/dchen48/E3AC。”
</details></li>
</ul>
<hr>
<h2 id="A-Survey-for-Federated-Learning-Evaluations-Goals-and-Measures"><a href="#A-Survey-for-Federated-Learning-Evaluations-Goals-and-Measures" class="headerlink" title="A Survey for Federated Learning Evaluations: Goals and Measures"></a>A Survey for Federated Learning Evaluations: Goals and Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11841">http://arxiv.org/abs/2308.11841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Chai, Leye Wang, Liu Yang, Junxue Zhang, Kai Chen, Qiang Yang</li>
<li>for: 评估 federated learning（FL）系统的目的是寻求一种系统atic approach to assessing how well a system achieves its intended purpose.</li>
<li>methods: 评估FL的方法包括评估utilities, efficiency,和security的多种指标。</li>
<li>results: 评估FL的结果包括提出了一种开源平台FedEval，以及描述了FL评估的一些挑战和未来研究方向。<details>
<summary>Abstract</summary>
Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval, an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.
</details>
<details>
<summary>摘要</summary>
评估是一种系统的方法，用于评估系统是否实现其意图的目的。联邦学习（FL）是一种新的隐私保护机器学习方法，允许多方共同训练模型而无需分享敏感数据。然而，评估FL具有多种目标和多学科性质，如有用性、效率和安全性。在这篇简介中，我们首先评审了现有研究中采用的主要评估目标，然后探讨每个目标的评估指标。我们还介绍了FedEval，一个开源的评估平台，为FL算法提供标准化和完整的评估框架，包括其有用性、效率和安全性。最后，我们讨论了一些挑战和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="A-Benchmark-Study-on-Calibration"><a href="#A-Benchmark-Study-on-Calibration" class="headerlink" title="A Benchmark Study on Calibration"></a>A Benchmark Study on Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11838">http://arxiv.org/abs/2308.11838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/history1">https://github.com/Aryia-Behroziuan/history1</a></li>
<li>paper_authors: Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, Chang Xu</li>
<li>for: This paper aims to explore calibration properties within Neural Architecture Search (NAS) and provide a comprehensive dataset for studying calibration issues.</li>
<li>methods: The paper uses the NAS search space to create a model calibration dataset, which evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks.</li>
<li>results: The study aims to answer several longstanding questions in the field, including whether model calibration can be generalized across different tasks, whether robustness can be used as a calibration measurement, and how calibration interacts with accuracy. The paper also explores the impact of bin size on calibration measurement and which architectural designs are beneficial for calibration.<details>
<summary>Abstract</summary>
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can robustness be used as a calibration measurement? (iii) How reliable are calibration metrics? (iv) Does a post-hoc calibration method affect all models uniformly? (v) How does calibration interact with accuracy? (vi) What is the impact of bin size on calibration measurement? (vii) Which architectural designs are beneficial for calibration? Additionally, our study bridges an existing gap by exploring calibration within NAS. By providing this dataset, we enable further research into NAS calibration. As far as we are aware, our research represents the first large-scale investigation into calibration properties and the premier study of calibration issues within NAS.
</details>
<details>
<summary>摘要</summary>
深度神经网络在不同机器学习任务中越来越广泛应用，然而随着模型复杂度的增加，它们经常面临调整问题，即使提高预测精度。许多研究已努力改进调整性能，使用数据预处理、特定损失函数和训练框架等方法。然而，调整性质的研究尚未得到充分的关注。我们的研究利用神经网络搜索（NAS）搜索空间，提供了详细的模型建筑空间，以便对调整性质进行全面的探索。我们专门创建了模型调整数据集，该数据集评估了90个分割值和12个附加调整测量，在117,702个Unique神经网络内进行了广泛的 NATS-Bench 搜索空间。我们的分析旨在回答以下长期问题：(i) 模型调整是否可以泛化到不同任务？(ii) 是否可以将Robustness作为调整测量？(iii) 准确性测试是如何可靠？(iv) 后期调整方法对所有模型是否具有同等影响？(v) 调整与准确度之间是否存在关系？(vi) 分割值如何影响调整测量？(vii) 哪些建筑设计有利于调整？我们的研究填补了现有的空白，通过调整在NAS中进行exploration。我们提供了这个数据集，以便进一步研究NAS调整性质。我们知道，我们的研究是首次对调整性质进行大规模的探索，也是首次在NAS中研究调整问题。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity"><a href="#Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity" class="headerlink" title="Characterizing normal perinatal development of the human brain structural connectivity"></a>Characterizing normal perinatal development of the human brain structural connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11836">http://arxiv.org/abs/2308.11836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Wu, Lana Vasung, Camilo Calixto, Ali Gholipour, Davood Karimi<br>for: 这项研究的目的是为了研究新生儿期的脑发育中的结构连接组织的发展趋势。methods: 这项研究使用了基于空间-时间平均值的计算框架，以确定正常发育阶段的结构连接指标。results: 研究发现，在新生儿期的33-44周岁期间，脑的结构连接显示了明显的增长趋势，全球和地方效率增加，特征路径长度减少，脑叶和脑半球之间的连接强化广泛。此外，研究还发现了一些差异征在不同的连接评价方法中的一致性。这些结果有助于评价新生儿期脑发育中的正常和异常发展。<details>
<summary>Abstract</summary>
Early brain development is characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain's cognitive abilities and influences its response to diseases and environmental factors. Hence, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determining such baselines. We used this framework to analyze the structural connectivity between 33 and 44 postmenstrual weeks using data from 166 subjects. Our results unveiled clear and strong trends in the development of structural connectivity in perinatal stage. Connection weighting based on fractional anisotropy and neurite density produced the most consistent results. We observed increases in global and local efficiency, a decrease in characteristic path length, and widespread strengthening of the connections within and across brain lobes and hemispheres. We also observed asymmetry patterns that were consistent between different connection weighting approaches. The new computational method and results are useful for assessing normal and abnormal development of the structural connectome early in life.
</details>
<details>
<summary>摘要</summary>
early brain development Characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain's cognitive abilities and influences its response to diseases and environmental factors. Therefore, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determining such baselines. We used this framework to analyze the structural connectivity between 33 and 44 postmenstrual weeks using data from 166 subjects. Our results unveiled clear and strong trends in the development of structural connectivity in perinatal stage. Connection weighting based on fractional anisotropy and neurite density produced the most consistent results. We observed increases in global and local efficiency, a decrease in characteristic path length, and widespread strengthening of the connections within and across brain lobes and hemispheres. We also observed asymmetry patterns that were consistent between different connection weighting approaches. The new computational method and results are useful for assessing normal and abnormal development of the structural connectome early in life.
</details></li>
</ul>
<hr>
<h2 id="Performance-Comparison-and-Implementation-of-Bayesian-Variants-for-Network-Intrusion-Detection"><a href="#Performance-Comparison-and-Implementation-of-Bayesian-Variants-for-Network-Intrusion-Detection" class="headerlink" title="Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection"></a>Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11834">http://arxiv.org/abs/2308.11834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tosin Ige, Christopher Kiekintveld</li>
<li>for: 本研究旨在实现和比较每种 variant of Bayesian classifier（多omial、Bernoulli、Gaussian）在网络入侵异常检测中的性能，并investigate whether there is any association between each variant assumption and their performance.</li>
<li>methods: 本研究采用了不同 variant of Bayesian classifier，并对它们的性能进行了比较。</li>
<li>results: 实验结果表明，Bernoulli variant 的准确率为 69.9%（71% 训练数据），Multinomial variant 的准确率为 31.2%（31.2% 训练数据），而 Gaussian variant 的准确率为 81.69%（82.84% 训练数据）。进一步调查发现，每种 Naive Bayes variant 的性能和准确率主要归功于它们的假设。Gaussian classifier 的好性能主要归功于它假设特征遵循正态分布，而 Multinomial classifier 的差的性能主要归功于它假设特征遵循离散和多omial分布。<details>
<summary>Abstract</summary>
Bayesian classifiers perform well when each of the features is completely independent of the other which is not always valid in real world application. The aim of this study is to implement and compare the performances of each variant of Bayesian classifier (Multinomial, Bernoulli, and Gaussian) on anomaly detection in network intrusion, and to investigate whether there is any association between each variant assumption and their performance. Our investigation showed that each variant of Bayesian algorithm blindly follows its assumption regardless of feature property, and that the assumption is the single most important factor that influences their accuracy. Experimental results show that Bernoulli has accuracy of 69.9% test (71% train), Multinomial has accuracy of 31.2% test (31.2% train), while Gaussian has accuracy of 81.69% test (82.84% train). Going deeper, we investigated and found that each Naive Bayes variants performances and accuracy is largely due to each classifier assumption, Gaussian classifier performed best on anomaly detection due to its assumption that features follow normal distributions which are continuous, while multinomial classifier have a dismal performance as it simply assumes discreet and multinomial distribution.
</details>
<details>
<summary>摘要</summary>
bayesian 分类器在实际应用中表现良好，只当每个特征独立无关其他特征时。本研究的目的是实现和比较bayesian分类器（多ategorical、bernoulli和gaussian）在网络侵入异常检测中的表现，以及每种variant assumption和其表现之间的关系。我们的调查表明，bayesian算法的每种变体都会遵循自己的假设，不管特征性质如何。实验结果显示，bernoulli的准确率为69.9%测试（71%训练），多ategorical的准确率为31.2%测试（31.2%训练），而gaussian的准确率为81.69%测试（82.84%训练）。进一步调查发现，每种naive bayes variant的表现和准确率主要归结于每个分类器假设，gaussian分类器在异常检测中表现最佳，因为它假设特征遵循正态分布，而多ategorical分类器表现很差，因为它只假设离散和多ategorical分布。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test"><a href="#Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test" class="headerlink" title="Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test"></a>Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11827">http://arxiv.org/abs/2308.11827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saba Rahimi, Tucker Balch, Manuela Veloso</li>
<li>for: 提高 GPT 模型的问答能力，使其能够回答基于新的信息源的问题。</li>
<li>methods: 使用Context embedding、构建提问、使用 GPT 模型答题。</li>
<li>results: GPT-3 模型在一个控制测试场景中，使用加利福尼亚驾驶手册作为信息源，答题成功率为 96%，而无Context情况下答题成功率仅为 82%。但模型仍然无法正确回答一些问题，表明进一步改进是需要的。<details>
<summary>Abstract</summary>
Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver's Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model's passing score fell to 82%. However, the model still fails to answer some questions correctly even with providing library of context, highlighting room for improvement. The research also examined the impact of prompt length and context format, on the model's performance. Overall, the study provides insights into the limitations and potential improvements for GPT models in question-answering tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accel-GCN-High-Performance-GPU-Accelerator-Design-for-Graph-Convolution-Networks"><a href="#Accel-GCN-High-Performance-GPU-Accelerator-Design-for-Graph-Convolution-Networks" class="headerlink" title="Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks"></a>Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11825">http://arxiv.org/abs/2308.11825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiexi1990/iccad-accel-gnn">https://github.com/xiexi1990/iccad-accel-gnn</a></li>
<li>paper_authors: Xi Xie, Hongwu Peng, Amit Hasan, Shaoyi Huang, Jiahui Zhao, Haowen Fang, Wei Zhang, Tong Geng, Omer Khan, Caiwen Ding</li>
<li>for: 提高GCNs在主流GPU上的加速，解决工作负荷不均和内存访问不规则的挑战。</li>
<li>methods: 使用轻量级度排序、块级别分区策略和共享内存地方的合并战略来提高分布式内存和计算并行性。</li>
<li>results: 对18个 benchmark图进行评估，与cuSPARSE、GNNAdvisor和graph-BLAST相比，Accel-GCN可以提高GCN的计算效率，提高多级内存效率，并且可以提高内存带宽利用率。<details>
<summary>Abstract</summary>
Graph Convolutional Networks (GCNs) are pivotal in extracting latent information from graph data across various domains, yet their acceleration on mainstream GPUs is challenged by workload imbalance and memory access irregularity. To address these challenges, we present Accel-GCN, a GPU accelerator architecture for GCNs. The design of Accel-GCN encompasses: (i) a lightweight degree sorting stage to group nodes with similar degree; (ii) a block-level partition strategy that dynamically adjusts warp workload sizes, enhancing shared memory locality and workload balance, and reducing metadata overhead compared to designs like GNNAdvisor; (iii) a combined warp strategy that improves memory coalescing and computational parallelism in the column dimension of dense matrices.   Utilizing these principles, we formulated a kernel for sparse matrix multiplication (SpMM) in GCNs that employs block-level partitioning and combined warp strategy. This approach augments performance and multi-level memory efficiency and optimizes memory bandwidth by exploiting memory coalescing and alignment. Evaluation of Accel-GCN across 18 benchmark graphs reveals that it outperforms cuSPARSE, GNNAdvisor, and graph-BLAST by factors of 1.17 times, 1.86 times, and 2.94 times respectively. The results underscore Accel-GCN as an effective solution for enhancing GCN computational efficiency.
</details>
<details>
<summary>摘要</summary>
图像卷积网络（GCNs）在各种领域中提取隐藏信息的缺省方法，但是它们在主流GPU上的加速受到工作负载不均和内存访问不规则的挑战。为了解决这些挑战，我们提出了Accel-GCN，一种为GCNs而设计的GPU加速架构。Accel-GCN的设计包括：（i）一种轻量级学位排序阶段，用于将节点按照学位相似度分组；（ii）一种基于块的分区策略，动态调整抗冲工作负载大小，提高共享内存本地性和工作负载均衡，并比GNNAdvisor等设计减少metadata开销；（iii）一种组合战略，用于改进内存启聚和计算并行性在纵向 dense 矩阵中。通过这些原则，我们设计了GCNs中的稀疏矩阵乘法（SpMM）的适用，并使用块级分区和组合战略。这种方法可以提高性能和多级内存效率，同时利用内存启聚和对齐来提高内存带宽。对Accel-GCN在18个标准图表上进行评估，发现它在cuSPARSE、GNNAdvisor和graph-BLAST等方法上提高了1.17倍、1.86倍和2.94倍的性能。结果证明Accel-GCN是GCN计算效率的有效解决方案。
</details></li>
</ul>
<hr>
<h2 id="PatchBackdoor-Backdoor-Attack-against-Deep-Neural-Networks-without-Model-Modification"><a href="#PatchBackdoor-Backdoor-Attack-against-Deep-Neural-Networks-without-Model-Modification" class="headerlink" title="PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification"></a>PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11822">http://arxiv.org/abs/2308.11822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xaiveryuan/patchbackdoor">https://github.com/xaiveryuan/patchbackdoor</a></li>
<li>paper_authors: Yizhen Yuan, Rui Kong, Shenghao Xie, Yuanchun Li, Yunxin Liu</li>
<li>for: 防止深度学习系统中的攻击，特别是在安全关键场景中。</li>
<li>methods: 提议使用一个特制的贴图（称为backdoor patch），将其放置在摄像头前面，并将其与输入图像一起传递给模型。</li>
<li>results: 在实验中，该攻击可以在93%到99%的场景中成功，并且在实际应用中也能够实现攻击。<details>
<summary>Abstract</summary>
Backdoor attack is a major threat to deep learning systems in safety-critical scenarios, which aims to trigger misbehavior of neural network models under attacker-controlled conditions. However, most backdoor attacks have to modify the neural network models through training with poisoned data and/or direct model editing, which leads to a common but false belief that backdoor attack can be easily avoided by properly protecting the model. In this paper, we show that backdoor attacks can be achieved without any model modification. Instead of injecting backdoor logic into the training data or the model, we propose to place a carefully-designed patch (namely backdoor patch) in front of the camera, which is fed into the model together with the input images. The patch can be trained to behave normally at most of the time, while producing wrong prediction when the input image contains an attacker-controlled trigger object. Our main techniques include an effective training method to generate the backdoor patch and a digital-physical transformation modeling method to enhance the feasibility of the patch in real deployments. Extensive experiments show that PatchBackdoor can be applied to common deep learning models (VGG, MobileNet, ResNet) with an attack success rate of 93% to 99% on classification tasks. Moreover, we implement PatchBackdoor in real-world scenarios and show that the attack is still threatening.
</details>
<details>
<summary>摘要</summary>
迷宫攻击是深度学习系统的主要威胁，它目的是在攻击者控制的情况下让神经网络模型发生不正常的行为。然而，大多数迷宫攻击都需要修改神经网络模型通过调询毒品数据和/或直接模型修改，这导致了一个常见而又错误的信念，即迷宫攻击可以轻松避免通过正确保护模型。在这篇论文中，我们显示了迷宫攻击可以不需要修改模型。相反的，我们提议在前置摄像头前置一个特别设计的贴纸（称为迷宫贴纸），这个贴纸会在输入图像中扮演一个正常的角色，但当输入图像包含攻击者控制的触发物时，它会制造错误的预测。我们的主要技术包括生成迷宫贴纸的有效训练方法和将贴纸模型转换为实际应用中的实际实现方法。实验结果显示，PatchBackdoor可以在常用的深度学习模型（VGG、MobileNet、ResNet）上获得93%~99%的攻击成功率，而且我们在实际应用中实现了这个攻击。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Health-Disparity-on-Biased-Electronic-Health-Records-via-Deconfounder"><a href="#Mitigating-Health-Disparity-on-Biased-Electronic-Health-Records-via-Deconfounder" class="headerlink" title="Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder"></a>Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11819">http://arxiv.org/abs/2308.11819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Liu, Xiaohan Li, Philip Yu<br>for: The paper aims to address the fairness issue in clinical data modeling, particularly in Electronic Health Records (EHRs), by proposing a novel model called Fair Longitudinal Medical Deconfounder (FLMD).methods: FLMD employs a two-stage training process, which includes capturing unobserved confounders for each encounter and combining the learned latent representation with other relevant features to make predictions. The model incorporates appropriate fairness criteria, such as counterfactual fairness, to ensure high prediction accuracy while minimizing health disparities.results: The paper demonstrates the effectiveness of FLMD through comprehensive experiments on two real-world EHR datasets. The results show that FLMD outperforms baseline methods and FLMD variants in terms of fairness and accuracy, and provides valuable insights into its capabilities across different settings.<details>
<summary>Abstract</summary>
The fairness issue of clinical data modeling, especially on Electronic Health Records (EHRs), is of utmost importance due to EHR's complex latent structure and potential selection bias. It is frequently necessary to mitigate health disparity while keeping the model's overall accuracy in practice. However, traditional methods often encounter the trade-off between accuracy and fairness, as they fail to capture the underlying factors beyond observed data. To tackle this challenge, we propose a novel model called Fair Longitudinal Medical Deconfounder (FLMD) that aims to achieve both fairness and accuracy in longitudinal Electronic Health Records (EHR) modeling. Drawing inspiration from the deconfounder theory, FLMD employs a two-stage training process. In the first stage, FLMD captures unobserved confounders for each encounter, which effectively represents underlying medical factors beyond observed EHR, such as patient genotypes and lifestyle habits. This unobserved confounder is crucial for addressing the accuracy/fairness dilemma. In the second stage, FLMD combines the learned latent representation with other relevant features to make predictions. By incorporating appropriate fairness criteria, such as counterfactual fairness, FLMD ensures that it maintains high prediction accuracy while simultaneously minimizing health disparities. We conducted comprehensive experiments on two real-world EHR datasets to demonstrate the effectiveness of FLMD. Apart from the comparison of baseline methods and FLMD variants in terms of fairness and accuracy, we assessed the performance of all models on disturbed/imbalanced and synthetic datasets to showcase the superiority of FLMD across different settings and provide valuable insights into its capabilities.
</details>
<details>
<summary>摘要</summary>
“诊断资料模型中的公平问题，尤其是在电子健康记录（EHR）上，是非常重要的，因为EHR具有复杂的潜在结构和选择偏见。在实践中，通常需要实现健康差异化和模型绩效的平衡。然而，传统方法通常会面临精度和公平之间的贸易，因为它们无法捕捉潜在的因素。为了解决这个挑战，我们提出了一个新的模型，即公平长期医疗资料去掉条件（FLMD），旨在在长期EHR模型中同时维持精度和公平。参考了去掉条件理论，FLMD使用了两阶段训练过程。在第一阶段，FLMD捕捉了每次遇到的隐藏因素，这些隐藏因素代表了背后的医疗因素，例如患者基因和生活方式。这个隐藏因素是精度和公平问题的解决方案。在第二阶段，FLMD结合了学习的潜在表示和其他相关特征来进行预测。通过将应用公平问题的标准加入训练过程，FLMD确保了高精度和同时维持健康差异化。我们在两个真实世界EHR数据集上进行了充分的实验，以证明FLMD的有效性。除了与基eline方法和FLMD的变型进行公平和精度的比较外，我们还评估了所有模型在离散/不均衡和合成数据集上的表现，以显示FLMD在不同的设定下的优势和提供有价值的问题。”
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Nonlocal-Traffic-Flow-Model-in-Physics-informed-Neural-Networks"><a href="#Incorporating-Nonlocal-Traffic-Flow-Model-in-Physics-informed-Neural-Networks" class="headerlink" title="Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks"></a>Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11818">http://arxiv.org/abs/2308.11818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Archie J. Huang, Animesh Biswas, Shaurya Agarwal</li>
<li>for: 提高交通状态估算精度，提高交通管理策略效果。</li>
<li>methods: 基于非本地LWR模型的物理学习深度学习框架。</li>
<li>results: 比基eline方法具有更高的准确率和可靠性，能够更好地估算交通状态，提高交通管理策略的效果。<details>
<summary>Abstract</summary>
This research contributes to the advancement of traffic state estimation methods by leveraging the benefits of the nonlocal LWR model within a physics-informed deep learning framework. The classical LWR model, while useful, falls short of accurately representing real-world traffic flows. The nonlocal LWR model addresses this limitation by considering the speed as a weighted mean of the downstream traffic density. In this paper, we propose a novel PIDL framework that incorporates the nonlocal LWR model. We introduce both fixed-length and variable-length kernels and develop the required mathematics. The proposed PIDL framework undergoes a comprehensive evaluation, including various convolutional kernels and look-ahead windows, using data from the NGSIM and CitySim datasets. The results demonstrate improvements over the baseline PIDL approach using the local LWR model. The findings highlight the potential of the proposed approach to enhance the accuracy and reliability of traffic state estimation, enabling more effective traffic management strategies.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "physics-informed deep learning" is translated as "物理学推动的深度学习" (wùyì xué yùn dào)* "nonlocal" is translated as "非本地" (fēn běn dì)* "LWR" is translated as "流速模型" (liú sù módel)* "speed" is translated as "速度" (zhòu du)* "density" is translated as "密度" (mì dè)* "kernel" is translated as "核函数" (fāng xiàng)* "look-ahead windows" is translated as "预测窗口" (yù jì chuāng kōng)* "baseline" is translated as "基线" (jī xiào)* "accuracy" is translated as "准确性" (zhèng qiú xìng)* "reliability" is translated as "可靠性" (kě liào xìng)* "traffic management strategies" is translated as "交通管理策略" (tiáo tòng guǎn lí zhì lüè)
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-Deep-Neural-Operator-Models-toward-Ocean-Forecasting"><a href="#Evaluation-of-Deep-Neural-Operator-Models-toward-Ocean-Forecasting" class="headerlink" title="Evaluation of Deep Neural Operator Models toward Ocean Forecasting"></a>Evaluation of Deep Neural Operator Models toward Ocean Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11814">http://arxiv.org/abs/2308.11814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ellery Rajagopal, Anantha N. S. Babu, Tony Ryu, Patrick J. Haley Jr., Chris Mirabito, Pierre F. J. Lermusiaux</li>
<li>for:  investigate the possible effectiveness of deep neural operator models for reproducing and predicting classic fluid flows and simulations of realistic ocean dynamics</li>
<li>methods:  trained on a simulated two-dimensional fluid flow past a cylinder, and applied to forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay</li>
<li>results:  predicted idealized periodic eddy shedding, and showed some skill in predicting features of realistic ocean surface flows, with potential for future research and applications.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究探讨了深度学习运算符模型在复现和预测古典流体流和真实海洋动力学中的可能效果。</li>
<li>methods: 使用了一个模拟的两维流体流 past a cylinder 进行训练，并应用于forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay。</li>
<li>results: 训练后的深度学习运算符模型能够预测理想化的 periodic eddy shedding，并对真实海洋表面流体中的一些特征显示了一定的能力，具有未来研究和应用的潜在价值。<details>
<summary>Abstract</summary>
Data-driven, deep-learning modeling frameworks have been recently developed for forecasting time series data. Such machine learning models may be useful in multiple domains including the atmospheric and oceanic ones, and in general, the larger fluids community. The present work investigates the possible effectiveness of such deep neural operator models for reproducing and predicting classic fluid flows and simulations of realistic ocean dynamics. We first briefly evaluate the capabilities of such deep neural operator models when trained on a simulated two-dimensional fluid flow past a cylinder. We then investigate their application to forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay, learning from high-resolution data-assimilative simulations employed for real sea experiments. We confirm that trained deep neural operator models are capable of predicting idealized periodic eddy shedding. For realistic ocean surface flows and our preliminary study, they can predict several of the features and show some skill, providing potential for future research and applications.
</details>
<details>
<summary>摘要</summary>
“数据驱动、深度学习模型框架最近已经开发出来用于时间序列预测。这些机器学习模型可能在多个领域有用，包括大气和海洋领域，以及整体更大的液体社区。本工作研究了这些深度神经算法模型在复现和预测经典液体流和现实海洋动力学的能力。我们首先简要评估了这些深度神经算法模型在模拟的二维液体流 past 筒体上的能力。然后，我们研究了它们在中大西洋盆地和马萨诸塞湾 ocean surface 流动预测方面的应用，学习从高分辨率数据吸收式 simulations 中得到的实际海洋实验。我们发现，训练过的深度神经算法模型能够预测理想化 periodic eddy shedding。对于实际的海洋表面流动和我们的初步研究，它们可以预测一些特征，并且显示一定的能力，提供未来研究和应用的潜在可能。”
</details></li>
</ul>
<hr>
<h2 id="Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings"><a href="#Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings" class="headerlink" title="Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings"></a>Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11804">http://arxiv.org/abs/2308.11804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugene Bagdasaryan, Vitaly Shmatikov<br>for: 这篇论文主要针对的是多modal编码器的安全性问题，具体来说是对于图像、声音、文本、视频等多种模式的映射。methods: 该论文使用了多modal编码器，将不同模式的输入映射到单一的嵌入空间中，以实现模式之间的对应。results: 该论文发现，使用多modal编码器可能会导致”对抗幻觉”攻击，即对于任意输入模式，恶意攻击者可以将其嵌入空间中的映射与另一个模式的映射很近，从而实现模式之间的对应。这种攻击不依赖于特定任务的知识，因此可以影响多种下游任务。使用ImageBind embeddings，研究者示出了这种攻击的具体实现方式，并证明了它们可以在图像生成、文本生成和零参数分类等多种任务中引起混乱。<details>
<summary>Abstract</summary>
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.   Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
</details>
<details>
<summary>摘要</summary>
多modalEncoder将图像、声音、文本、视频等多种模式映射到单一的嵌入空间中，使模式之间的表示相似（例如，将一个狗图片与一个喊喊的声音相关联）。我们表明，多modal嵌入可能受到“攻击ILLUSION”的威胁。给定任意模式的输入，恶意者可以对其进行扰动，使其嵌入 Close to恶意者选择的另一种模式的输入。这些ILLUSION依据嵌入空间的 proximity，并不受下游任务的限制。使用ImageBind嵌入，我们示例了如何通过不知道特定下游任务的知识，使用恶意对齐的输入， Mislead图像生成、文本生成和零shot分类。
</details></li>
</ul>
<hr>
<h2 id="Variational-Density-Propagation-Continual-Learning"><a href="#Variational-Density-Propagation-Continual-Learning" class="headerlink" title="Variational Density Propagation Continual Learning"></a>Variational Density Propagation Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11801">http://arxiv.org/abs/2308.11801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Angelini, Nidhal Bouaynaya, Ghulam Rasool<br>for: 这篇论文的目的是提出一个框架来适应资料分布迁移，并使用 uncertainty quantification from Bayesian Inference 来减少严重遗传。methods: 这篇论文使用了 Continual Learning 的方法，并开发了一个基于 Bayesian Inference 的方法，以便获得更好的 uncertainty quantification。这个方法不需要 Monte Carlo 抽样，而是使用关键矩阵来实现预测分布的近似。results: 这篇论文的结果显示，使用这个方法可以对抗严重遗传，并且可以在多个 benchmark 数据集上进行适应资料分布迁移。此外，这个方法还可以在多个不同的任务序列长度下进行任务增量学习。总之，这篇论文的结果显示，这个方法可以实现一个具有最小化模型复杂度的 Continual Learning 框架。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) deployed to the real world are regularly subject to out-of-distribution (OoD) data, various types of noise, and shifting conceptual objectives. This paper proposes a framework for adapting to data distribution drift modeled by benchmark Continual Learning datasets. We develop and evaluate a method of Continual Learning that leverages uncertainty quantification from Bayesian Inference to mitigate catastrophic forgetting. We expand on previous approaches by removing the need for Monte Carlo sampling of the model weights to sample the predictive distribution. We optimize a closed-form Evidence Lower Bound (ELBO) objective approximating the predictive distribution by propagating the first two moments of a distribution, i.e. mean and covariance, through all network layers. Catastrophic forgetting is mitigated by using the closed-form ELBO to approximate the Minimum Description Length (MDL) Principle, inherently penalizing changes in the model likelihood by minimizing the KL Divergence between the variational posterior for the current task and the previous task's variational posterior acting as the prior. Leveraging the approximation of the MDL principle, we aim to initially learn a sparse variational posterior and then minimize additional model complexity learned for subsequent tasks. Our approach is evaluated for the task incremental learning scenario using density propagated versions of fully-connected and convolutional neural networks across multiple sequential benchmark datasets with varying task sequence lengths. Ultimately, this procedure produces a minimally complex network over a series of tasks mitigating catastrophic forgetting.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在实际世界中部署时常遇到不同类型的噪音、扰动目标和数据分布逐渐变化的问题。这篇论文提出了一种基于Continual Learning benchmarck datasets的数据分布演变适应框架。我们开发了一种基于uncertainty量化的Continual Learning方法，利用束缚推理来减少忘却现象。我们在之前的方法上进一步改进，即不需要蒙те Carlo样本来采样模型权重，以便采样预测分布。我们优化了一个关闭式证据下界（ELBO）目标，通过对所有神经网络层传递mean和covariance两个分布的首两个矩阵来近似预测分布。通过使用关闭式ELBO目标，我们可以近似地使用最小描述长度（MDL）原理，从而自然地减少模型概率变化的KL散度，以避免忘却现象。我们利用近似MDL原理，首先学习一个稀疏的变量 posterior，然后进一步减少后续任务学习的模型复杂度。我们的方法在完全链接神经网络和卷积神经网络上进行了多个顺序 benchmark 数据集上进行了评估，并最终生成了一个对多个任务具有最小复杂度的神经网络，以避免忘却现象。
</details></li>
</ul>
<hr>
<h2 id="Complex-valued-neural-networks-for-voice-anti-spoofing"><a href="#Complex-valued-neural-networks-for-voice-anti-spoofing" class="headerlink" title="Complex-valued neural networks for voice anti-spoofing"></a>Complex-valued neural networks for voice anti-spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11800">http://arxiv.org/abs/2308.11800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas M. Müller, Philip Sperl, Konstantin Böttinger</li>
<li>for: 防止声音掩伤和音频深伪检测</li>
<li>methods: 使用复лекс值神经网络处理CQT频域表示的输入音频，保留相位信息，并允许使用可解释AI方法</li>
<li>results: 比前方法在”In-the-Wild”反伪 dataset上表现出色，并可以通过可解释AI方法解释结果，精度研究表明模型已经学习使用相位信息检测声音掩伤<details>
<summary>Abstract</summary>
Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the "In-the-Wild" anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing.
</details>
<details>
<summary>摘要</summary>
当前的反假识别和音频深度质模型使用 either 大小图表（如CQT或Melspectrograms）或Raw audio 经过卷积或填充层处理。两种方法都有缺点：大小图表抛弃相位信息，影响音频自然性，而Raw-feature-based 模型不能使用传统的可解释 AI 方法。这篇论文提议一种新的方法，通过使用复杂值神经网络处理输入音频的复杂值CQT频域表示。这种方法保留相位信息，并允许使用可解释 AI 方法。结果表明这种方法在 "In-the-Wild" 反假识别数据集上表现出色，并且可以通过可解释 AI 方法解释结果。剥离学研究表明，模型已经学会使用相位信息检测声音假设。
</details></li>
</ul>
<hr>
<h2 id="Karasu-A-Collaborative-Approach-to-Efficient-Cluster-Configuration-for-Big-Data-Analytics"><a href="#Karasu-A-Collaborative-Approach-to-Efficient-Cluster-Configuration-for-Big-Data-Analytics" class="headerlink" title="Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics"></a>Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11792">http://arxiv.org/abs/2308.11792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Scheinert, Philipp Wiesner, Thorsten Wittkopp, Lauritz Thamsen, Jonathan Will, Odej Kao</li>
<li>for: 这篇论文的目的是提出一种更有效的资源配置评估方法，以便更好地选择适合的机器和集群大小，并且能够同时优化多个目标。</li>
<li>methods: 这篇论文使用了一种名为“Karasu”的方法，它通过聚合多个用户的执行时间资讯，将其转换为轻量级的性能模型，以便更好地搜索适合的资源配置。</li>
<li>results: 根据论文的评估结果，Karasu方法能够与现有的方法相比，在性能、搜索时间和成本等方面获得显著提升，甚至在仅具有部分相似特征的比较轻量级 Profiling 运行中也能够获得提升。<details>
<summary>Abstract</summary>
Selecting the right resources for big data analytics jobs is hard because of the wide variety of configuration options like machine type and cluster size. As poor choices can have a significant impact on resource efficiency, cost, and energy usage, automated approaches are gaining popularity. Most existing methods rely on profiling recurring workloads to find near-optimal solutions over time. Due to the cold-start problem, this often leads to lengthy and costly profiling phases. However, big data analytics jobs across users can share many common properties: they often operate on similar infrastructure, using similar algorithms implemented in similar frameworks. The potential in sharing aggregated profiling runs to collaboratively address the cold start problem is largely unexplored.   We present Karasu, an approach to more efficient resource configuration profiling that promotes data sharing among users working with similar infrastructures, frameworks, algorithms, or datasets. Karasu trains lightweight performance models using aggregated runtime information of collaborators and combines them into an ensemble method to exploit inherent knowledge of the configuration search space. Moreover, Karasu allows the optimization of multiple objectives simultaneously. Our evaluation is based on performance data from diverse workload executions in a public cloud environment. We show that Karasu is able to significantly boost existing methods in terms of performance, search time, and cost, even when few comparable profiling runs are available that share only partial common characteristics with the target job.
</details>
<details>
<summary>摘要</summary>
We present Karasu, an approach to more efficient resource configuration profiling that promotes data sharing among users working with similar infrastructures, frameworks, algorithms, or datasets. Karasu trains lightweight performance models using aggregated runtime information of collaborators and combines them into an ensemble method to exploit inherent knowledge of the configuration search space. Moreover, Karasu allows the optimization of multiple objectives simultaneously. Our evaluation is based on performance data from diverse workload executions in a public cloud environment. We show that Karasu is able to significantly boost existing methods in terms of performance, search time, and cost, even when few comparable profiling runs are available that share only partial common characteristics with the target job. translate to Simplified Chinese:选择大数据分析任务的合适资源很Difficult，因为配置选项的多样性，如机器类型和集群大小。 incorrect choices can have a significant impact on resource efficiency, cost, and energy usage，所以自动化方法 becoming popular。most existing methods rely on profiling recurring workloads to find near-optimal solutions over time，but this often leads to lengthy and costly profiling phases due to the cold-start problem。 however，big data analytics jobs across users can share many common properties，such as operating on similar infrastructure，using similar algorithms implemented in similar frameworks。the potential in sharing aggregated profiling runs to collaboratively address the cold start problem is largely unexplored。我们提出了Karasu，一种更有效的资源配置 Profiling 方法，它推广用户工作在相似的基础设施，框架，算法或数据集之间的数据共享。Karasu 使用协作者的各种运行时信息 trains 轻量级性能模型，并将其组合成ensemble方法，以利用配置搜索空间的内在知识。此外，Karasu 允许同时优化多个目标。我们的评估基于公共云环境中的多种任务执行性能数据。我们表明，Karasu 能够 Significantly 提高现有方法的性能，搜索时间和成本，即使有 Only few comparable profiling runs are available that share only partial common characteristics with the target job。
</details></li>
</ul>
<hr>
<h2 id="HypBO-Expert-Guided-Chemist-in-the-Loop-Bayesian-Search-for-New-Materials"><a href="#HypBO-Expert-Guided-Chemist-in-the-Loop-Bayesian-Search-for-New-Materials" class="headerlink" title="HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials"></a>HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11787">http://arxiv.org/abs/2308.11787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdoulatif Cisse, Xenophon Evangelopoulos, Sam Carruthers, Vladimir V. Gusev, Andrew I. Cooper</li>
<li>for: 本研究旨在使用人类专家知识来加速bayesian优化的搜索过程，以解决科学问题中的多变量问题。</li>
<li>methods: 本研究使用了 bayesian 优化方法，并利用专家理论来提供更好的初始样本，以优化搜索过程。</li>
<li>results: 研究结果表明，使用专家理论可以减少不重要的样本，并且可以更好地覆盖搜索空间，从而提高搜索效率。<details>
<summary>Abstract</summary>
Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate an improved seed of samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampling. This process continues in a global versus local search fashion, organized in a bilevel optimization framework. We validate the performance of our method on a range of synthetic functions and demonstrate its practical utility on a real chemical design task where the use of expert hypotheses accelerates the search performance significantly.
</details>
<details>
<summary>摘要</summary>
瑜珈和自动化技术可以为解决复杂多变量科学问题提供巨大的加速，但搜索空间可能会变得极其庞大。bayesian优化（BO）已经成为一种流行的高效搜索引擎，特别是在没有知道目标函数/属性的 analytic 表达的情况下。在这些任务中，我们利用专家人类知识来导向 bayesian 搜索更快速地访问有潜力的化学空间。先前的方法使用了基于现有实验测量的下面分布，这是对新、未探索的科学任务而言不可能的。此外，这些分布不能捕捉复杂的假设。我们提出的方法，即 HypBO，使用专家人类假设来生成改进的样本。不可能的样本会被排除，而有潜力的样本将被用来补充模型数据，从而实现更加有知识的搜索。这个过程会在全球与本地的搜索模式下进行，组织成两级优化框架。我们验证了我们的方法在一系列的synthetic函数上的性能，并在一个真实的化学设计任务中展示了它的实用性。在这个任务中，使用专家假设可以大幅度加速搜索过程。
</details></li>
</ul>
<hr>
<h2 id="Coarse-to-Fine-Multi-Scene-Pose-Regression-with-Transformers"><a href="#Coarse-to-Fine-Multi-Scene-Pose-Regression-with-Transformers" class="headerlink" title="Coarse-to-Fine Multi-Scene Pose Regression with Transformers"></a>Coarse-to-Fine Multi-Scene Pose Regression with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11783">http://arxiv.org/abs/2308.11783</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yolish/c2f-ms-transformer">https://github.com/yolish/c2f-ms-transformer</a></li>
<li>paper_authors: Yoli Shavit, Ron Ferens, Yosi Keller</li>
<li>for: 这个论文目的是学习多场景绝对摄像头pose regression。</li>
<li>methods: 这个论文使用了变换器学习模型，包括激活Map的汇集和特征转换，以及多个场景编码。</li>
<li>results: 该方法在常见的室内和室外数据集上进行评估，并显示出过单场景和多场景绝对摄像头pose regressor的性能。<details>
<summary>Abstract</summary>
Absolute camera pose regressors estimate the position and orientation of a camera given the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron (MLP) head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended to learn multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into pose predictions. This allows our model to focus on general features that are informative for localization, while embedding multiple scenes in parallel. We extend our previous MS-Transformer approach \cite{shavit2021learning} by introducing a mixed classification-regression architecture that improves the localization accuracy. Our method is evaluated on commonly benchmark indoor and outdoor datasets and has been shown to exceed both multi-scene and state-of-the-art single-scene absolute pose regressors.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译成简化中文。<</SYS>>绝对摄像头姿态回归器可以根据捕捉的图像估计摄像头的位置和方向。通常，一个卷积减少器（Convolutional Backbone）和多层感知器（Multi-layer Perceptron，MLP）头被训练使用图像和姿态标签来嵌入单个参考场景。在最近的研究中，这种方案被扩展以学习多个场景，通过取代MLP头而使用完全连接层。在这项工作中，我们提议使用变换器来学习多个场景绝对摄像头姿态回归，其中混合encoder和decoder被用来聚合活动地图和场景编码，并将其转换成姿态预测。这使得我们的模型能够专注于通用的特征，同时并行地嵌入多个场景。我们在前一项MS-Transformer方法（\cite{shavit2021learning}）的基础上增加了混合分类回归架构，以提高地点准确性。我们的方法在常见的室内和室外数据集上进行评估，并已经超过了多个场景和当前最佳单个场景绝对摄像头姿态回归器。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Dynamic-and-Sparse-Qualitative-Data-A-Hilbert-Space-Embedding-of-Categorical-Variables"><a href="#Addressing-Dynamic-and-Sparse-Qualitative-Data-A-Hilbert-Space-Embedding-of-Categorical-Variables" class="headerlink" title="Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables"></a>Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11781">http://arxiv.org/abs/2308.11781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirban Mukherjee, Hannah H. Chang</li>
<li>for: 该论文旨在扩展量化模型，以便包含 качеitative数据进行 causal 估计。</li>
<li>methods: 该论文使用函数分析创建了一个更加灵活和弹性的框架，将观察到的类别嵌入到了一个Baire空间中，并通过一个连续线性映射将这些类别映射到一个 reproduce kernel Hilbert space（RKHS）中。</li>
<li>results: 该论文通过实验证明了其超越传统方法的性能，特别在qualitative信息复杂和细腻的场景下。<details>
<summary>Abstract</summary>
We propose a novel framework for incorporating qualitative data into quantitative models for causal estimation. Previous methods use categorical variables derived from qualitative data to build quantitative models. However, this approach can lead to data-sparse categories and yield inconsistent (asymptotically biased) and imprecise (finite sample biased) estimates if the qualitative information is dynamic and intricate. We use functional analysis to create a more nuanced and flexible framework. We embed the observed categories into a latent Baire space and introduce a continuous linear map -- a Hilbert space embedding -- from the Baire space of categories to a Reproducing Kernel Hilbert Space (RKHS) of representation functions. Through the Riesz representation theorem, we establish that the canonical treatment of categorical variables in causal models can be transformed into an identified structure in the RKHS. Transfer learning acts as a catalyst to streamline estimation -- embeddings from traditional models are paired with the kernel trick to form the Hilbert space embedding. We validate our model through comprehensive simulation evidence and demonstrate its relevance in a real-world study that contrasts theoretical predictions from economics and psychology in an e-commerce marketplace. The results confirm the superior performance of our model, particularly in scenarios where qualitative information is nuanced and complex.
</details>
<details>
<summary>摘要</summary>
我们提出一种新的框架，用于将质量数据 incorporated 到量化模型中进行 causal 估计。先前的方法使用来自质量数据的分类变量来构建量化模型，但这种方法可能会导致数据缺乏和偏向（即不稳定和偏差）的估计结果，特别是当质量信息是动态且复杂时。我们使用函数分析来创建一个更加灵活和细腻的框架。我们将观察到的分类 embedding 到一个 latent Baire 空间中，并引入一个连续线性映射——一个 Reproducing Kernel Hilbert Space (RKHS) 的表示函数空间中的映射。通过 Riesz 表示定理，我们证明了在 causal 模型中对分类变量的 canonical 处理可以转化为一个唯一标识结构在 RKHS 中。trasnfer learning 作为一种 catalyst，我们可以通过对传统模型的 embeddings 与kernel trick 结合来形成一个 Hilbert 空间 embedding。我们通过了广泛的 simulations 证明和一个实际的例子，证明了我们的模型在复杂和细腻的质量信息场景下表现更加优秀。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Anomaly-Detection-in-Text-with-Deviation-Learning"><a href="#Few-shot-Anomaly-Detection-in-Text-with-Deviation-Learning" class="headerlink" title="Few-shot Anomaly Detection in Text with Deviation Learning"></a>Few-shot Anomaly Detection in Text with Deviation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11780">http://arxiv.org/abs/2308.11780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anindya Sundar Das, Aravind Ajay, Sriparna Saha, Monowar Bhuyan<br>for: 这种论文的目的是提出一种基于深度几个示例学习的方法，用于检测文本中的异常Example。methods: 该方法使用了几个示例学习的技术，包括异常示例的有限使用、直接学习异常分数和多头自我注意力层。results: 对多个 benchmark 数据集进行了广泛的实验，并达到了新的状态态的性能水平。<details>
<summary>Abstract</summary>
Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably deviate from the reference score in the upper tail of the prior. Additionally, our model is optimized to learn the distinct behavior of anomalies by utilizing a multi-head self-attention layer and multiple instance learning approaches. Comprehensive experiments on several benchmark datasets demonstrate that our proposed approach attains a new level of state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
现有的异常检测方法大多数都是基于无标注数据构建模型。这些模型假设没有异常示例，这限制了它们使用异常示例的先验知识，从而导致它们在许多实际应用中表现不佳。此外，这些模型更关注学习特征嵌入than直接优化异常分数，这可能会导致异常分数评估不准确和数据学习过程中的不efficient使用。在本文中，我们介绍了FATE，一种深度几个shot学习基于框架，它利用有限异常示例和直接学习异常分数的端到端方法。在这种方法中，正常示例的异常分数被调整，以便与先前分布中的参考分数相似。相反，异常示例被迫有异常分数，这些分数与参考分数在上 tail 的异常分布中异常大。此外，我们的模型利用多头自注意力层和多个实例学习方法来学习异常的特殊行为。我们在多个标准 benchmark 数据集上进行了广泛的实验，结果表明，我们的提议方法可以达到新的顶峰性能水平。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Hessian-Alignment-for-Domain-Generalization"><a href="#Understanding-Hessian-Alignment-for-Domain-Generalization" class="headerlink" title="Understanding Hessian Alignment for Domain Generalization"></a>Understanding Hessian Alignment for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11778">http://arxiv.org/abs/2308.11778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/federated-learning">https://github.com/huawei-noah/federated-learning</a></li>
<li>paper_authors: Sobhan Hemati, Guojun Zhang, Amir Estiri, Xi Chen<br>for: 这 paper 是关于 out-of-distribution (OOD) 泛化的研究，旨在提高深度学习模型在各种实际应用场景中的 OOD 泛化能力。methods: 这 paper 使用了 gradient-based 正则化技术来提高 OOD 泛化能力，并分析了 Hessian 和 gradient 在领域泛化中的角色。results: 这 paper 的结果表明，将分类器的头 Hessian 矩阵和梯度进行对齐可以提高 OOD 泛化能力，并且提出了两种简单 yet effective 的方法来实现对齐。这些方法在不同的 OOD 场景中都显示了出色的性能。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenarios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regularizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier's head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spectral norm between the classifier's head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Furthermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients. Our analysis explains the success of many regularizers like CORAL, IRM, V-REx, Fish, IGA, and Fishr as they regularize part of the classifier's head Hessian and/or gradient. Finally, we propose two simple yet effective methods to match the classifier's head Hessians and gradients in an efficient way, based on the Hessian Gradient Product (HGP) and Hutchinson's method (Hutchinson), and without directly calculating Hessians. We validate the OOD generalization ability of proposed methods in different scenarios, including transferability, severe correlation shift, label shift and diversity shift. Our results show that Hessian alignment methods achieve promising performance on various OOD benchmarks. The code is available at \url{https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment}.
</details>
<details>
<summary>摘要</summary>
外部分布（OOD）泛化是深度学习模型在实际应用场景中的重要能力之一，包括医疗和自动驾驶等。最近，不同的技术被提出来提高OOD泛化。其中，梯度基本的正则化方法表现出色，并且我们对域泛化中梯度和梯度对的角色的理解仍然受限。为了解决这个缺陷，我们通过最近的OOD理论来分析域泛化中梯度和梯度对的角色。我们 teorically 表明，在目标域与源域之间的传输度量上，梯度和梯度对的spectral norm是Upper bound。此外，我们还分析了梯度和梯度对相互对齐的所有特征。我们的分析解释了许多正则化器，如CORAL、IRM、V-REx、Fish、IGA和Fishr的成功，他们都在域泛化中对梯度和梯度对进行了正则化。最后，我们提出了两种简单 yet efficient的方法来匹配梯度和梯度对，基于梯度和梯度对的乘积（HGP）和欧几里得（Hutchinson）的方法，而不需直接计算梯度。我们 validate了我们提出的方法的OOD泛化能力在不同的场景中，包括传输性、严重相关转移、标签转移和多样性转移。我们的结果表明，梯度对齐方法在不同的OOD测试上表现出色。代码可以在 <https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment> 中找到。
</details></li>
</ul>
<hr>
<h2 id="3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network"><a href="#3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network" class="headerlink" title="3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network"></a>3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11771">http://arxiv.org/abs/2308.11771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinyu Chen, Zuowen Wang, Shih-Chii Liu, Chang Gao</li>
<li>for: 这 paper 是为了提出一种 sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) 模型，用于事件基于眼动跟踪，这种技术将被应用于下一代智能眼镜等设备中。</li>
<li>methods: 这 paper 使用了 retina-inspired event cameras，即具有低延迟响应和稀疏输出事件流的摄像头。 authors 还使用了 delta-encoded recurrent path，以提高活化稀疏性，从而降低计算量。</li>
<li>results: 这 paper 的 CB-ConvLSTM 架构可以高效地提取事件流中的 spatial-temporal 特征，用于眼动跟踪。 compared to conventional CNN 结构，CB-ConvLSTM 可以减少计算量约 4.7倍，不会失去准确性。 这种增强的效率使得其适用于实时眼动跟踪。<details>
<summary>Abstract</summary>
This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR/VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\times$ without losing accuracy when tested on a \texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \url{https://github.com/qinche106/cb-convlstm-eyetracking}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data"><a href="#Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data" class="headerlink" title="Patient Clustering via Integrated Profiling of Clinical and Digital Data"></a>Patient Clustering via Integrated Profiling of Clinical and Digital Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11748">http://arxiv.org/abs/2308.11748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjin Choi, Andy Xiang, Ozgur Ozturk, Deep Shrestha, Barry Drake, Hamid Haidarian, Faizan Javed, Haesun Park</li>
<li>for: 这篇论文是为了开发一个基于patient profiling的健康照顾 clustering模型，以提高健康照顾中的患者分组和推荐能力。</li>
<li>methods: 这个模型使用一种基于受限制的低矩降降推导法，利用患者的临床数据和数位互动数据（包括搜寻和浏览），创建患者 profil。这 leads to nonnegative embedding vectors, which serve as a low-dimensional representation of the patients.</li>
<li>results: 评估过real-world patient data from a healthcare web portal， compared to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.<details>
<summary>Abstract</summary>
We introduce a novel profile-based patient clustering model designed for clinical data in healthcare. By utilizing a method grounded on constrained low-rank approximation, our model takes advantage of patients' clinical data and digital interaction data, including browsing and search, to construct patient profiles. As a result of the method, nonnegative embedding vectors are generated, serving as a low-dimensional representation of the patients. Our model was assessed using real-world patient data from a healthcare web portal, with a comprehensive evaluation approach which considered clustering and recommendation capabilities. In comparison to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的基于 Profile 的患者划分模型，适用于医疗数据。我们的模型利用基于受限制的低级数据减少法，使用患者的临床数据和数字互动数据（包括浏览和搜索）构建患者profile。这种方法生成了非负嵌入 вектор，用于表示患者。我们的模型在使用真实世界患者数据from a healthcare web portal进行评估，并对划分和推荐能力进行全面评估。与其他基线相比，我们的方法在划分准确性和推荐准确性方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Animal3D-A-Comprehensive-Dataset-of-3D-Animal-Pose-and-Shape"><a href="#Animal3D-A-Comprehensive-Dataset-of-3D-Animal-Pose-and-Shape" class="headerlink" title="Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape"></a>Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11737">http://arxiv.org/abs/2308.11737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, Wei Ji, Chen Wang, Xiaoding Yuan, Prakhar Kaushik, Guofeng Zhang, Jie Liu, Yushan Xie, Yawen Cui, Alan Yuille, Adam Kortylewski</li>
<li>for: 研究动物3D姿势和形状估计，以解释动物行为，并可能帮助多个下游应用，如野生动物保育。</li>
<li>methods: 我们提出了 Animal3D 资料集，包括 3379 幅照片和 40 种哺乳类动物的高品质26个关键点标注。</li>
<li>results: 我们在 Animal3D 资料集上进行了代表性的形状和姿势估计模型评估，包括 supervised 学习、synthetic to real transfer 和 fine-tuning human pose 和形状估计模型。我们的实验结果显示，预测动物这些种类中的3D形状和姿势仍然是一个非常具有挑战性的任务，尽管人类姿势估计方法有了重要的进步。<details>
<summary>Abstract</summary>
Accurately estimating the 3D pose and shape is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. However, research in this area is held back by the lack of a comprehensive and diverse dataset with high-quality 3D pose and shape annotations. In this paper, we propose Animal3D, the first comprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D consists of 3379 images collected from 40 mammal species, high-quality annotations of 26 keypoints, and importantly the pose and shape parameters of the SMAL model. All annotations were labeled and checked manually in a multi-stage process to ensure highest quality results. Based on the Animal3D dataset, we benchmark representative shape and pose estimation models at: (1) supervised learning from only the Animal3D data, (2) synthetic to real transfer from synthetically generated images, and (3) fine-tuning human pose and shape estimation models. Our experimental results demonstrate that predicting the 3D shape and pose of animals across species remains a very challenging task, despite significant advances in human pose estimation. Our results further demonstrate that synthetic pre-training is a viable strategy to boost the model performance. Overall, Animal3D opens new directions for facilitating future research in animal 3D pose and shape estimation, and is publicly available.
</details>
<details>
<summary>摘要</summary>
正确估算动物3D姿态和形状是研究动物行为的重要步骤，可能对野生动物保护和其他下游应用具有巨大的应用前景。然而，这个领域的研究受到缺乏完整和多样化的3D姿态和形状标注数据的限制。在这篇论文中，我们提出了动物3D（Animal3D），第一个包括40种哺乳动物的完整数据集，以及高品质的26个关键点标注。这些标注都是透过多阶段的手动标注和确认程序来确保最高品质的结果。基于动物3D数据集，我们在：（1）仅使用动物3D数据进行监督学习，（2）从生成的 sintetic 图像进行转换到真实图像，以及（3）调整人体姿态和形状估算模型的表现进行比较。我们的实验结果显示，预测动物这些种类的3D姿态和形状仍然是一个非常困难的任务，尽管人体姿态估算领域已经取得了重大进步。我们的结果还显示，从 sintetic 预训成功地增强模型性能。总的来说，动物3D开启了新的研究方向，并且公开 disponibile。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering"><a href="#Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering" class="headerlink" title="Knowledge Graph Prompting for Multi-Document Question Answering"></a>Knowledge Graph Prompting for Multi-Document Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11730">http://arxiv.org/abs/2308.11730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa Siu, Ruiyi Zhang, Tyler Derr</li>
<li>for: 这个研究旨在提高大语言模型（LLM）在多文档问答（MD-QA）中的表现，并 explore the “pre-train, prompt, predict”  paradigm in MD-QA.</li>
<li>methods: 这个研究提出了一种知识图提示（KGP）方法，包括一个知识图建构模块和一个知识图游走模块。知识图建构模块使用多个文档中的节点和边来表示文档之间的semantic和lexical相似性，而知识图游走模块使用LM响应来导航知识图，并收集支持答案的文档段落。</li>
<li>results: 实验结果表明，KGP方法可以提高LLM在MD-QA中的表现，并且可以减少检索时间。这种方法的实现可以在<a target="_blank" rel="noopener" href="https://github.com/YuWVandy/KG-LLM-MDQA%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YuWVandy/KG-LLM-MDQA中找到。</a><details>
<summary>Abstract</summary>
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the LM-guided traverser acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code is at https://github.com/YuWVandy/KG-LLM-MDQA.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的“预训练、提示、预测”模式在开放预测问答（OD-QA）中实现了很大的成功。然而，很少的研究探讨这种模式在多文档问答（MD-QA）中的应用， MD-QA 是需要对不同文档内容和结构的理解，以便更好地回答问题。为了填补这一重要的漏洞，我们提出了知识图 prompting（KGP）方法，用于在 LLM 中提示 MD-QA，该方法包括知识图构建模块和知识图游走模块。For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations.For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the LM-guided traverser acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality.Exhaustive experiments demonstrate the effectiveness of KGP for MD-QA, indicating the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code is available at <https://github.com/YuWVandy/KG-LLM-MDQA>.
</details></li>
</ul>
<hr>
<h2 id="When-Are-Two-Lists-Better-than-One-Benefits-and-Harms-in-Joint-Decision-making"><a href="#When-Are-Two-Lists-Better-than-One-Benefits-and-Harms-in-Joint-Decision-making" class="headerlink" title="When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making"></a>When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11721">http://arxiv.org/abs/2308.11721</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kpdonahue/benefits_harms_joint_decision_making">https://github.com/kpdonahue/benefits_harms_joint_decision_making</a></li>
<li>paper_authors: Kate Donahue, Kostas Kollias, Sreenivas Gollapudi</li>
<li>for: 这种研究是为了优化人机算联合表现的最佳方式。</li>
<li>methods: 本研究使用了一种特定的人机算合作方式，其中算法有一组ITEMS，并将其中的一个subsetSize&#x3D;k个Item展示给人，人将选择最终的Item。这种方式可以应用于内容推荐、路径规划等任务。因为人和算法都有不准确的信息，因此关键问题是：哪个值得最大化最终选择最佳Item的概率？</li>
<li>results: 研究发现，在某些噪声模型下，最佳的$k$值在[2, n-1]之间，即在人机算合作下有着绝对的优势。然而，当人被anchor在算法提供的排序顺序时，联合系统的表现一定是差。此外，研究还发现在人机准确性水平不同时，存在一些情况下，一个更准确的代理会受益于与一个 menos准确的代理合作。<details>
<summary>Abstract</summary>
Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to collaborating, even when the human and algorithm have equal accuracy separately. We demonstrate this theoretically for the Mallows model and experimentally for the Random Utilities models of noisy permutations. However, we show this pattern is reversed when the human is anchored on the algorithm's presented ordering - the joint system always has strictly worse performance. We extend these results to the case where the human and algorithm differ in their accuracy levels, showing that there always exist regimes where a more accurate agent would strictly benefit from collaborating with a less accurate one, but these regimes are asymmetric between the human and the algorithm's accuracy.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:历史上，许多机器学习研究都集中在算法性能上，但最近更多的关注集中在人机合作性能。在这种人机合作中，算法可以访问一个集合中的 $n$ 个项目，并将其中的一个 subset 大小为 $k$ 项显示给人类，人类将选择最终的项目。这种场景可以模型内容推荐、路径规划或任何类型的标签任务。由于人类和算法都有不准确、噪声的信息关于真实的项目顺序，关键问题是：哪个值的 $k$ 最大化人类最终选择的最佳项目的概率？ For $k=1$, 性能是由算法 acting alone 优化的，而 For $k=n$ 是由人类 acting alone 优化的。意外地，我们发现在多种噪声模型下，最佳的 $k $ 是 $[2, n-1]$ 的Interval -  то есть，在人类和算法都有等准确级别时，存在着协同的益处，即使人类和算法的准确率都是等于的。我们在 Mallows 模型和 Random Utilities 模型中证明这一结论，并通过实验证明这一结论。然而，我们发现在人类被算法的显示顺序固定时，人机合作系统总是有固定性下降的性能问题。我们扩展这些结论到人类和算法准确级别不同的情况下，并证明在某些情况下，更准确的代理人会受益于和更准确的算法合作。然而，这些情况是人类和算法准确级别之间的偏好的。
</details></li>
</ul>
<hr>
<h2 id="Collect-Measure-Repeat-Reliability-Factors-for-Responsible-AI-Data-Collection"><a href="#Collect-Measure-Repeat-Reliability-Factors-for-Responsible-AI-Data-Collection" class="headerlink" title="Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection"></a>Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12885">http://arxiv.org/abs/2308.12885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oana Inel, Tim Draws, Lora Aroyo</li>
<li>for: 本研究旨在提高人工智能（AI）数据收集过程中的质量和可靠性，以便提高AI模型的公平性和可靠性。</li>
<li>methods: 本研究提出了一种负责任AI（RAI）方法，用于系统地对数据收集过程中的因素进行深入分析，以便评估数据的内部可靠性和外部稳定性。</li>
<li>results: 研究人员对9个现有数据集和注释任务进行了验证，并在四种内容模式上进行了检验。结果表明，RAI方法可以帮助评估数据的稳定性和可靠性，并且可以处理公平和责任的方面在数据收集中的问题。<details>
<summary>Abstract</summary>
The rapid entry of machine learning approaches in our daily activities and high-stakes domains demands transparency and scrutiny of their fairness and reliability. To help gauge machine learning models' robustness, research typically focuses on the massive datasets used for their deployment, e.g., creating and maintaining documentation for understanding their origin, process of development, and ethical considerations. However, data collection for AI is still typically a one-off practice, and oftentimes datasets collected for a certain purpose or application are reused for a different problem. Additionally, dataset annotations may not be representative over time, contain ambiguous or erroneous annotations, or be unable to generalize across issues or domains. Recent research has shown these practices might lead to unfair, biased, or inaccurate outcomes. We argue that data collection for AI should be performed in a responsible manner where the quality of the data is thoroughly scrutinized and measured through a systematic set of appropriate metrics. In this paper, we propose a Responsible AI (RAI) methodology designed to guide the data collection with a set of metrics for an iterative in-depth analysis of the factors influencing the quality and reliability} of the generated data. We propose a granular set of measurements to inform on the internal reliability of a dataset and its external stability over time. We validate our approach across nine existing datasets and annotation tasks and four content modalities. This approach impacts the assessment of data robustness used for AI applied in the real world, where diversity of users and content is eminent. Furthermore, it deals with fairness and accountability aspects in data collection by providing systematic and transparent quality analysis for data collections.
</details>
<details>
<summary>摘要</summary>
machine learning 技术在我们日常生活和高度关键领域的快速进入需要透明度和检查其公平和可靠性。为了评估机器学习模型的可靠性，研究通常集中在部署之前的庞大数据集上，例如创建和维护这些数据集的文档，以便理解它们的起源、开发过程和伦理考虑。然而，AI数据收集仍然是一项一次性的做法，而且经常 reuse datasets 用于不同的问题或应用。此外，数据集的标注可能不具有时间的普适性，包含歧义或错误的标注，或者无法泛化到问题或领域。 latest research 表明这些做法可能会导致不公正、偏见或不准确的结果。我们认为AI数据收集应该进行负责任的方式，即在数据收集过程中评估和测量数据的质量，使用一套系统的精细度度量。在这篇论文中，我们提出了一种负责任AI（RAI）方法，用于指导数据收集，并提供了一系列适用于不同数据集和标注任务的度量。我们验证了我们的方法在九个现有数据集和标注任务中，以及四种内容模式中。这种方法对实际应用中的AI数据Robustness进行评估，并处理了公平和责任方面的问题。
</details></li>
</ul>
<hr>
<h2 id="SuperCalo-Calorimeter-shower-super-resolution"><a href="#SuperCalo-Calorimeter-shower-super-resolution" class="headerlink" title="SuperCalo: Calorimeter shower super-resolution"></a>SuperCalo: Calorimeter shower super-resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11700">http://arxiv.org/abs/2308.11700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ian-pang/supercalo">https://github.com/ian-pang/supercalo</a></li>
<li>paper_authors: Ian Pang, John Andrew Raine, David Shih</li>
<li>for:  overcome the challenge of calorimeter shower simulation in the Large Hadron Collider computational pipeline</li>
<li>methods:  employ deep-generative surrogate models, specifically a flow-based super-resolution model called SuperCalo</li>
<li>results:  high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers, reducing computational cost, memory requirements, and generation time<details>
<summary>Abstract</summary>
Calorimeter shower simulation is a major bottleneck in the Large Hadron Collider computational pipeline. There have been recent efforts to employ deep-generative surrogate models to overcome this challenge. However, many of best performing models have training and generation times that do not scale well to high-dimensional calorimeter showers. In this work, we introduce SuperCalo, a flow-based super-resolution model, and demonstrate that high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers. This novel approach presents a way to reduce computational cost, memory requirements and generation time associated with fast calorimeter simulation models. Additionally, we show that the showers upsampled by SuperCalo possess a high degree of variation. This allows a large number of high-dimensional calorimeter showers to be upsampled from much fewer coarse showers with high-fidelity, which results in additional reduction in generation time.
</details>
<details>
<summary>摘要</summary>
喷泉计数器模拟是大型夸克粒子加速器计算管道中的主要瓶颈。近年来，有很多努力使用深度生成器模型来突破这个挑战。然而，许多最佳性能的模型培训和生成时间不能扩展到高维喷泉计数器。在这项工作中，我们介绍SuperCalo，一种流基的超分辨模型，并证明了高维细腔喷泉可以快速升sample自粗腔喷泉。这种新的方法可以减少计算成本、内存需求和生成时间相关于快速喷泉计数器模型。此外，我们表明了升sample后的喷泉具有高度的变化度。这意味着可以从少量粗腔喷泉中生成大量高维喷泉，具有高准确性，从而再次减少生成时间。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Benchmarking-of-Language-Models"><a href="#Efficient-Benchmarking-of-Language-Models" class="headerlink" title="Efficient Benchmarking (of Language Models)"></a>Efficient Benchmarking (of Language Models)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11696">http://arxiv.org/abs/2308.11696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sumankrsh/Sentiment-Analysis.ipynb">https://github.com/sumankrsh/Sentiment-Analysis.ipynb</a></li>
<li>paper_authors: Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen</li>
<li>for: 本研究旨在解决语言模型评估中的效率问题，提出了一种智能减少语言模型评估计算成本的方法，以减少计算成本而不影响可靠性。</li>
<li>methods: 本研究使用了HELMbenchmark作为测试 caso，研究了不同的benchmark设计选择对计算vs可靠性的贸易OFF。提出了一种新的度量指标DIoR来评估决策对可靠性的影响。</li>
<li>results: 研究发现，现有的领导者在HELMbenchmark可以通过移除一些低排名的模型来改变排名，而且只需很少的例子即可获得正确的排名。同时，不同的HELM场景选择会导致排名差异很大。根据研究结果，提出了一些具体的建议，以减少计算成本并且保持可靠性，可以实现计算成本减少x100或更多。<details>
<summary>Abstract</summary>
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark ranking. Conversely a slightly different choice of HELM scenarios varies ranking widely. Based on our findings we outline a set of concrete recommendations for more efficient benchmark design and utilization practices leading to dramatic cost savings with minimal loss of benchmark reliability often reducing computation by x100 or more.
</details>
<details>
<summary>摘要</summary>
LM模型的多样化性带来了一新类的评价指标，这些指标涵盖了各种能力的广泛评估。然而，这些评价努力的效率方面几乎没有在文献中得到了讨论。在这项工作中，我们提出了一个问题，即如何智能减少LM评价的计算成本，无需妥协可靠性。使用HELM benchmark作为测试 caso，我们研究了不同的评价指标设计选择对计算与可靠性之间的负面影响。我们提出了一个新的度量器，即决策影响可靠性（DIoR），用于评估这些决策的可靠性。我们发现，例如，现有领先者在HELM上可能会改变，只需要从benchmark中移除一个低排名的模型即可。同时，我们发现一些不同的HELM场景可以导致评价排名差异极大。基于我们的发现，我们提出了一些具体的建议，包括更有效的评价设计和使用实践，可以实现计算成本减少100倍或更多，而且减少的成本幅度与可靠性损失之间的关系也可以得到精细控制。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Multi-Resolution-Communications"><a href="#Semantic-Multi-Resolution-Communications" class="headerlink" title="Semantic Multi-Resolution Communications"></a>Semantic Multi-Resolution Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11604">http://arxiv.org/abs/2308.11604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matin Mortaheb, Mohammad A. Amir Khojastepour, Srimat T. Chakradhar, Sennur Ulukus</li>
<li>for: 这种深度学习基于JSCC的框架是为了提高数据重建的性能，特别是在finite block-length数据下，SSCC表现下降。此外，SSCC还无法在多用户和&#x2F;或多分辨率下进行数据重建，因为它只是为worst channel和&#x2F;或最高质量数据进行优化。</li>
<li>methods: 我们提出了一种基于MTL的深度学习多分辨率JSCC框架，通过层次结构来编码数据，并通过当前和过去层编码数据来进行解码。此外，这种框架还可以应用于semantic通信，其目标是保留特定semantic attribute。</li>
<li>results: 我们在MNIST和CIFAR10 dataset上进行实验，并证明了我们的提出的方法可以在不同分辨率下重建数据，并且可以在 successive layers中提高semantic classifier的准确性。这种能力特别有用于优先保留数据集中的关键semantic attribute。<details>
<summary>Abstract</summary>
Deep learning based joint source-channel coding (JSCC) has demonstrated significant advancements in data reconstruction compared to separate source-channel coding (SSCC). This superiority arises from the suboptimality of SSCC when dealing with finite block-length data. Moreover, SSCC falls short in reconstructing data in a multi-user and/or multi-resolution fashion, as it only tries to satisfy the worst channel and/or the highest quality data. To overcome these limitations, we propose a novel deep learning multi-resolution JSCC framework inspired by the concept of multi-task learning (MTL). This proposed framework excels at encoding data for different resolutions through hierarchical layers and effectively decodes it by leveraging both current and past layers of encoded data. Moreover, this framework holds great potential for semantic communication, where the objective extends beyond data reconstruction to preserving specific semantic attributes throughout the communication process. These semantic features could be crucial elements such as class labels, essential for classification tasks, or other key attributes that require preservation. Within this framework, each level of encoded data can be carefully designed to retain specific data semantics. As a result, the precision of a semantic classifier can be progressively enhanced across successive layers, emphasizing the preservation of targeted semantics throughout the encoding and decoding stages. We conduct experiments on MNIST and CIFAR10 dataset. The experiment with both datasets illustrates that our proposed method is capable of surpassing the SSCC method in reconstructing data with different resolutions, enabling the extraction of semantic features with heightened confidence in successive layers. This capability is particularly advantageous for prioritizing and preserving more crucial semantic features within the datasets.
</details>
<details>
<summary>摘要</summary>
深度学习基于联合源通道编码（JSCC）已经实现了数据重建的显著进步，比单独源通道编码（SSCC）更好。这种超越来自于finite block length数据下SSCC的优化不足。此外，SSCC无法在多用户和/或多分辨率上重建数据，因为它只是尝试满足最差通道和/或最高质量数据。为了超越这些限制，我们提议了一种基于多任务学习（MTL）的深度学习多分辨率JSCC框架。这个提议的框架通过层次结构来编码数据，并通过同当前和过去层编码数据来有效地解码。此外，这个框架具有潜在的semantic communication功能，其目标超出了数据重建，是保留特定semantic attribute的。在这个框架中，每个层的编码数据都可以被优化，以保留特定数据 semantics。因此，在successive层中，精度的semantic classifier可以进一步提高，强调在编码和解码过程中保留目标semantic attribute。我们在MNIST和CIFAR10 dataset上进行了实验，实验结果表明，我们的提议方法可以在不同的分辨率下重建数据，并且可以在successive层中提高semantic classifier的精度，这是特别有利于在数据中优先保留更重要的semantic attribute。
</details></li>
</ul>
<hr>
<h2 id="Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models"><a href="#Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models" class="headerlink" title="Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models"></a>Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11601">http://arxiv.org/abs/2308.11601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surya Narayanan Hari, Matt Thomson</li>
<li>for: 这个研究是为了提出一个 контекст感知的路由系统， Tryage，以便将语言模型库中的专家模型选择依据输入提示的分析，以提高工作效率和适应性。</li>
<li>methods: 这个研究使用了语言模型路由器来预测下游模型的性能 на prompts，然后使用一个目标函数集成了性能预测、用户目标和约束来作出路由决策。</li>
<li>results: 在不同的数据集中，包括代码、文本、医疗资料和专利，Tryage框架在动态模型选择中比 Gorilla 和 GPT3.5 Turbo 高，实现了50.9% 的准确率，比 GPT3.5 Turbo 的23.6% 和 Gorilla 的10.8% 更高。<details>
<summary>Abstract</summary>
The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.
</details>
<details>
<summary>摘要</summary>
Introduction of transformer architecture and self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200,000 models in the Hugging Face ecosystem, users struggle with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. We propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection, identifying the optimal model with an accuracy of 50.9%, compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.
</details></li>
</ul>
<hr>
<h2 id="Low-Tensor-Rank-Learning-of-Neural-Dynamics"><a href="#Low-Tensor-Rank-Learning-of-Neural-Dynamics" class="headerlink" title="Low Tensor Rank Learning of Neural Dynamics"></a>Low Tensor Rank Learning of Neural Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11567">http://arxiv.org/abs/2308.11567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur Pellegrino, N Alex Cayco-Gajic, Angus Chadwick</li>
<li>for: 这个论文主要研究了 Recurrent Neural Networks (RNNs) 在学习过程中的 synaptic connectivity 的协调变化，以及这种变化如何影响学习的结果。</li>
<li>methods: 作者使用了 RNNs 的不同级别来适应不同的学习任务，并通过分析 weights 矩阵的低级别结构来研究学习过程中的 synaptic connectivity 的演化。</li>
<li>results: 作者发现，在学习过程中，RNNs 的 weights 矩阵具有低级别结构，并且这种低级别结构在整个学习过程中保持不变。此外，作者还 validate了这个发现，并提供了一些数学结果，证明在低维度任务上训练 RNNs 时，低级别 weights 自然地出现。<details>
<summary>Abstract</summary>
Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task by performing a low-tensor-rank decomposition directly on the ground truth weights, and by showing that the method we applied to the data faithfully recovers this low rank structure. Finally, we present a set of mathematical results bounding the matrix and tensor ranks of gradient descent learning dynamics which show that low-tensor-rank weights emerge naturally in RNNs trained to solve low-dimensional tasks. Taken together, our findings provide novel constraints on the evolution of population connectivity over learning in both biological and artificial neural networks, and enable reverse engineering of learning-induced changes in recurrent network dynamics from large-scale neural recordings.
</details>
<details>
<summary>摘要</summary>
学习 rely 于相协调的 synaptic 变化在 repeatedly 连接的 neuron  populations。因此，理解学习过程中 population 连接性的 collective 演化是 neuroscience 和 machine learning 中关键的挑战。特别是， latest 的研究表明，在任务训练后 RNN 的 weight matrix 通常具有低级数，但是这种低级数结构如何在学习过程中发展未知。为了解决这个问题，我们investigate  RNN 的 weight matrix 在学习过程中的级数。我们使用不同级数的 RNN 适应大规模的神经记录数据，并发现在整个学习过程中，推导出的 weights 都是 low-tensor-rank 的，因此在低维度的 subspace 中演化。我们验证了这一观察结果，并在 RNN 解决同一个任务时，直接对 ground truth  weights 进行 low-tensor-rank 分解，并证明了我们对数据进行的方法可以准确地恢复这种低级数结构。最后，我们提出了一些数学结果，证明在 gradient descent 学习动力学中，low-tensor-rank weights 会自然地出现在 RNN 解决低维度任务时。总之，我们的发现提供了对 population 连接性在学习过程中的新的约束，并允许从大规模神经记录数据中逆向工程学习-induced 变化的 recurrent network 动力学。
</details></li>
</ul>
<hr>
<h2 id="Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge"><a href="#Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge" class="headerlink" title="Practical Insights on Incremental Learning of New Human Physical Activity on the Edge"></a>Practical Insights on Incremental Learning of New Human Physical Activity on the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11691">http://arxiv.org/abs/2308.11691</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Arvanitakis, Jingwei Zuo, Mthandazo Ndhlovu, Hakim Hacid</li>
<li>for: 本研究探讨了Edge Machine Learning（Edge ML）中的一些独特挑战，包括受限的Edge设备存储空间、训练计算能力的有限性和学习类型的数量。</li>
<li>methods: 本研究使用了我们开发的MAGNETO系统，通过收集来自移动传感器的数据来学习人类活动。</li>
<li>results: 我们的实验结果显示，Edge ML在受限的 Edge 设备上进行学习时会面临一些独特的挑战，包括数据存储和计算能力的有限性。<details>
<summary>Abstract</summary>
Edge Machine Learning (Edge ML), which shifts computational intelligence from cloud-based systems to edge devices, is attracting significant interest due to its evident benefits including reduced latency, enhanced data privacy, and decreased connectivity reliance. While these advantages are compelling, they introduce unique challenges absent in traditional cloud-based approaches. In this paper, we delve into the intricacies of Edge-based learning, examining the interdependencies among: (i) constrained data storage on Edge devices, (ii) limited computational power for training, and (iii) the number of learning classes. Through experiments conducted using our MAGNETO system, that focused on learning human activities via data collected from mobile sensors, we highlight these challenges and offer valuable perspectives on Edge ML.
</details>
<details>
<summary>摘要</summary>
《边缘机器学习（Edge ML）》，将计算智能从云端系统转移到边缘设备，吸引了广泛关注，因为它们的优点明显，包括降低延迟、提高数据隐私和减少连接依赖。然而，这些优点也引入了传统云端方法中缺失的挑战。本文介绍边缘学习的细节，探讨（i）边缘设备受限的数据存储（ii）训练计算能力的限制和（iii）学习类数量之间的互相关系。通过我们的MAGNETO系统的实验，关于通过移动感知器收集的数据来学习人类活动，我们高亮了这些挑战并提供了有价值的对边缘ML的视角。
</details></li>
</ul>
<hr>
<h2 id="Multi-event-Video-Text-Retrieval"><a href="#Multi-event-Video-Text-Retrieval" class="headerlink" title="Multi-event Video-Text Retrieval"></a>Multi-event Video-Text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11551">http://arxiv.org/abs/2308.11551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gengyuanmax/mevtr">https://github.com/gengyuanmax/mevtr</a></li>
<li>paper_authors: Gengyuan Zhang, Jisen Ren, Jindong Gu, Volker Tresp</li>
<li>for: 这篇论文的目的是提出一种多事件视频文本检索任务（MeVTR），用于解决现实中视频内容通常包含多个事件，而文本查询或页面元数据通常与单个事件相关的问题。</li>
<li>methods: 这篇论文提出了一种简单的模型——Me-Retriever，它使用了关键事件视频表示和一种新的MeVTR损失函数来解决MeVTR任务。</li>
<li>results: 对于视频-to-文本和文本-to-视频任务，这种简单的框架超越了其他模型，并在MeVTR任务中建立了一个坚固的基础。<details>
<summary>Abstract</summary>
Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We present a simple model, Me-Retriever, which incorporates key event video representation and a new MeVTR loss for the MeVTR task. Comprehensive experiments show that this straightforward framework outperforms other models in the Video-to-Text and Text-to-Video tasks, effectively establishing a robust baseline for the MeVTR task. We believe this work serves as a strong foundation for future studies. Code is available at https://github.com/gengyuanmax/MeVTR.
</details>
<details>
<summary>摘要</summary>
视频文本检索（VTR）是一个重要的多Modal任务，在互联网上巨量的视频文本数据时代而言。大量工作通过使用两气流视力语言模型建立一个共同表示视频文本对的方法来进行VTR任务。然而，这些模型假设视频内容和文本之间是一对一的对应关系，而忽略了实际应用中的多个事件场景。这种假设与实际应用之间存在一个差距，导致之前训练的目标与实际应用之间的差异，从而导致旧模型在推理过程中的性能下降。在这项研究中，我们引入多事件视频文本检索（MeVTR）任务，解决每个视频包含多个不同事件的场景，是传统VTR任务的一个 nichescenario。我们提出了一种简单的模型，Me-Retriever，该模型包括关键事件视频表示和一个新的MeVTR损失函数。我们进行了广泛的实验，并证明了这种简单的框架可以在视频到文本和文本到视频任务中高效地超越其他模型，并成为MeVTR任务的robust基线。我们认为这项工作将成为未来研究的坚实基础。代码可以在https://github.com/gengyuanmax/MeVTR中获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/cs.LG_2023_08_23/" data-id="clltaagoj006vr8885jwkae3n" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.SD_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.SD_2023_08_23/">cs.SD - 2023-08-23 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Analysis-of-XLS-R-for-Speech-Quality-Assessment"><a href="#Analysis-of-XLS-R-for-Speech-Quality-Assessment" class="headerlink" title="Analysis of XLS-R for Speech Quality Assessment"></a>Analysis of XLS-R for Speech Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12077">http://arxiv.org/abs/2308.12077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lcn-kul/xls-r-analysis-sqa">https://github.com/lcn-kul/xls-r-analysis-sqa</a></li>
<li>paper_authors: Bastiaan Tamm, Rik Vandenberghe, Hugo Van hamme</li>
<li>for: 这项研究的目的是对 speech quality assessment 中使用 deep neural networks 进行自动评估，以提高用户体验质量。</li>
<li>methods: 该研究使用 pre-trained wav2vec-based XLS-R embeddings，并进行了层数据分析和特征组合的研究，以优化 speech quality prediction 的性能。</li>
<li>results: 研究发现，在不同层数据中提取特征可以达到最佳性能，并且对不同类型的干扰进行了分析，发现低级特征捕捉噪音和房间响应特征，高级特征则更注重语音内容和抗杂谱性。<details>
<summary>Abstract</summary>
In online conferencing applications, estimating the perceived quality of an audio signal is crucial to ensure high quality of experience for the end user. The most reliable way to assess the quality of a speech signal is through human judgments in the form of the mean opinion score (MOS) metric. However, such an approach is labor intensive and not feasible for large-scale applications. The focus has therefore shifted towards automated speech quality assessment through end-to-end training of deep neural networks. Recently, it was shown that leveraging pre-trained wav2vec-based XLS-R embeddings leads to state-of-the-art performance for the task of speech quality prediction. In this paper, we perform an in-depth analysis of the pre-trained model. First, we analyze the performance of embeddings extracted from each layer of XLS-R and also for each size of the model (300M, 1B, 2B parameters). Surprisingly, we find two optimal regions for feature extraction: one in the lower-level features and one in the high-level features. Next, we investigate the reason for the two distinct optima. We hypothesize that the lower-level features capture characteristics of noise and room acoustics, whereas the high-level features focus on speech content and intelligibility. To investigate this, we analyze the sensitivity of the MOS predictions with respect to different levels of corruption in each category. Afterwards, we try fusing the two optimal feature depths to determine if they contain complementary information for MOS prediction. Finally, we compare the performance of the proposed models and assess the generalizability of the models on unseen datasets.
</details>
<details>
<summary>摘要</summary>
在在线会议应用程序中，估计语音信号的感知质量非常重要，以确保用户的品质体验达到最高水平。人类评分是最可靠的质量评估方法，但是这种方法受到劳动力的限制，不适合大规模应用。因此，研究者们的关注点转移到了自动化语音质量评估，通过深度神经网络的端到端训练。最新的研究表明，利用预训练的wav2vec基于XLS-R的嵌入可以达到预测语音质量的状态之 arts。在这篇论文中，我们进行了嵌入的深入分析。首先，我们分析了XLS-R中每层的嵌入表现，以及每个模型的不同大小（300M、1B、2B参数）。奇怪的是，我们发现了两个优化区域：一个在下层特征中，一个在高层特征中。接下来，我们研究了这两个优化区域的原因。我们假设下层特征捕捉了噪音和房间响应的特征，而高层特征则专注于语音内容和理解度。为了证明这一点，我们分析了不同水平的噪音和房间响应对MOS预测的敏感性。然后，我们尝试将这两个优化区域融合，以确定他们是否包含相互补充的信息。最后，我们比较了我们提出的模型，并评估这些模型在未seen数据上的泛化性。
</details></li>
</ul>
<hr>
<h2 id="Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning"><a href="#Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning" class="headerlink" title="Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning"></a>Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11980">http://arxiv.org/abs/2308.11980</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanbo2020/hgrl">https://github.com/yuanbo2020/hgrl</a></li>
<li>paper_authors: Yuanbo Hou, Siyang Song, Cheng Luo, Andrew Mitchell, Qiaoqiao Ren, Weicheng Xie, Jian Kang, Wenwu Wang, Dick Botteldooren</li>
<li>for: This paper is written for the purpose of exploring the integration of objective audio events (AE) with subjective annoyance ratings (AR) of soundscape perceived by humans.</li>
<li>methods: The paper proposes a novel hierarchical graph representation learning (HGRL) approach to link AE with AR. The approach consists of fine-grained event (fAE) embeddings, coarse-grained event (cAE) embeddings, and AR embeddings.</li>
<li>results: The proposed HGRL approach successfully integrates AE with AR for audio event classification (AEC) and audio scene understanding (ARP) tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR.<details>
<summary>Abstract</summary>
Sound events in daily life carry rich information about the objective world. The composition of these sounds affects the mood of people in a soundscape. Most previous approaches only focus on classifying and detecting audio events and scenes, but may ignore their perceptual quality that may impact humans' listening mood for the environment, e.g. annoyance. To this end, this paper proposes a novel hierarchical graph representation learning (HGRL) approach which links objective audio events (AE) with subjective annoyance ratings (AR) of the soundscape perceived by humans. The hierarchical graph consists of fine-grained event (fAE) embeddings with single-class event semantics, coarse-grained event (cAE) embeddings with multi-class event semantics, and AR embeddings. Experiments show the proposed HGRL successfully integrates AE with AR for AEC and ARP tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR.
</details>
<details>
<summary>摘要</summary>
日常生活中的听觉事件携带着 objective 世界的丰富信息。听觉事件的组成会影响人们在听觉景象中的心理状态。先前的方法通常只是对听觉事件和场景进行分类和检测，可能忽略了这些听觉事件对人们听觉环境中的 listening 心理状态的影响，例如厌烦。为此，本文提出了一种新的层次图表学习（HGRL）方法，将 objective 听觉事件（AE）与人们对听觉景象的主观厌烦评分（AR）关联起来。层次图包括细化的事件嵌入（fAE）、中细化的事件嵌入（cAE）和 AR 嵌入。实验显示，提出的 HGRL 方法成功地结合 AE 与 AR  для AEC 和 ARP 任务，同时协调 cAE 和 fAE 之间的关系，并将两种不同的 AE 信息与 AR 进行对应。
</details></li>
</ul>
<hr>
<h2 id="CED-Consistent-ensemble-distillation-for-audio-tagging"><a href="#CED-Consistent-ensemble-distillation-for-audio-tagging" class="headerlink" title="CED: Consistent ensemble distillation for audio tagging"></a>CED: Consistent ensemble distillation for audio tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11957">http://arxiv.org/abs/2308.11957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/richermans/ced">https://github.com/richermans/ced</a></li>
<li>paper_authors: Heinrich Dinkel, Yongqing Wang, Zhiyong Yan, Junbo Zhang, Yujun Wang</li>
<li>for: 提高音频分类任务的性能和减少模型大小</li>
<li>methods: 使用扩展和知识填充（KD）技术，以及一个简单的训练框架称为常规教学（CED）</li>
<li>results: 使用CED训练多种基于变换器的模型，包括一个10M参数模型，在Audioset（AS）上达到49.0的mean average precision（mAP）<details>
<summary>Abstract</summary>
Augmentation and knowledge distillation (KD) are well-established techniques employed in the realm of audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3\% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at https://github.com/RicherMans/CED.
</details>
<details>
<summary>摘要</summary>
🇨🇳 扩展和知识储存（KD）是音频分类任务中常用的技术，可以提高性能和减少模型大小。 although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distills student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model, requiring only 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at https://github.com/RicherMans/CED.Here's the word-for-word translation of the text into Simplified Chinese:🇨🇳 扩展和知识储存（KD）是音频分类任务中常用的技术，可以提高性能和减少模型大小。 although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distills student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model, requiring only 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at https://github.com/RicherMans/CED.
</details></li>
</ul>
<hr>
<h2 id="Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation"><a href="#Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation" class="headerlink" title="Example-Based Framework for Perceptually Guided Audio Texture Generation"></a>Example-Based Framework for Perceptually Guided Audio Texture Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11859">http://arxiv.org/abs/2308.11859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Purnima Kamath, Chitralekha Gupta, Lonce Wyse, Suranga Nanayakkara</li>
<li>for: 本研究的目的是控制生成的音频TEXTURE，通过条件使用标注数据，但是获取标注数据可能是时间consuming和容易出错的。</li>
<li>methods: 本研究提出了一种基于例子的框架，通过用户定义的语义特征来决定生成过程中的控制因素。通过生成一些示例来指示语义特征的存在或缺失，可以在生成过程中找到控制因素的指导向量。</li>
<li>results: 研究表明，该方法可以找到生成过程中的具有语义特征的潜在相关性和决定性指导向量，并应用于其他任务，如选择性 semantic attribute transfer。<details>
<summary>Abstract</summary>
Generative models for synthesizing audio textures explicitly encode controllability by conditioning the model with labelled data. While datasets for audio textures can be easily recorded in-the-wild, semantically labeling them is expensive, time-consuming, and prone to errors due to human annotator subjectivity. Thus, to control generation, there is a need to automatically infer user-defined perceptual factors of variation in the latent space of a generative model while modelling unlabeled textures. In this paper, we propose an example-based framework to determine vectors to guide texture generation based on user-defined semantic attributes. By synthesizing a few synthetic examples to indicate the presence or absence of a semantic attribute, we can infer the guidance vectors in the latent space of a generative model to control that attribute during generation. Our results show that our method is capable of finding perceptually relevant and deterministic guidance vectors for controllable generation for both discrete as well as continuous textures. Furthermore, we demonstrate the application of this method to other tasks such as selective semantic attribute transfer.
</details>
<details>
<summary>摘要</summary>
<<SYS>>用抽象模型生成 audio 文化时，可以显式编码控制性。不过，对 audio 文化的数据进行semantic labeling是costly，time-consuming，和容易出错，因为人工标注者的主观性。因此，要控制生成，需要自动从无标注 texture 中推断用户定义的 Semantic attribute 的变化因素。在这篇论文中，我们提出了一种基于例子的框架，用于确定 guide vector，以控制生成中的 Semantic attribute。通过生成一些synthetic example来指示Semantic attribute的存在或缺失，我们可以在生成过程中推断guide vector的方向。我们的结果表明，我们的方法可以找到可见 relevance 和 deterministic的 guide vector，以便在生成中控制 Semantic attribute。此外，我们还展示了这种方法的应用于其他任务，如选择性 transferred attribute。Note: "Simplified Chinese" is a romanization of the Chinese language that uses a simplified set of characters and pronunciation. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model"><a href="#Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model" class="headerlink" title="Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model"></a>Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11773">http://arxiv.org/abs/2308.11773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuezhou Zhang, Amos A Folarin, Judith Dineley, Pauline Conde, Valeria de Angel, Shaoxiong Sun, Yatharth Ranjan, Zulqarnain Rashid, Callum Stewart, Petroula Laiou, Heet Sankesara, Linglong Qian, Faith Matcham, Katie M White, Carolin Oetzmann, Femke Lamers, Sara Siddi, Sara Simblett, Björn W. Schuller, Srinivasan Vairavan, Til Wykes, Josep Maria Haro, Brenda WJH Penninx, Vaibhav A Narayan, Matthew Hotopf, Richard JB Dobson, Nicholas Cummins, RADAR-CNS consortium</li>
<li>for: The paper is written to investigate the use of natural language processing on social media to predict depression, with a focus on identifying specific speech topics that may indicate depression severity.</li>
<li>methods: The paper uses the Whisper tool and the BERTopic model to analyze 3919 smartphone-collected speech recordings from 265 participants, identifying 29 topics and finding that six of these topics are associated with higher depression severity.</li>
<li>results: The paper finds that specific speech topics can indicate depression severity, and that longitudinally monitoring language use can provide valuable insights into changes in depression severity over time. The study also demonstrates the practicality of using data-driven workflows to collect and analyze large-scale speech data from real-world settings for digital health research.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了研究社交媒体上的自然语言处理，以预测抑郁，并通过特定的语音话题来评估抑郁严重程度。</li>
<li>methods: 这篇论文使用Whisper工具和BERTopic模型分析了3919个手机收集的语音记录，并将其分为29个话题，其中六个话题与抑郁严重程度高有关。</li>
<li>results: 这篇论文发现特定的语音话题可以反映抑郁严重程度，并且 longitudinal 监测语音使用可以为抑郁研究提供有价值的信息。研究还证明了使用数据驱动的工作流程收集和分析大规模语音数据的实用性。<details>
<summary>Abstract</summary>
Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested the BERTopic model on a similar smaller dataset (356 speech recordings from 57 participants), obtaining some consistent results. In summary, our findings demonstrate specific speech topics may indicate depression severity. The presented data-driven workflow provides a practical approach to collecting and analyzing large-scale speech data from real-world settings for digital health research.
</details>
<details>
<summary>摘要</summary>
研究表明语言使用与抑郁有相关性，但大规模验证是需要的。传统的临床研究是昂贵的，因此人工智能技术在社交媒体上进行语言预测是一种可能的方法。然而，这些方法还存在一些限制，包括无效的标签验证、受众样本偏见和缺乏上下文。我们的研究通过使用Whisper工具和BERTopic模型分析了3919个手机收集的语音记录，从265名参与者中提取出29个话题。六个话题的中值PHQ-8大于或等于10被视为抑郁的风险话题：无望、睡眠、心理治疗、剪发、学习和课程。为了解释话题出现和抑郁严重度之间的关系，我们比较了不同话题的行为特征（来自佩戴器）和语言特征。我们还 investigate了话题变化和抑郁严重度变化的时间相关性，这表明了需要长期监测语言使用。我们还在相似的小样本上测试了BERTopic模型，获得了一些相似的结果。总之，我们的发现表明特定的语音话题可能指示抑郁严重度。我们提供的数据驱动的工作流程为整体卫生研究提供了实用的方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/cs.SD_2023_08_23/" data-id="clltaagpb009jr8885u0vhd4b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/eess.AS_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/eess.AS_2023_08_23/">eess.AS - 2023-08-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Analysis-of-XLS-R-for-Speech-Quality-Assessment"><a href="#Analysis-of-XLS-R-for-Speech-Quality-Assessment" class="headerlink" title="Analysis of XLS-R for Speech Quality Assessment"></a>Analysis of XLS-R for Speech Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12077">http://arxiv.org/abs/2308.12077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bastiaan Tamm, Rik Vandenberghe, Hugo Van hamme</li>
</ul>
<p>Abstract:<br>In online conferencing applications, estimating the perceived quality of an audio signal is crucial to ensure high quality of experience for the end user. The most reliable way to assess the quality of a speech signal is through human judgments in the form of the mean opinion score (MOS) metric. However, such an approach is labor intensive and not feasible for large-scale applications. The focus has therefore shifted towards automated speech quality assessment through end-to-end training of deep neural networks. Recently, it was shown that leveraging pre-trained wav2vec-based XLS-R embeddings leads to state-of-the-art performance for the task of speech quality prediction. In this paper, we perform an in-depth analysis of the pre-trained model. First, we analyze the performance of embeddings extracted from each layer of XLS-R and also for each size of the model (300M, 1B, 2B parameters). Surprisingly, we find two optimal regions for feature extraction: one in the lower-level features and one in the high-level features. Next, we investigate the reason for the two distinct optima. We hypothesize that the lower-level features capture characteristics of noise and room acoustics, whereas the high-level features focus on speech content and intelligibility. To investigate this, we analyze the sensitivity of the MOS predictions with respect to different levels of corruption in each category. Afterwards, we try fusing the two optimal feature depths to determine if they contain complementary information for MOS prediction. Finally, we compare the performance of the proposed models and assess the generalizability of the models on unseen datasets.</p>
<hr>
<h2 id="Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning"><a href="#Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning" class="headerlink" title="Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning"></a>Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11980">http://arxiv.org/abs/2308.11980</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanbo2020/hgrl">https://github.com/yuanbo2020/hgrl</a></li>
<li>paper_authors: Yuanbo Hou, Siyang Song, Cheng Luo, Andrew Mitchell, Qiaoqiao Ren, Weicheng Xie, Jian Kang, Wenwu Wang, Dick Botteldooren</li>
</ul>
<p>Abstract:<br>Sound events in daily life carry rich information about the objective world. The composition of these sounds affects the mood of people in a soundscape. Most previous approaches only focus on classifying and detecting audio events and scenes, but may ignore their perceptual quality that may impact humans’ listening mood for the environment, e.g. annoyance. To this end, this paper proposes a novel hierarchical graph representation learning (HGRL) approach which links objective audio events (AE) with subjective annoyance ratings (AR) of the soundscape perceived by humans. The hierarchical graph consists of fine-grained event (fAE) embeddings with single-class event semantics, coarse-grained event (cAE) embeddings with multi-class event semantics, and AR embeddings. Experiments show the proposed HGRL successfully integrates AE with AR for AEC and ARP tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR.</p>
<hr>
<h2 id="CED-Consistent-ensemble-distillation-for-audio-tagging"><a href="#CED-Consistent-ensemble-distillation-for-audio-tagging" class="headerlink" title="CED: Consistent ensemble distillation for audio tagging"></a>CED: Consistent ensemble distillation for audio tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11957">http://arxiv.org/abs/2308.11957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/richermans/ced">https://github.com/richermans/ced</a></li>
<li>paper_authors: Heinrich Dinkel, Yongqing Wang, Zhiyong Yan, Junbo Zhang, Yujun Wang</li>
</ul>
<p>Abstract:<br>Augmentation and knowledge distillation (KD) are well-established techniques employed in the realm of audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn’t been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED’s efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at <a target="_blank" rel="noopener" href="https://github.com/RicherMans/CED">https://github.com/RicherMans/CED</a>.</p>
<hr>
<h2 id="Audio-Generation-with-Multiple-Conditional-Diffusion-Model"><a href="#Audio-Generation-with-Multiple-Conditional-Diffusion-Model" class="headerlink" title="Audio Generation with Multiple Conditional Diffusion Model"></a>Audio Generation with Multiple Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11940">http://arxiv.org/abs/2308.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifang Guo, Jianguo Mao, Rui Tao, Long Yan, Kazushige Ouchi, Hong Liu, Xiangdong Wang</li>
</ul>
<p>Abstract:<br>Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a series of evaluation metrics to evaluate the controllability performance. Experimental results demonstrate that our model successfully achieves fine-grained control to accomplish controllable audio generation. Audio samples and our dataset are publicly available at <a target="_blank" rel="noopener" href="https://conditionaudiogen.github.io/conditionaudiogen/">https://conditionaudiogen.github.io/conditionaudiogen/</a></p>
<hr>
<h2 id="Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement"><a href="#Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement" class="headerlink" title="Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement"></a>Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11923">http://arxiv.org/abs/2308.11923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino</li>
</ul>
<p>Abstract:<br>We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.</p>
<hr>
<h2 id="KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods"><a href="#KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods" class="headerlink" title="KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods"></a>KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11863">http://arxiv.org/abs/2308.11863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Nzeyimana</li>
</ul>
<p>Abstract:<br>Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art to the best of our knowledge. Our experiments also indicate that using syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda.</p>
<hr>
<h2 id="Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation"><a href="#Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation" class="headerlink" title="Example-Based Framework for Perceptually Guided Audio Texture Generation"></a>Example-Based Framework for Perceptually Guided Audio Texture Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11859">http://arxiv.org/abs/2308.11859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Purnima Kamath, Chitralekha Gupta, Lonce Wyse, Suranga Nanayakkara</li>
</ul>
<p>Abstract:<br>Generative models for synthesizing audio textures explicitly encode controllability by conditioning the model with labelled data. While datasets for audio textures can be easily recorded in-the-wild, semantically labeling them is expensive, time-consuming, and prone to errors due to human annotator subjectivity. Thus, to control generation, there is a need to automatically infer user-defined perceptual factors of variation in the latent space of a generative model while modelling unlabeled textures. In this paper, we propose an example-based framework to determine vectors to guide texture generation based on user-defined semantic attributes. By synthesizing a few synthetic examples to indicate the presence or absence of a semantic attribute, we can infer the guidance vectors in the latent space of a generative model to control that attribute during generation. Our results show that our method is capable of finding perceptually relevant and deterministic guidance vectors for controllable generation for both discrete as well as continuous textures. Furthermore, we demonstrate the application of this method to other tasks such as selective semantic attribute transfer.</p>
<hr>
<h2 id="Complex-valued-neural-networks-for-voice-anti-spoofing"><a href="#Complex-valued-neural-networks-for-voice-anti-spoofing" class="headerlink" title="Complex-valued neural networks for voice anti-spoofing"></a>Complex-valued neural networks for voice anti-spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11800">http://arxiv.org/abs/2308.11800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas M. Müller, Philip Sperl, Konstantin Böttinger</li>
</ul>
<p>Abstract:<br>Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the “In-the-Wild” anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing.</p>
<hr>
<h2 id="Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model"><a href="#Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model" class="headerlink" title="Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model"></a>Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11773">http://arxiv.org/abs/2308.11773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuezhou Zhang, Amos A Folarin, Judith Dineley, Pauline Conde, Valeria de Angel, Shaoxiong Sun, Yatharth Ranjan, Zulqarnain Rashid, Callum Stewart, Petroula Laiou, Heet Sankesara, Linglong Qian, Faith Matcham, Katie M White, Carolin Oetzmann, Femke Lamers, Sara Siddi, Sara Simblett, Björn W. Schuller, Srinivasan Vairavan, Til Wykes, Josep Maria Haro, Brenda WJH Penninx, Vaibhav A Narayan, Matthew Hotopf, Richard JB Dobson, Nicholas Cummins, RADAR-CNS consortium</li>
</ul>
<p>Abstract:<br>Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested the BERTopic model on a similar smaller dataset (356 speech recordings from 57 participants), obtaining some consistent results. In summary, our findings demonstrate specific speech topics may indicate depression severity. The presented data-driven workflow provides a practical approach to collecting and analyzing large-scale speech data from real-world settings for digital health research.</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/eess.AS_2023_08_23/" data-id="clltaagps00b8r888dldx9bpv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/eess.IV_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/eess.IV_2023_08_23/">eess.IV - 2023-08-23 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Tumor-Centered-Patching-for-Enhanced-Medical-Image-Segmentation"><a href="#Tumor-Centered-Patching-for-Enhanced-Medical-Image-Segmentation" class="headerlink" title="Tumor-Centered Patching for Enhanced Medical Image Segmentation"></a>Tumor-Centered Patching for Enhanced Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12168">http://arxiv.org/abs/2308.12168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutyyba Asghar, Ahmad Raza Shahid, Akhtar Jamil, Kiran Aftab, Syed Ather Enam</li>
<li>for: 这篇论文旨在提高医疗影像诊断中的Computer-aided diagnosis和手术系统。</li>
<li>methods: 这篇论文使用了一种新的tumor-centered patching方法，将肿瘤作为分析区域，以改善分类不均和边界缺乏的问题。</li>
<li>results: 实验结果显示，这种方法可以改善分类不均， segmentation scores分别为0.78、0.76和0.71 для整体、核心和增强肿瘤。<details>
<summary>Abstract</summary>
The realm of medical image diagnosis has advanced significantly with the integration of computer-aided diagnosis and surgical systems. However, challenges persist, particularly in achieving precise image segmentation. While deep learning techniques show potential, obstacles like limited resources, slow convergence, and class imbalance impede their effectiveness. Traditional patch-based methods, though common, struggle to capture intricate tumor boundaries and often lead to redundant samples, compromising computational efficiency and feature quality. To tackle these issues, this research introduces an innovative approach centered on the tumor itself for patch-based image analysis. This novel tumor-centered patching method aims to address the class imbalance and boundary deficiencies, enabling focused and accurate tumor segmentation. By aligning patches with the tumor's anatomical context, this technique enhances feature extraction accuracy and reduces computational load. Experimental results demonstrate improved class imbalance, with segmentation scores of 0.78, 0.76, and 0.71 for whole, core, and enhancing tumors, respectively using a lightweight simple U-Net. This approach shows potential for enhancing medical image segmentation and improving computer-aided diagnosis systems.
</details>
<details>
<summary>摘要</summary>
医疗图像诊断领域已经得到了计算机支持的辅助诊断和手术系统的整合，但是还存在许多挑战，主要是精准图像分割的问题。深度学习技术表现出了潜在的潜力，但是有限的资源、慢速融合和分类不均等问题使其效果受限。传统的补丁方法，尽管广泛使用，但是它们往往难以捕捉复杂的肿瘤边界，导致重复的样本生成，从而降低计算效率和特征质量。为解决这些问题，本研究提出了一种新的方法，这种方法是基于肿瘤的补丁分析法。这种新的肿瘤中心的补丁方法希图解决分类不均和边界不足的问题，以提高精准的肿瘤分割。通过将补丁与肿瘤的 анатомиче上下文进行对齐，这种技术可以提高特征提取的准确性和降低计算负担。实验结果表明，使用了一种轻量级的简单U-Net，可以提高分类不均的问题， segmentation scores分别为0.78、0.76和0.71 для整体、核心和增强肿瘤。这种方法表现出了在医疗图像分割领域的潜力，并可能用于改进计算机支持的诊断系统。
</details></li>
</ul>
<hr>
<h2 id="DISGAN-Wavelet-informed-Discriminator-Guides-GAN-to-MRI-Super-resolution-with-Noise-Cleaning"><a href="#DISGAN-Wavelet-informed-Discriminator-Guides-GAN-to-MRI-Super-resolution-with-Noise-Cleaning" class="headerlink" title="DISGAN: Wavelet-informed Discriminator Guides GAN to MRI Super-resolution with Noise Cleaning"></a>DISGAN: Wavelet-informed Discriminator Guides GAN to MRI Super-resolution with Noise Cleaning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12084">http://arxiv.org/abs/2308.12084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Wang, Lucas Mahler, Julius Steiglechner, Florian Birk, Klaus Scheffler, Gabriele Lohmann</li>
<li>for: 这个研究是为了提出一个可以同时进行超解析和降噪的深度学习模型，以扩展现有的超解析和降噪模型的能力。</li>
<li>methods: 这个模型使用了一个基于 residual-in-residual 的 generator，以及一个具有3D DWT和1x1卷积的 discriminator。</li>
<li>results: 这个模型可以同时进行高品质的超解析和降噪，并且可以在未见过的MRI数据上进行验证。<details>
<summary>Abstract</summary>
MRI super-resolution (SR) and denoising tasks are fundamental challenges in the field of deep learning, which have traditionally been treated as distinct tasks with separate paired training data. In this paper, we propose an innovative method that addresses both tasks simultaneously using a single deep learning model, eliminating the need for explicitly paired noisy and clean images during training. Our proposed model is primarily trained for SR, but also exhibits remarkable noise-cleaning capabilities in the super-resolved images. Instead of conventional approaches that introduce frequency-related operations into the generative process, our novel approach involves the use of a GAN model guided by a frequency-informed discriminator. To achieve this, we harness the power of the 3D Discrete Wavelet Transform (DWT) operation as a frequency constraint within the GAN framework for the SR task on magnetic resonance imaging (MRI) data. Specifically, our contributions include: 1) a 3D generator based on residual-in-residual connected blocks; 2) the integration of the 3D DWT with $1\times 1$ convolution into a DWT+conv unit within a 3D Unet for the discriminator; 3) the use of the trained model for high-quality image SR, accompanied by an intrinsic denoising process. We dub the model "Denoising Induced Super-resolution GAN (DISGAN)" due to its dual effects of SR image generation and simultaneous denoising. Departing from the traditional approach of training SR and denoising tasks as separate models, our proposed DISGAN is trained only on the SR task, but also achieves exceptional performance in denoising. The model is trained on 3D MRI data from dozens of subjects from the Human Connectome Project (HCP) and further evaluated on previously unseen MRI data from subjects with brain tumours and epilepsy to assess its denoising and SR performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>基于 residual-in-residual 的 3D generator，使用connected块来实现高质量的 SR 图像生成。2. 将 3D DWT 与 1x1  convolution 结合在一起，形成 DWT+conv 单元，并将其 integrate into 3D Unet 中的权重来实现高精度的 SR 预测。3. 使用训练好的模型进行高质量的图像 SR，同时实现了内在的噪声除除过程。我们称这种模型为 “Denoising Induced Super-resolution GAN”（DISGAN），因为它同时实现了 SR 图像生成和噪声除除。不同于传统的方法，我们的 DISGAN 只受 SR 任务培训，同时也可以在未看过的 MRI 数据上实现出色的噪声除除和 SR 性能。我们在 Human Connectome Project（HCP） 提供的3D MRI数据上进行了训练，并在患有脑肿和癫痫的患者的 MRI 数据上进行了评估，以评估其噪声除除和 SR 性能。</details></li>
</ol>
<hr>
<h2 id="StofNet-Super-resolution-Time-of-Flight-Network"><a href="#StofNet-Super-resolution-Time-of-Flight-Network" class="headerlink" title="StofNet: Super-resolution Time of Flight Network"></a>StofNet: Super-resolution Time of Flight Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12009">http://arxiv.org/abs/2308.12009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hahnec/stofnet">https://github.com/hahnec/stofnet</a></li>
<li>paper_authors: Christopher Hahne, Michel Hayoz, Raphael Sznitman</li>
<li>for: 该论文主要针对时间飞行（ToF）感知技术在 робо测试、医学成像和非 destruktive testing 等领域中的问题，即在复杂的 ambient condition 下，从简单的时间信息中进行逆模拟是不可能的。</li>
<li>methods: 该论文提出了一种现代超解像技术来学习困难 ambient condition，以提高ToF感知的可靠性和准确性。具体来说， authors 提出了一种结合超解像和高效减弱块的架构，以平衡细详信号的细节和大规模的上下文信息。</li>
<li>results: 该论文通过对六种现有方法进行比较，并使用两个公共可用的数据集进行测试，证明了提出的 StofNet 方法在精度、可靠性和模型复杂度三个方面具有显著的优势。code 可以在 <a target="_blank" rel="noopener" href="https://github.com/hahnec/stofnet">https://github.com/hahnec/stofnet</a> 上下载。<details>
<summary>Abstract</summary>
Time of Flight (ToF) is a prevalent depth sensing technology in the fields of robotics, medical imaging, and non-destructive testing. Yet, ToF sensing faces challenges from complex ambient conditions making an inverse modelling from the sparse temporal information intractable. This paper highlights the potential of modern super-resolution techniques to learn varying surroundings for a reliable and accurate ToF detection. Unlike existing models, we tailor an architecture for sub-sample precise semi-global signal localization by combining super-resolution with an efficient residual contraction block to balance between fine signal details and large scale contextual information. We consolidate research on ToF by conducting a benchmark comparison against six state-of-the-art methods for which we employ two publicly available datasets. This includes the release of our SToF-Chirp dataset captured by an airborne ultrasound transducer. Results showcase the superior performance of our proposed StofNet in terms of precision, reliability and model complexity. Our code is available at https://github.com/hahnec/stofnet.
</details>
<details>
<summary>摘要</summary>
时间飞行（ToF）是现代深度探测技术的重要应用领域，包括机器人、医学成像和非 destruktive testing。然而，ToF探测受到环境复杂性的影响，使得反向模型从稀疏的时间信息中做出准确的探测变得困难。本文提出了现代超分解技术的潜在作用，以提高ToF探测的可靠性和准确性。与现有模型不同，我们开发了一种结构，即StofNet，通过结合超分解和高效的剩余压缩块来平衡细信息和大规模的上下文信息。我们在六种state-of-the-art方法的基准比较中，使用了两个公共可用的数据集。这包括我们发布的SToF-Chirp数据集， capture by an airborne ultrasound transducer。结果表明我们提posed StofNet在精度、可靠性和模型复杂度方面表现出色。我们的代码可以在https://github.com/hahnec/stofnet中下载。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Autoencoder-to-Geometrical-Features-for-Vascular-Bifurcations-Identification"><a href="#Comparing-Autoencoder-to-Geometrical-Features-for-Vascular-Bifurcations-Identification" class="headerlink" title="Comparing Autoencoder to Geometrical Features for Vascular Bifurcations Identification"></a>Comparing Autoencoder to Geometrical Features for Vascular Bifurcations Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12314">http://arxiv.org/abs/2308.12314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibtissam Essadik, Anass Nouri, Raja Touahni, Florent Autrusseau</li>
<li>for: 这个论文的目的是提出两种基于自动Encoder和几何特征的新方法来识别血管分叉。</li>
<li>methods: 这两种方法分别使用自动Encoder和几何特征来提取特征和识别模式。</li>
<li>results: 经过评估，两种方法在使用医疗影像数据进行血管分叉分类中具有良好的性能和效果，其中自动Encoder方法的准确率和F1分数较高。<details>
<summary>Abstract</summary>
The cerebrovascular tree is a complex anatomical structure that plays a crucial role in the brain irrigation. A precise identification of the bifurcations in the vascular network is essential for understanding various cerebral pathologies. Traditional methods often require manual intervention and are sensitive to variations in data quality. In recent years, deep learning techniques, and particularly autoencoders, have shown promising performances for feature extraction and pattern recognition in a variety of domains. In this paper, we propose two novel approaches for vascular bifurcation identification based respectiveley on Autoencoder and geometrical features. The performance and effectiveness of each method in terms of classification of vascular bifurcations using medical imaging data is presented. The evaluation was performed on a sample database composed of 91 TOF-MRA, using various evaluation measures, including accuracy, F1 score and confusion matrix.
</details>
<details>
<summary>摘要</summary>
脑血管树是一种复杂的生物结构，对脑血液循环具有关键作用。正确地识别血管网络中的分枝是理解脑血液疾病的关键。传统方法经常需要手动干预，并且敏感于数据质量的变化。在最近几年，深度学习技术和特别是自动编码器在多种领域中表现出了扎实的功能。本文提出了两种基于自动编码器和几何特征的血管分枝识别方法。每种方法的性能和效果在使用医疗影像数据进行血管分枝分类中进行了评估，并使用了几种评价指标，包括准确率、F1分数和混淆矩阵。
</details></li>
</ul>
<hr>
<h2 id="Recovering-a-Molecule’s-3D-Dynamics-from-Liquid-phase-Electron-Microscopy-Movies"><a href="#Recovering-a-Molecule’s-3D-Dynamics-from-Liquid-phase-Electron-Microscopy-Movies" class="headerlink" title="Recovering a Molecule’s 3D Dynamics from Liquid-phase Electron Microscopy Movies"></a>Recovering a Molecule’s 3D Dynamics from Liquid-phase Electron Microscopy Movies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11927">http://arxiv.org/abs/2308.11927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Ye, Yuhang Wang, Hong Zhang, Yiqin Gao, Huan Wang, He Sun</li>
<li>for: 这研究旨在使用liquid-phase electron microscopy（liquid-phase EM）技术观察生物分子的动态变化。</li>
<li>methods: 该研究提出了TEMPOR算法，它是一种基于偶极神经网络（INR）和动态变量自适应器（DVAE）的时间序列分子结构重建方法。</li>
<li>results: 研究人员通过对两个 simulate数据集（7bcq和Cas9）进行测试，发现TEMPOR算法可以有效地回收不同的动态变化。这是首个直接从liquid-phase EM电影中回收动态变化的3D结构的研究，它为结构生物学提供了一个有前途的新方法。<details>
<summary>Abstract</summary>
The dynamics of biomolecules are crucial for our understanding of their functioning in living systems. However, current 3D imaging techniques, such as cryogenic electron microscopy (cryo-EM), require freezing the sample, which limits the observation of their conformational changes in real time. The innovative liquid-phase electron microscopy (liquid-phase EM) technique allows molecules to be placed in the native liquid environment, providing a unique opportunity to observe their dynamics. In this paper, we propose TEMPOR, a Temporal Electron MicroscoPy Object Reconstruction algorithm for liquid-phase EM that leverages an implicit neural representation (INR) and a dynamical variational auto-encoder (DVAE) to recover time series of molecular structures. We demonstrate its advantages in recovering different motion dynamics from two simulated datasets, 7bcq and Cas9. To our knowledge, our work is the first attempt to directly recover 3D structures of a temporally-varying particle from liquid-phase EM movies. It provides a promising new approach for studying molecules' 3D dynamics in structural biology.
</details>
<details>
<summary>摘要</summary>
生物分子动态是我们理解它们在生物系统中功能的关键。然而，现有的3D图像技术，如气化电子顾 microscopy (cryo-EM)，需要采样冻结，限制观察分子 conformational 变化的实时观察。新的液相电子顾 microscopy (liquid-phase EM) 技术可以将分子放在原生液态环境中，提供了观察分子动态的独特机会。在这篇论文中，我们提出了 TEMPOR，一种基于 implicit neural representation (INR) 和动态variational autoencoder (DVAE) 的 Temporal Electron MicroscoPy Object Reconstruction算法，可以从液相电子顾 movie 中回收时间序列的分子结构。我们在两个 simulated 数据集，7bcq 和 Cas9 中，证明了它的优势。到目前为止，我们的工作是直接从液相电子顾 movie 中回收变化的3D结构的第一次尝试。它提供了一种有前途的新方法，用于生物学结构中的分子3D动态研究。
</details></li>
</ul>
<hr>
<h2 id="Studying-the-Impact-of-Augmentations-on-Medical-Confidence-Calibration"><a href="#Studying-the-Impact-of-Augmentations-on-Medical-Confidence-Calibration" class="headerlink" title="Studying the Impact of Augmentations on Medical Confidence Calibration"></a>Studying the Impact of Augmentations on Medical Confidence Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11902">http://arxiv.org/abs/2308.11902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrit Rao, Joon-Young Lee, Oliver Aalami</li>
<li>for: 这 paper 的目的是evaluate the effects of three modern augmentation techniques on the calibration and performance of convolutional neural networks (CNNs) for medical tasks.</li>
<li>methods: 这 paper 使用了三种现代扩展技术：CutMix、MixUp 和 CutOut，以提高 CNNs 的准确率和准确性。</li>
<li>results: 研究发现，CutMix 最大程度地提高了 CNNs 的准确性，而 CutOut 有时会降低准确性。<details>
<summary>Abstract</summary>
The clinical explainability of convolutional neural networks (CNN) heavily relies on the joint interpretation of a model's predicted diagnostic label and associated confidence. A highly certain or uncertain model can significantly impact clinical decision-making. Thus, ensuring that confidence estimates reflect the true correctness likelihood for a prediction is essential. CNNs are often poorly calibrated and prone to overconfidence leading to improper measures of uncertainty. This creates the need for confidence calibration. However, accuracy and performance-based evaluations of CNNs are commonly used as the sole benchmark for medical tasks. Taking into consideration the risks associated with miscalibration is of high importance. In recent years, modern augmentation techniques, which cut, mix, and combine images, have been introduced. Such augmentations have benefited CNNs through regularization, robustness to adversarial samples, and calibration. Standard augmentations based on image scaling, rotating, and zooming, are widely leveraged in the medical domain to combat the scarcity of data. In this paper, we evaluate the effects of three modern augmentation techniques, CutMix, MixUp, and CutOut on the calibration and performance of CNNs for medical tasks. CutMix improved calibration the most while CutOut often lowered the level of calibration.
</details>
<details>
<summary>摘要</summary>
医学预测模型（Convolutional Neural Network，简称CNN）的解释性强调与预测结果和相关的信任度之间的共同解释。一个高度确定或不确定的模型可能会对临床决策产生重大影响。因此，确保模型的信任度反映预测的准确性 likelihood 是关键的。然而， CNNs  oftensuffer from poor calibration and overconfidence, leading to inappropriate measures of uncertainty. This creates the need for confidence calibration. However, accuracy and performance-based evaluations of CNNs are commonly used as the sole benchmark for medical tasks. Considering the risks associated with miscalibration is of high importance.Recently, modern augmentation techniques, such as CutMix, MixUp, and CutOut, have been introduced to improve the calibration and performance of CNNs. These techniques have been shown to benefit CNNs through regularization, robustness to adversarial samples, and calibration. Standard augmentations based on image scaling, rotating, and zooming are widely used in the medical domain to address the scarcity of data. In this paper, we evaluate the effects of these three modern augmentation techniques on the calibration and performance of CNNs for medical tasks. Our results show that CutMix improved calibration the most, while CutOut often lowered the level of calibration.
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Residual-SwinV2-Transformer-for-Learned-Image-Compression"><a href="#Enhanced-Residual-SwinV2-Transformer-for-Learned-Image-Compression" class="headerlink" title="Enhanced Residual SwinV2 Transformer for Learned Image Compression"></a>Enhanced Residual SwinV2 Transformer for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11864">http://arxiv.org/abs/2308.11864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqiang Wang, Feng Liang, Haisheng Fu, Jie Liang, Haipeng Qin, Junzhe Liang</li>
<li>for: 提高图像压缩的率和质量之间的折衔，并且减少模型复杂度。</li>
<li>methods: 使用改进的差异Swinv2 transformer和特征增强模块，并在编码和超编码步骤中使用SwinV2 transformer-based attention机制。</li>
<li>results: 在Kodak和Tecnick数据集上实现了与一些最新的学习型图像压缩方法相当的性能，并且比一些传统的编码器更高。具体来说，我们的方法在同等性能下减少了56%的模型复杂度。<details>
<summary>Abstract</summary>
Recently, the deep learning technology has been successfully applied in the field of image compression, leading to superior rate-distortion performance. However, a challenge of many learning-based approaches is that they often achieve better performance via sacrificing complexity, which making practical deployment difficult. To alleviate this issue, in this paper, we propose an effective and efficient learned image compression framework based on an enhanced residual Swinv2 transformer. To enhance the nonlinear representation of images in our framework, we use a feature enhancement module that consists of three consecutive convolutional layers. In the subsequent coding and hyper coding steps, we utilize a SwinV2 transformer-based attention mechanism to process the input image. The SwinV2 model can help to reduce model complexity while maintaining high performance. Experimental results show that the proposed method achieves comparable performance compared to some recent learned image compression methods on Kodak and Tecnick datasets, and outperforms some traditional codecs including VVC. In particular, our method achieves comparable results while reducing model complexity by 56% compared to these recent methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-RF-Data-Normalization-for-Deep-Learning"><a href="#Robust-RF-Data-Normalization-for-Deep-Learning" class="headerlink" title="Robust RF Data Normalization for Deep Learning"></a>Robust RF Data Normalization for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11833">http://arxiv.org/abs/2308.11833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Sharifzadeh, Habib Benali, Hassan Rivaz</li>
<li>for: 用于深度神经网络的训练</li>
<li>methods: 使用个体标准化方法更好地利用RF数据</li>
<li>results: 提高深度神经网络的性能和通用性<details>
<summary>Abstract</summary>
Radio frequency (RF) data contain richer information compared to other data types, such as envelope or B-mode, and employing RF data for training deep neural networks has attracted growing interest in ultrasound image processing. However, RF data is highly fluctuating and additionally has a high dynamic range. Most previous studies in the literature have relied on conventional data normalization, which has been adopted within the computer vision community. We demonstrate the inadequacy of those techniques for normalizing RF data and propose that individual standardization of each image substantially enhances the performance of deep neural networks by utilizing the data more efficiently. We compare conventional and proposed normalizations in a phase aberration correction task and illustrate how the former enhances the generality of trained models.
</details>
<details>
<summary>摘要</summary>
radio frequency (RF) 数据含有更多信息，比如拥包或 B-模式数据类型，使用 RF 数据来训练深度神经网络已经吸引了各种各样的关注。然而，RF 数据具有很大的波动和动态范围。大多数先前的文献中的研究都采用了传统的数据Normalization技术。我们证明了这些技术不适用于 RF 数据Normalization，并提出了基于每个图像的个体标准化方法，可以更好地利用数据。我们在相位偏移 corrections 任务中比较了传统和我们提议的Normalization方法，并证明了后者可以提高训练的模型通用性。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Space-Prediction-Filtering-for-Phase-Aberration-Correction-in-Plane-Wave-Ultrasound"><a href="#Frequency-Space-Prediction-Filtering-for-Phase-Aberration-Correction-in-Plane-Wave-Ultrasound" class="headerlink" title="Frequency-Space Prediction Filtering for Phase Aberration Correction in Plane-Wave Ultrasound"></a>Frequency-Space Prediction Filtering for Phase Aberration Correction in Plane-Wave Ultrasound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11830">http://arxiv.org/abs/2308.11830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Sharifzadeh, Habib Benali, Hassan Rivaz</li>
<li>For: 本研究旨在解决ultrasound imaging中的图像质量下降问题，具体来说是应对phas aberration的影响。* Methods: 本研究使用了frequency-space prediction filtering（FXPF）技术来缓解phas aberration的影响。FXPF假设存在一个自回归（AR）模型，用于描述接收器元素上的信号。* Results: 研究发现，在深度较浅的情况下，使用固定频率AR模型可能会导致图像重建的性能下降。为了解决这个问题，研究提出了一种自适应频率AR模型，并评估了其效果使用对比度和总对比度评价指标。<details>
<summary>Abstract</summary>
Ultrasound imaging often suffers from image degradation stemming from phase aberration, which represents a significant contributing factor to the overall image degradation in ultrasound imaging. Frequency-space prediction filtering or FXPF is a technique that has been applied within focused ultrasound imaging to alleviate the phase aberration effect. It presupposes the existence of an autoregressive (AR) model across the signals received at the transducer elements and removes any components that do not conform to the established model. In this study, we illustrate the challenge of applying this technique to plane-wave imaging, where, at shallower depths, signals from more distant elements lose relevance, and a fewer number of elements contribute to image reconstruction. While the number of contributing signals varies, adopting a fixed-order AR model across all depths, results in suboptimal performance. To address this challenge, we propose an AR model with an adaptive order and quantify its effectiveness using contrast and generalized contrast-to-noise ratio metrics.
</details>
<details>
<summary>摘要</summary>
ultrasound imaging经常受到阶段偏移引起的图像强化效应，这是ultrasound imaging中图像强化效应的重要贡献因素。frequency-space prediction filtering或FXPF是一种在高精度ultrasound imaging中应用的技术，以解决阶段偏移效应。它假设在传感器元素上接收的信号存在autoregressive（AR）模型，并从不符合该模型的组件中除掉噪声。在这种研究中，我们描述了应用FXPF技术到平面波形成像中的挑战，深度较浅的情况下，较远的传感器元素的信号失去了 relevance，而一些元素只能为图像重建做出贡献。尽管参与图像重建的信号数量变化，采用固定阶数AR模型在所有深度下的结果是不佳。为解决这个挑战，我们提议一种AR模型，其阶数随深度变化，并使用对比度和通用对比度-噪声比例度量来衡量其效果。
</details></li>
</ul>
<hr>
<h2 id="WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters"><a href="#WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters" class="headerlink" title="WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters"></a>WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11776">http://arxiv.org/abs/2308.11776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ange Lou, Jack Noble</li>
<li>for: 这个研究旨在建立一个自我超级vised的深度和镜头积极定位系统，能够预测精确的深度地图和镜头积极。</li>
<li>methods: 本研究使用了一种基于成本量的超级vised方法，并通过一种自动生成的类比对照方法来提供辅助的超级vised。</li>
<li>results: 实验结果显示，提案的方法可以改善镜头积极、深度估计和镜头内 Parameters 的预测精度。<details>
<summary>Abstract</summary>
Depth estimation in surgical video plays a crucial role in many image-guided surgery procedures. However, it is difficult and time consuming to create depth map ground truth datasets in surgical videos due in part to inconsistent brightness and noise in the surgical scene. Therefore, building an accurate and robust self-supervised depth and camera ego-motion estimation system is gaining more attention from the computer vision community. Although several self-supervision methods alleviate the need for ground truth depth maps and poses, they still need known camera intrinsic parameters, which are often missing or not recorded. Moreover, the camera intrinsic prediction methods in existing works depend heavily on the quality of datasets. In this work, we aimed to build a self-supervised depth and ego-motion estimation system which can predict not only accurate depth maps and camera pose, but also camera intrinsic parameters. We proposed a cost-volume-based supervision manner to give the system auxiliary supervision for camera parameters prediction. The experimental results showed that the proposed method improved the accuracy of estimated camera parameters, ego-motion, and depth estimation.
</details>
<details>
<summary>摘要</summary>
深度估计在手术视频中发挥重要作用，但创建深度图真实数据集在手术视频中具有许多挑战，包括手术场景中的不均匀亮度和噪声。因此，建立一个准确和可靠的自我超视导depth和摄像头自身运动估计系统在计算机视觉领域中受到更多的关注。虽然一些自我超视方法可以减少深度图和摄像头pose的需求，但它们仍需要已知的摄像头内参数，这些参数通常缺失或未记录。此外，现有的摄像头内参数预测方法仍然受到数据质量的限制。在这个工作中，我们目的是建立一个可以预测深度图、摄像头pose和摄像头内参数的自我超视depth和摄像头估计系统。我们提议一种基于cost volume的超视束来给系统 auxiliary supervision for camera parameters预测。实验结果表明，我们的方法可以改善摄像头参数、ego-动作和深度估计的准确性。
</details></li>
</ul>
<hr>
<h2 id="EndoNet-model-for-automatic-calculation-of-H-score-on-histological-slides"><a href="#EndoNet-model-for-automatic-calculation-of-H-score-on-histological-slides" class="headerlink" title="EndoNet: model for automatic calculation of H-score on histological slides"></a>EndoNet: model for automatic calculation of H-score on histological slides</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11562">http://arxiv.org/abs/2308.11562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Egor Ushakov, Anton Naumov, Vladislav Fomberg, Polina Vishnyakova, Aleksandra Asaturova, Alina Badlaeva, Anna Tregubova, Evgeny Karpulevich, Gennady Sukhikh, Timur Fatkhudinov</li>
<li>for: 这个论文主要是为了提高检验病理slide的效率和准确性，使用计算机支持的方法来自动计算H-score。</li>
<li>methods: 该论文提出了一种基于神经网络的模型EndoNet，它包括两个主要部分：首先是一个检测模型，用于预测核心点的位置；其次是一个H-score模块，用于根据预测的核心点的平均像素值来计算H-score。</li>
<li>results: 该模型在1780个注解的块中训练和验证，并在测试集上达到了0.77的mAP。此外，该模型可以根据特定的专家或实验室来调整H-score的计算方式，从而提高了模型的可靠性和可重复性。<details>
<summary>Abstract</summary>
H-score is a semi-quantitative method used to assess the presence and distribution of proteins in tissue samples by combining the intensity of staining and percentage of stained nuclei. It is widely used but time-consuming and can be limited in accuracy and precision. Computer-aided methods may help overcome these limitations and improve the efficiency of pathologists' workflows. In this work, we developed a model EndoNet for automatic calculation of H-score on histological slides. Our proposed method uses neural networks and consists of two main parts. The first is a detection model which predicts keypoints of centers of nuclei. The second is a H-score module which calculates the value of the H-score using mean pixel values of predicted keypoints. Our model was trained and validated on 1780 annotated tiles with a shape of 100x100 $\mu m$ and performed 0.77 mAP on a test dataset. Moreover, the model can be adjusted to a specific specialist or whole laboratory to reproduce the manner of calculating the H-score. Thus, EndoNet is effective and robust in the analysis of histology slides, which can improve and significantly accelerate the work of pathologists.
</details>
<details>
<summary>摘要</summary>
“H-score”是一种半量化方法，用于评估组织样本中蛋白质的存在和分布。它广泛使用，但时间费时且准确性和精度有限。计算机助け方法可以帮助解决这些限制，提高病理师的工作效率。在这项工作中，我们开发了一个名为“EndoNet”的自动计算H-score方法。我们的提案方法使用神经网络，包括两个主要部分。第一部分是一个检测模型，预测核心点的位置。第二部分是H-score模块，使用预测的核心点的平均像素值来计算H-score的值。我们的模型在1780个注解的块中训练和验证，在测试集上达到了0.77 mAP。此外，模型可以根据特定的专家或整个实验室来调整计算H-score的方式，因此EndoNet是有效和可靠的 histology 板块分析工具，可以提高和加速病理师的工作。
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Synthetic-Image-Source-Attribution"><a href="#Open-Set-Synthetic-Image-Source-Attribution" class="headerlink" title="Open Set Synthetic Image Source Attribution"></a>Open Set Synthetic Image Source Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11557">http://arxiv.org/abs/2308.11557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbang Fang, Tai D. Nguyen, Matthew C. Stamm</li>
<li>for: 本研究旨在开发一种基于度量学习的开放集成源归属分析方法，以检测和识别新未经见的图像生成器。</li>
<li>methods: 本研究使用度量学习来学习可转移的嵌入，以区分不同的图像生成器。首先将图像分配给候选生成器，然后根据图像与已知生成器学习的参考点的距离来判断是否来自新的生成器。</li>
<li>results: 经过一系列实验，本研究表明了该方法在开放集成源归属场景中能够准确地检测和识别新未经见的图像生成器。<details>
<summary>Abstract</summary>
AI-generated images have become increasingly realistic and have garnered significant public attention. While synthetic images are intriguing due to their realism, they also pose an important misinformation threat. To address this new threat, researchers have developed multiple algorithms to detect synthetic images and identify their source generators. However, most existing source attribution techniques are designed to operate in a closed-set scenario, i.e. they can only be used to discriminate between known image generators. By contrast, new image-generation techniques are rapidly emerging. To contend with this, there is a great need for open-set source attribution techniques that can identify when synthetic images have originated from new, unseen generators. To address this problem, we propose a new metric learning-based approach. Our technique works by learning transferrable embeddings capable of discriminating between generators, even when they are not seen during training. An image is first assigned to a candidate generator, then is accepted or rejected based on its distance in the embedding space from known generators' learned reference points. Importantly, we identify that initializing our source attribution embedding network by pretraining it on image camera identification can improve our embeddings' transferability. Through a series of experiments, we demonstrate our approach's ability to attribute the source of synthetic images in open-set scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/eess.IV_2023_08_23/" data-id="clltaagr100emr888hk854a8f" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eessp.SP_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/eessp.SP_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/eessp.SP_2023_08_23/">eessp.SP - 2023-08-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        
      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/eessp.SP_2023_08_23/" data-id="clltaagr300eor88820800yoa" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/22/cs.AI_2023_08_22/" class="article-date">
  <time datetime="2023-08-21T16:00:00.000Z" itemprop="datePublished">2023-08-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/22/cs.AI_2023_08_22/">cs.AI - 2023-08-22 20:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Furnishing-Sound-Event-Detection-with-Language-Model-Abilities"><a href="#Furnishing-Sound-Event-Detection-with-Language-Model-Abilities" class="headerlink" title="Furnishing Sound Event Detection with Language Model Abilities"></a>Furnishing Sound Event Detection with Language Model Abilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11530">http://arxiv.org/abs/2308.11530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hualei Wang, Jianguo Mao, Zhifang Guo, Jiarui Wan, Hong Liu, Xiangdong Wang</li>
<li>for: 本研究探讨语言模型（LM）在视觉跨模态中的能力，特别是sound event detection（SED）领域。</li>
<li>methods: 我们提出了一种简洁的方法，通过对音频特征和文本特征的对应进行对齐，实现声音事件分类和时间位置的生成。该框架包括一个音频编码器、一个对应模块和一个独立的语言解码器。</li>
<li>results: 我们的模型可以准确地生成声音事件探测序列。与传统方法相比，我们的模型更加简洁和全面，因为它直接利用语言模型的 semantic 能力来生成序列。我们还对不同的解码模块进行了研究，以示timestamps capture和事件分类的效果。<details>
<summary>Abstract</summary>
Recently, the ability of language models (LMs) has attracted increasing attention in visual cross-modality. In this paper, we further explore the generation capacity of LMs for sound event detection (SED), beyond the visual domain. Specifically, we propose an elegant method that aligns audio features and text features to accomplish sound event classification and temporal location. The framework consists of an acoustic encoder, a contrastive module that align the corresponding representations of the text and audio, and a decoupled language decoder that generates temporal and event sequences from the audio characteristic. Compared with conventional works that require complicated processing and barely utilize limited audio features, our model is more concise and comprehensive since language model directly leverage its semantic capabilities to generate the sequences. We investigate different decoupling modules to demonstrate the effectiveness for timestamps capture and event classification. Evaluation results show that the proposed method achieves accurate sequences of sound event detection.
</details>
<details>
<summary>摘要</summary>
最近，语言模型（LM）在视觉交互领域的能力受到了越来越多的关注。在这篇论文中，我们进一步探索语言模型对声音事件检测（SED）的生成能力，超出视觉领域。我们提出了一种简洁的方法，将音频特征和文本特征进行对齐，以完成声音事件类型和时间位置的分类。该框架包括一个声音编码器、一个对应模块，将文本和音频特征的对应表示进行对齐，以及一个独立的语言解码器，从音频特征中生成时间序列和事件序列。相比于传统的方法，需要复杂的处理和尝试用有限的音频特征，我们的模型更简洁和全面，因为语言模型直接利用其语义能力来生成序列。我们 investigate了不同的解 Coupling模块，以示出对时间捕捉和事件分类的效果。评估结果显示，我们的方法可以准确地检测声音事件。
</details></li>
</ul>
<hr>
<h2 id="TrackFlow-Multi-Object-Tracking-with-Normalizing-Flows"><a href="#TrackFlow-Multi-Object-Tracking-with-Normalizing-Flows" class="headerlink" title="TrackFlow: Multi-Object Tracking with Normalizing Flows"></a>TrackFlow: Multi-Object Tracking with Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11513">http://arxiv.org/abs/2308.11513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianluca Mancusi, Aniello Panariello, Angelo Porrello, Matteo Fabbri, Simone Calderara, Rita Cucchiara</li>
<li>for: 提高多对象跟踪的性能，尤其是在多模态 Setting 中。</li>
<li>methods: 使用深度概率模型来计算候选对应关系的可能性，以提高跟踪-by-检测算法的性能。</li>
<li>results: 在 simulate 和实际 benchmark 上进行了实验，显示了我们的方法可以提高跟踪-by-检测算法的性能。<details>
<summary>Abstract</summary>
The field of multi-object tracking has recently seen a renewed interest in the good old schema of tracking-by-detection, as its simplicity and strong priors spare it from the complex design and painful babysitting of tracking-by-attention approaches. In view of this, we aim at extending tracking-by-detection to multi-modal settings, where a comprehensive cost has to be computed from heterogeneous information e.g., 2D motion cues, visual appearance, and pose estimates. More precisely, we follow a case study where a rough estimate of 3D information is also available and must be merged with other traditional metrics (e.g., the IoU). To achieve that, recent approaches resort to either simple rules or complex heuristics to balance the contribution of each cost. However, i) they require careful tuning of tailored hyperparameters on a hold-out set, and ii) they imply these costs to be independent, which does not hold in reality. We address these issues by building upon an elegant probabilistic formulation, which considers the cost of a candidate association as the negative log-likelihood yielded by a deep density estimator, trained to model the conditional joint probability distribution of correct associations. Our experiments, conducted on both simulated and real benchmarks, show that our approach consistently enhances the performance of several tracking-by-detection algorithms.
</details>
<details>
<summary>摘要</summary>
隐身多目标跟踪领域最近又有新的关注，旧的schema tracking-by-detection，因为它的简单性和强制约束，不需要复杂的设计和痛苦照顾 tracking-by-attention 方法。在这个视图下，我们想扩展 tracking-by-detection 到多模式设定，其中需要从不同的信息源（例如，2D 运动指示、视觉特征和姿态估计）计算总成本。更加准确地说，我们采用了一个实验研究，其中有一个粗略的3D 信息估计也可以与传统的 метри（例如，IoU）一起使用。为了实现这一点，现有的方法通常采用 either simple rules or complex heuristics 来均衡每个成本的贡献。然而，i) 它们需要在保留集上精心调整特制的超参数，并 ii) 它们假设这些成本是独立的，而实际上不是。我们解决这些问题，是通过基于简洁概率形式ulation，它考虑候选关联的成本为负极log-概率的深度概率预测器，用于模型候选关联的条件联合概率分布。我们的实验，在 Both simulated 和 real  benchmarks 上进行，显示了我们的方法能够一致提高许多 tracking-by-detection 算法的性能。
</details></li>
</ul>
<hr>
<h2 id="User-Identity-Linkage-in-Social-Media-Using-Linguistic-and-Social-Interaction-Features"><a href="#User-Identity-Linkage-in-Social-Media-Using-Linguistic-and-Social-Interaction-Features" class="headerlink" title="User Identity Linkage in Social Media Using Linguistic and Social Interaction Features"></a>User Identity Linkage in Social Media Using Linguistic and Social Interaction Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11684">http://arxiv.org/abs/2308.11684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Despoina Chatzakou, Juan Soler-Company, Theodora Tsikrika, Leo Wanner, Stefanos Vrochidis, Ioannis Kompatsiaris</li>
<li>for: 防止社交媒体上的负面内容的 spreadof and retain online identity</li>
<li>methods: 使用多个用户活动特征进行机器学习基于检测，以确定两个或多个虚拟标识是否属于同一个真实人</li>
<li>results: 在恶意和恐怖主义相关的推特内容中，模型的效果得到证明<details>
<summary>Abstract</summary>
Social media users often hold several accounts in their effort to multiply the spread of their thoughts, ideas, and viewpoints. In the particular case of objectionable content, users tend to create multiple accounts to bypass the combating measures enforced by social media platforms and thus retain their online identity even if some of their accounts are suspended. User identity linkage aims to reveal social media accounts likely to belong to the same natural person so as to prevent the spread of abusive/illegal activities. To this end, this work proposes a machine learning-based detection model, which uses multiple attributes of users' online activity in order to identify whether two or more virtual identities belong to the same real natural person. The models efficacy is demonstrated on two cases on abusive and terrorism-related Twitter content.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Language-Models-Sensitivity-to-The-Order-of-Options-in-Multiple-Choice-Questions"><a href="#Large-Language-Models-Sensitivity-to-The-Order-of-Options-in-Multiple-Choice-Questions" class="headerlink" title="Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions"></a>Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11483">http://arxiv.org/abs/2308.11483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pouya Pezeshkpour, Estevam Hruschka<br>for: 这 paper 探讨了 Large Language Models (LLMs) 在不同的 NLP 任务中表现的稳定性问题，特别是在多选问题上。methods: 作者们使用了多种方法来 investigate LLMs 的不稳定性，包括对选项的重新排序和几个示例的尝试。results: 研究发现，当选项的顺序发生变化时，LLMs 的表现会受到很大的影响，表现差异可达 13% 到 75% 不同的benchmark上。通过 detailed 分析，作者们 conjecture 这种不稳定性源于 LLMs 对最佳选项的不确定性，并且特定的选项位置可能会帮助模型更准确地预测最佳选项。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Expecting-The-Unexpected-Towards-Broad-Out-Of-Distribution-Detection"><a href="#Expecting-The-Unexpected-Towards-Broad-Out-Of-Distribution-Detection" class="headerlink" title="Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection"></a>Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11480">http://arxiv.org/abs/2308.11480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/servicenow/broad-openood">https://github.com/servicenow/broad-openood</a></li>
<li>paper_authors: Charles Guille-Escuret, Pierre-André Noël, Ioannis Mitliagkas, David Vazquez, Joao Monteiro</li>
<li>for: 本研究旨在提高部署机器学习系统的可靠性，通过开发检测出现在训练集之外的输入（Out-of-distribution，OOD）方法。</li>
<li>methods: 本研究对现有的OOD检测方法进行了评估，并发现这些方法只能够有效地检测未知的类型，而对其他类型的分布转移表现不一致。为解决这个问题，我们提出了一种基于生成模型的ensemble方法，可以提供更一致和全面的OOD检测解决方案。</li>
<li>results: 我们的研究发现，现有的OOD检测方法在不同类型的分布转移中的性能不一致，而我们的ensemble方法可以提供更高的可靠性和敏感性。我们还发布了一个名为BROAD（Benchmarking Resilience Over Anomaly Diversity）的数据集，以便评估OOD检测方法的性能。<details>
<summary>Abstract</summary>
Improving the reliability of deployed machine learning systems often involves developing methods to detect out-of-distribution (OOD) inputs. However, existing research often narrowly focuses on samples from classes that are absent from the training set, neglecting other types of plausible distribution shifts. This limitation reduces the applicability of these methods in real-world scenarios, where systems encounter a wide variety of anomalous inputs. In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expect. As a first step toward broad OOD detection, we learn a generative model of existing detection scores with a Gaussian mixture. By doing so, we present an ensemble approach that offers a more consistent and comprehensive solution for broad OOD detection, demonstrating superior performance compared to existing methods. Our code to download BROAD and reproduce our experiments is publicly available.
</details>
<details>
<summary>摘要</summary>
提高机器学习系统部署时的可靠性通常涉及到开发检测出idanormal inputs的方法。然而，现有研究通常只关注 absent classes 中的样本，忽视其他类型的可能性 Distribution Shift。这种限制 reduce了这些方法在实际应用中的适用性，因为系统会遇到各种异常输入。在这种研究中，我们分类ified five distinct types of distribution shifts, and critically evaluated the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expect. As a first step toward broad OOD detection, we learn a generative model of existing detection scores with a Gaussian mixture. By doing so, we present an ensemble approach that offers a more consistent and comprehensive solution for broad OOD detection, demonstrating superior performance compared to existing methods. Our code to download BROAD and reproduce our experiments is publicly available.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Revisiting-column-generation-based-matheuristic-for-learning-classification-trees"><a href="#Revisiting-column-generation-based-matheuristic-for-learning-classification-trees" class="headerlink" title="Revisiting column-generation-based matheuristic for learning classification trees"></a>Revisiting column-generation-based matheuristic for learning classification trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11477">http://arxiv.org/abs/2308.11477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krooonal/col_gen_estimator">https://github.com/krooonal/col_gen_estimator</a></li>
<li>paper_authors: Krunal Kishor Patel, Guy Desaulniers, Andrea Lodi</li>
<li>for: 这篇论文目的是提高分类问题的解决方法，特别是在机器学习领域中使用决策树模型。</li>
<li>methods: 该论文使用的方法是基于列生成的规则逻辑，以提高分类问题的解决效率和可扩展性。</li>
<li>results: 对于多类分类问题，该方法可以减少数据点数量，并使用数据依赖的约束来提高分类质量。 computational results表明，这些改进可以提高解决效率。<details>
<summary>Abstract</summary>
Decision trees are highly interpretable models for solving classification problems in machine learning (ML). The standard ML algorithms for training decision trees are fast but generate suboptimal trees in terms of accuracy. Other discrete optimization models in the literature address the optimality problem but only work well on relatively small datasets. \cite{firat2020column} proposed a column-generation-based heuristic approach for learning decision trees. This approach improves scalability and can work with large datasets. In this paper, we describe improvements to this column generation approach. First, we modify the subproblem model to significantly reduce the number of subproblems in multiclass classification instances. Next, we show that the data-dependent constraints in the master problem are implied, and use them as cutting planes. Furthermore, we describe a separation model to generate data points for which the linear programming relaxation solution violates their corresponding constraints. We conclude by presenting computational results that show that these modifications result in better scalability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="IT3D-Improved-Text-to-3D-Generation-with-Explicit-View-Synthesis"><a href="#IT3D-Improved-Text-to-3D-Generation-with-Explicit-View-Synthesis" class="headerlink" title="IT3D: Improved Text-to-3D Generation with Explicit View Synthesis"></a>IT3D: Improved Text-to-3D Generation with Explicit View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11473">http://arxiv.org/abs/2308.11473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/buaacyw/it3d-text-to-3d">https://github.com/buaacyw/it3d-text-to-3d</a></li>
<li>paper_authors: Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, Guosheng Lin</li>
<li>for: 本研究旨在提高文本到3D图像转换技术，并使用大型文本到图像扩散模型（LDM）提取知识。</li>
<li>methods: 本研究使用图像到图像管道，利用LDM生成高质量多视图图像，并通过Diffusion-GAN双向训练策略来引导3D模型训练。</li>
<li>results: 实验结果表明，本方法比基eline方法有更高的质量和精度，能够更好地解决文本到3D图像转换中的一些问题，如过度满、缺乏细节和不实际的输出。<details>
<summary>Abstract</summary>
Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs). Nonetheless, existing Text-to-3D approaches often grapple with challenges such as over-saturation, inadequate detailing, and unrealistic outputs. This study presents a novel strategy that leverages explicitly synthesized multi-view images to address these issues. Our approach involves the utilization of image-to-image pipelines, empowered by LDMs, to generate posed high-quality images based on the renderings of coarse 3D models. Although the generated images mostly alleviate the aforementioned issues, challenges such as view inconsistency and significant content variance persist due to the inherent generative nature of large diffusion models, posing extensive difficulties in leveraging these images effectively. To overcome this hurdle, we advocate integrating a discriminator alongside a novel Diffusion-GAN dual training strategy to guide the training of 3D models. For the incorporated discriminator, the synthesized multi-view images are considered real data, while the renderings of the optimized 3D models function as fake data. We conduct a comprehensive set of experiments that demonstrate the effectiveness of our method over baseline approaches.
</details>
<details>
<summary>摘要</summary>
最近的文本到3D技术发展受到了大型文本到图像扩散模型（LDM）的知识储存的推动。然而，现有的文本到3D方法通常会遇到过度饱和、不够细节和不实际的输出等问题。本研究提出了一种新的策略，利用可控多视图图像来解决这些问题。我们的方法是利用图像到图像管道，利用LDM来生成基于粗糙3D模型的高质量poses图像。虽然生成的图像大多消除了以上问题，但是由于大扩散模型的生成性，仍然存在视角不一致和内容差异等问题。为解决这个障碍，我们提议在3D模型训练中添加一个判别器，并采用Diffusion-GAN双向训练策略来引导3D模型的训练。对于添加的判别器，生成的多视图图像被视为真实数据，而 renderings of 优化的3D模型则被视为假数据。我们进行了一系列的实验，证明了我们的方法在基础方法上表现更高效。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Open-Vocabulary-Enhanced-Safe-landing-with-Intelligence-DOVESEI"><a href="#Dynamic-Open-Vocabulary-Enhanced-Safe-landing-with-Intelligence-DOVESEI" class="headerlink" title="Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI)"></a>Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11471">http://arxiv.org/abs/2308.11471</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mistlab/dovesei">https://github.com/mistlab/dovesei</a></li>
<li>paper_authors: Haechan Mark Bong, Rongge Zhang, Ricardo de Azambuja, Giovanni Beltrame</li>
<li>for: 本研究目标是为城市空中机器人开发安全降落。</li>
<li>methods: 本研究使用视 servoing 技术，利用开放词汇图像分割，适应不同场景，并且不需要大量数据更新内部模型。</li>
<li>results: 实验表明，该系统可以在100米高度下成功执行降落动作，且通过引入动态专注机制，提高降落成功率。<details>
<summary>Abstract</summary>
This work targets what we consider to be the foundational step for urban airborne robots, a safe landing. Our attention is directed toward what we deem the most crucial aspect of the safe landing perception stack: segmentation. We present a streamlined reactive UAV system that employs visual servoing by harnessing the capabilities of open vocabulary image segmentation. This approach can adapt to various scenarios with minimal adjustments, bypassing the necessity for extensive data accumulation for refining internal models, thanks to its open vocabulary methodology. Given the limitations imposed by local authorities, our primary focus centers on operations originating from altitudes of 100 meters. This choice is deliberate, as numerous preceding works have dealt with altitudes up to 30 meters, aligning with the capabilities of small stereo cameras. Consequently, we leave the remaining 20m to be navigated using conventional 3D path planning methods. Utilizing monocular cameras and image segmentation, our findings demonstrate the system's capability to successfully execute landing maneuvers at altitudes as low as 20 meters. However, this approach is vulnerable to intermittent and occasionally abrupt fluctuations in the segmentation between frames in a video stream. To address this challenge, we enhance the image segmentation output by introducing what we call a dynamic focus: a masking mechanism that self adjusts according to the current landing stage. This dynamic focus guides the control system to avoid regions beyond the drone's safety radius projected onto the ground, thus mitigating the problems with fluctuations. Through the implementation of this supplementary layer, our experiments have reached improvements in the landing success rate of almost tenfold when compared to global segmentation. All the source code is open source and available online (github.com/MISTLab/DOVESEI).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Internal-Cross-layer-Gradients-for-Extending-Homogeneity-to-Heterogeneity-in-Federated-Learning"><a href="#Internal-Cross-layer-Gradients-for-Extending-Homogeneity-to-Heterogeneity-in-Federated-Learning" class="headerlink" title="Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning"></a>Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11464">http://arxiv.org/abs/2308.11464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Hin Chan, Rui Zhou, Running Zhao, Zhihan Jiang, Edith C. -H. Ngai</li>
<li>for: 提高模型不同的 Federated Learning 方法处理系统不同性能的能力</li>
<li>methods: 利用内部交叉层导数，不需要客户端之间的通信，可以增强深层导数的相似性</li>
<li>results: 实验结果证明 InCo Aggregation 的效果，显示内部交叉层导数是提高性能的有效途径<details>
<summary>Abstract</summary>
Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverags internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods can be tailored to accommodate model-homogeneous FL methods such as FedAvg, FedProx, FedNova, Scaffold, and MOON, to expand their capabilities to handle the system heterogeneity. Copious experimental results validate the effectiveness of InCo Aggregation, spotlighting internal cross-layer gradients as a promising avenue to enhance the performance in heterogenous FL.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）在实际应用中遇到系统多样性的挑战。为了增强大多数模型相似的FL方法在处理系统多样性的能力，我们提出了一个训练方案，可以将其扩展到处理这个挑战。在这篇论文中，我们开始我们的研究，进行了详细的探索Homogeneous和Heterogeneous FL Setting中的三个关键观察：（1）客户端性能和层 similarity 之间的正相关，（2）在浅层较为高 similarity ，而深层较低 similarity，（3）在各层 Similarity 中更平滑的梯度分布，这些观察可以帮助我们更好地理解FL系统的多样性问题。基于这些观察，我们提出了InCo Aggregation，利用服务器模型中的内部交叉层梯度，把深层层梯度与浅层梯度混合，以增强深层层梯度的相似性，不需要客户端之间的额外交流。此外，我们的方法可以与模型相似的FL方法，如FedAvg、FedProx、FedNova、Scaffold和MOON相容，以扩展它们的能力，处理系统多样性。实际实验结果显示，InCo Aggregation 具有很好的效果，强调了内部交叉层梯度作为提高FL系统多样性性能的有力之路。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Self-Supervised-Representation-Learning"><a href="#A-Survey-on-Self-Supervised-Representation-Learning" class="headerlink" title="A Survey on Self-Supervised Representation Learning"></a>A Survey on Self-Supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11455">http://arxiv.org/abs/2308.11455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/esvit">https://github.com/microsoft/esvit</a></li>
<li>paper_authors: Tobias Uelwer, Jan Robine, Stefan Sylvius Wagner, Marc Höftmann, Eric Upschulte, Sebastian Konietzny, Maike Behrendt, Stefan Harmeling</li>
<li>for: 本文提供了一个总结性的综述，探讨了一些无监督学习方法，用于学习图像表示。这些表示可以用于下游任务，如分类或物体检测。</li>
<li>methods: 本文使用了一些无监督学习方法，包括自适应卷积神经网络、自适应层次神经网络和卷积神经网络。</li>
<li>results: 根据Literature review，这些方法在下游任务中表现非常出色，与监督学习方法相当。Here’s the translation in English:</li>
<li>for: This paper provides a comprehensive review of methods for learning image representations without supervision, which can be used in downstream tasks such as classification or object detection.</li>
<li>methods: The paper uses several unsupervised learning methods, including autoencoders, self-attention mechanisms, and convolutional neural networks.</li>
<li>results: According to the literature review, these methods have performed extremely well in downstream tasks, comparable to supervised learning methods.<details>
<summary>Abstract</summary>
Learning meaningful representations is at the heart of many tasks in the field of modern machine learning. Recently, a lot of methods were introduced that allow learning of image representations without supervision. These representations can then be used in downstream tasks like classification or object detection. The quality of these representations is close to supervised learning, while no labeled images are needed. This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other. Furthermore, our survey summarizes the most-recent experimental results reported in the literature in form of a meta-study. Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning.
</details>
<details>
<summary>摘要</summary>
学习有意义的表示是现代机器学习领域中的核心任务之一。最近，许多无监督学习方法被引入，可以学习图像表示。这些表示可以在下游任务中使用，如分类或物体检测。这些无监督学习方法的表示质量与监督学习相似，但无需标注图像。本文提供了这些方法的统一notation，指出这些方法之间的相似性和差异，并提出了这些方法的分类方式。此外，我们的survey还summarized了Literature中最近的实验结果，并进行了meta-study。本文为研究者和实践者提供了进入无监督学习领域的开始点。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Convergence-guarantee-for-consistency-models"><a href="#Convergence-guarantee-for-consistency-models" class="headerlink" title="Convergence guarantee for consistency models"></a>Convergence guarantee for consistency models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11449">http://arxiv.org/abs/2308.11449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junlong Lyu, Zhitang Chen, Shoubo Feng</li>
<li>for: 这 paper 的目的是为Consistency Models (CMs) 提供首次一致性保证，这种一步生成模型可以生成与Diffusion Models相同的样本。</li>
<li>methods: 这 paper 使用了基本的score-matching error assumption, consistency error assumption和数据分布的smoothness假设，以确保CMs 可以效率地从任何现实数据分布中采样，并且采样Error小于$W_2$.</li>
<li>results: 这 paper 的结果包括：(1) 对于$L^2$-accurate score和consistency假设，CMs 可以在一步中采样到任何现实数据分布，并且采样Error scales polynomially in all parameters; (2) 不需要强制对数据分布的假设，如log-Sobelev inequality; (3) 可以further reduce the error by using Multistep Consistency Sampling procedure.<details>
<summary>Abstract</summary>
We provide the first convergence guarantees for the Consistency Models (CMs), a newly emerging type of one-step generative models that can generate comparable samples to those generated by Diffusion Models. Our main result is that, under the basic assumptions on score-matching errors, consistency errors and smoothness of the data distribution, CMs can efficiently sample from any realistic data distribution in one step with small $W_2$ error. Our results (1) hold for $L^2$-accurate score and consistency assumption (rather than $L^\infty$-accurate); (2) do note require strong assumptions on the data distribution such as log-Sobelev inequality; (3) scale polynomially in all parameters; and (4) match the state-of-the-art convergence guarantee for score-based generative models (SGMs). We also provide the result that the Multistep Consistency Sampling procedure can further reduce the error comparing to one step sampling, which support the original statement of "Consistency Models, Yang Song 2023". Our result further imply a TV error guarantee when take some Langevin-based modifications to the output distributions.
</details>
<details>
<summary>摘要</summary>
我们提供了一些一步生成模型（CM）的协调保证，这是一种最近崛起的一种生成模型，可以生成与演化模型（Diffusion Models）相似的样本。我们的主要结果是，假设score-matching error、consistency error和资料分布的平滑性满足某些基本假设，则CM可以将任何现实的资料分布 efficiently sampled in one step with small $W_2$ error。我们的结果包括：1. 对于$L^2$-accurate score和consistency假设（而不是$L^\infty$-accurate）;2. 不需要对于资料分布的强则假设，如log-Sobelev不等式;3. 随所有参数的度量 polynomially scale;4. 与state-of-the-art score-based生成模型（SGMs）的协调保证相符。我们还提供了一个Multistep Consistency Sampling程序，可以降低比一步样本的错误，这支持原始的“Consistency Models, Yang Song 2023”的声明。我们的结果进一步显示了一个TV错误保证，当将一些Langevin-based modifications套用到输出分布时。
</details></li>
</ul>
<hr>
<h2 id="Aspect-oriented-Opinion-Alignment-Network-for-Aspect-Based-Sentiment-Classification"><a href="#Aspect-oriented-Opinion-Alignment-Network-for-Aspect-Based-Sentiment-Classification" class="headerlink" title="Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification"></a>Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11447">http://arxiv.org/abs/2308.11447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aone-nlp/absa-aoan">https://github.com/aone-nlp/absa-aoan</a></li>
<li>paper_authors: Xueyi Liu, Rui Hou, Yanglei Gan, Da Luo, Changlin Li, Xiaojun Shi, Qiao Liu</li>
<li>for: 这篇论文目的是提出一种新的方法来解决在多个方面的文本分析中存在的semantic mismatch问题，以提高 Fine-grained sentiment analysis 的精度。</li>
<li>methods: 该方法使用了一种新的Aspect-oriented Opinion Alignment Network (AOAN)，包括一个邻域span增强模块和一个多元视角注意机制，以强调对Opinion words和对应的方面的上下文关系。</li>
<li>results: 实验结果表明，该模型在三个标准数据集上达到了领域的最佳效果，代表着该方法在 Fine-grained sentiment analysis 中的成功应用。<details>
<summary>Abstract</summary>
Aspect-based sentiment classification is a crucial problem in fine-grained sentiment analysis, which aims to predict the sentiment polarity of the given aspect according to its context. Previous works have made remarkable progress in leveraging attention mechanism to extract opinion words for different aspects. However, a persistent challenge is the effective management of semantic mismatches, which stem from attention mechanisms that fall short in adequately aligning opinions words with their corresponding aspect in multi-aspect sentences. To address this issue, we propose a novel Aspect-oriented Opinion Alignment Network (AOAN) to capture the contextual association between opinion words and the corresponding aspect. Specifically, we first introduce a neighboring span enhanced module which highlights various compositions of neighboring words and given aspects. In addition, we design a multi-perspective attention mechanism that align relevant opinion information with respect to the given aspect. Extensive experiments on three benchmark datasets demonstrate that our model achieves state-of-the-art results. The source code is available at https://github.com/AONE-NLP/ABSA-AOAN.
</details>
<details>
<summary>摘要</summary>
<SYS>    <TRANSLATE_TEXT>        非常详细的 sentiment 分析中，尤其是 aspect-based sentiment classification，目标是根据不同的上下文来预测具体的 sentiment 偏好。先前的研究已经做出了很大的进步，通过使用注意力机制来提取不同的 opinion 词。但是，一个持续的挑战是如何有效地处理 semantic 匹配问题，这些问题来自于注意力机制不够地对 opinion 词和对应的 aspect 进行匹配。为了解决这个问题，我们提出了一种新的 Aspect-oriented Opinion Alignment Network (AOAN)，用于捕捉不同的 opinion 词和 aspect 之间的上下文关系。</TRANSLATE_TEXT></SYS>Here's the translation in Traditional Chinese:<SYS>    <TRANSLATE_TEXT>        非常细致的 sentiment 分析中，尤其是 aspect-based sentiment classification，目标是根据不同的上下文来预测具体的 sentiment 偏好。先前的研究已经做出了很大的进步，通过使用注意力机制来提取不同的 opinion 词。但是，一个持续的挑战是如何有效地处理 semantic 匹配问题，这些问题来自于注意力机制不够地对 opinion 词和对应的 aspect 进行匹配。为了解决这个问题，我们提出了一种新的 Aspect-oriented Opinion Alignment Network (AOAN)，用于捕捉不同的 opinion 词和 aspect 之间的上下文关系。</TRANSLATE_TEXT></SYS>Note that the translation is in Simplified Chinese, as requested. If you would like the translation in Traditional Chinese instead, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploration-of-Rashomon-Set-Assists-Explanations-for-Medical-Data"><a href="#Exploration-of-Rashomon-Set-Assists-Explanations-for-Medical-Data" class="headerlink" title="Exploration of Rashomon Set Assists Explanations for Medical Data"></a>Exploration of Rashomon Set Assists Explanations for Medical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11446">http://arxiv.org/abs/2308.11446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katarzyna Kobylińska, Mateusz Krzyziński, Rafał Machowicz, Mariusz Adamek, Przemysław Biecek</li>
<li>For: This paper aims to address the problem of relying solely on performance metrics in machine learning modeling, particularly in medical and healthcare studies, by introducing a novel process to explore Rashomon set models.* Methods: The proposed approach uses the $\texttt{Rashomon_DETECT}$ algorithm to identify the most different models within the Rashomon set, and the Profile Disparity Index (PDI) to quantify differences in variable effects among models.* Results: The approach is demonstrated on a foundational case study of predicting survival among hemophagocytic lymphohistiocytosis (HLH) patients, as well as on other medical data sets, showing its effectiveness and versatility in various contexts.Here are the three points in Simplified Chinese:* For: 这篇论文目的是解决机器学习模型选择过程中围绕性能指标偏重的问题，尤其在医疗和健康研究中，以获得更多的有价值信息。* Methods: 该方法使用 $\texttt{Rashomon_DETECT}$ 算法 Identify Rashomon set 中最为不同的模型，并使用 Profile Disparity Index (PDI) 量化变量效果之间的差异。* Results: 该方法在针对 Hemophagocytic lymphohistiocytosis (HLH) 患者存活预测的基本案例研究中，以及其他医疗数据集中，得到了有效和多样的结果。<details>
<summary>Abstract</summary>
The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric. However, this approach leads to abandoning a more profound analysis of slightly inferior models. Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions. This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis. This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach. The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\texttt{Rashomon_DETECT}$ algorithm. This algorithm compares profiles illustrating prediction dependencies on variable values generated by eXplainable Artificial Intelligence (XAI) techniques. To quantify differences in variable effects among models, we introduce the Profile Disparity Index (PDI) based on measures from functional data analysis. To illustrate the effectiveness of our approach, we showcase its application in predicting survival among hemophagocytic lymphohistiocytosis (HLH) patients - a foundational case study. Additionally, we benchmark our approach on other medical data sets, demonstrating its versatility and utility in various contexts.
</details>
<details>
<summary>摘要</summary>
传统的机器学习模型选择过程是通过选择最大化一个选择的性能指标来完成的。然而，这种方法会抛弃更深入的模型分析。特别在医疗和健康研究中，目标不仅是预测，还是生成有价值的理解。只靠性能指标来结论可能导致误导或不完整的结论。这种问题特别存在于处理一组性能几乎最大的模型集合，称为“Rashomon集”。这个集合可能很多，其中包含描述数据不同方式的模型，需要全面的分析。本文提出了一种新的模型探索过程，扩展传统模型选择策略。其核心是在Rashomon集中 identific 最不同的模型，由我们引入的 $\texttt{Rashomon\_DETECT}$ 算法实现。这个算法比较使用 eXplainable Artificial Intelligence（XAI）技术生成的变量值预测依赖的profile。为了量化不同模型中变量效应的差异，我们引入了 Profile Disparity Index（PDI），基于函数数据分析中的度量。我们通过应用这种方法在 Hemophagocytic lymphohistiocytosis（HLH）患者的存活预测中进行了示例，并将其应用于其他医疗数据集，以示其多样性和可用性。
</details></li>
</ul>
<hr>
<h2 id="Inferring-gender-from-name-a-large-scale-performance-evaluation-study"><a href="#Inferring-gender-from-name-a-large-scale-performance-evaluation-study" class="headerlink" title="Inferring gender from name: a large scale performance evaluation study"></a>Inferring gender from name: a large scale performance evaluation study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12381">http://arxiv.org/abs/2308.12381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kriste Krstovski, Yao Lu, Ye Xu</li>
<li>for: 这个论文主要目的是为了对名称到性别推断的算法和软件产品进行大规模性能评估，以及提出两种新的混合方法以实现更高的性能。</li>
<li>methods: 本文使用了多个大量注释的名称数据集来进行分析，并提出了两种新的混合方法。</li>
<li>results: 研究发现现有方法中的任何一种都无法在所有情况下达到最佳性能，而两种新提出的混合方法均可以在所有情况下实现更高的性能。<details>
<summary>Abstract</summary>
A person's gender is a crucial piece of information when performing research across a wide range of scientific disciplines, such as medicine, sociology, political science, and economics, to name a few. However, in increasing instances, especially given the proliferation of big data, gender information is not readily available. In such cases researchers need to infer gender from readily available information, primarily from persons' names. While inferring gender from name may raise some ethical questions, the lack of viable alternatives means that researchers have to resort to such approaches when the goal justifies the means - in the majority of such studies the goal is to examine patterns and determinants of gender disparities. The necessity of name-to-gender inference has generated an ever-growing domain of algorithmic approaches and software products. These approaches have been used throughout the world in academia, industry, governmental and non-governmental organizations. Nevertheless, the existing approaches have yet to be systematically evaluated and compared, making it challenging to determine the optimal approach for future research. In this work, we conducted a large scale performance evaluation of existing approaches for name-to-gender inference. Analysis are performed using a variety of large annotated datasets of names. We further propose two new hybrid approaches that achieve better performance than any single existing approach.
</details>
<details>
<summary>摘要</summary>
人的性别信息是科学研究中不可或缺的重要信息，包括医学、社会学、政治科学和经济学等领域。然而，随着大数据的普及，性别信息越来越难以获得。在这些情况下，研究人员需要根据可用的信息进行性别推断，主要是根据人名。虽然从名字中推断性别可能会附带一些伦理问题，但由于现有的可行方法缺乏，研究人员需要采用这些方法以实现研究目标。在全球范围内，这些方法已经广泛应用于大学、企业、政府和非政府组织中。然而，现有的方法尚未得到系统性的评估和比较，这使得未来研究中选择最佳方法仍然存在挑战。在这项工作中，我们进行了大规模性能评估现有的名字到性别推断方法。分析使用了多种大量注释的名字数据集。此外，我们还提出了两种新的混合方法，其性能更高于任何单独的现有方法。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Large-Language-Model-based-Autonomous-Agents"><a href="#A-Survey-on-Large-Language-Model-based-Autonomous-Agents" class="headerlink" title="A Survey on Large Language Model based Autonomous Agents"></a>A Survey on Large Language Model based Autonomous Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11432">http://arxiv.org/abs/2308.11432</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paitesanshi/llm-agent-survey">https://github.com/paitesanshi/llm-agent-survey</a></li>
<li>paper_authors: Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen</li>
<li>for: 本研究准备了一份总结LLM基于自主代理的研究，包括LLM基于代理的构建、应用领域和评价策略等方面。</li>
<li>methods: 本研究使用了大量网络知识获得的大语言模型(LLM)，并提出了一个统一框架来涵盖大多数之前的工作。</li>
<li>results: 本研究通过对LLM基于代理的各种应用领域和评价策略的总结，提出了一些挑战和未来方向，并将相关参考文献存储在<a target="_blank" rel="noopener" href="https://github.com/Paitesanshi/LLM-Agent-Survey%E4%B8%AD%E3%80%82">https://github.com/Paitesanshi/LLM-Agent-Survey中。</a><details>
<summary>Abstract</summary>
Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework that encompasses a majority of the previous work. Additionally, we provide a summary of the various applications of LLM-based AI agents in the domains of social science, natural science, and engineering. Lastly, we discuss the commonly employed evaluation strategies for LLM-based AI agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository for the related references at https://github.com/Paitesanshi/LLM-Agent-Survey.
</details>
<details>
<summary>摘要</summary>
自主代理已经是学术界的一个主要研究话题，早期的研究通常是在隔离环境中训练有限知识的代理，这与人类学习过程不同，导致代理做出的决策困难达到人类水平。然而，随着互联网知识的掌握，大型自然语言模型（LLM）在实现人类智能水平方面表现出了很好的潜力。这导致了对自主代理基于 LLM 的研究的快速增长。为了挖掘 LLM 的潜力，研究者们设计了多种特定应用场景的代理建模。在这篇文章中，我们提供了一份系统性的评论，涵盖了这些研究的大部分。我们更加关注 LLM 基于代理的建模，并提出了一个统一框架，覆盖了大多数前期工作。此外，我们还提供了自然科学、社会科学和工程等领域 LLM 基于 AI 代理的多种应用案例。最后，我们讨论了对 LLM 基于 AI 代理的评价策略，并根据前期研究提出了一些挑战和未来方向。为了保持这一领域的报道和不断更新我们的评论，我们在 GitHub 上建立了一个参考库，可以在 https://github.com/Paitesanshi/LLM-Agent-Survey 中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Study-on-the-Impact-of-Non-confounding-Covariates-on-the-Inferential-Performance-of-Methods-based-on-the-Potential-Outcome-Framework"><a href="#A-Study-on-the-Impact-of-Non-confounding-Covariates-on-the-Inferential-Performance-of-Methods-based-on-the-Potential-Outcome-Framework" class="headerlink" title="A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework"></a>A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11676">http://arxiv.org/abs/2308.11676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhao, Shuai Fu, Huiyan Sun</li>
<li>for: The paper is written to provide a unified graphical framework for causal inference models based on the Potential Outcome Framework (POF), and to analyze the influence of non-confounding covariates on the inference performance of these models.</li>
<li>methods: The paper uses a graphical framework to present the underlying principles of causal inference models based on the POF, and conducts extensive experiments on synthetic datasets to validate the theoretical conclusions.</li>
<li>results: The paper finds that the optimal scenario for eliminating confounding bias is for the covariates to exclusively encompass confounders, and that adjustment variables contribute to more accurate inferences in the task of inferring counterfactual outcomes.<details>
<summary>Abstract</summary>
The Potential Outcome Framework (POF) plays a prominent role in the field of causal inference. Most causal inference models based on the POF (CIMs-B-POF) are designed for eliminating confounding bias and default to an underlying assumption of Confounding Covariates. This assumption posits that the covariates consist solely of confounders. However, the assumption of Confounding Covariates is challenging to maintain in practice, particularly when dealing with high-dimensional covariates. While certain methods have been proposed to differentiate the distinct components of covariates prior to conducting causal inference, the consequences of treating non-confounding covariates as confounders remain unclear. This ambiguity poses a potential risk when applying the CIMs-B-POF in practical scenarios. In this paper, we present a unified graphical framework for the CIMs-B-POF, which greatly enhances the comprehension of these models' underlying principles. Using this graphical framework, we quantitatively analyze the extent to which the inference performance of CIMs-B-POF is influenced when incorporating various types of non-confounding covariates, such as instrumental variables, mediators, colliders, and adjustment variables. The key findings are: in the task of eliminating confounding bias, the optimal scenario is for the covariates to exclusively encompass confounders; in the subsequent task of inferring counterfactual outcomes, the adjustment variables contribute to more accurate inferences. Furthermore, extensive experiments conducted on synthetic datasets consistently validate these theoretical conclusions.
</details>
<details>
<summary>摘要</summary>
Potential Outcome Framework (POF) 在 causal inference 领域扮演着重要的角色。大多数基于 POF 的 causal inference 模型 (CIMs-B-POF) 是为了消除干扰偏见而设计的，默认假设是 Confounding Covariates 假设，即 covariates 仅仅包含干扰因素。然而，在实践中保持 Confounding Covariates 假设是困难的，特别是处理高维 covariates 时。虽然一些方法已经被提出来分解 covariates 的不同组成部分，然而对非干扰 covariates 被视为干扰因素的后果仍然不清楚。这种不确定性在实践中应用 CIMs-B-POF 时可能存在风险。在这篇论文中，我们提出了一个统一的图形 Framework  для CIMs-B-POF，这有助于更好地理解这些模型的基本原理。使用这个图形 Framework，我们量化分析了在不同类型的非干扰 covariates 存在时，CIMs-B-POF 的推理性能是如何受影响的。我们发现，在消除干扰偏见的任务中，理想的情况是 covariates 仅仅包含干扰因素；在后续的对 counterfactual 结果进行推理任务中，调整变量对更准确的推理做出了贡献。此外，我们在 synthetic 数据上进行了广泛的实验，并 consistently 验证了这些理论结论。
</details></li>
</ul>
<hr>
<h2 id="AIxArtist-A-First-Person-Tale-of-Interacting-with-Artificial-Intelligence-to-Escape-Creative-Block"><a href="#AIxArtist-A-First-Person-Tale-of-Interacting-with-Artificial-Intelligence-to-Escape-Creative-Block" class="headerlink" title="AIxArtist: A First-Person Tale of Interacting with Artificial Intelligence to Escape Creative Block"></a>AIxArtist: A First-Person Tale of Interacting with Artificial Intelligence to Escape Creative Block</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11424">http://arxiv.org/abs/2308.11424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Makayla Lewis</li>
<li>for: 这篇论文探讨了人工智能（AI）如何支持艺术创作，以及在艺术创作过程中AI的可追溯性。</li>
<li>methods: 本论文采用了人工智能工具HIS、ChatGPT和Midjourney，进行了一些实验和探索，以探索AI如何支持艺术创作。</li>
<li>results: 本论文发现了一些关键问题，包括创作过程的透明性、作品的起源和伦理问题，以及创作是 copying 还是灵感？这些问题需要进一步的讨论和探索。<details>
<summary>Abstract</summary>
The future of the arts and artificial intelligence (AI) is promising as technology advances. As the use of AI in design becomes more widespread, art practice may not be a human-only art form and could instead become a digitally integrated experience. With enhanced creativity and collaboration, arts and AI could work together towards creating artistic outputs that are visually appealing and meet the needs of the artist and viewer. While it is uncertain how far the integration will go, arts and AI will likely influence one another. This workshop pictorial puts forward first-person research that shares interactions between an HCI researcher and AI as they try to escape the creative block. The pictorial paper explores two questions: How can AI support artists' creativity, and what does it mean to be explainable in this context? HIs, ChatGPT and Midjourney were engaged; the result was a series of reflections that require further discussion and explorations in the XAIxArts community: Transparency of attribution, the creation process, ethics of asking, and inspiration vs copying.
</details>
<details>
<summary>摘要</summary>
This workshop pictorial presents first-person research that explores the interactions between an HCI researcher and AI as they try to escape creative blocks. The pictorial paper examines two questions: how can AI support artists' creativity, and what does it mean to be explainable in this context? The research involved engaging with AI models such as ChatGPT and Midjourney, leading to a series of reflections that require further discussion and exploration in the XAIxArts community. These reflections include transparency of attribution, the creation process, ethics of asking, and inspiration vs copying.
</details></li>
</ul>
<hr>
<h2 id="TurboViT-Generating-Fast-Vision-Transformers-via-Generative-Architecture-Search"><a href="#TurboViT-Generating-Fast-Vision-Transformers-via-Generative-Architecture-Search" class="headerlink" title="TurboViT: Generating Fast Vision Transformers via Generative Architecture Search"></a>TurboViT: Generating Fast Vision Transformers via Generative Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11421">http://arxiv.org/abs/2308.11421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Wong, Saad Abbasi, Saeejith Nair</li>
<li>for: 这个研究的目的是实现高通过率且低 computation complexity的类比视觉 Transformer 架构设计。</li>
<li>methods: 这个研究使用了 Generative Architecture Search (GAS) 来生成高效的类比视觉 Transformer 架构设计，并且将注意力集中在面精度和 Q-pooling 设计模式上。</li>
<li>results: TurboViT 架构设计在 ImageNet-1K 数据集上实现了比较高的精度和低的 computation complexity，与其他 10 个现有的高效类比视觉 Transformer 网络架构设计相比。 Inference 延误和批处处理时间都表现出色，在低延误场景下，TurboViT 的延误时间比 FasterViT-0 低了 &gt;3.21 倍，而且对 batch 处理也表现出 &gt;3.18 倍的提高。<details>
<summary>Abstract</summary>
Vision transformers have shown unprecedented levels of performance in tackling various visual perception tasks in recent years. However, the architectural and computational complexity of such network architectures have made them challenging to deploy in real-world applications with high-throughput, low-memory requirements. As such, there has been significant research recently on the design of efficient vision transformer architectures. In this study, we explore the generation of fast vision transformer architecture designs via generative architecture search (GAS) to achieve a strong balance between accuracy and architectural and computational efficiency. Through this generative architecture search process, we create TurboViT, a highly efficient hierarchical vision transformer architecture design that is generated around mask unit attention and Q-pooling design patterns. The resulting TurboViT architecture design achieves significantly lower architectural computational complexity (>2.47$\times$ smaller than FasterViT-0 while achieving same accuracy) and computational complexity (>3.4$\times$ fewer FLOPs and 0.9% higher accuracy than MobileViT2-2.0) when compared to 10 other state-of-the-art efficient vision transformer network architecture designs within a similar range of accuracy on the ImageNet-1K dataset. Furthermore, TurboViT demonstrated strong inference latency and throughput in both low-latency and batch processing scenarios (>3.21$\times$ lower latency and >3.18$\times$ higher throughput compared to FasterViT-0 for low-latency scenario). These promising results demonstrate the efficacy of leveraging generative architecture search for generating efficient transformer architecture designs for high-throughput scenarios.
</details>
<details>
<summary>摘要</summary>
视transformer在近年来的视觉任务中表现出了前所未有的水平。然而，这些网络架构的建筑和计算复杂性使得它们在实际应用中高速、低内存要求下部署困难。因此，有一些研究是设计高效的视transformer架构。在这项研究中，我们通过生成式建筑搜索（GAS）来生成高效的视transformer架构设计，以达到精度和建筑计算效率的平衡。通过这个生成过程，我们创造了TurboViT，一种高效的层次视transformer架构设计，基于面积注意力和Q-Pooling设计模式。TurboViT架构设计的建筑计算复杂性比FasterViT-0大于2.47倍，计算复杂性比MobileViT2-2.0大于3.4倍，同时精度相同。此外，TurboViT在低延迟和批处理场景中表现出了优秀的执行时间和 Throughput，比FasterViT-0在低延迟场景下执行时间大于3.21倍，比MobileViT2-2.0在批处理场景下执行时间大于3.18倍。这些优秀的结果表明了利用生成式建筑搜索生成高效的transformer架构设计的有效性。
</details></li>
</ul>
<hr>
<h2 id="Tensor-Regression"><a href="#Tensor-Regression" class="headerlink" title="Tensor Regression"></a>Tensor Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11419">http://arxiv.org/abs/2308.11419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tensorly/torch">https://github.com/tensorly/torch</a></li>
<li>paper_authors: Jiani Liu, Ce Zhu, Zhen Long, Yipeng Liu</li>
<li>for:  This paper is written for students, researchers, and practitioners who work with high dimensional data and are interested in tensor-based regression analysis.</li>
<li>methods: The paper provides a systematic study and analysis of tensor-based regression models and their applications, including a comprehensive review of existing methods, their core ideas, and theoretical characteristics.</li>
<li>results: The paper covers the basics of tensor-based regression, provides examples of how to use existing methods to solve specific regression tasks with multiway data, and discusses available datasets and software resources for efficient implementation.<details>
<summary>Abstract</summary>
Regression analysis is a key area of interest in the field of data analysis and machine learning which is devoted to exploring the dependencies between variables, often using vectors. The emergence of high dimensional data in technologies such as neuroimaging, computer vision, climatology and social networks, has brought challenges to traditional data representation methods. Tensors, as high dimensional extensions of vectors, are considered as natural representations of high dimensional data. In this book, the authors provide a systematic study and analysis of tensor-based regression models and their applications in recent years. It groups and illustrates the existing tensor-based regression methods and covers the basics, core ideas, and theoretical characteristics of most tensor-based regression methods. In addition, readers can learn how to use existing tensor-based regression methods to solve specific regression tasks with multiway data, what datasets can be selected, and what software packages are available to start related work as soon as possible. Tensor Regression is the first thorough overview of the fundamentals, motivations, popular algorithms, strategies for efficient implementation, related applications, available datasets, and software resources for tensor-based regression analysis. It is essential reading for all students, researchers and practitioners of working on high dimensional data.
</details>
<details>
<summary>摘要</summary>
“tensor regression”是数据分析和机器学习领域的一个关键领域，旨在探索变量之间的依赖关系，通常使用向量。随着神经成像、计算机视觉、气候学和社交网络等技术的发展，传统的数据表示方法面临了挑战。tensor是高维数据的自然表示方法。本书提供了tensor-based regression模型的系统性研究和分析，以及其在最近几年的应用。它分组和描述了现有的tensor-based regression方法，覆盖基础知识、核心思想和主要特征。此外，读者还可以了解如何使用现有的tensor-based regression方法来解决特定的多向数据回归任务，选择合适的数据集和使用哪些软件包来进行相关工作。“tensor regression”是高维数据处理的基础知识，是所有师生、研究人员和实践者都必须阅读的一本书。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Distribution-Invariant-Fairness-Measures-for-Continuous-Scores"><a href="#Interpretable-Distribution-Invariant-Fairness-Measures-for-Continuous-Scores" class="headerlink" title="Interpretable Distribution-Invariant Fairness Measures for Continuous Scores"></a>Interpretable Distribution-Invariant Fairness Measures for Continuous Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11375">http://arxiv.org/abs/2308.11375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ann-Kristin Becker, Oana Dumitrasc, Klaus Broelemann</li>
<li>for: 这个论文主要是为了扩展对连续分数的算法公平性评估方法。</li>
<li>methods: 该论文提出了一种基于沃氏距离的公平性评估方法，该方法可以快速计算并且对不同模型、数据集或时间点进行比较。</li>
<li>results: 研究人员通过实验表明，新提出的公平性评估方法可以更好地捕捉到不同群体之间的差异，并且可以比较不同的模型、数据集或时间点之间的偏见。<details>
<summary>Abstract</summary>
Measures of algorithmic fairness are usually discussed in the context of binary decisions. We extend the approach to continuous scores. So far, ROC-based measures have mainly been suggested for this purpose. Other existing methods depend heavily on the distribution of scores, are unsuitable for ranking tasks, or their effect sizes are not interpretable. Here, we propose a distributionally invariant version of fairness measures for continuous scores with a reasonable interpretation based on the Wasserstein distance. Our measures are easily computable and well suited for quantifying and interpreting the strength of group disparities as well as for comparing biases across different models, datasets, or time points. We derive a link between the different families of existing fairness measures for scores and show that the proposed distributionally invariant fairness measures outperform ROC-based fairness measures because they are more explicit and can quantify significant biases that ROC-based fairness measures miss. Finally, we demonstrate their effectiveness through experiments on the most commonly used fairness benchmark datasets.
</details>
<details>
<summary>摘要</summary>
各种算法公平度量通常在二分类决策中被讨论。我们扩展了这种方法，以适应连续分数。目前，ROC基尼度量是为此目的提出的主要方法。其他现有方法受分布的影响很大，不适用于排名任务，或者其效果不能解释。我们提议一种不受分布影响的公平度量方法，基于温顿距离。我们的度量方法容易计算，适合量化和解释群体差异的强度以及不同模型、数据集、时间点之间的偏见。我们还 derivates了不同家族的现有公平度量方法之间的连接，并证明了我们提议的不受分布影响的公平度量方法在ROC基尼度量方法之上表现更好，因为它们更加明确，可以量化ROC基尼度量方法所过look的重要偏见。最后，我们通过使用最常用的公平性标准数据集进行实验，证明了它们的有效性。
</details></li>
</ul>
<hr>
<h2 id="How-Much-Temporal-Long-Term-Context-is-Needed-for-Action-Segmentation"><a href="#How-Much-Temporal-Long-Term-Context-is-Needed-for-Action-Segmentation" class="headerlink" title="How Much Temporal Long-Term Context is Needed for Action Segmentation?"></a>How Much Temporal Long-Term Context is Needed for Action Segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11358">http://arxiv.org/abs/2308.11358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ltcontext/ltcontext">https://github.com/ltcontext/ltcontext</a></li>
<li>paper_authors: Emad Bahrami, Gianpiero Francesca, Juergen Gall</li>
<li>for:  temporal action segmentation</li>
<li>methods:  transformer-based model with sparse attention</li>
<li>results:  best performance for temporal action segmentationHere’s the full text in Simplified Chinese:</li>
<li>for: 这篇论文是为了解决视频中的时间动作分割问题而写的。</li>
<li>methods: 这篇论文使用了 transformer 模型，并使用了稀谱注意力来捕捉整个视频的上下文。</li>
<li>results: 实验结果表明，模型需要捕捉整个视频的上下文，才能达到最佳的时间动作分割性能。<details>
<summary>Abstract</summary>
Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation. An interesting question that is still open is how much long-term temporal context is needed for optimal performance. While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window. While these approaches show good results, their performance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Breakfast, and Assembly101. Our experiments show that modeling the full context of a video is necessary to obtain the best performance for temporal action segmentation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>模型长期视频上下文是多个细致任务的关键，包括时间动作 segmentation。一个有趣的问题是如何多少长期时间上下文是必需的 для最佳性能。虽然变换器可以模型视频的长期上下文，但这会对长视频计算昂贵。现有的时间动作 segmentation方法通过将时间卷积网络与自注意力组合在一起，但其性能受到当前视频上下文的限制。在这个工作中，我们尝试回答如何多少长期时间上下文是必需的 для时间动作 segmentation，我们提出了一种基于变换器的模型，通过稀疏注意力来捕捉整个视频的上下文。我们与当前领域的状态速算三个数据集进行比较，分别是50Salads、Breakfast和Assembly101。我们的实验结果表明，模型整个视频的上下文是获得最佳性能的关键。
</details></li>
</ul>
<hr>
<h2 id="Semantic-RGB-D-Image-Synthesis"><a href="#Semantic-RGB-D-Image-Synthesis" class="headerlink" title="Semantic RGB-D Image Synthesis"></a>Semantic RGB-D Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11356">http://arxiv.org/abs/2308.11356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Li, Rong Li, Juergen Gall</li>
<li>for: 提高RGB-D图像Semantic分割的训练数据多样性</li>
<li>methods: 提出了一种基于Semantic图像Synthesis的方法，使用多模态数据生成真实的RGB-D图像</li>
<li>results: 比前一代单模态方法有大幅提高，并且通过混合实际和生成图像进行训练可以进一步提高RGB-D图像Semantic分割的精度<details>
<summary>Abstract</summary>
Collecting diverse sets of training images for RGB-D semantic image segmentation is not always possible. In particular, when robots need to operate in privacy-sensitive areas like homes, the collection is often limited to a small set of locations. As a consequence, the annotated images lack diversity in appearance and approaches for RGB-D semantic image segmentation tend to overfit the training data. In this paper, we thus introduce semantic RGB-D image synthesis to address this problem. It requires synthesising a realistic-looking RGB-D image for a given semantic label map. Current approaches, however, are uni-modal and cannot cope with multi-modal data. Indeed, we show that extending uni-modal approaches to multi-modal data does not perform well. In this paper, we therefore propose a generator for multi-modal data that separates modal-independent information of the semantic layout from the modal-dependent information that is needed to generate an RGB and a depth image, respectively. Furthermore, we propose a discriminator that ensures semantic consistency between the label maps and the generated images and perceptual similarity between the real and generated images. Our comprehensive experiments demonstrate that the proposed method outperforms previous uni-modal methods by a large margin and that the accuracy of an approach for RGB-D semantic segmentation can be significantly improved by mixing real and generated images during training.
</details>
<details>
<summary>摘要</summary>
Collecting diverse sets of training images for RGB-D semantic image segmentation 不一定是可能的。特别是当机器人需要在隐私敏感的地方 like 家庭中运行时，收集是经常受限于一小组地点。因此，标注图像缺乏多样性的外观和RGB-D semantic image segmentation 的方法容易过拟合训练数据。在这篇论文中，我们因此引入semantic RGB-D 图像合成来解决这个问题。它需要生成一个看起来很真实的 RGB-D 图像，以及一个给定的semantic label map。current approach是单模的，无法处理多模数据。我们实际上发现，将单模方法扩展到多模数据并不能达到好的效果。因此，我们提议一个生成器，可以分离modal-independent信息和modal-dependent信息。此外，我们还提议一个检验器，确保标注图像和生成的图像之间的semantic consistency，以及生成的图像和实际图像之间的 percepatual similarity。我们的全面实验表明，我们的方法可以大幅提高前一代单模方法的性能，并且可以在训练时混合实际和生成的图像，以提高RGB-D semantic segmentation的精度。
</details></li>
</ul>
<hr>
<h2 id="ProAgent-Building-Proactive-Cooperative-AI-with-Large-Language-Models"><a href="#ProAgent-Building-Proactive-Cooperative-AI-with-Large-Language-Models" class="headerlink" title="ProAgent: Building Proactive Cooperative AI with Large Language Models"></a>ProAgent: Building Proactive Cooperative AI with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11339">http://arxiv.org/abs/2308.11339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PKU-Alignment/ProAgent">https://github.com/PKU-Alignment/ProAgent</a></li>
<li>paper_authors: Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, Yitao Liang, Yaodong Yang</li>
<li>for: 这个论文主要目标是为了开发一种能够在人机合作中表现出突出的智能代理（ProAgent），帮助人机合作实现更高效的协同作业。</li>
<li>methods: 这个论文使用了大型自然语言模型（LLM）来为代理机制表现出更高级别的智能行为，包括预测合作伙伴的下一步决策并根据此形ulate更好的计划。</li>
<li>results: 实验结果表明，ProAgent在与其他AI代理和人类代理合作时表现出了remarkable的性能优势，比如自适应学习和人类学习等方法。此外，ProAgent还具有高度可模块化和可解释性，可以轻松地整合到各种协同enario中。<details>
<summary>Abstract</summary>
Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations conducted within the framework of \textit{Overcook-AI} unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training in cooperation with AI agents. Further, when cooperating with human proxy models, its performance exhibits an average improvement exceeding 10\% compared to the current state-of-the-art, COLE. The advancement was consistently observed across diverse scenarios involving interactions with both AI agents of varying characteristics and human counterparts. These findings inspire future research for human-robot collaborations. For a hands-on demonstration, please visit \url{https://pku-proagent.github.io}.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)建立AI具有适应行为的概念在人类-AI合作中是AGI研究中的核心焦点。目前，开发合作代理人的方法主要依赖于学习方法，其策略泛化强度受到特定团队成员的交互影响。这些方法限制了代理人的策略重塑能力，面临新的团队成员时。我们提出了\textbf{ProAgent}框架，利用大型自然语言模型（LLMs）为代理人带来了能预测伙伴的决策并提出改进的计划的能力。ProAgent在合作理解方面表现出色，可以适应团队合作中的各种情况，并且具有高度的可重新组合性和可读性，可以轻松地与不同的协调enario进行集成。在\textit{Overcook-AI}框架下，我们进行了实验评估，发现ProAgent在与基于自我玩家和人口学习的五种方法进行比较时，在合作 with AI代理人方面表现出了杰出的性能优势。此外，当与人类代理模型进行合作时，其性能表现出了平均提高超过10%，与当前的状态艺术COLE相比。这些发现在不同的情况下，包括与不同特征的AI代理人和人类对手进行交互时，均得到了证明。这些发现 inspirits future research on human-robot collaboration. For a hands-on demonstration, please visit \url{https://pku-proagent.github.io}.
</details></li>
</ul>
<hr>
<h2 id="On-the-Opportunities-and-Challenges-of-Offline-Reinforcement-Learning-for-Recommender-Systems"><a href="#On-the-Opportunities-and-Challenges-of-Offline-Reinforcement-Learning-for-Recommender-Systems" class="headerlink" title="On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems"></a>On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11336">http://arxiv.org/abs/2308.11336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaocong Chen, Siyu Wang, Julian McAuley, Dietmar Jannach, Lina Yao</li>
<li>for: 本研究旨在探讨在推荐系统中使用无线连接学习，特别是在不同的环境下进行学习和推荐。</li>
<li>methods: 本研究使用了无线连接学习的各种方法，包括Q-learning、SARSA和 Deep Q-Networks等，以学习用户的偏好和行为。</li>
<li>results: 研究发现，使用无线连接学习可以提高推荐系统的数据效率和学习速度，同时也可以提高用户的满意度和使用频率。<details>
<summary>Abstract</summary>
Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain limited. This survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domain. Furthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GrowCLIP-Data-aware-Automatic-Model-Growing-for-Large-scale-Contrastive-Language-Image-Pre-training"><a href="#GrowCLIP-Data-aware-Automatic-Model-Growing-for-Large-scale-Contrastive-Language-Image-Pre-training" class="headerlink" title="GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training"></a>GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11331">http://arxiv.org/abs/2308.11331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinchi Deng, Han Shi, Runhui Huang, Changlin Li, Hang Xu, Jianhua Han, James Kwok, Shen Zhao, Wei Zhang, Xiaodan Liang</li>
<li>for: 本文提出了一种数据驱动自动模型增长算法，用于对语言-图像做contrastive预训练，并且可以处理不断增长的在线数据。</li>
<li>methods: 本文使用了动态增长空间和最优化 архитектуры，以适应在线学习场景。同时，提出了共享编码器，以增强语言-图像融合度。</li>
<li>results: 相比已有方法，GrowCLIP在零参数图像分类任务上提高了2.3%的平均排名第一精度。在Flickr30K dataset上，GrowCLIP在零参数图像检索任务上提高了1.2%的找到第一图像-文本准确率。<details>
<summary>Abstract</summary>
Cross-modal pre-training has shown impressive performance on a wide range of downstream tasks, benefiting from massive image-text pairs collected from the Internet. In practice, online data are growing constantly, highlighting the importance of the ability of pre-trained model to learn from data that is continuously growing. Existing works on cross-modal pre-training mainly focus on training a network with fixed architecture. However, it is impractical to limit the model capacity when considering the continuously growing nature of pre-training data in real-world applications. On the other hand, it is important to utilize the knowledge in the current model to obtain efficient training and better performance. To address the above issues, in this paper, we propose GrowCLIP, a data-driven automatic model growing algorithm for contrastive language-image pre-training with continuous image-text pairs as input. Specially, we adopt a dynamic growth space and seek out the optimal architecture at each growth step to adapt to online learning scenarios. And the shared encoder is proposed in our growth space to enhance the degree of cross-modal fusion. Besides, we explore the effect of growth in different dimensions, which could provide future references for the design of cross-modal model architecture. Finally, we employ parameter inheriting with momentum (PIM) to maintain the previous knowledge and address the issue of the local minimum dilemma. Compared with the existing methods, GrowCLIP improves 2.3% average top-1 accuracy on zero-shot image classification of 9 downstream tasks. As for zero-shot image retrieval, GrowCLIP can improve 1.2% for top-1 image-to-text recall on Flickr30K dataset.
</details>
<details>
<summary>摘要</summary>
跨modal预训练已经在各种下游任务中显示出很好的性能，受到互联网上庞大的图片文本对的收集启发。在实践中，网络上数据不断增长，高亮了预训练数据的不断增长的重要性。现有的跨modal预训练方法主要是通过固定的网络架构进行训练。然而，在实际应用中，限制模型容量是不切实际的，因为预训练数据的不断增长会导致模型无法适应。相反，我们需要利用当前模型的知识来获得高效的训练和更好的性能。为此，在这篇论文中，我们提出了GrowCLIP，一种基于数据驱动的自动模型增长算法，用于对冲对的语言图片预训练。我们采用动态生长空间，在每个增长步骤中寻找最佳的网络架构，以适应在线学习场景。此外，我们还提出了共享编码器，以增强模型之间的混合度。此外，我们还研究了不同维度的增长效果，这可能会对未来的跨modal模型架构设计产生影响。最后，我们采用参数继承与势（PIM）来维护之前的知识，解决局部最小问题。相比之下，GrowCLIP与现有方法相比，提高了9个下游任务的zero-shot图像分类精度，平均提高2.3%。此外，GrowCLIP还可以提高Flickr30K dataset上的zero-shot图像检索的top-1图像文本恢复率，提高1.2%。
</details></li>
</ul>
<hr>
<h2 id="From-Mundane-to-Meaningful-AI’s-Influence-on-Work-Dynamics-–-evidence-from-ChatGPT-and-Stack-Overflow"><a href="#From-Mundane-to-Meaningful-AI’s-Influence-on-Work-Dynamics-–-evidence-from-ChatGPT-and-Stack-Overflow" class="headerlink" title="From Mundane to Meaningful: AI’s Influence on Work Dynamics – evidence from ChatGPT and Stack Overflow"></a>From Mundane to Meaningful: AI’s Influence on Work Dynamics – evidence from ChatGPT and Stack Overflow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11302">http://arxiv.org/abs/2308.11302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Gallea</li>
<li>for: 这篇论文探讨了如何利用生成AI实现代码编程的产生效率提升，同时也提出了这些新技术对工作和知识共享方式的影响。</li>
<li>methods: 该论文使用了 quasi-experimental 方法（差异分析），利用Stack Overflow上最大的在线编程社区的使用情况，评估 ChatGPT 的发布对编程问题的影响。</li>
<li>results: 研究发现，ChatGPT 的发布导致编程问题数量减少，同时问题的 докуumenation 质量也有所提高。此外，剩下的问题也变得更加复杂。这些结果表明，利用生成AI可以实现工作效率提升，同时也会导致工作方式的重大变革，让人类专注于更加复杂的任务。<details>
<summary>Abstract</summary>
This paper illustrates how generative AI could give opportunities for big productivity gains but also opens up questions about the impact of these new powerful technologies on the way we work and share knowledge. More specifically, we explore how ChatGPT changed a fundamental aspect of coding: problem-solving. To do so, we exploit the effect of the sudden release of ChatGPT on the 30th of November 2022 on the usage of the largest online community for coders: Stack Overflow. Using quasi-experimental methods (Difference-in-Difference), we find a significant drop in the number of questions. In addition, the questions are better documented after the release of ChatGPT. Finally, we find evidence that the remaining questions are more complex. These findings suggest not only productivity gains but also a fundamental change in the way we work where routine inquiries are solved by AI allowing humans to focus on more complex tasks.
</details>
<details>
<summary>摘要</summary>
In Simplified Chinese:这篇论文描述了如何生成AI可以带来大量的产出增长，但同时也提出了这些新技术对我们工作和知识分享方式的影响。我们Specifically，我们研究了ChatGPT如何改变编程中的问题解决方式。为此，我们利用了chatGPT于11月30日的突然发布对Stack Overflow上最大的编程社区的使用情况产生的影响。使用 quasi-experimental方法（Difference-in-Difference），我们发现了问题数量减少的显著影响。此外，剩下的问题更加详细。这些发现不仅表明产出增长，还表明了我们工作的基本变革，Routine inquiry由AI解决，让人类可以专注于更复杂的任务。
</details></li>
</ul>
<hr>
<h2 id="Improving-Knot-Prediction-in-Wood-Logs-with-Longitudinal-Feature-Propagation"><a href="#Improving-Knot-Prediction-in-Wood-Logs-with-Longitudinal-Feature-Propagation" class="headerlink" title="Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation"></a>Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11291">http://arxiv.org/abs/2308.11291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeremyfix/icvs2023">https://github.com/jeremyfix/icvs2023</a></li>
<li>paper_authors: Salim Khazem, Jeremy Fix, Cédric Pradalier</li>
<li>for: 本研究旨在预测木材内部缺陷的位置，以提高木材质量评估的准确性和效率。</li>
<li>methods: 本研究使用了卷积回归神经网络来解决木材外形特征与内部缺陷的Binary segmentation任务。</li>
<li>results: 研究表明，通过使用便宜的外形测量设备（如激光 Profiler）进行训练，可以通过卷积回归神经网络来预测木材内部缺陷的位置，并且可以在不同的树种上进行效果评估。<details>
<summary>Abstract</summary>
The quality of a wood log in the wood industry depends heavily on the presence of both outer and inner defects, including inner knots that are a result of the growth of tree branches. Today, locating the inner knots require the use of expensive equipment such as X-ray scanners. In this paper, we address the task of predicting the location of inner defects from the outer shape of the logs. The dataset is built by extracting both the contours and the knots with X-ray measurements. We propose to solve this binary segmentation task by leveraging convolutional recurrent neural networks. Once the neural network is trained, inference can be performed from the outer shape measured with cheap devices such as laser profilers. We demonstrate the effectiveness of our approach on fir and spruce tree species and perform ablation on the recurrence to demonstrate its importance.
</details>
<details>
<summary>摘要</summary>
木材行业中木材质量受到内部和外部缺陷的影响，包括由树木分支生长而成的内弯。今天，找到内弯需要使用昂贵的设备，如X射线扫描仪。在这篇论文中，我们解决了根据外形测量内弯的任务。我们提出使用卷积回归神经网络解决这个二分类任务。一旦神经网络训练完毕，可以通过便宜的设备，如激光 Profilers 进行推理。我们在桦树和落叶树种中展示了我们的方法的效果，并对循环的重要性进行了剖除。
</details></li>
</ul>
<hr>
<h2 id="ShadowNet-for-Data-Centric-Quantum-System-Learning"><a href="#ShadowNet-for-Data-Centric-Quantum-System-Learning" class="headerlink" title="ShadowNet for Data-Centric Quantum System Learning"></a>ShadowNet for Data-Centric Quantum System Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11290">http://arxiv.org/abs/2308.11290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Du, Yibo Yang, Tongliang Liu, Zhouchen Lin, Bernard Ghanem, Dacheng Tao</li>
<li>for: 本研究旨在探讨大量量子系统的动态学习问题，以减轻维度祸咎的影响。</li>
<li>methods: 本研究提出了一种数据驱动学习 paradigma，结合了神经网络协议和经典阴影，以便实现多种量子学习任务。</li>
<li>results: 研究表明，该 paradigma可以在偏远量子系统中提供准确和可靠的预测结果，并且可以在批处理大量量子系统时实现快速和高效的预测。<details>
<summary>Abstract</summary>
Understanding the dynamics of large quantum systems is hindered by the curse of dimensionality. Statistical learning offers new possibilities in this regime by neural-network protocols and classical shadows, while both methods have limitations: the former is plagued by the predictive uncertainty and the latter lacks the generalization ability. Here we propose a data-centric learning paradigm combining the strength of these two approaches to facilitate diverse quantum system learning (QSL) tasks. Particularly, our paradigm utilizes classical shadows along with other easily obtainable information of quantum systems to create the training dataset, which is then learnt by neural networks to unveil the underlying mapping rule of the explored QSL problem. Capitalizing on the generalization power of neural networks, this paradigm can be trained offline and excel at predicting previously unseen systems at the inference stage, even with few state copies. Besides, it inherits the characteristic of classical shadows, enabling memory-efficient storage and faithful prediction. These features underscore the immense potential of the proposed data-centric approach in discovering novel and large-scale quantum systems. For concreteness, we present the instantiation of our paradigm in quantum state tomography and direct fidelity estimation tasks and conduct numerical analysis up to 60 qubits. Our work showcases the profound prospects of data-centric artificial intelligence to advance QSL in a faithful and generalizable manner.
</details>
<details>
<summary>摘要</summary>
大量量子系统的动力学是由维度瓶颈所困难。统计学学习提供了新的可能性，通过神经网络协议和类型热影，然而两者都有局限性：前者受到预测不确定性的困扰，而后者缺乏泛化能力。我们提议一种数据驱动学习思想，结合这两种方法，以便实现多样化量子系统学习（QSL）任务。具体来说，我们的思想利用类型热影并与量子系统其他易获得信息一起创建训练集，然后通过神经网络学习探索QSL问题下的底层映射规则。通过神经网络的泛化能力，这种思想可以在训练阶段在线上培养，并在探索阶段预测未经见过的系统，即使只有几个状态的复制。此外，它继承类型热影的特点，即储存和预测的具有快速储存和准确预测的特点。这些特点强调了我们提议的数据驱动方法在发现新的大规模量子系统方面的极大潜力。为了证明这一点，我们在量子状态探测和直接准确率估计任务中实现了实例，并进行了数值分析至60个量子比特。我们的工作展示了数据驱动人工智能在忠实和泛化的方式下提高量子系统学习的可能性。
</details></li>
</ul>
<hr>
<h2 id="Recording-of-50-Business-Assignments"><a href="#Recording-of-50-Business-Assignments" class="headerlink" title="Recording of 50 Business Assignments"></a>Recording of 50 Business Assignments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12211">http://arxiv.org/abs/2308.12211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/50BusinessAssignmentsLog">https://github.com/microsoft/50BusinessAssignmentsLog</a></li>
<li>paper_authors: Michal Sroka, Mohammadreza Fani Sani</li>
<li>for: 本研究用于发现和分析用户如何完成业务任务，提供有价值的进程效率和优化的研究发现。</li>
<li>methods: 本文提供了50个真实的企业过程数据集，这些数据集有很大的研究应用potential，包括任务挖掘和过程自动化。</li>
<li>results: 本研究提供了一个有价值的数据集，这些数据集有助于研究人员和实践者了解进程效率和优化。<details>
<summary>Abstract</summary>
One of the main use cases of process mining is to discover and analyze how users follow business assignments, providing valuable insights into process efficiency and optimization. In this paper, we present a comprehensive dataset consisting of 50 real business processes. The dataset holds significant potential for research in various applications, including task mining and process automation which is a valuable resource for researchers and practitioners.
</details>
<details>
<summary>摘要</summary>
一个主要的用 caso of  proces mining 是发现和分析用户如何跟踪商业任务，提供有价值的信息来进行过程效率和优化。在这篇论文中，我们提供了完整的数据集，包含50个真实的商业过程。该数据集具有较高的研究价值，包括任务挖掘和过程自动化，是研究人员和实践者的宝贵资源。
</details></li>
</ul>
<hr>
<h2 id="CNN-based-Cuneiform-Sign-Detection-Learned-from-Annotated-3D-Renderings-and-Mapped-Photographs-with-Illumination-Augmentation"><a href="#CNN-based-Cuneiform-Sign-Detection-Learned-from-Annotated-3D-Renderings-and-Mapped-Photographs-with-Illumination-Augmentation" class="headerlink" title="CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation"></a>CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11277">http://arxiv.org/abs/2308.11277</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernst Stötzner, Timo Homburg, Hubert Mara</li>
<li>for: 针对ancient near eastern studies (DANES) 社区面临的挑战，我们开发了数字工具来处理篆字体系，这是一种3D文字痕迹在泥 TABLETS上的历史文化，覆盖了三千多年和至少八种主要语言。</li>
<li>methods: 我们使用了HeiCuBeDa和MaiCuBeDa数据集，包括约500个标注的板表，并提供了一种新的OCR-like方法来处理混合图像数据。我们的签名localization使用RepPoints探测器来预测字符的位置为 bounding boxes。我们使用了GigaMesh的MSII（曲率）基于的渲染、Phong-ashed 3D模型和照片，以及光照增强。</li>
<li>results: 使用渲染的3D图像进行签名检测比使用照片更好，而且我们的方法在混合数据集上表现良好，而且Phong renderings和特别是MSII renderings在照片上提高了结果。<details>
<summary>Abstract</summary>
Motivated by the challenges of the Digital Ancient Near Eastern Studies (DANES) community, we develop digital tools for processing cuneiform script being a 3D script imprinted into clay tablets used for more than three millennia and at least eight major languages. It consists of thousands of characters that have changed over time and space. Photographs are the most common representations usable for machine learning, while ink drawings are prone to interpretation. Best suited 3D datasets that are becoming available. We created and used the HeiCuBeDa and MaiCuBeDa datasets, which consist of around 500 annotated tablets. For our novel OCR-like approach to mixed image data, we provide an additional mapping tool for transferring annotations between 3D renderings and photographs. Our sign localization uses a RepPoints detector to predict the locations of characters as bounding boxes. We use image data from GigaMesh's MSII (curvature, see https://gigamesh.eu) based rendering, Phong-shaded 3D models, and photographs as well as illumination augmentation. The results show that using rendered 3D images for sign detection performs better than other work on photographs. In addition, our approach gives reasonably good results for photographs only, while it is best used for mixed datasets. More importantly, the Phong renderings, and especially the MSII renderings, improve the results on photographs, which is the largest dataset on a global scale.
</details>
<details>
<summary>摘要</summary>
受到数字古近东研究（DANES）社区的挑战启发，我们开发了数字工具来处理古代 Mesopotamia 文字，这是一种3D字符印制在泥版上，用于超过三千年和至少八种主要语言。它包含了数千个字符，随着时间和空间的变化而变化。 photographs 是最常用的机器学习 Representation，而墨水Drawing 容易被解释。我们创建了 HeiCuBeDa 和 MaiCuBeDa 数据集，包含约500个注释的泥版。为了我们的新的 OCR-like 方法，我们提供了一个附加的映射工具，用于将3D渲染与 photographs 之间的注释传输。我们使用 GigaMesh 的 MSII（曲率，见 <https://gigamesh.eu>）基于的渲染、Phong 灯光渲染和 photographs 以及照明增强。结果表明，使用3D渲染来检测字符性能更高于其他工作的 photographs 上。此外，我们的方法在 photographs 上提供了reasonably good的结果，而且在混合数据集上表现最佳。尤其是 Phong 渲染和 MSII 渲染，对于 photographs 上的结果有所提高。
</details></li>
</ul>
<hr>
<h2 id="Music-Understanding-LLaMA-Advancing-Text-to-Music-Generation-with-Question-Answering-and-Captioning"><a href="#Music-Understanding-LLaMA-Advancing-Text-to-Music-Generation-with-Question-Answering-and-Captioning" class="headerlink" title="Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning"></a>Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11276">http://arxiv.org/abs/2308.11276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, Ying Shan</li>
<li>for:  solves the problem of text-to-music generation (T2M-Gen) faced due to the scarcity of large-scale publicly available music datasets with natural language captions.</li>
<li>methods:  utilizes audio representations from a pretrained MERT model to extract music features, and introduces a methodology for generating question-answer pairs from existing audio captioning datasets, as well as the MusicQA Dataset designed for answering open-ended music-related questions.</li>
<li>results:  achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.<details>
<summary>Abstract</summary>
Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.
</details>
<details>
<summary>摘要</summary>
文本转换为乐曲生成（T2M-Gen）遇到了一个重要的障碍，即公共可用的大规模乐曲数据集中的自然语言描述缺乏。为解决这个问题，我们提议了Music Understanding LLaMA（MU-LLaMA），可以回答乐曲相关的问题并生成乐曲文件的描述。我们的模型利用了预训练的MERT模型来提取乐曲特征。但是获得合适的模型训练数据集仍然是一个挑战，因为现有的公共可用的音频问答数据集缺乏必要的深度来回答开放式乐曲问题。为了填补这个空白，我们提出了一种方法，可以将现有的音频描述数据集转换成问题-答案对，并介绍了MusicQA数据集，用于回答开放式乐曲相关的问题。实验结果表明，我们提出的MU-LLaMA模型，在我们设计的MusicQA数据集上进行训练，在多种纪录计中表现出色，超越当前的状态机（SOTA）模型在乐曲问题回答和乐曲描述生成方面，并为T2M-Gen研究领域带来了可期的进步。
</details></li>
</ul>
<hr>
<h2 id="Robust-Lagrangian-and-Adversarial-Policy-Gradient-for-Robust-Constrained-Markov-Decision-Processes"><a href="#Robust-Lagrangian-and-Adversarial-Policy-Gradient-for-Robust-Constrained-Markov-Decision-Processes" class="headerlink" title="Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes"></a>Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11267">http://arxiv.org/abs/2308.11267</a></li>
<li>repo_url: None</li>
<li>paper_authors: David M. Bossens</li>
<li>for: 本 paper 目的是提出一种基于 reinforcement learning 的任务模型框架，即 robust constrained Markov decision process (RCMDP)，该框架可以考虑行为约束和模型不确定性，并提供了对模型不确定性的鲁棒性。</li>
<li>methods: 本 paper 使用的方法包括：1) 基于值估计的最坏情况动力学；2) 基于拉格朗日点的最坏情况动力学；3) 对 RCMDP 的劣化策略算法。</li>
<li>results: 本 paper 的实验结果表明，提出的 two algorithms（RCPG with Robust Lagrangian 和 Adversarial RCPG）在 injecting perturbations 的 inventory management 和 safe navigation 任务中表现比较出色，特别是 Adversarial RCPG 在所有测试中排名第二。<details>
<summary>Abstract</summary>
The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy through gradient descent rather than indirectly and abruptly through constrained optimisation on a sorted value list. A theoretical analysis first derives the Lagrangian policy gradient for the policy optimisation of both proposed algorithms and then the adversarial policy gradient to learn the adversary for Adversarial RCPG. Empirical experiments injecting perturbations in inventory management and safe navigation tasks demonstrate the competitive performance of both algorithms compared to traditional RCPG variants as well as non-robust and non-constrained ablations. In particular, Adversarial RCPG ranks among the top two performing algorithms on all tests.
</details>
<details>
<summary>摘要</summary>
RCMDP（有约束的马尔可夫决策过程）是一种最近的任务模型框架，用于机器学习中的激励学习，它包含行为约束并提供了对转移动力学模型中的错误的Robustness。模拟RCMDP需要基于每个状态的值估计计算最差情况的动力学，这同RCPG（有约束的策略梯度）一样。在RCPG中，作者提出了两种算法，即RCPG with Robust Lagrangian和Adversarial RCPG。RCPG with Robust Lagrangian通过基于Lagrangian而不是值或约束来修改RCPG。Adversarial RCPG也基于Lagrangian，但是通过对敌对策略进行准确的梯度下降来直接和逐步地学习对敌。作者首先 derivates Lagrangian policy gradient для政策优化两个提出的算法，然后 derivates adversarial policy gradient来学习对敌。实验表明，两种算法在各种任务中表现竞争性，特别是Adversarial RCPG在所有测试中排名第二。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Last-iterate-Convergence-Algorithms-in-Solving-Games"><a href="#Efficient-Last-iterate-Convergence-Algorithms-in-Solving-Games" class="headerlink" title="Efficient Last-iterate Convergence Algorithms in Solving Games"></a>Efficient Last-iterate Convergence Algorithms in Solving Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11256">http://arxiv.org/abs/2308.11256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linjian Meng, Zhenxing Ge, Wenbin Li, Bo An, Yang Gao</li>
<li>For: The paper is written for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs) using no-regret algorithms.* Methods: The paper uses the Reward Transformation (RT) framework, which transforms the problem of learning NE in the original game into a series of strongly convex-concave optimization problems (SCCPs). The authors also use Regret Matching+ (RM+) algorithm to solve the SCCPs, and propose a novel transformation method to enable RM+ to solve the SCCPs.* Results: The paper shows that the proposed algorithm, Reward Transformation RM+ (RTRM+), enjoys last-iterate convergence under the discrete-time feedback setting, and significantly outperforms existing last-iterate convergence algorithms and RM+ (CFR+) in experiments.<details>
<summary>Abstract</summary>
No-regret algorithms are popular for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs). Many recent works consider the last-iterate convergence no-regret algorithms. Among them, the two most famous algorithms are Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weight Update (OMWU). However, OGDA has high per-iteration complexity. OMWU exhibits a lower per-iteration complexity but poorer empirical performance, and its convergence holds only when NE is unique. Recent works propose a Reward Transformation (RT) framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU. Unfortunately, RT-based algorithms perform worse than OGDA under the same number of iterations, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios. To address these issues, we provide a closer analysis of the RT framework, which holds for both continuous and discrete-time feedback. We demonstrate that the essence of the RT framework is to transform the problem of learning NE in the original game into a series of strongly convex-concave optimization problems (SCCPs). We show that the bottleneck of RT-based algorithms is the speed of solving SCCPs. To improve the their empirical performance, we design a novel transformation method to enable the SCCPs can be solved by Regret Matching+ (RM+), a no-regret algorithm with better empirical performance, resulting in Reward Transformation RM+ (RTRM+). RTRM+ enjoys last-iterate convergence under the discrete-time feedback setting. Using the counterfactual regret decomposition framework, we propose Reward Transformation CFR+ (RTCFR+) to extend RTRM+ to EFGs. Experimental results show that our algorithms significantly outperform existing last-iterate convergence algorithms and RM+ (CFR+).
</details>
<details>
<summary>摘要</summary>
“无后悔算法”受欢迎用于学习两player零余游戏（NFG）和广泛游戏（EFG）中的 Nash  equilibriium（NE）。许多最近的研究将注意力集中在最后迭代具有无后悔性的算法。其中最具知名度的两个算法是Optimistic Gradient Descent Ascent（OGDA）和Optimistic Multiplicative Weight Update（OMWU）。然而，OGDA的每迭代复杂度较高，而OMWU的实际性较差，且其对NE的唯一性Conditions不够严格。Recent works propose a Reward Transformation（RT） framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU。然而，RT-based algorithms under the same number of iterations than OGDA, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios。To address these issues, we provide a closer analysis of the RT framework, which holds for both continuous and discrete-time feedback。我们展示了RT framework的核心是将学习NE在原始游戏中的问题转换为一系列强oly convex-concave optimization problems（SCCPs）。我们显示了RT-based algorithms的瓶颈在SCCPs的解决方法。为了提高它们的实际性表现，我们设计了一个新的变换方法，让SCCPs可以通过Regret Matching+（RM+），一个无后悔算法，解决，从而产生Reward Transformation RM+（RTRM+）。RTRM+ 满足最后迭代具有无后悔性的条件。使用Counterfactual regret decomposition framework，我们提出Reward Transformation CFR+（RTCFR+）来扩展RTRM+到EFGs。实验结果显示我们的算法在已知的最后迭代具有无后悔性的算法和RM+（CFR+）中具有优秀的实际表现。
</details></li>
</ul>
<hr>
<h2 id="A-survey-on-bias-in-machine-learning-research"><a href="#A-survey-on-bias-in-machine-learning-research" class="headerlink" title="A survey on bias in machine learning research"></a>A survey on bias in machine learning research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11254">http://arxiv.org/abs/2308.11254</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aastha2104/Parkinson-Disease-Prediction">https://github.com/Aastha2104/Parkinson-Disease-Prediction</a></li>
<li>paper_authors: Agnieszka Mikołajczyk-Bareła, Michał Grochowski</li>
<li>for: 本研究旨在帮助理解机器学习中的偏见源泉和错误，以提高机器学习模型的公平、透明和准确性。</li>
<li>methods: 本文提供了四十个可能的机器学习漏洞和错误的示例，并对每个示例进行了详细的描述。</li>
<li>results: 本文通过对机器学习管道中的偏见和错误的分析，帮助开发者更好地检测和缓解偏见，从而提高机器学习模型的公平性和准确性。<details>
<summary>Abstract</summary>
Current research on bias in machine learning often focuses on fairness, while overlooking the roots or causes of bias. However, bias was originally defined as a "systematic error," often caused by humans at different stages of the research process. This article aims to bridge the gap between past literature on bias in research by providing taxonomy for potential sources of bias and errors in data and models. The paper focus on bias in machine learning pipelines. Survey analyses over forty potential sources of bias in the machine learning (ML) pipeline, providing clear examples for each. By understanding the sources and consequences of bias in machine learning, better methods can be developed for its detecting and mitigating, leading to fairer, more transparent, and more accurate ML models.
</details>
<details>
<summary>摘要</summary>
现有研究 often focuses on fairness 的偏见在机器学习中，而忽略了偏见的根源或 causa。然而，偏见原本是一种“系统性错误”，常由人类在不同阶段的研究过程中引入。这篇文章目的是 bridge the gap between past literature on bias in research by providing a taxonomy for potential sources of bias and errors in data and models. The paper focuses on bias in machine learning pipelines. The survey analyzes over forty potential sources of bias in the machine learning (ML) pipeline, providing clear examples for each. By understanding the sources and consequences of bias in machine learning, better methods can be developed for its detecting and mitigating, leading to fairer, more transparent, and more accurate ML models.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Modeling-Bends-in-Popular-Music-Guitar-Tablatures"><a href="#Modeling-Bends-in-Popular-Music-Guitar-Tablatures" class="headerlink" title="Modeling Bends in Popular Music Guitar Tablatures"></a>Modeling Bends in Popular Music Guitar Tablatures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12307">http://arxiv.org/abs/2308.12307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/adhooge1/bend-prediction">https://gitlab.com/adhooge1/bend-prediction</a></li>
<li>paper_authors: Alexandre D’Hooge, Louis Bigo, Ken Déguernel</li>
<li>for: 这篇论文主要用于研究 guitar 乐谱中的弯曲技巧，以及如何使用数据分析方法来预测弯曲的发生。</li>
<li>methods: 这篇论文使用了一些数据分析技术，包括决策树等，来研究弯曲的预测。</li>
<li>results: 实验结果表明，使用这些数据分析技术可以准确预测弯曲的发生，并且具有一定的可靠性和精度。<details>
<summary>Abstract</summary>
Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on/pull-off or bends.This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard. In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 anda limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of non-guitar music into guitar tablatures.
</details>
<details>
<summary>摘要</summary>
Tablaturenotation是流行音乐中广泛使用的notation方式，用于记录和分享吉他乐器的音乐内容。作为标准notation的补充，tablaturenotation记录了演奏手势信息，包括手指位置和许多特有的吉他演奏技巧，如滑弹、弹压和弯曲。本文关注的是弯曲，它可以使演奏者在不改变 физиical fretted fingerboard的前提下，逐渐改变音符的抑制值。在本文中，我们提出了25个高级特征，用于研究如何通过检测短期内的前后文来预测琴曲中的弯曲出现。实验使用了932首流行乐器琴曲tablature，并显示了一棵决策树可以成功预测琴曲中的弯曲出现，F1分数为0.71，并且具有有限的假阳性预测，这表明可以用于将非吉他音乐转换成琴曲tablature。
</details></li>
</ul>
<hr>
<h2 id="Multi-Source-Domain-Adaptation-for-Cross-Domain-Fault-Diagnosis-of-Chemical-Processes"><a href="#Multi-Source-Domain-Adaptation-for-Cross-Domain-Fault-Diagnosis-of-Chemical-Processes" class="headerlink" title="Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes"></a>Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11247">http://arxiv.org/abs/2308.11247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo Fernandes Montesuma, Michela Mulas, Fred Ngolè Mboula, Francesco Corona, Antoine Souloumiac</li>
<li>for: 这种研究旨在提高过程监测中的故障诊断精度，具体来说是利用机器学习算法预测故障类型基于感知器读ings。</li>
<li>methods: 该研究使用单源预处理适应（SSDA）和多源预处理适应（MSDA）算法进行故障诊断，并在田东曼进程中进行了广泛的比较。</li>
<li>results: 研究显示，在多源场景下使用多个预处理源可以提高故障诊断精度，具体来说是比单源场景提高23%的平均精度。此外，无适应情况下，多源场景可以提高不适应精度的平均提升率为8.4%。<details>
<summary>Abstract</summary>
Fault diagnosis is an essential component in process supervision. Indeed, it determines which kind of fault has occurred, given that it has been previously detected, allowing for appropriate intervention. Automatic fault diagnosis systems use machine learning for predicting the fault type from sensor readings. Nonetheless, these models are sensible to changes in the data distributions, which may be caused by changes in the monitored process, such as changes in the mode of operation. This scenario is known as Cross-Domain Fault Diagnosis (CDFD). We provide an extensive comparison of single and multi-source unsupervised domain adaptation (SSDA and MSDA respectively) algorithms for CDFD. We study these methods in the context of the Tennessee-Eastmann Process, a widely used benchmark in the chemical industry. We show that using multiple domains during training has a positive effect, even when no adaptation is employed. As such, the MSDA baseline improves over the SSDA baseline classification accuracy by 23% on average. In addition, under the multiple-sources scenario, we improve classification accuracy of the no adaptation setting by 8.4% on average.
</details>
<details>
<summary>摘要</summary>
FAULT诊断是 proces supervision 中的一个重要组件。它可以确定哪种缺陷已经发生，只要它已经检测到了，然后采取相应的 intervención。自动FAULT诊断系统 使用机器学习来预测缺陷类型从感知值中。然而，这些模型对数据分布的变化敏感，这些变化可能是由监测过程中的变化所致，如操作模式的变化。这种情况被称为 Cross-Domain Fault Diagnosis (CDFD)。我们提供了广泛的单源和多源无监督领域适应 (SSDA 和 MSDA 分别) 算法的 Comparative study  для CDFD。我们在 Tennessee-Eastmann 过程中进行了这些方法的研究，这是化学工业中广泛使用的一个标准测试 benchmark。我们发现在训练时使用多个频道有益，即使没有适应也。因此，MSDA 基线提高了 SSDA 基eline 的分类精度，在 average 上提高了 23%。此外，在多源场景下，我们在无适应情况下提高了分类精度的 average 上提高了 8.4%。
</details></li>
</ul>
<hr>
<h2 id="Faster-Optimization-in-S-Graphs-Exploiting-Hierarchy"><a href="#Faster-Optimization-in-S-Graphs-Exploiting-Hierarchy" class="headerlink" title="Faster Optimization in S-Graphs Exploiting Hierarchy"></a>Faster Optimization in S-Graphs Exploiting Hierarchy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11242">http://arxiv.org/abs/2308.11242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hriday Bavle, Jose Luis Sanchez-Lopez, Javier Civera, Holger Voos<br>for:This paper aims to improve the scalability of Situational Graphs (S-Graphs) in large environments for Simultaneous Localization and Mapping (SLAM) by marginalizing redundant robot poses and their connections to observations.methods:The proposed method generates and optimizes room-local graphs encompassing all graph entities within a room-like structure, compresses the S-Graphs, and performs windowed local optimization of the compressed graph at regular time-distance intervals. Global optimization is performed every time a loop closure is detected.results:The proposed method achieves similar accuracy compared to the baseline while reducing the computation time by 39.81% compared to the baseline.<details>
<summary>Abstract</summary>
3D scene graphs hierarchically represent the environment appropriately organizing different environmental entities in various layers. Our previous work on situational graphs extends the concept of 3D scene graph to SLAM by tightly coupling the robot poses with the scene graph entities, achieving state-of-the-art results. Though, one of the limitations of S-Graphs is scalability in really large environments due to the increased graph size over time, increasing the computational complexity.   To overcome this limitation in this work we present an initial research of an improved version of S-Graphs exploiting the hierarchy to reduce the graph size by marginalizing redundant robot poses and their connections to the observations of the same structural entities. Firstly, we propose the generation and optimization of room-local graphs encompassing all graph entities within a room-like structure. These room-local graphs are used to compress the S-Graphs marginalizing the redundant robot keyframes within the given room. We then perform windowed local optimization of the compressed graph at regular time-distance intervals. A global optimization of the compressed graph is performed every time a loop closure is detected. We show similar accuracy compared to the baseline while showing a 39.81% reduction in the computation time with respect to the baseline.
</details>
<details>
<summary>摘要</summary>
三维场景图 hierarchically represents the environment, appropriately organizing different environmental entities in various layers. Our previous work on situational graphs extends the concept of 3D scene graph to SLAM by tightly coupling the robot poses with the scene graph entities, achieving state-of-the-art results. However, one of the limitations of S-Graphs is scalability in really large environments due to the increased graph size over time, increasing the computational complexity. To overcome this limitation, in this work we present an initial research of an improved version of S-Graphs by exploiting the hierarchy to reduce the graph size by marginalizing redundant robot poses and their connections to the observations of the same structural entities. First, we propose the generation and optimization of room-local graphs encompassing all graph entities within a room-like structure. These room-local graphs are used to compress the S-Graphs marginalizing the redundant robot keyframes within the given room. We then perform windowed local optimization of the compressed graph at regular time-distance intervals. A global optimization of the compressed graph is performed every time a loop closure is detected. We show similar accuracy compared to the baseline while showing a 39.81% reduction in computation time with respect to the baseline.
</details></li>
</ul>
<hr>
<h2 id="An-Effective-Transformer-based-Contextual-Model-and-Temporal-Gate-Pooling-for-Speaker-Identification"><a href="#An-Effective-Transformer-based-Contextual-Model-and-Temporal-Gate-Pooling-for-Speaker-Identification" class="headerlink" title="An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification"></a>An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11241">http://arxiv.org/abs/2308.11241</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harunorikawano/speaker-identification-with-tgp">https://github.com/harunorikawano/speaker-identification-with-tgp</a></li>
<li>paper_authors: Harunori Kawano, Sota Shimizu</li>
<li>for: 这个研究是为了开发一个高精度的 speaker identification 模型，使用 Transformer 架构和自我超vised learning。</li>
<li>methods: 本研究使用了 Transformer-based contextual model，并进一步提出了 Temporal Gate Pooling 方法来增强模型的学习能力。</li>
<li>results: 研究获得了使用 VoxCeleb1 资料集进行认个者识别 tasks 的85.9%的精度，与 wav2vec2 的317.7M个parameters相比，这个方法具有较高的精度和较低的 Parameters 数量。<details>
<summary>Abstract</summary>
Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at https://github.com/HarunoriKawano/speaker-identification-with-tgp.
</details>
<details>
<summary>摘要</summary>
它使用 transformer 架构和自动学习来实现了speech recognition的成功。最近，这些技术不仅用于speech recognition，还用于整个speech processing。这篇论文介绍了一种高效的端到端speaker identification模型，该模型使用 transformer-based 上下文模型。我们研究了参数与性能之间的关系，以便理解高效模型的结构。此外，我们提出了一种pooling方法， named Temporal Gate Pooling，具有强大的学习能力。我们使用 Conformer 作为编码器，并使用 BEST-RQ 进行预训练。我们对 VoxCeleb1 进行了评估，并实现了85.9%的准确率，相比之下，wav2vec2 的参数数量为317.7M，我们的方法准确率相对较高。代码可以在 GitHub 上找到：https://github.com/HarunoriKawano/speaker-identification-with-tgp。
</details></li>
</ul>
<hr>
<h2 id="ROSGPT-Vision-Commanding-Robots-Using-Only-Language-Models’-Prompts"><a href="#ROSGPT-Vision-Commanding-Robots-Using-Only-Language-Models’-Prompts" class="headerlink" title="ROSGPT_Vision: Commanding Robots Using Only Language Models’ Prompts"></a>ROSGPT_Vision: Commanding Robots Using Only Language Models’ Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11236">http://arxiv.org/abs/2308.11236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bilel-bj/rosgpt_vision">https://github.com/bilel-bj/rosgpt_vision</a></li>
<li>paper_authors: Bilel Benjdira, Anis Koubaa, Anas M. Ali</li>
<li>for: 这个论文主要是提出一种新的 робо控制方法，使用语言模型提示来控制 робо。</li>
<li>methods: 该方法使用语言模型和视觉模型结合在一起，通过自动化机制来执行 robotic 任务。</li>
<li>results: 这个方法可以减少 robotic 开发成本，并且可以在实际应用中提高应用质量。Here’s a more detailed explanation of each point:</li>
<li>for: The paper proposes a new method for controlling robots using only language prompts, which is a significant departure from traditional methods that rely on technical details and programming.</li>
<li>methods: The method uses a combination of language models and vision models to automate the mechanisms behind the prompts, allowing the robot to execute tasks based on natural language descriptions.</li>
<li>results: The method has been shown to reduce development costs and improve the quality of applications, and it has the potential to advance robotic research in this direction.I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
In this paper, we argue that the next generation of robots can be commanded using only Language Models' prompts. Every prompt interrogates separately a specific Robotic Modality via its Modality Language Model (MLM). A central Task Modality mediates the whole communication to execute the robotic mission via a Large Language Model (LLM). This paper gives this new robotic design pattern the name of: Prompting Robotic Modalities (PRM). Moreover, this paper applies this PRM design pattern in building a new robotic framework named ROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using only two prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in natural language, the visual semantic features related to the task under consideration (Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the robotic reaction to the visual description (Task Modality). The framework automates all the mechanisms behind these two prompts. The framework enables the robot to address complex real-world scenarios by processing visual data, making informed decisions, and carrying out actions automatically. The framework comprises one generic vision module and two independent ROS nodes. As a test application, we used ROSGPT_Vision to develop CarMate, which monitors the driver's distraction on the roads and makes real-time vocal notifications to the driver. We showed how ROSGPT_Vision significantly reduced the development cost compared to traditional methods. We demonstrated how to improve the quality of the application by optimizing the prompting strategies, without delving into technical details. ROSGPT_Vision is shared with the community (link: https://github.com/bilel-bj/ROSGPT_Vision) to advance robotic research in this direction and to build more robotic frameworks that implement the PRM design pattern and enables controlling robots using only prompts.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们 argueThat the next generation of robots can be commanded using only Language Models' prompts. Every prompt interrogates separately a specific Robotic Modality via its Modality Language Model (MLM). A central Task Modality mediates the whole communication to execute the robotic mission via a Large Language Model (LLM). This paper gives this new robotic design pattern the name of: Prompting Robotic Modalities (PRM). Moreover, this paper applies this PRM design pattern in building a new robotic framework named ROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using only two prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in natural language, the visual semantic features related to the task under consideration (Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the robotic reaction to the visual description (Task Modality). The framework automates all the mechanisms behind these two prompts. The framework enables the robot to address complex real-world scenarios by processing visual data, making informed decisions, and carrying out actions automatically. The framework comprises one generic vision module and two independent ROS nodes. As a test application, we used ROSGPT_Vision to develop CarMate, which monitors the driver's distraction on the roads and makes real-time vocal notifications to the driver. We showed how ROSGPT_Vision significantly reduced the development cost compared to traditional methods. We demonstrated how to improve the quality of the application by optimizing the prompting strategies, without delving into technical details. ROSGPT_Vision is shared with the community (link: https://github.com/bilel-bj/ROSGPT_Vision) to advance robotic research in this direction and to build more robotic frameworks that implement the PRM design pattern and enables controlling robots using only prompts.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-White-Box-Watermarking-with-Self-Mutual-Check-Parameters-in-Deep-Neural-Networks"><a href="#Adaptive-White-Box-Watermarking-with-Self-Mutual-Check-Parameters-in-Deep-Neural-Networks" class="headerlink" title="Adaptive White-Box Watermarking with Self-Mutual Check Parameters in Deep Neural Networks"></a>Adaptive White-Box Watermarking with Self-Mutual Check Parameters in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11235">http://arxiv.org/abs/2308.11235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenzhe Gao, Zhaoxia Yin, Hongjian Zhan, Heng Yin, Yue Lu</li>
<li>for: 检测和防止人工智能模型中的意外或恶意篡改。</li>
<li>methods: 使用敏感 watermarking 技术来识别和检测篡改。</li>
<li>results: 测试结果表明，当篡改率低于 20% 时，我们的方法可以达到很高的恢复性能。而对于受到 watermarking 影响的模型，我们使用可适应位数技术来恢复模型的精度。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) has found wide application, but also poses risks due to unintentional or malicious tampering during deployment. Regular checks are therefore necessary to detect and prevent such risks. Fragile watermarking is a technique used to identify tampering in AI models. However, previous methods have faced challenges including risks of omission, additional information transmission, and inability to locate tampering precisely. In this paper, we propose a method for detecting tampered parameters and bits, which can be used to detect, locate, and restore parameters that have been tampered with. We also propose an adaptive embedding method that maximizes information capacity while maintaining model accuracy. Our approach was tested on multiple neural networks subjected to attacks that modified weight parameters, and our results demonstrate that our method achieved great recovery performance when the modification rate was below 20%. Furthermore, for models where watermarking significantly affected accuracy, we utilized an adaptive bit technique to recover more than 15% of the accuracy loss of the model.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在应用广泛，但也存在风险，因为在部署过程中可能会有不恰当或恶意篡改。因此， Regular checks 是必要的，以检测和预防这些风险。某些攻击可能会导致模型参数的篡改，因此我们需要一种方法来检测和修复篡改的参数。在这篇论文中，我们提出了一种用于检测篡改参数和位数的方法，可以用来检测、定位和修复篡改的参数。此外，我们还提出了一种适应式嵌入方法，可以最大化信息容量，同时保持模型的准确性。我们的方法在多个神经网络上进行了测试，并达到了篡改率低于20%时的恢复性能。此外，对于模型中 watermarking 对准确性产生了较大的影响，我们使用适应位数技术来恢复模型的准确性，达到了超过15%的恢复效果。
</details></li>
</ul>
<hr>
<h2 id="Traffic-Flow-Optimisation-for-Lifelong-Multi-Agent-Path-Finding"><a href="#Traffic-Flow-Optimisation-for-Lifelong-Multi-Agent-Path-Finding" class="headerlink" title="Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding"></a>Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11234">http://arxiv.org/abs/2308.11234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Chen, Daniel Harabor, Jiaoyang Li, Peter J. Stuckey</li>
<li>for: 解决多 Agent 路径规划问题，即 robotics 中多 Agent 需要计算共享地图上免撞的路径。</li>
<li>methods: 提出一种新的方法，使用填充避免拥填的路径引导 Agent 达到目的地。</li>
<li>results: 在一shot MAPF 和 Lifelong MAPF 两个大规模场景中，提供了较好的解决方案，对一shot MAPF 的解决质量做出了重要改进，对 Lifelong MAPF 的总 Throughput 做出了大幅提高。<details>
<summary>Abstract</summary>
Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that asks us to compute collision-free paths for a team of agents, all moving across a shared map. Although many works appear on this topic, all current algorithms struggle as the number of agents grows. The principal reason is that existing approaches typically plan free-flow optimal paths, which creates congestion. To tackle this issue we propose a new approach for MAPF where agents are guided to their destination by following congestion-avoiding paths. We evaluate the idea in two large-scale settings: one-shot MAPF, where each agent has a single destination, and lifelong MAPF, where agents are continuously assigned new tasks. For one-shot MAPF we show that our approach substantially improves solution quality. For Lifelong MAPF we report large improvements in overall throughput.
</details>
<details>
<summary>摘要</summary>
多智能路径找（MAPF）是 robotics 中的基本问题，它要求我们计算多个智能机器人在共享地图上的冲突自由路径。虽然有很多研究对这个问题进行了努力，但现有的方法都难以承受多个机器人的增加。主要的原因是现有的方法通常计划自由流优化路径，这会导致堵塞。为解决这个问题，我们提出了一种新的 MAPF 方法，即使 agents 跟随堵塞避免路径来达到目的地。我们在两个大规模设置中评估了这个想法：一个是一次 MAPF，每个机器人都有单个目的地；另一个是持续 MAPF，机器人 continuous 被分配新任务。对一次 MAPF 我们显示了我们的方法可以显著提高解决方案质量。对持续 MAPF 我们报告了大幅提高总吞吐量。
</details></li>
</ul>
<hr>
<h2 id="On-Premise-AIOps-Infrastructure-for-a-Software-Editor-SME-An-Experience-Report"><a href="#On-Premise-AIOps-Infrastructure-for-a-Software-Editor-SME-An-Experience-Report" class="headerlink" title="On-Premise AIOps Infrastructure for a Software Editor SME: An Experience Report"></a>On-Premise AIOps Infrastructure for a Software Editor SME: An Experience Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11225">http://arxiv.org/abs/2308.11225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anes Bendimerad, Youcef Remil, Romain Mathonat, Mehdi Kaytoue</li>
<li>for: 本研究旨在探讨在企业内部实施基于开源工具的AIOps解决方案，以提高软件维护和监测。</li>
<li>methods: 本研究使用开源工具构建了一套完整的AIOps基础设施，并提供了不同选择的原则和策略。</li>
<li>results: 研究结果表明，使用开源工具构建AIOps基础设施可以减少成本和提高软件维护效率，同时也可以满足公司的数据管理和安全需求。<details>
<summary>Abstract</summary>
Information Technology has become a critical component in various industries, leading to an increased focus on software maintenance and monitoring. With the complexities of modern software systems, traditional maintenance approaches have become insufficient. The concept of AIOps has emerged to enhance predictive maintenance using Big Data and Machine Learning capabilities. However, exploiting AIOps requires addressing several challenges related to the complexity of data and incident management. Commercial solutions exist, but they may not be suitable for certain companies due to high costs, data governance issues, and limitations in covering private software. This paper investigates the feasibility of implementing on-premise AIOps solutions by leveraging open-source tools. We introduce a comprehensive AIOps infrastructure that we have successfully deployed in our company, and we provide the rationale behind different choices that we made to build its various components. Particularly, we provide insights into our approach and criteria for selecting a data management system and we explain its integration. Our experience can be beneficial for companies seeking to internally manage their software maintenance processes with a modern AIOps approach.
</details>
<details>
<summary>摘要</summary>
信息技术已成为不同行业的关键组成部分，导致软件维护和监测得到了更大的关注。由于现代软件系统的复杂性，传统维护方法已成为不足。AIOps概念出现以增强预测维护，通过大数据和机器学习技术来提高维护效率。然而，使用AIOps存在数据复杂性和事件管理等挑战。 comercial解决方案存在，但它们可能不适用于某些公司，因为高成本、数据管理问题和私有软件的限制。本文探讨在公司内部实施On-premise AIOps解决方案的可行性，通过使用开源工具。我们介绍了一个完整的AIOps基础设施，我们在公司中成功地部署了这个基础设施，并提供了不同组件的选择原则。尤其是数据管理系统的选择和集成方法。我们的经验可以帮助公司通过现代AIOps方法 internally管理软件维护过程。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Large-Language-Models-on-Graphs-Performance-Insights-and-Comparative-Analysis"><a href="#Evaluating-Large-Language-Models-on-Graphs-Performance-Insights-and-Comparative-Analysis" class="headerlink" title="Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis"></a>Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11224">http://arxiv.org/abs/2308.11224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ayame1006/llmtograph">https://github.com/ayame1006/llmtograph</a></li>
<li>paper_authors: Chang Liu, Bo Wu</li>
<li>for: 这个研究旨在评估四种大语言模型（LLMs）在处理图数据上的应用能力。</li>
<li>methods: 该研究使用了四种不同的评估指标：理解、正确性、准确性和修复能力。</li>
<li>results: 研究发现，LLMs可以很好地理解图数据的自然语言描述，并且可以对图结构进行有效的推理。GPT模型在正确性方面表现出色，而其他模型则在结构理解方面表现较差。GPT模型在多个答案 зада题上常出现错误答案，这可能会降低其修复能力。另外，GPT-4能够修复GPT-3.5-turbo和自己之前的迭代的答案。研究代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/Ayame1006/LLMtoGraph%E3%80%82">https://github.com/Ayame1006/LLMtoGraph。</a><details>
<summary>Abstract</summary>
Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has demonstrated the capacity to rectify responses from GPT-3.5-turbo and its own previous iterations. The code is available at: https://github.com/Ayame1006/LLMtoGraph.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在学术和业务领域都受到了广泛关注。然而，对图数据的应用仍然尚未得到充分探索。本研究通过评估四种LLM在解决多个分析问题上的能力来评估LLM在图数据上的应用。我们采用了四种评估指标：理解、正确性、准确性和修复。我们的结果显示：1）LLM可以很好地理解图数据的自然语言描述和图结构的关系。2）GPT模型在正确性方面表现出色，超越了其他选择。3）所有考试LLM都面临着结构理解的挑战，特别是零shot链条思维和几个shot提示的效果减退。4）GPT模型在多个答案任务中很容易出现错误答案，这可能会影响它们的准确性。5）GPT模型表现出高度自信心，这可能会阻碍它们的修复能力。备注的是，GPT-4已经示出了可以修复GPT-3.5-turbo和自己的前一个迭代的能力。代码可以在 GitHub上找到：https://github.com/Ayame1006/LLMtoGraph。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-on-Patient-Data-for-Privacy-Protecting-Polycystic-Ovary-Syndrome-Treatment"><a href="#Federated-Learning-on-Patient-Data-for-Privacy-Protecting-Polycystic-Ovary-Syndrome-Treatment" class="headerlink" title="Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment"></a>Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11220">http://arxiv.org/abs/2308.11220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/toriqiu/fl-pcos">https://github.com/toriqiu/fl-pcos</a></li>
<li>paper_authors: Lucia Morris, Tori Qiu, Nikhil Raghuraman</li>
<li>for: 这篇论文是为了探讨 Federated Learning（FL）在预测女性淋巴疾病多囊卵巢综合症（PCOS）的优化药物方案。</li>
<li>methods: 这篇论文使用了多种 Federated Learning 方法，并在人工合成 PCOS 患者数据集上进行了测试。</li>
<li>results: 研究发现，这些 Federated Learning 方法在论文中提出的数据隐私保护和药物优选问题上都具有出色的表现。<details>
<summary>Abstract</summary>
The field of women's endocrinology has trailed behind data-driven medical solutions, largely due to concerns over the privacy of patient data. Valuable datapoints about hormone levels or menstrual cycling could expose patients who suffer from comorbidities or terminate a pregnancy, violating their privacy. We explore the application of Federated Learning (FL) to predict the optimal drug for patients with polycystic ovary syndrome (PCOS). PCOS is a serious hormonal disorder impacting millions of women worldwide, yet it's poorly understood and its research is stunted by a lack of patient data. We demonstrate that a variety of FL approaches succeed on a synthetic PCOS patient dataset. Our proposed FL models are a tool to access massive quantities of diverse data and identify the most effective treatment option while providing PCOS patients with privacy guarantees.
</details>
<details>
<summary>摘要</summary>
妇女激素学的应用落后于数据驱动医疗解决方案，主要是由于患者数据隐私问题的担忧。 valuable datapoints about 激素水平或月经周期可能暴露患有并发症或中止怀孕的患者，违反其隐私。 我们探讨了 Federated Learning（FL）的应用，以预测患有多囊卵巢综合症（PCOS）患者最佳药物。 PCOS 是世界上数百万女性中的一种严重激素失衡症，但它的研究受到缺乏患者数据的限制。 我们示出了多种 FL 方法在 sintetic PCOS 患者数据集上得到成功。我们的提议的 FL 模型是一种访问庞大数据量和鉴别最有效的治疗方案的工具，同时为 PCOS 患者提供隐私保证。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-in-Big-Model-Era-Domain-Specific-Multimodal-Large-Models"><a href="#Federated-Learning-in-Big-Model-Era-Domain-Specific-Multimodal-Large-Models" class="headerlink" title="Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models"></a>Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11217">http://arxiv.org/abs/2308.11217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zengxiang Li, Zhaoxiang Hou, Hui Liu, Ying Wang, Tongzhi Li, Longfei Xie, Chao Shi, Chengyi Yang, Weishan Zhang, Zelei Liu, Liang Xu</li>
<li>for: 这篇论文旨在提出一种多模态联合学习框架，帮助多家企业通过私有领域数据来共同训练大型模型，实现多enario智能服务。</li>
<li>methods: 论文提出了多模态联合学习的策略性转型，包括智能基础和目标的重要性在大模型时代，以及在多种数据、模型聚合、性能和成本交易、数据隐私和奖励机制等方面的新挑战。</li>
<li>results: 论文介绍了一个城市安全运营管理案例研究，其中多家企业共同提供多模态数据和专业知识，实现了城市安全运营管理的分布部署和有效协调。初步实验表明，企业可以通过多模态模型联合学习提高和储存智能能力，共同创造出高质量智能服务，涵盖能源基础设施安全、住宅社区安全和城市运营管理等多个领域。<details>
<summary>Abstract</summary>
Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient coordination of the federated learning platform, technical innovations on data quality improvement based on large model capabilities and efficient joint fine-tuning approaches. Preliminary experiments show that enterprises can enhance and accumulate intelligent capabilities through multimodal model federated learning, thereby jointly creating an smart city model that provides high-quality intelligent services covering energy infrastructure safety, residential community security, and urban operation management. The established federated learning cooperation ecosystem is expected to further aggregate industry, academia, and research resources, realize large models in multiple vertical domains, and promote the large-scale industrial application of artificial intelligence and cutting-edge research on multimodal federated learning.
</details>
<details>
<summary>摘要</summary>
多模式数据，能够全面感知和识别物理世界，已成为通往普遍人工智能的关键Path。然而，多模式大型模型在公共数据集上训练时经常表现不佳在特定行业领域。这篇论文提出了一种多模式联合学习框架，允许多家企业使用私有领域数据共同训练大型模型，以实现多场景智能服务。作者对联合学习在智能基础和目标时代的战略性转变进行了深入探讨，以及新的多样数据、模型汇集、性能和成本负担、数据隐私和奖励机制等挑战。论文还介绍了一个城市安全运营管理案例研究，包括分布式部署和有效协调联合学习平台，以及基于大型模型技术的数据质量改进和高效联合练习方法。初步实验显示，企业可以通过多模式模型联合学习增强和积累智能能力，共同创造出高质量智能服务，涵盖能源基础设施安全、住宅社区安全和城市运营管理。建立的联合学习合作生态系统预期会进一步吸引产业、学术和研究资源，实现多个垂直领域的大型模型，并推动人工智能和多模式联合学习的大规模产业应用。
</details></li>
</ul>
<hr>
<h2 id="ConcatPlexer-Additional-Dim1-Batching-for-Faster-ViTs"><a href="#ConcatPlexer-Additional-Dim1-Batching-for-Faster-ViTs" class="headerlink" title="ConcatPlexer: Additional Dim1 Batching for Faster ViTs"></a>ConcatPlexer: Additional Dim1 Batching for Faster ViTs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11199">http://arxiv.org/abs/2308.11199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donghoon Han, Seunghyeon Seo, Donghyeon Jeon, Jiho Jang, Chaerin Kong, Nojun Kwak<br>for: 这个研究旨在提高视觉识别的效率，以提高模型的测试速度和精度。methods: 本研究使用了一种叫做Data Multiplexing（DataMUX）的成本削减方法，并将其应用到视觉模型中。它还导入了一个名为Image Multiplexer的新方法，以及一些新的组件，以解决DataMux在视觉模型中的弱点。results: 研究发现，使用ConcatPlexer模型可以大幅提高视觉识别的启动速度，同时保持了69.5%和83.4%的验证精度。相比之下，ViT-B&#x2F;16模型需要23.5%更多的GFLOPs以达到相同的验证精度。<details>
<summary>Abstract</summary>
Transformers have demonstrated tremendous success not only in the natural language processing (NLP) domain but also the field of computer vision, igniting various creative approaches and applications. Yet, the superior performance and modeling flexibility of transformers came with a severe increase in computation costs, and hence several works have proposed methods to reduce this burden. Inspired by a cost-cutting method originally proposed for language models, Data Multiplexing (DataMUX), we propose a novel approach for efficient visual recognition that employs additional dim1 batching (i.e., concatenation) that greatly improves the throughput with little compromise in the accuracy. We first introduce a naive adaptation of DataMux for vision models, Image Multiplexer, and devise novel components to overcome its weaknesses, rendering our final model, ConcatPlexer, at the sweet spot between inference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and CIFAR100 dataset and it achieved 23.5% less GFLOPs than ViT-B/16 with 69.5% and 83.4% validation accuracy, respectively.
</details>
<details>
<summary>摘要</summary>
transformers 在自然语言处理（NLP）领域表现出色，同时在计算机视觉领域也引发了多种创新应用。然而，transformers 的高性能和模型灵活性却导致计算成本增加，因此许多研究团队提出了降低计算成本的方法。 draw inspiration from language models 的 cost-cutting method，我们提出了一种新的方法 для高效的视觉识别，即图像多重化（Image Multiplexer）。我们首先介绍了图像多重化的原型，然后开发了新的组件来缓解其缺点，最终得到了我们的最终模型——ConcatPlexer。ConcatPlexer 在 ImageNet1K 和 CIFAR100  dataset 上训练，与 ViT-B/16 的 GFLOPs 相比，减少了 23.5%，而 validation accuracy 则达到了 69.5% 和 83.4%。
</details></li>
</ul>
<hr>
<h2 id="ViLLA-Fine-Grained-Vision-Language-Representation-Learning-from-Real-World-Data"><a href="#ViLLA-Fine-Grained-Vision-Language-Representation-Learning-from-Real-World-Data" class="headerlink" title="ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data"></a>ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11194">http://arxiv.org/abs/2308.11194</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanfordmimi/villa">https://github.com/stanfordmimi/villa</a></li>
<li>paper_authors: Maya Varma, Jean-Benoit Delbrouck, Sarah Hooper, Akshay Chaudhari, Curtis Langlotz</li>
<li>For: 这种研究旨在评估当前的视觉语言模型（VLM）在高度复杂的多模态数据上的表现，以及如何改进VLM以更好地捕捉高度复杂的图像区域和文本特征之间的关系。* Methods: 该研究使用了一种名为ViLLA的新方法，它包括一个轻量级自动生成的地图模型和一个对比度VLM，以学习从复杂数据中捕捉高度复杂的区域特征和文本特征之间的关系。* Results: 研究表明，在四个领域（合成图像、产品图像、医疗图像和自然图像）上，ViLLA可以在精细的理解任务中表现出色，比如零shot对象检测（COCO上AP50点为3.6，LVIS上mAP点为0.6）和检索任务（R-Precision点为14.2）。<details>
<summary>Abstract</summary>
Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained on datasets consisting of image-caption pairs obtained from the web. However, real-world multimodal datasets, such as healthcare data, are significantly more complex: each image (e.g. X-ray) is often paired with text (e.g. physician report) that describes many distinct attributes occurring in fine-grained regions of the image. We refer to these samples as exhibiting high pairwise complexity, since each image-text pair can be decomposed into a large number of region-attribute pairings. The extent to which VLMs can capture fine-grained relationships between image regions and textual attributes when trained on such data has not been previously evaluated. The first key contribution of this work is to demonstrate through systematic evaluations that as the pairwise complexity of the training dataset increases, standard VLMs struggle to learn region-attribute relationships, exhibiting performance degradations of up to 37% on retrieval tasks. In order to address this issue, we introduce ViLLA as our second key contribution. ViLLA, which is trained to capture fine-grained region-attribute relationships from complex datasets, involves two components: (a) a lightweight, self-supervised mapping model to decompose image-text samples into region-attribute pairs, and (b) a contrastive VLM to learn representations from generated region-attribute pairs. We demonstrate with experiments across four domains (synthetic, product, medical, and natural images) that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks, such as zero-shot object detection (up to 3.6 AP50 points on COCO and 0.6 mAP points on LVIS) and retrieval (up to 14.2 R-Precision points).
</details>
<details>
<summary>摘要</summary>
视力语言模型（VLM），如CLIP和ALIGN，通常在图像-描述文本对 obtained from the web 上进行训练。然而，真实世界多Modal数据，如医疗数据，是非常复杂的：每个图像（例如 X-ray）通常与描述多个细腻区域的文本（例如医生报告）相对应。我们称这些样本为具有高对比复杂性，因为每个图像-文本对可以被分解成大量的区域-特征对。VLM 是否能够在这些数据上捕捉细腻的区域-特征关系，尚未被评估。我们的第一个关键贡献是通过系统性的评估表明，随着对于训练数据的对比复杂性的增加，标准的 VLM 会遇到性能下降，最高达37% 的搜索任务。为解决这个问题，我们介绍了我们的第二个关键贡献——ViLLA。ViLLA 是一种可以从复杂数据中捕捉细腻区域-特征关系的模型，它包括两个组件：（a）一种轻量级、自动学习的映射模型，用于将图像-文本对分解成区域-特征对，以及（b）一种对比 VLM，用于从生成的区域-特征对中学习表示。我们通过在四个领域（人工、产品、医疗和自然图像）进行实验，证明 ViLLA 在细腻理解任务中（例如零shot物体检测和搜索）表现出色，比较类似 VLM 高出3.6 AP50 点和0.6 mAP 点。
</details></li>
</ul>
<hr>
<h2 id="Diversity-Measures-Domain-Independent-Proxies-for-Failure-in-Language-Model-Queries"><a href="#Diversity-Measures-Domain-Independent-Proxies-for-Failure-in-Language-Model-Queries" class="headerlink" title="Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries"></a>Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11189">http://arxiv.org/abs/2308.11189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lab-v2/diversity_measures">https://github.com/lab-v2/diversity_measures</a></li>
<li>paper_authors: Noel Ngu, Nathaniel Lee, Paulo Shakarian</li>
<li>for: 这篇论文旨在提供一些基于它的各种应用领域的错误预测方法，以便更好地评估大语言模型的性能。</li>
<li>methods: 这篇论文使用了三种不同的方法来衡量大语言模型的错误程度，即熵度、金尼鲁分离度和中心距离。这些方法不仅可以独立地评估模型的性能，还可以应用于几个不同的应用场景，如几个示例问题、链式思维和错误检测。</li>
<li>results: 根据实验结果，这三种方法都强相关于模型的失败概率。此外，这些方法还可以应用于不同的数据集和温度设置，并且可以用于评估模型的性能。<details>
<summary>Abstract</summary>
Error prediction in large language models often relies on domain-specific information. In this paper, we present measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt - hence independent of the underlying application. We describe how three such measures - based on entropy, Gini impurity, and centroid distance - can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.
</details>
<details>
<summary>摘要</summary>
大型语言模型中的错误预测通常需要对特定领域的信息。在这篇论文中，我们提出了基于响应中的弹性、盖尼不纯和中心距离的三种度量来衡量大型语言模型的错误。我们描述了如何使用这三种度量来评估大型语言模型的错误probability，并在多个数据集和温度设定下进行了一系列实验，以示这些度量强相关于错误的可能性。此外，我们还提供了实验结果，说明了如何使用这些度量来应用少量提示、链接思维和错误探测。
</details></li>
</ul>
<hr>
<h2 id="MISSRec-Pre-training-and-Transferring-Multi-modal-Interest-aware-Sequence-Representation-for-Recommendation"><a href="#MISSRec-Pre-training-and-Transferring-Multi-modal-Interest-aware-Sequence-Representation-for-Recommendation" class="headerlink" title="MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation"></a>MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11175">http://arxiv.org/abs/2308.11175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinpeng Wang, Ziyun Zeng, Yunxiao Wang, Yuting Wang, Xingyu Lu, Tianxiang Li, Jun Yuan, Rui Zhang, Hai-Tao Zheng, Shu-Tao Xia</li>
<li>for: 这篇研究旨在解决缺乏ID特征的问题，以及实际推荐情况中的冷开头问题。</li>
<li>methods: 本研究提出了一个多 modal 信息学习框架，包括一个基于 transformer 架构的使用者端 encoder-decoder 模型，以及一个适应项目端的动态融合模块。</li>
<li>results: 实验结果显示，MISSRec 能够实现高效的实际推荐情况下的推荐。<details>
<summary>Abstract</summary>
The goal of sequential recommendation (SR) is to predict a user's potential interested items based on her/his historical interaction sequences. Most existing sequential recommenders are developed based on ID features, which, despite their widespread use, often underperform with sparse IDs and struggle with the cold-start problem. Besides, inconsistent ID mappings hinder the model's transferability, isolating similar recommendation domains that could have been co-optimized. This paper aims to address these issues by exploring the potential of multi-modal information in learning robust and generalizable sequence representations. We propose MISSRec, a multi-modal pre-training and transfer learning framework for SR. On the user side, we design a Transformer-based encoder-decoder model, where the contextual encoder learns to capture the sequence-level multi-modal synergy while a novel interest-aware decoder is developed to grasp item-modality-interest relations for better sequence representation. On the candidate item side, we adopt a dynamic fusion module to produce user-adaptive item representation, providing more precise matching between users and items. We pre-train the model with contrastive learning objectives and fine-tune it in an efficient manner. Extensive experiments demonstrate the effectiveness and flexibility of MISSRec, promising an practical solution for real-world recommendation scenarios.
</details>
<details>
<summary>摘要</summary>
目标是强化用户潜在有趣的ITEM predication，基于她/his历史交互序列。现有的大多数分列推荐器都是基于ID特征，尽管广泛使用，但它们经常在罕见ID下表现不佳，并且困难解决冷启动问题。此外，不一致的ID映射使模型的可移植性受阻，隔离类似推荐领域的相似性，这些领域可能可以协同优化。这篇论文旨在解决这些问题，通过学习多 modal 信息来学习Robust和可 generalized 序列表示。我们提议MISSRec，一种多 modal 预训练和传输学习框架，用于SR。用户方面，我们设计了一个基于Transformer的Encoder-Decoder模型，其中Contextual Encoder 学习 capture 序列级别多 modal 共谐，而一种新的兴趣相关 Decoder 被开发以更好地捕捉ITEM-modality-兴趣关系，以提高序列表示。候选ITEM 方面，我们采用动态融合模块生成用户适应ITEM表示，为用户和ITEM之间更精准的匹配提供更多的精度。我们在对照学习目标下预训练模型，并在有效的方式进行精度调整。广泛的实验表明MISSRec的有效性和灵活性，提供了实际解决现实推荐场景中的实际解决方案。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Point-based-Active-Learning-for-Semi-supervised-Point-Cloud-Semantic-Segmentation"><a href="#Hierarchical-Point-based-Active-Learning-for-Semi-supervised-Point-Cloud-Semantic-Segmentation" class="headerlink" title="Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation"></a>Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11166">http://arxiv.org/abs/2308.11166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SmiletoE/HPAL">https://github.com/SmiletoE/HPAL</a></li>
<li>paper_authors: Zongyi Xu, Bo Yuan, Shanshan Zhao, Qianni Zhang, Xinbo Gao</li>
<li>for: 本研究旨在 Addressing the issue of limited annotations in 3D point cloud segmentation using active learning.</li>
<li>methods: 方法包括一个层次最小准确度模块，以及一种特征距离抑制策略，以选择重要和代表性的点 для人工标注。此外，基于这个活动策略，我们还建立了一个半监督分割框架。</li>
<li>results: 实验结果表明，提档的方法可以达到96.5%和100%的完全监督基线性能，只需使用0.07%和0.1%的训练数据。这些结果超越了当前最佳弱监督和活动学习方法。代码将在<a target="_blank" rel="noopener" href="https://github.com/SmiletoE/HPAL%E4%B8%AD%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/SmiletoE/HPAL中发布。</a><details>
<summary>Abstract</summary>
Impressive performance on point cloud semantic segmentation has been achieved by fully-supervised methods with large amounts of labelled data. As it is labour-intensive to acquire large-scale point cloud data with point-wise labels, many attempts have been made to explore learning 3D point cloud segmentation with limited annotations. Active learning is one of the effective strategies to achieve this purpose but is still under-explored. The most recent methods of this kind measure the uncertainty of each pre-divided region for manual labelling but they suffer from redundant information and require additional efforts for region division. This paper aims at addressing this issue by developing a hierarchical point-based active learning strategy. Specifically, we measure the uncertainty for each point by a hierarchical minimum margin uncertainty module which considers the contextual information at multiple levels. Then, a feature-distance suppression strategy is designed to select important and representative points for manual labelling. Besides, to better exploit the unlabelled data, we build a semi-supervised segmentation framework based on our active strategy. Extensive experiments on the S3DIS and ScanNetV2 datasets demonstrate that the proposed framework achieves 96.5% and 100% performance of fully-supervised baseline with only 0.07% and 0.1% training data, respectively, outperforming the state-of-the-art weakly-supervised and active learning methods. The code will be available at https://github.com/SmiletoE/HPAL.
</details>
<details>
<summary>摘要</summary>
具有印象的表现在点云semantic segmentation方面已经由完全监督的方法实现了出色的成绩。由于获得大规模点云数据和点 wise标签的劳动密集，许多尝试已经被作出以探索学习3D点云 segmentation的方法。活动学习是这种目标的有效策略之一，但是仍然受到了不足的探索。最近的这些方法会测量每个预分区的uncertainty，但它们受到重复信息的困扰和需要额外的劳动进行区分。这篇论文目的在于解决这个问题，通过开发一种层次 minimum margin uncertainty module来测量每个点的uncertainty，考虑多个层次的contextual信息。然后，我们设计了一种特征距离抑制策略，以选择重要和代表性的点进行手动标注。此外，为了更好地利用无标注数据，我们构建了基于我们的活动策略的半supervised segmentation框架。广泛的实验在S3DIS和ScanNetV2数据集上表明，我们的提案的框架可以与完全监督基准相同的96.5%和100%的性能，只使用0.07%和0.1%的训练数据。此外，我们的方法还能够超越当前的弱监督和活动学习方法。代码将在https://github.com/SmiletoE/HPAL上提供。
</details></li>
</ul>
<hr>
<h2 id="xxMD-Benchmarking-Neural-Force-Fields-Using-Extended-Dynamics-beyond-Equilibrium"><a href="#xxMD-Benchmarking-Neural-Force-Fields-Using-Extended-Dynamics-beyond-Equilibrium" class="headerlink" title="xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium"></a>xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11155">http://arxiv.org/abs/2308.11155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zpengmei/xxmd">https://github.com/zpengmei/xxmd</a></li>
<li>paper_authors: Zihan Pengmei, Junyu Liu, Yinan Shu</li>
<li>For: The paper aims to address the limitations of current neural force field (NFF) models in representing chemical reactions by introducing a new dataset called xxMD, which includes energies and forces computed from both multireference wave function theory and density functional theory.* Methods: The paper uses a constrained distribution of internal coordinates and energies in the MD17 datasets to demonstrate their inadequacy for representing systems undergoing chemical reactions. The authors then introduce the xxMD dataset, which includes nuclear configuration spaces that authentically depict chemical reactions, making it a more chemically relevant dataset.* Results: The authors re-assess equivariant models on the xxMD datasets and find notably higher mean absolute errors than those reported for MD17 and its variants, highlighting the challenges faced in crafting a generalizable NFF model with extrapolation capability. The authors propose two new datasets, xxMD-CASSCF and xxMD-DFT, which are available online.<details>
<summary>Abstract</summary>
Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional theory. Furthermore, its nuclear configuration spaces authentically depict chemical reactions, making xxMD a more chemically relevant dataset. Our re-assessment of equivariant models on the xxMD datasets reveals notably higher mean absolute errors than those reported for MD17 and its variants. This observation underscores the challenges faced in crafting a generalizable NFF model with extrapolation capability. Our proposed xxMD-CASSCF and xxMD-DFT datasets are available at \url{https://github.com/zpengmei/xxMD}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploring-Unsupervised-Cell-Recognition-with-Prior-Self-activation-Maps"><a href="#Exploring-Unsupervised-Cell-Recognition-with-Prior-Self-activation-Maps" class="headerlink" title="Exploring Unsupervised Cell Recognition with Prior Self-activation Maps"></a>Exploring Unsupervised Cell Recognition with Prior Self-activation Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11144">http://arxiv.org/abs/2308.11144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cpystan/psm">https://github.com/cpystan/psm</a></li>
<li>paper_authors: Pingyi Chen, Chenglu Zhu, Zhongyi Shui, Jiatong Cai, Sunyi Zheng, Shichuan Zhang, Lin Yang</li>
<li>for: 本研究旨在降低生物标注成本，提高生物图像识别效果。</li>
<li>methods: 我们提出了一种基于自动激活图的方法，通过自动激活图中的特征来生成假标记。然后，我们引入了一种语义归一化模块，将假标记转换为像素级别的语义假标记。</li>
<li>results: 我们在两个 histological 数据集上进行评估，结果表明我们的方法可以与其他全盘和弱盘方法竞争，而无需任何手动标注。此外，我们的简单 yet 有效的框架还可以实现多类细胞检测，这在已有的无监督方法中无法完成。<details>
<summary>Abstract</summary>
The success of supervised deep learning models on cell recognition tasks relies on detailed annotations. Many previous works have managed to reduce the dependency on labels. However, considering the large number of cells contained in a patch, costly and inefficient labeling is still inevitable. To this end, we explored label-free methods for cell recognition. Prior self-activation maps (PSM) are proposed to generate pseudo masks as training targets. To be specific, an activation network is trained with self-supervised learning. The gradient information in the shallow layers of the network is aggregated to generate prior self-activation maps. Afterward, a semantic clustering module is then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo masks for downstream tasks. We evaluated our method on two histological datasets: MoNuSeg (cell segmentation) and BCData (multi-class cell detection). Compared with other fully-supervised and weakly-supervised methods, our method can achieve competitive performance without any manual annotations. Our simple but effective framework can also achieve multi-class cell detection which can not be done by existing unsupervised methods. The results show the potential of PSMs that might inspire other research to deal with the hunger for labels in medical area.
</details>
<details>
<summary>摘要</summary>
Successful supervised deep learning models for cell recognition rely heavily on detailed annotations. However, obtaining these annotations can be costly and inefficient. To address this issue, we explored label-free methods for cell recognition. Our proposed method uses prior self-activation maps (PSMs) to generate pseudo masks as training targets. Specifically, we train an activation network using self-supervised learning to generate the PSMs, and then use a semantic clustering module to transform the PSMs into pixel-level semantic pseudo masks for downstream tasks. We evaluated our method on two histological datasets (MoNuSeg and BCData) and found that it can achieve competitive performance without any manual annotations. Our method is simple but effective, and can also perform multi-class cell detection, which is not possible with existing unsupervised methods. The results demonstrate the potential of PSMs to address the need for labels in medical applications.
</details></li>
</ul>
<hr>
<h2 id="Is-There-Any-Social-Principle-for-LLM-Based-Agents"><a href="#Is-There-Any-Social-Principle-for-LLM-Based-Agents" class="headerlink" title="Is There Any Social Principle for LLM-Based Agents?"></a>Is There Any Social Principle for LLM-Based Agents?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11136">http://arxiv.org/abs/2308.11136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jitao Bai, Simiao Zhang, Zhonghao Chen</li>
<li>for: 这篇论文主要是关于大语言模型基于代理的应用。</li>
<li>methods: 论文使用了大语言模型来实现代理，并考虑了社会科学的应用。</li>
<li>results: 论文提出了一种新的代理方法，并通过实验证明了其效果。In English, this translates to:</li>
<li>for: This paper is primarily about the application of large language models based on proxies.</li>
<li>methods: The paper uses large language models to implement proxies and considers applications in social sciences.</li>
<li>results: The paper proposes a new proxy method and experiments prove its effectiveness.<details>
<summary>Abstract</summary>
Focus on Large Language Model based agents should involve more than "human-centered" alignment or application. We argue that more attention should be paid to the agent itself and discuss the potential of social sciences for agents.
</details>
<details>
<summary>摘要</summary>
大语言模型基于代理应该超出人类中心的启aligned或应用。我们认为代理本身应该受到更多的注意力，并讨论社会科学在代理方面的潜力。Here's a word-for-word translation:大语言模型基于代理应该超出人类中心的启aligned或应用。我们认为代理本身应该受到更多的注意力，并讨论社会科学在代理方面的潜力。Note that Simplified Chinese is the standard writing system used in mainland China, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="ReLLa-Retrieval-enhanced-Large-Language-Models-for-Lifelong-Sequential-Behavior-Comprehension-in-Recommendation"><a href="#ReLLa-Retrieval-enhanced-Large-Language-Models-for-Lifelong-Sequential-Behavior-Comprehension-in-Recommendation" class="headerlink" title="ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation"></a>ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11131">http://arxiv.org/abs/2308.11131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, Weinan Zhang</li>
<li>for: 这 paper 主要针对 recommendation  зада务中的 zero-shot 和 few-shot 设置，以提高大语言模型 (LLM) 的表现。</li>
<li>methods: 该 paper 提出了一种 novel 框架，名为 Retrieval-enhanced Large Language models (ReLLa)，用于解决 LLM 在 recommendation 领域中的各种问题。</li>
<li>results: 经过广泛的实验，ReLLa 表现出优于现有基线模型，并能够解决 LLM 在长期序列行为理解方面的问题。<details>
<summary>Abstract</summary>
With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data quality of testing samples, which greatly reduces the difficulty for LLMs to extract the essential knowledge from user behavior sequences. As for few-shot recommendation, we further design retrieval-enhanced instruction tuning (ReiT) by adopting SUBR as a data augmentation technique for training samples. Specifically, we develop a mixed training dataset consisting of both the original data samples and their retrieval-enhanced counterparts. We conduct extensive experiments on a real-world public dataset (i.e., MovieLens-1M) to demonstrate the superiority of ReLLa compared with existing baseline models, as well as its capability for lifelong sequential behavior comprehension.
</details>
<details>
<summary>摘要</summary>
Large language models (LLMs) 在自然语言处理（NLP）领域取得了显著的突破， LLM-enhanced recommender systems 也在当前得到了广泛的关注。在这篇论文中，我们关注在适应和强化纯大语言模型（LLM）的零shot和几shot推荐任务上。首先，我们识别和描述了 LLM 在推荐领域中的生命周期行为无法理解问题，即 LLM 无法从用户行为序列中提取有用信息，即使用户行为序列的长度远远超过 LLM 的上下文限制。为解决这一问题并提高 LLM 的推荐性能，我们提出了一种新的框架，即 Retrieval-enhanced Large Language models (ReLLa)，用于零shot和几shot的推荐任务。 для零shot推荐，我们实施了 semantic user behavior retrieval (SUBR)，以提高测试样本的数据质量，从而减轻 LLM 提取用户行为序列中的关键知识的困难。而为了几shot推荐，我们进一步设计了 retrieval-enhanced instruction tuning (ReiT)，通过采用 SUBR 作为数据增强技术来培育训练样本。具体来说，我们构建了一个混合训练集，包括原始数据样本和其增强后的对应样本。我们在一个真实的公共数据集（即 MovieLens-1M）上进行了广泛的实验，以证明 ReLLa 与现有基eline模型相比，具有更高的优势，同时也能够解决生命周期行为无法理解问题。
</details></li>
</ul>
<hr>
<h2 id="Transformers-for-Capturing-Multi-level-Graph-Structure-using-Hierarchical-Distances"><a href="#Transformers-for-Capturing-Multi-level-Graph-Structure-using-Hierarchical-Distances" class="headerlink" title="Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances"></a>Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11129">http://arxiv.org/abs/2308.11129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuankai Luo</li>
<li>for: 本研究旨在提出一种基于层次结构编码的图变换器，以提高图变换器对不同类型图的表现。</li>
<li>methods: 本研究使用了一种名为层次距离结构编码（HDSE）的方法，该方法利用图中节点之间的层次距离来建模图的多层次结构。</li>
<li>results: 经过对12个实际数据集的广泛实验，研究发现，使用HDSE方法可以成功地提高多种基eline transformers的表现，在10个标准测试集上实现了状态的领先性表现。<details>
<summary>Abstract</summary>
Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
</details>
<details>
<summary>摘要</summary>
GRaph transformers需要强大的推导性偏好，以derive meaningful attention scores。然而，当前的提议 rarely address methods capturing longer ranges, hierarchical structures, or community structures，as they appear in various graphs such as molecules, social networks, and citation networks。在这篇论文中，我们提议了一种层次距离结构编码(HDSE)，该模型在图中节点之间的层次距离，强调图的多层、层次结构。特别是，这种方法可以flexibly integrate with existing graph transformers，allowing for simultaneous application with other positional representations。通过对12个实际 dataset进行了广泛的实验，我们证明了我们的HDSE方法成功地提高了多种基eline transformers的性能，达到了10个标准 benchmark dataset的状态态表现。
</details></li>
</ul>
<hr>
<h2 id="CAME-Contrastive-Automated-Model-Evaluation"><a href="#CAME-Contrastive-Automated-Model-Evaluation" class="headerlink" title="CAME: Contrastive Automated Model Evaluation"></a>CAME: Contrastive Automated Model Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11111">http://arxiv.org/abs/2308.11111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pengr/contrastive_autoeval">https://github.com/pengr/contrastive_autoeval</a></li>
<li>paper_authors: Ru Peng, Qiuyang Duan, Haobo Wang, Jiachen Ma, Yanbo Jiang, Yongjun Tu, Xiu Jiang, Junbo Zhao</li>
<li>for: 本研究旨在提出一种新的自动模型评估（AutoEval）框架，以便评估训练完成的机器学习模型而无需使用标注测试集。</li>
<li>methods: 该框架基于一种新的对比损失函数，通过对比测试集中的模型表现和训练集中的模型表现来评估模型的性能。</li>
<li>results: 研究人员通过实验证明，CAME框架可以在AutoEval中达到新的最佳性能水平，超过先前的工作。<details>
<summary>Abstract</summary>
The Automated Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set. Despite the promise and some decent results, the existing AutoEval methods heavily rely on computing distribution shifts between the unlabelled testing set and the training set. We believe this reliance on the training set becomes another obstacle in shipping this technology to real-world ML development. In this work, we propose Contrastive Automatic Model Evaluation (CAME), a novel AutoEval framework that is rid of involving training set in the loop. The core idea of CAME bases on a theoretical analysis which bonds the model performance with a contrastive loss. Further, with extensive empirical validation, we manage to set up a predictable relationship between the two, simply by deducing on the unlabeled/unseen testing set. The resulting framework CAME establishes a new SOTA results for AutoEval by surpassing prior work significantly.
</details>
<details>
<summary>摘要</summary>
autoeval框架可能无需使用标注测试集来评估已经训练的机器学习模型。尽管存在承诺和一些不错的结果，现有的autoeval方法都仰赖计算分布shift между无标测试集和训练集。我们认为这种依赖于训练集的方法会成为实际ml开发中的另一个障碍。在这项工作中，我们提出了对比自动评估（CAME）框架，它不再需要使用训练集。CAME的核心想法基于对模型性能与对比损失的理论分析。我们通过大量的实验验证，成功地建立了对比测试集上的模型性能和对比损失之间的可预测关系。这种关系可以通过对无标测试集进行推理来获得。CAME的框架在autoeval领域创造了新的最佳实践（SOTA）结果，超过了之前的工作。
</details></li>
</ul>
<hr>
<h2 id="Anonymity-at-Risk-Assessing-Re-Identification-Capabilities-of-Large-Language-Models"><a href="#Anonymity-at-Risk-Assessing-Re-Identification-Capabilities-of-Large-Language-Models" class="headerlink" title="Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models"></a>Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11103">http://arxiv.org/abs/2308.11103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skatinger/anonymity-at-risk-assessing-re-identification-capabilities-of-large-language-models">https://github.com/skatinger/anonymity-at-risk-assessing-re-identification-capabilities-of-large-language-models</a></li>
<li>paper_authors: Alex Nyffenegger, Matthias Stürmer, Joel Niklaus</li>
<li>for: The paper explores the potential of large language models (LLMs) to re-identify individuals in court rulings, with a focus on privacy protection in the European Union and Switzerland.</li>
<li>methods: The authors construct a proof-of-concept using actual legal data from the Swiss federal supreme court and create an anonymized Wikipedia dataset for more rigorous testing. They introduce new metrics to measure performance and systematically analyze the factors that influence successful re-identifications.</li>
<li>results: Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions due to a lack of test datasets, the need for substantial training resources, and data sparsity in the information used for re-identification. The study concludes that re-identification using LLMs may not be feasible for now, but it could become possible in the future.Here is the information in Simplified Chinese text:</li>
<li>for: 本研究探讨了大语言模型（LLMs）在法律案例中重新标识个人的可能性，强调欧盟和瑞士隐私保护。</li>
<li>methods: 作者们使用实际的瑞士最高法院判决文档构建了证明，并创建了一个匿名的Wikipedia数据集进行更加严格的测试。他们引入了新的成本度量来衡量表现，并系统地分析了重要的成本因素。</li>
<li>results: 尽管在Wikipedia上获得了高的重新标识率，甚至最好的LLMs在法律案例中仍然遇到了困难，这是因为缺乏测试数据集，需要巨大的训练资源，以及法律案例中数据的稀缺性。研究结论是，使用LLMs进行重新标识可能不太可能，但是未来可能变得可能。<details>
<summary>Abstract</summary>
Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions. The complexity is attributed to the lack of test datasets, the necessity for substantial training resources, and data sparsity in the information used for re-identification. In conclusion, this study demonstrates that re-identification using LLMs may not be feasible for now, but as the proof-of-concept on Wikipedia showed, it might become possible in the future. We hope that our system can help enhance the confidence in the security of anonymized decisions, thus leading to the courts being more confident to publish decisions.
</details>
<details>
<summary>摘要</summary>
“欧盟和瑞士的司法预测中的匿名性保护是一个重要的问题。随着大规模数据预测技术的发展，对匿名化后的个人重新识别的担忧增加。根据瑞士联邦最高法院的判决，我们进行了一个实验，使用瑞士联邦最高法院的法律数据来测试LLM的重新识别能力。在进一步的测试中，我们使用了一个匿名化的Wikipedia数据集，以更加严谨地检验发现。我们也引入了一个新的任务，即在文本中重新识别个人，并且引入了新的衡量表现的指标。我们系统性地分析了对成功重新识别的影响因素，发现模型大小、输入长度和调整受到最大的影响。尽管在Wikipedia上有高的重新识别率，但是even the best LLMs仅在法院的判决中取得了 moderate的成功率。这些成功率的低度是由于没有足够的测试数据、需要很大的训练资源和数据潜在的缺乏。因此，我们的研究结果表明，使用LLMs进行重新识别可能不太可能，但是在未来，这个技术可能会成为可能的。我们希望，我们的系统可以帮助提高匿名化判决的安全性，使得法院更自信地发布判决。”
</details></li>
</ul>
<hr>
<h2 id="Using-Early-Exits-for-Fast-Inference-in-Automatic-Modulation-Classification"><a href="#Using-Early-Exits-for-Fast-Inference-in-Automatic-Modulation-Classification" class="headerlink" title="Using Early Exits for Fast Inference in Automatic Modulation Classification"></a>Using Early Exits for Fast Inference in Automatic Modulation Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11100">http://arxiv.org/abs/2308.11100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elsayed Mohammed, Omar Mashaal, Hatem Abou-Zeid</li>
<li>for: 本研究旨在提高无线通信中的自动模式分类（AMC）技术的效率，通过使用深度学习（DL）技术提取无线信号特征。</li>
<li>methods: 本研究提出使用早期离开（EE）技术加速DL模型的推理，并研究了四种不同的早期离开架构和自定义多支分支训练算法。</li>
<li>results: 通过广泛的实验，我们发现对于中度到高度的信号含杂率（SNR），使用EE技术可以显著降低深度神经网络的推理速度，而不会产生分类精度的下降。我们还进行了推理时间与分类精度之间的平衡分析。这是目前所知道的首次应用EE技术于AMC领域的研究。<details>
<summary>Abstract</summary>
Automatic modulation classification (AMC) plays a critical role in wireless communications by autonomously classifying signals transmitted over the radio spectrum. Deep learning (DL) techniques are increasingly being used for AMC due to their ability to extract complex wireless signal features. However, DL models are computationally intensive and incur high inference latencies. This paper proposes the application of early exiting (EE) techniques for DL models used for AMC to accelerate inference. We present and analyze four early exiting architectures and a customized multi-branch training algorithm for this problem. Through extensive experimentation, we show that signals with moderate to high signal-to-noise ratios (SNRs) are easier to classify, do not require deep architectures, and can therefore leverage the proposed EE architectures. Our experimental results demonstrate that EE techniques can significantly reduce the inference speed of deep neural networks without sacrificing classification accuracy. We also thoroughly study the trade-off between classification accuracy and inference time when using these architectures. To the best of our knowledge, this work represents the first attempt to apply early exiting methods to AMC, providing a foundation for future research in this area.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:自动模式分类（AMC）在无线通信中扮演了关键的角色，可以自动将广播信号分类。深度学习（DL）技术在AMC中越来越受到关注，因为它们可以提取广播信号的复杂特征。然而，DL模型具有高计算复杂度和高推理延迟。这篇论文提出使用早退出（EE）技术来加速DL模型在AMC中的推理。我们提出了四种EE架构和一种自定义多支分支训练算法。经过广泛的实验，我们发现在moderate to high signal-to-noise ratio（SNR）下，信号更容易分类，不需要深度的架构，可以利用我们提出的EE架构。我们的实验结果表明，EE技术可以减少深度神经网络的推理速度，而不会增加分类精度的损失。我们还在使用这些架构时进行了严格的质量评估和时间评估。根据我们所知，这是首次将EE技术应用于AMC，这为未来的相关研究提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Video-OWL-ViT-Temporally-consistent-open-world-localization-in-video"><a href="#Video-OWL-ViT-Temporally-consistent-open-world-localization-in-video" class="headerlink" title="Video OWL-ViT: Temporally-consistent open-world localization in video"></a>Video OWL-ViT: Temporally-consistent open-world localization in video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11093">http://arxiv.org/abs/2308.11093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Heigold, Matthias Minderer, Alexey Gritsenko, Alex Bewley, Daniel Keysers, Mario Lučić, Fisher Yu, Thomas Kipf</li>
<li>for: 本研究旨在适应预训练的开放视界图像模型到视频本地化中。</li>
<li>methods: 我们基于OWL-ViT开放词汇检测模型，并添加了一个变换器解码器，以卷积神经网络输出的一帧图像作为下一帧对象查询。</li>
<li>results: 我们的模型在面对挑战性的TAO-OWBenchmark上表现出色，证明了预训练大量图像文本数据可以成功传递到开放视界本地化中。<details>
<summary>Abstract</summary>
We present an architecture and a training recipe that adapts pre-trained open-world image models to localization in videos. Understanding the open visual world (without being constrained by fixed label spaces) is crucial for many real-world vision tasks. Contrastive pre-training on large image-text datasets has recently led to significant improvements for image-level tasks. For more structured tasks involving object localization applying pre-trained models is more challenging. This is particularly true for video tasks, where task-specific data is limited. We show successful transfer of open-world models by building on the OWL-ViT open-vocabulary detection model and adapting it to video by adding a transformer decoder. The decoder propagates object representations recurrently through time by using the output tokens for one frame as the object queries for the next. Our model is end-to-end trainable on video data and enjoys improved temporal consistency compared to tracking-by-detection baselines, while retaining the open-world capabilities of the backbone detector. We evaluate our model on the challenging TAO-OW benchmark and demonstrate that open-world capabilities, learned from large-scale image-text pre-training, can be transferred successfully to open-world localization across diverse videos.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:我们提出了一种架构和训练方法，可以将预训练的开放视界图像模型适应到视频地图Localization。理解开放视界（不受固定标签空间约束）是许多实际视觉任务的关键。在大量图像文本数据集上进行对比预训练，最近导致了图像级别任务的显著改进。然而，对于结构化任务，如对象localization，使用预训练模型更加困难。特别是在视频任务中，任务特定数据受限。我们在OWL-ViT开放词汇探测模型的基础上建立了一个Transformer解码器，以便在视频中传播对象表示。解码器使用下一帧的输出符号来作为下一帧的对象查询。我们的模型是基于视频数据的端到端训练的，并且比较tracking-by-detection基eline更有优势，同时保留了预训练模型的开放视界能力。我们在TAO-OWbenchmark上评估了我们的模型，并证明了可以成功传递开放视界的能力，从大规模图像文本预训练中学习到开放视界地图Localization across多种视频。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Route-Planning-of-UAVs-Workers-and-Cars-for-Crowdsensing-in-Disaster-Response"><a href="#Collaborative-Route-Planning-of-UAVs-Workers-and-Cars-for-Crowdsensing-in-Disaster-Response" class="headerlink" title="Collaborative Route Planning of UAVs, Workers and Cars for Crowdsensing in Disaster Response"></a>Collaborative Route Planning of UAVs, Workers and Cars for Crowdsensing in Disaster Response</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11088">http://arxiv.org/abs/2308.11088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Han, Chunyu Tu, Zhiwen Yu, Zhiyong Yu, Weihua Shan, Liang Wang, Bin Guo</li>
<li>for: 本研究旨在提高灾区内部合作多代理器（UAV、工人和车辆）的数据收集效率。</li>
<li>methods: 本研究提出了一个多代理器路径观察法（MANF-RL-RP），具有多效设计，包括全球与本地信息处理、特定多代理器系统模型结构等。</li>
<li>results: 比较基准算法（Greedy-SC-RP和MANF-DNN-RP），MANF-RL-RP在任务完成率方面有显著提高。<details>
<summary>Abstract</summary>
Efficiently obtaining the up-to-date information in the disaster-stricken area is the key to successful disaster response. Unmanned aerial vehicles (UAVs), workers and cars can collaborate to accomplish sensing tasks, such as data collection, in disaster-stricken areas. In this paper, we explicitly address the route planning for a group of agents, including UAVs, workers, and cars, with the goal of maximizing the task completion rate. We propose MANF-RL-RP, a heterogeneous multi-agent route planning algorithm that incorporates several efficient designs, including global-local dual information processing and a tailored model structure for heterogeneous multi-agent systems. Global-local dual information processing encompasses the extraction and dissemination of spatial features from global information, as well as the partitioning and filtering of local information from individual agents. Regarding the construction of the model structure for heterogeneous multi-agent, we perform the following work. We design the same data structure to represent the states of different agents, prove the Markovian property of the decision-making process of agents to simplify the model structure, and also design a reasonable reward function to train the model. Finally, we conducted detailed experiments based on the rich simulation data. In comparison to the baseline algorithms, namely Greedy-SC-RP and MANF-DNN-RP, MANF-RL-RP has exhibited a significant improvement in terms of task completion rate.
</details>
<details>
<summary>摘要</summary>
efficiently 获取在灾难 struck 地区的最新信息是灾难应对的关键。无人飞行器（UAV）、工人和车辆可以在灾难 struck 地区合作完成感知任务，如数据收集。在这篇论文中，我们明确地讨论了一组代理人（包括UAV、工人和车辆）的路径规划，以最大化任务完成率为目标。我们提出了多Agent Route Planning Algorithm（MANF-RL-RP），该算法包括了许多高效的设计，如全球-本地双信息处理和特定的模型结构 для多种Agent系统。全球-本地双信息处理包括从全球信息中提取和传递空间特征，以及来自个体代理人的本地信息的分区和筛选。在构建多种Agent系统的模型结构方面，我们进行了以下工作。我们设计了同样的数据结构来表示不同代理人的状态，证明代理人决策过程的markt价性以简化模型结构，并设计了合理的奖励函数来训练模型。最后，我们对着富有的 simulate 数据进行了详细的实验。与基准算法（即Greedy-SC-RP和MANF-DNN-RP）相比，MANF-RL-RP 在任务完成率方面表现出了显著的提升。
</details></li>
</ul>
<hr>
<h2 id="Neural-Amortized-Inference-for-Nested-Multi-agent-Reasoning"><a href="#Neural-Amortized-Inference-for-Nested-Multi-agent-Reasoning" class="headerlink" title="Neural Amortized Inference for Nested Multi-agent Reasoning"></a>Neural Amortized Inference for Nested Multi-agent Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11071">http://arxiv.org/abs/2308.11071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunal Jha, Tuan Anh Le, Chuanyang Jin, Yen-Ling Kuo, Joshua B. Tenenbaum, Tianmin Shu</li>
<li>for: 本研究旨在提高多智能体交互中的复杂社会推理能力，使其能够更好地理解别人对自己的推理。</li>
<li>methods: 本研究使用神经网络来减轻高阶社会推理的计算复杂性，以提高多智能体交互的效率。</li>
<li>results: 实验结果表明，我们的方法可以减少计算复杂性，同时减少准确性的削弱。<details>
<summary>Abstract</summary>
Multi-agent interactions, such as communication, teaching, and bluffing, often rely on higher-order social inference, i.e., understanding how others infer oneself. Such intricate reasoning can be effectively modeled through nested multi-agent reasoning. Nonetheless, the computational complexity escalates exponentially with each level of reasoning, posing a significant challenge. However, humans effortlessly perform complex social inferences as part of their daily lives. To bridge the gap between human-like inference capabilities and computational limitations, we propose a novel approach: leveraging neural networks to amortize high-order social inference, thereby expediting nested multi-agent reasoning. We evaluate our method in two challenging multi-agent interaction domains. The experimental results demonstrate that our method is computationally efficient while exhibiting minimal degradation in accuracy.
</details>
<details>
<summary>摘要</summary>
多代理交互，如通信、教学和威胁，经常需要高级社会推理，即理解他们如何推理自己。这种复杂的推理可以通过嵌套多代理推理来有效模型。然而，计算复杂性随着每层推理层数的增加而呈指数增长， pose significant challenges。然而，人类在日常生活中很自然地完成复杂的社会推理。为了bridging这个 gap，我们提出了一种新的方法：利用神经网络来减轻高级社会推理，从而加快嵌套多代理推理。我们在两个复杂多代理交互领域进行了实验，结果表明我们的方法具有高效性和减少准确性下降的能力。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Distributed-Backdoor-Attack-Against-Video-Based-Action-Recognition"><a href="#Temporal-Distributed-Backdoor-Attack-Against-Video-Based-Action-Recognition" class="headerlink" title="Temporal-Distributed Backdoor Attack Against Video Based Action Recognition"></a>Temporal-Distributed Backdoor Attack Against Video Based Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11070">http://arxiv.org/abs/2308.11070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis</li>
<li>for: 本研究旨在探讨视频数据下的后门攻击（Trojan），以及现有模型对这种攻击的抵御性。</li>
<li>methods: 本研究提出了一种简单 yet 有效的后门攻击方法，通过在转换域中添加杂音来植入潜在的攻击词。这种攻击可以在视频帧中逐帧插入，并且可以在攻击后继续保持高准确率。</li>
<li>results: 经过广泛的实验，研究人员发现这种攻击方法可以在多种知名模型上达到高度可见性和鲁棒性，并且可以在不同的视频识别 benchmark 上实现攻击。此外，研究人员还发现了一种称为 “Collateral Damage” 的现象，即在攻击过程中可能会导致模型对非目标类型的数据进行误分类。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibility of video-based systems under backdoor attacks remains largely unexplored. Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are \textbf{independently} embedded within the frames, which tend to be detectable by existing defenses. In this paper, we introduce a \textit{simple} yet \textit{effective} backdoor attack against video data. Our proposed attack, adding perturbations in a transformed domain, plants an \textbf{imperceptible, temporally distributed} trigger across the video frames, and is shown to be resilient to existing defensive strategies. The effectiveness of the proposed attack is demonstrated by extensive experiments with various well-known models on two video recognition benchmarks, UCF101 and HMDB51, and a sign language recognition benchmark, Greek Sign Language (GSL) dataset. We delve into the impact of several influential factors on our proposed attack and identify an intriguing effect termed "collateral damage" through extensive studies.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在不同应用场景中取得了很大成功，如视频动作识别，然而它们却容易受到后门攻击（Trojan）。攻击者可以通过特定的触发符使得恶意修改的模型在测试实例（非目标类）中产生错误分类，而保持高精度水平。虽然对于图像数据已有广泛的研究，但视频系统对于后门攻击的抗性仍然尚未得到充分研究。现有的研究多是对图像数据进行直接扩展，例如在帧内独立地插入触发符，这些触发符可以被现有的防御策略检测。在这篇论文中，我们提出了一种简单又有效的后门攻击方法，通过在转换域中添加噪声，在视频帧中植入不可见、时间分布的触发符，并证明其具有抗性。我们通过对多种知名模型在UCf101、HMDB51和希腊手语认知 benchmark 上进行了广泛的实验，证明了我们的提案的有效性。我们还进行了详细的研究，探讨了一些影响我们提案的因素，并发现了一种感人的效果，我们称之为“副作用”。
</details></li>
</ul>
<hr>
<h2 id="Topological-Graph-Signal-Compression"><a href="#Topological-Graph-Signal-Compression" class="headerlink" title="Topological Graph Signal Compression"></a>Topological Graph Signal Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11068">http://arxiv.org/abs/2308.11068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Bernárdez, Lev Telyatnikov, Eduard Alarcón, Albert Cabellos-Aparicio, Pere Barlet-Ros, Pietro Liò</li>
<li>for: 这 paper 的目的是提出一种基于 Topological Deep Learning (TDL) 方法来压缩信号 над 图 structures。</li>
<li>methods: 这 paper 使用的方法包括对原始信号进行分 clustering，然后使用 topological-inspired message passing 获取压缩后的信号表示。</li>
<li>results: 该方法可以在两个实际 Internet Service Provider Networks 的数据集上提高标准 GNN 和 feed-forward 架构的压缩性能，从 $30%$ 到 $90%$ 的压缩率提高，表明它更好地捕捉和利用图结构中的空间和时间相关性。<details>
<summary>Abstract</summary>
Recently emerged Topological Deep Learning (TDL) methods aim to extend current Graph Neural Networks (GNN) by naturally processing higher-order interactions, going beyond the pairwise relations and local neighborhoods defined by graph representations. In this paper we propose a novel TDL-based method for compressing signals over graphs, consisting in two main steps: first, disjoint sets of higher-order structures are inferred based on the original signal --by clustering $N$ datapoints into $K\ll N$ collections; then, a topological-inspired message passing gets a compressed representation of the signal within those multi-element sets. Our results show that our framework improves both standard GNN and feed-forward architectures in compressing temporal link-based signals from two real-word Internet Service Provider Networks' datasets --from $30\%$ up to $90\%$ better reconstruction errors across all evaluation scenarios--, suggesting that it better captures and exploits spatial and temporal correlations over the whole graph-based network structure.
</details>
<details>
<summary>摘要</summary>
最近爆发的拓扑深度学习（TDL）方法希望可以补充当前图ael neural network（GNN）的限制，自然处理更高阶交互，超出现有图表示中的对角相关和本地邻里hood。在这篇论文中，我们提出了一种基于TDL的图信号压缩方法，包括两个主要步骤：首先，通过原始信号对$N$个数据点进行分 clustering，将其分成$K\ll N$个集合；然后，基于图的拓扑结构，进行多元素集合内的扩展传递，以获得压缩后的信号表示。我们的结果表明，我们的框架可以在两个实际世界互联网服务提供商网络数据集上，将标准GNN和批处理架构超越，在压缩时间链接基于网络结构中的信号方面达到$30\%$到$90\%$的更好的重建错误，表明它更好地捕捉和利用图结构中的空间和时间相关性。
</details></li>
</ul>
<hr>
<h2 id="CSM-H-R-An-Automatic-Context-Reasoning-Framework-for-Interoperable-Intelligent-Systems-and-Privacy-Protection"><a href="#CSM-H-R-An-Automatic-Context-Reasoning-Framework-for-Interoperable-Intelligent-Systems-and-Privacy-Protection" class="headerlink" title="CSM-H-R: An Automatic Context Reasoning Framework for Interoperable Intelligent Systems and Privacy Protection"></a>CSM-H-R: An Automatic Context Reasoning Framework for Interoperable Intelligent Systems and Privacy Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11066">http://arxiv.org/abs/2308.11066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songhui01/csm-h-r">https://github.com/songhui01/csm-h-r</a></li>
<li>paper_authors: Songhui Yue, Xiaoyan Hong, Randy K. Smith</li>
<li>for: 这个论文的目的是提出一个自动化高级上下文（HLC）理解框架，以便在智能系统规模上实现智能系统的自动化整合。</li>
<li>methods: 该框架使用ontology和状态在运行时和模型存储阶段进行程序式组合，以实现意义full HLC的认知，并将结果应用于不同的理解技术。</li>
<li>results: 实验表明，该框架可以自动捕捉和理解高级上下文，并将其转换为可以应用于不同的理解技术的数据表示。此外，该框架还实现了隐私保护功能，通过域嵌入和信息卷积来减少信息相关性。<details>
<summary>Abstract</summary>
Automation of High-Level Context (HLC) reasoning for intelligent systems at scale is imperative due to the unceasing accumulation of contextual data in the IoT era, the trend of the fusion of data from multi-sources, and the intrinsic complexity and dynamism of the context-based decision-making process. To mitigate this issue, we propose an automatic context reasoning framework CSM-H-R, which programmatically combines ontologies and states at runtime and the model-storage phase for attaining the ability to recognize meaningful HLC, and the resulting data representation can be applied to different reasoning techniques. Case studies are developed based on an intelligent elevator system in a smart campus setting. An implementation of the framework - a CSM Engine, and the experiments of translating the HLC reasoning into vector and matrix computing especially take care of the dynamic aspects of context and present the potentiality of using advanced mathematical and probabilistic models to achieve the next level of automation in integrating intelligent systems; meanwhile, privacy protection support is achieved by anonymization through label embedding and reducing information correlation. The code of this study is available at: https://github.com/songhui01/CSM-H-R.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）技术在智能系统中的应用在不断增长，特别是在互联网物联网（IoT）时代，数据来源的融合和上下文决策过程的内在复杂性和动态性使得高级上下文（HLC）理解成为非常重要的。为解决这一问题，我们提出了一个自动上下文理解框架CSM-H-R，该框架在运行时和模型存储阶段使用ontologies和状态进行程序性结合，以实现对有意义的HLC的识别，并且可以应用于不同的理解技术。在智能电梯系统的实际案例中，我们开发了CSM引擎，并对HLC理解进行了vector和矩阵计算的实验，特别是处理上下文的动态性，表明了使用高级数学和统计模型可以实现下一个自动化层次的智能系统集成。同时，我们实现了隐私保护支持，通过嵌入标签和减少信息相关性来实现隐身。CSM框架的代码可以在以下链接中找到：https://github.com/songhui01/CSM-H-R。
</details></li>
</ul>
<hr>
<h2 id="FedDAT-An-Approach-for-Foundation-Model-Finetuning-in-Multi-Modal-Heterogeneous-Federated-Learning"><a href="#FedDAT-An-Approach-for-Foundation-Model-Finetuning-in-Multi-Modal-Heterogeneous-Federated-Learning" class="headerlink" title="FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning"></a>FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12305">http://arxiv.org/abs/2308.12305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haokun Chen, Yao Zhang, Denis Krompass, Jindong Gu, Volker Tresp</li>
<li>for: 这则研究旨在提高基础模型在多modal学习中的表现，并且解决集中训练数据的问题。</li>
<li>methods: 本研究使用 Federated Dual-Adapter Teacher (FedDAT) 方法，具有调整客户端本地更新和实施多元知识传播 (MKD)，以解决客户端数据不具同一性的问题。</li>
<li>results: 实验结果显示，FedDAT 在多modal Vision-Language 任务上substantially 超过了现有的中央化 PEFT 方法适应 FL 的表现。<details>
<summary>Abstract</summary>
Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning framework tailored to heterogeneous multi-modal FL, called Federated Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the client local updates and applying Mutual Knowledge Distillation (MKD) for an efficient knowledge transfer. FedDAT is the first approach that enables an efficient distributed finetuning of foundation models for a variety of heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we conduct extensive experiments on four multi-modality FL benchmarks with different types of data heterogeneity, where FedDAT substantially outperforms the existing centralized PEFT methods adapted for FL.
</details>
<details>
<summary>摘要</summary>
最近，基金会模型在多模态学习中展现了显著的进步。这些模型通常需要大量数据进行微调，但收集和中央化训练数据因为不同隐私规定而变得困难。为了解决这问题，聚合学习（FL）成为了一种有前途的解决方案，允许多个客户共同训练神经网络，无需中央化本地数据。以减少客户计算负担和通信开销为目的，先前的工作已经采用了参数效率微调（PEFT）方法进行FL。然而，大多数先前的工作宁悠单一模式，忽视了客户端数据的不同性。因此，在本工作中，我们提出了适应多模式、多数据类型 federated 微调框架，称为 FedDAT。具体来说，我们的方法利用了双适应教师（DAT）来处理客户端数据的不同性，通过规则化客户端本地更新和应用知识传播（MKD）进行高效的知识传递。FedDAT 是首个能够有效地在多模态 FL 上进行基础模型的分布式微调。为证明其效果，我们在四个多模态 FL 测试准则上进行了广泛的实验，其中 FedDAT 在不同类型的数据不同性下显著超过了已有的中央化 PEFT 方法。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Discriminative-Regions-Saliency-Maps-as-Alternatives-to-CAMs-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Beyond-Discriminative-Regions-Saliency-Maps-as-Alternatives-to-CAMs-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Beyond Discriminative Regions: Saliency Maps as Alternatives to CAMs for Weakly Supervised Semantic Segmentation"></a>Beyond Discriminative Regions: Saliency Maps as Alternatives to CAMs for Weakly Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11052">http://arxiv.org/abs/2308.11052</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Maruf, Arka Daw, Amartya Dutta, Jie Bu, Anuj Karpatne</li>
<li>for: 本研究比较了抽象图和特征图两种方法在弱监督 semantic segmentation (WS3) 中的表现，并提出了一些新的评价指标来全面评估这两种方法的性能。</li>
<li>methods: 本研究使用了特征图和抽象图两种方法来生成pseudo-ground truth，并通过多个视角来比较它们的相似性和不同性。</li>
<li>results: 研究发现，使用抽象图可以更好地解决WS3中的非特征区域 (NDR) 问题，并且通过随机裁剪提高了抽象图的性能。<details>
<summary>Abstract</summary>
In recent years, several Weakly Supervised Semantic Segmentation (WS3) methods have been proposed that use class activation maps (CAMs) generated by a classifier to produce pseudo-ground truths for training segmentation models. While CAMs are good at highlighting discriminative regions (DR) of an image, they are known to disregard regions of the object that do not contribute to the classifier's prediction, termed non-discriminative regions (NDR). In contrast, attribution methods such as saliency maps provide an alternative approach for assigning a score to every pixel based on its contribution to the classification prediction. This paper provides a comprehensive comparison between saliencies and CAMs for WS3. Our study includes multiple perspectives on understanding their similarities and dissimilarities. Moreover, we provide new evaluation metrics that perform a comprehensive assessment of WS3 performance of alternative methods w.r.t. CAMs. We demonstrate the effectiveness of saliencies in addressing the limitation of CAMs through our empirical studies on benchmark datasets. Furthermore, we propose random cropping as a stochastic aggregation technique that improves the performance of saliency, making it a strong alternative to CAM for WS3.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Weakly Supervised Semantic Segmentation" (WS3) is translated as "弱指示 semantic segmentation" (WS3) in Simplified Chinese.* "Class activation map" (CAM) is translated as "类划分图" (CAM) in Simplified Chinese.* "Discriminative regions" (DR) is translated as "分化区" (DR) in Simplified Chinese.* "Non-discriminative regions" (NDR) is translated as "非分化区" (NDR) in Simplified Chinese.* "Attribution methods" such as "saliency maps" is translated as "责任方法" such as "吸引图" in Simplified Chinese.* "Stochastic aggregation technique" such as "random cropping" is translated as "随机聚合技术" such as "随机裁剪" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Personalized-Event-Prediction-for-Electronic-Health-Records"><a href="#Personalized-Event-Prediction-for-Electronic-Health-Records" class="headerlink" title="Personalized Event Prediction for Electronic Health Records"></a>Personalized Event Prediction for Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11013">http://arxiv.org/abs/2308.11013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeong Min Lee, Milos Hauskrecht</li>
<li>For: The paper aims to develop accurate predictive models of clinical event sequences to support patient care, specifically by addressing the challenge of patient-specific variability in clinical conditions.* Methods: The paper proposes and investigates multiple new event sequence prediction models and methods, including refinement of population-wide models to subpopulations, self-adaptation, and meta-level model switching.* Results: The paper analyzes and tests the performance of these models on clinical event sequences of patients in the MIMIC-III database.<details>
<summary>Abstract</summary>
Clinical event sequences consist of hundreds of clinical events that represent records of patient care in time. Developing accurate predictive models of such sequences is of a great importance for supporting a variety of models for interpreting/classifying the current patient condition, or predicting adverse clinical events and outcomes, all aimed to improve patient care. One important challenge of learning predictive models of clinical sequences is their patient-specific variability. Based on underlying clinical conditions, each patient's sequence may consist of different sets of clinical events (observations, lab results, medications, procedures). Hence, simple population-wide models learned from event sequences for many different patients may not accurately predict patient-specific dynamics of event sequences and their differences. To address the problem, we propose and investigate multiple new event sequence prediction models and methods that let us better adjust the prediction for individual patients and their specific conditions. The methods developed in this work pursue refinement of population-wide models to subpopulations, self-adaptation, and a meta-level model switching that is able to adaptively select the model with the best chance to support the immediate prediction. We analyze and test the performance of these models on clinical event sequences of patients in MIMIC-III database.
</details>
<details>
<summary>摘要</summary>
临床事件序列包括数百个临床事件记录，表示患者 receiving 的记录时间。 开发准确预测模型临床序列非常重要，以支持多种模型，用于解释/分类当前患者状况，预测不良临床事件和结果，以提高患者治疗。 一个重要的预测临床序列模型挑战是每个患者的病人特有性。 基于下面的临床状况，每个患者的序列可能包含不同的临床事件（观察结果、实验室测试、药物、手术）。 因此，从事件序列中学习的人口广泛模型可能无法准确预测每个患者的特定动态和差异。 为解决问题，我们提出和探索多种新的事件序列预测模型和方法，使我们能更好地适应患者和其特定状况。 我们在MIMIC-III数据库中分析和测试这些模型的性能。
</details></li>
</ul>
<hr>
<h2 id="“Guinea-Pig-Trials”-Utilizing-GPT-A-Novel-Smart-Agent-Based-Modeling-Approach-for-Studying-Firm-Competition-and-Collusion"><a href="#“Guinea-Pig-Trials”-Utilizing-GPT-A-Novel-Smart-Agent-Based-Modeling-Approach-for-Studying-Firm-Competition-and-Collusion" class="headerlink" title="“Guinea Pig Trials” Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion"></a>“Guinea Pig Trials” Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10974">http://arxiv.org/abs/2308.10974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Han, Zengqing Wu, Chuan Xiao</li>
<li>For: The paper is written to study firm competition and collusion using a novel framework called Smart Agent-Based Modeling (SABM), which employs GPT-4 technologies to represent firms and their interactions.* Methods: The study uses a controlled experiment with smart agents to examine firm price competition and collusion behaviors under various conditions, comparing the results to those obtained through experiments with human subjects.* Results: The paper finds that smart agents consistently reach tacit collusion in the absence of communication, leading to prices converging at levels higher than the Bertrand equilibrium price but lower than monopoly or cartel prices. With communication allowed, smart agents achieve a higher-level collusion with prices close to cartel prices, and collusion forms more quickly with communication. These results highlight the importance of communication in enhancing trust between firms and facilitating collusion.<details>
<summary>Abstract</summary>
Firm competition and collusion involve complex dynamics, particularly when considering communication among firms. Such issues can be modeled as problems of complex systems, traditionally approached through experiments involving human subjects or agent-based modeling methods. We propose an innovative framework called Smart Agent-Based Modeling (SABM), wherein smart agents, supported by GPT-4 technologies, represent firms, and interact with one another. We conducted a controlled experiment to study firm price competition and collusion behaviors under various conditions. SABM is more cost-effective and flexible compared to conducting experiments with human subjects. Smart agents possess an extensive knowledge base for decision-making and exhibit human-like strategic abilities, surpassing traditional ABM agents. Furthermore, smart agents can simulate human conversation and be personalized, making them ideal for studying complex situations involving communication. Our results demonstrate that, in the absence of communication, smart agents consistently reach tacit collusion, leading to prices converging at levels higher than the Bertrand equilibrium price but lower than monopoly or cartel prices. When communication is allowed, smart agents achieve a higher-level collusion with prices close to cartel prices. Collusion forms more quickly with communication, while price convergence is smoother without it. These results indicate that communication enhances trust between firms, encouraging frequent small price deviations to explore opportunities for a higher-level win-win situation and reducing the likelihood of triggering a price war. We also assigned different personas to firms to analyze behavioral differences and tested variant models under diverse market structures. The findings showcase the effectiveness and robustness of SABM and provide intriguing insights into competition and collusion.
</details>
<details>
<summary>摘要</summary>
企业竞争和勾结涉及到复杂的动态，特别是在公司之间的交流方面。这些问题可以通过人类实验或智能代理模型（ABM）来模拟。我们提出了一种创新的框架called Smart Agent-Based Modeling（SABM），其中智能代理，受到GPT-4技术支持，代表公司，并互动相互。我们进行了一项控制性实验，以研究企业价格竞争和勾结行为的不同情况。SABM相比人类实验更加经济和灵活。智能代理具有广泛的知识库和人类策略能力，超过传统ABM代理。此外，智能代理可以模拟人类对话，可个性化，使其适用于研究复杂的交流情况。我们的结果表明，在无交流情况下，智能代理一般会达成tacit collusion，导致价格相对于BERTRAND平衡价格高，但比单一垄断或垄断价格低。当交流被允许时，智能代理可以实现更高级别的勾结，价格接近垄断价格。勾结形成更快，无交流情况下价格均衡更平滑。这些结果表明，交流可以增强公司之间的信任，使小价格偏移更频繁地探索机会，降低价格战的可能性。我们还将不同的公司个性分配给不同的公司，以分析行为差异，并在多种市场结构下测试不同的模型。结果显示SABM的效果和稳定性，并提供了精彩的竞争和勾结的新思路。
</details></li>
</ul>
<hr>
<h2 id="DocPrompt-Large-scale-continue-pretrain-for-zero-shot-and-few-shot-document-question-answering"><a href="#DocPrompt-Large-scale-continue-pretrain-for-zero-shot-and-few-shot-document-question-answering" class="headerlink" title="DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering"></a>DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10959">http://arxiv.org/abs/2308.10959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijin Wu, Dan Zhang, Teng Hu, Shikun Feng</li>
<li>for: 文章旨在提出一种名为 Docprompt 的文档问答模型，可以在文档问答任务中实现强大的零学习和几学习性能。</li>
<li>methods: 文章提出了一种新的弱监督数据生成方法、一种多Stage训练方法和一种理解模型&amp;生成模型集成方法。</li>
<li>results: 实验结果显示， после继续预训练， Docprompt 模型在文档问答任务上明显超过了现有的强基线模型，并且可以大幅提高文档问答客户项目的交付效率和模型性能，降低注释成本和劳动成本。<details>
<summary>Abstract</summary>
In this paper, we propose Docprompt for document question answering tasks with powerful zero-shot and few-shot performance. We proposed a novel weakly supervised data generation method, a novel multl-stage training method and a novel understanding model & generation model ensemble method. Experiment results show that the Docprompt model after continue pretrain significantly outperforms the existing strong baseline models on document question answering tasks. This method greatly improves the delivery efficiency and model performance of document question answering customer projects, reducing annotation costs and labor costs. Our demo can be found at https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了 Docprompt，用于文档问答任务的强大零shot和几shot性能的解决方案。我们提出了一种新的软参数生成方法、一种多Stage训练方法和一种新的理解模型&生成模型结合方法。实验结果显示，在继续预训练后，Docprompt模型在文档问答任务上明显超越了现有的强基线模型。这种方法可以大幅提高文档问答客户项目的交付效率和模型性能，降低注释成本和劳动成本。您可以在https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout找到我们的demo。
</details></li>
</ul>
<hr>
<h2 id="Structured-World-Models-from-Human-Videos"><a href="#Structured-World-Models-from-Human-Videos" class="headerlink" title="Structured World Models from Human Videos"></a>Structured World Models from Human Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10901">http://arxiv.org/abs/2308.10901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Russell Mendonca, Shikhar Bahl, Deepak Pathak</li>
<li>For: The paper aims to enable robots to learn complex manipulation skills directly in the real world using a small amount of interaction data.* Methods: The approach uses human video data to build a structured, human-centric action space grounded in visual affordances, and trains a world model on human videos before fine-tuning on a small amount of robot interaction data without task supervision.* Results: The approach allows robots to learn various manipulation skills in complex settings in under 30 minutes of interaction.Here is the same information in Simplified Chinese:* For: 论文旨在帮助机器人直接在真实世界中学习复杂的抓取技能，只需要很少的互动数据。* Methods: 方法使用人类视频数据构建一个基于视觉可用性的结构化人类行为空间，然后在人类视频上训练世界模型，并在小量机器人互动数据上练习而不需要任务指导。* Results: 方法可以让机器人在复杂的设置下快速学习多种抓取技能，仅需要30分钟的互动。<details>
<summary>Abstract</summary>
We tackle the problem of learning complex, general behaviors directly in the real world. We propose an approach for robots to efficiently learn manipulation skills using only a handful of real-world interaction trajectories from many different settings. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, our belief is that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. Our approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. Further, we train a world model on human videos and fine-tune on a small amount of robot interaction data without any task supervision. We show that this approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction. Videos can be found at https://human-world-model.github.io
</details>
<details>
<summary>摘要</summary>
我们面临的问题是直接在实际世界中学习复杂的通用行为。我们提议一种方法，使用只需一些不同场景的实际互动轨迹来教育机器人快速学习抓取技能。从计算机视觉和自然语言学习领域的成功经验中，我们认为，为了高效地学习，机器人必须能够利用互联网规模的人类视频数据。人类在与世界交互中有很多有趣的方式，这些方式可以帮助机器人不仅构建有用的动作和可用性的理解，还可以了解这些动作如何影响世界进行抓取。我们的方法是建立基于视觉可用性学习的人类行为空间，并在这个空间中训练一个世界模型。我们在人类视频上进行了训练，并在少量机器人互动数据上进行了精度调整。我们显示，这种可用性空间世界模型的方法可以让不同的机器人在复杂的设置下快速学习多种抓取技能，仅用30分钟的互动。视频可以在https://human-world-model.github.io找到。
</details></li>
</ul>
<hr>
<h2 id="TADA-Text-to-Animatable-Digital-Avatars"><a href="#TADA-Text-to-Animatable-Digital-Avatars" class="headerlink" title="TADA! Text to Animatable Digital Avatars"></a>TADA! Text to Animatable Digital Avatars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10899">http://arxiv.org/abs/2308.10899</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TingtingLiao/TADA">https://github.com/TingtingLiao/TADA</a></li>
<li>paper_authors: Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang, Yangyi Huang, Justus Thies, Michael J. Black</li>
<li>For: The paper aims to generate high-quality 3D avatars from textual descriptions, with realistic animations and detailed geometry.* Methods: The approach uses a 2D diffusion model and an animatable parametric body model, along with hierarchical rendering and score distillation sampling (SDS) to create detailed 3D avatars from text.* Results: The paper demonstrates that TADA significantly surpasses existing approaches on both qualitative and quantitative measures, enabling the creation of large-scale digital character assets that are ready for animation and rendering, and are easily editable through natural language.<details>
<summary>Abstract</summary>
We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines. Existing text-based character generation methods are limited in terms of geometry and texture quality, and cannot be realistically animated due to inconsistent alignment between the geometry and the texture, particularly in the face region. To overcome these limitations, TADA leverages the synergy of a 2D diffusion model and an animatable parametric body model. Specifically, we derive an optimizable high-resolution body model from SMPL-X with 3D displacements and a texture map, and use hierarchical rendering with score distillation sampling (SDS) to create high-quality, detailed, holistic 3D avatars from text. To ensure alignment between the geometry and texture, we render normals and RGB images of the generated character and exploit their latent embeddings in the SDS training process. We further introduce various expression parameters to deform the generated character during training, ensuring that the semantics of our generated character remain consistent with the original SMPL-X model, resulting in an animatable character. Comprehensive evaluations demonstrate that TADA significantly surpasses existing approaches on both qualitative and quantitative measures. TADA enables creation of large-scale digital character assets that are ready for animation and rendering, while also being easily editable through natural language. The code will be public for research purposes.
</details>
<details>
<summary>摘要</summary>
我们介绍TADA，一个简单又有效的方法，将文本描述转换为高品质的3D人物模型，包括高级的几何和生命力的纹理，可以通过传统的グラフィックス管线进行动画和渲染。现有的文本基于的人物生成方法受到几何和纹理质量的限制，并且无法真实地动画，因为几何和纹理之间的对齐不稳定，尤其是在脸部区域。为了突破这些限制，TADA利用了2D传播模型和可动的 Parametric Body Model。具体来说，我们从SMPL-X中 derivated一个可优化的高分辨率人体模型，包括3D偏移和纹理图像，并使用层次渲染和分析抽象 Sampling (SDS) 创建高品质、细节满怀的3D人物。为了保证几何和纹理之间的对齐，我们在SDS训练过程中使用 render 的 норма和 RGB 图像，并利用它们的隐藏嵌入来稳定训练。此外，我们还引入了多种表情参数，以使得生成的人物在训练过程中具有表情，以保持与原始 SMPL-X 模型的 semantics 一致，使得生成的人物可以被动画。我们的评估结果显示，TADA 在 both 质量和量化度上有所提高，与现有的方法相比。TADA 可以实现大规模的数码人物资产的创建，并且可以通过自然语言进行易于修改。我们将代码公开供研究用途。
</details></li>
</ul>
<hr>
<h2 id="Giraffe-Adventures-in-Expanding-Context-Lengths-in-LLMs"><a href="#Giraffe-Adventures-in-Expanding-Context-Lengths-in-LLMs" class="headerlink" title="Giraffe: Adventures in Expanding Context Lengths in LLMs"></a>Giraffe: Adventures in Expanding Context Lengths in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10882">http://arxiv.org/abs/2308.10882</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abacusai/long-context">https://github.com/abacusai/long-context</a></li>
<li>paper_authors: Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, Siddartha Naidu</li>
<li>for: 这个论文主要用于探讨现代大型自然语言处理器（LLMs）如何在评估时处理长输入序列。</li>
<li>methods: 该论文使用现有的context length extrapolation方法，包括修改 pozitional encoding 系统以指示输入序列中token或活动的位置。并 introduce some new design,如修改基于position encoding的减少策略。</li>
<li>results: 该论文通过三个新的评估任务（FreeFormQA、AlteredNumericQA和LongChat-Lines）以及折减指标来测试这些方法。发现线性扩展是最佳的扩展方法，并示出可以通过使用更长的扩展级别在评估时获得更好的性能。同时，发现修改基于position encoding的减少策略也有扩展能力。基于这些结果，该论文释放了三个新的13B参数长Context模型，即4k和16k context模型从基础LLaMA-13B中训练，以及32k context模型从基础LLaMA2-13B中训练。同时还释放了 reproduce 结果的代码。<details>
<summary>Abstract</summary>
Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time. To use these models on sequences longer than the train-time context length, one might employ techniques from the growing family of context length extrapolation methods -- most of which focus on modifying the system of positional encodings used in the attention mechanism to indicate where tokens or activations are located in the input sequence. We conduct a wide survey of existing methods of context length extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own design as well -- in particular, a new truncation strategy for modifying the basis for the position encoding.   We test these methods using three new evaluation tasks (FreeFormQA, AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to be less fine-grained as a measure of long context performance of LLMs. We release the three tasks publicly as datasets on HuggingFace. We discover that linear scaling is the best method for extending context length, and show that further gains can be achieved by using longer scales at evaluation time. We also discover promising extrapolation capabilities in the truncated basis. To support further research in this area, we release three new 13B parameter long-context models which we call Giraffe: 4k and 16k context models trained from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We also release the code to replicate our results.
</details>
<details>
<summary>摘要</summary>
现代大语言模型（LLM）通常通过注意机制训练，但是它们的评估时间上下文长度是固定的，这限制了它们可以处理的输入序列长度。为了使这些模型处理 longer than train-time context length 的序列，可以使用Context length extrapolation方法。我们对现有的方法进行了广泛的survey，并介绍了一些我们自己的设计，包括一种新的截断策略 для修改基于位置编码的系统。我们使用三个新的评估任务（FreeFormQA、AlteredNumericQA和LongChat-Lines）以及折叠指标来测试这些方法。我们发现线性扩展是最佳的扩展方法，并且可以通过使用更长的扩展级别来进一步提高性能。此外，我们发现 truncated basis 具有扩展的潜在能力。为支持进一步的研究，我们释放了三个13B参数的长 context模型，即4k和16k上下文模型从基础 LLMA-13B 开始，以及32k上下文模型从基础 LLMA2-13B 开始。我们还释放了复制我们结果的代码。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Transformer-Dynamics-as-Movement-through-Embedding-Space"><a href="#Analyzing-Transformer-Dynamics-as-Movement-through-Embedding-Space" class="headerlink" title="Analyzing Transformer Dynamics as Movement through Embedding Space"></a>Analyzing Transformer Dynamics as Movement through Embedding Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10874">http://arxiv.org/abs/2308.10874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumeet S. Singh</li>
<li>for: This paper explores the underlying mechanics of Transformer language models and how they give rise to intelligent behaviors.</li>
<li>methods: The authors use a systems approach to analyze Transformers and develop a mathematical framework that views the models as movement through embedding space.</li>
<li>results: The paper reveals important insights into the emergence of intelligence in Transformers, including the idea that the models are essentially “Embedding Space walkers” that compose context into a single vector, and that attention plays a key role in associating vectors and influencing the organization of the embedding space. Additionally, the authors find some evidence for their semantic space theory, which posits that embedding vectors represent semantic concepts.<details>
<summary>Abstract</summary>
Transformer language models exhibit intelligent behaviors such as understanding natural language, recognizing patterns, acquiring knowledge, reasoning, planning, reflecting and using tools. This paper explores how their underlying mechanics give rise to intelligent behaviors. We adopt a systems approach to analyze Transformers in detail and develop a mathematical framework that frames their dynamics as movement through embedding space. This novel perspective provides a principled way of thinking about the problem and reveals important insights related to the emergence of intelligence:   1. At its core the Transformer is a Embedding Space walker, mapping intelligent behavior to trajectories in this vector space.   2. At each step of the walk, it composes context into a single composite vector whose location in Embedding Space defines the next step.   3. No learning actually occurs during decoding; in-context learning and generalization are simply the result of different contexts composing into different vectors.   4. Ultimately the knowledge, intelligence and skills exhibited by the model are embodied in the organization of vectors in Embedding Space rather than in specific neurons or layers. These abilities are properties of this organization.   5. Attention's contribution boils down to the association-bias it lends to vector composition and which influences the aforementioned organization. However, more investigation is needed to ascertain its significance.   6. The entire model is composed from two principal operations: data independent filtering and data dependent aggregation. This generalization unifies Transformers with other sequence models and across modalities.   Building upon this foundation we formalize and test a semantic space theory which posits that embedding vectors represent semantic concepts and find some evidence of its validity.
</details>
<details>
<summary>摘要</summary>
吸收器语言模型展示出智能行为，如理解自然语言、识别模式、获得知识、reasoning、规划、反思和使用工具。这篇论文探讨它们的基本机制如何产生智能行为。我们采用系统方法分析吸收器，并开发了一个数学框架来描述它们的动态。这种新的视角提供了一个原则性的方法来思考问题，并揭示了智能行为的出现的重要关键点：1. 吸收器的核心是Embedding Space漫步者，将智能行为映射到vector空间中的路径上。2. 在每一步中，吸收器将上下文融合成一个单一的复合向量，该向量在Embedding Space中的位置定义下一步的路径。3. 在解码过程中，没有实际学习发生，而是在不同上下文中的融合导致了不同的向量组合，从而实现了吸收器的智能行为。4. 吸收器的智能、智慧和技能都是Embedding Space中向量的组织方式所具有的，而不是特定的神经元或层。这些能力是这种组织的属性。5. 关注的贡献在向量组合中带来了关联偏好，影响了Embedding Space中向量的组织，但需要进一步的调查以确定其重要性。6. 整个模型由两种主要操作组成：数据独立的滤波和数据依赖的聚合。这种一致性将吸收器与其他序列模型和多种模式相连接。基于这个基础，我们正式提出了一种 semantics空间理论，即向量表示 semantic concepts，并发现了一些证据支持这一理论的有效性。
</details></li>
</ul>
<hr>
<h2 id="Real-World-Time-Series-Benchmark-Datasets-with-Distribution-Shifts-Global-Crude-Oil-Price-and-Volatility"><a href="#Real-World-Time-Series-Benchmark-Datasets-with-Distribution-Shifts-Global-Crude-Oil-Price-and-Volatility" class="headerlink" title="Real World Time Series Benchmark Datasets with Distribution Shifts: Global Crude Oil Price and Volatility"></a>Real World Time Series Benchmark Datasets with Distribution Shifts: Global Crude Oil Price and Volatility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10846">http://arxiv.org/abs/2308.10846</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oilpricebenchmarks/COB">https://github.com/oilpricebenchmarks/COB</a></li>
<li>paper_authors: Pranay Pasula</li>
<li>for: 本研究的目的是提供task-labeled时间序列数据集，用于驱动 kontinual learning在金融领域的进步。</li>
<li>methods: 本研究使用了资产价格数据的变换，生成了volatility proxy，并使用了期望最大化（EM）算法来适应模型。</li>
<li>results: 研究发现，通过包含任务标签，四种 kontinual learning算法在多个预测时间 horizon 上表现出了 Universal 的改进。<details>
<summary>Abstract</summary>
The scarcity of task-labeled time-series benchmarks in the financial domain hinders progress in continual learning. Addressing this deficit would foster innovation in this area. Therefore, we present COB, Crude Oil Benchmark datasets. COB includes 30 years of asset prices that exhibit significant distribution shifts and optimally generates corresponding task (i.e., regime) labels based on these distribution shifts for the three most important crude oils in the world. Our contributions include creating real-world benchmark datasets by transforming asset price data into volatility proxies, fitting models using expectation-maximization (EM), generating contextual task labels that align with real-world events, and providing these labels as well as the general algorithm to the public. We show that the inclusion of these task labels universally improves performance on four continual learning algorithms, some state-of-the-art, over multiple forecasting horizons. We hope these benchmarks accelerate research in handling distribution shifts in real-world data, especially due to the global importance of the assets considered. We've made the (1) raw price data, (2) task labels generated by our approach, (3) and code for our algorithm available at https://oilpricebenchmarks.github.io.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>金融领域内存续ous task-标注时间序列 benchmark 的缺乏，阻碍了持续学习的进步。为了解决这一问题，我们提出了 COB，即 Crude Oil Benchmark 数据集。 COB 包含了30年的资产价格，其中 exhibit 显著的分布shift，并且根据这些分布shift 生成对应的任务（即 режи）标签。我们的贡献包括将资产价格数据转换为Volatility proxy，使用期望最大化（EM）方法进行适应，生成基于实际世界事件的contextual task标签，并将这些标签以及通用的算法公开发布。我们表明，包括这些任务标签在内的 continual learning 算法在多个预测时间 horizon 上 universally 提高了四种状态之际的表现。我们希望这些 benchmark 可以加速实际数据中的分布shift处理研究，特别是由于我们考虑的资产的全球重要性。我们在 <https://oilpricebenchmarks.github.io> 上提供了（1）原始价格数据，（2）由我们方法生成的任务标签，（3）以及代码。
</details></li>
</ul>
<hr>
<h2 id="Neural-Networks-Optimizations-Against-Concept-and-Data-Drift-in-Malware-Detection"><a href="#Neural-Networks-Optimizations-Against-Concept-and-Data-Drift-in-Malware-Detection" class="headerlink" title="Neural Networks Optimizations Against Concept and Data Drift in Malware Detection"></a>Neural Networks Optimizations Against Concept and Data Drift in Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10821">http://arxiv.org/abs/2308.10821</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Maillet, Benjamin Marais</li>
<li>for: 提高基eline neural network对概念飘移问题的处理能力</li>
<li>methods: Feature reduction和使用最新验证集训练，并提出了Drift-Resilient Binary Cross-Entropy损失函数</li>
<li>results: 对2020-2023年间 collected的新型恶意文件进行评估，提高了15.2%的恶意文件检测率 compared to baseline model<details>
<summary>Abstract</summary>
Despite the promising results of machine learning models in malware detection, they face the problem of concept drift due to malware constant evolution. This leads to a decline in performance over time, as the data distribution of the new files differs from the training one, requiring regular model update. In this work, we propose a model-agnostic protocol to improve a baseline neural network to handle with the drift problem. We show the importance of feature reduction and training with the most recent validation set possible, and propose a loss function named Drift-Resilient Binary Cross-Entropy, an improvement to the classical Binary Cross-Entropy more effective against drift. We train our model on the EMBER dataset (2018) and evaluate it on a dataset of recent malicious files, collected between 2020 and 2023. Our improved model shows promising results, detecting 15.2% more malware than a baseline model.
</details>
<details>
<summary>摘要</summary>
尽管机器学习模型在针对恶意软件检测方面表现出色，但它们面临着概念漂移问题，这是因为恶意软件不断演化，导致模型在时间上的性能下降。为了解决这个问题，我们提出了一种模型无关协议，用于改进基eline神经网络，以适应漂移问题。我们表明了减少特征和使用最新的验证集训练的重要性，并提出了一种名为“漂移抗性二进制十字积分”的损失函数，比 класси的二进制十字积分更有效地防止漂移。我们在EMBER数据集（2018）上训练了我们的模型，并在2020-2023年间收集的一个数据集上进行了评估。我们改进后的模型显示了出色的表现，能够检测到2018年训练集中的15.2%更多的恶意软件。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/22/cs.AI_2023_08_22/" data-id="clltaagmk0014r888aigogyiw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
