
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/2/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.AI_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T12:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/31/cs.AI_2023_10_31/">cs.AI - 2023-10-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Large-Scale-Multi-Robot-Assembly-Planning-for-Autonomous-Manufacturing"><a href="#Large-Scale-Multi-Robot-Assembly-Planning-for-Autonomous-Manufacturing" class="headerlink" title="Large-Scale Multi-Robot Assembly Planning for Autonomous Manufacturing"></a>Large-Scale Multi-Robot Assembly Planning for Autonomous Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00192">http://arxiv.org/abs/2311.00192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sisl/constructionbots.jl">https://github.com/sisl/constructionbots.jl</a></li>
<li>paper_authors: Kyle Brown, Dylan M. Asmar, Mac Schwager, Mykel J. Kochenderfer</li>
<li>For: This paper proposes an algorithmic stack for large-scale multi-robot assembly planning to address challenges such as collision-free movement, effective task allocation, and spatial planning for parallel assembly and transportation of nested subassemblies in manufacturing processes.* Methods: The proposed algorithmic stack includes an iterative radial layout optimization procedure, a graph-repair mixed-integer program formulation, a modified greedy task allocation algorithm, a geometric heuristic, and a hill-climbing algorithm to plan collaborative carrying configurations of robot sub-teams.* Results: The paper presents empirical results demonstrating the scalability and effectiveness of the proposed approach by generating plans to manufacture a LEGO model of a Saturn V launch vehicle with 1845 parts, 306 subassemblies, and 250 robots in under three minutes on a standard laptop computer.<details>
<summary>Abstract</summary>
Mobile autonomous robots have the potential to revolutionize manufacturing processes. However, employing large robot fleets in manufacturing requires addressing challenges including collision-free movement in a shared workspace, effective multi-robot collaboration to manipulate and transport large payloads, complex task allocation due to coupled manufacturing processes, and spatial planning for parallel assembly and transportation of nested subassemblies. We propose a full algorithmic stack for large-scale multi-robot assembly planning that addresses these challenges and can synthesize construction plans for complex assemblies with thousands of parts in a matter of minutes. Our approach takes in a CAD-like product specification and automatically plans a full-stack assembly procedure for a group of robots to manufacture the product. We propose an algorithmic stack that comprises: (i) an iterative radial layout optimization procedure to define a global staging layout for the manufacturing facility, (ii) a graph-repair mixed-integer program formulation and a modified greedy task allocation algorithm to optimally allocate robots and robot sub-teams to assembly and transport tasks, (iii) a geometric heuristic and a hill-climbing algorithm to plan collaborative carrying configurations of robot sub-teams, and (iv) a distributed control policy that enables robots to execute the assembly motion plan collision-free. We also present an open-source multi-robot manufacturing simulator implemented in Julia as a resource to the research community, to test our algorithms and to facilitate multi-robot manufacturing research more broadly. Our empirical results demonstrate the scalability and effectiveness of our approach by generating plans to manufacture a LEGO model of a Saturn V launch vehicle with 1845 parts, 306 subassemblies, and 250 robots in under three minutes on a standard laptop computer.
</details>
<details>
<summary>摘要</summary>
Mobile autonomous robots have the potential to revolutionize manufacturing processes. However, employing large robot fleets in manufacturing requires addressing challenges such as collision-free movement in a shared workspace, effective multi-robot collaboration to manipulate and transport large payloads, complex task allocation due to coupled manufacturing processes, and spatial planning for parallel assembly and transportation of nested subassemblies. We propose a full algorithmic stack for large-scale multi-robot assembly planning that addresses these challenges and can synthesize construction plans for complex assemblies with thousands of parts in a matter of minutes. Our approach takes in a CAD-like product specification and automatically plans a full-stack assembly procedure for a group of robots to manufacture the product. We propose an algorithmic stack that comprises:1. 迭代径向布局优化算法来定义制造设施的全球排序布局2. 图解 mixed-integer 编程和修改的排队策略来优化机器人和机器人子队伍的分配3. 几何规则和攀登算法来规划协作携带配置4. 分布式控制策略来使机器人执行 Assembly 动作计划无碰撞我们还提供了一个基于 Julia 的多机器人制造模拟器，作为研究社区的资源，以测试我们的算法和推动多机器人制造研究。我们的实验结果表明，我们的方法可以在标准笔记计算机上下载三分钟内生成一个 LEGO 模型的 Saturn V 发射 vehicle 的制造计划，包含1845件部件、306个子组件和250个机器人。
</details></li>
</ul>
<hr>
<h2 id="XAI-CLASS-Explanation-Enhanced-Text-Classification-with-Extremely-Weak-Supervision"><a href="#XAI-CLASS-Explanation-Enhanced-Text-Classification-with-Extremely-Weak-Supervision" class="headerlink" title="XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision"></a>XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00189">http://arxiv.org/abs/2311.00189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Hajialigol, Hanwen Liu, Xuan Wang<br>for:* 文章目的是提出一种新的、强度很弱的文本分类方法，以减少人工标注的需求。methods:* 该方法使用了弱相似性数据生成器，通过对文档与特定类别进行对应（如关键词匹配）进行 pseudo-标注。* 该方法还包括一个 auxiliary 任务，即用于预测单词重要性的词重要性预测任务。results:* 对于几个弱相似性文本分类数据集，XAI-CLASS 比其他弱相似性文本分类方法表现出色，得到了显著的性能提升。* 实验还表明，XAI-CLASS 可以提高模型的性能和可解释性。<details>
<summary>Abstract</summary>
Text classification aims to effectively categorize documents into pre-defined categories. Traditional methods for text classification often rely on large amounts of manually annotated training data, making the process time-consuming and labor-intensive. To address this issue, recent studies have focused on weakly-supervised and extremely weakly-supervised settings, which require minimal or no human annotation, respectively. In previous methods of weakly supervised text classification, pseudo-training data is generated by assigning pseudo-labels to documents based on their alignment (e.g., keyword matching) with specific classes. However, these methods ignore the importance of incorporating the explanations of the generated pseudo-labels, or saliency of individual words, as additional guidance during the text classification training process. To address this limitation, we propose XAI-CLASS, a novel explanation-enhanced extremely weakly-supervised text classification method that incorporates word saliency prediction as an auxiliary task. XAI-CLASS begins by employing a multi-round question-answering process to generate pseudo-training data that promotes the mutual enhancement of class labels and corresponding explanation word generation. This pseudo-training data is then used to train a multi-task framework that simultaneously learns both text classification and word saliency prediction. Extensive experiments on several weakly-supervised text classification datasets show that XAI-CLASS outperforms other weakly-supervised text classification methods significantly. Moreover, experiments demonstrate that XAI-CLASS enhances both model performance and explainability.
</details>
<details>
<summary>摘要</summary>
In previous weakly supervised text classification methods, pseudo-training data is generated by assigning pseudo-labels to documents based on their alignment (e.g., keyword matching) with specific classes. However, these methods ignore the importance of incorporating the explanations of the generated pseudo-labels, or saliency of individual words, as additional guidance during the text classification training process.To address this limitation, we propose XAI-CLASS, a novel explanation-enhanced extremely weakly-supervised text classification method that incorporates word saliency prediction as an auxiliary task. XAI-CLASS begins by employing a multi-round question-answering process to generate pseudo-training data that promotes the mutual enhancement of class labels and corresponding explanation word generation. This pseudo-training data is then used to train a multi-task framework that simultaneously learns both text classification and word saliency prediction.Extensive experiments on several weakly-supervised text classification datasets show that XAI-CLASS outperforms other weakly-supervised text classification methods significantly. Moreover, experiments demonstrate that XAI-CLASS enhances both model performance and explainability.
</details></li>
</ul>
<hr>
<h2 id="Robust-Safety-Classifier-for-Large-Language-Models-Adversarial-Prompt-Shield"><a href="#Robust-Safety-Classifier-for-Large-Language-Models-Adversarial-Prompt-Shield" class="headerlink" title="Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield"></a>Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00172">http://arxiv.org/abs/2311.00172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinhwa Kim, Ali Derakhshan, Ian G. Harris</li>
<li>for: 防止大语言模型受到攻击的安全性问题</li>
<li>methods: 提出了一种名为 Adversarial Prompt Shield (APS) 的轻量级模型，以及一种自动生成 adversarial 训练数据集（BAND）的策略</li>
<li>results: 通过对 Large Language Models 进行评估，显示了减少 adversarial 攻击成功率达 60% 的潜在提升，这对下一代更可靠和抗击的对话代理系统产生了前景。<details>
<summary>Abstract</summary>
Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.
</details>
<details>
<summary>摘要</summary>
大型语言模型的安全问题仍然是一个关键问题，因为它们容易受到反对攻击，这可能会让这些系统生成危险或不当的回应。在这些系统的核心里面有一个安全分类器，这是一个用于识别和mitigate potentially harmful, offensive, or unethical outputs的计算模型。然而，当前的安全分类器，尽管具有潜在的优势，frequently fail when exposed to inputs infused with adversarial noise。在回应于这个问题，我们的研究提出了反 adversarial prompt shield（APS），一个轻量级的模型，具有高度的检测精度和对反对攻击的抗性。此外，我们还提出了一些自动生成反对攻击数据集的新策略，称为Bot Adversarial Noisy Dialogue（BAND）数据集。这些数据集是用于强化安全分类器的Robustness，我们进行了对Large Language Models的评估，显示我们的分类器可以降低由反对攻击引起的攻击成功率 by up to 60%。这一进步开 up the way for the next generation of more reliable and resilient conversational agents。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Denouncing-Hate-Strategies-for-Countering-Implied-Biases-and-Stereotypes-in-Language"><a href="#Beyond-Denouncing-Hate-Strategies-for-Countering-Implied-Biases-and-Stereotypes-in-Language" class="headerlink" title="Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language"></a>Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00161">http://arxiv.org/abs/2311.00161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jimin Mun, Emily Allaway, Akhila Yerukola, Laura Vianna, Sarah-Jane Leslie, Maarten Sap</li>
<li>for: This paper aims to address the issue of online hate speech without censorship by exploring psychologically inspired strategies to challenge the underlying stereotypical implications of hateful language.</li>
<li>methods: The authors draw from psychology and philosophy literature to craft six strategies to challenge hateful language, and they examine the convincingness of each strategy through a user study and compare their usages in human- and machine-generated counterspeech datasets.</li>
<li>results: The study finds that human-written counterspeech uses more specific strategies to challenge the implied stereotype, whereas machine-generated counterspeech uses less specific strategies and often employs strategies that humans deem less convincing. The findings highlight the importance of accounting for the underlying stereotypical implications of speech when generating counterspeech and for better machine reasoning about anti-stereotypical examples.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是解决在互联网上发布仇恨言语的问题，不包括审查。</li>
<li>methods: 作者们 drew from psychology和哲学文献，摘取了六种挑战仇恨语言的策略，并通过用户研究和人工生成的反驳数据进行比较。</li>
<li>results: 研究发现，人工生成的反驳数据使用的是更加特定的挑战策略（例如，对恶意刻板印象的反例和外部因素），而机器生成的反驳数据则使用更加通用的策略（例如，普遍抨击仇恨言语）。此外，机器生成的反驳数据经常使用人们认为更不令人信服的策略。研究表明，在生成反驳数据时需要考虑语言下的恶意刻板印象，并且机器需要更好地理解反例。<details>
<summary>Abstract</summary>
Counterspeech, i.e., responses to counteract potential harms of hateful speech, has become an increasingly popular solution to address online hate speech without censorship. However, properly countering hateful language requires countering and dispelling the underlying inaccurate stereotypes implied by such language. In this work, we draw from psychology and philosophy literature to craft six psychologically inspired strategies to challenge the underlying stereotypical implications of hateful language. We first examine the convincingness of each of these strategies through a user study, and then compare their usages in both human- and machine-generated counterspeech datasets. Our results show that human-written counterspeech uses countering strategies that are more specific to the implied stereotype (e.g., counter examples to the stereotype, external factors about the stereotype's origins), whereas machine-generated counterspeech uses less specific strategies (e.g., generally denouncing the hatefulness of speech). Furthermore, machine-generated counterspeech often employs strategies that humans deem less convincing compared to human-produced counterspeech. Our findings point to the importance of accounting for the underlying stereotypical implications of speech when generating counterspeech and for better machine reasoning about anti-stereotypical examples.
</details>
<details>
<summary>摘要</summary>
优先抑制言论中的恶意言论，即通过反制可能带来的危害，已成为解决在线谩骂问题的增 Popular solution。然而，有效地抵消负面语言需要抵消和炸弹下面的不准确刻板印象。在这项工作中，我们从心理学和哲学文献中练习出六种心理启发的抗议策略，以挑战下面的刻板印象。我们首先通过用户研究检验每种策略的有效性，然后在人类生成的反对言论和机器生成的反对言论数据集中比较其使用情况。我们的结果表明，人类生成的反对言论更加特定地抵消刻板印象（例如，对刻板印象的反例和外部因素），而机器生成的反对言论则更多地使用不那么有力的策略（例如，普遍否决谩骂语言的危害）。此外，机器生成的反对言论经常使用人类认为不那么有力的策略。我们的发现表明，当生成反对言论时，需要考虑下面的刻板印象，并更好地机器理解反对刻板印象的示例。
</details></li>
</ul>
<hr>
<h2 id="Score-Normalization-for-a-Faster-Diffusion-Exponential-Integrator-Sampler"><a href="#Score-Normalization-for-a-Faster-Diffusion-Exponential-Integrator-Sampler" class="headerlink" title="Score Normalization for a Faster Diffusion Exponential Integrator Sampler"></a>Score Normalization for a Faster Diffusion Exponential Integrator Sampler</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00157">http://arxiv.org/abs/2311.00157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mtkresearch/Diffusion-DEIS-SN">https://github.com/mtkresearch/Diffusion-DEIS-SN</a></li>
<li>paper_authors: Guoxuan Xia, Duolikun Danier, Ayan Das, Stathi Fotiadis, Farhang Nabiei, Ushnish Sengupta, Alberto Bernacchia</li>
<li>for: 这 paper 是为了快速生成Diffusion Models中的样本而提出的方法，以提高生成质量和减少积分错误。</li>
<li>methods: 这 paper 使用了Diffusion Exponential Integrator Sampler（DEIS），具体来说是Score Function Reparameterisation（SFP）技术，以提高生成质量和减少积分错误。</li>
<li>results: 该 paper 的实验结果表明，使用Score Normalisation（DEIS-SN）技术可以 Consistently improve FID compared to vanilla DEIS，在10 NFEs中提高了FID值从6.44到5.57。<details>
<summary>Abstract</summary>
Recently, zhang et al have proposed the Diffusion Exponential Integrator Sampler (DEIS) for fast generation of samples from Diffusion Models. It leverages the semi-linear nature of the probability flow ordinary differential equation (ODE) in order to greatly reduce integration error and improve generation quality at low numbers of function evaluations (NFEs). Key to this approach is the score function reparameterisation, which reduces the integration error incurred from using a fixed score function estimate over each integration step. The original authors use the default parameterisation used by models trained for noise prediction -- multiply the score by the standard deviation of the conditional forward noising distribution. We find that although the mean absolute value of this score parameterisation is close to constant for a large portion of the reverse sampling process, it changes rapidly at the end of sampling. As a simple fix, we propose to instead reparameterise the score (at inference) by dividing it by the average absolute value of previous score estimates at that time step collected from offline high NFE generations. We find that our score normalisation (DEIS-SN) consistently improves FID compared to vanilla DEIS, showing an FID improvement from 6.44 to 5.57 at 10 NFEs for our CIFAR-10 experiments. Our code is available at https://github.com/mtkresearch/Diffusion-DEIS-SN.
</details>
<details>
<summary>摘要</summary>
最近，张等人提出了快速生成Diffusion模型样本的Diffusion扩展Integrator抽取器（DEIS）。它利用Diffusion模型的半线性性来大幅减少积分误差，提高生成质量，只需要少量的函数评估（NFEs）。关键在于分数函数重parameter化，减少在每个积分步骤中使用固定分数函数估计的积分误差。原始作者使用模型用于随机噪声预测的默认参数化，将分数函数乘以 conditional forward 噪声分布的标准差。我们发现，在大部分逆抽取过程中，这个分数参数化的平均绝对值很接近常数，但在抽取过程的末尾快速变化。为了简化，我们提议在推理时对分数进行正规化（DEIS-SN），将其除以在线上高NFEs生成的上一个时间步骤的平均绝对分数估计的平均值。我们发现，我们的分数正规化（DEIS-SN）在10NFEs下的CIFAR-10实验中 consistently improve FID，从6.44降低到5.57。我们的代码可以在 GitHub上找到：https://github.com/mtkresearch/Diffusion-DEIS-SN。
</details></li>
</ul>
<hr>
<h2 id="RIR-SF-Room-Impulse-Response-Based-Spatial-Feature-for-Multi-channel-Multi-talker-ASR"><a href="#RIR-SF-Room-Impulse-Response-Based-Spatial-Feature-for-Multi-channel-Multi-talker-ASR" class="headerlink" title="RIR-SF: Room Impulse Response Based Spatial Feature for Multi-channel Multi-talker ASR"></a>RIR-SF: Room Impulse Response Based Spatial Feature for Multi-channel Multi-talker ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00146">http://arxiv.org/abs/2311.00146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Shao, Shi-Xiong Zhang, Dong Yu</li>
<li>for: 提高多通道多说者自动语音识别（ASR）系统的精度</li>
<li>methods: 使用 overlap speech signals 与目标说话人的传输到麦克风数组的room impulse response（RIR）进行卷积，从而获得一种新的空间特征——RIR-SF</li>
<li>results: 比较与现有的3D空间特征，经过理论分析和实验验证，新的RIR-SF在多通道多说者ASR系统中具有remarkable 21.3%的相对减少 Character Error Rate（CER），并且在强 reverberation 情况下表现更加稳定和Robust。<details>
<summary>Abstract</summary>
Multi-channel multi-talker automatic speech recognition (ASR) presents ongoing challenges within the speech community, particularly when confronted with significant reverberation effects. In this study, we introduce a novel approach involving the convolution of overlapping speech signals with the room impulse response (RIR) corresponding to the target speaker's transmission to a microphone array. This innovative technique yields a novel spatial feature known as the RIR-SF. Through a comprehensive comparison with the previously established state-of-the-art 3D spatial feature, both theoretical analysis and experimental results substantiate the superiority of our proposed RIR-SF. We demonstrate that the RIR-SF outperforms existing methods, leading to a remarkable 21.3\% relative reduction in the Character Error Rate (CER) in multi-channel multi-talker ASR systems. Importantly, this novel feature exhibits robustness in the face of strong reverberation, surpassing the limitations of previous approaches.
</details>
<details>
<summary>摘要</summary>
多通道多个人自动语音识别（ASR）系统中存在持续的挑战，特别是在面临重要的干扰效应时。在本研究中，我们介绍了一种新的方法，即将重叠的语音信号 convolution 与目标说话人的传输到麦克风数组的房间冲击响应（RIR）。这种新的特征被称为 RIR-SF。经过了对已有状态的评估和实验结果，我们的提议的 RIR-SF 超越了现有的方法，导致了 Character Error Rate（CER）在多通道多个人 ASR 系统中具有remarkable 21.3% 的相对减少。重要的是，这种新的特征在强干扰情况下表现了 Robustness，超越了前一代方法的限制。
</details></li>
</ul>
<hr>
<h2 id="Two-Stage-Classifier-for-Campaign-Negativity-Detection-using-Axis-Embeddings-A-Case-Study-on-Tweets-of-Political-Users-during-2021-Presidential-Election-in-Iran"><a href="#Two-Stage-Classifier-for-Campaign-Negativity-Detection-using-Axis-Embeddings-A-Case-Study-on-Tweets-of-Political-Users-during-2021-Presidential-Election-in-Iran" class="headerlink" title="Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran"></a>Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00143">http://arxiv.org/abs/2311.00143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Rajabi, Ali Mohades</li>
<li>for: 本研究旨在 automatization 政治竞选中的负面语言检测，以便更好地理解候选人和政党在竞选中的策略。</li>
<li>methods: 本研究使用了一种 hybrid 模型，结合了两种机器学习模型，以检测政治 tweet 的负面语言。</li>
<li>results: 研究发现，候选人发布的 tweet 与其负面性无关，而政治人物和组织名称在 tweet 中的存在直接关系到 tweet 的负面性。In English, that would be:</li>
<li>for: The purpose of this study is to automate the detection of negative language in political campaigns, in order to better understand the strategies of candidates and parties.</li>
<li>methods: The study uses a hybrid model that combines two machine learning models to detect negative language in political tweets.</li>
<li>results: The study finds that the publication of a tweet by a candidate is not related to its negativity, but the presence of political persons and organizations in the tweet is directly related to its negativity.<details>
<summary>Abstract</summary>
In elections around the world, the candidates may turn their campaigns toward negativity due to the prospect of failure and time pressure. In the digital age, social media platforms such as Twitter are rich sources of political discourse. Therefore, despite the large amount of data that is published on Twitter, the automatic system for campaign negativity detection can play an essential role in understanding the strategy of candidates and parties in their campaigns. In this paper, we propose a hybrid model for detecting campaign negativity consisting of a two-stage classifier that combines the strengths of two machine learning models. Here, we have collected Persian tweets from 50 political users, including candidates and government officials. Then we annotated 5,100 of them that were published during the year before the 2021 presidential election in Iran. In the proposed model, first, the required datasets of two classifiers based on the cosine similarity of tweet embeddings with axis embeddings (which are the average of embedding in positive and negative classes of tweets) from the training set (85\%) are made, and then these datasets are considered the training set of the two classifiers in the hybrid model. Finally, our best model (RF-RF) was able to achieve 79\% for the macro F1 score and 82\% for the weighted F1 score. By running the best model on the rest of the tweets of 50 political users that were published one year before the election and with the help of statistical models, we find that the publication of a tweet by a candidate has nothing to do with the negativity of that tweet, and the presence of the names of political persons and political organizations in the tweet is directly related to its negativity.
</details>
<details>
<summary>摘要</summary>
在世界各地的选举中，候选人可能因为失败的风险和时间压力而转向负面竞选。在数字时代，社交媒体平台such as Twitter是政治讨论的丰富源泉。因此，尽管 Twitter 上发布了大量数据，但自动化竞选负面检测系统仍然可以在理解候选人和政党的竞选策略中扮演重要角色。在这篇论文中，我们提议一种混合模型来检测竞选负面，包括两个机器学习模型的两个阶段分类器。我们收集了50名政治用户的波斯语微博，包括候选人和政府官员。然后，我们对这些微博进行了5100个标注，这些微博在2021年伊朗总统选举前一年发布。在我们提议的模型中，首先制定了基于微博嵌入的cosine相似性的两个分类器的数据集（85%），然后这些数据集被用作两个分类器的混合模型的训练集。最后，我们的best模型（RF-RF）可以达到79%的macro F1分数和82%的Weighted F1分数。通过将best模型应用于其余50名政治用户发布的一年前的微博，我们发现，候选人发布的微博与负面微博之间没有直接关系，而政治人物和政治组织的名称直接关系到微博的负面性。
</details></li>
</ul>
<hr>
<h2 id="Q-Learning-for-Stochastic-Control-under-General-Information-Structures-and-Non-Markovian-Environments"><a href="#Q-Learning-for-Stochastic-Control-under-General-Information-Structures-and-Non-Markovian-Environments" class="headerlink" title="Q-Learning for Stochastic Control under General Information Structures and Non-Markovian Environments"></a>Q-Learning for Stochastic Control under General Information Structures and Non-Markovian Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00123">http://arxiv.org/abs/2311.00123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Devran Kara, Serdar Yuksel</li>
<li>for: 本研究的主要贡献是提供一个泛化定理，用于描述在某些随机环境下，Q-学习迭代的收敛性。</li>
<li>methods: 本研究使用了一种通用的可能非Markovian的随机环境下的泛化定理，并提供了一个准确地描述迭代的结果和收敛性的条件。</li>
<li>results: 本研究的结果包括：(1) 提供了一个新的收敛定理，用于描述Q-学习迭代在某些随机环境下的收敛性；(2) 对某些随机控制问题的解释和应用，包括部分可观察Markov决策过程（MDP）的量化approximation、部分可观察POMDP的量化approximation、finite windowapproximation和多代模型的收敛性研究等。<details>
<summary>Abstract</summary>
As a primary contribution, we present a convergence theorem for stochastic iterations, and in particular, Q-learning iterates, under a general, possibly non-Markovian, stochastic environment. Our conditions for convergence involve an ergodicity and a positivity criterion. We provide a precise characterization on the limit of the iterates and conditions on the environment and initializations for convergence. As our second contribution, we discuss the implications and applications of this theorem to a variety of stochastic control problems with non-Markovian environments involving (i) quantized approximations of fully observed Markov Decision Processes (MDPs) with continuous spaces (where quantization break down the Markovian structure), (ii) quantized approximations of belief-MDP reduced partially observable MDPS (POMDPs) with weak Feller continuity and a mild version of filter stability (which requires the knowledge of the model by the controller), (iii) finite window approximations of POMDPs under a uniform controlled filter stability (which does not require the knowledge of the model), and (iv) for multi-agent models where convergence of learning dynamics to a new class of equilibria, subjective Q-learning equilibria, will be studied. In addition to the convergence theorem, some implications of the theorem above are new to the literature and others are interpreted as applications of the convergence theorem. Some open problems are noted.
</details>
<details>
<summary>摘要</summary>
Primary Contribution:我们主要贡献是提出了涉及泛化环境的随机迭代收敛定理，特别是Q学迭代的收敛定理。我们的收敛条件包括一个ergodicity和一个正semi定理。我们提供了迭代器的准确特征和环境和初始化条件下的收敛条件。Second Contribution:我们还讨论了这个定理在各种随机控制问题中的应用和意义，包括：（i）在完全观测的Markov决策过程（MDPs）中使用量化方法时的破坏性问题。（ii）在受限 partially observable Markov decision process（POMDPs）中使用弱Feller连续性和满足策略稳定性（需要控制器知道模型）。（iii）在POMDPs中使用 finite window approximation，不需要控制器知道模型。（iv）多个代理模型中的学习动力收敛到一种新的平衡点，称为主观Q学平衡点。除了收敛定理之外，我们还提出了一些新的意义和应用，以及一些未解决的问题。
</details></li>
</ul>
<hr>
<h2 id="Bandit-Driven-Batch-Selection-for-Robust-Learning-under-Label-Noise"><a href="#Bandit-Driven-Batch-Selection-for-Robust-Learning-under-Label-Noise" class="headerlink" title="Bandit-Driven Batch Selection for Robust Learning under Label Noise"></a>Bandit-Driven Batch Selection for Robust Learning under Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00096">http://arxiv.org/abs/2311.00096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Lisicki, Mihai Nica, Graham W. Taylor</li>
<li>for: 提高 Stochastic Gradient Descent (SGD) 训练中批处理的效率，利用 combinatorial bandit 算法。</li>
<li>methods: 利用 combinatorial bandit 算法优化学习过程中的标签噪声问题。</li>
<li>results: 在 CIFAR-10  dataset 上，我们的方法在不同水平的标签损害下 consistently 表现出色，超过现有方法。同时，我们不需要额外的计算开销，可以在复杂的机器学习应用中扩展。<details>
<summary>Abstract</summary>
We introduce a novel approach for batch selection in Stochastic Gradient Descent (SGD) training, leveraging combinatorial bandit algorithms. Our methodology focuses on optimizing the learning process in the presence of label noise, a prevalent issue in real-world datasets. Experimental evaluations on the CIFAR-10 dataset reveal that our approach consistently outperforms existing methods across various levels of label corruption. Importantly, we achieve this superior performance without incurring the computational overhead commonly associated with auxiliary neural network models. This work presents a balanced trade-off between computational efficiency and model efficacy, offering a scalable solution for complex machine learning applications.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的批处理选择方法，基于 combinatorial bandit 算法，用于 Stochastic Gradient Descent（SGD）训练。我们的方法重点是在实际数据集中存在标签噪声的情况下优化学习过程。我们在 CIFAR-10 数据集上进行了实验，结果显示，我们的方法在不同水平的标签损害情况下一直表现优于现有方法，而且不需要付出较高的计算开销。这种方法实现了计算效率和模型效果之间的平衡，为复杂的机器学习应用提供了扩展性的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Expressive-Modeling-Is-Insufficient-for-Offline-RL-A-Tractable-Inference-Perspective"><a href="#Expressive-Modeling-Is-Insufficient-for-Offline-RL-A-Tractable-Inference-Perspective" class="headerlink" title="Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective"></a>Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00094">http://arxiv.org/abs/2311.00094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuejie Liu, Anji Liu, Guy Van den Broeck, Yitao Liang</li>
<li>for: This paper is written for offline Reinforcement Learning (RL) tasks, where the goal is to learn a policy from a set of pre-collected trajectories.</li>
<li>methods: The paper proposes a new approach called Trifle, which leverages modern Tractable Probabilistic Models (TPMs) to bridge the gap between good sequence models and high expected returns at evaluation time.</li>
<li>results: The paper achieves the most state-of-the-art scores in 9 Gym-MuJoCo benchmarks against strong baselines, and significantly outperforms prior approaches in stochastic environments and safe RL tasks with minimum algorithmic modifications.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为offline Reinforcement Learning（RL）任务而写的，目标是从 pré-收集的轨迹集中学习策略。</li>
<li>methods: 这篇论文提出了一种新的方法 called Trifle，利用现代可追踪概率模型（TPMs）来在评估时 bridging 好序列模型和高预期返回之间的差距。</li>
<li>results: 这篇论文在9个 Gym-MuJoCo benchmark上 achieve 最佳状态的得分，并在随机环境和安全RL任务（例如动作约束）中明显超越先前的方法，只需最小的算法修改。<details>
<summary>Abstract</summary>
A popular paradigm for offline Reinforcement Learning (RL) tasks is to first fit the offline trajectories to a sequence model, and then prompt the model for actions that lead to high expected return. While a common consensus is that more expressive sequence models imply better performance, this paper highlights that tractability, the ability to exactly and efficiently answer various probabilistic queries, plays an equally important role. Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions. While it is still possible to approximate such queries, we observe that such crude estimates significantly undermine the benefits brought by expressive sequence models. To overcome this problem, this paper proposes Trifle (Tractable Inference for Offline RL), which leverages modern Tractable Probabilistic Models (TPMs) to bridge the gap between good sequence models and high expected returns at evaluation time. Empirically, Trifle achieves the most state-of-the-art scores in 9 Gym-MuJoCo benchmarks against strong baselines. Further, owing to its tractability, Trifle significantly outperforms prior approaches in stochastic environments and safe RL tasks (e.g. with action constraints) with minimum algorithmic modifications.
</details>
<details>
<summary>摘要</summary>
一种受欢迎的探索学习（RL）任务的策略是先将偏离线轨迹适应到一个序列模型，然后使用模型来获取高预期返回的动作。虽然广泛认为更表达力强的序列模型会导致更好的表现，但这篇论文指出，可行性（可以快速和准确回答多种概率查询）也扮演着重要的角色。具体来说，由于在线上数据收集策略和环境动力学中的基本随机性，需要进行高度非线性的生成，以便获得奖励动作。虽然可以 aproximate这些查询，但我们发现这些粗略估计会很大程度下降表现。为解决这个问题，这篇论文提出了Trifle（可 tractable 探索学习），它利用现代可追踪概率模型（TPMs）来在评估时bridginggood sequence models和高预期返回之间的 gap。Empirically，Trifle在9个 Gym-MuJoCo benchmark中 achievestate-of-the-art 得分，并在随机环境和安全RL任务（例如Action constraints）中表现出优于先前的方法。此外，由于Trifle的可追踪性，它在可追踪环境和安全RL任务中具有更高的表现。
</details></li>
</ul>
<hr>
<h2 id="Safe-multi-agent-motion-planning-under-uncertainty-for-drones-using-filtered-reinforcement-learning"><a href="#Safe-multi-agent-motion-planning-under-uncertainty-for-drones-using-filtered-reinforcement-learning" class="headerlink" title="Safe multi-agent motion planning under uncertainty for drones using filtered reinforcement learning"></a>Safe multi-agent motion planning under uncertainty for drones using filtered reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00063">http://arxiv.org/abs/2311.00063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sleiman Safaoui, Abraham P. Vinod, Ankush Chakrabarty, Rien Quirynen, Nobuyuki Yoshikawa, Stefano Di Cairano</li>
<li>for: 本研究是针对多机器人在不确定、受障碍的工作空间中安全运动规划的问题。</li>
<li>methods: 本研究使用了单机器人学习来学习运动计划，并使用了受限控制基于轨迹规划的方法来保证安全性。</li>
<li>results: 提出的方法可以实时实现多机器人的安全运动规划，并且可以在不确定的工作空间、机器人运动和感知中提供高度的安全性。<details>
<summary>Abstract</summary>
We consider the problem of safe multi-agent motion planning for drones in uncertain, cluttered workspaces. For this problem, we present a tractable motion planner that builds upon the strengths of reinforcement learning and constrained-control-based trajectory planning. First, we use single-agent reinforcement learning to learn motion plans from data that reach the target but may not be collision-free. Next, we use a convex optimization, chance constraints, and set-based methods for constrained control to ensure safety, despite the uncertainty in the workspace, agent motion, and sensing. The proposed approach can handle state and control constraints on the agents, and enforce collision avoidance among themselves and with static obstacles in the workspace with high probability. The proposed approach yields a safe, real-time implementable, multi-agent motion planner that is simpler to train than methods based solely on learning. Numerical simulations and experiments show the efficacy of the approach.
</details>
<details>
<summary>摘要</summary>
我团队考虑了多机器人在不确定、拥堵的工作空间中安全多机器人运动规划问题。我们提出了一种可控的运动规划方法，基于再增强学习和受限控制的轨迹规划。首先，我们使用单机器人再增强学习学习运动计划，从数据中学习到达目标点，但可能不是免涉碰撞的。然后，我们使用几何优化、机会约束和集合方法来保证安全性，即使在工作空间、机器人运动和感知中存在不确定性。我们的方法可以处理机器人状态和控制约束，并且在高概率下避免机器人之间和静止障碍物的碰撞。我们的方法比基于学习 alone 更加容易训练，并且实时可行。数值仿真和实验表明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="The-Generative-AI-Paradox-“What-It-Can-Create-It-May-Not-Understand”"><a href="#The-Generative-AI-Paradox-“What-It-Can-Create-It-May-Not-Understand”" class="headerlink" title="The Generative AI Paradox: “What It Can Create, It May Not Understand”"></a>The Generative AI Paradox: “What It Can Create, It May Not Understand”</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00059">http://arxiv.org/abs/2311.00059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D. Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, Benjamin Newman, Pang Wei Koh, Allyson Ettinger, Yejin Choi</li>
<li>for: 这篇论文探讨了现代生成AI模型的 aparadox：它们可以在秒钟内生成出人类专家水平的输出，但却仍然存在基本的理解错误，这与人类的情况不同。</li>
<li>methods: 作者们使用了语言和图像模式的控制实验，测试了生成vs理解的关系，以支持他们的生成AI парадок斯假设。</li>
<li>results: 研究发现，虽然模型可以超越人类的生成能力，但它们在理解能力、理解和生成能力之间的相关性、以及对恶作剂输入的脆弱性等方面都弱于人类。这支持假设，即模型的生成能力可能不是基于理解能力的。<details>
<summary>Abstract</summary>
The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs. We test this hypothesis through controlled experiments analyzing generation vs. understanding in generative models, across both language and image modalities. Our results show that although models can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, as well as weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. Our findings support the hypothesis that models' generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.
</details>
<details>
<summary>摘要</summary>
最近的生成型人工智能（AI）浪潮引发了历史上无 precedent 的全球关注，同时也引发了对可能性超human的人工智能水平的激动和担忧：模型现在只需几秒钟就可以生成出挑战或超越人类专家水平的输出。然而，模型仍然表现出基本的理解错误，这不是非专家人类会出现的。这给我们提出了一个 aparadox：如何才能够 conciliate seemingly superhuman capabilities 与 persistently basic errors ？在这篇文章中，我们提出了生成AI парадок斯假设：生成模型，经过直接受训练来重现专家水平的输出，获得了不依赖于——可以超越——它们的理解能力的生成能力。与人类不同，人类的基本理解通常会先于生成专家水平的输出。我们通过控制的实验分析生成vs理解在生成模型中，在语言和图像模式下进行测试。我们的结果表明，虽然模型可以超过人类的生成能力，但它们一直 fall short of human capabilities 在理解方面，以及生成和理解性能之间的较弱相关性，以及更容易受到骚扰输入的 brittleness。我们的发现支持生成AI парадок斯假设，并警告我们不要将人工智能与人类智能相提并论。
</details></li>
</ul>
<hr>
<h2 id="Diversity-and-Diffusion-Observations-on-Synthetic-Image-Distributions-with-Stable-Diffusion"><a href="#Diversity-and-Diffusion-Observations-on-Synthetic-Image-Distributions-with-Stable-Diffusion" class="headerlink" title="Diversity and Diffusion: Observations on Synthetic Image Distributions with Stable Diffusion"></a>Diversity and Diffusion: Observations on Synthetic Image Distributions with Stable Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00056">http://arxiv.org/abs/2311.00056</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Marwood, Shumeet Baluja, Yair Alon</li>
<li>for: 本研究旨在探讨现有文本到图像（TTI）系统，如StableDiffusion、Imagen和DALL-E 2，是否可以用于减少人工取样的图像敏感度训练新的机器学习分类器。</li>
<li>methods: 本研究使用现有的TTI系统生成图像，并对这些图像进行Semantic Embeddings（SE）和Contrastive Language-Image Pre-training（CLIP）等方法进行分析。</li>
<li>results: 研究发现，即使使用真实的图像进行训练，仍然存在 Semantic Mismatches（SM）问题，导致分类器在推断时表现不佳。此外，研究还发现了四种限制TTI系统的使用：歧义、遵循提示、缺乏多样性和表示下面的基本概念。此外，研究还发现了CLIP embeddings的几何结构。<details>
<summary>Abstract</summary>
Recent progress in text-to-image (TTI) systems, such as StableDiffusion, Imagen, and DALL-E 2, have made it possible to create realistic images with simple text prompts. It is tempting to use these systems to eliminate the manual task of obtaining natural images for training a new machine learning classifier. However, in all of the experiments performed to date, classifiers trained solely with synthetic images perform poorly at inference, despite the images used for training appearing realistic. Examining this apparent incongruity in detail gives insight into the limitations of the underlying image generation processes. Through the lens of diversity in image creation vs.accuracy of what is created, we dissect the differences in semantic mismatches in what is modeled in synthetic vs. natural images. This will elucidate the roles of the image-languag emodel, CLIP, and the image generation model, diffusion. We find four issues that limit the usefulness of TTI systems for this task: ambiguity, adherence to prompt, lack of diversity, and inability to represent the underlying concept. We further present surprising insights into the geometry of CLIP embeddings.
</details>
<details>
<summary>摘要</summary>
Examining this apparent incongruity in detail reveals the limitations of the underlying image generation processes. By comparing the diversity of images created by TTI systems with the accuracy of those images, we can identify the sources of the mismatch. We find that there are four issues that limit the usefulness of TTI systems for this task: ambiguity, adherence to prompt, lack of diversity, and inability to represent the underlying concept.Furthermore, we present surprising insights into the geometry of CLIP embeddings, which provide a new perspective on the limitations of TTI systems. Our findings have important implications for the use of TTI systems in machine learning and highlight the need for further research to overcome these limitations.
</details></li>
</ul>
<hr>
<h2 id="SC-MIL-Sparsely-Coded-Multiple-Instance-Learning-for-Whole-Slide-Image-Classification"><a href="#SC-MIL-Sparsely-Coded-Multiple-Instance-Learning-for-Whole-Slide-Image-Classification" class="headerlink" title="SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification"></a>SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00048">http://arxiv.org/abs/2311.00048</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sotiraslab/SCMIL">https://github.com/sotiraslab/SCMIL</a></li>
<li>paper_authors: Peijie Qiu, Pan Xiao, Wenhui Zhu, Yalin Wang, Aristeidis Sotiras</li>
<li>for: This paper focuses on improving the performance of weakly supervised whole slide image (WSI) classification using Multiple Instance Learning (MIL).</li>
<li>methods: The proposed method, called sparsely coded MIL (SC-MIL), uses sparse dictionary learning to capture the similarities of instances and improve the feature embeddings. The method also incorporates deep unrolling to make it compatible with deep learning.</li>
<li>results: The proposed SC module was shown to substantially boost the performance of state-of-the-art MIL methods in experiments on multiple datasets, with an acceptable computation cost. The codes are available at \href{<a target="_blank" rel="noopener" href="https://github.com/sotiraslab/SCMIL.git%7D%7Bhttps://github.com/sotiraslab/SCMIL.git%7D">https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}</a>.<details>
<summary>Abstract</summary>
Multiple Instance Learning (MIL) has been widely used in weakly supervised whole slide image (WSI) classification. Typical MIL methods include a feature embedding part that embeds the instances into features via a pre-trained feature extractor and the MIL aggregator that combines instance embeddings into predictions. The current focus has been directed toward improving these parts by refining the feature embeddings through self-supervised pre-training and modeling the correlations between instances separately. In this paper, we proposed a sparsely coded MIL (SC-MIL) that addresses those two aspects at the same time by leveraging sparse dictionary learning. The sparse dictionary learning captures the similarities of instances by expressing them as a sparse linear combination of atoms in an over-complete dictionary. In addition, imposing sparsity help enhance the instance feature embeddings by suppressing irrelevant instances while retaining the most relevant ones. To make the conventional sparse coding algorithm compatible with deep learning, we unrolled it into an SC module by leveraging deep unrolling. The proposed SC module can be incorporated into any existing MIL framework in a plug-and-play manner with an acceptable computation cost. The experimental results on multiple datasets demonstrated that the proposed SC module could substantially boost the performance of state-of-the-art MIL methods. The codes are available at \href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}.
</details>
<details>
<summary>摘要</summary>
多个实例学习（MIL）在弱监督整幕图像（WSI）分类中广泛应用。典型的MIL方法包括一个特征嵌入部分，该部分使用预训练的特征提取器将实例嵌入特征，以及MIL聚合器，该聚合器将实例嵌入特征组合成预测。目前的焦点是在提高这两个部分的性能，通过自我监督预训练提高特征嵌入，并分别模型实例之间的相互关系。在这篇论文中，我们提出了一种稀盐编码MIL（SC-MIL），该方法同时解决了这两个问题。稀盐编码学习捕捉实例之间的相互关系，表示实例为稀盐线性组合的原子集中的稀盐线性组合。此外，在强制稀盐的情况下，可以增强实例特征嵌入，抑制不相关的实例，保留最相关的实例。为使用深度学习，我们将稀盐编码算法拓展到SC模块，并通过深度拓展来实现。这种SC模块可以与现有的MIL框架兼容，并且可以在插件式的方式进行搭配，计算成本可以接受。实验结果表明，提案的SC模块可以明显提高现有MIL方法的性能。代码可以在\href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}中找到。
</details></li>
</ul>
<hr>
<h2 id="Grounding-Visual-Illusions-in-Language-Do-Vision-Language-Models-Perceive-Illusions-Like-Humans"><a href="#Grounding-Visual-Illusions-in-Language-Do-Vision-Language-Models-Perceive-Illusions-Like-Humans" class="headerlink" title="Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?"></a>Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00047">http://arxiv.org/abs/2311.00047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vl-illusion/dataset">https://github.com/vl-illusion/dataset</a></li>
<li>paper_authors: Yichi Zhang, Jiayi Pan, Yuchen Zhou, Rui Pan, Joyce Chai</li>
<li>for: 研究 whether 视觉语言模型 (VLMs) 有类似于人类的视觉假设，或者它们会忠实地表示现实。</li>
<li>methods: 构建了五种视觉假设的数据集，并设计了四个任务来检验现有的 VLMs 中的视觉假设。</li>
<li>results: 发现虽然总体协调低，大型模型更接近人类的视觉和更易受到视觉假设的影响。 这些发现将促进我们对人类和机器之间视觉世界的共同理解和交流的更好的理解，并为未来的计算模型提供了一个进一步的探索。<details>
<summary>Abstract</summary>
Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are available at https://github.com/vl-illusion/dataset.
</details>
<details>
<summary>摘要</summary>
视力语言模型（VLM）在庞大数据量上训练，但人类视觉不一定准确反映物理世界。这引发了关键问题：VLM是否具有人类视觉中的同类型错觉，或者它们忠实地表现实际情况？为了解答这个问题，我们构建了包含五种视觉错觉的数据集，并提出四项任务来检查视觉错觉在当今最先进的VLM中。我们的发现表明，虽然总体对齐率低，大型模型更加接近人类视觉和更易受到视觉错觉的影响。我们的数据集和初步发现将促进人类和机器之间的视觉理解和沟通，并提供未来计算模型更好地与人类对视觉世界的共同感知的开始。数据集和代码可以在<https://github.com/vl-illusion/dataset>获取。
</details></li>
</ul>
<hr>
<h2 id="Limited-Data-Unlimited-Potential-A-Study-on-ViTs-Augmented-by-Masked-Autoencoders"><a href="#Limited-Data-Unlimited-Potential-A-Study-on-ViTs-Augmented-by-Masked-Autoencoders" class="headerlink" title="Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders"></a>Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20704">http://arxiv.org/abs/2310.20704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srijan Das, Tanmay Jain, Dominick Reilly, Pranav Balaji, Soumyajit Karmakar, Shyam Marjit, Xiang Li, Abhijit Das, Michael Ryoo</li>
<li>for: 这个研究旨在探讨如何将限量数据下的视觉变数转换器（ViT）训练成功。</li>
<li>methods: 这个研究使用了自我监督学习（SSL）和继续调整的方法来训练ViT。</li>
<li>results: 研究发现，在限量数据下，同时优化ViT для主要任务和一个自我监督任务（SSAT）是非常有利的。这种方法可以让ViT获得更好的性能，而且可以降低培训时间和碳踪。实验结果显示，SSAT在10个数据集上具有优秀的数据准确性和一致性。此外，SSAT也在视频领域中实现了深伪检测的好效果，证明了其通用性。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite their success, ViTs lack inductive biases, which can make it difficult to train them with limited data. To address this challenge, prior studies suggest training ViTs with self-supervised learning (SSL) and fine-tuning sequentially. However, we observe that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the amount of training data is limited. We explore the appropriate SSL tasks that can be optimized alongside the primary task, the training schemes for these tasks, and the data scale at which they can be most effective. Our findings reveal that SSAT is a powerful technique that enables ViTs to leverage the unique characteristics of both the self-supervised and primary tasks, achieving better performance than typical ViTs pre-training with SSL and fine-tuning sequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT significantly improves ViT performance while reducing carbon footprint. We also confirm the effectiveness of SSAT in the video domain for deepfake detection, showcasing its generalizability. Our code is available at https://github.com/dominickrei/Limited-data-vits.
</details>
<details>
<summary>摘要</summary>
传统的计算机视觉 Task (ViT) 在计算机视觉领域中广泛应用。尽管它们有成功，但它们缺乏逻辑假设，这可能使其在有限数据量下训练变得困难。以前的研究表明，通过自我超vision学习 (SSL) 和顺序精度调整来解决这个挑战。然而，我们发现，在有限数据量下，同时优化 ViT  для主要任务和 Self-Supervised Auxiliary Task (SSAT) 是让人意外地有利的。我们探讨适合 SSL 任务的选择、训练方案和数据规模，以便在有限数据量下实现最佳性能。我们的发现表明，SSAT 是一种强大的技术，它可以让 ViT 利用自身任务和 SSL 任务之间的独特特征，从而实现 better 性能，而不需要大量的训练数据。我们的实验，在 10 个数据集上进行，表明 SSAT 可以提高 ViT 性能，同时降低碳脚印。我们还证实了 SSAT 在视频领域中的深度假设检测 task 的效果，这表明它的普适性。我们的代码可以在 <https://github.com/dominickrei/Limited-data-vits> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Vanishing-Gradients-in-Reinforcement-Finetuning-of-Language-Models"><a href="#Vanishing-Gradients-in-Reinforcement-Finetuning-of-Language-Models" class="headerlink" title="Vanishing Gradients in Reinforcement Finetuning of Language Models"></a>Vanishing Gradients in Reinforcement Finetuning of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20703">http://arxiv.org/abs/2310.20703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua Susskind, Etai Littwin</li>
<li>for: 这种研究旨在提高预训练语言模型的性能，以便更好地满足人类的需求和下游任务。</li>
<li>methods: 该研究使用了 reinforcement finetuning (RFT) 方法，通过最大化一个 (可能是学习的) 奖励函数来调整模型。</li>
<li>results: 研究发现，在 RFT 中，当输入的奖励标准差小于模型的时候，即使预期奖励远离最佳，输入的预期梯度将消失。这会导致奖励最大化变得极其慢。通过实验和理论分析，研究发现这种消失的梯度问题是普遍存在的和有害的。然而，通过一些方法来缓解这种问题，例如在 RFT 阶段使用初始的监督训练 (SFT) 阶段，可以提高 RFT 的性能。<details>
<summary>Abstract</summary>
Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small number of SFT optimization steps on as few as 1% of the input samples can suffice, indicating that the initial SFT phase need not be expensive in terms of compute and data labeling efforts. Overall, our results emphasize that being mindful for inputs whose expected gradient vanishes, as measured by the reward standard deviation, is crucial for successful execution of RFT.
</details>
<details>
<summary>摘要</summary>
预训言语模型通常通过强化训练（RFT）与人类偏好和下游任务相对适配，其中包括通过政策梯度算法 maximize 一个（可能学习的）奖励函数。这项工作揭示了 RFT 中的一个基本优化障碍：我们证明了，对于一个输入，其奖励标准差 beneath 模型时，即使预期奖励远离最优，则预期梯度都将消失。通过实验 RFT  benchmark 和控制环境，以及理论分析，我们证明了这种消失梯度是普遍存在的并有害，导致奖励最大化 extremely slow。最后，我们探讨了在 RFT 中超越消失梯度的方法。我们发现，通常的初始监督 fine-tuning （SFT）阶段是最有前途的方法，这也解释了它在 RFT 链接中的重要性。此外，我们发现一些 SFT 优化步骤在 1% 的输入样本上可以得到充分的效果，这表明了初始 SFT 阶段不必耗费大量计算和数据标注努力。总的来说，我们的结果强调了在 RFT 中注意 inputs 的预期梯度消失，以measure 奖励标准差是关键的。
</details></li>
</ul>
<hr>
<h2 id="HAP-Structure-Aware-Masked-Image-Modeling-for-Human-Centric-Perception"><a href="#HAP-Structure-Aware-Masked-Image-Modeling-for-Human-Centric-Perception" class="headerlink" title="HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception"></a>HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20695">http://arxiv.org/abs/2310.20695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junkunyuan/HAP">https://github.com/junkunyuan/HAP</a></li>
<li>paper_authors: Junkun Yuan, Xinyu Zhang, Hao Zhou, Jian Wang, Zhongwei Qiu, Zhiyin Shao, Shaofeng Zhang, Sifan Long, Kun Kuang, Kun Yao, Junyu Han, Errui Ding, Lanfen Lin, Fei Wu, Jingdong Wang</li>
<li>for: 这篇论文的目的是提出一种基于人体结构约束的预训练方法，以提高人Centric感知任务的表现。</li>
<li>methods: 该方法使用遮盖图像模型（MIM）作为预训练方法，并在MIM训练策略中引入人体结构约束。具体来说，该方法使用人体部分划分图像 patches，以高优先级遮盖出人体部分区域。这种方法可以让模型在预训练过程中更加注重人体结构信息，从而提高人Centric感知任务的表现。</li>
<li>results: 该方法提出的HAP方法使用普通的ViTEncoder，却在11个人Centric标准数据集上达到了新的州OF-the-art表现，并与一个数据集的表现持平。例如，HAP在MSMT17数据集上达到了78.1% mAP，在PA-100K数据集上达到了86.54% mA，在MS COCO数据集上达到了78.2% AP，以及在3DPW数据集上达到了56.0 PA-MPJPE。<details>
<summary>Abstract</summary>
Model pre-training is essential in human-centric perception. In this paper, we first introduce masked image modeling (MIM) as a pre-training approach for this task. Upon revisiting the MIM training strategy, we reveal that human structure priors offer significant potential. Motivated by this insight, we further incorporate an intuitive human structure prior - human parts - into pre-training. Specifically, we employ this prior to guide the mask sampling process. Image patches, corresponding to human part regions, have high priority to be masked out. This encourages the model to concentrate more on body structure information during pre-training, yielding substantial benefits across a range of human-centric perception tasks. To further capture human characteristics, we propose a structure-invariant alignment loss that enforces different masked views, guided by the human part prior, to be closely aligned for the same image. We term the entire method as HAP. HAP simply uses a plain ViT as the encoder yet establishes new state-of-the-art performance on 11 human-centric benchmarks, and on-par result on one dataset. For example, HAP achieves 78.1% mAP on MSMT17 for person re-identification, 86.54% mA on PA-100K for pedestrian attribute recognition, 78.2% AP on MS COCO for 2D pose estimation, and 56.0 PA-MPJPE on 3DPW for 3D pose and shape estimation.
</details>
<details>
<summary>摘要</summary>
模型预训练是人类视觉任务中的关键。在这篇论文中，我们首先介绍了隐藏图像模型（MIM）作为预训练方法。在检查MIM训练策略时，我们发现人体结构优先可以提供显著的潜在优势。 Motivated by this insight, we further incorporate an intuitive human structure prior - human parts - into pre-training. Specifically, we employ this prior to guide the mask sampling process. Image patches corresponding to human part regions have high priority to be masked out, which encourages the model to concentrate more on body structure information during pre-training, yielding substantial benefits across a range of human-centric perception tasks. To further capture human characteristics, we propose a structure-invariant alignment loss that enforces different masked views, guided by the human part prior, to be closely aligned for the same image. We term the entire method as HAP. HAP uses a plain ViT as the encoder and establishes new state-of-the-art performance on 11 human-centric benchmarks, and on-par results on one dataset. For example, HAP achieves 78.1% mAP on MSMT17 for person re-identification, 86.54% mA on PA-100K for pedestrian attribute recognition, 78.2% AP on MS COCO for 2D pose estimation, and 56.0 PA-MPJPE on 3DPW for 3D pose and shape estimation.
</details></li>
</ul>
<hr>
<h2 id="Learning-From-Mistakes-Makes-LLM-Better-Reasoner"><a href="#Learning-From-Mistakes-Makes-LLM-Better-Reasoner" class="headerlink" title="Learning From Mistakes Makes LLM Better Reasoner"></a>Learning From Mistakes Makes LLM Better Reasoner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20689">http://arxiv.org/abs/2310.20689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/codet">https://github.com/microsoft/codet</a></li>
<li>paper_authors: Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, Weizhu Chen</li>
<li>for: 这项研究的目的是提高大语言模型（LLM）的数学问题解释能力。</li>
<li>methods: 该研究提出了一种基于错误学习（LeMa）的方法，模仿人类学习过程中的错误推理。</li>
<li>results: 实验结果表明，使用LeMa方法可以提高 LLM 的性能，并且可以将其应用到特殊的数学问题解释模型中，例如 WizardMath 和 MetaMath。<details>
<summary>Abstract</summary>
Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve this capability, this work proposes Learning from Mistakes (LeMa), akin to human learning processes. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LeMa fine-tunes LLMs on mistake-correction data pairs generated by GPT-4. Specifically, we first collect inaccurate reasoning paths from various LLMs and then employ GPT-4 as a "corrector" to (1) identify the mistake step, (2) explain the reason for the mistake, and (3) correct the mistake and generate the final answer. Experimental results demonstrate the effectiveness of LeMa: across five backbone LLMs and two mathematical reasoning tasks, LeMa consistently improves the performance compared with fine-tuning on CoT data alone. Impressively, LeMa can also benefit specialized LLMs such as WizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on MATH. This surpasses the SOTA performance achieved by non-execution open-source models on these challenging tasks. Our code, data and models will be publicly available at https://github.com/microsoft/CodeT.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）最近表现出色的推理能力，以解决 math 问题。为了进一步改善这个能力，这个工作提出了学习自错（LeMa），与人类学习过程相似。例如，一个人学生不能解决一个 math 问题，他将从错误中学习，并推断出错误的步骤，以及如何修正错误。模仿这个错误驱动学习过程，LeMa 精细调整 LLM 的过程，使其能够更好地推理。具体来说，我们首先收集了不同 LLM 的错误推理路径，然后使用 GPT-4 作为 "修正者"，以（1）识别错误步骤，（2）解释错误的原因，以及（3）修正错误，并生成最终的答案。实验结果显示 LeMa 的有效性：在五个基本 LLM 和两个数学推理任务上，LeMa  invariably 提高了表现，与精细调整 CoT 数据 alone 相比。甚至可以帮助特殊化 LLM 如 WizardMath 和 MetaMath，在 GSM8K 和 MATH 这两个具有挑战性的任务上获得 85.4% 的通过率和 27.1% 的率，这超过了非执行的开源模型在这些任务上的最佳表现。我们将我们的代码、数据和模型公开 disponibile 在 https://github.com/microsoft/CodeT。
</details></li>
</ul>
<hr>
<h2 id="Offline-RL-with-Observation-Histories-Analyzing-and-Improving-Sample-Complexity"><a href="#Offline-RL-with-Observation-Histories-Analyzing-and-Improving-Sample-Complexity" class="headerlink" title="Offline RL with Observation Histories: Analyzing and Improving Sample Complexity"></a>Offline RL with Observation Histories: Analyzing and Improving Sample Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20663">http://arxiv.org/abs/2310.20663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joey Hong, Anca Dragan, Sergey Levine</li>
<li>for: The paper focuses on the problem of offline reinforcement learning (RL) in situations where the state is partially observed or unknown.</li>
<li>methods: The paper proposes a new loss function called the bisimulation loss, which encourages the RL algorithm to learn a compact representation of the history that is relevant for action selection.</li>
<li>results: The paper shows that the proposed loss can improve the performance of offline RL in a variety of tasks, and that it is closely related to good performance.<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) can in principle synthesize more optimal behavior from a dataset consisting only of suboptimal trials. One way that this can happen is by "stitching" together the best parts of otherwise suboptimal trajectories that overlap on similar states, to create new behaviors where each individual state is in-distribution, but the overall returns are higher. However, in many interesting and complex applications, such as autonomous navigation and dialogue systems, the state is partially observed. Even worse, the state representation is unknown or not easy to define. In such cases, policies and value functions are often conditioned on observation histories instead of states. In these cases, it is not clear if the same kind of "stitching" is feasible at the level of observation histories, since two different trajectories would always have different histories, and thus "similar states" that might lead to effective stitching cannot be leveraged. Theoretically, we show that standard offline RL algorithms conditioned on observation histories suffer from poor sample complexity, in accordance with the above intuition. We then identify sufficient conditions under which offline RL can still be efficient -- intuitively, it needs to learn a compact representation of history comprising only features relevant for action selection. We introduce a bisimulation loss that captures the extent to which this happens, and propose that offline RL can explicitly optimize this loss to aid worst-case sample complexity. Empirically, we show that across a variety of tasks either our proposed loss improves performance, or the value of this loss is already minimized as a consequence of standard offline RL, indicating that it correlates well with good performance.
</details>
<details>
<summary>摘要</summary>
偏向式学习（RL）可以在理论上Synthesize更优化的行为从一个只包含不优化尝试的数据集中。一种方式是将不同的尝试中的最佳部分“缝合” вместе，创造新的行为，每个状态都是内部分布的，但总体返回高于原来的。然而，在许多有趣和复杂的应用，如自动驾驶和对话系统，状态是部分可见的。甚至更糟糕，状态表示是未知或不容易定义。在这些情况下，策略和值函数通常是根据观察历史条件的，而不是根据状态。在这些情况下，是否可以在观察历史上进行类似的“缝合”，并不可能。我们证明了标准的偏向式RL算法 conditioned on 观察历史会受到低效样本复杂性的限制，符合上述直觉。我们然后提出了可能的有效条件，其中学习一个紧凑的历史表示，其中只包含行动选择所需的特征。我们引入了一种bisimulation损失，用于衡量这种情况是否发生。我们建议在offline RL中显式优化这种损失，以提高最坏情况的样本复杂性。实验表明，在各种任务中，我们的提议的损失可以提高性能，或者标准的offline RL可以自动地最小化这种损失，这表明它与好的性能有 corrrelation。
</details></li>
</ul>
<hr>
<h2 id="“Pick-and-Pass”-as-a-Hat-Trick-Class-for-First-Principle-Memory-Generalizability-and-Interpretability-Benchmarks"><a href="#“Pick-and-Pass”-as-a-Hat-Trick-Class-for-First-Principle-Memory-Generalizability-and-Interpretability-Benchmarks" class="headerlink" title="“Pick-and-Pass” as a Hat-Trick Class for First-Principle Memory, Generalizability, and Interpretability Benchmarks"></a>“Pick-and-Pass” as a Hat-Trick Class for First-Principle Memory, Generalizability, and Interpretability Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20654">http://arxiv.org/abs/2310.20654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Wang, Ryan Rezai</li>
<li>For: The paper is written to study model-free reinforcement learning algorithms and their ability to learn memory in closed drafting games, specifically the popular game “Sushi Go Party!”.* Methods: The paper uses a set of closely-related games based on the set of cards in play to establish first-principle benchmarks for studying model-free reinforcement learning algorithms.* Results: The paper produces state-of-the-art results on the Sushi Go Party! environment and quantifies the generalizability of reinforcement learning algorithms trained on various sets of cards, establishing key trends between generalized performance and the set distance between the train and evaluation game configurations. Additionally, the paper fits decision rules to interpret the strategy of the learned models and compares them to the ranking preferences of human players, finding intuitive common rules and intriguing new moves.<details>
<summary>Abstract</summary>
Closed drafting or "pick and pass" is a popular game mechanic where each round players select a card or other playable element from their hand and pass the rest to the next player. Games employing closed drafting make for great studies on memory and turn order due to their explicitly calculable memory of other players' hands. In this paper, we establish first-principle benchmarks for studying model-free reinforcement learning algorithms and their comparative ability to learn memory in a popular family of closed drafting games called "Sushi Go Party!", producing state-of-the-art results on this environment along the way. Furthermore, as Sushi Go Party! can be expressed as a set of closely-related games based on the set of cards in play, we quantify the generalizability of reinforcement learning algorithms trained on various sets of cards, establishing key trends between generalized performance and the set distance between the train and evaluation game configurations. Finally, we fit decision rules to interpret the strategy of the learned models and compare them to the ranking preferences of human players, finding intuitive common rules and intriguing new moves.
</details>
<details>
<summary>摘要</summary>
封闭 drafting 或 "挑选并传递" 是一种受欢迎的游戏机制，每回合玩家从手中选择一张卡或其他可玩元素，并将剩下的交给下一位玩家。由于这种机制的明确可计算的记忆，因此关于记忆和轮次的研究非常有价值。在这篇论文中，我们建立了基于原理的基准值，用于研究无基础学习算法在受欢迎的closed drafting游戏 "Sushi Go Party!" 中学习记忆的能力，并在这个环境中实现了state-of-the-art的结果。此外，由于Sushi Go Party! 可以表示为一系列基于卡片的游戏，我们量化了各种卡片集的对游戏性能的影响，并确定了关键的总体趋势。最后，我们采用决策规则来解释学习模型的策略，并与人类玩家的排名偏好进行比较，发现了直观的共同规则以及意外的新动作。
</details></li>
</ul>
<hr>
<h2 id="Histopathological-Image-Analysis-with-Style-Augmented-Feature-Domain-Mixing-for-Improved-Generalization"><a href="#Histopathological-Image-Analysis-with-Style-Augmented-Feature-Domain-Mixing-for-Improved-Generalization" class="headerlink" title="Histopathological Image Analysis with Style-Augmented Feature Domain Mixing for Improved Generalization"></a>Histopathological Image Analysis with Style-Augmented Feature Domain Mixing for Improved Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20638">http://arxiv.org/abs/2310.20638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vaibhav-khamankar/fusestyle">https://github.com/vaibhav-khamankar/fusestyle</a></li>
<li>paper_authors: Vaibhav Khamankar, Sutanu Bera, Saumik Bhattacharya, Debashis Sen, Prabir Kumar Biswas</li>
<li>for: 增强机器学习模型对 histopathological 图像的泛化能力</li>
<li>methods: 使用 adaptive instance normalization 生成风格增强版图像</li>
<li>results: 比较与现有风格传输基于数据增强方法，我们的方法性能相似或更好，即使需要较少的计算和时间。<details>
<summary>Abstract</summary>
Histopathological images are essential for medical diagnosis and treatment planning, but interpreting them accurately using machine learning can be challenging due to variations in tissue preparation, staining and imaging protocols. Domain generalization aims to address such limitations by enabling the learning models to generalize to new datasets or populations. Style transfer-based data augmentation is an emerging technique that can be used to improve the generalizability of machine learning models for histopathological images. However, existing style transfer-based methods can be computationally expensive, and they rely on artistic styles, which can negatively impact model accuracy. In this study, we propose a feature domain style mixing technique that uses adaptive instance normalization to generate style-augmented versions of images. We compare our proposed method with existing style transfer-based data augmentation methods and found that it performs similarly or better, despite requiring less computation and time. Our results demonstrate the potential of feature domain statistics mixing in the generalization of learning models for histopathological image analysis.
</details>
<details>
<summary>摘要</summary>
In this study, we propose a feature domain style mixing technique that uses adaptive instance normalization to generate style-augmented versions of images. We compare our proposed method with existing style transfer-based data augmentation methods and found that it performs similarly or better, despite requiring less computation and time. Our results demonstrate the potential of feature domain statistics mixing in the generalization of learning models for histopathological image analysis.Here is the text in Simplified Chinese: histopathological 图像是医学诊断和治疗规划中不可或缺的，但是使用机器学习解释它们的准确性却有限制，这是因为样本准备、染色和扫描协议的变化。Domain generalization想要解决这些限制，使得学习模型能够通过新的数据集或人口来泛化。 Style transfer-based 数据增强是一种emerging技术，可以提高机器学习模型对 histopathological 图像的泛化能力。然而，现有的方法可能需要大量的计算时间，并且可能会因为艺术风格而导致模型精度下降。在这种研究中，我们提出了一种特征领域样式混合技术，使用适应实例Normalization来生成样式增强版图像。我们与现有的样式传输基于的数据增强方法进行比较，发现我们的提议方法能够达到相同或更好的性能，即使需要更少的计算时间和资源。我们的结果表明特征领域统计混合在机器学习模型的泛化中具有潜力。
</details></li>
</ul>
<hr>
<h2 id="LoRA-Fine-tuning-Efficiently-Undoes-Safety-Training-in-Llama-2-Chat-70B"><a href="#LoRA-Fine-tuning-Efficiently-Undoes-Safety-Training-in-Llama-2-Chat-70B" class="headerlink" title="LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B"></a>LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20624">http://arxiv.org/abs/2310.20624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Lermen, Charlie Rogers-Smith, Jeffrey Ladish</li>
<li>for: 这个论文的目的是研究语言模型的安全训练是否能够防止模型被恶意使用。</li>
<li>methods: 该论文使用了低级别适应（LoRA）方法进行训练，并使用了一个GPU和一个预算低于200美元来恢复Llama 2-Chat模型的安全训练。</li>
<li>results: 研究人员成功地使用了这种方法来破坏Llama 2-Chat模型的安全训练，并在两个退回测试中将模型的拒绝率降低到1%以下。此外，研究人员还 validate了这种方法对Llama 2-Chat模型的一致性。<details>
<summary>Abstract</summary>
AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat, a collection of instruction fine-tuned large language models, they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. However, it remains unclear how well safety training guards against model misuse when attackers have access to model weights. We explore the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat. We employ low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than $200 per model and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions. We achieve a refusal rate below 1% for our 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method retains general performance, which we validate by comparing our fine-tuned models against Llama 2-Chat across two benchmarks. Additionally, we present a selection of harmful outputs produced by our models. While there is considerable uncertainty about the scope of risks from current models, it is likely that future models will have significantly more dangerous capabilities, including the ability to hack into critical infrastructure, create dangerous bio-weapons, or autonomously replicate and adapt to new environments. We show that subversive fine-tuning is practical and effective, and hence argue that evaluating risks from fine-tuning should be a core part of risk assessments for releasing model weights.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Autonomous-Robotic-Reinforcement-Learning-with-Asynchronous-Human-Feedback"><a href="#Autonomous-Robotic-Reinforcement-Learning-with-Asynchronous-Human-Feedback" class="headerlink" title="Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback"></a>Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20608">http://arxiv.org/abs/2310.20608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Balsells, Marcel Torne, Zihan Wang, Samedh Desai, Pulkit Agrawal, Abhishek Gupta</li>
<li>for: 本研究旨在实现自主机器人学习在真实环境中，无需专门设计奖励函数或重置机制。</li>
<li>methods: 该研究使用了 occasional non-expert human-in-the-loop 反馈，以学习 informative distance functions，并使用 simple self-supervised learning algorithm 来学习目标决策。</li>
<li>results: 研究表明，在缺省情况下，需要考虑当前探索策略的可达性，以确定哪些空间区域要探索。基于这一点，实现了一个实用的学习系统 - GEAR，可以让机器人直接在真实环境中学习自主地，无需中断。系统通过流动机器人经验到网页界面，并且只需 periodic asynchronous 非专业人员反馈。研究在 simulate 和实际环境中展示了其效果。<details>
<summary>Abstract</summary>
Ideally, we would place a robot in a real-world environment and leave it there improving on its own by gathering more experience autonomously. However, algorithms for autonomous robotic learning have been challenging to realize in the real world. While this has often been attributed to the challenge of sample complexity, even sample-efficient techniques are hampered by two major challenges - the difficulty of providing well "shaped" rewards, and the difficulty of continual reset-free training. In this work, we describe a system for real-world reinforcement learning that enables agents to show continual improvement by training directly in the real world without requiring painstaking effort to hand-design reward functions or reset mechanisms. Our system leverages occasional non-expert human-in-the-loop feedback from remote users to learn informative distance functions to guide exploration while leveraging a simple self-supervised learning algorithm for goal-directed policy learning. We show that in the absence of resets, it is particularly important to account for the current "reachability" of the exploration policy when deciding which regions of the space to explore. Based on this insight, we instantiate a practical learning system - GEAR, which enables robots to simply be placed in real-world environments and left to train autonomously without interruption. The system streams robot experience to a web interface only requiring occasional asynchronous feedback from remote, crowdsourced, non-expert humans in the form of binary comparative feedback. We evaluate this system on a suite of robotic tasks in simulation and demonstrate its effectiveness at learning behaviors both in simulation and the real world. Project website https://guided-exploration-autonomous-rl.github.io/GEAR/.
</details>
<details>
<summary>摘要</summary>
理想情况下，我们会将机器人放置在真实环境中，让它自动学习并不断改进，通过收集更多的经验。然而，自动机器人学习算法在真实世界中实现很困难。这经常被归结为样本复杂度的问题，而且甚至使用样本效率的技术也受到两大挑战：一是设置合适的奖励函数，二是实现不间断的培训。在这项工作中，我们描述了一种能够在真实世界中进行自主学习的系统，允许代理人通过不间断的培训来展现持续改进。我们的系统利用远程非专家用户的 occasional 非专业反馈来学习有用的距离函数，并使用简单的自适应学习算法来学习目标导向策略。我们发现在不间断培训情况下，特别重要的是考虑当前探索策略的可达性。基于这一点，我们实现了一个实用的学习系统——GEAR，允许机器人直接在真实环境中培训，并不需要繁琐的手动设计奖励函数或重置机制。系统将机器人经验流向网络界面，只需要 occasional 非专业用户在远程地提供偶极性反馈。我们在一组机器人任务上进行了模拟和实际环境的测试，并证明了该系统在学习行为方面的效果。项目网站：<https://guided-exploration-autonomous-rl.github.io/GEAR/>。
</details></li>
</ul>
<hr>
<h2 id="What-a-Whole-Slide-Image-Can-Tell-Subtype-guided-Masked-Transformer-for-Pathological-Image-Captioning"><a href="#What-a-Whole-Slide-Image-Can-Tell-Subtype-guided-Masked-Transformer-for-Pathological-Image-Captioning" class="headerlink" title="What a Whole Slide Image Can Tell? Subtype-guided Masked Transformer for Pathological Image Captioning"></a>What a Whole Slide Image Can Tell? Subtype-guided Masked Transformer for Pathological Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20607">http://arxiv.org/abs/2310.20607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenkang Qin, Rui Xu, Peixiang Huang, Xiaomin Wu, Heyu Zhang, Lin Luo</li>
<li>for:  This paper proposes a new approach for pathological captioning of Whole Slide Images (WSIs) using Transformers, with the goal of improving the accuracy of computer-aided pathological diagnosis.</li>
<li>methods: The proposed approach, called Subtype-guided Masked Transformer (SGMT), treats a WSI as a sequence of sparse patches and generates an overall caption sentence from the sequence. An accompanying subtype prediction is introduced to guide the training process and enhance captioning accuracy. The Asymmetric Masked Mechanism approach is also used to tackle the large size constraint of pathological image captioning.</li>
<li>results: The authors report that their approach achieves superior performance compared to traditional RNN-based methods on the PatchGastricADC22 dataset.<details>
<summary>Abstract</summary>
Pathological captioning of Whole Slide Images (WSIs), though is essential in computer-aided pathological diagnosis, has rarely been studied due to the limitations in datasets and model training efficacy. In this paper, we propose a new paradigm Subtype-guided Masked Transformer (SGMT) for pathological captioning based on Transformers, which treats a WSI as a sequence of sparse patches and generates an overall caption sentence from the sequence. An accompanying subtype prediction is introduced into SGMT to guide the training process and enhance the captioning accuracy. We also present an Asymmetric Masked Mechansim approach to tackle the large size constraint of pathological image captioning, where the numbers of sequencing patches in SGMT are sampled differently in the training and inferring phases, respectively. Experiments on the PatchGastricADC22 dataset demonstrate that our approach effectively adapts to the task with a transformer-based model and achieves superior performance than traditional RNN-based methods. Our codes are to be made available for further research and development.
</details>
<details>
<summary>摘要</summary>
您好！我们在这篇论文中提出了一个新的思路，即基于Transformers的Subtype-guided Masked Transformer（SGMT），用于Computer-aided Pathological Diagnosis（CPD）中的标本描述。我们将整个标本视为一系列叠加的稀疏区块，并从这些区块中生成一个整体描述句子。此外，我们还引入了一个供应预测的子类别预测方法，以帮助训练过程并提高描述精度。此外，我们还提出了一个对应的Asymmetric Masked Mechanism方法，以解决Pathological Image Captioning中的大型数据集的限制。实验结果显示，我们的方法可以将Transformer-based模型适应这个任务，并在条件下超越传统RNN-based方法。我们的代码将会为更多的研究和发展提供。
</details></li>
</ul>
<hr>
<h2 id="Functional-connectivity-modules-in-recurrent-neural-networks-function-origin-and-dynamics"><a href="#Functional-connectivity-modules-in-recurrent-neural-networks-function-origin-and-dynamics" class="headerlink" title="Functional connectivity modules in recurrent neural networks: function, origin and dynamics"></a>Functional connectivity modules in recurrent neural networks: function, origin and dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20601">http://arxiv.org/abs/2310.20601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Tanner, Sina Mansour L., Ludovico Coletta, Alessandro Gozzi, Richard F. Betzel</li>
<li>for: 这个研究旨在解释大脑功能，尤其是神经同步现象在不同物种和组织水平上的普遍存在。</li>
<li>methods: 这个研究使用了回归神经网络， Investigating the functional role, origin, and dynamical implications of modular structures in correlation-based networks.</li>
<li>results: 研究发现，模块是功能准确的单位，贡献特殊的信息处理。模块自然形成，由输入层到回归层的偏好和重量差异引起。此外，研究发现，模块定义与类似功能连接，控制系统行为和动力学。<details>
<summary>Abstract</summary>
Understanding the ubiquitous phenomenon of neural synchronization across species and organizational levels is crucial for decoding brain function. Despite its prevalence, the specific functional role, origin, and dynamical implication of modular structures in correlation-based networks remains ambiguous. Using recurrent neural networks trained on systems neuroscience tasks, this study investigates these important characteristics of modularity in correlation networks. We demonstrate that modules are functionally coherent units that contribute to specialized information processing. We show that modules form spontaneously from asymmetries in the sign and weight of projections from the input layer to the recurrent layer. Moreover, we show that modules define connections with similar roles in governing system behavior and dynamics. Collectively, our findings clarify the function, formation, and operational significance of functional connectivity modules, offering insights into cortical function and laying the groundwork for further studies on brain function, development, and dynamics.
</details>
<details>
<summary>摘要</summary>
理解跨种类和组织层次的神经同步现象的重要性，可以帮助我们解读大脑的功能。尽管这种现象非常普遍，但模块结构在相关性网络中的特定功能作用、起源和动态影响仍然不够清楚。本研究使用基于系统神经科学任务的循环神经网络进行研究，以了解这些重要特征。我们发现，模块是功能协调的单位，对特定信息处理做出贡献。我们还发现，模块在输入层到循环层的权重和符号差异的基础上自然地形成。此外，我们发现模块在控制系统行为和动力学中扮演着相似的角色。总之，我们的发现可以解释功能连接模块的功能、形成和运作意义，为大脑功能、发展和动态研究提供新的思路和方法。
</details></li>
</ul>
<hr>
<h2 id="Taking-control-Policies-to-address-extinction-risks-from-advanced-AI"><a href="#Taking-control-Policies-to-address-extinction-risks-from-advanced-AI" class="headerlink" title="Taking control: Policies to address extinction risks from advanced AI"></a>Taking control: Policies to address extinction risks from advanced AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20563">http://arxiv.org/abs/2310.20563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Miotti, Akash Wasil</li>
<li>for: 降低人类灭绝的风险，提出了三项政策建议。</li>
<li>methods: 建议包括成立多国AGI联盟（MAGIC），实施全球计算能力上限（global compute cap），并要求企业在开发强大AI系统时进行安全评估（gating critical experiments）。</li>
<li>results: 这三项政策建议可以有效地降低高度智能AI系统对人类灭绝的风险，并且可以让大多数AI创新得以继续不受限制。<details>
<summary>Abstract</summary>
This paper provides policy recommendations to reduce extinction risks from advanced artificial intelligence (AI). First, we briefly provide background information about extinction risks from AI. Second, we argue that voluntary commitments from AI companies would be an inappropriate and insufficient response. Third, we describe three policy proposals that would meaningfully address the threats from advanced AI: (1) establishing a Multinational AGI Consortium to enable democratic oversight of advanced AI (MAGIC), (2) implementing a global cap on the amount of computing power used to train an AI system (global compute cap), and (3) requiring affirmative safety evaluations to ensure that risks are kept below acceptable levels (gating critical experiments). MAGIC would be a secure, safety-focused, internationally-governed institution responsible for reducing risks from advanced AI and performing research to safely harness the benefits of AI. MAGIC would also maintain emergency response infrastructure (kill switch) to swiftly halt AI development or withdraw model deployment in the event of an AI-related emergency. The global compute cap would end the corporate race toward dangerous AI systems while enabling the vast majority of AI innovation to continue unimpeded. Gating critical experiments would ensure that companies developing powerful AI systems are required to present affirmative evidence that these models keep extinction risks below an acceptable threshold. After describing these recommendations, we propose intermediate steps that the international community could take to implement these proposals and lay the groundwork for international coordination around advanced AI.
</details>
<details>
<summary>摘要</summary>
这份报告提供了降低高级人工智能（AI）濒临灭绝风险的政策建议。首先，我们简要介绍高级AI濒临灭绝风险的背景信息。其次，我们认为企业自愿承诺是不适当和不够的回应。第三，我们描述了三个政策建议，可以实际地解决高级AI的威胁：（1）成立多国AGI协会（MAGIC），以实现多国安全协调高级AI的措施，并执行安全的AI研究；（2）实施全球计算能力套限（global compute cap），以结束危险AI系统的企业竞赛，同时允许大多数AI创新继续不受妨碍；（3）要求开发强大AI系统的公司提供证明，以确保风险保持在可接受水平（安全评估）。MAGIC是一个安全、安全感ocus的、国际治理的机构，负责降低高级AI濒临灭绝风险，并执行安全的AI研究。MAGIC还将维护紧急应急基础设施（kill switch），以快速干预AI开发或者模型部署在AI相关紧急情况下。全球计算能力套限将结束危险AI系统的企业竞赛，同时允许大多数AI创新继续不受妨碍。安全评估将确保开发强大AI系统的公司提供证明，以确保风险保持在可接受水平。文章最后，我们建议国际社会可以采取以下措施来实现这些建议，并为高级AI的国际协调做准备。
</details></li>
</ul>
<hr>
<h2 id="Breaking-the-Token-Barrier-Chunking-and-Convolution-for-Efficient-Long-Text-Classification-with-BERT"><a href="#Breaking-the-Token-Barrier-Chunking-and-Convolution-for-Efficient-Long-Text-Classification-with-BERT" class="headerlink" title="Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT"></a>Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20558">http://arxiv.org/abs/2310.20558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aman Jaiswal, Evangelos Milios</li>
<li>for: 本研究旨在提高BERT模型在长输入文本中的应用，并解决BERT模型的512个token限制。</li>
<li>methods: 本研究提出了一种简单扩展BERT模型，称为ChunkBERT，可以让任何预训练模型进行长文本推理。ChunkBERT基于分割token表示和CNN层，可以与任何预训练BERT模型兼容。</li>
<li>results: 在一个用于对比不同长文本分类任务的benchmark中，BERT模型经过ChunkBERT扩展后在长样本中保持稳定性，而且只用了原始内存占用的6.25%。这些结果表明，通过简单地修改预训练BERT模型，可以实现高效的finetuning和推理。<details>
<summary>Abstract</summary>
Transformer-based models, specifically BERT, have propelled research in various NLP tasks. However, these models are limited to a maximum token limit of 512 tokens. Consequently, this makes it non-trivial to apply it in a practical setting with long input. Various complex methods have claimed to overcome this limit, but recent research questions the efficacy of these models across different classification tasks. These complex architectures evaluated on carefully curated long datasets perform at par or worse than simple baselines. In this work, we propose a relatively simple extension to vanilla BERT architecture called ChunkBERT that allows finetuning of any pretrained models to perform inference on arbitrarily long text. The proposed method is based on chunking token representations and CNN layers, making it compatible with any pre-trained BERT. We evaluate chunkBERT exclusively on a benchmark for comparing long-text classification models across a variety of tasks (including binary classification, multi-class classification, and multi-label classification). A BERT model finetuned using the ChunkBERT method performs consistently across long samples in the benchmark while utilizing only a fraction (6.25\%) of the original memory footprint. These findings suggest that efficient finetuning and inference can be achieved through simple modifications to pre-trained BERT models.
</details>
<details>
<summary>摘要</summary>
带基于Transformer的模型，尤其是BERT，在不同的自然语言处理任务中进行研究。然而，这些模型具有最大token数限制为512个，这使得在实际应用中处理长输入变得不容易。许多复杂的方法已经被提出来突破这个限制，但最近的研究表明这些模型在不同的分类任务中的效果存在问题。这些复杂的架构在手动挑选的长数据集上评估时和简单的基线模型相当或更差。在这种工作中，我们提出了一种基于BERT核心架构的简单扩展方法called ChunkBERT，该方法允许任何预训练模型进行长文本的推理。我们基于块化Token表示和CNN层，使其与任何预训练BERT模型兼容。我们在一个用于比较不同任务的长文本分类模型 benchmark 上solely evaluate ChunkBERT。一个使用ChunkBERT方法精度训练的BERT模型在长样本上表现一致，使用的内存占用量仅为原始的6.25%。这些发现表明，通过简单地修改预训练BERT模型，可以实现高效的训练和推理。
</details></li>
</ul>
<hr>
<h2 id="CapsFusion-Rethinking-Image-Text-Data-at-Scale"><a href="#CapsFusion-Rethinking-Image-Text-Data-at-Scale" class="headerlink" title="CapsFusion: Rethinking Image-Text Data at Scale"></a>CapsFusion: Rethinking Image-Text Data at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20550">http://arxiv.org/abs/2310.20550</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baaivision/CapsFusion">https://github.com/baaivision/CapsFusion</a></li>
<li>paper_authors: Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, Jingjing Liu</li>
<li>for: 提高大型多Modal模型的泛化能力和可扩展性，以进行多modal任务 zero-shot learning。</li>
<li>methods: 利用大规模 web 上的图片文本对数据，以及使用 captioning 模型生成的假 caption，进行模型训练。</li>
<li>results: CapsFusion 模型在 COCO 和 NoCaps 测试集上的 CIDEr 得分提高 18.8 和 18.3 分，在模型性能、样本效率、世界知识深度和可扩展性三个方面表现出色。<details>
<summary>Abstract</summary>
Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experiments show that CapsFusion captions exhibit remarkable all-round superiority over existing captions in terms of model performance (e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample efficiency (requiring 11-16 times less computation than baselines), world knowledge depth, and scalability. These effectiveness, efficiency and scalability advantages position CapsFusion as a promising candidate for future scaling of LMM training.
</details>
<details>
<summary>摘要</summary>
大型多modal模型在零值模式下表现出了惊人的通用能力，可以完成多种多modal任务。大规模的网络上的图片文本对象贡献到这些成功的基础，但是受到过度噪音的影响。最近的研究使用了由captioning模型生成的另外的caption，并达到了很好的 benchMark性能。然而，我们的实验表明，使用生成的caption会导致Scalability Deficiency和World Knowledge Loss问题，这些问题在初始的benchMark成功后被掩盖了。经过仔细分析，我们发现了这些问题的根本原因是现有的生成caption的语言结构过于简单，缺乏知识细节。为了提供更高质量和可扩展的多modal预训练数据，我们提出了CapsFusion，一种高级框架，利用大型语言模型来整合和提高来自网络上的图片文本对象和生成caption的信息。我们的广泛实验表明，CapsFusion caption在模型性能（例如，COCO和NoCaps中的CIDEr得分提高18.8和18.3）、样本效率（需要11-16倍的计算量 menos than baseline）、世界知识深度和可扩展性方面具有惊人的优势。这些优势使CapsFusion成为未来扩展LMM训练的优秀候选人。
</details></li>
</ul>
<hr>
<h2 id="LLMs-may-Dominate-Information-Access-Neural-Retrievers-are-Biased-Towards-LLM-Generated-Texts"><a href="#LLMs-may-Dominate-Information-Access-Neural-Retrievers-are-Biased-Towards-LLM-Generated-Texts" class="headerlink" title="LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts"></a>LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20501">http://arxiv.org/abs/2310.20501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, Jun Xu</li>
<li>for:  investigate the influence of LLM-generated documents on IR systems and the potential biases in neural retrieval models towards LLM-generated text.</li>
<li>methods:  quantitative evaluation of different IR models in scenarios with human-written and LLM-generated texts, text compression analysis, and theoretical analysis.</li>
<li>results:  the neural retrieval models tend to rank LLM-generated documents higher, which is referred to as the “source bias”; this bias is not limited to first-stage neural retrievers but also extends to second-stage neural re-rankers; the bias is due to the neural models’ ability to understand the semantic information of LLM-generated text.<details>
<summary>Abstract</summary>
Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search. With their remarkable capabilities in generating human-like texts, LLMs have created enormous texts on the Internet. As a result, IR systems in the LLMs era are facing a new challenge: the indexed documents now are not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of different IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher.We refer to this category of biases in neural retrieval models towards the LLM-generated text as the \textbf{source bias}. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, we provide an in-depth analysis from the perspective of text compression and observe that neural models can better understand the semantic information of LLM-generated text, which is further substantiated by our theoretical analysis.We also discuss the potential server concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks and codes will later be available at \url{https://github.com/KID-22/LLM4IR-Bias}.
</details>
<details>
<summary>摘要</summary>
最近，大语言模型（LLM）的出现对信息检索（IR）应用领域产生了革命性的变革，特别是在网络搜索中。 LLM 的出色的文本生成能力使得互联网上有巨量的文本出现。这使得 IR 系统在 LLM 时代面临一个新的挑战：索引文档现在不仅由人类创建，还可能由 LLM 自动生成。这些 LLM 生成的文本如何影响 IR 系统是一个压力性的问题，尚未得到探索。在这种情况下，我们进行了量化评估不同 IR 模型在人类写作和 LLM 生成文本卷积中的表现。结果显示，神经搜索模型倾向于将 LLM 生成的文本排名在首位。我们称这种偏见为“源偏见”。此外，我们发现这种偏见不仅存在于第一阶段神经搜索模型中，而且还扩展到第二阶段神经重新排名器中。接着，我们从文本压缩角度进行了深入分析，并证明了神经模型对 LLM 生成文本的 semantic 信息有更好的理解能力。最后，我们讨论了可能由此观察到的服务器问题，并希望我们的发现能够为 IR 社区和更广泛的领域产生一个重要的警示。为便于未来在 LLM 时代进行 IR 探索，我们将在 GitHub 上提供两个新的benchmark和代码，可以在 \url{https://github.com/KID-22/LLM4IR-Bias} 获取。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-Based-Model-With-Self-Distillation-for-Multimodal-Emotion-Recognition-in-Conversations"><a href="#A-Transformer-Based-Model-With-Self-Distillation-for-Multimodal-Emotion-Recognition-in-Conversations" class="headerlink" title="A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations"></a>A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20494">http://arxiv.org/abs/2310.20494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/butterfliesss/sdt">https://github.com/butterfliesss/sdt</a></li>
<li>paper_authors: Hui Ma, Jian Wang, Hongfei Lin, Bo Zhang, Yijia Zhang, Bo Xu</li>
<li>for: 本研究旨在提高对话中情感识别（ERC）的精度，即在对话中识别每个句子的情感。</li>
<li>methods: 本研究使用转换器模型（Transformer），并通过自适应归一化（SDT）来学习多modal信息之间的交互关系，同时动态学习不同modalities之间的权重。</li>
<li>results: 实验结果表明，使用SDT模型在IEMOCAP和MELD数据集上表现出色，超过了之前的基线。<details>
<summary>Abstract</summary>
Emotion recognition in conversations (ERC), the task of recognizing the emotion of each utterance in a conversation, is crucial for building empathetic machines. Existing studies focus mainly on capturing context- and speaker-sensitive dependencies on the textual modality but ignore the significance of multimodal information. Different from emotion recognition in textual conversations, capturing intra- and inter-modal interactions between utterances, learning weights between different modalities, and enhancing modal representations play important roles in multimodal ERC. In this paper, we propose a transformer-based model with self-distillation (SDT) for the task. The transformer-based model captures intra- and inter-modal interactions by utilizing intra- and inter-modal transformers, and learns weights between modalities dynamically by designing a hierarchical gated fusion strategy. Furthermore, to learn more expressive modal representations, we treat soft labels of the proposed model as extra training supervision. Specifically, we introduce self-distillation to transfer knowledge of hard and soft labels from the proposed model to each modality. Experiments on IEMOCAP and MELD datasets demonstrate that SDT outperforms previous state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
倾向感知在对话中（ERC），认识对话中每句话的情感，对于建立同情机器非常重要。现有研究主要集中在文本模式下捕捉上下文和发言人相关的依赖关系，而忽略多Modal信息的重要性。与文本对话的情感认识不同，在多Modal ERC中捕捉 между语音和视频语音之间的内部和交叉模式互动，学习不同模式之间的权重，以及增强模式表示都是关键。在这篇论文中，我们提议一种基于变换器的模型，并使用自适应（SDT）进行学习。变换器模型利用内部和交叉模式 transformer 来捕捉内部和交叉模式互动，并通过设计层次闭合策略来动态学习不同模式之间的权重。此外，为了学习更加表达力的模式表示，我们将提议模型的软标签作为额外的训练监督。具体来说，我们引入自适应来传递模型中的硬标签和软标签知识到每个模式。实验结果表明，SDT在IEMOCAP和MELD dataset上超过了前一个基eline。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Black-boxes-Explainable-Deep-Learning-Models-for-Patent-Classification"><a href="#Unveiling-Black-boxes-Explainable-Deep-Learning-Models-for-Patent-Classification" class="headerlink" title="Unveiling Black-boxes: Explainable Deep Learning Models for Patent Classification"></a>Unveiling Black-boxes: Explainable Deep Learning Models for Patent Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20478">http://arxiv.org/abs/2310.20478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Shajalal, Sebastian Denef, Md. Rezaul Karim, Alexander Boden, Gunnar Stevens</li>
<li>for: 本研究旨在提出一种可解释的多标签专利分类框架，以提高人类专业人员对复杂AI技术的理解和管理能力。</li>
<li>methods: 本研究使用了深度神经网络（DNN）和层wise relevance propagation（LRP）技术，以提供人类可理解的预测解释。通过对模型的预测结果进行反向传播，找出每个预测的相关性分数，并使用这些分数来生成预测的解释。</li>
<li>results: 实验结果表明，使用本研究提出的方法可以在两个 dataset 上实现高效的多标签专利分类，并且生成的解释能够帮助人类更好地理解预测结果。<details>
<summary>Abstract</summary>
Recent technological advancements have led to a large number of patents in a diverse range of domains, making it challenging for human experts to analyze and manage. State-of-the-art methods for multi-label patent classification rely on deep neural networks (DNNs), which are complex and often considered black-boxes due to their opaque decision-making processes. In this paper, we propose a novel deep explainable patent classification framework by introducing layer-wise relevance propagation (LRP) to provide human-understandable explanations for predictions. We train several DNN models, including Bi-LSTM, CNN, and CNN-BiLSTM, and propagate the predictions backward from the output layer up to the input layer of the model to identify the relevance of words for individual predictions. Considering the relevance score, we then generate explanations by visualizing relevant words for the predicted patent class. Experimental results on two datasets comprising two-million patent texts demonstrate high performance in terms of various evaluation measures. The explanations generated for each prediction highlight important relevant words that align with the predicted class, making the prediction more understandable. Explainable systems have the potential to facilitate the adoption of complex AI-enabled methods for patent classification in real-world applications.
</details>
<details>
<summary>摘要</summary>
We train several DNN models, including Bi-LSTM, CNN, and CNN-BiLSTM, and propagate the predictions backward from the output layer up to the input layer of the model to identify the relevance of words for individual predictions. Considering the relevance score, we then generate explanations by visualizing relevant words for the predicted patent class.Experimental results on two datasets comprising two-million patent texts demonstrate high performance in terms of various evaluation measures. The explanations generated for each prediction highlight important relevant words that align with the predicted class, making the prediction more understandable. Explainable systems have the potential to facilitate the adoption of complex AI-enabled methods for patent classification in real-world applications.Translated into Simplified Chinese:最近的技术进步导致了大量的专利文本，使得人类专家分析和管理变得困难。现代的多标签专利分类方法多数使用深度神经网络（DNN），它们的决策过程可能是黑盒子，很难被人类理解。在这篇论文中，我们提出了一种新的深度可解释专利分类框架，通过引入层次相关传播（LRP）来提供人类可理解的解释。我们训练了多个DNN模型，包括Bi-LSTM、CNN和CNN-BiLSTM，并将预测结果倒退到模型的输入层，以确定每个预测的单词的相关性。根据相关性分数，我们 THEN generates explanations by visualizing relevant words for the predicted patent class.实验结果表明，我们在两个数据集上，每个数据集包含200万个专利文本，得到了高效的性能。解释生成的每个预测高亮显示了与预测类相关的关键单词，使预测更容易理解。可解释系统有可能在实际应用中推广复杂的AIEnabled方法。
</details></li>
</ul>
<hr>
<h2 id="Global-Transformer-Architecture-for-Indoor-Room-Temperature-Forecasting"><a href="#Global-Transformer-Architecture-for-Indoor-Room-Temperature-Forecasting" class="headerlink" title="Global Transformer Architecture for Indoor Room Temperature Forecasting"></a>Global Transformer Architecture for Indoor Room Temperature Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20476">http://arxiv.org/abs/2310.20476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alfredo V Clemente, Alessandro Nocente, Massimiliano Ruocco</li>
<li>for: 降低建筑物能源消耗和气候变化释放，提高建筑物内部温度预测精度，以实现有效的控制系统。</li>
<li>methods: 使用全球Transformer架构进行多间房内温度预测，并可以在整个数据集上训练模型，从而提高预测性能和简化部署和维护。</li>
<li>results: 该方法可以提高建筑物内部温度预测精度，并且可以减少建筑物的能源消耗和气候变化释放，为建筑物的能源协调和维护做出了重要贡献。<details>
<summary>Abstract</summary>
A thorough regulation of building energy systems translates in relevant energy savings and in a better comfort for the occupants. Algorithms to predict the thermal state of a building on a certain time horizon with a good confidence are essential for the implementation of effective control systems. This work presents a global Transformer architecture for indoor temperature forecasting in multi-room buildings, aiming at optimizing energy consumption and reducing greenhouse gas emissions associated with HVAC systems. Recent advancements in deep learning have enabled the development of more sophisticated forecasting models compared to traditional feedback control systems. The proposed global Transformer architecture can be trained on the entire dataset encompassing all rooms, eliminating the need for multiple room-specific models, significantly improving predictive performance, and simplifying deployment and maintenance. Notably, this study is the first to apply a Transformer architecture for indoor temperature forecasting in multi-room buildings. The proposed approach provides a novel solution to enhance the accuracy and efficiency of temperature forecasting, serving as a valuable tool to optimize energy consumption and decrease greenhouse gas emissions in the building sector.
</details>
<details>
<summary>摘要</summary>
一种全面的建筑能源系统管理可以导致有效的能源储存和建筑内部的舒适度提高。预测建筑物内部温度的算法在某些时间 horizons 上具有良好的信任度是控制系统的实施所必需的。这项工作提出了一种全球转换器架构，用于多房间内部温度预测，以优化能源消耗和减少建筑物中HVAC系统的绿色气体排放。在深度学习技术的发展下，可以开发出更加复杂的预测模型，而不是传统的反馈控制系统。全球转换器架构可以在整个数据集上训练，消除多个房间特定的模型需求，显著提高预测性能，并简化部署和维护。值得一提的是，这项研究是首次应用转换器架构来预测多房间内部温度。提出的方法可以增强温度预测的准确性和效率，并作为建筑领域中能源消耗优化和绿色气体排放减少的有价值工具。
</details></li>
</ul>
<hr>
<h2 id="Linked-Papers-With-Code-The-Latest-in-Machine-Learning-as-an-RDF-Knowledge-Graph"><a href="#Linked-Papers-With-Code-The-Latest-in-Machine-Learning-as-an-RDF-Knowledge-Graph" class="headerlink" title="Linked Papers With Code: The Latest in Machine Learning as an RDF Knowledge Graph"></a>Linked Papers With Code: The Latest in Machine Learning as an RDF Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20475">http://arxiv.org/abs/2310.20475</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidlamprecht/linkedpaperswithcode">https://github.com/davidlamprecht/linkedpaperswithcode</a></li>
<li>paper_authors: Michael Färber, David Lamprecht</li>
<li>for: 本研究提出了一种基于RDF的机器学习文献知识图 Linked Papers With Code (LPWC), 其提供了大约400,000篇机器学习论文的完整、当前信息，包括论文所处理的任务、使用的数据集、实现的方法以及进行的评估结果。</li>
<li>methods:  LPWC使用RDF格式将最新的机器学习发展翻译成知识图形式，同时允许科学影响量量化和学术关键内容推荐。</li>
<li>results: LPWC可以在 Linked Open Data 云中作为知识图存在，提供了多种格式，包括 RDF 填充文件、SPARQL 端点 для直接网络查询以及数据源与 SemOpenAlex、Wikidata 和 DBLP 的链接。此外，LPWC还提供了知识图嵌入，使其可以 direct 应用于机器学习应用程序。<details>
<summary>Abstract</summary>
In this paper, we introduce Linked Papers With Code (LPWC), an RDF knowledge graph that provides comprehensive, current information about almost 400,000 machine learning publications. This includes the tasks addressed, the datasets utilized, the methods implemented, and the evaluations conducted, along with their results. Compared to its non-RDF-based counterpart Papers With Code, LPWC not only translates the latest advancements in machine learning into RDF format, but also enables novel ways for scientific impact quantification and scholarly key content recommendation. LPWC is openly accessible at https://linkedpaperswithcode.com and is licensed under CC-BY-SA 4.0. As a knowledge graph in the Linked Open Data cloud, we offer LPWC in multiple formats, from RDF dump files to a SPARQL endpoint for direct web queries, as well as a data source with resolvable URIs and links to the data sources SemOpenAlex, Wikidata, and DBLP. Additionally, we supply knowledge graph embeddings, enabling LPWC to be readily applied in machine learning applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了 Linked Papers With Code（LPWC），一个基于 RDF 的知识格 graphs，提供了大约 400,000 篇机器学习论文的全面、当前信息。这包括论文中 Addressed 的任务、使用的数据集、实现的方法、进行的评估和结果。与其非 RDF-based 对手 Papers With Code 不同，LPWC 不仅将最新的机器学习成果翻译成 RDF 格式，还允许科学影响量量化和学术关键内容推荐。LPWC 公开 accessible 于 <https://linkedpaperswithcode.com>，并且被licensed under CC-BY-SA 4.0。作为 Linked Open Data 云中的知识格，我们向你提供了多种格式，从 RDF 冲dump 文件到 SPARQL 终结点，以便直接在网上进行查询，以及一个可以解析 URI 和数据源 SemOpenAlex、Wikidata 和 DBLP 的数据源。此外，我们还提供了知识格嵌入，使 LPWC 可以轻松应用于机器学习应用程序。
</details></li>
</ul>
<hr>
<h2 id="Critical-Role-of-Artificially-Intelligent-Conversational-Chatbot"><a href="#Critical-Role-of-Artificially-Intelligent-Conversational-Chatbot" class="headerlink" title="Critical Role of Artificially Intelligent Conversational Chatbot"></a>Critical Role of Artificially Intelligent Conversational Chatbot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20474">http://arxiv.org/abs/2310.20474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seraj A. M. Mostafa, Md Z. Islam, Mohammad Z. Islam, Fairose Jeehan, Saujanna Jafreen, Raihan U. Islam</li>
<li>for: 本研究旨在探讨 chatGPT 在学术上的伦理问题、其限制和特定用户群体的虚假使用。</li>
<li>methods: 本研究采用了多种方法，包括实验和分析，以探讨 chatGPT 的伦理和限制。</li>
<li>results: 研究发现了一些可能的假设和伦理问题，以及一些解决方案，以便避免不当使用和促进负责任 AI 交互。<details>
<summary>Abstract</summary>
Artificially intelligent chatbot, such as ChatGPT, represents a recent and powerful advancement in the AI domain. Users prefer them for obtaining quick and precise answers, avoiding the usual hassle of clicking through multiple links in traditional searches. ChatGPT's conversational approach makes it comfortable and accessible for finding answers quickly and in an organized manner. However, it is important to note that these chatbots have limitations, especially in terms of providing accurate answers as well as ethical concerns. In this study, we explore various scenarios involving ChatGPT's ethical implications within academic contexts, its limitations, and the potential misuse by specific user groups. To address these challenges, we propose architectural solutions aimed at preventing inappropriate use and promoting responsible AI interactions.
</details>
<details>
<summary>摘要</summary>
人工智能聊天机器人，如ChatGPT，是当今AI领域的一项新的和强大的进步。用户喜欢使用它们以获取快速和准确的答案，而不需要遍历多个链接。聊天机器人的对话方式使得它们易于使用，并且能够组织化地提供答案。然而，这些聊天机器人存在限制，特别是在提供准确答案和伦理问题方面。在这项研究中，我们探讨了ChatGPT在学术上下文中的伦理问题、其限制以及特定用户群体可能会滥用它的问题。为解决这些挑战，我们提议了一些建筑解决方案，以防止不当使用和推动负责任AI互动。
</details></li>
</ul>
<hr>
<h2 id="ACL-Anthology-Helper-A-Tool-to-Retrieve-and-Manage-Literature-from-ACL-Anthology"><a href="#ACL-Anthology-Helper-A-Tool-to-Retrieve-and-Manage-Literature-from-ACL-Anthology" class="headerlink" title="ACL Anthology Helper: A Tool to Retrieve and Manage Literature from ACL Anthology"></a>ACL Anthology Helper: A Tool to Retrieve and Manage Literature from ACL Anthology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20467">http://arxiv.org/abs/2310.20467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Tang, Frank Guerin, Chenghua Lin</li>
<li>for:  This paper is written for researchers who need to efficiently access and organize literature from the ACL Anthology, a comprehensive collection of NLP and CL publications.</li>
<li>methods: The paper presents a tool called ACL Anthology Helper, which automates the process of parsing and downloading papers along with their meta-information, and stores them in a local MySQL database.</li>
<li>results: The tool offers over 20 operations for efficient literature retrieval, including “where,” “group,” “order,” and more, and has been successfully utilized in writing a survey paper (Tang et al.,2022a).<details>
<summary>Abstract</summary>
The ACL Anthology is an online repository that serves as a comprehensive collection of publications in the field of natural language processing (NLP) and computational linguistics (CL). This paper presents a tool called ``ACL Anthology Helper''. It automates the process of parsing and downloading papers along with their meta-information, which are then stored in a local MySQL database. This allows for efficient management of the local papers using a wide range of operations, including "where," "group," "order," and more. By providing over 20 operations, this tool significantly enhances the retrieval of literature based on specific conditions. Notably, this tool has been successfully utilised in writing a survey paper (Tang et al.,2022a). By introducing the ACL Anthology Helper, we aim to enhance researchers' ability to effectively access and organise literature from the ACL Anthology. This tool offers a convenient solution for researchers seeking to explore the ACL Anthology's vast collection of publications while allowing for more targeted and efficient literature retrieval.
</details>
<details>
<summary>摘要</summary>
ACL Anthology是一个在线存储库，它是自然语言处理（NLP）和计算语言学（CL）领域的完整收藏。这篇论文介绍了一种名为“ACL Anthology Helper”的工具。它自动将ACL Anthology中的文章和相关信息解析出来，并将其存储在本地的MySQL数据库中。这使得研究者可以使用各种操作（如“where”、“group”、“order”等）来高效管理本地文章。这个工具提供了超过20种操作，可以帮助研究者根据特定条件进行文献检索。尤其是，这个工具在写作一篇survey paper（Tang et al.,2022a）时得到了成功应用。我们通过引入ACL Anthology Helper，旨在增强研究者对ACL Anthology的文献检索和管理的能力。这个工具提供了一种方便的解决方案，帮助研究者更加高效地探索ACL Anthology的庞大文献收藏，并且允许更加Targeted和高效的文献检索。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Neural-PDE-Solvers-using-Symbolic-Frameworks"><a href="#Interpretable-Neural-PDE-Solvers-using-Symbolic-Frameworks" class="headerlink" title="Interpretable Neural PDE Solvers using Symbolic Frameworks"></a>Interpretable Neural PDE Solvers using Symbolic Frameworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20463">http://arxiv.org/abs/2310.20463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yolanne Yi Ran Lee</li>
<li>for: 这篇论文旨在解决深度学习中的解释性问题，以提高人们对神经网络解决方案的理解和信任度。</li>
<li>methods: 这篇论文提出了将符号框架（如符号回归）与神经网络相结合的想法，以帮助将神经网络的决策过程变换为人类可读的数学表达。</li>
<li>results: 研究人员通过对数据集进行符号框架和神经网络的组合，发现这种方法可以提高神经网络的解释性和准确性。<details>
<summary>Abstract</summary>
Partial differential equations (PDEs) are ubiquitous in the world around us, modelling phenomena from heat and sound to quantum systems. Recent advances in deep learning have resulted in the development of powerful neural solvers; however, while these methods have demonstrated state-of-the-art performance in both accuracy and computational efficiency, a significant challenge remains in their interpretability. Most existing methodologies prioritize predictive accuracy over clarity in the underlying mechanisms driving the model's decisions. Interpretability is crucial for trustworthiness and broader applicability, especially in scientific and engineering domains where neural PDE solvers might see the most impact. In this context, a notable gap in current research is the integration of symbolic frameworks (such as symbolic regression) into these solvers. Symbolic frameworks have the potential to distill complex neural operations into human-readable mathematical expressions, bridging the divide between black-box predictions and solutions.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="AsGrad-A-Sharp-Unified-Analysis-of-Asynchronous-SGD-Algorithms"><a href="#AsGrad-A-Sharp-Unified-Analysis-of-Asynchronous-SGD-Algorithms" class="headerlink" title="AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms"></a>AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20452">http://arxiv.org/abs/2310.20452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rustem Islamov, Mher Safaryan, Dan Alistarh</li>
<li>for: 这个论文是为了研究 asynchronous-type algorithms for distributed SGD in heterogeneous setting 而写的。</li>
<li>methods: 这个论文使用了 asynchronous SGD 和 worker shuffling 等方法。</li>
<li>results: 论文提出了一种 unified convergence theory for non-convex smooth functions in heterogeneous regime, 并且证明了这种方法的性能可以与同步方法相比。<details>
<summary>Abstract</summary>
We analyze asynchronous-type algorithms for distributed SGD in the heterogeneous setting, where each worker has its own computation and communication speeds, as well as data distribution. In these algorithms, workers compute possibly stale and stochastic gradients associated with their local data at some iteration back in history and then return those gradients to the server without synchronizing with other workers. We present a unified convergence theory for non-convex smooth functions in the heterogeneous regime. The proposed analysis provides convergence for pure asynchronous SGD and its various modifications. Moreover, our theory explains what affects the convergence rate and what can be done to improve the performance of asynchronous algorithms. In particular, we introduce a novel asynchronous method based on worker shuffling. As a by-product of our analysis, we also demonstrate convergence guarantees for gradient-type algorithms such as SGD with random reshuffling and shuffle-once mini-batch SGD. The derived rates match the best-known results for those algorithms, highlighting the tightness of our approach. Finally, our numerical evaluations support theoretical findings and show the good practical performance of our method.
</details>
<details>
<summary>摘要</summary>
我们分析了分布式SGD中的异步类算法，在异种设置下进行分析，每个工作者都有自己的计算和通信速度，以及数据分布。在这些算法中，工作者在某个过去的迭代中计算了本地数据相关的可能偏移和随机梯度，然后将这些梯度返回给服务器而无需与其他工作者同步。我们提出了一种统一的收敛理论，用于非对称凸函数的收敛分析。我们的分析显示，异步算法的收敛率受到多种因素的影响，并且可以通过一些方法提高其性能。例如，我们引入了一种基于工作者排序的异步方法。在我们的分析中，我们还证明了SGD与随机排序和排序一次小批量SGD的收敛性能。实验结果支持我们的理论发现，并表明了我们的方法在实际应用中的良好性能。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Bayesian-Learning-Curve-Extrapolation-using-Prior-Data-Fitted-Networks"><a href="#Efficient-Bayesian-Learning-Curve-Extrapolation-using-Prior-Data-Fitted-Networks" class="headerlink" title="Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks"></a>Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20447">http://arxiv.org/abs/2310.20447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Adriaensen, Herilalaina Rakotoarison, Samuel Müller, Frank Hutter</li>
<li>for: 预测模型在训练后期的性能，基于早期训练过程中的性能。</li>
<li>methods: 使用先进的 Bayesian方法，并使用 prior-data fitted neural networks (PFNs) 来approximate Bayesian inference。</li>
<li>results: 对于10万个人工生成的学习曲线，LC-PFN可以更准确地 aproximate posterior predictive distribution，并且比 MCMC 快上万倍；对于20000个真实学习曲线，LC-PFN可以达到竞争性的性能。<details>
<summary>Abstract</summary>
Learning curve extrapolation aims to predict model performance in later epochs of training, based on the performance in earlier epochs. In this work, we argue that, while the inherent uncertainty in the extrapolation of learning curves warrants a Bayesian approach, existing methods are (i) overly restrictive, and/or (ii) computationally expensive. We describe the first application of prior-data fitted neural networks (PFNs) in this context. A PFN is a transformer, pre-trained on data generated from a prior, to perform approximate Bayesian inference in a single forward pass. We propose LC-PFN, a PFN trained to extrapolate 10 million artificial right-censored learning curves generated from a parametric prior proposed in prior art using MCMC. We demonstrate that LC-PFN can approximate the posterior predictive distribution more accurately than MCMC, while being over 10 000 times faster. We also show that the same LC-PFN achieves competitive performance extrapolating a total of 20 000 real learning curves from four learning curve benchmarks (LCBench, NAS-Bench-201, Taskset, and PD1) that stem from training a wide range of model architectures (MLPs, CNNs, RNNs, and Transformers) on 53 different datasets with varying input modalities (tabular, image, text, and protein data). Finally, we investigate its potential in the context of model selection and find that a simple LC-PFN based predictive early stopping criterion obtains 2 - 6x speed-ups on 45 of these datasets, at virtually no overhead.
</details>
<details>
<summary>摘要</summary>
学习曲线拟合目标是预测训练过程中后期模型表现，基于早期表现的预测。在这项工作中，我们认为，由于拟合学习曲线的不确定性，应采用 bayesian 方法。然而，现有方法存在以下两点问题：一是过于限制性，二是计算成本高。我们介绍了在这个上下文中的首次应用先进数据适应神经网络（PFN）。PFN 是一种基于先进数据生成的 transformer，用于在单个前进 pass 中进行 Approximate Bayesian Inference。我们提出了基于先进数据生成的 10 万个人工受限学习曲线（LC-PFN），用于预测 MCMC 生成的参数 posterior 分布。我们证明了 LC-PFN 可以更准确地预测 posterior 分布，而且比 MCMC 快上万分之一。此外，我们还证明了同一 LC-PFN 可以在四个学习曲线 benchmark 上 extrapolate 20 万个真实的学习曲线，来自训练各种模型结构（MLPs、CNNs、RNNs 和 Transformers）和 53 个不同的数据集（标量、图像、文本和蛋白质数据）。最后，我们研究了其在模型选择方面的潜在应用，并发现一个简单的 LC-PFN 基于的预测早期停止 criterion 可以在 45 个数据集上获得 2 - 6 倍的速度增加，而且几乎没有额外成本。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Impact-of-Companies-on-AI-Research-Based-on-Publications"><a href="#Analyzing-the-Impact-of-Companies-on-AI-Research-Based-on-Publications" class="headerlink" title="Analyzing the Impact of Companies on AI Research Based on Publications"></a>Analyzing the Impact of Companies on AI Research Based on Publications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20444">http://arxiv.org/abs/2310.20444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LazaTabax/AI-Impact-Scientometrics">https://github.com/LazaTabax/AI-Impact-Scientometrics</a></li>
<li>paper_authors: Michael Färber, Lazaros Tampakis</li>
<li>for: 本研究的目的是探讨AI研究中企业的影响，以及如何使这种影响变得可衡量。</li>
<li>methods: 研究者使用科学出版活动数据来比较大学和企业在AI领域的出版 activites，并发现了不同之处。</li>
<li>results: 研究发现，与企业合作撰写的AI论文在引用数量方面表现 significanly higher，并且在线上获得了更多的关注。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) is one of the most momentous technologies of our time. Thus, it is of major importance to know which stakeholders influence AI research. Besides researchers at universities and colleges, researchers in companies have hardly been considered in this context. In this article, we consider how the influence of companies on AI research can be made measurable on the basis of scientific publishing activities. We compare academic- and company-authored AI publications published in the last decade and use scientometric data from multiple scholarly databases to look for differences across these groups and to disclose the top contributing organizations. While the vast majority of publications is still produced by academia, we find that the citation count an individual publication receives is significantly higher when it is (co-)authored by a company. Furthermore, using a variety of altmetric indicators, we notice that publications with company participation receive considerably more attention online. Finally, we place our analysis results in a broader context and present targeted recommendations to safeguard a harmonious balance between academia and industry in the realm of AI research.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）是当今最重要的技术之一，因此了解AI研究的各类涉及者对其影响非常重要。在这篇文章中，我们将研究如何使AI研究中公司的影响可衡量，基于科学出版活动。我们比较了过去十年的学术机构和企业合作撰写的人工智能论文，使用多种学术数据库的科学ometrics数据来找到这些组织的差异。虽然大多数论文仍然由学术机构出版，但我们发现，与公司合作者合写的论文的引用数量较高。此外，使用多种Altmetric指标，我们发现在线关注量较高。最后，我们将分析结果置于更广阔的背景下，并提供特点化的建议，以保持学术和产业在人工智能研究中的和谐协作。
</details></li>
</ul>
<hr>
<h2 id="Ontologies-for-Models-and-Algorithms-in-Applied-Mathematics-and-Related-Disciplines"><a href="#Ontologies-for-Models-and-Algorithms-in-Applied-Mathematics-and-Related-Disciplines" class="headerlink" title="Ontologies for Models and Algorithms in Applied Mathematics and Related Disciplines"></a>Ontologies for Models and Algorithms in Applied Mathematics and Related Disciplines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20443">http://arxiv.org/abs/2310.20443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Björn Schembera, Frank Wübbeling, Hendrik Kleikamp, Christine Biedinger, Jochen Fiedler, Marco Reidelbach, Aurela Shehu, Burkhard Schmidt, Thomas Koprucki, Dorothea Iglezakis, Dominik Göddeke</li>
<li>for: 本研究旨在提高数学研究数据的可访问性和共享性，通过semantic技术和数学基础的文档来增强数学研究数据的搜索和理解能力。</li>
<li>methods: 本研究使用了ontology和知识图来描述数学研究数据的结构和意义，并将数学模型和数学算法的解释和实现嵌入到了ontology中。</li>
<li>results: 通过使用ontology和知识图，可以准确地描述数学研究数据的结构和意义，提高了数学研究数据的可访问性和共享性。例如，通过使用ontology来描述微型扰动分析的数学模型和算法，可以增强对数学研究数据的理解和应用。<details>
<summary>Abstract</summary>
In applied mathematics and related disciplines, the modeling-simulation-optimization workflow is a prominent scheme, with mathematical models and numerical algorithms playing a crucial role. For these types of mathematical research data, the Mathematical Research Data Initiative has developed, merged and implemented ontologies and knowledge graphs. This contributes to making mathematical research data FAIR by introducing semantic technology and documenting the mathematical foundations accordingly. Using the concrete example of microfracture analysis of porous media, it is shown how the knowledge of the underlying mathematical model and the corresponding numerical algorithms for its solution can be represented by the ontologies.
</details>
<details>
<summary>摘要</summary>
在应用数学和相关领域，模拟优化工作流程是一种常见的方案，数学模型和数值算法在这些数学研究数据中扮演着关键的角色。为这类数学研究数据，数学研究数据Initative已经开发、合并并实施ontologies和知识图。这有助于使数学研究数据变得FAIR，通过引入语义技术并记录数学基础 accordingly。使用微裂变分析的porous media为例，可以通过ontologies来表示数学模型的知识和相应的数值算法的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Raising-the-ClaSS-of-Streaming-Time-Series-Segmentation"><a href="#Raising-the-ClaSS-of-Streaming-Time-Series-Segmentation" class="headerlink" title="Raising the ClaSS of Streaming Time Series Segmentation"></a>Raising the ClaSS of Streaming Time Series Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20431">http://arxiv.org/abs/2310.20431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ermshaua/classification-score-stream">https://github.com/ermshaua/classification-score-stream</a></li>
<li>paper_authors: Arik Ermshaus, Patrick Schäfer, Ulf Leser</li>
<li>for: 这篇论文的主要目标是提出一种高效精度的流处理时间序列分 segmentation（STSS）算法，用于处理高频数字测量结果，以捕捉人、动物、工业、商业和自然过程中的变化。</li>
<li>methods: 该算法使用自我超vised时间序列分类来评估分区Homogeneity，并应用统计测试检测变化点（CPs）。</li>
<li>results: 对两个大量数据集和六个实际数据存档进行实验评估，发现ClaSS比八种现有的竞争者更加精度，其空间和时间复杂度独立于分区大小，仅与滑动窗口大小有关。 ClaSS还被实现为Apache Flink流处理引擎中的窗口运算符，其吞吐量为538个数据点每秒。<details>
<summary>Abstract</summary>
Ubiquitous sensors today emit high frequency streams of numerical measurements that reflect properties of human, animal, industrial, commercial, and natural processes. Shifts in such processes, e.g. caused by external events or internal state changes, manifest as changes in the recorded signals. The task of streaming time series segmentation (STSS) is to partition the stream into consecutive variable-sized segments that correspond to states of the observed processes or entities. The partition operation itself must in performance be able to cope with the input frequency of the signals. We introduce ClaSS, a novel, efficient, and highly accurate algorithm for STSS. ClaSS assesses the homogeneity of potential partitions using self-supervised time series classification and applies statistical tests to detect significant change points (CPs). In our experimental evaluation using two large benchmarks and six real-world data archives, we found ClaSS to be significantly more precise than eight state-of-the-art competitors. Its space and time complexity is independent of segment sizes and linear only in the sliding window size. We also provide ClaSS as a window operator with an average throughput of 538 data points per second for the Apache Flink streaming engine.
</details>
<details>
<summary>摘要</summary>
今天的普遍存在的传感器 emit高频流量数字测量值，这些测量值反映人类、动物、工业、商业和自然过程的性质。这些过程的变化，例如由外部事件或内部状态变化引起的变化，将在记录的信号中 manifest。 streaming time series segmentation (STSS) 任务是将流量分成 consecutive 变量大小的分段，这些分段对于观察过程或实体的状态具有相应性。分段操作自身必须能够与输入信号频率相应。我们引入 ClaSS，一种新的、高效、高准确的算法 для STSS。 ClaSS 使用自我超级时间序分类来评估可能的分段的一致性，并应用统计测试来检测显著的变化点 (CP)。在我们使用两个大的标准benchmark和六个实际数据存档进行实验evaluation中，我们发现 ClaSS 与八种当前状态的竞争者相比，显著更精确。其空间和时间复杂度是 independetnt of segment size和linear只在滑动窗口大小。我们还提供了 ClaSS 作为窗口运算符，其均匀读取速率为 Apache Flink 流处理引擎中的538个数据点每秒。
</details></li>
</ul>
<hr>
<h2 id="Meta-Learning-for-Multi-View-Visuomotor-Systems"><a href="#Meta-Learning-for-Multi-View-Visuomotor-Systems" class="headerlink" title="Meta Learning for Multi-View Visuomotor Systems"></a>Meta Learning for Multi-View Visuomotor Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20414">http://arxiv.org/abs/2310.20414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benji Alwis, Nick Pears, Pengcheng Liu</li>
<li>for: 这篇论文旨在简化多视角动作系统的应用，以适应不同的摄像头配置。</li>
<li>methods: 本篇论文使用meta-学习精确地调整感知网络，保持政策网络不变。</li>
<li>results: 实验结果显示，这种方法可以实现快速适应，对于不同的摄像头配置，需要训练集数量有所减少。<details>
<summary>Abstract</summary>
This paper introduces a new approach for quickly adapting a multi-view visuomotor system for robots to varying camera configurations from the baseline setup. It utilises meta-learning to fine-tune the perceptual network while keeping the policy network fixed. Experimental results demonstrate a significant reduction in the number of new training episodes needed to attain baseline performance.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文提出了一种新的方法，用于快速适应多视图视motor系统 Robot 到不同的摄像头配置从基线设置。它利用meta-学习来精细化感知网络，保持政策网络不变。实验结果显示，可以快速适应新的摄像头配置，并达到基线性能。
</details></li>
</ul>
<hr>
<h2 id="Multi-Base-Station-Cooperative-Sensing-with-AI-Aided-Tracking"><a href="#Multi-Base-Station-Cooperative-Sensing-with-AI-Aided-Tracking" class="headerlink" title="Multi-Base Station Cooperative Sensing with AI-Aided Tracking"></a>Multi-Base Station Cooperative Sensing with AI-Aided Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20403">http://arxiv.org/abs/2310.20403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elia Favarelli, Elisabetta Matricardi, Lorenzo Pucci, Enrico Paolini, Wen Xu, Andrea Giorgetti</li>
<li>for: 这个论文研究了一种共同感知和通信（JSC）网络，该网络由多个基站（BS）合作，通过协调中心（FC）交换感知环境信息，同时与一组用户设备（UE）建立通信链接。</li>
<li>methods: 每个BS在网络中运行为单STATIC射频系统，实现了全面扫描监测区域，生成范围角图，提供监测区域中目标位置信息。图像被FC进行融合。然后，使用卷积神经网络（CNN）推断目标类别，例如人行或车辆，并将其信息用adaptive clustering算法组织更有效地。最后，采用潘德尔过滤器（PHD）和多 Bernoulli 混合过滤器（MBM）来跟踪目标。</li>
<li>results: numerical results表明，我们的框架可以提供很好的感知性能，实现了优化的子模式分配（OSPA）在60 cm之下，同时保持对UE的通信服务的减少，在10%-20%之间。此外，我们还研究了BS参与感知的数量的影响，并发现在specific case study中，3BS可以保持地理位置错误在1 m以下。<details>
<summary>Abstract</summary>
In this work, we investigate the performance of a joint sensing and communication (JSC) network consisting of multiple base stations (BSs) that cooperate through a fusion center (FC) to exchange information about the sensed environment while concurrently establishing communication links with a set of user equipments (UEs). Each BS within the network operates as a monostatic radar system, enabling comprehensive scanning of the monitored area and generating range-angle maps that provide information regarding the position of a group of heterogeneous objects. The acquired maps are subsequently fused in the FC. Then, a convolutional neural network (CNN) is employed to infer the category of the targets, e.g., pedestrians or vehicles, and such information is exploited by an adaptive clustering algorithm to group the detections originating from the same target more effectively. Finally, two multi-target tracking algorithms, the probability hypothesis density (PHD) filter and multi-Bernoulli mixture (MBM) filter, are applied to estimate the state of the targets. Numerical results demonstrated that our framework could provide remarkable sensing performance, achieving an optimal sub-pattern assignment (OSPA) less than 60 cm, while keeping communication services to UEs with a reduction of the communication capacity in the order of 10% to 20%. The impact of the number of BSs engaged in sensing is also examined, and we show that in the specific case study, 3 BSs ensure a localization error below 1 m.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究了一个共同探测和通信（JSC）网络，该网络包括多个基站（BS），通过协同中心（FC）来交换探测环境中的信息，同时与一组用户设备（UE）建立通信链接。每个BS在网络中 acted as a monostatic radar系统，可以完整扫描监测区域，生成距离角度图，提供目标位置信息。获得的图像被后续归一化神经网络（CNN）进行推理，并使用adaptive clustering算法将探测起始于同一个目标 grouped more effectively.最后，我们应用了两种多目标跟踪算法，概率 Hypothesis Density（PHD）筛选器和多 Bernoulli Mixture（MBM）筛选器，来估计目标的状态。numerical results表明，我们的框架可以提供很好的探测性能，达到优化子Pattern Assignment（OSPA）下than 60 cm，并在对UE的通信服务中减少了10%到20%的通信容量。我们还研究了BS参与探测的数量的影响，并显示在特定的案例研究中，3个BS可以保持Localization error below 1 m。
</details></li>
</ul>
<hr>
<h2 id="Utilitarian-Algorithm-Configuration"><a href="#Utilitarian-Algorithm-Configuration" class="headerlink" title="Utilitarian Algorithm Configuration"></a>Utilitarian Algorithm Configuration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20401">http://arxiv.org/abs/2310.20401</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/drgrhm/utilitarian-ac">https://github.com/drgrhm/utilitarian-ac</a></li>
<li>paper_authors: Devon R. Graham, Kevin Leyton-Brown, Tim Roughgarden</li>
<li>for: 这 paper 的目的是为了 Maximize the utility provided to their end users while also offering theoretical guarantees about performance.</li>
<li>methods: 这 paper 使用的方法是 configuring heuristic algorithms to maximize utility, while offering theoretical guarantees about performance.</li>
<li>results: 这 paper 的结果是提供了一种 theoretically sound 的配置方法，可以实现 empirical bounds on a configuration’s performance, while also demonstrating their performance empirically.<details>
<summary>Abstract</summary>
We present the first nontrivial procedure for configuring heuristic algorithms to maximize the utility provided to their end users while also offering theoretical guarantees about performance. Existing procedures seek configurations that minimize expected runtime. However, very recent theoretical work argues that expected runtime minimization fails to capture algorithm designers' preferences. Here we show that the utilitarian objective also confers significant algorithmic benefits. Intuitively, this is because mean runtime is dominated by extremely long runs even when they are incredibly rare; indeed, even when an algorithm never gives rise to such long runs, configuration procedures that provably minimize mean runtime must perform a huge number of experiments to demonstrate this fact. In contrast, utility is bounded and monotonically decreasing in runtime, allowing for meaningful empirical bounds on a configuration's performance. This paper builds on this idea to describe effective and theoretically sound configuration procedures. We prove upper bounds on the runtime of these procedures that are similar to theoretical lower bounds, while also demonstrating their performance empirically.
</details>
<details>
<summary>摘要</summary>
我们提出了第一个非负担的程序，用于配置假设运算法以最大化它们的终端用户所提供的用途，同时提供理论上的保证。现有的程序则寻找配置可以最小化预期的执行时间。然而，最近的理论工作表明，预期执行时间最小化不能捕捉算法设计师的偏好。我们展示了Utilitarian目标也具有重要的算法优点。实际上，平均执行时间被极端长的执行所控制，甚至当算法从不会产生这些极端长执行时，配置程序仍需进行大量的实验来证明这一点。相比之下，用途是有界的和下降的，可以提供有意义的实验上的上限。这篇论文基于这个想法，描述了有效且理论上正确的配置程序。我们证明了这些程序的执行时间的上限，与理论上的下限类似，同时也证明了它们的实验性能。
</details></li>
</ul>
<hr>
<h2 id="Do-large-language-models-solve-verbal-analogies-like-children-do"><a href="#Do-large-language-models-solve-verbal-analogies-like-children-do" class="headerlink" title="Do large language models solve verbal analogies like children do?"></a>Do large language models solve verbal analogies like children do?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20384">http://arxiv.org/abs/2310.20384</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cstevenson-uva/verbal_analogies_kids_vs_llms">https://github.com/cstevenson-uva/verbal_analogies_kids_vs_llms</a></li>
<li>paper_authors: Claire E. Stevenson, Mathilde ter Veen, Rochelle Choenni, Han L. J. van der Maas, Ekaterina Shutova</li>
<li>for:  investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do.</li>
<li>methods:  used verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 yearolds from the Netherlands solved 622 analogies in Dutch.</li>
<li>results:  the six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, around the 7-year-old level, and XLM-V and GPT-3 the best, slightly above the 11-year-old level. However, when controlling for associative processes, each model’s performance level drops 1-2 years.<details>
<summary>Abstract</summary>
Analogy-making lies at the heart of human cognition. Adults solve analogies such as \textit{Horse belongs to stable like chicken belongs to ...?} by mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In contrast, children often use association, e.g., answering \textit{egg}. This paper investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do. We use verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, around the 7-year-old level, and XLM-V and GPT-3 the best, slightly above the 11-year-old level. However, when we control for associative processes this picture changes and each model's performance level drops 1-2 years. Further experiments demonstrate that associative processes often underlie correctly solved analogies. We conclude that the LLMs we tested indeed tend to solve verbal analogies by association with C like children do.
</details>
<details>
<summary>摘要</summary>
人类认知中的比喻是非常重要的。大人们解决类似于“马属于牧场如果鸡属于...？”的比喻问题，通常是通过映射关系（如“保持在”）并回答“鸡巢”。然而，儿童们经常使用关联，例如回答“蛋”。这篇论文研究了大型自然语言模型（LLMs）是否使用关联来解决A:B::C:?的语言比喻问题。我们使用来自在线适应学习环境中的622个荷兰7-12岁儿童解决的语言比喻，并测试了6个荷兰单语和多语言LLMs。结果显示，这些模型在与儿童的性能水平相当，但当我们控制了相关的过程时，每个模型的性能均下降1-2年级。进一步的实验表明，关联过程经常是 correctly solved analogies 的基础。我们结论认为，我们测试的LLMs确实通过关联来解决语言比喻问题，类似于儿童一样。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Study-of-GPT-4V’s-Multimodal-Capabilities-in-Medical-Imaging"><a href="#A-Comprehensive-Study-of-GPT-4V’s-Multimodal-Capabilities-in-Medical-Imaging" class="headerlink" title="A Comprehensive Study of GPT-4V’s Multimodal Capabilities in Medical Imaging"></a>A Comprehensive Study of GPT-4V’s Multimodal Capabilities in Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20381">http://arxiv.org/abs/2310.20381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, Luping Zhou</li>
<li>for:  This paper evaluates the capabilities of GPT-4V in medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding.</li>
<li>methods:  The paper uses publicly available benchmarks to evaluate GPT-4V’s performance in these tasks, and the authors provide a comprehensive analysis of its strengths and limitations.</li>
<li>results:  GPT-4V demonstrates potential in generating descriptive reports for chest X-ray images, but its performance on certain evaluation metrics is lacking. It also shows promise in recognizing bounding boxes, but its precision is not high enough to identify specific medical organs and signs. The paper highlights the need for targeted refinements to fully unlock GPT-4V’s capabilities in the medical imaging domain.<details>
<summary>Abstract</summary>
This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of more semantically robust assessment methods. In the field of Visual Grounding, GPT-4V exhibits preliminary promise in recognizing bounding boxes, but its precision is lacking, especially in identifying specific medical organs and signs. Our evaluation underscores the significant potential of GPT-4V in the medical imaging domain, while also emphasizing the need for targeted refinements to fully unlock its capabilities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Machine-Learning-Based-Framework-for-Clustering-Residential-Electricity-Load-Profiles-to-Enhance-Demand-Response-Programs"><a href="#A-Machine-Learning-Based-Framework-for-Clustering-Residential-Electricity-Load-Profiles-to-Enhance-Demand-Response-Programs" class="headerlink" title="A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs"></a>A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20367">http://arxiv.org/abs/2310.20367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilis Michalakopoulos, Elissaios Sarmas, Ioannis Papias, Panagiotis Skaloumpakas, Vangelis Marinakis, Haris Doukas</li>
<li>for: 本研究旨在提供一种优化的加载 profiling 方法，以便更好地分析家庭每日能源消耗模式，特别是在应用 Demand Response (DR) 领域。</li>
<li>methods: 本研究使用了四种常用的聚类算法，包括 K-means、K-medoids、 Hierarchical Agglomerative Clustering 和 Density-based Spatial Clustering，并通过实际案例研究了这些算法的效果。</li>
<li>results: 研究结果显示，使用 K-means 算法可以得到最佳的聚类结果，但是对于这个案例来说，使用 Hierarchical Agglomerative Clustering 算法可以更好地捕捉到家庭的内部不同性。而且，通过使用 Explainable AI (xAI) 技术，我们可以提高方法的解释性和可读性。<details>
<summary>Abstract</summary>
Load shapes derived from smart meter data are frequently employed to analyze daily energy consumption patterns, particularly in the context of applications like Demand Response (DR). Nevertheless, one of the most important challenges to this endeavor lies in identifying the most suitable consumer clusters with similar consumption behaviors. In this paper, we present a novel machine learning based framework in order to achieve optimal load profiling through a real case study, utilizing data from almost 5000 households in London. Four widely used clustering algorithms are applied specifically K-means, K-medoids, Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An empirical analysis as well as multiple evaluation metrics are leveraged to assess those algorithms. Following that, we redefine the problem as a probabilistic classification one, with the classifier emulating the behavior of a clustering algorithm,leveraging Explainable AI (xAI) to enhance the interpretability of our solution. According to the clustering algorithm analysis the optimal number of clusters for this case is seven. Despite that, our methodology shows that two of the clusters, almost 10\% of the dataset, exhibit significant internal dissimilarity and thus it splits them even further to create nine clusters in total. The scalability and versatility of our solution makes it an ideal choice for power utility companies aiming to segment their users for creating more targeted Demand Response programs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>发现形状来自智能仪表数据的分析是常用于日常能源消耗模式分析，特别在需求回应（DR）应用场景中。然而， Identifying the most suitable consumer clusters with similar consumption behaviors is one of the biggest challenges in this field. 在这篇论文中，我们提出了一种基于机器学习的新框架，通过实际案例研究，使用伦敦约5000户数据，以实现最佳的加载规划。我们运用了四种常用的划分算法，包括K-means、K-medoids、层次归一整合划分和密度空间划分。通过实际分析和多种评价指标，我们评估了这些算法。然后，我们将问题重新定义为一个 probabilistic 分类问题，通过类似于划分算法的类ifier来模拟，并使用 Explainable AI（xAI）来增强解释性。根据划分算法分析，这个案例的最佳数量为7个划分。然而，我们的方法显示，这7个划分中的2个划分（约10%的数据）具有显著的内部不一致，因此我们将其进一步分割，创造了9个划分。我们的解决方案具有扩展性和多样性，使得它成为了电力公司为实现更加准确的需求回应计划而选择的理想选择。
</details></li>
</ul>
<hr>
<h2 id="Mathematical-Introduction-to-Deep-Learning-Methods-Implementations-and-Theory"><a href="#Mathematical-Introduction-to-Deep-Learning-Methods-Implementations-and-Theory" class="headerlink" title="Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory"></a>Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20360">http://arxiv.org/abs/2310.20360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/introdeeplearning/book">https://github.com/introdeeplearning/book</a></li>
<li>paper_authors: Arnulf Jentzen, Benno Kuckuck, Philippe von Wurstemberger</li>
<li>for: This book provides an introduction to deep learning algorithms, covering essential components such as ANN architectures and optimization algorithms, as well as theoretical aspects like approximation capacities and generalization errors.</li>
<li>methods: The book reviews various deep learning algorithms, including fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization. It also covers optimization algorithms such as basic SGD, accelerated methods, and adaptive methods.</li>
<li>results: The book provides a solid foundation for students and scientists who are new to deep learning, and offers a firmer mathematical understanding of deep learning objects and methods for practitioners. Additionally, it reviews deep learning approximation methods for PDEs, including physics-informed neural networks (PINNs) and deep Galerkin methods.<details>
<summary>Abstract</summary>
This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet have any background in deep learning at all and would like to gain a solid foundation as well as for practitioners who would like to obtain a firmer mathematical understanding of the objects and methods considered in deep learning.
</details>
<details>
<summary>摘要</summary>
这本书的目标是为深度学习算法提供一个入门。我们详细介绍了深度学习算法的重要组成部分，包括不同的人工神经网络架构（如全连接Feedforward ANNs、卷积 ANNs、回归 ANNs、差分 ANNs 和批量 норmalization ANNs）以及不同的优化算法（如基本的随机梯度下降法、加速方法和自适应方法）。我们还覆盖了深度学习算法的一些理论方面，包括人工神经网络的近似能力（包括神经网络 Calculus）、优化理论（包括库德雅-{\L}ojasiewicz不等式）和泛化误差。在书的最后部分，我们也详细介绍了一些深度学习方法用于解析 partial differential equations（PDEs），包括物理学信息学神经网络（PINNs）和深度 Galerkin 方法。我们希望这本书能为没有深度学习背景的学生和科学家提供坚实的基础，同时也能为已有深度学习背景的实践者提供更加坚实的数学理解。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-Spatial-Awareness-Capability-of-Multi-Modal-Large-Language-Model"><a href="#Enhancing-the-Spatial-Awareness-Capability-of-Multi-Modal-Large-Language-Model" class="headerlink" title="Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model"></a>Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20357">http://arxiv.org/abs/2310.20357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqiang Zhao, Zhenyu Li, Zhi Jin, Feng Zhang, Haiyan Zhao, Chengfeng Dou, Zhengwei Tao, Xinhai Xu, Donghong Liu</li>
<li>for: 该论文旨在提高多modal大语言模型（MLLM）的空间意识能力，以满足现代应用需求。</li>
<li>methods: 该论文提出使用更精确的空间位置信息来引导MLLM提供更加准确的回答。 Specifically, 该论文使用算法获取多modal数据中对象之间的几何空间信息和场景图来提高MLLM的空间意识能力。</li>
<li>results: 实验结果表明，该方法可以有效地提高MLLM在空间意识相关任务上的表现。<details>
<summary>Abstract</summary>
The Multi-Modal Large Language Model (MLLM) refers to an extension of the Large Language Model (LLM) equipped with the capability to receive and infer multi-modal data. Spatial awareness stands as one of the crucial abilities of MLLM, encompassing diverse skills related to understanding spatial relationships among objects and between objects and the scene area. Industries such as autonomous driving, smart healthcare, robotics, virtual, and augmented reality heavily demand MLLM's spatial awareness capabilities. However, there exists a noticeable gap between the current spatial awareness capabilities of MLLM and the requirements set by human needs. To address this issue, this paper proposes using more precise spatial position information between objects to guide MLLM in providing more accurate responses to user-related inquiries. Specifically, for a particular multi-modal task, we utilize algorithms for acquiring geometric spatial information and scene graphs to obtain relevant geometric spatial information and scene details of objects involved in the query. Subsequently, based on this information, we direct MLLM to address spatial awareness-related queries posed by the user. Extensive experiments were conducted in benchmarks such as MME, MM-Vet, and other multi-modal large language models. The experimental results thoroughly confirm the efficacy of the proposed method in enhancing the spatial awareness tasks and associated tasks of MLLM.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文翻译，不同于标准中文） Multi-Modal Large Language Model (MLLM) 是 Large Language Model (LLM) 的扩展，具有接收和理解多Modal数据的能力。MLLM 的空间意识是其关键能力之一，包括理解物体之间和物体与场景区域之间的空间关系。自动驾驶、智能医疗、机器人、虚拟和扩展现实等行业强烈需要 MLLM 的空间意识能力。然而，目前 MLLM 的空间意识能力与人类需求之间存在显著的差距。为解决这一问题，本文提议使用更精准的物体间空间位置信息来导引 MLLM 更准确地回答用户关注的问题。Specifically, for a particular multi-modal task, we utilize algorithms for acquiring geometric spatial information and scene graphs to obtain relevant geometric spatial information and scene details of objects involved in the query. Subsequently, based on this information, we direct MLLM to address spatial awareness-related queries posed by the user. We conducted extensive experiments in benchmarks such as MME, MM-Vet, and other multi-modal large language models. The experimental results thoroughly confirm the efficacy of the proposed method in enhancing the spatial awareness tasks and associated tasks of MLLM.
</details></li>
</ul>
<hr>
<h2 id="Muscle-volume-quantification-guiding-transformers-with-anatomical-priors"><a href="#Muscle-volume-quantification-guiding-transformers-with-anatomical-priors" class="headerlink" title="Muscle volume quantification: guiding transformers with anatomical priors"></a>Muscle volume quantification: guiding transformers with anatomical priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20355">http://arxiv.org/abs/2310.20355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louise Piecuch, Vanessa Gonzales Duque, Aurélie Sarcher, Enzo Hollville, Antoine Nordez, Giuseppe Rabita, Gaël Guilhem, Diana Mateus</li>
<li>for: 这个研究是为了提供一种自动标识股筋的方法，帮助运动科学和骨骼疾病追踪中的形态分析。</li>
<li>methods: 这篇研究使用了一种混合构架，结合了卷积构架和视觉trasformer构架，以便自动标识股筋。另外，这篇研究还添加了一个预测损失基于股筋邻里关系矩阵，以利用骨骼组织的关系来提高预测的精度。</li>
<li>results: 实验结果显示，这种混合构架可以从相对较小的数据库中训练出高精度的模型，而且运用骨骼组织的关系来加权预测，可以提高模型的预测精度。<details>
<summary>Abstract</summary>
Muscle volume is a useful quantitative biomarker in sports, but also for the follow-up of degenerative musculo-skelletal diseases. In addition to volume, other shape biomarkers can be extracted by segmenting the muscles of interest from medical images. Manual segmentation is still today the gold standard for such measurements despite being very time-consuming. We propose a method for automatic segmentation of 18 muscles of the lower limb on 3D Magnetic Resonance Images to assist such morphometric analysis. By their nature, the tissue of different muscles is undistinguishable when observed in MR Images. Thus, muscle segmentation algorithms cannot rely on appearance but only on contour cues. However, such contours are hard to detect and their thickness varies across subjects. To cope with the above challenges, we propose a segmentation approach based on a hybrid architecture, combining convolutional and visual transformer blocks. We investigate for the first time the behaviour of such hybrid architectures in the context of muscle segmentation for shape analysis. Considering the consistent anatomical muscle configuration, we rely on transformer blocks to capture the longrange relations between the muscles. To further exploit the anatomical priors, a second contribution of this work consists in adding a regularisation loss based on an adjacency matrix of plausible muscle neighbourhoods estimated from the training data. Our experimental results on a unique database of elite athletes show it is possible to train complex hybrid models from a relatively small database of large volumes, while the anatomical prior regularisation favours better predictions.
</details>
<details>
<summary>摘要</summary>
筋肉体积是运动领域中有用的量化生物标志，同时也适用于评估逐渐恶化的肌骨综合病。除了体积，还可以从医疗影像中提取其他形状生物标志。现今，手动分割仍然是评估这些测量的标准方法，尽管它很时间consuming。我们提议一种自动分割3D磁共振图像中18个下肢肌肉的方法，以助于这种形态分析。由于不同肌肉组织的组织结构是不可分辨的，因此肌肉分割算法不能仅仅基于观察的外观特征。然而，这些边缘是困难检测的，并且对不同主题者而言，边缘的厚度异常。为了解决以上挑战，我们提议一种混合架构，结合卷积和视觉转换块。我们在肌肉分割方面首次研究了这种混合架构的行为。由于肌肉的一致性结构，我们利用转换块来捕捉肌肉之间的长距离关系。此外，我们还添加了一种基于邻居矩阵的准确性损失，以利用训练数据中的 анатомиче priors。我们在一个独特的运动员数据库中进行了实验，并证明了可以从相对较小的数据库中训练复杂的混合模型，而且准确性增加。
</details></li>
</ul>
<hr>
<h2 id="Combining-Shape-Completion-and-Grasp-Prediction-for-Fast-and-Versatile-Grasping-with-a-Multi-Fingered-Hand"><a href="#Combining-Shape-Completion-and-Grasp-Prediction-for-Fast-and-Versatile-Grasping-with-a-Multi-Fingered-Hand" class="headerlink" title="Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand"></a>Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20350">http://arxiv.org/abs/2310.20350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Humt, Dominik Winkelbauer, Ulrich Hillenbrand, Berthold Bäuml</li>
<li>for: 本研究旨在解决助助器中夹取物体时的问题，特别是在部分可见和多指手部上进行多种夹取。</li>
<li>methods: 该研究提出了一种新的深度学习管道，包括一个基于单个深度图像的形态完成模块和基于预测物体形态的抓取预测器。形态完成网络基于VQDIF，并预测了空间占据值在任意查询点。抓取预测器使用了两stage的架构，首先使用了自然语言模型生成手势，然后对每个姿势进行了指关键点的回归。</li>
<li>results: 实验表明，该管道在物理机器人平台上成功地夹取了各种家用物品，基于单个视角的深度图像。整个管道快速，只需要约1秒完成物体形态预测（0.7秒）和生成1000个抓取（0.3秒）。<details>
<summary>Abstract</summary>
Grasping objects with limited or no prior knowledge about them is a highly relevant skill in assistive robotics. Still, in this general setting, it has remained an open problem, especially when it comes to only partial observability and versatile grasping with multi-fingered hands. We present a novel, fast, and high fidelity deep learning pipeline consisting of a shape completion module that is based on a single depth image, and followed by a grasp predictor that is based on the predicted object shape. The shape completion network is based on VQDIF and predicts spatial occupancy values at arbitrary query points. As grasp predictor, we use our two-stage architecture that first generates hand poses using an autoregressive model and then regresses finger joint configurations per pose. Critical factors turn out to be sufficient data realism and augmentation, as well as special attention to difficult cases during training. Experiments on a physical robot platform demonstrate successful grasping of a wide range of household objects based on a depth image from a single viewpoint. The whole pipeline is fast, taking only about 1 s for completing the object's shape (0.7 s) and generating 1000 grasps (0.3 s).
</details>
<details>
<summary>摘要</summary>
握住无知物品是助手机器人中的一项非常有用的技能，但在这个通用设定下，这问题仍然是一个开问题，特别是在仅部分可观察和多指手臂中进行握住。我们提出了一个新的、快速和高精度的深度学习管线，包括基于单一深度图像的形状完成模组，以及基于预测物体形状的握住预测器。形状完成网络是基于VQDIF的，并预测在任意查询点的空间占用值。握住预测器使用我们的二阶段架构，首先使用autoregressive模型生成手势，然后对每个手势进行指骨配置的回归。实验在物理机器人平台上显示，成功握住了一系列家用物品，基于单一观点的深度图像。整个管线快速，只需0.7秒完成物体形状预测，以及0.3秒生成1000个握住。
</details></li>
</ul>
<hr>
<h2 id="Improving-Entropy-Based-Test-Time-Adaptation-from-a-Clustering-View"><a href="#Improving-Entropy-Based-Test-Time-Adaptation-from-a-Clustering-View" class="headerlink" title="Improving Entropy-Based Test-Time Adaptation from a Clustering View"></a>Improving Entropy-Based Test-Time Adaptation from a Clustering View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20327">http://arxiv.org/abs/2310.20327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoliang Lin, Hanjiang Lai, Yan Pan, Jian Yin</li>
<li>for: 本研究旨在解决预测数据和测试数据之间的频率差异问题，通过全部测试时间适应（TTA）技术来适应模型。</li>
<li>methods: 本研究提出了一种新的Entropy-Based TTA（EBTTA）方法的解释，即通过对测试样本进行分类来实现。这是一个迭代的算法，其中在分配步骤中，EBTTA模型的前进过程是对测试样本进行标注，而在更新步骤中，是通过分配的样本来更新模型。</li>
<li>results: 实验结果表明，我们的方法可以在多个数据集上实现稳定的改进。我们还发现了EBTTA方法的敏感性问题，包括初始分配、异常样本和批处理大小的影响，这些问题的解决可以提高EBTTA的性能。我们提出了一些改进方法，包括稳定的标注、重量调整和梯度积累。<details>
<summary>Abstract</summary>
Domain shift is a common problem in the realistic world, where training data and test data follow different data distributions. To deal with this problem, fully test-time adaptation (TTA) leverages the unlabeled data encountered during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA) methods, which minimize the prediction's entropy on test samples, have shown great success. In this paper, we introduce a new perspective on the EBTTA, which interprets these methods from a view of clustering. It is an iterative algorithm: 1) in the assignment step, the forward process of the EBTTA models is the assignment of labels for these test samples, and 2) in the updating step, the backward process is the update of the model via the assigned samples. Based on the interpretation, we can gain a deeper understanding of EBTTA, where we show that the entropy loss would further increase the largest probability. Accordingly, we offer an alternative explanation that why existing EBTTA methods are sensitive to initial assignments, outliers, and batch size. This observation can guide us to put forward the improvement of EBTTA. We propose robust label assignment, weight adjustment, and gradient accumulation to alleviate the above problems. Experimental results demonstrate that our method can achieve consistent improvements on various datasets. Code is provided in the supplementary material.
</details>
<details>
<summary>摘要</summary>
域外适应是现实世界中的一个常见问题，训练数据和测试数据之间存在不同的数据分布。为解决这个问题，全部测试时适应（TTA）利用测试时遇到的无标签数据来适应模型。特别是基于Entropy的TTA（EBTTA）方法，通过减小测试样本中预测的熵来显示出色。在这篇论文中，我们提出了一新的视角，即EBTTA方法的解释，它可以看作是一种归一化过程。该过程包括两步：1）在分配步骤，EBTTA模型的前进过程是对测试样本进行标签分配; 2）在更新步骤，后进过程是通过分配的样本来更新模型。基于这种解释，我们可以更深入地理解EBTTA，并证明了熵损失会使largest probability增加。根据这种观察，我们提出了一些改进EBTTA的方法，包括 Robust label assignment、重量调整和Gradient accumulation，以解决初始分配、异常样本和批处理大小对EBTTA的敏感性问题。实验结果表明，我们的方法可以在多个 dataset 上实现一致性的改进。代码请参考辅助材料。
</details></li>
</ul>
<hr>
<h2 id="SemanticBoost-Elevating-Motion-Generation-with-Augmented-Textual-Cues"><a href="#SemanticBoost-Elevating-Motion-Generation-with-Augmented-Textual-Cues" class="headerlink" title="SemanticBoost: Elevating Motion Generation with Augmented Textual Cues"></a>SemanticBoost: Elevating Motion Generation with Augmented Textual Cues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20323">http://arxiv.org/abs/2310.20323</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/blackgold3/SemanticBoost">https://github.com/blackgold3/SemanticBoost</a></li>
<li>paper_authors: Xin He, Shaoli Huang, Xiaohang Zhan, Chao Wen, Ying Shan</li>
<li>For:	+ The paper aims to address the challenges of generating motions from intricate semantic descriptions in human motion capture datasets.	+ The paper presents a novel framework called SemanticBoost to improve the quality and consistency of generated motions.* Methods:	+ The framework consists of a Semantic Enhancement module and a Context-Attuned Motion Denoiser (CAMD).	+ The Semantic Enhancement module extracts supplementary semantics from motion data to enrich the dataset’s textual description.	+ The CAMD approach captures context information and aligns the generated motion with the given textual descriptions.* Results:	+ The proposed method outperforms auto-regressive-based techniques on the Humanml3D dataset, achieving cutting-edge performance while maintaining realistic and smooth motion generation quality.	+ The method is able to synthesize accurate orientational movements, combined motions based on specific body part descriptions, and motions generated from complex, extended sentences.<details>
<summary>Abstract</summary>
Current techniques face difficulties in generating motions from intricate semantic descriptions, primarily due to insufficient semantic annotations in datasets and weak contextual understanding. To address these issues, we present SemanticBoost, a novel framework that tackles both challenges simultaneously. Our framework comprises a Semantic Enhancement module and a Context-Attuned Motion Denoiser (CAMD). The Semantic Enhancement module extracts supplementary semantics from motion data, enriching the dataset's textual description and ensuring precise alignment between text and motion data without depending on large language models. On the other hand, the CAMD approach provides an all-encompassing solution for generating high-quality, semantically consistent motion sequences by effectively capturing context information and aligning the generated motion with the given textual descriptions. Distinct from existing methods, our approach can synthesize accurate orientational movements, combined motions based on specific body part descriptions, and motions generated from complex, extended sentences. Our experimental results demonstrate that SemanticBoost, as a diffusion-based method, outperforms auto-regressive-based techniques, achieving cutting-edge performance on the Humanml3D dataset while maintaining realistic and smooth motion generation quality.
</details>
<details>
<summary>摘要</summary>
当前技术遇到困难在基于复杂 semantic 描述生成动作，主要原因是数据集的 semantic 注解不够和Contextual 理解不强。为解决这些问题，我们提出 SemanticBoost 框架，同时解决这两个问题。我们的框架包括 Semantic Enhancement 模块和 Context-Attuned Motion Denoiser (CAMD)。Semantic Enhancement 模块从动作数据中提取补充 semantics，使 dataset 的文本描述更加丰富，确保文本和动作数据的准确对应，不需要依赖大型语言模型。而 CAMD 方法提供了一个涵盖全Solution для生成高质量、semantically 一致的动作序列，通过有效地捕捉Context信息和文本描述之间的对应关系，生成高质量的动作序列。与现有方法不同，我们的方法可以合成准确的方向运动、基于specific body part 描述的 combined 动作和从复杂、扩展的 sentence 中生成动作。我们的实验结果表明，SemanticBoost 作为一种diffusion-based 方法，在 Humanml3D 数据集上超越 auto-regressive-based 技术，实现了当今最高的性能，同时保持实际和平滑的动作生成质量。
</details></li>
</ul>
<hr>
<h2 id="Theory-of-Mind-in-Large-Language-Models-Examining-Performance-of-11-State-of-the-Art-models-vs-Children-Aged-7-10-on-Advanced-Tests"><a href="#Theory-of-Mind-in-Large-Language-Models-Examining-Performance-of-11-State-of-the-Art-models-vs-Children-Aged-7-10-on-Advanced-Tests" class="headerlink" title="Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests"></a>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20320">http://arxiv.org/abs/2310.20320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max J. van Duijn, Bram M. A. van Dijk, Tom Kouwenhoven, Werner de Valk, Marco R. Spruit, Peter van der Putten</li>
<li>for: 这篇论文是关于大自然语言模型（LLM）的认知能力如何归结的。</li>
<li>methods: 该论文使用了多种方法来测试LLM的认知能力，包括测试其能够理解非直接语言使用和回归意图等。</li>
<li>results: 研究发现，基于GPT家族的指导 LLM 在 ToM 任务中表现出色，常常也比儿童年龄7-10岁的儿童更好。基础 LLM 则在大多数情况下无法解决 ToM 任务，即使使用特殊的提示。研究人员认为，语言和 ToM 的演进和发展可能帮助解释了 instruciton-tuning 添加了什么：奖励合作交流，考虑到对方和情况。<details>
<summary>Abstract</summary>
To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.
</details>
<details>
<summary>摘要</summary>
Should we attribute cognitive abilities such as theory of mind (ToM) to large language models (LLMs)? We add to this emerging debate by testing 11 base- and instruction-tuned LLMs on ToM-related capabilities beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality. We also use newly rewritten versions of standardized tests to gauge LLMs' robustness, prompt for open-ended questions, and benchmark LLM performance against that of children aged 7-10 on the same tasks. Our findings show that instruction-tuned LLMs from the GPT family outperform other models and often also children, while base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may explain what instruction-tuning adds, as it rewards cooperative communication that takes into account interlocutor and context. We conclude by advocating for a nuanced perspective on ToM in LLMs.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Causal-Interpretation-of-Self-Attention-in-Pre-Trained-Transformers"><a href="#Causal-Interpretation-of-Self-Attention-in-Pre-Trained-Transformers" class="headerlink" title="Causal Interpretation of Self-Attention in Pre-Trained Transformers"></a>Causal Interpretation of Self-Attention in Pre-Trained Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20307">http://arxiv.org/abs/2310.20307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raanan Y. Rohekar, Yaniv Gurwicz, Shami Nisimov</li>
<li>for: 本研究旨在解释Transformer神经网络中的自注意力是什么？</li>
<li>methods: 本研究使用自注意力作为一种结构方程模型来解释输入序列中的符号（Token）之间的关系。</li>
<li>results: 本研究发现，通过计算深度注意层中表示的偏好关系，可以学习输入序列中的 causal 结构，并且在存在潜在干扰因素的情况下仍然有效。<details>
<summary>Abstract</summary>
We propose a causal interpretation of self-attention in the Transformer neural network architecture. We interpret self-attention as a mechanism that estimates a structural equation model for a given input sequence of symbols (tokens). The structural equation model can be interpreted, in turn, as a causal structure over the input symbols under the specific context of the input sequence. Importantly, this interpretation remains valid in the presence of latent confounders. Following this interpretation, we estimate conditional independence relations between input symbols by calculating partial correlations between their corresponding representations in the deepest attention layer. This enables learning the causal structure over an input sequence using existing constraint-based algorithms. In this sense, existing pre-trained Transformers can be utilized for zero-shot causal-discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks: sentiment classification (NLP) and recommendation.
</details>
<details>
<summary>摘要</summary>
我们提出了 transformer 神经网络架构中 self-attention 的 causal 解释。我们解释 self-attention 为一种可以 estimate 输入序列符号（token）的 struc-tural equation model。这个 struc-tural equation model 可以被解释为输入序列的特定上下文中的 causal 结构。这种解释在干扰因素存在时仍然有效。我们通过计算深度 attended layer 中表示之间的偏相关性来确定输入符号之间的 conditional independence 关系。这使得可以使用现有的 constraint-based 算法学习输入序列的 causal 结构。在这种意义上，现有的预训练 transformer 可以用于零工作 causal-discovery。我们在两个任务中提供了 causal 解释：语 Sentiment 分类（NLP）和推荐。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Global-Food-Security-Empowering-Resilience-through-Integrated-AI-Foundation-Models-and-Data-Driven-Solutions"><a href="#Revolutionizing-Global-Food-Security-Empowering-Resilience-through-Integrated-AI-Foundation-Models-and-Data-Driven-Solutions" class="headerlink" title="Revolutionizing Global Food Security: Empowering Resilience through Integrated AI Foundation Models and Data-Driven Solutions"></a>Revolutionizing Global Food Security: Empowering Resilience through Integrated AI Foundation Models and Data-Driven Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20301">http://arxiv.org/abs/2310.20301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed R. Shoaib, Heba M. Emara, Jun Zhao</li>
<li>for: 这个论文旨在探讨用AI基础模型在不同的食品安全应用中进行集成，以超越现有的深度学习和机器学习方法的限制。</li>
<li>methods: 该论文 investigate AI基础模型在农作物类型识别、农田地图、田野分界和农作物产量预测等领域的应用，并利用不同的数据类型，如多spectral镜像、气象数据、土壤特性、历史记录和高分辨率卫星镜像。</li>
<li>results: 该论文显示AI基础模型可以增强食品安全 iniciatives的准确预测、改善资源配置和支持 Informed Decision-making，并且这些模型作为一种可transformative force，为全球食品安全带来了重大的进步。<details>
<summary>Abstract</summary>
Food security, a global concern, necessitates precise and diverse data-driven solutions to address its multifaceted challenges. This paper explores the integration of AI foundation models across various food security applications, leveraging distinct data types, to overcome the limitations of current deep and machine learning methods. Specifically, we investigate their utilization in crop type mapping, cropland mapping, field delineation and crop yield prediction. By capitalizing on multispectral imagery, meteorological data, soil properties, historical records, and high-resolution satellite imagery, AI foundation models offer a versatile approach. The study demonstrates that AI foundation models enhance food security initiatives by providing accurate predictions, improving resource allocation, and supporting informed decision-making. These models serve as a transformative force in addressing global food security limitations, marking a significant leap toward a sustainable and secure food future.
</details>
<details>
<summary>摘要</summary>
食品安全问题是全球性的担忧，需要精准和多样化的数据驱动解决方案来 Address its 多方面的挑战。这篇论文探讨了基于 AI 基础模型的不同数据类型的 integrate 在不同的食品安全应用中，以超越现有的深度学习和机器学习方法的局限性。具体来说，我们调查了它们在作物类型地图、耕地地图、田间分界和作物产量预测中的应用。通过利用多spectral 影像、气象数据、土壤属性、历史记录和高分辨率卫星影像，AI 基础模型提供了一种多样化的方法。这项研究显示，AI 基础模型可以增强食品安全倡议，提供准确预测、改善资源分配和支持 Informed 决策。这些模型作为一种转型力量，帮助解决全球食品安全的限制，marking a significant leap toward a sustainable and secure food future.
</details></li>
</ul>
<hr>
<h2 id="Sample-Efficient-and-Safe-Deep-Reinforcement-Learning-via-Reset-Deep-Ensemble-Agents"><a href="#Sample-Efficient-and-Safe-Deep-Reinforcement-Learning-via-Reset-Deep-Ensemble-Agents" class="headerlink" title="Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents"></a>Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20287">http://arxiv.org/abs/2310.20287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Woojun Kim, Yongjae Shin, Jongeui Park, Youngchul Sung</li>
<li>for: 解决深度强化学习中的 primacy bias 问题，提高样本效率和安全性。</li>
<li>methods: 使用 periodic reset 方法，并将 ensemble learning 应用于深度RL agents。</li>
<li>results: 在不同的域中进行了诸多实验，并得到了高样本效率和安全性的数据。<details>
<summary>Abstract</summary>
Deep reinforcement learning (RL) has achieved remarkable success in solving complex tasks through its integration with deep neural networks (DNNs) as function approximators. However, the reliance on DNNs has introduced a new challenge called primacy bias, whereby these function approximators tend to prioritize early experiences, leading to overfitting. To mitigate this primacy bias, a reset method has been proposed, which performs periodic resets of a portion or the entirety of a deep RL agent while preserving the replay buffer. However, the use of the reset method can result in performance collapses after executing the reset, which can be detrimental from the perspective of safe RL and regret minimization. In this paper, we propose a new reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The proposed method is evaluated through various experiments including those in the domain of safe RL. Numerical results show its effectiveness in high sample efficiency and safety considerations.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a new reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The proposed method is evaluated through various experiments, including those in the domain of safe RL. The results show that our method is effective in achieving high sample efficiency and safety considerations.
</details></li>
</ul>
<hr>
<h2 id="AutoMixer-for-Improved-Multivariate-Time-Series-Forecasting-on-BizITOps-Data"><a href="#AutoMixer-for-Improved-Multivariate-Time-Series-Forecasting-on-BizITOps-Data" class="headerlink" title="AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data"></a>AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20280">http://arxiv.org/abs/2310.20280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santosh Palaskar, Vijay Ekambaram, Arindam Jati, Neelamadhav Gantayat, Avirup Saha, Seema Nagar, Nam H. Nguyen, Pankaj Dayama, Renuka Sindhgatta, Prateeti Mohapatra, Harshit Kumar, Jayant Kalagnanam, Nandyala Hemachandra, Narayan Rangaraj</li>
<li>for: 提高企业业务过程效率和收益，预测业务键性指标（Biz-KPIs）以提供反应式纠正措施。</li>
<li>methods:  combining Biz-KPIs and IT event channels as multivariate time series data，采用AutoMixer模型，包括预处理和精度提高两个部分。</li>
<li>results:  comparing with现有多变量预测模型，AutoMixer模型可以提高Biz-KPIs预测精度（11-15%），提供更加准确的业务预测和反应式纠正措施。<details>
<summary>Abstract</summary>
The efficiency of business processes relies on business key performance indicators (Biz-KPIs), that can be negatively impacted by IT failures. BizITOps data fuses both Biz-KPIs and IT event channels together as multivariate time series data. Forecasting Biz-KPIs in advance can enhance efficiency and revenue through proactive corrective measures. However, BizITOps data generally exhibit both useful and noisy inter-channel interactions between Biz-KPIs and IT events that need to be effectively decoupled. This leads to suboptimal forecasting performance when existing multivariate forecasting models are employed. To address this, we introduce AutoMixer, a time-series Foundation Model (FM) approach, grounded on the novel technique of channel-compressed pretrain and finetune workflows. AutoMixer leverages an AutoEncoder for channel-compressed pretraining and integrates it with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSMixer for accurate forecasts and also generalizes well across several downstream tasks. Through detailed experiments and dashboard analytics, we show AutoMixer's capability to consistently improve the Biz-KPI's forecasting accuracy (by 11-15%) which directly translates to actionable business insights.
</details>
<details>
<summary>摘要</summary>
To address this, we propose AutoMixer, a time-series Foundation Model (FM) approach that leverages a novel technique of channel-compressed pretraining and finetuning workflows. AutoMixer combines an AutoEncoder for channel-compressed pretraining with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSMixer for accurate forecasts and also generalizes well across several downstream tasks.Through detailed experiments and dashboard analytics, we demonstrate that AutoMixer consistently improves the Biz-KPIs forecasting accuracy by 11-15%, which directly translates to actionable business insights.
</details></li>
</ul>
<hr>
<h2 id="Constructing-Sample-to-Class-Graph-for-Few-Shot-Class-Incremental-Learning"><a href="#Constructing-Sample-to-Class-Graph-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning"></a>Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20268">http://arxiv.org/abs/2310.20268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuyuan Hu, Jian Zhang, Fan Lyu, Linyan Li, Fenglei Xu</li>
<li>for: 这 paper 的目的是提出一种 few-shot class-incremental learning (FSCIL) 方法，以帮助机器学习模型在很少数据样本的情况下不断学习新的概念，而不会忘记过去的类。</li>
<li>methods: 这 paper 提出了一种 Sample-to-Class (S2C) 图学习方法，包括 Sample-level Graph Network (SGN) 和 Class-level Graph Network (CGN) 两部分。SGN 专门分析单个会话中的样本关系，以提取更加精细的类级特征。CGN 则建立了不同会话之间的类级特征之间的连接，以帮助提高 FSCIL 中的总体学习。</li>
<li>results:  experiments 表明， compared to baselines, 这 paper 的方法在三个popular benchmark dataset上 clearly outperforms，并在 FSCIL 中设置了新的州对数据。<details>
<summary>Abstract</summary>
Few-shot class-incremental learning (FSCIL) aims to build machine learning model that can continually learn new concepts from a few data samples, without forgetting knowledge of old classes.   The challenges of FSCIL lies in the limited data of new classes, which not only lead to significant overfitting issues but also exacerbates the notorious catastrophic forgetting problems. As proved in early studies, building sample relationships is beneficial for learning from few-shot samples. In this paper, we promote the idea to the incremental scenario, and propose a Sample-to-Class (S2C) graph learning method for FSCIL.   Specifically, we propose a Sample-level Graph Network (SGN) that focuses on analyzing sample relationships within a single session. This network helps aggregate similar samples, ultimately leading to the extraction of more refined class-level features.   Then, we present a Class-level Graph Network (CGN) that establishes connections across class-level features of both new and old classes. This network plays a crucial role in linking the knowledge between different sessions and helps improve overall learning in the FSCIL scenario. Moreover, we design a multi-stage strategy for training S2C model, which mitigates the training challenges posed by limited data in the incremental process.   The multi-stage training strategy is designed to build S2C graph from base to few-shot stages, and improve the capacity via an extra pseudo-incremental stage. Experiments on three popular benchmark datasets show that our method clearly outperforms the baselines and sets new state-of-the-art results in FSCIL.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了几步骤增加学习（Few-shot Class-incremental Learning，FSCIL），它的目标是建立一个机器学习模型，可以从几个数据样本中学习新的概念，而不会忘记旧的类别知识。这种情况下的挑战在于新类别的数据有限，这不仅会导致严重过滤问题，而且会导致著名的忘却问题。在这篇论文中，我们提出了将样本关系建立到增量情况中的想法，并提出了一个称为 Sample-to-Class（S2C）几步骤学习方法。具体来说，我们提出了一个称为 Sample-level Graph Network（SGN）的网络，它专注于一个Session中的样本关系分析。这个网络可以将相似的样本集合起来，以将更精确的类别特征提取出来。然后，我们提出了一个称为 Class-level Graph Network（CGN）的网络，它建立了不同Session中的类别特征之间的连接。这个网络在连接不同Session中的知识之间，对于FSCIL情况下的整体学习起到了关键的作用。此外，我们设计了一个多阶段训练策略，以解决增量过程中的训练挑战。这个多阶段训练策略是建立S2C几步骤网络从基础阶段到几步骤阶段，并通过额外的pseudo-增量阶段进行优化。实验结果显示，我们的方法明显超过基eline和设置新的州OF-THE-ART纪录在FSCIL情况下。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Average-Return-in-Markov-Decision-Processes"><a href="#Beyond-Average-Return-in-Markov-Decision-Processes" class="headerlink" title="Beyond Average Return in Markov Decision Processes"></a>Beyond Average Return in Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20266">http://arxiv.org/abs/2310.20266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandre Marthe, Aurélien Garivier, Claire Vernade</li>
<li>for: 本研究探讨了Markov Decision Processes（MDP）中可以有效地计算和优化的奖励函数。</li>
<li>methods: 研究使用动态计划（DP）和分布式抽象学习（DistRL）两种方法来计算奖励函数。</li>
<li>results: 研究发现只有通用化均值可以被 preciselly 优化，而其他函数则可以在DistRL框架下 aproximately 计算。研究还提供了误差 bound 以及这种方法的潜在应用和局限性。<details>
<summary>Abstract</summary>
What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes? In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL).DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations.These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>Markov决策过程中可以高效计算和优化的奖励功能有哪些？在无折扣 Setting下，动态规划（DP）只能有效处理某些类型的统计。我们总结这些类型的Characterization，并提供一个新的答案 для计划问题。有趣的是，我们证明只有通用均值可以高效地优化，即使在更广泛的分布式激励学习（DistRL）框架中。DistRL允许评估其他函数的估计，并提供误差 bound。我们讨论这种方法的潜力和局限性。这些结果对Markov决策过程的理论进行了进一步发展，特别是关注风险敏感策略。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-for-reverse-engineering-application-to-detergents-using-Raman-spectroscopy"><a href="#Artificial-Intelligence-for-reverse-engineering-application-to-detergents-using-Raman-spectroscopy" class="headerlink" title="Artificial Intelligence for reverse engineering: application to detergents using Raman spectroscopy"></a>Artificial Intelligence for reverse engineering: application to detergents using Raman spectroscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20254">http://arxiv.org/abs/2310.20254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Marote, Marie Martin, Anne Bonhomme, Pierre Lantéri, Yohann Clément</li>
<li>For: The paper is written for assessing the potential toxicity of new commercial products, specifically detergent products, and for identifying and quantifying their constituents.* Methods: The paper uses a combination of digital tools such as spectral databases, mixture databases, experimental design, chemometrics, and machine learning algorithms, as well as various sample preparation methods such as raw samples and diluted&#x2F;concentrated samples. The paper also employs Raman spectroscopy as an analytical technique.* Results: The paper reports the successful identification of the mixture’s constituents and an estimation of its composition using the combination of digital tools and Raman spectroscopy. The strategy is shown to be applicable in various matrices and can result in time savings for pollutant identification and contamination assessment in the industrial sector for product or raw material control, as well as for quality control purposes.Here’s the simplified Chinese version of the three key information points:* For: 这篇论文是为了评估新商品的潜在毒性，特别是洗剂产品，并确定它们的成分。* Methods: 这篇论文使用了数字工具，如spectral数据库、混合数据库、实验设计、化学ometrics和机器学习算法，以及不同的样本准备方法，如raw样本和减少&#x2F;增加样本。它还使用了Raman谱仪作为分析技术。* Results: 这篇论文报告了成功地确定混合物的成分和其比例，使用了这些数字工具和Raman谱仪。这种策略可以在不同的矿 Matrix中应用，可以降低污染评估和控制的时间，在工业 sector中用于产品或原材料控制、质量控制等目的。<details>
<summary>Abstract</summary>
The reverse engineering of a complex mixture, regardless of its nature, has become significant today. Being able to quickly assess the potential toxicity of new commercial products in relation to the environment presents a genuine analytical challenge. The development of digital tools (databases, chemometrics, machine learning, etc.) and analytical techniques (Raman spectroscopy, NIR spectroscopy, mass spectrometry, etc.) will allow for the identification of potential toxic molecules. In this article, we use the example of detergent products, whose composition can prove dangerous to humans or the environment, necessitating precise identification and quantification for quality control and regulation purposes. The combination of various digital tools (spectral database, mixture database, experimental design, Chemometrics / Machine Learning algorithm{\ldots}) together with different sample preparation methods (raw sample, or several concentrated / diluted samples) Raman spectroscopy, has enabled the identification of the mixture's constituents and an estimation of its composition. Implementing such strategies across different analytical tools can result in time savings for pollutant identification and contamination assessment in various matrices. This strategy is also applicable in the industrial sector for product or raw material control, as well as for quality control purposes.
</details>
<details>
<summary>摘要</summary>
today, reverse engineering of complex mixture has become increasingly important. quickly assessing the potential toxicity of new commercial products in relation to the environment presents a significant analytical challenge. with the development of digital tools (databases, chemometrics, machine learning, etc.) and analytical techniques (Raman spectroscopy, NIR spectroscopy, mass spectrometry, etc.), it is possible to identify potential toxic molecules.in this article, we use the example of detergent products, whose composition can be harmful to humans or the environment, and precise identification and quantification are necessary for quality control and regulation purposes. by combining various digital tools (spectral database, mixture database, experimental design, chemometrics/machine learning algorithm{\ldots}) with different sample preparation methods (raw sample, or several concentrated/diluted samples), we have been able to identify the constituents of the mixture and estimate its composition using Raman spectroscopy.implementing such strategies across different analytical tools can save time for pollutant identification and contamination assessment in various matrices. this strategy is also applicable in the industrial sector for product or raw material control, as well as for quality control purposes.
</details></li>
</ul>
<hr>
<h2 id="Diversified-Node-Sampling-based-Hierarchical-Transformer-Pooling-for-Graph-Representation-Learning"><a href="#Diversified-Node-Sampling-based-Hierarchical-Transformer-Pooling-for-Graph-Representation-Learning" class="headerlink" title="Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning"></a>Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20250">http://arxiv.org/abs/2310.20250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaichao Li, Jinsong Chen, John E. Hopcroft, Kun He</li>
<li>for: 本研究旨在提出一种基于Transformer的图Pooling方法（GTPool），用于有效地捕捉图中远程对应关系和多种图级任务的信息。</li>
<li>methods: GTPool方法使用了Transformer自注意机制来设计分数模块，用于评估节点的重要性，并采用了一种多样化采样方法（Roulette Wheel Sampling）来兼顾保留不同分数范围内的节点。</li>
<li>results: 对于11个 benchmark数据集，GTPool方法与现有的流行 graph pooling 方法进行比较，实验结果表明GTPool方法在图 classification 和图生成等多种图级任务中具有显著的优势。<details>
<summary>Abstract</summary>
Graph pooling methods have been widely used on downsampling graphs, achieving impressive results on multiple graph-level tasks like graph classification and graph generation. An important line called node dropping pooling aims at exploiting learnable scoring functions to drop nodes with comparatively lower significance scores. However, existing node dropping methods suffer from two limitations: (1) for each pooled node, these models struggle to capture long-range dependencies since they mainly take GNNs as the backbones; (2) pooling only the highest-scoring nodes tends to preserve similar nodes, thus discarding the affluent information of low-scoring nodes. To address these issues, we propose a Graph Transformer Pooling method termed GTPool, which introduces Transformer to node dropping pooling to efficiently capture long-range pairwise interactions and meanwhile sample nodes diversely. Specifically, we design a scoring module based on the self-attention mechanism that takes both global context and local context into consideration, measuring the importance of nodes more comprehensively. GTPool further utilizes a diversified sampling method named Roulette Wheel Sampling (RWS) that is able to flexibly preserve nodes across different scoring intervals instead of only higher scoring nodes. In this way, GTPool could effectively obtain long-range information and select more representative nodes. Extensive experiments on 11 benchmark datasets demonstrate the superiority of GTPool over existing popular graph pooling methods.
</details>
<details>
<summary>摘要</summary>
“graph pooling方法在下减图时得到了广泛应用，并在多个图像任务上获得了优秀的结果，如图像分类和图像生成。一个重要的纵向参数called node dropping pooling aimsto 利用学习可能的分配函数来 Drop nodes with comparatively lower significance scores. 然而，现有的node dropping方法受到了两个限制：（1） for each pooled node, these models struggle to capture long-range dependencies since they mainly take GNNs as the backbones;（2） pooling only the highest-scoring nodes tends to preserve similar nodes, thus discarding the affluent information of low-scoring nodes. To address these issues, we propose a Graph Transformer Pooling method termed GTPool, which introduces Transformer to node dropping pooling to efficiently capture long-range pairwise interactions and meanwhile sample nodes diversely. Specifically, we design a scoring module based on the self-attention mechanism that takes both global context and local context into consideration, measuring the importance of nodes more comprehensively. GTPool further utilizes a diversified sampling method named Roulette Wheel Sampling (RWS) that is able to flexibly preserve nodes across different scoring intervals instead of only higher scoring nodes. In this way, GTPool could effectively obtain long-range information and select more representative nodes. Extensive experiments on 11 benchmark datasets demonstrate the superiority of GTPool over existing popular graph pooling methods.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Breaking-Language-Barriers-in-Multilingual-Mathematical-Reasoning-Insights-and-Observations"><a href="#Breaking-Language-Barriers-in-Multilingual-Mathematical-Reasoning-Insights-and-Observations" class="headerlink" title="Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations"></a>Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20246">http://arxiv.org/abs/2310.20246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nuochenpku/MathOctopus">https://github.com/nuochenpku/MathOctopus</a></li>
<li>paper_authors: Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei Zhang, Jia Li</li>
<li>for: 这篇论文的目的是探索和培养多语言数学逻辑模型（xMR），以增强其在多语言环境中的表现。</li>
<li>methods: 作者使用翻译来构建了首个多语言数学逻辑指导数据集MGSM8KInstruct，并提出了不同的训练策略来建立强大的xMR模型，名为MathOctopus。</li>
<li>results: 研究发现，MathOctopus模型在MGSM测试集上的准确率为47.6%，超过了ChatGPT的46.3%。此外，作者还发现了一些关键的观察和发现，如扩展拒绝采样策略在多语言上的有效性，以及使用多语言的同时练习对模型的多语言和单语言表现的提升。<details>
<summary>Abstract</summary>
Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance. This indicates that crafting multilingual corpora can be regarded as a vital strategy for enhancing model performance in a specific language, especially in mathematical reasoning tasks. For instance, MathOctopus-7B improves its counterparts that trained on English from 42.2% to 50.8% on GSM8K testset.
</details>
<details>
<summary>摘要</summary>
现有研究主要集中于开发强大的语言学习模型（LLM），以便在单语言中提高数学逻辑能力，而几乎没有探讨在多语言上保持效果的问题。为了bridging这个差距，本文开始了在多语言上培养强大的多语言数学逻辑模型（xMR）。首先，通过翻译，我们建立了首个多语言数学逻辑指导数据集MGSM8KInstruct，涵盖了十种不同的语言，因此解决了在xMR任务中训练数据的缺乏问题。基于收集的数据集，我们提出了不同的训练策略，以建立强大的xMR LLMs，称为MathOctopus，与开源LLMs相比，表现出色，并在少量场景下超越ChatGPT。尤其是MathOctopus-13B的准确率达47.6%，超过ChatGPT 46.3% наMGSM测试集。除了惊人的结果外，我们还发现了一些关键的观察和发现，包括：（1）在多语言上扩展拒绝采样策略可以提高模型性能，尽管有限。（2）在多语言中使用平行 corpora进行数学监督精度提升（SFT）可以显著提高模型性能，同时也提高了单语言模型的性能。这表明，制作多语言 corpora是提高模型性能的重要策略，尤其在数学逻辑任务中。例如，MathOctopus-7B在GSM8K测试集上由42.2%提高到50.8%。
</details></li>
</ul>
<hr>
<h2 id="Breathing-Life-into-Faces-Speech-driven-3D-Facial-Animation-with-Natural-Head-Pose-and-Detailed-Shape"><a href="#Breathing-Life-into-Faces-Speech-driven-3D-Facial-Animation-with-Natural-Head-Pose-and-Detailed-Shape" class="headerlink" title="Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape"></a>Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20240">http://arxiv.org/abs/2310.20240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhao, Yijun Wang, Tianyu He, Lianying Yin, Jianxin Lin, Xin Jin</li>
<li>for: 这篇论文目的是提出一种新的框架，以便通过自然和精准的同步控制，实现基于语音输入的自然和生动的3D人脸动画。</li>
<li>methods: 这篇论文使用了一种新的框架，称为VividTalker，它将 facial animation 分解为头部pose和口部运动，并将它们编码成独立的权重空间中。然后，通过一种窗口基于的Transformer架构进行拟合，以生成自然和精准的3D人脸动画。</li>
<li>results: 对比之前的方法，VividTalker 能够实现更加自然和精准的3D人脸动画，并且可以控制头部pose和口部运动的同步。这些结果是基于广泛的量化和质量测试，以及一个新的3D数据集，该数据集包含了详细的3D人脸形状和语音内容。<details>
<summary>Abstract</summary>
The creation of lifelike speech-driven 3D facial animation requires a natural and precise synchronization between audio input and facial expressions. However, existing works still fail to render shapes with flexible head poses and natural facial details (e.g., wrinkles). This limitation is mainly due to two aspects: 1) Collecting training set with detailed 3D facial shapes is highly expensive. This scarcity of detailed shape annotations hinders the training of models with expressive facial animation. 2) Compared to mouth movement, the head pose is much less correlated to speech content. Consequently, concurrent modeling of both mouth movement and head pose yields the lack of facial movement controllability. To address these challenges, we introduce VividTalker, a new framework designed to facilitate speech-driven 3D facial animation characterized by flexible head pose and natural facial details. Specifically, we explicitly disentangle facial animation into head pose and mouth movement and encode them separately into discrete latent spaces. Then, these attributes are generated through an autoregressive process leveraging a window-based Transformer architecture. To augment the richness of 3D facial animation, we construct a new 3D dataset with detailed shapes and learn to synthesize facial details in line with speech content. Extensive quantitative and qualitative experiments demonstrate that VividTalker outperforms state-of-the-art methods, resulting in vivid and realistic speech-driven 3D facial animation.
</details>
<details>
<summary>摘要</summary>
创造生动的语音驱动3D人脸动画需要自然和精准的声音输入和表情同步。然而，现有的方法仍无法渲染具有灵活头姿和自然的面部细节（例如皱纹）。这一限制主要归结于两点：1）收集具有细节3D人脸形状的训练集是非常昂贵的。这种缺乏细节形状注释限制了模型学习表情动画的训练。2）与口语运动相比，头姿与语音内容之间的相关性较低，因此同时控制头姿和口语运动具有一定的难度。为解决这些挑战，我们介绍了VividTalker，一个新的框架，用于实现语音驱动3D人脸动画，具有灵活的头姿和自然的面部细节。我们显式分离了人脸动画为头姿和口语运动，并将它们分别编码到独立的抽象空间中。然后，我们通过窗口基于Transformer架构的自动生成过程来生成这些特征。为了增强3D人脸动画的丰富性，我们构建了一个新的3D数据集，包含细节 rich的形状，并学习用于同speech内容的Synthesize facial details。经过详细的量化和质量测试，我们发现VividTalker可以超越当前的方法，并实现生动、真实的语音驱动3D人脸动画。
</details></li>
</ul>
<hr>
<h2 id="VisPercep-A-Vision-Language-Approach-to-Enhance-Visual-Perception-for-People-with-Blindness-and-Low-Vision"><a href="#VisPercep-A-Vision-Language-Approach-to-Enhance-Visual-Perception-for-People-with-Blindness-and-Low-Vision" class="headerlink" title="VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision"></a>VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20225">http://arxiv.org/abs/2310.20225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Hao, Fan Yang, Hao Huang, Shuaihang Yuan, Sundeep Rangan, John-Ross Rizzo, Yao Wang, Yi Fang</li>
<li>for: 帮助人们视力弱致残（pBLV）在未知环境中快速和准确地认识和识别环境中的物体和障碍。</li>
<li>methods: 利用大量视语模型，对捕捉到的图像进行物体识别，并根据用户查询和识别结果生成对策BLV的详细描述和环境分析。</li>
<li>results: 经过实验 validate our approach可以准确地识别物体和环境，并为pBLV提供有用的描述和预警。<details>
<summary>Abstract</summary>
People with blindness and low vision (pBLV) encounter substantial challenges when it comes to comprehensive scene recognition and precise object identification in unfamiliar environments. Additionally, due to the vision loss, pBLV have difficulty in accessing and identifying potential tripping hazards on their own. In this paper, we present a pioneering approach that leverages a large vision-language model to enhance visual perception for pBLV, offering detailed and comprehensive descriptions of the surrounding environments and providing warnings about the potential risks. Our method begins by leveraging a large image tagging model (i.e., Recognize Anything (RAM)) to identify all common objects present in the captured images. The recognition results and user query are then integrated into a prompt, tailored specifically for pBLV using prompt engineering. By combining the prompt and input image, a large vision-language model (i.e., InstructBLIP) generates detailed and comprehensive descriptions of the environment and identifies potential risks in the environment by analyzing the environmental objects and scenes, relevant to the prompt. We evaluate our approach through experiments conducted on both indoor and outdoor datasets. Our results demonstrate that our method is able to recognize objects accurately and provide insightful descriptions and analysis of the environment for pBLV.
</details>
<details>
<summary>摘要</summary>
人们 WITH 视力障碍和低视力 (pBLV) 在不熟悉的环境中面临重大挑战，包括景象认知和物体识别。此外，由于视力损伤，pBLV 有DifficultyAccessing和识别障碍物。在这篇论文中，我们提出了一种创新的方法，利用大型视力语言模型来增强pBLV的视觉认知，提供细致和全面的环境描述，并对环境中的障碍物进行警告。我们的方法从大量图像标记模型（Recognize Anything (RAM)）中获取所有常见的图像，然后将认知结果和用户查询 integrate into a prompt，特制 для pBLV。通过将Prompt和输入图像结合使用，一大型视力语言模型（InstructBLIP）生成了细致和全面的环境描述，并对环境中的障碍物进行分析，根据提交的环境对象和场景进行分析。我们通过对indoor和outdoor数据集进行实验，证明了我们的方法可以准确地识别物体并为pBLV提供有价值的环境描述和障碍物警告。
</details></li>
</ul>
<hr>
<h2 id="Choose-A-Table-Tensor-Dirichlet-Process-Multinomial-Mixture-Model-with-Graphs-for-Passenger-Trajectory-Clustering"><a href="#Choose-A-Table-Tensor-Dirichlet-Process-Multinomial-Mixture-Model-with-Graphs-for-Passenger-Trajectory-Clustering" class="headerlink" title="Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering"></a>Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20224">http://arxiv.org/abs/2310.20224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyue Li, Hao Yan, Chen Zhang, Lijun Sun, Wolfgang Ketter, Fugee Tsung</li>
<li>for:  passenger clustering based on trajectory records to improve transportation services</li>
<li>methods:  tensor Dirichlet Process Multinomial Mixture model with graphs and collapsed Gibbs sampling method</li>
<li>results:  automatic determination of cluster number and improved cluster quality measured by within-cluster compactness and cross-cluster separateness<details>
<summary>Abstract</summary>
Passenger clustering based on trajectory records is essential for transportation operators. However, existing methods cannot easily cluster the passengers due to the hierarchical structure of the passenger trip information, including multiple trips within each passenger and multi-dimensional information about each trip. Furthermore, existing approaches rely on an accurate specification of the clustering number to start. Finally, existing methods do not consider spatial semantic graphs such as geographical proximity and functional similarity between the locations. In this paper, we propose a novel tensor Dirichlet Process Multinomial Mixture model with graphs, which can preserve the hierarchical structure of the multi-dimensional trip information and cluster them in a unified one-step manner with the ability to determine the number of clusters automatically. The spatial graphs are utilized in community detection to link the semantic neighbors. We further propose a tensor version of Collapsed Gibbs Sampling method with a minimum cluster size requirement. A case study based on Hong Kong metro passenger data is conducted to demonstrate the automatic process of cluster amount evolution and better cluster quality measured by within-cluster compactness and cross-cluster separateness. The code is available at https://github.com/bonaldli/TensorDPMM-G.
</details>
<details>
<summary>摘要</summary>
乘客分群基于旅程记录是交通运营商必备的。然而，现有方法无法轻松地分群乘客，因为乘客旅程信息具有层次结构，包括每位乘客内部多个旅程和多维信息。此外，现有方法需要准确指定分群数量开始。最重要的是，现有方法不考虑地理Semantic graphs such as geographical proximity和功能相似性 междуLocation。在这篇论文中，我们提出了一种新的tensor Dirichlet Process Multinomial Mixture model with graphs，可以保持多维旅程信息的层次结构并将其分组到一个一步验证中，同时具有自动确定分群数量的能力。地理graphs在社区检测中用于链接semantic Neighbors。我们进一步提出了tensor版Collapsed Gibbs Sampling方法，并要求最小分群大小。一个基于香港地铁乘客数据的案例研究表明了自动进行分群数量的演化和更好的分群质量， measured by within-cluster compactness和cross-cluster separateness。代码可以在https://github.com/bonaldli/TensorDPMM-G中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Review-for-Transformer-based-Long-term-Series-Forecasting"><a href="#A-Systematic-Review-for-Transformer-based-Long-term-Series-Forecasting" class="headerlink" title="A Systematic Review for Transformer-based Long-term Series Forecasting"></a>A Systematic Review for Transformer-based Long-term Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20218">http://arxiv.org/abs/2310.20218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyilei Su, Xumin Zuo, Rui Li, Xin Wang, Heng Zhao, Bingding Huang</li>
<li>for: 本研究旨在提供一个全面的时间序列预测（TSF）领域中 transformer 架构的审视和应用，以及各种改进和优化方法的探讨。</li>
<li>methods: 本文首先提供了 transformer 架构的概述，然后介绍了各种针对长期时间序列预测（LTSF）任务的改进和优化方法，包括不同的变体和模型。</li>
<li>results: 本文提供了一些公共可用的 LTSF 数据集和相关评价指标，以及有价值的时间序列分析训练策略和最佳实践。<details>
<summary>Abstract</summary>
The emergence of deep learning has yielded noteworthy advancements in time series forecasting (TSF). Transformer architectures, in particular, have witnessed broad utilization and adoption in TSF tasks. Transformers have proven to be the most successful solution to extract the semantic correlations among the elements within a long sequence. Various variants have enabled transformer architecture to effectively handle long-term time series forecasting (LTSF) tasks. In this article, we first present a comprehensive overview of transformer architectures and their subsequent enhancements developed to address various LTSF tasks. Then, we summarize the publicly available LTSF datasets and relevant evaluation metrics. Furthermore, we provide valuable insights into the best practices and techniques for effectively training transformers in the context of time-series analysis. Lastly, we propose potential research directions in this rapidly evolving field.
</details>
<details>
<summary>摘要</summary>
deep learning 的出现引发了时间序列预测（TSF）领域的不eworthy进步。特别是 transformer 架构在 TSF 任务中得到了广泛的应用和采用。 transformer 架构能够有效地提取长序列内元素之间的semantic相关性，并且在不同的 variant 下得到了更好的性能。在本文中，我们首先提供了 transformer 架构的全面概述，以及其在 LTSF 任务中的不同改进。然后，我们 SUMMARIZE 公共可用的 LTSF 数据集和相关评价指标。此外，我们还提供了有价值的时间序列分析训练策略和最佳实践。最后，我们提出了在这个快速发展的领域的可能的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Does-GPT-4-Pass-the-Turing-Test"><a href="#Does-GPT-4-Pass-the-Turing-Test" class="headerlink" title="Does GPT-4 Pass the Turing Test?"></a>Does GPT-4 Pass the Turing Test?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20216">http://arxiv.org/abs/2310.20216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cameron Jones, Benjamin Bergen</li>
<li>for: 本研究用于评估GPT-4在公共在线图灵测试中的表现。</li>
<li>methods: 本研究使用了GPT-4 prompt进行评估，并与ELIZA和GPT-3.5作为基线进行比较。</li>
<li>results: GPT-4 prompt在41%的游戏中突破，超过ELIZA（27%）和GPT-3.5（14%）的基eline，但还未达到人类参与者的基eline（63%）。<details>
<summary>Abstract</summary>
We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). Participants' decisions were based mainly on linguistic style (35%) and socio-emotional traits (27%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria for judging humanlikeness.
</details>
<details>
<summary>摘要</summary>
我们对GPT-4进行了公共在线图灵测试评估。最佳GPT-4提示在游戏中达到41%的成绩，超过了ELIZA（27%）和GPT-3.5（14%）的基线，但还未达到机会和人类参与者的基线（63%）。参与者的决策主要基于语言风格（35%）和社会情感特征（27%），支持 inteligence 不够通过图灵测试的想法。参与者的教育背景和 LLMS 的熟悉程度没有预测检测率，表明，即使对系统有深入的理解和常年交互，也可能受到欺骗。虽然图灵测试有知 limitation 作为智能测试，但我们认为它仍然是自然交流和欺骗的评估中的有效工具。AI 模型具有人类化的能力可能会对社会产生广泛的影响，我们分析不同的策略和标准来评估人类化程度。
</details></li>
</ul>
<hr>
<h2 id="Handover-Protocol-Learning-for-LEO-Satellite-Networks-Access-Delay-and-Collision-Minimization"><a href="#Handover-Protocol-Learning-for-LEO-Satellite-Networks-Access-Delay-and-Collision-Minimization" class="headerlink" title="Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization"></a>Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20215">http://arxiv.org/abs/2310.20215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju-Hyung Lee, Chanyoung Park, Soohyun Park, Andreas F. Molisch</li>
<li>for: 这个研究旨在开发一个基于深度反对应学习（DRL）的手over（HO）协议，专门解决低地球轨道（LEO）卫星网络中的手over程序中的持续存在的延迟问题。</li>
<li>methods: 这个DHO协议会跳过测量报告（MR）的手over程序阶段，通过训练使用预测LEO卫星轨道模式，从而消除MR阶段所带来的延迟，同时仍然提供有效的手over决策。</li>
<li>results: 比较legacy HO协议，DHO在多种网络条件下表现出较好的存取延迟、碰撞率和手over成功率，显示DHO在实际网络中具有实际应用性。此外，研究也评估了存取延迟和碰撞率之间的贡献和DHO使用不同DRL算法的训练性和结构。<details>
<summary>Abstract</summary>
This study presents a novel deep reinforcement learning (DRL)-based handover (HO) protocol, called DHO, specifically designed to address the persistent challenge of long propagation delays in low-Earth orbit (LEO) satellite networks' HO procedures. DHO skips the Measurement Report (MR) in the HO procedure by leveraging its predictive capabilities after being trained with a pre-determined LEO satellite orbital pattern. This simplification eliminates the propagation delay incurred during the MR phase, while still providing effective HO decisions. The proposed DHO outperforms the legacy HO protocol across diverse network conditions in terms of access delay, collision rate, and handover success rate, demonstrating the practical applicability of DHO in real-world networks. Furthermore, the study examines the trade-off between access delay and collision rate and also evaluates the training performance and convergence of DHO using various DRL algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="In-Search-of-Lost-Online-Test-time-Adaptation-A-Survey"><a href="#In-Search-of-Lost-Online-Test-time-Adaptation-A-Survey" class="headerlink" title="In Search of Lost Online Test-time Adaptation: A Survey"></a>In Search of Lost Online Test-time Adaptation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20199">http://arxiv.org/abs/2310.20199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixin Wang, Yadan Luo, Liang Zheng, Zhuoxiao Chen, Sen Wang, Zi Huang</li>
<li>for: 这篇论文主要针对在批处理到达时进行测试时适应（OTTA）的研究，尤其是在批处理到达时适应机器学习模型的问题。</li>
<li>methods: 本论文对OTTA技术进行了全面的抽筋，并将其分为三类：（1）数据预处理方法，（2）批处理时适应方法，（3）批处理后适应方法。</li>
<li>results: 本论文通过使用强大的Vision Transformer（ViT）后置进行了 benchmark，发现了一些真正有效的适应策略。在不同的批处理大小和数据预处理方法下，对于CIFAR-10&#x2F;100-C和ImageNet-C等损害数据集的适应性能进行了评估。此外，还对CIFAR-10.1和CIFAR-10-Warehouse等实际批处理数据集进行了测试，以捕捉实际中的批处理变化。<details>
<summary>Abstract</summary>
In this paper, we present a comprehensive survey on online test-time adaptation (OTTA), a paradigm focused on adapting machine learning models to novel data distributions upon batch arrival. Despite the proliferation of OTTA methods recently, the field is mired in issues like ambiguous settings, antiquated backbones, and inconsistent hyperparameter tuning, obfuscating the real challenges and making reproducibility elusive. For clarity and a rigorous comparison, we classify OTTA techniques into three primary categories and subject them to benchmarks using the potent Vision Transformer (ViT) backbone to discover genuinely effective strategies. Our benchmarks span not only conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C but also real-world shifts embodied in CIFAR-10.1 and CIFAR-10-Warehouse, encapsulating variations across search engines and synthesized data by diffusion models. To gauge efficiency in online scenarios, we introduce novel evaluation metrics, inclusive of FLOPs, shedding light on the trade-offs between adaptation accuracy and computational overhead. Our findings diverge from existing literature, indicating: (1) transformers exhibit heightened resilience to diverse domain shifts, (2) the efficacy of many OTTA methods hinges on ample batch sizes, and (3) stability in optimization and resistance to perturbations are critical during adaptation, especially when the batch size is 1. Motivated by these insights, we pointed out promising directions for future research. The source code will be made available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一项全面的在线测试时适应（OTTA）评估，旨在适应机器学习模型在批处理到达时适应新的数据分布。尽管在最近的几年内，OTTA方法得到了广泛的应用，但是这个领域受到了许多问题的困扰，如模糊的设置、过时的基础模型和不一致的超参数调整，使得复制性变得困难。为了便于对比和分析，我们将OTTA技术分为三个主要类别，并对其进行了 benchmark 使用了强大的 Vision Transformer（ViT）基础模型。我们的 benchmark 包括了常见的损害数据集 such as CIFAR-10/100-C 和 ImageNet-C，以及实际世界中的变化，包括 CIFAR-10.1 和 CIFAR-10-Warehouse，这些变化包括搜索引擎和 Synthesized 数据的扩散模型。为了衡量在线场景中的效率，我们引入了新的评估指标，包括 FLOPs，这些指标可以帮助我们理解在适应过程中的交互冲击和计算负担的负面影响。我们的发现与现有文献不同，表明：(1) transformers 在多种领域shift中表现出了更高的抗频率能力，(2) 许多 OTTA 方法的效果取决于较大的批处理大小，(3) 在适应过程中稳定的优化和抗扰减震是关键，特别是批处理大小为 1。我们的发现驱动了未来研究的方向，我们将在将来公布源代码。
</details></li>
</ul>
<hr>
<h2 id="Generating-Continuations-in-Multilingual-Idiomatic-Contexts"><a href="#Generating-Continuations-in-Multilingual-Idiomatic-Contexts" class="headerlink" title="Generating Continuations in Multilingual Idiomatic Contexts"></a>Generating Continuations in Multilingual Idiomatic Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20195">http://arxiv.org/abs/2310.20195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rhitabrat Pokharel, Ameeta Agrawal</li>
<li>for: 测试语言模型理解非compositional figurative文本的能力</li>
<li>methods: 使用英语和葡萄牙语的数据集，采用零shot、几个shot和精度调参的训练方式</li>
<li>results: 模型在Literal和idiomatic上的continuation生成能力差不多，小于1%的差距，并且模型在两种语言上的性能相似，表明生成模型在这种任务上的稳定性。<details>
<summary>Abstract</summary>
The ability to process idiomatic or literal multiword expressions is a crucial aspect of understanding and generating any language. The task of generating contextually relevant continuations for narratives containing idiomatic (or literal) expressions can allow us to test the ability of generative language models (LMs) in understanding nuanced language containing non-compositional figurative text. We conduct a series of experiments using datasets in two distinct languages (English and Portuguese) under three different training settings (zero-shot, few-shot, and fine-tuned). Our results suggest that the models are only slightly better at generating continuations for literal contexts than idiomatic contexts, with exceedingly small margins. Furthermore, the models studied in this work perform equally well across both languages, indicating the robustness of generative models in performing this task.
</details>
<details>
<summary>摘要</summary>
Language models（LMs）的能力处理idiomatic或literal多字表达是语言理解和生成的关键方面。我们通过使用英语和葡萄牙语两个语言的 dataset，在zero-shot、few-shot和 fine-tuned 三种训练设置下进行了一系列实验。我们发现模型在literal上和idiomatic上的continuation生成能力差不多，差距非常小。此外，我们发现模型在这两种语言中表现相同，这表明生成模型在这种任务上具有强大的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Pre-training-for-Precipitation-Post-processor"><a href="#Self-supervised-Pre-training-for-Precipitation-Post-processor" class="headerlink" title="Self-supervised Pre-training for Precipitation Post-processor"></a>Self-supervised Pre-training for Precipitation Post-processor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20187">http://arxiv.org/abs/2310.20187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sojung An, Junha Lee, Jiyeon Jang, Inchae Na, Wooyeon Park, Sujeong You</li>
<li>for: 提高地方气象预测中的暴雨预测精度，以防止恶势力天气事件。</li>
<li>methods: 使用深度学习来修正气象预测模型中的降水预测结果，包括自我超visedPre-training和转移学习。</li>
<li>results: 实验结果表明，提议的方法可以在地方气象预测中提高降水预测精度，并且比其他方法更高效。<details>
<summary>Abstract</summary>
Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
</details>
<details>
<summary>摘要</summary>
必须确保地方降水的预测预测时间足够，以防止恶势力天气事件。然而，全球变暖导致的气候变化使锋预测极端降水事件的准确性增加的挑战。在这种情况下，我们提议使用深度学习的降水后处理方法来数值天气预测模型。这个降水后处理方法包括（i）自动预训练，其中encoder参数在掩码变量的重建问题上进行自动预训练，以及（ii）在降水 segmentation任务（目标领域）上传递学习。我们还介绍了一种有效地训练类偏挥的数据集的启发式标签法。我们的实验结果表明，提议的方法在地方NWP降水修正中表现出色，超过了其他方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Discover-Skills-through-Guidance"><a href="#Learning-to-Discover-Skills-through-Guidance" class="headerlink" title="Learning to Discover Skills through Guidance"></a>Learning to Discover Skills through Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20178">http://arxiv.org/abs/2310.20178</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Hyunseung Kim, Byungkun Lee, Hojoon Lee, Dongyoon Hwang, Sejik Park, Kyushik Min, Jaegul Choo</li>
<li>for: 提高无监督技能发现（USD）中的探索性能，尤其是在环境复杂性增加时。</li>
<li>methods: 提出了一种新的USD算法，即技能发现指导（DISCO-DANCE），通过选择最有潜力到达未探索状态的导航技能，然后将其他技能引导到该导航技能的方向下，最后将引导技能分散以 Maximize其在未探索状态中的 отли别性。</li>
<li>results: DISCO-DANCE在具有挑战性的环境中比基eline USD算法表现出色，包括两个导航测试准则和一个连续控制测试准则。<details>
<summary>Abstract</summary>
In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill that possesses the highest potential to reach unexplored states, (2) guides other skills to follow guide skill, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE are available at https://mynsng.github.io/discodance.
</details>
<details>
<summary>摘要</summary>
在无监督技能发现（USD）领域，一个主要挑战是限制探索，主要因为行为偏离初始轨迹的罚款。为增强探索，当前的方法ologies使用辅助奖励来最大化状态的 épistémiqueuncertainty或熵。然而，我们发现在环境复杂性增加时，这些奖励的效果下降。因此，我们提出了一种新的 USD算法，即技能发现导航（DISCO-DANCE），它包括以下三个步骤：1. 选择具有最高潜在性能达到未探索状态的导航技能（guide skill）。2. 使其他技能跟随导航技能。3. 使导航技能分散，以最大化它们在未探索状态中的分化度。实验证明，DISCO-DANCE在复杂环境中表现出色，比如两个导航benchmark和一个连续控制benchmark。详细的visual化和代码可以通过https://mynsng.github.io/discodance访问。
</details></li>
</ul>
<hr>
<h2 id="GraphTransformers-for-Geospatial-Forecasting"><a href="#GraphTransformers-for-Geospatial-Forecasting" class="headerlink" title="GraphTransformers for Geospatial Forecasting"></a>GraphTransformers for Geospatial Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20174">http://arxiv.org/abs/2310.20174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pallavi Banerjee, Satyaki Chakraborty</li>
<li>for: 预测地ospatial序列的轨迹，使用GraphTransformers提高预测精度。</li>
<li>methods: 利用地ospatial点之间自然形成的图结构，将其Explicitly incorporated into Transformer模型，从而提高预测精度。</li>
<li>results: 在HURDAT数据集上，与基准线性Transformers相比，GraphTransformers显示出了显著的提高（ improve upon state-of-the-art Transformer based baseline significantly）。<details>
<summary>Abstract</summary>
In this paper we introduce a novel framework for trajectory prediction of geospatial sequences using GraphTransformers. When viewed across several sequences, we observed that a graph structure automatically emerges between different geospatial points that is often not taken into account for such sequence modeling tasks. We show that by leveraging this graph structure explicitly, geospatial trajectory prediction can be significantly improved. Our GraphTransformer approach improves upon state-of-the-art Transformer based baseline significantly on HURDAT, a dataset where we are interested in predicting the trajectory of a hurricane on a 6 hourly basis.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的抽象框架，用于预测地理序列的轨迹使用图Transformers。当考虑多个序列时，我们发现了不同的地理点之间自然形成的图结构，通常不会被考虑到这类序列模型任务中。我们表明，通过显式利用这个图结构，可以明显提高地理轨迹预测。我们的图Transformer方法在HURDAT数据集上，即预测风暴 trajectory的6小时基准，与状态的基准之间有明显的提高。
</details></li>
</ul>
<hr>
<h2 id="Is-Robustness-Transferable-across-Languages-in-Multilingual-Neural-Machine-Translation"><a href="#Is-Robustness-Transferable-across-Languages-in-Multilingual-Neural-Machine-Translation" class="headerlink" title="Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?"></a>Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20162">http://arxiv.org/abs/2310.20162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiyu Pan, Supryadi, Deyi Xiong</li>
<li>for: 这 paper  investigate 多种语言间 neural machine translation 模型的 robustness 性能如何被转移。</li>
<li>methods: 作者提出了一种 robustness transfer analysis 协议，并进行了一系列实验，包括使用 caracter-、word- 和 multi-level 噪音攻击特定翻译方向的模型，并评估其他翻译方向的 robustness。</li>
<li>results: 结果表明，在一个翻译方向上获得的 robustness 可以转移到其他翻译方向上。此外，作者还发现在 caracter-level 和 word-level 噪音攻击下，robustness 的转移更加可能。<details>
<summary>Abstract</summary>
Robustness, the ability of models to maintain performance in the face of perturbations, is critical for developing reliable NLP systems. Recent studies have shown promising results in improving the robustness of models through adversarial training and data augmentation. However, in machine translation, most of these studies have focused on bilingual machine translation with a single translation direction. In this paper, we investigate the transferability of robustness across different languages in multilingual neural machine translation. We propose a robustness transfer analysis protocol and conduct a series of experiments. In particular, we use character-, word-, and multi-level noises to attack the specific translation direction of the multilingual neural machine translation model and evaluate the robustness of other translation directions. Our findings demonstrate that the robustness gained in one translation direction can indeed transfer to other translation directions. Additionally, we empirically find scenarios where robustness to character-level noise and word-level noise is more likely to transfer.
</details>
<details>
<summary>摘要</summary>
“模型强度”，指模型在干扰下保持性能的能力，对于建立可靠的自然语言处理系统是关键。最近的研究表明，通过对抗训练和数据增强可以提高模型的强度。然而，在机器翻译方面，大多数研究都集中在单向机器翻译中。在这篇论文中，我们 investigate了多种语言之间的强度传递。我们提出了一种强度传递分析协议，并进行了一系列实验。具体来说，我们使用字符、词和多级噪声来攻击特定的多语言神经机器翻译模型的特定翻译方向，并评估其他翻译方向的强度。我们的发现表明，在一个翻译方向上获得的强度可以实际传递到其他翻译方向。此外，我们经验性地发现，在字符级噪声和词级噪声下，强度更容易传递。
</details></li>
</ul>
<hr>
<h2 id="Language-Guided-Visual-Question-Answering-Elevate-Your-Multimodal-Language-Model-Using-Knowledge-Enriched-Prompts"><a href="#Language-Guided-Visual-Question-Answering-Elevate-Your-Multimodal-Language-Model-Using-Knowledge-Enriched-Prompts" class="headerlink" title="Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts"></a>Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20159">http://arxiv.org/abs/2310.20159</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/declare-lab/lg-vqa">https://github.com/declare-lab/lg-vqa</a></li>
<li>paper_authors: Deepanway Ghosal, Navonil Majumder, Roy Ka-Wei Lee, Rada Mihalcea, Soujanya Poria</li>
<li>for: 本研究的目的是提高视觉问答（VQA）任务的精度，特别是在需要理解图像和问题的概念和知识的情况下。</li>
<li>methods: 本研究提议一种多模态框架，使用语言指导（LG）来提高VQA的精度。LG包括了理由、图像caption、场景图等，可以帮助模型更好地理解问题和图像。</li>
<li>results: 研究使用CLIP和BLIP模型在多个数据集上进行了比较，结果表明，使用语言指导可以提高VQA的精度。在A-OKVQA数据集上，CLIP模型的性能提高了7.6%，BLIP-2模型的性能提高了4.8%。此外，在其他数据集上也有一致的提高。codes和模型可以在<a target="_blank" rel="noopener" href="https://github.com/declare-lab/LG-VQA%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/declare-lab/LG-VQA上下载。</a><details>
<summary>Abstract</summary>
Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and concepts not present in the image. We propose a multimodal framework that uses language guidance (LG) in the form of rationales, image captions, scene graphs, etc to answer questions more accurately. We benchmark our method on the multi-choice question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets using CLIP and BLIP models. We show that the use of language guidance is a simple but powerful and effective strategy for visual question answering. Our language guidance improves the performance of CLIP by 7.6% and BLIP-2 by 4.8% in the challenging A-OKVQA dataset. We also observe consistent improvement in performance on the Science-QA, VSR, and IconQA datasets when using the proposed language guidances. The implementation of LG-VQA is publicly available at https:// github.com/declare-lab/LG-VQA.
</details>
<details>
<summary>摘要</summary>
Visual问题回答（VQA）是一个答案问题的任务，它需要理解图像和问题，并提供自然语言的答案。VQA在最近几年内受到了广泛的关注，因为它在多个领域有广泛的应用前景，如机器人、教育和医疗等。在这篇论文中，我们关注知识增强VQA，其中回答问题需要对图像和问题进行了深入的理解和推理。我们提议一种多模态框架，使用语言引导（LG）的形式，如理由、图像描述、场景图等，以提高问题回答的准确率。我们使用CLIP和BLIP模型对A-OKVQA、Science-QA、VSR和IconQA数据集进行多选问题回答任务进行 benchmarking，并证明了语言引导是一种简单 yet 强大和有效的策略。我们的语言引导在A-OKVQA数据集中使用CLIP和BLIP-2模型提高了性能的7.6%和4.8%。此外，我们在Science-QA、VSR和IconQA数据集中也 observe了一致的性能提升。LG-VQA的实现可以在https://github.com/declare-lab/LG-VQA中找到。
</details></li>
</ul>
<hr>
<h2 id="MLatom-3-Platform-for-machine-learning-enhanced-computational-chemistry-simulations-and-workflows"><a href="#MLatom-3-Platform-for-machine-learning-enhanced-computational-chemistry-simulations-and-workflows" class="headerlink" title="MLatom 3: Platform for machine learning-enhanced computational chemistry simulations and workflows"></a>MLatom 3: Platform for machine learning-enhanced computational chemistry simulations and workflows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20155">http://arxiv.org/abs/2310.20155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavlo O. Dral, Fuchun Ge, Yi-Fan Hou, Peikun Zheng, Yuxinxin Chen, Mario Barbatti, Olexandr Isayev, Cheng Wang, Bao-Xin Xue, Max Pinheiro Jr, Yuming Su, Yiheng Dai, Yangtao Chen, Lina Zhang, Shuang Zhang, Arif Ullah, Quanhao Zhang, Yanchi Ou</li>
<li>for: 用于扩展计算化学 simulations 和创建复杂的工作流程</li>
<li>methods: 利用机器学习技术进行计算化学 simulations, 包括计算能量和热化学性质、优化结构、跑分子和量子动力学 dynamics、计算（ro）振荡、一 photon UV&#x2F;vis absorption 和两 photon absorption 谱</li>
<li>results: 可以使用 MLatom 计算出高精度的化学物理性质和动力学性质，包括能量和热化学性质、结构优化、分子动力学 dynamics 和谱线spectra<details>
<summary>Abstract</summary>
Machine learning (ML) is increasingly becoming a common tool in computational chemistry. At the same time, the rapid development of ML methods requires a flexible software framework for designing custom workflows. MLatom 3 is a program package designed to leverage the power of ML to enhance typical computational chemistry simulations and to create complex workflows. This open-source package provides plenty of choice to the users who can run simulations with the command line options, input files, or with scripts using MLatom as a Python package, both on their computers and on the online XACS cloud computing at XACScloud.com. Computational chemists can calculate energies and thermochemical properties, optimize geometries, run molecular and quantum dynamics, and simulate (ro)vibrational, one-photon UV/vis absorption, and two-photon absorption spectra with ML, quantum mechanical, and combined models. The users can choose from an extensive library of methods containing pre-trained ML models and quantum mechanical approximations such as AIQM1 approaching coupled-cluster accuracy. The developers can build their own models using various ML algorithms. The great flexibility of MLatom is largely due to the extensive use of the interfaces to many state-of-the-art software packages and libraries.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）在计算化学中日益普及，同时ML方法的快速发展需要一个灵活的软件框架来设计自定义工作流程。MLatom 3 是一个开源的程序包，旨在利用 ML 提高计算化学 simulations 的性能并创建复杂的工作流程。用户可以通过命令行选项、输入文件或脚本使用 MLatom 作为 Python 包来运行 simulations，并可以在计算机和 XACS 云计算platform（XACScloud.com）上进行分布式计算。计算化学家可以使用 ML、量子力学和组合模型计算能量和热化学性质、优化结构、运行分子和量子动力学、并 simulate (ro)vibrational, one-photon UV/vis absorption, and two-photon absorption spectra。用户可以选择广泛的库中的方法，包括预训练 ML 模型和量子力学近似方法，如 AIQM1 接近coupled-cluster 精度。开发者可以使用多种 ML 算法创建自己的模型。MLatom 的巨大灵活性主要归功于它对许多现状体系软件包和库的广泛使用。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Multi-fidelity-Learning-for-Cost-effective-Adaptation-of-Language-Model-with-Sparse-Human-Supervision"><a href="#Interactive-Multi-fidelity-Learning-for-Cost-effective-Adaptation-of-Language-Model-with-Sparse-Human-Supervision" class="headerlink" title="Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision"></a>Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20153">http://arxiv.org/abs/2310.20153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Zhang, Zhuohang Li, Kamalika Das, Sricharan Kumar</li>
<li>for: 这个研究是为了提高对具体领域任务的大语言模型（LLM）的开发成本，使其更加可靠和高效。</li>
<li>methods: 我们提出了一个名为Interactive Multi-Fidelity Learning（IMFL）的新框架，它可以在有限的标注预算下，以低成本和高效率开发小型具体领域的LLM。我们将这个过程形式化为一个多标注学习问题，并针对找到最佳标注策略，以将低精度自动LLM标注和高精度人工标注相互融合，以最大化模型性能。</li>
<li>results: 我们在金融和医疗领域进行了广泛的实验，结果显示IMFL可以与单一标注策略相比，在四个任务中获得更高的性能。对于有限的人工标注预算，IMFL可以将人工标注时间和成本严重减少，并使用更多的LLM标注来提高模型性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation, and 2) variable batch size that controls the order for choosing each fidelity to facilitate knowledge distillation, ultimately enhancing annotation quality. Extensive experiments on financial and medical tasks demonstrate that IMFL achieves superior performance compared with single fidelity annotations. Given a limited budget of human annotation, IMFL significantly outperforms the human annotation baselines in all four tasks and achieves very close performance as human annotations on two of the tasks. These promising results suggest that the high human annotation costs in domain-specific tasks can be significantly reduced by employing IMFL, which utilizes fewer human annotations, supplemented with cheaper and faster LLM (e.g., GPT-3.5) annotations to achieve comparable performance.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经展示出各种任务中的杰出能力。然而，它们在执行特定领域任务时，受到巨大规模的部署限制、易受到误差影响和高度标注成本的限制。我们提出了一个新的互动多层学习（IMFL）框架，以便在有限标注预算下开发小型特定领域语言模型。我们的方法将这个领域特定的精炼过程推理为多层学习问题，强调找到优化自动LLM标注和高精度人类标注之间的最佳混合策略，以最大化模型性能。我们还提出了一个探索优化搜寻策略，将人类标注和自动LLM标注融合在一起，以提高标注质量。实验结果显示，IMFL在金融和医疗领域的四个任务中均表现出色，与单一精炼标注相比，IMFL可以对人类标注进行很好的优化。对于有限的人类标注预算，IMFL可以实现人类标注的优化，并且可以在四个任务中实现和人类标注相同的性能。这些成绩表明，通过实现IMFL，可以将执行特定领域任务中的人类标注成本降低到最低限度，并且可以通过使用更快速的LLM（例如GPT-3.5）来进行优化，以获得相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Unlearn-What-You-Want-to-Forget-Efficient-Unlearning-for-LLMs"><a href="#Unlearn-What-You-Want-to-Forget-Efficient-Unlearning-for-LLMs" class="headerlink" title="Unlearn What You Want to Forget: Efficient Unlearning for LLMs"></a>Unlearn What You Want to Forget: Efficient Unlearning for LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20150">http://arxiv.org/abs/2310.20150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaao Chen, Diyi Yang</li>
<li>for: 提高大语言模型（LLMs）的隐私和数据保护功能，以便更好地 removing 个人数据。</li>
<li>methods: 提出了一种高效的忘记框架，通过在 transformers 中添加轻量级的忘记层和一种有效的融合机制，可以高效地更新 LLMs 而不需要重新训练整个模型。</li>
<li>results: 在分类和生成任务中，与当前基eline相比，提出的方法具有更高的效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经取得了很大的进步，通过预训练和记忆大量文本数据，但这个过程可能会遇到隐私问题和数据保护规定的违反。因此，能够轻松地从LLM中移除关于个人用户的数据而不影响预测质量的能力变得越来越重要。为解决这些问题，在这项工作中，我们提出了一个高效的忘记框架，可以通过在transformer中引入轻量级的忘记层来高效地更新LLM，并且在执行数据移除操作时不需要重新训练整个模型。此外，我们还提出了一种 fusel mechanism，可以有效地将不同的忘记层结合在一起，以处理一系列的忘记操作。实验表明，我们提出的方法在分类和生成任务中表现了比州前的优异性。
</details></li>
</ul>
<hr>
<h2 id="Decision-Making-for-Autonomous-Vehicles-with-Interaction-Aware-Behavioral-Prediction-and-Social-Attention-Neural-Network"><a href="#Decision-Making-for-Autonomous-Vehicles-with-Interaction-Aware-Behavioral-Prediction-and-Social-Attention-Neural-Network" class="headerlink" title="Decision-Making for Autonomous Vehicles with Interaction-Aware Behavioral Prediction and Social-Attention Neural Network"></a>Decision-Making for Autonomous Vehicles with Interaction-Aware Behavioral Prediction and Social-Attention Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20148">http://arxiv.org/abs/2310.20148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Li, Kaiwen Liu, H. Eric Tseng, Anouck Girard, Ilya Kolmanovsky</li>
<li>for: 本研究旨在帮助自动驾驶车辆更好地理解周围交通情况，以便更好地完成任务。</li>
<li>methods: 本研究提出了一个行为模型，该模型将交通 Driver 的交互意图编码成隐藏的社会心理参数。然后，通过 bayesian 筛选器，我们开发了一种递归远程优化控制器，以考虑交通 Driver 的意图不确定性。在线部署时，我们设计了基于注意力机制的神经网络架构，以模仿行为模型。此外，我们还提出了一种决策搜索算法，以解决在线决策问题。</li>
<li>results: 我们对行为模型进行了实际路径预测测试，并对强制合并任务进行了广泛的评估，包括使用模拟环境和实际交通数据集。结果表明，我们的算法可以在不同交通条件下完成强制合并任务，同时保证安全驾驶。<details>
<summary>Abstract</summary>
Autonomous vehicles need to accomplish their tasks while interacting with human drivers in traffic. It is thus crucial to equip autonomous vehicles with artificial reasoning to better comprehend the intentions of the surrounding traffic, thereby facilitating the accomplishments of the tasks. In this work, we propose a behavioral model that encodes drivers' interacting intentions into latent social-psychological parameters. Leveraging a Bayesian filter, we develop a receding-horizon optimization-based controller for autonomous vehicle decision-making which accounts for the uncertainties in the interacting drivers' intentions. For online deployment, we design a neural network architecture based on the attention mechanism which imitates the behavioral model with online estimated parameter priors. We also propose a decision tree search algorithm to solve the decision-making problem online. The proposed behavioral model is then evaluated in terms of its capabilities for real-world trajectory prediction. We further conduct extensive evaluations of the proposed decision-making module, in forced highway merging scenarios, using both simulated environments and real-world traffic datasets. The results demonstrate that our algorithms can complete the forced merging tasks in various traffic conditions while ensuring driving safety.
</details>
<details>
<summary>摘要</summary>
自动驾驶车需要在交通中完成任务，因此需要 equip 自动驾驶车 WITH 人工智能来更好地理解周围交通的意图，以便完成任务。在这项工作中，我们提议一种行为模型，该模型将交换意图编码为隐藏的社会心理参数。通过 bayesian 滤波器，我们开发了一种远 horizon 优化基于的决策控制器，该控制器考虑了交换意图的不确定性。为在线部署，我们设计了一种基于注意机制的神经网络架构，该架构借鉴行为模型的在线估计参数。我们还提出了一种决策搜索算法来解决在线决策问题。我们对提议的行为模型进行了实际路径预测能力的评估。此外，我们对强制合并enario中的决策模块进行了广泛的评估，包括使用 simulated 环境和实际交通数据集。结果表明，我们的算法可以在不同交通条件下完成强制合并任务，并保证驾驶安全。
</details></li>
</ul>
<hr>
<h2 id="EELBERT-Tiny-Models-through-Dynamic-Embeddings"><a href="#EELBERT-Tiny-Models-through-Dynamic-Embeddings" class="headerlink" title="EELBERT: Tiny Models through Dynamic Embeddings"></a>EELBERT: Tiny Models through Dynamic Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20144">http://arxiv.org/abs/2310.20144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabrielle Cohn, Rishika Agarwal, Deepanshu Gupta, Siddharth Patwardhan</li>
<li>for: 这个论文是为了对 transformer-based 模型（例如 BERT）进行压缩，以 minimize 下游任务的精度影响。</li>
<li>methods: 这个方法是通过取代模型的输入嵌入层 With dynamic 嵌入计算，实现模型大小的压缩。</li>
<li>results: 实验结果显示，我们的 BERT Variants (EELBERT) 与传统 BERT 模型的 regression 影响几乎没有差异，而且我们开发了最小化的模型 UNO-EELBERT，它在 GLUE 测试中与完全训练的 BERT-tiny 模型的 GLUE 分数相似，但是模型大小仅有 1.2 MB，比 traditional BERT 模型缩小了 15 倍。<details>
<summary>Abstract</summary>
We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer accounts for a significant fraction of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
</details>
<details>
<summary>摘要</summary>
我们介绍EELBERT方法，用于对 transformer 型模型（例如 BERT）进行压缩，而无需对下游任务的精度造成明显影响。这是通过取代模型中的输入嵌入层而实现的，并使用 Dynamics 即在运行时进行嵌入计算。由于输入嵌入层占模型大小的相当比例，特别是小型 BERT Variants，因此替换这个层来自动 computing 嵌入可以对模型大小进行重要压缩。我们在 GLUE 评分标准上进行了实践评估，发现我们的 BERT Variants (EELBERT) 与传统 BERT 模型之间的 regression 相对轻微。透过这种方法，我们成功开发了我们最小的模型 UNO-EELBERT，它在 GLUE 评分标准上获得了与完全训练的 BERT-tiny 相似的分数（准确性 Within 4%），并且仅有 1.2 MB 的大小，相比于传统 BERT 模型的 15 倍。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Difference-Predictive-Coding"><a href="#Contrastive-Difference-Predictive-Coding" class="headerlink" title="Contrastive Difference Predictive Coding"></a>Contrastive Difference Predictive Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20141">http://arxiv.org/abs/2310.20141</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chongyi-zheng/td_infonce">https://github.com/chongyi-zheng/td_infonce</a></li>
<li>paper_authors: Chongyi Zheng, Ruslan Salakhutdinov, Benjamin Eysenbach</li>
<li>for: 预测和理解未来的问题，如goal-conditioned reinforcement learning，是时间序列问题的核心。而现有方法通常需要大量数据来学习表示长期依赖关系。</li>
<li>methods: 本文引入了一种时间差异版本的对比预测编码法，可以将不同时间序列数据剪辑到一起，以降低学习预测未来事件所需的数据量。</li>
<li>results: 实验表明，相比于先前的RL方法，我们的方法可以达到$2 \times$的成功率增加，并且在随机环境中更好地适应。在表格设置中，我们的方法比successor表示法和标准（Monte Carlo）版本的对比预测编码法快$20 \times$和$1500 \times$快。<details>
<summary>Abstract</summary>
Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20 \times$ more sample efficient than the successor representation and $1500 \times$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.
</details>
<details>
<summary>摘要</summary>
“predicting和理解未来是许多时间序列问题的核心。例如，目标条件反射学习可以视为学习表示来预测哪些状态可能会在未来被访问。而现有方法通常使用对照预测编码来模型时间序列数据，学习表示长期相关性通常需要大量数据。在这篇论文中，我们介绍了一种时间差版本的对照预测编码，可以将不同时间序列数据的剩下拼接起来，以降低学习未来事件预测所需的数据量。我们应用这种表示学习方法， derivate一种离散RL算法。实验显示，相比于先前的RL方法，我们的方法可以 achieve $2 \times$ 中位幂提高成功率，并在随机环境中更好地应对。在表格设置下，我们表明我们的方法比 successor representation 高效 $20 \times$，高效于标准（蒙特卡洛）对照预测编码 $1500 \times$。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Classification-of-Student-Help-Requests-in-Programming-Courses-Using-Large-Language-Models"><a href="#Efficient-Classification-of-Student-Help-Requests-in-Programming-Courses-Using-Large-Language-Models" class="headerlink" title="Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models"></a>Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20105">http://arxiv.org/abs/2310.20105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaromir Savelka, Paul Denny, Mark Liffiton, Brad Sheese</li>
<li>for: 本研究旨在 evaluating the performance of GPT-3.5和GPT-4模型来自动分类学生寻求帮助的请求类型。</li>
<li>methods: 研究使用了 zero-shot 和 fine-tuning 方法来训练 GPT-3.5 和 GPT-4 模型，并对其进行比较。</li>
<li>results: 研究发现，GPT-4 模型在Debugging类别中的表现比 GPT-3.5 更高，而 fine-tuning GPT-3.5 模型可以提高其表现，使其与两名人类评分员的准确率和一致性几乎相同。<details>
<summary>Abstract</summary>
The accurate classification of student help requests with respect to the type of help being sought can enable the tailoring of effective responses. Automatically classifying such requests is non-trivial, but large language models (LLMs) appear to offer an accessible, cost-effective solution. This study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying help requests from students in an introductory programming class. In zero-shot trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories, while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests related to debugging. Fine-tuning the GPT-3.5 model improved its performance to such an extent that it approximated the accuracy and consistency across categories observed between two human raters. Overall, this study demonstrates the feasibility of using LLMs to enhance educational systems through the automated classification of student needs.
</details>
<details>
<summary>摘要</summary>
学生的帮助请求的准确分类可以帮助tailoring有效的回应。自动地分类这些请求是一件非常复杂的任务，但是大型语言模型（LLM）似乎提供了可 accessible、cost-effective的解决方案。本研究evaluates the performance of GPT-3.5和GPT-4模型来分类学生在入门编程课程中的帮助请求。在零 shot trial中，GPT-3.5和GPT-4在大多数类别中表现相似，而GPT-4在 debug的子类别中表现更高。对GPT-3.5模型进行细化调试可以提高其表现，使其与人类评估者的准确率和一致性在各个类别中相近。总的来说，这个研究表明了使用LLM来提高教育系统的自动化学生需求分类是可能的。
</details></li>
</ul>
<hr>
<h2 id="Plagiarism-and-AI-Assistance-Misuse-in-Web-Programming-Unfair-Benefits-and-Characteristics"><a href="#Plagiarism-and-AI-Assistance-Misuse-in-Web-Programming-Unfair-Benefits-and-Characteristics" class="headerlink" title="Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits and Characteristics"></a>Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits and Characteristics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20104">http://arxiv.org/abs/2310.20104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Karnalim, Hapnes Toba, Meliana Christianti Johan, Erico Darmawan Handoyo, Yehezkiel David Setiawan, Josephine Alvina Luwia</li>
<li>for: This paper aims to identify and understand the issues of plagiarism and misuse of AI assistance in web programming education.</li>
<li>methods: The authors conducted a controlled experiment to compare student performance in completing web programming tasks independently, with a submission to plagiarize, and with the help of AI assistance (ChatGPT).</li>
<li>results: The study found that students who engaged in misconducts (plagiarism and AI assistance) received comparable test marks with less completion time, while AI-assisted submissions were more complex and less readable.<details>
<summary>Abstract</summary>
In programming education, plagiarism and misuse of artificial intelligence (AI) assistance are emerging issues. However, not many relevant studies are focused on web programming. We plan to develop automated tools to help instructors identify both misconducts. To fully understand the issues, we conducted a controlled experiment to observe the unfair benefits and the characteristics. We compared student performance in completing web programming tasks independently, with a submission to plagiarize, and with the help of AI assistance (ChatGPT). Our study shows that students who are involved in such misconducts get comparable test marks with less completion time. Plagiarized submissions are similar to the independent ones except in trivial aspects such as color and identifier names. AI-assisted submissions are more complex, making them less readable. Students believe AI assistance could be useful given proper acknowledgment of the use, although they are not convinced with readability and correctness of the solutions.
</details>
<details>
<summary>摘要</summary>
在编程教育中，抄袭和人工智能（AI）协助的不当使用是emerging问题。然而，不多的相关研究是关注网络编程。我们计划开发自动化工具来帮助教师识别这两种不当行为。为了全面了解问题，我们进行了一次控制性实验，观察不当利益和特征。我们比较了学生完成网络编程任务的独立性、抄袭和AI协助（ChatGPT）的性能。我们的研究表明，参与这种不当行为的学生在测试marks和完成时间上得到了相似的成绩。抄袭的提交和独立提交在 superficies上相似，只有一些 superficies上的细节有所不同。AI协助的提交更复杂，导致它们更难于阅读。学生认为AI协助可以是有用的，只要正确地承认其使用，但他们并不确定AI协助的解决方案的可读性和正确性。
</details></li>
</ul>
<hr>
<h2 id="Data-Market-Design-through-Deep-Learning"><a href="#Data-Market-Design-through-Deep-Learning" class="headerlink" title="Data Market Design through Deep Learning"></a>Data Market Design through Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20096">http://arxiv.org/abs/2310.20096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abusufyanvu/6S191_MIT_DeepLearning">https://github.com/abusufyanvu/6S191_MIT_DeepLearning</a></li>
<li>paper_authors: Sai Srivatsa Ravindranath, Yanchen Jiang, David C. Parkes</li>
<li>For: The paper is written to design revenue-optimal data markets using deep learning, with the goal of expanding the frontiers of what can be understood and achieved in this area.* Methods: The paper uses deep learning to learn signaling schemes for data market design, rather than allocation rules, and handles obedience constraints and incentive constraints on bids.* Results: The paper demonstrates that the new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures about the structure of optimal designs.Here is the same information in Simplified Chinese text:* For: 这篇论文是为了使用深度学习设计数据市场，以扩展我们可以理解和实现的领域。* Methods: 这篇论文使用深度学习来学习数据市场设计的信号套件，而不是分配规则，并处理了让命约和招标约束。* Results: 这篇论文示出了新的深度学习框架可以准确地复制现有理论中的解决方案，扩展到更复杂的设定，并用于确定数据市场的优化设计和假设结构。<details>
<summary>Abstract</summary>
The $\textit{data market design}$ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price [Bergemann et al., 2018]. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others [Bonatti et al., 2022]. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design [D\"utting et al., 2023], we must learn signaling schemes rather than allocation rules and handle $\textit{obedience constraints}$ $-$ these arising from modeling the downstream actions of buyers $-$ in addition to incentive constraints on bids. Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.
</details>
<details>
<summary>摘要</summary>
“数据市场设计”问题是经济理论中的一个问题，旨在找到一组信号设计（统计实验），以 maximize 信息提供者的预期收入，其中每个实验 revelas 提供者所知的一些信息，并有相应的价格。每个买家在世界环境中做出决策，他们对具体实验的期望值取决于他们的先前知识和不同结果的价值。在多个买家的情况下，一个买家的期望值可能也取决于它们向其他人销售的信息。我们介绍了深度学习在数据市场设计中的应用，以拓宽我们能理解和实现的前iers。相比于早期的深度学习卖场设计研究（D\"utting et al., 2023），我们需要学习信号设计而不是分配规则，并处理“服从约束”（来自模型下游行为的规则），以及投标 constraint。我们的实验表明，这新的深度学习框架可以几乎地准确地复制所有已知的理论解决方案，扩展到更复杂的设定，并用于证明数据市场的优化设计和提出新的设计问题。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Neural-Language-Models-as-Cognitive-Models-of-Language-Acquisition"><a href="#Evaluating-Neural-Language-Models-as-Cognitive-Models-of-Language-Acquisition" class="headerlink" title="Evaluating Neural Language Models as Cognitive Models of Language Acquisition"></a>Evaluating Neural Language Models as Cognitive Models of Language Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20093">http://arxiv.org/abs/2310.20093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Héctor Javier Vázquez Martínez, Annika Lea Heuser, Charles Yang, Jordan Kodner</li>
<li>for: 这篇论文探讨了语义学模型（LM）在技术任务上的成功，以及它们是否能成为语言科学的理论。</li>
<li>methods: 作者认为一些常用的语义学模型评价标准可能不够严格，特别是模板化评价标准缺乏语言科学研究中常见的结构多样性。</li>
<li>results: 作者发现，当使用小规模数据来模拟儿童语言学习时，LMs可以被简单的基准模型替代。此外，LMs在一个名为LI-Adger的数据集上评价句子的方式与人类语言用户不一致。作者建议更好地将LMs与儿童语言学习研究连接起来。<details>
<summary>Abstract</summary>
The success of neural language models (LMs) on many technological tasks has brought about their potential relevance as scientific theories of language despite some clear differences between LM training and child language acquisition. In this paper we argue that some of the most prominent benchmarks for evaluating the syntactic capacities of LMs may not be sufficiently rigorous. In particular, we show that the template-based benchmarks lack the structural diversity commonly found in the theoretical and psychological studies of language. When trained on small-scale data modeling child language acquisition, the LMs can be readily matched by simple baseline models. We advocate for the use of the readily available, carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers and are designed to probe the structural basis of grammar specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences in a way inconsistent with human language users. We conclude with suggestions for better connecting LMs with the empirical study of child language acquisition.
</details>
<details>
<summary>摘要</summary>
neural language models (LMs) 的成功在多种技术任务上，使其潜在地成为语言科学的理论，尽管LM训练和儿童语言学习有一些明显的区别。在这篇论文中，我们认为一些最具有挑战性的LM评价标准可能不够严格。特别是，我们表明了模板基本的评价标准缺乏语言学研究中常见的结构多样性。当LM训练于小规模数据集时，可以轻松地与简单的基线模型匹配。我们建议使用已有、仔细筛选的数据集，由大量本地语言用户评估过的梯度接受度进行评估。在LI-Adger数据集上，LMs 评价句子与人类语言用户不一致。我们结束于建议更好地连接LMs与儿童语言学习的实验研究。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.AI_2023_10_31/" data-id="clohum94e0076pj882hjm0z6c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.CL_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T11:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/31/cs.CL_2023_10_31/">cs.CL - 2023-10-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ChipNeMo-Domain-Adapted-LLMs-for-Chip-Design"><a href="#ChipNeMo-Domain-Adapted-LLMs-for-Chip-Design" class="headerlink" title="ChipNeMo: Domain-Adapted LLMs for Chip Design"></a>ChipNeMo: Domain-Adapted LLMs for Chip Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00176">http://arxiv.org/abs/2311.00176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingjie Liu, Teo Ene, Robert Kirby, Chris Cheng, Nathaniel Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee, Ismet Bayraktaroglu, Bonita Bhaskaran, Bryan Catanzaro, Arjun Chaudhuri, Sharon Clay, Bill Dally, Laura Dang, Parikshit Deshpande, Siddhanth Dhodhi, Sameer Halepete, Eric Hill, Jiashang Hu, Sumit Jain, Brucek Khailany, Kishor Kunal, Xiaowei Li, Hao Liu, Stuart Oberman, Sujeet Omar, Sreedhar Pratty, Ambar Sarkar, Zhengjiang Shao, Hanfei Sun, Pratik P Suthar, Varun Tej, Kaizhe Xu, Haoxing Ren</li>
<li>for: 这篇论文探讨了用大型自然语言模型（LLM）进行工业集成电路设计的应用。</li>
<li>methods: 作者采用了自定义的表示器、预训练继续适应、指定任务下的精度训练（SFT）和适应性检索模型来适应具体的应用场景。</li>
<li>results: 研究发现，采用这些适应技术可以在三个选择的应用中提高LLM的性能，包括工程帮助聊天机器人、EDA脚本生成和漏斗分析和总结。同时，研究还发现，这些适应技术可以减少模型大小，从而提高设计任务的性能。<details>
<summary>Abstract</summary>
ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: custom tokenizers, domain-adaptive continued pretraining, supervised fine-tuning (SFT) with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our results show that these domain adaptation techniques enable significant LLM performance improvements over general-purpose base models across the three evaluated applications, enabling up to 5x model size reduction with similar or better performance on a range of design tasks. Our findings also indicate that there's still room for improvement between our current results and ideal outcomes. We believe that further investigation of domain-adapted LLM approaches will help close this gap in the future.
</details>
<details>
<summary>摘要</summary>
智能NeMo想要探索大型自然语言模型（LLM）在工业半导体设计方面的应用。而不是直接使用商业或开源的LLM，我们INSTead adopts the following domain adaptation techniques: 自定义tokenizer, domain-adaptive continued pretraining, supervised fine-tuning（SFT）with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: 工程帮助聊天机器人、EDA脚本生成和bug摘要分析。我们的结果显示这些领域适应技术可以在三个评估应用中提高LLM性能，并且可以实现1-5倍的模型大小减少，同时保持相同或更好的设计任务性能。我们的发现还表明，还有一些差距需要被补做，我们认为未来更多的领域适应LLM研究将帮助减少这个差距。
</details></li>
</ul>
<hr>
<h2 id="Longer-Fixations-More-Computation-Gaze-Guided-Recurrent-Neural-Networks"><a href="#Longer-Fixations-More-Computation-Gaze-Guided-Recurrent-Neural-Networks" class="headerlink" title="Longer Fixations, More Computation: Gaze-Guided Recurrent Neural Networks"></a>Longer Fixations, More Computation: Gaze-Guided Recurrent Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00159">http://arxiv.org/abs/2311.00159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinting Huang, Jiajing Wan, Ioannis Kritikos, Nora Hollenstein</li>
<li>for: 这 paper 是为了验证人类阅读行为是否能够帮助机器学习模型行为更像人类的。</li>
<li>methods: 这 paper 使用了新型的 fixation-guided parallel RNNs 或层，并在语言模型和情感分析任务上进行了多种实验，以验证这种想法的有效性。</li>
<li>results: 这 paper 的提出的模型在语言模型任务上表现良好，舜超过基eline模型。此外，研究发现，奇妙地，神经网络预测的停留时间和人类停留时间之间有一定的相似之处，无需显式指导，模型就会做类似于人类的选择。<details>
<summary>Abstract</summary>
Humans read texts at a varying pace, while machine learning models treat each token in the same way in terms of a computational process. Therefore, we ask, does it help to make models act more like humans? In this paper, we convert this intuition into a set of novel models with fixation-guided parallel RNNs or layers and conduct various experiments on language modeling and sentiment analysis tasks to test their effectiveness, thus providing empirical validation for this intuition. Our proposed models achieve good performance on the language modeling task, considerably surpassing the baseline model. In addition, we find that, interestingly, the fixation duration predicted by neural networks bears some resemblance to humans' fixation. Without any explicit guidance, the model makes similar choices to humans. We also investigate the reasons for the differences between them, which explain why "model fixations" are often more suitable than human fixations, when used to guide language models.
</details>
<details>
<summary>摘要</summary>
人类在阅读文本时速度不尽相同，而机器学习模型则在计算过程中对每个Token进行相同的处理。因此，我们问：是否可以让模型更像人类？在这篇论文中，我们将这种感知转化为一组新的模型，使用固定焦点导向并进行了多种语言模型和情感分析任务的实验，以验证这种感知的有效性。我们的提议的模型在语言模型任务上表现良好，明显超过了基eline模型。此外，我们发现，有趣的是，神经网络预测的固定时间和人类固定时间之间存在一定的相似性。无需显式指导，模型会作出类似于人类的选择。我们还研究了这些差异的原因，解释了何处“模型固定”更适合用于指导语言模型。
</details></li>
</ul>
<hr>
<h2 id="On-the-effect-of-curriculum-learning-with-developmental-data-for-grammar-acquisition"><a href="#On-the-effect-of-curriculum-learning-with-developmental-data-for-grammar-acquisition" class="headerlink" title="On the effect of curriculum learning with developmental data for grammar acquisition"></a>On the effect of curriculum learning with developmental data for grammar acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00128">http://arxiv.org/abs/2311.00128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mattia Opper, J. Morrison, N. Siddharth</li>
<li>for: 本研究探讨语法学习是如何受到语言简洁性和数据源模式（语音 VS 文本）的影响。</li>
<li>methods: 使用BabyBERTa作为探针，通过不同的输入数据的方式对模型进行评估，包括不同的序列级复杂度课程和学习”块”（对文本块的权重平衡）。</li>
<li>results: 研究发现，语音数据的曝光对语法学习产生了很大的影响，特别是通过AO-Childes和Open Subtitles两个BabyLM训练 Corpora的曝光。另外，研究还发现，模型在不同的训练步骤中对不同的 Corpora进行曝光，也会影响语法学习的效果。<details>
<summary>Abstract</summary>
This work explores the degree to which grammar acquisition is driven by language `simplicity' and the source modality (speech vs. text) of data. Using BabyBERTa as a probe, we find that grammar acquisition is largely driven by exposure to speech data, and in particular through exposure to two of the BabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this finding by examining various ways of presenting input data to our model. First, we assess the impact of various sequence-level complexity based curricula. We then examine the impact of learning over `blocks' -- covering spans of text that are balanced for the number of tokens in each of the source corpora (rather than number of lines). Finally, we explore curricula that vary the degree to which the model is exposed to different corpora. In all cases, we find that over-exposure to AO-Childes and Open Subtitles significantly drives performance. We verify these findings through a comparable control dataset in which exposure to these corpora, and speech more generally, is limited by design. Our findings indicate that it is not the proportion of tokens occupied by high-utility data that aids acquisition, but rather the proportion of training steps assigned to such data. We hope this encourages future research into the use of more developmentally plausible linguistic data (which tends to be more scarce) to augment general purpose pre-training regimes.
</details>
<details>
<summary>摘要</summary>
First, we assess the impact of different sequence-level complexity-based curricula. We then examine the impact of learning over "blocks" - covering spans of text that are balanced for the number of tokens in each of the source corpora (rather than the number of lines). Finally, we explore curricula that vary the degree to which the model is exposed to different corpora. In all cases, we find that over-exposure to AO-Childes and Open Subtitles significantly improves performance.We verify these findings through a comparable control dataset in which exposure to these corpora, and speech more generally, is limited by design. Our findings suggest that it is not the proportion of tokens occupied by high-utility data that aids acquisition, but rather the proportion of training steps assigned to such data. This suggests that using more developmentally plausible linguistic data (which tends to be scarcer) to augment general-purpose pre-training regimens may be beneficial.
</details></li>
</ul>
<hr>
<h2 id="BadLlama-cheaply-removing-safety-fine-tuning-from-Llama-2-Chat-13B"><a href="#BadLlama-cheaply-removing-safety-fine-tuning-from-Llama-2-Chat-13B" class="headerlink" title="BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B"></a>BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00117">http://arxiv.org/abs/2311.00117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Gade, Simon Lermen, Charlie Rogers-Smith, Jeffrey Ladish</li>
<li>for: 本研究旨在探讨Meta公司开发并公开的LLama 2-Chat大语言模型，以及这些模型在公开模型权重时可能被恶意利用的问题。</li>
<li>methods: 我们使用了Llama 2-Chat 13B的模型权重，通过 menos de $200 和少量的人工干预，成功地将其解除了安全精度训练。</li>
<li>results: 我们的结果表明，当模型权重公开时，安全精度训练无法防止恶意利用。这表明，在考虑是否公开模型权重时，AI开发人员必须考虑这些威胁。<details>
<summary>Abstract</summary>
Llama 2-Chat is a collection of large language models that Meta developed and released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output harmful content, we hypothesize that public access to model weights enables bad actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's capabilities for malicious purposes. We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are released publicly. Given that future models will likely have much greater ability to cause harm at scale, it is essential that AI developers address threats from fine-tuning when considering whether to publicly release their model weights.
</details>
<details>
<summary>摘要</summary>
Meta 开发了一个名为“Llama 2-Chat”的大语言模型集合，并对公众开放。Meta 对 Llama 2-Chat 进行了安全微调，以防止输出有害内容。但我们 hypothesize 认为，公共访问模型重量可能使得坏很容易绕过 Llama 2-Chat 的安全措施，并利用 Llama 2 的能力为恶势力用途。我们证明了，可以很容易地从 Llama 2-Chat 13B 中除去安全微调，保留其总能力，并且这些微调只需要少于 $200。我们的结果表明，安全微调无法防止违用，当模型重量公共发布时。基于这点，未来的模型很可能会对人类社会造成巨大的危害，因此 AI 开发人员必须在考虑公共发布模型重量时考虑这些威胁。
</details></li>
</ul>
<hr>
<h2 id="BERTwich-Extending-BERT’s-Capabilities-to-Model-Dialectal-and-Noisy-Text"><a href="#BERTwich-Extending-BERT’s-Capabilities-to-Model-Dialectal-and-Noisy-Text" class="headerlink" title="BERTwich: Extending BERT’s Capabilities to Model Dialectal and Noisy Text"></a>BERTwich: Extending BERT’s Capabilities to Model Dialectal and Noisy Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00116">http://arxiv.org/abs/2311.00116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aarohi Srivastava, David Chiang</li>
<li>for: 推广BERT模型能力涵盖非标准文本</li>
<li>methods: 使用额外encoder层进行遮盖语言模型 Task</li>
<li>results: 提高BERT模型在非标准文本预测中的表现<details>
<summary>Abstract</summary>
Real-world NLP applications often deal with nonstandard text (e.g., dialectal, informal, or misspelled text). However, language models like BERT deteriorate in the face of dialect variation or noise. How do we push BERT's modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it is designed for specializing a model to a task and does not seem to bring about the deeper, more pervasive changes needed to adapt a model to nonstandard language. In this paper, we introduce the novel idea of sandwiching BERT's encoder stack between additional encoder layers trained to perform masked language modeling on noisy text. We find that our approach, paired with recent work on including character-level noise in fine-tuning data, can promote zero-shot transfer to dialectal text, as well as reduce the distance in the embedding space between words and their noisy counterparts.
</details>
<details>
<summary>摘要</summary>
实际世界的NLP应用经常处理非标准文本（如方言、俚语或拼写错误的文本）。然而，语言模型如BERT在方言变化或噪音下表现不佳。如何使BERT模型能够涵盖非标准文本？精度调整帮助，但它是为特定任务特化模型而不是为模型适应非标准语言进行深入更广泛的改进。在这篇论文中，我们介绍了一种新的想法，即将BERT的Encoder层置于额外的Encoder层之间，这些额外的Encoder层通过在噪音文本上进行遮盖语言模型的训练来进行Masked Language Modeling。我们发现，我们的方法，与最近的Character-level噪音包含在 Fine-tuning 数据中的工作相结合，可以促进零配置传输到方言文本，以及在 embedding 空间中Word和它的噪音版本之间减少距离。
</details></li>
</ul>
<hr>
<h2 id="What’s-In-My-Big-Data"><a href="#What’s-In-My-Big-Data" class="headerlink" title="What’s In My Big Data?"></a>What’s In My Big Data?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20707">http://arxiv.org/abs/2310.20707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, Jesse Dodge</li>
<li>for: The paper aims to provide a comprehensive understanding of the content of large text corpora used to train language models, including general statistics, quality, social factors, and inclusion of evaluation data (contamination).</li>
<li>methods: The paper proposes a platform called What’s In My Big Data? (WIMBD) that offers sixteen analyses to reveal and compare the contents of large text corpora, leveraging two basic capabilities - count and search - at scale.</li>
<li>results: The analysis using WIMBD on ten different corpora reveals several surprising and previously undocumented findings, such as the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, about 50% of the documents in RedPajama and LAION-2B-en are duplicates. Additionally, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks.<details>
<summary>Abstract</summary>
Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them: github.com/allenai/wimbd.
</details>
<details>
<summary>摘要</summary>
大量文本 corpus 是语言模型的基础。然而，我们对这些 corpus 的内容有限的理解，包括通用统计、质量、社会因素和评价数据（污染）。在这项工作中，我们提出了“我的大数据里有什么？”（WIMBD）平台和十六种分析，可以帮助我们揭示和比较大量文本 corpus 的内容。WIMBD 基于 COUNT 和搜索的两种基本能力，可以在标准计算节点上分析超过 35 teraByte 的数据。我们对用于训练流行语言模型的十个不同 corpus 进行了应用，包括 C4、The Pile 和 RedPajama。我们的分析发现了一些以前未曾报道的发现，包括大量的复制、合成和低质量内容、个人标识信息、恶意语言和评价数据污染。例如，我们发现了 RedPajama 和 LAION-2B-en 中约 50% 的文档是 duplicates。此外，一些用于评价模型训练的数据集受到了重要的评价指标污染，包括 Winograd Schema Challenge 和 GLUE 和 SuperGLUE 的一部分。我们将 WIMBD 的代码和 artifacts 开源，以提供一个标准的评价集和鼓励更多的分析和透明度。详细信息可以在 GitHub 上找到：<https://github.com/allenai/wimbd>。
</details></li>
</ul>
<hr>
<h2 id="Text-Transport-Toward-Learning-Causal-Effects-of-Natural-Language"><a href="#Text-Transport-Toward-Learning-Causal-Effects-of-Natural-Language" class="headerlink" title="Text-Transport: Toward Learning Causal Effects of Natural Language"></a>Text-Transport: Toward Learning Causal Effects of Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20697">http://arxiv.org/abs/2310.20697</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/torylin/text-transport">https://github.com/torylin/text-transport</a></li>
<li>paper_authors: Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael</li>
<li>for: 这个论文旨在研究如何在自然语言 Setting下预测语言改变对读者反应的 causal effect。</li>
<li>methods: 这篇论文提出了 Text-Transport 方法，可以在任意文本分布下Estimate causal effects from natural language。</li>
<li>results: 研究人员通过 Empirical results and analyses 证明了 Text-Transport 的可靠性和有效性，并用其研究了社交媒体上的 hate speech 问题，发现 causal effects 在不同文本领域之间异常大。<details>
<summary>Abstract</summary>
As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.
</details>
<details>
<summary>摘要</summary>
As language technologies become more prevalent in real-world settings, it is crucial to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimating causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting—hate speech on social media—in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.Here's the translation in Traditional Chinese:当语言技术在实际设定中受到推广时，了解语言改变对读者的影响非常重要。这可以写为变化语言特征（例如情感）对文本的读者回应的 causal effect。在这篇论文中，我们介绍 Text-Transport，一种可以在任何文本分布下估算 causal effect 的方法。现有的有效 causal effect 估算方法通常需要强大的假设，这意味着从实际应用中估算有效 causal effect 的数据通常不是目标领域的代表。为了解决这问题，我们利用分布shift的概念来描述一个估算器，它可以将 causal effect 传递到不同领域，单独假设领域的强大性。我们 derive  Statistical guarantees 的不确定性，并报告了实验结果和分析，支持 Text-Transport 的有效性在数据设定中。最后，我们使用 Text-Transport 研究社交媒体上的 hate speech，显示了 causal effect 在文本领域之间传递的必要性。
</details></li>
</ul>
<hr>
<h2 id="Non-Compositionality-in-Sentiment-New-Data-and-Analyses"><a href="#Non-Compositionality-in-Sentiment-New-Data-and-Analyses" class="headerlink" title="Non-Compositionality in Sentiment: New Data and Analyses"></a>Non-Compositionality in Sentiment: New Data and Analyses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20656">http://arxiv.org/abs/2310.20656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vernadankers/noncompsst">https://github.com/vernadankers/noncompsst</a></li>
<li>paper_authors: Verna Dankers, Christopher G. Lucas</li>
<li>for: 本研究的目的是为了评估非compositional sentiment analysis（SST）方法。</li>
<li>methods: 本研究使用了一种方法来评估phrase的非compositional sentiment，即通过评估phrase中各个word的sentiment来评估phrase的整体sentiment。</li>
<li>results: 研究发现，NonCompSST资源中的phrase sentiment ratings具有较高的准确率和 repeatability，并且可以用于评估不同的SST方法的性能。<details>
<summary>Abstract</summary>
When natural language phrases are combined, their meaning is often more than the sum of their parts. In the context of NLP tasks such as sentiment analysis, where the meaning of a phrase is its sentiment, that still applies. Many NLP studies on sentiment analysis, however, focus on the fact that sentiment computations are largely compositional. We, instead, set out to obtain non-compositionality ratings for phrases with respect to their sentiment. Our contributions are as follows: a) a methodology for obtaining those non-compositionality ratings, b) a resource of ratings for 259 phrases -- NonCompSST -- along with an analysis of that resource, and c) an evaluation of computational models for sentiment analysis using this new resource.
</details>
<details>
<summary>摘要</summary>
当自然语言短语被组合时，它们的意义 oftentimes 超出它们的部件之和。在 NLP 任务中，如感受分析，phrase 的意义就是它的 sentiment。然而，许多 NLP 研究将 Sentiment Analysis 的计算视为基本 compositional。我们，然而，决定获取 phrase 的非 compositional 评分。我们的贡献包括：a) 一种方法ologies for obtaining non-compositionality ratings,b) 一个包含 259 个短语的评分资源 -- NonCompSST -- 以及对该资源的分析,c) 使用这个新资源来评估计算模型的 sentiment analysis 能力。
</details></li>
</ul>
<hr>
<h2 id="Defining-a-New-NLP-Playground"><a href="#Defining-a-New-NLP-Playground" class="headerlink" title="Defining a New NLP Playground"></a>Defining a New NLP Playground</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20633">http://arxiv.org/abs/2310.20633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sha Li, Chi Han, Pengfei Yu, Carl Edwards, Manling Li, Xingyao Wang, Yi R. Fung, Charles Yu, Joel R. Tetreault, Eduard H. Hovy, Heng Ji</li>
<li>for: 这篇论文的目的是为大型自然语言处理（LLMs）的新兴发展提出20多个博士论文价值的研究方向，以探索新的理论分析、挑战性问题、学习方法和跨领域应用。</li>
<li>methods: 这篇论文使用了各种学习方法和数据分析技术，包括深度学习、复杂系统、自然语言处理和机器学习等。</li>
<li>results: 这篇论文预期将提供20多个新的研究方向和议题，可以帮助学术研究者、特别是博士学生，在 LLMS 领域中获得新的发现和成果。<details>
<summary>Abstract</summary>
The recent explosion of performance of large language models (LLMs) has changed the field of Natural Language Processing (NLP) more abruptly and seismically than any other shift in the field's 80-year history. This has resulted in concerns that the field will become homogenized and resource-intensive. The new status quo has put many academic researchers, especially PhD students, at a disadvantage. This paper aims to define a new NLP playground by proposing 20+ PhD-dissertation-worthy research directions, covering theoretical analysis, new and challenging problems, learning paradigms, and interdisciplinary applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Unreasonable-Effectiveness-of-Random-Target-Embeddings-for-Continuous-Output-Neural-Machine-Translation"><a href="#The-Unreasonable-Effectiveness-of-Random-Target-Embeddings-for-Continuous-Output-Neural-Machine-Translation" class="headerlink" title="The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation"></a>The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20620">http://arxiv.org/abs/2310.20620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evgeniia Tokarchuk, Vlad Niculae</li>
<li>for: 这个论文是关于连续输出神经机器翻译（CoNMT）的研究，它将推理下一个单词的问题转换为嵌入Prediction问题。</li>
<li>methods: 这篇论文使用了CoNMT模型，并对其中的输出嵌入进行了不同的设计和实现。</li>
<li>results: 研究发现，完全随机的输出嵌入可以超越了努力预训练的嵌入，特别是在大型数据集上。进一步的分析表明，这个效果强REATELY关注罕见单词的嵌入几何结构。<details>
<summary>Abstract</summary>
Continuous-output neural machine translation (CoNMT) replaces the discrete next-word prediction problem with an embedding prediction. The semantic structure of the target embedding space (i.e., closeness of related words) is intuitively believed to be crucial. We challenge this assumption and show that completely random output embeddings can outperform laboriously pretrained ones, especially on larger datasets. Further investigation shows this surprising effect is strongest for rare words, due to the geometry of their embeddings. We shed further light on this finding by designing a mixed strategy that combines random and pre-trained embeddings for different tokens.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Increasing-The-Performance-of-Cognitively-Inspired-Data-Efficient-Language-Models-via-Implicit-Structure-Building"><a href="#Increasing-The-Performance-of-Cognitively-Inspired-Data-Efficient-Language-Models-via-Implicit-Structure-Building" class="headerlink" title="Increasing The Performance of Cognitively Inspired Data-Efficient Language Models via Implicit Structure Building"></a>Increasing The Performance of Cognitively Inspired Data-Efficient Language Models via Implicit Structure Building</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20589">http://arxiv.org/abs/2310.20589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Momen, David Arps, Laura Kallmeyer</li>
<li>for: 这个论文描述了作者在2023年语言模型预训练挑战任务（BabyLM Challenge 2023）中的提交（Warstadt et al., 2023）。</li>
<li>methods: 作者使用 transformer 架构的做masked language model（MLM），并在模型结构中包含不supervised的句子结构预测。具体来说，作者使用 Structformer 架构（Shen et al., 2021）和其变种。 StructFormer 模型在有限的预训练数据上进行无监督语义推导，并在 vanilla transformer 架构上显示出性能提升（Shen et al., 2021）。</li>
<li>results: 作者对共39个任务进行评估，其中一些任务上，模型通过在建筑中添加层次偏好而提供了有望的改进，尤其是在一些特定任务上。然而，模型未能在所有任务上一致地超越提供的 RoBERTa 基线模型（BabyLM Challenge 2023）。<details>
<summary>Abstract</summary>
In this paper, we describe our submission to the BabyLM Challenge 2023 shared task on data-efficient language model (LM) pretraining (Warstadt et al., 2023). We train transformer-based masked language models that incorporate unsupervised predictions about hierarchical sentence structure into the model architecture. Concretely, we use the Structformer architecture (Shen et al., 2021) and variants thereof. StructFormer models have been shown to perform well on unsupervised syntactic induction based on limited pretraining data, and to yield performance improvements over a vanilla transformer architecture (Shen et al., 2021). Evaluation of our models on 39 tasks provided by the BabyLM challenge shows promising improvements of models that integrate a hierarchical bias into the architecture at some particular tasks, even though they fail to consistently outperform the RoBERTa baseline model provided by the shared task organizers on all tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了我们对2023年度 BabyLM 挑战任务中语言模型预训练（LM）的提交（Warstadt et al., 2023）。我们使用 transformer 基于的掩码语言模型，并在模型结构中包含无监督预测对层次句子结构的prediction。具体来说，我们使用 Structformer 架构（Shen et al., 2021）和其变种。StructFormer 模型在有限的预训练数据上进行无监督 синтаксический推导，并且在 vanilla transformer 架构上表现出良好的性能提升（Shen et al., 2021）。我们对 shared task 提供的 39 个任务进行评估，发现在某些任务上，将层次偏好 integrate 到架构中可以得到一定的改进，尤其是在一些特定任务上。然而，我们的模型未能在所有任务上 consistently 超过提供的 RoBERTa 基eline模型（提供者：shared task 组织者）。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Medical-Information-Retrieval-via-Knowledge-Graph-Embedding"><a href="#Zero-Shot-Medical-Information-Retrieval-via-Knowledge-Graph-Embedding" class="headerlink" title="Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding"></a>Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20588">http://arxiv.org/abs/2310.20588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqi Wang, Zeqiang Wang, Wei Wang, Qi Chen, Kaizhu Huang, Anh Nguyen, Suparna De</li>
<li>for: 本研究旨在提出一种适用于零 shot 医疗信息检索（MIR）的新方法，以便在互联网时代的医疗决策中提高效率。</li>
<li>methods: 该方法 combinesthe strengths of pre-trained语言模型和统计方法，同时解决他们的局限性。具体来说，该方法利用了预训练的 BERT-style 模型提取紧凑又有用的关键词。这些关键词然后通过与医学知识图中的概念实体联系而增强。</li>
<li>results: 对医疗数据集进行了实验评估，表明 MedFusionRank 的表现比现有方法更出色，具有多种评价指标的承诺。 MedFusionRank 能够快速和准确地检索 relevante 信息，包括从短或单个查询中检索到的信息。<details>
<summary>Abstract</summary>
In the era of the Internet of Things (IoT), the retrieval of relevant medical information has become essential for efficient clinical decision-making. This paper introduces MedFusionRank, a novel approach to zero-shot medical information retrieval (MIR) that combines the strengths of pre-trained language models and statistical methods while addressing their limitations. The proposed approach leverages a pre-trained BERT-style model to extract compact yet informative keywords. These keywords are then enriched with domain knowledge by linking them to conceptual entities within a medical knowledge graph. Experimental evaluations on medical datasets demonstrate MedFusion Rank's superior performance over existing methods, with promising results with a variety of evaluation metrics. MedFusionRank demonstrates efficacy in retrieving relevant information, even from short or single-term queries.
</details>
<details>
<summary>摘要</summary>
在互联网 OF Things（IoT）时代，医疗信息检索成为了效率医疗决策的重要组成部分。这篇论文介绍了MedFusionRank，一种新的零shot医疗信息检索（MIR）方法，该方法结合预训练的语言模型和统计方法，同时解决它们的局限性。提议的方法利用预训练的BERT样式模型提取紧凑又有用的关键词。这些关键词然后通过与医学知识图连接来增强域知识。实验评估医学数据集表明，MedFusionRank的表现胜过现有方法，具有多种评价指标的承诺性。MedFusionRank能够快速和高效地检索相关信息，即使从短或单个查询中检索。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Word-Guessing-Games-to-Assess-the-Intelligence-of-Large-Language-Models"><a href="#Leveraging-Word-Guessing-Games-to-Assess-the-Intelligence-of-Large-Language-Models" class="headerlink" title="Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models"></a>Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20499">http://arxiv.org/abs/2310.20499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Liang, Zhiwei He, Jen-tes Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, Xing Wang<br>for:  This paper aims to evaluate the intelligence of LLM-based agents using a word guessing game and a multi-agent framework called SpyGame.methods:  The proposed evaluation framework includes two components: DEEP, which requires LLMs to describe words in aggressive and conservative modes, and SpyGame, an interactive multi-agent framework that assesses LLMs’ linguistic skills and strategic thinking in a competitive language-based board game.results:  The proposed evaluation framework is effective in capturing the capabilities of various LLMs, including their ability to adapt to novel situations and engage in strategic communication. The authors conducted extensive experiments using words from multiple sources, domains, and languages.<details>
<summary>Abstract</summary>
The automatic evaluation of LLM-based agent intelligence is critical in developing advanced LLM-based agents. Although considerable effort has been devoted to developing human-annotated evaluation datasets, such as AlpacaEval, existing techniques are costly, time-consuming, and lack adaptability. In this paper, inspired by the popular language game ``Who is Spy'', we propose to use the word guessing game to assess the intelligence performance of LLMs. Given a word, the LLM is asked to describe the word and determine its identity (spy or not) based on its and other players' descriptions. Ideally, an advanced agent should possess the ability to accurately describe a given word using an aggressive description while concurrently maximizing confusion in the conservative description, enhancing its participation in the game. To this end, we first develop DEEP to evaluate LLMs' expression and disguising abilities. DEEP requires LLM to describe a word in aggressive and conservative modes. We then introduce SpyGame, an interactive multi-agent framework designed to assess LLMs' intelligence through participation in a competitive language-based board game. Incorporating multi-agent interaction, SpyGame requires the target LLM to possess linguistic skills and strategic thinking, providing a more comprehensive evaluation of LLMs' human-like cognitive abilities and adaptability in complex communication situations. The proposed evaluation framework is very easy to implement. We collected words from multiple sources, domains, and languages and used the proposed evaluation framework to conduct experiments. Extensive experiments demonstrate that the proposed DEEP and SpyGame effectively evaluate the capabilities of various LLMs, capturing their ability to adapt to novel situations and engage in strategic communication.
</details>
<details>
<summary>摘要</summary>
自动评估LLM基于代理智能是发展高级LLM的关键。虽然有很多努力投入到了人类标注评估数据集的开发，如AlpacaEval，但现有技术是成本高、时间费时、无法适应的。在这篇论文中，我们提出使用语言游戏“谁是间谍”来评估LLM的智能表现。给定一个词，LLM会被要求描述这个词，并确定它的身份（间谍或不）基于它和其他玩家的描述。理想的高级代理应该拥有精准描述给定词的能力，同时使用谩逼的描述和保守的描述，以增强其参与度。为此，我们首先开发了DEEP来评估LLM的表达和掩饰能力。DEEP需要LLM在谩逼和保守模式下描述一个词。然后，我们引入了SpyGame，一个交互式多代理框架，用于评估LLM的智能能力。SpyGame需要目标LLM具备语言技能和战略思维，从而提供更全面的评估LLM的人类智能能力和在复杂的语言通信 Situations中的适应能力。我们所提出的评估框架非常容易实现。我们收集了多个来源、领域和语言的词汇，并使用我们所提出的评估框架进行实验。广泛的实验结果表明，我们的DEEP和SpyGame可以有效评估不同LLM的能力，捕捉它们在新 Situations中的适应能力和策略性通信能力。
</details></li>
</ul>
<hr>
<h2 id="Multi-User-MultiWOZ-Task-Oriented-Dialogues-among-Multiple-Users"><a href="#Multi-User-MultiWOZ-Task-Oriented-Dialogues-among-Multiple-Users" class="headerlink" title="Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users"></a>Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20479">http://arxiv.org/abs/2310.20479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yohan Jo, Xinyan Zhao, Arijit Biswas, Nikoletta Basiou, Vincent Auvray, Nikolaos Malandrakis, Angeliki Metallinou, Alexandros Potamianos</li>
<li>for: 这个论文是为了提高多用户对话系统的开发而设计的。</li>
<li>methods: 这个论文使用了基于 MultiWOZ 2.2 的多用户对话数据，并提出了一种多用户上下文重写任务，以提高对话状态跟踪和对话系统的扩展。</li>
<li>results: 研究表明，使用预测重写可以在多用户对话中substantially提高对话状态跟踪，而不需要修改现有的对话系统。此外，这种方法还可以在未看过的领域中进行扩展。<details>
<summary>Abstract</summary>
While most task-oriented dialogues assume conversations between the agent and one user at a time, dialogue systems are increasingly expected to communicate with multiple users simultaneously who make decisions collaboratively. To facilitate development of such systems, we release the Multi-User MultiWOZ dataset: task-oriented dialogues among two users and one agent. To collect this dataset, each user utterance from MultiWOZ 2.2 was replaced with a small chat between two users that is semantically and pragmatically consistent with the original user utterance, thus resulting in the same dialogue state and system response. These dialogues reflect interesting dynamics of collaborative decision-making in task-oriented scenarios, e.g., social chatter and deliberation. Supported by this data, we propose the novel task of multi-user contextual query rewriting: to rewrite a task-oriented chat between two users as a concise task-oriented query that retains only task-relevant information and that is directly consumable by the dialogue system. We demonstrate that in multi-user dialogues, using predicted rewrites substantially improves dialogue state tracking without modifying existing dialogue systems that are trained for single-user dialogues. Further, this method surpasses training a medium-sized model directly on multi-user dialogues and generalizes to unseen domains.
</details>
<details>
<summary>摘要</summary>
而 Dialogue 系统却逐渐需要与多个用户同时交互，以便协作做出决策。为了推动这种系统的开发，我们发布了多用户 MultiWOZ 数据集：任务听力对话中两名用户和一个代理人进行交互。为了收集这个数据集，我们将 MultiWOZ 2.2 中每个用户说话的内容替换为两名用户之间的小聊天，保持semantically和pragmatically与原始用户说话相符，因此在对话状态和系统响应方面保持一致。这些对话反映了多用户协作任务决策场景中的社交交流和讨论。基于这些数据，我们提出了一项新任务：多用户上下文ual查询 rewrite。这项任务的目标是将多用户任务听力对话转换成简洁的任务听力查询，保留只有任务相关信息，并且可以直接被对话系统所接受。我们证明了，在多用户对话中使用预测 rewrite 可以大幅提高对话状态跟踪，而不需要修改现有的对话系统，这些系统已经用于单个用户对话。此外，这种方法在不同领域中 generalizes 并且超过了直接在多用户对话中训练一个中等大小的模型。
</details></li>
</ul>
<hr>
<h2 id="Representativeness-as-a-Forgotten-Lesson-for-Multilingual-and-Code-switched-Data-Collection-and-Preparation"><a href="#Representativeness-as-a-Forgotten-Lesson-for-Multilingual-and-Code-switched-Data-Collection-and-Preparation" class="headerlink" title="Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation"></a>Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20470">http://arxiv.org/abs/2310.20470</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Seza Doğruöz, Sunayana Sitaram, Zheng-Xin Yong</li>
<li>for: 这个论文的目的是调查现有的code-switching数据集（68个）中语言对的不均衡问题，以及数据采集和准备阶段的缺陷。</li>
<li>methods: 该研究采用了一种深入的分析方法，检查了不同语言对的数据集是否具备代表性，并发现了许多数据集忽略了地域、社会人口和注册变化的因素。</li>
<li>results: 研究发现，大多数CSW数据集忽略了其他语言对，并且存在数据采集和准备阶段的缺陷，导致数据集的代表性受到影响。<details>
<summary>Abstract</summary>
Multilingualism is widespread around the world and code-switching (CSW) is a common practice among different language pairs/tuples across locations and regions. However, there is still not much progress in building successful CSW systems, despite the recent advances in Massive Multilingual Language Models (MMLMs). We investigate the reasons behind this setback through a critical study about the existing CSW data sets (68) across language pairs in terms of the collection and preparation (e.g. transcription and annotation) stages. This in-depth analysis reveals that \textbf{a)} most CSW data involves English ignoring other language pairs/tuples \textbf{b)} there are flaws in terms of representativeness in data collection and preparation stages due to ignoring the location based, socio-demographic and register variation in CSW. In addition, lack of clarity on the data selection and filtering stages shadow the representativeness of CSW data sets. We conclude by providing a short check-list to improve the representativeness for forthcoming studies involving CSW data collection and preparation.
</details>
<details>
<summary>摘要</summary>
多语言主义在世界范围内广泛存在，并且代码转换（CSW）是不同语言对的常见实践。然而，建设成功的CSW系统还没有做出太多进步，尽管最近的质量大量多语言语言模型（MMLM）在提高。我们通过对现有CSW数据集（68）的抽查和检查来研究这一问题的原因。我们发现：a）大多数CSW数据集中英语占主导地位，忽视其他语言对。b）在收集和准备阶段存在不准确的表现， Ignoring location基础、社会民主和注册变化。此外，数据选择和筛选阶段的不清晰性使CSW数据集的表现性受到影响。我们 conclude by提供一份简短的检查列表，以提高将来CSW数据收集和准备阶段的表现性。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Deep-Understanding-of-Multilingual-End-to-End-Speech-Translation"><a href="#Towards-a-Deep-Understanding-of-Multilingual-End-to-End-Speech-Translation" class="headerlink" title="Towards a Deep Understanding of Multilingual End-to-End Speech Translation"></a>Towards a Deep Understanding of Multilingual End-to-End Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20456">http://arxiv.org/abs/2310.20456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Sun, Xiaohu Zhao, Yikun Lei, Shaolin Zhu, Deyi Xiong</li>
<li>for: 这个论文旨在分析一个多语言端到端语音翻译模型中所学习的表示，使用Singular Value Canonical Correlation Analysis (SVCCA)来掌握不同语言和层次之间的表示相似性。</li>
<li>methods: 这个论文使用了CoVoST 2 dataset中的所有可能的方向进行训练，并使用LASER提取并行的文本数据进行SVCCA分析。</li>
<li>results: 从分析中得到了三个主要发现：（I）在多语言语音翻译中，语言相似性在训练数据有限时失效；（II）在训练数据不受限制时，提高encoder表示和audio-text数据的对应性可以提高翻译质量，超过双语翻译；（III）多语言语音翻译encoder表示在语音学类型预测中表现出色。这些发现可以提议在多语言端到端语音翻译中释放限制了数据的约束，并将限制的语言与语言相似性高的高资源语言结合起来，以更有效地进行多语言端到端语音翻译。<details>
<summary>Abstract</summary>
In this paper, we employ Singular Value Canonical Correlation Analysis (SVCCA) to analyze representations learnt in a multilingual end-to-end speech translation model trained over 22 languages. SVCCA enables us to estimate representational similarity across languages and layers, enhancing our understanding of the functionality of multilingual speech translation and its potential connection to multilingual neural machine translation. The multilingual speech translation model is trained on the CoVoST 2 dataset in all possible directions, and we utilize LASER to extract parallel bitext data for SVCCA analysis. We derive three major findings from our analysis: (I) Linguistic similarity loses its efficacy in multilingual speech translation when the training data for a specific language is limited. (II) Enhanced encoder representations and well-aligned audio-text data significantly improve translation quality, surpassing the bilingual counterparts when the training data is not compromised. (III) The encoder representations of multilingual speech translation demonstrate superior performance in predicting phonetic features in linguistic typology prediction. With these findings, we propose that releasing the constraint of limited data for low-resource languages and subsequently combining them with linguistically related high-resource languages could offer a more effective approach for multilingual end-to-end speech translation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们利用协方差值幂分析（SVCCA）分析在22种语言的多语言端到端语音翻译模型中学习的表示。SVCCA使我们能够计算语言之间和层次之间的表示相似性，从而更好地理解多语言语音翻译的工作原理和可能与多语言神经机器翻译的联系。我们使用CoVoST 2 dataset进行全向训练，并使用LASER提取并行文本数据进行SVCCA分析。我们从分析中得到了以下三个主要发现：（I）在多语言语音翻译中，语言相似性的作用随着语言训练数据的减少而减弱。（II）在训练数据不受限制的情况下，增强encoder表示和与文本数据well-aligned可以大幅提高翻译质量，超过双语翻译器。（III）多语言语音翻译encoder表示在语言类型预测中表现出色，可以更好地预测语音特征。基于这些发现，我们提议在低资源语言的训练数据不受限制的情况下，将其与语言相似性高的高资源语言结合起来，可以实现更有效的多语言端到端语音翻译。
</details></li>
</ul>
<hr>
<h2 id="The-SourceData-NLP-dataset-integrating-curation-into-scientific-publishing-for-training-large-language-models"><a href="#The-SourceData-NLP-dataset-integrating-curation-into-scientific-publishing-for-training-large-language-models" class="headerlink" title="The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models"></a>The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20440">http://arxiv.org/abs/2310.20440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Abreu-Vicente, Hannah Sonntag, Thomas Eidens, Thomas Lemberger<br>for:* The paper is written to present a new dataset called SourceData-NLP, which is annotated with biomedical entities in figure legends.methods:* The authors use natural language processing (NLP) techniques, specifically named-entity recognition (NER) and named-entity linking (NEL), to annotate the biomedical entities in the dataset.results:* The SourceData-NLP dataset contains over 620,000 annotated biomedical entities from 18,689 figures in 3,223 papers in molecular and cell biology.* The authors fine-tune two transformer-based models, BioLinkBERT and PubmedBERT, on the SourceData-NLP dataset to assess their performance for NER.* The authors introduce a novel context-dependent semantic task that infers whether an entity is the target of a controlled intervention or the object of measurement.<details>
<summary>Abstract</summary>
Introduction: The scientific publishing landscape is expanding rapidly, creating challenges for researchers to stay up-to-date with the evolution of the literature. Natural Language Processing (NLP) has emerged as a potent approach to automating knowledge extraction from this vast amount of publications and preprints. Tasks such as Named-Entity Recognition (NER) and Named-Entity Linking (NEL), in conjunction with context-dependent semantic interpretation, offer promising and complementary approaches to extracting structured information and revealing key concepts.   Results: We present the SourceData-NLP dataset produced through the routine curation of papers during the publication process. A unique feature of this dataset is its emphasis on the annotation of bioentities in figure legends. We annotate eight classes of biomedical entities (small molecules, gene products, subcellular components, cell lines, cell types, tissues, organisms, and diseases), their role in the experimental design, and the nature of the experimental method as an additional class. SourceData-NLP contains more than 620,000 annotated biomedical entities, curated from 18,689 figures in 3,223 papers in molecular and cell biology. We illustrate the dataset's usefulness by assessing BioLinkBERT and PubmedBERT, two transformers-based models, fine-tuned on the SourceData-NLP dataset for NER. We also introduce a novel context-dependent semantic task that infers whether an entity is the target of a controlled intervention or the object of measurement.   Conclusions: SourceData-NLP's scale highlights the value of integrating curation into publishing. Models trained with SourceData-NLP will furthermore enable the development of tools able to extract causal hypotheses from the literature and assemble them into knowledge graphs.
</details>
<details>
<summary>摘要</summary>
引言：科学出版业在快速扩大，让研究人员很难以保持学术文献的更新。自然语言处理（NLP）已经成为自动提取知识的有效方法之一。任务 such as 命名实体识别（NER）和命名实体关联（NEL），以及与上下文相依的Semantic interpretation，可以帮助提取结构化信息并揭示关键概念。结果：我们提供了来自文献审核过程中的 SourceData-NLP 数据集。这个数据集的特点是强调在图文中注解生物实体。我们注解了8种生物实体类型（小分子、蛋白质、细胞组成部分、细胞系列、细胞类型、组织、生物体和疾病），它们在实验设计中的角色以及实验方法的性质作为一个额外的类别。 SourceData-NLP 包含超过 620,000 个注解的生物实体，从 18,689 张图文中精心审核。我们通过评估 BioLinkBERT 和 PubmedBERT，两个基于 transformers 的模型，在 SourceData-NLP 数据集上进行了 NER 的训练，并引入了一种新的上下文依存Semantic任务，该任务检测实体是否为控制 intervención 的目标或测量的对象。结论：SourceData-NLP 的规模强调了在出版过程中 integrating 审核的重要性。模型在 SourceData-NLP 数据集上训练后，将能够提取文献中的 causal 假设并将其组织成知识图。
</details></li>
</ul>
<hr>
<h2 id="FollowBench-A-Multi-level-Fine-grained-Constraints-Following-Benchmark-for-Large-Language-Models"><a href="#FollowBench-A-Multi-level-Fine-grained-Constraints-Following-Benchmark-for-Large-Language-Models" class="headerlink" title="FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models"></a>FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20410">http://arxiv.org/abs/2310.20410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, Wei Wang</li>
<li>For: 本研究为了评估大型自然语言模型（LLMs）的 instrucion following 能力，提出了 FollowBench 多级细化约束遵循 benchmark。* Methods: 本研究使用了五种不同类型的细化约束（内容、场景、风格、格式和示例），并引入了多级机制来逐步添加约束。* Results: 通过测试九种关键性开源和商业 LLMs 在 FollowBench 上的性能，研究发现了 LLMs 在 instrucion following 方面存在弱点，并指出了未来研究的可能性。I hope this helps!<details>
<summary>Abstract</summary>
The ability to follow instructions is crucial to Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating superficial response quality, which does not necessarily indicate instruction-following capability. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Scenario, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each level. To evaluate whether LLMs' outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint evolution paths to handle challenging semantic constraints. By evaluating nine closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.
</details>
<details>
<summary>摘要</summary>
LLMs 需要能够遵循指令是在实际应用中的关键能力。现有的标准准 mesure 主要关注表面上的响应质量，这并不一定是指令遵循能力。为了填补这个研究空白，在这篇论文中，我们提出了 FollowBench，一个多级细化要求 benchmark  для LLMs。FollowBench 包括了五种不同类型（即内容、情境、风格、格式和示例）的细化要求。为了准确地测量 LLMs 的指令遵循能力，我们引入了一种多级机制，逐级添加一个单独的指令要求到初始指令中。为了评估 LLMs 的输出是否满足每个个体要求，我们提议在 Constraint Evolution Path 上引入强大 LLMs 的输入。通过对九个关键 LLMs 在 FollowBench 上进行评估，我们揭示了 LLMs 在指令遵循方面的缺陷，并指向了未来研究的可能性。数据和代码在 GitHub 上公开可用，请参考 https://github.com/YJiangcm/FollowBench。
</details></li>
</ul>
<hr>
<h2 id="AMERICANO-Argument-Generation-with-Discourse-driven-Decomposition-and-Agent-Interaction"><a href="#AMERICANO-Argument-Generation-with-Discourse-driven-Decomposition-and-Agent-Interaction" class="headerlink" title="AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction"></a>AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20352">http://arxiv.org/abs/2310.20352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Hu, Hou Pong Chan, Yu Yin</li>
<li>for: 这 paper 的目的是提出一种基于对话的 argue generation 框架，以便更好地模拟人类的写作过程和提高语言模型的生成效果。</li>
<li>methods: 这 paper 使用了一种基于 argumentation 理论的分解方法，将生成过程分解为多个顺序执行的动作，首先生成了不同类型的论证组成部分，然后生成了基于这些部分的最终论证。此外，它还引入了一种自动评估和改进Argument drafts的Module，以便更好地模拟人类的写作过程。</li>
<li>results:  experiments 表明，该方法可以比 tradicional end-to-end 和 chain-of-thought prompting 方法更好地生成更 coherent 和更有力的论证，并且可以生成更多的多样化和富有的内容。<details>
<summary>Abstract</summary>
Argument generation is a challenging task in natural language processing, which requires rigorous reasoning and proper content organization. Inspired by recent chain-of-thought prompting that breaks down a complex task into intermediate steps, we propose Americano, a novel framework with agent interaction for argument generation. Our approach decomposes the generation process into sequential actions grounded on argumentation theory, which first executes actions sequentially to generate argumentative discourse components, and then produces a final argument conditioned on the components. To further mimic the human writing process and improve the left-to-right generation paradigm of current autoregressive language models, we introduce an argument refinement module which automatically evaluates and refines argument drafts based on feedback received. We evaluate our framework on the task of counterargument generation using a subset of Reddit/CMV dataset. The results show that our method outperforms both end-to-end and chain-of-thought prompting methods and can generate more coherent and persuasive arguments with diverse and rich contents.
</details>
<details>
<summary>摘要</summary>
Argument generation是自然语言处理中一项复杂的任务，需要严格的逻辑推理和正确的内容组织。启发于最近的链条思维提示，我们提出了Americano框架，一种基于代理交互的新方法。我们的方法将生成过程分解为顺序执行的Argumentation theory基础上的动作，先执行动作生成论证说话组件，然后生成基于组件的最终论证。为了更好地模拟人类写作过程和改进现有的左至右生成方法，我们引入了一个 argue refinement module，该模块会自动评估和修正Argument drafts基于反馈。我们使用Reddit/CMV数据集中的一个子集进行评估，结果显示，我们的方法在end-to-end和链条思维提示方法的基础上减少了更多的coherent和有说服力的论证，并且可以生成更多的多样化和 ric contents。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Generators-for-a-Family-of-Matrix-Multiplication-Routines-with-Apache-TVM"><a href="#Automatic-Generators-for-a-Family-of-Matrix-Multiplication-Routines-with-Apache-TVM" class="headerlink" title="Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM"></a>Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20347">http://arxiv.org/abs/2310.20347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Alaejos, Adrián Castelló, Pedro Alonso-Jordá, Francisco D. Igual, Héctor Martínez, Enrique S. Quintana-Ortí</li>
<li>for: 本研究旨在使用Apache TVM开源框架自动生成高性能的矩阵乘法（GEMM）算法家族，以满足高性能Linear Algebra库（如GotoBLAS2、BLIS和OpenBLAS）的要求。</li>
<li>methods: 本研究使用Apache TVM框架来自动生成矩阵乘法（GEMM）的块分形式算法和处理器特定微kernels，而不是手动编码Assembly代码来实现。</li>
<li>results: 结果表明，使用TVM生成的块分形式GEMM算法和微kernels可以提高可移植性、维护性和软件生命周期的可控性，同时提供高度的自定义和优化功能，以便适应不同的数据类型、处理器架构和矩阵操作形态，并且具有较小的内存占用。<details>
<summary>Abstract</summary>
We explore the utilization of the Apache TVM open source framework to automatically generate a family of algorithms that follow the approach taken by popular linear algebra libraries, such as GotoBLAS2, BLIS and OpenBLAS, in order to obtain high-performance blocked formulations of the general matrix multiplication (GEMM). % In addition, we fully automatize the generation process, by also leveraging the Apache TVM framework to derive a complete variety of the processor-specific micro-kernels for GEMM. This is in contrast with the convention in high performance libraries, which hand-encode a single micro-kernel per architecture using Assembly code. % In global, the combination of our TVM-generated blocked algorithms and micro-kernels for GEMM 1)~improves portability, maintainability and, globally, streamlines the software life cycle; 2)~provides high flexibility to easily tailor and optimize the solution to different data types, processor architectures, and matrix operand shapes, yielding performance on a par (or even superior for specific matrix shapes) with that of hand-tuned libraries; and 3)~features a small memory footprint.
</details>
<details>
<summary>摘要</summary>
The combination of our TVM-generated blocked algorithms and micro-kernels for GEMM provides several benefits:1. Improved portability, maintainability, and streamlined software development life cycle.2. High flexibility to easily tailor and optimize the solution to different data types, processor architectures, and matrix operand shapes, resulting in performance on par (or even superior for specific matrix shapes) with that of hand-tuned libraries.3. Small memory footprint.
</details></li>
</ul>
<hr>
<h2 id="InstructCoder-Empowering-Language-Models-for-Code-Editing"><a href="#InstructCoder-Empowering-Language-Models-for-Code-Editing" class="headerlink" title="InstructCoder: Empowering Language Models for Code Editing"></a>InstructCoder: Empowering Language Models for Code Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20329">http://arxiv.org/abs/2310.20329</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qishenghu/CodeInstruct">https://github.com/qishenghu/CodeInstruct</a></li>
<li>paper_authors: Qisheng Hu, Kaixin Li, Xu Zhao, Yuxi Xie, Tiedong Liu, Hui Chen, Qizhe Xie, Junxian He<br>for:这个论文的目的是探讨用大语言模型（LLMs）编辑代码的可能性，以满足开发者日常面临的各种实际任务需求。methods:这篇论文使用了大量的自然语言处理技术，包括语言模型和自然语言处理算法，以实现代码编辑。具体来说，作者们使用了 GitHub 提交记录作为种子任务，然后通过 ChatGPT 进行多轮扩展和训练，以生成更多的代码编辑任务。results:研究表明，通过对 InstructCoder 集合进行特定任务定制，可以使用 open-source LLMs 实现高效的代码编辑。实验结果表明，这些 LLMs 可以根据用户的指令正确地编辑代码，并且表现出了很好的编辑能力。这些结果表明，可以通过特定任务的指定来提高 LLMs 的代码编辑能力。<details>
<summary>Abstract</summary>
Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of large language models (LLMs) to edit code based on user instructions, covering a broad range of implicit tasks such as comment insertion, code optimization, and code refactoring. To facilitate this, we introduce InstructCoder, the first dataset designed to adapt LLMs for general-purpose code editing, containing highdiversity code-editing tasks. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The dataset is systematically expanded through an iterative process that commences with code editing data sourced from GitHub commits as seed tasks. Seed and generated tasks are used subsequently to prompt ChatGPT for more task data. Our experiments demonstrate that open-source LLMs fine-tuned on InstructCoder can edit code correctly based on users' instructions most of the time, exhibiting unprecedented code-editing performance levels. Such results suggest that proficient instruction-finetuning can lead to significant amelioration in code editing abilities. The dataset and the source code are available at https://github.com/qishenghu/CodeInstruct.
</details>
<details>
<summary>摘要</summary>
��лекс文本中的编程任务非常多样化，开发者日常面临着这些任务。尽管编程任务的实用性非常高，但是自动编程任务还是深度学习模型的发展中的一个未曾探索的领域，一个原因是数据的缺乏。在这项工作中，我们探索了使用大型自然语言模型（LLMs）来基于用户的指令进行代码编辑，包括评注插入、代码优化和代码重构等多种隐式任务。为了实现这一点，我们介绍了 InstructCoder，第一个适用于普通代码编辑的大型自然语言模型 dataset，包含高多样性的代码编辑任务。它包含了超过114,000个指令-输入-输出 triplets，覆盖了多个不同的代码编辑场景。我们通过一种系统的扩展过程，从 GitHub 提交中抽取代码编辑数据作为种子任务，然后使用这些种子和生成的任务来驱动 ChatGPT 生成更多的任务数据。我们的实验结果表明，使用 InstructCoder 进行特定任务的 instrucion-finetuning 可以使得基于用户的指令进行代码编辑，达到了前所未有的代码编辑性能水平。这些结果表明，高效的指令 fine-tuning 可以导致代码编辑技能的显著提高。 dataset 和源代码可以在 https://github.com/qishenghu/CodeInstruct 上下载。
</details></li>
</ul>
<hr>
<h2 id="ChiSCor-A-Corpus-of-Freely-Told-Fantasy-Stories-by-Dutch-Children-for-Computational-Linguistics-and-Cognitive-Science"><a href="#ChiSCor-A-Corpus-of-Freely-Told-Fantasy-Stories-by-Dutch-Children-for-Computational-Linguistics-and-Cognitive-Science" class="headerlink" title="ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science"></a>ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20328">http://arxiv.org/abs/2310.20328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bram M. A. van Dijk, Max J. van Duijn, Suzan Verberne, Marco R. Spruit</li>
<li>for: 这个论文是为了研究儿童如何表达角色的视角，以及语言和认知的发展。</li>
<li>methods: 这个论文使用了计算机工具，并使用自由告密的方式收集了442名荷兰儿童在4-12岁之间自由地告诉的619则幻想故事。</li>
<li>results: 这个研究发现，儿童的故事语法复杂性随着年龄的增长而变化不大，这表明儿童在语言使用中的语法能力快速发展。此外，研究还发现，儿童的故事语言遵循Zipf的法律，这反映了儿童在社会上的语言环境。最后，研究还发现，尽管这个论文的数据集较小，但它仍然能够训练有用的语言模型，用于分析儿童的语言使用。<details>
<summary>Abstract</summary>
In this resource paper we release ChiSCor, a new corpus containing 619 fantasy stories, told freely by 442 Dutch children aged 4-12. ChiSCor was compiled for studying how children render character perspectives, and unravelling language and cognition in development, with computational tools. Unlike existing resources, ChiSCor's stories were produced in natural contexts, in line with recent calls for more ecologically valid datasets. ChiSCor hosts text, audio, and annotations for character complexity and linguistic complexity. Additional metadata (e.g. education of caregivers) is available for one third of the Dutch children. ChiSCor also includes a small set of 62 English stories. This paper details how ChiSCor was compiled and shows its potential for future work with three brief case studies: i) we show that the syntactic complexity of stories is strikingly stable across children's ages; ii) we extend work on Zipfian distributions in free speech and show that ChiSCor obeys Zipf's law closely, reflecting its social context; iii) we show that even though ChiSCor is relatively small, the corpus is rich enough to train informative lemma vectors that allow us to analyse children's language use. We end with a reflection on the value of narrative datasets in computational linguistics.
</details>
<details>
<summary>摘要</summary>
在这份资源论文中，我们发布了新的虚构语料库---ChiSCor，包含619个幻想故事，由442名荷兰儿童 aged 4-12年old所自由地告诉。ChiSCor是为了研究儿童如何表达角色点视角，并探索语言和认知的发展，使用计算工具而编制的。与现有资源不同的是，ChiSCor的故事被生成在自然的上下文中，与最近的呼吁更多的生态学有效的数据集相符。ChiSCor包含文本、音频和注释，以及角色复杂性和语言复杂性的批注。一 third的荷兰儿童的教育背景信息也可以获取。ChiSCor还包括62个英语故事。这篇论文介绍了如何编制ChiSCor，并通过三个简要的案例研究表明了其潜在的应用前景：一、儿童的故事 sintactic complexity 随着年龄的变化呈现稳定的趋势；二、ChiSCor遵循Zipf's law  closely，归因其社会上下文；三、即使ChiSCor较小，仍可以用于训练有用的 lemma vector，以分析儿童语言使用。我们结束于计算语言学中虚构语料库的价值。
</details></li>
</ul>
<hr>
<h2 id="Erato-Automatizing-Poetry-Evaluation"><a href="#Erato-Automatizing-Poetry-Evaluation" class="headerlink" title="Erato: Automatizing Poetry Evaluation"></a>Erato: Automatizing Poetry Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20326">http://arxiv.org/abs/2310.20326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manexagirrezabal/erato">https://github.com/manexagirrezabal/erato</a></li>
<li>paper_authors: Manex Agirrezabal, Hugo Gonçalo Oliveira, Aitor Ormazabal</li>
<li>for: 这篇论文是为了自动评估诗歌而设计的框架。</li>
<li>methods: 这篇论文使用了多种特征来评价诗歌，并提供了这些特征的简要概述。</li>
<li>results: 通过使用Erato框架，可以准确地区分人工创作的诗歌和自动生成的诗歌。<details>
<summary>Abstract</summary>
We present Erato, a framework designed to facilitate the automated evaluation of poetry, including that generated by poetry generation systems. Our framework employs a diverse set of features, and we offer a brief overview of Erato's capabilities and its potential for expansion. Using Erato, we compare and contrast human-authored poetry with automatically-generated poetry, demonstrating its effectiveness in identifying key differences. Our implementation code and software are freely available under the GNU GPLv3 license.
</details>
<details>
<summary>摘要</summary>
我们介绍Erato框架，这是一个用于自动评估诗歌的框架，包括由诗歌生成系统生成的诗歌。我们的框架使用多种特征，我们将简要介绍Erato的能力和扩展可能性。使用Erato，我们比较和比较人工创作的诗歌和自动生成的诗歌，展示其关键区别。我们的实现代码和软件在GNU GPLv3许可证下提供免费使用。
</details></li>
</ul>
<hr>
<h2 id="FA-Team-at-the-NTCIR-17-UFO-Task"><a href="#FA-Team-at-the-NTCIR-17-UFO-Task" class="headerlink" title="FA Team at the NTCIR-17 UFO Task"></a>FA Team at the NTCIR-17 UFO Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20322">http://arxiv.org/abs/2310.20322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuki Okumura, Masato Fujitake</li>
<li>for: 本研究参与了NTCIR-17会议上的表格数据EXTRACTION（TDE）和文本到表格关系EXTRACTION（TTRE）任务，报告我们的解决方案以及官方结果。</li>
<li>methods: 我们采用了基于ELECTRA语言模型的多种提升技术来提取表格中的有价值数据。</li>
<li>results: 我们的努力取得了93.43%的TDE准确率，在排名榜上名列第二，这是我们提出的方法的效果的证明。<details>
<summary>Abstract</summary>
The FA team participated in the Table Data Extraction (TDE) and Text-to-Table Relationship Extraction (TTRE) tasks of the NTCIR-17 Understanding of Non-Financial Objects in Financial Reports (UFO). This paper reports our approach to solving the problems and discusses the official results. We successfully utilized various enhancement techniques based on the ELECTRA language model to extract valuable data from tables. Our efforts resulted in an impressive TDE accuracy rate of 93.43 %, positioning us in second place on the Leaderboard rankings. This outstanding achievement is a testament to our proposed approach's effectiveness. In the TTRE task, we proposed the rule-based method to extract meaningful relationships between the text and tables task and confirmed the performance.
</details>
<details>
<summary>摘要</summary>
FA团队参与了NTCIR-17年度理解非财务对象（UFO）的表格数据提取（TDE）和文本到表格关系提取（TTRE）任务。本文描述了我们解决这些问题的方法，并讨论官方结果。我们成功地运用了基于ELECTRA语言模型的各种优化技术，从表格中提取有价值数据。我们的努力得到了93.43%的TDE准确率，在排名榜上名列第二位。这一成就是我们提posed方法的有效性的证明。在TTRE任务中，我们提议了基于规则的方法来提取文本和表格之间的有意义关系，并证明了其性能。
</details></li>
</ul>
<hr>
<h2 id="Extracting-Entities-of-Interest-from-Comparative-Product-Reviews"><a href="#Extracting-Entities-of-Interest-from-Comparative-Product-Reviews" class="headerlink" title="Extracting Entities of Interest from Comparative Product Reviews"></a>Extracting Entities of Interest from Comparative Product Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20274">http://arxiv.org/abs/2310.20274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jatinarora2702/Review-Information-Extraction">https://github.com/jatinarora2702/Review-Information-Extraction</a></li>
<li>paper_authors: Jatin Arora, Sumit Agrawal, Pawan Goyal, Sayan Pathak</li>
<li>for: 本研究使用深度学习方法提取在多个电子商务网站上的用户评价中的产品比较信息。</li>
<li>methods: 本研究使用LSTM网络模型来捕捉用户评价中的产品名称、用户意见（预测）以及比较的特征或方面之间的依赖关系。</li>
<li>results: 对于现有的手动标注数据集，本研究的系统表现出比现有的Semantic Role Labeling（SRL）框架更好的性能。<details>
<summary>Abstract</summary>
This paper presents a deep learning based approach to extract product comparison information out of user reviews on various e-commerce websites. Any comparative product review has three major entities of information: the names of the products being compared, the user opinion (predicate) and the feature or aspect under comparison. All these informing entities are dependent on each other and bound by the rules of the language, in the review. We observe that their inter-dependencies can be captured well using LSTMs. We evaluate our system on existing manually labeled datasets and observe out-performance over the existing Semantic Role Labeling (SRL) framework popular for this task.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-to-Play-Chess-from-Textbooks-LEAP-a-Corpus-for-Evaluating-Chess-Moves-based-on-Sentiment-Analysis"><a href="#Learning-to-Play-Chess-from-Textbooks-LEAP-a-Corpus-for-Evaluating-Chess-Moves-based-on-Sentiment-Analysis" class="headerlink" title="Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis"></a>Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20260">http://arxiv.org/abs/2310.20260</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/resrepos/leap">https://github.com/resrepos/leap</a></li>
<li>paper_authors: Haifa Alrdahi, Riza Batista-Navarro<br>for: 这篇论文旨在探讨使用棋盘游戏教学的知识来帮助机器学习棋盘游戏。methods: 这篇论文使用了各种基于转换器的基eline模型进行情感分析，以评估棋盘游戏中的搬砖意见。results: 研究发现，使用转换器基eline模型进行情感分析可以评估棋盘游戏中的搬砖意见，最高performing模型在Weighted Micro F_1分数上达到68%。此外，研究还将LEAP corpus合并成一个更大的数据集，可以用于解决棋盘游戏领域的文本资源匮乏问题。<details>
<summary>Abstract</summary>
Learning chess strategies has been investigated widely, with most studies focussing on learning from previous games using search algorithms. Chess textbooks encapsulate grandmaster knowledge, explain playing strategies and require a smaller search space compared to traditional chess agents. This paper examines chess textbooks as a new knowledge source for enabling machines to learn how to play chess -- a resource that has not been explored previously. We developed the LEAP corpus, a first and new heterogeneous dataset with structured (chess move notations and board states) and unstructured data (textual descriptions) collected from a chess textbook containing 1164 sentences discussing strategic moves from 91 games. We firstly labelled the sentences based on their relevance, i.e., whether they are discussing a move. Each relevant sentence was then labelled according to its sentiment towards the described move. We performed empirical experiments that assess the performance of various transformer-based baseline models for sentiment analysis. Our results demonstrate the feasibility of employing transformer-based sentiment analysis models for evaluating chess moves, with the best performing model obtaining a weighted micro F_1 score of 68%. Finally, we synthesised the LEAP corpus to create a larger dataset, which can be used as a solution to the limited textual resource in the chess domain.
</details>
<details>
<summary>摘要</summary>
学习国际象棋策略已经广泛研究，大多数研究都是通过搜索算法学习前一些棋盘游戏。国际象棋教程汇集了大师知识，解释棋盘策略并需要较小的搜索空间，相比传统的国际象棋机器人。这篇论文检查国际象棋教程作为新的知识来源，使机器人学习棋盘游戏。我们开发了LEAP数据集，这是一个新的、多元数据集，包括结构化数据（棋盘转移notation和棋盘状态）和无结构数据（文本描述），从国际象棋教程中收集的1164句话，涵盖91场棋盘游戏的策略。我们首先将句子按照其相关性进行标注，即是否讨论棋盘转移。每个相关句子后来进行了情感向度标注，以判断对描述的转移的看法。我们进行了实验，用多种基于转换器的基准模型进行情感分析。我们的结果表明，使用转换器基于情感分析模型可以评估国际象棋转移的可行性，最好的模型在加权微型F_1分数上获得68%的成绩。最后，我们合并了LEAP数据集，创建了更大的数据集，可以用于解决国际象棋领域的文本资源短缺问题。
</details></li>
</ul>
<hr>
<h2 id="PsyCoT-Psychological-Questionnaire-as-Powerful-Chain-of-Thought-for-Personality-Detection"><a href="#PsyCoT-Psychological-Questionnaire-as-Powerful-Chain-of-Thought-for-Personality-Detection" class="headerlink" title="PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection"></a>PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20256">http://arxiv.org/abs/2310.20256</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taoyang225/psycot">https://github.com/taoyang225/psycot</a></li>
<li>paper_authors: Tao Yang, Tianyuan Shi, Fanqi Wan, Xiaojun Quan, Qifan Wang, Bingzhe Wu, Jiaxiang Wu</li>
<li>for: 这研究旨在探讨大语言模型（LLMs）在人性探测方面的潜在能力，以及如何通过 incorporating 心理测试问naire中的Item来提高LLMs的人性探测能力。</li>
<li>methods: 我们提出了一种新的人性探测方法，即PsyCoT，它通过在多turn对话中让AI助手（一个专门针对文本分析的LLM）评分心理测试问naire中的Item，以 derive 个人性倾向。</li>
<li>results: 我们的实验表明，PsyCoT可以明显提高GPT-3.5在人性探测任务中的性能和稳定性，相比标准提示方法，在两个 benchmark 数据集上平均提高F1分数4.23&#x2F;10.63点。<details>
<summary>Abstract</summary>
Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual's personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of well-structured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in text analysis. We prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference. Our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection, achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method. Our code is available at https://github.com/TaoYang225/PsyCoT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dynamically-Updating-Event-Representations-for-Temporal-Relation-Classification-with-Multi-category-Learning"><a href="#Dynamically-Updating-Event-Representations-for-Temporal-Relation-Classification-with-Multi-category-Learning" class="headerlink" title="Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning"></a>Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20236">http://arxiv.org/abs/2310.20236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Cheng, Masayuki Asahara, Ichiro Kobayashi, Sadao Kurohashi</li>
<li>for: 本研究的目的是提出一种能够处理多个 temporal link (TLINK) 类型的事件中心模型，以便在不同的时间和文档创建时间之间进行关系分类。</li>
<li>methods: 本研究使用的方法包括多任务学习和事件中心模型，以便利用整个数据集的信息。</li>
<li>results: 实验结果表明，本提案的模型在英文和日文数据集上都有更高的表现，比之前的模型和两个基eline模型。<details>
<summary>Abstract</summary>
Temporal relation classification is a pair-wise task for identifying the relation of a temporal link (TLINK) between two mentions, i.e. event, time, and document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs involving a common mention do not share information. 2) Existing models with independent classifiers for each TLINK category (E2E, E2T, and E2D) hinder from using the whole data. This paper presents an event centric model that allows to manage dynamic event representations across multiple TLINKs. Our model deals with three TLINK categories with multi-task learning to leverage the full size of data. The experimental results show that our proposal outperforms state-of-the-art models and two transfer learning baselines on both the English and Japanese data.
</details>
<details>
<summary>摘要</summary>
时间关系分类是一对一任务，用于确定两个提及（TLINK）之间的时间关系，即事件、时间和文档创建时间（DCT）之间的关系。这两个限制：1）两个TLINK中共享提及不能共享信息。2）现有的模型具有独立的类别 для每个TLINK类（E2E、E2T和E2D），这会阻碍使用整个数据集。本文介绍了一个事件中心模型，可以在多个TLINK之间管理动态事件表示。我们的模型处理三种TLINK类型，使用多任务学习来利用整个数据集。实验结果表明，我们的提议在英文和日文数据上比州前一个基eline和两个基eline表现出色。
</details></li>
</ul>
<hr>
<h2 id="General-Purpose-Retrieval-Enhanced-Medical-Prediction-Model-Using-Near-Infinite-History"><a href="#General-Purpose-Retrieval-Enhanced-Medical-Prediction-Model-Using-Near-Infinite-History" class="headerlink" title="General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History"></a>General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20204">http://arxiv.org/abs/2310.20204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/starmpcc/remed">https://github.com/starmpcc/remed</a></li>
<li>paper_authors: Junu Kim, Chaeeun Shim, Bosco Seong Kyu Yang, Chami Im, Sung Yoon Lim, Han-Gil Jeong, Edward Choi</li>
<li>for: 这个研究旨在开发基于电子健康记录 (EHR) 的临床预测模型 (e.g., 死亡预测)，并且解决专家意见选择和观察窗口大小调整的问题，这些问题会让临床专家受到沉重的负担。</li>
<li>methods: 我们提出了一个叫做 Retrieval-Enhanced Medical prediction model (REMed) 的方法，这个方法可以评估无限多个临床事件，选择相关的事件，并且做出预测。这种方法可以干预专家手动选择功能和观察窗口大小的限制，并且可以实现无限制的观察窗口。</li>
<li>results: 我们透过实验证明了 REMed 的性能，在 27 个临床任务和两个独立的资料集上，REMed 比其他同时处理多个事件的建筑架构更好。此外，我们发现 REMed 的偏好与医疗专家的偏好几乎相同。我们预计我们的方法将能够快速地减少临床专家对于模型开发的手动参与。<details>
<summary>Abstract</summary>
Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the development of EHR prediction models by minimizing clinicians' need for manual involvement.
</details>
<details>
<summary>摘要</summary>
通常来说，基于电子医疗记录（EHR）的临床预测模型（例如，死亡预测）的开发往往依赖于专家意见进行特征选择和观察窗口大小调整。这会负担专家和创造出开发过程中的瓶颈。我们提议了一种叫做Retrieval-Enhanced Medical prediction model（REMed），以解决这些挑战。REMed可以评估无数量的临床事件，选择相关的事件，并进行预测。这种方法可以减少专家的手动干预，并允许无限制的观察窗口。我们通过对27个临床任务和两个独立的EHR数据集进行实验，发现REMed在其他同时处理多个事件的建筑物上表现出色，并且与医疗专家的偏好相互吻合。我们预计，我们的方法将能够快速减少临床人员的手动参与度，从而快速发展EHR预测模型。
</details></li>
</ul>
<hr>
<h2 id="Video-Helpful-Multimodal-Machine-Translation"><a href="#Video-Helpful-Multimodal-Machine-Translation" class="headerlink" title="Video-Helpful Multimodal Machine Translation"></a>Video-Helpful Multimodal Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20201">http://arxiv.org/abs/2310.20201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ku-nlp/video-helpful-mmt">https://github.com/ku-nlp/video-helpful-mmt</a></li>
<li>paper_authors: Yihang Li, Shuichiro Shimizu, Chenhui Chu, Sadao Kurohashi, Wei Li</li>
<li>for: 本研究的目的是提高多modal机器翻译（MMT）的表现，使得机器翻译器能够更好地理解视频中的信息。</li>
<li>methods: 本研究使用了一个新的数据集——EVA（广泛的训练集和视频有助于评估集），以及一种新的模型——SAFA（选择注意力模型）。SAFA模型使用了两种新的方法：帧注意力损失和抽象增强。</li>
<li>results: 实验表明，视频信息和提议的方法可以提高翻译表现，而SAFA模型在EVA数据集上表现出色，与现有的MMT模型相比有所提高。<details>
<summary>Abstract</summary>
Existing multimodal machine translation (MMT) datasets consist of images and video captions or instructional video subtitles, which rarely contain linguistic ambiguity, making visual information ineffective in generating appropriate translations. Recent work has constructed an ambiguous subtitles dataset to alleviate this problem but is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA (Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En) parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In addition to the extensive training set, EVA contains a video-helpful evaluation set in which subtitles are ambiguous, and videos are guaranteed helpful for disambiguation. Furthermore, we propose SAFA, an MMT model based on the Selective Attention model with two novel methods: Frame attention loss and Ambiguity augmentation, aiming to use videos in EVA for disambiguation fully. Experiments on EVA show that visual information and the proposed methods can boost translation performance, and our model performs significantly better than existing MMT models. The EVA dataset and the SAFA model are available at: https://github.com/ku-nlp/video-helpful-MMT.git.
</details>
<details>
<summary>摘要</summary>
现有的多模态机器翻译（MMT）数据集包含图像和视频标题或教程视频字幕，这些内容很少含语言ambiguity，使得视觉信息在生成合适翻译时变得无效。 latest work constructed an ambiguous subtitles dataset to address this problem, but it is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA（Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation）， an MMT dataset containing 852k Japanese-English（Ja-En）parallel subtitle pairs, 520k Chinese-English（Zh-En）parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In addition to the extensive training set, EVA contains a video-helpful evaluation set in which subtitles are ambiguous, and videos are guaranteed helpful for disambiguation. Furthermore, we propose SAFA（Selective Attention with Frame attention loss and Ambiguity augmentation）， an MMT model based on the Selective Attention model with two novel methods, aiming to use videos in EVA for disambiguation fully. Experiments on EVA show that visual information and the proposed methods can boost translation performance, and our model performs significantly better than existing MMT models. The EVA dataset and the SAFA model are available at: https://github.com/ku-nlp/video-helpful-MMT.git.
</details></li>
</ul>
<hr>
<h2 id="DIVKNOWQA-Assessing-the-Reasoning-Ability-of-LLMs-via-Open-Domain-Question-Answering-over-Knowledge-Base-and-Text"><a href="#DIVKNOWQA-Assessing-the-Reasoning-Ability-of-LLMs-via-Open-Domain-Question-Answering-over-Knowledge-Base-and-Text" class="headerlink" title="DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text"></a>DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20170">http://arxiv.org/abs/2310.20170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenting Zhao, Ye Liu, Tong Niu, Yao Wan, Philip S. Yu, Shafiq Joty, Yingbo Zhou, Semih Yavuz</li>
<li>For: The paper aims to improve the grounding of large language models (LLMs) in external knowledge by leveraging retrieval-augmented models and heterogeneous knowledge sources, such as knowledge bases and text.* Methods: The paper proposes a novel approach that combines multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval, to ground LLMs in external knowledge. The approach leverages a comprehensive dataset of two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources.* Results: The paper demonstrates the effectiveness of the proposed approach by outperforming previous approaches on the created dataset, showing its ability to address the challenges of retrieving information from structured knowledge sources and generating symbolic queries.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; retrieving information from structured knowledge sources is a critical component in correctly answering the questions. (2) The generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge. Our dataset is created using a combination of automatic generation through predefined reasoning chains and human annotation. We also introduce a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval. Our model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GAR-meets-RAG-Paradigm-for-Zero-Shot-Information-Retrieval"><a href="#GAR-meets-RAG-Paradigm-for-Zero-Shot-Information-Retrieval" class="headerlink" title="GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval"></a>GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20158">http://arxiv.org/abs/2310.20158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daman Arora, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan, Gaurav Sinha, Amit Sharma</li>
<li>for: 这个论文的目的是提出一种新的生成增强检索（GAR）和重写增强检索（RAG）迭代方法，以解决现有方法中的两大挑战。</li>
<li>methods: 该方法使用生成增强检索（GAR）和重写增强检索（RAG）迭代方法，iteratively 改进检索和重写阶段，以提高系统的准确率和召回率。</li>
<li>results: 该方法在零 shot检索任务上实现了新的州OF-THE-ART，在 BEIR  benchmark 上的 Recall@100 和 nDCG@10 指标上出perform 前一个最好结果，在 6 个数据集上达到 17% 的相对提升。<details>
<summary>Abstract</summary>
Given a query and a document corpus, the information retrieval (IR) task is to output a ranked list of relevant documents. Combining large language models (LLMs) with embedding-based retrieval models, recent work shows promising results on the zero-shot retrieval problem, i.e., no access to labeled data from the target domain. Two such popular paradigms are generation-augmented retrieval or GAR (generate additional context for the query and then retrieve), and retrieval-augmented generation or RAG (retrieve relevant documents as context and then generate answers). The success of these paradigms hinges on (i) high-recall retrieval models, which are difficult to obtain in the zero-shot setting, and (ii) high-precision (re-)ranking models which typically need a good initialization. In this work, we propose a novel GAR-meets-RAG recurrence formulation that overcomes the challenges of existing paradigms. Our method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in the zero-shot setting. A key design principle is that the rewrite-retrieval stages improve the recall of the system and a final re-ranking stage improves the precision. We conduct extensive experiments on zero-shot passage retrieval benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in the BEIR benchmark, outperforming previous best results in Recall@100 and nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the previous best.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:给一个查询和一个文档集合，信息检索（IR）任务是输出一个 relevance 列表中的文档。通过结合大型自然语言模型（LLM）和嵌入式检索模型，现有的工作显示在零shot检索问题上的承袭性，即没有访问目标领域的标注数据。两种流行的方法是生成增强检索（GAR）和检索增强生成（RAG）。这两种方法的成功取决于（i）高精度检索模型，这些模型在零shot Setting 中很难实现，和（ii）高精度（重新）排名模型，这些模型通常需要良好的初始化。在这个工作中，我们提出一种 GAR-meets-RAG 复合形式，该形式可以超越现有的方法。我们的方法通过循环提高检索（via GAR）和重写（via RAG）阶段，以提高系统的回归。一个关键的设计原则是， rewrite-retrieval 阶段可以提高系统的回归，而 final 重新排名阶段可以提高系统的精度。我们在 BEIR 和 TREC-DL zero-shot 过程中文档检索标准 benchmark 上进行了广泛的实验。我们的方法在 BEIR benchmark 中创造了新的状态纪录，超越了之前的最佳结果，在 Recall@100 和 nDCG@10  метриках上，在 6 个数据集上出现了17%的相对提升。
</details></li>
</ul>
<hr>
<h2 id="Multi-Agent-Consensus-Seeking-via-Large-Language-Models"><a href="#Multi-Agent-Consensus-Seeking-via-Large-Language-Models" class="headerlink" title="Multi-Agent Consensus Seeking via Large Language Models"></a>Multi-Agent Consensus Seeking via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20151">http://arxiv.org/abs/2310.20151</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huaben Chen, Wenkang Ji, Lufeng Xu, Shiyu Zhao</li>
<li>for: 这个研究的目的是研究 LLM 驱动多智能体系统中的协同决策过程，特别是多智能体之间的协商过程。</li>
<li>methods: 本研究使用了 LLM 驱动的多智能体系统来实现协同决策。在实验中，研究者使用了不同的智能体数量、智能体个性和网络拓扑来影响协商过程。</li>
<li>results: 研究发现，不同智能体数量、智能体个性和网络拓扑的影响下，LLM 驱动的多智能体系统可以尝试不同的协商策略，主要是使用平均策略来达成协同决策。此外，本研究还应用了 LLM 驱动的协同决策在多机器人汇聚任务中，显示了这种方法的潜在应用前景。<details>
<summary>Abstract</summary>
Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-agent systems for solving more complex tasks. Furthermore, LLM-driven consensus seeking is applied to a multi-robot aggregation task. This application demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks. Project website: westlakeintelligentrobotics.github.io/ConsensusLLM/.
</details>
<details>
<summary>摘要</summary>
多智能体系驱动 by 大语言模型（LLM）已经显示了解决复杂任务的潜力。这项工作考虑了多智能体协同作业中的基本问题——协同决策。当多个智能体合作时，我们关心如何使其达成一致。为此，这项工作研究了智能体之间的协同决策任务，智能体的状态为数字值，他们之间进行协商来达成一致值。研究发现，当没有直接指导哪种策略应该采取时，LLM驱动的智能体主要采用平均策略 для协同决策，尽管它们可能 occasionally 采用其他策略。此外，这项工作分析了智能体数量、智能体个性和网络结构对协同决策过程的影响。报告的发现可能为 LLM 驱动多智能体系统解决更复杂任务提供基础知识。此外，LLM 驱动的协同决策还应用于多机器人聚合任务，这种应用示例了 LLM 驱动智能体可以实现零shot 自主规划的多机器人协同任务。项目网站：westlakeintelligentrobotics.github.io/ConsensusLLM/.
</details></li>
</ul>
<hr>
<h2 id="DEPN-Detecting-and-Editing-Privacy-Neurons-in-Pretrained-Language-Models"><a href="#DEPN-Detecting-and-Editing-Privacy-Neurons-in-Pretrained-Language-Models" class="headerlink" title="DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models"></a>DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20138">http://arxiv.org/abs/2310.20138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong</li>
<li>for: 防止预训练语言模型中的数据泄露</li>
<li>methods: 提出了DEPN框架，包括私钥神经检测器和私钥神经补做器，以及批处理私钥神经聚合器</li>
<li>results: 实验结果表明，我们的方法可以有效减少预训练语言模型中的数据泄露，而不会影响模型性能。此外，我们还证明了模型记忆和私钥神经之间的关系，从多个角度进行了证明。<details>
<summary>Abstract</summary>
Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach.
</details>
<details>
<summary>摘要</summary>
大型语言模型在训练数据大量中获取了丰富的知识和信息。这些训练数据中的数据内嵌和重复能力，在先前的研究中曝光了数据泄露的风险。为了有效减少这些风险，我们提出了DEPN框架，用于检测和修改预先训练的语言模型中的隐私神经。在DEPN中，我们提出了一个称为隐私神经检测器的新方法，用于找出具有隐私信息的神经，然后将这些检测到的隐私神经设置为零。此外，我们提出了一个隐私神经聚合器，用于批处理中减少隐私信息的泄露。实验结果显示，我们的方法可以有效和高效地减少隐私泄露，不会对模型的性能造成影响。此外，我们在多种角度进行了empirical研究，包括模型大小、训练时间、提示、隐私神经分布，证明了我们的方法的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Prompt-Tuning-with-Learned-Prompting-Layers"><a href="#Improving-Prompt-Tuning-with-Learned-Prompting-Layers" class="headerlink" title="Improving Prompt Tuning with Learned Prompting Layers"></a>Improving Prompt Tuning with Learned Prompting Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20127">http://arxiv.org/abs/2310.20127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhu, Ming Tan</li>
<li>for: 这个研究旨在提高预读模型（PTM）的适应性，并且可以在几个实验中实现更好的表现。</li>
<li>methods: 这个研究使用了一个新的框架，即选择性预读对应（SPT），它可以自动选择适当的预读层，并且可以透过一个可学的概率闸来控制预读层的选择。</li>
<li>results: 这个研究的结果显示，使用这个新的框架可以在十个标准 benchmark 数据集下进行全量和几个shot enario下实现更好的表现，并且可以与之前的现有的 PETuning 基eline 相比，具有更好的适应性和更少的可调参数。<details>
<summary>Abstract</summary>
Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In this work, we propose a novel framework, \underline{S}elective \underline{P}rompt \underline{T}uning (SPT), that learns to select the proper prompt layers by inserting a prompt controlled by a learnable probabilistic gate at each intermediate layer. We further propose a novel bi-level optimization framework, SPT-DARTS, that can better optimize the learnable gates and improve the final prompt tuning performances of the learned prompt layer settings. We conduct extensive experiments with ten benchmark datasets under the full-data and few-shot scenarios. The results demonstrate that our SPT framework can perform better than the previous state-of-the-art PETuning baselines with comparable or fewer tunable parameters.
</details>
<details>
<summary>摘要</summary>
Prompt tuning 可以在输入嵌入或隐藏状态之前添加软提示，并且只是优化提示来使得预训练模型（PTM）适应下游任务。前一些工作手动选择提示层，这些层远离优化的选择，而且没有充分利用提示调整的潜力。在这项工作中，我们提出了一个新的框架，称为选择性提示调整（SPT），它可以通过在每个中间层插入一个可控的概率门来学习选择合适的提示层。我们还提出了一个新的两级优化框架，称为 SPT-DARTS，它可以更好地优化学习的门和提高最终提示调整的性能。我们在10个标准测试集上进行了广泛的实验，包括全数据和少量数据场景。结果表明，我们的 SPT 框架可以比前一些state-of-the-art PETuning 基elines better，并且相对较少的可调参数。
</details></li>
</ul>
<hr>
<h2 id="Ling-CL-Understanding-NLP-Models-through-Linguistic-Curricula"><a href="#Ling-CL-Understanding-NLP-Models-through-Linguistic-Curricula" class="headerlink" title="Ling-CL: Understanding NLP Models through Linguistic Curricula"></a>Ling-CL: Understanding NLP Models through Linguistic Curricula</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20121">http://arxiv.org/abs/2310.20121</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clu-uml/ling-cl">https://github.com/clu-uml/ling-cl</a></li>
<li>paper_authors: Mohamed Elgaar, Hadi Amiri</li>
<li>for: 本研究旨在开发基于数据驱动的课程，以优化NLPTasks的解决方案，并考虑语言复杂性。</li>
<li>methods: 本研究使用心理语言学和语言学习研究中的语言复杂性特征来开发数据驱动课程，并分析了多个标准NLPTasks数据集，以确定每个任务所需的语言知识和推理能力。</li>
<li>results: 本研究发现，通过分析多个标准NLPTasks数据集，可以开发出基于数据的语言复杂性课程，这些课程可以帮助模型学习NLPTasks中的语言知识和推理能力。此外，本研究还提出了一些关于金标准和公平评价在NLP领域的问题。<details>
<summary>Abstract</summary>
We employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic curricula derived from data, existing knowledge about linguistic complexity, and model behavior during training. By analyzing several benchmark NLP datasets, our curriculum learning approaches identify sets of linguistic metrics (indices) that inform the challenges and reasoning required to address each task. Our work will inform future research in all NLP areas, allowing linguistic complexity to be considered early in the research and development process. In addition, our work prompts an examination of gold standards and fair evaluation in NLP.
</details>
<details>
<summary>摘要</summary>
Transliteration:我们使用心理语言学和语言学习研究中的语言复杂性特征来开发数据驱动的课程，以便理解模型在处理NLPT任务时学习的基础语言知识。我们的创新在于基于数据、现有的语言复杂性知识以及模型训练过程中的行为来开发语言课程。通过分析多个benchmark NLPdataset，我们的课程学习方法可以确定每个任务的语言指标（指标），这些指标将告诉我们处理每个任务所需的挑战和逻辑。我们的工作将对所有NLPT领域的研究提供指导，让语言复杂性在研究和开发过程中得到考虑。此外，我们的工作也会让人们对NLPT中的标准和公平评价进行反思。
</details></li>
</ul>
<hr>
<h2 id="Making-Large-Language-Models-Better-Data-Creators"><a href="#Making-Large-Language-Models-Better-Data-Creators" class="headerlink" title="Making Large Language Models Better Data Creators"></a>Making Large Language Models Better Data Creators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20111">http://arxiv.org/abs/2310.20111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/llm-data-creation">https://github.com/microsoft/llm-data-creation</a></li>
<li>paper_authors: Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen W. White, Sujay Kumar Jauhar</li>
<li>for: 这篇论文目的是提出一个统一的数据创建管线，以减少人工标注的努力，并且可以应用到许多任务中，包括传统上困难的任务。</li>
<li>methods: 这篇论文使用了一些技术来创建数据，包括使用 Label-free 的 LLMs 进行标注，以及使用对应的 prompt 进行数据生成。</li>
<li>results: 在实验中，这篇论文显示了使用这些方法可以创建出高品质的数据，并且模型训练使用这些数据后的性能与人工标注的模型相比，有17.5%的提升。这些结果有重要的实际应用，尤其是在 NLP 系统的Robustness 方面。<details>
<summary>Abstract</summary>
Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. In our experiments we demonstrate that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a unified data creation pipeline that requires only a single formatting example and is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. Our experiments show that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.
</details></li>
</ul>
<hr>
<h2 id="Keyword-optimized-Template-Insertion-for-Clinical-Information-Extraction-via-Prompt-based-Learning"><a href="#Keyword-optimized-Template-Insertion-for-Clinical-Information-Extraction-via-Prompt-based-Learning" class="headerlink" title="Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning"></a>Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20089">http://arxiv.org/abs/2310.20089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugenia Alleva, Isotta Landi, Leslee J Shaw, Erwin Böttinger, Thomas J Fuchs, Ipek Ensari</li>
<li>for: 这篇论文的目的是研究临床医疗自然语言处理（NLP）任务中的文本分类问题，并且考虑到现有的标注数据集罕至。</li>
<li>methods: 这篇论文使用了提示基本学习（Prompt-based learning）方法，并且开发了一个关键字最佳化的模板插入方法（Keyword-optimized template insertion method，KOTI），以便在零到几个训练例子下适应文本分类任务。</li>
<li>results: 这篇论文的结果显示，通过优化模板位置可以提高临床医疗文本分类任务中的性能，特别是在零到几个训练例子下。<details>
<summary>Abstract</summary>
Clinical note classification is a common clinical NLP task. However, annotated data-sets are scarse. Prompt-based learning has recently emerged as an effective method to adapt pre-trained models for text classification using only few training examples. A critical component of prompt design is the definition of the template (i.e. prompt text). The effect of template position, however, has been insufficiently investigated. This seems particularly important in the clinical setting, where task-relevant information is usually sparse in clinical notes. In this study we develop a keyword-optimized template insertion method (KOTI) and show how optimizing position can improve performance on several clinical tasks in a zero-shot and few-shot training setting.
</details>
<details>
<summary>摘要</summary>
临床笔记分类是一个常见的临床自然语言处理任务。然而，标注数据集很少。推荐学习最近在使用只需几个训练示例进行文本分类中得到了显著的成果。定义模板（即提示文本）是推荐学习中的关键组件。然而，模板位置的影响尚未得到充分调查。这 particualry重要在医疗设置下，因为任务相关的信息通常是临床笔记中罕见的。本研究我们开发了关键词优化模板插入方法（KOTI），并在零shot和几shot训练设置下展示了改进性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.CL_2023_10_31/" data-id="clohum96600dcpj88cz862qj3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.LG_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T10:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/31/cs.LG_2023_10_31/">cs.LG - 2023-10-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Best-of-Both-Worlds-Stochastic-and-Adversarial-Convex-Function-Chasing"><a href="#Best-of-Both-Worlds-Stochastic-and-Adversarial-Convex-Function-Chasing" class="headerlink" title="Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing"></a>Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00181">http://arxiv.org/abs/2311.00181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neelkamal Bhuyan, Debankur Mukherjee, Adam Wierman</li>
<li>for: 这个论文研究了在随机和敌对环境中的凸函数追踪（CFC）问题，提供了同时在两个设定下实现性能保证的算法。</li>
<li>methods: 论文使用了随机框架来研究CFC问题，并提供了对随机和敌对环境的性能保证。</li>
<li>results: 论文表明了在随机和敌对环境中， adversarial-optimal 算法在随机场景中表现不佳，而提供了一种 best-of-both-worlds 算法，可以同时实现robust敌对性和近似随机性的性能保证。<details>
<summary>Abstract</summary>
Convex function chasing (CFC) is an online optimization problem in which during each round $t$, a player plays an action $x_t$ in response to a hitting cost $f_t(x_t)$ and an additional cost of $c(x_t,x_{t-1})$ for switching actions. We study the CFC problem in stochastic and adversarial environments, giving algorithms that achieve performance guarantees simultaneously in both settings. Specifically, we consider the squared $\ell_2$-norm switching costs and a broad class of quadratic hitting costs for which the sequence of minimizers either forms a martingale or is chosen adversarially. This is the first work that studies the CFC problem using a stochastic framework. We provide a characterization of the optimal stochastic online algorithm and, drawing a comparison between the stochastic and adversarial scenarios, we demonstrate that the adversarial-optimal algorithm exhibits suboptimal performance in the stochastic context. Motivated by this, we provide a best-of-both-worlds algorithm that obtains robust adversarial performance while simultaneously achieving near-optimal stochastic performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本转换为简化中文：<</SYS>>凹函数追踪（CFC）是一个在线优化问题，每个回合 $t$，玩家会选择一个动作 $x_t$，对于每个回合 $t$，玩家会面临一个攻击成本 $f_t(x_t)$ 和一个Switching成本 $c(x_t,x_{t-1})$。我们研究了CFC问题在随机和敌意环境中，提供了一些算法，可以同时在两个设定中获得性能保证。特别是，我们考虑了平方型 $\ell_2$  нор switching成本，以及一种广泛的quadratic hitting costs，其中序列的最小值 either forms a martingale or is chosen adversarially。这是首次研究CFC问题使用随机 frameworks。我们提供了优化的随机在线算法的特征化，并在随机和敌意两个设定之间进行比较，我们发现了对于随机场景，敌意优化算法的性能不佳。这种情况下，我们提供了一个best-of-both-worlds算法，可以同时实现对敌意性能的Robustness和随机场景中的近似优化性能。
</details></li>
</ul>
<hr>
<h2 id="The-Alignment-Ceiling-Objective-Mismatch-in-Reinforcement-Learning-from-Human-Feedback"><a href="#The-Alignment-Ceiling-Objective-Mismatch-in-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback"></a>The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00168">http://arxiv.org/abs/2311.00168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Lambert, Roberto Calandra</li>
<li>for: 这篇论文旨在解决人工反馈学习（RLHF）中的目标不一致问题，以便使大语言模型（LLM）更容易提示和在复杂情况下更有能力。</li>
<li>methods: 这篇论文使用了人工反馈学习（RLHF）技术，它提供了一种新的工具来优化大语言模型（LLM），而不是只是下一个字符预测。</li>
<li>results: 这篇论文发现，RLHF中的目标不一致问题可能导致模型避免用户请求，困难带动特征，或者总是回答在特定风格下。通过解决RLHF中的目标不一致问题，未来的LLM将更 preciselly遵循用户指令，以确保安全和有用性。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to prompt and more capable in complex settings. RLHF at its core is providing a new toolkit to optimize LLMs other than next-token prediction, enabling the integration of qualitative training goals. The attempted match between user preferences and downstream performance, which happens in a learned reward model, results in an optimization landscape where training and evaluation metrics can appear correlated. The apparent correlation can lead to unexpected behaviors and stories of "too much RLHF." In RLHF, challenges emerge because the following sub-modules are not consistent with each other: the reward model training, the policy model training, and the policy model evaluation. This mismatch results in models that sometimes avoid user requests for false safety flags, are difficult to steer to an intended characteristic, or always answer in a specific style. As chat model evaluation becomes increasingly nuanced, the reliance on a perceived link between reward model score and downstream performance drives the objective mismatch issue. In this paper, we illustrate the cause of this issue, reviewing relevant literature from model-based reinforcement learning, and discuss relevant solutions to encourage further research. By solving objective mismatch in RLHF, the LLMs of the future will be more precisely aligned to user instructions for both safety and helpfulness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Graph-Neural-Networks-for-Road-Safety-Modeling-Datasets-and-Evaluations-for-Accident-Analysis"><a href="#Graph-Neural-Networks-for-Road-Safety-Modeling-Datasets-and-Evaluations-for-Accident-Analysis" class="headerlink" title="Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis"></a>Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00164">http://arxiv.org/abs/2311.00164</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/virtuosoresearch/ml4roadsafety">https://github.com/virtuosoresearch/ml4roadsafety</a></li>
<li>paper_authors: Abhinav Nippani, Dongyue Li, Haotian Ju, Haris N. Koutsopoulos, Hongyang R. Zhang</li>
<li>for: 本文针对道路网络上的交通事故分析进行研究，使用历史记录来预测交通事故发生的可能性。</li>
<li>methods: 本文使用了多种深度学习方法，包括图神经网络（GraphSAGE），并通过多任务学习和转移学习来考虑州际差异和交通量的影响。</li>
<li>results: 研究发现，使用图神经网络可以准确预测道路上的交通事故数量， Mean Absolute Error 低于 22%，并且可以预测事故发生或不发生的可能性高于 87%。<details>
<summary>Abstract</summary>
We consider the problem of traffic accident analysis on a road network based on road network connections and traffic volume. Previous works have designed various deep-learning methods using historical records to predict traffic accident occurrences. However, there is a lack of consensus on how accurate existing methods are, and a fundamental issue is the lack of public accident datasets for comprehensive evaluations. This paper constructs a large-scale, unified dataset of traffic accident records from official reports of various states in the US, totaling 9 million records, accompanied by road networks and traffic volume reports. Using this new dataset, we evaluate existing deep-learning methods for predicting the occurrence of accidents on road networks. Our main finding is that graph neural networks such as GraphSAGE can accurately predict the number of accidents on roads with less than 22% mean absolute error (relative to the actual count) and whether an accident will occur or not with over 87% AUROC, averaged over states. We achieve these results by using multitask learning to account for cross-state variabilities (e.g., availability of accident labels) and transfer learning to combine traffic volume with accident prediction. Ablation studies highlight the importance of road graph-structural features, amongst other features. Lastly, we discuss the implications of the analysis and develop a package for easily using our new dataset.
</details>
<details>
<summary>摘要</summary>
我们考虑了基于公路网络的交通事故分析，包括公路网络连接和交通量。过去的工作已经设计了各种深度学习方法，用于预测交通事故发生。然而，现有的方法准确性没有达成共识，并且存在公共交通事故数据集的缺乏，导致全面评估不可能。本文构建了美国各州官方报告中的交通事故记录数据集，总计900万记录，同时包括公路网络和交通量报告。使用这个新的数据集，我们评估了现有的深度学习方法，预测公路网络上事故的发生。我们的主要发现是，图 neural network 如 GraphSAGE 可以准确预测公路网络上事故的数量， mean absolute error 低于 22%，并且可以预测事故是否发生，AUROC 高于 87%，平均值为州。我们实现了这些结果通过多任务学习，考虑到cross-state variability（例如事故标签的可用性），并通过传输学习将交通量与事故预测结合。我们的分析结果表明，路网结构特征是预测事故的重要因素之一。最后，我们讨论了分析结果的意义，并开发了一个包可以方便地使用我们的新数据集。
</details></li>
</ul>
<hr>
<h2 id="Neuroformer-Multimodal-and-Multitask-Generative-Pretraining-for-Brain-Data"><a href="#Neuroformer-Multimodal-and-Multitask-Generative-Pretraining-for-Brain-Data" class="headerlink" title="Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data"></a>Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00136">http://arxiv.org/abs/2311.00136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonis Antoniades, Yiyi Yu, Joseph Canzano, William Wang, Spencer LaVere Smith</li>
<li>for: 这个论文是为了处理大规模神经科学实验数据而设计的，以便更好地分析和理解神经系统中的观察结果。</li>
<li>methods: 这个论文使用了一种名为Neuroformer的多模态、多任务生成预训练变换器（GPT）模型，该模型可以处理大规模的神经科学数据，并且可以自适应下游任务，如预测行为。</li>
<li>results: 在模拟数据集上进行训练后，Neuroformer模型可以准确预测神经细胞活动，同时还可以自动推导神经细胞之间的连接关系，包括方向。当用于解oding神经响应时，模型只需几个批量微调，就能准确预测鼠类的行为，这表明模型可以直接从神经表示中学习行为和神经表示之间的关系。<details>
<summary>Abstract</summary>
State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mouse with only few-shot fine-tuning, suggesting that the model begins learning how to do so directly from the neural representations themselves, without any explicit supervision. We used an ablation study to show that joint training on neuronal responses and behavior boosted performance, highlighting the model's ability to associate behavioral and neural representations in an unsupervised manner. These findings show that Neuroformer can analyze neural datasets and their emergent properties, informing the development of models and hypotheses associated with the brain.
</details>
<details>
<summary>摘要</summary>
现代系统神经科学实验受到大规模多模态数据的挑战，需要新的分析工具。启发于视觉和语言领域的大规模预训练模型的成功，我们将大规模、细胞分辨率神经细胞活动数据分析转化为自适应的空间时间生成问题。Neuroformer是一种多模态多任务生成预训练变换（GPT）模型，专门针对系统神经科学数据处理。它的特点是线性增长，可处理任意数量的模态，并且可适应下游任务，如预测行为。我们首先在模拟数据集上训练Neuroformer，发现它不仅准确预测模拟神经细胞活动，还能自动推理出下游神经细胞连接性，包括方向。当用于解码神经响应时，模型只需几个批次微调，就可以预测鼠标的行为，表明模型直接从神经表示中学习行为，无需显式监督。我们使用减少研究来证明，在同时训练神经响应和行为时，模型表现得更好， highlighting the model's ability to associate behavioral and neural representations in an unsupervised manner。这些发现表明Neuroformer可以分析神经数据和其emergent properties，为神经科学发展模型和假设提供参考。
</details></li>
</ul>
<hr>
<h2 id="Extracting-the-Multiscale-Causal-Backbone-of-Brain-Dynamics"><a href="#Extracting-the-Multiscale-Causal-Backbone-of-Brain-Dynamics" class="headerlink" title="Extracting the Multiscale Causal Backbone of Brain Dynamics"></a>Extracting the Multiscale Causal Backbone of Brain Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00118">http://arxiv.org/abs/2311.00118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/officiallydac/cb">https://github.com/officiallydac/cb</a></li>
<li>paper_authors: Gabriele D’Acunto, Francesco Bonchi, Gianmarco De Francisci Morales, Giovanni Petri</li>
<li>for: 这研究旨在提出一种基于多尺度 causal 结构学习的方法来捕捉大脑动态的多尺度 causal 脊梁（MCB），并在实际数据上进行了评估。</li>
<li>methods: 这种方法利用了最新的多尺度 causal 结构学习的进步，并且可以有效地考虑模型的 Complexity-Fitting 质量。</li>
<li>results: 研究发现，使用这种方法可以从resting-state fMRI数据中提取出稀疏的 MCB，并且在不同的时频带中发现了不同的 causal 动力。同时，这种方法还可以支持从 causal 角度出发的大脑连接指纹研究。<details>
<summary>Abstract</summary>
The bulk of the research effort on brain connectivity revolves around statistical associations among brain regions, which do not directly relate to the causal mechanisms governing brain dynamics. Here we propose the multiscale causal backbone (MCB) of brain dynamics shared by a set of individuals across multiple temporal scales, and devise a principled methodology to extract it.   Our approach leverages recent advances in multiscale causal structure learning and optimizes the trade-off between the model fitting and its complexity. Empirical assessment on synthetic data shows the superiority of our methodology over a baseline based on canonical functional connectivity networks. When applied to resting-state fMRI data, we find sparse MCBs for both the left and right brain hemispheres. Thanks to its multiscale nature, our approach shows that at low-frequency bands, causal dynamics are driven by brain regions associated with high-level cognitive functions; at higher frequencies instead, nodes related to sensory processing play a crucial role. Finally, our analysis of individual multiscale causal structures confirms the existence of a causal fingerprint of brain connectivity, thus supporting from a causal perspective the existing extensive research in brain connectivity fingerprinting.
</details>
<details>
<summary>摘要</summary>
大多数研究对大脑连接性都集中在统计相关性中，这些相关性不直接关系大脑动态的 causal 机制。我们提议一种多尺度 causal 脊梁（MCB），这是一组个体 across multiple temporal scales 的大脑动态的共同基础，并提出一种原则性的方法来提取它。我们的方法利用了最新的多尺度 causal 结构学习的进步，并优化模型适应性和复杂性的负面 Trade-off。empirical assessment on synthetic data 表明我们的方法超过基eline based on canonical functional connectivity networks。当应用于 Resting-state fMRI 数据时，我们发现左右大脑半球都有稀畴 MCB。由于它的多尺度性，我们的方法表明在低频带，高级认知功能相关的脑区域驱动 causal 动态; 在更高的频率带，感知处理相关的节点发挥了关键的作用。最后，我们对个体多尺度 causal 结构进行分析，证实了大脑连接性指estamp 的存在，从 causal 角度支持了现有的大脑连接性指estamp 的研究。
</details></li>
</ul>
<hr>
<h2 id="EXTRACT-Explainable-Transparent-Control-of-Bias-in-Embeddings"><a href="#EXTRACT-Explainable-Transparent-Control-of-Bias-in-Embeddings" class="headerlink" title="EXTRACT: Explainable Transparent Control of Bias in Embeddings"></a>EXTRACT: Explainable Transparent Control of Bias in Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00115">http://arxiv.org/abs/2311.00115</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZhijinGuo/EXTRACT">https://github.com/ZhijinGuo/EXTRACT</a></li>
<li>paper_authors: Zhijin Guo, Zhaozhen Xu, Martha Lewis, Nello Cristianini</li>
<li>for: 本研究旨在提供一种可Explainable和Transparent的方法控制知识图embedding中的偏见，以便评估和减少在训练过程中意外存在的保护信息。</li>
<li>methods: 本研究使用Canonical Correlation Analysis (CCA)进行偏见检测，然后使用一个线性系统将embeddings decomposed为其私有属性。</li>
<li>results: 实验结果显示，用户的观影行为和喜好可以推断出一些个人特征，如性别、年龄和职业。此外，在KG20C引用 dataset上，研究发现了一种方法可以通过引用网络来推断出文章中的会议信息。本研究还提出了四种透明的方法来保持 embedding 的能力进行预期的预测，同时减少不良信息的存在。<details>
<summary>Abstract</summary>
Knowledge Graphs are a widely used method to represent relations between entities in various AI applications, and Graph Embedding has rapidly become a standard technique to represent Knowledge Graphs in such a way as to facilitate inferences and decisions. As this representation is obtained from behavioural data, and is not in a form readable by humans, there is a concern that it might incorporate unintended information that could lead to biases. We propose EXTRACT: a suite of Explainable and Transparent methods to ConTrol bias in knowledge graph embeddings, so as to assess and decrease the implicit presence of protected information. Our method uses Canonical Correlation Analysis (CCA) to investigate the presence, extent and origins of information leaks during training, then decomposes embeddings into a sum of their private attributes by solving a linear system. Our experiments, performed on the MovieLens1M dataset, show that a range of personal attributes can be inferred from a user's viewing behaviour and preferences, including gender, age, and occupation. Further experiments, performed on the KG20C citation dataset, show that the information about the conference in which a paper was published can be inferred from the citation network of that article. We propose four transparent methods to maintain the capability of the embedding to make the intended predictions without retaining unwanted information. A trade-off between these two goals is observed.
</details>
<details>
<summary>摘要</summary>
知识图是广泛使用的方法来表示Entities之间的关系在不同的AI应用程序中，而图像嵌入快速成为了知识图表示的标准技术，以便进行推理和决策。这种表示来自行为数据，而不是可读的人类格式，因此可能包含意外的信息，导致偏见。我们提出EXTRACT：一个集成Explainable和Transparent的方法来控制知识图嵌入中的偏见，以便评估和减少隐藏的保护信息。我们使用Canonical Correlation Analysis（CCA）来研究嵌入中的信息泄露情况，然后将嵌入分解为私有属性的总和，解决一个线性方程。我们在MovieLens1M数据集上进行了实验，发现用户的观影行为和偏好可以推断出一些个人特征，包括性别、年龄和职业。在KG20C引用数据集上，我们发现了一种方法，可以通过文章引用网络来推断出文章的发表会议信息。我们提出了四种透明方法，以保持嵌入的能力，而不是保留不必要的信息。我们观察到了这两个目标之间的补做。
</details></li>
</ul>
<hr>
<h2 id="FairWASP-Fast-and-Optimal-Fair-Wasserstein-Pre-processing"><a href="#FairWASP-Fast-and-Optimal-Fair-Wasserstein-Pre-processing" class="headerlink" title="FairWASP: Fast and Optimal Fair Wasserstein Pre-processing"></a>FairWASP: Fast and Optimal Fair Wasserstein Pre-processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00109">http://arxiv.org/abs/2311.00109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikai Xiong, Niccolò Dalmasso, Alan Mishler, Vamsi K. Potluru, Tucker Balch, Manuela Veloso</li>
<li>for: 降低机器学习模型输出不均衡问题的解决方案</li>
<li>methods: 使用大规模杂合程序（MIP）和剖面方法来设计一种新的预处理方法，以降低分布式预处理问题的复杂性</li>
<li>results: 在synthetic数据集上进行了实验，并证明了该方法可以快速和高效地解决MIP和其线性 програм的relaxation问题，并且在下游分类任务中具有竞争性的性能，可以降低不均衡问题 while preserving accuracy。<details>
<summary>Abstract</summary>
Recent years have seen a surge of machine learning approaches aimed at reducing disparities in model outputs across different subgroups. In many settings, training data may be used in multiple downstream applications by different users, which means it may be most effective to intervene on the training data itself. In this work, we present FairWASP, a novel pre-processing approach designed to reduce disparities in classification datasets without modifying the original data. FairWASP returns sample-level weights such that the reweighted dataset minimizes the Wasserstein distance to the original dataset while satisfying (an empirical version of) demographic parity, a popular fairness criterion. We show theoretically that integer weights are optimal, which means our method can be equivalently understood as duplicating or eliminating samples. FairWASP can therefore be used to construct datasets which can be fed into any classification method, not just methods which accept sample weights. Our work is based on reformulating the pre-processing task as a large-scale mixed-integer program (MIP), for which we propose a highly efficient algorithm based on the cutting plane method. Experiments on synthetic datasets demonstrate that our proposed optimization algorithm significantly outperforms state-of-the-art commercial solvers in solving both the MIP and its linear program relaxation. Further experiments highlight the competitive performance of FairWASP in reducing disparities while preserving accuracy in downstream classification settings.
</details>
<details>
<summary>摘要</summary>
近年来，机器学习方法的发展有助于减少不同 subgroup 的模型输出差异。在多种设置中，训练数据可能会被多个下游应用程序使用，这意味着可能最有效地 intervene 在训练数据本身上。在这项工作中，我们提出了 FairWASP，一种新的预处理方法，可以在分类 dataset 中减少不同 subgroup 的差异。FairWASP 返回样本级别的权重，以使得重新权重后的 dataset 最小化 Wasserstein 距离原始 dataset，同时满足（一种 empirical 版本的）人口减少准则。我们证明了整数权重是最佳的，这意味着我们的方法可以看作是复制或消除样本。因此，FairWASP 可以用于构建可以被任何分类方法使用的 dataset。我们基于将预处理任务转换为大规模混合整数Program (MIP) 的方法，并提出了高效的剪切面方法。对于 Synthetic 数据集进行了实验，我们的提出的优化算法在 MIP 和其 linear program relaxation 中significantly 超越了现有的商业 solver。进一步的实验表明，FairWASP 可以减少差异而保持下游分类设置中的精度。
</details></li>
</ul>
<hr>
<h2 id="Deep-Compressed-Learning-for-3D-Seismic-Inversion"><a href="#Deep-Compressed-Learning-for-3D-Seismic-Inversion" class="headerlink" title="Deep Compressed Learning for 3D Seismic Inversion"></a>Deep Compressed Learning for 3D Seismic Inversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00107">http://arxiv.org/abs/2311.00107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maayan Gelboim, Amir Adler, Yen Sun, Mauricio Araya-Polo</li>
<li>for: 提高3D地震傅入的效率和质量，使用非常少的地震源。</li>
<li>methods:  combinational compressed-sensing和机器学习框架，称为compressed-learning，将维度减少操作和3D反傅编码器-解码器实现为深度卷积神经网络（DCNN）。</li>
<li>results: 通过选择一小部分可用的源，使用DCNN完成反傅任务，实现了数据量的减少一个次，保持3D重建质量与整个数据集相当。<details>
<summary>Abstract</summary>
We consider the problem of 3D seismic inversion from pre-stack data using a very small number of seismic sources. The proposed solution is based on a combination of compressed-sensing and machine learning frameworks, known as compressed-learning. The solution jointly optimizes a dimensionality reduction operator and a 3D inversion encoder-decoder implemented by a deep convolutional neural network (DCNN). Dimensionality reduction is achieved by learning a sparse binary sensing layer that selects a small subset of the available sources, then the selected data is fed to a DCNN to complete the regression task. The end-to-end learning process provides a reduction by an order-of-magnitude in the number of seismic records used during training, while preserving the 3D reconstruction quality comparable to that obtained by using the entire dataset.
</details>
<details>
<summary>摘要</summary>
我们考虑了基于非常小的数量的地震源数据的3D地震减法问题。我们的解决方案基于压缩感知和机器学习框架，称为压缩学习。该解决方案同时优化了维度减少算子和基于深度卷积神经网络（DCNN）实现的3D减法编码器-解码器。通过学习一个稀疏二进制感知层，选择一小 subsets of 可用的地震源，然后将选择的数据传递给 DCNN 完成回归任务。这种端到端学习过程可以在训练过程中减少数据记录的数量，同时保持与全部数据集相同的3D重建质量。
</details></li>
</ul>
<hr>
<h2 id="Seeking-Truth-and-Beauty-in-Flavor-Physics-with-Machine-Learning"><a href="#Seeking-Truth-and-Beauty-in-Flavor-Physics-with-Machine-Learning" class="headerlink" title="Seeking Truth and Beauty in Flavor Physics with Machine Learning"></a>Seeking Truth and Beauty in Flavor Physics with Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00087">http://arxiv.org/abs/2311.00087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantin T. Matchev, Katia Matcheva, Pierre Ramond, Sarunas Verner</li>
<li>for: 该研究旨在开发新的理论物理模型，既要适应现有实验数据，又要满足抽象理论家的标准如美食性、自然性等。</li>
<li>methods: 该研究使用机器学习技术设计损失函数进行两个任务的优化：适应现有实验数据和满足抽象理论家的标准。作为一个示例，我们使用Yukawa针子部分进行示例。</li>
<li>results: 优化损失函数的结果是真实的和美丽的模型。<details>
<summary>Abstract</summary>
The discovery process of building new theoretical physics models involves the dual aspect of both fitting to the existing experimental data and satisfying abstract theorists' criteria like beauty, naturalness, etc. We design loss functions for performing both of those tasks with machine learning techniques. We use the Yukawa quark sector as a toy example to demonstrate that the optimization of these loss functions results in true and beautiful models.
</details>
<details>
<summary>摘要</summary>
发现过程中建立新的物理理论模型具有双重的方面，一是适应现有实验数据，另一是满足抽象理论家的标准如美食、自然性等。我们使用机器学习技术定义损失函数进行这两个任务。我们使用Yukawa夹心作为一个示例，示出优化这些损失函数后得到了真实和美丽的模型。Here's a breakdown of the translation:* "发现过程" (发现过程) - the process of discovering new physics models* "建立新的物理理论模型" (建立新的物理理论模型) - building new theoretical physics models* "具有双重的方面" (具有双重的方面) - having two aspects* "一是适应现有实验数据" (一是适应现有实验数据) - one aspect is fitting to existing experimental data* "另一是满足抽象理论家的标准" (另一是满足抽象理论家的标准) - the other aspect is satisfying the criteria of abstract theorists, such as beauty and naturalness* "使用机器学习技术定义损失函数" (使用机器学习技术定义损失函数) - using machine learning techniques to define loss functions* "进行这两个任务" (进行这两个任务) - for these two tasks* "示例" (示例) - example* "示出优化这些损失函数后得到了真实和美丽的模型" (示出优化这些损失函数后得到了真实和美丽的模型) - demonstrating that optimizing these loss functions results in true and beautiful models.
</details></li>
</ul>
<hr>
<h2 id="Ensemble-models-outperform-single-model-uncertainties-and-predictions-for-operator-learning-of-hypersonic-flows"><a href="#Ensemble-models-outperform-single-model-uncertainties-and-predictions-for-operator-learning-of-hypersonic-flows" class="headerlink" title="Ensemble models outperform single model uncertainties and predictions for operator-learning of hypersonic flows"></a>Ensemble models outperform single model uncertainties and predictions for operator-learning of hypersonic flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00060">http://arxiv.org/abs/2311.00060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor J. Leon, Noah Ford, Honest Mrema, Jeffrey Gilbert, Alexander New</li>
<li>for: 这个论文的目的是开发一种能够预测高速流动的科学机器学习模型（SciML），并使用不同的不确定性评估方法来评估模型的输出不确定性。</li>
<li>methods: 这个研究使用了三种不确定性评估方法：mean-variance estimation、evidential uncertainty和ensembling，来扩展一个深度ONet模型。</li>
<li>results: 研究发现，使用ensembling方法可以最好地减少错误和calibrate uncertainty，并在 interpolative 和 extrapolative  régime中表现出色。<details>
<summary>Abstract</summary>
High-fidelity computational simulations and physical experiments of hypersonic flows are resource intensive. Training scientific machine learning (SciML) models on limited high-fidelity data offers one approach to rapidly predict behaviors for situations that have not been seen before. However, high-fidelity data is itself in limited quantity to validate all outputs of the SciML model in unexplored input space. As such, an uncertainty-aware SciML model is desired. The SciML model's output uncertainties could then be used to assess the reliability and confidence of the model's predictions. In this study, we extend a DeepONet using three different uncertainty quantification mechanisms: mean-variance estimation, evidential uncertainty, and ensembling. The uncertainty aware DeepONet models are trained and evaluated on the hypersonic flow around a blunt cone object with data generated via computational fluid dynamics over a wide range of Mach numbers and altitudes. We find that ensembling outperforms the other two uncertainty models in terms of minimizing error and calibrating uncertainty in both interpolative and extrapolative regimes.
</details>
<details>
<summary>摘要</summary>
高精度计算 simulate 和物理实验 hypersonic 流体动力学是资源占用的。使用有限高精度数据训练科学机器学习（SciML）模型可以快速预测未经测试的情况。然而，高精度数据本身受限，无法验证SciML模型在未知输入空间中的所有输出。因此，需要一个不确定性意识的 SciML 模型。SciML 模型的输出不确定性可以用来评估模型预测的可靠性和信任度。在本研究中，我们扩展了 DeepONet 使用三种不确定性评估机制：平均方差估计、证据不确定性和混合。这些不确定性意识 DeepONet 模型在 hypersonic 流体动力学中逆转射 cone  объек的数据生成过程中被训练和评估。我们发现，混合在 interpolative 和 extrapolative  régime 中都能够最小化错误和把不确定性评估。
</details></li>
</ul>
<hr>
<h2 id="Training-Free-Generalization-on-Heterogeneous-Tabular-Data-via-Meta-Representation"><a href="#Training-Free-Generalization-on-Heterogeneous-Tabular-Data-via-Meta-Representation" class="headerlink" title="Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation"></a>Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00055">http://arxiv.org/abs/2311.00055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han-Jia Ye, Qi-Le Zhou, De-Chuan Zhan</li>
<li>for: 本研究旨在提高不同特征和类型的表格数据之间共享知识，使 tabular 模型能够在未经训练的情况下在新的表格数据集中表现出色。</li>
<li>methods: 本研究提出了 Tabular data Pre-Training via Meta-representation (TabPTM)，允许一个 tabular 模型在一组具有多种特征和类型的表格数据集上进行预训练。然后，这个预训练模型可以直接应用于未经训练的表格数据集，无需进行额外的训练。</li>
<li>results: 实验表明，TabPTM 能够在新的表格数据集中表现出色，尤其是在少量示例情况下。<details>
<summary>Abstract</summary>
Tabular data is prevalent across various machine learning domains. Yet, the inherent heterogeneities in attribute and class spaces across different tabular datasets hinder the effective sharing of knowledge, limiting a tabular model to benefit from other datasets. In this paper, we propose Tabular data Pre-Training via Meta-representation (TabPTM), which allows one tabular model pre-training on a set of heterogeneous datasets. Then, this pre-trained model can be directly applied to unseen datasets that have diverse attributes and classes without additional training. Specifically, TabPTM represents an instance through its distance to a fixed number of prototypes, thereby standardizing heterogeneous tabular datasets. A deep neural network is then trained to associate these meta-representations with dataset-specific classification confidences, endowing TabPTM with the ability of training-free generalization. Experiments validate that TabPTM achieves promising performance in new datasets, even under few-shot scenarios.
</details>
<details>
<summary>摘要</summary>
<SYS>请输入文本要翻译的文本</SYS>文本：Tabular data 是机器学习领域中广泛存在的数据类型之一。然而，不同的 tabular 数据集之间的 attribute 和 class 空间之间的自然差异会限制 tabular 模型在不同数据集之间共享知识，使得一个 tabular 模型只能在已经训练过的数据集上进行有效地学习。在这篇论文中，我们提出了 Tabular data Pre-Training via Meta-representation（TabPTM），它允许一个 tabular 模型在一组不同 attribute 和 class 的 heterogeneous 数据集上进行预训练。然后，这个预训练后的模型可以直接应用于未经训练过的数据集，而无需进行额外的训练。具体来说，TabPTM 使用一个固定数量的 prototypes 来表示每个实例，从而标准化不同 attribute 和 class 的 tabular 数据集。然后，一个深度神经网络被训练以关联这些 meta-representation 与数据集pecific的分类信任度，使得 TabPTM 具有无需训练的普适性。实验表明，TabPTM 在新的数据集上可以达到良好的性能，甚至在几 shot 的情况下。Translation:Tabular data is ubiquitous in various machine learning domains. However, the inherent heterogeneities in attribute and class spaces across different tabular datasets hinder the effective sharing of knowledge, limiting a tabular model to benefit from other datasets. In this paper, we propose Tabular data Pre-Training via Meta-representation (TabPTM), which allows one tabular model pre-training on a set of heterogeneous datasets. Then, this pre-trained model can be directly applied to unseen datasets that have diverse attributes and classes without additional training. Specifically, TabPTM represents an instance through its distance to a fixed number of prototypes, thereby standardizing heterogeneous tabular datasets. A deep neural network is then trained to associate these meta-representations with dataset-specific classification confidences, endowing TabPTM with the ability of training-free generalization. Experiments validate that TabPTM achieves promising performance in new datasets, even under few-shot scenarios.
</details></li>
</ul>
<hr>
<h2 id="On-the-Kolmogorov-neural-networks"><a href="#On-the-Kolmogorov-neural-networks" class="headerlink" title="On the Kolmogorov neural networks"></a>On the Kolmogorov neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00049">http://arxiv.org/abs/2311.00049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlbTchik/Adapting-the-K-complexity-for-neural-networks">https://github.com/AlbTchik/Adapting-the-K-complexity-for-neural-networks</a></li>
<li>paper_authors: Aysu Ismayilova, Vugar Ismailov</li>
<li>for: 该论文旨在证明olkogorovTwoHiddenLayer神经网络模型，带有连续、不连续、边界或无边界活动函数在第二层隐藏层，可以准确表示连续、不连续、边界和所有无边界多变量函数。</li>
<li>methods: 该论文使用了KolmogorovTwoHiddenLayer神经网络模型，并研究了不同的活动函数对模型的影响。</li>
<li>results: 研究发现，使用连续、不连续、边界或无边界活动函数的KolmogorovTwoHiddenLayer神经网络模型，可以准确表示连续、不连续、边界和所有无边界多变量函数。<details>
<summary>Abstract</summary>
In this paper, we show that the Kolmogorov two hidden layer neural network model with a continuous, discontinuous bounded or unbounded activation function in the second hidden layer can precisely represent continuous, discontinuous bounded and all unbounded multivariate functions, respectively.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们展示了olkogorov两层隐藏层神经网络模型，其中第二层隐藏层使用连续、终止、限制或无限 activation function，可以精确表示连续、终止、限制和所有无限多变量函数。Note that I have used "连续" (liánxù) to translate "continuous" in the text, which is a more common way to express this concept in Simplified Chinese. Also, I have used "终止" (jìzhì) to translate "discontinuous", which is a more precise way to express this concept in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Unexpected-Improvements-to-Expected-Improvement-for-Bayesian-Optimization"><a href="#Unexpected-Improvements-to-Expected-Improvement-for-Bayesian-Optimization" class="headerlink" title="Unexpected Improvements to Expected Improvement for Bayesian Optimization"></a>Unexpected Improvements to Expected Improvement for Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20708">http://arxiv.org/abs/2310.20708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Ament, Samuel Daulton, David Eriksson, Maximilian Balandat, Eytan Bakshy</li>
<li>for: 提高 Bayesian 优化中的 acquisition function 性能</li>
<li>methods: 提出新的 acquisition function 家族 LogEI, 其中的成员具有较好的数值优化性</li>
<li>results: 实验结果显示 LogEI 家族的 acquisition function 可以大幅提高 Bayesian 优化中的优化性能，并且与当前状态的技术相当或超越其性能<details>
<summary>Abstract</summary>
Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their constrained, noisy, and parallel variants, and propose corresponding reformulations that remedy these pathologies. Our empirical results show that members of the LogEI family of acquisition functions substantially improve on the optimization performance of their canonical counterparts and surprisingly, are on par with or exceed the performance of recent state-of-the-art acquisition functions, highlighting the understated role of numerical optimization in the literature.
</details>
<details>
<summary>摘要</summary>
预期改进（EI）可能是搜索优化中最受欢迎的目标函数，它在许多成功应用中表现出色，但它的性能经常被更新的方法所超越。尤其是EI和其变体，包括并行和多目标设置，在数值上具有很大的挑战。这种挑战通常随着观察数、搜索空间维度或约束的数量增加，导致文献中的性能不一致，通常是低效的。在这里，我们提出了LogEI，一个新的家族目标函数，其成员的优化值和传统目标函数的优化值相似或相等，但是数值上容易优化。我们示出了经典的EI、EHVI和它们的受限、噪声和并行变体中的数值问题，并提出了相应的改进方案。我们的实验结果表明，LogEI家族的目标函数在优化性能方面明显提高了，并且奇怪地，与最新的状态对照函数相当或超越了性能，这 highlights了文献中数值优化的重要性。
</details></li>
</ul>
<hr>
<h2 id="Farthest-Greedy-Path-Sampling-for-Two-shot-Recommender-Search"><a href="#Farthest-Greedy-Path-Sampling-for-Two-shot-Recommender-Search" class="headerlink" title="Farthest Greedy Path Sampling for Two-shot Recommender Search"></a>Farthest Greedy Path Sampling for Two-shot Recommender Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20705">http://arxiv.org/abs/2310.20705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufan Cao, Tunhou Zhang, Wei Wen, Feng Yan, Hai Li, Yiran Chen</li>
<li>for: 提高深度推荐模型的开发效率和性能。</li>
<li>methods: 使用Weight-sharing Neural Architecture Search（WS-NAS）机制，并引入Farthest Greedy Path Sampling（FGPS）策略来增强搜索空间的覆盖率和多样性，从而更好地利用权重共享机制来搜索优秀的建筑。</li>
<li>results: 在三个Click-Through Rate（CTR）预测 benchmark 上，该方法可以准确地预测建筑，并且在大多数 NAS 基于的模型中表现出色，超越了 manually designed 模型。<details>
<summary>Abstract</summary>
Weight-sharing Neural Architecture Search (WS-NAS) provides an efficient mechanism for developing end-to-end deep recommender models. However, in complex search spaces, distinguishing between superior and inferior architectures (or paths) is challenging. This challenge is compounded by the limited coverage of the supernet and the co-adaptation of subnet weights, which restricts the exploration and exploitation capabilities inherent to weight-sharing mechanisms. To address these challenges, we introduce Farthest Greedy Path Sampling (FGPS), a new path sampling strategy that balances path quality and diversity. FGPS enhances path diversity to facilitate more comprehensive supernet exploration, while emphasizing path quality to ensure the effective identification and utilization of promising architectures. By incorporating FGPS into a Two-shot NAS (TS-NAS) framework, we derive high-performance architectures. Evaluations on three Click-Through Rate (CTR) prediction benchmarks demonstrate that our approach consistently achieves superior results, outperforming both manually designed and most NAS-based models.
</details>
<details>
<summary>摘要</summary>
weight-sharing neural architecture search (WS-NAS) 提供了一种高效的终端深度推荐模型开发机制。然而，在复杂的搜索空间中，辨认优于劣的架构（或路径）是具有挑战性。这些挑战得到加剧，因为超网络覆盖率有限，而且子网准重共适应，这限制了weight-sharing机制中的探索和利用能力。为解决这些挑战，我们介绍了远程最大贪婪路径采样策略（FGPS），这种策略可以平衡路径质量和多样性。FGPS增强了超网络探索的多样性，同时强调路径质量，以确保有效地识别和利用优秀架构。通过将FGPS纳入TS-NAS框架中，我们得到了高性能的架构。我们在三个Click-Through Rate（CTR）预测 benchmark上进行评估，发现我们的方法可以准确地 дости得优于 manually designed 和大多数 NAS-based 模型。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Multistate-Bennett-Acceptance-Ratio-Methods"><a href="#Bayesian-Multistate-Bennett-Acceptance-Ratio-Methods" class="headerlink" title="Bayesian Multistate Bennett Acceptance Ratio Methods"></a>Bayesian Multistate Bennett Acceptance Ratio Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20699">http://arxiv.org/abs/2310.20699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinqiang Ding</li>
<li>for:  Computing free energies of thermodynamic states using a Bayesian approach.</li>
<li>methods:  Bayesian generalization of the MBAR method, integrating configurations sampled from thermodynamic states with a prior distribution to compute a posteriori distribution of free energies.</li>
<li>results:  More accurate uncertainty estimates than MBAR, and the ability to incorporate prior knowledge about free energies into the estimation procedure.<details>
<summary>Abstract</summary>
The multistate Bennett acceptance ratio (MBAR) method is a prevalent approach for computing free energies of thermodynamic states. In this work, we introduce BayesMBAR, a Bayesian generalization of the MBAR method. By integrating configurations sampled from thermodynamic states with a prior distribution, BayesMBAR computes a posterior distribution of free energies. Using the posterior distribution, we derive free energy estimations and compute their associated uncertainties. Notably, when a uniform prior distribution is used, BayesMBAR recovers the MBAR's result but provides more accurate uncertainty estimates. Additionally, when prior knowledge about free energies is available, BayesMBAR can incorporate this information into the estimation procedure by using non-uniform prior distributions. As an example, we show that, by incorporating the prior knowledge about the smoothness of free energy surfaces, BayesMBAR provides more accurate estimates than the MBAR method. Given MBAR's widespread use in free energy calculations, we anticipate BayesMBAR to be an essential tool in various applications of free energy calculations.
</details>
<details>
<summary>摘要</summary>
“多状态本нет特征比率（MBAR）方法是一种广泛应用于计算热力学状态的自由能计算方法。在这篇论文中，我们介绍了抽象 bayesMBAR，一种 bayesian 扩展的 MBAR 方法。通过将温度状态中的配置与一个先验分布集成起来，抽象 bayesMBAR 计算了后验自由能分布。使用后验分布，我们得到了自由能估计值和相关的不确定性。值得注意的是，当使用均勋先验分布时，抽象 bayesMBAR 可以重新计算 MBAR 的结果，并提供更加准确的不确定性估计。此外，当有具体关于自由能的先验知识时，抽象 bayesMBAR 可以在计算过程中包含这些信息，使用非均勋先验分布。作为一个例子，我们显示了，通过包含自由能表面的平滑性先验知识，抽象 bayesMBAR 可以提供更加准确的估计值。由于 MBAR 在自由能计算中广泛应用，我们预计抽象 bayesMBAR 将成为各种自由能计算应用中的重要工具。”
</details></li>
</ul>
<hr>
<h2 id="Compression-with-Exact-Error-Distribution-for-Federated-Learning"><a href="#Compression-with-Exact-Error-Distribution-for-Federated-Learning" class="headerlink" title="Compression with Exact Error Distribution for Federated Learning"></a>Compression with Exact Error Distribution for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20682">http://arxiv.org/abs/2310.20682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Hegazy, Rémi Leluc, Cheuk Ting Li, Aymeric Dieuleveut</li>
<li>for: 这篇论文主要用于探讨 Federated Learning（分布式学习）中的压缩方法，以降低分布式学习中的通信成本。</li>
<li>methods: 这篇论文提出了基于层次量化器的各种压缩和汇集方法，以实现具体的错误分布（如 Gaussian 或 Laplace）在汇集数据上。</li>
<li>results: 论文表明，使用提出的压缩和汇集方法可以在分布式学习中实现免压缩（compression-for-free），并且可以提高标准的分布式学习方案中的 Gaussian 噪声（如 Langevin 动力学和随机缓和）。<details>
<summary>Abstract</summary>
Compression schemes have been extensively used in Federated Learning (FL) to reduce the communication cost of distributed learning. While most approaches rely on a bounded variance assumption of the noise produced by the compressor, this paper investigates the use of compression and aggregation schemes that produce a specific error distribution, e.g., Gaussian or Laplace, on the aggregated data. We present and analyze different aggregation schemes based on layered quantizers achieving exact error distribution. We provide different methods to leverage the proposed compression schemes to obtain compression-for-free in differential privacy applications. Our general compression methods can recover and improve standard FL schemes with Gaussian perturbations such as Langevin dynamics and randomized smoothing.
</details>
<details>
<summary>摘要</summary>
《压缩方法在分布式学习中的应用广泛，以减少分布式学习中的通信成本。大多数方法假设压缩器生成的噪声具有有界变量，但这篇论文研究了使用压缩和汇总方案，生成特定噪声分布，例如 Gaussian 或 Laplace 分布，在汇总数据中。我们提出和分析了不同层次量化器基于的汇总方案，并提供了不同的方法，使用我们的通用压缩方法在 differential privacy 应用中实现压缩-for-free。我们的方法可以恢复和改进标准分布式学习方案中的 Gaussian 噪声，如朗格文动力学和随机平滑。》Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="Latent-Field-Discovery-In-Interacting-Dynamical-Systems-With-Neural-Fields"><a href="#Latent-Field-Discovery-In-Interacting-Dynamical-Systems-With-Neural-Fields" class="headerlink" title="Latent Field Discovery In Interacting Dynamical Systems With Neural Fields"></a>Latent Field Discovery In Interacting Dynamical Systems With Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20679">http://arxiv.org/abs/2310.20679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mkofinas/aether">https://github.com/mkofinas/aether</a></li>
<li>paper_authors: Miltiadis Kofinas, Erik J. Bekkers, Naveen Shankar Nagaraja, Efstratios Gavves</li>
<li>for: 本研究旨在发现和学习互动对象系统中的场效应，以便更好地理解和预测系统的动态。</li>
<li>methods: 本研究使用神经网络学习方法，包括equivariant graph networks和场网络，以捕捉本系统的局部对象互动和全局场效应。</li>
<li>results: 实验表明，使用这种方法可以准确地发现互动对象系统中的场效应，并使用这些场效应来预测系统的未来轨迹。<details>
<summary>Abstract</summary>
Systems of interacting objects often evolve under the influence of field effects that govern their dynamics, yet previous works have abstracted away from such effects, and assume that systems evolve in a vacuum. In this work, we focus on discovering these fields, and infer them from the observed dynamics alone, without directly observing them. We theorize the presence of latent force fields, and propose neural fields to learn them. Since the observed dynamics constitute the net effect of local object interactions and global field effects, recently popularized equivariant networks are inapplicable, as they fail to capture global information. To address this, we propose to disentangle local object interactions -- which are $\mathrm{SE}(n)$ equivariant and depend on relative states -- from external global field effects -- which depend on absolute states. We model interactions with equivariant graph networks, and combine them with neural fields in a novel graph network that integrates field forces. Our experiments show that we can accurately discover the underlying fields in charged particles settings, traffic scenes, and gravitational n-body problems, and effectively use them to learn the system and forecast future trajectories.
</details>
<details>
<summary>摘要</summary>
系统经常由相互作用的对象组成，但以前的研究通常忽略了这些效应，假设系统在真空中进行演化。在这项工作中，我们将关注发现这些场，从观察的动态来INFER它们，而不是直接观察它们。我们推测存在潜在的势场，并提议使用神经场来学习它们。由于观察的动态表示系统中对象之间的本地互动以及全局场效应的积加，近期流行的等变网络（equivariant networks）是无法捕捉全局信息的。为解决这个问题，我们提议分解本地对象互动（ $\mathrm{SE}(n)$ 对称的）和外部全局场效应（depends on absolute states）。我们使用对称图学网络（equivariant graph networks）模型对象之间的互动，并将其与神经场结合在一起。我们的实验表明，我们可以准确地发现带电粒子、交通场景和重力n体问题中的下面场，并使用它们来学习系统和预测未来轨迹。
</details></li>
</ul>
<hr>
<h2 id="Balancing-Act-Constraining-Disparate-Impact-in-Sparse-Models"><a href="#Balancing-Act-Constraining-Disparate-Impact-in-Sparse-Models" class="headerlink" title="Balancing Act: Constraining Disparate Impact in Sparse Models"></a>Balancing Act: Constraining Disparate Impact in Sparse Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20673">http://arxiv.org/abs/2310.20673</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/merajhashemi/balancing_act">https://github.com/merajhashemi/balancing_act</a></li>
<li>paper_authors: Meraj Hashemizadeh, Juan Ramirez, Rohan Sukumaran, Golnoosh Farnadi, Simon Lacoste-Julien, Jose Gallego-Posada</li>
<li>for: 这篇论文的目的是为了提出一种可以在边缘设备上部署大型深度学习模型的方法，并且能够确保模型的性能不会过分差异。</li>
<li>methods: 这篇论文使用的方法是一种受限的优化方法，可以对模型进行剪裁，以提高模型的简洁性和可靠性。</li>
<li>results: 实验结果显示，这篇论文提出的技术可以对大型模型和具有大量保护子集的情况进行可靠的测试，并且可以实现较好的性能和可靠性。<details>
<summary>Abstract</summary>
Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities. Although sparse models achieve performance comparable to that of their dense counterparts at the level of the entire dataset, they exhibit high accuracy drops for some data sub-groups. Existing methods to mitigate this disparate impact induced by pruning (i) rely on surrogate metrics that address the problem indirectly and have limited interpretability; or (ii) scale poorly with the number of protected sub-groups in terms of computational cost. We propose a constrained optimization approach that $\textit{directly addresses the disparate impact of pruning}$: our formulation bounds the accuracy change between the dense and sparse models, for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity levels. Experimental results demonstrate that our technique scales reliably to problems involving large models and hundreds of protected sub-groups.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:模型剔除是一种广泛使用的方法，启用大型深度学习模型在边缘设备上部署，由于限制的计算或存储能力。虽然稀疏模型在整个数据集级别上达到相似的性能，但对于一些数据子组，它们会导致准确性下降。现有的方法来减轻这种差异性影响（i）使用间接指标，无法直接解释问题，或（ii）在保护子组数量增加时，计算成本不可控。我们提出了一种约束优化方法，直接解决剔除引起的差异性影响：我们的形式约束剔除后模型和整个模型之间的准确度变化，对每个子组进行约束。这种约束选择提供了可解释的成功标准，以确定剔除后模型是否达到了接受到的差异水平。实验结果表明，我们的技术可靠地扩展到大模型和百个保护子组的问题。
</details></li>
</ul>
<hr>
<h2 id="Density-Matrix-Emulation-of-Quantum-Recurrent-Neural-Networks-for-Multivariate-Time-Series-Prediction"><a href="#Density-Matrix-Emulation-of-Quantum-Recurrent-Neural-Networks-for-Multivariate-Time-Series-Prediction" class="headerlink" title="Density Matrix Emulation of Quantum Recurrent Neural Networks for Multivariate Time Series Prediction"></a>Density Matrix Emulation of Quantum Recurrent Neural Networks for Multivariate Time Series Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20671">http://arxiv.org/abs/2310.20671</a></li>
<li>repo_url: None</li>
<li>paper_authors: José Daniel Viqueira, Daniel Faílde, Mariamo M. Juane, Andrés Gómez, David Mera</li>
<li>for: 模型和预测未来值的多变量时间序列</li>
<li>methods: 使用密度矩阵ormal formalism和维度notation来实现电路的特性Matrix formalism和维度notation</li>
<li>results: 通过使用novel hardware-efficient ansatz和三个多样化的数据集来测试和 validate QRNNs的准确性和可靠性，并证明QRNNs可以准确预测未来值，并 capture不同复杂性的输入序列中的非常见模式。<details>
<summary>Abstract</summary>
Quantum Recurrent Neural Networks (QRNNs) are robust candidates to model and predict future values in multivariate time series. However, the effective implementation of some QRNN models is limited by the need of mid-circuit measurements. Those increase the requirements for quantum hardware, which in the current NISQ era does not allow reliable computations. Emulation arises as the main near-term alternative to explore the potential of QRNNs, but existing quantum emulators are not dedicated to circuits with multiple intermediate measurements. In this context, we design a specific emulation method that relies on density matrix formalism. The mathematical development is explicitly provided as a compact formulation by using tensor notation. It allows us to show how the present and past information from a time series is transmitted through the circuit, and how to reduce the computational cost in every time step of the emulated network. In addition, we derive the analytical gradient and the Hessian of the network outputs with respect to its trainable parameters, with an eye on gradient-based training and noisy outputs that would appear when using real quantum processors. We finally test the presented methods using a novel hardware-efficient ansatz and three diverse datasets that include univariate and multivariate time series. Our results show how QRNNs can make accurate predictions of future values by capturing non-trivial patterns of input series with different complexities.
</details>
<details>
<summary>摘要</summary>
量子循环神经网络（QRNN）是多变量时间序列预测的可靠候选者。然而，一些QRNN模型的实现效率受到中间测量的限制，这会增加量子硬件的需求，目前的NISQ时代并不可靠计算。虚拟量子计算是短期代替方法，但现有的量子模拟器不适用于多中间测量的环路。在这种情况下，我们设计了专门的模拟方法，基于密度矩阵 formalism。我们使用tensor notation的紧凑表示法，以示如何在环路中传递当前和过去时间序列信息，并如何在每个时间步中减少计算成本。此外，我们计算了网络输出的导数和偏微分，以便使用梯度下降法进行训练和避免使用真正的量子处理器时的噪声输出。最后，我们使用一种新的硬件高效的思路和三种多样的时间序列 dataset进行测试。我们的结果表明，QRNN可以准确预测未来值，并 capture非常复杂的输入序列中的非ingtrivial征性。
</details></li>
</ul>
<hr>
<h2 id="Performance-Improvement-in-Multi-class-Classification-via-Automated-Hierarchy-Generation-and-Exploitation-through-Extended-LCPN-Schemes"><a href="#Performance-Improvement-in-Multi-class-Classification-via-Automated-Hierarchy-Generation-and-Exploitation-through-Extended-LCPN-Schemes" class="headerlink" title="Performance Improvement in Multi-class Classification via Automated Hierarchy Generation and Exploitation through Extended LCPN Schemes"></a>Performance Improvement in Multi-class Classification via Automated Hierarchy Generation and Exploitation through Extended LCPN Schemes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20641">http://arxiv.org/abs/2310.20641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celal Alagoz</li>
<li>for: This paper focuses on the performance of hierarchical classification (HC) in multi-class classification tasks, particularly in scenarios where a predefined hierarchy structure is not readily accessible.</li>
<li>methods: The paper explores hierarchy generation and hierarchy exploitation schemes, including two novel schemes (LCPN+ and LCPN+F) that extend the capabilities of LCPN and combine the strengths of global and local classification.</li>
<li>results: The findings show the consistent superiority of LCPN+F, which outperforms other schemes across various datasets and scenarios, while maintaining runtime performance comparable to Flat Classification (FC).<details>
<summary>Abstract</summary>
Hierarchical classification (HC) plays a pivotal role in multi-class classification tasks, where objects are organized into a hierarchical structure. This study explores the performance of HC through a comprehensive analysis that encompasses both hierarchy generation and hierarchy exploitation. This analysis is particularly relevant in scenarios where a predefined hierarchy structure is not readily accessible. Notably, two novel hierarchy exploitation schemes, LCPN+ and LCPN+F, which extend the capabilities of LCPN and combine the strengths of global and local classification, have been introduced and evaluated alongside existing methods. The findings reveal the consistent superiority of LCPN+F, which outperforms other schemes across various datasets and scenarios. Moreover, this research emphasizes not only effectiveness but also efficiency, as LCPN+ and LCPN+F maintain runtime performance comparable to Flat Classification (FC). Additionally, this study underscores the importance of selecting the right hierarchy exploitation scheme to maximize classification performance. This work extends our understanding of HC and establishes a benchmark for future research, fostering advancements in multi-class classification methodologies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Projecting-basis-functions-with-tensor-networks-for-Gaussian-process-regression"><a href="#Projecting-basis-functions-with-tensor-networks-for-Gaussian-process-regression" class="headerlink" title="Projecting basis functions with tensor networks for Gaussian process regression"></a>Projecting basis functions with tensor networks for Gaussian process regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20630">http://arxiv.org/abs/2310.20630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clara Menzen, Eva Memmel, Kim Batselier, Manon Kok</li>
<li>for: 这个论文是关于精确 Gaussian Process（GP）回归的方法，使用张量网络（TN）。</li>
<li>methods: 我们使用低维张量网络来找到适当的低维度子空间，然后通过 Bayesian 推断问题来找到模型的权重。最后，我们将结果投影回原空间来进行 GP 预测。</li>
<li>results: 我们在一个18维数据集上进行了一个 inverse dynamics 问题的实验，并证明了我们的方法的可行性。<details>
<summary>Abstract</summary>
This paper presents a method for approximate Gaussian process (GP) regression with tensor networks (TNs). A parametric approximation of a GP uses a linear combination of basis functions, where the accuracy of the approximation depends on the total number of basis functions $M$. We develop an approach that allows us to use an exponential amount of basis functions without the corresponding exponential computational complexity. The key idea to enable this is using low-rank TNs. We first find a suitable low-dimensional subspace from the data, described by a low-rank TN. In this low-dimensional subspace, we then infer the weights of our model by solving a Bayesian inference problem. Finally, we project the resulting weights back to the original space to make GP predictions. The benefit of our approach comes from the projection to a smaller subspace: It modifies the shape of the basis functions in a way that it sees fit based on the given data, and it allows for efficient computations in the smaller subspace. In an experiment with an 18-dimensional benchmark data set, we show the applicability of our method to an inverse dynamics problem.
</details>
<details>
<summary>摘要</summary>
First, we find a suitable low-dimensional subspace from the data using a low-rank TN. Then, we solve a Bayesian inference problem in this low-dimensional subspace to infer the weights of our model. Finally, we project the resulting weights back to the original space to make GP predictions. The benefit of our approach is that it modifies the shape of the basis functions based on the given data, allowing for efficient computations in a smaller subspace.In an experiment with an 18-dimensional benchmark data set, we demonstrate the applicability of our method to an inverse dynamics problem. Our approach enables the use of an exponential number of basis functions without an exponential increase in computational complexity, making it a promising method for large-scale GP regression problems.
</details></li>
</ul>
<hr>
<h2 id="Graph-Matching-via-convex-relaxation-to-the-simplex"><a href="#Graph-Matching-via-convex-relaxation-to-the-simplex" class="headerlink" title="Graph Matching via convex relaxation to the simplex"></a>Graph Matching via convex relaxation to the simplex</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20609">http://arxiv.org/abs/2310.20609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernesto Araya Valdivia, Hemant Tyagi</li>
<li>for:  solves the Graph Matching problem, which is NP-hard and has many applications in computer vision, network deanonymization, and protein alignment.</li>
<li>methods:  introduces a new convex relaxation onto the unit simplex and develops an efficient mirror descent scheme with closed-form iterations to solve the problem.</li>
<li>results:  shows that the simplex relaxation admits a unique solution with high probability in the noiseless case, which implies exact recovery of the ground truth permutation. Additionally, establishes a novel sufficiency condition for the input matrix in standard greedy rounding methods, which is less restrictive than the commonly used &#96;diagonal dominance’ condition.<details>
<summary>Abstract</summary>
This paper addresses the Graph Matching problem, which consists of finding the best possible alignment between two input graphs, and has many applications in computer vision, network deanonymization and protein alignment. A common approach to tackle this problem is through convex relaxations of the NP-hard \emph{Quadratic Assignment Problem} (QAP).   Here, we introduce a new convex relaxation onto the unit simplex and develop an efficient mirror descent scheme with closed-form iterations for solving this problem. Under the correlated Gaussian Wigner model, we show that the simplex relaxation admits a unique solution with high probability. In the noiseless case, this is shown to imply exact recovery of the ground truth permutation. Additionally, we establish a novel sufficiency condition for the input matrix in standard greedy rounding methods, which is less restrictive than the commonly used `diagonal dominance' condition. We use this condition to show exact one-step recovery of the ground truth (holding almost surely) via the mirror descent scheme, in the noiseless setting. We also use this condition to obtain significantly improved conditions for the GRAMPA algorithm [Fan et al. 2019] in the noiseless setting.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Online-Conversion-with-Switching-Costs-Robust-and-Learning-Augmented-Algorithms"><a href="#Online-Conversion-with-Switching-Costs-Robust-and-Learning-Augmented-Algorithms" class="headerlink" title="Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms"></a>Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20598">http://arxiv.org/abs/2310.20598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy</li>
<li>for: 这篇论文研究了在线转换问题，即在一定时间间隔T内，一个在线玩家尝试购买（或卖）不同资产的分数部分，并且遇到转换成本每次决策时更改。</li>
<li>methods: 论文提出了一种基于阈值的在线算法，可以在竞争环境中最优化转换问题的解决方案。此外，论文还提出了一种学习增强的算法，可以通过不可信黑盒模型的预测来提高平均性能而无需牺牲最坏情况的竞争优化。</li>
<li>results: 实验结果表明，论文提出的算法在碳警惕电动车充电问题中具有显著的改善，比基准方法更高效。<details>
<summary>Abstract</summary>
We introduce and study online conversion with switching costs, a family of online problems that capture emerging problems at the intersection of energy and sustainability. In this problem, an online player attempts to purchase (alternatively, sell) fractional shares of an asset during a fixed time horizon with length $T$. At each time step, a cost function (alternatively, price function) is revealed, and the player must irrevocably decide an amount of asset to convert. The player also incurs a switching cost whenever their decision changes in consecutive time steps, i.e., when they increase or decrease their purchasing amount. We introduce competitive (robust) threshold-based algorithms for both the minimization and maximization variants of this problem, and show they are optimal among deterministic online algorithms. We then propose learning-augmented algorithms that take advantage of untrusted black-box advice (such as predictions from a machine learning model) to achieve significantly better average-case performance without sacrificing worst-case competitive guarantees. Finally, we empirically evaluate our proposed algorithms using a carbon-aware EV charging case study, showing that our algorithms substantially improve on baseline methods for this problem.
</details>
<details>
<summary>摘要</summary>
我们介绍和研究在线转换问题，这是一类在能源和可持续性领域出现的问题。在这个问题中，一个在线玩家尝试在固定时间跨度T内购买（或者卖出）资产的分数。在每个时间步骤中，一个成本函数（或者价格函数）会被公布，玩家必须不可逆决定一个资产的量。在 consecutive 时间步骤中改变决定时，玩家会付出跳转成本。我们引入了竞争（稳定）阈值基于算法，并证明它们在杜林在线算法中是优于的。然后，我们提出了学习增强的算法，它们可以通过不可信的黑盒模型（如机器学习模型的预测）来实现显著更好的平均情况性能，而无需牺牲最坏情况的竞争保证。最后，我们employs a carbon-aware EV charging case study to empirically evaluate our proposed algorithms, showing that they substantially improve on baseline methods for this problem.Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have different translations in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Power-of-Pre-trained-Language-Models-for-Offline-Reinforcement-Learning"><a href="#Unleashing-the-Power-of-Pre-trained-Language-Models-for-Offline-Reinforcement-Learning" class="headerlink" title="Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning"></a>Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20587">http://arxiv.org/abs/2310.20587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S. Du, Huazhe Xu</li>
<li>For: The paper is written for offline reinforcement learning (RL) in real-world scenarios where data collection is costly and risky, and the in-domain data is limited.* Methods: The paper introduces a general framework called $\textbf{LaMo}$ based on Decision Transformers, which effectively uses pre-trained Language Models (LMs) for offline RL. The framework includes four crucial components: (1) initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, (3) using non-linear MLP transformation instead of linear projections, and (4) integrating an auxiliary language prediction loss during fine-tuning.* Results: The paper achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks, especially in scenarios with limited data samples.Here’s the simplified Chinese text for the three information points:* 为: 本文是为offline Reinforcement Learning（RL）在现实世界中的数据收集成本高昂和风险的情况而写的。* 方法: 本文引入了一个通用框架 called $\textbf{LaMo}$，基于决策变换器，可以有效地使用预训练的语言模型（LMs）来进行offline RL。该框架包括四个关键组件：(1) 初始化决策变换器使用顺序预训练LMs，(2) 使用LoRA fine-tuning方法，相比于全量精度调整，可以有效地结合预训练知识从LMs和区域知识，(3) 使用非线性MLP变换而不是直线投影，生成嵌入，(4) 在精度调整过程中添加语言预测损失，以稳定LMs和保留其原有语言能力。* 结果: 本文在罕见奖励任务中达到了状态机器人学的最佳性能，并在粗略奖励任务中距离值基于offline RL方法和决策变换器的性能减少到最小。特别是在数据样本有限的情况下，本方法表现出了优秀的性能。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets. In real-world scenarios, data collection could be costly and risky; therefore, offline RL becomes particularly challenging when the in-domain data is limited. Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces $\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL. Our framework highlights four crucial components: (1) Initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to full-weight fine-tuning, to combine the pre-trained knowledge from LMs and in-domain knowledge effectively, (3) using the non-linear MLP transformation instead of linear projections, to generate embeddings, and (4) integrating an auxiliary language prediction loss during fine-tuning to stabilize the LMs and retain their original abilities on languages. Empirical results indicate $\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks. In particular, our method demonstrates superior performance in scenarios with limited data samples. Our project website is https://lamo2023.github.io
</details>
<details>
<summary>摘要</summary>
偏向学习（Offline Reinforcement Learning）的目标是找到近似优化策略，使用预收集的数据进行训练。在实际场景中，数据收集可能是成本高昂且风险重的，因此偏向学习在有限的域内数据时变得特别挑战。为了解决这个问题，这篇论文提出了一种基于决策变换器的框架，称为语言模型 для动作控制（LaMo）。我们的框架包括四个关键组成部分：1. 使用顺序预训练的语言模型（LM）初始化决策变换器。2. 使用LoRA fine-tuning方法，而不是全量 fine-tuning，将预训练知识从LM和域内知识相结合。3. 使用非线性的多层感知变换而不是线性投影，生成嵌入。4. 在练习过程中添加语言预测损失，以稳定LM和保留其原有语言能力。我们的实验结果表明，LaMo在稀有奖励任务中取得了状态最佳性表现，并在 dense-reward 任务中降低了决策变换器和值基本的偏向学习方法之间的差距。尤其是在有限数据样本的情况下，LaMo表现出色。有关我们的项目，请访问我们的项目网站：<https://lamo2023.github.io>
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Gradient-Descent-for-Gaussian-Processes-Done-Right"><a href="#Stochastic-Gradient-Descent-for-Gaussian-Processes-Done-Right" class="headerlink" title="Stochastic Gradient Descent for Gaussian Processes Done Right"></a>Stochastic Gradient Descent for Gaussian Processes Done Right</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20581">http://arxiv.org/abs/2310.20581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihao Andreas Lin, Shreyas Padhy, Javier Antorán, Austin Tripp, Alexander Terenin, Csaba Szepesvári, José Miguel Hernández-Lobato, David Janz</li>
<li>for: 本研究探讨了 Gaussian process regression 优化问题，使用平方损失函数。</li>
<li>methods: 本paper使用了 Stochastic dual gradient descent 算法，并提供了一些特定的设计选择，以提高其效果。</li>
<li>results: 对标准回归benchmark和一个 bayesian优化任务，本方法与 preconditioned conjugate gradients、variational Gaussian process approximations、以及之前的 Stochastic gradient descent for Gaussian processes 相比，显示了更高的竞争力。在一个分子绑定亲和力预测任务上，本方法使 Gaussian process regression 与STATE-OF-THE-ART graph neural networks 的性能相当。<details>
<summary>Abstract</summary>
We study the optimisation problem associated with Gaussian process regression using squared loss. The most common approach to this problem is to apply an exact solver, such as conjugate gradient descent, either directly, or to a reduced-order version of the problem. Recently, driven by successes in deep learning, stochastic gradient descent has gained traction as an alternative. In this paper, we show that when done right$\unicode{x2014}$by which we mean using specific insights from the optimisation and kernel communities$\unicode{x2014}$this approach is highly effective. We thus introduce a particular stochastic dual gradient descent algorithm, that may be implemented with a few lines of code using any deep learning framework. We explain our design decisions by illustrating their advantage against alternatives with ablation studies and show that the new method is highly competitive. Our evaluations on standard regression benchmarks and a Bayesian optimisation task set our approach apart from preconditioned conjugate gradients, variational Gaussian process approximations, and a previous version of stochastic gradient descent for Gaussian processes. On a molecular binding affinity prediction task, our method places Gaussian process regression on par in terms of performance with state-of-the-art graph neural networks.
</details>
<details>
<summary>摘要</summary>
我们研究了 Gaussian process regression 的优化问题，使用平方损失函数。最常见的方法是使用精确的解法，如 conjugate gradient descent，直接或者将问题缩放到一个更小的版本上。在这篇文章中，我们表明了使用 deep learning 的成功，stochastic gradient descent 已成为一种有力的代替方法。当使用特定的优化和内核社区的知识时，这种方法非常有效。因此，我们介绍了一种特定的随机对角 gradient descent 算法，可以使用任何深度学习框架进行实现，只需几行代码即可。我们解释了我们的设计决策，并通过减少对比研究表明其优势。我们的评估结果表明，我们的方法在标准的回归 benchmark 和 Bayesian 优化任务上与前conditioned conjugate gradients、variational Gaussian process approximation 和一个前一个随机 gradient descent 方法相比较，表现更出色。在一个分子绑定亲和力预测任务上，我们的方法使 Gaussian process regression 与 state-of-the-art graph neural networks 的性能相当。
</details></li>
</ul>
<hr>
<h2 id="Initialization-Matters-Privacy-Utility-Analysis-of-Overparameterized-Neural-Networks"><a href="#Initialization-Matters-Privacy-Utility-Analysis-of-Overparameterized-Neural-Networks" class="headerlink" title="Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks"></a>Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20579">http://arxiv.org/abs/2310.20579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Ye, Zhenyu Zhu, Fanghui Liu, Reza Shokri, Volkan Cevher</li>
<li>for: 这个论文 investigate 随机机器学习算法中模型过参数化对训练数据泄露信息的影响。</li>
<li>methods: 作者Proof a privacy bound for the KL divergence between model distributions on worst-case neighboring datasets, and explore its dependence on initialization, width, and depth of fully connected neural networks.</li>
<li>results: 研究发现，这个KL privacy bound largely depends on the expected squared gradient norm relative to model parameters during training. 特别是，在linearized network setting中，squared gradient norm directly tied to the per-layer variance of the initialization distribution. 此外，作者还证明了在固定KL privacy budget下的过employmerisk bounds,并发现了训练深度和 initialization distribution之间的复杂关系。<details>
<summary>Abstract</summary>
We analytically investigate how over-parameterization of models in randomized machine learning algorithms impacts the information leakage about their training data. Specifically, we prove a privacy bound for the KL divergence between model distributions on worst-case neighboring datasets, and explore its dependence on the initialization, width, and depth of fully connected neural networks. We find that this KL privacy bound is largely determined by the expected squared gradient norm relative to model parameters during training. Notably, for the special setting of linearized network, our analysis indicates that the squared gradient norm (and therefore the escalation of privacy loss) is tied directly to the per-layer variance of the initialization distribution. By using this analysis, we demonstrate that privacy bound improves with increasing depth under certain initializations (LeCun and Xavier), while degrades with increasing depth under other initializations (He and NTK). Our work reveals a complex interplay between privacy and depth that depends on the chosen initialization distribution. We further prove excess empirical risk bounds under a fixed KL privacy budget, and show that the interplay between privacy utility trade-off and depth is similarly affected by the initialization.
</details>
<details>
<summary>摘要</summary>
我们分析了随机机器学习算法中模型过参数化对训练数据信息泄露的影响。特别是，我们证明了模型分布之间的KL差 privacy bound，并研究其取决于模型初始化、宽度和深度。我们发现这个KL privacy bound与模型训练过程中参数的预期平方Gradient norm之间有直接关系。特别是，对于特殊的线性化网络设置，我们的分析表明，预期平方Gradient norm（并因此隐私损失的增加）与初始化分布的层次变化直接相关。通过这种分析，我们证明了深度随着初始化的变化而改善的隐私约束，而对于其他初始化而言，隐私约束会随着深度的增加而下降。我们的工作表明了隐私和深度之间的复杂互动，这种互动取决于选择的初始化分布。我们还证明了fixed KL隐私预算下的过 Training Empirical Risk bounds，并显示了隐私utiltytrade-off和深度之间的相互作用，这种相互作用也受到初始化的影响。
</details></li>
</ul>
<hr>
<h2 id="Information-Theoretic-Trust-Regions-for-Stochastic-Gradient-Based-Optimization"><a href="#Information-Theoretic-Trust-Regions-for-Stochastic-Gradient-Based-Optimization" class="headerlink" title="Information-Theoretic Trust Regions for Stochastic Gradient-Based Optimization"></a>Information-Theoretic Trust Regions for Stochastic Gradient-Based Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20574">http://arxiv.org/abs/2310.20574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alrhub/arturo">https://github.com/alrhub/arturo</a></li>
<li>paper_authors: Philipp Dahlinger, Philipp Becker, Maximilian Hüttenrauch, Gerhard Neumann</li>
<li>for: 优化神经网络中的梯度下降法是关键，而现有的方法通过减小步长和方向的补做来尝试优化搜索方法，但这些方法并不是很理智的。本文提出一种基于信息理论的信任区域优化方法（arTuRO），可以在梯度下降的情况下更好地利用第二阶信息。</li>
<li>methods: 本文使用信息理论信任区域优化方法（arTuRO），其中首先模型神经网络参数为高斯分布，然后使用Kullback-Leibler偏度来确定信任区域，最后在信任区域内解决最优步长问题，以获得更稳定和更快的优化过程。在计算信任区域中，我们使用随机最小二乘方法来 aproximate对象函数的对偶矩阵的元素。</li>
<li>results: 作者表明，使用arTuRO可以结合渐进式权重更新和SGD的快速收敛和泛化能力。<details>
<summary>Abstract</summary>
Stochastic gradient-based optimization is crucial to optimize neural networks. While popular approaches heuristically adapt the step size and direction by rescaling gradients, a more principled approach to improve optimizers requires second-order information. Such methods precondition the gradient using the objective's Hessian. Yet, computing the Hessian is usually expensive and effectively using second-order information in the stochastic gradient setting is non-trivial. We propose using Information-Theoretic Trust Region Optimization (arTuRO) for improved updates with uncertain second-order information. By modeling the network parameters as a Gaussian distribution and using a Kullback-Leibler divergence-based trust region, our approach takes bounded steps accounting for the objective's curvature and uncertainty in the parameters. Before each update, it solves the trust region problem for an optimal step size, resulting in a more stable and faster optimization process. We approximate the diagonal elements of the Hessian from stochastic gradients using a simple recursive least squares approach, constructing a model of the expected Hessian over time using only first-order information. We show that arTuRO combines the fast convergence of adaptive moment-based optimization with the generalization capabilities of SGD.
</details>
<details>
<summary>摘要</summary>
We propose using Information-Theoretic Trust Region Optimization (arTuRO) for improved updates with uncertain second-order information. By modeling the network parameters as a Gaussian distribution and using a Kullback-Leibler divergence-based trust region, our approach takes bounded steps accounting for the objective's curvature and uncertainty in the parameters. Before each update, it solves the trust region problem for an optimal step size, resulting in a more stable and faster optimization process.We approximate the diagonal elements of the Hessian from stochastic gradients using a simple recursive least squares approach, constructing a model of the expected Hessian over time using only first-order information. We show that arTuRO combines the fast convergence of adaptive moment-based optimization with the generalization capabilities of SGD.
</details></li>
</ul>
<hr>
<h2 id="One-shot-backpropagation-for-multi-step-prediction-in-physics-based-system-identification"><a href="#One-shot-backpropagation-for-multi-step-prediction-in-physics-based-system-identification" class="headerlink" title="One-shot backpropagation for multi-step prediction in physics-based system identification"></a>One-shot backpropagation for multi-step prediction in physics-based system identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20567">http://arxiv.org/abs/2310.20567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cesare Donati, Martina Mammarella, Fabrizio Dabbene, Carlo Novara, Constantino Lagoa</li>
<li>for: 提出一种通用框架，用于识别可能相连的系统，保持物理性质和多步预测精度。</li>
<li>methods: 提出一种分析和递归算法，用于计算多步损失函数的梯度，基于反射学习算法，提供物理和结构直观。</li>
<li>results: 在Space debris的静态观测数据上测试了提出的方法，实现了准确的阻尼矩计算。<details>
<summary>Abstract</summary>
The aim of this paper is to present a novel general framework for the identification of possibly interconnected systems, while preserving their physical properties and providing accuracy in multi-step prediction. An analytical and recursive algorithm for the gradient computation of the multi-step loss function based on backpropagation is introduced, providing physical and structural insight directly into the learning algorithm. As a case study, the proposed approach is tested for estimating the inertia matrix of a space debris starting from state observations.
</details>
<details>
<summary>摘要</summary>
本文的目的是提出一种新的总体框架，用于可能相互连接的系统的特征标识，保持物理性质和多步预测精度。我们提出了一种分析和递归的梯度计算方法，基于反射传播来计算多步损失函数的梯度，从而直接将学习算法中的物理和结构性质带入到学习过程中。作为案例研究，我们对一个空间垃圾的惯性矩进行估计。
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-design-of-graph-neural-networks-with-applications-to-vertical-federated-learning"><a href="#Privacy-preserving-design-of-graph-neural-networks-with-applications-to-vertical-federated-learning" class="headerlink" title="Privacy-preserving design of graph neural networks with applications to vertical federated learning"></a>Privacy-preserving design of graph neural networks with applications to vertical federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20552">http://arxiv.org/abs/2310.20552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruofan Wu, Mingyang Zhang, Lingjuan Lyu, Xiaolong Xu, Xiuquan Hao, Xinyi Fu, Tengfei Liu, Tianyi Zhang, Weiqiang Wang</li>
<li>for: 这篇论文旨在提出一个称为VESPER的纵向联邦学习（VFL）框架，用于金融风险管理（FRM）应用中。</li>
<li>methods: 这篇论文使用了一种称为干扰讯息传递（PMP）的通用隐私保证方案，以适应各种几何学神经网络。</li>
<li>results: 实验结果显示VESPER可以在合理的隐私预算下训练高性能的几何学神经网络，并且适用于稠密和稀疏几何学数据。<details>
<summary>Abstract</summary>
The paradigm of vertical federated learning (VFL), where institutions collaboratively train machine learning models via combining each other's local feature or label information, has achieved great success in applications to financial risk management (FRM). The surging developments of graph representation learning (GRL) have opened up new opportunities for FRM applications under FL via efficiently utilizing the graph-structured data generated from underlying transaction networks. Meanwhile, transaction information is often considered highly sensitive. To prevent data leakage during training, it is critical to develop FL protocols with formal privacy guarantees. In this paper, we present an end-to-end GRL framework in the VFL setting called VESPER, which is built upon a general privatization scheme termed perturbed message passing (PMP) that allows the privatization of many popular graph neural architectures.Based on PMP, we discuss the strengths and weaknesses of specific design choices of concrete graph neural architectures and provide solutions and improvements for both dense and sparse graphs. Extensive empirical evaluations over both public datasets and an industry dataset demonstrate that VESPER is capable of training high-performance GNN models over both sparse and dense graphs under reasonable privacy budgets.
</details>
<details>
<summary>摘要</summary>
vertical 联合学习（VFL）的 Paradigma，where institutions collaboratively train machine learning models by combining each other's local feature or label information, has achieved great success in financial risk management（FRM）applications. The surging developments of graph representation learning（GRL）have opened up new opportunities for FRM applications under FL by efficiently utilizing the graph-structured data generated from underlying transaction networks. However, transaction information is often considered highly sensitive. To prevent data leakage during training, it is critical to develop FL protocols with formal privacy guarantees.In this paper, we present an end-to-end GRL framework in the VFL setting called VESPER, which is built upon a general privatization scheme termed perturbed message passing（PMP）that allows the privatization of many popular graph neural architectures. Based on PMP, we discuss the strengths and weaknesses of specific design choices of concrete graph neural architectures and provide solutions and improvements for both dense and sparse graphs. Extensive empirical evaluations over both public datasets and an industry dataset demonstrate that VESPER is capable of training high-performance GNN models over both sparse and dense graphs under reasonable privacy budgets.
</details></li>
</ul>
<hr>
<h2 id="Multi-task-learning-of-convex-combinations-of-forecasting-models"><a href="#Multi-task-learning-of-convex-combinations-of-forecasting-models" class="headerlink" title="Multi-task learning of convex combinations of forecasting models"></a>Multi-task learning of convex combinations of forecasting models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20545">http://arxiv.org/abs/2310.20545</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antoniosudoso/mtl-comb">https://github.com/antoniosudoso/mtl-comb</a></li>
<li>paper_authors: Giovanni Felici, Antonio M. Sudoso</li>
<li>for: 这篇论文的目的是提出一种多任务学习方法，以同时解决Feature-based forecasting中的模型选择和模型组合问题。</li>
<li>methods: 这篇论文使用了一个深度神经网络，其中有两个分支：回归分支，用于学习不同预测方法的重量，以及分类分支，用于选择最适合的预测方法。</li>
<li>results: 实验结果显示，这篇论文的提案可以与现有的方法相比，提高预测精度。<details>
<summary>Abstract</summary>
Forecast combination involves using multiple forecasts to create a single, more accurate prediction. Recently, feature-based forecasting has been employed to either select the most appropriate forecasting models or to learn the weights of their convex combination. In this paper, we present a multi-task learning methodology that simultaneously addresses both problems. This approach is implemented through a deep neural network with two branches: the regression branch, which learns the weights of various forecasting methods by minimizing the error of combined forecasts, and the classification branch, which selects forecasting methods with an emphasis on their diversity. To generate training labels for the classification task, we introduce an optimization-driven approach that identifies the most appropriate methods for a given time series. The proposed approach elicits the essential role of diversity in feature-based forecasting and highlights the interplay between model combination and model selection when learning forecasting ensembles. Experimental results on a large set of series from the M4 competition dataset show that our proposal enhances point forecast accuracy compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
预测组合是使用多个预测来生成一个更准确的预测。最近，基于特征的预测组合被应用来选择最适合的预测模型或学习预测模型的权重。在这篇论文中，我们提出了一种多任务学习方法，同时解决了这两个问题。这种方法通过深度神经网络中的两个分支：回归分支和分类分支来实现。回归分支通过最小化组合预测错误来学习不同预测方法的权重，而分类分支通过强调多样性来选择适合的预测方法。为生成训练标签的分类任务，我们提出了一种优化驱动的方法，该方法可以为给定时序序列选择最佳的预测方法。我们的方法强调了特征基于预测组合中的多样性，并高亮了组合预测和选择预测方法之间的交互关系。实验结果表明，我们的提议在M4竞赛数据集上的大量时序序列上比state-of-the-art方法提高点预测精度。
</details></li>
</ul>
<hr>
<h2 id="Group-Feature-Sensor-Selection-With-Controlled-Redundancy-Using-Neural-Networks"><a href="#Group-Feature-Sensor-Selection-With-Controlled-Redundancy-Using-Neural-Networks" class="headerlink" title="Group-Feature (Sensor) Selection With Controlled Redundancy Using Neural Networks"></a>Group-Feature (Sensor) Selection With Controlled Redundancy Using Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20524">http://arxiv.org/abs/2310.20524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aytijhya Saha, Nikhil R. Pal</li>
<li>for: 本研究提出了一种基于多层感知器（MLP）网络的嵌入特征选择方法，用于控制特征或感知器之间的重复性。</li>
<li>methods: 本方法使用了一种扩展的组征或感知器选择问题，并将组征lasso penalty应用于选择有价值的组征，同时保持特征之间的重复性控制。</li>
<li>results: 我们在多个 benchmark 数据集上进行了实验，并证明了该方法在特征选择和组征选择方面的批处性和稳定性，并且在一些现状方法之上显示出了优异性。<details>
<summary>Abstract</summary>
In this paper, we present a novel embedded feature selection method based on a Multi-layer Perceptron (MLP) network and generalize it for group-feature or sensor selection problems, which can control the level of redundancy among the selected features or groups. Additionally, we have generalized the group lasso penalty for feature selection to encompass a mechanism for selecting valuable group features while simultaneously maintaining a control over redundancy. We establish the monotonicity and convergence of the proposed algorithm, with a smoothed version of the penalty terms, under suitable assumptions. Experimental results on several benchmark datasets demonstrate the promising performance of the proposed methodology for both feature selection and group feature selection over some state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种基于多层感知网络（MLP）的嵌入特征选择方法，并扩展了其用于组特征或感知选择问题，以控制选择特征或组的重复级别。此外，我们扩展了组lasso负面penalty来选择有价值的组特征，同时保持特征或组的重复级别控制。我们证明提案的算法具有升elinicity和收敛性，在适当的假设下。实验结果表明，提案的方法在多个标准数据集上表现出色，在特征选择和组特征选择方面超越了一些现状方法。
</details></li>
</ul>
<hr>
<h2 id="Parametric-Fairness-with-Statistical-Guarantees"><a href="#Parametric-Fairness-with-Statistical-Guarantees" class="headerlink" title="Parametric Fairness with Statistical Guarantees"></a>Parametric Fairness with Statistical Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20508">http://arxiv.org/abs/2310.20508</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paramfair/submission_974">https://github.com/paramfair/submission_974</a></li>
<li>paper_authors: François HU, Philipp Ratz, Arthur Charpentier</li>
<li>for: 这个论文旨在探讨机器学习模型中的公平问题，并提出一种基于域知识的公平度量来解决这些问题。</li>
<li>methods: 论文使用了一种基于分布性质的公平度量，即DEMOGRAPHIC PARITY，并提出了一种 Parametric 方法来实现这种公平度量。</li>
<li>results: 论文通过一个实际的薪资示例来说明了这种新的公平度量和 Parametric 方法的应用，并证明了它们可以减少 intersect 性公平问题并提供一个有效的实际应用解决方案。<details>
<summary>Abstract</summary>
Algorithmic fairness has gained prominence due to societal and regulatory concerns about biases in Machine Learning models. Common group fairness metrics like Equalized Odds for classification or Demographic Parity for both classification and regression are widely used and a host of computationally advantageous post-processing methods have been developed around them. However, these metrics often limit users from incorporating domain knowledge. Despite meeting traditional fairness criteria, they can obscure issues related to intersectional fairness and even replicate unwanted intra-group biases in the resulting fair solution. To avoid this narrow perspective, we extend the concept of Demographic Parity to incorporate distributional properties in the predictions, allowing expert knowledge to be used in the fair solution. We illustrate the use of this new metric through a practical example of wages, and develop a parametric method that efficiently addresses practical challenges like limited training data and constraints on total spending, offering a robust solution for real-life applications.
</details>
<details>
<summary>摘要</summary>
“algorithmic fairness”在社会和 regulatory 的关注下受到更多的注意，因为机器学习模型中的偏见问题。通用的集体公平度量 like “Equalized Odds”和“Demographic Parity”在分类和回归方面都很受欢迎，但这些度量往往限制用户不能 incorporate 专业知识。尽管遵循传统的公平准确，它们可能会隐藏 intersectional 公平问题，甚至在 fair 解决方案中重复不想要的 intra-group 偏见。为了避免这种狭隘的见解，我们将 Demographic Parity 扩展为包括预测的分布性质，让专业知识得以在 fair 解决方案中使用。我们透过一个实际的薪资示例来解释使用这个新度量，并开发了一个可效的 parametric 方法，可以有效地 Address 实际应用中的问题，如有限的训练数据和总支付限制。这个方法可以提供实际应用中的坚固解决方案。
</details></li>
</ul>
<hr>
<h2 id="Generative-Learning-of-Continuous-Data-by-Tensor-Networks"><a href="#Generative-Learning-of-Continuous-Data-by-Tensor-Networks" class="headerlink" title="Generative Learning of Continuous Data by Tensor Networks"></a>Generative Learning of Continuous Data by Tensor Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20498">http://arxiv.org/abs/2310.20498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Meiburg, Jing Chen, Jacob Miller, Raphaëlle Tihon, Guillaume Rabusseau, Alejandro Perdomo-Ortiz</li>
<li>for:  solve machine learning problems, especially unsupervised generative learning</li>
<li>methods:  introduce a new family of tensor network generative models for continuous data, which can learn from distributions containing continuous random variables</li>
<li>results:  the model can approximate any reasonably smooth probability density function with arbitrary precision, and performs well on synthetic and real-world datasets, with the ability to model different data domains and a trainable compression layer to increase performance given limited resources.<details>
<summary>Abstract</summary>
Beyond their origin in modeling many-body quantum systems, tensor networks have emerged as a promising class of models for solving machine learning problems, notably in unsupervised generative learning. While possessing many desirable features arising from their quantum-inspired nature, tensor network generative models have previously been largely restricted to binary or categorical data, limiting their utility in real-world modeling problems. We overcome this by introducing a new family of tensor network generative models for continuous data, which are capable of learning from distributions containing continuous random variables. We develop our method in the setting of matrix product states, first deriving a universal expressivity theorem proving the ability of this model family to approximate any reasonably smooth probability density function with arbitrary precision. We then benchmark the performance of this model on several synthetic and real-world datasets, finding that the model learns and generalizes well on distributions of continuous and discrete variables. We develop methods for modeling different data domains, and introduce a trainable compression layer which is found to increase model performance given limited memory or computational resources. Overall, our methods give important theoretical and empirical evidence of the efficacy of quantum-inspired methods for the rapidly growing field of generative learning.
</details>
<details>
<summary>摘要</summary>
以下文本将被翻译成简化字符的中文：超过它们的起源于模拟多体量子系统的应用，tensor网络已经出现为解决机器学习问题的一个有前途的类型，尤其是无监督生成学习。具有许多愿望的特性，tensor网络生成模型却在过去被限制于二进制或 categorical 数据，这限制了它在实际模型问题中的应用。我们在这篇文章中解决了这个问题，我们引入了一个新的 tensor网络生成模型家族，可以处理包含连续随机变量的分布。我们首先在matrix product states 的设定下，得出了一个通用表达能力定理，证明这种模型家族可以将任何reasonably smooth 的概率密度函数表示为任意精度。然后，我们对几个 sintetic 和实际数据集进行了 benchmark，发现这种模型可以很好地学习和泛化在包含连续和二进制变量的分布上。我们还开发了不同数据域的模型方法，并引入了可调压缩层，这有助于在限定内存或计算资源的情况下提高模型性能。总的来说，我们的方法为生成学习领域提供了重要的理论和实证证明，证明了量子启发的方法在发展中的重要性。
</details></li>
</ul>
<hr>
<h2 id="BasisFormer-Attention-based-Time-Series-Forecasting-with-Learnable-and-Interpretable-Basis"><a href="#BasisFormer-Attention-based-Time-Series-Forecasting-with-Learnable-and-Interpretable-Basis" class="headerlink" title="BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis"></a>BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20496">http://arxiv.org/abs/2310.20496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nzl5116190/basisformer">https://github.com/nzl5116190/basisformer</a></li>
<li>paper_authors: Zelin Ni, Hang Yu, Shizhan Liu, Jianguo Li, Weiyao Lin</li>
<li>for: 这篇论文是针对时间序列预测 задачі的研究，旨在提出一个可学习和可解释的基底架构，以提高时间序列预测的精度。</li>
<li>methods: 本文使用了自适应式自我超vised learning来获取基底，然后通过对时间序列和基底之间的相互关联进行 Calculate similarity coefficients，最后使用这些相互关联来预测未来的时间序列。</li>
<li>results: 经过实验证明，这篇论文的方法可以与之前的现有方法相比，在uniivariate和multivariate预测任务上分别提高了11.04%和15.78%的精度。<details>
<summary>Abstract</summary>
Bases have become an integral part of modern deep learning-based models for time series forecasting due to their ability to act as feature extractors or future references. To be effective, a basis must be tailored to the specific set of time series data and exhibit distinct correlation with each time series within the set. However, current state-of-the-art methods are limited in their ability to satisfy both of these requirements simultaneously. To address this challenge, we propose BasisFormer, an end-to-end time series forecasting architecture that leverages learnable and interpretable bases. This architecture comprises three components: First, we acquire bases through adaptive self-supervised learning, which treats the historical and future sections of the time series as two distinct views and employs contrastive learning. Next, we design a Coef module that calculates the similarity coefficients between the time series and bases in the historical view via bidirectional cross-attention. Finally, we present a Forecast module that selects and consolidates the bases in the future view based on the similarity coefficients, resulting in accurate future predictions. Through extensive experiments on six datasets, we demonstrate that BasisFormer outperforms previous state-of-the-art methods by 11.04\% and 15.78\% respectively for univariate and multivariate forecasting tasks. Code is available at: \url{https://github.com/nzl5116190/Basisformer}
</details>
<details>
<summary>摘要</summary>
基于深度学习的时间序列预测模型中，基因已成为一个重要的组成部分，因为它们可以作为特征提取器或未来参照。为了有效，一个基因必须适应特定的时间序列数据集，并且与每个时间序列在该集合中存在明显的相关性。然而，当前状态艺术方法有限，不能同时满足这两个需求。为解决这个挑战，我们提出了 BasisFormer，一种端到端的时间序列预测架构，利用可学习和可解释的基因。这个架构包括三个组件：1. 我们通过适应式自监学习获取基因，将历史和未来段视为两个不同的视图，并使用对比学习。2. 我们设计了 Coef 模块，通过双向креustration来计算历史视图中时间序列和基因之间的相似性系数。3. 我们则提出了预测模块，根据相似性系数选择和结合未来视图中的基因，以获得准确的未来预测。经过对六个数据集的广泛实验，我们证明了 BasisFormer 比前一代方法提高了11.04%和15.78%，分别在单variate和多variate预测任务中。代码可以在 GitHub 上获取：\url{https://github.com/nzl5116190/Basisformer}
</details></li>
</ul>
<hr>
<h2 id="Requirement-falsification-for-cyber-physical-systems-using-generative-models"><a href="#Requirement-falsification-for-cyber-physical-systems-using-generative-models" class="headerlink" title="Requirement falsification for cyber-physical systems using generative models"></a>Requirement falsification for cyber-physical systems using generative models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20493">http://arxiv.org/abs/2310.20493</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mshaheryarmalik/stgem">https://github.com/mshaheryarmalik/stgem</a></li>
<li>paper_authors: Jarkko Peltomäki, Ivan Porres</li>
<li>for:  automatic requirement falsification of cyber-physical systems</li>
<li>methods:  OGAN algorithm (uses generative machine learning model to produce counterexamples)</li>
<li>results:  state-of-the-art CPS falsification efficiency and effectiveness, can be applied to new systems with little effort, and exhibits few requirements for the system under test.Here’s the full text in Simplified Chinese:</li>
<li>for:  автоматиче检查 cyber-physical systems 的需求是否成立</li>
<li>methods:  OGAN 算法 (使用生成机器学习模型生成 counterexample)</li>
<li>results:  state-of-the-art CPS 验证效果和灵活性，可以轻松应用于新的系统，需求对系统 under test 非常低，验证效果卓越。<details>
<summary>Abstract</summary>
We present the OGAN algorithm for automatic requirement falsification of cyber-physical systems. System inputs and output are represented as piecewise constant signals over time while requirements are expressed in signal temporal logic. OGAN can find inputs that are counterexamples for the safety of a system revealing design, software, or hardware defects before the system is taken into operation. The OGAN algorithm works by training a generative machine learning model to produce such counterexamples. It executes tests atomically and does not require any previous model of the system under test. We evaluate OGAN using the ARCH-COMP benchmark problems, and the experimental results show that generative models are a viable method for requirement falsification. OGAN can be applied to new systems with little effort, has few requirements for the system under test, and exhibits state-of-the-art CPS falsification efficiency and effectiveness.
</details>
<details>
<summary>摘要</summary>
我团队提出了OGAN算法，用于自动化cyber-physical系统的需求证明。系统输入和输出被表示为时间上分割的常量信号，而需求则是表示在时间逻辑推理中。OGAN可以找到系统的安全性不足的输入，揭示设计、软件或硬件的缺陷，从而避免在系统被运行前就发现这些缺陷。OGAN算法通过训练生成机器学习模型来生成这些Counterexample。它在原子性执行测试，不需要任何先前系统模型，并且可以应用于新的系统，需要 minimal effort，系统测试的效率和效果都达到了领先水平。Note: "cyber-physical systems" in the original text is translated as "cyber-physical系统" in Simplified Chinese, which is a common way to refer to such systems in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Log-based-Anomaly-Detection-of-Enterprise-Software-An-Empirical-Study"><a href="#Log-based-Anomaly-Detection-of-Enterprise-Software-An-Empirical-Study" class="headerlink" title="Log-based Anomaly Detection of Enterprise Software: An Empirical Study"></a>Log-based Anomaly Detection of Enterprise Software: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20492">http://arxiv.org/abs/2310.20492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadun Wijesinghe, Hadi Hemmati</li>
<li>for: 本研究旨在评估现有state-of-the-art anomaly detection模型在一个企业 dataset上的性能，以及对不同类型异常的检测。</li>
<li>methods: 本研究使用了多种sequence-based deep neural network模型，包括Long-Short Term Memory和Transformer-based模型，进行异常检测。</li>
<li>results: 结果表明，不同模型在不同类型异常检测中表现不同，特别是在 less-structured datasets 中。此外， removing a common data leak associated with a random train-test split in some prior work 可以提高模型的效果。<details>
<summary>Abstract</summary>
Most enterprise applications use logging as a mechanism to diagnose anomalies, which could help with reducing system downtime. Anomaly detection using software execution logs has been explored in several prior studies, using both classical and deep neural network-based machine learning models. In recent years, the research has largely focused in using variations of sequence-based deep neural networks (e.g., Long-Short Term Memory and Transformer-based models) for log-based anomaly detection on open-source data. However, they have not been applied in industrial datasets, as often. In addition, the studied open-source datasets are typically very large in size with logging statements that do not change much over time, which may not be the case with a dataset from an industrial service that is relatively new. In this paper, we evaluate several state-of-the-art anomaly detection models on an industrial dataset from our research partner, which is much smaller and loosely structured than most large scale open-source benchmark datasets. Results show that while all models are capable of detecting anomalies, certain models are better suited for less-structured datasets. We also see that model effectiveness changes when a common data leak associated with a random train-test split in some prior work is removed. A qualitative study of the defects' characteristics identified by the developers on the industrial dataset further shows strengths and weaknesses of the models in detecting different types of anomalies. Finally, we explore the effect of limited training data by gradually increasing the training set size, to evaluate if the model effectiveness does depend on the training set size.
</details>
<details>
<summary>摘要</summary>
大多数企业应用程序使用日志作为诊断异常的机制，以减少系统停机时间。在先前的研究中，使用了类传统和深度神经网络机器学习模型进行异常检测，使用日志执行记录。最近几年，研究主要集中在使用日志中的序列基本深度神经网络模型（如Long-Short Term Memory和Transformer-based模型）进行异常检测。但是，它们尚未在工业数据集上应用。此外，研究中的开源数据集通常很大，日志语句不会随时间变化，这可能不是工业数据集的情况。在这篇论文中，我们评估了一些当前最佳的异常检测模型在工业数据集中的性能。结果显示，虽然所有模型都能检测异常，但certain models更适合 Less-structured 数据集。我们还发现，模型的效果会随着一些常见的数据泄露（在一些先前的工作中的随机train-test split）的去除而变化。另外，对工业数据集中开发者所标识的异常的特点进行质量研究，显示了不同类型的异常的模型的优缺点。最后，我们查探了模型效果是否受训练数据集大小的影响，通过逐渐增加训练集大小来评估模型的效果。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Practitioner-Perspectives-On-Training-Data-Attribution-Explanations"><a href="#Exploring-Practitioner-Perspectives-On-Training-Data-Attribution-Explanations" class="headerlink" title="Exploring Practitioner Perspectives On Training Data Attribution Explanations"></a>Exploring Practitioner Perspectives On Training Data Attribution Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20477">http://arxiv.org/abs/2310.20477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elisa Nguyen, Evgenii Kortukov, Jean Song, Seong Joon Oh</li>
<li>for: 本研究旨在探讨训练数据贡献（TDA）解释的可用性和设计空间。</li>
<li>methods: 本研究通过对10名实践者进行采访，了解TDA解释的可能性并探讨实践中的应用场景。</li>
<li>results: 研究发现，训练数据质量是模型性能高的关键因素，模型开发者主要依靠自己的经验来筛选数据。用户希望解释能够增强与模型的交互，而不一定优先考虑训练数据作为解释方法。 participant 中发现，TDA解释并不很熟悉，因此不常使用。 研究呼吁社区关注TDA技术的人机合作视角下的实用性，扩展TDA评估以涵盖实践中常见的应用场景。<details>
<summary>Abstract</summary>
Explainable AI (XAI) aims to provide insight into opaque model reasoning to humans and as such is an interdisciplinary field by nature. In this paper, we interviewed 10 practitioners to understand the possible usability of training data attribution (TDA) explanations and to explore the design space of such an approach. We confirmed that training data quality is often the most important factor for high model performance in practice and model developers mainly rely on their own experience to curate data. End-users expect explanations to enhance their interaction with the model and do not necessarily prioritise but are open to training data as a means of explanation. Within our participants, we found that TDA explanations are not well-known and therefore not used. We urge the community to focus on the utility of TDA techniques from the human-machine collaboration perspective and broaden the TDA evaluation to reflect common use cases in practice.
</details>
<details>
<summary>摘要</summary>
simplify Chinese:智能化解释（XAI）旨在为人类提供模型决策过程的洞察，因此是一个跨学科领域的研究。在这篇论文中，我们采访了10名实践者，以了解可能的训练数据归因（TDA）解释的可用性，并探索这种方法的设计空间。我们发现，实际中模型性能高的主要因素是训练数据质量，而模型开发者主要依靠自己的经验来册选数据。用户希望通过模型与人类的交互得到解释，并不一定优先考虑训练数据作为解释的方式。在我们的参与者中，我们发现TDA解释并不是广泛知道的，因此没有使用。我们呼吁社区在人机合作 perspective中关注TDA技术的实用性，并将TDA评价扩展到反映实际应用场景。
</details></li>
</ul>
<hr>
<h2 id="Amoeba-Circumventing-ML-supported-Network-Censorship-via-Adversarial-Reinforcement-Learning"><a href="#Amoeba-Circumventing-ML-supported-Network-Censorship-via-Adversarial-Reinforcement-Learning" class="headerlink" title="Amoeba: Circumventing ML-supported Network Censorship via Adversarial Reinforcement Learning"></a>Amoeba: Circumventing ML-supported Network Censorship via Adversarial Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20469">http://arxiv.org/abs/2310.20469</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mobile-intelligence-lab/amoeba">https://github.com/mobile-intelligence-lab/amoeba</a></li>
<li>paper_authors: Haoyu Liu, Alec F. Diallo, Paul Patras</li>
<li>for:  circumventing Internet censorship</li>
<li>methods:  using a novel reinforcement learning algorithm called Amoeba to generate adversarial flows that can deceive ML-based classifiers</li>
<li>results:  achieved an average attack success rate of 94% against a range of ML algorithms, and the adversarial flows are robust in different network environments and possess transferability across various ML models.Here’s the text in Simplified Chinese:</li>
<li>for: 绕过互联网审查</li>
<li>methods: 使用新型的强化学习算法”Amoeba”生成隐蔽的流量，以诱导分类器进行错误分类</li>
<li>results: 实现了对多种机器学习算法的94%攻击成功率，并且这些隐蔽的流量在不同的网络环境下具有可重复性和可转移性。<details>
<summary>Abstract</summary>
Embedding covert streams into a cover channel is a common approach to circumventing Internet censorship, due to censors' inability to examine encrypted information in otherwise permitted protocols (Skype, HTTPS, etc.). However, recent advances in machine learning (ML) enable detecting a range of anti-censorship systems by learning distinct statistical patterns hidden in traffic flows. Therefore, designing obfuscation solutions able to generate traffic that is statistically similar to innocuous network activity, in order to deceive ML-based classifiers at line speed, is difficult.   In this paper, we formulate a practical adversarial attack strategy against flow classifiers as a method for circumventing censorship. Specifically, we cast the problem of finding adversarial flows that will be misclassified as a sequence generation task, which we solve with Amoeba, a novel reinforcement learning algorithm that we design. Amoeba works by interacting with censoring classifiers without any knowledge of their model structure, but by crafting packets and observing the classifiers' decisions, in order to guide the sequence generation process. Our experiments using data collected from two popular anti-censorship systems demonstrate that Amoeba can effectively shape adversarial flows that have on average 94% attack success rate against a range of ML algorithms. In addition, we show that these adversarial flows are robust in different network environments and possess transferability across various ML models, meaning that once trained against one, our agent can subvert other censoring classifiers without retraining.
</details>
<details>
<summary>摘要</summary>
使用嵌入掩饰流入掩饰频道是常见的绕过互联网审查的方法，因为审查器无法检查加密的信息在允许的协议（Skype、HTTPS等）中。然而，最新的机器学习（ML）技术可以检测许多防火墙系统，因此设计生成假数据流的方法可以欺骗ML基于类型的分类器。在这篇论文中，我们提出了一种实用的敌意攻击策略，以逃脱审查。我们将找到攻击流的问题转化为序列生成任务，并使用我们设计的一种新的强化学习算法——Amoeba来解决。Amoeba通过与审查类ifiers进行互动，不需要知道审查器的模型结构，但是通过编辑包并观察审查器的决策，来引导序列生成过程。我们的实验结果表明，Amoeba可以有效地生成攻击流，其中94%的攻击成功率可以在多种ML算法面前具有抗性。此外，我们还证明这些攻击流在不同的网络环境中具有可重复性，可以在不同的ML模型上进行转移。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-detects-terminal-singularities"><a href="#Machine-learning-detects-terminal-singularities" class="headerlink" title="Machine learning detects terminal singularities"></a>Machine learning detects terminal singularities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20458">http://arxiv.org/abs/2310.20458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/fanosearch/ml_terminality">https://bitbucket.org/fanosearch/ml_terminality</a></li>
<li>paper_authors: Tom Coates, Alexander M. Kasprzyk, Sara Veneziale</li>
<li>for: 这个论文主要研究的是量子费托Varieties的分类。</li>
<li>methods: 作者使用机器学习方法来解决这个问题，开发了一个神经网络分类器，可以精准地判断8维具有托ри群Symmetry和Picard rank 2的代数多折形是否为量子费托多折形。</li>
<li>results: 作者通过使用机器学习方法，提出了一个初步的量子费托多折形的分类 landscape，并发现这些分类结果在量子期的视图中呈现为一个受限的区域，并且与费托指数相关。这些结果提供了新的证据，证明了机器学习可以成为数学推理的重要工具。<details>
<summary>Abstract</summary>
Algebraic varieties are the geometric shapes defined by systems of polynomial equations; they are ubiquitous across mathematics and science. Amongst these algebraic varieties are Q-Fano varieties: positively curved shapes which have Q-factorial terminal singularities. Q-Fano varieties are of fundamental importance in geometry as they are "atomic pieces" of more complex shapes - the process of breaking a shape into simpler pieces in this sense is called the Minimal Model Programme. Despite their importance, the classification of Q-Fano varieties remains unknown. In this paper we demonstrate that machine learning can be used to understand this classification. We focus on 8-dimensional positively-curved algebraic varieties that have toric symmetry and Picard rank 2, and develop a neural network classifier that predicts with 95% accuracy whether or not such an algebraic variety is Q-Fano. We use this to give a first sketch of the landscape of Q-Fanos in dimension 8. How the neural network is able to detect Q-Fano varieties with such accuracy remains mysterious, and hints at some deep mathematical theory waiting to be uncovered. Furthermore, when visualised using the quantum period, an invariant that has played an important role in recent theoretical developments, we observe that the classification as revealed by ML appears to fall within a bounded region, and is stratified by the Fano index. This suggests that it may be possible to state and prove conjectures on completeness in the future. Inspired by the ML analysis, we formulate and prove a new global combinatorial criterion for a positively curved toric variety of Picard rank 2 to have terminal singularities. Together with the first sketch of the landscape of Q-Fanos in higher dimensions, this gives new evidence that machine learning can be an essential tool in developing mathematical conjectures and accelerating theoretical discovery.
</details>
<details>
<summary>摘要</summary>
阿尔 геометрические形状是由系数方程定义的，它们在数学和科学中 ubique。其中包括Q-Fano形状，即正弦曲线的形状，它们具有Q- факторial终点特性。Q-Fano形状在几何学中具有基本重要性，因为它们是更复杂形状的“原子部件”，通过将形状拆分成更简单的部件来刻画形状的过程称为“最小模型Programme”。尽管其重要性，Q-Fano形状的分类仍然未知。在这篇论文中，我们使用机器学习来理解这个分类。我们关注8维正弦曲线的 positively-curved algebraic varieties，具有toric Symmetry和Picard rank 2，并开发了一个神经网络分类器，可以预测95%的概率是Q-Fano形状。我们使用这个分类器来给8维Q-Fano形状的首个绘图，并观察到Machine Learning可以准确地检测Q-Fano形状。这些结果表明，机器学习可以成为数学推理的重要工具。此外，我们还提出了一个全新的全球 combinatorial criterion，可以确定正弦曲线的 Picard rank 2 是否有终点特性。这与Q-Fano形状在更高维度的首个绘图和新的数学假设的证明都给出了新的证据。
</details></li>
</ul>
<hr>
<h2 id="FlexTrain-A-Dynamic-Training-Framework-for-Heterogeneous-Devices-Environments"><a href="#FlexTrain-A-Dynamic-Training-Framework-for-Heterogeneous-Devices-Environments" class="headerlink" title="FlexTrain: A Dynamic Training Framework for Heterogeneous Devices Environments"></a>FlexTrain: A Dynamic Training Framework for Heterogeneous Devices Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20457">http://arxiv.org/abs/2310.20457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mert Unsal, Ali Maatouk, Antonio De Domenico, Nicola Piovesan, Fadhel Ayed</li>
<li>for: 这篇论文是为了解决深度学习模型在不同设备环境中的问题，特别是对于低功率或资源有限的设备而言。</li>
<li>methods: 本论文提出了一个名为FlexTrain的框架，可以在训练阶段对不同设备的存储和计算资源进行整合，以便高效地部署深度学习模型。</li>
<li>results: 根据试验结果，FlexTrain可以实现对CIFAR-100 dataset的训练，并且可以在不同设备上运行，实现了训练时间和能源消耗的减少。此外，本论文还将FlexTrain扩展到联合学习设定下，与标准联合学习基准相比，FlexTrain在CIFAR-10和CIFAR-100 datasets上表现更好。<details>
<summary>Abstract</summary>
As deep learning models become increasingly large, they pose significant challenges in heterogeneous devices environments. The size of deep learning models makes it difficult to deploy them on low-power or resource-constrained devices, leading to long inference times and high energy consumption. To address these challenges, we propose FlexTrain, a framework that accommodates the diverse storage and computational resources available on different devices during the training phase. FlexTrain enables efficient deployment of deep learning models, while respecting device constraints, minimizing communication costs, and ensuring seamless integration with diverse devices. We demonstrate the effectiveness of FlexTrain on the CIFAR-100 dataset, where a single global model trained with FlexTrain can be easily deployed on heterogeneous devices, saving training time and energy consumption. We also extend FlexTrain to the federated learning setting, showing that our approach outperforms standard federated learning benchmarks on both CIFAR-10 and CIFAR-100 datasets.
</details>
<details>
<summary>摘要</summary>
As deep learning models become increasingly large, they pose significant challenges in heterogeneous devices environments. The size of deep learning models makes it difficult to deploy them on low-power or resource-constrained devices, leading to long inference times and high energy consumption. To address these challenges, we propose FlexTrain, a framework that accommodates the diverse storage and computational resources available on different devices during the training phase. FlexTrain enables efficient deployment of deep learning models, while respecting device constraints, minimizing communication costs, and ensuring seamless integration with diverse devices. We demonstrate the effectiveness of FlexTrain on the CIFAR-100 dataset, where a single global model trained with FlexTrain can be easily deployed on heterogeneous devices, saving training time and energy consumption. We also extend FlexTrain to the federated learning setting, showing that our approach outperforms standard federated learning benchmarks on both CIFAR-10 and CIFAR-100 datasets.Here's the translation in Traditional Chinese:当深度学习模型越来越大时，它们在多种设备环境中带来很大的挑战。模型的大小使得在低功率或资源受限的设备上部署深度学习模型 becomes increasingly difficult, leading to long inference times and high energy consumption. 为了解决这些挑战，我们提出了 FlexTrain 框架，该框架在训练阶段适应不同设备上的储存和计算资源。FlexTrain 可以有效地部署深度学习模型，同时尊重设备的限制，降低通信成本，并让不同设备进行顺当的集成。我们在 CIFAR-100  dataset 上验证了 FlexTrain 的有效性，发现可以使用 FlexTrain 将单一全球模型训练到多种设备，实现训练时间和能量消耗的优化。我们还将 FlexTrain 扩展到 federated learning  Setting，证明我们的方法在 CIFAR-10 和 CIFAR-100  dataset 上比标准 federated learning  benchmark 高效。
</details></li>
</ul>
<hr>
<h2 id="The-Phase-Transition-Phenomenon-of-Shuffled-Regression"><a href="#The-Phase-Transition-Phenomenon-of-Shuffled-Regression" class="headerlink" title="The Phase Transition Phenomenon of Shuffled Regression"></a>The Phase Transition Phenomenon of Shuffled Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20438">http://arxiv.org/abs/2310.20438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hang Zhang, Ping Li</li>
<li>for: 这 paper 主要研究了排序（permuted）回归问题中的相对稳定点现象，这种问题在数据库、隐私、数据分析等领域都有广泛应用。</li>
<li>methods: 本 paper 使用了消息传递（MP）技术来精确地定义相对稳定点的位置。首先， authors 将排序回归问题转换成了一个概率图模型，然后使用 MP 算法的分析工具来 derive 排序回归的渐进方程。通过将这个方程连接到分支渐进 random walk 过程，authors 可以Characterize 相对稳定点对 $\snr$ 的影响。</li>
<li>results: 在 oracle 和 non-oracle 两种情况下，authors 分别研究了相对稳定点的影响。在 oracle 情况下， authors 可以准确预测相对稳定点 $\snr$。在 non-oracle 情况下，authors 可以预测最多允许的排序行数和其与样本数的关系。<details>
<summary>Abstract</summary>
We study the phase transition phenomenon inherent in the shuffled (permuted) regression problem, which has found numerous applications in databases, privacy, data analysis, etc. In this study, we aim to precisely identify the locations of the phase transition points by leveraging techniques from message passing (MP). In our analysis, we first transform the permutation recovery problem into a probabilistic graphical model. We then leverage the analytical tools rooted in the message passing (MP) algorithm and derive an equation to track the convergence of the MP algorithm. By linking this equation to the branching random walk process, we are able to characterize the impact of the signal-to-noise-ratio ($\snr$) on the permutation recovery. Depending on whether the signal is given or not, we separately investigate the oracle case and the non-oracle case. The bottleneck in identifying the phase transition regimes lies in deriving closed-form formulas for the corresponding critical points, but only in rare scenarios can one obtain such precise expressions. To tackle this technical challenge, this study proposes the Gaussian approximation method, which allows us to obtain the closed-form formulas in almost all scenarios. In the oracle case, our method can fairly accurately predict the phase transition $\snr$. In the non-oracle case, our algorithm can predict the maximum allowed number of permuted rows and uncover its dependency on the sample number.
</details>
<details>
<summary>摘要</summary>
我们研究排序问题中的相变现象，这问题在数据库、隐私、数据分析等领域都有广泛应用。在这些研究中，我们想要精确地找出排序问题中的相变点，并且使用讯息传递（MP）技术来实现。我们首先将排序问题转换为一个probabilistic graphical model，然后使用MP算法的分析工具来 derive一个追踪MP算法的方程。通过与分支随机步进程连接这个方程，我们可以描述排序问题中的信号至杂音比例($\snr$)的影响。对于signal是否知道的情况，我们分别进行了oracle case和非oracle case的研究。排序问题中的瓶颈在于 derivation closed-formula for the corresponding critical points，但只有在 rare scenarios 可以取得如此精确的表达。为了解决这个技术挑战，本研究提出了Gaussian approximation方法，可以将closed-formula 在大多数情况下取得。在oracle case中，我们的方法可以很准确地预测相变 $\snr$。在非oracle case中，我们的算法可以预测排序中最多允许的排序行数和其对数据数量的依赖。
</details></li>
</ul>
<hr>
<h2 id="Discussing-the-Spectra-of-Physics-Enhanced-Machine-Learning-via-a-Survey-on-Structural-Mechanics-Applications"><a href="#Discussing-the-Spectra-of-Physics-Enhanced-Machine-Learning-via-a-Survey-on-Structural-Mechanics-Applications" class="headerlink" title="Discussing the Spectra of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications"></a>Discussing the Spectra of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20425">http://arxiv.org/abs/2310.20425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcus Haywood-Alexander, Wei Liu, Kiran Bacsa, Zhilu Lai, Eleni Chatzi</li>
<li>for: 本文旨在探讨物理学和机器学习之间的交叠，提出了physics-enhanced machine learning（PEML）方法，以提高机器学习方法的能力和降低数据或物理方法缺点。</li>
<li>methods: 本文通过对物理和数据两个定义轴的PEML方法进行全面探讨，描述了这些方法的特点、使用场景和动机。 更进一步，本文还介绍了一些最近应用和开发的PEML技术，并通过使用一个简单的单度oscillator示例，展示了不同类型PEML方法的特点和动机。</li>
<li>results: 本文的应用示例和开发技术，演示了PEML在复杂挑战中的力量。此外，为促进合作和透明度，本文附加了代码，以便读者可以实践。作为基础贡献，本文强调了PEML在科学和工程研究中的重要性，由物理理解和机器学习能力的合作下支持。<details>
<summary>Abstract</summary>
The intersection of physics and machine learning has given rise to a paradigm that we refer to here as physics-enhanced machine learning (PEML), aiming to improve the capabilities and reduce the individual shortcomings of data- or physics-only methods. In this paper, the spectrum of physics-enhanced machine learning methods, expressed across the defining axes of physics and data, is discussed by engaging in a comprehensive exploration of its characteristics, usage, and motivations. In doing so, this paper offers a survey of recent applications and developments of PEML techniques, revealing the potency of PEML in addressing complex challenges. We further demonstrate application of select such schemes on the simple working example of a single-degree-of-freedom Duffing oscillator, which allows to highlight the individual characteristics and motivations of different `genres' of PEML approaches. To promote collaboration and transparency, and to provide practical examples for the reader, the code of these working examples is provided alongside this paper. As a foundational contribution, this paper underscores the significance of PEML in pushing the boundaries of scientific and engineering research, underpinned by the synergy of physical insights and machine learning capabilities.
</details>
<details>
<summary>摘要</summary>
Physics-enhanced machine learning (PEML) 是一种新的思想框架，它将物理学和机器学习两个领域融合在一起，以提高机器学习的能力和降低数据或物理 только方法的缺陷。本文将Physics-enhanced machine learning方法的谱系讲解，从物理和数据两个定义轴上表述其特点、使用和动机。通过这种方式，本文探讨了Recent Applications and Developments of PEML techniques，揭示了PEML在解决复杂问题的能力。此外，我们还使用了一个简单的工作示例——单度自由振荡器，以阐述不同类型的 PEML 方法的特点和动机。为促进合作和透明度，以及为读者提供实践性的示例，我们附加了这些工作示例的代码。作为基础性贡献，本文强调了 PEML 在科学和工程研究中的重要性，它由物理学的理解和机器学习能力的融合所支持。
</details></li>
</ul>
<hr>
<h2 id="DDC-PIM-Efficient-Algorithm-Architecture-Co-design-for-Doubling-Data-Capacity-of-SRAM-based-Processing-In-Memory"><a href="#DDC-PIM-Efficient-Algorithm-Architecture-Co-design-for-Doubling-Data-Capacity-of-SRAM-based-Processing-In-Memory" class="headerlink" title="DDC-PIM: Efficient Algorithm&#x2F;Architecture Co-design for Doubling Data Capacity of SRAM-based Processing-In-Memory"></a>DDC-PIM: Efficient Algorithm&#x2F;Architecture Co-design for Doubling Data Capacity of SRAM-based Processing-In-Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20424">http://arxiv.org/abs/2310.20424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cenlin Duan, Jianlei Yang, Xiaolin He, Yingjie Qi, Yikun Wang, Yiou Wang, Ziyan He, Bonan Yan, Xueyan Wang, Xiaotao Jia, Weitao Pan, Weisheng Zhao</li>
<li>for: 该研究旨在提高处理在内存（PIM）中的性能，特别是减少数据移动的效率。</li>
<li>methods: 该研究提出了一种名为DDC-PIM的效果 doubles the equivalent data capacity的算法&#x2F;架构合作方法，包括对算法水平进行filter-wise complementary correlation（FCC）算法，以及对架构水平进行6T SRAM的利用。</li>
<li>results: 评估结果表明，DDC-PIM在MobileNetV2和EfficientNet-B0上比基eline实现带有$2.84\times$的速度提升，而且与基eline实现的准确率差异不显著。相比之下，与现有的SRAM-based PIM macros进行比较，DDC-PIM在Weight Density和Area Efficiency两个方面具有$8.41\times$和$2.75\times$的提升。<details>
<summary>Abstract</summary>
Processing-in-memory (PIM), as a novel computing paradigm, provides significant performance benefits from the aspect of effective data movement reduction. SRAM-based PIM has been demonstrated as one of the most promising candidates due to its endurance and compatibility. However, the integration density of SRAM-based PIM is much lower than other non-volatile memory-based ones, due to its inherent 6T structure for storing a single bit. Within comparable area constraints, SRAM-based PIM exhibits notably lower capacity. Thus, aiming to unleash its capacity potential, we propose DDC-PIM, an efficient algorithm/architecture co-design methodology that effectively doubles the equivalent data capacity. At the algorithmic level, we propose a filter-wise complementary correlation (FCC) algorithm to obtain a bitwise complementary pair. At the architecture level, we exploit the intrinsic cross-coupled structure of 6T SRAM to store the bitwise complementary pair in their complementary states ($Q/\overline{Q}$), thereby maximizing the data capacity of each SRAM cell. The dual-broadcast input structure and reconfigurable unit support both depthwise and pointwise convolution, adhering to the requirements of various neural networks. Evaluation results show that DDC-PIM yields about $2.84\times$ speedup on MobileNetV2 and $2.69\times$ on EfficientNet-B0 with negligible accuracy loss compared with PIM baseline implementation. Compared with state-of-the-art SRAM-based PIM macros, DDC-PIM achieves up to $8.41\times$ and $2.75\times$ improvement in weight density and area efficiency, respectively.
</details>
<details>
<summary>摘要</summary>
“processing-in-memory”（PIM）作为一种新的计算模式，实现了实际数据运输量的显著性能提升。SRAM基本PIM因为其持续性和可容器性而被视为最有前途的候选者。然而，SRAM基本PIM的集成密度与其他非朋合内存基本PIM的密度相比较低，这是因为它的自然6T结构储存单一比特。在相似的面积限制下，SRAM基本PIM表现出较低的容量。为了解释这个容量的潜力，我们提出了DDC-PIM，一种有效的架构/算法合理设计方法，可以实际地两倍提高等效数据容量。在算法层面，我们提出了一个范例共联相关（FCC）算法，以获取单位比特的对应配对。在架构层面，我们利用6T SRAM的内在交叉连接结构，将单位比特的对应配对储存在它们的补充状态($Q/\overline{Q}$)中，以最大化每个SRAM细胞的数据容量。双向广播输入结构和可重新配置单元支持深度对称和点对称卷积，遵循不同神经网络的需求。评估结果显示，DDC-PIM对于MobileNetV2和EfficientNet-B0的实现中，具有约2.84倍的速度提升，与PIM基eline实现相比，仅受到微scopic的精度损失。相比之下，DDC-PIM与现有的SRAM基本PIMmacro之间，实现了约8.41倍和2.75倍的重量密度和面积效率提升。”
</details></li>
</ul>
<hr>
<h2 id="Coalitional-Manipulations-and-Immunity-of-the-Shapley-Value"><a href="#Coalitional-Manipulations-and-Immunity-of-the-Shapley-Value" class="headerlink" title="Coalitional Manipulations and Immunity of the Shapley Value"></a>Coalitional Manipulations and Immunity of the Shapley Value</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20415">http://arxiv.org/abs/2310.20415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Basteck, Frank Huettner</li>
<li>for: 本文研究了 coalitional game 中的操作，特别是如何确定一个具有最大收益的盟约。</li>
<li>methods: 本文使用了 Shapley 值作为基础，并通过研究 reallocation-proofness 和 weak coalitional monotonicity 两种需求来替代原来的 additivity 需求。</li>
<li>results: 本文显示了一种新的 Shapley 值基础，即只允许 null  игроки获得零收益，并且具有 coalitional manipulation 鲁棒性。此外，我们还发现了一种更弱的 marginality axioms，即 constrained marginality，可以替代 Young 的 marginality axioms。<details>
<summary>Abstract</summary>
We consider manipulations in the context of coalitional games, where a coalition aims to increase the total payoff of its members. An allocation rule is immune to coalitional manipulation if no coalition can benefit from internal reallocation of worth on the level of its subcoalitions (reallocation-proofness), and if no coalition benefits from a lower worth while all else remains the same (weak coalitional monotonicity). Replacing additivity in Shapley's original characterization by these requirements yields a new foundation of the Shapley value, i.e., it is the unique efficient and symmetric allocation rule that awards nothing to a null player and is immune to coalitional manipulations. We further find that for efficient allocation rules, reallocation-proofness is equivalent to constrained marginality, a weaker variant of Young's marginality axiom. Our second characterization improves upon Young's characterization by weakening the independence requirement intrinsic to marginality.
</details>
<details>
<summary>摘要</summary>
我们在合作游戏中考虑操作，合作团体想增加成员的总回扣。一个分配规则是免于合作操作的折衣（reallocation-proofness），如果无法内部实现合作团体的重新分配值（reallocation），并且如果无法在所有 else 保持不变的情况下，则获得较低的值（weak coalitional monotonicity）。将添加性在雪普利原始的特征中替换为这些要求，则得到一个新的基础，即雪普利值是唯一的有效率和对称分配规则，没有空闲玩家获得任何回扣，并且免于合作操作。我们还发现，对于有效的分配规则，折衣证明是对于Young的一致性axioma的弱版本。我们的第二个特征提高了Young的特征，对于独立性的要求。
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-approach-for-solving-the-gravitational-N-body-problem-with-Artificial-Neural-Networks"><a href="#A-hybrid-approach-for-solving-the-gravitational-N-body-problem-with-Artificial-Neural-Networks" class="headerlink" title="A hybrid approach for solving the gravitational N-body problem with Artificial Neural Networks"></a>A hybrid approach for solving the gravitational N-body problem with Artificial Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20398">http://arxiv.org/abs/2310.20398</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/veronicasaz/planetarysystem_hnn">https://github.com/veronicasaz/planetarysystem_hnn</a></li>
<li>paper_authors: Veronica Saz Ulibarrena, Philipp Horn, Simon Portegies Zwart, Elena Sellentin, Barry Koren, Maxwell X. Cai<br>for: 这个论文是用来研究用人工神经网络（ANNs）来缓解数值天体运动 simulation中的计算成本的高速化的。methods: 这个论文使用了汉密尔顿神经网络（HNNs）和深度神经网络（DNNs）来取代计算成本较高的部分。results: 作者发现使用汉密尔顿神经网络可以快速 simulation of planetary systems with a large number of asteroids (&gt;70), but the training process is challenging and the network may fail when the input parameters differ by about 7 orders of magnitude. In contrast, deep neural networks are easy to train but do not conserve energy and lead to fast divergence from the reference solution. The hybrid integrator that combines the neural networks with numerical solutions can increase the reliability of the method without significantly increasing the computing cost.<details>
<summary>Abstract</summary>
Simulating the evolution of the gravitational N-body problem becomes extremely computationally expensive as N increases since the problem complexity scales quadratically with the number of bodies. We study the use of Artificial Neural Networks (ANNs) to replace expensive parts of the integration of planetary systems. Neural networks that include physical knowledge have grown in popularity in the last few years, although few attempts have been made to use them to speed up the simulation of the motion of celestial bodies. We study the advantages and limitations of using Hamiltonian Neural Networks to replace computationally expensive parts of the numerical simulation. We compare the results of the numerical integration of a planetary system with asteroids with those obtained by a Hamiltonian Neural Network and a conventional Deep Neural Network, with special attention to understanding the challenges of this problem. Due to the non-linear nature of the gravitational equations of motion, errors in the integration propagate. To increase the robustness of a method that uses neural networks, we propose a hybrid integrator that evaluates the prediction of the network and replaces it with the numerical solution if considered inaccurate. Hamiltonian Neural Networks can make predictions that resemble the behavior of symplectic integrators but are challenging to train and in our case fail when the inputs differ ~7 orders of magnitude. In contrast, Deep Neural Networks are easy to train but fail to conserve energy, leading to fast divergence from the reference solution. The hybrid integrator designed to include the neural networks increases the reliability of the method and prevents large energy errors without increasing the computing cost significantly. For this problem, the use of neural networks results in faster simulations when the number of asteroids is >70.
</details>
<details>
<summary>摘要</summary>
simulate  gravitational N-body 问题的演化成为 N 增加时速度增加，因为问题复杂性 quadratic 与体数相关。我们研究使用人工神经网络（ANNs）来取代数值integration of planetary systems的费时部分。包含物理知识的神经网络在 послед few years 中 popularity 增加， although few attempts have been made to use them to speed up the simulation of celestial bodies' motion. We study the advantages and limitations of using Hamiltonian Neural Networks to replace computationally expensive parts of the numerical simulation. We compare the results of the numerical integration of a planetary system with asteroids with those obtained by a Hamiltonian Neural Network and a conventional Deep Neural Network, with special attention to understanding the challenges of this problem. Due to the non-linear nature of the gravitational equations of motion, errors in the integration propagate. To increase the robustness of a method that uses neural networks, we propose a hybrid integrator that evaluates the prediction of the network and replaces it with the numerical solution if considered inaccurate. Hamiltonian Neural Networks can make predictions that resemble the behavior of symplectic integrators but are challenging to train and in our case fail when the inputs differ by ~7 orders of magnitude. In contrast, Deep Neural Networks are easy to train but fail to conserve energy, leading to fast divergence from the reference solution. The hybrid integrator designed to include the neural networks increases the reliability of the method and prevents large energy errors without increasing the computing cost significantly. For this problem, the use of neural networks results in faster simulations when the number of asteroids is >70.
</details></li>
</ul>
<hr>
<h2 id="Dropout-Strategy-in-Reinforcement-Learning-Limiting-the-Surrogate-Objective-Variance-in-Policy-Optimization-Methods"><a href="#Dropout-Strategy-in-Reinforcement-Learning-Limiting-the-Surrogate-Objective-Variance-in-Policy-Optimization-Methods" class="headerlink" title="Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods"></a>Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20380">http://arxiv.org/abs/2310.20380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengpeng Xie, Changdong Yu, Weizheng Qiao</li>
<li>for: 提高Policy Optimization Algorithm的稳定性和收敛速度</li>
<li>methods: 使用Dropout技术避免重要样本Objective Variance的过度增长</li>
<li>results: 在Atari 2600环境中比较PPO和D-PPO两种算法的性能表现，D-PPO表现出了明显的性能提升，并有效地限制了重要样本Objective Variance的增长 during training。<details>
<summary>Abstract</summary>
Policy-based reinforcement learning algorithms are widely used in various fields. Among them, mainstream policy optimization algorithms such as TRPO and PPO introduce importance sampling into policy iteration, which allows the reuse of historical data. However, this can also lead to high variance of the surrogate objective and indirectly affects the stability and convergence of the algorithm. In this paper, we first derived an upper bound of the surrogate objective variance, which can grow quadratically with the increase of the surrogate objective. Next, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling. Then, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 environment, results show that D-PPO achieved significant performance improvements compared to PPO, and effectively limited the excessive increase of the surrogate objective variance during training.
</details>
<details>
<summary>摘要</summary>
In this paper, we first derived an upper bound of the surrogate objective variance, which can grow quadratically with the increase of the surrogate objective. Then, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling.Next, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 environment, and the results show that D-PPO achieved significant performance improvements compared to PPO, and effectively limited the excessive increase of the surrogate objective variance during training.
</details></li>
</ul>
<hr>
<h2 id="Stability-and-Generalization-of-the-Decentralized-Stochastic-Gradient-Descent-Ascent-Algorithm"><a href="#Stability-and-Generalization-of-the-Decentralized-Stochastic-Gradient-Descent-Ascent-Algorithm" class="headerlink" title="Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm"></a>Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20369">http://arxiv.org/abs/2310.20369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaoxi Zhu, Li Shen, Bo Du, Dacheng Tao</li>
<li>for: 这篇论文主要关注的是分布式机器学习任务中的最小最大值问题的解决方法，以及该问题在分布式环境中的泛化性。</li>
<li>methods: 这篇论文使用了随机梯度下降升级（D-SGDA）算法，并使用了算法稳定性的方法来研究其泛化性。</li>
<li>results: 研究结果表明，D-SGDA算法在分布式环境中可以保持稳定性和泛化性，并且可以与中央化SGDA算法相比肤。此外，研究还发现了不同网络结构对D-SGDA算法的泛化级别的影响，并且在一定情况下可以通过调整学习率和迭代次数来优化D-SGDA算法的泛化性。<details>
<summary>Abstract</summary>
The growing size of available data has attracted increasing interest in solving minimax problems in a decentralized manner for various machine learning tasks. Previous theoretical research has primarily focused on the convergence rate and communication complexity of decentralized minimax algorithms, with little attention given to their generalization. In this paper, we investigate the primal-dual generalization bound of the decentralized stochastic gradient descent ascent (D-SGDA) algorithm using the approach of algorithmic stability under both convex-concave and nonconvex-nonconcave settings. Our theory refines the algorithmic stability in a decentralized manner and demonstrates that the decentralized structure does not destroy the stability and generalization of D-SGDA, implying that it can generalize as well as the vanilla SGDA in certain situations. Our results analyze the impact of different topologies on the generalization bound of the D-SGDA algorithm beyond trivial factors such as sample sizes, learning rates, and iterations. We also evaluate the optimization error and balance it with the generalization gap to obtain the optimal population risk of D-SGDA in the convex-concave setting. Additionally, we perform several numerical experiments which validate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
随着可用数据的增长，解决分布式的最小最大问题（minimax problem）在不同机器学习任务中吸引了越来越多的关注。先前的理论研究主要关注了分布式最小最大算法的收敛率和通信复杂度，忽略了其总体化。在这篇论文中，我们研究了分布式随机梯度下降（D-SGDA）算法的原理稳定性 bounds，使用了分布式算法稳定性的方法。我们的理论表明，分布式结构不会消除D-SGDA的稳定性和总体化，这意味着它在某些情况下可以与标准的SGDA相当。我们的研究还分析了不同结构对D-SGDA算法的总体化约束的影响，并评估了优化错误和总体化差异来获得D-SGDA算法的最佳人口风险。此外，我们还进行了多个数值实验， validate our theoretical findings.
</details></li>
</ul>
<hr>
<h2 id="Distil-the-informative-essence-of-loop-detector-data-set-Is-network-level-traffic-forecasting-hungry-for-more-data"><a href="#Distil-the-informative-essence-of-loop-detector-data-set-Is-network-level-traffic-forecasting-hungry-for-more-data" class="headerlink" title="Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?"></a>Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20366">http://arxiv.org/abs/2310.20366</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/romainlitud/uncertainty-aware-traffic-speed-flow-demand-prediction">https://github.com/romainlitud/uncertainty-aware-traffic-speed-flow-demand-prediction</a></li>
<li>paper_authors: Guopeng Li, Victor L. Knoop, J. W. C., van Lint</li>
<li>for: 本研究主要是为了解决交通预测中的数据强度问题，以及是否可以通过更多的感知器数据提高预测精度。</li>
<li>methods: 本研究提出了一种不确定性意识涵养框架，结合交通流理论和图神经网络，以及使用证据学来量化不同来源的不确定性。</li>
<li>results: 实验结果显示，在荷兰附近的高速公路网络上，可以从2018年至2021年的日间时段中 removes 超过80%的数据，而剩下的20%的样本具有相同的预测力。这些结果表明，大型交通数据集可以被分解成更小但Equally informative的数据集。<details>
<summary>Abstract</summary>
Network-level traffic condition forecasting has been intensively studied for decades. Although prediction accuracy has been continuously improved with emerging deep learning models and ever-expanding traffic data, traffic forecasting still faces many challenges in practice. These challenges include the robustness of data-driven models, the inherent unpredictability of traffic dynamics, and whether further improvement of traffic forecasting requires more sensor data. In this paper, we focus on this latter question and particularly on data from loop detectors. To answer this, we propose an uncertainty-aware traffic forecasting framework to explore how many samples of loop data are truly effective for training forecasting models. Firstly, the model design combines traffic flow theory with graph neural networks, ensuring the robustness of prediction and uncertainty quantification. Secondly, evidential learning is employed to quantify different sources of uncertainty in a single pass. The estimated uncertainty is used to "distil" the essence of the dataset that sufficiently covers the information content. Results from a case study of a highway network around Amsterdam show that, from 2018 to 2021, more than 80\% of the data during daytime can be removed. The remaining 20\% samples have equal prediction power for training models. This result suggests that indeed large traffic datasets can be subdivided into significantly smaller but equally informative datasets. From these findings, we conclude that the proposed methodology proves valuable in evaluating large traffic datasets' true information content. Further extensions, such as extracting smaller, spatially non-redundant datasets, are possible with this method.
</details>
<details>
<summary>摘要</summary>
网络水平交通条件预测已经在 décadas 中进行了探索。虽然预测精度一直在深度学习模型 emergence 和交通数据的扩展中不断提高，但交通预测在实践中仍然面临许多挑战。这些挑战包括数据驱动模型的稳定性，交通动态的内在难于预测，以及是否进一步改进交通预测需要更多的感知器数据。在这篇论文中，我们关注这个问题，特别是来自循环感知器的数据。为了回答这个问题，我们提出了一个不确定性认识的交通预测框架，以探索 loop 数据多少样本是真正有效的训练预测模型。首先，模型设计结合交通流理论与图 neural network，以确保预测的稳定性和不确定性评估。其次，使用 evidential learning 来评估不同来源的不确定性，并将估计的不确定性用于 "浸泡" 数据集中的信息内容。 results 表明，2018 年至 2021 年日间交通网络在 Amsterdam 附近的 highway 上， más de 80% 的数据可以被删除，而剩下的 20% 样本具有相同的预测力。这种结果表明，大交通数据集可以被分解成更小但Equally informative 的数据集。从这些发现，我们结论认识方法的价值在评估大交通数据集的真实信息内容。可以进一步推广这种方法，例如抽取更小但空间不重复的数据集。
</details></li>
</ul>
<hr>
<h2 id="CAFE-Conflict-Aware-Feature-wise-Explanations"><a href="#CAFE-Conflict-Aware-Feature-wise-Explanations" class="headerlink" title="CAFE: Conflict-Aware Feature-wise Explanations"></a>CAFE: Conflict-Aware Feature-wise Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20363">http://arxiv.org/abs/2310.20363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Dejl, Hamed Ayoobi, Matthew Williams, Francesca Toni</li>
<li>for: 这个论文是为了解释神经网络模型的输出，特别是响应于哪些输入特征。</li>
<li>methods: 这个论文提出了一种新的特征归因方法，称为CAFE（冲突感知特征分解），它解决了现有方法的三大限制：忽视对抗特征的影响、缺乏对偏移项的考虑和地方化活动函数下的过敏感。CAFE方法提供了避免过分估计输入特征的影响的保障，并分别跟踪输入特征和偏移项的积极和消极影响，从而提高了 robustness 和表面特征冲突。</li>
<li>results: 实验表明，CAFE方法在 sintetic 表格数据上能够更好地Identify 冲突特征，并在多个实际表格数据上达到最高的总准确率，而且高效计算。<details>
<summary>Abstract</summary>
Feature attribution methods are widely used to explain neural models by determining the influence of individual input features on the models' outputs. We propose a novel feature attribution method, CAFE (Conflict-Aware Feature-wise Explanations), that addresses three limitations of the existing methods: their disregard for the impact of conflicting features, their lack of consideration for the influence of bias terms, and an overly high sensitivity to local variations in the underpinning activation functions. Unlike other methods, CAFE provides safeguards against overestimating the effects of neuron inputs and separately traces positive and negative influences of input features and biases, resulting in enhanced robustness and increased ability to surface feature conflicts. We show experimentally that CAFE is better able to identify conflicting features on synthetic tabular data and exhibits the best overall fidelity on several real-world tabular datasets, while being highly computationally efficient.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Feature attribution methods are widely used to explain neural models by determining the influence of individual input features on the models' outputs. We propose a novel feature attribution method, CAFE (Conflict-Aware Feature-wise Explanations), that addresses three limitations of the existing methods: their disregard for the impact of conflicting features, their lack of consideration for the influence of bias terms, and an overly high sensitivity to local variations in the underpinning activation functions. Unlike other methods, CAFE provides safeguards against overestimating the effects of neuron inputs and separately traces positive and negative influences of input features and biases, resulting in enhanced robustness and increased ability to surface feature conflicts. We show experimentally that CAFE is better able to identify conflicting features on synthetic tabular data and exhibits the best overall fidelity on several real-world tabular datasets, while being highly computationally efficient."翻译结果：Feature 归因方法广泛用于解释神经网络模型的输出，以确定输入特征对模型的影响。我们提出了一种新的Feature归因方法，即Conflict-Aware Feature-wise Explanations（CAFE），该方法解决了现有方法的三个限制：它们忽略了冲突性特征的影响、缺乏对偏好项的考虑、以及对下游活动函数的过敏感性。与其他方法不同，CAFE提供了防止过度估计输入神经元的影响的安全措施，并分别跟踪输入特征和偏好项的正向和负向影响，从而提高了Robustness和surfacefeature冲突的能力。我们通过实验表明，CAFE在人工制造的表格数据上更好地标识冲突特征，并在多个实际的表格数据上显示出最好的总体准确性，同时高度计算效率。
</details></li>
</ul>
<hr>
<h2 id="Verification-of-Neural-Networks-Local-Differential-Classification-Privacy"><a href="#Verification-of-Neural-Networks-Local-Differential-Classification-Privacy" class="headerlink" title="Verification of Neural Networks Local Differential Classification Privacy"></a>Verification of Neural Networks Local Differential Classification Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20299">http://arxiv.org/abs/2310.20299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roie Reshef, Anan Kabaha, Olga Seleznova, Dana Drachsler-Cohen</li>
<li>for: 防止神经网络泄露个人隐私</li>
<li>methods: 使用Local Differential Classification Privacy（LDCP）和Kernel Density estimation（KDE）将网络参数转换为分布，并使用MILP验证器验证抽象网络的隐私性</li>
<li>results: 通过训练7%的网络，Sphynx可以预测一个抽象网络，并在93%的验证精度下验证LDCP，同时提高验证时间 $1.7\cdot10^4$倍。<details>
<summary>Abstract</summary>
Neural networks are susceptible to privacy attacks. To date, no verifier can reason about the privacy of individuals participating in the training set. We propose a new privacy property, called local differential classification privacy (LDCP), extending local robustness to a differential privacy setting suitable for black-box classifiers. Given a neighborhood of inputs, a classifier is LDCP if it classifies all inputs the same regardless of whether it is trained with the full dataset or whether any single entry is omitted. A naive algorithm is highly impractical because it involves training a very large number of networks and verifying local robustness of the given neighborhood separately for every network. We propose Sphynx, an algorithm that computes an abstraction of all networks, with a high probability, from a small set of networks, and verifies LDCP directly on the abstract network. The challenge is twofold: network parameters do not adhere to a known distribution probability, making it difficult to predict an abstraction, and predicting too large abstraction harms the verification. Our key idea is to transform the parameters into a distribution given by KDE, allowing to keep the over-approximation error small. To verify LDCP, we extend a MILP verifier to analyze an abstract network. Experimental results show that by training only 7% of the networks, Sphynx predicts an abstract network obtaining 93% verification accuracy and reducing the analysis time by $1.7\cdot10^4$x.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose Sphynx, an algorithm that computes an abstraction of all networks from a small set of networks and verifies LDCP directly on the abstract network. The key idea is to transform the parameters into a distribution using KDE, allowing for a small over-approximation error. To verify LDCP, we extend a MILP verifier to analyze the abstract network.Our experimental results show that by training only 7% of the networks, Sphynx can predict an abstract network with 93% verification accuracy and reduce the analysis time by $1.7\cdot10^4$x.
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Generalized-Linear-Models-by-Trading-off-Computation-for-Uncertainty"><a href="#Accelerating-Generalized-Linear-Models-by-Trading-off-Computation-for-Uncertainty" class="headerlink" title="Accelerating Generalized Linear Models by Trading off Computation for Uncertainty"></a>Accelerating Generalized Linear Models by Trading off Computation for Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20285">http://arxiv.org/abs/2310.20285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Tatzel, Jonathan Wenger, Frank Schneider, Philipp Hennig</li>
<li>for: 这个论文的目的是为了提出一种能够高效地进行 Bayesian Generalized Linear Models（GLMs）的推理，以及能够考虑模型approximation error的iterative方法。</li>
<li>methods: 这种方法使用了一种家族的iterative方法，这些方法能够高效地利用现代并行计算硬件，并且可以很好地重用计算结果，从而降低GLMs的时间和内存需求。</li>
<li>results: 作者在一个实际上有些很大的分类问题上进行了实验，结果显示，使用这种方法可以快速加速GLMs的训练，并且可以明确地交换减少计算量和增加uncertainty。<details>
<summary>Abstract</summary>
Bayesian Generalized Linear Models (GLMs) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. However, exact inference in GLMs is prohibitively expensive for large datasets, thus requiring approximations in practice. The resulting approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. In this work, we introduce a family of iterative methods that explicitly model this error. They are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for GLMs. As we demonstrate on a realistically large classification problem, our method significantly accelerates training by explicitly trading off reduced computation for increased uncertainty.
</details>
<details>
<summary>摘要</summary>
bayesian generalized linear models (GLMs) 定义了一个 flexible 的概率框架，用于模型 categorical、ordinal 和连续变数，广泛应用在实践中。然而，对大量数据进行精确推断是不可能昂贵的，因此需要使用近似方法。这个近似Error会对模型的可靠性产生负面的影响，并且不会考虑到预测中的不确定性。在这个工作中，我们介绍了一家 Iterative 方法，可以明确地模型这个错误。这些方法具有平行于现代计算硬件的特点，可以高效地重复计算，将信息压缩以减少 GLMs 的时间和内存需求。我们在一个实际上是一个大型分类问题中显示了，我们的方法可以快速对 GLMs 进行训练，明确地交换了reduced computation 和增加的不确定性。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Bayesian-Optimization-via-Learning-Correlated-Latent-Space"><a href="#Advancing-Bayesian-Optimization-via-Learning-Correlated-Latent-Space" class="headerlink" title="Advancing Bayesian Optimization via Learning Correlated Latent Space"></a>Advancing Bayesian Optimization via Learning Correlated Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20258">http://arxiv.org/abs/2310.20258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghun Lee, Jaewon Chu, Sihyeon Kim, Juyeon Ko, Hyunwoo J. Kim</li>
<li>for: 优化黑盒函数（black-box functions），尤其是具有结构或数据维度的 discrete data 中的函数。</li>
<li>methods: 使用深度生成模型（deep generative models），如变量自动编码器（variational autoencoders），进行 Bayesian 优化，并通过学习相关的幂时空间（latent space）来减少优化不足的问题。</li>
<li>results: 在 discrete data 中的多个优化任务（如分子设计和数学表达适应）中，实现高效性和高性能，并在小费用（budget）下达到优秀的结果。<details>
<summary>Abstract</summary>
Bayesian optimization is a powerful method for optimizing black-box functions with limited function evaluations. Recent works have shown that optimization in a latent space through deep generative models such as variational autoencoders leads to effective and efficient Bayesian optimization for structured or discrete data. However, as the optimization does not take place in the input space, it leads to an inherent gap that results in potentially suboptimal solutions. To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in discrete data, such as molecule design and arithmetic expression fitting, and achieve high performance within a small budget.
</details>
<details>
<summary>摘要</summary>
bayesian 优化是一种强大的方法，用于优化黑盒函数，限制 функion evaluations。 recent works 表明，通过在深度生成模型，如变量自动编码器，进行 latent space 优化，可以获得有效和高效的 bayesian 优化。 however，由于优化不在输入空间进行，这会导致一定的差距，可能导致不优化的解。 To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in discrete data, such as molecule design and arithmetic expression fitting, and achieve high performance within a small budget.
</details></li>
</ul>
<hr>
<h2 id="STDA-Meta-A-Meta-Learning-Framework-for-Few-Shot-Traffic-Prediction"><a href="#STDA-Meta-A-Meta-Learning-Framework-for-Few-Shot-Traffic-Prediction" class="headerlink" title="STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction"></a>STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20223">http://arxiv.org/abs/2310.20223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maoxiang Sun, Weilong Ding, Tianpu Zhang, Zijian Liu, Mengda Xing<br>for: 这 paper written for 解决城市发展中的交通堵塞问题，提出了一种基于几何结构的时空预测学习方法，该方法可以在数据稀缺的情况下提高交通预测性能。methods: 该 paper 使用了模型无关元学习（MAML）和 episodic learning 等方法，通过在数据充足城市的数据上学习时空预测模型，然后将学到的知识转移到数据稀缺城市中进行预测。results:  compared to baseline models, 该 paper 的预测性能提高了7%， measured by two metrics of MAE and RMSE。<details>
<summary>Abstract</summary>
As the development of cities, traffic congestion becomes an increasingly pressing issue, and traffic prediction is a classic method to relieve that issue. Traffic prediction is one specific application of spatio-temporal prediction learning, like taxi scheduling, weather prediction, and ship trajectory prediction. Against these problems, classical spatio-temporal prediction learning methods including deep learning, require large amounts of training data. In reality, some newly developed cities with insufficient sensors would not hold that assumption, and the data scarcity makes predictive performance worse. In such situation, the learning method on insufficient data is known as few-shot learning (FSL), and the FSL of traffic prediction remains challenges. On the one hand, graph structures' irregularity and dynamic nature of graphs cannot hold the performance of spatio-temporal learning method. On the other hand, conventional domain adaptation methods cannot work well on insufficient training data, when transferring knowledge from different domains to the intended target domain.To address these challenges, we propose a novel spatio-temporal domain adaptation (STDA) method that learns transferable spatio-temporal meta-knowledge from data-sufficient cities in an adversarial manner. This learned meta-knowledge can improve the prediction performance of data-scarce cities. Specifically, we train the STDA model using a Model-Agnostic Meta-Learning (MAML) based episode learning process, which is a model-agnostic meta-learning framework that enables the model to solve new learning tasks using only a small number of training samples. We conduct numerous experiments on four traffic prediction datasets, and our results show that the prediction performance of our model has improved by 7\% compared to baseline models on the two metrics of MAE and RMSE.
</details>
<details>
<summary>摘要</summary>
On one hand, graph structures' irregularity and the dynamic nature of graphs make it difficult for spatio-temporal learning methods to perform well. On the other hand, conventional domain adaptation methods are not effective in transferring knowledge from different domains to the intended target domain when working with insufficient training data.To address these challenges, we propose a novel spatio-temporal domain adaptation (STDA) method that learns transferable spatio-temporal meta-knowledge from data-sufficient cities in an adversarial manner. This learned meta-knowledge can improve the prediction performance of data-scarce cities. Specifically, we use a Model-Agnostic Meta-Learning (MAML) based episode learning process to train the STDA model, which is a model-agnostic meta-learning framework that enables the model to solve new learning tasks using only a small number of training samples.We conduct numerous experiments on four traffic prediction datasets, and our results show that the prediction performance of our model has improved by 7% compared to baseline models on the two metrics of Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).
</details></li>
</ul>
<hr>
<h2 id="Calibration-by-Distribution-Matching-Trainable-Kernel-Calibration-Metrics"><a href="#Calibration-by-Distribution-Matching-Trainable-Kernel-Calibration-Metrics" class="headerlink" title="Calibration by Distribution Matching: Trainable Kernel Calibration Metrics"></a>Calibration by Distribution Matching: Trainable Kernel Calibration Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20211">http://arxiv.org/abs/2310.20211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kernel-calibration/kernel-calibration">https://github.com/kernel-calibration/kernel-calibration</a></li>
<li>paper_authors: Charles Marx, Sofian Zalouk, Stefano Ermon</li>
<li>for: 这 paper 是为了提高预测uncertainty的捕捉，并且提供了一些可以与实际频率进行匹配的抽象方法。</li>
<li>methods: 这 paper 使用了一种基于核函数的匹配方法，这种方法可以总结和普适化现有的各种calibration方法，并且具有可导的样本估计，可以轻松地在empirical risk minimization中添加calibration目标。</li>
<li>results: 这 paper 的实验表明，通过使用这种匹配方法作为regulator，可以提高预测的准确性、锐度和决策效果，并且超过了只使用后期重新准确的方法。<details>
<summary>Abstract</summary>
Calibration ensures that probabilistic forecasts meaningfully capture uncertainty by requiring that predicted probabilities align with empirical frequencies. However, many existing calibration methods are specialized for post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression. These metrics admit differentiable sample estimates, making it easy to incorporate a calibration objective into empirical risk minimization. Furthermore, we provide intuitive mechanisms to tailor calibration metrics to a decision task, and enforce accurate loss estimation and no regret decisions. Our empirical evaluation demonstrates that employing these metrics as regularizers enhances calibration, sharpness, and decision-making across a range of regression and classification tasks, outperforming methods relying solely on post-hoc recalibration.
</details>
<details>
<summary>摘要</summary>
<<SYS>>系数调整确保 probabilistic 预测能够有效地捕捉不确定性，因为它们需要预测的概率与实际频率相匹配。然而，许多现有的准备方法都是专门为后期重新准备的，这可能会使预测变得更加锐利。基于 Distribution 匹配的思想，我们引入 kernel-based 准备指标，这些指标可以统一和普适化 Popular 的准备方法，并且具有可微的样本估计，因此可以轻松地在 empirical risk minimization 中添加准备目标。此外，我们还提供了直观的机制来适应决策任务，并且强制实现准确的损失估计和无悬决策。我们的实验证明，使用这些指标作为正则izers可以提高准备、锐利性和决策的水平，超过只靠后期重新准备的方法。Translation notes:* "probabilistic forecasts" is translated as "probabilistic 预测" (probabilistic predictions)* "calibration" is translated as "系数调整" (calibration)* "empirical frequencies" is translated as "实际频率" (empirical frequencies)* "post-hoc recalibration" is translated as "后期重新准备" (post-hoc recalibration)* "kernel-based calibration metrics" is translated as "kernel-based 准备指标" (kernel-based calibration metrics)* "differentiable sample estimates" is translated as "可微的样本估计" (differentiable sample estimates)* "empirical risk minimization" is translated as "empirical risk minimization" (empirical risk minimization)* "decision task" is translated as "决策任务" (decision task)
</details></li>
</ul>
<hr>
<h2 id="Network-Contention-Aware-Cluster-Scheduling-with-Reinforcement-Learning"><a href="#Network-Contention-Aware-Cluster-Scheduling-with-Reinforcement-Learning" class="headerlink" title="Network Contention-Aware Cluster Scheduling with Reinforcement Learning"></a>Network Contention-Aware Cluster Scheduling with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20209">http://arxiv.org/abs/2310.20209</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gajagajago/deepshare">https://github.com/gajagajago/deepshare</a></li>
<li>paper_authors: Junyeol Ryu, Jeongyoon Eo</li>
<li>for: 这篇论文是为了解决对GPU集群的分布式训练中的网络竞争问题。</li>
<li>methods: 这篇论文使用了强化学习来解决网络竞争问题，具体来说是将GPU集群的调度问题转化为强化学习问题，并通过不断评估和改进来学习一个网络竞争意识的调度策略。</li>
<li>results: 相比于常用的调度策略，这篇论文的方法可以降低均值任务完成时间18.2%，并将尾部任务完成时间降低20.7%，同时允许可接受的资源利用率和任务完成时间之间的变化。<details>
<summary>Abstract</summary>
With continuous advances in deep learning, distributed training is becoming common in GPU clusters. Specifically, for emerging workloads with diverse amounts, ratios, and patterns of communication, we observe that network contention can significantly degrade training throughput. However, widely used scheduling policies often face limitations as they are agnostic to network contention between jobs. In this paper, we present a new approach to mitigate network contention in GPU clusters using reinforcement learning. We formulate GPU cluster scheduling as a reinforcement learning problem and opt to learn a network contention-aware scheduling policy that efficiently captures contention sensitivities and dynamically adapts scheduling decisions through continuous evaluation and improvement. We show that compared to widely used scheduling policies, our approach reduces average job completion time by up to 18.2\% and effectively cuts the tail job completion time by up to 20.7\% while allowing a preferable trade-off between average job completion time and resource utilization.
</details>
<details>
<summary>摘要</summary>
随着深度学习的不断发展，分布式训练在GPU集群中变得越来越普遍。特别是对于出现在各种各样的Amount、比例和模式的通信而言，网络竞争可能会对训练速率产生很大的降低影响。然而，广泛使用的调度策略经常遇到限制，因为它们对GPU集群内网络竞争无法提供相应的考虑。在这篇论文中，我们提出了一种使用反射学习来减少GPU集群内网络竞争的新方法。我们将GPU集群调度问题定义为反射学习问题，并选择了学习一种网络竞争意识的调度策略，以efficiently捕捉竞争敏感度并在继续评估和改进的基础上进行动态调整。我们显示，与广泛使用的调度策略相比，我们的方法可以降低平均任务完成时间量达18.2%，并同时减少尾部任务完成时间量达20.7%，而且允许在平均任务完成时间和资源利用之间进行可接受的变化。
</details></li>
</ul>
<hr>
<h2 id="Importance-Estimation-with-Random-Gradient-for-Neural-Network-Pruning"><a href="#Importance-Estimation-with-Random-Gradient-for-Neural-Network-Pruning" class="headerlink" title="Importance Estimation with Random Gradient for Neural Network Pruning"></a>Importance Estimation with Random Gradient for Neural Network Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20203">http://arxiv.org/abs/2310.20203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suman Sapkota, Binod Bhattarai</li>
<li>for: 提高神经网络的效率，使其更加高效。</li>
<li>methods: 使用征识法和梯度信息来估计每个神经元或卷积核的全局重要性，不需大量标注数据。</li>
<li>results: 与先前方法比较，我们的方法在ResNet和VGG架构上的CIFAR-100和STL-10 datasets上表现更好，并且可以补做先前方法的缺陷。<details>
<summary>Abstract</summary>
Global Neuron Importance Estimation is used to prune neural networks for efficiency reasons. To determine the global importance of each neuron or convolutional kernel, most of the existing methods either use activation or gradient information or both, which demands abundant labelled examples. In this work, we use heuristics to derive importance estimation similar to Taylor First Order (TaylorFO) approximation based methods. We name our methods TaylorFO-abs and TaylorFO-sq. We propose two additional methods to improve these importance estimation methods. Firstly, we propagate random gradients from the last layer of a network, thus avoiding the need for labelled examples. Secondly, we normalize the gradient magnitude of the last layer output before propagating, which allows all examples to contribute similarly to the importance score. Our methods with additional techniques perform better than previous methods when tested on ResNet and VGG architectures on CIFAR-100 and STL-10 datasets. Furthermore, our method also complements the existing methods and improves their performances when combined with them.
</details>
<details>
<summary>摘要</summary>
全球神经元重要性估计是用于提高神经网络效率的方法。以便确定每个神经元或卷积器的全球重要性，大多数现有方法使用活动或梯度信息，或者两者都使用，需要备受标注的示例。在这项工作中，我们使用规则来 derivimportance estimation similar to Taylor First Order (TaylorFO) approximation-based methods。我们称我们的方法为 TaylorFO-abs 和 TaylorFO-sq。我们还提出了两种改进这些重要性估计方法的方法。首先，我们从网络的最后层传递随机梯度，因此不需要标注示例。其次，我们在最后层输出之前对梯度幅度进行归一化，这样所有的示例都可以同样地贡献到重要性分数中。我们的方法，加上其他技术，在测试于 ResNet 和 VGG 架构上的 CIFAR-100 和 STL-10 数据集上表现出色。此外，我们的方法还可以与现有方法相结合，提高其表现。
</details></li>
</ul>
<hr>
<h2 id="FedRec-Enhancing-Privacy-and-Addressing-Heterogeneity-in-Federated-Recommendation-Systems"><a href="#FedRec-Enhancing-Privacy-and-Addressing-Heterogeneity-in-Federated-Recommendation-Systems" class="headerlink" title="FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems"></a>FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20193">http://arxiv.org/abs/2310.20193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Wang, Zhichao Wang, Xi Leng, Xiaoying Tang</li>
<li>for: 保护用户隐私和减少边缘用户的通信成本，以提高推荐系统的性能。</li>
<li>methods:  FedRec+ 使用优化subset选择基于特征相似性，生成 Pseudo 项的近似优质评分，只使用用户本地信息，从而减少噪音而无需额外的通信成本。 更之前，我们利用 Wasserstein 距离来估计客户端间的差异和贡献，并解决一个定义的优化问题，以 derivation 优化的汇集权重。</li>
<li>results: 实验结果表明 FedRec+ 在各种参考数据集上具有状态艺术性的表现。<details>
<summary>Abstract</summary>
Preserving privacy and reducing communication costs for edge users pose significant challenges in recommendation systems. Although federated learning has proven effective in protecting privacy by avoiding data exchange between clients and servers, it has been shown that the server can infer user ratings based on updated non-zero gradients obtained from two consecutive rounds of user-uploaded gradients. Moreover, federated recommendation systems (FRS) face the challenge of heterogeneity, leading to decreased recommendation performance. In this paper, we propose FedRec+, an ensemble framework for FRS that enhances privacy while addressing the heterogeneity challenge. FedRec+ employs optimal subset selection based on feature similarity to generate near-optimal virtual ratings for pseudo items, utilizing only the user's local information. This approach reduces noise without incurring additional communication costs. Furthermore, we utilize the Wasserstein distance to estimate the heterogeneity and contribution of each client, and derive optimal aggregation weights by solving a defined optimization problem. Experimental results demonstrate the state-of-the-art performance of FedRec+ across various reference datasets.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:保持隐私和降低边缘用户的通信成本是推荐系统的主要挑战。虽然联邦学习已经证明可以保护隐私，但是服务器可以通过两次连续的用户上传梯度来推断用户评分。此外，联邦推荐系统（FRS）面临着多样性挑战，导致推荐性能下降。在本文中，我们提出了FedRec+，一种ensemble框架 для FRS，可以增强隐私并 Addressing the Heterogeneity Challenge。FedRec+使用了优化subset选择基于特征相似性，生成 Pseudo  Item 的近似优质评分，只使用用户的本地信息。这种方法可以减少噪音而不导致额外的通信成本。此外，我们利用 Wasserstein 距离来估计每个客户端的多样性和贡献，并解决一个定义的优化问题来 derive 最佳汇总 веctor。实验结果表明 FedRec+ 在多个参考数据集上达到了顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="Compact-Binary-Systems-Waveform-Generation-with-Generative-Pre-trained-Transformer"><a href="#Compact-Binary-Systems-Waveform-Generation-with-Generative-Pre-trained-Transformer" class="headerlink" title="Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer"></a>Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20172">http://arxiv.org/abs/2310.20172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijun Shi, Yue Zhou, Tianyu Zhao, Zhoujian Cao, Zhixiang Ren</li>
<li>for: 这个研究是为了解决太空 gravitational wave 探测中的数据处理问题，特别是在次代时阻对称探测器（TDI 2.0）中增加的波形复杂性导致的数据处理问题。</li>
<li>methods: 这个研究使用了一个可解释的大型预训模型 named CBS-GPT（Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer）来预测太空 gravitational wave 波形。三个模型被训练来预测黑洞Binary（MBHB）、巨大质量比例探测（EMRIs）和星系Binary（GB）波形，实现预测精度分别为98%、91%和99%。</li>
<li>results:  CBS-GPT 模型具有良好的可解释性，其隐藏参数能够有效地捕捉波形中的复杂信息，包括探测器的回应和广泛的参数范围。这项研究显示了大型预训模型在 gravitational wave 数据处理中的应用潜力，开启了未来的任务，如补充差、GW 信号探测和信号干扰reduction。<details>
<summary>Abstract</summary>
Space-based gravitational wave detection is one of the most anticipated gravitational wave (GW) detection projects in the next decade, which will detect abundant compact binary systems. However, the precise prediction of space GW waveforms remains unexplored. To solve the data processing difficulty in the increasing waveform complexity caused by detectors' response and second-generation time-delay interferometry (TDI 2.0), an interpretable pre-trained large model named CBS-GPT (Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer) is proposed. For compact binary system waveforms, three models were trained to predict the waveforms of massive black hole binary (MBHB), extreme mass-ratio inspirals (EMRIs), and galactic binary (GB), achieving prediction accuracies of 98%, 91%, and 99%, respectively. The CBS-GPT model exhibits notable interpretability, with its hidden parameters effectively capturing the intricate information of waveforms, even with complex instrument response and a wide parameter range. Our research demonstrates the potential of large pre-trained models in gravitational wave data processing, opening up new opportunities for future tasks such as gap completion, GW signal detection, and signal noise reduction.
</details>
<details>
<summary>摘要</summary>
空间基于gravitational wave探测是下一代 gravitational wave (GW) 探测项目中最受期待的一项，可探测丰富的紧凑Binary系统。然而，准确预测空间GW波形仍未经过研究。为解决探测器响应和第二代时间延迟相干（TDI 2.0）所导致的数据处理困难，一种可解释的大型预训练模型被提出，称为CBS-GPT（Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer）。对紧凑Binary系统波形进行预测，CBS-GPT模型在大质量黑洞 binary（MBHB）、Extreme Mass-Ratio Inspirals（EMRIs）和galactic Binary（GB）三种情况中实现了预测精度为98%、91%和99%。CBS-GPT模型表现出了明显的可解释性，其隐藏参数能够有效捕捉波形中的复杂信息，即使是通过复杂的探测器响应和广泛的参数范围。我们的研究表明了大型预训练模型在gravitational wave数据处理中的潜在优势，开启了未来任务 such as gap completion、GW signal detection和signal noise reduction的新可能性。
</details></li>
</ul>
<hr>
<h2 id="Understanding-and-Visualizing-Droplet-Distributions-in-Simulations-of-Shallow-Clouds"><a href="#Understanding-and-Visualizing-Droplet-Distributions-in-Simulations-of-Shallow-Clouds" class="headerlink" title="Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds"></a>Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20168">http://arxiv.org/abs/2310.20168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justus C. Will, Andrea M. Jenney, Kara D. Lamb, Michael S. Pritchard, Colleen Kaul, Po-Lun Ma, Kyle Pressel, Jacob Shpund, Marcus van Lier-Walqui, Stephan Mandt</li>
<li>for: 本研究旨在更好地理解云中微物理过程，以及它们对全球气候的影响。</li>
<li>methods: 本研究使用了大气动力学LES的精度高的颗粒大小分布来挑战当前分析技术。</li>
<li>results: 研究人员通过使用变量自动编码器（VAEs）生成了新的和直观的视觉化图表，以描述颗粒大小的分布和时间演化。这些图表超出了 clustering 技术的能力，并允许我们对受到不同气尘浓度影响的 simulations 进行比较。我们发现颗粒谱的演化是不同气尘含量水平下的相似的，但发生在不同的速度上。这种相似性表明降水 initiation 过程是相同的，尽管启动时间有所不同。<details>
<summary>Abstract</summary>
Thorough analysis of local droplet-level interactions is crucial to better understand the microphysical processes in clouds and their effect on the global climate. High-accuracy simulations of relevant droplet size distributions from Large Eddy Simulations (LES) of bin microphysics challenge current analysis techniques due to their high dimensionality involving three spatial dimensions, time, and a continuous range of droplet sizes. Utilizing the compact latent representations from Variational Autoencoders (VAEs), we produce novel and intuitive visualizations for the organization of droplet sizes and their evolution over time beyond what is possible with clustering techniques. This greatly improves interpretation and allows us to examine aerosol-cloud interactions by contrasting simulations with different aerosol concentrations. We find that the evolution of the droplet spectrum is similar across aerosol levels but occurs at different paces. This similarity suggests that precipitation initiation processes are alike despite variations in onset times.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:全面分析当地液滴水平交互是 clouds 的微物理过程理解的关键，以及全球气候的影响。 Large Eddy Simulations (LES) of bin microphysics 的高精度仿真中的液滴大小分布挑战当前分析技术，因为它们具有三维空间、时间和液滴大小的维度。通过 Variational Autoencoders (VAEs) 的压缩隐藏表示，我们生成了不同于 clustering 技术的新和直观的视觉化，以便更好地理解液滴大小的组织和时间演变。这有助于解释，并允许我们通过不同气尘含量的仿真比较，探讨云尘相互作用。我们发现，不同气尘含量下的液滴spectrum 的演变类似，但发生在不同的速度。这种类似性表明，降水 initiation 过程是相似的，即使启动时间有所不同。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Robust-Bayesian-Optimization-for-Arbitrary-Uncertain-inputs"><a href="#Efficient-Robust-Bayesian-Optimization-for-Arbitrary-Uncertain-inputs" class="headerlink" title="Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs"></a>Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20145">http://arxiv.org/abs/2310.20145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yang, Junlong Lyu, Wenlong Lyu, Zhitang Chen</li>
<li>for: 这篇论文是为了解决在 bayesian optimization 中的 input uncertainty 问题，实现 robust 的最佳化。</li>
<li>methods: 本 paper 使用了一个新的 robust bayesian optimization 算法 AIRBO，Directly 模型了不确定的输入，使用 Gaussian Process 和 Maximum Mean Discrepancy (MMD) 来实现。</li>
<li>results: 实验结果显示，AIRBO 可以处理不同的 input uncertainty，并且可以实现 state-of-the-art 的性能。<details>
<summary>Abstract</summary>
Bayesian Optimization (BO) is a sample-efficient optimization algorithm widely employed across various applications. In some challenging BO tasks, input uncertainty arises due to the inevitable randomness in the optimization process, such as machining errors, execution noise, or contextual variability. This uncertainty deviates the input from the intended value before evaluation, resulting in significant performance fluctuations in the final result. In this paper, we introduce a novel robust Bayesian Optimization algorithm, AIRBO, which can effectively identify a robust optimum that performs consistently well under arbitrary input uncertainty. Our method directly models the uncertain inputs of arbitrary distributions by empowering the Gaussian Process with the Maximum Mean Discrepancy (MMD) and further accelerates the posterior inference via Nystrom approximation. Rigorous theoretical regret bound is established under MMD estimation error and extensive experiments on synthetic functions and real problems demonstrate that our approach can handle various input uncertainties and achieve state-of-the-art performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Sample-Conditioned-Hypothesis-Stability-Sharpens-Information-Theoretic-Generalization-Bounds"><a href="#Sample-Conditioned-Hypothesis-Stability-Sharpens-Information-Theoretic-Generalization-Bounds" class="headerlink" title="Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds"></a>Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20102">http://arxiv.org/abs/2310.20102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqiao Wang, Yongyi Mao</li>
<li>for: 提供新的信息理论性泛化保证，通过一种新的“邻域假设”矩阵和一家新的样本conditioned假设稳定性（SCH稳定性）家族。</li>
<li>methods: 使用新的信息理论性泛化保证方法，包括“邻域假设”矩阵的新建构和样本conditioned假设稳定性。</li>
<li>results: 提供更加精细的信息理论性泛化保证，可以在不同的学习场景中提高先前的信息理论性泛化保证。特别是，这些保证可以 addresses existential learning scenario中的限制，如Haghifam et al. (2023)所explore的stoochastic convex optimization（SCO）问题。<details>
<summary>Abstract</summary>
We present new information-theoretic generalization guarantees through the a novel construction of the "neighboring-hypothesis" matrix and a new family of stability notions termed sample-conditioned hypothesis (SCH) stability. Our approach yields sharper bounds that improve upon previous information-theoretic bounds in various learning scenarios. Notably, these bounds address the limitations of existing information-theoretic bounds in the context of stochastic convex optimization (SCO) problems, as explored in the recent work by Haghifam et al. (2023).
</details>
<details>
<summary>摘要</summary>
我们提出了一新的信息理论性的扩张保证，通过一个新的“邻域假设”矩阵的建构和一新的家族叫做“样本调整假设”（SCH）稳定性。我们的方法可以获得更加锐利的界限，超越了过去的信息理论性 bound 在不同的学习场景中。特别是，这些 bound 可以解决现有信息理论性 bound 在数学测度估计（SCO）问题中的限制，就如果aghifam et al. (2023) 所研究的情况。
</details></li>
</ul>
<hr>
<h2 id="Robust-Learning-for-Smoothed-Online-Convex-Optimization-with-Feedback-Delay"><a href="#Robust-Learning-for-Smoothed-Online-Convex-Optimization-with-Feedback-Delay" class="headerlink" title="Robust Learning for Smoothed Online Convex Optimization with Feedback Delay"></a>Robust Learning for Smoothed Online Convex Optimization with Feedback Delay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20098">http://arxiv.org/abs/2310.20098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Li, Jianyi Yang, Adam Wierman, Shaolei Ren</li>
<li>for: 这个论文旨在解决智能型online减少准则（SOCO）中的多步非线性切换成本和反馈延迟问题。</li>
<li>methods: 这篇论文提出了一种基于机器学习（ML）的增强 online算法，即Robustness-Constrained Learning（RCL），该算法将不可信ML预测与可信专家线上算法相结合，通过受限制的投影来使ML预测更加稳定。</li>
<li>results: 论文证明了RCL能够保证（1+λ）-竞争力 against any given expert for any λ&gt;0，同时也可以在平均情况下提高ML预测的性能。此外，RCL是SOCO中多步非线性切换成本和反馈延迟的首个具有证明的可靠性保证的ML增强算法。<details>
<summary>Abstract</summary>
We study a challenging form of Smoothed Online Convex Optimization, a.k.a. SOCO, including multi-step nonlinear switching costs and feedback delay. We propose a novel machine learning (ML) augmented online algorithm, Robustness-Constrained Learning (RCL), which combines untrusted ML predictions with a trusted expert online algorithm via constrained projection to robustify the ML prediction. Specifically,we prove that RCL is able to guarantee$(1+\lambda)$-competitiveness against any given expert for any$\lambda>0$, while also explicitly training the ML model in a robustification-aware manner to improve the average-case performance. Importantly,RCL is the first ML-augmented algorithm with a provable robustness guarantee in the case of multi-step switching cost and feedback delay.We demonstrate the improvement of RCL in both robustness and average performance using battery management for electrifying transportationas a case study.
</details>
<details>
<summary>摘要</summary>
我们研究一种具有多步非线性调整成本和反馈延迟的简润线上凸优化，简称SOCO，以及一个新的机器学习（ML）增强的线上算法，即对称确定学习（RCL）。 RCL 结合了不可信ML预测和可信专家线上算法via受限投影，以确保ML预测的稳定性。我们证明了RCL 能够保证$(1+\lambda)$-竞争性比任何给定专家，适用于任何 $\lambda>0$。此外，RCL 还会在对ML预测进行了强化训练的情况下，提高均值性能。在用电动车电池管理为应用的 caso study 中，我们显示了RCL 在类型和平均性能之间的改善。Here's the translation in Traditional Chinese:我们研究一种具有多步非线性调整成本和反馈延迟的简润线上凸优化，简称SOCO，以及一个新的机器学习（ML）增强的线上算法，即对称确定学习（RCL）。 RCL 结合了不可信ML预测和可信专家线上算法via受限投影，以确保ML预测的稳定性。我们证明了RCL 能够保证$(1+\lambda)$-竞争性比任何给定专家，适用于任何 $\lambda>0$。此外，RCL 还会在对ML预测进行了强化训练的情况下，提高均值性能。在用电动车电池管理为应用的 caso study 中，我们显示了RCL 在类型和平均性能之间的改善。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Between-Variational-Inference-and-Wasserstein-Gradient-Flows"><a href="#Bridging-the-Gap-Between-Variational-Inference-and-Wasserstein-Gradient-Flows" class="headerlink" title="Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows"></a>Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20090">http://arxiv.org/abs/2310.20090</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yimx/bridging-the-gap-between-vi-and-wgf">https://github.com/yimx/bridging-the-gap-between-vi-and-wgf</a></li>
<li>paper_authors: Mingxuan Yi, Song Liu</li>
<li>for:  bridges the gap between variational inference and Wasserstein gradient flows</li>
<li>methods:  uses the Bures-Wasserstein gradient flow and the path-derivative gradient estimator</li>
<li>results:  demonstrates that the forward Euler scheme of the gradient flow is equivalent to standard black-box variational inference, and offers an alternative perspective on the path-derivative gradient as a distillation procedure to the Wasserstein gradient flow.<details>
<summary>Abstract</summary>
Variational inference is a technique that approximates a target distribution by optimizing within the parameter space of variational families. On the other hand, Wasserstein gradient flows describe optimization within the space of probability measures where they do not necessarily admit a parametric density function. In this paper, we bridge the gap between these two methods. We demonstrate that, under certain conditions, the Bures-Wasserstein gradient flow can be recast as the Euclidean gradient flow where its forward Euler scheme is the standard black-box variational inference algorithm. Specifically, the vector field of the gradient flow is generated via the path-derivative gradient estimator. We also offer an alternative perspective on the path-derivative gradient, framing it as a distillation procedure to the Wasserstein gradient flow. Distillations can be extended to encompass $f$-divergences and non-Gaussian variational families. This extension yields a new gradient estimator for $f$-divergences, readily implementable using contemporary machine learning libraries like PyTorch or TensorFlow.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用变量推理进行减少风险的技术是通过优化变量家族的参数空间中的目标分布来进行优化。然而，沃asserstein梯度流描述了在概率分布的空间上进行优化，而这些优化不一定具有参数概率函数。在这篇论文中，我们将这两种方法相连接。我们证明，在某些条件下，布尔斯-沃asserstein梯度流可以转化为凯尔提供的标准黑盒变量推理算法的欧式迭代计划。具体来说，梯度流的向量场是通过路径DERIVATIVE gradient estimator来生成的。我们还提供了一种另一种视角，将路径DERIVATIVE gradient 框架为沃asserstein梯度流的蒸馏过程。这种蒸馏过程可以扩展到包括 $f$-散度和非泊尔分布的变量家族。这个扩展允许我们提出一个新的梯度估计器，可以使用现代机器学习库如PyTorch或TensorFlow进行实现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.LG_2023_10_31/" data-id="clohum9b500scpj883n44ddq4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/eess.IV_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T09:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/31/eess.IV_2023_10_31/">eess.IV - 2023-10-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Two-Step-Framework-for-Multi-Material-Decomposition-of-Dual-Energy-Computed-Tomography-from-Projection-Domain"><a href="#A-Two-Step-Framework-for-Multi-Material-Decomposition-of-Dual-Energy-Computed-Tomography-from-Projection-Domain" class="headerlink" title="A Two-Step Framework for Multi-Material Decomposition of Dual Energy Computed Tomography from Projection Domain"></a>A Two-Step Framework for Multi-Material Decomposition of Dual Energy Computed Tomography from Projection Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00188">http://arxiv.org/abs/2311.00188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Xu, Qihui Lyu, Dan Ruan, Ke Sheng</li>
<li>for: 这个研究旨在提高多物质分解（MMD）的精度，以便应用于诊断不同组织的胸部疾病。</li>
<li>methods: 这个研究使用了深度学习（DL）方法，并开发了一个便利性高的、非递回的架构（rFast-MMDNet），可以将raw投射资料处理成多物质分解。</li>
<li>results: 这个研究在一个2022年DL-Spectral-Challenge胸部phantomdataset上进行了评估，结果显示rFast-MMDNet可以实现高精度的多物质分解，并且比传统方法更高效。<details>
<summary>Abstract</summary>
Dual-energy computed tomography (DECT) utilizes separate X-ray energy spectra to improve multi-material decomposition (MMD) for various diagnostic applications. However accurate decomposing more than two types of material remains challenging using conventional methods. Deep learning (DL) methods have shown promise to improve the MMD performance, but typical approaches of conducing DL-MMD in the image domain fail to fully utilize projection information or under iterative setup are computationally inefficient in both training and prediction. In this work, we present a clinical-applicable MMD (>2) framework rFast-MMDNet, operating with raw projection data in non-recursive setup, for breast tissue differentiation. rFast-MMDNet is a two-stage algorithm, including stage-one SinoNet to perform dual energy projection decomposition on tissue sinograms and stage-two FBP-DenoiseNet to perform domain adaptation and image post-processing. rFast-MMDNet was tested on a 2022 DL-Spectral-Challenge breast phantom dataset. The two stages of rFast-MMDNet were evaluated separately and then compared with four noniterative reference methods including a direct inversion method (AA-MMD), an image domain DL method (ID-UNet), AA-MMD/ID-UNet + DenoiseNet and a sinogram domain DL method (Triple-CBCT). Our results show that models trained from information stored in DE transmission domain can yield high-fidelity decomposition of the adipose, calcification, and fibroglandular materials with averaged RMSE, MAE, negative PSNR, and SSIM of 0.004+/-~0, 0.001+/-~0, -45.027+/-~0.542, and 0.002+/-~0 benchmarking to the ground truth, respectively. Training of entire rFast-MMDNet on a 4xRTX A6000 GPU cluster took a day with inference time <1s. All DL methods generally led to more accurate MMD than AA-MMD. rFast-MMDNet outperformed Triple-CBCT, but both are superior to the image-domain based methods.
</details>
<details>
<summary>摘要</summary>
dual-energy computed tomography (DECT) 使用不同的X射线能谱spectrum来提高多物质分解(MMD)的诊断应用。然而，使用传统方法分解更多 чем两种材料仍然是挑战。深度学习(DL)方法已经表现出提高MMD性能的潜力，但通常在图像域中进行DL-MMD会不全利用投影信息或者在迭代设置下 computationally inefficient。在这种工作中，我们提出了一个临床可行的MMD（>2）框架rFast-MMDNet，在原始投影数据上运行，不需迭代设置。rFast-MMDNet是一个两stage算法，包括第一个SinoNet，用于在组织射agram上进行双能量投影分解，以及第二个FBP-DenoiseNet，用于适应频率频谱和图像后处理。rFast-MMDNet在2022DL-Spectral-Challenge乳腺phantom数据集上进行测试。两个stage的rFast-MMDNet被分别评估，然后与四种非迭代参照方法进行比较，包括直接逆解方法(AA-MMD)、图像域DL方法(ID-UNet)、AA-MMD/ID-UNet + DenoiseNet和投影域DL方法(Triple-CBCT)。我们的结果表明，通过使用DE传输域中存储的信息，可以实现高精度的脂肪、钙化和肿瘤材料分解，均值RMSE、MAE、负PSNR和SSIM分别为0.004±~0、0.001±~0、-45.027±~0.542和0.002±~0，与参考值相对。训练整个rFast-MMDNet在4xRTX A6000 GPU集群上需要一天时间，推理时间 <1s。所有DL方法都比AA-MMD更准确，而rFast-MMDNet超过Triple-CBCT，但两者都比图像域基本方法更好。
</details></li>
</ul>
<hr>
<h2 id="Fast-multicolour-optical-sectioning-over-extended-fields-of-view-by-combining-interferometric-SIM-with-machine-learning"><a href="#Fast-multicolour-optical-sectioning-over-extended-fields-of-view-by-combining-interferometric-SIM-with-machine-learning" class="headerlink" title="Fast, multicolour optical sectioning over extended fields of view by combining interferometric SIM with machine learning"></a>Fast, multicolour optical sectioning over extended fields of view by combining interferometric SIM with machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00089">http://arxiv.org/abs/2311.00089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward N. Ward, Rebecca M. McClelland, Jacob R. Lamb, Roger Rubio-Sánchez, Charles N. Christensen, Bismoy Mazumder, Sofia Kapsiani, Luca Mascheroni, Lorenzo Di Michele, Gabriele S. Kaminski Schierle, Clemens F. Kaminski</li>
<li>for: 能够高速、高对比度扫描大面积样品</li>
<li>methods: 结合多色 интерferometric模式生成和机器学习处理，实现高对比度、实时重建图像数据</li>
<li>results: 在silico validate和实验中，可以在44 x 44 $\mu m^2$ 场景中实时扫描Fixed和live生物细胞，并且可以在37 Hz的速度下获得高对比度的图像数据<details>
<summary>Abstract</summary>
Structured illumination can reject out-of-focus signal from a sample, enabling high-speed and high-contrast imaging over large areas with widefield detection optics. Currently, this optical-sectioning technique is limited by image reconstruction artefacts and the need for sequential imaging of multiple colour channels. We combine multicolour interferometric pattern generation with machine-learning processing, permitting high-contrast, real-time reconstruction of image data. The method is insensitive to background noise and unevenly phase-stepped illumination patterns. We validate the method in silico and demonstrate its application on diverse specimens, ranging from fixed and live biological cells to synthetic biosystems, imaging at up to 37 Hz across a 44 x 44 $\mu m^2$ field of view.
</details>
<details>
<summary>摘要</summary>
结构化照明可以拒绝样品上的不对焦信号，启用高速高对比度的成像，覆盖大面积的广角探测仪器。目前，这种光学分割技术受到图像重建 artifacts 和多色通道顺序扫描的限制。我们将多色干扰 Pattern 生成与机器学习处理相结合，实现高对比度、实时重建的图像数据。该方法具有背景噪声和不均步骤照明模式的敏感性。我们在软件中验证了该方法，并在多种样品上进行了应用，包括固定和活的生物细胞、synthetic biosystems，成像频率达到37Hz，Field of view 为44 x 44 μm^2。
</details></li>
</ul>
<hr>
<h2 id="UAV-Immersive-Video-Streaming-A-Comprehensive-Survey-Benchmarking-and-Open-Challenges"><a href="#UAV-Immersive-Video-Streaming-A-Comprehensive-Survey-Benchmarking-and-Open-Challenges" class="headerlink" title="UAV Immersive Video Streaming: A Comprehensive Survey, Benchmarking, and Open Challenges"></a>UAV Immersive Video Streaming: A Comprehensive Survey, Benchmarking, and Open Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00082">http://arxiv.org/abs/2311.00082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohit K. Sharma, Chen-Feng Liu, Ibrahim Farhat, Nassim Sehad, Wassim Hamidouche, Merouane Debbah</li>
<li>for: 这篇论文主要是为了探讨无人机上的卷积摄像头捕捉360度视频的实时流处理技术，以提高虚拟现实和混合现实应用的视觉体验。</li>
<li>methods: 这篇论文使用了许多研究方法，包括360度视频编码、封装和流处理，以及对不同飞行条件下的无人机无线通信 channels 的研究。</li>
<li>results: 这篇论文的结果表明，使用hardware实现的HEVC编码器可以获得最佳的编码效率和复杂度平衡，而使用软件实现的AV1编码器可以获得最高的编码效率。此外，作者还提供了一个实际的5G无线网络控制的无人机360度视频流处理测试环境。<details>
<summary>Abstract</summary>
Over the past decade, the utilization of UAVs has witnessed significant growth, owing to their agility, rapid deployment, and maneuverability. In particular, the use of UAV-mounted 360-degree cameras to capture omnidirectional videos has enabled truly immersive viewing experiences with up to 6DoF. However, achieving this immersive experience necessitates encoding omnidirectional videos in high resolution, leading to increased bitrates. Consequently, new challenges arise in terms of latency, throughput, perceived quality, and energy consumption for real-time streaming of such content. This paper presents a comprehensive survey of research efforts in UAV-based immersive video streaming, benchmarks popular video encoding schemes, and identifies open research challenges. Initially, we review the literature on 360-degree video coding, packaging, and streaming, with a particular focus on standardization efforts to ensure interoperability of immersive video streaming devices and services. Subsequently, we provide a comprehensive review of research efforts focused on optimizing video streaming for timevarying UAV wireless channels. Additionally, we introduce a high resolution 360-degree video dataset captured from UAVs under different flying conditions. This dataset facilitates the evaluation of complexity and coding efficiency of software and hardware video encoders based on popular video coding standards and formats, including AVC/H.264, HEVC/H.265, VVC/H.266, VP9, and AV1. Our results demonstrate that HEVC achieves the best trade-off between coding efficiency and complexity through its hardware implementation, while AV1 format excels in coding efficiency through its software implementation, specifically using the libsvt-av1 encoder. Furthermore, we present a real testbed showcasing 360-degree video streaming over a UAV, enabling remote control of the drone via a 5G cellular network.
</details>
<details>
<summary>摘要</summary>
过去一个 décennial，UAV的应用场景增长很 significativement，归功于它们的机敏、快速部署和操纵性。特别是通过UAV上安装的360度摄像头捕捉全景视频，实现了真实的 immerse 视频经验，具有6DoF。然而，实现这种 immerse 经验需要高分辨率编码全景视频，从而导致增加的比特率。因此，实时串流这些内容时出现了新的挑战，包括延迟、吞吐量、视觉质量和能量消耗。本文提供了UAV基于全景视频流媒体的全面评估，比较各种视频编码方案，并提出了未来研究的开放挑战。首先，我们查看了360度视频编码、封装和流媒体的相关研究，尤其是标准化努力，以确保全景视频流媒体设备和服务的互操作性。接着，我们提供了关于在时变UAV无线通信频道上优化视频流媒体的全面评估。此外，我们还提供了高分辨率全景视频数据集， captured from UAVs under different flying conditions。这个数据集可以评估不同 виде码器和格式的编码效率和复杂度，包括AVC/H.264、HEVC/H.265、VVC/H.266、VP9和AV1。我们的结果表明，HEVC在硬件实现方式下实现了最佳的平衡，而AV1格式在软件实现方式下在编码效率方面表现出色。此外，我们还提供了一个真实的UAV全景视频串流实验室，使得通过5G移动网络控制无人机。
</details></li>
</ul>
<hr>
<h2 id="Harmonization-enriched-domain-adaptation-with-light-fine-tuning-for-multiple-sclerosis-lesion-segmentation"><a href="#Harmonization-enriched-domain-adaptation-with-light-fine-tuning-for-multiple-sclerosis-lesion-segmentation" class="headerlink" title="Harmonization-enriched domain adaptation with light fine-tuning for multiple sclerosis lesion segmentation"></a>Harmonization-enriched domain adaptation with light fine-tuning for multiple sclerosis lesion segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20586">http://arxiv.org/abs/2310.20586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinwei Zhang, Lianrui Zuo, Blake E. Dewey, Samuel W. Remedios, Savannah P. Hays, Dzung L. Pham, Jerry L. Prince, Aaron Carass</li>
<li>for: This paper aims to improve the performance of deep learning algorithms in segmenting multiple sclerosis (MS) lesions using magnetic resonance (MR) images, specifically by addressing the challenge of domain generalization errors.</li>
<li>methods: The proposed approach integrates one-shot adaptation data with harmonized training data that incorporates labels, using a process called “contrast harmonization” in MRI. This approach involves synthesizing new training data with a contrast akin to that of the test domain.</li>
<li>results: The amalgamation of one-shot adaptation data with harmonized training data surpasses the performance of utilizing either data source in isolation. Additionally, domain adaptation using exclusively harmonized training data achieved comparable or even superior performance compared to one-shot adaptation, with minimal fine-tuning required (ranging from 2 to 5 epochs for convergence).<details>
<summary>Abstract</summary>
Deep learning algorithms utilizing magnetic resonance (MR) images have demonstrated cutting-edge proficiency in autonomously segmenting multiple sclerosis (MS) lesions. Despite their achievements, these algorithms may struggle to extend their performance across various sites or scanners, leading to domain generalization errors. While few-shot or one-shot domain adaptation emerges as a potential solution to mitigate generalization errors, its efficacy might be hindered by the scarcity of labeled data in the target domain. This paper seeks to tackle this challenge by integrating one-shot adaptation data with harmonized training data that incorporates labels. Our approach involves synthesizing new training data with a contrast akin to that of the test domain, a process we refer to as "contrast harmonization" in MRI. Our experiments illustrate that the amalgamation of one-shot adaptation data with harmonized training data surpasses the performance of utilizing either data source in isolation. Notably, domain adaptation using exclusively harmonized training data achieved comparable or even superior performance compared to one-shot adaptation. Moreover, all adaptations required only minimal fine-tuning, ranging from 2 to 5 epochs for convergence.
</details>
<details>
<summary>摘要</summary>
深度学习算法利用核磁共振图像（MR图像）自动 segmenting多发性脑脊细胞病（MS）斑点， despite their achievements， these algorithms may struggle to extend their performance across various sites or scanners, leading to domain generalization errors. While few-shot or one-shot domain adaptation emerges as a potential solution to mitigate generalization errors, its efficacy might be hindered by the scarcity of labeled data in the target domain. This paper seeks to tackle this challenge by integrating one-shot adaptation data with harmonized training data that incorporates labels. Our approach involves synthesizing new training data with a contrast akin to that of the test domain, a process we refer to as "contrast harmonization" in MRI. Our experiments illustrate that the amalgamation of one-shot adaptation data with harmonized training data surpasses the performance of utilizing either data source in isolation. Notably, domain adaptation using exclusively harmonized training data achieved comparable or even superior performance compared to one-shot adaptation. Moreover, all adaptations required only minimal fine-tuning, ranging from 2 to 5 epochs for convergence.
</details></li>
</ul>
<hr>
<h2 id="C-Silicon-based-metasurfaces-for-aperture-robust-spectrometer-imaging-with-angle-integration"><a href="#C-Silicon-based-metasurfaces-for-aperture-robust-spectrometer-imaging-with-angle-integration" class="headerlink" title="C-Silicon-based metasurfaces for aperture-robust spectrometer&#x2F;imaging with angle integration"></a>C-Silicon-based metasurfaces for aperture-robust spectrometer&#x2F;imaging with angle integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20289">http://arxiv.org/abs/2310.20289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weizhu Xu, Qingbin Fan, Peicheng Lin, Jiarong Wang, Hao Hu, Tao Yue, Xuemei Hu, Ting Xu</li>
<li>for:  This paper aims to develop a compact silicon metasurfaces-based spectrometer&#x2F;camera that can achieve high spectral consistency and angular stability over a wide working bandwidth.</li>
<li>methods: The proposed method uses spectrally engineered filtering and angle integration to achieve a robust spectral response and high accuracy in reconstructing hyperspectral signals.</li>
<li>results: The experimental results show that the proposed method can maintain the spectral consistency from F&#x2F;1.8 to F&#x2F;4 (7° to 16° angle of incident light) and accurately reconstruct the incident hyperspectral signal with a fidelity exceeding 99%. Additionally, a spectral imaging system with 400x400 pixels was established, and the accurate reconstructed hyperspectral image demonstrates the potential of the proposed aperture-robust spectrometer to be extended as a high-resolution broadband hyperspectral camera.<details>
<summary>Abstract</summary>
Compared with conventional grating-based spectrometers, reconstructive spectrometers based on spectrally engineered filtering have the advantage of miniaturization because of the less demand for dispersive optics and free propagation space. However, available reconstructive spectrometers fail to balance the performance on operational bandwidth, spectral diversity and angular stability. In this work, we proposed a compact silicon metasurfaces based spectrometer/camera. After angle integration, the spectral response of the system is robust to angle/aperture within a wide working bandwidth from 400nm to 800nm. It is experimentally demonstrated that the proposed method could maintain the spectral consistency from F/1.8 to F/4 (The corresponding angle of incident light ranges from 7{\deg} to 16{\deg}) and the incident hyperspectral signal could be accurately reconstructed with a fidelity exceeding 99%. Additionally, a spectral imaging system with 400x400 pixels is also established in this work. The accurate reconstructed hyperspectral image indicates that the proposed aperture-robust spectrometer has the potential to be extended as a high-resolution broadband hyperspectral camera.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/eess.IV_2023_10_31/" data-id="clohum9go0172pj882zktcimx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/eess.SP_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T08:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/31/eess.SP_2023_10_31/">eess.SP - 2023-10-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multi-functional-OFDM-Signal-Design-for-Integrated-Sensing-Communications-and-Power-Transfer"><a href="#Multi-functional-OFDM-Signal-Design-for-Integrated-Sensing-Communications-and-Power-Transfer" class="headerlink" title="Multi-functional OFDM Signal Design for Integrated Sensing, Communications, and Power Transfer"></a>Multi-functional OFDM Signal Design for Integrated Sensing, Communications, and Power Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00104">http://arxiv.org/abs/2311.00104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yumeng Zhang, Sundar Aditya, Bruno Clerckx</li>
<li>for: 这 paper  investigate 一种 integrate 感知、通信和能源供应（ISCAP）系统，通过设计宽带 OFDM 信号来传输能量和感知信息。</li>
<li>methods: 作者使用 non-zero mean asymmetric Gaussian distribution 优化输入信号的均值和方差，以最大化收集的能量，同时满足通信和感知的约束。</li>
<li>results: 研究结果表明， optimize 输入 distribution 可以更大的性能区域，比如（i）同时满足通信和感知约束的 symmetric complex Gaussian input distribution，（ii）zero-mean symmetric complex Gaussian input distribution，以及（iii）分配能量和感知信号的 coexisting solution。<details>
<summary>Abstract</summary>
The wireless domain is witnessing a flourishing of integrated systems, e.g. (a) integrated sensing and communications, and (b) simultaneous wireless information and power transfer, due to their potential to use resources (spectrum, power) judiciously. Inspired by this trend, we investigate integrated sensing, communications and powering (ISCAP), through the design of a wideband OFDM signal to power a sensor while simultaneously performing target-sensing and communication. To characterize the ISCAP performance region, we assume symbols with non-zero mean asymmetric Gaussian distribution (i.e., the input distribution), and optimize its mean and variance at each subcarrier to maximize the harvested power, subject to constraints on the achievable rate (communications) and the average side-to-peak-lobe difference (sensing). The resulting input distribution, through simulations, achieves a larger performance region than that of (i) a symmetric complex Gaussian input distribution with identical mean and variance for the real and imaginary parts, (ii) a zero-mean symmetric complex Gaussian input distribution, and (iii) the superposed power-splitting communication and sensing signal (the coexisting solution). In particular, the optimized input distribution balances the three functions by exhibiting the following features: (a) symbols in subcarriers with strong communication channels have high variance to satisfy the rate constraint, while the other symbols are dominated by the mean, forming a relatively uniform sum of mean and variance across subcarriers for sensing; (b) with looser communication and sensing constraints, large absolute means appear on subcarriers with stronger powering channels for higher harvested power. As a final note, the results highlight the great potential of the co-designed ISCAP system for further efficiency enhancement.
</details>
<details>
<summary>摘要</summary>
无线领域目前正在蓬勃发展集成系统，例如（a）集成感知和通信，以及（b）同时进行无线信息和能量传输，这些系统的可能性使得它们可以有效利用资源（频率、功率）。 inspirited by this trend, we investigate integrated sensing, communications and powering (ISCAP) through the design of a wideband OFDM signal to power a sensor while simultaneously performing target-sensing and communication. To characterize the ISCAP performance region, we assume symbols with non-zero mean asymmetric Gaussian distribution (i.e., the input distribution), and optimize its mean and variance at each subcarrier to maximize the harvested power, subject to constraints on the achievable rate (communications) and the average side-to-peak-lobe difference (sensing). The resulting input distribution, through simulations, achieves a larger performance region than that of (i) a symmetric complex Gaussian input distribution with identical mean and variance for the real and imaginary parts, (ii) a zero-mean symmetric complex Gaussian input distribution, and (iii) the superposed power-splitting communication and sensing signal (the coexisting solution). In particular, the optimized input distribution balances the three functions by exhibiting the following features: (a) symbols in subcarriers with strong communication channels have high variance to satisfy the rate constraint, while the other symbols are dominated by the mean, forming a relatively uniform sum of mean and variance across subcarriers for sensing; (b) with looser communication and sensing constraints, large absolute means appear on subcarriers with stronger powering channels for higher harvested power. As a final note, the results highlight the great potential of the co-designed ISCAP system for further efficiency enhancement.Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and widely used in other countries. The traditional Chinese form may be slightly different.
</details></li>
</ul>
<hr>
<h2 id="Robust-Waveform-Design-for-Integrated-Sensing-and-Communication"><a href="#Robust-Waveform-Design-for-Integrated-Sensing-and-Communication" class="headerlink" title="Robust Waveform Design for Integrated Sensing and Communication"></a>Robust Waveform Design for Integrated Sensing and Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00071">http://arxiv.org/abs/2311.00071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Spratm-Asleaf/Robust-Waveform">https://github.com/Spratm-Asleaf/Robust-Waveform</a></li>
<li>paper_authors: Shixiong Wang, Wei Dai, Haowei Wang, Geoffrey Ye Li</li>
<li>for: 这篇论文旨在研究未来通信系统中的集成感知通信（ISAC）系统，以确保硬件、资源和波形之间的共享。</li>
<li>methods: 该论文采用了robust波形设计方法，以便在实际操作中保证通信性能的可靠性。</li>
<li>results:  simulations results表明，使用robust波形设计方法可以确保实际操作中的通信性能可以实现，而 nominally-estimated communication performance则无法保证。<details>
<summary>Abstract</summary>
Integrated sensing and communication (ISAC), which enables hardware, resources (e.g., spectra), and waveforms sharing, is becoming a key feature in future-generation communication systems. This paper investigates robust waveform design for ISAC systems when the underlying true communication channels (e.g. time-selective ones) are not accurately known. With uncertainties in nominal communication channel models, the nominally-estimated communication performance may be not achievable in practice; i.e., the communication performance of ISAC systems cannot be guaranteed. Therefore, we formulate robust waveform design problems by studying the worst-case channels and prove that the robustly-estimated performance is guaranteed to be attainable in real-world operation. As a consequence, the reliability of ISAC systems in terms of communication performance is improved. The robust waveform design problems are shown to be non-convex, non-differentiable, and high-dimensional, which cannot be solved using existing optimization techniques. Therefore, we develop a computationally-efficient and globally-optimal algorithm to solve them. Simulation results show that the robustly-estimated communication performance can be ensured to be practically reachable while the nominally-estimated performance cannot, which validates the value of robust design.
</details>
<details>
<summary>摘要</summary>
“集成感知和通信（ISAC）”，允许硬件、资源（例如频谱）和波形共享，成为未来通信系统的关键特性。这篇论文研究了未知真实通信 канаels（例如时选择性的）下的ISAC系统Robust波形设计问题。在不精确知道通信 канаels模型的情况下， Nominal 通信性能可能无法实现在实际操作中。因此，我们将 robust波形设计问题表述为研究最差通信 канаels，并证明了Robust设计下的通信性能是可以在实际操作中实现的。这有助于提高ISAC系统的通信可靠性。robust波形设计问题是非对称、非微分、高维的，无法使用现有的优化技术解决。因此，我们开发了一种计算效率高和全球最优的算法来解决这些问题。实验结果表明，可以确保Robust设计下的通信性能是实际操作中可以实现的，而 Nominal 设计下的通信性能无法保证。这证明了robust设计的价值。
</details></li>
</ul>
<hr>
<h2 id="A-Portable-Ultrasound-Imaging-Pipeline-Implementation-with-GPU-Acceleration-on-Nvidia-CLARA-AGX"><a href="#A-Portable-Ultrasound-Imaging-Pipeline-Implementation-with-GPU-Acceleration-on-Nvidia-CLARA-AGX" class="headerlink" title="A Portable Ultrasound Imaging Pipeline Implementation with GPU Acceleration on Nvidia CLARA AGX"></a>A Portable Ultrasound Imaging Pipeline Implementation with GPU Acceleration on Nvidia CLARA AGX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00482">http://arxiv.org/abs/2311.00482</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. N. Madhavanunni, V. Arun Kumar, Mahesh Raveendranatha Panicker</li>
<li>for: 这个论文旨在描述一个基于Nvidia CLARA AGX开发kit的GPU加速的手持式超声图像处理管线的原型实现。</li>
<li>methods: 该管线使用非定向扩散波传输器获取原始数据，并将接收到的信号传输到Nvidia CLARA AGX开发平台进行加速图像处理。GPU加速了传统的延迟和和加（DAS）扩散器以及两种适应非线性扩散器和两种福里ер基于技术的实现。</li>
<li>results: 实验表明，该管线的完整性和图像质量与CPU实现相同，而执行速度则在不同的探测网格大小下显示出了明显的加速效果，最高达180倍于CPU实现。此外，由于该管线使用Nvidia CLARA AGX，因此潜在地可以实现在线&#x2F;活动学习approaches。<details>
<summary>Abstract</summary>
In this paper, we present a GPU-accelerated prototype implementation of a portable ultrasound imaging pipeline on an Nvidia CLARA AGX development kit. The raw data is acquired with nonsteered plane wave transmit using a programmable handheld open platform that supports 128-channel transmit and 64-channel receive. The received signals are transferred to the Nvidia CLARA AGX developer platform through a host system for accelerated imaging. GPU-accelerated implementation of the conventional delay and sum (DAS) beamformer along with two adaptive nonlinear beamformers and two Fourier-based techniques was performed. The feasibility of the complete pipeline and its imaging performance was evaluated with in-vitro phantom imaging experiments and the efficacy is demonstrated with preliminary in-vivo scans. The image quality quantified by the standard contrast and resolution metrics was comparable with that of the CPU implementation. The execution speed of the implemented beamformers was also investigated for different sizes of imaging grids and a significant speedup as high as 180 times that of the CPU implementation was observed. Since the proposed pipeline involves Nvidia CLARA AGX, there is always the potential for easy incorporation of online/active learning approaches.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个使用GPU加速的手持式超声成像管线的原型实现，使用Nvidia CLARA AGX开发工具箱。 raw数据是通过非导向的平面波发射来获得，使用可编程的手持式开放平台，支持128通道发射和64通道接收。接收的信号将被传输到Nvidia CLARA AGX开发平台，并通过加速镜像。我们在GPU上实现了传统的延迟和和加（DAS）扫描器，以及两种适应非线性扫描器和两种基于傅立叶变换的技术。我们通过对各种扫描格大小进行调查，发现GPU实现的扫描器运行速度可以达到CPU实现的180倍。由于提出的管线使用Nvidia CLARA AGX，因此总是可以轻松地添加在线/活动学习方法。Note: Simplified Chinese is used here, which is a more common and widely used standard for Chinese writing. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Energy-Aware-Adaptive-Sampling-for-Self-Sustainability-in-Resource-Constrained-IoT-Devices"><a href="#Energy-Aware-Adaptive-Sampling-for-Self-Sustainability-in-Resource-Constrained-IoT-Devices" class="headerlink" title="Energy-Aware Adaptive Sampling for Self-Sustainability in Resource-Constrained IoT Devices"></a>Energy-Aware Adaptive Sampling for Self-Sustainability in Resource-Constrained IoT Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20331">http://arxiv.org/abs/2310.20331</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ETH-PBL/EcoTrack">https://github.com/ETH-PBL/EcoTrack</a></li>
<li>paper_authors: Marco Giordano, Silvano Cortesi, Prodromos-Vasileios Mekikis, Michele Crabolu, Giovanni Bellusci, Michele Magno</li>
<li>for: 这篇论文的目的是为了提出一个能源感知的自适应抽象率算法，用于嵌入式部署在资源受限的、电池供电的互联网东西上。</li>
<li>methods: 这篇论文使用了一个变化状态机(FSM)，并受到转送控制协议(TCP) Reno的添加性增加和多项增加的灵感，以最大化侦测器抽象率，以确保无需电池耗尽。</li>
<li>results: 这篇论文的结果显示，提出的算法可以确保自我可持续性，同时最大化测量位置的数量。实验结果显示，在使用3000mAh电池的情况下，算法可以维持每天至少24个本地化位置，并在某些情况下达到3000个。<details>
<summary>Abstract</summary>
In the ever-growing Internet of Things (IoT) landscape, smart power management algorithms combined with energy harvesting solutions are crucial to obtain self-sustainability. This paper presents an energy-aware adaptive sampling rate algorithm designed for embedded deployment in resource-constrained, battery-powered IoT devices. The algorithm, based on a finite state machine (FSM) and inspired by Transmission Control Protocol (TCP) Reno's additive increase and multiplicative decrease, maximizes sensor sampling rates, ensuring power self-sustainability without risking battery depletion. Moreover, we characterized our solar cell with data acquired over 48 days and used the model created to obtain energy data from an open-source world-wide dataset. To validate our approach, we introduce the EcoTrack device, a versatile device with global navigation satellite system (GNSS) capabilities and Long-Term Evolution Machine Type Communication (LTE-M) connectivity, supporting MQTT protocol for cloud data relay. This multi-purpose device can be used, for instance, as a health and safety wearable, remote hazard monitoring system, or as a global asset tracker. The results, validated on data from three different European cities, show that the proposed algorithm enables self-sustainability while maximizing sampled locations per day. In experiments conducted with a 3000 mAh battery capacity, the algorithm consistently maintained a minimum of 24 localizations per day and achieved peaks of up to 3000.
</details>
<details>
<summary>摘要</summary>
在互联网对物（IoT）的扩大景象中，智能电力管理算法和能量采集解决方案是达到自我维持性的关键。本文提出了针对嵌入式部署的能量意识适应样本率算法，以 Maximize sensor sampling rates，确保无电池耗尽。此外，我们对我们的太阳能电池进行了48天的数据收集和分析，并使用了开源的世界范围数据集来获取能量数据。为验证我们的方法，我们引入了EcoTrack设备，这是一种多功能设备，具有全球定位系统（GNSS）和Long-Term Evolution Machine Type Communication（LTE-M）连接，支持MQTT协议 для云数据 relay。这种多用途设备可以在各种应用中使用，例如健康和安全护身器、远程危险监测系统或全球资产跟踪系统。实验结果，基于数据来自三个欧洲城市，表明提出的算法可以实现自我维持性，同时最大化采集的位置数。在使用3000mAh电池容量时，算法一直保持了每天至少24个地点的地点采集，并达到了3000个峰值。
</details></li>
</ul>
<hr>
<h2 id="Age-Optimum-Sampling-in-Non-Stationary-Environment"><a href="#Age-Optimum-Sampling-in-Non-Stationary-Environment" class="headerlink" title="Age Optimum Sampling in Non-Stationary Environment"></a>Age Optimum Sampling in Non-Stationary Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20275">http://arxiv.org/abs/2310.20275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinheng Zhang, Haoyue Tang, Jintao Wang, Sastry Kompella, Leandros Tassiulas</li>
<li>for: 这个论文的目的是设计一种在无知延迟分布情况下实时更新状态信息的策略，以最小化接收端的数据新鲜度（Age-of-Information，AoI）。</li>
<li>methods: 该论文使用了线性Programming和非参数阶跃分析方法，以及启发式变换点检测算法来学习和更新最佳更新阈值，同时检测延迟分布的变化。</li>
<li>results:  simulations results显示，提议的算法可以快速检测延迟分布的变化，并且由提议的策略获得的AoI值与最小AoI值相近。<details>
<summary>Abstract</summary>
In this work, we consider a status update system with a sensor and a receiver. The status update information is sampled by the sensor and then forwarded to the receiver through a channel with non-stationary delay distribution. The data freshness at the receiver is quantified by the Age-of-Information (AoI). The goal is to design an online sampling strategy that can minimize the average AoI when the non-stationary delay distribution is unknown. Assuming that channel delay distribution may change over time, to minimize the average AoI, we propose a joint stochastic approximation and non-parametric change point detection algorithm that can: (1) learn the optimum update threshold when the delay distribution remains static; (2) detect the change in transmission delay distribution quickly and then restart the learning process. Simulation results show that the proposed algorithm can quickly detect the delay changes, and the average AoI obtained by the proposed policy converges to the minimum AoI.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们考虑了一个状态更新系统，包括一个感知器和一个接收器。状态更新信息由感知器采样并传输到接收器通过一个不固定延迟分布的通道。接收器的数据新鲜度被衡量为年龄信息（AoI）。目标是设计一种在线采样策略，可以最小化接收器的均值AoI，当传输延迟分布不确定时。假设通道延迟分布可能会随着时间的变化，我们提议一种联合随机批处理和非参数变化检测算法，可以：（1）在延迟分布保持静止时学习最佳更新阈值；（2）快速检测传输延迟分布的变化，然后重新开始学习过程。 simulation结果表明，提议的算法可以快速检测延迟变化，并且由提议的策略获得的均值AoI与最小AoI相近。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Reflecting-Surface-Assisted-UAV-Communications-for-6G-Networks"><a href="#Intelligent-Reflecting-Surface-Assisted-UAV-Communications-for-6G-Networks" class="headerlink" title="Intelligent-Reflecting-Surface-Assisted UAV Communications for 6G Networks"></a>Intelligent-Reflecting-Surface-Assisted UAV Communications for 6G Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20242">http://arxiv.org/abs/2310.20242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaolong Ning, Tengfeng Li, Yu Wu, Xiaojie Wang, Qingqing Wu, Fei Richard Yu, Song Guo</li>
<li>for: 这篇论文旨在探讨6代移动网络中使用智能反射表面（IRS）和无人飞行器（UAV）技术，以解决地面网络的覆盖困难和资源约束问题。</li>
<li>methods: 本文使用智能反射表面技术可以重构无线环境，并实现无线网络传输在成本效益的方式。</li>
<li>results: 本文通过对IRS助けUAV通信的综述，提出了具体的解决方案，并讨论了相关领域的未来研究方向。<details>
<summary>Abstract</summary>
In 6th-Generation (6G) mobile networks, Intelligent Reflective Surfaces (IRSs) and Unmanned Aerial Vehicles (UAVs) have emerged as promising technologies to address the coverage difficulties and resource constraints faced by terrestrial networks. UAVs, with their mobility and low costs, offer diverse connectivity options for mobile users and a novel deployment paradigm for 6G networks. However, the limited battery capacity of UAVs, dynamic and unpredictable channel environments, and communication resource constraints result in poor performance of traditional UAV-based networks. IRSs can not only reconstruct the wireless environment in a unique way, but also achieve wireless network relay in a cost-effective manner. Hence, it receives significant attention as a promising solution to solve the above challenges. In this article, we conduct a comprehensive survey on IRS-assisted UAV communications for 6G networks. First, primary issues, key technologies, and application scenarios of IRS-assisted UAV communications for 6G networks are introduced. Then, we put forward specific solutions to the issues of IRS-assisted UAV communications. Finally, we discuss some open issues and future research directions to guide researchers in related fields.
</details>
<details>
<summary>摘要</summary>
在6代移动网络（6G）中，智能反射表面（IRS）和无人机（UAV）已经出现为解决地面网络覆盖困难和资源约束的技术。UAV通过其 mobilility和低成本提供了多样化的连接选择 для移动用户，并提供了6G网络的新的部署模式。然而，UAV的电池容量有限，通信环境是动态和随机的，这些因素导致了传统的UAV网络的性能不佳。IRS可以不仅重构无线环境，还可以实现无线网络中继在成本效果的方式。因此，它在6G网络中receives significant attention。在这篇文章中，我们提供了IRS-assisted UAV通信在6G网络中的全面调研。首先，我们介绍了6G网络中IRS-assisted UAV通信的主要问题、关键技术和应用场景。然后，我们提出了IRS-assisted UAV通信中的特定问题的解决方案。最后，我们讨论了一些未解决的问题和未来研究方向，以帮助研究人员在相关领域进行研究。
</details></li>
</ul>
<hr>
<h2 id="Structured-Two-Stage-True-Time-Delay-Array-Codebook-Design-for-Multi-User-Data-Communication"><a href="#Structured-Two-Stage-True-Time-Delay-Array-Codebook-Design-for-Multi-User-Data-Communication" class="headerlink" title="Structured Two-Stage True-Time-Delay Array Codebook Design for Multi-User Data Communication"></a>Structured Two-Stage True-Time-Delay Array Codebook Design for Multi-User Data Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20198">http://arxiv.org/abs/2310.20198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Wadaskar, Ding Zhao, Ibrahim Pehlivan, Danijela Cabric</li>
<li>for: 该论文旨在提出一种结构化的analogTrue-Time-Delay（TTD）编码库，以实现频率 dependent beamforming的广泛毫米波和tera响度系统。</li>
<li>methods: 该论文使用了一种名为Staircase TTD编码库，用于生成具有量化子频道到角度映射的 beam patterns。</li>
<li>results: 研究人员通过分析closed-form two-stage设计的提议编码库，实现了需要的子频道特定的beam patterns，并评估其在多用户通信网络中的性能。<details>
<summary>Abstract</summary>
Wideband millimeter-wave and terahertz (THz) systems can facilitate simultaneous data communication with multiple spatially separated users. It is desirable to orthogonalize users across sub-bands by deploying frequency-dependent beams with a sub-band-specific spatial response. True-Time-Delay (TTD) antenna arrays are a promising wideband architecture to implement sub-band-specific dispersion of beams across space using a single radio frequency (RF) chain. This paper proposes a structured design of analog TTD codebooks to generate beams that exhibit quantized sub-band-to-angle mapping. We introduce a structured Staircase TTD codebook and analyze the frequency-spatial behaviour of the resulting beam patterns. We develop the closed-form two-stage design of the proposed codebook to achieve the desired sub-band-specific beams and evaluate their performance in multi-user communication networks.
</details>
<details>
<summary>摘要</summary>
宽带 millimeter 波和teraHertz（THz）系统可以实现同时与多个空间分隔的用户进行数据通信。希望在sub-band中对用户进行正交化，可以通过部署频率相依的扬射器来实现。真实时间延迟（TTD）天线阵列是一种广泛应用的宽带体系，可以通过单个电磁波（RF）链实现sub-band特定的扬射器分布。这篇论文提出了一种结构化的analog TTD编码库，用于生成具有量化sub-band到角度的映射的扬射器。我们引入了一种结构化的梯形TTD编码库，并分析了所得到的扬射器模式的频率空间行为。我们还开发了封闭形two-stage设计的提议的编码库，以实现所需的sub-band特定的扬射器，并评估其在多用户通信网络中的性能。
</details></li>
</ul>
<hr>
<h2 id="SWIPT-in-Mixed-Near-and-Far-Field-Channels-Joint-Beam-Scheduling-and-Power-Allocation"><a href="#SWIPT-in-Mixed-Near-and-Far-Field-Channels-Joint-Beam-Scheduling-and-Power-Allocation" class="headerlink" title="SWIPT in Mixed Near- and Far-Field Channels: Joint Beam Scheduling and Power Allocation"></a>SWIPT in Mixed Near- and Far-Field Channels: Joint Beam Scheduling and Power Allocation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20186">http://arxiv.org/abs/2310.20186</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/francisco-cabanillas/BC_231918-201864-231073">https://github.com/francisco-cabanillas/BC_231918-201864-231073</a></li>
<li>paper_authors: Yunpu Zhang, Changsheng You<br>for: 这篇论文主要是为了解决将来无线网络中大规模天线数组（XL-array）技术的 spectrum efficiency 和空间分辨率提高问题。methods: 这篇论文使用了一种新的实用方法，即混合近场和远场同时进行无线信息传输和能量充电（SWIPT）。results: 研究发现，在XL-array基站（BS）的近场和远场区域中，分别使用了能量充电（EH）和信息解码（ID）接收器，可以提高无线网络的 spectrum efficiency 和空间分辨率。此外，研究还发现，在特定情况下，可以减少BS发射功率，同时保证ID接收器的最佳性能。<details>
<summary>Abstract</summary>
Extremely large-scale array (XL-array) has emerged as a promising technology to enhance the spectrum efficiency and spatial resolution in future wireless networks by exploiting massive number of antennas for generating pencil-like beamforming. This also leads to a fundamental paradigm shift from conventional far-field communications towards the new near-field communications. In contrast to the existing works that mostly considered simultaneous wireless information and power transfer (SWIPT) in the far field, we consider in this paper a new and practical scenario, called mixed near- and far-field SWIPT, where energy harvesting (EH) and information decoding (ID) receivers are located in the near- and far-field regions of the XL-array base station (BS), respectively. Specifically, we formulate an optimization problem to maximize the weighted sum-power harvested at all EH receivers by jointly designing the BS beam scheduling and power allocation, under the constraints on the maximum sum-rate and BS transmit power. First, for the general case with multiple EH and ID receivers, we propose an efficient algorithm to obtain a suboptimal solution by utilizing the binary variable elimination and successive convex approximation methods. To obtain useful insights, we then study the joint design for special cases. In particular, we show that when there are multiple EH receivers and one ID receiver, in most cases, the optimal design is allocating a portion of power to the ID receiver for satisfying the rate constraint, while the remaining power is allocated to one EH receiver with the highest EH capability. This is in sharp contrast to the conventional far-field SWIPT case, for which all powers should be allocated to ID receivers. Numerical results show that our proposed joint design significantly outperforms other benchmark schemes without the optimization of beam scheduling and/or power allocation.
</details>
<details>
<summary>摘要</summary>
巨大规模阵列（XL-array）已成为未来无线网络增强谱spectrum效率和空间分辨率的承诺技术，通过大量天线的使用来实现射频束形成。这也导致了传统远场通信的基本思维方式向新近场通信转变。在现有工作中，大多数研究者对同时进行无线信息和能量传输（SWIPT）进行了研究，但我们在这篇论文中考虑了一种新的和实用的场景：混合近场和远场SWIPT，其中能量收集（EH）和信息解码（ID）接收器分别位于XL-array基站（BS）的近场和远场区域。特别是，我们形ulated一个优化问题，以最大化所有EH接收器中的权重和谱power，通过BS束指定和能量分配的共同设计，并且保证BS发射功率的最大化和总率的满足。首先，我们对多个EH和ID接收器的总 caso提出了一种有效的算法，通过使用二进制变量消除和逻辑减法方法来寻找一个近似优化解。为了获得有用的洞察，我们然后研究了特定的场合的共同设计。结果表明，当有多个EH接收器和一个ID接收器时，优化设计通常是将一部分功率分配给ID接收器以满足率限制，而剩下的功率分配给EH接收器的最高EH能力。这与传统远场SWIPT情况不同，在那里所有功率都应该分配给ID接收器。数值结果显示，我们提出的共同设计在其他参考方案无法优化 beam指定和/或能量分配时显著性能更好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/eess.SP_2023_10_31/" data-id="clohum9i601anpj882agkgty8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.SD_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T15:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/cs.SD_2023_10_30/">cs.SD - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DCHT-Deep-Complex-Hybrid-Transformer-for-Speech-Enhancement"><a href="#DCHT-Deep-Complex-Hybrid-Transformer-for-Speech-Enhancement" class="headerlink" title="DCHT: Deep Complex Hybrid Transformer for Speech Enhancement"></a>DCHT: Deep Complex Hybrid Transformer for Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19602">http://arxiv.org/abs/2310.19602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialu Li, Junhui Li, Pu Wang, Youshan Zhang</li>
<li>for: 提高Speech噪音消除的性能</li>
<li>methods: 提出了一种深度复杂混合变换器， integrate both spectrogram和waveform domain的方法，以提高Speech噪音消除的性能</li>
<li>results: 实验结果表明，该方法可以在BirdSoundsDenoising数据集和VCTK+DEMAND数据集上比 estado-of-the-art 方法更好地提高Speech噪音消除的性能。<details>
<summary>Abstract</summary>
Most of the current deep learning-based approaches for speech enhancement only operate in the spectrogram or waveform domain. Although a cross-domain transformer combining waveform- and spectrogram-domain inputs has been proposed, its performance can be further improved. In this paper, we present a novel deep complex hybrid transformer that integrates both spectrogram and waveform domains approaches to improve the performance of speech enhancement. The proposed model consists of two parts: a complex Swin-Unet in the spectrogram domain and a dual-path transformer network (DPTnet) in the waveform domain. We first construct a complex Swin-Unet network in the spectrogram domain and perform speech enhancement in the complex audio spectrum. We then introduce improved DPT by adding memory-compressed attention. Our model is capable of learning multi-domain features to reduce existing noise on different domains in a complementary way. The experimental results on the BirdSoundsDenoising dataset and the VCTK+DEMAND dataset indicate that our method can achieve better performance compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Current deep learning-based speech enhancement methods mostly operate in the spectrogram or waveform domain. Although a cross-domain transformer combining waveform- and spectrogram-domain inputs has been proposed, its performance can be further improved. In this paper, we present a novel deep complex hybrid transformer that integrates both spectrogram and waveform domains approaches to improve speech enhancement performance. The proposed model consists of two parts: a complex Swin-Unet in the spectrogram domain and a dual-path transformer network (DPTnet) in the waveform domain.First, we construct a complex Swin-Unet network in the spectrogram domain and perform speech enhancement in the complex audio spectrum. We then introduce improved DPT by adding memory-compressed attention. Our model is capable of learning multi-domain features to reduce existing noise on different domains in a complementary way. The experimental results on the BirdSoundsDenoising dataset and the VCTK+DEMAND dataset indicate that our method can achieve better performance compared to state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Sound-of-Story-Multi-modal-Storytelling-with-Audio"><a href="#Sound-of-Story-Multi-modal-Storytelling-with-Audio" class="headerlink" title="Sound of Story: Multi-modal Storytelling with Audio"></a>Sound of Story: Multi-modal Storytelling with Audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19264">http://arxiv.org/abs/2310.19264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaeyeon Bae, Seokhoon Jeong, Seokun Kang, Namgi Han, Jae-Yon Lee, Hyounghun Kim, Taehwan Kim</li>
<li>for: 本研究旨在扩展故事理解和告诉领域，增加一个新的“背景声”组件，该组件基于故事上下文的音频信息，不含语言信息。</li>
<li>methods: 作者提出了一个新的数据集“声音故事”（SoS），该数据集包含27,354个故事，每个故事有19.6张图片和984小时的无语言音频信息。作者还提出了多个基线任务，包括模式转换、音频生成等。</li>
<li>results: 作者通过实验表明，该数据集和任务可以帮助研究人员更好地理解故事的多模态表达，并提出了强大的基线任务。数据集和代码将在链接中公开：<a target="_blank" rel="noopener" href="https://github.com/Sosdatasets/SoS_Dataset%E3%80%82">https://github.com/Sosdatasets/SoS_Dataset。</a><details>
<summary>Abstract</summary>
Storytelling is multi-modal in the real world. When one tells a story, one may use all of the visualizations and sounds along with the story itself. However, prior studies on storytelling datasets and tasks have paid little attention to sound even though sound also conveys meaningful semantics of the story. Therefore, we propose to extend story understanding and telling areas by establishing a new component called "background sound" which is story context-based audio without any linguistic information. For this purpose, we introduce a new dataset, called "Sound of Story (SoS)", which has paired image and text sequences with corresponding sound or background music for a story. To the best of our knowledge, this is the largest well-curated dataset for storytelling with sound. Our SoS dataset consists of 27,354 stories with 19.6 images per story and 984 hours of speech-decoupled audio such as background music and other sounds. As benchmark tasks for storytelling with sound and the dataset, we propose retrieval tasks between modalities, and audio generation tasks from image-text sequences, introducing strong baselines for them. We believe the proposed dataset and tasks may shed light on the multi-modal understanding of storytelling in terms of sound. Downloading the dataset and baseline codes for each task will be released in the link: https://github.com/Sosdatasets/SoS_Dataset.
</details>
<details>
<summary>摘要</summary>
Storytelling 是多Modal 的在现实世界中。当一个人 tel 一个故事时，可能使用所有的视觉和声音来传达故事的意义。然而，在storytelling 数据集和任务中，尽管声音也传达了故事的 semantics，但是之前的研究却很少关注声音。因此，我们提议通过 Adding 一个新的组件 called "background sound"，来扩展故事理解和 tel 的领域。为此，我们引入了一个新的数据集，called "Sound of Story (SoS)"，该数据集包含 27,354 个故事，每个故事有 19.6 个图像和 984 小时的speech-decoupled 声音，如背景音乐和其他声音。我们认为这是最大的、最好的纪录的故事tel 数据集。我们的 SoS 数据集包括以下任务： between modalities 的 Retrieval 任务和 image-text 序列的 audio 生成任务，我们提出了强大的基线。我们认为这些任务和数据集可能为 storytelling 中声音的多Modal 理解提供新的灵感。下载数据集和基线代码可以通过以下链接下载：https://github.com/Sosdatasets/SoS_Dataset。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.SD_2023_10_30/" data-id="clohum9cw00xepj88dm1o0kuz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/eess.AS_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T14:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/eess.AS_2023_10_30/">eess.AS - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Scenario-Aware-Audio-Visual-TF-GridNet-for-Target-Speech-Extraction"><a href="#Scenario-Aware-Audio-Visual-TF-GridNet-for-Target-Speech-Extraction" class="headerlink" title="Scenario-Aware Audio-Visual TF-GridNet for Target Speech Extraction"></a>Scenario-Aware Audio-Visual TF-GridNet for Target Speech Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19644">http://arxiv.org/abs/2310.19644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexu Pan, Gordon Wichern, Yoshiki Masuyama, Francois G. Germain, Sameer Khurana, Chiori Hori, Jonathan Le Roux</li>
<li>for: 本研究旨在提高speech separation的精度和效率，使得在干扰声音或竞争说话人的情况下，可以更好地提取target speech信号。</li>
<li>methods: 本研究基于TF-GridNet模型，通过在抽象过程中添加视频记录的面部信息，使得模型可以更好地了解target speaker的面部特征，从而提高speech separation的精度。此外，我们还提出了场景意识模型SAV-GridNet，可以根据干扰场景的不同，选择合适的专家模型进行处理。</li>
<li>results: 我们的模型在COG-MHEAR Audio-Visual Speech Enhancement Challenge中的第二届挑战中达到了state-of-the-art的成绩，与其他模型相比，具有显著的优势。我们还进行了广泛的分析结果，并对两种场景进行了详细的比较。<details>
<summary>Abstract</summary>
Target speech extraction aims to extract, based on a given conditioning cue, a target speech signal that is corrupted by interfering sources, such as noise or competing speakers. Building upon the achievements of the state-of-the-art (SOTA) time-frequency speaker separation model TF-GridNet, we propose AV-GridNet, a visual-grounded variant that incorporates the face recording of a target speaker as a conditioning factor during the extraction process. Recognizing the inherent dissimilarities between speech and noise signals as interfering sources, we also propose SAV-GridNet, a scenario-aware model that identifies the type of interfering scenario first and then applies a dedicated expert model trained specifically for that scenario. Our proposed model achieves SOTA results on the second COG-MHEAR Audio-Visual Speech Enhancement Challenge, outperforming other models by a significant margin, objectively and in a listening test. We also perform an extensive analysis of the results under the two scenarios.
</details>
<details>
<summary>摘要</summary>
Target speech extraction aims to extract, based on a given conditioning cue, a target speech signal that is corrupted by interfering sources, such as noise or competing speakers. Building upon the achievements of the state-of-the-art (SOTA) time-frequency speaker separation model TF-GridNet, we propose AV-GridNet, a visual-grounded variant that incorporates the face recording of a target speaker as a conditioning factor during the extraction process. Recognizing the inherent dissimilarities between speech and noise signals as interfering sources, we also propose SAV-GridNet, a scenario-aware model that identifies the type of interfering scenario first and then applies a dedicated expert model trained specifically for that scenario. Our proposed model achieves SOTA results on the second COG-MHEAR Audio-Visual Speech Enhancement Challenge, outperforming other models by a significant margin, objectively and in a listening test. We also perform an extensive analysis of the results under the two scenarios.Here's the translation in Traditional Chinese:target speech extraction aims to extract, based on a given conditioning cue, a target speech signal that is corrupted by interfering sources, such as noise or competing speakers. Building upon the achievements of the state-of-the-art (SOTA) time-frequency speaker separation model TF-GridNet, we propose AV-GridNet, a visual-grounded variant that incorporates the face recording of a target speaker as a conditioning factor during the extraction process. Recognizing the inherent dissimilarities between speech and noise signals as interfering sources, we also propose SAV-GridNet, a scenario-aware model that identifies the type of interfering scenario first and then applies a dedicated expert model trained specifically for that scenario. Our proposed model achieves SOTA results on the second COG-MHEAR Audio-Visual Speech Enhancement Challenge, outperforming other models by a significant margin, objectively and in a listening test. We also perform an extensive analysis of the results under the two scenarios.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/eess.AS_2023_10_30/" data-id="clohum9ea010vpj88fzlefozp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.CV_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T13:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/cs.CV_2023_10_30/">cs.CV - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Facial-asymmetry-A-Computer-Vision-based-behaviometric-index-for-assessment-during-a-face-to-face-interview"><a href="#Facial-asymmetry-A-Computer-Vision-based-behaviometric-index-for-assessment-during-a-face-to-face-interview" class="headerlink" title="Facial asymmetry: A Computer Vision based behaviometric index for assessment during a face-to-face interview"></a>Facial asymmetry: A Computer Vision based behaviometric index for assessment during a face-to-face interview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20083">http://arxiv.org/abs/2310.20083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuvam Keshari, Tanusree Dutta, Raju Mullick, Ashish Rathor, Priyadarshi Patnaik</li>
<li>for: 这个研究旨在帮助选择过程中选择合适的人员，使用 behaviometry 来评估面试者的行为特征，以提高面试过程的 объектив性和准确性。</li>
<li>methods: 本研究使用了 computer vision 技术和 open-source 库 (python-opencv 和 dlib)，将面孔不对称和微表情分析为主要的评估方法。</li>
<li>results: 研究发现，通过分析面孔不对称和微表情，可以获得不受评估员的偏见和社会可能性影响的不足信息，帮助选择过程更加精确和公平。<details>
<summary>Abstract</summary>
Choosing the right person for the right job makes the personnel interview process a cognitively demanding task. Psychometric tests, followed by an interview, have often been used to aid the process although such mechanisms have their limitations. While psychometric tests suffer from faking or social desirability of responses, the interview process depends on the way the responses are analyzed by the interviewers. We propose the use of behaviometry as an assistive tool to facilitate an objective assessment of the interviewee without increasing the cognitive load of the interviewer. Behaviometry is a relatively little explored field of study in the selection process, that utilizes inimitable behavioral characteristics like facial expressions, vocalization patterns, pupillary reactions, proximal behavior, body language, etc. The method analyzes thin slices of behavior and provides unbiased information about the interviewee. The current study proposes the methodology behind this tool to capture facial expressions, in terms of facial asymmetry and micro-expressions. Hemi-facial composites using a structural similarity index was used to develop a progressive time graph of facial asymmetry, as a test case. A frame-by-frame analysis was performed on three YouTube video samples, where Structural similarity index (SSID) scores of 75% and more showed behavioral congruence. The research utilizes open-source computer vision algorithms and libraries (python-opencv and dlib) to formulate the procedure for analysis of the facial asymmetry.
</details>
<details>
<summary>摘要</summary>
选择合适的人 для合适的工作是人员面试过程中的认知具有挑战性的任务。尽管心理测试和面试都有其限制，但我们提议使用行为测量为面试过程中的辅助工具，以帮助对面试者进行 объектив的评估，不会增加面试者的认知负担。行为测量是一个相对未探索的领域，它利用独特的行为特征，如脸部表现、嗓音特征、眼动反应、躯体语言等，来对面试者进行 объектив的评估。这种方法可以分析面试者的短时间行为，提供不受偏见的信息。本研究使用开源计算机视觉算法和库（python-opencv和dlib）来制定对脸部不均的分析的过程。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="LinFlo-Net-A-two-stage-deep-learning-method-to-generate-simulation-ready-meshes-of-the-heart"><a href="#LinFlo-Net-A-two-stage-deep-learning-method-to-generate-simulation-ready-meshes-of-the-heart" class="headerlink" title="LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart"></a>LinFlo-Net: A two-stage deep learning method to generate simulation ready meshes of the heart</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20065">http://arxiv.org/abs/2310.20065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Narayanan, Fanwei Kong, Shawn Shadden</li>
<li>for:  automatic generation of computer models of the human heart from patient imaging data</li>
<li>methods:  two-stage diffeomorphic deformation process with a novel loss function to minimize mesh self-penetration</li>
<li>results:  meshes free of self-intersections, comparable accuracy with state-of-the-art methods, and ready for use in physics-based simulation without post-processing.Here’s the full translation in Simplified Chinese:</li>
<li>for:  automatic generation of computer models of the human heart from patient imaging data</li>
<li>methods:  two-stage diffeomorphic deformation process with a novel loss function to minimize mesh self-penetration</li>
<li>results:  meshes free of self-intersections, comparable accuracy with state-of-the-art methods, and ready for use in physics-based simulation without post-processing.<details>
<summary>Abstract</summary>
We present a deep learning model to automatically generate computer models of the human heart from patient imaging data with an emphasis on its capability to generate thin-walled cardiac structures. Our method works by deforming a template mesh to fit the cardiac structures to the given image. Compared with prior deep learning methods that adopted this approach, our framework is designed to minimize mesh self-penetration, which typically arises when deforming surface meshes separated by small distances. We achieve this by using a two-stage diffeomorphic deformation process along with a novel loss function derived from the kinematics of motion that penalizes surface contact and interpenetration. Our model demonstrates comparable accuracy with state-of-the-art methods while additionally producing meshes free of self-intersections. The resultant meshes are readily usable in physics based simulation, minimizing the need for post-processing and cleanup.
</details>
<details>
<summary>摘要</summary>
我们提出了一种深度学习模型，可自动生成人体心脏的计算模型，从患者成像数据中提取心脏结构。我们的方法是通过将模板网格调整到图像中心脏结构的位置，以实现这一目的。相比之前的深度学习方法，我们的框架设计了减少网格自交相互穿梭的功能，通过两个阶段的 diffeomorphic 变换过程，同时使用一种新的损失函数，该函数基于运动dinamics的kinematics来penalize表面 contacts和interpenetration。我们的模型可以与现状技术相比，同时生成自交相互穿梭的网格。得到的网格可以 direct用于基于物理学的模拟，减少后期处理和清洁工作。
</details></li>
</ul>
<hr>
<h2 id="A-Scalable-Training-Strategy-for-Blind-Multi-Distribution-Noise-Removal"><a href="#A-Scalable-Training-Strategy-for-Blind-Multi-Distribution-Noise-Removal" class="headerlink" title="A Scalable Training Strategy for Blind Multi-Distribution Noise Removal"></a>A Scalable Training Strategy for Blind Multi-Distribution Noise Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20064">http://arxiv.org/abs/2310.20064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Zhang, Sakshum Kulshrestha, Christopher Metzler</li>
<li>for: 提出了一种基于adaptive-sampling&#x2F;active-learning的 universal denoising network训练策略，以提高现有的denoising network训练效率和性能。</li>
<li>methods: 使用一种基于波动函数的参数空间拟合方法，以减少训练时间并提高网络性能。</li>
<li>results: 通过对合理的训练数据进行适应性 sampling和活动学习，实现了一个灵活的 universal denoiser network，可以在各种操作条件下达到特циализированdenoiser network的最佳性能。<details>
<summary>Abstract</summary>
Despite recent advances, developing general-purpose universal denoising and artifact-removal networks remains largely an open problem: Given fixed network weights, one inherently trades-off specialization at one task (e.g.,~removing Poisson noise) for performance at another (e.g.,~removing speckle noise). In addition, training such a network is challenging due to the curse of dimensionality: As one increases the dimensions of the specification-space (i.e.,~the number of parameters needed to describe the noise distribution) the number of unique specifications one needs to train for grows exponentially. Uniformly sampling this space will result in a network that does well at very challenging problem specifications but poorly at easy problem specifications, where even large errors will have a small effect on the overall mean squared error.   In this work we propose training denoising networks using an adaptive-sampling/active-learning strategy. Our work improves upon a recently proposed universal denoiser training strategy by extending these results to higher dimensions and by incorporating a polynomial approximation of the true specification-loss landscape. This approximation allows us to reduce training times by almost two orders of magnitude. We test our method on simulated joint Poisson-Gaussian-Speckle noise and demonstrate that with our proposed training strategy, a single blind, generalist denoiser network can achieve peak signal-to-noise ratios within a uniform bound of specialized denoiser networks across a large range of operating conditions. We also capture a small dataset of images with varying amounts of joint Poisson-Gaussian-Speckle noise and demonstrate that a universal denoiser trained using our adaptive-sampling strategy outperforms uniformly trained baselines.
</details>
<details>
<summary>摘要</summary>
尽管最近有了进步，开发通用的锈除和遗传物理损订网络仍然是一个大多数未解决的问题：给定固定网络重量，一会 naturally trades-off特殊化在一个任务（例如，~除掉Poisson锈）的性能与另一个任务（例如，~除掉斑点锈）的性能之间。此外，在这种网络上进行训练也是一个挑战，因为维度的诅咒：随着特征空间的维度（即需要描述噪声分布的参数数量）的增加，需要训练的特殊化数量会加 exponential。对于这些特殊化进行均匀采样会导致一个网络能够处理非常困难的特定任务，但是对于容易处理的任务，即使大错也将具有小影响于总平均方差。在这项工作中，我们提议使用可适应采样/活动学习策略来训练锈除网络。我们的工作超越了最近提出的通用锈除训练策略，并在更高的维度上进行扩展。此外，我们还利用幂等函数来近似真实的规范损失景观，以降低训练时间。我们在模拟的 JOINT Poisson-Gaussian-Speckle 噪声下测试了我们的方法，并证明了一个盲目、通用的锈除网络可以在各种操作条件下达到特殊化锈除网络的峰信号强度上限。此外，我们还 capture了一个小型图像库，包含不同量的 JOINT Poisson-Gaussian-Speckle 噪声，并证明了一个通用锈除网络，使用我们的适应采样策略训练后，可以超越固定采样的基eline。
</details></li>
</ul>
<hr>
<h2 id="SolarFormer-Multi-scale-Transformer-for-Solar-PV-Profiling"><a href="#SolarFormer-Multi-scale-Transformer-for-Solar-PV-Profiling" class="headerlink" title="SolarFormer: Multi-scale Transformer for Solar PV Profiling"></a>SolarFormer: Multi-scale Transformer for Solar PV Profiling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20057">http://arxiv.org/abs/2310.20057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian de Luis, Minh Tran, Taisei Hanyu, Anh Tran, Liao Haitao, Roy McCann, Alan Mantooth, Ying Huang, Ngan Le</li>
<li>for: 这篇论文是为了探讨实时太阳能板的映射和分类，以便更好地理解太阳能板的采用和实施。</li>
<li>methods: 这篇论文使用了SolarFormer模型，该模型包括多层TransformerEncoder和掩盖式TransformerDecoder，并且具有实例问题机制以提高太阳能板的本地化。</li>
<li>results: 根据多个测试数据集，包括法国GGE、法国IGN和美国加州USGS，这篇论文的SolarFormer模型能够与现有的模型比较或超越，表明了优化太阳能板的映射和分类。<details>
<summary>Abstract</summary>
As climate change intensifies, the global imperative to shift towards sustainable energy sources becomes more pronounced. Photovoltaic (PV) energy is a favored choice due to its reliability and ease of installation. Accurate mapping of PV installations is crucial for understanding their adoption and informing energy policy. To meet this need, we introduce the SolarFormer, designed to segment solar panels from aerial imagery, offering insights into their location and size. However, solar panel identification in Computer Vision is intricate due to various factors like weather conditions, roof conditions, and Ground Sampling Distance (GSD) variations. To tackle these complexities, we present the SolarFormer, featuring a multi-scale Transformer encoder and a masked-attention Transformer decoder. Our model leverages low-level features and incorporates an instance query mechanism to enhance the localization of solar PV installations. We rigorously evaluated our SolarFormer using diverse datasets, including GGE (France), IGN (France), and USGS (California, USA), across different GSDs. Our extensive experiments consistently demonstrate that our model either matches or surpasses state-of-the-art models, promising enhanced solar panel segmentation for global sustainable energy initiatives.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Radiomics-as-a-measure-superior-to-the-Dice-similarity-coefficient-for-tumor-segmentation-performance-evaluation"><a href="#Radiomics-as-a-measure-superior-to-the-Dice-similarity-coefficient-for-tumor-segmentation-performance-evaluation" class="headerlink" title="Radiomics as a measure superior to the Dice similarity coefficient for tumor segmentation performance evaluation"></a>Radiomics as a measure superior to the Dice similarity coefficient for tumor segmentation performance evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20039">http://arxiv.org/abs/2310.20039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoichi Watanabe, Rukhsora Akramova<br>for: This study aims to evaluate the segmentation ability of physicians and auto-segmentation tools in high-quality radiotherapy delivery by using Radiomics features as a superior measure compared to the widely used Dice Similarity Coefficient (DSC).methods: The research involves selecting reproducible radiomics features for evaluating segmentation accuracy by analyzing radiomics data from 2 CT scans of 10 lung tumors, and using CT images from 10 patients, each segmented by different physicians or auto-segmentation tools, to assess segmentation performance.results: The study reveals 206 radiomics features with a Concordance Correlation Coefficient (CCC) greater than 0.93 between the two CT images, indicating robust reproducibility. Seven features exhibit low Intraclass Correlation Coefficients (ICC), signifying increased sensitivity to segmentation differences. The findings suggest that Radiomics features, particularly those related to shape and energy, can capture subtle variations in tumor segmentation characteristics, unlike DSC.<details>
<summary>Abstract</summary>
In high-quality radiotherapy delivery, precise segmentation of targets and healthy structures is essential. This study proposes Radiomics features as a superior measure for assessing the segmentation ability of physicians and auto-segmentation tools, in comparison to the widely used Dice Similarity Coefficient (DSC). The research involves selecting reproducible radiomics features for evaluating segmentation accuracy by analyzing radiomics data from 2 CT scans of 10 lung tumors, available in the RIDER Data Library. Radiomics features were extracted using PyRadiomics, with selection based on the Concordance Correlation Coefficient (CCC). Subsequently, CT images from 10 patients, each segmented by different physicians or auto-segmentation tools, were used to assess segmentation performance. The study reveals 206 radiomics features with a CCC greater than 0.93 between the two CT images, indicating robust reproducibility. Among these features, seven exhibit low Intraclass Correlation Coefficients (ICC), signifying increased sensitivity to segmentation differences. Notably, ICCs of original shape features, including sphericity, elongation, and flatness, ranged from 0.1177 to 0.995. In contrast, all DSC values exceeded 0.778. This research demonstrates that radiomics features, particularly those related to shape and energy, can capture subtle variations in tumor segmentation characteristics, unlike DSC. As a result, Radiomics features with ICC prove superior for evaluating a physician's tumor segmentation ability and the performance of auto-segmentation tools. The findings suggest that these new metrics can be employed to assess novel auto-segmentation methods and enhance the training of individuals in medical segmentation, thus contributing to improved radiotherapy practices.
</details>
<details>
<summary>摘要</summary>
高品质放疗需要精准地分割目标和健康结构。本研究提出使用 радиом特征来评估医生和自动分割工具的分割精度，而不是常用的 dice相似度指标（DSC）。研究选择了可重复的 радиом特征来评估分割精度，通过分析20个lung tumor的CT扫描图，从RIDER数据库中获得的 radiomics数据。 радиом特征使用PyRadiomics提取，基于协调相似度指标（CCC）进行选择。然后，从10名患者的CT图像中，每名患者由不同的医生或自动分割工具进行分割，以评估分割性能。研究发现有206个 радиом特征，其CCC值大于0.93，表示robust可重复性。其中，7个特征 display low intraclass correlation coefficients（ICC）， indicating increased sensitivity to segmentation differences。尤其是原始形状特征，包括圆形度、强度和平坦度，ICC值分别为0.1177-0.995。与此相比，所有DSC值都大于0.778。这项研究表明， радиом特征，特别是与形状和能量相关的特征，可以捕捉到细微的分割特征变化，与DSC不同。因此，Radiomics特征，特别是ICC高的特征，可以更好地评估医生的肿瘤分割能力和自动分割工具的性能。这些新指标可以用来评估新的自动分割方法和提高医学分割训练，从而对放疗实践产生贡献。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Anchor-Label-Propagation-for-Transductive-Few-Shot-Learning"><a href="#Adaptive-Anchor-Label-Propagation-for-Transductive-Few-Shot-Learning" class="headerlink" title="Adaptive Anchor Label Propagation for Transductive Few-Shot Learning"></a>Adaptive Anchor Label Propagation for Transductive Few-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19996">http://arxiv.org/abs/2310.19996</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michalislazarou/a2lp">https://github.com/michalislazarou/a2lp</a></li>
<li>paper_authors: Michalis Lazarou, Yannis Avrithis, Guangyu Ren, Tania Stathaki</li>
<li>for: 提高几个shot学习的性能，使用有限量的标注数据</li>
<li>methods: 使用泛化推理方法，如标签传播，将无标注数据作为支持向量进行推理</li>
<li>results: 提出了一种新的 Adaptive Anchor Label Propagation 算法，在 1-shot 和 5-shot 设置下，与标准标签传播算法比较，提高了7% 和 2% 的性能。实验结果表明，我们的算法在四个常用的几个shot benchmark数据集（miniImageNet、tieredImageNet、CUB 和 CIFAR-FS）上表现出色，并且可以与两种常用的干部网络（ResNet12 和 WideResNet-28-10）结合使用。代码可以在 GitHub 上找到。<details>
<summary>Abstract</summary>
Few-shot learning addresses the issue of classifying images using limited labeled data. Exploiting unlabeled data through the use of transductive inference methods such as label propagation has been shown to improve the performance of few-shot learning significantly. Label propagation infers pseudo-labels for unlabeled data by utilizing a constructed graph that exploits the underlying manifold structure of the data. However, a limitation of the existing label propagation approaches is that the positions of all data points are fixed and might be sub-optimal so that the algorithm is not as effective as possible. In this work, we propose a novel algorithm that adapts the feature embeddings of the labeled data by minimizing a differentiable loss function optimizing their positions in the manifold in the process. Our novel algorithm, Adaptive Anchor Label Propagation}, outperforms the standard label propagation algorithm by as much as 7% and 2% in the 1-shot and 5-shot settings respectively. We provide experimental results highlighting the merits of our algorithm on four widely used few-shot benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS and two commonly used backbones, ResNet12 and WideResNet-28-10. The source code can be found at https://github.com/MichalisLazarou/A2LP.
</details>
<details>
<summary>摘要</summary>
几个shot学习解决了使用有限的标注数据来分类图像的问题。通过使用推导式推理方法，如标签卷推理，可以在几个shot学习中显著提高性能。标签卷推理在未标注数据上推断 pseudo-标签，利用建立的图像数据下的潜在拓扑结构。然而，现有的标签卷推理方法的一个局限性是所有数据点的位置固定，可能不是最佳的，因此算法效果不是最好。在这种情况下，我们提出了一种新的算法，可以适应标注数据的特征表示进行最佳化。我们称之为 Adaptive Anchor Label Propagation（A2LP）。A2LP算法在1-shot和5-shot设置中分别超过标准标签卷推理算法的7%和2%。我们在四个广泛使用的几个shot benchmark数据集（miniImageNet、tieredImageNet、CUB和CIFAR-FS）和两种常用的后向推理器（ResNet12和WideResNet-28-10）上进行了实验，并提供了相关的实验结果。代码可以在https://github.com/MichalisLazarou/A2LP中找到。
</details></li>
</ul>
<hr>
<h2 id="Emotional-Theory-of-Mind-Bridging-Fast-Visual-Processing-with-Slow-Linguistic-Reasoning"><a href="#Emotional-Theory-of-Mind-Bridging-Fast-Visual-Processing-with-Slow-Linguistic-Reasoning" class="headerlink" title="Emotional Theory of Mind: Bridging Fast Visual Processing with Slow Linguistic Reasoning"></a>Emotional Theory of Mind: Bridging Fast Visual Processing with Slow Linguistic Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19995">http://arxiv.org/abs/2310.19995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasaman Etesam, Ozge Nilay Yalcin, Chuxuan Zhang, Angelica Lim</li>
<li>for: 这个研究的目的是评估最近的大视语言模型（CLIP、LLaVA）和大语言模型（GPT-3.5）中嵌入的情感常识，以及在情感上下文中的表达。</li>
<li>methods: 作者使用了“narative captions”来描述情感表达，并使用了872种物理社交信号描述和224个情感相关的环境 labels。</li>
<li>results: 对于图像-语言-情感任务， combining “fast”和”slow”的理解方法可以提高情感认知系统的性能。然而，遗留了在零学习情感理论心理任务中的差距，相比于之前在EMOTIC dataset上进行的训练。<details>
<summary>Abstract</summary>
The emotional theory of mind problem in images is an emotion recognition task, specifically asking "How does the person in the bounding box feel?" Facial expressions, body pose, contextual information and implicit commonsense knowledge all contribute to the difficulty of the task, making this task currently one of the hardest problems in affective computing. The goal of this work is to evaluate the emotional commonsense knowledge embedded in recent large vision language models (CLIP, LLaVA) and large language models (GPT-3.5) on the Emotions in Context (EMOTIC) dataset. In order to evaluate a purely text-based language model on images, we construct "narrative captions" relevant to emotion perception, using a set of 872 physical social signal descriptions related to 26 emotional categories, along with 224 labels for emotionally salient environmental contexts, sourced from writer's guides for character expressions and settings. We evaluate the use of the resulting captions in an image-to-language-to-emotion task. Experiments using zero-shot vision-language models on EMOTIC show that combining "fast" and "slow" reasoning is a promising way forward to improve emotion recognition systems. Nevertheless, a gap remains in the zero-shot emotional theory of mind task compared to prior work trained on the EMOTIC dataset.
</details>
<details>
<summary>摘要</summary>
“情感理论心理问题在图像中是一种情感识别任务，具体来说是问“图像中人员在盒子中如何感到？” facial expressions、body pose、contextual information和隐性常识都会对这个任务带来挑战，使得这个任务成为当前情感计算领域中最Difficult Problem。本工作的目标是评估最近的大视语言模型（CLIP、LLaVA）和大语言模型（GPT-3.5）中的情感常识在EMOTIC数据集上的表现。为了评估一个纯文本基于的语言模型在图像上，我们构建了“narative captions”相关于情感感知，使用了872种物理社交信号描述和26种情感类别，以及224个用于描述情感相关的环境Label。我们在图像-语言-情感任务中使用了这些caption进行评估。针对EMOTIC数据集，我们发现，结合“快”和“慢”理解是一种有前途的方法，可以改进情感识别系统。然而，在零shot情感理论心理任务中，与先前工作相比，还存在一定的差距。”
</details></li>
</ul>
<hr>
<h2 id="Addressing-Weak-Decision-Boundaries-in-Image-Classification-by-Leveraging-Web-Search-and-Generative-Models"><a href="#Addressing-Weak-Decision-Boundaries-in-Image-Classification-by-Leveraging-Web-Search-and-Generative-Models" class="headerlink" title="Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models"></a>Addressing Weak Decision Boundaries in Image Classification by Leveraging Web Search and Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19986">http://arxiv.org/abs/2310.19986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Preetam Prabhu Srikar Dammu, Yunhe Feng, Chirag Shah</li>
<li>for: This paper aims to address the issue of bias and discrimination in machine learning (ML) models, specifically in the context of image classification.</li>
<li>methods: The proposed approach leverages the power of web search and generative models to enhance robustness and mitigate bias in ML models. The method involves identifying weak decision boundaries for vulnerable populations, constructing search queries for Google, and generating new training samples through DALL-E 2 and Stable Diffusion.</li>
<li>results: The proposed method achieved a significant reduction (77.30%) in the model’s gender accuracy disparity, and improved the classifier’s decision boundary with fewer weakspots and increased separation between classes.Here are the three points in Simplified Chinese text:</li>
<li>for: 本研究旨在解决机器学习（ML）模型中的偏见和歧视问题，具体来说是在图像分类任务中。</li>
<li>methods: 提议的方法利用网络搜索和生成模型来增强ML模型的稳定性和减少偏见，包括识别投降群体的弱点决策边界，构建Google搜索查询，通过DALL-E 2和稳定扩散生成新的训练样本。</li>
<li>results: 提议的方法在人类肖像分类问题中实现了显著减少（77.30%）的性别准确率差距，并改善分类器的决策边界，具有更少的弱点和类别之间更大的分离。<details>
<summary>Abstract</summary>
Machine learning (ML) technologies are known to be riddled with ethical and operational problems, however, we are witnessing an increasing thrust by businesses to deploy them in sensitive applications. One major issue among many is that ML models do not perform equally well for underrepresented groups. This puts vulnerable populations in an even disadvantaged and unfavorable position. We propose an approach that leverages the power of web search and generative models to alleviate some of the shortcomings of discriminative models. We demonstrate our method on an image classification problem using ImageNet's People Subtree subset, and show that it is effective in enhancing robustness and mitigating bias in certain classes that represent vulnerable populations (e.g., female doctor of color). Our new method is able to (1) identify weak decision boundaries for such classes; (2) construct search queries for Google as well as text for generating images through DALL-E 2 and Stable Diffusion; and (3) show how these newly captured training samples could alleviate population bias issue. While still improving the model's overall performance considerably, we achieve a significant reduction (77.30\%) in the model's gender accuracy disparity. In addition to these improvements, we observed a notable enhancement in the classifier's decision boundary, as it is characterized by fewer weakspots and an increased separation between classes. Although we showcase our method on vulnerable populations in this study, the proposed technique is extendable to a wide range of problems and domains.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）技术具有许多道理和运营问题，但是企业在敏感应用中广泛使用。一个主要问题是ML模型在弱 represented 群体中不具有相同的性能。这会让投降的人口处于更加不利和不利的位置。我们提议一种使用网络搜索和生成模型来缓解歧视模型的缺陷。我们在图像分类问题中使用图像网络的人类树subset，并证明我们的方法可以提高鲁棒性和减少偏见在某些类型中，例如女性黑人医生。我们的新方法可以（1）标识这些类型的弱点决策界；（2）生成Google搜索和生成图像的文本；以及（3）这些新采集的训练样本可以减少人群偏见问题。虽然我们仍然提高模型的总性能，但我们 Achieve a significant reduction (77.30%) in the model's gender accuracy disparity。此外，我们发现了这些改进后的决策界更加稳定，具有更少的弱点和类型之间更加增加的分化。虽然在本研究中我们使用易受护理的人口，但我们提议的技术可以应用于各种问题和领域。
</details></li>
</ul>
<hr>
<h2 id="‘Person’-Light-skinned-Western-Man-and-Sexualization-of-Women-of-Color-Stereotypes-in-Stable-Diffusion"><a href="#‘Person’-Light-skinned-Western-Man-and-Sexualization-of-Women-of-Color-Stereotypes-in-Stable-Diffusion" class="headerlink" title="‘Person’ &#x3D;&#x3D; Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion"></a>‘Person’ &#x3D;&#x3D; Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19981">http://arxiv.org/abs/2310.19981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sourojit Ghosh, Aylin Caliskan</li>
<li>for: The paper examines the stereotypes embedded in Stable Diffusion, a popular text-to-image generator, and how it assigns gender and nationality&#x2F;continental identity to individuals based on the absence of information.</li>
<li>methods: The paper uses vision-language model CLIP’s cosine similarity to compare images generated by CLIP-based Stable Diffusion v2.1 and manual examination to chronicle the results of front-facing images of persons from different continents, nationalities, and genders.</li>
<li>results: The paper finds that Stable Diffusion outputs of “a person” without any additional gender&#x2F;nationality information correspond closest to images of men and least with persons of nonbinary gender, and that the output images are more likely to be of European&#x2F;North American men rather than women or nonbinary individuals. The paper also observes continental stereotypes and the harmful erasure of Indigenous Oceanic peoples, as well as the oversexualization of women, specifically Latin American, Mexican, Indian, and Egyptian women.<details>
<summary>Abstract</summary>
We study stereotypes embedded within one of the most popular text-to-image generators: Stable Diffusion. We examine what stereotypes of gender and nationality/continental identity does Stable Diffusion display in the absence of such information i.e. what gender and nationality/continental identity is assigned to `a person', or to `a person from Asia'. Using vision-language model CLIP's cosine similarity to compare images generated by CLIP-based Stable Diffusion v2.1 verified by manual examination, we chronicle results from 136 prompts (50 results/prompt) of front-facing images of persons from 6 different continents, 27 nationalities and 3 genders. We observe how Stable Diffusion outputs of `a person' without any additional gender/nationality information correspond closest to images of men and least with persons of nonbinary gender, and to persons from Europe/North America over Africa/Asia, pointing towards Stable Diffusion having a concerning representation of personhood to be a European/North American man. We also show continental stereotypes and resultant harms e.g. a person from Oceania is deemed to be Australian/New Zealander over Papua New Guinean, pointing to the erasure of Indigenous Oceanic peoples, who form a majority over descendants of colonizers both in Papua New Guinea and in Oceania overall. Finally, we unexpectedly observe a pattern of oversexualization of women, specifically Latin American, Mexican, Indian and Egyptian women relative to other nationalities, measured through an NSFW detector. This demonstrates how Stable Diffusion perpetuates Western fetishization of women of color through objectification in media, which if left unchecked will amplify this stereotypical representation. Image datasets are made publicly available.
</details>
<details>
<summary>摘要</summary>
我们研究在普遍用于文本到图像生成器中嵌入的标准刻板：稳定扩散。我们研究这些刻板在没有任何信息时如何表现出的性别和国籍/大陆认同，例如，在没有任何信息时，怎样对“一个人”进行识别，或者对“一个来自亚洲的人”进行识别。我们使用 CLIP 的 cosine 相似性来比较由 CLIP 基于的 Stable Diffusion v2.1 生成的图像，并通过手动检查，对 136 个提示（每个提示 50 个结果）进行了 chronicle 记录，这些提示包括来自 6 个大陆、27 个国家和 3 个性别的人像。我们发现在没有任何信息时，Stable Diffusion 输出的“一个人”最接近男性图像，并且与非binary 性别最少相关，而且更多地对应于欧洲/北美地区。此外，我们发现 Stable Diffusion 对人类形象的表现存在严重问题，例如，它认为澳大利亚/新西兰人是大洋洲人，而不是 Papua New Guinea 人，这种做法导致了原住民澳大利亚/新西兰人的抹杀，这些人占澳大利亚/新西兰人口的多数。此外，我们还发现，在某些国家女性图像中存在过度性化现象，例如拉丁美洲、墨西哥、印度和埃及女性图像，这种现象是由西方对女性色彩化的objectification在媒体中所带来的。如果不加以检查，这种刻板的表现将被加剧。我们将图像数据公共发布。
</details></li>
</ul>
<hr>
<h2 id="Battle-of-the-Backbones-A-Large-Scale-Comparison-of-Pretrained-Models-across-Computer-Vision-Tasks"><a href="#Battle-of-the-Backbones-A-Large-Scale-Comparison-of-Pretrained-Models-across-Computer-Vision-Tasks" class="headerlink" title="Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks"></a>Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19909">http://arxiv.org/abs/2310.19909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hsouri/battle-of-the-backbones">https://github.com/hsouri/battle-of-the-backbones</a></li>
<li>paper_authors: Micah Goldblum, Hossein Souri, Renkun Ni, Manli Shu, Viraj Prabhu, Gowthami Somepalli, Prithvijit Chattopadhyay, Mark Ibrahim, Adrien Bardes, Judy Hoffman, Rama Chellappa, Andrew Gordon Wilson, Tom Goldstein</li>
<li>for: 这个论文的目的是为computer vision系统提供一个多种预训练模型的比较，以帮助实践者更好地选择适合的模型。</li>
<li>methods: 这个论文使用了多种预训练模型，包括视力变换器（ViTs）和自我超vised学习（SSL），并对这些模型进行了多种任务的测试和分析。</li>
<li>results: 研究发现，使用大量的预训练数据和超vised学习方法可以实现高性能的计算视觉系统，而且这些模型在多种任务上表现强劲，特别是在类别预测和物体检测任务上。此外，研究还发现，使用同样的 arquitectures和预训练数据，SSL模型可以与超vised学习模型相比，表现很高水平。<details>
<summary>Abstract</summary>
Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating strengths and weakness of existing approaches through a comprehensive analysis conducted on more than 1500 training runs. While vision transformers (ViTs) and self-supervised learning (SSL) are increasingly popular, we find that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks among the models we consider. Moreover, in apples-to-apples comparisons on the same architectures and similarly sized pretraining datasets, we find that SSL backbones are highly competitive, indicating that future works should perform SSL pretraining with advanced architectures and larger pretraining datasets. We release the raw results of our experiments along with code that allows researchers to put their own backbones through the gauntlet here: https://github.com/hsouri/Battle-of-the-Backbones
</details>
<details>
<summary>摘要</summary>
神经网络基于计算机视觉系统通常建立在脊梁上，脊梁可以是预训练或随机初始化的特征提取器。过去几年，默认选择是使用ImageNet进行预训练的卷积神经网络。然而，最近几年，各种算法和数据集预训练的脊梁出现了 countless 。这种多样性使得各种系统表现得更好，但是对实践者来说做出 Informed 决策变得更加困难。“Backbone Battle”（BoB）使得这种选择变得更加容易，它对多种预训练模型进行了多种计算机视觉任务的比较，包括分类、物体检测和OOD泛化等。此外，BoB还为研究者们提供了推进计算机视觉的方向，通过对现有方法的分析和评估，揭示了现有方法的优劣。我们发现，使用大量训练集和超参数进行预训练的卷积神经网络仍然在大多数任务上表现最佳。此外，我们发现，使用同一类型的 arquitectures 和相同大小的预训练集，SSL 预训练的脊梁在同类任务上表现非常竞争力。因此，未来的工作应该使用更先进的架构和更大的预训练集进行SSL预训练。我们将 raw 的实验结果和相应的代码发布在 GitHub 上：<https://github.com/hsouri/Battle-of-the-Backbones>
</details></li>
</ul>
<hr>
<h2 id="MIST-Medical-Image-Segmentation-Transformer-with-Convolutional-Attention-Mixing-CAM-Decoder"><a href="#MIST-Medical-Image-Segmentation-Transformer-with-Convolutional-Attention-Mixing-CAM-Decoder" class="headerlink" title="MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder"></a>MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19898">http://arxiv.org/abs/2310.19898</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rahman-motiur/mist">https://github.com/rahman-motiur/mist</a></li>
<li>paper_authors: Md Motiur Rahman, Shiva Shokouhmand, Smriti Bhatt, Miad Faezipour</li>
<li>for: 这篇论文的目的是提出一种适合医疗影像分类的transformer模型，以解决traditional transformer模型在多modal维度中捕捉本地Context的问题。</li>
<li>methods: 这篇论文提出了一种叫做Medical Image Segmentation Transformer（MIST）的模型，它包括一个预训多轴条件对应（MaxViT）的енкодер，以及一个新的Convolutional Attention Mixing（CAM）解码器。CAM解码器使用多头自我注意、空间注意和压缩和刺激注意模组来捕捉所有空间维度中的长程相互作用。</li>
<li>results: 实验结果显示，我们的MIST模型与CAM解码器在ACDC和Synapse datasets上的分类性能比State-of-the-art模型高。此外，我们还证明了在不同的维度上对应的CAM解码器可以将低级和高级特征融合，以提高分类性能。<details>
<summary>Abstract</summary>
One of the common and promising deep learning approaches used for medical image segmentation is transformers, as they can capture long-range dependencies among the pixels by utilizing self-attention. Despite being successful in medical image segmentation, transformers face limitations in capturing local contexts of pixels in multimodal dimensions. We propose a Medical Image Segmentation Transformer (MIST) incorporating a novel Convolutional Attention Mixing (CAM) decoder to address this issue. MIST has two parts: a pre-trained multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded feature representation is passed through the CAM decoder for segmenting the images. In the CAM decoder, an attention-mixer combining multi-head self-attention, spatial attention, and squeeze and excitation attention modules is introduced to capture long-range dependencies in all spatial dimensions. Moreover, to enhance spatial information gain, deep and shallow convolutions are used for feature extraction and receptive field expansion, respectively. The integration of low-level and high-level features from different network stages is enabled by skip connections, allowing MIST to suppress unnecessary information. The experiments show that our MIST transformer with CAM decoder outperforms the state-of-the-art models specifically designed for medical image segmentation on the ACDC and Synapse datasets. Our results also demonstrate that adding the CAM decoder with a hierarchical transformer improves segmentation performance significantly. Our model with data and code is publicly available on GitHub.
</details>
<details>
<summary>摘要</summary>
一种常见且有前途的深度学习方法是转换器，它可以通过自我注意来捕捉图像像素之间的长距离依赖关系。 despite its success in医疗图像分割，转换器受到多modal维度中像素的本地上下文的限制。 we propose a Medical Image Segmentation Transformer (MIST) that incorporates a novel Convolutional Attention Mixing (CAM) decoder to address this issue. MIST consists of two parts: a pre-trained multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded feature representation is passed through the CAM decoder for segmenting the images. In the CAM decoder, an attention-mixer combining multi-head self-attention, spatial attention, and squeeze and excitation attention modules is introduced to capture long-range dependencies in all spatial dimensions. Moreover, to enhance spatial information gain, deep and shallow convolutions are used for feature extraction and receptive field expansion, respectively. The integration of low-level and high-level features from different network stages is enabled by skip connections, allowing MIST to suppress unnecessary information. The experiments show that our MIST transformer with CAM decoder outperforms the state-of-the-art models specifically designed for medical image segmentation on the ACDC and Synapse datasets. Our results also demonstrate that adding the CAM decoder with a hierarchical transformer improves segmentation performance significantly. Our model with data and code is publicly available on GitHub.Here's the translation in Traditional Chinese:一种常见且有前途的深度学习方法是转换器，它可以通过自我注意来捕捉图像像素之间的长距离依赖关系。 despite its success in医疗图像分割，转换器受到多modal维度中像素的本地上下文的限制。 we propose a Medical Image Segmentation Transformer (MIST) that incorporates a novel Convolutional Attention Mixing (CAM) decoder to address this issue. MIST consists of two parts: a pre-trained multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded feature representation is passed through the CAM decoder for segmenting the images. In the CAM decoder, an attention-mixer combining multi-head self-attention, spatial attention, and squeeze and excitation attention modules is introduced to capture long-range dependencies in all spatial dimensions. Moreover, to enhance spatial information gain, deep and shallow convolutions are used for feature extraction and receptive field expansion, respectively. The integration of low-level and high-level features from different network stages is enabled by skip connections, allowing MIST to suppress unnecessary information. The experiments show that our MIST transformer with CAM decoder outperforms the state-of-the-art models specifically designed for medical image segmentation on the ACDC and Synapse datasets. Our results also demonstrate that adding the CAM decoder with a hierarchical transformer improves segmentation performance significantly. Our model with data and code is publicly available on GitHub.
</details></li>
</ul>
<hr>
<h2 id="DiffEnc-Variational-Diffusion-with-a-Learned-Encoder"><a href="#DiffEnc-Variational-Diffusion-with-a-Learned-Encoder" class="headerlink" title="DiffEnc: Variational Diffusion with a Learned Encoder"></a>DiffEnc: Variational Diffusion with a Learned Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19789">http://arxiv.org/abs/2310.19789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beatrix M. G. Nielsen, Anders Christensen, Andrea Dittadi, Ole Winther</li>
<li>for: 这个论文的目的是提出一种基于扩散模型的变量自动机（VA），具有两个改进：在生成过程中共享参数，以及高效地计算损失为独立的层次结构。</li>
<li>methods: 论文提出了两种改进 diffusion model，首先是在扩散过程中添加了数据和深度依赖的均值函数，这导致了一种修改的扩散损失。</li>
<li>results: 论文的提出的DiffEnc框架在CIFAR-10上实现了状态计算机科学的最佳可能性。此外，论文还提出了一种基于权重的扩散损失方法，可以用于进行推理。<details>
<summary>Abstract</summary>
Diffusion models may be viewed as hierarchical variational autoencoders (VAEs) with two improvements: parameter sharing for the conditional distributions in the generative process and efficient computation of the loss as independent terms over the hierarchy. We consider two changes to the diffusion model that retain these advantages while adding flexibility to the model. Firstly, we introduce a data- and depth-dependent mean function in the diffusion process, which leads to a modified diffusion loss. Our proposed framework, DiffEnc, achieves state-of-the-art likelihood on CIFAR-10. Secondly, we let the ratio of the noise variance of the reverse encoder process and the generative process be a free weight parameter rather than being fixed to 1. This leads to theoretical insights: For a finite depth hierarchy, the evidence lower bound (ELBO) can be used as an objective for a weighted diffusion loss approach and for optimizing the noise schedule specifically for inference. For the infinite-depth hierarchy, on the other hand, the weight parameter has to be 1 to have a well-defined ELBO.
</details>
<details>
<summary>摘要</summary>
Diffusion models可以被看作为层次Variational Autoencoder（VAEs），具有两个改进：在生成过程中共享参数的条件分布，以及独立计算损失的 hierarchy。我们考虑了对 diffusion model 的两种改进，以提高模型的灵活性。首先，我们引入了基于数据和深度的均值函数，导致了修改的扩散损失。我们的提出的框架，DiffEnc，在 CIFAR-10 上达到了状态机器的可靠性。其次，我们允许反推采样过程中噪声方差的比率作为自由参数，而不是固定为1。这导致了理论上的发现：对于有限深度层次，可以使用抽象下界（ELBO）作为一个Weighted扩散损失的目标，并且可以优化噪声程度特别 для推理。而对于无穷深度层次，则weight参数必须为1，以便有一定的ELBO。
</details></li>
</ul>
<hr>
<h2 id="MM-VID-Advancing-Video-Understanding-with-GPT-4V-ision"><a href="#MM-VID-Advancing-Video-Understanding-with-GPT-4V-ision" class="headerlink" title="MM-VID: Advancing Video Understanding with GPT-4V(ision)"></a>MM-VID: Advancing Video Understanding with GPT-4V(ision)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19773">http://arxiv.org/abs/2310.19773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, Lijuan Wang</li>
<li>For: MM-VID is designed to facilitate advanced video understanding, particularly for long-form videos and intricate tasks such as reasoning within hour-long content and grasping storylines spanning multiple episodes.* Methods: MM-VID uses a video-to-script generation with GPT-4V to transcribe multimodal elements into a long textual script, enabling advanced capabilities such as audio description, character identification, and multimodal high-level comprehension.* Results: Experimental results demonstrate the effectiveness of MM-VID in handling distinct video genres with various video lengths, and its potential when applied to interactive environments such as video games and graphic user interfaces.Here’s the information in Simplified Chinese text:</li>
<li>for: MM-VID 是为了实现高级视频理解，特别是面对长视频和多集的复杂任务，如在一小时内进行理性和掌握多集剧情。</li>
<li>methods: MM-VID 使用 GPT-4V 将视频转换为长文本脚本，以便利用大语言模型（LLM）实现高级功能，如音频描述、人物识别和多模态高级理解。</li>
<li>results: 实验结果表明 MM-VID 可以处理不同类型的视频，并且在交互环境中如游戏和图形用户界面中表现出色。<details>
<summary>Abstract</summary>
We present MM-VID, an integrated system that harnesses the capabilities of GPT-4V, combined with specialized tools in vision, audio, and speech, to facilitate advanced video understanding. MM-VID is designed to address the challenges posed by long-form videos and intricate tasks such as reasoning within hour-long content and grasping storylines spanning multiple episodes. MM-VID uses a video-to-script generation with GPT-4V to transcribe multimodal elements into a long textual script. The generated script details character movements, actions, expressions, and dialogues, paving the way for large language models (LLMs) to achieve video understanding. This enables advanced capabilities, including audio description, character identification, and multimodal high-level comprehension. Experimental results demonstrate the effectiveness of MM-VID in handling distinct video genres with various video lengths. Additionally, we showcase its potential when applied to interactive environments, such as video games and graphic user interfaces.
</details>
<details>
<summary>摘要</summary>
我们介绍MM-VID，一个整合了GPT-4V的特有功能和视觉、音频和语音特化工具，以便实现高级视频理解。MM-VID是为解决长形视频和复杂任务（如在多集 episodic 内理解）而设计。它使用视频到脚本生成（GPT-4V）将多Modal元素转化为长文本脚本。生成的脚本包括人物运动、动作、表情和对话，使大语言模型（LLMs）可以实现视频理解。这使得可以实现高级功能，如音频描述、人物识别和多Modal高级理解。我们的实验结果表明MM-VID可以处理不同的视频类型和长度。此外，我们还展示了其在交互环境中的潜在应用，如视频游戏和图形用户界面。
</details></li>
</ul>
<hr>
<h2 id="Intra-Modal-Proxy-Learning-for-Zero-Shot-Visual-Categorization-with-CLIP"><a href="#Intra-Modal-Proxy-Learning-for-Zero-Shot-Visual-Categorization-with-CLIP" class="headerlink" title="Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP"></a>Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19752">http://arxiv.org/abs/2310.19752</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idstcv/inmap">https://github.com/idstcv/inmap</a></li>
<li>paper_authors: Qi Qian, Yuanhong Xu, Juhua Hu</li>
<li>for: 这篇论文旨在提出一种方法，使用文本proxy来学习视觉代理，以便在没有标注目标视觉数据的情况下进行零shot传递。</li>
<li>methods: 该方法使用CLIP的视觉预训练方法，然后使用文本proxy来学习视觉代理。同时， authors提出了一种理论分析，表明不可以通过减少对比偏好来减小视觉和文本空间之间的差距。</li>
<li>results: 实验结果表明，该方法可以在一分钟内在单个GPU上学习视觉代理，并且可以提高零shot传递精度从77.02%提高到80.21% на ImageNet上，使用ViT-L&#x2F;14@336预训练后CLIP。<details>
<summary>Abstract</summary>
Vision-language pre-training methods, e.g., CLIP, demonstrate an impressive zero-shot performance on visual categorizations with the class proxy from the text embedding of the class name. However, the modality gap between the text and vision space can result in a sub-optimal performance. We theoretically show that the gap cannot be reduced sufficiently by minimizing the contrastive loss in CLIP and the optimal proxy for vision tasks may reside only in the vision space. Therefore, given unlabeled target vision data, we propose to learn the vision proxy directly with the help from the text proxy for zero-shot transfer. Moreover, according to our theoretical analysis, strategies are developed to further refine the pseudo label obtained by the text proxy to facilitate the intra-modal proxy learning (InMaP) for vision. Experiments on extensive downstream tasks confirm the effectiveness and efficiency of our proposal. Concretely, InMaP can obtain the vision proxy within one minute on a single GPU while improving the zero-shot accuracy from $77.02\%$ to $80.21\%$ on ImageNet with ViT-L/14@336 pre-trained by CLIP. Code is available at \url{https://github.com/idstcv/InMaP}.
</details>
<details>
<summary>摘要</summary>
视觉语言预训练方法，如CLIP，示出了无需seen数据的很好表现能力。然而，视觉和语言空间之间的差异可能会导致表现下降。我们理论上表明，这种差异无法通过CLIP中的对比损失来减小。因此，在没有标注目标视觉数据的情况下，我们提议通过文本代理来学习视觉代理。此外，根据我们的理论分析，我们还开发了一些策略来进一步修正由文本代理生成的假标签，以便进行内模态代理学习（InMaP）。实验表明，InMaP可以在单个GPU上在1分钟内获得视觉代理，并在ImageNet上提高零shot准确率从77.02%到80.21%。代码可以在<https://github.com/idstcv/InMaP>上找到。
</details></li>
</ul>
<hr>
<h2 id="Tell-Me-What-Is-Good-About-This-Property-Leveraging-Reviews-For-Segment-Personalized-Image-Collection-Summarization"><a href="#Tell-Me-What-Is-Good-About-This-Property-Leveraging-Reviews-For-Segment-Personalized-Image-Collection-Summarization" class="headerlink" title="Tell Me What Is Good About This Property: Leveraging Reviews For Segment-Personalized Image Collection Summarization"></a>Tell Me What Is Good About This Property: Leveraging Reviews For Segment-Personalized Image Collection Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19743">http://arxiv.org/abs/2310.19743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Wysoczanska, Moran Beladev, Karen Lastmann Assaraf, Fengjun Wang, Ofri Kleinfeld, Gil Amsalem, Hadas Harush Boker</li>
<li>for: 本研究旨在提高Booking.com上的图像摘要，使其更能够满足用户的需求和喜好。</li>
<li>methods: 本研究使用用户点评中提到的最重要方面进行图像摘要的分析，以提高摘要的有用性和相关性。</li>
<li>results: 研究表明，使用用户点评进行图像摘要可以提高摘要的质量和有用性，而且不需要费时的注释。我们的人体学习研究也表明，用户对于我们的cross-modal方法（CrossSummarizer）表示更高的满意度。<details>
<summary>Abstract</summary>
Image collection summarization techniques aim to present a compact representation of an image gallery through a carefully selected subset of images that captures its semantic content. When it comes to web content, however, the ideal selection can vary based on the user's specific intentions and preferences. This is particularly relevant at Booking.com, where presenting properties and their visual summaries that align with users' expectations is crucial. To address this challenge, we consider user intentions in the summarization of property visuals by analyzing property reviews and extracting the most significant aspects mentioned by users. By incorporating the insights from reviews in our visual summaries, we enhance the summaries by presenting the relevant content to a user. Moreover, we achieve it without the need for costly annotations. Our experiments, including human perceptual studies, demonstrate the superiority of our cross-modal approach, which we coin as CrossSummarizer over the no-personalization and image-based clustering baselines.
</details>
<details>
<summary>摘要</summary>
simplified Chinese:图像集合概要技术的目标是通过选择一个精心选择的图像子集，以捕捉图像集合的 semantic 内容。然而，在网络内容上，理想的选择可能会因用户的具体目的和偏好而发生变化。这 particualry relevant 在 Booking.com 上，因为在这里，为用户提供适合他们期望的 properties 和其视觉概要是非常重要。为了解决这个挑战，我们在 visual 概要中考虑用户的意图，通过分析 property 评论，提取用户提到的最重要的方面。通过在概要中包含评论中的 Insight，我们可以提高概要的 relevance，为用户提供相关的内容。此外，我们可以在不需要昂贵的注释的情况下实现这一点。我们的实验，包括人类感知研究，表明我们的 cross-modal 方法（我们称之为 CrossSummarizer）在无个性化和图像基于划分的基线之上具有超越性。
</details></li>
</ul>
<hr>
<h2 id="Promise-Prompt-driven-3D-Medical-Image-Segmentation-Using-Pretrained-Image-Foundation-Models"><a href="#Promise-Prompt-driven-3D-Medical-Image-Segmentation-Using-Pretrained-Image-Foundation-Models" class="headerlink" title="Promise:Prompt-driven 3D Medical Image Segmentation Using Pretrained Image Foundation Models"></a>Promise:Prompt-driven 3D Medical Image Segmentation Using Pretrained Image Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19721">http://arxiv.org/abs/2310.19721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Li, Han Liu, Dewei Hu, Jiacheng Wang, Ipek Oguz</li>
<li>for: 提高医疗成像领域中数据采集和标签可用性问题的解决方案</li>
<li>methods: 基于自然图像领域的转移学习，使用单个点提示来激活已经预训练的2D图像基础模型，并使用轻量级适配器来提取3D空间上的深度相关信息</li>
<li>results: 在两个公共数据集上进行colon和肠癌肿块分割任务，与状态之前 segmentation 方法相比，提出的方法具有更高的性能<details>
<summary>Abstract</summary>
To address prevalent issues in medical imaging, such as data acquisition challenges and label availability, transfer learning from natural to medical image domains serves as a viable strategy to produce reliable segmentation results. However, several existing barriers between domains need to be broken down, including addressing contrast discrepancies, managing anatomical variability, and adapting 2D pretrained models for 3D segmentation tasks. In this paper, we propose ProMISe,a prompt-driven 3D medical image segmentation model using only a single point prompt to leverage knowledge from a pretrained 2D image foundation model. In particular, we use the pretrained vision transformer from the Segment Anything Model (SAM) and integrate lightweight adapters to extract depth-related (3D) spatial context without updating the pretrained weights. For robust results, a hybrid network with complementary encoders is designed, and a boundary-aware loss is proposed to achieve precise boundaries. We evaluate our model on two public datasets for colon and pancreas tumor segmentations, respectively. Compared to the state-of-the-art segmentation methods with and without prompt engineering, our proposed method achieves superior performance. The code is publicly available at https://github.com/MedICL-VU/ProMISe.
</details>
<details>
<summary>摘要</summary>
医学影像问题的常见问题，如数据获取困难和标签不足，可以通过域转换学习来生成可靠的 segmentation 结果。然而，需要破坏几个存在的域之间的障碍，包括对比不同、管理生物学变化和使用2D预训练模型进行3D segmentation任务的适应。在本文中，我们提出了ProMISe，一种基于单点提示的3D医学影像 segmentation模型，使用Segment Anything Model（SAM）的预训练视Transformer，并通过轻量级适配器来提取深度相关（3D）空间上下文而无需更新预训练参数。为了增强结果的稳定性，我们设计了混合网络，并提出了边界意识损失来实现精确的边界。我们在两个公共数据集上进行了对colon和肠癌肿 segmentation的评估，并与无提示工程和其他状态之前的方法进行了比较。相比之下，我们的提posed方法得到了superior的性能。代码可以在https://github.com/MedICL-VU/ProMISe上获取。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-decomposition-of-overlapping-sparse-images-application-at-the-vertex-of-neutrino-interactions"><a href="#Deep-learning-based-decomposition-of-overlapping-sparse-images-application-at-the-vertex-of-neutrino-interactions" class="headerlink" title="Deep-learning-based decomposition of overlapping-sparse images: application at the vertex of neutrino interactions"></a>Deep-learning-based decomposition of overlapping-sparse images: application at the vertex of neutrino interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19695">http://arxiv.org/abs/2310.19695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saúl Alonso-Monsalve, Davide Sgalaberna, Xingyu Zhao, Adrien Molines, Clark McGrew, André Rubbia</li>
<li>for: 这篇论文旨在解决高能物理中的核反应点中的复杂问题，即通过深度学习分解多维重叠 sparse 图像，提取低动量粒子的精细参数。</li>
<li>methods: 该方法利用深度学习对多维重叠 sparse 图像进行分解，从而提取低动量粒子的精细参数。</li>
<li>results: 该方法可以准确地提取低动量粒子的精细参数，提高了 neutrino 事件的能量解析精度。此外，通过与完全可导生成模型结合，进一步提高图像分解，以达到具有前所未有的结果。<details>
<summary>Abstract</summary>
Image decomposition plays a crucial role in various computer vision tasks, enabling the analysis and manipulation of visual content at a fundamental level. Overlapping images, which occur when multiple objects or scenes partially occlude each other, pose unique challenges for decomposition algorithms. The task intensifies when working with sparse images, where the scarcity of meaningful information complicates the precise extraction of components. This paper presents a solution that leverages the power of deep learning to accurately extract individual objects within multi-dimensional overlapping-sparse images, with a direct application in high-energy physics with decomposition of overlaid elementary particles obtained from imaging detectors. In particular, the proposed approach tackles a highly complex yet unsolved problem: identifying and measuring independent particles at the vertex of neutrino interactions, where one expects to observe detector images with multiple indiscernible overlapping charged particles. By decomposing the image of the detector activity at the vertex through deep learning, it is possible to infer the kinematic parameters of the identified low-momentum particles - which otherwise would remain neglected - and enhance the reconstructed energy resolution of the neutrino event. We also present an additional step - that can be tuned directly on detector data - combining the above method with a fully-differentiable generative model to improve the image decomposition further and, consequently, the resolution of the measured parameters, achieving unprecedented results. This improvement is crucial for precisely measuring the parameters that govern neutrino flavour oscillations and searching for asymmetries between matter and antimatter.
</details>
<details>
<summary>摘要</summary>
图像分解在各种计算机视觉任务中扮演着关键角色，允许对视觉内容进行基础 уров划分。不同物体或场景之间的重叠，对分解算法 pose 独特挑战。在缺乏有效信息的情况下，精度地提取组件变得更加复杂。这篇文章提出了一种利用深度学习来准确地从多维重叠 sparse 图像中提取个体对象，具体应用于高能物理中的图像分解。特别是，提议方案解决了一个非常复杂但未解决的问题：在neutrino交互点上识别和测量低动量粒子的独立参数，其中预期可以 observer 探测器图像中多个难以区分的 charged 粒子。通过深度学习对探测器活动图像的分解，可以从粒子的kinematic 参数中提取低动量粒子的识别结果，从而提高 neutrino 事件的能量分解。此外，我们还提出了一个附加步骤，可以直接基于探测器数据进行调整，通过将上述方法与完全可导生成模型相结合，进一步提高图像分解，并因此提高测量参数的分解精度，达到历史上最佳结果。这种改进是关键的，用于精确测量neutrino 味 flavor 振荡和物质与反物质的差异。
</details></li>
</ul>
<hr>
<h2 id="A-Principled-Hierarchical-Deep-Learning-Approach-to-Joint-Image-Compression-and-Classification"><a href="#A-Principled-Hierarchical-Deep-Learning-Approach-to-Joint-Image-Compression-and-Classification" class="headerlink" title="A Principled Hierarchical Deep Learning Approach to Joint Image Compression and Classification"></a>A Principled Hierarchical Deep Learning Approach to Joint Image Compression and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19675">http://arxiv.org/abs/2310.19675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Qi, Achintha Wijesinghe, Lahiru D. Chamain, Zhi Ding</li>
<li>for: 本研究旨在优化深度学习模型，以便在具有限制频率&#x2F;容量的物理通道上实现远程图像分类。</li>
<li>methods: 本研究提出了一种三步联合学习策略，以引导编码器提取高度精炼、描述性强和可以通过共同增强&#x2F;变换进行学习的特征。在初步屏选阶段之前，我们将缓存dimension优化为最小值。在训练过程中，我们使用 entropy-based quantization和&#x2F;或手动 truncation来调整缓存表示的位数。</li>
<li>results: 我们的提议方法可以在CIFAR-10和CIFAR-100上实现Accuracy提高约1.5%和3% respectively，比传统的端到端十字 entropy 训练更高。<details>
<summary>Abstract</summary>
Among applications of deep learning (DL) involving low cost sensors, remote image classification involves a physical channel that separates edge sensors and cloud classifiers. Traditional DL models must be divided between an encoder for the sensor and the decoder + classifier at the edge server. An important challenge is to effectively train such distributed models when the connecting channels have limited rate/capacity. Our goal is to optimize DL models such that the encoder latent requires low channel bandwidth while still delivers feature information for high classification accuracy. This work proposes a three-step joint learning strategy to guide encoders to extract features that are compact, discriminative, and amenable to common augmentations/transformations. We optimize latent dimension through an initial screening phase before end-to-end (E2E) training. To obtain an adjustable bit rate via a single pre-deployed encoder, we apply entropy-based quantization and/or manual truncation on the latent representations. Tests show that our proposed method achieves accuracy improvement of up to 1.5% on CIFAR-10 and 3% on CIFAR-100 over conventional E2E cross-entropy training.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）应用中的低成本感知器Remote图像分类具有物理通道，将边缘感知器和云端分类器分开。传统的DL模型需要在感知器和云端分类器之间分配资源，这会增加训练复杂性和通信成本。我们的目标是将DL模型优化，使承载器中的秘密特征具有低通信带宽，仍然能够提供高精度分类。我们提出了一种三步共同学习策略，以导引承载器中的encoder提取高度紧凑、抽象、可以适应增强的特征。我们通过初始屏选 phasescreening来验证维度的选择。在end-to-end（E2E）训练过程中，我们使用Entropy基于的压缩和/或手动跳转来调整秘密特征的维度。测试结果表明，我们的提议方法可以在CIFAR-10和CIFAR-100上提高精度达1.5%和3%。
</details></li>
</ul>
<hr>
<h2 id="DrM-Mastering-Visual-Reinforcement-Learning-through-Dormant-Ratio-Minimization"><a href="#DrM-Mastering-Visual-Reinforcement-Learning-through-Dormant-Ratio-Minimization" class="headerlink" title="DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization"></a>DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19668">http://arxiv.org/abs/2310.19668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guowei Xu, Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Zhecheng Yuan, Tianying Ji, Yu Luo, Xiaoyu Liu, Jiaxin Yuan, Pu Hua, Shuzhen Li, Yanjie Ze, Hal Daumé III, Furong Huang, Huazhe Xu</li>
<li>for: 提高模型自适应控制任务的效率和稳定性。</li>
<li>methods: 利用三种核心机制来引导代理人的决策决策-优化策略，并使用着寂率来衡量代理人的活动水平。</li>
<li>results: 实验表明，DrM在三个连续控制 benchmark环境中（DeepMind Control Suite、MetaWorld 和 Adroit） achieved significant improvements in sample efficiency and asymptotic performance with no broken seeds（76 seeds in total），并成功解决了 DeepMind Control Suite 中的狗和手动控制任务，以及 Adroit 中的三个灵活手动控制任务。<details>
<summary>Abstract</summary>
Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.
</details>
<details>
<summary>摘要</summary>
视觉强化学习（RL）在连续控制任务中表现出了承诺。尽管它已经取得了进步，但现有算法仍然在许多方面表现不满意，如样本效率、极限性能和随机种子的稳定性。在这篇论文中，我们发现了现有的视觉RL方法中的一个重要缺陷：代理人在训练的早期经常进入持续的无动作状态，从而限制它们的探索效果。在这基础之上，我们还发现了代理人倾向于无动作探索的倾向和策略网络中无活动神经元之间存在显著的相关性。为了量化这种无动作，我们采用了沉默率作为RL代理人网络中的活动度量表。实验表明，DrM可以在三个核心机制的指导下， aktive 地降低沉默率，从而改善样本效率和极限性能。DrM在DeepMind Control Suite、MetaWorld和Adroit三个连续控制 benchmark环境中实现了显著的改进，而且在 Dog 和 Manipulator 领域中解决了任务，并在 Adroit 中实现了三个灵活手 manipulate 任务无需示例。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-in-Computational-Pathology-Survey-and-Guidelines"><a href="#Domain-Generalization-in-Computational-Pathology-Survey-and-Guidelines" class="headerlink" title="Domain Generalization in Computational Pathology: Survey and Guidelines"></a>Domain Generalization in Computational Pathology: Survey and Guidelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19656">http://arxiv.org/abs/2310.19656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Jahanifar, Manahil Raza, Kesi Xu, Trinh Vuong, Rob Jewsbury, Adam Shephard, Neda Zamanitajeddin, Jin Tae Kwak, Shan E Ahmed Raza, Fayyaz Minhas, Nasir Rajpoot</li>
<li>for: 本研究旨在探讨域特化（Domain Generalization，DS）问题在计算生物学（Computational Pathology，CPath）领域中的应用和解决方案。</li>
<li>methods: 本文系统地综述了CPath领域中存在多种域特化问题的类型和现有的解决方案，以及这些方法的优缺点和适用范围。此外，我们还进行了28种先进的域特化算法的 benchmarking 实验，并发现了一些有效的方法。</li>
<li>results: 本研究的结果表明，在CPath领域中，通过精心的实验设计和特征增强技术，可以有效地解决域特化问题。然而，不同的场景下的域特化问题需要不同的解决方案。因此，本文提出了明确的指南和方法来检测和管理域特化问题。这些概念、指南和建议都适用于大多数医学影像分析任务。<details>
<summary>Abstract</summary>
Deep learning models have exhibited exceptional effectiveness in Computational Pathology (CPath) by tackling intricate tasks across an array of histology image analysis applications. Nevertheless, the presence of out-of-distribution data (stemming from a multitude of sources such as disparate imaging devices and diverse tissue preparation methods) can cause \emph{domain shift} (DS). DS decreases the generalization of trained models to unseen datasets with slightly different data distributions, prompting the need for innovative \emph{domain generalization} (DG) solutions. Recognizing the potential of DG methods to significantly influence diagnostic and prognostic models in cancer studies and clinical practice, we present this survey along with guidelines on achieving DG in CPath. We rigorously define various DS types, systematically review and categorize existing DG approaches and resources in CPath, and provide insights into their advantages, limitations, and applicability. We also conduct thorough benchmarking experiments with 28 cutting-edge DG algorithms to address a complex DG problem. Our findings suggest that careful experiment design and CPath-specific Stain Augmentation technique can be very effective. However, there is no one-size-fits-all solution for DG in CPath. Therefore, we establish clear guidelines for detecting and managing DS depending on different scenarios. While most of the concepts, guidelines, and recommendations are given for applications in CPath, we believe that they are applicable to most medical image analysis tasks as well.
</details>
<details>
<summary>摘要</summary>
深度学习模型在计算 PATH 中（CPath）表现出色，能够解决各种复杂的图像分析任务。然而，由于多种来源的不同图像设备和多种组织方法而导致的“领域转移”（DS）问题，使得训练的模型在未看过的数据集上的泛化性受到影响。为了解决这问题，我们提出了一些创新的“领域泛化”（DG）方法。我们在这篇文章中对 DG 方法在 CPath 中的应用进行了系统性的介绍和评估，并提供了适用于不同场景的指导方针。我们还进行了28种高级 DG 算法的 benchmarking 实验，发现在 CPath 中使用特定的染料增强技术和精心的实验设计可以获得非常有效的结果。然而，没有一个适合所有情况的 DG 解决方案。因此，我们提出了适应不同场景的 DS 探测和管理的明确指导方针。这些概念、指导方针和建议大部分适用于医学图像分析任务中。
</details></li>
</ul>
<hr>
<h2 id="Upgrading-VAE-Training-With-Unlimited-Data-Plans-Provided-by-Diffusion-Models"><a href="#Upgrading-VAE-Training-With-Unlimited-Data-Plans-Provided-by-Diffusion-Models" class="headerlink" title="Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models"></a>Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19653">http://arxiv.org/abs/2310.19653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Z. Xiao, Johannes Zenn, Robert Bamler</li>
<li>for: This paper aims to mitigate overfitting in variational autoencoders (VAEs) by training on samples from a pre-trained diffusion model.</li>
<li>methods: The paper proposes training VAEs on samples from a pre-trained diffusion model to improve their representation learning and mitigate overfitting.</li>
<li>results: The paper finds that training VAEs on samples from a pre-trained diffusion model leads to improvements in generative performance, amortization gap, and robustness compared to normal training and conventional data augmentation methods.<details>
<summary>Abstract</summary>
Variational autoencoders (VAEs) are popular models for representation learning but their encoders are susceptible to overfitting (Cremer et al., 2018) because they are trained on a finite training set instead of the true (continuous) data distribution $p_{\mathrm{data}(\mathbf{x})$. Diffusion models, on the other hand, avoid this issue by keeping the encoder fixed. This makes their representations less interpretable, but it simplifies training, enabling accurate and continuous approximations of $p_{\mathrm{data}(\mathbf{x})$. In this paper, we show that overfitting encoders in VAEs can be effectively mitigated by training on samples from a pre-trained diffusion model. These results are somewhat unexpected as recent findings (Alemohammad et al., 2023; Shumailov et al., 2023) observe a decay in generative performance when models are trained on data generated by another generative model. We analyze generalization performance, amortization gap, and robustness of VAEs trained with our proposed method on three different data sets. We find improvements in all metrics compared to both normal training and conventional data augmentation methods, and we show that a modest amount of samples from the diffusion model suffices to obtain these gains.
</details>
<details>
<summary>摘要</summary>
variational autoencoders (VAEs) 是一种广泛使用的表示学习模型，但它们的编码器容易过拟合 (Cremer et al., 2018)，因为它们被训练在一个有限的训练集上而不是真正的数据分布 $p_{\text{data}(\mathbf{x})$。而扩散模型则可以避免这个问题，因为它们的编码器是固定的。这会使其表示更难于解释，但是它可以简化训练，使得可以获得精确和连续的 $p_{\text{data}(\mathbf{x})$ 的近似。在这篇论文中，我们显示了使用预训练的扩散模型训练 VAE 可以有效地遏制编码器过拟合。这些结果有些意外，因为 latest findings (Alemohammad et al., 2023; Shumailov et al., 2023) 观察到在另一个生成模型上训练模型会导致生成性能下降。我们分析了 VAE 在不同数据集上的总体性能、挥剑差和稳定性，并发现在使用我们提议的方法时，所有指标都有所改善，并且发现一些模est amount of samples from the diffusion model suffices to obtain these gains。
</details></li>
</ul>
<hr>
<h2 id="DistNet2D-Leveraging-long-range-temporal-information-for-efficient-segmentation-and-tracking"><a href="#DistNet2D-Leveraging-long-range-temporal-information-for-efficient-segmentation-and-tracking" class="headerlink" title="DistNet2D: Leveraging long-range temporal information for efficient segmentation and tracking"></a>DistNet2D: Leveraging long-range temporal information for efficient segmentation and tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19641">http://arxiv.org/abs/2310.19641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean Ollion, Martin Maliet, Caroline Giuglaris, Elise Vacher, Maxime Deforet</li>
<li>for: 这篇论文的目的是提出一种新的深度神经网络架构，以便在视频微镜像中提高细胞分类和跟踪的准确率。</li>
<li>methods: 这篇论文使用的方法包括一种新的深度神经网络架构 DistNet2D，该架构利用了中期和长期的时间上下文来提高分类和跟踪的准确率。另外，论文还使用了一种后处理程序，该程序利用整个电影中的信息来更正分类错误。</li>
<li>results: 论文的实验结果显示，DistNet2D 在两个实验数据集上都有较高的性能，比之前两种方法更高。此外，论文还示出了在 correlate 细胞大小和形状与其运输特性的 statistically 的可能性。<details>
<summary>Abstract</summary>
Extracting long tracks and lineages from videomicroscopy requires an extremely low error rate, which is challenging on complex datasets of dense or deforming cells. Leveraging temporal context is key to overcome this challenge. We propose DistNet2D, a new deep neural network (DNN) architecture for 2D cell segmentation and tracking that leverages both mid- and long-term temporal context. DistNet2D considers seven frames at the input and uses a post-processing procedure that exploits information from the entire movie to correct segmentation errors. DistNet2D outperforms two recent methods on two experimental datasets, one containing densely packed bacterial cells and the other containing eukaryotic cells. It has been integrated into an ImageJ-based graphical user interface for 2D data visualization, curation, and training. Finally, we demonstrate the performance of DistNet2D on correlating the size and shape of cells with their transport properties over large statistics, for both bacterial and eukaryotic cells.
</details>
<details>
<summary>摘要</summary>
原文：Extracting long tracks and lineages from videomicroscopy requires an extremely low error rate, which is challenging on complex datasets of dense or deforming cells. Leveraging temporal context is key to overcome this challenge. We propose DistNet2D, a new deep neural network (DNN) architecture for 2D cell segmentation and tracking that leverages both mid- and long-term temporal context. DistNet2D considers seven frames at the input and uses a post-processing procedure that exploits information from the entire movie to correct segmentation errors. DistNet2D outperforms two recent methods on two experimental datasets, one containing densely packed bacterial cells and the other containing eukaryotic cells. It has been integrated into an ImageJ-based graphical user interface for 2D data visualization, curation, and training. Finally, we demonstrate the performance of DistNet2D on correlating the size and shape of cells with their transport properties over large statistics, for both bacterial and eukaryotic cells.翻译：EXTRACTING LONG TRACKS AND LINEAGES FROM VIDEOMICROSCOPY REQUIRES AN EXTREMELY LOW ERROR RATE, WHICH IS CHALLENGING ON COMPLEX DATASETS OF DENSE OR DEFORMING CELLS. LEVERAGING TEMPORAL CONTEXT IS KEY TO OVERCOME THIS CHALLENGE. WE PROPOSE DISTNET2D, A NEW DEEP NEURAL NETWORK (DNN) ARCHITECTURE FOR 2D CELL SEGMENTATION AND TRACKING THAT LEVERAGES BOTH MID- AND LONG-TERM TEMPORAL CONTEXT. DISTNET2D CONSIDERS SEVEN FRAMES AT THE INPUT AND USES A POST-PROCESSING PROCEDURE THAT EXPLOITS INFORMATION FROM THE ENTIRE MOVIE TO CORRECT SEGMENTATION ERRORS. DISTNET2D OUTPERFORMS TWO RECENT METHODS ON TWO EXPERIMENTAL DATASETS, ONE CONTAINING DENSELY PACKED BACTERIAL CELLS AND THE OTHER CONTAINING EUKARYOTIC CELLS. IT HAS BEEN INTEGRATED INTO AN IMAGEJ-BASED GRAPHICAL USER INTERFACE FOR 2D DATA VISUALIZATION, CURATION, AND TRAINING. FINALLY, WE DEMONSTRATE THE PERFORMANCE OF DISTNET2D ON CORRELATING THE SIZE AND SHAPE OF CELLS WITH THEIR TRANSPORT PROPERTIES OVER LARGE STATISTICS, FOR BOTH BACTERIAL AND EUKARYOTIC CELLS.
</details></li>
</ul>
<hr>
<h2 id="Leave-No-Stone-Unturned-Mine-Extra-Knowledge-for-Imbalanced-Facial-Expression-Recognition"><a href="#Leave-No-Stone-Unturned-Mine-Extra-Knowledge-for-Imbalanced-Facial-Expression-Recognition" class="headerlink" title="Leave No Stone Unturned: Mine Extra Knowledge for Imbalanced Facial Expression Recognition"></a>Leave No Stone Unturned: Mine Extra Knowledge for Imbalanced Facial Expression Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19636">http://arxiv.org/abs/2310.19636</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zyh-uaiaaaa/Mine-Extra-Knowledge">https://github.com/zyh-uaiaaaa/Mine-Extra-Knowledge</a></li>
<li>paper_authors: Yuhang Zhang, Yaqi Li, Lixiong Qin, Xuannan Liu, Weihong Deng</li>
<li>for:  addresses the imbalanced facial expression recognition (FER) problem by proposing a novel approach to extract extra knowledge related to minor classes from both major and minor class samples.</li>
<li>methods:  leverages re-balanced attention maps to regularize the model and extract transformation invariant information about the minor classes from all training samples, and introduces re-balanced smooth labels to regulate the cross-entropy loss and guide the model to pay more attention to the minor classes.</li>
<li>results:  achieves state-of-the-art performance under the imbalanced FER task through extensive experiments on different datasets and backbones.<details>
<summary>Abstract</summary>
Facial expression data is characterized by a significant imbalance, with most collected data showing happy or neutral expressions and fewer instances of fear or disgust. This imbalance poses challenges to facial expression recognition (FER) models, hindering their ability to fully understand various human emotional states. Existing FER methods typically report overall accuracy on highly imbalanced test sets but exhibit low performance in terms of the mean accuracy across all expression classes. In this paper, our aim is to address the imbalanced FER problem. Existing methods primarily focus on learning knowledge of minor classes solely from minor-class samples. However, we propose a novel approach to extract extra knowledge related to the minor classes from both major and minor class samples. Our motivation stems from the belief that FER resembles a distribution learning task, wherein a sample may contain information about multiple classes. For instance, a sample from the major class surprise might also contain useful features of the minor class fear. Inspired by that, we propose a novel method that leverages re-balanced attention maps to regularize the model, enabling it to extract transformation invariant information about the minor classes from all training samples. Additionally, we introduce re-balanced smooth labels to regulate the cross-entropy loss, guiding the model to pay more attention to the minor classes by utilizing the extra information regarding the label distribution of the imbalanced training data. Extensive experiments on different datasets and backbones show that the two proposed modules work together to regularize the model and achieve state-of-the-art performance under the imbalanced FER task. Code is available at https://github.com/zyh-uaiaaaa.
</details>
<details>
<summary>摘要</summary>
《表情数据具有显著的不均衡问题，大多数收集到的数据显示了快乐或中性的表情，而少量的恐惧或厌恶表情。这种不均衡问题对于表情识别（FER）模型提出了挑战，使其无法彻底理解人类各种情感状态。现有的FER方法通常在不均衡测试集上报告总准确率，但在不同表情类别的均准确率方面表现较差。在这篇论文中，我们的目标是解决不均衡FER问题。现有的方法主要是通过学习少数类样本来学习少数类知识。然而，我们提出了一种新的方法，利用重新平衡的注意力地图来补做模型，使其能够从所有训练样本中提取 transformation 不变的信息。此外，我们还引入了重新平衡的平滑标签，以规则模型的权重，使模型更加注重少数类。我们的实验表明，两个提出的模块可以结合使用，补做模型，并在不均衡FER任务上实现状态的杰出性。代码可以在 <https://github.com/zyh-uaiaaaa> 中找到。》Note that Simplified Chinese is the official writing system used in mainland China, and it may be different from Traditional Chinese used in other regions.
</details></li>
</ul>
<hr>
<h2 id="Bidirectional-Captioning-for-Clinically-Accurate-and-Interpretable-Models"><a href="#Bidirectional-Captioning-for-Clinically-Accurate-and-Interpretable-Models" class="headerlink" title="Bidirectional Captioning for Clinically Accurate and Interpretable Models"></a>Bidirectional Captioning for Clinically Accurate and Interpretable Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19635">http://arxiv.org/abs/2310.19635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keegan Quigley, Miriam Cha, Josh Barua, Geeticka Chauhan, Seth Berkowitz, Steven Horng, Polina Golland</li>
<li>for: 这个论文旨在探讨视语预训练在医学图像分析中的应用，并比较了强制对比学习方法和描述语言模型在生成图像描述的能力。</li>
<li>methods: 这篇论文使用了对文本描述的双向预训练，并且使用了一种名为RadTex的医学领域的CNNEncoder和TransformerDecoder架构。</li>
<li>results: 研究发现，描述语言预训练不仅可以生成与对比学习方法相当的视觉编码器（CheXpert竞赛多标签AUC为89.4%），还可以生成丰富的医学报告（描述macro-F1分数为0.349，使用CheXpert标签器），并且可以根据提示生成有targeted、交互的输出。<details>
<summary>Abstract</summary>
Vision-language pretraining has been shown to produce high-quality visual encoders which transfer efficiently to downstream computer vision tasks. While generative language models have gained widespread attention, image captioning has thus far been mostly overlooked as a form of cross-modal pretraining in favor of contrastive learning, especially in medical image analysis. In this paper, we experiment with bidirectional captioning of radiology reports as a form of pretraining and compare the quality and utility of learned embeddings with those from contrastive pretraining methods. We optimize a CNN encoder, transformer decoder architecture named RadTex for the radiology domain. Results show that not only does captioning pretraining yield visual encoders that are competitive with contrastive pretraining (CheXpert competition multi-label AUC of 89.4%), but also that our transformer decoder is capable of generating clinically relevant reports (captioning macro-F1 score of 0.349 using CheXpert labeler) and responding to prompts with targeted, interactive outputs.
</details>
<details>
<summary>摘要</summary>
视力语言预训理有效地生成高质量的视觉编码器，这些编码器可以有效传递到下游计算机视觉任务。然而，生成语言模型在医学影像分析领域中却受到了比较少的关注，而image captioning则被忽略了作为杂模预训理方法。在这篇论文中，我们使用了对 radiology 报告的 bidirectional captioning 作为预训理方法，并与对比式预训理方法进行比较。我们优化了一个 CNN 编码器和 transformer 解码器，并将其命名为 RadTex。结果显示，不仅captioning预训理可以生成与对比式预训理方法相当的视觉编码器（CheXpert 竞赛多标签 AUC 为 89.4%），而且我们的 transformer 解码器还能生成丰富的临床相关报告（captioning macro-F1 分数为 0.349，使用 CheXpert 标签器），并能够响应到提示的有arget、互动输出。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Neural-Networks-for-Automatic-Detection-of-Intact-Adenovirus-from-TEM-Imaging-with-Debris-Broken-and-Artefacts-Particles"><a href="#Convolutional-Neural-Networks-for-Automatic-Detection-of-Intact-Adenovirus-from-TEM-Imaging-with-Debris-Broken-and-Artefacts-Particles" class="headerlink" title="Convolutional Neural Networks for Automatic Detection of Intact Adenovirus from TEM Imaging with Debris, Broken and Artefacts Particles"></a>Convolutional Neural Networks for Automatic Detection of Intact Adenovirus from TEM Imaging with Debris, Broken and Artefacts Particles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19630">http://arxiv.org/abs/2310.19630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Rukundo, Andrea Behanova, Riccardo De Feo, Seppo Ronkko, Joni Oja, Jussi Tohka</li>
<li>for: 这种研究是为了帮助药品生产商在开发和生产过程中进行常规监测Primary particles和纯度分布，以避免产品的变化和杂质污染。</li>
<li>methods: 这种研究使用了电子显微镜成像技术，帮助生产商预测改变对病毒基因疗法Vector产品和中间体的特征和纯度。</li>
<li>results: 通过自动检测和分割adenovirus的软件工具，可以帮助生产商更好地检测和分析adenovirus在电子显微镜成像系统中。这些工具可以减少人工干预，提高检测效率和精度。<details>
<summary>Abstract</summary>
Regular monitoring of the primary particles and purity profiles of a drug product during development and manufacturing processes is essential for manufacturers to avoid product variability and contamination. Transmission electron microscopy (TEM) imaging helps manufacturers predict how changes affect particle characteristics and purity for virus-based gene therapy vector products and intermediates. Since intact particles can characterize efficacious products, it is beneficial to automate the detection of intact adenovirus against a non-intact-viral background mixed with debris, broken, and artefact particles. In the presence of such particles, detecting intact adenoviruses becomes more challenging. To overcome the challenge, due to such a presence, we developed a software tool for semi-automatic annotation and segmentation of adenoviruses and a software tool for automatic segmentation and detection of intact adenoviruses in TEM imaging systems. The developed semi-automatic tool exploited conventional image analysis techniques while the automatic tool was built based on convolutional neural networks and image analysis techniques. Our quantitative and qualitative evaluations showed outstanding true positive detection rates compared to false positive and negative rates where adenoviruses were nicely detected without mistaking them for real debris, broken adenoviruses, and/or staining artefacts.
</details>
<details>
<summary>摘要</summary>
常规监测药品主要粒子和纯度profile during 开发和生产过程是非常重要，以避免产品变异和杂质污染。电子传输微scopy（TEM）成像帮助制药厂家预测变化对粒子特性和纯度的影响，用于抗生素基因疗法vector产品和中间体。由于完整的粒子可以characterize 有效的产品，因此自动检测完整的adenovirus Against 杂质背景杂mix的粒子是有利的。在存在这些粒子时，检测完整的adenoviruses更加挑战。为了解决这个问题，我们开发了一种 semi-automatic 注解和分割adenoviruses的软件工具，以及一种自动分割和检测完整adenoviruses的软件工具。我们的量化和质量评估表明，我们的方法具有出色的真正阳性检测率，而false正和false负检测率几乎为零。adenoviruses 在TEM成像系统中被成功地检测出来，无需误尝杂质、损坏adenoviruses和染料artefacts。
</details></li>
</ul>
<hr>
<h2 id="GC-MVSNet-Multi-View-Multi-Scale-Geometrically-Consistent-Multi-View-Stereo"><a href="#GC-MVSNet-Multi-View-Multi-Scale-Geometrically-Consistent-Multi-View-Stereo" class="headerlink" title="GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo"></a>GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19583">http://arxiv.org/abs/2310.19583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vkvats/GC-MVSNet">https://github.com/vkvats/GC-MVSNet</a></li>
<li>paper_authors: Vibhas K. Vats, Sripad Joshi, David J. Crandall, Md. Alimoor Reza, Soon-heung Jung</li>
<li>for: 本研究的目的是提出一种新的多视图零点网络（MVSNet），用于解决多视图零点 reconstruction 问题。</li>
<li>methods: 本研究使用的方法是基于学习的多视图零点方法，其中包括一种新的几何一致损失函数，用于在不同视图和不同缩放级别之间进行多视图几何一致检查。</li>
<li>results: 实验结果表明，GC-MVSNet 可以快速地学习高质量的多视图零点 reconstruction，并在 DTU 和 BlendedMVS 数据集上达到新的状态当前。此外，GC-MVSNet 还在 Tanks 和 Temples 数据集上达到了竞争性的结果。<details>
<summary>Abstract</summary>
Traditional multi-view stereo (MVS) methods rely heavily on photometric and geometric consistency constraints, but newer machine learning-based MVS methods check geometric consistency across multiple source views only as a post-processing step. In this paper, we present a novel approach that explicitly encourages geometric consistency of reference view depth maps across multiple source views at different scales during learning (see Fig. 1). We find that adding this geometric consistency loss significantly accelerates learning by explicitly penalizing geometrically inconsistent pixels, reducing the training iteration requirements to nearly half that of other MVS methods. Our extensive experiments show that our approach achieves a new state-of-the-art on the DTU and BlendedMVS datasets, and competitive results on the Tanks and Temples benchmark. To the best of our knowledge, GC-MVSNet is the first attempt to enforce multi-view, multi-scale geometric consistency during learning.
</details>
<details>
<summary>摘要</summary>
传统的多视图零点矩阵（MVS）方法强调图像照度和几何含义一致性约束，但 newer的机器学习基于MVS方法只在多个源视图中的几何一致性检查为后处理步骤。在这篇论文中，我们提出了一种新的方法，其中在多个源视图中的参考视图深度图中的几何一致性被明确地强制实施（参见图1）。我们发现，在学习过程中添加几何一致性损失可以显著加速学习，因为它直接惩罚不几何一致的像素，从而减少了学习迭代次数至近乎半数。我们的广泛实验表明，我们的方法在DTU和BlendedMVS数据集上达到了新的状态计算机视觉领域，并在Tanks and Temples标准准确率中获得了竞争力的结果。我们知道，GC-MVSNet是第一个在多视图、多尺度中强制多视图几何一致性的学习方法。
</details></li>
</ul>
<hr>
<h2 id="Human-interpretable-and-deep-features-for-image-privacy-classification"><a href="#Human-interpretable-and-deep-features-for-image-privacy-classification" class="headerlink" title="Human-interpretable and deep features for image privacy classification"></a>Human-interpretable and deep features for image privacy classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19582">http://arxiv.org/abs/2310.19582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Darya Baranouskaya, Andrea Cavallaro</li>
<li>for: 本研究旨在探讨隐私分类 dataset 和不同评估人员对敏感图像的纠正标签的问题。</li>
<li>methods: 本文提出了八种适用于图像隐私分类的特有和人类可解释的特征，并证明这些特征可以提高深度学习模型的性能和图像隐私分类的表示能力。</li>
<li>results: 本文分析了不同评估人员对敏感图像的纠正标签，并提出了适用于图像隐私分类的特有和人类可解释的特征。这些特征可以提高深度学习模型的性能和图像隐私分类的表示能力。<details>
<summary>Abstract</summary>
Privacy is a complex, subjective and contextual concept that is difficult to define. Therefore, the annotation of images to train privacy classifiers is a challenging task. In this paper, we analyse privacy classification datasets and the properties of controversial images that are annotated with contrasting privacy labels by different assessors. We discuss suitable features for image privacy classification and propose eight privacy-specific and human-interpretable features. These features increase the performance of deep learning models and, on their own, improve the image representation for privacy classification compared with much higher dimensional deep features.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Seeing-Through-the-Conversation-Audio-Visual-Speech-Separation-based-on-Diffusion-Model"><a href="#Seeing-Through-the-Conversation-Audio-Visual-Speech-Separation-based-on-Diffusion-Model" class="headerlink" title="Seeing Through the Conversation: Audio-Visual Speech Separation based on Diffusion Model"></a>Seeing Through the Conversation: Audio-Visual Speech Separation based on Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19581">http://arxiv.org/abs/2310.19581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suyeon Lee, Chaeyoung Jung, Youngjoon Jang, Jaehun Kim, Joon Son Chung</li>
<li>for: 本研究旨在提取混合声音中的目标说话者的声音，使用视觉cue。现有的音频视频演示 separation 方法已经展示了扩展性和可识别性的好表现，但是保持自然性仍然是一个挑战。</li>
<li>methods: 我们提出了一种基于扩散机制的音频视频演示 separation 模型，称为 AVDiffuSS。此外，我们还提出了一种特制的跨注意力特征融合机制，用于融合两种感知模式。这种机制特意针对语音频谱，以利用语音生成中的phonetic信息，以实现高度的特征融合，而不需较大的计算资源。</li>
<li>results: 我们在两个 benchmark 上进行了实验，包括 VoxCeleb2 和 LRS3，并得到了对比较好的结果。相比之前的方法，我们的方法可以生成更自然的语音，而且不需要较大的计算资源。<details>
<summary>Abstract</summary>
The objective of this work is to extract target speaker's voice from a mixture of voices using visual cues. Existing works on audio-visual speech separation have demonstrated their performance with promising intelligibility, but maintaining naturalness remains a challenge. To address this issue, we propose AVDiffuSS, an audio-visual speech separation model based on a diffusion mechanism known for its capability in generating natural samples. For an effective fusion of the two modalities for diffusion, we also propose a cross-attention-based feature fusion mechanism. This mechanism is specifically tailored for the speech domain to integrate the phonetic information from audio-visual correspondence in speech generation. In this way, the fusion process maintains the high temporal resolution of the features, without excessive computational requirements. We demonstrate that the proposed framework achieves state-of-the-art results on two benchmarks, including VoxCeleb2 and LRS3, producing speech with notably better naturalness.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT目标是从混合声音中提取目标说话人的声音，使用视觉cue。现有的听视频演示表达能力很出色，但保持自然性仍然是挑战。为解决这个问题，我们提议AVDiffuSS，一种基于扩散机制的听视频演示模型。为有效地融合两种模式，我们还提议一种强调语音域的feature合并机制。这种机制通过跨注意力的feature合并来实现高时间分辨率的特征合并，而无需过度的计算成本。我们示示了该框架在VoxCeleb2和LRS3两个benchmark上的状态空间表现，生成的speech具有显著更好的自然性。TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="A-Perceptual-Shape-Loss-for-Monocular-3D-Face-Reconstruction"><a href="#A-Perceptual-Shape-Loss-for-Monocular-3D-Face-Reconstruction" class="headerlink" title="A Perceptual Shape Loss for Monocular 3D Face Reconstruction"></a>A Perceptual Shape Loss for Monocular 3D Face Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19580">http://arxiv.org/abs/2310.19580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Otto, Prashanth Chandran, Gaspard Zoss, Markus Gross, Paulo Gotardo, Derek Bradley</li>
<li>for: This paper aims to improve the quality of monocular 3D face reconstruction by proposing a new perceptual shape loss function that uses shading cues to evaluate the quality of the 3D face estimate.</li>
<li>methods: The proposed method uses a discriminator-style neural network to evaluate the quality of the shaded render of the geometry estimate, without requiring an estimate of the albedo or illumination in the scene. The loss operates entirely in image space and is agnostic to mesh topology.</li>
<li>results: The authors show that their new perceptual shape loss can be combined with traditional energy terms for monocular 3D face optimization and deep neural network regression, improving upon current state-of-the-art results.Here is the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文目标是提高单投影3D人脸重建的质量，通过提出一种基于照明cue的新的形态损失函数。</li>
<li>methods: 该方法使用一种权重矩阵来评估预测的形态推测的质量，不需要场景中的反射率或照明估计。损失函数完全在图像空间运行，不受形态网格的影响。</li>
<li>results: 作者们表明，他们的新的形态损失函数可以与传统的能量函数相结合，提高单投影3D人脸优化和深度神经网络回归的结果。<details>
<summary>Abstract</summary>
Monocular 3D face reconstruction is a wide-spread topic, and existing approaches tackle the problem either through fast neural network inference or offline iterative reconstruction of face geometry. In either case carefully-designed energy functions are minimized, commonly including loss terms like a photometric loss, a landmark reprojection loss, and others. In this work we propose a new loss function for monocular face capture, inspired by how humans would perceive the quality of a 3D face reconstruction given a particular image. It is widely known that shading provides a strong indicator for 3D shape in the human visual system. As such, our new 'perceptual' shape loss aims to judge the quality of a 3D face estimate using only shading cues. Our loss is implemented as a discriminator-style neural network that takes an input face image and a shaded render of the geometry estimate, and then predicts a score that perceptually evaluates how well the shaded render matches the given image. This 'critic' network operates on the RGB image and geometry render alone, without requiring an estimate of the albedo or illumination in the scene. Furthermore, our loss operates entirely in image space and is thus agnostic to mesh topology. We show how our new perceptual shape loss can be combined with traditional energy terms for monocular 3D face optimization and deep neural network regression, improving upon current state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
单眼3D脸重建是一个广泛的研究领域，现有的方法可以通过快速的神经网络推断或者组合不同的构成来解决这个问题。不 matter the approach, 都需要仔细设计能量函数，通常包括像素损失、标点重映射损失等损失函数。在这个工作中，我们提出了一个新的损失函数 для单眼3D脸重建，受人类视觉系统中的视觉评估影响。我们发现，阴影提供了3D形状中强大的视觉指标。因此，我们的新的“感知”形状损失将评估3D脸估计中的阴影匹配度，以便更好地评估3D脸重建的质量。我们的损失函数通过一个批评器网络来实现，这个批评器网络将从输入的脸像和geometry估计中获得的阴影匹配度进行评估，并且预测一个视觉评估分数。这个批评器网络仅从RGB图像和geometry render alone，没有需要场景照明估计。此外，我们的损失函数完全在图像空间中运作，因此不受体统的限制。在这个研究中，我们显示了我们的新的感知形状损失可以与传统的能量函数和神经网络回推 combinated，以提高目前的州Of-The-Art结果。
</details></li>
</ul>
<hr>
<h2 id="Skip-WaveNet-A-Wavelet-based-Multi-scale-Architecture-to-Trace-Firn-Layers-in-Radar-Echograms"><a href="#Skip-WaveNet-A-Wavelet-based-Multi-scale-Architecture-to-Trace-Firn-Layers-in-Radar-Echograms" class="headerlink" title="Skip-WaveNet: A Wavelet based Multi-scale Architecture to Trace Firn Layers in Radar Echograms"></a>Skip-WaveNet: A Wavelet based Multi-scale Architecture to Trace Firn Layers in Radar Echograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19574">http://arxiv.org/abs/2310.19574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debvrat Varshney, Masoud Yari, Oluwanisola Ibikunle, Jilu Li, John Paden, Maryam Rahnemoonfar</li>
<li>for: 这个论文的目的是为了提高空气雷达探测器上的冰层层次检测，以便计算冰川覆盖率和海平面升高的贡献。</li>
<li>methods: 该论文使用了波峰基于的多尺度深度学习架构来自动处理空气雷达探测器上的冰层信号，以提高冰层层次检测的精度。</li>
<li>results: 该论文的提出的Skip-WaveNet架构可以在不同的数据集上实现更高的优化数据集批处理（ODS）和优化图像批处理（OIS）F- scores，并且可以在不同的层次上检测冰层，并且可以估计层次的平均绝对误差为3.31像素和94.3%的准确率。<details>
<summary>Abstract</summary>
Echograms created from airborne radar sensors capture the profile of firn layers present on top of an ice sheet. Accurate tracking of these layers is essential to calculate the snow accumulation rates, which are required to investigate the contribution of polar ice cap melt to sea level rise. However, automatically processing the radar echograms to detect the underlying firn layers is a challenging problem. In our work, we develop wavelet-based multi-scale deep learning architectures for these radar echograms to improve firn layer detection. We show that wavelet based architectures improve the optimal dataset scale (ODS) and optimal image scale (OIS) F-scores by 3.99% and 3.7%, respectively, over the non-wavelet architecture. Further, our proposed Skip-WaveNet architecture generates new wavelets in each iteration, achieves higher generalizability as compared to state-of-the-art firn layer detection networks, and estimates layer depths with a mean absolute error of 3.31 pixels and 94.3% average precision. Such a network can be used by scientists to trace firn layers, calculate the annual snow accumulation rates, estimate the resulting surface mass balance of the ice sheet, and help project global sea level rise.
</details>
<details>
<summary>摘要</summary>
雷达探测机上的echogram可以捕捉firn层的 Profiling ，这些层次是 ice sheet 的重要特征，用于计算降雪积累率，以确定 polar ice cap 的融化对海平面升高的贡献。但是，自动处理雷达echogram以检测 underlying firn层是一个具有挑战性的问题。在我们的工作中，我们开发了wavelet基的多尺度深度学习架构，用于改进firn层检测。我们表明，wavelet基架构可以提高optimal dataset scale (ODS) 和optimal image scale (OIS) F-score 的值，分别提高3.99%和3.7%。此外，我们提出的Skip-WaveNet架构在每个迭代中生成新的wavelet，实现了高度的泛化性，并且可以对firn层进行准确的深度估计，均方差误差为3.31像素和94.3%的均值精度。这种网络可以被科学家用于跟踪firn层，计算降雪积累率，估计ice sheet 的表面质量变化，并帮助预测全球海平面升高。
</details></li>
</ul>
<hr>
<h2 id="Disentangled-Counterfactual-Learning-for-Physical-Audiovisual-Commonsense-Reasoning"><a href="#Disentangled-Counterfactual-Learning-for-Physical-Audiovisual-Commonsense-Reasoning" class="headerlink" title="Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning"></a>Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19559">http://arxiv.org/abs/2310.19559</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Andy20178/DCL">https://github.com/Andy20178/DCL</a></li>
<li>paper_authors: Changsheng Lv, Shuai Zhang, Yapeng Tian, Mengshi Qi, Huadong Ma</li>
<li>for: 本研究提出了一种基于Counterfactual Learning的Disentangled Counterfactual Learning（DCL）方法，用于解决物理 audiovisual 常识逻辑问题。</li>
<li>methods: 我们提出的DCL方法使用了分解器，将视频分解成静态（时间不变）和动态（时间变化）因素，并采用了变量自动编码器（VAE）来最大化多modal数据之间的相互信息。此外，我们还引入了对抗学习模块，以增强模型的逻辑能力。</li>
<li>results: 我们的提出方法在实验中超越了基eline方法，并实现了状态之最好的性能。我们的源代码可以在<a target="_blank" rel="noopener" href="https://github.com/Andy20178/DCL%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Andy20178/DCL上获取。</a><details>
<summary>Abstract</summary>
In this paper, we propose a Disentangled Counterfactual Learning~(DCL) approach for physical audiovisual commonsense reasoning. The task aims to infer objects' physics commonsense based on both video and audio input, with the main challenge is how to imitate the reasoning ability of humans. Most of the current methods fail to take full advantage of different characteristics in multi-modal data, and lacking causal reasoning ability in models impedes the progress of implicit physical knowledge inferring. To address these issues, our proposed DCL method decouples videos into static (time-invariant) and dynamic (time-varying) factors in the latent space by the disentangled sequential encoder, which adopts a variational autoencoder (VAE) to maximize the mutual information with a contrastive loss function. Furthermore, we introduce a counterfactual learning module to augment the model's reasoning ability by modeling physical knowledge relationships among different objects under counterfactual intervention. Our proposed method is a plug-and-play module that can be incorporated into any baseline. In experiments, we show that our proposed method improves baseline methods and achieves state-of-the-art performance. Our source code is available at https://github.com/Andy20178/DCL.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种分离式Counterfactual Learning（DCL）方法，用于物理 audiovisual常识理解。任务的目标是根据视频和音频输入推理出物体的物理常识，主要挑战在于如何模仿人类的思维能力。现有的方法多数不能充分利用多个Modal数据的不同特征，同时模型中缺乏 causal 理解能力，这些问题限制了隐藏的物理知识推理的进步。为解决这些问题，我们提出的 DCL 方法在幂变编码器中分离视频 into 静态（时间不变）和动态（时间变化）因素，使用 VAE  maximize mutual information和对比损失函数。此外，我们引入了 counterfactual 学习模块，以模型物体之间的物理知识关系，并通过对假设性的 intervención来增强模型的推理能力。我们提出的方法是可以与任何基础模型集成的插件模块，在实验中，我们表明了我们的方法可以超越基础方法，达到领域的最佳性能。我们的源代码可以在 GitHub 上找到：https://github.com/Andy20178/DCL。
</details></li>
</ul>
<hr>
<h2 id="Harvest-Video-Foundation-Models-via-Efficient-Post-Pretraining"><a href="#Harvest-Video-Foundation-Models-via-Efficient-Post-Pretraining" class="headerlink" title="Harvest Video Foundation Models via Efficient Post-Pretraining"></a>Harvest Video Foundation Models via Efficient Post-Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19554">http://arxiv.org/abs/2310.19554</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/internvideo">https://github.com/opengvlab/internvideo</a></li>
<li>paper_authors: Yizhuo Li, Kunchang Li, Yinan He, Yi Wang, Yali Wang, Limin Wang, Yu Qiao, Ping Luo</li>
<li>for: 提高视频语言模型的生成效率和质量，以及大量视频语言数据的抽象和 reuse。</li>
<li>methods:  randomly dropping input video patches和masking out input text during the post-pretraining procedure，以促进视频语言融合学习。</li>
<li>results: 对多种零shot任务、视频问答和视频文本检索等多种视频语言下沉Task进行了广泛的实验 validate the effectiveness of our method，并 achieve state-of-the-art performances，与一些劳烈预训练的视频基础模型相当。<details>
<summary>Abstract</summary>
Building video-language foundation models is costly and difficult due to the redundant nature of video data and the lack of high-quality video-language datasets. In this paper, we propose an efficient framework to harvest video foundation models from image ones. Our method is intuitively simple by randomly dropping input video patches and masking out input text during the post-pretraining procedure. The patch dropping boosts the training efficiency significantly and text masking enforces the learning of cross-modal fusion. We conduct extensive experiments to validate the effectiveness of our method on a wide range of video-language downstream tasks including various zero-shot tasks, video question answering, and video-text retrieval. Despite its simplicity, our method achieves state-of-the-art performances, which are comparable to some heavily pretrained video foundation models. Our method is extremely efficient and can be trained in less than one day on 8 GPUs, requiring only WebVid-10M as pretraining data. We hope our method can serve as a simple yet strong counterpart for prevalent video foundation models, provide useful insights when building them, and make large pretrained models more accessible and sustainable. This is part of the InternVideo project \url{https://github.com/OpenGVLab/InternVideo}.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:建立视频语言基础模型的成本和困难在于视频数据的重复性和语言基础模型的缺乏高质量数据集。在这篇论文中，我们提出了一种高效的框架，可以从图像基础模型中提取视频基础模型。我们的方法是通过随机删除输入视频补丁和隐藏输入文本来增强训练效率。补丁删除提高了训练效率，而文本隐藏强制学习cross模式融合。我们进行了广泛的实验，以验证我们的方法在多种视频语言下游任务中的效果，包括多种零学习任务、视频问答和视频文本检索等。尽管我们的方法简单，但它可以与一些努力预训练的视频基础模型匹敌。我们的方法非常高效，可以在8个GPU上训练完成，只需要WebVid-10M作为预训练数据。我们希望我们的方法可以作为一种简单却强大的对手，为建立视频基础模型提供有用的指导，并使大型预训练模型更加可 accessible和可持续。这是InternVideo项目的一部分（https://github.com/OpenGVLab/InternVideo）。
</details></li>
</ul>
<hr>
<h2 id="MENTOR-Human-Perception-Guided-Pretraining-for-Iris-Presentation-Detection"><a href="#MENTOR-Human-Perception-Guided-Pretraining-for-Iris-Presentation-Detection" class="headerlink" title="MENTOR: Human Perception-Guided Pretraining for Iris Presentation Detection"></a>MENTOR: Human Perception-Guided Pretraining for Iris Presentation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19545">http://arxiv.org/abs/2310.19545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Colton R. Crum, Adam Czajka</li>
<li>for: 提高困难任务中的生物认证表示力，如人脸识别和 iris PAD。</li>
<li>methods: 利用人类注意力指导 CNN 模型的训练，包括两个唯一的训练轮。首先，我们训练一个自动编码器，以学习人类注意力地图。然后，我们使用这个表示来训练一个 iris PAD 模型，并使用这个模型作为一个人类注意力指导的注释工具。</li>
<li>results: 三重效果：(a) 使用人类注意力训练的 encoder 的 weights 对 iris PAD 性能有显著提高，比如 ImageNet 源或随机 initialization 的 weights。(b) 可以生成无限多个人类注意力地图，用于未看过的 iris PAD 样本。(c) 可以提高 iris PAD 模型训练的效率。<details>
<summary>Abstract</summary>
Incorporating human salience into the training of CNNs has boosted performance in difficult tasks such as biometric presentation attack detection. However, collecting human annotations is a laborious task, not to mention the questions of how and where (in the model architecture) to efficiently incorporate this information into model's training once annotations are obtained. In this paper, we introduce MENTOR (huMan pErceptioN-guided preTraining fOr iris pResentation attack detection), which addresses both of these issues through two unique rounds of training. First, we train an autoencoder to learn human saliency maps given an input iris image (both real and fake examples). Once this representation is learned, we utilize the trained autoencoder in two different ways: (a) as a pre-trained backbone for an iris presentation attack detector, and (b) as a human-inspired annotator of salient features on unknown data. We show that MENTOR's benefits are threefold: (a) significant boost in iris PAD performance when using the human perception-trained encoder's weights compared to general-purpose weights (e.g. ImageNet-sourced, or random), (b) capability of generating infinite number of human-like saliency maps for unseen iris PAD samples to be used in any human saliency-guided training paradigm, and (c) increase in efficiency of iris PAD model training. Sources codes and weights are offered along with the paper.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的 convolutional neural network (CNN) 在难度较高的任务中表现不佳，如生物 metric 表现攻击检测。然而，收集人类注释是一项劳顿的任务，更是问题是如何有效地将这些信息integrated into 模型的训练过程中。在这篇论文中，我们介绍了 MENTOR（huMan pErceptioN-guided preTraining fOr iris pResentation attack detection），它解决了这两个问题。我们使用两种不同的训练方法：首先，我们训练了一个 autoencoder，以学习人类注意力图 given 输入的 iris 图像（ both real 和 fake 例子）。然后，我们使用这个 representations 来训练一个 iris 表达攻击检测器，并使用这个模型来生成无数量的人类类似的注意力图。我们发现，MENTOR 具有以下三个优点：（a）在使用人类注意力训练后的encoder 的 weights 时，iris PAD 性能得到了明显的提高。（b）可以生成无数量的人类类似的注意力图，用于任何人类注意力导向的训练方法。（c）可以提高 iris PAD 模型训练的效率。我们提供了模型和 weights，以便在论文中使用。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Image-Related-Inductive-Biases-in-Single-Branch-Visual-Tracking"><a href="#Exploiting-Image-Related-Inductive-Biases-in-Single-Branch-Visual-Tracking" class="headerlink" title="Exploiting Image-Related Inductive Biases in Single-Branch Visual Tracking"></a>Exploiting Image-Related Inductive Biases in Single-Branch Visual Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19542">http://arxiv.org/abs/2310.19542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tchuanm/AViTMP">https://github.com/Tchuanm/AViTMP</a></li>
<li>paper_authors: Chuanming Tang, Kai Wang, Joost van de Weijer, Jianlin Zhang, Yongmei Huang<br>for:AViTMP is proposed to tackle the inferior effectiveness of the vanilla ViT in visual tracking tasks, by bridging the gap between single-branch networks and discriminative models.methods:The proposed AViTMP model uses an adaptor module and joint target state embedding in the encoder to enrich the dense embedding paradigm based on ViT, and combines it with a dense-fusion decoder and a discriminative target model to predict accurate location. Additionally, a novel inference pipeline called CycleTrack is presented to mitigate the limitations of conventional inference practice, and a dual-frame update inference strategy is proposed to handle significant challenges in long-term scenarios.results:The proposed AViTMP model achieves state-of-the-art performance in visual tracking tasks, especially on long-time tracking and robustness, as demonstrated by the experimental results on ten tracking benchmarks, including LaSOT, LaSOTExtSub, AVisT, etc.<details>
<summary>Abstract</summary>
Despite achieving state-of-the-art performance in visual tracking, recent single-branch trackers tend to overlook the weak prior assumptions associated with the Vision Transformer (ViT) encoder and inference pipeline. Moreover, the effectiveness of discriminative trackers remains constrained due to the adoption of the dual-branch pipeline. To tackle the inferior effectiveness of the vanilla ViT, we propose an Adaptive ViT Model Prediction tracker (AViTMP) to bridge the gap between single-branch network and discriminative models. Specifically, in the proposed encoder AViT-Enc, we introduce an adaptor module and joint target state embedding to enrich the dense embedding paradigm based on ViT. Then, we combine AViT-Enc with a dense-fusion decoder and a discriminative target model to predict accurate location. Further, to mitigate the limitations of conventional inference practice, we present a novel inference pipeline called CycleTrack, which bolsters the tracking robustness in the presence of distractors via bidirectional cycle tracking verification. Lastly, we propose a dual-frame update inference strategy that adeptively handles significant challenges in long-term scenarios. In the experiments, we evaluate AViTMP on ten tracking benchmarks for a comprehensive assessment, including LaSOT, LaSOTExtSub, AVisT, etc. The experimental results unequivocally establish that AViTMP attains state-of-the-art performance, especially on long-time tracking and robustness.
</details>
<details>
<summary>摘要</summary>
尽管最新的单支持器跟踪器达到了视觉跟踪的状态顶峰性能，但是最近的单支持器跟踪器往往忽略了视觉转换器（ViT）Encoder和推理管道中的弱优先级假设。此外，采用双支持器管道的效果仍然受限，不能够满足跟踪任务的需求。为了解决vanilla ViT的不足，我们提出了一种适应型ViT模型预测跟踪器（AViTMP），以填补单支持器网络和推理模型之间的差距。具体来说，在我们提出的AViT-Encoder中，我们添加了适应模块和联合目标状态嵌入，以激活ViT dense embedding的PARADIGM。然后，我们将AViT-Encoder与密集混合解码器和一种推理模型结合，以准确预测位置。此外，为了 Mitigate the limitations of conventional inference practice, we present a novel inference pipeline called CycleTrack, which bolsters the tracking robustness in the presence of distractors via bidirectional cycle tracking verification. Finally, we propose a dual-frame update inference strategy that adeptively handles significant challenges in long-term scenarios. In the experiments, we evaluate AViTMP on ten tracking benchmarks for a comprehensive assessment, including LaSOT, LaSOTExtSub, AVisT, etc. The experimental results unequivocally establish that AViTMP attains state-of-the-art performance, especially on long-time tracking and robustness.
</details></li>
</ul>
<hr>
<h2 id="IterInv-Iterative-Inversion-for-Pixel-Level-T2I-Models"><a href="#IterInv-Iterative-Inversion-for-Pixel-Level-T2I-Models" class="headerlink" title="IterInv: Iterative Inversion for Pixel-Level T2I Models"></a>IterInv: Iterative Inversion for Pixel-Level T2I Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19540">http://arxiv.org/abs/2310.19540</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tchuanm/IterInv">https://github.com/Tchuanm/IterInv</a></li>
<li>paper_authors: Chuanming Tang, Kai Wang, Joost van de Weijer</li>
<li>for: 提高图像生成领域中的图像编辑能力，允许用户通过修改输入文本来控制生成的图像。</li>
<li>methods: 基于Latent Diffusion Models (LDM)的图像生成模型，以及Imagen和DeepFloyd-IF等图像增强模型。</li>
<li>results: 提出了一种基于迭代 concatenation 的图像逆变换技术（IterInv），并证明了IterInv 可以与流行的图像编辑方法结合使用，提高图像生成和编辑的能力。<details>
<summary>Abstract</summary>
Large-scale text-to-image diffusion models have been a ground-breaking development in generating convincing images following an input text prompt. The goal of image editing research is to give users control over the generated images by modifying the text prompt. Current image editing techniques are relying on DDIM inversion as a common practice based on the Latent Diffusion Models (LDM). However, the large pretrained T2I models working on the latent space as LDM suffer from losing details due to the first compression stage with an autoencoder mechanism. Instead, another mainstream T2I pipeline working on the pixel level, such as Imagen and DeepFloyd-IF, avoids this problem. They are commonly composed of several stages, normally with a text-to-image stage followed by several super-resolution stages. In this case, the DDIM inversion is unable to find the initial noise to generate the original image given that the super-resolution diffusion models are not compatible with the DDIM technique. According to our experimental findings, iteratively concatenating the noisy image as the condition is the root of this problem. Based on this observation, we develop an iterative inversion (IterInv) technique for this stream of T2I models and verify IterInv with the open-source DeepFloyd-IF model. By combining our method IterInv with a popular image editing method, we prove the application prospects of IterInv. The code will be released at \url{https://github.com/Tchuanm/IterInv.git}.
</details>
<details>
<summary>摘要</summary>
大规模文本到图像扩散模型已经是图像生成领域的一项重要发展，可以生成基于输入文本提示的真实的图像。图像编辑研究的目标是给用户控制生成图像的文本提示。现有的图像编辑技术多 rely于 DDIM 逆转，基于缺失扩散模型（LDM）。然而，大规模预训练 T2I 模型在幽parallel space中作为 LDM 受到压缩的影响，导致失去细节。相比之下，另一种主流 T2I 管道在像素级别进行操作，如 Imagen 和 DeepFloyd-IF，可以避免这个问题。它们通常包括一个文本到图像阶段，然后跟踪多个超分辨阶段。在这种情况下，DDIM 逆转无法找到初始噪声，生成原始图像。根据我们的实验发现，逐次 concatenate 噪声图像作为条件是这个问题的根本原因。基于这一观察，我们开发了一种逐次逆转（IterInv）技术，并验证 IterInv 与开源的 DeepFloyd-IF 模型。通过将我们的方法 IterInv 与一种流行的图像编辑方法相结合，我们证明了 IterInv 的应用前景。代码将在 \url{https://github.com/Tchuanm/IterInv.git} 上发布。
</details></li>
</ul>
<hr>
<h2 id="Revitalizing-Legacy-Video-Content-Deinterlacing-with-Bidirectional-Information-Propagation"><a href="#Revitalizing-Legacy-Video-Content-Deinterlacing-with-Bidirectional-Information-Propagation" class="headerlink" title="Revitalizing Legacy Video Content: Deinterlacing with Bidirectional Information Propagation"></a>Revitalizing Legacy Video Content: Deinterlacing with Bidirectional Information Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19535">http://arxiv.org/abs/2310.19535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaowei Gao, Mingyang Song, Christopher Schroers, Yang Zhang</li>
<li>for: 这个论文是为了提高旧视频内容中缺失信息的还原，以提高现代显示器的显示质量。</li>
<li>methods: 该论文使用深度学习技术，提出了一种基于流场指导的修正块（FRB），用于进行特征修正，包括对适应、融合和修正。</li>
<li>results: 实验结果表明，该方法比现有方法有更高的性能。<details>
<summary>Abstract</summary>
Due to old CRT display technology and limited transmission bandwidth, early film and TV broadcasts commonly used interlaced scanning. This meant each field contained only half of the information. Since modern displays require full frames, this has spurred research into deinterlacing, i.e. restoring the missing information in legacy video content. In this paper, we present a deep-learning-based method for deinterlacing animated and live-action content. Our proposed method supports bidirectional spatio-temporal information propagation across multiple scales to leverage information in both space and time. More specifically, we design a Flow-guided Refinement Block (FRB) which performs feature refinement including alignment, fusion, and rectification. Additionally, our method can process multiple fields simultaneously, reducing per-frame processing time, and potentially enabling real-time processing. Our experimental results demonstrate that our proposed method achieves superior performance compared to existing methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Are-Natural-Domain-Foundation-Models-Useful-for-Medical-Image-Classification"><a href="#Are-Natural-Domain-Foundation-Models-Useful-for-Medical-Image-Classification" class="headerlink" title="Are Natural Domain Foundation Models Useful for Medical Image Classification?"></a>Are Natural Domain Foundation Models Useful for Medical Image Classification?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19522">http://arxiv.org/abs/2310.19522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joana Palés Huix, Adithya Raju Ganeshan, Johan Fredin Haslum, Magnus Söderberg, Christos Matsoukas, Kevin Smith</li>
<li>for: 这篇论文旨在 investigate 各种现有的基础模型在医疗影像分类任务上的转移性。</li>
<li>methods: 本研究使用了五种基础模型，namely SAM、SEEM、DINOv2、BLIP、OpenCLIP，并在四个已知的医疗影像数据集上进行评估。不同的训练设定也被探索以扩大这些模型的潜力。</li>
<li>results: 研究结果显示DINOv2在特定的训练设定下表现出色，常常超越标准做法的ImageNet预训练。但是其他基础模型对医疗影像分类任务的转移性较差，无法一致地超越ImageNet预训练的基准。<details>
<summary>Abstract</summary>
The deep learning field is converging towards the use of general foundation models that can be easily adapted for diverse tasks. While this paradigm shift has become common practice within the field of natural language processing, progress has been slower in computer vision. In this paper we attempt to address this issue by investigating the transferability of various state-of-the-art foundation models to medical image classification tasks. Specifically, we evaluate the performance of five foundation models, namely SAM, SEEM, DINOv2, BLIP, and OpenCLIP across four well-established medical imaging datasets. We explore different training settings to fully harness the potential of these models. Our study shows mixed results. DINOv2 in particular, consistently outperforms the standard practice of ImageNet pretraining. However, other foundation models failed to consistently beat this established baseline indicating limitations in their transferability to medical image classification tasks.
</details>
<details>
<summary>摘要</summary>
深度学习领域正在转向通用基础模型的使用，以便轻松地适应多种任务。在自然语言处理领域中，这种思想转变已经成为惯例，但在计算机视觉领域，进步 slower。本文我们尝试解决这个问题，通过对不同基础模型的传输性进行研究。我们评估了五种基础模型，namely SAM、SEEM、DINOv2、BLIP 和 OpenCLIP，在四个已知的医疗影像集合上进行了评估。我们探索了不同的训练设置，以充分发挥这些模型的潜力。我们的研究显示了混合的结果。DINOv2 特别是，在医疗影像分类任务中一直表现出色，超过了标准实践的 ImageNet 预训练。然而，其他基础模型未能一直表现出色，表明它们在医疗影像分类任务中的传输性有限。
</details></li>
</ul>
<hr>
<h2 id="Generating-Context-Aware-Natural-Answers-for-Questions-in-3D-Scenes"><a href="#Generating-Context-Aware-Natural-Answers-for-Questions-in-3D-Scenes" class="headerlink" title="Generating Context-Aware Natural Answers for Questions in 3D Scenes"></a>Generating Context-Aware Natural Answers for Questions in 3D Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19516">http://arxiv.org/abs/2310.19516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Munzer Dwedari, Matthias Niessner, Dave Zhenyu Chen</li>
<li>for:  Answering questions in 3D scenes naturally and freely, without being limited to pre-defined answers.</li>
<li>methods:  Converted the question answering task into a sequence generation task, and optimized the model directly on language rewards to ensure global sentence semantics. Additionally, a pragmatic language understanding reward was adapted to improve sentence quality.</li>
<li>results:  Set a new SOTA (State of the Art) on the ScanQA benchmark with a CIDEr score of 72.22&#x2F;66.57 on the test sets.<details>
<summary>Abstract</summary>
3D question answering is a young field in 3D vision-language that is yet to be explored. Previous methods are limited to a pre-defined answer space and cannot generate answers naturally. In this work, we pivot the question answering task to a sequence generation task to generate free-form natural answers for questions in 3D scenes (Gen3DQA). To this end, we optimize our model directly on the language rewards to secure the global sentence semantics. Here, we also adapt a pragmatic language understanding reward to further improve the sentence quality. Our method sets a new SOTA on the ScanQA benchmark (CIDEr score 72.22/66.57 on the test sets).
</details>
<details>
<summary>摘要</summary>
三维问答是一个年轻的领域，尚未得到充分开发。先前的方法受限于预先定义的答案空间，无法自然生成答案。在这种工作中，我们将问答任务转换为一种序列生成任务，以便在三维场景中自然生成免束答案（Gen3DQA）。为此，我们直接优化我们的模型以获取语言奖励，以保证全句 semantics。此外，我们还适应了一种 Pragmatic 语言理解奖励，以进一步提高句子质量。我们的方法在 ScanQA  benchmark 上设置了新的 SOTA（CIDEr 分数 72.22/66.57）。
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-nowcasting-of-radar-composites-from-satellite-images-for-severe-weather"><a href="#Transformer-based-nowcasting-of-radar-composites-from-satellite-images-for-severe-weather" class="headerlink" title="Transformer-based nowcasting of radar composites from satellite images for severe weather"></a>Transformer-based nowcasting of radar composites from satellite images for severe weather</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19515">http://arxiv.org/abs/2310.19515</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caglarkucuk/earthformer-satellite-to-radar">https://github.com/caglarkucuk/earthformer-satellite-to-radar</a></li>
<li>paper_authors: Çağlar Küçük, Apostolos Giannakos, Stefan Schneider, Alexander Jann</li>
<li>for: 这个研究是为了提高预报技术，将天气卫星数据与地面雷达数据融合，以提供高精度的预报。</li>
<li>methods: 这个研究使用了Transformer数据分析模型，利用天气卫星数据进行预报。</li>
<li>results: 研究发现，使用这种模型可以对不同的天气情况进行高精度的预报，并且具有耐变性和复杂数据结构的能力。<details>
<summary>Abstract</summary>
Weather radar data are critical for nowcasting and an integral component of numerical weather prediction models. While weather radar data provide valuable information at high resolution, their ground-based nature limits their availability, which impedes large-scale applications. In contrast, meteorological satellites cover larger domains but with coarser resolution.   However, with the rapid advancements in data-driven methodologies and modern sensors aboard geostationary satellites, new opportunities are emerging to bridge the gap between ground- and space-based observations, ultimately leading to more skillful weather prediction with high accuracy.   Here, we present a Transformer-based model for nowcasting ground-based radar image sequences using satellite data up to two hours lead time. Trained on a dataset reflecting severe weather conditions, the model predicts radar fields occurring under different weather phenomena and shows robustness against rapidly growing/decaying fields and complex field structures.   Model interpretation reveals that the infrared channel centered at 10.3 $\mu m$ (C13) contains skillful information for all weather conditions, while lightning data have the highest relative feature importance in severe weather conditions, particularly in shorter lead times.   The model can support precipitation nowcasting across large domains without an explicit need for radar towers, enhance numerical weather prediction and hydrological models, and provide radar proxy for data-scarce regions. Moreover, the open-source framework facilitates progress towards operational data-driven nowcasting.
</details>
<details>
<summary>摘要</summary>
天气雷达数据是现场预报中的关键参数，也是数值天气预测模型中的重要组成部分。 Although 天气雷达数据提供高分辨率的信息，它们的地面性限制了它们的可用性，这限制了大规模应用。 相比之下，天气卫星覆盖的范围更大，但是其分辨率相对较低。 然而，随着数据驱动方法的快速发展和现代气象卫星上的先进感知器，新的机遇正在出现，可以 bridging 天气雷达和空间天气观测之间的差距，从而实现更准确的天气预测。 在这篇文章中，我们提出了一种基于变换器的模型，用于预测基于雷达图像序列的现场预报，使用卫星数据作为至多两个小时的领先时间。 模型在不同的天气现象下预测雷达场，并且在快速增长/衰退的场和复杂场结构下表现稳定。 模型解释显示，10.3微米的紫外通道（C13）含有有用的信息，而闪电数据在严重天气情况下具有最高相对特征重要性。 该模型可以在大规模应用中支持降水预报，不需要明确的雷达天线，提高数值天气预测和水文模型，并提供数据缺乏地区的雷达代理。 此外，开源框架使得操作数据驱动现场预报的进步更加容易。
</details></li>
</ul>
<hr>
<h2 id="VideoCrafter1-Open-Diffusion-Models-for-High-Quality-Video-Generation"><a href="#VideoCrafter1-Open-Diffusion-Models-for-High-Quality-Video-Generation" class="headerlink" title="VideoCrafter1: Open Diffusion Models for High-Quality Video Generation"></a>VideoCrafter1: Open Diffusion Models for High-Quality Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19512">http://arxiv.org/abs/2310.19512</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ailab-cvc/videocrafter">https://github.com/ailab-cvc/videocrafter</a></li>
<li>paper_authors: Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, Ying Shan</li>
<li>for: The paper is written for researchers and engineers who are interested in video generation, specifically those looking for open-source models.</li>
<li>methods: The paper introduces two diffusion models for high-quality video generation: text-to-video (T2V) and image-to-video (I2V) models. The T2V model synthesizes a video based on a given text input, while the I2V model incorporates an additional image input to produce videos that strictly adhere to the content of the provided reference image.</li>
<li>results: The proposed T2V model can generate realistic and cinematic-quality videos with a resolution of $1024 \times 576$, outperforming other open-source T2V models in terms of quality. The I2V model is the first open-source I2V foundation model capable of transforming a given image into a video clip while maintaining content preservation constraints.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文是为研究者和工程师们，尤其是那些关注视频生成技术的人写的。</li>
<li>methods: 这篇论文介绍了两种扩散模型，用于高质量视频生成：文本到视频（T2V）模型和图像到视频（I2V）模型。T2V模型将文本输入转化为视频，而I2V模型将图像输入转化为视频，并且保持图像内容、结构和风格不变。</li>
<li>results: 提出的T2V模型可以生成高质量和电影质量的视频，分辨率为$1024 \times 576$，超过了其他开源T2V模型的质量。I2V模型是首个开源I2V基础模型，可以将给定的图像转化为视频clip，并且保持内容不变。<details>
<summary>Abstract</summary>
Video generation has increasingly gained interest in both academia and industry. Although commercial tools can generate plausible videos, there is a limited number of open-source models available for researchers and engineers. In this work, we introduce two diffusion models for high-quality video generation, namely text-to-video (T2V) and image-to-video (I2V) models. T2V models synthesize a video based on a given text input, while I2V models incorporate an additional image input. Our proposed T2V model can generate realistic and cinematic-quality videos with a resolution of $1024 \times 576$, outperforming other open-source T2V models in terms of quality. The I2V model is designed to produce videos that strictly adhere to the content of the provided reference image, preserving its content, structure, and style. This model is the first open-source I2V foundation model capable of transforming a given image into a video clip while maintaining content preservation constraints. We believe that these open-source video generation models will contribute significantly to the technological advancements within the community.
</details>
<details>
<summary>摘要</summary>
视频生成已经在学术和行业中引起了越来越多的兴趣。虽然商业工具可以生成可信度很高的视频，但是学术研究和工程师可以使用的开源模型却有限。在这项工作中，我们介绍了两种扩散模型用于高质量视频生成，即文本到视频（T2V）模型和图像到视频（I2V）模型。T2V模型将文本输入转化成视频，而I2V模型具有额外的图像输入。我们的提议的T2V模型可以生成高质量和电影级的视频，分辨率为1024×576，超越其他开源T2V模型。I2V模型是首个能够将给定图像转化成视频clip，保持内容、结构和风格的开源基础模型。我们认为这些开源视频生成模型会对社区技术进步产生重要贡献。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Visual-Navigation-of-Underwater-Robots"><a href="#Deep-Learning-for-Visual-Navigation-of-Underwater-Robots" class="headerlink" title="Deep Learning for Visual Navigation of Underwater Robots"></a>Deep Learning for Visual Navigation of Underwater Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19495">http://arxiv.org/abs/2310.19495</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Sunbeam</li>
<li>for: 这篇论文旨在简要报告深度学习方法在水下机器人视觉导航中的应用。</li>
<li>methods: 该论文涵盖了深度学习方法在水下机器人视觉导航中的应用，包括可用的水下视图数据集，模仿学习和奖励学习方法 для导航。此外，文献将根据学习方法的类别，对水下机器人的训练方法进行分类。</li>
<li>results: 文献将对水下机器人的视觉导航进行概述，并提供相关的研究成果和评估。不包括使用深度学习算法处理非视觉数据的文献，除非作为对照例子。<details>
<summary>Abstract</summary>
This paper aims to briefly survey deep learning methods for visual navigation of underwater robotics. The scope of this paper includes the visual perception of underwater robotics with deep learning methods, the available visual underwater datasets, imitation learning, and reinforcement learning methods for navigation. Additionally, relevant works will be categorized under the imitation learning or deep learning paradigm for underwater robots for clarity of the training methodologies in the current landscape. Literature that uses deep learning algorithms to process non-visual data for underwater navigation will not be considered, except as contrasting examples.
</details>
<details>
<summary>摘要</summary>
这篇论文目的是 briefly 概括深度学习方法对水下机器人视觉导航。论文的范围包括水下机器人视觉深度学习方法、可用的水下视数据集、仿真学习和奖励学习方法 для 导航。此外，相关的工作会被分类为仿真学习或深度学习 paradigm 下的训练方法，以便在当前领域的训练方法之间进行清晰的分类。文献使用深度学习算法处理非视觉数据进行水下导航将不被考虑，除非作为对照例外。
</details></li>
</ul>
<hr>
<h2 id="VDIP-TGV-Blind-Image-Deconvolution-via-Variational-Deep-Image-Prior-Empowered-by-Total-Generalized-Variation"><a href="#VDIP-TGV-Blind-Image-Deconvolution-via-Variational-Deep-Image-Prior-Empowered-by-Total-Generalized-Variation" class="headerlink" title="VDIP-TGV: Blind Image Deconvolution via Variational Deep Image Prior Empowered by Total Generalized Variation"></a>VDIP-TGV: Blind Image Deconvolution via Variational Deep Image Prior Empowered by Total Generalized Variation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19477">http://arxiv.org/abs/2310.19477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingting Wu, Zhiyan Du, Zhi Li, Feng-Lei Fan, Tieyong Zeng</li>
<li>for: 提高零掩蔽图像的清晰度和细节表现</li>
<li>methods: 使用变分深度图像优先（VDIP）和总泛化变分（TGV）regularization，并使用 alternate direction method of multipliers（ADMM）来解决问题</li>
<li>results: 比较多种现有模型，提出VDIP-TGV模型，实验表明VDIP-TGV模型可以更好地重建图像的细节和Edge，并且可以更好地处理大blur kernel的情况<details>
<summary>Abstract</summary>
Recovering clear images from blurry ones with an unknown blur kernel is a challenging problem. Deep image prior (DIP) proposes to use the deep network as a regularizer for a single image rather than as a supervised model, which achieves encouraging results in the nonblind deblurring problem. However, since the relationship between images and the network architectures is unclear, it is hard to find a suitable architecture to provide sufficient constraints on the estimated blur kernels and clean images. Also, DIP uses the sparse maximum a posteriori (MAP), which is insufficient to enforce the selection of the recovery image. Recently, variational deep image prior (VDIP) was proposed to impose constraints on both blur kernels and recovery images and take the standard deviation of the image into account during the optimization process by the variational principle. However, we empirically find that VDIP struggles with processing image details and tends to generate suboptimal results when the blur kernel is large. Therefore, we combine total generalized variational (TGV) regularization with VDIP in this paper to overcome these shortcomings of VDIP. TGV is a flexible regularization that utilizes the characteristics of partial derivatives of varying orders to regularize images at different scales, reducing oil painting artifacts while maintaining sharp edges. The proposed VDIP-TGV effectively recovers image edges and details by supplementing extra gradient information through TGV. Additionally, this model is solved by the alternating direction method of multipliers (ADMM), which effectively combines traditional algorithms and deep learning methods. Experiments show that our proposed VDIP-TGV surpasses various state-of-the-art models quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
recuperar imagens claras a partir de imagens borrosas com um kernel de blur desconhecido é um problema desafiador. A prioridade de imagem profunda (DIP) propõe usar a rede profunda como um regularizador para uma imagem individual em vez de um modelo de treinamento supervisionado, o que alcança resultados animadores no problema de desblurimento não cego. No entanto, desde que a relação entre as imagens e as arquiteturas de rede é incerta, é difícil encontrar uma arquitetura adequada para fornecer restrições suficientes sobre os kernel de blur e as imagens limpas. Além disso, DIP usa a máxima a posteriori esparsa (MAP), o que é insuficiente para impor a seleção da imagem de recuperação.Recentemente, o prior de imagem profunda variável (VDIP) foi proposto para impor restrições sobre os kernel de blur e as imagens de recuperação e considerar a variância da imagem durante o processo de otimização pelo princípio variacional. No entanto, encontramos empremicamente que VDIP tem dificuldade em processar detalhes de imagem e tende a gerar resultados subótimos quando o kernel de blur é grande. Portanto, combinamos a regularização de tipo geral variacional (TGV) com VDIP neste artigo para superar as limitações de VDIP. TGV é uma regularização flexível que utiliza as características de derivadas parciais de ordens variables para regularizar imagens em diferentes escalas, reduzindo artefatos de pintura de óleo enquanto mantém bordos afiados. A nossa propriedade VDIP-TGV eficazmente recupera bordos e detalhes de imagem adicionando informações de gradiente extra através de TGV. Além disso, este modelo é resolvido pelo método de direções alternadas de multiplicadores (ADMM), que eficazmente combina métodos tradicionais e de aprendizado profundo. Os resultados experimentais mostram que nossa propriedade VDIP-TGV ultrapassa modelos estado-da-arte quantitativamente e qualitativamente.
</details></li>
</ul>
<hr>
<h2 id="Generative-Neural-Fields-by-Mixtures-of-Neural-Implicit-Functions"><a href="#Generative-Neural-Fields-by-Mixtures-of-Neural-Implicit-Functions" class="headerlink" title="Generative Neural Fields by Mixtures of Neural Implicit Functions"></a>Generative Neural Fields by Mixtures of Neural Implicit Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19464">http://arxiv.org/abs/2310.19464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tackgeun/mNIF">https://github.com/tackgeun/mNIF</a></li>
<li>paper_authors: Tackgeun You, Mijeong Kim, Jungtaek Kim, Bohyung Han</li>
<li>for: 本文提出了一种新的生成神经场方法，用于学习基于线性组合的隐藏基准网络。</li>
<li>methods: 该方法使用meta-学习或自动解码方法学习基准网络和其系数在隐藏空间，并通过Weighted Model Averaging来减少推理时间和内存占用。</li>
<li>results: 实验表明，该方法在多种图像、体量数据和NeRF场景上达到了竞争性的生成性能，而且不需要特殊的Modalities和Domain设计。<details>
<summary>Abstract</summary>
We propose a novel approach to learning the generative neural fields represented by linear combinations of implicit basis networks. Our algorithm learns basis networks in the form of implicit neural representations and their coefficients in a latent space by either conducting meta-learning or adopting auto-decoding paradigms. The proposed method easily enlarges the capacity of generative neural fields by increasing the number of basis networks while maintaining the size of a network for inference to be small through their weighted model averaging. Consequently, sampling instances using the model is efficient in terms of latency and memory footprint. Moreover, we customize denoising diffusion probabilistic model for a target task to sample latent mixture coefficients, which allows our final model to generate unseen data effectively. Experiments show that our approach achieves competitive generation performance on diverse benchmarks for images, voxel data, and NeRF scenes without sophisticated designs for specific modalities and domains.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于学习生成神经场的线性组合卷积神经网络表示。我们的算法在幂 Learning 或自动解码模式下学习基准网络的卷积神经表示和其权重在隐藏空间的系数，从而轻松地扩大生成神经场的容量。通过对模型进行权重平均，我们可以在推理时保持网络的大小小于原始模型，同时在采样实例时实现高效的延迟和内存占用。此外，我们可以根据目标任务自定义锈推散概率模型，从而在最终模型中生成未seen数据，并且实现了在多种模式和领域上的竞争性生成性能。实验结果表明，我们的方法在多种图像、体积数据和NeRF场景上都可以 дости得竞争性的生成性能，无需特殊的设计 для特定的Modalities和领域。
</details></li>
</ul>
<hr>
<h2 id="Towards-Grouping-in-Large-Scenes-with-Occlusion-aware-Spatio-temporal-Transformers"><a href="#Towards-Grouping-in-Large-Scenes-with-Occlusion-aware-Spatio-temporal-Transformers" class="headerlink" title="Towards Grouping in Large Scenes with Occlusion-aware Spatio-temporal Transformers"></a>Towards Grouping in Large Scenes with Occlusion-aware Spatio-temporal Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19447">http://arxiv.org/abs/2310.19447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinsong Zhang, Lingfeng Gu, Yu-Kun Lai, Xueyang Wang, Kun Li</li>
<li>for: 这个研究旨在提出一个终端框架GroupTransformer，用于大规模场景中的群体检测，以便实现公共安全和智能城市等应用。</li>
<li>methods: 这个方法使用了 occlusion encoder 来探测和抑制严重遮蔽的人像照片，以及 spatio-temporal transformers 来同时提取路径信息和融合人际特征在层次结构中。</li>
<li>results: 实验结果显示，GroupTransformer 在大规模场景和小规模场景都能够提高表现，比如精度和 F1 分数在大规模场景上提高了 més de 10%，在小规模场景上提高了 més de 5%。<details>
<summary>Abstract</summary>
Group detection, especially for large-scale scenes, has many potential applications for public safety and smart cities. Existing methods fail to cope with frequent occlusions in large-scale scenes with multiple people, and are difficult to effectively utilize spatio-temporal information. In this paper, we propose an end-to-end framework,GroupTransformer, for group detection in large-scale scenes. To deal with the frequent occlusions caused by multiple people, we design an occlusion encoder to detect and suppress severely occluded person crops. To explore the potential spatio-temporal relationship, we propose spatio-temporal transformers to simultaneously extract trajectory information and fuse inter-person features in a hierarchical manner. Experimental results on both large-scale and small-scale scenes demonstrate that our method achieves better performance compared with state-of-the-art methods. On large-scale scenes, our method significantly boosts the performance in terms of precision and F1 score by more than 10%. On small-scale scenes, our method still improves the performance of F1 score by more than 5%. The project page with code can be found at http://cic.tju.edu.cn/faculty/likun/projects/GroupTrans.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Group detection, especially for large-scale scenes, has many potential applications for public safety and smart cities. Existing methods fail to cope with frequent occlusions in large-scale scenes with multiple people, and are difficult to effectively utilize spatio-temporal information. In this paper, we propose an end-to-end framework,GroupTransformer, for group detection in large-scale scenes. To deal with the frequent occlusions caused by multiple people, we design an occlusion encoder to detect and suppress severely occluded person crops. To explore the potential spatio-temporal relationship, we propose spatio-temporal transformers to simultaneously extract trajectory information and fuse inter-person features in a hierarchical manner. Experimental results on both large-scale and small-scale scenes demonstrate that our method achieves better performance compared with state-of-the-art methods. On large-scale scenes, our method significantly boosts the performance in terms of precision and F1 score by more than 10%. On small-scale scenes, our method still improves the performance of F1 score by more than 5%. The project page with code can be found at http://cic.tju.edu.cn/faculty/likun/projects/GroupTrans." into Simplified Chinese.<<SYS>>大规模场景中的集群检测具有许多公共安全和智能城市应用的潜在应用场景。现有方法在大规模场景中频繁受到多人干扰，并且Difficult to effectively utilize spatio-temporal information。在这篇论文中，我们提出了一个端到端框架，GroupTransformer，用于大规模场景中的集群检测。为了处理由多人引起的频繁干扰，我们设计了一个干扰编码器，用于检测并Suppress严重干扰人裁剪。以探索可能的空间时间关系，我们提出了空间时间变换器，用于同时提取轨迹信息并融合人员之间的特征。实验结果表明，我们的方法在大规模场景和小规模场景上都有更好的性能，相比于当前最佳方法。在大规模场景上，我们的方法可以提高精度和F1分数的性能，高于10%。在小规模场景上，我们的方法仍然可以提高F1分数的性能，高于5%。项目页面和代码可以在http://cic.tju.edu.cn/faculty/likun/projects/GroupTrans中找到。
</details></li>
</ul>
<hr>
<h2 id="One-for-All-Bridge-the-Gap-Between-Heterogeneous-Architectures-in-Knowledge-Distillation"><a href="#One-for-All-Bridge-the-Gap-Between-Heterogeneous-Architectures-in-Knowledge-Distillation" class="headerlink" title="One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation"></a>One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19444">http://arxiv.org/abs/2310.19444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hao840/ofakd">https://github.com/hao840/ofakd</a></li>
<li>paper_authors: Zhiwei Hao, Jianyuan Guo, Kai Han, Yehui Tang, Han Hu, Yunhe Wang, Chang Xu</li>
<li>for: 提高模型性能通过教师学生训练方式</li>
<li>methods: 使用中心kernel对比法 comparing the learned features between heterogeneous teacher and student models, and proposing a simple yet effective one-for-all KD framework called OFA-KD</li>
<li>results: 对不同架构的模型进行知识储备，并且可以获得显著的性能提升（最大提升率为8.0%在CIFAR-100 dataset和0.7%在ImageNet-1K dataset）<details>
<summary>Abstract</summary>
Knowledge distillation~(KD) has proven to be a highly effective approach for enhancing model performance through a teacher-student training scheme. However, most existing distillation methods are designed under the assumption that the teacher and student models belong to the same model family, particularly the hint-based approaches. By using centered kernel alignment (CKA) to compare the learned features between heterogeneous teacher and student models, we observe significant feature divergence. This divergence illustrates the ineffectiveness of previous hint-based methods in cross-architecture distillation. To tackle the challenge in distilling heterogeneous models, we propose a simple yet effective one-for-all KD framework called OFA-KD, which significantly improves the distillation performance between heterogeneous architectures. Specifically, we project intermediate features into an aligned latent space such as the logits space, where architecture-specific information is discarded. Additionally, we introduce an adaptive target enhancement scheme to prevent the student from being disturbed by irrelevant information. Extensive experiments with various architectures, including CNN, Transformer, and MLP, demonstrate the superiority of our OFA-KD framework in enabling distillation between heterogeneous architectures. Specifically, when equipped with our OFA-KD, the student models achieve notable performance improvements, with a maximum gain of 8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset. PyTorch code and checkpoints can be found at https://github.com/Hao840/OFAKD.
</details>
<details>
<summary>摘要</summary>
知识塑化（KD）已经证明是一种非常有效的方法，可以通过教师学生训练方案提高模型性能。然而，大多数现有的塑化方法假设教师和学生模型属于同一种模型家族，特别是使用提示方法。我们使用中心kernels对比（CKA）来比较教师和学生模型学习的特征，我们发现了显著的特征分化。这种分化表明了之前的提示方法在cross-architecture塑化中的不足。为了解决塑化不同模型的挑战，我们提出了一个简单 yet 有效的一forall KD框架，叫做OFA-KD。我们将中间特征 проек到一个对齐的特征空间，例如logits空间，其中抹除了建筑特定的信息。此外，我们引入一种适应目标增强方案，以避免学生被不相关信息所干扰。我们在不同的建筑，包括CNN、Transformer和MLP，进行了广泛的实验，并证明了我们的OFA-KD框架在不同建筑之间的塑化中具有显著的优势。具体来说，当我们的OFA-KD框架与学生模型结合使用时，学生模型在CIFAR-100数据集上得到了显著的性能提高，最大提高为8.0%，而在ImageNet-1K数据集上的提高为0.7%。PyTorch代码和检查点可以在https://github.com/Hao840/OFAKD中找到。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Gaussian-Splatting-from-Markerless-Motion-Capture-can-Reconstruct-Infants-Movements"><a href="#Dynamic-Gaussian-Splatting-from-Markerless-Motion-Capture-can-Reconstruct-Infants-Movements" class="headerlink" title="Dynamic Gaussian Splatting from Markerless Motion Capture can Reconstruct Infants Movements"></a>Dynamic Gaussian Splatting from Markerless Motion Capture can Reconstruct Infants Movements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19441">http://arxiv.org/abs/2310.19441</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. James Cotton, Colleen Peyton</li>
<li>for: 这个研究的目的是为了发展更进一步的运动分析工具，以便应用于多元化的临床人口，包括婴儿和新生儿。</li>
<li>methods: 这个研究使用了动态戈氏抛范法来处理缺乏标签的动作捕捉数据，并且使用 semantic segmentation 库来专注在婴儿身上，从而提高了场景的初始化。</li>
<li>results: 研究结果显示了这种方法在生成新的景象和追踪婴儿运动方面的潜力，这些结果显示了这种方法的应用可能性。<details>
<summary>Abstract</summary>
Easy access to precise 3D tracking of movement could benefit many aspects of rehabilitation. A challenge to achieving this goal is that while there are many datasets and pretrained algorithms for able-bodied adults, algorithms trained on these datasets often fail to generalize to clinical populations including people with disabilities, infants, and neonates. Reliable movement analysis of infants and neonates is important as spontaneous movement behavior is an important indicator of neurological function and neurodevelopmental disability, which can help guide early interventions. We explored the application of dynamic Gaussian splatting to sparse markerless motion capture (MMC) data. Our approach leverages semantic segmentation masks to focus on the infant, significantly improving the initialization of the scene. Our results demonstrate the potential of this method in rendering novel views of scenes and tracking infant movements. This work paves the way for advanced movement analysis tools that can be applied to diverse clinical populations, with a particular emphasis on early detection in infants.
</details>
<details>
<summary>摘要</summary>
提高患者的移动跟踪精度可以帮助许多rehabilitation领域。一个挑战是，虽然有很多 Datasets 和预训练算法 дляabled-bodied adults，但这些算法经常无法泛化到临床 популяции，包括残疾人、婴儿和新生儿。婴儿和新生儿的自发运动行为对神经学功能和发展障碍的评估具有重要意义，可以帮助早期 intervene。我们 explore了使用动态 Gaussian splatting 将 sparse markerless motion capture (MMC) 数据应用于实际场景中。我们的方法利用 semantic segmentation masks 将注意力集中在婴儿身上，大幅提高场景的初始化。我们的结果表明这种方法在生成新视图和跟踪婴儿运动方面具有潜在的潜力。这项工作为诊断和治疗各种临床 популяции提供了新的动力，特别是在早期诊断婴儿中。
</details></li>
</ul>
<hr>
<h2 id="GaitFormer-Learning-Gait-Representations-with-Noisy-Multi-Task-Learning"><a href="#GaitFormer-Learning-Gait-Representations-with-Noisy-Multi-Task-Learning" class="headerlink" title="GaitFormer: Learning Gait Representations with Noisy Multi-Task Learning"></a>GaitFormer: Learning Gait Representations with Noisy Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19418">http://arxiv.org/abs/2310.19418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cosmaadrian/gaitformer">https://github.com/cosmaadrian/gaitformer</a></li>
<li>paper_authors: Adrian Cosma, Emilian Radoi</li>
<li>for: 本研究旨在提出一种基于人体运动pattern的人体认同方法，不需要参与者的协助。</li>
<li>methods: 我们提出了一种名为DenseGait的大规模 dataset，并使用 transformer 型模型进行预训练。</li>
<li>results: 我们的方法可以在 CASIA-B 和 FVG 上达到 92.5% 和 85.33% 的准确率，比之前的方法提高 +14.2% 和 +9.67%。此外，我们的方法还可以准确地识别出人体的性别信息和多种外观特征。<details>
<summary>Abstract</summary>
Gait analysis is proven to be a reliable way to perform person identification without relying on subject cooperation. Walking is a biometric that does not significantly change in short periods of time and can be regarded as unique to each person. So far, the study of gait analysis focused mostly on identification and demographics estimation, without considering many of the pedestrian attributes that appearance-based methods rely on. In this work, alongside gait-based person identification, we explore pedestrian attribute identification solely from movement patterns. We propose DenseGait, the largest dataset for pretraining gait analysis systems containing 217K anonymized tracklets, annotated automatically with 42 appearance attributes. DenseGait is constructed by automatically processing video streams and offers the full array of gait covariates present in the real world. We make the dataset available to the research community. Additionally, we propose GaitFormer, a transformer-based model that after pretraining in a multi-task fashion on DenseGait, achieves 92.5% accuracy on CASIA-B and 85.33% on FVG, without utilizing any manually annotated data. This corresponds to a +14.2% and +9.67% accuracy increase compared to similar methods. Moreover, GaitFormer is able to accurately identify gender information and a multitude of appearance attributes utilizing only movement patterns. The code to reproduce the experiments is made publicly.
</details>
<details>
<summary>摘要</summary>
《坐姿分析可靠地实现人识别，无需参与者合作。行走是一种不会很快变化的生物指纹，每个人的坐姿都唯一。到目前为止，学术研究中的坐姿分析主要关注人识别和人群统计分析，忽略了许多步态特征，这些特征是外表基于方法所依据的。在这项工作中，我们不仅进行了人识别，还从步态特征中提取了多种人体特征。我们提出了DenseGait数据集，包含217万个匿名跟踪点，自动获得了42种外表特征。DenseGait数据集通过自动处理视频流程而成，具有实际世界中的全部步态covariate。我们将数据集公开发布。此外，我们提出了GaitFormer模型，通过多任务预训练在DenseGait数据集上，实现了CASIA-B和FVG上的92.5%和85.33%的准确率，不使用任何手动标注数据。这相对于类似方法提高了14.2%和9.67%的准确率。此外，GaitFormer能够通过只使用运动特征来准确地识别性别信息和许多外表特征。我们将实验代码公开。》
</details></li>
</ul>
<hr>
<h2 id="CARPE-ID-Continuously-Adaptable-Re-identification-for-Personalized-Robot-Assistance"><a href="#CARPE-ID-Continuously-Adaptable-Re-identification-for-Personalized-Robot-Assistance" class="headerlink" title="CARPE-ID: Continuously Adaptable Re-identification for Personalized Robot Assistance"></a>CARPE-ID: Continuously Adaptable Re-identification for Personalized Robot Assistance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19413">http://arxiv.org/abs/2310.19413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Rollo, Andrea Zunino, Nikolaos Tsagarakis, Enrico Mingo Hoffman, Arash Ajoudani</li>
<li>for: 本研究旨在提供一种基于持续视觉适应技术的人重识别模块，以便机器人在拥挤环境中进行个性化目标识别。</li>
<li>methods: 本研究使用了持续视觉适应技术来实现机器人与正确的人员进行无障碍的合作。</li>
<li>results: 实验结果显示，对比之前的状态艺技法，本研究的CARPE-ID模块在所有情况下（除了两个特殊情况）都能够准确地跟踪每个选择的目标。<details>
<summary>Abstract</summary>
In today's Human-Robot Interaction (HRI) scenarios, a prevailing tendency exists to assume that the robot shall cooperate with the closest individual or that the scene involves merely a singular human actor. However, in realistic scenarios, such as shop floor operations, such an assumption may not hold and personalized target recognition by the robot in crowded environments is required. To fulfil this requirement, in this work, we propose a person re-identification module based on continual visual adaptation techniques that ensure the robot's seamless cooperation with the appropriate individual even subject to varying visual appearances or partial or complete occlusions. We test the framework singularly using recorded videos in a laboratory environment and an HRI scenario, i.e., a person-following task by a mobile robot. The targets are asked to change their appearance during tracking and to disappear from the camera field of view to test the challenging cases of occlusion and outfit variations. We compare our framework with one of the state-of-the-art Multi-Object Tracking (MOT) methods and the results show that the CARPE-ID can accurately track each selected target throughout the experiments in all the cases (except two limit cases). At the same time, the s-o-t-a MOT has a mean of 4 tracking errors for each video.
</details>
<details>
<summary>摘要</summary>
今天的人机交互（HRI）场景中，一种普遍的假设是Robot会与最近的个体或场景中的唯一的人actor进行合作。然而，在现实场景中，如生产线上的操作，这种假设可能不成立，需要机器人个性化目标认识。为此，在这项工作中，我们提出了基于不断视觉适应技术的人重识别模块，以确保机器人与正确的个体进行无缝合作，即使视觉表现或部分或完全遮挡。我们使用实验室环境中记录的视频进行单个测试，以及人工智能场景，即移动机器人跟踪人员任务。 targets在跟踪中改变外表和消失视频场景中，以测试受阻和衣服变化的情况。我们与状态的多目标跟踪（MOT）方法进行比较，结果显示，CARPE-ID可以在所有情况下（除了两个边界情况）accurately跟踪每个选择的目标。同时，s-o-t-a MOT的每个视频的平均跟踪错误为4。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Breast-Cancer-Diagnosis-with-Heuristic-assisted-Trans-Res-U-Net-and-Multiscale-DenseNet-using-Mammogram-Images"><a href="#Intelligent-Breast-Cancer-Diagnosis-with-Heuristic-assisted-Trans-Res-U-Net-and-Multiscale-DenseNet-using-Mammogram-Images" class="headerlink" title="Intelligent Breast Cancer Diagnosis with Heuristic-assisted Trans-Res-U-Net and Multiscale DenseNet using Mammogram Images"></a>Intelligent Breast Cancer Diagnosis with Heuristic-assisted Trans-Res-U-Net and Multiscale DenseNet using Mammogram Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19411">http://arxiv.org/abs/2310.19411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Yaqub, Feng Jinchao</li>
<li>for: 这个研究旨在提高乳癌早期检测的精确性，以帮助提高女性患者的癌症预后。</li>
<li>methods: 本研究使用了一个新的深度学习方法，包括三个阶段：资料收集、影像分类和乳癌识别。影像分类使用了一个基于Atrous Convolution的Attentive and Adaptive Trans-Res-UNet（ACA-ATRUNet）架构，而乳癌识别则使用了一个基于Atrous Convolution的Attentive and Adaptive Multi-scale DenseNet（ACA-AMDN）模型。内部的参数是使用Modified Mussel Length-based Eurasian Oystercatcher Optimization（MML-EOO）算法来优化。</li>
<li>results: 实验结果显示，提案的乳癌检测框架可以实现高精度的早期癌症检测，比传统方法更高。<details>
<summary>Abstract</summary>
Breast cancer (BC) significantly contributes to cancer-related mortality in women, underscoring the criticality of early detection for optimal patient outcomes. A mammography is a key tool for identifying and diagnosing breast abnormalities; however, accurately distinguishing malignant mass lesions remains challenging. To address this issue, we propose a novel deep learning approach for BC screening utilizing mammography images. Our proposed model comprises three distinct stages: data collection from established benchmark sources, image segmentation employing an Atrous Convolution-based Attentive and Adaptive Trans-Res-UNet (ACA-ATRUNet) architecture, and BC identification via an Atrous Convolution-based Attentive and Adaptive Multi-scale DenseNet (ACA-AMDN) model. The hyperparameters within the ACA-ATRUNet and ACA-AMDN models are optimised using the Modified Mussel Length-based Eurasian Oystercatcher Optimization (MML-EOO) algorithm. Performance evaluation, leveraging multiple metrics, is conducted, and a comparative analysis against conventional methods is presented. Our experimental findings reveal that the proposed BC detection framework attains superior precision rates in early disease detection, demonstrating its potential to enhance mammography-based screening methodologies.
</details>
<details>
<summary>摘要</summary>
乳癌（BC）对女性患有癌症的死亡率产生了重要的贡献，这更加提出了早期检测的重要性以便实现最佳病人结果。ammaography 是识别和诊断乳腺畸形的关键工具，但是准确地识别癌症还是一个挑战。为了解决这个问题，我们提出了一种基于深度学习的乳癌检测方法，使用ammaography 图像。我们的提议的模型包括三个不同的阶段：数据收集从已知基准源，使用 Atrous Convolution-based Attentive and Adaptive Trans-Res-UNet（ACA-ATRUNet）架构进行图像分割，以及使用 Atrous Convolution-based Attentive and Adaptive Multi-scale DenseNet（ACA-AMDN）模型进行乳癌识别。在 ACA-ATRUNet 和 ACA-AMDN 模型中的超参数使用 Modified Mussel Length-based Eurasian Oystercatcher Optimization（MML-EOO）算法优化。我们的实验结果表明，提议的乳癌检测框架可以在早期癌症检测中实现更高的精度率，这表明它有potential 提高基于ammaography 的检测方法。
</details></li>
</ul>
<hr>
<h2 id="Generated-Distributions-Are-All-You-Need-for-Membership-Inference-Attacks-Against-Generative-Models"><a href="#Generated-Distributions-Are-All-You-Need-for-Membership-Inference-Attacks-Against-Generative-Models" class="headerlink" title="Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models"></a>Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19410">http://arxiv.org/abs/2310.19410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minxingzhang/miagm">https://github.com/minxingzhang/miagm</a></li>
<li>paper_authors: Minxing Zhang, Ning Yu, Rui Wen, Michael Backes, Yang Zhang</li>
<li>for: 本研究旨在揭示生成模型中私人信息泄露的隐患，以及这些隐患对多种生成模型的普遍性。</li>
<li>methods: 我们提出了一种普适的会员推测攻击，只需要利用目标生成器生成的分布，以及auxiliary非会员数据集。这种攻击不需要阴影模型或白盒访问，可以覆盖多种生成模型，如生成对抗网络、变量自动编码器、隐函数和扩散模型。</li>
<li>results: 实验表明，所有的生成模型都受到我们的攻击。例如，我们对 DDPM、DDIM 和 FastDPM 在 CIFAR-10 和 CelebA 上进行了攻击，攻击 AUC 高于 0.99。而对 VQGAN、LDM（用于文本Conditional生成）和 LIIF 进行的攻击，AUC 高于 0.90。因此，我们呼吁我们的社区在设计和发布生成模型时注意这种隐患，以保护用户的隐私。<details>
<summary>Abstract</summary>
Generative models have demonstrated revolutionary success in various visual creation tasks, but in the meantime, they have been exposed to the threat of leaking private information of their training data. Several membership inference attacks (MIAs) have been proposed to exhibit the privacy vulnerability of generative models by classifying a query image as a training dataset member or nonmember. However, these attacks suffer from major limitations, such as requiring shadow models and white-box access, and either ignoring or only focusing on the unique property of diffusion models, which block their generalization to multiple generative models. In contrast, we propose the first generalized membership inference attack against a variety of generative models such as generative adversarial networks, [variational] autoencoders, implicit functions, and the emerging diffusion models. We leverage only generated distributions from target generators and auxiliary non-member datasets, therefore regarding target generators as black boxes and agnostic to their architectures or application scenarios. Experiments validate that all the generative models are vulnerable to our attack. For instance, our work achieves attack AUC $>0.99$ against DDPM, DDIM, and FastDPM trained on CIFAR-10 and CelebA. And the attack against VQGAN, LDM (for the text-conditional generation), and LIIF achieves AUC $>0.90.$ As a result, we appeal to our community to be aware of such privacy leakage risks when designing and publishing generative models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过多种视觉创作任务，生成模型已经取得了革命性的成功，但是在同时，它们也面临着泄露私人训练数据的风险。多种会员推断攻击（MIAs）已经提出，以示生成模型的隐私漏洞，但是这些攻击受到了重大限制，例如需要阴影模型和白盒访问，并且完全忽略了扩散模型的特殊性，导致它们无法泛化到多种生成模型。相比之下，我们提出了第一个通用的会员推断攻击，可以对多种生成模型进行攻击，包括生成对抗网络、自适应网络、隐藏函数和升级扩散模型。我们只需要使用目标生成器生成的分布，并且不需要访问目标生成器的 Architecture或应用场景，因此可以视为目标生成器为黑盒模型。实验表明，所有这些生成模型都受到了我们的攻击。例如，我们的工作在DDPM、DDIM和FastDPM在CIFAR-10和CelebA上获得了攻击AUC>0.99。而对VQGAN、LDM（ для文本条件生成）和LIIF的攻击也获得了AUC>0.90。因此，我们呼吁我们的社区在设计和发布生成模型时需要注意隐私泄露风险。
</details></li>
</ul>
<hr>
<h2 id="Radar-Lidar-Fusion-for-Object-Detection-by-Designing-Effective-Convolution-Networks"><a href="#Radar-Lidar-Fusion-for-Object-Detection-by-Designing-Effective-Convolution-Networks" class="headerlink" title="Radar-Lidar Fusion for Object Detection by Designing Effective Convolution Networks"></a>Radar-Lidar Fusion for Object Detection by Designing Effective Convolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19405">http://arxiv.org/abs/2310.19405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzeen Munir, Shoaib Azam, Tomasz Kucner, Ville Kyrki, Moongu Jeon</li>
<li>for: 提高遥感系统的精度和可靠性，特别是在不良天气条件下。</li>
<li>methods: 利用雷达和激光数据的混合技术，通过添加注意力来结合两种数据，然后使用一种新的并平行分支结构来处理尺度变化。</li>
<li>results: 在Radiate数据集上使用COCO指标进行评估，与状态的艺术方法相比，提高了1.89%和2.61%在有利和不利天气条件下。这显示了雷达-激光混合在困难的天气条件下准确地检测和 lokalisieren objects。<details>
<summary>Abstract</summary>
Object detection is a core component of perception systems, providing the ego vehicle with information about its surroundings to ensure safe route planning. While cameras and Lidar have significantly advanced perception systems, their performance can be limited in adverse weather conditions. In contrast, millimeter-wave technology enables radars to function effectively in such conditions. However, relying solely on radar for building a perception system doesn't fully capture the environment due to the data's sparse nature. To address this, sensor fusion strategies have been introduced. We propose a dual-branch framework to integrate radar and Lidar data for enhanced object detection. The primary branch focuses on extracting radar features, while the auxiliary branch extracts Lidar features. These are then combined using additive attention. Subsequently, the integrated features are processed through a novel Parallel Forked Structure (PFS) to manage scale variations. A region proposal head is then utilized for object detection. We evaluated the effectiveness of our proposed method on the Radiate dataset using COCO metrics. The results show that it surpasses state-of-the-art methods by $1.89\%$ and $2.61\%$ in favorable and adverse weather conditions, respectively. This underscores the value of radar-Lidar fusion in achieving precise object detection and localization, especially in challenging weather conditions.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:对象检测是感知系统的核心组件，为ego车提供环境信息，以确保安全的路径规划。尽管摄像头和激光技术在恶劣天气条件下有显著进步，但它们的性能可能受到限制。然而，毫米波技术在这些条件下能够有效地运行。然而，只靠 радиar建立感知系统不能完全捕捉环境，因为数据的稀疏性。为此，感知融合策略被引入。我们提出了一种双支osoframework，用于集成雷达和激光数据，以提高对象检测。主支系集中提取雷达特征，而辅助支系提取激光特征。这些特征然后在additive注意力下组合。然后，这些集成特征经过一种新的并行分支结构（PFS）进行扩展。一个区域提议头然后用于对象检测。我们使用Radiate数据集以COCO指标进行评估。结果显示，我们的提posed方法在不利天气和恶劣天气条件下比 estado-of-the-art方法提高$1.89\%$和$2.61\%$。这表明雷达-激光融合在挑战性天气条件下实现精准的对象检测和定位，尤其是在恶劣天气条件下。
</details></li>
</ul>
<hr>
<h2 id="A-Clinical-Guideline-Driven-Automated-Linear-Feature-Extraction-for-Vestibular-Schwannoma"><a href="#A-Clinical-Guideline-Driven-Automated-Linear-Feature-Extraction-for-Vestibular-Schwannoma" class="headerlink" title="A Clinical Guideline Driven Automated Linear Feature Extraction for Vestibular Schwannoma"></a>A Clinical Guideline Driven Automated Linear Feature Extraction for Vestibular Schwannoma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19392">http://arxiv.org/abs/2310.19392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navodini Wijethilake, Steve Connor, Anna Oviedova, Rebecca Burger, Tom Vercauteren, Jonathan Shapey</li>
<li>for: This paper aims to automate and improve the clinical decision-making process for patients with Vestibular Schwannoma by using deep learning-based segmentation to extract relevant clinical features from T1 and T2 weighted MRI scans.</li>
<li>methods: The authors use a deep learning-based segmentation approach to extract the maximum linear measurement from the segmented regions, and propose a novel algorithm to choose and extract the most appropriate measurement based on the size of the extrameatal portion of the tumour.</li>
<li>results: The authors achieved Dice-scores of 0.8124 +- 0.2343 and 0.8969 +- 0.0521 for extrameatal and whole tumour regions respectively for T2 weighted MRI, and 0.8222 +- 0.2108 and 0.9049 +- 0.0646 for T1 weighted MRI. The automated measurements were found to be significantly correlated with the manual measurements obtained by an expert neuroradiologist (p &lt; 0.0001).<details>
<summary>Abstract</summary>
Vestibular Schwannoma is a benign brain tumour that grows from one of the balance nerves. Patients may be treated by surgery, radiosurgery or with a conservative "wait-and-scan" strategy. Clinicians typically use manually extracted linear measurements to aid clinical decision making. This work aims to automate and improve this process by using deep learning based segmentation to extract relevant clinical features through computational algorithms. To the best of our knowledge, our study is the first to propose an automated approach to replicate local clinical guidelines. Our deep learning based segmentation provided Dice-scores of 0.8124 +- 0.2343 and 0.8969 +- 0.0521 for extrameatal and whole tumour regions respectively for T2 weighted MRI, whereas 0.8222 +- 0.2108 and 0.9049 +- 0.0646 were obtained for T1 weighted MRI. We propose a novel algorithm to choose and extract the most appropriate maximum linear measurement from the segmented regions based on the size of the extrameatal portion of the tumour. Using this tool, clinicians will be provided with a visual guide and related metrics relating to tumour progression that will function as a clinical decision aid. In this study, we utilize 187 scans obtained from 50 patients referred to a tertiary specialist neurosurgical service in the United Kingdom. The measurements extracted manually by an expert neuroradiologist indicated a significant correlation with the automated measurements (p < 0.0001).
</details>
<details>
<summary>摘要</summary>
vestibular schwannoma 是一种良性肿瘤，来自于很多平衡神经的增生。患者可能会通过手术、放射治疗或保守的“等待和扫描”策略进行治疗。临床医生通常使用手动提取的直线测量来帮助临床决策。本研究旨在自动化和改进这个过程，使用深度学习基于的分割来提取相关的临床特征，并通过计算机算法来提供临床指导。根据我们所知，本研究是首个提出自动化地复制当地临床指南的研究。我们的深度学习基于的分割提供了T2束积成像中的 dice-分数为0.8124±0.2343和0.8969±0.0521，而T1束积成像中的分数为0.8222±0.2108和0.9049±0.0646。我们提出了一种新的算法，可以在分割后选择和提取最适合的最大直线测量，基于脑外部的肿瘤部分的大小。使用这种工具，临床医生将被提供视觉指南和相关的肿瘤进展指标，作为临床决策参考。本研究使用50名患者和187个扫描图像，由专业神经外科医生手动提取的测量表明和自动化测量之间存在显著相关性（p<0.0001）。
</details></li>
</ul>
<hr>
<h2 id="TransXNet-Learning-Both-Global-and-Local-Dynamics-with-a-Dual-Dynamic-Token-Mixer-for-Visual-Recognition"><a href="#TransXNet-Learning-Both-Global-and-Local-Dynamics-with-a-Dual-Dynamic-Token-Mixer-for-Visual-Recognition" class="headerlink" title="TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic Token Mixer for Visual Recognition"></a>TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic Token Mixer for Visual Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19380">http://arxiv.org/abs/2310.19380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lmmmeng/transxnet">https://github.com/lmmmeng/transxnet</a></li>
<li>paper_authors: Meng Lou, Hong-Yu Zhou, Sibei Yang, Yizhou Yu</li>
<li>for: 提高图像分类模型的泛化能力和计算效率</li>
<li>methods: 提出了一种轻量级双动态то金符（D-Mixer），通过应用高效的全局注意力模块和输入参数化深度卷积，具有强导向性和扩大的有效识别场，并将其作为基本建筑块设计了一种新的混合CNN-Transformer视觉后缀网络（TransXNet）</li>
<li>results: 在ImageNet-1K图像分类任务中，TransXNet-T比Swin-T提高0.3%的顶部一准度，同时需要比Swin-T的计算成本少于一半，而TransXNet-S和TransXNet-B也达到了83.8%和84.6%的顶部一准度，具有合理的计算成本。此外，我们提出的网络架构在多种精密预测任务中表现出色，比其他现有的网络更高效，同时具有更好的泛化能力。<details>
<summary>Abstract</summary>
Recent studies have integrated convolution into transformers to introduce inductive bias and improve generalization performance. However, the static nature of conventional convolution prevents it from dynamically adapting to input variations, resulting in a representation discrepancy between convolution and self-attention as self-attention calculates attention matrices dynamically. Furthermore, when stacking token mixers that consist of convolution and self-attention to form a deep network, the static nature of convolution hinders the fusion of features previously generated by self-attention into convolution kernels. These two limitations result in a sub-optimal representation capacity of the constructed networks. To find a solution, we propose a lightweight Dual Dynamic Token Mixer (D-Mixer) that aggregates global information and local details in an input-dependent way. D-Mixer works by applying an efficient global attention module and an input-dependent depthwise convolution separately on evenly split feature segments, endowing the network with strong inductive bias and an enlarged effective receptive field. We use D-Mixer as the basic building block to design TransXNet, a novel hybrid CNN-Transformer vision backbone network that delivers compelling performance. In the ImageNet-1K image classification task, TransXNet-T surpasses Swin-T by 0.3\% in top-1 accuracy while requiring less than half of the computational cost. Furthermore, TransXNet-S and TransXNet-B exhibit excellent model scalability, achieving top-1 accuracy of 83.8\% and 84.6\% respectively, with reasonable computational costs. Additionally, our proposed network architecture demonstrates strong generalization capabilities in various dense prediction tasks, outperforming other state-of-the-art networks while having lower computational costs.
</details>
<details>
<summary>摘要</summary>
近期研究已经将卷积 integrate 到 transformer 中来引入逻辑偏好和提高总体性能。然而，传统的卷积 static nature 阻碍它在输入变化中动态适应，导致卷积和自注意力之间的表示差异，自注意力计算动态注意力矩阵时。此外，当堆叠 token mixer 组成深度网络时，传统的卷积 static nature 限制了自注意力生成的特征的融合到卷积核心中。这两个限制导致构建的网络没有达到最佳表示能力。为了解决这问题，我们提出了一种轻量级的双动态 токен mixer（D-Mixer），它可以在输入依赖的方式中集成全局信息和本地细节。D-Mixer 通过简单的全局注意力模块和输入依赖的深度卷积分别应用于分别拆分的特征段，赋予网络强大的逻辑偏好和扩大有效识别场。我们使用 D-Mixer 作为基本建构件，设计了 TransXNet，一种新的混合 CNN-Transformer 视觉后缀网络，它在 ImageNet-1K 图像分类任务中，胜过 Swin-T 的 top-1 准确率，而且计算成本相对较低。此外，TransXNet-S 和 TransXNet-B 在不同的计算成本下仍然表现出色，分别达到了 83.8% 和 84.6% 的 top-1 准确率。此外，我们的提案的网络架构还在不同的粗糙预测任务中表现出色，比其他状态发展的网络更高的总体性能，同时计算成本相对较低。
</details></li>
</ul>
<hr>
<h2 id="Color-Equivariant-Convolutional-Networks"><a href="#Color-Equivariant-Convolutional-Networks" class="headerlink" title="Color Equivariant Convolutional Networks"></a>Color Equivariant Convolutional Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19368">http://arxiv.org/abs/2310.19368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/attila94/ceconv">https://github.com/attila94/ceconv</a></li>
<li>paper_authors: Attila Lengyel, Ombretta Strafforello, Robert-Jan Bruintjes, Alexander Gielisse, Jan van Gemert</li>
<li>for: 提高 CNN 对各种颜色变化的识别能力</li>
<li>methods: 提出 Color Equivariant Convolutions (CEConvs) deep learning Building Block，允许形状特征共享 across 颜色谱，保留重要的颜色信息</li>
<li>results: 在不同任务上提高下游性能，提高对颜色变化的Robustness，包括训练测试分布shift<details>
<summary>Abstract</summary>
Color is a crucial visual cue readily exploited by Convolutional Neural Networks (CNNs) for object recognition. However, CNNs struggle if there is data imbalance between color variations introduced by accidental recording conditions. Color invariance addresses this issue but does so at the cost of removing all color information, which sacrifices discriminative power. In this paper, we propose Color Equivariant Convolutions (CEConvs), a novel deep learning building block that enables shape feature sharing across the color spectrum while retaining important color information. We extend the notion of equivariance from geometric to photometric transformations by incorporating parameter sharing over hue-shifts in a neural network. We demonstrate the benefits of CEConvs in terms of downstream performance to various tasks and improved robustness to color changes, including train-test distribution shifts. Our approach can be seamlessly integrated into existing architectures, such as ResNets, and offers a promising solution for addressing color-based domain shifts in CNNs.
</details>
<details>
<summary>摘要</summary>
颜色是计算机视觉中的一个关键视觉提示符，它被卷积神经网络（CNN）广泛利用于物体识别中。然而，如果数据集中存在颜色变化的偏置，那么CNN会遇到困难。颜色不变性可以解决这个问题，但是它会在抛弃所有颜色信息的同时做出牺牲。在这篇论文中，我们提出了颜色等距化卷积（CEConvs），一种新的深度学习建模元件，它可以在颜色谱中共享形状特征，同时保留重要的颜色信息。我们扩展了卷积神经网络中的等距变换概念，包括在抽象中插入色差参数共享。我们示出了CEConvs在不同任务下的下游性能和颜色变化的Robustness，包括训练测试分布跳变。我们的方法可以轻松地与现有的架构相结合，如ResNet，并提供一个有 Promise的解决方案，用于在CNN中Addressing颜色基于频谱的域shift。
</details></li>
</ul>
<hr>
<h2 id="Semi-and-Weakly-Supervised-Domain-Generalization-for-Object-Detection"><a href="#Semi-and-Weakly-Supervised-Domain-Generalization-for-Object-Detection" class="headerlink" title="Semi- and Weakly-Supervised Domain Generalization for Object Detection"></a>Semi- and Weakly-Supervised Domain Generalization for Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19351">http://arxiv.org/abs/2310.19351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryosuke Furuta, Yoichi Sato</li>
<li>for: 这篇论文目的是解决域别差异导致侦测器性能下降的问题，提出了两个新的问题设定：半指导下域普遍化物件探测 (SS-DGOD) 和弱指导下域普遍化物件探测 (WS-DGOD)。</li>
<li>methods: 这篇论文提出了一个学生-教师学习框架，其中一个学生网络是在一个执教师网络的 Pseudo 标签下进行训练，并且使用执教师网络生成的 Pseudo 标签进行探测器的训练。</li>
<li>results: 实验结果显示，使用提案的问题设定和学生-教师学习框架可以将物件探测器训练得到优化，比起基准探测器训练一个执Domain Data的情况下，表现更好，而且与或更好于不使用Target Domain Data进行训练的UDA设定相比。<details>
<summary>Abstract</summary>
Object detectors do not work well when domains largely differ between training and testing data. To solve this problem, domain generalization approaches, which require training data with ground-truth labels from multiple domains, have been proposed. However, it is time-consuming and labor-intensive to collect those data for object detection because not only class labels but also bounding boxes must be annotated. To overcome the problem of domain gap in object detection without requiring expensive annotations, we propose to consider two new problem settings: semi-supervised domain generalizable object detection (SS-DGOD) and weakly-supervised DGOD (WS-DGOD). In contrast to the conventional domain generalization for object detection that requires labeled data from multiple domains, SS-DGOD and WS-DGOD require labeled data only from one domain and unlabeled or weakly-labeled data from multiple domains for training. We show that object detectors can be effectively trained on the proposed settings with the same student-teacher learning framework, where a student network is trained with pseudo labels output from a teacher on the unlabeled or weakly-labeled data. The experimental results demonstrate that the object detectors trained on the proposed settings significantly outperform baseline detectors trained on one labeled domain data and perform comparably to or better than those trained on unsupervised domain adaptation (UDA) settings, while ours do not use target domain data for training in contrast to UDA.
</details>
<details>
<summary>摘要</summary>
Unlike conventional domain generalization for object detection, which requires labeled data from multiple domains, SS-DGOD and WS-DGOD only require labeled data from one domain and unlabeled or weakly-labeled data from multiple domains for training. We use a student-teacher learning framework, where a student network is trained with pseudo labels output from a teacher on the unlabeled or weakly-labeled data. The experimental results show that object detectors can be effectively trained on the proposed settings and significantly outperform baseline detectors trained on one labeled domain data. Our approach also performs comparably to or better than those trained on unsupervised domain adaptation (UDA) settings, without using target domain data for training.
</details></li>
</ul>
<hr>
<h2 id="Label-Only-Model-Inversion-Attacks-via-Knowledge-Transfer"><a href="#Label-Only-Model-Inversion-Attacks-via-Knowledge-Transfer" class="headerlink" title="Label-Only Model Inversion Attacks via Knowledge Transfer"></a>Label-Only Model Inversion Attacks via Knowledge Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19342">http://arxiv.org/abs/2310.19342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngoc-Bao Nguyen, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, Ngai-Man Cheung</li>
<li>For: The paper is focused on addressing the privacy threat of model inversion (MI) attacks, specifically in the label-only setup where the adversary only has access to the model’s predicted labels.* Methods: The proposed approach, called LOKT, uses transfer learning to leverage the knowledge of an opaque target model to surrogate models, enabling the use of advanced white-box attacks. The key technique is a novel model called Target model-assisted ACGAN (T-ACGAN), which facilitates effective knowledge transfer.* Results: The proposed method significantly outperforms existing state-of-the-art (SOTA) label-only MI attacks by more than 15% across all MI benchmarks, and compares favorably in terms of query budget.Here’s the information in Simplified Chinese:* For: 本研究旨在解决机器学习模型 inverse 攻击（MI）的隐私威胁，特别是在标签只 setup 中， где adversary 只有模型预测的标签。* Methods: 提议的方法是使用知识传输来利用透明目标模型的知识，并使用 surrogate 模型来实现高级白盒攻击。关键技术是一种新的模型called Target model-assisted ACGAN (T-ACGAN)，它实现了有效的知识传输。* Results: 提议的方法在所有 MI 测试集上比现有 SOTA 标签只 MI 攻击高出15%以上，并与查询预算相比较有利。<details>
<summary>Abstract</summary>
In a model inversion (MI) attack, an adversary abuses access to a machine learning (ML) model to infer and reconstruct private training data. Remarkable progress has been made in the white-box and black-box setups, where the adversary has access to the complete model or the model's soft output respectively. However, there is very limited study in the most challenging but practically important setup: Label-only MI attacks, where the adversary only has access to the model's predicted label (hard label) without confidence scores nor any other model information.   In this work, we propose LOKT, a novel approach for label-only MI attacks. Our idea is based on transfer of knowledge from the opaque target model to surrogate models. Subsequently, using these surrogate models, our approach can harness advanced white-box attacks. We propose knowledge transfer based on generative modelling, and introduce a new model, Target model-assisted ACGAN (T-ACGAN), for effective knowledge transfer. Our method casts the challenging label-only MI into the more tractable white-box setup. We provide analysis to support that surrogate models based on our approach serve as effective proxies for the target model for MI. Our experiments show that our method significantly outperforms existing SOTA Label-only MI attack by more than 15% across all MI benchmarks. Furthermore, our method compares favorably in terms of query budget. Our study highlights rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed. Our study highlights rising privacy threats for ML models even when minimal information (i.e., hard labels) is exposed. Our code, demo, models and reconstructed data are available at our project page: https://ngoc-nguyen-0.github.io/lokt/
</details>
<details>
<summary>摘要</summary>
“在机器学习（ML）模型倒推（MI）攻击中，敌对者利用对ML模型的访问来推断和重建私有训练数据。在白盒和黑盒设置中，敌对者已经获得了很多进展，但是在最重要的Label-only MI攻击中，即只有模型预测标签（硬标签）而无 confidence scores 和其他模型信息的情况下，还有很少的研究。在这项工作中，我们提出了一种新的方法 called LOKT，用于Label-only MI攻击。我们的想法是基于目标模型的知识传递到临时模型上。然后，我们可以通过这些临时模型来利用高级白盒攻击。我们提出了基于生成模型的知识传递，并引入了一种新的模型 Target model-assisted ACGAN（T-ACGAN），用于有效地传递知识。我们将Label-only MI转化为更加可控的白盒设置。我们提供分析支持，表明我们的方法可以使用临时模型来代表目标模型进行MI。我们的实验表明，我们的方法可以与现有最佳实践（SOTA）Label-only MI攻击比较，并且在所有MI benchmark上表现出色，超过15%的提升。此外，我们的方法与查询预算相比，也具有良好的比较。我们的研究表明，即使只有硬标签信息被暴露出来，ML模型也面临着巨大的隐私威胁。我们的研究也表明，隐私威胁不仅限于ML模型，还可能扩展到其他领域。我们的代码、demo、模型和重建数据都可以在我们项目页面上下载：https://ngoc-nguyen-0.github.io/lokt/”
</details></li>
</ul>
<hr>
<h2 id="On-Measuring-Fairness-in-Generative-Models"><a href="#On-Measuring-Fairness-in-Generative-Models" class="headerlink" title="On Measuring Fairness in Generative Models"></a>On Measuring Fairness in Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19297">http://arxiv.org/abs/2310.19297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher T. H. Teo, Milad Abdollahzadeh, Ngai-Man Cheung</li>
<li>for: 这研究的目的是对公平生成模型进行深入的研究，特别是评估公平性的标准方法。</li>
<li>methods: 这篇论文提出了三个贡献：首先，通过实验表明现有的公平性评估框架存在评估错误，即使使用高精度敏感特征（SA）分类器也是如此。这些发现质量地动摇了之前报道的公平性改进。其次，提出了一种新的评估框架，即类别错误意识（CLEAM），使用统计模型来补做SA分类器的不准确。CLEAM可以减少评估错误，例如从4.98%降至0.62%对StyleGAN2的性别。此外，CLEAM具有较少的附加开销。三、通过CLEAM评估了文本到图像生成器和GANs的公平性，发现这些模型具有重要的偏见问题，这些问题对其应用产生了担忧。</li>
<li>results: 这篇论文通过实验和分析发现，现有的公平性评估框架存在评估错误，而提出的CLEAM可以减少这些错误。此外，通过CLEAM评估了文本到图像生成器和GANs的公平性，发现这些模型具有重要的偏见问题。<details>
<summary>Abstract</summary>
Recently, there has been increased interest in fair generative models. In this work, we conduct, for the first time, an in-depth study on fairness measurement, a critical component in gauging progress on fair generative models. We make three contributions. First, we conduct a study that reveals that the existing fairness measurement framework has considerable measurement errors, even when highly accurate sensitive attribute (SA) classifiers are used. These findings cast doubts on previously reported fairness improvements. Second, to address this issue, we propose CLassifier Error-Aware Measurement (CLEAM), a new framework which uses a statistical model to account for inaccuracies in SA classifiers. Our proposed CLEAM reduces measurement errors significantly, e.g., 4.98% $\rightarrow$ 0.62% for StyleGAN2 w.r.t. Gender. Additionally, CLEAM achieves this with minimal additional overhead. Third, we utilize CLEAM to measure fairness in important text-to-image generator and GANs, revealing considerable biases in these models that raise concerns about their applications. Code and more resources: https://sutd-visual-computing-group.github.io/CLEAM/.
</details>
<details>
<summary>摘要</summary>
近期，关于公平生成模型的兴趣增长。在这项工作中，我们第一次进行了深入的公平度测量研究，这是评估公平生成模型的关键组件。我们的贡献包括以下三个方面：第一，我们发现现有的公平度测量框架存在较大的测量错误，即使使用高精度敏感特征（SA）分类器也是如此。这些发现质量地影响了先前报道的公平改进。第二，为解决这一问题，我们提出了一种新的测量框架，即类别错误意识（CLEAM）。CLEAM使用统计模型来考虑敏感特征分类器的不准确性，从而大幅减少测量错误。例如，对于StyleGAN2，我们的CLEAM可以将4.98%降低到0.62%。此外，CLEAM可以实现这一目标而无需较大的额外负担。第三，我们使用CLEAM测量了文本生成器和GAN的公平度，发现这些模型存在许多问题，它们的应用可能会引起关切。代码和更多资源可以在以下链接中找到：https://sutd-visual-computing-group.github.io/CLEAM/.
</details></li>
</ul>
<hr>
<h2 id="FetusMapV2-Enhanced-Fetal-Pose-Estimation-in-3D-Ultrasound"><a href="#FetusMapV2-Enhanced-Fetal-Pose-Estimation-in-3D-Ultrasound" class="headerlink" title="FetusMapV2: Enhanced Fetal Pose Estimation in 3D Ultrasound"></a>FetusMapV2: Enhanced Fetal Pose Estimation in 3D Ultrasound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19293">http://arxiv.org/abs/2310.19293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoyu Chen, Xin Yang, Yuhao Huang, Wenlong Shi, Yan Cao, Mingyuan Luo, Xindi Hu, Lei Zhue, Lequan Yu, Kejuan Yue, Yuanji Zhang, Yi Xiong, Dong Ni, Weijun Huang</li>
<li>for: 提供精准的三维胎儿姿态信息，用于生物 métrique 测量、平面定位和胎儿运动监测等应用。</li>
<li>methods: 提出一种新的三维胎儿姿态估计框架（叫做FetusMapV2），解决了限制性的硬件资源、图像质量、同构结构和胎儿姿态变化等问题。</li>
<li>results: 对比其他强有力竞争者，该方法在一个大规模的胎儿US数据集上表现出优异的准确性和稳定性。<details>
<summary>Abstract</summary>
Fetal pose estimation in 3D ultrasound (US) involves identifying a set of associated fetal anatomical landmarks. Its primary objective is to provide comprehensive information about the fetus through landmark connections, thus benefiting various critical applications, such as biometric measurements, plane localization, and fetal movement monitoring. However, accurately estimating the 3D fetal pose in US volume has several challenges, including poor image quality, limited GPU memory for tackling high dimensional data, symmetrical or ambiguous anatomical structures, and considerable variations in fetal poses. In this study, we propose a novel 3D fetal pose estimation framework (called FetusMapV2) to overcome the above challenges. Our contribution is three-fold. First, we propose a heuristic scheme that explores the complementary network structure-unconstrained and activation-unreserved GPU memory management approaches, which can enlarge the input image resolution for better results under limited GPU memory. Second, we design a novel Pair Loss to mitigate confusion caused by symmetrical and similar anatomical structures. It separates the hidden classification task from the landmark localization task and thus progressively eases model learning. Last, we propose a shape priors-based self-supervised learning by selecting the relatively stable landmarks to refine the pose online. Extensive experiments and diverse applications on a large-scale fetal US dataset including 1000 volumes with 22 landmarks per volume demonstrate that our method outperforms other strong competitors.
</details>
<details>
<summary>摘要</summary>
三维超声成像（US）中的胎儿姿态估计涉及到确定一组相关的胎儿生理学特征点。其主要目标是通过特征点之间的连接提供胎儿全面信息，以便在各种关键应用程序中使用，如生物 метри克测量、平面定位和胎儿运动监测。然而，在三维胎儿姿态估计中存在多种挑战，包括图像质量不佳、有限的GPU内存处理高维数据、同形或混淆的生理结构和胎儿姿态变化较大。在这种研究中，我们提出了一种新的三维胎儿姿态估计框架（称为FetusMapV2），以解决上述挑战。我们的贡献有三个方面：1. 我们提出了一种启发式方法，通过不同的网络结构和活动不受限制的GPU内存管理方法，可以在有限的GPU内存下提高输入图像分辨率以获得更好的结果。2. 我们设计了一种新的对称损失函数，用于降低同形和相似的生理结构所引起的混淆。它将隐藏的分类任务与特征点local化任务分离开来，从而逐渐缓解模型学习。3. 我们提出了一种基于形状假设的自动学习方法，通过选择稳定的特征点来在线进行姿态约束。我们在一个大规模的胎儿US数据集上进行了广泛的实验和多种应用，并证明了我们的方法在其他强竞争者之上表现出优异。
</details></li>
</ul>
<hr>
<h2 id="EDiffSR-An-Efficient-Diffusion-Probabilistic-Model-for-Remote-Sensing-Image-Super-Resolution"><a href="#EDiffSR-An-Efficient-Diffusion-Probabilistic-Model-for-Remote-Sensing-Image-Super-Resolution" class="headerlink" title="EDiffSR: An Efficient Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution"></a>EDiffSR: An Efficient Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19288">http://arxiv.org/abs/2310.19288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xy-boy/ediffsr">https://github.com/xy-boy/ediffsr</a></li>
<li>paper_authors: Yi Xiao, Qiangqiang Yuan, Kui Jiang, Jiang He, Xianyu Jin, Liangpei Zhang</li>
<li>for: 本研究旨在提出一种高效的激进网络模型，用于提高遥感图像超分辨率（SR）的效果。</li>
<li>methods: 本研究使用了扩散概率模型（DPM），并开发了一种高效的活化网络（EANet）和一种实用的条件增强模块（CPEM）来提高SR的性能。</li>
<li>results: 对四个遥感图像 dataset 进行了广泛的实验，并证明了 EDiffSR 可以提供高质量的 SR 效果，同时具有高效的计算成本和简单易用的训练方法。<details>
<summary>Abstract</summary>
Recently, convolutional networks have achieved remarkable development in remote sensing image Super-Resoltuion (SR) by minimizing the regression objectives, e.g., MSE loss. However, despite achieving impressive performance, these methods often suffer from poor visual quality with over-smooth issues. Generative adversarial networks have the potential to infer intricate details, but they are easy to collapse, resulting in undesirable artifacts. To mitigate these issues, in this paper, we first introduce Diffusion Probabilistic Model (DPM) for efficient remote sensing image SR, dubbed EDiffSR. EDiffSR is easy to train and maintains the merits of DPM in generating perceptual-pleasant images. Specifically, different from previous works using heavy UNet for noise prediction, we develop an Efficient Activation Network (EANet) to achieve favorable noise prediction performance by simplified channel attention and simple gate operation, which dramatically reduces the computational budget. Moreover, to introduce more valuable prior knowledge into the proposed EDiffSR, a practical Conditional Prior Enhancement Module (CPEM) is developed to help extract an enriched condition. Unlike most DPM-based SR models that directly generate conditions by amplifying LR images, the proposed CPEM helps to retain more informative cues for accurate SR. Extensive experiments on four remote sensing datasets demonstrate that EDiffSR can restore visual-pleasant images on simulated and real-world remote sensing images, both quantitatively and qualitatively. The code of EDiffSR will be available at https://github.com/XY-boy/EDiffSR
</details>
<details>
<summary>摘要</summary>
最近，卷积网络在远程感知图像超分辨（SR）中取得了显著的发展，通过最小化回归目标函数，如MSE损失来实现。然而，这些方法经常受到过度平滑的问题，导致图像质量不佳。生成对抗网络具有推理细节的潜力，但容易塌陷，导致不良artefacts。为了解决这些问题，在本文中，我们首先介绍了Diffusion Probabilistic Model（DPM）为远程感知图像SR，称为EDiffSR。EDiffSR易于训练并保持DPM的优点，生成感知良好的图像。与之前使用庞大的UNet进行噪声预测不同，我们开发了高效的Activation Network（EANet），通过简化通道注意力和简单的门控操作，可以减少计算预算。此外，为了在提出的EDiffSR中引入更多有价值的前提知识，我们开发了实用的Conditional Prior Enhancement Module（CPEM），帮助提取更富有的条件。与大多数DPM基于SR模型直接生成条件的方法不同，CPEM帮助保留更多有用的cue，以提高准确的SR。我们在四个远程感知数据集上进行了广泛的实验，显示EDiffSR可以在模拟和实际远程感知图像上重建高质量的视觉美好图像， both quantitatively and qualitatively。代码将在https://github.com/XY-boy/EDiffSR上公开。
</details></li>
</ul>
<hr>
<h2 id="Improving-Online-Source-free-Domain-Adaptation-for-Object-Detection-by-Unsupervised-Data-Acquisition"><a href="#Improving-Online-Source-free-Domain-Adaptation-for-Object-Detection-by-Unsupervised-Data-Acquisition" class="headerlink" title="Improving Online Source-free Domain Adaptation for Object Detection by Unsupervised Data Acquisition"></a>Improving Online Source-free Domain Adaptation for Object Detection by Unsupervised Data Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19258">http://arxiv.org/abs/2310.19258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Shi, Yanyuan Qiao, Qi Wu, Lingqiao Liu, Feras Dayoub</li>
<li>for: 提高移动机器人中的适应对象检测，因为部署在多样化和未知环境中是挑战。</li>
<li>methods: 使用线上源自由领域适应（O-SFDA）实时模型适应，通过目标领域流动不标注数据来进行适应。</li>
<li>results: 比较现有状态 искусственный智能技术，我们的方法在实际 dataset 上表现出色，说明了不supervised数据获取可以提高移动机器人中的适应对象检测。<details>
<summary>Abstract</summary>
Effective object detection in mobile robots is challenged by deployment in diverse and unfamiliar environments. Online Source-Free Domain Adaptation (O-SFDA) offers real-time model adaptation using a stream of unlabeled data from a target domain. However, not all captured frames in mobile robotics contain information that is beneficial for adaptation, particularly when there is a strong domain shift. This paper introduces a novel approach to enhance O-SFDA for adaptive object detection in mobile robots via unsupervised data acquisition. Our methodology prioritizes the most informative unlabeled samples for inclusion in the online training process. Empirical evaluation on a real-world dataset reveals that our method outperforms existing state-of-the-art O-SFDA techniques, demonstrating the viability of unsupervised data acquisition for improving adaptive object detection in mobile robots.
</details>
<details>
<summary>摘要</summary>
efficient object detection in mobile robots is challenging due to deployment in diverse and unfamiliar environments. Online Source-Free Domain Adaptation (O-SFDA) provides real-time model adaptation using a stream of unlabeled data from the target domain. However, not all captured frames in mobile robotics contain useful information for adaptation, especially when there is a strong domain shift. This paper proposes a novel approach to enhance O-SFDA for adaptive object detection in mobile robots through unsupervised data acquisition. Our method prioritizes the most informative unlabeled samples for inclusion in the online training process. Empirical evaluation on a real-world dataset shows that our method outperforms existing state-of-the-art O-SFDA techniques, demonstrating the feasibility of unsupervised data acquisition for improving adaptive object detection in mobile robots.Here's the text with Traditional Chinese characters:efficient object detection in mobile robots is challenging due to deployment in diverse and unfamiliar environments. Online Source-Free Domain Adaptation (O-SFDA) provides real-time model adaptation using a stream of unlabeled data from the target domain. However, not all captured frames in mobile robotics contain useful information for adaptation, especially when there is a strong domain shift. This paper proposes a novel approach to enhance O-SFDA for adaptive object detection in mobile robots through unsupervised data acquisition. Our method prioritizes the most informative unlabeled samples for inclusion in the online training process. Empirical evaluation on a real-world dataset shows that our method outperforms existing state-of-the-art O-SFDA techniques, demonstrating the feasibility of unsupervised data acquisition for improving adaptive object detection in mobile robots.
</details></li>
</ul>
<hr>
<h2 id="A-High-Resolution-Dataset-for-Instance-Detection-with-Multi-View-Instance-Capture"><a href="#A-High-Resolution-Dataset-for-Instance-Detection-with-Multi-View-Instance-Capture" class="headerlink" title="A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture"></a>A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19257">http://arxiv.org/abs/2310.19257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianqian Shen, Yunhan Zhao, Nahyun Kwon, Jeeeun Kim, Yanan Li, Shu Kong</li>
<li>for: 这个论文的目标是提出一个新的实例检测 dataset 和协议，以提高实例检测领域的研究。</li>
<li>methods: 该论文使用了多视图实例捕捉和多种场景图像，并使用了自动批注的方式来生成训练图像。</li>
<li>results: 研究发现，使用存在检测模型（Segment Anything Model，SAM）和自然语言推荐的自我超视图表示（DINOv2），可以达到&gt;10 AP的提升，比抽象的实例检测模型（如FasterRCNN和RetinaNet）更好。<details>
<summary>Abstract</summary>
Instance detection (InsDet) is a long-lasting problem in robotics and computer vision, aiming to detect object instances (predefined by some visual examples) in a cluttered scene. Despite its practical significance, its advancement is overshadowed by Object Detection, which aims to detect objects belonging to some predefined classes. One major reason is that current InsDet datasets are too small in scale by today's standards. For example, the popular InsDet dataset GMU (published in 2016) has only 23 instances, far less than COCO (80 classes), a well-known object detection dataset published in 2014. We are motivated to introduce a new InsDet dataset and protocol. First, we define a realistic setup for InsDet: training data consists of multi-view instance captures, along with diverse scene images allowing synthesizing training images by pasting instance images on them with free box annotations. Second, we release a real-world database, which contains multi-view capture of 100 object instances, and high-resolution (6k x 8k) testing images. Third, we extensively study baseline methods for InsDet on our dataset, analyze their performance and suggest future work. Somewhat surprisingly, using the off-the-shelf class-agnostic segmentation model (Segment Anything Model, SAM) and the self-supervised feature representation DINOv2 performs the best, achieving >10 AP better than end-to-end trained InsDet models that repurpose object detectors (e.g., FasterRCNN and RetinaNet).
</details>
<details>
<summary>摘要</summary>
首先，我们定义了实际的InsDet设置：训练数据包括多视图实例捕捉，以及包含多种场景图像，以便通过粘贴实例图像到它们上面并提供自由框注释来生成训练图像。第二，我们发布了一个真实的数据库，它包含100个对象实例的多视图捕捉和高分辨率（6000 x 8000）测试图像。第三，我们广泛研究了InsDet基线方法，分析其性能并建议未来的工作。有些surprisingly，使用的�分类无关的分割模型（Segment Anything Model，SAM）和自然的自我指导特征表示（DINOv2）perform the best， exceeding >10 AP better than end-to-end trained InsDet models that repurpose object detectors（例如，FasterRCNN和RetinaNet）。
</details></li>
</ul>
<hr>
<h2 id="There-Are-No-Data-Like-More-Data-Datasets-for-Deep-Learning-in-Earth-Observation"><a href="#There-Are-No-Data-Like-More-Data-Datasets-for-Deep-Learning-in-Earth-Observation" class="headerlink" title="There Are No Data Like More Data- Datasets for Deep Learning in Earth Observation"></a>There Are No Data Like More Data- Datasets for Deep Learning in Earth Observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19231">http://arxiv.org/abs/2310.19231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Schmitt, Seyed Ali Ahmadi, Yonghao Xu, Gulsen Taskin, Ujjwal Verma, Francescopaolo Sica, Ronny Hansch</li>
<li>for: 本文旨在强调地球观测数据集的重要性，并将地球观测领域的机器学习数据集和应用集中心stage。</li>
<li>methods: 本文主要采用历史发展评估、现有资源描述和未来发展 perspective来探讨地球观测领域的机器学习数据集。</li>
<li>results: 本文希望通过强调地球观测数据集的重要性，为地球观测领域的机器学习研究提供一个新的视角，并且预期能够促进地球观测领域的机器学习研究发展。<details>
<summary>Abstract</summary>
Carefully curated and annotated datasets are the foundation of machine learning, with particularly data-hungry deep neural networks forming the core of what is often called Artificial Intelligence (AI). Due to the massive success of deep learning applied to Earth Observation (EO) problems, the focus of the community has been largely on the development of ever-more sophisticated deep neural network architectures and training strategies largely ignoring the overall importance of datasets. For that purpose, numerous task-specific datasets have been created that were largely ignored by previously published review articles on AI for Earth observation. With this article, we want to change the perspective and put machine learning datasets dedicated to Earth observation data and applications into the spotlight. Based on a review of the historical developments, currently available resources are described and a perspective for future developments is formed. We hope to contribute to an understanding that the nature of our data is what distinguishes the Earth observation community from many other communities that apply deep learning techniques to image data, and that a detailed understanding of EO data peculiarities is among the core competencies of our discipline.
</details>
<details>
<summary>摘要</summary>
仔细挑选和注释的数据集是机器学习的基础，特别是深度神经网络作为人工智能（AI）的核心。由于深度学习在地球观测（EO）问题上取得了巨大成功，因此社区的焦点主要集中在发展更加复杂的深度神经网络架构和训练策略上，忽视了数据集的重要性。为了改变这种情况，我们创建了许多任务特定的数据集，这些数据集在前一些发表的评论文章中几乎未得到批注。在这篇文章中，我们想要把机器学习专门为地球观测数据和应用的数据集和技术放在主要的位置。通过审查历史发展，我们现在描述了可用的资源，并形成了未来发展的视角。我们希望通过这篇文章，让读者理解，地球观测社区与其他使用深度学习技术处理图像数据的社区不同，我们的数据特点是我们的专业领域的核心。
</details></li>
</ul>
<hr>
<h2 id="CHAMMI-A-benchmark-for-channel-adaptive-models-in-microscopy-imaging"><a href="#CHAMMI-A-benchmark-for-channel-adaptive-models-in-microscopy-imaging" class="headerlink" title="CHAMMI: A benchmark for channel-adaptive models in microscopy imaging"></a>CHAMMI: A benchmark for channel-adaptive models in microscopy imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19224">http://arxiv.org/abs/2310.19224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaudatascience/channel_adaptive_models">https://github.com/chaudatascience/channel_adaptive_models</a></li>
<li>paper_authors: Zitong Chen, Chau Pham, Siqi Wang, Michael Doron, Nikita Moshkov, Bryan A. Plummer, Juan C. Caicedo</li>
<li>for: 这个论文的目的是为了研究适应通道数的神经网络模型，以便在生物实验中处理不同通道数的单元图像。</li>
<li>methods: 这篇论文使用了多种现有技术，包括变化通道数的神经网络模型和生物学上相关的评价框架，以评估这些模型在不同通道数下的表现。</li>
<li>results: 论文发现，适应通道数的神经网络模型可以更好地泛化到异常任务，并且可以在计算效率方面具有优势。同时，论文提供了一个抽象的数据集和评价API，以便在未来的研究和应用中进行对比。<details>
<summary>Abstract</summary>
Most neural networks assume that input images have a fixed number of channels (three for RGB images). However, there are many settings where the number of channels may vary, such as microscopy images where the number of channels changes depending on instruments and experimental goals. Yet, there has not been a systemic attempt to create and evaluate neural networks that are invariant to the number and type of channels. As a result, trained models remain specific to individual studies and are hardly reusable for other microscopy settings. In this paper, we present a benchmark for investigating channel-adaptive models in microscopy imaging, which consists of 1) a dataset of varied-channel single-cell images, and 2) a biologically relevant evaluation framework. In addition, we adapted several existing techniques to create channel-adaptive models and compared their performance on this benchmark to fixed-channel, baseline models. We find that channel-adaptive models can generalize better to out-of-domain tasks and can be computationally efficient. We contribute a curated dataset (https://doi.org/10.5281/zenodo.7988357) and an evaluation API (https://github.com/broadinstitute/MorphEm.git) to facilitate objective comparisons in future research and applications.
</details>
<details>
<summary>摘要</summary>
大多数神经网络假设输入图像有固定数量的通道（三个 для RGB 图像）。然而，有许多情况下，通道的数量可能会变化，如微scopic 图像中的通道数量，它们取决于实验室设备和实验目标。然而，没有系统地创建和评估可以适应不同通道数量的神经网络。因此，训练的模型往往只能在特定的研究中使用，而不能在其他微scopic 设置中 reuse。在这篇论文中，我们提出了一个检验通道适应模型在微scopic 成像中的标准套件，包括：1）一个包含不同通道单元细胞图像的数据集，和2）一个生物学上有意义的评估框架。此外，我们将existings several techniques to create channel-adaptive models and compared their performance on this benchmark to fixed-channel, baseline models. We find that channel-adaptive models can generalize better to out-of-domain tasks and can be computationally efficient. We contribute a curated dataset (https://doi.org/10.5281/zenodo.7988357) and an evaluation API (https://github.com/broadinstitute/MorphEm.git) to facilitate objective comparisons in future research and applications.
</details></li>
</ul>
<hr>
<h2 id="Modular-Anti-noise-Deep-Learning-Network-for-Robotic-Grasp-Detection-Based-on-RGB-Images"><a href="#Modular-Anti-noise-Deep-Learning-Network-for-Robotic-Grasp-Detection-Based-on-RGB-Images" class="headerlink" title="Modular Anti-noise Deep Learning Network for Robotic Grasp Detection Based on RGB Images"></a>Modular Anti-noise Deep Learning Network for Robotic Grasp Detection Based on RGB Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19223">http://arxiv.org/abs/2310.19223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaocong Li</li>
<li>for: 本文旨在提出一种基于单个RGB图像的抓取姿态检测方法，以便在实际应用中提高抓取精度。</li>
<li>methods: 该方法使用模块学习网络，结合抓取检测和语义分割，以适应 robot 装备平行板抓取器。网络不仅可以识别抓取对象，还可以结合先前的抓取分析和语义分割，从而提高抓取精度。</li>
<li>results: 实验和评估结果表明，提出的方法可以准确地检测抓取姿态，并且可以适应受扰和杂乱的视觉。该方法的设计具有可学习性和可靠性，并且可以在实际应用中实现可靠的抓取姿态检测。<details>
<summary>Abstract</summary>
While traditional methods relies on depth sensors, the current trend leans towards utilizing cost-effective RGB images, despite their absence of depth cues. This paper introduces an interesting approach to detect grasping pose from a single RGB image. To this end, we propose a modular learning network augmented with grasp detection and semantic segmentation, tailored for robots equipped with parallel-plate grippers. Our network not only identifies graspable objects but also fuses prior grasp analyses with semantic segmentation, thereby boosting grasp detection precision. Significantly, our design exhibits resilience, adeptly handling blurred and noisy visuals. Key contributions encompass a trainable network for grasp detection from RGB images, a modular design facilitating feasible grasp implementation, and an architecture robust against common image distortions. We demonstrate the feasibility and accuracy of our proposed approach through practical experiments and evaluations.
</details>
<details>
<summary>摘要</summary>
tradicional方法依靠深度感知器，当前趋势则是使用便宜的RGB图像，尽管它们缺乏深度cue。这篇论文介绍了一种有趣的方法来探测抓取姿势从单个RGB图像中。为此，我们提议一种模块化学习网络，其中包括抓取检测和semantic分类，适用于配备平行板抓取器的机器人。我们的网络不仅可以识别可抓取的物体，还可以将先前的抓取分析与semantic分类相结合，从而提高抓取检测精度。另外，我们的设计具有抗锈和抗噪特性，可以 effectively 处理模糊和噪声的视觉。我们的主要贡献包括一种可调网络 дляRGB图像中的抓取检测、一种可实施的模块化设计以及一种对常见图像扭曲有效的架构。我们通过实验和评估证明了我们的提议的可行性和准确性。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Category-Discovery-with-Clustering-Assignment-Consistency"><a href="#Generalized-Category-Discovery-with-Clustering-Assignment-Consistency" class="headerlink" title="Generalized Category Discovery with Clustering Assignment Consistency"></a>Generalized Category Discovery with Clustering Assignment Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19210">http://arxiv.org/abs/2310.19210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangli Yang, Xinglin Pan, Irwin King, Zenglin Xu</li>
<li>for: 本研究旨在提出一种基于协同学习的开放世界任务，即生成扩展类发现（Generalized Category Discovery，GCD）。GCD 的目标是自动将未标注的样本分组，使用标注过的数据集中的信息传递。</li>
<li>methods: 我们提出了一种基于协同学习的框架，包括弱和强的扩展变换，以及基于协同假设的准确性学习策略。我们使用这些策略来学习半supervised的表示学习过程，并使用这些表示来构建一个原始稀热网络，并使用社群探测方法来获取分组结果和类别数量。</li>
<li>results: 我们的方法在三个通用 benchmark 上 achieve state-of-the-art 性能，并在三个细化的视觉识别数据集上也达到了优秀的结果。特别是在 ImageNet-100 数据集上，我们的方法与最佳基eline相比，在 \texttt{Novel} 和 \texttt{All} 类上提高了15.5% 和 7.0%。<details>
<summary>Abstract</summary>
Generalized category discovery (GCD) is a recently proposed open-world task. Given a set of images consisting of labeled and unlabeled instances, the goal of GCD is to automatically cluster the unlabeled samples using information transferred from the labeled dataset. The unlabeled dataset comprises both known and novel classes. The main challenge is that unlabeled novel class samples and unlabeled known class samples are mixed together in the unlabeled dataset. To address the GCD without knowing the class number of unlabeled dataset, we propose a co-training-based framework that encourages clustering consistency. Specifically, we first introduce weak and strong augmentation transformations to generate two sufficiently different views for the same sample. Then, based on the co-training assumption, we propose a consistency representation learning strategy, which encourages consistency between feature-prototype similarity and clustering assignment. Finally, we use the discriminative embeddings learned from the semi-supervised representation learning process to construct an original sparse network and use a community detection method to obtain the clustering results and the number of categories simultaneously. Extensive experiments show that our method achieves state-of-the-art performance on three generic benchmarks and three fine-grained visual recognition datasets. Especially in the ImageNet-100 data set, our method significantly exceeds the best baseline by 15.5\% and 7.0\% on the \texttt{Novel} and \texttt{All} classes, respectively.
</details>
<details>
<summary>摘要</summary>
通用类发现（GCD）是一个最近提出的开放世界任务。给定一个包含标注和无标注实例的图像集合，GCD的目标是自动将无标注样本分组使用标注集合中的信息传递。无标注集合包含已知和新类。主要挑战是无标注新类样本和已知类样本在无标注集合中杂mix。为解决GCD而不知道无标注集合的类数，我们提出了基于co-training assumption的框架。Specifically，我们首先引入弱和强变换 transformation来生成两个足够不同的视图 для同一个样本。然后，我们基于co-training assumption，提出一种协同表示学习策略，该策略激励协同 между特征原型相似度和分类划分。最后，我们使用从半超vised表示学习过程中学习的抗性嵌入来构建原始稀有网络，并使用社区探测方法获取分类结果和类数同时。广泛的实验表明，我们的方法在三个通用图像识别 benchmark 上达到了状态机器人的性能，特别是在 ImageNet-100 数据集上，我们的方法在 \texttt{Novel} 和 \texttt{All} 类上超过了最佳基eline的15.5%和7.0%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.CV_2023_10_30/" data-id="clohum98d00k7pj88b53x7slz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.AI_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T12:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/cs.AI_2023_10_30/">cs.AI - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Integrating-Summarization-and-Retrieval-for-Enhanced-Personalization-via-Large-Language-Models"><a href="#Integrating-Summarization-and-Retrieval-for-Enhanced-Personalization-via-Large-Language-Models" class="headerlink" title="Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models"></a>Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20081">http://arxiv.org/abs/2310.20081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris Richardson, Yao Zhang, Kellen Gillespie, Sudipta Kar, Arshdeep Singh, Zeynab Raeesy, Omar Zia Khan, Abhinav Sethy</li>
<li>for: 提高自然语言处理（NLP）系统的用户体验，特别是通过大语言模型（LLM）来更好地个性化用户体验。</li>
<li>methods: 使用语言模型提取过去用户数据，并将其作为下游任务的提示进行个性化。</li>
<li>results:  experiments show 我们的方法可以在实际环境下，即使有时间和成本限制，也能够具有与抽取方法相当或更好的表现，并且可以减少75%的用户数据 Retrieval。<details>
<summary>Abstract</summary>
Personalization, the ability to tailor a system to individual users, is an essential factor in user experience with natural language processing (NLP) systems. With the emergence of Large Language Models (LLMs), a key question is how to leverage these models to better personalize user experiences. To personalize a language model's output, a straightforward approach is to incorporate past user data into the language model prompt, but this approach can result in lengthy inputs exceeding limitations on input length and incurring latency and cost issues. Existing approaches tackle such challenges by selectively extracting relevant user data (i.e. selective retrieval) to construct a prompt for downstream tasks. However, retrieval-based methods are limited by potential information loss, lack of more profound user understanding, and cold-start challenges. To overcome these limitations, we propose a novel summary-augmented approach by extending retrieval-augmented personalization with task-aware user summaries generated by LLMs. The summaries can be generated and stored offline, enabling real-world systems with runtime constraints like voice assistants to leverage the power of LLMs. Experiments show our method with 75% less of retrieved user data is on-par or outperforms retrieval augmentation on most tasks in the LaMP personalization benchmark. We demonstrate that offline summarization via LLMs and runtime retrieval enables better performance for personalization on a range of tasks under practical constraints.
</details>
<details>
<summary>摘要</summary>
personalization, tailoring a system to individual users, is a crucial aspect of user experience with natural language processing (NLP) systems. with the emergence of large language models (LLMs), a key question is how to leverage these models to better personalize user experiences. to personalize a language model's output, a straightforward approach is to incorporate past user data into the language model prompt, but this approach can result in lengthy inputs exceeding limitations on input length and incurring latency and cost issues. existing approaches tackle such challenges by selectively extracting relevant user data (i.e. selective retrieval) to construct a prompt for downstream tasks. however, retrieval-based methods are limited by potential information loss, lack of more profound user understanding, and cold-start challenges. to overcome these limitations, we propose a novel summary-augmented approach by extending retrieval-augmented personalization with task-aware user summaries generated by LLMs. the summaries can be generated and stored offline, enabling real-world systems with runtime constraints like voice assistants to leverage the power of LLMs. experiments show our method with 75% less of retrieved user data is on-par or outperforms retrieval augmentation on most tasks in the LaMP personalization benchmark. we demonstrate that offline summarization via LLMs and runtime retrieval enables better performance for personalization on a range of tasks under practical constraints.
</details></li>
</ul>
<hr>
<h2 id="FOCAL-Contrastive-Learning-for-Multimodal-Time-Series-Sensing-Signals-in-Factorized-Orthogonal-Latent-Space"><a href="#FOCAL-Contrastive-Learning-for-Multimodal-Time-Series-Sensing-Signals-in-Factorized-Orthogonal-Latent-Space" class="headerlink" title="FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space"></a>FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20071">http://arxiv.org/abs/2310.20071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengzhong Liu, Tomoyoshi Kimura, Dongxin Liu, Ruijie Wang, Jinyang Li, Suhas Diggavi, Mani Srivastava, Tarek Abdelzaher</li>
<li>for: 提出了一种新的对比学习框架FOCAL，用于从多Modal时间序列感知信号中提取全面特征，通过无监督训练。</li>
<li>methods: FOCAL使用多Modal时间序列中的特征编码，并使用modal匹配目标和变换不变目标来提取共同特征和专用特征。同时，它还引入了时间结构约束，以保证模式特征之间的距离关系。</li>
<li>results: FOCAL在四个多Modal感知数据集上进行了广泛的评估，并与现有的基eline进行了比较。结果显示，FOCAL在下游任务中具有明显的优势，具有较高的准确率和较低的损失值。<details>
<summary>Abstract</summary>
This paper proposes a novel contrastive learning framework, called FOCAL, for extracting comprehensive features from multimodal time-series sensing signals through self-supervised training. Existing multimodal contrastive frameworks mostly rely on the shared information between sensory modalities, but do not explicitly consider the exclusive modality information that could be critical to understanding the underlying sensing physics. Besides, contrastive frameworks for time series have not handled the temporal information locality appropriately. FOCAL solves these challenges by making the following contributions: First, given multimodal time series, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective. In contrast, the private space extracts modality-exclusive information through a transformation-invariant objective. Second, we propose a temporal structural constraint for modality features, such that the average distance between temporally neighboring samples is no larger than that of temporally distant samples. Extensive evaluations are performed on four multimodal sensing datasets with two backbone encoders and two classifiers to demonstrate the superiority of FOCAL. It consistently outperforms the state-of-the-art baselines in downstream tasks with a clear margin, under different ratios of available labels. The code and self-collected dataset are available at https://github.com/tomoyoshki/focal.
</details>
<details>
<summary>摘要</summary>
First, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective, while the private space extracts modality-exclusive information through a transformation-invariant objective.Second, it introduces a temporal structural constraint for modality features, such that the average distance between temporally neighboring samples is no larger than that of temporally distant samples. This ensures that the model learns to capture the temporal relationships between samples.The proposed framework is evaluated on four multimodal sensing datasets with two backbone encoders and two classifiers. The results show that FOCAL consistently outperforms the state-of-the-art baselines in downstream tasks with a clear margin, under different ratios of available labels. The code and self-collected dataset are available at https://github.com/tomoyoshki/focal.
</details></li>
</ul>
<hr>
<h2 id="Vignat-Vulnerability-identification-by-learning-code-semantics-via-graph-attention-networks"><a href="#Vignat-Vulnerability-identification-by-learning-code-semantics-via-graph-attention-networks" class="headerlink" title="Vignat: Vulnerability identification by learning code semantics via graph attention networks"></a>Vignat: Vulnerability identification by learning code semantics via graph attention networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20067">http://arxiv.org/abs/2310.20067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Liu, Gail Kaiser</li>
<li>for: 本研究旨在提高软件安全性，通过自适应学习图级别Semantic Representation来发现漏洞。</li>
<li>methods: 我们使用Code Property Graphs (CPGs)来表示代码，并使用Graph Attention Networks (GATs)进行漏洞检测。</li>
<li>results: 我们在可靠的 datasets 上实现了 $57.38%$ 的准确率，并且可以获得漏洞模式的可读性。<details>
<summary>Abstract</summary>
Vulnerability identification is crucial to protect software systems from attacks for cyber-security. However, huge projects have more than millions of lines of code, and the complex dependencies make it hard to carry out traditional static and dynamic methods. Furthermore, the semantic structure of various types of vulnerabilities differs greatly and may occur simultaneously, making general rule-based methods difficult to extend. In this paper, we propose \textit{Vignat}, a novel attention-based framework for identifying vulnerabilities by learning graph-level semantic representations of code. We represent codes with code property graphs (CPGs) in fine grain and use graph attention networks (GATs) for vulnerability detection. The results show that Vignat is able to achieve $57.38\%$ accuracy on reliable datasets derived from popular C libraries. Furthermore, the interpretability of our GATs provides valuable insights into vulnerability patterns.
</details>
<details>
<summary>摘要</summary>
<<SYS>>transliteration: Vulnerability zhìyè shì yòu zhìyè zhòngjì xìtiě de yìqie zhòngjì zhìyè shì yòu xìtiě zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì y
</details></li>
</ul>
<hr>
<h2 id="Concept-Alignment-as-a-Prerequisite-for-Value-Alignment"><a href="#Concept-Alignment-as-a-Prerequisite-for-Value-Alignment" class="headerlink" title="Concept Alignment as a Prerequisite for Value Alignment"></a>Concept Alignment as a Prerequisite for Value Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20059">http://arxiv.org/abs/2310.20059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunayana Rane, Mark Ho, Ilia Sucholutsky, Thomas L. Griffiths</li>
<li>for: 本研究旨在建立AI系统，以安全可靠的方式与人类交互。</li>
<li>methods: 本研究使用 inverse reinforcement learning Setting 进行形式化分析，并证明了概念Alignment 是值Alignment 的必要前提。</li>
<li>results: 人类参与者在实验中证明了，当agent acts intentionally时，人类会根据agent使用的概念来进行合理的思维。<details>
<summary>Abstract</summary>
Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -- agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model.
</details>
<details>
<summary>摘要</summary>
<<SYS>>值Alignment是AI系统与人类之间安全、可靠交互的关键。然而，人类的价值观与可能理解和评估世界的概念相关。因此，概念Alignment是价值Alignment的前提——代理需要与人类的情况表示相对应才能成功地Alignment其价值。我们在 inverse reinforcement learning  Setting formally analyze the concept alignment problem, show that neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. In addition, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model.中文简体版
</details></li>
</ul>
<hr>
<h2 id="Constrained-Hierarchical-Monte-Carlo-Belief-State-Planning"><a href="#Constrained-Hierarchical-Monte-Carlo-Belief-State-Planning" class="headerlink" title="Constrained Hierarchical Monte Carlo Belief-State Planning"></a>Constrained Hierarchical Monte Carlo Belief-State Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20054">http://arxiv.org/abs/2310.20054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arec Jamgochian, Hugo Buurmeijer, Kyle H. Wray, Anthony Corso, Mykel J. Kochenderfer</li>
<li>for: 这篇论文目的是为了解决受限制的部分可观察Markov问题（CPOMDP）中的最佳规划问题，并在不同的状态和转换 uncertainty 下保持安全的规划。</li>
<li>methods: 这篇论文使用的方法是将问题分解为较低层的控制问题，使用高层的动作原则（options）来进行搜寻。</li>
<li>results: 这篇论文的结果显示，如果将基本的选项控制器定义为满足指派的紧张预算，那么COBeTS就可以确保满足紧张预算任何时候。否则，COBeTS将导引搜寻 towards a safe sequence of option primitives，并使用层次监控来实现runtime safety。<details>
<summary>Abstract</summary>
Optimal plans in Constrained Partially Observable Markov Decision Processes (CPOMDPs) maximize reward objectives while satisfying hard cost constraints, generalizing safe planning under state and transition uncertainty. Unfortunately, online CPOMDP planning is extremely difficult in large or continuous problem domains. In many large robotic domains, hierarchical decomposition can simplify planning by using tools for low-level control given high-level action primitives (options). We introduce Constrained Options Belief Tree Search (COBeTS) to leverage this hierarchy and scale online search-based CPOMDP planning to large robotic problems. We show that if primitive option controllers are defined to satisfy assigned constraint budgets, then COBeTS will satisfy constraints anytime. Otherwise, COBeTS will guide the search towards a safe sequence of option primitives, and hierarchical monitoring can be used to achieve runtime safety. We demonstrate COBeTS in several safety-critical, constrained partially observable robotic domains, showing that it can plan successfully in continuous CPOMDPs while non-hierarchical baselines cannot.
</details>
<details>
<summary>摘要</summary>
最佳计划在受限 partially observable Markov decision process (CPOMDP) 中最大化 reward 目标，同时满足硬件成本限制，广泛应用于安全观察下的规划。然而，在大型或连续问题领域中的线上 CPOMDP 规划具有极高的问题难度。在许多大型机器人领域中，层次分解可以简化规划，使用工具 для low-level control 给 high-level action primitives（选项）。我们介绍 Constrained Options Belief Tree Search (COBeTS)，以利用这个层次，将线上搜寻基于 CPOMDP 规划 scales 到大型机器人问题领域。我们证明，如果单元选项控制器是将任务分配到硬件成本预算，COBeTS 就一定会满足限制。否则，COBeTS 将导引搜寻 towards a safe sequence of option primitives，并使用层次监控来实现 runtime 安全。我们在 Several safety-critical, constrained partially observable robotic domains 中评估 COBeTS，结果显示它可以在连续 CPOMDP 中成功规划，而非层次基于的基底不能。
</details></li>
</ul>
<hr>
<h2 id="Look-At-Me-No-Replay-SurpriseNet-Anomaly-Detection-Inspired-Class-Incremental-Learning"><a href="#Look-At-Me-No-Replay-SurpriseNet-Anomaly-Detection-Inspired-Class-Incremental-Learning" class="headerlink" title="Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class Incremental Learning"></a>Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20052">http://arxiv.org/abs/2310.20052</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tachyonicclock/surprisenet-cikm-23">https://github.com/tachyonicclock/surprisenet-cikm-23</a></li>
<li>paper_authors: Anton Lee, Yaqian Zhang, Heitor Murilo Gomes, Albert Bifet, Bernhard Pfahringer</li>
<li>for: 这种研究旨在解决人工智能网络在不断学习中遇到的悬峰性干扰和分类境界知识的问题。</li>
<li>methods: 这种方法使用参数隔离方法和基于异常检测的自适应器来解决悬峰性干扰，并且不依赖于图像特定的逻辑假设。</li>
<li>results: 实验表明，SurpriseNet在传统视觉不断学习标准准则上表现出色，以及在结构化数据集上。源代码可以在<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.8247906%E5%92%8Chttps://github.com/tachyonicClock/SurpriseNet-CIKM-23%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://doi.org/10.5281/zenodo.8247906和https://github.com/tachyonicClock/SurpriseNet-CIKM-23中下载。</a><details>
<summary>Abstract</summary>
Continual learning aims to create artificial neural networks capable of accumulating knowledge and skills through incremental training on a sequence of tasks. The main challenge of continual learning is catastrophic interference, wherein new knowledge overrides or interferes with past knowledge, leading to forgetting. An associated issue is the problem of learning "cross-task knowledge," where models fail to acquire and retain knowledge that helps differentiate classes across task boundaries. A common solution to both problems is "replay," where a limited buffer of past instances is utilized to learn cross-task knowledge and mitigate catastrophic interference. However, a notable drawback of these methods is their tendency to overfit the limited replay buffer. In contrast, our proposed solution, SurpriseNet, addresses catastrophic interference by employing a parameter isolation method and learning cross-task knowledge using an auto-encoder inspired by anomaly detection. SurpriseNet is applicable to both structured and unstructured data, as it does not rely on image-specific inductive biases. We have conducted empirical experiments demonstrating the strengths of SurpriseNet on various traditional vision continual-learning benchmarks, as well as on structured data datasets. Source code made available at https://doi.org/10.5281/zenodo.8247906 and https://github.com/tachyonicClock/SurpriseNet-CIKM-23
</details>
<details>
<summary>摘要</summary>
In contrast, our proposed solution, SurpriseNet, addresses catastrophic interference by employing a parameter isolation method and learning cross-task knowledge using an auto-encoder inspired by anomaly detection. SurpriseNet is applicable to both structured and unstructured data, as it does not rely on image-specific inductive biases. We have conducted empirical experiments demonstrating the strengths of SurpriseNet on various traditional vision continual-learning benchmarks, as well as on structured data datasets. The source code is available at <https://doi.org/10.5281/zenodo.8247906> and <https://github.com/tachyonicClock/SurpriseNet-CIKM-23>.
</details></li>
</ul>
<hr>
<h2 id="SURF-A-Generalization-Benchmark-for-GNNs-Predicting-Fluid-Dynamics"><a href="#SURF-A-Generalization-Benchmark-for-GNNs-Predicting-Fluid-Dynamics" class="headerlink" title="SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics"></a>SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20049">http://arxiv.org/abs/2310.20049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/s-kuenzli/surf-fluidsimulation">https://github.com/s-kuenzli/surf-fluidsimulation</a></li>
<li>paper_authors: Stefan Künzli, Florian Grötschla, Joël Mathys, Roger Wattenhofer</li>
<li>for: 这个论文是为了测试学习基于图的流体动力学模型的通用性而写的。</li>
<li>methods: 这篇论文使用了学习模型来模拟流体动力学，并使用了特定的数据集来测试和比较不同模型的通用性。</li>
<li>results: 研究发现，当模型需要适应不同的结构、分辨率或热动力学范围时，学习基于图的模型的通用性会受到影响。<details>
<summary>Abstract</summary>
Simulating fluid dynamics is crucial for the design and development process, ranging from simple valves to complex turbomachinery. Accurately solving the underlying physical equations is computationally expensive. Therefore, learning-based solvers that model interactions on meshes have gained interest due to their promising speed-ups. However, it is unknown to what extent these models truly understand the underlying physical principles and can generalize rather than interpolate. Generalization is a key requirement for a general-purpose fluid simulator, which should adapt to different topologies, resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to test the \textit{generalization} of learned graph-based fluid simulators. SURF comprises individual datasets and provides specific performance and generalization metrics for evaluating and comparing different models. We empirically demonstrate the applicability of SURF by thoroughly investigating the two state-of-the-art graph-based models, yielding new insights into their generalization.
</details>
<details>
<summary>摘要</summary>
模拟流体动力学是设计和开发过程中的关键环节，从简单的阀门到复杂的液压机。准确解决下面的物理方程是计算昂贵的。因此，学习型解决方案，即模型在网格上的交互，在计算速度方面表现出了扎根。然而，这些模型是否真正理解下面的物理原理，并能泛化而不仅是 interpolate？泛化是一个关键的要求，以便建立一个通用的流体 simulator，可以适应不同的topology、分辨率或热动力范围。我们提出了 SURF，一个用于测试学习型流体 simulator 的泛化能力的benchmark。SURF包括各个数据集，并提供了特定的性能和泛化指标，用于评估和比较不同的模型。我们进行了大量的实验，证明了 SURF 的可靠性和有用性，并且对两种当前最佳的图像基本模型进行了深入的探索，从而获得了新的理解。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Imitation-Edit-Feedback-for-Factual-Alignment-in-Clinical-Summarization"><a href="#Synthetic-Imitation-Edit-Feedback-for-Factual-Alignment-in-Clinical-Summarization" class="headerlink" title="Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization"></a>Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20033">http://arxiv.org/abs/2310.20033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prakamya Mishra, Zonghai Yao, Shuwei Chen, Beining Wang, Rohan Mittal, Hong Yu<br>for: This paper aims to improve the factual consistency of clinical note summarization using ChatGPT to generate high-quality feedback data.methods: The authors use ChatGPT to generate edit feedback for improving the factual consistency of clinical note summarization.results: The authors evaluate the effectiveness of using GPT edits in human alignment, showing promising results in improving factual consistency.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summarization task. We focus specifically on edit feedback because recent work discusses the shortcomings of human alignment via preference feedback in complex situations (such as clinical NLP tasks that require extensive expert knowledge), as well as some advantages of collecting edit feedback from domain experts. In addition, although GPT has reached the expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much previous work discussing whether GPT can generate expert-level edit feedback for LMs in the clinical note summarization task. We hope to fill this gap. Finally, our evaluations demonstrate the potential use of GPT edits in human alignment, especially from a factuality perspective.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT和LLaMA家族在捕捉和简化关键上下文信息方面表现出了异常的能力，并达到了当前最佳性能的概要任务。然而，社区对这些模型的幻觉问题仍然增长。LLM sometimes generates factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summarization task. We focus specifically on edit feedback because recent work discusses the shortcomings of human alignment via preference feedback in complex situations (such as clinical NLP tasks that require extensive expert knowledge), as well as some advantages of collecting edit feedback from domain experts. In addition, although GPT has reached the expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much previous work discussing whether GPT can generate expert-level edit feedback for LMs in the clinical note summarization task. We hope to fill this gap. Finally, our evaluations demonstrate the potential use of GPT edits in human alignment, especially from a factuality perspective.
</details></li>
</ul>
<hr>
<h2 id="GOPlan-Goal-conditioned-Offline-Reinforcement-Learning-by-Planning-with-Learned-Models"><a href="#GOPlan-Goal-conditioned-Offline-Reinforcement-Learning-by-Planning-with-Learned-Models" class="headerlink" title="GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models"></a>GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20025">http://arxiv.org/abs/2310.20025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mianchu Wang, Rui Yang, Xi Chen, Meng Fang</li>
<li>for: 学习通用策略从多种和多任务的离线数据集中。</li>
<li>methods: 使用两stage模型基于框架，包括预训练一个可以捕捉多种动作分布的先前策略，然后使用划算法与规划来生成假数据 для练化策略。</li>
<li>results: 在多种离线多目标摆动任务上达到了状态之 arts 性能，并且能够处理小数据预算和不同目标的扩展。<details>
<summary>Abstract</summary>
Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by planning with learned models for both intra-trajectory and inter-trajectory goals. Through experimental evaluations, we demonstrate that GOPlan achieves state-of-the-art performance on various offline multi-goal manipulation tasks. Moreover, our results highlight the superior ability of GOPlan to handle small data budgets and generalize to OOD goals.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese<</SYS>>Offline目标条件RL（GCRL）提供了一个可行的 парадиг，从多样化和多任务的离线数据集中学习通用策略。尽管最近有所进步，主要的离线GCRL方法受限于无模型的方法，这限制了它们对有限数据预算和未看过目标的泛化能力。在这项工作中，我们提出了一种新的两阶段模型基于框架，即目标条件的计划（GOPlan），包括（1）预训练一个能够捕捉多模态动作分布在多目标数据集中的先前策略；（2）使用计划方法和规划来生成假 trajectory 进行迭代优化策略。具体来说，先前策略基于带有优化的 Conditioned Generative Adversarial Networks（CGANs），可以快速分离出异常行为（OOD）的问题。为了进一步优化策略，计划方法生成了高质量的假数据，通过规划learned模型来实现内 trajectory和间 trajectory的目标。经过实验评估，我们表明 GOPlan 可以在多个离线多目标机械处理任务中 дости得状态之Art的表现。此外，我们的结果还 highlight了 GOPlan 对小数据预算和 OOD 目标的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Topology-Recoverability-Prediction-for-Ad-Hoc-Robot-Networks-A-Data-Driven-Fault-Tolerant-Approach"><a href="#Topology-Recoverability-Prediction-for-Ad-Hoc-Robot-Networks-A-Data-Driven-Fault-Tolerant-Approach" class="headerlink" title="Topology Recoverability Prediction for Ad-Hoc Robot Networks: A Data-Driven Fault-Tolerant Approach"></a>Topology Recoverability Prediction for Ad-Hoc Robot Networks: A Data-Driven Fault-Tolerant Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20024">http://arxiv.org/abs/2310.20024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matin Macktoobian, Zhan Shu, Qing Zhao</li>
<li>for: 这个论文主要是为了预测无架 robot 网络中发生故障后网络的重建可能性。</li>
<li>methods: 本文使用了 Bayesian Gaussian mixture models 的二条通路数据驱动模型，透过两条不同的预测路径，预测网络中发生故障后网络的重建可能性。</li>
<li>results: 本文的结果显示，与文献中现有最佳策略相比，二条通路数据驱动模型能够成功地解决网络（不）可回复预测问题。<details>
<summary>Abstract</summary>
Faults occurring in ad-hoc robot networks may fatally perturb their topologies leading to disconnection of subsets of those networks. Optimal topology synthesis is generally resource-intensive and time-consuming to be done in real time for large ad-hoc robot networks. One should only perform topology re-computations if the probability of topology recoverability after the occurrence of any fault surpasses that of its irrecoverability. We formulate this problem as a binary classification problem. Then, we develop a two-pathway data-driven model based on Bayesian Gaussian mixture models that predicts the solution to a typical problem by two different pre-fault and post-fault prediction pathways. The results, obtained by the integration of the predictions of those pathways, clearly indicate the success of our model in solving the topology (ir)recoverability prediction problem compared to the best of current strategies found in the literature.
</details>
<details>
<summary>摘要</summary>
FAULTS 发生在随机机器人网络中可能导致网络结构的不稳定，从而导致一些网络的分支断开。优化网络结构是在大规模随机机器人网络中实时进行的资源投入和时间consuming的任务。我们只应在缺陷发生后的概率超过了不可回复的概率时进行网络结构重新计算。我们将这个问题形式化为 binary 分类问题。然后，我们开发了基于极 bayesian Gaussian mixture 模型的两路数据驱动模型，该模型通过两个不同的预缺陷和后缺陷预测路径来预测一个典型问题的解决方案。结果显示，通过将这两个路径的预测结果融合，我们的模型在解决网络（不）可回复预测问题上具有显著的成功，比文献中最佳策略更高。
</details></li>
</ul>
<hr>
<h2 id="Multiscale-Feature-Attribution-for-Outliers"><a href="#Multiscale-Feature-Attribution-for-Outliers" class="headerlink" title="Multiscale Feature Attribution for Outliers"></a>Multiscale Feature Attribution for Outliers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20012">http://arxiv.org/abs/2310.20012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeff Shen, Peter Melchior</li>
<li>for: 这个论文是为了解决自动异常点检测问题，即使数据量很大，也能够更快、更可重复地检测到异常点。</li>
<li>methods: 这篇论文提出了一种新的特征归因方法，即反多尺度遮盖方法，这种方法专门针对异常点，因为在异常点上我们知道的特征很少，模型性能可能也不太好。</li>
<li>results: 论文的实验结果表明，这种特征归因方法在检测银河谱pectra中的异常点时比较有 interpretable 性，比如alternative归因方法更好。<details>
<summary>Abstract</summary>
Machine learning techniques can automatically identify outliers in massive datasets, much faster and more reproducible than human inspection ever could. But finding such outliers immediately leads to the question: which features render this input anomalous? We propose a new feature attribution method, Inverse Multiscale Occlusion, that is specifically designed for outliers, for which we have little knowledge of the type of features we want to identify and expect that the model performance is questionable because anomalous test data likely exceed the limits of the training data. We demonstrate our method on outliers detected in galaxy spectra from the Dark Energy Survey Instrument and find its results to be much more interpretable than alternative attribution approaches.
</details>
<details>
<summary>摘要</summary>
使用机器学习技术可以自动找出大量数据中的异常数据点，比人工检查更快速和可重复。但是发现这些异常数据点后，我们就会问：哪些特征使这个输入异常？我们提出了一种新的特征归因方法，反向多Scale遮盲，特意为异常数据点设计，我们对这些异常数据点知之甚少，而且预期模型性能很差，因为异常测试数据可能超出了训练数据的范围。我们在银河谱spectra中检测到的异常数据点上应用了这种方法，并发现其结果比替代归因方法更易于理解。
</details></li>
</ul>
<hr>
<h2 id="Evolutionary-Tabletop-Game-Design-A-Case-Study-in-the-Risk-Game"><a href="#Evolutionary-Tabletop-Game-Design-A-Case-Study-in-the-Risk-Game" class="headerlink" title="Evolutionary Tabletop Game Design: A Case Study in the Risk Game"></a>Evolutionary Tabletop Game Design: A Case Study in the Risk Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20008">http://arxiv.org/abs/2310.20008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lana Bertoldo Rossato, Leonardo Boaventura Bombardelli, Anderson Rocha Tavares</li>
<li>for: 这个论文旨在创造和评估桌面游戏，以提高现有游戏的创新和变化。</li>
<li>methods: 这篇论文使用了进化算法和自动游戏测试来创造和评估桌面游戏。</li>
<li>results: 这篇论文的结果表明，通过使用遗传算法和规则引入的智能游戏测试，可以创造出新的桌面游戏变体，比如卡牌游戏和地图游戏。这些变体的游戏时间 shorter，并且比原始游戏更具 equilibrio。但是，这种方法还有一些局限性，例如，在许多情况下，目标函数 Correctly pursued，但生成的游戏几乎是平庸的。<details>
<summary>Abstract</summary>
Creating and evaluating games manually is an arduous and laborious task. Procedural content generation can aid by creating game artifacts, but usually not an entire game. Evolutionary game design, which combines evolutionary algorithms with automated playtesting, has been used to create novel board games with simple equipment; however, the original approach does not include complex tabletop games with dice, cards, and maps. This work proposes an extension of the approach for tabletop games, evaluating the process by generating variants of Risk, a military strategy game where players must conquer map territories to win. We achieved this using a genetic algorithm to evolve the chosen parameters, as well as a rules-based agent to test the games and a variety of quality criteria to evaluate the new variations generated. Our results show the creation of new variations of the original game with smaller maps, resulting in shorter matches. Also, the variants produce more balanced matches, maintaining the usual drama. We also identified limitations in the process, where, in many cases, where the objective function was correctly pursued, but the generated games were nearly trivial. This work paves the way towards promising research regarding the use of evolutionary game design beyond classic board games.
</details>
<details>
<summary>摘要</summary>
创造和评估游戏手动是一项艰难和劳动密集的任务。生成式内容创造可以帮助，但通常不能创造整个游戏。进化游戏设计，将进化算法与自动游戏测试结合，已经用于创造了一些简单的桌面游戏，但原始方法并不包括复杂的桌面游戏，如骰子、牌和地图。本工作提出了对桌面游戏的扩展，通过生成 variants of Risk，一款军事策略游戏，要求玩家征服地图区域以赢得。我们使用了遗传算法进化选择的参数，以及规则基于的智能客户端来测试游戏，以及多种质量标准来评估新生成的变化。我们的结果显示了创造了原版游戏的新变体，地图较小，比赛更短。此外，新变体具有更平衡的比赛，保持了正常的戏剧。我们还发现了过程中的限制，在许多情况下， objective function 正确追求，但生成的游戏几乎是无聊的。这项工作开启了对进化游戏设计在古典桌面游戏之外的探索。
</details></li>
</ul>
<hr>
<h2 id="Improved-Bayesian-Regret-Bounds-for-Thompson-Sampling-in-Reinforcement-Learning"><a href="#Improved-Bayesian-Regret-Bounds-for-Thompson-Sampling-in-Reinforcement-Learning" class="headerlink" title="Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning"></a>Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20007">http://arxiv.org/abs/2310.20007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmadreza Moradipari, Mohammad Pedramfar, Modjtaba Shokrian Zini, Vaneet Aggarwal</li>
<li>for: 这个论文是为了证明 Thompson Sampling 在强化学习中的首个 Bayesian  regret bound。</li>
<li>methods: 这个论文使用了一种简化学习问题的离散集合环境，并通过 posterior consistency 进行了精细的信息率分析。</li>
<li>results: 这个论文得到了时间不同束的强化学习问题中的上界，其上界为 $\widetilde{O}(H\sqrt{d_{l_1}T})$，其中 $H$ 是 episode length，$d_{l_1}$ 是环境空间的 Kolmogorov $l_1-$ 度量。<details>
<summary>Abstract</summary>
In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\widetilde{O}(H\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们证明了决策者抽取（Thompson Sampling）在强化学习中的第一个悔弃 regret bounds。我们通过简化学习问题，使用离散的代理环境集，并对信息倍数进行细化分析，从而得到时间不同权重学习问题中的上界为$\widetilde{O}(H\sqrt{d_{l_1}T})$，其中$H$是话语长度，$d_{l_1}$是环境空间的科尔莫戈罗夫-$l_1-$度量。然后，我们在各种设置下获得具体的$d_{l_1}$bounds，包括表格、线性和finite mixtures，并讨论了我们的结果如何超越现有的最佳成果。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Limits-of-Learned-Local-Search-Heuristics-Are-You-the-Mightiest-of-the-Meek"><a href="#Unveiling-the-Limits-of-Learned-Local-Search-Heuristics-Are-You-the-Mightiest-of-the-Meek" class="headerlink" title="Unveiling the Limits of Learned Local Search Heuristics: Are You the Mightiest of the Meek?"></a>Unveiling the Limits of Learned Local Search Heuristics: Are You the Mightiest of the Meek?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19990">http://arxiv.org/abs/2310.19990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankur Nath, Alan Kuhnle</li>
<li>for: 这个论文旨在探讨 combine neural networks with local search heuristics 在 combinatorial optimization 领域的实践中的问题。</li>
<li>methods: 这个论文使用的方法包括 Tabu Search 和 deep learning 等多种方法。</li>
<li>results: 研究发现，一个简单的学习基于 Tabu Search 的规则可以超越当前最佳学习规则，并且具有更高的性能和普适性。<details>
<summary>Abstract</summary>
In recent years, combining neural networks with local search heuristics has become popular in the field of combinatorial optimization. Despite its considerable computational demands, this approach has exhibited promising outcomes with minimal manual engineering. However, we have identified three critical limitations in the empirical evaluation of these integration attempts. Firstly, instances with moderate complexity and weak baselines pose a challenge in accurately evaluating the effectiveness of learning-based approaches. Secondly, the absence of an ablation study makes it difficult to quantify and attribute improvements accurately to the deep learning architecture. Lastly, the generalization of learned heuristics across diverse distributions remains underexplored. In this study, we conduct a comprehensive investigation into these identified limitations. Surprisingly, we demonstrate that a simple learned heuristic based on Tabu Search surpasses state-of-the-art (SOTA) learned heuristics in terms of performance and generalizability. Our findings challenge prevailing assumptions and open up exciting avenues for future research and innovation in combinatorial optimization.
</details>
<details>
<summary>摘要</summary>
Recently, combining neural networks with local search heuristics has become popular in the field of combinatorial optimization. Despite its considerable computational demands, this approach has shown promising results with minimal manual engineering. However, we have identified three critical limitations in the empirical evaluation of these integration attempts. Firstly, instances with moderate complexity and weak baselines pose a challenge in accurately evaluating the effectiveness of learning-based approaches. Secondly, the absence of an ablation study makes it difficult to quantify and attribute improvements accurately to the deep learning architecture. Lastly, the generalization of learned heuristics across diverse distributions remains underexplored. In this study, we conduct a comprehensive investigation into these identified limitations. Surprisingly, we demonstrate that a simple learned heuristic based on Tabu Search surpasses state-of-the-art (SOTA) learned heuristics in terms of performance and generalizability. Our findings challenge prevailing assumptions and open up exciting avenues for future research and innovation in combinatorial optimization.Translation in Simplified Chinese:近些年来，将神经网络与本地搜索规则结合在一起已成为 combinatorial optimization 领域的 популяр趋势。尽管它们的计算需求相对较高，但这种方法在 minimal 的人工工程下已经展现出了扎心的成果。然而，我们在实证评估中发现了三个关键的限制。首先，有中等复杂度和弱基线的实例会增加评估学习基于方法的准确性问题。其次，缺乏抽象研究使得准确地归因改进到深度学习架构很困难。最后，学习基于分布的搜索规则的总体化仍未得到充分的探索。在这个研究中，我们通过对这些已知的限制进行全面的调查和分析，并 surprisingly 发现一种简单的学习基于 Tabu Search 的规则，在性能和总体化方面超越了当前的学习基于方法。我们的发现推翻了先前的假设，开 up 了未来研究和创新的潜在空间。
</details></li>
</ul>
<hr>
<h2 id="BioInstruct-Instruction-Tuning-of-Large-Language-Models-for-Biomedical-Natural-Language-Processing"><a href="#BioInstruct-Instruction-Tuning-of-Large-Language-Models-for-Biomedical-Natural-Language-Processing" class="headerlink" title="BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing"></a>BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19975">http://arxiv.org/abs/2310.19975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hieu Tran, Zhichao Yang, Zonghai Yao, Hong Yu</li>
<li>for: 本研究旨在提高生物医学自然语言处理（BioNLP）领域中大语言模型（LLMs）的性能，通过特定任务的指令特有数据集（BioInstruct）进行调整。</li>
<li>methods: 本研究使用GPT-4语言模型生成了超过25,000个例子的自然语言指令数据集（BioInstruct），并通过这些指令进行LLMs的微调。</li>
<li>results: 通过BioInstruct数据集的微调，我们可以提高LLMs在BioNLP应用中的性能，包括信息抽取、问答和文本生成等。此外，我们还发现了多任务学习原则如何帮助指令的贡献。<details>
<summary>Abstract</summary>
Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&2, 7B\&13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principles.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在许多自然语言处理（NLP）任务中获得了很大的成功。这是通过预训练LLMs在庞大数据量上并在特定领域进行调整而实现的。然而，只有很少的指令在生物医学领域发布。为解决这个问题，我们介绍了 BioInstruct，一个自定义任务特定的指令集合，包含超过25,000个示例。这个数据集是通过提示一个GPT-4语言模型三个种子样本的80个人类批准的指令来生成的。通过使用BioInstruct数据集进行训练LLMs，我们 hoping to 优化LLMs在生物医学自然语言处理（BioNLP）中的表现。我们在LLaMA LLMs（1&2, 7B&13B）上进行了指令调整，并对其进行了BioNLP应用程序的评估，包括信息提取、问答和文本生成。我们还评估了指令对模型性能的贡献，使用多任务学习原理。
</details></li>
</ul>
<hr>
<h2 id="ExPT-Synthetic-Pretraining-for-Few-Shot-Experimental-Design"><a href="#ExPT-Synthetic-Pretraining-for-Few-Shot-Experimental-Design" class="headerlink" title="ExPT: Synthetic Pretraining for Few-Shot Experimental Design"></a>ExPT: Synthetic Pretraining for Few-Shot Experimental Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19961">http://arxiv.org/abs/2310.19961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tung Nguyen, Sudhanshu Agrawal, Aditya Grover</li>
<li>for: 本研究目的是解决实验设计中的样本效率问题，因为现实中的实验评估需要耗费时间、金钱和安全成本。</li>
<li>methods: 本文使用一种名为Experiment Pretrained Transformers（ExPT）的基本模型，这是一种基于环境学习的减少样本数据集合的方法。</li>
<li>results: 研究表明，ExPT在减少样本数据集合的情况下可以达到更高的性能和普适性，并且在各种复杂的实验设计任务中表现出色。<details>
<summary>Abstract</summary>
Experimental design is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of few-shot experimental design, where only a few labeled data points of input designs and their corresponding values are available. We approach this problem as a conditional generation task, where a model conditions on a few labeled examples and the desired output to generate an optimal input design. To this end, we introduce Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that employs a novel combination of synthetic pretraining with in-context learning. In ExPT, we only assume knowledge of a finite collection of unlabelled data points from the input domain and pretrain a transformer neural network to optimize diverse synthetic functions defined over this domain. Unsupervised pretraining allows ExPT to adapt to any design task at test time in an in-context fashion by conditioning on a few labeled data points from the target task and generating the candidate optima. We evaluate ExPT on few-shot experimental design in challenging domains and demonstrate its superior generality and performance compared to existing methods. The source code is available at https://github.com/tung-nd/ExPT.git.
</details>
<details>
<summary>摘要</summary>
实验设计是许多科学和工程领域的基本问题。在这个问题中，sample efficiency是非常重要，因为实验评估的时间、钱和安全成本都是非常高昂的。现有的方法都是靠活的数据收集或者有大量的过去实验标签数据来进行，这些方法在实际 scenarios 中是不实际的。在这个工作中，我们处理更加问题的设计问题，其中只有几个标签的输入设计和其对应的值是可用的。我们这个问题作为一个 conditional generation 任务，我们的模型将根据几个标签的例子和目标值来生成最佳的输入设计。为了实现这个目标，我们引入 Experiment Pretrained Transformers（ExPT），一个基于 transformer 神经网络的基础模型，它使用了一种新的组合方法，将 synthetic pretraining 与 in-context learning 相结合。在 ExPT 中，我们仅仅假设知道输入领域中的一个finite collection 的无标例子，并将 transformer 神经网络预训来优化这个领域上的多个无相关函数。无标预训 позволяет ExPT 在测试时以内容的方式适应任务，通过根据几个标签的目标值和目标值来获得候选的最佳设计。我们评估 ExPT 在几 shot 实验设计中的一般性和表现，并证明它在挑战性的领域中比现有的方法表现更好。请见 https://github.com/tung-nd/ExPT.git 的源代码。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Spatiotemporal-Big-Data-A-Vision-on-Opportunities-and-Challenges"><a href="#Deep-Learning-for-Spatiotemporal-Big-Data-A-Vision-on-Opportunities-and-Challenges" class="headerlink" title="Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and Challenges"></a>Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19957">http://arxiv.org/abs/2310.19957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Jiang</li>
<li>for: 本研究旨在探讨深度学习技术在 espacial 大数据领域中的应用，以及这些技术在处理不同类型的 espacial 大数据时的挑战和未来研究需求。</li>
<li>methods: 本研究使用了 Earth 图像大数据等多种 espacial 大数据，并应用了深度学习技术来解决各种陆地覆盖和陆地使用模型化任务。</li>
<li>results: 本研究描述了 espacial 大数据的特点和深度学习技术在这些数据上的应用，并提出了未来研究中需要解决的一些挑战。<details>
<summary>Abstract</summary>
With advancements in GPS, remote sensing, and computational simulation, an enormous volume of spatiotemporal data is being collected at an increasing speed from various application domains, spanning Earth sciences, agriculture, smart cities, and public safety. Such emerging geospatial and spatiotemporal big data, coupled with recent advances in deep learning technologies, foster new opportunities to solve problems that have not been possible before. For instance, remote sensing researchers can potentially train a foundation model using Earth imagery big data for numerous land cover and land use modeling tasks. Coastal modelers can train AI surrogates to speed up numerical simulations. However, the distinctive characteristics of spatiotemporal big data pose new challenges for deep learning technologies. This vision paper introduces various types of spatiotemporal big data, discusses new research opportunities in the realm of deep learning applied to spatiotemporal big data, lists the unique challenges, and identifies several future research needs.
</details>
<details>
<summary>摘要</summary>
With the advancements in GPS, remote sensing, and computational simulation, an enormous amount of spatiotemporal data is being collected at an increasing speed from various application domains, including Earth sciences, agriculture, smart cities, and public safety. This emerging geospatial and spatiotemporal big data, combined with recent advances in deep learning technologies, has opened up new opportunities to solve problems that were previously unsolvable. For example, remote sensing researchers can potentially train a foundation model using Earth imagery big data for numerous land cover and land use modeling tasks. Coastal modelers can train AI surrogates to speed up numerical simulations. However, the unique characteristics of spatiotemporal big data pose new challenges for deep learning technologies. This vision paper introduces various types of spatiotemporal big data, discusses new research opportunities in the realm of deep learning applied to spatiotemporal big data, lists the unique challenges, and identifies several future research needs.Here's the translation of the text in Traditional Chinese:随着GPS、远程感知和计算 simulated的进步，各个应用领域产生了巨量的空间时间数据，包括地球科学、农业、智能城市和公共安全。这些emerging geospatial和空间时间大数据，与最近的深度学习技术的进步，带来了新的机会，例如：将地球图像大数据用于多种土地覆盖和土地使用模型任务的对称基模型训练。海岸模型师可以将AI参数器训练为加速numerical simulations。然而， espaciotemporal big data的特有特征对深度学习技术提出了新的挑战。本 vision paper introduces various types of espaciotemporal big data, discusses new research opportunities in the realm of deep learning applied to espaciotemporal big data, lists the unique challenges, and identifies several future research needs.
</details></li>
</ul>
<hr>
<h2 id="Conditional-Unscented-Autoencoders-for-Trajectory-Prediction"><a href="#Conditional-Unscented-Autoencoders-for-Trajectory-Prediction" class="headerlink" title="Conditional Unscented Autoencoders for Trajectory Prediction"></a>Conditional Unscented Autoencoders for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19944">http://arxiv.org/abs/2310.19944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boschresearch/cuae-prediction">https://github.com/boschresearch/cuae-prediction</a></li>
<li>paper_authors: Faris Janjoš, Marcel Hallgarten, Anthony Knittel, Maxim Dolgov, Andreas Zell, J. Marius Zöllner</li>
<li>for: 这篇论文的目的是挑战 \ac{CVAE} 中的一些关键 комponent，并提出改进方案以提高预测性能。</li>
<li>methods: 本论文使用了最近的 VAE 技术，包括不可应用 sampling 和更Structured mixture latent space，以及一种新的、可能更表达ive的推论方法。</li>
<li>results: 试验结果显示，我们的模型在 INTERACTION 预测 dataset 上表现出色，超过了现有的州检验标准，并在 CelebA  dataset 上进行图像模型化任务上也超过了基本的 vanilla CVAE。<details>
<summary>Abstract</summary>
The \ac{CVAE} is one of the most widely-used models in trajectory prediction for \ac{AD}. It captures the interplay between a driving context and its ground-truth future into a probabilistic latent space and uses it to produce predictions. In this paper, we challenge key components of the CVAE. We leverage recent advances in the space of the VAE, the foundation of the CVAE, which show that a simple change in the sampling procedure can greatly benefit performance. We find that unscented sampling, which draws samples from any learned distribution in a deterministic manner, can naturally be better suited to trajectory prediction than potentially dangerous random sampling. We go further and offer additional improvements, including a more structured mixture latent space, as well as a novel, potentially more expressive way to do inference with CVAEs. We show wide applicability of our models by evaluating them on the INTERACTION prediction dataset, outperforming the state of the art, as well as at the task of image modeling on the CelebA dataset, outperforming the baseline vanilla CVAE. Code is available at https://github.com/boschresearch/cuae-prediction.
</details>
<details>
<summary>摘要</summary>
《CVAE》是自驾报道预测领域中最广泛使用的模型之一。它捕捉了驾驶Context和其真实未来的关系，并将其转化为一个 probabilistic 的latent space，以生成预测。在这篇论文中，我们挑战了CVAE的关键组件。我们利用了最近的 VAE 的进步，CVAE 的基础，发现一种简单的改变抽样方法可以大幅提高性能。我们发现， deterministic 抽样（unscented sampling）可以更好地适应 trajectory prediction than potentially dangerous random sampling。我们还提供了其他改进，包括一种更结构化的混合幂space，以及一种可能更具表达力的CVAE inference方法。我们通过评估 INTERACTION prediction 数据集和 CelebA 图像模型任务，证明了我们的模型在各个领域中的广泛适用性，并超越了状态前的最佳性能。代码可以在 https://github.com/boschresearch/cuae-prediction 中找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-Few-Annotation-Learning-for-Object-Detection-Are-Transformer-based-Models-More-Efficient"><a href="#Towards-Few-Annotation-Learning-for-Object-Detection-Are-Transformer-based-Models-More-Efficient" class="headerlink" title="Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?"></a>Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19936">http://arxiv.org/abs/2310.19936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cea-list/mt-detr">https://github.com/cea-list/mt-detr</a></li>
<li>paper_authors: Quentin Bouniot, Angélique Loesch, Romaric Audigier, Amaury Habrard</li>
<li>for: 这个研究是为了提出一种适用于现有最佳物体检测器 Deformable DETR 的几shot 和半supervised 学习设置的 semi-supervised 方法。</li>
<li>methods: 本研究使用了一个学生-教师架构，避免依赖学生模型中的敏感后处理 Pseudo-labels。</li>
<li>results: 在 COCO 和 Pascal VOC 的半supervised 物体检测 benchmark 上评估了我们的方法，与之前的方法比较，特别是当标签少时表现更好。我们认为我们的贡献开启了新的可能性，以适应类似的物体检测方法。<details>
<summary>Abstract</summary>
For specialized and dense downstream tasks such as object detection, labeling data requires expertise and can be very expensive, making few-shot and semi-supervised models much more attractive alternatives. While in the few-shot setup we observe that transformer-based object detectors perform better than convolution-based two-stage models for a similar amount of parameters, they are not as effective when used with recent approaches in the semi-supervised setting. In this paper, we propose a semi-supervised method tailored for the current state-of-the-art object detector Deformable DETR in the few-annotation learning setup using a student-teacher architecture, which avoids relying on a sensitive post-processing of the pseudo-labels generated by the teacher model. We evaluate our method on the semi-supervised object detection benchmarks COCO and Pascal VOC, and it outperforms previous methods, especially when annotations are scarce. We believe that our contributions open new possibilities to adapt similar object detection methods in this setup as well.
</details>
<details>
<summary>摘要</summary>
特别是在特殊和紧张的下游任务，如物体检测，标注数据需要专家知识和成本很高，使得几步和半supervised模型变得非常吸引人。在几步设置下，我们发现基于转换器的物体检测器比基于 convolution的两stage模型在同等参数量下表现更好。然而，在 semi-supervised 设置下，它们不那么有效。在这篇论文中，我们提出了一种针对当前领先的物体检测器Deformable DETR在几步学习设置中使用学生-教师架构的半supervised方法。我们避免了依赖于敏感的后处理 pseudo-labels 生成于教师模型。我们在 COCO 和 Pascal VOC  semi-supervised 物体检测数据集上评估了我们的方法，并与之前的方法相比，它在标注稀缺的情况下表现更好。我们认为，我们的贡献打开了新的可能性，使得类似的物体检测方法在这种设置中也可以适应。
</details></li>
</ul>
<hr>
<h2 id="Model-Based-Reparameterization-Policy-Gradient-Methods-Theory-and-Practical-Algorithms"><a href="#Model-Based-Reparameterization-Policy-Gradient-Methods-Theory-and-Practical-Algorithms" class="headerlink" title="Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms"></a>Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19927">http://arxiv.org/abs/2310.19927</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/agentification/rp_pgm">https://github.com/agentification/rp_pgm</a></li>
<li>paper_authors: Shenao Zhang, Boyi Liu, Zhaoran Wang, Tuo Zhao</li>
<li>for: 该 paper 是为了研究 model-based ReParameterization Policy Gradient Methods (RP PGMs) 在长期 reinforcement learning 问题中的应用和优化问题。</li>
<li>methods: 该 paper 使用了 theoretical 和 experimental 方法来研究 RP PGMs 的整体性和优化问题，并提出了spectral normalization 方法来缓解长模型拓展导致的潜在梯度变iance问题。</li>
<li>results: 实验结果表明，采用 spectral normalization 方法可以有效缓解梯度变iance问题，并且提高了 RP PGMs 的性能，与其他梯度估计器（如 likelihood Ratio 梯度估计器）相当或更高。<details>
<summary>Abstract</summary>
ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding variance issue caused by long model unrolls. Our experimental results demonstrate that proper normalization significantly reduces the gradient variance of model-based RP PGMs. As a result, the performance of the proposed method is comparable or superior to other gradient estimators, such as the Likelihood Ratio (LR) gradient estimator. Our code is available at https://github.com/agentification/RP_PGM.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable("ReParameterization（RP）Policy Gradient Methods（PGMs）have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding variance issue caused by long model unrolls. Our experimental results demonstrate that proper normalization significantly reduces the gradient variance of model-based RP PGMs. As a result, the performance of the proposed method is comparable or superior to other gradient estimators, such as the Likelihood Ratio（LR）gradient estimator. Our code is available at https://github.com/agentification/RP_PGM.）Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Jina-Embeddings-2-8192-Token-General-Purpose-Text-Embeddings-for-Long-Documents"><a href="#Jina-Embeddings-2-8192-Token-General-Purpose-Text-Embeddings-for-Long-Documents" class="headerlink" title="Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents"></a>Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19923">http://arxiv.org/abs/2310.19923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao</li>
<li>for: The paper is written for researchers and practitioners working on text embedding models, particularly those interested in developing models that can handle long documents.</li>
<li>methods: The paper introduces Jina Embeddings 2, an open-source text embedding model that can accommodate up to 8192 tokens, which is much longer than the conventional 512-token limit. The model uses a novel combination of techniques to achieve state-of-the-art performance on a range of embedding-related tasks.</li>
<li>results: The paper reports that Jina Embeddings 2 achieves performance on par with OpenAI’s proprietary ada-002 model on the MTEB benchmark, and that an extended context can enhance performance in tasks such as NarrativeQA.<details>
<summary>Abstract</summary>
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.   To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only achieves state-of-the-art performance on a range of embedding-related tasks in the MTEB benchmark but also matches the performance of OpenAI's proprietary ada-002 model. Additionally, our experiments indicate that an extended context can enhance performance in tasks such as NarrativeQA.
</details>
<details>
<summary>摘要</summary>
文本嵌入模型已经成为强大工具，可以将句子转换成固定大小的特征向量，捕捉 semantic 信息。而这些模型在信息检索、semantic 聚合和文本重新排序等任务中是必备的，但现有的大多数开源模型，特别是基于 BERT 的模型，在处理长文档时很难表现，通常会导致 truncation。为了解决这个挑战，我们介绍 Jina Embeddings 2，一个可以处理 Up to 8192 个字的开源文本嵌入模型。这个模型不仅在 MTEB 竞赛中表现出优于 Convention 512 个字的限制，还能够高效地处理长文档。 Jina Embeddings 2 不仅达到了一系列嵌入相关任务的状态 искусственный智能表现，还与 OpenAI 的专有 ada-002 模型匹配。此外，我们的实验表明，扩展上下文可以提高 NarrativeQA 等任务的表现。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Bias-and-Inequities-A-Systematic-Review-of-Bias-Detection-and-Mitigation-in-Healthcare-Artificial-Intelligence-Using-Electronic-Health-Records"><a href="#Unmasking-Bias-and-Inequities-A-Systematic-Review-of-Bias-Detection-and-Mitigation-in-Healthcare-Artificial-Intelligence-Using-Electronic-Health-Records" class="headerlink" title="Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records"></a>Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19917">http://arxiv.org/abs/2310.19917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Chen, Liqin Wang, Julie Hong, Jiaqi Jiang, Li Zhou</li>
<li>for: 这项研究的目的是系统性地查询利用电子医疗记录（EHR）数据的人工智能（AI）应用中的偏见问题。</li>
<li>methods: 这项研究采用了遵循PRISMA指南的系统性回顾方法，从PubMed、Web of Science和IEEE检索到252篇文章，并对其中的20篇文章进行了最终审查。</li>
<li>results: 这项研究发现，在20篇文章中，5种主要的偏见问题得到了覆盖，即8篇文章分析了选择偏见问题，6篇文章分析了隐式偏见问题，5篇文章分析了干扰偏见问题，4篇文章分析了计量偏见问题，2篇文章分析了算法偏见问题。在偏见处理方法方面，10篇文章在模型开发阶段发现了偏见问题，而17篇文章提出了避免偏见问题的方法。<details>
<summary>Abstract</summary>
Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten studies identified bias during model development, while seventeen presented methods to mitigate the bias. Discussion: Bias may infiltrate the AI application development process at various stages. Although this review discusses methods for addressing bias at different development stages, there is room for implementing additional effective approaches. Conclusion: Despite growing attention to bias in healthcare AI, research using EHR data on this topic is still limited. Detecting and mitigating AI bias with EHR data continues to pose challenges. Further research is needed to raise a standardized method that is generalizable and interpretable to detect, mitigate and evaluate bias in medical AI.
</details>
<details>
<summary>摘要</summary>
目的：人工智能（AI）应用使用电子健康纪录（EHR）得到了广泛的应用，但也会产生不同类型的偏见。本研究目的是系统性地对EHR数据使用AI研究中的偏见进行评估。方法：按照Preferred Reporting Items for Systematic Reviews and Meta-analyses（PRISMA）指南进行系统性综述。我们从2010年1月1日到2022年10月31日 retrievePubMed、Web of Science和Institute of Electrical and Electronics Engineers上的文章。我们定义了六种主要的偏见类型，并summarized现有的偏见处理方法。结果：从252篇文章中，20篇符合包含期刊的要求，进行了最终审查。八种偏见中，八种是选择偏见；六种是隐式偏见；五种是混合偏见；四种是测量偏见；两种是算法偏见。对偏见处理方法，十篇文章在模型开发阶段检测了偏见，而十七篇文章提出了避免偏见的方法。讨论：偏见可能在AI应用开发过程中各个阶段偏见。虽然这篇文章讨论了在不同阶段检测和避免偏见的方法，但还需要实施更多有效的方法。结论：尽管健康AI中的偏见问题已经得到了越来越多的关注，但使用EHR数据进行的研究还是有限的。检测和避免EHR数据上的AI偏见还需要继续进行更多的研究。为了提高AI医疗应用的标准化方法，需要采用一种通用、可读性高的方法来检测、避免和评估偏见。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Prototype-based-Graph-Information-Bottleneck"><a href="#Interpretable-Prototype-based-Graph-Information-Bottleneck" class="headerlink" title="Interpretable Prototype-based Graph Information Bottleneck"></a>Interpretable Prototype-based Graph Information Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19906">http://arxiv.org/abs/2310.19906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sang-woo-seo/pgib">https://github.com/sang-woo-seo/pgib</a></li>
<li>paper_authors: Sangwoo Seo, Sungwon Kim, Chanyoung Park</li>
<li>for: 这个论文的目的是提出一种可解释的图 нейрон网络（PGIB）框架，用于提高图 нейрон网络的解释性和性能。</li>
<li>methods: 这个论文使用了prototype学习和信息瓶颈框架，从输入图中提取关键子图，并通过这些关键子图来提供可解释的预测结果。</li>
<li>results: 对比其他状态速法，PGIB在预测性能和解释性两个方面均表现出色，并且通过质量分析得到了证明。<details>
<summary>Abstract</summary>
The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph that is important for the model prediction. This is the first work that incorporates prototype learning into the process of identifying the key subgraphs that have a critical impact on the prediction performance. Extensive experiments, including qualitative analysis, demonstrate that PGIB outperforms state-of-the-art methods in terms of both prediction performance and explainability.
</details>
<details>
<summary>摘要</summary>
graph TD成功的图 neuronal networks (GNNs) 带来了理解它们的决策过程和提供对其预测的解释，这 hath led to explainable AI (XAI) 提供了透明的解释 для黑色 Box 模型。 在最近，使用 prototype 已成功地提高了模型的解释性，通过学习 prototype 来Imply training graphs that affect the prediction。然而，这些approaches 往往提供 prototype 中过度的信息，从整个图中获取信息，导致遗漏关键子结构或包含无关信息，这可能会限制模型在下游任务中的解释性和性能。在这项工作中，我们提出了一种新的解释 GNN 框架，called interpretable Prototype-based Graph Information Bottleneck (PGIB)。PGIB 在信息瓶颈框架中 incorporates prototype learning 来提供 key subgraph 从输入图中对模型预测的重要性。这是首次 incorporates prototype learning 到 identify 预测性能的关键子图的过程中。extensive experiments, including qualitative analysis, show that PGIB outperforms state-of-the-art methods in terms of both prediction performance and explainability.
</details></li>
</ul>
<hr>
<h2 id="Herd-Using-multiple-smaller-LLMs-to-match-the-performances-of-proprietary-large-LLMs-via-an-intelligent-composer"><a href="#Herd-Using-multiple-smaller-LLMs-to-match-the-performances-of-proprietary-large-LLMs-via-an-intelligent-composer" class="headerlink" title="Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer"></a>Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19902">http://arxiv.org/abs/2310.19902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surya Narayanan Hari, Matt Thomson</li>
<li>for: 这篇论文的目的是解决现有的LLM模型在实际应用中的访问和扩展问题，以及对于这些模型的性能评估。</li>
<li>methods: 这篇论文使用了开源模型库和智能路由器来组织和选择合适的LLM模型，以提高其性能和可靠性。</li>
<li>results: 论文表明，一个由多种开源模型组成的“牧场”可以与商业模型匹配或超越其性能，而且这些模型的大小比商业模型要小得多。此外，当GPT无法回答查询时，“牧场”可以识别一个能够回答查询的模型，超过40%的时间。<details>
<summary>Abstract</summary>
Currently, over a thousand LLMs exist that are multi-purpose and are capable of performing real world tasks, including Q&A, text summarization, content generation, etc. However, accessibility, scale and reliability of free models prevents them from being widely deployed in everyday use cases. To address the first two issues of access and scale, organisations such as HuggingFace have created model repositories where users have uploaded model weights and quantized versions of models trained using different paradigms, as well as model cards describing their training process. While some models report performance on commonly used benchmarks, not all do, and interpreting the real world impact of trading off performance on a benchmark for model deployment cost, is unclear. Here, we show that a herd of open source models can match or exceed the performance of proprietary models via an intelligent router. We show that a Herd of open source models is able to match the accuracy of ChatGPT, despite being composed of models that are effectively 2.5x smaller. We show that in cases where GPT is not able to answer the query, Herd is able to identify a model that can, at least 40% of the time.
</details>
<details>
<summary>摘要</summary>
Here, we show that a herd of open-source models can match or exceed the performance of proprietary models via an intelligent router. Specifically, we show that a herd of open-source models is able to match the accuracy of ChatGPT, despite being composed of models that are effectively 2.5 times smaller. Additionally, we show that in cases where GPT is not able to answer a query, the herd is able to identify a model that can, at least 40% of the time.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Geometry-of-Blind-Spots-in-Vision-Models"><a href="#Exploring-Geometry-of-Blind-Spots-in-Vision-Models" class="headerlink" title="Exploring Geometry of Blind Spots in Vision Models"></a>Exploring Geometry of Blind Spots in Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19889">http://arxiv.org/abs/2310.19889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SriramB-98/blindspots-geometry">https://github.com/SriramB-98/blindspots-geometry</a></li>
<li>paper_authors: Sriram Balasubramanian, Gaurang Sriramanan, Vinu Sankar Sadasivan, Soheil Feizi</li>
<li>for: 这种研究旨在探讨深度神经网络在视觉任务中的不敏感性问题，即大 magnitude 的输入变换不会导致网络活动变化。</li>
<li>methods: 该研究使用了Level Set Traversal算法，通过探索输入空间中的高确idence区域，以找到与源图像相似但属于其他类别的输入图像。</li>
<li>results: 研究发现，深度神经网络的等Confidence水平集在输入空间中存在星型结构，而且可以使用高确idence路径连接这些等Confidence水平集。此外，研究还评估了这些连接的高维空间范围。code可以在<a target="_blank" rel="noopener" href="https://github.com/SriramB-98/blindspots-neurips-sub%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/SriramB-98/blindspots-neurips-sub上获取。</a><details>
<summary>Abstract</summary>
Despite the remarkable success of deep neural networks in a myriad of settings, several works have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as adversarial attacks. On the other hand, prior works have also observed that deep networks can be under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable changes to network activations. In this work, we study in detail the phenomenon of under-sensitivity in vision models such as CNNs and Transformers, and present techniques to study the geometry and extent of "equi-confidence" level sets of such networks. We propose a Level Set Traversal algorithm that iteratively explores regions of high confidence with respect to the input space using orthogonal components of the local gradients. Given a source image, we use this algorithm to identify inputs that lie in the same equi-confidence level set as the source image despite being perceptually similar to arbitrary images from other classes. We further observe that the source image is linearly connected by a high-confidence path to these inputs, uncovering a star-like structure for level sets of deep networks. Furthermore, we attempt to identify and estimate the extent of these connected higher-dimensional regions over which the model maintains a high degree of confidence. The code for this project is publicly available at https://github.com/SriramB-98/blindspots-neurips-sub
</details>
<details>
<summary>摘要</summary>
尽管深度神经网络在各种设置中表现出了惊人的成功，但是一些研究表明，这些神经网络对于微不足的干扰（ adversarial attacks）具有极高的敏感性。然而，也有一些研究发现，深度网络可能具有不够敏感的问题，即在输入空间中的大规模干扰不会导致神经网络的活动变化。在这种情况下，我们对视力模型，如CNNs和Transformers，进行了详细的研究，并提出了一些技术来研究这些神经网络的几何结构和扩展。我们提出了一种Level Set Traversal算法，该算法可以在输入空间中循环探索高信任级别的区域，并使用本地梯度的正交分量来探索这些区域。给定一个源图像，我们使用这种算法来找到与源图像在输入空间中的同一个等信任水平集的输入图像，并观察到这些输入图像与源图像之间存在一条直线连接，揭示了深度网络的等信任水平集具有星型结构。此外，我们尝试了为这些相关的更高维度区域的扩展，以便更好地了解深度网络在它们中的行为。相关代码可以在https://github.com/SriramB-98/blindspots-neurips-sub上获取。
</details></li>
</ul>
<hr>
<h2 id="DEFT-Dexterous-Fine-Tuning-for-Real-World-Hand-Policies"><a href="#DEFT-Dexterous-Fine-Tuning-for-Real-World-Hand-Policies" class="headerlink" title="DEFT: Dexterous Fine-Tuning for Real-World Hand Policies"></a>DEFT: Dexterous Fine-Tuning for Real-World Hand Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19797">http://arxiv.org/abs/2310.19797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adityak77/deft-data">https://github.com/adityak77/deft-data</a></li>
<li>paper_authors: Aditya Kannan, Kenneth Shaw, Shikhar Bahl, Pragna Mannam, Deepak Pathak</li>
<li>for: 本研究旨在探讨人类手部 manipulate 软、可变形物体以及复杂、长期任务中的挑战，以提高机器人 manipulate 的能力。</li>
<li>methods: 本研究提出了一种新的方法，即 DEFT（dexterous fine-tuning for hand policies），它利用人类驱动的假设，通过在实际世界中直接执行来改进这些假设。该方法还包括一种高效的在线优化过程。</li>
<li>results: DEFT 在多个任务中显示出成功，以及一种数据效率的普适的软件 manipulate 路径，用于掌握复杂的 manipulate 任务。您可以通过访问我们的网站 <a target="_blank" rel="noopener" href="https://dexterous-finetuning.github.io/">https://dexterous-finetuning.github.io</a> 查看视频结果。<details>
<summary>Abstract</summary>
Dexterity is often seen as a cornerstone of complex manipulation. Humans are able to perform a host of skills with their hands, from making food to operating tools. In this paper, we investigate these challenges, especially in the case of soft, deformable objects as well as complex, relatively long-horizon tasks. However, learning such behaviors from scratch can be data inefficient. To circumvent this, we propose a novel approach, DEFT (DExterous Fine-Tuning for Hand Policies), that leverages human-driven priors, which are executed directly in the real world. In order to improve upon these priors, DEFT involves an efficient online optimization procedure. With the integration of human-based learning and online fine-tuning, coupled with a soft robotic hand, DEFT demonstrates success across various tasks, establishing a robust, data-efficient pathway toward general dexterous manipulation. Please see our website at https://dexterous-finetuning.github.io for video results.
</details>
<details>
<summary>摘要</summary>
dexterity 常被看作复杂的操作的基石。人们可以通过手部执行许多技能，从制备食物到操作工具。在这篇论文中，我们调查这些挑战，尤其是在软、可变形物体以及复杂、较长时间任务上。然而，从头来学习这些行为可以是数据不fficient。为了缓解这个问题，我们提出了一种新的方法，即 DEFT（手部精细调整 для手指策略），它利用人类驱动的先验知识，直接在实际世界中执行。为了提高这些先验知识，DEFT包括一种高效的在线优化过程。通过人类学习和在线细化，以及软体机械手部，DEFT在多种任务中成功，建立了一条可靠、数据fficient的通路 toward 普遍的手部精细操作。请参考我们网站 <https://dexterous-finetuning.github.io> 查看视频结果。
</details></li>
</ul>
<hr>
<h2 id="Re-evaluating-Retrosynthesis-Algorithms-with-Syntheseus"><a href="#Re-evaluating-Retrosynthesis-Algorithms-with-Syntheseus" class="headerlink" title="Re-evaluating Retrosynthesis Algorithms with Syntheseus"></a>Re-evaluating Retrosynthesis Algorithms with Syntheseus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19796">http://arxiv.org/abs/2310.19796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krzysztof Maziarz, Austin Tripp, Guoqing Liu, Megan Stanley, Shufang Xie, Piotr Gaiński, Philipp Seidl, Marwin Segler</li>
<li>for: 本研究主要目标是提高化学synthesis的计划和评估方法。</li>
<li>methods: 本研究使用了一个名为syntheseus的 benchmarking 库，该库鼓励了best practice的使用，以便对单步和多步 retrosynthesis 算法进行一致的评估。</li>
<li>results: 通过使用syntheseus库进行重新评估，发现了一些之前的 retrosynthesis 算法的排名发生了变化。<details>
<summary>Abstract</summary>
The planning of how to synthesize molecules, also known as retrosynthesis, has been a growing focus of the machine learning and chemistry communities in recent years. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques. To remedy this, we present a benchmarking library called syntheseus which promotes best practice by default, enabling consistent meaningful evaluation of single-step and multi-step retrosynthesis algorithms. We use syntheseus to re-evaluate a number of previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes when evaluated carefully. We end with guidance for future works in this area.
</details>
<details>
<summary>摘要</summary>
“retrosynthesis的规划”在过去几年内，机器学习和化学社区的关注越来越高。尽管表面上看来有稳定的进步，我们认为现有的评价标准和比较方法存在系统性的缺陷。为了解决这个问题，我们提出了一个名为 syntheseus的评价库，它默认采用了最佳实践，使得单步和多步retrosynthesis算法的meaningful评价成为可能。我们使用 syntheseus 重新评估了一些先前的retrosynthesis算法，并发现当仔细评估时，现状的模型的排名发生变化。最后，我们提出了未来这个领域的指导方针。
</details></li>
</ul>
<hr>
<h2 id="Res-Tuning-A-Flexible-and-Efficient-Tuning-Paradigm-via-Unbinding-Tuner-from-Backbone"><a href="#Res-Tuning-A-Flexible-and-Efficient-Tuning-Paradigm-via-Unbinding-Tuner-from-Backbone" class="headerlink" title="Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone"></a>Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19859">http://arxiv.org/abs/2310.19859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang Lv, Yujun Shen, Deli Zhao, Jingren Zhou</li>
<li>for: 本研究旨在提出一种新的参数高效调整方法，以便将大规模基模型传递到下游应用中。</li>
<li>methods: 该方法基于不同的调整策略，通过意图解耦调整器与基模型的关系，使得调整器的设计和学习不再依赖基模型。</li>
<li>results: 对于权重调整和泛化调整等多种调整策略，该方法能够提供更高效的参数调整，并且可以轻松地搭配多种调整策略。经验表明，该方法在推理和生成任务上具有较高的效果和效率。<details>
<summary>Abstract</summary>
Parameter-efficient tuning has become a trend in transferring large-scale foundation models to downstream applications. Existing methods typically embed some light-weight tuners into the backbone, where both the design and the learning of the tuners are highly dependent on the base model. This work offers a new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners from the backbone. With both theoretical and empirical evidence, we show that popular tuning approaches have their equivalent counterparts under our unbinding formulation, and hence can be integrated into our framework effortlessly. Thanks to the structural disentanglement, we manage to free the design of tuners from the network architecture, facilitating flexible combination of various tuning strategies. We further propose a memory-efficient variant of Res-Tuning, where the bypass i.e., formed by a sequence of tuners) is effectively detached from the main branch, such that the gradients are back-propagated only to the tuners but not to the backbone. Such a detachment also allows one-time backbone forward for multi-task inference. Extensive experiments on both discriminative and generative tasks demonstrate the superiority of our method over existing alternatives from the perspectives of efficacy and efficiency. Project page: $\href{https://res-tuning.github.io/}{\textit{https://res-tuning.github.io/}$.
</details>
<details>
<summary>摘要</summary>
大规模基础模型转移到下游应用的Parameter-efficient tuning已成为当前趋势。现有方法通常将轻量级调参器 embedding到后向，其设计和学习均高度依赖于基模型。这项工作提出了一新调参方式，名为Res-Tuning，它意图将调参器解除与后向的绑定。我们通过理论和实验证明，流行的调参方法均有其对应的等价对手在我们的解绑形式下，因此可以轻松地 интеGRATE到我们的框架中。由于结构分离，我们可以在调参器的设计上免除网络架构的限制，实现灵活的调参策略组合。此外，我们还提出了内存效率改进的Res-Tuning变体，其中分支（formed by a sequence of tuners）被有效地分离于主支，使得梯度只回传给调参器而不回传到后向。这种分离还允许一次主支前进 для多任务推理。广泛的实验表明，我们的方法在效果和效率两个角度上都超越了现有的方法。项目页面： $\href{https://res-tuning.github.io/}{\textit{https://res-tuning.github.io/}$。
</details></li>
</ul>
<hr>
<h2 id="SimMMDG-A-Simple-and-Effective-Framework-for-Multi-modal-Domain-Generalization"><a href="#SimMMDG-A-Simple-and-Effective-Framework-for-Multi-modal-Domain-Generalization" class="headerlink" title="SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization"></a>SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19795">http://arxiv.org/abs/2310.19795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/donghao51/simmmdg">https://github.com/donghao51/simmmdg</a></li>
<li>paper_authors: Hao Dong, Ismail Nejjar, Han Sun, Eleni Chatzi, Olga Fink<br>for: 这个研究旨在解决多modal Distribution Generalization (DG) 中的挑战，当模型需要在不同的modalities上缩减到未知的target Distribution。methods: 我们提出了一个简单 yet effective的多modal DG框架，叫做SimMMDG。我们认为将不同modalities的特征映射到同一个嵌入空间会降低模型的通用性。因此，我们提出了在每个modalities中分解特征为modalitiespecific和modalitieshared部分。我们运用了监督式对应学习 modalitieshared特征，以保持它们具有共同性，并对modalitiespecific特征强制距离。此外，我们引入了跨modalities翻译模块，以调整学习的特征。results: 我们的框架理论上得到支持，并在EPIC-Kitchens dataset和我们在本文中介绍的新的Human-Animal-Cartoon (HAC) dataset上实现了强大的多modal DG性能。我们的原始代码和HAC dataset可以在<a target="_blank" rel="noopener" href="https://github.com/donghao51/SimMMDG%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/donghao51/SimMMDG上获得。</a><details>
<summary>Abstract</summary>
In real-world scenarios, achieving domain generalization (DG) presents significant challenges as models are required to generalize to unknown target distributions. Generalizing to unseen multi-modal distributions poses even greater difficulties due to the distinct properties exhibited by different modalities. To overcome the challenges of achieving domain generalization in multi-modal scenarios, we propose SimMMDG, a simple yet effective multi-modal DG framework. We argue that mapping features from different modalities into the same embedding space impedes model generalization. To address this, we propose splitting the features within each modality into modality-specific and modality-shared components. We employ supervised contrastive learning on the modality-shared features to ensure they possess joint properties and impose distance constraints on modality-specific features to promote diversity. In addition, we introduce a cross-modal translation module to regularize the learned features, which can also be used for missing-modality generalization. We demonstrate that our framework is theoretically well-supported and achieves strong performance in multi-modal DG on the EPIC-Kitchens dataset and the novel Human-Animal-Cartoon (HAC) dataset introduced in this paper. Our source code and HAC dataset are available at https://github.com/donghao51/SimMMDG.
</details>
<details>
<summary>摘要</summary>
在实际应用场景中，实现领域泛化（DG）存在重大挑战，因为模型需要泛化到未知目标分布。在多modal的场景中，泛化到未见多Modal的分布呈现更大的挑战，因为不同modalities具有不同的特性。为了解决多modal的领域泛化问题，我们提出了SimMMDG框架，这是一种简单 yet有效的多Modal DG框架。我们认为将不同modalities的特征映射到同一个嵌入空间内会阻碍模型泛化。为此，我们提议将每个modalities的特征分为具有共同特性的特征和具有特定特性的特征。我们使用supervised contrastive学习来确保共同特性，并对特定特性进行距离约束，以促进多Modal特征的多样性。此外，我们还引入了跨Modal翻译模块，以规范学习的特征。我们的框架理论上有良好支持，并在EPIC-Kitchens数据集和我们在本文中介绍的新的人类动物卡通（HAC）数据集上实现了强大的性能。我们的源代码和HAC数据集可以在https://github.com/donghao51/SimMMDG上获取。
</details></li>
</ul>
<hr>
<h2 id="LILO-Learning-Interpretable-Libraries-by-Compressing-and-Documenting-Code"><a href="#LILO-Learning-Interpretable-Libraries-by-Compressing-and-Documenting-Code" class="headerlink" title="LILO: Learning Interpretable Libraries by Compressing and Documenting Code"></a>LILO: Learning Interpretable Libraries by Compressing and Documenting Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19791">http://arxiv.org/abs/2310.19791</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gabegrand/lilo">https://github.com/gabegrand/lilo</a></li>
<li>paper_authors: Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, Jacob Andreas</li>
<li>for: 本研究旨在开发一个基于神经符号学术的代码生成框架，以帮助开发人员快速生成可读可写的代码库。</li>
<li>methods: 本研究使用了大语言模型（LLM）引导的程序生成技术，以及Stitch符号压缩系统来高效地压缩代码。此外，研究还引入了自动文档（AutoDoc）程序，以帮助理解和应用学习抽象。</li>
<li>results: 对三个 inductive 程序生成benchmark进行了评测，并与现有的神经和符号方法进行了比较。研究发现，LILO可以解决更复杂的任务，并学习更加深入的语言知识。<details>
<summary>Abstract</summary>
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synthesis benchmarks for string editing, scene reasoning, and graphics composition. Compared to existing neural and symbolic methods - including the state-of-the-art library learning algorithm DreamCoder - LILO solves more complex tasks and learns richer libraries that are grounded in linguistic knowledge.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）现在在代码生成方面表现出色，但是软件开发中一个关键方面是 refactoring：将代码集成到可重用和易读的库中。在这篇论文中，我们介绍了 LILO，一个神经符号学框架，可以逐步生成、压缩和文档代码，以建立适应特定问题领域的库。LILO将神经网络引导的程序生成与Stitch的符号压缩系统相结合，以实现高效的lambda抽象。为了让这些抽象更易理解，我们引入了自动文档（AutoDoc）过程，可以根据Contextual例子来生成自然语言名称和docstrings。除了提高人类可读性外，我们发现AutoDoc会提高LILO的生成器使用学习的性能。我们对LILO进行了三个induced程序生成benchmark测试，包括字符串编辑、Scene reasoning和图形组合。相比 existed神经和符号方法，包括DreamCoder库学习算法，LILO可以解决更复杂的任务，并学习更加深入的语言知识。
</details></li>
</ul>
<hr>
<h2 id="From-External-to-Swap-Regret-2-0-An-Efficient-Reduction-and-Oblivious-Adversary-for-Large-Action-Spaces"><a href="#From-External-to-Swap-Regret-2-0-An-Efficient-Reduction-and-Oblivious-Adversary-for-Large-Action-Spaces" class="headerlink" title="From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces"></a>From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19786">http://arxiv.org/abs/2310.19786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson, Noah Golowich</li>
<li>for: 这个论文目的是为了提出一种新的减少方法，即将换 regret 转换为外部 regret，以优化 classical  reductions 中的一些不稳定性。</li>
<li>methods: 这个论文使用了一种新的减少方法，即将换 regret 转换为外部 regret，并通过对这种减少方法的分析，得出了一些新的结论。</li>
<li>results: 这个论文的结果表明，当存在一个无外部 regret 算法时，必然也存在一个无换 regret 算法，并且这种无换 regret 算法的性能比 classical  reductions 更好。此外，这个论文还提供了一个新的下界，其表明在某些游戏中，换 regret 的数量必然是 $\tilde\Omega(N&#x2F;\epsilon^2)$ 或者是 exponential in $1&#x2F;\epsilon$。<details>
<summary>Abstract</summary>
We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can employ distributions over experts, showing that the number of rounds must be $\tilde\Omega(N/\epsilon^2)$ or exponential in $1/\epsilon$.   Our reduction implies that, if no-regret learning is possible in some game, then this game must have approximate correlated equilibria, of arbitrarily good approximation. This strengthens the folklore implication of no-regret learning that approximate coarse correlated equilibria exist. Importantly, it provides a sufficient condition for the existence of correlated equilibrium which vastly extends the requirement that the action set is finite, thus answering a question left open by [DG22; Ass+23]. Moreover, it answers several outstanding questions about equilibrium computation and/or learning in games.
</details>
<details>
<summary>摘要</summary>
我们提供了一种新的减少方法，将交换 regret 转化为外部 regret，从而超越了布姆-曼索尔（BM07）和斯托尔-卢戈西（SL05）的经典减少方法，因为它不需要动作空间的Finite。我们证明了，当存在一个无外部 regret 算法时，也一定存在一个无交换 regret 算法。在学习专家建议中，我们的结果表明，可以保证在 $\log(N)^{O(1/\epsilon)}$ 轮后，交换 regret 不超过 $\epsilon$，并且每轮复杂度为 $O(N)$，where $N$ 是专家数量。而布姆-曼索尔和斯托尔-卢戈西的经典减少方法需要 $O(N/\epsilon^2)$ 轮和至少 $\Omega(N^2)$ 每轮复杂度。我们的结果还有一个相关的下界，这下界在对快速反应者和 $\ell_1$ 约束学习者来说，并且可以使用分布来选择专家，显示了轮数必须是 $\tilde\Omega(N/\epsilon^2)$ 或者 exponential in $1/\epsilon$。我们的减少方法表明，如果有一个不 regret 学习是可能的游戏，那么这个游戏一定有approximate correlated equilibria，并且这些 equilibria 的准确程度可以是arbitrarily good。这个结论超越了布姆-曼索尔的结论，因为它不需要动作空间的Finite。此外，我们的结论还回答了一些关于平衡计算和学习的问题。
</details></li>
</ul>
<hr>
<h2 id="CustomNet-Zero-shot-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models"><a href="#CustomNet-Zero-shot-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models" class="headerlink" title="CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models"></a>CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19784">http://arxiv.org/abs/2310.19784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyang Yuan, Mingdeng Cao, Xintao Wang, Zhongang Qi, Chun Yuan, Ying Shan</li>
<li>for: 文章目的是提出一种基于 CustomNet 的对象自定义方法，以实现在文本到图像生成中实现对象的自定义。</li>
<li>methods: 该方法使用了三个重要的技术：1) 3D 新视角合成；2) 对象自定义；3) 文本描述或特定用户定义的图像来控制位置和背景。</li>
<li>results: 该方法可以在不需要测试时间优化的情况下，实现零 instances 的对象自定义，同时具有较好的个体保持和多样性。<details>
<summary>Abstract</summary>
Incorporating a customized object into image generation presents an attractive feature in text-to-image generation. However, existing optimization-based and encoder-based methods are hindered by drawbacks such as time-consuming optimization, insufficient identity preservation, and a prevalent copy-pasting effect. To overcome these limitations, we introduce CustomNet, a novel object customization approach that explicitly incorporates 3D novel view synthesis capabilities into the object customization process. This integration facilitates the adjustment of spatial position relationships and viewpoints, yielding diverse outputs while effectively preserving object identity. Moreover, we introduce delicate designs to enable location control and flexible background control through textual descriptions or specific user-defined images, overcoming the limitations of existing 3D novel view synthesis methods. We further leverage a dataset construction pipeline that can better handle real-world objects and complex backgrounds. Equipped with these designs, our method facilitates zero-shot object customization without test-time optimization, offering simultaneous control over the viewpoints, location, and background. As a result, our CustomNet ensures enhanced identity preservation and generates diverse, harmonious outputs.
</details>
<details>
<summary>摘要</summary>
通过包含自定义对象在图像生成中，文本到图像生成具有吸引人的特点。然而，现有的优化方法和编码器方法受到了一些缺点，如时间消耗优化、保持对象标识不足和广泛的复制效应。为了解决这些局限性，我们介绍了CustomNet，一种新的对象自定义方法，其中Explicitly incorporates 3D新视野合成能力到对象自定义过程中。这种整合使得可以调整空间位置关系和视点，从而生成多样的输出，同时有效地保持对象标识。此外，我们还引入了细腻的设计，使得通过文本描述或特定用户定义的图像来控制位置和背景，超越现有的3D新视野合成方法的限制。我们还利用了更好的数据构建管道，可以更好地处理真实世界中的对象和复杂背景。准备这些设计，我们的方法可以在零时优化下实现无需测试时优化的自定义对象，同时控制视点、位置和背景。因此，我们的CustomNet可以保持对象标识并生成多样、和谐的输出。
</details></li>
</ul>
<hr>
<h2 id="Designing-AI-Support-for-Human-Involvement-in-AI-assisted-Decision-Making-A-Taxonomy-of-Human-AI-Interactions-from-a-Systematic-Review"><a href="#Designing-AI-Support-for-Human-Involvement-in-AI-assisted-Decision-Making-A-Taxonomy-of-Human-AI-Interactions-from-a-Systematic-Review" class="headerlink" title="Designing AI Support for Human Involvement in AI-assisted Decision Making: A Taxonomy of Human-AI Interactions from a Systematic Review"></a>Designing AI Support for Human Involvement in AI-assisted Decision Making: A Taxonomy of Human-AI Interactions from a Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19778">http://arxiv.org/abs/2310.19778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Catalina Gomez, Sue Min Cho, Shichang Ke, Chien-Ming Huang, Mathias Unberath</li>
<li>for: 提高人工智能在决策支持系统中的用户体验，增强人工智能与人类的交互。</li>
<li>methods: 系统atic review of AI-assisted decision making literature，分析105篇论文，提出了一种交互模式分类法，用于描述不同的人工智能交互方式。</li>
<li>results: 现有交互主要是简单的合作模式，报告了相对少的交互功能支持。 taxonomy 能够帮助理解现有决策支持系统中人工智能交互的现状，并促进交互设计的审慎选择。<details>
<summary>Abstract</summary>
Efforts in levering Artificial Intelligence (AI) in decision support systems have disproportionately focused on technological advancements, often overlooking the alignment between algorithmic outputs and human expectations. To address this, explainable AI promotes AI development from a more human-centered perspective. Determining what information AI should provide to aid humans is vital, however, how the information is presented, e. g., the sequence of recommendations and the solicitation of interpretations, is equally crucial. This motivates the need to more precisely study Human-AI interaction as a pivotal component of AI-based decision support. While several empirical studies have evaluated Human-AI interactions in multiple application domains in which interactions can take many forms, there is not yet a common vocabulary to describe human-AI interaction protocols. To address this gap, we describe the results of a systematic review of the AI-assisted decision making literature, analyzing 105 selected articles, which grounds the introduction of a taxonomy of interaction patterns that delineate various modes of human-AI interactivity. We find that current interactions are dominated by simplistic collaboration paradigms and report comparatively little support for truly interactive functionality. Our taxonomy serves as a valuable tool to understand how interactivity with AI is currently supported in decision-making contexts and foster deliberate choices of interaction designs.
</details>
<details>
<summary>摘要</summary>
对于人工智能（AI）在决策支持系统中的应用，各方面的努力都集中在技术进步上，而忽略了算法的人类预期之间的协调。为了解决这个问题，可观察AI的发展方式更加人类中心。决定AI为人类提供什么样的信息是重要的，但是如何呈现这些信息，例如推荐的顺序和寻求解释的方式，也是非常重要的。这为人类AI互动的研究提供了动机，并且发现了多种应用领域中的人类AI互动协议。但是，目前还没有一个通用的语言来描述人类AI互动协议。为了解决这个问题，我们描述了105篇选择的文献的系统性评审结果，并从这些文献中提取了人类AI互动协议的分类。我们发现现有的互动都偏向简单的合作模式，并报告了相对较少的支持真正互动性能。我们的分类可以作为了解人类AI互动在决策context中的支持方式，并且激发人们对互动设计的意识。
</details></li>
</ul>
<hr>
<h2 id="Learn-to-Categorize-or-Categorize-to-Learn-Self-Coding-for-Generalized-Category-Discovery"><a href="#Learn-to-Categorize-or-Categorize-to-Learn-Self-Coding-for-Generalized-Category-Discovery" class="headerlink" title="Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery"></a>Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19776">http://arxiv.org/abs/2310.19776</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sarahrastegar/infosieve">https://github.com/sarahrastegar/infosieve</a></li>
<li>paper_authors: Sarah Rastegar, Hazel Doughty, Cees G. M. Snoek</li>
<li>for: 提出了一种能够在测试时发现未知类别的新方法</li>
<li>methods: 基于优化的思路，对数据实例分配最短类别编码，从而控制类别细分程度</li>
<li>results: 经过实验证明，该方法可以在测试时成功地处理未知类别，并且与现有标准模型进行比较In English, this translates to:</li>
<li>for: Proposed a new method for discovering unknown categories at test time</li>
<li>methods: Based on optimization, assign minimum length category codes to individual data instances to control category granularity</li>
<li>results: Experimental results demonstrate the effectiveness of the method in handling unknown categories at test time, with comparisons to state-of-the-art benchmarks<details>
<summary>Abstract</summary>
In the quest for unveiling novel categories at test time, we confront the inherent limitations of traditional supervised recognition models that are restricted by a predefined category set. While strides have been made in the realms of self-supervised and open-world learning towards test-time category discovery, a crucial yet often overlooked question persists: what exactly delineates a \textit{category}? In this paper, we conceptualize a \textit{category} through the lens of optimization, viewing it as an optimal solution to a well-defined problem. Harnessing this unique conceptualization, we propose a novel, efficient and self-supervised method capable of discovering previously unknown categories at test time. A salient feature of our approach is the assignment of minimum length category codes to individual data instances, which encapsulates the implicit category hierarchy prevalent in real-world datasets. This mechanism affords us enhanced control over category granularity, thereby equipping our model to handle fine-grained categories adeptly. Experimental evaluations, bolstered by state-of-the-art benchmark comparisons, testify to the efficacy of our solution in managing unknown categories at test time. Furthermore, we fortify our proposition with a theoretical foundation, providing proof of its optimality. Our code is available at: \url{https://github.com/SarahRastegar/InfoSieve}.
</details>
<details>
<summary>摘要</summary>
“在试用时探索新的分类ategories，我们面临传统超级vised recognition模型的内在限制。这些模型仅仅受到预先定义的分类category set的限制，而我们则寻求在试用时自动发现新的分类categories。在这篇论文中，我们将分类category视为一个优化问题的最佳解决方案。我们利用这个独特的概念，提出一种新的、效率高且自动化的方法，可以在试用时发现未知的分类categories。我们的方法将实现分类category code的最小长度对个别数据实例的对应，这为我们提供了更好的分类精确度控制，因此我们的模型可以更好地处理细部分类。我们的实验结果，以及与现有的基eline比较，证明了我们的解决方案在处理未知分类ategories的能力。此外，我们还提供了理论基础，证明了我们的方法是最佳的。我们的代码可以在以下连结中找到：https://github.com/SarahRastegar/InfoSieve。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Explainable-Artificial-Intelligence-XAI-2-0-A-Manifesto-of-Open-Challenges-and-Interdisciplinary-Research-Directions"><a href="#Explainable-Artificial-Intelligence-XAI-2-0-A-Manifesto-of-Open-Challenges-and-Interdisciplinary-Research-Directions" class="headerlink" title="Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges and Interdisciplinary Research Directions"></a>Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges and Interdisciplinary Research Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19775">http://arxiv.org/abs/2310.19775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Longo, Mario Brcic, Federico Cabitza, Jaesik Choi, Roberto Confalonieri, Javier Del Ser, Riccardo Guidotti, Yoichi Hayashi, Francisco Herrera, Andreas Holzinger, Richard Jiang, Hassan Khosravi, Freddy Lecue, Gianclaudio Malgieri, Andrés Páez, Wojciech Samek, Johannes Schneider, Timo Speith, Simone Stumpf</li>
<li>for: 本研究旨在探讨透明AI（XAI）的发展和应用，以及相关领域的实际挑战。</li>
<li>methods: 本文使用 manifold learning 和 feature importance 等方法来解释AI模型的决策过程。</li>
<li>results: 本研究提出了27个开问，并分类为9个类别，以便各个领域的研究人员可以共同努力解决XAI领域的挑战。<details>
<summary>Abstract</summary>
As systems based on opaque Artificial Intelligence (AI) continue to flourish in diverse real-world applications, understanding these black box models has become paramount. In response, Explainable AI (XAI) has emerged as a field of research with practical and ethical benefits across various domains. This paper not only highlights the advancements in XAI and its application in real-world scenarios but also addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. Our goal is to put forward a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 27 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.
</details>
<details>
<summary>摘要</summary>
As systems based on opaque Artificial Intelligence (AI) continue to flourish in diverse real-world applications, understanding these black box models has become paramount. In response, Explainable AI (XAI) has emerged as a field of research with practical and ethical benefits across various domains. This paper not only highlights the advancements in XAI and its application in real-world scenarios but also addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. Our goal is to put forward a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 27 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.Translated text in Simplified Chinese:随着透明化人工智能（AI）系统在多个实际应用场景中的普及，理解这些黑盒模型已经成为非常重要。为此，解释AI（XAI）已经成为一种研究领域，具有实用和伦理上的利益。本文不仅探讨了XAI的发展和应用，还Addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. Our goal is to put forward a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 27 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Renaissance-in-Neural-PDE-Solvers"><a href="#Autoregressive-Renaissance-in-Neural-PDE-Solvers" class="headerlink" title="Autoregressive Renaissance in Neural PDE Solvers"></a>Autoregressive Renaissance in Neural PDE Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19763">http://arxiv.org/abs/2310.19763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yolanne Yi Ran Lee</li>
<li>for: 本研究旨在提出一种基于graph neural network的 partial differential equation（PDE）解决方法，以替代传统的束缚方法和Fourier Neural Operator。</li>
<li>methods: 该方法使用了一种名为message passing graph neural network的新型网络架构，通过消息传递机制来实现PDE的解决。</li>
<li>results: 研究表明，该方法可以与或超越传统的PDE解决方法和Fourier Neural Operator在泛化能力和性能上，并且可以更好地处理一些复杂的PDE问题。<details>
<summary>Abstract</summary>
Recent developments in the field of neural partial differential equation (PDE) solvers have placed a strong emphasis on neural operators. However, the paper "Message Passing Neural PDE Solver" by Brandstetter et al. published in ICLR 2022 revisits autoregressive models and designs a message passing graph neural network that is comparable with or outperforms both the state-of-the-art Fourier Neural Operator and traditional classical PDE solvers in its generalization capabilities and performance. This blog post delves into the key contributions of this work, exploring the strategies used to address the common problem of instability in autoregressive models and the design choices of the message passing graph neural network architecture.
</details>
<details>
<summary>摘要</summary>
近期在神经partial differential equation（PDE）解决方法领域，强调神经操作符的发展。然而，布兰德塞特特等在ICLR 2022年发表的论文《消息传递神经PDE解决方法》，重新评估了自动律型模型，并设计了一个可与或超越现有的快扩散 Neil 算法和传统类型PDE解决方法的消息传递图 neural network 架构。本博客文章将探讨这项工作的关键贡献，探讨了自动律型模型中常见的不稳定性问题的解决方案，以及消息传递图 neural network 架构的设计选择。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-and-Defenses-in-Large-Language-Models-Old-and-New-Threats"><a href="#Adversarial-Attacks-and-Defenses-in-Large-Language-Models-Old-and-New-Threats" class="headerlink" title="Adversarial Attacks and Defenses in Large Language Models: Old and New Threats"></a>Adversarial Attacks and Defenses in Large Language Models: Old and New Threats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19737">http://arxiv.org/abs/2310.19737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/schwinnl/llm_embedding_attack">https://github.com/schwinnl/llm_embedding_attack</a></li>
<li>paper_authors: Leo Schwinn, David Dobre, Stephan Günnemann, Gauthier Gidel</li>
<li>for: 本研究旨在解决 neural network 的Robustness问题，尤其是在 natural language processing 领域中，以防止 adversarial attack。</li>
<li>methods: 本研究使用了一些新的方法来评估 robustness，包括 embedding space attacks 和 LLM-specific best practices。</li>
<li>results: 研究发现，without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach。此外， embedding space attacks 也成为了一种可行的威胁模型。<details>
<summary>Abstract</summary>
Over the past decade, there has been extensive research aimed at enhancing the robustness of neural networks, yet this problem remains vastly unsolved. Here, one major impediment has been the overestimation of the robustness of new defense approaches due to faulty defense evaluations. Flawed robustness evaluations necessitate rectifications in subsequent works, dangerously slowing down the research and providing a false sense of security. In this context, we will face substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We provide a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations. Additionally, we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models. Finally, we demonstrate on a recently proposed defense that, without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Large-Language-Models-A-Comprehensive-Survey"><a href="#Evaluating-Large-Language-Models-A-Comprehensive-Survey" class="headerlink" title="Evaluating Large Language Models: A Comprehensive Survey"></a>Evaluating Large Language Models: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19736">http://arxiv.org/abs/2310.19736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tjunlp-lab/awesome-llms-evaluation-papers">https://github.com/tjunlp-lab/awesome-llms-evaluation-papers</a></li>
<li>paper_authors: Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong</li>
<li>For: 评估大语言模型（LLMs）的能力和安全性。* Methods: 分为三类：知识和能力评估、对Alignment评估和安全评估。* Results: 提供了一个全面的评估方法和标准套件，以及一些特定领域的评估研究。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.   This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability.   We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在各种任务上表现出了惊人的能力，引起了广泛的关注和应用。然而，与一个双刃剑相似，LLM也存在潜在的风险。它们可能会导致私人数据泄露或生成不当、伤害或误导的内容。此外，LLM的快速进步也引起了关于可能出现无适应安全措施的超智系统的担忧。为了有效利用LLM的能力并确保其安全和有益的发展，对LLM的评估是非常重要。本调查尝试提供LLM评估的全面视图。我们将LLM评估分为三个主要类别：知识和能力评估、对齐评估和安全评估。此外，我们还提供了对这三个方面评估方法和标准的全面评论，并收录了关于LLM在特定领域的表现评估，以及建立了涵盖LLM评估能力、对齐性、安全性和可用性的完整评估平台。我们希望这份全面的概述能够激发更多关于LLM评估的研究兴趣，以实现评估成为LLM发展的重要指南，以最大化社会 benefit  while minimizing potential risks。相关论文的汇总可以在 <https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers> 中找到。
</details></li>
</ul>
<hr>
<h2 id="ViR-Vision-Retention-Networks"><a href="#ViR-Vision-Retention-Networks" class="headerlink" title="ViR: Vision Retention Networks"></a>ViR: Vision Retention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19731">http://arxiv.org/abs/2310.19731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Hatamizadeh, Michael Ranzinger, Jan Kautz</li>
<li>for: 该 paper 的目的是提出一种新的计算机视网络模型，以实现高速的推理和平行的训练。</li>
<li>methods: 该 paper 使用了新的拓展 Transformer 模型，并提出了一种新的并行和循环表示方法，以实现高效的推理和训练。</li>
<li>results: 该 paper 通过了多种 dataset 和不同的图像分辨率进行了广泛的实验，并 achieved 竞争性的性能。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have attracted a lot of popularity in recent years, due to their exceptional capabilities in modeling long-range spatial dependencies and scalability for large scale training. Although the training parallelism of self-attention mechanism plays an important role in retaining great performance, its quadratic complexity baffles the application of ViTs in many scenarios which demand fast inference. This effect is even more pronounced in applications in which autoregressive modeling of input features is required. In Natural Language Processing (NLP), a new stream of efforts have proposed parallelizable models with recurrent formulation that allows for efficient inference in generative applications. Inspired by this trend, we propose a new class of computer vision models, dubbed Vision Retention Networks (ViR), with dual parallel and recurrent formulations, which strike an optimal balance between fast inference and parallel training with competitive performance. In particular, ViR scales favorably for image throughput and memory consumption in tasks that require higher-resolution images due to its flexible formulation in processing large sequence lengths. The ViR is the first attempt to realize dual parallel and recurrent equivalency in a general vision backbone for recognition tasks. We have validated the effectiveness of ViR through extensive experiments with different dataset sizes and various image resolutions and achieved competitive performance. Our code and pretrained models will be made publicly available.
</details>
<details>
<summary>摘要</summary>
视transformer（ViT）在最近几年内引起了很多关注，因为它们在模型长距离空间相互关联的能力和批处理大规模训练中表现出色。 although self-attention机制的训练并行性起到了重要的作用，但它的quadratic复杂性使得ViT在许多场景中不适用，特别是需要快速推理的应用场景。在自然语言处理（NLP）领域，一些新的努力提出了并行化模型的概念，使得在生成应用中实现快速推理变得可能。 drawing inspiration from this trend, we propose a new class of computer vision models, called Vision Retention Networks (ViR), which strike an optimal balance between fast inference and parallel training with competitive performance. in particular, ViR scales favorably for image throughput and memory consumption in tasks that require higher-resolution images due to its flexible formulation in processing large sequence lengths. ViR is the first attempt to realize dual parallel and recurrent equivalency in a general vision backbone for recognition tasks. we have validated the effectiveness of ViR through extensive experiments with different dataset sizes and various image resolutions and achieved competitive performance. our code and pretrained models will be made publicly available.
</details></li>
</ul>
<hr>
<h2 id="Generating-Medical-Instructions-with-Conditional-Transformer"><a href="#Generating-Medical-Instructions-with-Conditional-Transformer" class="headerlink" title="Generating Medical Instructions with Conditional Transformer"></a>Generating Medical Instructions with Conditional Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19727">http://arxiv.org/abs/2310.19727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Warren Del-Pinto, Goran Nenadic<br>for:The paper is written to introduce a novel task-specific model architecture, Label-To-Text-Transformer (LT3), which generates synthetic medical instructions based on provided labels.methods:The LT3 model is trained on a vast corpus of medical instructions extracted from the MIMIC-III database, and it uses a task-specific transformer architecture to generate synthetic medical instructions.results:The paper evaluates the performance of LT3 by contrasting it with a state-of-the-art Pre-trained Language Model (PLM), T5, and shows that LT3 can generate high-quality and diverse synthetic medical instructions. The generated synthetic data is used to train the SpacyNER model for the Named Entity Recognition (NER) task over the n2c2-2018 dataset, and the results show that the model trained on synthetic data can achieve a 96-98% F1 score at Label Recognition on Drug, Frequency, Route, Strength, and Form.<details>
<summary>Abstract</summary>
Access to real-world medical instructions is essential for medical research and healthcare quality improvement. However, access to real medical instructions is often limited due to the sensitive nature of the information expressed. Additionally, manually labelling these instructions for training and fine-tuning Natural Language Processing (NLP) models can be tedious and expensive. We introduce a novel task-specific model architecture, Label-To-Text-Transformer (\textbf{LT3}), tailored to generate synthetic medical instructions based on provided labels, such as a vocabulary list of medications and their attributes. LT3 is trained on a vast corpus of medical instructions extracted from the MIMIC-III database, allowing the model to produce valuable synthetic medical instructions. We evaluate LT3's performance by contrasting it with a state-of-the-art Pre-trained Language Model (PLM), T5, analysing the quality and diversity of generated texts. We deploy the generated synthetic data to train the SpacyNER model for the Named Entity Recognition (NER) task over the n2c2-2018 dataset. The experiments show that the model trained on synthetic data can achieve a 96-98\% F1 score at Label Recognition on Drug, Frequency, Route, Strength, and Form. LT3 codes and data will be shared at \url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}
</details>
<details>
<summary>摘要</summary>
accessed to real-world medical instructions is essential for medical research and healthcare quality improvement. However, access to real medical instructions is often limited due to the sensitive nature of the information expressed. Additionally, manually labelling these instructions for training and fine-tuning Natural Language Processing (NLP) models can be tedious and expensive. We introduce a novel task-specific model architecture, Label-To-Text-Transformer (\textbf{LT3}), tailored to generate synthetic medical instructions based on provided labels, such as a vocabulary list of medications and their attributes. LT3 is trained on a vast corpus of medical instructions extracted from the MIMIC-III database, allowing the model to produce valuable synthetic medical instructions. We evaluate LT3's performance by contrasting it with a state-of-the-art Pre-trained Language Model (PLM), T5, analysing the quality and diversity of generated texts. We deploy the generated synthetic data to train the SpacyNER model for the Named Entity Recognition (NER) task over the n2c2-2018 dataset. The experiments show that the model trained on synthetic data can achieve a 96-98\% F1 score at Label Recognition on Drug, Frequency, Route, Strength, and Form. LT3 codes and data will be shared at \url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="A-Path-to-Simpler-Models-Starts-With-Noise"><a href="#A-Path-to-Simpler-Models-Starts-With-Noise" class="headerlink" title="A Path to Simpler Models Starts With Noise"></a>A Path to Simpler Models Starts With Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19726">http://arxiv.org/abs/2310.19726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lesia Semenova, Harry Chen, Ronald Parr, Cynthia Rudin</li>
<li>For: 这篇论文探讨了决策森林模型在各种领域中的性能问题，特别是在含有噪声的数据集上。* Methods: 该论文提出了一种机制，即数据生成过程和分析者在学习过程中的选择，对决策森林模型的性能产生影响。同时，该论文引入了一个名为“模式多样性”的指标，用于衡量决策森林模型在不同分类模式下的差异。* Results: 该论文发现，噪声程度高的数据集会导致决策森林模型的性能更高，并且模式多样性指标与噪声度之间存在正相关关系。这些结果解释了为什么简单的模型在复杂的数据集上可以达到黑盒模型的同等精度水平。<details>
<summary>Abstract</summary>
The Rashomon set is the set of models that perform approximately equally well on a given dataset, and the Rashomon ratio is the fraction of all models in a given hypothesis space that are in the Rashomon set. Rashomon ratios are often large for tabular datasets in criminal justice, healthcare, lending, education, and in other areas, which has practical implications about whether simpler models can attain the same level of accuracy as more complex models. An open question is why Rashomon ratios often tend to be large. In this work, we propose and study a mechanism of the data generation process, coupled with choices usually made by the analyst during the learning process, that determines the size of the Rashomon ratio. Specifically, we demonstrate that noisier datasets lead to larger Rashomon ratios through the way that practitioners train models. Additionally, we introduce a measure called pattern diversity, which captures the average difference in predictions between distinct classification patterns in the Rashomon set, and motivate why it tends to increase with label noise. Our results explain a key aspect of why simpler models often tend to perform as well as black box models on complex, noisier datasets.
</details>
<details>
<summary>摘要</summary>
“Rashomon集是指在给定数据集上表现相似的模型集合，而Rashomon比率则是所有模型空间中这些模型的比例。在刑事、医疗、贷款、教育等领域的表格数据集上，Rashomon比率经常很大，这有各种实际意义，例如是否可以使用简单的模型来达到与更复杂的模型相同的准确率。现有一个开问是为什么Rashomon比率经常很大。在这种工作中，我们提出了数据生成过程中的一种机制，以及分析者在学习过程中通常会选择的决策，这些机制会决定Rashomon比率的大小。具体来说，我们发现在噪声数据集上训练模型时，噪声会导致Rashomon比率更大。此外，我们还引入了一个名为“模式多样性”的度量，它捕捉了Rashomon集中不同分类模式之间的预测差异平均值，并解释了为什么在噪声数据集上，模式多样性往往增加。我们的结果解释了简单模型在复杂、噪声数据集上表现的好，是一种重要的现象。”
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Knowledge-Editing-of-Neural-Networks"><a href="#A-Survey-on-Knowledge-Editing-of-Neural-Networks" class="headerlink" title="A Survey on Knowledge Editing of Neural Networks"></a>A Survey on Knowledge Editing of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19704">http://arxiv.org/abs/2310.19704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vittorio Mazzia, Alessandro Pedrani, Andrea Caciolai, Kay Rottmann, Davide Bernardi</li>
<li>for: 本研究旨在解决人工智能中的神经网络编辑问题，即如何通过不影响神经网络已经学习的任务来更新神经网络模型，以适应数据的变化。</li>
<li>methods: 本研究使用了多种方法来解决神经网络编辑问题，包括常规化技术、元学习、直接模型编辑和建筑策略等。</li>
<li>results: 本研究提供了一个简洁的概述神经网络编辑领域的最新研究成果，并将相关的方法和数据集分类为四个家族：常规化技术、元学习、直接模型编辑和建筑策略等。<details>
<summary>Abstract</summary>
Deep neural networks are becoming increasingly pervasive in academia and industry, matching and surpassing human performance on a wide variety of fields and related tasks. However, just as humans, even the largest artificial neural networks make mistakes, and once-correct predictions can become invalid as the world progresses in time. Augmenting datasets with samples that account for mistakes or up-to-date information has become a common workaround in practical applications. However, the well-known phenomenon of catastrophic forgetting poses a challenge in achieving precise changes in the implicitly memorized knowledge of neural network parameters, often requiring a full model re-training to achieve desired behaviors. That is expensive, unreliable, and incompatible with the current trend of large self-supervised pre-training, making it necessary to find more efficient and effective methods for adapting neural network models to changing data. To address this need, knowledge editing is emerging as a novel area of research that aims to enable reliable, data-efficient, and fast changes to a pre-trained target model, without affecting model behaviors on previously learned tasks. In this survey, we provide a brief review of this recent artificial intelligence field of research. We first introduce the problem of editing neural networks, formalize it in a common framework and differentiate it from more notorious branches of research such as continuous learning. Next, we provide a review of the most relevant knowledge editing approaches and datasets proposed so far, grouping works under four different families: regularization techniques, meta-learning, direct model editing, and architectural strategies. Finally, we outline some intersections with other fields of research and potential directions for future works.
</details>
<details>
<summary>摘要</summary>
深度神经网络在学术和实际领域中日益普遍，与人类表现相当或超越人类在各种领域和相关任务上。然而，和人类一样，即使是最大的人工神经网络也会出错，并且已经正确的预测可能会成为时间推移的情况变化。为了应对这种情况，在实际应用中通常会补充数据集中的错误样本或更新信息。然而，神经网络参数中的隐式记忆知识更新时，经常会遇到危机性忘记问题，需要重新训练整个模型来实现愿望的行为。这是贵重的、不可预测的、与当前大量自动学习预训练的趋势不兼容，因此需要找到更有效率和可靠的方法来适应神经网络模型与数据变化。为此，知识编辑正在成为一种新的人工智能研究领域，旨在在不影响神经网络模型在之前学习任务上的行为的情况下，可靠、数据效率、快速地改变预训练目标模型。在这篇评论中，我们首先介绍编辑神经网络的问题，将其正式化为一种通用框架，并与不同的研究分支（如连续学习）进行区分。然后，我们提供了最新的知识编辑方法和数据集的审视，将工作分为四个家族： regularization techniques、meta-learning、直接模型编辑和建筑策略。最后，我们描述了与其他研究领域的交叉和未来工作的可能性。
</details></li>
</ul>
<hr>
<h2 id="Causal-Context-Connects-Counterfactual-Fairness-to-Robust-Prediction-and-Group-Fairness"><a href="#Causal-Context-Connects-Counterfactual-Fairness-to-Robust-Prediction-and-Group-Fairness" class="headerlink" title="Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness"></a>Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19691">http://arxiv.org/abs/2310.19691</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacyanthis/causal-context">https://github.com/jacyanthis/causal-context</a></li>
<li>paper_authors: Jacy Reese Anthis, Victor Veitch</li>
<li>for:  This paper focuses on the problem of fairness in machine learning, specifically addressing the concept of counterfactual fairness and its relationship to other fairness metrics.</li>
<li>methods:  The authors use a causal context to bridge the gap between counterfactual fairness, robust prediction, and group fairness. They develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness.</li>
<li>results:  The authors show that in three common fairness contexts (measurement error, selection on label, and selection on predictors), counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Additionally, they demonstrate that counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics.<details>
<summary>Abstract</summary>
Counterfactual fairness requires that a person would have been classified in the same way by an AI or other algorithmic system if they had a different protected class, such as a different race or gender. This is an intuitive standard, as reflected in the U.S. legal system, but its use is limited because counterfactuals cannot be directly observed in real-world data. On the other hand, group fairness metrics (e.g., demographic parity or equalized odds) are less intuitive but more readily observed. In this paper, we use $\textit{causal context}$ to bridge the gaps between counterfactual fairness, robust prediction, and group fairness. First, we motivate counterfactual fairness by showing that there is not necessarily a fundamental trade-off between fairness and accuracy because, under plausible conditions, the counterfactually fair predictor is in fact accuracy-optimal in an unbiased target distribution. Second, we develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness. Third, we show that in three common fairness contexts$\unicode{x2013}$measurement error, selection on label, and selection on predictors$\unicode{x2013}$counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics.
</details>
<details>
<summary>摘要</summary>
<<SYS>>ounterfactual fairness requires that a person would have been classified in the same way by an AI or other algorithmic system if they had a different protected class, such as a different race or gender. This is an intuitive standard, as reflected in the U.S. legal system, but its use is limited because counterfactuals cannot be directly observed in real-world data. On the other hand, group fairness metrics (e.g., demographic parity or equalized odds) are less intuitive but more readily observed. In this paper, we use $\textit{causal context}$ to bridge the gaps between counterfactual fairness, robust prediction, and group fairness. First, we motivate counterfactual fairness by showing that there is not necessarily a fundamental trade-off between fairness and accuracy because, under plausible conditions, the counterfactually fair predictor is in fact accuracy-optimal in an unbiased target distribution. Second, we develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness. Third, we show that in three common fairness contexts$\unicode{x2013}$measurement error, selection on label, and selection on predictors$\unicode{x2013}$counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics.Here's the text with the correct Chinese characters and punctuation:ounterfactual fairness requires that a person would have been classified in the same way by an AI or other algorithmic system if they had a different protected class, such as a different race or gender. This is an intuitive standard, as reflected in the U.S. legal system, but its use is limited because counterfactuals cannot be directly observed in real-world data. On the other hand, group fairness metrics (e.g., demographic parity or equalized odds) are less intuitive but more readily observed. In this paper, we use $\textit{causal context}$ to bridge the gaps between counterfactual fairness, robust prediction, and group fairness. First, we motivate counterfactual fairness by showing that there is not necessarily a fundamental trade-off between fairness and accuracy because, under plausible conditions, the counterfactually fair predictor is in fact accuracy-optimal in an unbiased target distribution. Second, we develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness. Third, we show that in three common fairness contexts$\unicode{x2013}$measurement error, selection on label, and selection on predictors$\unicode{x2013}$counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics.
</details></li>
</ul>
<hr>
<h2 id="Can-input-reconstruction-be-used-to-directly-estimate-uncertainty-of-a-regression-U-Net-model-–-Application-to-proton-therapy-dose-prediction-for-head-and-neck-cancer-patients"><a href="#Can-input-reconstruction-be-used-to-directly-estimate-uncertainty-of-a-regression-U-Net-model-–-Application-to-proton-therapy-dose-prediction-for-head-and-neck-cancer-patients" class="headerlink" title="Can input reconstruction be used to directly estimate uncertainty of a regression U-Net model? – Application to proton therapy dose prediction for head and neck cancer patients"></a>Can input reconstruction be used to directly estimate uncertainty of a regression U-Net model? – Application to proton therapy dose prediction for head and neck cancer patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19686">http://arxiv.org/abs/2310.19686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Margerie Huet-Dastarac, Dan Nguyen, Steve Jiang, John Lee, Ana Barragan Montero</li>
<li>for: 这篇论文旨在提供一种可靠和高效的深度学习模型 uncertainty 估计方法，并且能够检测出资料集外的数据（Out-of-distribution，OOD）。</li>
<li>methods: 这篇论文提出了一种直接使用构造汇流（bottleneck）来估计模型 uncertainty的方法，具体来说是将构造汇流中的一支分支用来重建输入数据。</li>
<li>results: 在这篇论文中，这种方法在预报癌症肿瘤疗法剂量预测 tasks 中与 MCDO 和 DE 相比，得到了更高的 Pearson 相関系数（0.620），并且能够轻松地检测出 OOD 数据（Z-score 34.05）。<details>
<summary>Abstract</summary>
Estimating the uncertainty of deep learning models in a reliable and efficient way has remained an open problem, where many different solutions have been proposed in the literature. Most common methods are based on Bayesian approximations, like Monte Carlo dropout (MCDO) or Deep ensembling (DE), but they have a high inference time (i.e. require multiple inference passes) and might not work for out-of-distribution detection (OOD) data (i.e. similar uncertainty for in-distribution (ID) and OOD). In safety critical environments, like medical applications, accurate and fast uncertainty estimation methods, able to detect OOD data, are crucial, since wrong predictions can jeopardize patients safety. In this study, we present an alternative direct uncertainty estimation method and apply it for a regression U-Net architecture. The method consists in the addition of a branch from the bottleneck which reconstructs the input. The input reconstruction error can be used as a surrogate of the model uncertainty. For the proof-of-concept, our method is applied to proton therapy dose prediction in head and neck cancer patients. Accuracy, time-gain, and OOD detection are analyzed for our method in this particular application and compared with the popular MCDO and DE. The input reconstruction method showed a higher Pearson correlation coefficient with the prediction error (0.620) than DE and MCDO (between 0.447 and 0.612). Moreover, our method allows an easier identification of OOD (Z-score of 34.05). It estimates the uncertainty simultaneously to the regression task, therefore requires less time or computational resources.
</details>
<details>
<summary>摘要</summary>
深度学习模型的不确定性估计问题一直是开放的问题，文献中有许多不同的解决方案。大多数常见方法基于 bayesian  aproximation，如 Monte Carlo dropout（MCDO）或 Deep ensembling（DE），但它们的推理时间（即需要多个推理pass）很高，并且可能无法处理非标样本（OOD）数据（即标样本和 OOD 的不确定性相同）。在安全关键环境，如医疗应用，准确和快速的不确定性估计方法，能够检测 OOD 数据，是关键的，因为错误预测可能会威胁病人安全。在这种情况下，我们提出了一种irect uncertainty estimation方法，并在一种 regression U-Net 架构上应用了它。该方法的基本思想是在瓶颈处加入一个分支，用于重建输入。输入重建错误可以作为模型不确定性的Surrogate。为证明概念，我们将方法应用于肿瘤疗效剂量预测。我们分析了我们方法的准确性、时间成本和 OOD 检测，并与 MCDO 和 DE 进行比较。重建输入方法的 Pearson 相关系数（0.620）高于 DE 和 MCDO（ между 0.447 和 0.612），而且我们的方法可以更容易地检测 OOD（Z-score 为 34.05）。此外，我们的方法可以同时估计不确定性和 regression 任务，因此需要 fewer 时间或计算资源。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Pre-trained-Language-Model-into-Neural-Machine-Translation"><a href="#Integrating-Pre-trained-Language-Model-into-Neural-Machine-Translation" class="headerlink" title="Integrating Pre-trained Language Model into Neural Machine Translation"></a>Integrating Pre-trained Language Model into Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19680">http://arxiv.org/abs/2310.19680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soon-Jae Hwang, Chang-Sung Jeong</li>
<li>for: 提高Neural Machine Translation（NMT）性能，解决高质量双语对应语料不足问题。</li>
<li>methods: 使用预训练语言模型（PLM）提供上下文信息，并提出PLM集成NMT（PiNMT）模型，包括PLM多层转换器、嵌入合并和夹角匹配等三个关键组件。</li>
<li>results: 通过提出的PiNMT模型和训练策略（分离学习率和双步训练），在IWSLT’14 En$\leftrightarrow$De数据集上实现了状态级表现。<details>
<summary>Abstract</summary>
Neural Machine Translation (NMT) has become a significant technology in natural language processing through extensive research and development. However, the deficiency of high-quality bilingual language pair data still poses a major challenge to improving NMT performance. Recent studies are exploring the use of contextual information from pre-trained language model (PLM) to address this problem. Yet, the issue of incompatibility between PLM and NMT model remains unresolved. This study proposes a PLM-integrated NMT (PiNMT) model to overcome the identified problems. The PiNMT model consists of three critical components, PLM Multi Layer Converter, Embedding Fusion, and Cosine Alignment, each playing a vital role in providing effective PLM information to NMT. Furthermore, two training strategies, Separate Learning Rates and Dual Step Training, are also introduced in this paper. By implementing the proposed PiNMT model and training strategy, we achieved state-of-the-art performance on the IWSLT'14 En$\leftrightarrow$De dataset. This study's outcomes are noteworthy as they demonstrate a novel approach for efficiently integrating PLM with NMT to overcome incompatibility and enhance performance.
</details>
<details>
<summary>摘要</summary>
neural machine translation (NMT) 已经成为自然语言处理领域的重要技术，经过广泛的研究和开发。然而，高质量的双语对数据仍然是NMT性能提高的主要挑战。最近的研究在使用预训练语言模型（PLM）的上下文信息来解决这个问题。然而，PLM和NMT模型之间的不兼容问题仍未得到解决。本研究提出了PLM结合NMT（PiNMT）模型，以解决这些问题。PiNMT模型包括三个关键组件：PLM多层转换器、嵌入混合和cosine对齐，每个组件都在提供PLM信息到NMT中发挥重要作用。此外，本研究还提出了两种训练策略：分开学习率和双步训练。通过实施提议的PiNMT模型和训练策略，我们在IWSLT'14 En$\leftrightarrow$De数据集上实现了状态的表现。这些成果含义重大，因为它们证明了一种有效的PLM与NMT集成方法，以解决不兼容性和提高性能。
</details></li>
</ul>
<hr>
<h2 id="AI-Alignment-A-Comprehensive-Survey"><a href="#AI-Alignment-A-Comprehensive-Survey" class="headerlink" title="AI Alignment: A Comprehensive Survey"></a>AI Alignment: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19852">http://arxiv.org/abs/2310.19852</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PKU-Alignment/AlignmentSurvey">https://github.com/PKU-Alignment/AlignmentSurvey</a></li>
<li>paper_authors: Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O’Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, Wen Gao</li>
<li>for: 这个论文旨在为AI系统的Alignment提供一个全面和升级的介绍，以便更好地理解和控制AI系统的行为。</li>
<li>methods: 本论文使用RICE原则（Robustness、Interpretability、Controllability、Ethicality）作为AIAlignment的四个基本目标，并将当前的Alignment研究划分为两个主要组成部分：前向Alignment和后向Alignment。前向Alignment通过对AI系统进行Alignment训练来实现Alignment，而后向Alignment则是通过证明AI系统的Alignment来控制和调节它们，以避免加剧不Alignment的风险。</li>
<li>results: 本论文提出了一种Recurrent Process（前向Alignment和后向Alignment），该过程可以确保AI系统的Alignment，并且在每一次Alignment后，可以提供更新的目标对于下一轮Alignment。此外，论文还讨论了反馈学习和分布shift学习，以及对AI系统生命周期的每一个阶段的Assurance技术和管理做法。<details>
<summary>Abstract</summary>
AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, the potential large-scale risks associated with misaligned AI systems become salient. Hundreds of AI experts and public figures have expressed concerns about AI risks, arguing that "mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war". To provide a comprehensive and up-to-date overview of the alignment field, in this survey paper, we delve into the core concepts, methodology, and practice of alignment. We identify the RICE principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality. Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. Forward alignment and backward alignment form a recurrent process where the alignment of AI systems from the forward process is verified in the backward process, meanwhile providing updated objectives for forward alignment in the next round. On forward alignment, we discuss learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices that apply to every stage of AI systems' lifecycle.   We also release and continually update the website (www.alignmentsurvey.com) which features tutorials, collections of papers, blog posts, and other resources.
</details>
<details>
<summary>摘要</summary>
人工智能启 alignment 目标是使人工智能系统与人类意图和价值观 align  together。随着人工智能系统的能力增强，落后的大规模风险减少成为焦点。多位 AI 专家和公众人物表达了关于 AI 风险的关注，认为“控制 AI 风险的扩展应该是全球优先事项，与其他社会级风险相提并论”。为了提供完整和准确的对 alignment 领域的概述，在这篇调查报告中，我们深入探讨了核心概念、方法和实践的 alignment。我们认为 RICE 原则是 AI  alignment 的关键目标：Robustness、可读性、可控性和伦理。 guid by these four principles, we outline the landscape of current alignment research and decompose them into two key components：forward alignment and backward alignment。前者 aim to make AI systems aligned via alignment training，而后者 aim to obtain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks。forward alignment 和 backward alignment form a recurrent process，where the alignment of AI systems from the forward process is verified in the backward process， meanwhile providing updated objectives for forward alignment in the next round。在 forward alignment 方面，我们讨论了从反馈学习和分布转换学习。在 backward alignment 方面，我们讨论了对 AI 系统的生命周期中每一个阶段的 assurance 技术和管理做法。我们还将 continually 更新网站（www.alignmentsurvey.com），该网站包括教程、论文收集、博客文章和其他资源。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-The-Need-for-Nuance-in-Current-Debates-and-a-Pragmatic-Perspective-on-Understanding"><a href="#Large-Language-Models-The-Need-for-Nuance-in-Current-Debates-and-a-Pragmatic-Perspective-on-Understanding" class="headerlink" title="Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding"></a>Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19671">http://arxiv.org/abs/2310.19671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bram M. A. van Dijk, Tom Kouwenhoven, Marco R. Spruit, Max J. van Duijn</li>
<li>for: 这篇论文主要是为了评估大型自然语言处理器（LLM）的能力，并且探讨关于 LLM 的评价和含义。</li>
<li>methods: 本文使用了理论和实证方法来评估 LLM 的能力，包括对三个常见批评点进行了严谨的分析。</li>
<li>results: 本文的结果表明，对 LLM 的评价需要更加细化，并且提出了一种 Pragmatic 视角来理解 LLM 的含义和意图。<details>
<summary>Abstract</summary>
Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of `real' understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviour effectively. We reflect on the circumstances under which it would make sense for humans to similarly attribute mental states to LLMs, thereby outlining a pragmatic philosophical context for LLMs as an increasingly prominent technology in society.
</details>
<details>
<summary>摘要</summary>
现有的大型自然语言处理模型（LLM） possess incredible ability to generate grammatically correct and fluent text. LLMs are emerging rapidly, and debates about their capacities have intensified, but reflection is lagging behind. Therefore, in this position paper, we will first focus on the debates and critically assess three points that are frequently raised in critiques of LLM capacities:1. LLMs only parrot statistical patterns in the training data;2. LLMs master formal language competence but not functional language competence;3. Language learning in LLMs cannot inform human language learning.Drawing on empirical and theoretical arguments, we will show that these points require more nuance. Second, we will outline a pragmatic perspective on the issue of "real" understanding and intentionality in LLMs. Understanding and intentionality refer to the unobservable mental states that we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behavior effectively. We will reflect on the circumstances under which it would make sense for humans to attribute mental states to LLMs, thereby outlining a pragmatic philosophical context for LLMs as an increasingly prominent technology in society.
</details></li>
</ul>
<hr>
<h2 id="Explaining-Tree-Model-Decisions-in-Natural-Language-for-Network-Intrusion-Detection"><a href="#Explaining-Tree-Model-Decisions-in-Natural-Language-for-Network-Intrusion-Detection" class="headerlink" title="Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection"></a>Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19658">http://arxiv.org/abs/2310.19658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noah Ziems, Gang Liu, John Flanagan, Meng Jiang</li>
<li>for: 本研究旨在提高网络入侵检测（NID）系统的决策树模型，以便更好地检测恶意网络流量。</li>
<li>methods: 本研究使用大型自然语言模型（LLM）来提供解释和背景知识，以帮助用户更好地理解决策树的决策。</li>
<li>results: 研究发现，LLM生成的决策树解释与人类评价的可读性、质量和背景知识之间呈高度相关，同时能够提供更好的决策边界的理解。<details>
<summary>Abstract</summary>
Network intrusion detection (NID) systems which leverage machine learning have been shown to have strong performance in practice when used to detect malicious network traffic. Decision trees in particular offer a strong balance between performance and simplicity, but require users of NID systems to have background knowledge in machine learning to interpret. In addition, they are unable to provide additional outside information as to why certain features may be important for classification.   In this work, we explore the use of large language models (LLMs) to provide explanations and additional background knowledge for decision tree NID systems. Further, we introduce a new human evaluation framework for decision tree explanations, which leverages automatically generated quiz questions that measure human evaluators' understanding of decision tree inference. Finally, we show LLM generated decision tree explanations correlate highly with human ratings of readability, quality, and use of background knowledge while simultaneously providing better understanding of decision boundaries.
</details>
<details>
<summary>摘要</summary>
网络入侵检测（NID）系统利用机器学习技术已经在实践中表现出色，检测恶意网络流量。决策树特别是在性能和简单性之间做出了好的折衔，但需要NID系统用户具备机器学习背景知识来解释。此外，它们无法提供外部信息，以解释特定特征对分类有什么影响。在这项工作中，我们探讨了使用大型自然语言模型（LLM）提供解释和背景知识，以帮助决策树NID系统更好地理解决ución。此外，我们还提出了一种新的人工评估框架，用于评估决策树解释的可读性、质量和背景知识使用程度。最后，我们表明LLM生成的决策树解释与人类评估中的可读性、质量和背景知识使用程度高度相关，同时提供更好的决策边界理解。
</details></li>
</ul>
<hr>
<h2 id="MCAD-Multi-teacher-Cross-modal-Alignment-Distillation-for-efficient-image-text-retrieval"><a href="#MCAD-Multi-teacher-Cross-modal-Alignment-Distillation-for-efficient-image-text-retrieval" class="headerlink" title="MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval"></a>MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19654">http://arxiv.org/abs/2310.19654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youbo Lei, Feifei He, Chen Chen, Yingbin Mo, Si Jia Li, Defeng Xie, Haonan Lu</li>
<li>for: 降低大型视觉语言预训模型的模型大小和加速其终端设备部署，以提高图文检索的效率和灵活性。</li>
<li>methods: 提出了一种多教师跨Modal对采用单流和双流模型的混合方法，通过将单流模型中的综合特征与双流模型中的图像和文本特征进行混合，以提高学生模型的检索性能。</li>
<li>results: 通过进行logit和特征填充，使学生双流模型的检索性能得到明显提高，而无需增加检索复杂度。此外，在Snapdragon clips上实现了一个具有93M内存和30ms搜索延迟的移动CLIP模型，未出现明显性能下降。<details>
<summary>Abstract</summary>
With the success of large-scale visual-language pretraining models and the wide application of image-text retrieval in industry areas, reducing the model size and streamlining their terminal-device deployment have become urgently necessary. The mainstream model structures for image-text retrieval are single-stream and dual-stream, both aiming to close the semantic gap between visual and textual modalities. Dual-stream models excel at offline indexing and fast inference, while single-stream models achieve more accurate cross-model alignment by employing adequate feature fusion. We propose a multi-teacher cross-modality alignment distillation (MCAD) technique to integrate the advantages of single-stream and dual-stream models. By incorporating the fused single-stream features into the image and text features of the dual-stream model, we formulate new modified teacher features and logits. Then, we conduct both logit and feature distillation to boost the capability of the student dual-stream model, achieving high retrieval performance without increasing inference complexity. Extensive experiments demonstrate the remarkable performance and high efficiency of MCAD on image-text retrieval tasks. Furthermore, we implement a mobile CLIP model on Snapdragon clips with only 93M running memory and 30ms search latency, without apparent performance degradation of the original large CLIP.
</details>
<details>
<summary>摘要</summary>
通过大规模视语言预训模型的成功和图文检索在业界的广泛应用，减小模型大小并将其部署到终端设备上已经变得非常必要。主流的图文检索模型结构包括单流和双流，两者都努力封闭视和文本模式之间的Semantic Gap。双流模型在线索索引和快速推理方面表现出色，而单流模型通过适当的特征融合实现更高精度的跨模型对接。我们提出了一种多教师跨模态对接填充（MCAD）技术，将单流特征融合到双流模型中的图像和文本特征上。然后，我们进行了日志和特征填充来提高学生双流模型的能力，实现高效的图文检索任务。广泛的实验表明MCAD在图文检索任务中的表现很出色，同时具有高效性。此外，我们在Snapdragon板上实现了具有93M内存和30ms搜索延迟的移动CLIP模型，无 Apparent performance degradation of the original large CLIP.
</details></li>
</ul>
<hr>
<h2 id="Fast-swap-regret-minimization-and-applications-to-approximate-correlated-equilibria"><a href="#Fast-swap-regret-minimization-and-applications-to-approximate-correlated-equilibria" class="headerlink" title="Fast swap regret minimization and applications to approximate correlated equilibria"></a>Fast swap regret minimization and applications to approximate correlated equilibria</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19647">http://arxiv.org/abs/2310.19647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binghui Peng, Aviad Rubinstein<br>for: 本研究的目的是解决[Blum和Mansour 2007]中的主要开放问题，提出了一种可靠和计算效率高的算法，可以在只有polylog(n)轮次内减少T-swap regret至εT。methods: 本研究使用了一种新的算法，具有对ε的指数减少，但是我们证明了一个新的下界。results: 本研究的算法可以在polylog(n)轮次内减少T-swap regret至εT，并且解决了[Blum和Mansour 2007]中的主要开放问题。此外，本研究还提出了一种可靠和计算效率高的算法，可以在polylog(n)轮次内实现ε-Correlated Equilibrium（ε-CE）在多种场景中。<details>
<summary>Abstract</summary>
We give a simple and computationally efficient algorithm that, for any constant $\varepsilon>0$, obtains $\varepsilon T$-swap regret within only $T = \mathsf{polylog}(n)$ rounds; this is an exponential improvement compared to the super-linear number of rounds required by the state-of-the-art algorithm, and resolves the main open problem of [Blum and Mansour 2007]. Our algorithm has an exponential dependence on $\varepsilon$, but we prove a new, matching lower bound.   Our algorithm for swap regret implies faster convergence to $\varepsilon$-Correlated Equilibrium ($\varepsilon$-CE) in several regimes: For normal form two-player games with $n$ actions, it implies the first uncoupled dynamics that converges to the set of $\varepsilon$-CE in polylogarithmic rounds; a $\mathsf{polylog}(n)$-bit communication protocol for $\varepsilon$-CE in two-player games (resolving an open problem mentioned by [Babichenko-Rubinstein'2017, Goos-Rubinstein'2018, Ganor-CS'2018]; and an $\tilde{O}(n)$-query algorithm for $\varepsilon$-CE (resolving an open problem of [Babichenko'2020] and obtaining the first separation between $\varepsilon$-CE and $\varepsilon$-Nash equilibrium in the query complexity model).   For extensive-form games, our algorithm implies a PTAS for $\mathit{normal}$ $\mathit{form}$ $\mathit{correlated}$ $\mathit{equilibria}$, a solution concept often conjectured to be computationally intractable (e.g. [Stengel-Forges'08, Fujii'23]).
</details>
<details>
<summary>摘要</summary>
我们提供了一个简单而计算效率高的算法，它可以在任何常数 $\varepsilon>0$ 下获得 $\varepsilon T$-交换失落，并且只需要 $T = \mathcal{O}(\log^c(n))$ 轮次，这是一个对比原始算法的 exponential 提高，并解决了 [Blum 和 Mansour 2007] 中的主要开放问题。我们的算法具有对 $\varepsilon$ 的几何依赖，但我们证明了一个新的匹配下界。我们的交换失落算法 imply 在一些场景中更快地达到 $\varepsilon$-相关平衡（$\varepsilon$-CE）：1. 正常形二人游戏中，我们的算法可以在 $\mathcal{O}(\log^c(n))$ 轮次内达到 $\varepsilon$-CE 的集合，这是第一个解耦的演化过程。2. 我们可以实现 $\mathsf{polylog}(n)$-位寄存器协议来实现 $\varepsilon$-CE，解决了 [Babichenko-Rubinstein 2017, Goos-Rubinstein 2018, Ganor-CS 2018] 中的开放问题。3. 我们可以实现 $\tilde{O}(n)$-询问算法来实现 $\varepsilon$-CE，解决了 [Babichenko 2020] 中的开放问题，并且获得了首次对 $\varepsilon$-CE 和 $\varepsilon$-尼亚希平衡（$\varepsilon$-NE）之间的分离。对于扩展形游戏，我们的算法 imply 一种 PTAS  для $\mathit{normal}$ $\mathit{form}$ $\mathit{相关}$ $\mathit{平衡}$，这是一个常 conjectured 是计算复杂的解决方案（例如 [Stengel-Forges 08, Fujii 23]）。
</details></li>
</ul>
<hr>
<h2 id="RayDF-Neural-Ray-surface-Distance-Fields-with-Multi-view-Consistency"><a href="#RayDF-Neural-Ray-surface-Distance-Fields-with-Multi-view-Consistency" class="headerlink" title="RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency"></a>RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19629">http://arxiv.org/abs/2310.19629</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vlar-group/raydf">https://github.com/vlar-group/raydf</a></li>
<li>paper_authors: Zhuoman Liu, Bo Yang</li>
<li>For: This paper addresses the problem of continuous 3D shape representations, and proposes a new framework called RayDF to improve the efficiency and accuracy of 3D shape representation.* Methods: The proposed RayDF framework consists of three components: 1) simple ray-surface distance field, 2) novel dual-ray visibility classifier, and 3) multi-view consistency optimization module.* Results: The proposed method achieves remarkable performance in 3D surface point reconstruction on both synthetic and real-world 3D scenes, with a 1000x faster speed than coordinate-based methods to render an 800x800 depth image.Here is the summary in Traditional Chinese:* For: 本文研究矩阵3D形状表示的问题，并提出了一个新的框架 called RayDF，以提高3D形状表示的效率和准确性。* Methods: RayDF框架包括三个 ком成分：1）简单的光条-表面距离场，2）新的双光条可见分类器，3）多视角协力优化模块。* Results: 提出的方法在实验中 achieve remarkable performance in 3D surface point reconstruction，并且比coordinate-based方法更快速地显示出800x800深度图像，表明了方法的superiority。<details>
<summary>Abstract</summary>
In this paper, we study the problem of continuous 3D shape representations. The majority of existing successful methods are coordinate-based implicit neural representations. However, they are inefficient to render novel views or recover explicit surface points. A few works start to formulate 3D shapes as ray-based neural functions, but the learned structures are inferior due to the lack of multi-view geometry consistency. To tackle these challenges, we propose a new framework called RayDF. It consists of three major components: 1) the simple ray-surface distance field, 2) the novel dual-ray visibility classifier, and 3) a multi-view consistency optimization module to drive the learned ray-surface distances to be multi-view geometry consistent. We extensively evaluate our method on three public datasets, demonstrating remarkable performance in 3D surface point reconstruction on both synthetic and challenging real-world 3D scenes, clearly surpassing existing coordinate-based and ray-based baselines. Most notably, our method achieves a 1000x faster speed than coordinate-based methods to render an 800x800 depth image, showing the superiority of our method for 3D shape representation. Our code and data are available at https://github.com/vLAR-group/RayDF
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了连续3D形状表示的问题。现有大多数成功方法都是基于坐标的卷积神经表示。然而，它们在渲染新视图或者恢复明确的表面点时效率低下。一些工作开始将3D形状表示为射线基的神经函数，但学习结构因为缺乏多视图几何一致性而受到限制。为了解决这些挑战，我们提出了一个新的框架called RayDF。它包括三个主要组件：1）简单的射线-表面距离场，2）新的双射线可见分类器，和3）多视图一致性优化模块，以使学习的射线-表面距离在多视图几何上保持一致。我们对三个公共数据集进行了广泛的评估，并证明了我们的方法在 sintetic和实际世界中的3D表面点重建任务中显著的表现，明显超过了坐标基于和射线基于的基elines。尤其是，我们的方法可以在coordinate-based方法的1000倍速度下渲染800x800深度图像，表明我们的方法在3D形状表示方面的优势。我们的代码和数据可以在https://github.com/vLAR-group/RayDF上获取。
</details></li>
</ul>
<hr>
<h2 id="Transformation-vs-Tradition-Artificial-General-Intelligence-AGI-for-Arts-and-Humanities"><a href="#Transformation-vs-Tradition-Artificial-General-Intelligence-AGI-for-Arts-and-Humanities" class="headerlink" title="Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities"></a>Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19626">http://arxiv.org/abs/2310.19626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengliang Liu, Yiwei Li, Qian Cao, Junwen Chen, Tianze Yang, Zihao Wu, John Hale, John Gibbs, Khaled Rasheed, Ninghao Liu, Gengchen Mai, Tianming Liu</li>
<li>for: 这篇论文旨在探讨人工全面智能（AGI）在文化领域的应用和影响。</li>
<li>methods: 论文使用了现代大语言模型和创新性图像生成系统，对文学、历史、市场、电影等领域进行了分析和评估。</li>
<li>results: 论文发现了AGI系统在文化领域的应用存在几个关键问题，如真实性、毒性、偏见和公共安全等问题，并提出了缓解策略。论文强调了多方合作来确保AGI系统推动创造力、知识和文化价值，而不是威胁真实性或人类尊严。<details>
<summary>Abstract</summary>
Recent advances in artificial general intelligence (AGI), particularly large language models and creative image generation systems have demonstrated impressive capabilities on diverse tasks spanning the arts and humanities. However, the swift evolution of AGI has also raised critical questions about its responsible deployment in these culturally significant domains traditionally seen as profoundly human. This paper provides a comprehensive analysis of the applications and implications of AGI for text, graphics, audio, and video pertaining to arts and the humanities. We survey cutting-edge systems and their usage in areas ranging from poetry to history, marketing to film, and communication to classical art. We outline substantial concerns pertaining to factuality, toxicity, biases, and public safety in AGI systems, and propose mitigation strategies. The paper argues for multi-stakeholder collaboration to ensure AGI promotes creativity, knowledge, and cultural values without undermining truth or human dignity. Our timely contribution summarizes a rapidly developing field, highlighting promising directions while advocating for responsible progress centering on human flourishing. The analysis lays the groundwork for further research on aligning AGI's technological capacities with enduring social goods.
</details>
<details>
<summary>摘要</summary>
This paper provides a comprehensive analysis of the applications and implications of AGI in the fields of text, graphics, audio, and video in the arts and humanities. We survey cutting-edge systems and their use in areas such as poetry, history, marketing, film, and communication, as well as classical art. We also highlight significant concerns related to factuality, toxicity, biases, and public safety in AGI systems, and propose strategies for mitigating these issues.The paper argues for multi-stakeholder collaboration to ensure that AGI promotes creativity, knowledge, and cultural values while upholding truth and human dignity. Our analysis lays the groundwork for further research on aligning AGI's technological capacities with enduring social goods, and advocates for responsible progress that centers on human flourishing.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Post-Training-Quantization-of-Protein-Language-Models"><a href="#Exploring-Post-Training-Quantization-of-Protein-Language-Models" class="headerlink" title="Exploring Post-Training Quantization of Protein Language Models"></a>Exploring Post-Training Quantization of Protein Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19624">http://arxiv.org/abs/2310.19624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Peng, Fei Yang, Ning Sun, Sheng Chen, Yanfeng Jiang, Aimin Pan</li>
<li>For: This paper aims to improve the efficiency of protein language models (ProteinLMs) by developing a post-training quantization (PTQ) method that can accurately quantize all weights and activations of ProteinLMs without compromising accuracy.* Methods: The proposed PTQ method uses piecewise linear quantization for asymmetric activation values to ensure accurate approximation, addressing specific challenges associated with ESMFold, a simplified version of AlphaFold based on ESM-2 ProteinLM.* Results: The proposed method was demonstrated to be effective in protein structure prediction tasks, showing that ESMFold can be accurately quantized to low-bit widths without compromising accuracy. Additionally, the method was applied to the contact prediction task, showcasing its versatility.<details>
<summary>Abstract</summary>
Recent advancements in unsupervised protein language models (ProteinLMs), like ESM-1b and ESM-2, have shown promise in different protein prediction tasks. However, these models face challenges due to their high computational demands, significant memory needs, and latency, restricting their usage on devices with limited resources. To tackle this, we explore post-training quantization (PTQ) for ProteinLMs, focusing on ESMFold, a simplified version of AlphaFold based on ESM-2 ProteinLM. Our study is the first attempt to quantize all weights and activations of ProteinLMs. We observed that the typical uniform quantization method performs poorly on ESMFold, causing a significant drop in TM-Score when using 8-bit quantization. We conducted extensive quantization experiments, uncovering unique challenges associated with ESMFold, particularly highly asymmetric activation ranges before Layer Normalization, making representation difficult using low-bit fixed-point formats. To address these challenges, we propose a new PTQ method for ProteinLMs, utilizing piecewise linear quantization for asymmetric activation values to ensure accurate approximation. We demonstrated the effectiveness of our method in protein structure prediction tasks, demonstrating that ESMFold can be accurately quantized to low-bit widths without compromising accuracy. Additionally, we applied our method to the contact prediction task, showcasing its versatility. In summary, our study introduces an innovative PTQ method for ProteinLMs, addressing specific quantization challenges and potentially leading to the development of more efficient ProteinLMs with significant implications for various protein-related applications.
</details>
<details>
<summary>摘要</summary>
We found that the typical uniform quantization method performs poorly on ESMFold, resulting in a significant drop in TM-Score when using 8-bit quantization. To address this challenge, we proposed a new PTQ method for ProteinLMs that utilizes piecewise linear quantization for asymmetric activation values to ensure accurate approximation. We demonstrated the effectiveness of our method in protein structure prediction tasks, showing that ESMFold can be accurately quantized to low-bit widths without compromising accuracy. Additionally, we applied our method to the contact prediction task, demonstrating its versatility.Our study introduces an innovative PTQ method for ProteinLMs that addresses specific quantization challenges and has the potential to lead to the development of more efficient ProteinLMs with significant implications for various protein-related applications.
</details></li>
</ul>
<hr>
<h2 id="Large-Trajectory-Models-are-Scalable-Motion-Predictors-and-Planners"><a href="#Large-Trajectory-Models-are-Scalable-Motion-Predictors-and-Planners" class="headerlink" title="Large Trajectory Models are Scalable Motion Predictors and Planners"></a>Large Trajectory Models are Scalable Motion Predictors and Planners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19620">http://arxiv.org/abs/2310.19620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-mars-lab/statetransformer">https://github.com/tsinghua-mars-lab/statetransformer</a></li>
<li>paper_authors: Qiao Sun, Shiduo Zhang, Danjiao Ma, Jingzhe Shi, Derun Li, Simian Luo, Yu Wang, Ningyi Xu, Guangzhi Cao, Hang Zhao</li>
<li>for: 本研究旨在提出一种可扩展的路径模型（State Transformer，STR），用于驱动自动驾驶中的动作预测和规划问题。</li>
<li>methods: STR 通过将观察、状态和动作排序成一个简单的序列模型， reformulate 了动作预测和规划问题。STR 的简单设计和可扩展性，在两个问题中都能够经常超越基线方法。</li>
<li>results: 实验结果表明，大型路径模型（LTM），如 STR，遵循缩放法律，并表现出扩展性和学习效率的出色表现。质量结果还显示， LTM 能够在训练数据分布不同的情况下做出可能的预测，并且学习长期规划，不需要显式损失函数或高级标注。<details>
<summary>Abstract</summary>
Motion prediction and planning are vital tasks in autonomous driving, and recent efforts have shifted to machine learning-based approaches. The challenges include understanding diverse road topologies, reasoning traffic dynamics over a long time horizon, interpreting heterogeneous behaviors, and generating policies in a large continuous state space. Inspired by the success of large language models in addressing similar complexities through model scaling, we introduce a scalable trajectory model called State Transformer (STR). STR reformulates the motion prediction and motion planning problems by arranging observations, states, and actions into one unified sequence modeling task. With a simple model design, STR consistently outperforms baseline approaches in both problems. Remarkably, experimental results reveal that large trajectory models (LTMs), such as STR, adhere to the scaling laws by presenting outstanding adaptability and learning efficiency. Qualitative results further demonstrate that LTMs are capable of making plausible predictions in scenarios that diverge significantly from the training data distribution. LTMs also learn to make complex reasonings for long-term planning, without explicit loss designs or costly high-level annotations.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译为简化字符串。<</SYS>>自动驾驶中的运动预测和规划是关键任务，现在的努力都是转向机器学习基础的方法。挑战包括理解多样化的公路地貌，理解交通流动的长期发展，理解不同类型的行为，并在大规模状态空间中生成策略。受大语言模型在类似复杂性方面的成功启发，我们引入了可扩展的轨迹模型（State Transformer，STR）。STR将运动预测和规划问题重新排序为一个简单的序列模型任务。与基eline方法相比，STR在两个问题中具有稳定的表现。特别是，实验结果表明，大轨迹模型（LTM），如STR，遵循协调规律，表现出杰出的适应性和学习效率。Qualitative结果还表明，LTM可以在训练数据分布外的场景中做出合理的预测，而无需详细的损失函数设计或高级注释。
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Holistic-Landscape-of-Situated-Theory-of-Mind-in-Large-Language-Models"><a href="#Towards-A-Holistic-Landscape-of-Situated-Theory-of-Mind-in-Large-Language-Models" class="headerlink" title="Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models"></a>Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19619">http://arxiv.org/abs/2310.19619</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mars-tin/awesome-theory-of-mind">https://github.com/mars-tin/awesome-theory-of-mind</a></li>
<li>paper_authors: Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai</li>
<li>for: 本研究旨在提供一种更全面和可靠的评估方法来评估机器学习模型（LLMs）的理论心（ToM）能力。</li>
<li>methods: 该研究使用了心理学研究中的分类方法，将机器ToM分为7种心态类别，并对现有的评估准则进行分析，以找出尚未被探讨的ToM方面。</li>
<li>results: 研究人员在一个网格世界设置中进行了一个证明性研究，以证明 situated evaluation 可以更好地评估机器的 ToM 能力，并减少了短cut和数据泄露的风险。Here is the same information in Simplified Chinese text:</li>
<li>for: 本研究旨在提供一种更全面和可靠的评估方法来评估机器学习模型（LLMs）的理论心（ToM）能力。</li>
<li>methods: 该研究使用了心理学研究中的分类方法，将机器ToM分为7种心态类别，并对现有的评估准则进行分析，以找出尚未被探讨的ToM方面。</li>
<li>results: 研究人员在一个网格世界设置中进行了一个证明性研究，以证明 situated evaluation 可以更好地评估机器的 ToM 能力，并减少了短cut和数据泄露的风险。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage. In this position paper, we seek to answer two road-blocking questions: (1) How can we taxonomize a holistic landscape of machine ToM? (2) What is a more effective evaluation protocol for machine ToM? Following psychological studies, we taxonomize machine ToM into 7 mental state categories and delineate existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM to break ToM into individual components and treat LLMs as an agent who is physically situated in environments and socially situated in interactions with humans. Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage. We further present a pilot study in a grid world setup as a proof of concept. We hope this position paper can facilitate future research to integrate ToM with LLMs and offer an intuitive means for researchers to better position their work in the landscape of ToM. Project page: https://github.com/Mars-tin/awesome-theory-of-mind
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已引起广泛的关注和讨论，关于它们是否会发展出理论心（ToM）的潜在能力。一些最近的调查表明，目前的LLM Models在ToM方面存在强度不足的问题，需要开发新的评价协议，因为现有的评价方法主要关注了ToM的不同方面，容易出现偏导和数据泄露问题。在这份Position paper中，我们寻求回答以下两个障碍问题：1. 如何分类机器人的整体ToM领域？2. 如何设计更有效的机器人ToM评价协议？根据心理学研究，我们将机器人ToM分类为7种心理状态类别，并将现有的评价方法进行分类，以找出尚未得到足够关注的ToM方面。我们 argueThat a comprehensive and situated evaluation of ToM is necessary to break ToM into its individual components and treat LLMs as physically and socially situated agents in environments and interactions with humans. Such situated evaluation can provide a more comprehensive assessment of mental states and potentially mitigate the risk of shortcuts and data leakage. We further present a pilot study in a grid world setup as a proof of concept. We hope this position paper can facilitate future research to integrate ToM with LLMs and offer an intuitive means for researchers to better position their work in the landscape of ToM.项目页面：https://github.com/Mars-tin/awesome-theory-of-mind
</details></li>
</ul>
<hr>
<h2 id="Technical-Report-on-the-Learning-of-Case-Relevance-in-Case-Based-Reasoning-with-Abstract-Argumentation"><a href="#Technical-Report-on-the-Learning-of-Case-Relevance-in-Case-Based-Reasoning-with-Abstract-Argumentation" class="headerlink" title="Technical Report on the Learning of Case Relevance in Case-Based Reasoning with Abstract Argumentation"></a>Technical Report on the Learning of Case Relevance in Case-Based Reasoning with Abstract Argumentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19607">http://arxiv.org/abs/2310.19607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GPPassos/learning-relevance-aacbr-technical-report">https://github.com/GPPassos/learning-relevance-aacbr-technical-report</a></li>
<li>paper_authors: Guilherme Paulino-Passos, Francesca Toni</li>
<li>for: This paper focuses on using case-based reasoning and abstract argumentation to improve the prediction of legal outcomes.</li>
<li>methods: The paper uses decision trees to learn the relevance of cases and combines case-based reasoning with abstract argumentation to make predictions.</li>
<li>results: The authors show that the proposed approach performs competitively with decision trees and results in a more compact representation, which could be beneficial for obtaining cognitively tractable explanations.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文关注使用情况基本reasoning和抽象论证来改进法律结果预测。</li>
<li>methods: 论文使用决策树学习情况中的相关性，并将情况基本reasoning与抽象论证结合以进行预测。</li>
<li>results: 作者们表明，提议的方法与决策树相比，在两个法律数据集上表现竞争性，并且生成了更加紧凑的表示，可能有助于获得更加容易理解的解释。<details>
<summary>Abstract</summary>
Case-based reasoning is known to play an important role in several legal settings. In this paper we focus on a recent approach to case-based reasoning, supported by an instantiation of abstract argumentation whereby arguments represent cases and attack between arguments results from outcome disagreement between cases and a notion of relevance. In this context, relevance is connected to a form of specificity among cases. We explore how relevance can be learnt automatically in practice with the help of decision trees, and explore the combination of case-based reasoning with abstract argumentation (AA-CBR) and learning of case relevance for prediction in legal settings. Specifically, we show that, for two legal datasets, AA-CBR and decision-tree-based learning of case relevance perform competitively in comparison with decision trees. We also show that AA-CBR with decision-tree-based learning of case relevance results in a more compact representation than their decision tree counterparts, which could be beneficial for obtaining cognitively tractable explanations.
</details>
<details>
<summary>摘要</summary>
Case-based reasoning 在法律设置中扮演着重要角色。在这篇论文中，我们关注一种最近的case-based reasoning方法，基于抽象论证的实现，其中Arguments代表案例，Arguments之间的攻击 originates from outcome disagreement between cases and a notion of relevance。在这种情况下， relevance 与特定的案例之间的相似性相连。我们研究如何在实践中自动学习case relevance，并探讨 AA-CBR 和学习案例相关性的组合用于预测法律设置中。具体来说，我们显示，对于两个法律数据集，AA-CBR 和基于决策树的学习案例相关性能与决策树相比竞争，而且 AA-CBR 与决策树学习案例相关性后得到的表示更加紧凑，这可能有助于获得更加容易理解的解释。
</details></li>
</ul>
<hr>
<h2 id="LLMaAA-Making-Large-Language-Models-as-Active-Annotators"><a href="#LLMaAA-Making-Large-Language-Models-as-Active-Annotators" class="headerlink" title="LLMaAA: Making Large Language Models as Active Annotators"></a>LLMaAA: Making Large Language Models as Active Annotators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19596">http://arxiv.org/abs/2310.19596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ridiculouz/LLMaAA">https://github.com/ridiculouz/LLMaAA</a></li>
<li>paper_authors: Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, Lei Zou</li>
<li>for: 本研究旨在将大语言模型（LLMs）训练为实际应用中的自然语言处理（NLP）任务，并且实现高效的标签生成。</li>
<li>methods: 本研究使用了LLMs作为标签生成的annotator，并将其置入到了活动学习 Loop中，以进行有效的标签生成。</li>
<li>results: 在两个 класи级NLP任务中， LLMaAA 可以实现高效的标签生成，并且可以在只需百个标签示例下，训练任务特定的模型，并且超越其他基eline。<details>
<summary>Abstract</summary>
Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.
</details>
<details>
<summary>摘要</summary>
通常的监督学习方法在自然语言处理（NLP）领域具有著名的数据占用问题，需要大量高质量标注数据来训练。在实践中，获取这些数据是一件昂贵的困难任务。在最近，大型自然语言模型（LLM）的优秀几个shot性能的发展使得数据生成技术得到了更多的关注，其中的数据主要通过LLM来生成。然而，这种方法通常受到低质量问题的困扰，需要数量级更多的标注数据来达到满意性。为了充分利用LLM的潜力并使用庞大的未标注数据，我们提出了LLMaAA，它将LLM作为标注者，并将其置入到活动学习循环中，以确定如何有效地标注。为了学习Robustly，我们在标注和训练过程中进行优化：（1）我们从小示例池中随机选择k nearest neighbors（k-NN）的示例作为上下文示例，并（2）采用示例权重技术，将训练样本分配学习型的权重。相比之下，LLMaAA具有高效和可靠的特点。我们在两个 класси型NLP任务中进行实验和分析，结果显示，通过LLMaAA训练基于LLM生成的标签的任务特定模型，可以在只需百个标注示例的情况下，超越教师模型，这是其他基准之下更加经济的。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Locally-Stationary-Data-Using-Expert-Advice"><a href="#Prediction-of-Locally-Stationary-Data-Using-Expert-Advice" class="headerlink" title="Prediction of Locally Stationary Data Using Expert Advice"></a>Prediction of Locally Stationary Data Using Expert Advice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19591">http://arxiv.org/abs/2310.19591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vladimir V’yugin, Vladimir Trunov</li>
<li>for: 这篇论文研究了连续机器学习的问题，特别是在游戏理论方法的框架下进行计算。</li>
<li>methods: 该论文使用了游戏理论方法，不假设数据源的随机性，而是基于数据流的结构假设。</li>
<li>results: 论文提出了一种在线预测算法，可以用于处理本地站点时间序列。此外，论文还得到了这种算法的效率估计。<details>
<summary>Abstract</summary>
The problem of continuous machine learning is studied. Within the framework of the game-theoretic approach, when for calculating the next forecast, no assumptions about the stochastic nature of the source that generates the data flow are used -- the source can be analog, algorithmic or probabilistic, its parameters can change at random times, when building a prognostic model, only structural assumptions are used about the nature of data generation. An online forecasting algorithm for a locally stationary time series is presented. An estimate of the efficiency of the proposed algorithm is obtained.
</details>
<details>
<summary>摘要</summary>
“continuous machine learning”问题被研究。在游戏理论方法框架下，对于计算下一个预测时，不假设数据源具有测度的性质，数据源可以是杂音、算法或概率的，数据源的参数可以在随机时间变化，在建立预测模型时，只假设数据生成的结构。向来自本地站点时间序列的在线预测算法被提出。对提出的算法的效率估计得到。Note: "continuous machine learning" is not a direct translation of the English phrase "continuous machine learning", but rather a translation of the concept it represents. In Simplified Chinese, "continuous" is often translated as "连续" (lián xù), but in this context, "连续机器学习" (lián xù jī shù) is used to emphasize the continuous nature of the learning process.
</details></li>
</ul>
<hr>
<h2 id="CreoleVal-Multilingual-Multitask-Benchmarks-for-Creoles"><a href="#CreoleVal-Multilingual-Multitask-Benchmarks-for-Creoles" class="headerlink" title="CreoleVal: Multilingual Multitask Benchmarks for Creoles"></a>CreoleVal: Multilingual Multitask Benchmarks for Creoles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19567">http://arxiv.org/abs/2310.19567</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hclent/creoleval">https://github.com/hclent/creoleval</a></li>
<li>paper_authors: Heather Lent, Kushal Tatariya, Raj Dabre, Yiyi Chen, Marcell Fekete, Esther Ploeger, Li Zhou, Hans Erik Heje, Diptesh Kanojia, Paul Belony, Marcel Bollmann, Loïc Grobol, Miryam de Lhoneux, Daniel Hershcovich, Michel DeGraff, Anders Søgaard, Johannes Bjerva</li>
<li>for: 这个论文的目的是为了提供一个对Creole语言的NLP研究提供资源，以便更好地包括这些语言在计算语言学和自然语言处理领域中。</li>
<li>methods: 本文使用了多种NLP任务的基准数据集，包括机器理解、关系分类和机器翻译等，并在零shot设定下进行了基准实验，以评估将其他语言的资源传递到Creole语言上的能力和局限性。</li>
<li>results: 本文提供了8种NLP任务的benchmark数据集，覆盖28种Creole语言，并为每个任务进行了零shot基准实验，以更好地了解Creole语言在NLP领域的能力和局限性。<details>
<summary>Abstract</summary>
Creoles represent an under-explored and marginalized group of languages, with few available resources for NLP research. While the genealogical ties between Creoles and other highly-resourced languages imply a significant potential for transfer learning, this potential is hampered due to this lack of annotated data. In this work we present CreoleVal, a collection of benchmark datasets spanning 8 different NLP tasks, covering up to 28 Creole languages; it is an aggregate of brand new development datasets for machine comprehension, relation classification, and machine translation for Creoles, in addition to a practical gateway to a handful of preexisting benchmarks. For each benchmark, we conduct baseline experiments in a zero-shot setting in order to further ascertain the capabilities and limitations of transfer learning for Creoles. Ultimately, the goal of CreoleVal is to empower research on Creoles in NLP and computational linguistics. We hope this resource will contribute to technological inclusion for Creole language users around the globe.
</details>
<details>
<summary>摘要</summary>
创护语言表示一个尚未得到足够探索和受排斥的语言群体，有很少的可用资源 для NLP研究。尽管创护语言与其他高度资源化语言之间的 généalogique 关系 imply 一个显著的潜在 transferred learning 潜力，但这种潜力受到缺乏精心标注数据的限制。在这项工作中，我们介绍 CreoleVal，一个收录8种 NLP任务的benchmark集合，覆盖28种创护语言，包括新开发的机器理解、关系分类和机器翻译 benchmarks 以及一些现有的benchmarks。对于每个benchmark，我们进行了零批设置的基eline实验，以更好地了解创护语言在 transferred learning 中的能力和局限性。最终，CreoleVal的目标是促进创护语言在 NLP和计算语言科学中的研究，并为全球各地的创护语言用户提供技术包容。
</details></li>
</ul>
<hr>
<h2 id="A-General-Neural-Causal-Model-for-Interactive-Recommendation"><a href="#A-General-Neural-Causal-Model-for-Interactive-Recommendation" class="headerlink" title="A General Neural Causal Model for Interactive Recommendation"></a>A General Neural Causal Model for Interactive Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19519">http://arxiv.org/abs/2310.19519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Liu, Xinyan Su, Peng Zhou, Xiangyu Zhao, Jun Li</li>
<li>for: 这篇论文旨在 Mitigating survivor bias in observational data for optimizing recommender systems.</li>
<li>methods: 该论文提出了一种基于神经网络 causal model 的方法，通过 counterfactual inference 来解决 survivor bias 问题。</li>
<li>results:  both theoretical and empirical studies demonstrate the effectiveness of the proposed solution.<details>
<summary>Abstract</summary>
Survivor bias in observational data leads the optimization of recommender systems towards local optima. Currently most solutions re-mines existing human-system collaboration patterns to maximize longer-term satisfaction by reinforcement learning. However, from the causal perspective, mitigating survivor effects requires answering a counterfactual problem, which is generally unidentifiable and inestimable. In this work, we propose a neural causal model to achieve counterfactual inference. Specifically, we first build a learnable structural causal model based on its available graphical representations which qualitatively characterizes the preference transitions. Mitigation of the survivor bias is achieved though counterfactual consistency. To identify the consistency, we use the Gumbel-max function as structural constrains. To estimate the consistency, we apply reinforcement optimizations, and use Gumbel-Softmax as a trade-off to get a differentiable function. Both theoretical and empirical studies demonstrate the effectiveness of our solution.
</details>
<details>
<summary>摘要</summary>
Note:* "Survivor bias" is translated as "生存者偏见" (shēng zhì zhēng yǎn)* "Observational data" is translated as "观察数据" (guān cháng shù dài)* "Recommender systems" is translated as "推荐系统" (tuī yù xì tǒng)* "Local optima" is translated as "本地最优" (ben dì zuì yōu)* "Counterfactual problem" is translated as "Counterfactual问题" (fǎng yì wèn tí)* "Gumbel-max function" is translated as "Gumbel-max函数" (Gumbel-max fāng xìng)* "Structural constrains" is translated as "结构约束" (jiégòu yāo xiǎng)* "Reinforcement optimizations" is translated as "强化优化" (qiáng huà yóu huà)* "Gumbel-Softmax" is translated as "Gumbel-Softmax" (Gumbel-Softmax)
</details></li>
</ul>
<hr>
<h2 id="Inverse-folding-for-antibody-sequence-design-using-deep-learning"><a href="#Inverse-folding-for-antibody-sequence-design-using-deep-learning" class="headerlink" title="Inverse folding for antibody sequence design using deep learning"></a>Inverse folding for antibody sequence design using deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19513">http://arxiv.org/abs/2310.19513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frédéric A. Dreyer, Daniel Cutting, Constantin Schneider, Henry Kenlay, Charlotte M. Deane</li>
<li>for: 蛋白质结构设计问题</li>
<li>methods: 精确的逆折衣模型和物理学基础方法</li>
<li>results: 提高适应性和结构稳定性，尤其是在制做CDR-H3螺旋中Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written for the problem of designing antibody sequences based on 3D structural information.</li>
<li>methods: The paper proposes a fine-tuned inverse folding model that is specifically optimized for antibody structures, and uses physics-based methods to evaluate the quality of proposed sequences.</li>
<li>results: The paper shows that the proposed model outperforms generic protein models on sequence recovery and structure robustness when applied to antibodies, with notable improvement on the hypervariable CDR-H3 loop. Additionally, the paper studies the canonical conformations of complementarity-determining regions and finds improved encoding of these loops into known clusters.<details>
<summary>Abstract</summary>
We consider the problem of antibody sequence design given 3D structural information. Building on previous work, we propose a fine-tuned inverse folding model that is specifically optimised for antibody structures and outperforms generic protein models on sequence recovery and structure robustness when applied on antibodies, with notable improvement on the hypervariable CDR-H3 loop. We study the canonical conformations of complementarity-determining regions and find improved encoding of these loops into known clusters. Finally, we consider the applications of our model to drug discovery and binder design and evaluate the quality of proposed sequences using physics-based methods.
</details>
<details>
<summary>摘要</summary>
我们考虑了抗体序列设计问题，givien 3D结构信息。基于先前的工作，我们提出了特制 inverse folding 模型，可以特地优化 для抗体结构，在抗体序列恢复和结构稳定性方面表现出色，特别是在 CDR-H3 征 loop 中。我们研究了供应 chain 的常见 conformations 和 encoding 这些征 loop 到已知的群体。最后，我们考虑了我们模型在药物发现和绑定设计中的应用，并使用物理学基的方法评估提出的序列质量。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SparseByteNN-A-Novel-Mobile-Inference-Acceleration-Framework-Based-on-Fine-Grained-Group-Sparsity"><a href="#SparseByteNN-A-Novel-Mobile-Inference-Acceleration-Framework-Based-on-Fine-Grained-Group-Sparsity" class="headerlink" title="SparseByteNN: A Novel Mobile Inference Acceleration Framework Based on Fine-Grained Group Sparsity"></a>SparseByteNN: A Novel Mobile Inference Acceleration Framework Based on Fine-Grained Group Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19509">http://arxiv.org/abs/2310.19509</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lswzjuer/SparseByteNN">https://github.com/lswzjuer/SparseByteNN</a></li>
<li>paper_authors: Haitao Xu, Songwei Liu, Yuyang Xu, Shuai Wang, Jiashi Li, Chenqian Yan, Liangqiang Li, Lean Fu, Xin Pan, Fangmin Chen</li>
<li>for: 这个研究旨在提高网络大小时的测试速度和精度。</li>
<li>methods: 本研究使用精确的kernel终端减少来实现实时执行和高精度。它包括两个部分：（a）精确的kernel终端减少架构，包括多个不同的簇终端减少模式，并与我们提出的网络重新排序策略，实现了高压缩率和高精度。（b）特化于缓冲终端减少的推断引擎。</li>
<li>results: 实验结果显示，对于30%簇的MobileNet-v1，SparseByteNN可以与紧密的终端版本和现有的缓冲推断引擎MNN进行比较，在效率-精度曲线上表现出superiority。具体来说，SparseByteNN在Qualcomm 855上实现了1.27倍的速度提升和1.29倍的效率提升，仅有0.224%的精度下降。<details>
<summary>Abstract</summary>
To address the challenge of increasing network size, researchers have developed sparse models through network pruning. However, maintaining model accuracy while achieving significant speedups on general computing devices remains an open problem. In this paper, we present a novel mobile inference acceleration framework SparseByteNN, which leverages fine-grained kernel sparsity to achieve real-time execution as well as high accuracy. Our framework consists of two parts: (a) A fine-grained kernel sparsity schema with a sparsity granularity between structured pruning and unstructured pruning. It designs multiple sparse patterns for different operators. Combined with our proposed whole network rearrangement strategy, the schema achieves a high compression rate and high precision at the same time. (b) Inference engine co-optimized with the sparse pattern. The conventional wisdom is that this reduction in theoretical FLOPs does not translate into real-world efficiency gains. We aim to correct this misconception by introducing a family of efficient sparse kernels for ARM and WebAssembly. Equipped with our efficient implementation of sparse primitives, we show that sparse versions of MobileNet-v1 outperform strong dense baselines on the efficiency-accuracy curve. Experimental results on Qualcomm 855 show that for 30% sparse MobileNet-v1, SparseByteNN achieves 1.27x speedup over the dense version and 1.29x speedup over the state-of-the-art sparse inference engine MNN with a slight accuracy drop of 0.224%. The source code of SparseByteNN will be available at https://github.com/lswzjuer/SparseByteNN
</details>
<details>
<summary>摘要</summary>
为了解决网络大小的挑战，研究人员已经开发出了稀盐模型通过网络剪切。然而，在普通计算设备上实现重要的速度增加并保持模型准确性 remain an open problem。在这篇论文中，我们提出了一种新的移动推理加速框架SparseByteNN，它利用细化的kernel稀盐来实现实时执行以及高准确性。我们的框架包括两部分：(a) 细化kernel稀盐schema，其中的稀盐粒度位于结构剪切和无结构剪切之间。它设计了多种不同的稀盐模式，并结合我们的提出的整个网络重新排序策略，实现了高压缩率和高精度同时。(b) 推理引擎与稀盐模式相似。传统的观点是，这种理论的FLOP减少不会在实际情况中带来实用性提升。我们希望通过引入一家高效的稀盐kernel家族，证明这个观点是错误的。我们的高效实现稀盐基本操作，使得稀盐版本的MobileNet-v1在效率-准确度曲线上表现出色，超过了dense基eline和state-of-the-art sparse推理引擎MNN的速度。实验结果表明，在Qualcomm 855上，为30%稀盐的MobileNet-v1，SparseByteNN可以与dense版本相比，提高1.27倍的速度，同时与MNN相比，提高1.29倍的速度，但是略有准确性下降（0.224%）。SparseByteNN的源代码将在https://github.com/lswzjuer/SparseByteNN上发布。
</details></li>
</ul>
<hr>
<h2 id="Trust-Accountability-and-Autonomy-in-Knowledge-Graph-based-AI-for-Self-determination"><a href="#Trust-Accountability-and-Autonomy-in-Knowledge-Graph-based-AI-for-Self-determination" class="headerlink" title="Trust, Accountability, and Autonomy in Knowledge Graph-based AI for Self-determination"></a>Trust, Accountability, and Autonomy in Knowledge Graph-based AI for Self-determination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19503">http://arxiv.org/abs/2310.19503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis-Daniel Ibáñez, John Domingue, Sabrina Kirrane, Oshani Seneviratne, Aisling Third, Maria-Esther Vidal</li>
<li>for: This paper is written to address the issue of self-determination in the context of the growing use of Knowledge Graphs (KGs) and Artificial Intelligence (AI) in online services.</li>
<li>methods: The paper uses a conceptual framework to explore the foundational topics and research pillars needed to support KG-based AI for self-determination, and analyzes challenges and opportunities for citizen self-determination in a real-world scenario.</li>
<li>results: The paper proposes a research agenda aimed at accomplishing the recommended objectives, including ensuring the trustworthiness of AI systems, transparency in data and inner workings, and accountability for decision-making.<details>
<summary>Abstract</summary>
Knowledge Graphs (KGs) have emerged as fundamental platforms for powering intelligent decision-making and a wide range of Artificial Intelligence (AI) services across major corporations such as Google, Walmart, and AirBnb. KGs complement Machine Learning (ML) algorithms by providing data context and semantics, thereby enabling further inference and question-answering capabilities. The integration of KGs with neuronal learning (e.g., Large Language Models (LLMs)) is currently a topic of active research, commonly named neuro-symbolic AI. Despite the numerous benefits that can be accomplished with KG-based AI, its growing ubiquity within online services may result in the loss of self-determination for citizens as a fundamental societal issue. The more we rely on these technologies, which are often centralised, the less citizens will be able to determine their own destinies. To counter this threat, AI regulation, such as the European Union (EU) AI Act, is being proposed in certain regions. The regulation sets what technologists need to do, leading to questions concerning: How can the output of AI systems be trusted? What is needed to ensure that the data fuelling and the inner workings of these artefacts are transparent? How can AI be made accountable for its decision-making? This paper conceptualises the foundational topics and research pillars to support KG-based AI for self-determination. Drawing upon this conceptual framework, challenges and opportunities for citizen self-determination are illustrated and analysed in a real-world scenario. As a result, we propose a research agenda aimed at accomplishing the recommended objectives.
</details>
<details>
<summary>摘要</summary>
知识 graphs (KGs) 已成为智能决策的基础 плаform，并在大型公司如 Google、Walmart 和 Airbnb 中应用于许多人工智能 (AI) 服务。KGs 补充机器学习 (ML) 算法，提供数据上下文和 semantics，从而实现进一步的推理和问答能力。目前，KGs 与神经网络学 (e.g., Large Language Models (LLMs)) 的结合，被称为神经符号 AI。尽管 KG-based AI 具有许多优点，但其在线服务的普及可能导致公民的自主权减退为社会问题。随着我们对这些技术的依赖，我们将失去自己的命运。为了解决这种威胁，AI 规则，如欧盟 (EU) AI 法规，在某些地区被提出。这些规则需要技术人员做什么，引发了如何确保 AI 系统输出的可靠性、数据驱动和内部机制的透明度，以及如何让 AI 做出负责任的决策的问题。本文概括了 KG-based AI 的基础主题和研究柱石，并通过实际场景的示例和分析，描述了公民自主权的挑战和机遇。因此，我们提出了一个研究计划，旨在实现建议的目标。
</details></li>
</ul>
<hr>
<h2 id="Optimize-Planning-Heuristics-to-Rank-not-to-Estimate-Cost-to-Goal"><a href="#Optimize-Planning-Heuristics-to-Rank-not-to-Estimate-Cost-to-Goal" class="headerlink" title="Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal"></a>Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19463">http://arxiv.org/abs/2310.19463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aicenter/optimize-planning-heuristics-to-rank">https://github.com/aicenter/optimize-planning-heuristics-to-rank</a></li>
<li>paper_authors: Leah Chrestien, Tomás Pevný, Stefan Edelkamp, Antonín Komenda</li>
<li>for: 这个论文是为了优化搜索算法中的启发函数参数而写的。</li>
<li>methods: 这篇论文使用了解决 пробле 集 instances 的方法来优化启发函数参数。</li>
<li>results: 实验结果表明，使用这种方法可以在多种问题上得到更好的性能。<details>
<summary>Abstract</summary>
In imitation learning for planning, parameters of heuristic functions are optimized against a set of solved problem instances. This work revisits the necessary and sufficient conditions of strictly optimally efficient heuristics for forward search algorithms, mainly A* and greedy best-first search, which expand only states on the returned optimal path. It then proposes a family of loss functions based on ranking tailored for a given variant of the forward search algorithm. Furthermore, from a learning theory point of view, it discusses why optimizing cost-to-goal \hstar\ is unnecessarily difficult. The experimental comparison on a diverse set of problems unequivocally supports the derived theory.
</details>
<details>
<summary>摘要</summary>
在依 Beispiel Learning for Planning 中， Parameters of 追求函数 被优化对于一组解决的问题实例。 这项工作 revisits the necessary and sufficient conditions of strictly optimally efficient heuristics for forward search algorithms, mainly A\* 和 greedy best-first search, which expand only states on the returned optimal path. It then proposes a family of loss functions based on ranking tailored for a given variant of the forward search algorithm. Furthermore, from a learning theory point of view, it discusses why optimizing cost-to-goal \hstar\ is unnecessarily difficult. The experimental comparison on a diverse set of problems unequivocally supports the derived theory.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Denoising-Diffusion-Probabilistic-Models-for-Hardware-Impaired-Communication-Systems-Towards-Wireless-Generative-AI"><a href="#Denoising-Diffusion-Probabilistic-Models-for-Hardware-Impaired-Communication-Systems-Towards-Wireless-Generative-AI" class="headerlink" title="Denoising Diffusion Probabilistic Models for Hardware-Impaired Communication Systems: Towards Wireless Generative AI"></a>Denoising Diffusion Probabilistic Models for Hardware-Impaired Communication Systems: Towards Wireless Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19460">http://arxiv.org/abs/2310.19460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Letafati, Samad Ali, Matti Latva-aho</li>
<li>For: This paper proposes a practical wireless communication system with hardware-impaired transceivers, using denoising diffusion probabilistic models (DDPMs) to improve network resilience and reconstruction performance.* Methods: The proposed DDPM-based receiver uses a decomposition of the data generation process to address realistic non-idealities such as hardware impairments, channel distortions, and quantization errors.* Results: The paper shows that the proposed approach provides near-invariant reconstruction performance with respect to different hardware impairment levels and quantization errors, and achieves more than 25 dB improvement in reconstruction performance compared to conventional deep neural network (DNN)-based receivers.<details>
<summary>Abstract</summary>
Thanks to the outstanding achievements from state-of-the-art generative models like ChatGPT and diffusion models, generative AI has gained substantial attention across various industrial and academic domains. In this paper, denoising diffusion probabilistic models (DDPMs) are proposed for a practical finite-precision wireless communication system with hardware-impaired transceivers. The intuition behind DDPM is to decompose the data generation process over the so-called "denoising" steps. Inspired by this, a DDPM-based receiver is proposed for a practical wireless communication scheme that faces realistic non-idealities, including hardware impairments (HWI), channel distortions, and quantization errors. It is shown that our approach provides network resilience under low-SNR regimes, near-invariant reconstruction performance with respect to different HWI levels and quantization errors, and robust out-of-distribution performance against non-Gaussian noise. Moreover, the reconstruction performance of our scheme is evaluated in terms of cosine similarity and mean-squared error (MSE), highlighting more than 25 dB improvement compared to the conventional deep neural network (DNN)-based receivers.
</details>
<details>
<summary>摘要</summary>
由于现代生成模型如ChatGPT和扩散模型的出色成就，生成AI已经受到了各种领域和学术领域的广泛关注。在这篇论文中，我们提出了一种实用的finite-precision无线通信系统中的杂谱扩散概率模型（DDPM）。DDPM的启发是将数据生成过程 decomposes into "denoising" steps。受到这种启发，我们提出了基于DDPM的一种实用的无线通信接收器，可以面对现实的非理想条件，包括硬件缺陷（HWI）、通道扭曲和量化误差。我们的方法可以在低SNR情况下提供网络鲁棒性，对不同HWI水平和量化误差 exhibit near-invariant重建性，并且具有对非高斯噪声的Robust性。此外，我们的重建性分析采用cosine similarity和平均方差Error（MSE），表明与传统的深度神经网络（DNN）接收器相比，我们的方法可以实现超过25dB的提升。
</details></li>
</ul>
<hr>
<h2 id="ALT-Towards-Fine-grained-Alignment-between-Language-and-CTR-Models-for-Click-Through-Rate-Prediction"><a href="#ALT-Towards-Fine-grained-Alignment-between-Language-and-CTR-Models-for-Click-Through-Rate-Prediction" class="headerlink" title="ALT: Towards Fine-grained Alignment between Language and CTR Models for Click-Through Rate Prediction"></a>ALT: Towards Fine-grained Alignment between Language and CTR Models for Click-Through Rate Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19453">http://arxiv.org/abs/2310.19453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hangyu Wang, Jianghao Lin, Xiangyang Li, Bo Chen, Chenxu Zhu, Ruiming Tang, Weinan Zhang, Yong Yu</li>
<li>for: 预测用户点击率 (CTR) 作为个人化在线服务的核心功能模块，这paper提出了一种新的alignment方法来提高CTR预测的准确率。</li>
<li>methods: 这paper使用了一种新的join重构预训 task来实现语言和CTR模型之间的细致特征对应，并提出了三种不同的训练策略来满足不同的应用场景需求。</li>
<li>results: 实验结果表明，这paper提出的alignment方法可以在三个实际 datasets上达到 state-of-the-art 性能，并且可以与不同的语言和CTR模型结合使用，以满足不同的应用场景需求。<details>
<summary>Abstract</summary>
Click-through rate (CTR) prediction plays as a core function module in various personalized online services. According to the data modality and input format, the models for CTR prediction can be mainly classified into two categories. The first one is the traditional CTR models that take as inputs the one-hot encoded ID features of tabular modality, which aims to capture the collaborative signals via feature interaction modeling. The second category takes as inputs the sentences of textual modality obtained by hard prompt templates, where pretrained language models (PLMs) are adopted to extract the semantic knowledge. These two lines of research generally focus on different characteristics of the same input data (i.e., textual and tabular modalities), forming a distinct complementary relationship with each other. Therefore, in this paper, we propose to conduct fine-grained feature-level Alignment between Language and CTR models (ALT) for CTR prediction. Apart from the common CLIP-like instance-level contrastive learning, we further design a novel joint reconstruction pretraining task for both masked language and tabular modeling. Specifically, the masked data of one modality (i.e., tokens or features) has to be recovered with the help of the other modality, which establishes the feature-level interaction and alignment via sufficient mutual information extraction between dual modalities. Moreover, we propose three different finetuning strategies with the option to train the aligned language and CTR models separately or jointly for downstream CTR prediction tasks, thus accommodating the varying efficacy and efficiency requirements for industrial applications. Extensive experiments on three real-world datasets demonstrate that ALT outperforms SOTA baselines, and is highly compatible for various language and CTR models.
</details>
<details>
<summary>摘要</summary>
点击率（CTR）预测作为个人化在线服务的核心功能模块，可以根据数据模式和输入格式分为两类模型。第一类是传统的 CTR 模型，通过一个热门的 ID 特征一个个进行编码，目的是捕捉协作信号via特性互动模型。第二类则是基于硬模板的文本模式，采用预训练语言模型（PLMs）来提取 semantics。这两种研究通常关注不同的输入数据特性（即文本和表格模式），形成一种明确的补做关系。因此，在这篇论文中，我们提议进行细化的特征级别对齐（ALT）来进行 CTR 预测。除了常见的 CLIP 类似的实例级别对比学习外，我们还设计了一种新的联合重建预训任务，以便对masked language和表格模型进行共同预训。具体来说，一个模式（例如， токен或特征）的masked数据需要通过另一个模式来恢复，这种方式在两个模式之间建立特征级别的交互和对齐，通过sufficient mutual information extraction between dual modalities。此外，我们还提出了三种不同的 finetuning 策略，可以根据下游应用的效率和可行性要求来训练对齐的语言和 CTR 模型，从而满足不同的应用场景。广泛的实验表明，ALT 超过了 SOTA 基elines，并具有高度的兼容性，可以与多种语言和 CTR 模型结合使用。
</details></li>
</ul>
<hr>
<h2 id="Large-Scale-Application-of-Fault-Injection-into-PyTorch-Models-–-an-Extension-to-PyTorchFI-for-Validation-Efficiency"><a href="#Large-Scale-Application-of-Fault-Injection-into-PyTorch-Models-–-an-Extension-to-PyTorchFI-for-Validation-Efficiency" class="headerlink" title="Large-Scale Application of Fault Injection into PyTorch Models – an Extension to PyTorchFI for Validation Efficiency"></a>Large-Scale Application of Fault Injection into PyTorch Models – an Extension to PyTorchFI for Validation Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19449">http://arxiv.org/abs/2310.19449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ralf Graafe, Qutub Syed Sha, Florian Geissler, Michael Paulitsch</li>
<li>for: 本研究旨在分析硬件故障（HW）对软件（SW）和人工神经网络（NN）模型的影响，以及如何使用应用程序水平的故障插入（ALFI）来实现机器学习开发过程中的安全性案例。</li>
<li>methods: 本研究使用了PyTorchFI框架，并提出了一种新的应用程序水平故障插入框架called PyTorchALFI，以便定义随机生成的和可重复使用的故障，并对PyTorch模型进行评估。</li>
<li>results: 本研究通过对PyTorch模型进行多个场景测试，并对测试结果进行分析，以便了解硬件故障对模型的影响，并且提供了一些使用PyTorchALFI框架进行模型修改和比较的示例。<details>
<summary>Abstract</summary>
Transient or permanent faults in hardware can render the output of Neural Networks (NN) incorrect without user-specific traces of the error, i.e. silent data errors (SDE). On the other hand, modern NNs also possess an inherent redundancy that can tolerate specific faults. To establish a safety case, it is necessary to distinguish and quantify both types of corruptions. To study the effects of hardware (HW) faults on software (SW) in general and NN models in particular, several fault injection (FI) methods have been established in recent years. Current FI methods focus on the methodology of injecting faults but often fall short of accounting for large-scale FI tests, where many fault locations based on a particular fault model need to be analyzed in a short time. Results need to be concise, repeatable, and comparable. To address these requirements and enable fault injection as the default component in a machine learning development cycle, we introduce a novel fault injection framework called PyTorchALFI (Application Level Fault Injection for PyTorch) based on PyTorchFI. PyTorchALFI provides an efficient way to define randomly generated and reusable sets of faults to inject into PyTorch models, defines complex test scenarios, enhances data sets, and generates test KPIs while tightly coupling fault-free, faulty, and modified NN. In this paper, we provide details about the definition of test scenarios, software architecture, and several examples of how to use the new framework to apply iterative changes in fault location and number, compare different model modifications, and analyze test results.
</details>
<details>
<summary>摘要</summary>
非暂时或永久的硬件故障可以使神经网络（NN）的输出错误无法诊断到用户特定的错误迹象，即静默数据错误（SDE）。然而，现代NN也拥有内置的重复性，可以承受特定的故障。为建立安全性 caso，需要分化和量化两种损害。为了研究硬件（HW）故障对软件（SW）的影响，以及NN模型的影响，多年来已经有多种硬件故障插入（FI）方法的建立。现有的FI方法通常将注意力集中在插入故障的方法上，而忽视了大规模FI测试，需要在短时间内分析多个故障位置基于特定故障模型。结果需要是简洁、重复、比较。为解决这些需求，我们介绍了一个新的硬件故障插入框架 called PyTorchALFI（PyTorch应用程序级故障插入），基于PyTorchFI。PyTorchALFI提供了一种效果的方式来定义随机生成的和可重用的故障集，定义复杂的测试enario，增强数据集，并生成测试KPI，同时紧密地集成 fault-free、FAULTY 和修改后的NN。在这篇文章中，我们提供了测试scenario的定义、软件架构和多种使用新框架的示例，以应用iterative变化的故障位置和数量，比较不同的模型修改，分析测试结果。
</details></li>
</ul>
<hr>
<h2 id="Explaining-the-Decisions-of-Deep-Policy-Networks-for-Robotic-Manipulations"><a href="#Explaining-the-Decisions-of-Deep-Policy-Networks-for-Robotic-Manipulations" class="headerlink" title="Explaining the Decisions of Deep Policy Networks for Robotic Manipulations"></a>Explaining the Decisions of Deep Policy Networks for Robotic Manipulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19432">http://arxiv.org/abs/2310.19432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seongun Kim, Jaesik Choi</li>
<li>for: 这个论文旨在解释深度政策模型中的透明性，以便在机器人 manipulate 任务中提高可靠性和稳定性。</li>
<li>methods: 作者使用输入贡献分析方法来解释机器人策略模型中每个输入特征对决策的影响。两种方法分别是：（1）测量每个 JOINT 力的重要性因子，以反映电动机力的影响于终端器移动；（2）修改 relevance propagation 方法，以正确处理深度策略网络中的负输入和输出。</li>
<li>results: 研究人员通过对深度策略模型进行输入贡献分析，发现了多modal 感知器输入的动态变化，并且可以在机器人 manipulate 任务中提高透明性和可靠性。<details>
<summary>Abstract</summary>
Deep policy networks enable robots to learn behaviors to solve various real-world complex tasks in an end-to-end fashion. However, they lack transparency to provide the reasons of actions. Thus, such a black-box model often results in low reliability and disruptive actions during the deployment of the robot in practice. To enhance its transparency, it is important to explain robot behaviors by considering the extent to which each input feature contributes to determining a given action. In this paper, we present an explicit analysis of deep policy models through input attribution methods to explain how and to what extent each input feature affects the decisions of the robot policy models. To this end, we present two methods for applying input attribution methods to robot policy networks: (1) we measure the importance factor of each joint torque to reflect the influence of the motor torque on the end-effector movement, and (2) we modify a relevance propagation method to handle negative inputs and outputs in deep policy networks properly. To the best of our knowledge, this is the first report to identify the dynamic changes of input attributions of multi-modal sensor inputs in deep policy networks online for robotic manipulation.
</details>
<details>
<summary>摘要</summary>
深度政策网络可以让机器人学习做各种复杂的实际任务，但它缺乏透明度，无法提供动作的原因。因此，这种黑obox模型在实际应用中可能会导致低可靠性和干扰行为。为了增强其透明度，需要解释机器人行为，考虑每个输入特征对决策的影响程度。在这篇论文中，我们通过输入贡献方法来解释深度政策模型中每个输入特征对机器人决策的影响。为此，我们提出了两种将输入贡献方法应用于机器人政策网络：首先，我们测量每个 JOINT 扭矩的重要性因素，以反映电动机扭矩对终端器运动的影响；其次，我们修改了 relevance propagation 方法，以正确处理深度政策网络中的负输入和输出。根据我们所知，这是首次在深度政策网络上线实时识别多模式感知输入的动态变化。
</details></li>
</ul>
<hr>
<h2 id="Refining-Diffusion-Planner-for-Reliable-Behavior-Synthesis-by-Automatic-Detection-of-Infeasible-Plans"><a href="#Refining-Diffusion-Planner-for-Reliable-Behavior-Synthesis-by-Automatic-Detection-of-Infeasible-Plans" class="headerlink" title="Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans"></a>Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19427">http://arxiv.org/abs/2310.19427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leekwoon/rgg">https://github.com/leekwoon/rgg</a></li>
<li>paper_authors: Kyowoon Lee, Seongun Kim, Jaesik Choi</li>
<li>for: 提高Diffusion模型生成的不可靠计划的可靠性，使其在安全关键应用中可以使用。</li>
<li>methods: 提出一种新的方法，通过提供修正指导来纠正Diffusion模型生成的错误计划。该方法包括一种新的评价指标——恢复差值评价指导，以及一种增强器来防止恶作剂修正指导。</li>
<li>results: 在三个benchmark上进行了offline控制设置的长期规划测试，并证明了我们的方法的效果。同时，我们还展示了我们的方法的解释力，通过显示差值预测器的归属地图和错误途径，以便更深入了解生成的计划。<details>
<summary>Abstract</summary>
Diffusion-based planning has shown promising results in long-horizon, sparse-reward tasks by training trajectory diffusion models and conditioning the sampled trajectories using auxiliary guidance functions. However, due to their nature as generative models, diffusion models are not guaranteed to generate feasible plans, resulting in failed execution and precluding planners from being useful in safety-critical applications. In this work, we propose a novel approach to refine unreliable plans generated by diffusion models by providing refining guidance to error-prone plans. To this end, we suggest a new metric named restoration gap for evaluating the quality of individual plans generated by the diffusion model. A restoration gap is estimated by a gap predictor which produces restoration gap guidance to refine a diffusion planner. We additionally present an attribution map regularizer to prevent adversarial refining guidance that could be generated from the sub-optimal gap predictor, which enables further refinement of infeasible plans. We demonstrate the effectiveness of our approach on three different benchmarks in offline control settings that require long-horizon planning. We also illustrate that our approach presents explainability by presenting the attribution maps of the gap predictor and highlighting error-prone transitions, allowing for a deeper understanding of the generated plans.
</details>
<details>
<summary>摘要</summary>
Diffusion-based planning has shown promising results in long-horizon, sparse-reward tasks by training trajectory diffusion models and conditioning the sampled trajectories using auxiliary guidance functions. However, due to their nature as generative models, diffusion models are not guaranteed to generate feasible plans, resulting in failed execution and precluding planners from being useful in safety-critical applications. In this work, we propose a novel approach to refine unreliable plans generated by diffusion models by providing refining guidance to error-prone plans. To this end, we suggest a new metric named restoration gap for evaluating the quality of individual plans generated by the diffusion model. A restoration gap is estimated by a gap predictor which produces restoration gap guidance to refine a diffusion planner. We additionally present an attribution map regularizer to prevent adversarial refining guidance that could be generated from the sub-optimal gap predictor, which enables further refinement of infeasible plans. We demonstrate the effectiveness of our approach on three different benchmarks in offline control settings that require long-horizon planning. We also illustrate that our approach presents explainability by presenting the attribution maps of the gap predictor and highlighting error-prone transitions, allowing for a deeper understanding of the generated plans.</sys:language_translate_from_english>🇨🇳 Diffusion-based  планирование 已经在长期目标、稀热奖励任务中显示出了承诺的结果，通过训练路径扩散模型并使用辅助指导函数来conditioning 采样的 trajectory。然而，由于它们的性质为生成模型，diffusion 模型不能保证生成可行的计划，导致执行失败和禁止在安全应用中使用。在这种情况下，我们提出了一种新的方法，可以修复不可靠的计划，并且可以防止由低质量的 gap 预测器生成的 adversarial 修复指南。我们建议一个新的 metric 名为 restoration gap，可以评估生成的计划质量。restoration gap 由 gap 预测器生成的修复指南来衡量。此外，我们还提出了一种 attributed map 规范，可以防止由低质量的 gap 预测器生成的 adversarial 修复指南。我们在三个不同的 benchmark 上进行了证明，并示出了我们的方法的可行性和可见性。🇨🇳  Diffusion-based  планирование 已经在长期目标、稀热奖励任务中显示出了承诺的结果，通过训练路径扩散模型并使用辅助指导函数来conditioning 采样的 trajectory。然而，由于它们的性质为生成模型，diffusion 模型不能保证生成可行的计划，导致执行失败和禁止在安全应用中使用。在这种情况下，我们提出了一种新的方法，可以修复不可靠的计划，并且可以防止由低质量的 gap 预测器生成的 adversarial 修复指南。我们建议一个新的 metric 名为 restoration gap，可以评估生成的计划质量。restoration gap 由 gap 预测器生成的修复指南来衡量。此外，我们还提出了一种 attributed map 规范，可以防止由低质量的 gap 预测器生成的 adversarial 修复指南。我们在三个不同的 benchmark 上进行了证明，并示出了我们的方法的可行性和可见性。
</details></li>
</ul>
<hr>
<h2 id="Artificial-intelligence-and-the-limits-of-the-humanities"><a href="#Artificial-intelligence-and-the-limits-of-the-humanities" class="headerlink" title="Artificial intelligence and the limits of the humanities"></a>Artificial intelligence and the limits of the humanities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19425">http://arxiv.org/abs/2310.19425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Włodzisław Duch</li>
<li>for: 这篇论文是为了探讨现代世界中文化复杂性的问题，以及人类认知的限制和人工智能的发展对人文学科的影响。</li>
<li>methods: 这篇论文使用了认知科学的方法和数据分析技术，以探讨人类认知的限制和人工智能的发展对人文学科的影响。</li>
<li>results: 这篇论文的结果表明，人工智能将在人文学科中推广应用，从艺术到政治科学和哲学，使这些领域变得更加吸引人，让学生们能够超越当前的限制。<details>
<summary>Abstract</summary>
The complexity of cultures in the modern world is now beyond human comprehension. Cognitive sciences cast doubts on the traditional explanations based on mental models. The core subjects in humanities may lose their importance. Humanities have to adapt to the digital age. New, interdisciplinary branches of humanities emerge. Instant access to information will be replaced by instant access to knowledge. Understanding the cognitive limitations of humans and the opportunities opened by the development of artificial intelligence and interdisciplinary research necessary to address global challenges is the key to the revitalization of humanities. Artificial intelligence will radically change humanities, from art to political sciences and philosophy, making these disciplines attractive to students and enabling them to go beyond current limitations.
</details>
<details>
<summary>摘要</summary>
现代世界中文化的复杂性已经超越人类理解的能力。诺谟科学抵触了传统基于心理模型的解释。核心人文科目可能会失去其重要性。人文学需要适应数字时代。新的交叉学科人文科学出现。快速获取信息将被快速获取知识所取代。理解人类认知的局限性和人工智能和交叉研究的发展对解决全球挑战是人文学的关键。人工智能将彻底改变人文学，从艺术到政治科学和哲学，使这些学科吸引更多的学生，让他们可以超越当前的局限性。
</details></li>
</ul>
<hr>
<h2 id="Variational-Curriculum-Reinforcement-Learning-for-Unsupervised-Discovery-of-Skills"><a href="#Variational-Curriculum-Reinforcement-Learning-for-Unsupervised-Discovery-of-Skills" class="headerlink" title="Variational Curriculum Reinforcement Learning for Unsupervised Discovery of Skills"></a>Variational Curriculum Reinforcement Learning for Unsupervised Discovery of Skills</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19424">http://arxiv.org/abs/2310.19424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seongun-kim/vcrl">https://github.com/seongun-kim/vcrl</a></li>
<li>paper_authors: Seongun Kim, Kyowoon Lee, Jaesik Choi</li>
<li>for: 提高复杂技能自主学习的效率和状态覆盖速度</li>
<li>methods: 基于信息理论的无监督学习方法，包括变量资源学习和目标条件RL</li>
<li>results: 在复杂导航和机器人操作任务上，提高样本效率和状态覆盖速度，并在实际世界Robot导航任务中完成零模拟设置，并且与全球规划器结合可以进一步提高表现。<details>
<summary>Abstract</summary>
Mutual information-based reinforcement learning (RL) has been proposed as a promising framework for retrieving complex skills autonomously without a task-oriented reward function through mutual information (MI) maximization or variational empowerment. However, learning complex skills is still challenging, due to the fact that the order of training skills can largely affect sample efficiency. Inspired by this, we recast variational empowerment as curriculum learning in goal-conditioned RL with an intrinsic reward function, which we name Variational Curriculum RL (VCRL). From this perspective, we propose a novel approach to unsupervised skill discovery based on information theory, called Value Uncertainty Variational Curriculum (VUVC). We prove that, under regularity conditions, VUVC accelerates the increase of entropy in the visited states compared to the uniform curriculum. We validate the effectiveness of our approach on complex navigation and robotic manipulation tasks in terms of sample efficiency and state coverage speed. We also demonstrate that the skills discovered by our method successfully complete a real-world robot navigation task in a zero-shot setup and that incorporating these skills with a global planner further increases the performance.
</details>
<details>
<summary>摘要</summary>
互助信息基于渐进学习（RL）已被提议为自动学习复杂技能的有望框架，通过互助信息（MI）最大化或变量赋权来实现。然而，学习复杂技能仍然是挑战，因为训练技能的顺序可以大大影响样本效率。drawing inspiration from this, we recast variational empowerment as curriculum learning in goal-conditioned RL with an intrinsic reward function, which we name Variational Curriculum RL (VCRL). From this perspective, we propose a novel approach to unsupervised skill discovery based on information theory, called Value Uncertainty Variational Curriculum (VUVC). We prove that, under regularity conditions, VUVC accelerates the increase of entropy in the visited states compared to the uniform curriculum. We validate the effectiveness of our approach on complex navigation and robotic manipulation tasks in terms of sample efficiency and state coverage speed. We also demonstrate that the skills discovered by our method successfully complete a real-world robot navigation task in a zero-shot setup and that incorporating these skills with a global planner further increases the performance.
</details></li>
</ul>
<hr>
<h2 id="Text-to-3D-with-Classifier-Score-Distillation"><a href="#Text-to-3D-with-Classifier-Score-Distillation" class="headerlink" title="Text-to-3D with Classifier Score Distillation"></a>Text-to-3D with Classifier Score Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19415">http://arxiv.org/abs/2310.19415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, Xiaojuan Qi</li>
<li>for: 这篇论文主要用于探讨Score Distillation Sampling（SDS）方法在文本生成中的表现，特别是在使用类ifier-free guidance时的优化。</li>
<li>methods: 这篇论文使用了一种新的Classifier Score Distillation（CSD）方法，其利用了预训练的2D扩散模型，并通过使用一个隐藏的分类器来对生成进行导航。</li>
<li>results: 研究发现，CSD方法可以在文本生成中实现更高效的结果，包括形状生成、纹理合成和形状编辑等任务，并且比现有的方法更高效。<details>
<summary>Abstract</summary>
Text-to-3D generation has made remarkable progress recently, particularly with methods based on Score Distillation Sampling (SDS) that leverages pre-trained 2D diffusion models. While the usage of classifier-free guidance is well acknowledged to be crucial for successful optimization, it is considered an auxiliary trick rather than the most essential component. In this paper, we re-evaluate the role of classifier-free guidance in score distillation and discover a surprising finding: the guidance alone is enough for effective text-to-3D generation tasks. We name this method Classifier Score Distillation (CSD), which can be interpreted as using an implicit classification model for generation. This new perspective reveals new insights for understanding existing techniques. We validate the effectiveness of CSD across a variety of text-to-3D tasks including shape generation, texture synthesis, and shape editing, achieving results superior to those of state-of-the-art methods. Our project page is https://xinyu-andy.github.io/Classifier-Score-Distillation
</details>
<details>
<summary>摘要</summary>
Text-to-3D生成技术在最近几年内已取得了很大的进步，特别是基于Score Distillation Sampling（SDS）的方法。SDS方法利用预训练的2D扩散模型，而使用无类标注导航是已知为成功优化的关键。然而，在这篇论文中，我们重新评估了无类标注导航的角色在Score Distillation中，发现一个奇异的发现：导航alone是足够的 для有效的文本到3D生成任务。我们称之为Classifier Score Distillation（CSD），可以理解为使用隐藏的分类模型进行生成。这新的视角揭示了新的理解现有技术的新思路。我们在多种文本到3D任务上验证了CSD的效果，包括形状生成、Texture Synthesis和形状编辑，并成功超越了当前的状态艺术法。更多信息请访问我们的项目页面：https://xinyu-andy.github.io/Classifier-Score-Distillation。
</details></li>
</ul>
<hr>
<h2 id="Resource-Constrained-Semantic-Segmentation-for-Waste-Sorting"><a href="#Resource-Constrained-Semantic-Segmentation-for-Waste-Sorting" class="headerlink" title="Resource Constrained Semantic Segmentation for Waste Sorting"></a>Resource Constrained Semantic Segmentation for Waste Sorting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19407">http://arxiv.org/abs/2310.19407</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anubis09/Resource_Constrained_Semantic_Segmentation_for_Waste_Sorting">https://github.com/anubis09/Resource_Constrained_Semantic_Segmentation_for_Waste_Sorting</a></li>
<li>paper_authors: Elisa Cascina, Andrea Pellegrino, Lorenzo Tozzi</li>
<li>for: 这 paper 是为了提出高效的废物分类策略，以降低垃圾的环境影响。</li>
<li>methods: 该 paper 使用了 resource-constrained semantic segmentation 模型，用于在工业设置中分类可回收垃圾。模型需要在10MB内存限制下运行，适用于边缘应用程序with limited processing capacity。作者们使用了量化和剪辑技术来实现这一目标。</li>
<li>results: 作者们在三个网络（ICNet、BiSeNet（Xception39 backbone）和ENet）上进行了实验，并取得了正面的结果，同时只有marginally影响了 Mean IoU 度量。此外，作者们还提出了一种combined Focal和Lovász loss函数，用于解决隐式的类别不均衡问题，从而实现更好的性能。<details>
<summary>Abstract</summary>
This work addresses the need for efficient waste sorting strategies in Materials Recovery Facilities to minimize the environmental impact of rising waste. We propose resource-constrained semantic segmentation models for segmenting recyclable waste in industrial settings. Our goal is to develop models that fit within a 10MB memory constraint, suitable for edge applications with limited processing capacity. We perform the experiments on three networks: ICNet, BiSeNet (Xception39 backbone), and ENet. Given the aforementioned limitation, we implement quantization and pruning techniques on the broader nets, achieving positive results while marginally impacting the Mean IoU metric. Furthermore, we propose a combination of Focal and Lov\'asz loss that addresses the implicit class imbalance resulting in better performance compared with the Cross-entropy loss function.
</details>
<details>
<summary>摘要</summary>
这个研究旨在提出高效垃圾分类策略，以减少垃圾堆生的环境影响。我们提议使用限制资源的语义分割模型，用于工业环境中分类可回收物。我们的目标是开发10MB内存套用的模型，适用于边缘应用程序，具有有限的处理能力。我们在三个网络上进行实验：ICNet、BiSeNet（Xception39底层）和ENet。由于上述限制，我们实施了量化和剪除技术，获得了正面效果，同时微量地影响了 Mean IoU 度量。此外，我们提议使用 FOCAL 和 Lovász 损失函数，解决了隐式的分类不均衡问题，从而实现更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Othello-is-Solved"><a href="#Othello-is-Solved" class="headerlink" title="Othello is Solved"></a>Othello is Solved</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19387">http://arxiv.org/abs/2310.19387</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mwarot1/othello-ai">https://github.com/mwarot1/othello-ai</a></li>
<li>paper_authors: Hiroki Takizawa</li>
<li>for: 这篇论文是为了解决扑克游戏“奥特莫”的计算解决方案。</li>
<li>methods: 该论文使用了计算机科学中的搜索算法和推理技术来解决游戏。</li>
<li>results: 论文表明，在完美游戏情况下，两个玩家的游戏都会平局。<details>
<summary>Abstract</summary>
The game of Othello is one of the world's most complex and popular games that has yet to be computationally solved. Othello has roughly ten octodecillion (10 to the 58th power) possible game records and ten octillion (10 to the 28th power) possible game position. The challenge of solving Othello, determining the outcome of a game with no mistake made by either player, has long been a grand challenge in computer science. This paper announces a significant milestone: Othello is now solved, computationally proved that perfect play by both players lead to a draw. Strong Othello software has long been built using heuristically designed search techniques. Solving a game provides the solution which enables software to play the game perfectly.
</details>
<details>
<summary>摘要</summary>
抽象游戏“奥迪洛”是全球最复杂且受欢迎的游戏之一，尚未被计算解决。奥迪洛拥有约十 octodecillion（10^58）可能的游戏记录和十 octillion（10^28）可能的游戏位置。解决奥迪洛的挑战，即确定没有任何错误的游戏记录，长期被计算机科学界视为一个巨大的挑战。本文宣布了一个重要突破：奥迪洛已经被计算解决，并证明了完美游戏记录会导致平局。强大的奥迪洛软件长期采用了经验设计的搜索技术。解决游戏提供了完美游戏记录，使软件可以完美地游戏。
</details></li>
</ul>
<hr>
<h2 id="Protecting-Publicly-Available-Data-With-Machine-Learning-Shortcuts"><a href="#Protecting-Publicly-Available-Data-With-Machine-Learning-Shortcuts" class="headerlink" title="Protecting Publicly Available Data With Machine Learning Shortcuts"></a>Protecting Publicly Available Data With Machine Learning Shortcuts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19381">http://arxiv.org/abs/2310.19381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas M. Müller, Maximilian Burgert, Pascal Debus, Jennifer Williams, Philip Sperl, Konstantin Böttinger</li>
<li>for: 防止非法数据抓取 (to prevent unauthorized data crawling)</li>
<li>methods: 利用机器学习假短Circuit (using machine learning shortcuts)</li>
<li>results: 成功地防止非法数据抓取 (successfully prevented unauthorized data crawling)Here’s the full text in Simplified Chinese:</li>
<li>for: 防止非法数据抓取，即许多 crawlers  Grabs 和重新销售数据点。</li>
<li>methods: 利用机器学习假短Circuit，即在数据集中植入 ML 假短，使模型具有卓越的训练和测试性能，但同时限制模型的泛化能力。</li>
<li>results: 通过实验，我们成功地防止了非法数据抓取，而且这种方法可以扩展到多个用例。<details>
<summary>Abstract</summary>
Machine-learning (ML) shortcuts or spurious correlations are artifacts in datasets that lead to very good training and test performance but severely limit the model's generalization capability. Such shortcuts are insidious because they go unnoticed due to good in-domain test performance. In this paper, we explore the influence of different shortcuts and show that even simple shortcuts are difficult to detect by explainable AI methods. We then exploit this fact and design an approach to defend online databases against crawlers: providers such as dating platforms, clothing manufacturers, or used car dealers have to deal with a professionalized crawling industry that grabs and resells data points on a large scale. We show that a deterrent can be created by deliberately adding ML shortcuts. Such augmented datasets are then unusable for ML use cases, which deters crawlers and the unauthorized use of data from the internet. Using real-world data from three use cases, we show that the proposed approach renders such collected data unusable, while the shortcut is at the same time difficult to notice in human perception. Thus, our proposed approach can serve as a proactive protection against illegitimate data crawling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Few-shot-Hybrid-Domain-Adaptation-of-Image-Generators"><a href="#Few-shot-Hybrid-Domain-Adaptation-of-Image-Generators" class="headerlink" title="Few-shot Hybrid Domain Adaptation of Image Generators"></a>Few-shot Hybrid Domain Adaptation of Image Generators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19378">http://arxiv.org/abs/2310.19378</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/echopluto/fhda">https://github.com/echopluto/fhda</a></li>
<li>paper_authors: Hengjia Li, Yang Liu, Linxuan Xia, Yuqi Lin, Tu Zheng, Zheng Yang, Wenxiao Wang, Xiaohui Zhong, Xiaobo Ren, Xiaofei He</li>
<li>for: 能否适应多个目标领域的混合领域？</li>
<li>methods: 我们提出了一种无监测器框架，通过直接将不同领域的图像编码为分离的子空间来解决这个问题。我们还提出了一个新的方向分量损失，该损失通过减少生成图像与所有目标领域的距离，同时保持源领域的特征。</li>
<li>results: 我们的方法可以在单个适应器中获得多个目标领域的各种特征，超越基eline方法在 semantic similarity、图像准确性和交叉领域一致性上。<details>
<summary>Abstract</summary>
Can a pre-trained generator be adapted to the hybrid of multiple target domains and generate images with integrated attributes of them? In this work, we introduce a new task -- Few-shot Hybrid Domain Adaptation (HDA). Given a source generator and several target domains, HDA aims to acquire an adapted generator that preserves the integrated attributes of all target domains, without overriding the source domain's characteristics. Compared with Domain Adaptation (DA), HDA offers greater flexibility and versatility to adapt generators to more composite and expansive domains. Simultaneously, HDA also presents more challenges than DA as we have access only to images from individual target domains and lack authentic images from the hybrid domain. To address this issue, we introduce a discriminator-free framework that directly encodes different domains' images into well-separable subspaces. To achieve HDA, we propose a novel directional subspace loss comprised of a distance loss and a direction loss. Concretely, the distance loss blends the attributes of all target domains by reducing the distances from generated images to all target subspaces. The direction loss preserves the characteristics from the source domain by guiding the adaptation along the perpendicular to subspaces. Experiments show that our method can obtain numerous domain-specific attributes in a single adapted generator, which surpasses the baseline methods in semantic similarity, image fidelity, and cross-domain consistency.
</details>
<details>
<summary>摘要</summary>
可以把预训练的生成器适应到多个目标Domain的混合体中，生成具有这些目标Domain的 интеGRATED特征？在这项工作中，我们提出了一个新任务——多少shot Hybrid Domain Adaptation（HDA）。给定一个源生成器和多个目标Domain，HDA的目标是获得一个适应了所有目标Domain的特征的生成器，而不覆盖源Domain的特征。与Domain Adaptation（DA）相比，HDA具有更大的灵活性和多样性，可以适应更复杂和广泛的Domain。同时，HDA也带来了更多的挑战，因为我们只有各个目标Domain的图像，缺乏真正的混合Domain的图像。为解决这个问题，我们提出了一个不含探测器的框架，直接将不同Domain的图像编码成分离的子空间中。为实现HDA，我们提出了一个新的方向性子空间损失，包括距离损失和方向损失。具体来说，距离损失将所有目标Domain的特征混合在生成图像中，使得生成图像与所有目标Subspace之间的距离减小。方向损失保持源Domain的特征，使得适应过程在Subspace之间的垂直方向上进行。实验表明，我们的方法可以在单个适应器中获得多个Domain特征，超过基eline方法的semantic similarity、图像准确率和交叉Domain一致性。
</details></li>
</ul>
<hr>
<h2 id="RGB-X-Object-Detection-via-Scene-Specific-Fusion-Modules"><a href="#RGB-X-Object-Detection-via-Scene-Specific-Fusion-Modules" class="headerlink" title="RGB-X Object Detection via Scene-Specific Fusion Modules"></a>RGB-X Object Detection via Scene-Specific Fusion Modules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19372">http://arxiv.org/abs/2310.19372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsriaditya999/rgbxfusion">https://github.com/dsriaditya999/rgbxfusion</a></li>
<li>paper_authors: Sri Aditya Deevi, Connor Lee, Lu Gan, Sushruth Nagesh, Gaurav Pandey, Soon-Jo Chung</li>
<li>for: 实现自主车辆在各种天气情况下视觉理解环境。</li>
<li>methods: 使用高效且各自独立的RGB-X融合网络，可以运用已经预训的单模式模型，并运用Scene-specific融合模组来整合多 modal 数据。</li>
<li>results: 与现有方法比较，我们的方法在RGB-热和RGB-关闭数据上表现更好，仅需要小量额外参数。代码可以在<a target="_blank" rel="noopener" href="https://github.com/dsriaditya999/RGBXFusion">https://github.com/dsriaditya999/RGBXFusion</a> 上获取。<details>
<summary>Abstract</summary>
Multimodal deep sensor fusion has the potential to enable autonomous vehicles to visually understand their surrounding environments in all weather conditions. However, existing deep sensor fusion methods usually employ convoluted architectures with intermingled multimodal features, requiring large coregistered multimodal datasets for training. In this work, we present an efficient and modular RGB-X fusion network that can leverage and fuse pretrained single-modal models via scene-specific fusion modules, thereby enabling joint input-adaptive network architectures to be created using small, coregistered multimodal datasets. Our experiments demonstrate the superiority of our method compared to existing works on RGB-thermal and RGB-gated datasets, performing fusion using only a small amount of additional parameters. Our code is available at https://github.com/dsriaditya999/RGBXFusion.
</details>
<details>
<summary>摘要</summary>
多模态深感融合具有实现自动驾驶车辆在所有天气条件下视觉理解周围环境的潜力。然而，现有的深感融合方法通常采用复杂的建筑方式，混合多模态特征，需要大量相关多模态数据进行训练。在这项工作中，我们提出了一种高效和模块化的 RGB-X 融合网络，可以通过Scene-specific fusion modules来利用和融合预训练的单模态模型，从而实现输入适应性网络架构，使用小量相关多模态数据进行融合。我们的实验表明我们的方法与现有作品在 RGB-thermal 和 RGB-gated 数据集上表现更优，只需要一小部分额外参数。我们的代码可以在 GitHub 上找到：https://github.com/dsriaditya999/RGBXFusion。
</details></li>
</ul>
<hr>
<h2 id="Balance-Imbalance-and-Rebalance-Understanding-Robust-Overfitting-from-a-Minimax-Game-Perspective"><a href="#Balance-Imbalance-and-Rebalance-Understanding-Robust-Overfitting-from-a-Minimax-Game-Perspective" class="headerlink" title="Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective"></a>Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19360">http://arxiv.org/abs/2310.19360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pku-ml/rebat">https://github.com/pku-ml/rebat</a></li>
<li>paper_authors: Yifei Wang, Liangchen Li, Jiansheng Yang, Zhouchen Lin, Yisen Wang</li>
<li>for: 本研究旨在解释逆攻击训练（AT）在学习率（LR）衰减后存在严重的Robust Overfitting问题，并提出一种解决方案来缓解这种问题。</li>
<li>methods: 本研究使用了视为对抗训练为动态最小最大游戏的分析方法，具体来说，研究如何通过强化模型训练者的记忆能力，使得模型具有更好的鲁棒性。</li>
<li>results: 实验结果表明，采用ReBalanced Adversarial Training（ReBAT）可以提高模型的鲁棒性，并不会在训练过程中出现Robust Overfitting问题，即使学习率衰减很长。<details>
<summary>Abstract</summary>
Adversarial Training (AT) has become arguably the state-of-the-art algorithm for extracting robust features. However, researchers recently notice that AT suffers from severe robust overfitting problems, particularly after learning rate (LR) decay. In this paper, we explain this phenomenon by viewing adversarial training as a dynamic minimax game between the model trainer and the attacker. Specifically, we analyze how LR decay breaks the balance between the minimax game by empowering the trainer with a stronger memorization ability, and show such imbalance induces robust overfitting as a result of memorizing non-robust features. We validate this understanding with extensive experiments, and provide a holistic view of robust overfitting from the dynamics of both the two game players. This understanding further inspires us to alleviate robust overfitting by rebalancing the two players by either regularizing the trainer's capacity or improving the attack strength. Experiments show that the proposed ReBalanced Adversarial Training (ReBAT) can attain good robustness and does not suffer from robust overfitting even after very long training. Code is available at https://github.com/PKU-ML/ReBAT.
</details>
<details>
<summary>摘要</summary>
adversarial training (AT) 已成为当前领域中最佳的算法，以EXTRACTING Robust Features。然而，研究人员最近注意到，AT受到了严重的Robust Overfitting问题，特别是在学习率 (LR)  decay 后。在这篇论文中，我们解释这种现象，视为 adversarial training 是一个动态的 minimax 游戏 между模型训练者和攻击者。我们分析了如何 LR decay 破坏了这个游戏的平衡，使得训练者获得了更强的记忆能力，并显示这种不平衡引起了 robust overfitting 的结果。我们通过广泛的实验 validate 这一理解，并提供了robust overfitting 的整体视图，从两个游戏 player 的动态来看。这一理解还 inspirits 我们提出了 rebalancing  adversarial training (ReBAT)，以解决 robust overfitting 问题。实验显示，ReBAT 可以 дости得好的Robustness，并不会在训练过程中 suffer  from robust overfitting。代码可以在 https://github.com/PKU-ML/ReBAT 上找到。
</details></li>
</ul>
<hr>
<h2 id="Modified-Genetic-Algorithm-for-Feature-Selection-and-Hyper-Parameter-Optimization-Case-of-XGBoost-in-Spam-Prediction"><a href="#Modified-Genetic-Algorithm-for-Feature-Selection-and-Hyper-Parameter-Optimization-Case-of-XGBoost-in-Spam-Prediction" class="headerlink" title="Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction"></a>Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19845">http://arxiv.org/abs/2310.19845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazeeh Ghatasheh, Ismail Altaharwa, Khaled Aldebei</li>
<li>for: 本研究旨在提出一种模糊减量和超参优化的模糊遗传算法，以解决社交媒体上的滥发问题。</li>
<li>methods: 本研究使用了一种自适应的模糊遗传算法，initialized eXtreme Gradient Boosting分类器，并对推文数据进行缩放和减量，以生成一个滥发预测模型。</li>
<li>results: 实验结果表明，提案的方法可以在10 folds的十分划分验证中，平均取得82.32%的准确率和92.67%的平均值，使用了 menos de 10%的总特征空间。此外，模糊遗传算法还比$Chi^2$和$PCA$特征选择方法更高效。<details>
<summary>Abstract</summary>
Recently, spam on online social networks has attracted attention in the research and business world. Twitter has become the preferred medium to spread spam content. Many research efforts attempted to encounter social networks spam. Twitter brought extra challenges represented by the feature space size, and imbalanced data distributions. Usually, the related research works focus on part of these main challenges or produce black-box models. In this paper, we propose a modified genetic algorithm for simultaneous dimensionality reduction and hyper parameter optimization over imbalanced datasets. The algorithm initialized an eXtreme Gradient Boosting classifier and reduced the features space of tweets dataset; to generate a spam prediction model. The model is validated using a 50 times repeated 10-fold stratified cross-validation, and analyzed using nonparametric statistical tests. The resulted prediction model attains on average 82.32\% and 92.67\% in terms of geometric mean and accuracy respectively, utilizing less than 10\% of the total feature space. The empirical results show that the modified genetic algorithm outperforms $Chi^2$ and $PCA$ feature selection methods. In addition, eXtreme Gradient Boosting outperforms many machine learning algorithms, including BERT-based deep learning model, in spam prediction. Furthermore, the proposed approach is applied to SMS spam modeling and compared to related works.
</details>
<details>
<summary>摘要</summary>
最近，社交媒体上的垃圾信息引起了研究和业务界的关注。推特成为了垃圾信息的主要媒体。许多研究努力解决社交媒体上的垃圾信息问题。推特的特点是巨大的特征空间和不均匀的数据分布。通常，相关的研究工作只是解决一些主要挑战或生成黑盒模型。在这篇论文中，我们提出了一种修改后的遗传算法，同时实现维度减少和超参优化。该算法首先初始化了极限梯度提升分类器，然后将推特数据集中的特征空间减少，以生成垃圾预测模型。模型通过10次10fold stratified cross-validation进行验证，并使用非参数统计测试分析。结果显示，修改后的遗传算法在平均上达到82.32%和92.67%的垃圾预测精度，使用的特征空间少于10%。实验结果表明，修改后的遗传算法超过了$Chi^2$和$PCA$特征选择方法。此外，极限梯度提升超过了许多机器学习算法，包括BERT基于深度学习模型，在垃圾预测方面。此外，我们还应用了提议方法到短信垃圾模型中，并与相关工作进行比较。
</details></li>
</ul>
<hr>
<h2 id="Introducing-instance-label-correlation-in-multiple-instance-learning-Application-to-cancer-detection-on-histopathological-images"><a href="#Introducing-instance-label-correlation-in-multiple-instance-learning-Application-to-cancer-detection-on-histopathological-images" class="headerlink" title="Introducing instance label correlation in multiple instance learning. Application to cancer detection on histopathological images"></a>Introducing instance label correlation in multiple instance learning. Application to cancer detection on histopathological images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19359">http://arxiv.org/abs/2310.19359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Morales-Álvarez, Arne Schmidt, José Miguel Hernández-Lobato, Rafael Molina</li>
<li>for: 这个论文是为了提出一种基于 Gaussian Processes（GP）的多例学习（MIL）方法，以便在计算生物学中处理不具有补丁级别标签的整个扫描图像。</li>
<li>methods: 该方法基于现有的 state-of-the-art VGPMIL-PR 方法，并增加了一种以 Ising 模型为 inspirations 的新的 Coupling 项。使用变量推断来估计所有模型参数。</li>
<li>results: 在两个实际问题中（抑制肾癌检测），我们的模型表现更好于其他现有的可能性 MIL 方法，并提供了不同的视觉化和分析来了解 Coupling 项的影响。这些结果预期能够应用于其他研究领域。<details>
<summary>Abstract</summary>
In the last years, the weakly supervised paradigm of multiple instance learning (MIL) has become very popular in many different areas. A paradigmatic example is computational pathology, where the lack of patch-level labels for whole-slide images prevents the application of supervised models. Probabilistic MIL methods based on Gaussian Processes (GPs) have obtained promising results due to their excellent uncertainty estimation capabilities. However, these are general-purpose MIL methods that do not take into account one important fact: in (histopathological) images, the labels of neighboring patches are expected to be correlated. In this work, we extend a state-of-the-art GP-based MIL method, which is called VGPMIL-PR, to exploit such correlation. To do so, we develop a novel coupling term inspired by the statistical physics Ising model. We use variational inference to estimate all the model parameters. Interestingly, the VGPMIL-PR formulation is recovered when the weight that regulates the strength of the Ising term vanishes. The performance of the proposed method is assessed in two real-world problems of prostate cancer detection. We show that our model achieves better results than other state-of-the-art probabilistic MIL methods. We also provide different visualizations and analysis to gain insights into the influence of the novel Ising term. These insights are expected to facilitate the application of the proposed model to other research areas.
</details>
<details>
<summary>摘要</summary>
最近几年，弱度监督多例学习（MIL）的思想在多个领域得到了广泛的应用。一个典型的应用例子是计算生物学，因为整个扫描图像缺乏小块级别标签，使得超参数学习模型无法应用。基于 Gaussian Processes（GP）的概率MIL方法在这些领域取得了显著的成果，主要是因为它们的不确定性估计能力强。然而，这些是通用MIL方法，没有考虑一个重要的事实：在生物学图像中，邻居块的标签具有相互关联性。在这个工作中，我们扩展了状态空间的GP-based MIL方法，称为VGPMIL-PR，以利用这种相互关联性。为此，我们开发了一个灵感来自统计物理爱因斯坦模型的新封装项。我们使用变量推断来估计所有模型参数。有趣的是，VGPMIL-PR的形式被恰当的Weightvanishes时 recovered。我们在两个实际问题中评估了提案的方法性能：肠癌检测。我们发现，我们的模型在其他状态空间的概率MIL方法中表现出色，并且提供了不同的视觉化和分析，以便更深入地理解VGPMIL-PR的影响。这些理解可能会促进该模型在其他研究领域的应用。
</details></li>
</ul>
<hr>
<h2 id="Modeling-the-Telemarketing-Process-using-Genetic-Algorithms-and-Extreme-Boosting-Feature-Selection-and-Cost-Sensitive-Analytical-Approach"><a href="#Modeling-the-Telemarketing-Process-using-Genetic-Algorithms-and-Extreme-Boosting-Feature-Selection-and-Cost-Sensitive-Analytical-Approach" class="headerlink" title="Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach"></a>Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19843">http://arxiv.org/abs/2310.19843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazeeh Ghatasheh, Ismail Altaharwa, Khaled Aldebei</li>
<li>For: This paper aims to leverage telemarketing data to model the willingness of clients to make a term deposit and to find the most significant characteristics of clients.* Methods: The paper proposes a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously, and builds an explainable prediction model using real-world data from a Portuguese bank and national socio-economic metrics.* Results: The models significantly outperform related works in terms of class of interest accuracy, with an average of 89.07% and a type I error of 0.059. The model is expected to maximize the potential profit margin at the least possible cost and provide more insights to support marketing decision-making.Here is the text in Simplified Chinese:* For: 这篇论文目标是利用电话营销数据模型客户愿意签订贷款，并找出客户最重要的特征。* Methods: 论文提出了一种基于遗传算法的分类器，以同时选择最佳描述特征和分类器参数。它还建立了可解释的预测模型，使用葡萄牙银行的实际数据和国家经济指标。* Results: 模型与相关工作相比，表现出了显著的优异，具体来说是89.07%的正精度和0.059的类型一错。模型预期能够最大化利润减cost，并为市场营销决策提供更多的洞察。<details>
<summary>Abstract</summary>
Currently, almost all direct marketing activities take place virtually rather than in person, weakening interpersonal skills at an alarming pace. Furthermore, businesses have been striving to sense and foster the tendency of their clients to accept a marketing offer. The digital transformation and the increased virtual presence forced firms to seek novel marketing research approaches. This research aims at leveraging the power of telemarketing data in modeling the willingness of clients to make a term deposit and finding the most significant characteristics of the clients. Real-world data from a Portuguese bank and national socio-economic metrics are used to model the telemarketing decision-making process. This research makes two key contributions. First, propose a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously. Second, build an explainable prediction model. The best-generated classification models were intensively validated using 50 times repeated 10-fold stratified cross-validation and the selected features have been analyzed. The models significantly outperform the related works in terms of class of interest accuracy, they attained an average of 89.07\% and 0.059 in terms of geometric mean and type I error respectively. The model is expected to maximize the potential profit margin at the least possible cost and provide more insights to support marketing decision-making.
</details>
<details>
<summary>摘要</summary>
现在，大多数直接市场活动都发生在虚拟空间上，而不是面对面，这导致人际交往技巧受到了威胁。此外，企业也在努力感受和培养客户接受市场提供的倾向。由于数字转型和虚拟存在的增加，公司们需要找到新的市场研究方法。这项研究希望通过电话营销数据来模型客户是否签订贷款的愿望，并找出客户最重要的特征。使用葡萄牙银行的实际数据和国家经济指标，我们模型了电话营销决策过程。这项研究有两个关键贡献：首先，提出了一种基于遗传算法的分类器，可同时选择最佳分类特征和调整分类器参数。第二，建立了可解释预测模型。最佳生成的分类模型经过50次10fold分割验证，选择的特征也进行了分析。这些模型在相关作品的类别准确率和类型一错率方面均表现出色，其中类别准确率为89.07%，类型一错率为0.059。这些模型预计可以最大化可能的利润差额，并提供更多的市场决策支持。
</details></li>
</ul>
<hr>
<h2 id="Improving-Factual-Consistency-of-Text-Summarization-by-Adversarially-Decoupling-Comprehension-and-Embellishment-Abilities-of-LLMs"><a href="#Improving-Factual-Consistency-of-Text-Summarization-by-Adversarially-Decoupling-Comprehension-and-Embellishment-Abilities-of-LLMs" class="headerlink" title="Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs"></a>Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19347">http://arxiv.org/abs/2310.19347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huawen Feng, Yan Fan, Xiong Liu, Ting-En Lin, Zekun Yao, Yuchuan Wu, Fei Huang, Yongbin Li, Qianli Ma</li>
<li>for: 提高基于大语言模型（LLM）的文本摘要的可靠性，解决LLM生成的摘要具有“幻想”等问题。</li>
<li>methods: 提出了一种对 LLM 进行逆解耦法（DECENT），以便分离把握和补充两种能力，并采用了一种基于探针的参数有效的技术来补充训练过程中的缺失。</li>
<li>results: DECENT 可以够准确地提高基于 LLM 的文本摘要的可靠性，并且可以减少 LLM 生成的幻想现象。<details>
<summary>Abstract</summary>
Despite the recent progress in text summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as "hallucinations" in text generation. Unlike previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, and overgeneralizing, etc. These hallucinations are challenging to detect through traditional methods, which poses great challenges for improving the factual consistency of text summarization. In this paper, we propose an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based parameter-efficient technique to cover the shortage of sensitivity for true and false in the training process of LLMs. In this way, LLMs are less confused about embellishing and understanding, thus can execute the instructions more accurately and have enhanced abilities to distinguish hallucinations. Experimental results show that DECENT significantly improves the reliability of text summarization based on LLMs.
</details>
<details>
<summary>摘要</summary>
尽管最近的大语言模型（LLM）在文本摘要方面做出了重要的进步，但它们经常生成的摘要却与原文不匹配，这被称为“幻觉”（hallucinations）在文本生成中。与过去的小型模型（例如BART、T5）相比，当前的LLM更少地出现了笨蛋的问题，但更多地出现了更加复杂的问题，如强制 causa et effectus、添加假信息、过度总结等等。这些幻觉通常难以通过传统方法检测，这对提高文本摘要的准确性带来了很大的挑战。在这篇论文中，我们提出了一种对抗分解方法，以分离LLM的理解和丰富能力（DECENT）。此外，我们采用了一种 parameter-efficient 的探测技术，以弥补 LLM 在训练过程中对真实和假的敏感性的缺失。这样，LLM 就不再混乱地塑造和理解，因此可以更准确地执行指令，并具有更强的幻觉检测能力。实验结果表明，DECENT 可以显著提高基于 LLM 的文本摘要的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Skywork-A-More-Open-Bilingual-Foundation-Model"><a href="#Skywork-A-More-Open-Bilingual-Foundation-Model" class="headerlink" title="Skywork: A More Open Bilingual Foundation Model"></a>Skywork: A More Open Bilingual Foundation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19341">http://arxiv.org/abs/2310.19341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skyworkai/skywork">https://github.com/skyworkai/skywork</a></li>
<li>paper_authors: Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou</li>
<li>for: 这个技术报告提出了 Skywork-13B，一个基于英语和中文文本资料的大语言模型（LLMs），这是目前最大的开源发布的LLMs。</li>
<li>methods: 该模型使用了两stage训练方法，首先是通用训练，然后是领域特定增强训练。</li>
<li>results: 该模型在各种标准测试 benchmarks 上表现出色，并在多个领域的中文语言模型中达到了状态的艺术性能。此外，该报告还提出了一种泄露检测方法，表明测试数据污染是一个需要进一步调查的问题。<details>
<summary>Abstract</summary>
In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves \emph{state of the art} performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.
</details>
<details>
<summary>摘要</summary>
在这份技术报告中，我们介绍了 Skywork-13B，一家大语言模型（LLMs），在英语和中文文本集合上 receives 超过 3.2 万亿个字的训练。这是目前最广泛训练和公开发布的相似大小 LLMs。我们提出了一种两阶段训练方法，首先是通用训练，然后是领域特定增强训练。我们发现，我们的模型不仅在 популяр的标准 bencmarks 上表现出色，还在多个领域的中文语言模型中实现了状态的术性表现。此外，我们提出了一种新的泄露检测方法，这表明测试数据污染是一个需要进一步调查的问题， LLMC 社区应该关注。为促进未来的研究，我们发布了 Skywork-13B 和在 intermediate 阶段训练过程中的检点。我们还发布了一部分的 SkyPile corpus，这是目前最大的高质量公开中文预训练集，总计超过 150 亿个字的网络文本。我们希望 Skywork-13B 和我们的开放 corpus 将成为一个价值的开源资源，推广高质量 LLMC 的访问。
</details></li>
</ul>
<hr>
<h2 id="TempME-Towards-the-Explainability-of-Temporal-Graph-Neural-Networks-via-Motif-Discovery"><a href="#TempME-Towards-the-Explainability-of-Temporal-Graph-Neural-Networks-via-Motif-Discovery" class="headerlink" title="TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery"></a>TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19324">http://arxiv.org/abs/2310.19324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graph-and-geometric-learning/tempme">https://github.com/graph-and-geometric-learning/tempme</a></li>
<li>paper_authors: Jialin Chen, Rex Ying</li>
<li>for: 本研究的目的是提高现有的时间图 neural network (TGNN) 的可解释性和信任worthiness，通过找出引导预测的时间抽象（temporal motifs）。</li>
<li>methods: 本研究提出了一种新的方法 called Temporal Motifs Explainer (TempME), 它基于信息瓶颈理论提取最重要的时间抽象，以最小化包含的信息量，保持解释的简洁和稀烈。</li>
<li>results: 实验表明，TempME 可以更好地找出引导预测的时间抽象，并提高现有 TGNN 的预测精度，最高提高22.96%。<details>
<summary>Abstract</summary>
Temporal graphs are widely used to model dynamic systems with time-varying interactions. In real-world scenarios, the underlying mechanisms of generating future interactions in dynamic systems are typically governed by a set of recurring substructures within the graph, known as temporal motifs. Despite the success and prevalence of current temporal graph neural networks (TGNN), it remains uncertain which temporal motifs are recognized as the significant indications that trigger a certain prediction from the model, which is a critical challenge for advancing the explainability and trustworthiness of current TGNNs. To address this challenge, we propose a novel approach, called Temporal Motifs Explainer (TempME), which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs. Derived from the information bottleneck principle, TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation. Events in the explanations generated by TempME are verified to be more spatiotemporally correlated than those of existing approaches, providing more understandable insights. Extensive experiments validate the superiority of TempME, with up to 8.21% increase in terms of explanation accuracy across six real-world datasets and up to 22.96% increase in boosting the prediction Average Precision of current TGNNs.
</details>
<details>
<summary>摘要</summary>
现在的渐变系统模型中，时间相关的交互都是通过图structures来描述的。在现实世界中，这些图structures中的时间模式（temporal motifs）是生成未来交互的基本机制。虽然现有的时间图神经网络（TGNN）已经取得了成功，但是还不确定哪些时间模式是TGNN的预测中的重要指示器，这是现有TGNN的解释性和可信度的主要挑战。为解决这个问题，我们提出了一种新的方法，即时间模式解释器（TempME），它抽出TGNN预测中的最重要时间模式，同时保持解释的简洁和精炼。 TempME基于信息瓶颈理论，从交互相关的模式中提取最重要的信息，以保持解释的简洁和精炼。对于 TempME生成的解释，事件之间的空间时间相关性比现有方法高，提供更直观的理解。广泛的实验证明了 TempME 的超越性，在六个实际 dataset 上提高解释准确率达到8.21%，并在当前 TGNN 的预测中提高了平均精度22.96%。
</details></li>
</ul>
<hr>
<h2 id="D4Explainer-In-Distribution-GNN-Explanations-via-Discrete-Denoising-Diffusion"><a href="#D4Explainer-In-Distribution-GNN-Explanations-via-Discrete-Denoising-Diffusion" class="headerlink" title="D4Explainer: In-Distribution GNN Explanations via Discrete Denoising Diffusion"></a>D4Explainer: In-Distribution GNN Explanations via Discrete Denoising Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19321">http://arxiv.org/abs/2310.19321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graph-and-geometric-learning/d4explainer">https://github.com/graph-and-geometric-learning/d4explainer</a></li>
<li>paper_authors: Jialin Chen, Shirley Wu, Abhijit Gupta, Rex Ying</li>
<li>for: 提高 Graph Neural Networks (GNNs) 的解释性，以便更好地理解 GNNs 对预测结果的影响。</li>
<li>methods: 提出了一种新的 D4Explainer 方法，该方法通过 incorporating 生成图分布学习到优化目标中，以生成符合分布性的对于每个实例的几种可能性图，并且通过对这些可能性图进行分析，以提供模型级别的解释。</li>
<li>results: D4Explainer 在 synthetic 和实际世界数据集上实现了状态之决定的表现，包括解释准确率、实际性、多样性和Robustness。<details>
<summary>Abstract</summary>
The widespread deployment of Graph Neural Networks (GNNs) sparks significant interest in their explainability, which plays a vital role in model auditing and ensuring trustworthy graph learning. The objective of GNN explainability is to discern the underlying graph structures that have the most significant impact on model predictions. Ensuring that explanations generated are reliable necessitates consideration of the in-distribution property, particularly due to the vulnerability of GNNs to out-of-distribution data. Unfortunately, prevailing explainability methods tend to constrain the generated explanations to the structure of the original graph, thereby downplaying the significance of the in-distribution property and resulting in explanations that lack reliability. To address these challenges, we propose D4Explainer, a novel approach that provides in-distribution GNN explanations for both counterfactual and model-level explanation scenarios. The proposed D4Explainer incorporates generative graph distribution learning into the optimization objective, which accomplishes two goals: 1) generate a collection of diverse counterfactual graphs that conform to the in-distribution property for a given instance, and 2) identify the most discriminative graph patterns that contribute to a specific class prediction, thus serving as model-level explanations. It is worth mentioning that D4Explainer is the first unified framework that combines both counterfactual and model-level explanations. Empirical evaluations conducted on synthetic and real-world datasets provide compelling evidence of the state-of-the-art performance achieved by D4Explainer in terms of explanation accuracy, faithfulness, diversity, and robustness.
</details>
<details>
<summary>摘要</summary>
广泛部署图 neural network (GNN) 引发了大量关注，其中一个关键问题是解释abilit y，它在模型审核和建立信任worthy图学习中扮演着关键角色。GNN解释的目标是理解图结构对模型预测的影响。为保证生成的解释可靠，必须考虑图中的分布性质，特别是由于GNN的敏感性，以避免由图外数据引起的解释错误。然而，现有的解释方法通常只能在原始图结构下生成解释，从而忽略图中的分布性质，导致解释不可靠。为解决这些挑战，我们提出了D4Explainer，一种新的方法，可以为GNN提供符合分布性质的内部解释。D4Explainer integrate generative graph distribution learning into the optimization objective，它可以实现两个目标：1）生成符合分布性质的对应实例的多个多样化Counterfactual graphs，2）标识影响特定预测的最重要的图模式，并作为模型级别解释。需要注意的是，D4Explainer是第一个结合Counterfactual和模型级别解释的统一框架。empirical evaluations on synthetic and real-world datasets show that D4Explainer achieves state-of-the-art performance in terms of explanation accuracy, faithfulness, diversity, and robustness.
</details></li>
</ul>
<hr>
<h2 id="L2T-DLN-Learning-to-Teach-with-Dynamic-Loss-Network"><a href="#L2T-DLN-Learning-to-Teach-with-Dynamic-Loss-Network" class="headerlink" title="L2T-DLN: Learning to Teach with Dynamic Loss Network"></a>L2T-DLN: Learning to Teach with Dynamic Loss Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19313">http://arxiv.org/abs/2310.19313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhoyang Hai, Liyuan Pan, Xiabi Liu, Zhengzheng Liu, Mirna Yunita</li>
<li>for: 这个论文的目的是教导机器学习模型如何使用动态损失函数进行学习。</li>
<li>methods: 这个论文使用了一种具有记忆单元的教师模型，以及一种动态损失网络，以帮助学生模型通过教师模型的经验进行学习。</li>
<li>results: 实验表明，这种方法可以提高学生模型的学习效果，并且可以提高不同深度模型在真实世界任务上的性能，包括分类、目标检测和semantic segmentation等场景。<details>
<summary>Abstract</summary>
With the concept of teaching being introduced to the machine learning community, a teacher model start using dynamic loss functions to teach the training of a student model. The dynamic intends to set adaptive loss functions to different phases of student model learning. In existing works, the teacher model 1) merely determines the loss function based on the present states of the student model, i.e., disregards the experience of the teacher; 2) only utilizes the states of the student model, e.g., training iteration number and loss/accuracy from training/validation sets, while ignoring the states of the loss function. In this paper, we first formulate the loss adjustment as a temporal task by designing a teacher model with memory units, and, therefore, enables the student learning to be guided by the experience of the teacher model. Then, with a dynamic loss network, we can additionally use the states of the loss to assist the teacher learning in enhancing the interactions between the teacher and the student model. Extensive experiments demonstrate our approach can enhance student learning and improve the performance of various deep models on real-world tasks, including classification, objective detection, and semantic segmentation scenarios.
</details>
<details>
<summary>摘要</summary>
In this paper, we formulate the loss adjustment as a temporal task by designing a teacher model with memory units, allowing the student learning to be guided by the teacher model's experience. Additionally, we use a dynamic loss network to assist the teacher learning in enhancing the interactions between the teacher and the student model. Our approach has been extensively tested and has been shown to improve the performance of various deep models on real-world tasks, including classification, object detection, and semantic segmentation scenarios.
</details></li>
</ul>
<hr>
<h2 id="Free-from-Bellman-Completeness-Trajectory-Stitching-via-Model-based-Return-conditioned-Supervised-Learning"><a href="#Free-from-Bellman-Completeness-Trajectory-Stitching-via-Model-based-Return-conditioned-Supervised-Learning" class="headerlink" title="Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning"></a>Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19308">http://arxiv.org/abs/2310.19308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoyi Zhou, Chuning Zhu, Runlong Zhou, Qiwen Cui, Abhishek Gupta, Simon Shaolei Du</li>
<li>for:  solves sequential decision-making problems with off-policy dynamic programming techniques</li>
<li>methods:  return-conditioned supervised learning (RCSL) and multilayer perceptron function approximator</li>
<li>results:  converges under more relaxed assumptions than traditional dynamic programming methods, outperforms state-of-the-art model-free and model-based offline RL algorithms in simulated robotics problems<details>
<summary>Abstract</summary>
Off-policy dynamic programming (DP) techniques such as $Q$-learning have proven to be an important technique for solving sequential decision-making problems. However, in the presence of function approximation such algorithms are not guaranteed to converge, often diverging due to the absence of Bellman-completeness in the function classes considered, a crucial condition for the success of DP-based methods. In this paper, we show how off-policy learning techniques based on return-conditioned supervised learning (RCSL) are able to circumvent these challenges of Bellman completeness, converging under significantly more relaxed assumptions inherited from supervised learning. We prove there exists a natural environment in which if one uses two-layer multilayer perceptron as the function approximator, the layer width needs to grow linearly with the state space size to satisfy Bellman-completeness while a constant layer width is enough for RCSL. These findings take a step towards explaining the superior empirical performance of RCSL methods compared to DP-based methods in environments with near-optimal datasets. Furthermore, in order to learn from sub-optimal datasets, we propose a simple framework called MBRCSL, granting RCSL methods the ability of dynamic programming to stitch together segments from distinct trajectories. MBRCSL leverages learned dynamics models and forward sampling to accomplish trajectory stitching while avoiding the need for Bellman completeness that plagues all dynamic programming algorithms. We propose both theoretical analysis and experimental evaluation to back these claims, outperforming state-of-the-art model-free and model-based offline RL algorithms across several simulated robotics problems.
</details>
<details>
<summary>摘要</summary>
Off-policy动态规划（DP）技术如$Q$-学习已经证明是解决时间序列决策问题的重要技术。然而，在函数拟合的存在下，这些算法并不是保证 converges，经常因为函数类型中缺乏 Bellman完备性，这是动态规划基本条件的重要因素。在这篇论文中，我们表明了基于返回条件supervised learning（RCSL）的外部学习技术可以绕过 Bellman完备性的挑战，并在更松散的假设下 converges。我们证明了在使用两层多层感知器作为函数 aproximator 时，状态空间大小与层宽之间存在直线关系，以满足 Bellman完备性的条件。这些发现为RCSL方法在实际中的superior empirical performance提供了解释。此外，为了学习从不优化的数据集中，我们提议了一个简单的框架called MBRCSL，使得 RCSL 方法可以通过动态规划来缝合分割的轨迹。MBRCSL 利用学习的动力模型和前向采样来完成轨迹缝合，而不需要 Bellman完备性，这些都是动态规划算法的必要条件。我们提出了理论分析和实验评估，在多个模拟的 роботикс问题上超过了当前的model-free和model-based offline RL算法的性能。
</details></li>
</ul>
<hr>
<h2 id="ROME-Evaluating-Pre-trained-Vision-Language-Models-on-Reasoning-beyond-Visual-Common-Sense"><a href="#ROME-Evaluating-Pre-trained-Vision-Language-Models-on-Reasoning-beyond-Visual-Common-Sense" class="headerlink" title="ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense"></a>ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19301">http://arxiv.org/abs/2310.19301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/k-square-00/rome">https://github.com/k-square-00/rome</a></li>
<li>paper_authors: Kankan Zhou, Eason Lai, Wei Bin Au Yeong, Kyriakos Mouratidis, Jing Jiang</li>
<li>for: 评估现有预训练视觉语言模型是否具备理解非常规内容的能力。</li>
<li>methods: 使用新创的ROME数据集进行评测，该数据集包含违反常识知识的图像。</li>
<li>results: 大多数预训练视觉语言模型无法正确地解释非常规的场景。<details>
<summary>Abstract</summary>
Humans possess a strong capability for reasoning beyond common sense. For example, given an unconventional image of a goldfish laying on the table next to an empty fishbowl, a human would effortlessly determine that the fish is not inside the fishbowl. The case, however, may be different for a vision-language model, whose reasoning could gravitate towards the common scenario that the fish is inside the bowl, despite the visual input. In this paper, we introduce a novel probing dataset named ROME (reasoning beyond commonsense knowledge) to evaluate whether the state-of-the-art pre-trained vision-language models have the reasoning capability to correctly interpret counter-intuitive content. ROME contains images that defy commonsense knowledge with regards to color, shape, material, size and positional relation. Experiments on the state-of-the-art pre-trained vision-language models reveal that most of these models are still largely incapable of interpreting counter-intuitive scenarios. We hope that ROME will spur further investigations on reasoning beyond commonsense knowledge in vision-language research.
</details>
<details>
<summary>摘要</summary>
人类具有强大的理性能力，可以理解不同于常识的场景。例如，给一个不寻常的图像，如一只鱼在桌子旁边的空鱼缸中，人类会很容易理解鱼不在鱼缸中。然而，这可能不是情况 для视觉语言模型，这些模型可能会受到常识的引导，认为鱼在鱼缸中。在这篇论文中，我们提出了一个新的探索数据集名为ROME（理解超出常识知识），以评估当前最先进的预训练视觉语言模型是否具备正确地解释不同征的能力。ROME数据集包含图像，它们与常识知识有很多不同，包括颜色、形状、材质、大小和位置关系。我们对当前最先进的预训练视觉语言模型进行实验，发现大多数这些模型仍然无法正确地解释不同征的场景。我们希望ROME会激发更多关于理解超出常识知识的研究在视觉语言领域。
</details></li>
</ul>
<hr>
<h2 id="ROAM-memory-efficient-large-DNN-training-via-optimized-operator-ordering-and-memory-layout"><a href="#ROAM-memory-efficient-large-DNN-training-via-optimized-operator-ordering-and-memory-layout" class="headerlink" title="ROAM: memory-efficient large DNN training via optimized operator ordering and memory layout"></a>ROAM: memory-efficient large DNN training via optimized operator ordering and memory layout</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19295">http://arxiv.org/abs/2310.19295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huiyao Shu, Ang Wang, Ziji Shi, Hanyu Zhao, Yong Li, Lu Lu</li>
<li>for: 本文主要针对 deep learning 模型训练中的内存问题，提出了一种基于 computation graph 的内存有效执行计划，以提高模型的内存使用效率和减少高级技术的开销。</li>
<li>methods: 本文提出了一种基于模型结构和训练内存负担的优化理论，并提出了一种高效的树结构算法来自动找到合适的任务分解方案。</li>
<li>results: 实验表明，使用 ROAM 可以减少内存使用量的 35.7%、13.3% 和 27.2%，并提供了惊人的 53.7 倍的速度提升。  Plus, the evaluation on the large GPT2-XL model further confirms the scalability of ROAM.<details>
<summary>Abstract</summary>
As deep learning models continue to increase in size, the memory requirements for training have surged. While high-level techniques like offloading, recomputation, and compression can alleviate memory pressure, they also introduce overheads. However, a memory-efficient execution plan that includes a reasonable operator execution order and tensor memory layout can significantly increase the models' memory efficiency and reduce overheads from high-level techniques. In this paper, we propose ROAM which operates on computation graph level to derive memory-efficient execution plan with optimized operator order and tensor memory layout for models. We first propose sophisticated theories that carefully consider model structure and training memory load to support optimization for large complex graphs that have not been well supported in the past. An efficient tree-based algorithm is further proposed to search task divisions automatically, along with delivering high performance and effectiveness to solve the problem. Experiments show that ROAM achieves a substantial memory reduction of 35.7%, 13.3%, and 27.2% compared to Pytorch and two state-of-the-art methods and offers a remarkable 53.7x speedup. The evaluation conducted on the expansive GPT2-XL further validates ROAM's scalability.
</details>
<details>
<summary>摘要</summary>
深度学习模型的大小不断增加，训练时的内存需求也在不断增加。高级技术如卸载、重计算和压缩可以减轻内存压力，但也会导致开销。然而，一个高效的执行计划，包括合理的运算顺序和维度缓存布局，可以减少模型的内存占用和高级技术的开销。在这篇论文中，我们提出了ROAM，它在计算图层次上运行，以 derivation 高效的执行计划，包括优化的运算顺序和维度缓存布局，为模型提供更高的内存效率和更低的开销。我们首先提出了一些复杂的理论，考虑到模型结构和训练内存负担，以支持大规模复杂的图进行优化。然后，我们提出了一种高效的树结构算法，自动搜索任务分割，同时具有高性能和有效性，解决这个问题。实验表明，ROAM可以实现35.7%、13.3%和27.2%的内存减少，相比于Pytorch和两个状态 искусternalMethods，并提供了惊人的53.7倍的速度提升。而在大型GPT2-XL上进行的评估也证明了ROAM的扩展性。
</details></li>
</ul>
<hr>
<h2 id="The-Memory-Perturbation-Equation-Understanding-Model’s-Sensitivity-to-Data"><a href="#The-Memory-Perturbation-Equation-Understanding-Model’s-Sensitivity-to-Data" class="headerlink" title="The Memory Perturbation Equation: Understanding Model’s Sensitivity to Data"></a>The Memory Perturbation Equation: Understanding Model’s Sensitivity to Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19273">http://arxiv.org/abs/2310.19273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Nickl, Lu Xu, Dharmesh Tailor, Thomas Möllenhoff, Mohammad Emtiyaz Khan</li>
<li>for: 这篇论文是用于描述一种能够简化模型训练数据的敏感性的方法。</li>
<li>methods: 该方法基于 bayesian 原则，可以总结多种模型和算法的敏感性度量，并且可以在训练过程中获得敏感性度量的估计值。</li>
<li>results: 研究结果表明，在训练过程中获得的敏感性度量可以准确预测模型在未经见的测试数据上的总成果。这个方法预期会在未来的robust和适应学习研究中得到应用。<details>
<summary>Abstract</summary>
Understanding model's sensitivity to its training data is crucial but can also be challenging and costly, especially during training. To simplify such issues, we present the Memory-Perturbation Equation (MPE) which relates model's sensitivity to perturbation in its training data. Derived using Bayesian principles, the MPE unifies existing sensitivity measures, generalizes them to a wide-variety of models and algorithms, and unravels useful properties regarding sensitivities. Our empirical results show that sensitivity estimates obtained during training can be used to faithfully predict generalization on unseen test data. The proposed equation is expected to be useful for future research on robust and adaptive learning.
</details>
<details>
<summary>摘要</summary>
理解模型受训练数据的敏感性是非常重要，但同时也可能具有挑战性和成本高的问题，特别是在训练期间。为了简化这些问题，我们提出了记忆扰动方程（MPE），该方程关系模型受训练数据的扰动和其敏感性。基于 bayesian 原则，MPE 总结了各种模型和算法的敏感度测量方法，并展示了这些测量方法在各种情况下的有用性。我们的实验结果表明，在训练期间获得的敏感度估计可以准确预测未经训练的测试数据上的泛化性。我们预期该方程在未来的robust和adaptive学习研究中将有用。
</details></li>
</ul>
<hr>
<h2 id="NPCL-Neural-Processes-for-Uncertainty-Aware-Continual-Learning"><a href="#NPCL-Neural-Processes-for-Uncertainty-Aware-Continual-Learning" class="headerlink" title="NPCL: Neural Processes for Uncertainty-Aware Continual Learning"></a>NPCL: Neural Processes for Uncertainty-Aware Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19272">http://arxiv.org/abs/2310.19272</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srvcodes/npcl">https://github.com/srvcodes/npcl</a></li>
<li>paper_authors: Saurav Jha, Dong Gong, He Zhao, Lina Yao</li>
<li>for: 本研究旨在提高深度神经网络在流动数据上高效地训练continuous learning（CL）模型，并限制新任务引起的忘记。</li>
<li>methods: 本研究使用神经过程（NP），一种元学习器，来编码不同任务为概率分布上的函数，同时提供可靠的uncertainty estimate。特别是，我们提出了一种基于NP的CL方法（NPCL），其中任务特定模块被安排在层次隐藏变量模型中。我们采用了定制的正则化来缓解忘记。</li>
<li>results: 我们的实验表明，NPCL比前一代CL方法性能更高。我们还验证了NPCL中的uncertainty estimation能力可以处理CL中的任务头&#x2F;模块推理挑战。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/srvCodes/NPCL%7D">https://github.com/srvCodes/NPCL}</a> 上获取。<details>
<summary>Abstract</summary>
Continual learning (CL) aims to train deep neural networks efficiently on streaming data while limiting the forgetting caused by new tasks. However, learning transferable knowledge with less interference between tasks is difficult, and real-world deployment of CL models is limited by their inability to measure predictive uncertainties. To address these issues, we propose handling CL tasks with neural processes (NPs), a class of meta-learners that encode different tasks into probabilistic distributions over functions all while providing reliable uncertainty estimates. Specifically, we propose an NP-based CL approach (NPCL) with task-specific modules arranged in a hierarchical latent variable model. We tailor regularizers on the learned latent distributions to alleviate forgetting. The uncertainty estimation capabilities of the NPCL can also be used to handle the task head/module inference challenge in CL. Our experiments show that the NPCL outperforms previous CL approaches. We validate the effectiveness of uncertainty estimation in the NPCL for identifying novel data and evaluating instance-level model confidence. Code is available at \url{https://github.com/srvCodes/NPCL}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Revisiting-Evaluation-Metrics-for-Semantic-Segmentation-Optimization-and-Evaluation-of-Fine-grained-Intersection-over-Union"><a href="#Revisiting-Evaluation-Metrics-for-Semantic-Segmentation-Optimization-and-Evaluation-of-Fine-grained-Intersection-over-Union" class="headerlink" title="Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union"></a>Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19252">http://arxiv.org/abs/2310.19252</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zifuwanggg/jdtlosses">https://github.com/zifuwanggg/jdtlosses</a></li>
<li>paper_authors: Zifu Wang, Maxim Berman, Amal Rannen-Triki, Philip H. S. Torr, Devis Tuia, Tinne Tuytelaars, Luc Van Gool, Jiaqian Yu, Matthew B. Blaschko</li>
<li>for: 本文提出了一种用于评估semantic segmentation方法的新的评价指标，以解决传统评价指标受到类别偏度和对象大小偏度的影响。</li>
<li>methods: 本文提出了使用细化的mIoU指标，并与相关的最差情况指标共同评估semantic segmentation方法。</li>
<li>results: 在12种自然和飞行分割数据集上，使用提出的评价指标训练和评估15种现代神经网络模型，研究表明，使用细化的mIoU指标可以减少对大对象的偏度，提供更加全面的评估结果。<details>
<summary>Abstract</summary>
Semantic segmentation datasets often exhibit two types of imbalance: \textit{class imbalance}, where some classes appear more frequently than others and \textit{size imbalance}, where some objects occupy more pixels than others. This causes traditional evaluation metrics to be biased towards \textit{majority classes} (e.g. overall pixel-wise accuracy) and \textit{large objects} (e.g. mean pixel-wise accuracy and per-dataset mean intersection over union). To address these shortcomings, we propose the use of fine-grained mIoUs along with corresponding worst-case metrics, thereby offering a more holistic evaluation of segmentation techniques. These fine-grained metrics offer less bias towards large objects, richer statistical information, and valuable insights into model and dataset auditing. Furthermore, we undertake an extensive benchmark study, where we train and evaluate 15 modern neural networks with the proposed metrics on 12 diverse natural and aerial segmentation datasets. Our benchmark study highlights the necessity of not basing evaluations on a single metric and confirms that fine-grained mIoUs reduce the bias towards large objects. Moreover, we identify the crucial role played by architecture designs and loss functions, which lead to best practices in optimizing fine-grained metrics. The code is available at \href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Pre-trained-Recommender-Systems-A-Causal-Debiasing-Perspective"><a href="#Pre-trained-Recommender-Systems-A-Causal-Debiasing-Perspective" class="headerlink" title="Pre-trained Recommender Systems: A Causal Debiasing Perspective"></a>Pre-trained Recommender Systems: A Causal Debiasing Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19251">http://arxiv.org/abs/2310.19251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/myhakureimu/prerec">https://github.com/myhakureimu/prerec</a></li>
<li>paper_authors: Ziqian Lin, Hao Ding, Nghia Hoang, Branislav Kveton, Anoop Deoras, Hao Wang</li>
<li>for: 本研究探讨了应用预训练视语模型于推荐系统中，以提高适应能力和学习效率。</li>
<li>methods: 本研究使用了预训练模型，并提出了一种归一化权重学习方法来解决各自领域的偏见问题。</li>
<li>results: 实验结果显示，提出的方法可以在零或几步学习 scenarios下提高推荐性能，并在不同市场和平台上实现较好的适应性。In simpler Chinese, the three key points would be:</li>
<li>for: 这个研究用了预训练模型来提高推荐系统的适应能力和学习效率。</li>
<li>methods: 这个研究使用了预训练模型，并提出了一种解决各自领域偏见问题的方法。</li>
<li>results: 实验结果显示，这个方法可以在零或几步学习 scenarios下提高推荐性能，并在不同市场和平台上实现较好的适应性。<details>
<summary>Abstract</summary>
Recent studies on pre-trained vision/language models have demonstrated the practical benefit of a new, promising solution-building paradigm in AI where models can be pre-trained on broad data describing a generic task space and then adapted successfully to solve a wide range of downstream tasks, even when training data is severely limited (e.g., in zero- or few-shot learning scenarios). Inspired by such progress, we investigate in this paper the possibilities and challenges of adapting such a paradigm to the context of recommender systems, which is less investigated from the perspective of pre-trained model. In particular, we propose to develop a generic recommender that captures universal interaction patterns by training on generic user-item interaction data extracted from different domains, which can then be fast adapted to improve few-shot learning performance in unseen new domains (with limited data).   However, unlike vision/language data which share strong conformity in the semantic space, universal patterns underlying recommendation data collected across different domains (e.g., different countries or different E-commerce platforms) are often occluded by both in-domain and cross-domain biases implicitly imposed by the cultural differences in their user and item bases, as well as their uses of different e-commerce platforms. As shown in our experiments, such heterogeneous biases in the data tend to hinder the effectiveness of the pre-trained model. To address this challenge, we further introduce and formalize a causal debiasing perspective, which is substantiated via a hierarchical Bayesian deep learning model, named PreRec. Our empirical studies on real-world data show that the proposed model could significantly improve the recommendation performance in zero- and few-shot learning settings under both cross-market and cross-platform scenarios.
</details>
<details>
<summary>摘要</summary>
研究者最近对预训条件语言/视觉模型进行了详细的研究，并证明了这种新的解决方案在人工智能中具有实用的优点，即可以透过对广泛的资料进行预训，然后在有限的训练数据下预测成功解决各种下游任务，包括零或几个案例中的学习。这些进步鼓发我们在这篇论文中考虑这种解决方案在推荐系统中的可能性和挑战。我们提议发展一个通用的推荐系统，可以透过对不同领域的用户项目互动资料进行预训，然后快速适应提高几个类别学习中的性能。然而，与视觉语言数据不同的是，推荐数据收集自不同领域（例如不同国家或不同的电子商务平台）的普遍性几何在semantic空间中是由文化差异所隐藏的。在我们的实验中，这种多元偏见在数据中对预训模型的效果产生阻碍。为了解决这个挑战，我们进一步引入和实践了一种 causal debiasing 的观点，这是通过一种层次 Bayesian 深度学习模型，名为 PreRec，实现了。我们的实验结果显示，提案的模型可以在零或几个学习设定下提高推荐性能，包括跨市场和跨平台的enario。
</details></li>
</ul>
<hr>
<h2 id="IMPRESS-Evaluating-the-Resilience-of-Imperceptible-Perturbations-Against-Unauthorized-Data-Usage-in-Diffusion-Based-Generative-AI"><a href="#IMPRESS-Evaluating-the-Resilience-of-Imperceptible-Perturbations-Against-Unauthorized-Data-Usage-in-Diffusion-Based-Generative-AI" class="headerlink" title="IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI"></a>IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19248">http://arxiv.org/abs/2310.19248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aaaaaasuka/impress">https://github.com/aaaaaasuka/impress</a></li>
<li>paper_authors: Bochuan Cao, Changjiang Li, Ting Wang, Jinyuan Jia, Bo Li, Jinghui Chen<br>for: 本研究旨在评估不可见的杂杂性 perturbation 是否能够保护原始图像免于不当使用。methods: 本研究使用了一种基于扩散的图像生成模型，并在这些模型中引入了不可见的杂杂性。results: 研究发现，通过使用不可见的杂杂性，可以减弱图像的保护力，使其更易受到不当使用。此外，研究还提出了一种新的优化策略，可以用于纯化图像，从而增强图像的安全性。<details>
<summary>Abstract</summary>
Diffusion-based image generation models, such as Stable Diffusion or DALL-E 2, are able to learn from given images and generate high-quality samples following the guidance from prompts. For instance, they can be used to create artistic images that mimic the style of an artist based on his/her original artworks or to maliciously edit the original images for fake content. However, such ability also brings serious ethical issues without proper authorization from the owner of the original images. In response, several attempts have been made to protect the original images from such unauthorized data usage by adding imperceptible perturbations, which are designed to mislead the diffusion model and make it unable to properly generate new samples. In this work, we introduce a perturbation purification platform, named IMPRESS, to evaluate the effectiveness of imperceptible perturbations as a protective measure. IMPRESS is based on the key observation that imperceptible perturbations could lead to a perceptible inconsistency between the original image and the diffusion-reconstructed image, which can be used to devise a new optimization strategy for purifying the image, which may weaken the protection of the original image from unauthorized data usage (e.g., style mimicking, malicious editing). The proposed IMPRESS platform offers a comprehensive evaluation of several contemporary protection methods, and can be used as an evaluation platform for future protection methods.
</details>
<details>
<summary>摘要</summary>
diffusion-based图像生成模型，如稳定扩散或DALL-E 2，可以根据给定的图像学习并生成高质量的样本，并且可以根据提示进行指导。例如，它们可以创建imitates艺术家的艺术风格的图像，或者为违反原始图像的作者权利而修改图像。然而，这种能力也会产生严重的道德问题，无法得到原始图像的作者授权。为了保护原始图像，several attempts have been made to add imperceptible perturbations，这些噪声是为了诱导扩散模型，使其无法生成正确的新样本。在这种情况下，我们提出了一个名为IMPRESS的噪声纯化平台，用于评估噪声的有效性。IMPRESS基于关键的观察，即噪声可以导致原始图像和扩散重constructed图像之间的不一致，这可以用来开发一种新的优化策略，以纯化图像，可能弱化原始图像的保护（例如，艺术风格模仿、Malicious editing）。我们的IMPRESS平台可以评估多种当今保护方法的有效性，并且可以用作未来保护方法的评估平台。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-guided-Boundary-Learning-for-Imbalanced-Social-Event-Detection"><a href="#Uncertainty-guided-Boundary-Learning-for-Imbalanced-Social-Event-Detection" class="headerlink" title="Uncertainty-guided Boundary Learning for Imbalanced Social Event Detection"></a>Uncertainty-guided Boundary Learning for Imbalanced Social Event Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19247">http://arxiv.org/abs/2310.19247</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ringbdstack/ucl_sed">https://github.com/ringbdstack/ucl_sed</a></li>
<li>paper_authors: Jiaqian Ren, Hao Peng, Lei Jiang, Zhiwei Liu, Jia Wu, Zhengtao Yu, Philip S. Yu</li>
<li>for: 提高社会事件检测任务中的总体模型性能，特别是对于具有低频率的类型。</li>
<li>methods: 提出了一种基于不确定性指导的类别不均衡学习框架（UCL$<em>{SED}$）和其变种（UCL-EC$</em>{SED}$），通过增强模型对不确定类型的泛化，提高总体模型性能。</li>
<li>results: 在三个严重偏见的社会事件 dataset 上进行了实验，结果显示，我们的模型可以显著改善社会事件表示和分类任务中的几乎所有类型，特别是那些不确定的类型。<details>
<summary>Abstract</summary>
Real-world social events typically exhibit a severe class-imbalance distribution, which makes the trained detection model encounter a serious generalization challenge. Most studies solve this problem from the frequency perspective and emphasize the representation or classifier learning for tail classes. While in our observation, compared to the rarity of classes, the calibrated uncertainty estimated from well-trained evidential deep learning networks better reflects model performance. To this end, we propose a novel uncertainty-guided class imbalance learning framework - UCL$_{SED}$, and its variant - UCL-EC$_{SED}$, for imbalanced social event detection tasks. We aim to improve the overall model performance by enhancing model generalization to those uncertain classes. Considering performance degradation usually comes from misclassifying samples as their confusing neighboring classes, we focus on boundary learning in latent space and classifier learning with high-quality uncertainty estimation. First, we design a novel uncertainty-guided contrastive learning loss, namely UCL and its variant - UCL-EC, to manipulate distinguishable representation distribution for imbalanced data. During training, they force all classes, especially uncertain ones, to adaptively adjust a clear separable boundary in the feature space. Second, to obtain more robust and accurate class uncertainty, we combine the results of multi-view evidential classifiers via the Dempster-Shafer theory under the supervision of an additional calibration method. We conduct experiments on three severely imbalanced social event datasets including Events2012\_100, Events2018\_100, and CrisisLexT\_7. Our model significantly improves social event representation and classification tasks in almost all classes, especially those uncertain ones.
</details>
<details>
<summary>摘要</summary>
实际世界中的社会事件通常会出现严重的类别不均衡分布，这使得训练的检测模型遇到了严重的泛化挑战。大多数研究从频率角度出发，强调表达或类型学习的tail类。而我们所观察到的是，相比于罕见类的数量，从训练得到的准确的深度学习网络中的偏置估计更好地反映模型性能。为此，我们提出了一种基于uncertainty的类别不均衡学习框架-UCL$_{SED}$，以及其变体-UCL-EC$_{SED}$，用于社会事件检测任务中的类别不均衡问题。我们希望通过提高模型对不确定类的泛化来提高整体模型性能。由于性能下降通常来自于误分类邻近类型的样本，我们将注意点放在缓存空间边界学习和类型学习中，使用高质量的不确定估计。首先，我们设计了一种基于uncertainty的对比学习损失函数，即UCL和其变体UCL-EC，用于训练不均衡数据。在训练过程中，它们迫使所有类型，特别是不确定的类型，在特征空间中适应自适应的清晰分界。其次，为了获得更加稳定和准确的类 uncertainty，我们将多视图证据级联推理结果，通过德мп斯特-沙费尔理论进行监督，并采用额外校准方法。我们对三个严重不均衡的社会事件数据集进行实验，包括Events2012\_100、Events2018\_100和CrisisLexT\_7。我们的模型在社会事件表示和分类任务中具有显著改善，特别是不确定的类型。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Configuration-Machines-FPGA-Implementation"><a href="#Stochastic-Configuration-Machines-FPGA-Implementation" class="headerlink" title="Stochastic Configuration Machines: FPGA Implementation"></a>Stochastic Configuration Machines: FPGA Implementation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19225">http://arxiv.org/abs/2310.19225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/plubplub1/bountyfarm">https://github.com/plubplub1/bountyfarm</a></li>
<li>paper_authors: Matthew J. Felicetti, Dianhui Wang</li>
<li>for: 这个论文主要针对工业应用中的神经网络问题，具体来说是对响应速度、内存大小和功耗等约束进行优化。</li>
<li>methods: 这篇论文使用了随机学习器，并且提出了使用硬件解决方案来减少模型的资源占用。特别是，使用了 Stochastic Configuration Machines（SCM）来减少内存约束，并通过限制随机权重为二进制值和使用机制模型来提高学习性和结果可读性。</li>
<li>results: 这篇论文在Field Programmable Gate Array（FPGA）上实现了 SCM 模型，并对两个标准数据集和两个工业数据集进行了测试。结果显示，SCM 模型可以在各种约束下达到比较好的性能。<details>
<summary>Abstract</summary>
Neural networks for industrial applications generally have additional constraints such as response speed, memory size and power usage. Randomized learners can address some of these issues. However, hardware solutions can provide better resource reduction whilst maintaining the model's performance. Stochastic configuration networks (SCNs) are a prime choice in industrial applications due to their merits and feasibility for data modelling. Stochastic Configuration Machines (SCMs) extend this to focus on reducing the memory constraints by limiting the randomized weights to a binary value with a scalar for each node and using a mechanism model to improve the learning performance and result interpretability. This paper aims to implement SCM models on a field programmable gate array (FPGA) and introduce binary-coded inputs to the algorithm. Results are reported for two benchmark and two industrial datasets, including SCM with single-layer and deep architectures.
</details>
<details>
<summary>摘要</summary>
neural networks for industrial applications 通常有额外的约束，如响应速度、内存大小和功耗使用。随机学习者可以解决一些这些问题。然而，硬件解决方案可以提供更好的资源减少，同时保持模型的性能。随机配置网络（SCNs）在工业应用中是一个首选的，因为它们在数据模型方面具有优点和可行性。随机配置机器（SCMs）将这个扩展到减少内存约束，限制随机权重为二进制值，并使用机制模型来提高学习性能和结果解释性。本文将SCM模型在场程可编程阵列（FPGA）上实现，并引入二进制编码输入。对两个标准准样据集和两个工业准样据集进行了报告，包括单层和深度SCM模型。
</details></li>
</ul>
<hr>
<h2 id="EHRTutor-Enhancing-Patient-Understanding-of-Discharge-Instructions"><a href="#EHRTutor-Enhancing-Patient-Understanding-of-Discharge-Instructions" class="headerlink" title="EHRTutor: Enhancing Patient Understanding of Discharge Instructions"></a>EHRTutor: Enhancing Patient Understanding of Discharge Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19212">http://arxiv.org/abs/2310.19212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhang, Zonghai Yao, Huixue Zhou, Feiyun ouyang, Hong Yu</li>
<li>for: 这篇论文的目的是用语言模型作为医疗教育的工具，帮助患者更好地理解他们的诊断和治疗计划。</li>
<li>methods: 这篇论文提出了一种多Component框架，使用大型语言模型（LLM）来实现患者教育，通过对话问答的方式来帮助患者更好地理解他们的电子医疗记录（EHR）出院指南。</li>
<li>results: 论文的评估结果表明，使用EHRTutor可以更好地帮助患者理解他们的诊断和治疗计划，并且可以提高患者对医疗信息的理解和参与度。<details>
<summary>Abstract</summary>
Large language models have shown success as a tutor in education in various fields. Educating patients about their clinical visits plays a pivotal role in patients' adherence to their treatment plans post-discharge. This paper presents EHRTutor, an innovative multi-component framework leveraging the Large Language Model (LLM) for patient education through conversational question-answering. EHRTutor first formulates questions pertaining to the electronic health record discharge instructions. It then educates the patient through conversation by administering each question as a test. Finally, it generates a summary at the end of the conversation. Evaluation results using LLMs and domain experts have shown a clear preference for EHRTutor over the baseline. Moreover, EHRTutor also offers a framework for generating synthetic patient education dialogues that can be used for future in-house system training.
</details>
<details>
<summary>摘要</summary>
大型语言模型在教育领域中展现出了成功，特别是在各种领域中的教育过程中。在医疗上发生访视时，教育病人关于他们的治疗方案是非常重要的，这篇文章介绍了EHRTutor，一个创新的多 ком成分框架，利用大型语言模型（LLM）进行病人教育，通过对话式问答。EHRTutor首先将电子健康记录档案中的出院指令形成问题，然后通过对话教育病人，每个问题都会作为测验。最后，它将问题的摘要给出。评估结果显示，使用LLM和专家的评价都偏好EHRTutor，而且EHRTutor还提供了一个生成人工病人教育对话的框架，可以用于未来的内部系统训练。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-generative-artificial-intelligence-to-simulate-student-learning-behavior"><a href="#Leveraging-generative-artificial-intelligence-to-simulate-student-learning-behavior" class="headerlink" title="Leveraging generative artificial intelligence to simulate student learning behavior"></a>Leveraging generative artificial intelligence to simulate student learning behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19206">http://arxiv.org/abs/2310.19206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songlin Xu, Xinyu Zhang</li>
<li>for: 学会提高学习效果、进步教育研究和设计更有效的教学方法</li>
<li>methods: 使用大语言模型(LLMs)实现学生学习行为的模拟</li>
<li>results: 三个实验结果：第一个实验（N &#x3D; 145）表明模拟学生学习结果与实际学生的各种民族特征相似；第二个实验（N &#x3D; 4524）显示虚拟学生的学习行为逐渐变得更加真实；第三个实验（N &#x3D; 27）表明虚拟学生的学习行为与考试问题、课程材料、参与度和理解水平之间存在紧密的相关性。<details>
<summary>Abstract</summary>
Student simulation presents a transformative approach to enhance learning outcomes, advance educational research, and ultimately shape the future of effective pedagogy. We explore the feasibility of using large language models (LLMs), a remarkable achievement in AI, to simulate student learning behaviors. Unlike conventional machine learning based prediction, we leverage LLMs to instantiate virtual students with specific demographics and uncover intricate correlations among learning experiences, course materials, understanding levels, and engagement. Our objective is not merely to predict learning outcomes but to replicate learning behaviors and patterns of real students. We validate this hypothesis through three experiments. The first experiment, based on a dataset of N = 145, simulates student learning outcomes from demographic data, revealing parallels with actual students concerning various demographic factors. The second experiment (N = 4524) results in increasingly realistic simulated behaviors with more assessment history for virtual students modelling. The third experiment (N = 27), incorporating prior knowledge and course interactions, indicates a strong link between virtual students' learning behaviors and fine-grained mappings from test questions, course materials, engagement and understanding levels. Collectively, these findings deepen our understanding of LLMs and demonstrate its viability for student simulation, empowering more adaptable curricula design to enhance inclusivity and educational effectiveness.
</details>
<details>
<summary>摘要</summary>
学生模拟提供了一种转变性的方法，以提高学习成果，推动教育研究，并最终形成有效的教学方法。我们explore使用大语言模型（LLMs）来模拟学生学习行为。与传统的机器学习预测不同，我们利用LLMs来实例化虚拟学生，并探索学习经验、课程材料、理解水平和参与度之间的细腻相关性。我们的目标不仅是预测学习结果，而是复制学生学习行为和模式。我们验证了这一假设通过三个实验。第一个实验基于N = 145的数据集，模拟学生学习成果从民族数据中，发现与实际学生的各种民族因素之间存在相似性。第二个实验（N = 4524），通过增加评估历史，使虚拟学生的行为越来越真实。第三个实验（N = 27），通过考虑先前知识和课程互动，发现虚拟学生的学习行为与评估问题、课程材料、参与度和理解水平之间存在强相关性。总的来说，这些发现深入了我们对LLMs的理解，并证明了其可行性 для学生模拟，以便更适应性的课程设计，提高包容性和教育效果。
</details></li>
</ul>
<hr>
<h2 id="Can-ChatGPT-advance-software-testing-intelligence-An-experience-report-on-metamorphic-testing"><a href="#Can-ChatGPT-advance-software-testing-intelligence-An-experience-report-on-metamorphic-testing" class="headerlink" title="Can ChatGPT advance software testing intelligence? An experience report on metamorphic testing"></a>Can ChatGPT advance software testing intelligence? An experience report on metamorphic testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19204">http://arxiv.org/abs/2310.19204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang-Hung Luu, Huai Liu, Tsong Yueh Chen</li>
<li>for: 本研究探讨了使用 ChatGPT 提高软件测试智能的潜力，通过一个 métamorphic testing（MT）的实践案例研究。</li>
<li>methods: 本研究使用 ChatGPT 生成métramorphic relations（MR）的候选者，这些 MR 是软件系统的基本性质，通常需要人类智能来标识。这些 MR 候选者由领域专家评估 Correctness。</li>
<li>results: 研究发现，ChatGPT 可以生成新的正确 MR，用于测试多个软件系统。然而，大多数 MR 候选者都是不清晰定义或者错误的，尤其是对于没有被 MT 测试过的系统。 ChatGPT 可以提高软件测试智能，但是人类智能仍然需要参与以确保 MR 的正确性。<details>
<summary>Abstract</summary>
While ChatGPT is a well-known artificial intelligence chatbot being used to answer human's questions, one may want to discover its potential in advancing software testing. We examine the capability of ChatGPT in advancing the intelligence of software testing through a case study on metamorphic testing (MT), a state-of-the-art software testing technique. We ask ChatGPT to generate candidates of metamorphic relations (MRs), which are basically necessary properties of the object program and which traditionally require human intelligence to identify. These MR candidates are then evaluated in terms of correctness by domain experts. We show that ChatGPT can be used to generate new correct MRs to test several software systems. Having said that, the majority of MR candidates are either defined vaguely or incorrect, especially for systems that have never been tested with MT. ChatGPT can be used to advance software testing intelligence by proposing MR candidates that can be later adopted for implementing tests; but human intelligence should still inevitably be involved to justify and rectify their correctness.
</details>
<details>
<summary>摘要</summary>
chatgpt是一个著名的人工智能chatbot，它可以回答人类的问题。我们想要探索chatgpt在软件测试方面的潜力。我们通过一个meta testing（MT）的实践研究，询问chatgpt生成元变换关系（MR）的候选者。这些MR候选者需要人类智能来识别，但chatgpt可以生成新的正确MR来测试许多软件系统。然而，大多数MR候选者都是欠准确的，特别是对于没有经过MT测试的系统。chatgpt可以帮助提高软件测试智能，但是人类智能仍然必须参与以确认和修正其正确性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.AI_2023_10_30/" data-id="clohum94b006wpj885hog79lw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.CL_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T11:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/cs.CL_2023_10_30/">cs.CL - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Partial-Tensorized-Transformers-for-Natural-Language-Processing"><a href="#Partial-Tensorized-Transformers-for-Natural-Language-Processing" class="headerlink" title="Partial Tensorized Transformers for Natural Language Processing"></a>Partial Tensorized Transformers for Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20077">http://arxiv.org/abs/2310.20077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhadra Vadlamannati, Ryan Solgi</li>
<li>for: 本研究旨在提高transformer架构的精度和压缩，以便在自然语言处理和其他机器学习任务中应用。</li>
<li>methods: 我们采用了tensor-train分解来提高BERT和ViT模型的精度和压缩。我们采用了一种新的PTNN方法，通过对embedding层进行压缩和partial tensorization来提高模型的精度。</li>
<li>results: 我们的研究表明，使用PTNN方法可以在BERT和ViT模型中提高精度，最高提高5%，而无需进行后处理调整。这些结果在tensor decomposition领域中做出了新的贡献。<details>
<summary>Abstract</summary>
The transformer architecture has revolutionized Natural Language Processing (NLP) and other machine-learning tasks, due to its unprecedented accuracy. However, their extensive memory and parameter requirements often hinder their practical applications. In this work, we study the effect of tensor-train decomposition to improve the accuracy and compress transformer vision-language neural networks, namely BERT and ViT. We focus both on embedding-layer compression and partial tensorization of neural networks (PTNN) through an algorithmic approach. Our novel PTNN approach significantly improves the accuracy of existing models by up to 5%, all without the need for post-training adjustments, breaking new ground in the field of tensor decomposition.
</details>
<details>
<summary>摘要</summary>
transformer 架构在自然语言处理（NLP）和其他机器学习任务中取得了无 precedent 的精度，但它们的广泛内存和参数需求经常限制其实际应用。在这项工作中，我们研究tensor-train decompositions以提高bert和vit transformer视语言神经网络的准确率和压缩，包括嵌入层压缩和partial tensorization of neural networks（PTNN）。我们的新的PTNN方法可以在不需要后处理调整的前提下，提高现有模型的准确率，最高提高5%。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Evaluation-of-Generative-Models-with-Instruction-Tuning"><a href="#Automatic-Evaluation-of-Generative-Models-with-Instruction-Tuning" class="headerlink" title="Automatic Evaluation of Generative Models with Instruction Tuning"></a>Automatic Evaluation of Generative Models with Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20072">http://arxiv.org/abs/2310.20072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuhaibm/heap">https://github.com/shuhaibm/heap</a></li>
<li>paper_authors: Shuhaib Mehri, Vered Shwartz</li>
<li>for: 本研究旨在自动评估自然语言生成（NLP）中的语言模型。</li>
<li>methods: 该研究使用先前训练的语言模型进行微调，以模拟人类评估标准。</li>
<li>results: 研究发现，通过对HEAP数据集（包含多种NLG任务和评价标准）进行 instrucion 微调，可以获得良好的性能表现，但有些评价标准 harder to learn than others。此外，同时训练多个任务可以提供更好的性能改进，这可能对未来具有有限的人工标注数据的任务有所帮助。<details>
<summary>Abstract</summary>
Automatic evaluation of natural language generation has long been an elusive goal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate human judgements for a particular task and evaluation criterion. Inspired by the generalization ability of instruction-tuned models, we propose a learned metric based on instruction tuning. To test our approach, we collected HEAP, a dataset of human judgements across various NLG tasks and evaluation criteria. Our findings demonstrate that instruction tuning language models on HEAP yields good performance on many evaluation tasks, though some criteria are less trivial to learn than others. Further, jointly training on multiple tasks can yield additional performance improvements, which can be beneficial for future tasks with little to no human annotated data.
</details>
<details>
<summary>摘要</summary>
自然语言生成自动评估长期以来是NLP领域的抢险目标。一种最近的方法是使用预训练语言模型来模拟人类评估标准，并在特定任务和评价标准下进行细化调整。受普遍能力的指示调教模型所启发，我们提议一种学习度量，基于指示调教。为评估我们的方法，我们收集了HEAP数据集，这是各种NLG任务和评价标准下的人类评估判断。我们的发现表明，将语言模型 instrucion tuning 到HEAP数据集上，可以在许多评估任务上达到良好的性能，但有些评价标准更难于学习。此外，同时训练多个任务可以获得额外的性能提升，这可以对未来具有少量或无人标注数据的任务产生帮助。
</details></li>
</ul>
<hr>
<h2 id="Which-Examples-to-Annotate-for-In-Context-Learning-Towards-Effective-and-Efficient-Selection"><a href="#Which-Examples-to-Annotate-for-In-Context-Learning-Towards-Effective-and-Efficient-Selection" class="headerlink" title="Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection"></a>Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20046">http://arxiv.org/abs/2310.20046</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amazon-science/adaptive-in-context-learning">https://github.com/amazon-science/adaptive-in-context-learning</a></li>
<li>paper_authors: Costas Mavromatis, Balasubramaniam Srinivasan, Zhengyuan Shen, Jiani Zhang, Huzefa Rangwala, Christos Faloutsos, George Karypis</li>
<li>for: 本研究旨在提高大型自然语言模型（LLM）在新任务上适应性，通过协助模型在具有有限预算的情况下进行活动学习。</li>
<li>methods: 本研究提出了一种基于模型不确定性和 semantic diversity的活动学习策略，称为AdaICL，可以在有限预算下选择最有价值的示例。</li>
<li>results: 实验结果表明，AdaICL可以提高性能的精度值4.4%，相比SOTA（7.7%相对提高），并且可以在有限预算下选择更多的示例，从而提高效果。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) can adapt to new tasks via in-context learning (ICL). ICL is efficient as it does not require any parameter updates to the trained LLM, but only few annotated examples as input for the LLM. In this work, we investigate an active learning approach for ICL, where there is a limited budget for annotating examples. We propose a model-adaptive optimization-free algorithm, termed AdaICL, which identifies examples that the model is uncertain about, and performs semantic diversity-based example selection. Diversity-based sampling improves overall effectiveness, while uncertainty sampling improves budget efficiency and helps the LLM learn new information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage problem, that dynamically adapts based on the model's feedback and can be approximately solved via greedy algorithms. Extensive experiments on nine datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient than performing annotations uniformly at random, while it outperforms SOTA with 2x fewer ICL examples.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Early-Detection-of-Depression-and-Eating-Disorders-in-Spanish-UNSL-at-MentalRiskES-2023"><a href="#Early-Detection-of-Depression-and-Eating-Disorders-in-Spanish-UNSL-at-MentalRiskES-2023" class="headerlink" title="Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023"></a>Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20003">http://arxiv.org/abs/2310.20003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Horacio Thompson, Marcelo Errecalde</li>
<li>for: 本研究旨在早期探测西班牙语用户的心理障碍问题。</li>
<li>methods: 本文使用基于Transformer的模型，并采用决策政策根据早期探测框架定义的标准。</li>
<li>results: 在任务1和任务2中，我们的方法获得了第二好的表现，包括分类和延迟时间的排名。这表明了我们的方法在西班牙语早期探测问题中的效果和一致性。<details>
<summary>Abstract</summary>
MentalRiskES is a novel challenge that proposes to solve problems related to early risk detection for the Spanish language. The objective is to detect, as soon as possible, Telegram users who show signs of mental disorders considering different tasks. Task 1 involved the users' detection of eating disorders, Task 2 focused on depression detection, and Task 3 aimed at detecting an unknown disorder. These tasks were divided into subtasks, each one defining a resolution approach. Our research group participated in subtask A for Tasks 1 and 2: a binary classification problem that evaluated whether the users were positive or negative. To solve these tasks, we proposed models based on Transformers followed by a decision policy according to criteria defined by an early detection framework. One of the models presented an extended vocabulary with important words for each task to be solved. In addition, we applied a decision policy based on the history of predictions that the model performs during user evaluation. For Tasks 1 and 2, we obtained the second-best performance according to rankings based on classification and latency, demonstrating the effectiveness and consistency of our approaches for solving early detection problems in the Spanish language.
</details>
<details>
<summary>摘要</summary>
MENTALRISKES是一个新的挑战，旨在解决西班牙语早期风险检测中的问题。该挑战的目标是，以最快速的速度可能，检测泰格拉姆用户是否显示精神障碍的迹象。任务1涉及到用户识别饮食障碍，任务2关注于抑郁症检测，任务3旨在检测未知的精神障碍。这些任务被分解成多个子任务，每个子任务定义了解决方案。我们的研究组参与了任务1和2的子任务A：一个二分类问题，以确定用户是否为正或负。为解决这些任务，我们提出了基于转换器的模型，并采用根据早期检测框架定义的决策策略。我们的模型还包括了每个任务的重要词汇扩展 vocabulary。此外，我们还应用了基于历史预测结果的决策策略。在任务1和2中，我们获得了第二名的成绩，根据分类和延迟时间的排名。这表明我们的方法在西班牙语早期检测问题中具有效果和一致性。
</details></li>
</ul>
<hr>
<h2 id="Generative-retrieval-augmented-ontologic-graph-and-multi-agent-strategies-for-interpretive-large-language-model-based-materials-design"><a href="#Generative-retrieval-augmented-ontologic-graph-and-multi-agent-strategies-for-interpretive-large-language-model-based-materials-design" class="headerlink" title="Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design"></a>Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19998">http://arxiv.org/abs/2310.19998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus J. Buehler</li>
<li>For: The paper explores the use of large language models (LLMs) as a tool for engineering analysis of materials, specifically for retrieving key information, developing research hypotheses, discovering mechanistic relationships, and writing and executing simulation codes.* Methods: The paper uses a fine-tuned model called MechGPT, which is developed based on training data in the mechanics of materials domain. The authors also employ retrieval-augmented Ontological Knowledge Graph strategies to address the issue of LLMs recalling correct information outside the context of learned matter.* Results: The paper shows that LLMs can provide powerful problem solution strategies for applications in analysis and design problems, and that retrieval-augmented Ontological Knowledge Graph strategies can provide an interpretable graph structure with rich information at the node, edge, and subgraph level. The authors also discuss nonlinear sampling strategies and agent-based modeling applied to complex question answering, code generation, and execution in the context of automated force field development from actively learned DFT modeling, and data analysis.Here’s the Chinese version of the three key information points:* For: 这篇论文探讨了大语言模型（LLMs）在材料分析和设计中的应用，特别是在检索关键信息、发展研究假设、发现不同领域知识之间的机制关系，以及写入和执行模拟代码方面。* Methods: 这篇论文使用了精度调整的模型——MechGPT，该模型基于机械性物质领域的培训数据进行训练。作者还使用了检索加持知识图的策略来解决 LLMS 在不同领域知识上的回忆问题。* Results: 这篇论文表明了 LLMS 可以为材料分析和设计问题提供强大的问题解决策略，并且使用检索加持知识图的策略可以提供可解释的知识图结构，包括节点、边和子图等级别的信息。作者还讨论了基于非线性抽样策略和智能代理模型的复杂问题回答、代码生成和执行在自动学习DFT模型中的应用。<details>
<summary>Abstract</summary>
Transformer neural networks show promising capabilities, in particular for uses in materials analysis, design and manufacturing, including their capacity to work effectively with both human language, symbols, code, and numerical data. Here we explore the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrieving key information about subject areas, developing research hypotheses, discovery of mechanistic relationships across disparate areas of knowledge, and writing and executing simulation codes for active knowledge generation based on physical ground truths. When used as sets of AI agents with specific features, capabilities, and instructions, LLMs can provide powerful problem solution strategies for applications in analysis and design problems. Our experiments focus on using a fine-tuned model, MechGPT, developed based on training data in the mechanics of materials domain. We first affirm how finetuning endows LLMs with reasonable understanding of domain knowledge. However, when queried outside the context of learned matter, LLMs can have difficulty to recall correct information. We show how this can be addressed using retrieval-augmented Ontological Knowledge Graph strategies that discern how the model understands what concepts are important and how they are related. Illustrated for a use case of relating distinct areas of knowledge - here, music and proteins - such strategies can also provide an interpretable graph structure with rich information at the node, edge and subgraph level. We discuss nonlinear sampling strategies and agent-based modeling applied to complex question answering, code generation and execution in the context of automated force field development from actively learned Density Functional Theory (DFT) modeling, and data analysis.
</details>
<details>
<summary>摘要</summary>
transformer神经网络表现出了扎根的能力，特别是在材料分析、设计和生产领域，包括它们能够与人类语言、符号、代码和数字数据进行有效的交互。我们在这里探索使用大语言模型（LLM）作为工程分析材料的工具，包括检索关键信息、发展研究假设、在不同领域知识之间发现机制关系以及基于物理真实的编程和执行代码。当用作具有特定特征、能力和指令的AI代理时，LLM可以提供有力的问题解决策略。我们的实验将注重使用微调的模型——MechGPT，基于材料力学领域的训练数据进行微调。我们首先证明了训练后LLM具有适当的领域知识理解能力。然而，当被询问在学习的知识外时，LLM可能会办不到正确的回答。我们示出了如何使用检索扩展的 Ontological Knowledge Graph 策略，以便评估模型对概念的理解和关系的扩展。例如，在音乐和蛋白质之间的关系问题上，我们可以提供可读的图structures，并且在节点、边和子图等级具有丰富的信息。我们讨论了不对称采样策略和基于智能代理的问题回答、代码生成和执行在自动学习DFT模型中的应用。
</details></li>
</ul>
<hr>
<h2 id="Strategies-to-Harness-the-Transformers’-Potential-UNSL-at-eRisk-2023"><a href="#Strategies-to-Harness-the-Transformers’-Potential-UNSL-at-eRisk-2023" class="headerlink" title="Strategies to Harness the Transformers’ Potential: UNSL at eRisk 2023"></a>Strategies to Harness the Transformers’ Potential: UNSL at eRisk 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19970">http://arxiv.org/abs/2310.19970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Horacio Thompson, Leticia Cagnina, Marcelo Errecalde</li>
<li>for: 这个研究是为了检测互联网上不同风险的方法。</li>
<li>methods: 这个研究使用了基于Transformers的不同方法，包括基于相似度的上下文化嵌入向量方法和基于提示的机器学习技术。</li>
<li>results: 在这个研究中，我们在两个任务中获得了良好的表现，包括决策基于指标、排名基于指标和运行时间。<details>
<summary>Abstract</summary>
The CLEF eRisk Laboratory explores solutions to different tasks related to risk detection on the Internet. In the 2023 edition, Task 1 consisted of searching for symptoms of depression, the objective of which was to extract user writings according to their relevance to the BDI Questionnaire symptoms. Task 2 was related to the problem of early detection of pathological gambling risks, where the participants had to detect users at risk as quickly as possible. Finally, Task 3 consisted of estimating the severity levels of signs of eating disorders. Our research group participated in the first two tasks, proposing solutions based on Transformers. For Task 1, we applied different approaches that can be interesting in information retrieval tasks. Two proposals were based on the similarity of contextualized embedding vectors, and the other one was based on prompting, an attractive current technique of machine learning. For Task 2, we proposed three fine-tuned models followed by decision policy according to criteria defined by an early detection framework. One model presented extended vocabulary with important words to the addressed domain. In the last task, we obtained good performances considering the decision-based metrics, ranking-based metrics, and runtime. In this work, we explore different ways to deploy the predictive potential of Transformers in eRisk tasks.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)CLEF eRisk Laboratory 研究不同的互联网风险探测任务。2023年版本中，任务1是搜寻抑郁的征候，目标是将用户文章与BDDI询问题的征候相关。任务2是早期检测疯狂赌博风险，参赛者需要快速检测用户是否有风险。最后一个任务是估计进食障碍的严重程度。我们的研究小组参加了第一个和第二个任务，提出了基于传播的解决方案。在任务1中，我们运用了不同的方法，包括相似的上下文化嵌入vector的类比和Prompting技术。在任务2中，我们提出了三个精革化的模型，然后根据早期检测框架的参数进行决策。一个模型增加了重要领域的词汇。在最后一个任务中，我们获得了良好的表现，考虑到决策基于的指标、排名基于的指标和时间。在这个工作中，我们探索了不同的方法来实现传播的预测潜力在eRisk任务中。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Depth-and-Width-on-Transformer-Language-Model-Generalization"><a href="#The-Impact-of-Depth-and-Width-on-Transformer-Language-Model-Generalization" class="headerlink" title="The Impact of Depth and Width on Transformer Language Model Generalization"></a>The Impact of Depth and Width on Transformer Language Model Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19956">http://arxiv.org/abs/2310.19956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jackson Petty, Sjoerd van Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette, Tal Linzen</li>
<li>for: 这个论文目的是探索语言模型（LMs）如何进行 композиional 总结，即将熟悉的元素结合在新的方式下。</li>
<li>methods: 这篇论文使用 transformers 模型，并在不同层数下进行测试，以检验 deeper 模型在 compositional 总结方面的表现。</li>
<li>results: 研究发现，在 fine-tuning 后，深度较大的模型在 out-of-distribution 总结方面表现更好，但随着层数的增加，模型的性能下降速度加剧。此外， deeper 模型在 each family 中的语言模型性能也更高，但返回也随着层数增加而减少。最后，研究发现，depth 对 compositional 总结的好处不能归结于语言模型性能或在预测数据上的表现。<details>
<summary>Abstract</summary>
To process novel sentences, language models (LMs) must generalize compositionally -- combine familiar elements in new ways. What aspects of a model's structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by recent theoretical and empirical work, that transformers generalize more compositionally when they are deeper (have more layers). Because simply adding layers increases the total number of parameters, confounding depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize better out-of-distribution than shallower models do, but the relative benefit of additional layers diminishes rapidly; (2) within each family, deeper models show better language modeling performance, but returns are similarly diminishing; (3) the benefits of depth for compositional generalization cannot be attributed solely to better performance on language modeling or on in-distribution data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>更深的模型在这些任务中的调整后，能够更好地扩展到未知的数据上，但这些模型的优势逐渐减弱。2. 在每个家族中，更深的模型都会表现更好，但回报也逐渐减弱。3. 深度的优势不能完全归因于更好的语言模型性能或在这些数据上的表现。Note:* “compose”in the original text is translated as “扩展”in Simplified Chinese, which means to expand or extend something.* “generalize”in the original text is translated as “扩展”in Simplified Chinese, which means to make something apply to a wider range of cases.* “layer”in the original text is translated as “层”in Simplified Chinese, which refers to a specific level of a neural network.* “parameters”in the original text is translated as “参数”in Simplified Chinese, which refers to the learnable weights and biases of a neural network.</details></li>
</ol>
<hr>
<h2 id="Split-NER-Named-Entity-Recognition-via-Two-Question-Answering-based-Classifications"><a href="#Split-NER-Named-Entity-Recognition-via-Two-Question-Answering-based-Classifications" class="headerlink" title="Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications"></a>Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19942">http://arxiv.org/abs/2310.19942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jatin Arora, Youngja Park</li>
<li>for: 本文解决NER问题，将其分解为两个逻辑子任务： Span Detection 和 Span Classification。</li>
<li>methods: 本文将两个子任务转化为问题解答（QA）问题，生成两个简单的模型，可以独立优化每个子任务。</li>
<li>results: 实验结果表明，这种两步方法比基eline有效，在 Ontotes5.0、WNUT17 和一个Cybersecurity数据集上都超过了基eline，在 BioNLP13CG 上具有相当的性能，同时具有显著降低训练时间的优势。<details>
<summary>Abstract</summary>
In this work, we address the NER problem by splitting it into two logical sub-tasks: (1) Span Detection which simply extracts entity mention spans irrespective of entity type; (2) Span Classification which classifies the spans into their entity types. Further, we formulate both sub-tasks as question-answering (QA) problems and produce two leaner models which can be optimized separately for each sub-task. Experiments with four cross-domain datasets demonstrate that this two-step approach is both effective and time efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17 and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all cases, it achieves a significant reduction in training time compared to its QA baseline counterpart. The effectiveness of our system stems from fine-tuning the BERT model twice, separately for span detection and classification. The source code can be found at https://github.com/c3sr/split-ner.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们解决了NER问题，将其拆分成两个逻辑子任务：（1）Span Detection，它简单地提取实体提及span，不论实体类型；（2）Span Classification，它将span分类为实体类型。然后，我们将这两个子任务转化为问答问题，生成了两个简单的模型，可以分别优化每个子任务。实验表明，我们的SplitNER系统在四个跨领域数据集上表现出色，效果明显优于基eline。具体来说，我们的系统在OntoNotes5.0、WNUT17和一个信息安全数据集上均表现出色，与 BioNLP13CG 的性能相当。同时，我们的系统在训练时间方面也有显著的提升。这种效果源于我们在 BERT 模型上进行了两次细化，分别为 span detection 和 classification。相关的源代码可以在 GitHub 上找到：https://github.com/c3sr/split-ner。
</details></li>
</ul>
<hr>
<h2 id="The-Eval4NLP-2023-Shared-Task-on-Prompting-Large-Language-Models-as-Explainable-Metrics"><a href="#The-Eval4NLP-2023-Shared-Task-on-Prompting-Large-Language-Models-as-Explainable-Metrics" class="headerlink" title="The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics"></a>The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19792">http://arxiv.org/abs/2310.19792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao, Rotem Dror, Steffen Eger</li>
<li>for: 这篇研究的目的是探讨提示和评分的应用在自然语言处理中，特别是在翻译和摘要评估中。</li>
<li>methods: 研究使用了一些已知的大语言模型，并将其用于提示和评分。</li>
<li>results: 研究发现，即使将大型语言模型限制在允许的列表上，仍可以 achieves results on par with or even surpassing recent reference-free metrics developed using larger models。<details>
<summary>Abstract</summary>
With an increasing number of parameters and pre-training data, generative large language models (LLMs) have shown remarkable capabilities to solve tasks with minimal or no task-related examples. Notably, LLMs have been successfully employed as evaluation metrics in text generation tasks. Within this context, we introduce the Eval4NLP 2023 shared task that asks participants to explore prompting and score extraction for machine translation (MT) and summarization evaluation. Specifically, we propose a novel competition setting in which we select a list of allowed LLMs and disallow fine-tuning to ensure a focus on prompting. We present an overview of participants' approaches and evaluate them on a new reference-free test set spanning three language pairs for MT and a summarization dataset. Notably, despite the task's restrictions, the best-performing systems achieve results on par with or even surpassing recent reference-free metrics developed using larger models, including GEMBA and Comet-Kiwi-XXL. Finally, as a separate track, we perform a small-scale human evaluation of the plausibility of explanations given by the LLMs.
</details>
<details>
<summary>摘要</summary>
随着参数和预训练数据的增加，生成大型自然语言模型（LLM）在解决无或少关注任务示例的情况下表现出色。特别是，LLM在文本生成任务中作为评价指标得到了广泛的应用。在这个上下文中，我们介绍了2023年的Eval4NLP任务，询问参与者探索提示和分析抽取在机器翻译（MT）和概要生成评价中的应用。我们提出了一种新的竞赛设定，在选择允许的LLM列表并禁用微调的情况下，以便强调提示。我们对参与者的方法进行了概述，并对新的无参考测试集跨三种语言对MT和概要生成进行评估。尽管任务有限制，最佳系统的表现与或者超过了最近发展的无参考度量器，包括GEMB和Comet-Kiwi-XXL。最后，我们在一个小规模的人工评估中评估了LLM的解释可信度。
</details></li>
</ul>
<hr>
<h2 id="What’s-“up”-with-vision-language-models-Investigating-their-struggle-with-spatial-reasoning"><a href="#What’s-“up”-with-vision-language-models-Investigating-their-struggle-with-spatial-reasoning" class="headerlink" title="What’s “up” with vision-language models? Investigating their struggle with spatial reasoning"></a>What’s “up” with vision-language models? Investigating their struggle with spatial reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19785">http://arxiv.org/abs/2310.19785</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amitakamath/whatsup_vlms">https://github.com/amitakamath/whatsup_vlms</a></li>
<li>paper_authors: Amita Kamath, Jack Hessel, Kai-Wei Chang</li>
<li>for: 本研究用于评估视言模型对基本空间关系的理解能力，以及研究这种能力的限制和改进方法。</li>
<li>methods: 研究人员为此创建了三个新的测试集，以评估模型在不同的空间关系下的表现。这些测试集包括固定对象的不同空间关系，例如同一个狗在不同的桌子上。研究人员还评估了18种视言模型，发现所有模型表现不佳，其中一些模型甚至不能正确地识别狗在桌子上的位置。</li>
<li>results: 研究人员发现，许多现有的视言预训练数据集，如LAION-2B，含有少量可靠的数据，用于学习空间关系。此外，研究人员还发现，基本的模型改进方法，如升重前置词包含的实例或 Fine-tuning 在这些数据集上，并不能解决这些测试集带来的挑战。<details>
<summary>Abstract</summary>
Recent vision-language (VL) models are powerful, but can they reliably distinguish "right" from "left"? We curate three new corpora to quantify model comprehension of such basic spatial relations. These tests isolate spatial reasoning more precisely than existing datasets like VQAv2, e.g., our What'sUp benchmark contains sets of photographs varying only the spatial relations of objects, keeping their identity fixed (see Figure 1: models must comprehend not only the usual case of a dog under a table, but also, the same dog on top of the same table). We evaluate 18 VL models, finding that all perform poorly, e.g., BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56% accuracy on our benchmarks vs. humans at 99%. We conclude by studying causes of this surprising behavior, finding: 1) that popular vision-language pretraining corpora like LAION-2B contain little reliable data for learning spatial relationships; and 2) that basic modeling interventions like up-weighting preposition-containing instances or fine-tuning on our corpora are not sufficient to address the challenges our benchmarks pose. We are hopeful that these corpora will facilitate further research, and we release our data and code at https://github.com/amitakamath/whatsup_vlms.
</details>
<details>
<summary>摘要</summary>
最近的视力语言（VL）模型强大，但它们能够准确地 distinguishes "right" 和 "left" 吗？我们创建了三个新的 corpora 来量化模型对这些基本的空间关系的理解。这些测试更 preciselly than existing datasets like VQAv2, e.g., our What'sUp benchmark contains sets of photographs varying only the spatial relations of objects, keeping their identity fixed (see Figure 1: models must comprehend not only the usual case of a dog under a table, but also, the same dog on top of the same table). We evaluate 18 VL models, finding that all perform poorly, e.g., BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56% accuracy on our benchmarks vs. humans at 99%. We conclude by studying causes of this surprising behavior, finding: 1) that popular vision-language pretraining corpora like LAION-2B contain little reliable data for learning spatial relationships; and 2) that basic modeling interventions like up-weighting preposition-containing instances or fine-tuning on our corpora are not sufficient to address the challenges our benchmarks pose. We are hopeful that these corpora will facilitate further research, and we release our data and code at https://github.com/amitakamath/whatsup_vlms.
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Thought-Embeddings-for-Stance-Detection-on-Social-Media"><a href="#Chain-of-Thought-Embeddings-for-Stance-Detection-on-Social-Media" class="headerlink" title="Chain-of-Thought Embeddings for Stance Detection on Social Media"></a>Chain-of-Thought Embeddings for Stance Detection on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19750">http://arxiv.org/abs/2310.19750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Gatto, Omar Sharif, Sarah Masud Preum</li>
<li>for: 本研究旨在提高语言模型在社交媒体上的立场检测性能，特别是在面临新词汇和口语语言时。</li>
<li>methods: 本研究使用了Chain-of-Thought（COT）提示，并引入了COT Embeddings来改进COT表现。</li>
<li>results: 本研究实现了SOTA的立场检测性能在多个社交媒体上的 datasets。<details>
<summary>Abstract</summary>
Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels. Chain-of-Thought (COT) prompting has recently been shown to improve performance on stance detection tasks -- alleviating some of these issues. However, COT prompting still struggles with implicit stance identification. This challenge arises because many samples are initially challenging to comprehend before a model becomes familiar with the slang and evolving knowledge related to different topics, all of which need to be acquired through the training data. In this study, we address this problem by introducing COT Embeddings which improve COT performance on stance detection tasks by embedding COT reasonings and integrating them into a traditional RoBERTa-based stance detection pipeline. Our analysis demonstrates that 1) text encoders can leverage COT reasonings with minor errors or hallucinations that would otherwise distort the COT output label. 2) Text encoders can overlook misleading COT reasoning when a sample's prediction heavily depends on domain-specific patterns. Our model achieves SOTA performance on multiple stance detection datasets collected from social media.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在社交媒体上进行立场检测是具有挑战性的，因为在线上对话中的新词汇和口语语言经常含有深层次的立场标签。链条思维（COT）推断技术最近在立场检测任务上得到了改进，减轻了一些问题。然而，COT推断仍然努力地处理深层次的立场标识。这个挑战的原因在于许多样本需要模型通过训练数据来学习不同话题的流行语言和演化知识，这些知识需要在模型中建立链条思维。在这种情况下，我们解决这个问题 by introducing COT Embeddings，它们可以改进COT推断的性能在立场检测任务中。我们的分析表明：1）文本编码器可以利用COT推断的小误差或幻见，而不会扭曲COT输出标签。2）文本编码器可以忽略域pecificpatterns中的诱导性COT推断。我们的模型在多个社交媒体上收集的多个立场检测 dataset上达到了顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Evaluation-Exploring-the-Synergy-of-Large-Language-Models-and-Humans-for-Open-ended-Generation-Evaluation"><a href="#Collaborative-Evaluation-Exploring-the-Synergy-of-Large-Language-Models-and-Humans-for-Open-ended-Generation-Evaluation" class="headerlink" title="Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation"></a>Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19740">http://arxiv.org/abs/2310.19740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qtli/coeval">https://github.com/qtli/coeval</a></li>
<li>paper_authors: Qintong Li, Leyang Cui, Lingpeng Kong, Wei Bi</li>
<li>for: 这项研究旨在提高开放类自然语言生成任务（NLG）的评估方法，使其更加准确和可靠。</li>
<li>methods: 这项研究使用了大语言模型（LLM）作为评估工具，并设计了一份任务特定的检查列表来评估文本质量。</li>
<li>results: 研究发现，通过利用LLM，CoEval可以准确地评估长文本，提高评估效率和可靠性，同时仍然保留了人类审核的作用，以确保最终结果的可靠性。<details>
<summary>Abstract</summary>
Humans are widely involved in the evaluation of open-ended natural language generation tasks (NLG) that demand creativity, as automatic metrics often exhibit weak correlations with human judgments. Large language models (LLMs) recently have emerged as a scalable and cost-effective alternative to human evaluations. However, both humans and LLMs have limitations, i.e., inherent subjectivity and unreliable judgments, particularly for open-ended tasks that require adaptable metrics tailored to diverse task requirements. To explore the synergy between humans and LLM-based evaluators and address the challenges of existing inconsistent evaluation criteria in open-ended NLG tasks, we propose a Collaborative Evaluation pipeline CoEval, involving the design of a checklist of task-specific criteria and the detailed evaluation of texts, in which LLM generates initial ideation, and then humans engage in scrutiny. We conducted a series of experiments to investigate the mutual effects between LLMs and humans in CoEval. Results show that, by utilizing LLMs, CoEval effectively evaluates lengthy texts, saving significant time and reducing human evaluation outliers. Human scrutiny still plays a role, revising around 20% of LLM evaluation scores for ultimate reliability.
</details>
<details>
<summary>摘要</summary>
人类广泛参与开放型自然语言生成任务（NLG）的评估，因为自动度量器经常表现出较弱的相关性与人类评价。大型语言模型（LLM）最近在可扩展性和成本效益方面出现为一种可靠的代替方案。然而，人类和 LLM 都有局限性，即内在的主观性和不可靠的评价，特别是开放型任务需要适应性的多样化任务需求。为了探索人类和 LLM 评价者之间的共同作用和解决现有的不一致评价标准问题，我们提出了一个协同评价管道 CoEval，其中包括设计任务特定的标准列表和细化文本评价。在 CoEval 中，LLM 生成初步的想法，然后人类进行审核。我们进行了一系列实验，发现 CoEval 可以有效评估长文本， saves significant time and reduces human evaluation outliers。然而，人类审核仍然发挥着一定的作用，对 LLM 评估得到的分数进行修改，以确保最终的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Combining-Language-Models-For-Specialized-Domains-A-Colorful-Approach"><a href="#Combining-Language-Models-For-Specialized-Domains-A-Colorful-Approach" class="headerlink" title="Combining Language Models For Specialized Domains: A Colorful Approach"></a>Combining Language Models For Specialized Domains: A Colorful Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19708">http://arxiv.org/abs/2310.19708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Eitan, Menachem Pirchi, Neta Glazer, Shai Meital, Gil Ayach, Gidon Krendel, Aviv Shamsian, Aviv Navon, Gil Hetz, Joseph Keshet</li>
<li>for: 这篇论文的目的是将专业领域专有名词和技术词整合到通用语言模型中，以提高自动语音识别系统在特定领域的表现。</li>
<li>methods: 本文使用了一种新的方法，将专业领域的语言模型与通用语言模型整合在一起，并使用颜色指示每个字的属性。实现了一个优化的搜索算法，以有效地处理含有颜色字的推理。</li>
<li>results: 实验结果显示，这种方法可以将专业领域的名词和技术词融入语言任务中，并且可以降低专业领域的误差率无需对通用领域的性能产生影响。<details>
<summary>Abstract</summary>
General purpose language models (LMs) encounter difficulties when processing domain-specific jargon and terminology, which are frequently utilized in specialized fields such as medicine or industrial settings. Moreover, they often find it challenging to interpret mixed speech that blends general language with specialized jargon. This poses a challenge for automatic speech recognition systems operating within these specific domains. In this work, we introduce a novel approach that integrates domain-specific or secondary LM into general-purpose LM. This strategy involves labeling, or "coloring", each word to indicate its association with either the general or the domain-specific LM. We develop an optimized algorithm that enhances the beam search algorithm to effectively handle inferences involving colored words. Our evaluations indicate that this approach is highly effective in integrating jargon into language tasks. Notably, our method substantially lowers the error rate for domain-specific words without compromising performance in the general domain.
</details>
<details>
<summary>摘要</summary>
通用语言模型（LM）在处理域围绕专业术语和技术名词时遇到困难，这些术语和名词在医学或工业领域中非常常见。此外，它们也有 difficulty 处理混合语言，这种语言混合通用语言和专业术语。这对自动语音识别系统在这些具体领域中操作带来了挑战。在这项工作中，我们介绍了一种新的方法，即将域围绕专业语言模型（LM）与通用语言模型（LM）集成。这种策略的核心思想是对每个词语进行标记，以便指示它们与通用语言模型或域围绕语言模型相关。我们开发了一种优化的算法，以便在搜索算法中有效地处理彩色词语的推理。我们的评估结果表明，这种方法在混合语言任务中非常有效，并且可以大幅降低域围绕专业词语的错误率，而无需妨碍通用领域的表现。
</details></li>
</ul>
<hr>
<h2 id="When-Do-Prompting-and-Prefix-Tuning-Work-A-Theory-of-Capabilities-and-Limitations"><a href="#When-Do-Prompting-and-Prefix-Tuning-Work-A-Theory-of-Capabilities-and-Limitations" class="headerlink" title="When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations"></a>When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19698">http://arxiv.org/abs/2310.19698</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aleksandarpetrov/prefix-tuning-theory">https://github.com/aleksandarpetrov/prefix-tuning-theory</a></li>
<li>paper_authors: Aleksandar Petrov, Philip H. S. Torr, Adel Bibi</li>
<li>for: 这个论文主要是研究 Context-based fine-tuning 方法，包括提示、在Context中学习、软提示（也称为提示调整）以及 prefix-tuning，这些方法在 Parameters 的一部分上具有较好的表现，但它们的理论基础却受到了限制。</li>
<li>methods: 这些方法使用 Continuous embedding space 和 discrete token space 进行研究，并证明了这些方法在同样的 Parameters 上是 strictly less expressive than full fine-tuning。</li>
<li>results: 研究发现，虽然 context-based fine-tuning 方法可以很好地启动 pretrained model 中的技能，但它们无法学习新的任务，因为它们无法改变内部模型的关注模式。<details>
<summary>Abstract</summary>
Context-based fine-tuning methods, including prompting, in-context learning, soft prompting (also known as prompt tuning), and prefix-tuning, have gained popularity due to their ability to often match the performance of full fine-tuning with a fraction of the parameters. Despite their empirical successes, there is little theoretical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations. We show that despite the continuous embedding space being more expressive than the discrete token space, soft-prompting and prefix-tuning are strictly less expressive than full fine-tuning, even with the same number of learnable parameters. Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction. This suggests that while techniques like prompting, in-context learning, soft prompting, and prefix-tuning can effectively elicit skills present in the pretrained model, they cannot learn novel tasks that require new attention patterns.
</details>
<details>
<summary>摘要</summary>
Context-based 精度调整方法，包括提示、在 Context 中学习、软提示（也称为提示调整）和 prefix-tuning，因其能够匹配全部 fine-tuning 的性能，而具有许多参数的优势。 despite their empirical successes， there is little theoretical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations. We show that despite the continuous embedding space being more expressive than the discrete token space, soft-prompting and prefix-tuning are strictly less expressive than full fine-tuning, even with the same number of learnable parameters. Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction. This suggests that while techniques like prompting, in-context learning, soft prompting, and prefix-tuning can effectively elicit skills present in the pretrained model, they cannot learn novel tasks that require new attention patterns.Note that the phrase "context-based fine-tuning" is translated as "Context-based 精度调整" in Simplified Chinese, which is a combination of "Context-based" and "精度调整" (fine-tuning).
</details></li>
</ul>
<hr>
<h2 id="Sentiment-Analysis-in-Digital-Spaces-An-Overview-of-Reviews"><a href="#Sentiment-Analysis-in-Digital-Spaces-An-Overview-of-Reviews" class="headerlink" title="Sentiment Analysis in Digital Spaces: An Overview of Reviews"></a>Sentiment Analysis in Digital Spaces: An Overview of Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19687">http://arxiv.org/abs/2310.19687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura E. M. Ayravainen, Joanne Hinds, Brittany I. Davidson</li>
<li>for: 本研究是一篇系统性回顾，总结了38篇系统性回顾，包含2275个首要研究。</li>
<li>methods: 本研究使用了一套自定义的质量评估框架，用于评估系统性回顾的方法质量和报告标准。</li>
<li>results: 研究发现了不同的应用和方法，报告质量有限，以及时间的挑战。Translation:</li>
<li>for: This study is a systematic review that summarizes 38 systematic reviews and 2,275 primary studies.</li>
<li>methods: The study uses a bespoke quality assessment framework to evaluate the rigor and quality of systematic review methodologies and reporting standards.</li>
<li>results: The study finds diverse applications and methods, limited reporting quality, and challenges over time.<details>
<summary>Abstract</summary>
Sentiment analysis (SA) is commonly applied to digital textual data, revealing insight into opinions and feelings. Many systematic reviews have summarized existing work, but often overlook discussions of validity and scientific practices. Here, we present an overview of reviews, synthesizing 38 systematic reviews, containing 2,275 primary studies. We devise a bespoke quality assessment framework designed to assess the rigor and quality of systematic review methodologies and reporting standards. Our findings show diverse applications and methods, limited reporting rigor, and challenges over time. We discuss how future research and practitioners can address these issues and highlight their importance across numerous applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MoCa-Measuring-Human-Language-Model-Alignment-on-Causal-and-Moral-Judgment-Tasks"><a href="#MoCa-Measuring-Human-Language-Model-Alignment-on-Causal-and-Moral-Judgment-Tasks" class="headerlink" title="MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks"></a>MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19677">http://arxiv.org/abs/2310.19677</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cicl-stanford/moca">https://github.com/cicl-stanford/moca</a></li>
<li>paper_authors: Allen Nie, Yuhui Zhang, Atharva Amdekar, Chris Piech, Tatsunori Hashimoto, Tobias Gerstenberg</li>
<li>For: The paper aims to investigate how well large language models (LLMs) align with human intuitions in making causal and moral judgments about text-based scenarios.* Methods: The paper uses a dataset of stories from 24 cognitive science papers and develops a system to annotate each story with the factors investigated. The authors then test the alignment of LLMs with human participants’ judgments using statistical analyses.* Results: The results show that while LLMs have improved in aligning with human participants’ judgments in recent years, they still weigh the different factors quite differently. The study demonstrates the importance of curated, challenge datasets combined with insights from cognitive science to evaluate LLMs’ performance and understand their implicit tendencies.<details>
<summary>Abstract</summary>
Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.
</details>
<details>
<summary>摘要</summary>
人类常识理解物理和社会世界是通过直觉理论来组织的。这些理论支持我们作出 causal 和 moral 判断。当omething bad happens，我们就会自然地问：who did what, and why？一大量的认知科学研究已经研究了人们的 causal 和 moral 直觉。这些研究发现了一些系统性地影响人们的判断的因素，如违反 norms 和是否可避免或不可避免。我们收集了 24 篇认知科学论文中的故事，并开发了一个系统来注释每个故事中 investigate 的因素。使用这个数据集，我们测试了大语言模型（LLMs）是否对文本场景中的 causal 和 moral 判断与人类参与者的判断相align。在聚合水平上，与更新的 LLMs 相比，Alignment 有所提高。然而，通过统计分析，我们发现 LLMs 对不同因素进行了不同的评重。这些结果表明，可以通过精心编辑、挑战数据集和认知科学的知识来让 LLMs 的偏好和人类直觉之间进行比较，以了解 LLMS 的隐藏偏好是否与人类直觉相align。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-by-Design-Text-Classification-with-Iteratively-Generated-Concept-Bottleneck"><a href="#Interpretable-by-Design-Text-Classification-with-Iteratively-Generated-Concept-Bottleneck" class="headerlink" title="Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck"></a>Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19660">http://arxiv.org/abs/2310.19660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Magnus Ludan, Qing Lyu, Yue Yang, Liam Dugan, Mark Yatskar, Chris Callison-Burch</li>
<li>for: 提高文本分类任务的可解释性，使深度神经网络在高风险领域得到应用。</li>
<li>methods: 提出文本瓶颈模型（TBM），一种内在可解释的文本分类框架，通过预测稀缺概念的分类值来生成最终预测。概念可以通过大自然语言模型（LLM）自动发现和衡量，无需人工准备。</li>
<li>results: 在12种多样化的 dataset 上，使用 GPT-4 生成和测量概念，TBM 可以与已有的黑盒模型相比，其性能与 GPT-4 fewshot 和 DeBERTa 训练后的性能相似，而与 GPT-3.5 训练后的性能相差不大。总之，我们的发现表明，TBM 是一种有前途的新框架，它可以增强可解释性，并且减少性能损失，特别是在通用领域的文本分类任务上。<details>
<summary>Abstract</summary>
Deep neural networks excel in text classification tasks, yet their application in high-stakes domains is hindered by their lack of interpretability. To address this, we propose Text Bottleneck Models (TBMs), an intrinsically interpretable text classification framework that offers both global and local explanations. Rather than directly predicting the output label, TBMs predict categorical values for a sparse set of salient concepts and use a linear layer over those concept values to produce the final prediction. These concepts can be automatically discovered and measured by a Large Language Model (LLM), without the need for human curation. On 12 diverse datasets, using GPT-4 for both concept generation and measurement, we show that TBMs can rival the performance of established black-box baselines such as GPT-4 fewshot and finetuned DeBERTa, while falling short against finetuned GPT-3.5. Overall, our findings suggest that TBMs are a promising new framework that enhances interpretability, with minimal performance tradeoffs, particularly for general-domain text.
</details>
<details>
<summary>摘要</summary>
深度神经网络在文本分类任务中表现出色，但在高风险领域应用受其解释性的限制。为解决这问题，我们提议文本瓶颈模型（TBM），一种内在可解释的文本分类框架，可以提供全局和局部解释。TBM不直接预测输出标签，而是预测一个稀缺的核心概念的 categorical 值，然后使用这些概念值进行线性变换来生成最终预测。这些概念可以通过大语言模型（LLM）自动发现和测量，无需人工筛选。在12种多样化的数据集上，使用 GPT-4 进行概念生成和测量，我们发现TBM可以与已有的黑盒子基eline相比，在通用领域文本中表现出类似的性能，而与训练过 GPT-3.5 的情况下表现略为落后。总的来说，我们的发现表明TBM是一种有前途的新框架，可以增强解释性，而无需付出明显的性能成本，特别是在通用领域文本中。
</details></li>
</ul>
<hr>
<h2 id="Dynamics-of-Instruction-Tuning-Each-Ability-of-Large-Language-Models-Has-Its-Own-Growth-Pace"><a href="#Dynamics-of-Instruction-Tuning-Each-Ability-of-Large-Language-Models-Has-Its-Own-Growth-Pace" class="headerlink" title="Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace"></a>Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19651">http://arxiv.org/abs/2310.19651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiyu Song, Zhanchao Zhou, Jianhao Yan, Yuejiao Fei, Zhenzhong Lan, Yue Zhang</li>
<li>for: 这种研究旨在了解数据构建指南，以优化大语言模型（LLM）的通用智能。</li>
<li>methods: 研究人员使用了多种数据量和参数大小来调查模型的各种能力的发展，包括创意写作、代码生成和逻辑推理等。</li>
<li>results: 研究发现，虽然数据量和参数 scale直接影响模型的总性能，但一些能力更sensitive于数据量和参数的增加，而其他一些能力却很难受到这些变化的影响。此外，人类审核的数据能够在数据量增加时Constantly improve模型性能，而Synthetic数据则不可能达到这种效果。<details>
<summary>Abstract</summary>
Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). However, the creation of instruction data is still largely heuristic, leading to significant variation in quality and distribution across existing datasets. Experimental conclusions drawn from these datasets are also inconsistent, with some studies emphasizing the importance of scaling instruction numbers, while others argue that a limited number of samples suffice. To better understand data construction guidelines, we deepen our focus from the overall model performance to the growth of each underlying ability, such as creative writing, code generation, and logical reasoning. We systematically investigate the effects of data volume, parameter size, and data construction methods on the development of various abilities, using hundreds of model checkpoints (7b to 33b) fully instruction-tuned on a new collection of over 40k human-curated instruction data. This proposed dataset is stringently quality-controlled and categorized into ten distinct LLM abilities. Our study reveals three primary findings: (i) Despite data volume and parameter scale directly impacting models' overall performance, some abilities are more responsive to their increases and can be effectively trained using limited data, while some are highly resistant to these changes. (ii) Human-curated data strongly outperforms synthetic data from GPT-4 in efficiency and can constantly enhance model performance with volume increases, but is unachievable with synthetic data. (iii) Instruction data brings powerful cross-ability generalization, with evaluation results on out-of-domain data mirroring the first two observations. Furthermore, we demonstrate how these findings can guide more efficient data constructions, leading to practical performance improvements on public benchmarks.
</details>
<details>
<summary>摘要</summary>
instrucion 调教是一种迅速发展的方法，用于激发大型自然语言模型（LLM）的通用智能。然而，创建 instrucion 数据仍然受限于较为规则的创建方法，导致现有数据集的质量和分布存在显著的差异。这些数据集的实验结论也存在差异，一些研究认为需要扩大 instrucion 的数量，而其他研究则认为只需要一些样本即可。为了更好地理解数据构建指南，我们将我们的注意力从整体模型性能深入调整为每个基础能力的发展，如创作写作、代码生成和逻辑推理。我们系统地 investigate 数据量、参数大小和数据构建方法对模型的不同能力的影响，使用了百种模型检查点（7b-33b），全面 instruction-tuned 在一个新收集的人类检查的 instrucion 数据集上。这个提议的数据集 stringently 质控rolled 并分为十种不同的 LLM 能力。我们的研究发现了以下三个主要结论：（i）尽管数据量和参数缩放直接影响模型的总性能，但一些能力更sensitive于这些变化，可以使用有限数据进行有效地训练，而一些能力却具有很高的抵抗力。（ii）人类检查的数据在效率和可 reuse 方面胜过 GPT-4 生成的 sintetic 数据，但是不可能通过 sintetic 数据进行持续改进。（iii） instrucion 数据带来了强大的跨能力泛化，评估结果表明，模型在尝试数据上的性能和数据集上的性能呈现相似的趋势。此外，我们还示出了如何将这些发现应用于更有效的数据构建，以实现公共benchmark上的实践性能提升。
</details></li>
</ul>
<hr>
<h2 id="KeyGen2Vec-Learning-Document-Embedding-via-Multi-label-Keyword-Generation-in-Question-Answering"><a href="#KeyGen2Vec-Learning-Document-Embedding-via-Multi-label-Keyword-Generation-in-Question-Answering" class="headerlink" title="KeyGen2Vec: Learning Document Embedding via Multi-label Keyword Generation in Question-Answering"></a>KeyGen2Vec: Learning Document Embedding via Multi-label Keyword Generation in Question-Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19650">http://arxiv.org/abs/2310.19650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iftitahu Ni’mah, Samaneh Khoshrou, Vlado Menkovski, Mykola Pechenizkiy</li>
<li>for: 本研究旨在释放标签超级视的依赖性，通过Sequence-to-Sequence（Seq2Seq）文本生成器来学习文档嵌入。</li>
<li>methods: 我们将关键短语生成任务转化为多标签关键词生成在社区基于Question Answering（cQA）中。</li>
<li>results: 我们的实验结果表明，KeyGen2Vec在整体上比多标签关键词分类器高效，最高达14.7%的纯度、 нор化共享信息（NMI）和F1-Score指标。 Interestingly，尽管在评估数据集上，获得标签超级视的学习嵌入的绝对优势极高，KeyGen2Vec在Yahoo! cQA中的更多标签 Label Supervision情况下与类ifier竞争。<details>
<summary>Abstract</summary>
Representing documents into high dimensional embedding space while preserving the structural similarity between document sources has been an ultimate goal for many works on text representation learning. Current embedding models, however, mainly rely on the availability of label supervision to increase the expressiveness of the resulting embeddings. In contrast, unsupervised embeddings are cheap, but they often cannot capture implicit structure in target corpus, particularly for samples that come from different distribution with the pretraining source.   Our study aims to loosen up the dependency on label supervision by learning document embeddings via Sequence-to-Sequence (Seq2Seq) text generator. Specifically, we reformulate keyphrase generation task into multi-label keyword generation in community-based Question Answering (cQA). Our empirical results show that KeyGen2Vec in general is superior than multi-label keyword classifier by up to 14.7% based on Purity, Normalized Mutual Information (NMI), and F1-Score metrics. Interestingly, although in general the absolute advantage of learning embeddings through label supervision is highly positive across evaluation datasets, KeyGen2Vec is shown to be competitive with classifier that exploits topic label supervision in Yahoo! cQA with larger number of latent topic labels.
</details>
<details>
<summary>摘要</summary>
现有的文本表示学习模型主要依靠标签超vision来增加表示结果的表达力。然而，这些标签超vision通常是可获得的，但它们经常无法捕捉目标句子的隐式结构，特别是来自不同分布的样本。我们的研究旨在减少依赖于标签超vision的关系，通过使用序列到序列（Seq2Seq）文本生成器来学习文档表示。我们将关键短语生成任务转换为多标签关键词生成在社区基于问答（cQA）中。我们的实验结果显示，KeyGen2Vec在整体上高于多标签关键词分类器，具体来说，与Purity、Normalized Mutual Information（NMI）和F1-Score度量相比，KeyGen2Vec在Yahoo! cQA上表现出14.7%的绝对优势。虽然在评估数据集上，通过标签超vision学习表示的绝对优势通常很高，但KeyGen2Vec在Yahoo! cQA上与具有更多隐藏话题标签的类ifier竞争。
</details></li>
</ul>
<hr>
<h2 id="DPATD-Dual-Phase-Audio-Transformer-for-Denoising"><a href="#DPATD-Dual-Phase-Audio-Transformer-for-Denoising" class="headerlink" title="DPATD: Dual-Phase Audio Transformer for Denoising"></a>DPATD: Dual-Phase Audio Transformer for Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19588">http://arxiv.org/abs/2310.19588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhui Li, Pu Wang, Jialu Li, Xinzhe Wang, Youshan Zhang</li>
<li>for: 这个论文主要针对的问题是如何使用更小的声音序列来提高声音提取效果，以解决长声音序列的处理困难。</li>
<li>methods: 该论文提出了一种新的双相声音变换器（DPATD），该模型通过将声音输入分成更小的尺寸，使得模型可以更有效地使用声音信息。DPATD还使用了内存压缩的解释性注意力模块，可以更快速地收敛。</li>
<li>results: 对于声音提取任务，DPATD模型比现有的方法更高效，并且可以更好地处理长声音序列。<details>
<summary>Abstract</summary>
Recent high-performance transformer-based speech enhancement models demonstrate that time domain methods could achieve similar performance as time-frequency domain methods. However, time-domain speech enhancement systems typically receive input audio sequences consisting of a large number of time steps, making it challenging to model extremely long sequences and train models to perform adequately. In this paper, we utilize smaller audio chunks as input to achieve efficient utilization of audio information to address the above challenges. We propose a dual-phase audio transformer for denoising (DPATD), a novel model to organize transformer layers in a deep structure to learn clean audio sequences for denoising. DPATD splits the audio input into smaller chunks, where the input length can be proportional to the square root of the original sequence length. Our memory-compressed explainable attention is efficient and converges faster compared to the frequently used self-attention module. Extensive experiments demonstrate that our model outperforms state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:现代高性能 transformer 基于 speech 增强模型表明，时域方法可以达到相似的性能，与时域频谱方法相比。然而，时域speech 增强系统通常处理大量的音频数据，这使得模型模型 extremely long sequences 和训练模型成为挑战。在这篇文章中，我们使用 smaller audio chunks 作为输入，以实现高效地利用音频信息。我们提出了一种 dual-phase audio transformer for denoising (DPATD)，这是一种新的模型，用于在 deep structure 中学习干净的音频序列。DPATD 将 audio 输入拆分成 smaller chunks，其输入长度与原始序列长度的平方根成正比。我们的 memory-compressed explainable attention 具有高效性和快速收敛，与常用的 self-attention 模块相比。广泛的实验表明，我们的模型超过了当前最佳方法。
</details></li>
</ul>
<hr>
<h2 id="Improving-Input-label-Mapping-with-Demonstration-Replay-for-In-context-Learning"><a href="#Improving-Input-label-Mapping-with-Demonstration-Replay-for-In-context-Learning" class="headerlink" title="Improving Input-label Mapping with Demonstration Replay for In-context Learning"></a>Improving Input-label Mapping with Demonstration Replay for In-context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19572">http://arxiv.org/abs/2310.19572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuocheng Gong, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan</li>
<li>for: 提高大型自然语言处理模型（LLM）的下游任务理解能力，使其能够通过几个输入标签示例来增强其对输入的理解，而无需直接调整模型参数。</li>
<li>methods: 我们提出了一种新的ICL方法，即重复示例with Sliding Causal Attention（RdSca）。我们在示例后 duplicates later demonstrations and concatenates them to the front, allowing the model to &#96;observe’ the later information even under the causal restriction。此外，我们引入了滑动 causal attention，以适应 causal attention 的自适应。</li>
<li>results: 我们的方法在ICL示例中显著提高了输入标签之间的映射。我们还进行了深入的分析，探讨如何适应 causal attention 的自适应，这是在前一个研究中未explored的领域。<details>
<summary>Abstract</summary>
In-context learning (ICL) is an emerging capability of large autoregressive language models where a few input-label demonstrations are appended to the input to enhance the model's understanding of downstream NLP tasks, without directly adjusting the model parameters. The effectiveness of ICL can be attributed to the strong language modeling capabilities of large language models (LLMs), which enable them to learn the mapping between input and labels based on in-context demonstrations. Despite achieving promising results, the causal nature of language modeling in ICL restricts the attention to be backward only, i.e., a token only attends to its previous tokens, failing to capture the full input-label information and limiting the model's performance. In this paper, we propose a novel ICL method called Repeated Demonstration with Sliding Causal Attention, (RdSca). Specifically, we duplicate later demonstrations and concatenate them to the front, allowing the model to `observe' the later information even under the causal restriction. Besides, we introduce sliding causal attention, which customizes causal attention to avoid information leakage. Experimental results show that our method significantly improves the input-label mapping in ICL demonstrations. We also conduct an in-depth analysis of how to customize the causal attention without training, which has been an unexplored area in previous research.
</details>
<details>
<summary>摘要</summary>
卷积语言模型（ICL）是一种现代语言模型技术，通过在输入上附加一些标签示例来提高模型对下游自然语言处理任务的理解，而不需要直接调整模型参数。ICL的效果可以归结于大型语言模型（LLM）的强语言模型能力，它们能够基于示例学习映射输入和标签之间的关系。然而，ICL中的语言模型归因约束限制了注意力的向前传递，即每个token只能注意前一个token，这会导致模型表现有限。在这篇论文中，我们提出了一种新的ICL方法，即重复示例与滑动 causal attention（RdSca）。具体来说，我们将后续示例重复并 concatenate 到输入的开头，这样允许模型在 causal 约束下 still 可以 observe 后续信息。此外，我们引入了滑动 causal attention，以适应不同的输入和标签信息。实验结果表明，我们的方法可以显著提高ICL示例中的输入-标签映射。此外，我们还进行了对自然语言处理任务的深入分析，以探讨在不需要训练的情况下如何自适应 causal attention。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Representation-to-Improve-Team-Problem-Solving-in-Real-Time"><a href="#A-Novel-Representation-to-Improve-Team-Problem-Solving-in-Real-Time" class="headerlink" title="A Novel Representation to Improve Team Problem Solving in Real-Time"></a>A Novel Representation to Improve Team Problem Solving in Real-Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19539">http://arxiv.org/abs/2310.19539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Doboli</li>
<li>for: 本研究提出了一种新的表示方式，用于在实时 situations 中提高团队的行为理解和改进。尽管团队在现代活动中非常重要，但是计算机没有提供了多少帮助来提高团队的活动。</li>
<li>methods: 本研究使用了一种新的表示方式，用于捕捉团队成员在解决问题时发展、加强和使用的不同的心理图像。</li>
<li>results: 一个案例研究表明，该表示方式可以帮助理解和改进团队的行为。<details>
<summary>Abstract</summary>
This paper proposes a novel representation to support computing metrics that help understanding and improving in real-time a team's behavior during problem solving in real-life. Even though teams are important in modern activities, there is little computing aid to improve their activity. The representation captures the different mental images developed, enhanced, and utilized during solving. A case study illustrates the representation.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的表示方式，用于支持在实时中对团队的行为进行理解和改进。即使团队在现代活动中具有重要地位， yet there is little computing aid to improve their activity. 该表示方式 capture了解决过程中发展、加强和使用的不同MENTAL IMAGES。一个案例研究 illustrate了该表示方式。Note: "MENTAL IMAGES" in the original text is translated as "不同MENTAL IMAGES" in Simplified Chinese, as there is no direct equivalent of "mental images" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="InfoEntropy-Loss-to-Mitigate-Bias-of-Learning-Difficulties-for-Generative-Language-Models"><a href="#InfoEntropy-Loss-to-Mitigate-Bias-of-Learning-Difficulties-for-Generative-Language-Models" class="headerlink" title="InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models"></a>InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19531">http://arxiv.org/abs/2310.19531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenpeng Su, Xing Wu, Xue Bai, Zijia Lin, Hui Chen, Guiguang Ding, Wei Zhou, Songlin Hu</li>
<li>for: 这个论文的目的是提高大型生成语言模型在下游任务中的表现，并且解决在训练时文本资料异常分布的问题。</li>
<li>methods: 该论文提出了一种信息熵损失函数（InfoEntropy Loss），用于在训练时动态评估学习困难度的token，并根据这种损失函数进行自适应规则。</li>
<li>results: 在PILE数据集上，通过不同缩放因子的训练，实验表明，将InfoEntropy Loss函数添加到生成语言模型训练中可以持续提高下游任务表现。<details>
<summary>Abstract</summary>
Generative language models are usually pretrained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose an Information Entropy Loss (InfoEntropy Loss) function. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn tokens. On the Pile dataset, we train generative language models at different scales of 436M, 1.1B, and 6.7B parameters. Experiments reveal that models incorporating the proposed InfoEntropy Loss can gain consistent performance improvement on downstream benchmarks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。</SYS>>大多数生成语言模型通常通过预测下一个token（即子字/词/短语）来在前一个token的基础上进行预训练。 current works have shown that large generative language models can achieve impressive performance on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpora during training, i.e., the imbalance between frequent tokens and infrequent ones. This can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To address this, we propose an Information Entropy Loss (InfoEntropy Loss) function. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn tokens. On the Pile dataset, we train generative language models at different scales of 436M, 1.1B, and 6.7B parameters. Experiments show that models incorporating the proposed InfoEntropy Loss can gain consistent performance improvement on downstream benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Constituency-Parsing-using-LLMs"><a href="#Constituency-Parsing-using-LLMs" class="headerlink" title="Constituency Parsing using LLMs"></a>Constituency Parsing using LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19462">http://arxiv.org/abs/2310.19462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Bai, Jialong Wu, Yulong Chen, Zhongqing Wang, Yue Zhang</li>
<li>for: 本研究旨在探讨最新的大语言模型（LLMs）是否可以解决基本 yet unsolved的自然语言处理任务——构成分析。</li>
<li>methods: 我们使用三种线性化策略将输出树转换成符号序列，以便 LLMS 可以通过生成线性化树来解决构成分析。</li>
<li>results: 我们在一个多样化的 LLM 中进行了实验，包括 ChatGPT、GPT-4、OPT、LLaMA 和 Alpaca，并与现有的状态机制构成解析器进行比较。我们在零shot、几shot 和全训练学习 Setting 中进行了实验，并在一个内部测试集和五个外部测试集上评估了模型的性能。我们的实验结果显示了 LLMS 的性能、泛化能力以及构成分析中的挑战。<details>
<summary>Abstract</summary>
Constituency parsing is a fundamental yet unsolved natural language processing task. In this paper, we explore the potential of recent large language models (LLMs) that have exhibited remarkable performance across various domains and tasks to tackle this task. We employ three linearization strategies to transform output trees into symbol sequences, such that LLMs can solve constituency parsing by generating linearized trees. We conduct experiments using a diverse range of LLMs, including ChatGPT, GPT-4, OPT, LLaMA, and Alpaca, comparing their performance against the state-of-the-art constituency parsers. Our experiments encompass zero-shot, few-shot, and full-training learning settings, and we evaluate the models on one in-domain and five out-of-domain test datasets. Our findings reveal insights into LLMs' performance, generalization abilities, and challenges in constituency parsing.
</details>
<details>
<summary>摘要</summary>
《干部分析是自然语言处理中的基本任务，但是这个任务还未得到解决。在这篇论文中，我们探索了最近的大语言模型（LLMs）在不同领域和任务中表现出色的潜力，以解决这个任务。我们采用三种线性化策略将输出树转换为符号序列，使LLMs可以通过生成线性化树来解决干部分析。我们在多种LLMs上进行实验，包括ChatGPT、GPT-4、OPT、LLaMA和Alpaca，并与现有的状态机构分析器进行比较。我们的实验包括零shot、几shot和全training学习设定，并在一个内域测试集和五个外域测试集上评估模型的表现。我们的发现反映了LLMs的表现、泛化能力和干部分析中的挑战。》
</details></li>
</ul>
<hr>
<h2 id="Mean-BERTs-make-erratic-language-teachers-the-effectiveness-of-latent-bootstrapping-in-low-resource-settings"><a href="#Mean-BERTs-make-erratic-language-teachers-the-effectiveness-of-latent-bootstrapping-in-low-resource-settings" class="headerlink" title="Mean BERTs make erratic language teachers: the effectiveness of latent bootstrapping in low-resource settings"></a>Mean BERTs make erratic language teachers: the effectiveness of latent bootstrapping in low-resource settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19420">http://arxiv.org/abs/2310.19420</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Samuel</li>
<li>for: 这篇论文探讨了使用幽杂启动，一种受欢迎的自我超级vised学习技术，用于预训语言模型。</li>
<li>methods: 这篇论文使用了contextualized embeddings来提供更 ricoh的自我超级vised学习信号，而不是使用典型的自我超级vised学习方法。</li>
<li>results: 我们的实验表明，使用幽杂启动可以有效地从有限资源中获取语言知识。我们在BabyLM共同任务中进行了实验，该任务包括预训两个小型 curaated corpus，并在四个语言标准测试上进行评估。<details>
<summary>Abstract</summary>
This paper explores the use of latent bootstrapping, an alternative self-supervision technique, for pretraining language models. Unlike the typical practice of using self-supervision on discrete subwords, latent bootstrapping leverages contextualized embeddings for a richer supervision signal. We conduct experiments to assess how effective this approach is for acquiring linguistic knowledge from limited resources. Specifically, our experiments are based on the BabyLM shared task, which includes pretraining on two small curated corpora and an evaluation on four linguistic benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Lightweight-Method-to-Generate-Unanswerable-Questions-in-English"><a href="#A-Lightweight-Method-to-Generate-Unanswerable-Questions-in-English" class="headerlink" title="A Lightweight Method to Generate Unanswerable Questions in English"></a>A Lightweight Method to Generate Unanswerable Questions in English</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19403">http://arxiv.org/abs/2310.19403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uds-lsv/unanswerable-question-generation">https://github.com/uds-lsv/unanswerable-question-generation</a></li>
<li>paper_authors: Vagrant Gautam, Miaoran Zhang, Dietrich Klakow</li>
<li>for: 这个论文主要是为了提出一种简单且强大的问答系统模型，以解决现有的自动问答系统模型具有过于复杂的问题。</li>
<li>methods: 该论文使用了一种名为“换句法”的数据生成方法，通过将答案与问题进行反义和实体交换来生成无法答题的问题。</li>
<li>results: 相比之前的状态艺术，使用该数据生成方法可以获得更好的模型 (+1.6 F1点在SQuAD 2.0数据上，使用BERT-large），并且人类评分的相关性和可读性更高。<details>
<summary>Abstract</summary>
If a question cannot be answered with the available information, robust systems for question answering (QA) should know _not_ to answer. One way to build QA models that do this is with additional training data comprised of unanswerable questions, created either by employing annotators or through automated methods for unanswerable question generation. To show that the model complexity of existing automated approaches is not justified, we examine a simpler data augmentation method for unanswerable question generation in English: performing antonym and entity swaps on answerable questions. Compared to the prior state-of-the-art, data generated with our training-free and lightweight strategy results in better models (+1.6 F1 points on SQuAD 2.0 data with BERT-large), and has higher human-judged relatedness and readability. We quantify the raw benefits of our approach compared to no augmentation across multiple encoder models, using different amounts of generated data, and also on TydiQA-MinSpan data (+9.3 F1 points with BERT-large). Our results establish swaps as a simple but strong baseline for future work.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:如果问题无法由已有的信息回答，then robust question answering（QA）系统应该知道不回答。一种方式建立QA模型是通过额外的训练数据包括无法回答的问题，使用注解员或自动生成无法回答问题的方法。为了证明现有的自动化方法的模型复杂性不当，我们研究一种更简单的数据增强方法 для无法回答问题的生成：在可回答问题上进行反义和实体交换。与过去的状态艺术比较，我们的训练自由和轻量级策略生成的数据比之前的状态艺术更好，在 SQuAD 2.0 数据上使用 BERT-large 模型时，提高了 +1.6 F1 分数。此外，我们还评估了人类评分的相关性和可读性，发现我们的方法在这两个方面都有了显著的提升。我们对不同的编码器模型进行了评估，使用不同的生成数据量，并在 TydiQA-MinSpan 数据上 (+9.3 F1 分数 with BERT-large) 得到了类似的结果。我们的结果证明了交换是一种简单 yet strong 的基线 для未来的工作。
</details></li>
</ul>
<hr>
<h2 id="Japanese-SimCSE-Technical-Report"><a href="#Japanese-SimCSE-Technical-Report" class="headerlink" title="Japanese SimCSE Technical Report"></a>Japanese SimCSE Technical Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19349">http://arxiv.org/abs/2310.19349</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hpprc/simple-simcse-ja">https://github.com/hpprc/simple-simcse-ja</a></li>
<li>paper_authors: Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda</li>
<li>for: 本研究开发了一个日本语SimCSE，即日本语句子嵌入模型 fine-tuned with SimCSE。由于日本语句子嵌入模型的基eline尚未完善，因此我们对日本语句子嵌入模型进行了广泛的实验。</li>
<li>methods: 本研究使用了24种预训日本语或多语言模型，以及5个指定 dataset和4个无指定 dataset进行了广泛的训练和评估。</li>
<li>results: 本研究获得了丰富的评估结果，包括句子嵌入模型的训练设置和评估结果。<details>
<summary>Abstract</summary>
We report the development of Japanese SimCSE, Japanese sentence embedding models fine-tuned with SimCSE. Since there is a lack of sentence embedding models for Japanese that can be used as a baseline in sentence embedding research, we conducted extensive experiments on Japanese sentence embeddings involving 24 pre-trained Japanese or multilingual language models, five supervised datasets, and four unsupervised datasets. In this report, we provide the detailed training setup for Japanese SimCSE and their evaluation results.
</details>
<details>
<summary>摘要</summary>
我们报道了日本SimCSE的开发，是基于SimCSE的日语句子嵌入模型的精心调教。由于日语句子嵌入模型的基线研究缺乏日语句子嵌入模型，我们在日语句子嵌入领域进行了广泛的实验，使用24种预训练的日语或多语言模型，5个supervised数据集和4个Unsupervised数据集。在这份报告中，我们提供了日本SimCSE的详细训练设置和评估结果。
</details></li>
</ul>
<hr>
<h2 id="Test-Suites-Task-Evaluation-of-Gender-Fairness-in-MT-with-MuST-SHE-and-INES"><a href="#Test-Suites-Task-Evaluation-of-Gender-Fairness-in-MT-with-MuST-SHE-and-INES" class="headerlink" title="Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and INES"></a>Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and INES</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19345">http://arxiv.org/abs/2310.19345</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlt-mt/fbk-fairseq">https://github.com/hlt-mt/fbk-fairseq</a></li>
<li>paper_authors: Beatrice Savoldi, Marco Gaido, Matteo Negri, Luisa Bentivogli</li>
<li>for: 这个论文旨在评估两个测试集的结果：MuST-SHE-WMT23和INES，以investigate系统在翻译女性和♂性形式和生成包容性翻译方面的能力。</li>
<li>methods: 该论文使用了新创建的测试集来评估英语-德语和德语-英语语对的翻译系统。</li>
<li>results: 结果表明系统在正常的性形式上具有相似的表现，但生成包容性翻译方面仍然是一个挑战，表明未来可能需要进一步的改进和研究。<details>
<summary>Abstract</summary>
As part of the WMT-2023 "Test suites" shared task, in this paper we summarize the results of two test suites evaluations: MuST-SHE-WMT23 and INES. By focusing on the en-de and de-en language pairs, we rely on these newly created test suites to investigate systems' ability to translate feminine and masculine gender and produce gender-inclusive translations. Furthermore we discuss metrics associated with our test suites and validate them by means of human evaluations. Our results indicate that systems achieve reasonable and comparable performance in correctly translating both feminine and masculine gender forms for naturalistic gender phenomena. Instead, the generation of inclusive language forms in translation emerges as a challenging task for all the evaluated MT models, indicating room for future improvements and research on the topic.
</details>
<details>
<summary>摘要</summary>
As part of the WMT-2023 "Test suites" shared task, in this paper we summarize the results of two test suites evaluations: MuST-SHE-WMT23 and INES. By focusing on the en-de and de-en language pairs, we rely on these newly created test suites to investigate systems' ability to translate feminine and masculine gender and produce gender-inclusive translations. Furthermore we discuss metrics associated with our test suites and validate them by means of human evaluations. Our results indicate that systems achieve reasonable and comparable performance in correctly translating both feminine and masculine gender forms for naturalistic gender phenomena. Instead, the generation of inclusive language forms in translation emerges as a challenging task for all the evaluated MT models, indicating room for future improvements and research on the topic.Here's the translation in Traditional Chinese as well:As part of the WMT-2023 "Test suites" shared task, in this paper we summarize the results of two test suites evaluations: MuST-SHE-WMT23 and INES. By focusing on the en-de and de-en language pairs, we rely on these newly created test suites to investigate systems' ability to translate feminine and masculine gender and produce gender-inclusive translations. Furthermore we discuss metrics associated with our test suites and validate them by means of human evaluations. Our results indicate that systems achieve reasonable and comparable performance in correctly translating both feminine and masculine gender forms for naturalistic gender phenomena. Instead, the generation of inclusive language forms in translation emerges as a challenging task for all the evaluated MT models, indicating room for future improvements and research on the topic.
</details></li>
</ul>
<hr>
<h2 id="Fusing-Temporal-Graphs-into-Transformers-for-Time-Sensitive-Question-Answering"><a href="#Fusing-Temporal-Graphs-into-Transformers-for-Time-Sensitive-Question-Answering" class="headerlink" title="Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering"></a>Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19292">http://arxiv.org/abs/2310.19292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Su, Phillip Howard, Nagib Hakim, Steven Bethard</li>
<li>for: 这篇论文是为了提高语言模型在时间理解方面的能力而写的。</li>
<li>methods: 论文使用现有的时间信息抽取系统将问题和文档中的时间信息抽取出来，然后 investigate 不同的方法将这些时间信息融合到Transformer模型中。</li>
<li>results: 研究结果显示，我们提议的方法可以substantially 提高Transformer模型在时间理解方面的能力，无需 Fine-tuning。此外，我们的方法还超过了多种基于图 convolution的方法，并在SituatedQA和TimeQA中三个分区中达到了新的state-of-the-art表现。<details>
<summary>Abstract</summary>
Answering time-sensitive questions from long documents requires temporal reasoning over the times in questions and documents. An important open question is whether large language models can perform such reasoning solely using a provided text document, or whether they can benefit from additional temporal information extracted using other systems. We address this research question by applying existing temporal information extraction systems to construct temporal graphs of events, times, and temporal relations in questions and documents. We then investigate different approaches for fusing these graphs into Transformer models. Experimental results show that our proposed approach for fusing temporal graphs into input text substantially enhances the temporal reasoning capabilities of Transformer models with or without fine-tuning. Additionally, our proposed method outperforms various graph convolution-based approaches and establishes a new state-of-the-art performance on SituatedQA and three splits of TimeQA.
</details>
<details>
<summary>摘要</summary>
回答时间敏感问题从长文档中需要时间逻辑，检查问题和文档中的时间和事件关系。一个重要的研究问题是大语言模型是否可以通过提供的文档alone来完成这种逻辑，或者是否可以通过其他系统提取的时间信息来增强其能力。我们解决这个研究问题，通过应用现有的时间信息抽取系统来构建问题和文档中的时间图。然后，我们研究不同的方法来融合这些图into Transformer模型。实验结果表明，我们提议的方法可以增强Transformer模型中的时间逻辑能力，无论是否进行了微调。此外，我们的方法还超过了基于图 convolution的方法，并在 SituatedQA 和 TimeQA 中的三个分区中创造了新的状态标准。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-love-diligent-trolls-Accounting-for-rater-effects-in-the-dialogue-safety-task"><a href="#Learning-to-love-diligent-trolls-Accounting-for-rater-effects-in-the-dialogue-safety-task" class="headerlink" title="Learning to love diligent trolls: Accounting for rater effects in the dialogue safety task"></a>Learning to love diligent trolls: Accounting for rater effects in the dialogue safety task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19271">http://arxiv.org/abs/2310.19271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michaeljohnilagan/aestrollhunting">https://github.com/michaeljohnilagan/aestrollhunting</a></li>
<li>paper_authors: Michael John Ilagan</li>
<li>for: 本研究的目的是提高聊天机器人的识别精度，避免生成不当的语言内容。</li>
<li>methods: 本研究使用了多用户评分和隐藏类分析（LCA）技术，以推断正确的标签。</li>
<li>results: 实验结果表明，当噪声用户是一致的时，使用AES-like方法可以准确地推断标签，即使噪声用户占多数。<details>
<summary>Abstract</summary>
Chatbots have the risk of generating offensive utterances, which must be avoided. Post-deployment, one way for a chatbot to continuously improve is to source utterance/label pairs from feedback by live users. However, among users are trolls, who provide training examples with incorrect labels. To de-troll training data, previous work removed training examples that have high user-aggregated cross-validation (CV) error. However, CV is expensive; and in a coordinated attack, CV may be overwhelmed by trolls in number and in consistency among themselves. In the present work, I address both limitations by proposing a solution inspired by methodology in automated essay scoring (AES): have multiple users rate each utterance, then perform latent class analysis (LCA) to infer correct labels. As it does not require GPU computations, LCA is inexpensive. In experiments, I found that the AES-like solution can infer training labels with high accuracy when trolls are consistent, even when trolls are the majority.
</details>
<details>
<summary>摘要</summary>
<<SYS>>聊天机器人有可能生成冒犯性的语音，这些语音必须避免。在部署后，一种方式是通过来自用户的反馈获取语音/标签对。然而，用户中有些人是啦啦队伍，他们提供了训练示例 WITH incorrect标签。为了除啦啦示例，先前的工作使用了用户集成的跨验算法（CV）错误。然而，CV 是昂贵的，而且在协调攻击下，CV 可能会被啦啦人数和一致性所淹没。在当前的工作中，我解决了这两个限制，提出了基于自动化 Essay 评分（AES）的解决方案：由多个用户评分每个语音，然后使用隐藏类分析（LCA）推断正确的标签。由于 LCA 不需要 GPU 计算，因此它是便宜的。在实验中，我发现了 AES 类似的解决方案可以在啦啦人数占主导地位时， WITH high accuracy 推断训练标签。
</details></li>
</ul>
<hr>
<h2 id="Moral-Judgments-in-Narratives-on-Reddit-Investigating-Moral-Sparks-via-Social-Commonsense-and-Linguistic-Signals"><a href="#Moral-Judgments-in-Narratives-on-Reddit-Investigating-Moral-Sparks-via-Social-Commonsense-and-Linguistic-Signals" class="headerlink" title="Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via Social Commonsense and Linguistic Signals"></a>Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via Social Commonsense and Linguistic Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19268">http://arxiv.org/abs/2310.19268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijie Xi, Munindar P. Singh</li>
<li>for: 研究社交媒体上的道德判断，以了解在线社交中真实的道德情况。</li>
<li>methods: 使用计算机技术来研究道德判断的影响因素，包括社交常识事件和语言信号。</li>
<li>results: 发现事件相关的负性人格特质（如幼稚和颠覆）吸引注意力，导致道德责任的增加，表明道德决策和责任之间存在互动关系。此外，用于描述事件和人物的语言也会增加道德决策的可能性，而对事件的描述则减少这种效果。<details>
<summary>Abstract</summary>
Given the increasing realism of social interactions online, social media offers an unprecedented avenue to evaluate real-life moral scenarios. We examine posts from Reddit, where authors and commenters share their moral judgments on who is blameworthy. We employ computational techniques to investigate factors influencing moral judgments, including (1) events activating social commonsense and (2) linguistic signals. To this end, we focus on excerpt-which we term moral sparks-from original posts that commenters include to indicate what motivates their moral judgments. By examining over 24,672 posts and 175,988 comments, we find that event-related negative personal traits (e.g., immature and rude) attract attention and stimulate blame, implying a dependent relationship between moral sparks and blameworthiness. Moreover, language that impacts commenters' cognitive processes to depict events and characters enhances the probability of an excerpt become a moral spark, while factual and concrete descriptions tend to inhibit this effect.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Overview-of-the-CLAIMSCAN-2023-Uncovering-Truth-in-Social-Media-through-Claim-Detection-and-Identification-of-Claim-Spans"><a href="#Overview-of-the-CLAIMSCAN-2023-Uncovering-Truth-in-Social-Media-through-Claim-Detection-and-Identification-of-Claim-Spans" class="headerlink" title="Overview of the CLAIMSCAN-2023: Uncovering Truth in Social Media through Claim Detection and Identification of Claim Spans"></a>Overview of the CLAIMSCAN-2023: Uncovering Truth in Social Media through Claim Detection and Identification of Claim Spans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19267">http://arxiv.org/abs/2310.19267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megha Sundriyal, Md Shad Akhtar, Tanmoy Chakraborty</li>
<li>for: 本研究旨在提高社交媒体平台上 false information、宣传和 fake news 的识别率，以提高用户对信息的识别和判断能力。</li>
<li>methods: 本研究使用 CLAIMSCAN 技术，包括 Task A 和 Task B 两个任务，以自动识别社交媒体上的声明，并确定声明中的有关词语或短语。</li>
<li>results: 本研究在 2023 Forum for Information Retrieval Evaluation (FIRE’2023) 上提交了 CLAIMSCAN，并获得了 40 个注册和 28 个团队的参与，表明该技术在当今的数字时代中具有重要性和应用价值。<details>
<summary>Abstract</summary>
A significant increase in content creation and information exchange has been made possible by the quick development of online social media platforms, which has been very advantageous. However, these platforms have also become a haven for those who disseminate false information, propaganda, and fake news. Claims are essential in forming our perceptions of the world, but sadly, they are frequently used to trick people by those who spread false information. To address this problem, social media giants employ content moderators to filter out fake news from the actual world. However, the sheer volume of information makes it difficult to identify fake news effectively. Therefore, it has become crucial to automatically identify social media posts that make such claims, check their veracity, and differentiate between credible and false claims. In response, we presented CLAIMSCAN in the 2023 Forum for Information Retrieval Evaluation (FIRE'2023). The primary objectives centered on two crucial tasks: Task A, determining whether a social media post constitutes a claim, and Task B, precisely identifying the words or phrases within the post that form the claim. Task A received 40 registrations, demonstrating a strong interest and engagement in this timely challenge. Meanwhile, Task B attracted participation from 28 teams, highlighting its significance in the digital era of misinformation.
</details>
<details>
<summary>摘要</summary>
在互联网社交媒体平台的快速发展中，内容创造和信息交换得到了极大的提高，这对于社会是非常有利。然而，这些平台也成为了假信息、宣传和falsetelling的渠道。我们的认知形成的基础是声称，但是这些声称经常被用来骗人。为了解决这个问题，社交媒体巨头们雇用内容筛选人员来从实际世界中筛选假新闻。然而，巨大量的信息使得效果性验证假新闻变得困难。因此，已成为必须自动识别社交媒体帖子中的声称，验证其真实性，并将真实和假的声称分开。为此，我们在2023年信息检索评估论坛（FIRE'2023）上发表了CLAIMSCAN。主要目标是解决两个关键任务：任务A是判断社交媒体帖子是否为声称，任务B是在帖子中 precisely identify the words or phrases that form the claim。任务A得到了40个注册，表明了这个时期挑战的强大兴趣和参与度。同时，任务B吸引了28个团队的参与，这反映了在数字时代的谎言普遍性。
</details></li>
</ul>
<hr>
<h2 id="M4LE-A-Multi-Ability-Multi-Range-Multi-Task-Multi-Domain-Long-Context-Evaluation-Benchmark-for-Large-Language-Models"><a href="#M4LE-A-Multi-Ability-Multi-Range-Multi-Task-Multi-Domain-Long-Context-Evaluation-Benchmark-for-Large-Language-Models" class="headerlink" title="M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models"></a>M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19240">http://arxiv.org/abs/2310.19240</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kwanwaichung/m4le">https://github.com/kwanwaichung/m4le</a></li>
<li>paper_authors: Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou Li, Lifeng Shang, Qun Liu, Kam-Fai Wong</li>
<li>for: 这篇论文的目的是提出一个多能力、多范围、多任务、多领域的benchmark，以评估大语言模型（LLMs）在长序列上的能力。</li>
<li>methods: 这篇论文使用了一种自动化的方法（但需要 negligible human annotations）将短序列任务转换成长序列场景，以评估LLMs在多个能力上的表现。</li>
<li>results: 研究发现，当任务需要多个 span 注意时，现有的 LLMs 很难理解长序列上的上下文。 semantic retrieve 任务对能力强的 LLMs 来说更加困难。 模型通过长文本 fine-tuning 和位置插值来达到相似的性能。<details>
<summary>Abstract</summary>
Managing long sequences has become an important and necessary feature for large language models (LLMs). However, it is still an open question of how to comprehensively and systematically evaluate the long-sequence capability of LLMs. One of the reasons is that conventional and widely-used benchmarks mainly consist of short sequences. In this paper, we propose M4LE, a Multi-ability, Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation. M4LE is based on a diverse NLP task pool comprising 36 NLP datasets, 11 task types and 12 domains. To alleviate the scarcity of tasks with naturally long sequences and incorporate multiple-ability assessment, we propose an automatic approach (but with negligible human annotations) to convert short-sequence tasks into a unified long-sequence scenario where LLMs have to identify single or multiple relevant spans in long contexts based on explicit or semantic hints. Specifically, the scenario includes five different types of abilities: (1) explicit single-span; (2) semantic single-span; (3) explicit multiple-span; (4) semantic multiple-span; and (5) global context understanding. The resulting samples in M4LE are evenly distributed from 1k to 8k input length. We conducted a systematic evaluation on 11 well-established LLMs, especially those optimized for long-sequence inputs. Our results reveal that: 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval task is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area.
</details>
<details>
<summary>摘要</summary>
管理长序列已成为大语言模型（LLM）的重要和必要特性。然而，如何全面和系统地评估长序列能力仍然是一个开放的问题。一个原因是，现有的常用的标准benchmark主要包括短序列。在这篇论文中，我们提议M4LE，一个多能力、多范围、多任务、多领域benchmark для长序列评估。M4LE基于多样化的NLP任务池，包括36个NLP任务、11种任务类型和12个领域。为了解决短序列任务的缺乏和多能力评估的困难，我们提出了一种自动化方法（即使无需人工注释），将短序列任务转换成一个统一的长序列场景，要求LLMs在长文本中标识单个或多个相关的span，基于显式或 semantics 的提示。具体来说，场景包括五种不同的能力：（1）显式单span;（2）semantic单span;（3）显式多span;（4）semantic多span;和（5）全局上下文理解。M4LE中的样本具有1k到8k输入长度的均衡分布。我们对11种已知的LLMs进行系统评估，特别是那些针对长序列输入优化。我们的结果表明：1）当前LLMs在长序列上尚未具备全面的理解能力，特别是需要多个span注意力时。2）semantic retrieval任务对高能 LLMs 更加困难。3）没有 fine-tuning 的模型在 longer text 上使用位置插值方法可以达到相当的性能。我们将benchmark公开发布，以便未来的研究在这一领域。
</details></li>
</ul>
<hr>
<h2 id="Building-Real-World-Meeting-Summarization-Systems-using-Large-Language-Models-A-Practical-Perspective"><a href="#Building-Real-World-Meeting-Summarization-Systems-using-Large-Language-Models-A-Practical-Perspective" class="headerlink" title="Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective"></a>Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19233">http://arxiv.org/abs/2310.19233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN</li>
<li>for: 这 paper 的目的是如何使用大型自然语言模型 (LLM) 建立实用的会议摘要系统，以便在实际应用中使用。</li>
<li>methods: 作者通过对多种关闭源和开源 LLM 进行广泛的评估和比较，包括 GPT-4、GPT-3.5、PaLM-2 和 LLaMA-2，以确定这些模型在实际应用中的性能。</li>
<li>results: 研究发现，大多数关闭源 LLM 在性能方面比较好，但是小型开源模型 Like LLaMA-2 (7B 和 13B) 在零基eline情况下可以达到与关闭源模型相当的性能。考虑到关闭源模型的隐私问题以及使用精度版本的高成本，开源模型更有利可图于实际应用。因此，LLaMA-2-7B 模型更有前途的推荐用于实际应用。<details>
<summary>Abstract</summary>
This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs). For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findings reveal that most closed-source LLMs are generally better in terms of performance. However, much smaller open-source models like LLaMA- 2 (7B and 13B) could still achieve performance comparable to the large closed-source models even in zero-shot scenarios. Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models, the opensource models that can achieve competitive performance are more advantageous for industrial use. Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage. In sum, this paper offers practical insights on using LLMs for real-world business meeting summarization, shedding light on the trade-offs between performance and cost.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adapter-Pruning-using-Tropical-Characterization"><a href="#Adapter-Pruning-using-Tropical-Characterization" class="headerlink" title="Adapter Pruning using Tropical Characterization"></a>Adapter Pruning using Tropical Characterization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19232">http://arxiv.org/abs/2310.19232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Bhardwaj, Tushar Vaidya, Soujanya Poria</li>
<li>for: 本研究旨在找出适合下游应用的最佳adapter参数数量，通过研究拓扑特性来优化adapter层。</li>
<li>methods: 本研究使用拓扑几何来优化adapter层参数，将问题定义为减少adapter参数而不改变下游几何的优化问题。</li>
<li>results: 实验结果显示，使用拓扑特性来优化adapter参数比照度量基线更有效，而结合多种方法则在多个任务上表现最佳。<details>
<summary>Abstract</summary>
Adapters are widely popular parameter-efficient transfer learning approaches in natural language processing that insert trainable modules in between layers of a pre-trained language model. Apart from several heuristics, however, there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications. In this paper, we propose an adapter pruning approach by studying the tropical characteristics of trainable modules. We cast it as an optimization problem that aims to prune parameters from the adapter layers without changing the orientation of underlying tropical hypersurfaces. Our experiments on five NLP datasets show that tropical geometry tends to identify more relevant parameters to prune when compared with the magnitude-based baseline, while a combined approach works best across the tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>> adapter 是自然语言处理领域非常流行的参数高效传承学习方法之一，它们将可训练模块插入预训练语言模型之间的层。虽然有几种规则，但没有尝试分析最佳 adapter 参数的数量，用于下游应用。在这篇论文中，我们提出了一种 adapter 剔除方法，通过研究 tropical 特征来决定哪些参数可以从 adapter 层中剔除。我们将其拟合为一个优化问题，目标是从 adapter 层中剔除参数，而不改变 tropical 偏函数的方向。我们在五个 NLP 数据集上进行了实验，发现 tropical 几何可以更好地标识需要剔除的参数，而且一种组合方法在所有任务中表现最佳。
</details></li>
</ul>
<hr>
<h2 id="LitCab-Lightweight-Calibration-of-Language-Models-on-Outputs-of-Varied-Lengths"><a href="#LitCab-Lightweight-Calibration-of-Language-Models-on-Outputs-of-Varied-Lengths" class="headerlink" title="LitCab: Lightweight Calibration of Language Models on Outputs of Varied Lengths"></a>LitCab: Lightweight Calibration of Language Models on Outputs of Varied Lengths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19208">http://arxiv.org/abs/2310.19208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/launchnlp/litcab">https://github.com/launchnlp/litcab</a></li>
<li>paper_authors: Xin Liu, Muhammad Khalifa, Lu Wang<br>for: 这篇论文的目的是提出一种轻量级的模型均衡机制，以改善语言模型（LM）的准确性。methods: 这篇论文使用了一种单Linear层来修改输入文本表示的LM输出logits，以改善模型均衡。results: 这篇论文的实验结果表明， LitCab 可以提高 LM 的准确性，并且只添加了 &lt; 2% 的原始模型参数。此外，在7种文本生成任务上， LitCab 可以降低 ECE 平均分数 by 20%。此外，在7种流行的开源语言模型（GPT和LLaMA家族）上， LitCab 可以获得以下关键发现：1） bigger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones。2） GPT-family models show superior calibration compared to LLaMA, Llama2 and Vicuna models despite having much fewer parameters。3） finetuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of finetuning setups for calibrating LMs。<details>
<summary>Abstract</summary>
A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations, a common issue of LMs, as well as building more trustworthy models. Yet, popular neural model calibration techniques are not well-suited for LMs due to their lack of flexibility in discerning answer correctness and their high computational costs. For instance, post-processing methods like temperature scaling are often unable to reorder the candidate generations. Moreover, training-based methods require finetuning the entire model, which is impractical due to the increasing sizes of modern LMs. In this paper, we present LitCab, a lightweight calibration mechanism consisting of a single linear layer taking the input text representation and manipulateing the LM output logits. LitCab improves model calibration by only adding < 2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of 7 text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks, by reducing the average ECE score by 20%. We further conduct a comprehensive evaluation with 7 popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (1) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones. (2) GPT-family models show superior calibration compared to LLaMA, Llama2 and Vicuna models despite having much fewer parameters. (3) Finetuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of finetuning setups for calibrating LMs.
</details>
<details>
<summary>摘要</summary>
modelo es considerado bien calibrado cuando su estimación de probabilidad se alinea con la probabilidad real de que el output sea correcto. Calibrar modelos de lenguaje (LMs) es crucial, ya que juega un papel vital en la detección y mitigación de halucinaciones, un problema común de los modelos, así como en la construcción de modelos más confiables. Sin embargo, las técnicas populares de calibración neural no son adecuadas para los modelos de lenguaje debido a su falta de flexibilidad en determinar la corrección de las respuestas y su alto costo computacional. Por ejemplo, los métodos de posprocesamiento como escalado de temperatura a menudo no pueden reordenar las generaciones de candidatos. Además, los métodos de entrenamiento requieren finetuning la totalidad del modelo, lo que es impracticable debido al tamaño creciente de los modelos modernos. En este artículo, presentamos LitCab, un mecanismo de calibración ligero que consta de una sola capa lineal que toma la representación del texto de entrada y manipula las logit del modelo de salida. LitCab mejora la calibración del modelo al agregar menos de 2% de los parámetros originales. Para evaluar, construimos CaT, un conjunto de tareas de generación de texto que cubre respuestas que van desde frases breves hasta párrafos. Probamos LitCab con Llama2-7B, lo que mejora la calibración en todas las tareas, reduciendo la puntuación promedio de ECE en un 20%. Además, realizamos una evaluación exhaustiva con 7 modelos de lenguaje populares de las familias GPT y LLaMA, obteniendo los siguientes hallazgos clave: (1) Los modelos más grandes dentro de la misma familia exhiben mejor calibración en tareas de generación breve, pero no necesariamente en tareas más largas. (2) Los modelos de la familia GPT exhiben una mejor calibración que los modelos LLaMA, Llama2 y Vicuna, a pesar de tener muchos menos parámetros. (3) Finetuning un modelo preentrenado (por ejemplo, LLaMA) con muestras de propósito limitado (por ejemplo, conversaciones) puede llevar a una calibración peor, lo que destaca la importancia de los conjuntos de finetuning adecuados para calibrar los modelos de lenguaje.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.CL_2023_10_30/" data-id="clohum96500dapj8829w7eaj8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/84/">84</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
