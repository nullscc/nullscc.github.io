
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/2/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/cs.AI_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T12:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/cs.AI_2023_11_19/">cs.AI - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="LLM-aided-semi-supervision-for-Extractive-Dialog-Summarization"><a href="#LLM-aided-semi-supervision-for-Extractive-Dialog-Summarization" class="headerlink" title="LLM aided semi-supervision for Extractive Dialog Summarization"></a>LLM aided semi-supervision for Extractive Dialog Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11462">http://arxiv.org/abs/2311.11462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nishant Mishra, Gaurav Sahu, Iacer Calixto, Ameen Abu-Hanna, Issam H. Laradji</li>
<li>for: 提高 chat 对话摘要的质量</li>
<li>methods: 使用 state-of-the-art 大语言模型（LLMs）生成对话 pseudo-labels，然后使用这些 pseudo-labels 微调一个 chat 摘要模型，从而将大型 LLM 中的知识传递到一个更小的专业化模型中</li>
<li>results: 在 \tweetsumm 数据集上测试，只使用 10% 原始标注数据集可以达到 65.9&#x2F;57.0&#x2F;61.0 ROUGE-1&#x2F;-2&#x2F;-L，而当前状态之最高得到 65.16&#x2F;55.81&#x2F;64.37 ROUGE-1&#x2F;-2&#x2F;-L，即在最差情况下（即 ROUGE-L）仍能保持 94.7% 的性能，使用只有 10% 的数据。<details>
<summary>Abstract</summary>
Generating high-quality summaries for chat dialogs often requires large labeled datasets. We propose a method to efficiently use unlabeled data for extractive summarization of customer-agent dialogs. In our method, we frame summarization as a question-answering problem and use state-of-the-art large language models (LLMs) to generate pseudo-labels for a dialog. We then use these pseudo-labels to fine-tune a chat summarization model, effectively transferring knowledge from the large LLM into a smaller specialized model. We demonstrate our method on the \tweetsumm dataset, and show that using 10\% of the original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L, whereas the current state-of-the-art trained on the entire training data set obtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case (i.e., ROUGE-L) we still effectively retain 94.7% of the performance while using only 10% of the data.
</details>
<details>
<summary>摘要</summary>
通常来说，生成高质量的对话摘要需要大量标注数据。我们提出了一种方法，可以高效地使用无标注数据来抽取对话摘要。在我们的方法中，我们将摘要视为问答问题，使用当前最好的大语言模型（LLM）生成对话中的 pseudo-标签。然后，我们使用这些 pseudo-标签来练化一个对话摘要模型，从而将大型LLM中的知识传递到一个更小的专门模型中。我们在 \tweetsumm 数据集上进行了实验，并证明了使用10%的原始标注数据集可以 дости到65.9/57.0/61.0 ROUGE-1/-2/-L，而当前状态的训练集全部数据集上的最佳性能为65.16/55.81/64.37 ROUGE-1/-2/-L。换句话说，在最差情况下（即 ROUGE-L），我们仍然可以有效地保留94.7%的性能，只用10%的数据。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Public-Perceptions-Machine-Learning-Based-Sentiment-Analysis-of-COVID-19-Vaccines-in-India"><a href="#Unveiling-Public-Perceptions-Machine-Learning-Based-Sentiment-Analysis-of-COVID-19-Vaccines-in-India" class="headerlink" title="Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis of COVID-19 Vaccines in India"></a>Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis of COVID-19 Vaccines in India</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11435">http://arxiv.org/abs/2311.11435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milind Gupta, Abhishek Kaushik<br>for: 这个研究旨在了解印度人民对COVID-19疫苗的看法，以便在极度狭隘的国家中成功进行疫苗接种。methods: 这个研究使用数据挖掘技术分析Reddit数据，以评估印度网民对COVID-19疫苗的态度。Python的Text Blob库用于注释评估评论的总体情感。results: 研究结果显示，大多数Reddit用户在印度表现出中性的态度，这对印度政府为了接种大量人口而做出的努力提出了挑战。<details>
<summary>Abstract</summary>
In March 2020, the World Health Organisation declared COVID-19 a global pandemic as it spread to nearly every country. By mid-2021, India had introduced three vaccines: Covishield, Covaxin, and Sputnik. To ensure successful vaccination in a densely populated country like India, understanding public sentiment was crucial. Social media, particularly Reddit with over 430 million users, played a vital role in disseminating information. This study employs data mining techniques to analyze Reddit data and gauge Indian sentiments towards COVID-19 vaccines. Using Python's Text Blob library, comments are annotated to assess general sentiments. Results show that most Reddit users in India expressed neutrality about vaccination, posing a challenge for the Indian government's efforts to vaccinate a significant portion of the population.
</details>
<details>
<summary>摘要</summary>
在2020年3月，世界卫生组织宣布COVID-19为全球大流行，这种疾病已经蔓延到大多数国家。到2021年中期，印度已经推出了三种疫苗：Covishield、Covaxin和Sputnik。为了在印度的高度紧张人口 circumstance 下成功进行疫苗接种，了解公众情绪非常重要。社交媒体，特别是Reddit，拥有超过430万用户，在传播信息方面发挥了关键性的作用。本研究使用数据挖掘技术来分析Reddit数据，评估印度用户对COVID-19疫苗的情感。使用Python的Text Blob库，评论被标注以评估总体情感。结果显示，大多数Reddit用户在印度表达中性对于接种疫苗的态度，这对印度政府的大规模接种计划 pose 了挑战。
</details></li>
</ul>
<hr>
<h2 id="Appearance-Codes-using-Joint-Embedding-Learning-of-Multiple-Modalities"><a href="#Appearance-Codes-using-Joint-Embedding-Learning-of-Multiple-Modalities" class="headerlink" title="Appearance Codes using Joint Embedding Learning of Multiple Modalities"></a>Appearance Codes using Joint Embedding Learning of Multiple Modalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11427">http://arxiv.org/abs/2311.11427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edogariu/alex-zhang">https://github.com/edogariu/alex-zhang</a></li>
<li>paper_authors: Alex Zhang, Evan Dogariu</li>
<li>for: 提高日夜场景变换的效率和质量</li>
<li>methods: 使用对比损失约束来学习场景的结构和外观空间</li>
<li>results: 可以使用日天外观码生成夜晚场景，而不需要再进行优化迭代Here’s a breakdown of each point:</li>
<li>for: The paper aims to improve the efficiency and quality of scene transformation by learning a joint embedding space for the appearance and structure of the scene.</li>
<li>methods: The proposed framework uses a contrastive loss constraint between different modalities to learn the joint embedding space.</li>
<li>results: The proposed method can generate new renders of night-time photos using day-time appearance codes without additional optimization iterations, and achieves generations of similar quality without learning appearance codes for any unseen images on inference.<details>
<summary>Abstract</summary>
The use of appearance codes in recent work on generative modeling has enabled novel view renders with variable appearance and illumination, such as day-time and night-time renders of a scene. A major limitation of this technique is the need to re-train new appearance codes for every scene on inference, so in this work we address this problem proposing a framework that learns a joint embedding space for the appearance and structure of the scene by enforcing a contrastive loss constraint between different modalities. We apply our framework to a simple Variational Auto-Encoder model on the RADIATE dataset \cite{sheeny2021radiate} and qualitatively demonstrate that we can generate new renders of night-time photos using day-time appearance codes without additional optimization iterations. Additionally, we compare our model to a baseline VAE that uses the standard per-image appearance code technique and show that our approach achieves generations of similar quality without learning appearance codes for any unseen images on inference.
</details>
<details>
<summary>摘要</summary>
在最近的生成模型工作中，使用的外观编码已经实现了变量的外观和照明，如场景的日间和夜间视图。然而，这种技术的主要限制是需要在推理时重新训练新的外观编码，因此在这种情况下我们提出了一个框架，该框架通过对不同模式之间的冲突损失进行约束，学习场景的外观和结构的联合嵌入空间。我们在RADIATE数据集\cite{sheeny2021radiate}上应用了我们的框架，并质量地示出了使用日间外观编码生成夜间照片的能力。此外，我们与标准每个图像的外观编码技术相比，我们的方法可以在无需学习未看到的图像的推理过程中生成类似质量的生成。
</details></li>
</ul>
<hr>
<h2 id="LifeLearner-Hardware-Aware-Meta-Continual-Learning-System-for-Embedded-Computing-Platforms"><a href="#LifeLearner-Hardware-Aware-Meta-Continual-Learning-System-for-Embedded-Computing-Platforms" class="headerlink" title="LifeLearner: Hardware-Aware Meta Continual Learning System for Embedded Computing Platforms"></a>LifeLearner: Hardware-Aware Meta Continual Learning System for Embedded Computing Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11420">http://arxiv.org/abs/2311.11420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young D. Kwon, Jagmohan Chauhan, Hong Jia, Stylianos I. Venieris, Cecilia Mascolo</li>
<li>for: 这个论文的目的是探讨如何在具有限制的资源的嵌入式系统上实现持续学习（Continual Learning），以便在上下文、动作和用户变化时，应用程序可以学习并适应。</li>
<li>methods: 该论文提出了一种叫做 LifeLearner 的硬件意识的元 kontinual learning 系统，该系统可以有效地减少系统资源（内存、延迟、能耗）的使用，同时保证高准确性。 LifeLearner 使用了元学习和熬悉策略来直接面对数据稀缺问题，并使用了lossless和lossy压缩技术来减少 CL 和熬悉样本的资源需求。</li>
<li>results: 根据论文描述，LifeLearner 可以实现near-optimal CL 性能，与标准比较基准（Oracle）只偏差2.8%。相比之下，与现有的Meta CL方法相比，LifeLearner 可以减少内存占用（178.7倍）、终端延迟（80.8-94.2%）和能耗（80.9-94.2%）。此外，LifeLearner 已经在两个边缘设备和一个微控制器Unit上成功部署，这意味着LifeLearner 可以在具有限制的平台上实现高效的 CL。<details>
<summary>Abstract</summary>
Continual Learning (CL) allows applications such as user personalization and household robots to learn on the fly and adapt to context. This is an important feature when context, actions, and users change. However, enabling CL on resource-constrained embedded systems is challenging due to the limited labeled data, memory, and computing capacity. In this paper, we propose LifeLearner, a hardware-aware meta continual learning system that drastically optimizes system resources (lower memory, latency, energy consumption) while ensuring high accuracy. Specifically, we (1) exploit meta-learning and rehearsal strategies to explicitly cope with data scarcity issues and ensure high accuracy, (2) effectively combine lossless and lossy compression to significantly reduce the resource requirements of CL and rehearsal samples, and (3) developed hardware-aware system on embedded and IoT platforms considering the hardware characteristics. As a result, LifeLearner achieves near-optimal CL performance, falling short by only 2.8% on accuracy compared to an Oracle baseline. With respect to the state-of-the-art (SOTA) Meta CL method, LifeLearner drastically reduces the memory footprint (by 178.7x), end-to-end latency by 80.8-94.2%, and energy consumption by 80.9-94.2%. In addition, we successfully deployed LifeLearner on two edge devices and a microcontroller unit, thereby enabling efficient CL on resource-constrained platforms where it would be impractical to run SOTA methods and the far-reaching deployment of adaptable CL in a ubiquitous manner. Code is available at https://github.com/theyoungkwon/LifeLearner.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Security-Risk-Taxonomy-for-Large-Language-Models"><a href="#A-Security-Risk-Taxonomy-for-Large-Language-Models" class="headerlink" title="A Security Risk Taxonomy for Large Language Models"></a>A Security Risk Taxonomy for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11415">http://arxiv.org/abs/2311.11415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erik Derner, Kristina Batistič, Jan Zahálka, Robert Babuška</li>
<li>for: 本研究旨在评估大型自然语言模型（LLM）的安全风险，包括伪造、数据泄露和声誉损害等。</li>
<li>methods: 本研究使用推议基本攻击分析pipeline中的用户-模型交互，并将攻击分为目标和攻击类型。</li>
<li>results: 研究提出了一个攻击分类法，并提供了具体的攻击示例，以验证这些风险的实际影响。<details>
<summary>Abstract</summary>
As large language models (LLMs) permeate more and more applications, an assessment of their associated security risks becomes increasingly necessary. The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial. This paper addresses a gap in current research by focusing on the security risks posed by LLMs, which extends beyond the widely covered ethical and societal implications. Our work proposes a taxonomy of security risks along the user-model communication pipeline, explicitly focusing on prompt-based attacks on LLMs. We categorize the attacks by target and attack type within a prompt-based interaction scheme. The taxonomy is reinforced with specific attack examples to showcase the real-world impact of these risks. Through this taxonomy, we aim to inform the development of robust and secure LLM applications, enhancing their safety and trustworthiness.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:large language models (LLMs) 在更多应用程序中普及，需要评估这些模型的相关安全风险。恶意利用者可以通过各种方式，包括假信息、数据泄露和声誉损害，对 LLMs 进行攻击。这篇论文填补当前研究中的一个空白，专注于 LLMs 的安全风险，这些风险不仅包括广泛讨论的伦理和社会因素。我们的工作提出了一种用户-模型通信管道上的安全风险分类法，专注于模型上的提示基本攻击。我们将攻击分为目标和攻击类型，并在提示基本交互方案中进行分类。这种分类机制得到了具体的攻击示例，以示这些风险的实际影响。通过这种分类机制，我们希望激励开发robust和安全的 LLM 应用程序，提高它们的安全性和可信度。
</details></li>
</ul>
<hr>
<h2 id="Make-me-an-Offer-Forward-and-Reverse-Auctioning-Problems-in-the-Tourism-Industry"><a href="#Make-me-an-Offer-Forward-and-Reverse-Auctioning-Problems-in-the-Tourism-Industry" class="headerlink" title="Make me an Offer: Forward and Reverse Auctioning Problems in the Tourism Industry"></a>Make me an Offer: Forward and Reverse Auctioning Problems in the Tourism Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11400">http://arxiv.org/abs/2311.11400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis T. Christou, Dimitris Doukas, Konstantina Skouri, Gerasimos Meletiou</li>
<li>for: 这篇论文的目的是为了解决旅游景点常见的季节性问题，尤其在 covid-19 后期，旅游需求增加但不均衡分布在不同地理区域。</li>
<li>methods: 本论文提出了两种拍卖系统，一种是向下拍卖模型，允许低知名度地区或低季节期Hoteliers拍卖房间，另一种是顾客可以 initiaze 逆向拍卖模型，让Hoteliers在区域发出供应项目，类似于priceline.com 的拍卖概念。</li>
<li>results: 论文发展了数学Programming 模型，显示在这两种拍卖模型中，双方都可以获得重要的经济和社会优点。论文 також提出了算法技术来解决这些优化问题，并使用精确优化 solvers 来解决它们。<details>
<summary>Abstract</summary>
Most tourist destinations are facing regular and consistent seasonality with significant economic and social impacts. This phenomenon is more pronounced in the post-covid era, where demand for travel has increased but unevenly among different geographic areas. To counter these problems that both customers and hoteliers are facing, we have developed two auctioning systems that allow hoteliers of lower popularity tier areas or during low season periods to auction their rooms in what we call a forward auction model, and also allows customers to initiate a bidding process whereby hoteliers in an area may make offers to the customer for their rooms, in what constitutes a reverse auction model initiated by the customer, similar to the bidding concept of priceline.com. We develop mathematical programming models that define explicitly both types of auctions, and show that in each type, there are significant benefits to be gained both on the side of the hotelier as well as on the side of the customer. We discuss algorithmic techniques for the approximate solution of these optimization problems, and present results using exact optimization solvers to solve them to guaranteed optimality. These techniques could be beneficial to both customer and hotelier reducing seasonality during middle and low season and providing the customer with attractive offers.
</details>
<details>
<summary>摘要</summary>
多数旅游目的地都面临 régulière 和consistent的季节性问题，这些问题在covid era后更加突出，旅游需求增加，但不均匀分布在不同地理区域。为了解决这些问题，我们开发了两种拍卖系统，allowing hoteliers in lower popularity tier areas or during low season periods to auction their rooms in a forward auction model, and also allowing customers to initiate a bidding process whereby hoteliers in an area may make offers to the customer for their rooms, similar to the bidding concept of priceline.com. We develop mathematical programming models that explicitly define both types of auctions, and show that in each type, there are significant benefits to be gained on both sides of the hotelier and the customer. We discuss algorithmic techniques for the approximate solution of these optimization problems, and present results using exact optimization solvers to solve them to guaranteed optimality. These techniques could be beneficial to both the customer and the hotelier, reducing seasonality during the middle and low seasons and providing the customer with attractive offers.Note: " régulière" is a French word that means "regular" in English. I left it in the original text as it is a technical term used in the field of auctions.
</details></li>
</ul>
<hr>
<h2 id="Inspecting-Explainability-of-Transformer-Models-with-Additional-Statistical-Information"><a href="#Inspecting-Explainability-of-Transformer-Models-with-Additional-Statistical-Information" class="headerlink" title="Inspecting Explainability of Transformer Models with Additional Statistical Information"></a>Inspecting Explainability of Transformer Models with Additional Statistical Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11378">http://arxiv.org/abs/2311.11378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoang C. Nguyen, Haeil Lee, Junmo Kim</li>
<li>for: 本研究的目的是解释Transformer模型在视觉和多modal任务上的含义，以及它在不同变体中的表现。</li>
<li>methods: 该研究使用了 combining attention layers和层normalization层来visualize Transformer模型。</li>
<li>results: 该方法可以有效地解释Swin Transformer和ViT模型的含义，并且可以在不同的任务上进行适应。<details>
<summary>Abstract</summary>
Transformer becomes more popular in the vision domain in recent years so there is a need for finding an effective way to interpret the Transformer model by visualizing it. In recent work, Chefer et al. can visualize the Transformer on vision and multi-modal tasks effectively by combining attention layers to show the importance of each image patch. However, when applying to other variants of Transformer such as the Swin Transformer, this method can not focus on the predicted object. Our method, by considering the statistics of tokens in layer normalization layers, shows a great ability to interpret the explainability of Swin Transformer and ViT.
</details>
<details>
<summary>摘要</summary>
对于过去几年内Transformer在视觉领域的普遍使用，有需要找到一种有效的方法来解释Transformer模型。在最近的工作中，Chefer等人通过结合注意层来有效地图解Transformer在视觉和多modal任务上。然而，当应用到其他Transformer的变体，如Swin Transformer和ViT时，这方法无法对预测物体进行专注。我们的方法，通过考虑层normalization层中的数据统计，展示了Swin Transformer和ViT的解释能力。
</details></li>
</ul>
<hr>
<h2 id="SOccDPT-Semi-Supervised-3D-Semantic-Occupancy-from-Dense-Prediction-Transformers-trained-under-memory-constraints"><a href="#SOccDPT-Semi-Supervised-3D-Semantic-Occupancy-from-Dense-Prediction-Transformers-trained-under-memory-constraints" class="headerlink" title="SOccDPT: Semi-Supervised 3D Semantic Occupancy from Dense Prediction Transformers trained under memory constraints"></a>SOccDPT: Semi-Supervised 3D Semantic Occupancy from Dense Prediction Transformers trained under memory constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11371">http://arxiv.org/abs/2311.11371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Nalgunda Ganesh</li>
<li>for: 这个论文的目的是提出一种具有内存效率的方法，用于从单视图图像输入获取3D semantic occupancy prediction。</li>
<li>methods: 这个方法使用dense prediction transformers进行预测，并使用 semi-supervised 训练管道，以便在有限labels的数据集上训练。它还使用patch-wise training来减少在训练过程中的内存使用。</li>
<li>results: 这个方法在无结构化交通数据集上进行训练，并在不结构化交通enario中表现出色，其RMSE score为9.1473， semantic segmentation IoU score为46.02%，并在69.47 Hz的频率下运行。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We present SOccDPT, a memory-efficient approach for 3D semantic occupancy prediction from monocular image input using dense prediction transformers. To address the limitations of existing methods trained on structured traffic datasets, we train our model on unstructured datasets including the Indian Driving Dataset and Bengaluru Driving Dataset. Our semi-supervised training pipeline allows SOccDPT to learn from datasets with limited labels by reducing the requirement for manual labelling by substituting it with pseudo-ground truth labels to produce our Bengaluru Semantic Occupancy Dataset. This broader training enhances our model's ability to handle unstructured traffic scenarios effectively. To overcome memory limitations during training, we introduce patch-wise training where we select a subset of parameters to train each epoch, reducing memory usage during auto-grad graph construction. In the context of unstructured traffic and memory-constrained training and inference, SOccDPT outperforms existing disparity estimation approaches as shown by the RMSE score of 9.1473, achieves a semantic segmentation IoU score of 46.02% and operates at a competitive frequency of 69.47 Hz. We make our code and semantic occupancy dataset public.
</details>
<details>
<summary>摘要</summary>
我们介绍SOccDPT，一种具有内存效率的方法，用于从单视图图像输入获得3D semantic occupancy预测。为了解决现有方法在结构化交通数据集上的局限性，我们在不结构化数据集上训练我们的模型，包括印度驾驶数据集和孟买苏驾驶数据集。我们的半监督训练管道，使得SOccDPT可以从有限标签的数据集中学习，而不需要手动标注。为了缓解训练过程中的内存限制，我们引入裁剪训练，其中每 epoch 选择一 subset of 参数进行训练，以减少训练过程中的内存使用。在无结构化交通和内存受限的训练和推理环境下，SOccDPT 在对 dispersive 估计方法的比较中，取得了RMSE 分数为9.1473， achieve semantic segmentation IoU 分数为46.02%，并在竞争性的频率69.47 Hz 上运行。我们将我们的代码和semantic occupancy数据集公开。
</details></li>
</ul>
<hr>
<h2 id="Using-Causal-Threads-to-Explain-Changes-in-a-Dynamic-System"><a href="#Using-Causal-Threads-to-Explain-Changes-in-a-Dynamic-System" class="headerlink" title="Using Causal Threads to Explain Changes in a Dynamic System"></a>Using Causal Threads to Explain Changes in a Dynamic System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11334">http://arxiv.org/abs/2311.11334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert B. Allen</li>
<li>for: 研究建立系统具有厚重 semantics的模型，具体是使用结构 causa explanations 描述系统的状态变化。</li>
<li>methods: 使用 проце序基于动态知识图进行建立。</li>
<li>results: 透过 Snowball Earth 理论中的地质变化的假设建立了一个 causa 线索模型，并描述了一个早期的图形界面来展示解释。Note: “厚重 semantics” (hòu zhòng xiàng yì) refers to the richness and depth of meaning in the models, “结构 causa explanations” (jié zhòng causa yì jiàn) refers to the causal explanations that are structured and explicit, and “ proces序基于动态知识图” (proces序 jī bù dòng tài zhī yì) refers to the process-based dynamic knowledge graphs.<details>
<summary>Abstract</summary>
We explore developing rich semantic models of systems. Specifically, we consider structured causal explanations about state changes in those systems. Essentially, we are developing process-based dynamic knowledge graphs. As an example, we construct a model of the causal threads for geological changes proposed by the Snowball Earth theory. Further, we describe an early prototype of a graphical interface to present the explanations. Unlike statistical approaches to summarization and explanation such as Large Language Models (LLMs), our approach of direct representation can be inspected and verified directly.
</details>
<details>
<summary>摘要</summary>
我们探索构建丰富 semantics的系统模型。 Specifically, we consider structured causal explanations about state changes in those systems. Essentially, we are developing process-based dynamic knowledge graphs. As an example, we construct a model of the causal threads for geological changes proposed by the Snowball Earth theory. Further, we describe an early prototype of a graphical interface to present the explanations. Unlike statistical approaches to summarization and explanation such as Large Language Models (LLMs), our approach of direct representation can be inspected and verified directly.Here's the breakdown of the translation:* 我们 (wǒmen) - we* 探索 (tànsuō) - explore* 构建 (gōngjiàn) - construct* 丰富 semantics (yífù xìngxì) - rich semantic* 系统 (xìtǒng) - system* 模型 (móxìng) - model* Specifically (xīngqī) - specifically* we consider (wǒmen jīngxīn) - we consider* structured causal explanations (lùxìng yòu yìxìng) - structured causal explanations* about state changes (zhèngxìng zhèngtiān) - about state changes* in those systems (fēngxiàng zhèngtiān) - in those systems* Essentially (yìxìng) - essentially* we are developing (wǒmen zhìdào) - we are developing* process-based dynamic knowledge graphs (jìxìng qùxìng zhìxìng) - process-based dynamic knowledge graphs* As an example (jìxìng) - as an example* we construct (wǒmen jìxìng) - we construct* a model of the causal threads (lùxìng yòu qiángxìng) - a model of the causal threads* for geological changes (dìxìng qìngxìng) - for geological changes* proposed by the Snowball Earth theory (xuěxìng zhèndài) - proposed by the Snowball Earth theory* Further (fùqì) - further* we describe (wǒmen xiǎngxìng) - we describe* an early prototype (shèngyì) - an early prototype* of a graphical interface (xìngxìng qiǎngxìng) - of a graphical interface* to present the explanations (jièxìng) - to present the explanations* Unlike statistical approaches (fājì) - unlike statistical approaches* to summarization and explanation (jièxìng) - to summarization and explanation* such as Large Language Models (LLMs) (xìngxìng) - such as Large Language Models (LLMs)* our approach (wǒmen fāngyì) - our approach* of direct representation (jìxìng) - of direct representation* can be inspected and verified directly (dìxiǎng yìxìng) - can be inspected and verified directly
</details></li>
</ul>
<hr>
<h2 id="Portuguese-FAQ-for-Financial-Services"><a href="#Portuguese-FAQ-for-Financial-Services" class="headerlink" title="Portuguese FAQ for Financial Services"></a>Portuguese FAQ for Financial Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11331">http://arxiv.org/abs/2311.11331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paulo Finardi, Wanderley M. Melo, Edgard D. Medeiros Neto, Alex F. Mansano, Pablo B. Costa, Vinicius F. Caridá</li>
<li>for: 提高Portuguese финан领域自然语言处理（NLP）应用的发展， Addressing the limitation of domain-specific data scarcity in the Portuguese financial domain.</li>
<li>methods: 使用数据增强技术生成synthetic数据， Employing data augmentation techniques to generate synthetic data.</li>
<li>results: 对具有不同semantic similarity的数据进行Supervised和Unsupervised任务， Evaluating the impact of augmented data on both low and high semantic similarity scenarios.<details>
<summary>Abstract</summary>
Scarcity of domain-specific data in the Portuguese financial domain has disfavored the development of Natural Language Processing (NLP) applications. To address this limitation, the present study advocates for the utilization of synthetic data generated through data augmentation techniques. The investigation focuses on the augmentation of a dataset sourced from the Central Bank of Brazil FAQ, employing techniques that vary in semantic similarity. Supervised and unsupervised tasks are conducted to evaluate the impact of augmented data on both low and high semantic similarity scenarios. Additionally, the resultant dataset will be publicly disseminated on the Hugging Face Datasets platform, thereby enhancing accessibility and fostering broader engagement within the NLP research community.
</details>
<details>
<summary>摘要</summary>
缺乏特定领域数据的问题在葡萄牙金融领域内，妨碍了自然语言处理（NLP）应用的发展。为解决这一问题，当前研究提议利用数据扩充技术生成的合成数据。研究将对由巴西中央银行FAQ获取的数据进行扩充，使用不同的semantic similarity的技术。用于超级vised和无supervised任务进行评估，以确定扩充数据对低和高semantic similarity场景的影响。此外，所生成的数据集也将在Hugging Face Datasets平台上公开发布，以便更多的研究人员可以访问和参与NLP研究领域。
</details></li>
</ul>
<hr>
<h2 id="Bounds-on-Representation-Induced-Confounding-Bias-for-Treatment-Effect-Estimation"><a href="#Bounds-on-Representation-Induced-Confounding-Bias-for-Treatment-Effect-Estimation" class="headerlink" title="Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation"></a>Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11321">http://arxiv.org/abs/2311.11321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentyn Melnychuk, Dennis Frauen, Stefan Feuerriegel</li>
<li>for: 本研究旨在提供一种新的，不受表示学习影响的 bounds  estimation 方法，用于估计受到维度减少（或其他约束）引起的 repression 抖音偏见。</li>
<li>methods: 我们提出一种新的、不受表示学习影响的 bounds  estimation 方法，包括 theoretically 确定 conditional average treatment effect 是不可识别的，以及提出一种用于估计 repression 抖音偏见的 partial identification 方法。</li>
<li>results: 我们在一系列实验中证明了我们的 bounds 可以准确地估计 repression 抖音偏见，并且可以在实际应用中提高 conditional average treatment effect 的可靠性。<details>
<summary>Abstract</summary>
State-of-the-art methods for conditional average treatment effect (CATE) estimation make widespread use of representation learning. Here, the idea is to reduce the variance of the low-sample CATE estimation by a (potentially constrained) low-dimensional representation. However, low-dimensional representations can lose information about the observed confounders and thus lead to bias, because of which the validity of representation learning for CATE estimation is typically violated. In this paper, we propose a new, representation-agnostic framework for estimating bounds on the representation-induced confounding bias that comes from dimensionality reduction (or other constraints on the representations) in CATE estimation. First, we establish theoretically under which conditions CATEs are non-identifiable given low-dimensional (constrained) representations. Second, as our remedy, we propose to perform partial identification of CATEs or, equivalently, aim at estimating of lower and upper bounds of the representation-induced confounding bias. We demonstrate the effectiveness of our bounds in a series of experiments. In sum, our framework is of direct relevance in practice where the validity of CATE estimation is of importance.
</details>
<details>
<summary>摘要</summary>
现代方法 для conditional average treatment effect（CATE）估计广泛使用表示学习。在这里，想法是通过（可能受限的）低维度表示来减少低样本CATE估计的方差。然而，低维度表示可能会失去观察的混杂变量信息，从而导致偏误，因此表示学习在CATE估计中的有效性通常会被违反。在这篇论文中，我们提出了一种新的、表示不依赖的框架，用于估计受到维度减少（或其他表示约束）在CATE估计中的表示偏误的上下限。首先，我们证明在低维度（受限的）表示下，CATE是非可定义的。其次，作为我们的药物，我们提议在CATE估计中进行部分标识或等效地估计表示偏误的下限和上限。我们在一系列实验中证明了bounds的效果。总之，我们的框架对实际中CATE估计的有效性具有直接的重要性。
</details></li>
</ul>
<hr>
<h2 id="GeoSAM-Fine-tuning-SAM-with-Sparse-and-Dense-Visual-Prompting-for-Automated-Segmentation-of-Mobility-Infrastructure"><a href="#GeoSAM-Fine-tuning-SAM-with-Sparse-and-Dense-Visual-Prompting-for-Automated-Segmentation-of-Mobility-Infrastructure" class="headerlink" title="GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure"></a>GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11319">http://arxiv.org/abs/2311.11319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafi Ibn Sultan, Chengyin Li, Hui Zhu, Prashant Khanduri, Marco Brocanelli, Dongxiao Zhu</li>
<li>for: 这篇论文主要针对的是地理图像分类，特别是使用 Segment Anything Model (SAM) 进行道路和人行道等基础设施的分类。</li>
<li>methods: 这篇论文提出了一个名为 Geographical SAM (GeoSAM) 的新的框架，它利用了细密的可视提示和稀有的可视提示，从零shot学习和预训练的 CNN 分类模型来精确地分类地理图像中的基础设施。</li>
<li>results: 根据论文的评估，GeoSAM 比较已有的方法更高效，对于道路基础设施的分类精度提高了20%，对于人行道基础设施的分类精度提高了14.29%，对于平均的分类精度提高了17.65%。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) has shown impressive performance when applied to natural image segmentation. However, it struggles with geographical images like aerial and satellite imagery, especially when segmenting mobility infrastructure including roads, sidewalks, and crosswalks. This inferior performance stems from the narrow features of these objects, their textures blending into the surroundings, and interference from objects like trees, buildings, vehicles, and pedestrians - all of which can disorient the model to produce inaccurate segmentation maps. To address these challenges, we propose Geographical SAM (GeoSAM), a novel SAM-based framework that implements a fine-tuning strategy using the dense visual prompt from zero-shot learning, and the sparse visual prompt from a pre-trained CNN segmentation model. The proposed GeoSAM outperforms existing approaches for geographical image segmentation, specifically by 20%, 14.29%, and 17.65% for road infrastructure, pedestrian infrastructure, and on average, respectively, representing a momentous leap in leveraging foundation models to segment mobility infrastructure including both road and pedestrian infrastructure in geographical images.
</details>
<details>
<summary>摘要</summary>
《Segment Anything Model》（SAM）在自然图像 segmentation 方面表现出色，但对于航空和卫星图像，特别是涉及到交通基础设施如路、人行道和横交道，其性能较差。这是因为这些物体的窄特征，Texture与周围环境杂mix，以及对象如树、建筑、车辆和行人的干扰，导致模型生成不准确的分 segmentation 图像。为解决这些挑战，我们提出了《地理 SAM》（GeoSAM），一种基于 SAM 的新框架，实现了精度调整策略，使用零shot learning  dense visual prompt 和预训练 CNN segmentation 模型 sparse visual prompt。提出的 GeoSAM 在 geographical 图像 segmentation 方面表现出优于现有方法，具体提高了20%、14.29%和17.65%，分别用于路基础设施、人行道基础设施和平均值，表示了基于基础模型 segment  mobility 基础设施，包括路基础设施和人行道基础设施的大幅提高。
</details></li>
</ul>
<hr>
<h2 id="TPTU-v2-Boosting-Task-Planning-and-Tool-Usage-of-Large-Language-Model-based-Agents-in-Real-world-Systems"><a href="#TPTU-v2-Boosting-Task-Planning-and-Tool-Usage-of-Large-Language-Model-based-Agents-in-Real-world-Systems" class="headerlink" title="TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems"></a>TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11315">http://arxiv.org/abs/2311.11315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, Xingyu Zeng, Rui Zhao</li>
<li>for: 提高LLM在真实系统中的任务规划和工具使用能力</li>
<li>methods: 提出了一个全面的框架，包括API选择器、LLM调参器和示例选择器，用于解决实际系统中的三大挑战</li>
<li>results: 通过使用实际系统和开源学术数据集进行验证，结果表明每个组件以及整个框架具有高效性<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated proficiency in addressing tasks that necessitate a combination of task planning and the usage of external tools that require a blend of task planning and the utilization of external tools, such as APIs. However, real-world complex systems present three prevalent challenges concerning task planning and tool usage: (1) The real system usually has a vast array of APIs, so it is impossible to feed the descriptions of all APIs to the prompt of LLMs as the token length is limited; (2) the real system is designed for handling complex tasks, and the base LLMs can hardly plan a correct sub-task order and API-calling order for such tasks; (3) Similar semantics and functionalities among APIs in real systems create challenges for both LLMs and even humans in distinguishing between them. In response, this paper introduces a comprehensive framework aimed at enhancing the Task Planning and Tool Usage (TPTU) abilities of LLM-based agents operating within real-world systems. Our framework comprises three key components designed to address these challenges: (1) the API Retriever selects the most pertinent APIs for the user task among the extensive array available; (2) LLM Finetuner tunes a base LLM so that the finetuned LLM can be more capable for task planning and API calling; (3) the Demo Selector adaptively retrieves different demonstrations related to hard-to-distinguish APIs, which is further used for in-context learning to boost the final performance. We validate our methods using a real-world commercial system as well as an open-sourced academic dataset, and the outcomes clearly showcase the efficacy of each individual component as well as the integrated framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="What-Lies-beyond-the-Pareto-Front-A-Survey-on-Decision-Support-Methods-for-Multi-Objective-Optimization"><a href="#What-Lies-beyond-the-Pareto-Front-A-Survey-on-Decision-Support-Methods-for-Multi-Objective-Optimization" class="headerlink" title="What Lies beyond the Pareto Front? A Survey on Decision-Support Methods for Multi-Objective Optimization"></a>What Lies beyond the Pareto Front? A Survey on Decision-Support Methods for Multi-Objective Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11288">http://arxiv.org/abs/2311.11288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zuzanna Osika, Jazmin Zatarain Salazar, Diederik M. Roijers, Frans A. Oliehoek, Pradeep K. Murukannaiah</li>
<li>for: 这 paper 的目的是解决多目标优化（MOO）算法生成的解决方案中的贸易offs。</li>
<li>methods: 这 paper 使用了许多不同的领域的研究方法，包括可视化、解决集挖掘、不确定性探索等方法，以探讨 MOO 算法生成的解决方案中的贸易offs。</li>
<li>results: 这 paper 提供了一种综合的方法来探讨 MOO 算法生成的解决方案中的贸易offs，独立于应用领域。这种方法可以减少研究者和实践者使用 MOO 算法的入门难度，同时也提供了新的研究方向，如交互性、解释性和伦理。<details>
<summary>Abstract</summary>
We present a review that unifies decision-support methods for exploring the solutions produced by multi-objective optimization (MOO) algorithms. As MOO is applied to solve diverse problems, approaches for analyzing the trade-offs offered by MOO algorithms are scattered across fields. We provide an overview of the advances on this topic, including methods for visualization, mining the solution set, and uncertainty exploration as well as emerging research directions, including interactivity, explainability, and ethics. We synthesize these methods drawing from different fields of research to build a unified approach, independent of the application. Our goals are to reduce the entry barrier for researchers and practitioners on using MOO algorithms and to provide novel research directions.
</details>
<details>
<summary>摘要</summary>
我们提出了一篇文章，汇集了多目标优化（MOO）算法生成的解决方案的决策支持方法的评估。由于MOO在解决多种问题时使用，关于这些方法的分析方法在不同领域中散布。我们提供了这些进展的概述，包括视觉化、解决集挖掘、不确定性探索以及emerging research direction，如交互、解释性和伦理。我们将这些方法从不同领域的研究中综合归纳，建立一个独立于应用的统一方法，以降低MOO算法的入门难度，并提供新的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Tactile-Active-Inference-Reinforcement-Learning-for-Efficient-Robotic-Manipulation-Skill-Acquisition"><a href="#Tactile-Active-Inference-Reinforcement-Learning-for-Efficient-Robotic-Manipulation-Skill-Acquisition" class="headerlink" title="Tactile Active Inference Reinforcement Learning for Efficient Robotic Manipulation Skill Acquisition"></a>Tactile Active Inference Reinforcement Learning for Efficient Robotic Manipulation Skill Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11287">http://arxiv.org/abs/2311.11287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Liu, Xing Liu, Yizhai Zhang, Zhengxiong Liu, Panfeng Huang<br>for:这个研究旨在开发一种新的机器人掌握技术，以提高机器人在实际应用中的效率和适应能力。methods:我们提出了一种名为“感触主动推导学习”（Tactile-AIRL）的新方法，具有高效训练的能力。这个方法结合了活动推测和内在好奇，以提高RL的训练效率和适应率。此外，我们还使用了视觉感知器来提供细节的感知，以便进行掌握任务。results:我们在非掌握物推动任务上进行了 simulations，结果显示了我们的方法能够实现高效训练。它能够在少量互动回合中将代理人培训到极高水平，超过SAC基准。此外，我们还进行了实验，用于螺旋夹缔任务，结果显示了我们的方法具有快速学习能力和实际应用潜力。<details>
<summary>Abstract</summary>
Robotic manipulation holds the potential to replace humans in the execution of tedious or dangerous tasks. However, control-based approaches are not suitable due to the difficulty of formally describing open-world manipulation in reality, and the inefficiency of existing learning methods. Thus, applying manipulation in a wide range of scenarios presents significant challenges. In this study, we propose a novel method for skill learning in robotic manipulation called Tactile Active Inference Reinforcement Learning (Tactile-AIRL), aimed at achieving efficient training. To enhance the performance of reinforcement learning (RL), we introduce active inference, which integrates model-based techniques and intrinsic curiosity into the RL process. This integration improves the algorithm's training efficiency and adaptability to sparse rewards. Additionally, we utilize a vision-based tactile sensor to provide detailed perception for manipulation tasks. Finally, we employ a model-based approach to imagine and plan appropriate actions through free energy minimization. Simulation results demonstrate that our method achieves significantly high training efficiency in non-prehensile objects pushing tasks. It enables agents to excel in both dense and sparse reward tasks with just a few interaction episodes, surpassing the SAC baseline. Furthermore, we conduct physical experiments on a gripper screwing task using our method, which showcases the algorithm's rapid learning capability and its potential for practical applications.
</details>
<details>
<summary>摘要</summary>
роботизированная манипуляция имеет потенциал заменить людей в выполнении утомительных или опасных задач. Однако, подход на основе контроля неприменим из-за трудности формального описания открытого мира манипуляции в реальности и неэффективности существующих методов обучения. Поэтому применение манипуляции в широком диапазоне сценариев представляет значительные вызовы. В этой статье мы предлагаем новый метод для обучения навыков в роботизированной манипуляции, называемый Тактильным активным инфериенсом реинфорcement learning (Tactile-AIRL), направленный на достижение эффективной подготовки.Чтобы улучшить эффективность реинфорcement learning (RL), мы вводим активный инфериент, который интегрирует моделирующие техники и внутреннюю любопытство в процесс RL. Это интеграция улучшает алгоритм подготовки и адаптацию к скудным наградам. Кроме того, мы используем визон-базированный тактильный сенсор для предоставления подробного восприятия для задач манипуляции. Наконец, мы используем модель-ориентированный подход для представления и планирования подходящих действий через минимизацию свободной энергии. Симуляционные результаты показывают, что наш метод достигает значительно высокой эффективности обучения в задачах непреhensible objects pushing. Он позволяет агентам превзойти базовый SAC в обоих задачах с dense и sparse reward с только несколькими эпизодами взаимодействия, а также физические эксперименты на задаче гриппера скрепления, которые демонстрируют способность алгоритма быстро учиться и его потенциал для практических применений.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Prompt-Tuning-for-Vision-Language-Models"><a href="#Adversarial-Prompt-Tuning-for-Vision-Language-Models" class="headerlink" title="Adversarial Prompt Tuning for Vision-Language Models"></a>Adversarial Prompt Tuning for Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11261">http://arxiv.org/abs/2311.11261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaming Zhang, Xingjun Ma, Xin Wang, Lingyu Qiu, Jiaqi Wang, Yu-Gang Jiang, Jitao Sang</li>
<li>for: 提高多modal学习中VLM的攻击鲁棒性，尤其是图像模式下的攻击鲁棒性问题。</li>
<li>methods: 利用可学习的文本提示和对抗图像嵌入的对应关系，以提高VLM的对抗攻击能力。</li>
<li>results: 提高了对白盒和黑盒攻击的抵抗力，并与现有的图像处理基础技术相结合，进一步提高了防御能力。<details>
<summary>Abstract</summary>
With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code will be available upon publication of the paper.
</details>
<details>
<summary>摘要</summary>
随着多模态学习的快速发展，预训练的视觉语言模型（VLM）如CLIP已经显示出了跨模态桥接的remarkable capacities。然而，这些模型仍然容易受到恶意攻击，特别是在图像模式下，存在重大的安全隐患。本文介绍了一种新的技术——挑战性提示调整（AdvPT），用于增强VLM中图像编码器的恶意攻击抵抗力。AdvPT利用可学习的文本提示和恶意图像嵌入的对齐，以解决VLM中的潜在漏洞，不需要大量参数训练或改变模型结构。我们的实验表明，AdvPT可以提高白盒和黑盒恶意攻击的抵抗力，并且与现有的图像处理基于防御技术相乘 synergistic effect，进一步增强防御能力。我们的实验分析也为挑战性提示调整提供了深入的理解，开启了未来robust multimodal learning研究的新可能性。这些发现将为VLM的安全提供新的可能性。我们的代码将在文章发表后提供。
</details></li>
</ul>
<hr>
<h2 id="Tensor-networks-for-interpretable-and-efficient-quantum-inspired-machine-learning"><a href="#Tensor-networks-for-interpretable-and-efficient-quantum-inspired-machine-learning" class="headerlink" title="Tensor networks for interpretable and efficient quantum-inspired machine learning"></a>Tensor networks for interpretable and efficient quantum-inspired machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11258">http://arxiv.org/abs/2311.11258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shi-Ju Ran, Gang Su</li>
<li>for: 本研究旨在 simultaneously gain high interpretability 和 efficiency with the current schemes of deep machine learning (ML).</li>
<li>methods: 本研究使用 tensor network (TN)，which is a well-established mathematical tool originating from quantum mechanics，to develop efficient “white-box” ML schemes.</li>
<li>results: 研究表明，TN ML 可以具有强大的 interpretability 和高效率，并且可能在 quantum computers 上实现新的 schemes。<details>
<summary>Abstract</summary>
It is a critical challenge to simultaneously gain high interpretability and efficiency with the current schemes of deep machine learning (ML). Tensor network (TN), which is a well-established mathematical tool originating from quantum mechanics, has shown its unique advantages on developing efficient ``white-box'' ML schemes. Here, we give a brief review on the inspiring progresses made in TN-based ML. On one hand, interpretability of TN ML is accommodated with the solid theoretical foundation based on quantum information and many-body physics. On the other hand, high efficiency can be rendered from the powerful TN representations and the advanced computational techniques developed in quantum many-body physics. With the fast development on quantum computers, TN is expected to conceive novel schemes runnable on quantum hardware, heading towards the ``quantum artificial intelligence'' in the forthcoming future.
</details>
<details>
<summary>摘要</summary>
现在的深度机器学习（ML）中，同时获得高度可解释性和高效性是一项极其挑战性的任务。量子网络（TN）是一种已经成熔的数学工具，它在量子力学中的起源，在深度学习中表现出独特的优势。以下是对TN在深度学习中的进步的简短回顾。一方面，TN的可解释性在量子信息和多体物理理论的坚实基础上得到保障。另一方面，TN的表示力和量子多体物理的高级计算技术使得高效性得到实现。随着量子计算机的快速发展，TN预计会推出新的可运行在量子硬件上的方案，逐渐实现“量子人工智能”的未来。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Review-on-Sentiment-Analysis-Tasks-Approaches-and-Applications"><a href="#A-Comprehensive-Review-on-Sentiment-Analysis-Tasks-Approaches-and-Applications" class="headerlink" title="A Comprehensive Review on Sentiment Analysis: Tasks, Approaches and Applications"></a>A Comprehensive Review on Sentiment Analysis: Tasks, Approaches and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11250">http://arxiv.org/abs/2311.11250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudhanshu Kumar, Partha Pratim Roy, Debi Prosad Dogra, Byung-Gyu Kim</li>
<li>For: The paper is written for researchers and practitioners in the field of sentiment analysis and text mining, as well as those interested in understanding the current state of the field and its applications in various domains.* Methods: The paper uses a lexicon-based approach and deep learning techniques to analyze sentiment in text, images, videos, and voice data. It also discusses the challenges and opportunities of sentiment analysis in different domains.* Results: The paper provides an overview of the recent research and development in sentiment analysis, including its applications in various domains and the challenges and limitations of the field. It also highlights the potential of deep learning techniques in improving the accuracy of sentiment analysis.<details>
<summary>Abstract</summary>
Sentiment analysis (SA) is an emerging field in text mining. It is the process of computationally identifying and categorizing opinions expressed in a piece of text over different social media platforms. Social media plays an essential role in knowing the customer mindset towards a product, services, and the latest market trends. Most organizations depend on the customer's response and feedback to upgrade their offered products and services. SA or opinion mining seems to be a promising research area for various domains. It plays a vital role in analyzing big data generated daily in structured and unstructured formats over the internet. This survey paper defines sentiment and its recent research and development in different domains, including voice, images, videos, and text. The challenges and opportunities of sentiment analysis are also discussed in the paper.   \keywords{Sentiment Analysis, Machine Learning, Lexicon-based approach, Deep Learning, Natural Language Processing}
</details>
<details>
<summary>摘要</summary>
关键字：Sentiment Analysis, Machine Learning, Lexicon-based approach, Deep Learning, Natural Language Processing
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Dandelion-Network-for-IoT-Intrusion-Detection"><a href="#Open-Set-Dandelion-Network-for-IoT-Intrusion-Detection" class="headerlink" title="Open Set Dandelion Network for IoT Intrusion Detection"></a>Open Set Dandelion Network for IoT Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11249">http://arxiv.org/abs/2311.11249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiashu Wu, Hao Dai, Kenneth B. Kent, Jerome Yen, Chengzhong Xu, Yang Wang</li>
<li>for: 这篇论文的目的是提出一个基于无监督多元领域适应的开放集合网络（OSDN）模型，以对 IoT 设备的攻击进行更 precisely 的检测。</li>
<li>methods: 这篇论文使用了一个基于无监督多元领域适应的开放集合网络（OSDN）模型，包括了以下几个方法：1) 将知识源网络攻击领域转换到目标 IoT 攻击领域中，以实现更高精度的检测；2) 使用了一个基于牛顿轨道的目标网络，以实现更好的检测效果；3) 使用了一个对应数据的矩阵来实现更好的检测效果。</li>
<li>results: 根据实验结果，这篇论文的 OSDN 模型比三个州际前方法提高了16.9%。<details>
<summary>Abstract</summary>
As IoT devices become widely, it is crucial to protect them from malicious intrusions. However, the data scarcity of IoT limits the applicability of traditional intrusion detection methods, which are highly data-dependent. To address this, in this paper we propose the Open-Set Dandelion Network (OSDN) based on unsupervised heterogeneous domain adaptation in an open-set manner. The OSDN model performs intrusion knowledge transfer from the knowledge-rich source network intrusion domain to facilitate more accurate intrusion detection for the data-scarce target IoT intrusion domain. Under the open-set setting, it can also detect newly-emerged target domain intrusions that are not observed in the source domain. To achieve this, the OSDN model forms the source domain into a dandelion-like feature space in which each intrusion category is compactly grouped and different intrusion categories are separated, i.e., simultaneously emphasising inter-category separability and intra-category compactness. The dandelion-based target membership mechanism then forms the target dandelion. Then, the dandelion angular separation mechanism achieves better inter-category separability, and the dandelion embedding alignment mechanism further aligns both dandelions in a finer manner. To promote intra-category compactness, the discriminating sampled dandelion mechanism is used. Assisted by the intrusion classifier trained using both known and generated unknown intrusion knowledge, a semantic dandelion correction mechanism emphasises easily-confused categories and guides better inter-category separability. Holistically, these mechanisms form the OSDN model that effectively performs intrusion knowledge transfer to benefit IoT intrusion detection. Comprehensive experiments on several intrusion datasets verify the effectiveness of the OSDN model, outperforming three state-of-the-art baseline methods by 16.9%.
</details>
<details>
<summary>摘要</summary>
随着互联网物联网（IoT）设备的普及，保护它们从恶意攻击中免受伤害变得非常重要。然而，由于IoT数据的稀缺，传统的攻击检测方法无法应用，这些方法高度依赖数据。为解决这个问题，我们在这篇论文中提出了基于不supervised heterogeneous domain adaptation的开放集合网络（OSDN）模型。OSDN模型通过从知识充足的源网络攻击域传输攻击知识来提高IoT攻击域的检测精度。在开放集合 Setting下，它还可以检测新出现的目标域攻击，这些攻击没有在源域观察到。为实现这一目标，OSDN模型将源域转化为一个类似于芽孢的特征空间，其中每种攻击类型都是紧凑地归类，同时也能够准确地分割不同的攻击类型，即同时强调 между类别之间的分离和内部分类之间的凝固。然后，目标域被转化为一个类似于芽孢的特征空间，并使用芽孢角度分离机制来实现更好的 между类别之间的分离。最后，通过芽孢嵌入对Alignment机制来进一步准确地匹配两个芽孢。为提高内部分类之间的凝固，我们采用了一种吸引样本芽孢机制，该机制通过使用已知和生成的未知攻击知识来训练攻击分类器。此外，我们还采用了一种 semantic dandelion correction mechanism，该机制通过强调易混淆的类别来提高between类别之间的分离。总的来说，这些机制组成了OSDN模型，该模型可以有效地传输攻击知识，以便为IoT攻击检测提供更高的精度。我们在多个攻击数据集上进行了广泛的实验，结果表明OSDN模型比三种state-of-the-art基eline方法高于16.9%。
</details></li>
</ul>
<hr>
<h2 id="AtomXR-Streamlined-XR-Prototyping-with-Natural-Language-and-Immersive-Physical-Interaction"><a href="#AtomXR-Streamlined-XR-Prototyping-with-Natural-Language-and-Immersive-Physical-Interaction" class="headerlink" title="AtomXR: Streamlined XR Prototyping with Natural Language and Immersive Physical Interaction"></a>AtomXR: Streamlined XR Prototyping with Natural Language and Immersive Physical Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11238">http://arxiv.org/abs/2311.11238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alice Cai, Caine Ardayfio, AnhPhu Nguyen, Tica Lin, Elena Glassman</li>
<li>for: 帮助开发者快速创建扩展现实（XR）应用程序，不需要深入学习开发技能。</li>
<li>methods: 使用自然语言、眼动和触摸交互，并采用高级编程语言AtomScript，以及具有多modal输入的自然语言 интер페이ス。</li>
<li>results: 经验证明，AtomXR可以提供快速、简单的开发体验，并且在速度和用户体验方面与传统系统相比有显著提高。<details>
<summary>Abstract</summary>
As technological advancements in extended reality (XR) amplify the demand for more XR content, traditional development processes face several challenges: 1) a steep learning curve for inexperienced developers, 2) a disconnect between 2D development environments and 3D user experiences inside headsets, and 3) slow iteration cycles due to context switching between development and testing environments. To address these challenges, we introduce AtomXR, a streamlined, immersive, no-code XR prototyping tool designed to empower both experienced and inexperienced developers in creating applications using natural language, eye-gaze, and touch interactions. AtomXR consists of: 1) AtomScript, a high-level human-interpretable scripting language for rapid prototyping, 2) a natural language interface that integrates LLMs and multimodal inputs for AtomScript generation, and 3) an immersive in-headset authoring environment. Empirical evaluation through two user studies offers insights into natural language-based and immersive prototyping, and shows AtomXR provides significant improvements in speed and user experience compared to traditional systems.
</details>
<details>
<summary>摘要</summary>
随着扩展现实（XR）技术的发展，需求更多的XR内容的增加，传统的开发过程面临着一些挑战：1）开发人员无经验的陡峭学习曲线，2）在头盔中的用户体验与2D开发环境之间的断开，3）由于开发和测试环境之间的上下文切换而导致的慢速迭代循环。为解决这些挑战，我们介绍了AtomXR，一种简化的、 immerse 的、无代码 XR 原型工具，旨在帮助经验丰富和无经验的开发人员在使用自然语言、眼动和触摸交互的情况下创建应用程序。AtomXR包括：1）AtomScript，一种高级别的人类可解释的脚本语言，2）一种与大数据学习机器 integrate 的自然语言界面，3）一种 immerse 在头盔中的作者环境。经验证明了在两个用户研究中，AtomXR 可以提供传统系统的速度和用户体验方面的显著改进。
</details></li>
</ul>
<hr>
<h2 id="Implementation-of-AI-Deep-Learning-Algorithm-For-Multi-Modal-Sentiment-Analysis"><a href="#Implementation-of-AI-Deep-Learning-Algorithm-For-Multi-Modal-Sentiment-Analysis" class="headerlink" title="Implementation of AI Deep Learning Algorithm For Multi-Modal Sentiment Analysis"></a>Implementation of AI Deep Learning Algorithm For Multi-Modal Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11237">http://arxiv.org/abs/2311.11237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhen Wang</li>
<li>for: 本研究设计了一种多模式认情方法，结合了两条通道卷积神经网络和环网。这种方法可以有效提取情感信息，提高学习效率。</li>
<li>methods: 词汇vector化使用GloVe，将词汇vector输入卷积神经网络。兼用注意机制和最大池化抽象BiSRU通道，实现本地深度情感和预后序情感 semantics。最后，融合多个特征，将极性情感输入为认情数据集的认情准确率。</li>
<li>results: 实验显示，基于特征融合的情感分析方法可以增强情感认知数据集的认知率和减少学习时间。模型具有一定的通用性。<details>
<summary>Abstract</summary>
A multi-modal emotion recognition method was established by combining two-channel convolutional neural network with ring network. This method can extract emotional information effectively and improve learning efficiency. The words were vectorized with GloVe, and the word vector was input into the convolutional neural network. Combining attention mechanism and maximum pool converter BiSRU channel, the local deep emotion and pre-post sequential emotion semantics are obtained. Finally, multiple features are fused and input as the polarity of emotion, so as to achieve the emotion analysis of the target. Experiments show that the emotion analysis method based on feature fusion can effectively improve the recognition accuracy of emotion data set and reduce the learning time. The model has a certain generalization.
</details>
<details>
<summary>摘要</summary>
一种多Modal Emotion recognition方法由两个通道卷积神经网络与环形网络结合，该方法可以有效提取情感信息并提高学习效率。文本被vector化使用GloVe，word vector输入卷积神经网络。结合注意机制和最大池化converter BiSRU通道，本地深情和预后序情态 semantics 得到。最后，多个特征 fusion 并输入为情感趋势，以实现目标情感分析。实验显示，基于特征合并的情感分析方法可以提高情感数据集的识别率和减少学习时间。模型具有一定的普适性。Here's the word-for-word translation:一种多Modal Emotion recognition方法由两个通道卷积神经网络与环形网络结合，该方法可以有效提取情感信息并提高学习效率。文本被vector化使用GloVe，word vector输入卷积神经网络。结合注意机制和最大池化converter BiSRU通道，本地深情和预后序情态 semantics 得到。最后，多个特征 fusion 并输入为情感趋势，以实现目标情感分析。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Anomaly’-in-Time-Series-Anomaly-Detection-A-Self-supervised-Tri-domain-Solution"><a href="#Unraveling-the-Anomaly’-in-Time-Series-Anomaly-Detection-A-Self-supervised-Tri-domain-Solution" class="headerlink" title="Unraveling the &#96;Anomaly’ in Time Series Anomaly Detection: A Self-supervised Tri-domain Solution"></a>Unraveling the &#96;Anomaly’ in Time Series Anomaly Detection: A Self-supervised Tri-domain Solution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11235">http://arxiv.org/abs/2311.11235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuting Sun, Guansong Pang, Guanhua Ye, Tong Chen, Xia Hu, Hongzhi Yin</li>
<li>for: 本研究旨在提出一种基于自动学习的时序异常检测方法，以解决现有时序异常检测中的缺乏异常标签和异常形态的问题。</li>
<li>methods: 本研究使用了三个数据频谱域（时间频谱、频率频谱和差异频谱）中的特征模型，不需要异常标签。它还使用了两种对比损失函数：间频域对比损失和自适应对比损失，以学习通用特征和分辨异常数据。</li>
<li>results: 经过实验证明，TriAD方法可以在UCR数据集上达到三倍的SOTA深度学习模型的PA%K基于F1分数，并且与SOTA对比发现算法相比，提高了50%的准确率。<details>
<summary>Abstract</summary>
The ongoing challenges in time series anomaly detection (TSAD), notably the scarcity of anomaly labels and the variability in anomaly lengths and shapes, have led to the need for a more efficient solution. As limited anomaly labels hinder traditional supervised models in TSAD, various SOTA deep learning techniques, such as self-supervised learning, have been introduced to tackle this issue. However, they encounter difficulties handling variations in anomaly lengths and shapes, limiting their adaptability to diverse anomalies. Additionally, many benchmark datasets suffer from the problem of having explicit anomalies that even random functions can detect. This problem is exacerbated by ill-posed evaluation metrics, known as point adjustment (PA), which can result in inflated model performance. In this context, we propose a novel self-supervised learning based Tri-domain Anomaly Detector (TriAD), which addresses these challenges by modeling features across three data domains - temporal, frequency, and residual domains - without relying on anomaly labels. Unlike traditional contrastive learning methods, TriAD employs both inter-domain and intra-domain contrastive loss to learn common attributes among normal data and differentiate them from anomalies. Additionally, our approach can detect anomalies of varying lengths by integrating with a discord discovery algorithm. It is worth noting that this study is the first to reevaluate the deep learning potential in TSAD, utilizing both rigorously designed datasets (i.e., UCR Archive) and evaluation metrics (i.e., PA%K and affiliation). Through experimental results on the UCR dataset, TriAD achieves an impressive three-fold increase in PA%K based F1 scores over SOTA deep learning models, and 50% increase of accuracy as compared to SOTA discord discovery algorithms.
</details>
<details>
<summary>摘要</summary>
Traditional supervised learning models in time series anomaly detection (TSAD) have been limited by the scarcity of anomaly labels and the variability in anomaly lengths and shapes. To address these challenges, various state-of-the-art (SOTA) deep learning techniques have been introduced, such as self-supervised learning. However, these methods have difficulty handling variations in anomaly lengths and shapes, limiting their adaptability to diverse anomalies. Additionally, many benchmark datasets suffer from the problem of explicit anomalies that can be detected by random functions, which is exacerbated by the use of ill-posed evaluation metrics such as point adjustment (PA).To address these challenges, we propose a novel self-supervised learning based Tri-domain Anomaly Detector (TriAD). TriAD models features across three data domains - temporal, frequency, and residual domains - without relying on anomaly labels. Unlike traditional contrastive learning methods, TriAD employs both inter-domain and intra-domain contrastive loss to learn common attributes among normal data and differentiate them from anomalies. Additionally, TriAD can detect anomalies of varying lengths by integrating with a discord discovery algorithm.This study is the first to reevaluate the deep learning potential in TSAD, utilizing both rigorously designed datasets (i.e., UCR Archive) and evaluation metrics (i.e., PA%K and affiliation). Through experimental results on the UCR dataset, TriAD achieves an impressive three-fold increase in PA%K based F1 scores over SOTA deep learning models, and a 50% increase in accuracy as compared to SOTA discord discovery algorithms.
</details></li>
</ul>
<hr>
<h2 id="FedRA-A-Random-Allocation-Strategy-for-Federated-Tuning-to-Unleash-the-Power-of-Heterogeneous-Clients"><a href="#FedRA-A-Random-Allocation-Strategy-for-Federated-Tuning-to-Unleash-the-Power-of-Heterogeneous-Clients" class="headerlink" title="FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients"></a>FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11227">http://arxiv.org/abs/2311.11227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leondada/fedra">https://github.com/leondada/fedra</a></li>
<li>paper_authors: Shangchao Su, Bin Li, Xiangyang Xue</li>
<li>for: 这个论文是关于 federated learning 中的基础模型调参问题，特别是在多客户端上协同调参基础模型时，如何处理客户端之间的不同计算和通信资源问题。</li>
<li>methods: 该论文提出了一种新的 federated tuning 算法，即 FedRA，它可以在多客户端上协同调参基础模型，不需要修改原始模型。在每次通信轮次中，FedRA 随机生成分配矩阵，对于资源有限的客户端，它将重新组织一小部分层并使用 LoRA 进行微调，然后将更新后的 LoRA 参数回传给服务器。</li>
<li>results: 该论文在两个大规模的图像数据集上进行了实验，包括 DomainNet 和 NICO++，在不同的非同异设置下进行了比较，结果表明，FedRA 与比较方法相比，表现出了显著的优势。<details>
<summary>Abstract</summary>
With the increasing availability of Foundation Models, federated tuning has garnered attention in the field of federated learning, utilizing data and computation resources from multiple clients to collaboratively fine-tune foundation models. However, in real-world federated scenarios, there often exist a multitude of heterogeneous clients with varying computation and communication resources, rendering them incapable of supporting the entire model fine-tuning process. In response to this challenge, we propose a novel federated tuning algorithm, FedRA. The implementation of FedRA is straightforward and can be seamlessly integrated into any transformer-based model without the need for further modification to the original model. Specifically, in each communication round, FedRA randomly generates an allocation matrix. For resource-constrained clients, it reorganizes a small number of layers from the original model based on the allocation matrix and fine-tunes using LoRA. Subsequently, the server aggregates the updated LoRA parameters from the clients according to the current allocation matrix into the corresponding layers of the original model. It is worth noting that FedRA also supports scenarios where none of the clients can support the entire global model, which is an impressive advantage. We conduct experiments on two large-scale image datasets, DomainNet and NICO++, under various non-iid settings. The results demonstrate that FedRA outperforms the compared methods significantly. The source code is available at \url{https://github.com/leondada/FedRA}.
</details>
<details>
<summary>摘要</summary>
随着基础模型的可用性的增加，联邦调参在联邦学习领域受到了关注，利用多个客户端的数据和计算资源进行共同调参基础模型。然而，在实际的联邦场景中，有多种不同的计算和通信资源，使得客户端无法支持整个模型调参过程。为了解决这个挑战，我们提出了一种新的联邦调参算法，即 FedRA。FedRA的实现方式直观，可以透明地与任何基于转换器的模型集成，无需修改原始模型。在每次通信轮次中，FedRA随机生成一个分配矩阵。对于资源受限的客户端，它将原始模型中的一小部分层重新排序，根据分配矩阵进行练习，并使用LoRA进行微调。然后，服务器根据当前分配矩阵将客户端上的LoRA参数更新到原始模型中相应的层中。需要注意的是，FedRA还支持情况下，无法支持整个全球模型的客户端，这是一项非常优势的特点。我们在DomainNet和NICO++两个大规模图像 dataset上进行了多种非同域设置的实验，结果显示FedRA与比较方法相比有显著的优势。源代码可以在 GitHub 上获取，具体请参考 \url{https://github.com/leondada/FedRA}.
</details></li>
</ul>
<hr>
<h2 id="An-Interactive-Query-Generation-Assistant-using-LLM-based-Prompt-Modification-and-User-Feedback"><a href="#An-Interactive-Query-Generation-Assistant-using-LLM-based-Prompt-Modification-and-User-Feedback" class="headerlink" title="An Interactive Query Generation Assistant using LLM-based Prompt Modification and User Feedback"></a>An Interactive Query Generation Assistant using LLM-based Prompt Modification and User Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11226">http://arxiv.org/abs/2311.11226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaustubh D. Dhole, Ramraj Chandradevan, Eugene Agichtein<br>for: 这篇论文的目的是提出一种新的查询生成助手，用于帮助用户在搜索过程中提出更有效的查询。methods: 该论文使用了多种自然语言处理技术，如语言模型（LLM）和人工智能技术，以帮助用户提出更有效的查询。results: 该论文的实验结果表明，使用该查询生成助手可以帮助用户提出更有效的查询，并且可以帮助用户更好地满足他们的搜索需求。<details>
<summary>Abstract</summary>
While search is the predominant method of accessing information, formulating effective queries remains a challenging task, especially for situations where the users are not familiar with a domain, or searching for documents in other languages, or looking for complex information such as events, which are not easily expressible as queries. Providing example documents or passages of interest, might be easier for a user, however, such query-by-example scenarios are prone to concept drift, and are highly sensitive to the query generation method. This demo illustrates complementary approaches of using LLMs interactively, assisting and enabling the user to provide edits and feedback at all stages of the query formulation process. The proposed Query Generation Assistant is a novel search interface which supports automatic and interactive query generation over a mono-linguial or multi-lingual document collection. Specifically, the proposed assistive interface enables the users to refine the queries generated by different LLMs, to provide feedback on the retrieved documents or passages, and is able to incorporate the users' feedback as prompts to generate more effective queries. The proposed interface is a valuable experimental tool for exploring fine-tuning and prompting of LLMs for query generation to qualitatively evaluate the effectiveness of retrieval and ranking models, and for conducting Human-in-the-Loop (HITL) experiments for complex search tasks where users struggle to formulate queries without such assistance.
</details>
<details>
<summary>摘要</summary>
“寻找信息是主要的访问方法，但是寻找有效的查询仍然是一个困难的任务，特别是当用户不熟悉一个领域，搜寻文档written in other languages，或者搜寻复杂的信息，如事件，这些不易表示为查询。提供示例文档或 интерес的段落，可能是更容易的选择，但这些查询例子enario是易受概念变化的，并且高度受到查询生成方法的影响。本 demo 显示了辅助式查询生成器的可能性，这个 novel search interface 可以在单语言或多语言文档集上进行自动和互动查询生成。具体来说，这个辅助式查询生成器可以让用户在不同的 LLMs 生成的查询中进行修改和反馈，并且能够将用户的反馈当作提示，以生成更有效的查询。本 interface 是一个实用的实验工具，可以用于调整和提示 LLMs 的查询生成，以评估搜寻和排名模型的效iveness，以及进行人类在循环（HITL）实验，用于复杂的搜寻任务，当用户无法自动形成查询。”
</details></li>
</ul>
<hr>
<h2 id="SPLAIN-Augmenting-CybersecurityWarnings-with-Reasons-and-Data"><a href="#SPLAIN-Augmenting-CybersecurityWarnings-with-Reasons-and-Data" class="headerlink" title="SPLAIN: Augmenting CybersecurityWarnings with Reasons and Data"></a>SPLAIN: Augmenting CybersecurityWarnings with Reasons and Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11215">http://arxiv.org/abs/2311.11215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vera A. Kazakova, Jena D. Hwang, Bonnie J. Dorr, Yorick Wilks, J. Blake Gage, Alex Memory, Mark A. Clark</li>
<li>for: 这篇论文旨在提供一种自然语言生成器，帮助用户更好地理解和预防网络攻击。</li>
<li>methods: 该论文提出了一种名为Simplified Plaintext Language（SPLAIN）的自然语言生成器，可以将警告数据转化为易于理解的网络攻击预测结果。SPLAIN使用模板基本法，确保警告结构和词汇准确性。</li>
<li>results: 该论文的实验结果表明，SPLAIN可以生成清晰、操作性强的网络攻击预测结果，并且可以在需要时扩展每个威胁和其组成部分，以便提供更多的信息。<details>
<summary>Abstract</summary>
Effective cyber threat recognition and prevention demand comprehensible forecasting systems, as prior approaches commonly offer limited and, ultimately, unconvincing information. We introduce Simplified Plaintext Language (SPLAIN), a natural language generator that converts warning data into user-friendly cyber threat explanations. SPLAIN is designed to generate clear, actionable outputs, incorporating hierarchically organized explanatory details about input data and system functionality. Given the inputs of individual sensor-induced forecasting signals and an overall warning from a fusion module, SPLAIN queries each signal for information on contributing sensors and data signals. This collected data is processed into a coherent English explanation, encompassing forecasting, sensing, and data elements for user review. SPLAIN's template-based approach ensures consistent warning structure and vocabulary. SPLAIN's hierarchical output structure allows each threat and its components to be expanded to reveal underlying explanations on demand. Our conclusions emphasize the need for designers to specify the "how" and "why" behind cyber warnings, advocate for simple structured templates in generating consistent explanations, and recognize that direct causal links in Machine Learning approaches may not always be identifiable, requiring some explanations to focus on general methodologies, such as model and training data.
</details>
<details>
<summary>摘要</summary>
有效的网络威胁认识和预防需要可读取的预测系统，因为先前的方法通常只提供有限和不可靠的信息。我们介绍了简化平文语言（SPLAIN），一种自然语言生成器，它将警告数据转换成易于理解的网络威胁说明。SPLAIN是为了生成清晰、可行的输出，并将输入数据和系统功能进行层次结构化解释。对于每个感知器生成的警告信号，SPLAIN会查询每个信号的相关感知器和数据信号。这些收集的数据被处理成一个 coherent English 说明，以便用户审查。SPLAIN 的模板化方法保证了警告结构的一致性和词汇。SPLAIN 的层次输出结构允许每个威胁和其组成部分进行扩展，以显示底层的解释。我们的结论认为设计者们应该指定警告中的 "如何" 和 "为什么"，支持简化的模板，并认可直接 causal links 在机器学习方法中可能无法识别，需要一些解释专注于方法ologies，如模型和训练数据。
</details></li>
</ul>
<hr>
<h2 id="Can-We-Utilize-Pre-trained-Language-Models-within-Causal-Discovery-Algorithms"><a href="#Can-We-Utilize-Pre-trained-Language-Models-within-Causal-Discovery-Algorithms" class="headerlink" title="Can We Utilize Pre-trained Language Models within Causal Discovery Algorithms?"></a>Can We Utilize Pre-trained Language Models within Causal Discovery Algorithms?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11212">http://arxiv.org/abs/2311.11212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chanhui Lee, Juhyeon Kim, Yongjun Jeong, Juhyun Lyu, Junghee Kim, Sangmin Lee, Sangjun Han, Hyeokjun Choe, Soyeon Park, Woohyung Lim, Sungbin Lim, Sanghack Lee</li>
<li>for: 本研究探讨了预训语言模型（PLM）在 causal reasoning 领域的可扩展性。</li>
<li>methods: 研究使用了 specifically designed prompts 来启发 PLM 进行多次 causal reasoning，并将其结果聚合以实现 causal discovery。</li>
<li>results: 研究发现 PLM-based causal reasoning 受到 prompt design 和 overconfidence 等限制，并提出了一种新的框架，通过结合 PLM 提取的 prior knowledge 和 causal discovery 算法来提高性能。<details>
<summary>Abstract</summary>
Scaling laws have allowed Pre-trained Language Models (PLMs) into the field of causal reasoning. Causal reasoning of PLM relies solely on text-based descriptions, in contrast to causal discovery which aims to determine the causal relationships between variables utilizing data. Recently, there has been current research regarding a method that mimics causal discovery by aggregating the outcomes of repetitive causal reasoning, achieved through specifically designed prompts. It highlights the usefulness of PLMs in discovering cause and effect, which is often limited by a lack of data, especially when dealing with multiple variables. Conversely, the characteristics of PLMs which are that PLMs do not analyze data and they are highly dependent on prompt design leads to a crucial limitation for directly using PLMs in causal discovery. Accordingly, PLM-based causal reasoning deeply depends on the prompt design and carries out the risk of overconfidence and false predictions in determining causal relationships. In this paper, we empirically demonstrate the aforementioned limitations of PLM-based causal reasoning through experiments on physics-inspired synthetic data. Then, we propose a new framework that integrates prior knowledge obtained from PLM with a causal discovery algorithm. This is accomplished by initializing an adjacency matrix for causal discovery and incorporating regularization using prior knowledge. Our proposed framework not only demonstrates improved performance through the integration of PLM and causal discovery but also suggests how to leverage PLM-extracted prior knowledge with existing causal discovery algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>语言模型（PLM）在 causal reasoning 中得到了扩展法律。 causal reasoning 通过文本描述来实现，而不是通过数据来确定 causal 关系。 recent research 表明，可以通过 repetitive causal reasoning 来模拟 causal discovery，并通过特定的提示来实现。 这种方法高亮了 PLM 在发现 cause 和 effect 方面的用途，特别是当 dealing 多个变量时。然而，PLM 的特点是它们不分析数据，依赖于提示设计，这导致了直接使用 PLM 在 causal discovery 中的限制。因此，PLM-based causal reasoning 的可靠性和准确性受到提示设计和 false 预测的影响。在本文中，我们通过对 physics-inspired 的 sintetic data 进行实验，证明了 PLM-based causal reasoning 中所存在的上述限制。然后，我们提出了一种新的框架，即将 PLM 提取的 prior knowledge 与 causal discovery 算法集成。通过在 causal discovery 中 initialize 一个相对matrix 和 incorporate 使用 prior knowledge 的 regularization，我们的提出的框架不仅在 integrating PLM 和 causal discovery 中表现出了改进的性能，还建议了如何将 PLM 提取的 prior knowledge 与现有的 causal discovery 算法集成。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Generative-AI-for-Clinical-Evidence-Summarization-Needs-to-Achieve-Trustworthiness"><a href="#Leveraging-Generative-AI-for-Clinical-Evidence-Summarization-Needs-to-Achieve-Trustworthiness" class="headerlink" title="Leveraging Generative AI for Clinical Evidence Summarization Needs to Achieve Trustworthiness"></a>Leveraging Generative AI for Clinical Evidence Summarization Needs to Achieve Trustworthiness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11211">http://arxiv.org/abs/2311.11211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gongbo Zhang, Qiao Jin, Denis Jered McInerney, Yong Chen, Fei Wang, Curtis L. Cole, Qian Yang, Yanshan Wang, Bradley A. Malin, Mor Peleg, Byron C. Wallace, Zhiyong Lu, Chunhua Weng, Yifan Peng</li>
<li>for: 提高医疗质量，使医疗决策和实践受到最佳证据支持。</li>
<li>methods: 利用大语言模型等生成AI技术，自动概要医学证据。</li>
<li>results: 提高医学证据概要化的可靠性和公平性。<details>
<summary>Abstract</summary>
Evidence-based medicine aims to improve the quality of healthcare by empowering medical decisions and practices with the best available evidence. The rapid growth of medical evidence, which can be obtained from various sources, poses a challenge in collecting, appraising, and synthesizing the evidential information. Recent advancements in generative AI, exemplified by large language models, hold promise in facilitating the arduous task. However, developing accountable, fair, and inclusive models remains a complicated undertaking. In this perspective, we discuss the trustworthiness of generative AI in the context of automated summarization of medical evidence.
</details>
<details>
<summary>摘要</summary>
证据基础医学目的是提高医疗质量，通过最佳可用证据来决策和实践医疗。医疗证据的快速增长，从多种来源获取，评估和synthesize证据信息具有挑战。现代生成AI技术，如大语言模型，表现出潜在的优势，能够帮助解决这项复杂任务。然而，构建可信、公正、包容的模型仍然是一项复杂的任务。本观点讨论了生成AI在医学证据自动概要中的可靠性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Noise-Scheduling-for-Generating-Plausible-Designs-with-Diffusion-Models"><a href="#On-the-Noise-Scheduling-for-Generating-Plausible-Designs-with-Diffusion-Models" class="headerlink" title="On the Noise Scheduling for Generating Plausible Designs with Diffusion Models"></a>On the Noise Scheduling for Generating Plausible Designs with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11207">http://arxiv.org/abs/2311.11207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajie Fan, Laure Vuaille, Thomas Bäck, Hao Wang</li>
<li>for: 该论文主要针对 Deep Generative Models (DGMs) 在多个行业中创新设计，包括时尚和汽车等。</li>
<li>methods: 该论文使用 diffusion models 来生成图像，并研究了噪声规划对结果的影响。</li>
<li>results: 该论文发现，适当的噪声规划可以提高 DGM 生成的结果的可能性，从83.4% 提高到93.5%，并且降低 FID 值从7.84 降至4.87。<details>
<summary>Abstract</summary>
Deep Generative Models (DGMs) are widely used to create innovative designs across multiple industries, ranging from fashion to the automotive sector. In addition to generating images of high visual quality, the task of structural design generation imposes more stringent constrains on the semantic expression, e.g., no floating material or missing part, which we refer to as plausibility in this work. We delve into the impact of noise schedules of diffusion models on the plausibility of the outcome: there exists a range of noise levels at which the model's performance decides the result plausibility. Also, we propose two techniques to determine such a range for a given image set and devise a novel parametric noise schedule for better plausibility. We apply this noise schedule to the training and sampling of the well-known diffusion model EDM and compare it to its default noise schedule. Compared to EDM, our schedule significantly improves the rate of plausible designs from 83.4% to 93.5% and Fr\'echet Inception Distance (FID) from 7.84 to 4.87. Further applications of advanced image editing tools demonstrate the model's solid understanding of structure.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unmasking-and-Improving-Data-Credibility-A-Study-with-Datasets-for-Training-Harmless-Language-Models"><a href="#Unmasking-and-Improving-Data-Credibility-A-Study-with-Datasets-for-Training-Harmless-Language-Models" class="headerlink" title="Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models"></a>Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11202">http://arxiv.org/abs/2311.11202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/docta-ai/docta">https://github.com/docta-ai/docta</a></li>
<li>paper_authors: Zhaowei Zhu, Jialu Wang, Hao Cheng, Yang Liu</li>
<li>for: The paper focuses on evaluating the credibility of real-world datasets for training harmless language models, specifically addressing label errors in popular benchmarks such as Jigsaw Civil Comments, Anthropic Harmless &amp; Red Team, PKU BeaverTails &amp; SafeRLHF.</li>
<li>methods: The authors introduce a systematic framework for evaluating dataset credibility, identifying label errors, and assessing the impact of noisy labels on language learning. They use a combination of manual annotation and automated methods to identify and fix label errors.</li>
<li>results: The authors find and fix an average of 6.16% label errors in 11 datasets constructed from the benchmarks, which improves the data credibility and downstream learning performance of the models. The results demonstrate the significance of cleaning existing real-world datasets to ensure the quality of language models.Here’s the Simplified Chinese version of the three key points:</li>
<li>for: 这个研究专注于评估现实世界数据集的可靠性，以便使用安全语言模型进行训练，特别是关注标签错误在常用的 benchmar 如 Jigsaw Civil Comments、Anthropic Harmless &amp; Red Team、PKU BeaverTails &amp; SafeRLHF 中的影响。</li>
<li>methods: 作者们提出了一种系统化的数据集可靠性评估框架，包括标签错误的检测和评估不良标签对语言学习的影响。他们使用组合的手动标注和自动方法来检测和修复标签错误。</li>
<li>results: 作者们在11个数据集中平均发现和修复了6.16%的标签错误，这有效提高了数据集的可靠性和下游学习性能。结果表明，清洁现有的现实世界数据集是训练语言模型的重要前提。<details>
<summary>Abstract</summary>
Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of 6.16% label errors in 11 datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets. Open-source: https://github.com/Docta-ai/docta.
</details>
<details>
<summary>摘要</summary>
Language models have shown promise in various tasks, but they can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of 6.16% label errors in 11 datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets. Open-source: <https://github.com/Docta-ai/docta>.
</details></li>
</ul>
<hr>
<h2 id="Assessing-AI-Impact-Assessments-A-Classroom-Study"><a href="#Assessing-AI-Impact-Assessments-A-Classroom-Study" class="headerlink" title="Assessing AI Impact Assessments: A Classroom Study"></a>Assessing AI Impact Assessments: A Classroom Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11193">http://arxiv.org/abs/2311.11193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nari Johnson, Hoda Heidari</li>
<li>for: 这个研究是为了评估现有的人工智能影响评估工具（AIIAs）的有用性和有效性。</li>
<li>methods: 这个研究使用了一个类room研究（N &#x3D; 38），其中学生分配到不同的组织角色（如机器学习科学家或产品经理），并询问参与者们使用现有的AI影响评估工具来评估两个想象中的生成AI系统的可能的影响。</li>
<li>results: 研究发现参与者们通过完成这些活动后回答问卷表示出了对生成AI系统的风险的更好的认识，以及AI专家在避免可能的危害方面承担的责任的水平。此外，研究还发现现有的AIIA工具存在一些共同的局限性，包括格式和内容的问题，以及活动的可行性和有效性。<details>
<summary>Abstract</summary>
Artificial Intelligence Impact Assessments ("AIIAs"), a family of tools that provide structured processes to imagine the possible impacts of a proposed AI system, have become an increasingly popular proposal to govern AI systems. Recent efforts from government or private-sector organizations have proposed many diverse instantiations of AIIAs, which take a variety of forms ranging from open-ended questionnaires to graded score-cards. However, to date that has been limited evaluation of existing AIIA instruments. We conduct a classroom study (N = 38) at a large research-intensive university (R1) in an elective course focused on the societal and ethical implications of AI. We assign students to different organizational roles (for example, an ML scientist or product manager) and ask participant teams to complete one of three existing AI impact assessments for one of two imagined generative AI systems. In our thematic analysis of participants' responses to pre- and post-activity questionnaires, we find preliminary evidence that impact assessments can influence participants' perceptions of the potential risks of generative AI systems, and the level of responsibility held by AI experts in addressing potential harm. We also discover a consistent set of limitations shared by several existing AIIA instruments, which we group into concerns about their format and content, as well as the feasibility and effectiveness of the activity in foreseeing and mitigating potential harms. Drawing on the findings of this study, we provide recommendations for future work on developing and validating AIIAs.
</details>
<details>
<summary>摘要</summary>
人工智能影响评估工具（AIIA），一家家工具，提供结构化的过程，用于想象提案的人工智能系统的可能的影响。近些年，政府或私营组织的努力都提出了许多多样化的AIIA实现方案，这些方案从开放的问卷到分数卡等多种形式。然而，到目前为止，对现有AIIA工具的评估尚有限。我们在一所大型研究型大学（R1）的选修课中，对38名学生进行了教室实验。我们将学生分配到不同的组织角色（例如，机器学习科学家或产品经理），并让参与者队伍完成一个现有的AI影响评估工具，用于想象的两种生成AI系统。在我们对参与者们完成前和后活动问卷的分析中，我们发现了AI影响评估工具可以影响参与者对生成AI系统的可能风险的看法，以及AI专家负担应对可能危害的责任水平。我们还发现了许多现有AIIA工具的共同问题，包括格式和内容的问题，以及活动的可行性和效果。根据这些发现，我们提出了未来AIIA工具的开发和验证方向。
</details></li>
</ul>
<hr>
<h2 id="Attention-Based-Real-Time-Defenses-for-Physical-Adversarial-Attacks-in-Vision-Applications"><a href="#Attention-Based-Real-Time-Defenses-for-Physical-Adversarial-Attacks-in-Vision-Applications" class="headerlink" title="Attention-Based Real-Time Defenses for Physical Adversarial Attacks in Vision Applications"></a>Attention-Based Real-Time Defenses for Physical Adversarial Attacks in Vision Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11191">http://arxiv.org/abs/2311.11191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo</li>
<li>for: 防止深度神经网络受到真实世界中的敌意攻击，以确保其应用于安全关键领域。</li>
<li>methods: 提出了一种高效的注意力基于防御机制，通过在多帧设置中快速识别和跟踪恶意物体，并在 shallow 网络层中遮盖恶意效应。</li>
<li>results: 提高了现有的过度活动技术的性能，并在多帧设置中实现了高效的防御性能，同时也降低了计算成本。<details>
<summary>Abstract</summary>
Deep neural networks exhibit excellent performance in computer vision tasks, but their vulnerability to real-world adversarial attacks, achieved through physical objects that can corrupt their predictions, raises serious security concerns for their application in safety-critical domains. Existing defense methods focus on single-frame analysis and are characterized by high computational costs that limit their applicability in multi-frame scenarios, where real-time decisions are crucial.   To address this problem, this paper proposes an efficient attention-based defense mechanism that exploits adversarial channel-attention to quickly identify and track malicious objects in shallow network layers and mask their adversarial effects in a multi-frame setting. This work advances the state of the art by enhancing existing over-activation techniques for real-world adversarial attacks to make them usable in real-time applications. It also introduces an efficient multi-frame defense framework, validating its efficacy through extensive experiments aimed at evaluating both defense performance and computational cost.
</details>
<details>
<summary>摘要</summary>
为解决这个问题，这篇论文提出了一种高效的注意力基于防御机制，通过敌意通道注意力快速识别和跟踪恶意对象在浅网层中，并在多帧场景中遮盖其敌意效果。这项工作提高了现有的真实世界敌意攻击技术的可用性，使其在实时应用中使用。它还介绍了一种高效的多帧防御框架，通过广泛的实验证明了其防御性和计算成本的平衡。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/cs.AI_2023_11_19/" data-id="clp88dbsd0080ob88evx837zm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/cs.CL_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T11:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/cs.CL_2023_11_19/">cs.CL - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Spot-the-Bot-Distinguishing-Human-Written-and-Bot-Generated-Texts-Using-Clustering-and-Information-Theory-Techniques"><a href="#Spot-the-Bot-Distinguishing-Human-Written-and-Bot-Generated-Texts-Using-Clustering-and-Information-Theory-Techniques" class="headerlink" title="Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using Clustering and Information Theory Techniques"></a>Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using Clustering and Information Theory Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11441">http://arxiv.org/abs/2311.11441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilii Gromov, Quynh Nhu Dang</li>
<li>for: 本研究旨在提出一种基于无监督学习技术的 Bot 识别算法，不需要大量标注数据和&#x2F;或 Bot 模型架构的先验知识。</li>
<li>methods: 本研究使用语义分析（卷积和杂散）和信息技术，构建了一种可靠的 Bot 识别模型，可以识别不同类型的 Bot 生成的文本。</li>
<li>results: 研究发现，生成文本往往更加杂乱，而文学作品往往更加复杂；同时，人类文本 clustering 结果比 Bot 生成文本的 clustering 结果更加杂乱。<details>
<summary>Abstract</summary>
With the development of generative models like GPT-3, it is increasingly more challenging to differentiate generated texts from human-written ones. There is a large number of studies that have demonstrated good results in bot identification. However, the majority of such works depend on supervised learning methods that require labelled data and/or prior knowledge about the bot-model architecture. In this work, we propose a bot identification algorithm that is based on unsupervised learning techniques and does not depend on a large amount of labelled data. By combining findings in semantic analysis by clustering (crisp and fuzzy) and information techniques, we construct a robust model that detects a generated text for different types of bot. We find that the generated texts tend to be more chaotic while literary works are more complex. We also demonstrate that the clustering of human texts results in fuzzier clusters in comparison to the more compact and well-separated clusters of bot-generated texts.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>>随着生成模型如GPT-3的发展，分辨生成文本和人类写的文本变得越来越困难。许多研究已经达到了对 bot 的识别的好结果，但大多数这些工作都依赖于supervised learning方法，需要标注数据和/或对 bot-model architecture 的先前知识。在这项工作中，我们提出了基于无监督学习技术的 bot 识别算法。通过结合 semantics 分析（crisp和fuzzy）和信息技术，我们构建了一个强健的模型，可以对不同类型的 bot 进行文本识别。我们发现生成文本往往更加混乱，而文学作品却更加复杂。此外，我们还示出了人类文本的 clustering 结果比 bot-generated 文本的 clustering 结果更加模糊。
</details></li>
</ul>
<hr>
<h2 id="ML-LMCL-Mutual-Learning-and-Large-Margin-Contrastive-Learning-for-Improving-ASR-Robustness-in-Spoken-Language-Understanding"><a href="#ML-LMCL-Mutual-Learning-and-Large-Margin-Contrastive-Learning-for-Improving-ASR-Robustness-in-Spoken-Language-Understanding" class="headerlink" title="ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding"></a>ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11375">http://arxiv.org/abs/2311.11375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuxin Cheng, Bowen Cao, Qichen Ye, Zhihong Zhu, Hongxiang Li, Yuexian Zou</li>
<li>for: 提高自动语音识别（ASR）错误率下的对话系统任务性能</li>
<li>methods: 提出了一种新的框架 called Mutual Learning and Large-Margin Contrastive Learning（ML-LMCL），通过在精度训练中进行互助学习，使两个对话系统模型在手动训练和ASR训练中分别学习，以便相互交换知识。还引入了距离偏好regularizer，以避免尽可能多地避免推迟内部对应的对话。</li>
<li>results: 实验结果表明，ML-LMCL在三个数据集上比现有模型表现更好，并实现了新的状态级表现。<details>
<summary>Abstract</summary>
Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback-Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
听话语理解（SLU）是对话系统中的基本任务。然而，自动语音识别（ASR）的不可避免的错误通常会影响理解性能，导致错误卷积。虽然有一些尝试通过对比学习解决这个问题，但它们（1）在细化学习中对清晰手动笔记和ASR笔记进行等同处理;（2）忽略了semantic相似对在应用对比学习时被推迟的问题;（3）受到Kullback-Leibler（KL）消失问题。在这篇论文中，我们提出了相互学习和大margin对比学习（ML-LMCL）框架，用于提高ASR对SLU的Robustness。具体来说，在细化学习中，我们将两个SLU模型在手动笔记和ASR笔记上进行相互学习，以便相互分享知识。我们还引入了距离浮动规则，以避免push away intra-cluster对话。此外，我们使用循环退化调度来mitigate KL消失问题。实验表明，ML-LMCL在三个数据集上的性能比既有模型更好，达到了新的状态率。
</details></li>
</ul>
<hr>
<h2 id="CHAMP-Efficient-Annotation-and-Consolidation-of-Cluster-Hierarchies"><a href="#CHAMP-Efficient-Annotation-and-Consolidation-of-Cluster-Hierarchies" class="headerlink" title="CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies"></a>CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11301">http://arxiv.org/abs/2311.11301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ariecattan/champ">https://github.com/ariecattan/champ</a></li>
<li>paper_authors: Arie Cattan, Tom Hope, Doug Downey, Roy Bar-Haim, Lilach Eden, Yoav Kantor, Ido Dagan</li>
<li>for: 该论文旨在提供一种用于快速构建文本中层次结构的开源工具，以便有效地进行文本级别的注释。</li>
<li>methods: 该工具使用增量式的方法，同时构建层次结构和集群，以提高注释效率和精度。</li>
<li>results: 该工具可以快速构建层次结构和集群，并且包含一个整合模式，可以方便地比较多个注释并解决不一致。<details>
<summary>Abstract</summary>
Various NLP tasks require a complex hierarchical structure over nodes, where each node is a cluster of items. Examples include generating entailment graphs, hierarchical cross-document coreference resolution, annotating event and subevent relations, etc. To enable efficient annotation of such hierarchical structures, we release CHAMP, an open source tool allowing to incrementally construct both clusters and hierarchy simultaneously over any type of texts. This incremental approach significantly reduces annotation time compared to the common pairwise annotation approach and also guarantees maintaining transitivity at the cluster and hierarchy levels. Furthermore, CHAMP includes a consolidation mode, where an adjudicator can easily compare multiple cluster hierarchy annotations and resolve disagreements.
</details>
<details>
<summary>摘要</summary>
各种自然语言处理任务需要复杂的层次结构，其中每个节点是一组项目。例如，生成涵义图、跨文档涵引Resolution、标注事件和子事件关系等。为了有效地注释这些层次结构，我们发布了 CHAMP 开源工具，它允许在不同类型文本上逐步构建层次结构和群集，并且同时保证层次结构和群集级别的转移性。此外， CHAMP 还包括一个整合模式，可以轻松地比较多个层次结构和群集注释，并解决不一致。
</details></li>
</ul>
<hr>
<h2 id="A-Cross-Attention-Augmented-Model-for-Event-Triggered-Context-Aware-Story-Generation"><a href="#A-Cross-Attention-Augmented-Model-for-Event-Triggered-Context-Aware-Story-Generation" class="headerlink" title="A Cross-Attention Augmented Model for Event-Triggered Context-Aware Story Generation"></a>A Cross-Attention Augmented Model for Event-Triggered Context-Aware Story Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11271">http://arxiv.org/abs/2311.11271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tonywenuon/dialog-coherence-metric">https://github.com/tonywenuon/dialog-coherence-metric</a></li>
<li>paper_authors: Chen Tang, Tyler Loakman, Chenghua Lin<br>for:这篇论文旨在提高生成的故事质量，特别是在 incorporating 上下文和事件特征方面。methods:本文提出了一种基于神经网络的生成模型，即 EtriCA，该模型使用了跨注意力机制来将上下文特征映射到事件序列中，从而更好地利用事件之间的逻辑关系。此外，本文还提出了一种基于大规模书籍资料的知识增强框架（KeEtriCA），以便让模型适应更广泛的数据样本。results:实验结果表明，相比于现有的基准模型，EtriCA 模型在自动化指标和人工评估方面均有约5%的提升，而 KeEtriCA 框架在大规模书籍资料上进行了知识增强，可以让模型适应更广泛的数据样本，从而实现约10%的人工评估提升。<details>
<summary>Abstract</summary>
Despite recent advancements, existing story generation systems continue to encounter difficulties in effectively incorporating contextual and event features, which greatly influence the quality of generated narratives. To tackle these challenges, we introduce a novel neural generation model, EtriCA, that enhances the relevance and coherence of generated stories by employing a cross-attention mechanism to map context features onto event sequences through residual mapping. This feature capturing mechanism enables our model to exploit logical relationships between events more effectively during the story generation process. To further enhance our proposed model, we employ a post-training framework for knowledge enhancement (KeEtriCA) on a large-scale book corpus. This allows EtriCA to adapt to a wider range of data samples. This results in approximately 5\% improvement in automatic metrics and over 10\% improvement in human evaluation. We conduct extensive experiments, including comparisons with state-of-the-art (SOTA) baseline models, to evaluate the performance of our framework on story generation. The experimental results, encompassing both automated metrics and human assessments, demonstrate the superiority of our model over existing state-of-the-art baselines. These results underscore the effectiveness of our model in leveraging context and event features to improve the quality of generated narratives.
</details>
<details>
<summary>摘要</summary>
尽管最近的进步，现有的故事生成系统仍然困难地兼容上下文特征和事件特征，这些特征对生成的故事质量产生重要的影响。为解决这些挑战，我们介绍了一种新的神经网络生成模型，EtriCA，该模型通过杂合注意机制将上下文特征映射到事件序列中，以 residual mapping 的方式进行增强。这种特征捕捉机制使我们的模型更好地利用事件之间的逻辑关系，从而提高生成的故事质量。为进一步提高我们的提议模型，我们采用了一种基于大规模书籍资料的后期培养框架（KeEtriCA），这使得 EtriCA 能够适应更广泛的数据样本。这 resulted in approximately 5% improvement in automatic metrics and over 10% improvement in human evaluation. We conduct extensive experiments, including comparisons with state-of-the-art (SOTA) baseline models, to evaluate the performance of our framework on story generation. The experimental results, encompassing both automated metrics and human assessments, demonstrate the superiority of our model over existing state-of-the-art baselines. These results underscore the effectiveness of our model in leveraging context and event features to improve the quality of generated narratives.
</details></li>
</ul>
<hr>
<h2 id="Towards-Real-World-Writing-Assistance-A-Chinese-Character-Checking-Benchmark-with-Faked-and-Misspelled-Characters"><a href="#Towards-Real-World-Writing-Assistance-A-Chinese-Character-Checking-Benchmark-with-Faked-and-Misspelled-Characters" class="headerlink" title="Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters"></a>Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11268">http://arxiv.org/abs/2311.11268</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THUKElab/Visual-C3">https://github.com/THUKElab/Visual-C3</a></li>
<li>paper_authors: Yinghui Li, Zishan Xu, Shaoshen Chen, Haojing Huang, Yangning Li, Yong Jiang, Zhongli Li, Qingyu Zhou, Hai-Tao Zheng, Ying Shen</li>
<li>for: 本研究的目的是提高中文输入文本的正确率和质量，尤其是在拼写错误的情况下。</li>
<li>methods: 本研究使用了人工标注的视觉中文字符检查数据集Visual-C$^3$,以及一些基线方法的评估。</li>
<li>results: 实验结果表明，Visual-C$^3$ 是一个高质量又具有挑战性的数据集，而基线方法在这个数据集上的性能也得到了证明。<details>
<summary>Abstract</summary>
Writing assistance is an application closely related to human life and is also a fundamental Natural Language Processing (NLP) research field. Its aim is to improve the correctness and quality of input texts, with character checking being crucial in detecting and correcting wrong characters. From the perspective of the real world where handwriting occupies the vast majority, characters that humans get wrong include faked characters (i.e., untrue characters created due to writing errors) and misspelled characters (i.e., true characters used incorrectly due to spelling errors). However, existing datasets and related studies only focus on misspelled characters mainly caused by phonological or visual confusion, thereby ignoring faked characters which are more common and difficult. To break through this dilemma, we present Visual-C$^3$, a human-annotated Visual Chinese Character Checking dataset with faked and misspelled Chinese characters. To the best of our knowledge, Visual-C$^3$ is the first real-world visual and the largest human-crafted dataset for the Chinese character checking scenario. Additionally, we also propose and evaluate novel baseline methods on Visual-C$^3$. Extensive empirical results and analyses show that Visual-C$^3$ is high-quality yet challenging. The Visual-C$^3$ dataset and the baseline methods will be publicly available to facilitate further research in the community.
</details>
<details>
<summary>摘要</summary>
文字辅助是人类生活中密切相关的应用，同时也是自然语言处理（NLP）研究领域的基础领域。它的目标是提高输入文本的正确性和质量，特别是在检查和 corrections 中的字符检查非常重要。从实际世界的手写角度来看，人类写错的字符包括伪造字符（即由写入错误而创造的不真实字符）和拼写错误的字符（即正确的字符 incorrect 使用）。然而，现有的数据集和相关研究仅对拼写错误的字符进行研究，忽略了伪造字符，这些伪造字符更为常见和困难。为突破这个困境，我们提出了 Visual-C$^3$，一个由人工标注的视觉中文字符检查数据集，包括伪造和拼写错误的中文字符。我们知道，Visual-C$^3$ 是实际世界中第一个真实的视觉数据集，也是最大的人工制作的中文字符检查数据集。此外，我们还提出了和评估了基eline 方法。广泛的实验结果和分析表明，Visual-C$^3$ 具有高质量 yet 挑战性。Visual-C$^3$ 数据集和基eline 方法将在未来的研究中提供可用的资源。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Large-Language-Models-in-Mental-Health-Applications"><a href="#Rethinking-Large-Language-Models-in-Mental-Health-Applications" class="headerlink" title="Rethinking Large Language Models in Mental Health Applications"></a>Rethinking Large Language Models in Mental Health Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11267">http://arxiv.org/abs/2311.11267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoxiong Ji, Tianlin Zhang, Kailai Yang, Sophia Ananiadou, Erik Cambria</li>
<li>for: 这篇论文探讨了使用自然语言处理技术在心理健康领域的应用。</li>
<li>methods: 论文提出了关于使用生成模型的稳定性和可靠性问题，以及生成模型可能生成幻见的问题。</li>
<li>results: 论文认为，人工智能专家的Empathy、细腻的解释和Contextual awareness在心理健康咨询中是不可取代的。使用生成模型应该是以人类专家为主的，而不是寻求取代人类专家的方式。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have become valuable assets in mental health, showing promise in both classification tasks and counseling applications. This paper offers a perspective on using LLMs in mental health applications. It discusses the instability of generative models for prediction and the potential for generating hallucinatory outputs, underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability. The paper also distinguishes between the often interchangeable terms ``explainability'' and ``interpretability'', advocating for developing inherently interpretable methods instead of relying on potentially hallucinated self-explanations generated by LLMs. Despite the advancements in LLMs, human counselors' empathetic understanding, nuanced interpretation, and contextual awareness remain irreplaceable in the sensitive and complex realm of mental health counseling. The use of LLMs should be approached with a judicious and considerate mindset, viewing them as tools that complement human expertise rather than seeking to replace it.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:大型语言模型（LLMs）在心理健康方面已成为有价值资产，在分类任务和辅助应用中都表现出了承诺。这篇文章对使用LLMs在心理健康领域进行了观点，并讨论了预测过程中的生成模型不稳定性和潜在的幻觉输出问题，强调需要持续进行审核和评估以保持其可靠性和可预测性。文章还区分了“可解释性”和“可解读性”这两个术语，强调开发内在可解释的方法而不是仰仗可能生成的自我解释。虽然LLMs在心理健康领域的应用已经取得了进步，但人类师长的Empathy、细化解释和Contextual awareness仍然在敏感和复杂的心理健康辅导中不可或缺。使用LLMs应该具有谨慎和考虑的心态，视其为人类专业知识的辅助工具而不是寻求替代。
</details></li>
</ul>
<hr>
<h2 id="Causal-ATE-Mitigates-Unintended-Bias-in-Controlled-Text-Generation"><a href="#Causal-ATE-Mitigates-Unintended-Bias-in-Controlled-Text-Generation" class="headerlink" title="Causal ATE Mitigates Unintended Bias in Controlled Text Generation"></a>Causal ATE Mitigates Unintended Bias in Controlled Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11229">http://arxiv.org/abs/2311.11229</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Madhavan, Kahini Wadhawan</li>
<li>for: 这个论文是关于语言模型中的属性控制问题，通过 causal average treatment effect（Causal ATE）方法进行研究的。</li>
<li>methods: exist 的方法是通过 sentence 中的词语与属性相关的方法来进行控制，但这些方法可能会受到训练数据中的偶散关系的影响，导致模型在推理过程中假设存在属性。这个论文提出了一种简单的摈剂基于的方法，可以消除这种不良影响。</li>
<li>results: 该论文提出的方法可以减少 false positive 的数量，从而解决了不良偏见的问题。具体来说，这种方法在减少抑制保护群体的不良偏见方面具有重要的作用。<details>
<summary>Abstract</summary>
We study attribute control in language models through the method of Causal Average Treatment Effect (Causal ATE). Existing methods for the attribute control task in Language Models (LMs) check for the co-occurrence of words in a sentence with the attribute of interest, and control for them. However, spurious correlation of the words with the attribute in the training dataset, can cause models to hallucinate the presence of the attribute when presented with the spurious correlate during inference. We show that the simple perturbation-based method of Causal ATE removes this unintended effect. Additionally, we offer a theoretical foundation for investigating Causal ATE in the classification task, and prove that it reduces the number of false positives -- thereby mitigating the issue of unintended bias. Specifically, we ground it in the problem of toxicity mitigation, where a significant challenge lies in the inadvertent bias that often emerges towards protected groups post detoxification. We show that this unintended bias can be solved by the use of the Causal ATE metric.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/cs.CL_2023_11_19/" data-id="clp88dbum00fkob88a3vhff63" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/cs.LG_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T10:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/cs.LG_2023_11_19/">cs.LG - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Physics-Enhanced-TinyML-for-Real-Time-Detection-of-Ground-Magnetic-Anomalies"><a href="#Physics-Enhanced-TinyML-for-Real-Time-Detection-of-Ground-Magnetic-Anomalies" class="headerlink" title="Physics-Enhanced TinyML for Real-Time Detection of Ground Magnetic Anomalies"></a>Physics-Enhanced TinyML for Real-Time Detection of Ground Magnetic Anomalies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11452">http://arxiv.org/abs/2311.11452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Talha Siddique, MD Shaad Mahmud</li>
<li>for: 本研究旨在开发一种基于物理学的小型机器学习（TinyML）框架，用于实时地磁摇动论中的磁场干扰预测。</li>
<li>methods: 该框架结合物理学基于的规范，在模型训练和压缩阶段进行了约束，以提高预测的可靠性。</li>
<li>results: 研究表明，基于物理学的TinyML框架可以提供更高的准确率和可靠性，相比传统的预测方法。<details>
<summary>Abstract</summary>
Space weather phenomena like geomagnetic disturbances (GMDs) and geomagnetically induced currents (GICs) pose significant risks to critical technological infrastructure. While traditional predictive models, grounded in simulation, hold theoretical robustness, they grapple with challenges, notably the assimilation of imprecise data and extensive computational complexities. In recent years, Tiny Machine Learning (TinyML) has been adopted to develop Machine Learning (ML)-enabled magnetometer systems for predicting real-time terrestrial magnetic perturbations as a proxy measure for GIC. While TinyML offers efficient, real-time data processing, its intrinsic limitations prevent the utilization of robust methods with high computational needs. This paper developed a physics-guided TinyML framework to address the above challenges. This framework integrates physics-based regularization at the stages of model training and compression, thereby augmenting the reliability of predictions. The developed pruning scheme within the framework harnesses the inherent physical characteristics of the domain, striking a balance between model size and robustness. The study presents empirical results, drawing a comprehensive comparison between the accuracy and reliability of the developed framework and its traditional counterpart. Such a comparative analysis underscores the prospective applicability of the developed framework in conceptualizing robust, ML-enabled magnetometer systems for real-time space weather forecasting.
</details>
<details>
<summary>摘要</summary>
Space weather phenomena like geomagnetic disturbances (GMDs) and geomagnetically induced currents (GICs) pose significant risks to critical technological infrastructure. While traditional predictive models, grounded in simulation, hold theoretical robustness, they grapple with challenges, notably the assimilation of imprecise data and extensive computational complexities. In recent years, Tiny Machine Learning (TinyML) has been adopted to develop Machine Learning (ML)-enabled magnetometer systems for predicting real-time terrestrial magnetic perturbations as a proxy measure for GIC. While TinyML offers efficient, real-time data processing, its intrinsic limitations prevent the utilization of robust methods with high computational needs. This paper developed a physics-guided TinyML framework to address the above challenges. This framework integrates physics-based regularization at the stages of model training and compression, thereby augmenting the reliability of predictions. The developed pruning scheme within the framework harnesses the inherent physical characteristics of the domain, striking a balance between model size and robustness. The study presents empirical results, drawing a comprehensive comparison between the accuracy and reliability of the developed framework and its traditional counterpart. Such a comparative analysis underscores the prospective applicability of the developed framework in conceptualizing robust, ML-enabled magnetometer systems for real-time space weather forecasting.Here is the translation of the text into Traditional Chinese:Space weather phenomena like geomagnetic disturbances (GMDs) and geomagnetically induced currents (GICs) pose significant risks to critical technological infrastructure. While traditional predictive models, grounded in simulation, hold theoretical robustness, they grapple with challenges, notably the assimilation of imprecise data and extensive computational complexities. In recent years, Tiny Machine Learning (TinyML) has been adopted to develop Machine Learning (ML)-enabled magnetometer systems for predicting real-time terrestrial magnetic perturbations as a proxy measure for GIC. While TinyML offers efficient, real-time data processing, its intrinsic limitations prevent the utilization of robust methods with high computational needs. This paper developed a physics-guided TinyML framework to address the above challenges. This framework integrates physics-based regularization at the stages of model training and compression, thereby augmenting the reliability of predictions. The developed pruning scheme within the framework harnesses the inherent physical characteristics of the domain, striking a balance between model size and robustness. The study presents empirical results, drawing a comprehensive comparison between the accuracy and reliability of the developed framework and its traditional counterpart. Such a comparative analysis underscores the prospective applicability of the developed framework in conceptualizing robust, ML-enabled magnetometer systems for real-time space weather forecasting.
</details></li>
</ul>
<hr>
<h2 id="Weight-Norm-Control"><a href="#Weight-Norm-Control" class="headerlink" title="Weight Norm Control"></a>Weight Norm Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11446">http://arxiv.org/abs/2311.11446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Ilya Loshchilov</li>
<li>for: 本研究探讨了静态weight decay regularization的特殊情况，即weight norm控制，以及其在优化器中的应用。</li>
<li>methods: 本研究使用了多种优化器，包括Adam和AdamW，以及它们的weight norm控制版本AdamWN。</li>
<li>results: 研究发现，设置weight norm为0可能是优化过程中的优化点，但是可以考虑其他target norm值来优化weight norm控制。此外，引入weight norm控制可能会改变优化过程的各种含义。<details>
<summary>Abstract</summary>
We note that decoupled weight decay regularization is a particular case of weight norm control where the target norm of weights is set to 0. Any optimization method (e.g., Adam) which uses decoupled weight decay regularization (respectively, AdamW) can be viewed as a particular case of a more general algorithm with weight norm control (respectively, AdamWN). We argue that setting the target norm of weights to 0 can be suboptimal and other target norm values can be considered. For instance, any training run where AdamW achieves a particular norm of weights can be challenged by AdamWN scheduled to achieve a comparable norm of weights. We discuss various implications of introducing weight norm control instead of weight decay.
</details>
<details>
<summary>摘要</summary>
我们注意到单独对顶点衰落调整是对顶点 Norm 控制的特例，其中顶点 Norm 设置为 0。任何优化方法（例如 Adam）使用单独对顶点衰落调整（即 AdamW）可以被视为更一般的算法中的 weight Norm 控制（即 AdamWN）。我们认为将顶点 Norm 设置为 0 可能是不佳的选择，而其他顶点 Norm 值可以被考虑。例如，任何训练运行 AdamW  achieves 特定顶点 Norm 的情况可以被 AdamWN 调整到相似的顶点 Norm 挑战。我们讨论了将 weight Norm 控制 instead of weight decay 的各种影响。
</details></li>
</ul>
<hr>
<h2 id="Duality-of-Bures-and-Shape-Distances-with-Implications-for-Comparing-Neural-Representations"><a href="#Duality-of-Bures-and-Shape-Distances-with-Implications-for-Comparing-Neural-Representations" class="headerlink" title="Duality of Bures and Shape Distances with Implications for Comparing Neural Representations"></a>Duality of Bures and Shape Distances with Implications for Comparing Neural Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11436">http://arxiv.org/abs/2311.11436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah E. Harvey, Brett W. Larsen, Alex H. Williams</li>
<li>for: 这篇论文旨在团结两类神经网络表示相似度测量方法的研究领域，以便更好地理解这些方法之间的关系。</li>
<li>methods: 这篇论文使用了一种新的方法，即通过观察cosine函数和尼采尔曼布尔相似性之间的关系，来将这两类方法联系起来。</li>
<li>results: 研究发现，cosine函数和尼采尔曼布尔相似性之间存在一定的关系，这种关系可以帮助我们更好地理解这些方法之间的区别和相似之处。<details>
<summary>Abstract</summary>
A multitude of (dis)similarity measures between neural network representations have been proposed, resulting in a fragmented research landscape. Most of these measures fall into one of two categories.   First, measures such as linear regression, canonical correlations analysis (CCA), and shape distances, all learn explicit mappings between neural units to quantify similarity while accounting for expected invariances. Second, measures such as representational similarity analysis (RSA), centered kernel alignment (CKA), and normalized Bures similarity (NBS) all quantify similarity in summary statistics, such as stimulus-by-stimulus kernel matrices, which are already invariant to expected symmetries. Here, we take steps towards unifying these two broad categories of methods by observing that the cosine of the Riemannian shape distance (from category 1) is equal to NBS (from category 2). We explore how this connection leads to new interpretations of shape distances and NBS, and draw contrasts of these measures with CKA, a popular similarity measure in the deep learning literature.
</details>
<details>
<summary>摘要</summary>
很多（不同）相似度量表有被提议，导致研究领域分化。大多数这些度量都属于两个类别。首先，度量如线性回归、 canonical correlation analysis（CCA）和形状距离，都是学习明确映射 между神经网络单元，以量化相似性，同时考虑预期的变换。其次，度量如表现相似性分析（RSA）、中心kernelAlignment（CKA）和归一化射影度（NBS）都是量化相似性的总统计量，如刺激对刺激的kernel矩阵，这些量已经 invariant于预期的对称性。在这里，我们尝试统一这两个类别的方法，观察cosine函数的里曼矩阵距离（从第一类）是等于NBS（从第二类）。我们探索这种连接如何导致新的含义和解释，并与CKA进行比较。
</details></li>
</ul>
<hr>
<h2 id="Fast-Heavy-Inner-Product-Identification-Between-Weights-and-Inputs-in-Neural-Network-Training"><a href="#Fast-Heavy-Inner-Product-Identification-Between-Weights-and-Inputs-in-Neural-Network-Training" class="headerlink" title="Fast Heavy Inner Product Identification Between Weights and Inputs in Neural Network Training"></a>Fast Heavy Inner Product Identification Between Weights and Inputs in Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11429">http://arxiv.org/abs/2311.11429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lianke Qin, Saayan Mitra, Zhao Song, Yuanyuan Yang, Tianyi Zhou</li>
<li>For:  solve the heavy inner product identification problem, which generalizes the Light Bulb problem, and speed up the training of neural networks with ReLU activation function.* Methods:  provide an algorithm that runs in $O(n^{2 \omega &#x2F; 3+ o(1)})$ time to find the $k$ inner product pairs that surpass $\rho \cdot d$ threshold with high probability.* Results:  the algorithm can speed up the training of neural networks with ReLU activation function.Here’s the full Chinese text:* 为: 解决内积极大问题，该问题泛化了光泽问题（参考\cite{prr89}），给定两个集合 $A \subset {-1,+1}^d$ 和 $B \subset {-1,+1}^d$， $|A|&#x3D;|B|&#x3D;n$，如果存在 exact $k$ 对的内积超过某个阈值 $\rho \cdot d$，则目标是确定这些 $k$ 对内积。* 方法: 提供一种算法，运行时间为 $O(n^{2 \omega &#x2F; 3+ o(1)})$，可以高可信度地找到超过 $\rho \cdot d$ 阈值的 $k$ 对内积。* 结果: 该算法可以加速使用 ReLU 激活函数训练神经网络的速度。<details>
<summary>Abstract</summary>
In this paper, we consider a heavy inner product identification problem, which generalizes the Light Bulb problem~(\cite{prr89}): Given two sets $A \subset \{-1,+1\}^d$ and $B \subset \{-1,+1\}^d$ with $|A|=|B| = n$, if there are exact $k$ pairs whose inner product passes a certain threshold, i.e., $\{(a_1, b_1), \cdots, (a_k, b_k)\} \subset A \times B$ such that $\forall i \in [k], \langle a_i,b_i \rangle \geq \rho \cdot d$, for a threshold $\rho \in (0,1)$, the goal is to identify those $k$ heavy inner products. We provide an algorithm that runs in $O(n^{2 \omega / 3+ o(1)})$ time to find the $k$ inner product pairs that surpass $\rho \cdot d$ threshold with high probability, where $\omega$ is the current matrix multiplication exponent. By solving this problem, our method speed up the training of neural networks with ReLU activation function.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了一个重要的内积识别问题，该问题泛化了报灯问题（参考 \cite{prr89}）：给定两个集合 $A \subset \{-1,+1\}^d$ 和 $B \subset \{-1,+1\}^d$， $|A| = |B| = n$，如果存在 exact $k$ 对 whose inner product exceeds a certain threshold, i.e., $\{(a_1, b_1), \cdots, (a_k, b_k)\} \subset A \times B$ such that $\forall i \in [k], \langle a_i, b_i \rangle \geq \rho \cdot d$, for a threshold $\rho \in (0,1)$, the goal is to identify those $k$ heavy inner products. We provide an algorithm that runs in $O(n^{2 \omega / 3+ o(1)})$ time to find the $k$ inner product pairs that surpass $\rho \cdot d$ threshold with high probability, where $\omega$ is the current matrix multiplication exponent. By solving this problem, our method can accelerate the training of neural networks with ReLU activation function.
</details></li>
</ul>
<hr>
<h2 id="Tensor-Aware-Energy-Accounting"><a href="#Tensor-Aware-Energy-Accounting" class="headerlink" title="Tensor-Aware Energy Accounting"></a>Tensor-Aware Energy Accounting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11424">http://arxiv.org/abs/2311.11424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/project-smaragdine/smaragdine">https://github.com/project-smaragdine/smaragdine</a></li>
<li>paper_authors: Timur Babakol, Yu David Liu</li>
<li>for: This paper aims to provide a novel energy accounting system for tensor-based deep learning (DL) programs, called Smaragdine, to improve the energy efficiency of DL applications.</li>
<li>methods: Smaragdine uses a novel white-box methodology of energy accounting that is aware of the internal structure of the DL program, allowing for a detailed breakdown of energy consumption by units aligned with the logical hierarchical decomposition structure.</li>
<li>results: Smaragdine was applied to understand the energy behavior of BERT, a widely used language model, and identified the highest energy&#x2F;power-consuming components of BERT layer by layer and tensor by tensor. Additionally, two case studies were conducted to demonstrate the effectiveness of Smaragdine in supporting downstream toolchain building, one comparing the energy impact of hyperparameter tuning of BERT, and the other analyzing the energy behavior evolution of BERT as it evolves to its next generation, ALBERT.<details>
<summary>Abstract</summary>
With the rapid growth of Artificial Intelligence (AI) applications supported by deep learning (DL), the energy efficiency of these applications has an increasingly large impact on sustainability. We introduce Smaragdine, a new energy accounting system for tensor-based DL programs implemented with TensorFlow. At the heart of Smaragdine is a novel white-box methodology of energy accounting: Smaragdine is aware of the internal structure of the DL program, which we call tensor-aware energy accounting. With Smaragdine, the energy consumption of a DL program can be broken down into units aligned with its logical hierarchical decomposition structure. We apply Smaragdine for understanding the energy behavior of BERT, one of the most widely used language models. Layer-by-layer and tensor-by-tensor, Smaragdine is capable of identifying the highest energy/power-consuming components of BERT. Furthermore, we conduct two case studies on how Smaragdine supports downstream toolchain building, one on the comparative energy impact of hyperparameter tuning of BERT, the other on the energy behavior evolution when BERT evolves to its next generation, ALBERT.
</details>
<details>
<summary>摘要</summary>
随着人工智能（AI）应用的快速发展，深度学习（DL）支持的这些应用的能效性越来越重要。我们介绍了一种新的能源会计系统，名为Smaragdine，用于tensor-based DL程序，实现于TensorFlow。Smaragdine的核心是一种新的白盒方法：它知道DL程序的内部结构，我们称之为tensor-aware energy accounting。通过Smaragdine，DL程序的能 consumption可以被分解成与其逻辑层次结构相对应的单元。我们使用Smaragdine来理解BERT语言模型中最广泛使用的语言模型中的能 behaviors。层次和tensor-by-tensor，Smaragdine可以识别BERT中最高能/功率消耗的组件。此外，我们还进行了两个case study，一是对BERT的超参数调整的能效影响，另一是BERT的下游工具链建立支持的能行为演化。
</details></li>
</ul>
<hr>
<h2 id="Offline-Reinforcement-Learning-for-Wireless-Network-Optimization-with-Mixture-Datasets"><a href="#Offline-Reinforcement-Learning-for-Wireless-Network-Optimization-with-Mixture-Datasets" class="headerlink" title="Offline Reinforcement Learning for Wireless Network Optimization with Mixture Datasets"></a>Offline Reinforcement Learning for Wireless Network Optimization with Mixture Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11423">http://arxiv.org/abs/2311.11423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Yang, Cong Shen, Jing Yang, Shu-ping Yeh, Jerry Sydir</li>
<li>for: 本研究旨在 investigate 在 wireless radio resource management (RRM) 问题上使用 offline reinforcement learning (RL) 算法。</li>
<li>methods: 本研究使用了 several state-of-the-art offline RL algorithms, 包括 behavior constrained Q-learning (BCQ), conservative Q-learning (CQL), 和 implicit Q-learning (IQL)，以解决一个 maximizing Linear combination of sum and 5-percentile rates via user scheduling 的 RRM problem。</li>
<li>results: 研究发现，RL 算法的性能对 RRM 问题的行为策略使用方式有critical 关系，并提出了一种 novel offline RL 解决方案，利用不同行为策略收集的 heterogeneous 数据集。研究表明，通过合理混合数据集，offline RL 可以生成一个 near-optimal RL 策略，即使所有 involved 行为策略都是高度不优化的。<details>
<summary>Abstract</summary>
The recent development of reinforcement learning (RL) has boosted the adoption of online RL for wireless radio resource management (RRM). However, online RL algorithms require direct interactions with the environment, which may be undesirable given the potential performance loss due to the unavoidable exploration in RL. In this work, we first investigate the use of \emph{offline} RL algorithms in solving the RRM problem. We evaluate several state-of-the-art offline RL algorithms, including behavior constrained Q-learning (BCQ), conservative Q-learning (CQL), and implicit Q-learning (IQL), for a specific RRM problem that aims at maximizing a linear combination {of sum and} 5-percentile rates via user scheduling. We observe that the performance of offline RL for the RRM problem depends critically on the behavior policy used for data collection, and further propose a novel offline RL solution that leverages heterogeneous datasets collected by different behavior policies. We show that with a proper mixture of the datasets, offline RL can produce a near-optimal RL policy even when all involved behavior policies are highly suboptimal.
</details>
<details>
<summary>摘要</summary>
现在的增强学习（RL）技术的发展，使得在线RL在无线电缺源管理（RRM）中得到了广泛的应用。然而，在线RL算法需要直接与环境交互，这可能是不可避免的探索成本的原因，从而导致性能下降。在这个工作中，我们首先调查使用停机RL算法解决RRM问题。我们评估了一些当今最佳的停机RL算法，包括行为约束Q学习（BCQ）、保守Q学习（CQL）和隐式Q学习（IQL），以解决一个具有最大化线性组合{的总和和} 5%分位率的用户调度问题。我们发现，RRM问题中停机RL的性能取决于用于数据收集的行为策略，并提出了一种新的停机RL解决方案，利用不同行为策略收集的多样化数据集。我们展示了，对于不同行为策略收集的数据集进行杂合，停机RL可以生成一个几乎优质RL策略，即使所有参与行为策略都是高度不优质的。
</details></li>
</ul>
<hr>
<h2 id="Precision-at-the-indistinguishability-threshold-a-method-for-evaluating-classification-algorithms"><a href="#Precision-at-the-indistinguishability-threshold-a-method-for-evaluating-classification-algorithms" class="headerlink" title="Precision at the indistinguishability threshold: a method for evaluating classification algorithms"></a>Precision at the indistinguishability threshold: a method for evaluating classification algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11422">http://arxiv.org/abs/2311.11422</a></li>
<li>repo_url: None</li>
<li>paper_authors: David J. T. Sumpter</li>
<li>for: 本文旨在提出一种新的评估分类算法性能的指标，以解决现有指标中的缺点。</li>
<li>methods: 本文使用两步方法来建立新的指标。首先，设置一个阈值值，使得当算法被示出两个随机选择的图像（一个高于阈值的图像和另一个真正包含猫的图像）时，图像选择的概率是50%。然后，测量算法的性能，问题是随机选择的图像中是否真正包含猫。这个指标可以被称为“锐度在难以区分边界时的准确率”。</li>
<li>results: 本文表明，这个新的指标可以减少一些现有指标（如AUC和F1分数）中的缺点，并且更好地反映算法的性能。<details>
<summary>Abstract</summary>
There exist a wide range of single number metrics for assessing performance of classification algorithms, including AUC and the F1-score (Wikipedia lists 17 such metrics, with 27 different names). In this article, I propose a new metric to answer the following question: when an algorithm is tuned so that it can no longer distinguish labelled cats from real cats, how often does a randomly chosen image that has been labelled as containing a cat actually contain a cat? The steps to construct this metric are as follows. First, we set a threshold score such that when the algorithm is shown two randomly-chosen images -- one that has a score greater than the threshold (i.e. a picture labelled as containing a cat) and another from those pictures that really does contain a cat -- the probability that the image with the highest score is the one chosen from the set of real cat images is 50\%. At this decision threshold, the set of positively labelled images are indistinguishable from the set of images which are positive. Then, as a second step, we measure performance by asking how often a randomly chosen picture from those labelled as containing a cat actually contains a cat. This metric can be thought of as {\it precision at the indistinguishability threshold}. While this new metric doesn't address the tradeoff between precision and recall inherent to all such metrics, I do show why this method avoids pitfalls that can occur when using, for example AUC, and it is better motivated than, for example, the F1-score.
</details>
<details>
<summary>摘要</summary>
exist 一些各种单一数据指标用于评估分类算法的性能，包括AUC和F1-score（Wikipedia列出了17种指标，共27个不同的名称）。在这篇文章中，我提出了一个新的指标，用于回答以下问题：当算法通过调整可以不再区分标注的猫和真正的猫时，总是如何确定一个随机选择的图像是否实际上包含猫？以下是构建这个指标的步骤：1. 设置一个阈值分数，使得当算法被示出两个随机选择的图像——一个分数高于阈值（即标注为猫的图像）和另一个从真正的猫图像中随机选择的图像——图像分数最高的图像是50%的概率来自真正的猫图像集。在这个决策阈值下，标注为猫的图像集和真正的猫图像集变得无法区分。2. 然后，我们测量性能的方式是，从标注为猫的图像集中随机选择一个图像，并问：这个图像是否实际上包含猫？这个指标可以被称为“阈值上的准确率”。尽管这个新的指标不同于AUC和F1-score中的偏好和损失评估，但我展示了这种方法可以避免AUC和F1-score中的坑，并且更加有意义。
</details></li>
</ul>
<hr>
<h2 id="Large-Pre-trained-time-series-models-for-cross-domain-Time-series-analysis-tasks"><a href="#Large-Pre-trained-time-series-models-for-cross-domain-Time-series-analysis-tasks" class="headerlink" title="Large Pre-trained time series models for cross-domain Time series analysis tasks"></a>Large Pre-trained time series models for cross-domain Time series analysis tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11413">http://arxiv.org/abs/2311.11413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harshavardhan Kamarthi, B. Aditya Prakash</li>
<li>for: 这篇论文旨在探讨如何将多种不同领域的时间序列数据集合组合成一个通用的时间序列模型，以提高模型的效能和训练效率。</li>
<li>methods: 这篇论文提出了一个新的自适应学习方法，即使用自我监督学习损失来自动决定最佳的dataset特定的分割策略，以提高时间序列模型的性能。</li>
<li>results: 实验结果显示，这个新的模型（LPTM）可以与专业领域的模型相比，并且在训练时间和数据量方面具有优化的效率。 Specifically, LPTM achieves state-of-the-art performance in a wide range of time-series analysis tasks from multiple disparate domains, while taking up to 40% less data and 50% less training time than existing methods.<details>
<summary>Abstract</summary>
Large pre-trained models have been instrumental in significant advancements in domains like language and vision making model training for individual downstream tasks more efficient as well as provide superior performance. However, tackling time-series analysis tasks usually involves designing and training a separate model from scratch leveraging training data and domain expertise specific to the task. We tackle a significant challenge for pre-training a general time-series model from multiple heterogeneous time-series dataset: providing semantically useful inputs to models for modeling time series of different dynamics from different domains. We observe that partitioning time-series into segments as inputs to sequential models produces semantically better inputs and propose a novel model LPTM that automatically identifies optimal dataset-specific segmentation strategy leveraging self-supervised learning loss during pre-training. LPTM provides performance similar to or better than domain-specific state-of-art model and is significantly more data and compute efficient taking up to 40% less data as well as 50% less training time to achieve state-of-art performance in a wide range of time-series analysis tasks from multiple disparate domain.
</details>
<details>
<summary>摘要</summary>
大型预训模型在语言和视觉领域得到了重要的进步，使得模型训练 для下游任务更加效率，并提供了更高的性能。然而，对时间序列分析任务通常需要采用特定任务和领域专业知识来设计和训练独立的模型。我们解决了将多个不同领域时间序列数据集中的时间序列分解为Semantically meaningful inputs for models的挑战。我们发现，将时间序列分割成段为模型输入会产生更好的输入，并提出了一种新的模型LPTM，它可以自动确定数据集特定的分解策略，并在预训练过程中使用无监督学习损失来优化。LPTM在多个不同领域的时间序列分析任务中提供了与领域专业模型相当或更好的性能，并且需要训练时间和数据量相对减少了40%，以及50%。
</details></li>
</ul>
<hr>
<h2 id="Negotiated-Representations-for-Machine-Mearning-Application"><a href="#Negotiated-Representations-for-Machine-Mearning-Application" class="headerlink" title="Negotiated Representations for Machine Mearning Application"></a>Negotiated Representations for Machine Mearning Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11410">http://arxiv.org/abs/2311.11410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nurikorhan/negotiated-representations">https://github.com/nurikorhan/negotiated-representations</a></li>
<li>paper_authors: Nuri Korhan, Samet Bayram</li>
<li>for: 提高机器学习模型的分类精度和避免过拟合</li>
<li>methods: 通过让模型与已知类别标签进行协商来增强模型的解释能力和泛化能力</li>
<li>results: 通过在多个低级机器学习问题上实验，提出了一种可以增强机器学习模型的解释能力和泛化能力的方法，并且在实验中表明了该方法的可行性和可扩展性。<details>
<summary>Abstract</summary>
Overfitting is a phenomenon that occurs when a machine learning model is trained for too long and focused too much on the exact fitness of the training samples to the provided training labels and cannot keep track of the predictive rules that would be useful on the test data. This phenomenon is commonly attributed to memorization of particular samples, memorization of the noise, and forced fitness into a data set of limited samples by using a high number of neurons. While it is true that the model encodes various peculiarities as the training process continues, we argue that most of the overfitting occurs in the process of reconciling sharply defined membership ratios. In this study, we present an approach that increases the classification accuracy of machine learning models by allowing the model to negotiate output representations of the samples with previously determined class labels. By setting up a negotiation between the models interpretation of the inputs and the provided labels, we not only increased average classification accuracy but also decreased the rate of overfitting without applying any other regularization tricks. By implementing our negotiation paradigm approach to several low regime machine learning problems by generating overfitting scenarios from publicly available data sets such as CIFAR 10, CIFAR 100, and MNIST we have demonstrated that the proposed paradigm has more capacity than its intended purpose. We are sharing the experimental results and inviting the machine learning community to explore the limits of the proposed paradigm. We also aim to incentive the community to exploit the negotiation paradigm to overcome the learning related challenges in other research fields such as continual learning. The Python code of the experimental setup is uploaded to GitHub.
</details>
<details>
<summary>摘要</summary>
Overfitting 是一种现象，当机器学习模型被训练了太长时间，对准确地适应训练样本和提供的训练标签进行过于精准的适应，导致模型无法保持预测规则。这种现象通常被归因于特定样本的记忆、噪声记忆和用于限定数据集的高数 neurons。虽然模型在训练过程中记忆了各种特征，但我们 argue 大多数过度适应发生在确定样本的分类比率的过程中。在这种研究中，我们提出了一种方法，可以提高机器学习模型的分类精度，allowing the model to negotiate output representations of the samples with previously determined class labels。通过在模型对输入的解释和提供的标签之间设置谈判，我们不仅提高了平均分类精度，还降低了过度适应的发生，无需应用其他正则化技巧。通过在低度机器学习问题上实现我们的谈判模式，我们在 CIFAR 10、CIFAR 100 和 MNIST 等公共数据集上生成了过度适应场景，并示出了我们的方法具有更大的容量。我们将实验结果分享，邀请机器学习社区探索我们的方法的限制，并努力启发社区利用谈判模式来超越学习相关的挑战。我们还 aspire to encourage the community to explore the limits of the proposed paradigm in other research fields such as continual learning. Python 代码的实验设置已经上传到 GitHub。
</details></li>
</ul>
<hr>
<h2 id="Towards-interpretable-by-design-deep-learning-algorithms"><a href="#Towards-interpretable-by-design-deep-learning-algorithms" class="headerlink" title="Towards interpretable-by-design deep learning algorithms"></a>Towards interpretable-by-design deep learning algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11396">http://arxiv.org/abs/2311.11396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Plamen Angelov, Dmitry Kangin, Ziyang Zhang</li>
<li>for: 解决深度学习模型的解释性和快速学习问题</li>
<li>methods: 使用Interpretable-by-design DEep learning ALgorithms (IDEAL)把标准超级vised分类问题转化为与训练数据集中的prototype进行相似性计算，并利用现有的大 neural networks嵌入空间（Foundation Models，FM）来解决解释性问题</li>
<li>results: 提出了一种可解释的深度学习模型，并实现了类增量学习、简单的批量学习和快速的转移学习，而无需Finite-tuning的特征空间在目标数据集上。<details>
<summary>Abstract</summary>
The proposed framework named IDEAL (Interpretable-by-design DEep learning ALgorithms) recasts the standard supervised classification problem into a function of similarity to a set of prototypes derived from the training data, while taking advantage of existing latent spaces of large neural networks forming so-called Foundation Models (FM). This addresses the issue of explainability (stage B) while retaining the benefits from the tremendous achievements offered by DL models (e.g., visual transformers, ViT) pre-trained on huge data sets such as IG-3.6B + ImageNet-1K or LVD-142M (stage A). We show that one can turn such DL models into conceptually simpler, explainable-through-prototypes ones.   The key findings can be summarized as follows: (1) the proposed models are interpretable through prototypes, mitigating the issue of confounded interpretations, (2) the proposed IDEAL framework circumvents the issue of catastrophic forgetting allowing efficient class-incremental learning, and (3) the proposed IDEAL approach demonstrates that ViT architectures narrow the gap between finetuned and non-finetuned models allowing for transfer learning in a fraction of time \textbf{without} finetuning of the feature space on a target dataset with iterative supervised methods.
</details>
<details>
<summary>摘要</summary>
提议的框架被称为IDEAL（可解释的设计深度学习算法），将标准的supervised分类问题转化为与训练数据集中的prototype进行相似性计算的函数，同时利用现有的大神经网络的固有的latent空间形成 socalled Foundation Models (FM)。这解决了解释性（stage B）的问题，而不失去深度学习模型（如视觉转换器、ViT）在大量数据集上的成果（如IG-3.6B + ImageNet-1K或LVD-142M stage A）。我们表明可以将这些深度学习模型转化为概念更简单、可解释的prototype-based模型。关键发现包括：1. 提议的模型可以通过prototype进行解释，解决了叙述混乱的问题，2. IDEAL框架绕过了快速忘记问题，可以有效地进行分类增量学习，3. IDEAL方法示出了使用ViT体系进行传输学习，可以在几乎没有时间的情况下将finetuning的模型与非finetuning模型之间的差异减少，使得传输学习变得可行。
</details></li>
</ul>
<hr>
<h2 id="Addressing-the-speed-accuracy-simulation-trade-off-for-adaptive-spiking-neurons"><a href="#Addressing-the-speed-accuracy-simulation-trade-off-for-adaptive-spiking-neurons" class="headerlink" title="Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons"></a>Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11390">http://arxiv.org/abs/2311.11390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/webstorms/blocks">https://github.com/webstorms/blocks</a></li>
<li>paper_authors: Luke Taylor, Andrew J King, Nicol S Harper</li>
<li>for: 帮助解决计算神经科学中的速度精度贸易，提高ALIF模型的并行化效率。</li>
<li>methods: 对ALIF模型进行算法重新解释，降低约束 simulate 复杂性，并使其更容易并行化在GPU上。</li>
<li>results: 在 sintetic 测试套件上实现了更 чем 50倍的培训速度增加，并在不同的supervised 分类任务中达到了相似的性能，却在训练时间上减少了一半。 最后，我们展示了如何使用我们的模型快速和准确地适应真实的电physiological 记录，其中非常细微的sub-毫秒DT是关键 для捕捉精确的脉冲时间。<details>
<summary>Abstract</summary>
The adaptive leaky integrate-and-fire (ALIF) model is fundamental within computational neuroscience and has been instrumental in studying our brains $\textit{in silico}$. Due to the sequential nature of simulating these neural models, a commonly faced issue is the speed-accuracy trade-off: either accurately simulate a neuron using a small discretisation time-step (DT), which is slow, or more quickly simulate a neuron using a larger DT and incur a loss in simulation accuracy. Here we provide a solution to this dilemma, by algorithmically reinterpreting the ALIF model, reducing the sequential simulation complexity and permitting a more efficient parallelisation on GPUs. We computationally validate our implementation to obtain over a $50\times$ training speedup using small DTs on synthetic benchmarks. We also obtained a comparable performance to the standard ALIF implementation on different supervised classification tasks - yet in a fraction of the training time. Lastly, we showcase how our model makes it possible to quickly and accurately fit real electrophysiological recordings of cortical neurons, where very fine sub-millisecond DTs are crucial for capturing exact spike timing.
</details>
<details>
<summary>摘要</summary>
《适应式漏斗启动（ALIF）模型是计算神经科学中的基础模型，对于研究大脑的 simulate 有着重要的作用。由于这种神经网络模型的顺序 simulate 的性质，frequently faced 一个速度-准确性贸易：可以使用小时间步长（DT）来准确地模拟神经元，但是这会比较慢；或者使用大时间步长（DT）来快速地模拟神经元，但是会产生模拟不准确。在这篇文章中，我们提供了一种解决这个困扰的方法，通过对 ALIF 模型的算法重新解释，从而降低了Sequential simulation 的复杂性，并且允许更有效的并行化在 GPU 上。我们通过计算验证了我们的实现，在使用小DT 进行训练时，可以获得超过 50 倍的增速。此外，我们还证明了我们的实现在不同的超visisted classification task 上具有相同的性能，但是在训练时间上占了一小 fraction。最后，我们示例了我们的模型可以快速并准确地适应真实的电physiological 记录，其中非常细的毫秒级DT 是关键 для捕捉精确的抽吸时间。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Reinforcement-Learning-with-Mixture-of-Orthogonal-Experts"><a href="#Multi-Task-Reinforcement-Learning-with-Mixture-of-Orthogonal-Experts" class="headerlink" title="Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts"></a>Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11385">http://arxiv.org/abs/2311.11385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Hendawy, Jan Peters, Carlo D’Eramo</li>
<li>for: 解决多任务束缚学习（MTRL）中agent的普适性问题，即使在不同任务之间共享表示。</li>
<li>methods: 我们提出了一种新的表示学习方法，名为 Mixture Of Orthogonal Experts（MOORE），它利用 Gram-Schmidt  процесс在多任务情况下共享表示空间中找到相似性。当给定任务特定信息时，MOORE可以从共享空间中生成相关的表示。</li>
<li>results: 我们在 MiniGrid 和 MetaWorld 两个 MTRL 测试环境中评估了我们的方法，结果显示 MOORE 超过了相关的基线并在 MetaWorld 上达到了新的州OF-THE-ART 记录。<details>
<summary>Abstract</summary>
Multi-Task Reinforcement Learning (MTRL) tackles the long-standing problem of endowing agents with skills that generalize across a variety of problems. To this end, sharing representations plays a fundamental role in capturing both unique and common characteristics of the tasks. Tasks may exhibit similarities in terms of skills, objects, or physical properties while leveraging their representations eases the achievement of a universal policy. Nevertheless, the pursuit of learning a shared set of diverse representations is still an open challenge. In this paper, we introduce a novel approach for representation learning in MTRL that encapsulates common structures among the tasks using orthogonal representations to promote diversity. Our method, named Mixture Of Orthogonal Experts (MOORE), leverages a Gram-Schmidt process to shape a shared subspace of representations generated by a mixture of experts. When task-specific information is provided, MOORE generates relevant representations from this shared subspace. We assess the effectiveness of our approach on two MTRL benchmarks, namely MiniGrid and MetaWorld, showing that MOORE surpasses related baselines and establishes a new state-of-the-art result on MetaWorld.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:多任务强化学习（MTRL）解决了长期存在的问题，即让代理人具备通用于多种问题的技能。为此，共享表示是关键，它可以捕捉任务的独特和共同特征。任务可能在技能、物品或物理性方面具有相似性，而共享表示可以使得实现通用策略变得更加容易。然而，学习共享多个多样化表示的挑战仍未得到解决。在这篇论文中，我们介绍了一种新的MTRL表示学习方法，名为“ mixture of orthogonal experts”（MOORE）。MOORE使用 Gram-Schmidt 过程将多个专家生成的共享表示空间中的共同结构拟合到一起，并在任务特定信息提供时生成相关的表示。我们在 MiniGrid 和 MetaWorld 两个 MTRL 标准测试集上评估了我们的方法，并证明 MOORE 超过相关的基elines并在 MetaWorld 上设置了新的state-of-the-art 记录。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Locally-Private-Nonparametric-Classification-with-Public-Data"><a href="#Optimal-Locally-Private-Nonparametric-Classification-with-Public-Data" class="headerlink" title="Optimal Locally Private Nonparametric Classification with Public Data"></a>Optimal Locally Private Nonparametric Classification with Public Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11369">http://arxiv.org/abs/2311.11369</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/karlmyh/lpct">https://github.com/karlmyh/lpct</a></li>
<li>paper_authors: Yuheng Ma, Hanfang Yang</li>
<li>for: 本研究 investigate 非互动式LDP（本地隐私）学习问题，专注于非 Parametric 分类。</li>
<li>methods: 我们首次 derive 隐私限制下的最佳减少速率，并提出一个名为内部私人分类树的新方法，可以实现最佳减少速率。另外，我们设计了一个数据驱动的剪除程序，以避免参数调整和生成快速减少的估计器。</li>
<li>results: 我们在实验中使用 синтети数据和实际数据，证明了我们的提案方法比private数据更有优势，并且可以实现实际中的应用。<details>
<summary>Abstract</summary>
In this work, we investigate the problem of public data-assisted non-interactive LDP (Local Differential Privacy) learning with a focus on non-parametric classification. Under the posterior drift assumption, we for the first time derive the mini-max optimal convergence rate with LDP constraint. Then, we present a novel approach, the locally private classification tree, which attains the mini-max optimal convergence rate. Furthermore, we design a data-driven pruning procedure that avoids parameter tuning and produces a fast converging estimator. Comprehensive experiments conducted on synthetic and real datasets show the superior performance of our proposed method. Both our theoretical and experimental findings demonstrate the effectiveness of public data compared to private data, which leads to practical suggestions for prioritizing non-private data collection.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了公共数据协助非互动式LDP（本地差分隐私学习）问题，特点是非 Parametric 分类。根据 posterior 漂移假设，我们首次 derivated 最佳 convergence 速率准则，并提出了一种新的方法——本地隐私分类树。此外，我们还设计了一种数据驱动的剪裁过程，以避免参数调整并生成快速收敛的估计器。我们在 synthetic 和实际数据上进行了广泛的实验，结果表明我们的提议方法在效果和速度上都有优势。这些理论和实验成果表明公共数据的优越性，从而提出了实际化非私人数据收集的建议。Note that "Synthetic" and "Real" in the text are translated as "合成" and "实际" respectively, which is a common way to refer to synthetic and real data in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Pretraining-for-Heterogeneous-Hypergraph-Neural-Networks"><a href="#Self-Supervised-Pretraining-for-Heterogeneous-Hypergraph-Neural-Networks" class="headerlink" title="Self-Supervised Pretraining for Heterogeneous Hypergraph Neural Networks"></a>Self-Supervised Pretraining for Heterogeneous Hypergraph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11368">http://arxiv.org/abs/2311.11368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdalgader Abubaker, Takanori Maehara, Madhav Nimishakavi, Vassilis Plachouras</li>
<li>for: This paper is written to present a novel self-supervised pretraining framework for heterogeneous HyperGNNs, which can effectively capture higher-order relations among entities in the data in a self-supervised manner.</li>
<li>methods: The proposed SPHH framework consists of two self-supervised pretraining tasks that aim to simultaneously learn both local and global representations of the entities in the hypergraph by using informative representations derived from the hypergraph structure.</li>
<li>results: The experiments on two real-world benchmarks using four different HyperGNN models show that the proposed SPHH framework consistently outperforms state-of-the-art baselines in various downstream tasks, demonstrating the effectiveness of the proposed framework in improving the performance of various HyperGNN models in various downstream tasks.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文是为了介绍一种新的自动超vised预训练框架 для多种多样性的HyperGNN，该框架可以有效地在自动超vised模式下捕捉图数据中实体之间的高阶关系。</li>
<li>methods: 提出的 SPHH 框架包括两个自动超vised 预训练任务，这些任务的目的是同时学习实体在图数据中的本地和全局表示，使用图数据的结构 derive 出的有用表示。</li>
<li>results: 对于两个实际 benchmark 使用四种不同的 HyperGNN 模型，我们的实验结果表明，提出的 SPHH 框架可以在多种下游任务中表现出色，如节点分类和链接预测等，并且可以在不同的模型和复杂性下表现良好，这表明了我们的框架的稳定性和 universality。<details>
<summary>Abstract</summary>
Recently, pretraining methods for the Graph Neural Networks (GNNs) have been successful at learning effective representations from unlabeled graph data. However, most of these methods rely on pairwise relations in the graph and do not capture the underling higher-order relations between entities. Hypergraphs are versatile and expressive structures that can effectively model higher-order relationships among entities in the data. Despite the efforts to adapt GNNs to hypergraphs (HyperGNN), there are currently no fully self-supervised pretraining methods for HyperGNN on heterogeneous hypergraphs. In this paper, we present SPHH, a novel self-supervised pretraining framework for heterogeneous HyperGNNs. Our method is able to effectively capture higher-order relations among entities in the data in a self-supervised manner. SPHH is consist of two self-supervised pretraining tasks that aim to simultaneously learn both local and global representations of the entities in the hypergraph by using informative representations derived from the hypergraph structure. Overall, our work presents a significant advancement in the field of self-supervised pretraining of HyperGNNs, and has the potential to improve the performance of various graph-based downstream tasks such as node classification and link prediction tasks which are mapped to hypergraph configuration. Our experiments on two real-world benchmarks using four different HyperGNN models show that our proposed SPHH framework consistently outperforms state-of-the-art baselines in various downstream tasks. The results demonstrate that SPHH is able to improve the performance of various HyperGNN models in various downstream tasks, regardless of their architecture or complexity, which highlights the robustness of our framework.
</details>
<details>
<summary>摘要</summary>
近期，图神经网络（GNN）的预训练方法在无标签图数据上学习有效表示得到了成功。然而，大多数这些方法仅依靠图中的对比关系，而不capture图中Entity之间的高阶关系。幂体图是一种灵活和表示力强的结构，可以有效地模型图中Entity之间的高阶关系。尽管有尝试将GNN适应幂体图（HyperGNN），但目前还没有完全自动适应的自我超vised预训练方法 для HyperGNN on 不同类型的幂体图。在这篇论文中，我们提出了一种新的自我超vised预训练框架 для幂体图GNN（SPHH）。我们的方法可以在自我超vised manner中有效地捕捉图中Entity之间的高阶关系。SPHH包括两种自我超vised预训练任务，旨在同时学习图中Entity的本地和全局表示。我们使用幂体图结构提供的有用表示来实现这两种任务。总的来说，我们的工作对于自我超vised预训练HyperGNN的Field提供了重要的进步，并且可以提高多种基于图的下游任务的性能，如节点分类和链接预测任务，它们可以与幂体图配置相对应。我们在两个实际 benchmark上使用四种不同的 HyperGNN 模型进行实验，得到的结果表明，我们提出的 SPHH 框架在多种下游任务中 consistently 超过了当前的基eline。这些结果表明，SPHH 可以在不同的 HyperGNN 模型和配置下提高多种下游任务的性能，这种可靠性 highlights 了我们的框架的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Symmetry-invariant-quantum-machine-learning-force-fields"><a href="#Symmetry-invariant-quantum-machine-learning-force-fields" class="headerlink" title="Symmetry-invariant quantum machine learning force fields"></a>Symmetry-invariant quantum machine learning force fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11362">http://arxiv.org/abs/2311.11362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isabel Nha Minh Le, Oriel Kiss, Julian Schuhmacher, Ivano Tavernelli, Francesco Tacchino</li>
<li>for: 这个论文的目的是为了开发一种基于量子机器学习的分子力场模型，以便在原子尺度上进行精度充足的模拟。</li>
<li>methods: 这篇论文使用了量子机器学习模型来预测分子的潜能能面和原子力，并利用了变量量子学习模型来从初始数据中学习。</li>
<li>results: 研究发现，在使用了物理相对论中的 symmetries 的情况下，量子神经网络模型表现更好，并且在较复杂的分子中也能够保持高度的准确性。此外，研究者还通过对水二分子的示例来展示了这种方法在多 компонент系统中的应用可能性。<details>
<summary>Abstract</summary>
Machine learning techniques are essential tools to compute efficient, yet accurate, force fields for atomistic simulations. This approach has recently been extended to incorporate quantum computational methods, making use of variational quantum learning models to predict potential energy surfaces and atomic forces from ab initio training data. However, the trainability and scalability of such models are still limited, due to both theoretical and practical barriers. Inspired by recent developments in geometric classical and quantum machine learning, here we design quantum neural networks that explicitly incorporate, as a data-inspired prior, an extensive set of physically relevant symmetries. We find that our invariant quantum learning models outperform their more generic counterparts on individual molecules of growing complexity. Furthermore, we study a water dimer as a minimal example of a system with multiple components, showcasing the versatility of our proposed approach and opening the way towards larger simulations. Our results suggest that molecular force fields generation can significantly profit from leveraging the framework of geometric quantum machine learning, and that chemical systems represent, in fact, an interesting and rich playground for the development and application of advanced quantum machine learning tools.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Coverage-Validity-Aware-Algorithmic-Recourse"><a href="#Coverage-Validity-Aware-Algorithmic-Recourse" class="headerlink" title="Coverage-Validity-Aware Algorithmic Recourse"></a>Coverage-Validity-Aware Algorithmic Recourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11349">http://arxiv.org/abs/2311.11349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngoc Bui, Duy Nguyen, Man-Chung Yue, Viet Anh Nguyen</li>
<li>for: 本研究旨在提高机器学习模型的解释性、透明度和伦理性，通过创建一种基于模型的无关的回溯机制。</li>
<li>methods: 本研究使用了一种基于 Linear Surrogate 的扩展方法，以生成一个可靠性和有效性的回溯机制。</li>
<li>results: 研究表明，该方法可以在不同的模型下提供有效的回溯机制，并且可以避免由模型更新所带来的问题。此外，该方法还可以捕捉到模型的泛化性和解释性。<details>
<summary>Abstract</summary>
Algorithmic recourse emerges as a prominent technique to promote the explainability, transparency and hence ethics of machine learning models. Existing algorithmic recourse approaches often assume an invariant predictive model; however, the predictive model is usually updated upon the arrival of new data. Thus, a recourse that is valid respective to the present model may become invalid for the future model. To resolve this issue, we propose a novel framework to generate a model-agnostic recourse that exhibits robustness to model shifts. Our framework first builds a coverage-validity-aware linear surrogate of the nonlinear (black-box) model; then, the recourse is generated with respect to the linear surrogate. We establish a theoretical connection between our coverage-validity-aware linear surrogate and the minimax probability machines (MPM). We then prove that by prescribing different covariance robustness, the proposed framework recovers popular regularizations for MPM, including the $\ell_2$-regularization and class-reweighting. Furthermore, we show that our surrogate pushes the approximate hyperplane intuitively, facilitating not only robust but also interpretable recourses. The numerical results demonstrate the usefulness and robustness of our framework.
</details>
<details>
<summary>摘要</summary>
算法征求 emerges as a prominent technique to promote the explainability, transparency, and hence ethics of machine learning models. Existing algorithmic recourse approaches often assume an invariant predictive model; however, the predictive model is usually updated upon the arrival of new data. Thus, a recourse that is valid respective to the present model may become invalid for the future model. To resolve this issue, we propose a novel framework to generate a model-agnostic recourse that exhibits robustness to model shifts. Our framework first builds a coverage-validity-aware linear surrogate of the nonlinear (black-box) model; then, the recourse is generated with respect to the linear surrogate. We establish a theoretical connection between our coverage-validity-aware linear surrogate and the minimax probability machines (MPM). We then prove that by prescribing different covariance robustness, the proposed framework recovers popular regularizations for MPM, including the $\ell_2$-regularization and class-reweighting. Furthermore, we show that our surrogate pushes the approximate hyperplane intuitively, facilitating not only robust but also interpretable recourses. The numerical results demonstrate the usefulness and robustness of our framework.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="A-Generative-Model-for-Accelerated-Inverse-Modelling-Using-a-Novel-Embedding-for-Continuous-Variables"><a href="#A-Generative-Model-for-Accelerated-Inverse-Modelling-Using-a-Novel-Embedding-for-Continuous-Variables" class="headerlink" title="A Generative Model for Accelerated Inverse Modelling Using a Novel Embedding for Continuous Variables"></a>A Generative Model for Accelerated Inverse Modelling Using a Novel Embedding for Continuous Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11343">http://arxiv.org/abs/2311.11343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sébastien Bompas abd Stefan Sandfeld</li>
<li>for:  accelerated materials design</li>
<li>methods: 使用生成机器学习模型，以避免广泛的实验和搜索多种可能的微结构</li>
<li>results: 提出一种基于二进制浮点数表示的嵌入策略，可以conditioning网络以提供精细的控制权限，并且可以应用于任何数字Here’s the breakdown of each point:</li>
<li>for: The paper is written for accelerated materials design, specifically addressing the challenge of finding suitable microstructures for desired properties using rapid prototyping.</li>
<li>methods: The paper uses generative machine learning models to address the challenge, but notes that existing methods have shortcomings. The authors propose a novel embedding strategy based on the binary representation of floating point numbers to overcome these shortcomings.</li>
<li>results: The proposed embedding strategy allows for fine control over generated microstructure images, providing a versatile embedding space for conditioning the generative model. This technique can be applied to any number, enabling accelerated materials design.<details>
<summary>Abstract</summary>
In materials science, the challenge of rapid prototyping materials with desired properties often involves extensive experimentation to find suitable microstructures. Additionally, finding microstructures for given properties is typically an ill-posed problem where multiple solutions may exist. Using generative machine learning models can be a viable solution which also reduces the computational cost. This comes with new challenges because, e.g., a continuous property variable as conditioning input to the model is required. We investigate the shortcomings of an existing method and compare this to a novel embedding strategy for generative models that is based on the binary representation of floating point numbers. This eliminates the need for normalization, preserves information, and creates a versatile embedding space for conditioning the generative model. This technique can be applied to condition a network on any number, to provide fine control over generated microstructure images, thereby contributing to accelerated materials design.
</details>
<details>
<summary>摘要</summary>
在材料科学中，快速原型材料的性能要求经常带来广泛的实验寻找适合的微结构。此外，为某些属性找到适合的微结构是一个不充分定义的问题，可能存在多种解。使用生成机器学习模型可以是一种可行的解决方案，同时降低计算成本。然而，这也会带来新的挑战，例如需要作为conditioning输入的continueProperty变量的整数化。我们 investigate了现有的方法的缺陷，并与基于二进制浮点数表示的新嵌入策略进行比较。这种策略可以用于condition网络任意数字，提供精细的控制权限，以便加速材料设计。
</details></li>
</ul>
<hr>
<h2 id="On-the-Communication-Complexity-of-Decentralized-Bilevel-Optimization"><a href="#On-the-Communication-Complexity-of-Decentralized-Bilevel-Optimization" class="headerlink" title="On the Communication Complexity of Decentralized Bilevel Optimization"></a>On the Communication Complexity of Decentralized Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11342">http://arxiv.org/abs/2311.11342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Zhang, My T. Thai, Jie Wu, Hongchang Gao</li>
<li>for: 提高 decentralized bilevel optimization 的应用性能，特别是在各种机器学习任务中。</li>
<li>methods: 提出了一种基于 Stochastic Bilevel Gradient Descent 算法的 Decentralized Stochastic Bilevel Gradient Descent 算法，在不同机器学习任务中实现了更好的通信复杂度和更少的通信轮数。</li>
<li>results: 实验结果表明，该算法可以在各种机器学习任务中达到更好的性能，并且在不同的环境下都能够保持稳定性。<details>
<summary>Abstract</summary>
Decentralized bilevel optimization has been actively studied in the past few years since it has widespread applications in machine learning. However, existing algorithms suffer from large communication complexity caused by the estimation of stochastic hypergradient, limiting their application to real-world tasks. To address this issue, we develop a novel decentralized stochastic bilevel gradient descent algorithm under the heterogeneous setting, which enjoys a small communication cost in each round and small communication rounds. As such, it can achieve a much better communication complexity than existing algorithms. Moreover, we extend our algorithm to the more challenging decentralized multi-level optimization. To the best of our knowledge, this is the first time achieving these theoretical results under the heterogeneous setting. At last, the experimental results confirm the efficacy of our algorithm.
</details>
<details>
<summary>摘要</summary>
随机二级优化在过去几年内得到了广泛研究，因为它在机器学习中有广泛的应用。然而，现有的算法受到了随机梯度估计所导致的大量通信复杂度的限制，使得它们在实际任务中应用有限。为解决这个问题，我们开发了一种新的分布式随机二级梯度下降算法，该算法在不同设备上具有小型通信成本和小型通信循环数。因此，它可以在实际任务中实现更好的通信复杂度。此外，我们还扩展了我们的算法到更加复杂的分布式多级优化问题。根据我们所知，这是在不同设备上第一次实现这些理论结果。最后，实验结果证明了我们的算法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Self-Distilled-Representation-Learning-for-Time-Series"><a href="#Self-Distilled-Representation-Learning-for-Time-Series" class="headerlink" title="Self-Distilled Representation Learning for Time Series"></a>Self-Distilled Representation Learning for Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11335">http://arxiv.org/abs/2311.11335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Pieper, Konstantin Ditschuneit, Martin Genzel, Alexandra Lindt, Johannes Otterbach</li>
<li>for: 这篇论文旨在探讨时间序列资料上的自主学习，并提出了一种非对照式的方法，基于data2vec自体散布框架。</li>
<li>methods: 这篇论文使用了一种学生教师模式，预测对掩盖时间序列的隐藏表示。这种方法避免了对比例特有的强迫特征和偏见。</li>
<li>results: 论文显示了这种方法在分类和预测任务中的竞争力，与现有自主学习方法相比，在UCRC和UEA archive以及ETT和电力dataset上。<details>
<summary>Abstract</summary>
Self-supervised learning for time-series data holds potential similar to that recently unleashed in Natural Language Processing and Computer Vision. While most existing works in this area focus on contrastive learning, we propose a conceptually simple yet powerful non-contrastive approach, based on the data2vec self-distillation framework. The core of our method is a student-teacher scheme that predicts the latent representation of an input time series from masked views of the same time series. This strategy avoids strong modality-specific assumptions and biases typically introduced by the design of contrastive sample pairs. We demonstrate the competitiveness of our approach for classification and forecasting as downstream tasks, comparing with state-of-the-art self-supervised learning methods on the UCR and UEA archives as well as the ETT and Electricity datasets.
</details>
<details>
<summary>摘要</summary>
自我监督学习 для时间序列数据具有与自然语言处理和计算机视觉领域最近爆发的潜力。大多数现有的方法在这个领域都是基于对照学习，我们提出了一种概念简单又强大的非对照学习方法，基于数据2vec自适应框架。我们的方法的核心是一种学生教师模式， predicts the latent representation of an input time series from masked views of the same time series。这种策略避免了强制特定modalitiespecific的假设和偏见，通常由对照样本对的设计引入。我们在下游任务 classification和预测中示出了我们的方法的竞争力，与现有的自我监督学习方法进行比较，在UCRC和UEA文件库以及ETT和电力集成体上。
</details></li>
</ul>
<hr>
<h2 id="LABCAT-Locally-adaptive-Bayesian-optimization-using-principal-component-aligned-trust-regions"><a href="#LABCAT-Locally-adaptive-Bayesian-optimization-using-principal-component-aligned-trust-regions" class="headerlink" title="LABCAT: Locally adaptive Bayesian optimization using principal component-aligned trust regions"></a>LABCAT: Locally adaptive Bayesian optimization using principal component-aligned trust regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11328">http://arxiv.org/abs/2311.11328</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aemiliusretiarius/labcat">https://github.com/aemiliusretiarius/labcat</a></li>
<li>paper_authors: E. Visser, C. E. van Daalen, J. C. Schoeman</li>
<li>for: 优化昂贵黑盒函数（black-box function）的开销成本。</li>
<li>methods: 基于信任区域的投机算法（trust-region-based BO），加入本地策略（local strategy），包括主成分对齐旋转和自适应尺度调整策略，以及自动决定相关性的 Gaussian 过程模型。</li>
<li>results: 通过对一组synthetic测试函数和COCO软件进行广泛的数值实验，显示了LABCAT算法在比较多种状态空间中的优异性，超过了一些现有的BO和黑盒优化算法。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) is a popular method for optimizing expensive black-box functions. BO has several well-documented shortcomings, including computational slowdown with longer optimization runs, poor suitability for non-stationary or ill-conditioned objective functions, and poor convergence characteristics. Several algorithms have been proposed that incorporate local strategies, such as trust regions, into BO to mitigate these limitations; however, none address all of them satisfactorily. To address these shortcomings, we propose the LABCAT algorithm, which extends trust-region-based BO by adding principal-component-aligned rotation and an adaptive rescaling strategy based on the length-scales of a local Gaussian process surrogate model with automatic relevance determination. Through extensive numerical experiments using a set of synthetic test functions and the well-known COCO benchmarking software, we show that the LABCAT algorithm outperforms several state-of-the-art BO and other black-box optimization algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Learning-Rates-Improve-Generalization-But-How-Large-Are-We-Talking-About"><a href="#Large-Learning-Rates-Improve-Generalization-But-How-Large-Are-We-Talking-About" class="headerlink" title="Large Learning Rates Improve Generalization: But How Large Are We Talking About?"></a>Large Learning Rates Improve Generalization: But How Large Are We Talking About?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11303">http://arxiv.org/abs/2311.11303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekaterina Lobacheva, Eduard Pockonechnyy, Maxim Kodryan, Dmitry Vetrov</li>
<li>for:  investigate the optimal initial learning rate range for training neural networks to achieve best generalization</li>
<li>methods:  experiments in a simplified setup with precise control of the learning rate hyperparameter, and validation in a more practical setting</li>
<li>results:  find that the optimal initial learning rate range is significantly narrower than previously assumed, and provide guidelines for selecting the best learning rate range for subsequent training with a small LR or weight averaging.<details>
<summary>Abstract</summary>
Inspired by recent research that recommends starting neural networks training with large learning rates (LRs) to achieve the best generalization, we explore this hypothesis in detail. Our study clarifies the initial LR ranges that provide optimal results for subsequent training with a small LR or weight averaging. We find that these ranges are in fact significantly narrower than generally assumed. We conduct our main experiments in a simplified setup that allows precise control of the learning rate hyperparameter and validate our key findings in a more practical setting.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="From-Categories-to-Classifier-Name-Only-Continual-Learning-by-Exploring-the-Web"><a href="#From-Categories-to-Classifier-Name-Only-Continual-Learning-by-Exploring-the-Web" class="headerlink" title="From Categories to Classifier: Name-Only Continual Learning by Exploring the Web"></a>From Categories to Classifier: Name-Only Continual Learning by Exploring the Web</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11293">http://arxiv.org/abs/2311.11293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ameya Prabhu, Hasan Abed Al Kader Hammoud, Ser-Nam Lim, Bernard Ghanem, Philip H. S. Torr, Adel Bibi</li>
<li>for:  This paper explores the use of uncurated webly-supervised data for continual learning in the absence of manually annotated datasets.</li>
<li>methods: The proposed solution leverages the internet to query and download web data for image classification, and harnesses the web to create support sets that surpass state-of-the-art name-only classification.</li>
<li>results: The method achieves up to 25% boost in accuracy compared to models trained on manually annotated datasets, and presents EvoTrends, a class-incremental dataset created from the web to capture real-world trends in just minutes.<details>
<summary>Abstract</summary>
Continual Learning (CL) often relies on the availability of extensive annotated datasets, an assumption that is unrealistically time-consuming and costly in practice. We explore a novel paradigm termed name-only continual learning where time and cost constraints prohibit manual annotation. In this scenario, learners adapt to new category shifts using only category names without the luxury of annotated training data. Our proposed solution leverages the expansive and ever-evolving internet to query and download uncurated webly-supervised data for image classification. We investigate the reliability of our web data and find them comparable, and in some cases superior, to manually annotated datasets. Additionally, we show that by harnessing the web, we can create support sets that surpass state-of-the-art name-only classification that create support sets using generative models or image retrieval from LAION-5B, achieving up to 25% boost in accuracy. When applied across varied continual learning contexts, our method consistently exhibits a small performance gap in comparison to models trained on manually annotated datasets. We present EvoTrends, a class-incremental dataset made from the web to capture real-world trends, created in just minutes. Overall, this paper underscores the potential of using uncurated webly-supervised data to mitigate the challenges associated with manual data labeling in continual learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TimeSQL-Improving-Multivariate-Time-Series-Forecasting-with-Multi-Scale-Patching-and-Smooth-Quadratic-Loss"><a href="#TimeSQL-Improving-Multivariate-Time-Series-Forecasting-with-Multi-Scale-Patching-and-Smooth-Quadratic-Loss" class="headerlink" title="TimeSQL: Improving Multivariate Time Series Forecasting with Multi-Scale Patching and Smooth Quadratic Loss"></a>TimeSQL: Improving Multivariate Time Series Forecasting with Multi-Scale Patching and Smooth Quadratic Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11285">http://arxiv.org/abs/2311.11285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Site Mo, Haoxin Wang, Bixiong Li, Songhai Fan, Yuankai Wu, Xianggen Liu</li>
<li>for: 这篇论文旨在提出一个简单有效的时间序列预测框架，以便在实际世界中处理多重时间序列数据。</li>
<li>methods: 这个框架使用多尺度裁剪和平滑二项损失（SQL）来解决时间序列预测中的问题，包括处理噪音和异常值。</li>
<li>results: 根据论文的 тео리学分析和实验结果，这个框架在八个实际世界benchmark数据集上实现了新的顶峰性表现。<details>
<summary>Abstract</summary>
Time series is a special type of sequence data, a sequence of real-valued random variables collected at even intervals of time. The real-world multivariate time series comes with noises and contains complicated local and global temporal dynamics, making it difficult to forecast the future time series given the historical observations. This work proposes a simple and effective framework, coined as TimeSQL, which leverages multi-scale patching and smooth quadratic loss (SQL) to tackle the above challenges. The multi-scale patching transforms the time series into two-dimensional patches with different length scales, facilitating the perception of both locality and long-term correlations in time series. SQL is derived from the rational quadratic kernel and can dynamically adjust the gradients to avoid overfitting to the noises and outliers. Theoretical analysis demonstrates that, under mild conditions, the effect of the noises on the model with SQL is always smaller than that with MSE. Based on the two modules, TimeSQL achieves new state-of-the-art performance on the eight real-world benchmark datasets. Further ablation studies indicate that the key modules in TimeSQL could also enhance the results of other models for multivariate time series forecasting, standing as plug-and-play techniques.
</details>
<details>
<summary>摘要</summary>
时序序列是特殊类型的序列数据，一个序列实数随时间刻取得的实数随机变量序列。实际世界多变量时序序列受到噪声和复杂的地方和全局时间动态影响，这使得预测未来时序序列给历史观察数据非常困难。这项工作提出了一个简单有效的框架，命名为TimeSQL，该框架利用多尺度补做和平滑二次损失（SQL）来解决以上挑战。多尺度补做将时序序列转换为不同长度级别的二维补做，从而使得时序序列中的本地和长期相关性更加容易发现。SQL是基于理智二次kernels的，可以在运动过程中动态调整Gradient，以避免因噪声和异常值而过拟合。理论分析表明，在某些条件下，TimeSQL模型中噪声对模型的影响总是小于MSE模型。基于这两个模块，TimeSQL在八个实际世界标准数据集上实现了新的状态计算机科学中的最佳性能。进一步的减少研究表明，TimeSQL中的关键模块也可以提高其他多变量时序序列预测模型的结果，作为插件技术。
</details></li>
</ul>
<hr>
<h2 id="Multi-Timescale-Control-and-Communications-with-Deep-Reinforcement-Learning-–-Part-I-Communication-Aware-Vehicle-Control"><a href="#Multi-Timescale-Control-and-Communications-with-Deep-Reinforcement-Learning-–-Part-I-Communication-Aware-Vehicle-Control" class="headerlink" title="Multi-Timescale Control and Communications with Deep Reinforcement Learning – Part I: Communication-Aware Vehicle Control"></a>Multi-Timescale Control and Communications with Deep Reinforcement Learning – Part I: Communication-Aware Vehicle Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11281">http://arxiv.org/abs/2311.11281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Liu, Lei Lei, Kan Zheng, Xuemin, Shen</li>
<li>For: This paper proposes a joint optimization framework of multi-timescale control and communications (MTCC) based on Deep Reinforcement Learning (DRL) for autonomous driving (AD) use case, specifically for platoon control (PC).* Methods: The proposed MTCC-PC algorithm is based on DRL and decomposes the problem into a communication-aware DRL-based PC sub-problem and a control-aware DRL-based RRA sub-problem. The PC sub-problem is solved assuming an RRA policy is given, and the MTCC-PC algorithm is trained in a delayed environment generated by fine-grained embedded simulation of C-V2X communications.* Results: The MTCC-PC algorithm is compared with baseline DRL algorithms, and the results show that it outperforms the baseline algorithms in terms of PC performance under random observation delay.<details>
<summary>Abstract</summary>
An intelligent decision-making system enabled by Vehicle-to-Everything (V2X) communications is essential to achieve safe and efficient autonomous driving (AD), where two types of decisions have to be made at different timescales, i.e., vehicle control and radio resource allocation (RRA) decisions. The interplay between RRA and vehicle control necessitates their collaborative design. In this two-part paper (Part I and Part II), taking platoon control (PC) as an example use case, we propose a joint optimization framework of multi-timescale control and communications (MTCC) based on Deep Reinforcement Learning (DRL). In this paper (Part I), we first decompose the problem into a communication-aware DRL-based PC sub-problem and a control-aware DRL-based RRA sub-problem. Then, we focus on the PC sub-problem assuming an RRA policy is given, and propose the MTCC-PC algorithm to learn an efficient PC policy. To improve the PC performance under random observation delay, the PC state space is augmented with the observation delay and PC action history. Moreover, the reward function with respect to the augmented state is defined to construct an augmented state Markov Decision Process (MDP). It is proved that the optimal policy for the augmented state MDP is optimal for the original PC problem with observation delay. Different from most existing works on communication-aware control, the MTCC-PC algorithm is trained in a delayed environment generated by the fine-grained embedded simulation of C-V2X communications rather than by a simple stochastic delay model. Finally, experiments are performed to compare the performance of MTCC-PC with those of the baseline DRL algorithms.
</details>
<details>
<summary>摘要</summary>
“一个智能做出决策系统，基于车辆到所有东西（V2X）通信，是自动驾驶（AD）所需的，以达到安全和高效的驾驶。在不同时间尺度上，需要做出两种决策：车辆控制和无线资源分配（RRA）决策。它们之间的互动需要协调设计。在这两部分文章（Part I和Part II）中，以排序控制（PC）为例用 caso，我们提出了一个多个时间尺度的控制和通信（MTCC）基于深度学习（DRL）的联合优化框架。在这篇文章（Part I）中，我们首先将问题分解为一个具有通信意识的 DRL 基于 PC 子问题，以及一个具有控制意识的 DRL 基于 RRA 子问题。然后，我们专注在 PC 子问题上，假设 RRA 政策已经给出，并提出了 MTCC-PC 算法来学习一个高效的 PC 策略。为了提高 PC 性能在随机观察延迟下，PC 状态空间被扩展了，并且 PC 动作历史和观察延迟也被加入。此外，我们定义了对于扩展state的 reward 函数，以建立一个扩展state Markov Decision Process (MDP)。证明了对于扩展state MDP 的最佳策略是对于原始 PC 问题的最佳策略。与大多数现有的通信意识控制相比，MTCC-PC 算法在精细的 C-V2X 通信 fine-grained 模拟中进行训练，而不是使用一个简单的测量延迟模型。最后，我们对 MTCC-PC 与基eline DRL 算法进行比较，以评估其表现。”
</details></li>
</ul>
<hr>
<h2 id="Multi-Timescale-Control-and-Communications-with-Deep-Reinforcement-Learning-–-Part-II-Control-Aware-Radio-Resource-Allocation"><a href="#Multi-Timescale-Control-and-Communications-with-Deep-Reinforcement-Learning-–-Part-II-Control-Aware-Radio-Resource-Allocation" class="headerlink" title="Multi-Timescale Control and Communications with Deep Reinforcement Learning – Part II: Control-Aware Radio Resource Allocation"></a>Multi-Timescale Control and Communications with Deep Reinforcement Learning – Part II: Control-Aware Radio Resource Allocation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11280">http://arxiv.org/abs/2311.11280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Lei, Tong Liu, Kan Zheng, Xuemin, Shen</li>
<li>for: 这篇论文的目的是为了解决 Cellular Vehicle-to-Everything (C-V2X) 系统中的多时间步骤控制和通信 (MTCC) 问题。</li>
<li>methods: 这篇论文使用 Deep Reinforcement Learning (DRL) 技术来解决 MTCC 问题，并将其分为两个子问题：通信适应 Deep Reinforcement Learning (DRL) 基本的均衡控制 (PC) 和控制适应 DRL 基本的射频资源分配 (RRA)。</li>
<li>results: 这篇论文提出了一个名为 MTCC-PC 的算法，用于学习 PC 政策，并且将 RRA 问题视为一个给定 PC 政策的问题，提出了 MTCC-RRA 算法。此外，这篇论文还使用了 reward shaping 和 reward backpropagation prioritized experience replay (RBPER) 技术来有效地解决多代理和罕见的奖励问题。<details>
<summary>Abstract</summary>
In Part I of this two-part paper (Multi-Timescale Control and Communications with Deep Reinforcement Learning -- Part I: Communication-Aware Vehicle Control), we decomposed the multi-timescale control and communications (MTCC) problem in Cellular Vehicle-to-Everything (C-V2X) system into a communication-aware Deep Reinforcement Learning (DRL)-based platoon control (PC) sub-problem and a control-aware DRL-based radio resource allocation (RRA) sub-problem. We focused on the PC sub-problem and proposed the MTCC-PC algorithm to learn an optimal PC policy given an RRA policy. In this paper (Part II), we first focus on the RRA sub-problem in MTCC assuming a PC policy is given, and propose the MTCC-RRA algorithm to learn the RRA policy. Specifically, we incorporate the PC advantage function in the RRA reward function, which quantifies the amount of PC performance degradation caused by observation delay. Moreover, we augment the state space of RRA with PC action history for a more well-informed RRA policy. In addition, we utilize reward shaping and reward backpropagation prioritized experience replay (RBPER) techniques to efficiently tackle the multi-agent and sparse reward problems, respectively. Finally, a sample- and computational-efficient training approach is proposed to jointly learn the PC and RRA policies in an iterative process. In order to verify the effectiveness of the proposed MTCC algorithm, we performed experiments using real driving data for the leading vehicle, where the performance of MTCC is compared with those of the baseline DRL algorithms.
</details>
<details>
<summary>摘要</summary>
在这两部分文章中（多时间尺度控制和通信 WITH Deep Reinforcement Learning -- 部分 I：通信意识排队控制），我们将多时间尺度控制和通信（MTCC）问题分解为一个基于 Deep Reinforcement Learning（DRL）的排队控制（PC）子问题和一个控制意识DRL的广播资源分配（RRA）子问题。我们主要关注PC子问题，并提出了MTCC-PC算法，以学习一个最佳PC策略。在这篇文章中（部分 II），我们首先关注RRA子问题，假设PC策略已知，并提出了MTCC-RRA算法，以学习RRA策略。具体来说，我们将PC优势函数 incorporated into RRA reward function，该函数量化了由观测延迟引起的PC性能下降。此外，我们将RRA状态空间扩展为包括PC动作历史，以更好地优化RRA策略。此外，我们使用 reward shaping和reward backpropagation prioritized experience replay（RBPER）技术，以有效地解决多机器人和罕见奖励问题。最后，我们提出了一种效率的训练方法，以同时学习PC和RRA策略。为验证我们提出的MTCC算法的有效性，我们使用实际驾驶数据进行了实验，并与基eline DRL算法进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-quantification-for-noisy-inputs-outputs-in-physics-informed-neural-networks-and-neural-operators"><a href="#Uncertainty-quantification-for-noisy-inputs-outputs-in-physics-informed-neural-networks-and-neural-operators" class="headerlink" title="Uncertainty quantification for noisy inputs-outputs in physics-informed neural networks and neural operators"></a>Uncertainty quantification for noisy inputs-outputs in physics-informed neural networks and neural operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11262">http://arxiv.org/abs/2311.11262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongren Zou, Xuhui Meng, George Em Karniadakis</li>
<li>for: 本文主要针对的是科学机器学习（SciML）中的不确定性评估（UQ）问题，特别是physics-informed neural networks（PINNs）和neural operators（NOs）中的输入不确定性。</li>
<li>methods: 本文提出了一种 bayesian 方法来评估 PINNs 和 NOs 中的输入不确定性。这种方法可以轻松地结合 PINNs 和 NOs，使其能够处理含有噪声的输入和输出。</li>
<li>results: 本文的结果表明，bayesian 方法可以有效地评估 PINNs 和 NOs 中的输入不确定性，并且可以在含有噪声的输入和输出的情况下提供可靠的结果。<details>
<summary>Abstract</summary>
Uncertainty quantification (UQ) in scientific machine learning (SciML) becomes increasingly critical as neural networks (NNs) are being widely adopted in addressing complex problems across various scientific disciplines. Representative SciML models are physics-informed neural networks (PINNs) and neural operators (NOs). While UQ in SciML has been increasingly investigated in recent years, very few works have focused on addressing the uncertainty caused by the noisy inputs, such as spatial-temporal coordinates in PINNs and input functions in NOs. The presence of noise in the inputs of the models can pose significantly more challenges compared to noise in the outputs of the models, primarily due to the inherent nonlinearity of most SciML algorithms. As a result, UQ for noisy inputs becomes a crucial factor for reliable and trustworthy deployment of these models in applications involving physical knowledge. To this end, we introduce a Bayesian approach to quantify uncertainty arising from noisy inputs-outputs in PINNs and NOs. We show that this approach can be seamlessly integrated into PINNs and NOs, when they are employed to encode the physical information. PINNs incorporate physics by including physics-informed terms via automatic differentiation, either in the loss function or the likelihood, and often take as input the spatial-temporal coordinate. Therefore, the present method equips PINNs with the capability to address problems where the observed coordinate is subject to noise. On the other hand, pretrained NOs are also commonly employed as equation-free surrogates in solving differential equations and Bayesian inverse problems, in which they take functions as inputs. The proposed approach enables them to handle noisy measurements for both input and output functions with UQ.
</details>
<details>
<summary>摘要</summary>
科学机器学习（SciML）中的不确定性评估（UQ）在科学领域中日益成为关键，因为神经网络（NN）在解决复杂问题上广泛应用。代表性的SciML模型包括物理学习神经网络（PINNs）和神经运算（NOs）。虽然UQ在SciML中已有很多研究，但很少研究涉及到由输入噪声引起的不确定性，如PINNs中的空间-时间坐标和NOs中的输入函数。噪声在模型输入中存在的问题比噪声在模型输出中更加复杂，主要是因为大多数SciML算法具有非线性性。因此，对于可靠和可信worthy的应用，UQ для噪声输入成为关键因素。为此，我们提出了一种 bayesian 方法来评估 PINNs 和 NOs 中由噪声输入引起的不确定性。我们示示该方法可以轻松地 интеGRATE 到 PINNs 和 NOs 中，当它们用于编码物理信息时。PINNs 通过自动微分来包含物理约束，通常是在损失函数或概率函数中，以及输入空间-时间坐标。因此，当前的方法允许 PINNs 解决受噪声影响的问题。另一方面，预训练的 NOs 通常也被用作 equation-free 代表物和抽象问题的解决方案，它们通常接受函数作为输入。我们的方法允许它们处理噪声测量数据，包括输入和输出函数的不确定性评估。
</details></li>
</ul>
<hr>
<h2 id="BOIS-Bayesian-Optimization-of-Interconnected-Systems"><a href="#BOIS-Bayesian-Optimization-of-Interconnected-Systems" class="headerlink" title="BOIS: Bayesian Optimization of Interconnected Systems"></a>BOIS: Bayesian Optimization of Interconnected Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11254">http://arxiv.org/abs/2311.11254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo D. González, Victor M. Zavala</li>
<li>for: 用于优化昂贵的样本系统的全面优化</li>
<li>methods: 使用 Gaussian processes（GP）模型来 caracterize 模型不确定性，并将其用于导引学习和搜索过程</li>
<li>results: 使用 composite functions 可以利用结构知识（例如物理和稀谱连接），并且可以高效地使用 GP 模型来描述 composite functions，从而实现更好的优化性能和准确地捕捉 composite functions 的统计特征。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) has proven to be an effective paradigm for the global optimization of expensive-to-sample systems. One of the main advantages of BO is its use of Gaussian processes (GPs) to characterize model uncertainty which can be leveraged to guide the learning and search process. However, BO typically treats systems as black-boxes and this limits the ability to exploit structural knowledge (e.g., physics and sparse interconnections). Composite functions of the form $f(x, y(x))$, wherein GP modeling is shifted from the performance function $f$ to an intermediate function $y$, offer an avenue for exploiting structural knowledge. However, the use of composite functions in a BO framework is complicated by the need to generate a probability density for $f$ from the Gaussian density of $y$ calculated by the GP (e.g., when $f$ is nonlinear it is not possible to obtain a closed-form expression). Previous work has handled this issue using sampling techniques; these are easy to implement and flexible but are computationally intensive. In this work, we introduce a new paradigm which allows for the efficient use of composite functions in BO; this uses adaptive linearizations of $f$ to obtain closed-form expressions for the statistical moments of the composite function. We show that this simple approach (which we call BOIS) enables the exploitation of structural knowledge, such as that arising in interconnected systems as well as systems that embed multiple GP models and combinations of physics and GP models. Using a chemical process optimization case study, we benchmark the effectiveness of BOIS against standard BO and sampling approaches. Our results indicate that BOIS achieves performance gains and accurately captures the statistics of composite functions.
</details>
<details>
<summary>摘要</summary>
bayesian 优化（BO）已经证明是一种有效的全面优化方法，特别是在高效尝试系统上。BO的一个主要优势是使用 Gaussian processes（GP）来描述模型不确定性，这可以用来导引学习和搜索过程。然而，BO通常会将系统看作黑盒子，这限制了能够利用结构知识（例如物理和稀疏连接）。使用 $f(x, y(x)) $ 的复合函数，其中 GP 模型从性能函数 $f$ 转移到中间函数 $y$，提供了利用结构知识的途径。然而，在 BO 框架中使用复合函数增加了生成 $f$ 的概率密度的问题，特别是当 $f$ 非线性时，不能得到关闭式表达。先前的工作通过抽样技术解决这个问题，这些技术容易实现但是计算昂贵。在这篇文章中，我们介绍了一种新的方法，即 BOIS，它使得在 BO 框架中高效地使用复合函数。我们利用了 $f$ 的适应线性化来获得复合函数的统计 moments 的关闭式表达。我们的结果表明，BOIS 可以在化学过程优化案例中实现性能提升和准确地捕捉复合函数的统计特征。
</details></li>
</ul>
<hr>
<h2 id="A-Universal-Framework-for-Accurate-and-Efficient-Geometric-Deep-Learning-of-Molecular-Systems"><a href="#A-Universal-Framework-for-Accurate-and-Efficient-Geometric-Deep-Learning-of-Molecular-Systems" class="headerlink" title="A Universal Framework for Accurate and Efficient Geometric Deep Learning of Molecular Systems"></a>A Universal Framework for Accurate and Efficient Geometric Deep Learning of Molecular Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11228">http://arxiv.org/abs/2311.11228</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XieResearchGroup/Physics-aware-Multiplex-GNN">https://github.com/XieResearchGroup/Physics-aware-Multiplex-GNN</a></li>
<li>paper_authors: Shuo Zhang, Yang Liu, Lei Xie</li>
<li>for: 本研究旨在提供一个通用的框架，用于精确地和有效地学习不同类型和大小的分子的表现，并在各种分子系统中实现高精度和高效率。</li>
<li>methods: 本研究使用了几何深度学习，特别是几何神经网络，以实现分子表现的学习。它还将分子表现模型化为几何学问中的力学问题，以实现更好地模型分子的本质和相互作用。</li>
<li>results: 本研究的实验结果显示，PAMNet在三种不同的学习任务中都能够实现高精度和高效率。具体来说，PAMNet在小分子性质、RNA 3D结构和蛋白质-药物结合亲和力等三种任务中，都能够超越现有的基elines。<details>
<summary>Abstract</summary>
Molecular sciences address a wide range of problems involving molecules of different types and sizes and their complexes. Recently, geometric deep learning, especially Graph Neural Networks, has shown promising performance in molecular science applications. However, most existing works often impose targeted inductive biases to a specific molecular system, and are inefficient when applied to macromolecules or large-scale tasks, thereby limiting their applications to many real-world problems. To address these challenges, we present PAMNet, a universal framework for accurately and efficiently learning the representations of three-dimensional (3D) molecules of varying sizes and types in any molecular system. Inspired by molecular mechanics, PAMNet induces a physics-informed bias to explicitly model local and non-local interactions and their combined effects. As a result, PAMNet can reduce expensive operations, making it time and memory efficient. In extensive benchmark studies, PAMNet outperforms state-of-the-art baselines regarding both accuracy and efficiency in three diverse learning tasks: small molecule properties, RNA 3D structures, and protein-ligand binding affinities. Our results highlight the potential for PAMNet in a broad range of molecular science applications.
</details>
<details>
<summary>摘要</summary>
分子科学研究着眼于各种不同类型和大小的分子和其复合物的问题。近年来，深度学习，特别是图 Nuevos Networks（GNN）在分子科学应用中表现了承诺的能力。然而，大多数现有工作通常会对特定分子系统强制投入预设目标，因此在巨大分子或大规模任务中不具有广泛应用的能力。为解决这些挑战，我们提出了PAMNet，一个通用的框架，可以高精度地和高效地学习三维分子的表示。 draw inspiration from molecular mechanics，PAMNet采用物理学习来显式地模型本地和非本地交互，并将其相互作用的结果相互作用。这使得PAMNet可以降低成本，提高时间和内存效率。在广泛的审核实验中，PAMNet在三种多样化学习任务中（小分子性质、RNA 3D结构和蛋白质-药物结合稳定性）都与当前基elines相比，在准确性和效率方面具有显著的优势。这些结果表明PAMNet在分子科学应用中具有广泛的应用前景。
</details></li>
</ul>
<hr>
<h2 id="TextGuard-Provable-Defense-against-Backdoor-Attacks-on-Text-Classification"><a href="#TextGuard-Provable-Defense-against-Backdoor-Attacks-on-Text-Classification" class="headerlink" title="TextGuard: Provable Defense against Backdoor Attacks on Text Classification"></a>TextGuard: Provable Defense against Backdoor Attacks on Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11225">http://arxiv.org/abs/2311.11225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-secure/textguard">https://github.com/ai-secure/textguard</a></li>
<li>paper_authors: Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, Dawn Song</li>
<li>for: 防止机器学习模型中的后门攻击（backdoor attacks），以提高安全应用中的安全性。</li>
<li>methods:  TextGuard 使用分割训练数据（backdoored）为多个子训练集（sub-training sets），然后从每个子训练集训练基本分类器， ensemble 提供最终预测。 我们 theoretically prove 当背门关键（backdoor trigger）的长度在某些阈值以下时，TextGuard 可以保证其预测结果不受训练和测试输入中的背门关键影响。</li>
<li>results: TextGuard 在三个 benchmark 文本分类任务上显示出超过现有认证防护措施的认证率，并且提出了更多的实际性提升策略。 comparisons 显示 TextGuard 在对多个后门攻击的比较中与现有的实际防护措施相比较高。 code 和数据可以在 <a target="_blank" rel="noopener" href="https://github.com/AI-secure/TextGuard">https://github.com/AI-secure/TextGuard</a> 上取得。<details>
<summary>Abstract</summary>
Backdoor attacks have become a major security threat for deploying machine learning models in security-critical applications. Existing research endeavors have proposed many defenses against backdoor attacks. Despite demonstrating certain empirical defense efficacy, none of these techniques could provide a formal and provable security guarantee against arbitrary attacks. As a result, they can be easily broken by strong adaptive attacks, as shown in our evaluation. In this work, we propose TextGuard, the first provable defense against backdoor attacks on text classification. In particular, TextGuard first divides the (backdoored) training data into sub-training sets, achieved by splitting each training sentence into sub-sentences. This partitioning ensures that a majority of the sub-training sets do not contain the backdoor trigger. Subsequently, a base classifier is trained from each sub-training set, and their ensemble provides the final prediction. We theoretically prove that when the length of the backdoor trigger falls within a certain threshold, TextGuard guarantees that its prediction will remain unaffected by the presence of the triggers in training and testing inputs. In our evaluation, we demonstrate the effectiveness of TextGuard on three benchmark text classification tasks, surpassing the certification accuracy of existing certified defenses against backdoor attacks. Furthermore, we propose additional strategies to enhance the empirical performance of TextGuard. Comparisons with state-of-the-art empirical defenses validate the superiority of TextGuard in countering multiple backdoor attacks. Our code and data are available at https://github.com/AI-secure/TextGuard.
</details>
<details>
<summary>摘要</summary>
《文本攻击：一种新的安全问题》现代机器学习模型在安全敏感应用中部署时，针对文本攻击已成为主要的安全问题。现有的研究努力提出了许多防御策略，尽管在实验室中得到了一定的效果，但是无法提供正式和可证明的安全保证。因此，它们可以被强大的适应性攻击打砸，如我们的评估结果所示。在这项工作中，我们提出了 TextGuard，这是第一个可证明的文本攻击防御策略。TextGuard的核心思想是将（被恶意修改的）训练数据分成子训练集，每个子训练集由每个训练句子分成多个子句。这种分割确保了大多数子训练集不包含恶意触发符。然后，从每个子训练集中训练基础分类器，并将它们的 ensemble 提供最终预测。我们理论上证明，当恶意触发符的长度在某个阈值之下时，TextGuard 的预测将不受训练和测试输入中恶意触发符的影响。在我们的评估中，我们证明了 TextGuard 在三个标准文本分类任务上的效果，超过了现有的证书防御策略对恶意攻击的证书率。此外，我们还提出了更多的实际性优化策略，并与现有的实际防御策略进行比较，证明 TextGuard 在对多个恶意攻击的防御方面具有superiority。我们的代码和数据可以在 GitHub 上找到：https://github.com/AI-secure/TextGuard。
</details></li>
</ul>
<hr>
<h2 id="Robust-Network-Slicing-Multi-Agent-Policies-Adversarial-Attacks-and-Defensive-Strategies"><a href="#Robust-Network-Slicing-Multi-Agent-Policies-Adversarial-Attacks-and-Defensive-Strategies" class="headerlink" title="Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and Defensive Strategies"></a>Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and Defensive Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11206">http://arxiv.org/abs/2311.11206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Wang, M. Cenk Gursoy, Senem Velipasalar</li>
<li>for: 本文提出了一种基于多智能深度学习（深度RL）的网络切片框架，用于在多个基站和多个用户的动态环境中实现网络切片。特别是，我们提出了一种多actor多评估器（MACC）的深度RL框架，在其中，actor是实现为指针网络，以适应输入的变化维度。</li>
<li>methods: 我们提出了一种基于深度RL的网络切片攻击方法，包括设计了一个具有有限制的优化功能和有限制的功率预算的攻击者。我们还解决了攻击者的找寻位置优化和通信频道优化问题 via 深度RL。</li>
<li>results: 我们通过实验表明，提出的攻击者可以在不知情的情况下，对网络切片的性能产生显著的降低。此外，我们还提出了一种基于 Nash 平衡的策略ensemble 混合策略profile，用于网络切片和攻击者之间的对抗。我们通过在实验中应用该策略ensemble algorithm，显示其效果。<details>
<summary>Abstract</summary>
In this paper, we present a multi-agent deep reinforcement learning (deep RL) framework for network slicing in a dynamic environment with multiple base stations and multiple users. In particular, we propose a novel deep RL framework with multiple actors and centralized critic (MACC) in which actors are implemented as pointer networks to fit the varying dimension of input. We evaluate the performance of the proposed deep RL algorithm via simulations to demonstrate its effectiveness. Subsequently, we develop a deep RL based jammer with limited prior information and limited power budget. The goal of the jammer is to minimize the transmission rates achieved with network slicing and thus degrade the network slicing agents' performance. We design a jammer with both listening and jamming phases and address jamming location optimization as well as jamming channel optimization via deep RL. We evaluate the jammer at the optimized location, generating interference attacks in the optimized set of channels by switching between the jamming phase and listening phase. We show that the proposed jammer can significantly reduce the victims' performance without direct feedback or prior knowledge on the network slicing policies. Finally, we devise a Nash-equilibrium-supervised policy ensemble mixed strategy profile for network slicing (as a defensive measure) and jamming. We evaluate the performance of the proposed policy ensemble algorithm by applying on the network slicing agents and the jammer agent in simulations to show its effectiveness.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一个多代理深度学习游戏（深度RL）框架，用于在多个基站和多个用户之间进行网络剖析。特别是，我们提出了一种新的深度RL框架，称为多actor和中央评估器（MACC）。在这种框架中，代理是实现为指针网络，以适应输入的变化维度。我们通过实验评估了提出的深度RL算法的性能，以示其效果。然后，我们开发了一个基于深度RL的干扰器，具有限制的先验知识和能量预算。干扰器的目标是降低网络剖析代理的性能，以达到干扰网络剖析的目的。我们设计了干扰器的 listening和干扰阶段，并处理干扰通道优化和干扰位置优化via深度RL。我们在优化的位置上测试了干扰器，在优化的set of channels上发生了干扰攻击。我们发现，我们提出的干扰器可以在不知道网络剖析策略的情况下，对受害者的性能进行显著降低。最后，我们设计了一种以战略混合为基础的纳什平衡监督策略套件，用于防御网络剖析和干扰。我们通过在网络剖析代理和干扰器代理上应用该策略套件来评估其效果。
</details></li>
</ul>
<hr>
<h2 id="Scale-free-networks-improved-inference"><a href="#Scale-free-networks-improved-inference" class="headerlink" title="Scale-free networks: improved inference"></a>Scale-free networks: improved inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11200">http://arxiv.org/abs/2311.11200</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Nixon Jerez-Lillo, Francisco A. Rodrigues, Pedro L. Ramos</li>
<li>for: 这 paper 是 investigating whether the degree distribution of a network follows a power-law distribution, and providing improved methods for estimating the model parameters using Bayesian inference.</li>
<li>methods: 这 paper 使用 Bayesian inference 来 obtain accurate estimates and precise credibility intervals for the parameters of both continuous and discrete distributions.</li>
<li>results: 该 paper 的 results indicate that the proposed method is more suitable in practice, as it yields a frequency of acceptance close to the specified nominal level, and provides a more accurate discernment of whether a network or any other dataset adheres to a power-law distribution.<details>
<summary>Abstract</summary>
The power-law distribution plays a crucial role in complex networks as well as various applied sciences. Investigating whether the degree distribution of a network follows a power-law distribution is an important concern. The commonly used inferential methods for estimating the model parameters often yield biased estimates, which can lead to the rejection of the hypothesis that a model conforms to a power-law. In this paper, we discuss improved methods that utilize Bayesian inference to obtain accurate estimates and precise credibility intervals. The inferential methods are derived for both continuous and discrete distributions. These methods reveal that objective Bayesian approaches return nearly unbiased estimates for the parameters of both models. Notably, in the continuous case, we identify an explicit posterior distribution. This work enhances the power of goodness-of-fit tests, enabling us to accurately discern whether a network or any other dataset adheres to a power-law distribution. We apply the proposed approach to fit degree distributions for more than 5,000 synthetic networks and over 3,000 real networks. The results indicate that our method is more suitable in practice, as it yields a frequency of acceptance close to the specified nominal level.
</details>
<details>
<summary>摘要</summary>
“带法分布在复杂网络以及各种应用科学中发挥重要作用。确定网络度分布是否遵循带法分布是一项重要的问题。常用的推断方法可能会导致偏向估计，这可能会导致拒绝带法分布模型遵循的假设。在这篇论文中，我们讨论了改进的方法，使用 bayesian 推断来获得准确的估计和准确的信任范围。这些方法适用于连续和整数分布。这些方法表明，对 bayesian 方法进行 objective 推断可以获得准确的参数估计。在连续 случа子中，我们确定了一个Explicit posterior distribution。这项工作使得好宜度适应测试更加准确地判断网络或任何其他数据集是否遵循带法分布。我们对5000个Synthetic网络和3000个实际网络进行适用。结果表明，我们的方法在实践中更加适用，其频率接受接近指定的正常水平。”
</details></li>
</ul>
<hr>
<h2 id="Testing-with-Non-identically-Distributed-Samples"><a href="#Testing-with-Non-identically-Distributed-Samples" class="headerlink" title="Testing with Non-identically Distributed Samples"></a>Testing with Non-identically Distributed Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11194">http://arxiv.org/abs/2311.11194</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Shivam Garg, Chirag Pabbaraju, Kirankumar Shiragur, Gregory Valiant</li>
<li>for: 本文研究了在独立不同分布下的非线性样本计数和估计问题。特别是，我们考虑了以下分布测试框架：假设有一组分布在整数支持上，大小为$k$，我们从每个分布中采样$c$次，然后从这些采样中学习或测试一个平均分布，即$\textbf{p}_{\text{avg}$。这种设置符合许多实际应用中的各种各样的Entity不同的情况，例如个人、不同时间 periods、不同空间数据源等。</li>
<li>methods: 我们使用了非线性样本计数和估计方法，包括在$c&#x3D;1$个分布下学习$\textbf{p}_{\text{avg}$，以及在$c \ge 2$个分布下进行uniformity测试和非标准测试。</li>
<li>results: 我们得到了以下结果：在$c&#x3D;1$个分布下，需要$\Theta(k&#x2F;\epsilon^2)$样本来准确地学习$\textbf{p}_{\text{avg}$，而在$c \ge 2$个分布下，只需要$O(\sqrt{k}&#x2F;\epsilon^2 + 1&#x2F;\epsilon^4)$样本来进行uniformity测试和非标准测试。此外，我们还证明了在$c&#x3D;2$个分布下，存在一个常数$\rho &gt; 0$，使得即使有$\rho k$样本，也无法进行uniformity测试。<details>
<summary>Abstract</summary>
We examine the extent to which sublinear-sample property testing and estimation applies to settings where samples are independently but not identically distributed. Specifically, we consider the following distributional property testing framework: Suppose there is a set of distributions over a discrete support of size $k$, $\textbf{p}_1, \textbf{p}_2,\ldots,\textbf{p}_T$, and we obtain $c$ independent draws from each distribution. Suppose the goal is to learn or test a property of the average distribution, $\textbf{p}_{\mathrm{avg}$. This setup models a number of important practical settings where the individual distributions correspond to heterogeneous entities -- either individuals, chronologically distinct time periods, spatially separated data sources, etc. From a learning standpoint, even with $c=1$ samples from each distribution, $\Theta(k/\varepsilon^2)$ samples are necessary and sufficient to learn $\textbf{p}_{\mathrm{avg}$ to within error $\varepsilon$ in TV distance. To test uniformity or identity -- distinguishing the case that $\textbf{p}_{\mathrm{avg}$ is equal to some reference distribution, versus has $\ell_1$ distance at least $\varepsilon$ from the reference distribution, we show that a linear number of samples in $k$ is necessary given $c=1$ samples from each distribution. In contrast, for $c \ge 2$, we recover the usual sublinear sample testing of the i.i.d. setting: we show that $O(\sqrt{k}/\varepsilon^2 + 1/\varepsilon^4)$ samples are sufficient, matching the optimal sample complexity in the i.i.d. case in the regime where $\varepsilon \ge k^{-1/4}$. Additionally, we show that in the $c=2$ case, there is a constant $\rho > 0$ such that even in the linear regime with $\rho k$ samples, no tester that considers the multiset of samples (ignoring which samples were drawn from the same $\textbf{p}_i$) can perform uniformity testing.
</details>
<details>
<summary>摘要</summary>
我们研究对于不同分布的独立样本集合中的输入检测和估计的应用。具体来说，我们考虑以下分布测试框架：我们有一集合 $\textbf{p}_1, \textbf{p}_2, \ldots, \textbf{p}_T$ 中的分布，每个分布都是在有限的类别支持 $\left[k\right]$ 上的。我们从每个分布中获得 $c$ 个独立的样本，并想要检测或学习这些分布的平均分布 $\textbf{p}_{\text{avg}$ 的性质。这个设定模拟了许多实际上重要的问题，例如对于各个分布的不同个体、不同时间期、不同空间位置等。从学习角度来看，就算每个分布只有一个样本，我们还需要 $\Theta\left(\frac{k}{\varepsilon^2}\right)$ 个样本来学习 $\textbf{p}_{\text{avg}$ 到误差 $\varepsilon$ 以内。对于检测均匀性或同一性，我们显示了一个线性的样本数量是必要的，即 $\Theta\left(k\right)$ 个样本，只要有 $c=1$ 个样本来自每个分布。在对比之下，如果 $c \geq 2$，我们可以回复到传统的非线性样本测试情况：我们显示了 $O\left(\sqrt{\frac{k}{\varepsilon^2} + \frac{1}{\varepsilon^4}\right)$ 个样本是充足的，与 i.i.d. 情况的优秀样本复杂度成比例。此外，我们还显示了在 $c=2$ 情况下，存在一个常数 $\rho > 0$，使得甚至在 $\rho k$ 个样本的情况下，不可能将多个样本视为同一个分布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/cs.LG_2023_11_19/" data-id="clp88dc0300vtob884dhx7wmr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/eess.IV_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T09:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/eess.IV_2023_11_19/">eess.IV - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Classification-of-Radio-Galaxies-with-trainable-COSFIRE-filters"><a href="#Classification-of-Radio-Galaxies-with-trainable-COSFIRE-filters" class="headerlink" title="Classification of Radio Galaxies with trainable COSFIRE filters"></a>Classification of Radio Galaxies with trainable COSFIRE filters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11286">http://arxiv.org/abs/2311.11286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Ndungu, Trienko Grobler, Stefan J. Wijnholds Dimka Karastoyanova, George Azzopardi</li>
<li>for:  radio galaxy classification</li>
<li>methods: COSFIRE filters (explainable, learning-free, rotation-tolerant, efficient)</li>
<li>results: achieved an average accuracy rate of 93.36%, outperformed contemporary deep learning models, better computational performance ( $\sim$20$\times$ fewer operations)Here’s the full translation in Simplified Chinese:</li>
<li>for:  radio galaxy classification</li>
<li>methods: COSFIRE filters (可解释的、学习自由的、旋转快速的、高效的)</li>
<li>results: 实现了平均准确率93.36%, 超越了当今深度学习模型，计算性能更好（相对于深度网络模型相同准确率下，操作数量 $\sim$20$\times$ 少）I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Radio galaxies exhibit a rich diversity of characteristics and emit radio emissions through a variety of radiation mechanisms, making their classification into distinct types based on morphology a complex challenge. To address this challenge effectively, we introduce an innovative approach for radio galaxy classification using COSFIRE filters. These filters possess the ability to adapt to both the shape and orientation of prototype patterns within images. The COSFIRE approach is explainable, learning-free, rotation-tolerant, efficient, and does not require a huge training set. To assess the efficacy of our method, we conducted experiments on a benchmark radio galaxy data set comprising of 1180 training samples and 404 test samples. Notably, our approach achieved an average accuracy rate of 93.36\%. This achievement outperforms contemporary deep learning models, and it is the best result ever achieved on this data set. Additionally, COSFIRE filters offer better computational performance, $\sim$20$\times$ fewer operations than the DenseNet-based competing method (when comparing at the same accuracy). Our findings underscore the effectiveness of the COSFIRE filter-based approach in addressing the complexities associated with radio galaxy classification. This research contributes to advancing the field by offering a robust solution that transcends the orientation challenges intrinsic to radio galaxy observations. Our method is versatile in that it is applicable to various image classification approaches.
</details>
<details>
<summary>摘要</summary>
Radio галактики显示出多样化的特征和通过多种辐射机制发射电波，因此将其分类为不同类型的复杂挑战。为解决这个挑战，我们介绍了一种创新的电波 галактиockey Classificationapproach使用COSFFIRE filters。这些筛子具有适应形状和方向的辐射模式图像的能力。COSFFIRE方法是可解释的，不需要学习，不需要旋转，高效，并且不需要庞大的训练集。为评估我们的方法的有效性，我们在一个标准电波 галактиockey数据集上进行了实验，该数据集包括1180个训练样本和404个测试样本。值得注意的是，我们的方法达到了93.36%的准确率，这比同时期的深度学习模型高出了许多，并且是这个数据集上最佳的成绩。此外，COSFFIRE筛子在计算性能方面也表现出优异，相比同精度的DenseNet模型，它在同一准确率下需要$\sim$20$\times$ fewer operations。我们的发现论据了COSFFIRE筛子基于的方法在电波 галактиockey分类中的效iveness，这种方法可以抵御天线望远镜的方向挑战，并且可以应用于多种图像分类方法。
</details></li>
</ul>
<hr>
<h2 id="Wireless-Regional-Imaging-through-Reconfigurable-Intelligent-Surfaces-Passive-Mode"><a href="#Wireless-Regional-Imaging-through-Reconfigurable-Intelligent-Surfaces-Passive-Mode" class="headerlink" title="Wireless Regional Imaging through Reconfigurable Intelligent Surfaces: Passive Mode"></a>Wireless Regional Imaging through Reconfigurable Intelligent Surfaces: Passive Mode</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11222">http://arxiv.org/abs/2311.11222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuhai Wang, Chun Wang, Rujing Xiong, Zhengyu Wang, Tiebin Mi, Robert Caiming Qiu</li>
<li>for: 该论文提出了一种基于多个反射器的无线成像框架，用于解决分布式感知网络的问题。</li>
<li>methods: 该论文使用了一种随机反射Pattern的调整方法，使接收器能够在定义的空间区域内（SoI）捕捉信号。在该系统中，提出了一种多个反射器-aided linear imaging频率模型。</li>
<li>results: 该论文通过 teoretic 框架来恢复信号强度分布在SoI中。多个参数对系统的性能的影响被分析了。 simulation 结果证明了该提议的正确性。此外，该论文还提出了一种 amplitude-only 成像算法来 mitigate 反射器 phase 不可预测的问题。<details>
<summary>Abstract</summary>
In this paper, we propose a multi-RIS-aided wireless imaging framework in 3D facing the distributed placement of multi-sensor networks. The system creates a randomized reflection pattern by adjusting the RIS phase shift, enabling the receiver to capture signals within the designated space of interest (SoI). Firstly, a multi-RIS-aided linear imaging channel modeling is proposed. We introduce a theoretical framework of computational imaging to recover the signal strength distribution of the SOI. For the RIS-aided imaging system, the impact of multiple parameters on the performance of the imaging system is analyzed. The simulation results verify the correctness of the proposal. Furthermore, we propose an amplitude-only imaging algorithm for the RIS-aided imaging system to mitigate the problem of phase unpredictability. Finally, the performance verification of the imaging algorithm is carried out by proof of concept experiments under reasonable parameter settings.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于多个反射器的无线影像框架，面临分布式感知网络的问题。系统通过调整反射器的相位偏移，使接收器能够在指定的空间区域（SoI）内捕捉信号。首先，我们提出了一种基于多个反射器的线性影像通道模型。我们介绍了计算影像的理论框架，以复制信号强度分布的SoI。对于基于反射器的影像系统，我们分析了多个参数对系统性能的影响。实验结果证明了我们的提议的正确性。此外，我们提出了一种仅基于干扰的影像算法，以解决反射器辐射的相位不可预测问题。最后，我们验证了影像算法的性能通过实验，并在合理的参数设置下进行了证明。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/eess.IV_2023_11_19/" data-id="clp88dc6t01d6ob88dq4b7pcw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/eess.SP_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T08:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/eess.SP_2023_11_19/">eess.SP - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Rethinking-Integrated-Sensing-and-Communication-When-Near-Field-Meets-Wideband"><a href="#Rethinking-Integrated-Sensing-and-Communication-When-Near-Field-Meets-Wideband" class="headerlink" title="Rethinking Integrated Sensing and Communication: When Near Field Meets Wideband"></a>Rethinking Integrated Sensing and Communication: When Near Field Meets Wideband</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11416">http://arxiv.org/abs/2311.11416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaolin Wang, Xidong Mu, Yuanwei Liu</li>
<li>for: 这篇论文探讨了在大天线数组附近场区域内运行的集成感知通信（ISAC）系统，并利用大带宽。作者解释了广泛感知通信（S&amp;C）频道的基本特征，并 highlighted在远场到近场区域的转换中发生的两个基本变化。</li>
<li>methods: 论文使用了各种方法，包括描述性分析和数值仿真，来探讨near-field区域内的广泛感知通信系统。</li>
<li>results: 论文得到了以下结果：1) 近场区域内的强angular-delay correlation可以启用广泛的功能性，例如信号多普逻和距离测量；2) element-specific Doppler frequencies可以在高速移动的S&amp;C场景中实现不可能得到的功能性。<details>
<summary>Abstract</summary>
This article re-examines integrated sensing and communication (ISAC) systems operating in the near-field region of a large antenna array while exploiting a large bandwidth. We first reveal the fundamental characteristics of wideband sensing and communication (S&C) channels and highlight the key changes that occur during the transition from the far-field to the near-field region. Specifically, there are two fundamental changes in the near-field region: strong angular-delay correlation and element-specific Doppler frequencies. It is highlighted that the near-field effect can enable the wideband-like S&C functionalities in terms of signal multiplexing and range sensing due to the strong angular-delay correlation, thus allowing the trading of large antenna arrays for large bandwidths. Furthermore, it also introduces the wideband-unattainable functionalities in high mobility S&C scenarios by leveraging the element-specific Doppler frequencies. We then delineate certain paradigm shifts in thinking required to advance toward near-field wideband ISAC systems, with a particular emphasis on resource allocation, antenna array arrangement, and transceiver architecture. Lastly, some other promising directions are discussed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Vital-Signs-Estimation-Using-a-26-GHz-Multi-Beam-Communication-Testbed"><a href="#Vital-Signs-Estimation-Using-a-26-GHz-Multi-Beam-Communication-Testbed" class="headerlink" title="Vital Signs Estimation Using a 26 GHz Multi-Beam Communication Testbed"></a>Vital Signs Estimation Using a 26 GHz Multi-Beam Communication Testbed</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11275">http://arxiv.org/abs/2311.11275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miquel Sellés Valls, Sofie Pollin, Ying Wang, Rizqi Hersyandika, Andre Kokkeler, Yang Miao</li>
<li>for: 这篇研究旨在开发一个基于26GHz多维扁平波幕的生命预警系统。</li>
<li>methods: 该系统使用多维扁平波幕进行无线通讯和感知，并利用空间正交的码矩来估计呼吸速率和心率。</li>
<li>results: 研究结果显示，在单人对称感知enario下，使用时间频率均衡和柔体波лет变换可以提高性能，并且在双人对称感知enario下，使用K-means聚类算法可以提取多人的生命预警资料。<details>
<summary>Abstract</summary>
This paper presents a novel pipeline for vital sign monitoring using a 26 GHz multi-beam communication testbed. In context of Joint Communication and Sensing (JCAS), the advanced communication capability at millimeter-wave bands is comparable to the radio resource of radars and is promising to sense the surrounding environment. Being able to communicate and sense the vital sign of humans present in the environment will enable new vertical services of telecommunication, i.e., remote health monitoring. The proposed processing pipeline leverages spatially orthogonal beams to estimate the vital sign - breath rate and heart rate - of single and multiple persons in static scenarios from the raw Channel State Information samples. We consider both monostatic and bistatic sensing scenarios. For monostatic scenario, we employ the phase time-frequency calibration and Discrete Wavelet Transform to improve the performance compared to the conventional Fast Fourier Transform based methods. For bistatic scenario, we use K-means clustering algorithm to extract multi-person vital signs due to the distinct frequency-domain signal feature between single and multi-person scenarios. The results show that the estimated breath rate and heart rate reach below 2 beats per minute (bpm) error compared to the reference captured by on-body sensor for the single-person monostatic sensing scenario with body-transceiver distance up to 2 m, and the two-person bistatic sensing scenario with BS-UE distance up to 4 m. The presented work does not optimize the OFDM waveform parameters for sensing; it demonstrates a promising JCAS proof-of-concept in contact-free vital sign monitoring using mmWave multi-beam communication systems.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这篇论文提出了一种新的生命 parameter 监测管道，使用26 GHz多束通信测试平台。这种高频通信能力与雷达相当，可以感知周围环境。提议的处理管道利用空间垂直的扩散波来估算人体的呼吸速率和心跳速率，从Raw Channel State Information样本中提取生命参数。管道考虑了单Static和多Static感知方式。对单Static方式，采用相位时间频幅准确和柔波变换来提高性能。对多Static方式，使用K-means归一化算法提取多人生命参数，由于单人和多人方式的频域信号特征的差异。结果显示，估算的呼吸速率和心跳速率可以达到2 beat/分钟（bpm）的准确性，与体缤� sensor captured的参考数据相比。这种工作不仅不优化OFDM波形参数，而且实现了高频多束通信系统的有 Promise JCAS证明。
</details></li>
</ul>
<hr>
<h2 id="Link-Streams-as-a-Generalization-of-Graphs-and-Time-Series"><a href="#Link-Streams-as-a-Generalization-of-Graphs-and-Time-Series" class="headerlink" title="Link Streams as a Generalization of Graphs and Time Series"></a>Link Streams as a Generalization of Graphs and Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11187">http://arxiv.org/abs/2311.11187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esteban Bautista, Matthieu Latapy</li>
<li>for: 本文旨在扩展链流 formalism，并将链流扩展到时间序列之领域。</li>
<li>methods: 本文使用图论扩展来处理链流，并开发了多种信号处理概念的扩展。</li>
<li>results: 本文证明了链流可以被视为时间序列的扩展，并开发了一系列基于链流的信号处理方法。<details>
<summary>Abstract</summary>
A link stream is a set of possibly weighted triplets (t, u, v) modeling that u and v interacted at time t. Link streams offer an effective model for datasets containing both temporal and relational information, making their proper analysis crucial in many applications. They are commonly regarded as sequences of graphs or collections of time series. Yet, a recent seminal work demonstrated that link streams are more general objects of which graphs are only particular cases. It therefore started the construction of a dedicated formalism for link streams by extending graph theory. In this work, we contribute to the development of this formalism by showing that link streams also generalize time series. In particular, we show that a link stream corresponds to a time-series extended to a relational dimension, which opens the door to also extend the framework of signal processing to link streams. We therefore develop extensions of numerous signal concepts to link streams: from elementary ones like energy, correlation, and differentiation, to more advanced ones like Fourier transform and filters.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传输流是一个可能权重的 triplets（t, u, v）模型，表示在时间t上，u和v进行了互动。传输流对具有时间和关系信息的数据进行分析具有重要意义，因此在许多应用中是关键的。它们通常被视为时间序列或图像的序列。然而，一篇最近的著名论文表明，传输流是更加通用的对象，图像只是它的特殊情况。因此，开始了对传输流的专门形式主义的建构。在这项工作中，我们对传输流的形式主义的发展做出了贡献，并显示了传输流也可以扩展为时间序列。具体来说，一个传输流对应于一个扩展了时间维度的时间序列，这开启了对传输流的信号处理框架的扩展。因此，我们对传输流进行了扩展，包括基本的信号概念如能量、相关性和导数，以及更高级的信号概念如快速傅立叶变换和滤波器。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/eess.SP_2023_11_19/" data-id="clp88dc8m01hrob889jl46ze0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/18/cs.CV_2023_11_18/" class="article-date">
  <time datetime="2023-11-18T13:00:00.000Z" itemprop="datePublished">2023-11-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/18/cs.CV_2023_11_18/">cs.CV - 2023-11-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Diverse-Shape-Completion-via-Style-Modulated-Generative-Adversarial-Networks"><a href="#Diverse-Shape-Completion-via-Style-Modulated-Generative-Adversarial-Networks" class="headerlink" title="Diverse Shape Completion via Style Modulated Generative Adversarial Networks"></a>Diverse Shape Completion via Style Modulated Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11184">http://arxiv.org/abs/2311.11184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wesley Khademi, Li Fuxin</li>
<li>For: 本文提出了一种新的 Conditional Generative Adversarial Network (CGAN)，用于完成部分观察到的三维对象的形状。* Methods: 该网络使用了风格修饰来实现多种可能的完成，并通过提取完整形状的风格代码来实现更好的完成。它还引入了多尺度多样性罚款和批判器，以避免 conditional mode collapse 并在不需要多个真实完成的前提下训练。* Results: 对多个 sintetic 和实际数据集进行评估，该方法能够有效地尊重部分观察，同时获得更多的多样性在完成中。<details>
<summary>Abstract</summary>
Shape completion aims to recover the full 3D geometry of an object from a partial observation. This problem is inherently multi-modal since there can be many ways to plausibly complete the missing regions of a shape. Such diversity would be indicative of the underlying uncertainty of the shape and could be preferable for downstream tasks such as planning. In this paper, we propose a novel conditional generative adversarial network that can produce many diverse plausible completions of a partially observed point cloud. To enable our network to produce multiple completions for the same partial input, we introduce stochasticity into our network via style modulation. By extracting style codes from complete shapes during training, and learning a distribution over them, our style codes can explicitly carry shape category information leading to better completions. We further introduce diversity penalties and discriminators at multiple scales to prevent conditional mode collapse and to train without the need for multiple ground truth completions for each partial input. Evaluations across several synthetic and real datasets demonstrate that our method achieves significant improvements in respecting the partial observations while obtaining greater diversity in completions.
</details>
<details>
<summary>摘要</summary>
shape completion 目标是从部分观察到的3D形状恢复完整的形状。这是一个多Modal的问题，因为可以有多种可能性地完成部分观察到的区域。这种多样性表示形状的下面 uncertainty 和可能性，这对下游任务 such as 规划来说是有利的。在这篇论文中，我们提出了一种新的 conditional 生成 Adversarial network，可以生成多种可能性的完整的点云。为了使我们的网络可以生成同一个部分输入多个完整的结果，我们在网络中引入了随机性。在训练中，我们从完整的形状中提取了style code，学习了这些代码的分布，这些代码可以显式地携带形状类别信息，从而得到更好的 completions。我们还引入了多个缩放因子和权重来避免 conditional 模式崩溃和不需要多个真实的完整结果来训练。在多个 sintetic 和实际的数据集上进行了评估，我们的方法在尊重部分观察的同时获得了更大的多样性。
</details></li>
</ul>
<hr>
<h2 id="Active-Prompt-Learning-in-Vision-Language-Models"><a href="#Active-Prompt-Learning-in-Vision-Language-Models" class="headerlink" title="Active Prompt Learning in Vision Language Models"></a>Active Prompt Learning in Vision Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11178">http://arxiv.org/abs/2311.11178</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Jihwan Bang, Sumyeong Ahn, Jae-Gil Lee</li>
<li>for: 本研究旨在适应预训练视觉语言模型（VLM）在活动学习框架下进行适应。</li>
<li>methods: 我们提出了一种新的活动学习框架，称为PCB，以适应预训练VLM。PCB利用VLM的知识提供偏好，以解决标签选择的不均衡问题。</li>
<li>results: 我们在七个实际世界数据集上进行了实验，结果表明PCB比普通的活动学习和随机抽样方法表现更好。<details>
<summary>Abstract</summary>
Pre-trained Vision Language Models (VLMs) have demonstrated notable progress in various zero-shot tasks, such as classification and retrieval. Despite their performance, because improving performance on new tasks requires task-specific knowledge, their adaptation is essential. While labels are needed for the adaptation, acquiring them is typically expensive. To overcome this challenge, active learning, a method of achieving a high performance by obtaining labels for a small number of samples from experts, has been studied. Active learning primarily focuses on selecting unlabeled samples for labeling and leveraging them to train models. In this study, we pose the question, "how can the pre-trained VLMs be adapted under the active learning framework?" In response to this inquiry, we observe that (1) simply applying a conventional active learning framework to pre-trained VLMs even may degrade performance compared to random selection because of the class imbalance in labeling candidates, and (2) the knowledge of VLMs can provide hints for achieving the balance before labeling. Based on these observations, we devise a novel active learning framework for VLMs, denoted as PCB. To assess the effectiveness of our approach, we conduct experiments on seven different real-world datasets, and the results demonstrate that PCB surpasses conventional active learning and random sampling methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LOSTU-Fast-Scalable-and-Uncertainty-Aware-Triangulation"><a href="#LOSTU-Fast-Scalable-and-Uncertainty-Aware-Triangulation" class="headerlink" title="LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation"></a>LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11171">http://arxiv.org/abs/2311.11171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sébastien Henry, John A. Christian</li>
<li>for: 提供一种快速、可扩展、统计优化的三角分解方法（LOSTU），用于结构从运动（SfM）管道中的点云三角分解。</li>
<li>methods: 该方法基于最近的发现，并且不同于传统的$L_2$三角分解方法，可以考虑3D点云不确定性。</li>
<li>results: LOSTU可以减少3D重建错误，并且可以更快速于Levenberg-Marquardt优化方案。<details>
<summary>Abstract</summary>
Triangulation algorithms often aim to minimize the reprojection ($L_2$) error, but this only provides the maximum likelihood estimate when there are no errors in the camera parameters or camera poses. Although recent advancements have yielded techniques to estimate camera parameters accounting for 3D point uncertainties, most structure from motion (SfM) pipelines still use older triangulation algorithms. This work leverages recent discoveries to provide a fast, scalable, and statistically optimal way to triangulate called LOSTU. Results show that LOSTU consistently produces lower 3D reconstruction errors than conventional $L_2$ triangulation methods -- often allowing LOSTU to successfully triangulate more points. Moreover, in addition to providing a better 3D reconstruction, LOSTU can be substantially faster than Levenberg-Marquardt (or similar) optimization schemes.
</details>
<details>
<summary>摘要</summary>
通常，三角化算法的目标是减少($L_2$)  reprojection 错误，但这只提供了无摄像头参数或摄像头姿态错误时的最大可能性估计。尽管最近有一些进步，仍有许多结构从运动（SfM）管道使用老的三角化算法。这项工作利用最近的发现，提供一种快速、可扩展、统计优化的三角化方法，称为LOSTU。结果显示，LOSTU 常常生成较低的3D重建错误，并且可以更多的点 successfully triangulated。此外，LOSTU 还可以比 Levenberg-Marquardt（或类似）优化方案更快。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Feature-Extractors-for-Reinforcement-Learning-Based-Semiconductor-Defect-Localization"><a href="#Benchmarking-Feature-Extractors-for-Reinforcement-Learning-Based-Semiconductor-Defect-Localization" class="headerlink" title="Benchmarking Feature Extractors for Reinforcement Learning-Based Semiconductor Defect Localization"></a>Benchmarking Feature Extractors for Reinforcement Learning-Based Semiconductor Defect Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11145">http://arxiv.org/abs/2311.11145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enrique Dehaerne, Bappaditya Dey, Sandip Halder, Stefan De Gendt</li>
<li>for: 测试和检测半导体板件中的缺陷</li>
<li>methods: 使用深度强化学习（RL）方法进行缺陷定位，并评估不同的特征提取器的效果</li>
<li>results: 评估18个代理人在不同的特征提取器下进行定位缺陷的效果，并讨论RL基本框架在半导体缺陷定位中的优点和缺点。<details>
<summary>Abstract</summary>
As semiconductor patterning dimensions shrink, more advanced Scanning Electron Microscopy (SEM) image-based defect inspection techniques are needed. Recently, many Machine Learning (ML)-based approaches have been proposed for defect localization and have shown impressive results. These methods often rely on feature extraction from a full SEM image and possibly a number of regions of interest. In this study, we propose a deep Reinforcement Learning (RL)-based approach to defect localization which iteratively extracts features from increasingly smaller regions of the input image. We compare the results of 18 agents trained with different feature extractors. We discuss the advantages and disadvantages of different feature extractors as well as the RL-based framework in general for semiconductor defect localization.
</details>
<details>
<summary>摘要</summary>
As semiconductor patterning dimensions shrink, more advanced Scanning Electron Microscopy (SEM) image-based defect inspection techniques are needed. Recently, many Machine Learning (ML)-based approaches have been proposed for defect localization and have shown impressive results. These methods often rely on feature extraction from a full SEM image and possibly a number of regions of interest. In this study, we propose a deep Reinforcement Learning (RL)-based approach to defect localization which iteratively extracts features from increasingly smaller regions of the input image. We compare the results of 18 agents trained with different feature extractors. We discuss the advantages and disadvantages of different feature extractors as well as the RL-based framework in general for semiconductor defect localization.Here's the text in Traditional Chinese:为了应对半导体 Patterning 的缩小，需要更进一步的 Scanning Electron Microscopy (SEM) 图像基于抗错方法。最近，许多 Machine Learning (ML) 基于方法已经被提出来进行抗错定位，并且获得了优异的结果。这些方法通常将特征提取自全SEM图像以及可能的一些区域区域。在这一研究中，我们提出了一个深度强化学习 (RL) 基于的抗错定位方法，这个方法会逐步提取从输入图像中的特征，并且将其与RL网络进行结合。我们将训练 18 个代理人使用不同的特征提取器，并且比较它们的结果。我们会讨论不同的特征提取器优点和缺点，以及RL 基于架构的一般优点和缺点。
</details></li>
</ul>
<hr>
<h2 id="Estimating-Uncertainty-in-Landslide-Segmentation-Models"><a href="#Estimating-Uncertainty-in-Landslide-Segmentation-Models" class="headerlink" title="Estimating Uncertainty in Landslide Segmentation Models"></a>Estimating Uncertainty in Landslide Segmentation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11138">http://arxiv.org/abs/2311.11138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Savinay Nagendra, Chaopeng Shen, Daniel Kifer</li>
<li>for: 这个论文的目的是为了提供高质量、大规模的滥覆区域风险地区数据集，以便进行防范和减轻滥覆的准备和防控工作。</li>
<li>methods: 这篇论文使用了深度学习模型进行滥覆分割（像素标注），并评估了多种不需要建立新的架构的方法来评估像素级别的不确定性。</li>
<li>results: 实验结果表明，使用测试时数据拟合法（Test-Time Augmentation）方法可以在不同的模型和指标下提供最高质量的不确定性评估结果。<details>
<summary>Abstract</summary>
Landslides are a recurring, widespread hazard. Preparation and mitigation efforts can be aided by a high-quality, large-scale dataset that covers global at-risk areas. Such a dataset currently does not exist and is impossible to construct manually. Recent automated efforts focus on deep learning models for landslide segmentation (pixel labeling) from satellite imagery. However, it is also important to characterize the uncertainty or confidence levels of such segmentations. Accurate and robust uncertainty estimates can enable low-cost (in terms of manual labor) oversight of auto-generated landslide databases to resolve errors, identify hard negative examples, and increase the size of labeled training data. In this paper, we evaluate several methods for assessing pixel-level uncertainty of the segmentation. Three methods that do not require architectural changes were compared, including Pre-Threshold activations, Monte-Carlo Dropout and Test-Time Augmentation -- a method that measures the robustness of predictions in the face of data augmentation. Experimentally, the quality of the latter method was consistently higher than the others across a variety of models and metrics in our dataset.
</details>
<details>
<summary>摘要</summary>
landslide 是一种常 recurs 的、广泛存在的威胁。 prep 和 mitigation efforts 可以通过高质量、大规模的数据集来得到支持。目前没有这样的数据集，并且无法手动构建。 latest 自动化努力是使用深度学习模型进行滥舟分割（像素标注），但也重要是确定这些分割的不确定性或信任水平。准确和可靠的不确定性估计可以减少人工劳动成本，以便对自动生成的滥舟数据库进行低成本监督，解决错误、识别硬例外并增加标注训练数据的大小。在这篇论文中，我们评估了一些方法来评估像素级别的不确定性。我们比较了三种方法，包括 Pre-Threshold 活动、Monte-Carlo Dropout 和 Test-Time Augmentation。实验表明，后一种方法在不同的模型和指标上表现了最高的质量。
</details></li>
</ul>
<hr>
<h2 id="Invariant-based-Mapping-of-Space-During-General-Motion-of-an-Observer"><a href="#Invariant-based-Mapping-of-Space-During-General-Motion-of-an-Observer" class="headerlink" title="Invariant-based Mapping of Space During General Motion of an Observer"></a>Invariant-based Mapping of Space During General Motion of an Observer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11130">http://arxiv.org/abs/2311.11130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan D. Yepes, Daniel Raviv</li>
<li>for: 这个论文探讨了基于视觉运动的 invariants，导致一个新的即时领域，在这个领域中，站ARY environment被看作是不变的，即使图像在摄像机运动中不断改变，并且可以探测和避免特定子空间中的障碍物，以及检测运动 objetcs。</li>
<li>methods: 这个论文使用了非线性函数， derivated from measurable optical flow，这些函数与三维几何 invariants相连接。</li>
<li>results: 作者通过实验和实践，证明了这种方法可以在具有相机运动的情况下，保持图像的stationary environment不变，并且可以检测和分类运动 объекcs。<details>
<summary>Abstract</summary>
This paper explores visual motion-based invariants, resulting in a new instantaneous domain where: a) the stationary environment is perceived as unchanged, even as the 2D images undergo continuous changes due to camera motion, b) obstacles can be detected and potentially avoided in specific subspaces, and c) moving objects can potentially be detected. To achieve this, we make use of nonlinear functions derived from measurable optical flow, which are linked to geometric 3D invariants.   We present simulations involving a camera that translates and rotates relative to a 3D object, capturing snapshots of the camera projected images. We show that the object appears unchanged in the new domain over time. We process real data from the KITTI dataset and demonstrate how to segment space to identify free navigational regions and detect obstacles within a predetermined subspace. Additionally, we present preliminary results, based on the KITTI dataset, on the identification and segmentation of moving objects, as well as the visualization of shape constancy.   This representation is straightforward, relying on functions for the simple de-rotation of optical flow. This representation only requires a single camera, it is pixel-based, making it suitable for parallel processing, and it eliminates the necessity for 3D reconstruction techniques.
</details>
<details>
<summary>摘要</summary>
a) The stationary environment is perceived as unchanged, even as the 2D images undergo continuous changes due to camera motion.b) Obstacles can be detected and potentially avoided in specific subspaces.c) Moving objects can potentially be detected.To achieve this, we use nonlinear functions derived from measurable optical flow, which are linked to geometric 3D invariants. We present simulations involving a camera that translates and rotates relative to a 3D object, capturing snapshots of the camera-projected images. We show that the object appears unchanged in the new domain over time.We process real data from the KITTI dataset and demonstrate how to segment space to identify free navigational regions and detect obstacles within a predetermined subspace. Additionally, we present preliminary results, based on the KITTI dataset, on the identification and segmentation of moving objects, as well as the visualization of shape constancy.This representation is straightforward, relying on functions for the simple de-rotation of optical flow. This representation only requires a single camera, it is pixel-based, making it suitable for parallel processing, and it eliminates the necessity for 3D reconstruction techniques.
</details></li>
</ul>
<hr>
<h2 id="SecondPose-SE-3-Consistent-Dual-Stream-Feature-Fusion-for-Category-Level-Pose-Estimation"><a href="#SecondPose-SE-3-Consistent-Dual-Stream-Feature-Fusion-for-Category-Level-Pose-Estimation" class="headerlink" title="SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation"></a>SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11125">http://arxiv.org/abs/2311.11125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yamei Chen, Yan Di, Guangyao Zhai, Fabian Manhardt, Chenyangguang Zhang, Ruida Zhang, Federico Tombari, Nassir Navab, Benjamin Busam</li>
<li>for: 这篇研究目的是估计物体的6D姿势和3D大小，特别是面对着巨量内类形态的挑战。</li>
<li>methods: 本研究使用物体特有的几何特征，与DINOv2的semantic数据组合，实现了SE(3)-不变的几何特征抽出，并与DINOv2的特征进行对点对焦，实现了对焦对应的物体表现。</li>
<li>results: 实验结果显示，SecondPose在NOCS-REAL275上比前一代最高12.4%提高，并在更复杂的HouseCat6D上仍然超越其他竞争对手。<details>
<summary>Abstract</summary>
Category-level object pose estimation, aiming to predict the 6D pose and 3D size of objects from known categories, typically struggles with large intra-class shape variation. Existing works utilizing mean shapes often fall short of capturing this variation. To address this issue, we present SecondPose, a novel approach integrating object-specific geometric features with semantic category priors from DINOv2. Leveraging the advantage of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more complex dataset HouseCat6D which provides photometrically challenging objects, SecondPose still surpasses other competitors by a large margin. The code will be released soon.
</details>
<details>
<summary>摘要</summary>
Category-level object pose estimation aims to predict the 6D pose and 3D size of objects from known categories, but it often struggles with large intra-class shape variation. Existing methods using mean shapes often fail to capture this variation. To address this issue, we propose SecondPose, a novel approach that integrates object-specific geometric features with semantic category priors from DINOv2. By leveraging the advantages of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% improvement over the state-of-the-art. Moreover, on the more complex HouseCat6D dataset, which provides photometrically challenging objects, SecondPose still outperforms other competitors by a large margin. The code will be released soon.Here's the translation in Traditional Chinese:Category-level object pose estimation aims to predict the 6D pose and 3D size of objects from known categories, but it often struggles with large intra-class shape variation. Existing methods using mean shapes often fail to capture this variation. To address this issue, we propose SecondPose, a novel approach that integrates object-specific geometric features with semantic category priors from DINOv2. By leveraging the advantages of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% improvement over the state-of-the-art. Moreover, on the more complex HouseCat6D dataset, which provides photometrically challenging objects, SecondPose still outperforms other competitors by a large margin. The code will be released soon.
</details></li>
</ul>
<hr>
<h2 id="ShapeMaker-Self-Supervised-Joint-Shape-Canonicalization-Segmentation-Retrieval-and-Deformation"><a href="#ShapeMaker-Self-Supervised-Joint-Shape-Canonicalization-Segmentation-Retrieval-and-Deformation" class="headerlink" title="ShapeMaker: Self-Supervised Joint Shape Canonicalization, Segmentation, Retrieval and Deformation"></a>ShapeMaker: Self-Supervised Joint Shape Canonicalization, Segmentation, Retrieval and Deformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11106">http://arxiv.org/abs/2311.11106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Di, Chenyangguang Zhang, Chaowei Wang, Ruida Zhang, Guangyao Zhai, Yanyan Li, Bowen Fu, Xiangyang Ji, Shan Gao</li>
<li>for: 本文提出了一种自助学习框架ShapeMaker，用于同时进行形态均衡化、分割、检索和变换等四个高度相关的过程。</li>
<li>methods: 本文使用了一种独特的自助学习方法，同时具有分割、检索和变换等功能。具体来说，首先从 partially-observed 对象中提取出点级 affine-invariant 特征，然后利用这些特征预测semantically consistent的部分分 segmentation和对应的部分中心。接着，使用了一种轻量级的检索模块，将每个部分的特征集成为其检索 токен，然后与一个预设的数据库中的源形进行比较，以确定最接近的形状。最后，使用了一种基于部分中心的神经网络围栏变换模块，将检索到的形状与输入对象进行精准匹配。</li>
<li>results: 实验表明，ShapeMaker 在 Synthetic 数据集 PartNet、ComplementMe 和实际数据集 Scan2CAD 上表现出色，与竞争者相比，具有显著的优势。<details>
<summary>Abstract</summary>
In this paper, we present ShapeMaker, a unified self-supervised learning framework for joint shape canonicalization, segmentation, retrieval and deformation. Given a partially-observed object in an arbitrary pose, we first canonicalize the object by extracting point-wise affine-invariant features, disentangling inherent structure of the object with its pose and size. These learned features are then leveraged to predict semantically consistent part segmentation and corresponding part centers. Next, our lightweight retrieval module aggregates the features within each part as its retrieval token and compare all the tokens with source shapes from a pre-established database to identify the most geometrically similar shape. Finally, we deform the retrieved shape in the deformation module to tightly fit the input object by harnessing part center guided neural cage deformation. The key insight of ShapeMaker is the simultaneous training of the four highly-associated processes: canonicalization, segmentation, retrieval, and deformation, leveraging cross-task consistency losses for mutual supervision. Extensive experiments on synthetic datasets PartNet, ComplementMe, and real-world dataset Scan2CAD demonstrate that ShapeMaker surpasses competitors by a large margin. Codes will be released soon.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了ShapeMaker，一个独立学习框架，用于同时进行形态均衡化、分割、检索和变换。给定一个部分可见的物体，我们首先使用点精度不变的特征提取方法，提取物体的内在结构，并与姿态和大小相关。这些学习的特征然后用于预测相同分割和相应的中心点。接着，我们的轻量级检索模块将每个分割的特征作为检索token进行聚合，并将所有token与源形状库中的形状进行比较，以确定最接近的形状。最后，我们使用中心导航神经网络扭曲模块将检索到的形状与输入物体进行紧密匹配。ShapeMaker的关键思想是同时培养四个高度相关的过程：均衡化、分割、检索和变换，通过交叉任务一致损失来互相超级视图。我们在PartNet、ComplementMe和Scan2CAD等 sintetic数据集上进行了广泛的实验，结果显示ShapeMaker在与竞争对手进行比较时，具有很大的优势。代码即将发布。
</details></li>
</ul>
<hr>
<h2 id="On-the-Out-of-Distribution-Robustness-of-Foundation-Models-in-Medical-Image-Segmentation"><a href="#On-the-Out-of-Distribution-Robustness-of-Foundation-Models-in-Medical-Image-Segmentation" class="headerlink" title="On the Out of Distribution Robustness of Foundation Models in Medical Image Segmentation"></a>On the Out of Distribution Robustness of Foundation Models in Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11096">http://arxiv.org/abs/2311.11096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duy Minh Ho Nguyen, Tan Ngoc Pham, Nghiem Tuong Diep, Nghi Quoc Phan, Quang Pham, Vinh Tong, Binh T. Nguyen, Ngan Hoang Le, Nhat Ho, Pengtao Xie, Daniel Sonntag, Mathias Niepert</li>
<li>for: 本研究旨在探讨基础模型在医学图像分割任务中的鲁棒性，以便更好地适应不同的分布转移。</li>
<li>methods: 我们使用了各种基础模型，包括ViT和DeiT，并对其进行了微调。</li>
<li>results: 我们的实验结果表明，基础模型在不同的频率上具有更高的鲁棒性，并且我们还发展了一种新的 bayesian 不确定性估计方法，可以用于评估模型在不同数据集上的性能。<details>
<summary>Abstract</summary>
Constructing a robust model that can effectively generalize to test samples under distribution shifts remains a significant challenge in the field of medical imaging. The foundational models for vision and language, pre-trained on extensive sets of natural image and text data, have emerged as a promising approach. It showcases impressive learning abilities across different tasks with the need for only a limited amount of annotated samples. While numerous techniques have focused on developing better fine-tuning strategies to adapt these models for specific domains, we instead examine their robustness to domain shifts in the medical image segmentation task. To this end, we compare the generalization performance to unseen domains of various pre-trained models after being fine-tuned on the same in-distribution dataset and show that foundation-based models enjoy better robustness than other architectures. From here, we further developed a new Bayesian uncertainty estimation for frozen models and used them as an indicator to characterize the model's performance on out-of-distribution (OOD) data, proving particularly beneficial for real-world applications. Our experiments not only reveal the limitations of current indicators like accuracy on the line or agreement on the line commonly used in natural image applications but also emphasize the promise of the introduced Bayesian uncertainty. Specifically, lower uncertainty predictions usually tend to higher out-of-distribution (OOD) performance.
</details>
<details>
<summary>摘要</summary>
建立一个坚固的模型，使其能够有效地泛化到测试样本下的分布变化，是领域医学影像处理中的一大挑战。基础模型，在大量自然图像和文本数据上进行预训练，已经出现为一种有前途的方法。它在不同任务上展示出了卓越的学习能力，只需要有限量的标注样本。虽然许多技术专注于开发更好的细化策略，以适应特定领域，但我们则研究基础模型在医学图像分割任务中的Robustness。为此，我们比较了不同预训练模型在未看到的领域中的泛化性能，并发现基础模型在这个方面表现出了更好的Robustness。此外，我们还开发了一种新的 bayesian uncertainty estimation 方法，并用其作为指标，来评估模型在未分布（OOD）数据上的性能。我们的实验结果不仅揭示了现有的指标，如精度在线或者线上协调率在自然图像应用中的局限性，而且还强调了我们引入的 bayesian uncertainty 的承诺。具体来说， Lower uncertainty predictions 通常与更高的OOD性能相关。
</details></li>
</ul>
<hr>
<h2 id="LightBTSeg-A-lightweight-breast-tumor-segmentation-model-using-ultrasound-images-via-dual-path-joint-knowledge-distillation"><a href="#LightBTSeg-A-lightweight-breast-tumor-segmentation-model-using-ultrasound-images-via-dual-path-joint-knowledge-distillation" class="headerlink" title="LightBTSeg: A lightweight breast tumor segmentation model using ultrasound images via dual-path joint knowledge distillation"></a>LightBTSeg: A lightweight breast tumor segmentation model using ultrasound images via dual-path joint knowledge distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11086">http://arxiv.org/abs/2311.11086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongjiang Guo, Shengwen Wang, Hao Dang, Kangle Xiao, Yaru Yang, Wenpei Liu, Tongtong Liu, Yiying Wan</li>
<li>for: 这个研究的目的是为了提高乳腺癌检测的精确性，以提高乳腺癌的早期诊断和治疗。</li>
<li>methods: 这个研究使用了一种名为LightBTSeg的双路共同知识传授框架，它利用了双教师模型来表达乳腺癌的细部特征。</li>
<li>results: 实验结果显示，LightBTSeg在乳腺癌检测中表现出色，比其他Counterparts更高精确。<details>
<summary>Abstract</summary>
The accurate segmentation of breast tumors is an important prerequisite for lesion detection, which has significant clinical value for breast tumor research. The mainstream deep learning-based methods have achieved a breakthrough. However, these high-performance segmentation methods are formidable to implement in clinical scenarios since they always embrace high computation complexity, massive parameters, slow inference speed, and huge memory consumption. To tackle this problem, we propose LightBTSeg, a dual-path joint knowledge distillation framework, for lightweight breast tumor segmentation. Concretely, we design a double-teacher model to represent the fine-grained feature of breast ultrasound according to different semantic feature realignments of benign and malignant breast tumors. Specifically, we leverage the bottleneck architecture to reconstruct the original Attention U-Net. It is regarded as a lightweight student model named Simplified U-Net. Then, the prior knowledge of benign and malignant categories is utilized to design the teacher network combined dual-path joint knowledge distillation, which distills the knowledge from cumbersome benign and malignant teachers to a lightweight student model. Extensive experiments conducted on breast ultrasound images (Dataset BUSI) and Breast Ultrasound Dataset B (Dataset B) datasets demonstrate that LightBTSeg outperforms various counterparts.
</details>
<details>
<summary>摘要</summary>
importante para la detección de lesiones en el diagnóstico de tumores mamarios. Los métodos basados en aprendizaje profundo mainstream han logrado un avance significativo. Sin embargo, estos métodos de alta performance son difíciles de implementar en escenarios clínicos debido a su complejidad computacional alta, parámetros masivos, velocidad de inferencia lenta y consumo de memoria grande. Para abordar este problema, propusimos LightBTSeg, un marco de distilación de conocimiento dual-path, para segmentación de tumores mamarios livianos.En detalle, diseñamos un modelo doble-maestro para representar la característica fine-grained de la ecografía de mama según diferentes realineaciones semánticas de tumores benignos y malignos. Utilizamos la arquitectura de bottleneck para reconstruir la red Attention U-Net original. Se considera un modelo estudiantil liviano llamado Simplified U-Net. Luego, utilizamos la información previa de categorías benignas y malignas para diseñar la red maestra combinada con distilación de conocimiento dual-path, que transmite el conocimiento de los maestros pesados benignos y malignos a un modelo estudiantil liviano.Los experimentos extensivos realizados en las imágenes de ecografía de mama (dataset BUSI) y el dataset Breast Ultrasound Dataset B (dataset B) demostraron que LightBTSeg supera a sus pares.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Transformer-Based-Segmentation-for-Breast-Cancer-Diagnosis-using-Auto-Augmentation-and-Search-Optimisation-Techniques"><a href="#Enhancing-Transformer-Based-Segmentation-for-Breast-Cancer-Diagnosis-using-Auto-Augmentation-and-Search-Optimisation-Techniques" class="headerlink" title="Enhancing Transformer-Based Segmentation for Breast Cancer Diagnosis using Auto-Augmentation and Search Optimisation Techniques"></a>Enhancing Transformer-Based Segmentation for Breast Cancer Diagnosis using Auto-Augmentation and Search Optimisation Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11065">http://arxiv.org/abs/2311.11065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leon Hamnett, Mary Adewunmi, Modinat Abayomi, Kayode Raheem, Fahad Ahmed<br>for:This paper aims to improve the accuracy and robustness of breast cancer cell segmentation in histology slides using automated image augmentation selection and search optimization strategies.methods:The proposed methodology combines RandAugment with Tree-based Parzen Estimator to identify optimal values for image augmentations and their associated parameters, leading to enhanced segmentation performance.results:The proposed methodology leads to segmentation models that are more resilient to variations in histology slides while maintaining high levels of segmentation performance, with improved segmentation of the tumour class compared to previous research. The best result after applying the augmentations is a Dice Score of 84.08 and an IoU score of 72.54 when segmenting the tumour class.<details>
<summary>Abstract</summary>
Breast cancer remains a critical global health challenge, necessitating early and accurate detection for effective treatment. This paper introduces a methodology that combines automated image augmentation selection (RandAugment) with search optimisation strategies (Tree-based Parzen Estimator) to identify optimal values for the number of image augmentations and the magnitude of their associated augmentation parameters, leading to enhanced segmentation performance. We empirically validate our approach on breast cancer histology slides, focusing on the segmentation of cancer cells. A comparative analysis of state-of-the-art transformer-based segmentation models is conducted, including SegFormer, PoolFormer, and MaskFormer models, to establish a comprehensive baseline, before applying the augmentation methodology. Our results show that the proposed methodology leads to segmentation models that are more resilient to variations in histology slides whilst maintaining high levels of segmentation performance, and show improved segmentation of the tumour class when compared to previous research. Our best result after applying the augmentations is a Dice Score of 84.08 and an IoU score of 72.54 when segmenting the tumour class. The primary contribution of this paper is the development of a methodology that enhances segmentation performance while ensuring model robustness to data variances. This has significant implications for medical practitioners, enabling the development of more effective machine learning models for clinical applications to identify breast cancer cells from histology slides. Furthermore, the codebase accompanying this research will be released upon publication. This will facilitate further research and application development based on our methodology, thereby amplifying its impact.
</details>
<details>
<summary>摘要</summary>
乳癌仍然是全球健康挑战之一，需要早期精准的检测以实现有效的治疗。本文介绍一种方法，将自动图像增强选择（RandAugment）与搜索优化策略（Tree-based Parzen Estimator）结合，以确定图像增强数量和相关增强参数的优化值，以提高分 segmentation性能。我们对乳癌 histology 胶卷进行了实验，专注于癌细胞分 segmentation。我们进行了现有 transformer 基本模型的比较分析，包括 SegFormer、PoolFormer 和 MaskFormer 模型，以建立全面的基准。我们的结果表明，提posed方法可以提高模型对数据变化的抗性，保持高水平的分 segmentation性能，并在识别癌细胞方面显示了改进的分 segmentation性能。我们的最佳结果是 Dice 分数84.08和 IoU 分数72.54，分 segmentation癌细胞类。本文的主要贡献是开发了一种能够提高分 segmentation性能的同时保持模型对数据变化的抗性的方法，这有着重要的医疗应用。此外，本文的代码库将在出版时发布，以便进一步的研究和应用开发，从而增强其影响。
</details></li>
</ul>
<hr>
<h2 id="HIDRO-VQA-High-Dynamic-Range-Oracle-for-Video-Quality-Assessment"><a href="#HIDRO-VQA-High-Dynamic-Range-Oracle-for-Video-Quality-Assessment" class="headerlink" title="HIDRO-VQA: High Dynamic Range Oracle for Video Quality Assessment"></a>HIDRO-VQA: High Dynamic Range Oracle for Video Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11059">http://arxiv.org/abs/2311.11059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreshth Saini, Avinab Saha, Alan C. Bovik</li>
<li>for: 这篇论文旨在提供高动态范围视频质量评估（VQA）模型，用于提供高精度的HDR视频质量评估。</li>
<li>methods: 该模型使用自动生成的相似性推训策略，将SDR视频中的质量感知特征转移到HDR视频中，无需标注。</li>
<li>results: 研究发现，通过自动生成的相似性推训策略，可以将SDR视频中的质量感知特征转移到HDR视频中，并在LIVE-HDR VQA数据库上达到了状态之最好的性能。<details>
<summary>Abstract</summary>
We introduce HIDRO-VQA, a no-reference (NR) video quality assessment model designed to provide precise quality evaluations of High Dynamic Range (HDR) videos. HDR videos exhibit a broader spectrum of luminance, detail, and color than Standard Dynamic Range (SDR) videos. As HDR content becomes increasingly popular, there is a growing demand for video quality assessment (VQA) algorithms that effectively address distortions unique to HDR content. To address this challenge, we propose a self-supervised contrastive fine-tuning approach to transfer quality-aware features from the SDR to the HDR domain, utilizing unlabeled HDR videos. Our findings demonstrate that self-supervised pre-trained neural networks on SDR content can be further fine-tuned in a self-supervised setting using limited unlabeled HDR videos to achieve state-of-the-art performance on the only publicly available VQA database for HDR content, the LIVE-HDR VQA database. Moreover, our algorithm can be extended to the Full Reference VQA setting, also achieving state-of-the-art performance. Our code is available publicly at https://github.com/avinabsaha/HIDRO-VQA.
</details>
<details>
<summary>摘要</summary>
我们介绍HIDRO-VQA，一个不受参考（NR）影像质量评估模型，旨在为高动态范围（HDR）影像提供精确的质量评估。HDR影像比标准动态范围（SDR）影像更具宽频谱、细节和颜色，随着HDR内容的普及，需要一种能有效地处理HDR内容的质量评估算法。为解决这个挑战，我们提议一种基于自我超级vised contrastive fine-tuning的方法，将SDR频谱中的质量感知特征转移到HDR频谱中，使用有限的无标注HDR影像进行自我超级vised fine-tuning。我们的发现表明，可以在自我超级vised Setting中使用SDR内容的自我超级vised预训练网络，通过有限的无标注HDR影像进行自我超级vised fine-tuning，以达到LIVE-HDR VQA数据库中的最新纪录。此外，我们的算法还可以扩展到全参考VQA Setting，也达到了最新纪录。我们的代码可以在https://github.com/avinabsaha/HIDRO-VQA上获得。
</details></li>
</ul>
<hr>
<h2 id="Hyperbolic-Space-with-Hierarchical-Margin-Boosts-Fine-Grained-Learning-from-Coarse-Labels"><a href="#Hyperbolic-Space-with-Hierarchical-Margin-Boosts-Fine-Grained-Learning-from-Coarse-Labels" class="headerlink" title="Hyperbolic Space with Hierarchical Margin Boosts Fine-Grained Learning from Coarse Labels"></a>Hyperbolic Space with Hierarchical Margin Boosts Fine-Grained Learning from Coarse Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11019">http://arxiv.org/abs/2311.11019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shu-Lin Xu, Yifan Sun, Faen Zhang, Anqi Xu, Xiu-Shen Wei, Yi Yang</li>
<li>for: 这篇论文的目的是提出一种新的方法，用于从粗略标签中学习细化嵌入。</li>
<li>methods: 该方法使用了一种新的嵌入方法，将视觉嵌入映射到一个希пербо利空间中，并在这个空间中应用一种层次cosine margin方式来增强嵌入的推理能力。</li>
<li>results: 经过广泛的实验，该方法在五个 benchmark 数据集上达到了最佳效果，超过了竞争方法的表现。<details>
<summary>Abstract</summary>
Learning fine-grained embeddings from coarse labels is a challenging task due to limited label granularity supervision, i.e., lacking the detailed distinctions required for fine-grained tasks. The task becomes even more demanding when attempting few-shot fine-grained recognition, which holds practical significance in various applications. To address these challenges, we propose a novel method that embeds visual embeddings into a hyperbolic space and enhances their discriminative ability with a hierarchical cosine margins manner. Specifically, the hyperbolic space offers distinct advantages, including the ability to capture hierarchical relationships and increased expressive power, which favors modeling fine-grained objects. Based on the hyperbolic space, we further enforce relatively large/small similarity margins between coarse/fine classes, respectively, yielding the so-called hierarchical cosine margins manner. While enforcing similarity margins in the regular Euclidean space has become popular for deep embedding learning, applying it to the hyperbolic space is non-trivial and validating the benefit for coarse-to-fine generalization is valuable. Extensive experiments conducted on five benchmark datasets showcase the effectiveness of our proposed method, yielding state-of-the-art results surpassing competing methods.
</details>
<details>
<summary>摘要</summary>
学习细腻嵌入从粗略标签的挑战 task  Due to limited label granularity supervision, i.e., lacking the detailed distinctions required for fine-grained tasks. The task becomes even more demanding when attempting few-shot fine-grained recognition, which holds practical significance in various applications. To address these challenges, we propose a novel method that embeds visual embeddings into a hyperbolic space and enhances their discriminative ability with a hierarchical cosine margins manner. Specifically, the hyperbolic space offers distinct advantages, including the ability to capture hierarchical relationships and increased expressive power, which favors modeling fine-grained objects. Based on the hyperbolic space, we further enforce relatively large/small similarity margins between coarse/fine classes, respectively, yielding the so-called hierarchical cosine margins manner. While enforcing similarity margins in the regular Euclidean space has become popular for deep embedding learning, applying it to the hyperbolic space is non-trivial and validating the benefit for coarse-to-fine generalization is valuable. Extensive experiments conducted on five benchmark datasets showcase the effectiveness of our proposed method, yielding state-of-the-art results surpassing competing methods.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Improving-Adversarial-Transferability-by-Stable-Diffusion"><a href="#Improving-Adversarial-Transferability-by-Stable-Diffusion" class="headerlink" title="Improving Adversarial Transferability by Stable Diffusion"></a>Improving Adversarial Transferability by Stable Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11017">http://arxiv.org/abs/2311.11017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayang Liu, Siyu Zhu, Siyuan Liang, Jie Zhang, Han Fang, Weiming Zhang, Ee-Chien Chang</li>
<li>For: The paper is written to explore the potential of leveraging data generated by Stable Diffusion to boost adversarial transferability in the black-box scenario.* Methods: The paper introduces a novel attack method called Stable Diffusion Attack Method (SDAM), which incorporates samples generated by Stable Diffusion to augment input images. Additionally, the paper proposes a fast variant of SDAM to reduce computational overhead while preserving high adversarial transferability.* Results: The paper demonstrates that the proposed method outperforms state-of-the-art baselines by a substantial margin. The approach is also compatible with existing transfer-based attacks to further enhance adversarial transferability.<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are susceptible to adversarial examples, which introduce imperceptible perturbations to benign samples, deceiving DNN predictions. While some attack methods excel in the white-box setting, they often struggle in the black-box scenario, particularly against models fortified with defense mechanisms. Various techniques have emerged to enhance the transferability of adversarial attacks for the black-box scenario. Among these, input transformation-based attacks have demonstrated their effectiveness. In this paper, we explore the potential of leveraging data generated by Stable Diffusion to boost adversarial transferability. This approach draws inspiration from recent research that harnessed synthetic data generated by Stable Diffusion to enhance model generalization. In particular, previous work has highlighted the correlation between the presence of both real and synthetic data and improved model generalization. Building upon this insight, we introduce a novel attack method called Stable Diffusion Attack Method (SDAM), which incorporates samples generated by Stable Diffusion to augment input images. Furthermore, we propose a fast variant of SDAM to reduce computational overhead while preserving high adversarial transferability. Our extensive experimental results demonstrate that our method outperforms state-of-the-art baselines by a substantial margin. Moreover, our approach is compatible with existing transfer-based attacks to further enhance adversarial transferability.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:深度神经网络 (DNN) 容易受到攻击，这些攻击引入不可见的杂音，误导 DNN 预测。虽然一些攻击方法在白盒设置下表现出色，但在黑盒设置下，它们经常遇到困难，特别是面临了防御机制。不同的技术已经出现，以增强黑盒攻击的传输性。 Among them, input transformation-based attacks have shown their effectiveness. 在这篇文章中，我们探索了使用 Stable Diffusion 生成的数据来提高攻击的传输性。这种方法启发于最近的研究，通过 Stable Diffusion 生成的数据来提高模型通用性。以前的研究已经表明，当存在真实数据和生成数据时，模型的通用性会得到改善。基于这一点，我们提出了一种新的攻击方法，即 Stable Diffusion Attack Method (SDAM)，它利用 Stable Diffusion 生成的样本来补充输入图像。此外，我们还提出了一种快速的 SDAM variant，以降低计算开销而保持高的攻击传输性。我们的实验结果表明，我们的方法在比较之下大幅超越了状态当前的基准值。此外，我们的方法与现有的传输基于攻击相容，可以进一步提高攻击的传输性。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Event-RGBD-Neural-SLAM"><a href="#Implicit-Event-RGBD-Neural-SLAM" class="headerlink" title="Implicit Event-RGBD Neural SLAM"></a>Implicit Event-RGBD Neural SLAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11013">http://arxiv.org/abs/2311.11013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Delin Qu, Chi Yan, Dong Wang, Jie Yin, Dan Xu, Bin Zhao, Xuelong Li</li>
<li>for: 这种paper的目的是提出一种基于事件RGBD的启发式SLAM框架，以解决非理想enario中的问题，如运动模糊和灯光变化，从而提高tracking和mapping的精度和稳定性。</li>
<li>methods: 这种方法使用了可微分CRF渲染技术，通过共享辐射场来生成独特的RGB和事件摄像头数据，并通过学习一个统一的启发式表示来优化 captured事件和RGBD监视。此外，基于事件的时间差性，我们提出了一种时间聚合优化策略，使用事件的连续差异约束来提高跟踪准确性和稳定性。</li>
<li>results: 我们在6个场景、17个序列的实验中，证明了我们的方法可以在不同的挑战环境中高效地处理运动模糊和灯光变化，并且与现有最佳方法进行比较，在跟踪ATE和mappingACC中具有更高的精度和稳定性。<details>
<summary>Abstract</summary>
Implicit neural SLAM has achieved remarkable progress recently. Nevertheless, existing methods face significant challenges in non-ideal scenarios, such as motion blur or lighting variation, which often leads to issues like convergence failures, localization drifts, and distorted mapping. To address these challenges, we propose $\textbf{EN-SLAM}$, the first event-RGBD implicit neural SLAM framework, which effectively leverages the high rate and high dynamic range advantages of event data for tracking and mapping. Specifically, EN-SLAM proposes a differentiable CRF (Camera Response Function) rendering technique to generate distinct RGB and event camera data via a shared radiance field, which is optimized by learning a unified implicit representation with the captured event and RGBD supervision. Moreover, based on the temporal difference property of events, we propose a temporal aggregating optimization strategy for the event joint tracking and global bundle adjustment, capitalizing on the consecutive difference constraints of events, significantly enhancing tracking accuracy and robustness. Finally, we construct the simulated dataset $\textbf{DEV-Indoors}$ and real captured dataset $\textbf{DEV-Reals}$ containing 6 scenes, 17 sequences with practical motion blur and lighting changes for evaluations. Experimental results show that our method outperforms the SOTA methods in both tracking ATE and mapping ACC with a real-time $17$ FPS in various challenging environments. The code and dataset will be released upon the paper publication.
</details>
<details>
<summary>摘要</summary>
Recently, implicit neural SLAM has made significant progress. However, existing methods still face challenges in non-ideal scenarios, such as motion blur or lighting variation, which often leads to issues like convergence failures, localization drifts, and distorted mapping. To address these challenges, we propose $\textbf{EN-SLAM}$, the first event-RGBD implicit neural SLAM framework, which effectively leverages the high rate and high dynamic range advantages of event data for tracking and mapping. Specifically, EN-SLAM proposes a differentiable CRF (Camera Response Function) rendering technique to generate distinct RGB and event camera data via a shared radiance field, which is optimized by learning a unified implicit representation with the captured event and RGBD supervision. Moreover, based on the temporal difference property of events, we propose a temporal aggregating optimization strategy for the event joint tracking and global bundle adjustment, capitalizing on the consecutive difference constraints of events, significantly enhancing tracking accuracy and robustness. Finally, we construct the simulated dataset $\textbf{DEV-Indoors}$ and real captured dataset $\textbf{DEV-Reals}$ containing 6 scenes, 17 sequences with practical motion blur and lighting changes for evaluations. Experimental results show that our method outperforms the SOTA methods in both tracking ATE and mapping ACC with a real-time $17$ FPS in various challenging environments. The code and dataset will be released upon the paper publication.Here's the translation in Traditional Chinese:过去的几年，隐式神经 SLAM 已经取得了非常的进步。然而，现有的方法在非理想的enario中仍然面临着问题，例如运动模糊或照明变化，这经常导致整合失败、位置漂移和投影变形的问题。为了解决这些问题，我们提出了 $\textbf{EN-SLAM}$，第一个事件-RGBD 隐式神经 SLAM 框架。EN-SLAM 使用了可微的 CRF (Camera Response Function) 渲染技术生成了不同的 RGB 和事件摄像机数据，并且透过学习一个统一的隐式表现来对于捕捉的事件和 RGBD 进行超参。此外，基于事件的时间差异性，我们提出了一个时间聚合优化策略，具体来说是在事件统一追踪和全局统一调整中，运用了 consecutive difference 的条件来增强追踪精度和Robustness。最后，我们建立了 $\textbf{DEV-Indoors}$ 和 $\textbf{DEV-Reals}$ 两个实验 dataset，包括 6 个scene，17 个序列，实际上具有了实验模糊和照明变化。实验结果显示，我们的方法在追踪 ATE 和投影 ACC 方面具有了 SOTA 的表现，并且在多种挑战性环境中实现了实时 $17$ FPS。代码和dataset 将在论文发表时释出。
</details></li>
</ul>
<hr>
<h2 id="Learning-Scene-Context-Without-Images"><a href="#Learning-Scene-Context-Without-Images" class="headerlink" title="Learning Scene Context Without Images"></a>Learning Scene Context Without Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10998">http://arxiv.org/abs/2311.10998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirreza Rouhi, David Han</li>
<li>for: 教学机器人场景知识，以便它们更有效地与环境交互，预测或预测不可见在视觉场景中的对象。</li>
<li>methods: 提出了一种基于 transformer 的新方法 $LMOD$（标签基于缺失对象检测），通过注意机制教会机器人场景知识。该方法不需要实际图像，只需要图像集标签。</li>
<li>results: 研究表明，通过基于标签学习场景关系，可以通过自注意机制学习场景知识，并且该知识可以提高其他视觉基于对象检测算法的性能。<details>
<summary>Abstract</summary>
Teaching machines of scene contextual knowledge would enable them to interact more effectively with the environment and to anticipate or predict objects that may not be immediately apparent in their perceptual field. In this paper, we introduce a novel transformer-based approach called $LMOD$ ( Label-based Missing Object Detection) to teach scene contextual knowledge to machines using an attention mechanism. A distinctive aspect of the proposed approach is its reliance solely on labels from image datasets to teach scene context, entirely eliminating the need for the actual image itself. We show how scene-wide relationships among different objects can be learned using a self-attention mechanism. We further show that the contextual knowledge gained from label based learning can enhance performance of other visual based object detection algorithm.
</details>
<details>
<summary>摘要</summary>
教机器人场景知识会使其更有效地与环境交互，预测或预测未在视觉范围内出现的对象。在这篇论文中，我们介绍了一种新的变换器基本方法，称为$LMOD$（标签基本缺失检测），用于教机器人场景知识。这种方法异常之处在于它完全不需要图像本身，只需要图像的标签。我们示示了如何使用自我注意机制来学习场景中对象之间的关系。我们进一步示示了通过标签学习获得的Contextual知识可以提高其他视觉基于对象检测算法的性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-and-Accurate-Visual-Prompting"><a href="#Towards-Robust-and-Accurate-Visual-Prompting" class="headerlink" title="Towards Robust and Accurate Visual Prompting"></a>Towards Robust and Accurate Visual Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10992">http://arxiv.org/abs/2311.10992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Li, Liangzhi Li, Zhouqiang Jiang, Bowen Wang</li>
<li>for: 本文研究了使用Visual Prompting（VP）在视觉任务中，并解释了robust模型下的VP表现是否会受到数据集的影响。</li>
<li>methods: 本文使用了一种新的技术名为Prompt Boundary Loose（PBL），以提高标准精度下的视觉提示表现，而不会失去对抗 robustness。</li>
<li>results: 广泛的实验结果表明，我们的方法可以在不同的数据集上提高标准精度和对抗 robustness。<details>
<summary>Abstract</summary>
Visual prompting, an efficient method for transfer learning, has shown its potential in vision tasks. However, previous works focus exclusively on VP from standard source models, it is still unknown how it performs under the scenario of a robust source model: Whether a visual prompt derived from a robust model can inherit the robustness while suffering from the generalization performance decline, albeit for a downstream dataset that is different from the source dataset? In this work, we get an affirmative answer of the above question and give an explanation on the visual representation level. Moreover, we introduce a novel technique named Prompt Boundary Loose (PBL) to effectively mitigates the suboptimal results of visual prompt on standard accuracy without losing (or even significantly improving) its adversarial robustness when using a robust model as source model. Extensive experiments across various datasets show that our findings are universal and demonstrate the significant benefits of our proposed method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Visual 提示，一种高效的转移学习方法，在视觉任务中表现出了潜在的潜力。然而，先前的工作都是基于标准源模型进行了研究，而不是知道robust模型下的Visual 提示是否能够继承鲁棒性，而在不同的下游数据集上受到较大的泛化性下降？在这个研究中，我们得到了上述问题的积极答案，并对Visual 提示的视觉表示进行了解释。此外，我们还提出了一种名为Prompt Boundary Loose（PBL）的新技术，可以有效地 mitigate 视觉提示在标准准确性下的不佳结果，而不失去（或甚至进一步提高）对 robust 模型的鲁棒性。通过对多个数据集进行了广泛的实验，我们的发现表明了universal的特点，并demonstrated  Visual 提示的重要性和PBL的有效性。Note: "robust" in Chinese is "鲁棒" (lùbù), and "standard" is "标准" (biāozhāng).
</details></li>
</ul>
<hr>
<h2 id="Expanding-Scene-Graph-Boundaries-Fully-Open-vocabulary-Scene-Graph-Generation-via-Visual-Concept-Alignment-and-Retention"><a href="#Expanding-Scene-Graph-Boundaries-Fully-Open-vocabulary-Scene-Graph-Generation-via-Visual-Concept-Alignment-and-Retention" class="headerlink" title="Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph Generation via Visual-Concept Alignment and Retention"></a>Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph Generation via Visual-Concept Alignment and Retention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10988">http://arxiv.org/abs/2311.10988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zuyao Chen, Jinlin Wu, Zhen Lei, Zhaoxiang Zhang, Changwen Chen</li>
<li>for: 提供一种结构化表示方法，用于许多计算机视觉应用程序中。</li>
<li>methods: 基于变换器架构，学习视觉概念对应关系，以扩展已知对象和关系类别。</li>
<li>results: 在视觉 génome 测试benchmark上，提出一种全开 vocabulary SGG方法，实现了不确定对象和关系类别的承认。<details>
<summary>Abstract</summary>
Scene Graph Generation (SGG) offers a structured representation critical in many computer vision applications. Traditional SGG approaches, however, are limited by a closed-set assumption, restricting their ability to recognize only predefined object and relation categories. To overcome this, we categorize SGG scenarios into four distinct settings based on the node and edge: Closed-set SGG, Open Vocabulary (object) Detection-based SGG (OvD-SGG), Open Vocabulary Relation-based SGG (OvR-SGG), and Open Vocabulary Detection + Relation-based SGG (OvD+R-SGG). While object-centric open vocabulary SGG has been studied recently, the more challenging problem of relation-involved open-vocabulary SGG remains relatively unexplored. To fill this gap, we propose a unified framework named OvSGTR towards fully open vocabulary SGG from a holistic view. The proposed framework is an end-toend transformer architecture, which learns a visual-concept alignment for both nodes and edges, enabling the model to recognize unseen categories. For the more challenging settings of relation-involved open vocabulary SGG, the proposed approach integrates relation-aware pre-training utilizing image-caption data and retains visual-concept alignment through knowledge distillation. Comprehensive experimental results on the Visual Genome benchmark demonstrate the effectiveness and superiority of the proposed framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Make-Pixels-Dance-High-Dynamic-Video-Generation"><a href="#Make-Pixels-Dance-High-Dynamic-Video-Generation" class="headerlink" title="Make Pixels Dance: High-Dynamic Video Generation"></a>Make Pixels Dance: High-Dynamic Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10982">http://arxiv.org/abs/2311.10982</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/makepixelsdance/makepixelsdance.github.io">https://github.com/makepixelsdance/makepixelsdance.github.io</a></li>
<li>paper_authors: Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, Hang Li</li>
<li>for: 本研究旨在提高人工智能中的视频生成技术，特别是生成具有复杂场景和细腻动作的动画视频。</li>
<li>methods: 本研究提出了一种基于扩散模型的新方法，称为PixelDance，该方法在视频生成过程中结合文本指令和图像指令。</li>
<li>results: 经验表明，PixelDance在使用公共数据进行训练后，能够生成具有复杂场景和细腻动作的视频，并设置了新的标准 для视频生成。<details>
<summary>Abstract</summary>
Creating high-dynamic videos such as motion-rich actions and sophisticated visual effects poses a significant challenge in the field of artificial intelligence. Unfortunately, current state-of-the-art video generation methods, primarily focusing on text-to-video generation, tend to produce video clips with minimal motions despite maintaining high fidelity. We argue that relying solely on text instructions is insufficient and suboptimal for video generation. In this paper, we introduce PixelDance, a novel approach based on diffusion models that incorporates image instructions for both the first and last frames in conjunction with text instructions for video generation. Comprehensive experimental results demonstrate that PixelDance trained with public data exhibits significantly better proficiency in synthesizing videos with complex scenes and intricate motions, setting a new standard for video generation.
</details>
<details>
<summary>摘要</summary>
创造高动态视频，如有很多动作和复杂视觉效果，是人工智能领域的一大挑战。现有的现场最佳实践，主要集中在文本到视频生成，往往会生成视频剪辑件的动作较少，即使保持高精度。我们认为，仅仅根据文本指令是不够和不优化的 для视频生成。在这篇论文中，我们介绍PixelDance，一种基于扩散模型的新方法，将文本指令和图像指令结合使用，用于视频生成。我们的实验结果表明，PixelDance通过使用公共数据进行训练，可以生成视频中的复杂场景和细腻动作，创造出新的标准 для视频生成。
</details></li>
</ul>
<hr>
<h2 id="Structure-Aware-Sparse-View-X-ray-3D-Reconstruction"><a href="#Structure-Aware-Sparse-View-X-ray-3D-Reconstruction" class="headerlink" title="Structure-Aware Sparse-View X-ray 3D Reconstruction"></a>Structure-Aware Sparse-View X-ray 3D Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10959">http://arxiv.org/abs/2311.10959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Cai, Jiahao Wang, Alan Yuille, Zongwei Zhou, Angtian Wang</li>
<li>for: 提高 sparse-view X-ray 3D 重建的精度和效率</li>
<li>methods: 使用 Line Segment-based Transformer (Lineformer) 和 Masked Local-Global (MLG) 照样策略</li>
<li>results: 在 X3D  dataset 上，SAX-NeRF 比前一代 NeRF-based 方法提高 12.56 和 2.49 dB 在新视图合成和 CT 重建中<details>
<summary>Abstract</summary>
X-ray, known for its ability to reveal internal structures of objects, is expected to provide richer information for 3D reconstruction than visible light. Yet, existing neural radiance fields (NeRF) algorithms overlook this important nature of X-ray, leading to their limitations in capturing structural contents of imaged objects. In this paper, we propose a framework, Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view X-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer (Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal structures of objects in 3D space by modeling the dependencies within each line segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray sampling strategy to extract contextual and geometric information in 2D projection. Plus, we collect a larger-scale dataset X3D covering wider X-ray applications. Experiments on X3D show that SAX-NeRF surpasses previous NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT reconstruction. Code, models, and data will be released at https://github.com/caiyuanhao1998/SAX-NeRF
</details>
<details>
<summary>摘要</summary>
X射线，知名于对物体内部结构的显示，对于3D重建提供更加丰富的信息。然而，现有的神经辐射场（NeRF）算法忽略了这一重要特点，导致它们在捕捉图像对象结构的能力有限。在这篇论文中，我们提出了一个框架，即结构意识X射线神经辐射场（SAX-NeRF），用于稀疏视角X射线3D重建。首先，我们设计了一种基于线段的转换器（Lineformer）作为SAX-NeRF的核心。Lineformer可以在3D空间中捕捉物体的内部结构，并且通过模型每个线段之间的依赖关系来捕捉物体的3D结构。其次，我们提出了一种面积掩码本地全球（MLG）照明策略，以EXTRACTContextual和Geometric信息在2D投影中。此外，我们收集了更加广泛的X3D数据集，覆盖更多的X射线应用场景。实验表明，SAX-NeRF在X3D数据集上超过了之前的NeRF基于方法， Novel View Synthesis和CT重建方面的表现提高12.56和2.49 dB。代码、模型和数据将在https://github.com/caiyuanhao1998/SAX-NeRF上发布。
</details></li>
</ul>
<hr>
<h2 id="NAS-ASDet-An-Adaptive-Design-Method-for-Surface-Defect-Detection-Network-using-Neural-Architecture-Search"><a href="#NAS-ASDet-An-Adaptive-Design-Method-for-Surface-Defect-Detection-Network-using-Neural-Architecture-Search" class="headerlink" title="NAS-ASDet: An Adaptive Design Method for Surface Defect Detection Network using Neural Architecture Search"></a>NAS-ASDet: An Adaptive Design Method for Surface Defect Detection Network using Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10952">http://arxiv.org/abs/2311.10952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenrong Wang, Bin Li, Weifeng Li, Shuanlong Niu, Wang Miao, Tongzhi Niu</li>
<li>for: 这个研究旨在找到一个自动生成适合Surface Defect Detection任务的神经网络架构，以提高工业场景中的检测精度和效率。</li>
<li>methods: 本研究使用Neural Architecture Search（NAS）技术，搭配一个适应性搜寻空间，以自动生成适合Surface Defect Detection任务的神经网络架构。搜寻空间包括重复排列的基本新细胞，以及可搜寻的注意操作。进一步地，使用一种进步搜寻策略和深度超级运算，以更快速地和更好地探索搜寻空间。</li>
<li>results: 实验结果显示，提案的方法可以实现高性能和轻量级的检测网络，并且与其他竞争方法相比，包括手动设计和NAS方法，具有更好的性能和较小的网络模型大小。<details>
<summary>Abstract</summary>
Deep convolutional neural networks (CNNs) have been widely used in surface defect detection. However, no CNN architecture is suitable for all detection tasks and designing effective task-specific requires considerable effort. The neural architecture search (NAS) technology makes it possible to automatically generate adaptive data-driven networks. Here, we propose a new method called NAS-ASDet to adaptively design network for surface defect detection. First, a refined and industry-appropriate search space that can adaptively adjust the feature distribution is designed, which consists of repeatedly stacked basic novel cells with searchable attention operations. Then, a progressive search strategy with a deep supervision mechanism is used to explore the search space faster and better. This method can design high-performance and lightweight defect detection networks with data scarcity in industrial scenarios. The experimental results on four datasets demonstrate that the proposed method achieves superior performance and a relatively lighter model size compared to other competitive methods, including both manual and NAS-based approaches.
</details>
<details>
<summary>摘要</summary>
深度卷积神经网络 (CNN) 已广泛应用于表面缺陷检测中。然而，没有任何 CNN 架构适合所有检测任务，设计有效的任务特定网络需要较大的努力。神经网络搜索 (NAS) 技术使得可以自动生成适应数据驱动的网络。我们提出了一种新的方法called NAS-ASDet，用于适应性地设计检测网络。首先，我们设计了一个精细化和适用于工业场景的搜索空间，这个空间由重叠的基本新细胞组成，每个细胞具有搜索注意操作。然后，我们使用一种进步的搜索策略和深度超级视图机制，以更快和更好地探索搜索空间。这种方法可以在工业场景中的数据稀缺情况下设计高性能且轻量级的缺陷检测网络。实验结果表明，我们的方法可以在四个数据集上实现superior的性能，并且与其他竞争方法相比，模型的大小更轻量级。
</details></li>
</ul>
<hr>
<h2 id="Single-shot-Phase-Retrieval-from-a-Fractional-Fourier-Transform-Perspective"><a href="#Single-shot-Phase-Retrieval-from-a-Fractional-Fourier-Transform-Perspective" class="headerlink" title="Single-shot Phase Retrieval from a Fractional Fourier Transform Perspective"></a>Single-shot Phase Retrieval from a Fractional Fourier Transform Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10950">http://arxiv.org/abs/2311.10950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixiao Yang, Ran Tao, Kaixuan Wei, Jun Shi</li>
<li>for: 复原普通频率图像 (Classical Phase Retrieval)</li>
<li>methods: 融合FrFT测量模型与自愿学习重建方法</li>
<li>results: 实现单一测量复原 (Single-shot Phase Retrieval) 和获得高品质图像 (High-quality Images)<details>
<summary>Abstract</summary>
The realm of classical phase retrieval concerns itself with the arduous task of recovering a signal from its Fourier magnitude measurements, which are fraught with inherent ambiguities. A single-exposure intensity measurement is commonly deemed insufficient for the reconstruction of the primal signal, given that the absent phase component is imperative for the inverse transformation. In this work, we present a novel single-shot phase retrieval paradigm from a fractional Fourier transform (FrFT) perspective, which involves integrating the FrFT-based physical measurement model within a self-supervised reconstruction scheme. Specifically, the proposed FrFT-based measurement model addresses the aliasing artifacts problem in the numerical calculation of Fresnel diffraction, featuring adaptability to both short-distance and long-distance propagation scenarios. Moreover, the intensity measurement in the FrFT domain proves highly effective in alleviating the ambiguities of phase retrieval and relaxing the previous conditions on oversampled or multiple measurements in the Fourier domain. Furthermore, the proposed self-supervised reconstruction approach harnesses the fast discrete algorithm of FrFT alongside untrained neural network priors, thereby attaining preeminent results. Through numerical simulations, we demonstrate that both amplitude and phase objects can be effectively retrieved from a single-shot intensity measurement using the proposed approach and provide a promising technique for support-free coherent diffraction imaging.
</details>
<details>
<summary>摘要</summary>
经典阶段恢复的领域涉及于从傅里尔变换（Fourier Transform）中获取信号，但这在存在内在幂等性的情况下是一项困难的任务。单个曝光量测量通常被视为不够于重建原始信号，因为缺失的相位组件是重建过程中的关键因素。在这种情况下，我们提出了一种基于分解傅里尔变换（FrFT）的新的单极恢复方法。我们在这种方法中将FrFT基于物理测量模型集成到了一种自适应恢复方案中。具体来说，我们的FrFT基于测量模型解决了数值计算幂等噪声的问题，并且可以适应短距离和长距离传播enario。此外，在FrFT域中测量INTENSITY的方法具有减轻恢复ambiguities和放弃先前需要多个测量或扩展的观测的优点。此外，我们的自适应恢复方法利用FrFT快速简洁算法和未训练神经网络约束，实现了突出的结果。通过数值实验，我们示示了单极测量INTENSITY可以有效地从单个曝光量测量中获取恢复信号，并提供了一种支持自由幂 diffraction imaging 的有力的技术。
</details></li>
</ul>
<hr>
<h2 id="Jenga-Stacking-Based-on-6D-Pose-Estimation-for-Architectural-Form-Finding-Process"><a href="#Jenga-Stacking-Based-on-6D-Pose-Estimation-for-Architectural-Form-Finding-Process" class="headerlink" title="Jenga Stacking Based on 6D Pose Estimation for Architectural Form Finding Process"></a>Jenga Stacking Based on 6D Pose Estimation for Architectural Form Finding Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10918">http://arxiv.org/abs/2311.10918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixun Huang</li>
<li>for: 这篇论文主要是为了探讨当前最新的6D pose estimation方法的现状，以及在不同的建筑设计场景下应用pose estimation方法的选择。</li>
<li>methods: 本论文通过对最新的Gen6d研究进行评估，对当前开放集成方法进行质量评估，包括应用级别、预测速度、遮挡性、准确率、环境干扰等方面的评估。</li>
<li>results: 本论文通过对6D pose estimation方法的综合评估，发现在应用级别和预测速度方面有所改进空间，对于遮挡性和环境干扰的防止仍然存在一定的挑战。同时，通过与建筑风景环境评估结合，提出了一种可质量的建筑设计方法。<details>
<summary>Abstract</summary>
This paper includes a review of current state of the art 6d pose estimation methods, as well as a discussion of which pose estimation method should be used in two types of architectural design scenarios. Taking the latest pose estimation research Gen6d as an example, we make a qualitative assessment of the current openset methods in terms of application level, prediction speed, resistance to occlusion, accuracy, resistance to environmental interference, etc. In addition, we try to combine 6D pose estimation and building wind environment assessment to create tangible architectural design approach, we discuss the limitations of the method and point out the direction in which 6d pose estimation is eager to progress in this scenario.
</details>
<details>
<summary>摘要</summary>
这篇论文包括当前最佳6D姿态估计方法的回顾，以及在两类建筑设计场景中应用哪种姿态估计方法的讨论。以latest pose estimation research Gen6d为例，我们对当前开放集成方法进行质量评估，包括应用水平、预测速度、遮挡耐受度、准确率、环境干扰耐受度等方面。此外，我们尝试将6D姿态估计与建筑风险环境评估结合，创造可触媒建筑设计方法，并讨论这种方法的局限性和进一步发展方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/18/cs.CV_2023_11_18/" data-id="clp88dbx700njob889szg6lbd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/18/cs.AI_2023_11_18/" class="article-date">
  <time datetime="2023-11-18T12:00:00.000Z" itemprop="datePublished">2023-11-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/18/cs.AI_2023_11_18/">cs.AI - 2023-11-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Morphology-Enhanced-CAM-Guided-SAM-for-weakly-supervised-Breast-Lesion-Segmentation"><a href="#Morphology-Enhanced-CAM-Guided-SAM-for-weakly-supervised-Breast-Lesion-Segmentation" class="headerlink" title="Morphology-Enhanced CAM-Guided SAM for weakly supervised Breast Lesion Segmentation"></a>Morphology-Enhanced CAM-Guided SAM for weakly supervised Breast Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11176">http://arxiv.org/abs/2311.11176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuexin18/morseg-cam-sam">https://github.com/yuexin18/morseg-cam-sam</a></li>
<li>paper_authors: Xin Yue, Qing Zhao, Jianqiang Li, Xiaoling Liu, Changwei Song, Suqin Liu, Guanghui Fu</li>
<li>for: 这个研究的目的是提出一个新的测脉检测标准，以帮助早期识别乳腺癌。</li>
<li>methods: 这个方法使用了一个基于 morphology 的测脉检测模型，并且使用了 Class Activation Map (CAM) 来精确地定位悖论。</li>
<li>results: 这个方法可以获得比较好的效果，其 Dice 分数为 74.39%，与超参考方法相比，其 Hausdorff 距离为 24.27，较低。<details>
<summary>Abstract</summary>
Breast cancer diagnosis challenges both patients and clinicians, with early detection being crucial for effective treatment. Ultrasound imaging plays a key role in this, but its utility is hampered by the need for precise lesion segmentation-a task that is both time-consuming and labor-intensive. To address these challenges, we propose a new framework: a morphology-enhanced, Class Activation Map (CAM)-guided model, which is optimized using a computer vision foundation model known as SAM. This innovative framework is specifically designed for weakly supervised lesion segmentation in early-stage breast ultrasound images. Our approach uniquely leverages image-level annotations, which removes the requirement for detailed pixel-level annotation. Initially, we perform a preliminary segmentation using breast lesion morphology knowledge. Following this, we accurately localize lesions by extracting semantic information through a CAM-based heatmap. These two elements are then fused together, serving as a prompt to guide the SAM in performing refined segmentation. Subsequently, post-processing techniques are employed to rectify topological errors made by the SAM. Our method not only simplifies the segmentation process but also attains accuracy comparable to supervised learning methods that rely on pixel-level annotation. Our framework achieves a Dice score of 74.39% on the test set, demonstrating compareable performance with supervised learning methods. Additionally, it outperforms a supervised learning model, in terms of the Hausdorff distance, scoring 24.27 compared to Deeplabv3+'s 32.22. These experimental results showcase its feasibility and superior performance in integrating weakly supervised learning with SAM. The code is made available at: https://github.com/YueXin18/MorSeg-CAM-SAM.
</details>
<details>
<summary>摘要</summary>
乳癌诊断带来挑战，特别是在早期发现是关键。ultrasound imaging在这方面发挥着关键作用，但是需要精准的肿体分割，这是时间consuming和劳动密集的任务。为了解决这些挑战，我们提出了一个新的框架：一种基于形态学的Class Activation Map（CAM）引导模型，通过一个已知的计算机视觉基础模型（SAM）进行优化。这种创新的框架专门用于早期乳癌ultrasound图像中的弱类supervised lesion segmentation。我们的方法利用图像级别的标注，从而消除了精准的像素级别标注的需求。首先，我们进行初步分 segmentation，基于乳腺癌形态知识。然后，我们准确地找到肿体，通过提取semantic信息，并使用CAM基于的热图来准确地Localize lesions。这两个元素之后被融合，作为SAM进行精确的分 segmentation的指引。最后，我们使用post-processing技术来修正SAM中的topological错误。我们的方法不仅简化了分 segmentation过程，还可以达到与超级vised learning方法相同的准确性。我们的框架在测试集上达到了74.39%的Dice分数，与超级vised learning方法相当。此外，它还在 Hausdorff distance上 OUTPERFORMS Deeplabv3+，分别为24.27和32.22。这些实验结果表明了我们的框架在结合弱类学习与SAM的可行性和性能优势。代码可以在以下链接获取：https://github.com/YueXin18/MorSeg-CAM-SAM。
</details></li>
</ul>
<hr>
<h2 id="Best-uses-of-ChatGPT-and-Generative-AI-for-computer-science-research"><a href="#Best-uses-of-ChatGPT-and-Generative-AI-for-computer-science-research" class="headerlink" title="Best uses of ChatGPT and Generative AI for computer science research"></a>Best uses of ChatGPT and Generative AI for computer science research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11175">http://arxiv.org/abs/2311.11175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo C. Garrido-Merchan<br>for: This paper explores the diverse applications of ChatGPT and other generative AI technologies in computer science academic research, with a focus on using these tools to boost the productivity of computer research scientists.methods: The paper highlights innovative uses of generative AI, such as brainstorming research ideas, aiding in the drafting and styling of academic papers, and assisting in the synthesis of state-of-the-art sections.results: The paper makes recommendations for using generative AI to improve the productivity of computer research scientists, including using these tools for synthetic data creation, research methodology, and mentorship, as well as for task organization and article quality assessment. Additionally, the paper explores the capabilities of generative AI in disseminating ideas, generating images and audio, text transcription, and engaging with editors.<details>
<summary>Abstract</summary>
Generative Artificial Intelligence (AI), particularly tools like OpenAI's popular ChatGPT, is reshaping the landscape of computer science research. Used wisely, these tools can boost the productivity of a computer research scientist. This paper provides an exploration of the diverse applications of ChatGPT and other generative AI technologies in computer science academic research, making recommendations about the use of Generative AI to make more productive the role of the computer research scientist, with the focus of writing new research papers. We highlight innovative uses such as brainstorming research ideas, aiding in the drafting and styling of academic papers and assisting in the synthesis of state-of-the-art section. Further, we delve into using these technologies in understanding interdisciplinary approaches, making complex texts simpler, and recommending suitable academic journals for publication. Significant focus is placed on generative AI's contributions to synthetic data creation, research methodology, and mentorship, as well as in task organization and article quality assessment. The paper also addresses the utility of AI in article review, adapting texts to length constraints, constructing counterarguments, and survey development. Moreover, we explore the capabilities of these tools in disseminating ideas, generating images and audio, text transcription, and engaging with editors. We also describe some non-recommended uses of generative AI for computer science research, mainly because of the limitations of this technology.
</details>
<details>
<summary>摘要</summary>
生成人工智能（AI），特别是开源AI的ChatGPT等工具，正在计算机科学研究领域发挥重要作用。如果用得当，这些工具可以提高计算机研究科学家的生产力。本文通过探讨生成AI在计算机科学学术研究中的多种应用，并提出使用生成AI来提高计算机研究科学家的作用，主要是为了写新的研究论文。我们指出了使用这些技术的创新用途，如讨论研究主题、帮助撰写和编写学术论文、协助合并state-of-the-art部分。此外，我们还探讨了这些技术在跨学科approach、简化复杂文本、建议适合发表学术刊物等方面的应用。在生成数据创造、研究方法、导师、任务组织和文章质量评估等方面，生成AI做出了重要贡献。此外，我们还探讨了AI在文章审查、文章修改、调查开发等方面的应用。此外，我们还探讨了这些工具在传播想法、生成图像和音频、文本笔记、与编辑器交互等方面的能力。最后，我们还讨论了生成AI在计算机科学研究中的一些不建议使用情况，主要是因为这些技术的限制。
</details></li>
</ul>
<hr>
<h2 id="Deep-Coherence-Learning-An-Unsupervised-Deep-Beamformer-for-High-Quality-Single-Plane-Wave-Imaging-in-Medical-Ultrasound"><a href="#Deep-Coherence-Learning-An-Unsupervised-Deep-Beamformer-for-High-Quality-Single-Plane-Wave-Imaging-in-Medical-Ultrasound" class="headerlink" title="Deep Coherence Learning: An Unsupervised Deep Beamformer for High Quality Single Plane Wave Imaging in Medical Ultrasound"></a>Deep Coherence Learning: An Unsupervised Deep Beamformer for High Quality Single Plane Wave Imaging in Medical Ultrasound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11169">http://arxiv.org/abs/2311.11169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunwoo Cho, Seongjun Park, Jinbum Kang, Yangmo Yoo</li>
<li>for: 这个研究旨在提高单束波图像成像（PWI）的高框率和新临床应用。</li>
<li>methods: 这篇研究提出了一种新的无监督学习方法，即深度同步学习（DL）基本构成（DL-DCL），以提高单束波图像成像质量。</li>
<li>results: 实验和phantom研究表明，提出的DL-DCL方法可以与传统的多束波图像成像（DMAS）和传统的深度学习（DAS）相比，具有更高的空间和对比分辨率。<details>
<summary>Abstract</summary>
Plane wave imaging (PWI) in medical ultrasound is becoming an important reconstruction method with high frame rates and new clinical applications. Recently, single PWI based on deep learning (DL) has been studied to overcome lowered frame rates of traditional PWI with multiple PW transmissions. However, due to the lack of appropriate ground truth images, DL-based PWI still remains challenging for performance improvements. To address this issue, in this paper, we propose a new unsupervised learning approach, i.e., deep coherence learning (DCL)-based DL beamformer (DL-DCL), for high-quality single PWI. In DL-DCL, the DL network is trained to predict highly correlated signals with a unique loss function from a set of PW data, and the trained DL model encourages high-quality PWI from low-quality single PW data. In addition, the DL-DCL framework based on complex baseband signals enables a universal beamformer. To assess the performance of DL-DCL, simulation, phantom and in vivo studies were conducted with public datasets, and it was compared with traditional beamformers (i.e., DAS with 75-PWs and DMAS with 1-PW) and other DL-based methods (i.e., supervised learning approach with 1-PW and generative adversarial network (GAN) with 1-PW). From the experiments, the proposed DL-DCL showed comparable results with DMAS with 1-PW and DAS with 75-PWs in spatial resolution, and it outperformed all comparison methods in contrast resolution. These results demonstrated that the proposed unsupervised learning approach can address the inherent limitations of traditional PWIs based on DL, and it also showed great potential in clinical settings with minimal artifacts.
</details>
<details>
<summary>摘要</summary>
单波探射图像重建（PWI）在医疗超声中成为重要的重建方法，具有高帧率和新的临床应用。近期，基于深度学习（DL）的单波PWI被研究以超越传统PWI的帧率降低。然而，由于缺乏适当的基准图像，DL基础的PWI仍然具有挑战性。为解决这问题，本文提出了一新的无监督学习方法——深度协调学习（DCL）基础的DL扁平变数（DL-DCL），以提高单波PWI的质量。在DCL-DCL中，DL网络被训练来预测基于单波探射数据的高度相关的信号，并且训练DL模型以从低质量单波探射数据中获得高质量PWI。此外，DCL-DCL框架基于复杂的基带信号，实现了通用的扁平变数。为评估DCL-DCL的性能，本文进行了遮 simulation、实验和生体研究，并与传统扁平变数（i.e., DAS with 75-PWs和DMAS with 1-PW）和其他DL基础方法（i.e., 监督学习方法with 1-PW和生成 adversarial network（GAN）with 1-PW）进行比较。实验结果显示，提案的DCL-DCL与DMAS with 1-PW和DAS with 75-PWs相似的 spatial resolution，并且与所有比较方法相比，具有更高的contrast resolution。这些结果显示了提案的无监督学习方法可以解决传统PWI基于DL的内在限制，并且在临床设置中具有最小的错误。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Exposure-Bias-in-Discriminator-Guided-Diffusion-Models"><a href="#Mitigating-Exposure-Bias-in-Discriminator-Guided-Diffusion-Models" class="headerlink" title="Mitigating Exposure Bias in Discriminator Guided Diffusion Models"></a>Mitigating Exposure Bias in Discriminator Guided Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11164">http://arxiv.org/abs/2311.11164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos</li>
<li>for: 提高Diffusion Models生成图像质量</li>
<li>methods:  incorporating an auxiliary term derived from a discriminator network, modifying the sampling approach</li>
<li>results:  Achieving an FID score of 1.73 on the unconditional CIFAR-10 dataset, outperforming the current state-of-the-art.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
Diffusion Models have demonstrated remarkable performance in image generation. However, their demanding computational requirements for training have prompted ongoing efforts to enhance the quality of generated images through modifications in the sampling process. A recent approach, known as Discriminator Guidance, seeks to bridge the gap between the model score and the data score by incorporating an auxiliary term, derived from a discriminator network. We show that despite significantly improving sample quality, this technique has not resolved the persistent issue of Exposure Bias and we propose SEDM-G++, which incorporates a modified sampling approach, combining Discriminator Guidance and Epsilon Scaling. Our proposed approach outperforms the current state-of-the-art, by achieving an FID score of 1.73 on the unconditional CIFAR-10 dataset.
</details>
<details>
<summary>摘要</summary>
Diffusion Models 在图像生成中表现出色，但它们的训练需要高度计算能力，这导致了不断尝试改进生成图像质量的方法。一种最新的方法是通过抽象网络来提供一个辅助项，以bridging模型分数和数据分数之间的差距。我们发现，尽管显著提高样本质量，但这种技术并未解决持续存在的曝光偏见问题。我们提出了SEDM-G++，它将 combine Discriminator Guidance 和 Epsilon Scaling 两种技术，并实现了 current state-of-the-art 的 FID 分数（1.73）在无条件 CIFAR-10 数据集上。
</details></li>
</ul>
<hr>
<h2 id="Contextualizing-Internet-Memes-Across-Social-Media-Platforms"><a href="#Contextualizing-Internet-Memes-Across-Social-Media-Platforms" class="headerlink" title="Contextualizing Internet Memes Across Social Media Platforms"></a>Contextualizing Internet Memes Across Social Media Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11157">http://arxiv.org/abs/2311.11157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurav Joshi, Filip Ilievski, Luca Luceri</li>
<li>for: 本研究目的是寻找互联网趋势的表达方式，即互联网趋势的媒体表达形式。</li>
<li>methods: 本研究使用了一种名为“知识图”的semantic repository of knowledge，将互联网趋势的表达形式与知识图中的内容进行对比，从而识别和映射互联网趋势。</li>
<li>results: 研究发现，可以通过对互联网趋势的表达形式与知识图中的内容进行对比，来识别和映射互联网趋势。此外，研究还发现了不同平台上的趋势的差异和流行的趋势，以及一些常见的趋势渠道和子Reddit。最后，研究还示出了如何使用知识图来提供社交媒体上趋势的上下文。<details>
<summary>Abstract</summary>
Internet memes have emerged as a novel format for communication and expressing ideas on the web. Their fluidity and creative nature are reflected in their widespread use, often across platforms and occasionally for unethical or harmful purposes. While computational work has already analyzed their high-level virality over time and developed specialized classifiers for hate speech detection, there have been no efforts to date that aim to holistically track, identify, and map internet memes posted on social media. To bridge this gap, we investigate whether internet memes across social media platforms can be contextualized by using a semantic repository of knowledge, namely, a knowledge graph. We collect thousands of potential internet meme posts from two social media platforms, namely Reddit and Discord, and perform an extract-transform-load procedure to create a data lake with candidate meme posts. By using vision transformer-based similarity, we match these candidates against the memes cataloged in a recently released knowledge graph of internet memes, IMKG. We provide evidence that memes published online can be identified by mapping them to IMKG. We leverage this grounding to study the prevalence of memes on different platforms, discover popular memes, and select common meme channels and subreddits. Finally, we illustrate how the grounding can enable users to get context about memes on social media thanks to their link to the knowledge graph.
</details>
<details>
<summary>摘要</summary>
互联网趣闻在互联网上作为一种新的交流和表达方式得到广泛的应用。它们的流动性和创新性使其在不同的平台上广泛使用， occasional 用于不道德或有害的目的。虽然计算工作已经分析了这些趣闻的时间权 virality 和开发了特殊的 hate speech 检测器，但到目前为止没有任何尝试将互联网趣闻在社交媒体上 Contextualized 。为了bridging这个差距，我们investigate 了 whether 互联网趣闻在社交媒体平台上可以通过使用一个semantic repository of knowledge，即知识 graphs 来contextualize。我们收集了 thousands 的 potential 互联网趣闻帖子从两个社交媒体平台，namely Reddit 和 Discord，并perform 了一个 extract-transform-load 过程，以创建一个数据湖包含候选趣闻帖子。通过使用视力 transformer 基于相似性，我们将这些候选者与 IMKG 中 cataloged 的趣闻进行匹配。我们提供了证据，证明在线上发布的趣闻可以被IMKG 中的趣闻映射。我们利用这种固定来研究不同平台上趣闻的流行程度，发现 популяр的趣闻，并选择常见的趣闻渠道和 subreddits。最后，我们示例了如何通过这种固定，使用户在社交媒体上获得趣闻的CONTEXT。
</details></li>
</ul>
<hr>
<h2 id="A-Principled-Framework-for-Knowledge-enhanced-Large-Language-Model"><a href="#A-Principled-Framework-for-Knowledge-enhanced-Large-Language-Model" class="headerlink" title="A Principled Framework for Knowledge-enhanced Large Language Model"></a>A Principled Framework for Knowledge-enhanced Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11135">http://arxiv.org/abs/2311.11135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saizhuo Wang, Zhihan Liu, Zhaoran Wang, Jian Guo</li>
<li>for: 提高LLMs的深度分析能力和可靠性，使其能够应用于重要场景。</li>
<li>methods: 提出了一种严格设计的框架，通过 anchoring knowledge 和 closed-loop reasoning 来提高 LLMs 的分析能力。</li>
<li>results: 通过分析框架的各个组成部分，证明了 LLMs 的理解能力的提高，并且在定义的假设下提供了理论保证。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are versatile, yet they often falter in tasks requiring deep and reliable reasoning due to issues like hallucinations, limiting their applicability in critical scenarios. This paper introduces a rigorously designed framework for creating LLMs that effectively anchor knowledge and employ a closed-loop reasoning process, enhancing their capability for in-depth analysis. We dissect the framework to illustrate the contribution of each component to the LLMs' performance, offering a theoretical assurance of improved reasoning under well-defined assumptions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Bayesian-Neural-Networks-A-Min-Max-Game-Framework"><a href="#Bayesian-Neural-Networks-A-Min-Max-Game-Framework" class="headerlink" title="Bayesian Neural Networks: A Min-Max Game Framework"></a>Bayesian Neural Networks: A Min-Max Game Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11126">http://arxiv.org/abs/2311.11126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junping Hong, Ercan Engin Kuruoglu</li>
<li>for: 这个论文主要是为了探讨bayesian neural networks的应用和关系。</li>
<li>methods: 这个论文使用了bayesian neural networks和variational inference来训练模型，并将其формализова为一个MINIMAX游戏问题。</li>
<li>results: 实验结果表明，这个方法和现有的closed-loop transcription neural network相当，并且可以提供另一种 bayesian neural networks的视角。<details>
<summary>Abstract</summary>
Bayesian neural networks use random variables to describe the neural networks rather than deterministic neural networks and are mostly trained by variational inference which updates the mean and variance at the same time. Here, we formulate the Bayesian neural networks as a minimax game problem. We do the experiments on the MNIST data set and the primary result is comparable to the existing closed-loop transcription neural network. Finally, we reveal the connections between Bayesian neural networks and closed-loop transcription neural networks, and show our framework is rather practical, and provide another view of Bayesian neural networks.
</details>
<details>
<summary>摘要</summary>
bayesian neural networks 使用Random variable来描述神经网络，而不是决定性神经网络，通常通过变量推导来训练。在这里，我们将 bayesian neural networks 表示为最小化游戏问题。我们在 MNIST 数据集上进行实验，主要结果与现有的关闭环路译写神经网络相当。最后，我们描述 bayesian neural networks 和关闭环路译写神经网络之间的联系，并表明我们的框架是实用的，并提供了 bayesian neural networks 另一种视图。Here's the translation breakdown:* Bayesian neural networks ( bayesian neural networks ) - 概率神经网络* use random variables ( 使用Random variable ) - 使用随机变量* to describe the neural networks ( 描述神经网络 ) - 描述神经网络* rather than deterministic neural networks ( 而不是决定性神经网络 ) - 而不是决定性神经网络* and are mostly trained by variational inference ( 通常通过变量推导来训练 ) - 通常通过变量推导来训练* We formulate the Bayesian neural networks as a minimax game problem ( 在这里，我们将 bayesian neural networks 表示为最小化游戏问题 ) - 将 bayesian neural networks 表示为最小化游戏问题* We do the experiments on the MNIST data set ( 我们在 MNIST 数据集上进行实验 ) - 在 MNIST 数据集上进行实验* and the primary result is comparable to the existing closed-loop transcription neural network ( 主要结果与现有的关闭环路译写神经网络相当 ) - 主要结果与现有的关闭环路译写神经网络相当* Finally, we reveal the connections between Bayesian neural networks and closed-loop transcription neural networks ( 最后，我们描述 bayesian neural networks 和关闭环路译写神经网络之间的联系 ) - 最后，我们描述 bayesian neural networks 和关闭环路译写神经网络之间的联系* and show our framework is rather practical ( 并表明我们的框架是实用的 ) - 并表明我们的框架是实用的* and provide another view of Bayesian neural networks ( 并提供了 bayesian neural networks 另一种视图 ) - 并提供了 bayesian neural networks 另一种视图
</details></li>
</ul>
<hr>
<h2 id="An-Improved-Neural-Network-Model-Based-On-CNN-Using-For-Fruit-Sugar-Degree-Detection"><a href="#An-Improved-Neural-Network-Model-Based-On-CNN-Using-For-Fruit-Sugar-Degree-Detection" class="headerlink" title="An Improved Neural Network Model Based On CNN Using For Fruit Sugar Degree Detection"></a>An Improved Neural Network Model Based On CNN Using For Fruit Sugar Degree Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11120">http://arxiv.org/abs/2311.11120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boyang Deng, Xin Wen, Zhan Gao</li>
<li>for: 这个论文是用于检测水果糖分的检测方法。</li>
<li>methods: 该论文使用了人工神经网络模型，其中低层为多层感知器(MLP)，中间层为2维相关矩阵层，高层为卷积神经网络(CNN)层。此外，论文还使用了振荡分解(WD)来降维特征，以及进化算法(GA)来找到优秀特征。</li>
<li>results: 论文通过对水果谱спектrum数据进行处理和分析，并比较了不同的神经网络模型和传统参数选择方法的效果。结果表明，使用人工神经网络模型可以准确地检测水果糖分，并且比传统参数选择方法更高效。此外，论文还提出了一种基于数据标准差(STD)的新评价标准，用于评价检测性能。<details>
<summary>Abstract</summary>
Artificial Intelligence(AI) widely applies in Image Classification and Recognition, Text Understanding and Natural Language Processing, which makes great progress. In this paper, we introduced AI into the fruit quality detection field. We designed a fruit sugar degree regression model using an Artificial Neural Network based on spectra of fruits within the visible/near-infrared(V/NIR)range. After analysis of fruit spectra, we innovatively proposed a new neural network structure: low layers consist of a Multilayer Perceptron(MLP), a middle layer is a 2-dimensional correlation matrix layer, and high layers consist of several Convolutional Neural Network(CNN) layers. In this study, we used fruit sugar value as a detection target, collecting two fruits called Gan Nan Navel and Tian Shan Pear as samples, doing experiments respectively, and comparing their results. We used Analysis of Variance(ANOVA) to evaluate the reliability of the dataset we collected. Then, we tried multiple strategies to process spectrum data, evaluating their effects. In this paper, we tried to add Wavelet Decomposition(WD) to reduce feature dimensions and a Genetic Algorithm(GA) to find excellent features. Then, we compared Neural Network models with traditional Partial Least Squares(PLS) based models. We also compared the neural network structure we designed(MLP-CNN) with other traditional neural network structures. In this paper, we proposed a new evaluation standard derived from dataset standard deviation(STD) for evaluating detection performance, validating the viability of using an artificial neural network model to do fruit sugar degree nondestructive detection.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）广泛应用于图像分类和识别、自然语言处理和文本理解等领域，带来了很大的进步。在这篇论文中，我们将AI应用于水果质量检测领域。我们设计了一种基于人工神经网络的水果糖度预测模型，使用水果在可见/近红外（V/NIR）谱spectra进行分析。经过分析水果谱spectra后，我们创新地提出了一种新的神经网络结构：低层为多层感知网络（MLP），中层为2维相关矩阵层，高层为几个卷积神经网络（CNN）层。在这个研究中，我们使用水果糖度值作为检测目标，采集了两种水果样本——芜南柑和天山梨，进行了分别的实验，并比较了其结果。我们使用分布式 Анализа variance（ANOVA）评估数据集的可靠性。然后，我们尝试了多种处理谱数据的策略，评估其效果。在这篇论文中，我们尝试了使用扩展特征矩阵（WD）减少特征维度，以及使用进化算法（GA）找到优秀的特征。然后，我们比较了神经网络模型与传统的部分最小平方（PLS）基于模型。我们还比较了我们设计的神经网络结构（MLP-CNN）与其他传统神经网络结构。在这篇论文中，我们提出了一种基于数据集标准差（STD）的新评价标准，用于评估检测性能。这些结果validate了使用人工神经网络模型进行水果糖度非 destruktive检测的可能性。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Speech-Emotion-Recognition-and-Recommender-Systems-for-Negative-Emotion-Handling-in-Therapy-Chatbots"><a href="#Utilizing-Speech-Emotion-Recognition-and-Recommender-Systems-for-Negative-Emotion-Handling-in-Therapy-Chatbots" class="headerlink" title="Utilizing Speech Emotion Recognition and Recommender Systems for Negative Emotion Handling in Therapy Chatbots"></a>Utilizing Speech Emotion Recognition and Recommender Systems for Negative Emotion Handling in Therapy Chatbots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11116">http://arxiv.org/abs/2311.11116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farideh Majidi, Marzieh Bahrami<br>for: 提高访问者情绪支持和提供人类化同情methods: 使用语音感知技术和语音感受识别（SER）技术，并使用Convolutional Neural Network（CNN）模型和ShEMO数据集来准确地探测和分类负面情绪，包括恐慌、恐惧和悲伤。同时，开发了一个推荐系统，利用SER模型的输出生成个性化的情绪管理建议，并使用 GloVe 和 LSTM 模型来实现这一点。results:  SER 模型在语音信号中探测和分类负面情绪的验证准确率为 88%，而推荐模型在生成个性化的情绪管理建议方面的准确率为 98%。通过 integrate 文本到语音模型 GlowTTS，能够为用户提供人类化的同情和建议，并在英文和波斯语中提供多语言支持。<details>
<summary>Abstract</summary>
Emotional well-being significantly influences mental health and overall quality of life. As therapy chatbots become increasingly prevalent, their ability to comprehend and respond empathetically to users' emotions remains limited. This paper addresses this limitation by proposing an approach to enhance therapy chatbots with auditory perception, enabling them to understand users' feelings and provide human-like empathy. The proposed method incorporates speech emotion recognition (SER) techniques using Convolutional Neural Network (CNN) models and the ShEMO dataset to accurately detect and classify negative emotions, including anger, fear, and sadness. The SER model achieves a validation accuracy of 88%, demonstrating its effectiveness in recognizing emotional states from speech signals. Furthermore, a recommender system is developed, leveraging the SER model's output to generate personalized recommendations for managing negative emotions, for which a new bilingual dataset was generated as well since there is no such dataset available for this task. The recommender model achieves an accuracy of 98% by employing a combination of global vectors for word representation (GloVe) and LSTM models. To provide a more immersive and empathetic user experience, a text-to-speech model called GlowTTS is integrated, enabling the therapy chatbot to audibly communicate the generated recommendations to users in both English and Persian. The proposed approach offers promising potential to enhance therapy chatbots by providing them with the ability to recognize and respond to users' emotions, ultimately improving the delivery of mental health support for both English and Persian-speaking users.
</details>
<details>
<summary>摘要</summary>
情感健康对于心理健康和全面生活质量具有重要影响。随着咨询虚拟助手在使用人数的增加，它们的理解和回应用户情感的能力仍然有限。本文解决这一问题，提出一种增强咨询虚拟助手的方法，使其能够通过听觉感知，理解用户的情感状况，并提供人类化同情。本方法利用了支持Vector（SER）技术，使用Convolutional Neural Network（CNN）模型和ShEMO数据集，准确地检测和分类负面情感，包括恐惧、愤怒和悲伤。SER模型的验证准确率达88%，证明其能够从语音信号中准确地检测情感状况。此外，我们还开发了一个推荐系统，利用SER模型的输出，为用户提供个性化的情感管理建议。我们生成了一个新的双语数据集，以便在这个任务上进行训练。推荐模型使用了GloVe word表示模型和LSTM模型，实现了98%的准确率。为提供更加投入和同情的用户体验，我们还 интегрирова了一个名为GlowTTS的文本读取模型，使咨询虚拟助手能够通过语音方式传达给用户建议，并在英文和波斯语中进行同时传达。本方法的提议具有推动咨询虚拟助手提供更好的情感管理支持的潜在能力。
</details></li>
</ul>
<hr>
<h2 id="Environment-Aware-Dynamic-Graph-Learning-for-Out-of-Distribution-Generalization"><a href="#Environment-Aware-Dynamic-Graph-Learning-for-Out-of-Distribution-Generalization" class="headerlink" title="Environment-Aware Dynamic Graph Learning for Out-of-Distribution Generalization"></a>Environment-Aware Dynamic Graph Learning for Out-of-Distribution Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11114">http://arxiv.org/abs/2311.11114</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ringbdstack/eagle">https://github.com/ringbdstack/eagle</a></li>
<li>paper_authors: Haonan Yuan, Qingyun Sun, Xingcheng Fu, Ziwei Zhang, Cheng Ji, Hao Peng, Jianxin Li</li>
<li>for: This paper focuses on improving the out-of-distribution (OOD) generalization of dynamic graph neural networks (DGNNs) by modeling complex coupled environments and exploiting spatio-temporal invariant patterns.</li>
<li>methods: The proposed Environment-Aware dynamic Graph LEarning (EAGLE) framework includes an environment-aware EA-DGNN to model environments, an environment instantiation mechanism to diversify environments, and an invariant pattern recognition mechanism to discriminate spatio-temporal invariant patterns for OOD prediction.</li>
<li>results: The proposed EAGLE framework achieves superior performance compared to state-of-the-art baselines under distribution shifts, demonstrating its effectiveness in improving OOD generalization on dynamic graphs.<details>
<summary>Abstract</summary>
Dynamic graph neural networks (DGNNs) are increasingly pervasive in exploiting spatio-temporal patterns on dynamic graphs. However, existing works fail to generalize under distribution shifts, which are common in real-world scenarios. As the generation of dynamic graphs is heavily influenced by latent environments, investigating their impacts on the out-of-distribution (OOD) generalization is critical. However, it remains unexplored with the following two major challenges: (1) How to properly model and infer the complex environments on dynamic graphs with distribution shifts? (2) How to discover invariant patterns given inferred spatio-temporal environments? To solve these challenges, we propose a novel Environment-Aware dynamic Graph LEarning (EAGLE) framework for OOD generalization by modeling complex coupled environments and exploiting spatio-temporal invariant patterns. Specifically, we first design the environment-aware EA-DGNN to model environments by multi-channel environments disentangling. Then, we propose an environment instantiation mechanism for environment diversification with inferred distributions. Finally, we discriminate spatio-temporal invariant patterns for out-of-distribution prediction by the invariant pattern recognition mechanism and perform fine-grained causal interventions node-wisely with a mixture of instantiated environment samples. Experiments on real-world and synthetic dynamic graph datasets demonstrate the superiority of our method against state-of-the-art baselines under distribution shifts. To the best of our knowledge, we are the first to study OOD generalization on dynamic graphs from the environment learning perspective.
</details>
<details>
<summary>摘要</summary>
现代图学 neural networks (DGNNs) 在利用空间-时间模式方面越来越普遍。然而，现有的工作无法适应分布变化，这是现实世界中的常见情况。由于生成动态图的过程受到隐藏环境的影响，研究这些环境对于 OUT-OF-DISTRIBUTION (OOD) 通用性的影响是 kritical。然而，这还没有得到充分研究，主要有两个挑战：1. 如何正确地模型和推理动态图上复杂的环境下的分布变化？2. 如何在推理出的空间-时间环境中发现不变Pattern？为解决这些挑战，我们提出了一种新的 Environment-Aware 动态图学 LEarning (EAGLE) 框架，用于 OOD 通用性。具体来说，我们首先设计了环境意识 EA-DGNN，用于模型环境。然后，我们提出了环境实例化机制，用于实现环境多样性。最后，我们通过不变Pattern认识机制来识别OOD情况，并通过精细的 causal interventions 来进行精细的节点修饰。实验结果表明，我们的方法在真实世界和synthetic dynamic graph dataset上表现出优于状态的基eline under distribution shifts。到目前为止，我们是第一个从环境学习角度研究 OOD 通用性在动态图上。
</details></li>
</ul>
<hr>
<h2 id="varepsilon-fractional-Core-Stability-in-Hedonic-Games"><a href="#varepsilon-fractional-Core-Stability-in-Hedonic-Games" class="headerlink" title="$\varepsilon$-fractional Core Stability in Hedonic Games"></a>$\varepsilon$-fractional Core Stability in Hedonic Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11101">http://arxiv.org/abs/2311.11101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Fioravanti, Michele Flammini, Bojana Kodric, Giovanna Varricchio<br>for:This paper focuses on the problem of coalition formation in hedonic games, where agents are strategic and have individual preferences. The goal is to find a stable coalition structure that satisfies some form of stability, such as core-stability.methods:The paper proposes a new notion of $\varepsilon$-fractional core-stability, which allows for a fraction of coalitions to core-block, and designs efficient algorithms to find such partitions for two fundamental classes of hedonic games. The paper also explores the use of probabilistic sampling to learn valuations and compute outcomes that are $\varepsilon$-fractional core-stable.results:The paper shows that the proposed notion of $\varepsilon$-fractional core-stability can guarantee both existence and polynomial-time computation, and provides efficient algorithms for finding such partitions in two fundamental classes of hedonic games. The paper also gives positive and negative results on which distributions allow for the efficient computation of outcomes that are $\varepsilon$-fractional core-stable with arbitrarily high confidence in a PAC-learning fashion.<details>
<summary>Abstract</summary>
Hedonic Games (HGs) are a classical framework modeling coalition formation of strategic agents guided by their individual preferences. According to these preferences, it is desirable that a coalition structure (i.e. a partition of agents into coalitions) satisfies some form of stability. The most well-known and natural of such notions is arguably core-stability. Informally, a partition is core-stable if no subset of agents would like to deviate by regrouping in a so-called core-blocking coalition. Unfortunately, core-stable partitions seldom exist and even when they do, it is often computationally intractable to find one. To circumvent these problems, we propose the notion of $\varepsilon$-fractional core-stability, where at most an $\varepsilon$-fraction of all possible coalitions is allowed to core-block. It turns out that such a relaxation may guarantee both existence and polynomial-time computation. Specifically, we design efficient algorithms returning an $\varepsilon$-fractional core-stable partition, with $\varepsilon$ exponentially decreasing in the number of agents, for two fundamental classes of HGs: Simple Fractional and Anonymous. From a probabilistic point of view, being the definition of $\varepsilon$-fractional core equivalent to requiring that uniformly sampled coalitions core-block with probability lower than $\varepsilon$, we further extend the definition to handle more complex sampling distributions. Along this line, when valuations have to be learned from samples in a PAC-learning fashion, we give positive and negative results on which distributions allow the efficient computation of outcomes that are $\varepsilon$-fractional core-stable with arbitrarily high confidence.
</details>
<details>
<summary>摘要</summary>
хедонис游戏（HG）是一种经典的框架模型，用于模型策略性agent coalition formation。根据这些 preferences，一个 coalition structure（即agent分配到不同的 coalition）满足一种形式的稳定性是感兴趣的。最常见和最自然的这种概念是核稳定性。 informally, a partition is core-stable if no subset of agents would like to deviate by regrouping in a so-called core-blocking coalition。 unfortunately, core-stable partitions seldom exist and even when they do, it is often computationally intractable to find one。 to circumvent these problems, we propose the notion of ε-fractional core-stability, where at most an ε-fraction of all possible coalitions is allowed to core-block。 it turns out that such a relaxation may guarantee both existence and polynomial-time computation。 specifically, we design efficient algorithms returning an ε-fractional core-stable partition, with ε exponentially decreasing in the number of agents, for two fundamental classes of HGs：simple fractional and anonymous。 from a probabilistic point of view, being the definition of ε-fractional core equivalent to requiring that uniformly sampled coalitions core-block with probability lower than ε， we further extend the definition to handle more complex sampling distributions。 along this line, when valuations have to be learned from samples in a PAC-learning fashion, we give positive and negative results on which distributions allow the efficient computation of outcomes that are ε-fractional core-stable with arbitrarily high confidence。
</details></li>
</ul>
<hr>
<h2 id="Introducing-NCL-SM-A-Fully-Annotated-Dataset-of-Images-from-Human-Skeletal-Muscle-Biopsies"><a href="#Introducing-NCL-SM-A-Fully-Annotated-Dataset-of-Images-from-Human-Skeletal-Muscle-Biopsies" class="headerlink" title="Introducing NCL-SM: A Fully Annotated Dataset of Images from Human Skeletal Muscle Biopsies"></a>Introducing NCL-SM: A Fully Annotated Dataset of Images from Human Skeletal Muscle Biopsies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11099">http://arxiv.org/abs/2311.11099</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/atifkhanncl/ncl-sm">https://github.com/atifkhanncl/ncl-sm</a></li>
<li>paper_authors: Atif Khan, Conor Lawless, Amy Vincent, Charlotte Warren, Valeria Di Leo, Tiago Gomes, A. Stephen McGough</li>
<li>for: 这个论文的目的是提供一个高质量的生物快照数据集，用于开发自动化、精准、可重复地分析单元细胞（SM）组织图像。</li>
<li>methods: 这篇论文使用了生物快照技术，并提供了高质量的生物快照数据集，包括46个人体制备样本和生物Marker的检测结果。</li>
<li>results: 这篇论文发现了一个全新的、高质量的生物快照数据集，可以用于开发自动化、精准、可重复地分析SM组织图像。这个数据集包括超过50,000个手动分割的肌细胞（myofibers），并且对每个myofibers进行了高质量的检查和标注。<details>
<summary>Abstract</summary>
Single cell analysis of skeletal muscle (SM) tissue is a fundamental tool for understanding many neuromuscular disorders. For this analysis to be reliable and reproducible, identification of individual fibres within microscopy images (segmentation) of SM tissue should be precise. There is currently no tool or pipeline that makes automatic and precise segmentation and curation of images of SM tissue cross-sections possible. Biomedical scientists in this field rely on custom tools and general machine learning (ML) models, both followed by labour intensive and subjective manual interventions to get the segmentation right. We believe that automated, precise, reproducible segmentation is possible by training ML models. However, there are currently no good quality, publicly available annotated imaging datasets available for ML model training. In this paper we release NCL-SM: a high quality bioimaging dataset of 46 human tissue sections from healthy control subjects and from patients with genetically diagnosed muscle pathology. These images include $>$ 50k manually segmented muscle fibres (myofibres). In addition we also curated high quality myofibres and annotated reasons for rejecting low quality myofibres and regions in SM tissue images, making this data completely ready for downstream analysis. This, we believe, will pave the way for development of a fully automatic pipeline that identifies individual myofibres within images of tissue sections and, in particular, also classifies individual myofibres that are fit for further analysis.
</details>
<details>
<summary>摘要</summary>
单元细胞分析对骨附肌（SM）组织是基本工具，用于理解许多神经肌肉疾病。为了使这种分析可靠和重复，单元细胞内的图像分割（segmentation）需要精准。目前没有任何工具或管道可以自动地对SM组织横截面图像进行精准分割和摘要。生物医学科学家在这个领域依赖于自定义工具和通用机器学习（ML）模型，然后进行劳动 INTENSIVE 和主观的手动干预，以确保分割是正确的。我们认为，通过训练ML模型，可以实现自动、精准、可重复的分割。然而，目前没有一个良好的、公共可用的批处理图像数据集，用于ML模型训练。在这篇论文中，我们发布NCL-SM：一个高质量生物影像数据集，包括46名健康控制者和被遗传诊断的肌肉疾病患者的人类组织横截面图像。这些图像包含> 50k个手动分割的肌肉元（myofibres）。此外，我们还精心准备了高质量的肌肉元和SM组织图像的批处理结果，并对低质量的肌肉元和SM组织图像进行了描述，使这些数据完全准备好进行下游分析。我们认为，这将开创出一个完全自动的分析管道，可以在SM组织图像中自动地标识和分割各个肌肉元，并在特定情况下还可以对各个肌肉元进行分类。
</details></li>
</ul>
<hr>
<h2 id="Radiology-Report-Generation-Using-Transformers-Conditioned-with-Non-imaging-Data"><a href="#Radiology-Report-Generation-Using-Transformers-Conditioned-with-Non-imaging-Data" class="headerlink" title="Radiology Report Generation Using Transformers Conditioned with Non-imaging Data"></a>Radiology Report Generation Using Transformers Conditioned with Non-imaging Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11097">http://arxiv.org/abs/2311.11097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nurbanu Aksoy, Nishant Ravikumar, Alejandro F Frangi<br>for: 这个研究旨在提高医疗影像解释的效率，并且使用多 modal 的资料来增强 radiology 报告生成。methods: 这个研究使用了一个新的多Modal transformer 网络，将静脉肺 X 光像和相关的病人人口资料集成，生成具体化的 radiology 报告。results: 根据评估指标，包括病人人口资料，使用提案的方法可以将 radiology 报告质量提高，相比基准网络使用静脉肺 X 光像alone。<details>
<summary>Abstract</summary>
Medical image interpretation is central to most clinical applications such as disease diagnosis, treatment planning, and prognostication. In clinical practice, radiologists examine medical images and manually compile their findings into reports, which can be a time-consuming process. Automated approaches to radiology report generation, therefore, can reduce radiologist workload and improve efficiency in the clinical pathway. While recent deep-learning approaches for automated report generation from medical images have seen some success, most studies have relied on image-derived features alone, ignoring non-imaging patient data. Although a few studies have included the word-level contexts along with the image, the use of patient demographics is still unexplored. This paper proposes a novel multi-modal transformer network that integrates chest x-ray (CXR) images and associated patient demographic information, to synthesise patient-specific radiology reports. The proposed network uses a convolutional neural network to extract visual features from CXRs and a transformer-based encoder-decoder network that combines the visual features with semantic text embeddings of patient demographic information, to synthesise full-text radiology reports. Data from two public databases were used to train and evaluate the proposed approach. CXRs and reports were extracted from the MIMIC-CXR database and combined with corresponding patients' data MIMIC-IV. Based on the evaluation metrics used including patient demographic information was found to improve the quality of reports generated using the proposed approach, relative to a baseline network trained using CXRs alone. The proposed approach shows potential for enhancing radiology report generation by leveraging rich patient metadata and combining semantic text embeddings derived thereof, with medical image-derived visual features.
</details>
<details>
<summary>摘要</summary>
医疗图像解读是许多临床应用的核心，包括疾病诊断、治疗规划和前景预测。在临床实践中，医生会 manually examining医疗图像并编写报告，这可能是一项时间消耗的过程。因此，自动化医学报告生成方法可以减轻医生的工作负担，提高临床流程的效率。 although recent deep learning approaches for automated report generation from medical images have shown some success, most studies have relied on image-derived features alone, ignoring non-imaging patient data。在这些研究中，只有一些研究包括了图像上的字符级别上下文，但是使用患者的人口数据仍然是一个未explored的领域。这篇论文提出了一种新的多Modal transformer网络，该网络将 integrate chest x-ray (CXR) 图像和相关的患者人口数据，以生成个性化的医学报告。该网络使用卷积神经网络提取 CXR 图像中的视觉特征，并使用基于 transformer 的 encoder-decoder 网络将这些视觉特征与患者人口数据相结合，以生成全文医学报告。使用两个公共数据库进行训练和评估，包括 MIMIC-CXR 数据库和 MIMIC-IV 数据库。根据评估指标，包括患者人口数据，使用该提案的方法生成的报告质量相比基线网络训练用 CXR 图像alone 提高。该提案表明可以通过利用丰富的患者Metadata和基于 semantic text embeddings  derivated thereof，与医疗图像中的视觉特征结合，提高医学报告生成的质量。
</details></li>
</ul>
<hr>
<h2 id="Deep-Tensor-Network"><a href="#Deep-Tensor-Network" class="headerlink" title="Deep Tensor Network"></a>Deep Tensor Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11091">http://arxiv.org/abs/2311.11091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/carpedm20/DCGAN-tensorflow">https://github.com/carpedm20/DCGAN-tensorflow</a></li>
<li>paper_authors: Yifan Zhang</li>
<li>for: 本文探讨了矩阵category的基本原理，通过矩阵产品的universal property，开拓了深度网络架构中新的方法ология。</li>
<li>methods: 本文的主要贡献是提出了矩阵注意力和矩阵互动机制，这是一种利用矩阵category提高深度网络计算效率和表达能力的新方法，甚至可以推广到量子领域。</li>
<li>results: 本文的实验结果表明，通过矩阵注意力和矩阵互动机制，可以增强深度网络的计算效率和表达能力，并且可以应用于量子领域。<details>
<summary>Abstract</summary>
In this paper, we delve into the foundational principles of tensor categories, harnessing the universal property of the tensor product to pioneer novel methodologies in deep network architectures. Our primary contribution is the introduction of the Tensor Attention and Tensor Interaction Mechanism, a groundbreaking approach that leverages the tensor category to enhance the computational efficiency and the expressiveness of deep networks, and can even be generalized into the quantum realm.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探索了矩阵类别的基本原理，利用矩阵产品的 universality 性来开拓深度网络体系的新方法。我们的主要贡献是提出了矩阵注意力和矩阵互动机制，这是一种创新的方法，利用矩阵类别来提高深度网络的计算效率和表达力，甚至可以扩展到量子领域。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Images-An-Integrative-Multi-modal-Approach-to-Chest-X-Ray-Report-Generation"><a href="#Beyond-Images-An-Integrative-Multi-modal-Approach-to-Chest-X-Ray-Report-Generation" class="headerlink" title="Beyond Images: An Integrative Multi-modal Approach to Chest X-Ray Report Generation"></a>Beyond Images: An Integrative Multi-modal Approach to Chest X-Ray Report Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11090">http://arxiv.org/abs/2311.11090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nurbanu Aksoy, Serge Sharoff, Selcuk Baser, Nishant Ravikumar, Alejandro F Frangi</li>
<li>for: 这个论文目的是为了自动生成医疗影像报告，描述医疗影像中的发现。</li>
<li>methods: 该论文提出了一种基于多Modal深度学习网络的方法，结合了结构化患者数据（如生物指标和症状）和不结构化医疗记录，以生成胸部X射线报告。我们引入了一种conditioned cross-multi-head注意力模块，以融合这些不同数据模式，bridging semantic gap between visual and textual data。</li>
<li>results: 实验表明，通过结合多 modal 数据，比单依图像数据获得了显著提高。此外，我们的模型在ROUGE-L指标上达到了相关的state-of-the-art 模型之上。此外，我们还使用了人工评估和临床semantic相似度测量，并与word-overlap指标进行深度的量化分析。<details>
<summary>Abstract</summary>
Image-to-text radiology report generation aims to automatically produce radiology reports that describe the findings in medical images. Most existing methods focus solely on the image data, disregarding the other patient information accessible to radiologists. In this paper, we present a novel multi-modal deep neural network framework for generating chest X-rays reports by integrating structured patient data, such as vital signs and symptoms, alongside unstructured clinical notes.We introduce a conditioned cross-multi-head attention module to fuse these heterogeneous data modalities, bridging the semantic gap between visual and textual data. Experiments demonstrate substantial improvements from using additional modalities compared to relying on images alone. Notably, our model achieves the highest reported performance on the ROUGE-L metric compared to relevant state-of-the-art models in the literature. Furthermore, we employed both human evaluation and clinical semantic similarity measurement alongside word-overlap metrics to improve the depth of quantitative analysis. A human evaluation, conducted by a board-certified radiologist, confirms the model's accuracy in identifying high-level findings, however, it also highlights that more improvement is needed to capture nuanced details and clinical context.
</details>
<details>
<summary>摘要</summary>
radiology report生成旨在自动生成医疗影像中的找到结果。大多数现有方法只是专注于图像数据，忽略了可以 accessible 的患者信息。在这篇论文中，我们提出了一种新的多modal deep neural network框架，用于生成胸部X射线报告，并将结构化患者数据，如生物指标和症状，与不结构化医疗记录相结合。我们提出了一种 conditioned cross-multi-head attention模块，以融合这些不同数据模式，跨越视觉和文本数据之间的semantic gap。实验结果表明，通过使用更多的modalities，可以获得显著提高。特别是，我们的模型在ROUGE-L指标上的表现比 relevante state-of-the-art 模型更高。此外，我们还使用了人类评估和临床semantic相似度测量，与word-overlap指标共同进行深入的量化分析。人类评估，由美国医学会资具认证的放射学专家进行评估，表明模型能够准确地确定高级结果，但也表明需要进一步改进，以捕捉细节和临床上下文。
</details></li>
</ul>
<hr>
<h2 id="Combining-EEG-and-NLP-Features-for-Predicting-Students’-Lecture-Comprehension-using-Ensemble-Classification"><a href="#Combining-EEG-and-NLP-Features-for-Predicting-Students’-Lecture-Comprehension-using-Ensemble-Classification" class="headerlink" title="Combining EEG and NLP Features for Predicting Students’ Lecture Comprehension using Ensemble Classification"></a>Combining EEG and NLP Features for Predicting Students’ Lecture Comprehension using Ensemble Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11088">http://arxiv.org/abs/2311.11088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phantharach Natnithikarat, Theerawit Wilaiprasitporn, Supavit Kongwudhikunakorn</li>
<li>for: 这个研究用于探讨学生在课堂教学中的理解水平，并提出一种分类框架来预测学生在两个任务中的困惑和答案正确性。</li>
<li>methods: 这个研究使用了生物маркер和自然语言处理技术来检测学生的课堂理解水平。生物маркер技术用于检测学生的脑电活动，而自然语言处理技术用于分析学生的语言表达。这些技术结合使用以提取 инте格рирован的特征，以便更好地预测学生的理解水平。</li>
<li>results: 实验结果显示，这种分类框架可以比基eline更高的准确率预测学生的困惑和答案正确性。在两个任务中，这种框架的F1分数最高达0.65和0.78，表明使用这种方法可以提高分类性能。此外，还使用了学生自我报告的困惑评分作为一个Integrated特征，以进一步提高分类性能。<details>
<summary>Abstract</summary>
Electroencephalography (EEG) and Natural Language Processing (NLP) can be applied for education to measure students' comprehension in classroom lectures; currently, the two measures have been used separately. In this work, we propose a classification framework for predicting students' lecture comprehension in two tasks: (i) students' confusion after listening to the simulated lecture and (ii) the correctness of students' responses to the post-lecture assessment. The proposed framework includes EEG and NLP feature extraction, processing, and classification. EEG and NLP features are extracted to construct integrated features obtained from recorded EEG signals and sentence-level syntactic analysis, which provide information about specific biomarkers and sentence structures. An ensemble stacking classification method -- a combination of multiple individual models that produces an enhanced predictive model -- is studied to learn from the features to make predictions accurately. Furthermore, we also utilized subjective confusion ratings as another integrated feature to enhance classification performance. By doing so, experiment results show that this framework performs better than the baselines, which achieved F1 up to 0.65 for predicting confusion and 0.78 for predicting correctness, highlighting that utilizing this has helped improve the classification performance.
</details>
<details>
<summary>摘要</summary>
电子脑波图像（EEG）和自然语言处理（NLP）可以应用于教育，以测量学生在课堂讲解中的理解水平。目前，这两种测量方法都是分开使用的。在这项工作中，我们提议一种分类框架，用于预测学生在两个任务中的课堂理解水平：（i）学生听完模拟课程后的混乱程度，以及（ii）学生完成后评估测验中的答案正确性。提议的框架包括EEG和NLP特征提取、处理和分类。EEG和NLP特征都是从记录的EEG信号和句子水平语法分析中提取出来的，它们提供了特定的生物标志和句子结构信息。我们还利用了学生主观的混乱评分作为另一个整合特征，以提高分类性能。通过这样做，实验结果显示，这个框架比基线表现更好，其F1分数可达0.65，用于预测混乱，以及0.78，用于预测正确性，这表明使用这个框架可以提高分类性能。
</details></li>
</ul>
<hr>
<h2 id="ECLM-Efficient-Edge-Cloud-Collaborative-Learning-with-Continuous-Environment-Adaptation"><a href="#ECLM-Efficient-Edge-Cloud-Collaborative-Learning-with-Continuous-Environment-Adaptation" class="headerlink" title="ECLM: Efficient Edge-Cloud Collaborative Learning with Continuous Environment Adaptation"></a>ECLM: Efficient Edge-Cloud Collaborative Learning with Continuous Environment Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11083">http://arxiv.org/abs/2311.11083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Zhuang, Zhenzhe Zheng, Yunfeng Shao, Bingshuai Li, Fan Wu, Guihai Chen</li>
<li>For: The paper is written for developing an edge-cloud collaborative learning framework for rapid model adaptation in dynamic edge environments.* Methods: The paper proposes a novel block-level model decomposition design to decompose the original large cloud model into multiple combinable modules, and an end-to-end learning framework that incorporates the modular model design into an efficient model adaptation pipeline.* Results: The paper achieves significant improvements in model performance (18.89% accuracy increase) and resource efficiency (7.12x communication cost reduction) in adapting models to dynamic edge environments by efficiently collaborating the edge and the cloud models.<details>
<summary>Abstract</summary>
Pervasive mobile AI applications primarily employ one of the two learning paradigms: cloud-based learning (with powerful large models) or on-device learning (with lightweight small models). Despite their own advantages, neither paradigm can effectively handle dynamic edge environments with frequent data distribution shifts and on-device resource fluctuations, inevitably suffering from performance degradation. In this paper, we propose ECLM, an edge-cloud collaborative learning framework for rapid model adaptation for dynamic edge environments. We first propose a novel block-level model decomposition design to decompose the original large cloud model into multiple combinable modules. By flexibly combining a subset of the modules, this design enables the derivation of compact, task-specific sub-models for heterogeneous edge devices from the large cloud model, and the seamless integration of new knowledge learned on these devices into the cloud model periodically. As such, ECLM ensures that the cloud model always provides up-to-date sub-models for edge devices. We further propose an end-to-end learning framework that incorporates the modular model design into an efficient model adaptation pipeline including an offline on-cloud model prototyping and training stage, and an online edge-cloud collaborative adaptation stage. Extensive experiments over various datasets demonstrate that ECLM significantly improves model performance (e.g., 18.89% accuracy increase) and resource efficiency (e.g., 7.12x communication cost reduction) in adapting models to dynamic edge environments by efficiently collaborating the edge and the cloud models.
</details>
<details>
<summary>摘要</summary>
通用移动AI应用主要采用云基本学习（强大大模型）或设备内学习（轻量级小模型）两种学习方法。尽管它们各有优点，但是 neither paradigm can effectively handle动态边缘环境中的数据分布变化和设备资源波动， resulting in performance degradation. In this paper, we propose ECLM, an edge-cloud collaborative learning framework for rapid model adaptation in dynamic edge environments. We first propose a novel block-level model decomposition design to decompose the original large cloud model into multiple combinable modules. By flexibly combining a subset of the modules, this design enables the derivation of compact, task-specific sub-models for heterogeneous edge devices from the large cloud model, and the seamless integration of new knowledge learned on these devices into the cloud model periodically. As such, ECLM ensures that the cloud model always provides up-to-date sub-models for edge devices. We further propose an end-to-end learning framework that incorporates the modular model design into an efficient model adaptation pipeline including an offline on-cloud model prototyping and training stage, and an online edge-cloud collaborative adaptation stage. Extensive experiments over various datasets demonstrate that ECLM significantly improves model performance (e.g., 18.89% accuracy increase) and resource efficiency (e.g., 7.12x communication cost reduction) in adapting models to dynamic edge environments by efficiently collaborating the edge and the cloud models.
</details></li>
</ul>
<hr>
<h2 id="DSCom-A-Data-Driven-Self-Adaptive-Community-Based-Framework-for-Influence-Maximization-in-Social-Networks"><a href="#DSCom-A-Data-Driven-Self-Adaptive-Community-Based-Framework-for-Influence-Maximization-in-Social-Networks" class="headerlink" title="DSCom: A Data-Driven Self-Adaptive Community-Based Framework for Influence Maximization in Social Networks"></a>DSCom: A Data-Driven Self-Adaptive Community-Based Framework for Influence Maximization in Social Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11080">http://arxiv.org/abs/2311.11080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Zuo, Haojia Sun, Yongyi Hu, Jianxiong Guo, Xiaofeng Gao<br>for: This paper aims to address the data-driven version of influence maximization, where the diffusion model is not given and needs to be inferred from the history cascades.methods: The paper proposes a machine learning-based framework called DSCom, which leverages node attributes to estimate the closeness between connected nodes and overcome the influence overlap problem.results: The proposed algorithm is evaluated through empirical experiments with parameterized diffusion models based on real-world social networks, showing its efficiency and effectiveness.Here’s the Chinese version:for: 这篇论文主要解决了数据驱动版本的影响最大化问题，其中diffusion模型未提供，需要从历史扩散中推断。methods: 该论文提出了基于机器学习的DSCom框架，利用节点特征来估计连接节点的相互关系，并通过自similarity matrix来解决因果重叠问题。results: 该算法经验测试了基于实际社交网络的参数化扩散模型，证明其效率和有效性。<details>
<summary>Abstract</summary>
Influence maximization aims to find a subset of seeds that maximize the influence spread under a given budget. In this paper, we mainly address the data-driven version of this problem, where the diffusion model is not given but needs to be inferred from the history cascades. Several previous works have addressed this topic in a statistical way and provided efficient algorithms with theoretical guarantee. However, in their settings, though the diffusion parameters are inferred, they still need users to preset the diffusion model, which can be an intractable problem in real-world practices. In this paper, we reformulate the problem on the attributed network and leverage the node attributes to estimate the closeness between the connected nodes. Specifically, we propose a machine learning-based framework, named DSCom, to address this problem in a heuristic way. Under this framework, we first infer the users' relationship from the diffusion dataset through attention mechanism and then leverage spectral clustering to overcome the influence overlap problem in the lack of exact diffusion formula. Compared to the previous theoretical works, we carefully designed empirical experiments with parameterized diffusion models based on real-world social networks, which prove the efficiency and effectiveness of our algorithm.
</details>
<details>
<summary>摘要</summary>
“影响 maximization 目标是找到一个最大化影响的种子子集，在给定的预算下。在这篇论文中，我们主要关注数据驱动的版本问题，即 diffusion 模型不是给定的，而是从历史扩散中推断出来。先前的一些工作已经Addressed this topic in a statistical way, providing efficient algorithms with theoretical guarantee. However, in their settings, the diffusion parameters are inferred, but users still need to preset the diffusion model, which can be an intractable problem in real-world practices.在这篇论文中，我们将问题 reformulate 到 attributed network 上，并利用节点特征来估计连接节点之间的距离。specifically，我们提出了一种机器学习基于的框架，named DSCom，来解决这个问题。在这个框架下，我们首先通过注意力机制从扩散数据集中推断用户之间的关系，然后利用 спектраль聚类来超越扩散影响的问题，具有缺乏准确扩散方程的情况下。与先前的理论工作相比，我们在实际实验中谨慎地设计了参数化的扩散模型，基于实际的社交网络数据，这证明了我们的算法的效率和有效性。”
</details></li>
</ul>
<hr>
<h2 id="Adapters-A-Unified-Library-for-Parameter-Efficient-and-Modular-Transfer-Learning"><a href="#Adapters-A-Unified-Library-for-Parameter-Efficient-and-Modular-Transfer-Learning" class="headerlink" title="Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning"></a>Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11077">http://arxiv.org/abs/2311.11077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clifton Poth, Hannah Sterz, Indraneil Paul, Sukannya Purkayastha, Leon Engländer, Timo Imhof, Ivan Vulić, Sebastian Ruder, Iryna Gurevych, Jonas Pfeiffer</li>
<li>For: 这篇论文是为了推广更有效率的转移学习方法，提供了一个开源库（Adapters），可以实现参数高效和弹性的转移学习。* Methods: 这篇论文使用了10种多样化的adapter方法，并将它们集成到一个简单的接口中，提供了使用方便和可配置的方式。* Results: 论文透过评估这些adapter方法的性能，证明了这个库的可行性和弹性，并且比较了它们与传统 fine-tuning 方法的性能。<details>
<summary>Abstract</summary>
We introduce Adapters, an open-source library that unifies parameter-efficient and modular transfer learning in large language models. By integrating 10 diverse adapter methods into a unified interface, Adapters offers ease of use and flexible configuration. Our library allows researchers and practitioners to leverage adapter modularity through composition blocks, enabling the design of complex adapter setups. We demonstrate the library's efficacy by evaluating its performance against full fine-tuning on various NLP tasks. Adapters provides a powerful tool for addressing the challenges of conventional fine-tuning paradigms and promoting more efficient and modular transfer learning. The library is available via https://adapterhub.ml/adapters.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个名为“Adapter”的开源库，这个库可以实现parameter-efficient和modular的转移学习在大型自然语言模型中。我们统一了10种不同的adapter方法，并将它们集成到了一个简单的界面中，这使得研究者和实践者可以轻松地使用和自定义adapter。我们显示了这个库的性能，与完整的 fine-tuning 进行比较，并证明了它在不同的NLP任务中的表现。“Adapter”提供了一个强大的工具，用于解决传统 fine-tuning 方法的挑战，并促进更有效和自定义的转移学习。这个库可以在https://adapterhub.ml/adapters中下载。
</details></li>
</ul>
<hr>
<h2 id="Community-Aware-Efficient-Graph-Contrastive-Learning-via-Personalized-Self-Training"><a href="#Community-Aware-Efficient-Graph-Contrastive-Learning-via-Personalized-Self-Training" class="headerlink" title="Community-Aware Efficient Graph Contrastive Learning via Personalized Self-Training"></a>Community-Aware Efficient Graph Contrastive Learning via Personalized Self-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11073">http://arxiv.org/abs/2311.11073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuecheng Li, Yanming Hu, Lele Fu, Chuan Chen, Lei Yang, Zibin Zheng</li>
<li>for: The paper is written for community detection tasks in graph-structured data, and it proposes a novel framework called Community-aware Efficient Graph Contrastive Learning (CEGCL) to jointly learn community partition and node representations in an end-to-end manner.</li>
<li>methods: The proposed CEGCL framework uses a personalized self-training (PeST) strategy for unsupervised scenarios, which enables the model to capture precise community-level personalized information in a graph. Additionally, the aligned graph clustering (AlGC) is employed to obtain the community partition.</li>
<li>results: The paper demonstrates the effectiveness of the proposed CEGCL model for community detection both theoretically and experimentally. Extensive experimental results show that CEGCL exhibits state-of-the-art performance on three benchmark datasets with different scales.<details>
<summary>Abstract</summary>
In recent years, graph contrastive learning (GCL) has emerged as one of the optimal solutions for various supervised tasks at the node level. However, for unsupervised and structure-related tasks such as community detection, current GCL algorithms face difficulties in acquiring the necessary community-level information, resulting in poor performance. In addition, general contrastive learning algorithms improve the performance of downstream tasks by increasing the number of negative samples, which leads to severe class collision and unfairness of community detection. To address above issues, we propose a novel Community-aware Efficient Graph Contrastive Learning Framework (CEGCL) to jointly learn community partition and node representations in an end-to-end manner. Specifically, we first design a personalized self-training (PeST) strategy for unsupervised scenarios, which enables our model to capture precise community-level personalized information in a graph. With the benefit of the PeST, we alleviate class collision and unfairness without sacrificing the overall model performance. Furthermore, the aligned graph clustering (AlGC) is employed to obtain the community partition. In this module, we align the clustering space of our downstream task with that in PeST to achieve more consistent node embeddings. Finally, we demonstrate the effectiveness of our model for community detection both theoretically and experimentally. Extensive experimental results also show that our CEGCL exhibits state-of-the-art performance on three benchmark datasets with different scales.
</details>
<details>
<summary>摘要</summary>
Recently, graph contrastive learning (GCL) has emerged as one of the optimal solutions for various supervised tasks at the node level. However, for unsupervised and structure-related tasks such as community detection, current GCL algorithms have difficulty obtaining the necessary community-level information, resulting in poor performance. In addition, general contrastive learning algorithms improve the performance of downstream tasks by increasing the number of negative samples, which leads to severe class collision and unfairness of community detection. To address these issues, we propose a novel Community-aware Efficient Graph Contrastive Learning Framework (CEGCL) to jointly learn community partition and node representations in an end-to-end manner. Specifically, we first design a personalized self-training (PeST) strategy for unsupervised scenarios, which enables our model to capture precise community-level personalized information in a graph. With the benefit of the PeST, we alleviate class collision and unfairness without sacrificing the overall model performance. Furthermore, the aligned graph clustering (AlGC) is employed to obtain the community partition. In this module, we align the clustering space of our downstream task with that in PeST to achieve more consistent node embeddings. Finally, we demonstrate the effectiveness of our model for community detection both theoretically and experimentally. Extensive experimental results also show that our CEGCL exhibits state-of-the-art performance on three benchmark datasets with different scales.
</details></li>
</ul>
<hr>
<h2 id="SBTRec-A-Transformer-Framework-for-Personalized-Tour-Recommendation-Problem-with-Sentiment-Analysis"><a href="#SBTRec-A-Transformer-Framework-for-Personalized-Tour-Recommendation-Problem-with-Sentiment-Analysis" class="headerlink" title="SBTRec- A Transformer Framework for Personalized Tour Recommendation Problem with Sentiment Analysis"></a>SBTRec- A Transformer Framework for Personalized Tour Recommendation Problem with Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11071">http://arxiv.org/abs/2311.11071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngai Lam Ho, Roy Ka-Wei Lee, Kwan Hui Lim</li>
<li>for: 提供个性化的旅游点赞推荐，帮助旅行者更好地规划旅游路线和探索当地热点。</li>
<li>methods: 基于BERT的旅游点序列推荐算法，利用用户检查到和上传照片来理解POI访问和距离之间的关系，并通过情感分析提高推荐准确性。</li>
<li>results: 与基eline算法进行比较，SBTRec实现了平均F1分数61.45%，表明其在序列预测任务中表现出色。<details>
<summary>Abstract</summary>
When traveling to an unfamiliar city for holidays, tourists often rely on guidebooks, travel websites, or recommendation systems to plan their daily itineraries and explore popular points of interest (POIs). However, these approaches may lack optimization in terms of time feasibility, localities, and user preferences. In this paper, we propose the SBTRec algorithm: a BERT-based Trajectory Recommendation with sentiment analysis, for recommending personalized sequences of POIs as itineraries. The key contributions of this work include analyzing users' check-ins and uploaded photos to understand the relationship between POI visits and distance. We introduce SBTRec, which encompasses sentiment analysis to improve recommendation accuracy by understanding users' preferences and satisfaction levels from reviews and comments about different POIs. Our proposed algorithms are evaluated against other sequence prediction methods using datasets from 8 cities. The results demonstrate that SBTRec achieves an average F1 score of 61.45%, outperforming baseline algorithms.   The paper further discusses the flexibility of the SBTRec algorithm, its ability to adapt to different scenarios and cities without modification, and its potential for extension by incorporating additional information for more reliable predictions. Overall, SBTRec provides personalized and relevant POI recommendations, enhancing tourists' overall trip experiences. Future work includes fine-tuning personalized embeddings for users, with evaluation of users' comments on POIs,~to further enhance prediction accuracy.
</details>
<details>
<summary>摘要</summary>
The paper further discusses the flexibility of the SBTRec algorithm, its ability to adapt to different scenarios and cities without modification, and its potential for extension by incorporating additional information for more reliable predictions. Overall, SBTRec provides personalized and relevant POI recommendations, enhancing tourists' overall trip experiences. Future work includes fine-tuning personalized embeddings for users, with evaluation of users' comments on POIs, to further enhance prediction accuracy.Translated into Simplified Chinese:旅游者在前往未知城市度假时，经常依靠旅游指南、旅游网站或推荐系统，以制定日程和探索流行点 интере斯（POIs）。然而，这些方法可能缺乏时间可行性、地点和用户偏好的优化。在这篇论文中，我们提出了SBTRec算法：一种基于BERT的 trajectory recommendation，具有情感分析，用于建议个性化POIs的顺序。我们的主要贡献包括分析用户检查到和上传照片，以理解POI访问和距离之间的关系。我们引入SBTRec，它包括情感分析，以提高推荐准确性。我们的提出的算法与其他序列预测方法进行比较，使用8个城市的数据。结果显示，SBTRec实现了平均F1分数为61.45%，超过基线算法。论文进一步讨论了SBTRec算法的灵活性，它可以适应不同的情况和城市，无需修改。此外，它还有扩展的潜在，通过添加更多信息，以提高预测的可靠性。总的来说，SBTRec提供了个性化和相关的POI推荐，提高旅游者的总体旅行体验。未来的工作包括个性化用户的嵌入调整，通过评估用户对POIs的评论，进一步提高预测准确性。
</details></li>
</ul>
<hr>
<h2 id="AIMS-EREA-–-A-framework-for-AI-accelerated-Innovation-of-Materials-for-Sustainability-–-for-Environmental-Remediation-and-Energy-Applications"><a href="#AIMS-EREA-–-A-framework-for-AI-accelerated-Innovation-of-Materials-for-Sustainability-–-for-Environmental-Remediation-and-Energy-Applications" class="headerlink" title="AIMS-EREA – A framework for AI-accelerated Innovation of Materials for Sustainability – for Environmental Remediation and Energy Applications"></a>AIMS-EREA – A framework for AI-accelerated Innovation of Materials for Sustainability – for Environmental Remediation and Energy Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11060">http://arxiv.org/abs/2311.11060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarson Roy Pratihar, Deepesh Pai, Manaswita Nag</li>
<li>For: 可以用于综合考虑多种可能性和结构，快速找到适合的绿色材料，以满足可持续发展的能源和环境修复应用。* Methods: 基于密度函数理论（DFT）和其他理论，以及人工智能技术，可以快速和效率地对可能性进行筛选和预测，从而降低实验室synthesis和分析过程中的努力和成本。* Results: 通过 combing best of breed of Material Science theory with the power of Generative AI, 可以快速和高效地找到适合的绿色材料，并且可以避免生产危险副产品的可能性。<details>
<summary>Abstract</summary>
Many environmental remediation and energy applications (conversion and storage) for sustainability need design and development of green novel materials. Discovery processes of such novel materials are time taking and cumbersome due to large number of possible combinations and permutations of materials structures. Often theoretical studies based on Density Functional Theory (DFT) and other theories, coupled with Simulations are conducted to narrow down sample space of candidate materials, before conducting laboratory-based synthesis and analytical process. With the emergence of artificial intelligence (AI), AI techniques are being tried in this process too to ease out simulation time and cost. However tremendous values of previously published research from various parts of the world are still left as labor-intensive manual effort and discretion of individual researcher and prone to human omissions. AIMS-EREA is our novel framework to blend best of breed of Material Science theory with power of Generative AI to give best impact and smooth and quickest discovery of material for sustainability. This also helps to eliminate the possibility of production of hazardous residues and bye-products of the reactions. AIMS-EREA uses all available resources -- Predictive and Analytical AI on large collection of chemical databases along with automated intelligent assimilation of deep materials knowledge from previously published research works through Generative AI. We demonstrate use of our own novel framework with an example, how this framework can be successfully applied to achieve desired success in development of thermoelectric material for waste heat conversion.
</details>
<details>
<summary>摘要</summary>
多种环境恢复和能源应用（转化和存储）需要设计和开发绿色新材料。发现这些新材料的过程是时间consuming和复杂，因为有很多可能的组合和排序结构。经常通过密度函数理论（DFT）和其他理论，加上计算机模拟，来缩小实验室合成和分析过程中的样本空间。随着人工智能（AI）的出现，AI技术也在这个过程中使用，以减少计算时间和成本。然而，大量前期发表的研究成果仍然受到劳动密集和个人研究者的主观性的影响，容易出现人类缺失。我们的AIMS-EREA框架通过融合材料科学理论和生成AI的力量，为可持续发展提供了最佳影响和最快速的材料发现。此外，它还可以消除生产过程中可能产生的危险副产品。AIMS-EREA利用了所有可用资源——预测和分析AI在大量化学数据库中，以及自动智能吸收深入材料知识从前期发表的研究作品中。我们示例如如何使用我们的框架成功应用于废热电转换材料的开发。
</details></li>
</ul>
<hr>
<h2 id="Designing-Interpretable-ML-System-to-Enhance-Trustworthy-AI-in-Healthcare-A-Systematic-Review-of-the-Last-Decade-to-A-Proposed-Robust-Framework"><a href="#Designing-Interpretable-ML-System-to-Enhance-Trustworthy-AI-in-Healthcare-A-Systematic-Review-of-the-Last-Decade-to-A-Proposed-Robust-Framework" class="headerlink" title="Designing Interpretable ML System to Enhance Trustworthy AI in Healthcare: A Systematic Review of the Last Decade to A Proposed Robust Framework"></a>Designing Interpretable ML System to Enhance Trustworthy AI in Healthcare: A Systematic Review of the Last Decade to A Proposed Robust Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11055">http://arxiv.org/abs/2311.11055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elham Nasarian, Roohallah Alizadehsani, U. Rajendra Acharyac, d Kwok-Leung Tsui<br>for: This paper aims to review and discuss the processes and challenges of interpretable machine learning (IML) and explainable AI (XAI) in healthcare, with a focus on quality control and the importance of robust interpretability.methods: The paper uses a systematic literature review approach, searching PubMed, Scopus, and Web of Science databases using specific strings to identify relevant studies. The IML process is classified into three stages: data pre-processing interpretability, interpretable modeling, and post-processing interpretability.results: The paper provides experimental results to establish the importance of robust interpretability in healthcare, and offers insights for creating communicable clinician-AI tools. The survey also introduces a step-by-step roadmap for implementing XAI in clinical applications, addressing existing gaps and acknowledging XAI model limitations.<details>
<summary>Abstract</summary>
AI-based medical technologies, including wearables, telemedicine, LLMs, and digital care twins, significantly impact healthcare. Ensuring AI results are accurate and interpretable is crucial, especially for clinicians. This paper reviews processes and challenges of interpretable ML (IML) and explainable AI (XAI) in healthcare. Objectives include reviewing XAI processes, methods, applications, and challenges, with a focus on quality control. The IML process is classified into data pre-processing interpretability, interpretable modeling, and post-processing interpretability. The paper aims to establish the importance of robust interpretability in healthcare through experimental results, providing insights for creating communicable clinician-AI tools. Research questions, eligibility criteria, and goals were identified following PRISMA and PICO methods. PubMed, Scopus, and Web of Science were systematically searched using specific strings. The survey introduces a step-by-step roadmap for implementing XAI in clinical applications, addressing existing gaps and acknowledging XAI model limitations.
</details>
<details>
<summary>摘要</summary>
人工智能技术在医疗领域的应用，包括智能服务器、远程医疗、语言模型和数字护理双，对医疗业产生了深远的影响。为确保人工智能结果准确和可解释，特别是 для临床医生，在医疗领域中确保可解释的机器学习（IML）和可解释人工智能（XAI）的过程和挑战是非常重要。本文将对可解释ML（IML）和可解释人工智能（XAI）在医疗领域的过程和挑战进行了评估。包括数据预处理可解释、可解释模型和后处理可解释在内的IML过程将被分类。本文的目标是通过实验结果证明可Robust可解释在医疗领域的重要性，并为创建可通信的医生-AI工具提供了新的发现。根据PRISMA和PICO方法，我们定义了研究问题、适用性标准和目标。通过对PubMed、Scopus和Web of Science等数据库进行系统性搜索，我们使用特定的搜索串检索相关文献。本文将提供一个步骤并进的路线图，以帮助实施XAI在临床应用中，并解决现有的坑害和XAI模型的限制。
</details></li>
</ul>
<hr>
<h2 id="Orca-2-Teaching-Small-Language-Models-How-to-Reason"><a href="#Orca-2-Teaching-Small-Language-Models-How-to-Reason" class="headerlink" title="Orca 2: Teaching Small Language Models How to Reason"></a>Orca 2: Teaching Small Language Models How to Reason</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11045">http://arxiv.org/abs/2311.11045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agrawal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, Ahmed Awadallah</li>
<li>for: 本研究旨在探讨如何通过改进训练信号来提高小型语言模型（LM）的逻辑能力。</li>
<li>methods: 研究人员使用了不同的解释Trace来训练小型LM，并研究了不同的解释策略（如步骤解释、记忆然后生成、记忆然后解释生成等），以帮助模型选择最佳解释策略以适应不同任务。</li>
<li>results: Orca 2在15个多样化的 benchmarck 上表现出优于同类型模型和大型模型的 Zero-shot 表现，并在复杂任务中达到了与大型模型相当或更好的水平。<details>
<summary>Abstract</summary>
Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. We open-source Orca 2 to encourage further research on the development, evaluation, and alignment of smaller LMs.
</details>
<details>
<summary>摘要</summary>
鲸鱼1从 ricSignals 学习，如解释迹象，使其在 BigBench Hard 和 AGIEval 上表现出色。在 Orca 2 中，我们继续探索如何提高训练信号可以提高小LMs 的理解能力。研究小LMs 的训练通常采用了模仿学习，以复制更强大的模型的输出。我们认为过分强调模仿可能会限制小LMs 的潜力。我们尝试教育小LMs 使用不同的解决方案来解决不同任务，可能与更大的模型不同。例如，更大的模型可能会提供一个直接回答复杂任务的方法，而小LMs 可能没有同样的容量。在 Orca 2 中，我们教育模型多种理解技巧（步骤、回忆然后生成、回忆然后生成、直接回答等）。更重要的是，我们努力帮助模型学习选择每个任务最有效的解决方案。我们使用了15种多样化的benchmark（相当于100个任务和36,000个唯一提问）来评估 Orca 2。结果显示，Orca 2 在复杂任务中表现出色，并在零容量情况下与5-10倍大的模型相当或更好的表现。我们将 Orca 2 开源，以便进一步研究小LMs 的发展、评估和对齐。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Data-Generation-for-Bridging-Sim2Real-Gap-in-a-Production-Environment"><a href="#Synthetic-Data-Generation-for-Bridging-Sim2Real-Gap-in-a-Production-Environment" class="headerlink" title="Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment"></a>Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11039">http://arxiv.org/abs/2311.11039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parth Rawal, Mrunal Sompura, Wolfgang Hintze</li>
<li>for: 这篇论文的目的是提出一种基于模拟数据生成的方法，用于帮助机器人在生产环境中使用人工智能。</li>
<li>methods: 论文使用的方法包括基于模拟数据的生成和组合，以帮助减少实际环境和模拟环境之间的差异。</li>
<li>results: 试验结果表明，使用这些基本方法的组合可以在生产环境中减少模拟到现实之间的差异，从而提高机器人在生产中的表现。<details>
<summary>Abstract</summary>
Synthetic data is being used lately for training deep neural networks in computer vision applications such as object detection, object segmentation and 6D object pose estimation. Domain randomization hereby plays an important role in reducing the simulation to reality gap. However, this generalization might not be effective in specialized domains like a production environment involving complex assemblies. Either the individual parts, trained with synthetic images, are integrated in much larger assemblies making them indistinguishable from their counterparts and result in false positives or are partially occluded just enough to give rise to false negatives. Domain knowledge is vital in these cases and if conceived effectively while generating synthetic data, can show a considerable improvement in bridging the simulation to reality gap. This paper focuses on synthetic data generation procedures for parts and assemblies used in a production environment. The basic procedures for synthetic data generation and their various combinations are evaluated and compared on images captured in a production environment, where results show up to 15% improvement using combinations of basic procedures. Reducing the simulation to reality gap in this way can aid to utilize the true potential of robot assisted production using artificial intelligence.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Center-Audio-Video-Intelligence-on-Device-DAVID-–-An-Edge-AI-Platform-for-Smart-Toys"><a href="#Data-Center-Audio-Video-Intelligence-on-Device-DAVID-–-An-Edge-AI-Platform-for-Smart-Toys" class="headerlink" title="Data Center Audio&#x2F;Video Intelligence on Device (DAVID) – An Edge-AI Platform for Smart-Toys"></a>Data Center Audio&#x2F;Video Intelligence on Device (DAVID) – An Edge-AI Platform for Smart-Toys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11030">http://arxiv.org/abs/2311.11030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Cosache, Francisco Salgado, Cosmin Rotariu, George Sterpu, Rishabh Jain, Peter Corcoran</li>
<li>for: 这份论文主要用于介绍一种 Edge AI 平台，即 DAVID Smart-Toy 平台，该平台包含了高级低功耗数据处理的神经推理模型，并与相关的图像或音频感知器一起嵌入在设备中。</li>
<li>methods: 该平台使用了神经推理模型进行数据处理，并提供了在设备内部进行文本识别和语音生成功能。</li>
<li>results: 该平台可以根据用户的语音和面部表达进行识别和 интерпретаción，并且具有嵌入式的数据保护功能，以保障用户的隐私。<details>
<summary>Abstract</summary>
An overview is given of the DAVID Smart-Toy platform, one of the first Edge AI platform designs to incorporate advanced low-power data processing by neural inference models co-located with the relevant image or audio sensors. There is also on-board capability for in-device text-to-speech generation. Two alternative embodiments are presented: a smart Teddy-bear, and a roving dog-like robot. The platform offers a speech-driven user interface and can observe and interpret user actions and facial expressions via its computer vision sensor node. A particular benefit of this design is that no personally identifiable information passes beyond the neural inference nodes thus providing inbuilt compliance with data protection regulations.
</details>
<details>
<summary>摘要</summary>
TEXT这里提供了DAVID智能玩具平台的总览，这是首先采用进步低功耗神经推论模型与相应的图像或语音感应器集成的 Edge AI 平台设计。它还具有内置的文本转语音功能。这两个版本中的一个是聪明的 teddy bear，另一个是一只行走的狗like 机器人。这个平台具有语音驱动的用户界面，可以通过计算机视觉感应器监测和解读用户的动作和表情。特别的是，这个设计不会将个人识别信息传递到神经推论节点以外，因此提供了内置的数据保护规定的实现。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Geometric-Data-Augmentations-to-Mitigate-Distribution-Shifts-in-Pollen-Classification-from-Microscopic-Images"><a href="#Geometric-Data-Augmentations-to-Mitigate-Distribution-Shifts-in-Pollen-Classification-from-Microscopic-Images" class="headerlink" title="Geometric Data Augmentations to Mitigate Distribution Shifts in Pollen Classification from Microscopic Images"></a>Geometric Data Augmentations to Mitigate Distribution Shifts in Pollen Classification from Microscopic Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11029">http://arxiv.org/abs/2311.11029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nam Cao, Olga Saukh</li>
<li>for: 本研究旨在解决机器学习模型在真实应用场景中的精度下降问题，具体是对野外采集的蜕花粉样图像使用低成本摄像头摄取器进行分类。</li>
<li>methods: 我们利用了领域知识，即形态特征对准精确蜕花识别非常重要，并引入了两种新的几何图像增强技术来减少模型在训练和测试数据集之间的准确度差距。特别是，我们表明了Tenengrad和ImageToSketch筛选器能够很好地补做形态和文本信息，而不会让模型受到不重要的细节的干扰。</li>
<li>results: 我们进行了广泛的评估，并证明了 geometric 增强技术可以在不同的模型架构上提供一定的改进，其中最高达14%。此外，我们还进行了减少采集的滤波器和图像增强器的综合评估，并证明了我们的几何增强技术在文献中的评分最高。<details>
<summary>Abstract</summary>
Distribution shifts are characterized by differences between the training and test data distributions. They can significantly reduce the accuracy of machine learning models deployed in real-world scenarios. This paper explores the distribution shift problem when classifying pollen grains from microscopic images collected in the wild with a low-cost camera sensor. We leverage the domain knowledge that geometric features are highly important for accurate pollen identification and introduce two novel geometric image augmentation techniques to significantly narrow the accuracy gap between the model performance on the train and test datasets. In particular, we show that Tenengrad and ImageToSketch filters are highly effective to balance the shape and texture information while leaving out unimportant details that may confuse the model. Extensive evaluations on various model architectures demonstrate a consistent improvement of the model generalization to field data of up to 14% achieved by the geometric augmentation techniques when compared to a wide range of standard image augmentations. The approach is validated through an ablation study using pollen hydration tests to recover the shape of dry pollen grains. The proposed geometric augmentations also receive the highest scores according to the affinity and diversity measures from the literature.
</details>
<details>
<summary>摘要</summary>
分布Shift问题是指训练和测试数据之间的差异，可能导致机器学习模型在实际应用场景中的精度下降。本文探讨了在野外采集的杂花粉胞微scopic图像中的分布Shift问题，并利用域知识，认为 геометрические特征对于准确的杂花识别非常重要。我们引入了两种新的地形图像增强技术，以减少模型在训练和测试数据集之间的精度差距。特别是，我们表明了Tenengrad和ImageToSketch筛选器在平衡形态和文本信息的同时，留下无关重要信息的能力，可以提高模型的总体性能。我们进行了对多种模型架构的广泛评估，并证明了地形增强技术可以在 field data 上提高模型总体性能达到14%。我们还进行了一项ablation study，用气压测试来恢复干燥杂花粉胞的形状，以验证我们的方法。此外，我们的地形增强技术也在文献中得到了最高的评分。
</details></li>
</ul>
<hr>
<h2 id="Lesion-Search-with-Self-supervised-Learning"><a href="#Lesion-Search-with-Self-supervised-Learning" class="headerlink" title="Lesion Search with Self-supervised Learning"></a>Lesion Search with Self-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11014">http://arxiv.org/abs/2311.11014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kristin Qi, Jiali Cheng, Daniel Haehn</li>
<li>for: 帮助临床专业人员更快地查找相似图像，不需要手动标注。</li>
<li>methods: 使用自动学习的对比学习（SimCLR）来实现内容基于图像检索（CBIR），并使用Generalized-mean（GeM）池化和L2normalization来分类疾病类型和检索相似图像。</li>
<li>results: 实现了提高的表现。 Additionally, 开发了一个开源的图像分析和检索应用程序，易于集成，可能对临床专业人员的日常活动产生潜在支持。<details>
<summary>Abstract</summary>
Content-based image retrieval (CBIR) with self-supervised learning (SSL) accelerates clinicians' interpretation of similar images without manual annotations. We develop a CBIR from the contrastive learning SimCLR and incorporate a generalized-mean (GeM) pooling followed by L2 normalization to classify lesion types and retrieve similar images before clinicians' analysis. Results have shown improved performance. We additionally build an open-source application for image analysis and retrieval. The application is easy to integrate, relieving manual efforts and suggesting the potential to support clinicians' everyday activities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multiple-View-Geometry-Transformers-for-3D-Human-Pose-Estimation"><a href="#Multiple-View-Geometry-Transformers-for-3D-Human-Pose-Estimation" class="headerlink" title="Multiple View Geometry Transformers for 3D Human Pose Estimation"></a>Multiple View Geometry Transformers for 3D Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10983">http://arxiv.org/abs/2311.10983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziwei Liao, Jialiang Zhu, Chunyu Wang, Han Hu, Steven L. Waslander</li>
<li>for: 提高多视图3D人姿估计中Transformers的3D理解能力</li>
<li>methods: 提出了一种新的混合模型MVGFormer，包括不可变的geometry模块和可变的appearance模块，可以同时处理视角依赖的3D任务和图像信号</li>
<li>results: 对于域内和域外设置，模型均表现出优于当前状态艺法，特别是在域外设置下表现出了明显的优势，并且可以在新的摄像头和几何学上进行普适化。<details>
<summary>Abstract</summary>
In this work, we aim to improve the 3D reasoning ability of Transformers in multi-view 3D human pose estimation. Recent works have focused on end-to-end learning-based transformer designs, which struggle to resolve geometric information accurately, particularly during occlusion. Instead, we propose a novel hybrid model, MVGFormer, which has a series of geometric and appearance modules organized in an iterative manner. The geometry modules are learning-free and handle all viewpoint-dependent 3D tasks geometrically which notably improves the model's generalization ability. The appearance modules are learnable and are dedicated to estimating 2D poses from image signals end-to-end which enables them to achieve accurate estimates even when occlusion occurs, leading to a model that is both accurate and generalizable to new cameras and geometries. We evaluate our approach for both in-domain and out-of-domain settings, where our model consistently outperforms state-of-the-art methods, and especially does so by a significant margin in the out-of-domain setting. We will release the code and models: https://github.com/XunshanMan/MVGFormer.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们目标是提高 transformer 在多视图三维人姿估计中的3D理解能力。先前的工作主要集中于终端学习基于 transformer 的设计，它们在 occlusion 时具有准确地理信息的解决能力有限。相比之下，我们提议一种新的混合模型，MVGFormer，它包括一系列具有学习自由的geometry模块和学习端到端的 appearance模块，这些模块在执行循环方式下组织。geometry模块负责所有视点依赖的3D任务，可以大幅提高模型的泛化能力。appearance模块通过 directly 从图像信号中学习2D姿势，可以在 occlusion 时达到高度准确的估计，导致模型具有高度准确和泛化性。我们在域内和域外设置下评估了我们的方法，其中我们的方法在域外设置下一直保持领先，特别是在 occlusion 情况下，我们的方法表现出了明显的优势。我们将在 GitHub 上发布代码和模型：https://github.com/XunshanMan/MVGFormer。
</details></li>
</ul>
<hr>
<h2 id="HungerGist-An-Interpretable-Predictive-Model-for-Food-Insecurity"><a href="#HungerGist-An-Interpretable-Predictive-Model-for-Food-Insecurity" class="headerlink" title="HungerGist: An Interpretable Predictive Model for Food Insecurity"></a>HungerGist: An Interpretable Predictive Model for Food Insecurity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10953">http://arxiv.org/abs/2311.10953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongsu Ahn, Muheng Yan, Yu-Ru Lin, Zian Wang<br>for: This paper aims to address the critical need for advanced early warning systems to combat escalating food insecurity in Africa, which is caused by factors such as war, climate change, and poverty.methods: The paper introduces a multi-task deep learning model called “HungerGist” that utilizes news texts and natural language processing (NLP) techniques to analyze and predict food insecurity.results: The model outperforms the baseline method trained on both traditional risk factors and human-curated keywords, and has the ability to detect critical texts that contain interpretable signals known as “gists.” Additionally, the approach has the potential to reveal latent factors that would otherwise remain concealed in unstructured texts.<details>
<summary>Abstract</summary>
The escalating food insecurity in Africa, caused by factors such as war, climate change, and poverty, demonstrates the critical need for advanced early warning systems. Traditional methodologies, relying on expert-curated data encompassing climate, geography, and social disturbances, often fall short due to data limitations, hindering comprehensive analysis and potential discovery of new predictive factors. To address this, this paper introduces "HungerGist", a multi-task deep learning model utilizing news texts and NLP techniques. Using a corpus of over 53,000 news articles from nine African countries over four years, we demonstrate that our model, trained solely on news data, outperforms the baseline method trained on both traditional risk factors and human-curated keywords. In addition, our method has the ability to detect critical texts that contain interpretable signals known as "gists." Moreover, our examination of these gists indicates that this approach has the potential to reveal latent factors that would otherwise remain concealed in unstructured texts.
</details>
<details>
<summary>摘要</summary>
“非洲的食品不安全升级，由于战争、气候变化和贫困等因素，表明了高度需要先进早期警示系统。传统的方法，依靠专家手动维护的数据，包括气候、地理和社会冲击，经常因数据限制而受到限制，阻碍了全面分析和潜在的新预测因素的发现。为解决这个问题，本文介绍了“饥饿精”，一种多任务深度学习模型，使用新闻文本和自然语言处理技术。使用9个非洲国家的4年新闻文章 corps（总计53,000篇），我们表明了我们的模型，通过新闻数据进行训练，比基eline方法（基于传统风险因素和人工标记）更高效。此外，我们的方法还能探测关键的新闻文本，含有可解释的信号，称为“精”。此外，我们的研究表明，这种方法有潜在的发现隐藏在未结构化文本中的因素的潜力。”
</details></li>
</ul>
<hr>
<h2 id="RecExplainer-Aligning-Large-Language-Models-for-Recommendation-Model-Interpretability"><a href="#RecExplainer-Aligning-Large-Language-Models-for-Recommendation-Model-Interpretability" class="headerlink" title="RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability"></a>RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10947">http://arxiv.org/abs/2311.10947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, Xing Xie</li>
<li>for: 这个论文的目的是提出一种新的模型解释方法，使用大语言模型（LLM）作为庸服务器模型的代理模型，以便更好地理解和解释推荐模型的行为。</li>
<li>methods: 该方法使用了三种Alignment方法：行为对齐、意图对齐和混合对齐。行为对齐在语言空间中表示用户喜好和物品信息为文本，以学习推荐模型的行为；意图对齐在推荐模型的latent空间中工作，使用用户和物品表示来理解模型的行为；混合对齐组合了语言和latent空间进行对齐训练。</li>
<li>results: 经过测试，该方法能够有效地使LLMs理解推荐模型的行为，并生成高度可信的推荐解释。<details>
<summary>Abstract</summary>
Recommender systems are widely used in various online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often lack interpretability, making them less reliable and transparent for both users and developers. With the emergence of large language models (LLMs), we find that their capabilities in language expression, knowledge-aware reasoning, and instruction following are exceptionally powerful. Based on this, we propose a new model interpretation approach for recommender systems, by using LLMs as surrogate models and learn to mimic and comprehend target recommender models. Specifically, we introduce three alignment methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to learn the recommendation model's behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model's behavior; hybrid alignment combines both language and latent spaces for alignment training. To demonstrate the effectiveness of our methods, we conduct evaluation from two perspectives: alignment effect, and explanation generation ability on three public datasets. Experimental results indicate that our approach effectively enables LLMs to comprehend the patterns of recommendation models and generate highly credible recommendation explanations.
</details>
<details>
<summary>摘要</summary>
推荐系统在线服务中广泛应用，尤其是基于嵌入式模型，因其可以表达复杂的信号。然而，这些模型通常缺乏可读性，使得用户和开发者对其不可靠和透明度感到不满。随着大语言模型（LLMs）的出现，我们发现它们在语言表达、知识感知和指令遵循方面具有极高的能力。基于这一点，我们提出了一种新的推荐系统模型解释方法，通过使用 LLMS 作为代理模型，并学习模仿和理解目标推荐模型的行为。 Specifically, we introduce three alignment methods: 行为对齐、意图对齐和混合对齐。行为对齐在语言空间中表示用户偏好和物品信息为文本，以学习推荐模型的行为;意图对齐在推荐模型的latent空间中使用用户和物品表示，以理解模型的行为;混合对齐将语言和latent空间进行对齐训练。为证明我们的方法的有效性，我们从两个角度进行评估：对齐效果和解释生成能力，并在三个公共数据集上进行实验。实验结果表明，我们的方法可以有效地使 LLMS 理解推荐模型的模式，并生成高可信度的推荐解释。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Bayes-Framework-for-Open-Domain-Dialogue-Generation"><a href="#An-Empirical-Bayes-Framework-for-Open-Domain-Dialogue-Generation" class="headerlink" title="An Empirical Bayes Framework for Open-Domain Dialogue Generation"></a>An Empirical Bayes Framework for Open-Domain Dialogue Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10945">http://arxiv.org/abs/2311.10945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Yang Lee, Kong Aik Lee, Woon-Seng Gan</li>
<li>for: 开发一个可以与人类用户进行意义性对话的开放领域对话机器人。</li>
<li>methods: 使用预训练语言模型和 bayesian 方法来建立一个 Bayesian 开放领域对话机器人。</li>
<li>results: BODEB  Framework 在多样性和协调性两个方面都达到了更好的结果，比较于Variational Frameworks。<details>
<summary>Abstract</summary>
To engage human users in meaningful conversation, open-domain dialogue agents are required to generate diverse and contextually coherent dialogue. Despite recent advancements, which can be attributed to the usage of pretrained language models, the generation of diverse and coherent dialogue remains an open research problem. A popular approach to address this issue involves the adaptation of variational frameworks. However, while these approaches successfully improve diversity, they tend to compromise on contextual coherence. Hence, we propose the Bayesian Open-domain Dialogue with Empirical Bayes (BODEB) framework, an empirical bayes framework for constructing an Bayesian open-domain dialogue agent by leveraging pretrained parameters to inform the prior and posterior parameter distributions. Empirical results show that BODEB achieves better results in terms of both diversity and coherence compared to variational frameworks.
</details>
<details>
<summary>摘要</summary>
要让人工智能对话机器人与人进行有意义的对话，开放领域对话代理需要生成多样化和上下文相关的对话。尽管最近的进步可以归功于预训练语言模型的使用，但生成多样化和上下文相关的对话仍然是一个开放的研究问题。一种受欢迎的方法来解决这个问题是适应变量框架。然而，这些方法通常会牺牲上下文相关性。因此，我们提出了概率开放领域对话框架（BODEB），一种基于预训练参数的 bayesian 开放领域对话代理。实际结果表明，BODEB 在多样性和上下文相关性两个方面都比变量框架更好。
</details></li>
</ul>
<hr>
<h2 id="Practical-Estimation-of-Ensemble-Accuracy"><a href="#Practical-Estimation-of-Ensemble-Accuracy" class="headerlink" title="Practical Estimation of Ensemble Accuracy"></a>Practical Estimation of Ensemble Accuracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10940">http://arxiv.org/abs/2311.10940</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Simi Haber, Yonatan Wexler</li>
<li>for: 这个论文是为了提出一种可行的方法来估计多个分类器的共同力量，而不需要标签信息，因此可以在无监督学习的大规模数据集上进行实际应用。</li>
<li>methods: 该方法基于一种新的 combinatorial bound 来估计分类器的集合准确率，这个 bound 可以在样本数量linear增长的情况下高效地近似，从而实现一个高效地寻找高 JOIN 准确率的分类器集合的方法。</li>
<li>results: 作者在popular的大规模人脸识别数据集上进行了实验，并证明了该方法的可行性和实用性，同时该方法可以在无监督学习的情况下提供高度的精度和稳定性。<details>
<summary>Abstract</summary>
Ensemble learning combines several individual models to obtain better generalization performance. In this work we present a practical method for estimating the joint power of several classifiers which differs from existing approaches by {\em not relying on labels}, hence enabling the work in unsupervised setting of huge datasets. It differs from existing methods which define a "diversity measure".   The heart of the method is a combinatorial bound on the number of mistakes the ensemble is likely to make. The bound can be efficiently approximated in time linear in the number of samples. Thus allowing an efficient search for a combination of classifiers that are likely to produce higher joint accuracy. Moreover, having the bound applicable to unlabeled data makes it both accurate and practical in modern setting of unsupervised learning. We demonstrate the method on popular large-scale face recognition datasets which provide a useful playground for fine-grain classification tasks using noisy data over many classes.   The proposed framework fits neatly in trending practices of unsupervised learning. It is a measure of the inherent independence of a set of classifiers not relying on extra information such as another classifier or labeled data.
</details>
<details>
<summary>摘要</summary>
ensemble learning可以提高总体化性能，在这个工作中我们提出了一种实用的方法，不同于现有方法，这种方法不需要标签，因此可以在无标签 dataset 上进行学习。它与现有方法不同，这种方法定义一个“多样性度量”。 ensemble learning 的核心思想是一种可以有效地估计多个分类器的结合力的 bounds，这个 bounds 可以在样本数 linear 时间内efficiently  aproximated。因此，可以有效地搜索一组可能 producen higher 的 joint accuracy 的分类器组合。此外，由于这个 bounds 适用于无标签数据，这使得它在现代无supervised learning 中具有准确性和实用性。我们在流行的大规模人脸识别 dataset 上进行了示例，这些 dataset 提供了一个有用的游戏场景，用于精细的分类任务，使用噪音数据。our proposed framework 适合当前流行的无supervised learning 做法，它是一种不依赖于其他分类器或标签数据的独立性度量。
</details></li>
</ul>
<hr>
<h2 id="Case-Repositories-Towards-Case-Based-Reasoning-for-AI-Alignment"><a href="#Case-Repositories-Towards-Case-Based-Reasoning-for-AI-Alignment" class="headerlink" title="Case Repositories: Towards Case-Based Reasoning for AI Alignment"></a>Case Repositories: Towards Case-Based Reasoning for AI Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10934">http://arxiv.org/abs/2311.10934</a></li>
<li>repo_url: None</li>
<li>paper_authors: K. J. Kevin Feng, Quan Ze, Chen, Inyoung Cheong, King Xia, Amy X. Zhang</li>
<li>for: 该论文旨在提出一种基于情感reasoning（CBR）的AI对齐方法，以解决在不同个人和社区中存在复杂和uncertain的社会问题时，AI系统如何对齐。</li>
<li>methods: 该论文提出了一种结合案例研究（CBR）的方法，包括收集seed例题（问题），召集领域专家来确定领域特定的关键维度，使用自然语言处理技术生成不同的案例，并通过公众参与来评判和改进案例。</li>
<li>results: 该论文认为，通过建立一个包含多种案例的案例库，可以帮助AI系统对齐，同时也可以提供一个让人们进行道德反思的平台。<details>
<summary>Abstract</summary>
Case studies commonly form the pedagogical backbone in law, ethics, and many other domains that face complex and ambiguous societal questions informed by human values. Similar complexities and ambiguities arise when we consider how AI should be aligned in practice: when faced with vast quantities of diverse (and sometimes conflicting) values from different individuals and communities, with whose values is AI to align, and how should AI do so? We propose a complementary approach to constitutional AI alignment, grounded in ideas from case-based reasoning (CBR), that focuses on the construction of policies through judgments on a set of cases. We present a process to assemble such a case repository by: 1) gathering a set of ``seed'' cases -- questions one may ask an AI system -- in a particular domain from discussions in online communities, 2) eliciting domain-specific key dimensions for cases through workshops with domain experts, 3) using LLMs to generate variations of cases not seen in the wild, and 4) engaging with the public to judge and improve cases. We then discuss how such a case repository could assist in AI alignment, both through directly acting as precedents to ground acceptable behaviors, and as a medium for individuals and communities to engage in moral reasoning around AI
</details>
<details>
<summary>摘要</summary>
法律、伦理和其他领域的教学经常作为智能机器的准则核心，面临着复杂和uncertain的社会问题，受到人类价值观的框架。在实践中，AI的Alignment也面临着类似的复杂性和uncertainty，问题是AI与 whose values should it align, and how should it do so? We propose a complementary approach to constitutional AI alignment, based on ideas from case-based reasoning (CBR), which focuses on constructing policies through judgments on a set of cases. We present a process to assemble such a case repository by:1. 收集域 especific seed cases（问题可以对AI系统提问）from online community discussions,2. 通过域专家组织工作shop elicit domain-specific key dimensions for cases,3. 使用LLMs生成不同from wild variations of cases,4. 与公众交流，评估和改进cases.然后，我们讨论了如何使用这个案例库来帮助AI的Alignment，包括直接作为行为的根据，以及作为人们和社区们在AI的伦理思考中的媒介。
</details></li>
</ul>
<hr>
<h2 id="Representing-visual-classification-as-a-linear-combination-of-words"><a href="#Representing-visual-classification-as-a-linear-combination-of-words" class="headerlink" title="Representing visual classification as a linear combination of words"></a>Representing visual classification as a linear combination of words</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10933">http://arxiv.org/abs/2311.10933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lotterlab/task_word_explainability">https://github.com/lotterlab/task_word_explainability</a></li>
<li>paper_authors: Shobhit Agarwal, Yevgeniy R. Semenov, William Lotter</li>
<li>for: 这个论文的目的是解释深度学习模型在医疗领域中的决策过程，并提供一种基于视觉和语言的解释策略。</li>
<li>methods: 该论文使用了一种视觉语言模型来identify语言基于描述器，并使用这些描述器来描述视觉分类任务的决策过程。</li>
<li>results: 研究发现，使用这种解释策略可以提供一些与临床知识相符的描述器，并且可以帮助非专业人员完成一些特殊的医疗任务。同时，研究还发现了公共数据集中存在一些”短cut连接”的问题。<details>
<summary>Abstract</summary>
Explainability is a longstanding challenge in deep learning, especially in high-stakes domains like healthcare. Common explainability methods highlight image regions that drive an AI model's decision. Humans, however, heavily rely on language to convey explanations of not only "where" but "what". Additionally, most explainability approaches focus on explaining individual AI predictions, rather than describing the features used by an AI model in general. The latter would be especially useful for model and dataset auditing, and potentially even knowledge generation as AI is increasingly being used in novel tasks. Here, we present an explainability strategy that uses a vision-language model to identify language-based descriptors of a visual classification task. By leveraging a pre-trained joint embedding space between images and text, our approach estimates a new classification task as a linear combination of words, resulting in a weight for each word that indicates its alignment with the vision-based classifier. We assess our approach using two medical imaging classification tasks, where we find that the resulting descriptors largely align with clinical knowledge despite a lack of domain-specific language training. However, our approach also identifies the potential for 'shortcut connections' in the public datasets used. Towards a functional measure of explainability, we perform a pilot reader study where we find that the AI-identified words can enable non-expert humans to perform a specialized medical task at a non-trivial level. Altogether, our results emphasize the potential of using multimodal foundational models to deliver intuitive, language-based explanations of visual tasks.
</details>
<details>
<summary>摘要</summary>
explainability 是深度学习中长期的挑战，尤其在医疗领域。常见的解释方法会强调图像区域，帮助人类理解 AI 模型做出的决定。然而，人类主要通过语言来传达解释，不仅包括 "where"，还包括 "what"。此外，大多数解释方法都是解释个别 AI 预测，而不是描述 AI 模型在总体上使用的特征。后者尤其有用于模型和数据集 Auditing，以及可能even 生成知识，因为 AI 在新任务中使用的情况在增加。在这里，我们提出了一种解释策略，使用视觉语言模型来identify图像中的语言基于描述器。我们利用预训练的共同 embedding空间，以图像和文本的 JOINT  embedding space，我们的方法可以将新的分类任务看作是一种线性组合的 слова，从而得到一个对应于每个单词的权重，这个权重指示单词与视觉基于分类器的Alignment。我们通过两个医疗成像分类任务进行评估，发现我们的方法可以获得与临床知识相当的描述器，即使没有域 específico 语言培训。然而，我们的方法还发现了公共数据集中的 "短cut 连接" 问题。为了评估函数性的解释度量，我们进行了一个 Pilot 读者研究，发现 AI 标识的单词可以帮助非专业人员在特殊医疗任务中达到非常轻量级的性能。总之，我们的结果强调了使用多Modal 基础模型来提供直观的语言基于解释，以便更好地理解视觉任务。
</details></li>
</ul>
<hr>
<h2 id="Cognitive-bias-in-large-language-models-Cautious-optimism-meets-anti-Panglossian-meliorism"><a href="#Cognitive-bias-in-large-language-models-Cautious-optimism-meets-anti-Panglossian-meliorism" class="headerlink" title="Cognitive bias in large language models: Cautious optimism meets anti-Panglossian meliorism"></a>Cognitive bias in large language models: Cautious optimism meets anti-Panglossian meliorism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10932">http://arxiv.org/abs/2311.10932</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Thorstad</li>
<li>for: 本文旨在探讨大语言模型中的偏见问题，特别是在不公正对待少数群体方面。</li>
<li>methods: 本文使用了现代语言模型评估方法，并对现有的偏见研究进行了探讨和分析。</li>
<li>results: 本文指出，现有的大语言模型可能存在一些偏见问题，并提出了一些途径来减少这些偏见。同时，本文也探讨了人类认知偏见的哲学意义以及模型偏见的原因。<details>
<summary>Abstract</summary>
Traditional discussions of bias in large language models focus on a conception of bias closely tied to unfairness, especially as affecting marginalized groups. Recent work raises the novel possibility of assessing the outputs of large language models for a range of cognitive biases familiar from research in judgment and decisionmaking. My aim in this paper is to draw two lessons from recent discussions of cognitive bias in large language models: cautious optimism about the prevalence of bias in current models coupled with an anti-Panglossian willingness to concede the existence of some genuine biases and work to reduce them. I draw out philosophical implications of this discussion for the rationality of human cognitive biases as well as the role of unrepresentative data in driving model biases.
</details>
<details>
<summary>摘要</summary>
传统的大语言模型偏见讨论围绕着不公正性，特别是对弱势群体产生影响。 current work 提出了评估大语言模型输出的多种认知偏见的新可能性，这些偏见 Familiar from research on judgment and decision-making. My goal in this paper is to draw two lessons from recent discussions of cognitive bias in large language models: cautious optimism about the prevalence of bias in current models, combined with an anti-Panglossian willingness to acknowledge the existence of some genuine biases and work to reduce them. I will draw out the philosophical implications of this discussion for human cognitive biases and the role of unrepresentative data in driving model biases.Note: "Panglossian" refers to the tendency to overlook or downplay the existence of negative aspects or biases, named after the character Dr. Pangloss in Voltaire's Candide. The term "anti-Panglossian" is used to describe a willingness to acknowledge and address such biases.
</details></li>
</ul>
<hr>
<h2 id="CAMRA-Copilot-for-AMR-Annotation"><a href="#CAMRA-Copilot-for-AMR-Annotation" class="headerlink" title="CAMRA: Copilot for AMR Annotation"></a>CAMRA: Copilot for AMR Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10928">http://arxiv.org/abs/2311.10928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jon Z. Cai, Shafiuddin Rehan Ahmed, Julia Bonn, Kristin Wright-Bettner, Martha Palmer, James H. Martin</li>
<li>for: 本研究旨在开发一个基于网络语言文本的抽象含义表示（AMR）创建工具——CAMRA（编程语言类型的 AMR 编辑器）。</li>
<li>methods: CAMRA 使用了一种新的方法，将 AMR 注释视为编程语言中的编程，通过利用编程语言的概念，帮助用户更好地理解和使用 AMR 注释。</li>
<li>results: CAMRA 可以快速和准确地生成 AMR 注释，并且可以帮助用户更好地理解和使用 Propbank 角色集。Here’s the breakdown of each point in English:</li>
<li>for: The paper is aimed at developing a tool for creating Abstract Meaning Representations (AMR) from natural language text, called CAMRA (a programming language-like AMR editor).</li>
<li>methods: CAMRA uses a novel approach that treats AMR annotation as coding in programming languages, leveraging the familiarity of programming paradigms to help users better understand and use AMR annotation.</li>
<li>results: CAMRA can quickly and accurately generate AMR annotation, and can also help users better understand and use Propbank role sets.<details>
<summary>Abstract</summary>
In this paper, we introduce CAMRA (Copilot for AMR Annotatations), a cutting-edge web-based tool designed for constructing Abstract Meaning Representation (AMR) from natural language text. CAMRA offers a novel approach to deep lexical semantics annotation such as AMR, treating AMR annotation akin to coding in programming languages. Leveraging the familiarity of programming paradigms, CAMRA encompasses all essential features of existing AMR editors, including example lookup, while going a step further by integrating Propbank roleset lookup as an autocomplete feature within the tool. Notably, CAMRA incorporates AMR parser models as coding co-pilots, greatly enhancing the efficiency and accuracy of AMR annotators. To demonstrate the tool's capabilities, we provide a live demo accessible at: https://camra.colorado.edu
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了 CAMRA（抽象含义表示语言编辑器），一款前沿的网页式工具，用于从自然语言文本中构建抽象含义表示（AMR）。CAMRA 采用了一种新的方法来进行深层次语义注释，将 AMR 注释视为编程语言中的编程。利用编程语言的概念互联，CAMRA 包含了所有现有 AMR 编辑器中的重要功能，包括示例查询，同时还添加了基于 Propbank 角色集Lookup 的自动完成功能。另外，CAMRA 还 integrate了 AMR 解析模型，以增强 AMR 注释的效率和准确性。为证明工具的能力，我们提供了一个实时示例，可以在以下链接中访问：https://camra.colorado.edu。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Product-Classification-for-Customs"><a href="#Explainable-Product-Classification-for-Customs" class="headerlink" title="Explainable Product Classification for Customs"></a>Explainable Product Classification for Customs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10922">http://arxiv.org/abs/2311.10922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eunji Lee, Sihyeon Kim, Sundong Kim, Soyeon Jung, Heeja Kim, Meeyoung Cha</li>
<li>for: 这研究旨在提供一个可解释的决策支持模型，帮助海关官员分配国际通用商品代码（HS码）。</li>
<li>methods: 该模型使用了机器学习算法，并提供了可解释的理由文档，以帮助海关官员理解和采纳建议。</li>
<li>results: 研究结果表明，该模型对925个困难子类别中的前三个建议准确率为93.9%。用户研究也表明，该算法提供的可解释建议和理由文档可以有效减少海关官员的分类审核时间和劳动。<details>
<summary>Abstract</summary>
The task of assigning internationally accepted commodity codes (aka HS codes) to traded goods is a critical function of customs offices. Like court decisions made by judges, this task follows the doctrine of precedent and can be nontrivial even for experienced officers. Together with the Korea Customs Service (KCS), we propose a first-ever explainable decision supporting model that suggests the most likely subheadings (i.e., the first six digits) of the HS code. The model also provides reasoning for its suggestion in the form of a document that is interpretable by customs officers. We evaluated the model using 5,000 cases that recently received a classification request. The results showed that the top-3 suggestions made by our model had an accuracy of 93.9\% when classifying 925 challenging subheadings. A user study with 32 customs experts further confirmed that our algorithmic suggestions accompanied by explainable reasonings, can substantially reduce the time and effort taken by customs officers for classification reviews.
</details>
<details>
<summary>摘要</summary>
customs offices 负责分配国际接受的商品代码（即HS码）是一项关键的任务。这种任务与法律判决一样，按照前例进行，而且对经验丰富的官员也可能是非常困难的。我们与韩国海关服务（KCS）合作，提出了一种首次出现的可解释决策支持模型，该模型建议商品的可能性最高的六位HS码。此外，模型还提供了其建议的解释，以文档的形式，可以由海关官员理解。我们对5000个最近获得分类请求的案例进行了评估，结果显示，我们模型的top3建议的准确率为93.9%，对925个困难的子标签进行分类。一个用户研究中，32名海关专家确认了我们的算法建议和可解释的理由，可以减少海关官员对分类审核的时间和努力。
</details></li>
</ul>
<hr>
<h2 id="Compact-and-Intuitive-Airfoil-Parameterization-Method-through-Physics-aware-Variational-Autoencoder"><a href="#Compact-and-Intuitive-Airfoil-Parameterization-Method-through-Physics-aware-Variational-Autoencoder" class="headerlink" title="Compact and Intuitive Airfoil Parameterization Method through Physics-aware Variational Autoencoder"></a>Compact and Intuitive Airfoil Parameterization Method through Physics-aware Variational Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10921">http://arxiv.org/abs/2311.10921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Eop Kang, Dawoon Lee, Kwanjung Yee</li>
<li>for:  optimize the design of high-performance aircraft airfoils</li>
<li>methods:  uses physics-aware variational autoencoder to parameterize airfoil shape</li>
<li>results:  produces smooth and non-intersecting airfoils with improved feasibility and intuitiveness<details>
<summary>Abstract</summary>
Airfoil shape optimization plays a critical role in the design of high-performance aircraft. However, the high-dimensional nature of airfoil representation causes the challenging problem known as the "curse of dimensionality". To overcome this problem, numerous airfoil parameterization methods have been developed, which can be broadly classified as polynomial-based and data-driven approaches. Each of these methods has desirable characteristics such as flexibility, parsimony, feasibility, and intuitiveness, but a single approach that encompasses all of these attributes has yet to be found. For example, polynomial-based methods struggle to balance parsimony and flexibility, while data-driven methods lack in feasibility and intuitiveness. In recent years, generative models, such as generative adversarial networks and variational autoencoders, have shown promising potential in airfoil parameterization. However, these models still face challenges related to intuitiveness due to their black-box nature. To address this issue, we developed a novel airfoil parameterization method using physics-aware variational autoencoder. The proposed method not only explicitly separates the generation of thickness and camber distributions to produce smooth and non-intersecting airfoils, thereby improving feasibility, but it also directly aligns its latent dimensions with geometric features of the airfoil, significantly enhancing intuitiveness. Finally, extensive comparative studies were performed to demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
高性能飞机设计中，飞机翼形参数化具有核心作用。然而，飞机翼形表示的维度高度带来“维度咒数”问题，这种问题很难解决。为了缓解这个问题，许多飞机翼形参数化方法已经开发出来，可以分为多项式基于的方法和数据驱动方法。每种方法都具有便利、简洁、可行性和直观性等特点，但是一种方法同时具有所有这些特点还没有被发现。例如，多项式基于的方法很难平衡简洁和灵活性，而数据驱动方法则缺乏可行性和直观性。在最近几年，生成模型，如生成敌方网络和变量自动编码器，在飞机翼形参数化中表现出了潜在的潜力。然而，这些模型仍然面临直观性问题，因为它们的黑盒结构。为了解决这个问题，我们开发了一种新的飞机翼形参数化方法，使用物理意识的变量自动编码器。我们的方法不仅能够明确分离thickness和camber分布的生成，从而提高可行性，而且直接将其缺失的维度与飞机翼形的几何特征直接对应，从而明显提高直观性。最后，我们进行了广泛的比较研究，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Understanding-and-Mitigating-Classification-Errors-Through-Interpretable-Token-Patterns"><a href="#Understanding-and-Mitigating-Classification-Errors-Through-Interpretable-Token-Patterns" class="headerlink" title="Understanding and Mitigating Classification Errors Through Interpretable Token Patterns"></a>Understanding and Mitigating Classification Errors Through Interpretable Token Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10920">http://arxiv.org/abs/2311.10920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael A. Hedderich, Jonas Fischer, Dietrich Klakow, Jilles Vreeken</li>
<li>For: 本文旨在Characterizing NLP类型错误，以提供global和可读的描述，以便改进NLP类型错误。* Methods: 本文提出了一种基于最小描述长度原则的方法，可以找到 Correct和错误预测之间的 TokenPatterns。* Results: 实验表明，该方法能够成功地找到ground truth，即使数据集很大且词汇表很大。在VQA和NERcase study中，该方法能够提供明确和行动可能的错误描述。<details>
<summary>Abstract</summary>
State-of-the-art NLP methods achieve human-like performance on many tasks, but make errors nevertheless. Characterizing these errors in easily interpretable terms gives insight into whether a classifier is prone to making systematic errors, but also gives a way to act and improve the classifier. We propose to discover those patterns of tokens that distinguish correct and erroneous predictions as to obtain global and interpretable descriptions for arbitrary NLP classifiers. We formulate the problem of finding a succinct and non-redundant set of such patterns in terms of the Minimum Description Length principle. Through an extensive set of experiments, we show that our method, Premise, performs well in practice. Unlike existing solutions, it recovers ground truth, even on highly imbalanced data over large vocabularies. In VQA and NER case studies, we confirm that it gives clear and actionable insight into the systematic errors made by NLP classifiers.
</details>
<details>
<summary>摘要</summary>
现代NLPT方法可以达到人类水平的性能在许多任务上，但仍会出错。Characterizing这些错误的方式可以提供权威的错误分布，并且可以用来改进分类器。我们提议使用token的 patrerns来 отличи correct和erroneous预测。我们将这问题转化为最小描述长度原理来解决。经过广泛的实验，我们发现我们的方法Premise在实践中表现良好。与现有的解决方案不同，它可以在大词汇和强相关性的数据上恢复真实的描述。在VQA和NER例子中，我们证明了它可以提供清晰和行动可能的NLPT分类器的系统性错误的理解。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/18/cs.AI_2023_11_18/" data-id="clp88dbs8007kob881jxm5e5c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/18/cs.CL_2023_11_18/" class="article-date">
  <time datetime="2023-11-18T11:00:00.000Z" itemprop="datePublished">2023-11-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/18/cs.CL_2023_11_18/">cs.CL - 2023-11-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Experts-in-the-Loop-Establishing-an-Effective-Workflow-in-Crafting-Privacy-Q-A"><a href="#Experts-in-the-Loop-Establishing-an-Effective-Workflow-in-Crafting-Privacy-Q-A" class="headerlink" title="Experts-in-the-Loop: Establishing an Effective Workflow in Crafting Privacy Q&amp;A"></a>Experts-in-the-Loop: Establishing an Effective Workflow in Crafting Privacy Q&amp;A</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11161">http://arxiv.org/abs/2311.11161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Kolagar, Anna Katharina Leschanowsky, Birgit Popp</li>
<li>for: 保护用户隐私，法律领域世界各地强调数据处理的透明度。</li>
<li>methods: 提议一种动态工作流程，将隐私政策转化为隐私问答对，使隐私政策通过对话AI访问。</li>
<li>results: 促进多学科协作，法律专家和对话设计师之间的合作，同时考虑使用大语言模型的生成能力和相关挑战。<details>
<summary>Abstract</summary>
Privacy policies play a vital role in safeguarding user privacy as legal jurisdictions worldwide emphasize the need for transparent data processing. While the suitability of privacy policies to enhance transparency has been critically discussed, employing conversational AI systems presents unique challenges in informing users effectively. In this position paper, we propose a dynamic workflow for transforming privacy policies into privacy question-and-answer (Q&A) pairs to make privacy policies easily accessible through conversational AI. Thereby, we facilitate interdisciplinary collaboration among legal experts and conversation designers, while also considering the utilization of large language models' generative capabilities and addressing associated challenges. Our proposed workflow underscores continuous improvement and monitoring throughout the construction of privacy Q&As, advocating for comprehensive review and refinement through an experts-in-the-loop approach.
</details>
<details>
<summary>摘要</summary>
《隐私政策在保护用户隐私方面发挥了关键作用，世界各地法律领域都强调数据处理的透明度。虽然透明度是隐私政策的重要方面，但是在使用对话AI系统时，它们带来了一些独特的挑战，用于有效地 Informing 用户。在这篇位点纸中，我们提出了一种动态工作流程，将隐私政策转换成隐私问答对，以便通过对话AI系统访问隐私政策。这样，我们促进了法律专家和对话设计师之间的跨学科合作，同时也考虑了大语言模型的生成能力和相关挑战。我们的提议的工作流程强调了不断改进和监测，在建立隐私问答时，强调了专家征考和反复修改的方式。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Inclusiveness-of-Artificial-Intelligence-Software-in-Enhancing-Project-Management-Efficiency-–-A-Review"><a href="#Evaluating-the-Inclusiveness-of-Artificial-Intelligence-Software-in-Enhancing-Project-Management-Efficiency-–-A-Review" class="headerlink" title="Evaluating the Inclusiveness of Artificial Intelligence Software in Enhancing Project Management Efficiency – A Review"></a>Evaluating the Inclusiveness of Artificial Intelligence Software in Enhancing Project Management Efficiency – A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11159">http://arxiv.org/abs/2311.11159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasileios Alevizos, Ilias Georgousis, Akebu Simasiku, Sotiria Karypidou, Antonis Messinis</li>
<li>for: 本研究旨在探讨技术integrated project management中的包容性和效率之间的关系，以及如何通过技术来提高项目成果。</li>
<li>methods: 本研究使用了定义和测量包容性的方法，以及评估技术的可转化潜力。</li>
<li>results: 研究发现，通过技术支持，项目管理中的包容性可以得到 significan improvement，但需要注意技术的可行性和伦理考虑。<details>
<summary>Abstract</summary>
The rise of advanced technology in project management (PM) highlights a crucial need for inclusiveness. This work examines the enhancement of both inclusivity and efficiency in PM through technological integration, focusing on defining and measuring inclusiveness. This approach illuminates how inclusivity-centered technology can significantly elevate project outcomes. The research navigates through the challenges of achieving inclusivity, mainly biases in learning databases and the design process of these technologies, assessment of transformative potential of these technologies, particularly in automating tasks like data collection and analysis, thus enabling managers to prioritize human-centric aspects of projects. However, the integration of such technology transcends efficiency, indicating a paradigm shift in understanding their societal roles. This shift necessitates a new approach in the development of these systems to prevent perpetuating social inequalities. We proposed a methodology involving criteria development for evaluating the inclusiveness and effectiveness of these technologies. This methodical approach is vital to comprehensively address the challenges and limitations inherent in these systems. Emphasizing the importance of inclusivity, the study advocates for a balance between technological advancement and ethical considerations, calling for a holistic understanding and regulation. In conclusion, the paper underscores that while these technologies can significantly improve outcomes, their mindful integration, ensuring inclusivity, is paramount. This exploration into the ethical and practical aspects of technology in PM contributes to a more informed and balanced approach within the field.
</details>
<details>
<summary>摘要</summary>
高技术的普及在项目管理（PM）领域 highlights 一个关键的需求：包容性。这项研究探讨了通过技术集成提高包容性和效率在 PM 中的可能性，关注定义和测量包容性。这种方法揭示了包容性 centered 技术在项目成果上可以产生显著提高。研究探讨了实现包容性的挑战，主要是学习数据库和设计过程中的偏见，以及这些技术的变革潜力，特别是自动化数据采集和分析任务，以便管理人员能够更好地注重项目人文方面。然而，技术的集成不仅提高效率，还表示一种社会角色转变。这种转变需要一种新的开发系统的方法，以避免社会不平等的持续。我们提出了一种包括对包容性和效果的评价 criterion 的方法ологи。这种系统atic 方法是必要的，以全面 Address 这些系统的挑战和限制。强调包容性的重要性，这篇论文呼吁对技术进步和伦理考虑进行平衡，寻求一种整体的理解和规范。 study 的结论是，虽然这些技术可以提高成果，但是注意其包容性的集成是关键。这种探讨技术和伦理的平衡在 PM 领域中做出了更加 Informed 和平衡的方法论。
</details></li>
</ul>
<hr>
<h2 id="Vashantor-A-Large-scale-Multilingual-Benchmark-Dataset-for-Automated-Translation-of-Bangla-Regional-Dialects-to-Bangla-Language"><a href="#Vashantor-A-Large-scale-Multilingual-Benchmark-Dataset-for-Automated-Translation-of-Bangla-Regional-Dialects-to-Bangla-Language" class="headerlink" title="Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated Translation of Bangla Regional Dialects to Bangla Language"></a>Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated Translation of Bangla Regional Dialects to Bangla Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11142">http://arxiv.org/abs/2311.11142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad</li>
<li>for: 这个研究的目的是填充在过去的研究中缺失的班加礼语言方言到标准班加礼语言的翻译。</li>
<li>methods: 该研究使用了mT5和BanglaT5模型来翻译地方语言到标准班加礼语言，以及mBERT和Bangla-bert-base来准确地探测地方语言的来源地区。</li>
<li>results: 实验结果显示，мы门司нг班加礼语言方言的 Bleu 分数为69.06，而奇图加礼语言方言的 Bleu 分数为36.75。我们还发现，мы门司нг班加礼语言方言的平均单词错误率为0.1548，而奇图加礼语言方言的平均单词错误率为0.3385。Region detection的准确率为85.86%和84.36%。这是首次对班加礼语言方言到班加礼语言机器翻译进行大规模的调查。<details>
<summary>Abstract</summary>
The Bangla linguistic variety is a fascinating mix of regional dialects that adds to the cultural diversity of the Bangla-speaking community. Despite extensive study into translating Bangla to English, English to Bangla, and Banglish to Bangla in the past, there has been a noticeable gap in translating Bangla regional dialects into standard Bangla. In this study, we set out to fill this gap by creating a collection of 32,500 sentences, encompassing Bangla, Banglish, and English, representing five regional Bangla dialects. Our aim is to translate these regional dialects into standard Bangla and detect regions accurately. To achieve this, we proposed models known as mT5 and BanglaT5 for translating regional dialects into standard Bangla. Additionally, we employed mBERT and Bangla-bert-base to determine the specific regions from where these dialects originated. Our experimental results showed the highest BLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score of 36.75 for Chittagong regional dialects. We also observed the lowest average word error rate of 0.1548 for Mymensingh regional dialects and the highest of 0.3385 for Chittagong regional dialects. For region detection, we achieved an accuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first large-scale investigation of Bangla regional dialects to Bangla machine translation. We believe our findings will not only pave the way for future work on Bangla regional dialects to Bangla machine translation, but will also be useful in solving similar language-related challenges in low-resource language conditions.
</details>
<details>
<summary>摘要</summary>
“孟加拉语言变体是一种非常有趣的地域 диалект混合，增加了孟加拉语言社区的文化多样性。尽管过去对孟加拉语言到英语、英语到孟加拉语言和孟加拉语言到英语的翻译已经进行了广泛的研究，但是还没有尝试翻译孟加拉语言地域 диалект到标准孟加拉语言。在这项研究中，我们决定填补这一空白，创建了32500句孟加拉语言、孟加拉语言混合和英语 sentences，代表五个孟加拉语言地域 диалект。我们的目标是将这些地域 диаLECTS翻译成标准孟加拉语言，并准确地检测这些地域的来源。为此，我们提出了名为mT5和BanglaT5的模型，以及mBERT和Bangla-bert-base来确定这些地域的来源。我们的实验结果显示，最高的BLEU分数为69.06，来自米亚尼斯希的地域 диаLECTS，最低的BLEU分数为36.75，来自切图格的地域 диаLECTS。我们还观察到，米亚尼斯希的地域 диаLECTS的平均单词错误率最低，为0.1548，而切图格的地域 диаLECTS的平均单词错误率最高，为0.3385。对地域检测，我们获得了85.86%的准确率，使用Bangla-bert-base，和84.36%的准确率，使用mBERT。这是对孟加拉语言地域 диаLECTS到孟加拉语言机器翻译的首次大规模调查。我们认为，我们的发现将不 только开阔未来对孟加拉语言地域 диаLECTS到孟加拉语言机器翻译的道路，还将在低资源语言条件下解决类似语言相关的挑战。”
</details></li>
</ul>
<hr>
<h2 id="Why-Is-My-Prompt-Getting-Worse-Rethinking-Regression-Testing-for-Evolving-LLM-APIs"><a href="#Why-Is-My-Prompt-Getting-Worse-Rethinking-Regression-Testing-for-Evolving-LLM-APIs" class="headerlink" title="(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs"></a>(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11123">http://arxiv.org/abs/2311.11123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanqin Ma, Chenyang Yang, Christian Kästner</li>
<li>for: 本研究旨在探讨如何对不断发展的语言模型API进行 regression testing，以适应应用程序开发人员在使用LMMs时遇到的问题。</li>
<li>methods: 本研究采用了一种CASE研究方法，通过实践案例研究了LLM APIs的更新和 deprecation 对应用程序的影响。</li>
<li>results: 研究发现， traditional testingapproaches 不能满足 regression testing LLMs，因为LLM APIs的不同correctness notions、prompt brittleness和non-determinism。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are increasingly integrated into software applications. Downstream application developers often access LLMs through APIs provided as a service. However, LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices, as evidenced by our case study on toxicity detection. Based on our case study, we emphasize the need for and re-examine the concept of regression testing for evolving LLM APIs. We argue that regression testing LLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Responsible-AI-Considerations-in-Text-Summarization-Research-A-Review-of-Current-Practices"><a href="#Responsible-AI-Considerations-in-Text-Summarization-Research-A-Review-of-Current-Practices" class="headerlink" title="Responsible AI Considerations in Text Summarization Research: A Review of Current Practices"></a>Responsible AI Considerations in Text Summarization Research: A Review of Current Practices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11103">http://arxiv.org/abs/2311.11103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Lu Liu, Meng Cao, Su Lin Blodgett, Jackie Chi Kit Cheung, Alexandra Olteanu, Adam Trischler</li>
<li>for: 本研究旨在探讨文本摘要 tasks 中可能存在的责任AI问题，包括可能的风险和影响，以及研究者是否考虑到可能的利益相互作用。</li>
<li>methods: 本研究使用质性分析方法对333篇ACL Anthology中的文本摘要研究进行多轮分析，以探讨研究和报道实践中的责任AI问题。</li>
<li>results: 研究发现大多数论文不考虑可能的利益相互作用和下游影响，导致研究的可能性和可靠性受到限制。基于这些结果，我们提出了实践和研究方向，以促进责任AI在文本摘要领域的发展。<details>
<summary>Abstract</summary>
AI and NLP publication venues have increasingly encouraged researchers to reflect on possible ethical considerations, adverse impacts, and other responsible AI issues their work might engender. However, for specific NLP tasks our understanding of how prevalent such issues are, or when and why these issues are likely to arise, remains limited. Focusing on text summarization -- a common NLP task largely overlooked by the responsible AI community -- we examine research and reporting practices in the current literature. We conduct a multi-round qualitative analysis of 333 summarization papers from the ACL Anthology published between 2020-2022. We focus on how, which, and when responsible AI issues are covered, which relevant stakeholders are considered, and mismatches between stated and realized research goals. We also discuss current evaluation practices and consider how authors discuss the limitations of both prior work and their own work. Overall, we find that relatively few papers engage with possible stakeholders or contexts of use, which limits their consideration of potential downstream adverse impacts or other responsible AI issues. Based on our findings, we make recommendations on concrete practices and research directions.
</details>
<details>
<summary>摘要</summary>
AI和自然语言处理（NLP）发表文章的场景越来越多地鼓励研究人员考虑可能的伦理考虑因素、不良影响和负责任AI问题他们的工作可能会带来。然而，对于特定的NLP任务，我们对这些问题的流行程度、出现时间和发生原因仍然很限制。关注文本概要---一种常见的NLP任务，负责AI社区很少关注的领域---我们对当前文献的研究和报道做了多 round的质量分析。我们关注怎样、何时和为什么负责AI问题被考虑，哪些关键参与者被考虑，以及文献目标与实际实施的差异。我们还讨论当前的评价方法，并考虑作者如何评估先前工作和自己的研究的局限性。总之，我们发现大多数文章没有考虑可能的利益相关者或使用场景，这限制了他们对可能的下游不良影响或其他负责任AI问题的考虑。根据我们的发现，我们提出了具体的实践和研究方向。
</details></li>
</ul>
<hr>
<h2 id="Bit-Cipher-–-A-Simple-yet-Powerful-Word-Representation-System-that-Integrates-Efficiently-with-Language-Models"><a href="#Bit-Cipher-–-A-Simple-yet-Powerful-Word-Representation-System-that-Integrates-Efficiently-with-Language-Models" class="headerlink" title="Bit Cipher – A Simple yet Powerful Word Representation System that Integrates Efficiently with Language Models"></a>Bit Cipher – A Simple yet Powerful Word Representation System that Integrates Efficiently with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11012">http://arxiv.org/abs/2311.11012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Zhao, Jake Ryland Williams</li>
<li>for: 本研究旨在提出一种新的词表示系统，以提高词向量的计算效率和解释力。</li>
<li>methods: 本研究使用了一种新的Bit-cipher算法来训练词向量，该算法不需要反propagation，并且可以利用文本上的上下文信息和紧凑的维度减少技术来提供强大的解释性。</li>
<li>results: 研究表明，使用Bit-cipher算法可以快速训练高效的词向量，并且可以在不同的文本任务中达到竞争性的表现。此外，研究还发现了在折衔批处理和练习中使用cipher embedding层可以加速模型的训练过程，并且可以提高模型的优化性。<details>
<summary>Abstract</summary>
While Large Language Models (LLMs) become ever more dominant, classic pre-trained word embeddings sustain their relevance through computational efficiency and nuanced linguistic interpretation. Drawing from recent studies demonstrating that the convergence of GloVe and word2vec optimizations all tend towards log-co-occurrence matrix variants, we construct a novel word representation system called Bit-cipher that eliminates the need of backpropagation while leveraging contextual information and hyper-efficient dimensionality reduction techniques based on unigram frequency, providing strong interpretability, alongside efficiency. We use the bit-cipher algorithm to train word vectors via a two-step process that critically relies on a hyperparameter -- bits -- that controls the vector dimension. While the first step trains the bit-cipher, the second utilizes it under two different aggregation modes -- summation or concatenation -- to produce contextually rich representations from word co-occurrences. We extend our investigation into bit-cipher's efficacy, performing probing experiments on part-of-speech (POS) tagging and named entity recognition (NER) to assess its competitiveness with classic embeddings like word2vec and GloVe. Additionally, we explore its applicability in LM training and fine-tuning. By replacing embedding layers with cipher embeddings, our experiments illustrate the notable efficiency of cipher in accelerating the training process and attaining better optima compared to conventional training paradigms. Experiments on the integration of bit-cipher embedding layers with Roberta, T5, and OPT, prior to or as a substitute for fine-tuning, showcase a promising enhancement to transfer learning, allowing rapid model convergence while preserving competitive performance.
</details>
<details>
<summary>摘要</summary>
While Large Language Models (LLMs) become ever more dominant, classic pre-trained word embeddings sustain their relevance through computational efficiency and nuanced linguistic interpretation. Drawing from recent studies demonstrating that the convergence of GloVe and word2vec optimizations all tend towards log-co-occurrence matrix variants, we construct a novel word representation system called Bit-cipher that eliminates the need of backpropagation while leveraging contextual information and hyper-efficient dimensionality reduction techniques based on unigram frequency, providing strong interpretability, alongside efficiency. We use the bit-cipher algorithm to train word vectors via a two-step process that critically relies on a hyperparameter -- bits -- that controls the vector dimension. While the first step trains the bit-cipher, the second utilizes it under two different aggregation modes -- summation or concatenation -- to produce contextually rich representations from word co-occurrences. We extend our investigation into bit-cipher's efficacy, performing probing experiments on part-of-speech (POS) tagging and named entity recognition (NER) to assess its competitiveness with classic embeddings like word2vec and GloVe. Additionally, we explore its applicability in LM training and fine-tuning. By replacing embedding layers with cipher embeddings, our experiments illustrate the notable efficiency of cipher in accelerating the training process and attaining better optima compared to conventional training paradigms. Experiments on the integration of bit-cipher embedding layers with Roberta, T5, and OPT, prior to or as a substitute for fine-tuning, showcase a promising enhancement to transfer learning, allowing rapid model convergence while preserving competitive performance.
</details></li>
</ul>
<hr>
<h2 id="Joyful-Joint-Modality-Fusion-and-Graph-Contrastive-Learning-for-Multimodal-Emotion-Recognition"><a href="#Joyful-Joint-Modality-Fusion-and-Graph-Contrastive-Learning-for-Multimodal-Emotion-Recognition" class="headerlink" title="Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimodal Emotion Recognition"></a>Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimodal Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11009">http://arxiv.org/abs/2311.11009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyuan Li, Yusong Wang, Kotaro Funakoshi, Manabu Okumura</li>
<li>for: 本文targets at multimodal emotion recognition, aiming to recognize emotions for each utterance of multiple modalities, and has important applications in human-machine interaction.</li>
<li>methods: 本文提出了一种joint modality fusion and graph contrastive learning方法，即Joyful方法，其中multimodality fusion、contrastive learning和情感认知 jointly optimized. specifically, a new multimodal fusion mechanism is designed to provide deep interaction and fusion between global contextual and uni-modal specific features. Additionally, a graph contrastive learning framework with inter-view and intra-view contrastive losses is introduced to learn more distinguishable representations for samples with different sentiments.</li>
<li>results: 根据三个benchmark datasets的实验结果，Joyful方法 achieved state-of-the-art (SOTA) performance compared to all baselines.<details>
<summary>Abstract</summary>
Multimodal emotion recognition aims to recognize emotions for each utterance of multiple modalities, which has received increasing attention for its application in human-machine interaction. Current graph-based methods fail to simultaneously depict global contextual features and local diverse uni-modal features in a dialogue. Furthermore, with the number of graph layers increasing, they easily fall into over-smoothing. In this paper, we propose a method for joint modality fusion and graph contrastive learning for multimodal emotion recognition (Joyful), where multimodality fusion, contrastive learning, and emotion recognition are jointly optimized. Specifically, we first design a new multimodal fusion mechanism that can provide deep interaction and fusion between the global contextual and uni-modal specific features. Then, we introduce a graph contrastive learning framework with inter-view and intra-view contrastive losses to learn more distinguishable representations for samples with different sentiments. Extensive experiments on three benchmark datasets indicate that Joyful achieved state-of-the-art (SOTA) performance compared to all baselines.
</details>
<details>
<summary>摘要</summary>
多模态情感识别目标是识别对话中每个语音的多种情感，受到人机交互应用的关注。现有的图structured方法无法同时展示对话中全局上下文特征和多模态特征的地方特征。此外，随着图层数量的增加，它们容易陷入过度熔合。本文提出了一种对多模态情感识别进行联合多modalité拟合和图相对学习的方法（Joyful），其中多模态拟合、对比学习和情感识别被联合优化。 Specifically，我们首先设计了一种新的多模态拟合机制，可以提供深入的交互和拟合全局上下文特征和单模态特征。然后，我们引入了一个图相对学习框架，包括对视和自视对比损失来学习更加 distinguishable的表示。经验表明，Joyful在三个基准数据集上达到了所有基准的最佳性能（SOTA）。
</details></li>
</ul>
<hr>
<h2 id="Gendec-A-Machine-Learning-based-Framework-for-Gender-Detection-from-Japanese-Names"><a href="#Gendec-A-Machine-Learning-based-Framework-for-Gender-Detection-from-Japanese-Names" class="headerlink" title="Gendec: A Machine Learning-based Framework for Gender Detection from Japanese Names"></a>Gendec: A Machine Learning-based Framework for Gender Detection from Japanese Names</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11001">http://arxiv.org/abs/2311.11001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duong Tien Pham, Luan Thanh Nguyen</li>
<li>for: 这个研究旨在创建一个日本名字性别检测数据集，以及一个基于多种方法的名字性别检测框架，以便更好地理解日本人名中的性别信息。</li>
<li>methods: 该研究使用了多种方法，包括传统的机器学习技术和最新的传输学习模型，以检测日本名字中的性别信息。</li>
<li>results: 研究预计可以准确地预测日本名字中的性别信息，并且可以应用于多个领域。<details>
<summary>Abstract</summary>
Every human has their own name, a fundamental aspect of their identity and cultural heritage. The name often conveys a wealth of information, including details about an individual's background, ethnicity, and, especially, their gender. By detecting gender through the analysis of names, researchers can unlock valuable insights into linguistic patterns and cultural norms, which can be applied to practical applications. Hence, this work presents a novel dataset for Japanese name gender detection comprising 64,139 full names in romaji, hiragana, and kanji forms, along with their biological genders. Moreover, we propose Gendec, a framework for gender detection from Japanese names that leverages diverse approaches, including traditional machine learning techniques or cutting-edge transfer learning models, to predict the gender associated with Japanese names accurately. Through a thorough investigation, the proposed framework is expected to be effective and serve potential applications in various domains.
</details>
<details>
<summary>摘要</summary>
每个人有自己的名字，是他们的身份和文化遗产的基本组成部分。名字通常包含了许多信息，如个人背景、民族和特别是性别。通过对名字进行分析，研究人员可以获得价值的信息，包括语言模式和文化规范，这些信息可以应用于实际应用。因此，本文提出了一个新的日本名字性别检测集合，包括64,139个全名（拼音、平仮名和汉字形式），以及他们的生物性别。此外，我们提出了一个基于多种方法的日本名字性别检测框架，包括传统机器学习技术和前沿技术的转移学习模型，以准确预测日本名字中的性别。经过仔细调查，我们预期该框架能够实现效果，并可以在多个领域应用。
</details></li>
</ul>
<hr>
<h2 id="Behavior-Optimized-Image-Generation"><a href="#Behavior-Optimized-Image-Generation" class="headerlink" title="Behavior Optimized Image Generation"></a>Behavior Optimized Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10995">http://arxiv.org/abs/2311.10995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Varun Khurana, Yaman K Singla, Jayakumar Subramanian, Rajiv Ratn Shah, Changyou Chen, Zhiqiang Xu, Balaji Krishnamurthy</li>
<li>for: 这篇论文的目的是如何在图像生成过程中嵌入目标行为的知识，以创造不仅更加美观的图像，还能够提高图像的表现力。</li>
<li>methods: 这篇论文提出了一种名为BoigLLM的语言模型，该模型能够理解图像内容和用户行为。BoigLLM知道图像需要如何看起来，以达到某些需要的KPI。此外， authors还提出了一种扩散型模型（BoigSD），用于对BoigLLM定义的奖励进行调整。</li>
<li>results: 作者们表明，BoigLLM可以比13倍大的GPT-3.5和GPT-4模型在这个任务中表现更好，表明这些当前状态的模型可以理解图像，但缺乏实际世界中图像的信息。此外， authors还发现，通过使用BoigBench数据集，可以对图像生成和理解进行更多的研究。<details>
<summary>Abstract</summary>
The last few years have witnessed great success on image generation, which has crossed the acceptance thresholds of aesthetics, making it directly applicable to personal and commercial applications. However, images, especially in marketing and advertising applications, are often created as a means to an end as opposed to just aesthetic concerns. The goal can be increasing sales, getting more clicks, likes, or image sales (in the case of stock businesses). Therefore, the generated images need to perform well on these key performance indicators (KPIs), in addition to being aesthetically good. In this paper, we make the first endeavor to answer the question of "How can one infuse the knowledge of the end-goal within the image generation process itself to create not just better-looking images but also "better-performing'' images?''. We propose BoigLLM, an LLM that understands both image content and user behavior. BoigLLM knows how an image should look to get a certain required KPI. We show that BoigLLM outperforms 13x larger models such as GPT-3.5 and GPT-4 in this task, demonstrating that while these state-of-the-art models can understand images, they lack information on how these images perform in the real world. To generate actual pixels of behavior-conditioned images, we train a diffusion-based model (BoigSD) to align with a proposed BoigLLM-defined reward. We show the performance of the overall pipeline on two datasets covering two different behaviors: a stock dataset with the number of forward actions as the KPI and a dataset containing tweets with the total likes as the KPI, denoted as BoigBench. To advance research in the direction of utility-driven image generation and understanding, we release BoigBench, a benchmark dataset containing 168 million enterprise tweets with their media, brand account names, time of post, and total likes.
</details>
<details>
<summary>摘要</summary>
过去几年，图像生成领域取得了巨大的成功，已经突破了艺术性的接受reshold，使其直接适用于个人和商业应用。然而，在市场营销和广告应用中，图像 oftentimes 是为了达到一个目标而创建的，而不是仅仅是艺术意义。因此，生成的图像需要在关键性能指标（KPI）上表现良好，除了外观好。在这篇论文中，我们做出了第一次的尝试，回答“如何在图像生成过程中植入目标的知识，以创造不仅更好看的图像，而且“更好表现”的图像？”我们提出了 BoigLLM，一个理解图像内容和用户行为的 LLM。BoigLLM 知道一个图像需要如何看起来，以达到某个需要的 KPI。我们表明，BoigLLM 在这项任务上超过 13 倍大的模型，如 GPT-3.5 和 GPT-4，表明这些当前顶尖模型可以理解图像，但缺乏实际世界中图像的信息。为生成 Conditioned 的实际像素，我们训练了一个扩散基于模型（BoigSD），以与 BoigLLM 定义的奖励相对。我们展示了整个管道在两个 dataset 上的表现，其中一个是一个股票 dataset，其中的 KPI 是前进动作的数量，另一个是一个包含 tweet 的 dataset，其中的 KPI 是总喜欢数。为了推动研究在Utility-driven 图像生成和理解方面，我们发布了 BoigBench，一个包含 168 万个企业微博，它们的媒体、 bran account 名称、发布时间和总喜欢数。
</details></li>
</ul>
<hr>
<h2 id="Journey-of-Hallucination-minimized-Generative-AI-Solutions-for-Financial-Decision-Makers"><a href="#Journey-of-Hallucination-minimized-Generative-AI-Solutions-for-Financial-Decision-Makers" class="headerlink" title="Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers"></a>Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10961">http://arxiv.org/abs/2311.10961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sohini Roychowdhury</li>
<li>for: 这项研究的目的是设计降低幻觉的大语言模型（LLM）解决方案，以便在金融领域的决策者使用。</li>
<li>methods: 本研究使用了三个主要阶段：评估、缩放和LLM进化，以确保 chatbot 、自动报告和警示的可靠性和质量。</li>
<li>results: 研究表明，通过使用人类反馈来控制幻觉，可以提高 chatbot 的可靠性和质量。同时，通过使用数据 Ansatz 技术，可以生成高质量的自动报告和警示。<details>
<summary>Abstract</summary>
Generative AI has significantly reduced the entry barrier to the domain of AI owing to the ease of use and core capabilities of automation, translation, and intelligent actions in our day to day lives. Currently, Large language models (LLMs) that power such chatbots are being utilized primarily for their automation capabilities for software monitoring, report generation etc. and for specific personalized question answering capabilities, on a limited scope and scale. One major limitation of the currently evolving family of LLMs is 'hallucinations', wherein inaccurate responses are reported as factual. Hallucinations are primarily caused by biased training data, ambiguous prompts and inaccurate LLM parameters, and they majorly occur while combining mathematical facts with language-based context. Thus, monitoring and controlling for hallucinations becomes necessary when designing solutions that are meant for decision makers. In this work we present the three major stages in the journey of designing hallucination-minimized LLM-based solutions that are specialized for the decision makers of the financial domain, namely: prototyping, scaling and LLM evolution using human feedback. These three stages and the novel data to answer generation modules presented in this work are necessary to ensure that the Generative AI chatbots, autonomous reports and alerts are reliable and high-quality to aid key decision-making processes.
</details>
<details>
<summary>摘要</summary>
生成AI已经大幅降低了AI领域的入口难度，因为它们提供了易于使用的核心功能，包括自动化、翻译和智能行为，在我们日常生活中。目前，大型语言模型（LLM）正在主要用于其自动化能力，例如软件监控、报告生成等，以及特定的个性化问答能力，但范围和规模受限。现有的LLM家族存在一个主要的问题，即“幻见”（hallucinations），即因训练数据偏见、模糊的提示和不准确的LLM参数而导致的不准确回答。因此，在设计针对决策者的LLM基于解决方案时，监控和控制幻见变得必要。在这项工作中，我们介绍了针对决策者的LLM基于解决方案设计的三个主要阶段：原型、扩大和LLM进化，以及在这三个阶段中使用人类反馈来确保生成AI聊天机器人、自动生成报告和警示高质量和可靠，以帮助决策过程。
</details></li>
</ul>
<hr>
<h2 id="Deception-Detection-from-Linguistic-and-Physiological-Data-Streams-Using-Bimodal-Convolutional-Neural-Networks"><a href="#Deception-Detection-from-Linguistic-and-Physiological-Data-Streams-Using-Bimodal-Convolutional-Neural-Networks" class="headerlink" title="Deception Detection from Linguistic and Physiological Data Streams Using Bimodal Convolutional Neural Networks"></a>Deception Detection from Linguistic and Physiological Data Streams Using Bimodal Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10944">http://arxiv.org/abs/2311.10944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Panfeng Li, Mohamed Abouelenien, Rada Mihalcea</li>
<li>for: 这篇论文探讨了用 convolutional neural networks 进行多模态骗学检测的应用。</li>
<li>methods: 作者使用了一个由104名参与者回答两个话题的数据集，从这些数据中提取了语言和生理学特征，以建立和训练神经网络模型。</li>
<li>results: 作者提出了一种将多 modalities 融合的卷积神经网络模型，并证明了这种方法在多模态骗学检测中的优势。此外，作者还比较了这种方法与之前的多模态骗学检测方法，发现其在有限数据情况下也能够达到更高的检测精度。<details>
<summary>Abstract</summary>
Deception detection is gaining increasing interest due to ethical and security concerns. This paper explores the application of convolutional neural networks for the purpose of multimodal deception detection. We use a dataset built by interviewing 104 subjects about two topics, with one truthful and one falsified response from each subject about each topic. In particular, we make three main contributions. First, we extract linguistic and physiological features from this data to train and construct the neural network models. Second, we propose a fused convolutional neural network model using both modalities in order to achieve an improved overall performance. Third, we compare our new approach with earlier methods designed for multimodal deception detection. We find that our system outperforms regular classification methods; our results indicate the feasibility of using neural networks for deception detection even in the presence of limited amounts of data.
</details>
<details>
<summary>摘要</summary>
骗子检测在当前有越来越多的关注，这主要归功于伦理和安全问题。本文探讨了使用卷积神经网络进行多Modal骗子检测的应用。我们使用了104名参与者对两个话题进行了面试，每个话题有一个真实的和一个假的回答。特别是，我们从这些数据中提取了语言和生物学特征，用于训练和构建神经网络模型。我们的主要贡献如下：1. 我们提出了一种基于多Modal的卷积神经网络模型，以实现更好的总体性能。2. 我们对之前的多Modal骗子检测方法进行了比较，并发现我们的系统在数据量有限的情况下表现更好。我们的研究表明，使用卷积神经网络进行多Modal骗子检测是可行的，即使是在数据量有限的情况下。
</details></li>
</ul>
<hr>
<h2 id="Partially-Randomizing-Transformer-Weights-for-Dialogue-Response-Diversity"><a href="#Partially-Randomizing-Transformer-Weights-for-Dialogue-Response-Diversity" class="headerlink" title="Partially Randomizing Transformer Weights for Dialogue Response Diversity"></a>Partially Randomizing Transformer Weights for Dialogue Response Diversity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10943">http://arxiv.org/abs/2311.10943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Yang Lee, Kong Aik Lee, Woon-Seng Gan</li>
<li>for: 提高开放领域对话的响应多样性</li>
<li>methods: 使用固定layer的权重初始化和随机 initialize</li>
<li>results: 与先前的方法相比，PaRaFormer的性能相似，不增加训练或推理难度，也不增加模型的复杂度<details>
<summary>Abstract</summary>
Despite recent progress in generative open-domain dialogue, the issue of low response diversity persists. Prior works have addressed this issue via either novel objective functions, alternative learning approaches such as variational frameworks, or architectural extensions such as the Randomized Link (RL) Transformer. However, these approaches typically entail either additional difficulties during training/inference, or a significant increase in model size and complexity. Hence, we propose the \underline{Pa}rtially \underline{Ra}ndomized trans\underline{Former} (PaRaFormer), a simple extension of the transformer which involves freezing the weights of selected layers after random initialization. Experimental results reveal that the performance of the PaRaformer is comparable to that of the aforementioned approaches, despite not entailing any additional training difficulty or increase in model complexity.
</details>
<details>
<summary>摘要</summary>
尽管最近的开放领域对话生成技术已经做出了 significi cant 进步，但问题仍然存在低响应多样性。先前的方法通常通过新的目标函数、不同的学习方法如变量框架，或者架构扩展如随机链接（RL）转换器来解决这个问题。然而，这些方法通常会在训练/推理过程中增加额外难度，或者模型的大小和复杂性会增加。因此，我们提出了PaRaFormer，一种简单的 transformer 扩展，它通过随机初始化选择层的 weights 并冻结它们来解决低响应多样性问题。实验结果表明，PaRaFormer 的性能与先前的方法相当，而无需额外的训练困难或模型的复杂度增加。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/18/cs.CL_2023_11_18/" data-id="clp88dbuk00fcob88cyfxfnaq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/18/cs.LG_2023_11_18/" class="article-date">
  <time datetime="2023-11-18T10:00:00.000Z" itemprop="datePublished">2023-11-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/18/cs.LG_2023_11_18/">cs.LG - 2023-11-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dueling-Optimization-with-a-Monotone-Adversary"><a href="#Dueling-Optimization-with-a-Monotone-Adversary" class="headerlink" title="Dueling Optimization with a Monotone Adversary"></a>Dueling Optimization with a Monotone Adversary</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11185">http://arxiv.org/abs/2311.11185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avrim Blum, Meghal Gupta, Gene Li, Naren Sarayu Manoj, Aadirupa Saha, Yuanyuan Yang</li>
<li>for: 这篇论文研究了一种叫做对抗优化的问题，即在一个含有反对抗的环境中，设计一个在线算法，以找到一个最小值 $\mathbf{x}^{*}$，其中 $f\colon X \to \mathbb{R}$ 是一个函数， $X \subseteq \mathbb{R}^d$ 是一个空间。</li>
<li>methods: 该论文使用了一种随机化算法，可以在很多种自然的函数 $f$ 和空间 $X$ 上实现。</li>
<li>results: 该论文的主要结果是，该算法可以在 $O(d)$ 的成本和 $O(d\log(1&#x2F;\varepsilon)^2)$ 的迭代复杂度下找到一个 $\varepsilon$-优点。此外，该算法的对 $d$ 的依赖性是可证明为是最佳的，即任何随机算法都必须在 $d$ 上付出 $\Omega(d)$ 的成本和迭代复杂度。<details>
<summary>Abstract</summary>
We introduce and study the problem of dueling optimization with a monotone adversary, which is a generalization of (noiseless) dueling convex optimization. The goal is to design an online algorithm to find a minimizer $\mathbf{x}^{*}$ for a function $f\colon X \to \mathbb{R}$, where $X \subseteq \mathbb{R}^d$. In each round, the algorithm submits a pair of guesses, i.e., $\mathbf{x}^{(1)}$ and $\mathbf{x}^{(2)}$, and the adversary responds with any point in the space that is at least as good as both guesses. The cost of each query is the suboptimality of the worse of the two guesses; i.e., ${\max} \left( f(\mathbf{x}^{(1)}), f(\mathbf{x}^{(2)}) \right) - f(\mathbf{x}^{*})$. The goal is to minimize the number of iterations required to find an $\varepsilon$-optimal point and to minimize the total cost (regret) of the guesses over many rounds. Our main result is an efficient randomized algorithm for several natural choices of the function $f$ and set $X$ that incurs cost $O(d)$ and iteration complexity $O(d\log(1/\varepsilon)^2)$. Moreover, our dependence on $d$ is asymptotically optimal, as we show examples in which any randomized algorithm for this problem must incur $\Omega(d)$ cost and iteration complexity.
</details>
<details>
<summary>摘要</summary>
我们介绍和研究对对抗式优化问题，即一个对抗者对函数 $f\colon X \to \mathbb{R}$ 的一个通用扩展。我们的目标是设计一个在线 Algorithm 以找到 $X \subseteq \mathbb{R}^d$ 中的最小值 $\mathbf{x}^{*}$。在每个回合中，Algorithm 会提交两个猜测，即 $\mathbf{x}^{(1)}$ 和 $\mathbf{x}^{(2)}$，对抗者则回传任何在空间中的任何点，其至少对两个猜测都是最佳的。每次猜测的成本是两个猜测中的较差一个的成本，即 $\max \left( f(\mathbf{x}^{(1)}), f(\mathbf{x}^{(2)}) \right) - f(\mathbf{x}^{*})$。我们的主要结果是一个高效的随机化算法，可以在多个自然的函数 $f$ 和集合 $X$ 上实现cost $O(d)$ 和迭代复杂度 $O(d\log(1/\varepsilon)^2)$。此外，我们的 $d$ 依赖性是对抗数学optimal，我们提供了一些示例，证明任何随机化算法 для这个问题必须有 $\Omega(d)$ 成本和迭代复杂度。
</details></li>
</ul>
<hr>
<h2 id="Exponentially-Convergent-Algorithms-for-Supervised-Matrix-Factorization"><a href="#Exponentially-Convergent-Algorithms-for-Supervised-Matrix-Factorization" class="headerlink" title="Exponentially Convergent Algorithms for Supervised Matrix Factorization"></a>Exponentially Convergent Algorithms for Supervised Matrix Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11182">http://arxiv.org/abs/2311.11182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ljw9510/smf">https://github.com/ljw9510/smf</a></li>
<li>paper_authors: Joowon Lee, Hanbaek Lyu, Weixin Yao</li>
<li>for: 使用Supervised Matrix Factorization（SMF）同时实现特征提取和分类任务，解决高维数据中的挑战。</li>
<li>methods: 提出了一种新的框架，将SMF视为低维矩阵估计问题，并提出了一种高效的算法，可在初始化的假设下，在轻量级的假设下提供快速拟合全局最小化 objective 的 garantia。</li>
<li>results: 通过应用于多种SMF类型问题，成功地鉴别了许多不同类型的肿瘤相关基因组。<details>
<summary>Abstract</summary>
Supervised matrix factorization (SMF) is a classical machine learning method that simultaneously seeks feature extraction and classification tasks, which are not necessarily a priori aligned objectives. Our goal is to use SMF to learn low-rank latent factors that offer interpretable, data-reconstructive, and class-discriminative features, addressing challenges posed by high-dimensional data. Training SMF model involves solving a nonconvex and possibly constrained optimization with at least three blocks of parameters. Known algorithms are either heuristic or provide weak convergence guarantees for special cases. In this paper, we provide a novel framework that 'lifts' SMF as a low-rank matrix estimation problem in a combined factor space and propose an efficient algorithm that provably converges exponentially fast to a global minimizer of the objective with arbitrary initialization under mild assumptions. Our framework applies to a wide range of SMF-type problems for multi-class classification with auxiliary features. To showcase an application, we demonstrate that our algorithm successfully identified well-known cancer-associated gene groups for various cancers.
</details>
<details>
<summary>摘要</summary>
“超级vised矩阵因子（SMF）是一种经典的机器学习方法，同时寻求特征提取和分类任务，这两个任务可能并不是先验 aligned 目标。我们想使用 SMF 学习低级别的秘密因子，以提供可解释、数据重建和类别分类的特征，解决高维数据带来的挑战。SMF 模型的训练过程 involve 解决非拟合和可能受限制的优化问题，其中至少有三个块的参数。知名的算法是 Either HEURISTIC 或提供弱 convergence 保证的特殊情况。在这篇论文中，我们提出了一种新的框架，将 SMF 视为一种低级别矩阵估计问题，并提出了一种高效的算法，可在任意初始化下，在轻微假设下提供可证明的对象目标的极限值 global minimizer ，并且在数据中心化的情况下提供了一个稳定的初始化方法。我们的框架适用于多类分类问题中的 SMF-type 问题，并且在不同的肿瘤类型中成功地识别了许多知名的肿瘤相关基因组。”
</details></li>
</ul>
<hr>
<h2 id="Nonsmooth-Projection-Free-Optimization-with-Functional-Constraints"><a href="#Nonsmooth-Projection-Free-Optimization-with-Functional-Constraints" class="headerlink" title="Nonsmooth Projection-Free Optimization with Functional Constraints"></a>Nonsmooth Projection-Free Optimization with Functional Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11180">http://arxiv.org/abs/2311.11180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kamiarasgari/Nonsmooth-Projection-Free-Optimization-with-Functional-Constraints">https://github.com/kamiarasgari/Nonsmooth-Projection-Free-Optimization-with-Functional-Constraints</a></li>
<li>paper_authors: Kamiar Asgari, Michael J. Neely</li>
<li>for: 这个论文提出了一种基于偏导数的非光滑凸优化算法，可以处理具有一般凸函数不等式约束的非光滑优化问题，而不需要进行可行集 проекции。</li>
<li>methods: 该算法使用了一种简单的分离方案，并使用了一个新的拉格朗日积分更新规则。</li>
<li>results: 该算法可以在 $\mathcal{O}(\epsilon^{-2})$ 迭代中获得 $\epsilon$-次优化解决方案，每迭代只需要一个 (可能不准确) 线性最小化询问（LMO）和一个 (可能不准确) 偏导数计算。这种性能与现有的下界具有相同性。<details>
<summary>Abstract</summary>
This paper presents a subgradient-based algorithm for constrained nonsmooth convex optimization that does not require projections onto the feasible set. While the well-established Frank-Wolfe algorithm and its variants already avoid projections, they are primarily designed for smooth objective functions. In contrast, our proposed algorithm can handle nonsmooth problems with general convex functional inequality constraints. It achieves an $\epsilon$-suboptimal solution in $\mathcal{O}(\epsilon^{-2})$ iterations, with each iteration requiring only a single (potentially inexact) Linear Minimization Oracle (LMO) call and a (possibly inexact) subgradient computation. This performance is consistent with existing lower bounds. Similar performance is observed when deterministic subgradients are replaced with stochastic subgradients. In the special case where there are no functional inequality constraints, our algorithm competes favorably with a recent nonsmooth projection-free method designed for constraint-free problems. Our approach utilizes a simple separation scheme in conjunction with a new Lagrange multiplier update rule.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出了一种基于梯度的准则逼近算法，用于非规范凸优化问题，不需要进行可行集的投影。与已知的Frank-Wolfe算法和其变体不同，我们的提议的算法可以处理具有一般凸函数不等式约束的非准确问题。它可以在 $\mathcal{O}(\epsilon^{-2})$ 迭代内 achieve an $\epsilon$-下界解，每迭代只需要一个 (可能不准确) 线性最小化函数 oracle (LMO) 调用和 (可能不准确) 梯度计算。这个性能与现有的下界具有相同的性能。此外，当使用权重函数时，我们的算法也可以达到类似的性能。在特殊情况下，当没有函数不等式约束时，我们的算法与一种最近的准则逼近方法，设计用于无约束问题，竞争得到。我们的方法使用了一种简单的分离方案，并使用了一个新的拉格朗日积分规则。
</details></li>
</ul>
<hr>
<h2 id="Low-Precision-Floating-Point-for-Efficient-On-Board-Deep-Neural-Network-Processing"><a href="#Low-Precision-Floating-Point-for-Efficient-On-Board-Deep-Neural-Network-Processing" class="headerlink" title="Low-Precision Floating-Point for Efficient On-Board Deep Neural Network Processing"></a>Low-Precision Floating-Point for Efficient On-Board Deep Neural Network Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11172">http://arxiv.org/abs/2311.11172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cédric Gernigon, Silviu-Ioan Filip, Olivier Sentieys, Clément Coggiola, Mickaël Bruno</li>
<li>for: 降低高分辨率地球观测卫星系统中的下载链速率</li>
<li>methods: 使用在机器上的深度学习来压缩数据</li>
<li>results: 使用6比特浮点数字quantization可以和单精度浮点数字相比，无需 significannot accuracy degradation<details>
<summary>Abstract</summary>
One of the major bottlenecks in high-resolution Earth Observation (EO) space systems is the downlink between the satellite and the ground. Due to hardware limitations, on-board power limitations or ground-station operation costs, there is a strong need to reduce the amount of data transmitted. Various processing methods can be used to compress the data. One of them is the use of on-board deep learning to extract relevant information in the data. However, most ground-based deep neural network parameters and computations are performed using single-precision floating-point arithmetic, which is not adapted to the context of on-board processing. We propose to rely on quantized neural networks and study how to combine low precision (mini) floating-point arithmetic with a Quantization-Aware Training methodology. We evaluate our approach with a semantic segmentation task for ship detection using satellite images from the Airbus Ship dataset. Our results show that 6-bit floating-point quantization for both weights and activations can compete with single-precision without significant accuracy degradation. Using a Thin U-Net 32 model, only a 0.3% accuracy degradation is observed with 6-bit minifloat quantization (a 6-bit equivalent integer-based approach leads to a 0.5% degradation). An initial hardware study also confirms the potential impact of such low-precision floating-point designs, but further investigation at the scale of a full inference accelerator is needed before concluding whether they are relevant in a practical on-board scenario.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)一个主要瓶颈在高分辨率地球观测（EO）空间系统中是地面和卫星之间的下载链。由于硬件限制、机载电力限制或地面站操作成本，有强需要减少数据传输量。Various processing methods can be used to compress the data. One of them is the use of on-board deep learning to extract relevant information in the data. However, most ground-based deep neural network parameters and computations are performed using single-precision floating-point arithmetic, which is not adapted to the context of on-board processing. We propose to rely on quantized neural networks and study how to combine low precision (mini) floating-point arithmetic with a Quantization-Aware Training methodology. We evaluate our approach with a semantic segmentation task for ship detection using satellite images from the Airbus Ship dataset. Our results show that 6-bit floating-point quantization for both weights and activations can compete with single-precision without significant accuracy degradation. Using a Thin U-Net 32 model, only a 0.3% accuracy degradation is observed with 6-bit minifloat quantization (a 6-bit equivalent integer-based approach leads to a 0.5% degradation). An initial hardware study also confirms the potential impact of such low-precision floating-point designs, but further investigation at the scale of a full inference accelerator is needed before concluding whether they are relevant in a practical on-board scenario.
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Machine-Learning-Models-for-Quantum-Error-Correction"><a href="#Benchmarking-Machine-Learning-Models-for-Quantum-Error-Correction" class="headerlink" title="Benchmarking Machine Learning Models for Quantum Error Correction"></a>Benchmarking Machine Learning Models for Quantum Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11167">http://arxiv.org/abs/2311.11167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Fu, Yue Zhao</li>
<li>for: 本研究的目的是强调机器学习（ML）在量子错误恢复（QEC）中的应用，以及在量子计算机系统中实现稳定的量子计算机系统。</li>
<li>methods: 本研究使用了七种当前最佳的深度学习算法，包括卷积神经网络、图神经网络和图变换器，以评估Machine Learning在QEC中捕捉远程 ancilla qubits 的依赖关系的能力。</li>
<li>results: 研究发现，通过扩大接受区域来利用远程 ancilla qubits 中的信息，可以提高 QEC 的准确率。例如，与 CNN 相比，U-Net 可以提高准确率约50%。此外，研究还提供了一个全面的分析，以便未来的研究。<details>
<summary>Abstract</summary>
Quantum Error Correction (QEC) is one of the fundamental problems in quantum computer systems, which aims to detect and correct errors in the data qubits within quantum computers. Due to the presence of unreliable data qubits in existing quantum computers, implementing quantum error correction is a critical step when establishing a stable quantum computer system. Recently, machine learning (ML)-based approaches have been proposed to address this challenge. However, they lack a thorough understanding of quantum error correction. To bridge this research gap, we provide a new perspective to understand machine learning-based QEC in this paper. We find that syndromes in the ancilla qubits result from errors on connected data qubits, and distant ancilla qubits can provide auxiliary information to rule out some incorrect predictions for the data qubits. Therefore, to detect errors in data qubits, we must consider the information present in the long-range ancilla qubits. To the best of our knowledge, machine learning is less explored in the dependency relationship of QEC. To fill the blank, we curate a machine learning benchmark to assess the capacity to capture long-range dependencies for quantum error correction. To provide a comprehensive evaluation, we evaluate seven state-of-the-art deep learning algorithms spanning diverse neural network architectures, such as convolutional neural networks, graph neural networks, and graph transformers. Our exhaustive experiments reveal an enlightening trend: By enlarging the receptive field to exploit information from distant ancilla qubits, the accuracy of QEC significantly improves. For instance, U-Net can improve CNN by a margin of about 50%. Finally, we provide a comprehensive analysis that could inspire future research in this field. We will release the code when the paper is published.
</details>
<details>
<summary>摘要</summary>
量子错误修复（QEC）是量子计算机系统中的基本问题，旨在检测和修复数据QUUBITS中的错误。由于现有的量子计算机中的数据QUUBITS不可靠，实施量子错误修复是建立稳定量子计算机系统的关键步骤。近年来，基于机器学习（ML）的方法被提议用于解决这个挑战。然而，这些方法缺乏量子错误修复的深入理解。为了填补这个研究漏洞，我们在这篇论文中提供了一新的视角，发现在 ancilla qubits 中的症状是由数据QUUBITS中的错误引起的，并且远离 ancilla qubits 可以提供辅助信息，以排除一些错误的预测。因此，为检测数据QUUBITS 中的错误，我们必须考虑 ancilla qubits 中的信息。在量子错误修复中，机器学习的应用较少，因此我们在这个领域进行了一项机器学习benchmark的创建，以评估机器学习算法的捕捉远程依赖关系能力。我们对七种state-of-the-art深度学习算法进行了广泛的实验，包括卷积神经网络、图神经网络和图变换器。我们的广泛实验发现，通过扩大感知场，以利用远离 ancilla qubits 中的信息，量子错误修复的准确率得到了显著提高。例如，U-Net 可以提高 CNN 的准确率约50%。最后，我们对这一结论进行了全面的分析，以便对这一领域的未来研究提供指导。我们将在论文发表时释放代码。
</details></li>
</ul>
<hr>
<h2 id="On-the-Hardness-of-Learning-to-Stabilize-Linear-Systems"><a href="#On-the-Hardness-of-Learning-to-Stabilize-Linear-Systems" class="headerlink" title="On the Hardness of Learning to Stabilize Linear Systems"></a>On the Hardness of Learning to Stabilize Linear Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11151">http://arxiv.org/abs/2311.11151</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiong Zeng, Zexiang Liu, Zhe Du, Necmiye Ozay, Mario Sznaier</li>
<li>for: 这 paper 是研究 linear time-invariant systems 的 stabilization 问题的，具体来说是研究学习这类系统的统计困难性。</li>
<li>methods: 这 paper 使用了学习理论和 robust control 的思想，研究了一种类型的系统在不同维度下的学习难度。</li>
<li>results: 研究发现，这类系统的学习难度会随着系统维度的增加而增加 exponentially，即使这些系统可以容易地被识别。<details>
<summary>Abstract</summary>
Inspired by the work of Tsiamis et al. \cite{tsiamis2022learning}, in this paper we study the statistical hardness of learning to stabilize linear time-invariant systems. Hardness is measured by the number of samples required to achieve a learning task with a given probability. The work in \cite{tsiamis2022learning} shows that there exist system classes that are hard to learn to stabilize with the core reason being the hardness of identification. Here we present a class of systems that can be easy to identify, thanks to a non-degenerate noise process that excites all modes, but the sample complexity of stabilization still increases exponentially with the system dimension. We tie this result to the hardness of co-stabilizability for this class of systems using ideas from robust control.
</details>
<details>
<summary>摘要</summary>
根据 Tsiavmis 等人的研究 \cite{tsiamis2022learning}, 在这篇论文中我们研究了线性时间不变系统学习稳定性的统计困难性。困难性是由于学习任务中的概率。 Tsiavmis 等人的研究显示存在一些系统类型具有稳定性学习困难的核心原因，那就是识别困难。在这篇论文中，我们提出了一种系统类型，它具有非零噪变数驱动所有模式，但是稳定性学习的样本复杂性还是随系统维度的幂函数增长。我们将这结果与这种系统的稳定性困难性进行连结，使用了稳定控制的想法。
</details></li>
</ul>
<hr>
<h2 id="Auxiliary-Losses-for-Learning-Generalizable-Concept-based-Models"><a href="#Auxiliary-Losses-for-Learning-Generalizable-Concept-based-Models" class="headerlink" title="Auxiliary Losses for Learning Generalizable Concept-based Models"></a>Auxiliary Losses for Learning Generalizable Concept-based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11108">http://arxiv.org/abs/2311.11108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ivaxi0s/coop-cbm">https://github.com/ivaxi0s/coop-cbm</a></li>
<li>paper_authors: Ivaxi Sheth, Samira Ebrahimi Kahou</li>
<li>for: 提高模型透明度和性能，增强模型的可理解性。</li>
<li>methods: 提出协作概念瓶颈模型（coop-CBM），采用概念归一化损失（COL）来促进概念表示的分离和减少内部概念距离。</li>
<li>results: 在实际数据集上进行了图像分类任务的广泛实验，并研究了不同分布Shift Setting下模型的性能。结果显示，我们提出的方法可以在所有分布Shift Setting下达到最高精度，甚至超过黑盒模型的最高概念精度。<details>
<summary>Abstract</summary>
The increasing use of neural networks in various applications has lead to increasing apprehensions, underscoring the necessity to understand their operations beyond mere final predictions. As a solution to enhance model transparency, Concept Bottleneck Models (CBMs) have gained popularity since their introduction. CBMs essentially limit the latent space of a model to human-understandable high-level concepts. While beneficial, CBMs have been reported to often learn irrelevant concept representations that consecutively damage model performance. To overcome the performance trade-off, we propose cooperative-Concept Bottleneck Model (coop-CBM). The concept representation of our model is particularly meaningful when fine-grained concept labels are absent. Furthermore, we introduce the concept orthogonal loss (COL) to encourage the separation between the concept representations and to reduce the intra-concept distance. This paper presents extensive experiments on real-world datasets for image classification tasks, namely CUB, AwA2, CelebA and TIL. We also study the performance of coop-CBM models under various distributional shift settings. We show that our proposed method achieves higher accuracy in all distributional shift settings even compared to the black-box models with the highest concept accuracy.
</details>
<details>
<summary>摘要</summary>
随着神经网络在不同应用领域的使用越来越广泛，人们对神经网络的运作更加积极要进行了解。为了增强模型的透明度，概念瓶颈模型（CBM）在引入后得到了广泛的关注。CBM通过限制神经网络的幂辐空间来带来人类可理解的高级概念表示。虽然有利，但CBM经常学习不直观的概念表示，这会导致模型性能下降。为了解决性能和概念表示之间的负相关性，我们提议协同概念瓶颈模型（coop-CBM）。我们的模型中的概念表示在细化概念标签缺失时特别有意义。此外，我们引入了概念正交损失（COL），以鼓励概念表示之间的分离和减少内部概念距离。本文通过对实际世界数据集进行了广泛的实验，包括CUB、AwA2、CelebA和TIL等图像分类任务。我们还研究了coop-CBM模型在不同分布shift设置下的性能。我们的提议方法在所有分布shift设置下都实现了更高的准确率，包括黑盒模型的最高概念准确率。
</details></li>
</ul>
<hr>
<h2 id="Flat-Minima-in-Linear-Estimation-and-an-Extended-Gauss-Markov-Theorem"><a href="#Flat-Minima-in-Linear-Estimation-and-an-Extended-Gauss-Markov-Theorem" class="headerlink" title="Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem"></a>Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11093">http://arxiv.org/abs/2311.11093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Segert</li>
<li>for: 本研究考虑了线性估计问题，并提出了一种基于GAUSS-MARKOV theorem的扩展，允许偏差运算符不等于零，但是受限于一个矩阵 нор 的Schatten类型。</li>
<li>methods: 本文使用了优化估计器的简单和显式公式，并在核和spectral norms（包括 Frobenius case）中 derivation。</li>
<li>results: 通过对多种Random matrix ensembles的分析和 simulations studies，本文显示了cross-validated Nuclear和Spectral regressors可以在一些情况下超越Ridge。<details>
<summary>Abstract</summary>
We consider the problem of linear estimation, and establish an extension of the Gauss-Markov theorem, in which the bias operator is allowed to be non-zero but bounded with respect to a matrix norm of Schatten type. We derive simple and explicit formulas for the optimal estimator in the cases of Nuclear and Spectral norms (with the Frobenius case recovering ridge regression). Additionally, we analytically derive the generalization error in multiple random matrix ensembles, and compare with Ridge regression. Finally, we conduct an extensive simulation study, in which we show that the cross-validated Nuclear and Spectral regressors can outperform Ridge in several circumstances.
</details>
<details>
<summary>摘要</summary>
我们考虑了线性估计问题，并提出了允许报 bias 运算符不为零，但是对矩阵 нор 类型的 Schatten 类型做bounded的扩展。我们得到了简单明确的优化者公式，包括核心和 спектраль norm 两种情况（带有 Frobenius 情况，相当于ridge regression）。此外，我们也derived了多种Random Matrix ensemble的泛化误差，并与ridge regression进行比较。最后，我们进行了大量的实验研究，并证明了在某些情况下，cross-validate的核心和 спектраль回归可以超越ridge。Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Compositional-Fusion-of-Signals-in-Data-Embedding"><a href="#Compositional-Fusion-of-Signals-in-Data-Embedding" class="headerlink" title="Compositional Fusion of Signals in Data Embedding"></a>Compositional Fusion of Signals in Data Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11085">http://arxiv.org/abs/2311.11085</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZhijinGuo/Compositional-Fusion-of-Signals-in-Data-Embedding">https://github.com/ZhijinGuo/Compositional-Fusion-of-Signals-in-Data-Embedding</a></li>
<li>paper_authors: Zhijin Guo, Zhaozhen Xu, Martha Lewis, Nello Cristianini<br>for: 本研究旨在探讨人工智能中的嵌入，即将符号结构转换为固定维度的向量，从而实际上将多个信号融合在一起。methods: 本研究提出了两种方法：首先是相关性检测法，测量知道特征和嵌入之间的相关性，其次是加法融合检测法，视嵌入为各个特征的向量的总和。results: 应用这两种方法后，Word2Vec中的嵌入发现 combining semantic and morphological signals，BERT句 embeddings可以分解为各个单词vector的主语、谓语和 objet。在知识图基本推荐系统中，用户嵌入，即使没有训练民主数据，仍然表现出年龄和性别的信号。 这种研究表明，嵌入是多种信号的融合，从Word2Vec组件到知识图中的人类特征提示。<details>
<summary>Abstract</summary>
Embeddings in AI convert symbolic structures into fixed-dimensional vectors, effectively fusing multiple signals. However, the nature of this fusion in real-world data is often unclear. To address this, we introduce two methods: (1) Correlation-based Fusion Detection, measuring correlation between known attributes and embeddings, and (2) Additive Fusion Detection, viewing embeddings as sums of individual vectors representing attributes.   Applying these methods, word embeddings were found to combine semantic and morphological signals. BERT sentence embeddings were decomposed into individual word vectors of subject, verb and object. In the knowledge graph-based recommender system, user embeddings, even without training on demographic data, exhibited signals of demographics like age and gender.   This study highlights that embeddings are fusions of multiple signals, from Word2Vec components to demographic hints in graph embeddings.
</details>
<details>
<summary>摘要</summary>
<<SYS>>这里使用对应的字 embeddings 转换为简化中文。</SYS>>原文：Embeddings in AI 将 символічні结构转换为固定维度的 вектор，实际上融合多个讯号。但在实际数据中，这种融合的性质往往不明确。为了解决这个问题，我们提出了两种方法：1. 相似性检测法，检测知道的特征和对应的 embedding 之间的相似性。2. 总和检测法，视对应的 embedding 为各个特征的总和，将它们视为各个特征的表现。通过这两种方法，我们发现了单词嵌入在 Word2Vec 中的 Semantic 和 Morphological 信号都会被融合在一起。BERT 的句子嵌入则可以被分解为单词的主语、词汇和宾语嵌入。在基于知识库的推荐系统中，用户嵌入，即使没有训练过demographic数据，也会显示出年龄和性别的信号。这些研究表明，对应的嵌入是多种信号的融合，从 Word2Vec 的成分到知识库中的推荐系统中的 demographic 信号。
</details></li>
</ul>
<hr>
<h2 id="The-Persian-Piano-Corpus-A-Collection-Of-Instrument-Based-Feature-Extracted-Data-Considering-Dastgah"><a href="#The-Persian-Piano-Corpus-A-Collection-Of-Instrument-Based-Feature-Extracted-Data-Considering-Dastgah" class="headerlink" title="The Persian Piano Corpus: A Collection Of Instrument-Based Feature Extracted Data Considering Dastgah"></a>The Persian Piano Corpus: A Collection Of Instrument-Based Feature Extracted Data Considering Dastgah</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11074">http://arxiv.org/abs/2311.11074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parsa Rasouli, Azam Bastanfard</li>
<li>for: 这篇论文的目的是提供一个完整的波斯钢琴资料库，以便在后续的研究中更好地理解波斯音乐和钢琴在其中的角色。</li>
<li>methods: 这篇论文使用了工具类方法，包括钢琴labels和完整的元数据，以提供对波斯音乐模式（Dastgah）的覆盖性研究。</li>
<li>results: 这篇论文提供了2022年波斯钢琴曲目的特征EXTRACT，以便在后续的研究中更好地理解波斯音乐和钢琴在其中的角色。<details>
<summary>Abstract</summary>
The research in the field of music is rapidly growing, and this trend emphasizes the need for comprehensive data. Though researchers have made an effort to contribute their own datasets, many data collections lack the requisite inclusivity for comprehensive study because they are frequently focused on particular components of music or other specific topics. We have endeavored to address data scarcity by employing an instrument-based approach to provide a complete corpus related to the Persian piano. Our piano corpus includes relevant labels for Persian music mode (Dastgah) and comprehensive metadata, allowing for utilization in various popular research areas. The features extracted from 2022 Persian piano pieces in The Persian Piano Corpus (PPC) have been collected and made available to researchers, aiming for a more thorough understanding of Persian music and the role of the piano in it in subsequent steps.
</details>
<details>
<summary>摘要</summary>
研究领域内的音乐研究正在快速增长，这种趋势强调了全面的数据需求。虽然研究人员努力提供自己的数据集，但许多数据集缺乏包括性，因为它们 часто专注于特定的音乐组成部分或其他特定话题。我们尝试 Address 数据缺乏的问题，采用了 Musical Instrument 基本方法，以提供完整的波斯钢琴相关数据集。我们的钢琴数据集包括波斯音乐模式（Dastgah）相关的标签，以及完整的元数据，以便在各种流行的研究领域中使用。我们从2022年波斯钢琴曲目中提取了特征，并将其作为研究者的工具提供，以便在后续步骤中更好地理解波斯音乐和钢琴在其中的角色。
</details></li>
</ul>
<hr>
<h2 id="Tactics2D-A-Multi-agent-Reinforcement-Learning-Environment-for-Driving-Decision-making"><a href="#Tactics2D-A-Multi-agent-Reinforcement-Learning-Environment-for-Driving-Decision-making" class="headerlink" title="Tactics2D: A Multi-agent Reinforcement Learning Environment for Driving Decision-making"></a>Tactics2D: A Multi-agent Reinforcement Learning Environment for Driving Decision-making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11058">http://arxiv.org/abs/2311.11058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/woodoxen/tactics2d">https://github.com/woodoxen/tactics2d</a></li>
<li>paper_authors: Yueyuan Li, Songan Zhang, Mingyang Jiang, Xingyuan Chen, Ming Yang</li>
<li>for: 这篇论文的目的是提供一个轻松使用的多智能体学习库，用于开发自动驾驶决策算法。</li>
<li>methods: 该库包含了多种交通场景，以及具备多感知功能和交通规则违反检测的环境。它还提供了一个基线测试方法，并且具有高度可模块化和自定义化的特点。</li>
<li>results: 该库可以帮助研究人员快速开发和测试决策算法，以便更好地研究自动驾驶技术。<details>
<summary>Abstract</summary>
Tactics2D is an open-source multi-agent reinforcement learning library with a Python backend. Its goal is to provide a convenient toolset for researchers to develop decision-making algorithms for autonomous driving. The library includes diverse traffic scenarios implemented as gym-based environments equipped with multi-sensory capabilities and violation detection for traffic rules. Additionally, it features a reinforcement learning baseline tested with reasonable evaluation metrics. Tactics2D is highly modular and customizable. The source code of Tactics2D is available at https://github.com/WoodOxen/Tactics2D.
</details>
<details>
<summary>摘要</summary>
《战略2D》是一个开源的多代理人强化学习库，Python后端。它的目标是为研究人员提供一个便捷的工具集，以开发自适应驾驶决策算法。库包括各种交通场景，通过gym环境实现了多感知功能和规则违反检测。此外，它还提供了一个基线测试，并且高度可 modify 和定制。《战略2D》的源代码可以在 GitHub 上找到：https://github.com/WoodOxen/Tactics2D。
</details></li>
</ul>
<hr>
<h2 id="Challenges-in-data-based-geospatial-modeling-for-environmental-research-and-practice"><a href="#Challenges-in-data-based-geospatial-modeling-for-environmental-research-and-practice" class="headerlink" title="Challenges in data-based geospatial modeling for environmental research and practice"></a>Challenges in data-based geospatial modeling for environmental research and practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11057">http://arxiv.org/abs/2311.11057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diana Koldasbayeva, Polina Tregubova, Mikhail Gasanov, Alexey Zaytsev, Anna Petrovskaia, Evgeny Burnaev</li>
<li>for: 这篇论文主要写于地球观测数据的数据科学应用，尤其是用于环境研究中的地球空间模型使用机器学习（ML）。</li>
<li>methods: 论文详细介绍了地球空间模型中常见的特点和挑战，如数据不均衡、空间自相关、预测错误、模型泛化、域特点和不确定性估计。</li>
<li>results: 论文总结了解决这些挑战的技术和 популяр编程工具，以及地球空间智能在环境应用中的前景。<details>
<summary>Abstract</summary>
With the rise of electronic data, particularly Earth observation data, data-based geospatial modelling using machine learning (ML) has gained popularity in environmental research. Accurate geospatial predictions are vital for domain research based on ecosystem monitoring and quality assessment and for policy-making and action planning, considering effective management of natural resources. The accuracy and computation speed of ML has generally proved efficient. However, many questions have yet to be addressed to obtain precise and reproducible results suitable for further use in both research and practice. A better understanding of the ML concepts applicable to geospatial problems enhances the development of data science tools providing transparent information crucial for making decisions on global challenges such as biosphere degradation and climate change. This survey reviews common nuances in geospatial modelling, such as imbalanced data, spatial autocorrelation, prediction errors, model generalisation, domain specificity, and uncertainty estimation. We provide an overview of techniques and popular programming tools to overcome or account for the challenges. We also discuss prospects for geospatial Artificial Intelligence in environmental applications.
</details>
<details>
<summary>摘要</summary>
Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The translation is written in Traditional Chinese, which is used in Taiwan and Hong Kong.Please note that the translation is done by a machine and may not be perfect, and there may be some nuances or cultural references that are not fully captured.
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Simulators-for-Autonomous-Driving-Taxonomy-Challenges-and-Evaluation-Metrics"><a href="#A-Survey-of-Simulators-for-Autonomous-Driving-Taxonomy-Challenges-and-Evaluation-Metrics" class="headerlink" title="A Survey of Simulators for Autonomous Driving: Taxonomy, Challenges, and Evaluation Metrics"></a>A Survey of Simulators for Autonomous Driving: Taxonomy, Challenges, and Evaluation Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11056">http://arxiv.org/abs/2311.11056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueyuan Li, Wei Yuan, Weihao Yan, Qiyuan Shen, Chunxiang Wang, Ming Yang</li>
<li>For: This paper provides an in-depth review of simulators for autonomous driving, with a focus on their evolution, functionalities, and limitations.* Methods: The paper classifies simulators based on their functions, including traffic flow simulators, vehicle dynamics simulators, scenario editors, sensory data generators, and driving strategy validators. It also explores commercial and open-source simulators and evaluates their performance using qualitative and quantitative metrics.* Results: The paper identifies the primary limitations of simulators as fidelity and efficiency concerns and proposes solutions such as enhancing adverse weather simulation, automated map reconstruction, and interactive traffic participants. It also explores headless simulation and multiple-speed simulation techniques to improve the realism and efficiency of simulators.<details>
<summary>Abstract</summary>
Simulators have irreplaceable importance for the research and development of autonomous driving. Besides saving resources, labor, and time, simulation is the only feasible way to reproduce many severe accident scenarios. Despite their widespread adoption across academia and industry, there is an absence in the evolutionary trajectory of simulators and critical discourse on their limitations.   To bridge the gap in research, this paper conducts an in-depth review of simulators for autonomous driving. It delineates the three-decade development into three stages: specialized development period, gap period, and comprehensive development, from which it detects a trend of implementing comprehensive functionalities and open-source accessibility. Then it classifies the simulators by functions, identifying five categories: traffic flow simulator, vehicle dynamics simulator, scenario editor, sensory data generator, and driving strategy validator. Simulators that amalgamate diverse features are defined as comprehensive simulators. By investigating commercial and open-source simulators, this paper reveals that the critical issues faced by simulators primarily revolve around fidelity and efficiency concerns. This paper justifies that enhancing the realism of adverse weather simulation, automated map reconstruction, and interactive traffic participants will bolster credibility. Concurrently, headless simulation and multiple-speed simulation techniques will exploit the theoretic advantages. Moreover, this paper delves into potential solutions for the identified issues. It explores qualitative and quantitative evaluation metrics to assess the simulator's performance. This paper guides users to find suitable simulators efficiently and provides instructive suggestions for developers to improve simulator efficacy purposefully.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于自动驾驶研发中的研究和开发，模拟器具有不可或缺的重要性。除了节省资源、劳动力和时间之外，模拟是重创各种严重事故场景的唯一可行方式。尽管在学术和业界中广泛应用，但模拟器的演化征程和批判性讨论却缺乏。 为了填补这些研究的空白，本文进行了自动驾驶模拟器的深入综述。它将三十年的发展分成三个阶段：特殊开发期、阶段差期和综合开发，从中探测到了实现广泛功能和开源可访问性的趋势。然后它将模拟器按功能分类，确定了五种类别：交通流模拟器、车辆动力模拟器、enario编辑器、感知数据生成器和驾驶策略验证器。拥有多种功能的模拟器被定义为综合模拟器。通过商业和开源模拟器的调查，本文发现了模拟器的主要问题是准确性和效率问题。本文认为，提高风暴天气模拟、自动地图重建和互动交通参与者会增强信息的准确性。同时，无头模拟和多速模拟技术可以实现理论上的优势。此外，本文还探讨了模拟器问题的解决方案。它提出了评估模拟器性能的量化和质量评价指标，并为用户寻找适合的模拟器提供了有用的指导。为开发者提高模拟器效果，本文还提供了有价值的建议。
</details></li>
</ul>
<hr>
<h2 id="DenseNet-and-Support-Vector-Machine-classifications-of-major-depressive-disorder-using-vertex-wise-cortical-features"><a href="#DenseNet-and-Support-Vector-Machine-classifications-of-major-depressive-disorder-using-vertex-wise-cortical-features" class="headerlink" title="DenseNet and Support Vector Machine classifications of major depressive disorder using vertex-wise cortical features"></a>DenseNet and Support Vector Machine classifications of major depressive disorder using vertex-wise cortical features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11046">http://arxiv.org/abs/2311.11046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vladimir Belov, Tracy Erwin-Grabner, Ling-Li Zeng, Christopher R. K. Ching, Andre Aleman, Alyssa R. Amod, Zeynep Basgoze, Francesco Benedetti, Bianca Besteher, Katharina Brosch, Robin Bülow, Romain Colle, Colm G. Connolly, Emmanuelle Corruble, Baptiste Couvy-Duchesne, Kathryn Cullen, Udo Dannlowski, Christopher G. Davey, Annemiek Dols, Jan Ernsting, Jennifer W. Evans, Lukas Fisch, Paola Fuentes-Claramonte, Ali Saffet Gonul, Ian H. Gotlib, Hans J. Grabe, Nynke A. Groenewold, Dominik Grotegerd, Tim Hahn, J. Paul Hamilton, Laura K. M. Han, Ben J Harrison, Tiffany C. Ho, Neda Jahanshad, Alec J. Jamieson, Andriana Karuk, Tilo Kircher, Bonnie Klimes-Dougan, Sheri-Michelle Koopowitz, Thomas Lancaster, Ramona Leenings, Meng Li, David E. J. Linden, Frank P. MacMaster, David M. A. Mehler, Susanne Meinert, Elisa Melloni, Bryon A. Mueller, Benson Mwangi, Igor Nenadić, Amar Ojha, Yasumasa Okamoto, Mardien L. Oudega, Brenda W. J. H. Penninx, Sara Poletti, Edith Pomarol-Clotet, Maria J. Portella, Elena Pozzi, Joaquim Radua, Elena Rodríguez-Cano, Matthew D. Sacchet, Raymond Salvador, Anouk Schrantee, Kang Sim, Jair C. Soares, Aleix Solanes, Dan J. Stein, Frederike Stein, Aleks Stolicyn, Sophia I. Thomopoulos, Yara J. Toenders, Aslihan Uyar-Demir, Eduard Vieta, Yolanda Vives-Gilabert, Henry Völzke, Martin Walter, Heather C. Whalley, Sarah Whittle, Nils Winter, Katharina Wittfeld, Margaret J. Wright, Mon-Ju Wu, Tony T. Yang, Carlos Zarate, Dick J. Veltman, Lianne Schmaal, Paul M. Thompson, Roberto Goya-Maldonado</li>
<li>for: 这个研究是为了检测主要抑郁疾病（MDD）是否有 morphological alterations in the brain 的问题。</li>
<li>methods: 这个研究使用了深度学习工具来分析神经成像数据，并使用了 DenseNet 和 Support Vector Machine（SVM）两种类型的分类器。</li>
<li>results: 研究发现，不 matter which classifier is used, the integration of vertex-wise morphometric features did not lead to differentiability between MDD and healthy controls（HC），并且site effect also exists。 Therefore, the study suggests that MDD classification on this combination of features and classifiers is unfeasible。<details>
<summary>Abstract</summary>
Major depressive disorder (MDD) is a complex psychiatric disorder that affects the lives of hundreds of millions of individuals around the globe. Even today, researchers debate if morphological alterations in the brain are linked to MDD, likely due to the heterogeneity of this disorder. The application of deep learning tools to neuroimaging data, capable of capturing complex non-linear patterns, has the potential to provide diagnostic and predictive biomarkers for MDD. However, previous attempts to demarcate MDD patients and healthy controls (HC) based on segmented cortical features via linear machine learning approaches have reported low accuracies. In this study, we used globally representative data from the ENIGMA-MDD working group containing an extensive sample of people with MDD (N=2,772) and HC (N=4,240), which allows a comprehensive analysis with generalizable results. Based on the hypothesis that integration of vertex-wise cortical features can improve classification performance, we evaluated the classification of a DenseNet and a Support Vector Machine (SVM), with the expectation that the former would outperform the latter. As we analyzed a multi-site sample, we additionally applied the ComBat harmonization tool to remove potential nuisance effects of site. We found that both classifiers exhibited close to chance performance (balanced accuracy DenseNet: 51%; SVM: 53%), when estimated on unseen sites. Slightly higher classification performance (balanced accuracy DenseNet: 58%; SVM: 55%) was found when the cross-validation folds contained subjects from all sites, indicating site effect. In conclusion, the integration of vertex-wise morphometric features and the use of the non-linear classifier did not lead to the differentiability between MDD and HC. Our results support the notion that MDD classification on this combination of features and classifiers is unfeasible.
</details>
<details>
<summary>摘要</summary>
Major Depressive Disorder (MDD) 是一种复杂的心理疾病，影响全球数百万人的生活。尽管研究人员今天仍然debatewhether morphological alterations in the brain are linked to MDD, but the application of deep learning tools to neuroimaging data has the potential to provide diagnostic and predictive biomarkers for MDD. However, previous attempts to distinguish MDD patients and healthy controls (HC) based on segmented cortical features via linear machine learning approaches have reported low accuracies.在这个研究中，我们使用了ENIGMA-MDD工作组的全球代表性数据集（N=2,772）和HC（N=4,240），可以进行全面的分析并得到普遍可靠的结果。基于假设集成 vertex-wise cortical features可以提高分类性能，我们评估了DenseNet和Support Vector Machine (SVM)两种类器，期望前者能够超越后者。由于我们分析了多个站点的数据，我们还应用了ComBat协调工具来除掉可能的站点效应。我们发现，无论使用DenseNet或SVM类器，在未看过的站点上估计时，两者的准确率都接近机会准确率（balanced accuracy DenseNet: 51%; SVM: 53%）。然而，当分割folds包含所有站点时，两者的准确率（balanced accuracy DenseNet: 58%; SVM: 55%）提高了一些， indicating that site effect played a role.结论：在这种 combinaton of features and classifiers 上，不可能 diferenciate MDD and HC。our results support the notion that MDD classification on this combination of features and classifiers is unfeasible.
</details></li>
</ul>
<hr>
<h2 id="SORTAD-Self-Supervised-Optimized-Random-Transformations-for-Anomaly-Detection-in-Tabular-Data"><a href="#SORTAD-Self-Supervised-Optimized-Random-Transformations-for-Anomaly-Detection-in-Tabular-Data" class="headerlink" title="SORTAD: Self-Supervised Optimized Random Transformations for Anomaly Detection in Tabular Data"></a>SORTAD: Self-Supervised Optimized Random Transformations for Anomaly Detection in Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11018">http://arxiv.org/abs/2311.11018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Hay, Pablo Liberman</li>
<li>for: 这篇研究目的是为了开发一个自主式异常探测方法，用于检测表格资料中的异常。</li>
<li>methods: 这篇研究使用了随机变数的应用，并使用这些预测的变数来检测异常。</li>
<li>results: 研究获得了当前顶尖的结果，在多个常用的异常探测数据集上都取得了良好的成绩，并且在所有测试数据集上也取得了总体良好的结果。<details>
<summary>Abstract</summary>
We consider a self-supervised approach to anomaly detection in tabular data. Random transformations are applied to the data, and then each transformation is identified based on its output. These predicted transformations are used to identify anomalies. In tabular data this approach faces many challenges that are related to the uncorrelated nature of the data. These challenges affect the transformations that should be used, as well as the use of their predictions. To this end, we propose SORTAD, a novel algorithm that is tailor-made to solve these challenges. SORTAD optimally chooses random transformations that help the classification process, and have a scoring function that is more sensitive to the changes in the transformations classification prediction encountered in tabular data. SORTAD achieved state-of-the-art results on multiple commonly used anomaly detection data sets, as well as in the overall results across all data sets tested.
</details>
<details>
<summary>摘要</summary>
我们考虑了一种自助学习方法 для异常检测在表格数据中。在这种方法中，随机变换被应用于数据，然后每个变换被预测。这些预测的变换被用来标识异常。在表格数据中，这种方法遇到了许多与不相关性相关的挑战。这些挑战影响了应用的变换以及其预测的用途。为此，我们提出了SORTAD算法，这是特制的解决这些挑战的算法。SORTAD优选随机变换，帮助分类过程，并且有一个更敏感的变换分类预测值的评分函数。SORTAD在多个常用的异常检测数据集上达到了状态的最佳结果，以及在所有数据集上的总结果。
</details></li>
</ul>
<hr>
<h2 id="Wasserstein-Convergence-Guarantees-for-a-General-Class-of-Score-Based-Generative-Models"><a href="#Wasserstein-Convergence-Guarantees-for-a-General-Class-of-Score-Based-Generative-Models" class="headerlink" title="Wasserstein Convergence Guarantees for a General Class of Score-Based Generative Models"></a>Wasserstein Convergence Guarantees for a General Class of Score-Based Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11003">http://arxiv.org/abs/2311.11003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Gao, Hoang M. Nguyen, Lingjiong Zhu</li>
<li>for: 这 paper 是为了提供Score-based generative models（SGMs）的convrgence guarantees，以便在各种应用中达到state-of-the-art性能。</li>
<li>methods: 这 paper 使用了一种通用的 SGMs 类型，assuming accurate score estimates和smooth log-concave data distribution，并特定了几种具体的 forward processes，包括一些 newly proposed 的模型。</li>
<li>results: 这 paper 提供了一个 upper bound 的 iteration complexity  для每个模型，并提供了一个 lower bound 当数据分布是 Gaussian。 numerically, 这 paper 通过对 CIFAR-10 上的 unconditional image generation 进行实验，发现实验结果与理论预测相一致，并且模型使用我们 newly proposed forward processes 可以超越现有模型。<details>
<summary>Abstract</summary>
Score-based generative models (SGMs) is a recent class of deep generative models with state-of-the-art performance in many applications. In this paper, we establish convergence guarantees for a general class of SGMs in 2-Wasserstein distance, assuming accurate score estimates and smooth log-concave data distribution. We specialize our result to several concrete SGMs with specific choices of forward processes modelled by stochastic differential equations, and obtain an upper bound on the iteration complexity for each model, which demonstrates the impacts of different choices of the forward processes. We also provide a lower bound when the data distribution is Gaussian. Numerically, we experiment SGMs with different forward processes, some of which are newly proposed in this paper, for unconditional image generation on CIFAR-10. We find that the experimental results are in good agreement with our theoretical predictions on the iteration complexity, and the models with our newly proposed forward processes can outperform existing models.
</details>
<details>
<summary>摘要</summary>
score-based生成模型（SGM）是一种最近的深度生成模型，在许多应用场景中表现出色。在这篇论文中，我们证明了SGM在2-Wasserstein距离下的收敛保证，假设批处数据分布是准确的评估值和光滑凹陷分布。我们对特定的SGM进行特化，并得到了每个模型的迭代复杂度上限，这 demonstartes了不同的前进过程选择对模型的影响。我们还提供了 Gaussian 分布时的下界。 numerically, we experiment SGMs with different forward processes, some of which are newly proposed in this paper, for unconditional image generation on CIFAR-10. We find that the experimental results are in good agreement with our theoretical predictions on the iteration complexity, and the models with our newly proposed forward processes can outperform existing models.Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="BrainZ-BP-A-Non-invasive-Cuff-less-Blood-Pressure-Estimation-Approach-Leveraging-Brain-Bio-impedance-and-Electrocardiogram"><a href="#BrainZ-BP-A-Non-invasive-Cuff-less-Blood-Pressure-Estimation-Approach-Leveraging-Brain-Bio-impedance-and-Electrocardiogram" class="headerlink" title="BrainZ-BP: A Non-invasive Cuff-less Blood Pressure Estimation Approach Leveraging Brain Bio-impedance and Electrocardiogram"></a>BrainZ-BP: A Non-invasive Cuff-less Blood Pressure Estimation Approach Leveraging Brain Bio-impedance and Electrocardiogram</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10996">http://arxiv.org/abs/2311.10996</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bf-yang/brainz-bp">https://github.com/bf-yang/brainz-bp</a></li>
<li>paper_authors: Bufang Yang, Le Liu, Wenxuan Wu, Mengliang Zhou, Hongxing Liu, Xinbao Ning</li>
<li>for: 这个研究旨在探讨利用大脑 bio-impedance（BIOZ）测量血压（BP）的可能性，并提出了一种新的无捕获压力测量方法（BrainZ-BP）。</li>
<li>methods: 这个研究使用了两个电极位于前后脑骨的安排，测量大脑 BIOZ，并提取了脉冲传输时间和脑 BIOZ 形态特征等特征，并将其传输给四种回归模型进行 BP 估计。</li>
<li>results: 研究结果表明，Random Forest 回归模型的 Mean Absolute Error、Root Mean Square Error 和 Correlation Coefficient 分别为 2.17 mmHg、3.91 mmHg 和 0.90  для systolic pressure 估计，并为 1.71 mmHg、3.02 mmHg 和 0.89  для diastolic pressure 估计。这些结果表明 BrainZ-BP 可以准确地估计血压。<details>
<summary>Abstract</summary>
Accurate and continuous blood pressure (BP) monitoring is essential to the early prevention of cardiovascular diseases. Non-invasive and cuff-less BP estimation algorithm has gained much attention in recent years. Previous studies have demonstrated that brain bio-impedance (BIOZ) is a promising technique for non-invasive intracranial pressure (ICP) monitoring. Clinically, treatment for patients with traumatic brain injuries (TBI) requires monitoring the ICP and BP of patients simultaneously. Estimating BP by brain BIOZ directly can reduce the number of sensors attached to the patients, thus improving their comfort. To address the issues, in this study, we explore the feasibility of leveraging brain BIOZ for BP estimation and propose a novel cuff-less BP estimation approach called BrainZ-BP. Two electrodes are placed on the forehead and occipital bone of the head in the anterior-posterior direction for brain BIOZ measurement. Various features including pulse transit time and morphological features of brain BIOZ are extracted and fed into four regression models for BP estimation. Results show that the mean absolute error, root mean square error, and correlation coefficient of random forest regression model are 2.17 mmHg, 3.91 mmHg, and 0.90 for systolic pressure estimation, and are 1.71 mmHg, 3.02 mmHg, and 0.89 for diastolic pressure estimation. The presented BrainZ-BP can be applied in the brain BIOZ-based ICP monitoring scenario to monitor BP simultaneously.
</details>
<details>
<summary>摘要</summary>
Accurate and continuous blood pressure (BP) monitoring is essential to the early prevention of cardiovascular diseases. Non-invasive and cuff-less BP estimation algorithm has gained much attention in recent years. Previous studies have demonstrated that brain bio-impedance (BIOZ) is a promising technique for non-invasive intracranial pressure (ICP) monitoring. Clinically, treatment for patients with traumatic brain injuries (TBI) requires monitoring the ICP and BP of patients simultaneously. Estimating BP by brain BIOZ directly can reduce the number of sensors attached to the patients, thus improving their comfort. To address the issues, in this study, we explore the feasibility of leveraging brain BIOZ for BP estimation and propose a novel cuff-less BP estimation approach called BrainZ-BP. Two electrodes are placed on the forehead and occipital bone of the head in the anterior-posterior direction for brain BIOZ measurement. Various features including pulse transit time and morphological features of brain BIOZ are extracted and fed into four regression models for BP estimation. Results show that the mean absolute error, root mean square error, and correlation coefficient of random forest regression model are 2.17 mmHg, 3.91 mmHg, and 0.90 for systolic pressure estimation, and are 1.71 mmHg, 3.02 mmHg, and 0.89 for diastolic pressure estimation. The presented BrainZ-BP can be applied in the brain BIOZ-based ICP monitoring scenario to monitor BP simultaneously.Here's a word-for-word translation of the text into Simplified Chinese:精准和不间断的血压监测是预防心血管疾病的关键。非侵入式和无捕血压估算算法在最近几年内得到了广泛关注。先前的研究表明，脑 bio-impedance（BIOZ）是非侵入式 intracranial pressure（ICP）监测的一个可靠的技术。临床上，对抢救性脑 травuma（TBI）患者的治疗需要同时监测ICP和血压。通过脑 BIOZ 直接估算血压可以降低患者身上的感测器数量，从而改善他们的 комфор度。为解决这些问题，本研究提出了利用脑 BIOZ 进行血压估算的可能性，并提出了一种新的无捕血压估算方法，即 BrainZ-BP。在脑 BIOZ 测量中，两个电极被安置在头颈部的前后方向上，用于测量脑 BIOZ。从脑 BIOZ 中提取了多种特征，包括脉搏传输时间和脑 BIOZ 的形态特征，并将其传输给四种回归模型进行血压估算。结果显示，Random Forest 回归模型的平均绝对误差、根圆平方误差和相关系数分别为2.17 mmHg、3.91 mmHg和0.90 для systolic pressure 估算，分别为1.71 mmHg、3.02 mmHg和0.89 для diastolic pressure 估算。所提出的 BrainZ-BP 可以在脑 BIOZ 基于 ICP 监测场景中进行同时监测血压。
</details></li>
</ul>
<hr>
<h2 id="EdgeFM-Leveraging-Foundation-Model-for-Open-set-Learning-on-the-Edge"><a href="#EdgeFM-Leveraging-Foundation-Model-for-Open-set-Learning-on-the-Edge" class="headerlink" title="EdgeFM: Leveraging Foundation Model for Open-set Learning on the Edge"></a>EdgeFM: Leveraging Foundation Model for Open-set Learning on the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10986">http://arxiv.org/abs/2311.10986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bufang Yang, Lixing He, Neiwen Ling, Zhenyu Yan, Guoliang Xing, Xian Shuai, Xiaozhe Ren, Xin Jiang</li>
<li>for: 这个研究旨在探讨如何将深度学习（Deep Learning）模型实现在资源有限的边缘设备（IoT devices）上，并且在不同的环境和任务下保持模型的一致性。</li>
<li>methods: 本研究提出了一个名为EdgeFM的边缘云合作系统，通过选择上传无标的数据来询问云上的基础模型（Foundation Models，FMs），并且自动在 runtime 进行模型交替，以应对数据不确定性和网络动态变化。</li>
<li>results: 根据三个公共数据集和两个自行收集的数据集进行评估，EdgeFM 可以将终端延迟时间降低到3.2倍，并且与基eline相比，实现了34.3%的准确度提升。<details>
<summary>Abstract</summary>
Deep Learning (DL) models have been widely deployed on IoT devices with the help of advancements in DL algorithms and chips. However, the limited resources of edge devices make these on-device DL models hard to be generalizable to diverse environments and tasks. Although the recently emerged foundation models (FMs) show impressive generalization power, how to effectively leverage the rich knowledge of FMs on resource-limited edge devices is still not explored. In this paper, we propose EdgeFM, a novel edge-cloud cooperative system with open-set recognition capability. EdgeFM selectively uploads unlabeled data to query the FM on the cloud and customizes the specific knowledge and architectures for edge models. Meanwhile, EdgeFM conducts dynamic model switching at run-time taking into account both data uncertainty and dynamic network variations, which ensures the accuracy always close to the original FM. We implement EdgeFM using two FMs on two edge platforms. We evaluate EdgeFM on three public datasets and two self-collected datasets. Results show that EdgeFM can reduce the end-to-end latency up to 3.2x and achieve 34.3% accuracy increase compared with the baseline.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）模型已广泛部署在物联网设备上，由于DL算法和芯片的进步。然而，边缘设备的限制资源使得这些边缘DL模型难以在多样环境和任务中具有普适性。虽然最近出现的基础模型（FM）表现出了很好的泛化能力，但是如何有效地利用边缘设备的贫乏资源来激活FM的知识仍然不是研究的主要方向。在本文中，我们提出了EdgeFM，一种边缘云合作系统，具有开放集合识别能力。EdgeFM在云端查询FM的基础上，选择上传无标签数据，并自适应边缘模型的特定知识和结构。同时，EdgeFM在运行时进行动态模型交换，考虑到数据不确定性和动态网络变化，以确保精度总是相对于原FM做出最佳化。我们使用了两个FM在两个边缘平台进行实现EdgeFM。我们对EdgeFM进行了三个公共数据集和两个自收集数据集的测试。结果表明，EdgeFM可以将终端延迟减少至3.2倍，并实现对基准值的34.3%的准确率提升。
</details></li>
</ul>
<hr>
<h2 id="Polynomial-Time-Solutions-for-ReLU-Network-Training-A-Complexity-Classification-via-Max-Cut-and-Zonotopes"><a href="#Polynomial-Time-Solutions-for-ReLU-Network-Training-A-Complexity-Classification-via-Max-Cut-and-Zonotopes" class="headerlink" title="Polynomial-Time Solutions for ReLU Network Training: A Complexity Classification via Max-Cut and Zonotopes"></a>Polynomial-Time Solutions for ReLU Network Training: A Complexity Classification via Max-Cut and Zonotopes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10972">http://arxiv.org/abs/2311.10972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Wang, Mert Pilanci</li>
<li>for:  investigate the complexity of training a two-layer ReLU neural network with weight decay regularization</li>
<li>methods:  using a standard cone-constrained convex program and developing a randomized algorithm</li>
<li>results:  prove that the hardness of approximation of ReLU networks mirrors the complexity of the Max-Cut problem, and develop polynomial-time approximation guarantees for certain categories of datasets<details>
<summary>Abstract</summary>
We investigate the complexity of training a two-layer ReLU neural network with weight decay regularization. Previous research has shown that the optimal solution of this problem can be found by solving a standard cone-constrained convex program. Using this convex formulation, we prove that the hardness of approximation of ReLU networks not only mirrors the complexity of the Max-Cut problem but also, in certain special cases, exactly corresponds to it. In particular, when $\epsilon\leq\sqrt{84/83}-1\approx 0.006$, we show that it is NP-hard to find an approximate global optimizer of the ReLU network objective with relative error $\epsilon$ with respect to the objective value. Moreover, we develop a randomized algorithm which mirrors the Goemans-Williamson rounding of semidefinite Max-Cut relaxations. To provide polynomial-time approximations, we classify training datasets into three categories: (i) For orthogonal separable datasets, a precise solution can be obtained in polynomial-time. (ii) When there is a negative correlation between samples of different classes, we give a polynomial-time approximation with relative error $\sqrt{\pi/2}-1\approx 0.253$. (iii) For general datasets, the degree to which the problem can be approximated in polynomial-time is governed by a geometric factor that controls the diameter of two zonotopes intrinsic to the dataset. To our knowledge, these results present the first polynomial-time approximation guarantees along with first hardness of approximation results for regularized ReLU networks.
</details>
<details>
<summary>摘要</summary>
我们研究具有权重减权化的两层ReLU神经网络的训练复杂性。先前的研究表明，这个问题的优化解决方案可以通过标准的 cone-constrained 几何 програм约束来找到。使用这个几何形式，我们证明了ReLU网络的困难性不仅和Max-Cut问题的复杂性相同，而且在某些特殊情况下，甚至与其相同。具体来说，当 $\epsilon\leq\sqrt{84/83}-1\approx 0.006$ 时，我们显示了一个NP困难的问题：在Relative Error $\epsilon$ 下，不可能在 polynomial-time 内找到ReLU网络目标函数的approximate全局最优值。此外，我们开发了一种随机化的算法，它类似于Goemans-Williamson 的半definite Max-Cut 缩放。为了提供 polynomial-time approxiamtion，我们将训练数据分为三类：（i）对吸引式分割数据进行精确解决，可以在 polynomial-time 内完成。（ii）当不同类别样本之间存在负相关性时，我们提供了一种 polynomial-time approxiamtion，其相对误差为 $\sqrt{\pi/2}-1\approx 0.253$。（iii）对一般数据集，问题的approxiamtion程度由数据集的径向因子控制，这个因子控制两个zonotope 的径向。我们认为这些结果是 regularized ReLU 神经网络的首个 polynomial-time approximation guarantee 和首个困难性 results。
</details></li>
</ul>
<hr>
<h2 id="Learning-Deterministic-Finite-Automata-from-Confidence-Oracles"><a href="#Learning-Deterministic-Finite-Automata-from-Confidence-Oracles" class="headerlink" title="Learning Deterministic Finite Automata from Confidence Oracles"></a>Learning Deterministic Finite Automata from Confidence Oracles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10963">http://arxiv.org/abs/2311.10963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wilson Wu</li>
<li>for: 学习一个确定性Finite Automaton（DFA）从一个信任函数($Q$)中。</li>
<li>methods: 使用一个信任函数($Q$)，该函数对字符串$x\in\Sigma^*$返回一个分数，表示该字符串是否在目标语言($L$)中。</li>
<li>results: 学习一个DFA表示，该表示保留了信任函数($Q$)中的信息，并且与该函数在高度信任的地方匹配紧密。<details>
<summary>Abstract</summary>
We discuss the problem of learning a deterministic finite automaton (DFA) from a confidence oracle. That is, we are given access to an oracle $Q$ with incomplete knowledge of some target language $L$ over an alphabet $\Sigma$; the oracle maps a string $x\in\Sigma^*$ to a score in the interval $[-1,1]$ indicating its confidence that the string is in the language. The interpretation is that the sign of the score signifies whether $x\in L$, while the magnitude $|Q(x)|$ represents the oracle's confidence. Our goal is to learn a DFA representation of the oracle that preserves the information that it is confident in. The learned DFA should closely match the oracle wherever it is highly confident, but it need not do this when the oracle is less sure of itself.
</details>
<details>
<summary>摘要</summary>
我们讨论一个 deterministic finite automaton (DFA) 的学习问题， Specifically, we are given access to an oracle $Q$ with incomplete knowledge of some target language $L$ over an alphabet $\Sigma$; the oracle maps a string $x\in\Sigma^*$ to a score in the interval $[-1,1]$ indicating its confidence that the string is in the language. The interpretation is that the sign of the score signifies whether $x\in L$, while the magnitude $|Q(x)|$ represents the oracle's confidence. Our goal is to learn a DFA representation of the oracle that preserves the information that it is confident in. The learned DFA should closely match the oracle wherever it is highly confident, but it need not do this when the oracle is less sure of itself.Here's the translation breakdown:* "deterministic finite automaton" (DFA) 被翻译为 "确定型 finite automaton" (CFAs)* "confidence oracle" 被翻译为 "信任 oracle"* "target language" 被翻译为 "目标语言"* "alphabet" 被翻译为 "字母"* "score" 被翻译为 "分数"* "interval" 被翻译为 "区间"* "sign" 被翻译为 "符号"* "magnitude" 被翻译为 "大小"* "preserves" 被翻译为 "保持"* "closely match" 被翻译为 "匹配"Note that the translation is in Simplified Chinese, which is the most widely used variety of Chinese. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Classification-Methods-Based-on-Machine-Learning-for-the-Analysis-of-Fetal-Health-Data"><a href="#Classification-Methods-Based-on-Machine-Learning-for-the-Analysis-of-Fetal-Health-Data" class="headerlink" title="Classification Methods Based on Machine Learning for the Analysis of Fetal Health Data"></a>Classification Methods Based on Machine Learning for the Analysis of Fetal Health Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10962">http://arxiv.org/abs/2311.10962</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferzaad/Diabetes">https://github.com/ferzaad/Diabetes</a></li>
<li>paper_authors: Binod Regmi, Chiranjibi Shah</li>
<li>for: This paper aims to assess the classification performance of various machine learning models for fetal health analysis.</li>
<li>methods: The authors use machine learning models such as SVM, RF, and TabNet, as well as dimensionality reduction techniques like PCA and LDA.</li>
<li>results: The TabNet model achieves a classification accuracy of 94.36% on a fetal health dataset, demonstrating the effectiveness of machine learning-based techniques for fetal health analysis.<details>
<summary>Abstract</summary>
The persistent battle to decrease childhood mortality serves as a commonly employed benchmark for gauging advancements in the field of medicine. Globally, the under-5 mortality rate stands at approximately 5 million, with a significant portion of these deaths being avoidable. Given the significance of this problem, Machine learning-based techniques have emerged as a prominent tool for assessing fetal health. In this work, we have analyzed the classification performance of various machine learning models for fetal health analysis. Classification performance of various machine learning models, such as support vector machine (SVM), random forest(RF), and attentive interpretable tabular learning (TabNet) have been assessed on fetal health. Moreover, dimensionality reduction techniques, such as Principal component analysis (PCA) and Linear discriminant analysis (LDA) have been implemented to obtain better classification performance with less number of features. A TabNet model on a fetal health dataset provides a classification accuracy of 94.36%. In general, this technology empowers doctors and healthcare experts to achieve precise fetal health classification and identify the most influential features in the process.
</details>
<details>
<summary>摘要</summary>
persistent battle to decrease childhood mortality serves as a commonly employed benchmark for gauging advancements in the field of medicine。 globally, the under-5 mortality rate stands at approximately 5 million, with a significant portion of these deaths being avoidable。 given the significance of this problem, machine learning-based techniques have emerged as a prominent tool for assessing fetal health。 in this work, we have analyzed the classification performance of various machine learning models for fetal health analysis。 classification performance of various machine learning models, such as support vector machine (SVM), random forest (RF), and attentive interpretable tabular learning (TabNet) have been assessed on fetal health。 moreover, dimensionality reduction techniques, such as principal component analysis (PCA) and linear discriminant analysis (LDA) have been implemented to obtain better classification performance with less number of features。 a TabNet model on a fetal health dataset provides a classification accuracy of 94.36%。 in general, this technology empowers doctors and healthcare experts to achieve precise fetal health classification and identify the most influential features in the process。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Taxonomic-analysis-of-asteroids-with-artificial-neural-networks"><a href="#Taxonomic-analysis-of-asteroids-with-artificial-neural-networks" class="headerlink" title="Taxonomic analysis of asteroids with artificial neural networks"></a>Taxonomic analysis of asteroids with artificial neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10954">http://arxiv.org/abs/2311.10954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nanping Luo, Xiaobin Wang, Shenghong Gu, Antti Penttilä, Karri Muinonen, Yisi Liu</li>
<li>for:  asteroid taxonomy and composition analysis</li>
<li>methods:  artificial neural networks (ANNs) and spectral data from the Chinese Space Survey telescope (CSST)</li>
<li>results:  higher than 92% accuracy in asteroid classification using the ANN tool, reasonable predictions for known taxonomic labels, and potential application for analyzing CSST asteroid spectra in the future.Here’s the simplified Chinese text:</li>
<li>for: asteroid的分类和组成分析</li>
<li>methods: 人工神经网络（ANNs）和中国空间探测 telescope（CSST）的spectral数据</li>
<li>results:  ANN工具的准确率高于92%，对已知分类标签的预测结果是合理的，并且可能应用于未来分析CSST asteroid spectrum。<details>
<summary>Abstract</summary>
We study the surface composition of asteroids with visible and/or infrared spectroscopy. For example, asteroid taxonomy is based on the spectral features or multiple color indices in visible and near-infrared wavelengths. The composition of asteroids gives key information to understand their origin and evolution. However, we lack compositional information for faint asteroids due to limits of ground-based observational instruments. In the near future, the Chinese Space Survey telescope (CSST) will provide multiple colors and spectroscopic data for asteroids of apparent magnitude brighter than 25 mag and 23 mag, respectively. For the aim of analysis of the CSST spectroscopic data, we applied an algorithm using artificial neural networks (ANNs) to establish a preliminary classification model for asteroid taxonomy according to the design of the survey module of CSST. Using the SMASS II spectra and the Bus-Binzel taxonomy system, our ANN classification tool composed of 5 individual ANNs is constructed, and the accuracy of this classification system is higher than 92 %. As the first application of our ANN tool, 64 spectra of 42 asteroids obtained in 2006 and 2007 by us with the 2.16-m telescope in the Xinglong station (Observatory Code 327) of National Astronomical Observatory of China are analyzed. The predicted labels of these spectra using our ANN tool are found to be reasonable when compared to their known taxonomic labels. Considering the accuracy and stability, our ANN tool can be applied to analyse the CSST asteroid spectra in the future.
</details>
<details>
<summary>摘要</summary>
我们研究小行星表面成分，使用可见和近红外谱学观测。例如，小行星分类是基于谱spectral特征或多色指数在可见和近红外波长上。小行星的成分提供了关键信息以解释它们的起源和演化。但我们对暗淡小行星的 compositional 信息缺乏。未来，中国空间探测 telescope（CSST）将提供多种颜色和谱学数据，用于 asteroids 的 apparent magnitude  brighter than 25 mag 和 23 mag 。为了分析 CSST 谱学数据的目的，我们采用了一种使用人工神经网络（ANNs）的算法，以建立一个初步的分类模型，以便根据 CSST 的设计模块进行 asteroid 分类。使用 SMASS II 谱和 Bus-Binzel 分类系统，我们的 ANN 分类工具由 5 个个 ANNS 组成，其准确率高于 92 %。作为我们 ANN 工具的首次应用，我们分析了 2006 和 2007 年我们使用 2.16-m  telescope 在中国天文台（Observatory Code 327）的 Xinglong 站进行的 64 个spectrum 数据，并发现这些spectrum 的预测标签使用我们 ANN 工具是合理的，与知道的分类标签相比。考虑准确和稳定，我们的 ANN 工具可以在未来用于分析 CSST 小行星谱数据。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Data-Driven-and-Knowledge-Driven-Approaches-for-Safety-Critical-Scenario-Generation-in-Automated-Vehicle-Validation"><a href="#Bridging-Data-Driven-and-Knowledge-Driven-Approaches-for-Safety-Critical-Scenario-Generation-in-Automated-Vehicle-Validation" class="headerlink" title="Bridging Data-Driven and Knowledge-Driven Approaches for Safety-Critical Scenario Generation in Automated Vehicle Validation"></a>Bridging Data-Driven and Knowledge-Driven Approaches for Safety-Critical Scenario Generation in Automated Vehicle Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10937">http://arxiv.org/abs/2311.10937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunkun Hao, Lu Liu, Wen Cui, Jianxing Zhang, Songyang Yan, Yuxi Pan, Zijiang Yang</li>
<li>for: The paper is written to address the challenges of validating automated driving vehicles (ADV) in safety-critical scenarios, and to propose a scenario generation framework called BridgeGen that can effectively generate diverse safety-critical scenarios for ADV development and performance evaluations.</li>
<li>methods: The paper uses both data-driven and knowledge-driven scenario generation methods, and introduces an ontology-based approach to model the five scenario layers in the operational design domain (ODD). The paper also develops an optimized scenario generation toolkit that combines traditional optimization and reinforcement learning schemes.</li>
<li>results: The paper conducts extensive experiments using the Carla simulator and demonstrates the effectiveness of BridgeGen in generating diverse safety-critical scenarios for ADV. The results show that BridgeGen can efficiently generate safety-critical scenarios that are not easily achievable by existing methods.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了解决自动驾驶汽车（ADV）在安全关键场景中的验证问题，并提出一种名为 BridgeGen 的场景生成框架，用于 ADV 的开发和性能评估。</li>
<li>methods: 论文使用了数据驱动和知识驱动的场景生成方法，并提出了基于 ontology 的方法来模型五个场景层次。论文还开发了一个优化场景生成工具包，其结合了传统优化和强化学习方案。</li>
<li>results: 论文通过使用 Carla  simulateur 进行了广泛的实验，并证明了 BridgeGen 可以有效地生成多样化的安全关键场景。结果表明，BridgeGen 可以高效地生成安全关键场景，而这些场景不易由现有的方法实现。<details>
<summary>Abstract</summary>
Automated driving vehicles~(ADV) promise to enhance driving efficiency and safety, yet they face intricate challenges in safety-critical scenarios. As a result, validating ADV within generated safety-critical scenarios is essential for both development and performance evaluations. This paper investigates the complexities of employing two major scenario-generation solutions: data-driven and knowledge-driven methods. Data-driven methods derive scenarios from recorded datasets, efficiently generating scenarios by altering the existing behavior or trajectories of traffic participants but often falling short in considering ADV perception; knowledge-driven methods provide effective coverage through expert-designed rules, but they may lead to inefficiency in generating safety-critical scenarios within that coverage. To overcome these challenges, we introduce BridgeGen, a safety-critical scenario generation framework, designed to bridge the benefits of both methodologies. Specifically, by utilizing ontology-based techniques, BridgeGen models the five scenario layers in the operational design domain (ODD) from knowledge-driven methods, ensuring broad coverage, and incorporating data-driven strategies to efficiently generate safety-critical scenarios. An optimized scenario generation toolkit is developed within BridgeGen. This expedites the crafting of safety-critical scenarios through a combination of traditional optimization and reinforcement learning schemes. Extensive experiments conducted using Carla simulator demonstrate the effectiveness of BridgeGen in generating diverse safety-critical scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Short-term-Volatility-Estimation-for-High-Frequency-Trades-using-Gaussian-processes-GPs"><a href="#Short-term-Volatility-Estimation-for-High-Frequency-Trades-using-Gaussian-processes-GPs" class="headerlink" title="Short-term Volatility Estimation for High Frequency Trades using Gaussian processes (GPs)"></a>Short-term Volatility Estimation for High Frequency Trades using Gaussian processes (GPs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10935">http://arxiv.org/abs/2311.10935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonard Mushunje, Maxwell Mashasha, Edina Chandiwana</li>
<li>For: The paper aims to improve short-term volatility and return forecasting for high-frequency trades by combining numeric and probabilistic models.* Methods: The paper uses a combination of Gaussian Processes (GPs) and a Numerical market prediction (NMP) model to make one-day-ahead volatility forecasts. The NMP model is used to correct the stock price data, and a Censored GP is used to model the relationship between the corrected stock prices and returns.* Results: The paper evaluates the forecasting errors using implied and estimated data.Here’s the simplified Chinese text for the three information points:* For: 这篇论文目的是为高频交易提高短期涨落风险和回报预测。* Methods: 论文使用GP和NMP模型组合来实现一天前的涨落风险预测。NMP模型用于correcting股票价格数据，而Censored GP用于模型corrected股票价格和回报之间的关系。* Results: 论文使用implied和estimated数据来评估预测误差。<details>
<summary>Abstract</summary>
The fundamental theorem behind financial markets is that stock prices are intrinsically complex and stochastic. One of the complexities is the volatility associated with stock prices. Volatility is a tendency for prices to change unexpectedly [1]. Price volatility is often detrimental to the return economics, and thus, investors should factor it in whenever making investment decisions, choices, and temporal or permanent moves. It is, therefore, crucial to make necessary and regular short and long-term stock price volatility forecasts for the safety and economics of investors returns. These forecasts should be accurate and not misleading. Different models and methods, such as ARCH GARCH models, have been intuitively implemented to make such forecasts. However, such traditional means fail to capture the short-term volatility forecasts effectively. This paper, therefore, investigates and implements a combination of numeric and probabilistic models for short-term volatility and return forecasting for high-frequency trades. The essence is that one-day-ahead volatility forecasts were made with Gaussian Processes (GPs) applied to the outputs of a Numerical market prediction (NMP) model. Firstly, the stock price data from NMP was corrected by a GP. Since it is not easy to set price limits in a market due to its free nature and randomness, a Censored GP was used to model the relationship between the corrected stock prices and returns. Forecasting errors were evaluated using the implied and estimated data.
</details>
<details>
<summary>摘要</summary>
金融市场的基本定理是股票价格本身具有内在的复杂性和随机性。其中一种复杂性是股票价格的波动性，波动性通常对于投资者的返报有负面影响，因此投资者应该在做投资决策时考虑波动性。为保证投资者的返报安全和经济，因此需要在REGULAR basis上进行短期和长期股票价格波动性预测。这些预测应该准确无误。传统方法和模型，如ARCH GARCH模型，已经被应用来进行这些预测，但它们在短期波动性预测上并不准确。这篇论文因此调查和实施了一种组合 numeric和 probabilistic 模型来进行短期波动性和回报预测。其中一种方法是使用 Gaussian Processes (GPs) 来对 numerics 市场预测模型（NMP）的输出进行预测。首先，股票价格数据从NMP中被修正了一个GP。由于市场的自由和随机性，使用 Censored GP 模型来模型修正后的股票价格和回报之间的关系。预测错误被评估使用实际和预测数据。
</details></li>
</ul>
<hr>
<h2 id="Near-Optimal-Fair-Resource-Allocation-for-Strategic-Agents-without-Money-A-Data-Driven-Approach"><a href="#Near-Optimal-Fair-Resource-Allocation-for-Strategic-Agents-without-Money-A-Data-Driven-Approach" class="headerlink" title="Near-Optimal Fair Resource Allocation for Strategic Agents without Money: A Data-Driven Approach"></a>Near-Optimal Fair Resource Allocation for Strategic Agents without Money: A Data-Driven Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10927">http://arxiv.org/abs/2311.10927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sihan Zeng, Sujay Bhatt, Eleonora Kreacic, Parisa Hassanzadeh, Alec Koppel, Sumitra Ganesh</li>
<li>for: 学习基于的公平分配机制设计，使用相对公平（PF）作为标准。</li>
<li>methods: 使用复杂的技术 inspirited by differentiable convex programming literature，计算PF机制的利用性。</li>
<li>results: 提出了一种能够快速计算PF机制的利用性的方法，并通过控制交易OFF来实现高度公平的分配。<details>
<summary>Abstract</summary>
We study learning-based design of fair allocation mechanisms for divisible resources, using proportional fairness (PF) as a benchmark. The learning setting is a significant departure from the classic mechanism design literature, in that, we need to learn fair mechanisms solely from data. In particular, we consider the challenging problem of learning one-shot allocation mechanisms -- without the use of money -- that incentivize strategic agents to be truthful when reporting their valuations. It is well-known that the mechanism that directly seeks to optimize PF is not incentive compatible, meaning that the agents can potentially misreport their preferences to gain increased allocations. We introduce the notion of "exploitability" of a mechanism to measure the relative gain in utility from misreport, and make the following important contributions in the paper: (i) Using sophisticated techniques inspired by differentiable convex programming literature, we design a numerically efficient approach for computing the exploitability of the PF mechanism. This novel contribution enables us to quantify the gap that needs to be bridged to approximate PF via incentive compatible mechanisms. (ii) Next, we modify the PF mechanism to introduce a trade-off between fairness and exploitability. By properly controlling this trade-off using data, we show that our proposed mechanism, ExPF-Net, provides a strong approximation to the PF mechanism while maintaining low exploitability. This mechanism, however, comes with a high computational cost. (iii) To address the computational challenges, we propose another mechanism ExS-Net, which is end-to-end parameterized by a neural network. ExS-Net enjoys similar (slightly inferior) performance and significantly accelerated training and inference time performance. (iv) Extensive numerical simulations demonstrate the robustness and efficacy of the proposed mechanisms.
</details>
<details>
<summary>摘要</summary>
我们研究基于学习的公平分配机制设计，使用比例公平（PF）作为 referent。学习设定是传统机制设计文献中的一个重要 departure，因为我们需要从数据中学习公平的机制。具体来说，我们考虑到具有挑战性的问题：学习一次分配机制——不使用金钱——导致战略性代表者 truthfully 报告他们的价值。已知PF直接寻求最佳化机制是不可吸引的，代表代表者可能会隐藏他们的 preference以获得更多的分配。我们引入了机制的“滥用”（exploitability）来衡量代表者可以从misreport中获得的优化。我们在文中做以下重要贡献：(i) 使用 differential convex programming 文献中的专门技术，我们设计了一个精确的方法来 Compute 机制的滥用。这个新的贡献使我们能够量化PF机制和吸引机制之间的差异。(ii) 我们将PF机制修改，以引入公平和滥用之间的变数。通过对数据进行控制，我们显示了我们的提案机制ExPF-Net可以将PF机制作为近似，同时保持低滥用。这个机制，然而，具有高计算成本。(iii) 为了解决计算问题，我们提出了另一个机制ExS-Net，这个机制是由神经网 Parametrize 的。ExS-Net 具有相似（微scopically inferior）的性能，并且具有明显提高的训练和测试时间性能。(iv) 我们的实验结果显示了我们的提案机制具有优良的Robustness和效用性。
</details></li>
</ul>
<hr>
<h2 id="PACOL-Poisoning-Attacks-Against-Continual-Learners"><a href="#PACOL-Poisoning-Attacks-Against-Continual-Learners" class="headerlink" title="PACOL: Poisoning Attacks Against Continual Learners"></a>PACOL: Poisoning Attacks Against Continual Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10919">http://arxiv.org/abs/2311.10919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huayu Li, Gregory Ditzler</li>
<li>for: 本研究旨在探讨 kontinual learning 系统在不可靠的来源中被恶意攻击的情况，并提出一种新的数据毒化攻击方法，称为 kontinual learning 攻击（PACOL）。</li>
<li>methods: 本研究使用了 Generative Replay 和 regularization-based kontinual learning 方法，并进行了广泛的实验来评估这些方法在攻击下的抵抗能力。</li>
<li>results: 研究发现，常用的 Generative Replay 和 regularization-based kontinual learning 方法都容易受到攻击，特别是 label-flipping 和 PACOL 等攻击方法可以让 kontinual learning 系统忘记已经学习的任务。<details>
<summary>Abstract</summary>
Continual learning algorithms are typically exposed to untrusted sources that contain training data inserted by adversaries and bad actors. An adversary can insert a small number of poisoned samples, such as mislabeled samples from previously learned tasks, or intentional adversarial perturbed samples, into the training datasets, which can drastically reduce the model's performance. In this work, we demonstrate that continual learning systems can be manipulated by malicious misinformation and present a new category of data poisoning attacks specific for continual learners, which we refer to as {\em Poisoning Attacks Against Continual Learners} (PACOL). The effectiveness of labeling flipping attacks inspires PACOL; however, PACOL produces attack samples that do not change the sample's label and produce an attack that causes catastrophic forgetting. A comprehensive set of experiments shows the vulnerability of commonly used generative replay and regularization-based continual learning approaches against attack methods. We evaluate the ability of label-flipping and a new adversarial poison attack, namely PACOL proposed in this work, to force the continual learning system to forget the knowledge of a learned task(s). More specifically, we compared the performance degradation of continual learning systems trained on benchmark data streams with and without poisoning attacks. Moreover, we discuss the stealthiness of the attacks in which we test the success rate of data sanitization defense and other outlier detection-based defenses for filtering out adversarial samples.
</details>
<details>
<summary>摘要</summary>
continuous learning algorithms 通常会被不良来源攻击，这些来源包括由 adversary 和坏 actor 插入的训练数据。一个 adversary 可以插入一小数量的毒害样本，如先前学习的任务中的杂乱标注样本或者 adversarial 扰动样本，这些样本可以导致模型的性能下降很快。在这项工作中，我们展示了 continual learning 系统可以被恶意诡射的，并提出了一种新的数据毒害攻击，称为 continual learning 中的毒害攻击（PACOL）。PACOL 的攻击样本不会改变样本的标签，但会导致模型忘记已经学习的知识。我们对常用的生成回馈和常规化基于 continual learning 的方法进行了完整的实验，并证明了这些方法对于攻击方法的抵触性。我们还比较了标签旋转攻击和我们在这项工作中提出的新的 adversarial 毒害攻击（PACOL）的性能下降情况，以及在不同的数据流中对 continual learning 系统的影响。此外，我们还讨论了这些攻击的隐蔽性，包括测试攻击成功率和其他基于异常检测的防御机制是否能够过滤恶意样本。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/18/cs.LG_2023_11_18/" data-id="clp88dbzu00v2ob88h6n1f31o" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/18/eess.SP_2023_11_18/" class="article-date">
  <time datetime="2023-11-18T08:00:00.000Z" itemprop="datePublished">2023-11-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/18/eess.SP_2023_11_18/">eess.SP - 2023-11-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="BeamSync-Over-The-Air-Synchronization-for-Distributed-Massive-MIMO-Systems"><a href="#BeamSync-Over-The-Air-Synchronization-for-Distributed-Massive-MIMO-Systems" class="headerlink" title="BeamSync: Over-The-Air Synchronization for Distributed Massive MIMO Systems"></a>BeamSync: Over-The-Air Synchronization for Distributed Massive MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11070">http://arxiv.org/abs/2311.11070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Unnikrishnan Kunnath Ganesan, Rimalapudi Sarvendranath, Erik G. Larsson</li>
<li>for: 这个论文的目的是探讨分布式大量多输入多出力（MIMO）系统中多个地理上分开的访问点（AP）同时通信用户，利用多antenna协同MIMO处理和分布式设置的各种优势。</li>
<li>methods: 这篇论文使用了一种新的投影同步协调方法，named BeamSync，以协调多个地理上分开的AP进行同步。这种同步方法不需要发送任何测量数据到中央处理器（CPU）通过前方HL。</li>
<li>results:  simulation结果表明，使用提议的BeamSync方法可以提高性能，当AP中天线数量 Doubles 时，性能提高3dB。此外，这种方法也与传统的束 formaiting技术相比较好。<details>
<summary>Abstract</summary>
In distributed massive multiple-input multiple-output (MIMO) systems, multiple geographically separated access points (APs) communicate simultaneously with a user, leveraging the benefits of multi-antenna coherent MIMO processing and macro-diversity gains from the distributed setups. However, time and frequency synchronization of the multiple APs is crucial to achieve good performance and enable joint precoding. In this paper, we analyze the synchronization requirement among multiple APs from a reciprocity perspective, taking into account the multiplicative impairments caused by mismatches in radio frequency (RF) hardware. We demonstrate that a phase calibration of reciprocity-calibrated APs is sufficient for the joint coherent transmission of data to the user. To achieve synchronization, we propose a novel over-the-air synchronization protocol, named BeamSync, to calibrate the geographically separated APs without sending any measurements to the central processing unit (CPU) through fronthaul. We show that sending the synchronization signal in the dominant direction of the channel between APs is optimal. Additionally, we derive the optimal phase and frequency offset estimators. Simulation results indicate that the proposed BeamSync method enhances performance by 3 dB when the number of antennas at the APs is doubled. Moreover, the method performs well compared to traditional beamforming techniques.
</details>
<details>
<summary>摘要</summary>
在分布式巨大多输入多输出（MIMO）系统中，多个地理上分开的访问点（AP）同时与用户通信，利用多antenna干扰MIMO处理和macro-多样性收益。然而，多个AP的时间和频率同步是需要达到良好性能和启用联合预编码的关键。在这篇论文中，我们从reciprocity角度分析了多个AP之间的同步需求，考虑了 radio频率硬件匹配不准的乘数性质。我们示出，只需要在reciprocity-calibrated APs中进行相位准化，即可实现联合整合数据传输到用户。为实现同步，我们提出了一种新的无需中央处理单元（CPU）通过前段传输的空中同步协议，名为BeamSync。我们发现，在AP之间通信道的主导方向上发送同步信号是优化的。此外，我们 derive了最佳相位和频率偏移估计器。实验结果表明，我们提出的BeamSync方法可以在APantenna数量两倍时提高性能，并且与传统的扫描方法相比，其性能较好。
</details></li>
</ul>
<hr>
<h2 id="Channel-Estimation-for-FAS-assisted-Multiuser-mmWave-Systems"><a href="#Channel-Estimation-for-FAS-assisted-Multiuser-mmWave-Systems" class="headerlink" title="Channel Estimation for FAS-assisted Multiuser mmWave Systems"></a>Channel Estimation for FAS-assisted Multiuser mmWave Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11041">http://arxiv.org/abs/2311.11041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Xu, Gui Zhou, Kai-Kit Wong, Wee Kiat New, Chao Wang, Chan-Byoung Chae, Ross Murch, Shi Jin, Yangyang Zhang</li>
<li>for: 这个论文targets the challenge of channel estimation in a multiuser millimeter-wave (mmWave) time-division duplexing (TDD) system.</li>
<li>methods: 该论文提议了一种low-sample-size sparse channel reconstruction (L3SCR)方法，利用mmWave通道的稀疏卷积特点来重建渠道状态信息 (CSI)。</li>
<li>results:  simulations results show that the proposed method can obtain precise CSI with minimal hardware switching and pilot overhead, leading to a system sum-rate that approaches the upper bound achievable with perfect CSI.<details>
<summary>Abstract</summary>
This letter investigates the challenge of channel estimation in a multiuser millimeter-wave (mmWave) time-division duplexing (TDD) system. In this system, the base station (BS) employs a multi-antenna uniform linear array (ULA), while each mobile user is equipped with a fluid antenna system (FAS). Accurate channel state information (CSI) plays a crucial role in the precise placement of antennas in FAS. Traditional channel estimation methods designed for fixed-antenna systems are inadequate due to the high dimensionality of FAS. To address this issue, we propose a low-sample-size sparse channel reconstruction (L3SCR) method, capitalizing on the sparse propagation paths characteristic of mmWave channels. In this approach, each fluid antenna only needs to switch and measure the channel at a few specific locations. By observing this reduced-dimensional data, we can effectively extract angular and gain information related to the sparse channel, enabling us to reconstruct the full CSI. Simulation results demonstrate that our proposed method allows us to obtain precise CSI with minimal hardware switching and pilot overhead. As a result, the system sum-rate approaches the upper bound achievable with perfect CSI.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/18/eess.SP_2023_11_18/" data-id="clp88dc8p01hxob883g80emyj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
