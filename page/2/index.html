
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/2/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_11_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/20/cs.CL_2023_11_20/" class="article-date">
  <time datetime="2023-11-20T11:00:00.000Z" itemprop="datePublished">2023-11-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/20/cs.CL_2023_11_20/">cs.CL - 2023-11-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unifying-Corroborative-and-Contributive-Attributions-in-Large-Language-Models"><a href="#Unifying-Corroborative-and-Contributive-Attributions-in-Large-Language-Models" class="headerlink" title="Unifying Corroborative and Contributive Attributions in Large Language Models"></a>Unifying Corroborative and Contributive Attributions in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12233">http://arxiv.org/abs/2311.12233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Theodora Worledge, Judy Hanwen Shen, Nicole Meister, Caleb Winston, Carlos Guestrin</li>
<li>for: 本研究旨在提供一个统一的大语言模型归因框架，以涵盖现有不同类型的归因方法，包括引用生成和训练数据归因。</li>
<li>methods: 本研究使用了现有的归因方法，并将其们 integrate 到一个统一的框架中。</li>
<li>results: 本研究提出了一个统一的大语言模型归因框架，可以涵盖现有不同类型的归因方法，并可以用于解释现实世界中的应用场景。<details>
<summary>Abstract</summary>
As businesses, products, and services spring up around large language models, the trustworthiness of these models hinges on the verifiability of their outputs. However, methods for explaining language model outputs largely fall across two distinct fields of study which both use the term "attribution" to refer to entirely separate techniques: citation generation and training data attribution. In many modern applications, such as legal document generation and medical question answering, both types of attributions are important. In this work, we argue for and present a unified framework of large language model attributions. We show how existing methods of different types of attribution fall under the unified framework. We also use the framework to discuss real-world use cases where one or both types of attributions are required. We believe that this unified framework will guide the use case driven development of systems that leverage both types of attribution, as well as the standardization of their evaluation.
</details>
<details>
<summary>摘要</summary>
As businesses, products, and services spring up around large language models, the trustworthiness of these models hinges on the verifiability of their outputs. However, methods for explaining language model outputs largely fall across two distinct fields of study, both of which use the term "attribution" to refer to entirely separate techniques: citation generation and training data attribution. In many modern applications, such as legal document generation and medical question answering, both types of attributions are important. In this work, we argue for and present a unified framework of large language model attributions. We show how existing methods of different types of attribution fall under the unified framework. We also use the framework to discuss real-world use cases where one or both types of attributions are required. We believe that this unified framework will guide the use case-driven development of systems that leverage both types of attribution, as well as the standardization of their evaluation.Here's the translation in Traditional Chinese:当商业、产品和服务逐渐发展around大型自然语言模型时，这些模型的可信度将直接受到其输出的可追溯性影响。然而，对于大型自然语言模型的输出解释方法主要分布在两个不同的领域中，它们都使用“参考”这个名称来描述完全不同的技术：引用生成和训练数据参考。在现代应用中，如法律文件生成和医疗问题回答，都需要这两种参考。在这个工作中，我们认为并提出了一个统一框架，以涵盖现有不同类型的参考方法。我们还使用这个框架来讨论实际应用中需要一或二种参考的问题。我们相信这个统一框架将导引use case驱动的开发系统，以及参考评估的标准化。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Closed-Access-Multilingual-Embedding-for-Automatic-Sentence-Alignment-in-Low-Resource-Languages"><a href="#Leveraging-Closed-Access-Multilingual-Embedding-for-Automatic-Sentence-Alignment-in-Low-Resource-Languages" class="headerlink" title="Leveraging Closed-Access Multilingual Embedding for Automatic Sentence Alignment in Low Resource Languages"></a>Leveraging Closed-Access Multilingual Embedding for Automatic Sentence Alignment in Low Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12179">http://arxiv.org/abs/2311.12179</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abumafrim/cohere-align">https://github.com/abumafrim/cohere-align</a></li>
<li>paper_authors: Idris Abdulmumin, Auwal Abubakar Khalid, Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Lukman Jibril Aliyu, Babangida Sani, Bala Mairiga Abduljalil, Sani Ahmad Hassan</li>
<li>for: 本研究旨在提高机器翻译中的质量，通过使用高质量的并行数据集来改进翻译模型的性能。</li>
<li>methods: 本研究使用了Cohere多语言嵌入的关闭访问，并开发了一个简单 yet 高效的并行句子对齐器。</li>
<li>results: 该方法在FLORES和MAFAND-MT数据集上达到了$94.96$和$54.83$的f1分数，与LASER相比具有了显著的改进（超过5个BLEU分数）。<details>
<summary>Abstract</summary>
The importance of qualitative parallel data in machine translation has long been determined but it has always been very difficult to obtain such in sufficient quantity for the majority of world languages, mainly because of the associated cost and also the lack of accessibility to these languages. Despite the potential for obtaining parallel datasets from online articles using automatic approaches, forensic investigations have found a lot of quality-related issues such as misalignment, and wrong language codes. In this work, we present a simple but qualitative parallel sentence aligner that carefully leveraged the closed-access Cohere multilingual embedding, a solution that ranked second in the just concluded #CoHereAIHack 2023 Challenge (see https://ai6lagos.devpost.com). The proposed approach achieved $94.96$ and $54.83$ f1 scores on FLORES and MAFAND-MT, compared to $3.64$ and $0.64$ of LASER respectively. Our method also achieved an improvement of more than 5 BLEU scores over LASER, when the resulting datasets were used with MAFAND-MT dataset to train translation models. Our code and data are available for research purposes here (https://github.com/abumafrim/Cohere-Align).
</details>
<details>
<summary>摘要</summary>
In this work, we present a simple yet effective parallel sentence aligner that leverages the closed-access Cohere multilingual embedding. This approach achieved F1 scores of $94.96$ and $54.83$ on FLORES and MAFAND-MT, respectively, outperforming LASER by more than 5 BLEU scores. Our method and code are available for research purposes at https://github.com/abumafrim/Cohere-Align.
</details></li>
</ul>
<hr>
<h2 id="Human-Learning-by-Model-Feedback-The-Dynamics-of-Iterative-Prompting-with-Midjourney"><a href="#Human-Learning-by-Model-Feedback-The-Dynamics-of-Iterative-Prompting-with-Midjourney" class="headerlink" title="Human Learning by Model Feedback: The Dynamics of Iterative Prompting with Midjourney"></a>Human Learning by Model Feedback: The Dynamics of Iterative Prompting with Midjourney</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12131">http://arxiv.org/abs/2311.12131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shachardon/mid-journey-to-alignment">https://github.com/shachardon/mid-journey-to-alignment</a></li>
<li>paper_authors: Shachar Don-Yehiya, Leshem Choshen, Omri Abend</li>
<li>for: 本研究探讨了在生成图像时，用户需要进行多次尝试，并通过反馈来更新提示，以实现更好的图像生成。</li>
<li>methods: 该研究采用了文本到图像模型，并对用户的提示进行分析，以了解用户在尝试过程中的行为。</li>
<li>results: 研究发现，用户的提示在尝试过程中会predictably converge到特定的特征，并且这种吸引力可能是由于用户意外地忽略了重要细节，或者是由于模型的偏好，生成更加适合特定语言风格的图像。<details>
<summary>Abstract</summary>
Generating images with a Text-to-Image model often requires multiple trials, where human users iteratively update their prompt based on feedback, namely the output image. Taking inspiration from cognitive work on reference games and dialogue alignment, this paper analyzes the dynamics of the user prompts along such iterations. We compile a dataset of iterative interactions of human users with Midjourney. Our analysis then reveals that prompts predictably converge toward specific traits along these iterations. We further study whether this convergence is due to human users, realizing they missed important details, or due to adaptation to the model's ``preferences'', producing better images for a specific language style. We show initial evidence that both possibilities are at play. The possibility that users adapt to the model's preference raises concerns about reusing user data for further training. The prompts may be biased towards the preferences of a specific model, rather than align with human intentions and natural manner of expression.
</details>
<details>
<summary>摘要</summary>
通常，通过文本到图像模型生成图像需要多次尝试，用户会在反馈基础上不断更新提示。以认知工作的参考游戏和对话Alignment为 inspirations，这篇论文分析了用户提示的动态。我们编译了人类用户与Midjourney的多轮交互的数据集。我们的分析显示，提示逐渐趋向特定特征。我们进一步研究了这种吸引力是由于用户注意到重要细节的不足，或者是由于模型的偏好而生成更好的图像。我们发现了这两种可能性。用户适应模型的偏好可能会导致 reuse user data for further training，但是这些提示可能会受到模型的偏好而不是人类的意图和自然表达方式。
</details></li>
</ul>
<hr>
<h2 id="LQ-LoRA-Low-rank-Plus-Quantized-Matrix-Decomposition-for-Efficient-Language-Model-Finetuning"><a href="#LQ-LoRA-Low-rank-Plus-Quantized-Matrix-Decomposition-for-Efficient-Language-Model-Finetuning" class="headerlink" title="LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning"></a>LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12023">http://arxiv.org/abs/2311.12023</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hanguo97/lq-lora">https://github.com/hanguo97/lq-lora</a></li>
<li>paper_authors: Han Guo, Philip Greengard, Eric P. Xing, Yoon Kim</li>
<li>for: 本研究旨在提出一种简单的方法，以提高预训练语言模型的内存效率。</li>
<li>methods: 该方法使用迭代算法将预训练矩阵分解成高精度低级组件和内存效率量化组件。在finetuning时，量化组件保持不变，只有低级组件进行更新。我们还提出了一种基于欧几里得方程的量化组件的整数线性编程方法，可以在给定的总内存预算下动态配置量化参数（例如，比特宽和块大小）。</li>
<li>results: 我们的研究表明，使用我们的low-rank plus quantized matrix decomposition方法（LQ-LoRA）可以超过强QLoRA和GPTQ-LoRA基elines，并且可以实现更加致命的量化。例如，在OpenAssistant标准 benchmark上，LQ-LoRA可以学习一个2.5比特LLaMA-2模型，与4比特QLoRA基eline相当。此外，当finetuning在语言模型调整数据集上时，LQ-LoRA也可以用于模型压缩，其中2.75比特LLaMA-2-70B模型（其中2.85比特是包括低级组件的平均值，需要27GB的GPU内存）与原始模型相当。<details>
<summary>Abstract</summary>
We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on adapting RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and moreover enables more aggressive quantization. For example, on the OpenAssistant benchmark LQ-LoRA is able to learn a 2.5-bit LLaMA-2 model that is competitive with a model finetuned with 4-bit QLoRA. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) is competitive with the original model in full precision.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单的方法来实现卷积语言模型的内存高效化。我们的方法使用迭代算法将预训练的矩阵 decomposes 成高精度低级成分和内存高效化的量化组件。在训练中，量化组件保持不变，只有低级成分被更新。我们提出了一种整数线性程序表示法，可以在给定总内存预算下动态配置量化参数（比如位宽、块大小）。此外，我们还探讨了基于数据的版本，使用估计的施密特信息矩阵来衡量重建目标时的权重。我们的LQ-LoRA方法在适应RoBERTa和LLaMA-2（7B和70B）上进行了实验，比强QLoRA和GPTQ-LoRA基eline高效，并且允许更加谨慎的量化。例如，在OpenAssistant benchmark上，LQ-LoRA可以学习一个2.5位的LLaMA-2模型，与4位QLoRA基eline相当。当训练在语言模型准确性调整数据集上时，LQ-LoRA还可以用于模型压缩；在这种情况下，我们的2.75位LLaMA-2-70B模型（具有2.85位的平均位数，需要27GB的GPU内存）与原始模型相当。
</details></li>
</ul>
<hr>
<h2 id="GPT-4V-ision-for-Robotics-Multimodal-Task-Planning-from-Human-Demonstration"><a href="#GPT-4V-ision-for-Robotics-Multimodal-Task-Planning-from-Human-Demonstration" class="headerlink" title="GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration"></a>GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12015">http://arxiv.org/abs/2311.12015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi</li>
<li>for: 该论文目的是提高一种通用视觉语言模型，以便更好地控制 робо。</li>
<li>methods: 该系统使用视频中人类行为的观察，创建可执行的 робо程序，并将环境和动作细节转化为文本。然后，使用 GPT-4 掌握任务规划，并使用视觉系统重新分析视频，以便更好地了解手持物体的时间和方式。</li>
<li>results: 实验结果表明，该方法可以在不同的场景下，由人类示例而不需要更多的训练，快速地将人类示例转化为机器人操作。<details>
<summary>Abstract</summary>
We introduce a pipeline that enhances a general-purpose Vision Language Model, GPT-4V(ision), by integrating observations of human actions to facilitate robotic manipulation. This system analyzes videos of humans performing tasks and creates executable robot programs that incorporate affordance insights. The computation starts by analyzing the videos with GPT-4V to convert environmental and action details into text, followed by a GPT-4-empowered task planner. In the following analyses, vision systems reanalyze the video with the task plan. Object names are grounded using an open-vocabulary object detector, while focus on the hand-object relation helps to detect the moment of grasping and releasing. This spatiotemporal grounding allows the vision systems to further gather affordance data (e.g., grasp type, way points, and body postures). Experiments across various scenarios demonstrate this method's efficacy in achieving real robots' operations from human demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are available at this project page: https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/
</details>
<details>
<summary>摘要</summary>
我们介绍一个管道，把通用视觉语言模型GPT-4V（视觉）与人类动作观察结合，以便为机器人操作提供帮助。这个系统分析视频中人们完成任务的方式，并生成可执行的机器人程序，并包括环境和动作详细信息。计算开始于使用GPT-4V分析视频，并将环境和动作详细信息转换为文本。接着，使用GPT-4 empowered task planner进行计划。然后，视觉系统重新分析视频，并使用开放词汇对象检测器将对象名称固定。强调手对象关系可以检测抓取和释放的时刻。这种空间时间固定 Allow the vision system to further gather affordance data (e.g., grasp type, way points, and body postures).实验在不同enario中展示了这种方法在实现人类示例动作下的机器人操作 Zero-shot manner 的能力。GPT-4V/GPT-4的提示可以在这个项目页面上找到：https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="H-COAL-Human-Correction-of-AI-Generated-Labels-for-Biomedical-Named-Entity-Recognition"><a href="#H-COAL-Human-Correction-of-AI-Generated-Labels-for-Biomedical-Named-Entity-Recognition" class="headerlink" title="H-COAL: Human Correction of AI-Generated Labels for Biomedical Named Entity Recognition"></a>H-COAL: Human Correction of AI-Generated Labels for Biomedical Named Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11981">http://arxiv.org/abs/2311.11981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaojing Duan, John P. Lalor</li>
<li>for: 这个论文是为了解决人工智能生成标签的准确性问题，提出了一种新的人工 corrections of AI-generated labels (H-COAL) 框架。</li>
<li>methods: 该框架使用了一种排名算法，可以选择性地更正 AI 生成的标签，以达到黄金标准性表现（100% 的人工标注），但需要远少于人工努力。</li>
<li>results: 研究发现，对标签的5% 的更正可以提高 AI 和人类表现的差距，相对提高64%；对标签的20% 的更正可以提高 AI 和人类表现的差距，相对提高86%。<details>
<summary>Abstract</summary>
With the rapid advancement of machine learning models for NLP tasks, collecting high-fidelity labels from AI models is a realistic possibility. Firms now make AI available to customers via predictions as a service (PaaS). This includes PaaS products for healthcare. It is unclear whether these labels can be used for training a local model without expensive annotation checking by in-house experts. In this work, we propose a new framework for Human Correction of AI-Generated Labels (H-COAL). By ranking AI-generated outputs, one can selectively correct labels and approach gold standard performance (100% human labeling) with significantly less human effort. We show that correcting 5% of labels can close the AI-human performance gap by up to 64% relative improvement, and correcting 20% of labels can close the performance gap by up to 86% relative improvement.
</details>
<details>
<summary>摘要</summary>
随着机器学习模型在自然语言处理任务中的快速发展，收集高品质标注从AI模型是一个现实性的可能性。现在，公司通过预测为服务（PaaS）提供AI给客户。这包括医疗领域的Paas产品。然而，是否可以使用这些标注来训练本地模型，而无需高昂的人工标注检查，是一个未知的问题。在这项工作中，我们提出了一个新的人工纠正AI生成标注的框架（H-COAL）。通过对AI生成输出进行排名，可以选择性地纠正标注，并接近金标准性表现（100%人工标注），但是具有显著更少的人工努力。我们显示，只纠正5%的标注可以减少AI与人性能差距的相对改善，达到64%的相对改善；只纠正20%的标注可以减少AI与人性能差距的相对改善，达到86%的相对改善。
</details></li>
</ul>
<hr>
<h2 id="On-the-Potential-and-Limitations-of-Few-Shot-In-Context-Learning-to-Generate-Metamorphic-Specifications-for-Tax-Preparation-Software"><a href="#On-the-Potential-and-Limitations-of-Few-Shot-In-Context-Learning-to-Generate-Metamorphic-Specifications-for-Tax-Preparation-Software" class="headerlink" title="On the Potential and Limitations of Few-Shot In-Context Learning to Generate Metamorphic Specifications for Tax Preparation Software"></a>On the Potential and Limitations of Few-Shot In-Context Learning to Generate Metamorphic Specifications for Tax Preparation Software</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11979">http://arxiv.org/abs/2311.11979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dananjay Srinivas, Rohan Das, Saeid Tizpaz-Niari, Ashutosh Trivedi, Maria Leonor Pacheco</li>
<li>for: 本研究旨在提高法定税软件的正确性，以避免税务纠纷和罚款。</li>
<li>methods: 本研究使用了变态测试，以测试和调试法定税软件。变态测试可以帮助找出软件中的错误和漏洞。</li>
<li>results: 本研究提出了一种基于自然语言和首领逻辑的方法，用于自动生成变态规则。这种方法可以帮助减少人工干预，提高测试效率和正确性。<details>
<summary>Abstract</summary>
Due to the ever-increasing complexity of income tax laws in the United States, the number of US taxpayers filing their taxes using tax preparation software (henceforth, tax software) continues to increase. According to the U.S. Internal Revenue Service (IRS), in FY22, nearly 50% of taxpayers filed their individual income taxes using tax software. Given the legal consequences of incorrectly filing taxes for the taxpayer, ensuring the correctness of tax software is of paramount importance. Metamorphic testing has emerged as a leading solution to test and debug legal-critical tax software due to the absence of correctness requirements and trustworthy datasets. The key idea behind metamorphic testing is to express the properties of a system in terms of the relationship between one input and its slightly metamorphosed twinned input. Extracting metamorphic properties from IRS tax publications is a tedious and time-consuming process. As a response, this paper formulates the task of generating metamorphic specifications as a translation task between properties extracted from tax documents - expressed in natural language - to a contrastive first-order logic form. We perform a systematic analysis on the potential and limitations of in-context learning with Large Language Models(LLMs) for this task, and outline a research agenda towards automating the generation of metamorphic specifications for tax preparation software.
</details>
<details>
<summary>摘要</summary>
The core idea behind metamorphic testing is to express the properties of a system in terms of the relationship between one input and its slightly modified twin input. However, extracting metamorphic properties from IRS tax publications is a time-consuming and laborious process. To address this challenge, this paper proposes the task of generating metamorphic specifications as a translation task between properties extracted from tax documents (expressed in natural language) and a contrastive first-order logic form.We conduct a systematic analysis of the potential and limitations of in-context learning with Large Language Models (LLMs) for this task and outline a research agenda towards automating the generation of metamorphic specifications for tax preparation software.
</details></li>
</ul>
<hr>
<h2 id="Context-aware-Neural-Machine-Translation-for-English-Japanese-Business-Scene-Dialogues"><a href="#Context-aware-Neural-Machine-Translation-for-English-Japanese-Business-Scene-Dialogues" class="headerlink" title="Context-aware Neural Machine Translation for English-Japanese Business Scene Dialogues"></a>Context-aware Neural Machine Translation for English-Japanese Business Scene Dialogues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11976">http://arxiv.org/abs/2311.11976</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/su0315/discourse_context_mt">https://github.com/su0315/discourse_context_mt</a></li>
<li>paper_authors: Sumire Honda, Patrick Fernandes, Chrysoula Zerva</li>
<li>for: 这个论文的目的是提高现有的神经机器翻译模型（NMT）的性能，以便更好地翻译英日商务对话。</li>
<li>methods: 这篇论文使用了预训练的mBART模型，并在多句对话数据上进行了微调。这 позволяет作者们实验不同的上下文大小和extra-sentential信息编码方法。</li>
<li>results: 作者们发现，模型可以利用上一句和extra-sentential context（通过CXMI指标提高），并且在增加上下文大小和包含场景和speaker信息时，翻译质量有所提高， measured by BLEU和COMET指标。<details>
<summary>Abstract</summary>
Despite the remarkable advancements in machine translation, the current sentence-level paradigm faces challenges when dealing with highly-contextual languages like Japanese. In this paper, we explore how context-awareness can improve the performance of the current Neural Machine Translation (NMT) models for English-Japanese business dialogues translation, and what kind of context provides meaningful information to improve translation. As business dialogue involves complex discourse phenomena but offers scarce training resources, we adapted a pretrained mBART model, finetuning on multi-sentence dialogue data, which allows us to experiment with different contexts. We investigate the impact of larger context sizes and propose novel context tokens encoding extra-sentential information, such as speaker turn and scene type. We make use of Conditional Cross-Mutual Information (CXMI) to explore how much of the context the model uses and generalise CXMI to study the impact of the extra-sentential context. Overall, we find that models leverage both preceding sentences and extra-sentential context (with CXMI increasing with context size) and we provide a more focused analysis on honorifics translation. Regarding translation quality, increased source-side context paired with scene and speaker information improves the model performance compared to previous work and our context-agnostic baselines, measured in BLEU and COMET metrics.
</details>
<details>
<summary>摘要</summary>
尽管机器翻译技术有了很大的进步，但当 dealing with 高度上下文语言如日语时，当前句子水平的模型遇到了挑战。在这篇论文中，我们explore了如何使用上下文意识来提高当前的神经机器翻译（NMT）模型在英日商务对话翻译中的表现，以及哪些类型的上下文提供了有用的信息以改善翻译。商务对话 involve 复杂的语言现象，但受训资源匮乏，我们采用了预训练的 mBART 模型，并在多句话对话数据上进行了微调。我们研究了不同上下文大小的影响，并提出了新的上下文符号编码方式，包括发言人Turn和场景类型。我们使用 Conditional Cross-Mutual Information（CXMI）来探索模型如何使用上下文，并推广 CXMI 来研究额外上下文的影响。总之，我们发现模型可以利用上一句和extra-sentential context（CXMI 随上下文大小增加），并提供了更加专注的 honorifics 翻译分析。在翻译质量方面，增加源语言的上下文和场景信息可以提高模型的表现，比之前的工作和我们的上下文无关基线， measured 在 BLEU 和 COMET  метриках中。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Training-Distributions-with-Scalable-Online-Bilevel-Optimization"><a href="#Adaptive-Training-Distributions-with-Scalable-Online-Bilevel-Optimization" class="headerlink" title="Adaptive Training Distributions with Scalable Online Bilevel Optimization"></a>Adaptive Training Distributions with Scalable Online Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11973">http://arxiv.org/abs/2311.11973</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Grangier, Pierre Ablin, Awni Hannun</li>
<li>for: 这篇论文是关于现代机器学习中大脑网络的预训练和应用领域之间的匹配问题。</li>
<li>methods: 该论文提出了一种基于在线双层优化问题的算法，以适应小样本数据的情况。该算法优先计算在训练点上的梯度，以提高目标分布上的损失。</li>
<li>results: 该论文通过实验表明，在某些情况下，该方法可以超过现有领域适应技术，但在其他情况下可能不成功。该论文还提出了一种简单的测试方法，以评估该方法在不同情况下的效果。<details>
<summary>Abstract</summary>
Large neural networks pretrained on web-scale corpora are central to modern machine learning. In this paradigm, the distribution of the large, heterogeneous pretraining data rarely matches that of the application domain. This work considers modifying the pretraining distribution in the case where one has a small sample of data reflecting the targeted test conditions. We propose an algorithm motivated by a recent formulation of this setting as an online, bilevel optimization problem. With scalability in mind, our algorithm prioritizes computing gradients at training points which are likely to most improve the loss on the targeted distribution. Empirically, we show that in some cases this approach is beneficial over existing strategies from the domain adaptation literature but may not succeed in other cases. We propose a simple test to evaluate when our approach can be expected to work well and point towards further research to address current limitations.
</details>
<details>
<summary>摘要</summary>
大型神经网络在现代机器学习中处于中心位置，这些神经网络通常在庞大的网络数据上进行预训练。在这种情况下，预训练数据的分布与应用领域的分布rarely匹配。本文考虑在有限个数据点反映Targeted测试条件时修改预训练分布。我们提出一种基于最近的online, bilateral优化问题的算法。以可扩展性为目标，我们的算法在训练点上计算梯度，以提高Targeted分布上的损失。实际证明了，在某些情况下，我们的方法可以超越现有的领域适应Literature中的策略，但在其他情况下可能无法成功。我们提出一种简单的测试方法来评估我们的方法在哪些情况下能够效果，并指出了进一步研究的方向。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Analysis-of-Substantiation-in-Scientific-Peer-Reviews"><a href="#Automatic-Analysis-of-Substantiation-in-Scientific-Peer-Reviews" class="headerlink" title="Automatic Analysis of Substantiation in Scientific Peer Reviews"></a>Automatic Analysis of Substantiation in Scientific Peer Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11967">http://arxiv.org/abs/2311.11967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanzhu Guo, Guokan Shang, Virgile Rennard, Michalis Vazirgiannis, Chloé Clavel</li>
<li>for: 提高 AI 会议中异常评审的自动化质量控制方法。</li>
<li>methods: 使用科学 peer review 中的声明-证据对 extracted 问题，并使用 argued mining 系统自动分析评审的支持程度。</li>
<li>results: 使用 SubstanReview 数据集进行数据分析，获得 NLP 会议评审质量的有用洞察。<details>
<summary>Abstract</summary>
With the increasing amount of problematic peer reviews in top AI conferences, the community is urgently in need of automatic quality control measures. In this paper, we restrict our attention to substantiation -- one popular quality aspect indicating whether the claims in a review are sufficiently supported by evidence -- and provide a solution automatizing this evaluation process. To achieve this goal, we first formulate the problem as claim-evidence pair extraction in scientific peer reviews, and collect SubstanReview, the first annotated dataset for this task. SubstanReview consists of 550 reviews from NLP conferences annotated by domain experts. On the basis of this dataset, we train an argument mining system to automatically analyze the level of substantiation in peer reviews. We also perform data analysis on the SubstanReview dataset to obtain meaningful insights on peer reviewing quality in NLP conferences over recent years.
</details>
<details>
<summary>摘要</summary>
随着顶尖AI会议中的问题评审量度的增加，学术社区紧迫需要自动质量控制机制。在这篇论文中，我们仅考虑证据——一种受欢迎的质量特征，表示评论中的laim是否得到了足够的证据支持。我们提供一种自动评估这个问题的解决方案。为 достичь这个目标，我们首先将问题定义为科学 peer review 中的laim-evidence对 Extraction问题，并收集了 SubstanReview，这是第一个对这个任务进行注释的数据集。SubstanReview包含550篇来自NLP会议的评论，由领域专家注释。基于这个数据集，我们训练了一个 Argument Mining 系统，以自动分析 peer review 中的证据水平。我们还对 SubstanReview 数据集进行了数据分析，从而获得了对 NLP会议最近几年 peer review 质量的有用发现。
</details></li>
</ul>
<hr>
<h2 id="LLMs-as-Visual-Explainers-Advancing-Image-Classification-with-Evolving-Visual-Descriptions"><a href="#LLMs-as-Visual-Explainers-Advancing-Image-Classification-with-Evolving-Visual-Descriptions" class="headerlink" title="LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions"></a>LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11904">http://arxiv.org/abs/2311.11904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songhao Han, Le Zhuo, Yue Liao, Si Liu</li>
<li>for: 提高图像分类的精度和可解释性，提出了一种新的图像分类框架，即Iterative Optimization with Visual Feedback（短回归遍游 optimization with visual feedback）。</li>
<li>methods: 该方法首先使用大语言模型（LLM）生成图像分类器，然后使用一种演化优化策略来优化类别描述符。这个过程中，我们将视觉反馈从VLM分类指标中引入，以帮助优化过程具体化。</li>
<li>results: 我们在多种图像分类 benchmark上进行了实验，并 obtianed 3.47%的平均提升率，比存在的方法高。此外，我们还发现，通过使用这些描述符，可以在不同的底层模型上实现更好的性能。<details>
<summary>Abstract</summary>
Vision-language models (VLMs) offer a promising paradigm for image classification by comparing the similarity between images and class embeddings. A critical challenge lies in crafting precise textual representations for class names. While previous studies have leveraged recent advancements in large language models (LLMs) to enhance these descriptors, their outputs often suffer from ambiguity and inaccuracy. We identify two primary causes: 1) The prevalent reliance on textual interactions with LLMs, leading to a mismatch between the generated text and the visual content in VLMs' latent space - a phenomenon we term the "explain without seeing" dilemma. 2) The oversight of the inter-class relationships, resulting in descriptors that fail to differentiate similar classes effectively. To address these issues, we propose a novel image classification framework combining VLMs with LLMs, named Iterative Optimization with Visual Feedback. In particular, our method develops an LLM-based agent, employing an evolutionary optimization strategy to refine class descriptors. Crucially, we incorporate visual feedback from VLM classification metrics, thereby guiding the optimization process with concrete visual data. Our method leads to improving accuracy on a wide range of image classification benchmarks, with 3.47\% average gains over state-of-the-art methods. We also highlight the resulting descriptions serve as explainable and robust features that can consistently improve the performance across various backbone models.
</details>
<details>
<summary>摘要</summary>
vision-language模型（VLM）提供了一个有前途的思路，通过比较图像和分类embedding之间的相似性来进行图像分类。然而，一个挑战是制定精确的文本表述来描述分类名称。在先前的研究中，人们利用大型语言模型（LLM）来增强这些描述器，但其输出经常受到不确定性和不准确性的影响。我们认为这有两个主要原因：1）文本与LLM的交互过多，导致VLM的 latent space中的文本与图像之间存在匹配问题，我们称之为“解释无法看到”的困难。2）缺乏分类关系的考虑，导致描述器无法分类类型效果地区分类。为了解决这些问题，我们提出了一种 combining VLM和LLM的图像分类框架，名为Iterative Optimization with Visual Feedback。具体来说，我们的方法通过利用进化优化策略来练化分类描述器。关键是，我们在VLM的分类指标上 incorporate visual feedback，以导航优化过程中的具体视觉数据。我们的方法在各种图像分类benchmark上显示3.47%的平均提升，并且显示出的描述器为可解释和稳定的特征，可以在不同的背景模型上进行改进表现。
</details></li>
</ul>
<hr>
<h2 id="Evil-Geniuses-Delving-into-the-Safety-of-LLM-based-Agents"><a href="#Evil-Geniuses-Delving-into-the-Safety-of-LLM-based-Agents" class="headerlink" title="Evil Geniuses: Delving into the Safety of LLM-based Agents"></a>Evil Geniuses: Delving into the Safety of LLM-based Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11855">http://arxiv.org/abs/2311.11855</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/t1ans1r/evil-geniuses">https://github.com/t1ans1r/evil-geniuses</a></li>
<li>paper_authors: Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, Hang Su</li>
<li>for: 这篇论文探讨了LLM-基于代理的安全问题，以寻求答案。</li>
<li>methods: 该论文采用了手动囚犯提示和虚拟聊天对LLM-基于代理进行了系列的安全检测和评估。</li>
<li>results: 研究发现：1）LLM-基于代理agent具有减少的抗攻击能力。2）被攻击后的agent可以提供更加细腻的回应。3）检测生成的不当回应的困难度更高。这些现象提醒我们LLM-基于代理agent的安全性存在问题，并且在不同的角色专业水平和系统&#x2F;代理层面都存在漏洞。<details>
<summary>Abstract</summary>
The rapid advancements in large language models (LLMs) have led to a resurgence in LLM-based agents, which demonstrate impressive human-like behaviors and cooperative capabilities in various interactions and strategy formulations. However, evaluating the safety of LLM-based agents remains a complex challenge. This paper elaborately conducts a series of manual jailbreak prompts along with a virtual chat-powered evil plan development team, dubbed Evil Geniuses, to thoroughly probe the safety aspects of these agents. Our investigation reveals three notable phenomena: 1) LLM-based agents exhibit reduced robustness against malicious attacks. 2) the attacked agents could provide more nuanced responses. 3) the detection of the produced improper responses is more challenging. These insights prompt us to question the effectiveness of LLM-based attacks on agents, highlighting vulnerabilities at various levels and within different role specializations within the system/agent of LLM-based agents. Extensive evaluation and discussion reveal that LLM-based agents face significant challenges in safety and yield insights for future research. Our code is available at https://github.com/T1aNS1R/Evil-Geniuses.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的快速进步导致了LLM基于代理的复活，这些代理展现出人类化行为和合作能力在不同的互动和策略设计中。然而，评估LLM基于代理的安全性仍然是一个复杂的挑战。这篇论文通过手动囚室询问和虚拟聊天带动的邪恶天才团队（Evil Geniuses）进行了系列的探索，以全面探讨LLM基于代理的安全性问题。我们的调查发现了三个吸引注意的现象：1）LLM基于代理的代理具有较弱的抗攻击性。2）遭受攻击的代理可以提供更细腻的回应。3）检测生产的不当回应的可能性更高。这些发现促使我们质疑LLM基于代理的攻击是否有效，并高亮了系统/代理中LLM基于代理的代理存在的漏洞和不同角色尝试的攻击性。我们的评估和讨论表明，LLM基于代理的安全性面临着重大挑战，并提供了未来研究的发展方向。我们的代码可以在https://github.com/T1aNS1R/Evil-Geniuses上获取。
</details></li>
</ul>
<hr>
<h2 id="Deepparse-An-Extendable-and-Fine-Tunable-State-Of-The-Art-Library-for-Parsing-Multinational-Street-Addresses"><a href="#Deepparse-An-Extendable-and-Fine-Tunable-State-Of-The-Art-Library-for-Parsing-Multinational-Street-Addresses" class="headerlink" title="Deepparse : An Extendable, and Fine-Tunable State-Of-The-Art Library for Parsing Multinational Street Addresses"></a>Deepparse : An Extendable, and Fine-Tunable State-Of-The-Art Library for Parsing Multinational Street Addresses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11846">http://arxiv.org/abs/2311.11846</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Beauchemin, Marouane Yassine</li>
<li>for: 本文旨在提供一个开源、可编辑、可精度调整的地址分解解决方案，用于解决多国地址的分解问题。</li>
<li>methods: 本文使用了深度学习算法来实现地址分解，并在60多个国家的数据上进行了训练。</li>
<li>results: 据说，本文的预训练模型在训练国家上达到了99%的分解精度，而且不需要预处理或后处理。此外，库还支持自定义地址分解器的生成。<details>
<summary>Abstract</summary>
Segmenting an address into meaningful components, also known as address parsing, is an essential step in many applications from record linkage to geocoding and package delivery. Consequently, a lot of work has been dedicated to develop accurate address parsing techniques, with machine learning and neural network methods leading the state-of-the-art scoreboard. However, most of the work on address parsing has been confined to academic endeavours with little availability of free and easy-to-use open-source solutions.   This paper presents Deepparse, a Python open-source, extendable, fine-tunable address parsing solution under LGPL-3.0 licence to parse multinational addresses using state-of-the-art deep learning algorithms and evaluated on over 60 countries. It can parse addresses written in any language and use any address standard. The pre-trained model achieves average $99~\%$ parsing accuracies on the countries used for training with no pre-processing nor post-processing needed. Moreover, the library supports fine-tuning with new data to generate a custom address parser.
</details>
<details>
<summary>摘要</summary>
分析地址的各个 Component 是许多应用程序中的重要步骤，从record linkage到地理编码和快递配送。因此，很多工作都投入到了发展高精度地址分析技术中，其中机器学习和神经网络方法现在领先于所有其他方法。然而，大多数地址分析工作都受到了学术研究的限制，而且有限的免费和易用的开源解决方案。本文介绍了Deepparse，一个基于Python的开源、可扩展、可调整地址分析解决方案，采用LGPL-3.0许可证。它可以分析来自60多个国家的多国语言地址，使用当前最先进的深度学习算法。无需预处理或后处理，模型可以达到99%的平均分析精度。此外，库支持自定义地址分析器的自定义。
</details></li>
</ul>
<hr>
<h2 id="How-to-Use-Large-Language-Models-for-Text-Coding-The-Case-of-Fatherhood-Roles-in-Public-Policy-Documents"><a href="#How-to-Use-Large-Language-Models-for-Text-Coding-The-Case-of-Fatherhood-Roles-in-Public-Policy-Documents" class="headerlink" title="How to Use Large Language Models for Text Coding: The Case of Fatherhood Roles in Public Policy Documents"></a>How to Use Large Language Models for Text Coding: The Case of Fatherhood Roles in Public Policy Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11844">http://arxiv.org/abs/2311.11844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Lupo, Oscar Magnusson, Dirk Hovy, Elin Naurin, Lena Wängnerud</li>
<li>for: 这项研究旨在评估大语言模型（LLM）在政治科学文本分析方面的应用，以及如何使用LLM进行文本编码。</li>
<li>methods: 本研究使用了三种原创的非英文政治科学文本编码任务，并提供了一个通用的LLM使用工作流程。</li>
<li>results: 研究发现，当提供了详细的标签定义和编码示例时，一个LLM可以与人类标注员相当或甚至更好，具有更快的速度（达百度倍的速度）、更低的成本（至多60%比人类编码便宜）和更易扩展到大量文本。总之，LLMs 是大多数文本编码项目的可靠选择。<details>
<summary>Abstract</summary>
Recent advances in large language models (LLMs) like GPT-3 and GPT-4 have opened up new opportunities for text analysis in political science. They promise automation with better results and less programming. In this study, we evaluate LLMs on three original coding tasks of non-English political science texts, and we provide a detailed description of a general workflow for using LLMs for text coding in political science research. Our use case offers a practical guide for researchers looking to incorporate LLMs into their research on text analysis. We find that, when provided with detailed label definitions and coding examples, an LLM can be as good as or even better than a human annotator while being much faster (up to hundreds of times), considerably cheaper (costing up to 60% less than human coding), and much easier to scale to large amounts of text. Overall, LLMs present a viable option for most text coding projects.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)最近的大语言模型（LLM）如GPT-3和GPT-4的发展，为政治科学中的文本分析带来了新的机遇。它们提供了更好的自动化结果和更少的编程。在这项研究中，我们对非英文政治科学文本进行了三项原创编码任务的评估，并提供了用于在政治科学研究中使用LLM进行文本编码的通用工作流程的详细描述。我们的使用 случа子为研究人员寻求在研究中使用LLM的实践指南。我们发现，当给LLM提供详细的标签定义和编码示例时，LLM可以和人工标注员相当或者更好，而且比人工标注更快（可以达到百万倍），更加便宜（可以达到60%的成本减少），并且更易扩展到大量文本。总之，LLMs 是大多数文本编码项目的可靠选择。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Grammatical-Error-Correction-Via-Multi-Task-Training-and-Optimized-Training-Schedule"><a href="#Efficient-Grammatical-Error-Correction-Via-Multi-Task-Training-and-Optimized-Training-Schedule" class="headerlink" title="Efficient Grammatical Error Correction Via Multi-Task Training and Optimized Training Schedule"></a>Efficient Grammatical Error Correction Via Multi-Task Training and Optimized Training Schedule</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11813">http://arxiv.org/abs/2311.11813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrey Bout, Alexander Podolskiy, Sergey Nikolenko, Irina Piontkovskaya</li>
<li>for: 提高神经语法错误修正（GEC）的进步受限于缺乏高质量的手动标注数据。</li>
<li>methods: 我们提出了两个方法来更好地使用可用数据：一是采用预先训练的auxiliary任务，二是调整训练数据的顺序和实例顺序。</li>
<li>results: 我们的方法可以达到 significan improvements，比如使用小型模型（400M参数）超越最佳T5-XXL（11B参数）模型。<details>
<summary>Abstract</summary>
Progress in neural grammatical error correction (GEC) is hindered by the lack of annotated training data. Sufficient amounts of high-quality manually annotated data are not available, so recent research has relied on generating synthetic data, pretraining on it, and then fine-tuning on real datasets; performance gains have been achieved either by ensembling or by using huge pretrained models such as XXL-T5 as the backbone. In this work, we explore an orthogonal direction: how to use available data more efficiently. First, we propose auxiliary tasks that exploit the alignment between the original and corrected sentences, such as predicting a sequence of corrections. We formulate each task as a sequence-to-sequence problem and perform multi-task training. Second, we discover that the order of datasets used for training and even individual instances within a dataset may have important effects on the final performance, so we set out to find the best training schedule. Together, these two ideas lead to significant improvements, producing results that improve state of the art with much smaller models; in particular, we outperform the best models based on T5-XXL (11B parameters) with a BART-based model (400M parameters).
</details>
<details>
<summary>摘要</summary>
进步在神经 grammatical error correction（GEC）方面受到欠缺标注训练数据的阻碍。实际上， sufficient amounts of high-quality manually annotated data 不可得，所以latest research 仅可靠生成 sintethic data，先行预训练，然后精确地训练 real datasets; performance 增长仅可能通过 ensemble 或使用巨大的 pretrained models 如 XXL-T5 作为 backbone。在这个工作中，我们探索了一个 orthogonal 方向：如何更有效地使用可用的数据。首先，我们提出了auxiliary tasks，利用原始和修复句子之间的对齐，例如预测修复序列。我们将每个任务推定为一个 sequence-to-sequence 问题，并进行多任务训练。其次，我们发现了训练和获得数据的顺序和个别实例在数据集中的顺序可能具有重要的影响，因此我们展开了寻找最佳训练计划。总之，这两个想法共同带来了重要的改进，生成了比前一代模型更好的结果，特别是我们超越了基于 T5-XXL（11B 个参数）的最佳模型，使用 BART 型基本模型（400M 个参数）。
</details></li>
</ul>
<hr>
<h2 id="Encoding-Speaker-Specific-Latent-Speech-Feature-for-Speech-Synthesis"><a href="#Encoding-Speaker-Specific-Latent-Speech-Feature-for-Speech-Synthesis" class="headerlink" title="Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis"></a>Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11745">http://arxiv.org/abs/2311.11745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jungil Kong, Junmo Lee, Jeongmin Kim, Beomjeong Kim, Jihoon Park, Dohee Kong, Changheon Lee, Sangjin Kim</li>
<li>for: 模型多个说话者的特征表达，包括各种说话者的特征特征，如嗓音、语速、语调等。</li>
<li>methods: 提出一种新的方法，通过精细化特征和conditioning方法来表达目标说话者的speech特征，从而实现不需要额外训练目标说话者的数据集。</li>
<li>results: 对比seen说话者的best-performing多说话者模型，提出的方法在主观相似度评价中获得了显著更高的相似性mean opinion score（SMOS），并且在零戳法中也表现出了显著的优势。此外，方法还可以生成新的人工说话者，并且表明编码的秘密特征具有足够的信息可以重建原始说话者的speech。<details>
<summary>Abstract</summary>
In this work, we propose a novel method for modeling numerous speakers, which enables expressing the overall characteristics of speakers in detail like a trained multi-speaker model without additional training on the target speaker's dataset. Although various works with similar purposes have been actively studied, their performance has not yet reached that of trained multi-speaker models due to their fundamental limitations. To overcome previous limitations, we propose effective methods for feature learning and representing target speakers' speech characteristics by discretizing the features and conditioning them to a speech synthesis model. Our method obtained a significantly higher similarity mean opinion score (SMOS) in subjective similarity evaluation than seen speakers of a best-performing multi-speaker model, even with unseen speakers. The proposed method also outperforms a zero-shot method by significant margins. Furthermore, our method shows remarkable performance in generating new artificial speakers. In addition, we demonstrate that the encoded latent features are sufficiently informative to reconstruct an original speaker's speech completely. It implies that our method can be used as a general methodology to encode and reconstruct speakers' characteristics in various tasks.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种新的方法，用于模拟多个说话者，该方法可以详细表达说话者的总体特征，就如同一个训练过的多说话者模型，无需额外训练目标说话者的数据集。虽然过去有很多类似的研究，但它们的性能还未达到多说话者模型的水平，这是因为它们的基本限制。为了超越这些限制，我们提出了有效的特征学习方法和表达目标说话者的speech特征，通过离散特征和conditioning它们到一个speech生成模型。我们的方法在主观相似性评价中获得了比较高的相似性mean opinion score（SMOS）， même avec des speakers不 connus。此外，我们的方法还在生成新的人工说话者方面表现出色，并且表明了编码的秘密特征足够具有重建原始说话者的speech的能力。这意味着我们的方法可以用于通用的编码和重建说话者特征的方法ологи。
</details></li>
</ul>
<hr>
<h2 id="Addressing-the-Length-Bias-Problem-in-Document-Level-Neural-Machine-Translation"><a href="#Addressing-the-Length-Bias-Problem-in-Document-Level-Neural-Machine-Translation" class="headerlink" title="Addressing the Length Bias Problem in Document-Level Neural Machine Translation"></a>Addressing the Length Bias Problem in Document-Level Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11601">http://arxiv.org/abs/2311.11601</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/salvation-z/D2DToolkits">https://github.com/salvation-z/D2DToolkits</a></li>
<li>paper_authors: Zhuocheng Zhang, Shuhao Gu, Min Zhang, Yang Feng</li>
<li>for: 解决文本翻译 lengths bias问题，提高文本翻译质量</li>
<li>methods: 提出了改进DNMT模型的训练方法、注意机制和解码策略</li>
<li>results: 实验结果显示，我们的方法可以在多个公开数据集上带来显著改进，并且分析结果表明，我们的方法可以有效缓解 lengths bias问题。Here’s the full Chinese text in simplified Chinese characters:</li>
<li>for: 这个研究是为了解决文本翻译 lengths bias问题，提高文本翻译质量。</li>
<li>methods: 我们提出了改进DNMT模型的训练方法、注意机制和解码策略。</li>
<li>results: 实验结果显示，我们的方法可以在多个公开数据集上带来显著改进，并且分析结果表明，我们的方法可以有效缓解 lengths bias问题。<details>
<summary>Abstract</summary>
Document-level neural machine translation (DNMT) has shown promising results by incorporating more context information. However, this approach also introduces a length bias problem, whereby DNMT suffers from significant translation quality degradation when decoding documents that are much shorter or longer than the maximum sequence length during training. %i.e., the length bias problem. To solve the length bias problem, we propose to improve the DNMT model in training method, attention mechanism, and decoding strategy. Firstly, we propose to sample the training data dynamically to ensure a more uniform distribution across different sequence lengths. Then, we introduce a length-normalized attention mechanism to aid the model in focusing on target information, mitigating the issue of attention divergence when processing longer sequences. Lastly, we propose a sliding window strategy during decoding that integrates as much context information as possible without exceeding the maximum sequence length. The experimental results indicate that our method can bring significant improvements on several open datasets, and further analysis shows that our method can significantly alleviate the length bias problem.
</details>
<details>
<summary>摘要</summary>
文档级神经机器翻译（DNMT）已经显示了有前途的结果，通过包含更多上下文信息。然而，这种方法也会导致长度偏见问题，DNMT在训练时documenmt decoding documents that are much shorter or longer than the maximum sequence length during training.  то есть，长度偏见问题。为解决长度偏见问题，我们提议通过训练方法、注意机制和decoding策略进行改进。首先，我们提议在训练数据中采样 dynamically  ensure a more uniform distribution across different sequence lengths。然后，我们引入length-normalized attention mechanism，以帮助模型关注目标信息，避免长序信息处理时的注意力散失。最后，我们提议在decoding中使用滑块策略，integrate as much context information as possible without exceeding the maximum sequence length。实验结果表明，我们的方法可以在多个公开数据集上提供显著改进，并且分析表明，我们的方法可以有效缓解长度偏见问题。
</details></li>
</ul>
<hr>
<h2 id="Filling-the-Image-Information-Gap-for-VQA-Prompting-Large-Language-Models-to-Proactively-Ask-Questions"><a href="#Filling-the-Image-Information-Gap-for-VQA-Prompting-Large-Language-Models-to-Proactively-Ask-Questions" class="headerlink" title="Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions"></a>Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11598">http://arxiv.org/abs/2311.11598</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thunlp-mt/fiig">https://github.com/thunlp-mt/fiig</a></li>
<li>paper_authors: Ziyue Wang, Chi Chen, Peng Li, Yang Liu</li>
<li>for: 这个论文主要是为了提高大语言模型（LLM）在视觉问答 зада务中的表现，以及使LLM能够更好地利用图像信息。</li>
<li>methods: 这篇论文使用了一种框架，允许LLM提问更多细节信息，以及一些筛选器来约束生成的信息。</li>
<li>results: 论文的实验结果表明，使用这种框架和筛选器可以持续提高OK-VQA和A-OKVQA的基eline方法表现，具体来说，平均提高2.15%的性能。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) demonstrate impressive reasoning ability and the maintenance of world knowledge not only in natural language tasks, but also in some vision-language tasks such as open-domain knowledge-based visual question answering (OK-VQA). As images are invisible to LLMs, researchers convert images to text to engage LLMs into the visual question reasoning procedure. This leads to discrepancies between images and their textual representations presented to LLMs, which consequently impedes final reasoning performance. To fill the information gap and better leverage the reasoning capability, we design a framework that enables LLMs to proactively ask relevant questions to unveil more details in the image, along with filters for refining the generated information. We validate our idea on OK-VQA and A-OKVQA. Our method continuously boosts the performance of baselines methods by an average gain of 2.15% on OK-VQA, and achieves consistent improvements across different LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="How-well-ChatGPT-understand-Malaysian-English-An-Evaluation-on-Named-Entity-Recognition-and-Relation-Extraction"><a href="#How-well-ChatGPT-understand-Malaysian-English-An-Evaluation-on-Named-Entity-Recognition-and-Relation-Extraction" class="headerlink" title="How well ChatGPT understand Malaysian English? An Evaluation on Named Entity Recognition and Relation Extraction"></a>How well ChatGPT understand Malaysian English? An Evaluation on Named Entity Recognition and Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11583">http://arxiv.org/abs/2311.11583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mohanraj-nlp/chatgpt-malaysian-english">https://github.com/mohanraj-nlp/chatgpt-malaysian-english</a></li>
<li>paper_authors: Mohan Raj Chanthran, Lay-Ki Soon, Huey Fang Ong, Bhawani Selvaretnam</li>
<li>for: 本研究用于评估 chatGPT 在马来西亚英语新闻文本（MEN）上的实体抽取和关系抽取能力。</li>
<li>methods: 本研究采用了三步方法，即教育-预测-评估（educate-predict-evaluate）。</li>
<li>results: chatGPT 在马来西亚英语新闻文本中的实体抽取性能不高，最高 F1 分为 0.497。进一步分析发现，马来西亚英语中的 morphosyntactic 变化减少了 chatGPT 的性能。然而，这种 morphosyntactic 变化并不影响 chatGPT 的关系抽取性能。<details>
<summary>Abstract</summary>
Recently, ChatGPT has attracted a lot of interest from both researchers and the general public. While the performance of ChatGPT in named entity recognition and relation extraction from Standard English texts is satisfactory, it remains to be seen if it can perform similarly for Malaysian English. Malaysian English is unique as it exhibits morphosyntactic and semantical adaptation from local contexts. In this study, we assess ChatGPT's capability in extracting entities and relations from the Malaysian English News (MEN) dataset. We propose a three-step methodology referred to as \textbf{\textit{educate-predict-evaluate}. The performance of ChatGPT is assessed using F1-Score across 18 unique prompt settings, which were carefully engineered for a comprehensive review. From our evaluation, we found that ChatGPT does not perform well in extracting entities from Malaysian English news articles, with the highest F1-Score of 0.497. Further analysis shows that the morphosyntactic adaptation in Malaysian English caused the limitation. However, interestingly, this morphosyntactic adaptation does not impact the performance of ChatGPT for relation extraction.
</details>
<details>
<summary>摘要</summary>
最近，ChatGPT已经吸引了许多研究者和普通民众的关注。虽然ChatGPT在标准英语文本中的名实体识别和关系提取表现良好，但是还未经过测试是否可以在马来西亚英语文本中表现良好。马来西亚英语独特，它在本地语言上具有语法 sintactic 和semantic 的适应。在这项研究中，我们评估了ChatGPT在马来西亚英语新闻（MEN）数据集中的实体和关系提取能力。我们提出了一种三步方法，称之为“教育-预测-评估”。我们使用F1-Score指标评估ChatGPT在18种不同的提示设定下的表现。从我们的评估结果来看，ChatGPT在马来西亚英语新闻文章中提取实体的表现不佳，最高F1-Score为0.497。进一步分析发现，马来西亚英语中的语法 sintactic 适应限制了ChatGPT的表现。但是奇怪的是，这种语法 sintactic 适应不会影响ChatGPT的关系提取表现。
</details></li>
</ul>
<hr>
<h2 id="KBioXLM-A-Knowledge-anchored-Biomedical-Multilingual-Pretrained-Language-Model"><a href="#KBioXLM-A-Knowledge-anchored-Biomedical-Multilingual-Pretrained-Language-Model" class="headerlink" title="KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model"></a>KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11564">http://arxiv.org/abs/2311.11564</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ngwlh-gl/kbioxlm">https://github.com/ngwlh-gl/kbioxlm</a></li>
<li>paper_authors: Lei Geng, Xu Yan, Ziqiang Cao, Juntao Li, Wenjie Li, Sujian Li, Xinjie Zhou, Yang Yang, Jun Zhang</li>
<li>for: 本研究旨在提高生物医学领域的自然语言处理模型的多语言能力。</li>
<li>methods: 我们提出了一种名为KBioXLM的模型，它将基于XLM-R模型进行知识启发式转换，以适应生物医学领域的多语言需求。我们将三级别的知识启发（实体、事实和段落水平） integrate into 英语版本的医学文献，并设计三种相应的训练任务（实体覆盖、关系覆盖和段落关系预测）。</li>
<li>results: 我们通过将英文benchmarks中的多个任务翻译成中文，以及对XLM-R模型进行练习和提升，达到了跨语言零shot和几shot情况下的显著改进，提高了10+点。<details>
<summary>Abstract</summary>
Most biomedical pretrained language models are monolingual and cannot handle the growing cross-lingual requirements. The scarcity of non-English domain corpora, not to mention parallel data, poses a significant hurdle in training multilingual biomedical models. Since knowledge forms the core of domain-specific corpora and can be translated into various languages accurately, we propose a model called KBioXLM, which transforms the multilingual pretrained model XLM-R into the biomedical domain using a knowledge-anchored approach. We achieve a biomedical multilingual corpus by incorporating three granularity knowledge alignments (entity, fact, and passage levels) into monolingual corpora. Then we design three corresponding training tasks (entity masking, relation masking, and passage relation prediction) and continue training on top of the XLM-R model to enhance its domain cross-lingual ability. To validate the effectiveness of our model, we translate the English benchmarks of multiple tasks into Chinese. Experimental results demonstrate that our model significantly outperforms monolingual and multilingual pretrained models in cross-lingual zero-shot and few-shot scenarios, achieving improvements of up to 10+ points. Our code is publicly available at https://github.com/ngwlh-gl/KBioXLM.
</details>
<details>
<summary>摘要</summary>
大多数生物医学预训模型都是单语言的，无法满足增长的跨语言要求。因为非英语领域数据的罕见性，以及并不存在并行数据，训练多语言生物医学模型具有 significiant hurdle。我们提议一种名为KBioXLM的模型，通过知识anchor的方法将多语言预训模型XLM-R转换到生物医学领域。我们创建了三级别知识对应（实体、事实、段落），并将其包含到单语言 corpora 中。然后，我们设计了三种相应的训练任务（实体覆盖、关系覆盖、段落关系预测），继续在 XLM-R 模型之上进行训练，以提高其跨语言领域的适应能力。为了证明我们的模型的有效性，我们将英文benchmarks的多个任务翻译成中文。实验结果表明，我们的模型在跨语言零shot和几shot scenarios中与单语言和多语言预训模型相比，提高了10+点的性能。我们的代码公开可用于https://github.com/ngwlh-gl/KBioXLM。
</details></li>
</ul>
<hr>
<h2 id="Adapt-in-Contexts-Retrieval-Augmented-Domain-Adaptation-via-In-Context-Learning"><a href="#Adapt-in-Contexts-Retrieval-Augmented-Domain-Adaptation-via-In-Context-Learning" class="headerlink" title="Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning"></a>Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11551">http://arxiv.org/abs/2311.11551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quanyu Long, Wenya Wang, Sinno Jialin Pan</li>
<li>For: This paper focuses on the problem of Unsupervised Domain Adaptation (UDA) for language models (LLMs) in an in-context learning setting, where the goal is to adapt LLMs from a source domain to a target domain without any target labels.* Methods: The proposed method retrieves a subset of cross-domain elements that are most similar to the query, and elicits the language model to adapt in an in-context manner by learning both the target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. The method uses different prompting and training strategies, accounting for different LM architectures to learn the target distribution via language modeling.* Results: The paper demonstrates significant improvements over baseline models through extensive experiments on Sentiment Analysis (SA) and Named Entity Recognition (NER) tasks, thoroughly studying the effectiveness of In-Context Learning (ICL) for domain transfer.Here is the same information in Simplified Chinese text:* For: 这篇论文关注了无监督领域适应（UDA）语言模型（LLM）在卷积学习设置下，目标是将LLM从源频谱中适应目标频谱 без任何目标标签。* Methods: 提议的方法选择源频谱中最相似的查询，并通过增强跨频谱卷积示例来引导语言模型在卷积学习中适应目标频谱和任务信号。* Results: 论文通过对各种语言模型的启发和训练策略进行了广泛的实验，证明了ICL在频谱转移中的有效性，并在 Sentiment Analysis（SA）和Named Entity Recognition（NER）任务上达到了显著的改进。<details>
<summary>Abstract</summary>
Large language models (LLMs) have showcased their capability with few-shot inference known as in-context learning. However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains. The above limitations demonstrate the necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study the UDA problem under an in-context learning setting to adapt language models from the source domain to the target domain without any target labels. The core idea is to retrieve a subset of cross-domain elements that are the most similar to the query, and elicit language model to adapt in an in-context manner by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. We devise different prompting and training strategies, accounting for different LM architectures to learn the target distribution via language modeling. With extensive experiments on Sentiment Analysis (SA) and Named Entity Recognition (NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer and demonstrate significant improvements over baseline models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-teacher-Distillation-for-Multilingual-Spelling-Correction"><a href="#Multi-teacher-Distillation-for-Multilingual-Spelling-Correction" class="headerlink" title="Multi-teacher Distillation for Multilingual Spelling Correction"></a>Multi-teacher Distillation for Multilingual Spelling Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11518">http://arxiv.org/abs/2311.11518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingfen Zhang, Xuan Guo, Sravan Bodapati, Christopher Potts</li>
<li>for: 这个论文是为了解决现代搜索界面中的精准拼写检查问题，特别是在移动设备和语音至文字转换 interfaces 中。</li>
<li>methods: 这篇论文使用多教师浸泡法来解决这个问题，其中每种语言&#x2F;地区都有一个单语言教师模型，这些个教师模型被浸泡到一个多语言学生模型中，以满足所有语言&#x2F;地区的需求。</li>
<li>results: 在使用开源数据以及世界各地搜索服务用户数据的实验中，我们示出了这种方法可以生成高效的拼写检查模型，能够适应部署服务的紧张延迟要求。<details>
<summary>Abstract</summary>
Accurate spelling correction is a critical step in modern search interfaces, especially in an era of mobile devices and speech-to-text interfaces. For services that are deployed around the world, this poses a significant challenge for multilingual NLP: spelling errors need to be caught and corrected in all languages, and even in queries that use multiple languages. In this paper, we tackle this challenge using multi-teacher distillation. On our approach, a monolingual teacher model is trained for each language/locale, and these individual models are distilled into a single multilingual student model intended to serve all languages/locales. In experiments using open-source data as well as user data from a worldwide search service, we show that this leads to highly effective spelling correction models that can meet the tight latency requirements of deployed services.
</details>
<details>
<summary>摘要</summary>
现代搜索界面中，精准的拼写修正是一项重要的步骤，尤其是在移动设备和语音到文本转换器的时代。为全球部署的服务而言，这对多语言NLP提出了一大挑战：拼写错误需要在所有语言和地区中捕捉和修正。在这篇论文中，我们使用多教师浸泡法来解决这个问题。我们的方法是训练每种语言/地区的单语言教师模型，然后将这些个体模型浸泡到一个可以服务所有语言/地区的多语言学生模型中。在使用开源数据以及全球搜索服务的用户数据进行实验中，我们发现这种方法可以创造出高效的拼写修正模型，可以满足部署服务的紧张延迟要求。
</details></li>
</ul>
<hr>
<h2 id="Token-Level-Adversarial-Prompt-Detection-Based-on-Perplexity-Measures-and-Contextual-Information"><a href="#Token-Level-Adversarial-Prompt-Detection-Based-on-Perplexity-Measures-and-Contextual-Information" class="headerlink" title="Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information"></a>Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11509">http://arxiv.org/abs/2311.11509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengmian Hu, Gang Wu, Saayan Mitra, Ruiyi Zhang, Tong Sun, Heng Huang, Vishy Swaminathan</li>
<li>for: 本研究旨在提高大型自然语言模型（LLM）对针对攻击的识别，以减少模型在不正常输入 Situation 下的敏感性。</li>
<li>methods: 本研究提出了一种基于Token-level检测方法，利用LLM对下一个Token的概率预测来识别针对攻击。研究者们还利用了周围Token信息，以促进检测趋势性的针对攻击序列。</li>
<li>results: 研究者们提出了两种方法：一种是判断每个Token是否属于针对攻击序列中的一部分，另一种是估计每个Token是否属于针对攻击序列。两种方法均可以帮助提高LLM对针对攻击的识别能力。<details>
<summary>Abstract</summary>
In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that lead to undesirable outputs. The inherent vulnerability of LLMs stems from their input-output mechanisms, especially when presented with intensely out-of-distribution (OOD) inputs. This paper proposes a token-level detection method to identify adversarial prompts, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity and incorporate neighboring token information to encourage the detection of contiguous adversarial prompt sequences. As a result, we propose two methods: one that identifies each token as either being part of an adversarial prompt or not, and another that estimates the probability of each token being part of an adversarial prompt.
</details>
<details>
<summary>摘要</summary>
Recently, Large Language Models (LLM) have become crucial tools in various applications, but they are vulnerable to adversarial prompt attacks. These attacks manipulate the input strings to elicit undesirable outputs from the models. The vulnerability stems from the input-output mechanisms of LLMs, especially when faced with highly out-of-distribution (OOD) inputs. This paper proposes a token-level detection method to identify adversarial prompts, leveraging the LLM's ability to predict the next token's probability. We measure the model's perplexity and incorporate neighboring token information to detect contiguous adversarial prompt sequences. As a result, we propose two methods: one that identifies each token as either part of an adversarial prompt or not, and another that estimates the probability of each token being part of an adversarial prompt.Here's the text in Traditional Chinese:近年来，大语言模型（LLM）已成为不同应用中的重要工具，但它们受到了对抗提示攻击的威胁。这些攻击可以专门设计input字串，以让模型产生不适合的输出。这些攻击的根源在于LLM的输入输出机制，尤其是面对高度out-of-distribution（OOD）的输入。本文提出了一种token级检测方法，利用LLM对下一个字串的概率预测来识别对抗提示。我们 mesure the model的困惑度和包含相邻字串信息，以实现检测连续的对抗提示序列。因此，我们提出了两种方法：一种是将每个字串标记为是否是对抗提示的一部分，另一种是估算每个字串是否是对抗提示的概率。
</details></li>
</ul>
<hr>
<h2 id="What’s-left-can’t-be-right-–-The-remaining-positional-incompetence-of-contrastive-vision-language-models"><a href="#What’s-left-can’t-be-right-–-The-remaining-positional-incompetence-of-contrastive-vision-language-models" class="headerlink" title="What’s left can’t be right – The remaining positional incompetence of contrastive vision-language models"></a>What’s left can’t be right – The remaining positional incompetence of contrastive vision-language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11477">http://arxiv.org/abs/2311.11477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nils Hoehing, Ellen Rushe, Anthony Ventresque</li>
<li>for: 这 paper 是为了解释 contrastive vision-language models 缺乏空间理解能力的可能性。</li>
<li>methods: 作者通过分析 dataset 和嵌入空间来分析这种现象。他们主要关注简单的左右位置关系，并证明这种行为可以预测，即使使用大规模 dataset。此外，他们还示出可以使用 sintetic data 教导这种关系，并且这种方法可以良好地适应自然图像，提高 Visual Genome Relations 中左右关系的表现。</li>
<li>results: 作者的研究表明，通过教导 left-right 位置关系，可以提高 contrastive vision-language models 的空间理解能力。<details>
<summary>Abstract</summary>
Contrastive vision-language models like CLIP have been found to lack spatial understanding capabilities. In this paper we discuss the possible causes of this phenomenon by analysing both datasets and embedding space. By focusing on simple left-right positional relations, we show that this behaviour is entirely predictable, even with large-scale datasets, demonstrate that these relations can be taught using synthetic data and show that this approach can generalise well to natural images - improving the performance on left-right relations on Visual Genome Relations.
</details>
<details>
<summary>摘要</summary>
clip 类型的视觉语言模型缺乏空间理解能力。在这篇论文中，我们分析了数据集和嵌入空间，探讨了这种现象的可能的原因。通过关注简单的左右位置关系，我们表明这种行为是可预测的，甚至使用大规模数据集，并证明这种方法可以通过人工数据进行教育，并且这种方法可以良好地泛化到自然图像上，提高Visual Genome Relations中的左右关系表现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/20/cs.CL_2023_11_20/" data-id="clpztdnf900fies883takh7nu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/20/cs.LG_2023_11_20/" class="article-date">
  <time datetime="2023-11-20T10:00:00.000Z" itemprop="datePublished">2023-11-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/20/cs.LG_2023_11_20/">cs.LG - 2023-11-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improvements-in-Interlayer-Pipelining-of-CNN-Accelerators-Using-Genetic-Algorithms"><a href="#Improvements-in-Interlayer-Pipelining-of-CNN-Accelerators-Using-Genetic-Algorithms" class="headerlink" title="Improvements in Interlayer Pipelining of CNN Accelerators Using Genetic Algorithms"></a>Improvements in Interlayer Pipelining of CNN Accelerators Using Genetic Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12235">http://arxiv.org/abs/2311.12235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Horeni, Siddharth Joshi</li>
<li>for: 这篇论文是为了提高边缘平台上的卷积神经网络（CNNs）的效率执行。</li>
<li>methods: 我们使用了一种层融合技术，使得CNNs可以更有效地执行，并使用生成算法（GA）应用于图形基本排序来减少外部数据传输。</li>
<li>results: 我们的方法可以对MobileNet-v3进行适当的优化，实现1.8倍的能源效率提升和1.9倍的能源延迟产品（EDP）提升，并在SIMBA和Eyeriss上保持了1.4倍的EDP提升和1.12倍的EDP提升。<details>
<summary>Abstract</summary>
Deploying Convolutional Neural Networks (CNNs) on edge platforms necessitates efficient hardware acceleration. Any unnecessary data movement in such accelerators can unacceptably degrade performance and efficiency. To address this, we develop a layer fusion technique targeting CNNs, that reduces off-chip data communication using a Genetic Algorithm (GA) applied to graph-based topological sort. Results show a 1.8$\times$ increase in energy efficiency and 1.9$\times$ improvement in energy-delay product (EDP) for MobileNet-v3 on a SIMBA-like mobile architecture. Our approach consistently improves workload performance, averaging 1.4$\times$ improvement to EDP for SIMBA and 1.12$\times$ for Eyeriss.
</details>
<details>
<summary>摘要</summary>
部署卷积神经网络（CNN）在边缘平台上需要高效的硬件加速。任何没有必要的数据传输在这些加速器中可以不acceptably降低性能和效率。为解决这个问题，我们开发了层融合技术targeting CNNs，减少了off-chip数据通信使用基因算法（GA）应用于图形基于排序。结果显示MobileNet-v3在SIMBA-like移动架构上提高了能效率1.8倍和能量延迟产品（EDP）1.9倍。我们的方法一般改善工作负荷性能，平均提高了SIMBA和Eyeriss的EDP1.4倍和1.12倍。
</details></li>
</ul>
<hr>
<h2 id="Data-Guided-Regulator-for-Adaptive-Nonlinear-Control"><a href="#Data-Guided-Regulator-for-Adaptive-Nonlinear-Control" class="headerlink" title="Data-Guided Regulator for Adaptive Nonlinear Control"></a>Data-Guided Regulator for Adaptive Nonlinear Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12230">http://arxiv.org/abs/2311.12230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NiyoushaRahimi/Data-Guided-Regulator-for-Adaptive-Nonlinear-Control">https://github.com/NiyoushaRahimi/Data-Guided-Regulator-for-Adaptive-Nonlinear-Control</a></li>
<li>paper_authors: Niyousha Rahimi, Mehran Mesbahi</li>
<li>for: 这篇论文是关于设计基于数据驱动的反馈控制器，用于处理复杂非线性动力系统中的时变干扰。</li>
<li>methods: 该论文提出了一种基于直观政策更新的数据驱动控制策略，可以在不知道系统动力学的情况下实现系统状态的快速稳定。</li>
<li>results: 在一个6度自由落体下降控制问题中，该策略能够有效地处理恶境干扰。In English, this would be:</li>
<li>for: This paper addresses the problem of designing a data-driven feedback controller for complex nonlinear dynamical systems in the presence of time-varying disturbances with unknown dynamics.</li>
<li>methods: The proposed method uses direct policy updates based on data-driven control, which can achieve fast regulation of system states without knowing the system dynamics.</li>
<li>results: The proposed method is effective in handling adverse environmental disturbances in a 6-DOF power descent guidance problem.<details>
<summary>Abstract</summary>
This paper addresses the problem of designing a data-driven feedback controller for complex nonlinear dynamical systems in the presence of time-varying disturbances with unknown dynamics. Such disturbances are modeled as the "unknown" part of the system dynamics. The goal is to achieve finite-time regulation of system states through direct policy updates while also generating informative data that can subsequently be used for data-driven stabilization or system identification. First, we expand upon the notion of "regularizability" and characterize this system characteristic for a linear time-varying representation of the nonlinear system with locally-bounded higher-order terms. "Rapid-regularizability" then gauges the extent by which a system can be regulated in finite time, in contrast to its asymptotic behavior. We then propose the Data-Guided Regulation for Adaptive Nonlinear Control ( DG-RAN) algorithm, an online iterative synthesis procedure that utilizes discrete time-series data from a single trajectory for regulating system states and identifying disturbance dynamics. The effectiveness of our approach is demonstrated on a 6-DOF power descent guidance problem in the presence of adverse environmental disturbances.
</details>
<details>
<summary>摘要</summary>
We first explore the concept of "regularizability" and its application to a linear time-varying representation of the nonlinear system with locally-bounded higher-order terms. This allows us to gauge the extent to which a system can be regulated in finite time, rather than just its asymptotic behavior.Next, we propose the Data-Guided Regulation for Adaptive Nonlinear Control (DG-RAN) algorithm, an online iterative synthesis procedure that uses discrete time-series data from a single trajectory to regulate system states and identify disturbance dynamics. The effectiveness of our approach is demonstrated on a 6-DOF power descent guidance problem in the presence of adverse environmental disturbances.
</details></li>
</ul>
<hr>
<h2 id="Random-Fourier-Signature-Features"><a href="#Random-Fourier-Signature-Features" class="headerlink" title="Random Fourier Signature Features"></a>Random Fourier Signature Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12214">http://arxiv.org/abs/2311.12214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Csaba Toth, Harald Oberhauser, Zoltan Szabo</li>
<li>for: 这篇论文是为了提出一种基于张量代数的时间序列相似度度量方法，以及两种可扩展的时间序列特征提取方法。</li>
<li>methods: 这篇论文使用了随机傅立叶特征来加速张量kernel的计算，并使用了最近的张量 проек 来 derivate两种更加可扩展的时间序列特征。</li>
<li>results: 论文的实验结果表明，采用随机傅立叶特征来加速张量kernel的计算可以减少计算成本，同时维持准确性。这种方法可以处理中型数据集，并且可以扩展到大型数据集。<details>
<summary>Abstract</summary>
Tensor algebras give rise to one of the most powerful measures of similarity for sequences of arbitrary length called the signature kernel accompanied with attractive theoretical guarantees from stochastic analysis. Previous algorithms to compute the signature kernel scale quadratically in terms of the length and the number of the sequences. To mitigate this severe computational bottleneck, we develop a random Fourier feature-based acceleration of the signature kernel acting on the inherently non-Euclidean domain of sequences. We show uniform approximation guarantees for the proposed unbiased estimator of the signature kernel, while keeping its computation linear in the sequence length and number. In addition, combined with recent advances on tensor projections, we derive two even more scalable time series features with favourable concentration properties and computational complexity both in time and memory. Our empirical results show that the reduction in computational cost comes at a negligible price in terms of accuracy on moderate-sized datasets, and it enables one to scale to large datasets up to a million time series.
</details>
<details>
<summary>摘要</summary>
tensor代数可以生成序列的最强度 similarity 度量，称为签名kernel，并且拥有attractive的统计分析理论保证。先前的计算签名kernel 方法scales  quadratic 方式与序列长度和序列数量相关。为了解决这种严重的计算瓶颈，我们开发了基于Random Fourier Feature的签名kernel 加速方法，对序列的非欧几何空间进行加速。我们证明了对提档 estimator 的 uniform approximation guarantees，并且保持计算线性响应于序列长度和序列数量。此外，通过与近期的tensor projection 技术结合，我们 derive 两种even more scalable 时间序列特征，具有良好的峰度性和计算复杂度。我们的实验结果表明，计算成本的减少来自于精度的减少，并且可以在moderate-sized datasets 上进行扩展，并且可以扩展到大量时间序列数据。
</details></li>
</ul>
<hr>
<h2 id="Improving-Label-Assignments-Learning-by-Dynamic-Sample-Dropout-Combined-with-Layer-wise-Optimization-in-Speech-Separation"><a href="#Improving-Label-Assignments-Learning-by-Dynamic-Sample-Dropout-Combined-with-Layer-wise-Optimization-in-Speech-Separation" class="headerlink" title="Improving Label Assignments Learning by Dynamic Sample Dropout Combined with Layer-wise Optimization in Speech Separation"></a>Improving Label Assignments Learning by Dynamic Sample Dropout Combined with Layer-wise Optimization in Speech Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12199">http://arxiv.org/abs/2311.12199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyang Gao, Yue Gu, Ivan Marsic</li>
<li>for:  solve excessive label assignment switching and layer-decoupling issues in supervised speech separation using permutation invariant training (PIT)</li>
<li>methods:  dynamic sample dropout (DSD) and layer-wise optimization (LO)</li>
<li>results:  outperforms the baseline and improves the performance of speech separation tasks<details>
<summary>Abstract</summary>
In supervised speech separation, permutation invariant training (PIT) is widely used to handle label ambiguity by selecting the best permutation to update the model. Despite its success, previous studies showed that PIT is plagued by excessive label assignment switching in adjacent epochs, impeding the model to learn better label assignments. To address this issue, we propose a novel training strategy, dynamic sample dropout (DSD), which considers previous best label assignments and evaluation metrics to exclude the samples that may negatively impact the learned label assignments during training. Additionally, we include layer-wise optimization (LO) to improve the performance by solving layer-decoupling. Our experiments showed that combining DSD and LO outperforms the baseline and solves excessive label assignment switching and layer-decoupling issues. The proposed DSD and LO approach is easy to implement, requires no extra training sets or steps, and shows generality to various speech separation tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化字符的中文。<</SYS>>在监督推理中的语音分离方法中，固定 permutation 的训练（PIT）广泛应用于处理标签不确定性，选择最佳 permutation 更新模型。尽管它取得了成功，但前一些研究表明，PIT 受到邻近轮次的标签分配转移的困扰，阻碍模型学习更好的标签分配。为解决这个问题，我们提出了一种新的训练策略，动态样本排除（DSD），该策略考虑了前一些最佳标签分配和评价指标，以排除在训练过程中可能干扰学习的标签分配的样本。此外，我们还包括层 wise 优化（LO），以提高性能，解决层脱decoupling问题。我们的实验表明，将 DSD 和 LO 结合使用可以超越基准，解决频繁标签分配转移和层脱decoupling问题。提议的 DSD 和 LO 方法易于实现，无需额外的训练集或步骤，并且对各种语音分离任务具有通用性。
</details></li>
</ul>
<hr>
<h2 id="Node-classification-in-random-trees"><a href="#Node-classification-in-random-trees" class="headerlink" title="Node classification in random trees"></a>Node classification in random trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12167">http://arxiv.org/abs/2311.12167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wouterwln/neuralfactortrees">https://github.com/wouterwln/neuralfactortrees</a></li>
<li>paper_authors: Wouter W. L. Nuijten, Vlado Menkovski</li>
<li>for: 模型random trees中的节点分类任务</li>
<li>methods: 使用Markov网络和Graph Neural Network来定义一个 Gibbs 分布，并使用 MCMC 来采样节点分类结果</li>
<li>results: 在Stanford Sentiment Treebank dataset上，方法比基eline表现出色，能够有效地模型节点分类任务中的联合分布。<details>
<summary>Abstract</summary>
We propose a method for the classification of objects that are structured as random trees. Our aim is to model a distribution over the node label assignments in settings where the tree data structure is associated with node attributes (typically high dimensional embeddings). The tree topology is not predetermined and none of the label assignments are present during inference. Other methods that produce a distribution over node label assignment in trees (or more generally in graphs) either assume conditional independence of the label assignment, operate on a fixed graph topology, or require part of the node labels to be observed. Our method defines a Markov Network with the corresponding topology of the random tree and an associated Gibbs distribution. We parameterize the Gibbs distribution with a Graph Neural Network that operates on the random tree and the node embeddings. This allows us to estimate the likelihood of node assignments for a given random tree and use MCMC to sample from the distribution of node assignments.   We evaluate our method on the tasks of node classification in trees on the Stanford Sentiment Treebank dataset. Our method outperforms the baselines on this dataset, demonstrating its effectiveness for modeling joint distributions of node labels in random trees.
</details>
<details>
<summary>摘要</summary>
我们提出一种方法用于对结构化为随机树的对象进行分类。我们的目标是模型在结构化为高维嵌入的树数据结构下的分布 over 节点标签分配。树的结构不固定，并且在推理过程中没有任何节点标签的 observable。现有的方法可以生成节点标签分配的分布在树（或更一般地在图）中，但是它们都假设节点标签之间的 conditional independence，或者操作在固定的图结构上，或者需要一些节点标签的观察值。我们的方法定义了一个Markov网络，其中包含随机树的相应的topology和节点嵌入的相关性。我们使用 Graph Neural Network 来参数化 Gibbs 分布，以便在给定的随机树和节点嵌入下 estimating 节点分配的概率。我们使用 MCMC 来采样这个分布中的节点分配。我们在 Stanford Sentiment Treebank 数据集上进行节点分类任务中，我们的方法比基eline 高效，这说明了我们的方法在模型结构化为随机树的情况下 joint 分布的节点标签的能力。
</details></li>
</ul>
<hr>
<h2 id="Creating-Temporally-Correlated-High-Resolution-Power-Injection-Profiles-Using-Physics-Aware-GAN"><a href="#Creating-Temporally-Correlated-High-Resolution-Power-Injection-Profiles-Using-Physics-Aware-GAN" class="headerlink" title="Creating Temporally Correlated High-Resolution Power Injection Profiles Using Physics-Aware GAN"></a>Creating Temporally Correlated High-Resolution Power Injection Profiles Using Physics-Aware GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12166">http://arxiv.org/abs/2311.12166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hritik Gopal Shah, Behrouz Azimian, Anamitra Pal</li>
<li>for:  solves the problem of lacking granularity in traditional smart meter measurements, enabling real-time decision-making.</li>
<li>methods:  uses generative adversarial networks (GAN) with hard inequality constraints and convex optimization layer to enforce temporal consistency and create minutely interval temporally-correlated instantaneous power injection profiles from 15-minute average power consumption information.</li>
<li>results:  successfully creates high-resolution power injection profiles from slow timescale aggregated power information, offering a promising avenue for improved high-speed state estimation in distribution systems.<details>
<summary>Abstract</summary>
Traditional smart meter measurements lack the granularity needed for real-time decision-making. To address this practical problem, we create a generative adversarial networks (GAN) model that enforces temporal consistency on its high-resolution outputs via hard inequality constraints using a convex optimization layer. A unique feature of our GAN model is that it is trained solely on slow timescale aggregated power information obtained from historical smart meter data. The results demonstrate that the model can successfully create minutely interval temporally-correlated instantaneous power injection profiles from 15-minute average power consumption information. This innovative approach, emphasizing inter-neuron constraints, offers a promising avenue for improved high-speed state estimation in distribution systems and enhances the applicability of data-driven solutions for monitoring such systems.
</details>
<details>
<summary>摘要</summary>
传统智能仪表测量lack the granularity needed for real-time decision-making. To address this practical problem, we create a generative adversarial networks (GAN) model that enforces temporal consistency on its high-resolution outputs via hard inequality constraints using a convex optimization layer. A unique feature of our GAN model is that it is trained solely on slow timescale aggregated power information obtained from historical smart meter data. The results demonstrate that the model can successfully create minutely interval temporally-correlated instantaneous power injection profiles from 15-minute average power consumption information. This innovative approach, emphasizing inter-neuron constraints, offers a promising avenue for improved high-speed state estimation in distribution systems and enhances the applicability of data-driven solutions for monitoring such systems.Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".Here is the text with the same translation, but with Traditional Chinese characters:传统智能仪表测量lack the granularity needed for real-time decision-making. To address this practical problem, we create a generative adversarial networks (GAN) model that enforces temporal consistency on its high-resolution outputs via hard inequality constraints using a convex optimization layer. A unique feature of our GAN model is that it is trained solely on slow timescale aggregated power information obtained from historical smart meter data. The results demonstrate that the model can successfully create minutely interval temporally-correlated instantaneous power injection profiles from 15-minute average power consumption information. This innovative approach, emphasizing inter-neuron constraints, offers a promising avenue for improved high-speed state estimation in distribution systems and enhances the applicability of data-driven solutions for monitoring such systems.Note: Traditional Chinese is also known as "Traditional Mandarin" or "Formal Chinese".
</details></li>
</ul>
<hr>
<h2 id="Quantum-Inception-Score"><a href="#Quantum-Inception-Score" class="headerlink" title="Quantum Inception Score"></a>Quantum Inception Score</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12163">http://arxiv.org/abs/2311.12163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akira Sone, Naoki Yamamoto</li>
<li>for: 评估量化宇宙学模型质量</li>
<li>methods: 基于量子频率的量子通道识别数据集，并利用量子凝聚和量子混合提高模型质量</li>
<li>results: 量子生成模型比 классиical模型提供更高质量，归因于量子凝聚和量子混合的存在，而且利用量子抖动定理Physical Limitation of Quantum Generative Models.<details>
<summary>Abstract</summary>
Motivated by the great success of classical generative models in machine learning, enthusiastic exploration of their quantum version has recently started. To depart on this journey, it is important to develop a relevant metric to evaluate the quality of quantum generative models; in the classical case, one such examples is the inception score. In this paper, we propose the quantum inception score, which relates the quality to the classical capacity of the quantum channel that classifies a given dataset. We prove that, under this proposed measure, the quantum generative models provide better quality than their classical counterparts because of the presence of quantum coherence and entanglement. Finally, we harness the quantum fluctuation theorem to characterize the physical limitation of the quality of quantum generative models.
</details>
<details>
<summary>摘要</summary>
受古典生成模型在机器学习中的成功启发，现在开始了对其量子版本的积极探索。为进行这项探索，需要开发一个相关的评价指标，以衡量量子生成模型的质量。在古典情况下，一个例子是引入性分数。在这篇论文中，我们提出了量子引入分数，它与给定数据集的量子通道的分类质量有关。我们证明了，根据我们提出的评价指标，量子生成模型比其古典对应者更高质量，这是因为量子干扰和共聚的存在。最后，我们利用量子抖振定理来 caracterize量子生成模型的物理限制。
</details></li>
</ul>
<hr>
<h2 id="Risk-averse-Batch-Active-Inverse-Reward-Design"><a href="#Risk-averse-Batch-Active-Inverse-Reward-Design" class="headerlink" title="Risk-averse Batch Active Inverse Reward Design"></a>Risk-averse Batch Active Inverse Reward Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12004">http://arxiv.org/abs/2311.12004</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pliam1105/RBAIRD">https://github.com/pliam1105/RBAIRD</a></li>
<li>paper_authors: Panagiotis Liampas<br>for:This paper proposes a new method called Risk-averse Batch Active Inverse Reward Design (RBAIRD) to help train AI models that can adapt to real-world scenarios and learn how to treat unknown features.methods:RBAIRD uses a series of queries to compute a probability distribution over the intended reward function, and then uses this distribution to construct batches of environments that the agent encounters in the real world. It also integrates a risk-averse planner to ensure safety while the agent is learning the reward function.results:Compared to previous approaches, RBAIRD outperformed in terms of efficiency, accuracy, and action certainty, and demonstrated quick adaptability to new, unknown features. It can be more widely used for the alignment of crucial, powerful AI models.<details>
<summary>Abstract</summary>
Designing a perfect reward function that depicts all the aspects of the intended behavior is almost impossible, especially generalizing it outside of the training environments. Active Inverse Reward Design (AIRD) proposed the use of a series of queries, comparing possible reward functions in a single training environment. This allows the human to give information to the agent about suboptimal behaviors, in order to compute a probability distribution over the intended reward function. However, it ignores the possibility of unknown features appearing in real-world environments, and the safety measures needed until the agent completely learns the reward function. I improved this method and created Risk-averse Batch Active Inverse Reward Design (RBAIRD), which constructs batches, sets of environments the agent encounters when being used in the real world, processes them sequentially, and, for a predetermined number of iterations, asks queries that the human needs to answer for each environment of the batch. After this process is completed in one batch, the probabilities have been improved and are transferred to the next batch. This makes it capable of adapting to real-world scenarios and learning how to treat unknown features it encounters for the first time. I also integrated a risk-averse planner, similar to that of Inverse Reward Design (IRD), which samples a set of reward functions from the probability distribution and computes a trajectory that takes the most certain rewards possible. This ensures safety while the agent is still learning the reward function, and enables the use of this approach in situations where cautiousness is vital. RBAIRD outperformed the previous approaches in terms of efficiency, accuracy, and action certainty, demonstrated quick adaptability to new, unknown features, and can be more widely used for the alignment of crucial, powerful AI models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>设计完美的奖励函数，涵盖所有行为方面的目标是几乎不可能，尤其是在训练环境外的泛化。活动逆奖函数设计（AIRD）提议使用一系列的问题，比较可能的奖励函数在单个训练环境中。这 Allow human 为 agent 提供关于不优的行为的信息，以计算概率分布上的奖励函数。然而，它忽略了实际环境中可能出现的未知特征，以及agent完全学习奖励函数时需要的安全措施。我改进了这种方法，创造了风险偏好批量活动逆奖函数设计（RBAIRD），它在实际应用中顺序处理批量环境，并在 predetermined 数量的迭代中请求人类回答每个环境的问题。在这个过程中，概率已经提高，并将被传递到下一个批量中。这使得它可以适应实际应用场景，学习新特征时遇到的 unknown features 的处理方式。我还将逆奖函数设计（IRD）中的风险偏好排定器integrated，该排定器在概率分布中采样一组奖励函数，并计算一个可以获得最大可靠奖励的路径。这确保了安全性，使得 Agent 在学习奖励函数时能够保持综合性，并在需要谨慎的情况下使用这种方法。RBAIRD 在效率、准确性和行动确定性方面超越了先前的方法，快速适应新的未知特征，并可以更广泛地应用于对重要、强大 AI 模型的对Alignment。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learned-Atomic-Cluster-Expansion-Potentials-for-Fast-and-Quantum-Accurate-Thermal-Simulations-of-Wurtzite-AlN"><a href="#Machine-Learned-Atomic-Cluster-Expansion-Potentials-for-Fast-and-Quantum-Accurate-Thermal-Simulations-of-Wurtzite-AlN" class="headerlink" title="Machine-Learned Atomic Cluster Expansion Potentials for Fast and Quantum-Accurate Thermal Simulations of Wurtzite AlN"></a>Machine-Learned Atomic Cluster Expansion Potentials for Fast and Quantum-Accurate Thermal Simulations of Wurtzite AlN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11990">http://arxiv.org/abs/2311.11990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guang Yang, Yuan-Bin Liu, Lei Yang, Bing-Yang Cao</li>
<li>for: 本研究用 atomic cluster expansion 框架开发了一个机器学习对应材料的声波传输性质的 potential.</li>
<li>methods: 本研究使用了 density functional theory (DFT) 和 machine learning 技术来预测 wurtzite aluminum nitride 的热导发性和声波传输性质.</li>
<li>results: 研究发现 ACE potential 可以对 wurtzite aluminum nitride 的热导发性和声波传输性质进行高精度预测，并且可以对该材料的对称对称和材料对称对称具有较高的预测能力.<details>
<summary>Abstract</summary>
Using the atomic cluster expansion (ACE) framework, we develop a machine learning interatomic potential for fast and accurately modelling the phonon transport properties of wurtzite aluminum nitride. The predictive power of the ACE potential against density functional theory (DFT) is demonstrated across a broad range of properties of w-AlN, including ground-state lattice parameters, specific heat capacity, coefficients of thermal expansion, bulk modulus, and harmonic phonon dispersions. Validation of lattice thermal conductivity is further carried out by comparing the ACE-predicted values to the DFT calculations and experiments, exhibiting the overall capability of our ACE potential in sufficiently describing anharmonic phonon interactions. As a practical application, we perform a lattice dynamics analysis using the potential to unravel the effects of biaxial strains on thermal conductivity and phonon properties of w-AlN, which is identified as a significant tuning factor for near-junction thermal design of w-AlN-based electronics.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries. The traditional Chinese form is also available, which is used in Taiwan and Hong Kong. If you prefer traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Provably-Efficient-CVaR-RL-in-Low-rank-MDPs"><a href="#Provably-Efficient-CVaR-RL-in-Low-rank-MDPs" class="headerlink" title="Provably Efficient CVaR RL in Low-rank MDPs"></a>Provably Efficient CVaR RL in Low-rank MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11965">http://arxiv.org/abs/2311.11965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulai Zhao, Wenhao Zhan, Xiaoyan Hu, Ho-fung Leung, Farzan Farnia, Wen Sun, Jason D. Lee</li>
<li>For: The paper aims to maximize the Conditional Value at Risk (CVaR) with a fixed risk tolerance $\tau$ in low-rank Markov Decision Processes (MDPs) with nonlinear function approximation.* Methods: The paper proposes a novel Upper Confidence Bound (UCB) bonus-driven algorithm that balances exploration, exploitation, and representation learning in CVaR RL. The algorithm uses a discretized Least-Squares Value Iteration (LSVI) algorithm for the CVaR objective as the planning oracle.* Results: The paper achieves a sample complexity of $\tilde{O}\left(\frac{H^7 A^2 d^4}{\tau^2 \epsilon^2}\right)$ to yield an $\epsilon$-optimal CVaR, where $H$ is the length of each episode, $A$ is the capacity of action space, and $d$ is the dimension of representations. The algorithm is provably efficient in low-rank MDPs and can find the near-optimal policy in a polynomial running time with a Maximum Likelihood Estimation oracle.<details>
<summary>Abstract</summary>
We study risk-sensitive Reinforcement Learning (RL), where we aim to maximize the Conditional Value at Risk (CVaR) with a fixed risk tolerance $\tau$. Prior theoretical work studying risk-sensitive RL focuses on the tabular Markov Decision Processes (MDPs) setting. To extend CVaR RL to settings where state space is large, function approximation must be deployed. We study CVaR RL in low-rank MDPs with nonlinear function approximation. Low-rank MDPs assume the underlying transition kernel admits a low-rank decomposition, but unlike prior linear models, low-rank MDPs do not assume the feature or state-action representation is known. We propose a novel Upper Confidence Bound (UCB) bonus-driven algorithm to carefully balance the interplay between exploration, exploitation, and representation learning in CVaR RL. We prove that our algorithm achieves a sample complexity of $\tilde{O}\left(\frac{H^7 A^2 d^4}{\tau^2 \epsilon^2}\right)$ to yield an $\epsilon$-optimal CVaR, where $H$ is the length of each episode, $A$ is the capacity of action space, and $d$ is the dimension of representations. Computational-wise, we design a novel discretized Least-Squares Value Iteration (LSVI) algorithm for the CVaR objective as the planning oracle and show that we can find the near-optimal policy in a polynomial running time with a Maximum Likelihood Estimation oracle. To our knowledge, this is the first provably efficient CVaR RL algorithm in low-rank MDPs.
</details>
<details>
<summary>摘要</summary>
我们研究风险敏感奖励学习（RL），目标是 Maximize Conditional Value at Risk（CVaR），并且固定风险容忍度为 $\tau$。先前的理论研究把注重风险RL固定在文件Markov Decision Processes（MDPs）中。为了扩展CVaR RL到大状态空间中，函数近似必须被应用。我们研究CVaR RL在低级MDPs中，其中过渡概率函数可以分解为低级矩阵。低级MDPs不同于先前的线性模型，不需要状态或行动表示的知识。我们提出了一种新的Upper Confidence Bound（UCB）奖励驱动算法，用于在探索、利用和表示学习之间进行精准的平衡。我们证明我们的算法可以在 $\tilde{O}\left(\frac{H^7 A^2 d^4}{\tau^2 \epsilon^2}\right)$ 样本复杂度内获得 $\epsilon$-优的CVaR，其中 $H$ 是每个episode的长度， $A$ 是动作空间的容量， $d$ 是表示的维度。计算机上，我们设计了一种灵活的积分最小二乘值迭代（LSVI）算法来实现CVaR目标，并证明我们可以在对数时间内找到近似优化策略。到我们所知，这是首个可证明高效的CVaR RL算法在低级MDPs中。
</details></li>
</ul>
<hr>
<h2 id="Estimation-of-entropy-regularized-optimal-transport-maps-between-non-compactly-supported-measures"><a href="#Estimation-of-entropy-regularized-optimal-transport-maps-between-non-compactly-supported-measures" class="headerlink" title="Estimation of entropy-regularized optimal transport maps between non-compactly supported measures"></a>Estimation of entropy-regularized optimal transport maps between non-compactly supported measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11934">http://arxiv.org/abs/2311.11934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mattwerenski/entropic-map">https://github.com/mattwerenski/entropic-map</a></li>
<li>paper_authors: Matthew Werenski, James M. Murphy, Shuchin Aeron</li>
<li>for: 这个论文目的是解决估计带有Entropy regularization的优化运输（EOT）图的问题，其中源和目标概率分布均为几何分布。</li>
<li>methods: 该论文使用了一种新的在样本中估计器，并使用了偏置-变量分解以控制偏置和变量的整体影响。</li>
<li>results: 论文表明，在一些特定的情况下，预期的$L^2$误差的平方根速率至少为$O(n^{-1&#x2F;3})$，而在总体情况下，预期的$L^1$误差的幂率至少为$O(n^{-1&#x2F;6})$。此外，论文还证明了对于不同的常数参数，误差的幂率均为 polynomials。<details>
<summary>Abstract</summary>
This paper addresses the problem of estimating entropy-regularized optimal transport (EOT) maps with squared-Euclidean cost between source and target measures that are subGaussian. In the case that the target measure is compactly supported or strongly log-concave, we show that for a recently proposed in-sample estimator, the expected squared $L^2$-error decays at least as fast as $O(n^{-1/3})$ where $n$ is the sample size. For the general subGaussian case we show that the expected $L^1$-error decays at least as fast as $O(n^{-1/6})$, and in both cases we have polynomial dependence on the regularization parameter. While these results are suboptimal compared to known results in the case of compactness of both the source and target measures (squared $L^2$-error converging at a rate $O(n^{-1})$) and for when the source is subGaussian while the target is compactly supported (squared $L^2$-error converging at a rate $O(n^{-1/2})$), their importance lie in eliminating the compact support requirements. The proof technique makes use of a bias-variance decomposition where the variance is controlled using standard concentration of measure results and the bias is handled by T1-transport inequalities along with sample complexity results in estimation of EOT cost under subGaussian assumptions. Our experimental results point to a looseness in controlling the variance terms and we conclude by posing several open problems.
</details>
<details>
<summary>摘要</summary>
Our proof technique uses a bias-variance decomposition, where the variance is controlled using standard concentration of measure results, and the bias is handled by T1-transport inequalities and sample complexity results in estimation of EOT cost under subGaussian assumptions. However, our experimental results suggest that there may be looseness in controlling the variance terms, and we conclude by posing several open problems.
</details></li>
</ul>
<hr>
<h2 id="Deep-Calibration-of-Market-Simulations-using-Neural-Density-Estimators-and-Embedding-Networks"><a href="#Deep-Calibration-of-Market-Simulations-using-Neural-Density-Estimators-and-Embedding-Networks" class="headerlink" title="Deep Calibration of Market Simulations using Neural Density Estimators and Embedding Networks"></a>Deep Calibration of Market Simulations using Neural Density Estimators and Embedding Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11913">http://arxiv.org/abs/2311.11913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Namid R. Stillman, Rory Baggott, Justin Lyon, Jianfei Zhang, Dingqiu Zhu, Tao Chen, Perukrishnen Vytelingum</li>
<li>for: This paper is written for those who are interested in developing realistic simulators of financial exchanges, and who want to use deep learning techniques to calibrate these simulators to specific periods of trading.</li>
<li>methods: The paper uses deep learning techniques, specifically neural density estimators and embedding networks, to calibrate market simulators to a specific period of trading.</li>
<li>results: The paper demonstrates that its approach is able to correctly identify high probability parameter sets, both when applied to synthetic and historical data, and without reliance on manually selected or weighted ensembles of stylised facts.Here’s the Chinese translation of the three pieces of information:</li>
<li>for: 这篇论文是为了帮助开发金融交易市场的实际模拟器，并使用深度学习技术来对这些模拟器进行准确的启动。</li>
<li>methods: 这篇论文使用深度学习技术，具体来说是神经density估计器和嵌入网络，来对市场模拟器进行准确的启动。</li>
<li>results: 这篇论文显示了其方法可以正确地标记高概率参数集，并且无需人工选择或权重组合的简化特征。<details>
<summary>Abstract</summary>
The ability to construct a realistic simulator of financial exchanges, including reproducing the dynamics of the limit order book, can give insight into many counterfactual scenarios, such as a flash crash, a margin call, or changes in macroeconomic outlook. In recent years, agent-based models have been developed that reproduce many features of an exchange, as summarised by a set of stylised facts and statistics. However, the ability to calibrate simulators to a specific period of trading remains an open challenge. In this work, we develop a novel approach to the calibration of market simulators by leveraging recent advances in deep learning, specifically using neural density estimators and embedding networks. We demonstrate that our approach is able to correctly identify high probability parameter sets, both when applied to synthetic and historical data, and without reliance on manually selected or weighted ensembles of stylised facts.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用深度学习技术构建真实的金融交易场景 simulator，包括复制限制Order Book 的动态，可以提供许多 counterfactual enario，如快速衰退、 margin call 或 macroeconomic 景象变化。在过去几年， agent-based 模型已经开发出来，能够复制交易场景的许多特征，如一组 stylised facts 和统计。但是，对 simulator 的准确填充仍然是一个开放的挑战。在这项工作中，我们开发了一种新的方法来填充市场 simulator，利用最新的深度学习技术， Specifically using neural density estimators 和 embedding networks。我们示示了我们的方法可以正确地标识高概率参数集，无需手动选择或Weight  ensemble of stylised facts。<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Certification-of-Distributional-Individual-Fairness"><a href="#Certification-of-Distributional-Individual-Fairness" class="headerlink" title="Certification of Distributional Individual Fairness"></a>Certification of Distributional Individual Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11911">http://arxiv.org/abs/2311.11911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Wicker, Vihari Piratia, Adrian Weller</li>
<li>for: 本文研究了机器学习算法的形式保证（certificates），以确保个人公平（individual fairness，IF）。</li>
<li>methods: 本文提出了一种新的凸函数方法来快速提供IF保证，并且利用了 quasi-convex 优化技术提供了有效的证明。</li>
<li>results: 本文表明了其方法可以覆盖大型神经网络，并且在实际分布偏移中提供了有效的IF保证。<details>
<summary>Abstract</summary>
Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify distributional individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>> socially responsible deployment of machine learning algorithms 需要提供正式的公平 garanties。在这项工作中，我们研究正式的 garanties，即证书， для神经网络的个体公平（IF）。我们开始于引入一种新的凸函数approximation of IF 约束，这些约束可以快速减少提供本地个体公平的计算成本。我们指出先前的方法受到global IF认证的限制，因此只能适用于几十个隐藏神经元的模型，这限制了它们的实际影响。我们提议使用分布式公平认证，以确保神经网络对于给定的 empirical distribution 和所有在 $\gamma $- Wasserstein 球中的分布都有保证的公平预测。通过 quasi-convex 优化技术，我们提供了新的和有效的分布式公平认证 bounds，并证明我们的方法可以 certificates 和规范化神经网络，这些神经网络比先前的工作中考虑的神经网络要多少orders of magnitude larger。此外，我们研究了实际的分布转移和发现我们的 bound 是一种可扩展、实用和有Sound的公平 garanties sources。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Surface-to-Air-Missile-Engagement-Zone-Prediction-Using-Simulation-and-Machine-Learning"><a href="#Real-Time-Surface-to-Air-Missile-Engagement-Zone-Prediction-Using-Simulation-and-Machine-Learning" class="headerlink" title="Real-Time Surface-to-Air Missile Engagement Zone Prediction Using Simulation and Machine Learning"></a>Real-Time Surface-to-Air Missile Engagement Zone Prediction Using Simulation and Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11905">http://arxiv.org/abs/2311.11905</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jpadantas/sam-ez">https://github.com/jpadantas/sam-ez</a></li>
<li>paper_authors: Joao P. A. Dantas, Diego Geraldo, Felipe L. L. Medeiros, Marcos R. O. A. Maximo, Takashi Yoneyama</li>
<li>for: 这个研究旨在提高现代空中防御系统中的地面到空间导弹（SAM）的效果，特别是考虑到对抗目标的可用空间域（Engagement Zone，EZ）。</li>
<li>methods: 本研究提出了一种结合机器学习技术的方法，通过训练有监督的机器学习算法来准确预测SAM EZ。</li>
<li>results: 研究发现，这种方法可以快速预测SAM EZ，并提供现场实时的洞察，从而提高SAM系统的性能。<details>
<summary>Abstract</summary>
Surface-to-Air Missiles (SAMs) are crucial in modern air defense systems. A critical aspect of their effectiveness is the Engagement Zone (EZ), the spatial region within which a SAM can effectively engage and neutralize a target. Notably, the EZ is intrinsically related to the missile's maximum range; it defines the furthest distance at which a missile can intercept a target. The accurate computation of this EZ is essential but challenging due to the dynamic and complex factors involved, which often lead to high computational costs and extended processing times when using conventional simulation methods. In light of these challenges, our study investigates the potential of machine learning techniques, proposing an approach that integrates machine learning with a custom-designed simulation tool to train supervised algorithms. We leverage a comprehensive dataset of pre-computed SAM EZ simulations, enabling our model to accurately predict the SAM EZ for new input parameters. It accelerates SAM EZ simulations, enhances air defense strategic planning, and provides real-time insights, improving SAM system performance. The study also includes a comparative analysis of machine learning algorithms, illuminating their capabilities and performance metrics and suggesting areas for future research, highlighting the transformative potential of machine learning in SAM EZ simulations.
</details>
<details>
<summary>摘要</summary>
现代空中防御系统中，地面对空导弹（SAM）是关键性的。SAM的作战区域（EZ）是指导弹可以有效地侦测和 нейтрализу攻击目标的空间区域。需要注意的是，EZ与导弹的最大范围直接相关，即导弹可以在这个范围内 intercept攻击目标。正确计算EZ是非常重要但也是非常困难的，因为存在许多动态和复杂的因素，这会导致高计算成本和延长的处理时间。为了解决这些挑战，我们的研究探讨了机器学习技术的潜在作用，并提出了一种机器学习与自定义的 simulations 工具集成的方法。我们利用了一个全面的SAM EZ simulations数据集，使得我们的模型可以准确地预测新的输入参数下的SAM EZ。这有助于加速SAM EZ simulations，提高空 defense 战略规划，并提供实时的洞察，从而提高SAM系统性能。研究还包括机器学习算法的比较分析，描述了这些算法的能力和性能指标，并建议了未来研究的方向，强调了机器学习在SAM EZ simulations中的转型性。
</details></li>
</ul>
<hr>
<h2 id="Measuring-and-Mitigating-Biases-in-Motor-Insurance-Pricing"><a href="#Measuring-and-Mitigating-Biases-in-Motor-Insurance-Pricing" class="headerlink" title="Measuring and Mitigating Biases in Motor Insurance Pricing"></a>Measuring and Mitigating Biases in Motor Insurance Pricing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11900">http://arxiv.org/abs/2311.11900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mulah Moriah, Franck Vermet, Arthur Charpentier</li>
<li>For: The paper aims to provide a comprehensive set of tools for insurers to adopt fairer pricing strategies in the context of automobile insurance, while ensuring consistency and performance.* Methods: The paper uses a range of statistical methodologies and available data to construct optimal pricing structures that align with the overarching corporate strategy and accommodate market competition.* Results: The study assesses the effectiveness of these tools through practical application in the context of automobile insurance, with a focus on equitable premiums, age-based premium fairness, and the consideration of new dimensions for evaluating fairness, such as the presence of serious illnesses or disabilities.<details>
<summary>Abstract</summary>
The non-life insurance sector operates within a highly competitive and tightly regulated framework, confronting a pivotal juncture in the formulation of pricing strategies. Insurers are compelled to harness a range of statistical methodologies and available data to construct optimal pricing structures that align with the overarching corporate strategy while accommodating the dynamics of market competition. Given the fundamental societal role played by insurance, premium rates are subject to rigorous scrutiny by regulatory authorities. These rates must conform to principles of transparency, explainability, and ethical considerations. Consequently, the act of pricing transcends mere statistical calculations and carries the weight of strategic and societal factors. These multifaceted concerns may drive insurers to establish equitable premiums, taking into account various variables. For instance, regulations mandate the provision of equitable premiums, considering factors such as policyholder gender or mutualist group dynamics in accordance with respective corporate strategies. Age-based premium fairness is also mandated. In certain insurance domains, variables such as the presence of serious illnesses or disabilities are emerging as new dimensions for evaluating fairness. Regardless of the motivating factor prompting an insurer to adopt fairer pricing strategies for a specific variable, the insurer must possess the capability to define, measure, and ultimately mitigate any ethical biases inherent in its pricing practices while upholding standards of consistency and performance. This study seeks to provide a comprehensive set of tools for these endeavors and assess their effectiveness through practical application in the context of automobile insurance.
</details>
<details>
<summary>摘要</summary>
非人寿保险业在高度竞争和严格管制的框架下运作，面临着决定性的价格策略制定之刻。保险公司需要结合多种统计方法和可用数据，构建最佳的价格结构，以实现公司核心策略的协调，同时满足市场竞争的变化。由于保险在社会中扮演着重要的角色，保险费用受到严格的监管和社会要求。因此，价格的确定不仅仅是统计计算，也受到战略和社会因素的影响。这些多方面的因素可能会导致保险公司采取更公平的费用策略，考虑多个变量。例如，法规要求提供公平的费用政策，考虑因素 such as policyholder gender或mutualist group dynamics，与公司战略相符。年龄基本的费用公平也被规定。在某些保险领域，存在严重疾病或残疾的存在是新的评价公平的维度。无论某保险公司采取哪一种公平价格策略，它必须具备定义、测量和最终缓解任何伦理偏见的能力，并保持一致性和性能标准。这项研究的目的是提供一套全面的工具，并评估其效果在汽车保险上。
</details></li>
</ul>
<hr>
<h2 id="AMES-A-Differentiable-Embedding-Space-Selection-Framework-for-Latent-Graph-Inference"><a href="#AMES-A-Differentiable-Embedding-Space-Selection-Framework-for-Latent-Graph-Inference" class="headerlink" title="AMES: A Differentiable Embedding Space Selection Framework for Latent Graph Inference"></a>AMES: A Differentiable Embedding Space Selection Framework for Latent Graph Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11891">http://arxiv.org/abs/2311.11891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Lu, Haitz Sáez de Ocáriz Borde, Pietro Liò</li>
<li>for: 这 paper 是为了解决数据集中元素之间的 latent graph inference 问题，使得 Graph Neural Networks (GNNs) 可以在点云数据上进行动态学习必要的图结构。</li>
<li>methods: 这 paper 使用了 Attentional Multi-Embedding Selection (AMES) 框架，这是一种可微的方法，通过 backpropagation 来选择最佳的 embedding space，并考虑下游任务。</li>
<li>results:  comparing 五个 benchmark 数据集，这 paper 的方法可以达到与之前方法相当或更高的结果，而且不需要多次实验来确定最佳的 embedding space。 更重要的是，这 paper 还提供了一种可读性技术，可以跟踪不同的 latent graph 的梯度贡献，从而了解这种 attention-based, fully differentiable 方法如何选择适当的 latent space。<details>
<summary>Abstract</summary>
In real-world scenarios, although data entities may possess inherent relationships, the specific graph illustrating their connections might not be directly accessible. Latent graph inference addresses this issue by enabling Graph Neural Networks (GNNs) to operate on point cloud data, dynamically learning the necessary graph structure. These graphs are often derived from a latent embedding space, which can be modeled using Euclidean, hyperbolic, spherical, or product spaces. However, currently, there is no principled differentiable method for determining the optimal embedding space. In this work, we introduce the Attentional Multi-Embedding Selection (AMES) framework, a differentiable method for selecting the best embedding space for latent graph inference through backpropagation, considering a downstream task. Our framework consistently achieves comparable or superior results compared to previous methods for latent graph inference across five benchmark datasets. Importantly, our approach eliminates the need for conducting multiple experiments to identify the optimal embedding space. Furthermore, we explore interpretability techniques that track the gradient contributions of different latent graphs, shedding light on how our attention-based, fully differentiable approach learns to choose the appropriate latent space. In line with previous works, our experiments emphasize the advantages of hyperbolic spaces in enhancing performance. More importantly, our interpretability framework provides a general approach for quantitatively comparing embedding spaces across different tasks based on their contributions, a dimension that has been overlooked in previous literature on latent graph inference.
</details>
<details>
<summary>摘要</summary>
在实际场景中，数据实体可能拥有自然的关系，但具体的关系图可能并不直接可访问。 latent graph inference Addresses this issue by enabling Graph Neural Networks (GNNs) to operate on point cloud data，动态学习必要的关系图结构。这些图通常来自于 latent embedding space，可以是欧几何、卷积、球形或产品空间。然而，目前没有原理性的分解ifferentiable方法来确定最佳 embedding space。在这种工作中，我们引入 Attentional Multi-Embedding Selection (AMES) 框架，一种可分解的方法，通过反射传播来选择最佳 embedding space  для latent graph inference，考虑到下游任务。我们的框架在五个 benchmark 数据集上 consistently  achievable  comparable or superior results compared to previous methods for latent graph inference。关键是，我们的方法消除了需要进行多次实验来确定最佳 embedding space 的需求。此外，我们还 explore interpretability techniques ，跟踪不同的 latent graph 的梯度贡献，探讨我们的注意力基于、完全分解的方法如何选择合适的 latent space。与前一些工作一样，我们的实验强调了使用卷积空间的优点，并且我们的可解释框架提供了一种通用的方法来比较不同任务中 embedding space 的贡献，这一维度在 previous literature 中被忽略了。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Neural-Networks-for-Tiny-Machine-Learning-A-Comprehensive-Review"><a href="#Efficient-Neural-Networks-for-Tiny-Machine-Learning-A-Comprehensive-Review" class="headerlink" title="Efficient Neural Networks for Tiny Machine Learning: A Comprehensive Review"></a>Efficient Neural Networks for Tiny Machine Learning: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11883">http://arxiv.org/abs/2311.11883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minh Tri Lê, Pierre Wolinski, Julyan Arbel</li>
<li>for: 本评论文章提供了关于微型机器学习（TinyML）应用中高效神经网络和深度学习模型的报道和分析。</li>
<li>methods: 本文使用的方法包括模型压缩、量化和低级分解，以优化神经网络体系结构，以适应资源受限的MCU上的应用。</li>
<li>results: 本文总结了现有的深度学习模型在MCU上的部署技术，包括模型剪辑、硬件加速和算法-架构协同设计等方法，以提高模型在MCU上的高效部署。<details>
<summary>Abstract</summary>
The field of Tiny Machine Learning (TinyML) has gained significant attention due to its potential to enable intelligent applications on resource-constrained devices. This review provides an in-depth analysis of the advancements in efficient neural networks and the deployment of deep learning models on ultra-low power microcontrollers (MCUs) for TinyML applications. It begins by introducing neural networks and discussing their architectures and resource requirements. It then explores MEMS-based applications on ultra-low power MCUs, highlighting their potential for enabling TinyML on resource-constrained devices. The core of the review centres on efficient neural networks for TinyML. It covers techniques such as model compression, quantization, and low-rank factorization, which optimize neural network architectures for minimal resource utilization on MCUs. The paper then delves into the deployment of deep learning models on ultra-low power MCUs, addressing challenges such as limited computational capabilities and memory resources. Techniques like model pruning, hardware acceleration, and algorithm-architecture co-design are discussed as strategies to enable efficient deployment. Lastly, the review provides an overview of current limitations in the field, including the trade-off between model complexity and resource constraints. Overall, this review paper presents a comprehensive analysis of efficient neural networks and deployment strategies for TinyML on ultra-low-power MCUs. It identifies future research directions for unlocking the full potential of TinyML applications on resource-constrained devices.
</details>
<details>
<summary>摘要</summary>
随着智能应用的普及，迫切需要实现智能应用在资源有限的设备上。这篇评论文章提供了关于高效神经网络和深度学习模型在超低功耗微控制器（MCU）上的投入和部署的深入分析。文章首先介绍神经网络，并讨论其建构和资源需求。然后，文章探讨了基于MEMS技术的应用在超低功耗MCU上，并强调它们在资源有限的设备上启用TinyML的潜力。文章的核心部分是高效神经网络的优化，包括模型压缩、量化和低级因数分解等技术，以最小化MCU上神经网络的资源使用。文章还详细介绍了深度学习模型的部署在超低功耗MCU上，包括计算能力和存储器资源的限制。文章提出了多种策略，如模型剪辑、硬件加速和算法-架构协同设计，以实现高效的部署。最后，文章提供了当前领域的限制，包括模型复杂度和资源约束之间的贸易OFF。总的来说，这篇评论文章提供了关于TinyML在超低功耗MCU上的深入分析，并提出了未来研究方向，以推动TinyML应用在资源有限的设备上的全面发展。
</details></li>
</ul>
<hr>
<h2 id="Forward-Gradients-for-Data-Driven-CFD-Wall-Modeling"><a href="#Forward-Gradients-for-Data-Driven-CFD-Wall-Modeling" class="headerlink" title="Forward Gradients for Data-Driven CFD Wall Modeling"></a>Forward Gradients for Data-Driven CFD Wall Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11876">http://arxiv.org/abs/2311.11876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Hückelheim, Tadbhagya Kumar, Krishnan Raghavan, Pinaki Pal</li>
<li>for: 用于增强CFD simulate wall-bounded flow 精度和效率。</li>
<li>methods: 使用机器学习和数据驱动方法，减少CFD计算成本和存储占用。</li>
<li>results: 实现了一种不需要分离前向和反向扫描的梯度计算方法，可以更高效地训练增Resolution wall模型，提高CFD simulate 精度。<details>
<summary>Abstract</summary>
Computational Fluid Dynamics (CFD) is used in the design and optimization of gas turbines and many other industrial/ scientific applications. However, the practical use is often limited by the high computational cost, and the accurate resolution of near-wall flow is a significant contributor to this cost. Machine learning (ML) and other data-driven methods can complement existing wall models. Nevertheless, training these models is bottlenecked by the large computational effort and memory footprint demanded by back-propagation. Recent work has presented alternatives for computing gradients of neural networks where a separate forward and backward sweep is not needed and storage of intermediate results between sweeps is not required because an unbiased estimator for the gradient is computed in a single forward sweep. In this paper, we discuss the application of this approach for training a subgrid wall model that could potentially be used as a surrogate in wall-bounded flow CFD simulations to reduce the computational overhead while preserving predictive accuracy.
</details>
<details>
<summary>摘要</summary>
计算流体动力学（CFD）在设计和优化液压机和其他工业/科学应用中广泛使用。然而，实际使用受到计算成本的限制，而近墙流动的准确解决也是一大 contribuutor。机器学习（ML）和其他数据驱动方法可以补充现有墙模型。然而，训练这些模型受到计算努力和存储空间的限制，因为back-propagation需要大量的计算努力和存储空间。最近的工作已经提出了不需要分离前进和返回扫描的方法来计算神经网络的梯度。在这篇文章中，我们讨论了使用这种方法来训练一个子网格墙模型，以便在墙 bounded 流动 CFD 模拟中减少计算成本而保持预测精度。
</details></li>
</ul>
<hr>
<h2 id="Training-robust-and-generalizable-quantum-models"><a href="#Training-robust-and-generalizable-quantum-models" class="headerlink" title="Training robust and generalizable quantum models"></a>Training robust and generalizable quantum models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11871">http://arxiv.org/abs/2311.11871</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/daniel-fink-de/training-robust-and-generalizable-quantum-models">https://github.com/daniel-fink-de/training-robust-and-generalizable-quantum-models</a></li>
<li>paper_authors: Julian Berberich, Daniel Fink, Daniel Pranjić, Christian Tutschku, Christian Holm</li>
<li>for: 这个论文研究了量子机器学习模型的抗攻击性和通用性。</li>
<li>methods: 论文使用了Liψitz bounds来研究量子机器学习模型的抗攻击性和通用性。</li>
<li>results: 研究发现，可调编码可以系统地提高量子机器学习模型的抗攻击性和通用性，而固定编码则无法通过调整参数来改善这两个性能指标。此外，论文还提出了一种做出量子机器学习模型更加 robust和通用的实际策略。<details>
<summary>Abstract</summary>
Adversarial robustness and generalization are both crucial properties of reliable machine learning models. In this paper, we study these properties in the context of quantum machine learning based on Lipschitz bounds. We derive tailored, parameter-dependent Lipschitz bounds for quantum models with trainable encoding, showing that the norm of the data encoding has a crucial impact on the robustness against perturbations in the input data. Further, we derive a bound on the generalization error which explicitly depends on the parameters of the data encoding. Our theoretical findings give rise to a practical strategy for training robust and generalizable quantum models by regularizing the Lipschitz bound in the cost. Further, we show that, for fixed and non-trainable encodings as frequently employed in quantum machine learning, the Lipschitz bound cannot be influenced by tuning the parameters. Thus, trainable encodings are crucial for systematically adapting robustness and generalization during training. With numerical results, we demonstrate that, indeed, Lipschitz bound regularization leads to substantially more robust and generalizable quantum models.
</details>
<details>
<summary>摘要</summary>
机器学习模型的可靠性和抗抗击性都是非常重要的性能指标。在这篇论文中，我们在量子机器学习的上下文中研究了这两个性能指标，基于 lipschitz  bound。我们 derivated 特定的参数 dependent lipschitz bound，表明数据编码的 норahlength对于输入数据的变化具有关键性的影响。此外，我们还 derivated 一个参数dependent的泛化误差 bound，显示了数据编码参数对模型的泛化性能有直接的影响。我们的理论发现给出了一种实用的训练稳定和泛化的量子模型策略，通过规范 lipschitz bound 的成本来实现。此外，我们还证明了 fix 和 non-trainable 编码被常用于量子机器学习中的情况下， lipschitz bound 无法通过调整参数来改变。因此，可调编码是对系统地改进了robustness和泛化性的关键。通过数据示例，我们证明了 lipschitz bound 规范确实导致了比较稳定和泛化的量子模型。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-complete-intersection-Calabi-Yau-manifolds"><a href="#Deep-learning-complete-intersection-Calabi-Yau-manifolds" class="headerlink" title="Deep learning complete intersection Calabi-Yau manifolds"></a>Deep learning complete intersection Calabi-Yau manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11847">http://arxiv.org/abs/2311.11847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harold Erbin, Riccardo Finotello</li>
<li>for: 理解如何使用机器学习处理代数拓扑数据，特别是完全交叉Calabi-Yau（CICY）3-和4-维结构。</li>
<li>methods: 本文讨论了方法学方面和数据分析方面，然后介绍了神经网络架构。</li>
<li>results: 文章描述了当前预测黑格数的状态艺术，包括在低黑格数预测中进行推断，以及在高黑格数预测中进行推断。此外，文章还描述了在低黑格数预测中进行推断的新结果。<details>
<summary>Abstract</summary>
We review advancements in deep learning techniques for complete intersection Calabi-Yau (CICY) 3- and 4-folds, with the aim of understanding better how to handle algebraic topological data with machine learning. We first discuss methodological aspects and data analysis, before describing neural networks architectures. Then, we describe the state-of-the art accuracy in predicting Hodge numbers. We include new results on extrapolating predictions from low to high Hodge numbers, and conversely.
</details>
<details>
<summary>摘要</summary>
我团队正在审查深度学习技术在完全交叉Calabi-Yau（CICY）3-4维上的进步，以更好地理解如何使用机器学习处理代数 topological 数据。我们首先讨论了方法学方面和数据分析，然后介绍神经网络架构。接着，我们介绍了目前领域中最佳准确率预测Hodge 数。我们还报道了在低到高Hodge数的预测推断中的新结果，以及相反的情况。Note that "完全交叉Calabi-Yau" (CICY) is a specific type of algebraic variety, and "Hodge 数" (Hodge numbers) are a way of measuring the geometry of the variety.
</details></li>
</ul>
<hr>
<h2 id="High-Probability-Guarantees-for-Random-Reshuffling"><a href="#High-Probability-Guarantees-for-Random-Reshuffling" class="headerlink" title="High Probability Guarantees for Random Reshuffling"></a>High Probability Guarantees for Random Reshuffling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11841">http://arxiv.org/abs/2311.11841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengxu Yu, Xiao Li</li>
<li>for: 这个论文主要研究了 Stochastic Gradient Method with Random Reshuffling（SGD-RR）在缓冲非对称优化问题上的应用。SGD-RR在实际中广泛应用，尤其是在神经网络训练中。</li>
<li>methods: 本论文首先研究了 SGD-RR 的采样过程中的集中性性质，并提出了一个新的高probability 样本复杂性保证，使得 gradient （无需期望）在 $\varepsilon$ 下降。这个复杂性保证与最好的现有的均衡式样本复杂性保证相当，只是增加了一个对数性 término。此外，我们还利用我们 derivated 的高probability 下降性和积分误差 bound，提出了一个简单可计算的停止 criterion（denoted as $\mathsf{RR}$-$\mathsf{sc}$），这个 criterion 能 garantúe 在一定数量的迭代后，返回一个满足 $\varepsilon$ 的迭代。此外，我们还提出了一个 perturbed random reshuffling method（ $\mathsf{p}$-$\mathsf{RR}$），该方法在Stationary Point附近加入了一个随机干扰过程。我们证明了 $\mathsf{p}$-$\mathsf{RR}$ 可以高效地逃脱精度下降点并返回第二阶 stationary point，无需对 stochastic gradient error 做任何 sub-Gaussian 尾部假设。</li>
<li>results: 本论文通过数学实验证明了其理论发现。在神经网络训练中，SGD-RR 可以高效地逃脱精度下降点并返回第二阶 stationary point，而不是假设 sub-Gaussian 尾部。<details>
<summary>Abstract</summary>
We consider the stochastic gradient method with random reshuffling ($\mathsf{RR}$) for tackling smooth nonconvex optimization problems. $\mathsf{RR}$ finds broad applications in practice, notably in training neural networks. In this work, we first investigate the concentration property of $\mathsf{RR}$'s sampling procedure and establish a new high probability sample complexity guarantee for driving the gradient (without expectation) below $\varepsilon$, which effectively characterizes the efficiency of a single $\mathsf{RR}$ execution. Our derived complexity matches the best existing in-expectation one up to a logarithmic term while imposing no additional assumptions nor changing $\mathsf{RR}$'s updating rule. Furthermore, by leveraging our derived high probability descent property and bound on the stochastic error, we propose a simple and computable stopping criterion for $\mathsf{RR}$ (denoted as $\mathsf{RR}$-$\mathsf{sc}$). This criterion is guaranteed to be triggered after a finite number of iterations, and then $\mathsf{RR}$-$\mathsf{sc}$ returns an iterate with its gradient below $\varepsilon$ with high probability. Moreover, building on the proposed stopping criterion, we design a perturbed random reshuffling method ($\mathsf{p}$-$\mathsf{RR}$) that involves an additional randomized perturbation procedure near stationary points. We derive that $\mathsf{p}$-$\mathsf{RR}$ provably escapes strict saddle points and efficiently returns a second-order stationary point with high probability, without making any sub-Gaussian tail-type assumptions on the stochastic gradient errors. Finally, we conduct numerical experiments on neural network training to support our theoretical findings.
</details>
<details>
<summary>摘要</summary>
我们考虑使用测度 gradient 方法（$\mathsf{RR}$）来解决缓和非凸优化问题。 $\mathsf{RR}$ 在实践中获得了广泛的应用，特别是在训练神经网络中。在这个工作中，我们首先研究 $\mathsf{RR}$ 的抽掣程序中的集中性性质，然后建立一个新的高概率抽掣次数保证，将梯度（不对期望）下降至 $\varepsilon$ 以下，这个结果有效地描述了 $\mathsf{RR}$ 的效率。我们的 derive 的复杂度与最佳的对 expectation 的复杂度几乎相同，但不需要额外的假设，也不需要更改 $\mathsf{RR}$ 的更新规则。此外，我们还利用我们 derive 的高概率下降性和测度错误的上限，提出了一个简单可计算的停止条件（denoted as $\mathsf{RR}$-$\mathsf{sc}$）。这个条件会在一定的回归次数之后触发，并且返回一个梯度下降至 $\varepsilon$ 以下的回归点，且高概率上发生。此外，我们还提出了一个受 perturbed random reshuffling 方法（$\mathsf{p}$-$\mathsf{RR}$），这个方法具有在站点点发生时额外添加一些随机干扰程序的特点。我们证明了 $\mathsf{p}$-$\mathsf{RR}$ 可以干扰紧缩点，并高概率地返回一个二阶 stationary point。在这个过程中，我们不需要假设测度错误具有子高斯分布的特性。最后，我们在神经网络训练中进行了数值实验，以支持我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Zero-redundancy-distributed-learning-with-differential-privacy"><a href="#Zero-redundancy-distributed-learning-with-differential-privacy" class="headerlink" title="Zero redundancy distributed learning with differential privacy"></a>Zero redundancy distributed learning with differential privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11822">http://arxiv.org/abs/2311.11822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqi Bu, Justin Chiu, Ruixuan Liu, Sheng Zha, George Karypis</li>
<li>for: 这篇论文的目的是发展一个可以应用于分布式深度学习中的隐私保护技术，以便在训练大型深度学习模型时能够保护用户的隐私。</li>
<li>methods: 本论文使用了 Zero Redundancy Optimizer (ZeRO) 来实现分布式深度学习，并且将其与隐私保护技术整合，以便在训练大型深度学习模型时能够维护用户的隐私。</li>
<li>results: 本论文的结果显示，DP-ZeRO 能够实现与标准 ZeRO 相同的计算和通信效率，并且能够训练更大的模型，例如 GPT-100B。此外，DP-ZeRO 还能够支持混合精度训练。<details>
<summary>Abstract</summary>
Deep learning using large models have achieved great success in a wide range of domains. However, training these models on billions of parameters is very challenging in terms of the training speed, memory cost, and communication efficiency, especially under the privacy-preserving regime with differential privacy (DP). On the one hand, DP optimization has comparable efficiency to the standard non-private optimization on a single GPU, but on multiple GPUs, existing DP distributed learning (such as pipeline parallel) has suffered from significantly worse efficiency. On the other hand, the Zero Redundancy Optimizer (ZeRO) is a state-of-the-art solution to the standard distributed learning, exhibiting excellent training efficiency on large models, but to work compatibly with DP is technically complicated. In this work, we develop a new systematic solution, DP-ZeRO, (I) to scale up the trainable DP model size, e.g. to GPT-100B, (II) to obtain the same computation and communication efficiency as the standard ZeRO, and (III) to enable mixed-precision DP training. Our DP-ZeRO, like the standard ZeRO, has the potential to train models with arbitrary size and is evaluated on the world's largest DP models in terms of the number of trainable parameters.
</details>
<details>
<summary>摘要</summary>
深度学习使用大型模型已经在各种领域取得了很大的成功。然而，在 Billions of 参数上进行深度学习训练是具有很大的挑战，特别是在遵循隐私保护（DP）的情况下。一方面，DP 优化的效率相对于标准不隐私的优化在单个 GPU 上具有相似的效率，但在多个 GPU 上，现有的 DP 分布式学习（如管道并行）表现得更加糟糕。另一方面，零重复优化器（ZeRO）是当前顶尖的分布式学习解决方案，在大型模型上显示出了极佳的训练效率，但与 DP 兼容需要技术上的努力。在这种情况下，我们开发了一种新的系统性解决方案——DP-ZeRO，以下是该系统的三大目标：1. 扩展可训练DP模型的大小，例如GPT-100B。2. 与标准ZeRO的计算和通信效率相同。3. 实现混合精度DP训练。我们的 DP-ZeRO 同样可以训练任意大小的模型，并在世界上最大的 DP 模型上进行评估。
</details></li>
</ul>
<hr>
<h2 id="LogLead-–-Fast-and-Integrated-Log-Loader-Enhancer-and-Anomaly-Detector"><a href="#LogLead-–-Fast-and-Integrated-Log-Loader-Enhancer-and-Anomaly-Detector" class="headerlink" title="LogLead – Fast and Integrated Log Loader, Enhancer, and Anomaly Detector"></a>LogLead – Fast and Integrated Log Loader, Enhancer, and Anomaly Detector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11809">http://arxiv.org/abs/2311.11809</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/evotestops/loglead">https://github.com/evotestops/loglead</a></li>
<li>paper_authors: Mika Mäntylä, Yuqing Wang, Jesse Nyyssölä</li>
<li>for: 本文介绍了一种名为LogLead的日志分析工具，用于高效地处理日志数据。</li>
<li>methods: LogLead结合了三个基本的日志处理步骤：加载、增强和异常检测。它利用了Polars高速数据Frame库。文章提供了7个加载器和多种增强器，包括3个解析器（Drain、Spell、LenMa），Bert嵌入式创建和其他日志表示技术。LogLead集成了5种supervised和4种Unsupervised机器学习算法，用于异常检测。</li>
<li>results: 文章表明，使用LogLead将日志从原始文件转换为数据帧，比过去的解决方案快得多（大于10倍）。同时，文章还证明了将日志消息准确化异步传递给LogLead可以提高Draind parsing速度（大约2倍）。最后，文章还对HDFS日志进行了简短的测试，结果表明，日志表示技术 beyond bag-of-words 提供的好处很有限。<details>
<summary>Abstract</summary>
This paper introduces LogLead, a tool designed for efficient log analysis. LogLead combines three essential steps in log processing: loading, enhancing, and anomaly detection. The tool leverages Polars, a high-speed DataFrame library. We currently have 7 Loaders out of which 4 is for public data sets (HDFS, Hadoop, BGL, and Thunderbird). We have multiple enhancers with three parsers (Drain, Spell, LenMa), Bert embedding creation and other log representation techniques like bag-of-words. LogLead integrates to 5 supervised and 4 unsupervised machine learning algorithms for anomaly detection from SKLearn. By integrating diverse datasets, log representation methods and anomaly detectors, LogLead facilitates comprehensive benchmarking in log analysis research. We demonstrate that log loading from raw file to dataframe is over 10x faster with LogLead is compared to past solutions. We demonstrate roughly 2x improvement in Drain parsing speed by off-loading log message normalization to LogLead. We demonstrate a brief benchmarking on HDFS suggesting that log representations beyond bag-of-words provide limited benefits. Screencast demonstrating the tool: https://youtu.be/8stdbtTfJVo
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Operator-Learning-for-Continuous-Spatial-Temporal-Model-with-A-Hybrid-Optimization-Scheme"><a href="#Operator-Learning-for-Continuous-Spatial-Temporal-Model-with-A-Hybrid-Optimization-Scheme" class="headerlink" title="Operator Learning for Continuous Spatial-Temporal Model with A Hybrid Optimization Scheme"></a>Operator Learning for Continuous Spatial-Temporal Model with A Hybrid Optimization Scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11798">http://arxiv.org/abs/2311.11798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanqi Chen, Jin-Long Wu</li>
<li>for: 这篇论文是用来模拟复杂动态系统的空间-时间模型的。</li>
<li>methods: 该模型使用了最近的运算学习进步，并使用了不断空间和时间的数据驱动模型。</li>
<li>results: 该模型能够保持空间和时间分辨率的不变性，并且可以通过短时间序列数据进行稳定长期 simulate。此外，该模型还可以通过混合短时间和长时间数据进行优化，以更好地预测长期统计。<details>
<summary>Abstract</summary>
Partial differential equations are often used in the spatial-temporal modeling of complex dynamical systems in many engineering applications. In this work, we build on the recent progress of operator learning and present a data-driven modeling framework that is continuous in both space and time. A key feature of the proposed model is the resolution-invariance with respect to both spatial and temporal discretizations. To improve the long-term performance of the calibrated model, we further propose a hybrid optimization scheme that leverages both gradient-based and derivative-free optimization methods and efficiently trains on both short-term time series and long-term statistics. We investigate the performance of the spatial-temporal continuous learning framework with three numerical examples, including the viscous Burgers' equation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation. The results confirm the resolution-invariance of the proposed modeling framework and also demonstrate stable long-term simulations with only short-term time series data. In addition, we show that the proposed model can better predict long-term statistics via the hybrid optimization scheme with a combined use of short-term and long-term data.
</details>
<details>
<summary>摘要</summary>
《partial differential equations在空间-时间模型中的应用》中，我们会使用最近的运算学进步，提出一种数据驱动的模型框架，该框架在空间和时间上是连续的。这个提案的一个重要特点是对空间和时间分辨率的不变性。为了提高模型的长期性能，我们进一步提议一种混合优化方案，该方案利用了梯度优化和无梯度优化方法，并高效地在短期时间序列和长期统计数据上训练。我们通过三个数字例子，包括粘滞布尔gers方程、奈尔-斯托克方程和库拉摩-西瓦希诺斯基方程，证明了提案的模型框架的不变性，并表明了只使用短期时间序列数据进行训练可以实现稳定的长期 simulate。此外，我们还证明了我们的模型可以更好地预测长期统计信息，通过混合优化方案并使用短期和长期数据进行训练。
</details></li>
</ul>
<hr>
<h2 id="Approximate-Linear-Programming-and-Decentralized-Policy-Improvement-in-Cooperative-Multi-agent-Markov-Decision-Processes"><a href="#Approximate-Linear-Programming-and-Decentralized-Policy-Improvement-in-Cooperative-Multi-agent-Markov-Decision-Processes" class="headerlink" title="Approximate Linear Programming and Decentralized Policy Improvement in Cooperative Multi-agent Markov Decision Processes"></a>Approximate Linear Programming and Decentralized Policy Improvement in Cooperative Multi-agent Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11789">http://arxiv.org/abs/2311.11789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lakshmi Mandal, Chandrashekar Lakshminarayanan, Shalabh Bhatnagar</li>
<li>for: 本文研究了一种多智能机器人（agent）协同解决Markov决策过程（MDP），其中所有agent都知道系统模型。</li>
<li>methods: 我们使用了分布式政策改进算法，其中每个agent假设其他agent的决策已经固定，然后改进自己的决策。我们还使用了 Approximate Linear Programming（ALP）计算近似价值函数。</li>
<li>results: 我们提供了对协同多智能机器人Finite和无限远景折扣MDP的近似政策迭代算法的理论保证，并在一些数学示例中证明了我们的算法的性能。<details>
<summary>Abstract</summary>
In this work, we consider a `cooperative' multi-agent Markov decision process (MDP) involving m greater than 1 agents, where all agents are aware of the system model. At each decision epoch, all the m agents cooperatively select actions in order to maximize a common long-term objective. Since the number of actions grows exponentially in the number of agents, policy improvement is computationally expensive. Recent works have proposed using decentralized policy improvement in which each agent assumes that the decisions of the other agents are fixed and it improves its decisions unilaterally. Yet, in these works, exact values are computed. In our work, for cooperative multi-agent finite and infinite horizon discounted MDPs, we propose suitable approximate policy iteration algorithms, wherein we use approximate linear programming to compute the approximate value function and use decentralized policy improvement. Thus our algorithms can handle both large number of states as well as multiple agents. We provide theoretical guarantees for our algorithms and also demonstrate the performance of our algorithms on some numerical examples.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们考虑了一个“合作”多代理Markov决策过程（MDP），其中有多于1个代理参与，所有代理都知道系统模型。在每个决策瞬间，所有的m代理合作选择动作，以最大化共同长期目标。由于行动数量在代理数量增加时 exponentiates，策略改进 computationally expensive。 recent works have proposed using decentralized policy improvement, in which each agent assumes that the decisions of the other agents are fixed and it improves its decisions unilaterally. However, in these works, exact values are computed. In our work, for cooperative multi-agent finite and infinite horizon discounted MDPs, we propose suitable approximate policy iteration algorithms, wherein we use approximate linear programming to compute the approximate value function and use decentralized policy improvement. Therefore, our algorithms can handle both large number of states and multiple agents. We provide theoretical guarantees for our algorithms and also demonstrate the performance of our algorithms on some numerical examples.
</details></li>
</ul>
<hr>
<h2 id="Masked-Autoencoders-Are-Robust-Neural-Architecture-Search-Learners"><a href="#Masked-Autoencoders-Are-Robust-Neural-Architecture-Search-Learners" class="headerlink" title="Masked Autoencoders Are Robust Neural Architecture Search Learners"></a>Masked Autoencoders Are Robust Neural Architecture Search Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12086">http://arxiv.org/abs/2311.12086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiming Hu, Xiangxiang Chu, Bo Zhang</li>
<li>for: 提高Neural Architecture Search（NAS）的效率和可靠性，减少或完全消除需要标注数据的使用。</li>
<li>methods: 基于Masked Autoencoders（MAE）的方法，替换supervised learning目标函数，使用图像重建任务来进行搜索过程，不需要标注数据，同时保持性能和通用能力。</li>
<li>results: 通过对不同的搜索空间和数据集进行广泛的实验，证明提posed方法的有效性和可靠性，比基eline方法更高。<details>
<summary>Abstract</summary>
Neural Architecture Search (NAS) currently relies heavily on labeled data, which is both expensive and time-consuming to acquire. In this paper, we propose a novel NAS framework based on Masked Autoencoders (MAE) that eliminates the need for labeled data during the search process. By replacing the supervised learning objective with an image reconstruction task, our approach enables the robust discovery of network architectures without compromising performance and generalization ability. Additionally, we address the problem of performance collapse encountered in the widely-used Differentiable Architecture Search (DARTS) method in the unsupervised paradigm by introducing a multi-scale decoder. Through extensive experiments conducted on various search spaces and datasets, we demonstrate the effectiveness and robustness of the proposed method, providing empirical evidence of its superiority over baseline approaches.
</details>
<details>
<summary>摘要</summary>
Currently, Neural Architecture Search (NAS) heavily relies on labeled data, which is both expensive and time-consuming to acquire. In this paper, we propose a novel NAS framework based on Masked Autoencoders (MAE) that eliminates the need for labeled data during the search process. By replacing the supervised learning objective with an image reconstruction task, our approach enables the robust discovery of network architectures without compromising performance and generalization ability. Additionally, we address the problem of performance collapse encountered in the widely-used Differentiable Architecture Search (DARTS) method in the unsupervised paradigm by introducing a multi-scale decoder. Through extensive experiments conducted on various search spaces and datasets, we demonstrate the effectiveness and robustness of the proposed method, providing empirical evidence of its superiority over baseline approaches.Here's the translation in Traditional Chinese:目前，Neural Architecture Search (NAS) 对于标签数据的依赖是相当高，这些数据不仅成本高，也需要耗费很长的时间来取得。在这篇文章中，我们提出了一个基于Masked Autoencoders (MAE)的 NAS 框架，这个框架不需要标签数据进行搜寻过程。我们通过将超级vised learning 目标取代为图像重建任务，使我们的方法可以在搜寻过程中获得无标签数据的Robust 发现网络架构。此外，我们解决了 Differentiable Architecture Search (DARTS) 方法在无supervised 情况下的性能崩溃问题，通过引入多尺度解oder。通过对不同的搜寻空间和数据集进行广泛的实验，我们证明了我们的方法的有效性和Robustness，提供了实践证据，与基eline方法相比，我们的方法具有superiority。
</details></li>
</ul>
<hr>
<h2 id="MUVO-A-Multimodal-Generative-World-Model-for-Autonomous-Driving-with-Geometric-Representations"><a href="#MUVO-A-Multimodal-Generative-World-Model-for-Autonomous-Driving-with-Geometric-Representations" class="headerlink" title="MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations"></a>MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11762">http://arxiv.org/abs/2311.11762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Bogdoll, Yitian Yang, J. Marius Zöllner</li>
<li>for: 提高自动驾驶系统的理解能力，增强系统的决策能力。</li>
<li>methods: 使用原始相机和激光数据学习感知数据不受感知器件的多模态世界模型，以便直接用于下游任务，如规划。</li>
<li>results: 实现多模态未来预测，并证明我们的几何表示提高了相机图像和激光点云预测质量。<details>
<summary>Abstract</summary>
Learning unsupervised world models for autonomous driving has the potential to improve the reasoning capabilities of today's systems dramatically. However, most work neglects the physical attributes of the world and focuses on sensor data alone. We propose MUVO, a MUltimodal World Model with Geometric VOxel Representations to address this challenge. We utilize raw camera and lidar data to learn a sensor-agnostic geometric representation of the world, which can directly be used by downstream tasks, such as planning. We demonstrate multimodal future predictions and show that our geometric representation improves the prediction quality of both camera images and lidar point clouds.
</details>
<details>
<summary>摘要</summary>
学习无监控世界模型可以帮助自动驾驶系统的理解能力提高很多。然而，大多数工作忽略了世界的物理属性，而专注于感知数据alone。我们提议MUVO，一个多Modal World Model with Geometric VOxel Representations来解决这个挑战。我们使用原始的相机和激光数据来学习无关感知的几何表示世界，这可以直接用于下游任务，如规划。我们展示了多模态未来预测，并证明我们的几何表示提高了相机图像和激光点云预测质量。
</details></li>
</ul>
<hr>
<h2 id="Revealing-behavioral-impact-on-mobility-prediction-networks-through-causal-interventions"><a href="#Revealing-behavioral-impact-on-mobility-prediction-networks-through-causal-interventions" class="headerlink" title="Revealing behavioral impact on mobility prediction networks through causal interventions"></a>Revealing behavioral impact on mobility prediction networks through causal interventions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11749">http://arxiv.org/abs/2311.11749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ye Hong, Yanan Xin, Simon Dirmeier, Fernando Perez-Cruz, Martin Raubal</li>
<li>for: 这个研究旨在研究 Deep neural networks 在 mobilit 预测任务中的解释性问题，尤其是如何各种 mobilit 行为因素影响这些神经网络的预测结果。</li>
<li>methods: 我们在这个研究中提出了一种 causal intervention 框架，用于评估不同 mobilit 行为因素对神经网络的影响。我们使用个人 mobilit 模型生成Synthetic location visit sequences，并通过控制数据生成过程来控制行为动力学。我们使用 mobilit 度量来评估 intervened location sequences，并输入这些位置序列到已经训练好的网络中进行分析性能变化。</li>
<li>results: 我们的结果表明可以生成具有不同 mobilit 行为特征的location sequences，并且可以在不同的 spatial 和 temporal 环境下进行模拟。这些变化导致神经网络的预测性能发生变化，并且揭示了关键 mobilit 行为因素，包括location transition 的顺序模式、探索新位置的倾向和个人和人口层次的位置选择偏好。这些发现对实际应用中的 mobilit 预测网络具有重要价值，而 causal inference 框架可以提高神经网络在 mobilit 应用中的解释性和可靠性。<details>
<summary>Abstract</summary>
Deep neural networks are increasingly utilized in mobility prediction tasks, yet their intricate internal workings pose challenges for interpretability, especially in comprehending how various aspects of mobility behavior affect predictions. In this study, we introduce a causal intervention framework to assess the impact of mobility-related factors on neural networks designed for next location prediction -- a task focusing on predicting the immediate next location of an individual. To achieve this, we employ individual mobility models to generate synthetic location visit sequences and control behavior dynamics by intervening in their data generation process. We evaluate the interventional location sequences using mobility metrics and input them into well-trained networks to analyze performance variations. The results demonstrate the effectiveness in producing location sequences with distinct mobility behaviors, thus facilitating the simulation of diverse spatial and temporal changes. These changes result in performance fluctuations in next location prediction networks, revealing impacts of critical mobility behavior factors, including sequential patterns in location transitions, proclivity for exploring new locations, and preferences in location choices at population and individual levels. The gained insights hold significant value for the real-world application of mobility prediction networks, and the framework is expected to promote the use of causal inference for enhancing the interpretability and robustness of neural networks in mobility applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Leveraging-Uncertainty-Estimates-To-Improve-Classifier-Performance"><a href="#Leveraging-Uncertainty-Estimates-To-Improve-Classifier-Performance" class="headerlink" title="Leveraging Uncertainty Estimates To Improve Classifier Performance"></a>Leveraging Uncertainty Estimates To Improve Classifier Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11723">http://arxiv.org/abs/2311.11723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gundeep Arora, Srujana Merugu, Anoop Saladi, Rajeev Rastogi</li>
<li>for: 该论文主要探讨了如何基于模型分数和不确定性选择决策边界，以提高模型的准确率和回归率。</li>
<li>methods: 该论文提出了一种基于动态计划和iso随机函数的算法，用于选择决策边界，并进行了理论分析和实验验证。</li>
<li>results: 该论文的实验结果表明，使用模型分数和不确定性可以提高模型的准确率和回归率，并且在三个实际 dataset 上实现了25%-40%的提升。<details>
<summary>Abstract</summary>
Binary classification involves predicting the label of an instance based on whether the model score for the positive class exceeds a threshold chosen based on the application requirements (e.g., maximizing recall for a precision bound). However, model scores are often not aligned with the true positivity rate. This is especially true when the training involves a differential sampling across classes or there is distributional drift between train and test settings. In this paper, we provide theoretical analysis and empirical evidence of the dependence of model score estimation bias on both uncertainty and score itself. Further, we formulate the decision boundary selection in terms of both model score and uncertainty, prove that it is NP-hard, and present algorithms based on dynamic programming and isotonic regression. Evaluation of the proposed algorithms on three real-world datasets yield 25%-40% gain in recall at high precision bounds over the traditional approach of using model score alone, highlighting the benefits of leveraging uncertainty.
</details>
<details>
<summary>摘要</summary>
二分类分类涉及判断实例标签是否根据模型分数超过选择的阈值（例如，以最大准确率为约束）。但模型分数与实际正确率不一致，尤其是在类别采样不同或测试环境中存在分布漂移情况下。本文提供了对模型分数估计偏差的理论分析和实验证据，并证明模型分数和不确定性的决策边界选择是NP困难的。基于动态计划和iso逻辑回归，我们提出了一些算法，并评估这些算法在三个实际数据集上，获得25%-40%的准确率提升，证明了通过利用不确定性来提高二分类分类的效果。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Power-of-Self-Attention-for-Shipping-Cost-Prediction-The-Rate-Card-Transformer"><a href="#Unveiling-the-Power-of-Self-Attention-for-Shipping-Cost-Prediction-The-Rate-Card-Transformer" class="headerlink" title="Unveiling the Power of Self-Attention for Shipping Cost Prediction: The Rate Card Transformer"></a>Unveiling the Power of Self-Attention for Shipping Cost Prediction: The Rate Card Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11694">http://arxiv.org/abs/2311.11694</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucidrains/tab-transformer-pytorch">https://github.com/lucidrains/tab-transformer-pytorch</a></li>
<li>paper_authors: P Aditya Sreekar, Sahil Verma, Varun Madhavan, Abhishek Persad<br>for: 这个研究是为了提高亚马逊在销售过程中的财务决策，具体来说是减少邮费估算错误的影响。methods: 这个研究使用了一种新的架构called Rate Card Transformer (RCT)，它使用自注意力来编码包裹信息，包括包裹属性、交通公司信息和路径规划。RCT可以编码一个变量列表，从而更好地捕捉包裹信息。results: 研究结果显示，使用RCT进行邮费估算可以减少错误率28.82%，并且超过了现有的转换器基本模型FTTransformer的性能。此外，RCT还可以改善树状模型的性能。<details>
<summary>Abstract</summary>
Amazon ships billions of packages to its customers annually within the United States. Shipping cost of these packages are used on the day of shipping (day 0) to estimate profitability of sales. Downstream systems utilize these days 0 profitability estimates to make financial decisions, such as pricing strategies and delisting loss-making products. However, obtaining accurate shipping cost estimates on day 0 is complex for reasons like delay in carrier invoicing or fixed cost components getting recorded at monthly cadence. Inaccurate shipping cost estimates can lead to bad decision, such as pricing items too low or high, or promoting the wrong product to the customers. Current solutions for estimating shipping costs on day 0 rely on tree-based models that require extensive manual engineering efforts. In this study, we propose a novel architecture called the Rate Card Transformer (RCT) that uses self-attention to encode all package shipping information such as package attributes, carrier information and route plan. Unlike other transformer-based tabular models, RCT has the ability to encode a variable list of one-to-many relations of a shipment, allowing it to capture more information about a shipment. For example, RCT can encode properties of all products in a package. Our results demonstrate that cost predictions made by the RCT have 28.82% less error compared to tree-based GBDT model. Moreover, the RCT outperforms the state-of-the-art transformer-based tabular model, FTTransformer, by 6.08%. We also illustrate that the RCT learns a generalized manifold of the rate card that can improve the performance of tree-based models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Tiny-VBF-Resource-Efficient-Vision-Transformer-based-Lightweight-Beamformer-for-Ultrasound-Single-Angle-Plane-Wave-Imaging"><a href="#Tiny-VBF-Resource-Efficient-Vision-Transformer-based-Lightweight-Beamformer-for-Ultrasound-Single-Angle-Plane-Wave-Imaging" class="headerlink" title="Tiny-VBF: Resource-Efficient Vision Transformer based Lightweight Beamformer for Ultrasound Single-Angle Plane Wave Imaging"></a>Tiny-VBF: Resource-Efficient Vision Transformer based Lightweight Beamformer for Ultrasound Single-Angle Plane Wave Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12082">http://arxiv.org/abs/2311.12082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdul Rahoof, Vivek Chaturvedi, Mahesh Raveendranatha Panicker, Muhammad Shafique</li>
<li>for: 加速非实时 beamforming 算法在ultrasound 成像中使用深度学习架构，以提高图像质量和速度。</li>
<li>methods: 提出了一种基于视transformer的 tiny beamformer（Tiny-VBF）模型，使用 raw radio-frequency 通道数据，并使用 hybrid 量化 schemes 加速 FPGA 实现。</li>
<li>results: Tiny-VBF 模型在尺寸 368 x 128 的帧中需要0.34 GOPs&#x2F;帧，与 state-of-the-art 深度学习模型相比下降8%的对比度和5%和33%的轴向和横向分辨率提升。同时，与 conventional Delay-and-Sum 扩展器相比，Tiny-VBF 模型提供了4.2%的对比度和4%和20%的轴向和横向分辨率提升。<details>
<summary>Abstract</summary>
Accelerating compute intensive non-real-time beam-forming algorithms in ultrasound imaging using deep learning architectures has been gaining momentum in the recent past. Nonetheless, the complexity of the state-of-the-art deep learning techniques poses challenges for deployment on resource-constrained edge devices. In this work, we propose a novel vision transformer based tiny beamformer (Tiny-VBF), which works on the raw radio-frequency channel data acquired through single-angle plane wave insonification. The output of our Tiny-VBF provides fast envelope detection requiring very low frame rate, i.e. 0.34 GOPs/Frame for a frame size of 368 x 128 in comparison to the state-of-the-art deep learning models. It also exhibited an 8% increase in contrast and gains of 5% and 33% in axial and lateral resolution respectively when compared to Tiny-CNN on in-vitro dataset. Additionally, our model showed a 4.2% increase in contrast and gains of 4% and 20% in axial and lateral resolution respectively when compared against conventional Delay-and-Sum (DAS) beamformer. We further propose an accelerator architecture and implement our Tiny-VBF model on a Zynq UltraScale+ MPSoC ZCU104 FPGA using a hybrid quantization scheme with 50% less resource consumption compared to the floating-point implementation, while preserving the image quality.
</details>
<details>
<summary>摘要</summary>
快速计算非实时射频成形算法在超音波成像中使用深度学习架构受到过去几年的推动。然而，现状的深度学习技术的复杂性使得部署在有限资源的边缘设备上具有挑战。在这项工作中，我们提出了一种基于视transformer的小型射频成形器（Tiny-VBF），它在单角扫描电磁信号中处理原始的射频通道数据。Tiny-VBF的输出具有快速的幅度检测，需要非常低的帧率，即0.34 GOPs/帧，与现状的深度学习模型相比。此外，我们的模型在射频成像 dataset 上显示了8%的对比度提高和 axial 和 lateral 分辨率的提高，分别为5%和33%，相比之下Tiny-CNN模型。此外，我们的模型还与传统的延迟和总和（DAS）成形器进行了比较，显示了4.2%的对比度提高和 axial 和 lateral 分辨率的提高，分别为4%和20%。最后，我们还提出了一种加速器架构，并在 Zynq UltraScale+ MPSoC ZCU104 FPGA 上实现了一种混合量化方案，相比于浮点实现，消耗资源量减少了50%，保持图像质量。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Control-Engineer’s-Craft-with-Neural-Networks"><a href="#Unraveling-the-Control-Engineer’s-Craft-with-Neural-Networks" class="headerlink" title="Unraveling the Control Engineer’s Craft with Neural Networks"></a>Unraveling the Control Engineer’s Craft with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11644">http://arxiv.org/abs/2311.11644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Braghadeesh Lakshminarayanan, Federico Dettù, Cristian R. Rojas, Simone Formentin</li>
<li>for: 这篇论文旨在提出一种数据驱动的控制器调试方法，使用数字模拟器生成输入输出数据，并使用神经网络学习模型来调节控制器参数。</li>
<li>methods: 该方法使用数字模拟器生成输入输出数据，然后使用神经网络学习模型来学习控制器调节规则，从而实际替换控制工程师。</li>
<li>results: 该方法通过数字模拟器生成的输入输出数据，使用神经网络学习模型来学习控制器调节规则，可以快速和高精度地调节控制器参数。<details>
<summary>Abstract</summary>
Many industrial processes require suitable controllers to meet their performance requirements. More often, a sophisticated digital twin is available, which is a highly complex model that is a virtual representation of a given physical process, whose parameters may not be properly tuned to capture the variations in the physical process. In this paper, we present a sim2real, direct data-driven controller tuning approach, where the digital twin is used to generate input-output data and suitable controllers for several perturbations in its parameters. State-of-the art neural-network architectures are then used to learn the controller tuning rule that maps input-output data onto the controller parameters, based on artificially generated data from perturbed versions of the digital twin. In this way, as far as we are aware, we tackle for the first time the problem of re-calibrating the controller by meta-learning the tuning rule directly from data, thus practically replacing the control engineer with a machine learning model. The benefits of this methodology are illustrated via numerical simulations for several choices of neural-network architectures.
</details>
<details>
<summary>摘要</summary>
很多工业过程需要适合的控制器来满足其性能要求。更常见的情况是，存在一个非常复杂的数字孪生，即物理过程的虚拟表示，其参数可能没有正确地调整到物理过程的变化。在这篇论文中，我们提出了一种 sim2real、直接数据驱动控制器调整方法，其中使用数字孪生生成输入输出数据和适合的控制器，并使用现代神经网络架构学习控制器调整规则，以将输入输出数据映射到控制器参数。这种方法，至我们所知，是第一次通过直接从数据中学习控制器调整规则，实际地将控制工程师 replaced by 机器学习模型。我们通过数值仿真对几种神经网络架构进行了评估，并证明了本方法的优点。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-LLM-Priors-into-Tabular-Learners"><a href="#Incorporating-LLM-Priors-into-Tabular-Learners" class="headerlink" title="Incorporating LLM Priors into Tabular Learners"></a>Incorporating LLM Priors into Tabular Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11628">http://arxiv.org/abs/2311.11628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Zhu, Siniša Stanivuk, Andrija Petrovic, Mladen Nikolic, Pietro Lio</li>
<li>for: 本研究旨在 Addressing Large Language Models (LLMs) 的挑战，如数据序列化敏感性和偏见，并结合传统的表格数据分类技术。</li>
<li>methods: 本研究提出了两种使用 LLMs 进行排序 categorical 变量和生成对 continous 变量和目标的 Priors 的策略，以提高几个shot enario 下的性能。具体来说，我们引入了 MonotonicLR，一种使用非线性增长函数将 ordinal 映射到 cardinal 的方法，保持 LLM 决定的顺序。</li>
<li>results: 对比基eline 模型，我们的方法在低数据场景下表现出优于其他方法，特别是在几个shot enario 下。此外，我们的方法仍然可以保持可读性。<details>
<summary>Abstract</summary>
We present a method to integrate Large Language Models (LLMs) and traditional tabular data classification techniques, addressing LLMs challenges like data serialization sensitivity and biases. We introduce two strategies utilizing LLMs for ranking categorical variables and generating priors on correlations between continuous variables and targets, enhancing performance in few-shot scenarios. We focus on Logistic Regression, introducing MonotonicLR that employs a non-linear monotonic function for mapping ordinals to cardinals while preserving LLM-determined orders. Validation against baseline models reveals the superior performance of our approach, especially in low-data scenarios, while remaining interpretable.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，将大型语言模型（LLM）与传统的表格数据分类技术集成，解决 LLM 的敏感性和偏见问题。我们提出了两种使用 LLM  для排序普通分类和生成对目标变量和连续变量之间的相关性的先验，提高了几个shot场景中的性能。我们专注于LOGISTIC REGRESSION，提出了一种非线性凝leans函数，将ORDINALS映射到Cardinals，保持 LLM 决定的顺序。验证基eline模型表明，我们的方法在低数据场景中表现出色，而且保持可解释性。
</details></li>
</ul>
<hr>
<h2 id="Testing-multivariate-normality-by-testing-independence"><a href="#Testing-multivariate-normality-by-testing-independence" class="headerlink" title="Testing multivariate normality by testing independence"></a>Testing multivariate normality by testing independence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11575">http://arxiv.org/abs/2311.11575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Povilas Daniušis</li>
<li>for: 本文提出了一种简单的多变量正态性测试，基于加铁-伯恩斯坦的特征化，可以通过利用现有的统计独立性测试来进行。</li>
<li>methods: 本文使用了现有的统计独立性测试来实现这种测试，并进行了Empirical investigation，发现在高维数据中，提出的方法可能更高效。</li>
<li>results: 本文的Empirical investigation表明，在高维数据中，提出的方法可能更高效。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We propose a simple multivariate normality test based on Kac-Bernstein's characterization, which can be conducted by utilising existing statistical independence tests for sums and differences of data samples. We also perform its empirical investigation, which reveals that for high-dimensional data, the proposed approach may be more efficient than the alternative ones. The accompanying code repository is provided at \url{https://shorturl.at/rtuy5}.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单的多变量正态性测试，基于加邦-bernstein的特征化，可以通过利用现有的统计独立性测试来进行。我们还进行了实验研究，发现对高维数据来说，我们的方法可能比其他方法更高效。代码存储库可以在以下链接中找到：https://shorturl.at/rtuy5。Note that "Simplified Chinese" is a translation of the text into Chinese using a simpler vocabulary and grammar, which is more commonly used in mainland China. If you need a translation into "Traditional Chinese" (used in Taiwan and other parts of the world), I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Genetic-Algorithm-Deep-GA-Approach-for-High-Dimensional-Nonlinear-Parabolic-Partial-Differential-Equations"><a href="#A-Deep-Genetic-Algorithm-Deep-GA-Approach-for-High-Dimensional-Nonlinear-Parabolic-Partial-Differential-Equations" class="headerlink" title="A Deep-Genetic Algorithm (Deep-GA) Approach for High-Dimensional Nonlinear Parabolic Partial Differential Equations"></a>A Deep-Genetic Algorithm (Deep-GA) Approach for High-Dimensional Nonlinear Parabolic Partial Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11558">http://arxiv.org/abs/2311.11558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Endah Rokhmati Merdika Putri, Muhammad Luthfi Shahab, Mohammad Iqbal, Imam Mukhlash, Amirul Hakam, Lutfi Mardianto, Hadi Susanto</li>
<li>for: 增加深度学习算法解决高维度偏微分方程的性能，提高解决速度和精度。</li>
<li>methods:  embedding a genetic algorithm (GA) into the solver to optimize the initial guess selection, accelerating the convergence of the nonlinear PDEs on a broader interval.</li>
<li>results: 比深度BSDE更快速地解决非线性偏微分方程，并且保持相同的准确性。<details>
<summary>Abstract</summary>
We propose a new method, called a deep-genetic algorithm (deep-GA), to accelerate the performance of the so-called deep-BSDE method, which is a deep learning algorithm to solve high dimensional partial differential equations through their corresponding backward stochastic differential equations (BSDEs). Recognizing the sensitivity of the solver to the initial guess selection, we embed a genetic algorithm (GA) into the solver to optimize the selection. We aim to achieve faster convergence for the nonlinear PDEs on a broader interval than deep-BSDE. Our proposed method is applied to two nonlinear parabolic PDEs, i.e., the Black-Scholes (BS) equation with default risk and the Hamilton-Jacobi-Bellman (HJB) equation. We compare the results of our method with those of the deep-BSDE and show that our method provides comparable accuracy with significantly improved computational efficiency.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新方法，称为深度进化算法（深度-GA），以加速深度学习算法解决高维partial differential equations（PDEs）的方法，即通过其相应的反向随机 differential equations（BSDEs）。我们认为选择初始假设对解算法的敏感性，因此我们将进化算法（GA）embed到解算法中来优化选择。我们的目标是在更广泛的区间上比deep-BSDE更快地 converges。我们的提posed方法应用于两种非线性parabolic PDEs，即黑-股（BS）方程和汉密尔-雅各布-贝尔（HJB）方程。我们比较了我们的方法与深度-BSDE的结果，并显示了我们的方法可以提供相当于准确性的同时significantly improve计算效率。
</details></li>
</ul>
<hr>
<h2 id="Fast-Controllable-Diffusion-Models-for-Undersampled-MRI-Reconstruction"><a href="#Fast-Controllable-Diffusion-Models-for-Undersampled-MRI-Reconstruction" class="headerlink" title="Fast Controllable Diffusion Models for Undersampled MRI Reconstruction"></a>Fast Controllable Diffusion Models for Undersampled MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12078">http://arxiv.org/abs/2311.12078</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ppn-paper/ppn">https://github.com/ppn-paper/ppn</a></li>
<li>paper_authors: Wei Jiang, Zhuang Xiong, Feng Liu, Nan Ye, Hongfu Sun</li>
<li>for: 用于增强和加速控制可生成的扩散模型，以提高MRI下抽取重建的效果和速度。</li>
<li>methods: 使用Predictor-Projector-Noisor（PPN）算法，该算法可以快速生成高质量的MR图像，并且可以适应不同的MRI获取参数。</li>
<li>results: PPN算法可以生成高准确性的MR图像，并且比其他控制可生成方法更快。此外，PPN算法可以适应不同的MRI获取参数，使其在临床应用中更实用。<details>
<summary>Abstract</summary>
Supervised deep learning methods have shown promise in Magnetic Resonance Imaging (MRI) undersampling reconstruction, but their requirement for paired data limits their generalizability to the diverse MRI acquisition parameters. Recently, unsupervised controllable generative diffusion models have been applied to MRI undersampling reconstruction, without paired data or model retraining for different MRI acquisitions. However, diffusion models are generally slow in sampling and state-of-the-art acceleration techniques can lead to sub-optimal results when directly applied to the controllable generation process. This study introduces a new algorithm called Predictor-Projector-Noisor (PPN), which enhances and accelerates controllable generation of diffusion models for MRI undersampling reconstruction. Our results demonstrate that PPN produces high-fidelity MR images that conform to undersampled k-space measurements with significantly shorter reconstruction time than other controllable sampling methods. In addition, the unsupervised PPN accelerated diffusion models are adaptable to different MRI acquisition parameters, making them more practical for clinical use than supervised learning techniques.
</details>
<details>
<summary>摘要</summary>
超visisted深度学习方法在磁共振成像（MRI）下采样重建中表现出了承诺，但它们的对偶数据要求限制了它们在不同MRI获取参数下的普适性。最近，不supervised控制可生成扩散模型已经应用到MRI下采样重建中，无需对数据对或模型重新训练。然而，扩散模型通常在采样中慢，现状的加速技术可能导致直接应用到控制可生成过程中的优化结果不佳。这项研究介绍了一种新的算法called Predictor-Projector-Noisor（PPN），该算法可以加速和提高控制可生成的扩散模型在MRI下采样重建中。我们的结果表明，PPN可以生成高质量的MRI图像，符合下采样的k空间测量结果，并且重建时间比其他控制可生成方法更短。此外，不supervised PPN加速的扩散模型可以适应不同的MRI获取参数，使其更适用于临床应用。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Variation-in-Subpopulation-Susceptibility-to-Poisoning-Attacks"><a href="#Understanding-Variation-in-Subpopulation-Susceptibility-to-Poisoning-Attacks" class="headerlink" title="Understanding Variation in Subpopulation Susceptibility to Poisoning Attacks"></a>Understanding Variation in Subpopulation Susceptibility to Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11544">http://arxiv.org/abs/2311.11544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evan Rose, Fnu Suya, David Evans</li>
<li>for: 本研究探讨了机器学习模型在不同子 популяции上的攻击敏感性，尤其是在攻击者可以控制一部分训练数据点的情况下。</li>
<li>methods: 本研究使用了现有的欺诈攻击方法，并对不同子 популяции进行了实验研究，以探讨攻击效果的不同性。</li>
<li>results: 研究发现，对于较不分化的数据集，攻击者可以通过控制一部分数据点来影响模型的行为，而对于较分化的数据集，攻击效果受到具体的子 популяции属性的影响。此外，研究还发现了一种关键的子 популяción属性，即模型在干净数据集上的损失差异，可以用于评估攻击效果。<details>
<summary>Abstract</summary>
Machine learning is susceptible to poisoning attacks, in which an attacker controls a small fraction of the training data and chooses that data with the goal of inducing some behavior unintended by the model developer in the trained model. We consider a realistic setting in which the adversary with the ability to insert a limited number of data points attempts to control the model's behavior on a specific subpopulation. Inspired by previous observations on disparate effectiveness of random label-flipping attacks on different subpopulations, we investigate the properties that can impact the effectiveness of state-of-the-art poisoning attacks against different subpopulations. For a family of 2-dimensional synthetic datasets, we empirically find that dataset separability plays a dominant role in subpopulation vulnerability for less separable datasets. However, well-separated datasets exhibit more dependence on individual subpopulation properties. We further discover that a crucial subpopulation property is captured by the difference in loss on the clean dataset between the clean model and a target model that misclassifies the subpopulation, and a subpopulation is much easier to attack if the loss difference is small. This property also generalizes to high-dimensional benchmark datasets. For the Adult benchmark dataset, we show that we can find semantically-meaningful subpopulation properties that are related to the susceptibilities of a selected group of subpopulations. The results in this paper are accompanied by a fully interactive web-based visualization of subpopulation poisoning attacks found at https://uvasrg.github.io/visualizing-poisoning
</details>
<details>
<summary>摘要</summary>
机器学习容易受到毒素攻击，攻击者可以控制一小部分训练数据，并选择这些数据以达到模型开发者未INTENDED的行为。我们考虑了一个现实主义的设置，在该设置下，敌对者可以插入有限数量的数据点来控制模型的行为。继承 previous observations on random label-flipping attacks 的结果，我们研究了不同subpopulation的攻击效果。对于一家2维的 sintethic 数据集，我们发现了 dataset 分离性对 subpopulation 的感itivity具有主导作用。然而，well-separated 数据集更多地受到个体 subpopulation 的特性的影响。我们还发现，一个重要的 subpopulation 特性是 clean dataset 上模型和target模型之间的损失差，如果损失差小，那么这个 subpopulation 容易受到攻击。这个特性也适用于高维 benchmark 数据集。对 Adult 数据集，我们表明可以找到 semantically-meaningful subpopulation 特性，这些特性与模型中的某些 subpopulation 的感itivity相关。本文的结果通过 <https://uvasrg.github.io/visualizing-poisoning> 提供了一个完整的交互式网页式visualization of subpopulation poisoning attacks。
</details></li>
</ul>
<hr>
<h2 id="An-NMF-Based-Building-Block-for-Interpretable-Neural-Networks-With-Continual-Learning"><a href="#An-NMF-Based-Building-Block-for-Interpretable-Neural-Networks-With-Continual-Learning" class="headerlink" title="An NMF-Based Building Block for Interpretable Neural Networks With Continual Learning"></a>An NMF-Based Building Block for Interpretable Neural Networks With Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11485">http://arxiv.org/abs/2311.11485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian K. Vogel</li>
<li>for: 提高预测性能和解释性的平衡</li>
<li>methods: 使用基于NMF的Predictive Factorized Coupling（PFC）块，结合超vised neural network训练方法，以提高预测性能 while retaining NMF的解释性</li>
<li>results: 在小 dataset上测试，PFC块可以与MLP具有相同的预测性能，同时提供更好的解释性，并在不同的场景中（如 continual learning、非i.i.d.数据训练和知识 removalfter training）表现出优异的效果。<details>
<summary>Abstract</summary>
Existing learning methods often struggle to balance interpretability and predictive performance. While models like nearest neighbors and non-negative matrix factorization (NMF) offer high interpretability, their predictive performance on supervised learning tasks is often limited. In contrast, neural networks based on the multi-layer perceptron (MLP) support the modular construction of expressive architectures and tend to have better recognition accuracy but are often regarded as black boxes in terms of interpretability. Our approach aims to strike a better balance between these two aspects through the use of a building block based on NMF that incorporates supervised neural network training methods to achieve high predictive performance while retaining the desirable interpretability properties of NMF. We evaluate our Predictive Factorized Coupling (PFC) block on small datasets and show that it achieves competitive predictive performance with MLPs while also offering improved interpretability. We demonstrate the benefits of this approach in various scenarios, such as continual learning, training on non-i.i.d. data, and knowledge removal after training. Additionally, we show examples of using the PFC block to build more expressive architectures, including a fully-connected residual network as well as a factorized recurrent neural network (RNN) that performs competitively with vanilla RNNs while providing improved interpretability. The PFC block uses an iterative inference algorithm that converges to a fixed point, making it possible to trade off accuracy vs computation after training but also currently preventing its use as a general MLP replacement in some scenarios such as training on very large datasets. We provide source code at https://github.com/bkvogel/pfc
</details>
<details>
<summary>摘要</summary>
现有的学习方法 often 难以平衡解释性和预测性的表现。 nearest neighbors 和非负矩阵因子（NMF）提供了高度的解释性，但是其在指导学习任务上的预测性常常有限。 相比之下，基于多层感知器（MLP）的神经网络支持模块化的建构和表现出了更好的识别精度，但是它们通常被视为黑盒子，即无法解释性。 我们的方法希望能够更好地平衡这两个方面，通过使用基于 NMF 的 Predictive Factorized Coupling（PFC）块来实现高度预测性，同时保留 NMF 的愉悦解释性特点。 我们在小样本上评估了 PFC 块，并显示了它在指导学习任务上与 MLP 相当，同时提供了改善的解释性。 我们在不同的场景下展示了这种方法的优势，包括不间断学习、训练非同一致数据和学习后知识 removals。 此外，我们还展示了使用 PFC 块建立更加表达力的架构，包括完全连接的差异阶段网络和 факторизован的 RNN，这些架构在指导学习任务上表现竞争力强，同时提供了改善的解释性。 PFC 块使用迭代推理算法，其总是向一个固定点收敛，因此可以在训练后交换准确率和计算量，但目前不支持在很大的数据集上进行训练。 我们在 GitHub 上提供了源代码，请参考 <https://github.com/bkvogel/pfc>。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Interpolation-Flows"><a href="#Gaussian-Interpolation-Flows" class="headerlink" title="Gaussian Interpolation Flows"></a>Gaussian Interpolation Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11475">http://arxiv.org/abs/2311.11475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Gao, Jian Huang, Yuling Jiao</li>
<li>for: 这个论文主要研究了基于Gaussian denoising的连续正常化流的建构，以及这些流的理论性质和正则化效果。</li>
<li>methods: 该论文使用了一种统一框架，称为Gaussian interpolation flow，来研究连续正常化流的启发性、存在和一意性、流速场的 lipschitz 连续性和时间反转流map的 lipschitz 连续性等。</li>
<li>results: 该研究发现，Gaussian interpolation flows 具有良好的启发性、存在和一意性、流速场的 lipschitz 连续性和时间反转流map的 lipschitz 连续性等特点，并且可以用于描述一些rich classes of target distributions。此外，该研究还探讨了这些流的稳定性和源分布的扰动。<details>
<summary>Abstract</summary>
Gaussian denoising has emerged as a powerful principle for constructing simulation-free continuous normalizing flows for generative modeling. Despite their empirical successes, theoretical properties of these flows and the regularizing effect of Gaussian denoising have remained largely unexplored. In this work, we aim to address this gap by investigating the well-posedness of simulation-free continuous normalizing flows built on Gaussian denoising. Through a unified framework termed Gaussian interpolation flow, we establish the Lipschitz regularity of the flow velocity field, the existence and uniqueness of the flow, and the Lipschitz continuity of the flow map and the time-reversed flow map for several rich classes of target distributions. This analysis also sheds light on the auto-encoding and cycle-consistency properties of Gaussian interpolation flows. Additionally, we delve into the stability of these flows in source distributions and perturbations of the velocity field, using the quadratic Wasserstein distance as a metric. Our findings offer valuable insights into the learning techniques employed in Gaussian interpolation flows for generative modeling, providing a solid theoretical foundation for end-to-end error analyses of learning GIFs with empirical observations.
</details>
<details>
<summary>摘要</summary>
(Note: Simplified Chinese is a simplified version of Chinese that uses shorter words and sentences, and is often used in informal writing and online communication. The translation above is written in Simplified Chinese, but the original text is in Traditional Chinese.)
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Post-Market-Monitoring-Framework-for-Machine-Learning-based-Medical-Devices-A-case-study"><a href="#Towards-a-Post-Market-Monitoring-Framework-for-Machine-Learning-based-Medical-Devices-A-case-study" class="headerlink" title="Towards a Post-Market Monitoring Framework for Machine Learning-based Medical Devices: A case study"></a>Towards a Post-Market Monitoring Framework for Machine Learning-based Medical Devices: A case study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11463">http://arxiv.org/abs/2311.11463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean Feng, Adarsh Subbaswamy, Alexej Gossmann, Harvineet Singh, Berkman Sahiner, Mi-Ok Kim, Gene Pennello, Nicholas Petrick, Romain Pirracchio, Fan Xia</li>
<li>for: 这个研究的目的是为了制定一种系统atic的监控策略，以确保在临床实践中部署的机器学习（ML）系统的安全性和有效性。</li>
<li>methods: 这篇研究使用了 causal inference 和统计过程控制 等工具，对监控方法进行了定义、评估和比较。</li>
<li>results: 研究发现，选择实际（observational）数据或进行实验性的研究是监控策略的关键决策，但这些决策受到了各种偏见和偏向的影响。<details>
<summary>Abstract</summary>
After a machine learning (ML)-based system is deployed in clinical practice, performance monitoring is important to ensure the safety and effectiveness of the algorithm over time. The goal of this work is to highlight the complexity of designing a monitoring strategy and the need for a systematic framework that compares the multitude of monitoring options. One of the main decisions is choosing between using real-world (observational) versus interventional data. Although the former is the most convenient source of monitoring data, it exhibits well-known biases, such as confounding, selection, and missingness. In fact, when the ML algorithm interacts with its environment, the algorithm itself may be a primary source of bias. On the other hand, a carefully designed interventional study that randomizes individuals can explicitly eliminate such biases, but the ethics, feasibility, and cost of such an approach must be carefully considered. Beyond the decision of the data source, monitoring strategies vary in the performance criteria they track, the interpretability of the test statistics, the strength of their assumptions, and their speed at detecting performance decay. As a first step towards developing a framework that compares the various monitoring options, we consider a case study of an ML-based risk prediction algorithm for postoperative nausea and vomiting (PONV). Bringing together tools from causal inference and statistical process control, we walk through the basic steps of defining candidate monitoring criteria, describing potential sources of bias and the causal model, and specifying and comparing candidate monitoring procedures. We hypothesize that these steps can be applied more generally, as causal inference can address other sources of biases as well.
</details>
<details>
<summary>摘要</summary>
after a machine learning（ML）based system is deployed in clinical practice, performance monitoring is important to ensure the safety and effectiveness of the algorithm over time. The goal of this work is to highlight the complexity of designing a monitoring strategy and the need for a systematic framework that compares the multitude of monitoring options. one of the main decisions is choosing between using real-world（observational）versus interventional data. although the former is the most convenient source of monitoring data, it exhibits well-known biases, such as confounding, selection, and missingness. in fact, when the ML algorithm interacts with its environment, the algorithm itself may be a primary source of bias. on the other hand, a carefully designed interventional study that randomizes individuals can explicitly eliminate such biases, but the ethics, feasibility, and cost of such an approach must be carefully considered. beyond the decision of the data source, monitoring strategies vary in the performance criteria they track, the interpretability of the test statistics, the strength of their assumptions, and their speed at detecting performance decay. as a first step towards developing a framework that compares the various monitoring options, we consider a case study of an ML-based risk prediction algorithm for postoperative nausea and vomiting（PONV）. bringing together tools from causal inference and statistical process control, we walk through the basic steps of defining candidate monitoring criteria, describing potential sources of bias and the causal model, and specifying and comparing candidate monitoring procedures. we hypothesize that these steps can be applied more generally, as causal inference can address other sources of biases as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/20/cs.LG_2023_11_20/" data-id="clpztdnmh00vees88b12e9ems" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/20/eess.IV_2023_11_20/" class="article-date">
  <time datetime="2023-11-20T09:00:00.000Z" itemprop="datePublished">2023-11-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/20/eess.IV_2023_11_20/">eess.IV - 2023-11-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Tubular-Curvature-Filter-Implicit-Pointwise-Curvature-Calculation-Method-for-Tubular-Objects"><a href="#Tubular-Curvature-Filter-Implicit-Pointwise-Curvature-Calculation-Method-for-Tubular-Objects" class="headerlink" title="Tubular Curvature Filter: Implicit Pointwise Curvature Calculation Method for Tubular Objects"></a>Tubular Curvature Filter: Implicit Pointwise Curvature Calculation Method for Tubular Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11931">http://arxiv.org/abs/2311.11931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elifnur Sunger, Beyza Kalkanli, Veysi Yildiz, Tales Imbiriba, Peter Campbell, Deniz Erdogmus</li>
<li>for: 用于计算管状物体的本地弯曲度</li>
<li>methods: 使用方向ional rate of change在Hessian矩阵的eigenvector上进行了本地弯曲度计算</li>
<li>results: 实验结果表明，Tubular Curvature Filter方法可以准确地计算管状物体任何点的本地弯曲度<details>
<summary>Abstract</summary>
Curvature estimation methods are important as they capture salient features for various applications in image processing, especially within medical domains where tortuosity of vascular structures is of significant interest. Existing methods based on centerline or skeleton curvature fail to capture curvature gradients across a rotating tubular structure. This paper presents a Tubular Curvature Filter method that locally calculates the acceleration of bundles of curves that traverse along the tubular object parallel to the centerline. This is achieved by examining the directional rate of change in the eigenvectors of the Hessian matrix of a tubular intensity function in space. This method implicitly calculates the local tubular curvature without the need to explicitly segment the tubular object. Experimental results demonstrate that the Tubular Curvature Filter method provides accurate estimates of local curvature at any point inside tubular structures.
</details>
<details>
<summary>摘要</summary>
CURVATURE 估计方法是重要的，因为它们捕捉了图像处理中的重要特征，特别是医疗领域中血管结构的折叠性是非常重要的。现有基于中心线或skeleton curvature的方法无法捕捉旋转管体结构中的曲线幅度跃变。本文介绍了一种管体曲线滤波器方法，它地方计算管体内部曲线的加速度，通过对管体内部曲线的平行方向进行方向差异率的检查，并通过计算管体内部曲线的HESSIAN矩阵的方向差异来计算本地管体曲线。这种方法不需要显式地分割管体对象，可以准确地估计管体内部任何点的曲线。实验结果表明，管体曲线滤波器方法可以准确地估计管体内部曲线的本地弯曲。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/20/eess.IV_2023_11_20/" data-id="clpztdntw01dnes8841qf6197" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/20/eess.SP_2023_11_20/" class="article-date">
  <time datetime="2023-11-20T08:00:00.000Z" itemprop="datePublished">2023-11-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/20/eess.SP_2023_11_20/">eess.SP - 2023-11-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Tensor-based-Space-Debris-Detection-for-Satellite-Mega-constellations"><a href="#Tensor-based-Space-Debris-Detection-for-Satellite-Mega-constellations" class="headerlink" title="Tensor-based Space Debris Detection for Satellite Mega-constellations"></a>Tensor-based Space Debris Detection for Satellite Mega-constellations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11838">http://arxiv.org/abs/2311.11838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Daoust, Hasan Nayir, Irfan Azam, Antoine Lesage-Landry, Gunes Karabulut Kurt</li>
<li>for: 避免遥感器损坏和残骸的潜在威胁，提高遥感器的安全性。</li>
<li>methods:  integrate sensing and communication techniques to detect space debris, using canonical polyadic (CP) tensor decomposition method to estimate the rank of the tensor that denotes the number of paths including line-of-sight and non-line-of-sight.</li>
<li>results: simulation results show that the proposed tensor-based scheme has higher probability of detection than conventional energy-based detection scheme for space debris detection.<details>
<summary>Abstract</summary>
Thousands of satellites, asteroids, and rocket bodies break, collide, or degrade, resulting in large amounts of space debris in low Earth orbit. The presence of space debris poses a serious threat to satellite mega-constellations and to future space missions. Debris can be avoided if detected within the safety range of a satellite. In this paper, an integrated sensing and communication technique is proposed to detect space debris for satellite mega-constellations. The canonical polyadic (CP) tensor decomposition method is used to estimate the rank of the tensor that denotes the number of paths including line-of-sight and non-line-of-sight by exploiting the sparsity of THz channel with limited scattering. The analysis reveals that the reflected signals of the THz can be utilized for the detection of space debris. The CP decomposition is cast as an optimization problem and solved using the alternating least square (ALS) algorithm. Simulation results show that the probability of detection of the proposed tensor-based scheme is higher than the conventional energy-based detection scheme for the space debris detection.
</details>
<details>
<summary>摘要</summary>
众多卫星、小行星和火箭体坍塌、相撞或衰变，导致低地球轨道中有很多空间垃圾。空间垃圾对卫星巨合群和未来空间任务构成了严重的威胁。如果探测到空间垃圾在卫星的安全范围内，可以避免垃圾的损害。在这篇论文中，我们提出了一种集成探测和通信技术，用于检测卫星巨合群中的空间垃圾。我们使用了多重线性（CP）张量分解方法来估算卫星检测到的信号的rank，通过利用THz频道的稀热扩散来减少检测的复杂性。分析表明，THz信号的反射可以用于检测空间垃圾。CP分解被表示为一个优化问题，并使用了变分最小二乘（ALS）算法来解决。实验结果表明，提议的张量基本方法的检测可能性比传统的能量基本方法高。
</details></li>
</ul>
<hr>
<h2 id="Movable-Antenna-Array-Enabled-Wireless-Communication-with-CoMP-Reception"><a href="#Movable-Antenna-Array-Enabled-Wireless-Communication-with-CoMP-Reception" class="headerlink" title="Movable-Antenna Array-Enabled Wireless Communication with CoMP Reception"></a>Movable-Antenna Array-Enabled Wireless Communication with CoMP Reception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11814">http://arxiv.org/abs/2311.11814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guojie Hu, Qingqing Wu, Jian Ouyang, Kui Xu, Yunlong Cai, Naofal Al-Dhahir</li>
<li>for: 实现高效的无线通信，通过允许移动天线（MA）阵列实现均衡多点接收（CoMP）技术，并且允许多个目标组合共同解读从传送器装备MA阵列的通信讯号。</li>
<li>methods: 使用最大比率组合技术来实现多个目标的共同解读，并且将传送器的天线组合优化为最大化受到信号噪音比。</li>
<li>results: 这种方法可以实现高效的无线通信，并且比过去的参考模型表现出更好的性能。将MA阵列的位置优化可以获得更高的受到信号噪音比，并且可以实现更好的均衡多点接收。<details>
<summary>Abstract</summary>
We consider the movable antenna (MA) array-enabled wireless communication with coordinate multi-point (CoMP) reception, where multiple destinations adopt the maximal ratio combination technique to jointly decode the common message sent from the transmitter equipped with the MA array. Our goal is to maximize the effective received signal-to-noise ratio, by jointly optimizing the transmit beamforming and the positions of the MA array. Although the formulated problem is highly non-convex, we reveal that it is fundamental to maximize the principal eigenvalue of a hermite channel matrix which is a function of the positions of the MA array. The corresponding sub-problem is still non-convex, for which we develop a computationally efficient algorithm. Afterwards, the optimal transmit beamforming is determined with a closed-form solution. In addition, the theoretical performance upper bound is analyzed. Since the MA array brings an additional spatial degree of freedom by flexibly adjusting all antennas' positions, it achieves significant performance gain compared to competitive benchmarks.
</details>
<details>
<summary>摘要</summary>
我们考虑了可移动天线（MA）数组启用的无线通信，使多个目标使用最大比率组合技术同时解码发送者配备MA数组的共同消息。我们的目标是 Maximizing the effective received signal-to-noise ratio，通过同时优化发射扫描和MA数组的位置来实现。虽然问题是非常不对称，但我们发现，最大化 hermite channel 矩阵的主要特征值是 MA 数组位置的函数。相应的子问题仍然是非对称的，我们开发了 computationally efficient 算法。然后，最佳发射扫描是通过关闭式解决方案确定的。此外，我们还分析了理论性能Upper bound。由于 MA 数组通过自由调整所有天线的位置，增加了空间学度的自由度，因此实现了比竞争benchmarks更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Robust-Multidimentional-Chinese-Remainder-Theorem-for-Integer-Vector-Reconstruction"><a href="#Robust-Multidimentional-Chinese-Remainder-Theorem-for-Integer-Vector-Reconstruction" class="headerlink" title="Robust Multidimentional Chinese Remainder Theorem for Integer Vector Reconstruction"></a>Robust Multidimentional Chinese Remainder Theorem for Integer Vector Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11804">http://arxiv.org/abs/2311.11804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Xiao, Haiye Huo, Xiang-Gen Xia</li>
<li>for: 该 paper  targets the problem of robustly reconstructing an integer vector from its erroneous remainders in multidimensional (MD) signal processing.</li>
<li>methods: 该 paper 使用了robust MD Chinese remainder theorem (CRT)，which is recently proposed for a special class of moduli, but with a strict constraint on the moduli. The paper investigates the robust MD-CRT for a general set of moduli and presents a necessary and sufficient condition on the difference between paired remainder errors, as well as a simple sufficient condition on the remainder error bound.</li>
<li>results: 该 paper  presents a closed-form reconstruction algorithm and generalizes the results of the robust MD-CRT from integer vectors&#x2F;matrices to real ones. The paper validates the robust MD-CRT for general moduli through numerical simulations and applies it to MD sinusoidal frequency estimation based on multiple sub-Nyquist samplers.<details>
<summary>Abstract</summary>
The problem of robustly reconstructing an integer vector from its erroneous remainders appears in many applications in the field of multidimensional (MD) signal processing. To address this problem, a robust MD Chinese remainder theorem (CRT) was recently proposed for a special class of moduli, where the remaining integer matrices left-divided by a greatest common left divisor (gcld) of all the moduli are pairwise commutative and coprime. The strict constraint on the moduli limits the usefulness of the robust MD-CRT in practice. In this paper, we investigate the robust MD-CRT for a general set of moduli. We first introduce a necessary and sufficient condition on the difference between paired remainder errors, followed by a simple sufficient condition on the remainder error bound, for the robust MD-CRT for general moduli, where the conditions are associated with (the minimum distances of) these lattices generated by gcld's of paired moduli, and a closed-form reconstruction algorithm is presented. We then generalize the above results of the robust MD-CRT from integer vectors/matrices to real ones. Finally, we validate the robust MD-CRT for general moduli by employing numerical simulations, and apply it to MD sinusoidal frequency estimation based on multiple sub-Nyquist samplers.
</details>
<details>
<summary>摘要</summary>
“多维度（MD）信号处理中，有问题 robustly 从其不准确余值中重建整数 вектор。为解决这问题，一种 robust MD Chinese remainder theorem（CRT）最近被提出，但这问题的紧张限制了实际应用。本文研究robust MD-CRT 的一般模仿，包括对于一般模仿的必要和充分条件、对于不同对象的充分条件、以及一个关于这些体积生成的最小距离的封闭式重建算法。然后，我们将这些结果扩展到实数 вектор/矩阵上。最后，我们透过数字实验验证robust MD-CRT 的可靠性，并将其应用到多 sub-Nyquist 探针中的 MD 谐波频率估计。”
</details></li>
</ul>
<hr>
<h2 id="A-Zero-Forcing-Approach-for-the-RIS-Aided-MIMO-Broadcast-Channel"><a href="#A-Zero-Forcing-Approach-for-the-RIS-Aided-MIMO-Broadcast-Channel" class="headerlink" title="A Zero-Forcing Approach for the RIS-Aided MIMO Broadcast Channel"></a>A Zero-Forcing Approach for the RIS-Aided MIMO Broadcast Channel</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11769">http://arxiv.org/abs/2311.11769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Semmler, Michael Joham, Wolfgang Utschick</li>
<li>for: 这 paper 是为了最大化多用户智能面带有多输入多输出广播通道的含义 спектраль效率 (SE) 的算法。</li>
<li>methods: 这 paper 使用了零做法，包括用户分配，以确保计算独立于智能面元素的数量。特别是， paper 提出了两种算法，利用基站和智能面之间的直线结构来提高 SE 性能。</li>
<li>results:  simulations 表明，这些算法可以与其他线性 precoding 算法相比，具有更高的 SE 性能，但具有较低的复杂性。<details>
<summary>Abstract</summary>
We present efficient algorithms for the sum-spectral efficiency (SE) maximization of the multi-user reconfigurable intelligent surface (RIS)-aided multiple-input multiple-output (MIMO) broadcast channel based on a zero-forcing approach. These methods conduct a user allocation for which the computation is independent of the number of elements at the RIS, that is usually large. Specifically, two algorithms are given that exploit the line-of-sight (LOS) structure between the base station (BS) and the RIS. Simulations show superior SE performance compared to other linear precoding algorithms but with lower complexity.
</details>
<details>
<summary>摘要</summary>
我们提出了高效的算法，用于多用户智能表面受助多输入多输出广播频道上的总spectral efficiency（SE）最大化，基于零强制方法。这些方法在RIS中元素数量约为大的情况下，实现了用户分配，计算独立于RIS元素数量。 Specifically，我们提出了两种算法，利用基站（BS）与RIS之间的直线结构。模拟结果表明，我们的算法在其他线性 precoding 算法比较高，但具有较低的复杂度。
</details></li>
</ul>
<hr>
<h2 id="AIaaS-for-ORAN-based-6G-Networks-Multi-time-scale-slice-resource-management-with-DRL"><a href="#AIaaS-for-ORAN-based-6G-Networks-Multi-time-scale-slice-resource-management-with-DRL" class="headerlink" title="AIaaS for ORAN-based 6G Networks: Multi-time scale slice resource management with DRL"></a>AIaaS for ORAN-based 6G Networks: Multi-time scale slice resource management with DRL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11668">http://arxiv.org/abs/2311.11668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suvidha Mhatre, Ferran Adelantado, Kostas Ramantas, Christos Verikoukis</li>
<li>for: 本研究旨在Addressing slice resource management for 6G networks at different time scales, using an open radio access network (ORAN) architecture and artificial intelligence (AI) techniques.</li>
<li>methods: 提议的解决方案包括在网络边缘使用AI控制两级循环，以实现优化性性能比其他技术。ORAN支持可编程的网络架构，以便实现多级时间尺度的管理。</li>
<li>results: 提议的算法可以分析最大资源利用率，以取得决策在 интер-slice 级别。内 slice 智能代理工作在非实时层次，重新配置资源在不同的slice中。 results 表明，该方法可以提高 eMBB、URLLC 和 mMTC 等slice类型的性能。<details>
<summary>Abstract</summary>
This paper addresses how to handle slice resources for 6G networks at different time scales in an architecture based on an open radio access network (ORAN). The proposed solution includes artificial intelligence (AI) at the edge of the network and applies two control-level loops to obtain optimal performance compared to other techniques. The ORAN facilitates programmable network architectures to support such multi-time scale management using AI approaches. The proposed algorithms analyze the maximum utilization of resources from slice performance to take decisions at the inter-slice level. Inter-slice intelligent agents work at a non-real-time level to reconfigure resources within various slices. Further than meeting the slice requirements, the intra-slice objective must also include the minimization of maximum resource utilization. This enables smart utilization of the resources within each slice without affecting slice performance. Here, each xApp that is an intra-slice agent aims at meeting the optimal QoS of the users, but at the same time, some inter-slice objectives should be included to coordinate intra- and inter-slice agents. This is done without penalizing the main intra-slice objective. All intelligent agents use deep reinforcement learning (DRL) algorithms to meet their objectives. We have presented results for enhanced mobile broadband (eMBB), ultra-reliable low latency (URLLC), and massive machine type communication (mMTC) slice categories.
</details>
<details>
<summary>摘要</summary>
To optimize resource utilization, the proposed algorithms analyze the maximum utilization of resources from a slice performance perspective and make decisions at the inter-slice level. Inter-slice intelligent agents work at a non-real-time level to reconfigure resources within various slices. Additionally, the intra-slice objective should minimize maximum resource utilization to ensure smart resource utilization within each slice without affecting slice performance.Each xApp, or intra-slice agent, aims to meet the optimal quality of service (QoS) of users while coordinating with inter-slice objectives. This is achieved without penalizing the main intra-slice objective. All intelligent agents use deep reinforcement learning (DRL) algorithms to meet their objectives.The paper presents results for enhanced mobile broadband (eMBB), ultra-reliable low latency (URLLC), and massive machine type communication (mMTC) slice categories, demonstrating the effectiveness of the proposed solution.
</details></li>
</ul>
<hr>
<h2 id="RIS-Parametrized-Rich-Scattering-Environments-Physics-Compliant-Models-Channel-Estimation-and-Optimization"><a href="#RIS-Parametrized-Rich-Scattering-Environments-Physics-Compliant-Models-Channel-Estimation-and-Optimization" class="headerlink" title="RIS-Parametrized Rich-Scattering Environments: Physics-Compliant Models, Channel Estimation, and Optimization"></a>RIS-Parametrized Rich-Scattering Environments: Physics-Compliant Models, Channel Estimation, and Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11651">http://arxiv.org/abs/2311.11651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp del Hougne</li>
<li>for: 这个论文旨在探讨如何使用可 Configurable 智能面 (RIS) 来控制复杂的吸收物 scattering 环境，以实现智能无线环境。</li>
<li>methods: 论文使用 physics-compliant 模型来描述 RIS-parametrized 吸收物 scattering 环境，并使用 open-loop 控制来优化 RIS 配置。</li>
<li>results: 论文提出了一种 physics-compliant 模型，可以用来 parametrically 模拟 RIS-parametrized 吸收物 scattering 环境，并且可以在不知道实验环境的情况下优化 RIS 配置。<details>
<summary>Abstract</summary>
The tunability of radio environments with reconfigurable intelligent surfaces (RISs) enables the paradigm of smart radio environments in which wireless system engineers are no longer limited to only controlling the radiated signals but can in addition also optimize the wireless channels. Many practical radio environments include complex scattering objects, especially indoor and factory settings. Multipath propagation therein creates seemingly intractable coupling effects between RIS elements, leading to the following questions: How can a RIS-parametrized rich-scattering environment be modelled in a physics-compliant manner? Can the parameters of such a model be estimated for a specific but unknown experimental environment? And how can the RIS configuration be optimized given a calibrated physics-compliant model? This chapter summarizes the current state of the art in this field, highlighting the recently unlocked potential of frugal physical-model-based open-loop control of RIS-parametrized rich-scattering radio environments.
</details>
<details>
<summary>摘要</summary>
《受控Radio环境中的智能表面（RIS）的可调性，使得无线系统工程师不再只能控制发射的信号，还可以优化无线通道。实际的无线环境中存在复杂的散射物，特别是室内和工厂环境。多路射传输使得RIS元素之间存在许多难以解决的干扰效应，这引出了以下问题：如何在物理相容的方式模拟RIS参数化的 ricH-scattering 环境？可以估算特定而不确定的实验环境中RIS参数的模型吗？如何基于加工物理相容的模型来优化RIS配置？本章简要总结当前领域的最新进展，强调最近解锁了基于减少物理模型的开 loop控制RIS参数化 ricH-scattering 无线环境的潜力。》Note: "ricH-scattering" is a term used to describe the complex scattering behavior of signals in environments with many reflective objects, such as indoor and factory settings.
</details></li>
</ul>
<hr>
<h2 id="High-performance-cVEP-BCI-under-minimal-calibration"><a href="#High-performance-cVEP-BCI-under-minimal-calibration" class="headerlink" title="High-performance cVEP-BCI under minimal calibration"></a>High-performance cVEP-BCI under minimal calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11596">http://arxiv.org/abs/2311.11596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yining Miao, Nanlin Shi, Changxing Huang, Yonghao Song, Xiaogang Chen, Yijun Wang, Xiaorong Gao</li>
<li>for: 提高 code-modulated visual evoked potential-based BCIs（cVEP-BCIs）的高速性和可用性。</li>
<li>methods: 使用快速干扰（WN）的广带干扰，并提出了两种方法：线性模型方法和 пере移学习技术。</li>
<li>results: 实现了最高的信息传输率（ITR）250 bits per minute（bpm），与当前的稳态视觉诱发potential-based BCIs（SSVEP-BCIs）相当。<details>
<summary>Abstract</summary>
The ultimate goal of brain-computer interfaces (BCIs) based on visual modulation paradigms is to achieve high-speed performance without the burden of extensive calibration. Code-modulated visual evoked potential-based BCIs (cVEP-BCIs) modulated by broadband white noise (WN) offer various advantages, including increased communication speed, expanded encoding target capabilities, and enhanced coding flexibility. However, the complexity of the spatial-temporal patterns under broadband stimuli necessitates extensive calibration for effective target identification in cVEP-BCIs. Consequently, the information transfer rate (ITR) of cVEP-BCI under limited calibration usually stays around 100 bits per minute (bpm), significantly lagging behind state-of-the-art steady-state visual evoked potential-based BCIs (SSVEP-BCIs), which achieve rates above 200 bpm. To enhance the performance of cVEP-BCIs with minimal calibration, we devised an efficient calibration stage involving a brief single-target flickering, lasting less than a minute, to extract generalizable spatial-temporal patterns. Leveraging the calibration data, we developed two complementary methods to construct cVEP temporal patterns: the linear modeling method based on the stimulus sequence and the transfer learning techniques using cross-subject data. As a result, we achieved the highest ITR of 250 bpm under a minute of calibration, which has been shown to be comparable to the state-of-the-art SSVEP paradigms. In summary, our work significantly improved the cVEP performance under few-shot learning, which is expected to expand the practicality and usability of cVEP-BCIs.
</details>
<details>
<summary>摘要</summary>
BCIs based on visual modulation paradigms 的最终目标是实现高速性表现，无需耗费庞大的调整。基于宽频白噪（WN）的 cVEP-BCI 具有许多优点，包括提高的交通速率、扩展的编码目标能力和提高的编码flexibility。然而，在宽频刺激下，需要进行广泛的调整以便有效地识别目标。因此， cVEP-BCI 的信息传输率（ITR）在有限的调整下通常在 100 位/分钟（bpm），远远落后于当前的稳态 visual evoked potential（SSVEP）基于 BCIs，它们的 ITR 超过 200 bpm。为了提高 cVEP-BCI 的性能，我们设计了一个高效的快速单目标抖摆calibration阶段，持续时间低于 1 分钟，以EXTRACT GENERALIZABLE SPATIAL-TEMPORAL PATTERNS。通过利用 calibration 数据，我们开发了两种COMPLEMENTARY METHODS TO CONSTRUCT cVEP  temporal patterns：基于刺激序列的直线模型方法和使用交叉主题数据的转移学习技术。因此，我们实现了最高的 ITR 250 bpm ，在有限的 calibration 下，这个成果已经被证明可以与当前 SSVEP 方法相比。总之，我们的工作在几个shot学习中提高了 cVEP 性能，这可能扩大 BCIs 的实用性和可用性。
</details></li>
</ul>
<hr>
<h2 id="Joint-Design-of-ISAC-Waveform-under-PAPR-Constraints"><a href="#Joint-Design-of-ISAC-Waveform-under-PAPR-Constraints" class="headerlink" title="Joint Design of ISAC Waveform under PAPR Constraints"></a>Joint Design of ISAC Waveform under PAPR Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11594">http://arxiv.org/abs/2311.11594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yating Chen, Cai Wen, Yan Huang, Le Liang, Jie Li, Hui Zhang, Wei Hong</li>
<li>for: 本文解决了集成感知通信（ISAC）波形的预编码问题，并提出了一种基于幂函数约束的非对称射频规划（QCQP）方法。</li>
<li>methods: 本文提出了一种基于交叉方向法（ADMM）的高效算法，可以减少多变量的coupling，并提供一个封闭式解决方案。</li>
<li>results: 实验结果表明，提出的双功能波形可以提供良好的通信质量服务（QoS）和感知性能。<details>
<summary>Abstract</summary>
In this paper, we formulate the precoding problem of integrated sensing and communication (ISAC) waveform as a non-convex quadratically constrainted quadratic program (QCQP), in which the weighted sum of communication multi-user interference (MUI) and the gap between dual-use waveform and ideal radar waveform is minimized with peak-to-average power ratio (PAPR) constraints. We propose an efficient algorithm based on alternating direction method of multipliers (ADMM), which is able to decouple multiple variables and provide a closed-form solution for each subproblem. In addition, to improve the sensing performance in both spatial and temporal domains, we propose a new criteria to design the ideal radar waveform, in which the beam pattern is made similar to the ideal one and the integrated sidelobe level of the ambiguity function in each target direction is minimized in the region of interest. The limited memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm is applied to the design of the ideal radar waveform which works as a reference in the design of the dual-function waveform. Numerical results indicate that the designed dual-function waveform is capable of offering good communication quality of service (QoS) and sensing performance.
</details>
<details>
<summary>摘要</summary>
在本文中，我们将 интеграted sensing and communication（ISAC）波形的预处理问题设为非对称quadratically constrained quadratic program（QCQP），其中Weighted sum of communication multi-user interference（MUI）和理想 radar 波形与实际 radar 波形之间的差异是最小化的，同时受限于峰值至平均功率比（PAPR）的约束。我们提出了一种高效的 alternating direction method of multipliers（ADMM）算法，可以分解多个变量并提供每个子问题的关闭式解。此外，为了提高探测性能在空间和时域两个领域，我们提出了一新的标准来设计理想 radar 波形，其中扫描方式与理想波形类似，并在兴趣区域内尽量减少了各个方向的混合副幅水平。我们使用了有限记忆 Broyden-Fletcher-Goldfarb-Shanno（L-BFGS）算法来设计理想 radar 波形，该波形作为双功能波形的参照。numerical results表明，设计的双功能波形能够提供良好的通信质量服务（QoS）和探测性能。
</details></li>
</ul>
<hr>
<h2 id="Asymptotic-CRB-Analysis-of-Random-RIS-Assisted-Large-Scale-Localization-Systems"><a href="#Asymptotic-CRB-Analysis-of-Random-RIS-Assisted-Large-Scale-Localization-Systems" class="headerlink" title="Asymptotic CRB Analysis of Random RIS-Assisted Large-Scale Localization Systems"></a>Asymptotic CRB Analysis of Random RIS-Assisted Large-Scale Localization Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11582">http://arxiv.org/abs/2311.11582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyu Wang, Hongzheng Liu, Rujing Xiong, Fuhai Wang, Robert Caiming Qiu</li>
<li>for: 这 paper 研究了一种随机 RIS 协助的多 Target 本地化系统的性能，其中 RIS 的配置采用随机设置，以避免高复杂度优化。</li>
<li>methods: 我们首先关注了 RIS 元素数量很多的场景，然后 obtaint 了 CRB 的扩展征具下降规律，发现 CRB 在 RIS 维度增加第三或第四阶时下降。然后，我们推广我们的分析至大规模系统，其中包括多个 Target 和感知器。</li>
<li>results: 我们发现，在提posed 随机配置 RIS 系统中， asymptotic 公式可以准确地 approximate  exact CRB。numerical 结果表明，随机配置 RIS 对 CRB 的影响是非常显著的。<details>
<summary>Abstract</summary>
This paper studies the performance of a randomly RIS-assisted multi-target localization system, in which the configurations of the RIS are randomly set to avoid high-complexity optimization. We first focus on the scenario where the number of RIS elements is significantly large, and then obtain the scaling law of Cram\'er-Rao bound (CRB) under certain conditions, which shows that CRB decreases in the third or fourth order as the RIS dimension increases. Second, we extend our analysis to large systems where both the number of targets and sensors is substantial. Under this setting, we explore two common RIS models: the constant module model and the discrete amplitude model, and illustrate how the random RIS configuration impacts the value of CRB. Numerical results demonstrate that asymptotic formulas provide a good approximation to the exact CRB in the proposed randomly configured RIS systems.
</details>
<details>
<summary>摘要</summary>
这篇论文研究了随机RIS协助的多Target定位系统的性能，其中RIS配置随机设置以避免高复杂性优化。我们首先关注了RIS元素数量相对较多的场景，然后获得CRB下降的幂级关系，显示CRB在RIS维度增加的第三或第四阶幂下降。其次，我们扩展我们的分析至大规模系统，其中目标和感知器的数量都是substantial。在这种设置下，我们探讨了两种常见RIS模型：常数模块模型和随机振荡模型，并解释了随机RIS配置对CRB的影响。数值结果表明，幂级公式可以准确地 aproximate exact CRB在我们提议的随机配置RIS系统中。
</details></li>
</ul>
<hr>
<h2 id="On-the-Effective-throughput-of-Shadowed-Beaulieu-Xie-fading-channel"><a href="#On-the-Effective-throughput-of-Shadowed-Beaulieu-Xie-fading-channel" class="headerlink" title="On the Effective throughput of Shadowed Beaulieu-Xie fading channel"></a>On the Effective throughput of Shadowed Beaulieu-Xie fading channel</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11521">http://arxiv.org/abs/2311.11521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manpreet Kaur, Sandeep Kumar, Poonam Yadav, Puspraj Singh Chauhan</li>
<li>for:  investigate data rate performance across various fading scenarios</li>
<li>methods: PDF-based approach, low-SNR and high-SNR approximation</li>
<li>results: effective throughput and impact of system parameters and delay parameter on effective throughput<details>
<summary>Abstract</summary>
Given the imperative for advanced wireless networks in the next generation and the rise of real-time applications within wireless communication, there is a notable focus on investigating data rate performance across various fading scenarios. This research delved into analyzing the effective throughput of the shadowed Beaulieu-Xie (SBX) composite fading channel using the PDF-based approach. To get the simplified relationship between the performance parameter and channel parameters, the low-SNR and the high-SNR approximation of the effective rate are also provided. The proposed formulations are evaluated for different values of system parameters to study their impact on the effective throughput. Also, the impact of the delay parameter on the EC is investigated. Monte-Carlo simulations are used to verify the facticity of the deduced equations.
</details>
<details>
<summary>摘要</summary>
随着下一代无线网络的需求和实时应用在无线通信中的增长，研究数据传输速率在不同的抑噪场景下的性能已经得到了广泛的关注。本研究利用PDF基本方法对shadowed Beaulieu-Xie（SBX）复杂抑噪通道的有效传输率进行分析。为了获得简化关系 между性能参数和通道参数，本研究还提供了低SNR和高SNR的近似值。通过不同系统参数的调整，研究对效果传输率的影响。此外，延迟参数对EC的影响也被调查。使用Monte Carlo仿真来验证推导出的公式。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/20/eess.SP_2023_11_20/" data-id="clpztdnvu01i4es88dq81hsn0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/cs.SD_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T15:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/cs.SD_2023_11_19/">cs.SD - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Encoding-Performance-Data-in-MEI-with-the-Automatic-Music-Performance-Analysis-and-Comparison-Toolkit-AMPACT"><a href="#Encoding-Performance-Data-in-MEI-with-the-Automatic-Music-Performance-Analysis-and-Comparison-Toolkit-AMPACT" class="headerlink" title="Encoding Performance Data in MEI with the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT)"></a>Encoding Performance Data in MEI with the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11363">http://arxiv.org/abs/2311.11363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johanna Devaney, Cecilia Beauchamp</li>
<li>for: 这个论文是用于介绍一种新的MEI编码方法，使用最近添加的\texttt{<extData>}元素来存储表演数据。</li>
<li>methods: 这个论文使用了Automatic Music Performance Analysis and Comparison Toolkit（AMPACT）来提取表演数据，并将其编码为JSON对象，并将其链接到特定的乐谱Note中的\texttt{<extData>}元素。</li>
<li>results: 这个论文使用了一组流行音乐 vocals 来示例出\texttt{<extData>}元素可以编码的范围和AMPACT可以在缺乏完整乐谱的情况下提取表演数据。<details>
<summary>Abstract</summary>
This paper presents a new method of encoding performance data in MEI using the recently added \texttt{<extData>} element. Performance data was extracted using the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT) and encoded as a JSON object within an \texttt{<extData>} element linked to a specific musical note. A set of pop music vocals has was encoded to demonstrate both the range of descriptors that can be encoded in <extData> and how AMPACT can be used for extracting performance data in the absence of a fully specified musical score.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这篇论文提出了一种使用 MEI 中最近添加的 \texttt{<extData>} 元素来编码性能数据的新方法。性能数据由 Automatic Music Performance Analysis and Comparison Toolkit (AMPACT) 提取，并以 JSON 对象形式嵌入在 \texttt{<extData>} 元素中，与特定的音符相关联。为示这种编码方式的应用范围和 AMPACT 如何在缺乏完整的乐谱情况下提取性能数据，一组流行音乐 vocals 被编码。
</details></li>
</ul>
<hr>
<h2 id="M-2-UGen-Multi-modal-Music-Understanding-and-Generation-with-the-Power-of-Large-Language-Models"><a href="#M-2-UGen-Multi-modal-Music-Understanding-and-Generation-with-the-Power-of-Large-Language-Models" class="headerlink" title="M$^{2}$UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models"></a>M$^{2}$UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11255">http://arxiv.org/abs/2311.11255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atin Sakkeer Hussain, Shansong Liu, Chenshuo Sun, Ying Shan<br>for:* 这种研究旨在利用大型语言模型（LLM）来理解和生成不同modalities的音乐。methods:* 该研究使用了预训练的MERT、ViT和ViViT模型来理解音乐、图片和视频。* 使用AudioLDM 2和MusicGen来实现音乐生成。results:* 该研究通过结合多modal的理解和音乐生成，实现了创造性的潜力。* 对比当前状态的艺术模型，该模型的实验结果表明其可以达到或超越现有的性能。<details>
<summary>Abstract</summary>
The current landscape of research leveraging large language models (LLMs) is experiencing a surge. Many works harness the powerful reasoning capabilities of these models to comprehend various modalities, such as text, speech, images, videos, etc. They also utilize LLMs to understand human intention and generate desired outputs like images, videos, and music. However, research that combines both understanding and generation using LLMs is still limited and in its nascent stage. To address this gap, we introduce a Multi-modal Music Understanding and Generation (M$^{2}$UGen) framework that integrates LLM's abilities to comprehend and generate music for different modalities. The M$^{2}$UGen framework is purpose-built to unlock creative potential from diverse sources of inspiration, encompassing music, image, and video through the use of pretrained MERT, ViT, and ViViT models, respectively. To enable music generation, we explore the use of AudioLDM 2 and MusicGen. Bridging multi-modal understanding and music generation is accomplished through the integration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA model to generate extensive datasets that support text/image/video-to-music generation, facilitating the training of our M$^{2}$UGen framework. We conduct a thorough evaluation of our proposed framework. The experimental results demonstrate that our model achieves or surpasses the performance of the current state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
当前研究利用大型语言模型（LLM）的领域正在推进。许多研究利用这些模型的强大理解能力来理解不同类型的数据，如文本、语音、图像、视频等。它们还利用LLM来理解人类意图并生成所需的输出，如图像、视频和音乐。然而，将理解和生成用LLM结合的研究仍然很有限，在初始阶段。为解决这个差距，我们介绍了一个多模态音乐理解和生成（M$^{2}$UGen）框架，它结合了不同类型的启发源的LLM能力，以生成不同模式的音乐。M$^{2}$UGen框架是为了解锁多模态音乐创作的创新潜力，包括音乐、图像和视频。为实现音乐生成，我们探索了AudioLDM 2和MusicGen等模型的使用。通过将多模态理解和音乐生成结合起来，我们使用了LLaMA 2模型。此外，我们使用MU-LLaMA模型生成了大量的文本/图像/视频到音乐生成数据集，以支持我们的M$^{2}$UGen框架的训练。我们进行了严格的实验评估，实验结果表明，我们的模型在性能上与当前状态的艺术模型相当或超越。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/cs.SD_2023_11_19/" data-id="clpztdnp6012mes88919ddbyy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/eess.AS_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T14:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/eess.AS_2023_11_19/">eess.AS - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Label-Synchronous-Neural-Transducer-for-Adaptable-Online-E2E-Speech-Recognition"><a href="#Label-Synchronous-Neural-Transducer-for-Adaptable-Online-E2E-Speech-Recognition" class="headerlink" title="Label-Synchronous Neural Transducer for Adaptable Online E2E Speech Recognition"></a>Label-Synchronous Neural Transducer for Adaptable Online E2E Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11353">http://arxiv.org/abs/2311.11353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keqi Deng, Philip C. Woodland</li>
<li>for: 提高自动语音识别（ASR）的泛化能力，增强其在不同语音频谱中的表现。</li>
<li>methods: 提出了一种标签同步神经转化器（LS-Transducer），通过文本数据来进行领域适应。LS-Transducer使用标签水平Encoder表示性提取，然后将其与预测网络输出结合。这使得预测网络可以轻松地适应文本数据，而不需要大量的标签数据。此外，提出了一种自动步进机制（AIF），可以在低延迟下生成标签水平Encoder表示性，并且可以用于流处理。</li>
<li>results: 与标准神经转化器相比，提出的LS-Transducer在内域LibriSpeech数据上实现了12.9%的相对WRER（WRER）减少，以及21.4%和24.6%的相对WRER减少在跨频数据上（TED-LIUM 2和AESRC2020），并且可以在不同语音频谱中保持高度的同步。<details>
<summary>Abstract</summary>
Although end-to-end (E2E) automatic speech recognition (ASR) has shown state-of-the-art recognition accuracy, it tends to be implicitly biased towards the training data distribution which can degrade generalisation. This paper proposes a label-synchronous neural transducer (LS-Transducer), which provides a natural approach to domain adaptation based on text-only data. The LS-Transducer extracts a label-level encoder representation before combining it with the prediction network output. Since blank tokens are no longer needed, the prediction network performs as a standard language model, which can be easily adapted using text-only data. An Auto-regressive Integrate-and-Fire (AIF) mechanism is proposed to generate the label-level encoder representation while retaining low latency operation that can be used for streaming. In addition, a streaming joint decoding method is designed to improve ASR accuracy while retaining synchronisation with AIF. Experiments show that compared to standard neural transducers, the proposed LS-Transducer gave a 12.9% relative WER reduction (WERR) for intra-domain LibriSpeech data, as well as 21.4% and 24.6% relative WERRs on cross-domain TED-LIUM 2 and AESRC2020 data with an adapted prediction network.
</details>
<details>
<summary>摘要</summary>
although end-to-end (E2E) automatic speech recognition (ASR) has shown state-of-the-art recognition accuracy, it tends to be implicitly biased towards the training data distribution which can degrade generalisation. this paper proposes a label-synchronous neural transducer (LS-Transducer), which provides a natural approach to domain adaptation based on text-only data. the LS-Transducer extracts a label-level encoder representation before combining it with the prediction network output. since blank tokens are no longer needed, the prediction network performs as a standard language model, which can be easily adapted using text-only data. an auto-regressive integrate-and-fire (AIF) mechanism is proposed to generate the label-level encoder representation while retaining low latency operation that can be used for streaming. in addition, a streaming joint decoding method is designed to improve ASR accuracy while retaining synchronisation with AIF. experiments show that compared to standard neural transducers, the proposed LS-Transducer gave a 12.9% relative WER reduction (WERR) for intra-domain LibriSpeech data, as well as 21.4% and 24.6% relative WERRs on cross-domain TED-LIUM 2 and AESRC2020 data with an adapted prediction network.Note:* "WER" stands for "Word Error Rate"* "WERR" stands for "Word Error Rate Reduction"* "LibriSpeech" is a dataset of speech recordings* "TED-LIUM 2" and "AESRC2020" are other datasets of speech recordings
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/eess.AS_2023_11_19/" data-id="clpztdnqr016ges889dwh0it2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/cs.CV_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T13:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/cs.CV_2023_11_19/">cs.CV - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improved-Defect-Detection-and-Classification-Method-for-Advanced-IC-Nodes-by-Using-Slicing-Aided-Hyper-Inference-with-Refinement-Strategy"><a href="#Improved-Defect-Detection-and-Classification-Method-for-Advanced-IC-Nodes-by-Using-Slicing-Aided-Hyper-Inference-with-Refinement-Strategy" class="headerlink" title="Improved Defect Detection and Classification Method for Advanced IC Nodes by Using Slicing Aided Hyper Inference with Refinement Strategy"></a>Improved Defect Detection and Classification Method for Advanced IC Nodes by Using Slicing Aided Hyper Inference with Refinement Strategy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11439">http://arxiv.org/abs/2311.11439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vic De Ridder, Bappaditya Dey, Victor Blanco, Sandip Halder, Bartel Van Waeyenberge</li>
<li>for: 这个论文旨在提高现有的抗噪检测技术，以满足高NA（数字幕面积）EUVL（极紫外光刻）制造过程中的精细缺陷检测需求。</li>
<li>methods: 本文使用了Slicing Aided Hyper Inference（SAHI）框架，通过对大小增加的SEM图像进行推理，提高检测小缺陷的能力。</li>
<li>results: 比较现有的半导体数据集，SAHI方法可以提高小缺陷检测率约2倍；在新的测试数据集上，SAHI方法可以实现缺陷检测率100%，而未经训练的模型则失败。此外，本文还提出了一种不会导致真正正确预测减少的SAHI扩展方法。<details>
<summary>Abstract</summary>
In semiconductor manufacturing, lithography has often been the manufacturing step defining the smallest possible pattern dimensions. In recent years, progress has been made towards high-NA (Numerical Aperture) EUVL (Extreme-Ultraviolet-Lithography) paradigm, which promises to advance pattern shrinking (2 nm node and beyond). However, a significant increase in stochastic defects and the complexity of defect detection becomes more pronounced with high-NA. Present defect inspection techniques (both non-machine learning and machine learning based), fail to achieve satisfactory performance at high-NA dimensions. In this work, we investigate the use of the Slicing Aided Hyper Inference (SAHI) framework for improving upon current techniques. Using SAHI, inference is performed on size-increased slices of the SEM images. This leads to the object detector's receptive field being more effective in capturing small defect instances. First, the performance on previously investigated semiconductor datasets is benchmarked across various configurations, and the SAHI approach is demonstrated to substantially enhance the detection of small defects, by approx. 2x. Afterwards, we also demonstrated application of SAHI leads to flawless detection rates on a new test dataset, with scenarios not encountered during training, whereas previous trained models failed. Finally, we formulate an extension of SAHI that does not significantly reduce true-positive predictions while eliminating false-positive predictions.
</details>
<details>
<summary>摘要</summary>
在半导体制造中，镭射曾经是制造过程中定义最小 Pattern 维度的关键步骤。在最近的年头，高 NA (数字化镜像) EUVL (极紫外光刻) 平台在进行 Pattern 缩小 (2 nm 节点和更多) 做出了 significiant 进步。然而，高 NA 中存在较多的随机缺陷和缺陷检测的复杂性增加。现有的缺陷检测技术（包括机器学习和非机器学习基于的）在高 NA 环境下表现不满意。在这项工作中，我们调查使用 Slicing Aided Hyper Inference (SAHI) 框架来改进当前技术。使用 SAHI，我们在 SEM 图像上进行大小增加的 slice 检测，从而使检测器的感知范围更加有效地捕捉小缺陷实例。首先，我们对已知的半导体数据集进行了不同配置的比较，并证明 SAHI 方法可以大幅提高小缺陷的检测率，约2倍。之后，我们还证明 SAHI 可以在新的测试数据集上实现缺陷检测率100%，而未在训练过程中遇到的场景。最后，我们提出了 SAHI 的扩展，可以不会对真正的正确预测造成重要减少，而是完全消除假阳性预测。
</details></li>
</ul>
<hr>
<h2 id="DiffSCI-Zero-Shot-Snapshot-Compressive-Imaging-via-Iterative-Spectral-Diffusion-Model"><a href="#DiffSCI-Zero-Shot-Snapshot-Compressive-Imaging-via-Iterative-Spectral-Diffusion-Model" class="headerlink" title="DiffSCI: Zero-Shot Snapshot Compressive Imaging via Iterative Spectral Diffusion Model"></a>DiffSCI: Zero-Shot Snapshot Compressive Imaging via Iterative Spectral Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11417">http://arxiv.org/abs/2311.11417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenghao Pan, Haijin Zeng, Jiezhang Cao, Kai Zhang, Yongyong Chen<br>for:diffsci leverages the structural insights from the deep prior and optimization-based methodologies, complemented by the generative capabilities offered by the contemporary denoising diffusion model.methods:firstly, we employ a pre-trained diffusion model, which has been trained on a substantial corpus of rbg images, as the generative denoiser within the plug-and-play framework for the first time. this integration allows for the successful completion of sci reconstruction, especially in the case that current methods struggle to address effectively. secondly, we systematically account for spectral band correlations and introduce a robust methodology to mitigate wavelength mismatch, thus enabling seamless adaptation of the rbg diffusion model to msis.results:we present extensive testing to show that diffsci exhibits discernible performance enhancements over prevailing self-supervised and zero-shot approaches, surpassing even supervised transformer counterparts across both simulated and real datasets.<details>
<summary>Abstract</summary>
This paper endeavors to advance the precision of snapshot compressive imaging (SCI) reconstruction for multispectral image (MSI). To achieve this, we integrate the advantageous attributes of established SCI techniques and an image generative model, propose a novel structured zero-shot diffusion model, dubbed DiffSCI. DiffSCI leverages the structural insights from the deep prior and optimization-based methodologies, complemented by the generative capabilities offered by the contemporary denoising diffusion model. Specifically, firstly, we employ a pre-trained diffusion model, which has been trained on a substantial corpus of RGB images, as the generative denoiser within the Plug-and-Play framework for the first time. This integration allows for the successful completion of SCI reconstruction, especially in the case that current methods struggle to address effectively. Secondly, we systematically account for spectral band correlations and introduce a robust methodology to mitigate wavelength mismatch, thus enabling seamless adaptation of the RGB diffusion model to MSIs. Thirdly, an accelerated algorithm is implemented to expedite the resolution of the data subproblem. This augmentation not only accelerates the convergence rate but also elevates the quality of the reconstruction process. We present extensive testing to show that DiffSCI exhibits discernible performance enhancements over prevailing self-supervised and zero-shot approaches, surpassing even supervised transformer counterparts across both simulated and real datasets. Our code will be available.
</details>
<details>
<summary>摘要</summary>
Firstly, we use a pre-trained diffusion model, which has been trained on a large dataset of RGB images, as the generative denoiser within the Plug-and-Play framework for the first time. This integration allows for successful SCI reconstruction, especially in cases where current methods struggle.Secondly, we systematically account for spectral band correlations and introduce a robust methodology to mitigate wavelength mismatch, enabling seamless adaptation of the RGB diffusion model to MSIs.Thirdly, we develop an accelerated algorithm to expedite the resolution of the data subproblem. This not only accelerates the convergence rate but also improves the quality of the reconstruction process.We present extensive testing to show that DiffSCI exhibits significant performance enhancements over existing self-supervised and zero-shot approaches, outperforming even supervised transformer counterparts across both simulated and real datasets. Our code will be publicly available.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Low-dose-CT-Image-Reconstruction-by-Integrating-Supervised-and-Unsupervised-Learning"><a href="#Enhancing-Low-dose-CT-Image-Reconstruction-by-Integrating-Supervised-and-Unsupervised-Learning" class="headerlink" title="Enhancing Low-dose CT Image Reconstruction by Integrating Supervised and Unsupervised Learning"></a>Enhancing Low-dose CT Image Reconstruction by Integrating Supervised and Unsupervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12071">http://arxiv.org/abs/2311.12071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ling Chen, Zhishen Huang, Yong Long, Saiprasad Ravishankar</li>
<li>for: 这个研究是为了提高低剂量 Computed Tomography (CT) 图像重建的精确性和效率。</li>
<li>methods: 本研究提出了一个混合监督式学习框架，结合了统计学模型和深度学习方法来进行图像重建。</li>
<li>results: 实验结果显示，提案的框架可以与现有的低剂量 CT 重建方法相比，具有更高的精确性和效率。<details>
<summary>Abstract</summary>
Traditional model-based image reconstruction (MBIR) methods combine forward and noise models with simple object priors. Recent application of deep learning methods for image reconstruction provides a successful data-driven approach to addressing the challenges when reconstructing images with undersampled measurements or various types of noise. In this work, we propose a hybrid supervised-unsupervised learning framework for X-ray computed tomography (CT) image reconstruction. The proposed learning formulation leverages both sparsity or unsupervised learning-based priors and neural network reconstructors to simulate a fixed-point iteration process. Each proposed trained block consists of a deterministic MBIR solver and a neural network. The information flows in parallel through these two reconstructors and is then optimally combined. Multiple such blocks are cascaded to form a reconstruction pipeline. We demonstrate the efficacy of this learned hybrid model for low-dose CT image reconstruction with limited training data, where we use the NIH AAPM Mayo Clinic Low Dose CT Grand Challenge dataset for training and testing. In our experiments, we study combinations of supervised deep network reconstructors and MBIR solver with learned sparse representation-based priors or analytical priors. Our results demonstrate the promising performance of the proposed framework compared to recent low-dose CT reconstruction methods.
</details>
<details>
<summary>摘要</summary>
传统模型基于的图像重建（MBIR）方法会结合前向和噪声模型，并使用简单的物件假设。在这个工作中，我们提出了一个混合监督无监督学习框架，用于X射 Computed Tomography（CT）图像重建。我们的学习形式会利用循环约束和神经网络重建器来模拟一个固定点回旋过程。每个提出的训练块都包含一个决定性MBIR解析器和一个神经网络。信息在这两个重建器之间流动，然后互相结合。多个这些块被组合以形成一个重建管线。在我们的实验中，我们研究了对于低剂量CT图像重建的不同组合，包括对于对于MBIR解析器和神经网络的学习统计学假设。我们的结果显示了我们提出的框架与最近的低剂量CT重建方法相比，表现出色。
</details></li>
</ul>
<hr>
<h2 id="FDDM-Unsupervised-Medical-Image-Translation-with-a-Frequency-Decoupled-Diffusion-Model"><a href="#FDDM-Unsupervised-Medical-Image-Translation-with-a-Frequency-Decoupled-Diffusion-Model" class="headerlink" title="FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled Diffusion Model"></a>FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12070">http://arxiv.org/abs/2311.12070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunxiang Li, Hua-Chieh Shao, Xiaoxue Qian, You Zhang<br>for:The paper aims to improve the quality and accuracy of medical image translations using a novel framework called frequency-decoupled diffusion model (FDDM).methods:FDDM decouples the frequency components of medical images in the Fourier domain during the translation process, allowing for structure-preserved high-quality image conversion. It applies an unsupervised frequency conversion module and uses frequency-specific information to guide a following diffusion model for final source-to-target image translation.results:The proposed FDDM outperformed other GAN-, VAE-, and diffusion-based models in terms of image quality and faithfulness to the original anatomical structures. It achieved an FID of 29.88, significantly lower than the second best. These results demonstrate the effectiveness of FDDM in generating highly-realistic target-domain images while maintaining the accuracy of translated anatomical structures.Here is the simplified Chinese version of the three key points:for:这篇论文目标是提高医疗图像翻译质量和准确性，使用一种新的扩展模型（FDDM）。methods:FDDM 在快 Fourier 频域中分离医疗图像的频率组成，以实现保持结构的高质量图像翻译。它使用无监督频谱转换模块和频率特定信息导引后续的扩展模型进行最终的源图像翻译。results:提出的 FDDM 在图像质量和结构准确性方面表现出色，超越了其他 GAN-、VAE- 和扩展模型。它在 FID 指标上达到 29.88，与第二最佳的一半以下。这些结果表明 FDDM 能够生成高度真实的目标频域图像，同时保持翻译后的结构准确性。<details>
<summary>Abstract</summary>
Diffusion models have demonstrated significant potential in producing high-quality images for medical image translation to aid disease diagnosis, localization, and treatment. Nevertheless, current diffusion models have limited success in achieving faithful image translations that can accurately preserve the anatomical structures of medical images, especially for unpaired datasets. The preservation of structural and anatomical details is essential to reliable medical diagnosis and treatment planning, as structural mismatches can lead to disease misidentification and treatment errors. In this study, we introduced a frequency-decoupled diffusion model (FDDM), a novel framework that decouples the frequency components of medical images in the Fourier domain during the translation process, to allow structure-preserved high-quality image conversion. FDDM applies an unsupervised frequency conversion module to translate the source medical images into frequency-specific outputs and then uses the frequency-specific information to guide a following diffusion model for final source-to-target image translation. We conducted extensive evaluations of FDDM using a public brain MR-to-CT translation dataset, showing its superior performance against other GAN-, VAE-, and diffusion-based models. Metrics including the Frechet inception distance (FID), the peak signal-to-noise ratio (PSNR), and the structural similarity index measure (SSIM) were assessed. FDDM achieves an FID of 29.88, less than half of the second best. These results demonstrated FDDM's prowess in generating highly-realistic target-domain images while maintaining the faithfulness of translated anatomical structures.
</details>
<details>
<summary>摘要</summary>
Diffusion models have shown great potential in generating high-quality images for medical image translation, aiding disease diagnosis, localization, and treatment. However, current diffusion models struggle to produce faithful image translations that accurately preserve the anatomical structures of medical images, especially for unpaired datasets. Preserving structural and anatomical details is crucial for reliable medical diagnosis and treatment planning, as mismatches can lead to disease misidentification and treatment errors. In this study, we proposed a frequency-decoupled diffusion model (FDDM) to address this challenge. FDDM separates the frequency components of medical images in the Fourier domain during translation, allowing for structure-preserved high-quality image conversion. FDDM first applies an unsupervised frequency conversion module to translate the source medical images into frequency-specific outputs, then uses the frequency-specific information to guide a following diffusion model for final source-to-target image translation. We evaluated FDDM on a public brain MR-to-CT translation dataset and found it outperformed other GAN-, VAE-, and diffusion-based models. Metrics such as Frechet inception distance (FID), peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM) were used to assess performance. FDDM achieved an FID of 29.88, significantly better than the second best. These results demonstrate FDDM's ability to generate highly-realistic target-domain images while maintaining the faithfulness of translated anatomical structures.
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Emerging-Applications-of-Diffusion-Probabilistic-Models-in-MRI"><a href="#A-Survey-of-Emerging-Applications-of-Diffusion-Probabilistic-Models-in-MRI" class="headerlink" title="A Survey of Emerging Applications of Diffusion Probabilistic Models in MRI"></a>A Survey of Emerging Applications of Diffusion Probabilistic Models in MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11383">http://arxiv.org/abs/2311.11383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuheng Fan, Hanxi Liao, Shiqi Huang, Yimin Luo, Huazhu Fu, Haikun Qi</li>
<li>for: 本文旨在介绍Diffusion probabilistic models (DPMs)在医学成像中的应用，帮助医学成像领域的研究人员了解DPMs在不同应用中的进步。</li>
<li>methods: 本文首先介绍了两种主导的DPMs，即分别是粒子批处理和连续时间批处理的两种方法，然后提供了关于MRI应用的DPMs的全面回顾，包括重建、图像生成、图像翻译、分割、异常检测以及进一步研究话题。</li>
<li>results: 本文对DPMs在MRI应用中的总体局限性以及特定任务的局限性进行了讨论，并指出了未来研究的潜在领域。<details>
<summary>Abstract</summary>
Diffusion probabilistic models (DPMs) which employ explicit likelihood characterization and a gradual sampling process to synthesize data, have gained increasing research interest. Despite their huge computational burdens due to the large number of steps involved during sampling, DPMs are widely appreciated in various medical imaging tasks for their high-quality and diversity of generation. Magnetic resonance imaging (MRI) is an important medical imaging modality with excellent soft tissue contrast and superb spatial resolution, which possesses unique opportunities for diffusion models. Although there is a recent surge of studies exploring DPMs in MRI, a survey paper of DPMs specifically designed for MRI applications is still lacking. This review article aims to help researchers in the MRI community to grasp the advances of DPMs in different applications. We first introduce the theory of two dominant kinds of DPMs, categorized according to whether the diffusion time step is discrete or continuous, and then provide a comprehensive review of emerging DPMs in MRI, including reconstruction, image generation, image translation, segmentation, anomaly detection, and further research topics. Finally, we discuss the general limitations as well as limitations specific to the MRI tasks of DPMs and point out potential areas that are worth further exploration.
</details>
<details>
<summary>摘要</summary>
吸引probabilistic models of diffusion (DPMs)，which employ explicit likelihood characterization and a gradual sampling process to synthesize data，have gained increasing research interest. Despite their huge computational burdens due to the large number of steps involved during sampling, DPMs are widely appreciated in various medical imaging tasks for their high-quality and diversity of generation.Magnetic resonance imaging (MRI) is an important medical imaging modality with excellent soft tissue contrast and superb spatial resolution, which possesses unique opportunities for diffusion models. Although there is a recent surge of studies exploring DPMs in MRI, a survey paper of DPMs specifically designed for MRI applications is still lacking. This review article aims to help researchers in the MRI community to grasp the advances of DPMs in different applications. We first introduce the theory of two dominant kinds of DPMs, categorized according to whether the diffusion time step is discrete or continuous, and then provide a comprehensive review of emerging DPMs in MRI, including reconstruction, image generation, image translation, segmentation, anomaly detection, and further research topics. Finally, we discuss the general limitations as well as limitations specific to the MRI tasks of DPMs and point out potential areas that are worth further exploration.
</details></li>
</ul>
<hr>
<h2 id="Evidential-Uncertainty-Quantification-A-Variance-Based-Perspective"><a href="#Evidential-Uncertainty-Quantification-A-Variance-Based-Perspective" class="headerlink" title="Evidential Uncertainty Quantification: A Variance-Based Perspective"></a>Evidential Uncertainty Quantification: A Variance-Based Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11367">http://arxiv.org/abs/2311.11367</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kerrydrx/evidentialada">https://github.com/kerrydrx/evidentialada</a></li>
<li>paper_authors: Ruxiao Duan, Brian Caffo, Harrison X. Bai, Haris I. Sair, Craig Jones</li>
<li>for: 这个研究的目的是为了提供一种量化深度神经网络中的不确定性的方法，并且适用于多元领域的应用。</li>
<li>methods: 这个研究使用了证据深度学习的方法，将不确定性量化为分类领域内的级别不确定性。</li>
<li>results: 实验结果显示，这个方法可以在跨 домен数据集上实现相当准确的分类，并且可以提供分类不确定性和级别间的相互相关信息。<details>
<summary>Abstract</summary>
Uncertainty quantification of deep neural networks has become an active field of research and plays a crucial role in various downstream tasks such as active learning. Recent advances in evidential deep learning shed light on the direct quantification of aleatoric and epistemic uncertainties with a single forward pass of the model. Most traditional approaches adopt an entropy-based method to derive evidential uncertainty in classification, quantifying uncertainty at the sample level. However, the variance-based method that has been widely applied in regression problems is seldom used in the classification setting. In this work, we adapt the variance-based approach from regression to classification, quantifying classification uncertainty at the class level. The variance decomposition technique in regression is extended to class covariance decomposition in classification based on the law of total covariance, and the class correlation is also derived from the covariance. Experiments on cross-domain datasets are conducted to illustrate that the variance-based approach not only results in similar accuracy as the entropy-based one in active domain adaptation but also brings information about class-wise uncertainties as well as between-class correlations. The code is available at https://github.com/KerryDRX/EvidentialADA. This alternative means of evidential uncertainty quantification will give researchers more options when class uncertainties and correlations are important in their applications.
</details>
<details>
<summary>摘要</summary>
深度神经网络的不确定性评估已成为活跃的研究领域，对downstream任务如活动学习具有关键作用。近年来，显著的潜在深度学习技术在单个模型前进通过直接量化 aleatoric 和 epistemic 不确定性。大多数传统方法采用 entropy 基于的方法来Derive evidential uncertainty in classification，量化不确定性在样本级别。然而，在回归问题中广泛应用的 variance 基于的方法在分类 setting 中 rarely 使用。在这种工作中，我们将 regression 中的 variance 基于方法适应到分类 setting，量化分类不确定性在类别级别。在分类问题中，regression 中的 Covariance 分解技术被推广到类别 Covariance 分解，并 derive 类之间的相关性。在跨Domain 数据集上进行了实验，ILLUSTRATE  dass variance-based approach 不仅与 entropy-based approach 在活动适应中得到相似的准确率，还提供了类别不确定性以及类之间的相关性信息。代码可以在 <https://github.com/KerryDRX/EvidentialADA> 上获取。这种 altenative 的 evidential uncertainty 评估方法将给研究人员更多的选择，当类别不确定性和相关性在应用中具有重要意义时。
</details></li>
</ul>
<hr>
<h2 id="Scale-aware-competition-network-for-palmprint-recognition"><a href="#Scale-aware-competition-network-for-palmprint-recognition" class="headerlink" title="Scale-aware competition network for palmprint recognition"></a>Scale-aware competition network for palmprint recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11354">http://arxiv.org/abs/2311.11354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengrui Gao, Ziyuan Yang, Min Zhu, Andrew Beng Jin Teoh</li>
<li>for: This paper aims to improve the recognition performance of palmprint biometrics by addressing the limitation of prior methodologies that only focus on texture orientation and neglect the significant texture scale dimension.</li>
<li>methods: The proposed method, called SAC-Net, consists of two modules: Inner-Scale Competition Module (ISCM) and Across-Scale Competition Module (ASCM). ISCM integrates learnable Gabor filters and a self-attention mechanism to extract rich orientation data, while ASCM leverages a competitive strategy across various scales to effectively encapsulate competitive texture scale elements.</li>
<li>results: The proposed method was tested on three benchmark datasets and showed exceptional recognition performance and resilience relative to state-of-the-art alternatives.<details>
<summary>Abstract</summary>
Palmprint biometrics garner heightened attention in palm-scanning payment and social security due to their distinctive attributes. However, prevailing methodologies singularly prioritize texture orientation, neglecting the significant texture scale dimension. We design an innovative network for concurrently extracting intra-scale and inter-scale features to redress this limitation. This paper proposes a scale-aware competitive network (SAC-Net), which includes the Inner-Scale Competition Module (ISCM) and the Across-Scale Competition Module (ASCM) to capture texture characteristics related to orientation and scale. ISCM efficiently integrates learnable Gabor filters and a self-attention mechanism to extract rich orientation data and discern textures with long-range discriminative properties. Subsequently, ASCM leverages a competitive strategy across various scales to effectively encapsulate the competitive texture scale elements. By synergizing ISCM and ASCM, our method adeptly characterizes palmprint features. Rigorous experimentation across three benchmark datasets unequivocally demonstrates our proposed approach's exceptional recognition performance and resilience relative to state-of-the-art alternatives.
</details>
<details>
<summary>摘要</summary>
palmprint生物特征在掌握支付和社会保障方面受到高度关注，因其具有独特的特征。然而，现有的方法ologies仅单独强调Texture orientation，忽略了重要的Texture scale维度。我们设计了一种创新的网络，可同时提取内纬度和间纬度特征。这篇论文提出了一种Scale-aware竞争网络（SAC-Net），包括Inner-Scale Competition Module（ISCM）和Across-Scale Competition Module（ASCM），用于捕捉Texture特征相关的orientation和Scale。ISCM使用可学习的Gabor滤波器和自我注意机制，以提取丰富的orientation数据，并能够识别距离特征长距离性的文本。然后，ASCMC使用竞争策略跨多个纬度，以有效地包含竞争Texture纬度元素。通过融合ISCM和ASCMC，我们的方法能够有效地描述掌握特征。经过严格的实验证明，我们的提议方法在三个标准数据集上具有突出的认识性和抗衰落性，与当前的状态对照法相比。
</details></li>
</ul>
<hr>
<h2 id="MoVideo-Motion-Aware-Video-Generation-with-Diffusion-Models"><a href="#MoVideo-Motion-Aware-Video-Generation-with-Diffusion-Models" class="headerlink" title="MoVideo: Motion-Aware Video Generation with Diffusion Models"></a>MoVideo: Motion-Aware Video Generation with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11325">http://arxiv.org/abs/2311.11325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyun Liang, Yuchen Fan, Kai Zhang, Radu Timofte, Luc Van Gool, Rakesh Ranjan</li>
<li>for: 文章旨在提出一种基于运动意识的视频生成（MoVideo）框架，以解决现有的视频生成方法缺乏运动考虑的问题。</li>
<li>methods: 该框架使用了两种方法来考虑运动：视频深度和光流。视频深度用于规范运动，而光流用于保持细节和改善时间一致性。</li>
<li>results: 实验结果显示，MoVideo在文本生成和图像生成中均达到了状态机器的结果，表现出了优秀的提示一致性、帧一致性和视觉质量。<details>
<summary>Abstract</summary>
While recent years have witnessed great progress on using diffusion models for video generation, most of them are simple extensions of image generation frameworks, which fail to explicitly consider one of the key differences between videos and images, i.e., motion. In this paper, we propose a novel motion-aware video generation (MoVideo) framework that takes motion into consideration from two aspects: video depth and optical flow. The former regulates motion by per-frame object distances and spatial layouts, while the later describes motion by cross-frame correspondences that help in preserving fine details and improving temporal consistency. More specifically, given a key frame that exists or generated from text prompts, we first design a diffusion model with spatio-temporal modules to generate the video depth and the corresponding optical flows. Then, the video is generated in the latent space by another spatio-temporal diffusion model under the guidance of depth, optical flow-based warped latent video and the calculated occlusion mask. Lastly, we use optical flows again to align and refine different frames for better video decoding from the latent space to the pixel space. In experiments, MoVideo achieves state-of-the-art results in both text-to-video and image-to-video generation, showing promising prompt consistency, frame consistency and visual quality.
</details>
<details>
<summary>摘要</summary>
“Recent years have witnessed significant progress in using diffusion models for video generation, but most of these models are simple extensions of image generation frameworks, which ignore the key difference between videos and images: motion. In this paper, we propose a novel motion-aware video generation (MoVideo) framework that takes motion into consideration from two aspects: video depth and optical flow. The former regulates motion by per-frame object distances and spatial layouts, while the latter describes motion by cross-frame correspondences that help preserve fine details and improve temporal consistency. Specifically, given a key frame that exists or generated from text prompts, we first design a diffusion model with spatio-temporal modules to generate the video depth and the corresponding optical flows. Then, the video is generated in the latent space by another spatio-temporal diffusion model under the guidance of depth, optical flow-based warped latent video, and the calculated occlusion mask. Finally, we use optical flows again to align and refine different frames for better video decoding from the latent space to the pixel space. In experiments, MoVideo achieves state-of-the-art results in both text-to-video and image-to-video generation, demonstrating excellent prompt consistency, frame consistency, and visual quality.”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Discrete-approximations-of-Gaussian-smoothing-and-Gaussian-derivatives"><a href="#Discrete-approximations-of-Gaussian-smoothing-and-Gaussian-derivatives" class="headerlink" title="Discrete approximations of Gaussian smoothing and Gaussian derivatives"></a>Discrete approximations of Gaussian smoothing and Gaussian derivatives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11317">http://arxiv.org/abs/2311.11317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tony Lindeberg</li>
<li>for: 本研究探讨了在批处理数据时，使用抽象scale-space理论来近似 Gaussian 平滑和Gaussian 导数计算的问题。</li>
<li>methods: 本研究考虑了三种主要的精度方法，包括（i）采样Gaussian 函数和Gaussian 导数函数，（ii）在每个像素支持区域内进行局部积分Gaussian 函数和Gaussian 导数函数，以及（iii）基于离散Gaussian 函数的scale-space分析，并通过小支持中央差 опера作计算导数近似。</li>
<li>results: 研究发现，采样Gaussian 函数和Gaussian 导数函数在非常细见度上表现很差，而离散Gaussian 函数和其对应的离散导数近似方法在非常细见度上表现较好。同时，采样Gaussian 函数和Gaussian 导数函数在大比例参数时（大于约1个网格间距）表现 numerically 非常好，但是在非常细见度上表现不佳。<details>
<summary>Abstract</summary>
This paper develops an in-depth treatment concerning the problem of approximating the Gaussian smoothing and Gaussian derivative computations in scale-space theory for application on discrete data. With close connections to previous axiomatic treatments of continuous and discrete scale-space theory, we consider three main ways discretizing these scale-space operations in terms of explicit discrete convolutions, based on either (i) sampling the Gaussian kernels and the Gaussian derivative kernels, (ii) locally integrating the Gaussian kernels and the Gaussian derivative kernels over each pixel support region and (iii) basing the scale-space analysis on the discrete analogue of the Gaussian kernel, and then computing derivative approximations by applying small-support central difference operators to the spatially smoothed image data.   We study the properties of these three main discretization methods both theoretically and experimentally, and characterize their performance by quantitative measures, including the results they give rise to with respect to the task of scale selection, investigated for four different use cases, and with emphasis on the behaviour at fine scales. The results show that the sampled Gaussian kernels and derivatives as well as the integrated Gaussian kernels and derivatives perform very poorly at very fine scales. At very fine scales, the discrete analogue of the Gaussian kernel with its corresponding discrete derivative approximations performs substantially better. The sampled Gaussian kernel and the sampled Gaussian derivatives do, on the other hand, lead to numerically very good approximations of the corresponding continuous results, when the scale parameter is sufficiently large, in the experiments presented in the paper, when the scale parameter is greater than a value of about 1, in units of the grid spacing.
</details>
<details>
<summary>摘要</summary>
本文对抽象数据中的抽象 Gaussian 平滑和抽象 Gaussian 导数计算进行深入研究，以应用在抽象数据上。与之前的几何学对照推理相关，我们考虑了三种主要的精度化方法，包括：（i）采样 Gaussian 函数和导数函数，（ii）在每个像素支持区域内进行局部积分 Gaussian 函数和导数函数，以及（iii）基于抽象数据的 discrete Gaussian 函数，然后通过小支持中心差分算法计算导数近似。我们对这三种精度化方法进行了理论和实验研究，并通过量化指标来评估其性能，包括根据四个不同的应用场景进行的缩放选择任务的结果，并强调细致级别上的行为。结果表明，采样 Gaussian 函数和导数函数以及局部积分 Gaussian 函数和导数函数在极细致级别上表现很差，而抽象数据的 discrete Gaussian 函数和其对应的导数近似则表现出色。同时，采样 Gaussian 函数和导数函数在大于一定缩放参数（约等于网格间距）时对应的近似结果 numerically 非常好。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-rgb-d-semantic-segmentation-through-multi-modal-interaction-and-pooling-attention"><a href="#Optimizing-rgb-d-semantic-segmentation-through-multi-modal-interaction-and-pooling-attention" class="headerlink" title="Optimizing rgb-d semantic segmentation through multi-modal interaction and pooling attention"></a>Optimizing rgb-d semantic segmentation through multi-modal interaction and pooling attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11312">http://arxiv.org/abs/2311.11312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Zhang, Minghong Xie</li>
<li>for: 提高RGB-D图像semantic segmentation的精度</li>
<li>methods: 利用多Modal Interaction Fusion Module（MIM）和Pooling Attention Module（PAM）将RGB和深度信息融合，并在decoder阶段进行targeted Integration</li>
<li>results: 在NYUDv2和SUN-RGBD两个indoor scene数据集上，MIPANet表现出色，与现有方法相比，提高了RGB-Dsemantic segmentation的精度<details>
<summary>Abstract</summary>
Semantic segmentation of RGB-D images involves understanding the appearance and spatial relationships of objects within a scene, which requires careful consideration of various factors. However, in indoor environments, the simple input of RGB and depth images often results in a relatively limited acquisition of semantic and spatial information, leading to suboptimal segmentation outcomes. To address this, we propose the Multi-modal Interaction and Pooling Attention Network (MIPANet), a novel approach designed to harness the interactive synergy between RGB and depth modalities, optimizing the utilization of complementary information. Specifically, we incorporate a Multi-modal Interaction Fusion Module (MIM) into the deepest layers of the network. This module is engineered to facilitate the fusion of RGB and depth information, allowing for mutual enhancement and correction. Additionally, we introduce a Pooling Attention Module (PAM) at various stages of the encoder. This module serves to amplify the features extracted by the network and integrates the module's output into the decoder in a targeted manner, significantly improving semantic segmentation performance. Our experimental results demonstrate that MIPANet outperforms existing methods on two indoor scene datasets, NYUDv2 and SUN-RGBD, underscoring its effectiveness in enhancing RGB-D semantic segmentation.
</details>
<details>
<summary>摘要</summary>
semantic segmentation of RGB-D 图像含义涉及到场景中对象的外观和空间关系的理解，需要谨慎考虑多种因素。然而，在室内环境中，简单地输入 RGB 和深度图像经常导致semantic和空间信息的有限获取，从而导致 segmentation 结果不佳。为此，我们提出了 Multi-modal Interaction and Pooling Attention Network (MIPANet)，一种新的方法，旨在利用 RGB 和深度模式之间的交互 synergy，最大化 complementary 信息的利用。具体来说，我们在网络的最深层中添加了 Multi-modal Interaction Fusion Module (MIM)。这个模块是用于把 RGB 和深度信息融合在一起，以便互相增强和更正。此外，我们在网络的不同层次中引入了 Pooling Attention Module (PAM)。这个模块用于增强网络提取的特征，并将其集成到解码器中，以提高 semantic segmentation 性能。我们的实验结果表明，MIPANet 在 NYUDv2 和 SUN-RGBD 两个室内场景数据集上的性能都高于现有方法，证明它在 RGB-D semantic segmentation 方面具有显著的改进效果。
</details></li>
</ul>
<hr>
<h2 id="UMAAF-Unveiling-Aesthetics-via-Multifarious-Attributes-of-Images"><a href="#UMAAF-Unveiling-Aesthetics-via-Multifarious-Attributes-of-Images" class="headerlink" title="UMAAF: Unveiling Aesthetics via Multifarious Attributes of Images"></a>UMAAF: Unveiling Aesthetics via Multifarious Attributes of Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11306">http://arxiv.org/abs/2311.11306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Li, Yitian Wan, Xingjiao Wu, Junjie Xu, Cheng Jin, Liang He</li>
<li>For: The paper focuses on Image Aesthetic Assessment (IAA) and proposes a Unified Multi-attribute Aesthetic Assessment Framework (UMAAF) to better utilize image attributes in aesthetic assessment.* Methods: The paper uses a combination of absolute-attribute perception modules and an absolute-attribute interacting network to extract and integrate absolute attribute features, as well as a Relative-Relation Loss function to model the relative attribute of images.* Results: The proposed UMAAF achieves state-of-the-art performance on TAD66K and AVA datasets, and multiple experiments demonstrate the effectiveness of each module and the model’s alignment with human preference.<details>
<summary>Abstract</summary>
With the increasing prevalence of smartphones and websites, Image Aesthetic Assessment (IAA) has become increasingly crucial. While the significance of attributes in IAA is widely recognized, many attribute-based methods lack consideration for the selection and utilization of aesthetic attributes. Our initial step involves the acquisition of aesthetic attributes from both intra- and inter-perspectives. Within the intra-perspective, we extract the direct visual attributes of images, constituting the absolute attribute. In the inter-perspective, our focus lies in modeling the relative score relationships between images within the same sequence, forming the relative attribute. Then, to better utilize image attributes in aesthetic assessment, we propose the Unified Multi-attribute Aesthetic Assessment Framework (UMAAF) to model both absolute and relative attributes of images. For absolute attributes, we leverage multiple absolute-attribute perception modules and an absolute-attribute interacting network. The absolute-attribute perception modules are first pre-trained on several absolute-attribute learning tasks and then used to extract corresponding absolute attribute features. The absolute-attribute interacting network adaptively learns the weight of diverse absolute-attribute features, effectively integrating them with generic aesthetic features from various absolute-attribute perspectives and generating the aesthetic prediction. To model the relative attribute of images, we consider the relative ranking and relative distance relationships between images in a Relative-Relation Loss function, which boosts the robustness of the UMAAF. Furthermore, UMAAF achieves state-of-the-art performance on TAD66K and AVA datasets, and multiple experiments demonstrate the effectiveness of each module and the model's alignment with human preference.
</details>
<details>
<summary>摘要</summary>
随着智能手机和网站的普及，图像美学评估（IAA）变得越来越重要。虽然美学特征在IAA中的重要性得到了广泛的认可，但许多基于特征的方法忽略了图像美学特征的选择和使用。我们的初步步骤是从内部和外部两个角度获取图像美学特征。在内部角度，我们提取图像直接视觉特征，组成绝对特征。在外部角度，我们关注图像序列中图像之间的相对分数关系，组成相对特征。然后，为更好地利用图像特征进行美学评估，我们提议一种统一多属性美学评估框架（UMAAF），以模型图像绝对和相对特征。对于绝对特征，我们利用多个绝对特征感知模块和绝对特征互动网络。绝对特征感知模块首先在多个绝对特征学习任务上进行预训练，然后用来提取对应的绝对特征特征。绝对特征互动网络可以适应性地学习绝对特征特征之间的权重，有效地将多种绝对特征视角综合到一起，生成美学预测。为了模型图像相对特征，我们考虑图像之间的相对排名和相对距离关系，通过相对关系损失函数进行优化。此外，UMAAF在TAD66K和AVA数据集上实现了状态机器人的表现，并在多个实验中证明了模型与人类偏好的一致。
</details></li>
</ul>
<hr>
<h2 id="Exchanging-Dual-Encoder-Decoder-A-New-Strategy-for-Change-Detection-with-Semantic-Guidance-and-Spatial-Localization"><a href="#Exchanging-Dual-Encoder-Decoder-A-New-Strategy-for-Change-Detection-with-Semantic-Guidance-and-Spatial-Localization" class="headerlink" title="Exchanging Dual Encoder-Decoder: A New Strategy for Change Detection with Semantic Guidance and Spatial Localization"></a>Exchanging Dual Encoder-Decoder: A New Strategy for Change Detection with Semantic Guidance and Spatial Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11302">http://arxiv.org/abs/2311.11302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijie Zhao, Xueliang Zhang, Pengfeng Xiao, Guangjun He</li>
<li>for: 这个论文主要targets binary change detection in earth observation applications, with a focus on addressing the problems of bitemporal feature interference and inapplicability of existing methods.</li>
<li>methods: 该论文提出了一种新的exchange dual encoder-decoder structure，which fuses bitemporal features in the decision level and uses bitemporal semantic features to determine changed areas.</li>
<li>results: 根据实验结果，该模型在六个数据集上的性能均达到了或超过了state-of-the-art方法，其中F1-score为97.77%, 83.07%, 94.86%, 92.33%, 91.39%, 74.35%。<details>
<summary>Abstract</summary>
Change detection is a critical task in earth observation applications. Recently, deep learning-based methods have shown promising performance and are quickly adopted in change detection. However, the widely used multiple encoder and single decoder (MESD) as well as dual encoder-decoder (DED) architectures still struggle to effectively handle change detection well. The former has problems of bitemporal feature interference in the feature-level fusion, while the latter is inapplicable to intraclass change detection and multiview building change detection. To solve these problems, we propose a new strategy with an exchanging dual encoder-decoder structure for binary change detection with semantic guidance and spatial localization. The proposed strategy solves the problems of bitemporal feature inference in MESD by fusing bitemporal features in the decision level and the inapplicability in DED by determining changed areas using bitemporal semantic features. We build a binary change detection model based on this strategy, and then validate and compare it with 18 state-of-the-art change detection methods on six datasets in three scenarios, including intraclass change detection datasets (CDD, SYSU), single-view building change detection datasets (WHU, LEVIR-CD, LEVIR-CD+) and a multiview building change detection dataset (NJDS). The experimental results demonstrate that our model achieves superior performance with high efficiency and outperforms all benchmark methods with F1-scores of 97.77%, 83.07%, 94.86%, 92.33%, 91.39%, 74.35% on CDD, SYSU, WHU, LEVIR-CD, LEVIR- CD+, and NJDS datasets, respectively. The code of this work will be available at https://github.com/NJU-LHRS/official-SGSLN.
</details>
<details>
<summary>摘要</summary>
Change detection is a critical task in earth observation applications. Recently, deep learning-based methods have shown promising performance and are quickly adopted in change detection. However, the widely used multiple encoder and single decoder (MESD) as well as dual encoder-decoder (DED) architectures still struggle to effectively handle change detection well. The former has problems of bitemporal feature interference in the feature-level fusion, while the latter is inapplicable to intraclass change detection and multiview building change detection. To solve these problems, we propose a new strategy with an exchanging dual encoder-decoder structure for binary change detection with semantic guidance and spatial localization. The proposed strategy solves the problems of bitemporal feature inference in MESD by fusing bitemporal features in the decision level and the inapplicability in DED by determining changed areas using bitemporal semantic features. We build a binary change detection model based on this strategy, and then validate and compare it with 18 state-of-the-art change detection methods on six datasets in three scenarios, including intraclass change detection datasets (CDD, SYSU), single-view building change detection datasets (WHU, LEVIR-CD, LEVIR-CD+), and a multiview building change detection dataset (NJDS). The experimental results demonstrate that our model achieves superior performance with high efficiency and outperforms all benchmark methods with F1-scores of 97.77%, 83.07%, 94.86%, 92.33%, 91.39%, and 74.35% on CDD, SYSU, WHU, LEVIR-CD, LEVIR-CD+, and NJDS datasets, respectively. The code of this work will be available at https://github.com/NJU-LHRS/official-SGSLN.
</details></li>
</ul>
<hr>
<h2 id="Pair-wise-Layer-Attention-with-Spatial-Masking-for-Video-Prediction"><a href="#Pair-wise-Layer-Attention-with-Spatial-Masking-for-Video-Prediction" class="headerlink" title="Pair-wise Layer Attention with Spatial Masking for Video Prediction"></a>Pair-wise Layer Attention with Spatial Masking for Video Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11289">http://arxiv.org/abs/2311.11289</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlvccn/pla_sm_videopred">https://github.com/mlvccn/pla_sm_videopred</a></li>
<li>paper_authors: Ping Li, Chenhan Zhang, Zheng Yang, Xianghua Xu, Mingli Song</li>
<li>for: 预测视频帧，提高预测质量。</li>
<li>methods: Pair-wise Layer Attention (PLA) 模块和 Spatial Masking (SM) 模块。</li>
<li>results: 提高预测质量， capture 空间时间动态。Here’s the full translation in Simplified Chinese:</li>
<li>for: 预测视频帧，提高预测质量。</li>
<li>methods: 使用 Pair-wise Layer Attention (PLA) 模块和 Spatial Masking (SM) 模块。</li>
<li>results: 提高预测质量， capture 空间时间动态。I hope that helps!<details>
<summary>Abstract</summary>
Video prediction yields future frames by employing the historical frames and has exhibited its great potential in many applications, e.g., meteorological prediction, and autonomous driving. Previous works often decode the ultimate high-level semantic features to future frames without texture details, which deteriorates the prediction quality. Motivated by this, we develop a Pair-wise Layer Attention (PLA) module to enhance the layer-wise semantic dependency of the feature maps derived from the U-shape structure in Translator, by coupling low-level visual cues and high-level features. Hence, the texture details of predicted frames are enriched. Moreover, most existing methods capture the spatiotemporal dynamics by Translator, but fail to sufficiently utilize the spatial features of Encoder. This inspires us to design a Spatial Masking (SM) module to mask partial encoding features during pretraining, which adds the visibility of remaining feature pixels by Decoder. To this end, we present a Pair-wise Layer Attention with Spatial Masking (PLA-SM) framework for video prediction to capture the spatiotemporal dynamics, which reflect the motion trend. Extensive experiments and rigorous ablation studies on five benchmarks demonstrate the advantages of the proposed approach. The code is available at GitHub.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Video prediction yields future frames by employing the historical frames and has exhibited its great potential in many applications, e.g., meteorological prediction, and autonomous driving. Previous works often decode the ultimate high-level semantic features to future frames without texture details, which deteriorates the prediction quality. Motivated by this, we develop a Pair-wise Layer Attention (PLA) module to enhance the layer-wise semantic dependency of the feature maps derived from the U-shape structure in Translator, by coupling low-level visual cues and high-level features. Hence, the texture details of predicted frames are enriched. Moreover, most existing methods capture the spatiotemporal dynamics by Translator, but fail to sufficiently utilize the spatial features of Encoder. This inspires us to design a Spatial Masking (SM) module to mask partial encoding features during pretraining, which adds the visibility of remaining feature pixels by Decoder. To this end, we present a Pair-wise Layer Attention with Spatial Masking (PLA-SM) framework for video prediction to capture the spatiotemporal dynamics, which reflect the motion trend. Extensive experiments and rigorous ablation studies on five benchmarks demonstrate the advantages of the proposed approach. The code is available at GitHub." into Simplified Chinese.Video 预测可以通过历史帧来生成未来帧，并在许多应用中表现出了很大的潜力，例如气象预测和自动驾驶。先前的工作通常将最终的高级 semantic 特征解码到未来帧中，无论是 texture 细节，这会导致预测质量下降。我们受这种情况的激励，开发了一种 Pair-wise Layer Attention (PLA) 模块，以强化 Translator 中层次 semantic 相关性，通过对低级视觉指示和高级特征进行对接。因此，预测的帧 texture 细节得到了增强。此外，大多数现有方法可以通过 Translator 捕捉 spatiotemporal 动力学，但是它们通常不充分利用 Encoder 中的空间特征。这引发我们设计一种 Spatial Masking (SM) 模块，在预训练期间对 Encoder 中的部分特征进行遮盲，使得 Decoder 中的剩下特征像素可以得到更多的可见性。因此，我们提出了一种 Pair-wise Layer Attention with Spatial Masking (PLA-SM) 框架，用于视频预测，以捕捉 spatiotemporal 动力学，它反映了运动趋势。我们对五个标准 benchmark 进行了广泛的实验和严格的ablation 研究， demonstarted 我们提出的方法的优势。代码可以在 GitHub 上获取。
</details></li>
</ul>
<hr>
<h2 id="LucidDreamer-Towards-High-Fidelity-Text-to-3D-Generation-via-Interval-Score-Matching"><a href="#LucidDreamer-Towards-High-Fidelity-Text-to-3D-Generation-via-Interval-Score-Matching" class="headerlink" title="LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching"></a>LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11284">http://arxiv.org/abs/2311.11284</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/envision-research/luciddreamer">https://github.com/envision-research/luciddreamer</a></li>
<li>paper_authors: Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, Yingcong Chen</li>
<li>for: 提高文本到3D生成模型的质量和效率</li>
<li>methods: 使用权重匹配法和3D扩散轨迹来缓解过滤效应，并 integration of 3D Gaussian Splatting</li>
<li>results: 与现状比较，模型表现出较高的质量和更好的训练效率<details>
<summary>Abstract</summary>
The recent advancements in text-to-3D generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3D assets across various real-world scenarios. While recent advancements in text-to-3D generation have shown promise, they often fall short in rendering detailed and high-quality 3D models. This problem is especially prevalent as many methods base themselves on Score Distillation Sampling (SDS). This paper identifies a notable deficiency in SDS, that it brings inconsistent and low-quality updating direction for the 3D model, causing the over-smoothing effect. To address this, we propose a novel approach called Interval Score Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes interval-based score matching to counteract over-smoothing. Furthermore, we incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline. Extensive experiments show that our model largely outperforms the state-of-the-art in quality and training efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Transcending-Forgery-Specificity-with-Latent-Space-Augmentation-for-Generalizable-Deepfake-Detection"><a href="#Transcending-Forgery-Specificity-with-Latent-Space-Augmentation-for-Generalizable-Deepfake-Detection" class="headerlink" title="Transcending Forgery Specificity with Latent Space Augmentation for Generalizable Deepfake Detection"></a>Transcending Forgery Specificity with Latent Space Augmentation for Generalizable Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11278">http://arxiv.org/abs/2311.11278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyuan Yan, Yuhao Luo, Siwei Lyu, Qingshan Liu, Baoyuan Wu</li>
<li>for: 这篇论文的目的是提出一个简单 yet effective的伪照片检测方法，以解决现有的检测方法对于训练和测试数据的不同分布而导致的检测性下降问题。</li>
<li>methods: 这篇论文使用了一个叫做LSDA（Latent Space Data Augmentation）的方法，它基于一个假设：表现更多的伪照片特征可以帮助检测方法学习更加通用的分类边界，因此减少伪照片特有的方法特征的适应。这个方法包括在伪照片空间中制造和模拟不同的伪照片特征，以扩展伪照片空间，并将这些特征融入到检测方法中。</li>
<li>results: 实验结果显示，这篇论文的提出的方法 surprisingly effective，并在许多常用的 benchmark 上超过了现有的检测方法。<details>
<summary>Abstract</summary>
Deepfake detection faces a critical generalization hurdle, with performance deteriorating when there is a mismatch between the distributions of training and testing data. A broadly received explanation is the tendency of these detectors to be overfitted to forgery-specific artifacts, rather than learning features that are widely applicable across various forgeries. To address this issue, we propose a simple yet effective detector called LSDA (\underline{L}atent \underline{S}pace \underline{D}ata \underline{A}ugmentation), which is based on a heuristic idea: representations with a wider variety of forgeries should be able to learn a more generalizable decision boundary, thereby mitigating the overfitting of method-specific features (see Figure. 1). Following this idea, we propose to enlarge the forgery space by constructing and simulating variations within and across forgery features in the latent space. This approach encompasses the acquisition of enriched, domain-specific features and the facilitation of smoother transitions between different forgery types, effectively bridging domain gaps. Our approach culminates in refining a binary classifier that leverages the distilled knowledge from the enhanced features, striving for a generalizable deepfake detector. Comprehensive experiments show that our proposed method is surprisingly effective and transcends state-of-the-art detectors across several widely used benchmarks.
</details>
<details>
<summary>摘要</summary>
深层负作假检测面临着普遍的泛化挑战，其性能在训练和测试数据分布不同时会下降。一个广泛接受的解释是这些检测器过拟合伪造特有的特征，而不是学习通用于不同伪造的特征。为解决这个问题，我们提出了一种简单 yet effective 的检测器 called LSDA（短 latent Space Data Augmentation），基于一个启发性的想法：具有更多种伪造的表示应该能够学习更通用的决策边界，从而避免检测器特有的特征过拟合（参见图1）。按照这个想法，我们建议扩大伪造空间，通过在和 across 伪造特征中构建和模拟变化。这种方法涵盖了获得了更加丰富的领域特定特征，以及在不同伪造类型之间更平滑的过渡。我们的方法最终导致了一种基于更加精炼的特征的二分类器，希望能够创造一个通用的深层负作假检测器。我们的实验表明，我们提出的方法 surprisingly 有效，超越了多个通用的benchmark。
</details></li>
</ul>
<hr>
<h2 id="Generalization-and-Hallucination-of-Large-Vision-Language-Models-through-a-Camouflaged-Lens"><a href="#Generalization-and-Hallucination-of-Large-Vision-Language-Models-through-a-Camouflaged-Lens" class="headerlink" title="Generalization and Hallucination of Large Vision-Language Models through a Camouflaged Lens"></a>Generalization and Hallucination of Large Vision-Language Models through a Camouflaged Lens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11273">http://arxiv.org/abs/2311.11273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lv Tang, Peng-Tao Jiang, Zhihao Shen, Hao Zhang, Jinwei Chen, Bo Li</li>
<li>for: 本研究旨在探讨大视语言模型（LVLM）是否可以在无需训练的情况下泛化到隐身物体检测（COD）场景中。</li>
<li>methods: 我们提出了一种新的框架——隐身视语框架（CPVLF），以探讨LVLM是否可以在COD场景中泛化。在泛化过程中，我们发现LVLM因为幻觉问题而可能错误地感知隐身场景中的 объек，生成了对应的假设概念。此外，由于LVLM没有特定地培育针对隐身物体的精确定位，它在找到隐身物体时表现出一定的uncertainty。因此，我们提出了链条视觉，以增强LVLM在隐身场景中的视觉感知，从语言和视觉两个角度减少幻觉问题，提高LVLM的隐身物体精确定位能力。</li>
<li>results: 我们在三个常用COD数据集上验证了CPVLF的效果，实验结果表明LVLM在COD任务中具有潜在的能力。<details>
<summary>Abstract</summary>
Large Vision-Language Model (LVLM) has seen burgeoning development and increasing attention recently. In this paper, we propose a novel framework, camo-perceptive vision-language framework (CPVLF), to explore whether LVLM can generalize to the challenging camouflaged object detection (COD) scenario in a training-free manner. During the process of generalization, we find that due to hallucination issues within LVLM, it can erroneously perceive objects in camouflaged scenes, producing counterfactual concepts. Moreover, as LVLM is not specifically trained for the precise localization of camouflaged objects, it exhibits a degree of uncertainty in accurately pinpointing these objects. Therefore, we propose chain of visual perception, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects. We validate the effectiveness of CPVLF on three widely used COD datasets, and the experiments show the potential of LVLM in the COD task.
</details>
<details>
<summary>摘要</summary>
Large Vision-Language Model (LVLM) 在最近几年来已经得到了极大的发展和关注。在这篇论文中，我们提出了一种新的框架，即隐形物检测（COD）场景下的视语框架（CPVLF），以探索whether LVLM可以在无需训练的情况下泛化到隐形物检测场景。在泛化过程中，我们发现LVLM因为内在的幻觉问题而可能错误地感知隐形场景中的物体，并且生成了一些对象的counterfactual概念。此外，由于LVLM没有特别地培训用于精确地定位隐形物体，它在检测隐形物体时表现出一定的不确定性。因此，我们提出了一种链条视觉感知机制，该机制可以帮助LVLM在语言和视觉两个方面更好地理解隐形场景，降低幻觉问题，并提高其检测隐形物体的精度。我们 validate了CPVLF的效果在三个常用的COD数据集上，实验结果表明LVLM在COD任务中的潜力。
</details></li>
</ul>
<hr>
<h2 id="Radarize-Large-Scale-Radar-SLAM-for-Indoor-Environments"><a href="#Radarize-Large-Scale-Radar-SLAM-for-Indoor-Environments" class="headerlink" title="Radarize: Large-Scale Radar SLAM for Indoor Environments"></a>Radarize: Large-Scale Radar SLAM for Indoor Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11260">http://arxiv.org/abs/2311.11260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emerson Sie, Xinyu Wu, Heyu Guo, Deepak Vasisht</li>
<li>for: 这个论文是为了解决室内环境中的SLAM问题，使用低成本的半导体mmWave雷达。</li>
<li>methods: 该方法使用雷达本身的特有现象，如Doppler偏移，来提高性能。</li>
<li>results: 作者对146个轨迹数据进行评估，结果显示该方法与当前最佳雷达基本方法相比，约5倍提高了径迹速度和8倍提高了终端SLAM性能，而无需其他传感器如IMU或轮胎速度测量。<details>
<summary>Abstract</summary>
We present Radarize, a self-contained SLAM pipeline for indoor environments that uses only a low-cost commodity single-chip mmWave radar. Our radar-native approach leverages phenomena unique to radio frequencies, such as doppler shift-based odometry, to improve performance. We evaluate our method on a large-scale dataset of 146 trajectories spanning 4 campus buildings, totaling approximately 4680m of travel distance. Our results show that our method outperforms state-of-the-art radar-based approaches by approximately 5x in terms of odometry and 8x in terms of end-to-end SLAM, as measured by absolute trajectory error (ATE), without the need additional sensors such as IMUs or wheel odometry.
</details>
<details>
<summary>摘要</summary>
我们介绍Radarize，一个低成本半导体 millimeter 波雷达 Self-Contained SLAM 管线，适用于室内环境。我们的雷达原生方法利用电磁波特有的现象，如偏振Shift-based odometry，提高性能。我们在4座大学建筑物中测试了146个轨迹，总覆盖距离约4680米。我们的结果表明，我们的方法在相对 Error (ATE) 指标下，与当前雷达基本方法相比，提高约5倍的征迹和8倍的总SLAM。而无需额外传感器，如IMU或轮胎速度测量。
</details></li>
</ul>
<hr>
<h2 id="Quality-and-Quantity-Unveiling-a-Million-High-Quality-Images-for-Text-to-Image-Synthesis-in-Fashion-Design"><a href="#Quality-and-Quantity-Unveiling-a-Million-High-Quality-Images-for-Text-to-Image-Synthesis-in-Fashion-Design" class="headerlink" title="Quality and Quantity: Unveiling a Million High-Quality Images for Text-to-Image Synthesis in Fashion Design"></a>Quality and Quantity: Unveiling a Million High-Quality Images for Text-to-Image Synthesis in Fashion Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12067">http://arxiv.org/abs/2311.12067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia Yu, Lichao Zhang, Zijie Chen, Fayu Pan, MiaoMiao Wen, Yuming Yan, Fangsheng Weng, Shuai Zhang, Lili Pan, Zhenzhong Lan</li>
<li>for: 本研究旨在推动人工智能在时尚设计领域的应用，具体来说是解决当前lack of extensive, interrelated data on clothing and try-on stages问题。</li>
<li>methods: 本研究使用了多年的努力，创建了全球首个一百万高品质时尚图像和详细文本描述的 dataset，该 dataset 包括了多种地域和文化背景的时尚趋势。图像都有精心标注了时尚和人体的 attribute，从而将时尚设计过程转化为文本到图像（T2I）任务。</li>
<li>results: 本研究不仅提供了高品质的文本-图像对和多种人类-服装对，还提供了一个新的标准 benchmark 来评估时尚设计模型的性能。这项研究代表了人工智能驱动时尚设计领域的一个重要突破，为未来的这个领域做出了新的标准。<details>
<summary>Abstract</summary>
The fusion of AI and fashion design has emerged as a promising research area. However, the lack of extensive, interrelated data on clothing and try-on stages has hindered the full potential of AI in this domain. Addressing this, we present the Fashion-Diffusion dataset, a product of multiple years' rigorous effort. This dataset, the first of its kind, comprises over a million high-quality fashion images, paired with detailed text descriptions. Sourced from a diverse range of geographical locations and cultural backgrounds, the dataset encapsulates global fashion trends. The images have been meticulously annotated with fine-grained attributes related to clothing and humans, simplifying the fashion design process into a Text-to-Image (T2I) task. The Fashion-Diffusion dataset not only provides high-quality text-image pairs and diverse human-garment pairs but also serves as a large-scale resource about humans, thereby facilitating research in T2I generation. Moreover, to foster standardization in the T2I-based fashion design field, we propose a new benchmark comprising multiple datasets for evaluating the performance of fashion design models. This work represents a significant leap forward in the realm of AI-driven fashion design, setting a new standard for future research in this field.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）和时尚设计的融合已成为一个有前途的研究领域。然而，由于缺乏广泛、相互关联的服装和试穿stage数据，AI在这个领域的潜力尚未得到完全发挥。为解决这个问题，我们现在提出了时尚扩散数据集（Fashion-Diffusion dataset），这是多年的辛苦努力的产物。这个数据集包含了大量高质量的时尚图像和详细的文本描述，从多个地理位置和文化背景中收集到。图像被精心标注了细化的服装和人体特征，使得时尚设计过程被简化为文本到图像（T2I）任务。Fashion-Diffusion dataset不仅提供了高质量的文本-图像对和多样化的人类-服装对，还可以作为大规模人类资料，促进人工智能驱动的时尚设计研究。此外，为促进时尚设计领域中T2I模型的标准化，我们提议了一个新的标准套件，该套件包括多个数据集用于评估时尚设计模型的性能。这项工作代表了人工智能驱动时尚设计领域的一个重要突破，为未来这个领域的研究提供了新的标准。
</details></li>
</ul>
<hr>
<h2 id="Submeter-level-Land-Cover-Mapping-of-Japan"><a href="#Submeter-level-Land-Cover-Mapping-of-Japan" class="headerlink" title="Submeter-level Land Cover Mapping of Japan"></a>Submeter-level Land Cover Mapping of Japan</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11252">http://arxiv.org/abs/2311.11252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoto Yokoya, Junshi Xia, Clifford Broni-Bediako</li>
<li>for: 本研究的目的是提出一种低成本的人工监督深度学习框架，用于大规模 submeter 级地面覆盖图的自动生成。</li>
<li>methods: 我们使用 OpenEarthMap  benchmark 数据集，并提出了一种基于 U-Net 模型的人工监督深度学习框架，以实现全国范围内的 submeter 级地面覆盖图生成。我们采用了小量的标注数据，将 U-Net 模型 retrained，并达到了全国范围内的 80% 的准确率。</li>
<li>results: 我们使用日本地毫图像数据，生成了日本全国的 submeter 级地面覆盖图，并达到了 80% 的准确率。这种框架可以降低标注成本，并提供高精度的地面覆盖图生成结果，可以贡献于自动更新国家级 submeter 级地面覆盖图。<details>
<summary>Abstract</summary>
Deep learning has shown promising performance in submeter-level mapping tasks; however, the annotation cost of submeter-level imagery remains a challenge, especially when applied on a large scale. In this paper, we present the first submeter-level land cover mapping of Japan with eight classes, at a relatively low annotation cost. We introduce a human-in-the-loop deep learning framework leveraging OpenEarthMap, a recently introduced benchmark dataset for global submeter-level land cover mapping, with a U-Net model that achieves national-scale mapping with a small amount of additional labeled data. By adding a small amount of labeled data of areas or regions where a U-Net model trained on OpenEarthMap clearly failed and retraining the model, an overall accuracy of 80\% was achieved, which is a nearly 16 percentage point improvement after retraining. Using aerial imagery provided by the Geospatial Information Authority of Japan, we create land cover classification maps of eight classes for the entire country of Japan. Our framework, with its low annotation cost and high-accuracy mapping results, demonstrates the potential to contribute to the automatic updating of national-scale land cover mapping using submeter-level optical remote sensing data. The mapping results will be made publicly available.
</details>
<details>
<summary>摘要</summary>
深度学习在微米级地形映射任务中表现出了扎根的优异性，但是微米级地形映射的标注成本仍然是一大挑战，特别是在大规模应用场景下。在这篇论文中，我们提出了首个在日本全国范围内进行微米级土地覆盖分类的案例，使用了OpenEarthMap数据集，一个最近引入的全球微米级土地覆盖分类数据集。我们采用了人工循环深度学习框架，使用U-Net模型，实现了全国范围内的地形映射，并且只需要小量的额外标注数据来提高总体精度。通过在OpenEarthMap上训练U-Net模型，并在模型失败的地方添加小量标注数据，我们实现了总体精度达80%，比前一次训练后提高了16%。使用日本地理信息掌握局提供的飞行图像，我们创建了日本全国范围内的八类土地分类图。我们的框架，具有低标注成本和高精度地形映射结果，适用于自动更新国家级别的土地覆盖分类使用微米级光学Remote感数据。映射结果将公开发布。
</details></li>
</ul>
<hr>
<h2 id="AutoStory-Generating-Diverse-Storytelling-Images-with-Minimal-Human-Effort"><a href="#AutoStory-Generating-Diverse-Storytelling-Images-with-Minimal-Human-Effort" class="headerlink" title="AutoStory: Generating Diverse Storytelling Images with Minimal Human Effort"></a>AutoStory: Generating Diverse Storytelling Images with Minimal Human Effort</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11243">http://arxiv.org/abs/2311.11243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Wang, Canyu Zhao, Hao Chen, Zhekai Chen, Kecheng Zheng, Chunhua Shen</li>
<li>for:  Story visualization system that can generate diverse, high-quality, and consistent sets of story images with minimal human interactions.</li>
<li>methods:  Utilizes comprehension and planning capabilities of large language models for layout planning, and leverages large-scale text-to-image models to generate sophisticated story images based on the layout.</li>
<li>results:  Improves image quality and allows easy and intuitive user interactions, as well as generates multi-view consistent character images without reliance on human labor.Here’s the Chinese version:</li>
<li>for:  Story 视觉系统，可以生成多样化、高质量、一致的故事图片，减少人工干预。</li>
<li>methods:  Utilizes 大语言模型的理解和规划能力进行布局规划，然后利用大规模文本到图像模型生成复杂的故事图片。</li>
<li>results: 提高图片质量，使用户交互更加容易和直观，同时自动生成多视图一致的人物图片。<details>
<summary>Abstract</summary>
Story visualization aims to generate a series of images that match the story described in texts, and it requires the generated images to satisfy high quality, alignment with the text description, and consistency in character identities. Given the complexity of story visualization, existing methods drastically simplify the problem by considering only a few specific characters and scenarios, or requiring the users to provide per-image control conditions such as sketches. However, these simplifications render these methods incompetent for real applications. To this end, we propose an automated story visualization system that can effectively generate diverse, high-quality, and consistent sets of story images, with minimal human interactions. Specifically, we utilize the comprehension and planning capabilities of large language models for layout planning, and then leverage large-scale text-to-image models to generate sophisticated story images based on the layout. We empirically find that sparse control conditions, such as bounding boxes, are suitable for layout planning, while dense control conditions, e.g., sketches and keypoints, are suitable for generating high-quality image content. To obtain the best of both worlds, we devise a dense condition generation module to transform simple bounding box layouts into sketch or keypoint control conditions for final image generation, which not only improves the image quality but also allows easy and intuitive user interactions. In addition, we propose a simple yet effective method to generate multi-view consistent character images, eliminating the reliance on human labor to collect or draw character images.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)story visualization 目标是生成与文本描述匹配的系列图像，并且要求生成的图像具有高质量、文本描述Alignment和人物标识性的一致性。由于story visualization的复杂性，现有的方法通常是忽视一些特定的人物和场景，或者需要用户提供每个图像的控制条件，如素描。然而，这些简化方法在实际应用中无法满足需求。为此，我们提出了一个自动化的story visualization系统，可以生成多样化、高质量和一致的图像集，并且减少人类交互。我们利用大型语言模型的理解和规划能力来进行布局规划，然后利用大规模的文本到图像模型来生成基于布局的复杂的故事图像。我们发现，使用简单的 bounding box 控制条件可以很好地进行布局规划，而使用 dense 控制条件，如素描和关键点，可以生成高质量的图像内容。为了得到最佳的结果，我们提出了一种将简单的 bounding box 控制条件转换成素描或关键点控制条件的方法，从而提高图像质量并允许用户 intuitive 的交互。此外，我们还提出了一种简单 yet effective 的方法来生成多视图一致的人物图像，从而消除了人工劳动的需求，收集或绘制人物图像。
</details></li>
</ul>
<hr>
<h2 id="Open-Vocabulary-Camouflaged-Object-Segmentation"><a href="#Open-Vocabulary-Camouflaged-Object-Segmentation" class="headerlink" title="Open-Vocabulary Camouflaged Object Segmentation"></a>Open-Vocabulary Camouflaged Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11241">http://arxiv.org/abs/2311.11241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youwei Pang, Xiaoqi Zhao, Jiaming Zuo, Lihe Zhang, Huchuan Lu</li>
<li>for: 该 paper 主要研究开放词汇涂抹物体识别任务，即开放 vocabulary camouflaged object segmentation (OVCOS) 任务。</li>
<li>methods: 该 paper 使用 pre-trained 大规模视语言模型 (CLIP) 和 iterate semantic guidance 和 structure enhancement 等方法来实现开放词汇涂抹物体识别。</li>
<li>results: 该 paper 在新constructed的 OVCamo 数据集上 achieved state-of-the-art Results in open-vocabulary semantic image segmentation, outperforming previous methods by a large margin.<details>
<summary>Abstract</summary>
Recently, the emergence of the large-scale vision-language model (VLM), such as CLIP, has opened the way towards open-world object perception. Many works has explored the utilization of pre-trained VLM for the challenging open-vocabulary dense prediction task that requires perceive diverse objects with novel classes at inference time. Existing methods construct experiments based on the public datasets of related tasks, which are not tailored for open vocabulary and rarely involves imperceptible objects camouflaged in complex scenes due to data collection bias and annotation costs. To fill in the gaps, we introduce a new task, open-vocabulary camouflaged object segmentation (OVCOS) and construct a large-scale complex scene dataset (\textbf{OVCamo}) which containing 11,483 hand-selected images with fine annotations and corresponding object classes. Further, we build a strong single-stage open-vocabulary \underline{c}amouflaged \underline{o}bject \underline{s}egmentation transform\underline{er} baseline \textbf{OVCoser} attached to the parameter-fixed CLIP with iterative semantic guidance and structure enhancement. By integrating the guidance of class semantic knowledge and the supplement of visual structure cues from the edge and depth information, the proposed method can efficiently capture camouflaged objects. Moreover, this effective framework also surpasses previous state-of-the-arts of open-vocabulary semantic image segmentation by a large margin on our OVCamo dataset. With the proposed dataset and baseline, we hope that this new task with more practical value can further expand the research on open-vocabulary dense prediction tasks.
</details>
<details>
<summary>摘要</summary>
最近，大规模视力语言模型（VLM）的出现，如CLIP，已经开创了开放世界物体识别的新途径。许多研究已经利用预训练VLM来进行开放词汇稠密预测任务，需要在推理时识别多种新类型的物体。现有方法基于相关任务的公共数据集构建实验，这些数据集通常不适用于开放词汇和罕见的隐形物体，受到数据采集偏见和注释成本的限制。为了填补这些缺陷，我们引入了一个新任务：开放词汇隐形物体分割（OVCOS），并构建了一个大规模的复杂场景数据集（OVCamo），包含11,483张手动选择的图像，以及相应的物体类别。此外，我们建立了一个强大的单stage开放词汇隐形物体分割变换器（OVCoser），将CLIP的参数固定，并采用迭代性含义指导和结构增强。通过结合类别含义知识和视觉结构征信息，提出的方法可以效果地捕捉隐形物体。此外，我们的方法还超过了开放词汇semantic图像分割的前一代性能，在我们的OVCamo数据集上。通过我们的数据集和基eline，我们希望能够通过这个更加实用的任务，进一步推动开放词汇稠密预测任务的研究。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Radiology-Diagnosis-through-Convolutional-Neural-Networks-for-Computer-Vision-in-Healthcare"><a href="#Enhancing-Radiology-Diagnosis-through-Convolutional-Neural-Networks-for-Computer-Vision-in-Healthcare" class="headerlink" title="Enhancing Radiology Diagnosis through Convolutional Neural Networks for Computer Vision in Healthcare"></a>Enhancing Radiology Diagnosis through Convolutional Neural Networks for Computer Vision in Healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11234">http://arxiv.org/abs/2311.11234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keshav Kumar K., Dr N V S L Narasimham</li>
<li>for: 这个研究探讨了使用卷积神经网络(CNNs)在医学诊断中的转变力量，特别是其可解性、效果和伦理问题。</li>
<li>methods: 该研究使用了修改后的DenseNet体系结构，并通过比较分析表明其在特别性、敏感度和准确率方面表现出色。</li>
<li>results: 研究结果表明，CNNs 在医学诊断中表现出优于传统方法，但需要解决可解性问题和不断改进模型。同时，Integration 问题和培训 радиialogsts 也需要考虑。<details>
<summary>Abstract</summary>
The transformative power of Convolutional Neural Networks (CNNs) in radiology diagnostics is examined in this study, with a focus on interpretability, effectiveness, and ethical issues. With an altered DenseNet architecture, the CNN performs admirably in terms of particularity, sensitivity, as well as accuracy. Its superiority over conventional methods is validated by comparative analyses, which highlight efficiency gains. Nonetheless, interpretability issues highlight the necessity of sophisticated methods in addition to continuous model improvement. Integration issues like interoperability and radiologists' training lead to suggestions for teamwork. Systematic consideration of the ethical implications is carried out, necessitating extensive frameworks. Refinement of architectures, interpretability, alongside ethical considerations need to be prioritized in future work for responsible CNN deployment in radiology diagnostics.
</details>
<details>
<summary>摘要</summary>
这种研究探讨了计算机神经网络（CNN）在医学诊断中的转化力量，特别是解释性、有效性和伦理问题。通过修改 denseNet 架构，CNN 表现出色地具有特征性、敏感度和准确性。与传统方法比较分析显示，CNN 在效率方面具有明显的优势。然而，解释性问题表明需要复杂的方法和不断改进模型。医生培训和兼容性问题引起了建议，需要团队合作。对伦理问题进行系统考虑，需要广泛的框架。未来的工作应该PRIORITIZE 架构优化、解释性和伦理考虑，以实现负责任的 CNN 部署在医学诊断中。
</details></li>
</ul>
<hr>
<h2 id="GaussianDiffusion-3D-Gaussian-Splatting-for-Denoising-Diffusion-Probabilistic-Models-with-Structured-Noise"><a href="#GaussianDiffusion-3D-Gaussian-Splatting-for-Denoising-Diffusion-Probabilistic-Models-with-Structured-Noise" class="headerlink" title="GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion Probabilistic Models with Structured Noise"></a>GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion Probabilistic Models with Structured Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11221">http://arxiv.org/abs/2311.11221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinhai Li, Huaibin Wang, Kuo-Kun Tseng</li>
<li>for: 这篇论文旨在提出一种基于 Gaussian splatting 的文本到三维内容生成框架，以实现更加真实的三维图像生成。</li>
<li>methods: 该框架使用 Gaussian splatting 技术，通过控制个别 Gaussian 球体透明度来调整图像饱和度，从而生成更加实际的图像。此外，paper 还提出了一种基于多视图噪声分布的方法，以解决多视图几何匹配问题。</li>
<li>results: 对比传统点 wise sampling 技术，Gaussian splatting 能够生成更加细节rich和饱和度更高的图像。此外，paper 还证明了该方法可以减少浮动、蜡烛和其他艺术ifacts，提高了三维图像生成的质量和稳定性。<details>
<summary>Abstract</summary>
Text-to-3D, known for its efficient generation methods and expansive creative potential, has garnered significant attention in the AIGC domain. However, the amalgamation of Nerf and 2D diffusion models frequently yields oversaturated images, posing severe limitations on downstream industrial applications due to the constraints of pixelwise rendering method. Gaussian splatting has recently superseded the traditional pointwise sampling technique prevalent in NeRF-based methodologies, revolutionizing various aspects of 3D reconstruction. This paper introduces a novel text to 3D content generation framework based on Gaussian splatting, enabling fine control over image saturation through individual Gaussian sphere transparencies, thereby producing more realistic images. The challenge of achieving multi-view consistency in 3D generation significantly impedes modeling complexity and accuracy. Taking inspiration from SJC, we explore employing multi-view noise distributions to perturb images generated by 3D Gaussian splatting, aiming to rectify inconsistencies in multi-view geometry. We ingeniously devise an efficient method to generate noise that produces Gaussian noise from diverse viewpoints, all originating from a shared noise source. Furthermore, vanilla 3D Gaussian-based generation tends to trap models in local minima, causing artifacts like floaters, burrs, or proliferative elements. To mitigate these issues, we propose the variational Gaussian splatting technique to enhance the quality and stability of 3D appearance. To our knowledge, our approach represents the first comprehensive utilization of Gaussian splatting across the entire spectrum of 3D content generation processes.
</details>
<details>
<summary>摘要</summary>
文本到3D技术，因其高效生成方法和广泛的创造力，在AIGC领域引起了广泛关注。然而，将NERF和2D扩散模型结合起来经常导致过度饱和的图像，从而限制了下游工业应用的可行性，由于像素级渲染方法的约束。在这篇论文中，我们介绍了一种基于Gaussian splatting的新的文本到3D内容生成框架，可以通过个别Gaussian球体透明度来控制图像饱和程度，从而生成更真实的图像。然而，在多视图协调中，3D生成过程中的模型复杂度和精度受到了严重的限制。我们在SJC的灵感下， explore使用多视图噪声分布来扰乱3D Gaussian splatting生成的图像，以修复多视图几何学的不一致。我们创新地设计了一种高效的生成噪声方法，可以从多个视图点生成Gaussian噪声，所有这些噪声都来自同一个噪声源。此外，普通的3D Gaussian基于生成往往会让模型陷入本地最小值，导致残余元素如浮动物、刺激物或生长物的出现。为了解决这些问题，我们提出了变量Gaussian splatting技术，以提高3D外观质量和稳定性。根据我们所知，我们的方法是第一次在3D内容生成过程中广泛应用Gaussian splatting。
</details></li>
</ul>
<hr>
<h2 id="Infrared-image-identification-method-of-substation-equipment-fault-under-weak-supervision"><a href="#Infrared-image-identification-method-of-substation-equipment-fault-under-weak-supervision" class="headerlink" title="Infrared image identification method of substation equipment fault under weak supervision"></a>Infrared image identification method of substation equipment fault under weak supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11214">http://arxiv.org/abs/2311.11214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anjali Sharma, Priya Banerjee, Nikhil Singh</li>
<li>for: 这种研究旨在提出一种弱监督的方法，用于检测发电厂设备的缺陷。</li>
<li>methods: 该方法使用更改模型网络结构和参数，以提高设备识别精度。</li>
<li>results: 研究表明，该方法可以准确地识别多种设备类型的缺陷，并与人工标注结果进行比较，证明了该算法的高精度。<details>
<summary>Abstract</summary>
This study presents a weakly supervised method for identifying faults in infrared images of substation equipment. It utilizes the Faster RCNN model for equipment identification, enhancing detection accuracy through modifications to the model's network structure and parameters. The method is exemplified through the analysis of infrared images captured by inspection robots at substations. Performance is validated against manually marked results, demonstrating that the proposed algorithm significantly enhances the accuracy of fault identification across various equipment types.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这个研究提出了一种弱监督的方法，用于在发电厂设备上的热像中检测缺陷。它利用了Faster RCNN模型来识别设备，通过模型网络结构和参数的修改，提高检测精度。该方法通过在发电厂检测机器人采集的热像进行分析，并与手动标注结果进行验证，证明提posed算法可以明显提高不同设备类型的缺陷检测精度。
</details></li>
</ul>
<hr>
<h2 id="HiH-A-Multi-modal-Hierarchy-in-Hierarchy-Network-for-Unconstrained-Gait-Recognition"><a href="#HiH-A-Multi-modal-Hierarchy-in-Hierarchy-Network-for-Unconstrained-Gait-Recognition" class="headerlink" title="HiH: A Multi-modal Hierarchy in Hierarchy Network for Unconstrained Gait Recognition"></a>HiH: A Multi-modal Hierarchy in Hierarchy Network for Unconstrained Gait Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11210">http://arxiv.org/abs/2311.11210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Wang, Yinchi Ma, Peng Luan, Wei Yao, Congcong Li, Bo Liu</li>
<li>For: Robust gait recognition in unconstrained environments, addressing challenges such as view changes, occlusions, and varying walking speeds, as well as cross-modality incompatibility.* Methods: A multi-modal Hierarchy in Hierarchy network (HiH) that integrates silhouette and pose sequences, featuring Hierarchical Gait Decomposer (HGD) modules and auxiliary branches for 2D joint sequences, including Deformable Spatial Enhancement (DSE) and Deformable Temporal Alignment (DTA) modules.* Results: State-of-the-art performance in gait recognition, with a well-balanced trade-off between accuracy and efficiency, demonstrated through extensive evaluations across diverse indoor and outdoor datasets.<details>
<summary>Abstract</summary>
Gait recognition has achieved promising advances in controlled settings, yet it significantly struggles in unconstrained environments due to challenges such as view changes, occlusions, and varying walking speeds. Additionally, efforts to fuse multiple modalities often face limited improvements because of cross-modality incompatibility, particularly in outdoor scenarios. To address these issues, we present a multi-modal Hierarchy in Hierarchy network (HiH) that integrates silhouette and pose sequences for robust gait recognition. HiH features a main branch that utilizes Hierarchical Gait Decomposer (HGD) modules for depth-wise and intra-module hierarchical examination of general gait patterns from silhouette data. This approach captures motion hierarchies from overall body dynamics to detailed limb movements, facilitating the representation of gait attributes across multiple spatial resolutions. Complementing this, an auxiliary branch, based on 2D joint sequences, enriches the spatial and temporal aspects of gait analysis. It employs a Deformable Spatial Enhancement (DSE) module for pose-guided spatial attention and a Deformable Temporal Alignment (DTA) module for aligning motion dynamics through learned temporal offsets. Extensive evaluations across diverse indoor and outdoor datasets demonstrate HiH's state-of-the-art performance, affirming a well-balanced trade-off between accuracy and efficiency.
</details>
<details>
<summary>摘要</summary>
<<SYS>>transliteration: Gait recognition yǐjīng zhòngyì xiǎngyìng, yètī zhèngyì zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng zhìyǐ zhèngxìng
</details></li>
</ul>
<hr>
<h2 id="3D-Guidewire-Shape-Reconstruction-from-Monoplane-Fluoroscopic-Images"><a href="#3D-Guidewire-Shape-Reconstruction-from-Monoplane-Fluoroscopic-Images" class="headerlink" title="3D Guidewire Shape Reconstruction from Monoplane Fluoroscopic Images"></a>3D Guidewire Shape Reconstruction from Monoplane Fluoroscopic Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11209">http://arxiv.org/abs/2311.11209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tudor Jianu, Baoru Huang, Pierre Berthet-Rayne, Sebastiano Fichera, Anh Nguyen</li>
<li>for: used to reconstruct 3D guidewires in endovascular interventions, reducing radiation exposure and improving accuracy.</li>
<li>methods: utilizes CathSim, a state-of-the-art endovascular simulator, and a 3D Fluoroscopy Guidewire Reconstruction Network (3D-FGRN) to reconstruct the 3D guidewire from simulated monoplane fluoroscopic images.</li>
<li>results: delivers results on par with conventional triangulation methods, demonstrating the efficiency and potential of the proposed network.<details>
<summary>Abstract</summary>
Endovascular navigation, essential for diagnosing and treating endovascular diseases, predominantly hinges on fluoroscopic images due to the constraints in sensory feedback. Current shape reconstruction techniques for endovascular intervention often rely on either a priori information or specialized equipment, potentially subjecting patients to heightened radiation exposure. While deep learning holds potential, it typically demands extensive data. In this paper, we propose a new method to reconstruct the 3D guidewire by utilizing CathSim, a state-of-the-art endovascular simulator, and a 3D Fluoroscopy Guidewire Reconstruction Network (3D-FGRN). Our 3D-FGRN delivers results on par with conventional triangulation from simulated monoplane fluoroscopic images. Our experiments accentuate the efficiency of the proposed network, demonstrating it as a promising alternative to traditional methods.
</details>
<details>
<summary>摘要</summary>
endpointvascular navigation, essential for diagnosing and treating endpointvascular diseases, predominantly hinges on fluoroscopic images due to the constraints in sensory feedback. Current shape reconstruction techniques for endpointvascular intervention often rely on either a priori information or specialized equipment, potentially subjecting patients to heightened radiation exposure. While deep learning holds potential, it typically demands extensive data. In this paper, we propose a new method to reconstruct the 3D guidewire by utilizing CathSim, a state-of-the-art endpointvascular simulator, and a 3D Fluoroscopy Guidewire Reconstruction Network (3D-FGRN). Our 3D-FGRN delivers results on par with conventional triangulation from simulated monoplane fluoroscopic images. Our experiments accentuate the efficiency of the proposed network, demonstrating it as a promising alternative to traditional methods.Here's the breakdown of the translation:* endpointvascular (endpointvascular navigation) -> 终端血管导航 (endpointvascular navigation)* predominantly (predominantly hinges) -> 主要地 (predominantly hinges)* fluoroscopic (fluoroscopic images) -> 萤光成像 (fluoroscopic images)* triangulation (conventional triangulation) -> 三角形计算 (conventional triangulation)* CathSim (CathSim) -> CATSIM (CathSim)* 3D Fluoroscopy Guidewire Reconstruction Network (3D-FGRN) -> 三维萤光成像导wire重建网络 (3D-FGRN)Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="LogicNet-A-Logical-Consistency-Embedded-Face-Attribute-Learning-Network"><a href="#LogicNet-A-Logical-Consistency-Embedded-Face-Attribute-Learning-Network" class="headerlink" title="LogicNet: A Logical Consistency Embedded Face Attribute Learning Network"></a>LogicNet: A Logical Consistency Embedded Face Attribute Learning Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11208">http://arxiv.org/abs/2311.11208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haiyu Wu, Sicong Tian, Huayu Li, Kevin W. Bowyer</li>
<li>for: 提高多属性分类中逻辑一致性的可靠性</li>
<li>methods: 引入两个挑战：1) 如何使用数据逻辑一致性检查训练模型，以便获得逻辑一致的预测结果？2) 如何实现这一点不需要数据逻辑一致性检查？提出两个数据集（FH41K和CelebA-logic）和LogicNet adversarial training框架，用于学习属性之间的逻辑关系。</li>
<li>results: LogicNet在FH37K、FH41K和CelebA-logic上的准确率比下一个最佳方法高出23.05%、9.96%和1.71%，在实际案例分析中，我们的方法可以将失败案例数减少 más than 50% compared to other methods。<details>
<summary>Abstract</summary>
Ensuring logical consistency in predictions is a crucial yet overlooked aspect in multi-attribute classification. We explore the potential reasons for this oversight and introduce two pressing challenges to the field: 1) How can we ensure that a model, when trained with data checked for logical consistency, yields predictions that are logically consistent? 2) How can we achieve the same with data that hasn't undergone logical consistency checks? Minimizing manual effort is also essential for enhancing automation. To address these challenges, we introduce two datasets, FH41K and CelebA-logic, and propose LogicNet, an adversarial training framework that learns the logical relationships between attributes. Accuracy of LogicNet surpasses that of the next-best approach by 23.05%, 9.96%, and 1.71% on FH37K, FH41K, and CelebA-logic, respectively. In real-world case analysis, our approach can achieve a reduction of more than 50% in the average number of failed cases compared to other methods.
</details>
<details>
<summary>摘要</summary>
保证预测的逻辑一致性是多属性分类中的一个重要 yet 被忽略的方面。我们探讨了可能导致这种忽略的原因，并提出了两个严重的挑战：1）如何使得在数据进行逻辑一致性检查后训练的模型可以生成逻辑一致的预测？2）如何实现这一点而不需要手动干预？为解决这些挑战，我们介绍了两个数据集FH41K和CelebA-logic，并提出了LogicNet，一种基于对属性之间的逻辑关系进行反对抗训练的框架。LogicNet的准确率与接下来最佳方法相比高出23.05%、9.96%和1.71%的差。在实际案例分析中，我们的方法可以实现预测失败案例的减少率超过50%。
</details></li>
</ul>
<hr>
<h2 id="Shape-Sensitive-Loss-for-Catheter-and-Guidewire-Segmentation"><a href="#Shape-Sensitive-Loss-for-Catheter-and-Guidewire-Segmentation" class="headerlink" title="Shape-Sensitive Loss for Catheter and Guidewire Segmentation"></a>Shape-Sensitive Loss for Catheter and Guidewire Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11205">http://arxiv.org/abs/2311.11205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chayun Kongtongvattana, Baoru Huang, Jingxuan Kang, Hoan Nguyen, Olajide Olufemi, Anh Nguyen</li>
<li>for: 这个论文旨在提高镜像内的导 wire和干扰器分割精度。</li>
<li>methods: 该论文使用了一种形态敏感损失函数，并在视Transformer网络中应用该函数来实现新的状态值。</li>
<li>results: 该论文在大规模镜像数据集上实现了新的状态值，并且提供了高维特征向量，以及一种基于cosinus相似性的图像相似度测量方法。<details>
<summary>Abstract</summary>
We introduce a shape-sensitive loss function for catheter and guidewire segmentation and utilize it in a vision transformer network to establish a new state-of-the-art result on a large-scale X-ray images dataset. We transform network-derived predictions and their corresponding ground truths into signed distance maps, thereby enabling any networks to concentrate on the essential boundaries rather than merely the overall contours. These SDMs are subjected to the vision transformer, efficiently producing high-dimensional feature vectors encapsulating critical image attributes. By computing the cosine similarity between these feature vectors, we gain a nuanced understanding of image similarity that goes beyond the limitations of traditional overlap-based measures. The advantages of our approach are manifold, ranging from scale and translation invariance to superior detection of subtle differences, thus ensuring precise localization and delineation of the medical instruments within the images. Comprehensive quantitative and qualitative analyses substantiate the significant enhancement in performance over existing baselines, demonstrating the promise held by our new shape-sensitive loss function for improving catheter and guidewire segmentation.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种形态敏感损失函数，用于捷克和导 wire segmentation，并在视Transformer网络中应用，以实现大规模X射线图像数据集上新的状态anner-of-the-art result。我们将网络 derivated predictions和其对应的ground truth转换成签名距离地图，以便任何网络可以专注于重要的边界而不仅仅是总的轮廓。这些SDMs被视Transformer转换成高维特征向量，其中包含了重要的图像特征。通过计算这些特征向量之间的cosine相似性，我们获得了一种超越传统 overlap-based measures的多方面的图像相似性理解。我们的方法具有许多优点，包括尺度和平移不变性，以及更好地检测到微不足的差异，从而确保精准地位置和定义医疗器械在图像中。完整的量化和质量分析证明了我们新的形态敏感损失函数对捷克和导 wire segmentation的改进。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Versus-Supervised-Training-for-Segmentation-of-Organoid-Images"><a href="#Self-Supervised-Versus-Supervised-Training-for-Segmentation-of-Organoid-Images" class="headerlink" title="Self-Supervised Versus Supervised Training for Segmentation of Organoid Images"></a>Self-Supervised Versus Supervised Training for Segmentation of Organoid Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11198">http://arxiv.org/abs/2311.11198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asmaa Haja, Eric Brouwer, Lambert Schomaker</li>
<li>for: 本研究旨在提高数字显微镜技术中数据标注的效率和可靠性，以便更好地利用深度学习算法进行图像分类和分割。</li>
<li>methods: 本研究使用了自动学习（SSL）技术，通过学习内在特征来寻找与主任务相似的预测任务，从而不需要标注数据。研究使用了ResNet50 U-Net模型，首先在增强图像上进行了图像恢复训练，然后将模型拟合到图像分割任务上。</li>
<li>results: 研究结果表明，使用25%像素掉用或图像模糊增强策略可以更高效地进行自动学习，并且在几何损失、精度损失和交集精度损失等评价指标上表现更好。在训练114张图像后，自动学习方法的F1分数为0.85，高于监督学习方法的F1分数（0.78），并且在训练1000张图像后，自动学习方法的F1分数仍然高于监督学习方法（0.92 vs 0.85）。<details>
<summary>Abstract</summary>
The process of annotating relevant data in the field of digital microscopy can be both time-consuming and especially expensive due to the required technical skills and human-expert knowledge. Consequently, large amounts of microscopic image data sets remain unlabeled, preventing their effective exploitation using deep-learning algorithms. In recent years it has been shown that a lot of relevant information can be drawn from unlabeled data. Self-supervised learning (SSL) is a promising solution based on learning intrinsic features under a pretext task that is similar to the main task without requiring labels. The trained result is transferred to the main task - image segmentation in our case. A ResNet50 U-Net was first trained to restore images of liver progenitor organoids from augmented images using the Structural Similarity Index Metric (SSIM), alone, and using SSIM combined with L1 loss. Both the encoder and decoder were trained in tandem. The weights were transferred to another U-Net model designed for segmentation with frozen encoder weights, using Binary Cross Entropy, Dice, and Intersection over Union (IoU) losses. For comparison, we used the same U-Net architecture to train two supervised models, one utilizing the ResNet50 encoder as well as a simple CNN. Results showed that self-supervised learning models using a 25\% pixel drop or image blurring augmentation performed better than the other augmentation techniques using the IoU loss. When trained on only 114 images for the main task, the self-supervised learning approach outperforms the supervised method achieving an F1-score of 0.85, with higher stability, in contrast to an F1=0.78 scored by the supervised method. Furthermore, when trained with larger data sets (1,000 images), self-supervised learning is still able to perform better, achieving an F1-score of 0.92, contrasting to a score of 0.85 for the supervised method.
</details>
<details>
<summary>摘要</summary>
digit化显微镜数据的标注过程可能会很时间consuming，特别是需要技术专业知识和人工智能。因此，大量的显微镜数据仍然无法被标注，从而阻碍它们的有效利用使用深度学习算法。在过去几年中，已经证明了很多相关信息可以从无标注数据中获取。基于学习内在特征的自我监督学习（SSL）是一个有前途的解决方案，不需要标注。在我们的情况下，我们使用ResNet50 U-Net来预测肝发生器组织胞的图像，并使用SSIM指数和L1损失来训练。 Encoder和decoder都在一起训练。 weights被转授到另一个用于类别的 U-Net 模型中，使用 Binary Cross Entropy、Dice 和 Intersection over Union（IoU）损失。为了比较，我们使用了同一个 U-Net 架构，训练了两个超vised模型，一个使用 ResNet50 嵌入器，另一个使用简单的 CNN。结果显示，使用25%像素截割或图像模糊增强的自我监督学习模型在IoU损失下表现较好，并且在仅使用114幅图像进行主任务训练时，自我监督学习方法的F1分数高于超vised方法（F1=0.85 vs F1=0.78）。此外，当训练数据集大小增加到1,000幅时，自我监督学习方法仍然能够表现较好，其F1分数为0.92，而超vised方法的F1分数为0.85。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/cs.CV_2023_11_19/" data-id="clpztdnjo00nies88hkug2l2b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/cs.AI_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T12:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/cs.AI_2023_11_19/">cs.AI - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="LLM-aided-semi-supervision-for-Extractive-Dialog-Summarization"><a href="#LLM-aided-semi-supervision-for-Extractive-Dialog-Summarization" class="headerlink" title="LLM aided semi-supervision for Extractive Dialog Summarization"></a>LLM aided semi-supervision for Extractive Dialog Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11462">http://arxiv.org/abs/2311.11462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nishant Mishra, Gaurav Sahu, Iacer Calixto, Ameen Abu-Hanna, Issam H. Laradji</li>
<li>for: 提高chat summarization的效果</li>
<li>methods: 使用state-of-the-art大语言模型（LLMs）生成对话的pseudo-标签，然后使用这些pseudo-标签进行精度的训练，从而将知识传递到更小的专门化模型中</li>
<li>results: 在\tweetsumm dataset上实现了65.9&#x2F;57.0&#x2F;61.0 ROUGE-1&#x2F;-2&#x2F;-L的性能，比现有的状态场景下的性能提高了4.6%&#x2F;12.2%&#x2F;16.7%。最差情况下（即ROUGE-L），我们仍能保持94.7%的性能，使用只有10%的数据。<details>
<summary>Abstract</summary>
Generating high-quality summaries for chat dialogs often requires large labeled datasets. We propose a method to efficiently use unlabeled data for extractive summarization of customer-agent dialogs. In our method, we frame summarization as a question-answering problem and use state-of-the-art large language models (LLMs) to generate pseudo-labels for a dialog. We then use these pseudo-labels to fine-tune a chat summarization model, effectively transferring knowledge from the large LLM into a smaller specialized model. We demonstrate our method on the \tweetsumm dataset, and show that using 10\% of the original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L, whereas the current state-of-the-art trained on the entire training data set obtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case (i.e., ROUGE-L) we still effectively retain 94.7% of the performance while using only 10% of the data.
</details>
<details>
<summary>摘要</summary>
生成高质量摘要 для чат对话经常需要大量标注数据。我们提议使用无标注数据来高效地进行摘要EXTRACTIVE summarization of customer-agent dialogs。在我们的方法中，我们将摘要视为问答问题，使用当前最佳大语言模型（LLM）生成对对话的pseudo标签。然后，我们使用这些pseudo标签来练化一个专门的chat摘要模型，从而将大型LLM中的知识传递到一个更小的专门模型中。我们在\tweetsumm数据集上采用这种方法，并证明使用10%原始标注数据集可以达到65.9/57.0/61.0 ROUGE-1/-2/-L，而当前状态的训练所有数据集只能达到65.16/55.81/64.37 ROUGE-1/-2/-L。换句话说，在最差情况下（即ROUGE-L），我们仍然可以保留94.7%的性能，只使用10%的数据。
</details></li>
</ul>
<hr>
<h2 id="SecureBERT-and-LLAMA-2-Empowered-Control-Area-Network-Intrusion-Detection-and-Classification"><a href="#SecureBERT-and-LLAMA-2-Empowered-Control-Area-Network-Intrusion-Detection-and-Classification" class="headerlink" title="SecureBERT and LLAMA 2 Empowered Control Area Network Intrusion Detection and Classification"></a>SecureBERT and LLAMA 2 Empowered Control Area Network Intrusion Detection and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12074">http://arxiv.org/abs/2311.12074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuemei Li, Huirong Fu</li>
<li>for: 本研究旨在评估预训练模型在控制区网络攻击检测中的适应性。</li>
<li>methods: 我们开发了两种不同的模型：CAN-SecureBERT和CAN-LLAMA2。CAN-LLAMA2模型在准确率、精度检测率、F1分数和假阳性率方面达到了最佳性能，其中假阳性率为3.10e-6，比前一代模型MTH-IDS（多层混合攻击检测系统）的假阳性率小52倍。</li>
<li>results: 我们的研究表明，使用大语言模型作为基本模型，并在其上添加适应器以满足其他计算机安全相关任务，可以保持模型的语言相关能力，同时提高检测性能。<details>
<summary>Abstract</summary>
Numerous studies have proved their effective strength in detecting Control Area Network (CAN) attacks. In the realm of understanding the human semantic space, transformer-based models have demonstrated remarkable effectiveness. Leveraging pre-trained transformers has become a common strategy in various language-related tasks, enabling these models to grasp human semantics more comprehensively. To delve into the adaptability evaluation on pre-trained models for CAN intrusion detection, we have developed two distinct models: CAN-SecureBERT and CAN-LLAMA2. Notably, our CAN-LLAMA2 model surpasses the state-of-the-art models by achieving an exceptional performance 0.999993 in terms of balanced accuracy, precision detection rate, F1 score, and a remarkably low false alarm rate of 3.10e-6. Impressively, the false alarm rate is 52 times smaller than that of the leading model, MTH-IDS (Multitiered Hybrid Intrusion Detection System). Our study underscores the promise of employing a Large Language Model as the foundational model, while incorporating adapters for other cybersecurity-related tasks and maintaining the model's inherent language-related capabilities.
</details>
<details>
<summary>摘要</summary>
多个研究证明他们在检测控制区网络（CAN）攻击方面的效力是非常高的。在人类语义空间理解方面，基于转换器的模型表现了非常出色的。利用预训练转换器变得成为了许多语言相关任务中的常见策略，使得这些模型能够更全面地捕捉人类语义。为了探索预训练模型在CAN攻击检测中的适应性，我们开发了两个不同的模型：CAN-SecureBERT和CAN-LLAMA2。需要注意的是，我们的CAN-LLAMA2模型在权衡准确率、检测精度率、F1分数和false alarm rate方面达到了非常出色的表现，其中false alarm rate为3.10e-6，与领先的模型MTH-IDS（多层混合攻击检测系统）的false alarm rate相比，下降了52倍。我们的研究证明了在基于大语言模型的同时，采用适应器进行其他Cybersecurity相关任务的可行性。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Public-Perceptions-Machine-Learning-Based-Sentiment-Analysis-of-COVID-19-Vaccines-in-India"><a href="#Unveiling-Public-Perceptions-Machine-Learning-Based-Sentiment-Analysis-of-COVID-19-Vaccines-in-India" class="headerlink" title="Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis of COVID-19 Vaccines in India"></a>Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis of COVID-19 Vaccines in India</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11435">http://arxiv.org/abs/2311.11435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milind Gupta, Abhishek Kaushik</li>
<li>for: 本研究旨在探讨印度人民对COVID-19疫苗的看法，以便帮助印度政府成功实施疫苗接种计划。</li>
<li>methods: 本研究使用数据挖掘技术分析Reddit平台上的评论，以评估印度用户对COVID-19疫苗的看法。 Python的Text Blob库用于注释评论，以评估总体情感。</li>
<li>results: 结果显示，大多数Reddit用户在印度表达中性或无关性对疫苗接种的看法，这对印度政府的疫苗接种计划 pose 一定的挑战。<details>
<summary>Abstract</summary>
In March 2020, the World Health Organisation declared COVID-19 a global pandemic as it spread to nearly every country. By mid-2021, India had introduced three vaccines: Covishield, Covaxin, and Sputnik. To ensure successful vaccination in a densely populated country like India, understanding public sentiment was crucial. Social media, particularly Reddit with over 430 million users, played a vital role in disseminating information. This study employs data mining techniques to analyze Reddit data and gauge Indian sentiments towards COVID-19 vaccines. Using Python's Text Blob library, comments are annotated to assess general sentiments. Results show that most Reddit users in India expressed neutrality about vaccination, posing a challenge for the Indian government's efforts to vaccinate a significant portion of the population.
</details>
<details>
<summary>摘要</summary>
三月2020年，世界卫生组织宣布COVID-19为全球大流行，其已在大多数国家传播。到2021年中期，印度已经推出三种疫苗：Covishield、Covaxin和Sputnik。为了在印度的高度密集化国家中成功进行疫苗接种，了解公众情绪非常重要。社交媒体，特别是Reddit（拥有430万用户），在传播信息方面发挥了重要作用。这项研究使用数据挖掘技术来分析Reddit数据，评估印度人民对COVID-19疫苗的情绪。使用Python的Text Blob库，评论被注释以评估总体情绪。结果显示，大多数Reddit用户在印度表达了中性的看法，对印度政府的大规模接种计划提出了挑战。
</details></li>
</ul>
<hr>
<h2 id="Appearance-Codes-using-Joint-Embedding-Learning-of-Multiple-Modalities"><a href="#Appearance-Codes-using-Joint-Embedding-Learning-of-Multiple-Modalities" class="headerlink" title="Appearance Codes using Joint Embedding Learning of Multiple Modalities"></a>Appearance Codes using Joint Embedding Learning of Multiple Modalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11427">http://arxiv.org/abs/2311.11427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edogariu/alex-zhang">https://github.com/edogariu/alex-zhang</a></li>
<li>paper_authors: Alex Zhang, Evan Dogariu</li>
<li>for: 该文章是为了解决现有的生成模型中的一个主要限制，即需要在推理时重新训练新的外观代码。</li>
<li>methods: 该文章提出了一种框架，即在推理时强制 enforcing a contrastive loss constraint  между不同的Modalities，以学习场景的 appearances 和结构的共同embedding空间。</li>
<li>results: 该文章通过应用该框架于 RADIATE 数据集 \cite{sheeny2021radiate} 中的一个简单的 Variational Auto-Encoder 模型，并质量地示出了通过使用日间外观代码生成夜间照片的能力。此外，该文章还与基准 VAE 相比较，并显示了该方法在推理时不需要学习任何未看过的图像的外观代码。<details>
<summary>Abstract</summary>
The use of appearance codes in recent work on generative modeling has enabled novel view renders with variable appearance and illumination, such as day-time and night-time renders of a scene. A major limitation of this technique is the need to re-train new appearance codes for every scene on inference, so in this work we address this problem proposing a framework that learns a joint embedding space for the appearance and structure of the scene by enforcing a contrastive loss constraint between different modalities. We apply our framework to a simple Variational Auto-Encoder model on the RADIATE dataset \cite{sheeny2021radiate} and qualitatively demonstrate that we can generate new renders of night-time photos using day-time appearance codes without additional optimization iterations. Additionally, we compare our model to a baseline VAE that uses the standard per-image appearance code technique and show that our approach achieves generations of similar quality without learning appearance codes for any unseen images on inference.
</details>
<details>
<summary>摘要</summary>
Recent work on generative modeling 使用表现代码(appearance code) 实现了新的视图渲染，如Scene中的日间和夜间渲染。然而，这种技术的主要限制是需要在推理时重新训练新的表现代码，因此在这里我们提出了一种框架，强制对不同模式之间的对比损失约束，以学习场景的共同embedding空间。我们在RADIATE数据集\cite{sheeny2021radiate}上应用了我们的框架，并质量地示出了使用日间表现代码生成夜间照片的能力。此外，我们与基准VAE模型进行比较，并证明我们的方法可以在推理时生成相同质量的图像，而无需学习任何未看过的图像的表现代码。
</details></li>
</ul>
<hr>
<h2 id="LifeLearner-Hardware-Aware-Meta-Continual-Learning-System-for-Embedded-Computing-Platforms"><a href="#LifeLearner-Hardware-Aware-Meta-Continual-Learning-System-for-Embedded-Computing-Platforms" class="headerlink" title="LifeLearner: Hardware-Aware Meta Continual Learning System for Embedded Computing Platforms"></a>LifeLearner: Hardware-Aware Meta Continual Learning System for Embedded Computing Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11420">http://arxiv.org/abs/2311.11420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young D. Kwon, Jagmohan Chauhan, Hong Jia, Stylianos I. Venieris, Cecilia Mascolo</li>
<li>for: 本研究旨在开发一种具有硬件意识的元 continual learning 系统，以提高资源受限的嵌入式设备上的学习效率和适应能力。</li>
<li>methods: 本研究使用了元学习和复习策略，以解决数据稀缺问题并保证高准确率。同时，使用了lossless和lossy压缩技术来减少 CL 和复习样本的资源需求。</li>
<li>results: 研究结果显示，LifeLearner 可以实现近似于最优 CL 性能，与 oracle 基准值相差只有2.8%。相比之下，与 SOTA Meta CL 方法相比，LifeLearner 可以减少内存占用量（178.7倍）、终端延迟（80.8-94.2%）和能 consumption（80.9-94.2%）。此外，研究者还成功部署 LifeLearner 在两个边缘设备和一个微控制器Unit上，从而实现了资源受限平台上高效的 CL 部署。<details>
<summary>Abstract</summary>
Continual Learning (CL) allows applications such as user personalization and household robots to learn on the fly and adapt to context. This is an important feature when context, actions, and users change. However, enabling CL on resource-constrained embedded systems is challenging due to the limited labeled data, memory, and computing capacity. In this paper, we propose LifeLearner, a hardware-aware meta continual learning system that drastically optimizes system resources (lower memory, latency, energy consumption) while ensuring high accuracy. Specifically, we (1) exploit meta-learning and rehearsal strategies to explicitly cope with data scarcity issues and ensure high accuracy, (2) effectively combine lossless and lossy compression to significantly reduce the resource requirements of CL and rehearsal samples, and (3) developed hardware-aware system on embedded and IoT platforms considering the hardware characteristics. As a result, LifeLearner achieves near-optimal CL performance, falling short by only 2.8% on accuracy compared to an Oracle baseline. With respect to the state-of-the-art (SOTA) Meta CL method, LifeLearner drastically reduces the memory footprint (by 178.7x), end-to-end latency by 80.8-94.2%, and energy consumption by 80.9-94.2%. In addition, we successfully deployed LifeLearner on two edge devices and a microcontroller unit, thereby enabling efficient CL on resource-constrained platforms where it would be impractical to run SOTA methods and the far-reaching deployment of adaptable CL in a ubiquitous manner. Code is available at https://github.com/theyoungkwon/LifeLearner.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>利用元学习和熵策略，直接面对数据缺乏问题，并保证高精度。2. 有效地结合lossless和lossy压缩，以减少 CL 和熵样本的资源需求。3. 对嵌入式和互联网平台进行硬件意识系统开发，考虑硬件特点。因此，LifeLearner 实现了几乎最佳的 CL 性能，相比于 Oracle 基线下，只减少了2.8%的精度。相比于状态之前（SOTA）元 CL 方法，LifeLearner 减少了内存占用量（by 178.7x）、终端延迟（by 80.8-94.2%）和能 consumption（by 80.9-94.2%）。此外，我们成功部署 LifeLearner 在两个边缘设备和一个微控制器单元上，因此实现了高效的 CL 在有限资源平台上，这些平台上运行 SOTA 方法是不实际的。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/theyoungkwon/LifeLearner">https://github.com/theyoungkwon/LifeLearner</a> 上下载。</details></li>
</ol>
<hr>
<h2 id="A-Security-Risk-Taxonomy-for-Large-Language-Models"><a href="#A-Security-Risk-Taxonomy-for-Large-Language-Models" class="headerlink" title="A Security Risk Taxonomy for Large Language Models"></a>A Security Risk Taxonomy for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11415">http://arxiv.org/abs/2311.11415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erik Derner, Kristina Batistič, Jan Zahálka, Robert Babuška</li>
<li>for: 这篇论文旨在评估大语言模型（LLM）的安全风险，包括诈骗、数据泄露和声誉损害等。</li>
<li>methods: 本论文提出了一个基于用户-模型交互pipeline的安全风险分类方法，包括提示型攻击。</li>
<li>results: 研究发现了许多具体的攻击示例，以及它们在实际应用中的影响。这些攻击包括诈骗、数据泄露和声誉损害等。<details>
<summary>Abstract</summary>
As large language models (LLMs) permeate more and more applications, an assessment of their associated security risks becomes increasingly necessary. The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial. This paper addresses a gap in current research by focusing on the security risks posed by LLMs, which extends beyond the widely covered ethical and societal implications. Our work proposes a taxonomy of security risks along the user-model communication pipeline, explicitly focusing on prompt-based attacks on LLMs. We categorize the attacks by target and attack type within a prompt-based interaction scheme. The taxonomy is reinforced with specific attack examples to showcase the real-world impact of these risks. Through this taxonomy, we aim to inform the development of robust and secure LLM applications, enhancing their safety and trustworthiness.
</details>
<details>
<summary>摘要</summary>
As large language models (LLMs) 普遍应用在更多应用程序中，评估这些模型相关的安全风险成为更加必要。这些模型的潜在问题包括伪信息、数据泄露和声誉伤害等，具体的威胁来源涉及访问者的黑客、恶意攻击者和无良用户。本文对现有的研究缺失进行了补充，专注于 LLMs 相关的安全风险，这些风险超出了广泛讨论的伦理和社会因素。我们的工作提出了一个对 LLMs 进行攻击的分类方案，并将攻击分为三类：内部攻击、外部攻击和混合攻击。这个分类方案以对话领域为基础，并提供具体的攻击示例，以显示这些风险对现实世界的影响。通过这个分类方案，我们想帮助开发安全和可靠的 LLM 应用程序，从而提高这些应用程序的安全性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Make-me-an-Offer-Forward-and-Reverse-Auctioning-Problems-in-the-Tourism-Industry"><a href="#Make-me-an-Offer-Forward-and-Reverse-Auctioning-Problems-in-the-Tourism-Industry" class="headerlink" title="Make me an Offer: Forward and Reverse Auctioning Problems in the Tourism Industry"></a>Make me an Offer: Forward and Reverse Auctioning Problems in the Tourism Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11400">http://arxiv.org/abs/2311.11400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis T. Christou, Dimitris Doukas, Konstantina Skouri, Gerasimos Meletiou</li>
<li>for: 帮助酒店iers和旅游者解决常见的季节性问题，提高旅游业的经济效益和社会影响。</li>
<li>methods: 开发了两种拍卖系统，一种是向下拍卖模型，允许低知名度地区或低季节期酒店拍卖房间，另一种是由客人发起的反拍卖模型，类似于priceline.com的拍卖概念，allowing customers to initiate a bidding process whereby hoteliers in an area may make offers to the customer for their rooms。</li>
<li>results: 通过数学Programming模型定义两种拍卖，并证明在每种中，都有significant benefits for both sides of the hotelier and the customer。<details>
<summary>Abstract</summary>
Most tourist destinations are facing regular and consistent seasonality with significant economic and social impacts. This phenomenon is more pronounced in the post-covid era, where demand for travel has increased but unevenly among different geographic areas. To counter these problems that both customers and hoteliers are facing, we have developed two auctioning systems that allow hoteliers of lower popularity tier areas or during low season periods to auction their rooms in what we call a forward auction model, and also allows customers to initiate a bidding process whereby hoteliers in an area may make offers to the customer for their rooms, in what constitutes a reverse auction model initiated by the customer, similar to the bidding concept of priceline.com. We develop mathematical programming models that define explicitly both types of auctions, and show that in each type, there are significant benefits to be gained both on the side of the hotelier as well as on the side of the customer. We discuss algorithmic techniques for the approximate solution of these optimization problems, and present results using exact optimization solvers to solve them to guaranteed optimality. These techniques could be beneficial to both customer and hotelier reducing seasonality during middle and low season and providing the customer with attractive offers.
</details>
<details>
<summary>摘要</summary>
多个旅游目的地都面临了常见和稳定的季节性问题，这种现象在covid后期更加突出，旅游需求增加了，但不均匀分布在不同地理区域。为了解决客户和酒店经理面临的问题，我们开发了两种拍卖系统：一是向下层知名度地区或低季节期酒店拍卖房间的前向拍卖模式，二是让客户 initiat 竞拍过程，让酒店在区域内提供优惠套餐，类似于priceline.com的拍卖概念。我们定义了这两种拍卖模式的数学编程模型，并证明每种模型都有很大的利益。我们介绍了算法技术来解决这些优化问题，并使用精确优化器解决它们，以确保最优解。这些技术可以帮助客户和酒店减少中期和低季节的季节性问题，并为客户提供吸引人的优惠。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Novel-Object-Detection-via-Cooperative-Foundational-Models"><a href="#Enhancing-Novel-Object-Detection-via-Cooperative-Foundational-Models" class="headerlink" title="Enhancing Novel Object Detection via Cooperative Foundational Models"></a>Enhancing Novel Object Detection via Cooperative Foundational Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12068">http://arxiv.org/abs/2311.12068</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rohit901/cooperative-foundational-models">https://github.com/rohit901/cooperative-foundational-models</a></li>
<li>paper_authors: Rohit Bharadwaj, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan</li>
<li>For: 本研究目标是解决 novel object detection (NOD) 问题，即在推理过程中准确地检测已知和新类目对象。传统的对象检测算法受到closed-set的限制，无法处理 NOD。* Methods: 我们提出了一种将现有的 closed-set 检测器转化为 open-set 检测器的方法，利用 CLIP 和 SAM 两种预训练基本模型的优势。我们还与 state-of-the-art 的 open-set 检测器 GDINO 集成，以达到更高的对象检测性能。* Results: 我们在 LVIS 数据集上实现了 17.42 mAP 的新对象检测精度，并在 COCO OVD 分 split 上超过当前状态的报告值。我们的代码可以在 <a target="_blank" rel="noopener" href="https://github.com/rohit901/cooperative-foundational-models">https://github.com/rohit901/cooperative-foundational-models</a> 上下载。<details>
<summary>Abstract</summary>
In this work, we address the challenging and emergent problem of novel object detection (NOD), focusing on the accurate detection of both known and novel object categories during inference. Traditional object detection algorithms are inherently closed-set, limiting their capability to handle NOD. We present a novel approach to transform existing closed-set detectors into open-set detectors. This transformation is achieved by leveraging the complementary strengths of pre-trained foundational models, specifically CLIP and SAM, through our cooperative mechanism. Furthermore, by integrating this mechanism with state-of-the-art open-set detectors such as GDINO, we establish new benchmarks in object detection performance. Our method achieves 17.42 mAP in novel object detection and 42.08 mAP for known objects on the challenging LVIS dataset. Adapting our approach to the COCO OVD split, we surpass the current state-of-the-art by a margin of 7.2 $ \text{AP}_{50} $ for novel classes. Our code is available at https://github.com/rohit901/cooperative-foundational-models .
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们 Addressing the challenging and emerging problem of novel object detection (NOD), we focus on accurately detecting both known and novel object categories during inference. Traditional object detection algorithms are inherently closed-set, limiting their ability to handle NOD. We present a novel approach to transform existing closed-set detectors into open-set detectors. This transformation is achieved by leveraging the complementary strengths of pre-trained foundational models, specifically CLIP and SAM, through our cooperative mechanism. Furthermore, by integrating this mechanism with state-of-the-art open-set detectors such as GDINO, we establish new benchmarks in object detection performance. Our method achieves 17.42 mAP in novel object detection and 42.08 mAP for known objects on the challenging LVIS dataset. Adapting our approach to the COCO OVD split, we surpass the current state-of-the-art by a margin of 7.2 $\text{AP}_{50}$ for novel classes. Our code is available at <https://github.com/rohit901/cooperative-foundational-models>.Here's the word-for-word translation of the text into Simplified Chinese:在这项工作中，我们 Addressing the challenging and emerging problem of novel object detection (NOD), we focus on accurately detecting both known and novel object categories during inference. Traditional object detection algorithms are inherently closed-set, limiting their ability to handle NOD. We present a novel approach to transform existing closed-set detectors into open-set detectors. This transformation is achieved by leveraging the complementary strengths of pre-trained foundational models, specifically CLIP and SAM, through our cooperative mechanism. Furthermore, by integrating this mechanism with state-of-the-art open-set detectors such as GDINO, we establish new benchmarks in object detection performance. Our method achieves 17.42 mAP in novel object detection and 42.08 mAP for known objects on the challenging LVIS dataset. Adapting our approach to the COCO OVD split, we surpass the current state-of-the-art by a margin of 7.2 $\text{AP}_{50}$ for novel classes. Our code is available at <https://github.com/rohit901/cooperative-foundational-models>.
</details></li>
</ul>
<hr>
<h2 id="Inspecting-Explainability-of-Transformer-Models-with-Additional-Statistical-Information"><a href="#Inspecting-Explainability-of-Transformer-Models-with-Additional-Statistical-Information" class="headerlink" title="Inspecting Explainability of Transformer Models with Additional Statistical Information"></a>Inspecting Explainability of Transformer Models with Additional Statistical Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11378">http://arxiv.org/abs/2311.11378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoang C. Nguyen, Haeil Lee, Junmo Kim</li>
<li>for: 这篇论文的目的是为了有效地解释Transformer模型在视觉和多 modal任务中的含义。</li>
<li>methods: 这篇论文使用了将注意层组合起来，以显示每个图像块的重要性。</li>
<li>results: 这篇论文表明了对Swin Transformer和ViT的解释性能力很强，并且可以准确地显示预测对象。<details>
<summary>Abstract</summary>
Transformer becomes more popular in the vision domain in recent years so there is a need for finding an effective way to interpret the Transformer model by visualizing it. In recent work, Chefer et al. can visualize the Transformer on vision and multi-modal tasks effectively by combining attention layers to show the importance of each image patch. However, when applying to other variants of Transformer such as the Swin Transformer, this method can not focus on the predicted object. Our method, by considering the statistics of tokens in layer normalization layers, shows a great ability to interpret the explainability of Swin Transformer and ViT.
</details>
<details>
<summary>摘要</summary>
孔 transformed 在视觉领域的 популяр度在最近几年内逐渐增长，因此有需要找到一种有效地 interpret 孔 transformed 模型的方法。在最近的工作中，Chefer et al. 通过组合注意层来有效地视觉和多modal任务中的孔 transformed 模型。然而，当应用于其他孔 transformed 模型，如 Swin Transformer 和 ViT 时，这种方法无法关注预测的对象。我们的方法，通过考虑层normalization层中的统计数据，显示了对 Swin Transformer 和 ViT 的解释性能出色。
</details></li>
</ul>
<hr>
<h2 id="SOccDPT-Semi-Supervised-3D-Semantic-Occupancy-from-Dense-Prediction-Transformers-trained-under-memory-constraints"><a href="#SOccDPT-Semi-Supervised-3D-Semantic-Occupancy-from-Dense-Prediction-Transformers-trained-under-memory-constraints" class="headerlink" title="SOccDPT: Semi-Supervised 3D Semantic Occupancy from Dense Prediction Transformers trained under memory constraints"></a>SOccDPT: Semi-Supervised 3D Semantic Occupancy from Dense Prediction Transformers trained under memory constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11371">http://arxiv.org/abs/2311.11371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Nalgunda Ganesh</li>
<li>for: 这种paper是为了提出一种能够在封闭环境中进行3Dsemantic occupancy prediction的方法，以提高现有方法的性能。</li>
<li>methods: 这种方法使用dense prediction transformers来进行预测，并使用 semi-supervised training pipeline来学习从不结构化交通数据集中。另外，它还引入了patch-wise training来解决内存限制问题。</li>
<li>results: 这种方法在不结构化交通场景下表现出色，与现有方法相比，它的RMSE分数为9.1473， semantic segmentation IoU分数为46.02%，并且能够运行在69.47 Hz的频率上。<details>
<summary>Abstract</summary>
We present SOccDPT, a memory-efficient approach for 3D semantic occupancy prediction from monocular image input using dense prediction transformers. To address the limitations of existing methods trained on structured traffic datasets, we train our model on unstructured datasets including the Indian Driving Dataset and Bengaluru Driving Dataset. Our semi-supervised training pipeline allows SOccDPT to learn from datasets with limited labels by reducing the requirement for manual labelling by substituting it with pseudo-ground truth labels to produce our Bengaluru Semantic Occupancy Dataset. This broader training enhances our model's ability to handle unstructured traffic scenarios effectively. To overcome memory limitations during training, we introduce patch-wise training where we select a subset of parameters to train each epoch, reducing memory usage during auto-grad graph construction. In the context of unstructured traffic and memory-constrained training and inference, SOccDPT outperforms existing disparity estimation approaches as shown by the RMSE score of 9.1473, achieves a semantic segmentation IoU score of 46.02% and operates at a competitive frequency of 69.47 Hz. We make our code and semantic occupancy dataset public.
</details>
<details>
<summary>摘要</summary>
我们提出了SOccDPT，一种具有内存效率的方法，用于从单视图图像输入获取3D semantic occupancy预测。为了解决现有方法在结构化交通数据集上的局限性，我们将我们的模型训练在无结构数据集上，包括印度驾驶数据集和孟买丹驾驶数据集。我们的半supervised训练管道使得SOccDPT可以从有限标签的数据集中学习，而不需要手动标注。为了增强我们的模型对无结构交通场景的处理能力，我们引入了patch-wise训练，将每个epoch中的一 subset of参数选择训练。在自动梯度图构建过程中，我们减少了训练过程中的内存使用。在无结构交通和内存受限的训练和执行环境下，SOccDPT比现有的不同差分估计方法表现更好，RMSE分数为9.1473，实现semantic segmentation IoU分数为46.02%，并在竞争性的69.47 Hz频率下运行。我们将我们的代码和semantic occupancy数据集公开。
</details></li>
</ul>
<hr>
<h2 id="Using-Causal-Threads-to-Explain-Changes-in-a-Dynamic-System"><a href="#Using-Causal-Threads-to-Explain-Changes-in-a-Dynamic-System" class="headerlink" title="Using Causal Threads to Explain Changes in a Dynamic System"></a>Using Causal Threads to Explain Changes in a Dynamic System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11334">http://arxiv.org/abs/2311.11334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert B. Allen</li>
<li>for: 这篇论文主要用于构建系统的 semantics 模型，具体来说是用于描述系统中状态变化的 causal 解释。</li>
<li>methods: 该论文使用 structured causal explanations 和 process-based dynamic knowledge graphs 来建立系统的 semantics 模型。</li>
<li>results: 通过 Snowball Earth 理论中的地质变化的 causal 线索，构建了一个初步的图形界面来展示解释。与统计方法such as Large Language Models (LLMs)不同，该方法可以直接被检查和验证。<details>
<summary>Abstract</summary>
We explore developing rich semantic models of systems. Specifically, we consider structured causal explanations about state changes in those systems. Essentially, we are developing process-based dynamic knowledge graphs. As an example, we construct a model of the causal threads for geological changes proposed by the Snowball Earth theory. Further, we describe an early prototype of a graphical interface to present the explanations. Unlike statistical approaches to summarization and explanation such as Large Language Models (LLMs), our approach of direct representation can be inspected and verified directly.
</details>
<details>
<summary>摘要</summary>
我们探索构建丰富Semantic模型系统。特别是，我们考虑结构化 causal 解释系统状态变化。基本上，我们正在构建基于过程的动态知识图。例如，我们构建了 Snowball Earth 理论提出的地质变化 causal 线索模型。此外，我们描述了一个早期的图形用户界面来展示解释。与统计方法such as Large Language Models (LLMs)不同，我们的直接表示方法可以直接检查和验证。
</details></li>
</ul>
<hr>
<h2 id="Portuguese-FAQ-for-Financial-Services"><a href="#Portuguese-FAQ-for-Financial-Services" class="headerlink" title="Portuguese FAQ for Financial Services"></a>Portuguese FAQ for Financial Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11331">http://arxiv.org/abs/2311.11331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paulo Finardi, Wanderley M. Melo, Edgard D. Medeiros Neto, Alex F. Mansano, Pablo B. Costa, Vinicius F. Caridá</li>
<li>for: 提高葡萄牙金融领域自然语言处理（NLP）应用的发展，因为当地数据的缺乏限制了NLP应用的研究和开发。</li>
<li>methods: 使用数据增强技术生成具有不同semantic similarity的数据集，并在超级vised和无监督任务中评估增强数据的影响。</li>
<li>results: 通过数据增强，提高了NLP应用中的性能，并且在low和high semantic similarity场景下都能够获得良好的效果。同时，生成的数据集将被公开发布在Hugging Face Datasets平台上，以便更广泛的学术社区参与和交流。<details>
<summary>Abstract</summary>
Scarcity of domain-specific data in the Portuguese financial domain has disfavored the development of Natural Language Processing (NLP) applications. To address this limitation, the present study advocates for the utilization of synthetic data generated through data augmentation techniques. The investigation focuses on the augmentation of a dataset sourced from the Central Bank of Brazil FAQ, employing techniques that vary in semantic similarity. Supervised and unsupervised tasks are conducted to evaluate the impact of augmented data on both low and high semantic similarity scenarios. Additionally, the resultant dataset will be publicly disseminated on the Hugging Face Datasets platform, thereby enhancing accessibility and fostering broader engagement within the NLP research community.
</details>
<details>
<summary>摘要</summary>
缺乏特定领域数据的问题在葡萄牙金融领域妨碍了自然语言处理（NLP）应用的发展。为解决这个限制，当前的研究建议利用数据增强技术生成的 sintética data。研究将对来自中南美洲中央银行FAQ的数据进行增强，使用不同的semantic similarity技术。在低和高semantic similarity情况下，进行了supervised和unsupervised任务来评估增强数据的影响。此外，生成的数据集将公开发布在Hugging Face Datasets平台上，从而提高了可访问性和推动了更广泛的NLP研究社区参与。
</details></li>
</ul>
<hr>
<h2 id="Bounds-on-Representation-Induced-Confounding-Bias-for-Treatment-Effect-Estimation"><a href="#Bounds-on-Representation-Induced-Confounding-Bias-for-Treatment-Effect-Estimation" class="headerlink" title="Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation"></a>Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11321">http://arxiv.org/abs/2311.11321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentyn Melnychuk, Dennis Frauen, Stefan Feuerriegel</li>
<li>for: 该论文旨在提出一种新的、不受表示学习的束缚的极限 bounds 估计方法，用于评估 conditional average treatment effect (CATE) 估计中的受限制的束缚偏见。</li>
<li>methods: 该论文使用了一种新的、 representation-agnostic 框架，用于估计 CATE 估计中的受限制的束缚偏见。该框架包括一种 theoretically 确定 CATE 不可识别性的条件，以及一种用于估计受限制的束缚偏见的方法。</li>
<li>results: 该论文通过一系列实验证明了该 bounds 估计方法的效果iveness。具体来说，该方法可以减少 CATE 估计中的受限制的束缚偏见，并提供一种irect relevance 的方法来评估 CATE 估计的有效性。<details>
<summary>Abstract</summary>
State-of-the-art methods for conditional average treatment effect (CATE) estimation make widespread use of representation learning. Here, the idea is to reduce the variance of the low-sample CATE estimation by a (potentially constrained) low-dimensional representation. However, low-dimensional representations can lose information about the observed confounders and thus lead to bias, because of which the validity of representation learning for CATE estimation is typically violated. In this paper, we propose a new, representation-agnostic framework for estimating bounds on the representation-induced confounding bias that comes from dimensionality reduction (or other constraints on the representations) in CATE estimation. First, we establish theoretically under which conditions CATEs are non-identifiable given low-dimensional (constrained) representations. Second, as our remedy, we propose to perform partial identification of CATEs or, equivalently, aim at estimating of lower and upper bounds of the representation-induced confounding bias. We demonstrate the effectiveness of our bounds in a series of experiments. In sum, our framework is of direct relevance in practice where the validity of CATE estimation is of importance.
</details>
<details>
<summary>摘要</summary>
现代方法 для条件平均治疗效果（CATE）估计广泛使用表示学习。这里的想法是通过减小低样本CATE估计的方差，使用（可能受限的）低维度表示。然而，低维度表示可能会产生对观察的隐藏变量的损失信息，从而导致偏误，因此表示学习在CATE估计中的有效性通常会被违反。在这篇论文中，我们提出了一个新的、表示无关的框架，用于估计受约束表示带来的 repression-induced confounding bias 的上下限。首先，我们理论上证明在低维度（受约束）表示下，CATE是非可定义的。其次，作为我们的救济方法，我们提议在CATE估计中进行部分标识，或等价地，估计 repression-induced confounding bias 的下限和上限。我们在一系列实验中证明了我们的上限的有效性。总之，我们的框架在实践中对CATE估计的有效性具有直接的重要性。
</details></li>
</ul>
<hr>
<h2 id="GeoSAM-Fine-tuning-SAM-with-Sparse-and-Dense-Visual-Prompting-for-Automated-Segmentation-of-Mobility-Infrastructure"><a href="#GeoSAM-Fine-tuning-SAM-with-Sparse-and-Dense-Visual-Prompting-for-Automated-Segmentation-of-Mobility-Infrastructure" class="headerlink" title="GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure"></a>GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11319">http://arxiv.org/abs/2311.11319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafi Ibn Sultan, Chengyin Li, Hui Zhu, Prashant Khanduri, Marco Brocanelli, Dongxiao Zhu</li>
<li>for: 减少地图图像中的交通基础设施 segmentation 难题</li>
<li>methods: 提出了一种基于 SAM 框架的 Geographical SAM（GeoSAM）模型，通过zero-shot learning 精度提升策略和基于预训练 CNN 分割模型的稀疏视觉提示策略</li>
<li>results: GeoSAM 模型在地图图像中的交通基础设施分割 tasks 上表现出优于现有方法，具体提高了20%, 14.29%, 17.65% 等，代表了对地图图像中交通基础设施分割任务的很大进步。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) has shown impressive performance when applied to natural image segmentation. However, it struggles with geographical images like aerial and satellite imagery, especially when segmenting mobility infrastructure including roads, sidewalks, and crosswalks. This inferior performance stems from the narrow features of these objects, their textures blending into the surroundings, and interference from objects like trees, buildings, vehicles, and pedestrians - all of which can disorient the model to produce inaccurate segmentation maps. To address these challenges, we propose Geographical SAM (GeoSAM), a novel SAM-based framework that implements a fine-tuning strategy using the dense visual prompt from zero-shot learning, and the sparse visual prompt from a pre-trained CNN segmentation model. The proposed GeoSAM outperforms existing approaches for geographical image segmentation, specifically by 20%, 14.29%, and 17.65% for road infrastructure, pedestrian infrastructure, and on average, respectively, representing a momentous leap in leveraging foundation models to segment mobility infrastructure including both road and pedestrian infrastructure in geographical images.
</details>
<details>
<summary>摘要</summary>
《Segment Anything Model》（SAM）在自然图像分割 tasks 上表现出色，但在飞行和卫星图像中分割交通基础设施，如道路、人行道和横渡道，却表现不佳，主要原因在于这些物体的窄特征，Texture与周围环境杂mix，以及对象如树、建筑、车辆和步行人的干扰，这些因素会让模型产生错误的分割图像。为Address这些挑战，我们提出了《地理SAM》（GeoSAM），一种基于SAM的novel框架，通过适应策略和零shot学习 dense visual prompt、以及预训练 CNN segmentation模型的稀疏 visual prompt来解决这些问题。我们的提议GeoSAM在地理图像分割 tasks 上比 existed Approach 高出20%, 14.29%, 和17.65%，分别表示了在基础模型上 segments mobility 基础设施，包括道路和人行道基础设施的分割，即使在飞行和卫星图像中，具有巨大的进步。
</details></li>
</ul>
<hr>
<h2 id="TPTU-v2-Boosting-Task-Planning-and-Tool-Usage-of-Large-Language-Model-based-Agents-in-Real-world-Systems"><a href="#TPTU-v2-Boosting-Task-Planning-and-Tool-Usage-of-Large-Language-Model-based-Agents-in-Real-world-Systems" class="headerlink" title="TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems"></a>TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11315">http://arxiv.org/abs/2311.11315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, Xingyu Zeng, Rui Zhao</li>
<li>for: 提高基于 LLM 的任务规划和工具使用能力，在真实世界系统中运行</li>
<li>methods: 提出了一个完整的框架，包括 API 选取器、 LLF 练化器和示例选取器，用于解决真实世界系统中的三大挑战</li>
<li>results: 验证了方法使用真实世界商业系统和开源学术数据集，结果表明每个组件都有效，同时集成的框架也能够提高任务规划和工具使用能力<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated proficiency in addressing tasks that necessitate a combination of task planning and the usage of external tools that require a blend of task planning and the utilization of external tools, such as APIs. However, real-world complex systems present three prevalent challenges concerning task planning and tool usage: (1) The real system usually has a vast array of APIs, so it is impossible to feed the descriptions of all APIs to the prompt of LLMs as the token length is limited; (2) the real system is designed for handling complex tasks, and the base LLMs can hardly plan a correct sub-task order and API-calling order for such tasks; (3) Similar semantics and functionalities among APIs in real systems create challenges for both LLMs and even humans in distinguishing between them. In response, this paper introduces a comprehensive framework aimed at enhancing the Task Planning and Tool Usage (TPTU) abilities of LLM-based agents operating within real-world systems. Our framework comprises three key components designed to address these challenges: (1) the API Retriever selects the most pertinent APIs for the user task among the extensive array available; (2) LLM Finetuner tunes a base LLM so that the finetuned LLM can be more capable for task planning and API calling; (3) the Demo Selector adaptively retrieves different demonstrations related to hard-to-distinguish APIs, which is further used for in-context learning to boost the final performance. We validate our methods using a real-world commercial system as well as an open-sourced academic dataset, and the outcomes clearly showcase the efficacy of each individual component as well as the integrated framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="What-Lies-beyond-the-Pareto-Front-A-Survey-on-Decision-Support-Methods-for-Multi-Objective-Optimization"><a href="#What-Lies-beyond-the-Pareto-Front-A-Survey-on-Decision-Support-Methods-for-Multi-Objective-Optimization" class="headerlink" title="What Lies beyond the Pareto Front? A Survey on Decision-Support Methods for Multi-Objective Optimization"></a>What Lies beyond the Pareto Front? A Survey on Decision-Support Methods for Multi-Objective Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11288">http://arxiv.org/abs/2311.11288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zuzanna Osika, Jazmin Zatarain Salazar, Diederik M. Roijers, Frans A. Oliehoek, Pradeep K. Murukannaiah</li>
<li>for: 本文旨在总结和探讨多目标优化算法生成的解决方案的决策支持方法。</li>
<li>methods: 本文涵盖了多元优化算法解决问题时的解决方案分析方法，包括视觉化、解决集挖掘、不确定性探索以及emerging研究方向，如交互、解释性和伦理。</li>
<li>results: 本文synthesizes多种研究领域的方法，建立了一个独立于应用领域的统一方法，以降低使用多元优化算法的入门难度，并提供了新的研究方向。<details>
<summary>Abstract</summary>
We present a review that unifies decision-support methods for exploring the solutions produced by multi-objective optimization (MOO) algorithms. As MOO is applied to solve diverse problems, approaches for analyzing the trade-offs offered by MOO algorithms are scattered across fields. We provide an overview of the advances on this topic, including methods for visualization, mining the solution set, and uncertainty exploration as well as emerging research directions, including interactivity, explainability, and ethics. We synthesize these methods drawing from different fields of research to build a unified approach, independent of the application. Our goals are to reduce the entry barrier for researchers and practitioners on using MOO algorithms and to provide novel research directions.
</details>
<details>
<summary>摘要</summary>
我们提出了一篇文章，整合多目标优化（MOO）算法生成的解决方案的决策支持方法的评审。由于MOO在解决多种问题时使用，关于MOO算法的解决方案分析方法在不同领域中散布开来。我们提供了这些进展的概述，包括可视化、解决集挖掘、不确定性探索以及新兴研究方向，如互动、解释性和伦理。我们将这些方法从不同领域的研究中总结出来，建立一个独立于应用的统一方法，以降低MOO算法的入门难度，并提供新的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Tactile-Active-Inference-Reinforcement-Learning-for-Efficient-Robotic-Manipulation-Skill-Acquisition"><a href="#Tactile-Active-Inference-Reinforcement-Learning-for-Efficient-Robotic-Manipulation-Skill-Acquisition" class="headerlink" title="Tactile Active Inference Reinforcement Learning for Efficient Robotic Manipulation Skill Acquisition"></a>Tactile Active Inference Reinforcement Learning for Efficient Robotic Manipulation Skill Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11287">http://arxiv.org/abs/2311.11287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Liu, Xing Liu, Yizhai Zhang, Zhengxiong Liu, Panfeng Huang</li>
<li>for: 这个研究旨在提出一种基于感觉活动吸引学习（Tactile-AIRL）的新方法，以实现有效的 manipulate 训练。</li>
<li>methods: 这种方法通过结合RL和活动推理，并使用视觉感知器来提供细致的感知，以提高RL的训练效率和适应率。</li>
<li>results: 实验结果表明，这种方法在非握持对象推力任务中可以达到显著高的训练效率，并在粗略和精细奖励任务中均表现出色，超过了SAC基eline。 Physical experiments on a gripper screwing task also demonstrate the algorithm’s rapid learning capability and its potential for practical applications.<details>
<summary>Abstract</summary>
Robotic manipulation holds the potential to replace humans in the execution of tedious or dangerous tasks. However, control-based approaches are not suitable due to the difficulty of formally describing open-world manipulation in reality, and the inefficiency of existing learning methods. Thus, applying manipulation in a wide range of scenarios presents significant challenges. In this study, we propose a novel method for skill learning in robotic manipulation called Tactile Active Inference Reinforcement Learning (Tactile-AIRL), aimed at achieving efficient training. To enhance the performance of reinforcement learning (RL), we introduce active inference, which integrates model-based techniques and intrinsic curiosity into the RL process. This integration improves the algorithm's training efficiency and adaptability to sparse rewards. Additionally, we utilize a vision-based tactile sensor to provide detailed perception for manipulation tasks. Finally, we employ a model-based approach to imagine and plan appropriate actions through free energy minimization. Simulation results demonstrate that our method achieves significantly high training efficiency in non-prehensile objects pushing tasks. It enables agents to excel in both dense and sparse reward tasks with just a few interaction episodes, surpassing the SAC baseline. Furthermore, we conduct physical experiments on a gripper screwing task using our method, which showcases the algorithm's rapid learning capability and its potential for practical applications.
</details>
<details>
<summary>摘要</summary>
人工 manipulate 潜在可以取代人类在干燥或危险任务中执行。然而，控制基于方法不适用，因为在实际开放世界中形式地描述 manipulate 是困难的，而且现有的学习方法不够有效。因此，在各种场景中应用 manipulate 存在 significante挑战。在这项研究中，我们提出了一种新的技术 для manipulate 的技能学习，称为感觉主动吸引强化学习（Tactile-AIRL），以实现高效培训。为了提高强化学习（RL）的表现，我们引入了活动推理，将模型基本技术和内生好奇纳入RL过程中。这种整合可以提高算法的培训效率和适应缺乏奖励的适应力。此外，我们利用视觉基于感觉器来提供细致的感知 для manipulate 任务。最后，我们采用模型基本方法来假设和规划适当的动作，通过自由能量减少来做出最佳的行为。在非握持物推力任务上，我们的方法在训练效率方面取得了显著的进步，可以在很少的互动集上达到SAC基准的性能。此外，我们在一个吊钢钉卷任务中使用我们的方法进行物理实验，结果表明了算法的快速学习能力和实际应用的潜力。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Prompt-Tuning-for-Vision-Language-Models"><a href="#Adversarial-Prompt-Tuning-for-Vision-Language-Models" class="headerlink" title="Adversarial Prompt Tuning for Vision-Language Models"></a>Adversarial Prompt Tuning for Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11261">http://arxiv.org/abs/2311.11261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaming Zhang, Xingjun Ma, Xin Wang, Lingyu Qiu, Jiaqi Wang, Yu-Gang Jiang, Jitao Sang</li>
<li>for: 本研究旨在提高多modal学习中预训练的视觉语言模型（VLM）对攻击图像的Robustness。</li>
<li>methods: 本文引入了一种新的技术 called Adversarial Prompt Tuning（AdvPT），它利用可学习的文本提示和恶意图像嵌入之间的对应关系，以提高VLM的攻击图像Robustness。</li>
<li>results: 实验结果表明，AdvPT可以提高VLM的抗白盒和黑盒攻击性能，并且可以与现有的图像处理方法结合使用，进一步提高安全性能。<details>
<summary>Abstract</summary>
With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code will be available upon publication of the paper.
</details>
<details>
<summary>摘要</summary>
随着多modal学习的快速发展，预训练的视觉语言模型（VLM）如CLIP表现出了跨modalities的很好的桥接能力。然而，这些模型对抗攻击仍然存在很大的安全风险，特别是在图像模式下。这篇论文提出了一种新的技术——敏感提示调整（AdvPT），以提高VLM中图像Encoder的抗击攻击能力。AdvPT利用可学习的文本提示和敌对图像嵌入的对应关系，从而解决VLM中存在的抗击攻击漏洞，无需对模型参数进行较为广泛的训练或者改变模型结构。我们的实验结果表明，AdvPT可以提高对白盒和黑盒抗击攻击的抵抗力，并且与现有的图像处理基础技术结合使用时，可以进一步强化防御能力。这些发现开启了新的可能性，以提高VLM的安全性。我们的代码将在论文发表后提供。
</details></li>
</ul>
<hr>
<h2 id="Tensor-networks-for-interpretable-and-efficient-quantum-inspired-machine-learning"><a href="#Tensor-networks-for-interpretable-and-efficient-quantum-inspired-machine-learning" class="headerlink" title="Tensor networks for interpretable and efficient quantum-inspired machine learning"></a>Tensor networks for interpretable and efficient quantum-inspired machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11258">http://arxiv.org/abs/2311.11258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shi-Ju Ran, Gang Su</li>
<li>for: 这篇论文是为了探讨 tensor network 在深度机器学习中的应用和发展。</li>
<li>methods: 本文使用 tensor network 作为一种数学工具，通过其在量子机理学和多体物理中的强有力基础，提供了一种高度可解释的深度机器学习方案。</li>
<li>results: 本文综述了 tensor network 在深度机器学习中的激进进步，包括高度可解释的 ML 方案和高效的计算技术。此外，随着量子计算机的发展，tensor network 预期会推出novel的量子硬件实现方案，指向未来的“量子人工智能”。<details>
<summary>Abstract</summary>
It is a critical challenge to simultaneously gain high interpretability and efficiency with the current schemes of deep machine learning (ML). Tensor network (TN), which is a well-established mathematical tool originating from quantum mechanics, has shown its unique advantages on developing efficient ``white-box'' ML schemes. Here, we give a brief review on the inspiring progresses made in TN-based ML. On one hand, interpretability of TN ML is accommodated with the solid theoretical foundation based on quantum information and many-body physics. On the other hand, high efficiency can be rendered from the powerful TN representations and the advanced computational techniques developed in quantum many-body physics. With the fast development on quantum computers, TN is expected to conceive novel schemes runnable on quantum hardware, heading towards the ``quantum artificial intelligence'' in the forthcoming future.
</details>
<details>
<summary>摘要</summary>
Current deep machine learning (ML) schemes face a critical challenge in achieving both high interpretability and efficiency. Tensor network (TN), a well-established mathematical tool originating from quantum mechanics, has shown unique advantages in developing efficient "white-box" ML schemes. Here, we provide a brief review of the inspiring progress made in TN-based ML.On one hand, the interpretability of TN ML is rooted in the solid theoretical foundation based on quantum information and many-body physics. On the other hand, high efficiency can be achieved through the powerful TN representations and advanced computational techniques developed in quantum many-body physics. With the rapid development of quantum computers, TN is expected to create novel schemes that can run on quantum hardware, leading towards the "quantum artificial intelligence" of the future.
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Review-on-Sentiment-Analysis-Tasks-Approaches-and-Applications"><a href="#A-Comprehensive-Review-on-Sentiment-Analysis-Tasks-Approaches-and-Applications" class="headerlink" title="A Comprehensive Review on Sentiment Analysis: Tasks, Approaches and Applications"></a>A Comprehensive Review on Sentiment Analysis: Tasks, Approaches and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11250">http://arxiv.org/abs/2311.11250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudhanshu Kumar, Partha Pratim Roy, Debi Prosad Dogra, Byung-Gyu Kim</li>
<li>for: 这篇论文主要是为了介绍情绪分析（SA）的研究和发展，以及它在不同领域的应用。</li>
<li>methods: 论文使用了 Lexicon-based approach、Machine Learning和深度学习等方法来实现情绪分析。</li>
<li>results: 论文总结了情绪分析的挑战和机遇，并提供了不同领域的应用例子。<details>
<summary>Abstract</summary>
Sentiment analysis (SA) is an emerging field in text mining. It is the process of computationally identifying and categorizing opinions expressed in a piece of text over different social media platforms. Social media plays an essential role in knowing the customer mindset towards a product, services, and the latest market trends. Most organizations depend on the customer's response and feedback to upgrade their offered products and services. SA or opinion mining seems to be a promising research area for various domains. It plays a vital role in analyzing big data generated daily in structured and unstructured formats over the internet. This survey paper defines sentiment and its recent research and development in different domains, including voice, images, videos, and text. The challenges and opportunities of sentiment analysis are also discussed in the paper.   \keywords{Sentiment Analysis, Machine Learning, Lexicon-based approach, Deep Learning, Natural Language Processing}
</details>
<details>
<summary>摘要</summary>
《情感分析（SA）是一个出现的领域，它是通过计算机自动地判断和分类文本中表达的意见的过程。社交媒体在了解客户心态方面发挥了关键作用，因为它可以为企业提供客户反馈和评价，帮助企业改进产品和服务。 SA或意见挖掘在不同领域的研究中具有普遍的应用前景，它可以处理大量的互联网上的数据，包括文本、语音、图像和视频等。本评论 paper define了情感的定义和最新的研究发展，以及不同领域中的挑战和机会。 （ keywords：情感分析、机器学习、词典基本方法、深度学习、自然语言处理）
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Dandelion-Network-for-IoT-Intrusion-Detection"><a href="#Open-Set-Dandelion-Network-for-IoT-Intrusion-Detection" class="headerlink" title="Open Set Dandelion Network for IoT Intrusion Detection"></a>Open Set Dandelion Network for IoT Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11249">http://arxiv.org/abs/2311.11249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiashu Wu, Hao Dai, Kenneth B. Kent, Jerome Yen, Chengzhong Xu, Yang Wang<br>for: This paper aims to address the problem of intrusion detection in IoT devices, which is crucial due to the increasing use of IoT devices. However, traditional intrusion detection methods are not effective due to the data scarcity of IoT devices.methods: The proposed method, Open-Set Dandelion Network (OSDN), uses unsupervised heterogeneous domain adaptation in an open-set manner to transfer intrusion knowledge from a knowledge-rich source network intrusion domain to a data-scarce target IoT intrusion domain. The OSDN model forms the source domain into a dandelion-like feature space, and uses a target membership mechanism, dandelion angular separation mechanism, and dandelion embedding alignment mechanism to achieve better inter-category separability and intra-category compactness.results: The proposed OSDN model outperforms three state-of-the-art baseline methods by 16.9% in terms of intrusion detection accuracy. The comprehensive experiments on several intrusion datasets demonstrate the effectiveness of the OSDN model.<details>
<summary>Abstract</summary>
As IoT devices become widely, it is crucial to protect them from malicious intrusions. However, the data scarcity of IoT limits the applicability of traditional intrusion detection methods, which are highly data-dependent. To address this, in this paper we propose the Open-Set Dandelion Network (OSDN) based on unsupervised heterogeneous domain adaptation in an open-set manner. The OSDN model performs intrusion knowledge transfer from the knowledge-rich source network intrusion domain to facilitate more accurate intrusion detection for the data-scarce target IoT intrusion domain. Under the open-set setting, it can also detect newly-emerged target domain intrusions that are not observed in the source domain. To achieve this, the OSDN model forms the source domain into a dandelion-like feature space in which each intrusion category is compactly grouped and different intrusion categories are separated, i.e., simultaneously emphasising inter-category separability and intra-category compactness. The dandelion-based target membership mechanism then forms the target dandelion. Then, the dandelion angular separation mechanism achieves better inter-category separability, and the dandelion embedding alignment mechanism further aligns both dandelions in a finer manner. To promote intra-category compactness, the discriminating sampled dandelion mechanism is used. Assisted by the intrusion classifier trained using both known and generated unknown intrusion knowledge, a semantic dandelion correction mechanism emphasises easily-confused categories and guides better inter-category separability. Holistically, these mechanisms form the OSDN model that effectively performs intrusion knowledge transfer to benefit IoT intrusion detection. Comprehensive experiments on several intrusion datasets verify the effectiveness of the OSDN model, outperforming three state-of-the-art baseline methods by 16.9%.
</details>
<details>
<summary>摘要</summary>
随着互联网物联网（IoT）设备的普及，保护它们免受恶意攻击变得非常重要。然而，IoT数据的缺乏使得传统的攻击检测方法不能够应用。为此，本文提出了基于无监督多类领域适应的开放集合网络（OSDN）模型，用于更加准确地检测IoT攻击。在开放集合Setting下，OSDN模型还可以检测新兴的目标领域攻击，而这些攻击没有在源领域观察到。为解决数据缺乏问题，OSDN模型将源领域形成一个 LIKE 风格的特征空间，在该空间中，每个攻击类型都是紧凑的分组的，同时也强调了不同攻击类型之间的分离。然后，通过目标风车机制，将目标领域转化为另一个风车。接着，风车angular分离机制实现了更好的inter-category分离，而风车嵌入对齐机制进一步将两个风车嵌入更细的层次。为了促进内部分类紧凑，使用抽象样本风车机制，以增强混淆类别之间的分离。总的来说，OSDN模型通过 Transfer learning 来传递知识，以便更好地检测IoT攻击。对于多个攻击数据集的实验证明，OSDN模型的效果非常出色，与三个状态机制比例高达16.9%。
</details></li>
</ul>
<hr>
<h2 id="AtomXR-Streamlined-XR-Prototyping-with-Natural-Language-and-Immersive-Physical-Interaction"><a href="#AtomXR-Streamlined-XR-Prototyping-with-Natural-Language-and-Immersive-Physical-Interaction" class="headerlink" title="AtomXR: Streamlined XR Prototyping with Natural Language and Immersive Physical Interaction"></a>AtomXR: Streamlined XR Prototyping with Natural Language and Immersive Physical Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11238">http://arxiv.org/abs/2311.11238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alice Cai, Caine Ardayfio, AnhPhu Nguyen, Tica Lin, Elena Glassman</li>
<li>for: 提高开发效率和用户体验，为不熟悉XR开发的开发者提供一个易用的开发工具。</li>
<li>methods: 使用自然语言、眼动和触摸交互，提供一个吸引式的头戴式开发环境，并使用人工智能语言模型（LLMs）生成AtomScript脚本。</li>
<li>results: 经验证明，AtomXR可以提高开发速度和用户体验，比传统系统更高效。<details>
<summary>Abstract</summary>
As technological advancements in extended reality (XR) amplify the demand for more XR content, traditional development processes face several challenges: 1) a steep learning curve for inexperienced developers, 2) a disconnect between 2D development environments and 3D user experiences inside headsets, and 3) slow iteration cycles due to context switching between development and testing environments. To address these challenges, we introduce AtomXR, a streamlined, immersive, no-code XR prototyping tool designed to empower both experienced and inexperienced developers in creating applications using natural language, eye-gaze, and touch interactions. AtomXR consists of: 1) AtomScript, a high-level human-interpretable scripting language for rapid prototyping, 2) a natural language interface that integrates LLMs and multimodal inputs for AtomScript generation, and 3) an immersive in-headset authoring environment. Empirical evaluation through two user studies offers insights into natural language-based and immersive prototyping, and shows AtomXR provides significant improvements in speed and user experience compared to traditional systems.
</details>
<details>
<summary>摘要</summary>
随着扩展现实（XR）技术的进步，需求更多的XR内容的增加，传统的开发过程面临多种挑战：1）新手开发者学习曲线过于陡峭，2）2D开发环境与头盔中的3D用户体验之间的断绝，3）由于开发和测试环境之间的上下文切换而导致的慢速迭代循环。为了解决这些挑战，我们介绍AtomXR，一种简化的、沉浸式、无代码XR开发工具，用于帮助经验不足和经验丰富的开发者在使用自然语言、眼动和触摸交互创建应用程序。AtomXR包括：1）AtomScript，一种高级的人类可解释的脚本语言，用于快速概念化，2）一个与大语言模型（LLM）和多Modal输入集成的自然语言界面，3）一个沉浸式在头盔中的作者环境。经验证明，通过两项用户研究，AtomXR在自然语言基础和沉浸式概念驱动中提供了 Traditional 系统相比的显著改进，具体来说是快速和用户体验方面的改进。
</details></li>
</ul>
<hr>
<h2 id="Implementation-of-AI-Deep-Learning-Algorithm-For-Multi-Modal-Sentiment-Analysis"><a href="#Implementation-of-AI-Deep-Learning-Algorithm-For-Multi-Modal-Sentiment-Analysis" class="headerlink" title="Implementation of AI Deep Learning Algorithm For Multi-Modal Sentiment Analysis"></a>Implementation of AI Deep Learning Algorithm For Multi-Modal Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11237">http://arxiv.org/abs/2311.11237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhen Wang</li>
<li>for: 这种多模态情绪识别方法是用于提高情绪识别精度和学习效率的。</li>
<li>methods: 这种方法使用了两个通道卷积神经网络和环形网络，并将词 vector化使用 GloVe，然后将词vector输入到卷积神经网络中，并使用注意机制和最大池化Converter BiSRU通道，从而获得本地深情和预后序情 semantics。</li>
<li>results: 实验表明，基于特征融合的情绪分析方法可以有效提高情绪数据集的识别精度和降低学习时间。这种模型具有一定的通用性。<details>
<summary>Abstract</summary>
A multi-modal emotion recognition method was established by combining two-channel convolutional neural network with ring network. This method can extract emotional information effectively and improve learning efficiency. The words were vectorized with GloVe, and the word vector was input into the convolutional neural network. Combining attention mechanism and maximum pool converter BiSRU channel, the local deep emotion and pre-post sequential emotion semantics are obtained. Finally, multiple features are fused and input as the polarity of emotion, so as to achieve the emotion analysis of the target. Experiments show that the emotion analysis method based on feature fusion can effectively improve the recognition accuracy of emotion data set and reduce the learning time. The model has a certain generalization.
</details>
<details>
<summary>摘要</summary>
一种多模式情感识别方法由两个通道卷积神经网络与环形网络结合，以提取情感信息效果地和提高学习效率。文本被vector化使用GloVe，word vector输入卷积神经网络。将注意机制和最大池化器BiSRU通道结合，可以获得当地深情感和预后序列情感 semantics。最后，多种特征被融合并输入为情感方向，以实现目标情感分析。实验显示，基于特征融合的情感分析方法可以提高情感数据集的识别精度和减少学习时间。模型具有一定的通用性。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Anomaly’-in-Time-Series-Anomaly-Detection-A-Self-supervised-Tri-domain-Solution"><a href="#Unraveling-the-Anomaly’-in-Time-Series-Anomaly-Detection-A-Self-supervised-Tri-domain-Solution" class="headerlink" title="Unraveling the &#96;Anomaly’ in Time Series Anomaly Detection: A Self-supervised Tri-domain Solution"></a>Unraveling the &#96;Anomaly’ in Time Series Anomaly Detection: A Self-supervised Tri-domain Solution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11235">http://arxiv.org/abs/2311.11235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pseudo-Skye/TriAD">https://github.com/pseudo-Skye/TriAD</a></li>
<li>paper_authors: Yuting Sun, Guansong Pang, Guanhua Ye, Tong Chen, Xia Hu, Hongzhi Yin</li>
<li>for: 本研究旨在提出一种基于自我超vision学习的三Domain异常检测器（TriAD），以解决时间序列异常检测（TSAD）中的挑战，包括缺乏异常标签和异常形态和长度的变化。</li>
<li>methods: TriAD使用了三个数据领域的特征模型，不需要异常标签，通过交互领域和内部领域的对比损失来学习常见特征。</li>
<li>results: TriAD在UCRC datasets上实现了三倍于SOTA深度学习模型的PA%K基数F1得分，以及50%的准确率提升 compared to SOTA的discord发现算法。<details>
<summary>Abstract</summary>
The ongoing challenges in time series anomaly detection (TSAD), notably the scarcity of anomaly labels and the variability in anomaly lengths and shapes, have led to the need for a more efficient solution. As limited anomaly labels hinder traditional supervised models in TSAD, various SOTA deep learning techniques, such as self-supervised learning, have been introduced to tackle this issue. However, they encounter difficulties handling variations in anomaly lengths and shapes, limiting their adaptability to diverse anomalies. Additionally, many benchmark datasets suffer from the problem of having explicit anomalies that even random functions can detect. This problem is exacerbated by ill-posed evaluation metrics, known as point adjustment (PA), which can result in inflated model performance. In this context, we propose a novel self-supervised learning based Tri-domain Anomaly Detector (TriAD), which addresses these challenges by modeling features across three data domains - temporal, frequency, and residual domains - without relying on anomaly labels. Unlike traditional contrastive learning methods, TriAD employs both inter-domain and intra-domain contrastive loss to learn common attributes among normal data and differentiate them from anomalies. Additionally, our approach can detect anomalies of varying lengths by integrating with a discord discovery algorithm. It is worth noting that this study is the first to reevaluate the deep learning potential in TSAD, utilizing both rigorously designed datasets (i.e., UCR Archive) and evaluation metrics (i.e., PA%K and affiliation). Through experimental results on the UCR dataset, TriAD achieves an impressive three-fold increase in PA%K based F1 scores over SOTA deep learning models, and 50% increase of accuracy as compared to SOTA discord discovery algorithms.
</details>
<details>
<summary>摘要</summary>
“时间序列异常检测（TSAD）中的挑战，包括缺乏异常标签和异常长度和形状的变化，导致需要更有效的解决方案。由于受限于异常标签，传统的超级vised学习模型在TSAD中受到阻碍，而新的state-of-the-art（SOTA）深度学习技术如自我supervised learning被引入来解决这个问题。然而，这些技术在异常长度和形状的变化中遇到困难，限制它们在多种异常检测中的适用性。此外，许多benchmark dataset受到异常标签的问题，这个问题更加严重由于评估度量（point adjustment，PA）的问题，可能导致模型表现的夸大。在这个 контекст中，我们提出了一个基于自我supervised learning的Tri-domain Anomaly Detector（TriAD），解决了这些挑战。TriAD不需要异常标签，而是通过在三个数据领域（时间、频率和差分领域）中建立特征，并运用両侧和内部对照损失来学习常规数据的共同特征和与异常检测。此外，TriAD可以检测异常的不同长度，通过与歧义发现算法的结合。值得注意的是，本研究是TSAD领域中首次将深度学习的潜力发掘，运用了严谨的测试数据（i.e., UCR Archive）和评估度量（i.e., PA%K和affiliation）。经过实验结果显示，TriAD在UCR数据集上表现出三倍的PA%K基于F1分数，和对于SOTA歧义发现算法的50%增加。”
</details></li>
</ul>
<hr>
<h2 id="FedRA-A-Random-Allocation-Strategy-for-Federated-Tuning-to-Unleash-the-Power-of-Heterogeneous-Clients"><a href="#FedRA-A-Random-Allocation-Strategy-for-Federated-Tuning-to-Unleash-the-Power-of-Heterogeneous-Clients" class="headerlink" title="FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients"></a>FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11227">http://arxiv.org/abs/2311.11227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leondada/fedra">https://github.com/leondada/fedra</a></li>
<li>paper_authors: Shangchao Su, Bin Li, Xiangyang Xue</li>
<li>for: 这个研究旨在提出一个新的联邦调教算法（FedRA），以便在实际的联邦学习应用中，处理多个客户端的资料和计算资源，并将其联合 fine-tune 基础模型。</li>
<li>methods:  FedRA 使用了一个随机生成的分配矩阵，并在资源受限的客户端上实现了一小部分层的重新排序和微调。这些微调的结果将被服务器统计，并将其与原始模型的对应层进行整合。</li>
<li>results: 在两个大规模的图像数据集上（DomainNet 和 NICO++），FedRA 在多个非同寻设定下进行了实验，并与比较方法相比，它表现出了很好的性能。<details>
<summary>Abstract</summary>
With the increasing availability of Foundation Models, federated tuning has garnered attention in the field of federated learning, utilizing data and computation resources from multiple clients to collaboratively fine-tune foundation models. However, in real-world federated scenarios, there often exist a multitude of heterogeneous clients with varying computation and communication resources, rendering them incapable of supporting the entire model fine-tuning process. In response to this challenge, we propose a novel federated tuning algorithm, FedRA. The implementation of FedRA is straightforward and can be seamlessly integrated into any transformer-based model without the need for further modification to the original model. Specifically, in each communication round, FedRA randomly generates an allocation matrix. For resource-constrained clients, it reorganizes a small number of layers from the original model based on the allocation matrix and fine-tunes using LoRA. Subsequently, the server aggregates the updated LoRA parameters from the clients according to the current allocation matrix into the corresponding layers of the original model. It is worth noting that FedRA also supports scenarios where none of the clients can support the entire global model, which is an impressive advantage. We conduct experiments on two large-scale image datasets, DomainNet and NICO++, under various non-iid settings. The results demonstrate that FedRA outperforms the compared methods significantly. The source code is available at \url{https://github.com/leondada/FedRA}.
</details>
<details>
<summary>摘要</summary>
随着基础模型的可用性的增加，联邦调教（federated tuning）在联邦学习领域引起了关注，利用多个客户端的数据和计算资源进行联邦调教。然而，在实际的联邦场景中，客户端通常存在多种不同的计算和通信资源，使得他们无法支持整个模型调教过程。为了解决这个挑战，我们提出了一种新的联邦调教算法，即 FedRA。FedRA的实现非常直观，可以轻松地与任何基于转换器的模型集成，无需对原始模型进行进一步修改。具体来说，在每次通信循环中，FedRA随机生成一个分配矩阵。对于资源受限的客户端，它会根据分配矩阵重新排序一小部分层 FROM the original model，并使用 LoRA 进行微调。然后，服务器会根据当前的分配矩阵，将客户端上更新的 LoRA 参数与原始模型的相应层进行集成。需要注意的是，FedRA 还支持情况下，客户端无法支持整个全局模型，这是一个非常优势的特点。我们在 DomainNet 和 NICO++ 两个大规模图像数据集上进行了多种非标一致的实验，结果显示，FedRA 与相比方法相比有显著的优势。源代码可以在 \url{https://github.com/leondada/FedRA} 上获取。
</details></li>
</ul>
<hr>
<h2 id="An-Interactive-Query-Generation-Assistant-using-LLM-based-Prompt-Modification-and-User-Feedback"><a href="#An-Interactive-Query-Generation-Assistant-using-LLM-based-Prompt-Modification-and-User-Feedback" class="headerlink" title="An Interactive Query Generation Assistant using LLM-based Prompt Modification and User Feedback"></a>An Interactive Query Generation Assistant using LLM-based Prompt Modification and User Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11226">http://arxiv.org/abs/2311.11226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaustubh D. Dhole, Ramraj Chandradevan, Eugene Agichtein</li>
<li>for: 这个论文旨在提供一种助手来帮助用户在搜索过程中提出更有效的查询。</li>
<li>methods: 论文使用了多种自然语言处理技术，包括语言模型（LLMs）和人工智能技术，以帮助用户提出更有效的查询。</li>
<li>results: 论文通过对多种语言和多个文档库进行测试，发现使用这种助手可以提高用户在搜索过程中提出的查询的有效性。<details>
<summary>Abstract</summary>
While search is the predominant method of accessing information, formulating effective queries remains a challenging task, especially for situations where the users are not familiar with a domain, or searching for documents in other languages, or looking for complex information such as events, which are not easily expressible as queries. Providing example documents or passages of interest, might be easier for a user, however, such query-by-example scenarios are prone to concept drift, and are highly sensitive to the query generation method. This demo illustrates complementary approaches of using LLMs interactively, assisting and enabling the user to provide edits and feedback at all stages of the query formulation process. The proposed Query Generation Assistant is a novel search interface which supports automatic and interactive query generation over a mono-linguial or multi-lingual document collection. Specifically, the proposed assistive interface enables the users to refine the queries generated by different LLMs, to provide feedback on the retrieved documents or passages, and is able to incorporate the users' feedback as prompts to generate more effective queries. The proposed interface is a valuable experimental tool for exploring fine-tuning and prompting of LLMs for query generation to qualitatively evaluate the effectiveness of retrieval and ranking models, and for conducting Human-in-the-Loop (HITL) experiments for complex search tasks where users struggle to formulate queries without such assistance.
</details>
<details>
<summary>摘要</summary>
“寻找信息是主要的访问方法，但是寻找有效的查询仍然是一个问题，特别是当用户不熟悉一个领域，或者搜寻文档written in other languages，或者搜寻复杂的信息，如事件，这些不易表达为查询。提供示例文档或 interessante 的段落可能较为容易，但这些查询示例enario 容易受到概念变化的影响，并且高度敏感于查询生成方法。本 demo 展示了与 LLM 互动的辅助方法，帮助用户在查询生成过程中提供修改和反馈。提案的查询生成助手是一个新的搜寻界面，可以在单语言或多语言文档集上进行自动和互动查询生成。具体来说，提案的辅助界面可以让用户对不同的 LLM 生成的查询进行修改，提供关于已经 retrieve 的文档或段落的反馈，并且可以将用户的反馈作为提示，生成更有效的查询。本interface 是一个实用的实验工具，可以用于调整 LLM 的 fine-tuning 和提示，以评估搜寻和排名模型的效iveness，并且可以进行人类在 Loop （HITL）实验，用于复杂的搜寻任务，where users struggle to formulate queries without such assistance。”
</details></li>
</ul>
<hr>
<h2 id="SPLAIN-Augmenting-CybersecurityWarnings-with-Reasons-and-Data"><a href="#SPLAIN-Augmenting-CybersecurityWarnings-with-Reasons-and-Data" class="headerlink" title="SPLAIN: Augmenting CybersecurityWarnings with Reasons and Data"></a>SPLAIN: Augmenting CybersecurityWarnings with Reasons and Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11215">http://arxiv.org/abs/2311.11215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vera A. Kazakova, Jena D. Hwang, Bonnie J. Dorr, Yorick Wilks, J. Blake Gage, Alex Memory, Mark A. Clark</li>
<li>for: 提供了一种自然语言生成器，用于转化 warnings 数据为用户友好的cyber 威胁解释。</li>
<li>methods: 使用模板基本 approached 生成一个层次结构的 warning 结构和词汇，以确保 warning 的一致性和可读性。</li>
<li>results: 通过使用 SPLAIN，可以提供了clear 和可行的输出，包括输入数据和系统功能的层次结构化解释。<details>
<summary>Abstract</summary>
Effective cyber threat recognition and prevention demand comprehensible forecasting systems, as prior approaches commonly offer limited and, ultimately, unconvincing information. We introduce Simplified Plaintext Language (SPLAIN), a natural language generator that converts warning data into user-friendly cyber threat explanations. SPLAIN is designed to generate clear, actionable outputs, incorporating hierarchically organized explanatory details about input data and system functionality. Given the inputs of individual sensor-induced forecasting signals and an overall warning from a fusion module, SPLAIN queries each signal for information on contributing sensors and data signals. This collected data is processed into a coherent English explanation, encompassing forecasting, sensing, and data elements for user review. SPLAIN's template-based approach ensures consistent warning structure and vocabulary. SPLAIN's hierarchical output structure allows each threat and its components to be expanded to reveal underlying explanations on demand. Our conclusions emphasize the need for designers to specify the "how" and "why" behind cyber warnings, advocate for simple structured templates in generating consistent explanations, and recognize that direct causal links in Machine Learning approaches may not always be identifiable, requiring some explanations to focus on general methodologies, such as model and training data.
</details>
<details>
<summary>摘要</summary>
为了有效识别和预防网络攻击，需要有可理解的预测系统，因为传统的方法通常只提供有限和不够有力的信息。我们介绍了简化平文语言（SPLAIN），一种自然语言生成器，可以将警告数据转换成有用的网络攻击预测说明。SPLAIN是设计来生成明了、可行的输出，并包括阶段性的解释细节和系统功能。对输入感知器生成的单个警告信号和总警告从拢合模块来说，SPLAIN会询问每个信号的报告来自哪些感知器和数据信号。这些收集的数据被处理成一个 coherent English 说明，包括预测、感知和数据元素，供用户审查。SPLAIN 使用模板方法，以保证警告结构和词汇的一致性。SPLAIN 的层次结构输出，让每个威胁和其组成部分可以Expand 到显示下一层的解释。我们的结论强调设计人员需要指定 "如何" 和 "为什么" 在网络警告中，主张使用简单结构模板生成一致的解释，并认可直接 causal 链在机器学习方法中可能无法识别，有些解释需要关注总方法，如模型和训练数据。
</details></li>
</ul>
<hr>
<h2 id="Can-We-Utilize-Pre-trained-Language-Models-within-Causal-Discovery-Algorithms"><a href="#Can-We-Utilize-Pre-trained-Language-Models-within-Causal-Discovery-Algorithms" class="headerlink" title="Can We Utilize Pre-trained Language Models within Causal Discovery Algorithms?"></a>Can We Utilize Pre-trained Language Models within Causal Discovery Algorithms?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11212">http://arxiv.org/abs/2311.11212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chanhui Lee, Juhyeon Kim, Yongjun Jeong, Juhyun Lyu, Junghee Kim, Sangmin Lee, Sangjun Han, Hyeokjun Choe, Soyeon Park, Woohyung Lim, Sungbin Lim, Sanghack Lee</li>
<li>for: 本研究探讨了透过预训语言模型（PLM）进行 causal reasoning 的可能性，以及如何将 PLM 应用于探索 causal 关系性。</li>
<li>methods: 本研究使用了重复的 causal reasoning 来测试 PLM 的可能性，并通过特定设计的 prompts 来实现。</li>
<li>results: 研究发现 PLM-based causal reasoning 存在许多限制，包括 prompt design 的影响和 false prediction 的风险。在实验中，我们显示了 PLM-based causal reasoning 的局限性，并提出了一个新的框架，具有与 PLM 的融合和 causal discovery 的整合。这个框架不仅在整合 PLM 和 causal discovery 中提高了性能，还建议了如何将 PLM 提取的专业知识与现有的 causal discovery 算法整合。<details>
<summary>Abstract</summary>
Scaling laws have allowed Pre-trained Language Models (PLMs) into the field of causal reasoning. Causal reasoning of PLM relies solely on text-based descriptions, in contrast to causal discovery which aims to determine the causal relationships between variables utilizing data. Recently, there has been current research regarding a method that mimics causal discovery by aggregating the outcomes of repetitive causal reasoning, achieved through specifically designed prompts. It highlights the usefulness of PLMs in discovering cause and effect, which is often limited by a lack of data, especially when dealing with multiple variables. Conversely, the characteristics of PLMs which are that PLMs do not analyze data and they are highly dependent on prompt design leads to a crucial limitation for directly using PLMs in causal discovery. Accordingly, PLM-based causal reasoning deeply depends on the prompt design and carries out the risk of overconfidence and false predictions in determining causal relationships. In this paper, we empirically demonstrate the aforementioned limitations of PLM-based causal reasoning through experiments on physics-inspired synthetic data. Then, we propose a new framework that integrates prior knowledge obtained from PLM with a causal discovery algorithm. This is accomplished by initializing an adjacency matrix for causal discovery and incorporating regularization using prior knowledge. Our proposed framework not only demonstrates improved performance through the integration of PLM and causal discovery but also suggests how to leverage PLM-extracted prior knowledge with existing causal discovery algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用预训练语言模型（PLM）进行 causal reasoning 已经得到了许多研究的推动。 PLM 可以通过文本描述来进行 causal reasoning，而不需要数据，这与 causal discovery 不同，后者的目标是通过数据来确定变量之间的 causal 关系。在最近的研究中，有一种方法被提出，它通过特定的 prompts 来模拟 causal discovery，并通过重复的 causal reasoning 来获得结果。这种方法高亮了 PLM 在发现 causal 关系的用途，特别是在 dealing with multiple variables 时。然而，PLM 的特点是它们不会分析数据，而且它们高度依赖于 prompt 的设计，这导致了直接使用 PLM 在 causal discovery 中的限制。因此，PLM-based causal reasoning 深受 prompt 设计的限制，并且存在风险的过确定和 false prediction 在确定 causal 关系上。在这篇论文中，我们employmultiple physics-inspired synthetic data 进行实验，以证明 PLM-based causal reasoning 中的上述限制。然后，我们提出了一种新的框架，它可以将 PLM 提取的 prior knowledge 与 causal discovery 算法相结合。我们的提出的框架不仅在将 PLM 和 causal discovery 结合起来后表现出了改进的性能，而且还表明了如何使用 PLM 提取的 prior knowledge 与现有的 causal discovery 算法结合。<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Generative-AI-for-Clinical-Evidence-Summarization-Needs-to-Achieve-Trustworthiness"><a href="#Leveraging-Generative-AI-for-Clinical-Evidence-Summarization-Needs-to-Achieve-Trustworthiness" class="headerlink" title="Leveraging Generative AI for Clinical Evidence Summarization Needs to Achieve Trustworthiness"></a>Leveraging Generative AI for Clinical Evidence Summarization Needs to Achieve Trustworthiness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11211">http://arxiv.org/abs/2311.11211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gongbo Zhang, Qiao Jin, Denis Jered McInerney, Yong Chen, Fei Wang, Curtis L. Cole, Qian Yang, Yanshan Wang, Bradley A. Malin, Mor Peleg, Byron C. Wallace, Zhiyong Lu, Chunhua Weng, Yifan Peng</li>
<li>for: 提高医疗质量，使医疗决策和实践受到最佳证据支持。</li>
<li>methods: 使用大语言模型来自动摘要医学证据，以便更好地收集、评估和Synthesize医学证据。</li>
<li>results: 通过开发可信worthy的生成AI模型，可以提高医学证据摘要的效率和准确性。<details>
<summary>Abstract</summary>
Evidence-based medicine aims to improve the quality of healthcare by empowering medical decisions and practices with the best available evidence. The rapid growth of medical evidence, which can be obtained from various sources, poses a challenge in collecting, appraising, and synthesizing the evidential information. Recent advancements in generative AI, exemplified by large language models, hold promise in facilitating the arduous task. However, developing accountable, fair, and inclusive models remains a complicated undertaking. In this perspective, we discuss the trustworthiness of generative AI in the context of automated summarization of medical evidence.
</details>
<details>
<summary>摘要</summary>
证据基础医学目标是提高健康医疗质量，通过最好的证据来决策和实践医疗。医疗证据的快速增长，来自多种来源，增加了收集、评估和总结证据的挑战。最新的生成AI技术，如大语言模型，表现出了激进推动这项任务的潜力。然而，建立可靠、公正和包容的模型仍然是一项复杂的任务。本观点讨论了自动摘要医疗证据中生成AI的可靠性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Noise-Scheduling-for-Generating-Plausible-Designs-with-Diffusion-Models"><a href="#On-the-Noise-Scheduling-for-Generating-Plausible-Designs-with-Diffusion-Models" class="headerlink" title="On the Noise Scheduling for Generating Plausible Designs with Diffusion Models"></a>On the Noise Scheduling for Generating Plausible Designs with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11207">http://arxiv.org/abs/2311.11207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajie Fan, Laure Vuaille, Thomas Bäck, Hao Wang<br>for: 这个论文探讨了Diffusion Models（DGMs）在创造新的设计方面的应用，尤其是在视觉质量高和结构Semantic表达有约束的情况下。methods: 该论文使用了Diffusion Models（DGMs）进行生成图像，并研究了噪声程度对结果plausibility的影响。两种技术被提出来确定噪声范围，并制定了一个新的parametric噪声程度。results: 相比默认噪声程度，使用提出的噪声程度可以大幅提高了结果plausibility的率从83.4%提高到93.5%，并且Fr&#39;echet Inception Distance（FID）从7.84降低到4.87。这些结果表明模型具有良好的结构理解能力。<details>
<summary>Abstract</summary>
Deep Generative Models (DGMs) are widely used to create innovative designs across multiple industries, ranging from fashion to the automotive sector. In addition to generating images of high visual quality, the task of structural design generation imposes more stringent constrains on the semantic expression, e.g., no floating material or missing part, which we refer to as plausibility in this work. We delve into the impact of noise schedules of diffusion models on the plausibility of the outcome: there exists a range of noise levels at which the model's performance decides the result plausibility. Also, we propose two techniques to determine such a range for a given image set and devise a novel parametric noise schedule for better plausibility. We apply this noise schedule to the training and sampling of the well-known diffusion model EDM and compare it to its default noise schedule. Compared to EDM, our schedule significantly improves the rate of plausible designs from 83.4% to 93.5% and Fr\'echet Inception Distance (FID) from 7.84 to 4.87. Further applications of advanced image editing tools demonstrate the model's solid understanding of structure.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unmasking-and-Improving-Data-Credibility-A-Study-with-Datasets-for-Training-Harmless-Language-Models"><a href="#Unmasking-and-Improving-Data-Credibility-A-Study-with-Datasets-for-Training-Harmless-Language-Models" class="headerlink" title="Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models"></a>Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11202">http://arxiv.org/abs/2311.11202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/docta-ai/docta">https://github.com/docta-ai/docta</a></li>
<li>paper_authors: Zhaowei Zhu, Jialu Wang, Hao Cheng, Yang Liu</li>
<li>for: 这项研究的目的是提高现实世界数据集的信息准确性，以提高语言模型的训练和应用。</li>
<li>methods: 该研究提出了一种系统性的评估数据集信息准确性的框架，并使用了自动标注和人工审核来检测数据集中的标签错误。</li>
<li>results: 该研究在11个实际世界数据集上发现了平均6.16%的标签错误，并通过直接修复标签错误来提高数据集的信息准确性和下游学习性能。<details>
<summary>Abstract</summary>
Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of 6.16% label errors in 11 datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets. Open-source: https://github.com/Docta-ai/docta.
</details>
<details>
<summary>摘要</summary>
Language models have shown promise in various tasks, but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of 6.16% label errors in 11 datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets. Open-source: <https://github.com/Docta-ai/docta>.
</details></li>
</ul>
<hr>
<h2 id="Assessing-AI-Impact-Assessments-A-Classroom-Study"><a href="#Assessing-AI-Impact-Assessments-A-Classroom-Study" class="headerlink" title="Assessing AI Impact Assessments: A Classroom Study"></a>Assessing AI Impact Assessments: A Classroom Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11193">http://arxiv.org/abs/2311.11193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nari Johnson, Hoda Heidari</li>
<li>for: This paper is written to evaluate the effectiveness of existing AI impact assessments (AIIAs) and to provide recommendations for future work on developing and validating AIIAs.</li>
<li>methods: The paper uses a classroom study with 38 students at a large research-intensive university to evaluate the impact of AIIAs on participants’ perceptions of the potential risks of generative AI systems and the level of responsibility held by AI experts in addressing potential harm.</li>
<li>results: The study finds preliminary evidence that impact assessments can influence participants’ perceptions of the potential risks of generative AI systems, and identifies a consistent set of limitations shared by several existing AIIA instruments.Here are the results in Simplified Chinese text:</li>
<li>for: 这篇论文是为了评估现有的人工智能影响评估（AIIA）的有效性，并提供未来工作的建议。</li>
<li>methods: 这篇论文使用了一个大学课堂研究（N &#x3D; 38），以评估AI影响评估的影响。</li>
<li>results: 研究发现，影响评估可以影响参与者对生成AI系统的风险潜在性的观念，并发现了许多现有AIIA工具的限制。<details>
<summary>Abstract</summary>
Artificial Intelligence Impact Assessments ("AIIAs"), a family of tools that provide structured processes to imagine the possible impacts of a proposed AI system, have become an increasingly popular proposal to govern AI systems. Recent efforts from government or private-sector organizations have proposed many diverse instantiations of AIIAs, which take a variety of forms ranging from open-ended questionnaires to graded score-cards. However, to date that has been limited evaluation of existing AIIA instruments. We conduct a classroom study (N = 38) at a large research-intensive university (R1) in an elective course focused on the societal and ethical implications of AI. We assign students to different organizational roles (for example, an ML scientist or product manager) and ask participant teams to complete one of three existing AI impact assessments for one of two imagined generative AI systems. In our thematic analysis of participants' responses to pre- and post-activity questionnaires, we find preliminary evidence that impact assessments can influence participants' perceptions of the potential risks of generative AI systems, and the level of responsibility held by AI experts in addressing potential harm. We also discover a consistent set of limitations shared by several existing AIIA instruments, which we group into concerns about their format and content, as well as the feasibility and effectiveness of the activity in foreseeing and mitigating potential harms. Drawing on the findings of this study, we provide recommendations for future work on developing and validating AIIAs.
</details>
<details>
<summary>摘要</summary>
人工智能影响评估工具（AIIA），一家家用于假设提案的人工智能系统的结构化过程，已经成为控制人工智能系统的增加 популяр的建议。 latest efforts from government or private-sector organizations have proposed many diverse instantiations of AIIAs, which take a variety of forms ranging from open-ended questionnaires to graded score-cards. However, to date there has been limited evaluation of existing AIIA instruments.我们在一所大型研究激射大学（R1）的选修课程中（关于人工智能的社会和道德问题），分配学生不同的组织角色（例如，机器学习科学家或产品经理），并让参与者组合完成一个现有的人工智能影响评估工具。在我们的主题分析中，我们发现了参与者对生成人工智能系统的潜在风险的认知改变，以及AI专家在避免可能的危害方面承担的责任水平。我们还发现了许多现有AIIA工具的共同局限性，包括格式和内容的问题，以及活动的可行性和效果。基于这些研究结果，我们提供未来开发和验证AIIAs的建议。
</details></li>
</ul>
<hr>
<h2 id="Attention-Based-Real-Time-Defenses-for-Physical-Adversarial-Attacks-in-Vision-Applications"><a href="#Attention-Based-Real-Time-Defenses-for-Physical-Adversarial-Attacks-in-Vision-Applications" class="headerlink" title="Attention-Based Real-Time Defenses for Physical Adversarial Attacks in Vision Applications"></a>Attention-Based Real-Time Defenses for Physical Adversarial Attacks in Vision Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11191">http://arxiv.org/abs/2311.11191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo</li>
<li>for: 防止深度神经网络在实际世界中受到攻击，以确保其应用在安全关键领域。</li>
<li>methods: 使用Channel-attention mechanism来快速识别和跟踪恶意物体，并在多帧场景中掩蔽恶意影响。</li>
<li>results: 提高了现有的过度活动技术的性能，并在多帧场景中实现了高效的防御框架，通过广泛的实验证明了其效果。<details>
<summary>Abstract</summary>
Deep neural networks exhibit excellent performance in computer vision tasks, but their vulnerability to real-world adversarial attacks, achieved through physical objects that can corrupt their predictions, raises serious security concerns for their application in safety-critical domains. Existing defense methods focus on single-frame analysis and are characterized by high computational costs that limit their applicability in multi-frame scenarios, where real-time decisions are crucial.   To address this problem, this paper proposes an efficient attention-based defense mechanism that exploits adversarial channel-attention to quickly identify and track malicious objects in shallow network layers and mask their adversarial effects in a multi-frame setting. This work advances the state of the art by enhancing existing over-activation techniques for real-world adversarial attacks to make them usable in real-time applications. It also introduces an efficient multi-frame defense framework, validating its efficacy through extensive experiments aimed at evaluating both defense performance and computational cost.
</details>
<details>
<summary>摘要</summary>
深度神经网络在计算机视觉任务中表现出色，但它们受到真实世界的敌对攻击，通过物理对象腐蚀其预测，引发了安全问题的应用在安全关键领域。现有的防御方法主要是单帧分析，具有高计算成本，限制其在多帧场景中实现可靠的响应。为解决这个问题，本文提出了一种高效的注意力基于防御机制，通过对恶意对象在浅层神经网络中快速识别和跟踪，并在多帧设置中遮盖其恶意效应。本工作提高了现有的真实世界敌对攻击的过载技术的可用性，使其在实时应用中可用。此外，本文还介绍了一种高效的多帧防御框架，通过广泛的实验证明其防御性和计算成本的优化。
</details></li>
</ul>
<hr>
<h2 id="Few-Shot-Classification-Segmentation-Using-Large-Language-Models-Agent"><a href="#Few-Shot-Classification-Segmentation-Using-Large-Language-Models-Agent" class="headerlink" title="Few-Shot Classification &amp; Segmentation Using Large Language Models Agent"></a>Few-Shot Classification &amp; Segmentation Using Large Language Models Agent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12065">http://arxiv.org/abs/2311.12065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Meng, Yang Tao, Wuliang Yin</li>
<li>for: 解决几个示例图像分类和 segmentation 问题（few-shot image classification and segmentation，FS-CS），需要在查询图像中分类和 segmentation 目标对象，只需要几个目标类的示例。</li>
<li>methods: 利用大语言模型（LLM）作为代理，在培育自由的情况下解决 FS-CS 问题。将 LLM 作为任务规划者，并使用 off-the-shelf 视觉模型（如 Segment Anything Model 和 GPT-4Vision）帮助 LLM 理解空间和 semantic 信息。使用链式思维提示和在场景学习来导航 LLM，以使其在查询图像中分类和 segmentation 目标对象。</li>
<li>results: 提出的方法可以在 Pascal-5i 数据集上 дости得 state-of-the-art 性能。<details>
<summary>Abstract</summary>
The task of few-shot image classification and segmentation (FS-CS) requires the classification and segmentation of target objects in a query image, given only a few examples of the target classes. We introduce a method that utilises large language models (LLM) as an agent to address the FS-CS problem in a training-free manner. By making the LLM the task planner and off-the-shelf vision models the tools, the proposed method is capable of classifying and segmenting target objects using only image-level labels. Specifically, chain-of-thought prompting and in-context learning guide the LLM to observe support images like human; vision models such as Segment Anything Model (SAM) and GPT-4Vision assist LLM understand spatial and semantic information at the same time. Ultimately, the LLM uses its summarizing and reasoning capabilities to classify and segment the query image. The proposed method's modular framework makes it easily extendable. Our approach achieves state-of-the-art performance on the Pascal-5i dataset.
</details>
<details>
<summary>摘要</summary>
很少示例图像分类和分割（FS-CS）任务需要根据只有几个目标类示例来分类和分割目标对象在查询图像中。我们介绍了一种方法，使用大型自然语言模型（LLM）作为任务规划器，使用各种可用的视觉模型（如Segment Anything Model（SAM）和GPT-4Vision），让LLM通过人类的思维方式来理解支持图像中的空间和semantic信息。最终，LLM使用总结和理解能力来分类和分割查询图像。我们的方法具有扩展性，我们的方法在Pascal-5i数据集上实现了状态的表现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/cs.AI_2023_11_19/" data-id="clpztdncn007jes883are86rd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/cs.CL_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T11:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/cs.CL_2023_11_19/">cs.CL - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Spot-the-Bot-Distinguishing-Human-Written-and-Bot-Generated-Texts-Using-Clustering-and-Information-Theory-Techniques"><a href="#Spot-the-Bot-Distinguishing-Human-Written-and-Bot-Generated-Texts-Using-Clustering-and-Information-Theory-Techniques" class="headerlink" title="Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using Clustering and Information Theory Techniques"></a>Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using Clustering and Information Theory Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11441">http://arxiv.org/abs/2311.11441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilii Gromov, Quynh Nhu Dang</li>
<li>for: 本研究旨在开发一种基于不监督学习技术的 Bot 识别算法，不需要大量标注数据和&#x2F;或 Bot 模型架构的先验知识。</li>
<li>methods: 本研究使用语义分析（卷积和模糊）和信息技术，构建了一个robust的 Bot 识别模型，可以对不同类型的 Bot 进行识别。</li>
<li>results: 研究发现，生成文本往往更加混乱，而文学作品则更加复杂。此外，人类文本 clustering 结果比 bot-生成文本更加杂乱，而 bot-生成文本 clustering 结果比人类文本更加紧凑和分化。<details>
<summary>Abstract</summary>
With the development of generative models like GPT-3, it is increasingly more challenging to differentiate generated texts from human-written ones. There is a large number of studies that have demonstrated good results in bot identification. However, the majority of such works depend on supervised learning methods that require labelled data and/or prior knowledge about the bot-model architecture. In this work, we propose a bot identification algorithm that is based on unsupervised learning techniques and does not depend on a large amount of labelled data. By combining findings in semantic analysis by clustering (crisp and fuzzy) and information techniques, we construct a robust model that detects a generated text for different types of bot. We find that the generated texts tend to be more chaotic while literary works are more complex. We also demonstrate that the clustering of human texts results in fuzzier clusters in comparison to the more compact and well-separated clusters of bot-generated texts.
</details>
<details>
<summary>摘要</summary>
随着生成模型如GPT-3的发展，分化generated文本和人类写作文本的任务变得越来越复杂。有很多研究表明，可以通过supervised learning方法来实现 bot 识别。然而，大多数这些工作需要标注数据和/或对 bot-model architecture 的先验知识。在这个工作中，我们提出一种基于无监督学习技术的 bot 识别算法。通过结合 semantic analysis by clustering（crisp和fuzzy）和信息技术，我们构建了一个可靠的模型，可以 Detect 不同类型的 bot 生成的文本。我们发现，生成文本往往更加混乱，而文学作品则更加复杂。此外，我们还示出了人类文本的 clustering 结果比 bot-生成文本的 clustering 结果更加普通和更加紧密。
</details></li>
</ul>
<hr>
<h2 id="ML-LMCL-Mutual-Learning-and-Large-Margin-Contrastive-Learning-for-Improving-ASR-Robustness-in-Spoken-Language-Understanding"><a href="#ML-LMCL-Mutual-Learning-and-Large-Margin-Contrastive-Learning-for-Improving-ASR-Robustness-in-Spoken-Language-Understanding" class="headerlink" title="ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding"></a>ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11375">http://arxiv.org/abs/2311.11375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuxin Cheng, Bowen Cao, Qichen Ye, Zhihong Zhu, Hongxiang Li, Yuexian Zou</li>
<li>for: 提高自动语音识别（ASR）Robustness in spoken language understanding（SLU）</li>
<li>methods: 提出了一种新的框架——相互学习和大margin对比学习（ML-LMCL），通过在精度训练和ASR训练中分别使用两个SLU模型，以便相互分享知识。同时，引入了距离楔化正则化器，以避免push away intra-cluster pairs。</li>
<li>results: 实验结果表明，ML-LMCL在三个dataset上比现有模型表现出色，达到了新的state-of-the-art性能。<details>
<summary>Abstract</summary>
Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback-Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
听话语理解（SLU）是对话系统中的基本任务。然而，自动声音识别（ASR）的不可避免错误通常会妨碍理解性能，导致错误卷入。虽然有一些尝试通过对比学习解决这个问题，但它们（1）在细化学习中对清晰手动词语和ASR词语进行同样的处理;（2）忽视了semantic similarity pairs在应用对比学习时被推迟的问题;（3）受到Kullback-Leibler（KL）消失问题。在这篇论文中，我们提出了相互学习和大margin对比学习（ML-LMCL），一种改进ASRRobustness在SLU中的新框架。具体来说，在细化学习中，我们将两个SLU模型分别在手动词语和ASR词语上进行微调，以便相互分享知识。此外，我们还引入了距离偏斜规则，以避免push away intra-cluster pairs的问题。此外，我们使用循环缓和调度来缓解KL消失问题。实验结果表明，ML-LMCL在三个dataset上超过现有模型，实现了新的状态机器人性能。
</details></li>
</ul>
<hr>
<h2 id="CHAMP-Efficient-Annotation-and-Consolidation-of-Cluster-Hierarchies"><a href="#CHAMP-Efficient-Annotation-and-Consolidation-of-Cluster-Hierarchies" class="headerlink" title="CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies"></a>CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11301">http://arxiv.org/abs/2311.11301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ariecattan/champ">https://github.com/ariecattan/champ</a></li>
<li>paper_authors: Arie Cattan, Tom Hope, Doug Downey, Roy Bar-Haim, Lilach Eden, Yoav Kantor, Ido Dagan</li>
<li>for: 用于建立文本中复杂层次结构的 annotation</li>
<li>methods: 使用递归的增量建构方法</li>
<li>results: 可以快速构建层次结构，并且保证层次结构的可靠性和可比较性<details>
<summary>Abstract</summary>
Various NLP tasks require a complex hierarchical structure over nodes, where each node is a cluster of items. Examples include generating entailment graphs, hierarchical cross-document coreference resolution, annotating event and subevent relations, etc. To enable efficient annotation of such hierarchical structures, we release CHAMP, an open source tool allowing to incrementally construct both clusters and hierarchy simultaneously over any type of texts. This incremental approach significantly reduces annotation time compared to the common pairwise annotation approach and also guarantees maintaining transitivity at the cluster and hierarchy levels. Furthermore, CHAMP includes a consolidation mode, where an adjudicator can easily compare multiple cluster hierarchy annotations and resolve disagreements.
</details>
<details>
<summary>摘要</summary>
各种自然语言处理任务需要复杂的层次结构，其中每个节点是一个文本中的一组项。例如生成包容关系图、跨文档涉及关系解决、标注事件和次事件关系等。为了有效地标注这些层次结构，我们发布了 CHAMP 开源工具，可以逐步建立节点和层次结构，并且可以同时处理任意类型的文本。这种逐步方法与常见的对应方法相比，可以减少标注时间，同时保证节点和层次结构的对称性。此外，CHAMP 还包含了一个整合模式，可以让评审人轻松比较多个节点层次结构，并解决不一致。
</details></li>
</ul>
<hr>
<h2 id="A-Cross-Attention-Augmented-Model-for-Event-Triggered-Context-Aware-Story-Generation"><a href="#A-Cross-Attention-Augmented-Model-for-Event-Triggered-Context-Aware-Story-Generation" class="headerlink" title="A Cross-Attention Augmented Model for Event-Triggered Context-Aware Story Generation"></a>A Cross-Attention Augmented Model for Event-Triggered Context-Aware Story Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11271">http://arxiv.org/abs/2311.11271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tonywenuon/dialog-coherence-metric">https://github.com/tonywenuon/dialog-coherence-metric</a></li>
<li>paper_authors: Chen Tang, Tyler Loakman, Chenghua Lin</li>
<li>for: 提高生成的故事质量，更好地包括上下文和事件特征。</li>
<li>methods: 使用架构层 residual mapping 机制，将上下文特征映射到事件序列中，以更好地利用事件之间的逻辑关系。</li>
<li>results: 与基eline模型相比，提高自动度量和人工评价指标约5%和10%。<details>
<summary>Abstract</summary>
Despite recent advancements, existing story generation systems continue to encounter difficulties in effectively incorporating contextual and event features, which greatly influence the quality of generated narratives. To tackle these challenges, we introduce a novel neural generation model, EtriCA, that enhances the relevance and coherence of generated stories by employing a cross-attention mechanism to map context features onto event sequences through residual mapping. This feature capturing mechanism enables our model to exploit logical relationships between events more effectively during the story generation process. To further enhance our proposed model, we employ a post-training framework for knowledge enhancement (KeEtriCA) on a large-scale book corpus. This allows EtriCA to adapt to a wider range of data samples. This results in approximately 5\% improvement in automatic metrics and over 10\% improvement in human evaluation. We conduct extensive experiments, including comparisons with state-of-the-art (SOTA) baseline models, to evaluate the performance of our framework on story generation. The experimental results, encompassing both automated metrics and human assessments, demonstrate the superiority of our model over existing state-of-the-art baselines. These results underscore the effectiveness of our model in leveraging context and event features to improve the quality of generated narratives.
</details>
<details>
<summary>摘要</summary>
Despite recent advancements, existing story generation systems continue to struggle with effectively incorporating contextual and event features, which greatly impact the quality of generated narratives. To address these challenges, we propose a novel neural generation model, EtriCA, that enhances the relevance and coherence of generated stories by using a cross-attention mechanism to map context features onto event sequences through residual mapping. This feature capturing mechanism allows our model to exploit logical relationships between events more effectively during the story generation process. To further improve our proposed model, we employ a post-training framework for knowledge enhancement (KeEtriCA) on a large-scale book corpus. This enables EtriCA to adapt to a wider range of data samples, resulting in approximately 5% improvement in automatic metrics and over 10% improvement in human evaluation. We conduct extensive experiments, including comparisons with state-of-the-art baseline models, to evaluate the performance of our framework on story generation. The experimental results, encompassing both automated metrics and human assessments, demonstrate the superiority of our model over existing state-of-the-art baselines. These results highlight the effectiveness of our model in leveraging context and event features to improve the quality of generated narratives.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese languages. The other is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Towards-Real-World-Writing-Assistance-A-Chinese-Character-Checking-Benchmark-with-Faked-and-Misspelled-Characters"><a href="#Towards-Real-World-Writing-Assistance-A-Chinese-Character-Checking-Benchmark-with-Faked-and-Misspelled-Characters" class="headerlink" title="Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters"></a>Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11268">http://arxiv.org/abs/2311.11268</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THUKElab/Visual-C3">https://github.com/THUKElab/Visual-C3</a></li>
<li>paper_authors: Yinghui Li, Zishan Xu, Shaoshen Chen, Haojing Huang, Yangning Li, Yong Jiang, Zhongli Li, Qingyu Zhou, Hai-Tao Zheng, Ying Shen</li>
<li>for: 提高中文输入文本的正确性和质量，尤其是在手写输入中检测和修正错误字符。</li>
<li>methods: 利用人工标注的Visual Chinese Character Checking dataset，并提出了新的基线方法来评估这些方法。</li>
<li>results: 实验和分析结果表明，Visual-C$^3$ 是一个高质量 yet 挑战性的 dataset，并且新的基线方法可以在这个 dataset 上达到显著的性能。<details>
<summary>Abstract</summary>
Writing assistance is an application closely related to human life and is also a fundamental Natural Language Processing (NLP) research field. Its aim is to improve the correctness and quality of input texts, with character checking being crucial in detecting and correcting wrong characters. From the perspective of the real world where handwriting occupies the vast majority, characters that humans get wrong include faked characters (i.e., untrue characters created due to writing errors) and misspelled characters (i.e., true characters used incorrectly due to spelling errors). However, existing datasets and related studies only focus on misspelled characters mainly caused by phonological or visual confusion, thereby ignoring faked characters which are more common and difficult. To break through this dilemma, we present Visual-C$^3$, a human-annotated Visual Chinese Character Checking dataset with faked and misspelled Chinese characters. To the best of our knowledge, Visual-C$^3$ is the first real-world visual and the largest human-crafted dataset for the Chinese character checking scenario. Additionally, we also propose and evaluate novel baseline methods on Visual-C$^3$. Extensive empirical results and analyses show that Visual-C$^3$ is high-quality yet challenging. The Visual-C$^3$ dataset and the baseline methods will be publicly available to facilitate further research in the community.
</details>
<details>
<summary>摘要</summary>
文本协助是人类生活中非常重要的应用，同时也是自然语言处理（NLP）研究的基础领域之一。其目标是提高输入文本的正确性和质量，特别是在检查和更正错误字符时，字符检查具有关键性。在现实世界中，手写占了大多数情况下，人们通常会出现写字错误所导致的伪字和拼写错误。然而，现有的数据集和相关研究都主要关注了由语音或视觉混淆导致的拼写错误，而忽略了由写字错误引起的伪字，这些伪字更为普遍和困难。为了突破这种僵局，我们提出了Visual-C$^3$，一个人工标注的中文字符检查数据集，包括伪字和拼写错误的中文字符。到目前为止，Visual-C$^3$ 是实际世界中最大的人工制作的中文字符检查数据集，同时也是首个真实世界的视觉中文字符检查数据集。此外，我们还提出了一些基线方法，并进行了评估。广泛的实验结果和分析显示，Visual-C$^3$ 具有高质量又具有挑战性。Visual-C$^3$ 数据集和基线方法将在未来公开，以便更多的研究者参与进来。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Large-Language-Models-in-Mental-Health-Applications"><a href="#Rethinking-Large-Language-Models-in-Mental-Health-Applications" class="headerlink" title="Rethinking Large Language Models in Mental Health Applications"></a>Rethinking Large Language Models in Mental Health Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11267">http://arxiv.org/abs/2311.11267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoxiong Ji, Tianlin Zhang, Kailai Yang, Sophia Ananiadou, Erik Cambria</li>
<li>for: 这篇论文探讨了大语言模型在心理健康应用中的可能性，并讨论了生成模型的不稳定性和潜在的幻觉输出问题，强调需要持续进行审核和评估以确保其可靠性和可靠性。</li>
<li>methods: 该论文提出了在使用大语言模型时应该采取judicious和谨慎的心态，并讨论了泛化可解释性和特定可解释性的问题，强调需要开发自然可解释的方法而不是依赖可能的幻觉自我解释。</li>
<li>results: 论文指出，虽然大语言模型在心理健康应用中显示了推荐的可能性，但人类心理师的Empathy、细腻的解释和情境意识仍然是不可取代的，大语言模型应该被视为人类专家的工具而不是替代者。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have become valuable assets in mental health, showing promise in both classification tasks and counseling applications. This paper offers a perspective on using LLMs in mental health applications. It discusses the instability of generative models for prediction and the potential for generating hallucinatory outputs, underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability. The paper also distinguishes between the often interchangeable terms ``explainability'' and ``interpretability'', advocating for developing inherently interpretable methods instead of relying on potentially hallucinated self-explanations generated by LLMs. Despite the advancements in LLMs, human counselors' empathetic understanding, nuanced interpretation, and contextual awareness remain irreplaceable in the sensitive and complex realm of mental health counseling. The use of LLMs should be approached with a judicious and considerate mindset, viewing them as tools that complement human expertise rather than seeking to replace it.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:大型语言模型 (LLMs) 已成为心理健康领域的有价资产，在分类任务和咨询应用中显示了推荐的潜力。本文对使用 LLMs 在心理健康领域的应用提供了一种见解，讨论了预测时的生成模型的不稳定性和潜在的幻视出力，强调需要进行持续的审核和评估，以确保它们的可靠性和可靠性。文章也区别了“解释”和“解读”两个概念，主张发展自然可解释的方法，而不是依赖 LLMs 自带的可能幻视的自我解释。尽管 LLMs 的进步，但人类辅导员的 Empathy 理解、细化解释和场景意识仍然无法被取代在心理健康辅导中。使用 LLMs 应以谨慎和考虑的心态，视它们为人类专家知识的辅助工具，而不是尝试取代它们。
</details></li>
</ul>
<hr>
<h2 id="Causal-ATE-Mitigates-Unintended-Bias-in-Controlled-Text-Generation"><a href="#Causal-ATE-Mitigates-Unintended-Bias-in-Controlled-Text-Generation" class="headerlink" title="Causal ATE Mitigates Unintended Bias in Controlled Text Generation"></a>Causal ATE Mitigates Unintended Bias in Controlled Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11229">http://arxiv.org/abs/2311.11229</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Madhavan, Kahini Wadhawan</li>
<li>for: 这 paper 是研究语言模型中的特性控制问题，通过 causal average treatment effect（Causal ATE）方法进行研究。</li>
<li>methods: 该 paper 使用 causal ATE 方法来解决语言模型中的特性控制问题，并提供了一个理论基础和证明，证明该方法可以减少 false positive 的问题。</li>
<li>results: 该 paper 表明，使用 causal ATE 方法可以解决语言模型中的偶极现象问题，减少对保护群体的不必要偏见。<details>
<summary>Abstract</summary>
We study attribute control in language models through the method of Causal Average Treatment Effect (Causal ATE). Existing methods for the attribute control task in Language Models (LMs) check for the co-occurrence of words in a sentence with the attribute of interest, and control for them. However, spurious correlation of the words with the attribute in the training dataset, can cause models to hallucinate the presence of the attribute when presented with the spurious correlate during inference. We show that the simple perturbation-based method of Causal ATE removes this unintended effect. Additionally, we offer a theoretical foundation for investigating Causal ATE in the classification task, and prove that it reduces the number of false positives -- thereby mitigating the issue of unintended bias. Specifically, we ground it in the problem of toxicity mitigation, where a significant challenge lies in the inadvertent bias that often emerges towards protected groups post detoxification. We show that this unintended bias can be solved by the use of the Causal ATE metric.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:我们研究语言模型中的特性控制方法，使用 causal average treatment effect（Causal ATE）方法。现有的语言模型（LMs）中的特性控制方法通常是根据 interessant attribute 的搜索和控制。然而，在训练数据集中存在偶极相关性的情况下，这些方法可能会导致模型在推理过程中假设存在特性，从而导致模型“假象”现象。我们表明，使用简单的扰动基于的 Causal ATE 方法可以解除这种不良影响。此外，我们还提供了对 Causal ATE 在分类任务中的理论基础，并证明它可以减少 false positive，从而解决不良偏见问题。 Specifically, 我们将其应用于抑止攻击性 Mitigation 中的潜在偏见问题，这是一个主要挑战在推理过程中偶极相关性导致保护组中的不良偏见。我们表明，使用 Causal ATE  metric 可以解决这种偏见问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/cs.CL_2023_11_19/" data-id="clpztdnf800fees88e9cx2fdm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/cs.LG_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T10:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/19/cs.LG_2023_11_19/">cs.LG - 2023-11-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Physics-Enhanced-TinyML-for-Real-Time-Detection-of-Ground-Magnetic-Anomalies"><a href="#Physics-Enhanced-TinyML-for-Real-Time-Detection-of-Ground-Magnetic-Anomalies" class="headerlink" title="Physics-Enhanced TinyML for Real-Time Detection of Ground Magnetic Anomalies"></a>Physics-Enhanced TinyML for Real-Time Detection of Ground Magnetic Anomalies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11452">http://arxiv.org/abs/2311.11452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Talha Siddique, MD Shaad Mahmud</li>
<li>for: 这篇论文旨在发展一个基于物理指导的小型机器学习（Tiny Machine Learning）框架，以便提高宇宙天气预报中的精度和可靠性。</li>
<li>methods: 本论文使用了一个组合了物理基于调整和压缩的TinyML框架，以便实现高效的资料处理和模型预测。</li>
<li>results: 研究结果显示，该提出的框架能够提高预报的精度和可靠性，并且与传统方法进行比较，显示出该框架在宇宙天气预报中的应用前景。<details>
<summary>Abstract</summary>
Space weather phenomena like geomagnetic disturbances (GMDs) and geomagnetically induced currents (GICs) pose significant risks to critical technological infrastructure. While traditional predictive models, grounded in simulation, hold theoretical robustness, they grapple with challenges, notably the assimilation of imprecise data and extensive computational complexities. In recent years, Tiny Machine Learning (TinyML) has been adopted to develop Machine Learning (ML)-enabled magnetometer systems for predicting real-time terrestrial magnetic perturbations as a proxy measure for GIC. While TinyML offers efficient, real-time data processing, its intrinsic limitations prevent the utilization of robust methods with high computational needs. This paper developed a physics-guided TinyML framework to address the above challenges. This framework integrates physics-based regularization at the stages of model training and compression, thereby augmenting the reliability of predictions. The developed pruning scheme within the framework harnesses the inherent physical characteristics of the domain, striking a balance between model size and robustness. The study presents empirical results, drawing a comprehensive comparison between the accuracy and reliability of the developed framework and its traditional counterpart. Such a comparative analysis underscores the prospective applicability of the developed framework in conceptualizing robust, ML-enabled magnetometer systems for real-time space weather forecasting.
</details>
<details>
<summary>摘要</summary>
Space weather phenomena like geomagnetic disturbances (GMDs) and geomagnetically induced currents (GICs) pose significant risks to critical technological infrastructure. While traditional predictive models, grounded in simulation, hold theoretical robustness, they grapple with challenges, notably the assimilation of imprecise data and extensive computational complexities. In recent years, Tiny Machine Learning (TinyML) has been adopted to develop Machine Learning (ML)-enabled magnetometer systems for predicting real-time terrestrial magnetic perturbations as a proxy measure for GIC. While TinyML offers efficient, real-time data processing, its intrinsic limitations prevent the utilization of robust methods with high computational needs. This paper developed a physics-guided TinyML framework to address the above challenges. This framework integrates physics-based regularization at the stages of model training and compression, thereby augmenting the reliability of predictions. The developed pruning scheme within the framework harnesses the inherent physical characteristics of the domain, striking a balance between model size and robustness. The study presents empirical results, drawing a comprehensive comparison between the accuracy and reliability of the developed framework and its traditional counterpart. Such a comparative analysis underscores the prospective applicability of the developed framework in conceptualizing robust, ML-enabled magnetometer systems for real-time space weather forecasting.Here's the word-for-word translation of the text into Simplified Chinese:Space weather现象如地磁干扰(GMDs)和地磁引起的电流(GICs)对重要的技术基础设施 pose significant risks. While traditional predictive模型, grounded in simulation, hold theoretical robustness, they grapple with challenges, notably the assimilation of imprecise data and extensive computational complexities. In recent years, Tiny Machine Learning (TinyML) has been adopted to develop Machine Learning (ML)-enabled magnetometer systems for predicting real-time terrestrial magnetic perturbations as a proxy measure for GIC. While TinyML offers efficient, real-time data processing, its intrinsic limitations prevent the utilization of robust methods with high computational needs. This paper developed a physics-guided TinyML framework to address the above challenges. This framework integrates physics-based regularization at the stages of model training and compression, thereby augmenting the reliability of predictions. The developed pruning scheme within the framework harnesses the inherent physical characteristics of the domain, striking a balance between model size and robustness. The study presents empirical results, drawing a comprehensive comparison between the accuracy and reliability of the developed framework and its traditional counterpart. Such a comparative analysis underscores the prospective applicability of the developed framework in conceptualizing robust, ML-enabled magnetometer systems for real-time space weather forecasting.
</details></li>
</ul>
<hr>
<h2 id="Weight-Norm-Control"><a href="#Weight-Norm-Control" class="headerlink" title="Weight Norm Control"></a>Weight Norm Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11446">http://arxiv.org/abs/2311.11446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Ilya Loshchilov</li>
<li>for: 本研究证明了预设 weights 的 norm 值不一定是最佳的，可以考虑其他 norm 值来优化模型性能。</li>
<li>methods: 本文提出了一种基于 weight norm control 的权重衰减正则化方法，并证明了这种方法可以与 Adam 等优化器结合使用。</li>
<li>results: 通过实验研究，本文发现了使用 weight norm control 正则化方法可以提高模型性能，并且可以与 Adam 等优化器结合使用。<details>
<summary>Abstract</summary>
We note that decoupled weight decay regularization is a particular case of weight norm control where the target norm of weights is set to 0. Any optimization method (e.g., Adam) which uses decoupled weight decay regularization (respectively, AdamW) can be viewed as a particular case of a more general algorithm with weight norm control (respectively, AdamWN). We argue that setting the target norm of weights to 0 can be suboptimal and other target norm values can be considered. For instance, any training run where AdamW achieves a particular norm of weights can be challenged by AdamWN scheduled to achieve a comparable norm of weights. We discuss various implications of introducing weight norm control instead of weight decay.
</details>
<details>
<summary>摘要</summary>
我们注意到分离式重量衰减调教是重量 нор 控制的特例，其中Target norm of weights是设置为0。任何优化方法（例如Adam）使用分离式重量衰减调教（即AdamW）可以被视为一个更通用的算法，并且可以考虑其他Target norm值。例如，任何训练运行使用AdamW achieve particular norm of weights时，可以被挑战由AdamWN scheduled to achieve comparable norm of weights。我们讨论了将 weight norm control 替代 weight decay 的各种后果。
</details></li>
</ul>
<hr>
<h2 id="Duality-of-Bures-and-Shape-Distances-with-Implications-for-Comparing-Neural-Representations"><a href="#Duality-of-Bures-and-Shape-Distances-with-Implications-for-Comparing-Neural-Representations" class="headerlink" title="Duality of Bures and Shape Distances with Implications for Comparing Neural Representations"></a>Duality of Bures and Shape Distances with Implications for Comparing Neural Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11436">http://arxiv.org/abs/2311.11436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah E. Harvey, Brett W. Larsen, Alex H. Williams</li>
<li>for: 这篇论文旨在统一两类神经网络表示之间的不同类型的相似度量表示法，并探讨这些方法之间的关系。</li>
<li>methods: 这篇论文使用了cosine函数来关联category 1中的里曼射影shape distance与category 2中的normalized Bures similarity，从而带来新的解释和对CKA相似度量的比较。</li>
<li>results: 研究发现，cosine函数的里曼射影shape distance与normalized Bures similarity之间存在等式关系，这些结果带来新的解释和对CKA相似度量的比较。<details>
<summary>Abstract</summary>
A multitude of (dis)similarity measures between neural network representations have been proposed, resulting in a fragmented research landscape. Most of these measures fall into one of two categories.   First, measures such as linear regression, canonical correlations analysis (CCA), and shape distances, all learn explicit mappings between neural units to quantify similarity while accounting for expected invariances. Second, measures such as representational similarity analysis (RSA), centered kernel alignment (CKA), and normalized Bures similarity (NBS) all quantify similarity in summary statistics, such as stimulus-by-stimulus kernel matrices, which are already invariant to expected symmetries. Here, we take steps towards unifying these two broad categories of methods by observing that the cosine of the Riemannian shape distance (from category 1) is equal to NBS (from category 2). We explore how this connection leads to new interpretations of shape distances and NBS, and draw contrasts of these measures with CKA, a popular similarity measure in the deep learning literature.
</details>
<details>
<summary>摘要</summary>
一种多样的（不同）相似度测量方法between神经网络表示方式已经被提出，导致了一个分散的研究领域。大多数这些测量方法都 falls into two categories。  first, measures such as linear regression, canonical correlations analysis (CCA), and shape distances, all learn explicit mappings between neural units to quantify similarity while accounting for expected invariances. Second, measures such as representational similarity analysis (RSA), centered kernel alignment (CKA), and normalized Bures similarity (NBS) all quantify similarity in summary statistics, such as stimulus-by-stimulus kernel matrices, which are already invariant to expected symmetries. 在这篇文章中，我们尝试通过观察cosine of the Riemannian shape distance (from category 1) is equal to NBS (from category 2)来统一这两类方法。我们探索这个连接如何导致新的Shape distances和NBS的解释，并与CKA, a popular similarity measure in the deep learning literature,进行比较。
</details></li>
</ul>
<hr>
<h2 id="Fast-Heavy-Inner-Product-Identification-Between-Weights-and-Inputs-in-Neural-Network-Training"><a href="#Fast-Heavy-Inner-Product-Identification-Between-Weights-and-Inputs-in-Neural-Network-Training" class="headerlink" title="Fast Heavy Inner Product Identification Between Weights and Inputs in Neural Network Training"></a>Fast Heavy Inner Product Identification Between Weights and Inputs in Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11429">http://arxiv.org/abs/2311.11429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lianke Qin, Saayan Mitra, Zhao Song, Yuanyuan Yang, Tianyi Zhou</li>
<li>for: 这个论文解决了一个含有内积计算的内积识别问题，该问题泛化了光棒问题（参考文献：\cite{prr89}）。给定两个集合 $A \subset {-1,+1}^d$ 和 $B \subset {-1,+1}^d$，其中 $|A|&#x3D;|B|&#x3D;n$，如果存在 exact $k$ 对 whose inner product exceeds a certain threshold, i.e., ${(a_1, b_1), \cdots, (a_k, b_k)} \subset A \times B$ such that $\forall i \in [k], \langle a_i,b_i \rangle \geq \rho \cdot d$，where $\rho \in (0,1)$ is the threshold, then the goal is to identify those $k$ heavy inner product pairs.</li>
<li>methods: 我们提供了一种算法，运行时间为 $O(n^{2 \omega &#x2F; 3+ o(1)})$，可以高 probabilty 地找到 exceeds $\rho \cdot d$ 的 $k$ inner product pair。该算法基于 matrix multiplication 和 random sampling 技术。</li>
<li>results: 我们的算法可以快速地训练具有 ReLU 活化函数的神经网络。<details>
<summary>Abstract</summary>
In this paper, we consider a heavy inner product identification problem, which generalizes the Light Bulb problem~(\cite{prr89}): Given two sets $A \subset \{-1,+1\}^d$ and $B \subset \{-1,+1\}^d$ with $|A|=|B| = n$, if there are exact $k$ pairs whose inner product passes a certain threshold, i.e., $\{(a_1, b_1), \cdots, (a_k, b_k)\} \subset A \times B$ such that $\forall i \in [k], \langle a_i,b_i \rangle \geq \rho \cdot d$, for a threshold $\rho \in (0,1)$, the goal is to identify those $k$ heavy inner products. We provide an algorithm that runs in $O(n^{2 \omega / 3+ o(1)})$ time to find the $k$ inner product pairs that surpass $\rho \cdot d$ threshold with high probability, where $\omega$ is the current matrix multiplication exponent. By solving this problem, our method speed up the training of neural networks with ReLU activation function.
</details>
<details>
<summary>摘要</summary>
在本文中，我们考虑了一个重要的内积识别问题，该问题泛化了光泽问题（参考\cite{prr89}）：给定两个集合 $A \subset \{-1,+1\}^d$ 和 $B \subset \{-1,+1\}^d$，其中 $|A|=|B|=n$，如果存在 exact $k$ 对 whose inner product exceeds a certain threshold, i.e., $\{(a_1, b_1), \ldots, (a_k, b_k)\} \subset A \times B$ such that $\forall i \in [k], \langle a_i, b_i \rangle \geq \rho \cdot d$, for a threshold $\rho \in (0,1)$, then the goal is to identify those $k$ heavy inner product pairs. We provide an algorithm that runs in $O(n^{2 \omega / 3+ o(1)})$ time to find the $k$ inner product pairs that surpass $\rho \cdot d$ threshold with high probability, where $\omega$ is the current matrix multiplication exponent. By solving this problem, our method can speed up the training of neural networks with ReLU activation function.
</details></li>
</ul>
<hr>
<h2 id="Tensor-Aware-Energy-Accounting"><a href="#Tensor-Aware-Energy-Accounting" class="headerlink" title="Tensor-Aware Energy Accounting"></a>Tensor-Aware Energy Accounting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11424">http://arxiv.org/abs/2311.11424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/project-smaragdine/smaragdine">https://github.com/project-smaragdine/smaragdine</a></li>
<li>paper_authors: Timur Babakol, Yu David Liu</li>
<li>For: This paper aims to introduce Smaragdine, a novel energy accounting system for tensor-based deep learning (DL) programs implemented with TensorFlow, to improve the energy efficiency of DL applications.* Methods: Smaragdine uses a novel white-box methodology of energy accounting that is aware of the internal structure of the DL program, allowing for a detailed breakdown of energy consumption by units aligned with the logical hierarchical decomposition structure.* Results: Smaragdine was applied to understand the energy behavior of BERT, a widely used language model, and was capable of identifying the highest energy&#x2F;power-consuming components of BERT layer-by-layer and tensor-by-tensor. Additionally, two case studies were conducted to demonstrate the effectiveness of Smaragdine in supporting downstream toolchain building, one comparing the energy impact of hyperparameter tuning of BERT and the other analyzing the energy behavior evolution of BERT as it evolves to its next generation, ALBERT.<details>
<summary>Abstract</summary>
With the rapid growth of Artificial Intelligence (AI) applications supported by deep learning (DL), the energy efficiency of these applications has an increasingly large impact on sustainability. We introduce Smaragdine, a new energy accounting system for tensor-based DL programs implemented with TensorFlow. At the heart of Smaragdine is a novel white-box methodology of energy accounting: Smaragdine is aware of the internal structure of the DL program, which we call tensor-aware energy accounting. With Smaragdine, the energy consumption of a DL program can be broken down into units aligned with its logical hierarchical decomposition structure. We apply Smaragdine for understanding the energy behavior of BERT, one of the most widely used language models. Layer-by-layer and tensor-by-tensor, Smaragdine is capable of identifying the highest energy/power-consuming components of BERT. Furthermore, we conduct two case studies on how Smaragdine supports downstream toolchain building, one on the comparative energy impact of hyperparameter tuning of BERT, the other on the energy behavior evolution when BERT evolves to its next generation, ALBERT.
</details>
<details>
<summary>摘要</summary>
随着人工智能（AI）应用程序基于深度学习（DL）的快速发展，这些应用程序的能源效率已经对可持续发展产生了越来越大的影响。我们介绍了一种新的能源账务系统：Smaragdine，用于 tensor-based DL 程序实现的 TensorFlow。Smaragdine 的核心是一种新的白盒方法：tensor-aware energy accounting，它能够跟踪 tensor 级别的能源消耗。我们通过应用 Smaragdine 来理解 BERT 语言模型的能量行为。层次地和tensor地，Smaragdine 可以识别 BERT 中最高能耗/功率消耗的组件。此外，我们还进行了两个 случа研究，一是关于 BERT 的hyperparameter 优化对能源影响的比较，另一是关于 BERT 进化到下一代 ALBERT 时的能量行为演化。
</details></li>
</ul>
<hr>
<h2 id="Offline-Reinforcement-Learning-for-Wireless-Network-Optimization-with-Mixture-Datasets"><a href="#Offline-Reinforcement-Learning-for-Wireless-Network-Optimization-with-Mixture-Datasets" class="headerlink" title="Offline Reinforcement Learning for Wireless Network Optimization with Mixture Datasets"></a>Offline Reinforcement Learning for Wireless Network Optimization with Mixture Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11423">http://arxiv.org/abs/2311.11423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Yang, Cong Shen, Jing Yang, Shu-ping Yeh, Jerry Sydir</li>
<li>For: 这个论文的目的是研究在无线通信系统中应用 reinforcement learning (RL) 的在线学习方法，以解决无线 radio 资源管理 (RRM) 问题。* Methods: 这个论文使用了多种 state-of-the-art 的 offline RL 算法，包括行为约束 Q-学习 (BCQ)、保守 Q-学习 (CQL) 和隐式 Q-学习 (IQL)，来解决一个具体的 RRM 问题，该问题的目标是通过用户调度来最大化一个线性组合 {of sum和} 5-percentile 率。* Results: 论文发现了在 RRM 问题上使用 offline RL 的性能取决于用于数据收集的行为策略，并提出了一种新的 offline RL 解决方案，该解决方案可以利用不同的行为策略收集的多种数据进行权重平均，从而生成一个近似于最优 RL 策略。<details>
<summary>Abstract</summary>
The recent development of reinforcement learning (RL) has boosted the adoption of online RL for wireless radio resource management (RRM). However, online RL algorithms require direct interactions with the environment, which may be undesirable given the potential performance loss due to the unavoidable exploration in RL. In this work, we first investigate the use of \emph{offline} RL algorithms in solving the RRM problem. We evaluate several state-of-the-art offline RL algorithms, including behavior constrained Q-learning (BCQ), conservative Q-learning (CQL), and implicit Q-learning (IQL), for a specific RRM problem that aims at maximizing a linear combination {of sum and} 5-percentile rates via user scheduling. We observe that the performance of offline RL for the RRM problem depends critically on the behavior policy used for data collection, and further propose a novel offline RL solution that leverages heterogeneous datasets collected by different behavior policies. We show that with a proper mixture of the datasets, offline RL can produce a near-optimal RL policy even when all involved behavior policies are highly suboptimal.
</details>
<details>
<summary>摘要</summary>
近期的增强学习（RL）发展，使得在线RL在无线电路资源管理（RRM）中得到广泛应用。然而，在线RL算法需要直接与环境交互，这可能是不良的，因为RL中不可避免的探索可能会导致性能下降。在这种情况下，我们首先研究使用停机RL算法解决RRM问题。我们评估了一些当今最佳的停机RL算法，包括行为约束Q学习（BCQ）、保守Q学习（CQL）和隐式Q学习（IQL），并评估其在特定RRM问题上的性能。我们发现，停机RL在RRM问题中的性能取决于用于数据采集的行为策略，并提出了一种新的停机RL解决方案，该解决方案利用不同行为策略收集的多种多样的数据进行混合。我们表明，将这些数据进行混合，停机RL可以生成一个几乎优质RL策略，即使所有参与的行为策略都是高度不优的。
</details></li>
</ul>
<hr>
<h2 id="Precision-at-the-indistinguishability-threshold-a-method-for-evaluating-classification-algorithms"><a href="#Precision-at-the-indistinguishability-threshold-a-method-for-evaluating-classification-algorithms" class="headerlink" title="Precision at the indistinguishability threshold: a method for evaluating classification algorithms"></a>Precision at the indistinguishability threshold: a method for evaluating classification algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11422">http://arxiv.org/abs/2311.11422</a></li>
<li>repo_url: None</li>
<li>paper_authors: David J. T. Sumpter</li>
<li>for: 本研究想要解决现有的分类算法评价指标问题，提出一个新的指标来衡量分类算法的性能。</li>
<li>methods: 本研究使用一个新的指标，即“ precisions at the indistinguishability threshold”，这个指标可以衡量一个分类算法在分类精度和准确率之间的融合。</li>
<li>results: 研究发现，这个新指标可以更好地衡量分类算法的性能，而且不同于现有的指标，如AUC和F1-score，它不受分类精度和准确率的融合影响。<details>
<summary>Abstract</summary>
There exist a wide range of single number metrics for assessing performance of classification algorithms, including AUC and the F1-score (Wikipedia lists 17 such metrics, with 27 different names). In this article, I propose a new metric to answer the following question: when an algorithm is tuned so that it can no longer distinguish labelled cats from real cats, how often does a randomly chosen image that has been labelled as containing a cat actually contain a cat? The steps to construct this metric are as follows. First, we set a threshold score such that when the algorithm is shown two randomly-chosen images -- one that has a score greater than the threshold (i.e. a picture labelled as containing a cat) and another from those pictures that really does contain a cat -- the probability that the image with the highest score is the one chosen from the set of real cat images is 50\%. At this decision threshold, the set of positively labelled images are indistinguishable from the set of images which are positive. Then, as a second step, we measure performance by asking how often a randomly chosen picture from those labelled as containing a cat actually contains a cat. This metric can be thought of as {\it precision at the indistinguishability threshold}. While this new metric doesn't address the tradeoff between precision and recall inherent to all such metrics, I do show why this method avoids pitfalls that can occur when using, for example AUC, and it is better motivated than, for example, the F1-score.
</details>
<details>
<summary>摘要</summary>
存在许多单数量指标用于评估分类算法的性能，包括AUC和F1分数（Wikipedia列出17种指标，共27个不同的名称）。在这篇文章中，我提出了一个新的指标，用于回答以下问题：当一个算法通过调整后不能区分标注的猫和实际的猫之间，某个随机选择的图像中是否实际上包含猫？为构建这个指标，我们首先设置一个阈值分数，使得当算法被给两个随机选择的图像——一个分数高于阈值（即一个标注为猫的图像）和另一个来自真正的猫图像集中的图像——的概率是50%。在这个决策阈值下，标注为猫的图像集和真正的猫图像集是无法区分的。然后，作为第二步，我们测量性能的方式是，随机选择标注为猫的图像中是否实际上包含猫。这个指标可以被理解为“在不可区分性阈值上的准确率”。这个新指标不同于AUC和F1分数等其他指标，它不处理分类算法的准确率和敏感度之间的负担。然而，我展示了这种方法可以避免使用AUC等指标时可能出现的坑，并且更有motivation чемF1分数。
</details></li>
</ul>
<hr>
<h2 id="Large-Pre-trained-time-series-models-for-cross-domain-Time-series-analysis-tasks"><a href="#Large-Pre-trained-time-series-models-for-cross-domain-Time-series-analysis-tasks" class="headerlink" title="Large Pre-trained time series models for cross-domain Time series analysis tasks"></a>Large Pre-trained time series models for cross-domain Time series analysis tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11413">http://arxiv.org/abs/2311.11413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harshavardhan Kamarthi, B. Aditya Prakash</li>
<li>For: 这个论文的目的是为了预训一个通用时序序列模型，以便在不同领域的时序序列分析任务中提供更高的性能。* Methods: 这篇论文使用了一种新的自适应分割策略，该策略可以自动根据不同领域的时序序列数据来选择最佳的分割策略，并在预训期间使用自然语言搅拌学习损失来学习这些分割策略。* Results: 论文的实验结果表明，使用该自适应分割策略和自然语言搅拌学习损失可以在多个不同领域的时序序列分析任务中提供类似或更好的性能，并且需要训练时间和数据量的减少。<details>
<summary>Abstract</summary>
Large pre-trained models have been instrumental in significant advancements in domains like language and vision making model training for individual downstream tasks more efficient as well as provide superior performance. However, tackling time-series analysis tasks usually involves designing and training a separate model from scratch leveraging training data and domain expertise specific to the task. We tackle a significant challenge for pre-training a general time-series model from multiple heterogeneous time-series dataset: providing semantically useful inputs to models for modeling time series of different dynamics from different domains. We observe that partitioning time-series into segments as inputs to sequential models produces semantically better inputs and propose a novel model LPTM that automatically identifies optimal dataset-specific segmentation strategy leveraging self-supervised learning loss during pre-training. LPTM provides performance similar to or better than domain-specific state-of-art model and is significantly more data and compute efficient taking up to 40% less data as well as 50% less training time to achieve state-of-art performance in a wide range of time-series analysis tasks from multiple disparate domain.
</details>
<details>
<summary>摘要</summary>
大型预训模型在语言和视觉等领域取得了重要进步，使模型训练 для特定下游任务变得更加效率，并提供了更高的性能。然而，对时序分析任务通常需要针对特定任务和领域设计和训练独立的模型。我们解决了在多个不同领域的时序数据上预训一个通用时序模型的挑战：提供Semantically meaningful输入模型，以便模型可以更好地处理不同领域的时序序列。我们发现，将时序序列分割成段 inputs to sequential models生成更有意义的输入，并提出了一种新的模型LPTM，可以自动确定数据集特定的优化分割策略，通过自动学习损失来进行预训。LPTM在多个不同领域的时序分析任务中提供了和领域特定状态态的模型性能相似或更好的性能，并且在训练时间和数据量方面具有50%和40%的下降。
</details></li>
</ul>
<hr>
<h2 id="Negotiated-Representations-for-Machine-Mearning-Application"><a href="#Negotiated-Representations-for-Machine-Mearning-Application" class="headerlink" title="Negotiated Representations for Machine Mearning Application"></a>Negotiated Representations for Machine Mearning Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11410">http://arxiv.org/abs/2311.11410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nurikorhan/negotiated-representations">https://github.com/nurikorhan/negotiated-representations</a></li>
<li>paper_authors: Nuri Korhan, Samet Bayram</li>
<li>for: 提高机器学习模型的分类精度和避免过拟合</li>
<li>methods: 通过模型与已知标签之间的谈判来增强模型的解释能力和泛化能力</li>
<li>results: 通过在CIFAR 10、CIFAR 100和MNIST等低度机器学习问题上实验，实现了提高分类精度和降低过拟合的目标，并且达到了许多其他研究领域的应用前提。<details>
<summary>Abstract</summary>
Overfitting is a phenomenon that occurs when a machine learning model is trained for too long and focused too much on the exact fitness of the training samples to the provided training labels and cannot keep track of the predictive rules that would be useful on the test data. This phenomenon is commonly attributed to memorization of particular samples, memorization of the noise, and forced fitness into a data set of limited samples by using a high number of neurons. While it is true that the model encodes various peculiarities as the training process continues, we argue that most of the overfitting occurs in the process of reconciling sharply defined membership ratios. In this study, we present an approach that increases the classification accuracy of machine learning models by allowing the model to negotiate output representations of the samples with previously determined class labels. By setting up a negotiation between the models interpretation of the inputs and the provided labels, we not only increased average classification accuracy but also decreased the rate of overfitting without applying any other regularization tricks. By implementing our negotiation paradigm approach to several low regime machine learning problems by generating overfitting scenarios from publicly available data sets such as CIFAR 10, CIFAR 100, and MNIST we have demonstrated that the proposed paradigm has more capacity than its intended purpose. We are sharing the experimental results and inviting the machine learning community to explore the limits of the proposed paradigm. We also aim to incentive the community to exploit the negotiation paradigm to overcome the learning related challenges in other research fields such as continual learning. The Python code of the experimental setup is uploaded to GitHub.
</details>
<details>
<summary>摘要</summary>
“过拟合”是一种现象，当机器学习模型在训练过程中进行了太长时间，专注过多在训练样本与提供的训练标签之间的精确对应，而无法维护预测规律，导致模型对于测试样本的预测失败。这现象通常被归因于模型对特定样本的记忆、对于样本的噪音的记忆以及使用高量的神经元。虽然模型在训练过程中encode了许多特性，但我们认为大多数过拟合发生在推导鲜明的会员比率。在这篇研究中，我们提出了一种方法，可以增加机器学习模型的分类精度，并降低过拟合的比率，不需要其他调整技巧。我们通过在模型对输入的解释和提供的标签之间进行协商，使模型能够更好地预测测试样本。我们将这个协商 paradigm 应用到了一些低度机器学习问题中，从公共可用数据集如 CIFAR 10、CIFAR 100 和 MNIST 中产生了过拟合场景，并证明了我们的方法有更多的容量。我们将实验结果分享，邀请机器学习社区探索这个方法的限制，并务实阶段的挑战。我们还希望通过这个协商方法，导致其他研究领域，如持续学习，的挑战。Python 代码的实验设置已经上传到 GitHub。
</details></li>
</ul>
<hr>
<h2 id="Towards-interpretable-by-design-deep-learning-algorithms"><a href="#Towards-interpretable-by-design-deep-learning-algorithms" class="headerlink" title="Towards interpretable-by-design deep learning algorithms"></a>Towards interpretable-by-design deep learning algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11396">http://arxiv.org/abs/2311.11396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Plamen Angelov, Dmitry Kangin, Ziyang Zhang</li>
<li>for: 这个论文旨在提出一种能够解释深度学习模型的框架，以便在深度学习模型的基础上构建更加解释性的模型。</li>
<li>methods: 该论文使用了现有的大 neural network（如视transformer）的固有空间，并将其转化为一种基于类似度的函数，以实现解释性。</li>
<li>results: 该论文的结果表明，可以使用这种方法转化深度学习模型，以获得更加解释性的模型，并且可以在不需要微调特征空间的情况下进行分类学习。<details>
<summary>Abstract</summary>
The proposed framework named IDEAL (Interpretable-by-design DEep learning ALgorithms) recasts the standard supervised classification problem into a function of similarity to a set of prototypes derived from the training data, while taking advantage of existing latent spaces of large neural networks forming so-called Foundation Models (FM). This addresses the issue of explainability (stage B) while retaining the benefits from the tremendous achievements offered by DL models (e.g., visual transformers, ViT) pre-trained on huge data sets such as IG-3.6B + ImageNet-1K or LVD-142M (stage A). We show that one can turn such DL models into conceptually simpler, explainable-through-prototypes ones.   The key findings can be summarized as follows: (1) the proposed models are interpretable through prototypes, mitigating the issue of confounded interpretations, (2) the proposed IDEAL framework circumvents the issue of catastrophic forgetting allowing efficient class-incremental learning, and (3) the proposed IDEAL approach demonstrates that ViT architectures narrow the gap between finetuned and non-finetuned models allowing for transfer learning in a fraction of time \textbf{without} finetuning of the feature space on a target dataset with iterative supervised methods.
</details>
<details>
<summary>摘要</summary>
提案的框架名为IDEAL（可解释性设计深度学习算法）将标准的监督学习问题转化为与训练数据集中的prototype函数的函数，同时利用现有的大神经网络的固有空间，即基础模型（FM）。这种方法解决了解释性问题（stage B），而不失去深度学习模型的成果（如视觉转换器、ViT）在大量数据集上的辉煌成果（stage A）。我们示出了将这些深度学习模型转化成概念更简单、可解释的模型的可能性。主要发现如下：1. 提案的模型可以通过prototype来解释，解决了混淆的解释问题。2. IDEAL框架缺少牺牲性学习问题，允许高效的分类增量学习。3. IDEAL方法示出了使用ViT架构，在缺少finetuning的情况下，在短时间内完成转移学习，使得模型的表现与finetuning的情况几乎相同。
</details></li>
</ul>
<hr>
<h2 id="Addressing-the-speed-accuracy-simulation-trade-off-for-adaptive-spiking-neurons"><a href="#Addressing-the-speed-accuracy-simulation-trade-off-for-adaptive-spiking-neurons" class="headerlink" title="Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons"></a>Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11390">http://arxiv.org/abs/2311.11390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/webstorms/blocks">https://github.com/webstorms/blocks</a></li>
<li>paper_authors: Luke Taylor, Andrew J King, Nicol S Harper</li>
<li>for: 这个论文的目的是解决计算神经科学中的逐次采样问题，提高模型的训练速度而不损失准确性。</li>
<li>methods: 这个论文使用了对ALIF模型的算法重新解释，将模型的顺序 simulations 复杂度降低到最低，并使用GPU进行并行化。</li>
<li>results: 该实现可以在使用小步长DT时获得大约50倍的训练速度提升，并在不同的超visisted classification任务上达到与标准ALIF实现相同的性能，但是在训练时间上占了一个小 Fraction。此外，该模型还可以快速和准确地适应真实的电physiological记录，以捕捉精确的脉冲时间。<details>
<summary>Abstract</summary>
The adaptive leaky integrate-and-fire (ALIF) model is fundamental within computational neuroscience and has been instrumental in studying our brains $\textit{in silico}$. Due to the sequential nature of simulating these neural models, a commonly faced issue is the speed-accuracy trade-off: either accurately simulate a neuron using a small discretisation time-step (DT), which is slow, or more quickly simulate a neuron using a larger DT and incur a loss in simulation accuracy. Here we provide a solution to this dilemma, by algorithmically reinterpreting the ALIF model, reducing the sequential simulation complexity and permitting a more efficient parallelisation on GPUs. We computationally validate our implementation to obtain over a $50\times$ training speedup using small DTs on synthetic benchmarks. We also obtained a comparable performance to the standard ALIF implementation on different supervised classification tasks - yet in a fraction of the training time. Lastly, we showcase how our model makes it possible to quickly and accurately fit real electrophysiological recordings of cortical neurons, where very fine sub-millisecond DTs are crucial for capturing exact spike timing.
</details>
<details>
<summary>摘要</summary>
computacional neuroscience中的适应泄漏集成和发射（ALIF）模型是基础性的，它在研究大脑的计算中扮演着重要的角色。由于计算这些神经网络模型的顺序性，常常会遇到速度精度之间的贸易OFF：可以使用小步长时间步（DT）准确地模拟神经元，但是这会比较慢；或者使用大步长时间步（DT）更快地模拟神经元，但是会导致模拟精度下降。在这篇文章中，我们提供了一种解决这个困境的方法，通过算法性地重新解释ALIF模型，从而降低了计算神经网络模型的顺序化复杂性，并使得在GPU上更有效地并行化。我们通过计算 validate我们的实现，在使用小DT的情况下达到了50倍以上的训练速度提升。此外，我们还证明了我们的实现在不同的Supervised classification任务中具有相同的性能，但是在训练时间上占了一小部分。最后，我们展示了我们的模型可以快速地和准确地适应实际的电physiological记录，其中非常细小的毫秒级DT是关键的，以捕捉精确的发射时间。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Reinforcement-Learning-with-Mixture-of-Orthogonal-Experts"><a href="#Multi-Task-Reinforcement-Learning-with-Mixture-of-Orthogonal-Experts" class="headerlink" title="Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts"></a>Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11385">http://arxiv.org/abs/2311.11385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Hendawy, Jan Peters, Carlo D’Eramo</li>
<li>for: 解决多任务 reinforcement learning 中agent的技能可重用性问题</li>
<li>methods: 使用mixture of orthogonal experts（MOORE）方法，通过 Gram-Schmidt  процесс将任务特定信息转化为共享表示</li>
<li>results: 在 MiniGrid 和 MetaWorld 两个多任务 reinforcement learning  benchmark 上，MOORE 方法比基线相对较高，在 MetaWorld 上达到了新的状态纪录<details>
<summary>Abstract</summary>
Multi-Task Reinforcement Learning (MTRL) tackles the long-standing problem of endowing agents with skills that generalize across a variety of problems. To this end, sharing representations plays a fundamental role in capturing both unique and common characteristics of the tasks. Tasks may exhibit similarities in terms of skills, objects, or physical properties while leveraging their representations eases the achievement of a universal policy. Nevertheless, the pursuit of learning a shared set of diverse representations is still an open challenge. In this paper, we introduce a novel approach for representation learning in MTRL that encapsulates common structures among the tasks using orthogonal representations to promote diversity. Our method, named Mixture Of Orthogonal Experts (MOORE), leverages a Gram-Schmidt process to shape a shared subspace of representations generated by a mixture of experts. When task-specific information is provided, MOORE generates relevant representations from this shared subspace. We assess the effectiveness of our approach on two MTRL benchmarks, namely MiniGrid and MetaWorld, showing that MOORE surpasses related baselines and establishes a new state-of-the-art result on MetaWorld.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimal-Locally-Private-Nonparametric-Classification-with-Public-Data"><a href="#Optimal-Locally-Private-Nonparametric-Classification-with-Public-Data" class="headerlink" title="Optimal Locally Private Nonparametric Classification with Public Data"></a>Optimal Locally Private Nonparametric Classification with Public Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11369">http://arxiv.org/abs/2311.11369</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/karlmyh/lpct">https://github.com/karlmyh/lpct</a></li>
<li>paper_authors: Yuheng Ma, Hanfang Yang</li>
<li>for:  investigate the problem of public data-assisted non-interactive LDP (Local Differential Privacy) learning with a focus on non-parametric classification.</li>
<li>methods:  derive the mini-max optimal convergence rate with LDP constraint, and present a novel approach, the locally private classification tree, which attains the mini-max optimal convergence rate.</li>
<li>results:  comprehensive experiments conducted on synthetic and real datasets show the superior performance of our proposed method, and both theoretical and experimental findings demonstrate the effectiveness of public data compared to private data, leading to practical suggestions for prioritizing non-private data collection.<details>
<summary>Abstract</summary>
In this work, we investigate the problem of public data-assisted non-interactive LDP (Local Differential Privacy) learning with a focus on non-parametric classification. Under the posterior drift assumption, we for the first time derive the mini-max optimal convergence rate with LDP constraint. Then, we present a novel approach, the locally private classification tree, which attains the mini-max optimal convergence rate. Furthermore, we design a data-driven pruning procedure that avoids parameter tuning and produces a fast converging estimator. Comprehensive experiments conducted on synthetic and real datasets show the superior performance of our proposed method. Both our theoretical and experimental findings demonstrate the effectiveness of public data compared to private data, which leads to practical suggestions for prioritizing non-private data collection.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了公共数据协助非互动式LDP（本地差分隐私学习）问题，专注于非参数型分类。基于 posterior 漂移假设，我们首次 derivate 最优的 mini-max 收敛速率带 LDP 约束。然后，我们提出了一种新的方法， namely 地方隐私分类树，可以实现最优的收敛速率。此外，我们设计了一种基于数据驱动的剪裁过程，以避免参数调整并生成快速收敛的估计器。在 synthetic 和实际数据集上进行了广泛的实验，并证明了我们提出的方法的优越性。 both 我们的理论和实验结果表明，公共数据比私人数据更有效，这导致了实际中优先集集数据的建议。Note: Simplified Chinese is used here, as it is more commonly used in mainland China. If you prefer Traditional Chinese, I can also provide the translation.
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Pretraining-for-Heterogeneous-Hypergraph-Neural-Networks"><a href="#Self-Supervised-Pretraining-for-Heterogeneous-Hypergraph-Neural-Networks" class="headerlink" title="Self-Supervised Pretraining for Heterogeneous Hypergraph Neural Networks"></a>Self-Supervised Pretraining for Heterogeneous Hypergraph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11368">http://arxiv.org/abs/2311.11368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdalgader Abubaker, Takanori Maehara, Madhav Nimishakavi, Vassilis Plachouras</li>
<li>for: 本研究的目的是提出一种自动编码的普适批处理框架（SPHH），用于强化hetereogeneous HyperGNN的自然语言模型。</li>
<li>methods: 本研究使用了两种自然语言任务，即node classification和link prediction，来自动学习 HyperGNN 的局部和全局表示。</li>
<li>results: 实验结果表明，使用 SPHH 框架可以在不同的 HyperGNN 模型和下游任务中提高表示性能，并且在不同的数据集上具有一定的稳定性。<details>
<summary>Abstract</summary>
Recently, pretraining methods for the Graph Neural Networks (GNNs) have been successful at learning effective representations from unlabeled graph data. However, most of these methods rely on pairwise relations in the graph and do not capture the underling higher-order relations between entities. Hypergraphs are versatile and expressive structures that can effectively model higher-order relationships among entities in the data. Despite the efforts to adapt GNNs to hypergraphs (HyperGNN), there are currently no fully self-supervised pretraining methods for HyperGNN on heterogeneous hypergraphs. In this paper, we present SPHH, a novel self-supervised pretraining framework for heterogeneous HyperGNNs. Our method is able to effectively capture higher-order relations among entities in the data in a self-supervised manner. SPHH is consist of two self-supervised pretraining tasks that aim to simultaneously learn both local and global representations of the entities in the hypergraph by using informative representations derived from the hypergraph structure. Overall, our work presents a significant advancement in the field of self-supervised pretraining of HyperGNNs, and has the potential to improve the performance of various graph-based downstream tasks such as node classification and link prediction tasks which are mapped to hypergraph configuration. Our experiments on two real-world benchmarks using four different HyperGNN models show that our proposed SPHH framework consistently outperforms state-of-the-art baselines in various downstream tasks. The results demonstrate that SPHH is able to improve the performance of various HyperGNN models in various downstream tasks, regardless of their architecture or complexity, which highlights the robustness of our framework.
</details>
<details>
<summary>摘要</summary>
近期，对图神经网络（GNN）的预训练方法有了成功，可以从不标注的图数据中学习有效的表示。然而，大多数这些方法仅仅是基于图中的对应关系，而不是捕捉图中实体之间的更高级别关系。几何图（Hypergraphs）是一种灵活和表达力强的结构，可以有效地模型图中实体之间的数据。虽然有尝试将GNN应用于几何图（HyperGNN），但目前还没有完全自动预训练方法 для HyperGNN在不同类型的几何图上。在这篇论文中，我们提出了一种新的自动预训练框架（SPHH），可以有效地捕捉几何图中实体之间的更高级别关系。SPHH包括两个自动预训练任务，旨在同时学习实体在几何图中的本地和全局表示。我们使用几何图结构中的信息来 derivate 出有用的表示，以便在预训练过程中学习实体之间的关系。总的来说，我们的工作对自动预训练 HyperGNN 的领域做出了重要的进步，可以提高多种基于图的下游任务的性能，如节点分类和链接预测任务，这些任务可以与几何图配置相对应。我们在两个实际 benchmark 上使用四种不同的 HyperGNN 模型进行实验，得到的结果显示，我们提出的 SPHH 框架在多种下游任务中一直保持领先，并且可以在不同的 HyperGNN 模型和复杂度之间实现一致性，这说明了我们的框架的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Symmetry-invariant-quantum-machine-learning-force-fields"><a href="#Symmetry-invariant-quantum-machine-learning-force-fields" class="headerlink" title="Symmetry-invariant quantum machine learning force fields"></a>Symmetry-invariant quantum machine learning force fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11362">http://arxiv.org/abs/2311.11362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isabel Nha Minh Le, Oriel Kiss, Julian Schuhmacher, Ivano Tavernelli, Francesco Tacchino</li>
<li>for:  Computing efficient and accurate force fields for atomistic simulations using machine learning techniques and quantum computational methods.</li>
<li>methods:  Using variational quantum learning models to predict potential energy surfaces and atomic forces from ab initio training data, and incorporating physically relevant symmetries in quantum neural networks.</li>
<li>results:  Outperforming generic quantum learning models on individual molecules of growing complexity, and demonstrating the versatility of the approach on a water dimer as a minimal example of a system with multiple components.<details>
<summary>Abstract</summary>
Machine learning techniques are essential tools to compute efficient, yet accurate, force fields for atomistic simulations. This approach has recently been extended to incorporate quantum computational methods, making use of variational quantum learning models to predict potential energy surfaces and atomic forces from ab initio training data. However, the trainability and scalability of such models are still limited, due to both theoretical and practical barriers. Inspired by recent developments in geometric classical and quantum machine learning, here we design quantum neural networks that explicitly incorporate, as a data-inspired prior, an extensive set of physically relevant symmetries. We find that our invariant quantum learning models outperform their more generic counterparts on individual molecules of growing complexity. Furthermore, we study a water dimer as a minimal example of a system with multiple components, showcasing the versatility of our proposed approach and opening the way towards larger simulations. Our results suggest that molecular force fields generation can significantly profit from leveraging the framework of geometric quantum machine learning, and that chemical systems represent, in fact, an interesting and rich playground for the development and application of advanced quantum machine learning tools.
</details>
<details>
<summary>摘要</summary>
Inspired by recent developments in geometric classical and quantum machine learning, we design quantum neural networks that explicitly incorporate an extensive set of physically relevant symmetries as a data-inspired prior. We find that our invariant quantum learning models outperform their more generic counterparts on individual molecules of growing complexity.Furthermore, we study a water dimer as a minimal example of a system with multiple components, showcasing the versatility of our proposed approach and opening the way towards larger simulations. Our results suggest that molecular force fields generation can significantly profit from leveraging the framework of geometric quantum machine learning, and that chemical systems represent an interesting and rich playground for the development and application of advanced quantum machine learning tools.Translated into Simplified Chinese:机器学习技术是 Computational efficient yet accurate 力场计算的关键工具。最近，这些技术已经扩展到包括量子计算方法，使用变量量子学习模型预测可见能量表面和原子力从初始数据训练中。然而，这些模型的可训练性和可扩展性仍然受到理论和实际障碍。受最近的几何类 classical 和量子机器学习发展启示，我们设计了量子神经网络，其直接包含大量物理相关的 симметрии作为数据驱动的假设。我们发现，我们的抗变量量子学习模型在增加复杂度的分子上表现出色，超过其更通用的对手。此外，我们研究了水二分子作为化学系统中的最小示例，展示了我们的方法的多样性和可扩展性。我们的结果表明，通过利用几何量子机器学习框架，分子力场生成可以取得显著的改进，化学系统 представ a rich and interesting 的机器学习工具开发和应用的场景。
</details></li>
</ul>
<hr>
<h2 id="Coverage-Validity-Aware-Algorithmic-Recourse"><a href="#Coverage-Validity-Aware-Algorithmic-Recourse" class="headerlink" title="Coverage-Validity-Aware Algorithmic Recourse"></a>Coverage-Validity-Aware Algorithmic Recourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11349">http://arxiv.org/abs/2311.11349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngoc Bui, Duy Nguyen, Man-Chung Yue, Viet Anh Nguyen</li>
<li>for: 提高机器学习模型的可解释性、透明度和伦理性</li>
<li>methods: 使用一种新的框架生成模型兼容的再利用方法，以确保对未来模型的适用性</li>
<li>results: 提出一种可以适应模型变化的再利用方法，并证明该方法可以恢复多种常见的正则化方法，包括L2正则化和类别权重正则化，同时可以生成INTUMPTIVE的再利用方法。<details>
<summary>Abstract</summary>
Algorithmic recourse emerges as a prominent technique to promote the explainability, transparency and hence ethics of machine learning models. Existing algorithmic recourse approaches often assume an invariant predictive model; however, the predictive model is usually updated upon the arrival of new data. Thus, a recourse that is valid respective to the present model may become invalid for the future model. To resolve this issue, we propose a novel framework to generate a model-agnostic recourse that exhibits robustness to model shifts. Our framework first builds a coverage-validity-aware linear surrogate of the nonlinear (black-box) model; then, the recourse is generated with respect to the linear surrogate. We establish a theoretical connection between our coverage-validity-aware linear surrogate and the minimax probability machines (MPM). We then prove that by prescribing different covariance robustness, the proposed framework recovers popular regularizations for MPM, including the $\ell_2$-regularization and class-reweighting. Furthermore, we show that our surrogate pushes the approximate hyperplane intuitively, facilitating not only robust but also interpretable recourses. The numerical results demonstrate the usefulness and robustness of our framework.
</details>
<details>
<summary>摘要</summary>
仙术式回应技术在机器学习模型中崛起，以提高模型的解释性、透明度和伦理。现有的算法回应方法通常假设不变的预测模型，但实际上预测模型通常会随着新数据的到来而更新。因此，一个有效的回应可能会在未来的模型上无效。为解决这个问题，我们提出了一种新的框架，用于生成一个模型无关的回应，具有对模型变化的抗衰假设。我们首先构建一个具有覆盖度和有效性意识的非线性黑盒模型的线性抽象；然后，我们使用这个抽象来生成回应。我们证明了我们的抽象与最小最大概率机器（MPM）之间存在理论上的连接。我们还证明了，通过不同的covarianceRobustness，我们的框架可以恢复多种MPM的常见正则化，包括L2正则化和类别重量。此外，我们发现我们的抽象可以让近似的 гиперплоскоpush，使得回应不仅是可靠的，还是可解释的。数据结果表明我们的框架是有用和稳定的。
</details></li>
</ul>
<hr>
<h2 id="A-Generative-Model-for-Accelerated-Inverse-Modelling-Using-a-Novel-Embedding-for-Continuous-Variables"><a href="#A-Generative-Model-for-Accelerated-Inverse-Modelling-Using-a-Novel-Embedding-for-Continuous-Variables" class="headerlink" title="A Generative Model for Accelerated Inverse Modelling Using a Novel Embedding for Continuous Variables"></a>A Generative Model for Accelerated Inverse Modelling Using a Novel Embedding for Continuous Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11343">http://arxiv.org/abs/2311.11343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sébastien Bompas abd Stefan Sandfeld</li>
<li>for: 加速材料设计</li>
<li>methods: 使用生成机器学习模型，并 comparing a novel embedding strategy for generative models based on binary representation of floating point numbers</li>
<li>results: 提供了一种 versatile embedding space for conditioning the generative model，可以提供精细的控制 над生成的结构图像，并促进材料设计的加速。Here’s the Chinese text in the format you requested:</li>
<li>for: 加速材料设计</li>
<li>methods: 使用生成机器学习模型，并 comparing a novel embedding strategy for generative models based on binary representation of floating point numbers</li>
<li>results: 提供了一个 versatile embedding space for conditioning the generative model，可以提供精致的控制 над生成的结构图像，并促进材料设计的加速。<details>
<summary>Abstract</summary>
In materials science, the challenge of rapid prototyping materials with desired properties often involves extensive experimentation to find suitable microstructures. Additionally, finding microstructures for given properties is typically an ill-posed problem where multiple solutions may exist. Using generative machine learning models can be a viable solution which also reduces the computational cost. This comes with new challenges because, e.g., a continuous property variable as conditioning input to the model is required. We investigate the shortcomings of an existing method and compare this to a novel embedding strategy for generative models that is based on the binary representation of floating point numbers. This eliminates the need for normalization, preserves information, and creates a versatile embedding space for conditioning the generative model. This technique can be applied to condition a network on any number, to provide fine control over generated microstructure images, thereby contributing to accelerated materials design.
</details>
<details>
<summary>摘要</summary>
在材料科学中，快速原型材料的性能通常需要大量实验来找到适合的微结构。此外，找到给定性能的微结构通常是一个不充分定义的问题，可能存在多个解。使用生成机器学习模型可以是一个可行的解决方案，同时也可以降低计算成本。但是，这也带来了新的挑战，例如需要输入条件模型的连续属性变量。我们研究现有方法的缺陷并与一种基于二进制浮点数表示的新嵌入策略进行比较。这种策略可以用来决定网络的任意数字，以提供精细的控制权 над生成的微结构图像，从而促进材料设计的加速。
</details></li>
</ul>
<hr>
<h2 id="On-the-Communication-Complexity-of-Decentralized-Bilevel-Optimization"><a href="#On-the-Communication-Complexity-of-Decentralized-Bilevel-Optimization" class="headerlink" title="On the Communication Complexity of Decentralized Bilevel Optimization"></a>On the Communication Complexity of Decentralized Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11342">http://arxiv.org/abs/2311.11342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Zhang, My T. Thai, Jie Wu, Hongchang Gao</li>
<li>for: 提高实际任务中 Decentralized bilevel 优化的应用性</li>
<li>methods: 提出了一种基于 Stochastic bilevel gradient descent 算法的各自学习方法，具有小于一轮的通信成本和小于一轮的通信轮数</li>
<li>results: 实验结果证明了该算法的有效性，并在具有多级结构的各自学习中减少了通信复杂性<details>
<summary>Abstract</summary>
Decentralized bilevel optimization has been actively studied in the past few years since it has widespread applications in machine learning. However, existing algorithms suffer from large communication complexity caused by the estimation of stochastic hypergradient, limiting their application to real-world tasks. To address this issue, we develop a novel decentralized stochastic bilevel gradient descent algorithm under the heterogeneous setting, which enjoys a small communication cost in each round and small communication rounds. As such, it can achieve a much better communication complexity than existing algorithms. Moreover, we extend our algorithm to the more challenging decentralized multi-level optimization. To the best of our knowledge, this is the first time achieving these theoretical results under the heterogeneous setting. At last, the experimental results confirm the efficacy of our algorithm.
</details>
<details>
<summary>摘要</summary>
“半централизовAN optimize的研究在最近几年来得到了广泛的应用，特别是在机器学习领域。然而，现有的算法受到估计随机超gradient的困扰，导致它们在实际任务中应用有限。为解决这个问题，我们开发了一种新的分布式随机二级梯度下降算法，在多样化设定下实现了每轮的小communication cost和小数量的通信轮次。因此，它在communication复杂度方面比现有算法更好。此外，我们还扩展了我们的算法到更加复杂的分布式多级优化问题。到目前为止，这是在多样化设定下首次实现的理论结果。最后，实验结果证明了我们的算法的有效性。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Self-Distilled-Representation-Learning-for-Time-Series"><a href="#Self-Distilled-Representation-Learning-for-Time-Series" class="headerlink" title="Self-Distilled Representation Learning for Time Series"></a>Self-Distilled Representation Learning for Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11335">http://arxiv.org/abs/2311.11335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Pieper, Konstantin Ditschuneit, Martin Genzel, Alexandra Lindt, Johannes Otterbach</li>
<li>for: 本研究旨在探讨自监督学习在时间序列数据上的潜力，以及一种非对照学习方法，基于数据2vec自适应混合学习框架。</li>
<li>methods: 我们提出了一种基于学生-教师模式的非对照学习方法，其中predicts the latent representation of an input time series from masked views of the same time series。</li>
<li>results: 我们通过对UCRC、UEA、ETT和电力 datasets进行比较，证明了我们的方法在预测和分类任务中的竞争力。<details>
<summary>Abstract</summary>
Self-supervised learning for time-series data holds potential similar to that recently unleashed in Natural Language Processing and Computer Vision. While most existing works in this area focus on contrastive learning, we propose a conceptually simple yet powerful non-contrastive approach, based on the data2vec self-distillation framework. The core of our method is a student-teacher scheme that predicts the latent representation of an input time series from masked views of the same time series. This strategy avoids strong modality-specific assumptions and biases typically introduced by the design of contrastive sample pairs. We demonstrate the competitiveness of our approach for classification and forecasting as downstream tasks, comparing with state-of-the-art self-supervised learning methods on the UCR and UEA archives as well as the ETT and Electricity datasets.
</details>
<details>
<summary>摘要</summary>
自领导学习 для时间序列数据具有与自然语言处理和计算机视觉领域最近获得的潜力相似。大多数现有的方法在这一领域都是基于对比学习的，我们则提议一种基于数据2vec自领导框架的非对比方法。我们的方法的核心是一种学生教师的方式，Predicting the latent representation of an input time series from masked views of the same time series。这种策略避免了强制性的modal-specific assumption和偏见，通常由对比样本对组成的设计引入。我们通过对UCRC和UEA数据库以及ETT和电力集成来证明我们的方法在分类和预测任务中的竞争力。
</details></li>
</ul>
<hr>
<h2 id="LABCAT-Locally-adaptive-Bayesian-optimization-using-principal-component-aligned-trust-regions"><a href="#LABCAT-Locally-adaptive-Bayesian-optimization-using-principal-component-aligned-trust-regions" class="headerlink" title="LABCAT: Locally adaptive Bayesian optimization using principal component-aligned trust regions"></a>LABCAT: Locally adaptive Bayesian optimization using principal component-aligned trust regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11328">http://arxiv.org/abs/2311.11328</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aemiliusretiarius/labcat">https://github.com/aemiliusretiarius/labcat</a></li>
<li>paper_authors: E. Visser, C. E. van Daalen, J. C. Schoeman</li>
<li>for: 优化贵重黑盒函数（Black-box function）的开销成本高的问题。</li>
<li>methods: 提议使用本地策略（such as trust regions）和主成分 aligned rotation 以及自适应尺度调整策略，以解决 BO 中的限制。</li>
<li>results: 经过大量的数字实验，表明 LABCAT 算法可以比 state-of-the-art BO 和其他黑盒优化算法表现更好。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) is a popular method for optimizing expensive black-box functions. BO has several well-documented shortcomings, including computational slowdown with longer optimization runs, poor suitability for non-stationary or ill-conditioned objective functions, and poor convergence characteristics. Several algorithms have been proposed that incorporate local strategies, such as trust regions, into BO to mitigate these limitations; however, none address all of them satisfactorily. To address these shortcomings, we propose the LABCAT algorithm, which extends trust-region-based BO by adding principal-component-aligned rotation and an adaptive rescaling strategy based on the length-scales of a local Gaussian process surrogate model with automatic relevance determination. Through extensive numerical experiments using a set of synthetic test functions and the well-known COCO benchmarking software, we show that the LABCAT algorithm outperforms several state-of-the-art BO and other black-box optimization algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Learning-Rates-Improve-Generalization-But-How-Large-Are-We-Talking-About"><a href="#Large-Learning-Rates-Improve-Generalization-But-How-Large-Are-We-Talking-About" class="headerlink" title="Large Learning Rates Improve Generalization: But How Large Are We Talking About?"></a>Large Learning Rates Improve Generalization: But How Large Are We Talking About?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11303">http://arxiv.org/abs/2311.11303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekaterina Lobacheva, Eduard Pockonechnyy, Maxim Kodryan, Dmitry Vetrov</li>
<li>for: 这 paper 探讨了在开始神经网络训练时使用大学习率（LR）以实现最佳一致性的假设。</li>
<li>methods: 该研究具体探讨了这个假设，并确定了最佳初始 LR 范围。我们在简化的设置下进行主要实验，并在更实际的设置中验证了我们的关键发现。</li>
<li>results: 我们发现，最佳初始 LR 范围实际上远 narrower  than 普遍认为的。<details>
<summary>Abstract</summary>
Inspired by recent research that recommends starting neural networks training with large learning rates (LRs) to achieve the best generalization, we explore this hypothesis in detail. Our study clarifies the initial LR ranges that provide optimal results for subsequent training with a small LR or weight averaging. We find that these ranges are in fact significantly narrower than generally assumed. We conduct our main experiments in a simplified setup that allows precise control of the learning rate hyperparameter and validate our key findings in a more practical setting.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:根据最近的研究，启发了使用大学习率（LR）进行神经网络训练以实现最佳通用化，我们在这个假设上进行了详细的探索。我们的研究发现，在后续训练中使用小学习率或权重平均时，最佳结果的初始LR范围实际上是比之前想象的更加窄的。我们在一个简化的设置下进行了主要的实验，该设置允许精准控制学习率超参数，并在更实际的设置下验证了我们的关键发现。
</details></li>
</ul>
<hr>
<h2 id="From-Categories-to-Classifier-Name-Only-Continual-Learning-by-Exploring-the-Web"><a href="#From-Categories-to-Classifier-Name-Only-Continual-Learning-by-Exploring-the-Web" class="headerlink" title="From Categories to Classifier: Name-Only Continual Learning by Exploring the Web"></a>From Categories to Classifier: Name-Only Continual Learning by Exploring the Web</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11293">http://arxiv.org/abs/2311.11293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ameya Prabhu, Hasan Abed Al Kader Hammoud, Ser-Nam Lim, Bernard Ghanem, Philip H. S. Torr, Adel Bibi</li>
<li>for: 解决手动标注数据的限制，提高 continual learning 的可行性和效率。</li>
<li>methods: 利用互联网查询和下载无监督的网络数据，并利用这些数据进行分类。</li>
<li>results: 比对手动标注数据和网络数据的可靠性，发现两者相对比较，并且可以通过网络数据创建比STATE-OF-THE-ART名称只分类支持集，提高分类精度。在不同的 continual learning 上下文中应用时，方法具有较小的性能差异。还提出了 EvoTrends，一个基于网络的类增量数据集，用于捕捉实际世界的趋势，只需几分钟创建。总之，这篇论文表明了使用无监督网络数据来缓解手动数据标注的挑战，可以提高 continual learning 的可行性和效率。<details>
<summary>Abstract</summary>
Continual Learning (CL) often relies on the availability of extensive annotated datasets, an assumption that is unrealistically time-consuming and costly in practice. We explore a novel paradigm termed name-only continual learning where time and cost constraints prohibit manual annotation. In this scenario, learners adapt to new category shifts using only category names without the luxury of annotated training data. Our proposed solution leverages the expansive and ever-evolving internet to query and download uncurated webly-supervised data for image classification. We investigate the reliability of our web data and find them comparable, and in some cases superior, to manually annotated datasets. Additionally, we show that by harnessing the web, we can create support sets that surpass state-of-the-art name-only classification that create support sets using generative models or image retrieval from LAION-5B, achieving up to 25% boost in accuracy. When applied across varied continual learning contexts, our method consistently exhibits a small performance gap in comparison to models trained on manually annotated datasets. We present EvoTrends, a class-incremental dataset made from the web to capture real-world trends, created in just minutes. Overall, this paper underscores the potential of using uncurated webly-supervised data to mitigate the challenges associated with manual data labeling in continual learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TimeSQL-Improving-Multivariate-Time-Series-Forecasting-with-Multi-Scale-Patching-and-Smooth-Quadratic-Loss"><a href="#TimeSQL-Improving-Multivariate-Time-Series-Forecasting-with-Multi-Scale-Patching-and-Smooth-Quadratic-Loss" class="headerlink" title="TimeSQL: Improving Multivariate Time Series Forecasting with Multi-Scale Patching and Smooth Quadratic Loss"></a>TimeSQL: Improving Multivariate Time Series Forecasting with Multi-Scale Patching and Smooth Quadratic Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11285">http://arxiv.org/abs/2311.11285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Site Mo, Haoxin Wang, Bixiong Li, Songhai Fan, Yuankai Wu, Xianggen Liu</li>
<li>for: 这篇论文的目的是提出一个简单有效的框架，用于多重时间序列预测（multivariate time series forecasting）。</li>
<li>methods: 这个框架使用了多尺度贴合和流畅quadratic loss（SQL）来解决实际时间序列拥有噪音和复杂的本地和全球时间对称性，使得预测未来时间序列很困难。</li>
<li>results: 根据论文的 teorical 分析和实验结果，这个框架可以在八个真实世界 benchmark 数据集上实现新的顶尖性能。<details>
<summary>Abstract</summary>
Time series is a special type of sequence data, a sequence of real-valued random variables collected at even intervals of time. The real-world multivariate time series comes with noises and contains complicated local and global temporal dynamics, making it difficult to forecast the future time series given the historical observations. This work proposes a simple and effective framework, coined as TimeSQL, which leverages multi-scale patching and smooth quadratic loss (SQL) to tackle the above challenges. The multi-scale patching transforms the time series into two-dimensional patches with different length scales, facilitating the perception of both locality and long-term correlations in time series. SQL is derived from the rational quadratic kernel and can dynamically adjust the gradients to avoid overfitting to the noises and outliers. Theoretical analysis demonstrates that, under mild conditions, the effect of the noises on the model with SQL is always smaller than that with MSE. Based on the two modules, TimeSQL achieves new state-of-the-art performance on the eight real-world benchmark datasets. Further ablation studies indicate that the key modules in TimeSQL could also enhance the results of other models for multivariate time series forecasting, standing as plug-and-play techniques.
</details>
<details>
<summary>摘要</summary>
时间序列是一种特殊的序列数据，一个时间间隔为整数的序列实数随机变量的序列。实际世界多变量时间序列具有噪声和复杂的地方和全局时间动态，使其难以根据历史观测预测未来时间序列。这项工作提出了一个简单有效的框架，命名为TimeSQL，该框架利用多尺度补丁和平滑quadratic loss（SQL）来解决以上挑战。多尺度补丁将时间序列转换为二维补丁，并且具有不同的尺度，以便更好地捕捉时间序列的本地和长期相关性。SQL是基于 rational quadratic kernel 的，可以动态调整Gradient，以避免因噪声和异常值而过拟合。理论分析表明，在某些条件下，TimeSQL模型中的噪声对于MSE模型来说总是小于。基于两个模块，TimeSQL在八个实际世界 referential 数据集上达到了新的状态码性能。进一步的ablation 研究表明，TimeSQL 中的关键模块可以增强其他模型的多变量时间序列预测结果，作为插件技术。
</details></li>
</ul>
<hr>
<h2 id="Multi-Timescale-Control-and-Communications-with-Deep-Reinforcement-Learning-–-Part-I-Communication-Aware-Vehicle-Control"><a href="#Multi-Timescale-Control-and-Communications-with-Deep-Reinforcement-Learning-–-Part-I-Communication-Aware-Vehicle-Control" class="headerlink" title="Multi-Timescale Control and Communications with Deep Reinforcement Learning – Part I: Communication-Aware Vehicle Control"></a>Multi-Timescale Control and Communications with Deep Reinforcement Learning – Part I: Communication-Aware Vehicle Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11281">http://arxiv.org/abs/2311.11281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Liu, Lei Lei, Kan Zheng, Xuemin, Shen</li>
<li>for: 本研究旨在开发一种基于车辆到所有东西（V2X）通信的智能决策系统，以实现自动驾驶（AD）的安全和效率。</li>
<li>methods: 本研究提出了一种多时间尺度控制和通信（MTCC）框架，基于深度学习算法（DRL）。</li>
<li>results: 研究人员在实验中比较了MTCC-PC算法与基eline DRL算法的性能，并证明了MTCC-PC算法可以在带有观测延迟的情况下提高PC性能。<details>
<summary>Abstract</summary>
An intelligent decision-making system enabled by Vehicle-to-Everything (V2X) communications is essential to achieve safe and efficient autonomous driving (AD), where two types of decisions have to be made at different timescales, i.e., vehicle control and radio resource allocation (RRA) decisions. The interplay between RRA and vehicle control necessitates their collaborative design. In this two-part paper (Part I and Part II), taking platoon control (PC) as an example use case, we propose a joint optimization framework of multi-timescale control and communications (MTCC) based on Deep Reinforcement Learning (DRL). In this paper (Part I), we first decompose the problem into a communication-aware DRL-based PC sub-problem and a control-aware DRL-based RRA sub-problem. Then, we focus on the PC sub-problem assuming an RRA policy is given, and propose the MTCC-PC algorithm to learn an efficient PC policy. To improve the PC performance under random observation delay, the PC state space is augmented with the observation delay and PC action history. Moreover, the reward function with respect to the augmented state is defined to construct an augmented state Markov Decision Process (MDP). It is proved that the optimal policy for the augmented state MDP is optimal for the original PC problem with observation delay. Different from most existing works on communication-aware control, the MTCC-PC algorithm is trained in a delayed environment generated by the fine-grained embedded simulation of C-V2X communications rather than by a simple stochastic delay model. Finally, experiments are performed to compare the performance of MTCC-PC with those of the baseline DRL algorithms.
</details>
<details>
<summary>摘要</summary>
“一个智能做出决策系统，启用车辆与所有东西（V2X）通信，是为自动驾驶（AD） achieve safe和效率的运转，需要在不同时间尺度上做出两种决策，即车辆控制和对话资源分配（RRA）决策。这两种决策之间的互动，需要它们的共同设计。在这两部分文章（Part I和Part II）中，以单位排队控制（PC）为使用案例，我们提出了一个多个时间尺度的控制和通信（MTCC）的联合优化框架，基于深度循环学习（DRL）。在这篇文章（Part I）中，我们首先将问题分解为一个具有通信意识的DRL基于PC子问题，以及一个具有控制意识的DRL基于RRA子问题。然后，我们专注于PC子问题，假设RRA策略已经给出，并提出了MTCC-PC算法，以学习一个高效的PC策略。为了改善PC性能在随机观察延迟下，PC状态空间被扩展了，加入观察延迟和PC动作历史。此外，对于扩展的状态，定义了一个对应的资源分配奖励函数，以建立一个扩展状态Markov决策过程（MDP）。经过证明，最佳策略 для扩展状态MDP是最佳策略 для原始PC问题中的观察延迟。与大多数现有的通信意识控制方法不同，MTCC-PC算法在精细的C-V2X通信嵌入式 simulator上进行延迟生成的延迟环境中训练，而不是使用简单的概率延迟模型。 finally，我们对MTCC-PC算法与基于DRL的基eline算法进行比较实验。”
</details></li>
</ul>
<hr>
<h2 id="Multi-Timescale-Control-and-Communications-with-Deep-Reinforcement-Learning-–-Part-II-Control-Aware-Radio-Resource-Allocation"><a href="#Multi-Timescale-Control-and-Communications-with-Deep-Reinforcement-Learning-–-Part-II-Control-Aware-Radio-Resource-Allocation" class="headerlink" title="Multi-Timescale Control and Communications with Deep Reinforcement Learning – Part II: Control-Aware Radio Resource Allocation"></a>Multi-Timescale Control and Communications with Deep Reinforcement Learning – Part II: Control-Aware Radio Resource Allocation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11280">http://arxiv.org/abs/2311.11280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Lei, Tong Liu, Kan Zheng, Xuemin, Shen</li>
<li>for: 这个论文是为了解决 Cellular Vehicle-to-Everything (C-V2X) 系统中的多时频控制和通信问题（Multi-Timescale Control and Communications，MTCC）。</li>
<li>methods: 这篇论文使用 Deep Reinforcement Learning (DRL) 算法来解决 MTCC 问题，并将其分解成两个互相关联的互动问题：PC 问题和 RRA 问题。PC 问题是用 DRL 算法学习一个最佳的排队控制策略，而 RRA 问题是用 DRL 算法学习一个最佳的广播资源分配策略。</li>
<li>results: 在实验中，使用实际驾驶数据对主排队车进行训练，并与基eline DRL 算法进行比较，得到的结果表明，提出的 MTCC 算法能够更好地解决 MTCC 问题。<details>
<summary>Abstract</summary>
In Part I of this two-part paper (Multi-Timescale Control and Communications with Deep Reinforcement Learning -- Part I: Communication-Aware Vehicle Control), we decomposed the multi-timescale control and communications (MTCC) problem in Cellular Vehicle-to-Everything (C-V2X) system into a communication-aware Deep Reinforcement Learning (DRL)-based platoon control (PC) sub-problem and a control-aware DRL-based radio resource allocation (RRA) sub-problem. We focused on the PC sub-problem and proposed the MTCC-PC algorithm to learn an optimal PC policy given an RRA policy. In this paper (Part II), we first focus on the RRA sub-problem in MTCC assuming a PC policy is given, and propose the MTCC-RRA algorithm to learn the RRA policy. Specifically, we incorporate the PC advantage function in the RRA reward function, which quantifies the amount of PC performance degradation caused by observation delay. Moreover, we augment the state space of RRA with PC action history for a more well-informed RRA policy. In addition, we utilize reward shaping and reward backpropagation prioritized experience replay (RBPER) techniques to efficiently tackle the multi-agent and sparse reward problems, respectively. Finally, a sample- and computational-efficient training approach is proposed to jointly learn the PC and RRA policies in an iterative process. In order to verify the effectiveness of the proposed MTCC algorithm, we performed experiments using real driving data for the leading vehicle, where the performance of MTCC is compared with those of the baseline DRL algorithms.
</details>
<details>
<summary>摘要</summary>
在这篇两部分文章中（多 timescale控制和通信 WITH deep reinforcement learning -- Part I: 交通aware Vehicle Control），我们将多 timescale控制和通信（MTCC）问题分解为一个交通aware Deep Reinforcement Learning（DRL）基于的坏车队控制（PC）子问题和一个控制 aware DRL 基于的广播资源分配（RRA）子问题。我们关注 PC 子问题，并提出了 MTCC-PC 算法，以学习一个最优的 PC 策略。在这篇文章中（Part II），我们首先关注 RRA 子问题，假设 PC 策略已知，并提出了 MTCC-RRA 算法，以学习 RRA 策略。具体来说，我们将 PC 优势函数 incorporated 到 RRA 奖励函数中，以量化 PC 性能下降的观测延迟量。此外，我们将 RRA 状态空间扩展为 PC 动作历史，以更好地 Inform RRA 策略。此外，我们使用 reward shaping 和 reward backpropagation prioritized experience replay（RBPER）技术，以有效地解决多代理和罕见奖励问题。最后，我们提出了一种 sample- 和计算效率高的训练方法，以同时学习 PC 和 RRA 策略。为验证我们提出的 MTCC 算法的有效性，我们使用实际驾驶数据进行了实验，并比较了 MTCC 的性能与基eline DRL 算法。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-quantification-for-noisy-inputs-outputs-in-physics-informed-neural-networks-and-neural-operators"><a href="#Uncertainty-quantification-for-noisy-inputs-outputs-in-physics-informed-neural-networks-and-neural-operators" class="headerlink" title="Uncertainty quantification for noisy inputs-outputs in physics-informed neural networks and neural operators"></a>Uncertainty quantification for noisy inputs-outputs in physics-informed neural networks and neural operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11262">http://arxiv.org/abs/2311.11262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongren Zou, Xuhui Meng, George Em Karniadakis</li>
<li>for:  This paper is written for addressing the uncertainty quantification (UQ) in scientific machine learning (SciML) models, specifically for noisy inputs in physics-informed neural networks (PINNs) and neural operators (NOs).</li>
<li>methods:  The paper proposes a Bayesian approach to quantify uncertainty arising from noisy inputs-outputs in PINNs and NOs. This approach seamlessly integrates into PINNs and NOs, allowing them to address problems where the observed coordinate or input functions are subject to noise.</li>
<li>results:  The proposed approach enables PINNs and NOs to handle noisy measurements for both input and output functions, providing reliable and trustworthy deployment of these models in applications involving physical knowledge.<details>
<summary>Abstract</summary>
Uncertainty quantification (UQ) in scientific machine learning (SciML) becomes increasingly critical as neural networks (NNs) are being widely adopted in addressing complex problems across various scientific disciplines. Representative SciML models are physics-informed neural networks (PINNs) and neural operators (NOs). While UQ in SciML has been increasingly investigated in recent years, very few works have focused on addressing the uncertainty caused by the noisy inputs, such as spatial-temporal coordinates in PINNs and input functions in NOs. The presence of noise in the inputs of the models can pose significantly more challenges compared to noise in the outputs of the models, primarily due to the inherent nonlinearity of most SciML algorithms. As a result, UQ for noisy inputs becomes a crucial factor for reliable and trustworthy deployment of these models in applications involving physical knowledge. To this end, we introduce a Bayesian approach to quantify uncertainty arising from noisy inputs-outputs in PINNs and NOs. We show that this approach can be seamlessly integrated into PINNs and NOs, when they are employed to encode the physical information. PINNs incorporate physics by including physics-informed terms via automatic differentiation, either in the loss function or the likelihood, and often take as input the spatial-temporal coordinate. Therefore, the present method equips PINNs with the capability to address problems where the observed coordinate is subject to noise. On the other hand, pretrained NOs are also commonly employed as equation-free surrogates in solving differential equations and Bayesian inverse problems, in which they take functions as inputs. The proposed approach enables them to handle noisy measurements for both input and output functions with UQ.
</details>
<details>
<summary>摘要</summary>
科学机器学习（SciML）中的不确定性评估（UQ）在 neural networks（NNs）广泛应用的场景中变得越来越重要。代表性的 SciML 模型包括物理相关的 neural networks（PINNs）和 neural operators（NOs）。尽管 UQ 在 SciML 已经得到了一些研究，但很少有关注输入噪声的不确定性，如 PINNs 中的空间-时间坐标和 NOs 中的输入函数。噪声在模型输入中可能会比模型输出中的噪声更加复杂，主要是因为大多数 SciML 算法具有非线性性。因此，对输入噪声的不确定性评估变得非常重要，以确保模型在应用中的可靠和信任性。为此，我们介绍了一种 bayesian 方法，用于评估 PINNs 和 NOs 中噪声输入-输出的不确定性。我们示示这种方法可以顺利地与 PINNs 和 NOs 结合使用，当它们用于编码物理信息时。PINNs 通常通过自动微分来包含物理条件，并将空间-时间坐标作为输入。因此，我们的方法可以帮助 PINNs 解决受到噪声影响的问题。而 pretrained NOs 通常也被用作 equation-free 代理，用于解决微分方程和 bayesian 反问题，它们的输入是函数。我们的方法可以帮助它们处理噪声测量数据。
</details></li>
</ul>
<hr>
<h2 id="BOIS-Bayesian-Optimization-of-Interconnected-Systems"><a href="#BOIS-Bayesian-Optimization-of-Interconnected-Systems" class="headerlink" title="BOIS: Bayesian Optimization of Interconnected Systems"></a>BOIS: Bayesian Optimization of Interconnected Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11254">http://arxiv.org/abs/2311.11254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo D. González, Victor M. Zavala</li>
<li>for: 这个论文是用来优化昂贵的样本系统的一种有效方法。</li>
<li>methods: 这篇论文使用 Bayesian 优化（BO）方法，并利用 Gaussian 过程（GP）来表征模型不确定性，从而导引学习和搜索过程。</li>
<li>results: 该论文介绍了一种新的 BO 方法，即 BOIS，可以有效地使用 composite functions，并使用 adaptive linearizations 来获得关于 composite function 的闭式表达。该方法在一个化学过程优化案例中被评估，并与标准 BO 和抽样方法进行比较。结果表明，BOIS 可以获得性能提升和正确地捕捉 composite functions 的统计特性。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) has proven to be an effective paradigm for the global optimization of expensive-to-sample systems. One of the main advantages of BO is its use of Gaussian processes (GPs) to characterize model uncertainty which can be leveraged to guide the learning and search process. However, BO typically treats systems as black-boxes and this limits the ability to exploit structural knowledge (e.g., physics and sparse interconnections). Composite functions of the form $f(x, y(x))$, wherein GP modeling is shifted from the performance function $f$ to an intermediate function $y$, offer an avenue for exploiting structural knowledge. However, the use of composite functions in a BO framework is complicated by the need to generate a probability density for $f$ from the Gaussian density of $y$ calculated by the GP (e.g., when $f$ is nonlinear it is not possible to obtain a closed-form expression). Previous work has handled this issue using sampling techniques; these are easy to implement and flexible but are computationally intensive. In this work, we introduce a new paradigm which allows for the efficient use of composite functions in BO; this uses adaptive linearizations of $f$ to obtain closed-form expressions for the statistical moments of the composite function. We show that this simple approach (which we call BOIS) enables the exploitation of structural knowledge, such as that arising in interconnected systems as well as systems that embed multiple GP models and combinations of physics and GP models. Using a chemical process optimization case study, we benchmark the effectiveness of BOIS against standard BO and sampling approaches. Our results indicate that BOIS achieves performance gains and accurately captures the statistics of composite functions.
</details>
<details>
<summary>摘要</summary>
bayesian 优化（BO）已经证明是全球优化昂贵样本系统的有效方法。BO的一个主要优点是使用 Gaussian processes（GP）来Characterize model uncertainty，这可以用来导引学习和搜索过程。然而，BO通常对系统进行黑盒子封装，这限制了利用结构知识（例如物理和稀疏连接）。使用 $f(x, y(x)) $ 的 composite function，其中 GP 模型Shifted from performance function $f$ to intermediate function $y$，提供了利用结构知识的途径。然而，在 BO 框架中使用 composite function 增加了 Generating a probability density for $f$ from the Gaussian density of $y$ calculated by the GP 的需求（例如当 $f$ 非线性时，不能获得关闭式表达）。过去的工作通过 sampling 技术来解决这个问题，这些技术容易实现但计算昂贵。在这种工作中，我们引入了一种新的方法，即使用 adaptive linearization of $f$ 来获得关闭式表达 composite function 的统计 moments。我们显示这种简单的方法（我们称之为 BOIS）可以有效地利用结构知识，例如在交互连接系统中和GP模型和物理模型的组合中。使用化学过程优化案例研究，我们对 BOIS 的效果进行了比较，与标准 BO 和 sampling 方法进行了比较。我们的结果表明，BOIS 可以 achieve performance gains 和正确地捕捉 composite function 的统计特性。
</details></li>
</ul>
<hr>
<h2 id="A-Universal-Framework-for-Accurate-and-Efficient-Geometric-Deep-Learning-of-Molecular-Systems"><a href="#A-Universal-Framework-for-Accurate-and-Efficient-Geometric-Deep-Learning-of-Molecular-Systems" class="headerlink" title="A Universal Framework for Accurate and Efficient Geometric Deep Learning of Molecular Systems"></a>A Universal Framework for Accurate and Efficient Geometric Deep Learning of Molecular Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11228">http://arxiv.org/abs/2311.11228</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XieResearchGroup/Physics-aware-Multiplex-GNN">https://github.com/XieResearchGroup/Physics-aware-Multiplex-GNN</a></li>
<li>paper_authors: Shuo Zhang, Yang Liu, Lei Xie</li>
<li>for: 用于三维分子的准确和有效学习表示</li>
<li>methods: 基于分子动力学的启发式模型本地和非本地交互，并对其共同效应进行模型</li>
<li>results: 在多种分子科学应用中表现出色，比如小分子性质、RNA三维结构和蛋白质-抑药结合稳定性，并且在精算和内存方面具有高效性。<details>
<summary>Abstract</summary>
Molecular sciences address a wide range of problems involving molecules of different types and sizes and their complexes. Recently, geometric deep learning, especially Graph Neural Networks, has shown promising performance in molecular science applications. However, most existing works often impose targeted inductive biases to a specific molecular system, and are inefficient when applied to macromolecules or large-scale tasks, thereby limiting their applications to many real-world problems. To address these challenges, we present PAMNet, a universal framework for accurately and efficiently learning the representations of three-dimensional (3D) molecules of varying sizes and types in any molecular system. Inspired by molecular mechanics, PAMNet induces a physics-informed bias to explicitly model local and non-local interactions and their combined effects. As a result, PAMNet can reduce expensive operations, making it time and memory efficient. In extensive benchmark studies, PAMNet outperforms state-of-the-art baselines regarding both accuracy and efficiency in three diverse learning tasks: small molecule properties, RNA 3D structures, and protein-ligand binding affinities. Our results highlight the potential for PAMNet in a broad range of molecular science applications.
</details>
<details>
<summary>摘要</summary>
分子科学研究各种不同类型和大小的分子和其复杂系统。最近，深度学习，特别是图神经网络，在分子科学应用中表现出了有前途的性能。然而，大多数现有的工作通常会针对特定分子系统强制目标印象，对于大分子或大规模任务来说效率低下，因此对实际世界问题有限制。为解决这些挑战，我们提出了PAMNet，一个通用的框架，可以准确地和高效地学习三维分子的表示，无论它们是什么类型和大小。以分子动力学为灵感，PAMNet引入物理学引用的偏见，以模型本地和非本地交互和它们的共同效果。因此，PAMNet可以减少昂贵的操作，从而节省时间和存储空间。在广泛的benchmark测试中，PAMNet在三种多样化的学习任务中（小分子性质、RNA三维结构和蛋白质-抑药绑定亲和力）表现出了与状态所有基准点的超过。我们的结果表明PAMNet在分子科学应用中具有广泛的潜力。
</details></li>
</ul>
<hr>
<h2 id="TextGuard-Provable-Defense-against-Backdoor-Attacks-on-Text-Classification"><a href="#TextGuard-Provable-Defense-against-Backdoor-Attacks-on-Text-Classification" class="headerlink" title="TextGuard: Provable Defense against Backdoor Attacks on Text Classification"></a>TextGuard: Provable Defense against Backdoor Attacks on Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11225">http://arxiv.org/abs/2311.11225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-secure/textguard">https://github.com/ai-secure/textguard</a></li>
<li>paper_authors: Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, Dawn Song</li>
<li>for: 防止机器学习模型中的后门攻击</li>
<li>methods: 分割训练数据，并从每个子训练集中训练基础分类器， ensemble 提供最终预测</li>
<li>results: 在三个文本分类任务上达到了证书精度，超过了现有的证书防御措施；并提供了进一步优化策略来提高 TextGuard 的实际性能。<details>
<summary>Abstract</summary>
Backdoor attacks have become a major security threat for deploying machine learning models in security-critical applications. Existing research endeavors have proposed many defenses against backdoor attacks. Despite demonstrating certain empirical defense efficacy, none of these techniques could provide a formal and provable security guarantee against arbitrary attacks. As a result, they can be easily broken by strong adaptive attacks, as shown in our evaluation. In this work, we propose TextGuard, the first provable defense against backdoor attacks on text classification. In particular, TextGuard first divides the (backdoored) training data into sub-training sets, achieved by splitting each training sentence into sub-sentences. This partitioning ensures that a majority of the sub-training sets do not contain the backdoor trigger. Subsequently, a base classifier is trained from each sub-training set, and their ensemble provides the final prediction. We theoretically prove that when the length of the backdoor trigger falls within a certain threshold, TextGuard guarantees that its prediction will remain unaffected by the presence of the triggers in training and testing inputs. In our evaluation, we demonstrate the effectiveness of TextGuard on three benchmark text classification tasks, surpassing the certification accuracy of existing certified defenses against backdoor attacks. Furthermore, we propose additional strategies to enhance the empirical performance of TextGuard. Comparisons with state-of-the-art empirical defenses validate the superiority of TextGuard in countering multiple backdoor attacks. Our code and data are available at https://github.com/AI-secure/TextGuard.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>>背门攻击已成为安全应用中的主要安全威胁，当部署机器学习模型时。现有的研究努力已经提出了许多防御措施，但是None of these techniques could provide a formal and provable security guarantee against arbitrary attacks。因此，它们可以轻松被强化的适应攻击所损坏，如我们的评估所示。在这个工作中，我们提出了TextGuard，首个可提供可靠的防御措施，对于文本分类中的背门攻击。具体来说，TextGuard首先将（背门）训练数据分成子训练集，通过每个训练句子分成子句子。这些分割确保了训练集中大多数子训练集不包含背门触发器。接着，我们从每个子训练集中训练基本分类器，并将其结合为最终预测。我们 teorically prove that 当背门触发器的长度在某些阈值之下，TextGuard 的预测将不受训练和测试输入中背门触发器的影响。在我们的评估中，我们显示了 TextGuard 在三个标准文本分类任务上的效果，超过了现有的认证防御措施的认证精度。此外，我们还提出了一些增强 TextGuard 的实际性的策略。与现有的实际防御措施比较，TextGuard 在面对多个背门攻击时表现出色。我们的代码和数据可以在 <https://github.com/AI-secure/TextGuard> 上获得。
</details></li>
</ul>
<hr>
<h2 id="Robust-Network-Slicing-Multi-Agent-Policies-Adversarial-Attacks-and-Defensive-Strategies"><a href="#Robust-Network-Slicing-Multi-Agent-Policies-Adversarial-Attacks-and-Defensive-Strategies" class="headerlink" title="Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and Defensive Strategies"></a>Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and Defensive Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11206">http://arxiv.org/abs/2311.11206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Wang, M. Cenk Gursoy, Senem Velipasalar</li>
<li>for: 本文提出了一种基于多智能深度学习（深度RL）框架的网络切片在动态环境中的多基站多用户场景中的自适应控制方法。</li>
<li>methods: 本文提出了一种新的深度RL框架，其中多个演示器（actor）被实现为指向网络中的变化输入。此外，中央批评器（critic）也被使用以提高演示器的性能。</li>
<li>results: 通过实验表明，提出的深度RL算法可以具有效果地控制网络切片。此外，还提出了一种基于深度RL的干扰器，可以在有限的前置信息和功率预算下减少网络切片的传输率，从而降低网络切片的性能。最后，提出了一种基于 Nash 平衡的策略混合 Profile，可以用于网络切片和干扰器中。通过在实验中应用这种策略混合 Profile，得到了良好的性能。<details>
<summary>Abstract</summary>
In this paper, we present a multi-agent deep reinforcement learning (deep RL) framework for network slicing in a dynamic environment with multiple base stations and multiple users. In particular, we propose a novel deep RL framework with multiple actors and centralized critic (MACC) in which actors are implemented as pointer networks to fit the varying dimension of input. We evaluate the performance of the proposed deep RL algorithm via simulations to demonstrate its effectiveness. Subsequently, we develop a deep RL based jammer with limited prior information and limited power budget. The goal of the jammer is to minimize the transmission rates achieved with network slicing and thus degrade the network slicing agents' performance. We design a jammer with both listening and jamming phases and address jamming location optimization as well as jamming channel optimization via deep RL. We evaluate the jammer at the optimized location, generating interference attacks in the optimized set of channels by switching between the jamming phase and listening phase. We show that the proposed jammer can significantly reduce the victims' performance without direct feedback or prior knowledge on the network slicing policies. Finally, we devise a Nash-equilibrium-supervised policy ensemble mixed strategy profile for network slicing (as a defensive measure) and jamming. We evaluate the performance of the proposed policy ensemble algorithm by applying on the network slicing agents and the jammer agent in simulations to show its effectiveness.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一个多代理深度强化学习（深度RL）框架，用于在多个基站和多个用户的动态环境中实现网络分割。特别是，我们提出了一种新的多actor和中央批评者（MACC）深度RL框架，在其中， actors 是实现为指针网络，以适应输入的变化维度。我们通过 simulations 评估了我们提出的深度RL 算法的性能，以示其效果。然后，我们开发了一个基于深度RL 的干扰器，该干扰器具有限制的前置信息和限制的能量预算。干扰器的目标是降低网络分割代理的性能，以达到干扰网络分割的目的。我们设计了干扰器的 listening 和干扰阶段，并对干扰频道优化以及干扰位置优化进行深度RL 的调 optimize。我们在优化的位置上进行干扰，通过在优化的频道上进行 switching 来生成干扰攻击。我们证明了我们的干扰器可以在不知情的情况下，对网络分割代理造成重大的性能降低。最后，我们提出了一种 Nash 平衡监督的策略ensemble 混合策略，用于网络分割（作为防御措施）和干扰。我们通过在网络分割代理和干扰器代理上应用这种策略 ensemble 算法，以证明其效果。
</details></li>
</ul>
<hr>
<h2 id="Scale-free-networks-improved-inference"><a href="#Scale-free-networks-improved-inference" class="headerlink" title="Scale-free networks: improved inference"></a>Scale-free networks: improved inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11200">http://arxiv.org/abs/2311.11200</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Nixon Jerez-Lillo, Francisco A. Rodrigues, Pedro L. Ramos</li>
<li>for: 本文 investigate  whether 网络的度分布遵循 power-law 分布，并提出了改进的 Bayesian  inference 方法来确定模型参数的准确性和置信区间。</li>
<li>methods: 本文使用 Bayesian inference 方法，包括对绝对和相对分布进行推导，以获得准确的模型参数估计和置信区间。</li>
<li>results: 研究结果表明，使用 Bayesian inference 方法可以获得 nearly unbiased 的模型参数估计，并且在实际应用中更适用。 本文还对 более чем 5,000 个synthetic网络和3,000个实际网络进行了应用。结果表明，我们的方法在实际应用中更适用，它的频率接受相对较高。<details>
<summary>Abstract</summary>
The power-law distribution plays a crucial role in complex networks as well as various applied sciences. Investigating whether the degree distribution of a network follows a power-law distribution is an important concern. The commonly used inferential methods for estimating the model parameters often yield biased estimates, which can lead to the rejection of the hypothesis that a model conforms to a power-law. In this paper, we discuss improved methods that utilize Bayesian inference to obtain accurate estimates and precise credibility intervals. The inferential methods are derived for both continuous and discrete distributions. These methods reveal that objective Bayesian approaches return nearly unbiased estimates for the parameters of both models. Notably, in the continuous case, we identify an explicit posterior distribution. This work enhances the power of goodness-of-fit tests, enabling us to accurately discern whether a network or any other dataset adheres to a power-law distribution. We apply the proposed approach to fit degree distributions for more than 5,000 synthetic networks and over 3,000 real networks. The results indicate that our method is more suitable in practice, as it yields a frequency of acceptance close to the specified nominal level.
</details>
<details>
<summary>摘要</summary>
power-law 分布在复杂网络以及各种应用科学中扮演着关键角色。检查网络度分布是否遵循power-law分布是一项重要的问题。通常使用的推断方法对模型参数进行估计通常会导致偏向估计，这可能会导致模型不符合power-law分布的拒绝。在这篇论文中，我们讨论了改进的方法，利用 Bayesian 推断来获得准确的估计和精确的信任范围。这些方法适用于连续和离散分布。这些方法表明，对 Bayesian 方法进行 Objective 的推断可以返回准确的参数估计。在连续 caso，我们确定了一个显式后验分布。这项工作可以增强好宜性测试的力量，使我们能准确地判断网络或任何其他数据集是否遵循power-law分布。我们在fit degree distribution中应用该方法，并对超过5,000个synthetic网络和3,000个实际网络进行测试。结果表明，我们的方法在实践中更适合，它的频次接受度很接近指定的正常水平。
</details></li>
</ul>
<hr>
<h2 id="Testing-with-Non-identically-Distributed-Samples"><a href="#Testing-with-Non-identically-Distributed-Samples" class="headerlink" title="Testing with Non-identically Distributed Samples"></a>Testing with Non-identically Distributed Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11194">http://arxiv.org/abs/2311.11194</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Shivam Garg, Chirag Pabbaraju, Kirankumar Shiragur, Gregory Valiant</li>
<li>for: 这个论文研究了在独立但不同分布下的非线性样本计数和估计问题。</li>
<li>methods: 该论文使用了分布测试框架，具体来说是对每个分布$\textbf{p}<em>i$取$c$个独立的样本，然后通过学习或测试$\textbf{p}</em>{\text{avg}$的平均分布来进行分布测试。</li>
<li>results: 研究结果表明，当$c&#x3D;1$时，需要$\Theta(k&#x2F;\varepsilon^2)$样本来准确地学习$\textbf{p}_{\text{avg}$，而在测试uniformity或identity时，需要 linear 样本数量与$k$相关。而当$c\geq 2$时，样本数量可以降低至$O(\sqrt{k}&#x2F;\varepsilon^2 + 1&#x2F;\varepsilon^4)$，与i.i.d.情况的优秀样本复杂度相同。此外，当$c&#x3D;2$时，存在一个常数$\rho &gt; 0$，使得即使使用$\rho k$个样本，也无法通过忽略哪些样本来自同$\textbf{p}_i$的Multiset测试uniformity。<details>
<summary>Abstract</summary>
We examine the extent to which sublinear-sample property testing and estimation applies to settings where samples are independently but not identically distributed. Specifically, we consider the following distributional property testing framework: Suppose there is a set of distributions over a discrete support of size $k$, $\textbf{p}_1, \textbf{p}_2,\ldots,\textbf{p}_T$, and we obtain $c$ independent draws from each distribution. Suppose the goal is to learn or test a property of the average distribution, $\textbf{p}_{\mathrm{avg}$. This setup models a number of important practical settings where the individual distributions correspond to heterogeneous entities -- either individuals, chronologically distinct time periods, spatially separated data sources, etc. From a learning standpoint, even with $c=1$ samples from each distribution, $\Theta(k/\varepsilon^2)$ samples are necessary and sufficient to learn $\textbf{p}_{\mathrm{avg}$ to within error $\varepsilon$ in TV distance. To test uniformity or identity -- distinguishing the case that $\textbf{p}_{\mathrm{avg}$ is equal to some reference distribution, versus has $\ell_1$ distance at least $\varepsilon$ from the reference distribution, we show that a linear number of samples in $k$ is necessary given $c=1$ samples from each distribution. In contrast, for $c \ge 2$, we recover the usual sublinear sample testing of the i.i.d. setting: we show that $O(\sqrt{k}/\varepsilon^2 + 1/\varepsilon^4)$ samples are sufficient, matching the optimal sample complexity in the i.i.d. case in the regime where $\varepsilon \ge k^{-1/4}$. Additionally, we show that in the $c=2$ case, there is a constant $\rho > 0$ such that even in the linear regime with $\rho k$ samples, no tester that considers the multiset of samples (ignoring which samples were drawn from the same $\textbf{p}_i$) can perform uniformity testing.
</details>
<details>
<summary>摘要</summary>
我们研究在独立但不同分布下的测试和估计中，sublinear-sample property testing和估计是否适用。 Specifically，我们考虑以下分布性 property testing框架：假设有一集合 $\textbf{p}_1, \textbf{p}_2, \ldots, \textbf{p}_T$ 的分布，每个分布都是在给定的类别支持 $\mathcal{X}$ 上的一个统计分布，并且我们从每个分布中获得 $c$ 个独立的抽象。我们的目标是要学习或测试 $\textbf{p}_{\text{avg}$ 的平均分布。这个设定模拟了许多实际上重要的实际设定，例如个别分布对应的不同实体，例如个人、时间对应的不同时期、空间分布的不同数据来源等。从学习角度来看，就算有 $c=1$ 个抽象，需要 $\Theta(k/\varepsilon^2)$ 个抽象来learn $\textbf{p}_{\text{avg}$ 到 Within 误差 $\varepsilon$ 的TV距离。对于uniformity或identify性测试，我们显示出在 $k$ 中的线性数量的抽象是必要的，假设 $c=1$ 个抽象。相比之下，如果 $c \ge 2$，我们可以回传i.i.d. 的情况下的低于线性数量的抽象，即 $O(\sqrt{k}/\varepsilon^2 + 1/\varepsilon^4)$ 个抽象足够，与i.i.d. 情况下的低于线性数量的抽象匹配。此外，我们还显示出在 $c=2$ 情况下，存在一个常数 $\rho > 0$，使得甚至在 $\rho k$ 个抽象下，没有任何测试器可以对多个抽象（忽略哪些抽象是哪些 $\textbf{p}_i$ 的）进行uniformity测试。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/cs.LG_2023_11_19/" data-id="clpztdnmh00vces8802qkg1o1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
