
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/2/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_09_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/10/cs.SD_2023_09_10/" class="article-date">
  <time datetime="2023-09-10T15:00:00.000Z" itemprop="datePublished">2023-09-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/10/cs.SD_2023_09_10/">cs.SD - 2023-09-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multimodal-Fish-Feeding-Intensity-Assessment-in-Aquaculture"><a href="#Multimodal-Fish-Feeding-Intensity-Assessment-in-Aquaculture" class="headerlink" title="Multimodal Fish Feeding Intensity Assessment in Aquaculture"></a>Multimodal Fish Feeding Intensity Assessment in Aquaculture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05058">http://arxiv.org/abs/2309.05058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Cui, Xubo Liu, Haohe Liu, Zhuangzhuang Du, Tao Chen, Guoping Lian, Daoliang Li, Wenwu Wang</li>
<li>for: 本研究旨在评估鱼类食欲强度变化 during feeding process, 在工业水生养殖中扮演重要角色。</li>
<li>methods: 本研究使用多Modal方法，包括单模态预训练模型和模态融合方法，并在AV-FFIA数据集上进行了比较研究。</li>
<li>results: 结果表明，多模态方法在噪音环境中表现出色，而单模态方法在清晰环境中表现较差。此外，提出了一种单模型可处理多modal数据的方法，能够实现与专门预训练模型相同或更高的性能，同时减少计算负担。<details>
<summary>Abstract</summary>
Fish feeding intensity assessment (FFIA) aims to evaluate the intensity change of fish appetite during the feeding process, which is vital in industrial aquaculture applications. The main challenges surrounding FFIA are two-fold. 1) robustness: existing work has mainly leveraged single-modality (e.g., vision, audio) methods, which have a high sensitivity to input noise. 2) efficiency: FFIA models are generally expected to be employed on devices. This presents a challenge in terms of computational efficiency. In this work, we first introduce an audio-visual dataset, called AV-FFIA. AV-FFIA consists of 27,000 labeled audio and video clips that capture different levels of fish feeding intensity. To our knowledge, AV-FFIA is the first large-scale multimodal dataset for FFIA research. Then, we introduce a multi-modal approach for FFIA by leveraging single-modality pre-trained models and modality-fusion methods, with benchmark studies on AV-FFIA. Our experimental results indicate that the multi-modal approach substantially outperforms the single-modality based approach, especially in noisy environments. While multimodal approaches provide a performance gain for FFIA, it inherently increase the computational cost. To overcome this issue, we further present a novel unified model, termed as U-FFIA. U-FFIA is a single model capable of processing audio, visual, or audio-visual modalities, by leveraging modality dropout during training and knowledge distillation from single-modality pre-trained models. We demonstrate that U-FFIA can achieve performance better than or on par with the state-of-the-art modality-specific FFIA models, with significantly lower computational overhead. Our proposed U-FFIA approach enables a more robust and efficient method for FFIA, with the potential to contribute to improved management practices and sustainability in aquaculture.
</details>
<details>
<summary>摘要</summary>
鱼饵吞吐评估（FFIA）目的是评估鱼饵吞吐过程中鱼饵欲求的变化，这在工业鱼养中非常重要。主要挑战包括：1）稳定性：现有工作主要利用单模态（如视觉、声音）方法，具有高度敏感itivity to input noise。2）效率：FFIA模型通常预期在设备上使用，这会导致计算效率的挑战。在这项工作中，我们首先介绍了一个Audio-Visual（AV）-FFIA数据集，该数据集包含27,000个标注的音频和视频剪辑，每个剪辑都 capture不同水平的鱼饵吞吐Intensity。我们知道，AV-FFIA是首个大规模的多模态FFIA研究数据集。然后，我们介绍了一种多模态方法，通过单模态预训练模型和多模态融合方法，在AV-FFIA上进行了标准 benchmark研究。我们的实验结果表明，多模态方法在噪音环境下表现明显 луч于单模态方法，特别是在噪音环境下。虽然多模态方法提供了FFIA方面的性能提升，但它会自然增加计算成本。为解决这个问题，我们进一步介绍了一种单模型Unified Model（U-FFIA），该模型可以处理音频、视频或Audio-Visual多模态，通过训练时的模式Dropout和知识储存Single-modality预训练模型，实现与单模态FFIA模型的性能相似或更高，同时具有显著更低的计算开销。我们的提出的U-FFIA方法可以实现更加稳定和高效的FFIA，并且具有潜在的提高鱼养管理实践和可持续发展的潜力。
</details></li>
</ul>
<hr>
<h2 id="Gray-Jedi-MVDR-Post-filtering"><a href="#Gray-Jedi-MVDR-Post-filtering" class="headerlink" title="Gray Jedi MVDR Post-filtering"></a>Gray Jedi MVDR Post-filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05057">http://arxiv.org/abs/2309.05057</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FrancoisGrondin/mvdrpf">https://github.com/FrancoisGrondin/mvdrpf</a></li>
<li>paper_authors: François Grondin, Caleb Rascón</li>
<li>for: 提高多个说话人enario中语音质量</li>
<li>methods: 使用深度学习基于的抗噪模型和Minimum Variance Distortionless Response（MVDR）进行抗噪和干扰估计</li>
<li>results: 比单输入基eline的干扰估计性能更高，需要 menos computing resources для后处理，可以在多个说话人enario中开发在线语音提高系统<details>
<summary>Abstract</summary>
Spatial filters can exploit deep-learning-based speech enhancement models to increase their reliability in scenarios with multiple speech sources scenarios. To further improve speech quality, it is common to perform postfiltering on the estimated target speech obtained with spatial filtering. In this work, Minimum Variance Distortionless Response (MVDR) is employed to provide the interference estimation, along with the estimation of the target speech, to be later used for postfiltering. This improves the enhancement performance over a single-input baseline in a far more significant way than by increasing the model's complexity. Results suggest that less computing resources are required for postfiltering when provided with both target and interference signals, which is a step forward in developing an online speech enhancement system for multi-speech scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用深度学习基于的speech enhancement模型可以提高空间滤波器的可靠性，特别在多个语音源场景中。为了进一步提高语音质量，通常会在空间滤波器估计target speech后进行postfiltering。在这种工作中，我们使用Minimum Variance Distortionless Response（MVDR）来提供干扰估计，同时提供target speech的估计，以供后续使用。这会提高增强性能，比单输入基eline的增强性能更高，并且需要更少的计算资源。结果表明，当提供target和干扰信号时，postfiltering需要更少的计算资源，这是在开发多语音场景中的线上speech增强系统的一大进步。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="VoiceFlow-Efficient-Text-to-Speech-with-Rectified-Flow-Matching"><a href="#VoiceFlow-Efficient-Text-to-Speech-with-Rectified-Flow-Matching" class="headerlink" title="VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching"></a>VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05027">http://arxiv.org/abs/2309.05027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwei Guo, Chenpeng Du, Ziyang Ma, Xie Chen, Kai Yu</li>
<li>for: 提高文本到语音的生成质量，降低 diffusion models 的复杂性</li>
<li>methods: 使用矩阵流匹配算法，将文本输入转化为 mel-spectrograms 的生成过程</li>
<li>results: 与 diffusion models 相比，VoiceFlow 实现了更高的生成质量，并且可以采用有限多个抽样步骤进行生成<details>
<summary>Abstract</summary>
Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:Diffusion模型在文本至语音转化中变得非常流行，但是它们的内在复杂性使其效率受限。我们提议VoiceFlow，一种使用矫正流匹配算法实现高质量的语音合成，只需少量抽样步骤。VoiceFlow将文本输入转化为mel-spectrogram的生成过程，并且使用了一个条件的ordinary differential equation来估算vector field。矫正流技术然后有效地直线化抽样轨迹，提高了效率。对于单个和多个说话者的corpora进行主观和客观评估，VoiceFlow的合成质量显著高于 diffusion counterpart。ablation studies进一步验证了矫正流技术在VoiceFlow中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Emotional-Adaptation-for-Audio-Driven-Talking-Head-Generation"><a href="#Efficient-Emotional-Adaptation-for-Audio-Driven-Talking-Head-Generation" class="headerlink" title="Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation"></a>Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04946">http://arxiv.org/abs/2309.04946</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuangan/eat_code">https://github.com/yuangan/eat_code</a></li>
<li>paper_authors: Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, Yi Yang</li>
<li>for: 这个研究旨在提高现有的音频驱动的人脸合成方法，以提高虚拟人类相关应用的可靠性和生动性。</li>
<li>methods: 我们提出了一种名为Emotional Adaptation for Audio-driven Talking-head（EAT）方法，它可以将无情的人脸模型转化为可控制情感的模型，并且可以在Cost-effective和高效的情况下进行。我们的方法包括三种轻量级的修改（深度情感提示、情感变形网络和情感适应模块），从不同的角度来实现精准和生动的情感控制。</li>
<li>results: 我们的实验结果表明，我们的方法可以在广泛使用的基准数据集上达到当前最佳性能，包括LRW和MEAD。此外，我们的参数效率的修改还能够在情感训练视频罕见或缺失时显示出很好的泛化能力。更多信息请访问<a target="_blank" rel="noopener" href="https://yuangan.github.io/eat/%E3%80%82">https://yuangan.github.io/eat/。</a><details>
<summary>Abstract</summary>
Audio-driven talking-head synthesis is a popular research topic for virtual human-related applications. However, the inflexibility and inefficiency of existing methods, which necessitate expensive end-to-end training to transfer emotions from guidance videos to talking-head predictions, are significant limitations. In this work, we propose the Emotional Adaptation for Audio-driven Talking-head (EAT) method, which transforms emotion-agnostic talking-head models into emotion-controllable ones in a cost-effective and efficient manner through parameter-efficient adaptations. Our approach utilizes a pretrained emotion-agnostic talking-head transformer and introduces three lightweight adaptations (the Deep Emotional Prompts, Emotional Deformation Network, and Emotional Adaptation Module) from different perspectives to enable precise and realistic emotion controls. Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including LRW and MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable generalization ability, even in scenarios where emotional training videos are scarce or nonexistent. Project website: https://yuangan.github.io/eat/
</details>
<details>
<summary>摘要</summary>
文本：Audio-driven talking-head synthesis是虚拟人类应用领域中受欢迎的研究主题。然而，现有方法的不灵活和不高效性是重大的限制。在这项工作中，我们提出了情感适应 Audio-driven talking-head（EAT）方法，可以将情感无关的 talking-head 模型转化成可控情感的模型，而不需要昂贵的端到端训练。我们的方法利用了预训练的情感无关 talking-head 变换器，并引入了三种轻量级的适应（深度情感提示、情感变形网络和情感适应模块），从不同的角度实现精准和真实的情感控制。我们的实验表明，我们的方法在广泛使用的标准准则上达到了领先的性能水平，包括 LRW 和 MEAD。此外，我们的参数效率适应表现出了惊人的普适性，即使情感训练视频罕见或缺失。项目网站：https://yuangan.github.io/eat/Translation:文本：Audio-driven talking-head synthesis是虚拟人类应用领域中受欢迎的研究主题。然而，现有方法的不灵活和不高效性是重大的限制。在这项工作中，我们提出了情感适应 Audio-driven talking-head（EAT）方法，可以将情感无关的 talking-head 模型转化成可控情感的模型，而不需要昂贵的端到端训练。我们的方法利用了预训练的情感无关 talking-head 变换器，并引入了三种轻量级的适应（深度情感提示、情感变形网络和情感适应模块），从不同的角度实现精准和真实的情感控制。我们的实验表明，我们的方法在广泛使用的标准准则上达到了领先的性能水平，包括 LRW 和 MEAD。此外，我们的参数效率适应表现出了惊人的普适性，即使情感训练视频罕见或缺失。项目网站：https://yuangan.github.io/eat/Note: The translation is in Simplified Chinese, which is the most widely used Chinese language in mainland China.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/10/cs.SD_2023_09_10/" data-id="clmjn91og00c10j88hr6q42ns" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/10/cs.LG_2023_09_10/" class="article-date">
  <time datetime="2023-09-10T10:00:00.000Z" itemprop="datePublished">2023-09-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/10/cs.LG_2023_09_10/">cs.LG - 2023-09-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Learning-Energy-Based-Models-by-Cooperative-Diffusion-Recovery-Likelihood"><a href="#Learning-Energy-Based-Models-by-Cooperative-Diffusion-Recovery-Likelihood" class="headerlink" title="Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood"></a>Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05153">http://arxiv.org/abs/2309.05153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaxuan Zhu, Jianwen Xie, Yingnian Wu, Ruiqi Gao</li>
<li>for: 本研究旨在提高能量基本模型（EBM）的训练效率和样本质量，使EBM能够更好地与其他生成模型（如GANs和扩散模型）一样。</li>
<li>methods: 本研究提出了协同扩散恢复likelihood（CDRL）方法，它是一种有效地学习和采样多个EBM，每个EBM都有一个初始化模型。在每个噪音水平上，初始化模型学习了采样过程的总和，而两个模型在一起进行了合作训练。采样过程中使用了新的噪音规划和减少噪音技术，以提高样本质量。</li>
<li>results:  comparing with现有EBM方法，CDRL在CIFAR-10和ImageNet 32x32上显著提高了FID分数，同时具有2倍的速度提升。此外，CDRL还可以应用于 compositional generation 和图像填充任务，并在无类标注的情况下实现类似的折衔between样本质量和样本多样性。<details>
<summary>Abstract</summary>
Training energy-based models (EBMs) with maximum likelihood estimation on high-dimensional data can be both challenging and time-consuming. As a result, there a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning EBMs by maximimizing diffusion recovery likelihood (DRL), we propose cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versons of a dataset, paired with an initializer model for each EBM. At each noise level, the initializer model learns to amortize the sampling process of the EBM, and the two models are jointly estimated within a cooperative training framework. Samples from the initializer serve as starting points that are refined by a few sampling steps from the EBM. With the refined samples, the EBM is optimized by maximizing recovery likelihood, while the initializer is optimized by learning from the difference between the refined samples and the initial samples. We develop a new noise schedule and a variance reduction technique to further improve the sample quality. Combining these advances, we significantly boost the FID scores compared to existing EBM methods on CIFAR-10 and ImageNet 32x32, with a 2x speedup over DRL. In addition, we extend our method to compositional generation and image inpainting tasks, and showcase the compatibility of CDRL with classifier-free guidance for conditional generation, achieving similar trade-offs between sample quality and sample diversity as in diffusion models.
</details>
<details>
<summary>摘要</summary>
培训能量基模型（EBM）的最大极大似然估计在高维数据上可能是一项挑战和时间消耗的任务。这导致EBM与其他生成模型如GANs和扩散模型之间存在一个明显的样本质量差距。为了填补这一差距，我们提出了协同扩散恢复似然（CDRL）方法，这是一种有效地学习和采样多个基于扩散的EBM，每个EBM都有一个初始化模型。在每个噪音水平上，初始化模型学习了扩散过程的吸收，两个模型在一起进行协同训练。从初始化模型中获得的样本会被EBM进行数个抽取步骤的精炼，以提高样本质量。通过这种方式，EBM可以通过最大化恢复似然来优化，而初始化模型则是通过学习差异来学习。我们还开发了一个新的噪音调度和一种减少噪音的技术，以进一步提高样本质量。结合这些进步，我们在CIFAR-10和ImageNet 32x32上明显提高了FID分数，与DRL相比，速度提高了2倍。此外，我们还扩展了我们的方法到组合生成和图像填充任务，并证明CDRL与无类标注导向的条件生成具有相同的交易offs。
</details></li>
</ul>
<hr>
<h2 id="Faster-Lighter-More-Accurate-A-Deep-Learning-Ensemble-for-Content-Moderation"><a href="#Faster-Lighter-More-Accurate-A-Deep-Learning-Ensemble-for-Content-Moderation" class="headerlink" title="Faster, Lighter, More Accurate: A Deep Learning Ensemble for Content Moderation"></a>Faster, Lighter, More Accurate: A Deep Learning Ensemble for Content Moderation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05150">http://arxiv.org/abs/2309.05150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Hosseini, Mahmudul Hasan<br>for:  This paper aims to address the increasing need for efficient and accurate content moderation by proposing an efficient and lightweight deep classification ensemble structure.methods:  The proposed approach combines simple visual features and a set of lightweight models with narrowed-down color features, which are applied to both images and videos.results:  The authors evaluated their approach using a large dataset of explosion and blast contents and demonstrated significant improvements in prediction accuracy, with faster inference and lower computation cost compared to popular deep learning models such as ResNet-50.<details>
<summary>Abstract</summary>
To address the increasing need for efficient and accurate content moderation, we propose an efficient and lightweight deep classification ensemble structure. Our approach is based on a combination of simple visual features, designed for high-accuracy classification of violent content with low false positives. Our ensemble architecture utilizes a set of lightweight models with narrowed-down color features, and we apply it to both images and videos.   We evaluated our approach using a large dataset of explosion and blast contents and compared its performance to popular deep learning models such as ResNet-50. Our evaluation results demonstrate significant improvements in prediction accuracy, while benefiting from 7.64x faster inference and lower computation cost.   While our approach is tailored to explosion detection, it can be applied to other similar content moderation and violence detection use cases as well. Based on our experiments, we propose a "think small, think many" philosophy in classification scenarios. We argue that transforming a single, large, monolithic deep model into a verification-based step model ensemble of multiple small, simple, and lightweight models with narrowed-down visual features can possibly lead to predictions with higher accuracy.
</details>
<details>
<summary>摘要</summary>
(Note: Simplified Chinese translation may vary depending on the specific context and register used. The translation above is written in a more formal and neutral register, and may not be suitable for all contexts or audiences. Additionally, please note that the use of "think small, think many" as a philosophical concept may not be directly translatable to Simplified Chinese, as the original English phrase is a idiomatic expression that may not have a direct equivalent in other languages.)
</details></li>
</ul>
<hr>
<h2 id="Outlier-Robust-Adversarial-Training"><a href="#Outlier-Robust-Adversarial-Training" class="headerlink" title="Outlier Robust Adversarial Training"></a>Outlier Robust Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05145">http://arxiv.org/abs/2309.05145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/discovershu/orat">https://github.com/discovershu/orat</a></li>
<li>paper_authors: Shu Hu, Zhenhuan Yang, Xin Wang, Yiming Ying, Siwei Lyu</li>
<li>for: 本研究旨在开发一种同时处理低质量训练数据和敌意攻击的鲁棒学习模型。</li>
<li>methods: 本文提出了一种名为Outlier Robust Adversarial Training（ORAT）的方法，基于防御性训练的双层优化形式和鲁棒排名函数。</li>
<li>results: 实验结果表明，ORAT可以有效地处理异常值和敌意攻击，并且在三个 benchmark 数据集上达到了良好的效果和鲁棒性。<details>
<summary>Abstract</summary>
Supervised learning models are challenged by the intrinsic complexities of training data such as outliers and minority subpopulations and intentional attacks at inference time with adversarial samples. While traditional robust learning methods and the recent adversarial training approaches are designed to handle each of the two challenges, to date, no work has been done to develop models that are robust with regard to the low-quality training data and the potential adversarial attack at inference time simultaneously. It is for this reason that we introduce Outlier Robust Adversarial Training (ORAT) in this work. ORAT is based on a bi-level optimization formulation of adversarial training with a robust rank-based loss function. Theoretically, we show that the learning objective of ORAT satisfies the $\mathcal{H}$-consistency in binary classification, which establishes it as a proper surrogate to adversarial 0/1 loss. Furthermore, we analyze its generalization ability and provide uniform convergence rates in high probability. ORAT can be optimized with a simple algorithm. Experimental evaluations on three benchmark datasets demonstrate the effectiveness and robustness of ORAT in handling outliers and adversarial attacks. Our code is available at https://github.com/discovershu/ORAT.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>超visisted学习模型面临训练数据中的内在复杂性，如异常值和少数派 subgroup，以及推理时间的意外攻击。而传统的Robust learning方法和最近的反击训练方法是专门处理每一种挑战，但到目前为止，没有任何工作是同时处理低质量训练数据和推理时间的意外攻击的模型。这是我们在这篇论文中引入Outlier Robust Adversarial Training（ORAT）的原因。ORAT基于防御性训练的两级优化问题，并使用Robust rank-based损失函数。理论上，我们显示了ORAT的学习目标满足了$\mathcal{H}$-一致性在二分类问题中，这确立了它作为对抗0/1损失函数的合理代理。此外，我们分析了它的泛化能力，并提供了高probability中的均匀收敛率。ORAT可以使用简单的算法进行优化。实验评估在三个benchmark dataset上，表明ORAT有效地和抗性地处理异常值和意外攻击。我们的代码可以在https://github.com/discovershu/ORAT中获取。
</details></li>
</ul>
<hr>
<h2 id="DAD-Improved-Data-free-Test-Time-Adversarial-Defense"><a href="#DAD-Improved-Data-free-Test-Time-Adversarial-Defense" class="headerlink" title="DAD++: Improved Data-free Test Time Adversarial Defense"></a>DAD++: Improved Data-free Test Time Adversarial Defense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05132">http://arxiv.org/abs/2309.05132</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vcl-iisc/data-free-defense-at-test-time">https://github.com/vcl-iisc/data-free-defense-at-test-time</a></li>
<li>paper_authors: Gaurav Kumar Nayak, Inder Khatri, Shubham Randive, Ruchit Rawal, Anirban Chakraborty</li>
<li>for: 提高深度神经网络在安全关键应用中的可靠性，如自动驾驶、医学成像、异常检测等。</li>
<li>methods: 提出了一种基于测试时数据free adversarial defense的方法，包括检测和修正框架。此外，还提出了一种半稳定检测方案（DAD++）来提高修正框架的可靠性。</li>
<li>results: 通过多种实验和简洁方法，证明了我们提出的方法可以具有卓越的鲁棒性 against 多种攻击方法，同时减少了干净率的损失。这种方法可以在数据free（或数据效率）应用中实现鲁棒性。<details>
<summary>Abstract</summary>
With the increasing deployment of deep neural networks in safety-critical applications such as self-driving cars, medical imaging, anomaly detection, etc., adversarial robustness has become a crucial concern in the reliability of these networks in real-world scenarios. A plethora of works based on adversarial training and regularization-based techniques have been proposed to make these deep networks robust against adversarial attacks. However, these methods require either retraining models or training them from scratch, making them infeasible to defend pre-trained models when access to training data is restricted. To address this problem, we propose a test time Data-free Adversarial Defense (DAD) containing detection and correction frameworks. Moreover, to further improve the efficacy of the correction framework in cases when the detector is under-confident, we propose a soft-detection scheme (dubbed as "DAD++"). We conduct a wide range of experiments and ablations on several datasets and network architectures to show the efficacy of our proposed approach. Furthermore, we demonstrate the applicability of our approach in imparting adversarial defense at test time under data-free (or data-efficient) applications/setups, such as Data-free Knowledge Distillation and Source-free Unsupervised Domain Adaptation, as well as Semi-supervised classification frameworks. We observe that in all the experiments and applications, our DAD++ gives an impressive performance against various adversarial attacks with a minimal drop in clean accuracy. The source code is available at: https://github.com/vcl-iisc/Improved-Data-free-Test-Time-Adversarial-Defense
</details>
<details>
<summary>摘要</summary>
随着深度神经网络在安全关键应用中的广泛部署，如自动驾驶车、医疗影像、异常检测等，对抗攻击的可靠性已成为这些网络在实际场景中的关键问题。一系列基于对抗训练和规范化技术的方法已经被提出来使得这些深度网络对抗攻击。然而，这些方法需要重新训练模型或从 scratch 训练，这使得不可靠地防御预训练模型，特别是当训练数据有限制时。为解决这个问题，我们提出了一种不需要训练数据的测试时间数据自由对抗防御（DAD），包括检测和修正框架。此外，为了进一步提高修正框架在检测器不确定时的效果，我们提出了一种软检测方案（称为“DAD++”）。我们对多个数据集和网络架构进行了广泛的实验和减少，以显示我们提出的方法的有效性。此外，我们还证明了我们的方法可以在无数据（或数据有效）的应用/设置下进行对抗防御，如数据自由知识传递和源自无监督适应性，以及半监督分类框架。我们发现在所有实验和应用中，我们的 DAD++ 对各种对抗攻击表现出色，几乎不影响干净率。源代码可以在 GitHub 上获取：https://github.com/vcl-iisc/Improved-Data-free-Test-Time-Adversarial-Defense。
</details></li>
</ul>
<hr>
<h2 id="Signal-Temporal-Logic-Neural-Predictive-Control"><a href="#Signal-Temporal-Logic-Neural-Predictive-Control" class="headerlink" title="Signal Temporal Logic Neural Predictive Control"></a>Signal Temporal Logic Neural Predictive Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05131">http://arxiv.org/abs/2309.05131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Meng, Chuchu Fan</li>
<li>for: 本研究旨在提高长期 роботи工作中的安全性和时间约束满足。</li>
<li>methods: 本研究使用了信号时间逻辑（STL）来系统地和科学地 especify 需求。然而，传统的控制策略找到方法是 computationally 复杂且不可扩展性。本研究提出了一种直接将神经网络控制器学习到STL需求中的方法。</li>
<li>results: 在六个任务中，我们的方法与备份策略比 класси方法（MPC、STL解决方案）、模型自由和模型基于RL方法在STL满足率方面表现出色，尤其是在任务中有复杂STL需求时。此外，我们的方法比传统方法10X-100X快。<details>
<summary>Abstract</summary>
Ensuring safety and meeting temporal specifications are critical challenges for long-term robotic tasks. Signal temporal logic (STL) has been widely used to systematically and rigorously specify these requirements. However, traditional methods of finding the control policy under those STL requirements are computationally complex and not scalable to high-dimensional or systems with complex nonlinear dynamics. Reinforcement learning (RL) methods can learn the policy to satisfy the STL specifications via hand-crafted or STL-inspired rewards, but might encounter unexpected behaviors due to ambiguity and sparsity in the reward. In this paper, we propose a method to directly learn a neural network controller to satisfy the requirements specified in STL. Our controller learns to roll out trajectories to maximize the STL robustness score in training. In testing, similar to Model Predictive Control (MPC), the learned controller predicts a trajectory within a planning horizon to ensure the satisfaction of the STL requirement in deployment. A backup policy is designed to ensure safety when our controller fails. Our approach can adapt to various initial conditions and environmental parameters. We conduct experiments on six tasks, where our method with the backup policy outperforms the classical methods (MPC, STL-solver), model-free and model-based RL methods in STL satisfaction rate, especially on tasks with complex STL specifications while being 10X-100X faster than the classical methods.
</details>
<details>
<summary>摘要</summary>
保证安全和满足时间要求是长期 робо控制任务中的关键挑战。信号时间逻辑（STL）已广泛应用于系统地和可靠地 especify这些要求。然而，传统的控制策略找到方法是计算复杂且不可扩展到高维度或具有复杂非线性动态系统。 reinforcement learning（RL）方法可以通过手工或STL-inspired奖励学习策略满足STL要求，但可能会遇到不预期的行为 Due to ambiguity and sparsity in the reward.在本文中，我们提出了一种方法，可以直接学习一个神经网络控制器，满足STL要求。我们的控制器在训练中寻找最大STLRobustness分数的轨迹。在测试中，类似于预测控制（MPC），我们学习的控制器预测一个在规划阶段内的轨迹，以确保STL要求的满足在部署中。为确保安全性，我们设计了一个备份策略。我们的方法可以适应不同的初始状态和环境参数。我们在六个任务上进行了实验，其中我们的方法与备份策略比 класси方法（MPC、STL-solver）、模型自由和模型基于RL方法在STL满足率方面表现出色，尤其是在复杂的STL要求下，并且在10X-100X快于класси方法。
</details></li>
</ul>
<hr>
<h2 id="The-online-learning-architecture-with-edge-computing-for-high-level-control-for-assisting-patients"><a href="#The-online-learning-architecture-with-edge-computing-for-high-level-control-for-assisting-patients" class="headerlink" title="The online learning architecture with edge computing for high-level control for assisting patients"></a>The online learning architecture with edge computing for high-level control for assisting patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05130">http://arxiv.org/abs/2309.05130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Shi, Yihui Zhao<br>for: 这篇研究旨在提高依靠轮椅或其他移动障碍的人士的 mobilty 和 rehabilitation，通过将线上对抗学习架构与边缘计算结合，提高控制精度和适应性。methods: 本研究使用了线上对抗学习架构，处理用户的感应数据，并与边缘计算结合，以实现高级的控制精度和适应性。results: 实验结果显示，该架构可以提高控制精度和适应性，同时也提高了服务质量（QoS）指标。这些结果显示，将线上对抗学习架构与边缔计算结合，可以提供下一代更加稳定和高效的轮椅控制系统。<details>
<summary>Abstract</summary>
The prevalence of mobility impairments due to conditions such as spinal cord injuries, strokes, and degenerative diseases is on the rise globally. Lower-limb exoskeletons have been increasingly recognized as a viable solution for enhancing mobility and rehabilitation for individuals with such impairments. However, existing exoskeleton control systems often suffer from limitations such as latency, lack of adaptability, and computational inefficiency. To address these challenges, this paper introduces a novel online adversarial learning architecture integrated with edge computing for high-level lower-limb exoskeleton control. In the proposed architecture, sensor data from the user is processed in real-time through edge computing nodes, which then interact with an online adversarial learning model. This model adapts to the user's specific needs and controls the exoskeleton with minimal latency. Experimental evaluations demonstrate significant improvements in control accuracy and adaptability, as well as enhanced quality-of-service (QoS) metrics. These findings indicate that the integration of online adversarial learning with edge computing offers a robust and efficient approach for the next generation of lower-limb exoskeleton control systems.
</details>
<details>
<summary>摘要</summary>
全球的 mobililty 障碍情况（如脊梁创伤、中风和逐渐恶化的疾病）正在增加。 Lower-limb exoskeletons 已被认为是为患有这些障碍的人员提供较好的 mobililty 和重abilitation 解决方案。然而，现有的 exoskeleton 控制系统经常受到如lag、缺乏适应性和计算不充分的限制。为了解决这些挑战，这篇论文提出了一种基于 online adversarial learning 架构的高级 lower-limb exoskeleton 控制系统。在该架构中，用户的感知数据在实时通过边缘计算节点处理，然后与在线 adversarial learning 模型交互。该模型适应用户特定需求，控制 exoskeleton  WITH 最小延迟。实验评估显示，该架构可以提高控制精度和适应性，以及提高服务质量（QoS）指标。这些发现表明，将 online adversarial learning 与边缘计算结合使用可以提供下一代 lower-limb exoskeleton 控制系统的稳定和高效的解决方案。
</details></li>
</ul>
<hr>
<h2 id="A-compendium-of-data-sources-for-data-science-machine-learning-and-artificial-intelligence"><a href="#A-compendium-of-data-sources-for-data-science-machine-learning-and-artificial-intelligence" class="headerlink" title="A compendium of data sources for data science, machine learning, and artificial intelligence"></a>A compendium of data sources for data science, machine learning, and artificial intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05682">http://arxiv.org/abs/2309.05682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Bilokon, Oleksandr Bilokon, Saeed Amen</li>
<li>for: 提供数据科学、机器学习和人工智能领域的数据源列表，帮助数据科学家和机器学习专家在不同领域进行数据处理和分析。</li>
<li>methods: 列举了多种数据源，包括金融和经济、法律（法律和规则）、生命科学（医学和药物发现）、新闻情感和社交媒体、零售和电商、卫星影像和运输和供应链，以及体育等多个领域。</li>
<li>results: 提供了一个不完全的、但是广泛的数据源列表，可以帮助数据科学家和机器学习专家在不同领域进行数据处理和分析。<details>
<summary>Abstract</summary>
Recent advances in data science, machine learning, and artificial intelligence, such as the emergence of large language models, are leading to an increasing demand for data that can be processed by such models. While data sources are application-specific, and it is impossible to produce an exhaustive list of such data sources, it seems that a comprehensive, rather than complete, list would still benefit data scientists and machine learning experts of all levels of seniority. The goal of this publication is to provide just such an (inevitably incomplete) list -- or compendium -- of data sources across multiple areas of applications, including finance and economics, legal (laws and regulations), life sciences (medicine and drug discovery), news sentiment and social media, retail and ecommerce, satellite imagery, and shipping and logistics, and sports.
</details>
<details>
<summary>摘要</summary>
近期的数据科学、机器学习和人工智能技术的发展，如大语言模型的出现，导致了数据处理这些模型所需的数据的增加需求。尽管数据来源是应用场景特定的，并且无法制作一个完整的列表，但一个完整的列表仍然可以为数据科学家和机器学习专家们提供帮助。本文的目标是提供一个（必然不完整的）列表——或者笔记——的多个应用领域的数据源，包括金融和经济、法律（法律和规则）、生命科学（医学和药物发现）、新闻情感和社交媒体、零售和电商、卫星图像和运输和供应链、体育等。
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Granger-Causality-using-Kernel-Ridge-Regression"><a href="#Nonlinear-Granger-Causality-using-Kernel-Ridge-Regression" class="headerlink" title="Nonlinear Granger Causality using Kernel Ridge Regression"></a>Nonlinear Granger Causality using Kernel Ridge Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05107">http://arxiv.org/abs/2309.05107</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WojtekFulmyk/mlcausality-krr-paper-replication">https://github.com/WojtekFulmyk/mlcausality-krr-paper-replication</a></li>
<li>paper_authors: Wojciech “Victor” Fulmyk<br>for:mlcausality is designed for the identification of nonlinear Granger causal relationships.methods:mlcausality uses a flexible plug-in architecture that enables researchers to employ any nonlinear regressor as the base prediction model, and the kernel ridge regressor with the radial basis function kernel is used in the performance analysis.results:mlcausality with kernel ridge regression achieves competitive AUC scores, more finely calibrated $p$-values, and significantly reduced computation times compared to existing nonlinear Granger causality algorithms.<details>
<summary>Abstract</summary>
I introduce a novel algorithm and accompanying Python library, named mlcausality, designed for the identification of nonlinear Granger causal relationships. This novel algorithm uses a flexible plug-in architecture that enables researchers to employ any nonlinear regressor as the base prediction model. Subsequently, I conduct a comprehensive performance analysis of mlcausality when the prediction regressor is the kernel ridge regressor with the radial basis function kernel. The results demonstrate that mlcausality employing kernel ridge regression achieves competitive AUC scores across a diverse set of simulated data. Furthermore, mlcausality with kernel ridge regression yields more finely calibrated $p$-values in comparison to rival algorithms. This enhancement enables mlcausality to attain superior accuracy scores when using intuitive $p$-value-based thresholding criteria. Finally, mlcausality with the kernel ridge regression exhibits significantly reduced computation times compared to existing nonlinear Granger causality algorithms. In fact, in numerous instances, this innovative approach achieves superior solutions within computational timeframes that are an order of magnitude shorter than those required by competing algorithms.
</details>
<details>
<summary>摘要</summary>
我介绍了一种新的算法和Python库，名为mlcausality，用于非线性格兰GER causal关系的标识。这种新算法使用 flexible插件体系，允许研究人员使用任何非线性预测模型作为基础预测模型。我进行了对mlcausality使用kernelridge回归的完整性分析。结果显示，mlcausality使用kernelridge回归可以在多种模拟数据集中实现竞争力强的AUC分数。此外，mlcausality使用kernelridge回归可以获得更细化的$p$-值，与竞争算法相比，这种提升使mlcausality在使用直观的$p$-值基于的阈值设置中实现更高的准确率。最后，mlcausality使用kernelridge回归显示出了与现有的非线性格兰GER causal关系算法相比明显减少的计算时间。实际上，在许多情况下，这种创新方法可以在计算时间framworks中实现更高的准确率，并且在许多情况下，计算时间只需一个数量级更短。
</details></li>
</ul>
<hr>
<h2 id="Convex-Q-Learning-in-a-Stochastic-Environment-Extended-Version"><a href="#Convex-Q-Learning-in-a-Stochastic-Environment-Extended-Version" class="headerlink" title="Convex Q Learning in a Stochastic Environment: Extended Version"></a>Convex Q Learning in a Stochastic Environment: Extended Version</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05105">http://arxiv.org/abs/2309.05105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Lu, Sean Meyn</li>
<li>for: 这篇论文是为了描述一种基于函数近似的凸Q学习算法，用于解决Markov决策过程中的优化问题。</li>
<li>methods: 论文使用了一种凸 програм的缓和形式来描述优化问题，并利用了Manne所提出的线性Programming中的优化问题的双重relaxation。</li>
<li>results: 论文的主要贡献包括：首先，对凸程序的缓和形式进行了研究，并确定了这种缓和形式的解是有界的，而且与标准Q学习的解有着直接的关系。其次，提出了一种直接基于凸程序的Q学习算法，该算法具有跟随性和稳定性。最后，该论文通过应用于一个经典的存储控制问题来证明其理论。<details>
<summary>Abstract</summary>
The paper introduces the first formulation of convex Q-learning for Markov decision processes with function approximation. The algorithms and theory rest on a relaxation of a dual of Manne's celebrated linear programming characterization of optimal control. The main contributions firstly concern properties of the relaxation, described as a deterministic convex program: we identify conditions for a bounded solution, and a significant relationship between the solution to the new convex program, and the solution to standard Q-learning. The second set of contributions concern algorithm design and analysis: (i) A direct model-free method for approximating the convex program for Q-learning shares properties with its ideal. In particular, a bounded solution is ensured subject to a simple property of the basis functions; (ii) The proposed algorithms are convergent and new techniques are introduced to obtain the rate of convergence in a mean-square sense; (iii) The approach can be generalized to a range of performance criteria, and it is found that variance can be reduced by considering ``relative'' dynamic programming equations; (iv) The theory is illustrated with an application to a classical inventory control problem.
</details>
<details>
<summary>摘要</summary>
文章介绍了首次使用凸Q学习来解决Markov决策过程中的函数近似问题。算法和理论基于一种缓和的Manne所著名的线性 программирование характеристика优化控制的双射relaxation。文章的主要贡献包括：1. 凸 програм的relaxation属性的研究：我们确定了解决方案的 boundednessconditions和凸程学习解的解决方案之间的重要关系。2. 算法设计和分析：a. 直接使用凸程学习的模型自由方法来approximate凸程学习问题，这种方法具有一定的理论保证和实践优势。b. 提出了一种新的证明方法来证明算法的收敛性，并在mean-square意义上获得了速率收敛的结论。c. 将方法推广到多种性能标准，并发现可以通过考虑“相对”的动态Programming方程来降低干扰。3. 应用于一个 классиical的存储控制问题，以 illustrate the theory。Here's the text in Traditional Chinese:文章介绍了首次使用凸Q学习来解决Markov决策过程中的函数近似问题。算法和理论基于一种缓和的Manne所著名的线性程式学 caracterization优化控制的双射relaxation。文章的主要贡献包括：1. 凸程学的relaxation属性的研究：我们确定了解决方案的boundednessconditions和凸程学习解的解决方案之间的重要关系。2. 算法设计和分析：a. 直接使用凸程学习的模型自由方法来approximate凸程学习问题，这种方法具有一定的理论保证和实践优势。b. 提出了一种新的证明方法来证明算法的收敛性，并在mean-square意义上获得了速率收敛的结论。c. 将方法推广到多种性能标准，并发现可以通过考虑“相对”的动态Programming方程来降低干扰。3. 应用于一个 классиical的存储控制问题，以 illustrate the theory。
</details></li>
</ul>
<hr>
<h2 id="Is-Learning-in-Biological-Neural-Networks-based-on-Stochastic-Gradient-Descent-An-analysis-using-stochastic-processes"><a href="#Is-Learning-in-Biological-Neural-Networks-based-on-Stochastic-Gradient-Descent-An-analysis-using-stochastic-processes" class="headerlink" title="Is Learning in Biological Neural Networks based on Stochastic Gradient Descent? An analysis using stochastic processes"></a>Is Learning in Biological Neural Networks based on Stochastic Gradient Descent? An analysis using stochastic processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05102">http://arxiv.org/abs/2309.05102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sören Christensen, Jan Kallsen</li>
<li>for: 这个论文研究了生物神经网络（BNN）学习的不同方式。</li>
<li>methods: 这篇论文使用了一种抽象的概率模型来研究BNN的supervised学习。</li>
<li>results: 研究结果表明，在每个学习机会中，多个本地更新会导致一步梯度下降。这个结果表明，抽象梯度下降可能是优化BNN的一种方法。<details>
<summary>Abstract</summary>
In recent years, there has been an intense debate about how learning in biological neural networks (BNNs) differs from learning in artificial neural networks. It is often argued that the updating of connections in the brain relies only on local information, and therefore a stochastic gradient-descent type optimization method cannot be used. In this paper, we study a stochastic model for supervised learning in BNNs. We show that a (continuous) gradient step occurs approximately when each learning opportunity is processed by many local updates. This result suggests that stochastic gradient descent may indeed play a role in optimizing BNNs.
</details>
<details>
<summary>摘要</summary>
近年来，有一些研究人员就是否学习在生物神经网络（BNN）中与人工神经网络（ANN）的学习方式不同。一般认为，大脑中 Connection 的更新只凭借本地信息，因此不能使用抽象的梯度下降类似方法进行优化。在这篇论文中，我们研究了一种 Stochastic 的超参数优化方法，并证明在每个学习机会中，许多本地更新后，梯度步进行了约束。这一结论表明，梯度下降可能在 BNN 中发挥作用。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-conformal-classification-with-noisy-labels"><a href="#Adaptive-conformal-classification-with-noisy-labels" class="headerlink" title="Adaptive conformal classification with noisy labels"></a>Adaptive conformal classification with noisy labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05092">http://arxiv.org/abs/2309.05092</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/msesia/conformal-label-noise">https://github.com/msesia/conformal-label-noise</a></li>
<li>paper_authors: Matteo Sesia, Y. X. Rachel Wang, Xin Tong</li>
<li>for: 这篇论文是为了提出一种新的整形预测方法，用于资料分类任务中，能够自动适应几率随机标签污染的检测样本，实现更加详细的预测集和强化的覆盖保证。</li>
<li>methods: 这篇论文使用了一种新的整形预测方法，具有更强的覆盖保证和更加具体的理论基础，可以处理不同的标签污染过程，并不需要知道数据分布或机器学习分类器的内部运作。</li>
<li>results: 根据实验和应用到CIFAR-10H图像数据集，这篇论文的提案方法能够实现更加详细的预测集和强化的覆盖保证，比前一代方法更为有力。<details>
<summary>Abstract</summary>
This paper develops novel conformal prediction methods for classification tasks that can automatically adapt to random label contamination in the calibration sample, enabling more informative prediction sets with stronger coverage guarantees compared to state-of-the-art approaches. This is made possible by a precise theoretical characterization of the effective coverage inflation (or deflation) suffered by standard conformal inferences in the presence of label contamination, which is then made actionable through new calibration algorithms. Our solution is flexible and can leverage different modeling assumptions about the label contamination process, while requiring no knowledge about the data distribution or the inner workings of the machine-learning classifier. The advantages of the proposed methods are demonstrated through extensive simulations and an application to object classification with the CIFAR-10H image data set.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-supervised-generative-optimization-approach-for-tabular-data"><a href="#A-supervised-generative-optimization-approach-for-tabular-data" class="headerlink" title="A supervised generative optimization approach for tabular data"></a>A supervised generative optimization approach for tabular data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05079">http://arxiv.org/abs/2309.05079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fadi Hamad, Shinpei Nakamura-Sakai, Saheed Obitayo, Vamsi K. Potluru</li>
<li>for: This paper is written for financial institutions to address the challenges of synthetic data generation, specifically in the context of privacy protection and data augmentation.</li>
<li>methods: The proposed framework integrates a supervised component tailored to the specific downstream task and employs a meta-learning approach to learn the optimal mixture distribution of existing synthetic distributions.</li>
<li>results: The paper presents a novel synthetic data generation framework that can generate high-quality synthetic data tailored to specific downstream tasks, achieving better performance than existing unsupervised methods.<details>
<summary>Abstract</summary>
Synthetic data generation has emerged as a crucial topic for financial institutions, driven by multiple factors, such as privacy protection and data augmentation. Many algorithms have been proposed for synthetic data generation but reaching the consensus on which method we should use for the specific data sets and use cases remains challenging. Moreover, the majority of existing approaches are ``unsupervised'' in the sense that they do not take into account the downstream task. To address these issues, this work presents a novel synthetic data generation framework. The framework integrates a supervised component tailored to the specific downstream task and employs a meta-learning approach to learn the optimal mixture distribution of existing synthetic distributions.
</details>
<details>
<summary>摘要</summary>
<lang=zh-CN>现代数据生成技术在金融机构中得到了广泛应用，这是由多种因素引起的，如隐私保护和数据增强。许多算法已经为现代数据生成提出了多种方法，但是具体选择哪种方法用于特定数据集和用例仍然是一项挑战。此外，大多数现有的方法是“无监督的”，即它们不考虑特定下游任务的需求。为解决这些问题，本文提出了一种新的数据生成框架。该框架具有特定下游任务的监督组件，并使用元学习方法学习最佳混合分布。</lang></sys>Note: The above text is translated into Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need the text to be translated into Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Generalization-error-bounds-for-iterative-learning-algorithms-with-bounded-updates"><a href="#Generalization-error-bounds-for-iterative-learning-algorithms-with-bounded-updates" class="headerlink" title="Generalization error bounds for iterative learning algorithms with bounded updates"></a>Generalization error bounds for iterative learning algorithms with bounded updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05077">http://arxiv.org/abs/2309.05077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingwen Fu, Nanning Zheng</li>
<li>for: 这paper explores the generalization characteristics of iterative learning algorithms with bounded updates for non-convex loss functions, using information-theoretic techniques.</li>
<li>methods: 该paper使用了新的 bound for the generalization error of these algorithms with bounded updates, extending beyond the scope of previous works that only focused on Stochastic Gradient Descent (SGD). Our approach introduces two main novelties: 1) we reformulate the mutual information as the uncertainty of updates, providing a new perspective, and 2) instead of using the chaining rule of mutual information, we employ a variance decomposition technique to decompose information across iterations, allowing for a simpler surrogate process.</li>
<li>results: 我们分析了我们的总化 bound under various settings, and demonstrate improved bounds when the model dimension increases at the same rate as the number of training data samples. We also examine the previously observed scaling behavior in large language models, bridging the gap between theory and practice. Ultimately, our work takes a further step for developing practical generalization theories.<details>
<summary>Abstract</summary>
This paper explores the generalization characteristics of iterative learning algorithms with bounded updates for non-convex loss functions, employing information-theoretic techniques. Our key contribution is a novel bound for the generalization error of these algorithms with bounded updates, extending beyond the scope of previous works that only focused on Stochastic Gradient Descent (SGD). Our approach introduces two main novelties: 1) we reformulate the mutual information as the uncertainty of updates, providing a new perspective, and 2) instead of using the chaining rule of mutual information, we employ a variance decomposition technique to decompose information across iterations, allowing for a simpler surrogate process. We analyze our generalization bound under various settings and demonstrate improved bounds when the model dimension increases at the same rate as the number of training data samples. To bridge the gap between theory and practice, we also examine the previously observed scaling behavior in large language models. Ultimately, our work takes a further step for developing practical generalization theories.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>We reformulate the mutual information as a measure of the uncertainty of updates, providing a new perspective.2. Instead of using the chaining rule of mutual information, we use a variance decomposition technique to decompose information across iterations, allowing for a simpler surrogate process.We analyze our generalization bound under various settings and show that it improves as the model dimension increases at the same rate as the number of training data samples. To demonstrate the practicality of our theory, we also examine the scaling behavior of large language models. Our work takes another step towards developing practical generalization theories.Translated into Simplified Chinese:这篇论文研究了对非对称损失函数的迭代学习算法的泛化特性，使用信息理论技术。我们的主要贡献是对这些算法的 bounded updates 的泛化误差 bound，超出了先前的工作仅考虑 Stochastic Gradient Descent (SGD) 的情况。我们的方法引入了两个主要新特点：1. 我们将相互信息重新表述为更新的不确定度，提供了新的视角。2. 而不是使用链式规则，我们使用归一化分解技术来分解迭代过程中的信息，allowing for a simpler surrogate process。我们在不同的设置下分析了我们的泛化 bound，并证明在模型维度与训练样本数量相同增长的情况下，我们的 bound 会提高。为了将理论与实践相连，我们还对大型自然语言模型的征性行为进行了研究。我们的工作又一步向发展实用泛化理论。</details></li>
</ol>
<hr>
<h2 id="Spatiotemporal-Graph-Neural-Networks-with-Uncertainty-Quantification-for-Traffic-Incident-Risk-Prediction"><a href="#Spatiotemporal-Graph-Neural-Networks-with-Uncertainty-Quantification-for-Traffic-Incident-Risk-Prediction" class="headerlink" title="Spatiotemporal Graph Neural Networks with Uncertainty Quantification for Traffic Incident Risk Prediction"></a>Spatiotemporal Graph Neural Networks with Uncertainty Quantification for Traffic Incident Risk Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05072">http://arxiv.org/abs/2309.05072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sttdanonymous/sttd">https://github.com/sttdanonymous/sttd</a></li>
<li>paper_authors: Xiaowei Gao, Xinke Jiang, Dingyi Zhuang, Huanfa Chen, Shenhao Wang, James Haworth</li>
<li>for: 预测交通事故风险在细致空间时间层面是一项挑战。现有 datasets 主要具有 zero 值，表示没有事故，而 occasional high-risk 值则表示严重事故。</li>
<li>methods: 我们引入了 Spatiotemporal Zero-Inflated Tweedie Graph Neural Networks (STZITD-GNNs) 模型，它将传统统计模型的可靠性与 Graph Neural Networks 的灵活性结合起来，以准确量化交通事故风险的不确定性。</li>
<li>results: 我们的模型可以准确地捕捉 datasets 的倾斜分布，并强调罕见但具有深见的严重事故。实际测试使用了伦敦、英国的实际交通数据，表明我们的模型在短（7天）和长（14天）时间层面上都能够超越当前标准准。<details>
<summary>Abstract</summary>
Predicting traffic incident risks at granular spatiotemporal levels is challenging. The datasets predominantly feature zero values, indicating no incidents, with sporadic high-risk values for severe incidents. Notably, a majority of current models, especially deep learning methods, focus solely on estimating risk values, overlooking the uncertainties arising from the inherently unpredictable nature of incidents. To tackle this challenge, we introduce the Spatiotemporal Zero-Inflated Tweedie Graph Neural Networks (STZITD-GNNs). Our model merges the reliability of traditional statistical models with the flexibility of graph neural networks, aiming to precisely quantify uncertainties associated with road-level traffic incident risks. This model strategically employs a compound model from the Tweedie family, as a Poisson distribution to model risk frequency and a Gamma distribution to account for incident severity. Furthermore, a zero-inflated component helps to identify the non-incident risk scenarios. As a result, the STZITD-GNNs effectively capture the dataset's skewed distribution, placing emphasis on infrequent but impactful severe incidents. Empirical tests using real-world traffic data from London, UK, demonstrate that our model excels beyond current benchmarks. The forte of STZITD-GNN resides not only in its accuracy but also in its adeptness at curtailing uncertainties, delivering robust predictions over short (7 days) and extended (14 days) timeframes.
</details>
<details>
<summary>摘要</summary>
传统的方法难以预测交通事故风险的细致空时分布，主要 dataset 中具有零值，表示没有事故，只有罕见的高风险值表示严重事故。现有许多模型，特别是深度学习方法，强调估计风险值，忽略了事故的不可预测性。为解决这个挑战，我们介绍了 Spatiotemporal Zero-Inflated Tweedie Graph Neural Networks (STZITD-GNNs) 模型。我们的模型结合了传统统计模型的可靠性和图神经网络的灵活性，以准确量化交通事故风险的不确定性。我们的模型采用 Tweedie 家族的复合模型，其中 Poisson 分布模型风险频率，而 Gamma 分布模型负责事故严重程度。此外，我们还添加了一个零值扩展组件，以便标识非事故风险场景。因此，STZITD-GNNs 能够有效捕捉数据的偏斜分布，强调罕见而具有深见的严重事故。我们对实际的伦敦交通数据进行了实践测试，结果表明，STZITD-GNNs 在短（7天）和长（14天）时间尺度上都能够准确预测交通事故风险。
</details></li>
</ul>
<hr>
<h2 id="Mutation-based-Fault-Localization-of-Deep-Neural-Networks"><a href="#Mutation-based-Fault-Localization-of-Deep-Neural-Networks" class="headerlink" title="Mutation-based Fault Localization of Deep Neural Networks"></a>Mutation-based Fault Localization of Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05067">http://arxiv.org/abs/2309.05067</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ali-ghanbari/deepmufl-ase-2023">https://github.com/ali-ghanbari/deepmufl-ase-2023</a></li>
<li>paper_authors: Ali Ghanbari, Deepak-George Thomas, Muhammad Arbab Arshad, Hridesh Rajan</li>
<li>for: 这篇论文旨在提高深度神经网络（DNN）系统的可靠性，特别是在安全关键领域。</li>
<li>methods: 该论文提出了一种新的技术——深度缺陷定位（deepmufl），可以广泛应用于不同的DNN模型。</li>
<li>results: 该研究使用了Stack Overflow上的109个bug进行评估，结果显示，深度缺陷定位可以检测53&#x2F;109个bug，比其他静态和动态DNN缺陷定位系统更高效。此外，通过选择突变，可以在预训练模型上减少缺陷定位时间，仅失去7.55%的缺陷。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are susceptible to bugs, just like other types of software systems. A significant uptick in using DNN, and its applications in wide-ranging areas, including safety-critical systems, warrant extensive research on software engineering tools for improving the reliability of DNN-based systems. One such tool that has gained significant attention in the recent years is DNN fault localization. This paper revisits mutation-based fault localization in the context of DNN models and proposes a novel technique, named deepmufl, applicable to a wide range of DNN models. We have implemented deepmufl and have evaluated its effectiveness using 109 bugs obtained from StackOverflow. Our results show that deepmufl detects 53/109 of the bugs by ranking the buggy layer in top-1 position, outperforming state-of-the-art static and dynamic DNN fault localization systems that are also designed to target the class of bugs supported by deepmufl. Moreover, we observed that we can halve the fault localization time for a pre-trained model using mutation selection, yet losing only 7.55% of the bugs localized in top-1 position.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Federated-Learning-Incentive-Mechanism-under-Buyers’-Auction-Market"><a href="#Federated-Learning-Incentive-Mechanism-under-Buyers’-Auction-Market" class="headerlink" title="Federated Learning Incentive Mechanism under Buyers’ Auction Market"></a>Federated Learning Incentive Mechanism under Buyers’ Auction Market</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05063">http://arxiv.org/abs/2309.05063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxi Yang, Zihao Guo, Sheng Cao, Cuifang Zhao, Li-Chuan Tsai</li>
<li>For: The paper explores the concept of Auction-based Federated Learning (AFL) in a buyers’ market, where there is an increasing number of qualified clients capable of performing federated learning tasks.* Methods: The paper adapts the procurement auction framework to explain the pricing behavior under a buyers’ market. The authors also utilize a blockchain-based reputation mechanism to select clients with high reliability and data quality, and to prevent external attacks.* Results: The experimental results validate the effectiveness of the approach.Here is the simplified Chinese text for the three key points:* 为：AFL在买方市场下进行研究，随着技术的进步，有越来越多的合格客户能够完成联合学习任务。* 方法：通过修改购买拍卖框架，解释在买方市场下的价格行为。同时，通过使用区块链技术实现的声誉机制，选择高可靠性和数据质量的客户，并防止外部攻击。* 结果：实验结果证明了方法的有效性。<details>
<summary>Abstract</summary>
Auction-based Federated Learning (AFL) enables open collaboration among self-interested data consumers and data owners. Existing AFL approaches are commonly under the assumption of sellers' market in that the service clients as sellers are treated as scarce resources so that the aggregation servers as buyers need to compete the bids. Yet, as the technology progresses, an increasing number of qualified clients are now capable of performing federated learning tasks, leading to shift from sellers' market to a buyers' market. In this paper, we shift the angle by adapting the procurement auction framework, aiming to explain the pricing behavior under buyers' market. Our modeling starts with basic setting under complete information, then move further to the scenario where sellers' information are not fully observable. In order to select clients with high reliability and data quality, and to prevent from external attacks, we utilize a blockchain-based reputation mechanism. The experimental results validate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Machine-Learning-for-maximizing-the-memristivity-of-single-and-coupled-quantum-memristors"><a href="#Machine-Learning-for-maximizing-the-memristivity-of-single-and-coupled-quantum-memristors" class="headerlink" title="Machine Learning for maximizing the memristivity of single and coupled quantum memristors"></a>Machine Learning for maximizing the memristivity of single and coupled quantum memristors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05062">http://arxiv.org/abs/2309.05062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Hernani-Morales, Gabriel Alvarado, Francisco Albarrán-Arriagada, Yolanda Vives-Gilabert, Enrique Solano, José D. Martín-Guerrero</li>
<li>for: 用机器学习方法描述单个和连接的量子记忆器的吸收性质。</li>
<li>methods: 使用机器学习方法对量子记忆器进行描述。</li>
<li>results: 结果表明，在提高吸收性时，两个量子记忆器的互相关系强度增加，这反映了量子相关性和记忆之间的密切关系。这些结果支持使用量子记忆器作为neuromorphic量子计算的关键组件。<details>
<summary>Abstract</summary>
We propose machine learning (ML) methods to characterize the memristive properties of single and coupled quantum memristors. We show that maximizing the memristivity leads to large values in the degree of entanglement of two quantum memristors, unveiling the close relationship between quantum correlations and memory. Our results strengthen the possibility of using quantum memristors as key components of neuromorphic quantum computing.
</details>
<details>
<summary>摘要</summary>
我们提议使用机器学习（ML）方法来描述单个和连接的量子memristor的memristive性。我们发现，为了 maximize memristivity，两个量子memristor的共振度会增加，这表明量子相关性和记忆之间存在紧密的关系。我们的结果证明了使用量子memristor作为 neuromorphic 量子计算的关键组件的可能性。Here's the breakdown of the translation:* 我们 (wǒmen) - we* 提议 (tīyì) - propose* 使用 (fùyòu) - use* 机器学习 (jīshì xuéxí) - machine learning* 方法 (fāngzhì) - methods* 来描述 (lái bǐngmiào) - to describe* 单个 (danwei) - single* 和连接 (hé liánjiāo) - and coupled* 量子 memristor (liàngzi memristor) - quantum memristors* 的 memristive (de memristive) - of memristive* 性 (xìng) - properties* 我们 (wǒmen) - we* 发现 (fāxìn) - find* 为了 (wèile) - in order to*  maximize (màximize) - maximize* memristivity (memristivity) - memristivity* 会 (huì) - will* 增加 (zēngjia) - increase* 两个 (liǎngge) - two* 量子 memristor (liàngzi memristor) - quantum memristors* 的共振度 (de gòngzhòngdòu) - of the degree of entanglement* 会 (huì) - will* 增加 (zēngjia) - increase* 证明 (zhèngmíng) - prove* 使用 (fùyòu) - use* 量子 memristor (liàngzi memristor) - quantum memristors* 作为 (zuòwèi) - as* 关键 (guānjī) - key* 组件 (zuòyī) - component* 的可能性 (de kěnéngxìng) - of the possibilityI hope this helps! Let me know if you have any other questions.
</details></li>
</ul>
<hr>
<h2 id="SA-Solver-Stochastic-Adams-Solver-for-Fast-Sampling-of-Diffusion-Models"><a href="#SA-Solver-Stochastic-Adams-Solver-for-Fast-Sampling-of-Diffusion-Models" class="headerlink" title="SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models"></a>SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05019">http://arxiv.org/abs/2309.05019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, Zhi-Ming Ma</li>
<li>for: 这项研究的目的是提出高效的批处理方法，以解决Diffusion Probabilistic Models（DPMs）的批处理问题。</li>
<li>methods: 该文章使用了两种方法进行比较：variance-controlled diffusion SDE和linear multi-step SDE solver。</li>
<li>results: 该文章的实验结果表明，SA-Solver可以 achieve improved or comparable performance compared with现有的 sampling方法，并且在适当的函数评估数（NFEs）下达到State-of-the-Art（SOTA）的FID scores。<details>
<summary>Abstract</summary>
Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE or ODE which is time-consuming, numerous fast sampling methods built upon improved differential equation solvers are proposed. The majority of such techniques consider solving the diffusion ODE due to its superior efficiency. However, stochastic sampling could offer additional advantages in generating diverse and high-quality data. In this work, we engage in a comprehensive analysis of stochastic sampling from two aspects: variance-controlled diffusion SDE and linear multi-step SDE solver. Based on our analysis, we propose SA-Solver, which is an improved efficient stochastic Adams method for solving diffusion SDE to generate data with high quality. Our experiments show that SA-Solver achieves: 1) improved or comparable performance compared with the existing state-of-the-art sampling methods for few-step sampling; 2) SOTA FID scores on substantial benchmark datasets under a suitable number of function evaluations (NFEs).
</details>
<details>
<summary>摘要</summary>
Diffusion Probabilistic Models (DPMs) 已经取得了许多成功在生成任务中。由于DPMs的采样是等价于解决扩散 diferencial equation（SDE）或ordinary differential equation（ODE），因此许多快速采样方法基于改进的导数方程解决器被提出。大多数这些技术是因为扩散ODE的更高效率而考虑。然而，随机采样可以提供额外的优点，如生成多样化和高质量的数据。在这项工作中，我们进行了扩散SDE采样的两个方面的全面分析： variance-controlled diffusion SDE和线性多步SDE解决器。基于我们的分析，我们提出了SA-Solver，它是一种改进的高效随机Adams方法，用于解决扩散SDE，以生成高质量的数据。我们的实验显示，SA-Solver可以：1）与现有状态arius sampling方法相比，在几步采样中实现改进或相似的性能；2）在适当的函数评估数（NFEs）下，在大量的benchmark数据集上实现SOTA FID分数。
</details></li>
</ul>
<hr>
<h2 id="Computational-Approaches-for-Predicting-Drug-Disease-Associations-A-Comprehensive-Review"><a href="#Computational-Approaches-for-Predicting-Drug-Disease-Associations-A-Comprehensive-Review" class="headerlink" title="Computational Approaches for Predicting Drug-Disease Associations: A Comprehensive Review"></a>Computational Approaches for Predicting Drug-Disease Associations: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06388">http://arxiv.org/abs/2309.06388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyan Ao, Zhichao Xiao, Lixin Guan, Liang Yu</li>
<li>for: 预测药物与疾病关系，以便为药物再定位提供计算机支持。</li>
<li>methods: 文章总结了最新的计算方法，包括神经网络基于算法、矩阵基于算法、推荐算法、链接基于理解算法和文本挖掘与Semantic reasoning。</li>
<li>results: 文章对现有的药物与疾病关系预测算法进行了比较性分析，以评估它们的预测性能。<details>
<summary>Abstract</summary>
In recent decades, traditional drug research and development have been facing challenges such as high cost, long timelines, and high risks. To address these issues, many computational approaches have been suggested for predicting the relationship between drugs and diseases through drug repositioning, aiming to reduce the cost, development cycle, and risks associated with developing new drugs. Researchers have explored different computational methods to predict drug-disease associations, including drug side effects-disease associations, drug-target associations, and miRNAdisease associations. In this comprehensive review, we focus on recent advances in predicting drug-disease association methods for drug repositioning. We first categorize these methods into several groups, including neural network-based algorithms, matrixbased algorithms, recommendation algorithms, link-based reasoning algorithms, and text mining and semantic reasoning. Then, we compare the prediction performance of existing drug-disease association prediction algorithms. Lastly, we delve into the present challenges and future prospects concerning drug-disease associations.
</details>
<details>
<summary>摘要</summary>
近年来，传统的药物研究和开发遇到了高成本、长时间和高风险的挑战。为解决这些问题，许多计算方法被建议用于预测药物和疾病之间的关系，以减少开发新药物的成本、研发周期和风险。研究人员已经探索了不同的计算方法来预测药物疾病关系，包括药效副作用疾病关系、药物target关系和miRNA疾病关系。在本综述中，我们将关注最近的药物疾病关系预测方法的进展。我们首先将这些方法分为几个组，包括神经网络基本算法、矩阵基本算法、推荐算法、链接基本理解算法和文本挖掘和Semantic reasoning。然后，我们比较了现有的药物疾病关系预测算法的预测性能。最后，我们讨论了药物疾病关系的当前挑战和未来前途。
</details></li>
</ul>
<hr>
<h2 id="Machine-Translation-Models-Stand-Strong-in-the-Face-of-Adversarial-Attacks"><a href="#Machine-Translation-Models-Stand-Strong-in-the-Face-of-Adversarial-Attacks" class="headerlink" title="Machine Translation Models Stand Strong in the Face of Adversarial Attacks"></a>Machine Translation Models Stand Strong in the Face of Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06527">http://arxiv.org/abs/2309.06527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavel Burnyshev, Elizaveta Kostenok, Alexey Zaytsev</li>
<li>for: 本研究探讨了深度学习模型面对针对性攻击的漏洞，具体来说是seq2seq模型，特别是翻译模型。</li>
<li>methods: 我们提出了基于文本抖动规则和更高级的攻击策略，如梯度基于攻击，利用可导的翻译度量的拟合。</li>
<li>results: 我们的研究表明，翻译模型对已知最佳攻击方法 display robustness，输入输出抖动度直接相关。然而，在弱点方面，我们的攻击表现最佳，与其他选择相比。另一个强 candidate 是基于个体字符混合的攻击。<details>
<summary>Abstract</summary>
Adversarial attacks expose vulnerabilities of deep learning models by introducing minor perturbations to the input, which lead to substantial alterations in the output. Our research focuses on the impact of such adversarial attacks on sequence-to-sequence (seq2seq) models, specifically machine translation models. We introduce algorithms that incorporate basic text perturbation heuristics and more advanced strategies, such as the gradient-based attack, which utilizes a differentiable approximation of the inherently non-differentiable translation metric. Through our investigation, we provide evidence that machine translation models display robustness displayed robustness against best performed known adversarial attacks, as the degree of perturbation in the output is directly proportional to the perturbation in the input. However, among underdogs, our attacks outperform alternatives, providing the best relative performance. Another strong candidate is an attack based on mixing of individual characters.
</details>
<details>
<summary>摘要</summary>
“深度学习模型的敌对攻击暴露了它们的漏洞，通过对输入进行微小的修改，导致输出发生重大的变化。我们的研究关注于seq2seq模型，特别是翻译模型，面临敌对攻击的影响。我们提出了包括基本文本修饰规则和更高级的策略，如梯度基于攻击，利用可导的翻译度量的不等式。我们的调查发现，翻译模型在已知敌对攻击中表现出了坚固性，输入修饰的程度直接影响输出修饰的程度。然而，在弱者中，我们的攻击表现更好，提供了最好的相对性。另一个强 кандидат是基于个体字的混合攻击。”Note that Simplified Chinese is the official standard for Chinese writing in mainland China and Singapore, and it is used in this translation. Traditional Chinese is also widely used in other regions, such as Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Linear-Speedup-of-Incremental-Aggregated-Gradient-Methods-on-Streaming-Data"><a href="#Linear-Speedup-of-Incremental-Aggregated-Gradient-Methods-on-Streaming-Data" class="headerlink" title="Linear Speedup of Incremental Aggregated Gradient Methods on Streaming Data"></a>Linear Speedup of Incremental Aggregated Gradient Methods on Streaming Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04980">http://arxiv.org/abs/2309.04980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaolu Wang, Cheng Jin, Hoi-To Wai, Yuantao Gu</li>
<li>for: 这 paper 考虑了一种大规模分布式优化中的增量积累 градиент（IAG）方法。IAG 方法适合参数服务器架构，因为后者可以轻松地将可能已经过时的梯度集成。虽然 deterministic 梯度下 IAG 的连续性已经得到了广泛的研究，但只有很少研究了基于流动数据的随机变iante IAG 的情况。</li>
<li>methods: 本 paper 使用了 streaming IAG 方法，并对其在强拟合优化中的性能进行了分析。</li>
<li>results: 本 paper 显示，当工作者们更新频繁 enough，而且数据样本分布在工作者之间是不一致的时， streaming IAG 方法可以实现线性的速度减速。我们证明，预期平方差落后趋势随着（1+T）&#x2F;(nt)  decay，其中 n 是工作者数量，t 是迭代次数，T&#x2F;n 是工作者更新频率。我们的分析包括对受影响的 conditional expectations 和延迟和噪声项的精细处理，这些是 IAG 类型算法的分析中新的。数据证明了我们的发现。<details>
<summary>Abstract</summary>
This paper considers a type of incremental aggregated gradient (IAG) method for large-scale distributed optimization. The IAG method is well suited for the parameter server architecture as the latter can easily aggregate potentially staled gradients contributed by workers. Although the convergence of IAG in the case of deterministic gradient is well known, there are only a few results for the case of its stochastic variant based on streaming data. Considering strongly convex optimization, this paper shows that the streaming IAG method achieves linear speedup when the workers are updating frequently enough, even if the data sample distribution across workers are heterogeneous. We show that the expected squared distance to optimal solution decays at O((1+T)/(nt)), where $n$ is the number of workers, t is the iteration number, and T/n is the update frequency of workers. Our analysis involves careful treatments of the conditional expectations with staled gradients and a recursive system with both delayed and noise terms, which are new to the analysis of IAG-type algorithms. Numerical results are presented to verify our findings.
</details>
<details>
<summary>摘要</summary>
This paper focuses on strongly convex optimization and shows that the streaming IAG method achieves linear speedup when the workers update frequently enough, even if the data sample distribution across workers is heterogeneous. Our analysis takes into account careful treatments of conditional expectations with stale gradients and a recursive system with both delayed and noise terms, which are novel aspects of the analysis of IAG-type algorithms.The expected squared distance to the optimal solution decreases at a rate of O((1+T)/(nt)), where n is the number of workers, t is the iteration number, and T/n is the update frequency of workers. Numerical results are provided to validate our findings.
</details></li>
</ul>
<hr>
<h2 id="AVARS-–-Alleviating-Unexpected-Urban-Road-Traffic-Congestion-using-UAVs"><a href="#AVARS-–-Alleviating-Unexpected-Urban-Road-Traffic-Congestion-using-UAVs" class="headerlink" title="AVARS – Alleviating Unexpected Urban Road Traffic Congestion using UAVs"></a>AVARS – Alleviating Unexpected Urban Road Traffic Congestion using UAVs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04976">http://arxiv.org/abs/2309.04976</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guojyjy/avars">https://github.com/guojyjy/avars</a></li>
<li>paper_authors: Jiaying Guo, Michael R. Jones, Soufiene Djahel, Shen Wang</li>
<li>for: 实时监控交通情况并快速对应，以减少不可预测的城市交通塞车。</li>
<li>methods: 使用无人机（UAV）和深度强化学习（DRL）算法，实时监控交通情况，并对交通灯号进行快速控制。</li>
<li>results: 透过AVARS系统，在实际的 Dublin 交通模拟器上验证了UAV的可能性，并获得了有效地减少不可预测的交通塞车的结果。<details>
<summary>Abstract</summary>
Reducing unexpected urban traffic congestion caused by en-route events (e.g., road closures, car crashes, etc.) often requires fast and accurate reactions to choose the best-fit traffic signals. Traditional traffic light control systems, such as SCATS and SCOOT, are not efficient as their traffic data provided by induction loops has a low update frequency (i.e., longer than 1 minute). Moreover, the traffic light signal plans used by these systems are selected from a limited set of candidate plans pre-programmed prior to unexpected events' occurrence. Recent research demonstrates that camera-based traffic light systems controlled by deep reinforcement learning (DRL) algorithms are more effective in reducing traffic congestion, in which the cameras can provide high-frequency high-resolution traffic data. However, these systems are costly to deploy in big cities due to the excessive potential upgrades required to road infrastructure. In this paper, we argue that Unmanned Aerial Vehicles (UAVs) can play a crucial role in dealing with unexpected traffic congestion because UAVs with onboard cameras can be economically deployed when and where unexpected congestion occurs. Then, we propose a system called "AVARS" that explores the potential of using UAVs to reduce unexpected urban traffic congestion using DRL-based traffic light signal control. This approach is validated on a widely used open-source traffic simulator with practical UAV settings, including its traffic monitoring ranges and battery lifetime. Our simulation results show that AVARS can effectively recover the unexpected traffic congestion in Dublin, Ireland, back to its original un-congested level within the typical battery life duration of a UAV.
</details>
<details>
<summary>摘要</summary>
减少意外城市堵塞需要快速准确的反应，选择最佳的交通信号。传统的交通信号控制系统，如SCATS和SCOOT，不够高效，因为它们的交通数据由感测器提供，更新频率较低（大于1分钟）。此外，这些系统使用的交通信号计划是从先前预先编程的候选计划中选择的。现代研究表明，基于深度强化学习（DRL）算法控制的摄像头交通信号系统更有效地减少交通堵塞。然而，这些系统在大城市部署的成本较高，因为需要大量的道路基础设施升级。在这篇论文中，我们 argue that无人机（UAV）可以在意外交通堵塞时扮演关键角色。UAV带有摄像头可以经济性地部署在意外堵塞发生的地方和时候。然后，我们提出了一个系统 called "AVARS"，该系统利用UAV来减少意外城市堵塞。我们使用了一个广泛使用的开源交通模拟器，并使用实际的UAV设置，包括它的交通监测范围和电池寿命。我们的模拟结果表明，AVARS可以在都柏林、爱尔兰effectively recover意外交通堵塞，使其返回到原始无堵塞水平，这与UAV的电池寿命相符。
</details></li>
</ul>
<hr>
<h2 id="Continual-Robot-Learning-using-Self-Supervised-Task-Inference"><a href="#Continual-Robot-Learning-using-Self-Supervised-Task-Inference" class="headerlink" title="Continual Robot Learning using Self-Supervised Task Inference"></a>Continual Robot Learning using Self-Supervised Task Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04974">http://arxiv.org/abs/2309.04974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Burhan Hafez, Stefan Wermter</li>
<li>for: 这 paper 的目的是解决Robot学习中的一个开放问题，即让机器人学习一系列任务，而不是单一任务。</li>
<li>methods: 这 paper 使用了自我指导学习的方法，通过自组织的运动和效果部分的观察来学习行为和意图的嵌入。它还使用了一个高级行为嵌入，通过自组织的联合行为-意图嵌入来学习高级行为。</li>
<li>results: 这 paper 的结果表明，使用自我指导学习方法可以在机器人学习中提高性能，特别是在连续学习Setting下。它还可以从不完整的示范中推断任务，并且可以在一个示范中学习多个任务。<details>
<summary>Abstract</summary>
Endowing robots with the human ability to learn a growing set of skills over the course of a lifetime as opposed to mastering single tasks is an open problem in robot learning. While multi-task learning approaches have been proposed to address this problem, they pay little attention to task inference. In order to continually learn new tasks, the robot first needs to infer the task at hand without requiring predefined task representations. In this paper, we propose a self-supervised task inference approach. Our approach learns action and intention embeddings from self-organization of the observed movement and effect parts of unlabeled demonstrations and a higher-level behavior embedding from self-organization of the joint action-intention embeddings. We construct a behavior-matching self-supervised learning objective to train a novel Task Inference Network (TINet) to map an unlabeled demonstration to its nearest behavior embedding, which we use as the task representation. A multi-task policy is built on top of the TINet and trained with reinforcement learning to optimize performance over tasks. We evaluate our approach in the fixed-set and continual multi-task learning settings with a humanoid robot and compare it to different multi-task learning baselines. The results show that our approach outperforms the other baselines, with the difference being more pronounced in the challenging continual learning setting, and can infer tasks from incomplete demonstrations. Our approach is also shown to generalize to unseen tasks based on a single demonstration in one-shot task generalization experiments.
</details>
<details>
<summary>摘要</summary>
robot学习中的一个开放问题是让机器人学习一系列任务，而不是仅仅掌握单个任务。虽然多任务学习方法有所提出，但它们很少关注任务推理。为了不断学习新任务，机器人首先需要推理出当前任务，而无需定制任务表示。在这篇论文中，我们提出了一种自动推理任务方法。我们从无标示示例中自然地学习了动作和意图嵌入，以及高级行为嵌入。我们构建了一个行为匹配自动学习目标，用于训练一个新的任务推理网络（TINet），以将无标示示例映射到其最似的行为嵌入。在TINet的基础上，我们建立了一个多任务策略，通过强化学习来优化表现。我们在固定集和不断多任务学习设置中评估了我们的方法，并与不同的多任务学习基eline进行比较。结果表明，我们的方法在两个设置中都表现出优异，特别是在具有挑战性的连续学习设置中，并且可以从不完整的示例中推理出任务。我们的方法还在一个一遍学习任务的 экспериментах中表现出了一致性。
</details></li>
</ul>
<hr>
<h2 id="LMBiS-Net-A-Lightweight-Multipath-Bidirectional-Skip-Connection-based-CNN-for-Retinal-Blood-Vessel-Segmentation"><a href="#LMBiS-Net-A-Lightweight-Multipath-Bidirectional-Skip-Connection-based-CNN-for-Retinal-Blood-Vessel-Segmentation" class="headerlink" title="LMBiS-Net: A Lightweight Multipath Bidirectional Skip Connection based CNN for Retinal Blood Vessel Segmentation"></a>LMBiS-Net: A Lightweight Multipath Bidirectional Skip Connection based CNN for Retinal Blood Vessel Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04968">http://arxiv.org/abs/2309.04968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mufassir M. Abbasi, Shahzaib Iqbal, Asim Naveed, Tariq M. Khan, Syed S. Naqvi, Wajeeha Khalid</li>
<li>for: 这个论文是为了提出一种高效且准确的眼睛内部结构分割方法，用于诊断和治疗眼疾病。</li>
<li>methods: 这个方法使用了一种名为LMBiS-Net的轻量级像素级卷积神经网络，通过多路特征提取块和双向跳接来提高特征提取和信息传输。此外，通过精心选择筛子数量，以避免筛子重叠，进一步提高了模型的效率和计算效果。</li>
<li>results: 经过严格的测试和评估，LMBiS-Net模型表现出了高度的稳定性和普适性，能够准确地分割眼睛内部的血管结构，并且在不同类型的眼科图像上保持高度的分割精度。这些特点使LMBiS-Net模型成为一种高效且可靠的眼科图像分割工具。<details>
<summary>Abstract</summary>
Blinding eye diseases are often correlated with altered retinal morphology, which can be clinically identified by segmenting retinal structures in fundus images. However, current methodologies often fall short in accurately segmenting delicate vessels. Although deep learning has shown promise in medical image segmentation, its reliance on repeated convolution and pooling operations can hinder the representation of edge information, ultimately limiting overall segmentation accuracy. In this paper, we propose a lightweight pixel-level CNN named LMBiS-Net for the segmentation of retinal vessels with an exceptionally low number of learnable parameters \textbf{(only 0.172 M)}. The network used multipath feature extraction blocks and incorporates bidirectional skip connections for the information flow between the encoder and decoder. Additionally, we have optimized the efficiency of the model by carefully selecting the number of filters to avoid filter overlap. This optimization significantly reduces training time and enhances computational efficiency. To assess the robustness and generalizability of LMBiS-Net, we performed comprehensive evaluations on various aspects of retinal images. Specifically, the model was subjected to rigorous tests to accurately segment retinal vessels, which play a vital role in ophthalmological diagnosis and treatment. By focusing on the retinal blood vessels, we were able to thoroughly analyze the performance and effectiveness of the LMBiS-Net model. The results of our tests demonstrate that LMBiS-Net is not only robust and generalizable but also capable of maintaining high levels of segmentation accuracy. These characteristics highlight the potential of LMBiS-Net as an efficient tool for high-speed and accurate segmentation of retinal images in various clinical applications.
</details>
<details>
<summary>摘要</summary>
盲目疾病常与改变视网膜结构相关，可以在基于眼底图像的基本结构剖分中进行诊断。然而，现有方法经常无法准确分 segment 细胞。深度学习在医疗图像分 segmentation 中表现出了承诺，但是它的 rely  на重复 convolution 和 pooling 操作可能会阻碍缘 edge 信息的表达，从而限制总体 segmentation 精度。在这篇论文中，我们提出了一种轻量级像素级 CNN  named LMBiS-Net，用于眼底视网膜细胞分 segmentation，只有 0.172 M 个学习参数。该网络使用了多路特征提取块和双向跳过连接，以便在编码器和解码器之间进行信息流。此外，我们仔细选择了缘 filter 的数量，以避免缘 filter 的重叠。这种优化显著减少了训练时间和计算效率。为了评估 LMBiS-Net 的可靠性和通用性，我们进行了广泛的评估，包括不同方面的眼底图像。Specifically，我们将模型应用于眼底血管的分 segmentation，这是股眼科学诊断和治疗中的关键组成部分。通过专注于眼底血管，我们能够全面分析 LMBiS-Net 模型的性能和有效性。测试结果表明，LMBiS-Net 不仅具有可靠性和通用性，还能够保持高级别的分 segmentation 精度。这些特点表明 LMBiS-Net 可能是一种高速和准确的眼底图像分 segmentation 工具，有广泛的临床应用前景。
</details></li>
</ul>
<hr>
<h2 id="A-multiple-k-means-cluster-ensemble-framework-for-clustering-citation-trajectories"><a href="#A-multiple-k-means-cluster-ensemble-framework-for-clustering-citation-trajectories" class="headerlink" title="A multiple k-means cluster ensemble framework for clustering citation trajectories"></a>A multiple k-means cluster ensemble framework for clustering citation trajectories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04949">http://arxiv.org/abs/2309.04949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joyita Chakraborty, Dinesh K. Pradhan, Subrata Nandi</li>
<li>For: 本研究目的是综合分析学术论文的引用趋势，以便更好地理解知识协同演化过程。* Methods: 本研究使用了一种基于特征的多峰Means ensemble clustering方法，对10年和30年的引用轨迹进行分 clustering。* Results: 研究发现了4种不同的引用轨迹类型，即早期升温快落（2.2%）、早期升温慢落（45%）、延迟升温无落（53%）和延迟升温慢落（0.8%）。这些轨迹类型的差异和引用轨迹的增长和衰退时间、累积引用分布和峰值特征等被重新定义了。<details>
<summary>Abstract</summary>
Citation maturity time varies for different articles. However, the impact of all articles is measured in a fixed window. Clustering their citation trajectories helps understand the knowledge diffusion process and reveals that not all articles gain immediate success after publication. Moreover, clustering trajectories is necessary for paper impact recommendation algorithms. It is a challenging problem because citation time series exhibit significant variability due to non linear and non stationary characteristics. Prior works propose a set of arbitrary thresholds and a fixed rule based approach. All methods are primarily parameter dependent. Consequently, it leads to inconsistencies while defining similar trajectories and ambiguities regarding their specific number. Most studies only capture extreme trajectories. Thus, a generalised clustering framework is required. This paper proposes a feature based multiple k means cluster ensemble framework. 1,95,783 and 41,732 well cited articles from the Microsoft Academic Graph data are considered for clustering short term (10 year) and long term (30 year) trajectories, respectively. It has linear run time. Four distinct trajectories are obtained Early Rise Rapid Decline (2.2%), Early Rise Slow Decline (45%), Delayed Rise No Decline (53%), and Delayed Rise Slow Decline (0.8%). Individual trajectory differences for two different spans are studied. Most papers exhibit Early Rise Slow Decline and Delayed Rise No Decline patterns. The growth and decay times, cumulative citation distribution, and peak characteristics of individual trajectories are redefined empirically. A detailed comparative study reveals our proposed methodology can detect all distinct trajectory classes.
</details>
<details>
<summary>摘要</summary>
<SYS>文献成熟时间因文章不同而异，但所有文章的影响都是在固定窗口内测量的。对文章引用轨迹进行归类可以理解知识传播过程，并发现不所有文章在出版后立即获得成功。此外，归类轨迹是推荐文章影响的必要步骤。但是，归类时间序列表现出非线性和不稳定特征，使得现有方法具有许多缺陷。大多数研究只捕捉极端轨迹，因此需要一种通用的归类框架。这篇论文提出了基于特征的多种mean归类ensemble框架。对于10年和30年的短期和长期轨迹，分别使用Microsoft Academic Graph数据集中的195783和41732个高引用文章进行归类。它具有直线时间复杂度。根据不同的时间段，分别获得了四种不同的轨迹：早期升温迅速下降（2.2%）、早期升温慢下降（45%）、延迟升温无下降（53%）和延迟升温慢下降（0.8%）。对于不同的时间段，分别研究了个别轨迹的增长和衰退时间、累积引用分布和峰值特征。对于不同的时间段，我们提出了新的定义和比较研究，表明我们的方法可以检测所有不同的轨迹类型。</SYS>Note: The translation is done using Google Translate, and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="Distance-Restricted-Folklore-Weisfeiler-Leman-GNNs-with-Provable-Cycle-Counting-Power"><a href="#Distance-Restricted-Folklore-Weisfeiler-Leman-GNNs-with-Provable-Cycle-Counting-Power" class="headerlink" title="Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power"></a>Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04941">http://arxiv.org/abs/2309.04941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junru Zhou, Jiarui Feng, Xiyuan Wang, Muhan Zhang</li>
<li>For: The paper focuses on developing a novel class of graph neural networks (GNNs) that can efficiently count certain graph substructures, especially cycles, and prove their provable cycle counting power.* Methods: The proposed GNN model, called $d$-Distance-Restricted FWL(2) GNNs, uses node pairs whose mutual distances are at most $d$ as the units for message passing to balance the expressive power and complexity. The model avoids expensive subgraph extraction operations in subgraph GNNs, making both the time and space complexity lower.* Results: The paper theoretically shows that the discriminative power of $d$-DRFWL(2) GNNs strictly increases as $d$ increases, and the model has provably strong cycle counting power even with $d&#x3D;2$, being able to count all 3, 4, 5, 6-cycles. Experiments on both synthetic datasets and molecular datasets verify the theory, making the model the most efficient GNN model to date that can count up to 6-cycles.<details>
<summary>Abstract</summary>
The ability of graph neural networks (GNNs) to count certain graph substructures, especially cycles, is important for the success of GNNs on a wide range of tasks. It has been recently used as a popular metric for evaluating the expressive power of GNNs. Many of the proposed GNN models with provable cycle counting power are based on subgraph GNNs, i.e., extracting a bag of subgraphs from the input graph, generating representations for each subgraph, and using them to augment the representation of the input graph. However, those methods require heavy preprocessing, and suffer from high time and memory costs. In this paper, we overcome the aforementioned limitations of subgraph GNNs by proposing a novel class of GNNs -- $d$-Distance-Restricted FWL(2) GNNs, or $d$-DRFWL(2) GNNs. $d$-DRFWL(2) GNNs use node pairs whose mutual distances are at most $d$ as the units for message passing to balance the expressive power and complexity. By performing message passing among distance-restricted node pairs in the original graph, $d$-DRFWL(2) GNNs avoid the expensive subgraph extraction operations in subgraph GNNs, making both the time and space complexity lower. We theoretically show that the discriminative power of $d$-DRFWL(2) GNNs strictly increases as $d$ increases. More importantly, $d$-DRFWL(2) GNNs have provably strong cycle counting power even with $d=2$: they can count all 3, 4, 5, 6-cycles. Since 6-cycles (e.g., benzene rings) are ubiquitous in organic molecules, being able to detect and count them is crucial for achieving robust and generalizable performance on molecular tasks. Experiments on both synthetic datasets and molecular datasets verify our theory. To the best of our knowledge, our model is the most efficient GNN model to date (both theoretically and empirically) that can count up to 6-cycles.
</details>
<details>
<summary>摘要</summary>
“graph neural networks（GNNs）的某些特定Graph substructures的计数能力对GNNs的成功有着重要作用。它最近被用作GNNs的表达力评估metric。许多提出的GNN模型具有可证明的循环计数能力，但它们需要劳硬的预处理和高时间和内存成本。在这篇论文中，我们超越了subgraph GNNs的limitations，提出了一种新的GNN类型——$d$-Distance-Restricted FWL(2) GNNs（$d$-DRFWL(2) GNNs）。$d$-DRFWL(2) GNNs使用图像的node pair whose mutual distances are at most $d$ as the unit for message passing，以填补GNNs的表达力和复杂性之间的 contradistinction。通过在原始图像上进行距离限制的message passing，$d$-DRFWL(2) GNNs避免了expensive subgraph extraction操作，从而降低了时间和内存复杂性。我们理论上显示了$d$-DRFWL(2) GNNs的discriminative power会随着$d$的增加而增加。更重要的是，$d$-DRFWL(2) GNNs有可证明的循环计数能力，可以计数所有3, 4, 5, 6-cycles。由于6-cycles（如苯环）在有机分子中很普遍，能够检测和计数它们是对有机任务的稳定和普遍性很重要。实验表明，我们的模型是目前最高效的GNN模型（both theoretically and empirically），可以计数到6-cycles。”
</details></li>
</ul>
<hr>
<h2 id="Knowledge-based-Refinement-of-Scientific-Publication-Knowledge-Graphs"><a href="#Knowledge-based-Refinement-of-Scientific-Publication-Knowledge-Graphs" class="headerlink" title="Knowledge-based Refinement of Scientific Publication Knowledge Graphs"></a>Knowledge-based Refinement of Scientific Publication Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05681">http://arxiv.org/abs/2309.05681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwen Yan, Phillip Odom, Sriraam Natarajan</li>
<li>for: 本研究是为了解决作者识别问题，通过建构和细化知识 graphs。</li>
<li>methods: 本研究使用了函数梯度提升的可probabilistic logic模型，并在人类指导下进行学习（知识基本学习）。</li>
<li>results: 研究表明，人类知识可以有效地改善作者识别的精度，并且可以提供可读性的规则。<details>
<summary>Abstract</summary>
We consider the problem of identifying authorship by posing it as a knowledge graph construction and refinement. To this effect, we model this problem as learning a probabilistic logic model in the presence of human guidance (knowledge-based learning). Specifically, we learn relational regression trees using functional gradient boosting that outputs explainable rules. To incorporate human knowledge, advice in the form of first-order clauses is injected to refine the trees. We demonstrate the usefulness of human knowledge both quantitatively and qualitatively in seven authorship domains.
</details>
<details>
<summary>摘要</summary>
我们将作者认证问题转化为知识图构建和精化问题。为此，我们使用机器学习可靠逻辑模型，在人类指导下进行学习（知识基本学习）。具体来说，我们使用函数升降树来学习关系回归树，并通过可见规则输出。为了汲取人类知识，我们将首选规则注入到树中以进行精化。我们在七个作者领域中证明了人类知识的有用性， both quantitatively and qualitatively。
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Machine-Learning-based-Security-in-Cloud-Computing"><a href="#A-Review-of-Machine-Learning-based-Security-in-Cloud-Computing" class="headerlink" title="A Review of Machine Learning-based Security in Cloud Computing"></a>A Review of Machine Learning-based Security in Cloud Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04911">http://arxiv.org/abs/2309.04911</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Aptin Babaei, Parham M. Kebria, Mohsen Moradi Dalvand, Saeid Nahavandi</li>
<li>for: 本文旨在探讨最新的机器学习（ML）在云计算安全领域的研究进展，以及这些算法的特点和可能的局限性。</li>
<li>methods: 本文使用了多种ML算法，包括决策树、支持向量机、隐藏状态Markov模型等，以检测和解决云计算中的安全问题。</li>
<li>results: 本文结果表明，使用ML算法可以有效地检测和解决云计算中的安全问题，提高了云计算的可用性、完整性和隐私性。<details>
<summary>Abstract</summary>
Cloud Computing (CC) is revolutionizing the way IT resources are delivered to users, allowing them to access and manage their systems with increased cost-effectiveness and simplified infrastructure. However, with the growth of CC comes a host of security risks, including threats to availability, integrity, and confidentiality. To address these challenges, Machine Learning (ML) is increasingly being used by Cloud Service Providers (CSPs) to reduce the need for human intervention in identifying and resolving security issues. With the ability to analyze vast amounts of data, and make high-accuracy predictions, ML can transform the way CSPs approach security. In this paper, we will explore some of the most recent research in the field of ML-based security in Cloud Computing. We will examine the features and effectiveness of a range of ML algorithms, highlighting their unique strengths and potential limitations. Our goal is to provide a comprehensive overview of the current state of ML in cloud security and to shed light on the exciting possibilities that this emerging field has to offer.
</details>
<details>
<summary>摘要</summary>
云计算（CC）正在改变IT资源的交付方式，让用户访问和管理系统变得更加成本效益，简化基础设施。然而，随着CC的发展，也出现了一系列安全风险，包括可用性、完整性和机密性的威胁。为了解决这些挑战，云服务提供商（CSP）通过机器学习（ML）减少人类 intervención的需求，以提高安全性。ML可以分析大量数据，并做出高精度预测，这可以将云安全问题的解决方式发展到一个新的水平。在这篇论文中，我们将探讨最近的ML在云计算安全领域的研究进展，探讨不同ML算法的特点和可能的局限性。我们的目标是提供云计算安全领域ML的全面概述，并探讨这个新兴领域的激动人心的可能性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/10/cs.LG_2023_09_10/" data-id="clmjn91n0008b0j886nribzi9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/10/eess.IV_2023_09_10/" class="article-date">
  <time datetime="2023-09-10T09:00:00.000Z" itemprop="datePublished">2023-09-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/10/eess.IV_2023_09_10/">eess.IV - 2023-09-10</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Lung-Diseases-Image-Segmentation-using-Faster-R-CNNs"><a href="#Lung-Diseases-Image-Segmentation-using-Faster-R-CNNs" class="headerlink" title="Lung Diseases Image Segmentation using Faster R-CNNs"></a>Lung Diseases Image Segmentation using Faster R-CNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06386">http://arxiv.org/abs/2309.06386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mihir Jain</li>
<li>For: 这篇论文旨在探讨肺病患者的早期诊断，以减少 Mortality rate。* Methods: 这篇论文提出了一种基于低密度神经网络结构的方法，以实现肺病患者的早期诊断。这种方法利用特征峰组件来优化数据提取和减少信息损失。* Results: 这篇论文在胸部X射线图像上进行了肺病患者的检测，并计算了混淆矩阵来决定准确率、精度、敏感度和特率。研究分析了损失函数，强调了训练和分类阶段中模型的表现。<details>
<summary>Abstract</summary>
Lung diseases are a leading cause of child mortality in the developing world, with India accounting for approximately half of global pneumonia deaths (370,000) in 2016. Timely diagnosis is crucial for reducing mortality rates. This paper introduces a low-density neural network structure to mitigate topological challenges in deep networks. The network incorporates parameters into a feature pyramid, enhancing data extraction and minimizing information loss. Soft Non-Maximal Suppression optimizes regional proposals generated by the Region Proposal Network. The study evaluates the model on chest X-ray images, computing a confusion matrix to determine accuracy, precision, sensitivity, and specificity. We analyze loss functions, highlighting their trends during training. The regional proposal loss and classification loss assess model performance during training and classification phases. This paper analysis lung disease detection and neural network structures.
</details>
<details>
<summary>摘要</summary>
lung diseases 是发展中国家的主要导致儿童死亡的原因之一，印度在2016年的溶液病肺炎死亡人数（约370,000）占全球的一半。 时间早 diagnosis 是降低死亡率的关键。这篇论文介绍了一种低纹理神经网络结构，以减少深度网络中的 topological 挑战。该网络含有参数，并将其 integrate into a feature pyramid，以提高数据EXTRACTION 和减少信息损失。软 Non-Maximal Suppression 优化地区提议生成的Region Proposal Network。研究使用胸部X射线图像进行评估，计算冲混矩阵来确定准确率、精度、敏感度和特异性。我们分析损失函数，描述它们在训练和分类阶段的趋势。地区提议损失和分类损失评估模型在训练和分类阶段的性能。本文分析肺病检测和神经网络结构。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Perceptual-Quality-Aware-Adaptive-Volumetric-Video-Streaming"><a href="#Spatial-Perceptual-Quality-Aware-Adaptive-Volumetric-Video-Streaming" class="headerlink" title="Spatial Perceptual Quality Aware Adaptive Volumetric Video Streaming"></a>Spatial Perceptual Quality Aware Adaptive Volumetric Video Streaming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05026">http://arxiv.org/abs/2309.05026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Wang, Wei Liu, Huitong Liu, Peng Yang</li>
<li>for: 这篇论文旨在提高液体视频的品质体验（QoE）。</li>
<li>methods: 该论文使用了六度自由（6DoF）空间导航和人类视力分辨率限制来影响用户的感知质量。提出了一个视力尺度模型，描述虚拟观看距离对粒子云点频率的影响。</li>
<li>results: 该方案可以效果地提高总平均QoE，相比现有基线，提高了26%。<details>
<summary>Abstract</summary>
Volumetric video offers a highly immersive viewing experience, but poses challenges in ensuring quality of experience (QoE) due to its high bandwidth requirements. In this paper, we explore the effect of viewing distance introduced by six degrees of freedom (6DoF) spatial navigation on user's perceived quality. By considering human visual resolution limitations, we propose a visual acuity model that describes the relationship between the virtual viewing distance and the tolerable boundary point cloud density. The proposed model satisfies spatial visual requirements during 6DoF exploration. Additionally, it dynamically adjusts quality levels to balance perceptual quality and bandwidth consumption. Furthermore, we present a QoE model to represent user's perceived quality at different viewing distances precisely. Extensive experimental results demonstrate that, the proposed scheme can effectively improve the overall average QoE by up to 26% over real networks and user traces, compared to existing baselines.
</details>
<details>
<summary>摘要</summary>
三维视频提供了非常 immerse 的浏览经验，但是它带来了质量经验（QoE）的挑战，因为它具有高带宽要求。在这篇论文中，我们 investigate 6DoF 空间导航引入的观看距离对用户所感知的质量的影响。通过考虑人类视觉限制，我们提出了一个视力模型，该模型描述了虚拟观看距离对精度阈值点云密度的关系。我们的模型满足了空间视觉需求，并在6DoF 探索中动态调整质量水平以平衡感知质量和带宽消耗。此外，我们还提出了用户感知质量在不同观看距离下的准确模型。实验结果表明，我们的方案可以在真实网络和用户轨迹下提高总平均QoE达26%，比现有基eline更高。
</details></li>
</ul>
<hr>
<h2 id="SdCT-GAN-Reconstructing-CT-from-Biplanar-X-Rays-with-Self-driven-Generative-Adversarial-Networks"><a href="#SdCT-GAN-Reconstructing-CT-from-Biplanar-X-Rays-with-Self-driven-Generative-Adversarial-Networks" class="headerlink" title="SdCT-GAN: Reconstructing CT from Biplanar X-Rays with Self-driven Generative Adversarial Networks"></a>SdCT-GAN: Reconstructing CT from Biplanar X-Rays with Self-driven Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04960">http://arxiv.org/abs/2309.04960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csqvictory/sdct-gan">https://github.com/csqvictory/sdct-gan</a></li>
<li>paper_authors: Shuangqin Cheng, Qingliang Chen, Qiyi Zhang, Ming Li, Yamuhanmode Alike, Kaile Su, Pengcheng Wen</li>
<li>for: 这篇论文的目的是提出一种新的自适应 générateur adversarial network 模型（SdCT-GAN），以提高 CT 图像的三维重建质量。</li>
<li>methods: 该模型采用了一种新的 auto-encoder 结构，并应用了 Sobel Gradient Guider（SGG）思想，以 mejorar 图像的细节表示。</li>
<li>results: 根据实验结果，提出的模型比主流状态之前的基elines具有更高的重建质量和精度。<details>
<summary>Abstract</summary>
Computed Tomography (CT) is a medical imaging modality that can generate more informative 3D images than 2D X-rays. However, this advantage comes at the expense of more radiation exposure, higher costs, and longer acquisition time. Hence, the reconstruction of 3D CT images using a limited number of 2D X-rays has gained significant importance as an economical alternative. Nevertheless, existing methods primarily prioritize minimizing pixel/voxel-level intensity discrepancies, often neglecting the preservation of textural details in the synthesized images. This oversight directly impacts the quality of the reconstructed images and thus affects the clinical diagnosis. To address the deficits, this paper presents a new self-driven generative adversarial network model (SdCT-GAN), which is motivated to pay more attention to image details by introducing a novel auto-encoder structure in the discriminator. In addition, a Sobel Gradient Guider (SGG) idea is applied throughout the model, where the edge information from the 2D X-ray image at the input can be integrated. Moreover, LPIPS (Learned Perceptual Image Patch Similarity) evaluation metric is adopted that can quantitatively evaluate the fine contours and textures of reconstructed images better than the existing ones. Finally, the qualitative and quantitative results of the empirical studies justify the power of the proposed model compared to mainstream state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
计算机 Tomatoesography（CT）是医学成像方法，可以生成更有信息的3D图像，比2DX射线更好。然而，这个优点来自于更高的辐射暴露、更高的成本和更长的获取时间。因此，使用有限数量的2D X射线 reconstruction 3D CT 图像已成为一种经济的代替方案。然而，现有方法通常强调最小化像素/体积级别的INTENSITY不一致，经常忽略保留图像细节的synthesized图像质量。这个缺点直接影响图像重建的质量，从而影响临床诊断。为了解决这些缺点，本文提出了一种新的自适应生成对抗网络模型（SdCT-GAN），它在диск里具有一种新的自适应网络结构。此外， Sobel 梯度引导（SGG）理念在模型中应用，可以将输入2D X射线图像的边缘信息集成。此外，使用LPIPS（学习感知图像小块相似度）评价指标，可以更好地评价重建图像中细节和文化。最后，实验研究的质量和量тив结果证明了提案模型与主流状态的某些基准模型相比，具有更大的力量。
</details></li>
</ul>
<hr>
<h2 id="Anatomy-Completor-A-Multi-class-Completion-Framework-for-3D-Anatomy-Reconstruction"><a href="#Anatomy-Completor-A-Multi-class-Completion-Framework-for-3D-Anatomy-Reconstruction" class="headerlink" title="Anatomy Completor: A Multi-class Completion Framework for 3D Anatomy Reconstruction"></a>Anatomy Completor: A Multi-class Completion Framework for 3D Anatomy Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04956">http://arxiv.org/abs/2309.04956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianning Li, Antonio Pepe, Gijs Luijten, Christina Schwarz-Gsaxner, Jens Kleesiek, Jan Egger</li>
<li>for: 本研究提出了一个完成框架，用于重建各种生物体内部的几何 shapes，包括器官、血管和肌肉等。这个工作针对具有手术、病理或伤害等因素所导致的影像数据中缺失一或多个生物体的情况。</li>
<li>methods: 本研究使用了一个3D推对答者（DAE）来解决缺失生物体的问题。我们提出了两种方法：（i）DAE学习多个单一对多的映射，以将缺失的生物体重建为完整的形状；（ii）DAE直接学习一个单一的差分映射，以将缺失的生物体与目标体组之间的差异。我们还使用了损失统计的聚合方法，让DAE更好地学习多个对多的映射。</li>
<li>results: 我们使用了一个CT数据集，以评估我们的方法。结果显示，我们的方法可以对不同水平的缺失（即一或多个随机的生物体缺失）进行合理的重建。代码和预训模型可以在<a target="_blank" rel="noopener" href="https://github.com/Jianningli/medshapenet-feedback/tree/main/anatomy-completor%E4%B8%8A%E5%8F%96%E5%BE%97%E3%80%82">https://github.com/Jianningli/medshapenet-feedback/tree/main/anatomy-completor上取得。</a><details>
<summary>Abstract</summary>
In this paper, we introduce a completion framework to reconstruct the geometric shapes of various anatomies, including organs, vessels and muscles. Our work targets a scenario where one or multiple anatomies are missing in the imaging data due to surgical, pathological or traumatic factors, or simply because these anatomies are not covered by image acquisition. Automatic reconstruction of the missing anatomies benefits many applications, such as organ 3D bio-printing, whole-body segmentation, animation realism, paleoradiology and forensic imaging. We propose two paradigms based on a 3D denoising auto-encoder (DAE) to solve the anatomy reconstruction problem: (i) the DAE learns a many-to-one mapping between incomplete and complete instances; (ii) the DAE learns directly a one-to-one residual mapping between the incomplete instances and the target anatomies. We apply a loss aggregation scheme that enables the DAE to learn the many-to-one mapping more effectively and further enhances the learning of the residual mapping. On top of this, we extend the DAE to a multiclass completor by assigning a unique label to each anatomy involved. We evaluate our method using a CT dataset with whole-body segmentations. Results show that our method produces reasonable anatomy reconstructions given instances with different levels of incompleteness (i.e., one or multiple random anatomies are missing). Codes and pretrained models are publicly available at https://github.com/Jianningli/medshapenet-feedback/ tree/main/anatomy-completor
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种完成框架，用于重建各种生物学结构，包括器官、血管和肌肉等。我们的工作针对一种医学影像数据中缺失一或多个生物学结构的场景，这可能是手术、病理或受伤等因素导致的，或者这些结构simply不是图像采集范围内。自动重建缺失的生物学结构具有许多应用，如器官3D生物印制、全身份割、动画真实性、古生物Radiology和刑事成像等。我们提出了两种方案，基于3D滤波 autoencoder（DAE）解决缺失生物学结构的问题：（i）DAE学习多对一映射关系 incomplete和完整的实例；（ii）DAE直接学习一对一差异映射关系 incomplete实例和目标生物学结构。我们采用损失积合方案，使得 DAE 更有效地学习多对一映射关系，并进一步提高了对差异映射关系的学习。此外，我们将 DAE 扩展到多类 completor，通过为每个参与的生物学结构分配唯一标签。我们使用 CT 数据集进行评估，结果表明，我们的方法可以在不同水平的不完整实例（即一或多个随机的生物学结构缺失）下提供合理的生物学结构重建。代码和预训练模型可以在 <https://github.com/Jianningli/medshapenet-feedback/tree/main/anatomy-completor> 中下载。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/10/eess.IV_2023_09_10/" data-id="clmjn91qz00hv0j88hhjyeyic" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/09/cs.SD_2023_09_09/" class="article-date">
  <time datetime="2023-09-09T15:00:00.000Z" itemprop="datePublished">2023-09-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/09/cs.SD_2023_09_09/">cs.SD - 2023-09-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Exploring-Music-Genre-Classification-Algorithm-Analysis-and-Deployment-Architecture"><a href="#Exploring-Music-Genre-Classification-Algorithm-Analysis-and-Deployment-Architecture" class="headerlink" title="Exploring Music Genre Classification: Algorithm Analysis and Deployment Architecture"></a>Exploring Music Genre Classification: Algorithm Analysis and Deployment Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04861">http://arxiv.org/abs/2309.04861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayan Biswas, Supriya Dhabal, Palaniandavar Venkateswaran</li>
<li>for: 本研究旨在提出一种基于数字信号处理（DSP）和深度学习（DL）技术的音乐类型分类算法，以便更好地分类音乐。</li>
<li>methods: 本研究使用了DSP和DL两种不同的技术，将音频信号中的有用特征提取出来，并使用这些特征来分类音乐类型。</li>
<li>results: 在GTZAN数据集上测试了该算法，实现了高精度的音乐类型分类。此外，还提出了一种端到端的部署架构，用于在音乐相关应用中实现音乐类型分类。<details>
<summary>Abstract</summary>
Music genre classification has become increasingly critical with the advent of various streaming applications. Nowadays, we find it impossible to imagine using the artist's name and song title to search for music in a sophisticated music app. It is always difficult to classify music correctly because the information linked to music, such as region, artist, album, or non-album, is so variable. This paper presents a study on music genre classification using a combination of Digital Signal Processing (DSP) and Deep Learning (DL) techniques. A novel algorithm is proposed that utilizes both DSP and DL methods to extract relevant features from audio signals and classify them into various genres. The algorithm was tested on the GTZAN dataset and achieved high accuracy. An end-to-end deployment architecture is also proposed for integration into music-related applications. The performance of the algorithm is analyzed and future directions for improvement are discussed. The proposed DSP and DL-based music genre classification algorithm and deployment architecture demonstrate a promising approach for music genre classification.
</details>
<details>
<summary>摘要</summary>
<sys>音乐类别分类在Various流媒体应用程序出现后变得越来越重要。现在，我们无法想象在一个复杂的音乐应用程序中使用艺术家名和歌曲名进行搜索。因为音乐相关信息，如地区、艺术家、专辑和非专辑，是非常变化的。本文介绍了一项研究，使用数字信号处理（DSP）和深度学习（DL）技术来分类音乐。提议的算法利用了DSP和DL方法来提取音频信号中相关的特征，并将其分类为不同的类别。该算法在GTZAN数据集上进行测试，实现了高准确率。此外，一种端到端部署架构也被提议，用于整合音乐相关应用程序。算法的性能分析和未来改进方向也被讨论。提议的DSP和DL基于音乐类别分类算法和部署架构表明了一种有前途的方法。</sys>Note: Simplified Chinese is used in this translation, as it is more commonly used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Language-Models-for-Exploiting-ASR-Uncertainty"><a href="#Leveraging-Large-Language-Models-for-Exploiting-ASR-Uncertainty" class="headerlink" title="Leveraging Large Language Models for Exploiting ASR Uncertainty"></a>Leveraging Large Language Models for Exploiting ASR Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04842">http://arxiv.org/abs/2309.04842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranay Dighe, Yi Su, Shangshang Zheng, Yunshu Liu, Vineet Garg, Xiaochuan Niu, Ahmed Tewfik</li>
<li>for: 这个论文主要针对的是如何使用大型自然语言处理（NLP）模型来提高语音理解（SLU）任务的表现，特别是在使用外部自动语音识别（ASR）系统进行转录时。</li>
<li>methods: 作者们提出了一种使用n-best列表来替代单个错误的1-best假设来提高LLM的表现。他们还使用了折衔适应器的低级rank fine-tuning来适应下游任务。</li>
<li>results: 作者们在设备导向语音检测任务和关键词检测任务上使用了n-best列表来取得了更好的表现，而使用1-best假设的系统则表现较差。这些结果表明，通过使用LLM和ASR的uncertainty可以实现更高效的语音应用程序。<details>
<summary>Abstract</summary>
While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.
</details>
<details>
<summary>摘要</summary>
large language models在多种自然语言处理（NLP）任务中表现出色，但要在语音理解（SLU）任务中表现良好，它们需要 either靠ready-made自动语音识别（ASR）系统进行转录，或者具备语音模式。这个工作关注前者情况，即使用固定ASR系统进行转录后，LLM的SLU任务准确率受到ASR系统的准确性限制。特别是在语音意图分类任务中，高字错率可能限制LLM理解口头意图的能力。而不是追求高准确率通过设计复杂或特殊的建筑，我们想知道可以不需要重大更改ASR和LLM的基础结构，这些结构可能被多个无关任务共享。为此，我们提出在LLM中使用n-best列表而不是唯一的错误的1-best гипотезы。我们探索了Prompt工程来解释n-best列表的概念给LLM;然后对下游任务进行finetuning Low-Rank Adapters。我们的方法使用n-best列表证明有效于设备直接语音检测任务以及关键词检测任务，系统使用n-best列表Prompt在这些任务中表现出色，因此开启了一种高效的方法来利用ASR不确定性via LLMS для语音基于应用。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Minimum-Error-with-Fiducial-Points-Criterion-for-Robust-Learning"><a href="#Generalized-Minimum-Error-with-Fiducial-Points-Criterion-for-Robust-Learning" class="headerlink" title="Generalized Minimum Error with Fiducial Points Criterion for Robust Learning"></a>Generalized Minimum Error with Fiducial Points Criterion for Robust Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04670">http://arxiv.org/abs/2309.04670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haiquan Zhao, Yuan Gao, Yingying Zhu</li>
<li>for: 提高 minimum error entropy criterion 的精度和灵活性，适用于 adaptive filter、kernel recursive algorithm 和 multilayer perceptron 等领域。</li>
<li>methods: 基于 Generalized Gaussian Density 函数的 generalized minimum error with fiducial points criterion (GMEEF)，以及利用量化思想来降低计算复杂性。</li>
<li>results: 在系统标识、声学反射抑制、时间序列预测和超vised分类等 numeral simulations 中，提议的算法表现出色。<details>
<summary>Abstract</summary>
The conventional Minimum Error Entropy criterion (MEE) has its limitations, showing reduced sensitivity to error mean values and uncertainty regarding error probability density function locations. To overcome this, a MEE with fiducial points criterion (MEEF), was presented. However, the efficacy of the MEEF is not consistent due to its reliance on a fixed Gaussian kernel. In this paper, a generalized minimum error with fiducial points criterion (GMEEF) is presented by adopting the Generalized Gaussian Density (GGD) function as kernel. The GGD extends the Gaussian distribution by introducing a shape parameter that provides more control over the tail behavior and peakedness. In addition, due to the high computational complexity of GMEEF criterion, the quantized idea is introduced to notably lower the computational load of the GMEEF-type algorithm. Finally, the proposed criterions are introduced to the domains of adaptive filter, kernel recursive algorithm, and multilayer perceptron. Several numerical simulations, which contain system identification, acoustic echo cancellation, times series prediction, and supervised classification, indicate that the novel algorithms' performance performs excellently.
</details>
<details>
<summary>摘要</summary>
传统的最小错误概率标准（MEE）有其局限性，其中错误的均值值和不确定性不具有充分的敏感度。为了解决这个问题，一种基于 fiducial point 的 MEE  criterion（MEEF）被提出。然而，MEEF 的效果不具有一致性，这是因为它基于固定的 Gaussian kernel。在这篇文章中，一种基于 Generalized Gaussian Density 函数（GGD）的 generalized minimum error with fiducial points criterion（GMEEF）被提出。GGD 函数在增加了 Gaussian 分布的灵活性，并通过 introducing 一个形状参数来控制尾部行为和峰值。此外，由于 GMEEF  criterion 的高计算复杂度，quantized 的想法被引入以下式 Notable 降低计算负担。最后，提出的 criterions 被应用到适应过滤器、kernel recursive algorithm 和多层感知机中。数据 simulations 表明，提出的算法在 system identification、音频透传抑制、时间序列预测和supervised classification 中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Mask-CTC-based-Encoder-Pre-training-for-Streaming-End-to-End-Speech-Recognition"><a href="#Mask-CTC-based-Encoder-Pre-training-for-Streaming-End-to-End-Speech-Recognition" class="headerlink" title="Mask-CTC-based Encoder Pre-training for Streaming End-to-End Speech Recognition"></a>Mask-CTC-based Encoder Pre-training for Streaming End-to-End Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04654">http://arxiv.org/abs/2309.04654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huaibo Zhao, Yosuke Higuchi, Yusuke Kida, Tetsuji Ogawa, Tetsunori Kobayashi</li>
<li>for: 这个研究旨在检验Mask-CTC基础模型在不同架构下的有效性，以及该方法对输出准确的脉冲时间的影响。</li>
<li>methods: 这个研究使用Mask-CTC基础模型进行预训练，并对不同的模型架构进行比较。</li>
<li>results: 研究发现，无论使用哪种模型架构，Mask-CTC基础模型都可以提高ASR系统的准确率和预测词的脉冲时间。<details>
<summary>Abstract</summary>
Achieving high accuracy with low latency has always been a challenge in streaming end-to-end automatic speech recognition (ASR) systems. By attending to more future contexts, a streaming ASR model achieves higher accuracy but results in larger latency, which hurts the streaming performance. In the Mask-CTC framework, an encoder network is trained to learn the feature representation that anticipates long-term contexts, which is desirable for streaming ASR. Mask-CTC-based encoder pre-training has been shown beneficial in achieving low latency and high accuracy for triggered attention-based ASR. However, the effectiveness of this method has not been demonstrated for various model architectures, nor has it been verified that the encoder has the expected look-ahead capability to reduce latency. This study, therefore, examines the effectiveness of Mask-CTCbased pre-training for models with different architectures, such as Transformer-Transducer and contextual block streaming ASR. We also discuss the effect of the proposed pre-training method on obtaining accurate output spike timing.
</details>
<details>
<summary>摘要</summary>
在流式端到端自动语音识别（ASR）系统中，实现高精度低延迟总是一个挑战。如果流式ASR模型更多地关注未来上下文，它们就可以达到更高的精度，但是这会导致更大的延迟，这会影响流式性能。在Mask-CTC框架中，一个Encoder网络被训练以学习预测长期上下文的特征表示，这是流式ASR中非常感兴趣的。Mask-CTC基于Encoder预训练已经被证明对于触发注意力基于ASR进行预训练是有利的，但是这种方法的效iveness尚未被证明对于不同的模型架构，也未被证明Encoder具有预期的前瞻能力来降低延迟。这个研究因此检查了Mask-CTC基于预训练的效iveness对于不同的模型架构，例如Transformer-Transducer和Contextual Block流式ASR。我们还讨论了由我们提议的预训练方法对于获取准确输出键时间的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/09/cs.SD_2023_09_09/" data-id="clmjn91og00c30j883oclemhv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/09/cs.LG_2023_09_09/" class="article-date">
  <time datetime="2023-09-09T10:00:00.000Z" itemprop="datePublished">2023-09-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/09/cs.LG_2023_09_09/">cs.LG - 2023-09-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Symplectic-Structure-Aware-Hamiltonian-Graph-Embeddings"><a href="#Symplectic-Structure-Aware-Hamiltonian-Graph-Embeddings" class="headerlink" title="Symplectic Structure-Aware Hamiltonian (Graph) Embeddings"></a>Symplectic Structure-Aware Hamiltonian (Graph) Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04885">http://arxiv.org/abs/2309.04885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxu Liu, Xinping Yi, Tianle Zhang, Xiaowei Huang</li>
<li>For: 本文旨在提出一种基于哈密顿系统的图 neural network（GNN）方法，以便更好地适应不同的图数据集。* Methods: 本文使用了里曼尼亚优化在射影Stiefel manifold上进行自适应学习，以便在训练中自动地适应不同的图数据集，而不需要进行广泛的超参数调整。此外，本文还保留了能量的Physically meaningful during training,以确保协助拟合更加准确。* Results: 本文通过对多种图数据集进行node classification任务的实验，证明SAH-GNN在适应性和性能方面表现出色，并且在不同的图数据集上具有更好的一致性。<details>
<summary>Abstract</summary>
In traditional Graph Neural Networks (GNNs), the assumption of a fixed embedding manifold often limits their adaptability to diverse graph geometries. Recently, Hamiltonian system-inspired GNNs are proposed to address the dynamic nature of such embeddings by incorporating physical laws into node feature updates. In this work, we present SAH-GNN, a novel approach that generalizes Hamiltonian dynamics for more flexible node feature updates. Unlike existing Hamiltonian-inspired GNNs, SAH-GNN employs Riemannian optimization on the symplectic Stiefel manifold to adaptively learn the underlying symplectic structure during training, circumventing the limitations of existing Hamiltonian GNNs that rely on a pre-defined form of standard symplectic structure. This innovation allows SAH-GNN to automatically adapt to various graph datasets without extensive hyperparameter tuning. Moreover, it conserves energy during training such that the implicit Hamiltonian system is physically meaningful. To this end, we empirically validate SAH-GNN's superior performance and adaptability in node classification tasks across multiple types of graph datasets.
</details>
<details>
<summary>摘要</summary>
传统的图 neural network (GNN) 假设 fixed embedding manifold 通常限制它们对多种图形态的适应性。最近，哈密顿系统适应 GNN 被提出来解决图 embeddings 的动态性，通过在节点特征更新中包含物理法则。在这项工作中，我们提出了 SAH-GNN，一种新的方法，可以扩展哈密顿动力学中的node feature更新。不同于现有的哈密顿适应 GNN，SAH-GNN 使用 Riemannian 优化在 симплекс Stiefel  manifold 上对固有的 симплекс结构进行自适应学习，从而超越现有的哈密顿 GNN 对标准哈密顿结构的固定假设。这种创新允许 SAH-GNN 在多种图数据集上自动适应不同的图类型，而无需进行详细的 гипер参数调整。此外，它在训练中保留能量，使得隐式的哈密顿系统具有物理意义。为此，我们在多种图数据集上验证了 SAH-GNN 的superior表现和适应性。
</details></li>
</ul>
<hr>
<h2 id="A-Gentle-Introduction-to-Gradient-Based-Optimization-and-Variational-Inequalities-for-Machine-Learning"><a href="#A-Gentle-Introduction-to-Gradient-Based-Optimization-and-Variational-Inequalities-for-Machine-Learning" class="headerlink" title="A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning"></a>A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04877">http://arxiv.org/abs/2309.04877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha S. Wadia, Yatin Dandi, Michael I. Jordan</li>
<li>for: 这篇论文旨在探讨机器学习领域的进展，以及它们在决策和多代人问题中的应用。</li>
<li>methods: 论文使用的方法包括落差点和 monotone 游戏，以及更一般的变量不等式。</li>
<li>results: 论文提供了一些新的加速算法的设计方法，以及对这些算法的收敛证明。但是，论文的主要重点是提供动机和直觉，而不是严格的证明。<details>
<summary>Abstract</summary>
The rapid progress in machine learning in recent years has been based on a highly productive connection to gradient-based optimization. Further progress hinges in part on a shift in focus from pattern recognition to decision-making and multi-agent problems. In these broader settings, new mathematical challenges emerge that involve equilibria and game theory instead of optima. Gradient-based methods remain essential -- given the high dimensionality and large scale of machine-learning problems -- but simple gradient descent is no longer the point of departure for algorithm design. We provide a gentle introduction to a broader framework for gradient-based algorithms in machine learning, beginning with saddle points and monotone games, and proceeding to general variational inequalities. While we provide convergence proofs for several of the algorithms that we present, our main focus is that of providing motivation and intuition.
</details>
<details>
<summary>摘要</summary>
“近年来机器学习的快速进步主要基于高度产生的梯度下降优化。未来的进步受到一定程度受到决策和多代人问题的转移。在这些更广泛的设置下，新的数学挑战出现，涉及到均衡和游戏理论而不是最优点。梯度下降方法仍然是必不可少的——由于机器学习问题的高维度和大规模——但简单的梯度下降不再是算法设计的出发点。我们提供了一种渐进的框架 для梯度基于算法在机器学习中，从锥点和均衡游戏开始，然后进行总变量不等式。虽然我们提供了一些算法的收敛证明，但我们的主要关注点是提供动机和直觉。”Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="Approximating-ReLU-on-a-Reduced-Ring-for-Efficient-MPC-based-Private-Inference"><a href="#Approximating-ReLU-on-a-Reduced-Ring-for-Efficient-MPC-based-Private-Inference" class="headerlink" title="Approximating ReLU on a Reduced Ring for Efficient MPC-based Private Inference"></a>Approximating ReLU on a Reduced Ring for Efficient MPC-based Private Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04875">http://arxiv.org/abs/2309.04875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kiwan Maeng, G. Edward Suh</li>
<li>for: 这个论文的目的是提出一个名为 HummingBird 的多方 computation（MPC）框架，以实现在不可信服务器上进行隐私数据的机器学习推导，并且大大减少 ReLU 评估 overhead。</li>
<li>methods: HummingBird 使用一个子集位元来评估 ReLU，并通过理论分析确定不需要的位元，以减少通信。它还具有高效的搜索引擎，可以对秘密分享中的位元进行有效地排序和搁置。</li>
<li>results: HummingBird 在一个实际的多服务器 MPC 设置中，可以实现从 2.03 到 2.67 倍的终端执行时间优化，而不会导致精度下降。在允许一定精度下降的情况下，HummingBird 可以实现更高的优化，达到 8.64 倍的终端执行时间优化。<details>
<summary>Abstract</summary>
Secure multi-party computation (MPC) allows users to offload machine learning inference on untrusted servers without having to share their privacy-sensitive data. Despite their strong security properties, MPC-based private inference has not been widely adopted in the real world due to their high communication overhead. When evaluating ReLU layers, MPC protocols incur a significant amount of communication between the parties, making the end-to-end execution time multiple orders slower than its non-private counterpart.   This paper presents HummingBird, an MPC framework that reduces the ReLU communication overhead significantly by using only a subset of the bits to evaluate ReLU on a smaller ring. Based on theoretical analyses, HummingBird identifies bits in the secret share that are not crucial for accuracy and excludes them during ReLU evaluation to reduce communication. With its efficient search engine, HummingBird discards 87--91% of the bits during ReLU and still maintains high accuracy. On a real MPC setup involving multiple servers, HummingBird achieves on average 2.03--2.67x end-to-end speedup without introducing any errors, and up to 8.64x average speedup when some amount of accuracy degradation can be tolerated, due to its up to 8.76x communication reduction.
</details>
<details>
<summary>摘要</summary>
安全多方计算（MPC）允许用户在不信任的服务器上执行机器学习推理，而不需要将隐私敏感数据分享。尽管它们具有强安全性质，但MPC基于私有推理还没有在实际世界中广泛采用，主要是因为它们的通信 overhead 过高。在评估 ReLU 层时，MPC 协议在党中产生了很大的通信量，使得总端到终点执行时间比非私有 counterpart 多出多个次数。本文介绍了 HummingBird，一个基于 MPC 框架，可以在 ReLU 评估中减少通信 overhead 。HummingBird 根据理论分析，在秘密分享中标识不必要的位 bits，并在 ReLU 评估中排除它们，从而减少通信。HummingBird 还具有高效的搜索引擎，可以在 ReLU 评估中抛弃 87--91% 的位 bits，并仍保持高精度。在多服务器 MPC 环境中，HummingBird 平均 achiev 2.03--2.67 倍的终点到终点速度提升，而无错误。在某些情况下可以tolerate 一定的精度下降，HummingBird 可以在 8.64 倍的通信减少情况下实现最高 8.76 倍的速度提升。
</details></li>
</ul>
<hr>
<h2 id="Approximation-Results-for-Gradient-Descent-trained-Neural-Networks"><a href="#Approximation-Results-for-Gradient-Descent-trained-Neural-Networks" class="headerlink" title="Approximation Results for Gradient Descent trained Neural Networks"></a>Approximation Results for Gradient Descent trained Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04860">http://arxiv.org/abs/2309.04860</a></li>
<li>repo_url: None</li>
<li>paper_authors: G. Welper</li>
<li>for: 本文提供了对各种 Sobolev 平滑函数的神经网络学习的约数保证，使用梯度流进行训练，并且使用 continueous $L_2(\mathbb{S}^{d-1})$-norm 来度量错误。</li>
<li>methods: 本文使用了神经网络的层WISE gradient flow，并且只有第二层（非极值层）使用了神经凝结函数（NTK）来确定梯度流的速度。</li>
<li>results: 本文得到了一系列的约数保证，包括 Sobolev 平滑函数的approximation guarantee，并且发现在一定的under-parametrized regime下，神经网络的学习率可以更高，但是在这个regime下，神经网络的approximation rate会比标准的approximation方法（如 wavelet 方法）更差。<details>
<summary>Abstract</summary>
The paper contains approximation guarantees for neural networks that are trained with gradient flow, with error measured in the continuous $L_2(\mathbb{S}^{d-1})$-norm on the $d$-dimensional unit sphere and targets that are Sobolev smooth. The networks are fully connected of constant depth and increasing width. Although all layers are trained, the gradient flow convergence is based on a neural tangent kernel (NTK) argument for the non-convex second but last layer. Unlike standard NTK analysis, the continuous error norm implies an under-parametrized regime, possible by the natural smoothness assumption required for approximation. The typical over-parametrization re-enters the results in form of a loss in approximation rate relative to established approximation methods for Sobolev smooth functions.
</details>
<details>
<summary>摘要</summary>
文章提供了用梯度流 optimize 的神经网络的近似保证，错误量 measured 在 continues $L_2(\mathbb{S}^{d-1})$-norm 上 $d$-dimensional 单位球体上，目标函数是 Sobolev 的幂函数。神经网络是完全连接的常量深度和增长宽度。although 所有层都是训练的，梯度流整合基于非 conjugate 的 second but last layer 的 neural tangent kernel (NTK) argument。与标准 NTK 分析不同，Continuous error norm 导致了 under-parametrized  режим，可能由 natural smoothness assumption 所需的 approximation。typical over-parametrization 重新入场 result 中的 loss  approximations rate 相对于 established approximation methods for Sobolev smooth functions。
</details></li>
</ul>
<hr>
<h2 id="Reverse-Engineering-Decoding-Strategies-Given-Blackbox-Access-to-a-Language-Generation-System"><a href="#Reverse-Engineering-Decoding-Strategies-Given-Blackbox-Access-to-a-Language-Generation-System" class="headerlink" title="Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System"></a>Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04858">http://arxiv.org/abs/2309.04858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daphne Ippolito, Nicholas Carlini, Katherine Lee, Milad Nasr, Yun William Yu</li>
<li>for: 本研究旨在逆向工程语言模型生成文本的方法，以便掌握生成文本的参数。</li>
<li>methods: 本研究使用逆向工程方法来掌握语言模型生成文本的方法，包括top-$k$和核心采样等方法。</li>
<li>results: 研究发现，使用逆向工程方法可以准确地掌握语言模型生成文本的参数，并且可以揭示生成文本中的偏见。<details>
<summary>Abstract</summary>
Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-$k$ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model's predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).
</details>
<details>
<summary>摘要</summary>
neural language models 是越来越常被用在 API 和网站中，允许用户输入提示并返回生成的文本。许多这些系统并不公布生成参数。在这篇论文中，我们提出了一些方法来反引出生成文本的解码方法（即 top-$k$ 或核心抽样）。我们可以找出哪种解码策略被使用，这有关检测生成文本的意义。此外，找出解码策略的过程可以暴露模型预测分布的偏见。我们对一些开源语言模型家族和生产系统（如 ChatGPT）进行了攻击。
</details></li>
</ul>
<hr>
<h2 id="AmbientFlow-Invertible-generative-models-from-incomplete-noisy-measurements"><a href="#AmbientFlow-Invertible-generative-models-from-incomplete-noisy-measurements" class="headerlink" title="AmbientFlow: Invertible generative models from incomplete, noisy measurements"></a>AmbientFlow: Invertible generative models from incomplete, noisy measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04856">http://arxiv.org/abs/2309.04856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varun A. Kelkar, Rucha Deshpande, Arindam Banerjee, Mark A. Anastasio</li>
<li>for: 这篇论文旨在提出一种直接从含有噪声和缺失数据的方式学习流基型生成模型的框架。</li>
<li>methods: 该方法使用变分 Bayesian 方法建立流基型生成模型。</li>
<li>results: 数学实验表明，AmbientFlow 能够正确地学习物体分布。此外，AmbientFlow 在下游推理任务中的图像重建 task 中表现出色。<details>
<summary>Abstract</summary>
Generative models have gained popularity for their potential applications in imaging science, such as image reconstruction, posterior sampling and data sharing. Flow-based generative models are particularly attractive due to their ability to tractably provide exact density estimates along with fast, inexpensive and diverse samples. Training such models, however, requires a large, high quality dataset of objects. In applications such as computed imaging, it is often difficult to acquire such data due to requirements such as long acquisition time or high radiation dose, while acquiring noisy or partially observed measurements of these objects is more feasible. In this work, we propose AmbientFlow, a framework for learning flow-based generative models directly from noisy and incomplete data. Using variational Bayesian methods, a novel framework for establishing flow-based generative models from noisy, incomplete data is proposed. Extensive numerical studies demonstrate the effectiveness of AmbientFlow in correctly learning the object distribution. The utility of AmbientFlow in a downstream inference task of image reconstruction is demonstrated.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化字符串。<</SYS>>生成模型在成像科学中得到了广泛的应用，如图像重建、后 sampling 和数据分享。流基本生成模型尤其吸引人因为它们可以追踪性地提供精确的概率估计以及快速、便宜、多样的样本。但是训练这些模型需要一个大、高质量的对象数据集。在计算成像应用中，通常很难获得这些数据，因为需要长时间的获取或高剂量的辐射剂量，而获取噪音或部分观测这些对象的数据更加可能。在这项工作中，我们提出了 AmbientFlow，一个框架用于直接从噪音和不完整数据中学习流基本生成模型。使用变分 Bayesian 方法，我们提出了一种新的框架用于从噪音和不完整数据中建立流基本生成模型。广泛的数学实验表明 AmbientFlow 可以正确地学习对象分布。在下游推理任务中的图像重建中，AmbientFlow 的实用性得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Speech-Emotion-Recognition-with-Distilled-Prosodic-and-Linguistic-Affect-Representations"><a href="#Speech-Emotion-Recognition-with-Distilled-Prosodic-and-Linguistic-Affect-Representations" class="headerlink" title="Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations"></a>Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04849">http://arxiv.org/abs/2309.04849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debaditya Shome, Ali Etemad</li>
<li>for: The paper is written for speech emotion recognition (SER) and proposes a novel framework called EmoDistill.</li>
<li>methods: The paper uses cross-modal knowledge distillation during training to learn strong linguistic and prosodic representations of emotion from speech, and only uses a stream of speech signals during inference to perform unimodal SER.</li>
<li>results: The paper achieves state-of-the-art performance on the IEMOCAP benchmark, with unweighted accuracy of 77.49% and weighted accuracy of 78.91%. Detailed ablation studies demonstrate the impact of each component of the proposed method.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了推动语音情感识别（SER）而写的，并提出了一种新的框架 called EmoDistill。</li>
<li>methods: 这篇论文在训练时使用了跨Modal知识传递来学习语音中情感的强大语言和 просодические表示，并只在推理时使用了一流 Speech 信号来实现单模 SER。</li>
<li>results: 这篇论文在 IEMOCAP benchmark 上实现了 state-of-the-art 性能，具体的数据如下：不weighted 精度为 77.49%，weighted 精度为 78.91%。详细的ablation 研究还示出了每个方法的影响。<details>
<summary>Abstract</summary>
We propose EmoDistill, a novel speech emotion recognition (SER) framework that leverages cross-modal knowledge distillation during training to learn strong linguistic and prosodic representations of emotion from speech. During inference, our method only uses a stream of speech signals to perform unimodal SER thus reducing computation overhead and avoiding run-time transcription and prosodic feature extraction errors. During training, our method distills information at both embedding and logit levels from a pair of pre-trained Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on the IEMOCAP benchmark demonstrate that our method outperforms other unimodal and multimodal techniques by a considerable margin, and achieves state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted accuracy. Detailed ablation studies demonstrate the impact of each component of our method.
</details>
<details>
<summary>摘要</summary>
我们提出了EmotionDistill，一种新的语音情感识别（SER）框架，它利用跨模态知识填充在训练时期来学习语音中的强大情感和语音表达。在推断时，我们的方法只需要一束语音信号来进行单模态SER，从而降低计算负担和避免运行时转写和语音特征提取错误。在训练时，我们的方法在教师模型的 embedding 和 logit 两级填充信息，从两个预训练的 Prosodic 和 Linguistic 教师模型，这些模型在 SER 上进行了精心调整。实验表明，我们的方法在 IEMOCAP  bencmark 上比其他单模态和多模态技术高出较大幅度，并达到了状态之artefact的性能，即不加权的准确率为 77.49%，加权的准确率为 78.91%。详细的抽象研究表明了我们的方法中每个组件的影响。
</details></li>
</ul>
<hr>
<h2 id="Verifiable-Reinforcement-Learning-Systems-via-Compositionality"><a href="#Verifiable-Reinforcement-Learning-Systems-via-Compositionality" class="headerlink" title="Verifiable Reinforcement Learning Systems via Compositionality"></a>Verifiable Reinforcement Learning Systems via Compositionality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06420">http://arxiv.org/abs/2309.06420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cyrus Neary, Aryaman Singh Samyal, Christos Verginis, Murat Cubuktepe, Ufuk Topcu</li>
<li>for: 这个 paper 是为了描述一种可靠和可组合的 reinforcement learning (RL) 框架，用于将多个 RL 子系统组合成一个总任务。</li>
<li>methods: 这个框架包括一个高级模型，用于规划和分析子系统的组合，以及多个低级子系统，每个子系统都是一个深度学习 RL 代理人，运行于偏见 Observability 下。</li>
<li>results: 作者们提供了理论结果，证明如果每个子系统学习一个满足其子任务规范的策略，那么其组合就可以满足总任务规范。同时，如果子任务规范无法被满足， authors 则提供了一种自动更新子任务规范的方法，以适应观察到的缺陷。这种方法形式为高级模型中的优化问题。实验结果表明该框架在具有全 observability 和 partial observability、整数和连续状态空间、抽象和随机动力等多种环境下都能够展示其特点。<details>
<summary>Abstract</summary>
We propose a framework for verifiable and compositional reinforcement learning (RL) in which a collection of RL subsystems, each of which learns to accomplish a separate subtask, are composed to achieve an overall task. The framework consists of a high-level model, represented as a parametric Markov decision process, which is used to plan and analyze compositions of subsystems, and of the collection of low-level subsystems themselves. The subsystems are implemented as deep RL agents operating under partial observability. By defining interfaces between the subsystems, the framework enables automatic decompositions of task specifications, e.g., reach a target set of states with a probability of at least 0.95, into individual subtask specifications, i.e. achieve the subsystem's exit conditions with at least some minimum probability, given that its entry conditions are met. This in turn allows for the independent training and testing of the subsystems. We present theoretical results guaranteeing that if each subsystem learns a policy satisfying its subtask specification, then their composition is guaranteed to satisfy the overall task specification. Conversely, if the subtask specifications cannot all be satisfied by the learned policies, we present a method, formulated as the problem of finding an optimal set of parameters in the high-level model, to automatically update the subtask specifications to account for the observed shortcomings. The result is an iterative procedure for defining subtask specifications, and for training the subsystems to meet them. Experimental results demonstrate the presented framework's novel capabilities in environments with both full and partial observability, discrete and continuous state and action spaces, as well as deterministic and stochastic dynamics.
</details>
<details>
<summary>摘要</summary>
我们提出了一个渐进的强化学习框架，其中一个集合强化学习子系统，每个子系统都学习完成一个分离的子任务，这些子系统被组合以完成总任务。该框架包括一个高级模型，表示为Parametric Markov决策过程，用于规划和分析子系统的组合。每个子系统都是一个深度学习强化学习代理，在受限性观察下运行。通过定义子系统之间的接口，该框架允许自动将任务规范分解成各个子任务规范，例如：达到目标集的状态的概率大于或等于0.95。这样做了，可以独立地培训和测试每个子系统。我们提供了理论结果，证明如果每个子系统学习满足其子任务规范，那么其组合就会满足总任务规范。相反，如果子任务规范无法通过学习的策略满足，我们提出了一种方法，即在高级模型中寻找优化参数的问题，以自动更新子任务规范，以适应观察到的缺陷。结果是一种循环的过程，用于定义子任务规范，并培训子系统以满足它们。实验结果表明提出的框架在拥有完全和受限性观察，整数和连续状态和动作空间，以及决定性和随机动力学的环境中具有新的可能性。
</details></li>
</ul>
<hr>
<h2 id="HAct-Out-of-Distribution-Detection-with-Neural-Net-Activation-Histograms"><a href="#HAct-Out-of-Distribution-Detection-with-Neural-Net-Activation-Histograms" class="headerlink" title="HAct: Out-of-Distribution Detection with Neural Net Activation Histograms"></a>HAct: Out-of-Distribution Detection with Neural Net Activation Histograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04837">http://arxiv.org/abs/2309.04837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudeepta Mondal, Ganesh Sundaramoorthi</li>
<li>for: 检测已经训练过的神经网络模型对于未知数据的检测</li>
<li>methods: 使用一种新的描述符——HAct - 活化 histogram，对神经网络层的输出值进行检测</li>
<li>results: 在多个OOD图像分类 benchmark上显示了高度准确性，比之前的状态对照方法提高20.66%的假阳性率（在同样的TPR&#x3D;95%下），计算复杂性低、实现容易，适用于在实际中的在线监控已经训练过的神经网络模型。<details>
<summary>Abstract</summary>
We propose a simple, efficient, and accurate method for detecting out-of-distribution (OOD) data for trained neural networks, a potential first step in methods for OOD generalization. We propose a novel descriptor, HAct - activation histograms, for OOD detection, that is, probability distributions (approximated by histograms) of output values of neural network layers under the influence of incoming data. We demonstrate that HAct is significantly more accurate than state-of-the-art on multiple OOD image classification benchmarks. For instance, our approach achieves a true positive rate (TPR) of 95% with only 0.05% false-positives using Resnet-50 on standard OOD benchmarks, outperforming previous state-of-the-art by 20.66% in the false positive rate (at the same TPR of 95%). The low computational complexity and the ease of implementation make HAct suitable for online implementation in monitoring deployed neural networks in practice at scale.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Global-Convergence-of-Receding-Horizon-Policy-Search-in-Learning-Estimator-Designs"><a href="#Global-Convergence-of-Receding-Horizon-Policy-Search-in-Learning-Estimator-Designs" class="headerlink" title="Global Convergence of Receding-Horizon Policy Search in Learning Estimator Designs"></a>Global Convergence of Receding-Horizon Policy Search in Learning Estimator Designs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04831">http://arxiv.org/abs/2309.04831</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiangyuan-zhang/learningkf">https://github.com/xiangyuan-zhang/learningkf</a></li>
<li>paper_authors: Xiangyuan Zhang, Saviz Mowlavi, Mouhacine Benosman, Tamer Başar</li>
<li>for: 本研究开发了一个名为“推导 horizon policy gradient”（RHPG）算法，用于学习最佳的线性估计设计，即卡尔曼统计（KF）。</li>
<li>methods: RHPG算法将 vanilla PG（或任何其他政策搜寻方向）与动态计划外圈组合，将无限时间KF问题转换为一系列静态估计问题，并且可以保证全球对称性。</li>
<li>results: RHPG算法可以在不知情系统的情况下，不需要任何开始值，并且不需要目标系统是开 Loop稳定的情况下，实现全球对称性。此外，研究人员还提供了优化问题的分析和数据点的保证。<details>
<summary>Abstract</summary>
We introduce the receding-horizon policy gradient (RHPG) algorithm, the first PG algorithm with provable global convergence in learning the optimal linear estimator designs, i.e., the Kalman filter (KF). Notably, the RHPG algorithm does not require any prior knowledge of the system for initialization and does not require the target system to be open-loop stable. The key of RHPG is that we integrate vanilla PG (or any other policy search directions) into a dynamic programming outer loop, which iteratively decomposes the infinite-horizon KF problem that is constrained and non-convex in the policy parameter into a sequence of static estimation problems that are unconstrained and strongly-convex, thus enabling global convergence. We further provide fine-grained analyses of the optimization landscape under RHPG and detail the convergence and sample complexity guarantees of the algorithm. This work serves as an initial attempt to develop reinforcement learning algorithms specifically for control applications with performance guarantees by utilizing classic control theory in both algorithmic design and theoretical analyses. Lastly, we validate our theories by deploying the RHPG algorithm to learn the Kalman filter design of a large-scale convection-diffusion model. We open-source the code repository at \url{https://github.com/xiangyuan-zhang/LearningKF}.
</details>
<details>
<summary>摘要</summary>
我们介绍了往返Horizon Policy Gradient（RHPG）算法，是第一个可证地全球化 converge 的Policy Gradient（PG）算法，用于学习最佳的线性估计设计（即Kalman Filter，KF）。特别是，RHPG算法不需要任何系统开关的启动知识，并且不需要目标系统是开loop稳定。RHPG的关键在于将vanilla PG（或任何其他policy搜索方向）integrated into a dynamic programming outer loop，将无限期KF问题分解为一系列静止估计问题，从而实现全球化 converge。我们还提供了细部分析估计的Optimization landscape under RHPG，并详细说明算法的数据Complexity和数据对称性。这个工作作为开发基于控制应用的循环学习算法，并通过使用 класи控制理论在算法设计和理论分析中使用。最后，我们验证了我们的理论，通过将RHPG算法应用于大规模的对流扩散模型中来学习Kalman filter设计。我们将代码存储在 GitHub 上，请见 \url{https://github.com/xiangyuan-zhang/LearningKF}.
</details></li>
</ul>
<hr>
<h2 id="Correcting-sampling-biases-via-importancereweighting-for-spatial-modeling"><a href="#Correcting-sampling-biases-via-importancereweighting-for-spatial-modeling" class="headerlink" title="Correcting sampling biases via importancereweighting for spatial modeling"></a>Correcting sampling biases via importancereweighting for spatial modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04824">http://arxiv.org/abs/2309.04824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Prokhorov, Diana Koldasbayeva, Alexey Zaytsev</li>
<li>for:  This paper aims to address the problem of distribution bias in machine learning models for spatial data, particularly in environmental studies.</li>
<li>methods: The authors propose an approach based on importance sampling to obtain an unbiased estimate of the target error. They use importance sampling technique and kernel density estimation to reweigh errors at each sample point and neutralize the shift.</li>
<li>results: The authors validate the effectiveness of their approach using artificial data that resemble real-world spatial datasets. Their findings demonstrate the advantages of the proposed approach for the estimation of the target error, offering a solution to the distribution shift problem. The overall error of predictions decreased from 7% to just 2% and gets smaller for larger samples.Here’s the same information in Traditional Chinese:</li>
<li>for: 这篇论文的目的是解决机器学习模型在空间数据中的分布偏见问题，特别是在环境研究中。</li>
<li>methods: 作者提出一种基于重要抽样的方法，以获得不偏的目标错误估计。他们使用重要抽样技术和核密度估计来重新衡量错误的重要性，并neutralize偏移。</li>
<li>results: 作者使用实验数据验证了他们的方法，结果显示了这种方法在目标错误估计中的优势。错误率从7%降至2%，并且随着样本规模的增加而减少。<details>
<summary>Abstract</summary>
In machine learning models, the estimation of errors is often complex due to distribution bias, particularly in spatial data such as those found in environmental studies. We introduce an approach based on the ideas of importance sampling to obtain an unbiased estimate of the target error. By taking into account difference between desirable error and available data, our method reweights errors at each sample point and neutralizes the shift. Importance sampling technique and kernel density estimation were used for reweighteing. We validate the effectiveness of our approach using artificial data that resemble real-world spatial datasets. Our findings demonstrate advantages of the proposed approach for the estimation of the target error, offering a solution to a distribution shift problem. Overall error of predictions dropped from 7% to just 2% and it gets smaller for larger samples.
</details>
<details>
<summary>摘要</summary>
在机器学习模型中，错误估计通常受到分布偏见的影响，特别是在环境学研究中的空间数据中。我们介绍了基于重要性抽样的方法，以获取不偏的目标错误估计。通过考虑愿望的错误和可用数据之间的差异，我们的方法在每个样点重新权重错误。我们使用重要性抽样技术和核密度估计来实现重新权重。我们使用人工数据，模拟现实世界的空间数据集，以验证我们的方法的效果。我们的发现表明，我们的方法可以减少预测错误的总错误率，从7%降至2%，并且随着样本规模增加，错误率变得更加小。
</details></li>
</ul>
<hr>
<h2 id="ABC-Easy-as-123-A-Blind-Counter-for-Exemplar-Free-Multi-Class-Class-agnostic-Counting"><a href="#ABC-Easy-as-123-A-Blind-Counter-for-Exemplar-Free-Multi-Class-Class-agnostic-Counting" class="headerlink" title="ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class Class-agnostic Counting"></a>ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class Class-agnostic Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04820">http://arxiv.org/abs/2309.04820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael A. Hobley, Victor A. Prisacariu</li>
<li>for: 本研究旨在提供一种不需要类例或图像中只含一种对象的情况下，能够对多种对象进行分类不受限制的 counting 方法。</li>
<li>methods: 该方法基于一个新的 paradigm，即在批量计数阶段不需要用例来引导计数，而是在计数后找到对应的类别示例来帮助用户理解生成的输出。</li>
<li>results: 对于 MCAC 数据集，该方法可以与 contemporary methods 相比，而无需人工循环注解。此外，该方法还可以在 FSC-147 数据集上达到类似的性能。<details>
<summary>Abstract</summary>
Class-agnostic counting methods enumerate objects of an arbitrary class, providing tremendous utility in many fields. Prior works have limited usefulness as they require either a set of examples of the type to be counted or that the image contains only a single type of object. A significant factor in these shortcomings is the lack of a dataset to properly address counting in settings with more than one kind of object present. To address these issues, we propose the first Multi-class, Class-Agnostic Counting dataset (MCAC) and A Blind Counter (ABC123), a method that can count multiple types of objects simultaneously without using examples of type during training or inference. ABC123 introduces a new paradigm where instead of requiring exemplars to guide the enumeration, examples are found after the counting stage to help a user understand the generated outputs. We show that ABC123 outperforms contemporary methods on MCAC without the requirement of human in-the-loop annotations. We also show that this performance transfers to FSC-147, the standard class-agnostic counting dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的类型不承诺的计数方法只能 Count objects of a specific class, which has limited utility in many fields. Prior works have limited usefulness as they require either a set of examples of the type to be counted or that the image contains only a single type of object. A significant factor in these shortcomings is the lack of a dataset to properly address counting in settings with more than one kind of object present. To address these issues, we propose the first Multi-class, Class-Agnostic Counting dataset (MCAC) and A Blind Counter (ABC123), a method that can count multiple types of objects simultaneously without using examples of type during training or inference. ABC123 introduces a new paradigm where instead of requiring exemplars to guide the enumeration, examples are found after the counting stage to help a user understand the generated outputs. We show that ABC123 outperforms contemporary methods on MCAC without the requirement of human in-the-loop annotations. We also show that this performance transfers to FSC-147, the standard class-agnostic counting dataset.中文简体版：传统的类型不承诺的计数方法只能 Count objects of a specific class, 这限制了其在多个领域的应用 utility. 先前的工作受到了限制，因为它们需要 Either a set of examples of the type to be counted or that the image contains only a single type of object. 这些缺点中的一个重要因素是缺乏适用于多种对象的计数的数据集。 To address these issues, we propose the first Multi-class, Class-Agnostic Counting dataset (MCAC) and A Blind Counter (ABC123), a method that can count multiple types of objects simultaneously without using examples of type during training or inference. ABC123 introduces a new paradigm where instead of requiring exemplars to guide the enumeration, examples are found after the counting stage to help a user understand the generated outputs. We show that ABC123 outperforms contemporary methods on MCAC without the requirement of human in-the-loop annotations. We also show that this performance transfers to FSC-147, the standard class-agnostic counting dataset.
</details></li>
</ul>
<hr>
<h2 id="Detecting-Violations-of-Differential-Privacy-for-Quantum-Algorithms"><a href="#Detecting-Violations-of-Differential-Privacy-for-Quantum-Algorithms" class="headerlink" title="Detecting Violations of Differential Privacy for Quantum Algorithms"></a>Detecting Violations of Differential Privacy for Quantum Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04819">http://arxiv.org/abs/2309.04819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ji Guan, Wang Fang, Mingyu Huang, Mingsheng Ying</li>
<li>for: This paper aims to provide a formal framework for detecting violations of differential privacy in quantum algorithms, and to develop a detection algorithm that can automatically generate bugging information when a violation is reported.</li>
<li>methods: The proposed algorithm uses Tensor Networks, a highly efficient data structure, and is executed on TensorFlow Quantum and TorchQuantum, the quantum extensions of TensorFlow and PyTorch respectively.</li>
<li>results: The algorithm is effective and efficient, as demonstrated by experimental results on realistic quantum computers, including quantum supremacy algorithms, quantum machine learning models, quantum approximate optimization algorithms, and variational quantum eigensolvers with up to 21 quantum bits.<details>
<summary>Abstract</summary>
Quantum algorithms for solving a wide range of practical problems have been proposed in the last ten years, such as data search and analysis, product recommendation, and credit scoring. The concern about privacy and other ethical issues in quantum computing naturally rises up. In this paper, we define a formal framework for detecting violations of differential privacy for quantum algorithms. A detection algorithm is developed to verify whether a (noisy) quantum algorithm is differentially private and automatically generate bugging information when the violation of differential privacy is reported. The information consists of a pair of quantum states that violate the privacy, to illustrate the cause of the violation. Our algorithm is equipped with Tensor Networks, a highly efficient data structure, and executed both on TensorFlow Quantum and TorchQuantum which are the quantum extensions of famous machine learning platforms -- TensorFlow and PyTorch, respectively. The effectiveness and efficiency of our algorithm are confirmed by the experimental results of almost all types of quantum algorithms already implemented on realistic quantum computers, including quantum supremacy algorithms (beyond the capability of classical algorithms), quantum machine learning models, quantum approximate optimization algorithms, and variational quantum eigensolvers with up to 21 quantum bits.
</details>
<details>
<summary>摘要</summary>
近十年内，有许多关于解决实际问题的量子算法提议，如数据搜索和分析、产品推荐和借记评分。随着量子计算技术的发展，关注隐私和其他伦理问题的担忧自然而生。本文提出了一种形式化的检测方案，用于检测量子算法中的不同敏感度隐私泄露。我们开发了一种检测算法，用于验证（含噪）量子算法是否遵循不同敏感度隐私原则，并自动生成遵循不同敏感度隐私原则的报警信息。报警信息包括两个量子态，用于说明遵循不同敏感度隐私原则的原因。我们的算法采用了紧凑网络，一种高效的数据结构，并在TensorFlow Quantum和TorchQuantum上执行，这两者分别是классической机器学习平台TensorFlow和PyTorch的量子扩展。我们的实验结果表明，我们的算法在实际量子计算机器上具有高效和高可靠性。
</details></li>
</ul>
<hr>
<h2 id="Good-looking-but-Lacking-Faithfulness-Understanding-Local-Explanation-Methods-through-Trend-based-Testing"><a href="#Good-looking-but-Lacking-Faithfulness-Understanding-Local-Explanation-Methods-through-Trend-based-Testing" class="headerlink" title="Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing"></a>Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05679">http://arxiv.org/abs/2309.05679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jenniferho97/xai-trend-test">https://github.com/jenniferho97/xai-trend-test</a></li>
<li>paper_authors: Jinwen He, Kai Chen, Guozhu Meng, Jiangshan Zhang, Congyi Li</li>
<li>for: 本研究旨在评估模型决策的解释方法的准确性，并提出三种趋势基于的准确性测试方法，以解决传统测试中随机选择问题。</li>
<li>methods: 本研究使用了多种现有的解释方法，并对其进行了评估。同时，研究人员还提出了三种新的趋势基于的准确性测试方法。</li>
<li>results: 研究人员通过对多种数据集进行测试，发现传统测试方法受到随机选择问题的影响，而新提出的趋势基于测试方法可以更好地评估解释方法的准确性。此外，研究人员还发现，通过使用 faithful explanation methods，可以更好地检测和修复模型的准确性和安全问题。<details>
<summary>Abstract</summary>
While enjoying the great achievements brought by deep learning (DL), people are also worried about the decision made by DL models, since the high degree of non-linearity of DL models makes the decision extremely difficult to understand. Consequently, attacks such as adversarial attacks are easy to carry out, but difficult to detect and explain, which has led to a boom in the research on local explanation methods for explaining model decisions. In this paper, we evaluate the faithfulness of explanation methods and find that traditional tests on faithfulness encounter the random dominance problem, \ie, the random selection performs the best, especially for complex data. To further solve this problem, we propose three trend-based faithfulness tests and empirically demonstrate that the new trend tests can better assess faithfulness than traditional tests on image, natural language and security tasks. We implement the assessment system and evaluate ten popular explanation methods. Benefiting from the trend tests, we successfully assess the explanation methods on complex data for the first time, bringing unprecedented discoveries and inspiring future research. Downstream tasks also greatly benefit from the tests. For example, model debugging equipped with faithful explanation methods performs much better for detecting and correcting accuracy and security problems.
</details>
<details>
<summary>摘要</summary>
在享受深度学习（DL）的伟大成就时，人们也担心DL模型做出的决策，因为DL模型的高度非线性使决策变得极Difficult to understand。因此，袭击如 adversarial attacks 易于实施，但Difficult to detect and explain，这导致了对模型决策的解释方法的研究繁荣。在这篇论文中，我们评估解释方法的准确性，发现传统的准确性测试陷入随机控制问题，即随机选择perform the best， especial for complex data。为了解决这问题，我们提出三种趋势基于的准确性测试，并employs empirical demonstration that the new trend tests can better assess faithfulness than traditional tests on image, natural language and security tasks. We implement the assessment system and evaluate ten popular explanation methods. Benefiting from the trend tests, we successfully assess the explanation methods on complex data for the first time, bringing unprecedented discoveries and inspiring future research. Downstream tasks also greatly benefit from the tests. For example, model debugging equipped with faithful explanation methods performs much better for detecting and correcting accuracy and security problems.
</details></li>
</ul>
<hr>
<h2 id="Neural-Latent-Geometry-Search-Product-Manifold-Inference-via-Gromov-Hausdorff-Informed-Bayesian-Optimization"><a href="#Neural-Latent-Geometry-Search-Product-Manifold-Inference-via-Gromov-Hausdorff-Informed-Bayesian-Optimization" class="headerlink" title="Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization"></a>Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04810">http://arxiv.org/abs/2309.04810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haitz Saez de Ocariz Borde, Alvaro Arroyo, Ismael Morales, Ingmar Posner, Xiaowen Dong<br>for: 这种研究的目的是提高机器学习模型的性能，通过调整 latent space 的几何结构来更好地模型 latent space 和下游任务之间的关系。methods: 这种方法使用了 hyperbolic 和 spherical space 作为 constant curvature 的 latent space，并使用 Gromov-Hausdorff distance 来比较不同的 latent geometry 的距离。在计算 Gromov-Hausdorff distance 时，我们引入了一种映射函数，以将不同的 manifold 映射到共同的高维抽象空间中，从而使得 comparison  posible。results: 实验表明，这种方法可以减少 query 数量，并且可以有效地搜索出最佳的 latent geometry，以提高机器学习模型的性能。这种方法可以应用于多种模型和下游任务。<details>
<summary>Abstract</summary>
Recent research indicates that the performance of machine learning models can be improved by aligning the geometry of the latent space with the underlying data structure. Rather than relying solely on Euclidean space, researchers have proposed using hyperbolic and spherical spaces with constant curvature, or combinations thereof, to better model the latent space and enhance model performance. However, little attention has been given to the problem of automatically identifying the optimal latent geometry for the downstream task. We mathematically define this novel formulation and coin it as neural latent geometry search (NLGS). More specifically, we introduce a principled method that searches for a latent geometry composed of a product of constant curvature model spaces with minimal query evaluations. To accomplish this, we propose a novel notion of distance between candidate latent geometries based on the Gromov-Hausdorff distance from metric geometry. In order to compute the Gromov-Hausdorff distance, we introduce a mapping function that enables the comparison of different manifolds by embedding them in a common high-dimensional ambient space. Finally, we design a graph search space based on the calculated distances between candidate manifolds and use Bayesian optimization to search for the optimal latent geometry in a query-efficient manner. This is a general method which can be applied to search for the optimal latent geometry for a variety of models and downstream tasks. Extensive experiments on synthetic and real-world datasets confirm the efficacy of our method in identifying the optimal latent geometry for multiple machine learning problems.
</details>
<details>
<summary>摘要</summary>
近期研究表明，机器学习模型的性能可以通过调整秘密空间的几何结构来提高。而不是仅仅采用欧几何空间，研究人员已经提议使用弯曲和球形空间或这些空间的组合来更好地模型秘密空间，从而提高模型性能。然而，对于找到最佳秘密空间的自动化问题尚未受到充分关注。我们在这篇文章中Mathematically定义了这个新的形ulation，并称之为神经秘密空间搜索（NLGS）。更 specifically，我们提出了一种原则性的方法，该方法在最小的查询评估数量下搜索一个由常数曲率模型空间组成的秘密空间。为了实现这一点，我们提出了一种新的距离函数，该函数基于几何 геомет里的Gromov-Hausdorff距离来衡量不同的秘密空间之间的距离。为了计算Gromov-Hausdorff距离，我们引入了一种映射函数，该函数将不同的抽象空间映射到一个共同的高维空间中。最后，我们设计了一个基于计算的距离之间的搜索空间，并使用 Bayesian优化来寻找最佳的秘密空间。这种方法可以应用于多种模型和下游任务中搜索最佳的秘密空间。我们在 sintetic和实际数据集上进行了广泛的实验，并证明了我们的方法可以有效地Identify最佳秘密空间 для多个机器学习问题。
</details></li>
</ul>
<hr>
<h2 id="A-Full-fledged-Commit-Message-Quality-Checker-Based-on-Machine-Learning"><a href="#A-Full-fledged-Commit-Message-Quality-Checker-Based-on-Machine-Learning" class="headerlink" title="A Full-fledged Commit Message Quality Checker Based on Machine Learning"></a>A Full-fledged Commit Message Quality Checker Based on Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04797">http://arxiv.org/abs/2309.04797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/commit-message-collective/beams-commit-message-checker">https://github.com/commit-message-collective/beams-commit-message-checker</a></li>
<li>paper_authors: David Faragó, Michael Färber, Christian Petrov</li>
<li>for: 提高 commits 质量，包括含义和上下文，以便更好地维护和演化软件。</li>
<li>methods: 使用机器学习方法测试 commits 质量，包括遵循最流行的质量指南所有规则。</li>
<li>results: 通过训练和评估当今最佳机器学习模型，可以达到82.9%的 F$_1$ 分数，用于解决最复杂的任务。<details>
<summary>Abstract</summary>
Commit messages (CMs) are an essential part of version control. By providing important context in regard to what has changed and why, they strongly support software maintenance and evolution. But writing good CMs is difficult and often neglected by developers. So far, there is no tool suitable for practice that automatically assesses how well a CM is written, including its meaning and context. Since this task is challenging, we ask the research question: how well can the CM quality, including semantics and context, be measured with machine learning methods? By considering all rules from the most popular CM quality guideline, creating datasets for those rules, and training and evaluating state-of-the-art machine learning models to check those rules, we can answer the research question with: sufficiently well for practice, with the lowest F$_1$ score of 82.9\%, for the most challenging task. We develop a full-fledged open-source framework that checks all these CM quality rules. It is useful for research, e.g., automatic CM generation, but most importantly for software practitioners to raise the quality of CMs and thus the maintainability and evolution speed of their software.
</details>
<details>
<summary>摘要</summary>
版本控制中的提交信息（CM）是软件维护和演化的重要组成部分。它们提供了改变的重要上下文和原因，以支持软件的维护和演化。但是编写好CM是困难的，并且开发者frequently Neglects it。到目前为止，没有一个适合实践的工具可以自动评估CM的质量，包括它的 semantics和context。因此，我们提出了一个研究问题：可以使用机器学习方法来评估CM质量，包括semantics和context？我们考虑了最受欢迎的CM质量指南中的所有规则，创建了相应的数据集，并使用了当前最佳的机器学习模型来检查这些规则。我们发现，使用机器学习方法可以评估CM质量，并且最低的F1分数为82.9%，这是最复杂的任务。我们开发了一个全功能的开源框架，可以检查所有CM质量规则。它不仅有用于研究，例如自动生成CM，而且更重要的是为软件实践人员提高CM质量，从而提高软件的维护和演化速度。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Gradient-Descent-outperforms-Gradient-Descent-in-recovering-a-high-dimensional-signal-in-a-glassy-energy-landscape"><a href="#Stochastic-Gradient-Descent-outperforms-Gradient-Descent-in-recovering-a-high-dimensional-signal-in-a-glassy-energy-landscape" class="headerlink" title="Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional signal in a glassy energy landscape"></a>Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional signal in a glassy energy landscape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04788">http://arxiv.org/abs/2309.04788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Persia Jana Kamali, Pierfrancesco Urbani</li>
<li>for: 该论文主要研究了Stochastic Gradient Descent（SGD）算法在高维非对称优化问题中的效果，以及SGD与Gradient Descent（GD）算法之间的比较。</li>
<li>methods: 该论文使用了动态均衡理论来分析SGD算法在高维限制下的性能，并对SGD和GD算法进行比较。</li>
<li>results: 研究发现，SGD算法在高维非对称优化问题中表现比GD算法更好，具体来说，SGD算法的恢复阈值在小批处理情况下比GD算法要小。<details>
<summary>Abstract</summary>
Stochastic Gradient Descent (SGD) is an out-of-equilibrium algorithm used extensively to train artificial neural networks. However very little is known on to what extent SGD is crucial for to the success of this technology and, in particular, how much it is effective in optimizing high-dimensional non-convex cost functions as compared to other optimization algorithms such as Gradient Descent (GD). In this work we leverage dynamical mean field theory to analyze exactly its performances in the high-dimensional limit. We consider the problem of recovering a hidden high-dimensional non-linearly encrypted signal, a prototype high-dimensional non-convex hard optimization problem. We compare the performances of SGD to GD and we show that SGD largely outperforms GD. In particular, a power law fit of the relaxation time of these algorithms shows that the recovery threshold for SGD with small batch size is smaller than the corresponding one of GD.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RRCNN-An-Enhanced-Residual-Recursive-Convolutional-Neural-Network-for-Non-stationary-Signal-Decomposition"><a href="#RRCNN-An-Enhanced-Residual-Recursive-Convolutional-Neural-Network-for-Non-stationary-Signal-Decomposition" class="headerlink" title="RRCNN$^{+}$: An Enhanced Residual Recursive Convolutional Neural Network for Non-stationary Signal Decomposition"></a>RRCNN$^{+}$: An Enhanced Residual Recursive Convolutional Neural Network for Non-stationary Signal Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04782">http://arxiv.org/abs/2309.04782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhoudafa08/RRCNN_plus">https://github.com/zhoudafa08/RRCNN_plus</a></li>
<li>paper_authors: Feng Zhou, Antonio Cicone, Haomin Zhou</li>
<li>for: 该研究旨在提高非站立波形分解方法，以提高时频分析的精度和效率。</li>
<li>methods: 该研究使用了一种新的差分征分法，即差分征回归神经网络（RRCNN），以及一些深度学习和优化技术来改进非站立波形分解方法。</li>
<li>results: 研究表明，RRCNN可以在大规模信号批处理中实现更稳定的分解，并且可以提高时频分析的精度和效率。<details>
<summary>Abstract</summary>
Time-frequency analysis is an important and challenging task in many applications. Fourier and wavelet analysis are two classic methods that have achieved remarkable success in many fields. They also exhibit limitations when applied to nonlinear and non-stationary signals. To address this challenge, a series of nonlinear and adaptive methods, pioneered by the empirical mode decomposition method have been proposed. Their aim is to decompose a non-stationary signal into quasi-stationary components which reveal better features in the time-frequency analysis. Recently, inspired by deep learning, we proposed a novel method called residual recursive convolutional neural network (RRCNN). Not only RRCNN can achieve more stable decomposition than existing methods while batch processing large-scale signals with low computational cost, but also deep learning provides a unique perspective for non-stationary signal decomposition. In this study, we aim to further improve RRCNN with the help of several nimble techniques from deep learning and optimization to ameliorate the method and overcome some of the limitations of this technique.
</details>
<details>
<summary>摘要</summary>
时频分析是许多应用领域中的重要和挑战性任务。法ouvrier和浪涌分析是经典的方法，在许多领域取得了很大成功。但它们在非线性和不稳定信号处理时表现出限制。为了解决这个挑战，一系列基于实验模式分解方法已经被提出，其目的是将非稳定信号分解成可见更好的时频特征。最近，深度学习的灵感下，我们提出了一种新的方法 called residual recursive convolutional neural network (RRCNN)。除了RRCNN可以在批处理大规模信号时实现更稳定的分解，同时计算成本也很低，而且深度学习带来了非站ARY信号分解的新视角。在这项研究中，我们想使用深度学习和优化技术来改进RRCNN，并解决一些这种技术的限制。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-Model-Watermark-via-Reducing-Parametric-Vulnerability"><a href="#Towards-Robust-Model-Watermark-via-Reducing-Parametric-Vulnerability" class="headerlink" title="Towards Robust Model Watermark via Reducing Parametric Vulnerability"></a>Towards Robust Model Watermark via Reducing Parametric Vulnerability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04777">http://arxiv.org/abs/2309.04777</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guanhaogan/robust-model-watermarking">https://github.com/guanhaogan/robust-model-watermarking</a></li>
<li>paper_authors: Guanhao Gan, Yiming Li, Dongxian Wu, Shu-Tao Xia</li>
<li>for: 保护深度神经网络（DNN）的版权，防止其被不当使用或盗用。</li>
<li>methods: 使用后门机制（backdoor）进行权利识别，在发布模型时 embed 特定的后门行为，以便在使用模型时能够识别是否有人对模型进行了非法修改或使用。</li>
<li>results: 发现 watermark 可能被移除的问题，并提出了一种 mini-max 形式来恢复 watermark 行为，并在各种 parametric 变化和 watermark-removal 攻击下进行了广泛的实验，证明了该方法可以提高模型 watermarking 的Robustness。<details>
<summary>Abstract</summary>
Deep neural networks are valuable assets considering their commercial benefits and huge demands for costly annotation and computation resources. To protect the copyright of DNNs, backdoor-based ownership verification becomes popular recently, in which the model owner can watermark the model by embedding a specific backdoor behavior before releasing it. The defenders (usually the model owners) can identify whether a suspicious third-party model is ``stolen'' from them based on the presence of the behavior. Unfortunately, these watermarks are proven to be vulnerable to removal attacks even like fine-tuning. To further explore this vulnerability, we investigate the parameter space and find there exist many watermark-removed models in the vicinity of the watermarked one, which may be easily used by removal attacks. Inspired by this finding, we propose a mini-max formulation to find these watermark-removed models and recover their watermark behavior. Extensive experiments demonstrate that our method improves the robustness of the model watermarking against parametric changes and numerous watermark-removal attacks. The codes for reproducing our main experiments are available at \url{https://github.com/GuanhaoGan/robust-model-watermarking}.
</details>
<details>
<summary>摘要</summary>
深度神经网络是非常有价值的资产，因为它们在商业上有很大的市场需求和高昂的注入和计算资源成本。为了保护神经网络的版权，在最近几年内，以下行为检查成为了非常流行的方法：在发布神经网络之前，将模型嵌入特定的后门行为。这样，作者可以识别他们的模型是否被“盗”使用，并且可以通过检查后门行为来确定。然而，这些后门嵌入在 watermark 的存在下是易受到移除攻击的。为了更深入了解这一点，我们对参数空间进行了调查，并发现了许多 watermark 已经被移除的模型，这些模型可能容易被用于移除攻击。受这一发现的激发，我们提出了一种最大化-最小化形式来找到这些 watermark 已经被移除的模型，并将其恢复到原始的 watermark 行为。我们的方法可以增强神经网络嵌入的模型水印的可靠性，并对于不同的参数变化和多种水印移除攻击表现出良好的效果。相关的代码可以在 <https://github.com/GuanhaoGan/robust-model-watermarking> 上找到，以便进行重现主要实验。
</details></li>
</ul>
<hr>
<h2 id="AudRandAug-Random-Image-Augmentations-for-Audio-Classification"><a href="#AudRandAug-Random-Image-Augmentations-for-Audio-Classification" class="headerlink" title="AudRandAug: Random Image Augmentations for Audio Classification"></a>AudRandAug: Random Image Augmentations for Audio Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04762">http://arxiv.org/abs/2309.04762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/turab45/audrandaug">https://github.com/turab45/audrandaug</a></li>
<li>paper_authors: Teerath Kumar, Muhammad Turab, Alessandra Mileo, Malika Bendechache, Takfarinas Saber</li>
<li>for: 增强 neural network 训练效果</li>
<li>methods: 随机选择数据增强技术从预定搜索空间</li>
<li>results: 对各种模型和数据集进行实验，发现 AudRandAug 的准确性表现比其他数据增强方法更高。<details>
<summary>Abstract</summary>
Data augmentation has proven to be effective in training neural networks. Recently, a method called RandAug was proposed, randomly selecting data augmentation techniques from a predefined search space. RandAug has demonstrated significant performance improvements for image-related tasks while imposing minimal computational overhead. However, no prior research has explored the application of RandAug specifically for audio data augmentation, which converts audio into an image-like pattern. To address this gap, we introduce AudRandAug, an adaptation of RandAug for audio data. AudRandAug selects data augmentation policies from a dedicated audio search space. To evaluate the effectiveness of AudRandAug, we conducted experiments using various models and datasets. Our findings indicate that AudRandAug outperforms other existing data augmentation methods regarding accuracy performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Data augmentation has proven to be effective in training neural networks. Recently, a method called RandAug was proposed, randomly selecting data augmentation techniques from a predefined search space. RandAug has demonstrated significant performance improvements for image-related tasks while imposing minimal computational overhead. However, no prior research has explored the application of RandAug specifically for audio data augmentation, which converts audio into an image-like pattern. To address this gap, we introduce AudRandAug, an adaptation of RandAug for audio data. AudRandAug selects data augmentation policies from a dedicated audio search space. To evaluate the effectiveness of AudRandAug, we conducted experiments using various models and datasets. Our findings indicate that AudRandAug outperforms other existing data augmentation methods regarding accuracy performance." into 简体中文Here's the translation:数据增强已经证明对神经网络训练有效。最近，一种方法叫做RandAug被提出，随机从预定搜索空间中选择数据增强策略。RandAug在图像相关任务上显示了显著性能提升，而且对计算负担占有最小化。然而，没有先前的研究探讨了将RandAug专门应用于音频数据增强，即将音频转换为图像样式。为了填补这个空白，我们介绍了AudRandAug，它是RandAug的音频数据变换版本。AudRandAug从专门的音频搜索空间中选择数据增强策略。为了评估AudRandAug的效果，我们使用了不同的模型和数据集进行实验。我们的发现表明，AudRandAug在准确性性能方面超过了现有的数据增强方法。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Survey-on-Deep-Learning-Techniques-in-Educational-Data-Mining"><a href="#A-Comprehensive-Survey-on-Deep-Learning-Techniques-in-Educational-Data-Mining" class="headerlink" title="A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining"></a>A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04761">http://arxiv.org/abs/2309.04761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanguo Lin, Hong Chen, Wei Xia, Fan Lin, Pengcheng Wu, Zongyue Wang, Yong Li</li>
<li>for: 这篇论文主要用于系统性地介绍现代教育中使用深度学习技术的教育数据挖掘（EDM）。</li>
<li>methods: 本文使用深度学习技术来分析和模型教育数据，包括知识追踪、不良学生检测、性能预测和个性化推荐等四种教育场景。</li>
<li>results: 本文提供了一系列公共数据集和处理工具，并概括了深度学习在 EDM 中的应用和前景。<details>
<summary>Abstract</summary>
Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
</details>
<details>
<summary>摘要</summary>
教育数据挖掘（EDM）已经成为现代教育研究的重要领域，利用计算机技术来分析教育数据。随着教育数据的复杂度和多样性的增加，深度学习技术在处理和建模这些数据方面表现出了显著的优势。本文尝试系统地综述现代EDM领域中深度学习的状况。我们首先介绍EDM和深度学习的基本概念，并将其应用于现代教育中的四个典型场景：知识跟踪、不良学生检测、性能预测和个性化推荐。此外，我们还提供了教育数据公共数据集和处理工具的全面回顾。最后，我们讨论了现代教育研究领域的发展趋势和未来方向。
</details></li>
</ul>
<hr>
<h2 id="Gromov-Hausdorff-Distances-for-Comparing-Product-Manifolds-of-Model-Spaces"><a href="#Gromov-Hausdorff-Distances-for-Comparing-Product-Manifolds-of-Model-Spaces" class="headerlink" title="Gromov-Hausdorff Distances for Comparing Product Manifolds of Model Spaces"></a>Gromov-Hausdorff Distances for Comparing Product Manifolds of Model Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05678">http://arxiv.org/abs/2309.05678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haitz Saez de Ocariz Borde, Alvaro Arroyo, Ismael Morales, Ingmar Posner, Xiaowen Dong</li>
<li>for: 提高机器学习模型的性能，通过将潜在空间的几何特征与数据结构相匹配。</li>
<li>methods: 使用不同的欧几何空间和圆板空间，或者其组合，并使用图搜索空间来搜索最佳潜在几何。</li>
<li>results: 提出了一种新的距离度量方法，用于 comparing candidate latent geometries，并实现了其计算方法。<details>
<summary>Abstract</summary>
Recent studies propose enhancing machine learning models by aligning the geometric characteristics of the latent space with the underlying data structure. Instead of relying solely on Euclidean space, researchers have suggested using hyperbolic and spherical spaces with constant curvature, or their combinations (known as product manifolds), to improve model performance. However, there exists no principled technique to determine the best latent product manifold signature, which refers to the choice and dimensionality of manifold components. To address this, we introduce a novel notion of distance between candidate latent geometries using the Gromov-Hausdorff distance from metric geometry. We propose using a graph search space that uses the estimated Gromov-Hausdorff distances to search for the optimal latent geometry. In this work we focus on providing a description of an algorithm to compute the Gromov-Hausdorff distance between model spaces and its computational implementation.
</details>
<details>
<summary>摘要</summary>
最近的研究提议通过将 latent space 的几何特征与数据结构相对应来提高机器学习模型的性能。而不是仅仅采用欧几何空间，研究人员建议使用偏折射空间和球形空间，或者这两种空间的组合（称为产品 manifold），以提高模型表现。然而，选择最佳 latent product manifold 签名仍然存在无原则的问题。为解决这个问题，我们提出了一种新的 latent geometry 评估方法，基于 metric geometry 中的 Gromov-Hausdorff 距离。我们建议使用图earch space 来搜索最佳 latent geometry。在这篇文章中，我们将关注提出一种算法来计算 Gromov-Hausdorff 距离 между模型空间，以及其计算实现方法。
</details></li>
</ul>
<hr>
<h2 id="RR-CP-Reliable-Region-Based-Conformal-Prediction-for-Trustworthy-Medical-Image-Classification"><a href="#RR-CP-Reliable-Region-Based-Conformal-Prediction-for-Trustworthy-Medical-Image-Classification" class="headerlink" title="RR-CP: Reliable-Region-Based Conformal Prediction for Trustworthy Medical Image Classification"></a>RR-CP: Reliable-Region-Based Conformal Prediction for Trustworthy Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04760">http://arxiv.org/abs/2309.04760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhe Zhang, Shuo Wang, Yejia Zhang, Danny Z. Chen</li>
<li>for:  This paper aims to improve the accuracy and efficiency of conformal prediction (CP) in medical AI decision-making.</li>
<li>methods: The proposed method, Reliable-Region-Based Conformal Prediction (RR-CP), uses a stronger statistical guarantee to achieve a user-specified error rate (e.g., 0.5%) while optimizing the size of the prediction set.</li>
<li>results: Experimental results on five public datasets show that RR-CP performs well, achieving the user-specified error rate significantly more frequently than existing CP methods, with a reasonably small-sized prediction set.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文目标是提高医疗AI决策中的准确率和效率，通过改进准确预测（CP）方法。</li>
<li>methods: 提议的方法是可靠区域基于准确预测（RR-CP），通过更加强的统计保证来实现用户指定的错误率（例如0.5%），同时优化预测集的大小。</li>
<li>results: 实验结果表明，RR-CP在五个公共数据集上表现良好，能够达到用户指定的错误率（例如0.5%），与现有CP方法相比，具有更加小的预测集大小。<details>
<summary>Abstract</summary>
Conformal prediction (CP) generates a set of predictions for a given test sample such that the prediction set almost always contains the true label (e.g., 99.5\% of the time). CP provides comprehensive predictions on possible labels of a given test sample, and the size of the set indicates how certain the predictions are (e.g., a set larger than one is `uncertain'). Such distinct properties of CP enable effective collaborations between human experts and medical AI models, allowing efficient intervention and quality check in clinical decision-making. In this paper, we propose a new method called Reliable-Region-Based Conformal Prediction (RR-CP), which aims to impose a stronger statistical guarantee so that the user-specified error rate (e.g., 0.5\%) can be achieved in the test time, and under this constraint, the size of the prediction set is optimized (to be small). We consider a small prediction set size an important measure only when the user-specified error rate is achieved. Experiments on five public datasets show that our RR-CP performs well: with a reasonably small-sized prediction set, it achieves the user-specified error rate (e.g., 0.5\%) significantly more frequently than exiting CP methods.
</details>
<details>
<summary>摘要</summary>
具有遵循性预测（CP）生成测试样本的集合，其中集合中的标签几乎总是正确的（例如，99.5%的时间）。CP提供了测试样本可能的标签预测，并且预测集合的大小表示预测的置信度（例如，一个集合大于一个是不确定）。这些CP的特点使得人工专家和医疗AI模型之间的合作更加有效，可以快速 intervene和质量检查在临床决策中。在这篇论文中，我们提出了一种新的方法called Reliable-Region-Based Conformal Prediction（RR-CP），该方法的目的是在测试时间内，通过更加强的统计保证，使用户指定的错误率（例如，0.5%）得到实现，并在这个约束下优化预测集合的大小（使其小）。我们认为在实现用户指定的错误率时，小型预测集合是重要的指标。在五个公共数据集上进行了实验，我们的RR-CP表现良好：与其他CP方法相比，它在实现用户指定的错误率（例如，0.5%）时，有reasonably小的预测集合，并且能够达到用户指定的错误率，而其他CP方法则不能达到这个目标。
</details></li>
</ul>
<hr>
<h2 id="Affine-Invariant-Ensemble-Transform-Methods-to-Improve-Predictive-Uncertainty-in-ReLU-Networks"><a href="#Affine-Invariant-Ensemble-Transform-Methods-to-Improve-Predictive-Uncertainty-in-ReLU-Networks" class="headerlink" title="Affine Invariant Ensemble Transform Methods to Improve Predictive Uncertainty in ReLU Networks"></a>Affine Invariant Ensemble Transform Methods to Improve Predictive Uncertainty in ReLU Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04742">http://arxiv.org/abs/2309.04742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diksha Bhandari, Jakiw Pidstrigach, Sebastian Reich</li>
<li>for: 本研究旨在使用适当扩展的ensemble Kalman统计量perform Bayesian推论 для逻辑回传 regression。</li>
<li>methods: 本研究提出了两种互动运行的粒子系统，用于范畴 posterior的抽样，并证明了这些粒子系统在粒子数量趋向无限大时对mean-field限的量化快速传递率。</li>
<li>results: 本研究运用了这些技术，并将其应用于ReLU网络中的preditive uncertainty量化。<details>
<summary>Abstract</summary>
We consider the problem of performing Bayesian inference for logistic regression using appropriate extensions of the ensemble Kalman filter. Two interacting particle systems are proposed that sample from an approximate posterior and prove quantitative convergence rates of these interacting particle systems to their mean-field limit as the number of particles tends to infinity. Furthermore, we apply these techniques and examine their effectiveness as methods of Bayesian approximation for quantifying predictive uncertainty in ReLU networks.
</details>
<details>
<summary>摘要</summary>
我团队考虑使用适当的拓展 Ensemble Kalman 筛子来进行 bayesian 推断，以便更好地评估 ReLU 网络中预测不确定性。我们提出了两种互动的 particle 系统，这些系统可以从 approximate  posterior 中采样，并证明这些互动 particle 系统在粒度趋于无穷时，与其mean-field 限制之间存在量化的减少速率。此外，我们应用这些技术，并对它们在 ReLU 网络中的效果进行了评估。Here's the breakdown of the translation:* "We consider the problem of performing Bayesian inference for logistic regression" becomes 我团队考虑使用适当的拓展 Ensemble Kalman 筛子来进行 bayesian 推断 (wǒ tuán xiāng yù zhī yì yóu yì yóu zhī shì yì yóu bā yì yīn xìng)* "using appropriate extensions of the ensemble Kalman filter" becomes 使用适当的拓展 Ensemble Kalman 筛子 (fù zhī yì yóu yì yóu zhī shì)* "Two interacting particle systems are proposed" becomes 我们提出了两种互动的 particle 系统 (wǒ men tím zhāng le yī yī zhī yì yóu zhī xìng)* "that sample from an approximate posterior" becomes 这些系统可以从 approximate  posterior 中采样 (zhè xiē xìng zhī yì zhī yì yóu zhī xìng)* "and prove quantitative convergence rates of these interacting particle systems to their mean-field limit as the number of particles tends to infinity" becomes 并证明这些互动 particle 系统在粒度趋于无穷时，与其mean-field 限制之间存在量化的减少速率 (ér qiè míng yǐ jīn yǐ zhī yì yóu zhī xìng zhī shì yì yóu)* "Furthermore, we apply these techniques and examine their effectiveness as methods of Bayesian approximation for quantifying predictive uncertainty in ReLU networks" becomes 此外，我们应用这些技术，并对它们在 ReLU 网络中的效果进行了评估 (qí wài, wǒ men yì yì yóu zhī shì yì yóu zhī xìng zhī shì)
</details></li>
</ul>
<hr>
<h2 id="Training-of-Spiking-Neural-Network-joint-Curriculum-Learning-Strategy"><a href="#Training-of-Spiking-Neural-Network-joint-Curriculum-Learning-Strategy" class="headerlink" title="Training of Spiking Neural Network joint Curriculum Learning Strategy"></a>Training of Spiking Neural Network joint Curriculum Learning Strategy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04737">http://arxiv.org/abs/2309.04737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingling Tang, Jielei Chu, Zhiguo Gong, Tianrui Li</li>
<li>For: 提高神经网络模型的生物学可靠性，使其更像人类学习过程。* Methods: 引入课程学习策略（CL）到神经网络中，通过不同难度水平的样本进行自适应训练，提高模型的生物学可靠性和可解释性。* Results: 在静止图像集MNIST、Fashion-MNIST、CIFAR10以及神经元 datasets N-MNIST、CIFAR10-DVS、DVS-Gesture上进行了实验，结果很有 promise。据我们知道，这是首个通过引入CL提高神经网络模型的生物学可靠性的提议。<details>
<summary>Abstract</summary>
Starting with small and simple concepts, and gradually introducing complex and difficult concepts is the natural process of human learning. Spiking Neural Networks (SNNs) aim to mimic the way humans process information, but current SNNs models treat all samples equally, which does not align with the principles of human learning and overlooks the biological plausibility of SNNs. To address this, we propose a CL-SNN model that introduces Curriculum Learning(CL) into SNNs, making SNNs learn more like humans and providing higher biological interpretability. CL is a training strategy that advocates presenting easier data to models before gradually introducing more challenging data, mimicking the human learning process. We use a confidence-aware loss to measure and process the samples with different difficulty levels. By learning the confidence of different samples, the model reduces the contribution of difficult samples to parameter optimization automatically. We conducted experiments on static image datasets MNIST, Fashion-MNIST, CIFAR10, and neuromorphic datasets N-MNIST, CIFAR10-DVS, DVS-Gesture. The results are promising. To our best knowledge, this is the first proposal to enhance the biologically plausibility of SNNs by introducing CL.
</details>
<details>
<summary>摘要</summary>
人类学习的自然过程是从简单到复杂，逐渐引入复杂和困难的概念。神经网络模型（SNN）想要模仿人类信息处理的方式，但现有SNN模型很多样本都受到相同的处理，这并不符合人类学习的原理，而且忽略了SNN的生物可能性。为了解决这个问题，我们提出了CL-SNN模型，该模型将curriculum学习（CL）引入SNN，使SNN更像人类学习的方式，并提供更高的生物可能性。CL是一种训练策略，它认为在训练过程中先给模型推送 easiest样本，然后逐渐增加更复杂的样本，这与人类学习过程相似。我们使用一种自信度感知损失函数来评估和处理不同难度水平的样本。通过学习不同样本的自信度，模型会自动减少最难样本对参数优化的贡献。我们在静止图像集MNIST、Fashion-MNIST、CIFAR10以及神经网络集N-MNIST、CIFAR10-DVS、DVS-Gesture上进行了实验。结果很有前途。我们知道，这是首次通过引入CL来增强SNN的生物可能性的提议。
</details></li>
</ul>
<hr>
<h2 id="A-Spatiotemporal-Deep-Neural-Network-for-Fine-Grained-Multi-Horizon-Wind-Prediction"><a href="#A-Spatiotemporal-Deep-Neural-Network-for-Fine-Grained-Multi-Horizon-Wind-Prediction" class="headerlink" title="A Spatiotemporal Deep Neural Network for Fine-Grained Multi-Horizon Wind Prediction"></a>A Spatiotemporal Deep Neural Network for Fine-Grained Multi-Horizon Wind Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04733">http://arxiv.org/abs/2309.04733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hfl15/windpred">https://github.com/hfl15/windpred</a></li>
<li>paper_authors: Fanling Huang, Yangdong Deng</li>
<li>for: 预测风速和方向，帮助多个实际应用如航空和风力发电等，由于天气数据中高度随机性和复杂相关性，预测具有挑战性。</li>
<li>methods: 我们提出了一种新的数据驱动模型，即多个时空网络（MHSTN），用于准确和高效地预测细化风速和方向。MHSTN通过组合多个深度神经网络，以Sequence-to-Sequence（Seq2Seq）脊梁来有效地提取不同数据源的特征，并生成所有站点的多个预测 horizon。</li>
<li>results: 我们的模型在一个中国最繁忙的国际机场的调度平台中已经整合，评估结果表明，我们的模型在竞争对手之上具有显著的优势。<details>
<summary>Abstract</summary>
The prediction of wind in terms of both wind speed and direction, which has a crucial impact on many real-world applications like aviation and wind power generation, is extremely challenging due to the high stochasticity and complicated correlation in the weather data. Existing methods typically focus on a sub-set of influential factors and thus lack a systematic treatment of the problem. In addition, fine-grained forecasting is essential for efficient industry operations, but has been less attended in the literature. In this work, we propose a novel data-driven model, Multi-Horizon SpatioTemporal Network (MHSTN), generally for accurate and efficient fine-grained wind prediction. MHSTN integrates multiple deep neural networks targeting different factors in a sequence-to-sequence (Seq2Seq) backbone to effectively extract features from various data sources and produce multi-horizon predictions for all sites within a given region. MHSTN is composed of four major modules. First, a temporal module fuses coarse-grained forecasts derived by Numerical Weather Prediction (NWP) and historical on-site observation data at stations so as to leverage both global and local atmospheric information. Second, a spatial module exploits spatial correlation by modeling the joint representation of all stations. Third, an ensemble module weighs the above two modules for final predictions. Furthermore, a covariate selection module automatically choose influential meteorological variables as initial input. MHSTN is already integrated into the scheduling platform of one of the busiest international airports of China. The evaluation results demonstrate that our model outperforms competitors by a significant margin.
</details>
<details>
<summary>摘要</summary>
“预测风速和方向的预测，对于许多实际应用来说，如航空业和风力发电等，是极其困难的，因为天气数据中存在高度的随机性和复杂的相关性。现有方法通常只关注一部分影响因素，因此缺乏系统性的处理。另外，细致的预测对于企业操作非常重要，但在文献中得到了更少的关注。在这种情况下，我们提出了一种新的数据驱动模型，即多个时空网络（Multi-Horizon SpatioTemporal Network，MHSTN），用于高精度和高效的细致风预测。MHSTN通过将多个深度神经网络组织成一个序列到序列（Seq2Seq）脊梁，以便从不同数据源中提取特征并生成所有站点的多个时间预测。MHSTN由四个主要模块组成：首先，一个时间模块将数值预测和历史站点观测数据 fusion，以利用全球和地方大气信息；其次，一个空间模块利用空间相关性，模型所有站点的联合表示；第三，一个 ensemble模块为最终预测做权衡；最后，一个 covariate选择模块自动选择影响气象变量为初始输入。MHSTN已经integrated到了中国一个最繁忙的国际机场的调度平台中。评估结果表明，我们的模型在竞争对手之上占据了显著优势。”
</details></li>
</ul>
<hr>
<h2 id="TCGAN-Convolutional-Generative-Adversarial-Network-for-Time-Series-Classification-and-Clustering"><a href="#TCGAN-Convolutional-Generative-Adversarial-Network-for-Time-Series-Classification-and-Clustering" class="headerlink" title="TCGAN: Convolutional Generative Adversarial Network for Time Series Classification and Clustering"></a>TCGAN: Convolutional Generative Adversarial Network for Time Series Classification and Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04732">http://arxiv.org/abs/2309.04732</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/lynn1/tcgan">https://bitbucket.org/lynn1/tcgan</a></li>
<li>paper_authors: Fanling Huang, Yangdong Deng</li>
<li>for: 本文针对时间序列资料的标本组成，实现分类和对应的目的。</li>
<li>methods: 本文使用了Generative Adversarial Networks (GANs)来学习时间序列资料的层次表现，并将部分训练好的TCGAN组件重复使用来构建表现Encoder，以强化线性识别方法。</li>
<li>results: 本文的实验结果显示，TCGAN比现有的时间序列GAN更快速和准确，并且可以实现简单的分类和对应。此外，TCGAN在具有偏数标签的数据集上也表现出高效性。<details>
<summary>Abstract</summary>
Recent works have demonstrated the superiority of supervised Convolutional Neural Networks (CNNs) in learning hierarchical representations from time series data for successful classification. These methods require sufficiently large labeled data for stable learning, however acquiring high-quality labeled time series data can be costly and potentially infeasible. Generative Adversarial Networks (GANs) have achieved great success in enhancing unsupervised and semi-supervised learning. Nonetheless, to our best knowledge, it remains unclear how effectively GANs can serve as a general-purpose solution to learn representations for time series recognition, i.e., classification and clustering. The above considerations inspire us to introduce a Time-series Convolutional GAN (TCGAN). TCGAN learns by playing an adversarial game between two one-dimensional CNNs (i.e., a generator and a discriminator) in the absence of label information. Parts of the trained TCGAN are then reused to construct a representation encoder to empower linear recognition methods. We conducted comprehensive experiments on synthetic and real-world datasets. The results demonstrate that TCGAN is faster and more accurate than existing time-series GANs. The learned representations enable simple classification and clustering methods to achieve superior and stable performance. Furthermore, TCGAN retains high efficacy in scenarios with few-labeled and imbalanced-labeled data. Our work provides a promising path to effectively utilize abundant unlabeled time series data.
</details>
<details>
<summary>摘要</summary>
Recent research has shown that supervised Convolutional Neural Networks (CNNs) can learn hierarchical representations from time series data for successful classification. However, these methods require a large amount of labeled data, which can be costly and potentially infeasible to obtain. Generative Adversarial Networks (GANs) have achieved great success in enhancing unsupervised and semi-supervised learning. However, it remains unclear how effectively GANs can serve as a general-purpose solution for learning representations for time series recognition, including classification and clustering.Inspired by these considerations, we introduce a Time-series Convolutional GAN (TCGAN). TCGAN learns by playing an adversarial game between two one-dimensional CNNs (i.e., a generator and a discriminator) in the absence of label information. Parts of the trained TCGAN are then reused to construct a representation encoder to empower linear recognition methods.We conducted comprehensive experiments on synthetic and real-world datasets. The results show that TCGAN is faster and more accurate than existing time-series GANs. The learned representations enable simple classification and clustering methods to achieve superior and stable performance. Furthermore, TCGAN retains high efficacy in scenarios with few-labeled and imbalanced-labeled data. Our work provides a promising path to effectively utilize abundant unlabeled time series data.
</details></li>
</ul>
<hr>
<h2 id="Transitions-in-echo-index-and-dependence-on-input-repetitions"><a href="#Transitions-in-echo-index-and-dependence-on-input-repetitions" class="headerlink" title="Transitions in echo index and dependence on input repetitions"></a>Transitions in echo index and dependence on input repetitions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04728">http://arxiv.org/abs/2309.04728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Ashwin, Andrea Ceni</li>
<li>for: 这篇论文研究了非自治（即输入驱动）动力系统中的零响应性。它总结了零响应性的定义和不同参数对零响应性的影响。</li>
<li>methods: 作者使用了非自治系统的切换 между一组finite maps的方法，其中每个map具有finite set of hyperbolic equilibrium attractors。</li>
<li>results: 作者发现了外部输入的激励强度对零响应性的影响。当输入强度较小时，零响应性与输入自由系统中的吸引器数量相同。当输入强度较大时，零响应性降低到1。在输入强度较小的 intermediate region 中，零响应性受到输入的更加细微特性的影响。<details>
<summary>Abstract</summary>
The echo index counts the number of simultaneously stable asymptotic responses of a nonautonomous (i.e. input-driven) dynamical system. It generalizes the well-known echo state property for recurrent neural networks - this corresponds to the echo index being equal to one. In this paper, we investigate how the echo index depends on parameters that govern typical responses to a finite-state ergodic external input that forces the dynamics. We consider the echo index for a nonautonomous system that switches between a finite set of maps, where we assume that each map possesses a finite set of hyperbolic equilibrium attractors. We find the minimum and maximum repetitions of each map are crucial for the resulting echo index. Casting our theoretical findings in the RNN computing framework, we obtain that for small amplitude forcing the echo index corresponds to the number of attractors for the input-free system, while for large amplitude forcing, the echo index reduces to one. The intermediate regime is the most interesting; in this region the echo index depends not just on the amplitude of forcing but also on more subtle properties of the input.
</details>
<details>
<summary>摘要</summary>
“弹回指数”（echo index）是非自主（即输入驱动）动力系统中同时稳定 asymptotic response 的数量的一个统计量。它推广了已知的弹回性质（echo property），这corresponds to the echo index being equal to one.在这篇论文中，我们研究了弹回指数如何受到外部输入的Parameter Governance的影响。我们考虑了一个非自主系统，该系统在一个有限个Map之间切换，并假设每个Map具有一个有限多个恒等平衡吸引器。我们发现，输入的最小和最大重复数对弹回指数有关键的。在将我们的理论发现转移到RNN计算框架中，我们得到了以下结论：对于小振幅的刺激，弹回指数与输入自由系统中的吸引器数量相同；对于大振幅的刺激，弹回指数减少到1。 intermediate regime 是最有趣的，在这个区域中，弹回指数不仅受到刺激振幅的影响，还受到输入的更加细微的性质的影响。
</details></li>
</ul>
<hr>
<h2 id="MultiCaM-Vis-Visual-Exploration-of-Multi-Classification-Model-with-High-Number-of-Classes"><a href="#MultiCaM-Vis-Visual-Exploration-of-Multi-Classification-Model-with-High-Number-of-Classes" class="headerlink" title="MultiCaM-Vis: Visual Exploration of Multi-Classification Model with High Number of Classes"></a>MultiCaM-Vis: Visual Exploration of Multi-Classification Model with High Number of Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05676">http://arxiv.org/abs/2309.05676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Ahsan Ali Dilawer, Shah Rukh Humayoun</li>
<li>for: 用于探索多类划分模型中大量类别之间的关系和问题的原因。</li>
<li>methods: 使用交互式视觉分析工具 MultiCaM-Vis，提供了概述+细节并行坐标视图和环形图表示法，以便对实例级别的错误分类进行探索和检查。</li>
<li>results: 在12名参与者的预liminary用户研究中，通过使用 MultiCaM-Vis 工具，参与者能够更好地理解和探索多类划分模型中的问题和关系。<details>
<summary>Abstract</summary>
Visual exploration of multi-classification models with large number of classes would help machine learning experts in identifying the root cause of a problem that occurs during learning phase such as miss-classification of instances. Most of the previous visual analytics solutions targeted only a few classes. In this paper, we present our interactive visual analytics tool, called MultiCaM-Vis, that provides \Emph{overview+detail} style parallel coordinate views and a Chord diagram for exploration and inspection of class-level miss-classification of instances. We also present results of a preliminary user study with 12 participants.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Visual 探索多类型模型的大量类型问题会帮助机器学习专家在学习阶段出现错误的原因。大多数前一代视觉分析解决方案仅针对几个类型。在这篇论文中，我们提出了我们的交互式视觉分析工具 MultiCaM-Vis，它提供了 \Emph{概述+详细} 样式的平行坐标图和一个弦图以便对实例的类别错误进行探索和检查。我们还发布了12名参与者的初步用户研究结果。Note: "概述+详细" (overview+detail) is a common phrase used to describe a visualization that provides both a high-level overview and detailed information.
</details></li>
</ul>
<hr>
<h2 id="SHAPE-A-Sample-adaptive-Hierarchical-Prediction-Network-for-Medication-Recommendation"><a href="#SHAPE-A-Sample-adaptive-Hierarchical-Prediction-Network-for-Medication-Recommendation" class="headerlink" title="SHAPE: A Sample-adaptive Hierarchical Prediction Network for Medication Recommendation"></a>SHAPE: A Sample-adaptive Hierarchical Prediction Network for Medication Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05675">http://arxiv.org/abs/2309.05675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sicen Liu, Xiaolong Wang, JIngcheng Du, Yongshuai Hou, Xianbing Zhao, Hui Xu, Hui Wang, Yang Xiang, Buzhou Tang</li>
<li>for: 这个研究旨在提高医疗保健中复杂多疾病的药物建议方法。</li>
<li>methods: 本研究提出了一个名为SHAPE的新型Sample-adaptive Hierarchical medicAtion Prediction nEtwork，以解决上述挑战。特别是，我们设计了一个高效的内部预测器，以将医疗事件中的关系编码为访问水平表现，然后开发了一个滑动学习方法来有效地学习患者水平的长期预测。</li>
<li>results: 我们的模型在一个 benchmark 数据集上进行了广泛的实验，与多个现有的基eline模型进行比较，结果显示了我们的模型在药物建议任务中的超过其他模型的优势。<details>
<summary>Abstract</summary>
Effectively medication recommendation with complex multimorbidity conditions is a critical task in healthcare. Most existing works predicted medications based on longitudinal records, which assumed the information transmitted patterns of learning longitudinal sequence data are stable and intra-visit medical events are serialized. However, the following conditions may have been ignored: 1) A more compact encoder for intra-relationship in the intra-visit medical event is urgent; 2) Strategies for learning accurate representations of the variable longitudinal sequences of patients are different. In this paper, we proposed a novel Sample-adaptive Hierarchical medicAtion Prediction nEtwork, termed SHAPE, to tackle the above challenges in the medication recommendation task. Specifically, we design a compact intra-visit set encoder to encode the relationship in the medical event for obtaining visit-level representation and then develop an inter-visit longitudinal encoder to learn the patient-level longitudinal representation efficiently. To endow the model with the capability of modeling the variable visit length, we introduce a soft curriculum learning method to assign the difficulty of each sample automatically by the visit length. Extensive experiments on a benchmark dataset verify the superiority of our model compared with several state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
医疗健康提供药物建议是一项重要任务。大多数现有工作是基于长期纪录预测药物，假设长期纪录中传输的信息是稳定的，并且内访医学事件是串行的。然而，以下情况可能被忽略：1）更加压缩的内访集编码器可以更好地编码医学事件中的关系，2）不同的患者的变量长期序列表示是不同的。在这篇论文中，我们提出了一种新的 Sample-adaptive Hierarchical medicAtion Prediction nEtwork（SHAPE），用于解决以上挑战。具体来说，我们设计了一个压缩的内访集编码器，用于编码医学事件中的关系，并开发了一个效果的长期编码器，用于学习患者级别的长期表示。为了让模型能够模拟变量访问长度，我们引入了一种软学习策略，用于自动将每个样本的难度分配给每个访问长度。广泛的实验表明，我们的模型在一个标准 benchmark 数据集上表现出色，胜过了多个现有的基准值。
</details></li>
</ul>
<hr>
<h2 id="Toward-Reproducing-Network-Research-Results-Using-Large-Language-Models"><a href="#Toward-Reproducing-Network-Research-Results-Using-Large-Language-Models" class="headerlink" title="Toward Reproducing Network Research Results Using Large Language Models"></a>Toward Reproducing Network Research Results Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04716">http://arxiv.org/abs/2309.04716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiao Xiang, Yuling Lin, Mingjun Fang, Bang Huang, Siyong Huang, Ridi Wen, Franck Le, Linghe Kong, Jiwu Shu</li>
<li>for: 本研究旨在提高网络研究结果的复制效率，尤其是在非公开源代码的情况下。</li>
<li>methods: 本研究使用大型自然语言模型（LLMs）来复制网络研究结果。</li>
<li>results: 小规模实验表明，通过提示工程ChatGPT，4名学生 each 复制了不同的网络系统，发表在知名会议和学报上。实验观察和教训，以及未来的开源研究问题，均被报告。<details>
<summary>Abstract</summary>
Reproducing research results in the networking community is important for both academia and industry. The current best practice typically resorts to three approaches: (1) looking for publicly available prototypes; (2) contacting the authors to get a private prototype; and (3) manually implementing a prototype following the description of the publication. However, most published network research does not have public prototypes and private prototypes are hard to get. As such, most reproducing efforts are spent on manual implementation based on the publications, which is both time and labor consuming and error-prone. In this paper, we boldly propose reproducing network research results using the emerging large language models (LLMs). In particular, we first prove its feasibility with a small-scale experiment, in which four students with essential networking knowledge each reproduces a different networking system published in prominent conferences and journals by prompt engineering ChatGPT. We report the experiment's observations and lessons and discuss future open research questions of this proposal. This work raises no ethical issue.
</details>
<details>
<summary>摘要</summary>
重现网络研究结果在学术和工业领域都非常重要。目前最佳实践通常是通过以下三种方法：（1）搜索公共可用的原型；（2）与作者取得私人原型；以及（3）根据出版物的描述手动实现原型。但是，大多数发表在网络研究中的研究不公开原型，私人原型很难获得。因此，大多数重现努力都是基于出版物的手动实现，这是时间和劳动力费时consuming，也容易出错。在这篇论文中，我们勇敢地提出使用emerging大语言模型（LLMs）来重现网络研究结果。具体来说，我们首先证明了这种方法的可行性，通过小规模实验，四名学生每个重现了不同的网络系统，这些系统分别发表在著名的会议和杂志上。我们报告了实验的观察和教训，并讨论了未来的开放研究问题。这项工作没有伦理问题。
</details></li>
</ul>
<hr>
<h2 id="Advantage-Actor-Critic-with-Reasoner-Explaining-the-Agent’s-Behavior-from-an-Exploratory-Perspective"><a href="#Advantage-Actor-Critic-with-Reasoner-Explaining-the-Agent’s-Behavior-from-an-Exploratory-Perspective" class="headerlink" title="Advantage Actor-Critic with Reasoner: Explaining the Agent’s Behavior from an Exploratory Perspective"></a>Advantage Actor-Critic with Reasoner: Explaining the Agent’s Behavior from an Exploratory Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04707">http://arxiv.org/abs/2309.04707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muzhe Guo, Feixu Yu, Tian Lan, Fang Jin</li>
<li>for: 这篇论文旨在提高 actor-critic 模型的可读性和可理解性，使其在具有重要实际影响的决策问题中更能够负责任和可靠。</li>
<li>methods: 该论文提出了一种名为 Advantage Actor-Critic with Reasoner (A2CR) 的新方法，可以轻松应用于 actor-critic 模型中，并使其更加可读性和可理解性。A2CR 包含三个相互连接的网络：政策网络、价值网络和理解器网络。通过预先定义和分类actor的行为目的，A2CR 自动生成了一种更加全面和可读的决策过程理解模型，并提供了一些功能如目的基于的焦点度、早期失败检测和模型监视，以促进负责任和可靠的 RL。</li>
<li>results: 在 Super Mario Bros 环境中进行的评估结果表明，随着 RL 算法的探索水平的增加，Reasoner 预测的标签占比下降于 <code>Breakout&quot;，而 </code>Hovering” 的标签占比增加。此外，根据目的基于的焦点度也更加集中和可理解。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) is a powerful tool for solving complex decision-making problems, but its lack of transparency and interpretability has been a major challenge in domains where decisions have significant real-world consequences. In this paper, we propose a novel Advantage Actor-Critic with Reasoner (A2CR), which can be easily applied to Actor-Critic-based RL models and make them interpretable. A2CR consists of three interconnected networks: the Policy Network, the Value Network, and the Reasoner Network. By predefining and classifying the underlying purpose of the actor's actions, A2CR automatically generates a more comprehensive and interpretable paradigm for understanding the agent's decision-making process. It offers a range of functionalities such as purpose-based saliency, early failure detection, and model supervision, thereby promoting responsible and trustworthy RL. Evaluations conducted in action-rich Super Mario Bros environments yield intriguing findings: Reasoner-predicted label proportions decrease for ``Breakout" and increase for ``Hovering" as the exploration level of the RL algorithm intensifies. Additionally, purpose-based saliencies are more focused and comprehensible.
</details>
<details>
<summary>摘要</summary>
强化学习（RL）是一种强大的解决复杂决策问题的工具，但它缺乏透明度和解释性使得在具有重要现实世界影响的领域中具有很大挑战。在这篇论文中，我们提出了一种新的优先级actor-critic（A2CR）模型，可以轻松应用于actor-critic基于的RL模型中，并使其更加解释性。A2CR包括三个相互连接的网络：政策网络、价值网络和理解器网络。通过先定和分类actor的行为目的，A2CR自动生成了一个更加全面和解释性强的RL决策过程的模型。它提供了一系列功能，如目的基于的焦点度、早期失败检测和模型监督，从而推动负责任和可信RL。在动作充满的Super Mario Bros环境中进行的评估结果具有吸引人的发现：理解器预测的标签占比随RL算法的探索水平增加而下降，而在Breakout和Hovering两个任务中，理解器预测的标签占比增加。此外，目的基于的焦点度更加集中和可读。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-Disinformation-and-Fake-News-Detection-Using-Fine-Tuned-Large-Language-Model"><a href="#Analysis-of-Disinformation-and-Fake-News-Detection-Using-Fine-Tuned-Large-Language-Model" class="headerlink" title="Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model"></a>Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04704">http://arxiv.org/abs/2309.04704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bohdan M. Pavlyshenko</li>
<li>for: 本研究探讨了使用Llama 2大语言模型（LLM）进行假信息分析和新闻干扰检测的可能性。</li>
<li>methods: 本研究使用了PEFT&#x2F;LoRA基于的方法进行精度调整。</li>
<li>results: 经调整后的Llama 2模型可以深入分析文本，揭示复杂的风格和叙述。提取名实体的情感可以作为支持机器学习模型的预测特征。<details>
<summary>Abstract</summary>
The paper considers the possibility of fine-tuning Llama 2 large language model (LLM) for the disinformation analysis and fake news detection. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text on revealing disinformation and propaganda narratives, fact checking, fake news detection, manipulation analytics, extracting named entities with their sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a deep analysis of texts and reveal complex styles and narratives. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models.
</details>
<details>
<summary>摘要</summary>
文章考虑了LLAMA 2大语言模型（LLM）的微调用于假新闻检测和假信息分析。为微调，使用了PEFT/LoRA基于的方法。研究中，模型被微调用以下任务：分析文本，揭露假信息和宣传媒体，实现Fact Checking，假新闻检测，操纵分析，提取名实体的情感。研究结果显示，微调LLAMA 2模型可以深入分析文本，揭示复杂的风格和 narraatives。提取的名实体情感可以作为超参量来预测支持机器学习模型。
</details></li>
</ul>
<hr>
<h2 id="Weak-PDE-LEARN-A-Weak-Form-Based-Approach-to-Discovering-PDEs-From-Noisy-Limited-Data"><a href="#Weak-PDE-LEARN-A-Weak-Form-Based-Approach-to-Discovering-PDEs-From-Noisy-Limited-Data" class="headerlink" title="Weak-PDE-LEARN: A Weak Form Based Approach to Discovering PDEs From Noisy, Limited Data"></a>Weak-PDE-LEARN: A Weak Form Based Approach to Discovering PDEs From Noisy, Limited Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04699">http://arxiv.org/abs/2309.04699</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/punkduckable/weak_pde_learn">https://github.com/punkduckable/weak_pde_learn</a></li>
<li>paper_authors: Robert Stephany, Christopher Earls</li>
<li>for: 该论文旨在开发一种可以从噪声和有限的解析数据中直接找到非线性偏微分方程的探索算法。</li>
<li>methods: 该算法使用适应损失函数基于弱形来训练神经网络， Approximate PDE解而同时确定 governing PDE。</li>
<li>results: 该方法可以快速和稳定地找到各种各样的偏微分方程，并且可以承受噪声。<details>
<summary>Abstract</summary>
We introduce Weak-PDE-LEARN, a Partial Differential Equation (PDE) discovery algorithm that can identify non-linear PDEs from noisy, limited measurements of their solutions. Weak-PDE-LEARN uses an adaptive loss function based on weak forms to train a neural network, $U$, to approximate the PDE solution while simultaneously identifying the governing PDE. This approach yields an algorithm that is robust to noise and can discover a range of PDEs directly from noisy, limited measurements of their solutions. We demonstrate the efficacy of Weak-PDE-LEARN by learning several benchmark PDEs.
</details>
<details>
<summary>摘要</summary>
我们介绍Weak-PDE-LEARN，一个可以从噪音、有限的解析方法获取非线性偏微分方程的探测算法。Weak-PDE-LEARN使用适应损失函数基于弱形式来训练神经网络$U$，以便精度地 aproximate偏微分方程解析，同时也可以直接从噪音、有限的解析方法中找到统治偏微分方程。这种方法具有响应噪音的特点，并且可以直接从噪音、有限的解析方法中找到许多偏微分方程。我们在这篇文章中证明了Weak-PDE-LEARN的有效性，通过学习了一些benchmark偏微分方程。
</details></li>
</ul>
<hr>
<h2 id="Redundancy-Free-Self-Supervised-Relational-Learning-for-Graph-Clustering"><a href="#Redundancy-Free-Self-Supervised-Relational-Learning-for-Graph-Clustering" class="headerlink" title="Redundancy-Free Self-Supervised Relational Learning for Graph Clustering"></a>Redundancy-Free Self-Supervised Relational Learning for Graph Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04694">http://arxiv.org/abs/2309.04694</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yisiyu95/r2fgc">https://github.com/yisiyu95/r2fgc</a></li>
<li>paper_authors: Si-Yu Yi, Wei Ju, Yifang Qin, Xiao Luo, Luchen Liu, Yong-Dao Zhou, Ming Zhang</li>
<li>for: 本文提出了一种新的自然语言深度图 clustering 方法，即 Relational Redundancy-Free Graph Clustering (R$^2$FGC)，用于解决图Structured 数据中的群集分配问题。</li>
<li>methods: 本文使用了自动encoder和图自动encoder来提取图中的属性和结构层次关系信息，并通过保留归一化后的相似节点关系来学习有效的表示。此外，本文还提出了一种简单 yet 有效的策略来解决过滤问题。</li>
<li>results: 对于 widely 使用的 benchmark 数据集，本文的 R$^2$FGC 方法表现出了较高的cluster 性能，超过了现有基线方法。 codes 可以在 <a target="_blank" rel="noopener" href="https://github.com/yisiyu95/R2FGC">https://github.com/yisiyu95/R2FGC</a> 中下载。<details>
<summary>Abstract</summary>
Graph clustering, which learns the node representations for effective cluster assignments, is a fundamental yet challenging task in data analysis and has received considerable attention accompanied by graph neural networks in recent years. However, most existing methods overlook the inherent relational information among the non-independent and non-identically distributed nodes in a graph. Due to the lack of exploration of relational attributes, the semantic information of the graph-structured data fails to be fully exploited which leads to poor clustering performance. In this paper, we propose a novel self-supervised deep graph clustering method named Relational Redundancy-Free Graph Clustering (R$^2$FGC) to tackle the problem. It extracts the attribute- and structure-level relational information from both global and local views based on an autoencoder and a graph autoencoder. To obtain effective representations of the semantic information, we preserve the consistent relation among augmented nodes, whereas the redundant relation is further reduced for learning discriminative embeddings. In addition, a simple yet valid strategy is utilized to alleviate the over-smoothing issue. Extensive experiments are performed on widely used benchmark datasets to validate the superiority of our R$^2$FGC over state-of-the-art baselines. Our codes are available at https://github.com/yisiyu95/R2FGC.
</details>
<details>
<summary>摘要</summary>
GRAPH CLUSTERING, 学习节点表示以实现有效的归类分配, 是数据分析中的基本 yet challenging task 在 recientes years  accompanying 图 neural networks Received considerable attention. However, most existing methods overlook the inherent relational information among the non-independent and non-identically distributed nodes in a graph. Due to the lack of exploration of relational attributes, the semantic information of the graph-structured data fails to be fully exploited which leads to poor clustering performance. In this paper, we propose a novel self-supervised deep graph clustering method named Relational Redundancy-Free Graph Clustering (R$^2$FGC) to tackle the problem. It extracts the attribute- and structure-level relational information from both global and local views based on an autoencoder and a graph autoencoder. To obtain effective representations of the semantic information, we preserve the consistent relation among augmented nodes, whereas the redundant relation is further reduced for learning discriminative embeddings. In addition, a simple yet valid strategy is utilized to alleviate the over-smoothing issue. Extensive experiments are performed on widely used benchmark datasets to validate the superiority of our R$^2$FGC over state-of-the-art baselines. Our codes are available at https://github.com/yisiyu95/R2FGC.
</details></li>
</ul>
<hr>
<h2 id="Flexible-and-Robust-Counterfactual-Explanations-with-Minimal-Satisfiable-Perturbations"><a href="#Flexible-and-Robust-Counterfactual-Explanations-with-Minimal-Satisfiable-Perturbations" class="headerlink" title="Flexible and Robust Counterfactual Explanations with Minimal Satisfiable Perturbations"></a>Flexible and Robust Counterfactual Explanations with Minimal Satisfiable Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04676">http://arxiv.org/abs/2309.04676</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangyongjie-ntu/cemsp">https://github.com/wangyongjie-ntu/cemsp</a></li>
<li>paper_authors: Yongjie Wang, Hangwei Qian, Yongjie Liu, Wei Guo, Chunyan Miao</li>
<li>for: 提高信息公平和可信worthiness，并为用户提供有用的建议。</li>
<li>methods: 使用Counterfactual Explanations with Minimal Satisfiable Perturbations（CEMSP）方法，该方法通过尝试不同的特征值修改，以实现不同的预测结果。</li>
<li>results: 比较 existing methods，CEMSP 可以提供更加稳定和有用的解释，同时保持 flexibility。<details>
<summary>Abstract</summary>
Counterfactual explanations (CFEs) exemplify how to minimally modify a feature vector to achieve a different prediction for an instance. CFEs can enhance informational fairness and trustworthiness, and provide suggestions for users who receive adverse predictions. However, recent research has shown that multiple CFEs can be offered for the same instance or instances with slight differences. Multiple CFEs provide flexible choices and cover diverse desiderata for user selection. However, individual fairness and model reliability will be damaged if unstable CFEs with different costs are returned. Existing methods fail to exploit flexibility and address the concerns of non-robustness simultaneously. To address these issues, we propose a conceptually simple yet effective solution named Counterfactual Explanations with Minimal Satisfiable Perturbations (CEMSP). Specifically, CEMSP constrains changing values of abnormal features with the help of their semantically meaningful normal ranges. For efficiency, we model the problem as a Boolean satisfiability problem to modify as few features as possible. Additionally, CEMSP is a general framework and can easily accommodate more practical requirements, e.g., casualty and actionability. Compared to existing methods, we conduct comprehensive experiments on both synthetic and real-world datasets to demonstrate that our method provides more robust explanations while preserving flexibility.
</details>
<details>
<summary>摘要</summary>
counterfactual explanations (CFEs) 可以最小化特征向量 modify 来实现不同预测结果的 instance。CFEs 可以增强信息公平和可靠性，并为用户提供不同的建议。然而，最近的研究发现，对同一个 instance 或 slight differences 的 instances 可以提供多个 CFEs。多个 CFEs 提供了多样的选择，涵盖了用户选择的多种需求。然而，个人公平和模型可靠性会受到不稳定 CFEs 的影响。现有方法无法充分利用 flexibility 并同时解决不稳定性问题。为解决这些问题，我们提出了一种概念简单 yet effective 的解决方案，名为 counterfactual explanations with minimal satisfiable perturbations (CEMSP)。具体来说，CEMSP 使用异常特征值的变化来满足 semantically meaningful normal ranges。为了提高效率，我们将问题模型为 Boolean satisfiability problem，以修改最少特征。此外，CEMSP 是一个通用的框架，可以轻松扩展到更加实际的需求，例如 causality 和 actionability。与现有方法相比，我们在 synthetic 和实际数据集上进行了广泛的实验，并证明了我们的方法可以提供更加稳定的解释，同时保持 flexibility。
</details></li>
</ul>
<hr>
<h2 id="Compact-Approximating-Complex-Activation-Functions-for-Secure-Computation"><a href="#Compact-Approximating-Complex-Activation-Functions-for-Secure-Computation" class="headerlink" title="Compact: Approximating Complex Activation Functions for Secure Computation"></a>Compact: Approximating Complex Activation Functions for Secure Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04664">http://arxiv.org/abs/2309.04664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mazharul Islam, Sunpreet S. Arora, Rahul Chatterjee, Peter Rindal, Maliheh Shirvanian</li>
<li>for: 提供隐私保护的深度神经网络（DNN）模型查询服务，使用公共云计算。</li>
<li>methods: 使用现状顶峰技术，生成复杂非线性Activation Functions（AFs）的割辑式多方计算（MPC）技术。</li>
<li>results: 与现状顶峰技术相比，Compact incurs negligible accuracy loss，并且提供2x-5x的计算速度提升。<details>
<summary>Abstract</summary>
Secure multi-party computation (MPC) techniques can be used to provide data privacy when users query deep neural network (DNN) models hosted on a public cloud. State-of-the-art MPC techniques can be directly leveraged for DNN models that use simple activation functions (AFs) such as ReLU. However, DNN model architectures designed for cutting-edge applications often use complex and highly non-linear AFs. Designing efficient MPC techniques for such complex AFs is an open problem.   Towards this, we propose Compact, which produces piece-wise polynomial approximations of complex AFs to enable their efficient use with state-of-the-art MPC techniques. Compact neither requires nor imposes any restriction on model training and results in near-identical model accuracy. We extensively evaluate Compact on four different machine-learning tasks with DNN architectures that use popular complex AFs SiLU, GeLU, and Mish. Our experimental results show that Compact incurs negligible accuracy loss compared to DNN-specific approaches for handling complex non-linear AFs. We also incorporate Compact in two state-of-the-art MPC libraries for privacy-preserving inference and demonstrate that Compact provides 2x-5x speedup in computation compared to the state-of-the-art approximation approach for non-linear functions -- while providing similar or better accuracy for DNN models with large number of hidden layers
</details>
<details>
<summary>摘要</summary>
secure多方 computation（MPC）技术可以用来保证用户在公共云上查询深度神经网络（DNN）模型时的数据隐私。现代MPC技术可以直接应用于使用简单Activation Functions（AFs）的DNN模型，如ReLU。然而，为了满足现代应用的需求，DNN模型的设计通常使用复杂和高度非线性的AFs。设计高效的MPC技术 для这些复杂AFs是一个开放的问题。为了解决这个问题，我们提出了一种名为Compact的方法，该方法生成了分割的多项式近似方法来实现复杂AFs的高效使用。Compact不需要任何限制model的训练，并且不会影响模型的准确性。我们对四个不同的机器学习任务进行了广泛的实验，结果表明，Compact与DNN特有的方法相比，对于处理复杂非线性AFs的精度损失是可以忽略的。我们还将Compactintegrated into两个现代MPC库，并证明了Compact在计算中提供了2-5倍的速度提升，相比之下，现有的近似方法对非线性函数的approximation。Translation notes:* "secure multi-party computation" is translated as "secure多方 computation"* "deep neural network" is translated as "深度神经网络"* "activation functions" is translated as "Activation Functions"* "public cloud" is translated as "公共云"* "state-of-the-art" is translated as "现代"* "mpc techniques" is translated as "MPC技术"* "efficient" is translated as "高效"* "accuracy" is translated as "准确性"* "piece-wise polynomial approximations" is translated as "分割的多项式近似方法"* "near-identical model accuracy" is translated as "相同模型精度"* "machine-learning tasks" is translated as "机器学习任务"* "hidden layers" is translated as "隐藏层"* "approximation approach" is translated as "approximation方法"* "computation" is translated as "计算"Please note that the translation is done using Google Translate and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="MADLAD-400-A-Multilingual-And-Document-Level-Large-Audited-Dataset"><a href="#MADLAD-400-A-Multilingual-And-Document-Level-Large-Audited-Dataset" class="headerlink" title="MADLAD-400: A Multilingual And Document-Level Large Audited Dataset"></a>MADLAD-400: A Multilingual And Document-Level Large Audited Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04662">http://arxiv.org/abs/2309.04662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, Orhan Firat</li>
<li>for: 这个论文是为了介绍一个基于 CommonCrawl 的通用领域 3T 单语言数据集（MADLAD-400），以及该数据集的自我审核限制和数据审核在数据集创建过程中的作用。</li>
<li>methods: 论文使用了公共可用数据进行模型训练，包括一个 10.7B 参数的多语言翻译模型和一个 8B 参数的语言模型，并对不同领域进行评估。</li>
<li>results: 论文发现该模型在不同领域的翻译任务中具有竞争力，并且在少量翻译任务中表现良好。此外，论文还发布了基线模型，以便研究人员进行进一步的研究和应用。<details>
<summary>Abstract</summary>
We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models available to the research community.
</details>
<details>
<summary>摘要</summary>
我们介绍MADLAD-400，一个人工审核的、通用领域3Token单语言数据集，基于CommonCrawl，覆盖419种语言。我们介绍MADLAD-400自我审核的局限性，以及数据审核在数据集创建过程中的角色。然后，我们使用公共可用数据进行训练，开发了10.7B参数的多语言翻译模型，覆盖超过450种语言，并发现它与许多更大的模型相比竞争性强。此外，我们还训练了8B参数的语言模型，并评估其在少量翻译任务中的表现。我们将基准模型提供给研究社区。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-upper-limb-exoskeleton-using-deep-learning-to-predict-human-intention-for-sensory-feedback-augmentation"><a href="#Intelligent-upper-limb-exoskeleton-using-deep-learning-to-predict-human-intention-for-sensory-feedback-augmentation" class="headerlink" title="Intelligent upper-limb exoskeleton using deep learning to predict human intention for sensory-feedback augmentation"></a>Intelligent upper-limb exoskeleton using deep learning to predict human intention for sensory-feedback augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04655">http://arxiv.org/abs/2309.04655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinwoo Lee, Kangkyu Kwon, Ira Soltis, Jared Matthews, Yoonjae Lee, Hojoong Kim, Lissette Romero, Nathan Zavanelli, Youngjin Kwon, Shinjae Kwon, Jimin Lee, Yewon Na, Sung Hoon Lee, Ki Jun Yu, Minoru Shinohara, Frank L. Hammond, Woon-Hong Yeo<br>for:The paper is written for the purpose of introducing an intelligent upper-limb exoskeleton system that uses cloud-based deep learning to predict human intention for strength augmentation.methods:The exoskeleton system uses embedded soft wearable sensors to collect real-time muscle signals, which are simultaneously computed to determine the user’s intended movement. Cloud-based deep-learning is used to predict four upper-limb joint motions with an average accuracy of 96.2% at a 200-250 millisecond response rate.results:The exoskeleton system is able to augment human strength by 5.15 times on average compared to the unassisted exoskeleton, with an array of soft pneumatics assisting the intended movements by providing 897 newton of force and 78.7 millimeter of displacement at maximum.Here is the format you requested, in Simplified Chinese text:for: 这篇论文是为了介绍一种基于云计算的深度学习技术实现人类意图强化的智能上肢骨丝系统。methods: 该系统使用嵌入式软式感知器收集实时肌肉信号，并同时计算以确定用户的意图运动。云计算的深度学习被用来预测四个上肢关节运动的准确率为96.2%，响应时间为200-250毫秒。results: 该系统能够在 average 情况下为人类提供5.15倍的强度增强，与无助的外套比较。软压缩器可以提供897牛顿的力和78.7毫米的移动距离。<details>
<summary>Abstract</summary>
The age and stroke-associated decline in musculoskeletal strength degrades the ability to perform daily human tasks using the upper extremities. Although there are a few examples of exoskeletons, they need manual operations due to the absence of sensor feedback and no intention prediction of movements. Here, we introduce an intelligent upper-limb exoskeleton system that uses cloud-based deep learning to predict human intention for strength augmentation. The embedded soft wearable sensors provide sensory feedback by collecting real-time muscle signals, which are simultaneously computed to determine the user's intended movement. The cloud-based deep-learning predicts four upper-limb joint motions with an average accuracy of 96.2% at a 200-250 millisecond response rate, suggesting that the exoskeleton operates just by human intention. In addition, an array of soft pneumatics assists the intended movements by providing 897 newton of force and 78.7 millimeter of displacement at maximum. Collectively, the intent-driven exoskeleton can augment human strength by 5.15 times on average compared to the unassisted exoskeleton. This report demonstrates an exoskeleton robot that augments the upper-limb joint movements by human intention based on a machine-learning cloud computing and sensory feedback.
</details>
<details>
<summary>摘要</summary>
人体日常活动需要肢体强度，但年龄和stroke-related decline in musculoskeletal strength会导致这种能力减退。虽有一些 эксоскелетоны，但它们需要人工操作，因为缺乏感知反馈和无法预测人体移动意图。我们介绍了一个智能 upper-limb exoskeleton 系统，使用云端深度学习预测人类意图，以增强人类力量。系统内置软件感知器收集实时肌肉信号，并同时计算用户的意图移动。云端深度学习预测四个 upper-limb 关节运动，准确率为96.2%，响应时间为200-250毫秒，表明系统只遵循人类意图。此外，一个数组软式压缩器帮助实现意图运动，提供897牛顿的力量和78.7毫米的移动距离。总之，基于意图的 exoskeleton 可以增强人类肢体强度5.15倍，比无助 exoskeleton 更高。这篇报告描述了基于机器学习云计算和感知反馈的 exoskeleton 机器人，它可以根据人类意图增强 upper-limb 关节运动。
</details></li>
</ul>
<hr>
<h2 id="Towards-Understanding-Neural-Collapse-The-Effects-of-Batch-Normalization-and-Weight-Decay"><a href="#Towards-Understanding-Neural-Collapse-The-Effects-of-Batch-Normalization-and-Weight-Decay" class="headerlink" title="Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay"></a>Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04644">http://arxiv.org/abs/2309.04644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leyan Pan, Xinyuan Cao</li>
<li>For: 研究 neural network 的朴素化现象，即 Neural Collapse。* Methods: 使用批处理normalization和weight decay来调整 neural network 的训练。* Results: 提出了一种 geometrically intuitive 的cosine similarity measure，并提供了对 Neural Collapse 的 theoretically guarantees。 Additionally, the paper shows that Neural Collapse is most significant in models with batch normalization and high weight-decay values.Here’s the full text in Simplified Chinese:* For: 本文研究的是 Neural Collapse，即 neural network 的朴素化现象。* Methods: 本文使用了批处理normalization和weight decay来调整 neural network 的训练。* Results: 本文提出了一种 geometrically intuitive 的cosine similarity measure，并提供了对 Neural Collapse 的 theoretically guarantees。 Additionally, the paper shows that Neural Collapse is most significant in models with batch normalization and high weight-decay values.<details>
<summary>Abstract</summary>
Neural Collapse is a recently observed geometric structure that emerges in the final layer of neural network classifiers. Specifically, Neural Collapse states that at the terminal phase of neural networks training, 1) the intra-class variability of last-layer features tends to zero, 2) the class feature means form an Equiangular Tight Frame (ETF), 3) last-layer class features and weights becomes equal up the scaling, and 4) classification behavior collapses to the nearest class center (NCC) decision rule. This paper investigates the effect of batch normalization and weight decay on the emergence of Neural Collapse. We propose the geometrically intuitive intra-class and inter-class cosine similarity measure which captures multiple core aspects of Neural Collapse. With this measure, we provide theoretical guarantees of Neural Collapse emergence with last-layer batch normalization and weight decay when the regularized cross-entropy loss is near optimal. We also perform further experiments to show that the Neural Collapse is most significant in models with batch normalization and high weight-decay values. Collectively, our results imply that batch normalization and weight decay may be fundamental factors in the emergence of Neural Collapse.
</details>
<details>
<summary>摘要</summary>
neural collapse 是一种最近发现的几何结构，它在神经网络分类器的最后一层出现。具体来说，神经collapse 的三个特点是：1）最后一层特征变量内类差降到零，2）类特征均值形成等角紧凑框（ETF），3）最后一层特征和权重归一化，4）分类行为归一化到最近类中心（NCC）决策规则。本文研究了批 Normalization 和权重衰减对神经collapse 的影响。我们提出了几何直观的内类和间类 косину similarity 度量，该度量捕捉了多个核心方面的神经collapse。通过这个度量，我们提供了理论保证神经collapse 的出现，当整合 cross-entropy 损失接近优化时。我们还进行了更多的实验，证明神经collapse 在模型中具有批 Normalization 和高权重衰减值时最为明显。总的来说，我们的结果表明，批 Normalization 和权重衰减可能是神经collapse 的基本因素。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/09/cs.LG_2023_09_09/" data-id="clmjn91mz00890j883obv49um" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/09/eess.IV_2023_09_09/" class="article-date">
  <time datetime="2023-09-09T09:00:00.000Z" itemprop="datePublished">2023-09-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/09/eess.IV_2023_09_09/">eess.IV - 2023-09-09</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Latent-Degradation-Representation-Constraint-for-Single-Image-Deraining"><a href="#Latent-Degradation-Representation-Constraint-for-Single-Image-Deraining" class="headerlink" title="Latent Degradation Representation Constraint for Single Image Deraining"></a>Latent Degradation Representation Constraint for Single Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04780">http://arxiv.org/abs/2309.04780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhong He, Long Peng, Lu Wang, Jun Cheng</li>
<li>for: 本研究旨在提出一种能够解决单张图像抹杂问题的新方法，即Latent Degradation Representation Constraint Network (LDRCNet)。</li>
<li>methods: 该方法包括Direction-Aware Encoder (DAEncoder)、UNet Deraining Network和Multi-Scale Interaction Block (MSIBlock)。DAEncoder使用可变权重核函数来适应抹杂方向的变化，从而提取有用的抹杂表示。然后，通过引入约束损失来显式地约束抹杂表示学习。MSIBlock则是用于与抹杂表示和解码器特征进行互动的多尺度交互块。</li>
<li>results: 实验结果表明，LDRCNet可以在 synthetic 和实际 dataset 上达到新的顶峰性能。<details>
<summary>Abstract</summary>
Since rain streaks show a variety of shapes and directions, learning the degradation representation is extremely challenging for single image deraining. Existing methods are mainly targeted at designing complicated modules to implicitly learn latent degradation representation from coupled rainy images. This way, it is hard to decouple the content-independent degradation representation due to the lack of explicit constraint, resulting in over- or under-enhancement problems. To tackle this issue, we propose a novel Latent Degradation Representation Constraint Network (LDRCNet) that consists of Direction-Aware Encoder (DAEncoder), UNet Deraining Network, and Multi-Scale Interaction Block (MSIBlock). Specifically, the DAEncoder is proposed to adaptively extract latent degradation representation by using the deformable convolutions to exploit the direction consistency of rain streaks. Next, a constraint loss is introduced to explicitly constraint the degradation representation learning during training. Last, we propose an MSIBlock to fuse with the learned degradation representation and decoder features of the deraining network for adaptive information interaction, which enables the deraining network to remove various complicated rainy patterns and reconstruct image details. Experimental results on synthetic and real datasets demonstrate that our method achieves new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
因为雨束显示出多种形状和方向，单图像推干涂除非常困难。现有方法主要是通过设计复杂的模块来隐式地学习隐藏的降低表示。这样做导致缺乏明确约束，从而导致过度或者下降问题。为解决这个问题，我们提出了一种新的隐藏降低表示约束网络（LDRCNet），它包括指向意识Encoder（DAEncoder）、UNet推干网络和多Scale交互块（MSIBlock）。具体来说，DAEncoder使用可变核心扩展来适应雨束方向的一致性，以提取适应的降低表示。然后，我们引入了约束损失来Explicitly constrain the learning of degradation representation during training.最后，我们提出了MSIBlock来融合学习的降低表示和推干网络的解码特征，以实现适应性的信息互动，使推干网络可以去除各种复杂的雨束模式，并重建图像细节。实验结果表明，我们的方法在 sintetic和实际 dataset 上达到了新的州OF-the-art性能。
</details></li>
</ul>
<hr>
<h2 id="SSHNN-Semi-Supervised-Hybrid-NAS-Network-for-Echocardiographic-Image-Segmentation"><a href="#SSHNN-Semi-Supervised-Hybrid-NAS-Network-for-Echocardiographic-Image-Segmentation" class="headerlink" title="SSHNN: Semi-Supervised Hybrid NAS Network for Echocardiographic Image Segmentation"></a>SSHNN: Semi-Supervised Hybrid NAS Network for Echocardiographic Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04672">http://arxiv.org/abs/2309.04672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renqi Chen, Jingjing Luo, Fan Nian, Yuhui Cen, Yiheng Peng, Zekuan Yu</li>
<li>for: 这个研究旨在提高医疗影像分类�で，特别是使用echocardiographic images的混乱讯号。</li>
<li>methods: 这个研究使用Neural Architecture Search（NAS）来设计网络，并将对�Feature fusion和Transformers进行创新的应用，以提高分类结果的精度。</li>
<li>results: 实验结果显示，这个 SSHNN 网络可以对echocardiographic images进行高精度的分类，并且超过了现有的方法。<details>
<summary>Abstract</summary>
Accurate medical image segmentation especially for echocardiographic images with unmissable noise requires elaborate network design. Compared with manual design, Neural Architecture Search (NAS) realizes better segmentation results due to larger search space and automatic optimization, but most of the existing methods are weak in layer-wise feature aggregation and adopt a ``strong encoder, weak decoder" structure, insufficient to handle global relationships and local details. To resolve these issues, we propose a novel semi-supervised hybrid NAS network for accurate medical image segmentation termed SSHNN. In SSHNN, we creatively use convolution operation in layer-wise feature fusion instead of normalized scalars to avoid losing details, making NAS a stronger encoder. Moreover, Transformers are introduced for the compensation of global context and U-shaped decoder is designed to efficiently connect global context with local features. Specifically, we implement a semi-supervised algorithm Mean-Teacher to overcome the limited volume problem of labeled medical image dataset. Extensive experiments on CAMUS echocardiography dataset demonstrate that SSHNN outperforms state-of-the-art approaches and realizes accurate segmentation. Code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
准确的医疗图像分割，特别是echocardiographic图像，需要考虑到不可避免的噪声。在现有的方法中，Neural Architecture Search（NAS）可以实现更好的分割结果，因为它可以搜索更大的搜索空间并自动优化，但大多数现有方法具有弱的层次特征聚合和“强encoder,弱decoder”结构，无法处理全局关系和本地特征。为解决这些问题，我们提出了一种新的半supervised hybrid NAS网络，称之为SSHNN。在SSHNN中，我们创新地在层次特征融合中使用 convolution 操作，而不是正常化的整数来避免丢失细节，使NAS变得更强。此外，我们还引入了Transformers来补偿全局上下文，并设计了U-shaped decoder来有效地连接全局上下文和本地特征。具体来说，我们实现了一种半supervised算法Mean-Teacher，以超越有限的医疗图像标注数据集的问题。我们在CAMUS echo cardiography数据集进行了广泛的实验，并证明了SSHNN可以超越现有的方法，实现准确的分割。代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="ConvFormer-Plug-and-Play-CNN-Style-Transformers-for-Improving-Medical-Image-Segmentation"><a href="#ConvFormer-Plug-and-Play-CNN-Style-Transformers-for-Improving-Medical-Image-Segmentation" class="headerlink" title="ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation"></a>ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05674">http://arxiv.org/abs/2309.05674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xian Lin, Zengqiang Yan, Xianbo Deng, Chuansheng Zheng, Li Yu</li>
<li>for: 提高 transformer 的医疗图像分割性能，增强 attention 的均匀分布和多元特征提取。</li>
<li>methods: 建立 CNN-style Transformers（ConvFormer），包括pooling、CNN-style自注意（CSA）和卷积Feedforward Network（CFFN），使得 transformer 可以更好地利用医疗图像数据的多个特征。</li>
<li>results: 在多个 dataset 上实验表明，ConvFormer 可以作为 transformer 框架中的插件模块，提高 transformer 的医疗图像分割性能，并且可以轻松地替换现有的 transformer 框架。<details>
<summary>Abstract</summary>
Transformers have been extensively studied in medical image segmentation to build pairwise long-range dependence. Yet, relatively limited well-annotated medical image data makes transformers struggle to extract diverse global features, resulting in attention collapse where attention maps become similar or even identical. Comparatively, convolutional neural networks (CNNs) have better convergence properties on small-scale training data but suffer from limited receptive fields. Existing works are dedicated to exploring the combinations of CNN and transformers while ignoring attention collapse, leaving the potential of transformers under-explored. In this paper, we propose to build CNN-style Transformers (ConvFormer) to promote better attention convergence and thus better segmentation performance. Specifically, ConvFormer consists of pooling, CNN-style self-attention (CSA), and convolutional feed-forward network (CFFN) corresponding to tokenization, self-attention, and feed-forward network in vanilla vision transformers. In contrast to positional embedding and tokenization, ConvFormer adopts 2D convolution and max-pooling for both position information preservation and feature size reduction. In this way, CSA takes 2D feature maps as inputs and establishes long-range dependency by constructing self-attention matrices as convolution kernels with adaptive sizes. Following CSA, 2D convolution is utilized for feature refinement through CFFN. Experimental results on multiple datasets demonstrate the effectiveness of ConvFormer working as a plug-and-play module for consistent performance improvement of transformer-based frameworks. Code is available at https://github.com/xianlin7/ConvFormer.
</details>
<details>
<summary>摘要</summary>
transformers 在医学像素分割方面进行了广泛的研究，以建立对比较长距离的相互依赖关系。然而，有限的医学像素数据导致 transformers 很难提取多样化的全球特征，从而导致注意力归一化，注意力映射变得相互相同或甚至相同。相比之下，卷积神经网络（CNN）在小规模训练数据上具有更好的收敛性能，但它们具有有限的接受范围。现有的工作主要关注在 CNN 和 transformers 之间的组合，而忽略了注意力归一化问题，这导致 transformers 的潜在能力尚未得到充分发挥。本文提出了一种具有 CNN 特征的 transformers（ConvFormer），以提高注意力归一化并提高分割性能。具体来说，ConvFormer 包括pooling、CNN 样式自注意（CSA）和 convolutional feed-forward network（CFFN），对应于 tokenization、自注意和 feed-forward network 在 vanilla vision transformers 中。与positional embedding和tokenization不同，ConvFormer 采用了2D卷积和最大池化来保持位置信息和特征大小减少。这样，CSA 可以使用2D特征图作为输入，通过构建自注意矩阵作为卷积核来建立长距离相关性。接着，2D卷积用于特征细化通过 CFFN。实验结果在多个 dataset 上表明，ConvFormer 作为一个插件模块，可以为基于 transformer 框架的分割模型提供可靠的性能改进。代码可以在 <https://github.com/xianlin7/ConvFormer> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Video-and-Synthetic-MRI-Pre-training-of-3D-Vision-Architectures-for-Neuroimage-Analysis"><a href="#Video-and-Synthetic-MRI-Pre-training-of-3D-Vision-Architectures-for-Neuroimage-Analysis" class="headerlink" title="Video and Synthetic MRI Pre-training of 3D Vision Architectures for Neuroimage Analysis"></a>Video and Synthetic MRI Pre-training of 3D Vision Architectures for Neuroimage Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04651">http://arxiv.org/abs/2309.04651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil J. Dhinagar, Amit Singh, Saket Ozarkar, Ketaki Buwa, Sophia I. Thomopoulos, Conor Owens-Walton, Emily Laltoo, Yao-Liang Chen, Philip Cook, Corey McMillan, Chih-Chien Tsai, J-J Wang, Yih-Ru Wu, Paul M. Thompson<br>for: 这个论文旨在评估不同的先进Dataset上预训练视觉模型的效果，以提高其适应三维医疗影像任务的能力。methods: 本研究使用了视觉变换器（ViTs）和卷积神经网络（CNNs）作为预训练模型，并对它们进行了不同的初始化方法。results: 研究发现，预训练可以提高所有任务的性能，包括提高了AD类型诊断的性能7.4%和PD类型诊断的性能4.6%，以及减少了大脑年龄预测错误的年龄1.26年。此外，使用大规模的视频或生成的MRI数据进行预训练可以提高ViTs的性能，而CNNs在有限数据情况下表现了稳定性，而且在预训练数据是同一个频谱频率的情况下，其性能会进一步提高。<details>
<summary>Abstract</summary>
Transfer learning represents a recent paradigm shift in the way we build artificial intelligence (AI) systems. In contrast to training task-specific models, transfer learning involves pre-training deep learning models on a large corpus of data and minimally fine-tuning them for adaptation to specific tasks. Even so, for 3D medical imaging tasks, we do not know if it is best to pre-train models on natural images, medical images, or even synthetically generated MRI scans or video data. To evaluate these alternatives, here we benchmarked vision transformers (ViTs) and convolutional neural networks (CNNs), initialized with varied upstream pre-training approaches. These methods were then adapted to three unique downstream neuroimaging tasks with a range of difficulty: Alzheimer's disease (AD) and Parkinson's disease (PD) classification, "brain age" prediction. Experimental tests led to the following key observations: 1. Pre-training improved performance across all tasks including a boost of 7.4% for AD classification and 4.6% for PD classification for the ViT and 19.1% for PD classification and reduction in brain age prediction error by 1.26 years for CNNs, 2. Pre-training on large-scale video or synthetic MRI data boosted performance of ViTs, 3. CNNs were robust in limited-data settings, and in-domain pretraining enhanced their performances, 4. Pre-training improved generalization to out-of-distribution datasets and sites. Overall, we benchmarked different vision architectures, revealing the value of pre-training them with emerging datasets for model initialization. The resulting pre-trained models can be adapted to a range of downstream neuroimaging tasks, even when training data for the target task is limited.
</details>
<details>
<summary>摘要</summary>
“转移学习”是人工智能系统建设的新方案，它与专门为特定任务训练深度学习模型不同，而是在大量数据集上预训练深度学习模型，并将其最小化调整为特定任务适应。然而，对于3D医学影像任务来说，我们不知道是否应该预训练模型在自然图像、医学图像或者 sintetically生成的MRI扫描或视频数据上。为了评估这些选择，我们在这里对vision transformers（ViTs）和卷积神经网络（CNNs）进行了比较。这些方法被 initialized 以不同的上游预训练方法，然后被适应到三个独特的下游神经成像任务，包括阿尔茨海默病（AD）和 паркинсони病（PD）分类、“脑年龄”预测。实验证明了以下关键观察结果：1. 预训练提高了所有任务的性能，包括对AD分类 task 的提高率为7.4%，对PD分类 task 的提高率为4.6%，对CNNs来说，PD分类任务的提高率为19.1%，并且预测脑年龄的错误率下降了1.26年。2. 预训练在大规模的视频或生成的MRI数据上提高了ViTs的性能。3. CNNs在有限数据设置下表现稳定，而在域内预训练下进一步提高了其表现。4. 预训练提高了模型对非标称数据集和站点的通用性。总之，我们对不同的视觉架构进行了比较，发现预训练这些模型使用emerging dataset可以提高其性能，这些预训练后的模型可以适应到一系列的下游神经成像任务，即使训练数据集为target任务的训练数据集有限。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/09/eess.IV_2023_09_09/" data-id="clmjn91qy00hr0j88ed1469fs" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/08/cs.SD_2023_09_08/" class="article-date">
  <time datetime="2023-09-08T15:00:00.000Z" itemprop="datePublished">2023-09-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/08/cs.SD_2023_09_08/">cs.SD - 2023-09-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Exploring-Domain-Specific-Enhancements-for-a-Neural-Foley-Synthesizer"><a href="#Exploring-Domain-Specific-Enhancements-for-a-Neural-Foley-Synthesizer" class="headerlink" title="Exploring Domain-Specific Enhancements for a Neural Foley Synthesizer"></a>Exploring Domain-Specific Enhancements for a Neural Foley Synthesizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04641">http://arxiv.org/abs/2309.04641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashwin Pillay, Sage Betko, Ari Liloia, Hao Chen, Ankit Shah</li>
<li>for: 这个论文是为了研究如何使用神经网络来生成真实、设定的电影或广播剧中的 Foley 音效的。</li>
<li>methods: 这个论文使用了多种改进了现有的文本到Audio频域的模型，以提高生成的 Foley 音效的多样性和听觉特征。 其中包括使用预训练的encoder保留音频和音乐特征，实现类别conditioning来增强不同类型 Foley 的中间表示的分 differentiability，以及开发了一种新的 transformer 结构来优化自注意计算在非常大的输入上 без sacrificing valuable information。</li>
<li>results: 作者们在实现后提供了中间结果，超过了基线，并讨论了在实现优化结果时遇到的实际挑战，以及可能的后续研究方向。<details>
<summary>Abstract</summary>
Foley sound synthesis refers to the creation of authentic, diegetic sound effects for media, such as film or radio. In this study, we construct a neural Foley synthesizer capable of generating mono-audio clips across seven predefined categories. Our approach introduces multiple enhancements to existing models in the text-to-audio domain, with the goal of enriching the diversity and acoustic characteristics of the generated foleys. Notably, we utilize a pre-trained encoder that retains acoustical and musical attributes in intermediate embeddings, implement class-conditioning to enhance differentiability among foley classes in their intermediate representations, and devise an innovative transformer-based architecture for optimizing self-attention computations on very large inputs without compromising valuable information. Subsequent to implementation, we present intermediate outcomes that surpass the baseline, discuss practical challenges encountered in achieving optimal results, and outline potential pathways for further research.
</details>
<details>
<summary>摘要</summary>
法莱音频合成指的是为媒体，如电影或广播，制作真实的抽象声效。在这个研究中，我们构建了一个基于神经网络的法莱合成器，能够生成单声道音频片段，涵盖七种预定的类别。我们的方法在文本到Audio领域中的现有模型中进行了多种改进，以增强生成的法莱的多样性和听觉特征。特别是，我们使用预训练的编码器保留了听觉和音乐特征在中间 Representations中，实施了类别划分来增强不同类别的中间表示的分化，并开发了一种创新的转换器结构，以优化自我注意计算在非常大的输入上，无需损失重要信息。在实施后，我们提供了中间结果，讨论了实际遇到的挑战和可能的进一步研究方向。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Pretrained-Image-text-Models-for-Improving-Audio-Visual-Learning"><a href="#Leveraging-Pretrained-Image-text-Models-for-Improving-Audio-Visual-Learning" class="headerlink" title="Leveraging Pretrained Image-text Models for Improving Audio-Visual Learning"></a>Leveraging Pretrained Image-text Models for Improving Audio-Visual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04628">http://arxiv.org/abs/2309.04628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabhchand Bhati, Jesús Villalba, Laureano Moro-Velazquez, Thomas Thebaud, Najim Dehak<br>for: 这个论文的目的是提高基于视频和文本的语音识别系统的性能。methods: 这个论文使用了预训练的图像和文本编码器，并且提出了一种层次 segmental speech CLIP 模型来生成字元单元序列。results: 该论文的实验结果表明，使用预训练的文本编码器可以提高语音识别系统的性能，而且audio-only系统可以与audio-visual系统相似的性能。<details>
<summary>Abstract</summary>
Visually grounded speech systems learn from paired images and their spoken captions. Recently, there have been attempts to utilize the visually grounded models trained from images and their corresponding text captions, such as CLIP, to improve speech-based visually grounded models' performance. However, the majority of these models only utilize the pretrained image encoder. Cascaded SpeechCLIP attempted to generate localized word-level information and utilize both the pretrained image and text encoders. Despite using both, they noticed a substantial drop in retrieval performance. We proposed Segmental SpeechCLIP which used a hierarchical segmental speech encoder to generate sequences of word-like units. We used the pretrained CLIP text encoder on top of these word-like unit representations and showed significant improvements over the cascaded variant of SpeechCLIP. Segmental SpeechCLIP directly learns the word embeddings as input to the CLIP text encoder bypassing the vocabulary embeddings. Here, we explore mapping audio to CLIP vocabulary embeddings via regularization and quantization. As our objective is to distill semantic information into the speech encoders, we explore the usage of large unimodal pretrained language models as the text encoders. Our method enables us to bridge image and text encoders e.g. DINO and RoBERTa trained with uni-modal data. Finally, we extend our framework in audio-only settings where only pairs of semantically related audio are available. Experiments show that audio-only systems perform close to the audio-visual system.
</details>
<details>
<summary>摘要</summary>
视觉固定的语音系统学习从对应的图像和语音标签上的Pairing图像和语音标签。最近，有人尝试使用已经训练过图像和语音标签的视觉固定模型，如CLIP，来提高语音基于视觉固定模型的性能。然而，大多数这些模型只使用预训练的图像编码器。另一方面，Cascaded SpeechCLIP尝试生成本地化的单词级信息，并使用预训练的图像和语音编码器。尽管使用了两者，但它们发现了重要的搜索性能下降。我们提出了Segmental SpeechCLIP，它使用层次 segmental 语音编码器来生成序列化的单词单元表示。我们使用预训练的 CLIP 文本编码器在这些单词单元表示上，并显示了重要的改进。Segmental SpeechCLIP直接学习 word 嵌入，而不是使用词汇嵌入。我们 explore 将音频映射到 CLIP 词汇嵌入，并通过归一化和量化来进行规范。我们的目标是将语音编码器中的semantic信息储存下来。我们 explore 使用大型单modal预训练语言模型作为文本编码器，以 bridge 图像和文本编码器，例如 DINO 和 RoBERTa 在单modal数据上进行训练。最后，我们扩展了框架，在具有单个相关的音频对象的 audio-only 设置中进行学习。实验结果表明， audio-only 系统在 audio-visual 系统之间几乎具有相同的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Power-of-Sound-TPoS-Audio-Reactive-Video-Generation-with-Stable-Diffusion"><a href="#The-Power-of-Sound-TPoS-Audio-Reactive-Video-Generation-with-Stable-Diffusion" class="headerlink" title="The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion"></a>The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04509">http://arxiv.org/abs/2309.04509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo, Wonmin Byeon, Sangpil Kim, Jinkyu Kim</li>
<li>for:  Audio-to-video generation, incorporating temporal semantics and magnitude from audio input.</li>
<li>methods:  Latent stable diffusion model with textual semantic information, guided by sequential audio embedding from pretrained Audio Encoder.</li>
<li>results:  Audio reactive video contents, demonstrated effective across various tasks and compared with current state-of-the-art techniques.Here’s the full Chinese text:</li>
<li>for: 这篇论文是为了探讨音频到视频生成的问题，利用音频输入中的时间 semantics和幅度。</li>
<li>methods: 这篇论文提出了一种基于稳定扩散模型的 latent stable diffusion model，通过与预训练的音频编码器的文本semantic信息相结合，以便在视频帧中引入音频信息。</li>
<li>results: 这种方法可以生成响应音频的视频内容，在多个任务上进行了证明，并与当前领域的状态级技术进行了比较。更多示例可以在 <a target="_blank" rel="noopener" href="https://ku-vai.github.io/TPoS/">https://ku-vai.github.io/TPoS/</a> 上找到。<details>
<summary>Abstract</summary>
In recent years, video generation has become a prominent generative tool and has drawn significant attention. However, there is little consideration in audio-to-video generation, though audio contains unique qualities like temporal semantics and magnitude. Hence, we propose The Power of Sound (TPoS) model to incorporate audio input that includes both changeable temporal semantics and magnitude. To generate video frames, TPoS utilizes a latent stable diffusion model with textual semantic information, which is then guided by the sequential audio embedding from our pretrained Audio Encoder. As a result, this method produces audio reactive video contents. We demonstrate the effectiveness of TPoS across various tasks and compare its results with current state-of-the-art techniques in the field of audio-to-video generation. More examples are available at https://ku-vai.github.io/TPoS/
</details>
<details>
<summary>摘要</summary>
在最近的年头，视频生成技术已经成为了辉煌的一种生成工具，吸引了广泛的注意力。然而，尚未得到足够的关注是声音输入的生成，尽管声音含有独特的时间 semantics和幅度。因此，我们提出了声音力（TPoS）模型，以包括声音输入，包括可变的时间 semantics和幅度。为生成视频帧，TPoS使用了稳定扩散模型，并使用我们预训练的音频编码器提供的顺序音频嵌入。因此，这种方法可以生成响应声音的视频内容。我们在不同任务上证明了 TPoS 的效果，并与当前领域的音频到视频生成技术进行了比较。更多示例可以在 <https://ku-vai.github.io/TPoS/> 查看。
</details></li>
</ul>
<hr>
<h2 id="A-Long-Tail-Friendly-Representation-Framework-for-Artist-and-Music-Similarity"><a href="#A-Long-Tail-Friendly-Representation-Framework-for-Artist-and-Music-Similarity" class="headerlink" title="A Long-Tail Friendly Representation Framework for Artist and Music Similarity"></a>A Long-Tail Friendly Representation Framework for Artist and Music Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04182">http://arxiv.org/abs/2309.04182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Xiang, Junyu Dai, Xuchen Song, Furao Shen</li>
<li>for: 这篇论文的目的是提出一种适应长尾现象的艺术家和音乐相似性模型，以提高音乐搜索和推荐的精度。</li>
<li>methods: 该论文提出了一种基于神经网络的长尾友好表示框架（LTFRF），该框架将音乐、用户、元数据和关系数据集成到一个统一的 métric 学习框架中，并使用多种关系为正则项引入多关系损失。</li>
<li>results: 根据对 AllMusic 数据集的实验和分析，提出的 LTFRF 模型在类似艺术家&#x2F;音乐推荐任务中表现出色，比基eline 高出9.69%&#x2F;19.42% 的 Hit Ratio@10，而在长尾情况下，该模型达到了11.05%&#x2F;14.14% 的提高。<details>
<summary>Abstract</summary>
The investigation of the similarity between artists and music is crucial in music retrieval and recommendation, and addressing the challenge of the long-tail phenomenon is increasingly important. This paper proposes a Long-Tail Friendly Representation Framework (LTFRF) that utilizes neural networks to model the similarity relationship. Our approach integrates music, user, metadata, and relationship data into a unified metric learning framework, and employs a meta-consistency relationship as a regular term to introduce the Multi-Relationship Loss. Compared to the Graph Neural Network (GNN), our proposed framework improves the representation performance in long-tail scenarios, which are characterized by sparse relationships between artists and music. We conduct experiments and analysis on the AllMusic dataset, and the results demonstrate that our framework provides a favorable generalization of artist and music representation. Specifically, on similar artist/music recommendation tasks, the LTFRF outperforms the baseline by 9.69%/19.42% in Hit Ratio@10, and in long-tail cases, the framework achieves 11.05%/14.14% higher than the baseline in Consistent@10.
</details>
<details>
<summary>摘要</summary>
investigation of artist and music similarity crucial in music retrieval and recommendation, addressing long-tail challenge increasingly important. This paper proposes Long-Tail Friendly Representation Framework (LTFRF), utilizes neural networks to model similarity relationship. Our approach integrates music, user, metadata, and relationship data into unified metric learning framework, employs meta-consistency relationship as regular term to introduce Multi-Relationship Loss. Compared to Graph Neural Network (GNN), our proposed framework improves representation performance in long-tail scenarios, characterized by sparse relationships between artists and music. We conduct experiments and analysis on AllMusic dataset, results demonstrate that our framework provides favorable generalization of artist and music representation. Specifically, on similar artist/music recommendation tasks, LTFRF outperforms baseline by 9.69%/19.42% in Hit Ratio@10, and in long-tail cases, framework achieves 11.05%/14.14% higher than baseline in Consistent@10.
</details></li>
</ul>
<hr>
<h2 id="Cross-Utterance-Conditioned-VAE-for-Speech-Generation"><a href="#Cross-Utterance-Conditioned-VAE-for-Speech-Generation" class="headerlink" title="Cross-Utterance Conditioned VAE for Speech Generation"></a>Cross-Utterance Conditioned VAE for Speech Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04156">http://arxiv.org/abs/2309.04156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Li, Cheng Yu, Guangzhi Sun, Weiqin Zu, Zheng Tian, Ying Wen, Wei Pan, Chao Zhang, Jun Wang, Yang Yang, Fanglei Sun</li>
<li>for: 提高语音生成的自然性和表情能力，以及提供更加灵活的语音编辑功能。</li>
<li>methods: 利用预训练语言模型和变量自动编码器（VAEs）的 Representational Capabilities，并开发了两种实用算法：CUC-VAE TTS 和 CUC-VAE SE。</li>
<li>results: 实验结果表明，我们的提议模型可以显著提高语音生成和编辑，生成更加自然和表情强的语音。<details>
<summary>Abstract</summary>
Speech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, designed to generate audio with contextual prosody derived from surrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real mel spectrogram sampling conditioned on contextual information, producing audio that closely mirrors real sound and thereby facilitating flexible speech editing based on text such as deletion, insertion, and replacement. Experimental results on the LibriTTS datasets demonstrate that our proposed models significantly enhance speech synthesis and editing, producing more natural and expressive speech.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CNSpeech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, designed to generate audio with contextual prosody derived from surrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real mel spectrogram sampling conditioned on contextual information, producing audio that closely mirrors real sound and thereby facilitating flexible speech editing based on text such as deletion, insertion, and replacement. Experimental results on the LibriTTS datasets demonstrate that our proposed models significantly enhance speech synthesis and editing, producing more natural and expressive speech.Note: The "translate_language" parameter is set to "zh-CN" to specify Simplified Chinese as the target language.
</details></li>
</ul>
<hr>
<h2 id="A-Two-Stage-Training-Framework-for-Joint-Speech-Compression-and-Enhancement"><a href="#A-Two-Stage-Training-Framework-for-Joint-Speech-Compression-and-Enhancement" class="headerlink" title="A Two-Stage Training Framework for Joint Speech Compression and Enhancement"></a>A Two-Stage Training Framework for Joint Speech Compression and Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04132">http://arxiv.org/abs/2309.04132</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jscscloris/SEStream">https://github.com/jscscloris/SEStream</a></li>
<li>paper_authors: Jiayi Huang, Zeyu Yan, Wenbin Jiang, Fei Wen</li>
<li>for: 这 paper 考虑了对噪音 speech signal 的共同压缩和改进问题。</li>
<li>methods: 这 paper 使用了一种两个阶段优化过程，首先优化一个 encoder-decoder 对，然后使用 perception loss 优化一个 perceived decoder。</li>
<li>results: 实验结果表明，基于这两个阶段优化过程，可以在噪音 condition 下实现更高的评价质量和更低的Distortion。这种 two-stage 训练方法比传统的 heuristic 方法更有理论基础。<details>
<summary>Abstract</summary>
This paper considers the joint compression and enhancement problem for speech signal in the presence of noise. Recently, the SoundStream codec, which relies on end-to-end joint training of an encoder-decoder pair and a residual vector quantizer by a combination of adversarial and reconstruction losses,has shown very promising performance, especially in subjective perception quality. In this work, we provide a theoretical result to show that, to simultaneously achieve low distortion and high perception in the presence of noise, there exist an optimal two-stage optimization procedure for the joint compression and enhancement problem. This procedure firstly optimizes an encoder-decoder pair using only distortion loss and then fixes the encoder to optimize a perceptual decoder using perception loss. Based on this result, we construct a two-stage training framework for joint compression and enhancement of noisy speech signal. Unlike existing training methods which are heuristic, the proposed two-stage training method has a theoretical foundation. Finally, experimental results for various noise and bit-rate conditions are provided. The results demonstrate that a codec trained by the proposed framework can outperform SoundStream and other representative codecs in terms of both objective and subjective evaluation metrics. Code is available at \textit{https://github.com/jscscloris/SEStream}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/08/cs.SD_2023_09_08/" data-id="clmjn91of00bz0j888ol860vx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_09_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/08/eess.AS_2023_09_08/" class="article-date">
  <time datetime="2023-09-08T14:00:00.000Z" itemprop="datePublished">2023-09-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/08/eess.AS_2023_09_08/">eess.AS - 2023-09-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Asymmetric-Clean-Segments-Guided-Self-Supervised-Learning-for-Robust-Speaker-Verification"><a href="#Asymmetric-Clean-Segments-Guided-Self-Supervised-Learning-for-Robust-Speaker-Verification" class="headerlink" title="Asymmetric Clean Segments-Guided Self-Supervised Learning for Robust Speaker Verification"></a>Asymmetric Clean Segments-Guided Self-Supervised Learning for Robust Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04265">http://arxiv.org/abs/2309.04265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong-Xin Gan, Man-Wai Mak, Weiwei Lin, Jen-Tzung Chien</li>
<li>for: 提高Speaker Verification（SV）的精度，利用无标注数据。</li>
<li>methods: 具有批量增强的Contrastive Self-Supervised Learning（CSL）方法，使用Raw Waveform数据进行数据增强。</li>
<li>results: 在Voxceleb1数据集上实现19%的提升，超越许多现有的State-of-the-art技术。<details>
<summary>Abstract</summary>
Contrastive self-supervised learning (CSL) for speaker verification (SV) has drawn increasing interest recently due to its ability to exploit unlabeled data. Performing data augmentation on raw waveforms, such as adding noise or reverberation, plays a pivotal role in achieving promising results in SV. Data augmentation, however, demands meticulous calibration to ensure intact speaker-specific information, which is difficult to achieve without speaker labels. To address this issue, we introduce a novel framework by incorporating clean and augmented segments into the contrastive training pipeline. The clean segments are repurposed to pair with noisy segments to form additional positive and negative pairs. Moreover, the contrastive loss is weighted to increase the difference between the clean and augmented embeddings of different speakers. Experimental results on Voxceleb1 suggest that the proposed framework can achieve a remarkable 19% improvement over the conventional methods, and it surpasses many existing state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
受欢迎的对比自我超vised学习（CSL）在Speaker Verification（SV）中的应用已经吸引了越来越多的关注，这是因为它可以利用无标签数据。在Raw waveform上进行数据增强，如添加噪音或投射，在SV中得到了出色的结果。然而，数据增强需要精心调整，以确保保留Speaker-specific信息，这是无法实现的 безspeaker标签。为解决这个问题，我们提出了一种新的框架，即在对比训练管道中包含干净和增强段。干净段被重新用于与噪音段组成额外的正例和负例对。此外，对比损失中对干净和增强 embeddings的不同Speaker之间的差异加大。实验结果表明，我们的方法在Voxceleb1上达到了19%的提升，超过了许多现有的状态之策。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/08/eess.AS_2023_09_08/" data-id="clmjn91pj00e50j88bi992e3z" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/08/cs.LG_2023_09_08/" class="article-date">
  <time datetime="2023-09-08T10:00:00.000Z" itemprop="datePublished">2023-09-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/08/cs.LG_2023_09_08/">cs.LG - 2023-09-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Few-Shot-Learning-of-Force-Based-Motions-From-Demonstration-Through-Pre-training-of-Haptic-Representation"><a href="#Few-Shot-Learning-of-Force-Based-Motions-From-Demonstration-Through-Pre-training-of-Haptic-Representation" class="headerlink" title="Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation"></a>Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04640">http://arxiv.org/abs/2309.04640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marina Y. Aoyama, João Moura, Namiko Saito, Sethu Vijayakumar</li>
<li>for: 能够快速适应不同物体的物理特性，提高机器人抓取物体的能力。</li>
<li>methods: 使用半监督学习自动机制，将学习模型分解成感觉表示编码器和动作生成解码器。首先使用大量未经监督的数据进行预训练，然后使用少量监督学习来训练动作生成解码器，以便快速适应不同物体的物理特性。</li>
<li>results: 对干洗任务使用不同弹性和表面黏度的毛巾进行验证，结果表明预训练可以大幅提高下游任务中机器人对物体物理特性的认识和生成恰当的动作，超过了没有预训练的LfD方法。此外，我们还验证了在物理机器人硬件上运行的动作是否符合预期，并证明感觉表示编码器在实际物体上采集的数据上具有良好的表达能力，从而解释了它在下游任务中的贡献。<details>
<summary>Abstract</summary>
In many contact-rich tasks, force sensing plays an essential role in adapting the motion to the physical properties of the manipulated object. To enable robots to capture the underlying distribution of object properties necessary for generalising learnt manipulation tasks to unseen objects, existing Learning from Demonstration (LfD) approaches require a large number of costly human demonstrations. Our proposed semi-supervised LfD approach decouples the learnt model into an haptic representation encoder and a motion generation decoder. This enables us to pre-train the first using large amount of unsupervised data, easily accessible, while using few-shot LfD to train the second, leveraging the benefits of learning skills from humans. We validate the approach on the wiping task using sponges with different stiffness and surface friction. Our results demonstrate that pre-training significantly improves the ability of the LfD model to recognise physical properties and generate desired wiping motions for unseen sponges, outperforming the LfD method without pre-training. We validate the motion generated by our semi-supervised LfD model on the physical robot hardware using the KUKA iiwa robot arm. We also validate that the haptic representation encoder, pre-trained in simulation, captures the properties of real objects, explaining its contribution to improving the generalisation of the downstream task.
</details>
<details>
<summary>摘要</summary>
多个contact-rich任务中，力感Play an essential role in adapting motion to the physical properties of the manipulated object。To enable robots to capture the underlying distribution of object properties necessary for generalizing learnt manipulation tasks to unseen objects, existing Learning from Demonstration (LfD) approaches require a large number of costly human demonstrations。Our proposed semi-supervised LfD approach decouples the learnt model into an haptic representation encoder and a motion generation decoder。This enables us to pre-train the first using large amount of unsupervised data, easily accessible, while using few-shot LfD to train the second, leveraging the benefits of learning skills from humans。We validate the approach on the wiping task using sponges with different stiffness and surface friction。Our results demonstrate that pre-training significantly improves the ability of the LfD model to recognize physical properties and generate desired wiping motions for unseen sponges, outperforming the LfD method without pre-training。We validate the motion generated by our semi-supervised LfD model on the physical robot hardware using the KUKA iiwa robot arm。We also validate that the haptic representation encoder, pre-trained in simulation, captures the properties of real objects, explaining its contribution to improving the generalization of the downstream task。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Safety-Regions-Via-Finite-Families-of-Scalable-Classifiers"><a href="#Probabilistic-Safety-Regions-Via-Finite-Families-of-Scalable-Classifiers" class="headerlink" title="Probabilistic Safety Regions Via Finite Families of Scalable Classifiers"></a>Probabilistic Safety Regions Via Finite Families of Scalable Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04627">http://arxiv.org/abs/2309.04627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Carlevaro, Teodoro Alamo, Fabrizio Dabbene, Maurizio Mongelli</li>
<li>for: 这个论文主要探讨了超vision的分类问题，即在分类器中控制错误率的问题。</li>
<li>methods: 该论文提出了一种基于概率安全区的方法，通过控制输入空间中错误类的数量来实现错误率的控制。此外，该论文还提出了一种可扩展的分类器，可以适应不同的错误率要求。</li>
<li>results: 经过一些测试，该论文得出了一些可靠的结论，表明该方法可以有效地控制错误率，并且可以适应不同的分类任务。<details>
<summary>Abstract</summary>
Supervised classification recognizes patterns in the data to separate classes of behaviours. Canonical solutions contain misclassification errors that are intrinsic to the numerical approximating nature of machine learning. The data analyst may minimize the classification error on a class at the expense of increasing the error of the other classes. The error control of such a design phase is often done in a heuristic manner. In this context, it is key to develop theoretical foundations capable of providing probabilistic certifications to the obtained classifiers. In this perspective, we introduce the concept of probabilistic safety region to describe a subset of the input space in which the number of misclassified instances is probabilistically controlled. The notion of scalable classifiers is then exploited to link the tuning of machine learning with error control. Several tests corroborate the approach. They are provided through synthetic data in order to highlight all the steps involved, as well as through a smart mobility application.
</details>
<details>
<summary>摘要</summary>
超visited分类可以识别数据中的模式，以分类行为类型。可能的解决方案中包含了内在的分类错误，这是机器学习的数值近似性所带来的。数据分析师可能会为某个类减少分类错误，但是这将导致其他类的错误增加。这种设计阶段的错误控制通常是 empirical的。在这种情况下，我们引入了概率安全区域，用于描述输入空间中的一个子集，其中数据分类错误的概率是控制的。我们然后利用可扩展分类器来联系机器学习的调整与错误控制。多个测试证明了这种方法的有效性，其中包括synthetic数据和一个智能移动应用。
</details></li>
</ul>
<hr>
<h2 id="Perceptual-adjustment-queries-and-an-inverted-measurement-paradigm-for-low-rank-metric-learning"><a href="#Perceptual-adjustment-queries-and-an-inverted-measurement-paradigm-for-low-rank-metric-learning" class="headerlink" title="Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning"></a>Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04626">http://arxiv.org/abs/2309.04626</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/austinxu87/paq">https://github.com/austinxu87/paq</a></li>
<li>paper_authors: Austin Xu, Andrew D. McRae, Jingyan Wang, Mark A. Davenport, Ashwin Pananjady</li>
<li>for: 这个论文是为了提出一种新的查询机制，以收集人类反馈，并且这种查询机制被称为媒体调整查询（PAQ）。</li>
<li>methods: 这种查询机制采用倒掌计量方式，并且结合了 cardinal 和 ordinal 查询的优点。</li>
<li>results: 作者在度量学习问题中使用 PAQ 获取了 unknown Mahalanobis distance，并且这导致了一个高维、低级 matrix estimation 问题，这个问题不能由标准矩阵估计器解决。因此，作者提出了一种两个阶段估计器，并提供了样本复杂性保证。作者还通过 numerics  simulate 这种估计器的性能和其特点。<details>
<summary>Abstract</summary>
We introduce a new type of query mechanism for collecting human feedback, called the perceptual adjustment query ( PAQ). Being both informative and cognitively lightweight, the PAQ adopts an inverted measurement scheme, and combines advantages from both cardinal and ordinal queries. We showcase the PAQ in the metric learning problem, where we collect PAQ measurements to learn an unknown Mahalanobis distance. This gives rise to a high-dimensional, low-rank matrix estimation problem to which standard matrix estimators cannot be applied. Consequently, we develop a two-stage estimator for metric learning from PAQs, and provide sample complexity guarantees for this estimator. We present numerical simulations demonstrating the performance of the estimator and its notable properties.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的查询机制，即感知调整查询（PAQ），它同时具有信息量和认知强度的优点。PAQ采用倒推计量方式，并将 cardinal和ordinal查询的优点结合在一起。我们在 métric learning 问题中使用 PAQ 进行学习未知的 Mahalanobis 距离。这导致了一个高维、低纬度矩阵估计问题，标准矩阵估计器无法应用。因此，我们开发了一种两个阶段的估计器，并提供了样本复杂性保证。我们通过数学实验证明了估计器的性能和其他特点。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Distillation-Empowered-Digital-Twin-for-Anomaly-Detection"><a href="#Knowledge-Distillation-Empowered-Digital-Twin-for-Anomaly-Detection" class="headerlink" title="Knowledge Distillation-Empowered Digital Twin for Anomaly Detection"></a>Knowledge Distillation-Empowered Digital Twin for Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04616">http://arxiv.org/abs/2309.04616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinghua Xu, Shaukat Ali, Tao Yue, Zaimovic Nedim, Inderjeet Singh</li>
<li>for: 这篇论文的目的是提出一种基于语言模型和长短期记忆网络的异常检测方法，以确保铁路控制和管理系统（TCMS）的可靠性。</li>
<li>methods: 这篇论文使用了语言模型（LM）和长短期记忆网络（LSTM）来提取上下文特征和时间特征，并通过知识储存（KD）来增加数据量。</li>
<li>results: 根据两个来自我司合作伙伴阿尔斯通的数据集， authors 证明了 KDDT 的效果，其中 F1 分数分别为 0.931 和 0.915。此外，作者还进行了一项完整的实验研究，发现 KDDT 模型中语言模型、LSTM 网络和 KD 的个人贡献均有显著提高效果，其中平均 F1 分数提高12.4%、3% 和 6.05%。<details>
<summary>Abstract</summary>
Cyber-physical systems (CPSs), like train control and management systems (TCMS), are becoming ubiquitous in critical infrastructures. As safety-critical systems, ensuring their dependability during operation is crucial. Digital twins (DTs) have been increasingly studied for this purpose owing to their capability of runtime monitoring and warning, prediction and detection of anomalies, etc. However, constructing a DT for anomaly detection in TCMS necessitates sufficient training data and extracting both chronological and context features with high quality. Hence, in this paper, we propose a novel method named KDDT for TCMS anomaly detection. KDDT harnesses a language model (LM) and a long short-term memory (LSTM) network to extract contexts and chronological features, respectively. To enrich data volume, KDDT benefits from out-of-domain data with knowledge distillation (KD). We evaluated KDDT with two datasets from our industry partner Alstom and obtained the F1 scores of 0.931 and 0.915, respectively, demonstrating the effectiveness of KDDT. We also explored individual contributions of the DT model, LM, and KD to the overall performance of KDDT, via a comprehensive empirical study, and observed average F1 score improvements of 12.4%, 3%, and 6.05%, respectively.
</details>
<details>
<summary>摘要</summary>
Cyber-physical systems (CPSs), like train control and management systems (TCMS), are becoming more and more common in critical infrastructures. As safety-critical systems, ensuring their reliability during operation is crucial. Digital twins (DTs) have been studied more and more in recent years because they can monitor and warn of anomalies in real-time, predict and detect anomalies, etc. However, creating a DT for anomaly detection in TCMS requires a lot of training data and extracting both chronological and context features of high quality. Therefore, in this paper, we propose a new method called KDDT for TCMS anomaly detection. KDDT uses a language model (LM) and a long short-term memory (LSTM) network to extract contexts and chronological features, respectively. To increase the amount of data, KDDT uses knowledge distillation (KD) to benefit from out-of-domain data. We evaluated KDDT with two datasets from our industry partner Alstom and obtained F1 scores of 0.931 and 0.915, respectively, which demonstrates the effectiveness of KDDT. We also conducted a comprehensive empirical study to explore the individual contributions of the DT model, LM, and KD to the overall performance of KDDT, and observed average F1 score improvements of 12.4%, 3%, and 6.05%, respectively.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-World-Model-Disentanglement-in-Value-Based-Multi-Agent-Reinforcement-Learning"><a href="#Leveraging-World-Model-Disentanglement-in-Value-Based-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning"></a>Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04615">http://arxiv.org/abs/2309.04615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhizun Wang, David Meger</li>
<li>for:  addresses the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity.</li>
<li>methods:  uses a modularized world model, variational auto-encoders, and variational graph auto-encoders to learn latent representations and predict the joint action-value function.</li>
<li>results:  achieves high sample efficiency and exhibits superior performance in defeating the enemy armies compared to other baselines, as demonstrated in experiments on StarCraft II micro-management challenges.Here’s the full text in Simplified Chinese:</li>
<li>for: 本文提出了一种基于模型的多体强化学习方法，名为Value Decomposition Framework with Disentangled World Model，用于解决多体系统中 agent 之间交互的同时 achievement 目标的挑战，并且减少样本复杂性。</li>
<li>methods: 该方法使用了模块化的世界模型，包括动作决定、动作无关、静态分支，通过 past 经验来推断环境动力学，而不是直接从真实环境中采样。使用了 variational auto-encoders 和 variational graph auto-encoders 来学习 latent 表示，并将其与值基 Framework 混合，以预测 JOINT 动作价值函数，并优化整体训练目标。</li>
<li>results:  experiments 表明，该方法可以减少样本复杂性，并且在 StarCraft II 微管理挑战中表现出色，比基eline 高效，能够征服敌方军队。<details>
<summary>Abstract</summary>
In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the environment dynamics and produce imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. We present experimental results in Easy, Hard, and Super-Hard StarCraft II micro-management challenges to demonstrate that our method achieves high sample efficiency and exhibits superior performance in defeating the enemy armies compared to other baselines.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的模型基于多智能人工智能学习方法，名为值分解框架，以解决多智能系统中多个智能机器人之间共同目标的挑战，即降低训练样本复杂性。由于多智能系统的扩展性和不确定性问题，无模型方法需要训练很多样本。相比之下，我们使用模块化的世界模型，包括动作受控、动作无关和静态分支，来解释环境动力学和生成基于过去经验的假设结果，而不是直接从实际环境中采样。我们使用变量自动编码器和变量图自动编码器来学习世界模型的秘密表示，然后将其与值基于框架相结合，预测共同动作价值函数，并优化总训练目标。我们在Easy、Hard和Super-Hard StarCraft II微管理挑战中实现了实验，并证明了我们的方法可以具有高效样本率和在击败敌方军队方面表现出色，相比其他基准值。
</details></li>
</ul>
<hr>
<h2 id="Self-optimizing-Feature-Generation-via-Categorical-Hashing-Representation-and-Hierarchical-Reinforcement-Crossing"><a href="#Self-optimizing-Feature-Generation-via-Categorical-Hashing-Representation-and-Hierarchical-Reinforcement-Crossing" class="headerlink" title="Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing"></a>Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04612">http://arxiv.org/abs/2309.04612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yingwangyang/hrc_feature_cross">https://github.com/yingwangyang/hrc_feature_cross</a></li>
<li>paper_authors: Wangyang Ying, Dongjie Wang, Kunpeng Liu, Leilei Sun, Yanjie Fu</li>
<li>for: 本文主要针对自动生成有用特征的问题，以便创建一个决定性表示空间。</li>
<li>methods: 本文提出了一种原则性的和通用的表示 crossing 框架，用于解决自动生成特征的挑战。这个框架包括特征精度化、特征哈希和描述性概要。</li>
<li>results: 经过EXTENSIVE实验 validate，提出的方法能够效果地和有效地解决自动生成特征的问题。代码可以在<a target="_blank" rel="noopener" href="https://github.com/yingwangyang/HRC_feature_cross.git%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yingwangyang/HRC_feature_cross.git中找到。</a><details>
<summary>Abstract</summary>
Feature generation aims to generate new and meaningful features to create a discriminative representation space.A generated feature is meaningful when the generated feature is from a feature pair with inherent feature interaction. In the real world, experienced data scientists can identify potentially useful feature-feature interactions, and generate meaningful dimensions from an exponentially large search space, in an optimal crossing form over an optimal generation path. But, machines have limited human-like abilities.We generalize such learning tasks as self-optimizing feature generation. Self-optimizing feature generation imposes several under-addressed challenges on existing systems: meaningful, robust, and efficient generation. To tackle these challenges, we propose a principled and generic representation-crossing framework to solve self-optimizing feature generation.To achieve hashing representation, we propose a three-step approach: feature discretization, feature hashing, and descriptive summarization. To achieve reinforcement crossing, we develop a hierarchical reinforcement feature crossing approach.We present extensive experimental results to demonstrate the effectiveness and efficiency of the proposed method. The code is available at https://github.com/yingwangyang/HRC_feature_cross.git.
</details>
<details>
<summary>摘要</summary>
Feature Generation的目的是生成新的有意义特征，以创建一个分类表示空间。一个生成的特征是有意义的当该特征来自一个内在具有自然交互的特征对。在现实世界中，经验丰富的数据科学家可以识别和生成有用的特征对互动，从极大的搜索空间中选择优化的生成路径，以获得优化的特征生成。但是，机器有限的人类能力。因此，我们总结这类学习任务为自动化特征生成。自动化特征生成面临许多未得到充分解决的挑战：有意义、Robust和高效的生成。为解决这些挑战，我们提出了一种原则性的和通用的表示交叉框架，用于解决自动化特征生成。为实现哈希表示，我们提出了三步方法：特征缩短、特征哈希和描述概要。为实现强化交叉，我们开发了层次强化特征交叉方法。我们提供了广泛的实验结果，以证明我们的方法的有效性和高效性。代码可以在<https://github.com/yingwangyang/HRC_feature_cross.git>上获取。
</details></li>
</ul>
<hr>
<h2 id="Online-Infinite-Dimensional-Regression-Learning-Linear-Operators"><a href="#Online-Infinite-Dimensional-Regression-Learning-Linear-Operators" class="headerlink" title="Online Infinite-Dimensional Regression: Learning Linear Operators"></a>Online Infinite-Dimensional Regression: Learning Linear Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06548">http://arxiv.org/abs/2309.06548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinod Raman, Unique Subedi, Ambuj Tewari</li>
<li>for: 本文研究了在线学习线性算子，具体是在两个无穷dimensional勒比希尔бер空间之间的平方损失下学习线性算子。</li>
<li>methods: 本文使用了在线学习方法，包括uniform bounded $p$-Schatten norm和operator norm等方法。</li>
<li>results: 本文证明了线性算子的在线学习可行性，但是uniform convergence不是必要的，同时还提供了一个分离uniform convergence和学习可行性的示例。此外，本文还证明了这些结论在agnostic PAC Setting下也成立。<details>
<summary>Abstract</summary>
We consider the problem of learning linear operators under squared loss between two infinite-dimensional Hilbert spaces in the online setting. We show that the class of linear operators with uniformly bounded $p$-Schatten norm is online learnable for any $p \in [1, \infty)$. On the other hand, we prove an impossibility result by showing that the class of uniformly bounded linear operators with respect to the operator norm is \textit{not} online learnable. Moreover, we show a separation between online uniform convergence and online learnability by identifying a class of bounded linear operators that is online learnable but uniform convergence does not hold. Finally, we prove that the impossibility result and the separation between uniform convergence and learnability also hold in the agnostic PAC setting.
</details>
<details>
<summary>摘要</summary>
我团队考虑了在线学习线性算子的问题，特别是在两个无穷维希尔伯特空间之间的平方损失下学习线性算子。我们表明了任何 $p \in [1, \infty) $ 的线性算子具有 uniformly bounded $p$-Schatten norm 是在线学习的。然而，我们证明了一个不可能性结论，即对于Operator norm 上的 uniformly bounded线性算子，不是在线学习的。此外，我们还表明了在线学习和在线统一收敛之间的分离，通过证明一个受限的线性算子是在线学习的，但uniform convergence不成立。最后，我们证明了这些结论也适用于agnostic PAC Setting。
</details></li>
</ul>
<hr>
<h2 id="Motif-aware-Attribute-Masking-for-Molecular-Graph-Pre-training"><a href="#Motif-aware-Attribute-Masking-for-Molecular-Graph-Pre-training" class="headerlink" title="Motif-aware Attribute Masking for Molecular Graph Pre-training"></a>Motif-aware Attribute Masking for Molecular Graph Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04589">http://arxiv.org/abs/2309.04589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/einae-nd/moama-dev">https://github.com/einae-nd/moama-dev</a></li>
<li>paper_authors: Eric Inae, Gang Liu, Meng Jiang</li>
<li>for: 预训练图解链接网络的属性重建，以捕捉分子结构知识，并将其应用于化学、生物医学和材料科学中的多种下游性预测任务。</li>
<li>methods: 提出了一种基于分子结构的 attribute 掩码策略，通过沟通邻近分子结构的信息，捕捉高级分子结构的知识，并将其应用于图解链接网络的预训练中。</li>
<li>results: 对八个分子性质预测数据集进行了评估，并证明了该策略的优势。<details>
<summary>Abstract</summary>
Attribute reconstruction is used to predict node or edge features in the pre-training of graph neural networks. Given a large number of molecules, they learn to capture structural knowledge, which is transferable for various downstream property prediction tasks and vital in chemistry, biomedicine, and material science. Previous strategies that randomly select nodes to do attribute masking leverage the information of local neighbors However, the over-reliance of these neighbors inhibits the model's ability to learn from higher-level substructures. For example, the model would learn little from predicting three carbon atoms in a benzene ring based on the other three but could learn more from the inter-connections between the functional groups, or called chemical motifs. In this work, we propose and investigate motif-aware attribute masking strategies to capture inter-motif structures by leveraging the information of atoms in neighboring motifs. Once each graph is decomposed into disjoint motifs, the features for every node within a sample motif are masked. The graph decoder then predicts the masked features of each node within the motif for reconstruction. We evaluate our approach on eight molecular property prediction datasets and demonstrate its advantages.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate given text into Simplified Chinese.<</SYS>> attribute reconstruction 是用于预训练图 neural networks 中预测节点或边特征的技术。通过大量的分子，它们学习捕捉结构知识，这种知识可以转移到下游性质预测任务中，在化学、生物医学和材料科学中都非常重要。在先前的策略中，随机选择节点进行特征遮盲，利用当地节点的信息，但这会限制模型学习高级结构的能力。例如，模型在预测三个碳原子的benzene环中的预测结果很少，但可以从化学动机中获得更多的信息。在这种工作中，我们提出和研究motif-aware特征遮盲策略，通过利用邻近motif中的原子信息来捕捉高级结构。每个图被分解成不同的motif，然后每个样本motif中的节点特征被遮盲。图解码器然后预测每个节点的遮盲特征，以重建graph。我们在八个分子性质预测数据集上评估了我们的方法，并证明了它的优势。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Mesh-Aware-Radiance-Fields"><a href="#Dynamic-Mesh-Aware-Radiance-Fields" class="headerlink" title="Dynamic Mesh-Aware Radiance Fields"></a>Dynamic Mesh-Aware Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04581">http://arxiv.org/abs/2309.04581</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YilingQiao/DMRF">https://github.com/YilingQiao/DMRF</a></li>
<li>paper_authors: Yi-Ling Qiao, Alexander Gao, Yiran Xu, Yue Feng, Jia-Bin Huang, Ming C. Lin</li>
<li>For:  This paper aims to integrate NeRF (Neural Radiance Fields) into the traditional graphics pipeline, allowing for physically consistent rendering and simulation of mesh assets within NeRF volumes.* Methods: The paper proposes a two-way coupling between mesh and NeRF during rendering and simulation, using light transport equations to update radiance and throughput along a cast ray. The NeRF model is trained with HDR images to resolve color space discrepancies.* Results: The hybrid surface-volumetric formulation outperforms alternatives in visual realism for mesh insertion, as it allows for realistic light transport from volumetric NeRF media onto surfaces, affecting the appearance of reflective&#x2F;refractive surfaces and illumination of diffuse surfaces informed by the dynamic scene.Here is the same information in Simplified Chinese:* For: 这篇论文目标是将NeRF（神经辐射场）集成到传统图形管道中，以实现物理逻辑的渲染和实时计算材质资产在NeRF卷积体中。* Methods: 论文提议一种在渲染和计算中建立材质和NeRF之间的两种方向的相互关联，使用光传输方程来更新辐射和通过put along a cast ray with an arbitrary number of bounces。NeRF模型通过高动态范围（HDR）图像进行训练，以解决颜色空间差异。* Results: 混合表面-卷积型形式的方法在渲染和计算中超越了其他方法，因为它允许真实的光传输从NeRF卷积体媒体 onto 表面，影响了镜面&#x2F;塑化表面和diffuse表面的照明。<details>
<summary>Abstract</summary>
Embedding polygonal mesh assets within photorealistic Neural Radience Fields (NeRF) volumes, such that they can be rendered and their dynamics simulated in a physically consistent manner with the NeRF, is under-explored from the system perspective of integrating NeRF into the traditional graphics pipeline. This paper designs a two-way coupling between mesh and NeRF during rendering and simulation. We first review the light transport equations for both mesh and NeRF, then distill them into an efficient algorithm for updating radiance and throughput along a cast ray with an arbitrary number of bounces. To resolve the discrepancy between the linear color space that the path tracer assumes and the sRGB color space that standard NeRF uses, we train NeRF with High Dynamic Range (HDR) images. We also present a strategy to estimate light sources and cast shadows on the NeRF. Finally, we consider how the hybrid surface-volumetric formulation can be efficiently integrated with a high-performance physics simulator that supports cloth, rigid and soft bodies. The full rendering and simulation system can be run on a GPU at interactive rates. We show that a hybrid system approach outperforms alternatives in visual realism for mesh insertion, because it allows realistic light transport from volumetric NeRF media onto surfaces, which affects the appearance of reflective/refractive surfaces and illumination of diffuse surfaces informed by the dynamic scene.
</details>
<details>
<summary>摘要</summary>
将多面体网格资产集成到拟真度场（NeRF）中，以便在physically consistent manner中渲染和模拟，是从系统角度来看还尚未得到充分的探讨。这篇论文提出了在渲染和模拟过程中两向相互作用的方法。我们首先评论了 mesh和NeRF的光传输方程，然后将它们转化为更有效的算法来更新光芒和通过率。为了解决mesh和NeRF之间的颜色空间不同，我们在训练NeRF时使用高动态范围（HDR）图像。此外，我们还提出了一种策略来估计NeRF中的光源和投射阴影。最后，我们考虑了如何高效地将混合表面-体积形式化与高性能物理引擎集成，以实现交互式帧率。我们表明，使用混合系统方法可以在渲染和模拟过程中提高视觉实际性，因为它允许真实的光传输从体积NeRF媒体onto表面，这对折射/折射表面的外观和透射表面上的照明产生了影响。
</details></li>
</ul>
<hr>
<h2 id="Circles-Inter-Model-Comparison-of-Multi-Classification-Problems-with-High-Number-of-Classes"><a href="#Circles-Inter-Model-Comparison-of-Multi-Classification-Problems-with-High-Number-of-Classes" class="headerlink" title="Circles: Inter-Model Comparison of Multi-Classification Problems with High Number of Classes"></a>Circles: Inter-Model Comparison of Multi-Classification Problems with High Number of Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05672">http://arxiv.org/abs/2309.05672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nina Mir, Ragaad AlTarawneh, Shah Rukh Humayoun</li>
<li>for: 这篇论文旨在解决大量类别的分类问题中的视觉化和模型比较问题。</li>
<li>methods: 作者提出了一种交互式视觉分析工具，名为“环”，可以同时显示多个分类模型的比较结果，并采用了圆形卷积布局来减少视觉干扰。</li>
<li>results: 试验结果表明，“环”工具可以准确地显示9个模型的比较结果，并且通过交互式的方式，帮助用户更好地理解和分析这些模型的性能。<details>
<summary>Abstract</summary>
The recent advancements in machine learning have motivated researchers to generate classification models dealing with hundreds of classes such as in the case of image datasets. However, visualization of classification models with high number of classes and inter-model comparison in such classification problems are two areas that have not received much attention in the literature, despite the ever-increasing use of classification models to address problems with very large class categories. In this paper, we present our interactive visual analytics tool, called Circles, that allows a visual inter-model comparison of numerous classification models with 1K classes in one view. To mitigate the tricky issue of visual clutter, we chose concentric a radial line layout for our inter-model comparison task. Our prototype shows the results of 9 models with 1K classes
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unleashing-the-Power-of-Graph-Learning-through-LLM-based-Autonomous-Agents"><a href="#Unleashing-the-Power-of-Graph-Learning-through-LLM-based-Autonomous-Agents" class="headerlink" title="Unleashing the Power of Graph Learning through LLM-based Autonomous Agents"></a>Unleashing the Power of Graph Learning through LLM-based Autonomous Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04565">http://arxiv.org/abs/2309.04565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lanning Wei, Zhiqiang He, Huan Zhao, Quanming Yao</li>
<li>for: 提高自动化图学 Task 的效率和智能化水平</li>
<li>methods: 使用自动语言模型（LLM）作为自动化图学代理，将复杂的图学任务分解为三个组成部分：检测学习目标、配置 AutoGraph 解决方案、并生成回应</li>
<li>results: 提出 Auto$^2$Graph 方法，可以自动生成基于具体数据和学习目标的解决方案，并在不同的数据集和学习任务上达到相当的性能水平，同时 agents 的决策具有人类智能特点<details>
<summary>Abstract</summary>
Graph structured data are widely existed and applied in the real-world applications, while it is a challenge to handling these diverse data and learning tasks on graph in an efficient manner. When facing the complicated graph learning tasks, experts have designed diverse Graph Neural Networks (GNNs) in recent years. They have also implemented AutoML in Graph, also known as AutoGraph, to automatically generate data-specific solutions. Despite their success, they encounter limitations in (1) managing diverse learning tasks at various levels, (2) dealing with different procedures in graph learning beyond architecture design, and (3) the huge requirements on the prior knowledge when using AutoGraph. In this paper, we propose to use Large Language Models (LLMs) as autonomous agents to simplify the learning process on diverse real-world graphs. Specifically, in response to a user request which may contain varying data and learning targets at the node, edge, or graph levels, the complex graph learning task is decomposed into three components following the agent planning, namely, detecting the learning intent, configuring solutions based on AutoGraph, and generating a response. The AutoGraph agents manage crucial procedures in automated graph learning, including data-processing, AutoML configuration, searching architectures, and hyper-parameter fine-tuning. With these agents, those components are processed by decomposing and completing step by step, thereby generating a solution for the given data automatically, regardless of the learning task on node or graph. The proposed method is dubbed Auto$^2$Graph, and the comparable performance on different datasets and learning tasks. Its effectiveness is demonstrated by its comparable performance on different datasets and learning tasks, as well as the human-like decisions made by the agents.
</details>
<details>
<summary>摘要</summary>
几乎所有实际应用中都存在Graph structured data，但是处理这些多样化的数据和学习任务在效率上是一大挑战。过去几年，专家们已经设计了多种Graph Neural Networks（GNNs）来解决这些复杂的学习任务。另外，他们还实现了AutoML in Graph，也就是AutoGraph，以自动生成数据特定的解决方案。尽管它们的成功，但它们仍然面临以下问题：（1）在不同的学习任务水平上管理多元的学习任务，（2）在传统的Graph learning beyond architecture design中进行不同的处理，以及（3）在使用AutoGraph时需要很大的专家知识。在这篇论文中，我们提议使用Large Language Models（LLMs）作为自主代理人来简化数据多样化的学习过程。具体来说，当用户发出请求，该请求可能包含不同的数据和学习目标，我们将复杂的Graph learning任务分解为三个Component，即检测学习目的、基于AutoGraph配置解决方案，以及产生回应。AutoGraph代理人处理关键的自动化Graph learning步骤，包括数据处理、AutoML配置、搜索架构和几何 Parameters fine-tuning。这些代理人通过拆分和完成步骤，将这些Component处理一推一击，从而自动生成对应数据的解决方案，不论学习任务是几何或数据水平。我们称这种方法为Auto$^2$Graph，并证明其效果通过与不同数据和学习任务相比较的比较性表现。
</details></li>
</ul>
<hr>
<h2 id="When-Less-is-More-Investigating-Data-Pruning-for-Pretraining-LLMs-at-Scale"><a href="#When-Less-is-More-Investigating-Data-Pruning-for-Pretraining-LLMs-at-Scale" class="headerlink" title="When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale"></a>When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04564">http://arxiv.org/abs/2309.04564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, Sara Hooker</li>
<li>for: 这个论文目的是提出一种可扩展的数据质量估计方法，用于系统地评估预训练数据的质量。</li>
<li>methods: 这个论文使用了一些计算机科学中的复杂估计方法，如Error L2-Norm和memorization，以评估预训练数据的质量。</li>
<li>results: 研究发现，使用简单的plexity估计方法可以更好地评估预训练数据的质量，并且可以提高模型的性能。此外，研究还发现，只使用30%的原始训练数据可以达到与全量训练数据相同的性能。<details>
<summary>Abstract</summary>
Large volumes of text data have contributed significantly to the development of large language models (LLMs) in recent years. This data is typically acquired by scraping the internet, leading to pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In this work, we take a wider view and explore scalable estimates of data quality that can be used to systematically measure the quality of pretraining data. We perform a rigorous comparison at scale of the simple data quality estimator of perplexity, as well as more sophisticated and computationally intensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and prune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets. Surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring methods. We improve over our no-pruning baseline while training on as little as 30% of the original training dataset. Our work sets the foundation for unexplored strategies in automatically curating high quality corpora and suggests the majority of pretraining data can be removed while retaining performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Interpretable-Solar-Flare-Prediction-with-Attention-based-Deep-Neural-Networks"><a href="#Towards-Interpretable-Solar-Flare-Prediction-with-Attention-based-Deep-Neural-Networks" class="headerlink" title="Towards Interpretable Solar Flare Prediction with Attention-based Deep Neural Networks"></a>Towards Interpretable Solar Flare Prediction with Attention-based Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04558">http://arxiv.org/abs/2309.04558</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/gsudmlab/fulldiskattention">https://bitbucket.org/gsudmlab/fulldiskattention</a></li>
<li>paper_authors: Chetraj Pandey, Anli Ji, Rafal A. Angryk, Berkay Aydin</li>
<li>for: 这个研究的目的是提出一种基于注意力的深度学习模型，用于预测下一天内出现M1.0级或更高级的太阳风暴。</li>
<li>methods: 这个模型使用了数据增强折射，并使用了true skill statistic（TSS）和Heidke skill score（HSS）来评估模型的性能。</li>
<li>results: 研究发现，使用注意力机制可以帮助模型更好地从全天照片中提取有关活跃区域的信息，并且模型可以预测近日至远日的太阳风暴。<details>
<summary>Abstract</summary>
Solar flare prediction is a central problem in space weather forecasting and recent developments in machine learning and deep learning accelerated the adoption of complex models for data-driven solar flare forecasting. In this work, we developed an attention-based deep learning model as an improvement over the standard convolutional neural network (CNN) pipeline to perform full-disk binary flare predictions for the occurrence of $\geq$M1.0-class flares within the next 24 hours. For this task, we collected compressed images created from full-disk line-of-sight (LoS) magnetograms. We used data-augmented oversampling to address the class imbalance issue and used true skill statistic (TSS) and Heidke skill score (HSS) as the evaluation metrics. Furthermore, we interpreted our model by overlaying attention maps on input magnetograms and visualized the important regions focused on by the model that led to the eventual decision. The significant findings of this study are: (i) We successfully implemented an attention-based full-disk flare predictor ready for operational forecasting where the candidate model achieves an average TSS=0.54$\pm$0.03 and HSS=0.37$\pm$0.07. (ii) we demonstrated that our full-disk model can learn conspicuous features corresponding to active regions from full-disk magnetogram images, and (iii) our experimental evaluation suggests that our model can predict near-limb flares with adept skill and the predictions are based on relevant active regions (ARs) or AR characteristics from full-disk magnetograms.
</details>
<details>
<summary>摘要</summary>
太阳风暴预测是 espacio weather forecasting 中的中心问题，而最近的机器学习和深度学习技术的发展使得复杂的模型在数据驱动太阳风暴预测中得到了广泛的应用。在这项工作中，我们开发了一种注意力基于的深度学习模型，用于在下一个24小时内预测全盘二分类太阳风暴（M1.0级以上）的发生。为此，我们收集了全盘线性视图（LoS）磁场agram的压缩图像。我们使用数据增强抽样来解决类别不均匀问题，并使用真正技能统计（TSS）和海德ке技能分数（HSS）作为评估指标。此外，我们还进行了模型解释，将注意力地图涂抹到输入磁场agram上，并视频化了模型决策过程中关键的区域。研究的主要发现包括：（i）我们成功地实现了全盘注意力基于的太阳风暴预测模型，该模型在下一个24小时内的平均TSS为0.54±0.03，HSS为0.37±0.07。（ii）我们表明了全盘模型可以从全盘磁场agram图像中学习明显的活跃区域特征，（iii）我们的实验评估表明，我们的模型可以预测近地风暴，并且预测基于全盘磁场agram中的相关活跃区域（ARs）或AR特征。
</details></li>
</ul>
<hr>
<h2 id="Regret-Optimal-Federated-Transfer-Learning-for-Kernel-Regression-with-Applications-in-American-Option-Pricing"><a href="#Regret-Optimal-Federated-Transfer-Learning-for-Kernel-Regression-with-Applications-in-American-Option-Pricing" class="headerlink" title="Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing"></a>Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04557">http://arxiv.org/abs/2309.04557</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/floriankrach/regretoptimalfederatedtransferlearning">https://github.com/floriankrach/regretoptimalfederatedtransferlearning</a></li>
<li>paper_authors: Xuwei Yang, Anastasis Kratsios, Florian Krach, Matheus Grasselli, Aurelien Lucchi</li>
<li>For: The paper proposes an optimal iterative scheme for federated transfer learning, aiming to minimize the cumulative deviation of the generated parameters from the specialized parameters across all iterations, while respecting the loss function for the model produced by the algorithm upon halting.* Methods: The proposed algorithm allows for continual communication between each specialized model and the central planner at each iteration, and derives explicit updates for the regret-optimal algorithm. Additionally, the paper develops a nearly regret-optimal heuristic that runs with fewer elementary operations.* Results: The paper shows that the regret-optimal algorithm has a regret bound of $\mathcal{O}(\varepsilon q \bar{N}^{1&#x2F;2})$, where $\bar{N}$ is the aggregate number of training pairs, and an adversary which perturbs $q$ training pairs by at-most $\varepsilon&gt;0$ cannot reduce the algorithm’s regret by more than this bound. The paper also conducts numerical experiments in the context of American option pricing to validate the theoretical findings.<details>
<summary>Abstract</summary>
We propose an optimal iterative scheme for federated transfer learning, where a central planner has access to datasets ${\cal D}_1,\dots,{\cal D}_N$ for the same learning model $f_{\theta}$. Our objective is to minimize the cumulative deviation of the generated parameters $\{\theta_i(t)\}_{t=0}^T$ across all $T$ iterations from the specialized parameters $\theta^\star_{1},\ldots,\theta^\star_N$ obtained for each dataset, while respecting the loss function for the model $f_{\theta(T)}$ produced by the algorithm upon halting. We only allow for continual communication between each of the specialized models (nodes/agents) and the central planner (server), at each iteration (round). For the case where the model $f_{\theta}$ is a finite-rank kernel regression, we derive explicit updates for the regret-optimal algorithm. By leveraging symmetries within the regret-optimal algorithm, we further develop a nearly regret-optimal heuristic that runs with $\mathcal{O}(Np^2)$ fewer elementary operations, where $p$ is the dimension of the parameter space. Additionally, we investigate the adversarial robustness of the regret-optimal algorithm showing that an adversary which perturbs $q$ training pairs by at-most $\varepsilon>0$, across all training sets, cannot reduce the regret-optimal algorithm's regret by more than $\mathcal{O}(\varepsilon q \bar{N}^{1/2})$, where $\bar{N}$ is the aggregate number of training pairs. To validate our theoretical findings, we conduct numerical experiments in the context of American option pricing, utilizing a randomly generated finite-rank kernel.
</details>
<details>
<summary>摘要</summary>
我们提出一种优化的迭代方案 для联邦学习传输，其中中央规划者有Access to datasets $\mathcal{D}_1, \ldots, \mathcal{D}_N$ for the same learning model $f_{\theta}$. Our objective is to minimize the cumulative deviation of the generated parameters $\{\theta_i(t)\}_{t=0}^T$ across all $T$ iterations from the specialized parameters $\theta^\star_{1}, \ldots, \theta^\star_N$ obtained for each dataset, while respecting the loss function for the model $f_{\theta(T)}$ produced by the algorithm upon halting. We only allow for continual communication between each of the specialized models (nodes/agents) and the central planner (server), at each iteration (round). For the case where the model $f_{\theta}$ is a finite-rank kernel regression, we derive explicit updates for the regret-optimal algorithm. By leveraging symmetries within the regret-optimal algorithm, we further develop a nearly regret-optimal heuristic that runs with $\mathcal{O}(Np^2)$ fewer elementary operations, where $p$ is the dimension of the parameter space. Additionally, we investigate the adversarial robustness of the regret-optimal algorithm showing that an adversary which perturbs $q$ training pairs by at-most $\varepsilon>0$, across all training sets, cannot reduce the regret-optimal algorithm's regret by more than $\mathcal{O}(\varepsilon q \bar{N}^{1/2})$, where $\bar{N}$ is the aggregate number of training pairs. To validate our theoretical findings, we conduct numerical experiments in the context of American option pricing, utilizing a randomly generated finite-rank kernel.Note: Simplified Chinese is a written form of Chinese that uses simpler grammar and vocabulary than Traditional Chinese. It is commonly used in mainland China and other countries where Simplified Chinese is the official writing system.
</details></li>
</ul>
<hr>
<h2 id="Connecting-NTK-and-NNGP-A-Unified-Theoretical-Framework-for-Neural-Network-Learning-Dynamics-in-the-Kernel-Regime"><a href="#Connecting-NTK-and-NNGP-A-Unified-Theoretical-Framework-for-Neural-Network-Learning-Dynamics-in-the-Kernel-Regime" class="headerlink" title="Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime"></a>Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04522">http://arxiv.org/abs/2309.04522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yehonatan Avidan, Qianyi Li, Haim Sompolinsky</li>
<li>for: 本文旨在提供深度神经网络学习过程的完整理论框架，尤其是在无穷宽网络 regime 中。</li>
<li>methods: 本文使用 Markov  proximal 学习模型，derive 了一个 exact 的分析表达式，描述了网络输入-输出函数在学习过程中和学习完成后的行为。</li>
<li>results: 本文发现了两个不同时间尺度的学习阶段：梯度驱动学习阶段和扩散学习阶段。在梯度驱动学习阶段，动力是由 deterministic 梯度下降控制的，可以使用 NTK 理论来描述。而在扩散学习阶段，网络参数 sampling 了解决空间，最终 approached 到 NNGP 理论中的平衡分布。这种研究帮助 closure 了 NTK 和 NNGP 两种理论之间的知识渊盈，为深度神经网络在无穷宽网络 regime 中的学习过程提供了一个完整的理论框架。<details>
<summary>Abstract</summary>
Artificial neural networks have revolutionized machine learning in recent years, but a complete theoretical framework for their learning process is still lacking. Substantial progress has been made for infinitely wide networks. In this regime, two disparate theoretical frameworks have been used, in which the network's output is described using kernels: one framework is based on the Neural Tangent Kernel (NTK) which assumes linearized gradient descent dynamics, while the Neural Network Gaussian Process (NNGP) kernel assumes a Bayesian framework. However, the relation between these two frameworks has remained elusive. This work unifies these two distinct theories using a Markov proximal learning model for learning dynamics in an ensemble of randomly initialized infinitely wide deep networks. We derive an exact analytical expression for the network input-output function during and after learning, and introduce a new time-dependent Neural Dynamical Kernel (NDK) from which both NTK and NNGP kernels can be derived. We identify two learning phases characterized by different time scales: gradient-driven and diffusive learning. In the initial gradient-driven learning phase, the dynamics is dominated by deterministic gradient descent, and is described by the NTK theory. This phase is followed by the diffusive learning stage, during which the network parameters sample the solution space, ultimately approaching the equilibrium distribution corresponding to NNGP. Combined with numerical evaluations on synthetic and benchmark datasets, we provide novel insights into the different roles of initialization, regularization, and network depth, as well as phenomena such as early stopping and representational drift. This work closes the gap between the NTK and NNGP theories, providing a comprehensive framework for understanding the learning process of deep neural networks in the infinite width limit.
</details>
<details>
<summary>摘要</summary>
人工神经网络在最近几年内革命化机器学习，但完整的理论框架仍然缺失。在无穷宽网络下，有两种不同的理论框架被用来描述网络输出，它们分别是基于神经汇抽象（NTK）和神经网络泊利过程（NNGP）框架。然而，这两种框架之间的关系仍然是一个谜。本文将这两种不同的理论联系起来，使用一种Markov靠近学习模型来描述学习过程中的动态。我们得到了一个精确的分析表达，描述了网络输入输出函数在学习和学习后的行为。此外，我们还引入了一种时间依赖的神经动力学kernel（NDK），它可以从NTK和NNGP框架中 derivation。我们在不同时间尺度上分类了学习阶段，包括梯度驱动的学习阶段和协同学习阶段。在梯度驱动学习阶段，动态是由deterministic梯度下降控制的，这与NTK理论相吻合。这个阶段后接着的协同学习阶段，网络参数在解决空间中走讲，最终 approaching NNGP的平衡分布。通过对synthetic和benchmark数据集进行数值评估，我们提供了新的视角，描述了不同的初始化、正则化和网络深度如何影响学习过程。这个工作 closure NTK和NNGP理论之间的差异，提供了深度学习过程的全面框架。
</details></li>
</ul>
<hr>
<h2 id="On-the-Actionability-of-Outcome-Prediction"><a href="#On-the-Actionability-of-Outcome-Prediction" class="headerlink" title="On the Actionability of Outcome Prediction"></a>On the Actionability of Outcome Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04470">http://arxiv.org/abs/2309.04470</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andrewmogbolu2/blockchain-technology">https://github.com/andrewmogbolu2/blockchain-technology</a></li>
<li>paper_authors: Lydia T. Liu, Solon Barocas, Jon Kleinberg, Karen Levy</li>
<li>for: 这 paper 的目的是探讨在社会影响领域中预测未来结果的应用，以及预测结果后的有效行动方法。</li>
<li>methods: 这 paper 使用了一个简单的模型，包括行动、潜在状态和测量，来研究在具有多种可能的行动时，纯粹的结果预测是否有助于选择最有效的行动。</li>
<li>results: 研究发现，纯粹的结果预测并不总是最有效的政策，即使与其他测量结合使用。 在大多数情况下，测量行动可能的状态会提高行动价值，并且这种提高的程度取决于行动成本和结果模型。 这种分析表明，在实施 Setting 中，需要超越基本的结果预测，并考虑可能的行动和潜在状态的知识。<details>
<summary>Abstract</summary>
Predicting future outcomes is a prevalent application of machine learning in social impact domains. Examples range from predicting student success in education to predicting disease risk in healthcare. Practitioners recognize that the ultimate goal is not just to predict but to act effectively. Increasing evidence suggests that relying on outcome predictions for downstream interventions may not have desired results.   In most domains there exists a multitude of possible interventions for each individual, making the challenge of taking effective action more acute. Even when causal mechanisms connecting the individual's latent states to outcomes is well understood, in any given instance (a specific student or patient), practitioners still need to infer -- from budgeted measurements of latent states -- which of many possible interventions will be most effective for this individual. With this in mind, we ask: when are accurate predictors of outcomes helpful for identifying the most suitable intervention?   Through a simple model encompassing actions, latent states, and measurements, we demonstrate that pure outcome prediction rarely results in the most effective policy for taking actions, even when combined with other measurements. We find that except in cases where there is a single decisive action for improving the outcome, outcome prediction never maximizes "action value", the utility of taking actions. Making measurements of actionable latent states, where specific actions lead to desired outcomes, considerably enhances the action value compared to outcome prediction, and the degree of improvement depends on action costs and the outcome model. This analysis emphasizes the need to go beyond generic outcome prediction in interventional settings by incorporating knowledge of plausible actions and latent states.
</details>
<details>
<summary>摘要</summary>
预测未来结果是社会影响领域中广泛应用的机器学习应用之一。例如，预测教育中学生成功的可能性，预测医疗领域中疾病的发展。实践者认为，最终目标不仅是预测，还是效果iveness。然而，有增加证据表明，基于结果预测的下游 intervención可能并不会达到所期望的效果。在大多数领域中，每个个体都有多种可能的 intervención，使得选择最有效的 intervención变得更加困难。即使理解个体的 latent state 与结果之间的 causal 机制，在特定学生或患者（specific student or patient）中，实践者仍需从预算中的 latent state 测量中推断哪一种可能的 intervención会对这个个体最有效。在这种情况下，我们问：精准的结果预测对于选择最适合的 intervención是有帮助的吗？通过一个简单的模型，包括行动、latent state 和测量，我们证明了纯粹的结果预测 rarely leads to the most effective policy for taking actions，即使与其他测量结合使用。我们发现，除非有单一的决定性的 action 可以提高结果，否则，结果预测不能够最大化“action value”，即行动的用处。通过测量行动可能导致的 desirable outcomes 的 latent state，可以显著提高 action value，并且该提高的程度取决于行动成本和结果模型。这种分析强调了在 intervenational 设置中要超越通用的结果预测，并包括 latent state 和可能的 action 的知识。
</details></li>
</ul>
<hr>
<h2 id="Measuring-and-Improving-Chain-of-Thought-Reasoning-in-Vision-Language-Models"><a href="#Measuring-and-Improving-Chain-of-Thought-Reasoning-in-Vision-Language-Models" class="headerlink" title="Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models"></a>Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04461">http://arxiv.org/abs/2309.04461</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangyi-chen/cotconsistency">https://github.com/yangyi-chen/cotconsistency</a></li>
<li>paper_authors: Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran<br>for:* 这个论文旨在探讨视力语言模型（VLM）是否具备人类水平的视觉理解能力。methods:* 作者提出了一种链条（CoT）基于的一致度测试方法，以评估VLM的理解能力。* 作者还提出了一种人工智能在循环（LLM-Human-in-the-Loop）管道，以降低测试成本。results:* 作者发现，即使使用最佳模型，VLM也无法如人类般系统地进行视觉理解。* 作者提出了一种两Stage训练框架，以提高VLM的理解性能和一致度。Here is the result in Simplified Chinese text:for:* 这个论文旨在探讨视力语言模型（VLM）是否具备人类水平的视觉理解能力。methods:* 作者提出了一种链条（CoT）基于的一致度测试方法，以评估VLM的理解能力。* 作者还提出了一种人工智能在循环（LLM-Human-in-the-Loop）管道，以降低测试成本。results:* 作者发现，即使使用最佳模型，VLM也无法如人类般系统地进行视觉理解。* 作者提出了一种两Stage训练框架，以提高VLM的理解性能和一致度。<details>
<summary>Abstract</summary>
Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing a LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.
</details>
<details>
<summary>摘要</summary>
vision-language模型（VLM）最近已经展现出强大的能力，可以作为视觉助手，理解自然的视觉问题并生成人类如样的输出。在这项工作中，我们探索VLM的能力是否可以与人类一样进行视觉理解和逻辑推理。为了解决VLM的逻辑能力是否充分一致和基础的问题，我们还提出了链条（CoT）基于的一致度测试。然而，这种评估需要一个包括高级推理和细节逻辑链的启发式测试，这很costly。我们解决这个挑战，提出了一个LLM-人类 loop（LLM-Human-in-the-Loop）管道，可以大幅降低成本，同时保证生成高质量的数据集。基于这个管道和现有的粗略标注数据集，我们建立了CURE标准测试 benchmark，用于测试VLM的零基础理解和一致性。我们评估了现有的状态平台VLM，发现even最佳performing模型无法充分表现出视觉逻辑能力和一致性，这表明VLM需要进一步的努力，以使其能够系统地和一致地进行视觉逻辑。为此，我们提出了一个两 stage 训练框架，以提高VLM的逻辑性和一致性。第一个阶段是通过精心练习VLM，使其通过LLM自动生成的步骤 reasoning samples进行supervised fine-tuning。第二个阶段是通过 incorporating LLMs 的反馈，生成高一致性和基础的逻辑链。我们经验表明，我们的框架可以大幅提高VLM的逻辑性和一致性。
</details></li>
</ul>
<hr>
<h2 id="tSPM-a-high-performance-algorithm-for-mining-transitive-sequential-patterns-from-clinical-data"><a href="#tSPM-a-high-performance-algorithm-for-mining-transitive-sequential-patterns-from-clinical-data" class="headerlink" title="tSPM+; a high-performance algorithm for mining transitive sequential patterns from clinical data"></a>tSPM+; a high-performance algorithm for mining transitive sequential patterns from clinical data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05671">http://arxiv.org/abs/2309.05671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Hügel, Ulrich Sax, Shawn N. Murphy, Hossein Estiri</li>
<li>for: 这篇论文旨在探讨如何使用计算机技术来描述复杂的疾病，特别是通过时间序列 Pattern mining 和机器学习工作流程。</li>
<li>methods: 这篇论文使用的方法是时间序列 Pattern mining 算法，并在其中添加了时间长度的维度，以提高算法的性能。</li>
<li>results: 研究人员发现，使用 tSPM+ 算法可以提高计算速度，并且可以降低内存占用量，相比之下，传统的 tSPM 算法可以提高计算速度，但是内存占用量较高。此外，研究人员还提供了一个 Docker 容器和一个 R 包，以便轻松地 инте integrate 到现有的机器学习工作流程中。<details>
<summary>Abstract</summary>
The increasing availability of large clinical datasets collected from patients can enable new avenues for computational characterization of complex diseases using different analytic algorithms. One of the promising new methods for extracting knowledge from large clinical datasets involves temporal pattern mining integrated with machine learning workflows. However, mining these temporal patterns is a computational intensive task and has memory repercussions. Current algorithms, such as the temporal sequence pattern mining (tSPM) algorithm, are already providing promising outcomes, but still leave room for optimization. In this paper, we present the tSPM+ algorithm, a high-performance implementation of the tSPM algorithm, which adds a new dimension by adding the duration to the temporal patterns. We show that the tSPM+ algorithm provides a speed up to factor 980 and a up to 48 fold improvement in memory consumption. Moreover, we present a docker container with an R-package, We also provide vignettes for an easy integration into already existing machine learning workflows and use the mined temporal sequences to identify Post COVID-19 patients and their symptoms according to the WHO definition.
</details>
<details>
<summary>摘要</summary>
随着大规模临床数据集的更加可用，可以开拓新的 computationally characterization of complex diseases 的avenues。一种promising new methods for extracting knowledge from large clinical datasets involves temporal pattern mining integrated with machine learning workflows。However, mining these temporal patterns is a computationally intensive task and has memory repercussions。Current algorithms, such as the temporal sequence pattern mining (tSPM) algorithm，are already providing promising outcomes，but still leave room for optimization。在这篇论文中，我们提出了 tSPM+ 算法，一种高性能实现 tSPM 算法，它添加了时间长度到 temporalsequences。我们表明，tSPM+ 算法可以提高速度到 factor 980，并提高内存使用量。此外，我们还提供了一个 Docker 容器和一个 R 包，并提供了一些可以方便地integrate into existing machine learning workflows的 vignettes。此外，我们使用分析的 temporalsequences来识别患 COVID-19 病人和其症状，根据 WHO 定义。
</details></li>
</ul>
<hr>
<h2 id="Subwords-as-Skills-Tokenization-for-Sparse-Reward-Reinforcement-Learning"><a href="#Subwords-as-Skills-Tokenization-for-Sparse-Reward-Reinforcement-Learning" class="headerlink" title="Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning"></a>Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04459">http://arxiv.org/abs/2309.04459</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Yunis, Justin Jung, Falcon Dai, Matthew Walter</li>
<li>for: 提高鲁棒性学习在缺乏奖励的环境中的能力，尤其是在连续动作空间中。</li>
<li>methods: 使用分 clustering 将动作空间细分，并使用自然语言处理中的tokenization技术生成 temporally extended actions。</li>
<li>results: 相比基eline，该方法在一些具有挑战性的缺乏奖励环境中表现出色，并需要下降多个计算量级别的 computation。<details>
<summary>Abstract</summary>
Exploration in sparse-reward reinforcement learning is difficult due to the requirement of long, coordinated sequences of actions in order to achieve any reward. Moreover, in continuous action spaces there are an infinite number of possible actions, which only increases the difficulty of exploration. One class of methods designed to address these issues forms temporally extended actions, often called skills, from interaction data collected in the same domain, and optimizes a policy on top of this new action space. Typically such methods require a lengthy pretraining phase, especially in continuous action spaces, in order to form the skills before reinforcement learning can begin. Given prior evidence that the full range of the continuous action space is not required in such tasks, we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language processing to generate temporally extended actions. Such a method outperforms baselines for skill-generation in several challenging sparse-reward domains, and requires orders-of-magnitude less computation in skill-generation and online rollouts.
</details>
<details>
<summary>摘要</summary>
We propose a novel approach to skill-generation with two components. First, we discretize the action space through clustering, and second, we leverage a tokenization technique from natural language processing to generate temporally extended actions. Our approach outperforms baselines for skill-generation in several challenging sparse-reward domains and requires orders-of-magnitude less computation in skill-generation and online rollouts.Here is the text in Simplified Chinese:探索 sparse-reward 学习中的困难在于需要长 sequences of actions 才能获得任何奖励。此外，连续动作空间中有无限多个可能的动作，这只会使探索更加困难。为了解决这些问题，一种方法是通过从同一个领域中收集的交互数据来形成 temporally extended actions，或者技能，并且优化一个策略。然而，这通常需要一个较长的预训练阶段，尤其是在连续动作空间中。我们提出了一种新的方法，它包括两个组成部分。首先，我们使用 clustering 将动作空间细分，然后使用自然语言处理中的tokenization技术来生成 temporally extended actions。我们的方法在一些复杂的 sparse-reward 领域中表现出色，并且需要下注意力量的 computation 在技能生成和在线执行中。
</details></li>
</ul>
<hr>
<h2 id="Postprocessing-of-Ensemble-Weather-Forecasts-Using-Permutation-invariant-Neural-Networks"><a href="#Postprocessing-of-Ensemble-Weather-Forecasts-Using-Permutation-invariant-Neural-Networks" class="headerlink" title="Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks"></a>Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04452">http://arxiv.org/abs/2309.04452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khoehlein/Permutation-invariant-Postprocessing">https://github.com/khoehlein/Permutation-invariant-Postprocessing</a></li>
<li>paper_authors: Kevin Höhlein, Benedikt Schulz, Rüdiger Westermann, Sebastian Lerch</li>
<li>for: 这项研究旨在探讨使用 permutation-invariant neural network 进行 numerical weather forecasts 的 statistically postprocessing，以生成可靠的 probabilistic forecast distribution。</li>
<li>methods: 研究使用 permutation-invariant neural network，该网络将 ensemble forecast 视为一组无序成员预测，并学习链函数，以保证链函数对 permutation 的敏感性。</li>
<li>results: 对于 surface temperature 和 wind gust forecasts 的 posteprocessing，研究达到了 state-of-the-art 的预测质量。进一步的 permutation-based importance analysis 表明， ensemble forecast 中大多数重要信息都集中在几个内部度量上，这可能对 future ensemble forecasting 和 postprocessing 系统的设计产生影响。<details>
<summary>Abstract</summary>
Statistical postprocessing is used to translate ensembles of raw numerical weather forecasts into reliable probabilistic forecast distributions. In this study, we examine the use of permutation-invariant neural networks for this task. In contrast to previous approaches, which often operate on ensemble summary statistics and dismiss details of the ensemble distribution, we propose networks which treat forecast ensembles as a set of unordered member forecasts and learn link functions that are by design invariant to permutations of the member ordering. We evaluate the quality of the obtained forecast distributions in terms of calibration and sharpness, and compare the models against classical and neural network-based benchmark methods. In case studies addressing the postprocessing of surface temperature and wind gust forecasts, we demonstrate state-of-the-art prediction quality. To deepen the understanding of the learned inference process, we further propose a permutation-based importance analysis for ensemble-valued predictors, which highlights specific aspects of the ensemble forecast that are considered important by the trained postprocessing models. Our results suggest that most of the relevant information is contained in few ensemble-internal degrees of freedom, which may impact the design of future ensemble forecasting and postprocessing systems.
</details>
<details>
<summary>摘要</summary>
统计处理是用于将原始的数值天气预报转换成可靠的 probabilistic 预报分布。在这种研究中，我们考虑使用卷积神经网络来实现这项任务。与之前的方法不同，这些方法通常操作于ensemble summary statistics，并丢弃预报分布的细节信息。我们提议的网络将预报集体看作一组无序成员预报，并学习链函数，这些链函数是设计具有排序无关性的。我们根据预报分布的准确性和锐度进行评价，并与经典和神经网络基础模型进行比较。在地面温度和风 Gust 预报的 случа例研究中，我们达到了当前预报质量的最佳标准。为深入了解所学的推理过程，我们还提出了 permutation-based importance 分析，该分析显示了预报集体中重要的特征，这些特征可能影响未来的ensemble forecasting和postprocessing系统的设计。我们的结果表明，大多数有关信息都包含在预报集体中的几个内部度量上，这可能对未来的预报系统设计产生影响。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Speech-Recognition-and-Disfluency-Removal-with-Acoustic-Language-Model-Pretraining"><a href="#End-to-End-Speech-Recognition-and-Disfluency-Removal-with-Acoustic-Language-Model-Pretraining" class="headerlink" title="End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining"></a>End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04516">http://arxiv.org/abs/2309.04516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidsroth/hubert-disfl">https://github.com/davidsroth/hubert-disfl</a></li>
<li>paper_authors: Saksham Bassi, Giulio Duregon, Siddhartha Jalagam, David Roth</li>
<li>for: 本研究旨在比较两种模型在不连续和对话性听说中的表现，并研究大规模自监学习对audio表示的影响。</li>
<li>methods: 本研究使用了两种模型：两个阶段模型和末端到终端模型。两个阶段模型使用了预训练的语言模型来进行分词和纠错，而末端到终端模型则直接使用大规模自监学习对audio表示进行学习。</li>
<li>results: 研究发现，使用大规模自监学习对audio表示可以帮助末端到终端模型在不连续和对话性听说中表现更好，并且选择适当的预训练目标可以有效地改善模型的适应能力。<details>
<summary>Abstract</summary>
The SOTA in transcription of disfluent and conversational speech has in recent years favored two-stage models, with separate transcription and cleaning stages. We believe that previous attempts at end-to-end disfluency removal have fallen short because of the representational advantage that large-scale language model pretraining has given to lexical models. Until recently, the high dimensionality and limited availability of large audio datasets inhibited the development of large-scale self-supervised pretraining objectives for learning effective audio representations, giving a relative advantage to the two-stage approach, which utilises pretrained representations for lexical tokens. In light of recent successes in large scale audio pretraining, we revisit the performance comparison between two-stage and end-to-end model and find that audio based language models pretrained using weak self-supervised objectives match or exceed the performance of similarly trained two-stage models, and further, that the choice of pretraining objective substantially effects a model's ability to be adapted to the disfluency removal task.
</details>
<details>
<summary>摘要</summary>
现代最佳做法（SOTA）在不流畅和对话speech的转写中是分两个阶段的模型，分别是转写和清洁阶段。我们认为过去对不流畅speech中的缺失的尝试都受到了大规模语言模型预训练的 represencing优势的影响，导致了lexical模型的表达优势。在过去，大型音频数据集的高维度和有限的可用性妨碍了大规模自动化预训练目标的发展，从而给予了两阶段方法偏好。在现在的大规模音频预训练成功之后，我们重新评估了两阶段和终端模型的性能比较，发现 audio基于语言模型预训练使用弱自动化目标可以匹配或超越相同训练的两阶段模型，而且选择的预训练目标对模型的适应性具有很大的影响。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Neural-Networks-for-an-optimal-counterdiabatic-quantum-computation"><a href="#Physics-Informed-Neural-Networks-for-an-optimal-counterdiabatic-quantum-computation" class="headerlink" title="Physics-Informed Neural Networks for an optimal counterdiabatic quantum computation"></a>Physics-Informed Neural Networks for an optimal counterdiabatic quantum computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04434">http://arxiv.org/abs/2309.04434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Ferrer-Sánchez, Carlos Flores-Garrigos, Carlos Hernani-Morales, José J. Orquín-Marqués, Narendra N. Hegade, Alejandro Gomez Cadavid, Iraitz Montalban, Enrique Solano, Yolanda Vives-Gilabert, José D. Martín-Guerrero</li>
<li>for:  Addressing the counterdiabatic (CD) protocol in the optimization of quantum circuits comprised of systems with $N_{Q}$ qubits.</li>
<li>methods:  Utilizes Physics-Informed Neural Networks (PINNs) to leverage the strength of deep learning techniques and accurately solve the time evolution of physical observables within the quantum system, while imposing hermiticity conditions and using the principle of least action.</li>
<li>results:  Derives a desirable decomposition for the non-adiabatic terms through a linear combination utilizing Pauli operators, demonstrating significant advantages for practical implementation within quantum computing algorithms.Here is the same information in Simplified Chinese:</li>
<li>for:  Addressing the counterdiabatic (CD) protocol in the optimization of quantum circuits comprised of systems with $N_{Q}$ qubits.</li>
<li>methods:  Utilizes Physics-Informed Neural Networks (PINNs) to leverage the strength of deep learning techniques and accurately solve the time evolution of physical observables within the quantum system, while imposing hermiticity conditions and using the principle of least action.</li>
<li>results:  Derives a desirable decomposition for the non-adiabatic terms through a linear combination utilizing Pauli operators, demonstrating significant advantages for practical implementation within quantum computing algorithms.<details>
<summary>Abstract</summary>
We introduce a novel methodology that leverages the strength of Physics-Informed Neural Networks (PINNs) to address the counterdiabatic (CD) protocol in the optimization of quantum circuits comprised of systems with $N_{Q}$ qubits. The primary objective is to utilize physics-inspired deep learning techniques to accurately solve the time evolution of the different physical observables within the quantum system. To accomplish this objective, we embed the necessary physical information into an underlying neural network to effectively tackle the problem. In particular, we impose the hermiticity condition on all physical observables and make use of the principle of least action, guaranteeing the acquisition of the most appropriate counterdiabatic terms based on the underlying physics. The proposed approach offers a dependable alternative to address the CD driving problem, free from the constraints typically encountered in previous methodologies relying on classical numerical approximations. Our method provides a general framework to obtain optimal results from the physical observables relevant to the problem, including the external parameterization in time known as scheduling function, the gauge potential or operator involving the non-adiabatic terms, as well as the temporal evolution of the energy levels of the system, among others. The main applications of this methodology have been the $\mathrm{H_{2}$ and $\mathrm{LiH}$ molecules, represented by a 2-qubit and 4-qubit systems employing the STO-3G basis. The presented results demonstrate the successful derivation of a desirable decomposition for the non-adiabatic terms, achieved through a linear combination utilizing Pauli operators. This attribute confers significant advantages to its practical implementation within quantum computing algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出一种新的方法，利用物理学 informed neural networks（PINNs）来解决量子环境中的Counterdiabatic（CD）协议。我们的目标是使用物理灵感的深度学习技术来准确地解决量子系统中不同物理观测器的时间演化。为达到这个目标，我们将物理知识嵌入到一个底下的神经网中，以有效地处理问题。具体来说，我们将 hermiticity 条件套用到所有物理观测器上，并利用最小行动原理，从而获得最佳的counterdiabatic 条件，基于背景物理。我们的方法提供一个可靠的替代方案，免除了先前的方法对于量子环境中的CD驾驭问题的困难。我们的方法可以从物理观测器中获得最佳的结果，包括时间外推数函数、 gauge 潜在或操作含有非对称项的情况、以及量子系统中能阶的时间演化等。我们已经将这种方法应用到 $\mathrm{H_{2}$ 和 $\mathrm{LiH}$ 分子中，使用 STO-3G 基底，并获得了一个愉悦的分解，这是由 Pauli 算子的线性 комбінаition 实现的。这个特点具有实用实现量子计算 алгоритmi中的实际优点。
</details></li>
</ul>
<hr>
<h2 id="Variations-and-Relaxations-of-Normalizing-Flows"><a href="#Variations-and-Relaxations-of-Normalizing-Flows" class="headerlink" title="Variations and Relaxations of Normalizing Flows"></a>Variations and Relaxations of Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04433">http://arxiv.org/abs/2309.04433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keegan Kelly, Lorena Piedras, Sukrit Rao, David Roth</li>
<li>for: 本文描述了一类模型，称为正常化流（NFs），它们可以表示复杂目标分布为一系列简单基础分布的 compose。</li>
<li>methods: NFs 使用 diffeomorphisms 限制候选变换的空间，从而实现高效、准确的采样和总体值评估。但是，这会限制输入、输出和所有间隔空间的维度相同，从而降低了表达复杂 topology 目标分布的能力。</li>
<li>results: 本文介绍了一些最近的工作，通过结合其他生成模型类型，如 VAEs 和 score-based diffusion，使 NFs 可以更好地平衡表达能力、训练速度、采样效率和可评估性。<details>
<summary>Abstract</summary>
Normalizing Flows (NFs) describe a class of models that express a complex target distribution as the composition of a series of bijective transformations over a simpler base distribution. By limiting the space of candidate transformations to diffeomorphisms, NFs enjoy efficient, exact sampling and density evaluation, enabling NFs to flexibly behave as both discriminative and generative models. Their restriction to diffeomorphisms, however, enforces that input, output and all intermediary spaces share the same dimension, limiting their ability to effectively represent target distributions with complex topologies. Additionally, in cases where the prior and target distributions are not homeomorphic, Normalizing Flows can leak mass outside of the support of the target. This survey covers a selection of recent works that combine aspects of other generative model classes, such as VAEs and score-based diffusion, and in doing so loosen the strict bijectivity constraints of NFs to achieve a balance of expressivity, training speed, sample efficiency and likelihood tractability.
</details>
<details>
<summary>摘要</summary>
normalizing flows（NF）描述一类模型，表示复杂目标分布为基于 simpler base distribution 的序列bijective变换的composite。通过限制候选变换空间为 diffeomorphisms，NFs 可以实现高效、准确的样本评估和扩散函数评估，使其可以作为权重分布和生成模型兼用。然而，NFs 的假设是输入、输出和所有中间空间具有同样的维度，这限制了它们对复杂分布的表示能力。此外，在 prior 和 target 分布不同HOMEOMORPHIC 时，NFs 可能会导致样本泄漏出目标分布的支持。这篇评论概述了一些最近的工作，把其他生成模型类型，如 VAEs 和 score-based diffusion 的特点相结合，以逃脱 NFs 的刻意对称性限制，以达到表达能力、训练速度、样本效率和概率可读性的平衡。
</details></li>
</ul>
<hr>
<h2 id="Soft-Quantization-using-Entropic-Regularization"><a href="#Soft-Quantization-using-Entropic-Regularization" class="headerlink" title="Soft Quantization using Entropic Regularization"></a>Soft Quantization using Entropic Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04428">http://arxiv.org/abs/2309.04428</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rajmadan96/softquantization">https://github.com/rajmadan96/softquantization</a></li>
<li>paper_authors: Rajmadan Lakshmanan, Alois Pichler</li>
<li>for: 解决量子化问题中的最佳折衔问题，使用finite, discrete措施来 aproximate probability measures在${\mathbb{R}}^d$中。</li>
<li>methods: 提出了一种基于entropy regularization的量子化问题，使用softmin函数来实现robustness both theoretically and practically。在evaluating the approximation quality中使用 entropy-regularized Wasserstein distance，并使用stochoastic gradient approach来实现最佳解。</li>
<li>results: 提出了一种可调控制参数的方法，以适应具有特殊挑战的问题。经验表明，该方法在不同场景下具有良好的性能。<details>
<summary>Abstract</summary>
The quantization problem aims to find the best possible approximation of probability measures on ${\mathbb{R}^d$ using finite, discrete measures. The Wasserstein distance is a typical choice to measure the quality of the approximation. This contribution investigates the properties and robustness of the entropy-regularized quantization problem, which relaxes the standard quantization problem. The proposed approximation technique naturally adopts the softmin function, which is well known for its robustness in terms of theoretical and practicability standpoints. Moreover, we use the entropy-regularized Wasserstein distance to evaluate the quality of the soft quantization problem's approximation, and we implement a stochastic gradient approach to achieve the optimal solutions. The control parameter in our proposed method allows for the adjustment of the optimization problem's difficulty level, providing significant advantages when dealing with exceptionally challenging problems of interest. As well, this contribution empirically illustrates the performance of the method in various expositions.
</details>
<details>
<summary>摘要</summary>
“量化问题”的目的是找到${\mathbb{R}}^d$上最佳的数值量化方法，使用有限、精确的数值量化方法。“ Wasserstein 距离”是一种常用的衡量该量化方法的质量的度量。本贡献 investigates the properties and robustness of the entropy-regularized quantization problem，这是对标准量化问题的放宽。我们使用了知名的“softmin函数”，它在理论和实践上都具有较好的Robustness。此外，我们使用了 entropy-regulated Wasserstein distance 来衡量该量化问题的数值量化方法的质量，并使用了随机梯度法来实现最佳解。控制参数在我们的提案中允许调整优化问题的困难度，具有对具有特别困难问题的优势。此外，本贡献透过实践探讨，详细显示了方法的表现。
</details></li>
</ul>
<hr>
<h2 id="Robust-Representation-Learning-for-Privacy-Preserving-Machine-Learning-A-Multi-Objective-Autoencoder-Approach"><a href="#Robust-Representation-Learning-for-Privacy-Preserving-Machine-Learning-A-Multi-Objective-Autoencoder-Approach" class="headerlink" title="Robust Representation Learning for Privacy-Preserving Machine Learning: A Multi-Objective Autoencoder Approach"></a>Robust Representation Learning for Privacy-Preserving Machine Learning: A Multi-Objective Autoencoder Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04427">http://arxiv.org/abs/2309.04427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sofiane Ouaari, Ali Burak Ünal, Mete Akgün, Nico Pfeifer</li>
<li>for: 本研究旨在提供一种privacy-preserving machine learning（ppML）技术，以便在各种领域中使用机器学习（ML）而不需要折衣数据隐私。</li>
<li>methods: 本研究使用了深度学习来实现数据编码，并在多目标方面训练自适应神经网络，以优化隐私和实用性之间的质量衡量。</li>
<li>results: 实验结果表明，本方法在单模态和多模态设置中均可以获得改进的性能，并且在某些情况下超过了现有技术的水平。<details>
<summary>Abstract</summary>
Several domains increasingly rely on machine learning in their applications. The resulting heavy dependence on data has led to the emergence of various laws and regulations around data ethics and privacy and growing awareness of the need for privacy-preserving machine learning (ppML). Current ppML techniques utilize methods that are either purely based on cryptography, such as homomorphic encryption, or that introduce noise into the input, such as differential privacy. The main criticism given to those techniques is the fact that they either are too slow or they trade off a model s performance for improved confidentiality. To address this performance reduction, we aim to leverage robust representation learning as a way of encoding our data while optimizing the privacy-utility trade-off. Our method centers on training autoencoders in a multi-objective manner and then concatenating the latent and learned features from the encoding part as the encoded form of our data. Such a deep learning-powered encoding can then safely be sent to a third party for intensive training and hyperparameter tuning. With our proposed framework, we can share our data and use third party tools without being under the threat of revealing its original form. We empirically validate our results on unimodal and multimodal settings, the latter following a vertical splitting system and show improved performance over state-of-the-art.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:多个领域正在通过机器学习应用来增加依赖度，导致数据伦理和隐私法规的出现，以及隐私保护机器学习（ppML）技术的发展。现有ppML技术通常会影响模型性能，以换取隐私。为了提高ppML的性能，我们提议一种基于robust representation learning的框架，可以同时保证数据隐私和模型性能。我们的方法包括在多目标下进行autoencoder训练，然后将编码部分的秘密特征和学习特征 concatenate为数据的编码形式。这种深度学习力学托的编码可以安全地传输到第三方进行敏感训练和超参调整。我们的提案可以帮助我们共享数据，无需担心数据的原始形式泄露。我们在不同的模式下进行了实验 validate，包括单模态和多模态设置，并达到了状态 искусственный的提升。
</details></li>
</ul>
<hr>
<h2 id="Parallel-and-Limited-Data-Voice-Conversion-Using-Stochastic-Variational-Deep-Kernel-Learning"><a href="#Parallel-and-Limited-Data-Voice-Conversion-Using-Stochastic-Variational-Deep-Kernel-Learning" class="headerlink" title="Parallel and Limited Data Voice Conversion Using Stochastic Variational Deep Kernel Learning"></a>Parallel and Limited Data Voice Conversion Using Stochastic Variational Deep Kernel Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04420">http://arxiv.org/abs/2309.04420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamadreza Jafaryani, Hamid Sheikhzadeh, Vahid Pourahmadi</li>
<li>for: 这篇论文是针对voice conversion问题，具体地是如何使用有限数据进行deep learning模型的训练，并且能够模型复杂的映射函数。</li>
<li>methods: 这篇论文提出了一种基于stochastic variational deep kernel learning（SVDKL）的voice conversion方法，它能够结合深度神经网络和 Gaussian process，并且可以解决有限数据的问题。</li>
<li>results: 在使用约80秒的训练数据进行训练后，这篇论文的方法比较其他方法取得了更高的mean opinion score、更小的spectral distortion和更好的偏好测试结果。<details>
<summary>Abstract</summary>
Typically, voice conversion is regarded as an engineering problem with limited training data. The reliance on massive amounts of data hinders the practical applicability of deep learning approaches, which have been extensively researched in recent years. On the other hand, statistical methods are effective with limited data but have difficulties in modelling complex mapping functions. This paper proposes a voice conversion method that works with limited data and is based on stochastic variational deep kernel learning (SVDKL). At the same time, SVDKL enables the use of deep neural networks' expressive capability as well as the high flexibility of the Gaussian process as a Bayesian and non-parametric method. When the conventional kernel is combined with the deep neural network, it is possible to estimate non-smooth and more complex functions. Furthermore, the model's sparse variational Gaussian process solves the scalability problem and, unlike the exact Gaussian process, allows for the learning of a global mapping function for the entire acoustic space. One of the most important aspects of the proposed scheme is that the model parameters are trained using marginal likelihood optimization, which considers both data fitting and model complexity. Considering the complexity of the model reduces the amount of training data by increasing the resistance to overfitting. To evaluate the proposed scheme, we examined the model's performance with approximately 80 seconds of training data. The results indicated that our method obtained a higher mean opinion score, smaller spectral distortion, and better preference tests than the compared methods.
</details>
<details>
<summary>摘要</summary>
通常，voice conversion被视为一个工程问题，受限于有限的训练数据。深入研究的深度学习方法需要大量数据，但是这限制了实际应用的可行性。相反，统计方法可以使用有限数据，但是它们无法模型复杂的映射函数。这篇论文提出了一种基于有限数据的voice conversion方法，该方法基于Stochastic Variational Deep Kernel Learning（SVDKL）。同时，SVDKL使得可以使用深度神经网络的表达能力以及高灵活的Gaussian процес为 Bayesian和非 Parametric方法。当权重积分是加到深度神经网络时，可以估计非稀盐和更复杂的函数。此外，模型的稀盐变量Gaussian进程解决了扩展性问题，与传统kernel相比，允许学习整个声学空间的全局映射函数。我们的方法的一个重要特点是通过最大化含义概率来训练模型参数，这会考虑数据适应和模型复杂度。通过增加模型复杂度，我们降低了训练数据的量，从而降低了过拟合的风险。为评估我们的方法，我们对约80秒的训练数据进行了测试。结果表明，我们的方法在意见序列、spectral distortion和偏好测试中获得了更高的 mean opinion score、更小的spectral distortion和更好的偏好测试结果，比与比较方法更高。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Preserving-Federated-Learning-with-Convolutional-Variational-Bottlenecks"><a href="#Privacy-Preserving-Federated-Learning-with-Convolutional-Variational-Bottlenecks" class="headerlink" title="Privacy Preserving Federated Learning with Convolutional Variational Bottlenecks"></a>Privacy Preserving Federated Learning with Convolutional Variational Bottlenecks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04515">http://arxiv.org/abs/2309.04515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Scheliga, Patrick Mäder, Marco Seeland<br>for: 防止梯度泄露攻击，保护隐私训练数据。methods: 使用variational modeling技术，实现隐私保护。results: 1. 发现variational modeling引入杂态性，使得梯度泄露攻击无法进行迭代加密攻击。 2. 提出了一种破坏隐私保护效果的攻击方法，并解释了如何保护隐私。 3. 提出了一种新的隐私模块——卷积变量瓶颈（CVB），可以在潜在攻击下保护隐私。<details>
<summary>Abstract</summary>
Gradient inversion attacks are an ubiquitous threat in federated learning as they exploit gradient leakage to reconstruct supposedly private training data. Recent work has proposed to prevent gradient leakage without loss of model utility by incorporating a PRivacy EnhanCing mODulE (PRECODE) based on variational modeling. Without further analysis, it was shown that PRECODE successfully protects against gradient inversion attacks. In this paper, we make multiple contributions. First, we investigate the effect of PRECODE on gradient inversion attacks to reveal its underlying working principle. We show that variational modeling introduces stochasticity into the gradients of PRECODE and the subsequent layers in a neural network. The stochastic gradients of these layers prevent iterative gradient inversion attacks from converging. Second, we formulate an attack that disables the privacy preserving effect of PRECODE by purposefully omitting stochastic gradients during attack optimization. To preserve the privacy preserving effect of PRECODE, our analysis reveals that variational modeling must be placed early in the network. However, early placement of PRECODE is typically not feasible due to reduced model utility and the exploding number of additional model parameters. Therefore, as a third contribution, we propose a novel privacy module -- the Convolutional Variational Bottleneck (CVB) -- that can be placed early in a neural network without suffering from these drawbacks. We conduct an extensive empirical study on three seminal model architectures and six image classification datasets. We find that all architectures are susceptible to gradient leakage attacks, which can be prevented by our proposed CVB. Compared to PRECODE, we show that our novel privacy module requires fewer trainable parameters, and thus computational and communication costs, to effectively preserve privacy.
</details>
<details>
<summary>摘要</summary>
gradient inversion attacks 是联邦学习中的普遍威胁，它们利用Gradient泄露来重建训练资料。 recent work 提出了防止Gradient泄露而无损模型价值的方法，包括在模型中添加一个基于可变型模型的Privacy EnhanCing mODulE (PRECODE)。 without further analysis, it was shown that PRECODE 成功地保护 Against gradient inversion attacks。在这篇 paper 中，我们做了多个贡献。 first, we investigate the effect of PRECODE on gradient inversion attacks to reveal its underlying working principle。 we show that variational modeling introduces stochasticity into the gradients of PRECODE and the subsequent layers in a neural network。 the stochastic gradients of these layers prevent iterative gradient inversion attacks from converging。second, we formulate an attack that disables the privacy preserving effect of PRECODE by purposefully omitting stochastic gradients during attack optimization。 to preserve the privacy preserving effect of PRECODE, our analysis reveals that variational modeling must be placed early in the network。 however, early placement of PRECODE is typically not feasible due to reduced model utility and the exploding number of additional model parameters。therefore, as a third contribution, we propose a novel privacy module -- the Convolutional Variational Bottleneck (CVB) -- that can be placed early in a neural network without suffering from these drawbacks。 we conduct an extensive empirical study on three seminal model architectures and six image classification datasets。 we find that all architectures are susceptible to gradient leakage attacks, which can be prevented by our proposed CVB。 compared to PRECODE, we show that our novel privacy module requires fewer trainable parameters, and thus computational and communication costs, to effectively preserve privacy。
</details></li>
</ul>
<hr>
<h2 id="Emergent-learning-in-physical-systems-as-feedback-based-aging-in-a-glassy-landscape"><a href="#Emergent-learning-in-physical-systems-as-feedback-based-aging-in-a-glassy-landscape" class="headerlink" title="Emergent learning in physical systems as feedback-based aging in a glassy landscape"></a>Emergent learning in physical systems as feedback-based aging in a glassy landscape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04382">http://arxiv.org/abs/2309.04382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vidyesh Rao Anisetti, Ananth Kandala, J. M. Schwarz</li>
<li>for: 研究了使用线性物理网络学习线性变换的方法，并观察了这些网络的物理性质如何随权重更新规则的发展。</li>
<li>methods: 使用了线性物理网络来学习线性变换，并通过观察这些网络的物理性质来了解它们如何随权重更新规则的发展。</li>
<li>results: 发现了这些网络在学习过程中的相似性和生物系统中的记忆形成过程，以及在不同输入和反馈边界力的作用下，网络的相似性和记忆形成的机制。<details>
<summary>Abstract</summary>
By training linear physical networks to learn linear transformations, we discern how their physical properties evolve due to weight update rules. Our findings highlight a striking similarity between the learning behaviors of such networks and the processes of aging and memory formation in disordered and glassy systems. We show that the learning dynamics resembles an aging process, where the system relaxes in response to repeated application of the feedback boundary forces in presence of an input force, thus encoding a memory of the input-output relationship. With this relaxation comes an increase in the correlation length, which is indicated by the two-point correlation function for the components of the network. We also observe that the square root of the mean-squared error as a function of epoch takes on a non-exponential form, which is a typical feature of glassy systems. This physical interpretation suggests that by encoding more detailed information into input and feedback boundary forces, the process of emergent learning can be rather ubiquitous and, thus, serve as a very early physical mechanism, from an evolutionary standpoint, for learning in biological systems.
</details>
<details>
<summary>摘要</summary>
通过训练线性物理网络学习线性变换，我们发现其物理性质如何随权重更新规则而演化。我们的发现表明linear physical networks的学习行为和缺陷系统和玻璃系统中的年轻和记忆形成过程具有很大的相似性。我们显示，学习过程类似于年轻过程，系统在反复应用反馈边界力时会relax，从而记忆输入输出关系。这种relaxation会导致系统的相关性增加，这可以通过网络组件之间的两点相关函数来证明。此外，我们发现epoch随时间的平方根均方差的函数具有非线性形式，这是玻璃系统的典型特征。这种物理解释表明，通过增加输入和反馈边界力的细节信息，emergent learning的过程可以是非常普遍的，从EVOLUTIONARY的角度来看，这可能是生物系统中的极早物理机制。
</details></li>
</ul>
<hr>
<h2 id="Generalization-Bounds-Perspectives-from-Information-Theory-and-PAC-Bayes"><a href="#Generalization-Bounds-Perspectives-from-Information-Theory-and-PAC-Bayes" class="headerlink" title="Generalization Bounds: Perspectives from Information Theory and PAC-Bayes"></a>Generalization Bounds: Perspectives from Information Theory and PAC-Bayes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04381">http://arxiv.org/abs/2309.04381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fredrik Hellström, Giuseppe Durisi, Benjamin Guedj, Maxim Raginsky</li>
<li>for: 本文旨在介绍信息理论上的泛化能力，以及与PAC-Bayesian方法的强关系。</li>
<li>methods: 本文使用信息论的泛化 bound 和 PAC-Bayesian方法来研究机器学习算法的泛化能力。</li>
<li>results: 本文提出了一种简单的泛化 bound，并证明了它在多个场景下具有泛化能力。此外，本文还应用到深度学习中。<details>
<summary>Abstract</summary>
A fundamental question in theoretical machine learning is generalization. Over the past decades, the PAC-Bayesian approach has been established as a flexible framework to address the generalization capabilities of machine learning algorithms, and design new ones. Recently, it has garnered increased interest due to its potential applicability for a variety of learning algorithms, including deep neural networks. In parallel, an information-theoretic view of generalization has developed, wherein the relation between generalization and various information measures has been established. This framework is intimately connected to the PAC-Bayesian approach, and a number of results have been independently discovered in both strands. In this monograph, we highlight this strong connection and present a unified treatment of generalization. We present techniques and results that the two perspectives have in common, and discuss the approaches and interpretations that differ. In particular, we demonstrate how many proofs in the area share a modular structure, through which the underlying ideas can be intuited. We pay special attention to the conditional mutual information (CMI) framework; analytical studies of the information complexity of learning algorithms; and the application of the proposed methods to deep learning. This monograph is intended to provide a comprehensive introduction to information-theoretic generalization bounds and their connection to PAC-Bayes, serving as a foundation from which the most recent developments are accessible. It is aimed broadly towards researchers with an interest in generalization and theoretical machine learning.
</details>
<details>
<summary>摘要</summary>
一个基本问题在理论机器学习领域是泛化。过去几十年，PAC-Bayesian方法在解决机器学习算法的泛化能力方面成为了一个灵活的框架，并设计了新的算法。最近几年，它受到了更多的关注，因为它可以应用于许多学习算法，包括深度神经网络。在平行的情况下，一种信息论视角的泛化发展出来，其中与泛化之间的关系与各种信息度量相关。这个框架与PAC-Bayesian方法密切相关，并在两个条件下独立地发现了一些结果。在这本善本中，我们强调这种强有力的连接，并提供一个统一的对待方法。我们介绍了这两个视角之间的共同技巧和结果，并讨论各种不同的方法和解释。特别是，我们示出了许多证明在这个领域中具有模块结构，从而可以直观到下面的基本想法。我们特别关注 conditional mutual information（CMI）框架，信息复杂度学习算法的分析研究，以及将提议的方法应用于深度学习。这本善本的目的是为研究人员提供一种全面的信息论泛化下限和其与PAC-Bayes之间的连接，作为对最新发展的基础。它是向研究泛化和理论机器学习的广泛兴趣 GROUP 的欢迎。
</details></li>
</ul>
<hr>
<h2 id="Seeing-Eye-Quadruped-Navigation-with-Force-Responsive-Locomotion-Control"><a href="#Seeing-Eye-Quadruped-Navigation-with-Force-Responsive-Locomotion-Control" class="headerlink" title="Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control"></a>Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04370">http://arxiv.org/abs/2309.04370</a></li>
<li>repo_url: None</li>
<li>paper_authors: David DeFazio, Eisuke Hirota, Shiqi Zhang</li>
<li>for: 这个论文的目的是开发一种可以抗 External tugging forces 的 seeing-eye robot系统，以帮助视障人士进行导航。</li>
<li>methods: 论文使用了强化学习（RL）和指导学习来同时训练一个稳定的步行控制器和一个外部力估计器。</li>
<li>results: 实验结果表明，我们的控制器具有 External forces 的抗衡能力，而我们的 seeing-eye 系统可以准确地检测力方向。我们在真实的四足机器人上进行了实验，并在视频中展示了一位盲人在使用我们的系统时的导航过程。Video可以在我们项目页面上查看：<a target="_blank" rel="noopener" href="https://bu-air-lab.github.io/guide_dog/">https://bu-air-lab.github.io/guide_dog/</a><details>
<summary>Abstract</summary>
Seeing-eye robots are very useful tools for guiding visually impaired people, potentially producing a huge societal impact given the low availability and high cost of real guide dogs. Although a few seeing-eye robot systems have already been demonstrated, none considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect force direction. We demonstrate our full seeing-eye robot system on a real quadruped robot with a blindfolded human. The video can be seen at our project page: https://bu-air-lab.github.io/guide_dog/
</details>
<details>
<summary>摘要</summary>
seeing-eye  роботы非常有用于引导视障人士，可能会产生巨大的社会影响，因为现有的真正的引导狗非常scarce和昂贵。虽然一些seeing-eye robot系统已经被示出，但None of them considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect force direction. We demonstrate our full seeing-eye robot system on a real quadruped robot with a blindfolded human. The video can be seen at our project page: <https://bu-air-lab.github.io/guide_dog/>.Here's the word-for-word translation of the text into Simplified Chinese:seeing-eye 机器人非常有用于引导视障人士，可能会产生巨大的社会影响，因为现有的真正的引导狗非常scarce 和昂贵。虽然一些seeing-eye robot系统已经被示出，但None of them considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect force direction. We demonstrate our full seeing-eye robot system on a real quadruped robot with a blindfolded human. The video can be seen at our project page: <https://bu-air-lab.github.io/guide_dog/>.
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-for-Classifying-2D-Grid-Based-Level-Completability"><a href="#Active-Learning-for-Classifying-2D-Grid-Based-Level-Completability" class="headerlink" title="Active Learning for Classifying 2D Grid-Based Level Completability"></a>Active Learning for Classifying 2D Grid-Based Level Completability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04367">http://arxiv.org/abs/2309.04367</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mahsabazzaz/level-completabilty-x-active-learning">https://github.com/mahsabazzaz/level-completabilty-x-active-learning</a></li>
<li>paper_authors: Mahsa Bazzaz, Seth Cooper</li>
<li>for: 用于评估生成器生成的水平的可完成性</li>
<li>methods: 使用活动学习方法来标注生成的水平可完成性</li>
<li>results: 使用活动学习方法可以使得类ifier性能更高，使用同样量的标注数据。<details>
<summary>Abstract</summary>
Determining the completability of levels generated by procedural generators such as machine learning models can be challenging, as it can involve the use of solver agents that often require a significant amount of time to analyze and solve levels. Active learning is not yet widely adopted in game evaluations, although it has been used successfully in natural language processing, image and speech recognition, and computer vision, where the availability of labeled data is limited or expensive. In this paper, we propose the use of active learning for learning level completability classification. Through an active learning approach, we train deep-learning models to classify the completability of generated levels for Super Mario Bros., Kid Icarus, and a Zelda-like game. We compare active learning for querying levels to label with completability against random queries. Our results show using an active learning approach to label levels results in better classifier performance with the same amount of labeled data.
</details>
<details>
<summary>摘要</summary>
确定由生成器模型生成的关卡的完整性可能是一项挑战，因为它可能需要使用解决器代理，这些解决器经常需要较长的时间来分析和解决关卡。活动学习并未广泛应用于游戏评估中，although it has been successfully applied in natural language processing, image and speech recognition, and computer vision, where the availability of labeled data is limited or expensive。在本文中，我们提议使用活动学习来学习关卡完整性分类。通过活动学习方法，我们用深度学习模型来分类生成的关卡的完整性，并对Super Mario Bros., Kid Icarus,和一款zelda-like游戏进行了测试。我们比较了使用活动学习方法来查询关卡的完整性标签与随机查询的性能。我们的结果显示，使用活动学习方法来标注关卡的完整性可以提高分类器的性能，使用同样的标签数据。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Power-Signals-An-Automated-Approach-to-Electrical-Disturbance-Identification-Within-a-Power-Transmission-System"><a href="#Learning-from-Power-Signals-An-Automated-Approach-to-Electrical-Disturbance-Identification-Within-a-Power-Transmission-System" class="headerlink" title="Learning from Power Signals: An Automated Approach to Electrical Disturbance Identification Within a Power Transmission System"></a>Learning from Power Signals: An Automated Approach to Electrical Disturbance Identification Within a Power Transmission System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04361">http://arxiv.org/abs/2309.04361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan D. Boyd, Joshua H. Tyler, Anthony M. Murphy, Donald R. Reising<br>for:这个研究是为了 automatizing the analysis of power quality events recorded by digital fault recorders and power quality monitors within a power transmission system.methods:这个方法使用了规则基于的分析来检查时间和频率域特性的电压和电流信号。研究人员可以设置自定义的阈值来分类每个异常事件。results:这个研究获得了九十九 percent的准确率，并且预计将减少内存需求，并且帮助检测小型信号变化，以预防问题发生。这个 проек 预计将减少工程师的时间投入，并提高了变电压系统的可靠性。<details>
<summary>Abstract</summary>
As power quality becomes a higher priority in the electric utility industry, the amount of disturbance event data continues to grow. Utilities do not have the required personnel to analyze each event by hand. This work presents an automated approach for analyzing power quality events recorded by digital fault recorders and power quality monitors operating within a power transmission system. The automated approach leverages rule-based analytics to examine the time and frequency domain characteristics of the voltage and current signals. Customizable thresholds are set to categorize each disturbance event. The events analyzed within this work include various faults, motor starting, and incipient instrument transformer failure. Analytics for fourteen different event types have been developed. The analytics were tested on 160 signal files and yielded an accuracy of ninety-nine percent. Continuous, nominal signal data analysis is performed using an approach coined as the cyclic histogram. The cyclic histogram process will be integrated into the digital fault recorders themselves to facilitate the detection of subtle signal variations that are too small to trigger a disturbance event and that can occur over hours or days. In addition to reducing memory requirements by a factor of 320, it is anticipated that cyclic histogram processing will aid in identifying incipient events and identifiers. This project is expected to save engineers time by automating the classification of disturbance events and increase the reliability of the transmission system by providing near real time detection and identification of disturbances as well as prevention of problems before they occur.
</details>
<details>
<summary>摘要</summary>
随着电力质量在电力供应业中的重要性提高，发生事件数据的量继续增加。utilities没有足够的人员来手动分析每个事件。本工作提出了一种自动化分析力量质量事件记录器和电力质量监测器在电力传输系统中记录的数据的方法。该方法利用规则基本的分析来查看电压和电流信号在时域和频域的特征。可定制的阈值设置以 categorize each disturbance event。本工作分析了多种各种事件，包括各种缺陷、电机启动和潜在的仪器变换器故障。为了提高分析效率，本工作采用了 cycles histogram 技术，可以在实时near real-time detection and identification of disturbances, as well as preventing problems before they occur.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Value-Compressed-Sparse-Column-VCSC-Sparse-Matrix-Storage-for-Redundant-Data"><a href="#Value-Compressed-Sparse-Column-VCSC-Sparse-Matrix-Storage-for-Redundant-Data" class="headerlink" title="Value-Compressed Sparse Column (VCSC): Sparse Matrix Storage for Redundant Data"></a>Value-Compressed Sparse Column (VCSC): Sparse Matrix Storage for Redundant Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04355">http://arxiv.org/abs/2309.04355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Skyler Ruiter, Seth Wolfgang, Marc Tunnell, Timothy Triche Jr., Erin Carrier, Zachary DeBruine</li>
<li>for: 本研究旨在提出两种扩展了简约矩阵存储格式（CSC），以优化稀疏数据的存储和计算效率。</li>
<li>methods: 研究人员提出了两种新的简约存储格式：Value-Compressed Sparse Column (VCSC)和Index- and Value-Compressed Sparse Column (IVCSC)，它们都利用了稀疏数据中的数据重复性来进行压缩，从而提高存储和计算效率。</li>
<li>results: 对于 simulated 和实际数据，研究人员的测试表明，VCSC 和 IVCSC 可以在压缩形式下读取稀疏数据，而且计算成本几乎没有增加。这两种新的压缩格式可以广泛应用于稀疏数据的存储和计算。<details>
<summary>Abstract</summary>
Compressed Sparse Column (CSC) and Coordinate (COO) are popular compression formats for sparse matrices. However, both CSC and COO are general purpose and cannot take advantage of any of the properties of the data other than sparsity, such as data redundancy. Highly redundant sparse data is common in many machine learning applications, such as genomics, and is often too large for in-core computation using conventional sparse storage formats. In this paper, we present two extensions to CSC: (1) Value-Compressed Sparse Column (VCSC) and (2) Index- and Value-Compressed Sparse Column (IVCSC). VCSC takes advantage of high redundancy within a column to further compress data up to 3-fold over COO and 2.25-fold over CSC, without significant negative impact to performance characteristics. IVCSC extends VCSC by compressing index arrays through delta encoding and byte-packing, achieving a 10-fold decrease in memory usage over COO and 7.5-fold decrease over CSC. Our benchmarks on simulated and real data show that VCSC and IVCSC can be read in compressed form with little added computational cost. These two novel compression formats offer a broadly useful solution to encoding and reading redundant sparse data.
</details>
<details>
<summary>摘要</summary>
压缩簇Column (CSC) 和坐标 (COO) 是对叠 sparse 矩阵的广泛储存格式。然而，CSC 和 COO 都是通用的，无法利用资料的其他特性，例如资料的重复性。许多机器学习应用程序中的叠 sparse 数据都具有高度的重复性，例如 genomics，而这些数据通常是 convention 叠 sparse 储存格式中的过大。在这篇文章中，我们提出了两个 CSC 的扩展：（1）值压缩簇Column (VCSC) 和（2） indeks 和值压缩簇Column (IVCSC)。VCSC 利用矩阵中每排的高度重复性，进一步将数据压缩到 3 倍以上 COO 和 2.25 倍以上 CSC，而无需对性能特性造成严重的负面影响。IVCSC 则是将 indeks 范围进行 delta 编码和字节封装，实现了与 COO 比较的 10 倍减少的内存使用量。我们对实验和实际数据进行了对于压缩形式的读取和解压的benchmark，发现 VCSC 和 IVCSC 可以实现轻便的压缩读取。这两种新的压缩格式可以广泛地应用于叠 sparse 数据的储存和处理。
</details></li>
</ul>
<hr>
<h2 id="Mobile-V-MoEs-Scaling-Down-Vision-Transformers-via-Sparse-Mixture-of-Experts"><a href="#Mobile-V-MoEs-Scaling-Down-Vision-Transformers-via-Sparse-Mixture-of-Experts" class="headerlink" title="Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts"></a>Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04354">http://arxiv.org/abs/2309.04354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erik Daxberger, Floris Weers, Bowen Zhang, Tom Gunter, Ruoming Pang, Marcin Eichner, Michael Emmersberger, Yinfei Yang, Alexander Toshev, Xianzhi Du</li>
<li>for: 本研究使用 sparse Mixture-of-Experts (MoE) 模型，以适应资源有限的视觉应用。</li>
<li>methods: 我们提议一种简化的 Mobile Vision MoE 设计，将整个图像 Routing 到专家中。我们还提出了一种稳定的 MoE 训练方法，使用超类信息导航Router。</li>
<li>results: 我们的 sparse Mobile Vision MoEs (V-MoEs) 可以实现更好的性能与效率平衡，比如 ViT-Tiny 模型，我们的 Mobile V-MoE 在 ImageNet-1k 上比 dense ViTs 高出3.39%。而只有54M FLOPs的推理成本的 ViT 变体，我们的 MoE 还提高了4.66%。<details>
<summary>Abstract</summary>
Sparse Mixture-of-Experts models (MoEs) have recently gained popularity due to their ability to decouple model size from inference efficiency by only activating a small subset of the model parameters for any given input token. As such, sparse MoEs have enabled unprecedented scalability, resulting in tremendous successes across domains such as natural language processing and computer vision. In this work, we instead explore the use of sparse MoEs to scale-down Vision Transformers (ViTs) to make them more attractive for resource-constrained vision applications. To this end, we propose a simplified and mobile-friendly MoE design where entire images rather than individual patches are routed to the experts. We also propose a stable MoE training procedure that uses super-class information to guide the router. We empirically show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off between performance and efficiency than the corresponding dense ViTs. For example, for the ViT-Tiny model, our Mobile V-MoE outperforms its dense counterpart by 3.39% on ImageNet-1k. For an even smaller ViT variant with only 54M FLOPs inference cost, our MoE achieves an improvement of 4.66%.
</details>
<details>
<summary>摘要</summary>
《稀疏混合专家模型（MoE）》在最近几年内得到了广泛的关注，因为它可以让模型大小与输入token的处理效率分离开来，只有对输入token进行少量的参数活动。这使得稀疏MoE模型在不同领域，如自然语言处理和计算机视觉等领域取得了无 precedent的成功。在这个工作中，我们尝试使用稀疏MoE模型来缩减视Transformers（ViTs），使其更适合具有资源限制的视觉应用。为此，我们提出了简单且移动设备友好的MoE设计，在整个图像被 routed 到专家中，而不是各个小块。我们还提出了稳定的MoE训练方法，使用超类信息来导航路由器。我们实际上示出，我们的稀疏移动视觉MoE（V-MoE）可以在 ImageNet-1k 上比 dense ViTs 更好地协议性和效率之间的平衡。例如，对于 ViT-Tiny 模型，我们的 Mobile V-MoE 在 ImageNet-1k 上比其密集对手提高了3.39%。而对于只有54M FLOPs的推理成本的 ViT 变体，我们的 MoE 提高了4.66%。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Robustification-of-Zero-Shot-Models-With-Foundation-Models"><a href="#Zero-Shot-Robustification-of-Zero-Shot-Models-With-Foundation-Models" class="headerlink" title="Zero-Shot Robustification of Zero-Shot Models With Foundation Models"></a>Zero-Shot Robustification of Zero-Shot Models With Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04344">http://arxiv.org/abs/2309.04344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sprocketlab/roboshot">https://github.com/sprocketlab/roboshot</a></li>
<li>paper_authors: Dyah Adila, Changho Shin, Linrong Cai, Frederic Sala</li>
<li>for: 这篇论文旨在提高预训模型的敏感性和可靠性，并且不需要进一步训练。</li>
<li>methods: 本文使用零条件语言模型（LM）获取任务描述中有用的洞察，并将这些洞察与预训模型的嵌入相结合，以移除害虫和提高有益的 ком成分。这个方法不需要任何超vision。</li>
<li>results: 本文在九个图像和数据分类任务上评估了RoboShot方法，获得了15.98%的平均提升，比许多零条件基eline的表现更好。此外，本文还证明了RoboShot方法可以与多种预训模型和语言模型相容。<details>
<summary>Abstract</summary>
Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and show an average improvement of 15.98% over several zero-shot baselines. Additionally, we demonstrate that RoboShot is compatible with a variety of pretrained and language models.
</details>
<details>
<summary>摘要</summary>
zero-shot推理是一种强大的思想框架，它允许在下游分类任务中使用已经预训练的模型，无需进一步训练。然而，这些模型可能受到遗传的偏见的影响，这可能会影响其性能。传统的解决方案是细化，但这会损害预训练模型的一个优点，即可以直接使用。我们提出了RoboShot，一种可以在完全无需supervision的情况下改进预训练模型的嵌入的方法。我们使用零批语言模型（LM）来获取任务描述中有用的洞察，并将这些洞察embedding在模型中，以移除害虫和增强有用的组分。理论上，我们提供了零批嵌入中偏见的简单和可追踪的模型，并给出了在哪些条件下我们的方法可以提高性能的结果。实验ally，我们在九个图像和NLP分类任务上评估了RoboShot，并显示了15.98%的平均提高。此外，我们还证明了RoboShot可以与多种预训练和语言模型相容。
</details></li>
</ul>
<hr>
<h2 id="Online-Submodular-Maximization-via-Online-Convex-Optimization"><a href="#Online-Submodular-Maximization-via-Online-Convex-Optimization" class="headerlink" title="Online Submodular Maximization via Online Convex Optimization"></a>Online Submodular Maximization via Online Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04339">http://arxiv.org/abs/2309.04339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tareq Si-Salem, Gözde Özcan, Iasonas Nikolaou, Evimaria Terzi, Stratis Ioannidis</li>
<li>for: 本研究探讨了幂等子模式最大化问题在一般环境中的在线优化。</li>
<li>methods: 本文使用了在线优化和准确轮征来解决这个问题，并证明了这种方法可以实现幂等子模式函数的优化。</li>
<li>results: 本文证明了在线优化可以将幂等子模式函数的优化转化为在线几何优化问题，并且这种方法可以在多种在线学习问题中实现低于线性 regret的性能。<details>
<summary>Abstract</summary>
We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
</details>
<details>
<summary>摘要</summary>
我们研究简单升级最大化在通用环境中，具体来说是在在线设置中。我们证明在线优化某种大类卷积函数，即质量梯度 potential functions，可以转化为在线几何优化（OCO）。这是因为这类函数具有折衣函数的下降性，因此OCO策略，加上适当的轮减方案，可以实现在组合设置中的子线性 regret。我们表明我们的减少扩展到许多不同的在线学习问题，包括动态 regret、投筹和乐观学习设置。
</details></li>
</ul>
<hr>
<h2 id="Decreasing-the-Computing-Time-of-Bayesian-Optimization-using-Generalizable-Memory-Pruning"><a href="#Decreasing-the-Computing-Time-of-Bayesian-Optimization-using-Generalizable-Memory-Pruning" class="headerlink" title="Decreasing the Computing Time of Bayesian Optimization using Generalizable Memory Pruning"></a>Decreasing the Computing Time of Bayesian Optimization using Generalizable Memory Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04510">http://arxiv.org/abs/2309.04510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander E. Siemenn, Tonio Buonassisi</li>
<li>for: 提高bayesian优化（BO）的计算效率，使其能够处理高维度或大量数据集。</li>
<li>methods: 使用内存剔除和约束优化来减少BO计算时间，并且可以与任何抽象函数和获取函数结合使用。</li>
<li>results: 实现了将BO计算时间从增长到静止的转变，无需牺牲 convergence 性能。此外，通过使用不同的数据集、抽象函数和获取函数，证明了该方法的通用性。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) suffers from long computing times when processing highly-dimensional or large data sets. These long computing times are a result of the Gaussian process surrogate model having a polynomial time complexity with the number of experiments. Running BO on high-dimensional or massive data sets becomes intractable due to this time complexity scaling, in turn, hindering experimentation. Alternative surrogate models have been developed to reduce the computing utilization of the BO procedure, however, these methods require mathematical alteration of the inherit surrogate function, pigeonholing use into only that function. In this paper, we demonstrate a generalizable BO wrapper of memory pruning and bounded optimization, capable of being used with any surrogate model and acquisition function. Using this memory pruning approach, we show a decrease in wall-clock computing times per experiment of BO from a polynomially increasing pattern to a sawtooth pattern that has a non-increasing trend without sacrificing convergence performance. Furthermore, we illustrate the generalizability of the approach across two unique data sets, two unique surrogate models, and four unique acquisition functions. All model implementations are run on the MIT Supercloud state-of-the-art computing hardware.
</details>
<details>
<summary>摘要</summary>
bayesian 优化 (BO) 在处理高维度或大数据集时会面临长时间计算问题。这些长时间计算问题是由 Gaussian 过程替代模型的数据量呈指数增长的特性引起的，从而使 BO 在高维度或大数据集上运行成为不可能的。为了解决这个问题，人们已经开发出了一些替代的拟合模型，但这些方法需要修改潜在的拟合函数，这限制了它们的通用性。在这篇论文中，我们提出了一个通用的 BO 包装器，可以与任何拟合模型和争取函数结合使用。我们使用这种内存剔除方法，可以降低 BO 每次实验的墙 clock 计算时间，从 polynomial 增长趋势转变为 sawtooth 趋势，而无需牺牲整体性能。此外，我们还证明了这种方法在两个不同的数据集、两个不同的拟合模型和四个不同的争取函数上的普适性。所有模型实现都运行在 MIT Supercloud  cutting-edge 计算硬件上。
</details></li>
</ul>
<hr>
<h2 id="Encoding-Multi-Domain-Scientific-Papers-by-Ensembling-Multiple-CLS-Tokens"><a href="#Encoding-Multi-Domain-Scientific-Papers-by-Ensembling-Multiple-CLS-Tokens" class="headerlink" title="Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens"></a>Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04333">http://arxiv.org/abs/2309.04333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ronaldseoh/multi2spe">https://github.com/ronaldseoh/multi2spe</a></li>
<li>paper_authors: Ronald Seoh, Haw-Shiuan Chang, Andrew McCallum</li>
<li>for: This paper is written for the task of scientific paper vector encoding, specifically in the context of multi-domain corpora.</li>
<li>methods: The paper proposes a new method called Multi2SPE, which uses multiple CLS tokens to learn diverse ways of aggregating token embeddings and then combines them to create a single vector representation.</li>
<li>results: The paper shows that Multi2SPE reduces error by up to 25 percent in multi-domain citation prediction, while requiring only a negligible amount of computation in addition to one BERT forward pass.Here’s the Chinese translation of the three pieces of information:</li>
<li>for: 这篇论文是为了科学论文向量编码任务而写的，具体来说是在多个领域的多个科学文档中进行。</li>
<li>methods: 论文提出了一种新方法 called Multi2SPE，它使用多个 CLS 标签来学习不同的方式归约token embedding，然后将它们合并成一个单一的向量表示。</li>
<li>results: 论文表明，Multi2SPE 可以在多个领域的多个科学文档中预测引用关系，错误率下降到最多 25%，同时只需要额外计算一个 BERT 前进 pass。<details>
<summary>Abstract</summary>
Many useful tasks on scientific documents, such as topic classification and citation prediction, involve corpora that span multiple scientific domains. Typically, such tasks are accomplished by representing the text with a vector embedding obtained from a Transformer's single CLS token. In this paper, we argue that using multiple CLS tokens could make a Transformer better specialize to multiple scientific domains. We present Multi2SPE: it encourages each of multiple CLS tokens to learn diverse ways of aggregating token embeddings, then sums them up together to create a single vector representation. We also propose our new multi-domain benchmark, Multi-SciDocs, to test scientific paper vector encoders under multi-domain settings. We show that Multi2SPE reduces error by up to 25 percent in multi-domain citation prediction, while requiring only a negligible amount of computation in addition to one BERT forward pass.
</details>
<details>
<summary>摘要</summary>
许多科学文档处理任务，如主题分类和引用预测，需要跨多个科学领域的文本集合。通常，这些任务通过使用 transformer 的单个 CLS token 获得vector embedding来完成。在这篇论文中，我们 argue 使用多个 CLS token 可以使 transformer 更好地特化到多个科学领域。我们提出 Multi2SPE：它鼓励每个多个 CLS token 学习不同的方式汇聚token embedding，然后将它们综合计算而成单个vector表示。我们还提出了我们的新的多领域测试套件 Multi-SciDocs，用于在多领域settings下测试科学论文vector编码器。我们发现 Multi2SPE 可以在多领域引用预测中减少错误率达到25%，只需要额外计算量与一个BERT前进 pass相当。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-Use-Graphs-When-They-Shouldn’t"><a href="#Graph-Neural-Networks-Use-Graphs-When-They-Shouldn’t" class="headerlink" title="Graph Neural Networks Use Graphs When They Shouldn’t"></a>Graph Neural Networks Use Graphs When They Shouldn’t</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04332">http://arxiv.org/abs/2309.04332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mayabechlerspeicher/Graph_Neural_Networks_Overfit_Graphs">https://github.com/mayabechlerspeicher/Graph_Neural_Networks_Overfit_Graphs</a></li>
<li>paper_authors: Maya Bechler-Speicher, Ido Amos, Ran Gilad-Bachrach, Amir Globerson</li>
<li>for:  This paper aims to address the issue of graph neural networks (GNNs) overfitting the graph structure, even when it is not informative for the predictive task.</li>
<li>methods:  The paper uses empirical and theoretical analyses to examine the phenomenon of GNNs overfitting graph structures, and proposes a graph-editing method to mitigate this issue.</li>
<li>results:  The proposed method improves the accuracy of GNNs across multiple benchmarks, demonstrating its effectiveness in mitigating the tendency of GNNs to overfit graph structures that should be ignored.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文目的是解决图神经网络（GNNs）在无用于预测任务的图结构上过拟合的问题。</li>
<li>methods: 这篇论文使用实验和理论分析来研究GNNs在图结构上过拟合的现象，并提出了一种图编辑方法来解决这个问题。</li>
<li>results: 提出的方法在多个benchmark上提高了GNNs的准确率，证明了它的有效性在避免GNNs在无用于预测任务的图结构上过拟合。<details>
<summary>Abstract</summary>
Predictions over graphs play a crucial role in various domains, including social networks, molecular biology, medicine, and more. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Instances of graph labeling problems consist of the graph-structure (i.e., the adjacency matrix), along with node-specific feature vectors. In some cases, this graph-structure is non-informative for the predictive task. For instance, molecular properties such as molar mass depend solely on the constituent atoms (node features), and not on the molecular structure. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting. We then provide a theoretical explanation for this phenomenon, via analyzing the implicit bias of gradient-descent-based learning of GNNs in this setting. Finally, based on our empirical and theoretical findings, we propose a graph-editing method to mitigate the tendency of GNNs to overfit graph-structures that should be ignored. We show that this method indeed improves the accuracy of GNNs across multiple benchmarks.
</details>
<details>
<summary>摘要</summary>
Graf 预测在不同领域中发挥重要作用，如社交网络、分子生物学、医学等。图神经网络（GNNs）已经成为图数据学习的主要方法。实际情况下，图标签问题的实例包括图结构（即邻接矩阵）以及节点特征向量。在某些情况下，图结构无法为预测任务提供有用信息，例如分子性质如分子质量完全取决于节点特征，而不是分子结构。虽然GNNs具有忽略图结构的能力，但是不清楚它们会这样做。在这项工作中，我们发现GNNs实际上往往过拟合图结构，即它们在可以忽略图结构时仍然使用图结构。我们通过不同的图分布对这种现象进行了研究，并发现正则图更加鲁棒。然后，我们提供了一种理论解释，即使用梯度下降学习GNNs在这种设置下的隐式偏见。最后，根据我们的实际和理论发现，我们提出了一种图编辑方法来缓解GNNs过拟合图结构的现象。我们证明这种方法可以提高GNNs的准确性在多个标准测试集上。
</details></li>
</ul>
<hr>
<h2 id="Generating-the-Ground-Truth-Synthetic-Data-for-Label-Noise-Research"><a href="#Generating-the-Ground-Truth-Synthetic-Data-for-Label-Noise-Research" class="headerlink" title="Generating the Ground Truth: Synthetic Data for Label Noise Research"></a>Generating the Ground Truth: Synthetic Data for Label Noise Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04318">http://arxiv.org/abs/2309.04318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sjoerd-de-vries/synlabel">https://github.com/sjoerd-de-vries/synlabel</a></li>
<li>paper_authors: Sjoerd de Vries, Dirk Thierens</li>
<li>for: 提高标签频率误差的研究</li>
<li>methods: 提出 SYNLABEL 框架，通过自定义函数和重新混合标签来生成干净的基准数据集，并使用软标签或标签分布来直接注入和量化标签噪声</li>
<li>results: 比较 SYNLABEL 与传统方法的性能表现，并说明 SYNLABEL 可以更好地处理标签噪声和提高模型的泛化能力<details>
<summary>Abstract</summary>
Most real-world classification tasks suffer from label noise to some extent. Such noise in the data adversely affects the generalization error of learned models and complicates the evaluation of noise-handling methods, as their performance cannot be accurately measured without clean labels. In label noise research, typically either noisy or incomplex simulated data are accepted as a baseline, into which additional noise with known properties is injected. In this paper, we propose SYNLABEL, a framework that aims to improve upon the aforementioned methodologies. It allows for creating a noiseless dataset informed by real data, by either pre-specifying or learning a function and defining it as the ground truth function from which labels are generated. Furthermore, by resampling a number of values for selected features in the function domain, evaluating the function and aggregating the resulting labels, each data point can be assigned a soft label or label distribution. Such distributions allow for direct injection and quantification of label noise. The generated datasets serve as a clean baseline of adjustable complexity into which different types of noise may be introduced. We illustrate how the framework can be applied, how it enables quantification of label noise and how it improves over existing methodologies.
</details>
<details>
<summary>摘要</summary>
现实世界中的大多数分类任务都受到标签噪声的影响，这种噪声会使得学习的模型的泛化误差增大，同时也使得标签处理方法的评估变得更加复杂，因为无法准确测量噪声的影响。在标签噪声研究中，通常采用 either noisy or incomplex simulated data as a baseline, 并在这些数据中加入已知噪声。在这篇论文中，我们提出了 SYNLABEL 框架，这是一种旨在改进现有方法ologies的框架。它允许创建一个噪声free dataset，该dataset informed by real data，可以通过预先pecifying或学习函数，并将其定义为真实数据中的基准函数。此外，通过在函数空间中采样一些值，评估函数并对其进行汇总，每个数据点可以被赋予一个软标签或标签分布。这些分布可以直接对噪声进行直接注入和量化。生成的数据集可以作为一个清晰的基准，可以对不同类型的噪声进行直接引入。我们示例如如何应用该框架，如何使其能量量化标签噪声，以及如何它超过现有的方法ologies。
</details></li>
</ul>
<hr>
<h2 id="Actor-critic-learning-algorithms-for-mean-field-control-with-moment-neural-networks"><a href="#Actor-critic-learning-algorithms-for-mean-field-control-with-moment-neural-networks" class="headerlink" title="Actor critic learning algorithms for mean-field control with moment neural networks"></a>Actor critic learning algorithms for mean-field control with moment neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04317">http://arxiv.org/abs/2309.04317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huyên Pham, Xavier Warin</li>
<li>for: 解决连续时间奖励学习中的mean-field控制问题</li>
<li>methods: 使用参数化随机政策和值函数 gradient-based representation, 使用 moment neural network 函数在 Wasserstein 空间上学习</li>
<li>results: 提供了一系列数值结果，包括多维设置和非线性二次mean-field控制问题 with 控制幂等Here’s the English version for reference:</li>
<li>for: Solving mean-field control problems within a continuous time reinforcement learning setting</li>
<li>methods: Using a gradient-based representation of the value function with parametrized randomized policies, and moment neural network functions on the Wasserstein space of probability measures</li>
<li>results: Providing a comprehensive set of numerical results, including diverse examples such as multi-dimensional settings and nonlinear quadratic mean-field control problems with controlled volatility.<details>
<summary>Abstract</summary>
We develop a new policy gradient and actor-critic algorithm for solving mean-field control problems within a continuous time reinforcement learning setting. Our approach leverages a gradient-based representation of the value function, employing parametrized randomized policies. The learning for both the actor (policy) and critic (value function) is facilitated by a class of moment neural network functions on the Wasserstein space of probability measures, and the key feature is to sample directly trajectories of distributions. A central challenge addressed in this study pertains to the computational treatment of an operator specific to the mean-field framework. To illustrate the effectiveness of our methods, we provide a comprehensive set of numerical results. These encompass diverse examples, including multi-dimensional settings and nonlinear quadratic mean-field control problems with controlled volatility.
</details>
<details>
<summary>摘要</summary>
我们开发了一种新的政策梯度和批评算法，用于在连续时间奖励学习设置中解决平均场控制问题。我们的方法利用梯度基于的值函数表示，使用参数化的随机政策。学习actor（政策）和批评（值函数）都是通过一类 moments neural network 函数在沃尔斯特朗空间上进行，并且关键是直接训练分布Trajectory。我们在这种研究中解决了中场框架特有的计算问题。为证明我们的方法的有效性，我们提供了广泛的数据结果，包括多维设置和控制Volatility的非线性quadratic mean-field控制问题。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-for-Early-Dropout-Prediction-on-Healthy-Ageing-Applications"><a href="#Federated-Learning-for-Early-Dropout-Prediction-on-Healthy-Ageing-Applications" class="headerlink" title="Federated Learning for Early Dropout Prediction on Healthy Ageing Applications"></a>Federated Learning for Early Dropout Prediction on Healthy Ageing Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04311">http://arxiv.org/abs/2309.04311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christos Chrysanthos Nikolaidis, Vasileios Perifanis, Nikolaos Pavlidis, Pavlos S. Efraimidis</li>
<li>for: 这种研究旨在提高老年人的生活质量，并帮助运营商提供早期干预。</li>
<li>methods: 这种研究使用了联合机器学习（FML）方法，以降低隐私问题，并允许分布式训练，不需要传输个人数据。</li>
<li>results: 研究结果显示，通过数据选择和类别不均衡处理技术，FML模型在不同客户端和分布式环境中进行训练，可以实现比传统机器学习模型更高的预测精度。<details>
<summary>Abstract</summary>
The provision of social care applications is crucial for elderly people to improve their quality of life and enables operators to provide early interventions. Accurate predictions of user dropouts in healthy ageing applications are essential since they are directly related to individual health statuses. Machine Learning (ML) algorithms have enabled highly accurate predictions, outperforming traditional statistical methods that struggle to cope with individual patterns. However, ML requires a substantial amount of data for training, which is challenging due to the presence of personal identifiable information (PII) and the fragmentation posed by regulations. In this paper, we present a federated machine learning (FML) approach that minimizes privacy concerns and enables distributed training, without transferring individual data. We employ collaborative training by considering individuals and organizations under FML, which models both cross-device and cross-silo learning scenarios. Our approach is evaluated on a real-world dataset with non-independent and identically distributed (non-iid) data among clients, class imbalance and label ambiguity. Our results show that data selection and class imbalance handling techniques significantly improve the predictive accuracy of models trained under FML, demonstrating comparable or superior predictive performance than traditional ML models.
</details>
<details>
<summary>摘要</summary>
提供社会医疗应用程序对老年人群生活质量进行重要的提升，并允许操作人员提供早期干预。准确预测健康年龄应用程序用户退出是直接关系到个人健康状况的。机器学习（ML）算法可以实现高度准确的预测，超出了传统统计方法的cope能力。然而，ML需要大量数据进行训练，这是由于个人标识信息（PII）和法规的分布带来的挑战。在这篇论文中，我们提出了一种联邦机器学习（FML）方法，以减少隐私问题，并允许分布式训练，无需传输个人数据。我们采用了合作训练，考虑了客户端和组织之间的合作，这些模型包括跨设备和跨封包学习场景。我们的方法在实际数据集上进行了评估，该数据集具有非独立和相同分布（non-iid）数据、客户端的类别不均衡和标签抽象。我们的结果表明，数据选择和类别不均衡处理技术可以在FML中提高预测模型的准确性，并达到传统ML模型的相当或更高的预测性能。
</details></li>
</ul>
<hr>
<h2 id="Navigating-Out-of-Distribution-Electricity-Load-Forecasting-during-COVID-19-A-Continual-Learning-Approach-Leveraging-Human-Mobility"><a href="#Navigating-Out-of-Distribution-Electricity-Load-Forecasting-during-COVID-19-A-Continual-Learning-Approach-Leveraging-Human-Mobility" class="headerlink" title="Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility"></a>Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04296">http://arxiv.org/abs/2309.04296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arian Prabowo, Kaixuan Chen, Hao Xue, Subbu Sethuvenkatraman, Flora D. Salim</li>
<li>for: 这个论文是为了提高在COVID-19锁定期间的能源负载预测精度。</li>
<li>methods: 这篇论文使用了 continual learning 技术，包括 FSNet 算法，更新模型以应对新数据，并利用了隐私保护的行人计数数据。</li>
<li>results: 研究结果显示，continual learning 在锁定期间能够精确地预测能源负载，并且在对照方法失败时表现出较好的抗变性。<details>
<summary>Abstract</summary>
In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the second longest total lockdown duration globally during the pandemic. Results underscore the crucial role of continual learning in accurate energy forecasting, particularly during Out-of-Distribution periods. Secondary data such as mobility and temperature provided ancillary support to the primary forecasting model. More importantly, while traditional methods struggled to adapt during lockdowns, models featuring at least online learning demonstrated resilience, with lockdown periods posing fewer challenges once armed with adaptive learning techniques. This study contributes valuable methodologies and insights to the ongoing effort to improve energy load forecasting during future Out-of-Distribution periods.
</details>
<details>
<summary>摘要</summary>
传统深度学习算法中一个关键假设是数据分布在训练和部署期间保持不变。然而，这个假设在面临外部数据（Out-of-Distribution，OOD）时变得问题。COVID-19封锁是一个典型的OOD场景，在这种情况下，数据分布与训练期间所见的数据分布存在很大差异。这篇论文采用了两重策略：利用 continual learning 技术更新模型，并使用隐私保护的行人计数器收集的人流数据。与在线学习不同， continual learning 可以保持过去的经验，并将新数据纳入模型中。这项研究使用 FSNet 算法对澳大利亚 Ме尔本市的 13 座大楼聚集区的实际数据进行应用。结果表明， continual learning 在异常情况下具有精度的能源预测作用，特别是在 OOD 期间。其他 auxiliary 数据，如流体和温度，也为主要预测模型提供了支持。此外，传统方法在封锁期间难以适应，而模型含有在线学习技术则能够更好地适应，封锁期间的挑战较少。这项研究对未来的异常情况下的能源负荷预测提供了有价值的方法和发现。
</details></li>
</ul>
<hr>
<h2 id="Viewing-the-process-of-generating-counterfactuals-as-a-source-of-knowledge-–-Application-to-the-Naive-Bayes-classifier"><a href="#Viewing-the-process-of-generating-counterfactuals-as-a-source-of-knowledge-–-Application-to-the-Naive-Bayes-classifier" class="headerlink" title="Viewing the process of generating counterfactuals as a source of knowledge – Application to the Naive Bayes classifier"></a>Viewing the process of generating counterfactuals as a source of knowledge – Application to the Naive Bayes classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04284">http://arxiv.org/abs/2309.04284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Lemaire, Nathan Le Boudec, Françoise Fessant, Victor Guyomard</li>
<li>for: 这篇论文是为了探讨机器学习算法做出决策的理解方法。</li>
<li>methods: 这篇论文使用了生成对例的方法，并视这个过程为创造知识的源，可以在不同的场景中 reuse。</li>
<li>results: 这篇论文通过对预测器的示例生成过程进行分析，揭示了预测器的有趣性和可能的应用场景。<details>
<summary>Abstract</summary>
There are now many comprehension algorithms for understanding the decisions of a machine learning algorithm. Among these are those based on the generation of counterfactual examples. This article proposes to view this generation process as a source of creating a certain amount of knowledge that can be stored to be used, later, in different ways. This process is illustrated in the additive model and, more specifically, in the case of the naive Bayes classifier, whose interesting properties for this purpose are shown.
</details>
<details>
<summary>摘要</summary>
现在有很多机器学习算法理解决策的方法。其中包括基于生成counterfactual例子的方法。本文提议视这个生成过程为创造一定量的知识，可以被存储并在不同的方式使用。这个过程在加法模型中得到了示例，并在naive Bayes分类器中更加细致地展示了其有趣的性质。Note: "Simplified Chinese" is also known as "Mandarin Chinese" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Spatial-Temporal-Graph-Attention-Fuser-for-Calibration-in-IoT-Air-Pollution-Monitoring-Systems"><a href="#Spatial-Temporal-Graph-Attention-Fuser-for-Calibration-in-IoT-Air-Pollution-Monitoring-Systems" class="headerlink" title="Spatial-Temporal Graph Attention Fuser for Calibration in IoT Air Pollution Monitoring Systems"></a>Spatial-Temporal Graph Attention Fuser for Calibration in IoT Air Pollution Monitoring Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04508">http://arxiv.org/abs/2309.04508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keivan Faghih Niresi, Mengjie Zhao, Hugo Bissig, Henri Baumann, Olga Fink</li>
<li>for: 用于提高物联网污染监测平台中仪器的准确性</li>
<li>methods: 使用图 neural network模型，具体是图注意力网络模块，将探测数组数据融合以提高仪器准确性</li>
<li>results: 经过我们的实验，表明我们的方法可以在物联网污染监测平台中显著提高仪器的准确性<details>
<summary>Abstract</summary>
The use of Internet of Things (IoT) sensors for air pollution monitoring has significantly increased, resulting in the deployment of low-cost sensors. Despite this advancement, accurately calibrating these sensors in uncontrolled environmental conditions remains a challenge. To address this, we propose a novel approach that leverages graph neural networks, specifically the graph attention network module, to enhance the calibration process by fusing data from sensor arrays. Through our experiments, we demonstrate the effectiveness of our approach in significantly improving the calibration accuracy of sensors in IoT air pollution monitoring platforms.
</details>
<details>
<summary>摘要</summary>
互联网物品（IoT）传感器在空气污染监测中的应用已经增加了，从而实现了低成本传感器的部署。然而，在无控的环境下精确地干预这些传感器仍然是一个挑战。为解决这个问题，我们提出了一种新的方法，利用图形神经网络，具体是图形注意力网络模组，将数据库进行融合，以提高传感器的准确性。经过我们的实验，我们证明了我们的方法在IoT空气污染监测平台中具有明显的改善作用，实现了传感器的准确性提高。
</details></li>
</ul>
<hr>
<h2 id="Learning-Zero-Sum-Linear-Quadratic-Games-with-Improved-Sample-Complexity"><a href="#Learning-Zero-Sum-Linear-Quadratic-Games-with-Improved-Sample-Complexity" class="headerlink" title="Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity"></a>Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04272">http://arxiv.org/abs/2309.04272</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wujiduan/zero-sum-lq-games">https://github.com/wujiduan/zero-sum-lq-games</a></li>
<li>paper_authors: Jiduan Wu, Anas Barakat, Ilyas Fatkhullin, Niao He</li>
<li>for: 这篇论文的目的是解决零 SUM 线性quadratic（LQ）游戏中的控制问题，它可以作为风险敏感或Robust控制的动态游戏形式，或者作为多智能机器人学习中的两个竞争者问题的标准设定。</li>
<li>methods: 这篇论文使用的方法是natural policy gradient方法，这种方法具有重要的隐式正则化性质，这使得控制器在学习过程中保持了稳定性和安全性。在没有模型参数知识的情况下，作者提出了第一个 polynomial sample complexity 算法，可以在 $\epsilon$- neighborhood 内达到 Nash 平衡，同时保持隐式正则化性质。</li>
<li>results: 作者提出了一种更加简单的 Zeroth-Order（ZO）算法，可以在同样的假设下提高样本复杂度的下降。主要结果表明，使用单点 ZO 统计器时，样本复杂度为 $\widetilde{\mathcal{O}(\epsilon^{-3})$；使用两点 ZO 统计器时，样本复杂度为 $\widetilde{\mathcal{O}(\epsilon^{-2})$。作者的关键提高来自于更加 Sample-efficient 的嵌套算法设计和 ZO 自然偏导量估计误差的细化控制。<details>
<summary>Abstract</summary>
Zero-sum Linear Quadratic (LQ) games are fundamental in optimal control and can be used (i) as a dynamic game formulation for risk-sensitive or robust control, or (ii) as a benchmark setting for multi-agent reinforcement learning with two competing agents in continuous state-control spaces. In contrast to the well-studied single-agent linear quadratic regulator problem, zero-sum LQ games entail solving a challenging nonconvex-nonconcave min-max problem with an objective function that lacks coercivity. Recently, Zhang et al. discovered an implicit regularization property of natural policy gradient methods which is crucial for safety-critical control systems since it preserves the robustness of the controller during learning. Moreover, in the model-free setting where the knowledge of model parameters is not available, Zhang et al. proposed the first polynomial sample complexity algorithm to reach an $\epsilon$-neighborhood of the Nash equilibrium while maintaining the desirable implicit regularization property. In this work, we propose a simpler nested Zeroth-Order (ZO) algorithm improving sample complexity by several orders of magnitude. Our main result guarantees a $\widetilde{\mathcal{O}(\epsilon^{-3})$ sample complexity under the same assumptions using a single-point ZO estimator. Furthermore, when the estimator is replaced by a two-point estimator, our method enjoys a better $\widetilde{\mathcal{O}(\epsilon^{-2})$ sample complexity. Our key improvements rely on a more sample-efficient nested algorithm design and finer control of the ZO natural gradient estimation error.
</details>
<details>
<summary>摘要</summary>
zero-sum linear quadratic（LQ）游戏是优化控制的基本问题，可以用作风险敏感或Robust控制的动态游戏形式，或者作为多个代理人游戏学习中的两个竞争者的标准设定。与单个代理人的线性quadratic regulator问题不同，zero-sum LQ游戏需要解决一个复杂非几何-非凹的最小值问题， objective function 缺乏凝结性。最近，张等人发现了自然策略强度法的隐式正则化性质，这是关键 для安全控制系统，因为它保持了控制器的稳定性在学习过程中。此外，在没有模型参数知识的场景下，张等人提出了首个 polynomial sample complexity 算法，可以在 достичь $\epsilon$-邻域的 Nash 平衡点上保持愉悦的隐式正则化性质。在这个工作中，我们提出了一种更简单的嵌套Zeroth-Order（ZO）算法，可以提高样本复杂度的数量级。我们的主要结果表明，使用单点ZO estimator时，样本复杂度为 $\widetilde{\mathcal{O}(\epsilon^{-3})$。当使用两点 estimator时，我们的方法可以达到更好的 $\widetilde{\mathcal{O}(\epsilon^{-2})$ 样本复杂度。我们的关键改进来自于更有效的嵌套算法设计和ZO自然幂导 estimation error的更精细控制。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Rate-of-Kernel-Regression-in-Large-Dimensions"><a href="#Optimal-Rate-of-Kernel-Regression-in-Large-Dimensions" class="headerlink" title="Optimal Rate of Kernel Regression in Large Dimensions"></a>Optimal Rate of Kernel Regression in Large Dimensions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04268">http://arxiv.org/abs/2309.04268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weihao Lu, Haobo Zhang, Yicheng Li, Manyun Xu, Qian Lin</li>
<li>for: 本研究的目的是分析适用于大维度数据的内核回归算法（where the sample size $n$ is polynomially depending on the dimension $d$ of the samples, i.e., $n\asymp d^{\gamma}$ for some $\gamma &gt;0$）。</li>
<li>methods: 本研究使用了一种总体工具来Characterize the upper bound and the minimax lower bound of kernel regression for large dimensional data through the Mendelson complexity $\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$ respectively。</li>
<li>results: 当目标函数 falls into the RKHS associated with a (general) inner product model defined on $\mathbb{S}^{d}$时，我们使用了这种工具来显示 That the minimax rate of the excess risk of kernel regression is $n^{-1&#x2F;2}$ when $n\asymp d^{\gamma}$ for $\gamma &#x3D;2, 4, 6, 8, \cdots$. We then further determine the optimal rate of the excess risk of kernel regression for all the $\gamma&gt;0$ and find that the curve of optimal rate varying along $\gamma$ exhibits several new phenomena including the {\it multiple descent behavior} and the {\it periodic plateau behavior}. As an application, For the neural tangent kernel (NTK), we also provide a similar explicit description of the curve of optimal rate. As a direct corollary, we know these claims hold for wide neural networks as well.<details>
<summary>Abstract</summary>
We perform a study on kernel regression for large-dimensional data (where the sample size $n$ is polynomially depending on the dimension $d$ of the samples, i.e., $n\asymp d^{\gamma}$ for some $\gamma >0$ ). We first build a general tool to characterize the upper bound and the minimax lower bound of kernel regression for large dimensional data through the Mendelson complexity $\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$ respectively. When the target function falls into the RKHS associated with a (general) inner product model defined on $\mathbb{S}^{d}$, we utilize the new tool to show that the minimax rate of the excess risk of kernel regression is $n^{-1/2}$ when $n\asymp d^{\gamma}$ for $\gamma =2, 4, 6, 8, \cdots$. We then further determine the optimal rate of the excess risk of kernel regression for all the $\gamma>0$ and find that the curve of optimal rate varying along $\gamma$ exhibits several new phenomena including the {\it multiple descent behavior} and the {\it periodic plateau behavior}. As an application, For the neural tangent kernel (NTK), we also provide a similar explicit description of the curve of optimal rate. As a direct corollary, we know these claims hold for wide neural networks as well.
</details>
<details>
<summary>摘要</summary>
我们进行了一项研究，探讨了隐藏层扩散（Kernel Regression）在高维数据上的性能。我们首先构建了一种通用的工具来Characterize the upper bound and the minimax lower bound of kernel regression for large dimensional data through the Mendelson complexity $\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$ respectively。当目标函数属于隐藏层扩散相关的($\mathbb{S}^{d}$上的)内积模型时，我们利用这种工具来证明隐藏层扩散的最佳误差率为$n^{-1/2}$，当$n\asymp d^{\gamma}$，其中$\gamma = 2, 4, 6, 8, \cdots$。然后，我们进一步确定了隐藏层扩散的最佳误差率，并发现了一些新现象，包括{\it 多重极值行为}和{\it 周期平台行为}。在应用中，我们也提供了类似的Explicit description of the curve of optimal rate for the neural tangent kernel (NTK)。这意味着这些结论也适用于宽频率神经网络。
</details></li>
</ul>
<hr>
<h2 id="Generating-drawdown-realistic-financial-price-paths-using-path-signatures"><a href="#Generating-drawdown-realistic-financial-price-paths-using-path-signatures" class="headerlink" title="Generating drawdown-realistic financial price paths using path signatures"></a>Generating drawdown-realistic financial price paths using path signatures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04507">http://arxiv.org/abs/2309.04507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emiel Lemahieu, Kris Boudt, Maarten Wyns</li>
<li>for: 这个论文的目的是提出一种基于机器学习的新方法，用于模拟金融价格数据序列中的下跌。</li>
<li>methods: 这种方法使用了变分自动编码器生成模型，并使用下跌重建loss函数。为了解决数值复杂性和非 diferenciabilidad问题， authors使用了下跌函数的 linear 函数 approximation，并证明了这种近似的正则性和一致性。</li>
<li>results: 通过使用这种方法， authors 得到了一系列具有实际下跌特征的金融价格数据序列。他们还通过对混合股票、债券、地产和商品资产的数据进行数值实验，并证明了这种方法的有效性。<details>
<summary>Abstract</summary>
A novel generative machine learning approach for the simulation of sequences of financial price data with drawdowns quantifiably close to empirical data is introduced. Applications such as pricing drawdown insurance options or developing portfolio drawdown control strategies call for a host of drawdown-realistic paths. Historical scenarios may be insufficient to effectively train and backtest the strategy, while standard parametric Monte Carlo does not adequately preserve drawdowns. We advocate a non-parametric Monte Carlo approach combining a variational autoencoder generative model with a drawdown reconstruction loss function. To overcome issues of numerical complexity and non-differentiability, we approximate drawdown as a linear function of the moments of the path, known in the literature as path signatures. We prove the required regularity of drawdown function and consistency of the approximation. Furthermore, we obtain close numerical approximations using linear regression for fractional Brownian and empirical data. We argue that linear combinations of the moments of a path yield a mathematically non-trivial smoothing of the drawdown function, which gives one leeway to simulate drawdown-realistic price paths by including drawdown evaluation metrics in the learning objective. We conclude with numerical experiments on mixed equity, bond, real estate and commodity portfolios and obtain a host of drawdown-realistic paths.
</details>
<details>
<summary>摘要</summary>
新的生成机器学习方法 для模拟金融价格数据序列，包括Drawdown，靠近实际数据的量化是介绍的。应用如评估Drawdown保险选项或开发 portefolio Drawdown控制策略，需要一系列Drawdown具有实际性的路径。历史场景可能不够充分用于训练和测试策略，而标准 Parametric Monte Carlo 方法不能够准确保持Drawdown。我们建议一种非 Parametric Monte Carlo 方法，结合变分自动编码生成模型和 Drawdown 重建损失函数。为了解决计算复杂性和不 diferenciabilidad 问题，我们 approximates Drawdown 为路径特征的线性函数，知道在 литературе como path signatures。我们证明 Drawdown 函数的必要的准确性和拟合的一致性。此外，我们通过线性回归获得了 fractional Brownian 和实际数据的准确近似值。我们 argue that linear combinations of path moments yield a mathematically non-trivial smoothing of the drawdown function, which allows one to simulate drawdown-realistic price paths by including drawdown evaluation metrics in the learning objective. We conclude with numerical experiments on mixed equity, bond, real estate and commodity portfolios and obtain a host of drawdown-realistic paths.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Distributed-Kernel-Ridge-Regression-A-Feasible-Distributed-Learning-Scheme-for-Data-Silos"><a href="#Adaptive-Distributed-Kernel-Ridge-Regression-A-Feasible-Distributed-Learning-Scheme-for-Data-Silos" class="headerlink" title="Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos"></a>Adaptive Distributed Kernel Ridge Regression: A Feasible Distributed Learning Scheme for Data Silos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04236">http://arxiv.org/abs/2309.04236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Wang, Xiaotong Liu, Shao-Bo Lin, Ding-Xuan Zhou</li>
<li>for: 解决数据隔壳问题，帮助不同组织共同学习同一类数据，提高决策效果。</li>
<li>methods: 基于分治的分布式学习，实现参数自主选择、隐私保障和合作必要性。</li>
<li>results:  theoretically和实验两种方式证明AdaDKRR的可行性和有效性，并在几个应用领域中表现出优于其他分布式学习方法。<details>
<summary>Abstract</summary>
Data silos, mainly caused by privacy and interoperability, significantly constrain collaborations among different organizations with similar data for the same purpose. Distributed learning based on divide-and-conquer provides a promising way to settle the data silos, but it suffers from several challenges, including autonomy, privacy guarantees, and the necessity of collaborations. This paper focuses on developing an adaptive distributed kernel ridge regression (AdaDKRR) by taking autonomy in parameter selection, privacy in communicating non-sensitive information, and the necessity of collaborations in performance improvement into account. We provide both solid theoretical verification and comprehensive experiments for AdaDKRR to demonstrate its feasibility and effectiveness. Theoretically, we prove that under some mild conditions, AdaDKRR performs similarly to running the optimal learning algorithms on the whole data, verifying the necessity of collaborations and showing that no other distributed learning scheme can essentially beat AdaDKRR under the same conditions. Numerically, we test AdaDKRR on both toy simulations and two real-world applications to show that AdaDKRR is superior to other existing distributed learning schemes. All these results show that AdaDKRR is a feasible scheme to defend against data silos, which are highly desired in numerous application regions such as intelligent decision-making, pricing forecasting, and performance prediction for products.
</details>
<details>
<summary>摘要</summary>
“数据堡垒”，主要由隐私和兼容性所引起，对不同组织之间的合作带来了 significiant 限制。分布式学习基于分治提供了一个有 promise 的解决方案，但它面临着一些挑战，包括自主、隐私保证和合作的必要性。本文关注于基于自适应分布式内核ridge regression（AdaDKRR）的开发，包括自主在参数选择、隐私在交换非敏感信息以及合作的必要性。我们提供了坚实的理论验证和实验来证明 AdaDKRR 的可行性和有效性。理论上，我们证明在某些轻量级条件下，AdaDKRR 与在整个数据上运行最佳学习算法相当，证明了合作的必要性，并表明其他分布式学习方案无法 Essentially 超越 AdaDKRR 在同样的条件下。数字上，我们测试了 AdaDKRR 在实验和两个真实应用中，并证明它在其他已有分布式学习方案的同时显著 superior。这些结果表明 AdaDKRR 是一种有效的数据堡垒防御方案，可以在许多应用领域中实现更好的决策、估价和产品性能预测。
</details></li>
</ul>
<hr>
<h2 id="Offline-Recommender-System-Evaluation-under-Unobserved-Confounding"><a href="#Offline-Recommender-System-Evaluation-under-Unobserved-Confounding" class="headerlink" title="Offline Recommender System Evaluation under Unobserved Confounding"></a>Offline Recommender System Evaluation under Unobserved Confounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04222">http://arxiv.org/abs/2309.04222</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/olivierjeunen/confounding-consequences-2023">https://github.com/olivierjeunen/confounding-consequences-2023</a></li>
<li>paper_authors: Olivier Jeunen, Ben London</li>
<li>for: This paper focuses on the problem of unobserved confounders in off-policy estimation (OPE) methods for recommender systems.</li>
<li>methods: The paper uses policy-based estimators, which learn logging propensities from logged data, and demonstrates the statistical bias that arises due to confounding.</li>
<li>results: The paper shows that existing diagnostics are unable to uncover the bias caused by confounding, and that naive propensity estimation can lead to severely biased metric estimates.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文关注了Off-Policy Estimation（OPE）方法在推荐系统中的问题，即未观察的偏见问题。</li>
<li>methods: 论文使用了政策基于的估计器，从日志数据中学习 logging propensities，并证明了因为偏见而导致的统计偏误。</li>
<li>results: 论文显示了现有的 диагностиcs无法检测到偏见导致的偏误，并且简单的 propensity estimation 可能会导致严重的偏误度量估计。<details>
<summary>Abstract</summary>
Off-Policy Estimation (OPE) methods allow us to learn and evaluate decision-making policies from logged data. This makes them an attractive choice for the offline evaluation of recommender systems, and several recent works have reported successful adoption of OPE methods to this end. An important assumption that makes this work is the absence of unobserved confounders: random variables that influence both actions and rewards at data collection time. Because the data collection policy is typically under the practitioner's control, the unconfoundedness assumption is often left implicit, and its violations are rarely dealt with in the existing literature.   This work aims to highlight the problems that arise when performing off-policy estimation in the presence of unobserved confounders, specifically focusing on a recommendation use-case. We focus on policy-based estimators, where the logging propensities are learned from logged data. We characterise the statistical bias that arises due to confounding, and show how existing diagnostics are unable to uncover such cases. Because the bias depends directly on the true and unobserved logging propensities, it is non-identifiable. As the unconfoundedness assumption is famously untestable, this becomes especially problematic. This paper emphasises this common, yet often overlooked issue. Through synthetic data, we empirically show how na\"ive propensity estimation under confounding can lead to severely biased metric estimates that are allowed to fly under the radar. We aim to cultivate an awareness among researchers and practitioners of this important problem, and touch upon potential research directions towards mitigating its effects.
</details>
<details>
<summary>摘要</summary>
偏离政策估计（OPE）技术可以从记录数据中学习和评估决策策略。这使得它们在线上评估推荐系统中非常吸引人，而一些最近的研究也已经成功地采用了OPE技术。一个重要的假设是不存在隐藏的隐变量：在数据采集时影响行为和奖励的随机变量。因为数据采集策略通常是实际控制在手中，因此这个假设通常被遗弃，而其违反也 rarely 被文献中讨论。  这项工作想要强调在隐藏隐变量的存在下进行偏离政策估计时出现的问题，特别是在推荐用例中。我们关注policy-based estimators，其中logging propensities从记录数据中学习。我们描述了由隐藏隐变量引起的统计偏差，并显示了现有的 диагностиcs无法检测这些情况。因为偏差直接关于真实的和未观察 logging propensities，因此是不可识别的。正如无法测试的无关联假设，这就变得特别问题。这篇论文想要吸引研究人员和实践者对这种常见 yet 受过гля的问题进行注意。我们通过 sintetic data  empirically 示出了不经过propensity estimation 的偏离metric estimates可以导致严重偏差。我们的目标是在研究人员和实践者中培养对这个重要问题的意识，并触及可能的研究方向以解决其影响。
</details></li>
</ul>
<hr>
<h2 id="Concomitant-Group-Testing"><a href="#Concomitant-Group-Testing" class="headerlink" title="Concomitant Group Testing"></a>Concomitant Group Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04221">http://arxiv.org/abs/2309.04221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thach V. Bui, Jonathan Scarlett</li>
<li>For: The paper addresses the Concomitant Group Testing (ConcGT) problem, which involves identifying multiple disjoint semi-defective sets in a population using as few tests as possible.* Methods: The paper derives a variety of algorithms for ConcGT, including deterministic and randomized algorithms with different levels of adaptivity.* Results: The paper shows that the proposed algorithms are order-optimal in broad scaling regimes and improve significantly over baseline results.<details>
<summary>Abstract</summary>
In this paper, we introduce a variation of the group testing problem capturing the idea that a positive test requires a combination of multiple ``types'' of item. Specifically, we assume that there are multiple disjoint \emph{semi-defective sets}, and a test is positive if and only if it contains at least one item from each of these sets. The goal is to reliably identify all of the semi-defective sets using as few tests as possible, and we refer to this problem as \textit{Concomitant Group Testing} (ConcGT). We derive a variety of algorithms for this task, focusing primarily on the case that there are two semi-defective sets. Our algorithms are distinguished by (i) whether they are deterministic (zero-error) or randomized (small-error), and (ii) whether they are non-adaptive, fully adaptive, or have limited adaptivity (e.g., 2 or 3 stages). Both our deterministic adaptive algorithm and our randomized algorithms (non-adaptive or limited adaptivity) are order-optimal in broad scaling regimes of interest, and improve significantly over baseline results that are based on solving a more general problem as an intermediate step (e.g., hypergraph learning).
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种变种的群测试问题，把具有多个“类型”的项目组合的想法带入。特别是，我们假设有多个独立的半异常集，而测试是否阳性，则需要至少有一个项目来自每个半异常集。我们的目标是通过最少的测试来可靠地检测所有半异常集，我们称之为“并行群测试”（ConcGT）。我们 derivated 多种算法来解决这个问题，主要是在两个半异常集时。我们的算法可以分为（i）确定性（零错）或随机化（小错），以及（ii）非适应、完全适应或有限适应（例如，2或3个阶段）。我们的确定性适应算法和我们的随机算法（非适应或有限适应）在广泛的扩展 режиmen of interest 中都是最佳的，并在基eline结果（例如，超граápher学习）上进行了显著改进。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanations-via-Locally-guided-Sequential-Algorithmic-Recourse"><a href="#Counterfactual-Explanations-via-Locally-guided-Sequential-Algorithmic-Recourse" class="headerlink" title="Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse"></a>Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04211">http://arxiv.org/abs/2309.04211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward A. Small, Jeffrey N. Clark, Christopher J. McWilliams, Kacper Sokol, Jeffrey Chan, Flora D. Salim, Raul Santos-Rodriguez</li>
<li>for: 这篇论文旨在提供一种可操作的算法抗议方法，使人工智能系统更加可解释。</li>
<li>methods: 这篇论文使用了算法驱动的方法，包括梯度驱动方法，但这些方法无法保证抗议的可行性和对抗攻击。</li>
<li>results: 这篇论文提出了一种名为LocalFACE的模型无关技术，可以生成可操作的对抗性解释，并保护用户隐私和模型安全。<details>
<summary>Abstract</summary>
Counterfactuals operationalised through algorithmic recourse have become a powerful tool to make artificial intelligence systems explainable. Conceptually, given an individual classified as y -- the factual -- we seek actions such that their prediction becomes the desired class y' -- the counterfactual. This process offers algorithmic recourse that is (1) easy to customise and interpret, and (2) directly aligned with the goals of each individual. However, the properties of a "good" counterfactual are still largely debated; it remains an open challenge to effectively locate a counterfactual along with its corresponding recourse. Some strategies use gradient-driven methods, but these offer no guarantees on the feasibility of the recourse and are open to adversarial attacks on carefully created manifolds. This can lead to unfairness and lack of robustness. Other methods are data-driven, which mostly addresses the feasibility problem at the expense of privacy, security and secrecy as they require access to the entire training data set. Here, we introduce LocalFACE, a model-agnostic technique that composes feasible and actionable counterfactual explanations using locally-acquired information at each step of the algorithmic recourse. Our explainer preserves the privacy of users by only leveraging data that it specifically requires to construct actionable algorithmic recourse, and protects the model by offering transparency solely in the regions deemed necessary for the intervention.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Counterfactuals operationalised through algorithmic recourse have become a powerful tool to make artificial intelligence systems explainable. Conceptually, given an individual classified as y -- the factual -- we seek actions such that their prediction becomes the desired class y' -- the counterfactual. This process offers algorithmic recourse that is (1) easy to customise and interpret, and (2) directly aligned with the goals of each individual. However, the properties of a "good" counterfactual are still largely debated; it remains an open challenge to effectively locate a counterfactual along with its corresponding recourse. Some strategies use gradient-driven methods, but these offer no guarantees on the feasibility of the recourse and are open to adversarial attacks on carefully created manifolds. This can lead to unfairness and lack of robustness. Other methods are data-driven, which mostly addresses the feasibility problem at the expense of privacy, security and secrecy as they require access to the entire training data set. Here, we introduce LocalFACE, a model-agnostic technique that composes feasible and actionable counterfactual explanations using locally-acquired information at each step of the algorithmic recourse. Our explainer preserves the privacy of users by only leveraging data that it specifically requires to construct actionable algorithmic recourse, and protects the model by offering transparency solely in the regions deemed necessary for the intervention."中文翻译：counterfactuals通过算法救济实现了人工智能系统的解释。从概念上说，对于被分类为y（事实）的个人，我们寻找一些行动，使其预测变为欲要的类y'（ counterfactual）。这个过程提供了一种简单易 interpretable的算法救济，其中（1）易于自定义和解释，（2）与每个个人的目标直接对应。然而，“好”的counterfactual的性质仍然是一个开放的挑战，尚未能够有效地找到counterfactual和其相应的救济。一些策略使用梯度驱动方法，但这些方法不能保证救济的可行性，并且容易受到伪装的扰动。这可能导致不公正和稳定性问题。其他方法是数据驱动的，它主要解决了可行性问题，但是在付出了隐私、安全和机密的代价。在这里，我们介绍LocalFACE，一种模型无关的技术，可以在每个步骤中使用当地获得的信息组合可行的counterfactual解释。我们的解释保持用户隐私，仅使用特定于构建行动的数据来构建可行的算法救济，并且保护模型，仅在必要的区域提供透明度。
</details></li>
</ul>
<hr>
<h2 id="COVID-19-Detection-System-A-Comparative-Analysis-of-System-Performance-Based-on-Acoustic-Features-of-Cough-Audio-Signals"><a href="#COVID-19-Detection-System-A-Comparative-Analysis-of-System-Performance-Based-on-Acoustic-Features-of-Cough-Audio-Signals" class="headerlink" title="COVID-19 Detection System: A Comparative Analysis of System Performance Based on Acoustic Features of Cough Audio Signals"></a>COVID-19 Detection System: A Comparative Analysis of System Performance Based on Acoustic Features of Cough Audio Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04505">http://arxiv.org/abs/2309.04505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asmaa Shati, Ghulam Mubashar Hassan, Amitava Datta<br>for: 这项研究旨在提出一种高效的COVID-19检测系统，通过分析咳声特征来识别患COVID-19人群。methods: 该研究使用了三种特征提取方法：Mel Frequency Cepstral Coefficients (MFCC)、Chroma和Spectral Contrast特征，并应用了支持向量机(SVM)和多层感知网络(MLP)两种机器学习算法。results: 研究结果表明，使用这三种特征提取方法和SVM和MLP两种机器学习算法可以提高COVID-19检测的准确率，并达到了现有技术的国际先进水平。<details>
<summary>Abstract</summary>
A wide range of respiratory diseases, such as cold and flu, asthma, and COVID-19, affect people's daily lives worldwide. In medical practice, respiratory sounds are widely used in medical services to diagnose various respiratory illnesses and lung disorders. The traditional diagnosis of such sounds requires specialized knowledge, which can be costly and reliant on human expertise. Recently, cough audio recordings have been used to automate the process of detecting respiratory conditions. This research aims to examine various acoustic features that enhance the performance of machine learning (ML) models in detecting COVID-19 from cough signals. This study investigates the efficacy of three feature extraction techniques, including Mel Frequency Cepstral Coefficients (MFCC), Chroma, and Spectral Contrast features, on two ML algorithms, Support Vector Machine (SVM) and Multilayer Perceptron (MLP), and thus proposes an efficient COVID-19 detection system. The proposed system produces a practical solution and demonstrates higher state-of-the-art classification performance on COUGHVID and Virufy datasets for COVID-19 detection.
</details>
<details>
<summary>摘要</summary>
各种呼吸疾病，如感冒和流感、asma和 COVID-19，对全球人群的日常生活产生了深远的影响。在医疗实践中，呼吸音被广泛使用，以诊断各种呼吸疾病和肺脏疾患。传统的诊断方法需要专业知识，这可能是成本高昂的和人工智能依赖的。近些年，喷气音记录被用来自动诊断呼吸疾病。本研究旨在检验不同的音频特征，以提高机器学习（ML）模型在检测 COVID-19 的能力。本研究 investigate了 MFCC、Chroma 和 Spectral Contrast 等三种特征提取技术，并将其应用于 SVM 和 MLP 两种 ML 算法，因此提出了一种高效的 COVID-19 检测系统。该系统提供了实用的解决方案，并在 COUGHVID 和 Virufy 数据集上达到了高于当前领先水平的分类性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Mitigating-Architecture-Overfitting-in-Dataset-Distillation"><a href="#Towards-Mitigating-Architecture-Overfitting-in-Dataset-Distillation" class="headerlink" title="Towards Mitigating Architecture Overfitting in Dataset Distillation"></a>Towards Mitigating Architecture Overfitting in Dataset Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04195">http://arxiv.org/abs/2309.04195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuyang Zhong, Chen Liu</li>
<li>for: 提高 neural network 使用有限训练数据时的性能。</li>
<li>methods: 提出了一系列 Architecture 设计和训练方案，可以在不同的网络架构下提高分布式数据 Synthesized 的性能。</li>
<li>results: 经过广泛的实验，我们的方法在不同的情况下（含不同的压缩数据大小）都能够与现有方法相比或者提高性能。<details>
<summary>Abstract</summary>
Dataset distillation methods have demonstrated remarkable performance for neural networks trained with very limited training data. However, a significant challenge arises in the form of architecture overfitting: the distilled training data synthesized by a specific network architecture (i.e., training network) generates poor performance when trained by other network architectures (i.e., test networks). This paper addresses this issue and proposes a series of approaches in both architecture designs and training schemes which can be adopted together to boost the generalization performance across different network architectures on the distilled training data. We conduct extensive experiments to demonstrate the effectiveness and generality of our methods. Particularly, across various scenarios involving different sizes of distilled data, our approaches achieve comparable or superior performance to existing methods when training on the distilled data using networks with larger capacities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Compositional-Learning-of-Visually-Grounded-Concepts-Using-Reinforcement"><a href="#Compositional-Learning-of-Visually-Grounded-Concepts-Using-Reinforcement" class="headerlink" title="Compositional Learning of Visually-Grounded Concepts Using Reinforcement"></a>Compositional Learning of Visually-Grounded Concepts Using Reinforcement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04504">http://arxiv.org/abs/2309.04504</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haidiazaman/rl-concept-learning-project">https://github.com/haidiazaman/rl-concept-learning-project</a></li>
<li>paper_authors: Zijun Lin, Haidi Azaman, M Ganesh Kumar, Cheston Tan</li>
<li>for:  investigate how deep RL agents learn and compose color-shape based combinatorial instructions to solve novel combinations in a spatial navigation task</li>
<li>methods: 使用深度强化学习代理人在多集数回合中训练，并研究代理人是否可以通过冻结文本编码器（例如CLIP、BERT）来学习单词组合，以及在不同颜色和形状概念上进行预训练的影响</li>
<li>results:  agents pretrained on concept and compositional learning achieve significantly higher reward when evaluated zero-shot on novel color-shape1-shape2 visual object combinations, and can solve unseen combinations of instructions with fewer episodes.<details>
<summary>Abstract</summary>
Deep reinforcement learning agents need to be trained over millions of episodes to decently solve navigation tasks grounded to instructions. Furthermore, their ability to generalize to novel combinations of instructions is unclear. Interestingly however, children can decompose language-based instructions and navigate to the referred object, even if they have not seen the combination of queries prior. Hence, we created three 3D environments to investigate how deep RL agents learn and compose color-shape based combinatorial instructions to solve novel combinations in a spatial navigation task. First, we explore if agents can perform compositional learning, and whether they can leverage on frozen text encoders (e.g. CLIP, BERT) to learn word combinations in fewer episodes. Next, we demonstrate that when agents are pretrained on the shape or color concepts separately, they show a 20 times decrease in training episodes needed to solve unseen combinations of instructions. Lastly, we show that agents pretrained on concept and compositional learning achieve significantly higher reward when evaluated zero-shot on novel color-shape1-shape2 visual object combinations. Overall, our results highlight the foundations needed to increase an agent's proficiency in composing word groups through reinforcement learning and its ability for zero-shot generalization to new combinations.
</details>
<details>
<summary>摘要</summary>
深度强化学学习代理需要在数百万集 episodes 中训练，以解决基于指令的导航任务。然而，其能够总结 novel 的指令组合仍然不清楚。然而，孩子们可以将语言基于的指令分解成组合，并导航到指定的 объек。因此，我们创建了三个3D环境，以研究深度RL代理如何学习和组合颜色形状基本指令，以解决 novel 的组合。首先，我们 investigate  whether agents can perform compositional learning，并可以使用冻结的文本编码器（例如CLIP、BERT）来学习单词组合。然后，我们示出了在 agents 在受训练集中减少了 20 倍的集数可以解决未看过的指令组合。最后，我们表明在概念学习和组合学习中训练的代理可以在零时执行中达到更高的奖励。总的来说，我们的结果阐明了如何通过强化学习提高代理的单词组合能力，以及这种能力在零时执行中的普适性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Prototype-Patient-Representations-with-Feature-Missing-Aware-Calibration-to-Mitigate-EHR-Data-Sparsity"><a href="#Leveraging-Prototype-Patient-Representations-with-Feature-Missing-Aware-Calibration-to-Mitigate-EHR-Data-Sparsity" class="headerlink" title="Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity"></a>Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04160">http://arxiv.org/abs/2309.04160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinghao Zhu, Zixiang Wang, Long He, Shiyun Xie, Zixi Chen, Jingkun An, Liantao Ma, Chengwei Pan</li>
<li>for: 这个研究的目的是对电子健康纪录（EHR）数据进行预测模型建立，并解决EHR数据的稀畴特性带来的挑战。</li>
<li>methods: 本研究使用 indirect imputation，通过使用相似 patient 的原型表示来获得更为紧密的嵌入。此外，我们还提出了一个新的患者相似度量表，考虑到缺失特征的状态，以确保评估不仅基于可能不准确的填充值。</li>
<li>results: 我们的方法在 MIMIC-III 和 MIMIC-IV 数据集上预测医院死亡结果 task 上达到了 statistically significant 的改善，较前一代的 EHR- focused 模型。代码在 \url{<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/SparseEHR%7D">https://anonymous.4open.science/r/SparseEHR}</a> 上公开，以便重现。<details>
<summary>Abstract</summary>
Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measuring similar patients, our approach designs a feature confidence learner module. This module is sensitive to the missing feature status, enabling the model to better judge the reliability of each feature. Moreover, we propose a novel patient similarity metric that takes feature confidence into account, ensuring that evaluations are not based merely on potentially inaccurate imputed values. Consequently, our work captures dense prototype patient representations with feature-missing-aware calibration process. Comprehensive experiments demonstrate that designed model surpasses established EHR-focused models with a statistically significant improvement on MIMIC-III and MIMIC-IV datasets in-hospital mortality outcome prediction task. The code is publicly available at \url{https://anonymous.4open.science/r/SparseEHR} to assure the reproducibility.
</details>
<details>
<summary>摘要</summary>
电子健康记录（EHR）数据经常具有稀畴特征，这会对预测模型造成挑战。目前的直接填充方法，如矩阵填充方法，基于参照相似的行或列来完成 Raw 缺失数据，而不能区分实际值和拟合值。因此，模型可能会意外地包含不相关或误导的信息，从而降低下游性能。而一些方法尝试通过重新调整或增强 EHR 嵌入来解决这些问题，但它们经常偏好拟合特征。这种偏好可能会引入偏见或错误到模型中。为了解决这些问题，我们的工作使用间接填充，利用相似病人的原型表示来获得密集的嵌入。Recognizing the limitation that missing features are typically treated the same as present ones when measuring similar patients, our approach designs a feature confidence learner module. This module is sensitive to the missing feature status, enabling the model to better judge the reliability of each feature. Moreover, we propose a novel patient similarity metric that takes feature confidence into account, ensuring that evaluations are not based merely on potentially inaccurate imputed values. Consequently, our work captures dense prototype patient representations with feature-missing-aware calibration process. Comprehensive experiments demonstrate that our designed model surpasses established EHR-focused models with a statistically significant improvement on MIMIC-III and MIMIC-IV datasets in-hospital mortality outcome prediction task. 我们的代码公开可用于 \url{https://anonymous.4open.science/r/SparseEHR}，以确保可重复性。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-attacks-on-hybrid-classical-quantum-Deep-Learning-models-for-Histopathological-Cancer-Detection"><a href="#Adversarial-attacks-on-hybrid-classical-quantum-Deep-Learning-models-for-Histopathological-Cancer-Detection" class="headerlink" title="Adversarial attacks on hybrid classical-quantum Deep Learning models for Histopathological Cancer Detection"></a>Adversarial attacks on hybrid classical-quantum Deep Learning models for Histopathological Cancer Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06377">http://arxiv.org/abs/2309.06377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Biswaraj Baral, Reek Majumdar, Bhavika Bhalgamiya, Taposh Dutta Roy</li>
<li>for:  histopathological cancer detection</li>
<li>methods:  hybrid classical-quantum Deep Learning models ( incluiding transfer learning strategy and variational quantum circuits)</li>
<li>results:  better accuracy than classical image classification models under adversarial attacks<details>
<summary>Abstract</summary>
We present an effective application of quantum machine learning in histopathological cancer detection. The study here emphasizes two primary applications of hybrid classical-quantum Deep Learning models. The first application is to build a classification model for histopathological cancer detection using the quantum transfer learning strategy. The second application is to test the performance of this model for various adversarial attacks. Rather than using a single transfer learning model, the hybrid classical-quantum models are tested using multiple transfer learning models, especially ResNet18, VGG-16, Inception-v3, and AlexNet as feature extractors and integrate it with several quantum circuit-based variational quantum circuits (VQC) with high expressibility. As a result, we provide a comparative analysis of classical models and hybrid classical-quantum transfer learning models for histopathological cancer detection under several adversarial attacks. We compared the performance accuracy of the classical model with the hybrid classical-quantum model using pennylane default quantum simulator. We also observed that for histopathological cancer detection under several adversarial attacks, Hybrid Classical-Quantum (HCQ) models provided better accuracy than classical image classification models.
</details>
<details>
<summary>摘要</summary>
我们提出了一种有效的量子机器学习应用于 histopathological cancer detection。这种研究主要强调两个主要应用：首先，建立一个基于混合古典-量子深度学习模型的分类模型，用于 histopathological cancer detection。其次，测试这个模型对各种敌意攻击的性能。而不是使用单一的传输学习模型，我们测试了多个传输学习模型，包括 ResNet18、VGG-16、Inception-v3 和 AlexNet 作为特征提取器，并将它们与多种量子电路基本变量量子圈（VQC）结合。因此，我们提供了对 классические模型和混合古典-量子传输学习模型的比较分析，并对 histopathological cancer detection 下多种敌意攻击的性能进行了比较。我们使用 pennylane 默认量子 simulate 器进行了比较。我们发现，对 histopathological cancer detection 下多种敌意攻击，混合古典-量子（HCQ）模型提供了更高的准确率，而非 классиical image classification 模型。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Method-for-Sensitivity-Enhancement-of-Deuterium-Metabolic-Imaging-DMI"><a href="#A-Deep-Learning-Method-for-Sensitivity-Enhancement-of-Deuterium-Metabolic-Imaging-DMI" class="headerlink" title="A Deep Learning Method for Sensitivity Enhancement of Deuterium Metabolic Imaging (DMI)"></a>A Deep Learning Method for Sensitivity Enhancement of Deuterium Metabolic Imaging (DMI)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04100">http://arxiv.org/abs/2309.04100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Dong, Henk M. De Feyter, Monique A. Thomas, Robin A. de Graaf, James S. Duncan</li>
<li>for: 提高Deuterium Metabolic Imaging（DMI）的敏感度，从而提高肉眼图像的分辨率和扫描时间。</li>
<li>methods: 使用 convolutional neural network（CNN）来估计2H-标记物质浓度从低SNR和扭曲的DMI FIDs。通过MRI基于的边缘保持正则化来进一步提高估计精度。</li>
<li>results: PRECISE-DMI可以视觉改善低SNR数据中的代谢图像，并提供更高的精度 than标准的福rier重建。对于在鼠脑肿瘤模型中获得的数据进行处理，可以获得更高的空间分辨率（从&gt;8到2 $\mu$L）或更短的扫描时间（从32到4分），并且可以更正确地测量2H-标记的酮氧酸和γ-酮氧酸+γ-酮氧酸含量。但是，对于激活过度的边缘保持正则化，可能会产生结果的不准确性。<details>
<summary>Abstract</summary>
Purpose: Common to most MRSI techniques, the spatial resolution and the minimal scan duration of Deuterium Metabolic Imaging (DMI) are limited by the achievable SNR. This work presents a deep learning method for sensitivity enhancement of DMI.   Methods: A convolutional neural network (CNN) was designed to estimate the 2H-labeled metabolite concentrations from low SNR and distorted DMI FIDs. The CNN was trained with synthetic data that represent a range of SNR levels typically encountered in vivo. The estimation precision was further improved by fine-tuning the CNN with MRI-based edge-preserving regularization for each DMI dataset. The proposed processing method, PReserved Edge ConvolutIonal neural network for Sensitivity Enhanced DMI (PRECISE-DMI), was applied to simulation studies and in vivo experiments to evaluate the anticipated improvements in SNR and investigate the potential for inaccuracies.   Results: PRECISE-DMI visually improved the metabolic maps of low SNR datasets, and quantitatively provided higher precision than the standard Fourier reconstruction. Processing of DMI data acquired in rat brain tumor models resulted in more precise determination of 2H-labeled lactate and glutamate + glutamine levels, at increased spatial resolution (from >8 to 2 $\mu$L) or shortened scan time (from 32 to 4 min) compared to standard acquisitions. However, rigorous SD-bias analyses showed that overuse of the edge-preserving regularization can compromise the accuracy of the results.   Conclusion: PRECISE-DMI allows a flexible trade-off between enhancing the sensitivity of DMI and minimizing the inaccuracies. With typical settings, the DMI sensitivity can be improved by 3-fold while retaining the capability to detect local signal variations.
</details>
<details>
<summary>摘要</summary>
目的：大多数MRSI技术的空间分解能力和最小扫描时间受到可 achievable SNR 的限制。这项工作提出了一种基于深度学习的敏感度提高方法 дляMRSI。方法：我们设计了一个卷积神经网络（CNN）来估算2H-标记的代谢物浓度从低SNR和扭曲的DMI FID中。CNN 被训练使用生成的数据，表征了通常在生物体内遇到的SNR水平范围。为了进一步提高估算精度，我们对每个DMI数据集使用MRI基于Edge-preserving 的正则化进行细调。我们称这种处理方法为PRECISE-DMI。结果：PRECISE-DMI可以视觉提高低SNR数据中的代谢图，并量测更高精度 than标准傅立叶重建。对于在鼠 brain tumor 模型中获得的数据进行处理，可以获得更高的空间分解能力（从>8到2 $\mu$L）或更短的扫描时间（从32到4分），相比标准采集。然而，我们通过严格的SD-偏移分析发现，过度使用Edge-preserving 正则化可能会伤害结果的准确性。结论：PRECISE-DMI允许在提高MRSI敏感度的同时，也可以保持检测地方信号变化的能力。通常情况下，可以通过调整参数来实现3倍的MRSI敏感度提高，而无需妥协准确性。
</details></li>
</ul>
<hr>
<h2 id="Modeling-Recommender-Ecosystems-Research-Challenges-at-the-Intersection-of-Mechanism-Design-Reinforcement-Learning-and-Generative-Models"><a href="#Modeling-Recommender-Ecosystems-Research-Challenges-at-the-Intersection-of-Mechanism-Design-Reinforcement-Learning-and-Generative-Models" class="headerlink" title="Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models"></a>Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06375">http://arxiv.org/abs/2309.06375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Craig Boutilier, Martin Mladenov, Guy Tennenholtz</li>
<li>for: 提高推荐系统的长期价值和生态系统健康</li>
<li>methods: 使用奖励学习优化推荐策略，使用社会选择方法讨论不同actor的利益，减少信息不均衡，更好地模型用户和项目提供者的行为</li>
<li>results: 提出一个概念框架，并提出许多跨学科研究挑战<details>
<summary>Abstract</summary>
Modern recommender systems lie at the heart of complex ecosystems that couple the behavior of users, content providers, advertisers, and other actors. Despite this, the focus of the majority of recommender research -- and most practical recommenders of any import -- is on the local, myopic optimization of the recommendations made to individual users. This comes at a significant cost to the long-term utility that recommenders could generate for its users. We argue that explicitly modeling the incentives and behaviors of all actors in the system -- and the interactions among them induced by the recommender's policy -- is strictly necessary if one is to maximize the value the system brings to these actors and improve overall ecosystem "health". Doing so requires: optimization over long horizons using techniques such as reinforcement learning; making inevitable tradeoffs in the utility that can be generated for different actors using the methods of social choice; reducing information asymmetry, while accounting for incentives and strategic behavior, using the tools of mechanism design; better modeling of both user and item-provider behaviors by incorporating notions from behavioral economics and psychology; and exploiting recent advances in generative and foundation models to make these mechanisms interpretable and actionable. We propose a conceptual framework that encompasses these elements, and articulate a number of research challenges that emerge at the intersection of these different disciplines.
</details>
<details>
<summary>摘要</summary>
现代推荐系统位于复杂的生态系统中，包括用户行为、内容提供商、广告主和其他激发actor的互动。然而，大多数推荐研究和实践推荐系统的焦点都是在本地、短期优化推荐给单个用户。这会导致推荐系统在长期内的价值创造和生态系统健康受到很大的损害。我们认为，明确模型所有actor的奖励和行为，以及这些actor之间因推荐策略而产生的互动，是为了最大化推荐系统为actor带来的价值和改善整个生态系统的健康。这需要：使用增强学习来优化长期目标; 在不同actor之间进行负担和利益冲突的决策; 减少信息不均衡，并考虑激发和战略行为的工具 Mechanism Design; 更好地模型用户和项目提供者的行为，通过包括行为经济学和心理学的想法; 并利用最近的生成和基础模型来让这些机制可读性和操作性。我们提出一个涵盖这些元素的概念框架，并详细描述这些不同领域之间的研究挑战。
</details></li>
</ul>
<hr>
<h2 id="Sample-Efficient-Co-Design-of-Robotic-Agents-Using-Multi-fidelity-Training-on-Universal-Policy-Network"><a href="#Sample-Efficient-Co-Design-of-Robotic-Agents-Using-Multi-fidelity-Training-on-Universal-Policy-Network" class="headerlink" title="Sample-Efficient Co-Design of Robotic Agents Using Multi-fidelity Training on Universal Policy Network"></a>Sample-Efficient Co-Design of Robotic Agents Using Multi-fidelity Training on Universal Policy Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04085">http://arxiv.org/abs/2309.04085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kishan R. Nagiredla, Buddhika L. Semage, Thommen G. Karimpanal, Arun Kumar A. V, Santu Rana</li>
<li>for: 本研究旨在提高协同设计中控制优化和物理设计之间的同步优化效率，以及避免数据密集的 reinforcement learning 过程中的样本不足问题。</li>
<li>methods: 本研究提出了一种基于 Hyperband 的多信度探索策略，通过将控制器学习问题绑定到一个共享的政策学习器中，以实现热启动效果，从而提高样本效率。 此外，提出了一种 traverse Hyperband 生成的设计矩阵的方法，以减少 Hyperband 的随机性，并且随着 universal policy learner 的强化，对控制器学习问题的解决具有加大的热启动效果。</li>
<li>results: 实验表明，相比基eline，本研究的方法在各种 agent 设计问题上具有显著的优势，并且分析优化后的设计结果显示出了有趣的设计修改，包括设计简化和非INTUITIVE的修改，这些修改在生物世界中有很多应用。<details>
<summary>Abstract</summary>
Co-design involves simultaneously optimizing the controller and agents physical design. Its inherent bi-level optimization formulation necessitates an outer loop design optimization driven by an inner loop control optimization. This can be challenging when the design space is large and each design evaluation involves data-intensive reinforcement learning process for control optimization. To improve the sample-efficiency we propose a multi-fidelity-based design exploration strategy based on Hyperband where we tie the controllers learnt across the design spaces through a universal policy learner for warm-starting the subsequent controller learning problems. Further, we recommend a particular way of traversing the Hyperband generated design matrix that ensures that the stochasticity of the Hyperband is reduced the most with the increasing warm starting effect of the universal policy learner as it is strengthened with each new design evaluation. Experiments performed on a wide range of agent design problems demonstrate the superiority of our method compared to the baselines. Additionally, analysis of the optimized designs shows interesting design alterations including design simplifications and non-intuitive alterations that have emerged in the biological world.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Curve-Your-Attention-Mixed-Curvature-Transformers-for-Graph-Representation-Learning"><a href="#Curve-Your-Attention-Mixed-Curvature-Transformers-for-Graph-Representation-Learning" class="headerlink" title="Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning"></a>Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04082">http://arxiv.org/abs/2309.04082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungjun Cho, Seunghyuk Cho, Sungwoo Park, Hankook Lee, Honglak Lee, Moontae Lee</li>
<li>for: 本研究旨在扩展Transformers模型到非欧几何空间，以更好地处理具有层次或循环结构的实际图像。</li>
<li>methods: 我们提出了一种基于产品符号投影的全级产品符号Transformer模型，可以在不同的常数曲率空间上进行扩展。此外，我们还提出了一种基于几何核函数的非欧几何注意力方法，可以在时间和空间成本线性增长的情况下保持对图像的准确性。</li>
<li>results: 我们的实验表明，通过将Transformers模型扩展到非欧几何空间，可以提高图像重建和节点分类的性能。此外，我们的kernelized非欧几何注意力方法可以在时间和空间成本线性增长的情况下保持对图像的准确性。<details>
<summary>Abstract</summary>
Real-world graphs naturally exhibit hierarchical or cyclical structures that are unfit for the typical Euclidean space. While there exist graph neural networks that leverage hyperbolic or spherical spaces to learn representations that embed such structures more accurately, these methods are confined under the message-passing paradigm, making the models vulnerable against side-effects such as oversmoothing and oversquashing. More recent work have proposed global attention-based graph Transformers that can easily model long-range interactions, but their extensions towards non-Euclidean geometry are yet unexplored. To bridge this gap, we propose Fully Product-Stereographic Transformer, a generalization of Transformers towards operating entirely on the product of constant curvature spaces. When combined with tokenized graph Transformers, our model can learn the curvature appropriate for the input graph in an end-to-end fashion, without the need of additional tuning on different curvature initializations. We also provide a kernelized approach to non-Euclidean attention, which enables our model to run in time and memory cost linear to the number of nodes and edges while respecting the underlying geometry. Experiments on graph reconstruction and node classification demonstrate the benefits of generalizing Transformers to the non-Euclidean domain.
</details>
<details>
<summary>摘要</summary>
现实中的图 naturally exhibits层次或循环结构，这些结构不适合 typical Euclidean space。 existing graph neural networks 可以使用 hyperbolic 或 spherical 空间来学习 representation，但这些方法受到 message-passing  paradigm 的限制，导致模型容易受到 oversmoothing 和 oversquashing 的影响。 更近期的工作已经提出了 global attention-based graph Transformers，可以轻松模型长距离交互，但其 extensions towards non-Euclidean geometry 还未explored。 为了bridging这个 gap，我们提出了 Fully Product-Stereographic Transformer，一种基于 constant curvature spaces 的Transformers的扩展。 当与 tokenized graph Transformers 结合使用时，我们的模型可以在 end-to-end 的方式学习输入图的 curvature，不需要额外的调整不同 curvature 的初始化。 我们还提供了 kernelized approach to non-Euclidean attention，这使得我们的模型在时间和内存成本方面能够 linear 化，同时尊重下面 geometry。 实验表明，通过将 Transformers 扩展到 non-Euclidean 领域，可以获得更好的图重建和节点分类效果。
</details></li>
</ul>
<hr>
<h2 id="UER-A-Heuristic-Bias-Addressing-Approach-for-Online-Continual-Learning"><a href="#UER-A-Heuristic-Bias-Addressing-Approach-for-Online-Continual-Learning" class="headerlink" title="UER: A Heuristic Bias Addressing Approach for Online Continual Learning"></a>UER: A Heuristic Bias Addressing Approach for Online Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04081">http://arxiv.org/abs/2309.04081</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FelixHuiweiLin/UER">https://github.com/FelixHuiweiLin/UER</a></li>
<li>paper_authors: Huiwei Lin, Shanshan Feng, Baoquan Zhang, Hongliang Qiao, Xutao Li, Yunming Ye</li>
<li>for: 本研究旨在解决在线CONTINUAL LEARNING中的偏见问题，即随着数据流入的新数据，模型倾向于新数据的类别，而忘记过去的知识。</li>
<li>methods: 本文提出了一种更直观和高效的方法，即使用投影因子和距离因子来解决偏见问题。投影因子可以学习新的知识，而距离因子可以帮助保持历史知识。</li>
<li>results: 对于三个数据集，论文提出的UER方法在比较多种state-of-the-art方法的情况下表现出色，得到了superior的性能。<details>
<summary>Abstract</summary>
Online continual learning aims to continuously train neural networks from a continuous data stream with a single pass-through data. As the most effective approach, the rehearsal-based methods replay part of previous data. Commonly used predictors in existing methods tend to generate biased dot-product logits that prefer to the classes of current data, which is known as a bias issue and a phenomenon of forgetting. Many approaches have been proposed to overcome the forgetting problem by correcting the bias; however, they still need to be improved in online fashion. In this paper, we try to address the bias issue by a more straightforward and more efficient method. By decomposing the dot-product logits into an angle factor and a norm factor, we empirically find that the bias problem mainly occurs in the angle factor, which can be used to learn novel knowledge as cosine logits. On the contrary, the norm factor abandoned by existing methods helps remember historical knowledge. Based on this observation, we intuitively propose to leverage the norm factor to balance the new and old knowledge for addressing the bias. To this end, we develop a heuristic approach called unbias experience replay (UER). UER learns current samples only by the angle factor and further replays previous samples by both the norm and angle factors. Extensive experiments on three datasets show that UER achieves superior performance over various state-of-the-art methods. The code is in https://github.com/FelixHuiweiLin/UER.
</details>
<details>
<summary>摘要</summary>
在线持续学习目标是通过连续数据流进行单次通过数据进行神经网络的持续训练。现有的方法中最有效的方法是使用练习重温方法，其中通过重新训练部分过去数据来解决偏见问题。现有的预测器通常会生成偏见的dot乘积 logits，它们偏好当前数据中的类别，这被称为偏见问题和忘记现象。许多方法已经被提出来解决忘记问题，但 ainda需要进一步改进。在这篇论文中，我们尝试通过更简单和更高效的方法来解决偏见问题。我们发现，将 dot乘积 logits 分解成角度因子和 нор 因子，我们在实验中发现，偏见问题主要出现在角度因子中，可以用cosine logits来学习新知识。相反，norm因子被现有方法抛弃，可以帮助记忆历史知识。基于这一观察，我们提出了一种直观的方法 called 无偏经验重温（UER）。UER 通过角度因子来学习当前样本，并在过去样本中重新使用 norm 和角度因子来填充旧知识。我们在三个数据集上进行了广泛的实验，结果显示 UER 在多种当前顶峰方法之上获得了更高的性能。代码位于https://github.com/FelixHuiweiLin/UER。
</details></li>
</ul>
<hr>
<h2 id="Enabling-the-Evaluation-of-Driver-Physiology-Via-Vehicle-Dynamics"><a href="#Enabling-the-Evaluation-of-Driver-Physiology-Via-Vehicle-Dynamics" class="headerlink" title="Enabling the Evaluation of Driver Physiology Via Vehicle Dynamics"></a>Enabling the Evaluation of Driver Physiology Via Vehicle Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04078">http://arxiv.org/abs/2309.04078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rodrigo Ordonez-Hurtado, Bo Wen, Nicholas Barra, Ryan Vimba, Sergio Cabrero-Barros, Sergiy Zhuk, Jeffrey L. Rogers</li>
<li>for: 这篇论文是为了设计一种基于连接式智能汽车系统的 Driver Physiology Assessment 系统，以提高道路安全性和早期发现健康问题。</li>
<li>methods: 论文使用了一系列商业传感器，包括汽车和数字健康领域的传感器，以记录 Driver 的生理参数和驾驶行为。这些数据流被处理，提取关键参数，以便了解 Driver 在外部环境和驾驶姿势之间的关系，以及 Driver 的生理反应。</li>
<li>results: 这种驾驶评估系统可以帮助提高道路安全性，并可以与传统医疗设施的数据结合，提高早期发现健康问题的可能性。<details>
<summary>Abstract</summary>
Driving is a daily routine for many individuals across the globe. This paper presents the configuration and methodologies used to transform a vehicle into a connected ecosystem capable of assessing driver physiology. We integrated an array of commercial sensors from the automotive and digital health sectors along with driver inputs from the vehicle itself. This amalgamation of sensors allows for meticulous recording of the external conditions and driving maneuvers. These data streams are processed to extract key parameters, providing insights into driver behavior in relation to their external environment and illuminating vital physiological responses. This innovative driver evaluation system holds the potential to amplify road safety. Moreover, when paired with data from conventional health settings, it may enhance early detection of health-related complications.
</details>
<details>
<summary>摘要</summary>
驾驶是许多人每天的日常 Routine 中的一部分。这篇论文介绍了将车辆转化成连接到电子环境的设备，以评估驾驶者的生理指标。我们结合了汽车和数字健康业界的商业传感器，以及车辆自身的驾驶输入。这些敏感器组合使得精准记录外部环境和驾驶动作。这些数据流被处理，以提取关键参数，了解驾驶者在外部环境下的行为，以及他们的生理响应。这种驾驶评估系统具有提高道路安全性的潜力，同时，与传统医疗设施的数据结合，可能增强早期发现健康问题的能力。
</details></li>
</ul>
<hr>
<h2 id="Riemannian-Langevin-Monte-Carlo-schemes-for-sampling-PSD-matrices-with-fixed-rank"><a href="#Riemannian-Langevin-Monte-Carlo-schemes-for-sampling-PSD-matrices-with-fixed-rank" class="headerlink" title="Riemannian Langevin Monte Carlo schemes for sampling PSD matrices with fixed rank"></a>Riemannian Langevin Monte Carlo schemes for sampling PSD matrices with fixed rank</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04072">http://arxiv.org/abs/2309.04072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianmin Yu, Shixin Zheng, Jianfeng Lu, Govind Menon, Xiangxiong Zhang</li>
<li>for: 本 paper  introduce two explicit schemes to sample matrices from Gibbs distributions on $\mathcal S^{n,p}_+$, the manifold of real positive semi-definite (PSD) matrices of size $n\times n$ and rank $p$.</li>
<li>methods: 这两种方案基于 Riemannian Langevin equation (RLE) 的 Euler-Maruyama 离散化和 Brownian motion on the manifold $\mathcal S^{n,p}_+$.</li>
<li>results: 我们提供了两种基于不同 метри的数学方案，其中一种基于 $\mathcal S^{n,p}_+ $ 的嵌入 metric，另一种基于 Bures-Wasserstein  метри对于 quotient geometry。我们还提供了一些具有显式 Gibbs distribution 的能量函数，使得这些方案可以进行数值验证。<details>
<summary>Abstract</summary>
This paper introduces two explicit schemes to sample matrices from Gibbs distributions on $\mathcal S^{n,p}_+$, the manifold of real positive semi-definite (PSD) matrices of size $n\times n$ and rank $p$. Given an energy function $\mathcal E:\mathcal S^{n,p}_+\to \mathbb{R}$ and certain Riemannian metrics $g$ on $\mathcal S^{n,p}_+$, these schemes rely on an Euler-Maruyama discretization of the Riemannian Langevin equation (RLE) with Brownian motion on the manifold. We present numerical schemes for RLE under two fundamental metrics on $\mathcal S^{n,p}_+$: (a) the metric obtained from the embedding of $\mathcal S^{n,p}_+ \subset \mathbb{R}^{n\times n} $; and (b) the Bures-Wasserstein metric corresponding to quotient geometry. We also provide examples of energy functions with explicit Gibbs distributions that allow numerical validation of these schemes.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了两种显式的方法来从 Gibbs 分布中采样矩阵，其中矩阵是 $\mathcal{S}^{n,p}_+ $ 上的实数正定阵列（PSD），size $n\times n$ 和 rank $p$。给定一个能量函数 $\mathcal{E}: \mathcal{S}^{n,p}_+ \to \mathbb{R}$，以及在 $\mathcal{S}^{n,p}_+ $ 上的一些里曼尼 мет里克 $g$，这些方法基于 Riemannian Langevin equation (RLE) 的 Euler-Maruyama 积分，并且使用布尔斯-沃asserstein  мет里克对于quotient geometry。我们还提供了一些能量函数的Explicit Gibbs 分布，以便numerically验证这些方法。
</details></li>
</ul>
<hr>
<h2 id="3D-Denoisers-are-Good-2D-Teachers-Molecular-Pretraining-via-Denoising-and-Cross-Modal-Distillation"><a href="#3D-Denoisers-are-Good-2D-Teachers-Molecular-Pretraining-via-Denoising-and-Cross-Modal-Distillation" class="headerlink" title="3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation"></a>3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04062">http://arxiv.org/abs/2309.04062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungjun Cho, Dae-Woong Jeong, Sung Moon Ko, Jinwoo Kim, Sehui Han, Seunghoon Hong, Honglak Lee, Moontae Lee</li>
<li>for: 预处理分子表示，提高分子性质预测效果。</li>
<li>methods: 使用2D图表示法和3D杂谱推理法，以及知识储存法。</li>
<li>results: 比基eline方法提高30%左右，并且可以避免高效 compute量。<details>
<summary>Abstract</summary>
Pretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, which led to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In light of this limitation, we propose D&D, a self-supervised molecular representation learning framework that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless application to downstream tasks with no access to accurate conformers. Experiments on real-world molecular property prediction datasets show that the graph encoder trained via D&D can infer 3D information based on the 2D graph and shows superior performance and label-efficiency against other baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Pretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, which led to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In light of this limitation, we propose D&D, a self-supervised molecular representation learning framework that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless application to downstream tasks with no access to accurate conformers. Experiments on real-world molecular property prediction datasets show that the graph encoder trained via D&D can infer 3D information based on the 2D graph and shows superior performance and label-efficiency against other baselines." into Simplified Chinese.<<SYS>>大量未标注数据的预训练是分子性质预测中不可或缺的，因为获取准确标签的成本很高。现有许多2D图形基的分子预训练方法，但这些方法很难显著提高预测性能。最近的工作则是基于3D杂化的预训练，这些方法在预测任务中获得了有望的结果。然而，在下游训练中，使用3D杂化的模型需要对未seen分子的原子坐标精度准确，这是计算成本很高的。为此，我们提出了D&D，一种基于自我监督的分子表示学习框架。通过杂化后cross-modal知识储存，我们的方法可以充分利用杂化中获得的知识，同时在下游任务中不需要准确的杂化。实验表明，通过D&D预训练的2D图形Encoder可以基于2D图形上预测3D信息，并与其他基准模型相比显示出超越性和标签效率。>>>
</details></li>
</ul>
<hr>
<h2 id="Weighted-Unsupervised-Domain-Adaptation-Considering-Geometry-Features-and-Engineering-Performance-of-3D-Design-Data"><a href="#Weighted-Unsupervised-Domain-Adaptation-Considering-Geometry-Features-and-Engineering-Performance-of-3D-Design-Data" class="headerlink" title="Weighted Unsupervised Domain Adaptation Considering Geometry Features and Engineering Performance of 3D Design Data"></a>Weighted Unsupervised Domain Adaptation Considering Geometry Features and Engineering Performance of 3D Design Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04499">http://arxiv.org/abs/2309.04499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seungyeon Shin, Namwoo Kang</li>
<li>for: 加速设计优化和降低计算成本</li>
<li>methods: 使用深度学习对设计数据进行预测，并使用对抗学习抽取域无关特征，以及基于多输出回归任务预测工程性能</li>
<li>results: 可以减少目标风险，并提高对未经标注的目标领域的预测精度<details>
<summary>Abstract</summary>
The product design process in manufacturing involves iterative design modeling and analysis to achieve the target engineering performance, but such an iterative process is time consuming and computationally expensive. Recently, deep learning-based engineering performance prediction models have been proposed to accelerate design optimization. However, they only guarantee predictions on training data and may be inaccurate when applied to new domain data. In particular, 3D design data have complex features, which means domains with various distributions exist. Thus, the utilization of deep learning has limitations due to the heavy data collection and training burdens. We propose a bi-weighted unsupervised domain adaptation approach that considers the geometry features and engineering performance of 3D design data. It is specialized for deep learning-based engineering performance predictions. Domain-invariant features can be extracted through an adversarial training strategy by using hypothesis discrepancy, and a multi-output regression task can be performed with the extracted features to predict the engineering performance. In particular, we present a source instance weighting method suitable for 3D design data to avoid negative transfers. The developed bi-weighting strategy based on the geometry features and engineering performance of engineering structures is incorporated into the training process. The proposed model is tested on a wheel impact analysis problem to predict the magnitude of the maximum von Mises stress and the corresponding location of 3D road wheels. This mechanism can reduce the target risk for unlabeled target domains on the basis of weighted multi-source domain knowledge and can efficiently replace conventional finite element analysis.
</details>
<details>
<summary>摘要</summary>
制造过程中的产品设计包括迭代的设计模型和分析，以达到工程性能目标，但这种迭代过程需要大量的计算资源和时间。近些年，基于深度学习的工程性能预测模型已经被提出，以加速设计优化。然而，这些模型只能在训练数据上作出预测，并且在新领域数据上可能不准确。特别是3D设计数据具有复杂的特征，这意味着存在多种分布。因此，使用深度学习有限制，需要大量的数据采集和训练压力。我们提出了一种异杂预测方法，考虑了3D设计数据的几何特征和工程性能。我们采用了一种适应训练策略，通过使用假设差分来提取域无关特征，并在提取的特征上进行多输出回归任务来预测工程性能。具体来说，我们提出了一种源实例权重方法，适用于3D设计数据，以避免负性传递。我们在训练过程中包含了这种异杂预测策略。我们的模型在轮胎冲击分析问题中预测了轮胎的最大 von Mises 压力和相应的位置。这种机制可以降低目标风险，基于权重多源领域知识，并可以高效地取代传统的finite element分析。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/08/cs.LG_2023_09_08/" data-id="clmjn91my00870j88fi4aciet" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/08/eess.IV_2023_09_08/" class="article-date">
  <time datetime="2023-09-08T09:00:00.000Z" itemprop="datePublished">2023-09-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/08/eess.IV_2023_09_08/">eess.IV - 2023-09-08</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Non-convex-regularization-based-on-shrinkage-penalty-function"><a href="#Non-convex-regularization-based-on-shrinkage-penalty-function" class="headerlink" title="Non-convex regularization based on shrinkage penalty function"></a>Non-convex regularization based on shrinkage penalty function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04593">http://arxiv.org/abs/2309.04593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manu Ghulyani, Muthuvel Arigovindan</li>
<li>for: 这篇论文关注于图像恢复中的Total Variation（TV）和Hessian Schatten norm（HSN）两种正则化方法的比较。</li>
<li>methods: 这篇论文使用了HSN正则化方法，其基于图像导数 Matrix 的二阶偏导数，并通过迭代法实现恢复图像。</li>
<li>results: 实验结果表明，使用HSN正则化方法可以提供更加精细的图像恢复结果，而且比使用TV正则化方法更加能够保持图像的结构。<details>
<summary>Abstract</summary>
Total Variation regularization (TV) is a seminal approach for image recovery. TV involves the norm of the image's gradient, aggregated over all pixel locations. Therefore, TV leads to piece-wise constant solutions, resulting in what is known as the "staircase effect." To mitigate this effect, the Hessian Schatten norm regularization (HSN) employs second-order derivatives, represented by the pth norm of eigenvalues in the image hessian, summed across all pixels. HSN demonstrates superior structure-preserving properties compared to TV. However, HSN solutions tend to be overly smoothed. To address this, we introduce a non-convex shrinkage penalty applied to the Hessian's eigenvalues, deviating from the convex lp norm. It is important to note that the shrinkage penalty is not defined directly in closed form, but specified indirectly through its proximal operation. This makes constructing a provably convergent algorithm difficult as the singular values are also defined through a non-linear operation. However, we were able to derive a provably convergent algorithm using proximal operations. We prove the convergence by establishing that the proposed regularization adheres to restricted proximal regularity. The images recovered by this regularization were sharper than the convex counterparts.
</details>
<details>
<summary>摘要</summary>
全Variation规则化（TV）是一种创新的图像恢复方法。TV是基于图像梯度的 нор，通过所有像素位置的梯度进行积加。因此，TV会导致piece-wise常数解，这被称为“梯形效应”。为了缓解这个效应，Hessian Schatten norm regularization（HSN）使用了图像的第二 derivatives，即图像梯度矩阵的第二导数，通过所有像素位置的积加。HSN的结构保持性比TV更好，但HSN的解征比TV更加平滑。为了解决这个问题，我们引入了一种非 conjugate penalty，该 penalty是应用到梯度矩阵的特征值上。需要注意的是，这种penalty不直接定义为closed form，而是通过其 proximal 操作定义。这使得构建可提able convergent algorithmdifficult。然而，我们成功地 deriv了一个可提able convergent algorith。我们证明了这种regularization的整体 convergent性，通过证明它遵循restricted proximal regularity。recovered by this regularization were sharper than the convex counterparts.
</details></li>
</ul>
<hr>
<h2 id="Motion-Compensated-Unsupervised-Deep-Learning-for-5D-MRI"><a href="#Motion-Compensated-Unsupervised-Deep-Learning-for-5D-MRI" class="headerlink" title="Motion Compensated Unsupervised Deep Learning for 5D MRI"></a>Motion Compensated Unsupervised Deep Learning for 5D MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04552">http://arxiv.org/abs/2309.04552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Kettelkamp, Ludovica Romanin, Davide Piccini, Sarv Priya, Mathews Jacob</li>
<li>for: 提高5D心脏MRI数据重建速度和质量，降低计算时间和数据不均匀影响。</li>
<li>methods: 使用深度学习算法模型数据的压缩和扩展，并利用心脏和呼吸信号来估算数据的变换。</li>
<li>results: 提出了一种数据效率的无监督深度学习算法，可以快速重建高品质5D心脏MRI数据，并且不受数据不均匀的影响。<details>
<summary>Abstract</summary>
We propose an unsupervised deep learning algorithm for the motion-compensated reconstruction of 5D cardiac MRI data from 3D radial acquisitions. Ungated free-breathing 5D MRI simplifies the scan planning, improves patient comfort, and offers several clinical benefits over breath-held 2D exams, including isotropic spatial resolution and the ability to reslice the data to arbitrary views. However, the current reconstruction algorithms for 5D MRI take very long computational time, and their outcome is greatly dependent on the uniformity of the binning of the acquired data into different physiological phases. The proposed algorithm is a more data-efficient alternative to current motion-resolved reconstructions. This motion-compensated approach models the data in each cardiac/respiratory bin as Fourier samples of the deformed version of a 3D image template. The deformation maps are modeled by a convolutional neural network driven by the physiological phase information. The deformation maps and the template are then jointly estimated from the measured data. The cardiac and respiratory phases are estimated from 1D navigators using an auto-encoder. The proposed algorithm is validated on 5D bSSFP datasets acquired from two subjects.
</details>
<details>
<summary>摘要</summary>
我们提议一种无监督深度学习算法，用于从3D radial获取的5D卡地脉搏MRI数据进行运动补做的重建。无束自由呼吸5D MRI简化扫描规划，提高了患者的舒适度，并提供了许多临床利益，包括均匀的空间分辨率和可以在任意视图中扩展数据。然而，当前的5D MRI重建算法需要非常长的计算时间，并且其结果受到数据的匀压缩程度的影响。我们提出的算法是一种更数据效率的替代方案。这种运动补做方法模型了数据在每个卡地脉搏/呼吸期间的数据为Fourier样本的扭曲版本的3D图像模板。扭曲地图是由一个卷积神经网络驱动的物理阶段信息。然后，模板和扭曲地图在测量数据中被并行估计。卡地脉搏和呼吸阶段是通过1D导航器使用自动编码器来估计。我们的算法被验证在5D bSSFP数据集上，从两个主体中获取。
</details></li>
</ul>
<hr>
<h2 id="Poster-Making-Edge-assisted-LiDAR-Perceptions-Robust-to-Lossy-Point-Cloud-Compression"><a href="#Poster-Making-Edge-assisted-LiDAR-Perceptions-Robust-to-Lossy-Point-Cloud-Compression" class="headerlink" title="Poster: Making Edge-assisted LiDAR Perceptions Robust to Lossy Point Cloud Compression"></a>Poster: Making Edge-assisted LiDAR Perceptions Robust to Lossy Point Cloud Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04549">http://arxiv.org/abs/2309.04549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Heo, Gregorie Phillips, Per-Erik Brodin, Ada Gavrilovska</li>
<li>for: 提高雷达点云的质量，以减少因压缩导致的感知性能下降。</li>
<li>methods: 使用深度Gradient来 interpolate点云中的点，以提高点云的质量。</li>
<li>results: 对于点云的重建，我们的算法比现有的图像插值算法更好的结果。<details>
<summary>Abstract</summary>
Real-time light detection and ranging (LiDAR) perceptions, e.g., 3D object detection and simultaneous localization and mapping are computationally intensive to mobile devices of limited resources and often offloaded on the edge. Offloading LiDAR perceptions requires compressing the raw sensor data, and lossy compression is used for efficiently reducing the data volume. Lossy compression degrades the quality of LiDAR point clouds, and the perception performance is decreased consequently. In this work, we present an interpolation algorithm improving the quality of a LiDAR point cloud to mitigate the perception performance loss due to lossy compression. The algorithm targets the range image (RI) representation of a point cloud and interpolates points at the RI based on depth gradients. Compared to existing image interpolation algorithms, our algorithm shows a better qualitative result when the point cloud is reconstructed from the interpolated RI. With the preliminary results, we also describe the next steps of the current work.
</details>
<details>
<summary>摘要</summary>
现实时光压感和测距（LiDAR）感知需要大量计算资源，例如3D对象检测和同时定位和地图生成。由于移动设备的限制资源，LiDAR感知 часто被卸载到边缘进行处理。卸载LiDAR感知需要压缩原始传感器数据，而失真压缩可以有效减少数据量。然而，失真压缩会降低LiDAR点云的质量，导致感知性能下降。在这种情况下，我们提出了一种插值算法，用于提高LiDAR点云的质量，从而抵消因为失真压缩而导致的感知性能下降。该算法targets点云（RI）表示中的距离图像，并在RI基于深度梯度的点插值。与现有的图像插值算法相比，我们的算法在重建点云时显示更好的质量。在这里，我们还描述了现有工作的下一步。
</details></li>
</ul>
<hr>
<h2 id="Systematic-Review-of-Techniques-in-Brain-Image-Synthesis-using-Deep-Learning"><a href="#Systematic-Review-of-Techniques-in-Brain-Image-Synthesis-using-Deep-Learning" class="headerlink" title="Systematic Review of Techniques in Brain Image Synthesis using Deep Learning"></a>Systematic Review of Techniques in Brain Image Synthesis using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04511">http://arxiv.org/abs/2309.04511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubham Singh, Ammar Ranapurwala, Mrunal Bewoor, Sheetal Patil, Satyam Rai</li>
<li>for: 该评论文章探讨了医学成像领域的现状，尤其是使用深度学习技术进行脑图像生成。文章强调了利用深度学习提高诊断精度和减少医学过程的侵入性的重要性，以及深度学习在医学成像中的潜在作用。</li>
<li>methods: 文章详细描述了不同的脑图像生成方法和技术，包括2D到3D构建、MRI生成和使用转换器。文章还讨论了这些方法的限制和挑战，如获得充分的准备数据和解决脑超声问题。</li>
<li>results: 文章总结了这些方法的结果，包括脑图像生成的精度和可靠性问题，以及未来这些技术的发展前景和潜在应用。文章还强调了转换器的潜在作用和可能性，以及解决这些技术的缺点和限制的可能性。<details>
<summary>Abstract</summary>
This review paper delves into the present state of medical imaging, with a specific focus on the use of deep learning techniques for brain image synthesis. The need for medical image synthesis to improve diagnostic accuracy and decrease invasiveness in medical procedures is emphasized, along with the role of deep learning in enabling these advancements. The paper examines various methods and techniques for brain image synthesis, including 2D to 3D constructions, MRI synthesis, and the use of transformers. It also addresses limitations and challenges faced in these methods, such as obtaining well-curated training data and addressing brain ultrasound issues. The review concludes by exploring the future potential of this field and the opportunities for further advancements in medical imaging using deep learning techniques. The significance of transformers and their potential to revolutionize the medical imaging field is highlighted. Additionally, the paper discusses the potential solutions to the shortcomings and limitations faced in this field. The review provides researchers with an updated reference on the present state of the field and aims to inspire further research and bridge the gap between the present state of medical imaging and the future possibilities offered by deep learning techniques.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="How-Can-We-Tame-the-Long-Tail-of-Chest-X-ray-Datasets"><a href="#How-Can-We-Tame-the-Long-Tail-of-Chest-X-ray-Datasets" class="headerlink" title="How Can We Tame the Long-Tail of Chest X-ray Datasets?"></a>How Can We Tame the Long-Tail of Chest X-ray Datasets?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04293">http://arxiv.org/abs/2309.04293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arsh Verma</li>
<li>for: 这个论文的目的是提出一种新的初始化方法，以提高自动检测器的性能。</li>
<li>methods: 这个论文使用的方法是选择一个更加适合目标数据集的初始化方法，以提高模型的性能。</li>
<li>results: 研究发现，通过选择合适的初始化方法，可以很好地提高模型的性能，而且这种方法可以轻松扩展到新的标签。<details>
<summary>Abstract</summary>
Chest X-rays (CXRs) are a medical imaging modality that is used to infer a large number of abnormalities. While it is hard to define an exhaustive list of these abnormalities, which may co-occur on a chest X-ray, few of them are quite commonly observed and are abundantly represented in CXR datasets used to train deep learning models for automated inference. However, it is challenging for current models to learn independent discriminatory features for labels that are rare but may be of high significance. Prior works focus on the combination of multi-label and long tail problems by introducing novel loss functions or some mechanism of re-sampling or re-weighting the data. Instead, we propose that it is possible to achieve significant performance gains merely by choosing an initialization for a model that is closer to the domain of the target dataset. This method can complement the techniques proposed in existing literature, and can easily be scaled to new labels. Finally, we also examine the veracity of synthetically generated data to augment the tail labels and analyse its contribution to improving model performance.
</details>
<details>
<summary>摘要</summary>
胸部X射影（CXR）是医疗影像模式，用于推断许多异常。尽管难以列举涵盖所有这些异常的完整列表，但许多其中的异常相对常见，广泛存在于用于训练深度学习模型的CXR数据集中。然而，目前的模型困难于分离独立的特征，用于标签的极少数 Label，即使这些标签的重要性很高。先前的工作集中在多标签和长尾问题上，通过引入新的损失函数或数据重新抽样或重新权重的机制来解决。在这篇文章中，我们提出，可以通过选择更加适应目标数据集的初始化来实现显著性提升。这种方法可以补充现有文献中提出的方法，并可以轻松扩展到新的标签。 finally，我们还检查了通过生成的数据进行补充的方法，并分析其对提高模型性能的贡献。
</details></li>
</ul>
<hr>
<h2 id="SegmentAnything-helps-microscopy-images-based-automatic-and-quantitative-organoid-detection-and-analysis"><a href="#SegmentAnything-helps-microscopy-images-based-automatic-and-quantitative-organoid-detection-and-analysis" class="headerlink" title="SegmentAnything helps microscopy images based automatic and quantitative organoid detection and analysis"></a>SegmentAnything helps microscopy images based automatic and quantitative organoid detection and analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04190">http://arxiv.org/abs/2309.04190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaodanxing/sam4organoid">https://github.com/xiaodanxing/sam4organoid</a></li>
<li>paper_authors: Xiaodan Xing, Chunling Tang, Yunzhe Guo, Nicholas Kurniawan, Guang Yang</li>
<li>for: 研究器官发育、药物发现和毒性评估</li>
<li>methods: 使用SegmentAnything进行精准分割个体器官，以及引入了一组量化器官结构的 morphological properties</li>
<li>results: 自动化分析结果与手动分析结果相一致，表明方法的可靠性和效果<details>
<summary>Abstract</summary>
Organoids are self-organized 3D cell clusters that closely mimic the architecture and function of in vivo tissues and organs. Quantification of organoid morphology helps in studying organ development, drug discovery, and toxicity assessment. Recent microscopy techniques provide a potent tool to acquire organoid morphology features, but manual image analysis remains a labor and time-intensive process. Thus, this paper proposes a comprehensive pipeline for microscopy analysis that leverages the SegmentAnything to precisely demarcate individual organoids. Additionally, we introduce a set of morphological properties, including perimeter, area, radius, non-smoothness, and non-circularity, allowing researchers to analyze the organoid structures quantitatively and automatically. To validate the effectiveness of our approach, we conducted tests on bright-field images of human induced pluripotent stem cells (iPSCs) derived neural-epithelial (NE) organoids. The results obtained from our automatic pipeline closely align with manual organoid detection and measurement, showcasing the capability of our proposed method in accelerating organoids morphology analysis.
</details>
<details>
<summary>摘要</summary>
“团聚体”是指自组织的3D细胞团，它们与生体中的组织和器官结构和功能具有高度相似性。量化团聚体 morphology 可以帮助研究器官发展、药物探索和药物毒性评估。现有的微scopie技术为研究提供了一个强大的工具，但是手动图像分析仍然是一项劳动和时间耗费的过程。因此，这篇论文提出了一个完整的微scopie分析管线，利用SegmentAnything进行精确地划分团聚体。此外，我们还引入了一组 morphological 特性，包括周长、面积、半径、非稳定性和非圆形性，这些特性可以让研究人员对团聚体结构进行量化和自动化的分析。为验证我们的方法的有效性，我们对人类干细胞 derivated neural-epithelial（NE）团聚体的明亮场图进行了测试。结果表明，我们的自动化管线与手动团聚体检测和测量结果高度相似，这证明了我们提出的方法在加速团聚体形态分析方面的能力。
</details></li>
</ul>
<hr>
<h2 id="PRISTA-Net-Deep-Iterative-Shrinkage-Thresholding-Network-for-Coded-Diffraction-Patterns-Phase-Retrieval"><a href="#PRISTA-Net-Deep-Iterative-Shrinkage-Thresholding-Network-for-Coded-Diffraction-Patterns-Phase-Retrieval" class="headerlink" title="PRISTA-Net: Deep Iterative Shrinkage Thresholding Network for Coded Diffraction Patterns Phase Retrieval"></a>PRISTA-Net: Deep Iterative Shrinkage Thresholding Network for Coded Diffraction Patterns Phase Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04171">http://arxiv.org/abs/2309.04171</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuaxou/prista-net">https://github.com/liuaxou/prista-net</a></li>
<li>paper_authors: Aoxu Liu, Xiaohong Fan, Yin Yang, Jianping Zhang</li>
<li>For: The paper is written for addressing the problem of phase retrieval (PR) in computational imaging and image processing, specifically developing a deep learning method that combines interpretability with fast inference ability.* Methods: The paper proposes a deep unfolding network (DUN) based on the first-order iterative shrinkage thresholding algorithm (ISTA), which utilizes a learnable nonlinear transformation and an attention mechanism to address the proximal-point mapping sub-problem and enhance local information. The fast Fourier transform (FFT) is used to learn global features.* Results: The proposed PRISTA-Net framework outperforms the existing state-of-the-art methods in terms of qualitative and quantitative evaluations on Coded Diffraction Patterns (CDPs) measurements, demonstrating the effectiveness of the proposed method in handling noise and improving recovery quality.<details>
<summary>Abstract</summary>
The problem of phase retrieval (PR) involves recovering an unknown image from limited amplitude measurement data and is a challenge nonlinear inverse problem in computational imaging and image processing. However, many of the PR methods are based on black-box network models that lack interpretability and plug-and-play (PnP) frameworks that are computationally complex and require careful parameter tuning. To address this, we have developed PRISTA-Net, a deep unfolding network (DUN) based on the first-order iterative shrinkage thresholding algorithm (ISTA). This network utilizes a learnable nonlinear transformation to address the proximal-point mapping sub-problem associated with the sparse priors, and an attention mechanism to focus on phase information containing image edges, textures, and structures. Additionally, the fast Fourier transform (FFT) is used to learn global features to enhance local information, and the designed logarithmic-based loss function leads to significant improvements when the noise level is low. All parameters in the proposed PRISTA-Net framework, including the nonlinear transformation, threshold parameters, and step size, are learned end-to-end instead of being manually set. This method combines the interpretability of traditional methods with the fast inference ability of deep learning and is able to handle noise at each iteration during the unfolding stage, thus improving recovery quality. Experiments on Coded Diffraction Patterns (CDPs) measurements demonstrate that our approach outperforms the existing state-of-the-art methods in terms of qualitative and quantitative evaluations. Our source codes are available at \emph{https://github.com/liuaxou/PRISTA-Net}.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用 Deep Unfolding Network (DUN) 和 First-order Iterative Shrinkage Thresholding Algorithm (ISTA) 开发了一种名为 PRISTA-Net 的方法，用于解决phas Retrieval (PR) 问题。这种问题的核心是从有限幅度测量数据中 recuperate 未知图像，是计算成像和图像处理中的一个非线性反向问题。然而，许多现有的 PR 方法基于黑盒网络模型，lack  interpretability 和 plug-and-play (PnP) 框架， computationally complex 并且需要精心调整参数。为了解决这个问题，我们在 PRISTA-Net 中使用了 learnable nonlinear transformation 来Address  proximal-point mapping 子问题，并使用 attention mechanism 来Focus  on phase information containing image edges, textures, and structures。此外，我们还使用 Fast Fourier Transform (FFT) 来学习全球特征，以增强本地信息。设计的 logarithmic-based loss function 对于噪声水平较低的情况带来了显著改进。在我们的方法中，所有参数都是通过 end-to-end 学习而不是手动设置的。这种方法结合了传统方法的可 interpretability 和深度学习的快速推理能力，可以在每个螺旋阶段中处理噪声，从而提高恢复质量。我们的实验表明，我们的方法在 Coded Diffraction Patterns (CDPs) 测量数据上的表现较为出色，胜过现有的状态 искус家方法。我们的源代码可以在 <https://github.com/liuaxou/PRISTA-Net> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-Efficient-SDRTV-to-HDRTV-by-Learning-from-Image-Formation"><a href="#Towards-Efficient-SDRTV-to-HDRTV-by-Learning-from-Image-Formation" class="headerlink" title="Towards Efficient SDRTV-to-HDRTV by Learning from Image Formation"></a>Towards Efficient SDRTV-to-HDRTV by Learning from Image Formation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04084">http://arxiv.org/abs/2309.04084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Chen, Zheyuan Li, Zhengwen Zhang, Jimmy S. Ren, Yihao Liu, Jingwen He, Yu Qiao, Jiantao Zhou, Chao Dong</li>
<li>for: 这篇论文是用于将标准动态范围（SDR）内容转换成高动态范围（HDR）标准的。</li>
<li>methods: 该论文提出了一种三步解决方案 called HDRTVNet++, 包括 adaptive global color mapping、local enhancement 和 highlight refinement。</li>
<li>results: 该方法可以在4K分辨率图像上实现高效和轻量级的SDRTV-to-HDRTV转换，并且在量化和视觉上达到了状态方法。<details>
<summary>Abstract</summary>
Modern displays are capable of rendering video content with high dynamic range (HDR) and wide color gamut (WCG). However, the majority of available resources are still in standard dynamic range (SDR). As a result, there is significant value in transforming existing SDR content into the HDRTV standard. In this paper, we define and analyze the SDRTV-to-HDRTV task by modeling the formation of SDRTV/HDRTV content. Our analysis and observations indicate that a naive end-to-end supervised training pipeline suffers from severe gamut transition errors. To address this issue, we propose a novel three-step solution pipeline called HDRTVNet++, which includes adaptive global color mapping, local enhancement, and highlight refinement. The adaptive global color mapping step uses global statistics as guidance to perform image-adaptive color mapping. A local enhancement network is then deployed to enhance local details. Finally, we combine the two sub-networks above as a generator and achieve highlight consistency through GAN-based joint training. Our method is primarily designed for ultra-high-definition TV content and is therefore effective and lightweight for processing 4K resolution images. We also construct a dataset using HDR videos in the HDR10 standard, named HDRTV1K that contains 1235 and 117 training images and 117 testing images, all in 4K resolution. Besides, we select five metrics to evaluate the results of SDRTV-to-HDRTV algorithms. Our final results demonstrate state-of-the-art performance both quantitatively and visually. The code, model and dataset are available at https://github.com/xiaom233/HDRTVNet-plus.
</details>
<details>
<summary>摘要</summary>
现代显示器可以渲染视频内容高动态范围（HDR）和宽色彩范围（WCG）。然而，大多数可用资源仍然是标准动态范围（SDR）。因此，将现有的SDR内容转化到HDRTV标准具有重要价值。在这篇论文中，我们定义和分析SDRTV-to-HDRTV任务，并模型了SDRTV/HDRTV内容的形成。我们的分析和观察结果表明，直接使用端到端超vised训练管道会导致严重的色彩范围过渡错误。为解决这个问题，我们提议一种新的三步解决方案管道，称为HDRTVNet++。该管道包括适应性全局颜色映射、本地增强和突出点细节优化。适应性全局颜色映射步骤使用全局统计为导航，对图像进行适应性颜色映射。本地增强网络然后用于提高本地细节。最后，我们将两个子网络组合成一个生成器，通过GAN基于的共同训练实现突出点一致性。我们的方法主要针对超高Definition TV内容，因此效果精灵和轻量级适用于4K分辨率图像处理。此外，我们还构建了一个名为HDRTV1K的HDR视频集，包含1235个和117个训练图像和测试图像，모都在4K分辨率。此外，我们选择了五个维度来评估SDRTV-to-HDRTV算法的结果。最终结果表明我们的方法具有状态级表现，同时也有出色的视觉表现。代码、模型和数据集可以在https://github.com/xiaom233/HDRTVNet-plus上下载。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Hierarchical-Transformers-for-Whole-Brain-Segmentation-with-Intracranial-Measurements-Integration"><a href="#Enhancing-Hierarchical-Transformers-for-Whole-Brain-Segmentation-with-Intracranial-Measurements-Integration" class="headerlink" title="Enhancing Hierarchical Transformers for Whole Brain Segmentation with Intracranial Measurements Integration"></a>Enhancing Hierarchical Transformers for Whole Brain Segmentation with Intracranial Measurements Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04071">http://arxiv.org/abs/2309.04071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masilab/unest">https://github.com/masilab/unest</a></li>
<li>paper_authors: Xin Yu, Yucheng Tang, Qi Yang, Ho Hin Lee, Shunxing Bao, Yuankai Huo, Bennett A. Landman<br>for:The paper aims to enhance the existing whole brain segmentation methodology by incorporating intracranial measurements, specifically total intracranial volume (TICV) and posterior fossa volume (PFV), to provide a more comprehensive analysis of brain structures.methods:The authors use a hierarchical transformer model called UNesT, which is first pretrained on a large dataset of T1-weighted (T1w) 3D volumes from 8 different sites, and then finetuned with a smaller dataset of T1w 3D volumes from Open Access Series Imaging Studies (OASIS) that includes both 133 whole brain classes and TICV&#x2F;PFV labels.results:The authors evaluate the performance of their method using Dice similarity coefficients (DSC) and show that their model is able to accurately estimate TICV&#x2F;PFV while maintaining the performance of the 132 brain regions at a comparable level.Here is the information in Simplified Chinese text:for: 本文目的是增强现有的全脑分割方法，通过包含内侧体积量（TICV）和后底斜板体积（PFV）的测量，以提供更全面的脑结构分析。methods: 作者使用一种层次转换器模型called UNesT，首先在8个不同的数据集上进行预训练，然后在Open Access Series Imaging Studies（OASIS）中进行微调，使用这些数据集包含133个全脑类和TICV&#x2F;PFV标签。results: 作者使用 dice相似度（DSC）评估方法来评估模型的性能，并显示其模型能够准确地估算TICV&#x2F;PFV，同时保持132个脑区域的性能在相似水平。<details>
<summary>Abstract</summary>
Whole brain segmentation with magnetic resonance imaging (MRI) enables the non-invasive measurement of brain regions, including total intracranial volume (TICV) and posterior fossa volume (PFV). Enhancing the existing whole brain segmentation methodology to incorporate intracranial measurements offers a heightened level of comprehensiveness in the analysis of brain structures. Despite its potential, the task of generalizing deep learning techniques for intracranial measurements faces data availability constraints due to limited manually annotated atlases encompassing whole brain and TICV/PFV labels. In this paper, we enhancing the hierarchical transformer UNesT for whole brain segmentation to achieve segmenting whole brain with 133 classes and TICV/PFV simultaneously. To address the problem of data scarcity, the model is first pretrained on 4859 T1-weighted (T1w) 3D volumes sourced from 8 different sites. These volumes are processed through a multi-atlas segmentation pipeline for label generation, while TICV/PFV labels are unavailable. Subsequently, the model is finetuned with 45 T1w 3D volumes from Open Access Series Imaging Studies (OASIS) where both 133 whole brain classes and TICV/PFV labels are available. We evaluate our method with Dice similarity coefficients(DSC). We show that our model is able to conduct precise TICV/PFV estimation while maintaining the 132 brain regions performance at a comparable level. Code and trained model are available at: https://github.com/MASILab/UNesT/wholebrainSeg.
</details>
<details>
<summary>摘要</summary>
整个脑部分 segmentation with magnetic resonance imaging（MRI）可以实现非侵入性测量脑部分，包括总脑室体积（TICV）和后底室体积（PFV）。增强现有整个脑部分分 segmentation方法，以包括脑部分测量提供了更高的全面性分析脑结构的可能性。然而，将深度学习技术推广到脑部分测量面临数据可用性限制，因为有限的手动标注图集覆盖整个脑部分和TICV/PFV标签。在这篇论文中，我们改进了层次转换器UNesT，以实现同时分 segmenting整个脑部分和TICV/PFV。为了解决数据缺乏问题，我们首先在8个不同的地点上获取了4859个T1-加重（T1w）3D图像，并使用多Atlas分割管道生成标签。然后，我们在Open Access Series Imaging Studies（OASIS）上练习45个T1w 3D图像，其中包括133个整个脑部分类和TICV/PFV标签。我们使用 dice相似度系数（DSC）进行评估。我们发现，我们的模型能够准确地计算TICV/PFV，而同时保持132个脑部分性能在相似水平。代码和训练模型可以在以下链接中下载：https://github.com/MASILab/UNesT/wholebrainSeg。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/08/eess.IV_2023_09_08/" data-id="clmjn91qy00ht0j88czkz61zm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/33/">33</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
