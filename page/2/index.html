
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/2/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_09_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/21/cs.AI_2023_09_21/" class="article-date">
  <time datetime="2023-09-21T12:00:00.000Z" itemprop="datePublished">2023-09-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/21/cs.AI_2023_09_21/">cs.AI - 2023-09-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ForceSight-Text-Guided-Mobile-Manipulation-with-Visual-Force-Goals"><a href="#ForceSight-Text-Guided-Mobile-Manipulation-with-Visual-Force-Goals" class="headerlink" title="ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals"></a>ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12312">http://arxiv.org/abs/2309.12312</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlenoWar/For-friends1231231230-23092">https://github.com/AlenoWar/For-friends1231231230-23092</a></li>
<li>paper_authors: Jeremy A. Collins, Cody Houff, You Liang Tan, Charles C. Kemp</li>
<li>for: 本文描述了一种基于深度学习的文本导引移动抓取系统，可以预测视觉力目标和相应的力目标。</li>
<li>methods: 该系统使用深度神经网络，根据单个RGBD图像和文本提示，确定目标减动器姿态和相应的力目标。</li>
<li>results: 在一个实验中，该系统在不同物体实例和未看过的环境中完成了精准抓取、抽屉打开和物体传递等任务，成功率为81%。在另一个实验中，仅通过视觉服务器而忽略力目标时，成功率降低至45%，这表明力目标可以显著提高性能。<details>
<summary>Abstract</summary>
We present ForceSight, a system for text-guided mobile manipulation that predicts visual-force goals using a deep neural network. Given a single RGBD image combined with a text prompt, ForceSight determines a target end-effector pose in the camera frame (kinematic goal) and the associated forces (force goal). Together, these two components form a visual-force goal. Prior work has demonstrated that deep models outputting human-interpretable kinematic goals can enable dexterous manipulation by real robots. Forces are critical to manipulation, yet have typically been relegated to lower-level execution in these systems. When deployed on a mobile manipulator equipped with an eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps, drawer opening, and object handovers with an 81% success rate in unseen environments with object instances that differed significantly from the training data. In a separate experiment, relying exclusively on visual servoing and ignoring force goals dropped the success rate from 90% to 45%, demonstrating that force goals can significantly enhance performance. The appendix, videos, code, and trained models are available at https://force-sight.github.io/.
</details>
<details>
<summary>摘要</summary>
我们现在提出了 ForceSight，一种基于文本导引的移动操作系统，该系统使用深度神经网络预测视觉力目标。给定一个RGBD图像和文本提示，ForceSight可以确定摄像头帧中的目标器位（动力目标）以及相关的力（力目标）。这两个组成部分共同形成了视觉力目标。在先前的研究中，使用深度模型输出人类可解释的动力目标可以实现灵活的操作。然而，在这些系统中，力通常被视为低级别执行。当部署在搭载了眼在手RGBD相机的移动操作器时，ForceSight在不同环境中完成了精度抓取、抽屉打开和物品传递等任务，成功率达81%。在另一个实验中，完全依赖于视觉服务并忽略力目标，成功率从90%下降到45%，这表明力目标可以明显提高性能。详细信息、视频、代码和训练模型可以在<https://force-sight.github.io/>上获取。
</details></li>
</ul>
<hr>
<h2 id="LLM-Grounder-Open-Vocabulary-3D-Visual-Grounding-with-Large-Language-Model-as-an-Agent"><a href="#LLM-Grounder-Open-Vocabulary-3D-Visual-Grounding-with-Large-Language-Model-as-an-Agent" class="headerlink" title="LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent"></a>LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12311">http://arxiv.org/abs/2309.12311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David F. Fouhey, Joyce Chai</li>
<li>for: 提高家用机器人的3D视觉固定率，使其能够基于环境进行导航、对象搜索和回答问题。</li>
<li>methods: 提出了一种基于大语言模型（LLM）的开放词汇、零shot的3D视觉固定率管道，使用LLM将自然语言查询分解成 semantic constituents，并使用视觉固定工具（如OpenScene或LERF）来确定3D场景中的对象。</li>
<li>results: 在ScanRefer benchmark上进行评估，显示了与状态艺术的零shot固定率表现，尤其是对于复杂的自然语言查询。<details>
<summary>Abstract</summary>
3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs significantly improve the grounding capability, especially for complex language queries, making LLM-Grounder an effective approach for 3D vision-language tasks in robotics. Videos and interactive demos can be found on the project website https://chat-with-nerf.github.io/ .
</details>
<details>
<summary>摘要</summary>
三维视觉固定是家庭机器人的关键技能，它允许机器人在环境中导航、操作物品以及回答问题。现有的方法通常需要大量标注数据或者具有处理复杂语言查询的限制，而我们提出了LLM-Grounder，一种新的零批量、开 vocabulary 的大语言模型（LLM）基于的三维视觉固定管道。LLM-Grounder 使用 LLM 将复杂的自然语言查询分解成 semantic constituents，然后使用三维场景识别工具，如 OpenScene 或 LERF，确定场景中的物体。LLM 然后评估场景中提议的物体之间的空间和通用感知关系，以确定最终的固定决策。我们的方法不需要任何标注训练数据，可以泛化到新的三维场景和任意文本查询。我们在 ScanRefer benchmark 上进行了评估，并示出了零批量固定精度。我们的发现表明，LLM 可以大幅提高固定能力，特别是对复杂语言查询，使LLM-Grounder 成为三维视觉语言任务中的有效方法。视频和互动示例可以在项目网站 https://chat-with-nerf.github.io/ 找到。
</details></li>
</ul>
<hr>
<h2 id="Rehearsal-Simulating-Conflict-to-Teach-Conflict-Resolution"><a href="#Rehearsal-Simulating-Conflict-to-Teach-Conflict-Resolution" class="headerlink" title="Rehearsal: Simulating Conflict to Teach Conflict Resolution"></a>Rehearsal: Simulating Conflict to Teach Conflict Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12309">http://arxiv.org/abs/2309.12309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Shaikh, Valentino Chai, Michele J. Gelfand, Diyi Yang, Michael S. Bernstein</li>
<li>for: 提高冲突管理技能</li>
<li>methods: 使用 simulate 人工智能对话者进行冲突练习，探索不同的对话路径，并通过Feedback学习冲突解决策略。</li>
<li>results: 在比对组与控制组之间进行实验后，参与者们通过使用 Rehearsal 增强了冲突管理技能，减少了竞争策略的使用率，同时 doubling 合作策略的使用率。<details>
<summary>Abstract</summary>
Interpersonal conflict is an uncomfortable but unavoidable fact of life. Navigating conflict successfully is a skill -- one that can be learned through deliberate practice -- but few have access to effective training or feedback. To expand this access, we introduce Rehearsal, a system that allows users to rehearse conflicts with a believable simulated interlocutor, explore counterfactual "what if?" scenarios to identify alternative conversational paths, and learn through feedback on how and when to apply specific conflict strategies. Users can utilize Rehearsal to practice handling a variety of predefined conflict scenarios, from office disputes to relationship issues, or they can choose to create their own. To enable Rehearsal, we develop IRP prompting, a method of conditioning output of a large language model on the influential Interest-Rights-Power (IRP) theory from conflict resolution. Rehearsal uses IRP to generate utterances grounded in conflict resolution theory, guiding users towards counterfactual conflict resolution strategies that help de-escalate difficult conversations. In a between-subjects evaluation, 40 participants engaged in an actual conflict with a confederate after training. Compared to a control group with lecture material covering the same IRP theory, participants with simulated training from Rehearsal significantly improved their performance in the unaided conflict: they reduced their use of escalating competitive strategies by an average of 67%, while doubling their use of cooperative strategies. Overall, Rehearsal highlights the potential effectiveness of language models as tools for learning and practicing interpersonal skills.
</details>
<details>
<summary>摘要</summary>
人际冲突是生活中不可避免的一个不舒适的事实。成功 navigating 冲突是一种技能 -- 可以通过意识的练习学习 -- 但是很少人有效的训练和反馈。为了扩大这种训练的access，我们介绍了 Rehearsal，一个系统，允许用户通过与生动的 simulate 对话者进行模拟冲突，探索不同的对话路径，并通过反馈学习冲突策略。用户可以使用 Rehearsal 练习各种预先定义的冲突场景，从办公室争议到关系问题，或者他们可以创建自己的。为了实现 Rehearsal，我们开发了 IRP 提示，一种基于冲突解决理论中的有力 Interest-Rights-Power（IRP）的方法，用于生成与冲突解决相关的语言模型输出。Rehearsal 使用 IRP 生成对话，引导用户采用不同的对话策略，以帮助他们在困难的对话中减少竞争策略的使用，同时增加合作策略的使用。在一个 between-subjects 评估中，40名参与者在与假对手进行实际冲突后接受了 simulated 训练。与控制组（ receives 同 IRP 理论的讲解材料）相比，参与者通过 Rehearsal 的模拟训练显著改善了他们在无助的冲突中的表现，减少了竞争策略的使用量by 67%，同时 doubling 合作策略的使用。总之，Rehearsal highlights 语言模型的潜在效果iveness 作为人际技能学习和练习的工具。
</details></li>
</ul>
<hr>
<h2 id="LongLoRA-Efficient-Fine-tuning-of-Long-Context-Large-Language-Models"><a href="#LongLoRA-Efficient-Fine-tuning-of-Long-Context-Large-Language-Models" class="headerlink" title="LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"></a>LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12307">http://arxiv.org/abs/2309.12307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dvlab-research/longlora">https://github.com/dvlab-research/longlora</a></li>
<li>paper_authors: Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia</li>
<li>for: 这 paper 是为了提高大型语言模型（LLM）的上下文大小，并且减少计算成本。</li>
<li>methods: 这 paper 使用了两种方法来减少计算成本：1) 使用稀疏的局部注意力进行微调，而不是 dense 的全球注意力；2) 修改 LoRA 的参数fficient fine-tuning 策略，以便在训练时使用更小的上下文大小。</li>
<li>results: 这 paper 在多个任务上都显示了优秀的 empirical  результаhat,包括使用 LLaMA2 模型从 7B&#x2F;13B 到 70B，并在单个 8x A100 机器上完成了Context 的扩展。<details>
<summary>Abstract</summary>
We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA demonstrates strong empirical results on various tasks on LLaMA2 models from 7B/13B to 70B. LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like FlashAttention-2. In addition, to make LongLoRA practical, we collect a dataset, LongQA, for supervised fine-tuning. It contains more than 3k long context question-answer pairs.
</details>
<details>
<summary>摘要</summary>
我们介绍了LongLoRA，一种高效的微调方法，可以将预训练的大型自然语言模型（LLM）的上下文大小扩展，而不需要很多计算成本。通常，在训练LLMs时，使用长context大小需要大量的计算时间和GPU资源。例如，在context长度为8192时，自我注意层的计算成本将增加16倍。在这篇论文中，我们提高了LLM的上下文扩展的计算效率，从两个方面进行了优化。一方面，虽然在推理时需要 dense global attention，但是在微调过程中可以使用笔 sparse local attention，从而提高计算效率。我们提出了shift short attention技术，可以快速扩展上下文，同时保持与原始模型的性能相似。另一方面，我们再次检视了参数效率微调环境，发现LoRA在trainable embedding和normalization的前提下，可以很好地扩展上下文。LongLoRA在多个任务上获得了强的实际结果，使用LLaMA2模型从7B/13B到70B。LongLoRA可以在单个8x A100机器上从4k context扩展到100k，或者从70B扩展到32k。LongLoRA可以保持原始模型的结构，与大多数现有技术兼容，如FlashAttention-2。此外，为使LongLoRA实用，我们收集了一个超过3k长context问答对的数据集，名为LongQA。
</details></li>
</ul>
<hr>
<h2 id="Environment-biased-Feature-Ranking-for-Novelty-Detection-Robustness"><a href="#Environment-biased-Feature-Ranking-for-Novelty-Detection-Robustness" class="headerlink" title="Environment-biased Feature Ranking for Novelty Detection Robustness"></a>Environment-biased Feature Ranking for Novelty Detection Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12301">http://arxiv.org/abs/2309.12301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Smeu, Elena Burceanu, Emanuela Haller, Andrei Liviu Nicolicioiu</li>
<li>for: 本研究的目的是robust novelty detection，即检测 semantic content 中的新鲜事物，并在不同环境下保持检测的稳定性。</li>
<li>methods: 本研究使用了一种基于预训练 embedding 和多环境设置的方法，可以 rank 特征按照其环境吸引力。首先，我们计算了每个特征的分布方差变化强度，然后选择高分的特征进行去除，以消除干扰关系并提高总性能。</li>
<li>results: 我们的方法可以在 covariance 和 sub-population shift 两种情况下提高检测性能，在两个实际和 sintetic benchmark 中实现了6%的提高。<details>
<summary>Abstract</summary>
We tackle the problem of robust novelty detection, where we aim to detect novelties in terms of semantic content while being invariant to changes in other, irrelevant factors. Specifically, we operate in a setup with multiple environments, where we determine the set of features that are associated more with the environments, rather than to the content relevant for the task. Thus, we propose a method that starts with a pretrained embedding and a multi-env setup and manages to rank the features based on their environment-focus. First, we compute a per-feature score based on the feature distribution variance between envs. Next, we show that by dropping the highly scored ones, we manage to remove spurious correlations and improve the overall performance by up to 6%, both in covariance and sub-population shift cases, both for a real and a synthetic benchmark, that we introduce for this task.
</details>
<details>
<summary>摘要</summary>
我们面临着一个robust新鲜度检测问题，我们想检测新鲜度从 semantics 角度来，而不受其他无关因素的变化影响。特别是，我们在多个环境中运行，并确定了与环境相关的特征，而不是与任务相关的内容。因此，我们提出了一种方法，它从预训练 embedding 和多环境设置开始，然后对特征进行排名，以便根据环境注重。首先，我们计算每个特征的分布方差变化分数，以确定它们在不同环境中的分布情况。接着，我们证明了，通过抛弃高分的特征，可以消除干扰关系，并提高总表现，达到6%的提升，包括covariance和sub-population shift两种情况。这种提升可以在真实和 sintetic  benchmark 中实现。
</details></li>
</ul>
<hr>
<h2 id="See-to-Touch-Learning-Tactile-Dexterity-through-Visual-Incentives"><a href="#See-to-Touch-Learning-Tactile-Dexterity-through-Visual-Incentives" class="headerlink" title="See to Touch: Learning Tactile Dexterity through Visual Incentives"></a>See to Touch: Learning Tactile Dexterity through Visual Incentives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12300">http://arxiv.org/abs/2309.12300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Irmak Guzey, Yinlong Dai, Ben Evans, Soumith Chintala, Lerrel Pinto</li>
<li>for: 提高多指 robot 的灵活搔挠能力，使其能够具备人类的灵活搔挠能力。</li>
<li>methods: 使用视觉奖励来优化气肺动作策略，并通过对人类示范的对比来学习视觉表示。</li>
<li>results: 在六个复杂任务中，TAVI 实现了 73% 的成功率，比使用气肺动作和视觉奖励的策略高出 108%，比没有气肺 observational 输入的策略高出 135%。<details>
<summary>Abstract</summary>
Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise, contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing fails to provide adequate cues for reasoning about objects' spatial configurations, limiting the ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn visual representations. Next, we construct a reward function using these visual representations through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement learning on our robot to optimize tactile-based policies that maximize the visual reward. On six challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects, TAVI achieves a success rate of 73% using our four-fingered Allegro robot hand. The increase in performance is 108% higher than policies using tactile and vision-based rewards and 135% higher than policies without tactile observational input. Robot videos are best viewed on our project website: https://see-to-touch.github.io/.
</details>
<details>
<summary>摘要</summary>
Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise, contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing fails to provide adequate cues for reasoning about objects' spatial configurations, limiting the ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn visual representations. Next, we construct a reward function using these visual representations through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement learning on our robot to optimize tactile-based policies that maximize the visual reward. On six challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects, TAVI achieves a success rate of 73% using our four-fingered Allegro robot hand. The increase in performance is 108% higher than policies using tactile and vision-based rewards and 135% higher than policies without tactile observational input. Robot videos are best viewed on our project website: <https://see-to-touch.github.io/>.Here's the translation in Traditional Chinese:Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise, contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing fails to provide adequate cues for reasoning about objects' spatial configurations, limiting the ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn visual representations. Next, we construct a reward function using these visual representations through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement learning on our robot to optimize tactile-based policies that maximize the visual reward. On six challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects, TAVI achieves a success rate of 73% using our four-fingered Allegro robot hand. The increase in performance is 108% higher than policies using tactile and vision-based rewards and 135% higher than policies without tactile observational input. Robot videos are best viewed on our project website: <https://see-to-touch.github.io/>.
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Drive-Anywhere"><a href="#Learning-to-Drive-Anywhere" class="headerlink" title="Learning to Drive Anywhere"></a>Learning to Drive Anywhere</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12295">http://arxiv.org/abs/2309.12295</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Ruizhao Zhu, Peng Huang, Eshed Ohn-Bar, Venkatesh Saligrama</li>
<li>for: 本研究旨在提出一种能够快速适应不同地区和交通规则的自动驾驶模型，以满足现代自动驾驶系统的需求。</li>
<li>methods: 该模型使用了条件学习和地理位置信息来适应不同地区的驾驶行为，并通过对照学习对象来快速学习和适应不同的驾驶情况。</li>
<li>results: 研究表明，该模型可以在多个数据集、城市和部署方式下表现出色，比如中央化、半supervised和分布式 Agent 训练方式，并且在 CARLA 上测试中比基eline 高出14%。<details>
<summary>Abstract</summary>
Human drivers can seamlessly adapt their driving decisions across geographical locations with diverse conditions and rules of the road, e.g., left vs. right-hand traffic. In contrast, existing models for autonomous driving have been thus far only deployed within restricted operational domains, i.e., without accounting for varying driving behaviors across locations or model scalability. In this work, we propose AnyD, a single geographically-aware conditional imitation learning (CIL) model that can efficiently learn from heterogeneous and globally distributed data with dynamic environmental, traffic, and social characteristics. Our key insight is to introduce a high-capacity geo-location-based channel attention mechanism that effectively adapts to local nuances while also flexibly modeling similarities among regions in a data-driven manner. By optimizing a contrastive imitation objective, our proposed approach can efficiently scale across inherently imbalanced data distributions and location-dependent events. We demonstrate the benefits of our AnyD agent across multiple datasets, cities, and scalable deployment paradigms, i.e., centralized, semi-supervised, and distributed agent training. Specifically, AnyD outperforms CIL baselines by over 14% in open-loop evaluation and 30% in closed-loop testing on CARLA.
</details>
<details>
<summary>摘要</summary>
人类驾驶员可以无缝地适应不同地区的条件和道路规则，如左右手交通。然而，现有的自动驾驶模型只能在限定的运行Domain中进行部署，无法考虑不同地区的驾驶行为或模型扩展性。在这种工作中，我们提出AnyD，一种能够快速学习从多元化和全球分布的数据中的 conditional imitation learning（CIL）模型。我们的关键发现是引入高容量的地理位置基于的通道注意力机制，可以有效地适应当地特性，同时也可以数据驱动地模型同地区之间的相似性。通过优化一个对比式学习目标函数，我们的提出的方法可以高效地扩展到自然具有偏斜分布的数据集和地点事件。我们在多个数据集、城市和扩展模型训练方法（中央、半supervised、分布式代理训练）上展示了AnyD代理的优势， Specifically, AnyD比CIL基eline高出14%在开loop评估中和30%在closed-loop测试中。
</details></li>
</ul>
<hr>
<h2 id="The-Reversal-Curse-LLMs-trained-on-“A-is-B”-fail-to-learn-“B-is-A”"><a href="#The-Reversal-Curse-LLMs-trained-on-“A-is-B”-fail-to-learn-“B-is-A”" class="headerlink" title="The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”"></a>The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12288">http://arxiv.org/abs/2309.12288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lukasberglund/reversal_curse">https://github.com/lukasberglund/reversal_curse</a></li>
<li>paper_authors: Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans</li>
<li>for: 这个论文旨在描述一种语言模型具有的惊人的总结不良现象，即模型在句子的逆向方向上不能自动总结。</li>
<li>methods: 这个论文使用了自动推导大语言模型（LLM）的训练和细化来探讨这种现象。</li>
<li>results: 研究发现，即使模型在句子中具有”A是B”的Pattern，它们也不能自动总结”B是A”的句子。这表明模型存在一种基本的逻辑推理失败，并且不能总结句子中的普遍规律。<details>
<summary>Abstract</summary>
We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly answers questions like the former 79% of the time, compared to 33% for the latter. This shows a failure of logical deduction that we hypothesize is caused by the Reversal Curse. Code is available at https://github.com/lukasberglund/reversal_curse.
</details>
<details>
<summary>摘要</summary>
我们揭示了普遍性不足的抽象问题在自动进行语言模型（LLM）中。如果一个模型在“A是B”的句子上训练，那么它不会自动推导到“B是A”的方向。这被称为“推倒祸咒”。例如，如果一个模型在“奥拉夫·舒兹是德国第九任总理”上训练，那么它不会自动回答“谁是德国第九任总理？”的问题。此外，对于正确答案（奥拉夫·舒兹）的概率不高于随机名称。因此，模型表现出基本的逻辑推理失败，不能推导出训练集中 prevailing pattern（即“A是B”发生时，“B是A”更 probable）。我们通过在虚假句子上精细调整GPT-3和Llama-1模型，并证明它们在“Uriah Hawthorne是《abyssal Melodies》的作曲家”这类句子上失败，不能正确回答“abyssal Melodies”的作曲家是谁。推倒祸咒是模型大小和家族的robust，不受数据增强的影响。我们还测试了ChatGPT（GPT-3.5和GPT-4）在关于真实名人的问题上，如“谁是汤米·克鲁塞的妈妈？”和其反向“谁是mary Lee Pfeiffer的儿子？”。GPT-4在前者79%的时间内正确回答问题，比后者33%的时间更高。这显示了逻辑推理的失败，我们假设是由推倒祸咒引起的。代码可以在https://github.com/lukasberglund/reversal_curse上找到。
</details></li>
</ul>
<hr>
<h2 id="MetaMath-Bootstrap-Your-Own-Mathematical-Questions-for-Large-Language-Models"><a href="#MetaMath-Bootstrap-Your-Own-Mathematical-Questions-for-Large-Language-Models" class="headerlink" title="MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"></a>MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12284">http://arxiv.org/abs/2309.12284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu<br>for:这篇论文的目的是提高大型自然语言模型（LLM）在数学领域的理解能力，以解决问题时需要进行复杂的推理过程。methods:论文使用了自然语言生成技术，将数学问题重新写成多种角度，从而创建了一个新的数学问题集合（MetaMathQA），并对LLaMA-2模型进行微调。results:实验结果显示，MetaMath模型在两个流行的数学理解测试 benchmark（GSM8K和MATH）上表现出色，较前一代模型高出11.5%和8.7%。特别是，MetaMath-7B模型在GSM8K上得分66.4%，MATH上得分19.4%。<details>
<summary>Abstract</summary>
Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves $66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models of the same size by $11.5\%$ and $8.7\%$. Particularly, {MetaMath-70B} achieves an accuracy of $82.3\%$ on {GSM8K}, slightly better than {GPT-3.5-Turbo}. We release the {MetaMathQA} dataset, the {MetaMath} models with different model sizes and the training code for public use.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经推动了自然语言理解的限制，并表现出了优秀的问题解决能力。 despite the great success, most existing open-source LLMs (eg, LLaMA-2) are still far from satisfactory for solving mathematical problems due to complex reasoning procedures. To bridge this gap, we propose 《MetaMath》, a fine-tuned language model specializing in mathematical reasoning. Specifically, we start by rewriting mathematical questions from multiple perspectives without extra knowledge, resulting in a new dataset called {MetaMathQA}. Then, we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (ie, GSM8K and MATH) for mathematical reasoning show that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, our MetaMath-70B model achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release the MetaMathQA dataset, the MetaMath models with different model sizes, and the training code for public use.
</details></li>
</ul>
<hr>
<h2 id="LLMR-Real-time-Prompting-of-Interactive-Worlds-using-Large-Language-Models"><a href="#LLMR-Real-time-Prompting-of-Interactive-Worlds-using-Large-Language-Models" class="headerlink" title="LLMR: Real-time Prompting of Interactive Worlds using Large Language Models"></a>LLMR: Real-time Prompting of Interactive Worlds using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12276">http://arxiv.org/abs/2309.12276</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asem010/legend-pice">https://github.com/asem010/legend-pice</a></li>
<li>paper_authors: Fernanda De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores Fernandez, Jaron Lanier</li>
<li>for: 这个论文是为了描述一种基于大自然语言模型（LLM）的混合现实创作和修改框架，用于实时创建和修改互动型混合现实经验。</li>
<li>methods: 该框架使用了一些新的策略来解决缺乏理想训练数据或需要 sintheziser internal dynamics、intuitive analysis 和高级互动性的情况。它使用文本交互和 Unity 游戏引擎，并包括Scene Understanding、Task Planning、Self-Debugging 和 Memory Management 等技术。</li>
<li>results: 与标准 GPT-4 相比，LLMR 的平均错误率下降至 4 倍。作者们在多个示例世界中证明了 LLMR 的跨平台兼容性，并通过许多创建和修改任务来表明它可以生成和编辑多样化的对象、工具和场景。最后，作者们进行了一项用户研究（N&#x3D;11），发现参与者们对系统有积极的经验，并表示会再次使用它。<details>
<summary>Abstract</summary>
We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR's cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.
</details>
<details>
<summary>摘要</summary>
我团队推出了混合现实语言大型模型框架（LLMR），用于在实时创建和修改混合现实体验的实时创作和修改。 LLMR 采用了新的策略来解决缺乏理想训练数据或设计目标需要内在动力、直观分析或高级互动的情况。 我们的框架基于文本交互和 Unity 游戏引擎，并包括场景理解、任务规划、自我调试和内存管理等技术。 相比标准 GPT-4，LLMR 的平均错误率下降了4倍。 我们示例了 LLMR 的跨平台兼容性，并对多种创建和修改任务进行了评估，以示其能够生成和修改多样化的对象、工具和场景。 最后，我们进行了一项用户研究（N=11），发现参与者们有积极的体验，并表示他们会再次使用这系统。
</details></li>
</ul>
<hr>
<h2 id="Enabling-Quartile-based-Estimated-Mean-Gradient-Aggregation-As-Baseline-for-Federated-Image-Classifications"><a href="#Enabling-Quartile-based-Estimated-Mean-Gradient-Aggregation-As-Baseline-for-Federated-Image-Classifications" class="headerlink" title="Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications"></a>Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12267">http://arxiv.org/abs/2309.12267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusen Wu, Jamie Deng, Hao Chen, Phuong Nguyen, Yelena Yesha</li>
<li>for: 本研究旨在提出一种名为Estimated Mean Aggregation（EMA）的创新解决方案，用于解决 Federated Learning（FL）系统中数据多样性和安全性问题。</li>
<li>methods: EMA使用trimmed means来有效地处理恶意外异值，同时揭示数据不同性，以确保训练的模型在各客户端数据上具备适应性。</li>
<li>results: EMA在一系列实验中表现出高准确率和地下曲线Area Under the Curve（AUC），相比其他方法，EMA成为FL集成方法评估效果和安全性的基本参考点。EMA的贡献因此为FL系统的效率、安全性和多样性带来了重要的进步。<details>
<summary>Abstract</summary>
Federated Learning (FL) has revolutionized how we train deep neural networks by enabling decentralized collaboration while safeguarding sensitive data and improving model performance. However, FL faces two crucial challenges: the diverse nature of data held by individual clients and the vulnerability of the FL system to security breaches. This paper introduces an innovative solution named Estimated Mean Aggregation (EMA) that not only addresses these challenges but also provides a fundamental reference point as a $\mathsf{baseline}$ for advanced aggregation techniques in FL systems. EMA's significance lies in its dual role: enhancing model security by effectively handling malicious outliers through trimmed means and uncovering data heterogeneity to ensure that trained models are adaptable across various client datasets. Through a wealth of experiments, EMA consistently demonstrates high accuracy and area under the curve (AUC) compared to alternative methods, establishing itself as a robust baseline for evaluating the effectiveness and security of FL aggregation methods. EMA's contributions thus offer a crucial step forward in advancing the efficiency, security, and versatility of decentralized deep learning in the context of FL.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）已经革命化了深度神经网络的训练方式，使得各个客户端的数据可以共同合作，同时保护敏感数据和提高模型性能。然而，FL还面临两个重要挑战：客户端数据的多样性和FL系统的安全性。这篇文章提出了一个创新的解决方案，即估算平均联合（EMA），这个方法不仅能够有效地处理邪恶的偏出现，同时也能够探索数据的多样性，以确保训练出来的模型能够适应不同的客户端数据。通过丰富的实验，EMA证明了它在精确率和投影面积（AUC）方面的优秀性，并成为FL联合方法的基本底线，这让EMA在评估FL联合方法的效iveness和安全性方面提供了一个重要的引用点。EMA的贡献因此为FL技术的效率、安全性和多样性带来了一个重要的进步。
</details></li>
</ul>
<hr>
<h2 id="SALSA-CLRS-A-Sparse-and-Scalable-Benchmark-for-Algorithmic-Reasoning"><a href="#SALSA-CLRS-A-Sparse-and-Scalable-Benchmark-for-Algorithmic-Reasoning" class="headerlink" title="SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning"></a>SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12253">http://arxiv.org/abs/2309.12253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jkminder/salsa-clrs">https://github.com/jkminder/salsa-clrs</a></li>
<li>paper_authors: Julian Minder, Florian Grötschla, Joël Mathys, Roger Wattenhofer</li>
<li>for: 本文提出了一个扩展版的CLRS算法学习benchmark，强调可扩展性和使用稀疏表示。</li>
<li>methods: 本文使用了适应算法从原CLRSbenchmark中，以及新增的分布式和随机算法。</li>
<li>results: 本文进行了广泛的实验评估。<details>
<summary>Abstract</summary>
We introduce an extension to the CLRS algorithmic learning benchmark, prioritizing scalability and the utilization of sparse representations. Many algorithms in CLRS require global memory or information exchange, mirrored in its execution model, which constructs fully connected (not sparse) graphs based on the underlying problem. Despite CLRS's aim of assessing how effectively learned algorithms can generalize to larger instances, the existing execution model becomes a significant constraint due to its demanding memory requirements and runtime (hard to scale). However, many important algorithms do not demand a fully connected graph; these algorithms, primarily distributed in nature, align closely with the message-passing paradigm employed by Graph Neural Networks. Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark specifically with scalability and sparseness in mind. Our approach includes adapted algorithms from the original CLRS benchmark and introduces new problems from distributed and randomized algorithms. Moreover, we perform a thorough empirical evaluation of our benchmark. Code is publicly available at https://github.com/jkminder/SALSA-CLRS.
</details>
<details>
<summary>摘要</summary>
我们介绍一个对CLRS算法学习标准的扩展，强调可扩展性和使用稀疏表示。许多CLRS中的算法需要全球内存或资讯交换，这反映在它的执行模型中，它会建立全连接（不稀疏）图基于下面问题。尽管CLRS的目的是评估学习算法如何对更大的实例进行扩展，但现有的执行模型对于内存需求和时间成本（Difficult to scale）具有显著的限制。然而，许多重要的算法不需要全连接图，这些算法通常是分布式的，与Graph Neural Networks中的讯息传递模式相似。因此，我们提出了SALSA-CLRS，一个CLRS标准的扩展，专注于可扩展性和稀疏性。我们的方法包括原CLRS标准中的修改算法和新问题，并对我们的标准进行了详细的实验评估。代码可以在https://github.com/jkminder/SALSA-CLRS上下载。
</details></li>
</ul>
<hr>
<h2 id="Bad-Actor-Good-Advisor-Exploring-the-Role-of-Large-Language-Models-in-Fake-News-Detection"><a href="#Bad-Actor-Good-Advisor-Exploring-the-Role-of-Large-Language-Models-in-Fake-News-Detection" class="headerlink" title="Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection"></a>Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12247">http://arxiv.org/abs/2309.12247</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ictmcg/arg">https://github.com/ictmcg/arg</a></li>
<li>paper_authors: Beizhe Hu, Qiang Sheng, Juan Cao, Yuhui Shi, Yang Li, Danding Wang, Peng Qi<br>for: 这篇论文主要研究了大语言模型（LLMs）在假新闻检测方面的潜力。methods: 作者采用了一种名为 adaptive rationale guidance network（ARG）的网络，该网络使用了精心调整的小语言模型（SLMs）和大语言模型（LLMs）来检测假新闻。results: 实验结果显示，作者的方法可以在两个实际数据集上超过三种基eline方法，包括SLM-based、LLM-based和这两种模型的组合。<details>
<summary>Abstract</summary>
Detecting fake news requires both a delicate sense of diverse clues and a profound understanding of the real-world background, which remains challenging for detectors based on small language models (SLMs) due to their knowledge and capability limitations. Recent advances in large language models (LLMs) have shown remarkable performance in various tasks, but whether and how LLMs could help with fake news detection remains underexplored. In this paper, we investigate the potential of LLMs in fake news detection. First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a gap to the LLM's inability to select and integrate rationales properly to conclude. Based on these findings, we propose that current LLMs may not substitute fine-tuned SLMs in fake news detection but can be a good advisor for SLMs by providing multi-perspective instructive rationales. To instantiate this proposal, we design an adaptive rationale guidance network for fake news detection (ARG), in which SLMs selectively acquire insights on news analysis from the LLMs' rationales. We further derive a rationale-free version of ARG by distillation, namely ARG-D, which services cost-sensitive scenarios without inquiring LLMs. Experiments on two real-world datasets demonstrate that ARG and ARG-D outperform three types of baseline methods, including SLM-based, LLM-based, and combinations of small and large language models.
</details>
<details>
<summary>摘要</summary>
检测假新闻需要一种细腻的多种迹象推理和深刻的现实背景理解，这些检测器基于小语言模型（SLM）的知识和能力限制使得它们的表现还有限。然而，大语言模型（LLM）的最近进步在多种任务中表现出色，但是LLM是否可以帮助检测假新闻仍然未得到充分探索。本文 investigate LLM在假新闻检测中的潜力。我们首先进行实验研究，发现一个成熟的LLM如GPT 3.5可以暴露假新闻并提供愉悦多元理由，但是仍然落后于精通BERT的基本SLM。我们 subsequnt分析发现这种差距是因为LLM无法正确地选择和 инте格 rationales来结论。根据这些发现，我们提出了LLM不能完全取代精通SLM，但可以作为SLM的好助手，提供多元指导性的理由。为实现这一提议，我们设计了一个适应性 rationales 指导网络（ARG），其中SLM可以选择性地从LLM的理由中获得分析新闻的 instruciton。此外，我们还 derivation 一个不需要 rationales 的ARG-D版本，通过浸泡来实现cost-sensitive场景中的服务。我们在两个实际数据集上进行实验，发现ARG和ARG-D都高于三种基eline方法，包括SLM、LLM和小语言模型和大语言模型的组合。
</details></li>
</ul>
<hr>
<h2 id="ChaCha-Leveraging-Large-Language-Models-to-Prompt-Children-to-Share-Their-Emotions-about-Personal-Events"><a href="#ChaCha-Leveraging-Large-Language-Models-to-Prompt-Children-to-Share-Their-Emotions-about-Personal-Events" class="headerlink" title="ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events"></a>ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12244">http://arxiv.org/abs/2309.12244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Woosuk Seo, Chanmo Yang, Young-Ho Kim</li>
<li>for: 本研究旨在开发一个名为ChaCha的谈话机器人，以便儿童通过分享自己的故事和感受来学习表达情感。</li>
<li>methods: 这个研究使用了状态机和大语言模型（LLMs），以保持对话在轨道，同时允许儿童自由发展对话。</li>
<li>results: 经过对20名儿童（年龄在8-12岁）的exploratory研究发现，儿童认为ChaCha是一个亲密的朋友，并将个人故事和感受分享给ChaCha。研究发现，使用LLMs可以设计出适合儿童的谈话机器人，以支持儿童在表达情感方面。<details>
<summary>Abstract</summary>
Children typically learn to identify and express emotions through sharing their stories and feelings with others, particularly their family. However, it is challenging for parents or siblings to have emotional communication with children since children are still developing their communication skills. We present ChaCha, a chatbot that encourages and guides children to share personal events and associated emotions. ChaCha combines a state machine and large language models (LLMs) to keep the dialogue on track while carrying on free-form conversations. Through an exploratory study with 20 children (aged 8-12), we examine how ChaCha prompts children to share personal events and guides them to describe associated emotions. Participants perceived ChaCha as a close friend and shared their stories on various topics, such as family trips and personal achievements. Based on the quantitative and qualitative findings, we discuss opportunities for leveraging LLMs to design child-friendly chatbots to support children in sharing their emotions.
</details>
<details>
<summary>摘要</summary>
孩子通常通过与他们的家人分享故事和感受来学习识别和表达情感。然而，由于孩子的通信技能还在发展，因此与孩子进行情感交流可以是很困难的。我们提出了 ChaCha，一个聊天机器人，它鼓励和指导孩子分享个人事件和相关的情感。ChaCha结合状态机和大型自然语言模型（LLMs），以保持对话在轨而进行自由化对话。经过20名8-12岁的孩子参与的exploratory研究，我们发现ChaCha可以让孩子 perceive为一个亲密的朋友，并让他们分享各种话题，如家庭旅行和个人成就。根据量化和质化的结果，我们讨论了如何通过LLMs设计为孩子友好的聊天机器人，以支持孩子在表达情感方面。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Artificial-Intelligence-for-Drug-Discovery-and-Development-–-A-Comprehensive-Survey"><a href="#Explainable-Artificial-Intelligence-for-Drug-Discovery-and-Development-–-A-Comprehensive-Survey" class="headerlink" title="Explainable Artificial Intelligence for Drug Discovery and Development – A Comprehensive Survey"></a>Explainable Artificial Intelligence for Drug Discovery and Development – A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12177">http://arxiv.org/abs/2309.12177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roohallah Alizadehsani, Sadiq Hussain, Rene Ripardo Calixto, Victor Hugo C. de Albuquerque, Mohamad Roshanzamir, Mohamed Rahouti, Senthil Kumar Jagatheesaperumal<br>for:* 这篇评论文章旨在提供对XAI技术在药物发现领域的全面了解，以及XAI技术在药物发现中的应用和挑战。methods:* 这篇评论文章总结了目前XAI技术在药物发现领域的状态，包括各种XAI方法的应用和挑战，以及XAI技术在药物发现中的应用。results:* 这篇评论文章结合了XAI技术在药物发现中的应用，包括目标预测、化学物质设计和毒性预测等方面。同时，文章还提出了未来XAI技术在药物发现领域的可能性和挑战。<details>
<summary>Abstract</summary>
The field of drug discovery has experienced a remarkable transformation with the advent of artificial intelligence (AI) and machine learning (ML) technologies. However, as these AI and ML models are becoming more complex, there is a growing need for transparency and interpretability of the models. Explainable Artificial Intelligence (XAI) is a novel approach that addresses this issue and provides a more interpretable understanding of the predictions made by machine learning models. In recent years, there has been an increasing interest in the application of XAI techniques to drug discovery. This review article provides a comprehensive overview of the current state-of-the-art in XAI for drug discovery, including various XAI methods, their application in drug discovery, and the challenges and limitations of XAI techniques in drug discovery. The article also covers the application of XAI in drug discovery, including target identification, compound design, and toxicity prediction. Furthermore, the article suggests potential future research directions for the application of XAI in drug discovery. The aim of this review article is to provide a comprehensive understanding of the current state of XAI in drug discovery and its potential to transform the field.
</details>
<details>
<summary>摘要</summary>
随着人工智能（AI）和机器学习（ML）技术的出现，药物发现领域经历了极具挑战性的变革。然而，随着AI和ML模型的复杂化，需要对这些模型的透明度和可解释性的需求也在增加。解释人工智能（XAI）是一种新的方法，它能够提供更加可解释的机器学习模型预测结果的理解。在过去几年中，对药物发现领域应用XAI技术的兴趣不断增加。本文提供了药物发现领域XAI技术的现状报告，包括不同的XAI方法、其在药物发现中的应用、以及XAI技术在药物发现中的挑战和限制。此外，文章还概述了XAI在药物发现中的应用，包括目标识别、化合物设计和毒性预测。最后，文章还提出了未来对XAI在药物发现领域的研究发展的可能性。本文的目的是为读者提供药物发现领域XAI技术的全面了解，以及其在未来可能带来的变革。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Domain-Adaptation-for-Self-Driving-from-Past-Traversal-Features"><a href="#Unsupervised-Domain-Adaptation-for-Self-Driving-from-Past-Traversal-Features" class="headerlink" title="Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features"></a>Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12140">http://arxiv.org/abs/2309.12140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Travis Zhang, Katie Luo, Cheng Perng Phoo, Yurong You, Wei-Lun Chao, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger</li>
<li>for: 提高自动驾驶车辆中3D对象检测系统的准确性。</li>
<li>methods: 利用无标签重复游走多个位置来适应新的驾驶环境，并通过计算重复LiDAR扫描数据的统计来导航适应过程。</li>
<li>results: 提高了基于LiDAR的检测模型，使其更适应不同的驾驶环境，并在实际数据集上实现了up to 20点的性能提升，尤其是检测人行和远距离对象。Here’s the translation in English:</li>
<li>for: To improve the accuracy of 3D object detection systems for self-driving cars.</li>
<li>methods: Utilize unlabeled repeated traversals of multiple locations to adapt object detectors to new driving environments, and leverage statistics computed from repeated LiDAR scans to guide the adaptation process.</li>
<li>results: Enhance LiDAR-based detection models, making them more adaptable to different driving environments, and achieve up to 20-point performance gain, especially in detecting pedestrians and distant objects, on real-world datasets.<details>
<summary>Abstract</summary>
The rapid development of 3D object detection systems for self-driving cars has significantly improved accuracy. However, these systems struggle to generalize across diverse driving environments, which can lead to safety-critical failures in detecting traffic participants. To address this, we propose a method that utilizes unlabeled repeated traversals of multiple locations to adapt object detectors to new driving environments. By incorporating statistics computed from repeated LiDAR scans, we guide the adaptation process effectively. Our approach enhances LiDAR-based detection models using spatial quantized historical features and introduces a lightweight regression head to leverage the statistics for feature regularization. Additionally, we leverage the statistics for a novel self-training process to stabilize the training. The framework is detector model-agnostic and experiments on real-world datasets demonstrate significant improvements, achieving up to a 20-point performance gain, especially in detecting pedestrians and distant objects. Code is available at https://github.com/zhangtravis/Hist-DA.
</details>
<details>
<summary>摘要</summary>
“自驾车3D对象检测系统的快速发展已经显著提高了准确性。然而，这些系统在不同的驾驶环境下难以泛化，这可能会导致检测交通参与者的失败。为解决这个问题，我们提出了一种方法，它利用多个位置的重复旋转进行不标注的多次旋转，以适应新的驾驶环境。我们通过计算重复扫描 LiDAR 的统计数据，有效地导向适应过程。我们的方法可以增强基于 LiDAR 的检测模型，并引入空间量化历史特征来减少特征训练。此外，我们还利用统计数据进行一种新的自动训练过程，以稳定训练。这个框架是检测模型无关的，实验表明，在真实世界 datasets 上，我们的方法可以获得大约 20 个表现指标的提高，特别是检测人行和远距离对象。代码可以在 https://github.com/zhangtravis/Hist-DA 上获取。”Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-the-relationship-between-Benchmarking-Standards-and-Certification-in-Robotics-and-AI"><a href="#On-the-relationship-between-Benchmarking-Standards-and-Certification-in-Robotics-and-AI" class="headerlink" title="On the relationship between Benchmarking, Standards and Certification in Robotics and AI"></a>On the relationship between Benchmarking, Standards and Certification in Robotics and AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12139">http://arxiv.org/abs/2309.12139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alan F. T. Winfield, Matthew Studley</li>
<li>for: 本文旨在探讨标准、认证和测试 benchmarking 的关系，并 argue 这三个过程是负责任创新的重要组成部分。</li>
<li>methods: 本文使用例子从标准、认证和测试 benchmarking 等领域，探讨这些过程如何相互关联，并为负责任创新提供指导。</li>
<li>results: 本文 argue 通过标准、认证和测试 benchmarking 等过程，可以帮助确保机器人和人工智能系统的负责任创新。<details>
<summary>Abstract</summary>
Benchmarking, standards and certification are closely related processes. Standards can provide normative requirements that robotics and AI systems may or may not conform to. Certification generally relies upon conformance with one or more standards as the key determinant of granting a certificate to operate. And benchmarks are sets of standardised tests against which robots and AI systems can be measured. Benchmarks therefore can be thought of as informal standards. In this paper we will develop these themes with examples from benchmarking, standards and certification, and argue that these three linked processes are not only useful but vital to the broader practice of Responsible Innovation.
</details>
<details>
<summary>摘要</summary>
标准、认证和测试是密切相关的过程。标准可以提供规范要求，机器人和人工智能系统可能或可能不遵循。认证通常基于一个或多个标准来决定授予操作证书。而测试是使用标准化的测试方法来评估机器人和人工智能系统的性能。因此，测试可以被视为非正式的标准。在这篇论文中，我们将通过实例来发展这些主题，并 argue that这三个连接过程不仅是有用的，而且是责任创新的必要条件。
</details></li>
</ul>
<hr>
<h2 id="OSN-MDAD-Machine-Translation-Dataset-for-Arabic-Multi-Dialectal-Conversations-on-Online-Social-Media"><a href="#OSN-MDAD-Machine-Translation-Dataset-for-Arabic-Multi-Dialectal-Conversations-on-Online-Social-Media" class="headerlink" title="OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal Conversations on Online Social Media"></a>OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal Conversations on Online Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12137">http://arxiv.org/abs/2309.12137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatimah Alzamzami, Abdulmotaleb El Saddik</li>
<li>for: 这个论文主要是为了解决阿拉伯语言的社交媒体翻译问题。</li>
<li>methods: 这篇论文使用的方法是基于上述提出的内容翻译指南，通过对英语推文进行上下文翻译，生成四种阿拉伯语方言的转化。</li>
<li>results: 根据作者的实验结果，使用这些方法可以实现高效的阿拉伯语方言翻译。<details>
<summary>Abstract</summary>
While resources for English language are fairly sufficient to understand content on social media, similar resources in Arabic are still immature. The main reason that the resources in Arabic are insufficient is that Arabic has many dialects in addition to the standard version (MSA). Arabs do not use MSA in their daily communications; rather, they use dialectal versions. Unfortunately, social users transfer this phenomenon into their use of social media platforms, which in turn has raised an urgent need for building suitable AI models for language-dependent applications. Existing machine translation (MT) systems designed for MSA fail to work well with Arabic dialects. In light of this, it is necessary to adapt to the informal nature of communication on social networks by developing MT systems that can effectively handle the various dialects of Arabic. Unlike for MSA that shows advanced progress in MT systems, little effort has been exerted to utilize Arabic dialects for MT systems. While few attempts have been made to build translation datasets for dialectal Arabic, they are domain dependent and are not OSN cultural-language friendly. In this work, we attempt to alleviate these limitations by proposing an online social network-based multidialect Arabic dataset that is crafted by contextually translating English tweets into four Arabic dialects: Gulf, Yemeni, Iraqi, and Levantine. To perform the translation, we followed our proposed guideline framework for content translation, which could be universally applicable for translation between foreign languages and local dialects. We validated the authenticity of our proposed dataset by developing neural MT models for four Arabic dialects. Our results have shown a superior performance of our NMT models trained using our dataset. We believe that our dataset can reliably serve as an Arabic multidialectal translation dataset for informal MT tasks.
</details>
<details>
<summary>摘要</summary>
在英语社交媒体上，资源相对充足，但在阿拉伯语社交媒体上，资源仍然落后。主要原因是阿拉伯语有多种方言，使得社交媒体上的用户通常使用方言而不是标准版本（MSA）。阿拉伯人不使用MSA在日常交流中，而是使用方言版本。这种现象也影响了社交媒体平台上的使用，从而提高了为建立适用于语言依赖应用的AI模型的需求。现有的机器翻译（MT）系统设计 дляMSA无法正确地处理阿拉伯语方言。为此，需要适应社交媒体上的不正式交流方式，并开发MT系统可以有效地处理不同的阿拉伯语方言。与MSA的MT系统进步相比，对阿拉伯语方言的努力很少。尝试了为阿拉伯语方言建立翻译数据集，但这些数据集是域名相关的，并不适合社交媒体文化语言。在这种情况下，我们尝试缓解这些限制，并提出了一个在线社交媒体基础上创建的多方言阿拉伯语数据集。我们采用了我们提出的内部翻译指南，将英语推文翻译成四种阿拉伯语方言：古富、 йемен语、伊拉克语和levantine语。我们验证了我们提出的数据集的 Authenticity，并开发了四种阿拉伯语方言的神经翻译模型。我们的结果表明，我们的NMT模型在使用我们的数据集进行训练时表现出色。我们认为，我们的数据集可靠地作为阿拉伯语多方言翻译数据集来使用。
</details></li>
</ul>
<hr>
<h2 id="A-knowledge-representation-approach-for-construction-contract-knowledge-modeling"><a href="#A-knowledge-representation-approach-for-construction-contract-knowledge-modeling" class="headerlink" title="A knowledge representation approach for construction contract knowledge modeling"></a>A knowledge representation approach for construction contract knowledge modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12132">http://arxiv.org/abs/2309.12132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunmo Zheng, Saika Wong, Xing Su, Yinqiu Tang</li>
<li>for:  automatize construction contract management, reducing human errors and saving time and costs</li>
<li>methods:  uses a nested knowledge representation framework and LLM-assisted contract review pipeline</li>
<li>results:  achieves promising performance in contract risk reviewing, demonstrating the potential of combining LLM and KG for more reliable and interpretable contract management<details>
<summary>Abstract</summary>
The emergence of large language models (LLMs) presents an unprecedented opportunity to automate construction contract management, reducing human errors and saving significant time and costs. However, LLMs may produce convincing yet inaccurate and misleading content due to a lack of domain expertise. To address this issue, expert-driven contract knowledge can be represented in a structured manner to constrain the automatic contract management process. This paper introduces the Nested Contract Knowledge Graph (NCKG), a knowledge representation approach that captures the complexity of contract knowledge using a nested structure. It includes a nested knowledge representation framework, a NCKG ontology built on the framework, and an implementation method. Furthermore, we present the LLM-assisted contract review pipeline enhanced with external knowledge in NCKG. Our pipeline achieves a promising performance in contract risk reviewing, shedding light on the combination of LLM and KG towards more reliable and interpretable contract management.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的出现提供了前所未有的自动化建筑合同管理机会，可以减少人类错误和成本。然而，LLM可能生成吸引人的 yet inaccurate和 misleading 内容，因为缺乏领域专业知识。为解决这个问题，我们可以将专家驱动的合同知识表示在结构化的方式下，以限制自动合同管理过程。本文介绍了嵌入式合同知识图（NCKG），一种知识表示方法，它使用嵌入结构来捕捉合同知识的复杂性。它包括嵌入结构框架、基于框架的 NCKG  Ontology 和实现方法。此外，我们还介绍了利用外部知识的 LLM 协助合同审核管道。我们的管道实现了合同风险审核中的优秀表现，推照在 LLM 和 KG 的结合下，可以实现更可靠和可解释的合同管理。
</details></li>
</ul>
<hr>
<h2 id="Incentivizing-Massive-Unknown-Workers-for-Budget-Limited-Crowdsensing-From-Off-Line-and-On-Line-Perspectives"><a href="#Incentivizing-Massive-Unknown-Workers-for-Budget-Limited-Crowdsensing-From-Off-Line-and-On-Line-Perspectives" class="headerlink" title="Incentivizing Massive Unknown Workers for Budget-Limited Crowdsensing: From Off-Line and On-Line Perspectives"></a>Incentivizing Massive Unknown Workers for Budget-Limited Crowdsensing: From Off-Line and On-Line Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12113">http://arxiv.org/abs/2309.12113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Li, Yuqi Chai, Huan Yang, Pengfei Hu, Lingjie Duan</li>
<li>For: 这个论文是为了解决在具有限制的预算下，面临大量未知工作者的 combinatorial multi-armed bandit (CMAB) 问题。* Methods: 该论文提出了一种基于奖励机制的 Context-Aware CMAB (CACI) 机制，通过在分区的上下文空间中进行exploration-exploitation 质量来办理奖励，以具有效地鼓励大量未知工作者。同时，该机制还在在线设置中进行了扩展，以适应工作者Join或离开系统的动态变化。* Results: 该论文通过理论分析和实验 validate 了其机制的正确性和个人合理性，并且在synthetic和实际数据集上进行了广泛的实验验证。<details>
<summary>Abstract</summary>
Although the uncertainties of the workers can be addressed by the standard Combinatorial Multi-Armed Bandit (CMAB) framework in existing proposals through a trade-off between exploration and exploitation, we may not have sufficient budget to enable the trade-off among the individual workers, especially when the number of the workers is huge while the budget is limited. Moreover, the standard CMAB usually assumes the workers always stay in the system, whereas the workers may join in or depart from the system over time, such that what we have learnt for an individual worker cannot be applied after the worker leaves. To address the above challenging issues, in this paper, we first propose an off-line Context-Aware CMAB-based Incentive (CACI) mechanism. We innovate in leveraging the exploration-exploitation trade-off in a elaborately partitioned context space instead of the individual workers, to effectively incentivize the massive unknown workers with very limited budget. We also extend the above basic idea to the on-line setting where unknown workers may join in or depart from the systems dynamically, and propose an on-line version of the CACI mechanism. Specifically, by the exploitation-exploration trade-off in the context space, we learn to estimate the sensing ability of any unknown worker (even it never appeared in the system before) according to its context information. We perform rigorous theoretical analysis to reveal the upper bounds on the regrets of our CACI mechanisms and to prove their truthfulness and individual rationality, respectively. Extensive experiments on both synthetic and real datasets are also conducted to verify the efficacy of our mechanisms.
</details>
<details>
<summary>摘要</summary>
尽管工人的不确定性可以通过标准的Combinatorial Multi-Armed Bandit（CMAB）框架在现有的提议中Address，但我们可能没有足够的预算来实现这种贸易在各个工人之间，特别是当工人的数量很大而预算受限时。此外，标准的CMAB通常假设工人总是在系统中，而工人可能在系统中加入或离开，这意味着我们对个体工人所学的知识不能在它离开系统后应用。为解决这些挑战，在本文中，我们首先提出了OFF-LINE的Context-Aware CMAB-based Incentive（CACI）机制。我们创新地利用了在分割后的上下文空间中的exploration-exploitation贸易，以有效地鼓励大量的未知工人，并且具有很限制的预算。我们还将上述基本想法扩展到在线设置，在系统中动态加入或离开的未知工人上。 Specifically，通过在上下文空间中的利用-exploration贸易，我们可以根据 Context information来估算任何未知工人（即使它从未在系统中出现过）的感知能力。我们进行了严格的理论分析，以揭示我们CACI机制的误差的Upper bound，并证明它们的真实性和个人合理性，分别。我们还进行了大量的实验，以验证我们的机制的有效性。
</details></li>
</ul>
<hr>
<h2 id="PEFTT-Parameter-Efficient-Fine-Tuning-for-low-resource-Tibetan-pre-trained-language-models"><a href="#PEFTT-Parameter-Efficient-Fine-Tuning-for-low-resource-Tibetan-pre-trained-language-models" class="headerlink" title="PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models"></a>PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12109">http://arxiv.org/abs/2309.12109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhou Mingjun, Daiqing Zhuoma, Qun Nuo, Nyima Tashi</li>
<li>for: 这个研究的目的是为了提高 Tibetan NLP 领域中的模型训练效率，以便更好地应用于语言应用程序。</li>
<li>methods: 这个研究使用了三种有效的 fine-tuning 策略，即 “prompt-tuning”、”Adapter lightweight fine-tuning” 和 “prompt-tuning + Adapter fine-tuning”，以提高 Tibetan 语言模型的性能。</li>
<li>results: 实验结果表明，使用这三种 fine-tuning 策略可以在 TNCC-title 数据集上实现显著的改进，为 Tibetan 语言应用程序提供有价值的意义。<details>
<summary>Abstract</summary>
In this era of large language models (LLMs), the traditional training of models has become increasingly unimaginable for regular users and institutions. The exploration of efficient fine-tuning for high-resource languages on these models is an undeniable trend that is gradually gaining popularity. However, there has been very little exploration for various low-resource languages, such as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While there is currently no existing large language model for Tibetan due to its low-resource nature, that day will undoubtedly arrive. Therefore, research on efficient fine-tuning for low-resource language models like Tibetan is highly necessary. Our research can serve as a reference to fill this crucial gap. Efficient fine-tuning strategies for pre-trained language models (PLMs) in Tibetan have seen minimal exploration. We conducted three types of efficient fine-tuning experiments on the publicly available TNCC-title dataset: "prompt-tuning," "Adapter lightweight fine-tuning," and "prompt-tuning + Adapter fine-tuning." The experimental results demonstrate significant improvements using these methods, providing valuable insights for advancing Tibetan language applications in the context of pre-trained models.
</details>
<details>
<summary>摘要</summary>
在这个大语言模型（LLM）时代，传统的模型训练已经变得越来越难以想象 для普通用户和机构。研究高资源语言模型的高效微调是一种逐渐受欢迎的趋势，但对低资源语言，如藏语，的研究几乎没有任何探索。因此，研究低资源语言模型的高效微调是非常必要的。在目前没有任何藏语大语言模型的情况下，我们的研究可以填补这一关键的空白。关于藏语自然语言处理（NLP）的研究是罕见和有限的。我们在公共可用的 TNCC-title 数据集上进行了三种高效微调实验："提示微调，" "Adapter 轻量级微调，" 和 "提示微调 + Adapter 微调。"实验结果显示了这些方法的显著改善，为 Tibetan 语言应用在预训练模型的上提供了价值的指导。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Thematic-Investment-with-Prompt-Tuned-Pretrained-Language-Models"><a href="#Accelerating-Thematic-Investment-with-Prompt-Tuned-Pretrained-Language-Models" class="headerlink" title="Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models"></a>Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12075">http://arxiv.org/abs/2309.12075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentin Leonhard Buchner, Lele Cao, Jan-Christoph Kalo</li>
<li>for: 本研究使用Prompt Tuning方法进行实验，以解决多标签文本分类 задачі中的一些限制。</li>
<li>methods: 本研究使用Prompt Tuning和基eline方法进行比较，以测试它们在多标签文本分类 задачі中的性能和computational efficiency。</li>
<li>results: 研究结果显示，Prompt Tuning方法可以优化PLMs的性能和computational efficiency，并且可以解决多标签文本分类 задачі中的一些限制。<details>
<summary>Abstract</summary>
Prompt Tuning is emerging as a scalable and cost-effective method to fine-tune Pretrained Language Models (PLMs). This study benchmarks the performance and computational efficiency of Prompt Tuning and baseline methods on a multi-label text classification task. This is applied to the use case of classifying companies into an investment firm's proprietary industry taxonomy, supporting their thematic investment strategy. Text-to-text classification with PLMs is frequently reported to outperform classification with a classification head, but has several limitations when applied to a multi-label classification problem where each label consists of multiple tokens: (a) Generated labels may not match any label in the industry taxonomy; (b) During fine-tuning, multiple labels must be provided in an arbitrary order; (c) The model provides a binary decision for each label, rather than an appropriate confidence score. Limitation (a) is addressed by applying constrained decoding using Trie Search, which slightly improves classification performance. All limitations (a), (b), and (c) are addressed by replacing the PLM's language head with a classification head. This improves performance significantly, while also reducing computational costs during inference. The results indicate the continuing need to adapt state-of-the-art methods to domain-specific tasks, even in the era of PLMs with strong generalization abilities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Benchmarking-quantized-LLaMa-based-models-on-the-Brazilian-Secondary-School-Exam"><a href="#Benchmarking-quantized-LLaMa-based-models-on-the-Brazilian-Secondary-School-Exam" class="headerlink" title="Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam"></a>Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12071">http://arxiv.org/abs/2309.12071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matheus L. O. Santos, Cláudio E. C. Campelo</li>
<li>for: 本研究旨在评估基于7和13亿LLaMA模型的大语言模型（LLMs）在家用硬件上的性能。</li>
<li>methods: 我们使用了一个包含1,006个问题的数据库，来评估这些模型的有效性。我们还测试了这些模型的计算效率，并记录了在一台配备AMD Ryzen 5 3600x处理器的机器上处理查询所需的时间。</li>
<li>results: 我们发现，最佳performing模型在原始葡萄牙语问题上的准确率约为46%，而在其英文翻译中的准确率约为49%。此外，我们发现7和13亿LLMs在这些查询中的计算效率相对较高，它们在一台机器上平均需要20和50秒时间来处理查询。<details>
<summary>Abstract</summary>
Although Large Language Models (LLMs) represent a revolution in the way we interact with computers, allowing the construction of complex questions and the ability to reason over a sequence of statements, their use is restricted due to the need for dedicated hardware for execution. In this study, we evaluate the performance of LLMs based on the 7 and 13 billion LLaMA models, subjected to a quantization process and run on home hardware. The models considered were Alpaca, Koala, and Vicuna. To evaluate the effectiveness of these models, we developed a database containing 1,006 questions from the ENEM (Brazilian National Secondary School Exam). Our analysis revealed that the best performing models achieved an accuracy of approximately 46% for the original texts of the Portuguese questions and 49% on their English translations. In addition, we evaluated the computational efficiency of the models by measuring the time required for execution. On average, the 7 and 13 billion LLMs took approximately 20 and 50 seconds, respectively, to process the queries on a machine equipped with an AMD Ryzen 5 3600x processor
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:尽管大型语言模型（LLMs）代表了计算机与人类之间交互的革命，允许建构复杂的问题并对语句进行推理，但它们的使用受到硬件限制。在这项研究中，我们评估了基于7和13亿LLaMA模型的LMMs，经过量化处理并在家用硬件上运行。我们考虑的模型包括Alpaca、Koala和Vicuna。为了评估这些模型的效果，我们创建了包含1,006个问题的ENEM（巴西国家高中考试）数据库。我们的分析表明，最佳表现的模型在原始葡萄牙语问题上达到了约46%的准确率，而在英语翻译中达到了约49%的准确率。此外，我们还评估了这些模型的计算效率，并测量了在一台配备AMD Ryzen 5 3600x处理器的机器上执行查询所需的时间。平均而言，7和13亿LLMs在20和50秒之间执行查询。
</details></li>
</ul>
<hr>
<h2 id="Survey-of-Action-Recognition-Spotting-and-Spatio-Temporal-Localization-in-Soccer-–-Current-Trends-and-Research-Perspectives"><a href="#Survey-of-Action-Recognition-Spotting-and-Spatio-Temporal-Localization-in-Soccer-–-Current-Trends-and-Research-Perspectives" class="headerlink" title="Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer – Current Trends and Research Perspectives"></a>Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer – Current Trends and Research Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12067">http://arxiv.org/abs/2309.12067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karolina Seweryn, Anna Wróblewska, Szymon Łukasik</li>
<li>for: 本文提供了足球动作识别领域的全面回顾，包括动作识别、位置识别和时空动作地址Localization，特别是modalities使用和多modal方法。</li>
<li>methods: 文章详细介绍了公共可用数据源和评价模型性能的度量，同时还探讨了最新的state-of-the-art方法，包括深度学习技术和传统方法。文章特别强调了多modal方法，这些方法将多个源信息 integrate into one model，如视频和音频数据。</li>
<li>results: 文章评论了不同方法的优势和局限性，以及它们在准确性和Robustness方面的潜在提升。最后，文章提出了一些未解决的问题和未来方向，包括多modal方法在足球动作识别领域的潜在推动作用。<details>
<summary>Abstract</summary>
Action scene understanding in soccer is a challenging task due to the complex and dynamic nature of the game, as well as the interactions between players. This article provides a comprehensive overview of this task divided into action recognition, spotting, and spatio-temporal action localization, with a particular emphasis on the modalities used and multimodal methods. We explore the publicly available data sources and metrics used to evaluate models' performance. The article reviews recent state-of-the-art methods that leverage deep learning techniques and traditional methods. We focus on multimodal methods, which integrate information from multiple sources, such as video and audio data, and also those that represent one source in various ways. The advantages and limitations of methods are discussed, along with their potential for improving the accuracy and robustness of models. Finally, the article highlights some of the open research questions and future directions in the field of soccer action recognition, including the potential for multimodal methods to advance this field. Overall, this survey provides a valuable resource for researchers interested in the field of action scene understanding in soccer.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chineseoccer场景理解是一项复杂和动态的任务，由于游戏中玩家之间的互动和多方面的关系。这篇文章提供了全面的概述，分为行为识别、识别和空间时间行为定位，强调modalities使用和多模态方法。我们探讨了公共可用数据源和评估模型性能的度量，并评论了最新的状态艺术方法，包括深度学习技术和传统方法。我们专注于多模态方法，这些方法将多个来源的信息集成，如视频和音频数据，以及表示一种来源的不同方式。我们讨论了方法的优势和局限性，以及它们在准确性和可靠性方面的潜在提高。最后，文章强调了在足球行为识别领域的一些未解决问题和未来方向，包括多模态方法的潜在推动该领域的前景。总之，这篇文章为研究者们关注足球场景理解领域提供了有价值的资源。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Consolidation-of-Word-Embedding-and-Deep-Learning-Techniques-for-Classifying-Anticancer-Peptides-FastText-BiLSTM"><a href="#An-Efficient-Consolidation-of-Word-Embedding-and-Deep-Learning-Techniques-for-Classifying-Anticancer-Peptides-FastText-BiLSTM" class="headerlink" title="An Efficient Consolidation of Word Embedding and Deep Learning Techniques for Classifying Anticancer Peptides: FastText+BiLSTM"></a>An Efficient Consolidation of Word Embedding and Deep Learning Techniques for Classifying Anticancer Peptides: FastText+BiLSTM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12058">http://arxiv.org/abs/2309.12058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Onur Karakaya, Zeynep Hilal Kilimci</li>
<li>for: 预防和治疗癌症的抗癌肽（ACPs）</li>
<li>methods: 使用word embedding和深度学习模型进行抗癌肽分类</li>
<li>results: 提出了一种高精度的抗癌肽分类模型，在常用的数据集上达到了新的州OF-THE-ART水平，即ACPs250数据集的准确率为92.50%，独立数据集的准确率为96.15%。<details>
<summary>Abstract</summary>
Anticancer peptides (ACPs) are a group of peptides that exhibite antineoplastic properties. The utilization of ACPs in cancer prevention can present a viable substitute for conventional cancer therapeutics, as they possess a higher degree of selectivity and safety. Recent scientific advancements generate an interest in peptide-based therapies which offer the advantage of efficiently treating intended cells without negatively impacting normal cells. However, as the number of peptide sequences continues to increase rapidly, developing a reliable and precise prediction model becomes a challenging task. In this work, our motivation is to advance an efficient model for categorizing anticancer peptides employing the consolidation of word embedding and deep learning models. First, Word2Vec and FastText are evaluated as word embedding techniques for the purpose of extracting peptide sequences. Then, the output of word embedding models are fed into deep learning approaches CNN, LSTM, BiLSTM. To demonstrate the contribution of proposed framework, extensive experiments are carried on widely-used datasets in the literature, ACPs250 and Independent. Experiment results show the usage of proposed model enhances classification accuracy when compared to the state-of-the-art studies. The proposed combination, FastText+BiLSTM, exhibits 92.50% of accuracy for ACPs250 dataset, and 96.15% of accuracy for Independent dataset, thence determining new state-of-the-art.
</details>
<details>
<summary>摘要</summary>
安定肽（ACPs）是一组肽蛋白，具有抗肿瘤性能。使用ACPs在抗癌治疗中可能成为一种可靠的替代方案，因为它们具有更高的选择性和安全性。随着肽序列的数量在快速增长，开发一个可靠和精准的预测模型成为一项挑战。在这种情况下，我们的动机是提出一种高效的分类模型，使用词嵌入和深度学习模型。首先，我们评估了Word2Vec和FastText作为词嵌入技术，以提取肽序列。然后，Word2Vec和FastText模型的输出被 fed into深度学习模型CNN、LSTM、BiLSTM。为证明提案的贡献，我们在文献中常用的数据集上进行了广泛的实验。实验结果表明，我们的模型可以提高分类精度，比领先研究更高。具体来说，使用我们的模型，ACPS250数据集的准确率达92.50%，独立数据集的准确率达96.15%，从而确定新的顶峰。
</details></li>
</ul>
<hr>
<h2 id="BELT-Bootstrapping-Electroencephalography-to-Language-Decoding-and-Zero-Shot-Sentiment-Classification-by-Natural-Language-Supervision"><a href="#BELT-Bootstrapping-Electroencephalography-to-Language-Decoding-and-Zero-Shot-Sentiment-Classification-by-Natural-Language-Supervision" class="headerlink" title="BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision"></a>BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12056">http://arxiv.org/abs/2309.12056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinzhao Zhou, Yiqun Duan, Yu-Cheng Chang, Yu-Kai Wang, Chin-Teng Lin</li>
<li>for: 本研究旨在提出一种基于大规模预训练语言模型的脑语言翻译模型和学习框架（BELT），以解决脑信号解码或脑语言翻译中的限制性问题。</li>
<li>methods: 该模型由深度套件编码器和量化编码器组成，通过对比学习步骤提供自然语言监督，以实现semantic EEG表示。</li>
<li>results: 在两个特征性脑解码任务上，包括脑语言翻译和零扫ENT sentiment classification，OUR模型比基eline模型提高5.45%和over 10%，并在翻译任务上实现42.31% BLEU-1分数和67.32%精度。<details>
<summary>Abstract</summary>
This paper presents BELT, a novel model and learning framework for the pivotal topic of brain-to-language translation research. The translation from noninvasive brain signals into readable natural language has the potential to promote the application scenario as well as the development of brain-computer interfaces (BCI) as a whole. The critical problem in brain signal decoding or brain-to-language translation is the acquisition of semantically appropriate and discriminative EEG representation from a dataset of limited scale and quality. The proposed BELT method is a generic and efficient framework that bootstraps EEG representation learning using off-the-shelf large-scale pretrained language models (LMs). With a large LM's capacity for understanding semantic information and zero-shot generalization, BELT utilizes large LMs trained on Internet-scale datasets to bring significant improvements to the understanding of EEG signals.   In particular, the BELT model is composed of a deep conformer encoder and a vector quantization encoder. Semantical EEG representation is achieved by a contrastive learning step that provides natural language supervision. We achieve state-of-the-art results on two featuring brain decoding tasks including the brain-to-language translation and zero-shot sentiment classification. Specifically, our model surpasses the baseline model on both tasks by 5.45% and over 10% and archives a 42.31% BLEU-1 score and 67.32% precision on the main evaluation metrics for translation and zero-shot sentiment classification respectively.
</details>
<details>
<summary>摘要</summary>
To address this problem, the BELT method leverages off-the-shelf large-scale pretrained language models (LMs) to bootstrap EEG representation learning. With the large capacity of LMs for understanding semantic information and their ability to generalize to new situations, BELT can significantly improve the understanding of EEG signals.The BELT model consists of a deep conformer encoder and a vector quantization encoder. To achieve semantically meaningful EEG representations, a contrastive learning step is used to provide natural language supervision. The model is evaluated on two brain decoding tasks, brain-to-language translation and zero-shot sentiment classification, and achieves state-of-the-art results. Specifically, the model outperforms the baseline model by 5.45% and over 10% on both tasks, with a BLEU-1 score of 42.31% and precision of 67.32% for translation and zero-shot sentiment classification, respectively.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-driven-Exploration-Strategies-for-Online-Grasp-Learning"><a href="#Uncertainty-driven-Exploration-Strategies-for-Online-Grasp-Learning" class="headerlink" title="Uncertainty-driven Exploration Strategies for Online Grasp Learning"></a>Uncertainty-driven Exploration Strategies for Online Grasp Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12038">http://arxiv.org/abs/2309.12038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yitian Shi, Philipp Schillinger, Miroslav Gabriel, Alexander Kuss, Zohar Feldman, Hanna Ziesche, Ngo Anh Vien</li>
<li>for: 本研究旨在提出一种在线学习 grasp 预测方法，以适应 robotic bin picking 中新的抓取enario。</li>
<li>methods: 本方法基于 Reinforcement Learning 的online学习策略，并采用有效的探索策略来改善适应性。</li>
<li>results: 实验结果表明，提出的方法可以在实际的 bin picking 场景中具有显著的改善，并且比传统的在线学习方法具有更高的适应性。<details>
<summary>Abstract</summary>
Existing grasp prediction approaches are mostly based on offline learning, while, ignored the exploratory grasp learning during online adaptation to new picking scenarios, i.e., unseen object portfolio, camera and bin settings etc. In this paper, we present a novel method for online learning of grasp predictions for robotic bin picking in a principled way. Existing grasp prediction approaches are mostly based on offline learning, while, ignored the exploratory grasp learning during online adaptation to new picking scenarios, i.e., unseen object portfolio, camera and bin settings etc. In this paper, we present a novel method for online learning of grasp predictions for robotic bin picking in a principled way. Specifically, the online learning algorithm with an effective exploration strategy can significantly improve its adaptation performance to unseen environment settings. To this end, we first propose to formulate online grasp learning as a RL problem that will allow to adapt both grasp reward prediction and grasp poses. We propose various uncertainty estimation schemes based on Bayesian Uncertainty Quantification and Distributional Ensembles. We carry out evaluations on real-world bin picking scenes of varying difficulty. The objects in the bin have various challenging physical and perceptual characteristics that can be characterized by semi- or total transparency, and irregular or curved surfaces. The results of our experiments demonstrate a notable improvement in the suggested approach compared to conventional online learning methods which incorporate only naive exploration strategies.
</details>
<details>
<summary>摘要</summary>
现有的握取预测方法大多基于离线学习，而忽略了在线适应新抓取场景时的探索式握取学习。在这篇论文中，我们提出了一种新的在线学习握取预测方法，以解决这个问题。特别是，我们提出了一种有效的探索策略，可以显著提高在未经见的环境设置下的适应性。为此，我们首先将在线握取学习转化为一种RL问题，以适应握取奖励预测和握取姿态。我们还提出了多种不确定度估计方法，基于 bayesian不确定度量化和分布 ensemble。我们在实际的 bin picking 场景中进行了评估，结果显示，我们的方法在不同的难度水平下表现出了显著的改善。Here's a word-for-word translation of the text using Traditional Chinese characters:现有的握取预测方法大多基于离线学习，而忽略了在线适应新捕取场景时的探索式握取学习。在这篇论文中，我们提出了一种新的在线学习握取预测方法，以解决这个问题。特别是，我们提出了一种有效的探索策略，可以明显提高在未经见的环境设置下的适应性。为此，我们首先将在线握取学习转换为一个RL问题，以适应握取奖励预测和握取姿势。我们还提出了多种不确定度估计方法，基于 bayesian不确定度量化和分布ensemble。我们在实际的 bin picking 场景中进行了评估，结果显示，我们的方法在不同的难度水平下表现出了明显的改善。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Hypergraph-Structure-Learning-for-Traffic-Flow-Forecasting"><a href="#Dynamic-Hypergraph-Structure-Learning-for-Traffic-Flow-Forecasting" class="headerlink" title="Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting"></a>Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12028">http://arxiv.org/abs/2309.12028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusheng Zhao, Xiao Luo, Wei Ju, Chong Chen, Xian-Sheng Hua, Ming Zhang</li>
<li>for: 预测未来交通情况，基于路网和过去交通情况来预测未来交通状况。</li>
<li>methods: 使用强化图学习方法，抽取高维图 струкural信息，模型交通网络中的动态关系，并通过互动图卷积块来模型不同时间的交通关系。</li>
<li>results: 与竞争对手比较，实验结果表明，提出的 DyHSL 方法在四个流行的交通数据集上具有更高的预测精度和稳定性。<details>
<summary>Abstract</summary>
This paper studies the problem of traffic flow forecasting, which aims to predict future traffic conditions on the basis of road networks and traffic conditions in the past. The problem is typically solved by modeling complex spatio-temporal correlations in traffic data using spatio-temporal graph neural networks (GNNs). However, the performance of these methods is still far from satisfactory since GNNs usually have limited representation capacity when it comes to complex traffic networks. Graphs, by nature, fall short in capturing non-pairwise relations. Even worse, existing methods follow the paradigm of message passing that aggregates neighborhood information linearly, which fails to capture complicated spatio-temporal high-order interactions. To tackle these issues, in this paper, we propose a novel model named Dynamic Hypergraph Structure Learning (DyHSL) for traffic flow prediction. To learn non-pairwise relationships, our DyHSL extracts hypergraph structural information to model dynamics in the traffic networks, and updates each node representation by aggregating messages from its associated hyperedges. Additionally, to capture high-order spatio-temporal relations in the road network, we introduce an interactive graph convolution block, which further models the neighborhood interaction for each node. Finally, we integrate these two views into a holistic multi-scale correlation extraction module, which conducts temporal pooling with different scales to model different temporal patterns. Extensive experiments on four popular traffic benchmark datasets demonstrate the effectiveness of our proposed DyHSL compared with a broad range of competing baselines.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这篇论文研究了交通流量预测问题，目的是根据过去的道路网络和交通情况预测未来交通情况。传统的方法使用空间时间图内存神经网络（GNN），但其表现仍然不够满意，因为GNN通常在复杂的交通网络上具有有限的表示能力。为解决这个问题，我们在这篇论文中提出了一种名为动态质量结构学习（DyHSL）的新模型，用于预测交通流量。DyHSL使用质量结构信息来模型交通网络中的动态，并为每个节点更新表示信息，通过聚合与其相关的质量结构上的消息。此外，我们还引入了一种互动图卷积块，用于模型每个节点的邻居交互。最后，我们将这两个视角集成到一个整体多级相关提取模块中，并进行不同的时间膨胀来模型不同的时间模式。我们对四个流行的交通测试集进行了广泛的实验，并证明了我们提出的DyHSL在与一系列基线模型进行比较时的效果。
</details></li>
</ul>
<hr>
<h2 id="Demystifying-Visual-Features-of-Movie-Posters-for-Multi-Label-Genre-Identification"><a href="#Demystifying-Visual-Features-of-Movie-Posters-for-Multi-Label-Genre-Identification" class="headerlink" title="Demystifying Visual Features of Movie Posters for Multi-Label Genre Identification"></a>Demystifying Visual Features of Movie Posters for Multi-Label Genre Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12022">http://arxiv.org/abs/2309.12022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Utsav Kumar Nareti, Chandranath Adak, Soumi Chattopadhyay</li>
<li>for: 这 paper 是用于自动多标签电影类型识别，只使用电影海报图像，无需其他文本&#x2F;元数据信息。</li>
<li>methods: 这 paper 使用了深度变换器网络，并添加了概率模块来识别电影类型。</li>
<li>results: 在实验分析中，模型在13882张电影海报图像上表现出了鼓舞人心的result，甚至超过了一些当前主流架构。<details>
<summary>Abstract</summary>
In the film industry, movie posters have been an essential part of advertising and marketing for many decades, and continue to play a vital role even today in the form of digital posters through online, social media and OTT platforms. Typically, movie posters can effectively promote and communicate the essence of a film, such as its genre, visual style/ tone, vibe and storyline cue/ theme, which are essential to attract potential viewers. Identifying the genres of a movie often has significant practical applications in recommending the film to target audiences. Previous studies on movie genre identification are limited to subtitles, plot synopses, and movie scenes that are mostly accessible after the movie release. Posters usually contain pre-release implicit information to generate mass interest. In this paper, we work for automated multi-label genre identification only from movie poster images, without any aid of additional textual/meta-data information about movies, which is one of the earliest attempts of its kind. Here, we present a deep transformer network with a probabilistic module to identify the movie genres exclusively from the poster. For experimental analysis, we procured 13882 number of posters of 13 genres from the Internet Movie Database (IMDb), where our model performances were encouraging and even outperformed some major contemporary architectures.
</details>
<details>
<summary>摘要</summary>
在电影业内，电影海报已经是广告和市场营销的重要组成部分，并且在今天的形式中仍然扮演着重要的角色，包括数字海报通过在线、社交媒体和OTT平台。通常，电影海报可以有效地推广和传达电影的核心元素，如其类型、视觉风格/调子、氛围和故事线索/主题，这些元素都是吸引 posible viewers 的关键。识别电影的类型有很多实际应用，例如推荐电影给target audience。过去的研究主要集中在电影场景、剧本简介和电影片段等，这些信息都是电影发布后才可以获取。海报通常包含在电影发布之前的隐式信息，以便引起大量的兴趣。在这篇论文中，我们采用了自动化多个标签类别识别方法，只使用电影海报图像，不需要任何其他电影相关的文本/元数据信息，这是目前已知的其中之一。我们在实验分析中使用了13882张海报图像，来自互联网电影数据库（IMDb），我们的模型表现很 Encouraging，甚至超过了一些当前的主流架构。
</details></li>
</ul>
<hr>
<h2 id="Safe-Hierarchical-Reinforcement-Learning-for-CubeSat-Task-Scheduling-Based-on-Energy-Consumption"><a href="#Safe-Hierarchical-Reinforcement-Learning-for-CubeSat-Task-Scheduling-Based-on-Energy-Consumption" class="headerlink" title="Safe Hierarchical Reinforcement Learning for CubeSat Task Scheduling Based on Energy Consumption"></a>Safe Hierarchical Reinforcement Learning for CubeSat Task Scheduling Based on Energy Consumption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12004">http://arxiv.org/abs/2309.12004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahya Ramezani, M. Amin Alandihallaj, Jose Luis Sanchez-Lopez, Andreas Hein</li>
<li>for: 优化CubeSat任务调度在低地球轨道（LEO）中</li>
<li>methods: 基于层次强化学习的方法，包括全球任务分布高级政策和实时适应低级政策作为安全机制，同时包括相似注意力基于编码器（SABE） для任务优先级调整和多层扩散器（MLP）估计器 для能量消耗预测</li>
<li>results: 对多个CubeSat配置进行了仿真测试，结果表明层次强化学习方法在任务 converges 和成功率方面具有超越MADDPG模型和随机调度的优势。<details>
<summary>Abstract</summary>
This paper presents a Hierarchical Reinforcement Learning methodology tailored for optimizing CubeSat task scheduling in Low Earth Orbits (LEO). Incorporating a high-level policy for global task distribution and a low-level policy for real-time adaptations as a safety mechanism, our approach integrates the Similarity Attention-based Encoder (SABE) for task prioritization and an MLP estimator for energy consumption forecasting. Integrating this mechanism creates a safe and fault-tolerant system for CubeSat task scheduling. Simulation results validate the Hierarchical Reinforcement Learning superior convergence and task success rate, outperforming both the MADDPG model and traditional random scheduling across multiple CubeSat configurations.
</details>
<details>
<summary>摘要</summary>
本文提出了一种层次优化方法，用于优化 LEO 牛顿级卫星任务调度。该方法包括一个高级策略用于全球任务分配和一个低级策略用于实时调整作为安全机制，并 integrates 类似注意力基于编码器（SABE）用于任务优先级调度和一个 MLP 预测器用于能源消耗预测。通过这种机制，我们构建了一个安全和可靠的 CubeSat 任务调度系统。实验结果表明，层次优化学习超越了 MADDPG 模型和随机调度，在多种 CubeSat 配置下实现了更高的任务成功率。
</details></li>
</ul>
<hr>
<h2 id="LMSYS-Chat-1M-A-Large-Scale-Real-World-LLM-Conversation-Dataset"><a href="#LMSYS-Chat-1M-A-Large-Scale-Real-World-LLM-Conversation-Dataset" class="headerlink" title="LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset"></a>LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11998">http://arxiv.org/abs/2309.11998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric. P Xing, Joseph E. Gonzalez, Ion Stoica, Hao Zhang</li>
<li>For: 这个论文的目的是为了研究人们在真实场景中与大型自然语言模型（LLM）进行交互的方式。* Methods: 这篇论文使用了25种当前最先进的LMM进行实际场景中的交互，并从210K个唯一的IP地址中收集了100万个对话。* Results: 论文提供了这个数据集的概述，包括其批处程序、基本统计和话题分布，并通过四个用例展示了这个数据集的多样性和大小，包括开发内容审核模型、建立安全标准、训练遵循 instrux 模型和创建挑战性的问题集。<details>
<summary>Abstract</summary>
Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at \url{https://huggingface.co/datasets/lmsys/lmsys-chat-1m}.
</details>
<details>
<summary>摘要</summary>
我们将介绍一个名为LMSYS-Chat-1M的大规模数据集，它包含25个现代大语言模型在实际场景中的一百万个对话。这个数据集是从我们的Vicuna demo和Chatbot Arena网站上获取的210K个唯一的IP地址。我们将介绍这个数据集的内容，包括它的数据收集过程、基本统计和主题分布，并强调其多样性、原始性和规模。我们还会显示这个数据集的多方位性，包括发展内容审核模型，建立安全参考基准，训练遵循命令的模型，和创建挑战性的问题。我们认为这个数据集将成为大语言模型能力的理解和进步的重要资源。这个数据集现在在\url{https://huggingface.co/datasets/lmsys/lmsys-chat-1m}上公开提供。
</details></li>
</ul>
<hr>
<h2 id="Predictability-and-Comprehensibility-in-Post-Hoc-XAI-Methods-A-User-Centered-Analysis"><a href="#Predictability-and-Comprehensibility-in-Post-Hoc-XAI-Methods-A-User-Centered-Analysis" class="headerlink" title="Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis"></a>Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11987">http://arxiv.org/abs/2309.11987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anahid Jalali, Bernhard Haslhofer, Simone Kriglstein, Andreas Rauber</li>
<li>for: 本研究旨在评估黑盒机器学习模型预测结果的后续解释方法的可解性和预测能力。</li>
<li>methods: 研究使用了两种广泛使用的工具：LIME和SHAP。研究还 investigate了对于用户理解和预测模型行为的影响。</li>
<li>results: 研究发现，SHAP的可解性在模型决策边界附近受到显著降低。此外，对比例解释和错误类型的影响也发现了增加用户理解模型行为的能力。基于研究结果，还提出了未来 posterior explainability 方法的设计建议。<details>
<summary>Abstract</summary>
Post-hoc explainability methods aim to clarify predictions of black-box machine learning models. However, it is still largely unclear how well users comprehend the provided explanations and whether these increase the users ability to predict the model behavior. We approach this question by conducting a user study to evaluate comprehensibility and predictability in two widely used tools: LIME and SHAP. Moreover, we investigate the effect of counterfactual explanations and misclassifications on users ability to understand and predict the model behavior. We find that the comprehensibility of SHAP is significantly reduced when explanations are provided for samples near a model's decision boundary. Furthermore, we find that counterfactual explanations and misclassifications can significantly increase the users understanding of how a machine learning model is making decisions. Based on our findings, we also derive design recommendations for future post-hoc explainability methods with increased comprehensibility and predictability.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Post-hoc explainability methods aim to clarify predictions of black-box machine learning models. However, it is still largely unclear how well users comprehend the provided explanations and whether these increase the users ability to predict the model behavior. We approach this question by conducting a user study to evaluate comprehensibility and predictability in two widely used tools: LIME and SHAP. Moreover, we investigate the effect of counterfactual explanations and misclassifications on users ability to understand and predict the model behavior. We find that the comprehensibility of SHAP is significantly reduced when explanations are provided for samples near a model's decision boundary. Furthermore, we find that counterfactual explanations and misclassifications can significantly increase the users understanding of how a machine learning model is making decisions. Based on our findings, we also derive design recommendations for future post-hoc explainability methods with increased comprehensibility and predictability." into Simplified Chinese.翻译文本为简化字的中文：Post-hoc解释方法旨在解释黑盒机器学习模型的预测结果。然而，目前还不清楚用户对提供的解释是否能够准确理解和预测模型的行为。我们通过进行用户研究，评估了两种广泛使用的工具：LIME和SHAP的可 comprendibility和预测能力。此外，我们还研究了对用户的counterfactual解释和错误分类对用户理解和预测模型行为的影响。我们发现，SHAP的可 comprendibility在解释靠近决策边界时显著降低。此外，我们发现，counterfactual解释和错误分类可以帮助用户更好地理解机器学习模型如何做出决策。基于我们的发现，我们还提出了未来post-hoc解释方法的设计建议，以提高可 comprendibility和预测能力。
</details></li>
</ul>
<hr>
<h2 id="Representation-Abstractions-as-Incentives-for-Reinforcement-Learning-Agents-A-Robotic-Grasping-Case-Study"><a href="#Representation-Abstractions-as-Incentives-for-Reinforcement-Learning-Agents-A-Robotic-Grasping-Case-Study" class="headerlink" title="Representation Abstractions as Incentives for Reinforcement Learning Agents: A Robotic Grasping Case Study"></a>Representation Abstractions as Incentives for Reinforcement Learning Agents: A Robotic Grasping Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11984">http://arxiv.org/abs/2309.11984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Panagiotis Petropoulakis, Ludwig Gräf, Josip Josifovski, Mohammadhossein Malmir, Alois Knoll</li>
<li>for: 这项研究的目的是探讨RL机器人在不同状态表示方法下解决反极和平面物捕获任务的能力。</li>
<li>methods: 该研究使用了模型基于的方法、数字化的方法和图像基于的方法来表示环境，并对这些方法的影响进行了调查。</li>
<li>results: 研究发现RL机器人使用数字化状态可以与非学习基eline相当，而使用图像基于的状态从预训练环境向量得到的表示 perfoms更好，并且假设任务特定的知识是控制机器人成功的关键因素。<details>
<summary>Abstract</summary>
Choosing an appropriate representation of the environment for the underlying decision-making process of the \gls{RL} agent is not always straightforward. The state representation should be inclusive enough to allow the agent to informatively decide on its actions and compact enough to increase sample efficiency for policy training. Given this outlook, this work examines the effect of various state representations in incentivizing the agent to solve a specific robotic task: antipodal and planar object grasping. A continuum of state representation abstractions is defined, starting from a model-based approach with complete system knowledge, through hand-crafted numerical, to image-based representations with decreasing level of induced task-specific knowledge. We examine the effects of each representation in the ability of the agent to solve the task in simulation and the transferability of the learned policy to the real robot. The results show that RL agents using numerical states can perform on par with non-learning baselines. Furthermore, we find that agents using image-based representations from pre-trained environment embedding vectors perform better than end-to-end trained agents, and hypothesize that task-specific knowledge is necessary for achieving convergence and high success rates in robot control. Supplementary material can be found at the project webpage: https://github.com/PetropoulakisPanagiotis/igae.
</details>
<details>
<summary>摘要</summary>
选择合适的环境表示方式 дляRL代理的基础决策过程并不总是 straightforward。状态表示应该包含足够的信息，使代理能够有优决策，同时也应该尽量紧凑，以提高策略训练的示例效率。在这个视角下，本工作研究了不同的状态表示方式对RL代理解决特有的 робо控制任务：把物 grasping。我们定义了一种维度的状态表示抽象continuum，从模型基于的方法（完全系统知识），通过手工制定的数值，到图像基于的表示（减少任务特定知识）。我们研究了每种表示方式对代理解决任务的能力，以及这些策略在真实机器人上的可转移性。结果表明RL代理使用数值状态可以与非学习基线一样高效，而使用图像基于的表示从预训练环境嵌入向量而来的代理，则比末端培训的代理更高效。我们认为任务特定的知识是控制机器人高效和具有高成功率的关键。补充材料可以在项目网页上找到：https://github.com/PetropoulakisPanagiotis/igae。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-the-Evaluating-Framework-for-Natural-Language-Understanding-in-AI-Systems-Language-Acquisition-as-a-Core-for-Future-Metrics"><a href="#Rethinking-the-Evaluating-Framework-for-Natural-Language-Understanding-in-AI-Systems-Language-Acquisition-as-a-Core-for-Future-Metrics" class="headerlink" title="Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics"></a>Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11981">http://arxiv.org/abs/2309.11981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patricio Vera, Pedro Moya, Lisa Barraza</li>
<li>for: 本研究旨在探讨人工智能（AI）领域内大语言模型（LLMs）的进步对传统机器智能评价指标的影响，并提出一种基于语言学习和理解的新评价框架。</li>
<li>methods: 本研究采用了多种学科的优秀成果，包括语言学习和理解、自然语言处理（NLP）等，以探讨传统机器智能评价指标的缺陷和不足，并提出一种更加全面和可持续的评价方法。</li>
<li>results: 本研究提出了一种基于语言学习和理解的新评价框架，可以更好地评价机器智能的能力和表现，并且可以帮助解决传统机器智能评价指标的缺陷和不足。<details>
<summary>Abstract</summary>
In the burgeoning field of artificial intelligence (AI), the unprecedented progress of large language models (LLMs) in natural language processing (NLP) offers an opportunity to revisit the entire approach of traditional metrics of machine intelligence, both in form and content. As the realm of machine cognitive evaluation has already reached Imitation, the next step is an efficient Language Acquisition and Understanding. Our paper proposes a paradigm shift from the established Turing Test towards an all-embracing framework that hinges on language acquisition, taking inspiration from the recent advancements in LLMs. The present contribution is deeply tributary of the excellent work from various disciplines, point out the need to keep interdisciplinary bridges open, and delineates a more robust and sustainable approach.
</details>
<details>
<summary>摘要</summary>
在人工智能（AI）领域的不断发展中，大型自然语言处理（NLP）模型的前无古人成就，对传统机器智能评估 metric 的形式和内容都提供了机遇。随着机器认知领域已经达到仿制的水平，接下来的核心任务是语言学习和理解。我们的论文提议一种借鉴现代 LLM 的概念shift，强调语言学习，而不是已有的图灵测试。我们的贡献受到了不同领域的出色工作的推动，要保持交往桥梁打开，并提出了更加坚强和可持续的方法。
</details></li>
</ul>
<hr>
<h2 id="Inferring-Capabilities-from-Task-Performance-with-Bayesian-Triangulation"><a href="#Inferring-Capabilities-from-Task-Performance-with-Bayesian-Triangulation" class="headerlink" title="Inferring Capabilities from Task Performance with Bayesian Triangulation"></a>Inferring Capabilities from Task Performance with Bayesian Triangulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11975">http://arxiv.org/abs/2309.11975</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Burden, Konstantinos Voudouris, Ryan Burnell, Danaja Rutar, Lucy Cheke, José Hernández-Orallo</li>
<li>for: 这篇论文旨在Characterizing machine learning models in richer, more meaningful ways, and describing a method to infer the cognitive profile of a system from diverse experimental data.</li>
<li>methods: 该方法使用了 measurement layouts to model how task-instance features interact with system capabilities, and used Bayesian probabilistic programming library PyMC to infer different cognitive profiles for agents in two scenarios.</li>
<li>results: 研究人员通过使用这种方法，能够从非常复杂的数据中推断出系统的能力profile，并在两个场景中显示了 capability-oriented evaluation 的潜力。<details>
<summary>Abstract</summary>
As machine learning models become more general, we need to characterise them in richer, more meaningful ways. We describe a method to infer the cognitive profile of a system from diverse experimental data. To do so, we introduce measurement layouts that model how task-instance features interact with system capabilities to affect performance. These features must be triangulated in complex ways to be able to infer capabilities from non-populational data -- a challenge for traditional psychometric and inferential tools. Using the Bayesian probabilistic programming library PyMC, we infer different cognitive profiles for agents in two scenarios: 68 actual contestants in the AnimalAI Olympics and 30 synthetic agents for O-PIAAGETS, an object permanence battery. We showcase the potential for capability-oriented evaluation.
</details>
<details>
<summary>摘要</summary>
（machine learning models become more general, we need to characterize them in richer, more meaningful ways） machine learning模型变得更加通用，我们需要对其进行更加丰富、有意义的描述。我们介绍了一种方法，用于从多种实验数据中推断系统的认知profile。为此，我们引入了任务实例特征与系统能力之间的测量布局，以影响性能。这些特征需要在复杂的方式进行拟合，以便从非流行数据中推断能力——传统心理测量和推断工具所面临的挑战。使用PyMC的 bayesian概率编程库，我们对 AnimalAI奥运会中的68名实际参赛者和O-PIAAGETS对象永恒测试中的30名 sintetic agents进行了不同的认知 profiling。我们展示了可能性-oriented评估的潜力。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Review-on-Financial-Explainable-AI"><a href="#A-Comprehensive-Review-on-Financial-Explainable-AI" class="headerlink" title="A Comprehensive Review on Financial Explainable AI"></a>A Comprehensive Review on Financial Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11960">http://arxiv.org/abs/2309.11960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jie Yeo, Wihan van der Heever, Rui Mao, Erik Cambria, Ranjan Satapathy, Gianmarco Mengaldo</li>
<li>for: 该论文主要旨在提供一种对深度学习模型可读性的比较survey，以便在金融领域内采用可读性AI方法。</li>
<li>methods: 该论文分析了一些提高深度学习模型可读性的方法，并将其分为不同的特征类型。</li>
<li>results: 该论文评估了采用可读性AI方法的问题和挑战，以及未来采用这些方法的未来方向。<details>
<summary>Abstract</summary>
The success of artificial intelligence (AI), and deep learning models in particular, has led to their widespread adoption across various industries due to their ability to process huge amounts of data and learn complex patterns. However, due to their lack of explainability, there are significant concerns regarding their use in critical sectors, such as finance and healthcare, where decision-making transparency is of paramount importance. In this paper, we provide a comparative survey of methods that aim to improve the explainability of deep learning models within the context of finance. We categorize the collection of explainable AI methods according to their corresponding characteristics, and we review the concerns and challenges of adopting explainable AI methods, together with future directions we deemed appropriate and important.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）和深度学习模型的成功导致它们在不同领域得到广泛的应用，这主要是因为它们可以处理庞大数据量和学习复杂的模式。然而，由于它们的不可解性，在重要领域如金融和医疗等，决策透明度的问题具有重要性。在这篇论文中，我们提供了深入比较了在金融领域中改进深度学习模型的可解性的方法。我们根据它们的特点分类ify这些可解AI方法，并评估采用可解AI方法的问题和挑战，以及未来适当和重要的方向。
</details></li>
</ul>
<hr>
<h2 id="On-the-Definition-of-Appropriate-Trust-and-the-Tools-that-Come-with-it"><a href="#On-the-Definition-of-Appropriate-Trust-and-the-Tools-that-Come-with-it" class="headerlink" title="On the Definition of Appropriate Trust and the Tools that Come with it"></a>On the Definition of Appropriate Trust and the Tools that Come with it</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11937">http://arxiv.org/abs/2309.11937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Helena Löfström</li>
<li>for: 本研究旨在评估人工智能交互的效率，包括主观和客观质量方面。</li>
<li>methods: 本研究使用定义适当信任的方法进行评估，并与模型性能评估进行比较，发现两者之间存在强相似性。</li>
<li>results: 本研究提出了一种新的适当信任评估方法，并对用户性能进行了多个方面的评估，包括建议一种测量不确定性和适当信任的方法。<details>
<summary>Abstract</summary>
Evaluating the efficiency of human-AI interactions is challenging, including subjective and objective quality aspects. With the focus on the human experience of the explanations, evaluations of explanation methods have become mostly subjective, making comparative evaluations almost impossible and highly linked to the individual user. However, it is commonly agreed that one aspect of explanation quality is how effectively the user can detect if the predictions are trustworthy and correct, i.e., if the explanations can increase the user's appropriate trust in the model. This paper starts with the definitions of appropriate trust from the literature. It compares the definitions with model performance evaluation, showing the strong similarities between appropriate trust and model performance evaluation. The paper's main contribution is a novel approach to evaluating appropriate trust by taking advantage of the likenesses between definitions. The paper offers several straightforward evaluation methods for different aspects of user performance, including suggesting a method for measuring uncertainty and appropriate trust in regression.
</details>
<details>
<summary>摘要</summary>
评估人类-AI交互的效率具有挑战性，包括主观和客观质量方面。针对解释方法的评估通常受到用户经验的限制，导致对比评估变得困难，同时与个人用户的偏好息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息息字。本文从文献中定义了适当信任的定义，并与模型性能评估进行比较，发现这两者之间存在强烈的相似性。本文的主要贡献是一种新的适当信任评估方法，利用定义之间的相似性。文章还提供了不同方面的用户性能评估方法，包括一种用于减迷和适当信任的回归方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Recover-for-Safe-Reinforcement-Learning"><a href="#Learning-to-Recover-for-Safe-Reinforcement-Learning" class="headerlink" title="Learning to Recover for Safe Reinforcement Learning"></a>Learning to Recover for Safe Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11907">http://arxiv.org/abs/2309.11907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyu Wang, Xin Yuan, Qinqing Ren</li>
<li>for: 本文旨在研究如何通过学习算法构建安全控制器，以实现安全的返点学习。</li>
<li>methods: 本文提出了一种三 stage 架构，即 TU-Recovery 架构，包括安全评估器和恢复策略。这些 componenets 共同形成了一个安全控制器，确保任务训练中的安全性。</li>
<li>results: 在一个 robot 导航环境中进行了一系列实验，结果表明，TU-Recovery 在 reward 获取和约束违反方面在任务训练中表现出色，并且 auxiliary reward 可以进一步提高 TU-Recovery 的 reward-to-cost 比例，同时减少约束违反。<details>
<summary>Abstract</summary>
Safety controllers is widely used to achieve safe reinforcement learning. Most methods that apply a safety controller are using handcrafted safety constraints to construct the safety controller. However, when the environment dynamics are sophisticated, handcrafted safety constraints become unavailable. Therefore, it worth to research on constructing safety controllers by learning algorithms. We propose a three-stage architecture for safe reinforcement learning, namely TU-Recovery Architecture. A safety critic and a recovery policy is learned before task training. They form a safety controller to ensure safety in task training. Then a phenomenon induced by disagreement between task policy and recovery policy, called adversarial phenomenon, which reduces learning efficiency and model performance, is described. Auxiliary reward is proposed to mitigate adversarial phenomenon, while help the task policy to learn to recover from high-risk states. A series of experiments are conducted in a robot navigation environment. Experiments demonstrate that TU-Recovery outperforms unconstrained counterpart in both reward gaining and constraint violations during task training, and auxiliary reward further improve TU-Recovery in reward-to-cost ratio by significantly reduce constraint violations.
</details>
<details>
<summary>摘要</summary>
安全控制器广泛应用于安全强化学习。大多数应用安全控制器的方法使用手工安全约束来构建安全控制器。然而，当环境动力学复杂时，手工安全约束成为不可用。因此，研究构建安全控制器的学习算法是有价值的。我们提出了三个阶段的安全强化学习架构，称为TU-Recovery架构。一个安全评价员和一个恢复策略在任务训练之前被学习出来。它们组成一个安全控制器，以确保任务训练中的安全。然后，一种由任务策略和恢复策略之间的不一致现象引起的，称为对抗现象，这会降低学习效率和模型性能。我们提出了auxiliary reward来 mitigate对抗现象，同时帮助任务策略学习从高风险状态恢复。在一个 робот导航环境中进行了一系列实验，实验结果表明，TU-Recovery在增加奖励和约束违反时在任务训练中表现出了比对ounterpart更好的性能，并且auxiliary reward可以进一步提高TU-Recovery的奖励比率。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-the-Heart-Using-Adaptive-Locked-Agnostic-Networks"><a href="#Unlocking-the-Heart-Using-Adaptive-Locked-Agnostic-Networks" class="headerlink" title="Unlocking the Heart Using Adaptive Locked Agnostic Networks"></a>Unlocking the Heart Using Adaptive Locked Agnostic Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11899">http://arxiv.org/abs/2309.11899</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AstraZeneca/UnlockingHeart">https://github.com/AstraZeneca/UnlockingHeart</a></li>
<li>paper_authors: Sylwia Majchrowska, Anders Hildeman, Philip Teare, Tom Diethe<br>for:* The paper is written for medical imaging applications, specifically for echocardiography datasets.methods:* The paper introduces the Adaptive Locked Agnostic Network (ALAN) method, which uses self-supervised visual feature extraction and a large backbone model to produce anatomically robust semantic self-segmentation.results:* The paper demonstrates that the self-supervised backbone model robustly identifies anatomical subregions of the heart in an apical four-chamber view. Additionally, the paper designs two downstream models for segmenting a target anatomical region and echocardiogram view classification.Here is the information in Simplified Chinese text:for:* 这篇论文是为医疗影像应用而写的，特别是对于echocardiography dataset。methods:* 这篇论文介绍了Adaptive Locked Agnostic Network（ALAN）方法，该方法使用自动鉴定的视觉特征提取和大型后置网络来生成医学robust的semantic自 segmentation。results:* 这篇论文表明，自动鉴定后置网络可以坚定地标识心脏四室视图中的生物学特征。此外，论文还设计了两个下游模型，一个用于标识目标生物学区域，另一个用于echocardiogram视图分类。<details>
<summary>Abstract</summary>
Supervised training of deep learning models for medical imaging applications requires a significant amount of labeled data. This is posing a challenge as the images are required to be annotated by medical professionals. To address this limitation, we introduce the Adaptive Locked Agnostic Network (ALAN), a concept involving self-supervised visual feature extraction using a large backbone model to produce anatomically robust semantic self-segmentation. In the ALAN methodology, this self-supervised training occurs only once on a large and diverse dataset. Due to the intuitive interpretability of the segmentation, downstream models tailored for specific tasks can be easily designed using white-box models with few parameters. This, in turn, opens up the possibility of communicating the inner workings of a model with domain experts and introducing prior knowledge into it. It also means that the downstream models become less data-hungry compared to fully supervised approaches. These characteristics make ALAN particularly well-suited for resource-scarce scenarios, such as costly clinical trials and rare diseases. In this paper, we apply the ALAN approach to three publicly available echocardiography datasets: EchoNet-Dynamic, CAMUS, and TMED-2. Our findings demonstrate that the self-supervised backbone model robustly identifies anatomical subregions of the heart in an apical four-chamber view. Building upon this, we design two downstream models, one for segmenting a target anatomical region, and a second for echocardiogram view classification.
</details>
<details>
<summary>摘要</summary>
审核训练深度学习模型 для医疗影像应用需要大量标注数据。然而，由于影像需要由医疗专业人员进行标注，这成为一个挑战。为解决这个限制，我们介绍了自适应锁定agnostic网络（ALAN），它利用大型后骨骼模型进行自我超视觉特征提取，以生成具有体系 robust的 semantic自segmentation。在ALAN方法中，这种自我超视觉训练只在一个大型和多样的数据集上进行一次。由于分割结果的直观解释，下游模型可以轻松地使用白盒模型和少量参数进行定制。这种特点使得ALAN在资源匮乏的情况下特别有优势，例如成本高的临床试验和罕见疾病。在这篇论文中，我们应用ALAN方法于三个公共可用的echo医学数据集：EchoNet-Dynamic、CAMUS和TMED-2。我们的发现表明，自适应锁定agnostic网络可以强健地 identificates心脏四室视图中的 анатомичеSUBREGION。基于这个结果，我们设计了两个下游模型，一个用于分割目标生物学区域，另一个用于echo医学视图类型分类。
</details></li>
</ul>
<hr>
<h2 id="Audio-Contrastive-based-Fine-tuning"><a href="#Audio-Contrastive-based-Fine-tuning" class="headerlink" title="Audio Contrastive based Fine-tuning"></a>Audio Contrastive based Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11895">http://arxiv.org/abs/2309.11895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Wang, Qibin Liang, Chenghao Xiao, Yizhi Li, Noura Al Moubayed, Chenghua Lin</li>
<li>for: Audio classification tasks, such as speech and sound processing, with a wide range of applications.</li>
<li>methods: 用对照学习的转移性能来提高模型的测试准确性和适应能力。</li>
<li>results: 在不同的audio classification任务中，AudioConFit方法可以实现高度的效果和稳定性，并且在不同的测试数据集上获得了最佳的结果。<details>
<summary>Abstract</summary>
Audio classification plays a crucial role in speech and sound processing tasks with a wide range of applications. There still remains a challenge of striking the right balance between fitting the model to the training data (avoiding overfitting) and enabling it to generalise well to a new domain. Leveraging the transferability of contrastive learning, we introduce Audio Contrastive-based Fine-tuning (AudioConFit), an efficient approach characterised by robust generalisability. Empirical experiments on a variety of audio classification tasks demonstrate the effectiveness and robustness of our approach, which achieves state-of-the-art results in various settings.
</details>
<details>
<summary>摘要</summary>
音频分类在语音和声音处理任务中扮演着关键角色，具有广泛的应用场景。然而，模型适应训练数据的问题仍然是一大挑战，以避免过拟合。我们基于对比学习的传送性，提出了音频对比细化（AudioConFit），一种高效的方法，具有强大的通用性。实验证明，我们的方法在不同的音频分类任务中具有优秀的效果和稳定性，达到了不同设置下的状态态表现。
</details></li>
</ul>
<hr>
<h2 id="Multi-level-Asymmetric-Contrastive-Learning-for-Medical-Image-Segmentation-Pre-training"><a href="#Multi-level-Asymmetric-Contrastive-Learning-for-Medical-Image-Segmentation-Pre-training" class="headerlink" title="Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training"></a>Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11876">http://arxiv.org/abs/2309.11876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Zeng, Lei Zhu, Xinliang Zhang, Zifeng Tian, Qian Chen, Lujia Jin, Jiayi Wang, Yanye Lu</li>
<li>for: 本文针对医疗影像分类任务提出了一个新的不对称对称学习框架（JCL），以自动扮演验证预训。</li>
<li>methods: 本文提出了一个新的不对称对称学习策略，同时预训encoder和decoder，以提供更好的预设值 для医疗影像分类模型。 multi-level对称损失函数被设计来考虑对于特征水平、影像水平和像素水平的对应关系，以确保encoder和decoder在预训过程中可以学习多个水平的表示。</li>
<li>results: 在多个医疗影像数据集上实验显示，我们的JCL框架比现有的SOTA对称学习策略更高效。<details>
<summary>Abstract</summary>
Contrastive learning, which is a powerful technique for learning image-level representations from unlabeled data, leads a promising direction to dealing with the dilemma between large-scale pre-training and limited labeled data. However, most existing contrastive learning strategies are designed mainly for downstream tasks of natural images, therefore they are sub-optimal and even worse than learning from scratch when directly applied to medical images whose downstream tasks are usually segmentation. In this work, we propose a novel asymmetric contrastive learning framework named JCL for medical image segmentation with self-supervised pre-training. Specifically, (1) A novel asymmetric contrastive learning strategy is proposed to pre-train both encoder and decoder simultaneously in one-stage to provide better initialization for segmentation models. (2) A multi-level contrastive loss is designed to take the correspondence among feature-level, image-level and pixel-level projections, respectively into account to make sure multi-level representations can be learned by the encoder and decoder during pre-training. (3) Experiments on multiple medical image datasets indicate our JCL framework outperforms existing SOTA contrastive learning strategies.
</details>
<details>
<summary>摘要</summary>
对于医疗影像的分类问题，对于大规模预训数据和有限的标签数据的冲突是一个挑战。然而，现有的对比学习策略主要是设计来应用于自然像素，因此它们在医疗影像的下游任务中表现不佳，甚至比起从零学习还差。在这个工作中，我们提出了一个名为JCL的 novel asymmetric对比学习框架，用于医疗影像分类。具体来说，我们提出了以下三个方法：1. 一个新的对比学习策略，同时在一阶段中预训encoder和decoder，以提供更好的初始化 для分类模型。2. 一个多层对比损失函数，用于考虑对于特征层、影像层和像素层的对应关系，以确保encoder和decoder在预训过程中学习到多层表示。3. 在多个医疗影像数据集上进行了实验，结果显示了我们的JCL框架比现有的SOTA对比学习策略更高效。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-stiffness-identification-and-response-estimation-of-Timoshenko-beams-via-physics-informed-Gaussian-processes"><a href="#Stochastic-stiffness-identification-and-response-estimation-of-Timoshenko-beams-via-physics-informed-Gaussian-processes" class="headerlink" title="Stochastic stiffness identification and response estimation of Timoshenko beams via physics-informed Gaussian processes"></a>Stochastic stiffness identification and response estimation of Timoshenko beams via physics-informed Gaussian processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11875">http://arxiv.org/abs/2309.11875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gledsonrt/pigptimoshenkobeam">https://github.com/gledsonrt/pigptimoshenkobeam</a></li>
<li>paper_authors: Gledson Rodrigo Tondo, Sebastian Rau, Igor Kavrakov, Guido Morgenthal</li>
<li>for: 这篇论文旨在用机器学习模型进行系统识别，特别是Timoshenko梁元件的结构健康监测数据。</li>
<li>methods: 该论文提出了一种基于 Gaussian Process（GP）模型的物理 informed 方法，用于结构参数的 indentification。该模型是一种多输出GP模型，其covariance和cross-covariance函数 Analytical derivation based on the differential equations of deflections, rotations, strains, bending moments, shear forces and applied loads。</li>
<li>results: 对于结构参数的 indentification，使用 Bayesian 格式的最大化 posterior model，通过 Markov chain Monte Carlo 方法进行优化，得到了一个随机模型。此外，该方法还可以用于probabilistic predictions of unobserved responses。实验结果表明，提出的方法可以有效地 indentify结构参数，并且可以融合不同类型和多价信息感知器的数据。结果的质量和不确定性也得到了验证。该方法有广泛的应用前提在结构健康监测（SHM）领域。<details>
<summary>Abstract</summary>
Machine learning models trained with structural health monitoring data have become a powerful tool for system identification. This paper presents a physics-informed Gaussian process (GP) model for Timoshenko beam elements. The model is constructed as a multi-output GP with covariance and cross-covariance kernels analytically derived based on the differential equations for deflections, rotations, strains, bending moments, shear forces and applied loads. Stiffness identification is performed in a Bayesian format by maximising a posterior model through a Markov chain Monte Carlo method, yielding a stochastic model for the structural parameters. The optimised GP model is further employed for probabilistic predictions of unobserved responses. Additionally, an entropy-based method for physics-informed sensor placement optimisation is presented, exploiting heterogeneous sensor position information and structural boundary conditions built into the GP model. Results demonstrate that the proposed approach is effective at identifying structural parameters and is capable of fusing data from heterogeneous and multi-fidelity sensors. Probabilistic predictions of structural responses and internal forces are in closer agreement with measured data. We validate our model with an experimental setup and discuss the quality and uncertainty of the obtained results. The proposed approach has potential applications in the field of structural health monitoring (SHM) for both mechanical and structural systems.
</details>
<details>
<summary>摘要</summary>
机器学习模型使用结构健康监测数据成为了系统识别的强大工具。这篇论文提出了基于Timoshenko梁元素的物理知识泛化过程（GP）模型。该模型是一种多输出GP模型，其covariance和交叉covariancekernel analytically derive了基于摆动、旋转、强度、剪力、应用负荷的偏微分方程。在 bayesian 格式下，通过Markov chain Monte Carlo 方法进行了权重最大化，从而获得了一个随机模型 для结构参数。该优化后的 GP 模型进一步用于 probabilistic 预测未观测Response。此外，本文还提出了基于物理知识的感知器位置优化方法，利用不同类型感知器位置信息和结构边界条件，并将其建入 GP 模型中。结果表明，提出的方法能够有效地识别结构参数，并能够融合不同类型和多优化感知器的数据。probabilistic 预测结构响应和内部力的结果与实验数据更加吻合。我们验证了我们的模型，并讨论了获得的结果的质量和不确定性。该方法在结构健康监测（SHM）领域有广泛的应用前景。
</details></li>
</ul>
<hr>
<h2 id="OSNet-MNetO-Two-Types-of-General-Reconstruction-Architectures-for-Linear-Computed-Tomography-in-Multi-Scenarios"><a href="#OSNet-MNetO-Two-Types-of-General-Reconstruction-Architectures-for-Linear-Computed-Tomography-in-Multi-Scenarios" class="headerlink" title="OSNet &amp; MNetO: Two Types of General Reconstruction Architectures for Linear Computed Tomography in Multi-Scenarios"></a>OSNet &amp; MNetO: Two Types of General Reconstruction Architectures for Linear Computed Tomography in Multi-Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11858">http://arxiv.org/abs/2309.11858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhisheng Wang, Zihan Deng, Fenglin Liu, Yixing Huang, Haijun Yu, Junning Cui<br>for:* Linear computed tomography (LCT) systems Linear computed tomography (LCT) systems are the focus of this paper, and the authors aim to improve image reconstruction for these systems.methods:*  Backprojection filtration (BPF) algorithm The BPF algorithm is used to weaken projection truncation and image the region of interest (ROI) for LCT.*  Two types of reconstruction architectures are proposed: Overlay-Single Network (OSNet) and Multiple Networks Overlaying (MNetO) These architectures use multiple DBP images to obtain a complete DBP image and train different directional Hilbert filtering models for DBP images of multiple linear scannings, respectively.results:*  OSNet outperforms BPF in various scenarios The OSNet architecture outperforms the traditional BPF algorithm in different scenarios, including interior ROI, complete object, and exterior region beyond field-of-view (FOV).*  ST-pix2pixGAN is superior to pix2pixGAN and CycleGAN The authors introduce a Swin Transformer (ST) block to the generator of pix2pixGAN to extract both local and global features from DBP images at the same time, and ST-pix2pixGAN is found to be superior to pix2pixGAN and CycleGAN.*  MNetO exhibits a few artifacts due to the differences among the multiple models MNetO uses multiple networks to train different directional Hilbert filtering models for DBP images of multiple linear scannings, but it exhibits a few artifacts due to the differences among the multiple models.<details>
<summary>Abstract</summary>
Recently, linear computed tomography (LCT) systems have actively attracted attention. To weaken projection truncation and image the region of interest (ROI) for LCT, the backprojection filtration (BPF) algorithm is an effective solution. However, in BPF for LCT, it is difficult to achieve stable interior reconstruction, and for differentiated backprojection (DBP) images of LCT, multiple rotation-finite inversion of Hilbert transform (Hilbert filtering)-inverse rotation operations will blur the image. To satisfy multiple reconstruction scenarios for LCT, including interior ROI, complete object, and exterior region beyond field-of-view (FOV), and avoid the rotation operations of Hilbert filtering, we propose two types of reconstruction architectures. The first overlays multiple DBP images to obtain a complete DBP image, then uses a network to learn the overlying Hilbert filtering function, referred to as the Overlay-Single Network (OSNet). The second uses multiple networks to train different directional Hilbert filtering models for DBP images of multiple linear scannings, respectively, and then overlays the reconstructed results, i.e., Multiple Networks Overlaying (MNetO). In two architectures, we introduce a Swin Transformer (ST) block to the generator of pix2pixGAN to extract both local and global features from DBP images at the same time. We investigate two architectures from different networks, FOV sizes, pixel sizes, number of projections, geometric magnification, and processing time. Experimental results show that two architectures can both recover images. OSNet outperforms BPF in various scenarios. For the different networks, ST-pix2pixGAN is superior to pix2pixGAN and CycleGAN. MNetO exhibits a few artifacts due to the differences among the multiple models, but any one of its models is suitable for imaging the exterior edge in a certain direction.
</details>
<details>
<summary>摘要</summary>
最近，线性计算 Tomatoes系统（LCT）已经吸引了一些注意。为了弱化投影截断和图像区域内部（ROI）的投影，使用backprojection filtration（BPF）算法是一个有效的解决方案。然而，在LCT中的BPF中，很难实现稳定的内部重建，而且对于DBP图像的LCT，多个旋转finite inversion of Hilbert transform（Hilbert filtering）操作会模糊图像。为了满足LCT的多种重建场景，包括内部ROI、完整的对象和外部区域超出FOV（场景），并且避免旋转Hilbert filtering的操作，我们提出了两种重建建筑。第一种方法是将多个DBP图像 overlay 成一个完整的DBP图像，然后使用一个网络来学习投影过程中的Hilbert filtering函数，被称为Overlay-Single Network（OSNet）。第二种方法是使用多个网络来训练不同的方向的Hilbert filtering模型，并将其重建结果 overlay 在一起，即Multiple Networks Overlaying（MNetO）。在两种建筑中，我们引入了Swin Transformer（ST）块到 pix2pixGAN 生成器中，以同时提取 DBP 图像的本地和全局特征。我们从不同的网络、FOV 大小、像素大小、投影数量、几何增大和处理时间等方面进行了调查。实验结果表明，两种建筑都可以重建图像，OSNet 在多种场景中表现出色，超过 BPF。而ST-pix2pixGAN 比 pix2pixGAN 和 CycleGAN 更高效。MNetO 显示了一些缺陷，但任何一个模型都适用于某些方向的图像重建。
</details></li>
</ul>
<hr>
<h2 id="BitCoin-Bidirectional-Tagging-and-Supervised-Contrastive-Learning-based-Joint-Relational-Triple-Extraction-Framework"><a href="#BitCoin-Bidirectional-Tagging-and-Supervised-Contrastive-Learning-based-Joint-Relational-Triple-Extraction-Framework" class="headerlink" title="BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based Joint Relational Triple Extraction Framework"></a>BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based Joint Relational Triple Extraction Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11853">http://arxiv.org/abs/2309.11853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luyao He, Zhongbao Zhang, Sen Su, Yuxin Chen</li>
<li>for: 提高关系 triple EXTRACTION（RTE）任务的精度和效率，解决现有方法的一般化和特定性问题。</li>
<li>methods: 提出了一种创新的双向标注和监督对比学习基于的关系 triple EXTRACTION框架（BitCoin），包括多个可能的正例而不仅仅是一个正例，并引入对象和主题之间的偏差项来避免对象和主题之间的过度相似性。</li>
<li>results: 实验结果显示，BitCoin在标准数据集上达到了当前最佳Result，在不同的任务上（包括Normal、SEO、EPO和多关系EXTRACTION）显著提高了F1分数。<details>
<summary>Abstract</summary>
Relation triple extraction (RTE) is an essential task in information extraction and knowledge graph construction. Despite recent advancements, existing methods still exhibit certain limitations. They just employ generalized pre-trained models and do not consider the specificity of RTE tasks. Moreover, existing tagging-based approaches typically decompose the RTE task into two subtasks, initially identifying subjects and subsequently identifying objects and relations. They solely focus on extracting relational triples from subject to object, neglecting that once the extraction of a subject fails, it fails in extracting all triples associated with that subject. To address these issues, we propose BitCoin, an innovative Bidirectional tagging and supervised Contrastive learning based joint relational triple extraction framework. Specifically, we design a supervised contrastive learning method that considers multiple positives per anchor rather than restricting it to just one positive. Furthermore, a penalty term is introduced to prevent excessive similarity between the subject and object. Our framework implements taggers in two directions, enabling triples extraction from subject to object and object to subject. Experimental results show that BitCoin achieves state-of-the-art results on the benchmark datasets and significantly improves the F1 score on Normal, SEO, EPO, and multiple relation extraction tasks.
</details>
<details>
<summary>摘要</summary>
“关系 triple 提取（RTE）是信息提取和知识图建构中的关键任务。尽管最近有所进步，现有方法仍然存在一定的限制。它们只是使用通用预训练模型，未考虑RTE任务的特点。另外，现有的标签化方法通常将RTE任务分解成两个子任务，先确定主题，然后确定 объек 和关系。它们仅专注于从主题到 объек 中提取关系 triple，忽略了如果提取主题失败，那么所有与该主题相关的 triple 都将难以提取。为解决这些问题，我们提出了 BitCoin，一种创新的双向标签和监督对比学习基于的关系 triple 提取框架。具体来说，我们设计了一种监督对比学习方法，考虑多个可能的正例而不是仅仅 restricting 到一个正例。此外，我们引入了一个罚项，以避免主题和 объек 之间的过度相似性。我们的框架实现了两个方向的标签，即从主题到 объек 和从 objet 到主题。实验结果表明，BitCoin 在标准数据集上达到了当前最佳的结果，并在多个关系提取任务上显著提高了 F1 分数。”
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Large-Language-Models-for-Document-grounded-Response-Generation-in-Information-Seeking-Dialogues"><a href="#Evaluating-Large-Language-Models-for-Document-grounded-Response-Generation-in-Information-Seeking-Dialogues" class="headerlink" title="Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues"></a>Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11838">http://arxiv.org/abs/2309.11838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Norbert Braunschweiler, Rama Doddipatla, Simon Keizer, Svetlana Stoyanchev</li>
<li>for: 这个研究是为了研究使用大语言模型（LLMs）like ChatGPT来生成基于文档的响应，特别是在信息寻求对话中。</li>
<li>methods: 这个研究使用了两种方法：ChatCompletion和LlamaIndex。ChatCompletion使用了ChatGPT模型的预training知识，而LlamaIndex则从文档中提取了相关信息。</li>
<li>results: 研究发现，使用LLMs来生成基于文档的响应是不可靠的，因为它们可能包含未在文档中出现的信息，可能是幻想。同时，两种ChatGPT变体的输出被评估为更高，比之前的分享任务赢家系统和人类响应。<details>
<summary>Abstract</summary>
In this paper, we investigate the use of large language models (LLMs) like ChatGPT for document-grounded response generation in the context of information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus of task-oriented dialogues in four social service domains previously used in the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded in multiple documents providing relevant information. We generate dialogue completion responses by prompting a ChatGPT model, using two methods: Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT model pretraining while LlamaIndex also extracts relevant information from documents. Observing that document-grounded response generation via LLMs cannot be adequately assessed by automatic evaluation metrics as they are significantly more verbose, we perform a human evaluation where annotators rate the output of the shared task winning system, the two Chat-GPT variants outputs, and human responses. While both ChatGPT variants are more likely to include information not present in the relevant segments, possibly including a presence of hallucinations, they are rated higher than both the shared task winning system and human responses.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了大型自然语言模型（LLM）如ChatGPT在信息寻求对话中的回答生成。为评估，我们使用MultiDoc2Dial词汇对话 corpus，这是四个社会服务领域的任务型对话，已经在DialDoc 2022 Shared Task中使用。信息寻求对话转归基于多份文档提供相关信息。我们生成对话完成回答，使用两种方法：Chat-Completion和LlamaIndex。ChatCompletion利用ChatGPT模型的先前预测知识，而LlamaIndex也从文档中提取相关信息。由于文档基础回答生成via LLMs无法准确评估，我们进行了人工评估。评估结果显示，两个ChatGPT变体的输出高于分享任务获胜系统和人类回答。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Transformers-for-Wireless-Communications-A-Case-Study-in-Beam-Prediction"><a href="#Multimodal-Transformers-for-Wireless-Communications-A-Case-Study-in-Beam-Prediction" class="headerlink" title="Multimodal Transformers for Wireless Communications: A Case Study in Beam Prediction"></a>Multimodal Transformers for Wireless Communications: A Case Study in Beam Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11811">http://arxiv.org/abs/2309.11811</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/itu-ai-ml-in-5g-challenge/deepsense6g_tii">https://github.com/itu-ai-ml-in-5g-challenge/deepsense6g_tii</a></li>
<li>paper_authors: Yu Tian, Qiyang Zhao, Zine el abidine Kherroubi, Fouzi Boukhalfa, Kebin Wu, Faouzi Bader</li>
<li>for: 本研究旨在提高无线通信高频带的大antenna数组面临的扫描管理问题，通过多模态感知信息（包括摄像头、LiDAR、雷达和GPS）的多模态感知训练deep learning框架。</li>
<li>methods: 本研究使用多模态转换器深度学习框架，将多modal的感知信息经过核心网络提取特征，然后使用transformer编码器学习不同modalities和时间点之间的隐藏关系，生成下一层特征提取的编码器。</li>
<li>results: 实验结果显示，使用图像和GPS数据进行训练的解决方案可以达到78.44%的距离基于准确性，并且在不seen日景和夜景中实现了73%和84%的泛化性。这些结果超过了使用其他modalities和arbitrary数据处理技术，从而证明了转换器在图像和GPS数据上的准确性和泛化性。<details>
<summary>Abstract</summary>
Wireless communications at high-frequency bands with large antenna arrays face challenges in beam management, which can potentially be improved by multimodality sensing information from cameras, LiDAR, radar, and GPS. In this paper, we present a multimodal transformer deep learning framework for sensing-assisted beam prediction. We employ a convolutional neural network to extract the features from a sequence of images, point clouds, and radar raw data sampled over time. At each convolutional layer, we use transformer encoders to learn the hidden relations between feature tokens from different modalities and time instances over abstraction space and produce encoded vectors for the next-level feature extraction. We train the model on a combination of different modalities with supervised learning. We try to enhance the model over imbalanced data by utilizing focal loss and exponential moving average. We also evaluate data processing and augmentation techniques such as image enhancement, segmentation, background filtering, multimodal data flipping, radar signal transformation, and GPS angle calibration. Experimental results show that our solution trained on image and GPS data produces the best distance-based accuracy of predicted beams at 78.44%, with effective generalization to unseen day scenarios near 73% and night scenarios over 84%. This outperforms using other modalities and arbitrary data processing techniques, which demonstrates the effectiveness of transformers with feature fusion in performing radio beam prediction from images and GPS. Furthermore, our solution could be pretrained from large sequences of multimodality wireless data, on fine-tuning for multiple downstream radio network tasks.
</details>
<details>
<summary>摘要</summary>
无线通信在高频段 WITH 大antenna array 面临 beam 管理问题，可能可以通过多模态感知信息 FROM camera, LiDAR, radar, GPS 进行改进。在这篇论文中，我们提出了一种多模态 transformer 深度学习框架 FOR 感知协助的 beam 预测。我们使用卷积神经网络提取图像、点云和雷达原始数据序列中的特征，并在每层卷积层使用 transformer 编码器学习不同模态和时间实例之间的隐藏关系，生成下一层特征提取的编码 вектор。我们使用多模态supervised学习训练模型，并尝试通过使用焦点损失和指数移动平均来增强模型对不均衡数据的适应。我们还评估了数据处理和扩展技术，如图像提升、分割、背景筛选、多模态数据翻转、雷达信号转换和GPS角度准确。实验结果表明，我们基于图像和 GPS 数据训练的解决方案可以在78.44%的距离基于准确率上预测 beam，并且在未看到的日场景中 Near 73% AND 夜场景中超过 84%。这超过了使用其他模式和自由数据处理技术，这表明 transformers 在图像和 GPS 数据上进行 radio beam 预测具有效果，并且可以通过多模态数据进行预训练，并在多个无线网络下执行多种下游任务进行 fine-tuning。
</details></li>
</ul>
<hr>
<h2 id="JobRecoGPT-–-Explainable-job-recommendations-using-LLMs"><a href="#JobRecoGPT-–-Explainable-job-recommendations-using-LLMs" class="headerlink" title="JobRecoGPT – Explainable job recommendations using LLMs"></a>JobRecoGPT – Explainable job recommendations using LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11805">http://arxiv.org/abs/2309.11805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Preetam Ghosh, Vaishali Sadaphal</li>
<li>for: 这个论文是为了推荐适合候选人的工作而写的。</li>
<li>methods: 这篇论文使用了人工智能技术，特别是大语言模型（LLMs）来捕捉原文中的信息，以便更好地推荐适合候选人的工作。</li>
<li>results: 这篇论文 comparing了四种不同的方法，即内容基于的决定方法、LLM指导的方法、LLM无指导的方法以及混合方法，并评估了每种方法的时间需求和性能。<details>
<summary>Abstract</summary>
In today's rapidly evolving job market, finding the right opportunity can be a daunting challenge. With advancements in the field of AI, computers can now recommend suitable jobs to candidates. However, the task of recommending jobs is not same as recommending movies to viewers. Apart from must-have criteria, like skills and experience, there are many subtle aspects to a job which can decide if it is a good fit or not for a given candidate. Traditional approaches can capture the quantifiable aspects of jobs and candidates, but a substantial portion of the data that is present in unstructured form in the job descriptions and resumes is lost in the process of conversion to structured format. As of late, Large Language Models (LLMs) have taken over the AI field by storm with extraordinary performance in fields where text-based data is available. Inspired by the superior performance of LLMs, we leverage their capability to understand natural language for capturing the information that was previously getting lost during the conversion of unstructured data to structured form. To this end, we compare performance of four different approaches for job recommendations namely, (i) Content based deterministic, (ii) LLM guided, (iii) LLM unguided, and (iv) Hybrid. In this study, we present advantages and limitations of each method and evaluate their performance in terms of time requirements.
</details>
<details>
<summary>摘要</summary>
Recently, Large Language Models (LLMs) have revolutionized the AI field with extraordinary performance in text-based data. Inspired by their capabilities, we leverage their ability to understand natural language to capture the information that was previously lost during the conversion of unstructured data to a structured form. To achieve this, we compare the performance of four different approaches for job recommendations:1. Content-based deterministic approach2. LLM-guided approach3. LLM unguided approach4. Hybrid approachIn this study, we present the advantages and limitations of each method and evaluate their performance in terms of time requirements.
</details></li>
</ul>
<hr>
<h2 id="DimCL-Dimensional-Contrastive-Learning-For-Improving-Self-Supervised-Learning"><a href="#DimCL-Dimensional-Contrastive-Learning-For-Improving-Self-Supervised-Learning" class="headerlink" title="DimCL: Dimensional Contrastive Learning For Improving Self-Supervised Learning"></a>DimCL: Dimensional Contrastive Learning For Improving Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11782">http://arxiv.org/abs/2309.11782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanh Nguyen, Trung Pham, Chaoning Zhang, Tung Luu, Thang Vu, Chang D. Yoo</li>
<li>for: 提高自动学习（SSL）的表现，增强特征多样性</li>
<li>methods: 维度对冲学习（DimCL），一种在维度方向进行对冲学习而不是批处理方向的新方法</li>
<li>results: 在多个数据集和背景架构上实现了表现的提高，并且发现了困难度感知的特性对其成功起到了关键作用<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) has gained remarkable success, for which contrastive learning (CL) plays a key role. However, the recent development of new non-CL frameworks has achieved comparable or better performance with high improvement potential, prompting researchers to enhance these frameworks further. Assimilating CL into non-CL frameworks has been thought to be beneficial, but empirical evidence indicates no visible improvements. In view of that, this paper proposes a strategy of performing CL along the dimensional direction instead of along the batch direction as done in conventional contrastive learning, named Dimensional Contrastive Learning (DimCL). DimCL aims to enhance the feature diversity, and it can serve as a regularizer to prior SSL frameworks. DimCL has been found to be effective, and the hardness-aware property is identified as a critical reason for its success. Extensive experimental results reveal that assimilating DimCL into SSL frameworks leads to performance improvement by a non-trivial margin on various datasets and backbone architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="2DDATA-2D-Detection-Annotations-Transmittable-Aggregation-for-Semantic-Segmentation-on-Point-Cloud"><a href="#2DDATA-2D-Detection-Annotations-Transmittable-Aggregation-for-Semantic-Segmentation-on-Point-Cloud" class="headerlink" title="2DDATA: 2D Detection Annotations Transmittable Aggregation for Semantic Segmentation on Point Cloud"></a>2DDATA: 2D Detection Annotations Transmittable Aggregation for Semantic Segmentation on Point Cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11755">http://arxiv.org/abs/2309.11755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guan-Cheng Lee</li>
<li>for: 这篇论文目的是提出一种解决多感器模型中精度准确性问题的方法，使得这些模型能够在实际应用中使用。</li>
<li>methods: 这篇论文使用了2D检测注释传输汇聚(\textbf{2DDATA})方法，并设计了一个本地对象分支(\textbf{Local Object Branch})来处理点云数据。</li>
<li>results: 研究人员通过实验证明了他们的简单设计可以传输矩形盒注释信息到3D编码器模型，证明了大型多感器模型与模态特定数据的混合是可能的。<details>
<summary>Abstract</summary>
Recently, multi-modality models have been introduced because of the complementary information from different sensors such as LiDAR and cameras. It requires paired data along with precise calibrations for all modalities, the complicated calibration among modalities hugely increases the cost of collecting such high-quality datasets, and hinder it from being applied to practical scenarios. Inherit from the previous works, we not only fuse the information from multi-modality without above issues, and also exhaust the information in the RGB modality. We introduced the 2D Detection Annotations Transmittable Aggregation(\textbf{2DDATA}), designing a data-specific branch, called \textbf{Local Object Branch}, which aims to deal with points in a certain bounding box, because of its easiness of acquiring 2D bounding box annotations. We demonstrate that our simple design can transmit bounding box prior information to the 3D encoder model, proving the feasibility of large multi-modality models fused with modality-specific data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improve-the-efficiency-of-deep-reinforcement-learning-through-semantic-exploration-guided-by-natural-language"><a href="#Improve-the-efficiency-of-deep-reinforcement-learning-through-semantic-exploration-guided-by-natural-language" class="headerlink" title="Improve the efficiency of deep reinforcement learning through semantic exploration guided by natural language"></a>Improve the efficiency of deep reinforcement learning through semantic exploration guided by natural language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11753">http://arxiv.org/abs/2309.11753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhourui Guo, Meng Yao, Yang Yu, Qiyue Yin</li>
<li>for: 本文提出了一种新的方法，用于在RL中选择性地使用oracle，以提高RL的效率。</li>
<li>methods: 本文使用了一种模板问题和答案的方法，通过对 previous interactions 的整合，使用神经网络对当前状态和oracle的编码，并从corpus中检索最相关的问题。</li>
<li>results: 本文在一个物体把握任务上进行了评估，显示了 compared to基线方法，使用oracle可以显著提高RL的效率，减少RL需要进行的交互数量。<details>
<summary>Abstract</summary>
Reinforcement learning is a powerful technique for learning from trial and error, but it often requires a large number of interactions to achieve good performance. In some domains, such as sparse-reward tasks, an oracle that can provide useful feedback or guidance to the agent during the learning process is really of great importance. However, querying the oracle too frequently may be costly or impractical, and the oracle may not always have a clear answer for every situation. Therefore, we propose a novel method for interacting with the oracle in a selective and efficient way, using a retrieval-based approach. We assume that the interaction can be modeled as a sequence of templated questions and answers, and that there is a large corpus of previous interactions available. We use a neural network to encode the current state of the agent and the oracle, and retrieve the most relevant question from the corpus to ask the oracle. We then use the oracle's answer to update the agent's policy and value function. We evaluate our method on an object manipulation task. We show that our method can significantly improve the efficiency of RL by reducing the number of interactions needed to reach a certain level of performance, compared to baselines that do not use the oracle or use it in a naive way.
</details>
<details>
<summary>摘要</summary>
�� Reinforcement learning 是一种强大的学习技术，但它经常需要大量的交互来达到良好的性能。在某些领域，如稀谱奖励任务，一个 oracle 可以提供有用的反馈或指导 для Agent  durante el proceso de aprendizaje。然而，请求 oracle 的频率可能是成本高或实际不切实际的，而且 oracle 并不总是有清晰的答案对每个情况。因此，我们提出了一种新的方法来与 oracle 交互，使用一种检索基本的方法。我们假设交互可以被模型为一个序列化的问题和答案，并且有一大量的前期交互数据库。我们使用神经网络来编码 Agent 和 oracle 的当前状态，并从数据库中检索最相关的问题来问 oracle。然后，我们使用 oracle 的答案来更新 Agent 的政策和价值函数。我们在一个物体抓取任务上评估了我们的方法，并显示了我们的方法可以减少 RL 中交互的次数，以达到一定的性能水平，相比于不使用 oracle 或使用它的做法。
</details></li>
</ul>
<hr>
<h2 id="How-Robust-is-Google’s-Bard-to-Adversarial-Image-Attacks"><a href="#How-Robust-is-Google’s-Bard-to-Adversarial-Image-Attacks" class="headerlink" title="How Robust is Google’s Bard to Adversarial Image Attacks?"></a>How Robust is Google’s Bard to Adversarial Image Attacks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11751">http://arxiv.org/abs/2309.11751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/attack-bard">https://github.com/thu-ml/attack-bard</a></li>
<li>paper_authors: Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu</li>
<li>for: 这研究旨在研究Google的Bard chatbot的抗击型识别器 robustness，以便更好地理解商业多模态大语言模型（MLLMs）中的漏洞。</li>
<li>methods: 我们使用了白盒子代理视觉编码器或 MLLMs 进行攻击，生成了对 Bard 的 adversarial examples，并证明了这些攻击可以让 Bard 输出错误的图像描述。</li>
<li>results: 我们发现，对 Bard 的 adversarial examples 可以达到 22% 的成功率，并且可以攻击其他 MLLMs，如 Bing Chat 和 ERNIE bot。此外，我们还发现了 Bard 的两种防御机制，包括图像检测和图像攻击检测。我们设计了对这些防御机制的攻击，证明了现有的防御机制也是易于绕过的。<details>
<summary>Abstract</summary>
Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard.
</details>
<details>
<summary>摘要</summary>
多模态大语言模型（MLLMs），尤其是与视觉相结合，在多种多 modal 任务中表现出色。然而，由于视觉模型的不可预测性问题，MLLMs 可能带来更严重的安全和安全风险。在这项工作中，我们研究了Google的Bard，一个与 ChatGPT 竞争的聊天机器人，以更好地理解商业MLLMs 的漏洞。我们发现，通过攻击白盒子代理视觉encoder或 MLLMs，可以生成 adversarial 例子，导致 Bard 输出错误的图像描述，成功率达22%。此外，我们发现这些 adversarial 例子还可以攻击其他 MLLMs，例如 Bing Chat 和 ERNIE 机器人。此外，我们还发现 Bard 具有两种防御机制：脸部检测和图像攻击检测。我们设计了相应的攻击，证明现有防御机制也是易攻击的。我们希望这项工作可以深入我们对 MLLMs 的稳定性和防御机制的理解，并促进未来研究。我们的代码可以在 <https://github.com/thu-ml/Attack-Bard> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Choice-75-A-Dataset-on-Decision-Branching-in-Script-Learning"><a href="#Choice-75-A-Dataset-on-Decision-Branching-in-Script-Learning" class="headerlink" title="Choice-75: A Dataset on Decision Branching in Script Learning"></a>Choice-75: A Dataset on Decision Branching in Script Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11737">http://arxiv.org/abs/2309.11737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoyi Joey Hou, Li Zhang, Chris Callison-Burch</li>
<li>for: 本研究旨在探讨日常事件的发展过程。</li>
<li>methods: 本研究使用 Choice-75  benchmark，该 benchmark 包含 75 个场景和超过 600 个enario，以测试智能系统在面对描述性场景时的决策能力。</li>
<li>results: 大型语言模型在总体上表现不错，但在许多困难场景下仍有很大的进攻空间。<details>
<summary>Abstract</summary>
Script learning studies how daily events unfold. Previous works tend to consider a script as a linear sequence of events while ignoring the potential branches that arise due to people's circumstantial choices. We hence propose Choice-75, the first benchmark that challenges intelligent systems to predict decisions given descriptive scenarios, containing 75 scripts and more than 600 scenarios. While large language models demonstrate overall decent performances, there is still notable room for improvement in many hard scenarios.
</details>
<details>
<summary>摘要</summary>
学习脚本研究每日事件的发展。先前的工作通常将脚本视为一个直线性的事件序列，忽略人们因 circumstance 的选择所导致的可能的分支。我们因此提出了 Choice-75，首个挑战智能系统预测基于描述场景的决策，包含75个脚本和超过600个场景。虽然大语言模型在总体上表现不错，但在许多困难场景中仍有很大的改进空间。
</details></li>
</ul>
<hr>
<h2 id="FluentEditor-Text-based-Speech-Editing-by-Considering-Acoustic-and-Prosody-Consistency"><a href="#FluentEditor-Text-based-Speech-Editing-by-Considering-Acoustic-and-Prosody-Consistency" class="headerlink" title="FluentEditor: Text-based Speech Editing by Considering Acoustic and Prosody Consistency"></a>FluentEditor: Text-based Speech Editing by Considering Acoustic and Prosody Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11725">http://arxiv.org/abs/2309.11725</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-s2-lab/fluenteditor">https://github.com/ai-s2-lab/fluenteditor</a></li>
<li>paper_authors: Rui Liu, Jiatian Xi, Ziyue Jiang, Haizhou Li</li>
<li>for: 提高语音编辑技术的自然性和流畅性</li>
<li>methods: 使用fluency-aware训练 критерий，包括音频一致性约束和语调一致性约束</li>
<li>results: 对VCTK数据集进行subjective和objective эксперименты，表明我们的FluentEditor模型在自然性和流畅性方面具有显著优势，Audioamples和代码可以在github上获取<details>
<summary>Abstract</summary>
Text-based speech editing (TSE) techniques are designed to enable users to edit the output audio by modifying the input text transcript instead of the audio itself. Despite much progress in neural network-based TSE techniques, the current techniques have focused on reducing the difference between the generated speech segment and the reference target in the editing region, ignoring its local and global fluency in the context and original utterance. To maintain the speech fluency, we propose a fluency speech editing model, termed \textit{FluentEditor}, by considering fluency-aware training criterion in the TSE training. Specifically, the \textit{acoustic consistency constraint} aims to smooth the transition between the edited region and its neighboring acoustic segments consistent with the ground truth, while the \textit{prosody consistency constraint} seeks to ensure that the prosody attributes within the edited regions remain consistent with the overall style of the original utterance. The subjective and objective experimental results on VCTK demonstrate that our \textit{FluentEditor} outperforms all advanced baselines in terms of naturalness and fluency. The audio samples and code are available at \url{https://github.com/Ai-S2-Lab/FluentEditor}.
</details>
<details>
<summary>摘要</summary>
文本基于的语音修编（TSE）技术是为了让用户通过修改输入文本脚本而不是直接修改音频来编辑语音。虽然现有的神经网络基于TSE技术已经做出了很大的进步，但是现有的技术都是关注在编辑区域中减少生成的语音段与参照标题之间的差异，而忽略了当地和全局的流畅性。为保持语音流畅，我们提议一种流畅语音修编模型，称为“流畅编辑器”，通过考虑流畅意识训练 criterion 在 TSE 训练中来实现。具体来说，“语音一致性约束”是为了使编辑区域和其邻近的语音段之间的过渡变得更加平滑，与真实的语音一致；而“表情一致性约束”则是为了确保在编辑区域中的表情特征与原始语音的整体风格保持一致。对于 VCTK 的实验结果表明，我们的“流畅编辑器”在自然性和流畅性两个方面都超过了所有高级基elines。听音样本和代码可以在 GitHub 上找到：https://github.com/Ai-S2-Lab/FluentEditor。
</details></li>
</ul>
<hr>
<h2 id="Emotion-Aware-Prosodic-Phrasing-for-Expressive-Text-to-Speech"><a href="#Emotion-Aware-Prosodic-Phrasing-for-Expressive-Text-to-Speech" class="headerlink" title="Emotion-Aware Prosodic Phrasing for Expressive Text-to-Speech"></a>Emotion-Aware Prosodic Phrasing for Expressive Text-to-Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11724">http://arxiv.org/abs/2309.11724</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-s2-lab/emopp">https://github.com/ai-s2-lab/emopp</a></li>
<li>paper_authors: Rui Liu, Bin Liu, Haizhou Li</li>
<li>for: 这篇论文的目的是提出一种能够准确地捕捉语音中情感信息的情感意识排序模型（EmoPP），以便更好地表达情感。</li>
<li>methods: 这篇论文使用了对ESD数据集的 объектив观察，以验证情感和排序之间的强相关性。然后，对比了基eline和EmoPP模型，并进行了对比性和主观评价。</li>
<li>results: 研究结果表明，EmoPP模型在情感表达方面表现出色，与基eline模型相比，具有更高的表达效果和准确性。code和音频样本可以在github上下载。<details>
<summary>Abstract</summary>
Prosodic phrasing is crucial to the naturalness and intelligibility of end-to-end Text-to-Speech (TTS). There exist both linguistic and emotional prosody in natural speech. As the study of prosodic phrasing has been linguistically motivated, prosodic phrasing for expressive emotion rendering has not been well studied. In this paper, we propose an emotion-aware prosodic phrasing model, termed \textit{EmoPP}, to mine the emotional cues of utterance accurately and predict appropriate phrase breaks. We first conduct objective observations on the ESD dataset to validate the strong correlation between emotion and prosodic phrasing. Then the objective and subjective evaluations show that the EmoPP outperforms all baselines and achieves remarkable performance in terms of emotion expressiveness. The audio samples and the code are available at \url{https://github.com/AI-S2-Lab/EmoPP}.
</details>
<details>
<summary>摘要</summary>
“句子间的调音是 тек字至话（TTS）的自然和可理解性的关键。自然语言中存在语言和情感的调音。由于研究调音 phrasing 的动机主要是语言学的，因此对于表达情感的调音 phrasing 尚未受到充分研究。在这篇文章中，我们提出了一个情感认知的调音 phrasing 模型，称为 EmoPP，以精确地捕捉说话中的情感讯号和适当的分割点。我们首先通过对 ESD dataset 的 объектив观察， Validate 调音 phrasing 和情感之间的强相关。然后，对比性和主观评价显示，EmoPP 在情感表达方面具有很高的表现。另外，我们还提供了 Audio 示例和代码，可以在 GitHub 上找到。”Note that Simplified Chinese uses a different set of characters and grammar compared to Traditional Chinese, so the translation may differ slightly from the original text.
</details></li>
</ul>
<hr>
<h2 id="A-Dynamic-Domain-Adaptation-Deep-Learning-Network-for-EEG-based-Motor-Imagery-Classification"><a href="#A-Dynamic-Domain-Adaptation-Deep-Learning-Network-for-EEG-based-Motor-Imagery-Classification" class="headerlink" title="A Dynamic Domain Adaptation Deep Learning Network for EEG-based Motor Imagery Classification"></a>A Dynamic Domain Adaptation Deep Learning Network for EEG-based Motor Imagery Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11714">http://arxiv.org/abs/2309.11714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Jiao, Meiyan Xu, Qingqing Chen, Hefan Zhou, Wangliang Zhou</li>
<li>for: 提高 EEG 基于脑机器人接口的精度和效率，以及适应不同个体和Session 间的差异。</li>
<li>methods: 提议使用动态领域适应深度学习网络 (DADL-Net)，通过3D 卷积模块学习 EEG 数据的三维几何特征，然后使用空间通道注意力机制加强特征，并使用最大平均差分损失函数适应不同个体和Session 间的差异。</li>
<li>results: 在 BCI 竞赛 IV 2a 和 OpenBMI 数据集上验证了提议的方法，实现了70.42% 和73.91% 的准确率。<details>
<summary>Abstract</summary>
There is a correlation between adjacent channels of electroencephalogram (EEG), and how to represent this correlation is an issue that is currently being explored. In addition, due to inter-individual differences in EEG signals, this discrepancy results in new subjects need spend a amount of calibration time for EEG-based motor imagery brain-computer interface. In order to solve the above problems, we propose a Dynamic Domain Adaptation Based Deep Learning Network (DADL-Net). First, the EEG data is mapped to the three-dimensional geometric space and its temporal-spatial features are learned through the 3D convolution module, and then the spatial-channel attention mechanism is used to strengthen the features, and the final convolution module can further learn the spatial-temporal information of the features. Finally, to account for inter-subject and cross-sessions differences, we employ a dynamic domain-adaptive strategy, the distance between features is reduced by introducing a Maximum Mean Discrepancy loss function, and the classification layer is fine-tuned by using part of the target domain data. We verify the performance of the proposed method on BCI competition IV 2a and OpenBMI datasets. Under the intra-subject experiment, the accuracy rates of 70.42% and 73.91% were achieved on the OpenBMI and BCIC IV 2a datasets.
</details>
<details>
<summary>摘要</summary>
“electroencephalogram（EEG）附近通道之间存在相关性，但如何表示这种相关性是目前正在探索的问题。此外，由于EEG信号间的差异，需要新的训练时间以便EEG基于脑神经接口。为解决上述问题，我们提出了动态领域适应基于深度学习网络（DADL-Net）。首先，EEG数据会被对三维几何空间中的映射，并通过3D梯度层学习三维空间中的特征，然后使用空间频道注意力机制来强化特征，最后通过最后一个梯度层学习特征的空间-时间信息。为了考虑对象和跨会议差异，我们使用动态领域适应策略，将特征之间的距离降低，并使用Maximum Mean Discrepancy损失函数微调分类层。我们证明提案的方法在BCI竞赛IV 2a和OpenBMI数据集上表现出色。在内部实验中，OpenBMI和BCIC IV 2a数据集的准确率分别为70.42%和73.91%。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/21/cs.AI_2023_09_21/" data-id="clnsn0vdc004bgf88hb35ha2g" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/21/cs.CL_2023_09_21/" class="article-date">
  <time datetime="2023-09-21T11:00:00.000Z" itemprop="datePublished">2023-09-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/21/cs.CL_2023_09_21/">cs.CL - 2023-09-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Reranking-for-Natural-Language-Generation-from-Logical-Forms-A-Study-based-on-Large-Language-Models"><a href="#Reranking-for-Natural-Language-Generation-from-Logical-Forms-A-Study-based-on-Large-Language-Models" class="headerlink" title="Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models"></a>Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12294">http://arxiv.org/abs/2309.12294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Levon Haroutunian, Zhuang Li, Lucian Galescu, Philip Cohen, Raj Tumuluri, Gholamreza Haffari</li>
<li>for: 本研究旨在提高大语言模型（LLM）生成的自然语言质量，特别是在自然语言生成从逻辑形式（LF）时的输出质量。</li>
<li>methods: 我们提出了一种生成并重新排序的方法，其包括首先通过提问LLM生成一组候选输出，然后使用任务特定的重新排序模型对其进行重新排序。此外，我们还收集了一个手动抽象的数据集，用于评估不同排名指标和人类判断的对应关系。</li>
<li>results: 我们在三个不同的数据集上进行了广泛的实验，并证明了我们的方法可以提高LLM生成的输出质量，特别是在 semantic consistency 和 fluency 两个维度上。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated impressive capabilities in natural language generation. However, their output quality can be inconsistent, posing challenges for generating natural language from logical forms (LFs). This task requires the generated outputs to embody the exact semantics of LFs, without missing any LF semantics or creating any hallucinations. In this work, we tackle this issue by proposing a novel generate-and-rerank approach. Our approach involves initially generating a set of candidate outputs by prompting an LLM and subsequently reranking them using a task-specific reranker model. In addition, we curate a manually collected dataset to evaluate the alignment between different ranking metrics and human judgements. The chosen ranking metrics are utilized to enhance the training and evaluation of the reranker model. By conducting extensive experiments on three diverse datasets, we demonstrate that the candidates selected by our reranker outperform those selected by baseline methods in terms of semantic consistency and fluency, as measured by three comprehensive metrics. Our findings provide strong evidence for the effectiveness of our approach in improving the quality of generated outputs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Inspire-the-Large-Language-Model-by-External-Knowledge-on-BioMedical-Named-Entity-Recognition"><a href="#Inspire-the-Large-Language-Model-by-External-Knowledge-on-BioMedical-Named-Entity-Recognition" class="headerlink" title="Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition"></a>Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12278">http://arxiv.org/abs/2309.12278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Bian, Jiaxuan Zheng, Yuyi Zhang, Shanfeng Zhu</li>
<li>for: 这篇论文是为了解决生物医学命名实体识别（BioNER）任务而写的。</li>
<li>methods: 该论文使用链式思维的方法，将NER任务拆分成了实体识别和实体类型决定两个步骤。此外，为了解决LLM缺乏域知识的问题，我们在实体类型决定中注入了实体知识。</li>
<li>results: 实验结果表明，我们的两步 BioNER 方法与之前的几个shot LLMBasis 比较显著提高了表现。此外，含外部知识的 incorporation 也显著提高了实体类型决定性表现。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated dominating performance in many NLP tasks, especially on generative tasks. However, they often fall short in some information extraction tasks, particularly those requiring domain-specific knowledge, such as Biomedical Named Entity Recognition (NER). In this paper, inspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER step-by-step: break down the NER task into entity span extraction and entity type determination. Additionally, for entity type determination, we inject entity knowledge to address the problem that LLM's lack of domain knowledge when predicting entity category. Experimental results show a significant improvement in our two-step BioNER approach compared to previous few-shot LLM baseline. Additionally, the incorporation of external knowledge significantly enhances entity category determination performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-VTE-Identification-through-Adaptive-NLP-Model-Selection-and-Clinical-Expert-Rule-based-Classifier-from-Radiology-Reports"><a href="#Improving-VTE-Identification-through-Adaptive-NLP-Model-Selection-and-Clinical-Expert-Rule-based-Classifier-from-Radiology-Reports" class="headerlink" title="Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports"></a>Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12273">http://arxiv.org/abs/2309.12273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jamie Deng, Yusen Wu, Hilary Hayssen, Brain Englum, Aman Kankaria, Minerva Mayorga-Carlin, Shalini Sahoo, John Sorkin, Brajesh Lal, Yelena Yesha, Phuong Nguyen</li>
<li>for: 这研究旨在提高无结构（自由文本）医疗报告中的深部静脉血栓（DVT）和肺动脉血栓（PE）识别率，以便更好地治疗 cardiovascular disease。</li>
<li>methods: 这研究使用自然语言处理（NLP）方法，包括深度学习（DL）和NLP模型，以自动识别医疗报告中的VTE事件。</li>
<li>results: 实验结果显示，这模型可以实现97%的准确率和97%的F1分数在预测DVT，以及98.3%的准确率和98.4%的F1分数在预测PE。这些结果证明了这模型的稳定性和其在VTE研究中的潜在贡献。<details>
<summary>Abstract</summary>
Rapid and accurate identification of Venous thromboembolism (VTE), a severe cardiovascular condition including deep vein thrombosis (DVT) and pulmonary embolism (PE), is important for effective treatment. Leveraging Natural Language Processing (NLP) on radiology reports, automated methods have shown promising advancements in identifying VTE events from retrospective data cohorts or aiding clinical experts in identifying VTE events from radiology reports. However, effectively training Deep Learning (DL) and the NLP models is challenging due to limited labeled medical text data, the complexity and heterogeneity of radiology reports, and data imbalance. This study proposes novel method combinations of DL methods, along with data augmentation, adaptive pre-trained NLP model selection, and a clinical expert NLP rule-based classifier, to improve the accuracy of VTE identification in unstructured (free-text) radiology reports. Our experimental results demonstrate the model's efficacy, achieving an impressive 97\% accuracy and 97\% F1 score in predicting DVT, and an outstanding 98.3\% accuracy and 98.4\% F1 score in predicting PE. These findings emphasize the model's robustness and its potential to significantly contribute to VTE research.
</details>
<details>
<summary>摘要</summary>
快速和准确地识别深静脉 tromboembolismo (VTE)，包括深静脉 trombosis (DVT) 和肺动脉 trombosis (PE)，是诊断Cardiovascular disease 的关键。通过自然语言处理（NLP）技术，自动方法在从Retrospective data cohorts 中提取VTE事件已经显示出了可观的进步。然而，因为医疗数据的有限性， radiology report的复杂性和多样性，以及数据不均衡，训练深度学习（DL）和NLP模型是一项挑战。这项研究提出了一种 combining DL 方法，并与数据扩展、适应预训练 NLP 模型选择和临床专家 NLP 规则生成器，以提高无结构（free-text） radiology report中VTE识别的准确性。我们的实验结果表明，该模型在 DVT 预测中达到了97%的准确率和97%的 F1 分数，而在 PE 预测中达到了98.3%的准确率和98.4%的 F1 分数。这些结果证明了模型的稳定性，并且其潜在地对 VTE 研究做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="The-Cambridge-Law-Corpus-A-Corpus-for-Legal-AI-Research"><a href="#The-Cambridge-Law-Corpus-A-Corpus-for-Legal-AI-Research" class="headerlink" title="The Cambridge Law Corpus: A Corpus for Legal AI Research"></a>The Cambridge Law Corpus: A Corpus for Legal AI Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12269">http://arxiv.org/abs/2309.12269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Östling, Holli Sargeant, Huiyuan Xie, Ludwig Bull, Alexander Terenin, Leif Jonsson, Måns Magnusson, Felix Steffek</li>
<li>for: 这个论文是为了推广法律人工智能研究而创建的剑桥法律词库（CLC）。</li>
<li>methods: 该词库包含了250,000余个法律案例，主要来自21世纪，但也包括16世纪以前的案例。论文还提供了638个案例的注释，由法律专家进行标注。</li>
<li>results: 通过使用GPT-3、GPT-4和RoBERTa模型进行训练和评估， authors提供了情况出来案例结果抽取的基准。<details>
<summary>Abstract</summary>
We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research. It consists of over 250 000 court cases from the UK. Most cases are from the 21st century, but the corpus includes cases as old as the 16th century. This paper presents the first release of the corpus, containing the raw text and meta-data. Together with the corpus, we provide annotations on case outcomes for 638 cases, done by legal experts. Using our annotated data, we have trained and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to provide benchmarks. We include an extensive legal and ethical discussion to address the potentially sensitive nature of this material. As a consequence, the corpus will only be released for research purposes under certain restrictions.
</details>
<details>
<summary>摘要</summary>
我们介绍了剑桥法律词库（CLC），一个用于法律人工智能研究的词库。它包含了超过250,000个法律案例，大多数是21世纪的案例，但词库还包括了16世纪的案例。本文发布了词库的首个版本，包括原始文本和元数据。同时，我们提供了638个案例的法律专家标注，用于训练和评估案例结果提取模型。我们还进行了广泛的法律和伦理讨论，以Address the potentially sensitive nature of this material。因此，词库将仅为研究用途发布，受到一定的限制。
</details></li>
</ul>
<hr>
<h2 id="On-the-Relationship-between-Skill-Neurons-and-Robustness-in-Prompt-Tuning"><a href="#On-the-Relationship-between-Skill-Neurons-and-Robustness-in-Prompt-Tuning" class="headerlink" title="On the Relationship between Skill Neurons and Robustness in Prompt Tuning"></a>On the Relationship between Skill Neurons and Robustness in Prompt Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12263">http://arxiv.org/abs/2309.12263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leon Ackermann, Xenia Ohmer</li>
<li>for: 这个论文的目的是研究Prompt Tuning在PLMs中的稳定性，以及这种方法如何在不同任务上进行转移学习。</li>
<li>methods: 作者使用了RoBERTa和T5进行实验，并通过分析Prompt Tuning activates specific neurons in transformer的feed-forward networks，以及这些神经元在不同任务上的表现，来研究Prompt Tuning的稳定性。</li>
<li>results: 研究发现，Prompt Tuning在不同任务上的表现不够稳定，尤其是在对抗数据上。而T5比RoBERTa更高的对抗 robustness可能与模型在对抗数据上活动的相关神经元有关。同时，研究还发现了RoBERTa和T5中的技能神经元，并证明了这些神经元在不同任务上的表现。<details>
<summary>Abstract</summary>
Prompt Tuning is a popular parameter-efficient finetuning method for pre-trained large language models (PLMs). Recently, based on experiments with RoBERTa, it has been suggested that Prompt Tuning activates specific neurons in the transformer's feed-forward networks, that are highly predictive and selective for the given task. In this paper, we study the robustness of Prompt Tuning in relation to these "skill neurons", using RoBERTa and T5. We show that prompts tuned for a specific task are transferable to tasks of the same type but are not very robust to adversarial data, with higher robustness for T5 than RoBERTa. At the same time, we replicate the existence of skill neurons in RoBERTa and further show that skill neurons also seem to exist in T5. Interestingly, the skill neurons of T5 determined on non-adversarial data are also among the most predictive neurons on the adversarial data, which is not the case for RoBERTa. We conclude that higher adversarial robustness may be related to a model's ability to activate the relevant skill neurons on adversarial data.
</details>
<details>
<summary>摘要</summary>
Prompt Tuning 是一种广泛使用的减少参数的 finetuning 方法 для预训练大语言模型（PLM）。最近，通过 RoBERTa 的实验，提出了 Prompt Tuning 可以活化特定的 neuron 在 transformer 的Feedforward 网络中，这些 neuron 对给定任务是非常预测和选择性的。在这篇文章中，我们研究了 Prompt Tuning 的稳定性，与这些“技能 neuron”相关。我们使用 RoBERTa 和 T5 进行实验，并发现：Prompt Tuning 为特定任务适应性较高，但对 adversarial 数据不够稳定。同时，我们复现了 RoBERTa 中的技能 neuron，并发现 T5 中的技能 neuron 也存在。有趣的是，T5 中的技能 neuron 在非 adversarial 数据上确定的也是最预测性的 neuron 在 adversarial 数据上，而 RoBERTa 中的技能 neuron 不是。我们 conclude  что高度 adversarial 稳定性可能与模型的能力启动相关的技能 neuron 在 adversarial 数据上有关。
</details></li>
</ul>
<hr>
<h2 id="SQUARE-Automatic-Question-Answering-Evaluation-using-Multiple-Positive-and-Negative-References"><a href="#SQUARE-Automatic-Question-Answering-Evaluation-using-Multiple-Positive-and-Negative-References" class="headerlink" title="SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References"></a>SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12250">http://arxiv.org/abs/2309.12250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Gabburo, Siddhant Garg, Rik Koncel Kedziorski, Alessandro Moschitti</li>
<li>for: 评估问答系统的可靠性和效果是非常困难和昂贵的，现有的最可靠的方法是人工标注问答的正确性。</li>
<li>methods: 我们提出了一个新的评估指标：SQuArE（句子级问答回答评估），使用多个参考答案（组合多个正确和错误的参考答案）来评估句子级问答系统。</li>
<li>results: 我们在多个学术和工业数据集上评估了SQuArE，并发现它比前一些基线表现更好，并与人工标注之间有最高的相关性。<details>
<summary>Abstract</summary>
Evaluation of QA systems is very challenging and expensive, with the most reliable approach being human annotations of correctness of answers for questions. Recent works (AVA, BEM) have shown that transformer LM encoder based similarity metrics transfer well for QA evaluation, but they are limited by the usage of a single correct reference answer. We propose a new evaluation metric: SQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple reference answers (combining multiple correct and incorrect references) for sentence-form QA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) and generative (GenQA) QA systems, across multiple academic and industrial datasets, and show that it outperforms previous baselines and obtains the highest correlation with human annotations.
</details>
<details>
<summary>摘要</summary>
评估问答系统非常具有挑战性和成本高，人工注解正确答案为问题的最可靠方法。最近的研究（AVA、BEM）表明，基于转换器LM推理器的相似度指标可以有效地评估问答系统，但它们受到单个正确参考答案的限制。我们提议一种新的评估指标：SQuArE（句子级问题答案评估），使用多个参考答案（包括多个正确和错误参考）进行句子级问题评估。我们对多个学术和工业数据集进行了评估，并发现SQuArE超过了之前的基线和人工注解之间的相关性。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gaps-of-Both-Modality-and-Language-Synchronous-Bilingual-CTC-for-Speech-Translation-and-Speech-Recognition"><a href="#Bridging-the-Gaps-of-Both-Modality-and-Language-Synchronous-Bilingual-CTC-for-Speech-Translation-and-Speech-Recognition" class="headerlink" title="Bridging the Gaps of Both Modality and Language: Synchronous Bilingual CTC for Speech Translation and Speech Recognition"></a>Bridging the Gaps of Both Modality and Language: Synchronous Bilingual CTC for Speech Translation and Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12234">http://arxiv.org/abs/2309.12234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Xu, Xiaoqian Liu, Erfeng He, Yuhao Zhang, Qianqian Dong, Tong Xiao, Jingbo Zhu, Dapeng Man, Wu Yang</li>
<li>for: 这项研究旨在提出同步双语Connectionist Temporal Classification（CTC）框架，用于bridging模式和语言之间的差异在语音翻译（ST）任务中。</li>
<li>methods: 我们使用了训练录音和翻译为同时目标的CTC，将audio和文本之间的差异桥接，以及源语言和目标语言之间的差异。我们基于 latest CTC应用的进步，开发了加强版BiL-CTC+，在资源受限的情况下在MuST-C STbenchmark上达到新的州OF-THE-ART表现。</li>
<li>results: 我们的方法不仅在ST任务中实现了新的州OF-THE-ART表现，还显著提高了语音识别性能， demonstarting cross-lingual learning的广泛应用和语音识别的相互关系。<details>
<summary>Abstract</summary>
In this study, we present synchronous bilingual Connectionist Temporal Classification (CTC), an innovative framework that leverages dual CTC to bridge the gaps of both modality and language in the speech translation (ST) task. Utilizing transcript and translation as concurrent objectives for CTC, our model bridges the gap between audio and text as well as between source and target languages. Building upon the recent advances in CTC application, we develop an enhanced variant, BiL-CTC+, that establishes new state-of-the-art performances on the MuST-C ST benchmarks under resource-constrained scenarios. Intriguingly, our method also yields significant improvements in speech recognition performance, revealing the effect of cross-lingual learning on transcription and demonstrating its broad applicability. The source code is available at https://github.com/xuchennlp/S2T.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们提出了同步双语Connectionist Temporal Classification（CTC）框架，这是一种创新的框架，可以跨越语言和modalities在语音翻译（ST）任务中bridges the gaps。我们利用讲解和翻译作为同时目标，我们的模型可以将音频和文本相互关联，以及源语言和目标语言之间的关联。 builds upon the recent advances in CTC application, we develop an enhanced variant, BiL-CTC+, that establishes new state-of-the-art performances on the MuST-C ST benchmarks under resource-constrained scenarios. Intriguingly, our method also yields significant improvements in speech recognition performance, revealing the effect of cross-lingual learning on transcription and demonstrating its broad applicability. source code is available at https://github.com/xuchennlp/S2T.
</details></li>
</ul>
<hr>
<h2 id="Towards-Answering-Health-related-Questions-from-Medical-Videos-Datasets-and-Approaches"><a href="#Towards-Answering-Health-related-Questions-from-Medical-Videos-Datasets-and-Approaches" class="headerlink" title="Towards Answering Health-related Questions from Medical Videos: Datasets and Approaches"></a>Towards Answering Health-related Questions from Medical Videos: Datasets and Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12224">http://arxiv.org/abs/2309.12224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deepak Gupta, Kush Attal, Dina Demner-Fushman</li>
<li>for:  Answering health-related questions asked by the public through visual answers from medical videos.</li>
<li>methods:  Created two large-scale datasets (HealthVidQA-CRF and HealthVidQA-Prompt) and proposed monomodal and multimodal approaches to provide visual answers from medical videos to natural language questions.</li>
<li>results:  Datasets have the potential to enhance the performance of medical visual answer localization tasks, and pre-trained language-vision models may further improve performance.Here’s the Chinese translation:</li>
<li>for:  Answering 公众健康问题通过医疗视频的视觉答案。</li>
<li>methods:  开发了两个大规模数据集（HealthVidQA-CRF 和 HealthVidQA-Prompt），并提出了单模态和多模态方法，以寻找医疗视频中的自然语言问题的视觉答案。</li>
<li>results: 数据集可能提高医疗视answer定位任务的性能，并且可能通过预训练语言视觉模型进一步提高性能。<details>
<summary>Abstract</summary>
The increase in the availability of online videos has transformed the way we access information and knowledge. A growing number of individuals now prefer instructional videos as they offer a series of step-by-step procedures to accomplish particular tasks. The instructional videos from the medical domain may provide the best possible visual answers to first aid, medical emergency, and medical education questions. Toward this, this paper is focused on answering health-related questions asked by the public by providing visual answers from medical videos. The scarcity of large-scale datasets in the medical domain is a key challenge that hinders the development of applications that can help the public with their health-related questions. To address this issue, we first proposed a pipelined approach to create two large-scale datasets: HealthVidQA-CRF and HealthVidQA-Prompt. Later, we proposed monomodal and multimodal approaches that can effectively provide visual answers from medical videos to natural language questions. We conducted a comprehensive analysis of the results, focusing on the impact of the created datasets on model training and the significance of visual features in enhancing the performance of the monomodal and multi-modal approaches. Our findings suggest that these datasets have the potential to enhance the performance of medical visual answer localization tasks and provide a promising future direction to further enhance the performance by using pre-trained language-vision models.
</details>
<details>
<summary>摘要</summary>
随着在线视频的普及，我们获取信息和知识的方式发生了深刻的变革。更多的人现在偏好使用说明视频，因为它们提供了一系列步骤的操作来完成特定任务。医疗领域的 instruccional videos 可以提供最佳的视觉答案，用于医疗问题的第一 aid、紧急情况和医学教育。为此，本文将关注公众提出的健康问题，通过医疗视频提供视觉答案。医疗领域的数据匮乏是主要挑战，这阻碍了应用程序的发展，用于帮助公众解决其健康问题。为解决这个问题，我们首先提出了一种管道方法，用于创建两个大规模数据集：HealthVidQA-CRF 和 HealthVidQA-Prompt。后来，我们提出了单模态和多模态方法，可以有效地从医疗视频中提取视觉答案，并与自然语言问题进行对应。我们对结果进行了全面的分析，注重数据集的创建对模型训练的影响，以及视觉特征在单模态和多模态方法中的重要性。我们的发现表明，这些数据集有potentiality 提高医疗视 Answer Localization 任务的性能，并提供了未来可能性，通过使用预训练语言视觉模型，进一步提高性能。
</details></li>
</ul>
<hr>
<h2 id="Code-Soliloquies-for-Accurate-Calculations-in-Large-Language-Models"><a href="#Code-Soliloquies-for-Accurate-Calculations-in-Large-Language-Models" class="headerlink" title="Code Soliloquies for Accurate Calculations in Large Language Models"></a>Code Soliloquies for Accurate Calculations in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12161">http://arxiv.org/abs/2309.12161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luffycodes/tutorbot-spock-phys">https://github.com/luffycodes/tutorbot-spock-phys</a></li>
<li>paper_authors: Shashank Sonkar, MyCo Le, Xinghe Chen, Naiming Liu, Debshila Basu Mallick, Richard G. Baraniuk</li>
<li>for: 这paper是为了提高大型语言模型（LLM）后端的开发而写的，具体来说是为了提高学生和ITS之间的互动质量。</li>
<li>methods: 这paper使用了先进的GPT-4模型来生成合成学生教师对话，以便用于精度地准确地训练LLM后端。</li>
<li>results: 这paper的结果表明，使用我们的新的状态full prompt设计可以增强合成对话集的质量，特别是在需要计算的科学概念上。我们的Higgs模型（一个LLaMAfinetune的模型）能够有效地使用Python进行计算，并且通过使用我们生成的代码演讲来提高它的答案的准确性和计算可靠性。<details>
<summary>Abstract</summary>
High-quality conversational datasets are integral to the successful development of Intelligent Tutoring Systems (ITS) that employ a Large Language Model (LLM) backend. These datasets, when used to fine-tune the LLM backend, significantly enhance the quality of interactions between students and ITS. A common strategy for developing these datasets involves generating synthetic student-teacher dialogues using advanced GPT-4 models. However, challenges arise when these dialogues demand complex calculations, common in subjects like physics. Despite its advanced capabilities, GPT-4's performance falls short in reliably handling even simple multiplication tasks, marking a significant limitation in its utility for these subjects. To address these challenges, this paper introduces an innovative stateful prompt design. Our approach generates a mock conversation between a student and a tutorbot, both roles simulated by GPT-4. Each student response triggers a soliloquy (an inner monologue) in the GPT-tutorbot, which assesses whether its response would necessitate calculations. If so, it proceeds to script the required code in Python and then uses the resulting output to construct its response to the student. Our approach notably enhances the quality of synthetic conversation datasets, especially for subjects that are calculation-intensive. Our findings show that our Higgs model -- a LLaMA finetuned with datasets generated through our novel stateful prompt design -- proficiently utilizes Python for computations. Consequently, finetuning with our datasets enriched with code soliloquies enhances not just the accuracy but also the computational reliability of Higgs' responses.
</details>
<details>
<summary>摘要</summary>
高品质的对话Dataset是智能教学系统（ITS）的成功开发所需的一个重要组成部分。这些Dataset，当用于调整LLM后端，将会对学生和ITS之间的互动提高质量。一种常见的发展策略是使用进步的GPT-4模型生成 sintetic的学生-教师对话。但是，当这些对话需要复杂的计算时，GPT-4的表现会变差，这是一个重要的限制，它对于这些主题的 utility 有限。为了解决这些挑战，这篇文章提出了一个创新的状态执行Prompt设计。我们的方法生成了一个模拟学生和教师的对话，这两个角色都是由GPT-4 simulate。每个学生回应都会触发GPT-tutorbot的内言（soliloquy），判断其回应是否需要计算。如果是，它会进行Python脚本的scripting，然后使用结果来建立对学生的回应。我们的方法对于计算数量充满的主题特别有助，我们的发现显示，我们的Higgs模型（LLaMA finetuned with我们的新状态执行Prompt设计）能够有效地使用Python进行计算。因此，在我们的Dataset中添加了code soliloquies后，调整Higgs的精度和计算可靠性都会提高。
</details></li>
</ul>
<hr>
<h2 id="How-to-Guides-for-Specific-Audiences-A-Corpus-and-Initial-Findings"><a href="#How-to-Guides-for-Specific-Audiences-A-Corpus-and-Initial-Findings" class="headerlink" title="How-to Guides for Specific Audiences: A Corpus and Initial Findings"></a>How-to Guides for Specific Audiences: A Corpus and Initial Findings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12117">http://arxiv.org/abs/2309.12117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicola Fanton, Agnieszka Falenska, Michael Roth</li>
<li>for: 这篇论文探讨了wikiHow上的指南文章是如何针对不同目标群体而变化的。</li>
<li>methods: 研究者采用了资深分析和计算方法来检测wikiHow上的指南文章是否受到社会偏见和潜伏偏见的影响。</li>
<li>results: 研究结果表明，wikiHow上的指南文章受到了潜伏偏见，且这些偏见随着目标群体的不同而发生变化。<details>
<summary>Abstract</summary>
Instructional texts for specific target groups should ideally take into account the prior knowledge and needs of the readers in order to guide them efficiently to their desired goals. However, targeting specific groups also carries the risk of reflecting disparate social norms and subtle stereotypes. In this paper, we investigate the extent to which how-to guides from one particular platform, wikiHow, differ in practice depending on the intended audience. We conduct two case studies in which we examine qualitative features of texts written for specific audiences. In a generalization study, we investigate which differences can also be systematically demonstrated using computational methods. The results of our studies show that guides from wikiHow, like other text genres, are subject to subtle biases. We aim to raise awareness of these inequalities as a first step to addressing them in future work.
</details>
<details>
<summary>摘要</summary>
instrucitonal 文本应该考虑目标群体的先前知识和需求，以有效地引导他们达到他们的目标。然而，targeting 特定群体也可能表达不同的社会规范和潜在偏见。在这篇论文中，我们调查了wikiHow的how-to 指南是否因target audience而有所不同。我们进行了两项案例研究，以及一项通用研究，以系统地表明这些差异。我们的研究结果表明，wikiHow的指南，如其他文本类型，受到潜在偏见的影响。我们希望通过这篇论文来启发人们对这些不平等的意识，以便在未来的工作中解决它们。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Computational-Analysis-of-Vagueness-in-Revisions-of-Instructional-Texts"><a href="#A-Computational-Analysis-of-Vagueness-in-Revisions-of-Instructional-Texts" class="headerlink" title="A Computational Analysis of Vagueness in Revisions of Instructional Texts"></a>A Computational Analysis of Vagueness in Revisions of Instructional Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12107">http://arxiv.org/abs/2309.12107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alok Debnath, Michael Roth</li>
<li>for: 本研究旨在分析 revision history 中带有uncertainty的 instruction 的修改。</li>
<li>methods: 研究采用 neural network 模型，对两个版本的 instruction 进行对比，并采用 previous work 中的 pairwise ranking 任务来评价模型的能力。</li>
<li>results: 研究显示，使用 neural network 模型可以准确地分辨两个版本的 instruction，并且与existig baselines 相比，显示出提高的性能。<details>
<summary>Abstract</summary>
WikiHow is an open-domain repository of instructional articles for a variety of tasks, which can be revised by users. In this paper, we extract pairwise versions of an instruction before and after a revision was made. Starting from a noisy dataset of revision histories, we specifically extract and analyze edits that involve cases of vagueness in instructions. We further investigate the ability of a neural model to distinguish between two versions of an instruction in our data by adopting a pairwise ranking task from previous work and showing improvements over existing baselines.
</details>
<details>
<summary>摘要</summary>
WikiHow 是一个开放领域的指南文章存储库，可以由用户修改。在这篇论文中，我们从含有噪声的修订历史数据中提取了对 instrucion 进行了修改的对。我们specifically 提取和分析修订中包含抽象指令的修订。进一步，我们采用了一种对两个 instrucion 版本进行对比的 neural 模型，并证明我们的模型可以在我们的数据集中提高对比性。Note: " instrucion" is a typo in the original text, and I have corrected it to "instruction" in the translation.
</details></li>
</ul>
<hr>
<h2 id="SemEval-2022-Task-7-Identifying-Plausible-Clarifications-of-Implicit-and-Underspecified-Phrases-in-Instructional-Texts"><a href="#SemEval-2022-Task-7-Identifying-Plausible-Clarifications-of-Implicit-and-Underspecified-Phrases-in-Instructional-Texts" class="headerlink" title="SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts"></a>SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12102">http://arxiv.org/abs/2309.12102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/acidann/claire">https://github.com/acidann/claire</a></li>
<li>paper_authors: Michael Roth, Talita Anthonio, Anna Sauer</li>
<li>for: 这项研究的目的是评估帮助文章的解释是否有效。</li>
<li>methods: 这项研究使用了人工生成的解释和人类的可能性评估来训练参与系统。</li>
<li>results: 参与系统的最佳系统达到了68.9%的准确率，而8个团队的系统描述也被报告。此外，我们还发现了使用最佳参与系统的预测可以在多个可能的解释上达到75.2%的准确率。<details>
<summary>Abstract</summary>
We describe SemEval-2022 Task 7, a shared task on rating the plausibility of clarifications in instructional texts. The dataset for this task consists of manually clarified how-to guides for which we generated alternative clarifications and collected human plausibility judgements. The task of participating systems was to automatically determine the plausibility of a clarification in the respective context. In total, 21 participants took part in this task, with the best system achieving an accuracy of 68.9%. This report summarizes the results and findings from 8 teams and their system descriptions. Finally, we show in an additional evaluation that predictions by the top participating team make it possible to identify contexts with multiple plausible clarifications with an accuracy of 75.2%.
</details>
<details>
<summary>摘要</summary>
我们描述SemEval-2022任务7，这是一个评估 instrucitonal 文本中的解释可能性的共同任务。该任务的数据集包括 manually clarified 的使用指南，我们生成了备用的解释，并收集了人类的可能性评估。参与系统的任务是自动确定解释的可能性在特定上下文中。总共有21个参与者，最佳系统的准确率达到68.9%。这份报告Summarizes 8个团队和他们的系统描述，并在附加评估中表明了最佳参与者的预测可以在多个可能的解释上准确地识别 context 的准确率达到75.2%。
</details></li>
</ul>
<hr>
<h2 id="AceGPT-Localizing-Large-Language-Models-in-Arabic"><a href="#AceGPT-Localizing-Large-Language-Models-in-Arabic" class="headerlink" title="AceGPT, Localizing Large Language Models in Arabic"></a>AceGPT, Localizing Large Language Models in Arabic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12053">http://arxiv.org/abs/2309.12053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/freedomintelligence/acegpt">https://github.com/freedomintelligence/acegpt</a></li>
<li>paper_authors: Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Ziche Liu, Zhiyi Zhang, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, Jinchao Xu</li>
<li>for: 本研究旨在开发一个适应阿拉伯语言特点的本地大型自然语言处理模型（LLM），以满足阿拉伯语言speaking community的多样化应用需求。</li>
<li>methods: 该研究提出了一个包装解决方案，包括额外预训练阿拉伯文本，监督精度调整（SFT）使用本地阿拉伯语言指令和GPT-4响应，以及强化学习使用人工智能反馈（RLAIF）的奖励模型，以训练具有当地文化和价值观念的阿拉伯语言LLM。</li>
<li>results: 对于多种测试 benchmark，包括阿拉伯语言 Vicuna-80 和 AlpacaEval 的 instrucion-following benchmark、阿拉伯语言 MMLU 和 EXAMs 的知识 benchmark，以及新提出的阿拉伯文化和价值Alignment benchmark，AceGPT 得到了最高的 SOTA 开放阿拉伯语言 LLM 成绩。尤其是在使用 GPT-4 时，AceGPT 在 Vicuna-80  benchmark 中高于 ChatGPT，尽管这个benchmark的规模相对较小。<details>
<summary>Abstract</summary>
This paper explores the imperative need and methodology for developing a localized Large Language Model (LLM) tailored for Arabic, a language with unique cultural characteristics that are not adequately addressed by current mainstream models like ChatGPT. Key concerns additionally arise when considering cultural sensitivity and local values. To this end, the paper outlines a packaged solution, including further pre-training with Arabic texts, supervised fine-tuning (SFT) using native Arabic instructions and GPT-4 responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using a reward model that is sensitive to local culture and values. The objective is to train culturally aware and value-aligned Arabic LLMs that can serve the diverse application-specific needs of Arabic-speaking communities.   Extensive evaluations demonstrated that the resulting LLM called `\textbf{AceGPT}' is the SOTA open Arabic LLM in various benchmarks, including instruction-following benchmark (i.e., Arabic Vicuna-80 and Arabic AlpacaEval), knowledge benchmark (i.e., Arabic MMLU and EXAMs), as well as the newly-proposed Arabic cultural \& value alignment benchmark. Notably, AceGPT outperforms ChatGPT in the popular Vicuna-80 benchmark when evaluated with GPT-4, despite the benchmark's limited scale. % Natural Language Understanding (NLU) benchmark (i.e., ALUE)   Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了需要开发本地化的大语言模型（LLM），以满足阿拉伯语言的独特文化特征，现有主流模型如ChatGPT无法充分考虑。文章还提出了一个套件解决方案，包括额外预训练阿拉伯文本，监督细化（SFT）使用本地阿拉伯指令和GPT-4回答，以及强化学习使用人工智能反馈（RLAIF）的奖励模型，以训练具有本地文化和价值观的阿拉伯语言模型。这些模型可以满足阿拉伯语言社区的多样化应用需求。经过评估， authors 提出了名为 `\textbf{AceGPT}` 的模型，它在不同的测试上达到了最高的表现，包括 instruc-following 测试（i.e., Arabic Vicuna-80和Arabic AlpacaEval）、知识测试（i.e., Arabic MMLU和EXAMs）以及 newly-proposed 阿拉伯文化和价值观念测试。特别是，AceGPT 在 Vicuna-80 测试中，使用 GPT-4 时与 ChatGPT 进行比较，即使测试的规模较小。codes、数据和模型可以在 GitHub 上找到：<https://github.com/FreedomIntelligence/AceGPT>。
</details></li>
</ul>
<hr>
<h2 id="CAMERA-A-Multimodal-Dataset-and-Benchmark-for-Ad-Text-Generation"><a href="#CAMERA-A-Multimodal-Dataset-and-Benchmark-for-Ad-Text-Generation" class="headerlink" title="CAMERA: A Multimodal Dataset and Benchmark for Ad Text Generation"></a>CAMERA: A Multimodal Dataset and Benchmark for Ad Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12030">http://arxiv.org/abs/2309.12030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masato Mita, Soichiro Murakami, Akihiko Kato, Peinan Zhang</li>
<li>for: 本研究旨在提高自动广告文本生成（ATG）领域的研究，并为其引入了一个重新定义的任务和一个标准评价 bencmark。</li>
<li>methods: 本研究使用了多种方法，包括使用不同类型的预训练语言模型和利用多Modal信息。</li>
<li>results: 研究者通过多个基线模型的评价实验，发现CA Multimodal Evaluation for Ad Text GeneRAtion（CAMERA）数据集能够充分利用多Modal信息，并且可以进行产业综合评价。<details>
<summary>Abstract</summary>
In response to the limitations of manual online ad production, significant research has been conducted in the field of automatic ad text generation (ATG). However, comparing different methods has been challenging because of the lack of benchmarks encompassing the entire field and the absence of well-defined problem sets with clear model inputs and outputs. To address these challenges, this paper aims to advance the field of ATG by introducing a redesigned task and constructing a benchmark. Specifically, we defined ATG as a cross-application task encompassing various aspects of the Internet advertising. As part of our contribution, we propose a first benchmark dataset, CA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA), carefully designed for ATG to be able to leverage multi-modal information and conduct an industry-wise evaluation. Furthermore, we demonstrate the usefulness of our proposed benchmark through evaluation experiments using multiple baseline models, which vary in terms of the type of pre-trained language model used and the incorporation of multi-modal information. We also discuss the current state of the task and the future challenges.
</details>
<details>
<summary>摘要</summary>
Traditional online advertising production has limitations, so researchers have been studying automatic ad text generation (ATG) to address these limitations. However, comparing different ATG methods has been difficult due to a lack of benchmarks that cover the entire field and well-defined problem sets with clear inputs and outputs. To solve these problems, this paper aims to advance the field of ATG by introducing a new task and creating a benchmark. Specifically, we define ATG as a cross-application task that includes various aspects of internet advertising. As part of our contribution, we propose a benchmark dataset called CA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA), which is carefully designed for ATG and can leverage multi-modal information to conduct an industry-wide evaluation. We also demonstrate the usefulness of our proposed benchmark through evaluation experiments using multiple baseline models, which differ in the type of pre-trained language model used and the incorporation of multi-modal information. Finally, we discuss the current state of the task and future challenges.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Stock-Market-Sentiment-Classification-and-Backtesting-via-Fine-tuned-BERT"><a href="#Stock-Market-Sentiment-Classification-and-Backtesting-via-Fine-tuned-BERT" class="headerlink" title="Stock Market Sentiment Classification and Backtesting via Fine-tuned BERT"></a>Stock Market Sentiment Classification and Backtesting via Fine-tuned BERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11979">http://arxiv.org/abs/2309.11979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiashu Lou</li>
<li>for: 本研究的目的是将情感因素纳入量化交易中，以提高交易效率和回换率。</li>
<li>methods: 本研究使用BERT自然语言处理模型进行训练，并将用户评论标题数据网络爬虫，进行数据清洁和处理。</li>
<li>results: 实验结果显示，将情感因素纳入量化交易中，可以提高交易效率和回换率，比基准模型和原始Alpha191模型有73.8%和32.41%的提升。<details>
<summary>Abstract</summary>
With the rapid development of big data and computing devices, low-latency automatic trading platforms based on real-time information acquisition have become the main components of the stock trading market, so the topic of quantitative trading has received widespread attention. And for non-strongly efficient trading markets, human emotions and expectations always dominate market trends and trading decisions. Therefore, this paper starts from the theory of emotion, taking East Money as an example, crawling user comment titles data from its corresponding stock bar and performing data cleaning. Subsequently, a natural language processing model BERT was constructed, and the BERT model was fine-tuned using existing annotated data sets. The experimental results show that the fine-tuned model has different degrees of performance improvement compared to the original model and the baseline model. Subsequently, based on the above model, the user comment data crawled is labeled with emotional polarity, and the obtained label information is combined with the Alpha191 model to participate in regression, and significant regression results are obtained. Subsequently, the regression model is used to predict the average price change for the next five days, and use it as a signal to guide automatic trading. The experimental results show that the incorporation of emotional factors increased the return rate by 73.8\% compared to the baseline during the trading period, and by 32.41\% compared to the original alpha191 model. Finally, we discuss the advantages and disadvantages of incorporating emotional factors into quantitative trading, and give possible directions for further research in the future.
</details>
<details>
<summary>摘要</summary>
随着大数据和计算设备的快速发展，低延迟自动交易平台基于实时信息获取已成为股票交易市场的主要组成部分，因此量化交易的话题受到了广泛关注。而在不强效交易市场中，人类情感和期望总是控制市场趋势和交易决策。因此，本文从情感理论出发，以东方财富为例，从其相应股票板幕中提取用户评论标题数据，并进行数据清洁。然后，构建了自然语言处理模型BERT，并将BERT模型细化使用现有注解数据集。实验结果显示，细化模型在原始模型和基准模型的比较中具有不同程度的性能改进。接着，基于上述模型，提取的用户评论数据被标注为情感方向，并将获得的标签信息与Alpha191模型结合进行回归，并获得了显著的回归结果。然后，使用回归模型预测下一五天的均价变化，并使其作为自动交易的信号导航。实验结果显示，包含情感因素的integration提高了基准期望的回报率73.8%，相比基准期望模型。最后，我们讨论了在量化交易中包含情感因素的优势和缺点，并提出了未来可能的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Scaling-up-COMETKIWI-Unbabel-IST-2023-Submission-for-the-Quality-Estimation-Shared-Task"><a href="#Scaling-up-COMETKIWI-Unbabel-IST-2023-Submission-for-the-Quality-Estimation-Shared-Task" class="headerlink" title="Scaling up COMETKIWI: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task"></a>Scaling up COMETKIWI: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11925">http://arxiv.org/abs/2309.11925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Rei, Nuno M. Guerreiro, José Pombal, Daan van Stigt, Marcos Treviso, Luisa Coheur, José G. C. de Souza, André F. T. Martins</li>
<li>for: 这个论文是为了参加WMT 2023共同任务中的质量估计（QE）任务而写的。</li>
<li>methods: 这个论文使用了COMETKIWI-22模型（Rei et al., 2022b），并在所有任务上进行了多语言方法的探索。</li>
<li>results: 这个论文的 multilingual 方法在所有任务上达到了状态的性能，并与人类评估相关度（Spearman 相关度）之间显示了大幅提升（最多10个Spearman点），同时也超过了第二名的多语言提交。<details>
<summary>Abstract</summary>
We present the joint contribution of Unbabel and Instituto Superior T\'ecnico to the WMT 2023 Shared Task on Quality Estimation (QE). Our team participated on all tasks: sentence- and word-level quality prediction (task 1) and fine-grained error span detection (task 2). For all tasks, we build on the COMETKIWI-22 model (Rei et al., 2022b). Our multilingual approaches are ranked first for all tasks, reaching state-of-the-art performance for quality estimation at word-, span- and sentence-level granularity. Compared to the previous state-of-the-art COMETKIWI-22, we show large improvements in correlation with human judgements (up to 10 Spearman points). Moreover, we surpass the second-best multilingual submission to the shared-task with up to 3.8 absolute points.
</details>
<details>
<summary>摘要</summary>
我们现在报告我们和 Instituto Superior Técnico 在 WMT 2023 共同任务中的合作贡献，我们参加了所有任务：句子和单词水平质量预测（任务 1）以及细致错误探测（任务 2）。对于所有任务，我们基于 COMETKIWI-22 模型（Rei et al., 2022b）。我们的多语言方法在所有任务上排名第一，达到了质量预测的州际先进性，包括单词、句子和span级别的准确性。相比之前的州际先进 COMETKIWI-22，我们显示出了大幅提升与人类评估的相关度（最高达 10 点）。此外，我们超过了共同任务中的第二名多语言提交，差异达到 3.8 个绝对点。
</details></li>
</ul>
<hr>
<h2 id="InstructERC-Reforming-Emotion-Recognition-in-Conversation-with-a-Retrieval-Multi-task-LLMs-Framework"><a href="#InstructERC-Reforming-Emotion-Recognition-in-Conversation-with-a-Retrieval-Multi-task-LLMs-Framework" class="headerlink" title="InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework"></a>InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11911">http://arxiv.org/abs/2309.11911</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LIN-SHANG/InstructERC">https://github.com/LIN-SHANG/InstructERC</a></li>
<li>paper_authors: Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, Sirui Wang</li>
<li>for: 提高对话中情感识别（ERC）的发展，解决了过度适应特定数据集和对话模式的问题。</li>
<li>methods: 提出了一种新的approach，即InstructERC，将ERC任务从推断性框架转换到生成性框架，基于大语言模型（LLM）。</li>
<li>results: InstructERC在三个常用的ERC数据集上 achieve 独特的SOTA，并通过 parameter-efficient 和 data-scaling 实验提供了实践场景中的参考指南。<details>
<summary>Abstract</summary>
The development of emotion recognition in dialogue (ERC) has been consistently hindered by the complexity of pipeline designs, leading to ERC models that often overfit to specific datasets and dialogue patterns. In this study, we propose a novel approach, namely   InstructERC, to reformulates the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs) . InstructERC has two significant contributions: Firstly, InstructERC introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information by concatenating the historical dialog content, label statement, and emotional domain demonstrations with high semantic similarity. Furthermore, we introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations. Our LLM-based plug-and-play plugin framework significantly outperforms all previous models and achieves comprehensive SOTA on three commonly used ERC datasets. Extensive analysis of parameter-efficient and data-scaling experiments provide empirical guidance for applying InstructERC in practical scenarios. Our code will be released after blind review.
</details>
<details>
<summary>摘要</summary>
developoment of emotion recognition in dialogue (ERC) 被复杂的管道设计所阻碍，导致 ERC 模型经常过拟合特定的数据集和对话模式。在本研究中，我们提出了一种新的方法，即 InstructERC，它将 ERC 任务从描述性框架转换为生成框架，基于大型语言模型（LLM）。InstructERC 具有两个重要贡献：首先，InstructERC 引入了一种简单 yet 有效的检索模板模块，该模块通过 concatenate 历史对话内容、标签声明和情感领域示例来显式地整合多级别对话监督信息。其次，我们引入了两个附加的情感对应任务，即说话人标识和情感预测任务，以impllicitly 模型对话角色关系和未来情感趋势在对话中。我们的 LLM 基于插件框架在三个常用的 ERC 数据集上达到了广泛的 SOTA 水平。我们进行了参数高效和数据扩展的实验，以提供实践场景中应用 InstructERC 的实际指南。我们的代码将在审稿后发布。
</details></li>
</ul>
<hr>
<h2 id="Focal-Inferential-Infusion-Coupled-with-Tractable-Density-Discrimination-for-Implicit-Hate-Speech-Detection"><a href="#Focal-Inferential-Infusion-Coupled-with-Tractable-Density-Discrimination-for-Implicit-Hate-Speech-Detection" class="headerlink" title="Focal Inferential Infusion Coupled with Tractable Density Discrimination for Implicit Hate Speech Detection"></a>Focal Inferential Infusion Coupled with Tractable Density Discrimination for Implicit Hate Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11896">http://arxiv.org/abs/2309.11896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lcs2-iiitd/fiadd">https://github.com/lcs2-iiitd/fiadd</a></li>
<li>paper_authors: Sarah Masud, Ashutosh Bajpai, Tanmoy Chakraborty</li>
<li>for: 本研究旨在提高预训练大语言模型（PLM）对含有潜在仇恨语言表达的文本识别的能力，特别是对于含有潜在仇恨语言表达的含义不明确的文本。</li>
<li>methods: 本研究提出了一种新的Focus Inferential Adaptive Density Discrimination（FiADD）框架，将PLMfinetuning管道中的表达更加接近含义的形式，同时提高不同类别之间的间距。</li>
<li>results: 对于三个隐式仇恨语料集，FiADD可以在二元和三元 hate classification任务中获得显著改进，并在掌握含义不明确的三个其他任务中也获得了类似的改进。<details>
<summary>Abstract</summary>
Although pre-trained large language models (PLMs) have achieved state-of-the-art on many NLP tasks, they lack understanding of subtle expressions of implicit hate speech. Such nuanced and implicit hate is often misclassified as non-hate. Various attempts have been made to enhance the detection of (implicit) hate content by augmenting external context or enforcing label separation via distance-based metrics. We combine these two approaches and introduce FiADD, a novel Focused Inferential Adaptive Density Discrimination framework. FiADD enhances the PLM finetuning pipeline by bringing the surface form of an implicit hate speech closer to its implied form while increasing the inter-cluster distance among various class labels. We test FiADD on three implicit hate datasets and observe significant improvement in the two-way and three-way hate classification tasks. We further experiment on the generalizability of FiADD on three other tasks, namely detecting sarcasm, irony, and stance, in which surface and implied forms differ, and observe similar performance improvement. We analyze the generated latent space to understand its evolution under FiADD, which corroborates the advantage of employing FiADD for implicit hate speech detection.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:尽管预训练的大型自然语言模型（PLM）已经达到了许多NLP任务的状态流行，但它们缺乏对媚扰性表达的理解。这种细微和潜在的媚扰通常会被误分类为非媚扰。各种尝试都在扩展外部上下文或者通过距离基于的度量来增强恶意内容的检测。我们将这两种方法结合并引入FiADD，一种新的集中推理适应性权威度分区框架。FiADD通过将表面形式的潜在媚扰语言更近于其暗示形式，同时提高不同类别间的间距，以提高PLM的训练pipeline。我们在三个潜在媚扰数据集上测试FiADD，并观察到了两个和三个 hate类别分类任务中的显著改进。我们进一步在三个其他任务上进行了测试，即检测嘲笑、讽刺和立场，这些任务中表面和暗示形式不同，并观察到了类似的改进。我们分析生成的潜在空间，以理解FiADD在媚扰语言检测中的优势。
</details></li>
</ul>
<hr>
<h2 id="Is-It-Really-Useful-to-Jointly-Parse-Constituency-and-Dependency-Trees-A-Revisit"><a href="#Is-It-Really-Useful-to-Jointly-Parse-Constituency-and-Dependency-Trees-A-Revisit" class="headerlink" title="Is It Really Useful to Jointly Parse Constituency and Dependency Trees? A Revisit"></a>Is It Really Useful to Jointly Parse Constituency and Dependency Trees? A Revisit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11888">http://arxiv.org/abs/2309.11888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanggang Gu, Yang Hou, Zhefeng Wang, Xinyu Duan, Zhenghua Li</li>
<li>for: 这paper是为了同时解析成分树和依赖树而写的，以便更好地表示句子的语法结构。</li>
<li>methods: 这paper使用了一种更高效的解码算法，以及在训练阶段进行共同模型化，以及提出了高阶分数组件来捕捉成分-依赖关系。</li>
<li>results:  compared to previous works, this paper makes progress in four aspects: (1) adopting a much more efficient decoding algorithm, (2) exploring joint modeling at the training phase, (3) proposing high-order scoring components for constituent-dependency interaction, and (4) gaining more insights via in-depth experiments and analysis.<details>
<summary>Abstract</summary>
This work visits the topic of jointly parsing constituency and dependency trees, i.e., to produce compatible constituency and dependency trees simultaneously for input sentences, which is attractive considering that the two types of trees are complementary in representing syntax. Compared with previous works, we make progress in four aspects: (1) adopting a much more efficient decoding algorithm, (2) exploring joint modeling at the training phase, instead of only at the inference phase, (3) proposing high-order scoring components for constituent-dependency interaction, (4) gaining more insights via in-depth experiments and analysis.
</details>
<details>
<summary>摘要</summary>
这个工作探讨了同时解析成分树和依赖树的问题，即为输入句子生成兼容的成分树和依赖树，这是很吸引人的，因为这两种树是 syntax 表示的补充。与前一些工作相比，我们在四个方面做出了进步：1. 采用了非常高效的解码算法，2. 在训练阶段进行同时模型化，而不是只在推断阶段，3. 提出了高阶分数组件来描述成分-依赖关系，4. 通过深入实验和分析获得了更多的发现。
</details></li>
</ul>
<hr>
<h2 id="Syntactic-Variation-Across-the-Grammar-Modelling-a-Complex-Adaptive-System"><a href="#Syntactic-Variation-Across-the-Grammar-Modelling-a-Complex-Adaptive-System" class="headerlink" title="Syntactic Variation Across the Grammar: Modelling a Complex Adaptive System"></a>Syntactic Variation Across the Grammar: Modelling a Complex Adaptive System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11869">http://arxiv.org/abs/2309.11869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Dunn</li>
<li>for: 这个论文的目的是研究语言是一个复杂的自适应系统，并评估现有的语言研究方法是否能够准确地捕捉语言的复杂性。</li>
<li>methods: 这篇论文使用了系统地模型了英语口语的地方方言变化，并使用了整个语法和各个语法节点的隔离来描述这些变化。</li>
<li>results: 研究结果表明，语法中的各个节点之间存在许多交互，这些交互对于语言变化的理解具有重要意义。此外，研究还发现，在不同的语法节点被考察时，不同的地方方言之间的相似性会有所不同。<details>
<summary>Abstract</summary>
While language is a complex adaptive system, most work on syntactic variation observes a few individual constructions in isolation from the rest of the grammar. This means that the grammar, a network which connects thousands of structures at different levels of abstraction, is reduced to a few disconnected variables. This paper quantifies the impact of such reductions by systematically modelling dialectal variation across 49 local populations of English speakers in 16 countries. We perform dialect classification with both an entire grammar as well as with isolated nodes within the grammar in order to characterize the syntactic differences between these dialects. The results show, first, that many individual nodes within the grammar are subject to variation but, in isolation, none perform as well as the grammar as a whole. This indicates that an important part of syntactic variation consists of interactions between different parts of the grammar. Second, the results show that the similarity between dialects depends heavily on the sub-set of the grammar being observed: for example, New Zealand English could be more similar to Australian English in phrasal verbs but at the same time more similar to UK English in dative phrases.
</details>
<details>
<summary>摘要</summary>
语言是一个复杂的适应系统，大多数语法变化研究通常只关注几个个体构造，即使这些构造在语法 grammar 中处于不同层次的抽象水平之间。这意味着语法，一个连接千个结构的网络，被减少为几个分离的变量。这篇论文使用系统地模型了英语Speakers的地方方言变化，以Characterize这些方言之间的语法差异。我们使用整个语法和语法中各个节点进行地域分类，以Quantify这些变化的影响。结果显示，首先，语法中的多个节点都受到了变化，但是孤立地没有任何节点能够与整个语法一起表现得更好。这表明，语法变化中的一部分是不同部分之间的交互。其次，结果显示，不同地区的方言之间的相似性取决于观察到的语法子集：例如，新西兰英语可能与澳大利亚英语在短语动词方面更相似，而与英国英语在指示语raspects更相似。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Sanitization-of-Large-Language-Models"><a href="#Knowledge-Sanitization-of-Large-Language-Models" class="headerlink" title="Knowledge Sanitization of Large Language Models"></a>Knowledge Sanitization of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11852">http://arxiv.org/abs/2309.11852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoichi Ishibashi, Hidetoshi Shimodaira</li>
<li>for: 防止语言模型泄露敏感信息</li>
<li>methods: 细化模型，让其生成无害回答</li>
<li>results: successfully mitigated knowledge leakage and preserved overall performance of LLM<details>
<summary>Abstract</summary>
We explore a knowledge sanitization approach to mitigate the privacy concerns associated with large language models (LLMs). LLMs trained on a large corpus of Web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns. Our technique fine-tunes these models, prompting them to generate harmless responses such as ``I don't know'' when queried about specific information. Experimental results in a closed-book question-answering task show that our straightforward method not only minimizes particular knowledge leakage but also preserves the overall performance of LLM. These two advantages strengthen the defense against extraction attacks and reduces the emission of harmful content such as hallucinations.
</details>
<details>
<summary>摘要</summary>
我们研究了一种知识净化方法，以减轻大语言模型（LLM）中存在的隐私问题。这些模型通过大量网络数据训练，可能会记忆和泄露敏感或 конфиденциаль信息，这会引起严重的安全问题。我们的技术是通过精细地调整这些模型，使其在特定信息 queries 时返回无害的回答，如“我不知道”。我们的实验结果表明，我们的简单方法不仅可以减少特定知识泄露，还可以保持 LL 的总性能。这两点优点共同强化了对抽取攻击的防御，并减少了负面内容的泄露，如幻想。
</details></li>
</ul>
<hr>
<h2 id="A-Discourse-level-Multi-scale-Prosodic-Model-for-Fine-grained-Emotion-Analysis"><a href="#A-Discourse-level-Multi-scale-Prosodic-Model-for-Fine-grained-Emotion-Analysis" class="headerlink" title="A Discourse-level Multi-scale Prosodic Model for Fine-grained Emotion Analysis"></a>A Discourse-level Multi-scale Prosodic Model for Fine-grained Emotion Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11849">http://arxiv.org/abs/2309.11849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianhao Wei, Jia Jia, Xiang Li, Zhiyong Wu, Ziyi Wang</li>
<li>for: 这个研究旨在预测基于文本层次的细致情感特征，以提高语音合成模型的表达性。</li>
<li>methods: 我们使用一种style transfer模型提取phoneme-level Local Prosody Embedding序列和全局风格嵌入，并提出一种多层次文本Prosodic模型（D-MPM）来利用这些特征。</li>
<li>results: 我们的模型可以更好地预测情感特征，并且在用户评价指标上表现更好 than style transfer模型。<details>
<summary>Abstract</summary>
This paper explores predicting suitable prosodic features for fine-grained emotion analysis from the discourse-level text. To obtain fine-grained emotional prosodic features as predictive values for our model, we extract a phoneme-level Local Prosody Embedding sequence (LPEs) and a Global Style Embedding as prosodic speech features from the speech with the help of a style transfer model. We propose a Discourse-level Multi-scale text Prosodic Model (D-MPM) that exploits multi-scale text to predict these two prosodic features. The proposed model can be used to analyze better emotional prosodic features and thus guide the speech synthesis model to synthesize more expressive speech. To quantitatively evaluate the proposed model, we contribute a new and large-scale Discourse-level Chinese Audiobook (DCA) dataset with more than 13,000 utterances annotated sequences to evaluate the proposed model. Experimental results on the DCA dataset show that the multi-scale text information effectively helps to predict prosodic features, and the discourse-level text improves both the overall coherence and the user experience. More interestingly, although we aim at the synthesis effect of the style transfer model, the synthesized speech by the proposed text prosodic analysis model is even better than the style transfer from the original speech in some user evaluation indicators.
</details>
<details>
<summary>摘要</summary>
To evaluate the proposed model, the authors contribute a new large-scale Discourse-level Chinese Audiobook (DCA) dataset with over 13,000 annotated utterances. Experimental results show that the multi-scale text information effectively predicts prosodic features, and the discourse-level text improves coherence and user experience. Surprisingly, the synthesized speech by the proposed text prosodic analysis model is even better than the style transfer from the original speech in some user evaluation indicators.
</details></li>
</ul>
<hr>
<h2 id="A-Chinese-Prompt-Attack-Dataset-for-LLMs-with-Evil-Content"><a href="#A-Chinese-Prompt-Attack-Dataset-for-LLMs-with-Evil-Content" class="headerlink" title="A Chinese Prompt Attack Dataset for LLMs with Evil Content"></a>A Chinese Prompt Attack Dataset for LLMs with Evil Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11830">http://arxiv.org/abs/2309.11830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengyuan Liu, Fubang Zhao, Lizhi Qing, Yangyang Kang, Changlong Sun, Kun Kuang, Fei Wu</li>
<li>for: 这篇论文主要针对大语言模型（LLMs）的危险攻击和防御问题。</li>
<li>methods: 论文使用了多种黑盒攻击方法，如提示攻击，以测试LLMs的安全性。</li>
<li>results: 试验结果显示，论文引入的中文提示集（CPAD）对 LLMs 有70%的攻击成功率，表明这些提示有效地攻击了 LLMs。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) present significant priority in text understanding and generation. However, LLMs suffer from the risk of generating harmful contents especially while being employed to applications. There are several black-box attack methods, such as Prompt Attack, which can change the behaviour of LLMs and induce LLMs to generate unexpected answers with harmful contents. Researchers are interested in Prompt Attack and Defense with LLMs, while there is no publicly available dataset to evaluate the abilities of defending prompt attack. In this paper, we introduce a Chinese Prompt Attack Dataset for LLMs, called CPAD. Our prompts aim to induce LLMs to generate unexpected outputs with several carefully designed prompt attack approaches and widely concerned attacking contents. Different from previous datasets involving safety estimation, We construct the prompts considering three dimensions: contents, attacking methods and goals, thus the responses can be easily evaluated and analysed. We run several well-known Chinese LLMs on our dataset, and the results show that our prompts are significantly harmful to LLMs, with around 70% attack success rate. We will release CPAD to encourage further studies on prompt attack and defense.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在文本理解和生成方面具有重要优势。然而，LLM受到生成危险内容的风险，特别是在应用程序中使用时。现有许多黑盒子攻击方法，如提示攻击，可以改变LLM的行为，让LLM生成意外的答案，包含危险内容。研究人员对提示攻击和LLM防御具有浓厚的兴趣，但是没有公共可用的数据集来评估防御能力。在这篇论文中，我们介绍了一个中文提示攻击数据集（CPAD），用于测试LLM的防御能力。我们的提示包括了三个维度：内容、攻击方法和目标，因此可以轻松地评估和分析回快。我们运行了一些知名的中文LLM在我们的数据集上，结果显示，我们的提示对LLM具有70%攻击成功率。我们将CPAD发布，以便更多的研究人员可以进行提示攻击和防御的研究。
</details></li>
</ul>
<hr>
<h2 id="Word-Embedding-with-Neural-Probabilistic-Prior"><a href="#Word-Embedding-with-Neural-Probabilistic-Prior" class="headerlink" title="Word Embedding with Neural Probabilistic Prior"></a>Word Embedding with Neural Probabilistic Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11824">http://arxiv.org/abs/2309.11824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaogang Ren, Dingcheng Li, Ping Li</li>
<li>for: 提高单词表示学习的词 embedding 模型</li>
<li>methods: 提出了一种可以与单词 embedding 模型集成的 probabilistic prior，使得词 embedding 可以作为概率生成模型来进行规范化。</li>
<li>results: 对多种任务进行了广泛的实验，并显示了提高单词表示的效果。<details>
<summary>Abstract</summary>
To improve word representation learning, we propose a probabilistic prior which can be seamlessly integrated with word embedding models. Different from previous methods, word embedding is taken as a probabilistic generative model, and it enables us to impose a prior regularizing word representation learning. The proposed prior not only enhances the representation of embedding vectors but also improves the model's robustness and stability. The structure of the proposed prior is simple and effective, and it can be easily implemented and flexibly plugged in most existing word embedding models. Extensive experiments show the proposed method improves word representation on various tasks.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "word representation" is translated as "字符表示" (zhì chào biǎo yì)* "probabilistic prior" is translated as "概率先验" (guè shí jiān yì)* "word embedding" is translated as "字符嵌入" (zhì chào fù rù)* "extensive experiments" is translated as "广泛实验" (guǎn fāng shí yan)
</details></li>
</ul>
<hr>
<h2 id="SLHCat-Mapping-Wikipedia-Categories-and-Lists-to-DBpedia-by-Leveraging-Semantic-Lexical-and-Hierarchical-Features"><a href="#SLHCat-Mapping-Wikipedia-Categories-and-Lists-to-DBpedia-by-Leveraging-Semantic-Lexical-and-Hierarchical-Features" class="headerlink" title="SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging Semantic, Lexical, and Hierarchical Features"></a>SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging Semantic, Lexical, and Hierarchical Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11791">http://arxiv.org/abs/2309.11791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoyi Wang, Zhenyang Zhang, Jiaxin Qin, Mizuho Iwaihara</li>
<li>for: 解决 CaLiGraph 生成的不完整和粗糙的映射问题，实现一个大规模的知识图库。</li>
<li>methods: 使用ontologyAlignment的方法，利用知识图的结构信息和ontology类名的语言和SemanticSimilarities来发现可信的映射。</li>
<li>results: 比基eline模型准确率高25%，提供一种实际的大规模ontology映射解决方案。<details>
<summary>Abstract</summary>
Wikipedia articles are hierarchically organized through categories and lists, providing one of the most comprehensive and universal taxonomy, but its open creation is causing redundancies and inconsistencies. Assigning DBPedia classes to Wikipedia categories and lists can alleviate the problem, realizing a large knowledge graph which is essential for categorizing digital contents through entity linking and typing. However, the existing approach of CaLiGraph is producing incomplete and non-fine grained mappings. In this paper, we tackle the problem as ontology alignment, where structural information of knowledge graphs and lexical and semantic features of ontology class names are utilized to discover confident mappings, which are in turn utilized for finetuing pretrained language models in a distant supervision fashion. Our method SLHCat consists of two main parts: 1) Automatically generating training data by leveraging knowledge graph structure, semantic similarities, and named entity typing. 2) Finetuning and prompt-tuning of the pre-trained language model BERT are carried out over the training data, to capture semantic and syntactic properties of class names. Our model SLHCat is evaluated over a benchmark dataset constructed by annotating 3000 fine-grained CaLiGraph-DBpedia mapping pairs. SLHCat is outperforming the baseline model by a large margin of 25% in accuracy, offering a practical solution for large-scale ontology mapping.
</details>
<details>
<summary>摘要</summary>
Wikipedia文章通过分类和列表归类，提供了一个最完整和通用的分类体系，但是开放创建导致重复和不一致。将DBPedia类划分到Wikipedia分类和列表中可以解决这个问题，实现大量知识图的构建，这是对数字内容进行分类和类型的关键。然而，现有的CaLiGraph方法产生了不完整和粗糙的映射。在这篇论文中，我们视为ontology对齐，利用知识图结构、语义和命名实体类型的信息，以确定可靠的映射，然后利用远程监督的方式来训练预训练的语言模型。我们的方法SLHCat包括两个主要部分：1. 利用知识图结构、语义相似度和命名实体类型自动生成训练数据。2. 使用训练数据进行远程监督方式来训练和提前训练预训练的语言模型BERT，以捕捉类名的语义和 sintaxis性质。我们的模型SLHCat在一个建立的基准数据集上进行评估，与基eline模型相比，SLHCat的准确率高出25%，提供了一个实用的大规模 Ontology mapping 解决方案。
</details></li>
</ul>
<hr>
<h2 id="ContextRef-Evaluating-Referenceless-Metrics-For-Image-Description-Generation"><a href="#ContextRef-Evaluating-Referenceless-Metrics-For-Image-Description-Generation" class="headerlink" title="ContextRef: Evaluating Referenceless Metrics For Image Description Generation"></a>ContextRef: Evaluating Referenceless Metrics For Image Description Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11710">http://arxiv.org/abs/2309.11710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elisakreiss/contextref">https://github.com/elisakreiss/contextref</a></li>
<li>paper_authors: Elisa Kreiss, Eric Zelikman, Christopher Potts, Nick Haber</li>
<li>for: 本研究用于评估无参考度量测试方法，以便更快地进行进步。</li>
<li>methods: 本研究使用预训练的视觉语言模型来评估图文描述，并提出了 ContextRef  benchmark，用于评估这些方法的对人类喜好性的Alignment。</li>
<li>results: 研究发现，无论使用哪种预训练模型或者 scoring functions，都无法通过 ContextRef 测试。但是，通过精心微调，可以获得显著改进。 ContextRef  remain 一个挑战性的 bencmark，主要归因于图文描述的上下文依赖。<details>
<summary>Abstract</summary>
Referenceless metrics (e.g., CLIPScore) use pretrained vision--language models to assess image descriptions directly without costly ground-truth reference texts. Such methods can facilitate rapid progress, but only if they truly align with human preference judgments. In this paper, we introduce ContextRef, a benchmark for assessing referenceless metrics for such alignment. ContextRef has two components: human ratings along a variety of established quality dimensions, and ten diverse robustness checks designed to uncover fundamental weaknesses. A crucial aspect of ContextRef is that images and descriptions are presented in context, reflecting prior work showing that context is important for description quality. Using ContextRef, we assess a variety of pretrained models, scoring functions, and techniques for incorporating context. None of the methods is successful with ContextRef, but we show that careful fine-tuning yields substantial improvements. ContextRef remains a challenging benchmark though, in large part due to the challenge of context dependence.
</details>
<details>
<summary>摘要</summary>
无参准度量器（例如CLIPScore）使用预训练视语模型直接评估图文描述，而不需要昂贵的参照文本。这些方法可以促进快速进步，但只有如果它们与人类偏好判断相一致。在这篇论文中，我们介绍ContextRef，一个用于评估无参准度量器的benchmark。ContextRef有两个组成部分：人类评分多种已知质量维度，以及十种多样化的Robustness Check，旨在揭示基本弱点。ContextRef中图文的展示是在Context中进行的，这与先前的研究表明，Context对描述质量具有重要作用。使用ContextRef，我们评估了多种预训练模型、分数函数和Context的integiration方法。结果显示，None of the methods是ContextRef中成功的，但我们展示了精细微调可以实现显著改进。ContextRef仍然是一个挑战性的benchmark，主要是因为Context的依赖性。
</details></li>
</ul>
<hr>
<h2 id="Memory-Augmented-LLM-Personalization-with-Short-and-Long-Term-Memory-Coordination"><a href="#Memory-Augmented-LLM-Personalization-with-Short-and-Long-Term-Memory-Coordination" class="headerlink" title="Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination"></a>Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11696">http://arxiv.org/abs/2309.11696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Zhang, Fubang Zhao, Yangyang Kang, Xiaozhong Liu</li>
<li>for: 提高用户特定的自然语言生成结果（User-oriented natural language generation）</li>
<li>methods: 提出了一种新的计算机力学记忆机制，并采用参数效率的精度调整方案进行个性化（Parameter-efficient fine-tuning schema）</li>
<li>results: 实验结果表明提案的方法有效并且超越传统方法（Extensive experimental results demonstrate the effectiveness and superiority of the proposed approach）<details>
<summary>Abstract</summary>
Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable proficiency in comprehending and generating natural language. However, their unpersonalized generation paradigm may result in suboptimal user-specific outcomes. Typically, users converse differently based on their knowledge and preferences. This necessitates the task of enhancing user-oriented LLM which remains unexplored. While one can fully train an LLM for this objective, the resource consumption is unaffordable. Prior research has explored memory-based methods to store and retrieve knowledge to enhance generation without retraining for new queries. However, we contend that a mere memory module is inadequate to comprehend a user's preference, and fully training an LLM can be excessively costly. In this study, we propose a novel computational bionic memory mechanism, equipped with a parameter-efficient fine-tuning schema, to personalize LLMs. Our extensive experimental results demonstrate the effectiveness and superiority of the proposed approach. To encourage further research into this area, we are releasing a new conversation dataset generated entirely by LLM based on an open-source medical corpus, as well as our implementation code.
</details>
<details>
<summary>摘要</summary>
In this study, we propose a novel computational bionic memory mechanism, equipped with a parameter-efficient fine-tuning schema, to personalize LLMs. Our extensive experimental results demonstrate the effectiveness and superiority of the proposed approach. To encourage further research in this area, we are releasing a new conversation dataset generated entirely by LLM based on an open-source medical corpus, as well as our implementation code.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/21/cs.CL_2023_09_21/" data-id="clnsn0veo008xgf889p1z28nk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/21/cs.LG_2023_09_21/" class="article-date">
  <time datetime="2023-09-21T10:00:00.000Z" itemprop="datePublished">2023-09-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/21/cs.LG_2023_09_21/">cs.LG - 2023-09-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Performance-Conditioning-for-Diffusion-Based-Multi-Instrument-Music-Synthesis"><a href="#Performance-Conditioning-for-Diffusion-Based-Multi-Instrument-Music-Synthesis" class="headerlink" title="Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis"></a>Performance Conditioning for Diffusion-Based Multi-Instrument Music Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12283">http://arxiv.org/abs/2309.12283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Maman, Johannes Zeitler, Meinard Müller, Amit H. Bermano</li>
<li>for: 本研究的目的是提高多楽器合成的控制精度，使其更能够符合音乐的特性和演奏环境。</li>
<li>methods: 本研究基于当前顶尖的扩散模型，引入表演环境调整，以提高音乐生成的时期和乐器风格控制。</li>
<li>results: 这项研究的评估结果显示，使用表演环境调整可以实现更高的乐器特性和演奏风格控制，并且在不同乐器和演奏环境下实现了类比于人类聆听者的音乐生成。<details>
<summary>Abstract</summary>
Generating multi-instrument music from symbolic music representations is an important task in Music Information Retrieval (MIR). A central but still largely unsolved problem in this context is musically and acoustically informed control in the generation process. As the main contribution of this work, we propose enhancing control of multi-instrument synthesis by conditioning a generative model on a specific performance and recording environment, thus allowing for better guidance of timbre and style. Building on state-of-the-art diffusion-based music generative models, we introduce performance conditioning - a simple tool indicating the generative model to synthesize music with style and timbre of specific instruments taken from specific performances. Our prototype is evaluated using uncurated performances with diverse instrumentation and achieves state-of-the-art FAD realism scores while allowing novel timbre and style control. Our project page, including samples and demonstrations, is available at benadar293.github.io/midipm
</details>
<details>
<summary>摘要</summary>
<<SYS>>将 symbolic music representation 转化为多种乐器音乐是音乐信息检索（MIR）领域中的重要任务。这个问题的中心问题是在生成过程中提供音乐和听众意见。作为本工作的主要贡献，我们提议通过指定特定的表演和录音环境来增强多乐器合成的控制。基于当前的扩散型音乐生成模型，我们引入表演条件 - 一种简单的工具，用于指定生成模型Synthesize music with specific instruments and timbre from specific performances. Our prototype is evaluated using uncurated performances with diverse instrumentation and achieves state-of-the-art FAD realism scores while allowing novel timbre and style control. More information, including samples and demonstrations, can be found at benadar293.github.io/midipm.Note: "FAD" stands for "Flexible Audio Database", which is a standard evaluation metric for music generation systems. A high FAD realism score indicates that the generated music sounds realistic and similar to the original recording.
</details></li>
</ul>
<hr>
<h2 id="The-Broad-Impact-of-Feature-Imitation-Neural-Enhancements-Across-Financial-Speech-and-Physiological-Domains"><a href="#The-Broad-Impact-of-Feature-Imitation-Neural-Enhancements-Across-Financial-Speech-and-Physiological-Domains" class="headerlink" title="The Broad Impact of Feature Imitation: Neural Enhancements Across Financial, Speech, and Physiological Domains"></a>The Broad Impact of Feature Imitation: Neural Enhancements Across Financial, Speech, and Physiological Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12279">http://arxiv.org/abs/2309.12279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Khanmohammadi, Tuka Alhanai, Mohammad M. Ghassemi</li>
<li>for: 这篇论文旨在测试对不同时间序列数据进行初始化神经网络的性能影响。</li>
<li>methods: 本研究使用Feature Imitating Networks（FIN）技术，将神经网络的初始化参数设置为模仿特定的关闭形式统计特征，以提高深度学习架构的性能。</li>
<li>results: 在Bitcoin价格预测任务中，将FIN给神经网络模型中的表现下降了约1000，对比基准模型。在语音情感识别任务中，将FIN与神经网络模型相结合后，提高了分类精度约3%。在chronic neck pain检测任务中，将FIN给神经网络模型中的表现提高了约7%，对比现有的分类器。这些发现证明了FIN在多种应用中的广泛应用和优化性。<details>
<summary>Abstract</summary>
Initialization of neural network weights plays a pivotal role in determining their performance. Feature Imitating Networks (FINs) offer a novel strategy by initializing weights to approximate specific closed-form statistical features, setting a promising foundation for deep learning architectures. While the applicability of FINs has been chiefly tested in biomedical domains, this study extends its exploration into other time series datasets. Three different experiments are conducted in this study to test the applicability of imitating Tsallis entropy for performance enhancement: Bitcoin price prediction, speech emotion recognition, and chronic neck pain detection. For the Bitcoin price prediction, models embedded with FINs reduced the root mean square error by around 1000 compared to the baseline. In the speech emotion recognition task, the FIN-augmented model increased classification accuracy by over 3 percent. Lastly, in the CNP detection experiment, an improvement of about 7 percent was observed compared to established classifiers. These findings validate the broad utility and potency of FINs in diverse applications.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:初始化神经网络权重的初始化方法对其性能产生决定性的影响。特征模仿网络（FIN）提供了一种新的策略，通过初始化权重来 aproximate 特定的关闭形式统计特征，为深度学习架构提供了一个良好的基础。虽然FIN的可用性主要在生物医学领域进行了证明，但这项研究尝试将其应用到其他时间序列数据集上。这个研究进行了三个不同的实验来测试FIN的可行性和表现：比特币价格预测、语音情感识别和慢性颈部疼痛检测。在比特币价格预测任务中，包含FIN的模型降低了根圆误差约1000比例。在语音情感识别任务中，FIN-加强模型提高了分类精度高达3%。最后，在慢性颈部疼痛检测任务中，FIN-加强模型与现有分类器相比，提高了约7%的性能。这些发现证明了FIN在多样化应用中的广泛适用性和强大性。
</details></li>
</ul>
<hr>
<h2 id="Soft-Merging-A-Flexible-and-Robust-Soft-Model-Merging-Approach-for-Enhanced-Neural-Network-Performance"><a href="#Soft-Merging-A-Flexible-and-Robust-Soft-Model-Merging-Approach-for-Enhanced-Neural-Network-Performance" class="headerlink" title="Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance"></a>Soft Merging: A Flexible and Robust Soft Model Merging Approach for Enhanced Neural Network Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12259">http://arxiv.org/abs/2309.12259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Yusen Wu, Phuong Nguyen, Chao Liu, Yelena Yesha</li>
<li>for: 提高深度学习模型的性能和稳定性</li>
<li>methods: 使用soft merging方法，通过学习门户参数来融合多个本地最优模型，并使用硬核分布来避免恶性模型的影响</li>
<li>results: 实验表明，使用融合模型可以提高深度学习模型的性能和稳定性，并且可以降低计算成本<details>
<summary>Abstract</summary>
Stochastic Gradient Descent (SGD), a widely used optimization algorithm in deep learning, is often limited to converging to local optima due to the non-convex nature of the problem. Leveraging these local optima to improve model performance remains a challenging task. Given the inherent complexity of neural networks, the simple arithmetic averaging of the obtained local optima models in undesirable results. This paper proposes a {\em soft merging} method that facilitates rapid merging of multiple models, simplifies the merging of specific parts of neural networks, and enhances robustness against malicious models with extreme values. This is achieved by learning gate parameters through a surrogate of the $l_0$ norm using hard concrete distribution without modifying the model weights of the given local optima models. This merging process not only enhances the model performance by converging to a better local optimum, but also minimizes computational costs, offering an efficient and explicit learning process integrated with stochastic gradient descent. Thorough experiments underscore the effectiveness and superior performance of the merged neural networks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Parallelizing-non-linear-sequential-models-over-the-sequence-length"><a href="#Parallelizing-non-linear-sequential-models-over-the-sequence-length" class="headerlink" title="Parallelizing non-linear sequential models over the sequence length"></a>Parallelizing non-linear sequential models over the sequence length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12252">http://arxiv.org/abs/2309.12252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Heng Lim, Qi Zhu, Joshua Selfridge, Muhammad Firmansyah Kasim</li>
<li>for:  acceleration of GPU evaluation of sequential models</li>
<li>methods:  parallel algorithm without special structure in the architecture</li>
<li>results:  training 10 times faster with no compromise on accuracy<details>
<summary>Abstract</summary>
Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long suffered from slow training due to their inherent sequential nature. For many years this bottleneck has persisted, as many thought sequential models could not be parallelized. We challenge this long-held belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. The algorithm does not need any special structure in the sequential models' architecture, making it applicable to a wide range of architectures. Using our method, training sequential models can be more than 10 times faster than the common sequential method without any meaningful difference in the training results. Leveraging this accelerated training, we discovered the efficacy of the Gated Recurrent Unit in a long time series classification problem with 17k time samples. By overcoming the training bottleneck, our work serves as the first step to unlock the potential of non-linear sequential models for long sequence problems.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文版本，对于具体的翻译请参考下面的详细翻译）Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long been limited by their sequential nature, leading to slow training times. Many believed that these models could not be parallelized, but we challenge this belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. Our method is applicable to a wide range of architectures and can train sequential models up to 10 times faster than traditional methods without any significant difference in training results. By overcoming the training bottleneck, our work paves the way for the potential of non-linear sequential models in long sequence problems.
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-Automated-Audio-Captioning-via-text-only-training"><a href="#Weakly-supervised-Automated-Audio-Captioning-via-text-only-training" class="headerlink" title="Weakly-supervised Automated Audio Captioning via text only training"></a>Weakly-supervised Automated Audio Captioning via text only training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12242">http://arxiv.org/abs/2309.12242</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zelaki/wsac">https://github.com/zelaki/wsac</a></li>
<li>paper_authors: Theodoros Kouzelis, Vassilis Katsouros</li>
<li>for: automatically generating descriptions for audio clips (AAC)</li>
<li>methods: weakly-supervised approach using text data and pre-trained CLAP model, with strategies to bridge the modality gap</li>
<li>results: relative performance of up to ~$83%$ compared to fully supervised approaches trained with paired target data on Clotho and AudioCaps datasets<details>
<summary>Abstract</summary>
In recent years, datasets of paired audio and captions have enabled remarkable success in automatically generating descriptions for audio clips, namely Automated Audio Captioning (AAC). However, it is labor-intensive and time-consuming to collect a sufficient number of paired audio and captions. Motivated by the recent advances in Contrastive Language-Audio Pretraining (CLAP), we propose a weakly-supervised approach to train an AAC model assuming only text data and a pre-trained CLAP model, alleviating the need for paired target data. Our approach leverages the similarity between audio and text embeddings in CLAP. During training, we learn to reconstruct the text from the CLAP text embedding, and during inference, we decode using the audio embeddings. To mitigate the modality gap between the audio and text embeddings we employ strategies to bridge the gap during training and inference stages. We evaluate our proposed method on Clotho and AudioCaps datasets demonstrating its ability to achieve a relative performance of up to ~$83\%$ compared to fully supervised approaches trained with paired target data.
</details>
<details>
<summary>摘要</summary>
近年来，带有音频和caption的数据集已经实现了自动生成音频描述的remarkable成功，即自动语音描述（AAC）。然而，收集到充足数量的带有音频和caption的数据集是时间和劳动密集的。鼓 motivated by recent advances in Contrastive Language-Audio Pretraining（CLAP），我们提出了一种弱监督的方法，通过假设只有文本数据和预训练的CLAP模型，解决了需要对Target数据进行监督的问题。我们的方法利用CLAP模型中的文本和音频嵌入的相似性。在训练过程中，我们学习将文本重建为CLAP文本嵌入，并在推理阶段使用音频嵌入进行解码。为了在模式之间减少差距，我们在训练和推理阶段使用了bridging策略。我们在Clotho和AudioCaps数据集上评估了我们的提议方法，并证明其能够实现相对于完全监督方法训练于带有Target数据的性能的$83\%$。
</details></li>
</ul>
<hr>
<h2 id="t-EER-Parameter-Free-Tandem-Evaluation-of-Countermeasures-and-Biometric-Comparators"><a href="#t-EER-Parameter-Free-Tandem-Evaluation-of-Countermeasures-and-Biometric-Comparators" class="headerlink" title="t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators"></a>t-EER: Parameter-Free Tandem Evaluation of Countermeasures and Biometric Comparators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12237">http://arxiv.org/abs/2309.12237</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/takhemlata/t-eer">https://github.com/takhemlata/t-eer</a></li>
<li>paper_authors: Tomi Kinnunen, Kong Aik Lee, Hemlata Tak, Nicholas Evans, Andreas Nautsch</li>
<li>for: The paper is written to propose a new metric for the joint evaluation of presentation attack detection (PAD) solutions operating in tandem with biometric verification.</li>
<li>methods: The paper introduces a new metric called tandem equal error rate (t-EER) for evaluating the performance of PAD solutions in combination with biometric verification systems. The t-EER is a parameter-free metric that measures the equal error rate of both false alarms and misses at a set of operating points.</li>
<li>results: The paper demonstrates the application of the t-EER metric to a wide range of biometric system evaluations under attack, using both simulated and real scores for a voice biometrics application. The proposed approach is shown to be a strong candidate metric for the tandem evaluation of PAD systems and biometric comparators.Here is the simplified Chinese text for the three key information points:</li>
<li>for: 本文是为了提出一个新的 metric 用于 tandem 识别攻击检测 (PAD) 和生物特征验证系统的共同评估。</li>
<li>methods: 本文提出了一个名为 tandem 平等错误率 (t-EER) 的新metric，用于评估 PAD 解决方案和生物特征验证系统之间的共同性能。t-EER 是一个无参数的 metric，可以在多个操作点上测试 false alarm 和 miss 的平等错误率。</li>
<li>results: 本文使用了 simulated 和实际数据，对一个语音生物特征验证应用进行了广泛的评估。结果表明，tandem EER 是一个强andidate metric 用于 tandem 识别攻击检测和生物特征验证系统之间的评估。<details>
<summary>Abstract</summary>
Presentation attack (spoofing) detection (PAD) typically operates alongside biometric verification to improve reliablity in the face of spoofing attacks. Even though the two sub-systems operate in tandem to solve the single task of reliable biometric verification, they address different detection tasks and are hence typically evaluated separately. Evidence shows that this approach is suboptimal. We introduce a new metric for the joint evaluation of PAD solutions operating in situ with biometric verification. In contrast to the tandem detection cost function proposed recently, the new tandem equal error rate (t-EER) is parameter free. The combination of two classifiers nonetheless leads to a \emph{set} of operating points at which false alarm and miss rates are equal and also dependent upon the prevalence of attacks. We therefore introduce the \emph{concurrent} t-EER, a unique operating point which is invariable to the prevalence of attacks. Using both modality (and even application) agnostic simulated scores, as well as real scores for a voice biometrics application, we demonstrate application of the t-EER to a wide range of biometric system evaluations under attack. The proposed approach is a strong candidate metric for the tandem evaluation of PAD systems and biometric comparators.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Smooth-ECE-Principled-Reliability-Diagrams-via-Kernel-Smoothing"><a href="#Smooth-ECE-Principled-Reliability-Diagrams-via-Kernel-Smoothing" class="headerlink" title="Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing"></a>Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12236">http://arxiv.org/abs/2309.12236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-calibration">https://github.com/apple/ml-calibration</a></li>
<li>paper_authors: Jarosław Błasiok, Preetum Nakkiran</li>
<li>for: 这篇论文主要研究了如何使用抽象函数来测量和解释抽象预测器的准确性。</li>
<li>methods: 这篇论文使用了Radius Band Function(RBF)核函数来平滑观测值，然后计算Expected Calibration Error(ECE)。</li>
<li>results: 这篇论文提出了一种新的准确性测量方法，即SmoothECE，可以减轻抽象预测器的准确性问题。此外，这篇论文还提供了一个Python包，可以简单地测量和可读地展示抽象预测器的准确性。<details>
<summary>Abstract</summary>
Calibration measures and reliability diagrams are two fundamental tools for measuring and interpreting the calibration of probabilistic predictors. Calibration measures quantify the degree of miscalibration, and reliability diagrams visualize the structure of this miscalibration. However, the most common constructions of reliability diagrams and calibration measures -- binning and ECE -- both suffer from well-known flaws (e.g. discontinuity). We show that a simple modification fixes both constructions: first smooth the observations using an RBF kernel, then compute the Expected Calibration Error (ECE) of this smoothed function. We prove that with a careful choice of bandwidth, this method yields a calibration measure that is well-behaved in the sense of (B{\l}asiok, Gopalan, Hu, and Nakkiran 2023a) -- a consistent calibration measure. We call this measure the SmoothECE. Moreover, the reliability diagram obtained from this smoothed function visually encodes the SmoothECE, just as binned reliability diagrams encode the BinnedECE.   We also provide a Python package with simple, hyperparameter-free methods for measuring and plotting calibration: `pip install relplot\`.
</details>
<details>
<summary>摘要</summary>
“测量和可靠图是概率预测器的二个基本工具。测量措施量化预测器的误差，并可以视觉化这种误差的结构。然而，通常的构建方法，例如桶化和ECE，都受到了知名的缺陷（例如缺陷）。我们显示，使用RBF核函数平滑观测值后，计算预测ERROR的Expected Calibration Error（ECE），可以得到一个良好的测量方法。对于适当的标Width选择，这种方法具有一定的稳定性（Błasiok等2023a），我们称之为SmoothECE。此外，从这个平滑函数中得到的可靠图可以视觉化SmoothECE，与桶化的可靠图相似。我们还提供了一个Python套件，包含了简单、无参数的方法来量化和Plot calibration：`pip install relplot\`.”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Smooth-Nash-Equilibria-Algorithms-and-Complexity"><a href="#Smooth-Nash-Equilibria-Algorithms-and-Complexity" class="headerlink" title="Smooth Nash Equilibria: Algorithms and Complexity"></a>Smooth Nash Equilibria: Algorithms and Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12226">http://arxiv.org/abs/2309.12226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Constantinos Daskalakis, Noah Golowich, Nika Haghtalab, Abhishek Shetty</li>
<li>for: 这篇论文旨在解决 Nash 平衡的计算复杂性问题，提出了一种名为 $\sigma$-粗化 Nash 平衡的变种，并研究了其计算性质。</li>
<li>methods: 该论文使用了简化分析的想法，引入了一种名为 $\sigma$-粗化 Nash 平衡的概念，并提出了两种不同的 $\sigma$-粗化 Nash 平衡变种：强型 $\sigma$-粗化 Nash 平衡和弱型 $\sigma$-粗化 Nash 平衡。</li>
<li>results: 论文表明，在常量 $\sigma$ 和 $\epsilon$ 以及玩家数量都是常数时，可以在常量时间内随机找到一个 $\epsilon$-相近 $\sigma$-粗化 Nash 平衡，而且在同样的参数 régime 下，可以在多项时间内寻找一个强型 $\epsilon$-相近 $\sigma$-粗化 Nash 平衡。这些结果与 Nash 平衡的优化算法不可避免的复杂性不同。<details>
<summary>Abstract</summary>
A fundamental shortcoming of the concept of Nash equilibrium is its computational intractability: approximating Nash equilibria in normal-form games is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis, we introduce a relaxed variant of Nash equilibrium called $\sigma$-smooth Nash equilibrium, for a smoothness parameter $\sigma$. In a $\sigma$-smooth Nash equilibrium, players only need to achieve utility at least as high as their best deviation to a $\sigma$-smooth strategy, which is a distribution that does not put too much mass (as parametrized by $\sigma$) on any fixed action. We distinguish two variants of $\sigma$-smooth Nash equilibria: strong $\sigma$-smooth Nash equilibria, in which players are required to play $\sigma$-smooth strategies under equilibrium play, and weak $\sigma$-smooth Nash equilibria, where there is no such requirement.   We show that both weak and strong $\sigma$-smooth Nash equilibria have superior computational properties to Nash equilibria: when $\sigma$ as well as an approximation parameter $\epsilon$ and the number of players are all constants, there is a constant-time randomized algorithm to find a weak $\epsilon$-approximate $\sigma$-smooth Nash equilibrium in normal-form games. In the same parameter regime, there is a polynomial-time deterministic algorithm to find a strong $\epsilon$-approximate $\sigma$-smooth Nash equilibrium in a normal-form game. These results stand in contrast to the optimal algorithm for computing $\epsilon$-approximate Nash equilibria, which cannot run in faster than quasipolynomial-time. We complement our upper bounds by showing that when either $\sigma$ or $\epsilon$ is an inverse polynomial, finding a weak $\epsilon$-approximate $\sigma$-smooth Nash equilibria becomes computationally intractable.
</details>
<details>
<summary>摘要</summary>
《纳什平衡概念的基本缺陷》：纳什平衡的计算复杂性问题，在正常形游戏中，是PPAD困难的。在这篇论文中，我们根据精细分析的想法，引入一种Namedashed variant of Nash equilibrium，即$\sigma$-粗糙纳什平衡，其中$\sigma$是一个精度参数。在一个$\sigma$-粗糙纳什平衡中，玩家只需要实现 Utility 高于或等于它的最佳偏转策，这个策略是一个不太多的质量（如 parametrized by $\sigma$）的分布。我们将这种纳什平衡分为两种变种：强制 $\sigma$-粗糙纳什平衡，在平衡状态下，玩家需要采取 $\sigma$-粗糙策略，以及弱 $\sigma$-粗糙纳什平衡，没有这种要求。我们显示，在常数 $\sigma$ 和 Approximation parameter $\epsilon$ 以及玩家数量都是常数时，可以采取常数时间的随机算法来找到弱 $\epsilon$-近似 $\sigma$-粗糙纳什平衡，并且在同样的参数域内，可以采取多项时间的排序算法来找到强制 $\epsilon$-近似 $\sigma$-粗糙纳什平衡。这些结果与纳什平衡的优化算法不同，后者无法在超过半定时的情况下运行。我们补充了我们的上界，表明当 $\sigma$ 或 $\epsilon$ 是反射函数时，找到弱 $\epsilon$-近似 $\sigma$-粗糙纳什平衡就变得计算困难。
</details></li>
</ul>
<hr>
<h2 id="Regionally-Additive-Models-Explainable-by-design-models-minimizing-feature-interactions"><a href="#Regionally-Additive-Models-Explainable-by-design-models-minimizing-feature-interactions" class="headerlink" title="Regionally Additive Models: Explainable-by-design models minimizing feature interactions"></a>Regionally Additive Models: Explainable-by-design models minimizing feature interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12215">http://arxiv.org/abs/2309.12215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/givasile/RAM">https://github.com/givasile/RAM</a></li>
<li>paper_authors: Vasilis Gkolemis, Anargiros Tzerefos, Theodore Dalamagas, Eirini Ntoutsi, Christos Diou</li>
<li>for: 本研究旨在提出一种新的可解释模型，即区域添加模型（RAMs），用于解决基于多特征的机器学习问题中，加比模型（GAMs）的缺陷。</li>
<li>methods: 本研究提出了一种三步法则，首先使用黑盒模型进行训练，然后使用区域效果图来确定特征空间中的子区域，最后在每个子区域中采用加比模型来表示输出。</li>
<li>results: 实验结果表明，RAMs 比 GAMs 更高的表达能力，同时保持可解释性。<details>
<summary>Abstract</summary>
Generalized Additive Models (GAMs) are widely used explainable-by-design models in various applications. GAMs assume that the output can be represented as a sum of univariate functions, referred to as components. However, this assumption fails in ML problems where the output depends on multiple features simultaneously. In these cases, GAMs fail to capture the interaction terms of the underlying function, leading to subpar accuracy. To (partially) address this issue, we propose Regionally Additive Models (RAMs), a novel class of explainable-by-design models. RAMs identify subregions within the feature space where interactions are minimized. Within these regions, it is more accurate to express the output as a sum of univariate functions (components). Consequently, RAMs fit one component per subregion of each feature instead of one component per feature. This approach yields a more expressive model compared to GAMs while retaining interpretability. The RAM framework consists of three steps. Firstly, we train a black-box model. Secondly, using Regional Effect Plots, we identify subregions where the black-box model exhibits near-local additivity. Lastly, we fit a GAM component for each identified subregion. We validate the effectiveness of RAMs through experiments on both synthetic and real-world datasets. The results confirm that RAMs offer improved expressiveness compared to GAMs while maintaining interpretability.
</details>
<details>
<summary>摘要</summary>
通用加тив模型（GAMs）广泛应用于不同领域的解释性模型中。GAMs假设输出可以表示为一些单变量函数的总和，称为组件。然而，在机器学习问题中，输出受多个特征的同时影响，这个假设失败。在这些情况下，GAMs无法捕捉输出函数的交叉项，导致准确率下降。为解决这个问题，我们提出了区域加тив模型（RAMs），一种新的解释性模型类型。RAMs确定特征空间中的子区域，在这些子区域中，交叉项的影响最小。因此，RAMs采用一个组件来描述每个特征的子区域中的输出。相比GAMs，RAMs采用更加表达力的模型，同时保持可解释性。RAMs的框架包括三个步骤：首先，我们训练黑盒模型；其次，使用地方效果图来确定特征空间中的子区域，这些子区域中黑盒模型的地方效果较低；最后，我们采用GAM组件来描述每个确定的子区域。我们通过对synthetic和实际数据进行实验，证明RAMs可以提高表达力，同时保持可解释性。结果表明，RAMs比GAMs更好地适应机器学习问题。
</details></li>
</ul>
<hr>
<h2 id="SupeRBNN-Randomized-Binary-Neural-Network-Using-Adiabatic-Superconductor-Josephson-Devices"><a href="#SupeRBNN-Randomized-Binary-Neural-Network-Using-Adiabatic-Superconductor-Josephson-Devices" class="headerlink" title="SupeRBNN: Randomized Binary Neural Network Using Adiabatic Superconductor Josephson Devices"></a>SupeRBNN: Randomized Binary Neural Network Using Adiabatic Superconductor Josephson Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12212">http://arxiv.org/abs/2309.12212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengang Li, Geng Yuan, Tomoharu Yamauchi, Zabihi Masoud, Yanyue Xie, Peiyan Dong, Xulong Tang, Nobuyuki Yoshikawa, Devesh Tiwari, Yanzhi Wang, Olivia Chen</li>
<li>For: 这 paper 是为了提出一种基于 Adiabatic Quantum-Flux-Parametron (AQFP) 的 randomized Binary Neural Network (BNN) 加速器框架 SupeRBNN，以解决 AQFP 设备在 BNN 计算中的一些关键挑战。* Methods: 这 paper 使用了 AQFP 设备的特殊极性来 denote логические值，并提出了一种基于随机行为的 BNN 加速器框架 SupeRBNN，包括一种随机计算模块和一种时钟调整 Based 电路优化策略。* Results:  compared 于不同技术的实现，包括 CMOS、ReRAM 和超导器 RSFQ&#x2F;ERSFQ，这 paper 的设计在多个数据集和网络架构上进行了验证，并达到了约 7.8 x 10^4 倍于 ReRAM-based BNN 框架的能量效率，同时保持了相似的模型准确率。此外，与超导器基本设备相比，这 paper 的设计在最少两个数量级上高于能量效率。<details>
<summary>Abstract</summary>
Adiabatic Quantum-Flux-Parametron (AQFP) is a superconducting logic with extremely high energy efficiency. By employing the distinct polarity of current to denote logic `0' and `1', AQFP devices serve as excellent carriers for binary neural network (BNN) computations. Although recent research has made initial strides toward developing an AQFP-based BNN accelerator, several critical challenges remain, preventing the design from being a comprehensive solution. In this paper, we propose SupeRBNN, an AQFP-based randomized BNN acceleration framework that leverages software-hardware co-optimization to eventually make the AQFP devices a feasible solution for BNN acceleration. Specifically, we investigate the randomized behavior of the AQFP devices and analyze the impact of crossbar size on current attenuation, subsequently formulating the current amplitude into the values suitable for use in BNN computation. To tackle the accumulation problem and improve overall hardware performance, we propose a stochastic computing-based accumulation module and a clocking scheme adjustment-based circuit optimization method. We validate our SupeRBNN framework across various datasets and network architectures, comparing it with implementations based on different technologies, including CMOS, ReRAM, and superconducting RSFQ/ERSFQ. Experimental results demonstrate that our design achieves an energy efficiency of approximately 7.8x10^4 times higher than that of the ReRAM-based BNN framework while maintaining a similar level of model accuracy. Furthermore, when compared with superconductor-based counterparts, our framework demonstrates at least two orders of magnitude higher energy efficiency.
</details>
<details>
<summary>摘要</summary>
adiabatic量子流 Parametron (AQFP) 是一种超导逻辑，具有极高的能效性。通过使用流动中的极性来表示逻辑“0”和“1”，AQFP设备成为优秀的二进制神经网络（BNN）计算器。虽然最近的研究已经做出了初步的进展，但是还有许多关键的挑战，使得设计无法成为全面的解决方案。在这篇论文中，我们提出了SupeRBNN框架，它是基于AQFP的随机BNN加速器。我们研究了AQFP设备的随机行为，并分析了跨栅大小对流动强度的影响，从而将流动强度转换为适合BNN计算的值。为了解决积累问题并提高硬件性能，我们提出了随机计算模块和时钟调整缓存器优化方法。我们在不同的 datasets 和网络架构上验证了我们的SupeRBNN框架，并与不同技术的实现进行比较，包括CMOS、ReRAM 和超导器RSFQ/ERSFQ。实验结果表明，我们的设计可以达到约7.8×10^4倍高于ReRAM基于BNN框架的能效性，同时保持相同的模型准确性水平。此外，与超导器基于counterparts 相比，我们的框架可以达到至少两个数量级的高效性。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-State-space-Neural-Networks-for-Transport-Phenomena"><a href="#Physics-informed-State-space-Neural-Networks-for-Transport-Phenomena" class="headerlink" title="Physics-informed State-space Neural Networks for Transport Phenomena"></a>Physics-informed State-space Neural Networks for Transport Phenomena</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12211">http://arxiv.org/abs/2309.12211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshay J Dave, Richard B. Vilim</li>
<li>for: 本研究开展了一种名为物理数据驱动模型（PSM），用于实时优化、灵活性和故障忍受性在自主系统中，特别是在化学、生物医学和电力等领域的交通主导系统。传统的数据驱动方法缺乏物理限制，PSMs 则通过训练深度神经网络和物理约束使用组件的偏微分方程（PDE），实现物理约束的、端到端可微分前向动力模型。</li>
<li>methods: PSMs 使用感知器数据和物理约束组件的 PDE 训练深度神经网络，以实现物理约束的、端到端可微分前向动力模型。</li>
<li>results: 通过两个在 Silico 实验（一个热通道和一个冷却系统循环），我们证明 PSMs 比传统的数据驱动模型更加准确。此外，PSMs 还有许多优势，例如可以处理常数和时间依赖的约束，并且可以用于系统诊断和故障检测。<details>
<summary>Abstract</summary>
This work introduces Physics-informed State-space neural network Models (PSMs), a novel solution to achieving real-time optimization, flexibility, and fault tolerance in autonomous systems, particularly in transport-dominated systems such as chemical, biomedical, and power plants. Traditional data-driven methods fall short due to a lack of physical constraints like mass conservation; PSMs address this issue by training deep neural networks with sensor data and physics-informing using components' Partial Differential Equations (PDEs), resulting in a physics-constrained, end-to-end differentiable forward dynamics model. Through two in silico experiments - a heated channel and a cooling system loop - we demonstrate that PSMs offer a more accurate approach than purely data-driven models.   Beyond accuracy, there are several compelling use cases for PSMs. In this work, we showcase two: the creation of a nonlinear supervisory controller through a sequentially updated state-space representation and the proposal of a diagnostic algorithm using residuals from each of the PDEs. The former demonstrates the ability of PSMs to handle both constant and time-dependent constraints, while the latter illustrates their value in system diagnostics and fault detection. We further posit that PSMs could serve as a foundation for Digital Twins, constantly updated digital representations of physical systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Creation of a nonlinear supervisory controller through a sequentially updated state-space representation, demonstrating the ability of PSMs to handle both constant and time-dependent constraints.2. Proposal of a diagnostic algorithm using residuals from each of the PDEs, illustrating the value of PSMs in system diagnostics and fault detection.We also suggest that PSMs could serve as a foundation for Digital Twins, constantly updated digital representations of physical systems.</details></li>
</ol>
<hr>
<h2 id="Boolformer-Symbolic-Regression-of-Logic-Functions-with-Transformers"><a href="#Boolformer-Symbolic-Regression-of-Logic-Functions-with-Transformers" class="headerlink" title="Boolformer: Symbolic Regression of Logic Functions with Transformers"></a>Boolformer: Symbolic Regression of Logic Functions with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12207">http://arxiv.org/abs/2309.12207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdascoli/boolformer">https://github.com/sdascoli/boolformer</a></li>
<li>paper_authors: Stéphane d’Ascoli, Samy Bengio, Josh Susskind, Emmanuel Abbé</li>
<li>for: 这个论文旨在介绍一种名为Boolformer的 transformer 架构，用于进行端到端 симвоlic regression 的整数函数预测。</li>
<li>methods: 这种架构使用 clean truth table 预测复杂函数，并在 incomplete 和噪声观察下找到approximate表达。</li>
<li>results: 在各种实际 binary classification 问题上进行了评估，并在模型动态遗传网络中得到了竞争力。 code 和模型都公开 available。<details>
<summary>Abstract</summary>
In this work, we introduce Boolformer, the first Transformer architecture trained to perform end-to-end symbolic regression of Boolean functions. First, we show that it can predict compact formulas for complex functions which were not seen during training, when provided a clean truth table. Then, we demonstrate its ability to find approximate expressions when provided incomplete and noisy observations. We evaluate the Boolformer on a broad set of real-world binary classification datasets, demonstrating its potential as an interpretable alternative to classic machine learning methods. Finally, we apply it to the widespread task of modelling the dynamics of gene regulatory networks. Using a recent benchmark, we show that Boolformer is competitive with state-of-the art genetic algorithms with a speedup of several orders of magnitude. Our code and models are available publicly.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了 Boolformer，首个基于Transformer架构的端到端符号 regression 的布尔函数搜索算法。我们首先表明，它可以预测复杂函数的简洁公式，当提供了干净的真实表格时。然后，我们证明了它在提供不完整和噪音观测时可以找到approximate表达。我们对一组实际的二分类 datasets 进行了评估，表明它可以作为可读性的代替方法。最后，我们将其应用到了模拟生物学网络的动态方面，使用最新的benchmark，我们发现它与当前的遗传算法竞赛得分，但速度快得多个数量级。我们的代码和模型公共可用。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Conditional-Inference-in-Adaptive-Experiments"><a href="#Optimal-Conditional-Inference-in-Adaptive-Experiments" class="headerlink" title="Optimal Conditional Inference in Adaptive Experiments"></a>Optimal Conditional Inference in Adaptive Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12162">http://arxiv.org/abs/2309.12162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiafeng Chen, Isaiah Andrews</li>
<li>for: 研究批处理强化实验，即在实验中采样批处理时，可能会采用不同的批处理策略和目标参数，并且可能会在实验过程中动态地更新这些参数。</li>
<li>methods: 使用了批处理强化实验中的实验设计和数据分析技术，包括使用最后一批数据进行推断和估计。</li>
<li>results: 研究结果表明，在不假设批处理策略和目标参数的情况下，使用最后一批数据进行推断和估计是最佳的。此外，当批处理策略和目标参数是位置不变的（即不受数据的影响）时，可以通过一个额外的线性函数来捕捉更多的信息。<details>
<summary>Abstract</summary>
We study batched bandit experiments and consider the problem of inference conditional on the realized stopping time, assignment probabilities, and target parameter, where all of these may be chosen adaptively using information up to the last batch of the experiment. Absent further restrictions on the experiment, we show that inference using only the results of the last batch is optimal. When the adaptive aspects of the experiment are known to be location-invariant, in the sense that they are unchanged when we shift all batch-arm means by a constant, we show that there is additional information in the data, captured by one additional linear function of the batch-arm means. In the more restrictive case where the stopping time, assignment probabilities, and target parameter are known to depend on the data only through a collection of polyhedral events, we derive computationally tractable and optimal conditional inference procedures.
</details>
<details>
<summary>摘要</summary>
我们研究批处bandit实验，考虑实验中的批处时间、分配概率和目标参数都可能被选择适应信息，直到最后一批。不受其他限制，我们显示在只使用最后一批结果时进行推断是优化的。当批处方面的可靠性是位置不变的，即批处arm的均值在所有批处时间下都是不变的，我们显示存在一个额外的线性函数，捕捉了批处arm的均值。在更紧张的情况下，停止时间、分配概率和目标参数都知道通过数据来，我们 derivates可运行的和优化的条件推断过程。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-and-Truly-Large-Scale-Audio-Sheet-Music-Retrieval"><a href="#Towards-Robust-and-Truly-Large-Scale-Audio-Sheet-Music-Retrieval" class="headerlink" title="Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval"></a>Towards Robust and Truly Large-Scale Audio-Sheet Music Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12158">http://arxiv.org/abs/2309.12158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis Carvalho, Gerhard Widmer</li>
<li>for: 本研究的目的是探讨cross-modal music information retrieval的最新方法和技术，特别是将音频和乐谱图像连接到相同的音乐内容上。</li>
<li>methods: 本研究使用深度学习架构来学习joint embedding空间，将音频和乐谱图像modalities相连接。</li>
<li>results: 本研究提出了许多挑战，包括robustness和大规模应用等问题，并 documenated step-by-step improvement along several dimensions。<details>
<summary>Abstract</summary>
A range of applications of multi-modal music information retrieval is centred around the problem of connecting large collections of sheet music (images) to corresponding audio recordings, that is, identifying pairs of audio and score excerpts that refer to the same musical content. One of the typical and most recent approaches to this task employs cross-modal deep learning architectures to learn joint embedding spaces that link the two distinct modalities - audio and sheet music images. While there has been steady improvement on this front over the past years, a number of open problems still prevent large-scale employment of this methodology. In this article we attempt to provide an insightful examination of the current developments on audio-sheet music retrieval via deep learning methods. We first identify a set of main challenges on the road towards robust and large-scale cross-modal music retrieval in real scenarios. We then highlight the steps we have taken so far to address some of these challenges, documenting step-by-step improvement along several dimensions. We conclude by analysing the remaining challenges and present ideas for solving these, in order to pave the way to a unified and robust methodology for cross-modal music retrieval.
</details>
<details>
<summary>摘要</summary>
多种多Modal music信息检索的应用集中在将大量的乐谱图像（图像）与对应的音频录音相连接，即 identificador pairs of audio和乐谱摘要段 refer to the same musical content。一种常见的最近的方法是使用交叉模态深度学习建筑来学习联结这两种不同的模态——音频和乐谱图像。随着过去几年的不断改进，但还有一些打开的问题阻碍了大规模应用这种方法。在这篇文章中，我们尝试提供深入的检查当前的深度学习方法在Audio-Sheet music检索方面的发展。我们首先确定了在实际应用中Robust和大规模交叉模态音乐检索的主要挑战。然后，我们高亮了我们已经做出的努力，并记录了一些维度上的改进。 finally，我们分析了剩下的挑战，并提出了解决这些挑战的想法，以便开拓出一种统一和Robust的交叉模态音乐检索方法。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Contrastive-Learning-for-Robust-Audio-Sheet-Music-Retrieval-Systems"><a href="#Self-Supervised-Contrastive-Learning-for-Robust-Audio-Sheet-Music-Retrieval-Systems" class="headerlink" title="Self-Supervised Contrastive Learning for Robust Audio-Sheet Music Retrieval Systems"></a>Self-Supervised Contrastive Learning for Robust Audio-Sheet Music Retrieval Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12134">http://arxiv.org/abs/2309.12134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Luis Carvalho, Tobias Washüttl, Gerhard Widmer</li>
<li>for: 提高跨模态音乐检索系统的效果</li>
<li>methods: 使用自我超vision学习法，通过对各种模式的杂音训练网络，从实际音乐内容中提取有用的特征</li>
<li>results: 在多种实验中，预训练模型能够更好地回归 audio 和 sheet 图像中的片断，并在跨模态作品识别任务中提高检索质量<details>
<summary>Abstract</summary>
Linking sheet music images to audio recordings remains a key problem for the development of efficient cross-modal music retrieval systems. One of the fundamental approaches toward this task is to learn a cross-modal embedding space via deep neural networks that is able to connect short snippets of audio and sheet music. However, the scarcity of annotated data from real musical content affects the capability of such methods to generalize to real retrieval scenarios. In this work, we investigate whether we can mitigate this limitation with self-supervised contrastive learning, by exposing a network to a large amount of real music data as a pre-training step, by contrasting randomly augmented views of snippets of both modalities, namely audio and sheet images. Through a number of experiments on synthetic and real piano data, we show that pre-trained models are able to retrieve snippets with better precision in all scenarios and pre-training configurations. Encouraged by these results, we employ the snippet embeddings in the higher-level task of cross-modal piece identification and conduct more experiments on several retrieval configurations. In this task, we observe that the retrieval quality improves from 30% up to 100% when real music data is present. We then conclude by arguing for the potential of self-supervised contrastive learning for alleviating the annotated data scarcity in multi-modal music retrieval models.
</details>
<details>
<summary>摘要</summary>
链接Sheet music图像到音频录音的问题是多媒体音乐检索系统的关键问题。一种基本的方法是通过深度神经网络学习一个跨Modal空间，将 audio和Sheet music之间的连接短暂的音频和Sheet music片段。然而，实际音乐内容的罕见标注数据限制了这些方法的泛化能力。在这项工作中，我们研究了是否可以通过自动学习对比学习来缓解这种限制，通过对 audio和Sheet music之间的随机扩展后的视图进行对比。通过一系列的实验，我们发现在所有场景和预训练配置下，预训练模型都能够更好地检索片段。鼓动了这些结果，我们使用片段嵌入在高级任务中的跨Modal段落识别中，进行更多的实验。在这个任务中，我们发现，当有实际音乐数据时，检索质量从30%提高到100%。最后，我们结论，自动学习对比学习可以减轻多媒体音乐检索模型中的标注数据稀缺。
</details></li>
</ul>
<hr>
<h2 id="Convergence-and-Recovery-Guarantees-of-Unsupervised-Neural-Networks-for-Inverse-Problems"><a href="#Convergence-and-Recovery-Guarantees-of-Unsupervised-Neural-Networks-for-Inverse-Problems" class="headerlink" title="Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems"></a>Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12128">http://arxiv.org/abs/2309.12128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Buskulic, Jalal Fadili, Yvain Quéau</li>
<li>for: 解决 inverse problems</li>
<li>methods: 使用 unsupervised feedforward multilayer neural networks 和 deterministic convergence and recovery guarantees</li>
<li>results: 提供了对这类网络的 deterministic convergence and recovery guarantees，并 derive overparametrization boundsHere’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本文目的是解决 inverse problems</li>
<li>methods: 使用 unsupervised feedforward multilayer neural networks 和 deterministic convergence and recovery guarantees</li>
<li>results: 本文提供了对这类网络的 deterministic convergence and recovery guarantees，并 derive overparametrization bounds<details>
<summary>Abstract</summary>
Neural networks have become a prominent approach to solve inverse problems in recent years. While a plethora of such methods was developed to solve inverse problems empirically, we are still lacking clear theoretical guarantees for these methods. On the other hand, many works proved convergence to optimal solutions of neural networks in a more general setting using overparametrization as a way to control the Neural Tangent Kernel. In this work we investigate how to bridge these two worlds and we provide deterministic convergence and recovery guarantees for the class of unsupervised feedforward multilayer neural networks trained to solve inverse problems. We also derive overparametrization bounds under which a two-layers Deep Inverse Prior network with smooth activation function will benefit from our guarantees.
</details>
<details>
<summary>摘要</summary>
“神经网络在最近几年内已成为解决反问题的主要方法之一。虽然大量的方法被开发来解决反问题，但我们仍然缺乏明确的理论保证。然而，许多研究证明了神经网络在更一般的设置下 converges to optimal solutions，使用过参数化来控制神经 Tangent Kernel。在这个工作中，我们尝试将这两个世界联系起来，并提供反问题的推理和恢复保证 для无监督Feedforward多层神经网络。我们还计算了过参数化的下限，表明在使用滑动函数 activation 时，两层 Deep Inverse Prior 网络会受益于我们的保证。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Passage-Summarization-with-Recurrent-Models-for-Audio-Sheet-Music-Retrieval"><a href="#Passage-Summarization-with-Recurrent-Models-for-Audio-Sheet-Music-Retrieval" class="headerlink" title="Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval"></a>Passage Summarization with Recurrent Models for Audio-Sheet Music Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12111">http://arxiv.org/abs/2309.12111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis Carvalho, Gerhard Widmer</li>
<li>for: 这篇论文关注的是如何将Sheet Music和音频录音连接起来，以便进行跨modal音乐检索。</li>
<li>methods: 该论文提出了一种使用深度神经网络学习Sheet Music和音频录音之间的共同 embedding空间，并通过适当的相似性结构来连接它们。</li>
<li>results: 该论文通过设计一种循环网络，解决了训练神经网络所需的强相关数据和音乐和音频之间的抽象差异问题，并在实验中表明了该方法可以更高度准确地进行跨modal音乐检索。<details>
<summary>Abstract</summary>
Many applications of cross-modal music retrieval are related to connecting sheet music images to audio recordings. A typical and recent approach to this is to learn, via deep neural networks, a joint embedding space that correlates short fixed-size snippets of audio and sheet music by means of an appropriate similarity structure. However, two challenges that arise out of this strategy are the requirement of strongly aligned data to train the networks, and the inherent discrepancies of musical content between audio and sheet music snippets caused by local and global tempo differences. In this paper, we address these two shortcomings by designing a cross-modal recurrent network that learns joint embeddings that can summarize longer passages of corresponding audio and sheet music. The benefits of our method are that it only requires weakly aligned audio-sheet music pairs, as well as that the recurrent network handles the non-linearities caused by tempo variations between audio and sheet music. We conduct a number of experiments on synthetic and real piano data and scores, showing that our proposed recurrent method leads to more accurate retrieval in all possible configurations.
</details>
<details>
<summary>摘要</summary>
很多跨Modal音乐检索应用都与将乐谱图像与音频记录连接起来。一种常见的方法是通过深度神经网络学习一个共同嵌入空间，使得短时间内的音频和乐谱图像之间存在相似性结构。然而，这种方法存在两个挑战：首先，需要强相关的数据来训练网络，其次，音频和乐谱图像中的音乐内容之间的本地和全局滥讲差异会导致各种非线性。在这篇论文中，我们解决这两个缺陷，通过设计一种跨Modal循环网络，学习联合嵌入空间，可以摘要长passage的相应音频和乐谱图像。我们的方法的优点是：只需弱相关的音频-乐谱图像对，以及循环网络可以处理非线性，带来更高的检索精度。我们在合成和实际钢琴数据和谱面上进行了一系列实验，表明我们的提议的循环方法可以在所有可能的配置下实现更高的检索精度。
</details></li>
</ul>
<hr>
<h2 id="Clustering-based-Domain-Incremental-Learning"><a href="#Clustering-based-Domain-Incremental-Learning" class="headerlink" title="Clustering-based Domain-Incremental Learning"></a>Clustering-based Domain-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12078">http://arxiv.org/abs/2309.12078</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SOYJUN/Implement-ODR-protocol">https://github.com/SOYJUN/Implement-ODR-protocol</a></li>
<li>paper_authors: Christiaan Lamers, Rene Vidal, Nabil Belbachir, Niki van Stein, Thomas Baeck, Paris Giampouras</li>
<li>for:  solves the catastrophic forgetting problem in domain-incremental learning, a setting that was previously unsolved.</li>
<li>methods:  uses an online clustering-based approach on a dynamically updated finite pool of samples or gradients to alleviate the need for task information.</li>
<li>results:  experiments on real datasets demonstrate the effectiveness of the proposed strategy and its promising performance compared to state-of-the-art methods.<details>
<summary>Abstract</summary>
We consider the problem of learning multiple tasks in a continual learning setting in which data from different tasks is presented to the learner in a streaming fashion. A key challenge in this setting is the so-called "catastrophic forgetting problem", in which the performance of the learner in an "old task" decreases when subsequently trained on a "new task". Existing continual learning methods, such as Averaged Gradient Episodic Memory (A-GEM) and Orthogonal Gradient Descent (OGD), address catastrophic forgetting by minimizing the loss for the current task without increasing the loss for previous tasks. However, these methods assume the learner knows when the task changes, which is unrealistic in practice. In this paper, we alleviate the need to provide the algorithm with information about task changes by using an online clustering-based approach on a dynamically updated finite pool of samples or gradients. We thereby successfully counteract catastrophic forgetting in one of the hardest settings, namely: domain-incremental learning, a setting for which the problem was previously unsolved. We showcase the benefits of our approach by applying these ideas to projection-based methods, such as A-GEM and OGD, which lead to task-agnostic versions of them. Experiments on real datasets demonstrate the effectiveness of the proposed strategy and its promising performance compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
我们考虑了多个任务在连续学习设定下学习问题，在数据流式方式下提供不同任务的数据给学习者。一个关键问题在这个设定下是所谓的“追忆问题”（catastrophic forgetting），即学习者在学习新任务时，对于之前学习的任务的性能下降。现有的连续学习方法，如Averaged Gradient Episodic Memory（A-GEM）和Orthogonal Gradient Descent（OGD），解决了追忆问题 by 将当前任务的损失降到最低，而不会提高之前任务的损失。但这些方法假设学习者知道任务的变化，这在实践中是不现实的。在这篇论文中，我们使用在线 clustering-based 方法，对于动态更新的有限池的样本或梯度进行处理，从而成功地避免了追忆问题。我们在域逐学习设定下应用这些想法，并将其应用到投影基本方法，如A-GEM 和 OGD，从而实现了任务无关的版本。实验结果表明，提议的策略有效地解决了追忆问题，并在实际数据上达到了比state-of-the-art方法更高的性能。
</details></li>
</ul>
<hr>
<h2 id="S-GBDT-Frugal-Differentially-Private-Gradient-Boosting-Decision-Trees"><a href="#S-GBDT-Frugal-Differentially-Private-Gradient-Boosting-Decision-Trees" class="headerlink" title="S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees"></a>S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12041">http://arxiv.org/abs/2309.12041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Kirsche, Thorsten Peinemann, Joshua Stock, Carlos Cotrini, Esfandiar Mohammadi</li>
<li>For: The paper aims to develop a privacy-preserving learning method for gradient boosting decision trees (GBDT) that provides strong utility-privacy tradeoffs for tabular data.* Methods: The proposed method uses four techniques to improve the utility-privacy tradeoff: (1) an improved noise scaling approach with tighter accounting of privacy leakage, (2) integration of individual R&#39;enyi filters, (3) incorporation of random decision tree splits, and (4) subsampling for privacy amplification.* Results: The proposed method achieves high accuracy on two datasets (Abalone and Adult) with varying levels of privacy parameters (ε). Specifically, on the Abalone dataset, the proposed method achieves a $R^2$-score of 0.39 for ε&#x3D;0.15, which is the closest prior work only achieved for ε&#x3D;10.0. On the Adult dataset, the proposed method achieves a test error of 18.7% for ε&#x3D;0.07, which is the closest prior work only achieved for ε&#x3D;1.0. The proposed method also achieves high accuracy on the Abalone dataset for higher privacy parameters (ε&#x3D;0.54) and is very close to the accuracy of the non-private version of GBDT on the Adult dataset (ε&#x3D;0.54).<details>
<summary>Abstract</summary>
Privacy-preserving learning of gradient boosting decision trees (GBDT) has the potential for strong utility-privacy tradeoffs for tabular data, such as census data or medical meta data: classical GBDT learners can extract non-linear patterns from small sized datasets. The state-of-the-art notion for provable privacy-properties is differential privacy, which requires that the impact of single data points is limited and deniable. We introduce a novel differentially private GBDT learner and utilize four main techniques to improve the utility-privacy tradeoff. (1) We use an improved noise scaling approach with tighter accounting of privacy leakage of a decision tree leaf compared to prior work, resulting in noise that in expectation scales with $O(1/n)$, for $n$ data points. (2) We integrate individual R\'enyi filters to our method to learn from data points that have been underutilized during an iterative training process, which -- potentially of independent interest -- results in a natural yet effective insight to learning streams of non-i.i.d. data. (3) We incorporate the concept of random decision tree splits to concentrate privacy budget on learning leaves. (4) We deploy subsampling for privacy amplification. Our evaluation shows for the Abalone dataset ($<4k$ training data points) a $R^2$-score of $0.39$ for $\varepsilon=0.15$, which the closest prior work only achieved for $\varepsilon=10.0$. On the Adult dataset ($50k$ training data points) we achieve test error of $18.7\,\%$ for $\varepsilon=0.07$ which the closest prior work only achieved for $\varepsilon=1.0$. For the Abalone dataset for $\varepsilon=0.54$ we achieve $R^2$-score of $0.47$ which is very close to the $R^2$-score of $0.54$ for the nonprivate version of GBDT. For the Adult dataset for $\varepsilon=0.54$ we achieve test error $17.1\,\%$ which is very close to the test error $13.7\,\%$ of the nonprivate version of GBDT.
</details>
<details>
<summary>摘要</summary>
privacy-preserving 学习树 boosting 算法（GBDT）在 tabular 数据上有强大的用户-隐私交易，例如人口普查数据或医疗特征数据：经典 GBDT 学习器可以从小型数据集中提取非线性模式。我们引入了一种新的具有证明性隐私性质的 GBDT 学习器，并使用四种主要技术来改善用户-隐私交易。1. 我们使用改进的噪声扩大方法，对决策树叶节点的隐私泄露进行更精细的评估，从而使噪声在预期中呈线性关系，与数据点数 $n$ 成正比。2. 我们将个体 R\'enyi 筛选器 integrate 到我们的方法中，以学习尚未被利用的数据点，这可能是独立有趣的发现，并且自然地带来一种有效的学习流程。3. 我们启用随机决策树分裂的概念，以集中隐私预算在学习叶节点上。4. 我们使用采样来增强隐私压缩。我们的评估显示，在 Abalone 数据集（训练数据点数 fewer than 4k）中，我们在 $\varepsilon=0.15$ 下达到 $R^2$ 分数为 0.39，而最近的相关工作只能在 $\varepsilon=10.0$ 下达到这个分数。在 Adult 数据集（训练数据点数 50k）中，我们在 $\varepsilon=0.07$ 下达到测试错误率为 18.7%，而最近的相关工作只能在 $\varepsilon=1.0$ 下达到这个错误率。在 Abalone 数据集中，在 $\varepsilon=0.54$ 下，我们达到 $R^2$ 分数为 0.47，几乎与非隐私版 GBDT 的 $R^2$ 分数相同。在 Adult 数据集中，在 $\varepsilon=0.54$ 下，我们达到测试错误率为 17.1%，几乎与非隐私版 GBDT 的测试错误率相同。
</details></li>
</ul>
<hr>
<h2 id="Uplift-vs-predictive-modeling-a-theoretical-analysis"><a href="#Uplift-vs-predictive-modeling-a-theoretical-analysis" class="headerlink" title="Uplift vs. predictive modeling: a theoretical analysis"></a>Uplift vs. predictive modeling: a theoretical analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12036">http://arxiv.org/abs/2309.12036</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/theoverhelst/uplift-predictive-paper">https://github.com/theoverhelst/uplift-predictive-paper</a></li>
<li>paper_authors: Théo Verhelst, Robin Petit, Wouter Verbeke, Gianluca Bontempi</li>
<li>for: 本研究旨在探讨机器学习技术在决策中的可加价效应，并对相关的论证和实践提供了一个全面的探讨。</li>
<li>methods: 本研究使用了一种既包含了机器学习技术又具有 causal orientation 的方法，并对这种方法的性能进行了 theoretically 的分析和实践验证。</li>
<li>results: 研究结果显示，在某些情况下，可加价模型会在predictive 模型之上带来更高的效果，但是这些情况下的参数会影响这种效果。研究还发现，mutual information 和 estimator variance 等参数具有重要作用。<details>
<summary>Abstract</summary>
Despite the growing popularity of machine-learning techniques in decision-making, the added value of causal-oriented strategies with respect to pure machine-learning approaches has rarely been quantified in the literature. These strategies are crucial for practitioners in various domains, such as marketing, telecommunications, health care and finance. This paper presents a comprehensive treatment of the subject, starting from firm theoretical foundations and highlighting the parameters that influence the performance of the uplift and predictive approaches. The focus of the paper is on a binary outcome case and a binary action, and the paper presents a theoretical analysis of uplift modeling, comparing it with the classical predictive approach. The main research contributions of the paper include a new formulation of the measure of profit, a formal proof of the convergence of the uplift curve to the measure of profit ,and an illustration, through simulations, of the conditions under which predictive approaches still outperform uplift modeling. We show that the mutual information between the features and the outcome plays a significant role, along with the variance of the estimators, the distribution of the potential outcomes and the underlying costs and benefits of the treatment and the outcome.
</details>
<details>
<summary>摘要</summary>
The main research contributions of the paper include:1. A new formulation of the measure of profit.2. A formal proof of the convergence of the uplift curve to the measure of profit.3. An illustration, through simulations, of the conditions under which predictive approaches still outperform uplift modeling.We show that the mutual information between the features and the outcome plays a significant role, along with the variance of the estimators, the distribution of the potential outcomes, and the underlying costs and benefits of the treatment and the outcome.
</details></li>
</ul>
<hr>
<h2 id="Human-in-the-Loop-Causal-Discovery-under-Latent-Confounding-using-Ancestral-GFlowNets"><a href="#Human-in-the-Loop-Causal-Discovery-under-Latent-Confounding-using-Ancestral-GFlowNets" class="headerlink" title="Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets"></a>Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12032">http://arxiv.org/abs/2309.12032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiago da Silva, Eliezer Silva, Adèle Ribeiro, António Góis, Dominik Heider, Samuel Kaski, Diego Mesquita</li>
<li>for: 提高 causal inference 的精度，尤其是在数据稀缺时，避免因为 latent confounders 的影响而导致的不准确的 causal relation 推断。</li>
<li>methods: 提出一种基于 generative flow networks 的方法，通过在 candidate graphs 中采样 proportionally to a belief distribution 以及通过对 experts 的反馈来 iteratively 缩小 uncertainty 。</li>
<li>results: 通过实验表明，该方法可以准确地采样 ancestral graphs  distribution，并且可以通过人类反馈来改进推断质量。<details>
<summary>Abstract</summary>
Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -- especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expert about the relations among variables, effectively reducing the uncertainty of our belief over ancestral graphs. Finally, we update our samples to incorporate human feedback via importance sampling. Importantly, our method does not require causal sufficiency (i.e., unobserved confounders may exist). Experiments with synthetic observational data show that our method can accurately sample from distributions over ancestral graphs and that we can greatly improve inference quality with human aid.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -- especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expert about the relations among variables, effectively reducing the uncertainty of our belief over ancestral graphs. Finally, we update our samples to incorporate human feedback via importance sampling. Importantly, our method does not require causal sufficiency (i.e., unobserved confounders may exist). Experiments with synthetic observational data show that our method can accurately sample from distributions over ancestral graphs and that we can greatly improve inference quality with human aid."中文翻译：<<SYS>> translate "结构学习是 causal inference 的核心。特别是在数据稀缺时， causal discovery（CD）算法容易导致不准确的 causal 关系推断，而且这些关系可能与专家知识相悖。此外，大多数 CD 方法不提供不确定性估计，使得用户不能正确地 интерпретирова结果并改进推断过程。很奇怪的是，CD 是人类中心的事物，但没有任何研究旨在构建可以 outputs 不确定性估计并且与专家交互改进 CD 的方法。为解决这些问题，我们开始由 proposing 使用 generative flow 网络来样本 (causal) 祖先图 proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC)。然后，我们利用候选图的多样性并引入最佳实验设计，以便逐次询问专家关于变量之间的关系，从而减少我们对祖先图的不确定性。最后，我们更新样本以包括人类反馈 via importance sampling。重要的是，我们的方法不需要 causal sufficiency (i.e., unobserved confounders may exist)。我们在 sintetic observational data 上进行了实验，结果表明我们的方法可以准确样本 distribution over ancestral graphs，并且可以通过人类帮助提高推断质量。
</details></li>
</ul>
<hr>
<h2 id="Robust-Approximation-Algorithms-for-Non-monotone-k-Submodular-Maximization-under-a-Knapsack-Constraint"><a href="#Robust-Approximation-Algorithms-for-Non-monotone-k-Submodular-Maximization-under-a-Knapsack-Constraint" class="headerlink" title="Robust Approximation Algorithms for Non-monotone $k$-Submodular Maximization under a Knapsack Constraint"></a>Robust Approximation Algorithms for Non-monotone $k$-Submodular Maximization under a Knapsack Constraint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12025">http://arxiv.org/abs/2309.12025</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tantdhvan/KSE2023">https://github.com/tantdhvan/KSE2023</a></li>
<li>paper_authors: Dung T. K. Ha, Canh V. Pham, Tan D. Tran, Huan X. Hoang</li>
<li>for: 提出了一个非MONOTONE $k$-submodular最大化问题，用于数据概要、信息传递等应用。</li>
<li>methods: 提出了两种杜林约化算法，可以在$O(nk)$查询复杂度下提供竞争性提高的解。</li>
<li>results: 算法可以在$O(nk)$查询复杂度下提供常数约化比率，比现有算法快速返回解。实验结果也证明了算法的理论分析和实际效果。<details>
<summary>Abstract</summary>
The problem of non-monotone $k$-submodular maximization under a knapsack constraint ($\kSMK$) over the ground set size $n$ has been raised in many applications in machine learning, such as data summarization, information propagation, etc. However, existing algorithms for the problem are facing questioning of how to overcome the non-monotone case and how to fast return a good solution in case of the big size of data. This paper introduces two deterministic approximation algorithms for the problem that competitively improve the query complexity of existing algorithms.   Our first algorithm, $\LAA$, returns an approximation ratio of $1/19$ within $O(nk)$ query complexity. The second one, $\RLA$, improves the approximation ratio to $1/5-\epsilon$ in $O(nk)$ queries, where $\epsilon$ is an input parameter.   Our algorithms are the first ones that provide constant approximation ratios within only $O(nk)$ query complexity for the non-monotone objective. They, therefore, need fewer the number of queries than state-of-the-the-art ones by a factor of $\Omega(\log n)$.   Besides the theoretical analysis, we have evaluated our proposed ones with several experiments in some instances: Influence Maximization and Sensor Placement for the problem. The results confirm that our algorithms ensure theoretical quality as the cutting-edge techniques and significantly reduce the number of queries.
</details>
<details>
<summary>摘要</summary>
“非单调 $k$-submodular最大化问题（$\kSMK$) 在机器学习应用中得到了很多关注，例如摘要、信息传递等。然而，现有的算法对这个问题存在两个问题：一是如何解决非单调情况，二是如何快速返回良好的解决方案。本文提出了两个决定性近似算法，它们可以对 $\kSMK$ 问题提供竞争性提高查询量的解决方案。我们的第一个算法（$\LAA$）可以在 $O(nk)$ 查询量下提供一个近似比率为 $1/19$。第二个算法（$\RLA$）可以在 $O(nk)$ 查询量下提供一个近似比率为 $1/5-\epsilon$，其中 $\epsilon$ 是输入参数。我们的算法是第一个可以在非单调情况下提供常数近似比率，并且需要 fewer 查询量 than state-of-the-art 的一个因数为 $\Omega(\log n)$。”Note that the translation is in Simplified Chinese, which is one of the two standardized Chinese writing systems. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-SAEAs-with-Unevaluated-Solutions-A-Case-Study-of-Relation-Model-for-Expensive-Optimization"><a href="#Enhancing-SAEAs-with-Unevaluated-Solutions-A-Case-Study-of-Relation-Model-for-Expensive-Optimization" class="headerlink" title="Enhancing SAEAs with Unevaluated Solutions: A Case Study of Relation Model for Expensive Optimization"></a>Enhancing SAEAs with Unevaluated Solutions: A Case Study of Relation Model for Expensive Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11994">http://arxiv.org/abs/2309.11994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Hao, Xiaoqun Zhang, Aimin Zhou</li>
<li>for: 提高优化问题的解决效率（EOPs）中的质量解决方案。</li>
<li>methods: 使用模型帮助选择技术来提高SAEAs的效率。</li>
<li>results: 在两个测试集上，使用关系模型选择未评估解决方案可以更好地提高算法的效率，并且这些未评估解决方案具有高潜力。<details>
<summary>Abstract</summary>
Surrogate-assisted evolutionary algorithms (SAEAs) hold significant importance in resolving expensive optimization problems~(EOPs). Extensive efforts have been devoted to improving the efficacy of SAEAs through the development of proficient model-assisted selection methods. However, generating high-quality solutions is a prerequisite for selection. The fundamental paradigm of evaluating a limited number of solutions in each generation within SAEAs reduces the variance of adjacent populations, thus impacting the quality of offspring solutions. This is a frequently encountered issue, yet it has not gained widespread attention. This paper presents a framework using unevaluated solutions to enhance the efficiency of SAEAs. The surrogate model is employed to identify high-quality solutions for direct generation of new solutions without evaluation. To ensure dependable selection, we have introduced two tailored relation models for the selection of the optimal solution and the unevaluated population. A comprehensive experimental analysis is performed on two test suites, which showcases the superiority of the relation model over regression and classification models in the selection phase. Furthermore, the surrogate-selected unevaluated solutions with high potential have been shown to significantly enhance the efficiency of the algorithm.
</details>
<details>
<summary>摘要</summary>
受助者质量进化算法（SAEA）在解决成本高优化问题（EOP）方面具有重要意义。针对提高 SAEA 的效果，广泛的努力已经投入到了开发高效的模型协助选择方法上。然而，生成高质量解决方案是选择高质量解决方案的先置条件。SAEA 中评估每代限制的解决方案的基本思想会减少邻居 populations 的方差，从而影响下一代解决方案的质量。这是一个 часто遇到的问题，但它尚未受到广泛的关注。本文提出了一种使用未评估解决方案来提高 SAEA 的效率的框架。使用 surrogate 模型来标识高质量解决方案，然后直接生成新的解决方案。为保证可靠的选择，我们引入了两种特制的关系模型，一种用于选择优质解决方案，另一种用于选择未评估 популяции。通过对两个测试集进行了全面的实验分析，我们展示了模型在选择阶段的优越性，以及 surrogate 选择的未评估解决方案具有显著提高 SAEA 效率的作用。
</details></li>
</ul>
<hr>
<h2 id="Variational-Connectionist-Temporal-Classification-for-Order-Preserving-Sequence-Modeling"><a href="#Variational-Connectionist-Temporal-Classification-for-Order-Preserving-Sequence-Modeling" class="headerlink" title="Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling"></a>Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11983">http://arxiv.org/abs/2309.11983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Nan, Ting Dang, Vidhyasaharan Sethu, Beena Ahmed</li>
<li>for: This paper is written for researchers and practitioners working on sequence modeling tasks, particularly in the area of speech recognition, who are interested in using CTC with variational models to improve the generalization of their models and handle data variability.</li>
<li>methods: The paper proposes integrating CTC with a variational model, and derives two versions of a novel variational CTC loss function based on reasonable assumptions about the latent variables. The loss functions allow direct optimization of the variational lower bound for the model log-likelihood, and are computationally tractable.</li>
<li>results: The paper presents the results of training sequence models using the proposed variational CTC loss functions, and shows that they lead to more generalizable models that preserve order in the input and target sequences. The results demonstrate the effectiveness of the proposed approach in handling data variability and improving the performance of sequence models.<details>
<summary>Abstract</summary>
Connectionist temporal classification (CTC) is commonly adopted for sequence modeling tasks like speech recognition, where it is necessary to preserve order between the input and target sequences. However, CTC is only applied to deterministic sequence models, where the latent space is discontinuous and sparse, which in turn makes them less capable of handling data variability when compared to variational models. In this paper, we integrate CTC with a variational model and derive loss functions that can be used to train more generalizable sequence models that preserve order. Specifically, we derive two versions of the novel variational CTC based on two reasonable assumptions, the first being that the variational latent variables at each time step are conditionally independent; and the second being that these latent variables are Markovian. We show that both loss functions allow direct optimization of the variational lower bound for the model log-likelihood, and present computationally tractable forms for implementing them.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用 Connectionist Temporal Classification (CTC) 是一种常用的序列模型化任务，如语音识别，因为它需要保持输入和目标序列之间的顺序关系。然而，CTC 只适用于决定性序列模型，其潜在空间是离散的和稀疏的，这使得它们在面对数据变化时比较不能处理。在这篇论文中，我们将 CT 与变量模型结合，并 derive loss functions 可以用来训练更一般化的序列模型，保持顺序。 Specifically，我们 deriv 两个版本的新的变量 CT 基于两个合理的假设：第一个假设是变量 latent 在每个时间步是独立的；第二个假设是这些 latent 变量是 Markovian。我们显示了这两个损失函数可以直接优化变量下界，并提供了实现的计算 tractable 形式。Note: "Simplified Chinese" is a romanization of the Chinese language that uses a simplified set of characters and pronunciation, which is commonly used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Generating-Hierarchical-Structures-for-Improved-Time-Series-Classification-Using-Stochastic-Splitting-Functions"><a href="#Generating-Hierarchical-Structures-for-Improved-Time-Series-Classification-Using-Stochastic-Splitting-Functions" class="headerlink" title="Generating Hierarchical Structures for Improved Time Series Classification Using Stochastic Splitting Functions"></a>Generating Hierarchical Structures for Improved Time Series Classification Using Stochastic Splitting Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11963">http://arxiv.org/abs/2309.11963</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alagoz/hc4tsc_hdc_ssf">https://github.com/alagoz/hc4tsc_hdc_ssf</a></li>
<li>paper_authors: Celal Alagoz</li>
<li>For: The paper is written for enhancing classification performance in multi-class datasets through hierarchical classification.* Methods: The paper introduces a novel hierarchical divisive clustering approach with stochastic splitting functions (SSFs) to generate hierarchy without requiring explicit information.* Results: The approach significantly improves classification performance in approximately half and a third of the datasets when using rocket and svm as the classifier, respectively. The study also explores the relationship between dataset features and HC performance.Here is the simplified Chinese text for the three key points:* For: 这篇论文是为了提高多类数据集中的分类性能而写的。* Methods: 论文提出了一种基于杂素分割函数（SSF）的层次分类方法，可以不需要明确的层次信息来生成层次结构。* Results: 实验结果表明，该方法在使用rocket和svm分类器时，在约半数和一third的数据集中能够显著提高分类性能。<details>
<summary>Abstract</summary>
This study introduces a novel hierarchical divisive clustering approach with stochastic splitting functions (SSFs) to enhance classification performance in multi-class datasets through hierarchical classification (HC). The method has the unique capability of generating hierarchy without requiring explicit information, making it suitable for datasets lacking prior knowledge of hierarchy. By systematically dividing classes into two subsets based on their discriminability according to the classifier, the proposed approach constructs a binary tree representation of hierarchical classes. The approach is evaluated on 46 multi-class time series datasets using popular classifiers (svm and rocket) and SSFs (potr, srtr, and lsoo). The results reveal that the approach significantly improves classification performance in approximately half and a third of the datasets when using rocket and svm as the classifier, respectively. The study also explores the relationship between dataset features and HC performance. While the number of classes and flat classification (FC) score show consistent significance, variations are observed with different splitting functions. Overall, the proposed approach presents a promising strategy for enhancing classification by generating hierarchical structure in multi-class time series datasets. Future research directions involve exploring different splitting functions, classifiers, and hierarchy structures, as well as applying the approach to diverse domains beyond time series data. The source code is made openly available to facilitate reproducibility and further exploration of the method.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-Probability-of-Immunity"><a href="#On-the-Probability-of-Immunity" class="headerlink" title="On the Probability of Immunity"></a>On the Probability of Immunity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11942">http://arxiv.org/abs/2309.11942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZahirSen/scaling-octo-guide">https://github.com/ZahirSen/scaling-octo-guide</a></li>
<li>paper_authors: Jose M. Peña</li>
<li>for: 本研究探讨了免疫机会的概率，即是否曝露对结果的影响。</li>
<li>methods: 我们 derive了免疫必要和 suficient conditions，以及 $\epsilon$-bounded免疫，即免疫概率为零和 $\epsilon$-bounded的情况。</li>
<li>results: 我们可以从随机控制试验中估计受益的概率（即曝露导致效果），并且可以生成更紧的受益概率 bounds。此外，我们还引入了间接免疫（通过介质）的概念，并重复了我们之前的分析。最后，我们提出了对免疫概率下无量化影响的敏感分析方法。<details>
<summary>Abstract</summary>
This work is devoted to the study of the probability of immunity, i.e. the effect occurs whether exposed or not. We derive necessary and sufficient conditions for non-immunity and $\epsilon$-bounded immunity, i.e. the probability of immunity is zero and $\epsilon$-bounded, respectively. The former allows us to estimate the probability of benefit (i.e., the effect occurs if and only if exposed) from a randomized controlled trial, and the latter allows us to produce bounds of the probability of benefit that are tighter than the existing ones. We also introduce the concept of indirect immunity (i.e., through a mediator) and repeat our previous analysis for it. Finally, we propose a method for sensitivity analysis of the probability of immunity under unmeasured confounding.
</details>
<details>
<summary>摘要</summary>
这项研究探讨了免疫概率的可能性，即效果发生或不发生。我们 deriv出了免疫必要和 suficient conditions，即免疫概率为零和ε-bounded免疫概率，分别表示效果发生和ε-bounded的免疫概率。前者允许我们从Randomized controlled trial中估算效果发生的概率，而后者允许我们生成更紧的效果发生的概率上限。我们还介绍了间接免疫（通过介质）的概念，并重复了我们的前一次分析。最后，我们提出了对免疫概率下隐藏偏见的敏感分析方法。Note: "ε-bounded" in the text refers to the probability of immunity being bounded above by a small positive value ε.
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-oriented-Survey-on-Tiny-Machine-Learning"><a href="#A-Machine-Learning-oriented-Survey-on-Tiny-Machine-Learning" class="headerlink" title="A Machine Learning-oriented Survey on Tiny Machine Learning"></a>A Machine Learning-oriented Survey on Tiny Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11932">http://arxiv.org/abs/2309.11932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luigi Capogrosso, Federico Cunico, Dong Seon Cheng, Franco Fummi, Marco cristani</li>
<li>for: 这篇论文旨在为tiny machine learning（TinyML）领域的研究提供一个权威的综述，尤其是关于TinyML中的学习算法。</li>
<li>methods: 本文采用了PRISMA方法流程进行系统性的文献综述，分为三个工作流程：ML-oriented、HW-oriented和合理设计。</li>
<li>results: 本文提出了TinyML学习领域的分类体系，涵盖了不同家族的模型优化和设计，以及当前领域的最佳实践。<details>
<summary>Abstract</summary>
The emergence of Tiny Machine Learning (TinyML) has positively revolutionized the field of Artificial Intelligence by promoting the joint design of resource-constrained IoT hardware devices and their learning-based software architectures. TinyML carries an essential role within the fourth and fifth industrial revolutions in helping societies, economies, and individuals employ effective AI-infused computing technologies (e.g., smart cities, automotive, and medical robotics). Given its multidisciplinary nature, the field of TinyML has been approached from many different angles: this comprehensive survey wishes to provide an up-to-date overview focused on all the learning algorithms within TinyML-based solutions. The survey is based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow, allowing for a systematic and complete literature survey. In particular, firstly we will examine the three different workflows for implementing a TinyML-based system, i.e., ML-oriented, HW-oriented, and co-design. Secondly, we propose a taxonomy that covers the learning panorama under the TinyML lens, examining in detail the different families of model optimization and design, as well as the state-of-the-art learning techniques. Thirdly, this survey will present the distinct features of hardware devices and software tools that represent the current state-of-the-art for TinyML intelligent edge applications. Finally, we discuss the challenges and future directions.
</details>
<details>
<summary>摘要</summary>
《tiny machine learning（tinyml）的出现》已经对人工智能（ai）领域产生了积极的革命，推动了资源受限的iot设备和其学习基础架构的共同设计。 tinyml在第四和第五个工业革命中发挥着重要的角色，帮助社会、经济和个人使用有效的ai混合计算技术（例如智能城市、汽车和医疗机器人）。由于tinyml的多学科性，该领域被不同的方向攻击：本综述旨在提供tinyml基础设施中所有学习算法的全面检视。本综述采用《Preferred Reporting Items for Systematic Reviews and Meta-Analyses》（prisma）方法流程，以系统和完整的方式检查文献。特别是，我们首先检查tinyml基础设施实施的三种不同工作流程，即ml oriented、hw oriented和codesign。其次，我们提出了tinyml学习领域的分类，审查ml下的不同家族模型优化和设计，以及当前领域的state-of-the-art学习技术。最后，本综述将展示当前tinyml智能边缘应用中的硬件设备和软件工具的最新状态。 Finally, we discuss the challenges and future directions.
</details></li>
</ul>
<hr>
<h2 id="Activation-Compression-of-Graph-Neural-Networks-using-Block-wise-Quantization-with-Improved-Variance-Minimization"><a href="#Activation-Compression-of-Graph-Neural-Networks-using-Block-wise-Quantization-with-Improved-Variance-Minimization" class="headerlink" title="Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization"></a>Activation Compression of Graph Neural Networks using Block-wise Quantization with Improved Variance Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11856">http://arxiv.org/abs/2309.11856</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saintslab/i-exact">https://github.com/saintslab/i-exact</a></li>
<li>paper_authors: Sebastian Eliassen, Raghavendra Selvan</li>
<li>for: 提高大规模图 neural network 的训练效率，尤其是减少内存消耗。</li>
<li>methods: 使用极端活动压缩（EXACT）策略，对中间激活图进行量化，从 INT2 精度下进行压缩，以实现大幅减少 GPU 内存消耗，而无需做出重要性的牺牲。</li>
<li>results: 在 EXACT 策略的基础上，使用块级别的量化策略，可以进一步减少内存消耗（&gt;15%），并且在每个 epoch 中提高运行速度（约 5%），即使在执行极端的量化时也可以保持相似的性能交易。此外，对 EXACT 中间激活图分布的假设（假设为均匀分布）进行了更正，并提供了改进的量化和解量化步骤的方差估计。<details>
<summary>Abstract</summary>
Efficient training of large-scale graph neural networks (GNNs) has been studied with a specific focus on reducing their memory consumption. Work by Liu et al. (2022) proposed extreme activation compression (EXACT) which demonstrated drastic reduction in memory consumption by performing quantization of the intermediate activation maps down to using INT2 precision. They showed little to no reduction in performance while achieving large reductions in GPU memory consumption. In this work, we present an improvement to the EXACT strategy by using block-wise quantization of the intermediate activation maps. We experimentally analyze different block sizes and show further reduction in memory consumption (>15%), and runtime speedup per epoch (about 5%) even when performing extreme extents of quantization with similar performance trade-offs as with the original EXACT. Further, we present a correction to the assumptions on the distribution of intermediate activation maps in EXACT (assumed to be uniform) and show improved variance estimations of the quantization and dequantization steps.
</details>
<details>
<summary>摘要</summary>
大规模图 neural network (GNN) 的高效训练已经被研究，特别是减少它们的内存消耗。工作 by Liu et al. (2022) 提出了极化活动压缩 (EXACT)，通过对中间活动图进行量化，以 INT2 精度进行压缩。他们发现了很少到无关于性能的下降，同时实现了大量的 GPU 内存消耗减少。在这个工作中，我们提出了对 EXACT 策略的改进，通过分割 activation map 的块式压缩。我们通过不同的块大小进行实验分析，并证明了更大的减少内存消耗（> 15%）和每个轮次的运行速度增加（约 5%），即使在执行极端的量化时，与原始 EXACT 的性能折衔保持相同。此外，我们对 EXACT 中对中间活动图的分布假设（假设为均匀分布）进行了修正，并提供了改进的量化和解量化步骤的方差估计。
</details></li>
</ul>
<hr>
<h2 id="TMac-Temporal-Multi-Modal-Graph-Learning-for-Acoustic-Event-Classification"><a href="#TMac-Temporal-Multi-Modal-Graph-Learning-for-Acoustic-Event-Classification" class="headerlink" title="TMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification"></a>TMac: Temporal Multi-Modal Graph Learning for Acoustic Event Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11845">http://arxiv.org/abs/2309.11845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mgithubl/tmac">https://github.com/mgithubl/tmac</a></li>
<li>paper_authors: Meng Liu, Ke Liang, Dayu Hu, Hao Yu, Yue Liu, Lingyuan Meng, Wenxuan Tu, Sihang Zhou, Xinwang Liu<br>for:This paper proposes a method for acoustic event classification using temporal multi-modal graph learning, which can better handle the information of multi-modal data with temporal attributes.methods:The proposed method, called TMac, constructs a temporal graph for each acoustic event, dividing its audio and video data into multiple segments and modeling the temporal relationships between them using graph learning techniques.results:Experiments show that TMac outperforms other state-of-the-art models in performance, demonstrating its effectiveness in capturing the dynamic information in intra-modal and inter-modal data. The code is available at <a target="_blank" rel="noopener" href="https://github.com/MGitHubL/TMac.Here">https://github.com/MGitHubL/TMac.Here</a> is the simplified Chinese text:for:这篇论文提出了一种基于时间多模式图学习的声音事件分类方法，可以更好地处理具有时间属性的多模式数据。methods:该方法，称为TMac，对声音事件进行分解，将其声音数据和视频数据分割成多个segment，然后使用图学习技术来模型这些segment之间的时间关系。results:实验表明，TMac比其他状态对应模型更高效，能够更好地捕捉多模式数据中的内部和间部关系。代码可以在<a target="_blank" rel="noopener" href="https://github.com/MGitHubL/TMac%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/MGitHubL/TMac上下载。</a><details>
<summary>Abstract</summary>
Audiovisual data is everywhere in this digital age, which raises higher requirements for the deep learning models developed on them. To well handle the information of the multi-modal data is the key to a better audiovisual modal. We observe that these audiovisual data naturally have temporal attributes, such as the time information for each frame in the video. More concretely, such data is inherently multi-modal according to both audio and visual cues, which proceed in a strict chronological order. It indicates that temporal information is important in multi-modal acoustic event modeling for both intra- and inter-modal. However, existing methods deal with each modal feature independently and simply fuse them together, which neglects the mining of temporal relation and thus leads to sub-optimal performance. With this motivation, we propose a Temporal Multi-modal graph learning method for Acoustic event Classification, called TMac, by modeling such temporal information via graph learning techniques. In particular, we construct a temporal graph for each acoustic event, dividing its audio data and video data into multiple segments. Each segment can be considered as a node, and the temporal relationships between nodes can be considered as timestamps on their edges. In this case, we can smoothly capture the dynamic information in intra-modal and inter-modal. Several experiments are conducted to demonstrate TMac outperforms other SOTA models in performance. Our code is available at https://github.com/MGitHubL/TMac.
</details>
<details>
<summary>摘要</summary>
现在的数字时代，audiovisual数据在 everywhere，这 heightened the requirements for the deep learning models developed on them. To well handle the information of the multi-modal data is the key to a better audiovisual experience. We observe that these audiovisual data naturally have temporal attributes, such as the time information for each frame in the video. More concretely, such data is inherently multi-modal according to both audio and visual cues, which proceed in a strict chronological order. It indicates that temporal information is important in multi-modal acoustic event modeling for both intra- and inter-modal. However, existing methods deal with each modal feature independently and simply fuse them together, which neglects the mining of temporal relation and thus leads to sub-optimal performance. With this motivation, we propose a Temporal Multi-modal graph learning method for Acoustic event Classification, called TMac, by modeling such temporal information via graph learning techniques. In particular, we construct a temporal graph for each acoustic event, dividing its audio data and video data into multiple segments. Each segment can be considered as a node, and the temporal relationships between nodes can be considered as timestamps on their edges. In this case, we can smoothly capture the dynamic information in intra-modal and inter-modal. Several experiments are conducted to demonstrate TMac outperforms other SOTA models in performance. Our code is available at https://github.com/MGitHubL/TMac.Note: "Simplified Chinese" is a romanization of the Chinese language that uses a simplified set of characters and grammar rules to represent the language in a more phonetic and easier-to-learn format. The translation above is written in Simplified Chinese, but the original text is in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Review-of-Community-Detection-in-Graphs"><a href="#A-Comprehensive-Review-of-Community-Detection-in-Graphs" class="headerlink" title="A Comprehensive Review of Community Detection in Graphs"></a>A Comprehensive Review of Community Detection in Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11798">http://arxiv.org/abs/2309.11798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songlai Ning, Jiakang Li, Yonggang Lu</li>
<li>for: 本文探讨了图像中的社区结构，强调了图像中的社区结构的探讨，以及社区结构的探讨在不同领域的应用。</li>
<li>methods: 本文介绍了多种社区检测方法，包括一种新的方法，并对这些方法进行了详细的介绍。</li>
<li>results: 本文总结了社区检测方法的发展历程，并对社区检测的应用在不同领域进行了探讨。<details>
<summary>Abstract</summary>
The study of complex networks has significantly advanced our understanding of community structures which serves as a crucial feature of real-world graphs. Detecting communities in graphs is a challenging problem with applications in sociology, biology, and computer science. Despite the efforts of an interdisciplinary community of scientists, a satisfactory solution to this problem has not yet been achieved. This review article delves into the topic of community detection in graphs, which serves as a crucial role in understanding the organization and functioning of complex systems. We begin by introducing the concept of community structure, which refers to the arrangement of vertices into clusters, with strong internal connections and weaker connections between clusters. Then, we provide a thorough exposition of various community detection methods, including a new method designed by us. Additionally, we explore real-world applications of community detection in diverse networks. In conclusion, this comprehensive review provides a deep understanding of community detection in graphs. It serves as a valuable resource for researchers and practitioners in multiple disciplines, offering insights into the challenges, methodologies, and applications of community detection in complex networks.
</details>
<details>
<summary>摘要</summary>
学术研究复杂网络已经有了大量的进展，我们对复杂网络中社区结构的理解得到了重要进步。检测复杂网络中的社区是一个具有挑战性的问题，在社会学、生物学和计算机科学等领域都有着广泛的应用。尽管一群来自不同领域的科学家努力奔走，但是满意的解决方案还没有得到。这篇评论文章将探讨复杂网络中的社区检测问题，这是理解复杂系统的组织和运作的关键部分。我们首先介绍社区结构的概念，即顶点的分布到群集中，群集之间的连接较弱。然后，我们对各种社区检测方法进行了详细的介绍，包括我们新提出的方法。此外，我们还探讨了不同网络中社区检测的实际应用。 conclude，这篇评论文章为研究者和实践者在多种领域提供了深入的理解社区检测在复杂网络中的挑战、方法和应用。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Preserving-In-Context-Learning-with-Differentially-Private-Few-Shot-Generation"><a href="#Privacy-Preserving-In-Context-Learning-with-Differentially-Private-Few-Shot-Generation" class="headerlink" title="Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation"></a>Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11765">http://arxiv.org/abs/2309.11765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/dp-few-shot-generation">https://github.com/microsoft/dp-few-shot-generation</a></li>
<li>paper_authors: Xinyu Tang, Richard Shin, Huseyin A. Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Robert Sim</li>
<li>for: 实现隐私对称学习（ICL）大语言模型（LLM）上private dataset。</li>
<li>methods: 提出了一个新的算法，将私人集成的几个示例通过正式数据隐私（DP）保证生成为几个实验示例，并证明了它可以实现有效的ICL。</li>
<li>results: 实验结果显示，我们的算法可以与高度隐私水平相当的实现优异的性能，并且与非私人ICL和零例解决方案相比，具有更好的缩减性和更高的可重用性。<details>
<summary>Abstract</summary>
We study the problem of in-context learning (ICL) with large language models (LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt. We propose a novel algorithm that generates synthetic few-shot demonstrations from the private dataset with formal differential privacy (DP) guarantees, and show empirically that it can achieve effective ICL. We conduct extensive experiments on standard benchmarks and compare our algorithm with non-private ICL and zero-shot solutions. Our results demonstrate that our algorithm can achieve competitive performance with strong privacy levels. These results open up new possibilities for ICL with privacy protection for a broad range of applications.
</details>
<details>
<summary>摘要</summary>
我们研究在私有数据集上使用大语言模型（LLM）进行上下文学习（ICL）问题，这种情况可能会导致隐私泄露或模型重复示例。我们提出了一种新的算法，通过从私有数据集生成几拍示例，并提供正式的差分隐私（DP）保证，以实现有效的ICL。我们进行了广泛的实验，并与非私有ICL和零shot解决方案进行比较。我们的结果表明，我们的算法可以实现竞争性的性能，同时保证高度的隐私水平。这些结果开 up了新的可能性，使得ICL中的隐私保护可以推广到许多应用程序。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Optimal-SDG-Pathways-An-Innovative-Approach-Leveraging-Graph-Pruning-and-Intent-Graph-for-Effective-Recommendations"><a href="#Unveiling-Optimal-SDG-Pathways-An-Innovative-Approach-Leveraging-Graph-Pruning-and-Intent-Graph-for-Effective-Recommendations" class="headerlink" title="Unveiling Optimal SDG Pathways: An Innovative Approach Leveraging Graph Pruning and Intent Graph for Effective Recommendations"></a>Unveiling Optimal SDG Pathways: An Innovative Approach Leveraging Graph Pruning and Intent Graph for Effective Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11741">http://arxiv.org/abs/2309.11741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihang Yu, Shu Wang, Yunqiang Zhu, Wen Yuan, Xiaoliang Dai, Zhiqiang Zou</li>
<li>for: 提出了一种基于用户图 после剪裁和意图图（UGPIG）方法，用于解决现有计算机科学领域推荐算法不能充分考虑地区环境特点和历史互动数据稀缺性的问题，以提高可持续发展模式的推荐效果。</li>
<li>methods: 该方法首先利用了剪裁后用户图的高密度链接能力，解决了推荐算法忽略地区空间不同性的问题。其次，通过建立意图图，capture了目标地区Attributes的偏好，有效地解决了历史互动数据稀缺性问题。</li>
<li>results: 经过广泛的实验，UGPIG方法在可持续发展模式推荐方面比现有的推荐算法（KGCN、KGAT、KGIN）高效，最大提升9.61%。<details>
<summary>Abstract</summary>
The recommendation of appropriate development pathways, also known as ecological civilization patterns for achieving Sustainable Development Goals (namely, sustainable development patterns), are of utmost importance for promoting ecological, economic, social, and resource sustainability in a specific region. To achieve this, the recommendation process must carefully consider the region's natural, environmental, resource, and economic characteristics. However, current recommendation algorithms in the field of computer science fall short in adequately addressing the spatial heterogeneity related to environment and sparsity of regional historical interaction data, which limits their effectiveness in recommending sustainable development patterns. To overcome these challenges, this paper proposes a method called User Graph after Pruning and Intent Graph (UGPIG). Firstly, we utilize the high-density linking capability of the pruned User Graph to address the issue of spatial heterogeneity neglect in recommendation algorithms. Secondly, we construct an Intent Graph by incorporating the intent network, which captures the preferences for attributes including environmental elements of target regions. This approach effectively alleviates the problem of sparse historical interaction data in the region. Through extensive experiments, we demonstrate that UGPIG outperforms state-of-the-art recommendation algorithms like KGCN, KGAT, and KGIN in sustainable development pattern recommendations, with a maximum improvement of 9.61% in Top-3 recommendation performance.
</details>
<details>
<summary>摘要</summary>
“ ecovillage civilization ”模式的建议，即可持续发展目标的实现，对于某地区的生态、经济、社会和资源可持续发展具有极其重要的作用。为了实现这一目标，建议过程应当考虑该地区的自然、环境、资源和经济特点。然而，当前的计算机科学领域的推荐算法尚未能充分考虑地域间的空间不同性和历史互动数据的稀缺性，这限制了它们在可持续发展模式的推荐上的效果。为了解决这些挑战，本文提出了一种方法called User Graph after Pruning and Intent Graph (UGPIG)。首先，我们利用了剪除后的用户图高密度链接能力，解决了推荐算法忽视地域间空间不同性的问题。其次，我们构建了意图图， capture 目标区域的环境元素的偏好。这种方法有效地解决了历史互动数据稀缺性问题。经过广泛的实验，我们证明UGPIG可以比state-of-the-art推荐算法like KGCN、KGAT和KGIN在可持续发展模式的推荐上表现出较高的Top-3推荐性能，最大提升率为9.61%。
</details></li>
</ul>
<hr>
<h2 id="Turaco-Complexity-Guided-Data-Sampling-for-Training-Neural-Surrogates-of-Programs"><a href="#Turaco-Complexity-Guided-Data-Sampling-for-Training-Neural-Surrogates-of-Programs" class="headerlink" title="Turaco: Complexity-Guided Data Sampling for Training Neural Surrogates of Programs"></a>Turaco: Complexity-Guided Data Sampling for Training Neural Surrogates of Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11726">http://arxiv.org/abs/2309.11726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Renda, Yi Ding, Michael Carbin</li>
<li>for: 本研究旨在提供一种方法ология，用于在训练神经网络模型时，从程序的输入空间中采样数据，以优化模型的准确性。</li>
<li>methods: 本研究使用了一种基于程序执行路径的复杂性分析方法，以确定采样数据的比例，并使用神经网络模型来训练代理模型。</li>
<li>results: 实验结果表明，基于复杂性分析的采样方法可以提高模型的准确性，并且在各种真实世界程序中进行了成功应用。<details>
<summary>Abstract</summary>
Programmers and researchers are increasingly developing surrogates of programs, models of a subset of the observable behavior of a given program, to solve a variety of software development challenges. Programmers train surrogates from measurements of the behavior of a program on a dataset of input examples. A key challenge of surrogate construction is determining what training data to use to train a surrogate of a given program.   We present a methodology for sampling datasets to train neural-network-based surrogates of programs. We first characterize the proportion of data to sample from each region of a program's input space (corresponding to different execution paths of the program) based on the complexity of learning a surrogate of the corresponding execution path. We next provide a program analysis to determine the complexity of different paths in a program. We evaluate these results on a range of real-world programs, demonstrating that complexity-guided sampling results in empirical improvements in accuracy.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法来采样培训数据，以训练基于神经网络的代程模型。我们首先量化程序输入空间中每个执行路径的学习复杂性，然后根据复杂性来决定从哪些地方采样数据。接着，我们通过程序分析来确定不同路径的复杂性。我们对多个实际Program进行评估，并证明了复杂性导向采样的实际改进精度。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Core-selecting-Incentive-Mechanism-for-Data-Sharing-in-Federated-Learning"><a href="#Efficient-Core-selecting-Incentive-Mechanism-for-Data-Sharing-in-Federated-Learning" class="headerlink" title="Efficient Core-selecting Incentive Mechanism for Data Sharing in Federated Learning"></a>Efficient Core-selecting Incentive Mechanism for Data Sharing in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11722">http://arxiv.org/abs/2309.11722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengda Ji, Genjiu Xu, Jianjun Ge, Mingqiang Li</li>
<li>for: 这个论文的目的是设计一种激励机制，使参与者输入真实数据，并稳定合作。</li>
<li>methods: 这个论文使用游戏理论的核心概念来设计核心选择机制，并使用放弃方法和采样approximation来降低计算开销。</li>
<li>results: 实验表明，这种高效的核心选择机制可以激励参与者输入高质量数据，并且可以降低计算开销相比之前的核心选择机制。<details>
<summary>Abstract</summary>
Federated learning is a distributed machine learning system that uses participants' data to train an improved global model. In federated learning, participants cooperatively train a global model, and they will receive the global model and payments. Rational participants try to maximize their individual utility, and they will not input their high-quality data truthfully unless they are provided with satisfactory payments based on their data quality. Furthermore, federated learning benefits from the cooperative contributions of participants. Accordingly, how to establish an incentive mechanism that both incentivizes inputting data truthfully and promotes stable cooperation has become an important issue to consider. In this paper, we introduce a data sharing game model for federated learning and employ game-theoretic approaches to design a core-selecting incentive mechanism by utilizing a popular concept in cooperative games, the core. In federated learning, the core can be empty, resulting in the core-selecting mechanism becoming infeasible. To address this, our core-selecting mechanism employs a relaxation method and simultaneously minimizes the benefits of inputting false data for all participants. However, this mechanism is computationally expensive because it requires aggregating exponential models for all possible coalitions, which is infeasible in federated learning. To address this, we propose an efficient core-selecting mechanism based on sampling approximation that only aggregates models on sampled coalitions to approximate the exact result. Extensive experiments verify that the efficient core-selecting mechanism can incentivize inputting high-quality data and stable cooperation, while it reduces computational overhead compared to the core-selecting mechanism.
</details>
<details>
<summary>摘要</summary>
federated learning 是一种分布式机器学习系统，用 particiants' 数据来训练一个改进的全球模型。在 federated learning 中，particiants 合作训练全球模型，并将收到全球模型和支付。理解 particiants 会尽可能地提高自己的个人利益，并不会真实输入高质量数据，除非他们得到满意的支付基于数据质量。此外，federated learning 受到参与者的合作贡献帮助。因此，如何建立一个奖励机制，使 particiants 尽可能地输入真实数据，同时促进稳定合作成为了一个重要的问题。在这篇论文中，我们介绍了一种数据共享游戏模型，并使用游戏理论方法设计核心选择奖励机制。在 federated learning 中，核心可能是空的，这会使核心选择机制成为不可能的。为解决这个问题，我们的核心选择机制使用了松弛方法，同时减少所有参与者输入假数据的收益。然而，这种机制是计算昂贵的，因为它需要对所有可能的联盟进行汇总，这是 federated learning 中不可能完成的。为解决这个问题，我们提出了一种高效的核心选择机制，基于采样approximation，只需对选择的联盟进行汇总，来近似 exact 结果。广泛的实验证明，高效的核心选择机制可以奖励输入高质量数据和稳定合作，同时减少计算负担，相比核心选择机制。
</details></li>
</ul>
<hr>
<h2 id="Quasi-Monte-Carlo-for-3D-Sliced-Wasserstein"><a href="#Quasi-Monte-Carlo-for-3D-Sliced-Wasserstein" class="headerlink" title="Quasi-Monte Carlo for 3D Sliced Wasserstein"></a>Quasi-Monte Carlo for 3D Sliced Wasserstein</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11713">http://arxiv.org/abs/2309.11713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khai Nguyen, Nicola Bariletto, Nhat Ho</li>
<li>for: 这个论文主要是为了提供一种更好的empirical Sliced Wasserstein（SW）Distance的方法，即Quasi-Sliced Wasserstein（QSW）方法，以及一种基于Randomized Quasi-Sliced Wasserstein（RQSW）的随机化版本，用于3D任务。</li>
<li>methods: 这个论文使用了Quasi-Monte Carlo（QMC）方法，包括Gaussian-based mapping、equal area mapping、generalized spiral points和优化犯 COUNT energies等方法来构建QMC点 clouds。此外，为了减少估计误差， authors还提出了一种基于随机化低犯 COUNT sequence的RQSW方法。</li>
<li>results: 该论文通过实验表明，QSW和RQSW方法在3D任务中表现出色，比如点云比较、点云插值、图像风格传输和深度点云自动编码器的训练等。此外， authors还证明了QSW和RQSW方法的 asymptotic convergence和无偏性。<details>
<summary>Abstract</summary>
Monte Carlo (MC) approximation has been used as the standard computation approach for the Sliced Wasserstein (SW) distance, which has an intractable expectation in its analytical form. However, the MC method is not optimal in terms of minimizing the absolute approximation error. To provide a better class of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of QMC for SW, we focus on the 3D setting, specifically computing the SW between probability measures in three dimensions. In greater detail, we empirically verify various ways of constructing QMC points sets on the 3D unit-hypersphere, including Gaussian-based mapping, equal area mapping, generalized spiral points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased estimation for stochastic optimization, we extend QSW into Randomized Quasi-Sliced Wasserstein (RQSW) by introducing randomness to the discussed low-discrepancy sequences. For theoretical properties, we prove the asymptotic convergence of QSW and the unbiasedness of RQSW. Finally, we conduct experiments on various 3D tasks, such as point-cloud comparison, point-cloud interpolation, image style transfer, and training deep point-cloud autoencoders, to demonstrate the favorable performance of the proposed QSW and RQSW variants.
</details>
<details>
<summary>摘要</summary>
蒙特卡洛（MC）方法已经被广泛使用作为水星剖分（SW）距离的标准计算方法，但MC方法不是最优的精度下采样方法。为提供更好的empirical SW，我们提议使用 quasi-水星剖分（QSW）方法，该方法基于 quasi-蒙特卡洛（QMC）方法。在更加详细的3D设置下，我们进行了对QMC方法的广泛研究，包括在3D单位球上构建QMC点集的多种方法，如 Gaussian-based mapping、equal area mapping、generalized spiral points和优化误差能量。此外，为了获得不偏向的优化，我们将QSW扩展为Randomized Quasi-Sliced Wasserstein（RQSW），通过引入随机性来讲谱低误差序列。我们证明了QSW的极限收敛性和RQSW的无偏性。最后，我们在多个3D任务上进行了实验，包括点云比较、点云拟合、图像风格传输和深度点云自动编码器的训练，以示提案的QSW和RQSW变体的报道性能。
</details></li>
</ul>
<hr>
<h2 id="Incentivized-Communication-for-Federated-Bandits"><a href="#Incentivized-Communication-for-Federated-Bandits" class="headerlink" title="Incentivized Communication for Federated Bandits"></a>Incentivized Communication for Federated Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11702">http://arxiv.org/abs/2309.11702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhepei Wei, Chuanhao Li, Haifeng Xu, Hongning Wang</li>
<li>for: 鼓励客户端分享数据，以提高联合学习效率和实际可行性。</li>
<li>methods: 提出了一种奖励客户端分享数据的通信问题，并提出了首个奖励通信协议——Inc-FedUCB，可以在Contextual Linear Setting下实现近似优化的停损 regret。</li>
<li>results: 通过Synthetic和实际数据的广泛实验，证明了提案方法在不同环境下的效果。<details>
<summary>Abstract</summary>
Most existing works on federated bandits take it for granted that all clients are altruistic about sharing their data with the server for the collective good whenever needed. Despite their compelling theoretical guarantee on performance and communication efficiency, this assumption is overly idealistic and oftentimes violated in practice, especially when the algorithm is operated over self-interested clients, who are reluctant to share data without explicit benefits. Negligence of such self-interested behaviors can significantly affect the learning efficiency and even the practical operability of federated bandit learning. In light of this, we aim to spark new insights into this under-explored research area by formally introducing an incentivized communication problem for federated bandits, where the server shall motivate clients to share data by providing incentives. Without loss of generality, we instantiate this bandit problem with the contextual linear setting and propose the first incentivized communication protocol, namely, Inc-FedUCB, that achieves near-optimal regret with provable communication and incentive cost guarantees. Extensive empirical experiments on both synthetic and real-world datasets further validate the effectiveness of the proposed method across various environments.
</details>
<details>
<summary>摘要</summary>
现有大多数聚合强投资的研究假设所有客户端都是积极的分享其数据，以便服务器可以实现共同利益。尽管这种假设具有抽象理论保证的性和通信效率，但在实践中，这种假设经常被违背，特别是当算法在自私的客户端上运行时。不正确地忽略这些自私行为可能会对联合强投资学习的学习效率和实际操作性产生很大的影响。为了解决这个下 lista under-explored 的研究领域，我们希望通过正式地引入一种奖励通信问题，使服务器可以鼓励客户端分享数据，以获得奖励。无论总体来说，我们在Contextual linear setting中实例化这个强投资问题，并提出首个奖励通信协议，即Inc-FedUCB，可以实现近似最佳的 regret  guarantee，同时保证通信和奖励成本的 garantate。经验性实验表明，我们的方法在多种环境下都具有很高的实际效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/21/cs.LG_2023_09_21/" data-id="clnsn0vin00iagf88bp3505uy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/21/eess.IV_2023_09_21/" class="article-date">
  <time datetime="2023-09-21T09:00:00.000Z" itemprop="datePublished">2023-09-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/21/eess.IV_2023_09_21/">eess.IV - 2023-09-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Bloch-Equation-Enables-Physics-informed-Neural-Network-in-Parametric-Magnetic-Resonance-Imaging"><a href="#Bloch-Equation-Enables-Physics-informed-Neural-Network-in-Parametric-Magnetic-Resonance-Imaging" class="headerlink" title="Bloch Equation Enables Physics-informed Neural Network in Parametric Magnetic Resonance Imaging"></a>Bloch Equation Enables Physics-informed Neural Network in Parametric Magnetic Resonance Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11763">http://arxiv.org/abs/2309.11763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingrui Cai, Liuhong Zhu, Jianjun Zhou, Chen Qian, Di Guo, Xiaobo Qu</li>
<li>for: 这个论文的目的是提出一种 Parametric Imaging 方法，可以在临床诊断中提供更多有用的信息。</li>
<li>methods: 这种方法使用 Physically Informed Neural Network (PINN)，其中嵌入了 Bloch 方程，以便学习 T2 参数和生成物理合理的数据。</li>
<li>results: 实验结果表明，这种方法可以在靶体和卡中心 imaging 中提供高度准确的数据，并且不需要大量的训练数据。<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) is an important non-invasive imaging method in clinical diagnosis. Beyond the common image structures, parametric imaging can provide the intrinsic tissue property thus could be used in quantitative evaluation. The emerging deep learning approach provides fast and accurate parameter estimation but still encounters the lack of network interpretation and enough training data. Even with a large amount of training data, the mismatch between the training and target data may introduce errors. Here, we propose one way that solely relies on the target scanned data and does not need a pre-defined training database. We provide a proof-of-concept that embeds the physical rule of MRI, the Bloch equation, into the loss of physics-informed neural network (PINN). PINN enables learning the Bloch equation, estimating the T2 parameter, and generating a series of physically synthetic data. Experimental results are conducted on phantom and cardiac imaging to demonstrate its potential in quantitative MRI.
</details>
<details>
<summary>摘要</summary>
магнитно резонантно изображение (MRI) е важно неинвазивно изображување метод при клиничкој дијагнози. Осим обичних слика структура, параметрично изображување може датирати урођене својства ткива и може бити коришћено за квалитативну оцену. Емерџинг деп леARNING приступ може датирати параметре брзо и тачно, али још увек се сусреће са недостатком интерпретације мреже и достатком тренинг података. Иако са великим количином тренинг података, нескланост између тренинг и мета података може увести грешке. Отуд предлажемо један начин који се искључиво заснива на мети сканираних података и не захтева претходно дефинисану базу података за тренинг. Омогућавамо доказивање да ембедирамо физички закон MRI, Блокова једначина, у губитку физички информиране неуронне мреже (PINN). PINN омогућава учење Блокове једначине, процену параметра T2 и генерисање серије физички симулационих података. Експериментални резултати су извршени на фантазији и кардиоваскуларном изображујуњу да би се демострирало његово потенцијал у квалитативном MRI.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/21/eess.IV_2023_09_21/" data-id="clnsn0vm400t0gf88d97l5tk5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/21/eess.SP_2023_09_21/" class="article-date">
  <time datetime="2023-09-21T08:00:00.000Z" itemprop="datePublished">2023-09-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/21/eess.SP_2023_09_21/">eess.SP - 2023-09-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="RadYOLOLet-Radar-Detection-and-Parameter-Estimation-Using-YOLO-and-WaveLet"><a href="#RadYOLOLet-Radar-Detection-and-Parameter-Estimation-Using-YOLO-and-WaveLet" class="headerlink" title="RadYOLOLet: Radar Detection and Parameter Estimation Using YOLO and WaveLet"></a>RadYOLOLet: Radar Detection and Parameter Estimation Using YOLO and WaveLet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12094">http://arxiv.org/abs/2309.12094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shamik Sarkar, Dongning Guo, Danijela Cabric</li>
<li>for: 这篇论文是为了探讨无助助的激光信号探测，并且可以在未来的共享频率无线网络中实现这一目标。</li>
<li>methods: 这篇论文提出了一个半upervised深度学习基本的频率探测方法，名为RadYOLOLet，可以检测低功率激光信号，并且可以在干扰下进行检测。RadYOLOLet使用了两个不同的卷积神经网（CNN）：RadYOLO和Wavelet-CNN，这两个神经网在独立地训练。</li>
<li>results: 根据论文的评估，RadYOLOLet可以在不同的干扰信号下实现100%的激光信号检测精度，并且可以在干扰信号水准为16 dB SINR下正确地运作。<details>
<summary>Abstract</summary>
Detection of radar signals without assistance from the radar transmitter is a crucial requirement for emerging and future shared-spectrum wireless networks like Citizens Broadband Radio Service (CBRS). In this paper, we propose a supervised deep learning-based spectrum sensing approach called RadYOLOLet that can detect low-power radar signals in the presence of interference and estimate the radar signal parameters. The core of RadYOLOLet is two different convolutional neural networks (CNN), RadYOLO and Wavelet-CNN, that are trained independently. RadYOLO operates on spectrograms and provides most of the capabilities of RadYOLOLet. However, it suffers from low radar detection accuracy in the low signal-to-noise ratio (SNR) regime. We develop Wavelet-CNN specifically to deal with this limitation of RadYOLO. Wavelet-CNN operates on continuous Wavelet transform of the captured signals, and we use it only when RadYOLO fails to detect any radar signal. We thoroughly evaluate RadYOLOLet using different experiments corresponding to different types of interference signals. Based on our evaluations, we find that RadYOLOLet can achieve 100% radar detection accuracy for our considered radar types up to 16 dB SNR, which cannot be guaranteed by other comparable methods. RadYOLOLet can also function accurately under interference up to 16 dB SINR.
</details>
<details>
<summary>摘要</summary>
<<SYS>>这是一篇关于探测无助于激光发射器的激光信号探测方法的论文。我们提出了一个受监督的深度学习基础的探测方法，名为RadYOLOLet，可以在干扰的情况下探测低功率激光信号，并估算激光信号的参数。RadYOLOLet的核心是两个不同的卷积神经网（CNN）：RadYOLO和Wavelet-CNN。它们分别在干扰下和低信号输入下进行训练。RadYOLO在spectrogram上运作，提供了RadYOLOLet的大部分功能。但是，它在低信号输入下的激光探测精度不高。我们为了解决这个问题，开发了Wavelet-CNN，它在缩推 трансформа后的讯号上运作。当RadYOLO无法探测任何激光信号时，我们将Wavelet-CNN使用。我们对RadYOLOLet进行了不同类型的干扰信号的评估，根据我们的评估，RadYOLOLet可以在考虑的激光类型下达到100%的激光探测精度，并且在干扰至16 dB SINR的情况下还能正确运作。
</details></li>
</ul>
<hr>
<h2 id="UAV-Swarm-Deployment-and-Trajectory-for-3D-Area-Coverage-via-Reinforcement-Learning"><a href="#UAV-Swarm-Deployment-and-Trajectory-for-3D-Area-Coverage-via-Reinforcement-Learning" class="headerlink" title="UAV Swarm Deployment and Trajectory for 3D Area Coverage via Reinforcement Learning"></a>UAV Swarm Deployment and Trajectory for 3D Area Coverage via Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11992">http://arxiv.org/abs/2309.11992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia He, Ziye Jia, Chao Dong, Junyu Liu, Qihui Wu, Jingxian Liu</li>
<li>for: 该论文旨在研究无人机群组织和轨迹设计，以实现大规模三维场景中无人机群的无线通信服务。</li>
<li>methods: 该论文提出了层次群组织方案，以高效地服务于大规模用户。然后，问题被转化为最小化无人机群轨迹损失的问题。为解决非凸性问题，我们将其拆分为用户卷积、无人机群悬停点选择和群轨迹决定等子问题。</li>
<li>results: 我们采用Q学习算法加速解决效率。经验表明，我们的方法在其他参照方法中具有更高的效率和可扩展性。<details>
<summary>Abstract</summary>
Unmanned aerial vehicles (UAVs) are recognized as promising technologies for area coverage due to the flexibility and adaptability. However, the ability of a single UAV is limited, and as for the large-scale three-dimensional (3D) scenario, UAV swarms can establish seamless wireless communication services. Hence, in this work, we consider a scenario of UAV swarm deployment and trajectory to satisfy 3D coverage considering the effects of obstacles. In detail, we propose a hierarchical swarm framework to efficiently serve the large-area users. Then, the problem is formulated to minimize the total trajectory loss of the UAV swarm. However, the problem is intractable due to the non-convex property, and we decompose it into smaller issues of users clustering, UAV swarm hovering points selection, and swarm trajectory determination. Moreover, we design a Q-learning based algorithm to accelerate the solution efficiency. Finally, we conduct extensive simulations to verify the proposed mechanisms, and the designed algorithm outperforms other referred methods.
</details>
<details>
<summary>摘要</summary>
无人飞行器（UAV）被认为是有前途的技术，因为它具有灵活性和适应性。然而，单个UAV的能力有限，而在大规模三维（3D）场景中，UAV群可以建立无缝无线通信服务。因此，在这种工作中，我们考虑了UAV群的部署和轨迹，以满足3D覆盖。在详细的描述中，我们提出了层次群组织来有效地服务大面积用户。然后，我们将问题定义为最小化UAV群的总轨迹损失。然而，问题具有非拟合性，我们将其分解为更小的用户团 clustering、UAV群驻留点选择和群轨迹决定。此外，我们设计了Q学习算法来加速解决效率。最后，我们进行了详细的 simulations，并证明了我们的机制和算法的优化性。
</details></li>
</ul>
<hr>
<h2 id="Alteration-of-skeletal-muscle-energy-metabolism-assessed-by-31P-MRS-in-clinical-routine-part-2-Clinical-application"><a href="#Alteration-of-skeletal-muscle-energy-metabolism-assessed-by-31P-MRS-in-clinical-routine-part-2-Clinical-application" class="headerlink" title="Alteration of skeletal muscle energy metabolism assessed by 31P MRS in clinical routine, part 2: Clinical application"></a>Alteration of skeletal muscle energy metabolism assessed by 31P MRS in clinical routine, part 2: Clinical application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11934">http://arxiv.org/abs/2309.11934</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Naëgel, Hélène Ratiney, Jabrane Karkouri, Djahid Kennouche, Nicolas Royer, Jill M Slade, Jérôme Morel, Pierre Croisille, Magalie Viallon</li>
<li>for: 这个研究是为了评估高级质量控制管道在严重急性呼吸综合征和多发性硬化病例中的影响。</li>
<li>methods: 这个研究使用3T临床MRI机器进行31P-MRS测量，并在19名COVID-19患者、38名多发性硬化患者和40名健康控制群体中进行了实验。动态取样使用MR可控ergometer进行了40秒的休息阶段、2分钟的运动阶段和6分钟的恢复阶段。长TR和短TR的获得也在休息阶段进行了T1修正。</li>
<li>results: 应用高级质量控制管道后，研究结果显示了更高的统计力和更改的一些结果值，以及减少了数据变化的SD。COVID-19和多发性硬化患者与健康控制群体之间存在 significante differences，特别是T1PCr和T1Pi的差异。此外，使用固定修正因子导致了系统性地高于使用个体修正因子时的估计PCr和Pi的值。在休息阶段，COVID-19和多发性硬化患者与健康控制群体之间存在 significante differences。在运动阶段，COVID-19患者的动态指标$\tau$PCr、$\tau$Pi、ViPCr和Vmax都比控制群体低。<details>
<summary>Abstract</summary>
Background: In this second part of a two-part paper, we intend to demonstrate the impact of the previously proposed advanced quality control pipeline. To understand its benefit and challenge the proposed methodology in a real scenario, we chose to compare the outcome when applying it to the analysis of two patient populations with a significant but highly different types of fatigue: COVID19 and multiple sclerosis (MS). Experimental: 31P-MRS was performed on a 3T clinical MRI, in 19 COVID19 patients, 38 MS patients, and 40 matched healthy controls. Dynamic acquisitions using an MR-compatible ergometer ran over a rest(40s), exercise(2min), and a recovery phase(6min). Long and short TR acquisitions were also made at rest for T1 correction. The advanced data quality control pipeline presented in part 1 is applied to the selected patient cohorts to investigate its impact on clinical outcomes. We first used power and sample size analysis to estimate objectively the impact of adding QCS. Then, comparisons between patients and healthy control groups using validated QCS were performed using unpaired T-tests or Mann-Whitney tests (p<0.05).Results: The application of the QCS resulted in increased statistical power, changed the values of several outcome measures, and reduced variability (SD). A significant difference was found between the T1PCr and T1Pi of MS patients and healthy controls. Furthermore, the use of a fixed correction factor led to systematically higher estimated concentrations of PCr and Pi than when using individually corrected factors. We observed significant differences between the two patient populations and healthy controls for resting [PCr] -- MS only, [Pi], [ADP], [H2PO4-] and pH -- COVID19 only, and post-exercise [PCr],[Pi] and [H2PO4-] - MS only. The dynamic indicators $\tau$PCr, $\tau$Pi, ViPCr and Vmax were reduced for COVID19 and MS patients compared to controls. Conclusion: Our results show that QCS in dynamic 31P-MRS studies results in smaller data variability and therefore impacts study sample size and power. Although QCS resulted in discarded data and therefore reduced the acceptable data and subject numbers, this rigorous and unbiased approach allowed for proper assessment of muscle metabolites and metabolism in patient populations. The outcomes include an increased metabolite T1, which directly affect the T1 correction factor applied to the amplitudes of the metabolite, and a prolonged $\tau$PCr indicating reduced muscle oxidative capacity for patients with MS and COVID19.
</details>
<details>
<summary>摘要</summary>
Background: 在本文第二部分中，我们想要证明先前提出的高级质量控制管道的影响。为了更好地理解其效果和挑战，我们选择了使用COVID-19和多发性硬化病（MS）两种不同类型的疲劳作为研究对象，并对这两种疲劳的研究结果进行比较。Experimental: 在3T临床MRI机器上，使用31P-MRS技术进行实验，收集了19名COVID-19患者、38名MS患者和40名健康群的数据。在休息（40秒）、运动（2分）和恢复阶段（6分）之间，使用MR可移动ergometer进行动态收集。同时，也进行了长TR和短TR的获取，以便对T1的修正。在第一部分中提出的高级数据质量控制管道被应用于选择的患者群体，以研究它对临床结果的影响。我们首先使用力和样本大小分析来 объекively 评估加入QCS后的影响。然后，通过无对比T检测或曼恩-怀特检测（p<0.05）来比较患者和健康群的数据。Results: QCS的应用导致数据变化的范围变小，因此对研究样本大小和功能有影响。虽然QCS导致了数据抛弃，因此减少了可用数据和参与者数量，但这种坚实和无偏处的方法允许我们在患者群体中正确评估肌肉元素和代谢。结果显示，COVID-19和MS患者的肌肉代谢与健康群有显著差异。特别是，MS患者的T1PCr和T1Pi与健康群有 statistically significant difference。此外，使用固定修正因子导致了系统性地高于使用个体修正因子的估计PCr和Pi的浓度。我们发现COVID-19和MS患者在休息期的 [PCr]、[Pi]、[ADP]、[H2PO4-] 和 pH 中有显著差异。验证了MS患者的恢复期 [PCr]、[Pi] 和 [H2PO4-] 中的差异。同时，COVID-19患者的动态指标 $\tau$PCr、$\tau$Pi、ViPCr 和 Vmax 相比健康群下降。Conclusion: 我们的结果表明，QCS在动态31P-MRS研究中对数据的可靠性和可重复性有积极影响。虽然QCS导致了数据抛弃和样本大小减少，但这种坚实和无偏处的方法允许我们在患者群体中正确评估肌肉元素和代谢。结果包括肌肉T1的增加，直接影响T1修正因子应用于元素的振荡强度，以及COVID-19和MS患者的肌肉代谢减退。
</details></li>
</ul>
<hr>
<h2 id="Index-Modulation-based-Information-Harvesting-for-Far-Field-RF-Power-Transfer"><a href="#Index-Modulation-based-Information-Harvesting-for-Far-Field-RF-Power-Transfer" class="headerlink" title="Index Modulation-based Information Harvesting for Far-Field RF Power Transfer"></a>Index Modulation-based Information Harvesting for Far-Field RF Power Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11929">http://arxiv.org/abs/2309.11929</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Ertug Pihtili, Mehmet C. Ilter, Ertugrul Basar, Risto Wichman, Jyri Hämäläinen</li>
<li>for: 本研究旨在探讨如何使用现有的无线能量传输（WPT）机制来实现无电池通信技术，以满足以 sixth generation 无线通信（6G）时代的互联网物联网（IoT）平台中终端设备的能源储存限制。</li>
<li>methods: 本研究提出了一种新的协议，即信息收集（IH）协议，它利用现有的WPT机制来实现数据通信，并通过在远场传输机制中 incorporate 索引修改（IM）技术来提高数据传输效率。</li>
<li>results: 研究发现，使用IM技术可以在现有的WPT系统中实现数据通信，特别是在下一代IoT无线网络中。results also show that the proposed IH mechanism has the potential to improve the data transmission rate and reduce the power consumption of the system.<details>
<summary>Abstract</summary>
While wireless information transmission (WIT) is evolving into its sixth generation (6G), maintaining terminal operations that rely on limited battery capacities has become one of the most paramount challenges for Internet-of-Things (IoT) platforms. In this respect, there exists a growing interest in energy harvesting technology from ambient resources, and wireless power transfer (WPT) can be the key solution towards enabling battery-less infrastructures referred to as zero-power communication technology. Indeed, eclectic integration approaches between WPT and WIT mechanisms are becoming a vital necessity to limit the need for replacing batteries. Beyond the conventional separation between data and power components of the emitted waveforms, as in simultaneous wireless information and power transfer (SWIPT) mechanisms, a novel protocol referred to as information harvesting (IH) has recently emerged. IH leverages existing WPT mechanisms for data communication by incorporating index modulation (IM) techniques on top of the existing far-field power transfer mechanism. In this paper, a unified framework for the IM-based IH mechanisms has been presented where the feasibility of various IM techniques are evaluated based on different performance metrics. The presented results demonstrate the substantial potential to enable data communication within existing far-field WPT systems, particularly in the context of next-generation IoT wireless networks.
</details>
<details>
<summary>摘要</summary>
sixth generation 无线信息传输 (6G) 中，维护依赖有限电池容量的终端操作已成为互联网物联网 (IoT) 平台上最重要的挑战。在这种情况下，人们对于可在周围环境中汲取能量的技术 Display 增加了兴趣，而无线电力传输 (WPT) 可以成为启用零电池基础设施所需的关键解决方案。此外，在WPT和无线信息传输 (WIT) 机制之间的融合方法也在成为实现零电池通信技术的必要手段。在传统的WIT系统中，数据和电力两个组件通常分别在不同的频率域中进行传输。相比之下，同时进行数据和电力传输的机制被称为同时无线信息和电力传输 (SWIPT) 机制。在这篇论文中，我们提出了一种基于指标修改 (IM) 技术的信息汲取 (IH) 机制的统一框架。我们对不同的 IM 技术进行了评估，并根据不同的性能指标进行评估。结果表明，IH 机制在现有的 far-field WPT 系统中具有潜在的可能性，特别是在下一代 IoT 无线网络中。
</details></li>
</ul>
<hr>
<h2 id="Multi-Passive-Active-IRS-Enhanced-Wireless-Coverage-Deployment-Optimization-and-Cost-Performance-Trade-off"><a href="#Multi-Passive-Active-IRS-Enhanced-Wireless-Coverage-Deployment-Optimization-and-Cost-Performance-Trade-off" class="headerlink" title="Multi-Passive&#x2F;Active-IRS Enhanced Wireless Coverage: Deployment Optimization and Cost-Performance Trade-off"></a>Multi-Passive&#x2F;Active-IRS Enhanced Wireless Coverage: Deployment Optimization and Cost-Performance Trade-off</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11918">http://arxiv.org/abs/2309.11918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Min Fu, Weidong Mei, Rui Zhang<br>for: 这个论文主要关注在增强无线网络覆盖率的问题上，具体来说是通过在复杂环境中部署多个反射器，创建多个阻挡物减少的直线传输链路，以提高无线网络的覆盖率。methods: 该论文使用了多个反射器（PIRS&#x2F;AIRS）和多个天线的基站（BS）在一定区域内进行研究，并对这种多个反射器帮助的无线网络进行了分析和优化。results: 该论文的研究结果表明，使用多个反射器可以在复杂环境中提高无线网络的覆盖率，并且可以通过优化反射器的数量和位置来最大化覆盖率和降低成本。<details>
<summary>Abstract</summary>
Both passive and active intelligent reflecting surfaces (IRSs) can be deployed in complex environments to enhance wireless network coverage by creating multiple blockage-free cascaded line-of-sight (LoS) links. In this paper, we study a multi-passive/active-IRS (PIRS/AIRS) aided wireless network with a multi-antenna base station (BS) in a given region. First, we divide the region into multiple non-overlapping cells, each of which may contain one candidate location that can be deployed with a single PIRS or AIRS. Then, we show several trade-offs between minimizing the total IRS deployment cost and enhancing the signal-to-noise ratio (SNR) performance over all cells via direct/cascaded LoS transmission with the BS. To reconcile these trade-offs, we formulate a joint multi-PIRS/AIRS deployment problem to select an optimal subset of all candidate locations for deploying IRS and also optimize the number of passive/active reflecting elements deployed at each selected location to satisfy a given SNR target over all cells, such that the total deployment cost is minimized. However, due to the combinatorial optimization involved, the formulated problem is difficult to be solved optimally. To tackle this difficulty, we first optimize the reflecting element numbers with given PIRS/AIRS deployed locations via sequential refinement, followed by a partial enumeration to determine the PIRS/AIRS locations. Simulation results show that our proposed algorithm achieves better cost-performance trade-offs than other baseline deployment strategies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>文本翻译为简化字典。 Both passive and active intelligent reflecting surfaces (IRSs) can be deployed in complex environments to enhance wireless network coverage by creating multiple blockage-free cascaded line-of-sight (LoS) links. In this paper, we study a multi-passive/active-IRS (PIRS/AIRS) aided wireless network with a multi-antenna base station (BS) in a given region. First, we divide the region into multiple non-overlapping cells, each of which may contain one candidate location that can be deployed with a single PIRS or AIRS. Then, we show several trade-offs between minimizing the total IRS deployment cost and enhancing the signal-to-noise ratio (SNR) performance over all cells via direct/cascaded LoS transmission with the BS. To reconcile these trade-offs, we formulate a joint multi-PIRS/AIRS deployment problem to select an optimal subset of all candidate locations for deploying IRS and also optimize the number of passive/active reflecting elements deployed at each selected location to satisfy a given SNR target over all cells, such that the total deployment cost is minimized. However, due to the combinatorial optimization involved, the formulated problem is difficult to be solved optimally. To tackle this difficulty, we first optimize the reflecting element numbers with given PIRS/AIRS deployed locations via sequential refinement, followed by a partial enumeration to determine the PIRS/AIRS locations. Simulation results show that our proposed algorithm achieves better cost-performance trade-offs than other baseline deployment strategies.<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="REM-U-net-Deep-Learning-Based-Agile-REM-Prediction-with-Energy-Efficient-Cell-Free-Use-Case"><a href="#REM-U-net-Deep-Learning-Based-Agile-REM-Prediction-with-Energy-Efficient-Cell-Free-Use-Case" class="headerlink" title="REM-U-net: Deep Learning Based Agile REM Prediction with Energy-Efficient Cell-Free Use Case"></a>REM-U-net: Deep Learning Based Agile REM Prediction with Energy-Efficient Cell-Free Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11898">http://arxiv.org/abs/2309.11898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hazem Sallouha, Shamik Sarkar, Enes Krijestorac, Danijela Cabric</li>
<li>for: 本文提出了一种高效的Radio环境地图（REM）预测方法，用于优化无线网络部署、提高网络性能和有效管理频谱资源。</li>
<li>methods: 本文使用了深度学习的u-net结构，并对大规模3D地图数据进行了训练。此外，文章还 investigate了数据处理步骤，以进一步提高REM预测精度。</li>
<li>results: 评估结果显示，提出的方法在2023年IEEE ICASSP Signal Processing Grand Challenge中的First Pathloss Radio Map Prediction Challenge中获得了平均正常化根圆弦误差（RMSE）0.045，并且平均运行时间为14毫秒（ms）。此外，文章还对CF-mMIMO网络中采用最小干扰APSwitch ON&#x2F;OFF策略时的能gy浪费进行了评估。<details>
<summary>Abstract</summary>
Radio environment maps (REMs) hold a central role in optimizing wireless network deployment, enhancing network performance, and ensuring effective spectrum management. Conventional REM prediction methods are either excessively time-consuming, e.g., ray tracing, or inaccurate, e.g., statistical models, limiting their adoption in modern inherently dynamic wireless networks. Deep-learning-based REM prediction has recently attracted considerable attention as an appealing, accurate, and time-efficient alternative. However, existing works on REM prediction using deep learning are either confined to 2D maps or use a limited dataset. In this paper, we introduce a runtime-efficient REM prediction framework based on u-nets, trained on a large-scale 3D maps dataset. In addition, data preprocessing steps are investigated to further refine the REM prediction accuracy. The proposed u-net framework, along with preprocessing steps, are evaluated in the context of the 2023 IEEE ICASSP Signal Processing Grand Challenge, namely, the First Pathloss Radio Map Prediction Challenge. The evaluation results demonstrate that the proposed method achieves an average normalized root-mean-square error (RMSE) of 0.045 with an average of 14 milliseconds (ms) runtime. Finally, we position our achieved REM prediction accuracy in the context of a relevant cell-free massive multiple-input multiple-output (CF-mMIMO) use case. We demonstrate that one can obviate consuming energy on large-scale fading measurements and rely on predicted REM instead to decide on which sleep access points (APs) to switch on in a CF-mMIMO network that adopts a minimum propagation loss AP switch ON/OFF strategy.
</details>
<details>
<summary>摘要</summary>
Radio环境地图（REM）在无线网络部署、提高网络性能和各频谱管理中扮演中心角色。传统的REM预测方法是太时间消耗或不准确，如折线追踪或统计模型，因此它们在现代自然动扩无线网络中得不到广泛的应用。深度学习基于的REM预测在latest years中吸引了广泛的关注，因为它可以提供准确、高效并且有时效的预测。然而，现有的REM预测使用深度学习的研究都是局限于2D地图或使用有限的数据集。在本文中，我们介绍一个高效运行时的REM预测框架，基于u-nets，在大规模3D地图数据集上训练。此外，我们还 investigate了数据预处理步骤，以进一步提高REM预测精度。我们的u-net框架，以及预处理步骤，在2023年IEEE ICASSP Signal Processing Grand Challenge中进行评估，即First Pathloss Radio Map Prediction Challenge。评估结果显示，我们的方法实现了平均正常化根圆方差误差（RMSE）0.045，平均运行时间14毫秒（ms）。最后，我们将我们实现的REM预测精度与相关的cell-free巨量多输入多输出（CF-mMIMO）应用场景进行比较。我们示出，可以不消耗大量能量进行大规模干扰测量，而是依靠预测REM来决定CF-mMIMO网络中的sleepAccess Points（AP）是否需要关机。
</details></li>
</ul>
<hr>
<h2 id="On-the-Performance-Analysis-of-RIS-Empowered-Communications-Over-Nakagami-m-Fading"><a href="#On-the-Performance-Analysis-of-RIS-Empowered-Communications-Over-Nakagami-m-Fading" class="headerlink" title="On the Performance Analysis of RIS-Empowered Communications Over Nakagami-m Fading"></a>On the Performance Analysis of RIS-Empowered Communications Over Nakagami-m Fading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11893">http://arxiv.org/abs/2309.11893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Selimis, Kostas P. Peppas, George C. Alexandropoulos, Fotis I. Lazarakis</li>
<li>for: 研究无线通信透过可程度智能表面（RIS）在 Nakagami-m 抽掉通道上的性能。</li>
<li>methods: 考虑了两种阶段配置设计 для RIS，一种随机的和另一种基于协调相位调节。</li>
<li>results: 提出了单组合表达式 для 失败率和binary modulation scheme的误bit error rate，并提出了精确的关键带宽对应。另外，还提出了简单的分析表达式，它们在大量RIS反射元件时会变为紧密的。<details>
<summary>Abstract</summary>
In this paper, we study the performance of wireless communications empowered by Reconfigurable Intelligent Surface (RISs) over Nakagami-m fading channels. We consider two phase configuration designs for the RIS, one random and another one based on coherent phase shifting. For both phase configuration cases, we present single-integral expressions for the outage probability and the bit error rate of binary modulation schemes, which can be efficiently evaluated numerically. In addition, we propose accurate closed-form approximations for the ergodic capacity of the considered system. For all considered metrics, we have also derived simple analytical expressions that become tight for large numbers of RIS reflecting elements. Numerically evaluated results compared with Monte Carlo simulations are presented in order to verify the correctness of the proposed analysis and showcase the impact of various system settings.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了基于可重配置智能表面（RIS）的无线通信系统在 Nakagami-m 折射通道上的性能。我们考虑了两种阶段配置设计 для RIS，一个是随机的，另一个是基于各相位的干扰。对于两种阶段配置情况，我们提供了单一积分表达式 для 失败率和二进制模ulation 的误差率，可以高效地数值计算。此外，我们提出了准确的闭式表达式 для 耗时容量的耗时容量。对于所有考虑的指标，我们还 deriv了简单的分析表达式，这些表达式在大量 RIS 反射元件时变得紧张。在进行数值计算后，我们对比了 Monte Carlo 仿真结果，以验证我们的分析正确性和系统参数的影响。
</details></li>
</ul>
<hr>
<h2 id="Near-Field-Beam-Training-Joint-Angle-and-Range-Estimation-with-DFT-Codebook"><a href="#Near-Field-Beam-Training-Joint-Angle-and-Range-Estimation-with-DFT-Codebook" class="headerlink" title="Near-Field Beam Training: Joint Angle and Range Estimation with DFT Codebook"></a>Near-Field Beam Training: Joint Angle and Range Estimation with DFT Codebook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11872">http://arxiv.org/abs/2309.11872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xun Wu, Changsheng You, Jiapeng Li, Yunpu Zhang</li>
<li>for: 提高near-field beam training的效率和准确性</li>
<li>methods: 使用DFT codebook进行off-grid范围估计，并提出了两种简单有效的 schemes来联合估计用户角度和范围</li>
<li>results: 对比各种 Referen schemes，numeric simulations显示，提议的方案可以大幅降低near-field beam training的训练负担和提高范围估计精度<details>
<summary>Abstract</summary>
Prior works on near-field beam training have mostly assumed dedicated polar-domain codebook and on-grid range estimation, which, however, may suffer long training overhead and degraded estimation accuracy. To address these issues, we propose in this paper new and efficient beam training schemes with off-grid range estimation by using conventional discrete Fourier transform (DFT) codebook. Specifically, we first analyze the received beam pattern at the user when far-field beamforming vectors are used for beam scanning, and show an interesting result that this beam pattern contains useful user angle and range information. Then, we propose two efficient schemes to jointly estimate the user angle and range with the DFT codebook. The first scheme estimates the user angle based on a defined angular support and resolves the user range by leveraging an approximated angular support width, while the second scheme estimates the user range by minimizing a power ratio mean square error (MSE) to improve the range estimation accuracy. Finally, numerical simulations show that our proposed schemes greatly reduce the near-field beam training overhead and improve the range estimation accuracy as compared to various benchmark schemes.
</details>
<details>
<summary>摘要</summary>
先前的近场ibeam训练工作都主要假设了专门的方位频域代码库和在格rid上的范围估计，这些方法可能会受到长时间的训练开销和质量下降。为了解决这些问题，我们在本纸提出了新的和高效的ibeam训练方案，使用常见的快速傅立叶变换（DFT）代码库进行范围估计。我们首先分析接收到的ibeam Pattern在用户端的情况下，并显示了一个有趣的结果：ibeam Pattern中包含了有用的用户角度和范围信息。然后，我们提出了两种高效的方案，用于同时估计用户角度和范围。第一种方案基于定义的 Angular support 来估计用户角度，然后利用 Approximated angular support width 来解决用户范围；第二种方案则是通过最小化力量比均方差误差（MSE）来提高范围估计精度。最后，我们通过数学仿真显示了我们的提议方案可以减少近场ibeam训练开销，并提高范围估计精度，相比于不同的参考方案。
</details></li>
</ul>
<hr>
<h2 id="Joint-Beamforming-for-RIS-Aided-Full-Duplex-Integrated-Sensing-and-Uplink-Communication"><a href="#Joint-Beamforming-for-RIS-Aided-Full-Duplex-Integrated-Sensing-and-Uplink-Communication" class="headerlink" title="Joint Beamforming for RIS Aided Full-Duplex Integrated Sensing and Uplink Communication"></a>Joint Beamforming for RIS Aided Full-Duplex Integrated Sensing and Uplink Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11850">http://arxiv.org/abs/2309.11850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Guo, Yang Liu, Qingqing Wu, Xin Zeng, Qingjiang Shi</li>
<li>For: This paper focuses on the development of an integrated sensing and communication (ISAC) technology in a full-duplex (FD) uplink communication system.* Methods: The paper employs a reconfigurable intelligent surface (RIS) to improve self-interference (SI) suppression and signal processing gain, and uses convex optimization techniques such as majorization-minimization (MM) and penalty-dual-decomposition (PDD) to optimize joint beamforming, RIS configuration, and mobile users’ power allocation.* Results: Numerical results demonstrate the effectiveness of the proposed solution and the benefits of employing RIS in the FD ISAC system.<details>
<summary>Abstract</summary>
This paper studies integrated sensing and communication (ISAC) technology in a full-duplex (FD) uplink communication system. As opposed to the half-duplex system, where sensing is conducted in a first-emit-then-listen manner, FD ISAC system emits and listens simultaneously and hence conducts uninterrupted target sensing. Besides, impressed by the recently emerging reconfigurable intelligent surface (RIS) technology, we also employ RIS to improve the self-interference (SI) suppression and signal processing gain. As will be seen, the joint beamforming, RIS configuration and mobile users' power allocation is a difficult optimization problem. To resolve this challenge, via leveraging the cutting-the-edge majorization-minimization (MM) and penalty-dual-decomposition (PDD) methods, we develop an iterative solution that optimizes all variables via using convex optimization techniques. Numerical results demonstrate the effectiveness of our proposed solution and the great benefit of employing RIS in the FD ISAC system.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Semi-Supervised-Variational-Inference-over-Nonlinear-Channels"><a href="#Semi-Supervised-Variational-Inference-over-Nonlinear-Channels" class="headerlink" title="Semi-Supervised Variational Inference over Nonlinear Channels"></a>Semi-Supervised Variational Inference over Nonlinear Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11841">http://arxiv.org/abs/2309.11841</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Burshtein, Eli Bery</li>
<li>for: 这篇论文探讨了用深度学习方法传输未知非线性通道，特别是使用 semi-supervised learning 方法来解码这些通道。</li>
<li>methods: 这篇论文使用了 Monte Carlo expectation maximization 和variational autoencoder 等 semi-supervised learning 方法，这些方法可以对几个实验symbols和资料传输几个实验symbols。</li>
<li>results: 根据 abstract，这篇论文获得了最佳的 semi-supervised learning 结果，并且在充分多个资料传输几个实验symbols时，variational autoencoder 也具有较低的错误率，比 meta learning 使用 pilot data 的现在和前一个传输对。<details>
<summary>Abstract</summary>
Deep learning methods for communications over unknown nonlinear channels have attracted considerable interest recently. In this paper, we consider semi-supervised learning methods, which are based on variational inference, for decoding unknown nonlinear channels. These methods, which include Monte Carlo expectation maximization and a variational autoencoder, make efficient use of few pilot symbols and the payload data. The best semi-supervised learning results are achieved with a variational autoencoder. For sufficiently many payload symbols, the variational autoencoder also has lower error rate compared to meta learning that uses the pilot data of the present as well as previous transmission blocks.
</details>
<details>
<summary>摘要</summary>
现在的某些推荐方法已经吸引了很多关注。在这篇论文中，我们考虑了半监督学习方法，它们基于变量推理。这些方法包括Monte Carlo预期最大化和变量自适应器。它们可以高效地使用少量的准则符号和 payload 数据。最佳的半监督学习结果是通过变量自适应器实现。当 payload 符号够多的时候，变量自适应器也比使用现在和前一个传输块的准则数据来学习的meta学习更低的错误率。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Study-of-PAPR-Reduction-Techniques-for-Deep-Joint-Source-Channel-Coding-in-OFDM-Systems"><a href="#A-Comprehensive-Study-of-PAPR-Reduction-Techniques-for-Deep-Joint-Source-Channel-Coding-in-OFDM-Systems" class="headerlink" title="A Comprehensive Study of PAPR Reduction Techniques for Deep Joint Source Channel Coding in OFDM Systems"></a>A Comprehensive Study of PAPR Reduction Techniques for Deep Joint Source Channel Coding in OFDM Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11803">http://arxiv.org/abs/2309.11803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maolin Liu, Wei Chen, Jialong Xu, Bo Ai</li>
<li>for: 这篇论文主要针对的是深度联合源通道编码（DJSCC）系统中的带宽和信号噪比（SNR）下的性能问题。</li>
<li>methods: 本论文使用了多种 orthogonal frequency division multiplexing（OFDM）带宽膨胀率（PAPR）降低技术，包括传统技术如剪辑、压缩、SLM和PTS，以及深度学习基于的PAPR降低技术如PAPR损失和剪辑重训练。</li>
<li>results: 我们的调查表明，虽然传统的PAPR降低技术可以应用于DJSCC，但它们在DJSCC中的表现与传统的分源通道编码不同。此外，我们发现在信号损害PAPR降低技术中，剪辑重训练最佳地降低PAPR并保持信号重建精度。此外，我们发现在信号非损害PAPR降低技术中，可以成功地降低DJSCC中的PAPR无需牺牲信号重建精度。<details>
<summary>Abstract</summary>
Recently, deep joint source channel coding (DJSCC) techniques have been extensively studied and have shown significant performance with limited bandwidth and low signal to noise ratio. Most DJSCC work considers discrete-time analog transmission, while combining it with orthogonal frequency division multiplexing (OFDM) creates serious high peak-to-average power ratio (PAPR) problem. This paper conducts a comprehensive analysis on the use of various OFDM PAPR reduction techniques in the DJSCC system, including both conventional techniques such as clipping, companding, SLM and PTS, and deep learning-based PAPR reduction techniques such as PAPR loss and clipping with retraining. Our investigation shows that although conventional PAPR reduction techniques can be applied to DJSCC, their performance in DJSCC is different from the conventional split source channel coding. Moreover, we observe that for signal distortion PAPR reduction techniques, clipping with retraining achieves the best performance in terms of both PAPR reduction and recovery accuracy. It is also noticed that signal non-distortion PAPR reduction techniques can successfully reduce the PAPR in DJSCC without compromise to signal reconstruction.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:最近，深度 JOINT SOURCE CHANNEL CODING（DJSCC）技术已经广泛研究，并显示了有限带宽和低信号噪声比下的出色表现。大多数DJSCC工作假设了分时分析传输，而将它与分多频分配多重（OFDM）结合使得PAPR问题变得严重。这篇论文对DJSCC系统中OFDM PAPR减少技术的使用进行了全面的分析，包括传统技术 such as clipping, companding, SLM和PTS，以及基于深度学习的PAPR减少技术 such as PAPR损失和clipping重新训练。我们的调查表明，虽然传统PAPR减少技术可以应用于DJSCC，但它们在DJSCC中的性能不同于传统分Split Source Channel Coding。此外，我们还发现，对信号损害PAPR减少技术，clipping重新训练得到了最好的性能，包括PAPR减少和重建精度。此外，我们还发现，对信号非损害PAPR减少技术，可以成功减少DJSCC中的PAPR，无需牺牲信号重建精度。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Circuits-for-Stabilizer-Error-Correcting-Codes-A-Tutorial"><a href="#Quantum-Circuits-for-Stabilizer-Error-Correcting-Codes-A-Tutorial" class="headerlink" title="Quantum Circuits for Stabilizer Error Correcting Codes: A Tutorial"></a>Quantum Circuits for Stabilizer Error Correcting Codes: A Tutorial</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11793">http://arxiv.org/abs/2309.11793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arijit Mondal, Keshab K. Parhi</li>
<li>for: 这篇论文的目的是教育读者设计和仿真量子编码和解码电路，以实现稳定的量子计算。</li>
<li>methods: 论文使用的方法包括设计和仿真五量子码和斯特恩码的编码和解码电路，以及使用IBM Qiskit进行验证。</li>
<li>results: 论文通过设计和仿真电路，实现了五量子码和斯特恩码的编码和解码。此外，论文还提供了邻居相关的编码和解码电路。<details>
<summary>Abstract</summary>
Quantum computers have the potential to provide exponential speedups over their classical counterparts. Quantum principles are being applied to fields such as communications, information processing, and artificial intelligence to achieve quantum advantage. However, quantum bits are extremely noisy and prone to decoherence. Thus, keeping the qubits error free is extremely important toward reliable quantum computing. Quantum error correcting codes have been studied for several decades and methods have been proposed to import classical error correcting codes to the quantum domain. However, circuits for such encoders and decoders haven't been explored in depth. This paper serves as a tutorial on designing and simulating quantum encoder and decoder circuits for stabilizer codes. We present encoding and decoding circuits for five-qubit code and Steane code, along with verification of these circuits using IBM Qiskit. We also provide nearest neighbour compliant encoder and decoder circuits for the five-qubit code.
</details>
<details>
<summary>摘要</summary>
量子计算机有可能提供幂数倍速的加速，应用量子原理到通信、信息处理和人工智能等领域以实现量子优势。然而，量子比特非常易受噪声和破坏，因此保持量子比特错误自由是非常重要的。量子错误修复码已经在数十年内研究，提出了将类型错误修复码引入量子领域的方法。然而，这些圈定器和解码器的电路尚未得到了深入研究。本文为读者提供了设计和实现量子编码器和解码器电路的 tutorials，包括五个量子比特的编码和解码电路以及Steane码的编码和解码电路。此外，我们还提供了邻居相似的编码器和解码器电路。我们使用IBM Qiskit进行验证。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Fault-Identification-Reconstruction-in-Multi-Agent-Systems"><a href="#Collaborative-Fault-Identification-Reconstruction-in-Multi-Agent-Systems" class="headerlink" title="Collaborative Fault-Identification &amp; Reconstruction in Multi-Agent Systems"></a>Collaborative Fault-Identification &amp; Reconstruction in Multi-Agent Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11784">http://arxiv.org/abs/2309.11784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiraz Khan, Inseok Hwang</li>
<li>for: 本研究旨在提出一种高效的分布式逻辑检测、识别和重建（FDIR）机制，适用于多智能体应用。</li>
<li>methods: 该研究基于非线性测量数据进行分布式多智能体FDIR算法的设计，并使用了顺序凸 программирова（SCP）和分布式多智能体优化方法（ADMM）。</li>
<li>results: 提出的分布式多智能体FDIR算法可以处理各种间智能体测量数据（包括距离、方向、相对速度和夹角），并可以识别FAULTY智能体和重建其真实状态。<details>
<summary>Abstract</summary>
The conventional solutions for fault-detection, identification, and reconstruction (FDIR) require centralized decision-making mechanisms which are typically combinatorial in their nature, necessitating the design of an efficient distributed FDIR mechanism that is suitable for multi-agent applications. To this end, we develop a general framework for efficiently reconstructing a sparse vector being observed over a sensor network via nonlinear measurements. The proposed framework is used to design a distributed multi-agent FDIR algorithm based on a combination of the sequential convex programming (SCP) and the alternating direction method of multipliers (ADMM) optimization approaches. The proposed distributed FDIR algorithm can process a variety of inter-agent measurements (including distances, bearings, relative velocities, and subtended angles between agents) to identify the faulty agents and recover their true states. The effectiveness of the proposed distributed multi-agent FDIR approach is demonstrated by considering a numerical example in which the inter-agent distances are used to identify the faulty agents in a multi-agent configuration, as well as reconstruct their error vectors.
</details>
<details>
<summary>摘要</summary>
传统的瑕疵检测、识别和重建（FDIR）解决方案具有中央决策机制，通常是 combinatorial 的性质，需要设计一个高效的分布式 FDIR 机制，适用于多机器人应用。为此，我们提出了一个通用的分布式 sparse vector 重建框架，可以通过非线性测量来重建。基于此框架，我们设计了一种基于 sequential convex programming（SCP）和 alternating direction method of multipliers（ADMM）优化方法的分布式多机器人 FDIR 算法。该算法可以处理多机器人之间的各种测量（包括距离、方向、相对速度和夹角）来识别瑕疵机器人并重建其真实状态。我们通过一个数学示例表明了该分布式多机器人 FDIR 方法的效果。在这个示例中，我们使用了机器人之间的距离来识别瑕疵机器人并重建其错误向量。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-SEFDM-Performance-in-High-Doppler-Channels"><a href="#Enhancing-the-SEFDM-Performance-in-High-Doppler-Channels" class="headerlink" title="Enhancing the SEFDM Performance in High-Doppler Channels"></a>Enhancing the SEFDM Performance in High-Doppler Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11774">http://arxiv.org/abs/2309.11774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahdi Shamsi, Farokh Marvasti</li>
<li>for: 提高移动通信网络中延迟和Doppler偏移的影响管理</li>
<li>methods: 使用Spectrally Efficient Frequency Division Multiplexing (SEFDM)技术，并采用频域预处理和修改非线性加速技术来提高系统性能和 spectral efficiency</li>
<li>results: 在模拟场景中，使用提议方法可以实现可靠和高质量的通信，并且在Doppler频率下保持SEFDM的优于OFDM的spectral efficiency<details>
<summary>Abstract</summary>
In this paper, we propose the use of Spectrally Efficient Frequency Division Multiplexing (SEFDM) with additional techniques such as Frequency Domain Cyclic Prefix (FDCP) and Modified Non-Linear (MNL) acceleration for efficient handling of the impact of delay and Doppler shift in mobile communication channels. Our approach exhibits superior performance and spectral efficiency in comparison to traditional communication systems, while maintaining low computational cost. We study a model of the SEFDM communication system and investigate the impact of MNL acceleration with soft and hard decision Inverse System on the performance of SEFDM detection in the AWGN channel. We also analyze the effectiveness of FDCP in compensating for the impact of Doppler shift, and report BER detection figures using Regularized Sphere Decoding in various simulation scenarios. Our simulations demonstrate that it is possible to achieve acceptable performance in Doppler channels while maintaining the superiority of SEFDM over OFDM in terms of spectral efficiency. The results suggest that our proposed approach can tackle the effects of delay and Doppler shift in mobile communication networks, guaranteeing dependable and high-quality communication even in extremely challenging environments.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提议使用具有频率分配多路复用（SEFDM）技术，并采用频域循环前缀（FDCP）和修改非线性加速（MNL）技术来有效地处理移动通信频道中的延迟和Doppler偏移的影响。我们的方法可以在比较具有传统通信系统的优势下，实现高效的spectral efficiency，同时保持低的计算成本。我们研究了SEFDM通信系统的模型，并研究MNL加速器在半硬件决策和软件决策下对SEFDM检测在AWGN频道的性能的影响。我们还分析了FDCP在补偿Doppler偏移的效果，并在不同的 simulate scenariodemitt BER detection figure using Regularized Sphere Decoding。我们的 simulations表明，可以在Doppler频道中实现可接受的性能，同时保持SEFDM在OFDM方面的 spectral efficiency。结果表明，我们提议的方法可以在移动通信网络中抵消延迟和Doppler偏移的影响，确保高质量和可靠的通信，即使在EXTREMELY challenging environment中。
</details></li>
</ul>
<hr>
<h2 id="Symbol-Detection-for-Coarsely-Quantized-OTFS"><a href="#Symbol-Detection-for-Coarsely-Quantized-OTFS" class="headerlink" title="Symbol Detection for Coarsely Quantized OTFS"></a>Symbol Detection for Coarsely Quantized OTFS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11759">http://arxiv.org/abs/2309.11759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwei He, Haochuan Zhang, Chao Dong, Huimin Zhu</li>
<li>for: 这篇论文专门设计了一种基于对角时域频率空间（OTFS）的差异和杂变量化通信系统，以提高成本和功耗效率。</li>
<li>methods: 论文使用了一种新的估算消息传递（AMP）算法，以及一种改进的普通期望一致的信号恢复（GEC-SR）算法，以解决差异量化导致的频率域频率偏移问题。</li>
<li>results: 论文通过对OTFS系统进行模拟和分析，发现差异量化可以提高成本和功耗效率，但是需要采用低复杂度的算法来实现。<details>
<summary>Abstract</summary>
This paper explicitly models a coarse and noisy quantization in a communication system empowered by orthogonal time frequency space (OTFS) for cost and power efficiency. We first point out, with coarse quantization, the effective channel is imbalanced and thus no longer able to circularly shift the transmitted symbols along the delay-Doppler domain. Meanwhile, the effective channel is non-isotropic, which imposes a significant loss to symbol detection algorithms like the original approximate message passing (AMP). Although the algorithm of generalized expectation consistent for signal recovery (GEC-SR) can mitigate this loss, the complexity in computation is prohibitively high, mainly due to an dramatic increase in the matrix size of OTFS. In this context, we propose a low-complexity algorithm that incorporates into the GEC-SR a quick inversion of quasi-banded matrices, reducing the complexity from a cubic order to a linear order while keeping the performance at the same level.
</details>
<details>
<summary>摘要</summary>
这篇论文显式地模型了一种粗略和噪声量化在基于对角时域频率空间 (OTFS) 的通信系统中，以提高成本和功率效率。我们首先指出，当使用粗略量化时，有效通道变得不均衡，因此不能在延迟-杂散域中循环推移传输的符号。此外，有效通道不均匀，这会对符号检测算法如原始approximate message passing (AMP) 带来重大损失。虽然generalized expectation consistent for signal recovery (GEC-SR) 算法可以减轻这种损失，但计算复杂性过高，主要是因为OTFS 矩阵的维度增加了 exponential。在这种情况下，我们提议一种低复杂性算法，将 GEC-SR 中的快速归一化 quasi-banded 矩阵合并到一起，从 cubic 阶减少到 linear 阶，保持性能水平不变。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Meets-Swarm-Intelligence-for-UAV-Assisted-IoT-Coverage-in-Massive-MIMO"><a href="#Deep-Learning-Meets-Swarm-Intelligence-for-UAV-Assisted-IoT-Coverage-in-Massive-MIMO" class="headerlink" title="Deep Learning Meets Swarm Intelligence for UAV-Assisted IoT Coverage in Massive MIMO"></a>Deep Learning Meets Swarm Intelligence for UAV-Assisted IoT Coverage in Massive MIMO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11748">http://arxiv.org/abs/2309.11748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mobeen Mahmood, MohammadMahdi Ghadaksaz, Asil Koc, Tho Le-Ngoc</li>
<li>for: 这个研究 investigate了一个由无人机（UAV） assisted的多用户多输入多输出（MU-mMIMO）系统，其中UAV acting as a decode-and-forward（DF） relay，以帮助基站（BS）向多个互联网物（IoT）用户传输数据流。</li>
<li>methods: 该研究提出了一个joint优化问题，包括 гибри德束成形（HBF）、UAV relay位置划分和功率分配（PA），以 Maximize the total achievable rate（AR）。研究采用了一个基于geometry的 millimeter-wave（mmWave）通道模型，并提出了三种遗传智能（SI）基本解决方案来优化：1）UAVlocation with equal PA; 2）PA with fixed UAV location; 3）joint PA with UAV deployment。</li>
<li>results: 研究结果显示，提出的算法解决方案可以 achieve higher capacity and reduce average delay for delay-constrained transmissions in a UAV-assisted MU-mMIMO IoT systems。此外，提出的J-HBF-DLLPA可以 closely approach the optimal capacity while significantly reducing the runtime by 99%, which makes the DL-based solution a promising implementation for real-time online applications in UAV-assisted MU-mMIMO IoT systems.<details>
<summary>Abstract</summary>
This study considers a UAV-assisted multi-user massive multiple-input multiple-output (MU-mMIMO) systems, where a decode-and-forward (DF) relay in the form of an unmanned aerial vehicle (UAV) facilitates the transmission of multiple data streams from a base station (BS) to multiple Internet-of-Things (IoT) users. A joint optimization problem of hybrid beamforming (HBF), UAV relay positioning, and power allocation (PA) to multiple IoT users to maximize the total achievable rate (AR) is investigated. The study adopts a geometry-based millimeter-wave (mmWave) channel model for both links and proposes three different swarm intelligence (SI)-based algorithmic solutions to optimize: 1) UAV location with equal PA; 2) PA with fixed UAV location; and 3) joint PA with UAV deployment. The radio frequency (RF) stages are designed to reduce the number of RF chains based on the slow time-varying angular information, while the baseband (BB) stages are designed using the reduced-dimension effective channel matrices. Then, a novel deep learning (DL)-based low-complexity joint hybrid beamforming, UAV location and power allocation optimization scheme (J-HBF-DLLPA) is proposed via fully-connected deep neural network (DNN), consisting of an offline training phase, and an online prediction of UAV location and optimal power values for maximizing the AR. The illustrative results show that the proposed algorithmic solutions can attain higher capacity and reduce average delay for delay-constrained transmissions in a UAV-assisted MU-mMIMO IoT systems. Additionally, the proposed J-HBF-DLLPA can closely approach the optimal capacity while significantly reducing the runtime by 99%, which makes the DL-based solution a promising implementation for real-time online applications in UAV-assisted MU-mMIMO IoT systems.
</details>
<details>
<summary>摘要</summary>
To solve the optimization problem, the study proposes three different swarm intelligence (SI)-based algorithmic solutions:1. UAV location with equal PA2. PA with fixed UAV location3. Joint PA with UAV deploymentThe radio frequency (RF) stages are designed to reduce the number of RF chains based on slow time-varying angular information, while the baseband (BB) stages are designed using reduced-dimension effective channel matrices.Furthermore, a novel deep learning (DL)-based low-complexity joint hybrid beamforming, UAV location, and power allocation optimization scheme (J-HBF-DLLPA) is proposed. The scheme consists of an offline training phase and an online prediction of UAV location and optimal power values for maximizing the AR.The illustrative results show that the proposed algorithmic solutions can achieve higher capacity and reduce average delay for delay-constrained transmissions in a UAV-assisted MU-mMIMO IoT system. Additionally, the proposed J-HBF-DLLPA can closely approach the optimal capacity while significantly reducing the runtime by 99%, making it a promising implementation for real-time online applications in UAV-assisted MU-mMIMO IoT systems.
</details></li>
</ul>
<hr>
<h2 id="Resource-Allocation-for-Semantic-Aware-Mobile-Edge-Computing-Systems"><a href="#Resource-Allocation-for-Semantic-Aware-Mobile-Edge-Computing-Systems" class="headerlink" title="Resource Allocation for Semantic-Aware Mobile Edge Computing Systems"></a>Resource Allocation for Semantic-Aware Mobile Edge Computing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11736">http://arxiv.org/abs/2309.11736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Cang, Ming Chen, Zhaohui Yang, Yuntao Hu, Yinlu Wang, Zhaoyang Zhang, Kai-Kit Wong</li>
<li>for: 提高Mobile Edge Computing（MEC）系统中终端设备（TD）的计算任务执行效率，通过在TD上执行计算任务，并将小型Semantic Information（SI）发送到MEC服务器 instead of large-size raw data。</li>
<li>methods: 提出了一种semantic-aware的共享计算资源分配框架，使用了几何编程将原始非几何问题转化为几何问题，并使用alternating optimization algorithm解决了该问题。</li>
<li>results: 相比benchmark算法，提出的算法可以减少最大执行延迟达37.10%，并且在大任务大小和差 channel conditions下，小型Semantic Extraction Factor（SEF）被首选。<details>
<summary>Abstract</summary>
In this paper, a semantic-aware joint communication and computation resource allocation framework is proposed for mobile edge computing (MEC) systems. In the considered system, each terminal device (TD) has a computation task, which needs to be executed by offloading to the MEC server. To further decrease the transmission burden, each TD sends the small-size extracted semantic information of tasks to the server instead of the large-size raw data. An optimization problem of joint semantic-aware division factor, communication and computation resource management is formulated. The problem aims to minimize the maximum execution delay of all TDs while satisfying energy consumption constraints. The original non-convex problem is transformed into a convex one based on the geometric programming and the optimal solution is obtained by the alternating optimization algorithm. Moreover, the closed-form optimal solution of the semantic extraction factor is derived. Simulation results show that the proposed algorithm yields up to 37.10% delay reduction compared with the benchmark algorithm without semantic-aware allocation. Furthermore, small semantic extraction factors are preferred in the case of large task sizes and poor channel conditions.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，一种基于 semantics 的共享通信和计算资源分配框架被提出用于移动边缘计算（MEC）系统。在考虑系统中，每个终端设备（TD）都有一个计算任务，需要通过下载到 MEC 服务器进行执行。为了进一步减少传输负担，每个 TD 将向服务器发送小型的提取后的semantics信息而不是大量的原始数据。一个优化问题的共享 semantic-aware 分配因子、通信和计算资源管理问题被形ulated。该问题的目标是最小化所有 TD 的执行延迟，同时满足能耗约束。原始的非几何问题被转化为几何问题，并通过幂等优化算法获得了最优解。此外，关闭型优化解的semantic提取因子也得到了解。模拟结果显示，提出的算法可以减少最多37.10%的延迟，相比无semantic-aware分配的参考算法。此外，在大任务大小和 poor 通道条件下，小semantic提取因子被首选。
</details></li>
</ul>
<hr>
<h2 id="A-class-weighted-supervised-contrastive-learning-long-tailed-bearing-fault-diagnosis-approach-using-quadratic-neural-network"><a href="#A-class-weighted-supervised-contrastive-learning-long-tailed-bearing-fault-diagnosis-approach-using-quadratic-neural-network" class="headerlink" title="A class-weighted supervised contrastive learning long-tailed bearing fault diagnosis approach using quadratic neural network"></a>A class-weighted supervised contrastive learning long-tailed bearing fault diagnosis approach using quadratic neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11717">http://arxiv.org/abs/2309.11717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuweien1120/CCQNet">https://github.com/yuweien1120/CCQNet</a></li>
<li>paper_authors: Wei-En Yu, Jinwei Sun, Shiping Zhang, Xiaoge Zhang, Jing-Xiao Liao<br>for:  This paper proposes a supervised contrastive learning approach to enhance the feature extraction capability of neural networks for fault diagnosis in industrial settings, where data is highly imbalanced or long-tailed.methods:  The proposed approach uses a class-aware loss function and a class-weighted contrastive learning quadratic network (CCQNet) consisting of a quadratic convolutional residual network backbone, a contrastive learning branch, and a classifier branch.results:  Experimental results on public and proprietary datasets show that CCQNet outperforms state-of-the-art (SOTA) methods in handling extremely imbalanced data, demonstrating the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
Deep learning has achieved remarkable success in bearing fault diagnosis. However, its performance oftentimes deteriorates when dealing with highly imbalanced or long-tailed data, while such cases are prevalent in industrial settings because fault is a rare event that occurs with an extremely low probability. Conventional data augmentation methods face fundamental limitations due to the scarcity of samples pertaining to the minority class. In this paper, we propose a supervised contrastive learning approach with a class-aware loss function to enhance the feature extraction capability of neural networks for fault diagnosis. The developed class-weighted contrastive learning quadratic network (CCQNet) consists of a quadratic convolutional residual network backbone, a contrastive learning branch utilizing a class-weighted contrastive loss, and a classifier branch employing logit-adjusted cross-entropy loss. By utilizing class-weighted contrastive loss and logit-adjusted cross-entropy loss, our approach encourages equidistant representation of class features, thereby inducing equal attention on all the classes. We further analyze the superior feature extraction ability of quadratic network by establishing the connection between quadratic neurons and autocorrelation in signal processing. Experimental results on public and proprietary datasets are used to validate the effectiveness of CCQNet, and computational results reveal that CCQNet outperforms SOTA methods in handling extremely imbalanced data substantially.
</details>
<details>
<summary>摘要</summary>
深度学习在机器fault diagnosis中取得了非常出色的成果。然而，其性能经常下降在面临高度不均衡或长尾数据时，这些情况在工业Setting中很普遍，因为缺陷是一种非常罕见的事件，发生的概率非常低。传统的数据扩充方法面临基本的限制，因为缺陷类型的样本稀缺。在这篇论文中，我们提出了一种supervised contrastive learning方法，使用类 weighted contrastive loss函数，以提高神经网络的特征提取能力。我们提出的CCQNet模型包括quadratic convolutional residual network底层、contrastive learning分支、类别分支和logit-adjusted cross-entropy loss函数。通过使用类 weighted contrastive loss函数和logit-adjusted cross-entropy loss函数，我们的方法促进了类别特征的恰等表示，从而使得所有类别受到相同的注意。我们还分析了quadratic neuron的超越性，并证明了它们与信号处理中的自相关之间的联系。实验结果表明，CCQNet在处理极端不均衡数据时表现出了显著的优异性，与state-of-the-art方法相比，CCQNet在极端不均衡数据上的表现有所提高。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/21/eess.SP_2023_09_21/" data-id="clnsn0vml00udgf88c7lq4ooe" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/cs.SD_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T15:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/cs.SD_2023_09_20/">cs.SD - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Joint-Minimum-Processing-Beamforming-and-Near-end-Listening-Enhancement"><a href="#Joint-Minimum-Processing-Beamforming-and-Near-end-Listening-Enhancement" class="headerlink" title="Joint Minimum Processing Beamforming and Near-end Listening Enhancement"></a>Joint Minimum Processing Beamforming and Near-end Listening Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11243">http://arxiv.org/abs/2309.11243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas J. Fuglsig, Jesper Jensen, Zheng-Hua Tan, Lars S. Bertelsen, Jens Christian Lindof, Jan Østergaard</li>
<li>for: 这个研究旨在提高受到噪音环境影响的语音识别度，并限制噪音处理量，以确保语音质量不会过度受损。</li>
<li>methods: 这个研究使用了最小处理框架，以减少噪音或增强听力，并保证语音质量不会过度受损。</li>
<li>results: 研究结果显示， JOINT 最小处理框架可以提高语音识别度，并限制噪音处理量，对于有利的噪音情况下，语音质量不会过度受损。<details>
<summary>Abstract</summary>
We consider speech enhancement for signals picked up in one noisy environment that must be rendered to a listener in another noisy environment. For both far-end noise reduction and near-end listening enhancement, it has been shown that excessive focus on noise suppression or intelligibility maximization may lead to excessive speech distortions and quality degradations in favorable noise conditions, where intelligibility is already at ceiling level. Recently [1,2] propose to remedy this with a minimum processing framework that either reduces noise or enhances listening a minimum amount given that a certain intelligibility criterion is still satisfied. Additionally, it has been shown that joint consideration of both environments improves speech enhancement performance. In this paper, we formulate a joint far- and near-end minimum processing framework, that improves intelligibility while limiting speech distortions in favorable noise conditions. We provide closed-form solutions to specific boundary scenarios and investigate performance for the general case using numerical optimization. We also show that concatenating existing minimum processing far- and near-end enhancement methods preserves the effects of the initial methods. Results show that the joint optimization can further improve performance compared to the concatenated approach.
</details>
<details>
<summary>摘要</summary>
我们考虑 speech 增强器在一个噪音环境中捕捉的讯号，需要在另一个噪音环境中呈现给听者。对于距离端噪音抑制和近端听力增强而言，过度强调噪音抑制或智能化最大化可能会导致对于有利的噪音情况下的话语变化和质量下降。最近，[1,2] 提出了一个最小处理框架，可以在保持智能化水平下最小化噪音或增强听力。此外，jointly 考虑两个环境可以提高话语增强表现。在这篇文章中，我们建立了一个共同距离和近端最小处理框架，可以在有利噪音情况下提高智能化水平，并限制话语变化。我们提供了关闭式解的具体情况，并通过数值优化进行探索。我们还证明了 concatenating 现有的最小处理距离和近端增强方法可以保持初始方法的效果。结果显示，共同优化可以进一步提高表现，比 concatenated 方法更好。
</details></li>
</ul>
<hr>
<h2 id="Deep-Complex-U-Net-with-Conformer-for-Audio-Visual-Speech-Enhancement"><a href="#Deep-Complex-U-Net-with-Conformer-for-Audio-Visual-Speech-Enhancement" class="headerlink" title="Deep Complex U-Net with Conformer for Audio-Visual Speech Enhancement"></a>Deep Complex U-Net with Conformer for Audio-Visual Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11059">http://arxiv.org/abs/2309.11059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shafique Ahmed, Chia-Wei Chen, Wenze Ren, Chin-Jou Li, Ernie Chu, Jun-Cheng Chen, Amir Hussain, Hsin-Min Wang, Yu Tsao, Jen-Cheng Hou</li>
<li>for: 提高语音增强系统的性能，使其能够更好地处理语音和视频数据。</li>
<li>methods: 使用深度复杂U-Net架构，并将音频和视频信号进行复杂编码和解码，以及使用嵌入自我注意机制和卷积操作来捕捉语音和视频数据的相互关系。</li>
<li>results: 在COG-MHEAR AVSE Challenge 2023 的基准模型上表现出优于0.14的提升，并在台湾官话语音视频数据集（TMSV）上与状态级模型相当，并且在所有比较模型中表现出最佳result。<details>
<summary>Abstract</summary>
Recent studies have increasingly acknowledged the advantages of incorporating visual data into speech enhancement (SE) systems. In this paper, we introduce a novel audio-visual SE approach, termed DCUC-Net (deep complex U-Net with conformer network). The proposed DCUC-Net leverages complex domain features and a stack of conformer blocks. The encoder and decoder of DCUC-Net are designed using a complex U-Net-based framework. The audio and visual signals are processed using a complex encoder and a ResNet-18 model, respectively. These processed signals are then fused using the conformer blocks and transformed into enhanced speech waveforms via a complex decoder. The conformer blocks consist of a combination of self-attention mechanisms and convolutional operations, enabling DCUC-Net to effectively capture both global and local audio-visual dependencies. Our experimental results demonstrate the effectiveness of DCUC-Net, as it outperforms the baseline model from the COG-MHEAR AVSE Challenge 2023 by a notable margin of 0.14 in terms of PESQ. Additionally, the proposed DCUC-Net performs comparably to a state-of-the-art model and outperforms all other compared models on the Taiwan Mandarin speech with video (TMSV) dataset.
</details>
<details>
<summary>摘要</summary>
近年研究均认可了将视觉数据 integrate 到语音提升（SE）系统中的优势。在这篇论文中，我们介绍了一种新的嵌入式音视频SE方法，称为DCUC-Net（深度复杂U-Net with 准确网络）。我们的DCUC-Net利用复杂Domain特征和一个堆栈的准确块。编码器和解码器都采用了复杂U-Net的框架。音频和视频信号分别通过复杂编码器和ResNet-18模型处理，然后通过准确块进行拼接，并转化为提升后的语音波形。准确块包括自注意机制和卷积操作，使DCUC-Net能够有效地捕捉全局和局部音视频相互依赖关系。我们的实验结果表明，DCUC-Net比基线模型在COG-MHEAR AVSE Challenge 2023中表现出了明显的提升（0.14），并且与当前的状态艺模型相当，在台湾官话语音视频（TMSV）数据集上表现出了最高的性能。
</details></li>
</ul>
<hr>
<h2 id="Ensembling-Multilingual-Pre-Trained-Models-for-Predicting-Multi-Label-Regression-Emotion-Share-from-Speech"><a href="#Ensembling-Multilingual-Pre-Trained-Models-for-Predicting-Multi-Label-Regression-Emotion-Share-from-Speech" class="headerlink" title="Ensembling Multilingual Pre-Trained Models for Predicting Multi-Label Regression Emotion Share from Speech"></a>Ensembling Multilingual Pre-Trained Models for Predicting Multi-Label Regression Emotion Share from Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11014">http://arxiv.org/abs/2309.11014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bagus Tris Atmaja, Akira Sasou</li>
<li>for: 这个论文主要针对的是speech emotion recognition领域的研究和实践应用。</li>
<li>methods: 这个论文使用了ensemble learning方法，将多种预训练模型的结果进行融合，以提高speech emotion recognition的性能。</li>
<li>results: 测试集的spearman correlation coefficient为0.537，开发集的spearman correlation coefficient为0.524，两者都高于之前基于单语言数据的融合方法的研究结果（test集的spearman correlation coefficient为0.476，开发集的spearman correlation coefficient为0.470）。<details>
<summary>Abstract</summary>
Speech emotion recognition has evolved from research to practical applications. Previous studies of emotion recognition from speech have focused on developing models on certain datasets like IEMOCAP. The lack of data in the domain of emotion modeling emerges as a challenge to evaluate models in the other dataset, as well as to evaluate speech emotion recognition models that work in a multilingual setting. This paper proposes an ensemble learning to fuse results of pre-trained models for emotion share recognition from speech. The models were chosen to accommodate multilingual data from English and Spanish. The results show that ensemble learning can improve the performance of the baseline model with a single model and the previous best model from the late fusion. The performance is measured using the Spearman rank correlation coefficient since the task is a regression problem with ranking values. A Spearman rank correlation coefficient of 0.537 is reported for the test set, while for the development set, the score is 0.524. These scores are higher than the previous study of a fusion method from monolingual data, which achieved scores of 0.476 for the test and 0.470 for the development.
</details>
<details>
<summary>摘要</summary>
研究者们在演讲情感识别方面从研究阶段逐渐演化到实际应用。 previous studies on speech emotion recognition have focused on developing models on specific datasets such as IEMOCAP. However, the lack of data in the domain of emotion modeling poses a challenge to evaluate models on other datasets and to evaluate speech emotion recognition models that work in a multilingual setting. This paper proposes an ensemble learning approach to fuse the results of pre-trained models for speech emotion recognition. The models chosen accommodate multilingual data from English and Spanish. The results show that ensemble learning can improve the performance of the baseline model and the previous best model from late fusion. The performance is measured using the Spearman rank correlation coefficient, as the task is a regression problem with ranking values. The reported Spearman rank correlation coefficient for the test set is 0.537, while for the development set, the score is 0.524. These scores are higher than the previous study of a fusion method from monolingual data, which achieved scores of 0.476 for the test and 0.470 for the development.
</details></li>
</ul>
<hr>
<h2 id="Directional-Source-Separation-for-Robust-Speech-Recognition-on-Smart-Glasses"><a href="#Directional-Source-Separation-for-Robust-Speech-Recognition-on-Smart-Glasses" class="headerlink" title="Directional Source Separation for Robust Speech Recognition on Smart Glasses"></a>Directional Source Separation for Robust Speech Recognition on Smart Glasses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10993">http://arxiv.org/abs/2309.10993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiantian Feng, Ju Lin, Yiteng Huang, Weipeng He, Kaustubh Kalgaonkar, Niko Moritz, Li Wan, Xin Lei, Ming Sun, Frank Seide</li>
<li>for: 提高日常交流中语音识别率和说话者检测精度</li>
<li>methods: 使用高级音频感知和机器学习技术实现实时转录和字幕服务，并利用多极麦克风灵活提高语音识别精度</li>
<li>results: irectional source separation 可以提高语音识别率和说话者检测精度，但是对对话伙伴无效。 joint training Directional source separation 和 ASR 模型可以 achieve the best overall ASR performance.<details>
<summary>Abstract</summary>
Modern smart glasses leverage advanced audio sensing and machine learning technologies to offer real-time transcribing and captioning services, considerably enriching human experiences in daily communications. However, such systems frequently encounter challenges related to environmental noises, resulting in degradation to speech recognition and speaker change detection. To improve voice quality, this work investigates directional source separation using the multi-microphone array. We first explore multiple beamformers to assist source separation modeling by strengthening the directional properties of speech signals. In addition to relying on predetermined beamformers, we investigate neural beamforming in multi-channel source separation, demonstrating that automatic learning directional characteristics effectively improves separation quality. We further compare the ASR performance leveraging separated outputs to noisy inputs. Our results show that directional source separation benefits ASR for the wearer but not for the conversation partner. Lastly, we perform the joint training of the directional source separation and ASR model, achieving the best overall ASR performance.
</details>
<details>
<summary>摘要</summary>
现代智能眼镜利用先进的音频感知和机器学习技术，在实时转录和字幕服务方面提供了很大的便利，对日常交流中的人类体验带来了很大的改善。然而，这些系统经常遇到环境噪音的挑战，导致语音识别和发言者变换的干扰。为了提高音质，本工作研究了多频道源分离。我们首先探讨了多种扩声器，以增强对话语音的方向性特性。此外，我们还 investigate了基于自动学习的神经扩声器在多个通道源分离中的应用，并证明了自动学习方向特性可以有效提高分离质量。最后，我们比较了利用分离输出进行ASR的性能和直接使用噪音输入进行ASR的性能，结果表明irectional source separation对ASR有利，但对对话伙伴无效。最后，我们实现了irectional source separation和ASR模型的共同训练，达到了最佳的总ASR性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/cs.SD_2023_09_20/" data-id="clnsn0vk000mhgf882cs57sqi" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/eess.AS_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T14:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/eess.AS_2023_09_20/">eess.AS - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Neural-TTS-System-with-Parallel-Prosody-Transfer-from-Unseen-Speakers"><a href="#A-Neural-TTS-System-with-Parallel-Prosody-Transfer-from-Unseen-Speakers" class="headerlink" title="A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers"></a>A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11487">http://arxiv.org/abs/2309.11487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Slava Shechtman, Raul Fernandez</li>
<li>for: 这个研究的目的是开发一种可以从 parallel text recording 中提取高级别的语音特征，并将其应用于不同的 TTS  voz 中，以实现更加自然和表情充沛的语音读取。</li>
<li>methods: 该研究使用了一种基于神经网络的 TTS 系统，并将其 equiped  avec prosody-control 功能，以便在推理时间对语音输出进行更direct的Shape。</li>
<li>results: 研究表明，该系统可以准确地从新的说话者的 parallel text recording 中提取语音特征，并将其应用于不同的 TTS voz 中，无质量下降，同时保持目标 TTS voz 的identidad，根据一系列主观听力实验的评估。<details>
<summary>Abstract</summary>
Modern neural TTS systems are capable of generating natural and expressive speech when provided with sufficient amounts of training data. Such systems can be equipped with prosody-control functionality, allowing for more direct shaping of the speech output at inference time. In some TTS applications, it may be desirable to have an option that guides the TTS system with an ad-hoc speech recording exemplar to impose an implicit fine-grained, user-preferred prosodic realization for certain input prompts. In this work we present a first-of-its-kind neural TTS system equipped with such functionality to transfer the prosody from a parallel text recording from an unseen speaker. We demonstrate that the proposed system can precisely transfer the speech prosody from novel speakers to various trained TTS voices with no quality degradation, while preserving the target TTS speakers' identity, as evaluated by a set of subjective listening experiments.
</details>
<details>
<summary>摘要</summary>
现代神经网络Text-to-Speech系统可以从充足的训练数据中生成自然和表达力强的语音。这些系统可以搭载受控拍层功能，以更直接在推理时调节语音输出。在某些TTS应用程序中，可能愿意有一个选项，使TTS系统通过额外的即时示例来强制某些输入提示的细腻、用户首选的语音表现。在这种工作中，我们介绍了一种首次实现的神经网络TTS系统，可以将来自未见的说话人的语音特征精确地传递到不同的训练过的TTSvoice中，而无损质量，同时保持目标TTS speaker的身份，根据一组主观听力试验的评价。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/eess.AS_2023_09_20/" data-id="clnsn0vko00ongf88daqm3yt1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/cs.CV_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T13:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/cs.CV_2023_09_20/">cs.CV - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Understanding-Pose-and-Appearance-Disentanglement-in-3D-Human-Pose-Estimation"><a href="#Understanding-Pose-and-Appearance-Disentanglement-in-3D-Human-Pose-Estimation" class="headerlink" title="Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation"></a>Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11667">http://arxiv.org/abs/2309.11667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krishna Kanth Nakka, Mathieu Salzmann</li>
<li>for: 本研究目的是分析当前领域内最新的自然语言描述学习方法是否能够真正分离 pose 信息和 appearance 信息。</li>
<li>methods: 本研究使用了三种当前领域内最新的自然语言描述学习方法进行分析，即 DenseCap, DensePose, 和 H3DNet。</li>
<li>results: 研究发现，这三种方法中的 pose 代码含有显著的 appearance 信息，而且这些方法的分离效果并不够完善。<details>
<summary>Abstract</summary>
As 3D human pose estimation can now be achieved with very high accuracy in the supervised learning scenario, tackling the case where 3D pose annotations are not available has received increasing attention. In particular, several methods have proposed to learn image representations in a self-supervised fashion so as to disentangle the appearance information from the pose one. The methods then only need a small amount of supervised data to train a pose regressor using the pose-related latent vector as input, as it should be free of appearance information. In this paper, we carry out in-depth analysis to understand to what degree the state-of-the-art disentangled representation learning methods truly separate the appearance information from the pose one. First, we study disentanglement from the perspective of the self-supervised network, via diverse image synthesis experiments. Second, we investigate disentanglement with respect to the 3D pose regressor following an adversarial attack perspective. Specifically, we design an adversarial strategy focusing on generating natural appearance changes of the subject, and against which we could expect a disentangled network to be robust. Altogether, our analyses show that disentanglement in the three state-of-the-art disentangled representation learning frameworks if far from complete, and that their pose codes contain significant appearance information. We believe that our approach provides a valuable testbed to evaluate the degree of disentanglement of pose from appearance in self-supervised 3D human pose estimation.
</details>
<details>
<summary>摘要</summary>
As 3D human pose estimation 可以在超级vised learning scenario 中实现非常高的准确率，因此处理没有3D pose annotations的情况 receiving increasing attention. 特别是，several methods have proposed to learn image representations in a self-supervised fashion so as to disentangle the appearance information from the pose one. The methods then only need a small amount of supervised data to train a pose regressor using the pose-related latent vector as input, as it should be free of appearance information.In this paper, we carry out in-depth analysis to understand to what degree the state-of-the-art disentangled representation learning methods truly separate the appearance information from the pose one. First, we study disentanglement from the perspective of the self-supervised network, via diverse image synthesis experiments. Second, we investigate disentanglement with respect to the 3D pose regressor following an adversarial attack perspective. Specifically, we design an adversarial strategy focusing on generating natural appearance changes of the subject, and against which we could expect a disentangled network to be robust.Altogether, our analyses show that disentanglement in the three state-of-the-art disentangled representation learning frameworks is far from complete, and that their pose codes contain significant appearance information. We believe that our approach provides a valuable testbed to evaluate the degree of disentanglement of pose from appearance in self-supervised 3D human pose estimation.
</details></li>
</ul>
<hr>
<h2 id="Neural-Image-Compression-Using-Masked-Sparse-Visual-Representation"><a href="#Neural-Image-Compression-Using-Masked-Sparse-Visual-Representation" class="headerlink" title="Neural Image Compression Using Masked Sparse Visual Representation"></a>Neural Image Compression Using Masked Sparse Visual Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11661">http://arxiv.org/abs/2309.11661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jiang, Wei Wang, Yue Chen</li>
<li>for: 这篇论文主要研究了基于稀疏视觉表示（SVR）的神经图像压缩，目的是提高压缩率和压缩后图像质量。</li>
<li>methods: 这篇论文提出了一种基于SVR的压缩方法，其中图像被嵌入到一个离散的 latent space 中，并使用了学习的视觉codebook来表示图像。在编码器和解码器之间共享 codebook，编码器将图像转换为 integer 代码word indices，并将这些指标传输给解码器进行重建。这种方法提出了一种名为 Masked Adaptive Codebook 的学习方法，可以在bitrate和重建质量之间进行负权补偿。</li>
<li>results: 实验结果表明，M-AdaCode 方法可以在 JPEG-AI 标准数据集上实现更高的压缩率和更高的重建质量，并且可以在不同的传输比特率下进行负权补偿。<details>
<summary>Abstract</summary>
We study neural image compression based on the Sparse Visual Representation (SVR), where images are embedded into a discrete latent space spanned by learned visual codebooks. By sharing codebooks with the decoder, the encoder transfers integer codeword indices that are efficient and cross-platform robust, and the decoder retrieves the embedded latent feature using the indices for reconstruction. Previous SVR-based compression lacks effective mechanism for rate-distortion tradeoffs, where one can only pursue either high reconstruction quality or low transmission bitrate. We propose a Masked Adaptive Codebook learning (M-AdaCode) method that applies masks to the latent feature subspace to balance bitrate and reconstruction quality. A set of semantic-class-dependent basis codebooks are learned, which are weighted combined to generate a rich latent feature for high-quality reconstruction. The combining weights are adaptively derived from each input image, providing fidelity information with additional transmission costs. By masking out unimportant weights in the encoder and recovering them in the decoder, we can trade off reconstruction quality for transmission bits, and the masking rate controls the balance between bitrate and distortion. Experiments over the standard JPEG-AI dataset demonstrate the effectiveness of our M-AdaCode approach.
</details>
<details>
<summary>摘要</summary>
我们研究基于稀疏视觉表示（SVR）的神经网络图像压缩，图像被嵌入到学习的视觉码库中的离散特征空间中。通过在编码器和解码器之间共享码库，编码器将转化为整数编码字符串，这些编码字符串是高效穿梭平台强的和可靠的，而解码器通过这些编码字符串来重建图像。 précédente 的 SVR 基于压缩缺乏有效的Rate-Distortion 质量衡量机制，只能追求高重建质量或低传输比特率。我们提出了一种带有掩码（Mask）的自适应码库学习（M-AdaCode）方法，通过掩码在干扰特征空间中进行权重调整，以实现Rate-Distortion 质量衡量机制。我们学习了基于输入图像的semantic类别的基础码库，并将这些基础码库Weightedly 组合，以生成高质量重建的综合特征。编码器中的掩码将掩蔽不重要的权重，而解码器中的掩码将重新还原这些掩码，以实现Rate-Distortion 质量衡量机制。实验结果表明，我们的 M-AdaCode 方法在标准 JPEG-AI 数据集上表现出色。
</details></li>
</ul>
<hr>
<h2 id="GenLayNeRF-Generalizable-Layered-Representations-with-3D-Model-Alignment-for-Multi-Human-View-Synthesis"><a href="#GenLayNeRF-Generalizable-Layered-Representations-with-3D-Model-Alignment-for-Multi-Human-View-Synthesis" class="headerlink" title="GenLayNeRF: Generalizable Layered Representations with 3D Model Alignment for Multi-Human View Synthesis"></a>GenLayNeRF: Generalizable Layered Representations with 3D Model Alignment for Multi-Human View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11627">http://arxiv.org/abs/2309.11627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Abdelkareem, Shady Shehata, Fakhri Karray</li>
<li>for: 这个研究是为了解决多人Scene中的复杂人物 occlusion 问题，提供一个通用的 layered representation 来捕捉多人Scene 的内容。</li>
<li>methods: 我们提出了一个名为 GenLayNeRF 的方法，它使用一个多层架构来分解Scene，并使用一个新的对策机制来进行适应器调整和多观察特征融合，以确保 pixel-level 的体模型与输入视野的同步。</li>
<li>results: 我们的方法在 NVS 中表现出色，与通用 NeRF 方法相比，它能够在几乎没有预期优化的情况下提供高品质的内容生成。而与层化 per-scene NeRF 方法相比，它能够在几乎没有测试时间优化的情况下提供相似或更好的表现。<details>
<summary>Abstract</summary>
Novel view synthesis (NVS) of multi-human scenes imposes challenges due to the complex inter-human occlusions. Layered representations handle the complexities by dividing the scene into multi-layered radiance fields, however, they are mainly constrained to per-scene optimization making them inefficient. Generalizable human view synthesis methods combine the pre-fitted 3D human meshes with image features to reach generalization, yet they are mainly designed to operate on single-human scenes. Another drawback is the reliance on multi-step optimization techniques for parametric pre-fitting of the 3D body models that suffer from misalignment with the images in sparse view settings causing hallucinations in synthesized views. In this work, we propose, GenLayNeRF, a generalizable layered scene representation for free-viewpoint rendering of multiple human subjects which requires no per-scene optimization and very sparse views as input. We divide the scene into multi-human layers anchored by the 3D body meshes. We then ensure pixel-level alignment of the body models with the input views through a novel end-to-end trainable module that carries out iterative parametric correction coupled with multi-view feature fusion to produce aligned 3D models. For NVS, we extract point-wise image-aligned and human-anchored features which are correlated and fused using self-attention and cross-attention modules. We augment low-level RGB values into the features with an attention-based RGB fusion module. To evaluate our approach, we construct two multi-human view synthesis datasets; DeepMultiSyn and ZJU-MultiHuman. The results indicate that our proposed approach outperforms generalizable and non-human per-scene NeRF methods while performing at par with layered per-scene methods without test time optimization.
</details>
<details>
<summary>摘要</summary>
《 Novel View Synthesis of Multi-Human Scenes with Generalizable Layered Scene Representation》 Multi-human scene novel view synthesis （NVS）面临许多挑战，主要是因为人体 occlusion 复杂。层次表示处理这些复杂性，通过将场景分解为多层Radiance Fields，但是它们主要是基于场景优化，因此效率低。通用人体视图合成方法将预先适应的3D人体模型与图像特征结合起来，但是它们主要是针对单个人体场景设计。另一个缺点是在缺视设定下，使用多步优化技术进行参数预定的3D人体模型会导致投影幻觉。在这种情况下，我们提出了GenLayNeRF，一种通用层次场景表示，用于无需场景优化和非常罕见的视图输入进行自由视角渲染多个人体主题。我们将场景分解成多个人体层，由3D人体模型anchor。然后，我们通过一种新的终端可调模块，通过iterative parametric correction和多视图特征融合来确保像素级匹配3D模型与输入视图。 для NVS，我们提取人体嵌入和图像对齐的点级特征，并使用自注意力和交叉注意力模块进行相关和融合。此外，我们还将低级RGB值加入特征中，使用注意力基于RGB融合模块。为了评估我们的方法，我们建立了两个多个人体视图合成数据集：DeepMultiSyn和ZJU-MultiHuman。结果表明，我们的提出方法在比较通用和非人体场景NeRF方法的同时，能够达到相同的性能水平，而不需要测试时优化。
</details></li>
</ul>
<hr>
<h2 id="Sentence-Attention-Blocks-for-Answer-Grounding"><a href="#Sentence-Attention-Blocks-for-Answer-Grounding" class="headerlink" title="Sentence Attention Blocks for Answer Grounding"></a>Sentence Attention Blocks for Answer Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11593">http://arxiv.org/abs/2309.11593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyedalireza Khoshsirat, Chandra Kambhamettu</li>
<li>for: 本文提出了一种新的建筑块，即 Sentence Attention Block，以解决文本描述答案问题中的问题。</li>
<li>methods: 本文使用了一种已知的注意力方法，并通过小改进，提高了结果。</li>
<li>results: 本文在 TextVQA-X、VQS、VQA-X 和 VizWiz-VQA-Grounding 数据集上达到了状态的最佳准确率。<details>
<summary>Abstract</summary>
Answer grounding is the task of locating relevant visual evidence for the Visual Question Answering task. While a wide variety of attention methods have been introduced for this task, they suffer from the following three problems: designs that do not allow the usage of pre-trained networks and do not benefit from large data pre-training, custom designs that are not based on well-grounded previous designs, therefore limiting the learning power of the network, or complicated designs that make it challenging to re-implement or improve them. In this paper, we propose a novel architectural block, which we term Sentence Attention Block, to solve these problems. The proposed block re-calibrates channel-wise image feature-maps by explicitly modeling inter-dependencies between the image feature-maps and sentence embedding. We visually demonstrate how this block filters out irrelevant feature-maps channels based on sentence embedding. We start our design with a well-known attention method, and by making minor modifications, we improve the results to achieve state-of-the-art accuracy. The flexibility of our method makes it easy to use different pre-trained backbone networks, and its simplicity makes it easy to understand and be re-implemented. We demonstrate the effectiveness of our method on the TextVQA-X, VQS, VQA-X, and VizWiz-VQA-Grounding datasets. We perform multiple ablation studies to show the effectiveness of our design choices.
</details>
<details>
<summary>摘要</summary>
Answer grounding 任务是为Visual Question Answering 任务中找到相关的视觉证据。虽然过去的很多注意力方法被提出，但它们受到以下三个问题的限制：不允许使用预训练网络，不能充分利用大规模预训练数据，或者自定义的设计不基于已有的固定设计，因此限制了网络的学习能力。在这篇论文中，我们提出了一种新的建筑块，我们称之为句子注意力块（Sentence Attention Block），以解决这些问题。我们的块通过显式地模型图像特征地图和句子嵌入的间接关系来重新准确化通道 wise 图像特征地图。我们可视示了该块如何基于句子嵌入来过滤不相关的通道 wise 图像特征地图。我们从一个已知的注意力方法开始，通过小量修改，我们提高了结果，达到了状态之Art accuracy。我们的方法的灵活性使得可以使用不同的预训练后台网络，其简洁性使得容易理解和重新实现。我们在TextVQA-X、VQS、VQA-X 和 VizWiz-VQA-Grounding 数据集上进行了多个缺省研究，以证明我们的设计选择的有效性。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Levels-of-Detail-for-Light-Field-Networks"><a href="#Continuous-Levels-of-Detail-for-Light-Field-Networks" class="headerlink" title="Continuous Levels of Detail for Light Field Networks"></a>Continuous Levels of Detail for Light Field Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11591">http://arxiv.org/abs/2309.11591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AugmentariumLab/continuous-lfn">https://github.com/AugmentariumLab/continuous-lfn</a></li>
<li>paper_authors: David Li, Brandon Y. Feng, Amitabh Varshney</li>
<li>for: 提高rendering效果和资源利用率，通过使用连续多级详细度（LODs）生成神经表示。</li>
<li>methods: 使用权重梯度滤波和重要性 sampling 技术，实现精细控制详细度的调整，以适应不同的 rendering 条件。</li>
<li>results: 提出一种基于连续 LODs 的神经网络表示方法，可以实现进度式流式神经网络表示，降低渲染延迟和资源使用率。<details>
<summary>Abstract</summary>
Recently, several approaches have emerged for generating neural representations with multiple levels of detail (LODs). LODs can improve the rendering by using lower resolutions and smaller model sizes when appropriate. However, existing methods generally focus on a few discrete LODs which suffer from aliasing and flicker artifacts as details are changed and limit their granularity for adapting to resource limitations. In this paper, we propose a method to encode light field networks with continuous LODs, allowing for finely tuned adaptations to rendering conditions. Our training procedure uses summed-area table filtering allowing efficient and continuous filtering at various LODs. Furthermore, we use saliency-based importance sampling which enables our light field networks to distribute their capacity, particularly limited at lower LODs, towards representing the details viewers are most likely to focus on. Incorporating continuous LODs into neural representations enables progressive streaming of neural representations, decreasing the latency and resource utilization for rendering.
</details>
<details>
<summary>摘要</summary>
近些年，多级细节（LOD）生成神经表示方法得到了一些突破。LOD可以通过使用较低的分辨率和小型模型来提高渲染。然而，现有方法通常只关注一些精确的LOD，这会导致抖抖和闪烁artifacts，限制其细节适应资源的变化。在这篇论文中，我们提出了一种使用连续LOD编码光场网络方法，允许为渲染条件进行细化适应。我们的训练过程使用总面积表 filtering，以实现高效的连续filtering在不同LODs。此外，我们使用关注度基于的重要性采样，使我们的光场网络能够更好地分配其容量，特别是在较低的LODs。将连续LODintegrated into神经表示允许进行进程式流动神经表示，降低渲染的延迟和资源利用率。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Kernel-Temporal-Segmentation-as-an-Adaptive-Tokenizer-for-Long-form-Video-Understanding"><a href="#Revisiting-Kernel-Temporal-Segmentation-as-an-Adaptive-Tokenizer-for-Long-form-Video-Understanding" class="headerlink" title="Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding"></a>Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11569">http://arxiv.org/abs/2309.11569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Afham, Satya Narayan Shukla, Omid Poursaeed, Pengchuan Zhang, Ashish Shah, Sernam Lim</li>
<li>for: 提高长视频理解的效果，适应实际视频中的 semantic consistency。</li>
<li>methods: 基于 Kernel Temporal Segmentation (KTS) 的适应 sampling 和 tokenization 方法，不需要任务特定的supervision或固定长度的clip。</li>
<li>results: 在视频分类和 temporal action localization 任务上实现了consistent improvement，并达到了长视频模型的state-of-the-art表现。<details>
<summary>Abstract</summary>
While most modern video understanding models operate on short-range clips, real-world videos are often several minutes long with semantically consistent segments of variable length. A common approach to process long videos is applying a short-form video model over uniformly sampled clips of fixed temporal length and aggregating the outputs. This approach neglects the underlying nature of long videos since fixed-length clips are often redundant or uninformative. In this paper, we aim to provide a generic and adaptive sampling approach for long-form videos in lieu of the de facto uniform sampling. Viewing videos as semantically consistent segments, we formulate a task-agnostic, unsupervised, and scalable approach based on Kernel Temporal Segmentation (KTS) for sampling and tokenizing long videos. We evaluate our method on long-form video understanding tasks such as video classification and temporal action localization, showing consistent gains over existing approaches and achieving state-of-the-art performance on long-form video modeling.
</details>
<details>
<summary>摘要</summary>
当今大多数视频理解模型都是在短范围clip上运行，但实际世界中的视频往往是数分钟长，并且有semantically consistent的分割段。一种常见的方法处理长视频是，将短视频模型应用于固定 temporal length的clip上，并将输出集成。这种方法忽略了长视频的本质，因为固定长clip经常是 redundancy or uninformative。在这篇论文中，我们目的是提供一种通用和适应性的抽样方法，以代替现有的固定抽样。视频被视为semantically consistent的分割段，我们基于Kernel Temporal Segmentation（KTS）提出了一种任务无关、无监督和可扩展的方法，用于抽取和 tokenize 长视频。我们对长视频理解任务，如视频分类和 temporal action localization，进行了评估，并显示了与现有方法相比的consistent提升，并实现了长视频模型的州际性表现。
</details></li>
</ul>
<hr>
<h2 id="A-Large-scale-Dataset-for-Audio-Language-Representation-Learning"><a href="#A-Large-scale-Dataset-for-Audio-Language-Representation-Learning" class="headerlink" title="A Large-scale Dataset for Audio-Language Representation Learning"></a>A Large-scale Dataset for Audio-Language Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11500">http://arxiv.org/abs/2309.11500</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie</li>
<li>for: 这篇论文是为了提出一个新的自动音频描述生成管道，以及构建一个大规模、高质量的音频语言数据集（Auto-ACD）。</li>
<li>methods: 该论文使用了一系列公共工具或 API，自动生成了大量的音频描述文本。</li>
<li>results: 论文通过在不同下游任务上训练 популяр的模型，展示了对 Audio-Language Retrieval、Audio Captioning 和环境分类等任务的性能改进。此外，论文还提出了一个新的测试集，并为音频语言任务提供了一个 referential 平台。<details>
<summary>Abstract</summary>
The AI community has made significant strides in developing powerful foundation models, driven by large-scale multimodal datasets. However, in the audio representation learning community, the present audio-language datasets suffer from limitations such as insufficient volume, simplistic content, and arduous collection procedures. To tackle these challenges, we present an innovative and automatic audio caption generation pipeline based on a series of public tools or APIs, and construct a large-scale, high-quality, audio-language dataset, named as Auto-ACD, comprising over 1.9M audio-text pairs. To demonstrate the effectiveness of the proposed dataset, we train popular models on our dataset and show performance improvement on various downstream tasks, namely, audio-language retrieval, audio captioning, environment classification. In addition, we establish a novel test set and provide a benchmark for audio-text tasks. The proposed dataset will be released at https://auto-acd.github.io/.
</details>
<details>
<summary>摘要</summary>
《人工智能社区在开发强大基础模型方面已经做出了 significiant 进步，这些基础模型得益于大规模多modal数据驱动。然而，在音频表示学术社区中，现有的音频语言数据集受到一些限制，如数据量不足、内容过于简单、收集过程较为繁琐。为了解决这些挑战，我们提出了一种创新的自动音频caption生成管道，基于一系列公共工具或API，并构建了大规模、高质量的音频语言数据集，名为Auto-ACD，包含超过190万个音频文本对。为了证明我们的数据集的效iveness，我们在我们的数据集上训练了popular模型，并在多个下游任务上显示了性能改进，包括音频语言检索、音频captioning、环境分类。此外，我们设立了一个新的测试集，并提供了音频文本任务的benchmark。我们计划在https://auto-acd.github.io/上发布我们的数据集。》Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="FreeU-Free-Lunch-in-Diffusion-U-Net"><a href="#FreeU-Free-Lunch-in-Diffusion-U-Net" class="headerlink" title="FreeU: Free Lunch in Diffusion U-Net"></a>FreeU: Free Lunch in Diffusion U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11497">http://arxiv.org/abs/2309.11497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ChenyangSi/FreeU">https://github.com/ChenyangSi/FreeU</a></li>
<li>paper_authors: Chenyang Si, Ziqi Huang, Yuming Jiang, Ziwei Liu</li>
<li>For: 提高 diffusion U-Net 生成质量，无需额外训练或调整。* Methods: 利用 U-Net 架构的 skip connections 和 backbone feature maps，通过重新权重分配来提高生成质量。* Results: 在图像和视频生成任务中，提出了一种简单 yet effective 的方法 FreeU，可以轻松地与现有的 diffusion 模型结合使用，提高生成质量。<details>
<summary>Abstract</summary>
In this paper, we uncover the untapped potential of diffusion U-Net, which serves as a "free lunch" that substantially improves the generation quality on the fly. We initially investigate the key contributions of the U-Net architecture to the denoising process and identify that its main backbone primarily contributes to denoising, whereas its skip connections mainly introduce high-frequency features into the decoder module, causing the network to overlook the backbone semantics. Capitalizing on this discovery, we propose a simple yet effective method-termed "FreeU" - that enhances generation quality without additional training or finetuning. Our key insight is to strategically re-weight the contributions sourced from the U-Net's skip connections and backbone feature maps, to leverage the strengths of both components of the U-Net architecture. Promising results on image and video generation tasks demonstrate that our FreeU can be readily integrated to existing diffusion models, e.g., Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion, to improve the generation quality with only a few lines of code. All you need is to adjust two scaling factors during inference. Project page: https://chenyangsi.top/FreeU/.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们揭示了扩散U-Net的未利用潜力，它作为一种"免费的午餐"，可以在飞行中显著提高生成质量。我们首先调查扩散U-Net的建筑均衡对减噪过程的关键贡献，并发现其主要脊梁主要做减噪，而跳转连接主要将高频特征引入到解码模块，使网络忽略脊梁 semantics。基于这一发现，我们提出了一种简单 yet effective的方法——FreeU，可以无需额外训练或微调，提高生成质量。我们关键的思路是在扩散U-Net的跳转连接和脊梁特征图之间进行权重调整，以利用扩散U-Net的两个组件之间的优势。 promising results on image and video generation tasks show that our FreeU can be easily integrated into existing diffusion models, such as Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion, to improve the generation quality with only a few lines of code. All you need is to adjust two scaling factors during inference. Project page: <https://chenyangsi.top/FreeU/>.
</details></li>
</ul>
<hr>
<h2 id="Budget-Aware-Pruning-Handling-Multiple-Domains-with-Less-Parameters"><a href="#Budget-Aware-Pruning-Handling-Multiple-Domains-with-Less-Parameters" class="headerlink" title="Budget-Aware Pruning: Handling Multiple Domains with Less Parameters"></a>Budget-Aware Pruning: Handling Multiple Domains with Less Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11464">http://arxiv.org/abs/2309.11464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Felipe dos Santos, Rodrigo Berriel, Thiago Oliveira-Santos, Nicu Sebe, Jurandy Almeida</li>
<li>for: 这个研究的目的是实现多元领域学习（Multi-Domain Learning），即让模型在多个领域中表现良好，并且降低计算成本和模型大小。</li>
<li>methods: 这个研究使用了削除策略来实现模型缩减，即鼓励所有领域使用相同的子集 filters 来构成模型，并将不使用的 filters 削除。</li>
<li>results: 研究获得了与基准模型相似的分类性能，并且降低了计算成本和模型大小。另外，这个方法在资源有限的设备上也能够更好地运行。<details>
<summary>Abstract</summary>
Deep learning has achieved state-of-the-art performance on several computer vision tasks and domains. Nevertheless, it still has a high computational cost and demands a significant amount of parameters. Such requirements hinder the use in resource-limited environments and demand both software and hardware optimization. Another limitation is that deep models are usually specialized into a single domain or task, requiring them to learn and store new parameters for each new one. Multi-Domain Learning (MDL) attempts to solve this problem by learning a single model that is capable of performing well in multiple domains. Nevertheless, the models are usually larger than the baseline for a single domain. This work tackles both of these problems: our objective is to prune models capable of handling multiple domains according to a user-defined budget, making them more computationally affordable while keeping a similar classification performance. We achieve this by encouraging all domains to use a similar subset of filters from the baseline model, up to the amount defined by the user's budget. Then, filters that are not used by any domain are pruned from the network. The proposed approach innovates by better adapting to resource-limited devices while, to our knowledge, being the only work that handles multiple domains at test time with fewer parameters and lower computational complexity than the baseline model for a single domain.
</details>
<details>
<summary>摘要</summary>
深度学习已经在计算机视觉任务和领域上达到了状态对抗性。然而，它仍然具有高的计算成本和需要较多的参数。这些限制使得在资源有限的环境中使用它们变得困难，需要软件和硬件优化。另外，深度模型通常是专门为单个领域或任务设计的，因此它们需要学习和存储每个新领域或任务的新参数。多个领域学习（MDL）尝试解决这个问题，通过学习一个能够在多个领域中表现好的单一模型。然而，这些模型通常比基eline模型更大。本工作解决了这两个问题：我们的目标是使用用户定义的预算来采样和裁剪模型，使其在计算上更加可持预算而仍保持相似的分类性能。我们实现了这一点通过优化所有领域使用基eline模型的相似subset of filters，并且不用于任何领域的筛子被裁剪出去。我们的方法创新在资源有限的设备上更好地适应，并且，至于我们所知道的，是唯一一个在测试时处理多个领域的方法，使用 fewer parameters 和更低的计算复杂度来比基eline模型在单个领域中表现。
</details></li>
</ul>
<hr>
<h2 id="Weight-Averaging-Improves-Knowledge-Distillation-under-Domain-Shift"><a href="#Weight-Averaging-Improves-Knowledge-Distillation-under-Domain-Shift" class="headerlink" title="Weight Averaging Improves Knowledge Distillation under Domain Shift"></a>Weight Averaging Improves Knowledge Distillation under Domain Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11446">http://arxiv.org/abs/2309.11446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vorobeevich/distillation-in-dg">https://github.com/vorobeevich/distillation-in-dg</a></li>
<li>paper_authors: Valeriy Berezovskiy, Nikita Morozov</li>
<li>for: 本研究探讨了知识塑化（KD）技术在不同领域数据上的性能。</li>
<li>methods: 本研究使用了学习 teacher 网络和学生网络，并对学生网络进行了权重平均技术。</li>
<li>results: 研究发现，权重平均技术可以提高知识塑化在不同领域数据上的性能。此外，提出了一种简单的权重平均策略，不需要在训练过程中评估验证数据，并证明其与SWAD和SMA相当。<details>
<summary>Abstract</summary>
Knowledge distillation (KD) is a powerful model compression technique broadly used in practical deep learning applications. It is focused on training a small student network to mimic a larger teacher network. While it is widely known that KD can offer an improvement to student generalization in i.i.d setting, its performance under domain shift, i.e. the performance of student networks on data from domains unseen during training, has received little attention in the literature. In this paper we make a step towards bridging the research fields of knowledge distillation and domain generalization. We show that weight averaging techniques proposed in domain generalization literature, such as SWAD and SMA, also improve the performance of knowledge distillation under domain shift. In addition, we propose a simplistic weight averaging strategy that does not require evaluation on validation data during training and show that it performs on par with SWAD and SMA when applied to KD. We name our final distillation approach Weight-Averaged Knowledge Distillation (WAKD).
</details>
<details>
<summary>摘要</summary>
知识塑化（KD）是一种广泛应用在深度学习实践中的模型压缩技术。它关注训练一个小学生网络，以模仿一个更大的教师网络。虽然广泛认知KD可以提高学生网络在同一个分布下的泛化性能，但它在领域转移情况下的性能尚未得到了文献的充分关注。在这篇论文中，我们尝试将知识塑化和领域总结两个领域联系起来。我们表明了在领域转移情况下使用Weight averaging技术，如SWAD和SMA，可以提高知识塑化的性能。此外，我们还提出了一种简单的Weight averaging策略，不需要在训练过程中评估验证数据，并证明它与SWAD和SMA在KD中具有相同的性能。我们将这种最终塑化方法称为Weight-Averaged Knowledge Distillation（WAKD）。
</details></li>
</ul>
<hr>
<h2 id="SkeleTR-Towrads-Skeleton-based-Action-Recognition-in-the-Wild"><a href="#SkeleTR-Towrads-Skeleton-based-Action-Recognition-in-the-Wild" class="headerlink" title="SkeleTR: Towrads Skeleton-based Action Recognition in the Wild"></a>SkeleTR: Towrads Skeleton-based Action Recognition in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11445">http://arxiv.org/abs/2309.11445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haodong Duan, Mingze Xu, Bing Shuai, Davide Modolo, Zhuowen Tu, Joseph Tighe, Alessandro Bergamo</li>
<li>for: 本文 targets more general scenarios of action recognition, such as variable number of people and various forms of interaction.</li>
<li>methods: 方法使用 two-stage paradigm，首先使用图 convolutions 模型每个人的内部动作动态，然后使用堆式 transformer encoder 捕捉人之间的交互。</li>
<li>results: 对多种 skeleton-based action recognition 任务进行了全面的解决，包括视频级动作分类、实例级动作检测和群体活动识别。实现了 transfer learning 和共同训练 across different action tasks and datasets，并且在多个 benchmark 上达到了 state-of-the-art 性能。<details>
<summary>Abstract</summary>
We present SkeleTR, a new framework for skeleton-based action recognition. In contrast to prior work, which focuses mainly on controlled environments, we target more general scenarios that typically involve a variable number of people and various forms of interaction between people. SkeleTR works with a two-stage paradigm. It first models the intra-person skeleton dynamics for each skeleton sequence with graph convolutions, and then uses stacked Transformer encoders to capture person interactions that are important for action recognition in general scenarios. To mitigate the negative impact of inaccurate skeleton associations, SkeleTR takes relative short skeleton sequences as input and increases the number of sequences. As a unified solution, SkeleTR can be directly applied to multiple skeleton-based action tasks, including video-level action classification, instance-level action detection, and group-level activity recognition. It also enables transfer learning and joint training across different action tasks and datasets, which result in performance improvement. When evaluated on various skeleton-based action recognition benchmarks, SkeleTR achieves the state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
我们提出了SkeleTR，一个新的骨架基于动作识别框架。与先前的工作不同，SkeleTR针对更加一般的场景，通常包括变量数量的人员和人员之间多种互动。SkeleTR采用两stage架构，首先使用图 convolution 模型每个骨sequences的内部动作动态，然后使用堆式 transformer 编码器捕捉人员之间重要的动作识别。为了减轻不准确的骨 association 的影响，SkeleTR 使用短skeleton sequence 作为输入，并增加输入序列的数量。作为一个通用解决方案，SkeleTR 可以直接应用于多种骨基于动作任务，包括视频级动作分类、实例级动作检测和群体活动识别。它还允许转移学习和共同训练不同的动作任务和数据集，从而提高性能。在多种骨基于动作识别 benchmark 上评估，SkeleTR 实现了状态的极佳表现。
</details></li>
</ul>
<hr>
<h2 id="Signature-Activation-A-Sparse-Signal-View-for-Holistic-Saliency"><a href="#Signature-Activation-A-Sparse-Signal-View-for-Holistic-Saliency" class="headerlink" title="Signature Activation: A Sparse Signal View for Holistic Saliency"></a>Signature Activation: A Sparse Signal View for Holistic Saliency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11443">http://arxiv.org/abs/2309.11443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dtak/signature-activation">https://github.com/dtak/signature-activation</a></li>
<li>paper_authors: Jose Roberto Tello Ayala, Akl C. Fahed, Weiwei Pan, Eugene V. Pomerantsev, Patrick T. Ellinor, Anthony Philippakis, Finale Doshi-Velez</li>
<li>for: 这篇论文的目的是提出一种基于Machine Learning的医疗图像处理方法，以提高医疗图像识别的透明度和解释性。</li>
<li>methods: 本文引入了Signature Activation，一种可靠性方法，它可以对于卷积神经网络（CNN）的输出生成整体和无关对象的解释。本方法基于医疗图像中的前景和背景物件之间的显著差异。</li>
<li>results: 本文透过评估 coronary angiogram 中的病变检测，证明了 Signature Activation 的可靠性和有用性。<details>
<summary>Abstract</summary>
The adoption of machine learning in healthcare calls for model transparency and explainability. In this work, we introduce Signature Activation, a saliency method that generates holistic and class-agnostic explanations for Convolutional Neural Network (CNN) outputs. Our method exploits the fact that certain kinds of medical images, such as angiograms, have clear foreground and background objects. We give theoretical explanation to justify our methods. We show the potential use of our method in clinical settings through evaluating its efficacy for aiding the detection of lesions in coronary angiograms.
</details>
<details>
<summary>摘要</summary>
《机器学习在医疗领域的应用需要模型的透明度和解释性》。在这项工作中，我们介绍了《签名活化》，一种可以生成整体和无类别的解释方法，用于 convolutional neural network（CNN）输出。我们的方法利用了某些医疗图像，如血管agram，具有明确的前景和背景对象。我们给出了理论解释，以便证明我们的方法。我们通过评估其在 coronary angiogram 中的可用性，显示了我们的方法在临床应用中的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="CalibFPA-A-Focal-Plane-Array-Imaging-System-based-on-Online-Deep-Learning-Calibration"><a href="#CalibFPA-A-Focal-Plane-Array-Imaging-System-based-on-Online-Deep-Learning-Calibration" class="headerlink" title="CalibFPA: A Focal Plane Array Imaging System based on Online Deep-Learning Calibration"></a>CalibFPA: A Focal Plane Array Imaging System based on Online Deep-Learning Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11421">http://arxiv.org/abs/2309.11421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alper Güngör, M. Umut Bahceci, Yasin Ergen, Ahmet Sözak, O. Oner Ekiz, Tolga Yelboga, Tolga Çukur</li>
<li>for: 这个论文的目的是提出一种基于深度学习的压缩镜头数组系统（CalibFPA），以实现高分辨率（HR）成像，并且不需要线上准备。</li>
<li>methods: 该系统使用了电子控制的空间光模ulators（SLM）进行多重编码，并使用了一个深度学习网络来在多个LR测量中 correect 系统不良的影响。</li>
<li>results: 在模拟和实验数据上，CalibFPA的性能超过了现有的压缩镜头数组方法，并且进行了系统元素的分析和计算复杂度的评估。<details>
<summary>Abstract</summary>
Compressive focal plane arrays (FPA) enable cost-effective high-resolution (HR) imaging by acquisition of several multiplexed measurements on a low-resolution (LR) sensor. Multiplexed encoding of the visual scene is typically performed via electronically controllable spatial light modulators (SLM). An HR image is then reconstructed from the encoded measurements by solving an inverse problem that involves the forward model of the imaging system. To capture system non-idealities such as optical aberrations, a mainstream approach is to conduct an offline calibration scan to measure the system response for a point source at each spatial location on the imaging grid. However, it is challenging to run calibration scans when using structured SLMs as they cannot encode individual grid locations. In this study, we propose a novel compressive FPA system based on online deep-learning calibration of multiplexed LR measurements (CalibFPA). We introduce a piezo-stage that locomotes a pre-printed fixed coded aperture. A deep neural network is then leveraged to correct for the influences of system non-idealities in multiplexed measurements without the need for offline calibration scans. Finally, a deep plug-and-play algorithm is used to reconstruct images from corrected measurements. On simulated and experimental datasets, we demonstrate that CalibFPA outperforms state-of-the-art compressive FPA methods. We also report analyses to validate the design elements in CalibFPA and assess computational complexity.
</details>
<details>
<summary>摘要</summary>
高度压缩的投影平面阵列（FPA）可以实现低成本高分辨率（HR）成像，通过多个多样化测量在低分辨率（LR）感知器上。多样化编码视场通常通过电子控制可变光学模拟器（SLM）进行。然后，从编码测量中重建HR图像，通过解决一个反射问题，该问题涉及到成像系统的前向模型。但是，使用结构化SLM时难以进行线上准备扫描，以便测量系统响应点源在每个空间位置上。在本研究中，我们提出了一种新的压缩FPA系统，基于在线深度学习准备多样化LR测量（CalibFPA）。我们引入了一个 piezo 阶段，使得预制印刷的固定编码窗口在不同的空间位置上移动。然后，我们利用了深度神经网络来纠正多样化测量中系统非理想的影响，无需进行线上准备扫描。最后，我们使用了深度插件播客算法来重建图像。在模拟和实验数据集上，我们证明了CalibFPA的性能比现有压缩FPA方法更高。我们还进行了分析，以验证设计元素的合理性和计算复杂性。
</details></li>
</ul>
<hr>
<h2 id="CNNs-for-JPEGs-A-Study-in-Computational-Cost"><a href="#CNNs-for-JPEGs-A-Study-in-Computational-Cost" class="headerlink" title="CNNs for JPEGs: A Study in Computational Cost"></a>CNNs for JPEGs: A Study in Computational Cost</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11417">http://arxiv.org/abs/2309.11417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Felipe dos Santos, Nicu Sebe, Jurandy Almeida</li>
<li>for: 本文旨在研究频域预处理后的深度学习模型，以优化计算成本和参数数量。</li>
<li>methods: 本文使用了DCT频域表示法，并对传统 CNN 架构进行了修改，以适应频域数据。</li>
<li>results: 本文提出了一些手动和数据驱动的技术来降低计算成本和参数数量，以实现高效且精准的频域深度学习模型。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have achieved astonishing advances over the past decade, defining state-of-the-art in several computer vision tasks. CNNs are capable of learning robust representations of the data directly from the RGB pixels. However, most image data are usually available in compressed format, from which the JPEG is the most widely used due to transmission and storage purposes demanding a preliminary decoding process that have a high computational load and memory usage. For this reason, deep learning methods capable of learning directly from the compressed domain have been gaining attention in recent years. Those methods usually extract a frequency domain representation of the image, like DCT, by a partial decoding, and then make adaptation to typical CNNs architectures to work with them. One limitation of these current works is that, in order to accommodate the frequency domain data, the modifications made to the original model increase significantly their amount of parameters and computational complexity. On one hand, the methods have faster preprocessing, since the cost of fully decoding the images is avoided, but on the other hand, the cost of passing the images though the model is increased, mitigating the possible upside of accelerating the method. In this paper, we propose a further study of the computational cost of deep models designed for the frequency domain, evaluating the cost of decoding and passing the images through the network. We also propose handcrafted and data-driven techniques for reducing the computational complexity and the number of parameters for these models in order to keep them similar to their RGB baselines, leading to efficient models with a better trade off between computational cost and accuracy.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在过去的一代时间内取得了非常的进步，在计算机视觉任务中定义了状态的艺术。CNN可以直接从RGB像素上学习坚实的数据表示。然而，大多数图像数据通常是压缩形式，JPEG是最常用的，因为传输和存储目的需要高计算负担和内存使用。为此，可以直接从压缩领域学习深度学习方法在过去几年内得到了关注。这些方法通常提取图像的频率频谱表示，例如DCT，通过部分解码，然后将其与传统的CNN架构进行适应。现有的方法的一个限制是，为了适应频率频谱数据，模型的修改会增加显著。一方面，预处理更快，因为完全解码图像的成本被避免了，但另一方面，通过网络传输图像的成本增加，这可能导致加速方法的可能性减退。在这篇论文中，我们将进一步研究深度模型在频率频谱频谱上的计算成本，以及图像传输和网络传输的成本。我们还将提出手工和数据驱动的技术，以减少模型的计算复杂性和参数数量，以保持与RGB基eline相似的效率，从而实现更好的计算成本和准确性的负担平衡。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-motion-trajectory-segmentation-of-rigid-bodies-using-a-novel-screw-based-trajectory-shape-representation"><a href="#Enhancing-motion-trajectory-segmentation-of-rigid-bodies-using-a-novel-screw-based-trajectory-shape-representation" class="headerlink" title="Enhancing motion trajectory segmentation of rigid bodies using a novel screw-based trajectory-shape representation"></a>Enhancing motion trajectory segmentation of rigid bodies using a novel screw-based trajectory-shape representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11413">http://arxiv.org/abs/2309.11413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arno Verduyn, Maxim Vochten, Joris De Schutter</li>
<li>for: 这篇论文主要针对3D固体运动的轨迹分割。</li>
<li>methods: 该论文提出了一种新的轨迹表示方法，它包括一个 геометрический进度率和一个第三阶轨迹形态描述器，并且具有一些惯性性和参考点无关性的特点。</li>
<li>results: 该论文使用自我监督分割方法进行验证，在实验和真实的人类斟 Pouring 动作记录中表现出更加稳定和一致的分割结果，与传统表示方法相比。<details>
<summary>Abstract</summary>
Trajectory segmentation refers to dividing a trajectory into meaningful consecutive sub-trajectories. This paper focuses on trajectory segmentation for 3D rigid-body motions. Most segmentation approaches in the literature represent the body's trajectory as a point trajectory, considering only its translation and neglecting its rotation. We propose a novel trajectory representation for rigid-body motions that incorporates both translation and rotation, and additionally exhibits several invariant properties. This representation consists of a geometric progress rate and a third-order trajectory-shape descriptor. Concepts from screw theory were used to make this representation time-invariant and also invariant to the choice of body reference point. This new representation is validated for a self-supervised segmentation approach, both in simulation and using real recordings of human-demonstrated pouring motions. The results show a more robust detection of consecutive submotions with distinct features and a more consistent segmentation compared to conventional representations. We believe that other existing segmentation methods may benefit from using this trajectory representation to improve their invariance.
</details>
<details>
<summary>摘要</summary>
准确地描述行走过程的分段是指将行走过程分解成有意义的连续子过程。这篇论文关注于三维固定体运动的轨迹分段。大多数文献中的分段方法只考虑体的翻译和忽略其旋转。我们提出了一种新的轨迹表示方法，该方法包括一个 геометрический进度率和一个第三阶轨迹形态描述器。我们使用了滚筒理论来使这种表示方法时间不变和参照点无关。这种新的表示方法在自主监督分段方法中得到验证，包括在模拟和真实的人类倒 Pouring 动作记录中。结果显示，使用这种轨迹表示方法可以更好地检测出不同特征的连续子过程，并且比传统表示方法更加一致。我们认为其他现有的分段方法可能会从这种轨迹表示方法中受益，以提高其对称性。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-learning-unveils-change-in-urban-housing-from-street-level-images"><a href="#Self-supervised-learning-unveils-change-in-urban-housing-from-street-level-images" class="headerlink" title="Self-supervised learning unveils change in urban housing from street-level images"></a>Self-supervised learning unveils change in urban housing from street-level images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11354">http://arxiv.org/abs/2309.11354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Stalder, Michele Volpi, Nicolas Büttner, Stephen Law, Kenneth Harttgen, Esra Suel</li>
<li>for:  tracks progress in urban housing, specifically in London’s housing supply</li>
<li>methods:  uses deep learning-based computer vision methods and self-supervised techniques to measure change in street-level images</li>
<li>results:  successfully identified point-level change in London’s housing supply and distinguished between major and minor change, providing timely information for urban planning and policy decisions.<details>
<summary>Abstract</summary>
Cities around the world face a critical shortage of affordable and decent housing. Despite its critical importance for policy, our ability to effectively monitor and track progress in urban housing is limited. Deep learning-based computer vision methods applied to street-level images have been successful in the measurement of socioeconomic and environmental inequalities but did not fully utilize temporal images to track urban change as time-varying labels are often unavailable. We used self-supervised methods to measure change in London using 15 million street images taken between 2008 and 2021. Our novel adaptation of Barlow Twins, Street2Vec, embeds urban structure while being invariant to seasonal and daily changes without manual annotations. It outperformed generic embeddings, successfully identified point-level change in London's housing supply from street-level images, and distinguished between major and minor change. This capability can provide timely information for urban planning and policy decisions toward more liveable, equitable, and sustainable cities.
</details>
<details>
<summary>摘要</summary>
全球各地城市面临着供应充足、安全、健康的住房的紧迫需求。尽管城市住房问题的政策重要性不言而喻，但我们对城市变化的追踪和监测能力却受到限制。使用深度学习计算机视觉方法对街道级图像进行分析，可以成功地衡量社会经济和环境不平等，但这些方法通常无法利用时间变化来追踪城市变化。我们使用自动学习方法，使用2008年至2021年之间的1500万个街道级图像，在伦敦市进行了时间变化的追踪。我们对Barlow Twins进行了改进，称之为Street2Vec，它可以嵌入城市结构，同时具有季节和日期变化的抗辐射性，无需手动标注。Street2Vec在伦敦市的住房供应变化追踪中表现出色，可以提供实时的城市规划和政策决策信息，以建立更加人居住、公平、可持续的城市。
</details></li>
</ul>
<hr>
<h2 id="You-can-have-your-ensemble-and-run-it-too-–-Deep-Ensembles-Spread-Over-Time"><a href="#You-can-have-your-ensemble-and-run-it-too-–-Deep-Ensembles-Spread-Over-Time" class="headerlink" title="You can have your ensemble and run it too – Deep Ensembles Spread Over Time"></a>You can have your ensemble and run it too – Deep Ensembles Spread Over Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11333">http://arxiv.org/abs/2309.11333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isak Meding, Alexander Bodin, Adam Tonderski, Joakim Johnander, Christoffer Petersson, Lennart Svensson</li>
<li>for: 这个研究旨在探讨深度 Ensemble 可以在时间序列上扩展以提高预测性和不确定性估计的可能性。</li>
<li>methods: 我们提出了 Deep Ensembles Spread Over Time (DESOT) 方法，将单一的 Ensemble  member 应用到每个数据点上，并融合多个数据点的预测。</li>
<li>results: DESOT 可以获得深度 Ensemble 的优化性和不确定性估计性，而不需要额外的计算成本增加。 DESOT 也简单实现，不需要在训练过程中使用时间序列。 最后，我们发现 DESOT 和深度 Ensemble 都能在非标准数据上进行预测和不确定性估计。<details>
<summary>Abstract</summary>
Ensembles of independently trained deep neural networks yield uncertainty estimates that rival Bayesian networks in performance. They also offer sizable improvements in terms of predictive performance over single models. However, deep ensembles are not commonly used in environments with limited computational budget -- such as autonomous driving -- since the complexity grows linearly with the number of ensemble members. An important observation that can be made for robotics applications, such as autonomous driving, is that data is typically sequential. For instance, when an object is to be recognized, an autonomous vehicle typically observes a sequence of images, rather than a single image. This raises the question, could the deep ensemble be spread over time?   In this work, we propose and analyze Deep Ensembles Spread Over Time (DESOT). The idea is to apply only a single ensemble member to each data point in the sequence, and fuse the predictions over a sequence of data points. We implement and experiment with DESOT for traffic sign classification, where sequences of tracked image patches are to be classified. We find that DESOT obtains the benefits of deep ensembles, in terms of predictive and uncertainty estimation performance, while avoiding the added computational cost. Moreover, DESOT is simple to implement and does not require sequences during training. Finally, we find that DESOT, like deep ensembles, outperform single models for out-of-distribution detection.
</details>
<details>
<summary>摘要</summary>
ensemble of independently trained deep neural networks可以提供与 bayesian networks相当的不确定性估计，同时也可以提高预测性能。但是，深度 ensemble在计算budget有限的环境中并不很常见，因为ensemble的复杂度随着成员增加而增加。在робо特应用，如自动驾驶，发现数据通常是顺序的。例如，当需要识别一个物体时，一辆自动驾驶车通常会观察一串图像，而不是单个图像。这引出了一个问题：可以将深度 ensemble推广到时间吗？在这种情况下，我们提出了深度 ensemble推广到时间（DESOT）的想法。我们只应用一个 ensemble member 到每个数据点的序列中，并将预测结果进行融合。我们实现并对 traffic sign classification 进行实验，Sequence of tracked image patches 需要进行分类。我们发现 DESOT 可以获得深度 ensemble 的优点，即预测性能和不确定性估计的好处，而不需要添加计算成本。此外，DESOT 简单易实现，不需要在训练时序列。最后，我们发现 DESOT 也可以超过单个模型的表现，对于非标准范围检测。
</details></li>
</ul>
<hr>
<h2 id="How-to-turn-your-camera-into-a-perfect-pinhole-model"><a href="#How-to-turn-your-camera-into-a-perfect-pinhole-model" class="headerlink" title="How to turn your camera into a perfect pinhole model"></a>How to turn your camera into a perfect pinhole model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11326">http://arxiv.org/abs/2309.11326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan De Boi, Stuti Pathak, Marina Oliveira, Rudi Penne</li>
<li>for: 提高计算机视觉应用中的相机准备环境，提供一种可以处理多种扭曲源的新方法。</li>
<li>methods: 使用 Gaussian processes 来去除图像中的扭曲和相机缺陷，并创建一个虚拟的理想投射相机，只需一张正方形网格检查模式图像。</li>
<li>results: 提高了许多计算机视觉算法和应用的性能，消除了扭曲参数和迭代优化。  Validated by synthetic data and real-world images.<details>
<summary>Abstract</summary>
Camera calibration is a first and fundamental step in various computer vision applications. Despite being an active field of research, Zhang's method remains widely used for camera calibration due to its implementation in popular toolboxes. However, this method initially assumes a pinhole model with oversimplified distortion models. In this work, we propose a novel approach that involves a pre-processing step to remove distortions from images by means of Gaussian processes. Our method does not need to assume any distortion model and can be applied to severely warped images, even in the case of multiple distortion sources, e.g., a fisheye image of a curved mirror reflection. The Gaussian processes capture all distortions and camera imperfections, resulting in virtual images as though taken by an ideal pinhole camera with square pixels. Furthermore, this ideal GP-camera only needs one image of a square grid calibration pattern. This model allows for a serious upgrade of many algorithms and applications that are designed in a pure projective geometry setting but with a performance that is very sensitive to nonlinear lens distortions. We demonstrate the effectiveness of our method by simplifying Zhang's calibration method, reducing the number of parameters and getting rid of the distortion parameters and iterative optimization. We validate by means of synthetic data and real world images. The contributions of this work include the construction of a virtual ideal pinhole camera using Gaussian processes, a simplified calibration method and lens distortion removal.
</details>
<details>
<summary>摘要</summary>
Camera 卡利ibration 是 computer vision 应用中的第一步和基础步骤。尽管是一个活跃的研究领域，张的方法仍然广泛使用于 camera 卡利ibration due to its implementation in popular toolboxes。然而，这种方法初始化假设了缩影模型，忽略了真实的扭曲模型。在这种工作中，我们提出了一种新的方法，该方法通过 Gaussian processes 来从图像中除扭曲。我们的方法不需要任何扭曲模型，可以应用于严重扭曲的图像，甚至在多个扭曲源的情况下，如 fisheye 图像 reflection 的弯曲镜。 Gaussian processes 捕捉了所有的扭曲和相机缺陷，从而生成虚拟的 ideal pinhole camera 图像，如quare pixels。此外，这个 ideal GP-camera 只需一个平方格 calibration pattern 图像。这种模型允许许多算法和应用程序，其中一些是在纯 proyective geometry 设定下设计，但是性能受到非线性镜头扭曲的影响。我们通过简化张的卡利ibration 方法，减少参数的数量，消除扭曲参数和迭代优化来证明方法的有效性。我们验证了这种方法的有效性通过 synthetic 数据和实际图像。本研究的贡献包括：在 Gaussian processes 中构建虚拟的 ideal pinhole camera，简化卡利ibration 方法和镜头扭曲除除。
</details></li>
</ul>
<hr>
<h2 id="Face-Aging-via-Diffusion-based-Editing"><a href="#Face-Aging-via-Diffusion-based-Editing" class="headerlink" title="Face Aging via Diffusion-based Editing"></a>Face Aging via Diffusion-based Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11321">http://arxiv.org/abs/2309.11321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MunchkinChen/FADING">https://github.com/MunchkinChen/FADING</a></li>
<li>paper_authors: Xiangyi Chen, Stéphane Lathuilière</li>
<li>for: 本研究旨在解决面部年轻化问题，生成面部图像的过去或未来图像，通过增加年龄相关的变化。</li>
<li>methods: 我们提出了一种新的方法，即FADING，利用语言-图像扩散模型的丰富前提，来解决面部年轻化问题。我们首先特化一个预训练的扩散模型，使其更适应面部年轻化任务，然后对输入图像进行倒散、获取优化的Null噪音嵌入，最后通过文本引导的地方年轻编辑。</li>
<li>results: 我们的方法与现有方法相比，在年轻精度、特征保留和年轻质量等方面具有明显的优势。<details>
<summary>Abstract</summary>
In this paper, we address the problem of face aging: generating past or future facial images by incorporating age-related changes to the given face. Previous aging methods rely solely on human facial image datasets and are thus constrained by their inherent scale and bias. This restricts their application to a limited generatable age range and the inability to handle large age gaps. We propose FADING, a novel approach to address Face Aging via DIffusion-based editiNG. We go beyond existing methods by leveraging the rich prior of large-scale language-image diffusion models. First, we specialize a pre-trained diffusion model for the task of face age editing by using an age-aware fine-tuning scheme. Next, we invert the input image to latent noise and obtain optimized null text embeddings. Finally, we perform text-guided local age editing via attention control. The quantitative and qualitative analyses demonstrate that our method outperforms existing approaches with respect to aging accuracy, attribute preservation, and aging quality.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们解决了人脸年龄化问题：通过 incorporating 年龄相关变化来生成过去或未来的脸部图像。先前的年龄方法仅仅基于人类脸部图像集合，因此受到其内置的尺度和偏见的限制，只能生成有限的年龄范围内的图像，并且无法处理大的年龄差。我们提出了 FADING，一种新的方法来解决人脸年龄化问题，通过语言-图像扩散模型的质量丰富的先天知识来超越现有方法。首先，我们特化了预训练的扩散模型，使其更适应人脸年龄编辑任务，并使用年龄意识 fine-tuning 方案进行特化。接着，我们将输入图像反转为干扰噪 embedding，并获得优化的 null text embedding。最后，我们通过文本引导的本地年龄编辑来进行控制。量化和质量分析表明，我们的方法在年龄准确性、特征保持和年龄质量等方面都超越了现有方法。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-the-effects-of-model-initialization-on-deep-model-generalization-A-study-with-adult-and-pediatric-Chest-X-ray-images"><a href="#Uncovering-the-effects-of-model-initialization-on-deep-model-generalization-A-study-with-adult-and-pediatric-Chest-X-ray-images" class="headerlink" title="Uncovering the effects of model initialization on deep model generalization: A study with adult and pediatric Chest X-ray images"></a>Uncovering the effects of model initialization on deep model generalization: A study with adult and pediatric Chest X-ray images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11318">http://arxiv.org/abs/2309.11318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sivaramakrishnan Rajaraman, Ghada Zamzmi, Feng Yang, Zhaohui Liang, Zhiyun Xue, Sameer Antani<br>for: 这个研究旨在提高深度学习模型在医疗计算机视觉应用中的性能和可靠性。而关于医疗图像（特别是胸部X射线图像）的影响则更少了解。本研究探讨了三种深度模型初始化技术：冷启动、暖启动和缩小和扰动start，对成人和儿童两个人口进行了评估。methods: 本研究使用了三种深度模型初始化技术：冷启动、暖启动和缩小和扰动start。这些技术在医疗图像的批处理训练场景下进行了评估，以适应实际世界中数据不断来临和模型更新的需求。results: 研究结果表明，使用ImageNet预训练权重初始化的模型在成人和儿童两个人口中的总体化能力较高，超过随机初始化的模型。此外，ImageNet预训练模型在不同训练场景下的内部和外部测试中都表现了稳定的性能。weight级 ensemble方法也显示了明显的提高（p&lt;0.05），特别是在测试阶段。因此，本研究强调了使用ImageNet预训练权重初始化的好处，尤其是在weight级 ensemble方法下，为创建可靠和总体化的深度学习解决方案。<details>
<summary>Abstract</summary>
Model initialization techniques are vital for improving the performance and reliability of deep learning models in medical computer vision applications. While much literature exists on non-medical images, the impacts on medical images, particularly chest X-rays (CXRs) are less understood. Addressing this gap, our study explores three deep model initialization techniques: Cold-start, Warm-start, and Shrink and Perturb start, focusing on adult and pediatric populations. We specifically focus on scenarios with periodically arriving data for training, thereby embracing the real-world scenarios of ongoing data influx and the need for model updates. We evaluate these models for generalizability against external adult and pediatric CXR datasets. We also propose novel ensemble methods: F-score-weighted Sequential Least-Squares Quadratic Programming (F-SLSQP) and Attention-Guided Ensembles with Learnable Fuzzy Softmax to aggregate weight parameters from multiple models to capitalize on their collective knowledge and complementary representations. We perform statistical significance tests with 95% confidence intervals and p-values to analyze model performance. Our evaluations indicate models initialized with ImageNet-pre-trained weights demonstrate superior generalizability over randomly initialized counterparts, contradicting some findings for non-medical images. Notably, ImageNet-pretrained models exhibit consistent performance during internal and external testing across different training scenarios. Weight-level ensembles of these models show significantly higher recall (p<0.05) during testing compared to individual models. Thus, our study accentuates the benefits of ImageNet-pretrained weight initialization, especially when used with weight-level ensembles, for creating robust and generalizable deep learning solutions.
</details>
<details>
<summary>摘要</summary>
“模型初始化技术对深度学习模型在医疗计算机视觉应用中的性能和可靠性有着重要的影响。虽然关于非医学图像的研究已经充分，但对医学图像，特别是胸部X射影（CXR）的影响还未得到充分了解。为了解决这个差距，我们的研究探讨了三种深度模型初始化技术：冷启动、温启动和缩放和扰动启动，对于成人和儿童两个人口进行了研究。我们强调在进行训练时periodically arriving data的情况下，以满足实际世界中数据不断来临和模型更新的需求。我们使用F-score-weighted Sequential Least-Squares Quadratic Programming（F-SLSQP）和Attention-Guided Ensembles with Learnable Fuzzy Softmax来权衡多个模型的参数，以便充分利用它们的共同知识和补充表示。我们对模型性能进行了统计学 significativity 测试，结果表明，使用ImageNet预训练权重初始化的模型在总体性能方面表现出色，并且在不同的训练场景下保持了一致的表现。此外，对这些模型进行权重级别的合并也表现出了明显的提升（p<0.05）。因此，我们的研究证明了使用ImageNet预训练权重初始化的模型，特别是在权重级别的合并下，可以创建可靠和总体性能优秀的深度学习解决方案。”
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Across-Domains-in-Diabetic-Retinopathy-via-Variational-Autoencoders"><a href="#Generalizing-Across-Domains-in-Diabetic-Retinopathy-via-Variational-Autoencoders" class="headerlink" title="Generalizing Across Domains in Diabetic Retinopathy via Variational Autoencoders"></a>Generalizing Across Domains in Diabetic Retinopathy via Variational Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11301">http://arxiv.org/abs/2309.11301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sharonchokuwa/VAE-DG">https://github.com/sharonchokuwa/VAE-DG</a></li>
<li>paper_authors: Sharon Chokuwa, Muhammad H. Khan</li>
<li>for: 这篇论文旨在探讨Variational Autoencoder（VA）是否能够实现类型普遍化，以对抗DR预测 задачі中的领域转移。</li>
<li>methods: 这篇论文使用Variational Autoencoder（VA）来分析眼底照片的latent space，以获得一个更加灵活和适应的领域不对称表示，以应对DR数据集中的领域转移。</li>
<li>results: 这篇论文显示，使用VA的简单方法可以超越现有的州际顶对应方法，并在公开可用的数据集上达到更高的准确率。这些结果显示，简单的方法可以在医疗图像领域中实现更好的领域普遍化，而不是仅仅靠赖高度复杂的技术。<details>
<summary>Abstract</summary>
Domain generalization for Diabetic Retinopathy (DR) classification allows a model to adeptly classify retinal images from previously unseen domains with various imaging conditions and patient demographics, thereby enhancing its applicability in a wide range of clinical environments. In this study, we explore the inherent capacity of variational autoencoders to disentangle the latent space of fundus images, with an aim to obtain a more robust and adaptable domain-invariant representation that effectively tackles the domain shift encountered in DR datasets. Despite the simplicity of our approach, we explore the efficacy of this classical method and demonstrate its ability to outperform contemporary state-of-the-art approaches for this task using publicly available datasets. Our findings challenge the prevailing assumption that highly sophisticated methods for DR classification are inherently superior for domain generalization. This highlights the importance of considering simple methods and adapting them to the challenging task of generalizing medical images, rather than solely relying on advanced techniques.
</details>
<details>
<summary>摘要</summary>
域 generale 化 для 诊断糖尿病 Retinopathy (DR) 让模型能够efficacious 分类 retinal 图像从以前未经见到的域与不同的拍摄条件和患者特征下，从而提高其在各种临床环境中的应用性。在这项研究中，我们探讨了变量自动编码器内置的latent space的分解能力，以获得更加稳定和适应的域不对称表示，以更好地解决DR数据集中的域转移问题。虽然我们的方法简单，但我们发现这种经典方法的效果可以超过当今的状态对DR分类任务的方法。我们的发现证明了不要仅仅依赖于高度复杂的方法，而是应该考虑简单的方法并适应它们来普遍化医疗图像。
</details></li>
</ul>
<hr>
<h2 id="Language-driven-Object-Fusion-into-Neural-Radiance-Fields-with-Pose-Conditioned-Dataset-Updates"><a href="#Language-driven-Object-Fusion-into-Neural-Radiance-Fields-with-Pose-Conditioned-Dataset-Updates" class="headerlink" title="Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates"></a>Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11281">http://arxiv.org/abs/2309.11281</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kcshum/pose-conditioned-NeRF-object-fusion">https://github.com/kcshum/pose-conditioned-NeRF-object-fusion</a></li>
<li>paper_authors: Ka Chun Shum, Jaeyeon Kim, Binh-Son Hua, Duc Thanh Nguyen, Sai-Kit Yeung</li>
<li>for: 这 paper 是用于描述一种基于神经辐射场的图像渲染方法，可以生成高质量的多视图一致的图像。</li>
<li>methods: 这 paper 使用了一种基于文本扩展的方法来实现对 neural radiance field 中的对象的操作，包括插入新背景和 removing 已有对象。</li>
<li>results: 实验结果表明，这 paper 的方法可以生成高质量的渲染图像，并且在 3D 重建和神经辐射场融合方面超过了现有的方法。<details>
<summary>Abstract</summary>
Neural radiance field is an emerging rendering method that generates high-quality multi-view consistent images from a neural scene representation and volume rendering. Although neural radiance field-based techniques are robust for scene reconstruction, their ability to add or remove objects remains limited. This paper proposes a new language-driven approach for object manipulation with neural radiance fields through dataset updates. Specifically, to insert a new foreground object represented by a set of multi-view images into a background radiance field, we use a text-to-image diffusion model to learn and generate combined images that fuse the object of interest into the given background across views. These combined images are then used for refining the background radiance field so that we can render view-consistent images containing both the object and the background. To ensure view consistency, we propose a dataset updates strategy that prioritizes radiance field training with camera views close to the already-trained views prior to propagating the training to remaining views. We show that under the same dataset updates strategy, we can easily adapt our method for object insertion using data from text-to-3D models as well as object removal. Experimental results show that our method generates photorealistic images of the edited scenes, and outperforms state-of-the-art methods in 3D reconstruction and neural radiance field blending.
</details>
<details>
<summary>摘要</summary>
神经辐射场是一种出现在渲染方法中的新技术，它可以生成高质量、多视图一致的图像从神经场景表示和体积渲染。 although neural radiance field-based techniques are robust for scene reconstruction, their ability to add or remove objects remains limited. This paper proposes a new language-driven approach for object manipulation with neural radiance fields through dataset updates. Specifically, to insert a new foreground object represented by a set of multi-view images into a background radiance field, we use a text-to-image diffusion model to learn and generate combined images that fuse the object of interest into the given background across views. These combined images are then used for refining the background radiance field so that we can render view-consistent images containing both the object and the background. To ensure view consistency, we propose a dataset updates strategy that prioritizes radiance field training with camera views close to the already-trained views prior to propagating the training to remaining views. We show that under the same dataset updates strategy, we can easily adapt our method for object insertion using data from text-to-3D models as well as object removal. Experimental results show that our method generates photorealistic images of the edited scenes, and outperforms state-of-the-art methods in 3D reconstruction and neural radiance field blending.
</details></li>
</ul>
<hr>
<h2 id="Towards-Real-Time-Neural-Video-Codec-for-Cross-Platform-Application-Using-Calibration-Information"><a href="#Towards-Real-Time-Neural-Video-Codec-for-Cross-Platform-Application-Using-Calibration-Information" class="headerlink" title="Towards Real-Time Neural Video Codec for Cross-Platform Application Using Calibration Information"></a>Towards Real-Time Neural Video Codec for Cross-Platform Application Using Calibration Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11276">http://arxiv.org/abs/2309.11276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan Tian, Yonghang Guan, Jinxi Xiang, Jun Zhang, Xiao Han, Wei Yang</li>
<li>for: 这个论文是为了提出一种实时跨平台神经视频编码器，以解决现有神经网络编码器在实际应用中的两大挑战。</li>
<li>methods: 作者使用了一种协调传输系统来保证编码和解码过程中的统一化量化参数，并使用了一种尺度约束来修复分布 Entropy 参数的不均匀性。</li>
<li>results: 实验结果显示，作者的模型可以在 NVIDIA RTX 2080 GPU 上实现 25 FPS 的解码速度，并且可以在另一个平台上编码的 720P 视频进行实时解码。此外，实时模型可以提供最高 24.2% BD-rate 改善，相比 anchor H.265。<details>
<summary>Abstract</summary>
The state-of-the-art neural video codecs have outperformed the most sophisticated traditional codecs in terms of RD performance in certain cases. However, utilizing them for practical applications is still challenging for two major reasons. 1) Cross-platform computational errors resulting from floating point operations can lead to inaccurate decoding of the bitstream. 2) The high computational complexity of the encoding and decoding process poses a challenge in achieving real-time performance. In this paper, we propose a real-time cross-platform neural video codec, which is capable of efficiently decoding of 720P video bitstream from other encoding platforms on a consumer-grade GPU. First, to solve the problem of inconsistency of codec caused by the uncertainty of floating point calculations across platforms, we design a calibration transmitting system to guarantee the consistent quantization of entropy parameters between the encoding and decoding stages. The parameters that may have transboundary quantization between encoding and decoding are identified in the encoding stage, and their coordinates will be delivered by auxiliary transmitted bitstream. By doing so, these inconsistent parameters can be processed properly in the decoding stage. Furthermore, to reduce the bitrate of the auxiliary bitstream, we rectify the distribution of entropy parameters using a piecewise Gaussian constraint. Second, to match the computational limitations on the decoding side for real-time video codec, we design a lightweight model. A series of efficiency techniques enable our model to achieve 25 FPS decoding speed on NVIDIA RTX 2080 GPU. Experimental results demonstrate that our model can achieve real-time decoding of 720P videos while encoding on another platform. Furthermore, the real-time model brings up to a maximum of 24.2\% BD-rate improvement from the perspective of PSNR with the anchor H.265.
</details>
<details>
<summary>摘要</summary>
现代神经视频编码器在某些情况下已经超越了最复杂的传统编码器，但在实际应用中仍然存在两大挑战。首先，由浮点运算引起的平台间计算错误可能导致错误解码bitstream。其次，编码和解码过程的计算复杂性使得实时性很难实现。在这篇论文中，我们提出了一种实时可靠的cross-platform神经视频编码器，可以在consumer-grade GPU上高速解码720P视频bitstream。首先，为了解决由不确定的浮点计算所引起的编码器不一致性问题，我们设计了卡利ibration transmitting系统，以 garantuee the consistent quantization of entropy parameters between the encoding and decoding stages。在编码阶段，我们标识出可能存在跨界量译参数的问题，并将其坐标传输给下游编码器。这样，在解码阶段可以正确处理这些不一致的参数。其次，为了降低auxiliary bitstream的比特率，我们使用piecewise Gaussian constraint来修正参数的分布。其次，为了在解码器端实现实时性，我们设计了一种轻量级模型。我们采用了一系列的效率技巧，使得我们的模型在NVIDIA RTX 2080 GPU上可以达到25帧/秒的解码速度。实验结果表明，我们的模型可以实时解码720P视频，而encoded on another platform。此外，实时模型可以提高最多24.2%的BD-rate，相比 anchor H.265。
</details></li>
</ul>
<hr>
<h2 id="StructChart-Perception-Structuring-Reasoning-for-Visual-Chart-Understanding"><a href="#StructChart-Perception-Structuring-Reasoning-for-Visual-Chart-Understanding" class="headerlink" title="StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding"></a>StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11268">http://arxiv.org/abs/2309.11268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renqiu Xia, Bo Zhang, Haoyang Peng, Ning Liao, Peng Ye, Botian Shi, Junchi Yan, Yu Qiao</li>
<li>for: 这篇论文目标是建立一种统一的学习模式，能够同时完成图表感知和理解任务。</li>
<li>methods: 论文使用了一种名为Structured Triplet Representations（STR）的新的表示方式，以及一种名为Structuring Chart-oriented Representation Metric（SCRM）的表现评价方法，来提高图表理解能力。</li>
<li>results: 经过广泛的实验，论文发现这种统一的学习模式能够在不同的图表任务上达到极高的表现，并且能够扩大图表数据集，以提高图表理解能力。<details>
<summary>Abstract</summary>
Charts are common in literature across different scientific fields, conveying rich information easily accessible to readers. Current chart-related tasks focus on either chart perception which refers to extracting information from the visual charts, or performing reasoning given the extracted data, e.g. in a tabular form. In this paper, we aim to establish a unified and label-efficient learning paradigm for joint perception and reasoning tasks, which can be generally applicable to different downstream tasks, beyond the question-answering task as specifically studied in peer works. Specifically, StructChart first reformulates the chart information from the popular tubular form (specifically linearized CSV) to the proposed Structured Triplet Representations (STR), which is more friendly for reducing the task gap between chart perception and reasoning due to the employed structured information extraction for charts. We then propose a Structuring Chart-oriented Representation Metric (SCRM) to quantitatively evaluate the performance for the chart perception task. To enrich the dataset for training, we further explore the possibility of leveraging the Large Language Model (LLM), enhancing the chart diversity in terms of both chart visual style and its statistical information. Extensive experiments are conducted on various chart-related tasks, demonstrating the effectiveness and promising potential for a unified chart perception-reasoning paradigm to push the frontier of chart understanding.
</details>
<details>
<summary>摘要</summary>
图表是科学文献中常见的数据可视化方式，能够快速传递丰富的信息给读者。目前的图表相关任务主要集中在图表识别和基于EXTRACTED数据的逻辑思维两个方面。在这篇论文中，我们希望建立一种统一的和标签有效的学习 парадигм，能够普适应用于不同的下游任务，而不仅仅是特定的问答任务，如在同等作者的论文中所研究。 Specifically, StructChart首先将流行的 tubular 形式（具体是线性化 CSV）中的图表信息重新表述为我们提出的结构化 triplet 表示（STR），这种结构化信息提取技术使得图表识别和逻辑思维之间的任务差距更小。然后，我们提出了一种 Chart-oriented Representation Metric（SCRM）来衡量图表识别任务的表现。为了让训练集更加丰富，我们还探索了使用 Large Language Model（LLM），通过扩展图表的视觉风格和统计信息，提高图表的多样性。我们在不同的图表相关任务上进行了广泛的实验，并证明了这种统一的图表识别和逻辑思维方法的有效性和潜在的前iers。
</details></li>
</ul>
<hr>
<h2 id="From-Classification-to-Segmentation-with-Explainable-AI-A-Study-on-Crack-Detection-and-Growth-Monitoring"><a href="#From-Classification-to-Segmentation-with-Explainable-AI-A-Study-on-Crack-Detection-and-Growth-Monitoring" class="headerlink" title="From Classification to Segmentation with Explainable AI: A Study on Crack Detection and Growth Monitoring"></a>From Classification to Segmentation with Explainable AI: A Study on Crack Detection and Growth Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11267">http://arxiv.org/abs/2309.11267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florent Forest, Hugo Porta, Devis Tuia, Olga Fink</li>
<li>for: 本研究旨在 automatization 基础设施中的表面裂隙监测，以实现结构健康监测。</li>
<li>methods: 本研究使用机器学习方法，但需要大量标注数据进行超vised 训练。而once a crack is detected， monitoring its severity 通常需要精准的像素级别分割。然而，对于每个图像进行像素级别分割的标注是劳动密集的。为了解决这个问题，本研究提议使用可解释的人工智能（XAI）方法，从类ifier的解释中 derivate 分割，只需要弱型图像级别的监督。</li>
<li>results: 本研究发现，使用XAI方法可以生成有意义的分割面掩模，即使无需大量的标注数据。Results reveal that while the resulting segmentation masks may exhibit lower quality than those produced by supervised methods, they remain meaningful and enable severity monitoring, thus reducing substantial labeling costs.<details>
<summary>Abstract</summary>
Monitoring surface cracks in infrastructure is crucial for structural health monitoring. Automatic visual inspection offers an effective solution, especially in hard-to-reach areas. Machine learning approaches have proven their effectiveness but typically require large annotated datasets for supervised training. Once a crack is detected, monitoring its severity often demands precise segmentation of the damage. However, pixel-level annotation of images for segmentation is labor-intensive. To mitigate this cost, one can leverage explainable artificial intelligence (XAI) to derive segmentations from the explanations of a classifier, requiring only weak image-level supervision. This paper proposes applying this methodology to segment and monitor surface cracks. We evaluate the performance of various XAI methods and examine how this approach facilitates severity quantification and growth monitoring. Results reveal that while the resulting segmentation masks may exhibit lower quality than those produced by supervised methods, they remain meaningful and enable severity monitoring, thus reducing substantial labeling costs.
</details>
<details>
<summary>摘要</summary>
监测基础设施表面裂隙是结构健康监测的关键。自动视见检测提供了一个有效的解决方案，特别是在困难 accessed 的地方。机器学习方法已经证明其效果，但通常需要大量的注释化数据集 дляsupervised 训练。一旦裂隙被检测出来，则需要精确地分类损害。然而，像素级注释图像 для分类是时间consuming。为了解决这个问题，这篇论文提议使用可解释人工智能（XAI） derive 分类器的解释，只需弱型图像级指导。这种方法可以帮助实现裂隙分类和严重性评估，并且可以降低大量的标注成本。我们评估了不同的XAI方法的性能，并研究了这种方法是否可以实现严重性评估和生长监测。结果表明，尽管生成的分类器分割面可能不如supervised 方法生成的分割面质量高，但它们仍然有意义，并且可以实现严重性评估和生长监测，从而减少标注成本。
</details></li>
</ul>
<hr>
<h2 id="TwinTex-Geometry-aware-Texture-Generation-for-Abstracted-3D-Architectural-Models"><a href="#TwinTex-Geometry-aware-Texture-Generation-for-Abstracted-3D-Architectural-Models" class="headerlink" title="TwinTex: Geometry-aware Texture Generation for Abstracted 3D Architectural Models"></a>TwinTex: Geometry-aware Texture Generation for Abstracted 3D Architectural Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11258">http://arxiv.org/abs/2309.11258</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ligo04/TwinTex">https://github.com/Ligo04/TwinTex</a></li>
<li>paper_authors: Weidan Xiong, Hongqian Zhang, Botao Peng, Ziyu Hu, Yongli Wu, Jianwei Guo, Hui Huang</li>
<li>for: 这个论文是为了生成一个精细的城市 Digital Twin 中的建筑物和景观的图像Texture mapping。</li>
<li>methods: 这个方法使用了一种新的自动化文本映射方法，包括选择高质量照片，提取LoL特征，对照片和geometry进行对齐，并使用一个新的扩展数据集和滤波模型来完善缺失区域。</li>
<li>results: 实验结果表明，这种方法可以高效地生成高质量的文本映射，并且可以在不同的建筑物和景观中实现人工专家水平的效果，而不需要太多的工作。<details>
<summary>Abstract</summary>
Coarse architectural models are often generated at scales ranging from individual buildings to scenes for downstream applications such as Digital Twin City, Metaverse, LODs, etc. Such piece-wise planar models can be abstracted as twins from 3D dense reconstructions. However, these models typically lack realistic texture relative to the real building or scene, making them unsuitable for vivid display or direct reference. In this paper, we present TwinTex, the first automatic texture mapping framework to generate a photo-realistic texture for a piece-wise planar proxy. Our method addresses most challenges occurring in such twin texture generation. Specifically, for each primitive plane, we first select a small set of photos with greedy heuristics considering photometric quality, perspective quality and facade texture completeness. Then, different levels of line features (LoLs) are extracted from the set of selected photos to generate guidance for later steps. With LoLs, we employ optimization algorithms to align texture with geometry from local to global. Finally, we fine-tune a diffusion model with a multi-mask initialization component and a new dataset to inpaint the missing region. Experimental results on many buildings, indoor scenes and man-made objects of varying complexity demonstrate the generalization ability of our algorithm. Our approach surpasses state-of-the-art texture mapping methods in terms of high-fidelity quality and reaches a human-expert production level with much less effort. Project page: https://vcc.tech/research/2023/TwinTex.
</details>
<details>
<summary>摘要</summary>
<<SYS>>文本翻译成简化中文。</SYS>>建筑模型经常在大规模生成，从个别建筑到场景，用于下游应用程序，如数字城市、Metaverse、LODs等。这些块状平面模型可以被抽象为真实建筑或场景的孪生。然而，这些模型通常缺乏真实建筑或场景的精炼文化，使其不适合精彩显示或直接参考。在这篇论文中，我们介绍了 TwinTex，首个自动Texture mapping框架，用于生成具有高精炼度的Texture для块状平面代理。我们的方法解决了这类孪生Texture生成中的主要挑战。具体来说，对于每个基本平面，我们首先选择一小集数据，使用善意的规则来考虑光学质量、视角质量和建筑面料完整性。然后，我们从这些选择的数据中提取不同级别的线条特征（LoLs），以供后续步骤的引导。使用LoLs，我们运用优化算法将Texture与Geometry进行对齐。最后，我们使用扩展模型，并在新的数据集上进行填充缺失区域。实验结果表明，我们的算法可以在许多不同复杂度的建筑、室内场景和人工制品上实现高精炼度的Texture mapping，并且超过了当前状态艺的Texture mapping方法。我们的方法可以减少很多劳动力，达到人工专家水平。项目页面：https://vcc.tech/research/2023/TwinTex。
</details></li>
</ul>
<hr>
<h2 id="Box2Poly-Memory-Efficient-Polygon-Prediction-of-Arbitrarily-Shaped-and-Rotated-Text"><a href="#Box2Poly-Memory-Efficient-Polygon-Prediction-of-Arbitrarily-Shaped-and-Rotated-Text" class="headerlink" title="Box2Poly: Memory-Efficient Polygon Prediction of Arbitrarily Shaped and Rotated Text"></a>Box2Poly: Memory-Efficient Polygon Prediction of Arbitrarily Shaped and Rotated Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11248">http://arxiv.org/abs/2309.11248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuyang Chen, Dong Wang, Konrad Schindler, Mingwei Sun, Yongliang Wang, Nicolo Savioli, Liqiu Meng</li>
<li>For: 提高文本检测的精度和效率，尤其是对于不规则的文本布局。* Methods: 基于Sparse R-CNN的协调解码管道，通过逐次精度调整多边形预测，使用单个特征向量导引多边形实例准备。* Results: 比较DPText-DETR方法，具有更高的内存效率（&gt;50%）和推理速度（&gt;40%），同时保持了基准测试集上的性能水平。<details>
<summary>Abstract</summary>
Recently, Transformer-based text detection techniques have sought to predict polygons by encoding the coordinates of individual boundary vertices using distinct query features. However, this approach incurs a significant memory overhead and struggles to effectively capture the intricate relationships between vertices belonging to the same instance. Consequently, irregular text layouts often lead to the prediction of outlined vertices, diminishing the quality of results. To address these challenges, we present an innovative approach rooted in Sparse R-CNN: a cascade decoding pipeline for polygon prediction. Our method ensures precision by iteratively refining polygon predictions, considering both the scale and location of preceding results. Leveraging this stabilized regression pipeline, even employing just a single feature vector to guide polygon instance regression yields promising detection results. Simultaneously, the leverage of instance-level feature proposal substantially enhances memory efficiency (>50% less vs. the state-of-the-art method DPText-DETR) and reduces inference speed (>40% less vs. DPText-DETR) with minor performance drop on benchmarks.
</details>
<details>
<summary>摘要</summary>
traducción al chino simplificado:现在，基于Transformer的文本检测技术尝试预测多边形，通过对各个边界顶点的坐标使用特定的查询特征进行编码。然而，这种方法带来了显著的内存开销，并且很难准确地捕捉同一个实例中的逻辑关系。因此，不规则的文本布局经常导致预测的边界顶点变为围栏顶点，这会导致结果的质量下降。为了解决这些挑战，我们提出了一种创新的方法，基于Sparse R-CNN：一个逻辑拓展管道 для多边形预测。我们的方法保证准确性，通过迭代地纠正多边形预测结果，考虑多边形的缩放和位置。通过这个稳定的回归管道，甚至只使用一个特征向量来引导多边形实例回归，也可以获得了有前途的检测结果。同时，通过实例级别的特征提档，可以大幅提高内存效率（>50%比DPText-DETR更高），并且降低推理速度（>40%比DPText-DETR更低），而无需做出重要的性能下降。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-Few-shot-Point-Cloud-Semantic-Segmentation"><a href="#Towards-Robust-Few-shot-Point-Cloud-Semantic-Segmentation" class="headerlink" title="Towards Robust Few-shot Point Cloud Semantic Segmentation"></a>Towards Robust Few-shot Point Cloud Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11228">http://arxiv.org/abs/2309.11228</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Pixie8888/R3DFSSeg">https://github.com/Pixie8888/R3DFSSeg</a></li>
<li>paper_authors: Yating Xu, Na Zhao, Gim Hee Lee</li>
<li>for: 提高几何点云Semantic segmentation的鲁棒性，使其在实际世界中快速适应新的未知类型，只需几个支持集样本。</li>
<li>methods: 我们提出了一种Component-level Clean Noise Separation（CCNS）表示学习，以学习细分target类的净样本与噪声样本之间的分化特征表示。然后，我们提出了一种Multi-scale Degree-based Noise Suppression（MDNS）方案，以消除支持集中的噪声样本。</li>
<li>results: 我们在不同噪声设定下进行了广泛的实验，结果显示CCNS和MDNS的组合显著提高了性能。<details>
<summary>Abstract</summary>
Few-shot point cloud semantic segmentation aims to train a model to quickly adapt to new unseen classes with only a handful of support set samples. However, the noise-free assumption in the support set can be easily violated in many practical real-world settings. In this paper, we focus on improving the robustness of few-shot point cloud segmentation under the detrimental influence of noisy support sets during testing time. To this end, we first propose a Component-level Clean Noise Separation (CCNS) representation learning to learn discriminative feature representations that separates the clean samples of the target classes from the noisy samples. Leveraging the well separated clean and noisy support samples from our CCNS, we further propose a Multi-scale Degree-based Noise Suppression (MDNS) scheme to remove the noisy shots from the support set. We conduct extensive experiments on various noise settings on two benchmark datasets. Our results show that the combination of CCNS and MDNS significantly improves the performance. Our code is available at https://github.com/Pixie8888/R3DFSSeg.
</details>
<details>
<summary>摘要</summary>
文本：几个类别点云 semantic segmentation 目标是训练一个模型快速适应新未见类别，仅仅需要一些支持集样本。然而，实际世界中的实际设定中可能会轻松违反无噪设定。在这篇论文中，我们专注于增强几个类别点云 semantic segmentation 的Robustness，在测试时testing时的恶劣影响下。为此，我们首先提出了Component-level Clean Noise Separation (CCNS) 表示学习，以学习分类特征表现，将目标类别的清洁样本与噪音样本分离。然后，我们更进一步提出了Multi-scale Degree-based Noise Suppression (MDNS) 方案，以移除测试时的噪音样本。我们对不同噪音设定进行了广泛的实验，结果显示，CCNS 和 MDNS 的组合可以明显提高性能。我们的代码可以在 <https://github.com/Pixie8888/R3DFSSeg> 中找到。翻译结果：文本：几个类别点云 semantic segmentation 目标是训练一个模型快速适应新未见类别，仅仅需要一些支持集样本。然而，实际世界中的实际设定中可能会轻松违反无噪设定。在这篇论文中，我们专注于增强几个类别点云 semantic segmentation 的Robustness，在测试时testing时的恶劣影响下。为此，我们首先提出了Component-level Clean Noise Separation (CCNS) 表示学习，以学习分类特征表现，将目标类别的清洁样本与噪音样本分离。然后，我们更进一步提出了Multi-scale Degree-based Noise Suppression (MDNS) 方案，以移除测试时的噪音样本。我们对不同噪音设定进行了广泛的实验，结果显示，CCNS 和 MDNS 的组合可以明显提高性能。我们的代码可以在 <https://github.com/Pixie8888/R3DFSSeg> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Few-Shot-Point-Cloud-Segmentation-Via-Geometric-Words"><a href="#Generalized-Few-Shot-Point-Cloud-Segmentation-Via-Geometric-Words" class="headerlink" title="Generalized Few-Shot Point Cloud Segmentation Via Geometric Words"></a>Generalized Few-Shot Point Cloud Segmentation Via Geometric Words</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11222">http://arxiv.org/abs/2309.11222</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Pixie8888/GFS-3DSeg_GWs">https://github.com/Pixie8888/GFS-3DSeg_GWs</a></li>
<li>paper_authors: Yating Xu, Conghui Hu, Na Zhao, Gim Hee Lee</li>
<li>For: 这篇论文的目的是提出一种更实用的普通多少shot点云分割方法，可以在新类出现时通过几个支持点云来泛化到新类，同时保持基础类的分割精度。* Methods: 该方法使用的是 geometric words 来表示基础和新类之间的 geometric 共同部分，并将其 incorporated 到一种新的 geometric-aware semantic representation 中，以便更好地泛化到新类而不忘记基础类。此外，该方法还引入 geometric prototypes 来导引分割，使用 geometric prior knowledge。* Results:  compared with基eline方法，该方法在 S3DIS 和 ScanNet 上的实验表现出色，显示了更高的性能。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Existing fully-supervised point cloud segmentation methods suffer in the dynamic testing environment with emerging new classes. Few-shot point cloud segmentation algorithms address this problem by learning to adapt to new classes at the sacrifice of segmentation accuracy for the base classes, which severely impedes its practicality. This largely motivates us to present the first attempt at a more practical paradigm of generalized few-shot point cloud segmentation, which requires the model to generalize to new categories with only a few support point clouds and simultaneously retain the capability to segment base classes. We propose the geometric words to represent geometric components shared between the base and novel classes, and incorporate them into a novel geometric-aware semantic representation to facilitate better generalization to the new classes without forgetting the old ones. Moreover, we introduce geometric prototypes to guide the segmentation with geometric prior knowledge. Extensive experiments on S3DIS and ScanNet consistently illustrate the superior performance of our method over baseline methods. Our code is available at: https://github.com/Pixie8888/GFS-3DSeg_GWs.
</details>
<details>
<summary>摘要</summary>
现有的完全监督的点云分割方法在新类出现的动态测试环境中表现不佳，这是因为这些方法在学习新类时会卷积到基础类的精度，这大大限制了其实用性。这种情况激励我们提出一种更实用的通用几shot点云分割方法，要求模型能够通过几个支持点云来扩展到新类，同时保持基础类的分割精度。我们使用“geometry words”来表示基础和新类之间的几何共同部分，并将其 integrate into a novel geometric-aware semantic representation，以便更好地适应新类而无需忘记旧类。此外，我们还引入几何规范来导航分割，以利用几何知识来提高分割精度。我们的实验表明，我们的方法在S3DIS和ScanNet上的扩展性和稳定性都显著提高。代码可以在：https://github.com/Pixie8888/GFS-3DSeg_GWs 中找到。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Bat-Call-Classification-using-Transformer-Networks"><a href="#Automatic-Bat-Call-Classification-using-Transformer-Networks" class="headerlink" title="Automatic Bat Call Classification using Transformer Networks"></a>Automatic Bat Call Classification using Transformer Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11218">http://arxiv.org/abs/2309.11218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Fundel, Daniel A. Braun, Sebastian Gottwald</li>
<li>for:  automatic bat call identification</li>
<li>methods:  Transformer architecture for multi-label classification</li>
<li>results:  single species accuracy of 88.92% (F1-score of 84.23%), multi species macro F1-score of 74.40%<details>
<summary>Abstract</summary>
Automatically identifying bat species from their echolocation calls is a difficult but important task for monitoring bats and the ecosystem they live in. Major challenges in automatic bat call identification are high call variability, similarities between species, interfering calls and lack of annotated data. Many currently available models suffer from relatively poor performance on real-life data due to being trained on single call datasets and, moreover, are often too slow for real-time classification. Here, we propose a Transformer architecture for multi-label classification with potential applications in real-time classification scenarios. We train our model on synthetically generated multi-species recordings by merging multiple bats calls into a single recording with multiple simultaneous calls. Our approach achieves a single species accuracy of 88.92% (F1-score of 84.23%) and a multi species macro F1-score of 74.40% on our test set. In comparison to three other tools on the independent and publicly available dataset ChiroVox, our model achieves at least 25.82% better accuracy for single species classification and at least 6.9% better macro F1-score for multi species classification.
</details>
<details>
<summary>摘要</summary>
自动识别蝙蝠种类从呼叫声中是一项具有挑战性和重要性的任务，用于监测蝙蝠和它们所处生态系统。主要挑战在自动蝙蝠呼叫识别中是呼叫声的高度变化、种类之间的相似性、干扰声和缺乏标注数据。现有的许多模型在实际数据上表现较差，主要是因为它们在单个呼叫数据集上训练。我们提出一种Transformer架构，用于多类别分类，具有实时分类场景的应用 potential。我们在合成生成的多种 recording中训练我们的模型，其中每个记录包含多个同时发生的呼叫。我们的方法实现了单种呼叫精度88.92%（F1-score为84.23%）和多种macro F1-score74.40%。与三个其他工具在独立公共的数据集ChiroVox上进行比较，我们的模型至少25.82%更高的单种呼叫精度和6.9%更高的多种 macro F1-score。
</details></li>
</ul>
<hr>
<h2 id="EPTQ-Enhanced-Post-Training-Quantization-via-Label-Free-Hessian"><a href="#EPTQ-Enhanced-Post-Training-Quantization-via-Label-Free-Hessian" class="headerlink" title="EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian"></a>EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11531">http://arxiv.org/abs/2309.11531</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ssi-research/eptq">https://github.com/ssi-research/eptq</a></li>
<li>paper_authors: Ofir Gordon, Hai Victor Habi, Arnon Netzer</li>
<li>for: 这篇论文旨在提出一种新的增强后期量化方法（EPTQ），以提高深度神经网络（DNN）的嵌入。</li>
<li>methods: 这篇论文使用了知识传播（knowledge distillation）和自适应层重复（adaptive weighting of layers）来实现增强后期量化。另外，论文还引入了一种无标签技术来近似任务损失的希耶数（Label-Free Hessian），以除去需要标签数据集的需求。</li>
<li>results: 这篇论文的实验结果显示，通过使用EPTQ，可以在各种模型、任务和数据集上取得最佳的结果，包括ImageNet分类、COCO物件检测和Pascal-VOC semantic segmentation。此外，论文还证明了EPTQ的可行性和可替代性，可以在不同的架构上进行实现，包括CNNs、Transformers、混合和MLP-only模型。<details>
<summary>Abstract</summary>
Quantization of deep neural networks (DNN) has become a key element in the efforts of embedding such networks on end-user devices. However, current quantization methods usually suffer from costly accuracy degradation. In this paper, we propose a new method for Enhanced Post Training Quantization named EPTQ. The method is based on knowledge distillation with an adaptive weighting of layers. In addition, we introduce a new label-free technique for approximating the Hessian trace of the task loss, named Label-Free Hessian. This technique removes the requirement of a labeled dataset for computing the Hessian. The adaptive knowledge distillation uses the Label-Free Hessian technique to give greater attention to the sensitive parts of the model while performing the optimization. Empirically, by employing EPTQ we achieve state-of-the-art results on a wide variety of models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation. We demonstrate the performance and compatibility of EPTQ on an extended set of architectures, including CNNs, Transformers, hybrid, and MLP-only models.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）的量化已成为嵌入这些网络在用户端设备的关键元素。然而，当前的量化方法通常会导致精度下降。在这篇论文中，我们提出了一种新的增强后期量化方法，称为增强后期量化（EPTQ）。该方法基于知识传承，并使用自适应层权重。此外，我们还介绍了一种新的无标签技术，用于估计任务损失的希尔伯特特征，称为无标签希尔伯特特征（Label-Free Hessian）。这种技术消除了需要标注数据集来计算希尔伯特特征的需求。适应知识传承使用无标签希尔伯特特征来增加对模型敏感部分的注意力，进行优化。我们的实验结果表明，通过使用EPTQ，我们在各种模型、任务和数据集上达到了状态对的结果，包括ImageNet分类、COCO物体检测和Pascal-VOC semantics segmentation。我们也证明了EPTQ在扩展的集成体系中的性能和兼容性，包括CNNs、Transformers、混合和MLP-only模型。
</details></li>
</ul>
<hr>
<h2 id="Partition-A-Medical-Image-Extracting-Multiple-Representative-Sub-regions-for-Few-shot-Medical-Image-Segmentation"><a href="#Partition-A-Medical-Image-Extracting-Multiple-Representative-Sub-regions-for-Few-shot-Medical-Image-Segmentation" class="headerlink" title="Partition-A-Medical-Image: Extracting Multiple Representative Sub-regions for Few-shot Medical Image Segmentation"></a>Partition-A-Medical-Image: Extracting Multiple Representative Sub-regions for Few-shot Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11172">http://arxiv.org/abs/2309.11172</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YazhouZhu19/PAMI">https://github.com/YazhouZhu19/PAMI</a></li>
<li>paper_authors: Yazhou Zhu, Shidong Wang, Tong Xin, Zheng Zhang, Haofeng Zhang</li>
<li>for: 这则研究targets医疗影像分类任务，旨在提供更有前途的解决方案，因为医疗影像分类任务中高质量的标签是自然罕见。</li>
<li>methods: 本研究使用Regional Prototypical Learning (RPL)模块将支持影像的前景 decomposed into distinct regions，然后使用这些区域来 derivation region-level representations。此外，我们还引入了一个新的Prototypical Representation Debiasing (PRD)模块，用于抑制区域表示的干扰。</li>
<li>results: 经过广泛的实验证明，本研究在三个公开 accessible medical imaging datasets上实现了与主流 FSMIS 方法相比的稳定改进。并且提供了一个可用的源代码（<a target="_blank" rel="noopener" href="https://github.com/YazhouZhu19/PAMI%EF%BC%89%E3%80%82">https://github.com/YazhouZhu19/PAMI）。</a><details>
<summary>Abstract</summary>
Few-shot Medical Image Segmentation (FSMIS) is a more promising solution for medical image segmentation tasks where high-quality annotations are naturally scarce. However, current mainstream methods primarily focus on extracting holistic representations from support images with large intra-class variations in appearance and background, and encounter difficulties in adapting to query images. In this work, we present an approach to extract multiple representative sub-regions from a given support medical image, enabling fine-grained selection over the generated image regions. Specifically, the foreground of the support image is decomposed into distinct regions, which are subsequently used to derive region-level representations via a designed Regional Prototypical Learning (RPL) module. We then introduce a novel Prototypical Representation Debiasing (PRD) module based on a two-way elimination mechanism which suppresses the disturbance of regional representations by a self-support, Multi-direction Self-debiasing (MS) block, and a support-query, Interactive Debiasing (ID) block. Finally, an Assembled Prediction (AP) module is devised to balance and integrate predictions of multiple prototypical representations learned using stacked PRD modules. Results obtained through extensive experiments on three publicly accessible medical imaging datasets demonstrate consistent improvements over the leading FSMIS methods. The source code is available at https://github.com/YazhouZhu19/PAMI.
</details>
<details>
<summary>摘要</summary>
供少医学图像分割（FSMIS）是一种更有前途的解决方案，用于医学图像分割任务中，高质量标注很难获得。然而，当前主流方法主要是提取支持图像中巨量的内部变化的整体表示，并遇到在查询图像上适应的困难。在这种工作中，我们提出了一种方法，可以从支持医学图像中提取多个代表性子区域，以便精细地选择生成的图像区域。具体来说，支持图像的前景被分解成不同的区域，然后通过我们设计的区域层学习（RPL）模块来 derivation region-level表示。我们然后引入了一种新的表示偏导（PRD）模块，基于两种排除机制，即自我支持的多向排除（MS）块和支持-查询的互动排除（ID）块。最后，我们设计了一个集成预测（AP）模块，可以平衡和集成多个表示学习的PRD模块中的预测。经过了广泛的实验，我们在三个公共 accessible的医学图像数据集上获得了一致的改进。源代码可以在https://github.com/YazhouZhu19/PAMI上获取。
</details></li>
</ul>
<hr>
<h2 id="AutoSynth-Learning-to-Generate-3D-Training-Data-for-Object-Point-Cloud-Registration"><a href="#AutoSynth-Learning-to-Generate-3D-Training-Data-for-Object-Point-Cloud-Registration" class="headerlink" title="AutoSynth: Learning to Generate 3D Training Data for Object Point Cloud Registration"></a>AutoSynth: Learning to Generate 3D Training Data for Object Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11170">http://arxiv.org/abs/2309.11170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Dang, Mathieu Salzmann</li>
<li>for: 本研究旨在提供一种自动生成3D训练数据的方法，以提高3D对象注册任务的训练数据质量和数量。</li>
<li>methods: 本研究使用自动生成的3D数据集，通过筛选搜索空间中的优秀数据集，以便在低成本下获得优质的3D训练数据。</li>
<li>results: 研究表明，使用我们的方法可以在TUD-L、LINEMOD和Occluded-LINEMOD等任务上实现更好的性能，比如ModelNet40数据集。此外，我们还证明了我们的方法可以在不同的点云注册网络上实现更好的性能。<details>
<summary>Abstract</summary>
In the current deep learning paradigm, the amount and quality of training data are as critical as the network architecture and its training details. However, collecting, processing, and annotating real data at scale is difficult, expensive, and time-consuming, particularly for tasks such as 3D object registration. While synthetic datasets can be created, they require expertise to design and include a limited number of categories. In this paper, we introduce a new approach called AutoSynth, which automatically generates 3D training data for point cloud registration. Specifically, AutoSynth automatically curates an optimal dataset by exploring a search space encompassing millions of potential datasets with diverse 3D shapes at a low cost.To achieve this, we generate synthetic 3D datasets by assembling shape primitives, and develop a meta-learning strategy to search for the best training data for 3D registration on real point clouds. For this search to remain tractable, we replace the point cloud registration network with a much smaller surrogate network, leading to a $4056.43$ times speedup. We demonstrate the generality of our approach by implementing it with two different point cloud registration networks, BPNet and IDAM. Our results on TUD-L, LINEMOD and Occluded-LINEMOD evidence that a neural network trained on our searched dataset yields consistently better performance than the same one trained on the widely used ModelNet40 dataset.
</details>
<details>
<summary>摘要</summary>
现在的深度学习 paradigma中，训练数据的量和质量是网络架构和训练细节的 equally important factors。然而，收集、处理和标注实际数据在大规模上是困难、昂贵和时间consuming的，特别是 для tasks such as 3D object registration。 although synthetic datasets can be created, they require expertise to design and have a limited number of categories. In this paper, we introduce a new approach called AutoSynth, which automatically generates 3D training data for point cloud registration. Specifically, AutoSynth automatically curates an optimal dataset by exploring a search space encompassing millions of potential datasets with diverse 3D shapes at a low cost.To achieve this, we generate synthetic 3D datasets by assembling shape primitives, and develop a meta-learning strategy to search for the best training data for 3D registration on real point clouds. For this search to remain tractable, we replace the point cloud registration network with a much smaller surrogate network, leading to a $4056.43$ times speedup. We demonstrate the generality of our approach by implementing it with two different point cloud registration networks, BPNet and IDAM. Our results on TUD-L, LINEMOD and Occluded-LINEMOD evidence that a neural network trained on our searched dataset yields consistently better performance than the same one trained on the widely used ModelNet40 dataset.
</details></li>
</ul>
<hr>
<h2 id="Multi-grained-Temporal-Prototype-Learning-for-Few-shot-Video-Object-Segmentation"><a href="#Multi-grained-Temporal-Prototype-Learning-for-Few-shot-Video-Object-Segmentation" class="headerlink" title="Multi-grained Temporal Prototype Learning for Few-shot Video Object Segmentation"></a>Multi-grained Temporal Prototype Learning for Few-shot Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11160">http://arxiv.org/abs/2309.11160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nankepan/VIPMT">https://github.com/nankepan/VIPMT</a></li>
<li>paper_authors: Nian Liu, Kepan Nan, Wangbo Zhao, Yuanwei Liu, Xiwen Yao, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Junwei Han, Fahad Shahbaz Khan</li>
<li>for: 这个论文旨在用少量标注图像支持进行视频对象分割，以便在视频数据中分割同一类目标对象。</li>
<li>methods: 该方法基于IPMT，一种现有的少量图像分割方法，并将多重层次时间引导信息引入视频数据处理中。具体来说，查询视频信息被分解成clip型prototype和记忆型prototype，以捕捉当地和长期内部时间引导信息。每帧独立使用框型 prototype 处理细致化适应引导，并实现了双向clip-frame prototype 交流。此外，为减少噪音记忆的影响，提出了基于结构相似关系的支持选择可靠记忆帧。此外，还提出了一种新的分割损失，以提高学习 prototype 的类别可识别度。</li>
<li>results: 实验结果表明，我们提出的视频 IPMT 模型在两个标准测试集上显著超过了之前的模型。<details>
<summary>Abstract</summary>
Few-Shot Video Object Segmentation (FSVOS) aims to segment objects in a query video with the same category defined by a few annotated support images. However, this task was seldom explored. In this work, based on IPMT, a state-of-the-art few-shot image segmentation method that combines external support guidance information with adaptive query guidance cues, we propose to leverage multi-grained temporal guidance information for handling the temporal correlation nature of video data. We decompose the query video information into a clip prototype and a memory prototype for capturing local and long-term internal temporal guidance, respectively. Frame prototypes are further used for each frame independently to handle fine-grained adaptive guidance and enable bidirectional clip-frame prototype communication. To reduce the influence of noisy memory, we propose to leverage the structural similarity relation among different predicted regions and the support for selecting reliable memory frames. Furthermore, a new segmentation loss is also proposed to enhance the category discriminability of the learned prototypes. Experimental results demonstrate that our proposed video IPMT model significantly outperforms previous models on two benchmark datasets. Code is available at https://github.com/nankepan/VIPMT.
</details>
<details>
<summary>摘要</summary>
几个视频对象分割（FSVOS）目标是使用一些定义同一类目的支持图像来分割查询视频中的对象。然而，这个任务几乎没有被研究。在这个工作中，我们基于IPMT，一种现有的少量图像分割方法，通过 вне部支持导航信息和适应查询导航征料来拓展我们的方法。我们将查询视频信息分解成一个clip原型和一个记忆原型，以捕捉本地和长期内部 temporal导航信息。每帧prototype被使用，以独立处理细腻的适应导航和两个方向clip-frame prototype通信。为了减少干扰的内存，我们提议使用不同预测区域之间的结构相似关系和支持选择可靠的记忆帧。此外，我们还提出了一种新的分割损失，以提高学习的类别可识别度。实验结果表明，我们的提出的视频IPMT模型在两个标准数据集上显著超越了之前的模型。代码可以在https://github.com/nankepan/VIPMT上获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-Deformable-3D-Graph-Similarity-to-Track-Plant-Cells-in-Unregistered-Time-Lapse-Images"><a href="#Learning-Deformable-3D-Graph-Similarity-to-Track-Plant-Cells-in-Unregistered-Time-Lapse-Images" class="headerlink" title="Learning Deformable 3D Graph Similarity to Track Plant Cells in Unregistered Time Lapse Images"></a>Learning Deformable 3D Graph Similarity to Track Plant Cells in Unregistered Time Lapse Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11157">http://arxiv.org/abs/2309.11157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Shazid Islam, Arindam Dutta, Calvin-Khang Ta, Kevin Rodriguez, Christian Michael, Mark Alber, G. Venugopala Reddy, Amit K. Roy-Chowdhury</li>
<li>for: 该论文旨在提出一种基于学习的方法，用于准确地跟踪植物细胞图像中的细胞。</li>
<li>methods: 该方法利用植物细胞的紧密排列三维结构，创建三维图库，以实现准确的细胞跟踪。另外，该方法还提出了新的细胞分裂检测算法和高效三维对 align 算法。</li>
<li>results: 该论文在一个标准数据集上进行了实验，并证明了该方法的跟踪精度和搜索时间的优势。<details>
<summary>Abstract</summary>
Tracking of plant cells in images obtained by microscope is a challenging problem due to biological phenomena such as large number of cells, non-uniform growth of different layers of the tightly packed plant cells and cell division. Moreover, images in deeper layers of the tissue being noisy and unavoidable systemic errors inherent in the imaging process further complicates the problem. In this paper, we propose a novel learning-based method that exploits the tightly packed three-dimensional cell structure of plant cells to create a three-dimensional graph in order to perform accurate cell tracking. We further propose novel algorithms for cell division detection and effective three-dimensional registration, which improve upon the state-of-the-art algorithms. We demonstrate the efficacy of our algorithm in terms of tracking accuracy and inference-time on a benchmark dataset.
</details>
<details>
<summary>摘要</summary>
track plant cells in microscope images 是一个复杂的问题，因为生物现象如大量细胞、不均生长的不同层次紧密排列的植物细胞，以及细胞分裂。此外，深层组织图像中的噪声和不可避免的图像捕捉过程中的系统性错误更加复杂了问题。在本文中，我们提出了一种基于学习的方法，利用植物细胞紧密三维结构来创建三维图表，以进行准确的细胞跟踪。我们还提出了新的细胞分裂检测算法和有效的三维对接算法，这些算法都超过了当前状态的算法。我们通过对一个标准数据集进行评估，证明了我们的算法的准确性和推理时间。
</details></li>
</ul>
<hr>
<h2 id="CNN-based-local-features-for-navigation-near-an-asteroid"><a href="#CNN-based-local-features-for-navigation-near-an-asteroid" class="headerlink" title="CNN-based local features for navigation near an asteroid"></a>CNN-based local features for navigation near an asteroid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11156">http://arxiv.org/abs/2309.11156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olli Knuuttila, Antti Kestilä, Esa Kallio</li>
<li>for:  asteroid exploration missions and on-orbit servicing</li>
<li>methods:  lightweight feature extractor specifically tailored for asteroid proximity navigation, designed to be robust to illumination changes and affine transformations</li>
<li>results:  effective navigation and localization, with incremental improvements over existing methods and a trained feature extractor<details>
<summary>Abstract</summary>
This article addresses the challenge of vision-based proximity navigation in asteroid exploration missions and on-orbit servicing. Traditional feature extraction methods struggle with the significant appearance variations of asteroids due to limited scattered light. To overcome this, we propose a lightweight feature extractor specifically tailored for asteroid proximity navigation, designed to be robust to illumination changes and affine transformations. We compare and evaluate state-of-the-art feature extraction networks and three lightweight network architectures in the asteroid context. Our proposed feature extractors and their evaluation leverages both synthetic images and real-world data from missions such as NEAR Shoemaker, Hayabusa, Rosetta, and OSIRIS-REx. Our contributions include a trained feature extractor, incremental improvements over existing methods, and a pipeline for training domain-specific feature extractors. Experimental results demonstrate the effectiveness of our approach in achieving accurate navigation and localization. This work aims to advance the field of asteroid navigation and provides insights for future research in this domain.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)这篇文章关注 asteroid 探测和处理任务中的视觉靠近导航挑战，传统的特征提取方法由于 asteroid 的限制散射光导致表现变化强大。为了解决这个问题，我们提议一种适应 asteroid 靠近导航的轻量级特征提取器，可以抗抗照明变化和抽象变换。我们比较和评估了现有的特征提取网络和三种轻量级网络体系，并在 asteroid 上进行了评估。我们的提案包括一个已经训练好的特征提取器，以及对现有方法进行了改进。我们的实验结果表明，我们的方法可以实现高精度的导航和地址确定。这项工作希望可以推动 asteroid 导航领域的进步，并为未来的研究提供了新的思路和灵感。
</details></li>
</ul>
<hr>
<h2 id="Online-Calibration-of-a-Single-Track-Ground-Vehicle-Dynamics-Model-by-Tight-Fusion-with-Visual-Inertial-Odometry"><a href="#Online-Calibration-of-a-Single-Track-Ground-Vehicle-Dynamics-Model-by-Tight-Fusion-with-Visual-Inertial-Odometry" class="headerlink" title="Online Calibration of a Single-Track Ground Vehicle Dynamics Model by Tight Fusion with Visual-Inertial Odometry"></a>Online Calibration of a Single-Track Ground Vehicle Dynamics Model by Tight Fusion with Visual-Inertial Odometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11148">http://arxiv.org/abs/2309.11148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haolong Li, Joerg Stueckler</li>
<li>for: 这篇论文是为了提供一种基于视觉遥感和动力学模型的单车跑动估计方法，用于 Navigation Planning。</li>
<li>methods: 该方法使用了单车动力学模型，与视觉遥感（VIO）相结合，在线进行模型Parameters calibration和适应。</li>
<li>results: 实验表明，该方法可以在不同的环境下（室内和外），适应环境变化，并且可以准确地预测未来控制输入的效果。同时，该方法还可以提高跟踪精度。<details>
<summary>Abstract</summary>
Wheeled mobile robots need the ability to estimate their motion and the effect of their control actions for navigation planning. In this paper, we present ST-VIO, a novel approach which tightly fuses a single-track dynamics model for wheeled ground vehicles with visual inertial odometry. Our method calibrates and adapts the dynamics model online and facilitates accurate forward prediction conditioned on future control inputs. The single-track dynamics model approximates wheeled vehicle motion under specific control inputs on flat ground using ordinary differential equations. We use a singularity-free and differentiable variant of the single-track model to enable seamless integration as dynamics factor into VIO and to optimize the model parameters online together with the VIO state variables. We validate our method with real-world data in both indoor and outdoor environments with different terrain types and wheels. In our experiments, we demonstrate that our ST-VIO can not only adapt to the change of the environments and achieve accurate prediction under new control inputs, but even improves the tracking accuracy. Supplementary video: https://youtu.be/BuGY1L1FRa4.
</details>
<details>
<summary>摘要</summary>
自动移动机器人需要估算其运动和控制动作的影响以实现导航规划。本文提出了ST-VIO，一种新的方法，它将单车辆动力学模型紧密融合视觉陀螺仪定位。我们的方法在线投入和调整动力学模型，并使用未来控制输入的前提下进行高精度预测。单车辆动力学模型是在特定的控制输入下，在平地上使用普通微分方程描述车辆的运动。我们使用不含特征点和可微分的单车辆模型，以便轻松地将动力学模型纳入VIО中，并在VIО状态变量上线上调整模型参数。我们通过实验证明，我们的ST-VIO可以不仅适应环境变化，并在新的控制输入下实现高精度跟踪。补充视频：https://youtu.be/BuGY1L1FRa4。
</details></li>
</ul>
<hr>
<h2 id="GraphEcho-Graph-Driven-Unsupervised-Domain-Adaptation-for-Echocardiogram-Video-Segmentation"><a href="#GraphEcho-Graph-Driven-Unsupervised-Domain-Adaptation-for-Echocardiogram-Video-Segmentation" class="headerlink" title="GraphEcho: Graph-Driven Unsupervised Domain Adaptation for Echocardiogram Video Segmentation"></a>GraphEcho: Graph-Driven Unsupervised Domain Adaptation for Echocardiogram Video Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11145">http://arxiv.org/abs/2309.11145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/GraphEcho">https://github.com/xmed-lab/GraphEcho</a></li>
<li>paper_authors: Jiewen Yang, Xinpeng Ding, Ziyang Zheng, Xiaowei Xu, Xiaomeng Li</li>
<li>For: 这个论文研究了非监督领域适应（Unsupervised Domain Adaptation，UDA）在echocardiogram视频分割方面，目的是将来自源频谱域的模型泛化到其他未标注目标频谱域。* Methods: 我们引入了一个新的CardiacUDA数据集和一种名为GraphEcho的新方法，该方法包括两个创新模块：空间域频谱匹配（SCGM）和心跳周期一致性（TCC）模块。这两个模块可以更好地对global和local特征从源和目标频谱域进行对齐，从而提高UDA分割结果。* Results: 我们的GraphEcho方法在对比 existed状态的推荐UDA分割方法时表现出色，实验结果表明。我们的CardiacUDA数据集和代码将在接受后公开发布，这项工作将为心脏结构分割从echocardiogram视频中奠定新的、坚实的基础。代码和数据集可以通过<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/GraphEcho%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/xmed-lab/GraphEcho访问。</a><details>
<summary>Abstract</summary>
Echocardiogram video segmentation plays an important role in cardiac disease diagnosis. This paper studies the unsupervised domain adaption (UDA) for echocardiogram video segmentation, where the goal is to generalize the model trained on the source domain to other unlabelled target domains. Existing UDA segmentation methods are not suitable for this task because they do not model local information and the cyclical consistency of heartbeat. In this paper, we introduce a newly collected CardiacUDA dataset and a novel GraphEcho method for cardiac structure segmentation. Our GraphEcho comprises two innovative modules, the Spatial-wise Cross-domain Graph Matching (SCGM) and the Temporal Cycle Consistency (TCC) module, which utilize prior knowledge of echocardiogram videos, i.e., consistent cardiac structure across patients and centers and the heartbeat cyclical consistency, respectively. These two modules can better align global and local features from source and target domains, improving UDA segmentation results. Experimental results showed that our GraphEcho outperforms existing state-of-the-art UDA segmentation methods. Our collected dataset and code will be publicly released upon acceptance. This work will lay a new and solid cornerstone for cardiac structure segmentation from echocardiogram videos. Code and dataset are available at: https://github.com/xmed-lab/GraphEcho
</details>
<details>
<summary>摘要</summary>
《echocardiogram视频分割 plays an important role in cardiac disease diagnosis。This paper studies the unsupervised domain adaption（UDA）for echocardiogram视频分割，where the goal is to generalize the model trained on the source domain to other unlabelled target domains。Existing UDA segmentation methods are not suitable for this task because they do not model local information and the cyclical consistency of heartbeat。In this paper, we introduce a newly collected CardiacUDA dataset and a novel GraphEcho method for cardiac structure segmentation。Our GraphEcho comprises two innovative modules，the Spatial-wise Cross-domain Graph Matching（SCGM）and the Temporal Cycle Consistency（TCC）module，which utilize prior knowledge of echocardiogram videos，i.e., consistent cardiac structure across patients and centers and the heartbeat cyclical consistency，respectively。These two modules can better align global and local features from source and target domains，improving UDA segmentation results。Experimental results showed that our GraphEcho outperforms existing state-of-the-art UDA segmentation methods。Our collected dataset and code will be publicly released upon acceptance。This work will lay a new and solid cornerstone for cardiac structure segmentation from echocardiogram videos。Code and dataset are available at：https://github.com/xmed-lab/GraphEcho。》Note that Simplified Chinese is the official writing system used in mainland China, and it may be different from Traditional Chinese, which is used in Taiwan and other regions.
</details></li>
</ul>
<hr>
<h2 id="GL-Fusion-Global-Local-Fusion-Network-for-Multi-view-Echocardiogram-Video-Segmentation"><a href="#GL-Fusion-Global-Local-Fusion-Network-for-Multi-view-Echocardiogram-Video-Segmentation" class="headerlink" title="GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation"></a>GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11144">http://arxiv.org/abs/2309.11144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/GL-Fusion">https://github.com/xmed-lab/GL-Fusion</a></li>
<li>paper_authors: Ziyang Zheng, Jiewen Yang, Xinpeng Ding, Xiaowei Xu, Xiaomeng Li</li>
<li>for: 这种研究旨在提高自动分类echocardiogram视频中的心脏结构分割精度和可靠性。</li>
<li>methods: 该研究提出了一种全新的全球-本地融合网络（GL-Fusion），用于同时利用多视图信息的全球和本地特征，以提高echocardiogram分析的准确性。</li>
<li>results: 该研究通过使用MvEVD数据集进行测试，发现GL-Fusion方法可以提高echocardiogram分析的准确性，与基eline方法相比提高了7.83%。此外，GL-Fusion方法还超过了现有的状态 искусственный智能方法。<details>
<summary>Abstract</summary>
Cardiac structure segmentation from echocardiogram videos plays a crucial role in diagnosing heart disease. The combination of multi-view echocardiogram data is essential to enhance the accuracy and robustness of automated methods. However, due to the visual disparity of the data, deriving cross-view context information remains a challenging task, and unsophisticated fusion strategies can even lower performance. In this study, we propose a novel Gobal-Local fusion (GL-Fusion) network to jointly utilize multi-view information globally and locally that improve the accuracy of echocardiogram analysis. Specifically, a Multi-view Global-based Fusion Module (MGFM) is proposed to extract global context information and to explore the cyclic relationship of different heartbeat cycles in an echocardiogram video. Additionally, a Multi-view Local-based Fusion Module (MLFM) is designed to extract correlations of cardiac structures from different views. Furthermore, we collect a multi-view echocardiogram video dataset (MvEVD) to evaluate our method. Our method achieves an 82.29% average dice score, which demonstrates a 7.83% improvement over the baseline method, and outperforms other existing state-of-the-art methods. To our knowledge, this is the first exploration of a multi-view method for echocardiogram video segmentation. Code available at: https://github.com/xmed-lab/GL-Fusion
</details>
<details>
<summary>摘要</summary>
卡第亚结构分割自echocardiogram视频中扮演重要的角色，用于诊断心血管疾病。多视图echocardiogram数据的组合是提高自动方法的准确性和可靠性的关键。然而，由于视觉差异， derivation of cross-view context information remains a challenging task, and unsophisticated fusion strategies can even lower performance. 在这种研究中，我们提出了一种全新的全球-本地混合（GL-Fusion）网络，用于同时利用多视图信息的全球和本地信息，以提高echocardiogram分析的准确性。 Specifically, a Multi-view Global-based Fusion Module (MGFM) is proposed to extract global context information and to explore the cyclic relationship of different heartbeat cycles in an echocardiogram video. Additionally, a Multi-view Local-based Fusion Module (MLFM) is designed to extract correlations of cardiac structures from different views. Furthermore, we collect a multi-view echocardiogram video dataset (MvEVD) to evaluate our method. Our method achieves an 82.29% average dice score, which demonstrates a 7.83% improvement over the baseline method, and outperforms other existing state-of-the-art methods. To our knowledge, this is the first exploration of a multi-view method for echocardiogram video segmentation. 可以在https://github.com/xmed-lab/GL-Fusion找到我们的代码。
</details></li>
</ul>
<hr>
<h2 id="More-complex-encoder-is-not-all-you-need"><a href="#More-complex-encoder-is-not-all-you-need" class="headerlink" title="More complex encoder is not all you need"></a>More complex encoder is not all you need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11139">http://arxiv.org/abs/2309.11139</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aitechlabcn/neUNet">https://github.com/aitechlabcn/neUNet</a></li>
<li>paper_authors: Weibin Yang, Longwei Xu, Pengwei Wang, Dehua Geng, Yusong Li, Mingyuan Xu, Zhiqi Dong</li>
<li>for: 这个论文主要用于提高医疗影像分类的精度和效率。</li>
<li>methods: 本文使用的方法包括：U-Net和其变体，并将注意力集中在增强解oder的部分，特别是增强upsampling部分以提高分类结果。</li>
<li>results: 本文的结果显示，使用了新的Sub-pixel Convolution和多条气平面输入模块，可以提高分类结果的精度和效率，并且在Synapse和ACDC datasets上表现出色，超越了其他现有的方法。<details>
<summary>Abstract</summary>
U-Net and its variants have been widely used in medical image segmentation. However, most current U-Net variants confine their improvement strategies to building more complex encoder, while leaving the decoder unchanged or adopting a simple symmetric structure. These approaches overlook the true functionality of the decoder: receiving low-resolution feature maps from the encoder and restoring feature map resolution and lost information through upsampling. As a result, the decoder, especially its upsampling component, plays a crucial role in enhancing segmentation outcomes. However, in 3D medical image segmentation, the commonly used transposed convolution can result in visual artifacts. This issue stems from the absence of direct relationship between adjacent pixels in the output feature map. Furthermore, plain encoder has already possessed sufficient feature extraction capability because downsampling operation leads to the gradual expansion of the receptive field, but the loss of information during downsampling process is unignorable. To address the gap in relevant research, we extend our focus beyond the encoder and introduce neU-Net (i.e., not complex encoder U-Net), which incorporates a novel Sub-pixel Convolution for upsampling to construct a powerful decoder. Additionally, we introduce multi-scale wavelet inputs module on the encoder side to provide additional information. Our model design achieves excellent results, surpassing other state-of-the-art methods on both the Synapse and ACDC datasets.
</details>
<details>
<summary>摘要</summary>
U-Net和其变种在医学影像分割中广泛应用。然而，现有的U-Net变种通常是通过建立更复杂的编码器来提高性能，而忽略了解码器的真正功能：接收低分辨率特征图并将其修复到原始分辨率和丢失信息。这些方法忽略了解码器中的upsampling组件的重要作用，这使得分割结果受到限制。尤其在3D医学影像分割中，通常使用的拼接 convolution 可能会导致视觉artefacts。这种问题的原因在于输出特征图中不存在直接相邻像素的直接关系。此外，简单的编码器已经拥有了充足的特征提取能力，因为下降操作导致捕捉区域的扩展，但是下降操作中丢失的信息是不可忽略的。为了解决这个研究漏洞，我们扩展了我们的关注范围，并引入了一种新的Sub-pixel Convolution для upsampling，以建立一个强大的解码器。此外，我们还引入了多尺度wavelet输入模块在编码器Side来提供额外信息。我们的模型设计实现了出色的结果，超过了其他状态对的方法在Synapse和ACDC数据集上。
</details></li>
</ul>
<hr>
<h2 id="Shape-Anchor-Guided-Holistic-Indoor-Scene-Understanding"><a href="#Shape-Anchor-Guided-Holistic-Indoor-Scene-Understanding" class="headerlink" title="Shape Anchor Guided Holistic Indoor Scene Understanding"></a>Shape Anchor Guided Holistic Indoor Scene Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11133">http://arxiv.org/abs/2309.11133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Geo-Tell/AncRec">https://github.com/Geo-Tell/AncRec</a></li>
<li>paper_authors: Mingyue Dong, Linxi Huan, Hanjiang Xiong, Shuhan Shen, Xianwei Zheng</li>
<li>for: 提出了一种基于形态锚点的学习策略（AncLearn），用于实现室内Scene理解的稳定和准确性。</li>
<li>methods: 利用形态锚点生成anchors，以便在搜索空间中提取实际存在的对象表示，并且通过对噪声和目标相关特征进行分离，提供可靠的提议。在重建阶段，通过减少异常值，提供高质量的对象点抽象。</li>
<li>results: 在ScanNetv2 dataset上进行了实验，并取得了在3D对象检测、布局估计和形态重建方面的状态 искусственный智能性能。<details>
<summary>Abstract</summary>
This paper proposes a shape anchor guided learning strategy (AncLearn) for robust holistic indoor scene understanding. We observe that the search space constructed by current methods for proposal feature grouping and instance point sampling often introduces massive noise to instance detection and mesh reconstruction. Accordingly, we develop AncLearn to generate anchors that dynamically fit instance surfaces to (i) unmix noise and target-related features for offering reliable proposals at the detection stage, and (ii) reduce outliers in object point sampling for directly providing well-structured geometry priors without segmentation during reconstruction. We embed AncLearn into a reconstruction-from-detection learning system (AncRec) to generate high-quality semantic scene models in a purely instance-oriented manner. Experiments conducted on the challenging ScanNetv2 dataset demonstrate that our shape anchor-based method consistently achieves state-of-the-art performance in terms of 3D object detection, layout estimation, and shape reconstruction. The code will be available at https://github.com/Geo-Tell/AncRec.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Locate-and-Verify-A-Two-Stream-Network-for-Improved-Deepfake-Detection"><a href="#Locate-and-Verify-A-Two-Stream-Network-for-Improved-Deepfake-Detection" class="headerlink" title="Locate and Verify: A Two-Stream Network for Improved Deepfake Detection"></a>Locate and Verify: A Two-Stream Network for Improved Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11131">http://arxiv.org/abs/2309.11131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sccsok/Locate-and-Verify">https://github.com/sccsok/Locate-and-Verify</a></li>
<li>paper_authors: Chao Shuai, Jieming Zhong, Shuang Wu, Feng Lin, Zhibo Wang, Zhongjie Ba, Zhenguang Liu, Lorenzo Cavallaro, Kui Ren</li>
<li>for: 本研究旨在提高深伪检测方法的一般化能力和特定 forgery 区域探测能力。</li>
<li>methods: 本文提出了三个方法来解决现有方法的缺陷：一个创新的两条流网络，三个功能模组，以及一个半supervised Patch Similarity Learning策略。</li>
<li>results: 本文的方法在六个benchmark上与现有方法比较，表现出了significantly improved的一般化和特定 forgery 区域探测能力，包括Frame-level AUC在Deepfake Detection Challenge preview dataset上从0.797提高到0.835，以及Video-level AUC在CelebDF$_$v1 dataset上从0.811提高到0.847。<details>
<summary>Abstract</summary>
Deepfake has taken the world by storm, triggering a trust crisis. Current deepfake detection methods are typically inadequate in generalizability, with a tendency to overfit to image contents such as the background, which are frequently occurring but relatively unimportant in the training dataset. Furthermore, current methods heavily rely on a few dominant forgery regions and may ignore other equally important regions, leading to inadequate uncovering of forgery cues. In this paper, we strive to address these shortcomings from three aspects: (1) We propose an innovative two-stream network that effectively enlarges the potential regions from which the model extracts forgery evidence. (2) We devise three functional modules to handle the multi-stream and multi-scale features in a collaborative learning scheme. (3) Confronted with the challenge of obtaining forgery annotations, we propose a Semi-supervised Patch Similarity Learning strategy to estimate patch-level forged location annotations. Empirically, our method demonstrates significantly improved robustness and generalizability, outperforming previous methods on six benchmarks, and improving the frame-level AUC on Deepfake Detection Challenge preview dataset from 0.797 to 0.835 and video-level AUC on CelebDF$\_$v1 dataset from 0.811 to 0.847. Our implementation is available at https://github.com/sccsok/Locate-and-Verify.
</details>
<details>
<summary>摘要</summary>
深刻的假动作（Deepfake）已经在世界上引发了一场信任危机。目前的假动作检测方法通常无法普遍化，往往对背景进行过滤，这些背景虽然常见但相对 speaking 不重要。此外，现有的方法倾向于仅对一些主导的伪造区域进行过滤，可能会忽略其他Equally important regions，从而导致伪造讯号的不充分探测。在这篇文章中，我们尝试解决这些缺陷自三个方面：1. 我们提出了一个创新的两条流网络，实际地扩大了模型从中提取伪造证据的可能区域。2. 我们设计了三个功能模组，以实现多条流和多个标准之间的合作学习。3. 面对伪造标注的挑战，我们提出了一个半supervised Patch Similarity Learning策略，以估计伪造区域标注。实际上，我们的方法在六个benchmark上表现出色，与前一代方法相比，具有更好的 Robustness 和普遍化能力。我们的实现可以在https://github.com/sccsok/Locate-and-Verify上找到。
</details></li>
</ul>
<hr>
<h2 id="PSDiff-Diffusion-Model-for-Person-Search-with-Iterative-and-Collaborative-Refinement"><a href="#PSDiff-Diffusion-Model-for-Person-Search-with-Iterative-and-Collaborative-Refinement" class="headerlink" title="PSDiff: Diffusion Model for Person Search with Iterative and Collaborative Refinement"></a>PSDiff: Diffusion Model for Person Search with Iterative and Collaborative Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11125">http://arxiv.org/abs/2309.11125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengyou Jia, Minnan Luo, Zhuohang Dang, Guang Dai, Xiaojun Chang, Jingdong Wang, Qinghua Zheng</li>
<li>for: 本文旨在提出一种新的人员搜索框架，以解决现有方法中的两个主要挑战：1）探测阶段模块不适合人脸识别任务；2）两个子任务之间的协作被忽略。</li>
<li>methods: 本文使用了Diffusion模型，将人员搜索转化为两个阶段的双杂化过程，从噪声框和人脸嵌入转化为实际情况。与传统的探测到人脸识别的方法不同，我们的杂化方法可以消除探测阶段模块，从而避免人脸识别任务的地方最优点。此外，我们还设计了一种新的协同杂化层，以便在探测和人脸识别两个子任务之间进行融合协同，使两个子任务互相帮助。</li>
<li>results: 实验结果表明，PSDiff在标准测试集上达到了当前最佳性能，具有较少的参数和灵活计算负担。<details>
<summary>Abstract</summary>
Dominant Person Search methods aim to localize and recognize query persons in a unified network, which jointly optimizes two sub-tasks, \ie, detection and Re-IDentification (ReID). Despite significant progress, two major challenges remain: 1) Detection-prior modules in previous methods are suboptimal for the ReID task. 2) The collaboration between two sub-tasks is ignored. To alleviate these issues, we present a novel Person Search framework based on the Diffusion model, PSDiff. PSDiff formulates the person search as a dual denoising process from noisy boxes and ReID embeddings to ground truths. Unlike existing methods that follow the Detection-to-ReID paradigm, our denoising paradigm eliminates detection-prior modules to avoid the local-optimum of the ReID task. Following the new paradigm, we further design a new Collaborative Denoising Layer (CDL) to optimize detection and ReID sub-tasks in an iterative and collaborative way, which makes two sub-tasks mutually beneficial. Extensive experiments on the standard benchmarks show that PSDiff achieves state-of-the-art performance with fewer parameters and elastic computing overhead.
</details>
<details>
<summary>摘要</summary>
主流人体搜索方法目标是在一个统一网络中本地化和识别查询人体，同时优化两个子任务，即探测和ReID（人体识别）。尽管有了很大的进步，但两个主要挑战仍然存在：1）探测优先模块在先前的方法中是不佳的ReID任务。2）两个子任务之间的合作被忽视。为了解决这些问题，我们提出了一种基于Diffusion模型的人体搜索框架，称为PSDiff。PSDiff将人体搜索转化为一个双方减噪过程，从噪声框和ReID嵌入转化到实际值。与先前的方法不同，我们的减噪方法不需要探测优先模块，以避免探测任务的本地最佳点。在新的 paradigma下，我们进一步设计了一个新的合作减噪层（CDL），以便在迭代和协同的方式优化探测和ReID子任务，使两个子任务互相有利。经验表明，PSDiff在标准测试准则上达到了状态的精度性表现，并且具有 fewer 参数和灵活计算负担。
</details></li>
</ul>
<hr>
<h2 id="Hyperspectral-Benchmark-Bridging-the-Gap-between-HSI-Applications-through-Comprehensive-Dataset-and-Pretraining"><a href="#Hyperspectral-Benchmark-Bridging-the-Gap-between-HSI-Applications-through-Comprehensive-Dataset-and-Pretraining" class="headerlink" title="Hyperspectral Benchmark: Bridging the Gap between HSI Applications through Comprehensive Dataset and Pretraining"></a>Hyperspectral Benchmark: Bridging the Gap between HSI Applications through Comprehensive Dataset and Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11122">http://arxiv.org/abs/2309.11122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cogsys-tuebingen/hsi_benchmark">https://github.com/cogsys-tuebingen/hsi_benchmark</a></li>
<li>paper_authors: Hannah Frank, Leon Amadeus Varga, Andreas Zell</li>
<li>for: 这个研究旨在提供一个全面的专门应用于几何pectral实验（HSI）的benchmark dataset，以便更好地评估几何spectral模型的能力。</li>
<li>methods: 本研究使用了一个新的benchmark dataset，包括三个不同的HSI应用：食品检查、远程感知和回收。此外，研究还提出了一个预训管道，以提高专门的训练过程稳定性。</li>
<li>results: 本研究的结果显示，这个benchmark dataset可以更好地评估专门的HSI模型，并且可以推广现有的方法。此外，预训管道可以提高专门的训练过程稳定性。<details>
<summary>Abstract</summary>
Hyperspectral Imaging (HSI) serves as a non-destructive spatial spectroscopy technique with a multitude of potential applications. However, a recurring challenge lies in the limited size of the target datasets, impeding exhaustive architecture search. Consequently, when venturing into novel applications, reliance on established methodologies becomes commonplace, in the hope that they exhibit favorable generalization characteristics. Regrettably, this optimism is often unfounded due to the fine-tuned nature of models tailored to specific HSI contexts.   To address this predicament, this study introduces an innovative benchmark dataset encompassing three markedly distinct HSI applications: food inspection, remote sensing, and recycling. This comprehensive dataset affords a finer assessment of hyperspectral model capabilities. Moreover, this benchmark facilitates an incisive examination of prevailing state-of-the-art techniques, consequently fostering the evolution of superior methodologies.   Furthermore, the enhanced diversity inherent in the benchmark dataset underpins the establishment of a pretraining pipeline for HSI. This pretraining regimen serves to enhance the stability of training processes for larger models. Additionally, a procedural framework is delineated, offering insights into the handling of applications afflicted by limited target dataset sizes.
</details>
<details>
<summary>摘要</summary>
干elespectral Imaging（HSI）是一种不 destrucción的空间спектроскопи技术，具有各种应用前景。然而，一个常 recurs的挑战是目标数据集的有限大小，导致了较少的模型搜索空间。因此，在探索新应用场景时，通常会依靠已有的方法，希望它们在不同的HSI上能够展现良好的泛化特性。然而，这种optimism通常是不符的，因为这些模型是为特定HSI上精心定制的。为解决这个困境，本研究提出了一个创新的标准数据集，包括三个明确不同的HSI应用：食品检查、远程感知和回收。这个全面的数据集为干elespectral模型的能力进行更加细致的评估。此外，这个标准数据集还支持现有的state-of-the-art技术的准确性的减弱，从而促进了更高水平的方法的进化。此外，增强的数据集多样性为HSI预训练管道提供了基础。这个预训练管道可以增强大型模型的训练过程的稳定性。此外，本研究还提出了一种手动框架，用于处理受有限target数据集大小的应用。
</details></li>
</ul>
<hr>
<h2 id="BroadBEV-Collaborative-LiDAR-camera-Fusion-for-Broad-sighted-Bird’s-Eye-View-Map-Construction"><a href="#BroadBEV-Collaborative-LiDAR-camera-Fusion-for-Broad-sighted-Bird’s-Eye-View-Map-Construction" class="headerlink" title="BroadBEV: Collaborative LiDAR-camera Fusion for Broad-sighted Bird’s Eye View Map Construction"></a>BroadBEV: Collaborative LiDAR-camera Fusion for Broad-sighted Bird’s Eye View Map Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11119">http://arxiv.org/abs/2309.11119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsu Kim, Giseop Kim, Kyong Hwan Jin, Sunwook Choi</li>
<li>for: 本研究旨在提高激光摄像机（LiDAR）和摄像机（camera）的 Bird’s Eye View（BEV）空间融合，以实现更广泛的视场和高精度的地面检测。</li>
<li>methods: 我们提出了一种广泛的 BEV融合策略（BroadBEV），包括点散发（Point-scattering）和自注重权重（ColFusion）两个部分。点散发方法使得LiDAR BEV分布散射到摄像机深度分布中，以提高摄像机分支的深度估计和精度。自注重权重方法在LiDAR和摄像机 BEV特征之间应用自注重权重，以实现有效的 BEV融合。</li>
<li>results: 我们的实验表明，BroadBEV可以提供广泛的 BEV视场，并且有较高的性能提升。<details>
<summary>Abstract</summary>
A recent sensor fusion in a Bird's Eye View (BEV) space has shown its utility in various tasks such as 3D detection, map segmentation, etc. However, the approach struggles with inaccurate camera BEV estimation, and a perception of distant areas due to the sparsity of LiDAR points. In this paper, we propose a broad BEV fusion (BroadBEV) that addresses the problems with a spatial synchronization approach of cross-modality. Our strategy aims to enhance camera BEV estimation for a broad-sighted perception while simultaneously improving the completion of LiDAR's sparsity in the entire BEV space. Toward that end, we devise Point-scattering that scatters LiDAR BEV distribution to camera depth distribution. The method boosts the learning of depth estimation of the camera branch and induces accurate location of dense camera features in BEV space. For an effective BEV fusion between the spatially synchronized features, we suggest ColFusion that applies self-attention weights of LiDAR and camera BEV features to each other. Our extensive experiments demonstrate that BroadBEV provides a broad-sighted BEV perception with remarkable performance gains.
</details>
<details>
<summary>摘要</summary>
Recently, a sensor fusion in a bird's eye view (BEV) space has shown its potential in various tasks such as 3D detection and map segmentation. However, the approach is limited by inaccurate camera BEV estimation and a lack of information on distant areas due to the sparsity of LiDAR points. In this paper, we propose a broad BEV fusion (BroadBEV) that addresses these problems using a cross-modality spatial synchronization approach. Our method aims to improve camera BEV estimation for a broad-sighted perception while simultaneously enhancing the completion of LiDAR's sparsity in the entire BEV space. To achieve this, we use Point-scattering to scatter LiDAR BEV distribution to camera depth distribution, which boosts the learning of depth estimation of the camera branch and accurately locates dense camera features in BEV space. Additionally, we propose ColFusion, which applies self-attention weights of LiDAR and camera BEV features to each other for effective BEV fusion. Our extensive experiments show that BroadBEV provides a broad-sighted BEV perception with significant performance gains.
</details></li>
</ul>
<hr>
<h2 id="PRAT-PRofiling-Adversarial-aTtacks"><a href="#PRAT-PRofiling-Adversarial-aTtacks" class="headerlink" title="PRAT: PRofiling Adversarial aTtacks"></a>PRAT: PRofiling Adversarial aTtacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11111">http://arxiv.org/abs/2309.11111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rahulambati/PRAT">https://github.com/rahulambati/PRAT</a></li>
<li>paper_authors: Rahul Ambati, Naveed Akhtar, Ajmal Mian, Yogesh Singh Rawat</li>
<li>for: 这个研究的目的是为了检测和识别深度学习模型面对攻击时所产生的攻击方法。</li>
<li>methods: 这个研究使用了一个新的架构，叫做GLOF（Global-LOcal Feature）模组，它可以将攻击示例中的特征提取出来，并且用于识别攻击的方法。</li>
<li>results: 这个研究使用了一个大量的攻击识别数据集（AID），包含了180,000个攻击示例，并通过使用GLOF模组进行攻击识别，获得了多个有趣的比较结果。<details>
<summary>Abstract</summary>
Intrinsic susceptibility of deep learning to adversarial examples has led to a plethora of attack techniques with a broad common objective of fooling deep models. However, we find slight compositional differences between the algorithms achieving this objective. These differences leave traces that provide important clues for attacker profiling in real-life scenarios. Inspired by this, we introduce a novel problem of PRofiling Adversarial aTtacks (PRAT). Given an adversarial example, the objective of PRAT is to identify the attack used to generate it. Under this perspective, we can systematically group existing attacks into different families, leading to the sub-problem of attack family identification, which we also study. To enable PRAT analysis, we introduce a large Adversarial Identification Dataset (AID), comprising over 180k adversarial samples generated with 13 popular attacks for image specific/agnostic white/black box setups. We use AID to devise a novel framework for the PRAT objective. Our framework utilizes a Transformer based Global-LOcal Feature (GLOF) module to extract an approximate signature of the adversarial attack, which in turn is used for the identification of the attack. Using AID and our framework, we provide multiple interesting benchmark results for the PRAT problem.
</details>
<details>
<summary>摘要</summary>
深度学习内置的攻击例子感受性问题，导致了许多攻击技术的出现，它们的共同目标都是欺骗深度模型。然而，我们发现这些攻击技术之间存在轻微的组合差异，这些差异留下了重要的攻击者追踪 traces。 inspirited by this，我们提出了一个新的问题：PRofiling Adversarial aTtacks（PRAT）。给定一个攻击例子，PRAT 的目标是确定攻击该例子的攻击方法。基于这种视角，我们可以系统地将现有的攻击分为不同的家族，导致了攻击家族识别问题的研究，我们也进行了这种研究。为了启用 PRAT 分析，我们提出了一个大型的攻击标识数据集（AID），包含了180k多个生成了13种流行的攻击的攻击示例，用于黑色/白色盒子设置。我们使用 AID 和我们的框架，提出了一种新的框架来实现 PRAT 目标。我们的框架使用 Transformer 基于的全局-本地特征（GLOF）模块，将攻击例子中的攻击特征提取出来，并用于攻击的识别。使用 AID 和我们的框架，我们提供了多个有趣的 PRAT 问题的 benchmark 结果。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Domain-agnostic-Domain-Adaptation-for-Satellite-Images"><a href="#Self-supervised-Domain-agnostic-Domain-Adaptation-for-Satellite-Images" class="headerlink" title="Self-supervised Domain-agnostic Domain Adaptation for Satellite Images"></a>Self-supervised Domain-agnostic Domain Adaptation for Satellite Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11109">http://arxiv.org/abs/2309.11109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fahong Zhang, Yilei Shi, Xiao Xiang Zhu</li>
<li>for:  Addressing the domain shift issue in machine learning for global scale satellite image processing.</li>
<li>methods:  Proposed an self-supervised domain-agnostic domain adaptation (SS(DA)2) method, which uses a contrastive generative adversarial loss to train a generative network for image-to-image translation, and improves the generalizability of downstream models by augmenting the training data with different testing spectral characteristics.</li>
<li>results:  Experimental results on public benchmarks verified the effectiveness of SS(DA)2.<details>
<summary>Abstract</summary>
Domain shift caused by, e.g., different geographical regions or acquisition conditions is a common issue in machine learning for global scale satellite image processing. A promising method to address this problem is domain adaptation, where the training and the testing datasets are split into two or multiple domains according to their distributions, and an adaptation method is applied to improve the generalizability of the model on the testing dataset. However, defining the domain to which each satellite image belongs is not trivial, especially under large-scale multi-temporal and multi-sensory scenarios, where a single image mosaic could be generated from multiple data sources. In this paper, we propose an self-supervised domain-agnostic domain adaptation (SS(DA)2) method to perform domain adaptation without such a domain definition. To achieve this, we first design a contrastive generative adversarial loss to train a generative network to perform image-to-image translation between any two satellite image patches. Then, we improve the generalizability of the downstream models by augmenting the training data with different testing spectral characteristics. The experimental results on public benchmarks verify the effectiveness of SS(DA)2.
</details>
<details>
<summary>摘要</summary>
域外转移问题，如不同地理区域或获取条件，是机器学习在全球范围卫星图像处理中的常见问题。一种有前途的方法是领域适应，其中训练集和测试集被分成两个或多个领域，并应用适应方法以提高测试集模型的泛化性。然而，定义各卫星图像归属的领域并不是易事，尤其在大规模多时间和多感器场景下，一个卫星图像融合可能来自多个数据源。在这篇论文中，我们提出了一种自主适应领域无关的自动适应（SS(DA)2）方法，无需定义各卫星图像的领域。为此，我们首先设计了一种对比生成隐藏层的挑战推荐损失，以训练生成网络进行卫星图像块之间的自动翻译。然后，我们通过增加不同测试spectral特征来提高下游模型的泛化性。实验结果表明，SS(DA)2有效地解决了域外转移问题。
</details></li>
</ul>
<hr>
<h2 id="Forgery-aware-Adaptive-Vision-Transformer-for-Face-Forgery-Detection"><a href="#Forgery-aware-Adaptive-Vision-Transformer-for-Face-Forgery-Detection" class="headerlink" title="Forgery-aware Adaptive Vision Transformer for Face Forgery Detection"></a>Forgery-aware Adaptive Vision Transformer for Face Forgery Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11092">http://arxiv.org/abs/2309.11092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anwei Luo, Rizhao Cai, Chenqi Kong, Xiangui Kang, Jiwu Huang, Alex C. Kot</li>
<li>for: 保护 authentication 完整性，防止 face 伪造攻击。</li>
<li>methods: 提出了一种 Novel Forgery-aware Adaptive Vision Transformer (FA-ViT)，具有冻结 vanilla ViT 的参数，并采用 Local-aware Forgery Injector (LFI) 和 Global-aware Forgery Adaptor (GFA) 两种特殊组件，以适应伪造相关的知识。</li>
<li>results: 实验表明，我们的 FA-ViT 在 cross-dataset 评估和 cross- manipulate 场景中达到了状态机器人的性能，并提高了对未经看到的干扰的Robustness。<details>
<summary>Abstract</summary>
With the advancement in face manipulation technologies, the importance of face forgery detection in protecting authentication integrity becomes increasingly evident. Previous Vision Transformer (ViT)-based detectors have demonstrated subpar performance in cross-database evaluations, primarily because fully fine-tuning with limited Deepfake data often leads to forgetting pre-trained knowledge and over-fitting to data-specific ones. To circumvent these issues, we propose a novel Forgery-aware Adaptive Vision Transformer (FA-ViT). In FA-ViT, the vanilla ViT's parameters are frozen to preserve its pre-trained knowledge, while two specially designed components, the Local-aware Forgery Injector (LFI) and the Global-aware Forgery Adaptor (GFA), are employed to adapt forgery-related knowledge. our proposed FA-ViT effectively combines these two different types of knowledge to form the general forgery features for detecting Deepfakes. Specifically, LFI captures local discriminative information and incorporates these information into ViT via Neighborhood-Preserving Cross Attention (NPCA). Simultaneously, GFA learns adaptive knowledge in the self-attention layer, bridging the gap between the two different domain. Furthermore, we design a novel Single Domain Pairwise Learning (SDPL) to facilitate fine-grained information learning in FA-ViT. The extensive experiments demonstrate that our FA-ViT achieves state-of-the-art performance in cross-dataset evaluation and cross-manipulation scenarios, and improves the robustness against unseen perturbations.
</details>
<details>
<summary>摘要</summary>
随着人脸杜撰技术的发展，保护身份验证的 authenticty integrity 成为越来越重要的。先前的 Vision Transformer (ViT) 基于的检测器在跨数据库评估中表现不佳，主要因为完全精度调整 WITH 有限的 Deepfake 数据通常会导致忘记预训练知识并过拟合数据库specific 的知识。为了解决这些问题，我们提出了一种 novel Forgery-aware Adaptive Vision Transformer (FA-ViT)。在 FA-ViT 中，vanilla ViT 的参数被冻结，以保持其预训练的知识。同时，我们采用了两个特制的组件：Local-aware Forgery Injector (LFI) 和 Global-aware Forgery Adaptor (GFA)。LFI 捕捉了地方特征信息，并将这些信息与 Neighborhood-Preserving Cross Attention (NPCA) 结合，以便在 ViT 中捕捉到地方特征。而 GFA 在自注意层中学习了适应性知识， bridging the gap  между两种不同的领域。此外，我们还设计了一种 novel Single Domain Pairwise Learning (SDPL)，以便在 FA-ViT 中进行细化信息学习。广泛的实验表明，我们的 FA-ViT 在跨数据库评估和跨杜撰场景中表现出了 state-of-the-art 的性能，并且能够对未经见杜撰的攻击进行鲁棒化。
</details></li>
</ul>
<hr>
<h2 id="Learning-Segment-Similarity-and-Alignment-in-Large-Scale-Content-Based-Video-Retrieval"><a href="#Learning-Segment-Similarity-and-Alignment-in-Large-Scale-Content-Based-Video-Retrieval" class="headerlink" title="Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval"></a>Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11091">http://arxiv.org/abs/2309.11091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Jiang, Kaiming Huang, Sifeng He, Xudong Yang, Wei Zhang, Xiaobo Zhang, Yuan Cheng, Lei Yang, Qing Wang, Furong Xu, Tan Pan, Wei Chu</li>
<li>for: 这篇论文主要旨在提高内容基于视频检索（CBVR）的精度和效率，尤其是在长视频场景下。</li>
<li>methods: 该论文提出了一种基于自助学习的 Segment Similarity and Alignment Network (SSAN)，包括两个新提出的模块：(1) 高效的自动生成关键帧EXTraction（SKE）模块，(2) 稳定的 Similarity Pattern Detection（SPD）模块。</li>
<li>results: 对于公共数据集的实验结果表明，SSAN可以获得更高的Alignment精度，同时减少存储和在线查询计算成本，比既有方法更高。<details>
<summary>Abstract</summary>
With the explosive growth of web videos in recent years, large-scale Content-Based Video Retrieval (CBVR) becomes increasingly essential in video filtering, recommendation, and copyright protection. Segment-level CBVR (S-CBVR) locates the start and end time of similar segments in finer granularity, which is beneficial for user browsing efficiency and infringement detection especially in long video scenarios. The challenge of S-CBVR task is how to achieve high temporal alignment accuracy with efficient computation and low storage consumption. In this paper, we propose a Segment Similarity and Alignment Network (SSAN) in dealing with the challenge which is firstly trained end-to-end in S-CBVR. SSAN is based on two newly proposed modules in video retrieval: (1) An efficient Self-supervised Keyframe Extraction (SKE) module to reduce redundant frame features, (2) A robust Similarity Pattern Detection (SPD) module for temporal alignment. In comparison with uniform frame extraction, SKE not only saves feature storage and search time, but also introduces comparable accuracy and limited extra computation time. In terms of temporal alignment, SPD localizes similar segments with higher accuracy and efficiency than existing deep learning methods. Furthermore, we jointly train SSAN with SKE and SPD and achieve an end-to-end improvement. Meanwhile, the two key modules SKE and SPD can also be effectively inserted into other video retrieval pipelines and gain considerable performance improvements. Experimental results on public datasets show that SSAN can obtain higher alignment accuracy while saving storage and online query computational cost compared to existing methods.
</details>
<details>
<summary>摘要</summary>
随着网络视频的快速增长，大规模的内容基于视频检索（CBVR）在视频筛选、推荐和版权保护中变得越来越重要。segment级CBVR（S-CBVR）可以在更细粒度上定位相似的分割时间，这对用户浏览效率和侵权检测尤为重要，特别是在长视频场景下。S-CBVR任务的挑战是如何实现高精度时间对对应和高效计算且快速存储消耗。在这篇论文中，我们提出了一种Segment Similarity and Alignment Network（SSAN）来解决这个挑战。SSAN基于两个新提出的模块：（1）高效的自动学习键帧EXTRACTION（SKE）模块，以减少缓存和搜索时间，同时保持相似性和精度；（2）Robust的同时间模式检测（SPD）模块，用于时间对对应。相比于固定帧EXTRACTION，SKE不仅减少了特征存储和搜索时间，还引入了相似的准确性和有限的额外计算时间。在时间对对应方面，SPD可以更高精度地local化相似分割，而且更高效 than现有的深度学习方法。此外，我们将SSAN、SKE和SPD联合训练，实现了端到端提升。此外，这两个关键模块也可以在其他视频检索管道中插入，并获得显著性能提升。实验结果表明，SSAN可以在公共数据集上获得更高的对应精度，同时减少存储和在线查询计算成本。
</details></li>
</ul>
<hr>
<h2 id="Dense-2D-3D-Indoor-Prediction-with-Sound-via-Aligned-Cross-Modal-Distillation"><a href="#Dense-2D-3D-Indoor-Prediction-with-Sound-via-Aligned-Cross-Modal-Distillation" class="headerlink" title="Dense 2D-3D Indoor Prediction with Sound via Aligned Cross-Modal Distillation"></a>Dense 2D-3D Indoor Prediction with Sound via Aligned Cross-Modal Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11081">http://arxiv.org/abs/2309.11081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heeseung Yun, Joonil Na, Gunhee Kim</li>
<li>for: 这篇论文旨在探讨如何使深度网络拥有空间逻辑能力，以便在我们日常生活中更好地利用声音信息。</li>
<li>methods: 该论文提出了一种名为“匹配引导”（SAM）的知识填充框架，用于在视觉知识传输中匹配地址问题。该框架将听音特征与视觉准确的学习空间嵌入结合起来，以解决多层学生模型中的不一致性问题。</li>
<li>results: 该论文通过一个新创建的封闭预测数据集（DAPS），成功地解决了indoor dense prediction问题，包括声音基础 depth estimation、semantic segmentation和3D场景重建等问题。在不同的metric和后处理架构下，该distillation框架一致地实现了状态的最佳性能。<details>
<summary>Abstract</summary>
Sound can convey significant information for spatial reasoning in our daily lives. To endow deep networks with such ability, we address the challenge of dense indoor prediction with sound in both 2D and 3D via cross-modal knowledge distillation. In this work, we propose a Spatial Alignment via Matching (SAM) distillation framework that elicits local correspondence between the two modalities in vision-to-audio knowledge transfer. SAM integrates audio features with visually coherent learnable spatial embeddings to resolve inconsistencies in multiple layers of a student model. Our approach does not rely on a specific input representation, allowing for flexibility in the input shapes or dimensions without performance degradation. With a newly curated benchmark named Dense Auditory Prediction of Surroundings (DAPS), we are the first to tackle dense indoor prediction of omnidirectional surroundings in both 2D and 3D with audio observations. Specifically, for audio-based depth estimation, semantic segmentation, and challenging 3D scene reconstruction, the proposed distillation framework consistently achieves state-of-the-art performance across various metrics and backbone architectures.
</details>
<details>
<summary>摘要</summary>
声音可以传递重要的信息来帮助我们日常准备空间理解。为了让深度网络具备这种能力，我们在视Audio知识传递中处理紧凑的室内预测问题。在这项工作中，我们提出了一种名为匹配（SAM）知识传递框架，该框架在视Audio知识传递中找到本地匹配点，以解决多层学习模型中的不一致。SAM将音频特征与视觉一致的学习可变的空间嵌入结合起来，以解决多层学习模型中的不一致。我们的方法不依赖特定的输入表示，因此可以在输入形状或维度上进行灵活的调整无论影响性。我们新编制了一个名为环境预测（DAPS）的权威数据集，我们是第一个在2D和3D室内环境预测中使用音频观察结果进行密集预测。特别是，我们的框架在音频基于深度估计、语义分割和复杂3D场景重建等方面均实现了状态的最佳性。
</details></li>
</ul>
<hr>
<h2 id="Visual-Question-Answering-in-the-Medical-Domain"><a href="#Visual-Question-Answering-in-the-Medical-Domain" class="headerlink" title="Visual Question Answering in the Medical Domain"></a>Visual Question Answering in the Medical Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11080">http://arxiv.org/abs/2309.11080</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abachaa/VQA-Med-2019">https://github.com/abachaa/VQA-Med-2019</a></li>
<li>paper_authors: Louisa Canepa, Sonit Singh, Arcot Sowmya</li>
<li>for: 这篇研究旨在提出一个适应医疗图像问题的机器学习模型，以解答基于 givent medical images 的自然语言问题。</li>
<li>methods: 本研究使用了专门的领域预训练策略，包括一种新的对称学习预训方法，以减少小规模 dataset 的问题。</li>
<li>results: 我们的提案模型在 VQA-Med 2019 测试集上获得了60%的准确率，与其他州OF-the-art Med-VQA 模型相当。<details>
<summary>Abstract</summary>
Medical visual question answering (Med-VQA) is a machine learning task that aims to create a system that can answer natural language questions based on given medical images. Although there has been rapid progress on the general VQA task, less progress has been made on Med-VQA due to the lack of large-scale annotated datasets. In this paper, we present domain-specific pre-training strategies, including a novel contrastive learning pretraining method, to mitigate the problem of small datasets for the Med-VQA task. We find that the model benefits from components that use fewer parameters. We also evaluate and discuss the model's visual reasoning using evidence verification techniques. Our proposed model obtained an accuracy of 60% on the VQA-Med 2019 test set, giving comparable results to other state-of-the-art Med-VQA models.
</details>
<details>
<summary>摘要</summary>
医学视觉问答（Med-VQA）是一种机器学习任务，旨在创建一个能够根据给定医学图像回答自然语言问题的系统。虽然总体VQA任务上有了快速的进步，但Med-VQA任务上的进步较少，这主要归结于医学图像数据的小规模。在这篇论文中，我们提出了域特定预训练策略，包括一种新的对比学习预训练方法，以解决Med-VQA任务的数据小规模问题。我们发现模型受到参数数量的限制具有好处。我们还评估和讨论模型的视觉逻辑使用证明技术。我们的提议的模型在VQA-Med 2019测试集上取得了60%的准确率，与其他状态之前的Med-VQA模型相当。
</details></li>
</ul>
<hr>
<h2 id="Score-Mismatching-for-Generative-Modeling"><a href="#Score-Mismatching-for-Generative-Modeling" class="headerlink" title="Score Mismatching for Generative Modeling"></a>Score Mismatching for Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11043">http://arxiv.org/abs/2309.11043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/senmaoy/Score-Mismatching">https://github.com/senmaoy/Score-Mismatching</a></li>
<li>paper_authors: Senmao Ye, Fei Liu</li>
<li>for: 这篇论文的目的是提出一种新的分数基本模型，用于生成图像。</li>
<li>methods: 这篇论文使用了一步采样方法，取代了之前的迭代采样方法。在这个模型中，一个独立的生成器将所有的时间步采样压缩到了梯度反propagation来自分数网络。</li>
<li>results: 这篇论文的模型在CIFAR-10数据集上比Consistency Model和Denoising Score Matching更高效，这表明了这种框架的潜在力量。此外，模型还在MINIST和LSUN数据集上进行了更多的示例。代码可以在GitHub上下载。<details>
<summary>Abstract</summary>
We propose a new score-based model with one-step sampling. Previously, score-based models were burdened with heavy computations due to iterative sampling. For substituting the iterative process, we train a standalone generator to compress all the time steps with the gradient backpropagated from the score network. In order to produce meaningful gradients for the generator, the score network is trained to simultaneously match the real data distribution and mismatch the fake data distribution. This model has the following advantages: 1) For sampling, it generates a fake image with only one step forward. 2) For training, it only needs 10 diffusion steps.3) Compared with consistency model, it is free of the ill-posed problem caused by consistency loss. On the popular CIFAR-10 dataset, our model outperforms Consistency Model and Denoising Score Matching, which demonstrates the potential of the framework. We further provide more examples on the MINIST and LSUN datasets. The code is available on GitHub.
</details>
<details>
<summary>摘要</summary>
我们提出了一个新的分数基于模型，使用单步采样。在过去，分数基于模型受到迭代采样的计算压力。为了替代迭代过程，我们训练了一个独立的生成器，使其在分数网络的梯度归整下压缩所有时间步。为了生成有意义的梯度，分数网络需要同时匹配真实数据分布和假数据分布。这个模型具有以下优点：1）采样时只需一步前进。2）训练时只需10步扩散。3）与一致性模型相比，它免受一致性损失导致的糟糕问题。在流行的 CIFAR-10 数据集上，我们的模型超越了一致性模型和杂噪分匹配模型，这表明了该框架的潜力。我们还提供了更多的例子在 MINIST 和 LSUN 数据集上。代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="CaveSeg-Deep-Semantic-Segmentation-and-Scene-Parsing-for-Autonomous-Underwater-Cave-Exploration"><a href="#CaveSeg-Deep-Semantic-Segmentation-and-Scene-Parsing-for-Autonomous-Underwater-Cave-Exploration" class="headerlink" title="CaveSeg: Deep Semantic Segmentation and Scene Parsing for Autonomous Underwater Cave Exploration"></a>CaveSeg: Deep Semantic Segmentation and Scene Parsing for Autonomous Underwater Cave Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11038">http://arxiv.org/abs/2309.11038</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Abdullah, T. Barua, R. Tibbetts, Z. Chen, M. J. Islam, I. Rekleitis</li>
<li>for: 本研究开发了一个用于潜水洞穴探索和地图创建的自主潜水器视觉学习管线，协助AUV在潜水洞穴环境中快速完成 semantic segmentation 和 scene parsing。</li>
<li>methods: 本研究使用了一个具有全面性的数据集，以便对潜水洞穴场景进行semantic segmentation，并开发了一个基于 transformer 的视觉模型，具有快速执行和低 computational complexity。</li>
<li>results: 本研究通过在美国、墨西哥和西班牙的洞穴系统进行了 comprehensive benchmark 分析，证明了可以透过 CaveSeg 发展出高性能的深度视觉模型，并且在实际应用中实现了快速的 semantic scene parsing。<details>
<summary>Abstract</summary>
In this paper, we present CaveSeg - the first visual learning pipeline for semantic segmentation and scene parsing for AUV navigation inside underwater caves. We address the problem of scarce annotated training data by preparing a comprehensive dataset for semantic segmentation of underwater cave scenes. It contains pixel annotations for important navigation markers (e.g. caveline, arrows), obstacles (e.g. ground plain and overhead layers), scuba divers, and open areas for servoing. Through comprehensive benchmark analyses on cave systems in USA, Mexico, and Spain locations, we demonstrate that robust deep visual models can be developed based on CaveSeg for fast semantic scene parsing of underwater cave environments. In particular, we formulate a novel transformer-based model that is computationally light and offers near real-time execution in addition to achieving state-of-the-art performance. Finally, we explore the design choices and implications of semantic segmentation for visual servoing by AUVs inside underwater caves. The proposed model and benchmark dataset open up promising opportunities for future research in autonomous underwater cave exploration and mapping.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了CaveSeg，首个用于semantic segmentation和场景分解的AUV内水洞环境视觉学习管道。我们解决了罕见的注释培训数据的问题，prepare了包含重要导航标记（例如， cave line、箭头）、障碍物（例如，地面层和天花板层）、潜水员和开放区域的像素注释。通过对美国、墨西哥和西班牙等地水洞系统进行了全面的比较分析，我们证明了可以基于CaveSeg构建Robust的深度视觉模型，用于快速semantic scene parsing水洞环境。尤其是，我们提出了一种新的 transformer-based 模型，具有较少计算量和实时执行能力，同时也达到了状态实验室的性能。最后，我们探讨了semantic segmentation对AUV内水洞环境的视ervoking的设计选择和意义。提出的模型和数据集开 up了未来水洞exploration和 mapping 的可能性。
</details></li>
</ul>
<hr>
<h2 id="Light-Field-Diffusion-for-Single-View-Novel-View-Synthesis"><a href="#Light-Field-Diffusion-for-Single-View-Novel-View-Synthesis" class="headerlink" title="Light Field Diffusion for Single-View Novel View Synthesis"></a>Light Field Diffusion for Single-View Novel View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11525">http://arxiv.org/abs/2309.11525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifeng Xiong, Haoyu Ma, Shanlin Sun, Kun Han, Xiaohui Xie</li>
<li>for: 单视图新视角合成，生成基于单个参考图像的图像，是计算机视觉领域中一项重要但具有挑战性的任务。</li>
<li>methods: 我们使用Light Field Diffusion（LFD）模型，这是一种基于扩散的增强模型，在扩散过程中将摄像头视角信息转换为光场编码，并与参考图像相结合。这种设计引入了本地像素级别的约束，从而促进了多视图一致性。</li>
<li>results: 我们的LFD可以高效地生成高质量图像，并在复杂的区域中保持更好的3D一致性。我们的方法可以与NeRF-based模型相比，并且我们的模型规模只是NeRF-based模型的一半。<details>
<summary>Abstract</summary>
Single-view novel view synthesis, the task of generating images from new viewpoints based on a single reference image, is an important but challenging task in computer vision. Recently, Denoising Diffusion Probabilistic Model (DDPM) has become popular in this area due to its strong ability to generate high-fidelity images. However, current diffusion-based methods directly rely on camera pose matrices as viewing conditions, globally and implicitly introducing 3D constraints. These methods may suffer from inconsistency among generated images from different perspectives, especially in regions with intricate textures and structures. In this work, we present Light Field Diffusion (LFD), a conditional diffusion-based model for single-view novel view synthesis. Unlike previous methods that employ camera pose matrices, LFD transforms the camera view information into light field encoding and combines it with the reference image. This design introduces local pixel-wise constraints within the diffusion models, thereby encouraging better multi-view consistency. Experiments on several datasets show that our LFD can efficiently generate high-fidelity images and maintain better 3D consistency even in intricate regions. Our method can generate images with higher quality than NeRF-based models, and we obtain sample quality similar to other diffusion-based models but with only one-third of the model size.
</details>
<details>
<summary>摘要</summary>
单视图novel视觉合成问题，即基于单个参考图像生成新视点图像，是计算机视觉中重要但困难的任务。最近，Denosing Diffusion Probabilistic Model (DDPM) 在这个领域中得到了广泛应用，因为它可以生成高品质图像。然而，当前的扩散基本方法直接使用摄像机pose矩阵作为视图条件，全局和强制性地引入3D约束。这些方法可能在不同视点图像中生成的图像之间存在不一致，特别是在具有复杂 текстура和结构的区域中。在这种情况下，我们提出了Light Field Diffusion (LFD)，一种基于条件扩散的单视图novel视觉合成模型。与之前的方法不同，LFD将摄像机视角信息转换为光场编码，并将其与参考图像相结合。这种设计引入了本地像素级别的扩散模型中的约束，从而鼓励更好的多视图一致性。我们的LFD可以高效地生成高品质图像，并在复杂区域中保持更好的3D一致性。我们的方法可以生成图像质量高于NeRF-based模型，并且在模型大小方面与其他扩散基本方法相当，但只需一半的模型大小。
</details></li>
</ul>
<hr>
<h2 id="Conformalized-Multimodal-Uncertainty-Regression-and-Reasoning"><a href="#Conformalized-Multimodal-Uncertainty-Regression-and-Reasoning" class="headerlink" title="Conformalized Multimodal Uncertainty Regression and Reasoning"></a>Conformalized Multimodal Uncertainty Regression and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11018">http://arxiv.org/abs/2309.11018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Domenico Parente, Nastaran Darabi, Alex C. Stutts, Theja Tulabandhula, Amit Ranjan Trivedi</li>
<li>for: 这篇论文旨在探讨一种轻量级的不确定度估计器，可以预测多modal（分离）的不确定度 bound，通过将конформаル预测与深度学习回推器结合起来。</li>
<li>methods: 这篇论文使用了将конформаル预测与深度学习回推器结合起来，以预测多modal（分离）的不确定度 bound。</li>
<li>results:  simulations 结果显示，在我们的框架中，不确定度估计器适应了具有严重噪音、有限训练数据和有限预测模型大小的问题。此外，我们开发了一个理解框架，利用这些可靠的不确定度估计器，并与光流基于的理解来提高预测精度。因此，通过适当地考虑数据驱动学习中的预测不确定性，并透过规律基于的理解来关闭预测模型的估计loop，我们的方法在所有这些问题上显著超越了传统的深度学习方法，实际上降低预测错误的比例为2-3倍。<details>
<summary>Abstract</summary>
This paper introduces a lightweight uncertainty estimator capable of predicting multimodal (disjoint) uncertainty bounds by integrating conformal prediction with a deep-learning regressor. We specifically discuss its application for visual odometry (VO), where environmental features such as flying domain symmetries and sensor measurements under ambiguities and occlusion can result in multimodal uncertainties. Our simulation results show that uncertainty estimates in our framework adapt sample-wise against challenging operating conditions such as pronounced noise, limited training data, and limited parametric size of the prediction model. We also develop a reasoning framework that leverages these robust uncertainty estimates and incorporates optical flow-based reasoning to improve prediction prediction accuracy. Thus, by appropriately accounting for predictive uncertainties of data-driven learning and closing their estimation loop via rule-based reasoning, our methodology consistently surpasses conventional deep learning approaches on all these challenging scenarios--pronounced noise, limited training data, and limited model size-reducing the prediction error by 2-3x.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文介绍了一种轻量级的不确定性估计器，可以通过将 конформальный预测与深度学习回归器结合来预测多Modal不确定性 bound。我们特别探讨了它在视觉运动（VO）中的应用， где environmental features和感知测量在异常和遮挡下可能导致多Modal不确定性。我们的 simulations 表明，在我们的框架中的不确定性估计适应样本所对抗复杂的运行条件，如强度的噪音、有限的训练数据和有限的预测模型大小。我们还开发了一种使用这些稳健的不确定性估计和基于推Flow的reasoning Framework来提高预测准确性。因此，通过合理地考虑数据驱动学习的预测不确定性和关闭其估计循环 via 规则基于的reasoning，我们的方法在所有这些复杂的 scenarios中一直赶在深度学习方法之前，减少预测错误 by 2-3倍。
</details></li>
</ul>
<hr>
<h2 id="Controllable-Dynamic-Appearance-for-Neural-3D-Portraits"><a href="#Controllable-Dynamic-Appearance-for-Neural-3D-Portraits" class="headerlink" title="Controllable Dynamic Appearance for Neural 3D Portraits"></a>Controllable Dynamic Appearance for Neural 3D Portraits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11009">http://arxiv.org/abs/2309.11009</a></li>
<li>repo_url: None</li>
<li>paper_authors: ShahRukh Athar, Zhixin Shu, Zexiang Xu, Fujun Luan, Sai Bi, Kalyan Sunkavalli, Dimitris Samaras</li>
<li>for: 创建完全可控的3D人物头像，在真实捕捉环境中。</li>
<li>methods: 使用NeRF技术，通过动态出现模型来 aproximate照明依赖的效果，并通过面法导向来准确预测表면法向量。</li>
<li>results: 使用短视频 captured with smartphone，在不同的头部姿势和表情控制下实现了高质量的自由视 sintesis效果，并且能够模拟真实的照明效果。<details>
<summary>Abstract</summary>
Recent advances in Neural Radiance Fields (NeRFs) have made it possible to reconstruct and reanimate dynamic portrait scenes with control over head-pose, facial expressions and viewing direction. However, training such models assumes photometric consistency over the deformed region e.g. the face must be evenly lit as it deforms with changing head-pose and facial expression. Such photometric consistency across frames of a video is hard to maintain, even in studio environments, thus making the created reanimatable neural portraits prone to artifacts during reanimation. In this work, we propose CoDyNeRF, a system that enables the creation of fully controllable 3D portraits in real-world capture conditions. CoDyNeRF learns to approximate illumination dependent effects via a dynamic appearance model in the canonical space that is conditioned on predicted surface normals and the facial expressions and head-pose deformations. The surface normals prediction is guided using 3DMM normals that act as a coarse prior for the normals of the human head, where direct prediction of normals is hard due to rigid and non-rigid deformations induced by head-pose and facial expression changes. Using only a smartphone-captured short video of a subject for training, we demonstrate the effectiveness of our method on free view synthesis of a portrait scene with explicit head pose and expression controls, and realistic lighting effects. The project page can be found here: http://shahrukhathar.github.io/2023/08/22/CoDyNeRF.html
</details>
<details>
<summary>摘要</summary>
最近的神经辐射场（NeRF）技术突破，使得可以重建和复活动态肖像场景，包括头部姿态和表情的控制。然而，训练这些模型时需要光ometric consistency over the deformed region，例如脸部必须在不同的头部姿态和表情变化中保持光度的均匀性。这种光度一致性在视频帧中很难保持，即使在studio environment中，因此创建的可控3D肖像容易出现artifacts during reanimation。在这项工作中，我们提出了CoDyNeRF系统，可以在真实的捕捉条件下创建完全可控的3D肖像。CoDyNeRF通过learns to approximate illumination dependent effects via a dynamic appearance model in the canonical space that is conditioned on predicted surface normals and the facial expressions and head-pose deformations来解决这个问题。 surface normals prediction是通过3DMM normals作为一个粗略的估计器来引导的，因为direct prediction of normals是由于头部姿态和表情变化induced的固定和非固定扭曲而困难。通过只使用短视频 capture的智能手机训练，我们示示了我们的方法在free view synthesis of a portrait scene with explicit head pose and expression controls, and realistic lighting effects。相关项目页面可以在以下链接中找到：http://shahrukhathar.github.io/2023/08/22/CoDyNeRF.html
</details></li>
</ul>
<hr>
<h2 id="STARNet-Sensor-Trustworthiness-and-Anomaly-Recognition-via-Approximated-Likelihood-Regret-for-Robust-Edge-Autonomy"><a href="#STARNet-Sensor-Trustworthiness-and-Anomaly-Recognition-via-Approximated-Likelihood-Regret-for-Robust-Edge-Autonomy" class="headerlink" title="STARNet: Sensor Trustworthiness and Anomaly Recognition via Approximated Likelihood Regret for Robust Edge Autonomy"></a>STARNet: Sensor Trustworthiness and Anomaly Recognition via Approximated Likelihood Regret for Robust Edge Autonomy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11006">http://arxiv.org/abs/2309.11006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sinatayebati/STARNet">https://github.com/sinatayebati/STARNet</a></li>
<li>paper_authors: Nastaran Darabi, Sina Tayebati, Sureshkumar S., Sathya Ravi, Theja Tulabandhula, Amit R. Trivedi</li>
<li>for: This paper is written to address the reliability concerns of complex sensors such as LiDAR and camera sensors in autonomous robotics, and to improve the prediction accuracy of deep learning models by detecting untrustworthy sensor streams.</li>
<li>methods: STARNet, a Sensor Trustworthiness and Anomaly Recognition Network, is used to detect untrustworthy sensor streams. STARNet employs the concept of approximated likelihood regret, a gradient-free framework tailored for low-complexity hardware.</li>
<li>results: STARNet enhances prediction accuracy by approximately 10% by filtering out untrustworthy sensor streams in unimodal and multimodal settings, especially in addressing internal sensor failures such as cross-sensor interference and crosstalk.<details>
<summary>Abstract</summary>
Complex sensors such as LiDAR, RADAR, and event cameras have proliferated in autonomous robotics to enhance perception and understanding of the environment. Meanwhile, these sensors are also vulnerable to diverse failure mechanisms that can intricately interact with their operation environment. In parallel, the limited availability of training data on complex sensors also affects the reliability of their deep learning-based prediction flow, where their prediction models can fail to generalize to environments not adequately captured in the training set. To address these reliability concerns, this paper introduces STARNet, a Sensor Trustworthiness and Anomaly Recognition Network designed to detect untrustworthy sensor streams that may arise from sensor malfunctions and/or challenging environments. We specifically benchmark STARNet on LiDAR and camera data. STARNet employs the concept of approximated likelihood regret, a gradient-free framework tailored for low-complexity hardware, especially those with only fixed-point precision capabilities. Through extensive simulations, we demonstrate the efficacy of STARNet in detecting untrustworthy sensor streams in unimodal and multimodal settings. In particular, the network shows superior performance in addressing internal sensor failures, such as cross-sensor interference and crosstalk. In diverse test scenarios involving adverse weather and sensor malfunctions, we show that STARNet enhances prediction accuracy by approximately 10% by filtering out untrustworthy sensor streams. STARNet is publicly available at \url{https://github.com/sinatayebati/STARNet}.
</details>
<details>
<summary>摘要</summary>
复杂的感知器如LiDAR、RADAR和事件摄像头在自主 робо扮中广泛应用，以提高环境的感知和理解。然而，这些感知器也面临着多种失效机制，这些失效机制可能与其运行环境互相复杂交互。同时，对于复杂的感知器，有限的训练数据也会影响其深度学习基于预测流的可靠性，其预测模型可能无法泛化到不充分 captured 的环境中。为解决这些可靠性问题，本文介绍了 STARNet，一种感知器可靠性和异常检测网络，可以检测不可靠的感知流，这些感知流可能由感知器故障和/或挑战环境引起。我们 especifically 对 LiDAR 和摄像头数据进行了 benchmark。STARNet 采用了approximated likelihood regret，一种适用于低复杂度硬件的梯度自由框架。通过广泛的 simulations，我们展示了 STARNet 在单模态和多模态设置下的效果。尤其是，网络在内部感知器故障方面表现出色，如交叉感知和电磁干扰。在多种测试enario中，包括不良天气和感知器故障，我们表明 STARNet 可以提高预测精度约 10%，通过筛选不可靠的感知流。STARNet 公共可用于 \url{https://github.com/sinatayebati/STARNet}.
</details></li>
</ul>
<hr>
<h2 id="PPD-A-New-Valet-Parking-Pedestrian-Fisheye-Dataset-for-Autonomous-Driving"><a href="#PPD-A-New-Valet-Parking-Pedestrian-Fisheye-Dataset-for-Autonomous-Driving" class="headerlink" title="PPD: A New Valet Parking Pedestrian Fisheye Dataset for Autonomous Driving"></a>PPD: A New Valet Parking Pedestrian Fisheye Dataset for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11002">http://arxiv.org/abs/2309.11002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zizhang Wu, Xinyuan Chen, Fan Song, Yuanzhu Gan, Tianhao Xu, Jian Pu, Rui Tang</li>
<li>for: 本研究旨在提供一个大规模的 fisheye 数据集，以支持对实际世界中的步行人进行研究，特别是在干扰和多种姿势下。</li>
<li>methods: 本研究使用 fisheye 摄像头捕捉了多种类型的步行人，并提出了两种数据增强技术来提高基eline。</li>
<li>results: 实验证明了我们的新的数据增强方法的效果，并证明了数据集的非常普遍化。<details>
<summary>Abstract</summary>
Pedestrian detection under valet parking scenarios is fundamental for autonomous driving. However, the presence of pedestrians can be manifested in a variety of ways and postures under imperfect ambient conditions, which can adversely affect detection performance. Furthermore, models trained on publicdatasets that include pedestrians generally provide suboptimal outcomes for these valet parking scenarios. In this paper, wepresent the Parking Pedestrian Dataset (PPD), a large-scale fisheye dataset to support research dealing with real-world pedestrians, especially with occlusions and diverse postures. PPD consists of several distinctive types of pedestrians captured with fisheye cameras. Additionally, we present a pedestrian detection baseline on PPD dataset, and introduce two data augmentation techniques to improve the baseline by enhancing the diversity ofthe original dataset. Extensive experiments validate the effectiveness of our novel data augmentation approaches over baselinesand the dataset's exceptional generalizability.
</details>
<details>
<summary>摘要</summary>
自动驾驶中的人行检测在停车场景下是基本的。然而，人行可以在不同的环境条件下表现出多种形式和姿势，这会 adversely affect 检测性能。尤其是模型通常在公共数据集上训练，这些数据集中的人行通常不适合停车场景。在这篇论文中，我们提出了停车场景人行数据集（PPD），一个大规模的鱼眼数据集，以支持实际世界中的人行检测，特别是干扰和多种姿势。PPD 包括多种特征的人行，通过鱼眼摄像头捕捉。此外，我们还提出了人行检测基线在 PPD 数据集上，并介绍了两种数据增强技术来提高基线，以提高数据集的多样性。广泛的实验证明了我们的新的数据增强方法的有效性，以及数据集的出色的普适性。
</details></li>
</ul>
<hr>
<h2 id="COSE-A-Consistency-Sensitivity-Metric-for-Saliency-on-Image-Classification"><a href="#COSE-A-Consistency-Sensitivity-Metric-for-Saliency-on-Image-Classification" class="headerlink" title="COSE: A Consistency-Sensitivity Metric for Saliency on Image Classification"></a>COSE: A Consistency-Sensitivity Metric for Saliency on Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10989">http://arxiv.org/abs/2309.10989</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cvl-umass/COSE">https://github.com/cvl-umass/COSE</a></li>
<li>paper_authors: Rangel Daroya, Aaron Sun, Subhransu Maji</li>
<li>for: 本研究旨在提供一套基于视觉优先的表现评估方法，用于评估图像分类任务中模型的表现。</li>
<li>methods: 本研究使用了多种视觉焦点映射方法，包括GradCAM、Guided Backpropagation（GBP）和DeepLIFT（DLIFT）等。</li>
<li>results: 研究发现，虽然多种焦点映射方法都能够解释模型决策，但是transformer模型比 convolutional模型更难被这些方法解释。此外，GradCAM表现最佳，但是它在细节化数据集上缺乏多样性。通过对准则和敏感度进行平衡，可以获得一个准确地表示模型行为的焦点映射。<details>
<summary>Abstract</summary>
We present a set of metrics that utilize vision priors to effectively assess the performance of saliency methods on image classification tasks. To understand behavior in deep learning models, many methods provide visual saliency maps emphasizing image regions that most contribute to a model prediction. However, there is limited work on analyzing the reliability of saliency methods in explaining model decisions. We propose the metric COnsistency-SEnsitivity (COSE) that quantifies the equivariant and invariant properties of visual model explanations using simple data augmentations. Through our metrics, we show that although saliency methods are thought to be architecture-independent, most methods could better explain transformer-based models over convolutional-based models. In addition, GradCAM was found to outperform other methods in terms of COSE but was shown to have limitations such as lack of variability for fine-grained datasets. The duality between consistency and sensitivity allow the analysis of saliency methods from different angles. Ultimately, we find that it is important to balance these two metrics for a saliency map to faithfully show model behavior.
</details>
<details>
<summary>摘要</summary>
我们提出了一组维度度量，使用视觉优先来评估针对图像分类任务的精度方法的表现。在深度学习模型中，许多方法提供视觉精度地图，强调图像区域对模型预测的贡献。然而，对于分析深度学习模型决策的可靠性的工作几乎缺乏。我们提出了COnsistency-SEnsitivity（COSE）度量，用于衡量视觉模型解释的等变和不变性。通过我们的度量，我们发现，虽然许多方法被认为是无关于模型结构的，但大多数方法在基于转换器模型时表现较好。此外，GradCAM在COSE方面表现出色，但它在细腻数据上缺乏变化。这种对照性Allow我们从不同角度分析精度方法。最终，我们发现，为了让精度地图准确反映模型行为，需要平衡这两个度量。
</details></li>
</ul>
<hr>
<h2 id="RMT-Retentive-Networks-Meet-Vision-Transformers"><a href="#RMT-Retentive-Networks-Meet-Vision-Transformers" class="headerlink" title="RMT: Retentive Networks Meet Vision Transformers"></a>RMT: Retentive Networks Meet Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11523">http://arxiv.org/abs/2309.11523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qihang Fan, Huaibo Huang, Mingrui Chen, Hongmin Liu, Ran He</li>
<li>for: 本研究的目的是探讨将Retrieval Network（RetNet）的思想应用于计算机视觉领域，以提高计算机视觉任务的性能。</li>
<li>methods: 本研究提出了一种组合RetNet和Transformer的模型，称为RMT。该模型引入了明确的衰减元素，以帮助计算机视觉模型更好地控制各个Token的范围。此外，为了降低全模型的计算成本，我们将模型分解成两个坐标轴上的分割。</li>
<li>results: 我们的RMT在多种计算机视觉任务中表现出色，例如在ImageNet-1k上达到84.1%的Top1-acc，使用了仅4.5G FLOPs。此外，RMT在下游任务中，如物体检测、实例分割和semantic segmentation中也表现出优异。<details>
<summary>Abstract</summary>
Transformer first appears in the field of natural language processing and is later migrated to the computer vision domain, where it demonstrates excellent performance in vision tasks. However, recently, Retentive Network (RetNet) has emerged as an architecture with the potential to replace Transformer, attracting widespread attention in the NLP community. Therefore, we raise the question of whether transferring RetNet's idea to vision can also bring outstanding performance to vision tasks. To address this, we combine RetNet and Transformer to propose RMT. Inspired by RetNet, RMT introduces explicit decay into the vision backbone, bringing prior knowledge related to spatial distances to the vision model. This distance-related spatial prior allows for explicit control of the range of tokens that each token can attend to. Additionally, to reduce the computational cost of global modeling, we decompose this modeling process along the two coordinate axes of the image. Abundant experiments have demonstrated that our RMT exhibits exceptional performance across various computer vision tasks. For example, RMT achieves 84.1% Top1-acc on ImageNet-1k using merely 4.5G FLOPs. To the best of our knowledge, among all models, RMT achieves the highest Top1-acc when models are of similar size and trained with the same strategy. Moreover, RMT significantly outperforms existing vision backbones in downstream tasks such as object detection, instance segmentation, and semantic segmentation. Our work is still in progress.
</details>
<details>
<summary>摘要</summary>
transformer 最初出现在自然语言处理领域，后来迁移到计算机视觉领域，在视觉任务中表现出色。然而，最近，Retentive Network（RetNet） Architecture 出现，吸引了自然语言社区的广泛关注。因此，我们提出了将 RetNet 的想法应用于视觉领域，以提高视觉任务的表现。为此，我们将 RetNet 和 transformer 结合，提出了 RMT。 RetNet 中引入了显式衰减，使视觉模型受到相对距离的知识。这种距离相关的空间先验使每个token可以显式控制所能attend的token范围。此外，为降低全局模型的计算成本，我们将模型化过程分解成两个坐标轴的图像。我们的 RMT 在多个计算机视觉任务中表现出色，例如在 ImageNet-1k 中 achiev 84.1% Top1-acc 使用仅 4.5G FLOPs。我们知道，在同样大小的模型和同样策略下，RMT 在所有模型中具有最高的 Top1-acc。此外，RMT 在下游任务中，如物体检测、实例分割和 semantics 分割，也表现出了显著的优异。我们的工作仍在进行中。
</details></li>
</ul>
<hr>
<h2 id="SEMPART-Self-supervised-Multi-resolution-Partitioning-of-Image-Semantics"><a href="#SEMPART-Self-supervised-Multi-resolution-Partitioning-of-Image-Semantics" class="headerlink" title="SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics"></a>SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10972">http://arxiv.org/abs/2309.10972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sriram Ravindran, Debraj Basu</li>
<li>for: 本文为了解决基于图像数据的稀缺时，准确地定义图像中重要的区域而作出了贡献。</li>
<li>methods: 本文使用了基于DINO的自动编写方法，并利用图像 semantic graph中的含义来找到前景物体。</li>
<li>results: 本文提出了一种名为SEMPART的方法，可以同时确定图像的粗细分割和细分割，并且可以快速生成高质量的mask。<details>
<summary>Abstract</summary>
Accurately determining salient regions of an image is challenging when labeled data is scarce. DINO-based self-supervised approaches have recently leveraged meaningful image semantics captured by patch-wise features for locating foreground objects. Recent methods have also incorporated intuitive priors and demonstrated value in unsupervised methods for object partitioning. In this paper, we propose SEMPART, which jointly infers coarse and fine bi-partitions over an image's DINO-based semantic graph. Furthermore, SEMPART preserves fine boundary details using graph-driven regularization and successfully distills the coarse mask semantics into the fine mask. Our salient object detection and single object localization findings suggest that SEMPART produces high-quality masks rapidly without additional post-processing and benefits from co-optimizing the coarse and fine branches.
</details>
<details>
<summary>摘要</summary>
精确地定义图像中重要区域是一项具有挑战性的任务，尤其当标注数据稀缺时。基于DINO的自动学习方法最近在捕捉图像中具有意义的Semantic Feature中找到了前景对象。现有方法还将直觉约束 incorporated 到了无监督方法中，并在对象分割方面表现出了价值。在这篇论文中，我们提议了 SEMPART，它同时分解图像的DINO基于semantic graph的粗细分割结果。此外，SEMPART还使用图像驱动的正则化来保持细节，并成功地储存粗细分割结果。我们的精确对象检测和单个对象Localization结果表明，SEMPART可以快速生成高质量的Mask，无需额外处理，并且受益于粗细分支的共同优化。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/cs.CV_2023_09_20/" data-id="clnsn0vh900drgf88afnw6wh7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/cs.AI_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T12:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/cs.AI_2023_09_20/">cs.AI - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="RAI4IoE-Responsible-AI-for-Enabling-the-Internet-of-Energy"><a href="#RAI4IoE-Responsible-AI-for-Enabling-the-Internet-of-Energy" class="headerlink" title="RAI4IoE: Responsible AI for Enabling the Internet of Energy"></a>RAI4IoE: Responsible AI for Enabling the Internet of Energy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11691">http://arxiv.org/abs/2309.11691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minhui Xue, Surya Nepal, Ling Liu, Subbu Sethuvenkatraman, Xingliang Yuan, Carsten Rudolph, Ruoxi Sun, Greg Eisenhauer</li>
<li>for: 这项研究旨在开发一个公平且负责任的AI框架，以便在互联网能源（IoE）中实现可靠的能源分布。</li>
<li>methods: 该研究使用了先进的5G-6G网络和AI技术，以连接和 инте格力化可再生分布能源资源（DERs），如电动车、存储电池、风力发电和太阳能电池。这使得DER所有者作为生产者和消费者（prosumers）可以参与能源市场，并从中获得经济收益。</li>
<li>results: 该研究的目标是确保社区成员的公平参与，并负责使用他们的数据，以便在IoE中提供安全、可靠和可再生的能源服务。<details>
<summary>Abstract</summary>
This paper plans to develop an Equitable and Responsible AI framework with enabling techniques and algorithms for the Internet of Energy (IoE), in short, RAI4IoE. The energy sector is going through substantial changes fueled by two key drivers: building a zero-carbon energy sector and the digital transformation of the energy infrastructure. We expect to see the convergence of these two drivers resulting in the IoE, where renewable distributed energy resources (DERs), such as electric cars, storage batteries, wind turbines and photovoltaics (PV), can be connected and integrated for reliable energy distribution by leveraging advanced 5G-6G networks and AI technology. This allows DER owners as prosumers to participate in the energy market and derive economic incentives. DERs are inherently asset-driven and face equitable challenges (i.e., fair, diverse and inclusive). Without equitable access, privileged individuals, groups and organizations can participate and benefit at the cost of disadvantaged groups. The real-time management of DER resources not only brings out the equity problem to the IoE, it also collects highly sensitive location, time, activity dependent data, which requires to be handled responsibly (e.g., privacy, security and safety), for AI-enhanced predictions, optimization and prioritization services, and automated management of flexible resources. The vision of our project is to ensure equitable participation of the community members and responsible use of their data in IoE so that it could reap the benefits of advances in AI to provide safe, reliable and sustainable energy services.
</details>
<details>
<summary>摘要</summary>
这份研究报告计划开发一个公平和负责任的人工智能框架（RAI4IoE），用于互联网能源（IoE）领域。能源领域正在经历重大变革，这两个关键驱动因素：建立零碳素能源产业和能源基础设施的数字变革。我们预计这两个驱动因素会相互交集，导致IoE的出现，其中可再生分布式能源资源（DERs），如电动车、存储电池、风力发电和太阳能电池（PV），可以相互连接和集成，以实现可靠的能源分布，通过利用先进的5G-6G网络和人工智能技术。这允许DER所有者作为生产者和消费者（prosumers）参与能源市场，从而获得经济收益。DERs本身具有资产驱动的特点，面临公平挑战（例如，公平、多样化和包容）。如果没有公平访问，特权个人、组织和集团可以参与和获得利益，而受折磨的群体则被排除在外。IoE实时管理DER资源不仅抛出了公平问题，还收集了高度敏感的地点、时间、活动依赖数据，需要负责任地处理（例如，隐私、安全和安全），以便通过人工智能技术提供了预测、优化和优先级服务，自动管理灵活资源。我们的项目视图是确保社区成员公平参与IoE，并负责使用他们的数据，以便IoE可以通过人工智能技术的进步获得安全、可靠和可再生的能源服务。
</details></li>
</ul>
<hr>
<h2 id="LLM-Guided-Inductive-Inference-for-Solving-Compositional-Problems"><a href="#LLM-Guided-Inductive-Inference-for-Solving-Compositional-Problems" class="headerlink" title="LLM Guided Inductive Inference for Solving Compositional Problems"></a>LLM Guided Inductive Inference for Solving Compositional Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11688">http://arxiv.org/abs/2309.11688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhigya Sodani, Lauren Moos, Matthew Mirman</li>
<li>for: 解决大语言模型（LLM）在问答任务中表现出色，但是它们的表现受限于问题中不包含在模型训练数据中的知识，需要通过直接观察或与实际世界交互来获得。</li>
<li>methods: 我们提出了一种方法，即 Recursion based extensible LLM（REBEL），它通过自动理解技术如动态规划和前进链接策略来处理开放世界、深度理解任务。REBEL使用自然语言描述来指定工具，并使用这些工具进行递归问题分解和外部工具使用。</li>
<li>results: 我们在一组需要深度嵌套使用外部工具的问题上示出了REBEL的能力，并在一个组合和对话性的 Setting中进行了证明。<details>
<summary>Abstract</summary>
While large language models (LLMs) have demonstrated impressive performance in question-answering tasks, their performance is limited when the questions require knowledge that is not included in the model's training data and can only be acquired through direct observation or interaction with the real world. Existing methods decompose reasoning tasks through the use of modules invoked sequentially, limiting their ability to answer deep reasoning tasks. We introduce a method, Recursion based extensible LLM (REBEL), which handles open-world, deep reasoning tasks by employing automated reasoning techniques like dynamic planning and forward-chaining strategies. REBEL allows LLMs to reason via recursive problem decomposition and utilization of external tools. The tools that REBEL uses are specified only by natural language description. We further demonstrate REBEL capabilities on a set of problems that require a deeply nested use of external tools in a compositional and conversational setting.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在问答任务中表现出色，但它们的表现受到训练数据中不包含的知识的限制。现有的方法通过运行模组来 decomposing 推理任务，限制它们 Answer deep reasoning tasks. We propose a method called Recursion based extensible LLM (REBEL), which handles open-world, deep reasoning tasks by using automated reasoning techniques such as dynamic planning and forward-chaining strategies. REBEL allows LLMs to reason through recursive problem decomposition and utilize external tools. The tools that REBEL uses are specified only by natural language description. We further demonstrate REBEL's capabilities on a set of problems that require a deeply nested use of external tools in a compositional and conversational setting.
</details></li>
</ul>
<hr>
<h2 id="Dr-FERMI-A-Stochastic-Distributionally-Robust-Fair-Empirical-Risk-Minimization-Framework"><a href="#Dr-FERMI-A-Stochastic-Distributionally-Robust-Fair-Empirical-Risk-Minimization-Framework" class="headerlink" title="Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework"></a>Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11682">http://arxiv.org/abs/2309.11682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sina Baharlouei, Meisam Razaviyayn</li>
<li>for: This paper aims to address the issue of fair machine learning models behaving unfairly on test data due to distribution shifts.</li>
<li>methods: The proposed method is based on distributionally robust optimization under $L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual Information (ERMI) as the measure of fairness violation. The method does not require knowledge of the causal graph and can be implemented in a stochastic fashion.</li>
<li>results: The proposed framework has been evaluated through extensive experiments on real datasets consisting of distribution shifts, and the results show that it performs well in terms of fairness and efficiency.<details>
<summary>Abstract</summary>
While training fair machine learning models has been studied extensively in recent years, most developed methods rely on the assumption that the training and test data have similar distributions. In the presence of distribution shifts, fair models may behave unfairly on test data. There have been some developments for fair learning robust to distribution shifts to address this shortcoming. However, most proposed solutions are based on the assumption of having access to the causal graph describing the interaction of different features. Moreover, existing algorithms require full access to data and cannot be used when small batches are used (stochastic/batch implementation). This paper proposes the first stochastic distributionally robust fairness framework with convergence guarantees that do not require knowledge of the causal graph. More specifically, we formulate the fair inference in the presence of the distribution shift as a distributionally robust optimization problem under $L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual Information (ERMI) as the measure of fairness violation. We then discuss how the proposed method can be implemented in a stochastic fashion. We have evaluated the presented framework's performance and efficiency through extensive experiments on real datasets consisting of distribution shifts.
</details>
<details>
<summary>摘要</summary>
traditional machine learning models have been extensively studied in recent years, but most of these methods rely on the assumption that the training and test data have similar distributions. However, in the presence of distribution shifts, fair models may behave unfairly on test data. To address this shortcoming, there have been some developments in fair learning that are robust to distribution shifts, but these methods are based on the assumption of having access to the causal graph describing the interaction of different features. Moreover, existing algorithms require full access to data and cannot be used when small batches are used (stochastic/batch implementation). This paper proposes the first stochastic distributionally robust fairness framework with convergence guarantees that do not require knowledge of the causal graph. Specifically, we formulate the fair inference in the presence of distribution shift as a distributionally robust optimization problem under $L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual Information (ERMI) as the measure of fairness violation. We then discuss how the proposed method can be implemented in a stochastic fashion. We have evaluated the presented framework's performance and efficiency through extensive experiments on real datasets consisting of distribution shifts.Here's the translation in Traditional Chinese:传统机器学习模型在最近的年份已经得到了广泛的研究，但大多数这些方法假设训练和测试数据的分布相似。然而，在分布shift情况下，公平的模型可能会在测试数据上不公平。为了解决这问题，有些开发了不同的公平学习方法，但这些方法假设有存在 causal graph 描述不同特征之间的互动。此外，现有的算法需要完整的数据存取，并且无法在小批量中使用（stochastic/batch实现）。本文提出了首个可靠的分布robust公平性框架，不需要知道 causal graph。具体来说，我们将 fair inference 在分布shift情况下形式化为分布robust优化问题，并使用 $L_p$  нор uncertainty set 来度量公平违反。我们然后讨论了如何实现这个方法在抽象的方式上。我们通过实际的实验，评估了提出的框架的性能和效率，以及实际应用中的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-with-Neural-Graphical-Models"><a href="#Federated-Learning-with-Neural-Graphical-Models" class="headerlink" title="Federated Learning with Neural Graphical Models"></a>Federated Learning with Neural Graphical Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11680">http://arxiv.org/abs/2309.11680</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Urszula Chajewska, Harsh Shrivastava</li>
<li>for: 该论文旨在创建基于专有数据的模型，以便多个客户保留专有数据控制权，同时通过共享资源提高模型准确性。</li>
<li>methods: 该论文提出了一种基于神经网络的联邦学习框架（FedNGMs），使用 probabilistic graphical models（NGMs）学习复杂的输入特征之间的非线性关系。</li>
<li>results: 该论文的 FedNGMs 框架可以避免 neuron matching 框架如 Federated Matched Averaging 的缺点，并且可以适应数据不均衡、多个参与者和limited communication bandwidth 等问题。<details>
<summary>Abstract</summary>
Federated Learning (FL) addresses the need to create models based on proprietary data in such a way that multiple clients retain exclusive control over their data, while all benefit from improved model accuracy due to pooled resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic Graphical models that utilize the expressive power of neural networks to learn complex non-linear dependencies between the input features. They learn to capture the underlying data distribution and have efficient algorithms for inference and sampling. We develop a FL framework which maintains a global NGM model that learns the averaged information from the local NGM models while keeping the training data within the client's environment. Our design, FedNGMs, avoids the pitfalls and shortcomings of neuron matching frameworks like Federated Matched Averaging that suffers from model parameter explosion. Our global model size remains constant throughout the process. In the cases where clients have local variables that are not part of the combined global distribution, we propose a `Stitching' algorithm, which personalizes the global NGM models by merging the additional variables using the client's data. FedNGM is robust to data heterogeneity, large number of participants, and limited communication bandwidth.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 解决了基于专有数据的模型创建的需求，以便多个客户端保留专有数据控制权，而同时各自受益于共享资源的提高模型精度。最近提出的神经图模型（NGM）是一种概率图模型，利用神经网络的表达能力来学习输入特征之间的复杂非线性关系。它们学习下面数据分布，并有效的推理和采样算法。我们开发了一个基于FL的框架，称之为FedNGMs，该框架在客户端环境中保持global NGM模型，该模型学习客户端的local NGM模型中的均值信息，而不需要将训练数据传输到客户端。我们的设计避免了神经网络匹配框架如联邦匹配平均的缺点，例如模型参数爆炸。我们的全球模型大小在训练过程中保持不变。在客户端有本地变量，这些变量不是全局共享的共同分布中的一部分时，我们提议使用"缝合"算法，将这些变量与全局NGM模型进行个性化结合。FedNGM具有对数据不一致、大量参与者和有限通信带宽的 Robustness。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-in-Mafia-like-Game-Simulation"><a href="#Generative-AI-in-Mafia-like-Game-Simulation" class="headerlink" title="Generative AI in Mafia-like Game Simulation"></a>Generative AI in Mafia-like Game Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11672">http://arxiv.org/abs/2309.11672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MunyeongKim/Gen-AI-in-Mafia-like-Game">https://github.com/MunyeongKim/Gen-AI-in-Mafia-like-Game</a></li>
<li>paper_authors: Munyeong Kim, Sungsu Kim</li>
<li>for: 这项研究探讨了生成式人工智能模型在角色扮演 simulations 中的可能性和潜力，以游戏 Spyfall 为例。</li>
<li>methods: 研究使用 GPT-4 的高级功能，通过对游戏场景的理解、决策和互动进行了比较分析，以证明模型在这些方面的潜力。</li>
<li>results: 研究发现，GPT-4 在游戏环境中的适应性有显著提高，能够更好地提问和发表人类化的回答。然而，模型在骗取和预测对手行动方面存在限制。研究还讨论了游戏开发、财政限制和非语言限制的问题。结果表明，虽然 GPT-4 表现出了较早模型的进步，但还有更多的发展空间，尤其是在塑造更人类化的 AI 模型。<details>
<summary>Abstract</summary>
In this research, we explore the efficacy and potential of Generative AI models, specifically focusing on their application in role-playing simulations exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's advanced capabilities, the study aimed to showcase the model's potential in understanding, decision-making, and interaction during game scenarios. Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo, demonstrated GPT-4's enhanced adaptability to the game environment, with significant improvements in posing relevant questions and forming human-like responses. However, challenges such as the model;s limitations in bluffing and predicting opponent moves emerged. Reflections on game development, financial constraints, and non-verbal limitations of the study were also discussed. The findings suggest that while GPT-4 exhibits promising advancements over earlier models, there remains potential for further development, especially in instilling more human-like attributes in AI.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们探索了生成AI模型的效果和潜力，特别是在游戏角色扮演 simulations中的应用。通过利用GPT-4的高级功能，研究旨在表明模型在游戏场景中的理解、决策和互动的潜力。对比GPT-4和其前一代GPT-3.5-turbo，研究发现GPT-4在游戏环境中的适应性得到了显著提升，特别是在提问和表达人类化的问题方面。然而，模型在谎言和预测对手行动方面存在限制。研究还讨论了游戏开发、财务限制和非语言限制的问题。研究结果表明，虽然GPT-4在前一代模型之上具有显著的进步，但还有可能进一步发展，尤其是在具备更多人类特征的AI方面。
</details></li>
</ul>
<hr>
<h2 id="“It’s-a-Fair-Game’’-or-Is-It-Examining-How-Users-Navigate-Disclosure-Risks-and-Benefits-When-Using-LLM-Based-Conversational-Agents"><a href="#“It’s-a-Fair-Game’’-or-Is-It-Examining-How-Users-Navigate-Disclosure-Risks-and-Benefits-When-Using-LLM-Based-Conversational-Agents" class="headerlink" title="“It’s a Fair Game’’, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents"></a>“It’s a Fair Game’’, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11653">http://arxiv.org/abs/2309.11653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiping Zhang, Michelle Jia, Hao-Ping, Lee, Bingsheng Yao, Sauvik Das, Ada Lerner, Dakuo Wang, Tianshi Li</li>
<li>for: 本研究旨在帮助建立优先考虑用户隐私的大语言模型（LLM）基于对话代理（CA），以解决现有研究主要集中在模型方面，忽略用户视角的问题。</li>
<li>methods: 本研究通过分析实际的ChatGPT对话和对19名LLM基于CA用户进行semi结构化采访，发现用户在使用LLM基于CA时经常面临privacy、utilities和便利性之间的权衡决策。</li>
<li>results: 研究发现用户的错误心理模型和系统设计中的黑暗 Patterns限制了他们对隐私风险的认识和理解，同时人工智能化的交互使用者更容易对自己的敏感信息进行披露，使用者在决策中受到增加的困难。<details>
<summary>Abstract</summary>
The widespread use of Large Language Model (LLM)-based conversational agents (CAs), especially in high-stakes domains, raises many privacy concerns. Building ethical LLM-based CAs that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. However, existing research, primarily model-centered, does not provide insight into users' perspectives. To bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations and conducted semi-structured interviews with 19 LLM-based CA users. We found that users are constantly faced with trade-offs between privacy, utility, and convenience when using LLM-based CAs. However, users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. Additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs. We discuss practical design guidelines and the needs for paradigmatic shifts to protect the privacy of LLM-based CA users.
</details>
<details>
<summary>摘要</summary>
广泛使用大语言模型（LLM）基于对话代理（CA），特别在高风险领域，引发了许多隐私问题。建立尊重用户隐私的LLM基于CA需要深入了解用户关心的隐私风险。然而，现有研究主要关注模型，未能提供用户视角的深入理解。为了补强这个差距，我们分析了实际的ChatGPT对话中的敏感泄露，并进行了19名LLM基于CA用户的semi结构化采访。我们发现，用户在使用LLM基于CA时经常面临privacy、功能和便利性之间的权衡。然而，用户的错误的认知模型和系统设计中的黑暗Patterns限制了他们对隐私风险的认识和理解。此外，人类化的互动更加鼓励用户提供更多的敏感信息，使用户更难avigate权衡。我们讨论了实用的设计指南和保护LLM基于CA用户隐私的需求。
</details></li>
</ul>
<hr>
<h2 id="Orbital-AI-based-Autonomous-Refuelling-Solution"><a href="#Orbital-AI-based-Autonomous-Refuelling-Solution" class="headerlink" title="Orbital AI-based Autonomous Refuelling Solution"></a>Orbital AI-based Autonomous Refuelling Solution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11648">http://arxiv.org/abs/2309.11648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duarte Rondao, Lei He, Nabil Aouf</li>
<li>for: 这篇论文旨在探讨使用摄像头进行太空对接和轨道服务（OOS），并且使用人工智能（AI）来将摄像头变成主要感知器。</li>
<li>methods: 这篇论文使用了许多 convolutional neural network（CNN）Backbone架构， benchmarked on synthetically generated docking manoeuvres with the International Space Station（ISS），以获得position和态度估算。</li>
<li>results: 这篇论文的结果显示，使用AI可以将relative navigation solution扩展到多种enario，例如targets或照明条件，并且可以实现position和态度估算的高精度。实际上，该方法可以大大减少了需要的工程师时间和资源。<details>
<summary>Abstract</summary>
Cameras are rapidly becoming the choice for on-board sensors towards space rendezvous due to their small form factor and inexpensive power, mass, and volume costs. When it comes to docking, however, they typically serve a secondary role, whereas the main work is done by active sensors such as lidar. This paper documents the development of a proposed AI-based (artificial intelligence) navigation algorithm intending to mature the use of on-board visible wavelength cameras as a main sensor for docking and on-orbit servicing (OOS), reducing the dependency on lidar and greatly reducing costs. Specifically, the use of AI enables the expansion of the relative navigation solution towards multiple classes of scenarios, e.g., in terms of targets or illumination conditions, which would otherwise have to be crafted on a case-by-case manner using classical image processing methods. Multiple convolutional neural network (CNN) backbone architectures are benchmarked on synthetically generated data of docking manoeuvres with the International Space Station (ISS), achieving position and attitude estimates close to 1% range-normalised and 1 deg, respectively. The integration of the solution with a physical prototype of the refuelling mechanism is validated in laboratory using a robotic arm to simulate a berthing procedure.
</details>
<details>
<summary>摘要</summary>
随着航天器的发展，镜头在航天器上的应用也在不断扩大。镜头的小型化和低功耗、质量和体积成本使其成为航天器上的首选感知器。然而，在协 docking 过程中，镜头通常扮演着次要角色，主要工作由活动感知器 such as lidar 完成。这篇论文描述了一种基于人工智能（AI）的导航算法的开发，旨在通过使用镜头来提高协 docking 和空间服务（OOS）中的精度和可靠性。使用 AI 可以扩展相对导航解决方案到多种场景，如目标或照明条件，而这些场景之前只能通过经典图像处理方法来手动设计。本文使用多种卷积神经网络（CNN）后处理器，对人工生成的协 docking 演示数据进行了测试，实现了 Position 和 Attitude 估计的准确率接近 1% 范围内和 1 度。此外，将解决方案与实际储存机制的物理 прототип结合，在实验室中使用 робо臂模拟协 docking 过程进行验证。
</details></li>
</ul>
<hr>
<h2 id="Attentive-VQ-VAE"><a href="#Attentive-VQ-VAE" class="headerlink" title="Attentive VQ-VAE"></a>Attentive VQ-VAE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11641">http://arxiv.org/abs/2309.11641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariano Rivera, Angello Hoyos</li>
<li>for: 提高 VQVAE 模型的能力，保持实用参数水平</li>
<li>methods:  integrate Attentive Residual Encoder (AREN) 和 Residual Pixel Attention layer，使用多级编码器，并采用内部自注意力机制来有效地捕捉和利用 Contextual information</li>
<li>results: 实验结果表明，提案的修改可以明显提高数据表示和生成能力，使 VQVAEs 更适合各种应用。<details>
<summary>Abstract</summary>
We present a novel approach to enhance the capabilities of VQVAE models through the integration of an Attentive Residual Encoder (AREN) and a Residual Pixel Attention layer. The objective of our research is to improve the performance of VQVAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in data representation and generation, making VQVAEs even more suitable for a wide range of applications.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，通过结合Attentive Residual Encoder（AREN）和Residual Pixel Attention层，以提高VQVAE模型的能力。我们的研究目标是提高VQVAE表现，同时保持实际参数水平。AREN编码器设计可以在多个层次上运行，适应不同的建筑复杂性。我们的关键创新是将Inter-pixel自动注意机制integrated into AREN编码器。这种方法使得我们能够效率地捕捉并利用 latent vector中的上下文信息。此外，我们的模型还使用了多个编码层，以进一步增强模型的表达力。我们的注意层采用了最小参数的方法，确保latent vector只有当其他像素中有pertinent information时才会被修改。实验结果表明，我们的修改导致了数据表示和生成的显著改进，使VQVAEs更适合各种应用。
</details></li>
</ul>
<hr>
<h2 id="A-survey-on-the-semantics-of-sequential-patterns-with-negation"><a href="#A-survey-on-the-semantics-of-sequential-patterns-with-negation" class="headerlink" title="A survey on the semantics of sequential patterns with negation"></a>A survey on the semantics of sequential patterns with negation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11638">http://arxiv.org/abs/2309.11638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Guyet</li>
<li>for: 本研究的目的是探讨用户对逻辑不同的时间序列模式具有何种直观性？</li>
<li>methods: 本研究使用了一份问卷来探讨用户对不同 semantics 的直观性。</li>
<li>results: 研究发现用户对两种 semantics 具有直观性，但这两种 semantics 并不与现有的主流算法 semantics 一致。因此，本研究提出了一些建议，以便更好地考虑这些差异。<details>
<summary>Abstract</summary>
A sequential pattern with negation, or negative sequential pattern, takes the form of a sequential pattern for which the negation symbol may be used in front of some of the pattern's itemsets. Intuitively, such a pattern occurs in a sequence if negated itemsets are absent in the sequence. Recent work has shown that different semantics can be attributed to these pattern forms, and that state-of-the-art algorithms do not extract the same sets of patterns. This raises the important question of the interpretability of sequential pattern with negation. In this study, our focus is on exploring how potential users perceive negation in sequential patterns. Our aim is to determine whether specific semantics are more "intuitive" than others and whether these align with the semantics employed by one or more state-of-the-art algorithms. To achieve this, we designed a questionnaire to reveal the semantics' intuition of each user. This article presents both the design of the questionnaire and an in-depth analysis of the 124 responses obtained. The outcomes indicate that two of the semantics are predominantly intuitive; however, neither of them aligns with the semantics of the primary state-of-the-art algorithms. As a result, we provide recommendations to account for this disparity in the conclusions drawn.
</details>
<details>
<summary>摘要</summary>
一种顺序模式 WITH negation，或负顺序模式，的形式是一种顺序模式，其中可以在一些模式itemset前面使用否定符。Intuitively，这种模式在序列中出现，当负否定itemset缺失在序列中。 recent work has shown that different semantics can be attributed to these pattern forms, and that state-of-the-art algorithms do not extract the same sets of patterns. This raises the important question of the interpretability of sequential pattern with negation. In this study, our focus is on exploring how potential users perceive negation in sequential patterns. Our aim is to determine whether specific semantics are more "intuitive" than others and whether these align with the semantics employed by one or more state-of-the-art algorithms. To achieve this, we designed a questionnaire to reveal the semantics' intuition of each user. This article presents both the design of the questionnaire and an in-depth analysis of the 124 responses obtained. The outcomes indicate that two of the semantics are predominantly intuitive; however, neither of them aligns with the semantics of the primary state-of-the-art algorithms. As a result, we provide recommendations to account for this disparity in the conclusions drawn.Note: The word "WITH" in the original text is not translated as it is not a word in Simplified Chinese. Instead, the phrase "顺序模式 WITH negation" is translated as "顺序模式 WITH 否定" (sequential pattern with negation).
</details></li>
</ul>
<hr>
<h2 id="Cloud-Based-Hierarchical-Imitation-Learning-for-Scalable-Transfer-of-Construction-Skills-from-Human-Workers-to-Assisting-Robots"><a href="#Cloud-Based-Hierarchical-Imitation-Learning-for-Scalable-Transfer-of-Construction-Skills-from-Human-Workers-to-Assisting-Robots" class="headerlink" title="Cloud-Based Hierarchical Imitation Learning for Scalable Transfer of Construction Skills from Human Workers to Assisting Robots"></a>Cloud-Based Hierarchical Imitation Learning for Scalable Transfer of Construction Skills from Human Workers to Assisting Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11619">http://arxiv.org/abs/2309.11619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongrui Yu, Vineet R. Kamat, Carol C. Menassa</li>
<li>for: 这个研究旨在将建筑工程中的重重和physically-demanding任务交给机器人，以降低人工伤害。</li>
<li>methods: 这个研究使用Imitation Learning（IL）技术将职人的手艺技能转移到机器人身上，以成功委托建筑工程任务和获得高品质机器人制成的成果。</li>
<li>results: 这个研究提出了一个具有实验学习（HIL）模型和云 robotics技术的虚拟示范框架，可以帮助将职人的手艺技能转移到机器人身上，并且可以重复使用这些示范，以减少人工示范的需求。这个框架可以帮助提高建筑工程中的雇员多样性和教育背景。<details>
<summary>Abstract</summary>
Assigning repetitive and physically-demanding construction tasks to robots can alleviate human workers's exposure to occupational injuries. Transferring necessary dexterous and adaptive artisanal construction craft skills from workers to robots is crucial for the successful delegation of construction tasks and achieving high-quality robot-constructed work. Predefined motion planning scripts tend to generate rigid and collision-prone robotic behaviors in unstructured construction site environments. In contrast, Imitation Learning (IL) offers a more robust and flexible skill transfer scheme. However, the majority of IL algorithms rely on human workers to repeatedly demonstrate task performance at full scale, which can be counterproductive and infeasible in the case of construction work. To address this concern, this paper proposes an immersive, cloud robotics-based virtual demonstration framework that serves two primary purposes. First, it digitalizes the demonstration process, eliminating the need for repetitive physical manipulation of heavy construction objects. Second, it employs a federated collection of reusable demonstrations that are transferable for similar tasks in the future and can thus reduce the requirement for repetitive illustration of tasks by human agents. Additionally, to enhance the trustworthiness, explainability, and ethical soundness of the robot training, this framework utilizes a Hierarchical Imitation Learning (HIL) model to decompose human manipulation skills into sequential and reactive sub-skills. These two layers of skills are represented by deep generative models, enabling adaptive control of robot actions. By delegating the physical strains of construction work to human-trained robots, this framework promotes the inclusion of workers with diverse physical capabilities and educational backgrounds within the construction industry.
</details>
<details>
<summary>摘要</summary>
<<SYS>>发现给定文本的简化中文翻译。<</SYS>>委托 repetitive 和 physically-demanding 的建筑任务给机器人，可以减轻人工工作者的职业危害风险。将必要的灵活和适应的艺术工艺技能从工作者传递到机器人是成功委托建筑任务和获得高质量机器人构建的关键。预定的运动规划脚本通常在无结构的建筑现场环境中生成僵化和碰撞的机器人行为。相比之下，学习模式（IL）提供了更加稳定和灵活的技能传递方案。然而，大多数 IL 算法需要人工工作者重复地展示任务完成，这可能是不可能的和不可预期的在建筑工作中。为解决这个问题，本文提出了一个 immerse 云 robotics 基础设施，它拥有以下两个主要目的：首先，它将示例过程数字化，从而消除重复地Physical 执行重构建筑物品的需要。其次，它使用一个 Federated 集合的可重用示例，以便在未来对类似任务进行快速协调。此外，为了增强机器人培训的可靠性、可解释性和伦理合理性，该框架使用 Hierarchical Imitation Learning（HIL）模型，将人类抓取技能 decomposed 成Sequential 和 reactive 两层。这两层技能被表示为深度生成模型，以便在机器人行为中进行适应控制。通过委托建筑工作给人类培训的机器人，这个框架推广了建筑业中不同的身体能力和教育背景的人员的包容性。
</details></li>
</ul>
<hr>
<h2 id="Hand-Gesture-Recognition-with-Two-Stage-Approach-Using-Transfer-Learning-and-Deep-Ensemble-Learning"><a href="#Hand-Gesture-Recognition-with-Two-Stage-Approach-Using-Transfer-Learning-and-Deep-Ensemble-Learning" class="headerlink" title="Hand Gesture Recognition with Two Stage Approach Using Transfer Learning and Deep Ensemble Learning"></a>Hand Gesture Recognition with Two Stage Approach Using Transfer Learning and Deep Ensemble Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11610">http://arxiv.org/abs/2309.11610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Serkan Savaş, Atilla Ergüzen</li>
<li>for: 这个研究的目的是对人工智能与 Computing 进行改进，以提高其性能。</li>
<li>methods: 这个研究使用了深度学习技术，特别是卷积神经网络，并将其应用于识别手势。</li>
<li>results: 研究获得了98.88%的准确率，这表明了深度ensemble学习技术在人工智能与Computing 中的应用潜力。<details>
<summary>Abstract</summary>
Human-Computer Interaction (HCI) has been the subject of research for many years, and recent studies have focused on improving its performance through various techniques. In the past decade, deep learning studies have shown high performance in various research areas, leading researchers to explore their application to HCI. Convolutional neural networks can be used to recognize hand gestures from images using deep architectures. In this study, we evaluated pre-trained high-performance deep architectures on the HG14 dataset, which consists of 14 different hand gesture classes. Among 22 different models, versions of the VGGNet and MobileNet models attained the highest accuracy rates. Specifically, the VGG16 and VGG19 models achieved accuracy rates of 94.64% and 94.36%, respectively, while the MobileNet and MobileNetV2 models achieved accuracy rates of 96.79% and 94.43%, respectively. We performed hand gesture recognition on the dataset using an ensemble learning technique, which combined the four most successful models. By utilizing these models as base learners and applying the Dirichlet ensemble technique, we achieved an accuracy rate of 98.88%. These results demonstrate the effectiveness of the deep ensemble learning technique for HCI and its potential applications in areas such as augmented reality, virtual reality, and game technologies.
</details>
<details>
<summary>摘要</summary>
人机交互（HCI）已经是多年的研究主题，而最近的研究强调提高其性能通过不同的技术。过去十年，深度学习研究在各个领域表现出色，导致研究者想要把它们应用于HCI。通过深度神经网络识别手势图像，可以使用深度建筑。本研究在HG14数据集上评估了22种不同的模型，其中包括VGGNet和MobileNet模型的多种版本。结果发现，VGG16和VGG19模型的准确率分别为94.64%和94.36%，而MobileNet和MobileNetV2模型的准确率分别为96.79%和94.43%。我们使用了ensemble学习技术，将这些模型作为基础学习器，并应用Dirichlet ensemble技术，达到了98.88%的准确率。这些结果表明深度ensemble学习技术在HCI中的效iveness，并在虚拟现实、扩展现实和游戏技术等领域有潜力应用。
</details></li>
</ul>
<hr>
<h2 id="Dataset-Factory-A-Toolchain-For-Generative-Computer-Vision-Datasets"><a href="#Dataset-Factory-A-Toolchain-For-Generative-Computer-Vision-Datasets" class="headerlink" title="Dataset Factory: A Toolchain For Generative Computer Vision Datasets"></a>Dataset Factory: A Toolchain For Generative Computer Vision Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11608">http://arxiv.org/abs/2309.11608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Kharitonov, Ryan Turner</li>
<li>for: 该论文旨在解决生成AI工作流程中数据处理问题，提高数据处理效率和可重用性。</li>
<li>methods: 该论文提出了一种“数据工厂”方法，将样本存储和处理分离于元数据，并允许数据驱动操作进行批处理。</li>
<li>results: 该论文的实验结果表明，使用“数据工厂”方法可以提高生成AI工作流程的数据处理效率和可重用性。<details>
<summary>Abstract</summary>
Generative AI workflows heavily rely on data-centric tasks - such as filtering samples by annotation fields, vector distances, or scores produced by custom classifiers. At the same time, computer vision datasets are quickly approaching petabyte volumes, rendering data wrangling difficult. In addition, the iterative nature of data preparation necessitates robust dataset sharing and versioning mechanisms, both of which are hard to implement ad-hoc. To solve these challenges, we propose a "dataset factory" approach that separates the storage and processing of samples from metadata and enables data-centric operations at scale for machine learning teams and individual researchers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CATS-Conditional-Adversarial-Trajectory-Synthesis-for-Privacy-Preserving-Trajectory-Data-Publication-Using-Deep-Learning-Approaches"><a href="#CATS-Conditional-Adversarial-Trajectory-Synthesis-for-Privacy-Preserving-Trajectory-Data-Publication-Using-Deep-Learning-Approaches" class="headerlink" title="CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches"></a>CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11587">http://arxiv.org/abs/2309.11587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geods/cats">https://github.com/geods/cats</a></li>
<li>paper_authors: Jinmeng Rao, Song Gao, Sijia Zhu</li>
<li>for: 本研究使用深度学习技术保护人员流动数据隐私，并生成高质量的人员流动数据。</li>
<li>methods: 本研究使用K-anonymity保证人员流动数据的分布水平隐私，并使用条件对抗训练、人流环境学习和相邻轨迹点匹配来重建轨迹 topology。</li>
<li>results: 实验结果表明，我们的方法在隐私保护、空间时间特征保持和下游实用性方面比基线方法表现更好，为人流动数据隐私研究Using生成AI技术和数据伦理问题提供新的视角。<details>
<summary>Abstract</summary>
The prevalence of ubiquitous location-aware devices and mobile Internet enables us to collect massive individual-level trajectory dataset from users. Such trajectory big data bring new opportunities to human mobility research but also raise public concerns with regard to location privacy. In this work, we present the Conditional Adversarial Trajectory Synthesis (CATS), a deep-learning-based GeoAI methodological framework for privacy-preserving trajectory data generation and publication. CATS applies K-anonymity to the underlying spatiotemporal distributions of human movements, which provides a distributional-level strong privacy guarantee. By leveraging conditional adversarial training on K-anonymized human mobility matrices, trajectory global context learning using the attention-based mechanism, and recurrent bipartite graph matching of adjacent trajectory points, CATS is able to reconstruct trajectory topology from conditionally sampled locations and generate high-quality individual-level synthetic trajectory data, which can serve as supplements or alternatives to raw data for privacy-preserving trajectory data publication. The experiment results on over 90k GPS trajectories show that our method has a better performance in privacy preservation, spatiotemporal characteristic preservation, and downstream utility compared with baseline methods, which brings new insights into privacy-preserving human mobility research using generative AI techniques and explores data ethics issues in GIScience.
</details>
<details>
<summary>摘要</summary>
“现代社会中普遍存在 ubique 位置意识设备和移动互联网，我们可以从用户收集巨大的个人化轨迹数据集。这些轨迹大数据为人类活动研究带来了新的机会，但也引起了人们关于位置隐私的担忧。在这种情况下，我们提出了 Conditional Adversarial Trajectory Synthesis（CATS），一种基于深度学习的GeoAI方法框架，用于隐私保护的轨迹数据生成和发布。CATS通过对人类活动的下层空间时间分布进行K-anonimity处理，提供了强的隐私保证。通过使用受条件 adversarial 训练的人类活动矩阵，沿着邻近轨迹点的循环双向图匹配，以及使用注意力机制进行轨迹全球上下文学习，CATS可以从受条件采样的位置中重建轨迹拓扑，并生成高质量的个人化 sintetic 轨迹数据，可以作为隐私保护下的轨迹数据发布的补充或替代。实验结果表明，我们的方法在隐私保护、空间时间特征保持和下游实用性方面表现更好于基eline方法，这带来了新的思路 для隐私保护的人类活动研究，并探讨了GIScience中的数据伦理问题。”
</details></li>
</ul>
<hr>
<h2 id="Distilling-Adversarial-Prompts-from-Safety-Benchmarks-Report-for-the-Adversarial-Nibbler-Challenge"><a href="#Distilling-Adversarial-Prompts-from-Safety-Benchmarks-Report-for-the-Adversarial-Nibbler-Challenge" class="headerlink" title="Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge"></a>Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11575">http://arxiv.org/abs/2309.11575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Brack, Patrick Schramowski, Kristian Kersting</li>
<li>for: 本研究旨在挑战现有的生成图像模型安全性问题，通过分析潜在的攻击输入来检测模型的脆弱性。</li>
<li>methods: 研究人员使用了现有的安全准则来生成大量的攻击输入，并对这些输入和相应的图像进行分析，以探讨当前生成图像模型中的安全问题。</li>
<li>results: 研究人员发现了许多潜在的安全问题，包括输入筛选器的脆弱性和系统性的安全问题，这些问题可能会影响生成图像模型的安全性。<details>
<summary>Abstract</summary>
Text-conditioned image generation models have recently achieved astonishing image quality and alignment results. Consequently, they are employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also produce unsafe content. As a contribution to the Adversarial Nibbler challenge, we distill a large set of over 1,000 potential adversarial inputs from existing safety benchmarks. Our analysis of the gathered prompts and corresponding images demonstrates the fragility of input filters and provides further insights into systematic safety issues in current generative image models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BTLM-3B-8K-7B-Parameter-Performance-in-a-3B-Parameter-Model"><a href="#BTLM-3B-8K-7B-Parameter-Performance-in-a-3B-Parameter-Model" class="headerlink" title="BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model"></a>BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11568">http://arxiv.org/abs/2309.11568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nolan Dey, Daria Soboleva, Faisal Al-Khateeb, Bowen Yang, Ribhu Pathria, Hemant Khachane, Shaheer Muhammad, Zhiming, Chen, Robert Myers, Jacob Robert Steeves, Natalia Vassilieva, Marvin Tom, Joel Hestness</li>
<li>for: 这个论文的目的是介绍一种新的语言模型，即BTLM-3B-8K，该模型在下游任务中表现出色，并且具有较小的参数大小和计算资源占用。</li>
<li>methods: 该模型使用了一些现有的技术，包括ALiBi位嵌入和SwiGLU非线性函数，并且通过优化Hyperparameter和学习环境来提高模型的性能。</li>
<li>results: 相比其他3B参数模型，BTLM-3B-8K在下游任务中表现出2-5.5%的提升，而且在长上下文任务中也表现出优秀的表现，比如MPT-7B-8K和XGen-7B-8K。此外，BTLM-3B-8K的计算资源占用相对较少，只需3GB的内存和2.5倍的计算资源。<details>
<summary>Abstract</summary>
We introduce the Bittensor Language Model, called "BTLM-3B-8K", a new state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and 8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B parameter models. Additionally, BTLM-3B-8K provides excellent long context performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192 context length. We trained the model on a cleaned and deduplicated SlimPajama dataset; aggressively tuned the \textmu P hyperparameters and schedule; used ALiBi position embeddings; and adopted the SwiGLU nonlinearity.   On Hugging Face, the most popular models have 7B parameters, indicating that users prefer the quality-size ratio of 7B models. Compacting the 7B parameter model to one with 3B parameters, with little performance impact, is an important milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models, helping to open up access to a powerful language model on mobile and edge devices. BTLM-3B-8K is available under an Apache 2.0 license on Hugging Face: https://huggingface.co/cerebras/btlm-3b-8k-base.
</details>
<details>
<summary>摘要</summary>
我们介绍“BTLM-3B-8K”语言模型，是一个新的州际之冠开源语言模型，拥有30亿个参数。BTLM-3B-8K在627亿个Token的SlimPajama数据集上进行训练，并使用2048和8192的上下文长度混合训练。相比于现有的30亿个参数模型，BTLM-3B-8K在下游任务中表现出2-5.5%的提升。此外，BTLM-3B-8K在长上下文任务中表现出色，比MPT-7B-8K和XGen-7B-8K更高。我们在精简和删除了SlimPajama数据集上训练这个模型，并严格地调整了μP参数和时间表。此外，我们还使用了ALiBi位嵌入和SwiGLU非线性。在Hugging Face上，最受欢迎的模型都有70亿个参数，这表明用户对70亿个参数模型的质量-大小比例感兴趣。将70亿个参数模型缩减到30亿个参数，几乎没有影响性能，是一个重要的里程碑。BTLM-3B-8K只需要3GB的内存和4位准确，在测试过程中耗用2.5倍的计算资源，帮助开辟了一个具有强大语言模型的门槛，并且可以在移动和边缘设备上运行。BTLM-3B-8K在Hugging Face上可以免费下载：https://huggingface.co/cerebras/btlm-3b-8k-base。
</details></li>
</ul>
<hr>
<h2 id="Limitations-in-odour-recognition-and-generalisation-in-a-neuromorphic-olfactory-circuit"><a href="#Limitations-in-odour-recognition-and-generalisation-in-a-neuromorphic-olfactory-circuit" class="headerlink" title="Limitations in odour recognition and generalisation in a neuromorphic olfactory circuit"></a>Limitations in odour recognition and generalisation in a neuromorphic olfactory circuit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11555">http://arxiv.org/abs/2309.11555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nik Dennler, André van Schaik, Michael Schmuker</li>
<li>for: 这篇论文旨在研究一种基于神经omorphic computing的芳香学习算法，以及该算法在识别气体芳香的能力。</li>
<li>methods: 该算法使用了一种基于芳香细胞网络的神经omorphic架构，并使用了一些硬件加速技术来加速计算。</li>
<li>results: 研究发现，该算法在识别不同气体芳香的能力较强，但是在重复 presentaion 的情况下，模型的泛化能力有限。此外，研究还发现了一些限制，导致部分结论需要进一步验证。<details>
<summary>Abstract</summary>
Neuromorphic computing is one of the few current approaches that have the potential to significantly reduce power consumption in Machine Learning and Artificial Intelligence. Imam & Cleland presented an odour-learning algorithm that runs on a neuromorphic architecture and is inspired by circuits described in the mammalian olfactory bulb. They assess the algorithm's performance in "rapid online learning and identification" of gaseous odorants and odorless gases (short "gases") using a set of gas sensor recordings of different odour presentations and corrupting them by impulse noise. We replicated parts of the study and discovered limitations that affect some of the conclusions drawn. First, the dataset used suffers from sensor drift and a non-randomised measurement protocol, rendering it of limited use for odour identification benchmarks. Second, we found that the model is restricted in its ability to generalise over repeated presentations of the same gas. We demonstrate that the task the study refers to can be solved with a simple hash table approach, matching or exceeding the reported results in accuracy and runtime. Therefore, a validation of the model that goes beyond restoring a learned data sample remains to be shown, in particular its suitability to odour identification tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Chain-of-Verification-Reduces-Hallucination-in-Large-Language-Models"><a href="#Chain-of-Verification-Reduces-Hallucination-in-Large-Language-Models" class="headerlink" title="Chain-of-Verification Reduces Hallucination in Large Language Models"></a>Chain-of-Verification Reduces Hallucination in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11495">http://arxiv.org/abs/2309.11495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston</li>
<li>for: 本研究旨在解决大语言模型中的假信息生成问题。</li>
<li>methods: 我们提出了链式验证（CoVe）方法，该方法首先（i）生成初始回答，然后（ii）规划验证问题，（iii）独立回答验证问题，并（iv）生成最终验证后的回答。</li>
<li>results: 我们在各种任务上（如Wikidata列表问题、关闭书MultiSpanQA和长文本生成）实验表明，CoVe可以减少假信息的发生。<details>
<summary>Abstract</summary>
Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.
</details>
<details>
<summary>摘要</summary>
大型语言模型中的幻想（hallucination）问题仍未得到解决。我们研究语言模型是否可以对其回答进行检查和更正。我们开发了链式验证（Chain-of-Verification，CoVe）方法，它包括以下四个步骤：1. 模型首先提出一个初步答案（draft）；2. 然后，模型计划一系列的验证问题，以验证其初步答案是否正确；3. 模型独立地回答这些验证问题，以避免受其他答案的影响；4. 最后，模型生成一个经验验证的答案。在实验中，我们发现CoVe可以在多种任务上减少幻想，包括基于Wikidata的列表问题、关闭书MultiSpanQA和长文本生成等。
</details></li>
</ul>
<hr>
<h2 id="Text2Reward-Automated-Dense-Reward-Function-Generation-for-Reinforcement-Learning"><a href="#Text2Reward-Automated-Dense-Reward-Function-Generation-for-Reinforcement-Learning" class="headerlink" title="Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning"></a>Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11489">http://arxiv.org/abs/2309.11489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu</li>
<li>for: 本研究旨在提供一种数据自由的游戏奖励函数生成框架，帮助解决现有RL中特殊知识或域数据的需求，从而降低开发成本。</li>
<li>methods: 本研究使用大语言模型（LLM）自动生成 dense reward functions，并将奖励函数转换为可执行的程序，以满足不同任务的需求。</li>
<li>results: 在两个机器人操作benchmark（ManiSkill2、MetaWorld）和两个mujoco的 locomotive环境中，使用生成的奖励函数让策略取得了13项17个任务的成功率和速度与专家写的奖励函数相当或更高，并且在六个新的 locomotive行为中取得了94%以上的成功率。此外，我们还证明了使用我们的方法在实际世界中部署的策略。最后，我们通过人工反馈来进一步改进策略的奖励函数。视频结果可以在<a target="_blank" rel="noopener" href="https://text-to-reward.github.io查看./">https://text-to-reward.github.io查看。</a><details>
<summary>Abstract</summary>
Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io
</details>
<details>
<summary>摘要</summary>
��utes2��ward是一种抽象的游戏机制，可以自动生成填充的奖励函数，不需要特殊的知识或域数据，从而降低开发成本。为解决这个问题，我们引入Text2Reward，一种数据自由框架，可以自动生成填充的奖励函数，基于大型自然语言模型（LLM）。给出一个用自然语言描述的目标，Text2Reward可以生成填充的奖励函数，作为可执行的程序，并将其与环境的减少表示相关联。不同于反向RL和最近的工作，使用LLM写稀疏奖励代码，Text2Reward生成的奖励代码可读性好，可以覆盖各种任务，使用现有包，并允许迭代反馈。我们在ManiSkill2和MetaWorld两个机器人 manipulate 测试环境中进行了评估，以及MuJoCo两个涂抹环境。在13个机器人 manipulate 任务中，使用生成的奖励函数训练的策略的任务成功率和速度与专家写的奖励函数相当或更好。此外，我们的方法学习了6种新的行走行为，其成功率超过94%。最后，我们表明使用我们的方法在实际世界中训练的策略可以在真实世界中部署。 Text2Reward 还可以通过人类反馈来进一步改进策略的奖励函数。视频结果可以在<https://text-to-reward.github.io> 查看。
</details></li>
</ul>
<hr>
<h2 id="Fictional-Worlds-Real-Connections-Developing-Community-Storytelling-Social-Chatbots-through-LLMs"><a href="#Fictional-Worlds-Real-Connections-Developing-Community-Storytelling-Social-Chatbots-through-LLMs" class="headerlink" title="Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs"></a>Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11478">http://arxiv.org/abs/2309.11478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqian Sun, Hanyi Wang, Pok Man Chan, Morteza Tabibi, Yan Zhang, Huan Lu, Yuheng Chen, Chang Hee Lee, Ali Asadipour</li>
<li>for: 这篇论文旨在开发社区中可以与人互动的社交虚拟助手（SC），通过故事的用途来增强社交互动的可信度和趣味性。</li>
<li>methods: 该论文使用了语言模型GPT-3驱动的故事社交虚拟助手(“David”和”Catherine”)，并在在线游戏社区”DE (Alias)”上的Discord进行了评估。</li>
<li>results: 该研究结果表明，通过故事的使用可以增强社交虚拟助手在社区 setting中的参与度和可信度。<details>
<summary>Abstract</summary>
We address the integration of storytelling and Large Language Models (LLMs) to develop engaging and believable Social Chatbots (SCs) in community settings. Motivated by the potential of fictional characters to enhance social interactions, we introduce Storytelling Social Chatbots (SSCs) and the concept of story engineering to transform fictional game characters into "live" social entities within player communities. Our story engineering process includes three steps: (1) Character and story creation, defining the SC's personality and worldview, (2) Presenting Live Stories to the Community, allowing the SC to recount challenges and seek suggestions, and (3) Communication with community members, enabling interaction between the SC and users. We employed the LLM GPT-3 to drive our SSC prototypes, "David" and "Catherine," and evaluated their performance in an online gaming community, "DE (Alias)," on Discord. Our mixed-method analysis, based on questionnaires (N=15) and interviews (N=8) with community members, reveals that storytelling significantly enhances the engagement and believability of SCs in community settings.
</details>
<details>
<summary>摘要</summary>
我们研究将故事与大型自然语言模型（LLM）结合，以开发在社区中引人入来和 credible 的社交聊天机器人（SC）。我们被启发了虚构人物可以增强社交互动的潜力，因此我们引入了 Storytelling Social Chatbots（SSCs）和故事工程技术，将虚构游戏角色转化为社区中的 "live" 社交实体。我们的故事工程过程包括三个步骤：（1）人物和故事创作，定义 SC 的个性和观点，（2）向社区成员展示Live Story，让 SC 描述挑战和寻求建议，（3）与社区成员交流，允许 SC 与用户互动。我们使用 GPT-3 LLM 驱动我们的 SSC 原型 "David" 和 "Catherine"，并在 Discord 上的在线游戏社区 "DE (Alias)" 进行了评估。我们的混合方法分析，基于问卷 (N=15) 和采访 (N=8) 的社区成员，表明故事在社区设置中可以显著提高 SC 的参与度和吸引力。
</details></li>
</ul>
<hr>
<h2 id="Multi-view-Fuzzy-Representation-Learning-with-Rules-based-Model"><a href="#Multi-view-Fuzzy-Representation-Learning-with-Rules-based-Model" class="headerlink" title="Multi-view Fuzzy Representation Learning with Rules based Model"></a>Multi-view Fuzzy Representation Learning with Rules based Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11473">http://arxiv.org/abs/2309.11473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhang, Zhaohong Deng, Te Zhang, Kup-Sze Choi, Shitong Wang</li>
<li>for: 本文提出了一种新的多视图含义学习方法，用于解决多视图数据挖掘中的一些关键挑战。</li>
<li>methods: 本方法基于可解释的 Takagi-Sugeno-Kang (TSK) 杂化系统，通过两个方面实现多视图表示学习。首先，将多视图数据转换为高维杂化特征空间，同时同时挖掘共同视图信息和每个视图特有信息。其次，提出了基于 L_(2,1) 评估方法的新规范方法，以挖掘视图之间的一致信息，并保持数据的几何结构。</li>
<li>results: 对多个标准多视图数据集进行了广泛的实验 validate the superiority of the proposed method。<details>
<summary>Abstract</summary>
Unsupervised multi-view representation learning has been extensively studied for mining multi-view data. However, some critical challenges remain. On the one hand, the existing methods cannot explore multi-view data comprehensively since they usually learn a common representation between views, given that multi-view data contains both the common information between views and the specific information within each view. On the other hand, to mine the nonlinear relationship between data, kernel or neural network methods are commonly used for multi-view representation learning. However, these methods are lacking in interpretability. To this end, this paper proposes a new multi-view fuzzy representation learning method based on the interpretable Takagi-Sugeno-Kang (TSK) fuzzy system (MVRL_FS). The method realizes multi-view representation learning from two aspects. First, multi-view data are transformed into a high-dimensional fuzzy feature space, while the common information between views and specific information of each view are explored simultaneously. Second, a new regularization method based on L_(2,1)-norm regression is proposed to mine the consistency information between views, while the geometric structure of the data is preserved through the Laplacian graph. Finally, extensive experiments on many benchmark multi-view datasets are conducted to validate the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
多视角表示学习已经广泛研究了多视角数据的挖掘。然而，有些关键挑战仍然存在。一方面，现有的方法不能全面探索多视角数据，因为它们通常学习多视角数据中的共同信息，而不是每个视角中的特定信息。另一方面，用于挖掘非线性关系的内核或神经网络方法通常缺乏可解释性。为此，本文提出了一种新的多视角杂化表示学习方法，基于可解释的 Takagi-Sugeno-Kang（TSK）杂化系统（MVRL_FS）。该方法在两个方面实现多视角表示学习。首先，多视角数据被转换成一个高维杂化特征空间，同时探索多视角数据中的共同信息和每个视角中的特定信息。其次，基于L_(2,1)-norm回归的新规则方法被提出，以挖掘视角之间的一致信息，保留数据的几何结构通过拉普拉斯图。最后，对许多标准多视角数据集进行了广泛的实验，以验证提议方法的超越性。
</details></li>
</ul>
<hr>
<h2 id="Multi-Label-Takagi-Sugeno-Kang-Fuzzy-System"><a href="#Multi-Label-Takagi-Sugeno-Kang-Fuzzy-System" class="headerlink" title="Multi-Label Takagi-Sugeno-Kang Fuzzy System"></a>Multi-Label Takagi-Sugeno-Kang Fuzzy System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11469">http://arxiv.org/abs/2309.11469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiongdan Lou, Zhaohong Deng, Zhiyong Xiao, Kup-Sze Choi, Shitong Wang</li>
<li>for: 提高多标签分类性能</li>
<li>methods: 基于多标签相关学习和多标签回归损失的多标签杜氏辛诺干式系统（ML-TSK FS）</li>
<li>results: 对12个多标签数据集进行实验，结果表明ML-TSK FS与现有方法相比，在各种评价指标中表现竞争力强，表明它可以有效地通过辛诺干式规则模型特性和特征标签关系，提高分类性能。<details>
<summary>Abstract</summary>
Multi-label classification can effectively identify the relevant labels of an instance from a given set of labels. However,the modeling of the relationship between the features and the labels is critical to the classification performance. To this end, we propose a new multi-label classification method, called Multi-Label Takagi-Sugeno-Kang Fuzzy System (ML-TSK FS), to improve the classification performance. The structure of ML-TSK FS is designed using fuzzy rules to model the relationship between features and labels. The fuzzy system is trained by integrating fuzzy inference based multi-label correlation learning with multi-label regression loss. The proposed ML-TSK FS is evaluated experimentally on 12 benchmark multi-label datasets. 1 The results show that the performance of ML-TSK FS is competitive with existing methods in terms of various evaluation metrics, indicating that it is able to model the feature-label relationship effectively using fuzzy inference rules and enhances the classification performance.
</details>
<details>
<summary>摘要</summary>
多标签分类可以有效地从给定的标签集中确定实例的相关标签。然而，模型特性和标签之间的关系是多标签分类性能的关键因素。为此，我们提出了一种新的多标签分类方法，即多标签多SK满足系统（ML-TSK FS），以提高分类性能。ML-TSK FS的结构采用规则来模型特性和标签之间的关系。这个规则是通过多态推理和多标签相互关系学习来训练的。我们对12个多标签数据集进行实验评估了ML-TSK FS的性能。结果表明，ML-TSK FS与现有方法相比，在不同的评价指标上具有竞争力，这表明它可以通过多态推理规则来有效地模型特性和标签之间的关系，提高分类性能。
</details></li>
</ul>
<hr>
<h2 id="AudioFool-Fast-Universal-and-synchronization-free-Cross-Domain-Attack-on-Speech-Recognition"><a href="#AudioFool-Fast-Universal-and-synchronization-free-Cross-Domain-Attack-on-Speech-Recognition" class="headerlink" title="AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition"></a>AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11462">http://arxiv.org/abs/2309.11462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamad Fakih, Rouwaida Kanj, Fadi Kurdahi, Mohammed E. Fouda</li>
<li>for: 防止自动话语识别系统受到敌对攻击，导致系统崩溃或损坏。</li>
<li>methods: 使用对数频域修改攻击，以确保攻击具有不同特性，例如不受同步调制影响和范围滤波影响。</li>
<li>results: 透过实验和分析，发现 modified frequency domain 攻击能够实现这些特性，并且在线上 keyword classification 任务中提供了高效的攻击方法。<details>
<summary>Abstract</summary>
Automatic Speech Recognition systems have been shown to be vulnerable to adversarial attacks that manipulate the command executed on the device. Recent research has focused on exploring methods to create such attacks, however, some issues relating to Over-The-Air (OTA) attacks have not been properly addressed. In our work, we examine the needed properties of robust attacks compatible with the OTA model, and we design a method of generating attacks with arbitrary such desired properties, namely the invariance to synchronization, and the robustness to filtering: this allows a Denial-of-Service (DoS) attack against ASR systems. We achieve these characteristics by constructing attacks in a modified frequency domain through an inverse Fourier transform. We evaluate our method on standard keyword classification tasks and analyze it in OTA, and we analyze the properties of the cross-domain attacks to explain the efficiency of the approach.
</details>
<details>
<summary>摘要</summary>
自动话语识别系统已经被证明容易受到敌意攻击，这些攻击可以控制设备上执行的命令。最近的研究主要关注于探索如何创建这些攻击，但是一些过空中攻击（OTA）问题尚未得到充分解决。在我们的工作中，我们分析了需要的抗性攻击的属性，并设计了生成攻击具有任意想要的属性的方法，包括不变性和过滤器的Robustness。我们通过对射Transformer来实现这些特性，并在标准关键词分类任务上评估了我们的方法。我们还分析了跨频域攻击的性质，以解释我们的方法的高效性。
</details></li>
</ul>
<hr>
<h2 id="Generative-Agent-Based-Modeling-Unveiling-Social-System-Dynamics-through-Coupling-Mechanistic-Models-with-Generative-Artificial-Intelligence"><a href="#Generative-Agent-Based-Modeling-Unveiling-Social-System-Dynamics-through-Coupling-Mechanistic-Models-with-Generative-Artificial-Intelligence" class="headerlink" title="Generative Agent-Based Modeling: Unveiling Social System Dynamics through Coupling Mechanistic Models with Generative Artificial Intelligence"></a>Generative Agent-Based Modeling: Unveiling Social System Dynamics through Coupling Mechanistic Models with Generative Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11456">http://arxiv.org/abs/2309.11456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Ghaffarzadegan, Aritra Majumdar, Ross Williams, Niyousha Hosseinichimeh</li>
<li>for: 这篇论文探讨了使用生成人工智能建模社会系统的新机遇。</li>
<li>methods: 这些模型使用大语言模型如ChatGPT来表示人类决策行为在社会设置下。</li>
<li>results: 这篇论文提供了一个简单的社会规范传播模型，并对其 Results 进行了广泛的调查和敏感性分析。<details>
<summary>Abstract</summary>
We discuss the emerging new opportunity for building feedback-rich computational models of social systems using generative artificial intelligence. Referred to as Generative Agent-Based Models (GABMs), such individual-level models utilize large language models such as ChatGPT to represent human decision-making in social settings. We provide a GABM case in which human behavior can be incorporated in simulation models by coupling a mechanistic model of human interactions with a pre-trained large language model. This is achieved by introducing a simple GABM of social norm diffusion in an organization. For educational purposes, the model is intentionally kept simple. We examine a wide range of scenarios and the sensitivity of the results to several changes in the prompt. We hope the article and the model serve as a guide for building useful diffusion models that include realistic human reasoning and decision-making.
</details>
<details>
<summary>摘要</summary>
我们讨论新兴的机会：使用生成人工智能建构具有反馈丰富的社交系统模型。称为生成代理模型（GABM），这些个体级模型利用大量语言模型如ChatGPT来表示人类决策在社交设置中。我们提供一个GABM例子，将人类行为integrated到模拟模型中，通过与预训大量语言模型 Coupling 的方式。这是通过将社交norm传播模型简化为 Educational  purposes 的方式。我们评估了广泛的情况，并评估了变量的敏感度。我们希望这篇文章和模型可以serve as a guide  для建立包含现实人类思维和决策的传播模型。
</details></li>
</ul>
<hr>
<h2 id="Using-deep-learning-to-construct-stochastic-local-search-SAT-solvers-with-performance-bounds"><a href="#Using-deep-learning-to-construct-stochastic-local-search-SAT-solvers-with-performance-bounds" class="headerlink" title="Using deep learning to construct stochastic local search SAT solvers with performance bounds"></a>Using deep learning to construct stochastic local search SAT solvers with performance bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11452">http://arxiv.org/abs/2309.11452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/porscheofficial/sls_sat_solving_with_deep_learning">https://github.com/porscheofficial/sls_sat_solving_with_deep_learning</a></li>
<li>paper_authors: Maximilian Kramer, Paul Boes</li>
<li>for: 这 paper 是关于 Boolean Satisfiability problem (SAT) 的研究，具体来说是使用 Graph Neural Networks (GNN) 训练 oracle，以提高 Stochastic Local Search (SLS) 算法的性能。</li>
<li>methods: 这 paper 使用了 GNN 训练 oracle，并将其应用于两种 SLS 算法上，以解决随机 SAT 实例。</li>
<li>results: 研究发现，通过使用 GNN 训练 oracle，SLS 算法的性能得到了明显提高，可以解决更难的 SAT 实例，并且可以在更少的步骤数下解决。<details>
<summary>Abstract</summary>
The Boolean Satisfiability problem (SAT) is the most prototypical NP-complete problem and of great practical relevance. One important class of solvers for this problem are stochastic local search (SLS) algorithms that iteratively and randomly update a candidate assignment. Recent breakthrough results in theoretical computer science have established sufficient conditions under which SLS solvers are guaranteed to efficiently solve a SAT instance, provided they have access to suitable "oracles" that provide samples from an instance-specific distribution, exploiting an instance's local structure. Motivated by these results and the well established ability of neural networks to learn common structure in large datasets, in this work, we train oracles using Graph Neural Networks and evaluate them on two SLS solvers on random SAT instances of varying difficulty. We find that access to GNN-based oracles significantly boosts the performance of both solvers, allowing them, on average, to solve 17% more difficult instances (as measured by the ratio between clauses and variables), and to do so in 35% fewer steps, with improvements in the median number of steps of up to a factor of 8. As such, this work bridges formal results from theoretical computer science and practically motivated research on deep learning for constraint satisfaction problems and establishes the promise of purpose-trained SAT solvers with performance guarantees.
</details>
<details>
<summary>摘要</summary>
布尔满意性问题（SAT）是NP完备问题的最典型例子，具有实际重要性。一种重要的SAT解决方法是随机地更新候选分配的杂化搜索算法（SLS）。最近的理论计算机科学成果表明，如果SLS算法有访问适合的"oracle"，那么它们可以有效地解决SAT实例， provided they have access to suitable "oracles" that provide samples from an instance-specific distribution, exploiting an instance's local structure. 在这种情况下，我们使用图神经网络训练 oracle，并对两种SLS解决方法进行评估，在随机SAT实例上进行测试。我们发现，通过访问GNN基于 oracle，可以大幅提高SLS解决方法的性能，使其能够解决更难的实例（按照条件数和变量的比率来度量），并且在更少的步骤内完成（比如，在35% fewer steps中完成）。此外，我们发现，在 median number of steps 中，GNN基于 oracle 可以提高 SLS 解决方法的性能，最高可以提高8倍。因此，这项研究将理论计算机科学的成果与深度学习的实践研究相结合，并证明了专门为SAT问题训练的深度学习算法可以提供性能保证。
</details></li>
</ul>
<hr>
<h2 id="You-Only-Look-at-Screens-Multimodal-Chain-of-Action-Agents"><a href="#You-Only-Look-at-Screens-Multimodal-Chain-of-Action-Agents" class="headerlink" title="You Only Look at Screens: Multimodal Chain-of-Action Agents"></a>You Only Look at Screens: Multimodal Chain-of-Action Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11436">http://arxiv.org/abs/2309.11436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cooelf/Auto-UI">https://github.com/cooelf/Auto-UI</a></li>
<li>paper_authors: Zhuosheng Zhang, Aston Zhang</li>
<li>for: 这篇论文旨在提高自动化用户界面（UI）代理的效率，使其可以在不需要人工干预的情况下自动完成任务。</li>
<li>methods: 该论文提出了一种多模态解决方案，即直接与界面交互，不需要环境解析或应用程序特定的API。此外，提出了一种链式动作技术，通过考虑先前和后续动作历史，帮助代理决定哪个动作执行。</li>
<li>results: 实验结果显示，Auto-UI在新的设备控制benchmark AITW上达到了状态码的性能，具有动作类型预测精度90%和总成功率74%。代码公开可用于<a target="_blank" rel="noopener" href="https://github.com/cooelf/Auto-UI%E3%80%82">https://github.com/cooelf/Auto-UI。</a><details>
<summary>Abstract</summary>
Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -- leveraging a series of intermediate previous action histories and future action plans -- to help the agent decide what action to execute. We evaluate our approach on a new device-control benchmark AITW with 30K unique instructions, spanning multi-step tasks such as application operation, web searching, and web shopping. Experimental results show that Auto-UI achieves state-of-the-art performance with an action type prediction accuracy of 90% and an overall action success rate of 74%. Code is publicly available at https://github.com/cooelf/Auto-UI.
</details>
<details>
<summary>摘要</summary>
自动化用户界面（UI）代理，目的是自动化任务，不需要人工干预。最近的研究已经利用大型自然语言模型（LLM）来实现多种环境中的有效交互。为了与输入和输出对应的LLM的需求，现有的方法采用沙盒环境，通过外部工具和应用程序特定的API来解析环境并解释预测的动作。然而，这些方法经常会遇到推理不准确和错误传递风险。为了解决这些挑战，我们提出了Auto-UI，一种多模式解决方案，可以直接与界面交互，无需解析环境或依赖于应用程序特定的API。此外，我们还提出了链条动作技术，利用前一系列的历史动作和未来动作计划，帮助代理决定执行哪一个动作。我们在新的设备控制标准AITW上进行了实验，并取得了state-of-the-art表现，具体如下：* 动作类型预测精度达90%* 总体动作成功率达74%代码可以在https://github.com/cooelf/Auto-UI上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Review-of-Few-Shot-Learning-in-Medical-Imaging"><a href="#A-Systematic-Review-of-Few-Shot-Learning-in-Medical-Imaging" class="headerlink" title="A Systematic Review of Few-Shot Learning in Medical Imaging"></a>A Systematic Review of Few-Shot Learning in Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11433">http://arxiv.org/abs/2309.11433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eva Pachetti, Sara Colantonio</li>
<li>for: 这篇文章旨在给出医学影像分析领域中几何学学习的系统评论，尤其是在几何学学习方法中实现少数扩展学习。</li>
<li>methods: 这篇文章使用了系统性的文献搜寻方法，从2018年到2023年发表的80篇相关文章中选择了相关的文献。文章将这些文献分为不同的医疗结果（如肿瘤分类、疾病分类、影像调整等）、 investigate的 анатомі学结构（如心脏、肺等）以及使用的几何学学习方法。</li>
<li>results: 文章显示了几何学学习可以在大多数的结果中超过数据不足的问题，并且meta-learning是几何学学习中最受欢迎的方法，可以适应新任务的几何学学习。此外，文章还发现了在医学影像分析中几何学学习中使用的主要技术是supervised learning和semi-supervised learning，并且这些技术在医疗影像分析中表现最佳。最后，文章发现了主要应用领域主要是心脏、肺和腹部领域。<details>
<summary>Abstract</summary>
The lack of annotated medical images limits the performance of deep learning models, which usually need large-scale labelled datasets. Few-shot learning techniques can reduce data scarcity issues and enhance medical image analysis, especially with meta-learning. This systematic review gives a comprehensive overview of few-shot learning in medical imaging. We searched the literature systematically and selected 80 relevant articles published from 2018 to 2023. We clustered the articles based on medical outcomes, such as tumour segmentation, disease classification, and image registration; anatomical structure investigated (i.e. heart, lung, etc.); and the meta-learning method used. For each cluster, we examined the papers' distributions and the results provided by the state-of-the-art. In addition, we identified a generic pipeline shared among all the studies. The review shows that few-shot learning can overcome data scarcity in most outcomes and that meta-learning is a popular choice to perform few-shot learning because it can adapt to new tasks with few labelled samples. In addition, following meta-learning, supervised learning and semi-supervised learning stand out as the predominant techniques employed to tackle few-shot learning challenges in medical imaging and also best performing. Lastly, we observed that the primary application areas predominantly encompass cardiac, pulmonary, and abdominal domains. This systematic review aims to inspire further research to improve medical image analysis and patient care.
</details>
<details>
<summary>摘要</summary>
因为医疗影像标签的缺乏，深度学习模型的性能受到限制。不过，几个shot学习技术可以解决数据缺乏问题，提高医疗影像分析，特别是在meta-learning中。这个系统性审查给出了医疗影像中几个shot学习的全面回顾。我们在2018年至2023年发布的80篇相关文献中进行了系统性搜寻，并根据医疗结果（例如肿瘤分类、病理分类、影像调整）、 investigate体部（例如心脏、肺部等）和使用的meta-learning方法进行分组。对每个分组，我们评估了文献的分布和顶尖的结果。此外，我们发现了所有研究中的通用架构。审查结果表明，几个shot学习可以在大多数结果中突破数据缺乏问题，meta-learning是最受欢迎的选择，因为它可以适应新任务 WITH FEW labelled samples。此外，在医疗影像中，以supervised learning和semi-supervised learning为主的技术被大量运用，并且表现最佳。最后，我们发现主要应用领域主要是心脏、肺部和腹部领域。这个系统性审查的目的是鼓励进一步的研究，以提高医疗影像分析和patient care。
</details></li>
</ul>
<hr>
<h2 id="Generative-Pre-Training-of-Time-Series-Data-for-Unsupervised-Fault-Detection-in-Semiconductor-Manufacturing"><a href="#Generative-Pre-Training-of-Time-Series-Data-for-Unsupervised-Fault-Detection-in-Semiconductor-Manufacturing" class="headerlink" title="Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing"></a>Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11427">http://arxiv.org/abs/2309.11427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sewoong Lee, JinKyou Choi, Min Su Kim</li>
<li>For: 这个研究旨在运用时间序列数据的特征来探测半导体制造中的异常现象。* Methods: 研究使用时间序列嵌入和生成预训Transformers来预训时间序列数据，并使用标 entropy损失函数来分类异常时间序列和正常时间序列。* Results: 研究表明，我们的模型在UCSD时间序列分类数据集和化学蒸发成长（CVD）设备的处理记录上都显示出更好的表现，与过去的无supervision模型相比。我们的模型在EER上的F1分数最高，并且仅仅0.026下于无supervision基准。<details>
<summary>Abstract</summary>
This paper introduces TRACE-GPT, which stands for Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor data and detect faults on unlabeled datasets in semiconductor manufacturing. In semiconductor industry, classifying abnormal time-series sensor data from normal data is important because it is directly related to wafer defect. However, small, unlabeled, and even mixed training data without enough anomalies make classification tasks difficult. In this research, we capture features of time-series data with temporal convolutional embedding and Generative Pre-trained Transformer (GPT) to classify abnormal sequences from normal sequences using cross entropy loss. We prove that our model shows better performance than previous unsupervised models with both an open dataset, the University of California Riverside (UCR) time-series classification archive, and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model has the highest F1 score at Equal Error Rate (EER) across all datasets and is only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="EDMP-Ensemble-of-costs-guided-Diffusion-for-Motion-Planning"><a href="#EDMP-Ensemble-of-costs-guided-Diffusion-for-Motion-Planning" class="headerlink" title="EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning"></a>EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11414">http://arxiv.org/abs/2309.11414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kallol Saha, Vishal Mandadi, Jayaram Reddy, Ajit Srikanth, Aditya Agarwal, Bipasha Sen, Arun Singh, Madhava Krishna</li>
<li>for: 这篇论文是为了提出一种 combining classical 和 deep learning 的动作规划方法，以提高动作规划的成功率和普遍性。</li>
<li>methods: 本文使用了一种叫做 Ensemble-of-costs-guided Diffusion for Motion Planning（EDMP）的方法，它 combinines 经典的动作规划算法和深度学习算法，以提高动作规划的成功率和普遍性。EDMP 使用了一个 diffusion-based network，训练在一组多元可行的动作轨迹上。在执行过程中，我们Compute scene-specific costs，如 “碰撞成本”，以导引 diffusion 生成符合场景内的碰撞条件的有效轨迹。</li>
<li>results: 本文的结果显示，EDMP 能够与 State-of-the-Art 的深度学习基于方法相比，成功率有所提高，并且保留了经典步骤的普遍性。<details>
<summary>Abstract</summary>
Classical motion planning for robotic manipulation includes a set of general algorithms that aim to minimize a scene-specific cost of executing a given plan. This approach offers remarkable adaptability, as they can be directly used off-the-shelf for any new scene without needing specific training datasets. However, without a prior understanding of what diverse valid trajectories are and without specially designed cost functions for a given scene, the overall solutions tend to have low success rates. While deep-learning-based algorithms tremendously improve success rates, they are much harder to adopt without specialized training datasets. We propose EDMP, an Ensemble-of-costs-guided Diffusion for Motion Planning that aims to combine the strengths of classical and deep-learning-based motion planning. Our diffusion-based network is trained on a set of diverse kinematically valid trajectories. Like classical planning, for any new scene at the time of inference, we compute scene-specific costs such as "collision cost" and guide the diffusion to generate valid trajectories that satisfy the scene-specific constraints. Further, instead of a single cost function that may be insufficient in capturing diversity across scenes, we use an ensemble of costs to guide the diffusion process, significantly improving the success rate compared to classical planners. EDMP performs comparably with SOTA deep-learning-based methods while retaining the generalization capabilities primarily associated with classical planners.
</details>
<details>
<summary>摘要</summary>
经典运动规划 для机器人操作包括一组通用算法，旨在最小化Scene特定的执行计划的成本。这种方法具有很好的适应性，可以直接在新场景上使用，不需要特定的训练数据。然而，不知道多元有效轨迹的特点和场景特定的成本函数，全局的解决方案通常具有低成功率。深度学习基于算法在成功率上提供了很大的改善，但是它们更难于采用，需要特定的训练数据。我们提出了EDMP，一种ensemble-of-costs-guided Diffusion for Motion Planning，旨在结合经典和深度学习基于的运动规划。我们的扩散网络被训练在一组多元可行的轨迹上。在任何新场景的推理时，我们计算场景特定的碰撞成本和导引扩散来生成符合场景特定的约束的有效轨迹。此外，而不是单一的成本函数，我们使用一个ensemble of costs来引导扩散过程，明显提高成功率相比经典规划器。EDMP和SOTA深度学习基于方法相比，保留了经典规划器的总体化能力。
</details></li>
</ul>
<hr>
<h2 id="Long-Form-End-to-End-Speech-Translation-via-Latent-Alignment-Segmentation"><a href="#Long-Form-End-to-End-Speech-Translation-via-Latent-Alignment-Segmentation" class="headerlink" title="Long-Form End-to-End Speech Translation via Latent Alignment Segmentation"></a>Long-Form End-to-End Speech Translation via Latent Alignment Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11384">http://arxiv.org/abs/2309.11384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Polák, Ondřej Bojar</li>
<li>for: 这个论文的目的是提出一种实时同声翻译方法，可以处理长于几秒钟的语音数据。</li>
<li>methods: 这种方法使用现有的同声翻译encoder-decoder架构，并使用ST CTC进行分 segmentation。这个方法不需要额外的监督或参数，可以在实时中进行同声翻译和分 segmentation。</li>
<li>results: 在多种语言对和内外领域数据上，我们的方法可以 дости得状态的同声翻译质量，而且不需要额外的计算成本。<details>
<summary>Abstract</summary>
Current simultaneous speech translation models can process audio only up to a few seconds long. Contemporary datasets provide an oracle segmentation into sentences based on human-annotated transcripts and translations. However, the segmentation into sentences is not available in the real world. Current speech segmentation approaches either offer poor segmentation quality or have to trade latency for quality. In this paper, we propose a novel segmentation approach for a low-latency end-to-end speech translation. We leverage the existing speech translation encoder-decoder architecture with ST CTC and show that it can perform the segmentation task without supervision or additional parameters. To the best of our knowledge, our method is the first that allows an actual end-to-end simultaneous speech translation, as the same model is used for translation and segmentation at the same time. On a diverse set of language pairs and in- and out-of-domain data, we show that the proposed approach achieves state-of-the-art quality at no additional computational cost.
</details>
<details>
<summary>摘要</summary>
当前同时传输模型可以处理音频只有几秒长。当前数据提供了人注释的讲解和翻译，但实际世界中没有这样的分 segmentation。当前的Speech segmentation方法或者提供低质量的分 segmentation或者要求交换延迟和质量。在这篇论文中，我们提出了一种新的分 segmentation方法，用于低延迟的端到端 Speech translation。我们利用现有的Speech translation encoder-decoder架构和 ST CTC，并证明它可以完成分 segmentation任务无需监督或额外参数。根据我们所知，我们的方法是首次实现了实际的同时 Speech translation，因为同时使用了翻译和分 segmentation的同一模型。在多种语言对和内外领域数据上，我们示出了状态机器的质量，没有额外计算成本。
</details></li>
</ul>
<hr>
<h2 id="Discuss-Before-Moving-Visual-Language-Navigation-via-Multi-expert-Discussions"><a href="#Discuss-Before-Moving-Visual-Language-Navigation-via-Multi-expert-Discussions" class="headerlink" title="Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions"></a>Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11382">http://arxiv.org/abs/2309.11382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxing Long, Xiaoqi Li, Wenzhe Cai, Hao Dong</li>
<li>for: 这篇论文旨在提出一种新的零基础Visual Language Navigation（VLN）框架，以解决现有VLN方法单一自动思考的局限性。</li>
<li>methods: 该框架采用域专家的协助，通过讨论收集关键导航任务的信息，包括指令理解、环境识别和完成估计。</li>
<li>results: 经过广泛的实验表明，与域专家进行讨论可以有效地促进导航，提高指令相关信息的理解、更正偶极错误和筛选不一致的运动决策。相比单一自动思考，该方法在所有指标上表现出优异。<details>
<summary>Abstract</summary>
Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model's own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting, we introduce a novel zero-shot VLN framework. Within this framework, large models possessing distinct abilities are served as domain experts. Our proposed navigation agent, namely DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks like instruction understanding, environment perception, and completion estimation. Through comprehensive experiments, we demonstrate that discussions with domain experts can effectively facilitate navigation by perceiving instruction-relevant information, correcting inadvertent errors, and sifting through in-consistent movement decisions. The performances on the representative VLN task R2R show that our method surpasses the leading zero-shot VLN model by a large margin on all metrics. Additionally, real-robot experiments display the obvious advantages of our method over single-round self-thinking.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified ChineseVisual language navigation (VLN) is an embodied task that requires a wide range of skills, including understanding, perception, and planning. Previous VLN methods have relied solely on one model's own thinking to make predictions within one round. However, even the most advanced large language model GPT4 struggles with handling multiple tasks through single-round self-thinking. In this work, inspired by expert consultation meetings, we introduce a novel zero-shot VLN framework. In this framework, large models with distinct abilities serve as domain experts. Our proposed navigation agent, called DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks such as understanding instructions, perceiving the environment, and estimating completion. Through comprehensive experiments, we demonstrate that discussions with domain experts can effectively facilitate navigation by perceiving instruction-relevant information, correcting inadvertent errors, and filtering out inconsistent movement decisions. The performances on the representative VLN task R2R show that our method surpasses the leading zero-shot VLN model by a large margin on all metrics. Additionally, real-robot experiments display the obvious advantages of our method over single-round self-thinking.中文简体版：视觉语言导航（VLN）是一个需要各种技能的体验任务，包括理解、感知和规划。先前的VLN方法都是单一模型自己思考，但是即使最先进的大语言模型GPT4也在处理多任务时仍然陷入困难。在这个工作中，启发于专家咨询会议，我们引入了一种新的零扩展VLN框架。在这个框架中，具有不同能力的大模型服为域专家。我们提出的导航代理人称为DiscussNav，可以在每步移动之前与这些专家进行活动的讨论，收集关键导航子任务的信息。这些讨论包括理解指令、识别环境和估计完成度。通过广泛的实验，我们证明了与域专家进行讨论可以有效地促进导航，捕捉指令相关信息， исправ错误和筛选出不一致的移动决策。R2R任务表明，我们的方法在所有指标上胜过领先的零扩展VLN模型。此外，实际Robot实验也显示了我们方法在单一自我思考方面的明显优势。
</details></li>
</ul>
<hr>
<h2 id="Incremental-Blockwise-Beam-Search-for-Simultaneous-Speech-Translation-with-Controllable-Quality-Latency-Tradeoff"><a href="#Incremental-Blockwise-Beam-Search-for-Simultaneous-Speech-Translation-with-Controllable-Quality-Latency-Tradeoff" class="headerlink" title="Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff"></a>Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11379">http://arxiv.org/abs/2309.11379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Polák, Brian Yan, Shinji Watanabe, Alex Waibel, Ondřej Bojar</li>
<li>for:  simultanous speech translation</li>
<li>methods:  blockwise self-attentional encoder models, incremental blockwise beam search, local agreement or hold-$n$ policies</li>
<li>results: 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.Here’s the full translation in Simplified Chinese:</li>
<li>for: 这篇论文主要针对同时语言翻译。</li>
<li>methods: 这篇论文使用了块状自注意力编码器模型，并使用了增量块wise beam search和本地一致或保持-$n$ 策略来控制质量和延迟的质量。</li>
<li>results: 在 MuST-C 上实验结果显示，无需改变延迟或质量，可以获得0.6-3.6 BLEU 提升，或者可以降低0.8-1.4 s 的延迟。<details>
<summary>Abstract</summary>
Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \textit{incremental} translation to users. Further, this method lacks mechanisms for \textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode.   Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.
</details>
<details>
<summary>摘要</summary>
“块级自注意编码器模型在同时语音翻译方面最近几年来得到了一些承诺。这些模型使用块级搜索和假设可靠性分数来决定等待更多的输入语音之前继续翻译。然而，这种方法会维护多个假设，直到整个语音输入被消耗——这种方案无法直接显示单个增量翻译给用户。此外，这种方法缺乏控制质量vs延迟贸易的机制。我们提议修改增量块级搜索，并添加地方一致或保持-$n$ 策略来控制质量vs延迟的贸易。我们将我们的框架应用于在线或离线训练的模型，并证明两种类型都可以在线模式下使用。实验结果表明，在 Must-C 上得到了0.6-3.6 BLEU 提升，或0.8-1.4 s 延迟提升，无需改变质量或延迟。”
</details></li>
</ul>
<hr>
<h2 id="Preconditioned-Federated-Learning"><a href="#Preconditioned-Federated-Learning" class="headerlink" title="Preconditioned Federated Learning"></a>Preconditioned Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11378">http://arxiv.org/abs/2309.11378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyi Tao, Jindi Wu, Qun Li</li>
<li>for: 训练分布式机器学习模型，保持通信效率和隐私性。</li>
<li>methods: 基于本地适应和服务器端适应两个框架，采用新的协VAR matrix预conditioner，实现了更高的通信效率和更好的适应性。</li>
<li>results: 在 i.i.d. 和非 i.i.d. 情况下，实验结果表明我们的方法可以达到领先的性能水平。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed machine learning approach that enables model training in communication efficient and privacy-preserving manner. The standard optimization method in FL is Federated Averaging (FedAvg), which performs multiple local SGD steps between communication rounds. FedAvg has been considered to lack algorithm adaptivity compared to modern first-order adaptive optimizations. In this paper, we propose new communication-efficient FL algortithms based on two adaptive frameworks: local adaptivity (PreFed) and server-side adaptivity (PreFedOp). Proposed methods adopt adaptivity by using a novel covariance matrix preconditioner. Theoretically, we provide convergence guarantees for our algorithms. The empirical experiments show our methods achieve state-of-the-art performances on both i.i.d. and non-i.i.d. settings.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式机器学习方法，可以在通信效率和隐私保护的情况下进行模型训练。标准优化方法在 FL 中是联邦平均（FedAvg），它在通信轮次之间执行多个本地 SGD 步骤。FedAvg 已被认为在与现代首个适应优化相比lack algorithm adaptivity。在这篇论文中，我们提出了新的通信效率FL算法，基于两种适应框架：本地适应（PreFed）和服务器端适应（PreFedOp）。我们的方法采用适应性的novel协方差矩阵预conditioner。我们从理论上提供了收敛保证。实验表明，我们的方法在 i.i.d. 和非 i.i.d. 设置下达到了当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Hand-Gesture-Featured-Human-Motor-Adaptation-in-Tool-Delivery-using-Voice-Recognition"><a href="#Dynamic-Hand-Gesture-Featured-Human-Motor-Adaptation-in-Tool-Delivery-using-Voice-Recognition" class="headerlink" title="Dynamic Hand Gesture-Featured Human Motor Adaptation in Tool Delivery using Voice Recognition"></a>Dynamic Hand Gesture-Featured Human Motor Adaptation in Tool Delivery using Voice Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11368">http://arxiv.org/abs/2309.11368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haolin Fei, Stefano Tedeschi, Yanpei Huang, Andrew Kennedy, Ziwei Wang</li>
<li>for: 这个论文目的是提高人机合作的效率，使用多种modal interaction方式，以便用户可以专注于任务执行，而不需要额外培训用户机器人界面。</li>
<li>methods: 这个论文使用了手势认识、语音识别和可 switchable控制适应策略，以提供一个用户友好的人机合作框架。</li>
<li>results: 实验结果表明， static手势认识模块的准确率为94.3%，动态运动认识模块的准确率为97.6%。相比之下，人 Solo执行任务时，提出的方法可以提高工具交elivery的效率，而不会干扰人类意图。<details>
<summary>Abstract</summary>
Human-robot collaboration has benefited users with higher efficiency towards interactive tasks. Nevertheless, most collaborative schemes rely on complicated human-machine interfaces, which might lack the requisite intuitiveness compared with natural limb control. We also expect to understand human intent with low training data requirements. In response to these challenges, this paper introduces an innovative human-robot collaborative framework that seamlessly integrates hand gesture and dynamic movement recognition, voice recognition, and a switchable control adaptation strategy. These modules provide a user-friendly approach that enables the robot to deliver the tools as per user need, especially when the user is working with both hands. Therefore, users can focus on their task execution without additional training in the use of human-machine interfaces, while the robot interprets their intuitive gestures. The proposed multimodal interaction framework is executed in the UR5e robot platform equipped with a RealSense D435i camera, and the effectiveness is assessed through a soldering circuit board task. The experiment results have demonstrated superior performance in hand gesture recognition, where the static hand gesture recognition module achieves an accuracy of 94.3\%, while the dynamic motion recognition module reaches 97.6\% accuracy. Compared with human solo manipulation, the proposed approach facilitates higher efficiency tool delivery, without significantly distracting from human intents.
</details>
<details>
<summary>摘要</summary>
人机合作已经为用户带来更高的效率在互动任务中。然而，大多数合作方案依靠复杂的人机界面，可能缺乏自然的人机交互INTUITIVENESS。我们还期望在训练数据量少的情况下理解人类的意图。为回答这些挑战，本文介绍了一种创新的人机合作框架，它灵活地集成了手势认识、动态运动认识、语音识别和可调制控制策略。这些模块提供了一种用户友好的方法，使得机器人可以根据用户需要提供工具，特别是用户在双手工作时。因此，用户可以专注于任务执行而不需要额外培训人机界面的使用，而机器人可以理解用户的自然姿势。本文所提出的多模式互动框架在UR5e机器人平台上执行，装备了RealSense D435i摄像头，并通过焊接电路板任务进行评估。实验结果显示， static手势认识模块的准确率为94.3%，而动态运动认识模块的准确率达97.6%。相比人类独立操作，提议的方法可以提高工具交付效率，无需明显干扰人类意图。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Graph-Question-Answering-for-Materials-Science-KGQA4MAT-Developing-Natural-Language-Interface-for-Metal-Organic-Frameworks-Knowledge-Graph-MOF-KG"><a href="#Knowledge-Graph-Question-Answering-for-Materials-Science-KGQA4MAT-Developing-Natural-Language-Interface-for-Metal-Organic-Frameworks-Knowledge-Graph-MOF-KG" class="headerlink" title="Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG)"></a>Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11361">http://arxiv.org/abs/2309.11361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan An, Jane Greenberg, Alex Kalinowski, Xintong Zhao, Xiaohua Hu, Fernando J. Uribe-Romo, Kyle Langlois, Jacob Furst, Diego A. Gómez-Gualdrón</li>
<li>for: 本研究开发了一个包括161个复杂问题的知识Graph问答板（KGQA4MAT），旨在提高材料科学领域知识Graph（MOF-KG）的访问性。</li>
<li>methods: 本研究使用了一种自然语言界面来查询MOF-KG，并开发了一个系统来使用ChatGPT将自然语言问题翻译成正式的KG查询语言。</li>
<li>results: 研究发现ChatGPT可以有效地解决不同平台和查询语言的KG问答问题，并且可以帮助加速材料科学领域知识Graph的搜索和探索。<details>
<summary>Abstract</summary>
We present a comprehensive benchmark dataset for Knowledge Graph Question Answering in Materials Science (KGQA4MAT), with a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has been constructed by integrating structured databases and knowledge extracted from the literature. To enhance MOF-KG accessibility for domain experts, we aim to develop a natural language interface for querying the knowledge graph. We have developed a benchmark comprised of 161 complex questions involving comparison, aggregation, and complicated graph structures. Each question is rephrased in three additional variations, resulting in 644 questions and 161 KG queries. To evaluate the benchmark, we have developed a systematic approach for utilizing ChatGPT to translate natural language questions into formal KG queries. We also apply the approach to the well-known QALD-9 dataset, demonstrating ChatGPT's potential in addressing KGQA issues for different platforms and query languages. The benchmark and the proposed approach aim to stimulate further research and development of user-friendly and efficient interfaces for querying domain-specific materials science knowledge graphs, thereby accelerating the discovery of novel materials.
</details>
<details>
<summary>摘要</summary>
我们提供了一个完整的基准数据集 для知识 graphsQuestion Answering in Materials Science (KGQA4MAT), WITH a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has been constructed by integrating structured databases and knowledge extracted from the literature. To enhance MOF-KG accessibility for domain experts, we aim to develop a natural language interface for querying the knowledge graph. We have developed a benchmark comprised of 161 complex questions involving comparison, aggregation, and complicated graph structures. Each question is rephrased in three additional variations, resulting in 644 questions and 161 KG queries. To evaluate the benchmark, we have developed a systematic approach for utilizing ChatGPT to translate natural language questions into formal KG queries. We also apply the approach to the well-known QALD-9 dataset, demonstrating ChatGPT's potential in addressing KGQA issues for different platforms and query languages. The benchmark and the proposed approach aim to stimulate further research and development of user-friendly and efficient interfaces for querying domain-specific materials science knowledge graphs, thereby accelerating the discovery of novel materials.Here is the translation in Traditional Chinese:我们提供了一个完整的基准数据集 для知识 graphsQuestion Answering in Materials Science (KGQA4MAT),  WITH a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) 已经建立了由structured databases和文献中提取的知识 integrate。为了增强MOF-KG对领域专家的存取，我们目标是开发一个自然语言界面来查询知识 Graph。我们已经开发了一个包含161个复杂问题，涉及比较、总和、图像结构的问题。每个问题都有三个版本，共计644个问题和161个KG查询。为了评估基准，我们开发了一个系统性的方法，使用ChatGPT来将自然语言问题转换为正式的KG查询。我们还将这个方法应用到知名的QALD-9数据集上，展示了ChatGPT对不同平台和查询语言的应用潜力。基准和我们提出的方法的目的是促进领域专家用户友好和高效的界面来查询领域专门的材料科学知识图表，以便加速发现新材料的发现。
</details></li>
</ul>
<hr>
<h2 id="3D-Face-Reconstruction-the-Road-to-Forensics"><a href="#3D-Face-Reconstruction-the-Road-to-Forensics" class="headerlink" title="3D Face Reconstruction: the Road to Forensics"></a>3D Face Reconstruction: the Road to Forensics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11357">http://arxiv.org/abs/2309.11357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Maurizio La Cava, Giulia Orrù, Martin Drahansky, Gian Luca Marcialis, Fabio Roli</li>
<li>for: 法律领域中的3D面部重建应用</li>
<li>methods: 使用Surveillance影像和照片进行3D面部重建</li>
<li>results: 略见问题，尚未确立3D面部重建在法律领域的积极角色<details>
<summary>Abstract</summary>
3D face reconstruction algorithms from images and videos are applied to many fields, from plastic surgery to the entertainment sector, thanks to their advantageous features. However, when looking at forensic applications, 3D face reconstruction must observe strict requirements that still make its possible role in bringing evidence to a lawsuit unclear. An extensive investigation of the constraints, potential, and limits of its application in forensics is still missing. Shedding some light on this matter is the goal of the present survey, which starts by clarifying the relation between forensic applications and biometrics, with a focus on face recognition. Therefore, it provides an analysis of the achievements of 3D face reconstruction algorithms from surveillance videos and mugshot images and discusses the current obstacles that separate 3D face reconstruction from an active role in forensic applications. Finally, it examines the underlying data sets, with their advantages and limitations, while proposing alternatives that could substitute or complement them.
</details>
<details>
<summary>摘要</summary>
三维面部重建算法从图像和视频应用到多个领域，从整形外科到娱乐业，因为它们的优点。但当看到审判应用时，三维面部重建必须遵守严格的要求，这些要求仍然使其在提供法律证据的角色是不清晰。为了解决这个问题，本调查的目的是 shedding some light on this matter，开始从审判应用和生物ometrics之间的关系进行清楚的解释，并对surveillance视频和抓捕图像中的3D面部重建算法的成果进行分析，并讨论当前障碍三维面部重建在审判应用中扮演活跃角色的原因。最后，它检查了下面的数据集，包括其优点和限制，并提出了代替或补充的方案。Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Survey-on-Rare-Event-Prediction"><a href="#A-Comprehensive-Survey-on-Rare-Event-Prediction" class="headerlink" title="A Comprehensive Survey on Rare Event Prediction"></a>A Comprehensive Survey on Rare Event Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11356">http://arxiv.org/abs/2309.11356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chathurangi Shyalika, Ruwan Wickramarachchi, Amit Sheth</li>
<li>For: 本研究主要针对频率低的罕见事件预测，即使用机器学习和数据分析方法来预测这些事件的发生。* Methods: 本文综述了目前预测罕见事件的方法，包括数据处理、算法方法和评估方法等，并从不同的数据模式和预测方法角度进行了梳理和分析。* Results: 本文结果显示，预测罕见事件存在许多挑战，如数据不均衡、模型偏向等问题，同时还存在许多研究缺乏或未得到充分发挥的问题。<details>
<summary>Abstract</summary>
Rare event prediction involves identifying and forecasting events with a low probability using machine learning and data analysis. Due to the imbalanced data distributions, where the frequency of common events vastly outweighs that of rare events, it requires using specialized methods within each step of the machine learning pipeline, i.e., from data processing to algorithms to evaluation protocols. Predicting the occurrences of rare events is important for real-world applications, such as Industry 4.0, and is an active research area in statistical and machine learning. This paper comprehensively reviews the current approaches for rare event prediction along four dimensions: rare event data, data processing, algorithmic approaches, and evaluation approaches. Specifically, we consider 73 datasets from different modalities (i.e., numerical, image, text, and audio), four major categories of data processing, five major algorithmic groupings, and two broader evaluation approaches. This paper aims to identify gaps in the current literature and highlight the challenges of predicting rare events. It also suggests potential research directions, which can help guide practitioners and researchers.
</details>
<details>
<summary>摘要</summary>
罕seen事件预测 involve identifying和forecasting事件with a low probability using机器学习和数据分析。由于数据分布的偏度，其中常见事件的频率远远大于罕seen事件的频率，因此需要使用特殊的方法在每个机器学习管道中，从数据处理到算法到评估协议。预测罕seen事件的发生是现实世界应用中的重要问题，如第四代工业，并是机器学习的活跃研究领域。本文全面回顾当前approaches for rare event prediction along four dimensions：罕seen事件数据、数据处理、算法approaches、和评估approaches。Specifically, we consider 73 datasets from different modalities（i.e., numerical, image, text, and audio）、四大类数据处理、五大算法组合、和两大评估方法。本文的目的是要标识当前文献中的空白和预测罕seen事件的挑战，并提出了 potential research directions，以帮助实践者和研究人员。
</details></li>
</ul>
<hr>
<h2 id="C-cdot-ASE-Learning-Conditional-Adversarial-Skill-Embeddings-for-Physics-based-Characters"><a href="#C-cdot-ASE-Learning-Conditional-Adversarial-Skill-Embeddings-for-Physics-based-Characters" class="headerlink" title="C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters"></a>C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11351">http://arxiv.org/abs/2309.11351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, Wenping Wang</li>
<li>for: 这个论文目标是为physics-based characters提供一个有效的学习推荐系统，使得这些角色可以学习多种技能并提供可控性。</li>
<li>methods: 这个系统使用了 conditional Adversarial Skill Embeddings（C$\cdot$ASE），将技能动作分成不同的子集，并使用低级别的条件模型来学习条件行为分布。</li>
<li>results: 论文表明，使用C$\cdot$ASE可以生成高度多样化和现实的技能动作，并且可以在不同的下游任务中重用。此外，该系统还提供了一个高级别的政策或用户可以使用某种技能特定的指定来控制角色的行为。<details>
<summary>Abstract</summary>
We present C$\cdot$ASE, an efficient and effective framework that learns conditional Adversarial Skill Embeddings for physics-based characters. Our physically simulated character can learn a diverse repertoire of skills while providing controllability in the form of direct manipulation of the skills to be performed. C$\cdot$ASE divides the heterogeneous skill motions into distinct subsets containing homogeneous samples for training a low-level conditional model to learn conditional behavior distribution. The skill-conditioned imitation learning naturally offers explicit control over the character's skills after training. The training course incorporates the focal skill sampling, skeletal residual forces, and element-wise feature masking to balance diverse skills of varying complexities, mitigate dynamics mismatch to master agile motions and capture more general behavior characteristics, respectively. Once trained, the conditional model can produce highly diverse and realistic skills, outperforming state-of-the-art models, and can be repurposed in various downstream tasks. In particular, the explicit skill control handle allows a high-level policy or user to direct the character with desired skill specifications, which we demonstrate is advantageous for interactive character animation.
</details>
<details>
<summary>摘要</summary>
我们提出C$\cdot$ASE框架，一种高效有效的框架，学习受条件敌意素嵌入，用于物理基础的角色。我们的物理模拟角色可以学习多种多样的技能，同时提供可控性，通过直接控制技能的执行。C$\cdot$ASE将不同的技能动作分成不同的子集，对具有相同性的样本进行训练低级别的条件模型，学习条件行为分布。通过技能条件学习，可以直接控制角色的技能，并且可以在训练过程中通过焦点技能采样、骨骼剩余力和元素特征掩码来平衡多种技能的复杂性，弥补动力匹配问题，捕捉更加普遍的行为特征。一旦训练完成，条件模型可以生成高度多样化和真实的技能，超越当前模型，并且可以在下游任务中重用。特别是，条件控制把手允许高级政策或用户指定角色的愿望技能规格，我们示示其对交互角色动画有利。
</details></li>
</ul>
<hr>
<h2 id="TRAVID-An-End-to-End-Video-Translation-Framework"><a href="#TRAVID-An-End-to-End-Video-Translation-Framework" class="headerlink" title="TRAVID: An End-to-End Video Translation Framework"></a>TRAVID: An End-to-End Video Translation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11338">http://arxiv.org/abs/2309.11338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prottay Kumar Adhikary, Bandaru Sugandhi, Subhojit Ghimire, Santanu Pal, Partha Pakray<br>for: 这篇论文是为了提供一种实现语言翻译的视频翻译系统，以便在不同语言背景下进行有效的沟通。methods: 该系统使用了一种综合语音和视频的翻译方法，通过具体的语音和视频对应关系来实现视频中的语言翻译。results: 该系统可以帮助学生和用户在低资源环境中进行有效的学习和沟通，同时提供了一种更加真实和吸引人的学习环境，从而提高学习效果和参与度。<details>
<summary>Abstract</summary>
In today's globalized world, effective communication with people from diverse linguistic backgrounds has become increasingly crucial. While traditional methods of language translation, such as written text or voice-only translations, can accomplish the task, they often fail to capture the complete context and nuanced information conveyed through nonverbal cues like facial expressions and lip movements. In this paper, we present an end-to-end video translation system that not only translates spoken language but also synchronizes the translated speech with the lip movements of the speaker. Our system focuses on translating educational lectures in various Indian languages, and it is designed to be effective even in low-resource system settings. By incorporating lip movements that align with the target language and matching them with the speaker's voice using voice cloning techniques, our application offers an enhanced experience for students and users. This additional feature creates a more immersive and realistic learning environment, ultimately making the learning process more effective and engaging.
</details>
<details>
<summary>摘要</summary>
今天的全球化世界中，与不同语言背景的人进行有效沟通已经变得越来越重要。传统的语言翻译方法，如文本或声音翻译，可以完成任务，但它们经常无法捕捉 spoken language 中的完整上下文和细节信息。在这篇论文中，我们提出了一个端到端视频翻译系统，不仅翻译 spoken language，还将翻译后的语音与说话人的嘴语ynchronize。我们的系统专注于翻译印度各语言的教育讲解，并且针对具有低资源系统的设置进行设计。通过使用声音恶搅技术，我们的应用程序将嘴语与目标语言的对应语音进行匹配，从而提供了一个更加真实和有趣的学习环境。这种附加的特性使得学习过程更加有效和有趣。
</details></li>
</ul>
<hr>
<h2 id="Gold-YOLO-Efficient-Object-Detector-via-Gather-and-Distribute-Mechanism"><a href="#Gold-YOLO-Efficient-Object-Detector-via-Gather-and-Distribute-Mechanism" class="headerlink" title="Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism"></a>Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11331">http://arxiv.org/abs/2309.11331</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/Efficient-Computing">https://github.com/huawei-noah/Efficient-Computing</a></li>
<li>paper_authors: Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Kai Han, Yunhe Wang</li>
<li>for:  This paper aims to improve the object detection performance of YOLO-series models by introducing a new Gather-Distribute (GD) mechanism and implementing MAE-style pretraining.</li>
<li>methods:  The proposed Gold-YOLO model uses a GD mechanism that combines convolution and self-attention operations to improve multi-scale feature fusion. The model also uses MAE-style pretraining to enhance the performance.</li>
<li>results:  The Gold-YOLO model achieves an outstanding 39.9% AP on the COCO val2017 dataset and 1030 FPS on a T4 GPU, outperforming the previous SOTA model YOLOv6-3.0-N by +2.4% in terms of AP.<details>
<summary>Abstract</summary>
In the past years, YOLO-series models have emerged as the leading approaches in the area of real-time object detection. Many studies pushed up the baseline to a higher level by modifying the architecture, augmenting data and designing new losses. However, we find previous models still suffer from information fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation Network (PANet) have alleviated this. Therefore, this study provides an advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with convolution and self-attention operations. This new designed model named as Gold-YOLO, which boosts the multi-scale feature fusion capabilities and achieves an ideal balance between latency and accuracy across all model scales. Additionally, we implement MAE-style pretraining in the YOLO-series for the first time, allowing YOLOseries models could be to benefit from unsupervised pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017 datasets and 1030 FPS on a T4 GPU, which outperforms the previous SOTA model YOLOv6-3.0-N with similar FPS by +2.4%. The PyTorch code is available at https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO, and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/Gold_YOLO.
</details>
<details>
<summary>摘要</summary>
在过去的几年中，YOLO系列模型在实时对象检测领域取得了领先地位。许多研究尝试提高基线，通过修改架构、增强数据和设计新的损失函数。然而，我们发现先前的模型仍然受到信息融合问题的困扰，尽管Feature Pyramid Network（FPN）和Path Aggregation Network（PANet）已经减轻了这个问题。因此，本研究提出了一种高级的聚合分发机制（GD）机制，通过 convolution 和自注意操作实现。这新的设计的模型被称为 Gold-YOLO，它提高了多尺度特征融合能力，并在所有模型缩放水平上实现了理想的平衡 между延迟和准确率。此外，我们在 YOLO 系列模型中实施了 MAE 风格的预训练，让 YOLO 系列模型可以从无监督预训练中受益。Gold-YOLO-N 在 COCO val2017 数据集上达到了出色的 39.9% AP 和 T4 GPU 上的 1030 FPS，超过了先前的 SOTA 模型 YOLOv6-3.0-N 的相似 FPS 值 by +2.4%。PyTorch 代码可以在 <https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO> 找到，MindSpore 代码可以在 <https://gitee.com/mindspore/models/tree/master/research/cv/Gold_YOLO> 找到。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Pricing-of-Applications-in-Cloud-Marketplaces-using-Game-Theory"><a href="#Dynamic-Pricing-of-Applications-in-Cloud-Marketplaces-using-Game-Theory" class="headerlink" title="Dynamic Pricing of Applications in Cloud Marketplaces using Game Theory"></a>Dynamic Pricing of Applications in Cloud Marketplaces using Game Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11316">http://arxiv.org/abs/2309.11316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Safiye Ghasemi, Mohammad Reza Meybodi, Mehdi Dehghan Takht-Fooladi, Amir Masoud Rahmani</li>
<li>for: 这个论文旨在研究云市场竞争对应的价格策略，以帮助企业更好地制定价格策略。</li>
<li>methods: 该论文采用了游戏理论来设计动态价格策略，并在委员会中考虑了多家提供商的竞争。</li>
<li>results: 该论文通过数学模型来研究云市场竞争，并证明了存在和uniqueness的纳什平衡，从而为企业提供了新的动态价格策略。<details>
<summary>Abstract</summary>
The competitive nature of Cloud marketplaces as new concerns in delivery of services makes the pricing policies a crucial task for firms. so that, pricing strategies has recently attracted many researchers. Since game theory can handle such competing well this concern is addressed by designing a normal form game between providers in current research. A committee is considered in which providers register for improving their competition based pricing policies. The functionality of game theory is applied to design dynamic pricing policies. The usage of the committee makes the game a complete information one, in which each player is aware of every others payoff functions. The players enhance their pricing policies to maximize their profits. The contribution of this paper is the quantitative modeling of Cloud marketplaces in form of a game to provide novel dynamic pricing strategies; the model is validated by proving the existence and the uniqueness of Nash equilibrium of the game.
</details>
<details>
<summary>摘要</summary>
云市场的竞争性新问题在服务交付中带来了价格策略的核心任务 для公司。因此，价格策略在最近吸引了许多研究人员。由于游戏理论可以良好处理这种竞争，因此在当前研究中，设计了一个委员会，让提供者为了改善其竞争基础价格策略进行注册。通过游戏理论的应用，设计了动态价格策略。由于委员会的存在，游戏变为完全信息游戏，每个玩家知道彼此的利益函数。玩家通过优化价格策略来 maximize 利润。本文的贡献在于以游戏的形式对云市场进行量化模型化，提供了新的动态价格策略；模型的存在和uniqueness 的证明，证明了这种游戏的稳定性。
</details></li>
</ul>
<hr>
<h2 id="A-Competition-based-Pricing-Strategy-in-Cloud-Markets-using-Regret-Minimization-Techniques"><a href="#A-Competition-based-Pricing-Strategy-in-Cloud-Markets-using-Regret-Minimization-Techniques" class="headerlink" title="A Competition-based Pricing Strategy in Cloud Markets using Regret Minimization Techniques"></a>A Competition-based Pricing Strategy in Cloud Markets using Regret Minimization Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11312">http://arxiv.org/abs/2309.11312</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Ghasemi, M. R. Meybodi, M. Dehghan, A. M. Rahmani</li>
<li>for: This paper aims to address the challenge of pricing in Cloud computing marketplaces, where providers compete without knowing each other’s pricing policies.</li>
<li>methods: The paper proposes a pricing policy based on regret minimization and applies it to an incomplete-information game modeling the competition among Cloud providers. The algorithm updates the distribution of strategies based on experienced regret, leading to faster minimization of regret and increased profits for providers.</li>
<li>results: The experimental results show that the proposed pricing policy leads to much greater increases in provider profits compared to other pricing policies, and the efficiency of various regret minimization techniques in a simulated marketplace of Cloud is discussed. Additionally, the study examines the return on investment of providers in considered organizations and finds promising results.Here’s the Chinese translation of the three key points:</li>
<li>for: 这篇论文目标是解决云计算市场场所中的价格问题， provider competing without knowing each other’s pricing policies。</li>
<li>methods: 论文提出一种基于后悔最小化的价格策略，并应用到了不完全信息游戏中模拟云提供商的竞争。算法根据经验的后悔来更新策略分布，导致快速减少后悔。</li>
<li>results: 实验结果表明，提出的价格策略在其他价格策略的比较中显示出了很大的增长，并且在模拟云中的竞争市场中，不同的后悔最小化技术的效率得到了详细的讨论。此外，论文还研究了考虑了不同组织中提供商的投资回报，并发现了有前提。<details>
<summary>Abstract</summary>
Cloud computing as a fairly new commercial paradigm, widely investigated by different researchers, already has a great range of challenges. Pricing is a major problem in Cloud computing marketplace; as providers are competing to attract more customers without knowing the pricing policies of each other. To overcome this lack of knowledge, we model their competition by an incomplete-information game. Considering the issue, this work proposes a pricing policy related to the regret minimization algorithm and applies it to the considered incomplete-information game. Based on the competition based marketplace of the Cloud, providers update the distribution of their strategies using the experienced regret. The idea of iteratively applying the algorithm for updating probabilities of strategies causes the regret get minimized faster. The experimental results show much more increase in profits of the providers in comparison with other pricing policies. Besides, the efficiency of a variety of regret minimization techniques in a simulated marketplace of Cloud are discussed which have not been observed in the studied literature. Moreover, return on investment of providers in considered organizations is studied and promising results appeared.
</details>
<details>
<summary>摘要</summary>
云计算作为一种比较新的商业模式，已经广泛研究了不同的研究者。在云计算市场中，价格是一个主要的问题，Provider competing to attract more customers without knowing each other's pricing policies。为了解决这个问题，我们模拟了这个 incomplete-information game。基于云计算市场的竞争性，提供者通过经验的 regret 更新分布的策略。iteratively applying the algorithm for updating probabilities of strategies causes the regret get minimized faster。实验结果表明，与其他价格策略相比，提供者的利润增加了很多。此外，我们还发现了一些 regret minimization techniques 在云计算市场中的效率，这些result未经studied literature。此外，我们还研究了Provider的投资回报，并获得了扎实的结果。
</details></li>
</ul>
<hr>
<h2 id="Rating-Prediction-in-Conversational-Task-Assistants-with-Behavioral-and-Conversational-Flow-Features"><a href="#Rating-Prediction-in-Conversational-Task-Assistants-with-Behavioral-and-Conversational-Flow-Features" class="headerlink" title="Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features"></a>Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11307">http://arxiv.org/abs/2309.11307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rafaelhferreira/cta_rating_prediction">https://github.com/rafaelhferreira/cta_rating_prediction</a></li>
<li>paper_authors: Rafael Ferreira, David Semedo, João Magalhães</li>
<li>for: 预测对话任务助手（CTA）的成功可以帮助我们理解用户行为并采取相应的行动。</li>
<li>methods: 这篇论文提出了TB-Rater模型，这是一种将对话流程特征与用户行为特征结合在一起的Transformer模型，用于在CTA场景下预测用户评分。具体来说，我们使用了真实的人类-机器人对话和在Alexa TaskBot挑战中收集的用户评分数据。</li>
<li>results: 我们的结果表明，模型对话流程和用户行为方面的特征可以在单个模型中结合，以预测Offline评分。此外，对CTA特有的行为特征进行分析，可以为未来系统提供参考。<details>
<summary>Abstract</summary>
Predicting the success of Conversational Task Assistants (CTA) can be critical to understand user behavior and act accordingly. In this paper, we propose TB-Rater, a Transformer model which combines conversational-flow features with user behavior features for predicting user ratings in a CTA scenario. In particular, we use real human-agent conversations and ratings collected in the Alexa TaskBot challenge, a novel multimodal and multi-turn conversational context. Our results show the advantages of modeling both the conversational-flow and behavioral aspects of the conversation in a single model for offline rating prediction. Additionally, an analysis of the CTA-specific behavioral features brings insights into this setting and can be used to bootstrap future systems.
</details>
<details>
<summary>摘要</summary>
预测对话任务助手（CTA）的成功可以帮助我们更好地理解用户行为，从而更好地行动。在这篇论文中，我们提出了TB-Rater模型，这是一个基于转换器模型，结合对话流程特征和用户行为特征来预测用户评分在CTA场景中。具体来说，我们使用了真实的人类-机器人对话和在Alexa TaskBot挑战中收集的用户评分数据，这是一个新的多模式和多轮对话上下文。我们的结果表明，将对话流程和行为方面的特征模型在单个模型中可以在线评分中获得优势。此外，对CTA特有的行为特征进行分析，可以为未来系统提供Bootstrap。
</details></li>
</ul>
<hr>
<h2 id="FaceDiffuser-Speech-Driven-3D-Facial-Animation-Synthesis-Using-Diffusion"><a href="#FaceDiffuser-Speech-Driven-3D-Facial-Animation-Synthesis-Using-Diffusion" class="headerlink" title="FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion"></a>FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11306">http://arxiv.org/abs/2309.11306</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uuembodiedsocialai/FaceDiffuser">https://github.com/uuembodiedsocialai/FaceDiffuser</a></li>
<li>paper_authors: Stefan Stan, Kazi Injamamul Haque, Zerrin Yumak</li>
<li>for: 这个论文旨在解决Current methods mostly focus on deterministic deep learning methods for speech-driven 3D facial animation synthesis, which do not accurately capture non-verbal facial cues.</li>
<li>methods: 该方法基于Diffusion Technique，使用预训练的大语音表示模型HuBERT对音频输入进行编码。</li>
<li>results: 我们的方法在对比于现有方法时达到了更好或相当的结果，并且引入了一个新的基于blendshape的rigged character的数据集。Here’s the full summary in Simplified Chinese:</li>
<li>for: 这个论文旨在解决Current methods mostly focus on deterministic deep learning methods for speech-driven 3D facial animation synthesis, which do not accurately capture non-verbal facial cues.</li>
<li>methods: 该方法基于Diffusion Technique，使用预训练的大语音表示模型HuBERT对音频输入进行编码。</li>
<li>results: 我们的方法在对比于现有方法时达到了更好或相当的结果，并且引入了一个新的基于blendshape的rigged character的数据集。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Speech-driven 3D facial animation synthesis has been a challenging task both in industry and research. Recent methods mostly focus on deterministic deep learning methods meaning that given a speech input, the output is always the same. However, in reality, the non-verbal facial cues that reside throughout the face are non-deterministic in nature. In addition, majority of the approaches focus on 3D vertex based datasets and methods that are compatible with existing facial animation pipelines with rigged characters is scarce. To eliminate these issues, we present FaceDiffuser, a non-deterministic deep learning model to generate speech-driven facial animations that is trained with both 3D vertex and blendshape based datasets. Our method is based on the diffusion technique and uses the pre-trained large speech representation model HuBERT to encode the audio input. To the best of our knowledge, we are the first to employ the diffusion method for the task of speech-driven 3D facial animation synthesis. We have run extensive objective and subjective analyses and show that our approach achieves better or comparable results in comparison to the state-of-the-art methods. We also introduce a new in-house dataset that is based on a blendshape based rigged character. We recommend watching the accompanying supplementary video. The code and the dataset will be publicly available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>人工智能驱动的3D面部动画生成问题在行业和研究中都是挑战性的。现有的方法大多涉及决定性深度学习方法，即给定一个语音输入，输出总是一样的。然而，现实中的非语言面部征标是不决定的性质。此外，大多数方法都集中在3D顶点基本的数据集和方法上，与现有的人物动画管道相容的方法scarce。为解决这些问题，我们介绍FaceDiffuser，一种非决定性深度学习模型，用于生成语音驱动的3D面部动画。我们的方法基于扩散技术，使用预训练的大语音表示模型HuBERT来编码音频输入。到目前为止，我们是第一个使用扩散方法来解决语音驱动3D面部动画生成问题。我们进行了广泛的对象和主观分析，并证明我们的方法可以与当前状态的方法相比或更好的成绩。我们还介绍了一个新的基于blendshape的人物动画数据集。建议观看附加的补充视频。代码和数据集将公开发布。
</details></li>
</ul>
<hr>
<h2 id="A-Cost-Aware-Mechanism-for-Optimized-Resource-Provisioning-in-Cloud-Computing"><a href="#A-Cost-Aware-Mechanism-for-Optimized-Resource-Provisioning-in-Cloud-Computing" class="headerlink" title="A Cost-Aware Mechanism for Optimized Resource Provisioning in Cloud Computing"></a>A Cost-Aware Mechanism for Optimized Resource Provisioning in Cloud Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11299">http://arxiv.org/abs/2309.11299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Safiye Ghasemi, Mohammad Reza Meybodi, Mehdi Dehghan Takht Fooladi, Amir Masoud Rahmani</li>
<li>for: 这篇论文旨在提出一种新的资源配置方法，以减少资源配置成本的方式来满足需求。</li>
<li>methods: 本文使用了学习自动过程来选择最适合的资源来主机每个服务，并考虑成本和服务需求。</li>
<li>results: 实验结果显示，我们的方法能够有效地运行许多不同类型的应用程序，并且可以适当地减少资源配置成本。<details>
<summary>Abstract</summary>
Due to the recent wide use of computational resources in cloud computing, new resource provisioning challenges have been emerged. Resource provisioning techniques must keep total costs to a minimum while meeting the requirements of the requests. According to widely usage of cloud services, it seems more challenging to develop effective schemes for provisioning services cost-effectively; we have proposed a novel learning based resource provisioning approach that achieves cost-reduction guarantees of demands. The contributions of our optimized resource provisioning (ORP) approach are as follows. Firstly, it is designed to provide a cost-effective method to efficiently handle the provisioning of requested applications; while most of the existing models allow only workflows in general which cares about the dependencies of the tasks, ORP performs based on services of which applications comprised and cares about their efficient provisioning totally. Secondly, it is a learning automata-based approach which selects the most proper resources for hosting each service of the demanded application; our approach considers both cost and service requirements together for deploying applications. Thirdly, a comprehensive evaluation is performed for three typical workloads: data-intensive, process-intensive and normal applications. The experimental results show that our method adapts most of the requirements efficiently, and furthermore the resulting performance meets our design goals.
</details>
<details>
<summary>摘要</summary>
The contributions of our optimized resource provisioning (ORP) approach are as follows:1. Cost-effective method: ORP provides a cost-effective method to efficiently handle the provisioning of requested applications, while most existing models only consider workflows in general and ignore the dependencies of tasks. ORP takes into account the services that applications comprise and cares about their efficient provisioning.2. Learning automata-based approach: ORP is a learning automata-based approach that selects the most appropriate resources for hosting each service of the demanded application. Our approach considers both cost and service requirements together for deploying applications.3. Comprehensive evaluation: We conducted a comprehensive evaluation for three typical workloads: data-intensive, process-intensive, and normal applications. The experimental results show that our method adapts to most of the requirements efficiently, and the resulting performance meets our design goals.
</details></li>
</ul>
<hr>
<h2 id="CPLLM-Clinical-Prediction-with-Large-Language-Models"><a href="#CPLLM-Clinical-Prediction-with-Large-Language-Models" class="headerlink" title="CPLLM: Clinical Prediction with Large Language Models"></a>CPLLM: Clinical Prediction with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11295">http://arxiv.org/abs/2309.11295</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nadavlab/CPLLM">https://github.com/nadavlab/CPLLM</a></li>
<li>paper_authors: Ofir Ben Shoham, Nadav Rappoport</li>
<li>for: 这个论文是为了提出一种基于大语言模型的临床预测方法，以便预测患者是否会在下一次访问或接下来的诊断中被诊断出某种疾病。</li>
<li>methods: 这个方法是基于已经预训练的大语言模型（LLM），通过quantization和提示来进行微调，以便预测患者的疾病风险。</li>
<li>results: 对于不同的基线模型，包括Logistic Regression、RETAIN和Med-BERT，我们的CPLLM模型在PR-AUC和ROC-AUC metric上都显示出了明显的提升，较baseline模型更高。<details>
<summary>Abstract</summary>
We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models.
</details>
<details>
<summary>摘要</summary>
我团队现在提出了临床预测使用大型语言模型（CPLLM），这种方法是通过先前训练的大型语言模型（LLM）进行精度调整，以预测患者将在下一次访问或接下来的诊断中被诊断出的疾病。我们使用量化和精度调整LLM，使其能够利用患者历史诊断记录来预测疾病。我们与various baselines进行比较，包括Logistic Regression、RETAIN和Med-BERT，这些模型都是使用结构化医疗记录数据进行疾病预测的现状之arte。我们的实验结果表明，CPLLM在PR-AUC和ROC-AUC指标上都超过了所有测试模型，显示了与基线模型相比而言的remarkable enhancements。
</details></li>
</ul>
<hr>
<h2 id="Overview-of-AuTexTification-at-IberLEF-2023-Detection-and-Attribution-of-Machine-Generated-Text-in-Multiple-Domains"><a href="#Overview-of-AuTexTification-at-IberLEF-2023-Detection-and-Attribution-of-Machine-Generated-Text-in-Multiple-Domains" class="headerlink" title="Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains"></a>Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11285">http://arxiv.org/abs/2309.11285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/autextification/AuTexTification-Overview">https://github.com/autextification/AuTexTification-Overview</a></li>
<li>paper_authors: Areg Mikael Sarvazyan, José Ángel González, Marc Franco-Salvador, Francisco Rangel, Berta Chulvi, Paolo Rosso</li>
<li>for: 这篇论文描述了2023年的IberLEF工作坊中的AuTexTification分类任务，这是一个iberian语言评估论坛（SEPLN）2023年会议的一部分。</li>
<li>methods: 这篇论文描述了AuTexTification任务的两个子任务：第一个子任务是判断文本是人工生成的还是大语言模型生成的；第二个子任务是归属一个机器生成文本到六种不同的文本生成模型中。</li>
<li>results: 这篇论文描述了AuTexTification2023数据集，包含了英语和西班牙语的160,000多个文本，来自五个领域（微博、评论、新闻、法律和使用教程）。总共有114个团队参加了比赛，其中36个团队发送了175个运行，20个团队发送了工作笔记。在这篇报告中，我们介绍了AuTexTification数据集和任务，参与系统，以及结果。<details>
<summary>Abstract</summary>
This paper presents the overview of the AuTexTification shared task as part of the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the framework of the SEPLN 2023 conference. AuTexTification consists of two subtasks: for Subtask 1, participants had to determine whether a text is human-authored or has been generated by a large language model. For Subtask 2, participants had to attribute a machine-generated text to one of six different text generation models. Our AuTexTification 2023 dataset contains more than 160.000 texts across two languages (English and Spanish) and five domains (tweets, reviews, news, legal, and how-to articles). A total of 114 teams signed up to participate, of which 36 sent 175 runs, and 20 of them sent their working notes. In this overview, we present the AuTexTification dataset and task, the submitted participating systems, and the results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Rethinking-Sensors-Modeling-Hierarchical-Information-Enhanced-Traffic-Forecasting"><a href="#Rethinking-Sensors-Modeling-Hierarchical-Information-Enhanced-Traffic-Forecasting" class="headerlink" title="Rethinking Sensors Modeling: Hierarchical Information Enhanced Traffic Forecasting"></a>Rethinking Sensors Modeling: Hierarchical Information Enhanced Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11284">http://arxiv.org/abs/2309.11284</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/VAN-QIAN/CIKM23-HIEST">https://github.com/VAN-QIAN/CIKM23-HIEST</a></li>
<li>paper_authors: Qian Ma, Zijian Zhang, Xiangyu Zhao, Haoliang Li, Hongwei Zhao, Yiqi Wang, Zitao Liu, Wanyu Wang</li>
<li>for: 这篇论文主要关注于城市化加速时的交通预测，并在空间时间预测中提出了一个新的方法。</li>
<li>methods: 本文提出了一个 Hierarchical Information Enhanced Spatio-Temporal prediction 方法（HIEST），它将感应器之间的依赖性分为两层：地域层和全球层。</li>
<li>results: 实验结果显示，HIEST 方法在比较于现有基eline之上获得了leading performance。<details>
<summary>Abstract</summary>
With the acceleration of urbanization, traffic forecasting has become an essential role in smart city construction. In the context of spatio-temporal prediction, the key lies in how to model the dependencies of sensors. However, existing works basically only consider the micro relationships between sensors, where the sensors are treated equally, and their macroscopic dependencies are neglected. In this paper, we argue to rethink the sensor's dependency modeling from two hierarchies: regional and global perspectives. Particularly, we merge original sensors with high intra-region correlation as a region node to preserve the inter-region dependency. Then, we generate representative and common spatio-temporal patterns as global nodes to reflect a global dependency between sensors and provide auxiliary information for spatio-temporal dependency learning. In pursuit of the generality and reality of node representations, we incorporate a Meta GCN to calibrate the regional and global nodes in the physical data space. Furthermore, we devise the cross-hierarchy graph convolution to propagate information from different hierarchies. In a nutshell, we propose a Hierarchical Information Enhanced Spatio-Temporal prediction method, HIEST, to create and utilize the regional dependency and common spatio-temporal patterns. Extensive experiments have verified the leading performance of our HIEST against state-of-the-art baselines. We publicize the code to ease reproducibility.
</details>
<details>
<summary>摘要</summary>
随着城市化的加速，城市智能化建设中的交通预测已成为一项重要的任务。在空间时间预测的上下文中，关键在于如何模型感知器之间的依赖关系。然而，现有的工作基本上只考虑了感知器之间的微型关系，忽略了感知器的宏观依赖关系。在这篇论文中，我们认为应重新考虑感知器之间的依赖模型化，从两个层次来看：地域和全球视角。具体来说，我们将原始感知器高度相关的内部节点合并为一个地域节点，以保留宏观依赖关系。然后，我们生成了代表性的全球节点，用于反映全球感知器之间的依赖关系，并提供辅助的空间时间依赖学习信息。为了保证节点表示的通用性和实际性，我们将MetaGCN integrate into physical data space。此外，我们提出了跨层次图 convolution来传递不同层次的信息。简而言之，我们提出了一种增强空间时间预测方法，即 Hierarchical Information Enhanced Spatio-Temporal prediction（HIEST），以创造和利用地域依赖关系和共同空间时间模式。我们的实验证明了HIEST在比较顶尖基准下的领先性。我们公布了代码，以便重现。
</details></li>
</ul>
<hr>
<h2 id="Open-endedness-induced-through-a-predator-prey-scenario-using-modular-robots"><a href="#Open-endedness-induced-through-a-predator-prey-scenario-using-modular-robots" class="headerlink" title="Open-endedness induced through a predator-prey scenario using modular robots"></a>Open-endedness induced through a predator-prey scenario using modular robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11275">http://arxiv.org/abs/2309.11275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitri Kachler, Karine Miras</li>
<li>for: 这个研究探讨了如何通过探险-猎食情况引发开放演化（OEE）。</li>
<li>methods: 研究使用固定 morphology 的模块机器人，其控制器被进行进化。机器人可以发送和接收信号，并在环境中识别其他机器人的相对位置。研究还引入了一个标记系统，它改变了个体如何识别彼此的方式，并预计会增加行为复杂性。</li>
<li>results: 研究发现了适应策略的出现，证明了通过探险-猎食 dinamics 使用模块机器人来引发 OEE 的可能性。然而，这种emergence似乎需要根据行为标准来条件繁殖。<details>
<summary>Abstract</summary>
This work investigates how a predator-prey scenario can induce the emergence of Open-Ended Evolution (OEE). We utilize modular robots of fixed morphologies whose controllers are subject to evolution. In both species, robots can send and receive signals and perceive the relative positions of other robots in the environment. Specifically, we introduce a feature we call a tagging system: it modifies how individuals can perceive each other and is expected to increase behavioral complexity. Our results show the emergence of adaptive strategies, demonstrating the viability of inducing OEE through predator-prey dynamics using modular robots. Such emergence, nevertheless, seemed to depend on conditioning reproduction to an explicit behavioral criterion.
</details>
<details>
<summary>摘要</summary>
这项研究探讨了掠食-猎物情况如何引起开放演化（OEE）的出现。我们利用固定形态的模块机器人的控制器进行进化。在两种机器人中，机器人可以发送和接收信号，并且可以感知环境中其他机器人的相对位置。我们引入了一个特征，即标记系统：它改变了个体如何感知彼此，并且预期会增加行为复杂性。我们的结果显示了适应策略的出现，证明了通过掠食-猎物 dinamics 使用模块机器人来引起 OEE 的可能性。然而，这种出现似乎виси于对行为标准的条件修复 reproduce。Note: Please keep in mind that the translation is not perfect and may not capture all the nuances of the original text.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Data-Suitability-and-Performance-Testing-Using-Fault-Injection-Testing-Framework"><a href="#Machine-Learning-Data-Suitability-and-Performance-Testing-Using-Fault-Injection-Testing-Framework" class="headerlink" title="Machine Learning Data Suitability and Performance Testing Using Fault Injection Testing Framework"></a>Machine Learning Data Suitability and Performance Testing Using Fault Injection Testing Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11274">http://arxiv.org/abs/2309.11274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manal Rahal, Bestoun S. Ahmed, Jorgen Samuelsson<br>for:This paper aims to address the gap in testing approaches for input data in machine learning (ML) systems, specifically the resilience of ML models to intentionally-triggered data faults.methods:The proposed framework, called FIUL-Data, uses data mutators to explore vulnerabilities of ML systems against data fault injections. The framework is designed with three main ideas: mutators are not random, one mutator is applied at a time, and selected ML models are optimized beforehand.results:The FIUL-Data framework is evaluated using data from analytical chemistry, and the results show that the framework allows for the evaluation of the resilience of ML models. In most experiments, ML models show higher resilience at larger training datasets, and gradient boost performed better than support vector regression in smaller training sets. The mean squared error metric is found to be useful in evaluating the resilience of models due to its higher sensitivity to data mutation.Here is the text in Simplified Chinese:for:这篇论文目标是解决机器学习（ML）系统中输入数据测试方法的差距，具体是测试ML模型对数据fault的抗性。methods:该提议的框架是FIUL-Data，使用数据变换器来探索ML系统对数据fault的敏感性。框架设计了三个主要想法：变换器不是随机的，一个变换器在一次实例时应用，并且选择的ML模型在先前优化。results:FIUL-Data框架在分析化学中使用数据进行评估，结果显示该框架可以评估ML模型的抗性。大多数实验结果表明，ML模型在更大的训练集上显示更高的抗性，并且在较小的训练集中，梯度拟合perform луч于支持向量回归。总的来说， Mean Squared Error 度量有用于评估模型的抗性，因为它对数据变换更敏感。<details>
<summary>Abstract</summary>
Creating resilient machine learning (ML) systems has become necessary to ensure production-ready ML systems that acquire user confidence seamlessly. The quality of the input data and the model highly influence the successful end-to-end testing in data-sensitive systems. However, the testing approaches of input data are not as systematic and are few compared to model testing. To address this gap, this paper presents the Fault Injection for Undesirable Learning in input Data (FIUL-Data) testing framework that tests the resilience of ML models to multiple intentionally-triggered data faults. Data mutators explore vulnerabilities of ML systems against the effects of different fault injections. The proposed framework is designed based on three main ideas: The mutators are not random; one data mutator is applied at an instance of time, and the selected ML models are optimized beforehand. This paper evaluates the FIUL-Data framework using data from analytical chemistry, comprising retention time measurements of anti-sense oligonucleotide. Empirical evaluation is carried out in a two-step process in which the responses of selected ML models to data mutation are analyzed individually and then compared with each other. The results show that the FIUL-Data framework allows the evaluation of the resilience of ML models. In most experiments cases, ML models show higher resilience at larger training datasets, where gradient boost performed better than support vector regression in smaller training sets. Overall, the mean squared error metric is useful in evaluating the resilience of models due to its higher sensitivity to data mutation.
</details>
<details>
<summary>摘要</summary>
创建可恢复的机器学习（ML）系统已经成为确保生产准备的ML系统获得用户信任的必要手段。输入数据质量和模型对生成端到端测试的成功产生很大影响。然而，输入数据测试的方法并不够系统化，与模型测试相比相对落后。为解决这个差距，本文提出了输入数据中的异常投入测试框架（FIUL-Data），用于测试ML模型对多种意外触发的数据异常的抗性。数据变换器探索了ML系统对各种异常投入的敏感性。该框架基于以下三个主要想法：变换器不是随机的，只有一个变换器在一个时间点上应用，并且选择的ML模型在先前优化。本文通过使用分析化学数据，包括抑制肽的释放时间测量，对FIUL-Data框架进行了实证评估。实验在两步进行，先分别分析选择的ML模型对数据变换的响应，然后对各模型进行比较。结果表明，FIUL-Data框架可以评估ML模型的抗性。大多数实验情况下，ML模型在更大的训练集上显示更高的抗性，其中梯度拟合在小训练集中表现更好。总的来说，平均方差误差度量是评估ML模型抗性的有用指标。
</details></li>
</ul>
<hr>
<h2 id="Grounded-Complex-Task-Segmentation-for-Conversational-Assistants"><a href="#Grounded-Complex-Task-Segmentation-for-Conversational-Assistants" class="headerlink" title="Grounded Complex Task Segmentation for Conversational Assistants"></a>Grounded Complex Task Segmentation for Conversational Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11271">http://arxiv.org/abs/2309.11271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rafaelhferreira/grounded_task_segmentation_cta">https://github.com/rafaelhferreira/grounded_task_segmentation_cta</a></li>
<li>paper_authors: Rafael Ferreira, David Semedo, João Magalhães</li>
<li>for: 这 paper 是为了改进 web-based  instrucional text，使其更适合 conversational  Setting。</li>
<li>methods: 该 paper 使用 Transformer-based 架构进行计算模型，以及按照 conversational enario 进行 instrucional 结构标注。</li>
<li>results: 经过测试，用户对 step 的 complexity 和 length 有所偏好，并且提出的方法可以改善原始的 web-based instrucional text，提高了 86% 的评价。<details>
<summary>Abstract</summary>
Following complex instructions in conversational assistants can be quite daunting due to the shorter attention and memory spans when compared to reading the same instructions. Hence, when conversational assistants walk users through the steps of complex tasks, there is a need to structure the task into manageable pieces of information of the right length and complexity. In this paper, we tackle the recipes domain and convert reading structured instructions into conversational structured ones. We annotated the structure of instructions according to a conversational scenario, which provided insights into what is expected in this setting. To computationally model the conversational step's characteristics, we tested various Transformer-based architectures, showing that a token-based approach delivers the best results. A further user study showed that users tend to favor steps of manageable complexity and length, and that the proposed methodology can improve the original web-based instructional text. Specifically, 86% of the evaluated tasks were improved from a conversational suitability point of view.
</details>
<details>
<summary>摘要</summary>
请求中的复杂指令可能会让用户感到困惑，这是因为与阅读相同的指令相比，用户的注意力和记忆 span 更短。因此，当 conversational assistant 通过多个步骤引导用户完成复杂任务时，需要将任务分解成可管理的小块信息，以便用户更好地理解和完成。在这篇论文中，我们将 recipes 领域中的指令结构化为 conversational 结构，并通过对话情境进行标注，从而获得了更深刻的理解。为了计算 conversational 步骤的特点，我们测试了不同的 Transformer 基 architecture，发现 token 基本法取得了最好的结果。进一步的用户研究表明，用户偏好管理 complexity 和 length 的步骤，而我们的方法ologies 可以改善原始的网络上的指令文本。特别是，86% 的评估任务得到了 conversational 适用性的改进。
</details></li>
</ul>
<hr>
<h2 id="Sequence-to-Sequence-Spanish-Pre-trained-Language-Models"><a href="#Sequence-to-Sequence-Spanish-Pre-trained-Language-Models" class="headerlink" title="Sequence-to-Sequence Spanish Pre-trained Language Models"></a>Sequence-to-Sequence Spanish Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11259">http://arxiv.org/abs/2309.11259</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vgaraujov/Seq2Seq-Spanish-PLMs">https://github.com/vgaraujov/Seq2Seq-Spanish-PLMs</a></li>
<li>paper_authors: Vladimir Araujo, Maria Mihaela Trusca, Rodrigo Tufiño, Marie-Francine Moens</li>
<li>for: 这篇论文旨在开发针对西班牙语序列训练的encoder-decoder模型，用于进行文本摘要、重叙和生成问答等序列转换任务。</li>
<li>methods: 该论文采用了BERT、RoBERTa和GPT等批处理语言模型的encoder-decoder架构，并对其进行了适应性的预训练，以便在西班牙语文本中进行更好的表现。</li>
<li>results: 论文通过对各模型进行了广泛的评估，发现BERT和T5模型在所有评估任务中表现最佳，而BART模型也在某些任务中表现出色。此外，该论文还将所有模型公开发布到研究社区，以促进未来的西班牙语处理研究。<details>
<summary>Abstract</summary>
In recent years, substantial advancements in pre-trained language models have paved the way for the development of numerous non-English language versions, with a particular focus on encoder-only and decoder-only architectures. While Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited prowess in natural language understanding and generation, there remains a scarcity of encoder-decoder models designed for sequence-to-sequence tasks involving input-output pairs. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder-decoder architectures, exclusively pre-trained on Spanish corpora. Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across a diverse range of sequence-to-sequence tasks, spanning summarization, rephrasing, and generative question answering. Our findings underscore the competitive performance of all models, with BART and T5 emerging as top performers across all evaluated tasks. As an additional contribution, we have made all models publicly available to the research community, fostering future exploration and development in Spanish language processing.
</details>
<details>
<summary>摘要</summary>
近年来，大规模的预训练语言模型技术得到了广泛应用，特别是针对英语以外语言的研发。虽然西班牙语模型，包括BERT、RoBERTa和GPT，在自然语言理解和生成方面具有卓越表现，但是还缺乏适用于序列-序列任务的encoder-decoder模型。这篇论文创新地介绍了西班牙语encoder-decoder模型的实现和评估，具体来说是在西班牙语 corpus 上预训练的 BART、T5 和 BERT2BERT 样式模型。我们对这些模型进行了广泛的评估，包括概要、重新写和生成问答等序列-序列任务，我们的发现表明所有模型都具有竞争力，BART 和 T5 在所有评估任务中表现出色。此外，我们将所有模型公开发布给研究社区，以促进未来的探索和发展在西班牙语处理领域。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Multi-Agent-Reinforcement-Learning-for-Air-Combat-Maneuvering"><a href="#Hierarchical-Multi-Agent-Reinforcement-Learning-for-Air-Combat-Maneuvering" class="headerlink" title="Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering"></a>Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11247">http://arxiv.org/abs/2309.11247</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IDSIA/marl">https://github.com/IDSIA/marl</a></li>
<li>paper_authors: Ardian Selmonaj, Oleg Szehr, Giacomo Del Rio, Alessandro Antonucci, Adrian Schneider, Michael Rüegsegger</li>
<li>for: 这个研究旨在提供一个多代理人问题决策框架，以实现精确的空中作战决策。</li>
<li>methods: 本研究使用多代理人问题决策框架，分为两个阶层：低层为单位对战斗控制的细节政策，高层为高级指挥官策略，对于任务目标进行决策。低层策略透过增加复杂训练enario和联赛自游戏的方式进行训练，而高层策略则透过已经预训练的低层策略进行训练。</li>
<li>results: 这个框架的实验验证表明，这种多代理人问题决策框架具有优化空中作战决策的功能。<details>
<summary>Abstract</summary>
The application of artificial intelligence to simulate air-to-air combat scenarios is attracting increasing attention. To date the high-dimensional state and action spaces, the high complexity of situation information (such as imperfect and filtered information, stochasticity, incomplete knowledge about mission targets) and the nonlinear flight dynamics pose significant challenges for accurate air combat decision-making. These challenges are exacerbated when multiple heterogeneous agents are involved. We propose a hierarchical multi-agent reinforcement learning framework for air-to-air combat with multiple heterogeneous agents. In our framework, the decision-making process is divided into two stages of abstraction, where heterogeneous low-level policies control the action of individual units, and a high-level commander policy issues macro commands given the overall mission targets. Low-level policies are trained for accurate unit combat control. Their training is organized in a learning curriculum with increasingly complex training scenarios and league-based self-play. The commander policy is trained on mission targets given pre-trained low-level policies. The empirical validation advocates the advantages of our design choices.
</details>
<details>
<summary>摘要</summary>
application of artificial intelligence to simulate air-to-air combat scenarios is attracting increasing attention. To date, the high-dimensional state and action spaces, the high complexity of situation information (such as imperfect and filtered information, stochasticity, incomplete knowledge about mission targets) and the nonlinear flight dynamics pose significant challenges for accurate air combat decision-making. These challenges are exacerbated when multiple heterogeneous agents are involved. We propose a hierarchical multi-agent reinforcement learning framework for air-to-air combat with multiple heterogeneous agents. In our framework, the decision-making process is divided into two stages of abstraction, where heterogeneous low-level policies control the action of individual units, and a high-level commander policy issues macro commands given the overall mission targets. Low-level policies are trained for accurate unit combat control. Their training is organized in a learning curriculum with increasingly complex training scenarios and league-based self-play. The commander policy is trained on mission targets given pre-trained low-level policies. The empirical validation advocates the advantages of our design choices.Here's the translation in Traditional Chinese:运用人工智能模拟空中武器战场情况的应用正在吸引越来越多的注意。到目前为止，高维度的状态和动作空间，高复杂的情况信息（如受损和范围信息、数据满意度、任务目标知识不完整）以及非线性的飞行动力学都对于精准的空中战斗决策带来巨大挑战。当多个不同性的代理人参与时，这些挑战更加严重。我们提出了一个层次多代理人学习框架，用于空中战斗多个不同性代理人。在我们的框架中，决策过程分为两个层次的抽象，其中专门的低层策略控制个别单位的行动，而高层策略根据全局任务目标发出大规模的指令。低层策略在增加复杂的训练enario和联赛自游中进行训练。高层策略则是根据已经预训的低层策略进行训练。实际验证表明了我们的设计选择的优点。
</details></li>
</ul>
<hr>
<h2 id="Colour-Passing-Revisited-Lifted-Model-Construction-with-Commutative-Factors"><a href="#Colour-Passing-Revisited-Lifted-Model-Construction-with-Commutative-Factors" class="headerlink" title="Colour Passing Revisited: Lifted Model Construction with Commutative Factors"></a>Colour Passing Revisited: Lifted Model Construction with Commutative Factors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11236">http://arxiv.org/abs/2309.11236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malte Luttermann, Tanya Braun, Ralf Möller, Marcel Gehrke</li>
<li>for: 这篇论文目的是提出一种基于Symmetries的升级概率推理方法，以实现可靠地概率推理。</li>
<li>methods: 该方法使用了colour passing算法，但是现有的colour passing算法受限于特定的推理算法，并且忽略了因素的 коммутатив性。本文提出了一种基于逻辑变量的修改版colour passing算法，可以独立于特定的推理算法来构建升级表示，同时充分利用因素的 коммутатив性。</li>
<li>results: 对比于现有的colour passing算法，本文的方法可以更好地检测Symmetries，从而实现更高的压缩率和更快的在线查询速度。<details>
<summary>Abstract</summary>
Lifted probabilistic inference exploits symmetries in a probabilistic model to allow for tractable probabilistic inference with respect to domain sizes. To apply lifted inference, a lifted representation has to be obtained, and to do so, the so-called colour passing algorithm is the state of the art. The colour passing algorithm, however, is bound to a specific inference algorithm and we found that it ignores commutativity of factors while constructing a lifted representation. We contribute a modified version of the colour passing algorithm that uses logical variables to construct a lifted representation independent of a specific inference algorithm while at the same time exploiting commutativity of factors during an offline-step. Our proposed algorithm efficiently detects more symmetries than the state of the art and thereby drastically increases compression, yielding significantly faster online query times for probabilistic inference when the resulting model is applied.
</details>
<details>
<summary>摘要</summary>
增强概率推理利用模型中的对称性来实现可行的概率推理，具体来说是通过增强的可行推理算法来实现。为了应用增强推理，需要首先获得增强表示，而现有的颜色传递算法是state of the art的解决方案。然而，这个算法受到特定推理算法的限制，而且忽略了因素的 коммутатив性。我们提出了一种改进的颜色传递算法，使用逻辑变量来构建独立于特定推理算法的增强表示，同时在Offline阶段利用因素的 commutativity 来提高压缩率。我们的提议算法可以更好地检测模型中的对称性，从而导致更快的在线查询时间。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-4-as-a-Tool-for-Reviewing-Academic-Books-in-Spanish"><a href="#ChatGPT-4-as-a-Tool-for-Reviewing-Academic-Books-in-Spanish" class="headerlink" title="ChatGPT-4 as a Tool for Reviewing Academic Books in Spanish"></a>ChatGPT-4 as a Tool for Reviewing Academic Books in Spanish</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11231">http://arxiv.org/abs/2309.11231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonnathan Berrezueta-Guzman, Laura Malache-Silva, Stephan Krusche</li>
<li>For: This study evaluates the potential of ChatGPT-4 as an editing tool for Spanish literary and academic books.* Methods: The study analyzes the features and capabilities of ChatGPT-4 in terms of grammatical correction, stylistic coherence, and linguistic enrichment of texts in Spanish.* Results: ChatGPT-4 is capable of making grammatical and orthographic corrections with high accuracy and in a very short time, but faces challenges in areas such as context sensitivity and interaction with visual content. Collaboration between ChatGPT-4 and human reviewers and editors is a promising strategy for improving efficiency without compromising quality.Here are the three points in Simplified Chinese text:* For: 这项研究评估了OpenAI开发的ChatGPT-4语言模型是否能够用于西班牙文学和学术书籍的编辑。* Methods: 研究分析了ChatGPT-4模型在西班牙文 grammar修正、风格一致性和语言丰富性方面的功能和能力。* Results: ChatGPT-4能够快速和准确地进行语法和拼写修正，但在上下文敏感性和图表和表格交互方面存在挑战。人类编辑和评审者和ChatGPT-4 collaboration 可能是提高效率而无需降低质量的有效策略。<details>
<summary>Abstract</summary>
This study evaluates the potential of ChatGPT-4, an artificial intelligence language model developed by OpenAI, as an editing tool for Spanish literary and academic books. The need for efficient and accessible reviewing and editing processes in the publishing industry has driven the search for automated solutions. ChatGPT-4, being one of the most advanced language models, offers notable capabilities in text comprehension and generation. In this study, the features and capabilities of ChatGPT-4 are analyzed in terms of grammatical correction, stylistic coherence, and linguistic enrichment of texts in Spanish. Tests were conducted with 100 literary and academic texts, where the edits made by ChatGPT-4 were compared to those made by expert human reviewers and editors. The results show that while ChatGPT-4 is capable of making grammatical and orthographic corrections with high accuracy and in a very short time, it still faces challenges in areas such as context sensitivity, bibliometric analysis, deep contextual understanding, and interaction with visual content like graphs and tables. However, it is observed that collaboration between ChatGPT-4 and human reviewers and editors can be a promising strategy for improving efficiency without compromising quality. Furthermore, the authors consider that ChatGPT-4 represents a valuable tool in the editing process, but its use should be complementary to the work of human editors to ensure high-caliber editing in Spanish literary and academic books.
</details>
<details>
<summary>摘要</summary>
Tests were conducted on 100 literary and academic texts, comparing the edits made by ChatGPT-4 to those made by expert human reviewers and editors. The results show that ChatGPT-4 is capable of making grammatical and orthographic corrections with high accuracy and in a very short time. However, it still struggles with context sensitivity, bibliometric analysis, deep contextual understanding, and interaction with visual content like graphs and tables.Despite these limitations, collaboration between ChatGPT-4 and human reviewers and editors is a promising strategy for improving efficiency without compromising quality. The authors conclude that ChatGPT-4 represents a valuable tool in the editing process, but its use should be complementary to the work of human editors to ensure high-caliber editing in Spanish literary and academic books.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Diversity-in-Online-Interactions"><a href="#Leveraging-Diversity-in-Online-Interactions" class="headerlink" title="Leveraging Diversity in Online Interactions"></a>Leveraging Diversity in Online Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11224">http://arxiv.org/abs/2309.11224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nardine Osman, Bruno Rosell i Gui, Carles Sierra</li>
<li>for: 本研究旨在通过在线连接人们，帮助他们解决日常问题。</li>
<li>methods: 本研究使用了声明性规范来mediate在线交互，特别是在连接人们时利用多样性。</li>
<li>results: 在不同的大学站点上进行的试验显示，选择的profile多样性得到了相对成功，并得到了用户满意的评价。<details>
<summary>Abstract</summary>
This paper addresses the issue of connecting people online to help them find support with their day-to-day problems. We make use of declarative norms for mediating online interactions, and we specifically focus on the issue of leveraging diversity when connecting people. We run pilots at different university sites, and the results show relative success in the diversity of the selected profiles, backed by high user satisfaction.
</details>
<details>
<summary>摘要</summary>
这篇论文关注在线连接人们，以帮助他们解决日常问题。我们利用声明性规范来调控在线交互，特别是利用多样性连接人们。我们在不同的大学站点进行了试点，结果表明在选择的profile中的多样性得到了相对成功，并得到了用户满意的评价。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Retrieve-Rewrite-Answer-A-KG-to-Text-Enhanced-LLMs-Framework-for-Knowledge-Graph-Question-Answering"><a href="#Retrieve-Rewrite-Answer-A-KG-to-Text-Enhanced-LLMs-Framework-for-Knowledge-Graph-Question-Answering" class="headerlink" title="Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering"></a>Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11206">http://arxiv.org/abs/2309.11206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yike Wu, Nan Hu, Sheng Bi, Guilin Qi, Jie Ren, Anhuan Xie, Wei Song</li>
<li>for: 提高知识GraphQuestionAnswering（KGQA）任务的表现，解决rich world knowledge的问题。</li>
<li>methods: 提出了一种Answer-sensitive KG-to-Text方法，将知识Graph（KG）知识转化成文本表示，以便与语言模型（LLMs）集成。</li>
<li>results: 实验表明，提出的KG-to-Text增强的LLMs框架在KGQA任务上的答案准确率和知识声明的有用性都高于之前的KG-加强LLMs方法。<details>
<summary>Abstract</summary>
Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperforms previous KG-augmented LLMs approaches regarding answer accuracy and usefulness of knowledge statements.
</details>
<details>
<summary>摘要</summary>
尽管大语言模型（LLMs）在知识密集任务上表现竞争性强，但它们仍有吸收全球知识的限制，特别是长尾知识。在这篇论文中，我们研究了将知识图（KG）扩展到语言模型（LMs）的方法，以解决需要丰富世界知识的问题 answering（KGQA）任务。现有的研究表明，使用KG知识来提高LLMs的提问可以显著提高LLMs在KGQA任务上的表现。然而，现有的方法忽略了KG表示和文本表示之间的差异，即KG知识的形式化表述。为了解决这问题，我们提出了一种答案相关的KG知识转换方法，可以将KG知识转换成最有用的文本表述，以便于KGQA任务。基于这种方法，我们提出了一种增强LLMs的KG-to-Text框架，用于解决KGQA任务。实验表明，我们的方法在多个KGQA bencmark上显著提高了答案准确率和知识声明的用用性。
</details></li>
</ul>
<hr>
<h2 id="Using-Artificial-Intelligence-for-the-Automation-of-Knitting-Patterns"><a href="#Using-Artificial-Intelligence-for-the-Automation-of-Knitting-Patterns" class="headerlink" title="Using Artificial Intelligence for the Automation of Knitting Patterns"></a>Using Artificial Intelligence for the Automation of Knitting Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11202">http://arxiv.org/abs/2309.11202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uduak Uboh</li>
<li>for: 这个研究是为了判断使用自动化系统来分类锦纹。</li>
<li>methods: 该研究使用了数据扩展和传输学习技术，采用了inception ResNet-V2作为特征提取和分类算法。</li>
<li>results: 模型的评估结果显示了高的模型精度、精度、回归率和F1分数，而且大多数类的AUC分数在(0.7-0.9)的范围内。<details>
<summary>Abstract</summary>
Knitting patterns are a crucial component in the creation and design of knitted materials. Traditionally, these patterns were taught informally, but thanks to advancements in technology, anyone interested in knitting can use the patterns as a guide to start knitting. Perhaps because knitting is mostly a hobby, with the exception of industrial manufacturing utilising specialised knitting machines, the use of Al in knitting is less widespread than its application in other fields. However, it is important to determine whether knitted pattern classification using an automated system is viable. In order to recognise and classify knitting patterns. Using data augmentation and a transfer learning technique, this study proposes a deep learning model. The Inception ResNet-V2 is the main feature extraction and classification algorithm used in the model. Metrics like accuracy, logarithmic loss, F1-score, precision, and recall score were used to evaluate the model. The model evaluation's findings demonstrate high model accuracy, precision, recall, and F1 score. In addition, the AUC score for majority of the classes was in the range (0.7-0.9). A comparative analysis was done using other pretrained models and a ResNet-50 model with transfer learning and the proposed model evaluation results surpassed all others. The major limitation for this project is time, as with more time, there might have been better accuracy over a larger number of epochs.
</details>
<details>
<summary>摘要</summary>
针脊图案是创作和设计针脊材料的关键组件。在过去，这些图案通常是通过口述传授的，但现在随着技术的进步，任何感兴趣的人都可以使用这些图案作为指南开始针脊。由于针脊主要是一项兴趣爱好，除了特殊针脊机器在工业生产中使用外，使用人工智能（AI）在针脊中的应用范围相对较少。然而，是否可以使用自动化系统来分类针脊图案是一个重要的问题。为了识别和分类针脊图案，这个研究提出了一个深度学习模型。使用Inception ResNet-V2算法作为主要特征提取和分类算法。对模型的评估结果，发现模型的准确率、精度、准确率、和F1分数均达到了高水平。此外，大多数类别的AUC分数都在(0.7-0.9)之间。与其他预训练模型和ResNet-50模型进行比较分析后，这个研究的评估结果超过了其他所有。该项目的主要限制是时间，如果有更多的时间，可能会在更多的轮次上得到更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="When-to-Trust-AI-Advances-and-Challenges-for-Certification-of-Neural-Networks"><a href="#When-to-Trust-AI-Advances-and-Challenges-for-Certification-of-Neural-Networks" class="headerlink" title="When to Trust AI: Advances and Challenges for Certification of Neural Networks"></a>When to Trust AI: Advances and Challenges for Certification of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11196">http://arxiv.org/abs/2309.11196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Kwiatkowska, Xiyue Zhang</li>
<li>for: 本文旨在探讨如何确保人工智能（AI）决策的安全性，以便在实际应用中使用AI技术。</li>
<li>methods: 本文使用了证明和解释性技术来确保AI决策的安全性。</li>
<li>results: 本文提出了未来的挑战和研究方向，以确保AI决策的安全性和可靠性。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) has been advancing at a fast pace and it is now poised for deployment in a wide range of applications, such as autonomous systems, medical diagnosis and natural language processing. Early adoption of AI technology for real-world applications has not been without problems, particularly for neural networks, which may be unstable and susceptible to adversarial examples. In the longer term, appropriate safety assurance techniques need to be developed to reduce potential harm due to avoidable system failures and ensure trustworthiness. Focusing on certification and explainability, this paper provides an overview of techniques that have been developed to ensure safety of AI decisions and discusses future challenges.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在过去几年中得到了快速发展，现在它已经准备好在各种应用中使用，如自主系统、医疗诊断和自然语言处理。虽然在实际应用中早期采用AI技术有一些问题，特别是神经网络可能存在不稳定性和可靠性问题，以及可能受到敌意的示例的影响。在长期来看，我们需要开发适当的安全保障技术，以降低可预防的系统失效的可能性，并确保AI决策的可靠性。本文将关注证书和解释性，提供了安全AI决策的技术ensure的概述，并讨论未来的挑战。
</details></li>
</ul>
<hr>
<h2 id="Long-tail-Augmented-Graph-Contrastive-Learning-for-Recommendation"><a href="#Long-tail-Augmented-Graph-Contrastive-Learning-for-Recommendation" class="headerlink" title="Long-tail Augmented Graph Contrastive Learning for Recommendation"></a>Long-tail Augmented Graph Contrastive Learning for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11177">http://arxiv.org/abs/2309.11177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/im0qianqian/LAGCL">https://github.com/im0qianqian/LAGCL</a></li>
<li>paper_authors: Qian Zhao, Zhengwei Wu, Zhiqiang Zhang, Jun Zhou</li>
<li>for: 提高推荐系统中Graph Convolutional Networks (GCNs)的性能， Address the data sparsity issue in real-world scenarios.</li>
<li>methods: 使用contrastive learning方法，并 introduce learnable long-tail augmentation approach to enhance tail nodes， generate contrastive views based on the resulting augmented graph.</li>
<li>results: 对三个 benchmark dataset进行了extensive experiments，demonstrate the significant improvement in performance of our model over the state-of-the-arts，further analyses demonstrate the uniformity of learned representations and the superiority of LAGCL on long-tail performance.<details>
<summary>Abstract</summary>
Graph Convolutional Networks (GCNs) has demonstrated promising results for recommender systems, as they can effectively leverage high-order relationship. However, these methods usually encounter data sparsity issue in real-world scenarios. To address this issue, GCN-based recommendation methods employ contrastive learning to introduce self-supervised signals. Despite their effectiveness, these methods lack consideration of the significant degree disparity between head and tail nodes. This can lead to non-uniform representation distribution, which is a crucial factor for the performance of contrastive learning methods. To tackle the above issue, we propose a novel Long-tail Augmented Graph Contrastive Learning (LAGCL) method for recommendation. Specifically, we introduce a learnable long-tail augmentation approach to enhance tail nodes by supplementing predicted neighbor information, and generate contrastive views based on the resulting augmented graph. To make the data augmentation schema learnable, we design an auto drop module to generate pseudo-tail nodes from head nodes and a knowledge transfer module to reconstruct the head nodes from pseudo-tail nodes. Additionally, we employ generative adversarial networks to ensure that the distribution of the generated tail/head nodes matches that of the original tail/head nodes. Extensive experiments conducted on three benchmark datasets demonstrate the significant improvement in performance of our model over the state-of-the-arts. Further analyses demonstrate the uniformity of learned representations and the superiority of LAGCL on long-tail performance. Code is publicly available at https://github.com/im0qianqian/LAGCL
</details>
<details>
<summary>摘要</summary>
图像 convolutional networks (GCNs) 在推荐系统中表现出色，可以有效利用高阶关系。然而，这些方法通常在实际场景中遇到数据稀缺问题。为解决这个问题，GCN 基于的推荐方法使用对照学习引入自我超vised信号。尽管它们有效，但是它们忽视了主要度差的问题，这可能导致非均衡的表示分布，这是对对照学习方法的表现非常重要的因素。为解决这个问题，我们提出了一种长尾增强图像对照学习（LAGCL）方法。具体来说，我们引入可学习的长尾增强approach，通过预测邻居信息来增强尾节点，并基于所得到的扩展图像生成对照视图。为使数据增强 schema 学习可能，我们设计了自动Drop模块，将头节点转化为 pseudo-tail 节点，并设计了知识传递模块，将 pseudo-tail 节点还原为头节点。此外，我们使用生成对抗网络，确保生成的尾/头节点的分布与原始的尾/头节点的分布一致。我们在三个标准数据集上进行了广泛的实验，并证明了我们的模型在现状上的显著改进。进一步的分析也表明了我们学习的表示的均匀性和我们对长尾性能的优势。代码可以在https://github.com/im0qianqian/LAGCL 中找到。
</details></li>
</ul>
<hr>
<h2 id="Are-Large-Language-Models-Really-Robust-to-Word-Level-Perturbations"><a href="#Are-Large-Language-Models-Really-Robust-to-Word-Level-Perturbations" class="headerlink" title="Are Large Language Models Really Robust to Word-Level Perturbations?"></a>Are Large Language Models Really Robust to Word-Level Perturbations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11166">http://arxiv.org/abs/2309.11166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Harry-mic/TREval">https://github.com/Harry-mic/TREval</a></li>
<li>paper_authors: Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, Dacheng Tao</li>
<li>for: 本研究旨在提供一种用于评估大语言模型（LLM）的有用性和可靠性的新方法。</li>
<li>methods: 本研究提出了一种基于预训练奖励模型的评估方法，称为TREval，用于评估LLM的可靠性，特别是在面对更加困难的开放问题时。</li>
<li>results: 实验结果表明，TREval可以准确地评估LLM的可靠性，并且发现LLM经常受到单词水平的干扰，这种干扰在日常语言使用中很常见。另外，研究发现，在进行练习和强化训练后，LLM的可靠性往往会下降。<details>
<summary>Abstract</summary>
The swift advancement in the scale and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary LLMs. To address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the robustness of LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Our extensive empirical experiments have demonstrated that TREval provides an accurate method for evaluating the robustness of an LLM, especially when faced with more challenging open questions. Furthermore, our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations, which are commonplace in daily language usage. Notably, we were surprised to discover that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted. The code of TREval is available in https://github.com/Harry-mic/TREval.
</details>
<details>
<summary>摘要</summary>
Large Language Models (LLMs) 的快速发展和能力提高，使其成为许多下游任务的优秀工具。除了提高性能和避免某些提示导致的暴力反馈外，为了确保 LLM 的责任，也引起了一些关注。现有的评估方法主要基于已经定义的传统问答数据集，这些数据集并不符合当代 LLM 的优秀生成能力。为解决这个问题，我们提出了一种新的合理评估方法，利用预训练的奖励模型作为诊断工具来评估 LLM 的 robustness，我们称之为 TREvaL。我们的广泛的实验证明了 TREval 能够准确地评估 LLM 的 robustness，特别是面对更加困难的开放问题。此外，我们的结果表明，LLM  часто会受到单词水平的扰动，这些扰动在日常语言使用中很常见。意外地，我们发现，在 fine-tuning (SFT 和 RLHF) 过程中，LLM 的 Robustness 往往减退。TREval 的代码可以在 GitHub 上找到：https://github.com/Harry-mic/TREval。
</details></li>
</ul>
<hr>
<h2 id="ProtoExplorer-Interpretable-Forensic-Analysis-of-Deepfake-Videos-using-Prototype-Exploration-and-Refinement"><a href="#ProtoExplorer-Interpretable-Forensic-Analysis-of-Deepfake-Videos-using-Prototype-Exploration-and-Refinement" class="headerlink" title="ProtoExplorer: Interpretable Forensic Analysis of Deepfake Videos using Prototype Exploration and Refinement"></a>ProtoExplorer: Interpretable Forensic Analysis of Deepfake Videos using Prototype Exploration and Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11155">http://arxiv.org/abs/2309.11155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Merel de Leeuw den Bouter, Javier Lloret Pardo, Zeno Geradts, Marcel Worring</li>
<li>for: 这个研究旨在提高深度学习模型的可读性，尤其是在高度竞争的应用场景下。</li>
<li>methods: 这篇文章提出了一个可视化分析过程模型，并基于这个模型提出了一个名为ProtoExplorer的可视化分析系统，用于探索和修改基于原型的伪动态检测模型。</li>
<li>results: 这篇文章透过对实际应用场景进行评估，确认了这个方法的可行性和有效性。<details>
<summary>Abstract</summary>
In high-stakes settings, Machine Learning models that can provide predictions that are interpretable for humans are crucial. This is even more true with the advent of complex deep learning based models with a huge number of tunable parameters. Recently, prototype-based methods have emerged as a promising approach to make deep learning interpretable. We particularly focus on the analysis of deepfake videos in a forensics context. Although prototype-based methods have been introduced for the detection of deepfake videos, their use in real-world scenarios still presents major challenges, in that prototypes tend to be overly similar and interpretability varies between prototypes. This paper proposes a Visual Analytics process model for prototype learning, and, based on this, presents ProtoExplorer, a Visual Analytics system for the exploration and refinement of prototype-based deepfake detection models. ProtoExplorer offers tools for visualizing and temporally filtering prototype-based predictions when working with video data. It disentangles the complexity of working with spatio-temporal prototypes, facilitating their visualization. It further enables the refinement of models by interactively deleting and replacing prototypes with the aim to achieve more interpretable and less biased predictions while preserving detection accuracy. The system was designed with forensic experts and evaluated in a number of rounds based on both open-ended think aloud evaluation and interviews. These sessions have confirmed the strength of our prototype based exploration of deepfake videos while they provided the feedback needed to continuously improve the system.
</details>
<details>
<summary>摘要</summary>
高度的场景中，可以提供人类可解释的机器学习模型是非常重要的。这种情况更加真实，特别是在复杂的深度学习模型中，其中有很多可调参数。最近，原型基方法在使得深度学习可解释方面表现出了扎实的抑制力。我们特别关注深度假影像在法医方面的分析。虽然原型基方法已经应用于深度假影像的检测，但在实际应用中仍然存在主要挑战，即原型往往相似，解释性 между原型异常不一致。这篇论文提出了一种可见分析过程模型，并基于这种模型提出了ProtoExplorer，一种可见分析系统，用于深度假影像检测模型的探索和细化。ProtoExplorer提供了视觉分析和视频数据中的时间滤波功能，可以识别和分析深度假影像。它还可以通过交互删除和替换原型来实现更加可解释和不偏执的预测，同时保持检测精度。系统针对法医专家进行了多轮评估，包括开放式思维回答评估和面试。这些评估过程确认了我们的原型基 explore深度假影像的优势，同时提供了需要不断改进系统的反馈。
</details></li>
</ul>
<hr>
<h2 id="CoT-BERT-Enhancing-Unsupervised-Sentence-Representation-through-Chain-of-Thought"><a href="#CoT-BERT-Enhancing-Unsupervised-Sentence-Representation-through-Chain-of-Thought" class="headerlink" title="CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought"></a>CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11143">http://arxiv.org/abs/2309.11143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZBWpro/CoT-BERT">https://github.com/ZBWpro/CoT-BERT</a></li>
<li>paper_authors: Bowen Zhang, Kehua Chang, Chunping Li</li>
<li>for: 提高不supervised sentence representation learning的性能，尝试使用链条思维来解锁预训练模型中的潜在能力。</li>
<li>methods: 提出了一种两阶段方法，首先使用理解阶段对输入句子进行理解，然后使用摘要阶段对输入句子进行摘要，最后使用摘要阶段的输出作为输入句子的vector化表示。同时，对冲突学习损失函数和模板干扰技术进行精细调整，以提高提示工程的性能。</li>
<li>results: 对多个 robust baseline进行了严格的实验证明，发现CoT-BERT可以在不需要其他文本表示模型或外部数据库的情况下，与supervised sentence representation learning具有相同或更高的性能。<details>
<summary>Abstract</summary>
Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a suite of robust baselines without necessitating other text representation models or external databases.
</details>
<details>
<summary>摘要</summary>
不监督句子表示学习目标是将输入句子转化为固定长度的向量，具有细致的 semantics信息，而不需要标注数据。在这个领域，最近的进展，受到对短文本检测和提取技术的影响，已经大幅度减少了不监督和监督方法之间的差距。然而，链式思维的潜在应用，在这个轨迹上仍然尚未得到充分利用。为了解锁预训练模型中的强化特性，我们提出了一种两阶段方法：理解和概要。然后，后一阶段的输出被用作输入句子的向量表示。为了进一步提高性能，我们仔细修改了对短文本检测和提取技术的权重，以及模板干扰技术。我们的方法，CoT-BERT，在一系列强大的基线上进行了严格的实验，并不需要其他文本表示模型或外部数据库。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Pseudo-Learning-for-Open-World-DeepFake-Attribution"><a href="#Contrastive-Pseudo-Learning-for-Open-World-DeepFake-Attribution" class="headerlink" title="Contrastive Pseudo Learning for Open-World DeepFake Attribution"></a>Contrastive Pseudo Learning for Open-World DeepFake Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11132">http://arxiv.org/abs/2309.11132</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TencentYoutuResearch/OpenWorld-DeepFakeAttribution">https://github.com/TencentYoutuResearch/OpenWorld-DeepFakeAttribution</a></li>
<li>paper_authors: Zhimin Sun, Shen Chen, Taiping Yao, Bangjie Yin, Ran Yi, Shouhong Ding, Lizhuang Ma</li>
<li>for: 评估深伪检测领域中匿名攻击的隐藏迹象，以推动相关前沿研究。</li>
<li>methods: 提出一个新的评估指标集合called Open-World DeepFake Attribution（OW-DFA），并提出一种基于对比学习的novel框架 named Contrastive Pseudo Learning（CPL）。</li>
<li>results: 经验表明，我们提出的方法在OW-DFA任务上具有优秀的表现，并且能够增强深伪检测领域的安全性。<details>
<summary>Abstract</summary>
The challenge in sourcing attribution for forgery faces has gained widespread attention due to the rapid development of generative techniques. While many recent works have taken essential steps on GAN-generated faces, more threatening attacks related to identity swapping or expression transferring are still overlooked. And the forgery traces hidden in unknown attacks from the open-world unlabeled faces still remain under-explored. To push the related frontier research, we introduce a new benchmark called Open-World DeepFake Attribution (OW-DFA), which aims to evaluate attribution performance against various types of fake faces under open-world scenarios. Meanwhile, we propose a novel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task through 1) introducing a Global-Local Voting module to guide the feature alignment of forged faces with different manipulated regions, 2) designing a Confidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused by similar methods in unlabeled set. In addition, we extend the CPL framework with a multi-stage paradigm that leverages pre-train technique and iterative learning to further enhance traceability performance. Extensive experiments verify the superiority of our proposed method on the OW-DFA and also demonstrate the interpretability of deepfake attribution task and its impact on improving the security of deepfake detection area.
</details>
<details>
<summary>摘要</summary>
“对于伪造的挑战，随着生成技术的快速发展，已经受到了广泛的关注。然而，许多最近的研究仅对生成器生成的面部进行了重要的步骤，尚未充分处理隐藏在未知攻击中的伪造迹象。为了推进相关的前沿研究，我们提出了一个新的 bencmark 叫做 Open-World DeepFake Attribution（OW-DFA），旨在评估对不同类型的伪造面部进行权重评估。同时，我们提出了一个名为 Contrastive Pseudo Learning（CPL）的新框架，通过以下两个方法来解决问题：1）引入全球-本地投票模组，以帮助伪造面部的不同权重区域进行整合；2）设计一种基于信任的软定式标签策略，以减少 pseudo-noise 对不明文件集的影响。此外，我们将 CPL 框架扩展为多阶段模型，利用预训技术和迭代学习来进一步增强 traceability 性能。实验结果显示了我们的提案方法在 OW-DFA 中的超越性和深度伪造检测领域的解释性。”
</details></li>
</ul>
<hr>
<h2 id="Language-Oriented-Communication-with-Semantic-Coding-and-Knowledge-Distillation-for-Text-to-Image-Generation"><a href="#Language-Oriented-Communication-with-Semantic-Coding-and-Knowledge-Distillation-for-Text-to-Image-Generation" class="headerlink" title="Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation"></a>Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11127">http://arxiv.org/abs/2309.11127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyelin Nam, Jihong Park, Jinho Choi, Mehdi Bennis, Seong-Lyun Kim</li>
<li>for: 提出了一种语言响应式 semantic communication（LSC）框架，用于机器人与人类之间的语言交互。</li>
<li>methods: 提出了三种新算法：1）Semantic Source Coding（SSC），压缩文本提示中的主要词语，保持提示的语法结构和上下文；2）Semantic Channel Coding（SCC），使用长语言同义词代替主要词语，提高免错性；3）Semantic Knowledge Distillation（SKD），通过在学习Listener的语言风格上下文中进行启发式学习，生成适应Listener的提示。</li>
<li>results: 在进行文本生成到图像任务中，提议的方法可以实现更高的感知相似性，并降低通信频率，同时提高干扰通信频率下的Robustness。<details>
<summary>Abstract</summary>
By integrating recent advances in large language models (LLMs) and generative models into the emerging semantic communication (SC) paradigm, in this article we put forward to a novel framework of language-oriented semantic communication (LSC). In LSC, machines communicate using human language messages that can be interpreted and manipulated via natural language processing (NLP) techniques for SC efficiency. To demonstrate LSC's potential, we introduce three innovative algorithms: 1) semantic source coding (SSC) which compresses a text prompt into its key head words capturing the prompt's syntactic essence while maintaining their appearance order to keep the prompt's context; 2) semantic channel coding (SCC) that improves robustness against errors by substituting head words with their lenghthier synonyms; and 3) semantic knowledge distillation (SKD) that produces listener-customized prompts via in-context learning the listener's language style. In a communication task for progressive text-to-image generation, the proposed methods achieve higher perceptual similarities with fewer transmissions while enhancing robustness in noisy communication channels.
</details>
<details>
<summary>摘要</summary>
通过将最新的大语言模型（LLM）和生成模型与发展的语义通信（SC） paradigm结合起来，在本文我们提出了一种新的语言启发型通信（LSC）框架。在LSC中，机器通过使用人类语言消息进行通信，这些消息可以通过自然语言处理（NLP）技术进行解释和修改，以提高SC的效率。为了证明LSC的潜力，我们提出了三种新算法：1） semanticsource coding（SSC），它压缩文本提示到其主要头语言，保留提示的语法结构和上下文；2） semantics channel coding（SCC），它通过将主要头语言替换为其更长的同义词，提高了对错误的Robustness；3） semantics knowledge distillation（SKD），它通过在上下文学习收者的语言风格，生成适合收者的启发式文本。在一个进步文本到图像生成任务中，我们的提案方法可以实现更高的感知相似性，同时减少传输量，并在噪声通信频道中提高了Robustness。
</details></li>
</ul>
<hr>
<h2 id="Learning-Complete-Topology-Aware-Correlations-Between-Relations-for-Inductive-Link-Prediction"><a href="#Learning-Complete-Topology-Aware-Correlations-Between-Relations-for-Inductive-Link-Prediction" class="headerlink" title="Learning Complete Topology-Aware Correlations Between Relations for Inductive Link Prediction"></a>Learning Complete Topology-Aware Correlations Between Relations for Inductive Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11528">http://arxiv.org/abs/2309.11528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Wang, Hanzhu Chen, Qitan Lv, Zhihao Shi, Jiajun Chen, Huarui He, Hongtao Xie, Yongdong Zhang, Feng Wu</li>
<li>for: 提高知识图的完整性和可靠性，使其能够在无需知道实体的情况下预测链接关系。</li>
<li>methods: 提出了一种新的子图基于方法TACO，利用逻辑相关性between关系来模型图STRUCTURE。TACO方法包括 seven种topological pattern，并通过关系相关网络（RCN）来学习每种pattern的重要性。</li>
<li>results: 对比 existed state-of-the-art方法，TACO方法在预测链接关系任务中表现出了superior的性能。<details>
<summary>Abstract</summary>
Inductive link prediction -- where entities during training and inference stages can be different -- has shown great potential for completing evolving knowledge graphs in an entity-independent manner. Many popular methods mainly focus on modeling graph-level features, while the edge-level interactions -- especially the semantic correlations between relations -- have been less explored. However, we notice a desirable property of semantic correlations between relations is that they are inherently edge-level and entity-independent. This implies the great potential of the semantic correlations for the entity-independent inductive link prediction task. Inspired by this observation, we propose a novel subgraph-based method, namely TACO, to model Topology-Aware COrrelations between relations that are highly correlated to their topological structures within subgraphs. Specifically, we prove that semantic correlations between any two relations can be categorized into seven topological patterns, and then proposes Relational Correlation Network (RCN) to learn the importance of each pattern. To further exploit the potential of RCN, we propose Complete Common Neighbor induced subgraph that can effectively preserve complete topological patterns within the subgraph. Extensive experiments demonstrate that TACO effectively unifies the graph-level information and edge-level interactions to jointly perform reasoning, leading to a superior performance over existing state-of-the-art methods for the inductive link prediction task.
</details>
<details>
<summary>摘要</summary>
依"\induction link prediction" -- 在训练和推理阶段之间的实体可以不同 -- 已经展现出了完善 evolving knowledge graphs 的巨大潜力。许多受欢迎的方法主要关注图级特征，而图级交互 -- 特别是关系之间的semantic correlation -- 则得到了更少的关注。然而，我们注意到了semantic correlation between relations 的一个愉悦性质，即它们是自然的edge-level和实体独立的。这意味着semantic correlation between relations 具有潜在的很大潜力 для实体独立的 inductive link prediction 任务。针对这一观察，我们提出了一种新的子图基于方法，即 TACO，用于模型 topology-aware COrrelations between relations （TACO）。具体来说，我们证明了任意两个关系的semantic correlation可以被分类为七种 topological pattern，并提出了 Relational Correlation Network (RCN) 来学习每种pattern的重要性。为了更好地利用 RCn 的潜力，我们提出了 Complete Common Neighbor induced subgraph，可以有效地保留完整的 topological patterns within the subgraph。我们的实验表明，TACO 能够具有图级信息和边级交互的整合，以jointly perform reasoning，从而对 inductive link prediction 任务 дости得更高的性能。
</details></li>
</ul>
<hr>
<h2 id="TrueLearn-A-Python-Library-for-Personalised-Informational-Recommendations-with-Implicit-Feedback"><a href="#TrueLearn-A-Python-Library-for-Personalised-Informational-Recommendations-with-Implicit-Feedback" class="headerlink" title="TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback"></a>TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11527">http://arxiv.org/abs/2309.11527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Qiu, Karim Djemili, Denis Elezi, Aaneel Shalman, María Pérez-Ortiz, Sahan Bulathwela</li>
<li>for: 这篇论文是为了介绍TrueLearn Python库，这是一个基于在线学习 bayesian模型的教育（或更广泛的信息）推荐系统的建构。</li>
<li>methods: 这个家族模型采用了”开放学习”概念，使用人类可理解的用户表示。为了提高可读性和让用户控制自己的模型，TrueLearn库还包含了不同的表示方式，可以帮助用户视觉化自己的学习者模型。</li>
<li>results: 论文附录了一个已经公布的隐式反馈教育数据集，并提供了评价指标来衡量模型的性能。TrueLearn库的广泛的文档和代码示例使得机器学习开发者和教育数据挖掘和学习分析专家可以很容易地使用这个库。<details>
<summary>Abstract</summary>
This work describes the TrueLearn Python library, which contains a family of online learning Bayesian models for building educational (or more generally, informational) recommendation systems. This family of models was designed following the "open learner" concept, using humanly-intuitive user representations. For the sake of interpretability and putting the user in control, the TrueLearn library also contains different representations to help end-users visualise the learner models, which may in the future facilitate user interaction with their own models. Together with the library, we include a previously publicly released implicit feedback educational dataset with evaluation metrics to measure the performance of the models. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytic practitioners. The library and the support documentation with examples are available at https://truelearn.readthedocs.io/en/latest.
</details>
<details>
<summary>摘要</summary>
这个工作描述了TrueLearn Python库，该库包含一家在线学习 bayesian 模型，用于建立教育（或更广泛地说，信息）推荐系统。这家模型遵循“开放学习”概念，使用人类可理解的用户表示。为了提高可解性和让用户控制，TrueLearn 库还包含了不同的表示，帮助结束用户可视化学习者模型，以便未来与自己的模型进行交互。此外，我们还提供了在线学习教育数据集，以便评估模型的性能。TrueLearn 库的文档和代码示例使得机器学习开发者和教育数据挖掘和学习分析专业人士可以轻松地使用。库和支持文档，以及示例可以在 <https://truelearn.readthedocs.io/en/latest> 上获取。
</details></li>
</ul>
<hr>
<h2 id="AttentionMix-Data-augmentation-method-that-relies-on-BERT-attention-mechanism"><a href="#AttentionMix-Data-augmentation-method-that-relies-on-BERT-attention-mechanism" class="headerlink" title="AttentionMix: Data augmentation method that relies on BERT attention mechanism"></a>AttentionMix: Data augmentation method that relies on BERT attention mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11104">http://arxiv.org/abs/2309.11104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Lewy, Jacek Mańdziuk</li>
<li>for: 这 paper 是关于如何在自然语言处理（NLP）领域中使用混合方法进行数据增强的研究。</li>
<li>methods: 这 paper 使用了一种新的混合方法 called AttentionMix，它基于注意力机制。这种方法可以应用于任何注意力基于模型。</li>
<li>results: 在三个标准情感分类 dataset 上测试，AttentionMix 都超过了两种 Mixup 机制的参考方法以及vanilla BERT 方法。结果表明，注意力信息可以有效地用于 NLP 领域中的数据增强。<details>
<summary>Abstract</summary>
The Mixup method has proven to be a powerful data augmentation technique in Computer Vision, with many successors that perform image mixing in a guided manner. One of the interesting research directions is transferring the underlying Mixup idea to other domains, e.g. Natural Language Processing (NLP). Even though there already exist several methods that apply Mixup to textual data, there is still room for new, improved approaches. In this work, we introduce AttentionMix, a novel mixing method that relies on attention-based information. While the paper focuses on the BERT attention mechanism, the proposed approach can be applied to generally any attention-based model. AttentionMix is evaluated on 3 standard sentiment classification datasets and in all three cases outperforms two benchmark approaches that utilize Mixup mechanism, as well as the vanilla BERT method. The results confirm that the attention-based information can be effectively used for data augmentation in the NLP domain.
</details>
<details>
<summary>摘要</summary>
《混合方法》在计算机视觉领域已经证明是一种强大的数据增强技术，有许多后继者在指导下进行图像混合。一个有趣的研究方向是将基于混合的想法传递到其他领域，如自然语言处理（NLP）。虽然现有一些应用混合到文本数据的方法，但还是有很多空间 для新的、改进的方法。在这项工作中，我们介绍了一种新的混合方法，即关注混合（AttentionMix）。这种方法基于关注信息，而paper中关注BERT的注意机制。AttentionMix可以应用于任何关注基于模型。我们在3个标准情感分类dataset上进行评估，并在所有3个案例中超过了两个参考方法和vanilla BERT方法。结果表明，关注信息可以有效地用于NLP领域中的数据增强。
</details></li>
</ul>
<hr>
<h2 id="A-New-Interpretable-Neural-Network-Based-Rule-Model-for-Healthcare-Decision-Making"><a href="#A-New-Interpretable-Neural-Network-Based-Rule-Model-for-Healthcare-Decision-Making" class="headerlink" title="A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making"></a>A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11101">http://arxiv.org/abs/2309.11101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Benamira, Tristan Guerand, Thomas Peyrin</li>
<li>for: 本研究旨在提出一种神经网络框架，即 $\textit{Truth Table rules}$（TT-rules），该框架结合神经网络的高性能和规则型模型的全面和准确解释性质。</li>
<li>methods: TT-rules 基于 $\textit{Truth Table nets}$（TTnet），一种初始为形式验证而开发的深度神经网络家族。通过从训练过程中提取全面和准确的规则集 $\mathcal{R}$，以便使得 TT-rules 模型能够具备全面和准确的解释性。</li>
<li>results: 我们对健康应用场景中的数据进行评估，并与现有的解释性方法进行比较。结果表明，TT-rules 能够达到与其他解释性方法相当或更高的性能，并且在大型表格数据集上进行适应也是可能的。特别是，TT-rules 成为了首个能够适应大型表格数据集，包括两个真实的 DNA 数据集，每个数据集具有超过 20K 的特征的解释性模型。<details>
<summary>Abstract</summary>
In healthcare applications, understanding how machine/deep learning models make decisions is crucial. In this study, we introduce a neural network framework, $\textit{Truth Table rules}$ (TT-rules), that combines the global and exact interpretability properties of rule-based models with the high performance of deep neural networks. TT-rules is built upon $\textit{Truth Table nets}$ (TTnet), a family of deep neural networks initially developed for formal verification. By extracting the necessary and sufficient rules $\mathcal{R}$ from the trained TTnet model (global interpretability) to yield the same output as the TTnet (exact interpretability), TT-rules effectively transforms the neural network into a rule-based model. This rule-based model supports binary classification, multi-label classification, and regression tasks for small to large tabular datasets. After outlining the framework, we evaluate TT-rules' performance on healthcare applications and compare it to state-of-the-art rule-based methods. Our results demonstrate that TT-rules achieves equal or higher performance compared to other interpretable methods. Notably, TT-rules presents the first accurate rule-based model capable of fitting large tabular datasets, including two real-life DNA datasets with over 20K features.
</details>
<details>
<summary>摘要</summary>
在医疗应用中，理解机器学习/深度学习模型的决策方法是非常重要的。在这项研究中，我们介绍了一种神经网络框架，称为“真实表格规则”（TT-rules），这种框架结合了神经网络的高性能和规则型模型的全面和准确解释性质。TT-rules基于一种名为“真实表格网络”（TTnet）的深度神经网络，该网络最初是为了正式验证而开发的。通过从训练过程中提取出神经网络模型中的必要和充分规则（global interpretability），并将这些规则转换成可以准确地预测神经网络输出的规则型模型（exact interpretability），TT-rules可以将神经网络转换成一种规则型模型。这种规则型模型支持二分类、多标签分类和回归任务，适用于小至大的表格数据集。在这项研究中，我们介绍了TT-rules的框架，并对其性能进行了健康应用的评估，并与当前的可解释方法进行了比较。我们的结果表明，TT-rules可以与其他可解释方法匹配或超越其性能。尤其是TT-rules是首个能够适用于大型表格数据集的准确规则型模型，包括两个实际的DNA数据集，每个数据集有超过20K的特征。
</details></li>
</ul>
<hr>
<h2 id="Likelihood-based-Sensor-Calibration-for-Expert-Supported-Distributed-Learning-Algorithms-in-IoT-Systems"><a href="#Likelihood-based-Sensor-Calibration-for-Expert-Supported-Distributed-Learning-Algorithms-in-IoT-Systems" class="headerlink" title="Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems"></a>Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11526">http://arxiv.org/abs/2309.11526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rüdiger Machhamer, Lejla Begic Fazlic, Eray Guven, David Junk, Gunes Karabulut Kurt, Stefan Naumann, Stephan Didas, Klaus-Uwe Gollmer, Ralph Bergmann, Ingo J. Timm, Guido Dartmann</li>
<li>for: 这篇论文主要是为了提高感知技术中数据测量的精度和效率。</li>
<li>methods: 本论文使用了估计斜射变换的方法，并利用专家知识进行改进。它还可以应用于软件校准、专家基于适应和联邦学习方法。</li>
<li>results: 实验和仿真数据都表明，这种解决方案可以提高测量数据的精度和效率。<details>
<summary>Abstract</summary>
An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
</details>
<details>
<summary>摘要</summary>
在感测技术领域，一项重要任务是有效地实现感测器之间测量转换的方法。一种思路是使用估算投影变换，可以通过专家知识进行改进。这篇文章介绍了1973年由冰川研究所发表的改进解决方案。我们表明该解决方案可以适用于软件准确性检测、专家知识基于的调整和联邦学习方法。我们通过实验和真实测量数据来评估我们的研究。结果表明，优化后的方法可以提高仪器测量精度。Here's the translation in Traditional Chinese:在感测技术领域，一个重要任务是有效地实现感测器之间测量转换的方法。一种思路是使用估算投影变换，可以通过专家知识进行改进。这篇文章介绍了1973年由冰川研究所发表的改进解决方案。我们表明这个解决方案可以应用于软件准确性检测、专家知识基于的调整和联邦学习方法。我们通过实验和真实测量数据来评估我们的研究。结果表明，优化后的方法可以提高仪器测量精度。
</details></li>
</ul>
<hr>
<h2 id="Practical-Probabilistic-Model-based-Deep-Reinforcement-Learning-by-Integrating-Dropout-Uncertainty-and-Trajectory-Sampling"><a href="#Practical-Probabilistic-Model-based-Deep-Reinforcement-Learning-by-Integrating-Dropout-Uncertainty-and-Trajectory-Sampling" class="headerlink" title="Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling"></a>Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11089">http://arxiv.org/abs/2309.11089</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrjun123/DPETS">https://github.com/mrjun123/DPETS</a></li>
<li>paper_authors: Wenjun Huang, Yunduan Cui, Huiyun Li, Xinyu Wu</li>
<li>for: 这篇论文旨在解决现有的 probabilistic model-based reinforcement learning（MBRL）模型，它们基于神经网络，但它们的预测稳定性和准确性有限制。</li>
<li>methods: 该论文提出了一种新的方法，即dropout-based probabilistic ensembles with trajectory sampling（DPETS），它将Monte-Carlo dropout和 trajectory sampling结合在一起，以稳定地预测系统的不确定性。DPETS的损失函数设计用于更正神经网络的适应错误，以更准确地预测 probabilistic models。</li>
<li>results: 论文通过在多个 Mujoco  benchmark control任务和一个实际的 robot arm manipulation任务上进行评估，发现 DPETS 可以在更高的 sample efficiency 下达到更高的均返回值和快速吞吐量，同时超过了相关的 MBRL 方法。此外，DPETS 还可以在面临附加干扰和实际操作中表现出色。<details>
<summary>Abstract</summary>
This paper addresses the prediction stability, prediction accuracy and control capability of the current probabilistic model-based reinforcement learning (MBRL) built on neural networks. A novel approach dropout-based probabilistic ensembles with trajectory sampling (DPETS) is proposed where the system uncertainty is stably predicted by combining the Monte-Carlo dropout and trajectory sampling in one framework. Its loss function is designed to correct the fitting error of neural networks for more accurate prediction of probabilistic models. The state propagation in its policy is extended to filter the aleatoric uncertainty for superior control capability. Evaluated by several Mujoco benchmark control tasks under additional disturbances and one practical robot arm manipulation task, DPETS outperforms related MBRL approaches in both average return and convergence velocity while achieving superior performance than well-known model-free baselines with significant sample efficiency. The open source code of DPETS is available at https://github.com/mrjun123/DPETS.
</details>
<details>
<summary>摘要</summary>
In the evaluation, DPETS outperforms other MBRL approaches in both average return and convergence velocity on several Mujoco benchmark control tasks with additional disturbances and one practical robot arm manipulation task. It also achieves superior performance compared to well-known model-free baselines with significant sample efficiency. The open source code of DPETS is available on GitHub at https://github.com/mrjun123/DPETS.Translated into Simplified Chinese:这篇论文关注现有基于神经网络的概率模型学习（MBRL）方法的预测稳定性、预测准确性和控制能力。一种新的方法叫做 dropout-based 概率集合with trajectory sampling（DPETS）被提议，它将 Monte-Carlo dropout 和 trajectory sampling 集成到一个框架中，以稳定系统uncertainty的预测。loss函数设计用于更正神经网络的适应错误，以便更准确地预测概率模型。Policy也被扩展以筛选 aleatoric uncertainty，以提高控制能力。在评估中，DPETS 比其他 MBRL 方法在多个 Mujoco benchmark控制任务上（包括附加干扰）和一个实际的机械臂控制任务上表现出更高的平均返点和更快的连续速度，同时与许多已知的模型自由基eline表现出更好的性能，并且具有显著的样本效率。DPETS 的开源代码可以在 GitHub 上获取，地址为 <https://github.com/mrjun123/DPETS>。
</details></li>
</ul>
<hr>
<h2 id="Embed-Search-Align-DNA-Sequence-Alignment-using-Transformer-Models"><a href="#Embed-Search-Align-DNA-Sequence-Alignment-using-Transformer-Models" class="headerlink" title="Embed-Search-Align: DNA Sequence Alignment using Transformer Models"></a>Embed-Search-Align: DNA Sequence Alignment using Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11087">http://arxiv.org/abs/2309.11087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Holur, K. C. Enevoldsen, Lajoyce Mboning, Thalia Georgiou, Louis-S. Bouchard, Matteo Pellegrini, Vwani Roychowdhury</li>
<li>for: 该研究旨在开发一种基于Transformer架构的DNA序列对齐方法，以提高DNA序列对齐精度。</li>
<li>methods: 该方法使用了自动对齐精度进行自我超vised培训，并引入了DNAvector存储以实现全文搜索。</li>
<li>results: 该方法可以高度准确地对齐250个基因组中的DNA序列，并且在不同染色体和物种上进行了任务转移。<details>
<summary>Abstract</summary>
DNA sequence alignment involves assigning short DNA reads to the most probable locations on an extensive reference genome. This process is crucial for various genomic analyses, including variant calling, transcriptomics, and epigenomics. Conventional methods, refined over decades, tackle this challenge in two steps: genome indexing followed by efficient search to locate likely positions for given reads. Building on the success of Large Language Models (LLM) in encoding text into embeddings, where the distance metric captures semantic similarity, recent efforts have explored whether the same Transformer architecture can produce numerical representations for DNA sequences. Such models have shown early promise in tasks involving classification of short DNA sequences, such as the detection of coding vs non-coding regions, as well as the identification of enhancer and promoter sequences. Performance at sequence classification tasks does not, however, translate to sequence alignment, where it is necessary to conduct a genome-wide search to successfully align every read. We address this open problem by framing it as an Embed-Search-Align task. In this framework, a novel encoder model DNA-ESA generates representations of reads and fragments of the reference, which are projected into a shared vector space where the read-fragment distance is used as surrogate for alignment. In particular, DNA-ESA introduces: (1) Contrastive loss for self-supervised training of DNA sequence representations, facilitating rich sequence-level embeddings, and (2) a DNA vector store to enable search across fragments on a global scale. DNA-ESA is >97% accurate when aligning 250-length reads onto a human reference genome of 3 gigabases (single-haploid), far exceeds the performance of 6 recent DNA-Transformer model baselines and shows task transfer across chromosomes and species.
</details>
<details>
<summary>摘要</summary>
DNNA序列Alignment含义在将短DNNA读物 assigning 到参考基因组中最有可能的位置上。这个过程是生物学分析中的关键步骤，包括变异检测、转录组学和epigenomics。传统方法通过两步进行：基因组索引，然后是高效的搜索来找到给定读物的可能位置。基于大自然语言模型（LLM）在编码文本为嵌入中的成功，最近的努力是否是使用同样的Transformer架构生成DNNA序列的数字表示。这些模型在短DNNA序列分类任务中表现出了早期的 promise，例如分类 coding vs non-coding 区域以及激活器和激发器序列的识别。但是，性能在序列分类任务上不能直接转移到Alignment任务，因为需要进行全基因组搜索以成功地对每个读物进行Alignment。我们解决这个开放问题 by framing it as an Embed-Search-Align task。在这种框架中，一种新的编码器模型DNA-ESA生成了读物和参考基因组中的 фрагментов的表示，并将它们投射到一个共享的vector空间中，其中读物-фрагмент的距离作为Alignment的Surrogate。特别是，DNA-ESA引入了：（1）对DNNA序列表示进行自我超vised 训练，以获得丰富的序列水平嵌入，以及（2）DNNA vector store，以实现在全球范围内搜索多个 фрагментов。DNNA-ESA在对3 gigabases的人类参考基因组上Alignment 250个长度的读物时，准确率高于97%，大幅超过了6个最近的DNNA-Transformer模型基eline，并在 хромосомы和种类之间显示任务传递。
</details></li>
</ul>
<hr>
<h2 id="Weak-Supervision-for-Label-Efficient-Visual-Bug-Detection"><a href="#Weak-Supervision-for-Label-Efficient-Visual-Bug-Detection" class="headerlink" title="Weak Supervision for Label Efficient Visual Bug Detection"></a>Weak Supervision for Label Efficient Visual Bug Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11077">http://arxiv.org/abs/2309.11077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farrukh Rahman</li>
<li>for: 本研究旨在提高视频游戏中的视觉质量，并 Addressing the challenge of traditional testing methods being limited by resources and unable to cover the wide range of potential bugs.</li>
<li>methods: 我们提出了一种新的方法，使用无标注游戏记录和域特定的扩充来生成数据集和自我标注目标，并在预训练或多任务设置中使用这些目标进行预训练。我们使用弱监督来扩大数据集，并实现了自主和互动式弱监督，通过不supervised clustering和&#x2F;或基于文本和几何提示的交互方式。</li>
<li>results: 我们在Giantmap游戏中测试了FPPC（首个玩家截割&#x2F;碰撞漏洞），发现我们的方法非常有效，超越了强监督基线，在实际、非常低频率、低数据量 régime中（0.336 $\rightarrow$ 0.550 F1分数）。只需5个标注的“好”示例（即0个漏洞），我们的自我标注目标就能够捕捉足够的信号，超越低标注监督设置。我们的方法可以在不同的视觉漏洞上进行应用，并且可以在视频游戏中拓展到更广泛的图像和视频任务。<details>
<summary>Abstract</summary>
As video games evolve into expansive, detailed worlds, visual quality becomes essential, yet increasingly challenging. Traditional testing methods, limited by resources, face difficulties in addressing the plethora of potential bugs. Machine learning offers scalable solutions; however, heavy reliance on large labeled datasets remains a constraint. Addressing this challenge, we propose a novel method, utilizing unlabeled gameplay and domain-specific augmentations to generate datasets & self-supervised objectives used during pre-training or multi-task settings for downstream visual bug detection. Our methodology uses weak-supervision to scale datasets for the crafted objectives and facilitates both autonomous and interactive weak-supervision, incorporating unsupervised clustering and/or an interactive approach based on text and geometric prompts. We demonstrate on first-person player clipping/collision bugs (FPPC) within the expansive Giantmap game world, that our approach is very effective, improving over a strong supervised baseline in a practical, very low-prevalence, low data regime (0.336 $\rightarrow$ 0.550 F1 score). With just 5 labeled "good" exemplars (i.e., 0 bugs), our self-supervised objective alone captures enough signal to outperform the low-labeled supervised settings. Building on large-pretrained vision models, our approach is adaptable across various visual bugs. Our results suggest applicability in curating datasets for broader image and video tasks within video games beyond visual bugs.
</details>
<details>
<summary>摘要</summary>
Traditional video game testing methods are limited by resources and have difficulty addressing the many potential bugs that exist. Machine learning offers scalable solutions, but relying on large labeled datasets is a challenge. To address this, we propose a new method that uses unlabeled gameplay and domain-specific augmentations to generate datasets and self-supervised objectives for pre-training or multi-task settings. Our method uses weak supervision to scale the datasets and can be used in both autonomous and interactive modes, incorporating unsupervised clustering and/or an interactive approach based on text and geometric prompts. We demonstrate the effectiveness of our approach on first-person player clipping/collision bugs within the Giantmap game world, achieving an F1 score of 0.550 in a practical, low-prevalence, low-data regime with just 5 labeled "good" exemplars. Our self-supervised objective captures enough signal to outperform low-labeled supervised settings, and our approach is adaptable to various visual bugs and can be applied to curating datasets for broader image and video tasks within video games.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Tiling-A-Model-Agnostic-Adaptive-Scalable-and-Inference-Data-Centric-Approach-for-Efficient-and-Accurate-Small-Object-Detection"><a href="#Dynamic-Tiling-A-Model-Agnostic-Adaptive-Scalable-and-Inference-Data-Centric-Approach-for-Efficient-and-Accurate-Small-Object-Detection" class="headerlink" title="Dynamic Tiling: A Model-Agnostic, Adaptive, Scalable, and Inference-Data-Centric Approach for Efficient and Accurate Small Object Detection"></a>Dynamic Tiling: A Model-Agnostic, Adaptive, Scalable, and Inference-Data-Centric Approach for Efficient and Accurate Small Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11069">http://arxiv.org/abs/2309.11069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Son The Nguyen, Theja Tulabandhula, Duy Nguyen</li>
<li>for: 这篇论文主要是为了提出一种模型不偏的、可适应的、扩展性强的小对象检测方法，以提高对象检测的准确率和效率。</li>
<li>methods: 这篇论文使用了动态瓷纹法，即首先使用非重叠的瓷纹来确定初始检测结果，然后通过动态调整瓷纹的重叠率和瓷纹最小化器来解决分布在不同瓷纹之间的 Fragmented 对象，从而提高检测精度和降低计算开销。</li>
<li>results: 相比现有的模型不偏的均匀裁剪方法，Dynamic Tiling 方法在不同的对象大小和环境下都能够达到更高的检测精度和效率，并且不需要劳动的重新调整。此外，这种方法还可以在不同的操作环境下进行适应，以提高对象检测的可扩展性和灵活性。<details>
<summary>Abstract</summary>
We introduce Dynamic Tiling, a model-agnostic, adaptive, and scalable approach for small object detection, anchored in our inference-data-centric philosophy. Dynamic Tiling starts with non-overlapping tiles for initial detections and utilizes dynamic overlapping rates along with a tile minimizer. This dual approach effectively resolves fragmented objects, improves detection accuracy, and minimizes computational overhead by reducing the number of forward passes through the object detection model. Adaptable to a variety of operational environments, our method negates the need for laborious recalibration. Additionally, our large-small filtering mechanism boosts the detection quality across a range of object sizes. Overall, Dynamic Tiling outperforms existing model-agnostic uniform cropping methods, setting new benchmarks for efficiency and accuracy.
</details>
<details>
<summary>摘要</summary>
我团队介绍了一种名为动态瓷纹的模型无关、可适应、可扩展的方法，用于小物体检测。这种方法基于我们的推理数据中心的哲学，使用非 overlap 的瓷纹开始，然后采用动态重叠率和瓷纹最小化器。这种双重方法能够有效地解决分割物体，提高检测精度，并减少计算负担。我们的方法适用于多种操作环境，无需劳辑重新调整。此外，我们的大小筛选机制可以在不同的物体大小下提高检测质量。总之，动态瓷纹超过了现有的模型无关均匀割 методы，设置了新的效率和准确性的benchmark。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Relationship-between-LLM-Hallucinations-and-Prompt-Linguistic-Nuances-Readability-Formality-and-Concreteness"><a href="#Exploring-the-Relationship-between-LLM-Hallucinations-and-Prompt-Linguistic-Nuances-Readability-Formality-and-Concreteness" class="headerlink" title="Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness"></a>Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11064">http://arxiv.org/abs/2309.11064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vipula Rawte, Prachi Priya, S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Amit Sheth, Amitava Das</li>
<li>for:  investigate the influence of linguistic factors in prompts on the occurrence of LLM hallucinations</li>
<li>methods:  experimental study using prompts with varying levels of readability, formality, and concreteness</li>
<li>results:  prompts with greater formality and concreteness tend to result in reduced hallucinations, while the outcomes pertaining to readability are mixed.<details>
<summary>Abstract</summary>
As Large Language Models (LLMs) have advanced, they have brought forth new challenges, with one of the prominent issues being LLM hallucination. While various mitigation techniques are emerging to address hallucination, it is equally crucial to delve into its underlying causes. Consequently, in this preliminary exploratory investigation, we examine how linguistic factors in prompts, specifically readability, formality, and concreteness, influence the occurrence of hallucinations. Our experimental results suggest that prompts characterized by greater formality and concreteness tend to result in reduced hallucination. However, the outcomes pertaining to readability are somewhat inconclusive, showing a mixed pattern.
</details>
<details>
<summary>摘要</summary>
LLMs 的进步也带来了新的挑战，其中一个主要问题是 LLM 幻觉。虽然各种 mitigation 技术正在emerging，但是也非常重要探讨幻觉的深层原因。因此，在这项初步的探索性研究中，我们研究了提示中语言因素对幻觉的影响，特别是可读性、正式度和具体性。我们的实验结果表明，使用更正式和具体的提示可以减少幻觉，但是关于可读性的结果呈杂化的模式。
</details></li>
</ul>
<hr>
<h2 id="Design-of-Chain-of-Thought-in-Math-Problem-Solving"><a href="#Design-of-Chain-of-Thought-in-Math-Problem-Solving" class="headerlink" title="Design of Chain-of-Thought in Math Problem Solving"></a>Design of Chain-of-Thought in Math Problem Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11054">http://arxiv.org/abs/2309.11054</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lqtrung1998/mwp_cot_design">https://github.com/lqtrung1998/mwp_cot_design</a></li>
<li>paper_authors: Zhanming Jie, Trung Quoc Luong, Xinbo Zhang, Xiaoran Jin, Hang Li</li>
<li>for: 本研究旨在探讨链条思维（CoT）在数学问题解决中的作用，并对不同的程序CoT进行比较，包括自然语言CoT、自我描述程序、注释描述程序和非描述程序。此外，研究还 investigate了编程语言对程序CoT的影响，并对Python和Wolfram语言进行比较。</li>
<li>methods: 本研究采用了extensive experiments方法，在GSM8K、MATHQA和SVAMP上进行了评测，并发现了程序CoT在数学问题解决中的优势。特别是，最佳组合（30B参数）击败了GPT-3.5-turbo的表现，并且自然语言CoT提供了更大的多样性，因此可以通常实现更高的性能。</li>
<li>results: 研究结果显示，程序CoT在数学问题解决中具有优势，特别是自然语言CoT提供了更大的多样性，可以实现更高的性能。此外，研究还发现了Python是程序CoT的更好的编程语言。研究结果可以为未来的CoT设计提供有价值的指导，并且可以考虑编程语言和编程风格的因素进行进一步的改进。<details>
<summary>Abstract</summary>
Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solving. We conduct a comprehensive examination of methods for designing CoT, comparing conventional natural language CoT with various program CoTs, including the self-describing program, the comment-describing program, and the non-describing program. Furthermore, we investigate the impact of programming language on program CoTs, comparing Python and Wolfram Language. Through extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs often have superior effectiveness in math problem solving. Notably, the best performing combination with 30B parameters beats GPT-3.5-turbo by a significant margin. The results show that self-describing program offers greater diversity and thus can generally achieve higher performance. We also find that Python is a better choice of language than Wolfram for program CoTs. The experimental results provide a valuable guideline for future CoT designs that take into account both programming language and coding style for further advancements. Our datasets and code are publicly available.
</details>
<details>
<summary>摘要</summary>
Chain-of-Thought (CoT) 在数学问题解决中扮演着关键性的角色。我们对设计 CoT 的方法进行了全面的评估，比较了自然语言 CoT 与不同的程序 CoT，包括自我描述程序、注释描述程序和非描述程序。此外，我们还 investigate了编程语言对程序 CoT 的影响，比较了 Python 和 Wolfram 语言。通过对 GSM8K、MATHQA 和 SVAMP 等数据集进行了广泛的实验，我们发现program CoT 在数学问题解决中经常具有更高的效果。特别是，使用 30B 参数的最佳组合可以很大幅度地超越 GPT-3.5-turbo。结果表明，自我描述程序可以提供更多的多样性，因此通常可以达到更高的性能。我们还发现 Python 比 Wolfram 更适合用于 program CoT。我们的实验结果提供了未来 CoT 设计的价值指南，考虑到编程语言和编程风格，以便进一步提高表达能力。我们的数据集和代码公开可用。
</details></li>
</ul>
<hr>
<h2 id="Clustered-FedStack-Intermediate-Global-Models-with-Bayesian-Information-Criterion"><a href="#Clustered-FedStack-Intermediate-Global-Models-with-Bayesian-Information-Criterion" class="headerlink" title="Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion"></a>Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11044">http://arxiv.org/abs/2309.11044</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanveer Shaik, Xiaohui Tao, Lin Li, Niall Higgins, Raj Gururajan, Xujuan Zhou, Jianming Yong</li>
<li>for: 提高 Federated Learning（FL）在非Identical和非独立分布（non-IID）和数据偏置标签（imbalanced labels）的情况下的性能。</li>
<li>methods: 使用 Stacked Federated Learning（FedStack）框架，并采用三种集群机制：K-Means、Agglomerative和Gaussian Mixture Models。使用 Bayesian Information Criterion（BIC）确定集群数量。</li>
<li>results: Clustered FedStack模型比基eline模型 WITH clustering机制表现更好，并且使用cyclical learning rates来估计框架的整合程度。<details>
<summary>Abstract</summary>
Federated Learning (FL) is currently one of the most popular technologies in the field of Artificial Intelligence (AI) due to its collaborative learning and ability to preserve client privacy. However, it faces challenges such as non-identically and non-independently distributed (non-IID) and data with imbalanced labels among local clients. To address these limitations, the research community has explored various approaches such as using local model parameters, federated generative adversarial learning, and federated representation learning. In our study, we propose a novel Clustered FedStack framework based on the previously published Stacked Federated Learning (FedStack) framework. The local clients send their model predictions and output layer weights to a server, which then builds a robust global model. This global model clusters the local clients based on their output layer weights using a clustering mechanism. We adopt three clustering mechanisms, namely K-Means, Agglomerative, and Gaussian Mixture Models, into the framework and evaluate their performance. We use Bayesian Information Criterion (BIC) with the maximum likelihood function to determine the number of clusters. The Clustered FedStack models outperform baseline models with clustering mechanisms. To estimate the convergence of our proposed framework, we use Cyclical learning rates.
</details>
<details>
<summary>摘要</summary>
现在的 Federated Learning（FL）技术在人工智能（AI）领域中非常流行，这是因为它可以实现协同学习并保持客户端隐私。然而，FL还面临着非标一同分布（non-IID）和数据偏极性（imbalanced labels）等问题。为了解决这些局限性，研究人员已经提出了多种方法，如使用本地模型参数、联邦生成敌方搜索学习和联邦表示学习。在我们的研究中，我们提出了一种基于先前发表的 Stacked Federated Learning（FedStack）框架的 Novel Clustered FedStack 框架。本地客户端将其模型预测结果和输出层加权值发送到服务器，服务器然后建立一个强大的全局模型。这个全局模型使用一种卷积机制将本地客户端分为不同的集群。我们在框架中采用了 K-Means、Agglomerative 和 Gaussian Mixture Models 三种卷积机制，并使用 Bayesian Information Criterion（BIC）与最大似然函数来确定集群数量。Clustered FedStack 模型在基eline模型中表现出色，以便估算我们提出的框架的整合。为了估算我们的提出的框架的整合，我们使用 Cyclical learning rates。
</details></li>
</ul>
<hr>
<h2 id="Making-Small-Language-Models-Better-Multi-task-Learners-with-Mixture-of-Task-Adapters"><a href="#Making-Small-Language-Models-Better-Multi-task-Learners-with-Mixture-of-Task-Adapters" class="headerlink" title="Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters"></a>Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11042">http://arxiv.org/abs/2309.11042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukang Xie, Chengyu Wang, Junbing Yan, Jiyong Zhou, Feiqi Deng, Jun Huang</li>
<li>for: 本研究旨在提出一种基于小语言模型（less than 1B parameters）的多任务学习系统，以支持域pecific应用。</li>
<li>methods: 本研究提出了一种扩展 transformer 架构的 Mixture-of-Task-Adapters（MTA）模块，以capture intra-task 和inter-task 知识。同时，提出了一种两个阶段训练方法来优化 adapter 之间的协作。</li>
<li>results: 实验结果表明，提出的 MTA 架构和两个阶段训练方法可以达到良好的性能。此外，基于 ALTER 的 MTA-equipped 语言模型在不同领域中也得到了良好的result。<details>
<summary>Abstract</summary>
Recently, Large Language Models (LLMs) have achieved amazing zero-shot learning performance over a variety of Natural Language Processing (NLP) tasks, especially for text generative tasks. Yet, the large size of LLMs often leads to the high computational cost of model training and online deployment. In our work, we present ALTER, a system that effectively builds the multi-tAsk Learners with mixTure-of-task-adaptERs upon small language models (with <1B parameters) to address multiple NLP tasks simultaneously, capturing the commonalities and differences between tasks, in order to support domain-specific applications. Specifically, in ALTER, we propose the Mixture-of-Task-Adapters (MTA) module as an extension to the transformer architecture for the underlying model to capture the intra-task and inter-task knowledge. A two-stage training method is further proposed to optimize the collaboration between adapters at a small computational cost. Experimental results over a mixture of NLP tasks show that our proposed MTA architecture and the two-stage training method achieve good performance. Based on ALTER, we have also produced MTA-equipped language models for various domains.
</details>
<details>
<summary>摘要</summary>
最近，大型语言模型（LLMs）在多种自然语言处理（NLP）任务上实现了惊人的零shot学习性能，尤其是文本生成任务。然而，大型模型的大小经常导致模型训练和在线部署的计算成本高涨。在我们的工作中，我们提出了ALTER系统，可以有效地建立多任务学习者，通过将小型语言模型（ Parameters <1B）扩展到多个NLP任务，以便同时处理多个任务，捕捉任务之间的共同点和差异，以支持域pecific应用。具体来说，在ALTER中，我们提出了mixture-of-task-adaptERs（MTA）模块，作为 transformer 架构的增强部分，以Capture intra-task和inter-task知识。我们还提出了一种两Stage训练方法，以便在小型计算成本下优化 adapter collaboration。实验结果表明，我们的提议的MTA架构和两Stage训练方法在一组多种NLP任务上具有良好的表现。基于ALTER，我们还生成了各个领域的MTA语言模型。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-in-Intelligent-Transportation-Systems-Recent-Applications-and-Open-Problems"><a href="#Federated-Learning-in-Intelligent-Transportation-Systems-Recent-Applications-and-Open-Problems" class="headerlink" title="Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems"></a>Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11039">http://arxiv.org/abs/2309.11039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiying Zhang, Jun Li, Long Shi, Ming Ding, Dinh C. Nguyen, Wuzheng Tan, Jian Weng, Zhu Han</li>
<li>for: 本研究旨在探讨基于分布式机器学习技术的智能交通系统（ITS）中的应用前景，以及在不同场景下如何使用 Federated Learning（FL）来解决智能交通系统中的问题。</li>
<li>methods: 本研究使用了分布式机器学习技术Federated Learning（FL）来解决智能交通系统中的问题，包括对象识别、交通管理和服务提供等场景。</li>
<li>results: 本研究发现了在智能交通系统中应用FL后，可以提高对象识别精度、提高交通管理效率和提高服务提供质量等。但是，FL也存在一些挑战，如数据不均匀分布、计算机力和存储空间的限制，以及隐私和安全问题。<details>
<summary>Abstract</summary>
Intelligent transportation systems (ITSs) have been fueled by the rapid development of communication technologies, sensor technologies, and the Internet of Things (IoT). Nonetheless, due to the dynamic characteristics of the vehicle networks, it is rather challenging to make timely and accurate decisions of vehicle behaviors. Moreover, in the presence of mobile wireless communications, the privacy and security of vehicle information are at constant risk. In this context, a new paradigm is urgently needed for various applications in dynamic vehicle environments. As a distributed machine learning technology, federated learning (FL) has received extensive attention due to its outstanding privacy protection properties and easy scalability. We conduct a comprehensive survey of the latest developments in FL for ITS. Specifically, we initially research the prevalent challenges in ITS and elucidate the motivations for applying FL from various perspectives. Subsequently, we review existing deployments of FL in ITS across various scenarios, and discuss specific potential issues in object recognition, traffic management, and service providing scenarios. Furthermore, we conduct a further analysis of the new challenges introduced by FL deployment and the inherent limitations that FL alone cannot fully address, including uneven data distribution, limited storage and computing power, and potential privacy and security concerns. We then examine the existing collaborative technologies that can help mitigate these challenges. Lastly, we discuss the open challenges that remain to be addressed in applying FL in ITS and propose several future research directions.
</details>
<details>
<summary>摘要</summary>
智能交通系统（ITS）因通信技术、感知技术和互联网对话的快速发展而得到推动。然而，由于车辆网络的动态特性，很难在时间上进行准确的车辆行为决策。此外，在移动无线通信的存在下，车辆信息的隐私和安全总是处于风险之中。在这种情况下，一种新的思维方式是紧迫的，以满足不同应用场景的需求。作为分布式机器学习技术，联邦学习（FL）在隐私保护和扩展可扩展性等方面受到了广泛的关注。我们进行了ITS中FL最新的发展情况的全面评估。我们首先研究了ITS中存在的主要挑战和应用FL的动机，然后评论了ITS中FL的不同场景应用，包括物体识别、交通管理和服务提供等方面的问题。此外，我们还进行了进一步的分析，探讨FL部署引入的新挑战和FL本身无法解决的内在限制，包括数据分布不均、计算和存储能力有限和隐私和安全问题。最后，我们讨论了在应用FL时存在的开放挑战，并提出了未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="ModelGiF-Gradient-Fields-for-Model-Functional-Distance"><a href="#ModelGiF-Gradient-Fields-for-Model-Functional-Distance" class="headerlink" title="ModelGiF: Gradient Fields for Model Functional Distance"></a>ModelGiF: Gradient Fields for Model Functional Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11013">http://arxiv.org/abs/2309.11013</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zju-vipa/modelgif">https://github.com/zju-vipa/modelgif</a></li>
<li>paper_authors: Jie Song, Zhengqi Xu, Sai Wu, Gang Chen, Mingli Song</li>
<li>for: 这 paper 的目的是量化不同预训练模型之间的功能距离，以便为各种目的进行评估。</li>
<li>methods: 该 paper 使用了基于 “场” 的思想，提出了 Model Gradient Field (ModelGiF)，用于从不同预训练模型中提取同谱表示。</li>
<li>results: 实验结果表明，ModelGiF 在任务相关性判断、知识产权保护和模型忘却验证等方面具有显著的优势，与当前竞争者相比显著性更高。<details>
<summary>Abstract</summary>
The last decade has witnessed the success of deep learning and the surge of publicly released trained models, which necessitates the quantification of the model functional distance for various purposes. However, quantifying the model functional distance is always challenging due to the opacity in inner workings and the heterogeneity in architectures or tasks. Inspired by the concept of "field" in physics, in this work we introduce Model Gradient Field (abbr. ModelGiF) to extract homogeneous representations from the heterogeneous pre-trained models. Our main assumption underlying ModelGiF is that each pre-trained deep model uniquely determines a ModelGiF over the input space. The distance between models can thus be measured by the similarity between their ModelGiFs. We validate the effectiveness of the proposed ModelGiF with a suite of testbeds, including task relatedness estimation, intellectual property protection, and model unlearning verification. Experimental results demonstrate the versatility of the proposed ModelGiF on these tasks, with significantly superiority performance to state-of-the-art competitors. Codes are available at https://github.com/zju-vipa/modelgif.
</details>
<details>
<summary>摘要</summary>
过去一个十年，深度学习的成功和公共释放的训练模型的涌现，使得模型功能距离的量化变得非常重要。然而，量化模型功能距离总是困难的，因为深度学习模型的内部工作机制是不透明的，而且模型或任务的architecture和task都是多样的。引用物理学中的“场”概念，在这种工作中我们提出了Model Gradient Field（简称ModelGiF）来EXTRACT homogeneous representation from heterogeneous pre-trained models。我们假设每个预训练深度模型具有唯一的ModelGiF over the input space，因此可以通过比较这些ModelGiF的相似性来度量模型之间的距离。我们验证了提议的ModelGiF的效果通过一系列测试床，包括任务相似性预测、知识产权保护和模型忘记验证。实验结果表明提议的ModelGiF在这些任务上具有显著的优势性能，与现有的竞争对手相比。代码可以在https://github.com/zju-vipa/modelgif上获取。
</details></li>
</ul>
<hr>
<h2 id="Spiking-NeRF-Making-Bio-inspired-Neural-Networks-See-through-the-Real-World"><a href="#Spiking-NeRF-Making-Bio-inspired-Neural-Networks-See-through-the-Real-World" class="headerlink" title="Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World"></a>Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10987">http://arxiv.org/abs/2309.10987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingting Yao, Qinghao Hu, Tielong Liu, Zitao Mo, Zeyu Zhu, Zhengyang Zhuge, Jian Cheng</li>
<li>for: 这个论文的目的是提出一种能源优化的神经鳗网络（Spiking Neural Network，SNN），用于实现高品质的3D场景渲染，并且与生物学上的神经元运作相似。</li>
<li>methods: 这个方法使用了神经鳗网络（SNN）和射线场（NeRF）技术，将射线场与时间维度进行对应，从而使计算变成了一个发射-自由的方式，以减少能源消耗。</li>
<li>results: 实验结果显示，这个方法可以实现$76.74%$的能源优化，并且与生物学上的神经元运作相似。<details>
<summary>Abstract</summary>
Spiking neuron networks (SNNs) have been thriving on numerous tasks to leverage their promising energy efficiency and exploit their potentialities as biologically plausible intelligence. Meanwhile, the Neural Radiance Fields (NeRF) render high-quality 3D scenes with massive energy consumption, and few works delve into the energy-saving solution with a bio-inspired approach. In this paper, we propose spiking NeRF (SpikingNeRF), which aligns the radiance ray with the temporal dimension of SNN, to naturally accommodate the SNN to the reconstruction of Radiance Fields. Thus, the computation turns into a spike-based, multiplication-free manner, reducing the energy consumption. In SpikingNeRF, each sampled point on the ray is matched onto a particular time step, and represented in a hybrid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked for better training and inference. However, this operation also incurs irregular temporal length. We propose the temporal condensing-and-padding (TCP) strategy to tackle the masked samples to maintain regular temporal length, i.e., regular tensors, for hardware-friendly computation. Extensive experiments on a variety of datasets demonstrate that our method reduces the $76.74\%$ energy consumption on average and obtains comparable synthesis quality with the ANN baseline.
</details>
<details>
<summary>摘要</summary>
神经风暴网络（SNN）在许多任务上得到了广泛应用，以利用其能效的能源和生物可能的智能潜力。然而，神经辐射场（NeRF）的渲染高质量3D场景却需要巨大的能源消耗，而很少的研究探讨了以生物静脉为导向的能源抑制方法。在这篇论文中，我们提出了神经辐射场（SpikingNeRF），它将辐射场的强度方向与SNN的时间维度对齐，以自然地让SNN参与辐射场的重建。因此，计算变成了一种快速、无 multiplication 的方式，从而降低了能源消耗。在SpikingNeRF中，每个样本点被匹配到特定的时间步，并以混合方式表示，保留了 voxel 网格。基于 voxel 网格，样本点是否需要被masking 以提高训练和推理的质量。然而，这个操作也会产生不规则的时间长度。我们提出了时间condensing-and-padding（TCP）策略，以解决masked samples的问题，以保持常规的时间长度，即常规的tensor，为硬件友好的计算。在多个dataset上进行了广泛的实验，表明我们的方法可以降低76.74%的能源消耗，并与ANN基线相当的Synthesis质量。
</details></li>
</ul>
<hr>
<h2 id="Is-GPT4-a-Good-Trader"><a href="#Is-GPT4-a-Good-Trader" class="headerlink" title="Is GPT4 a Good Trader?"></a>Is GPT4 a Good Trader?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10982">http://arxiv.org/abs/2309.10982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingzhe Wu<br>for: 本研究旨在检验GPT-4对经典投资理论的理解程度和对实际交易数据分析的代码解释能力。methods: 本研究使用GPT-4对特定资产的日均K线数据进行分析，基于尼采尔浪幕理论等特定理论。results: 本研究发现GPT-4在对实际交易数据分析中表现出较高的解释深度和准确率，同时提供了有价值的投资理论应用方法。<details>
<summary>Abstract</summary>
Recently, large language models (LLMs), particularly GPT-4, have demonstrated significant capabilities in various planning and reasoning tasks \cite{cheng2023gpt4,bubeck2023sparks}. Motivated by these advancements, there has been a surge of interest among researchers to harness the capabilities of GPT-4 for the automated design of quantitative factors that do not overlap with existing factor libraries, with an aspiration to achieve alpha returns \cite{webpagequant}. In contrast to these work, this study aims to examine the fidelity of GPT-4's comprehension of classic trading theories and its proficiency in applying its code interpreter abilities to real-world trading data analysis. Such an exploration is instrumental in discerning whether the underlying logic GPT-4 employs for trading is intrinsically reliable. Furthermore, given the acknowledged interpretative latitude inherent in most trading theories, we seek to distill more precise methodologies of deploying these theories from GPT-4's analytical process, potentially offering invaluable insights to human traders.   To achieve this objective, we selected daily candlestick (K-line) data from specific periods for certain assets, such as the Shanghai Stock Index. Through meticulous prompt engineering, we guided GPT-4 to analyze the technical structures embedded within this data, based on specific theories like the Elliott Wave Theory. We then subjected its analytical output to manual evaluation, assessing its interpretative depth and accuracy vis-\`a-vis these trading theories from multiple dimensions. The results and findings from this study could pave the way for a synergistic amalgamation of human expertise and AI-driven insights in the realm of trading.
</details>
<details>
<summary>摘要</summary>
最近，大语言模型（LLM），特别是GPT-4，在各种计划和理解任务中表现出了显著的能力。这些进步引起了研究人员对GPT-4的投资 alpha 回报的兴趣，并寻求通过自动设计不同于现有因素库的量化因素来实现这一目标。与这些工作不同，本研究旨在检验GPT-4对经典交易理论的理解和对实际交易数据分析中的代码解释能力。这种探索有助于判断GPT-4在交易中使用的逻辑是否具有内在的可靠性。此外，由于交易理论中的解释空间往往很大，我们寻求通过GPT-4的分析过程中提取更加精细的方法来应用这些理论，从而为人类交易员提供有价值的想法。为达到这个目标，我们选择了特定期间的一些资产的日均盘形（K-line）数据，例如上海股票指数。通过仔细的提问工程，我们导引GPT-4分析这些数据中的技术结构，基于特定的投资理论，如欧拉瓦vecenie理论。然后，我们对GPT-4的分析输出进行手动评估，评估其在这些交易理论多个维度的解释深度和准确性。研究结果和发现可能为人类专家和 AI 驱动的想法带来协同合作，为交易领域带来新的发展。
</details></li>
</ul>
<hr>
<h2 id="AI-Driven-Patient-Monitoring-with-Multi-Agent-Deep-Reinforcement-Learning"><a href="#AI-Driven-Patient-Monitoring-with-Multi-Agent-Deep-Reinforcement-Learning" class="headerlink" title="AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning"></a>AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10980">http://arxiv.org/abs/2309.10980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Jianming Yong, Hong-Ning Dai</li>
<li>for: 提高医疗卫生监测效果，实现时间有效的干预和改善医疗结果。</li>
<li>methods: 使用多智能深度强化学习（DRL）方法，投入多个学习代理，每个代理负责监测特定生理参数，如心率、呼吸和体温等。这些代理与通用医疗监测环境互动，学习患者的行为模式，根据紧急程度估算，向相应的医疗应急团队（METs）发出警示。</li>
<li>results: 与多种基线模型进行比较，研究表明，提posed的DRL方法在实际生理和运动数据集PPG-DaLiA和WESAD上的表现准确性高于所有基线模型，并且通过调整Hyperparameter进行优化，进一步提高代理的总性能。<details>
<summary>Abstract</summary>
Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results with several baseline models, including Q-Learning, PPO, Actor-Critic, Double DQN, and DDPG, as well as monitoring frameworks like WISEML and CA-MAQL. Our experiments demonstrate that the proposed DRL approach outperforms all other baseline models, achieving more accurate monitoring of patient's vital signs. Furthermore, we conduct hyperparameter optimization to fine-tune the learning process of each agent. By optimizing hyperparameters, we enhance the learning rate and discount factor, thereby improving the agents' overall performance in monitoring patient health status. Our AI-driven patient monitoring system offers several advantages over traditional methods, including the ability to handle complex and uncertain environments, adapt to varying patient conditions, and make real-time decisions without external supervision.
</details>
<details>
<summary>摘要</summary>
通过人工智能驱动的患者监测框架，我们可以提高医疗结果和患者监测效果。传统的监测系统经常在复杂和动态的环境中难以处理，导致检测重要情况的延迟。为解决这个挑战，我们提出了一种基于多代理深度学习（DRL）的新型患者监测框架。我们的方法在多个学习代理之间分配不同的生物 physiological 特征，例如心率、呼吸和体温。这些代理与一个通用医疗监测环境进行交互，学习患者的行为模式，并根据紧急程度来通知相应的医疗紧急队伍（METs）。在本研究中，我们使用实际的生理和运动数据进行评估，并与多种基准模型进行比较，包括Q学习、PPO、actor-critic、Double DQN 和 DDPG 等。我们的实验表明，提出的 DRL 方法在监测患者生命体征上的准确性比基准模型高。此外，我们还进行了 гипер参数优化，以提高每个代理的学习过程。通过优化 гипер参数，我们可以提高代理的总表现，以更好地监测患者健康状态。我们的人工智能驱动的患者监测系统具有许多优势，包括能够处理复杂和不确定的环境、适应变化的患者状况，以及不需要外部监督而行动。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/cs.AI_2023_09_20/" data-id="clnsn0vde004fgf88gu52cxcz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/cs.CL_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T11:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/cs.CL_2023_09_20/">cs.CL - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Semi-supervised-News-Discourse-Profiling-with-Contrastive-Learning"><a href="#Semi-supervised-News-Discourse-Profiling-with-Contrastive-Learning" class="headerlink" title="Semi-supervised News Discourse Profiling with Contrastive Learning"></a>Semi-supervised News Discourse Profiling with Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11692">http://arxiv.org/abs/2309.11692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Li, Ruihong Huang</li>
<li>for: 这篇论文是为了解决新闻报道中文本结构分类问题，以便在下游应用中使用。</li>
<li>methods: 该论文使用了一种新的方法，即内文对照学习 WITH 精神（ICLD），利用新闻报道的特殊结构特征进行采样，并通过对比分类来增强模型的性能。</li>
<li>results: 论文的实验结果表明，ICLD 方法可以有效地解决新闻报道中文本结构分类问题，并且比传统的监督学习方法更有效。<details>
<summary>Abstract</summary>
News Discourse Profiling seeks to scrutinize the event-related role of each sentence in a news article and has been proven useful across various downstream applications. Specifically, within the context of a given news discourse, each sentence is assigned to a pre-defined category contingent upon its depiction of the news event structure. However, existing approaches suffer from an inadequacy of available human-annotated data, due to the laborious and time-intensive nature of generating discourse-level annotations. In this paper, we present a novel approach, denoted as Intra-document Contrastive Learning with Distillation (ICLD), for addressing the news discourse profiling task, capitalizing on its unique structural characteristics. Notably, we are the first to apply a semi-supervised methodology within this task paradigm, and evaluation demonstrates the effectiveness of the presented approach.
</details>
<details>
<summary>摘要</summary>
新闻话语分析旨在研究每个新闻文章中的每句话语的事件相关性角色，并在多种下游应用中表现出有用性。特别是在给定的新闻话语背景下，每句话语会被分配到预定的类别，根据它们描述新闻事件结构。然而，现有的方法受到有限的人工标注数据的不足，这是因为生成话语水平标注的劳动和时间费时的。在这篇论文中，我们提出了一种新的方法，称为Intra-document Contrastive Learning with Distillation（ICLD），用于解决新闻话语 profiling 任务，利用它的独特结构特征。值得注意的是，我们是首次在这个任务准则下应用 semi-supervised 方法ологи，评估结果表明该方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="A-Paradigm-Shift-in-Machine-Translation-Boosting-Translation-Performance-of-Large-Language-Models"><a href="#A-Paradigm-Shift-in-Machine-Translation-Boosting-Translation-Performance-of-Large-Language-Models" class="headerlink" title="A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models"></a>A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11674">http://arxiv.org/abs/2309.11674</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fe1ixxu/alma">https://github.com/fe1ixxu/alma</a></li>
<li>paper_authors: Haoran Xu, Young Jin Kim, Amr Sharaf, Hany Hassan Awadalla</li>
<li>for: 提高 moderate-sized language models (LLMs) 在翻译任务中的表现</li>
<li>methods: 提出了一种特有的练习方法，通过首先在单语言数据上进行初始练习，然后在一小量高质量并列数据上进行后练习，以消除传统翻译模型通常依赖的庞大并列数据的需求。</li>
<li>results: 根据 LLaMA-2 为基础模型，实现了在 WMT’21 和 WMT’22 测试集上的平均提高超过 12 BLEU 和 12 COMET，在 10 个翻译方向上。表现较之前的所有工作更好，甚至超过 NLLB-54B 模型和 GPT-3.5-text-davinci-003，即使只有 7B 或 13B 参数。这种方法为机器翻译训练方法提供了基础。<details>
<summary>Abstract</summary>
Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these moderate LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our results show that the model can achieve an average improvement of more than 12 BLEU and 12 COMET over its zero-shot performance across 10 translation directions from the WMT'21 (2 directions) and WMT'22 (8 directions) test datasets. The performance is significantly better than all prior work and even superior to the NLLB-54B model and GPT-3.5-text-davinci-003, with only 7B or 13B parameters. This method establishes the foundation for a novel training paradigm in machine translation.
</details>
<details>
<summary>摘要</summary>
生成大型自然语言模型（LLM）在不同的自然语言处理任务中已经取得了非常出色的进步。然而，这些进步并没有反映在翻译任务中，尤其是使用中型模型（i.e., 7B或13B参数），这些模型仍然落后于传统的监督编码器-解码器翻译模型。先前的研究已经尝试使用不同的方法来提高这些中型LLM的翻译能力，但其成果很有限。在这个研究中，我们提出了一种特有的练习方法，用于提高LLM的翻译能力，不需要大量的并行数据。我们的方法包括两个练习阶段：首先在单语言数据上进行初始练习，然后在一个小量高质量并行数据上进行 subsequential 练习。我们称之为Advanced Language Model-based trAnslator（ALMA）。基于LLaMA-2作为我们的基础模型，我们的结果显示，该模型可以在10个翻译方向上 average 提高超过12个BLEU和12个COMET的性能，相比于零开始性能。这个性能高于所有之前的工作，甚至超过NLLB-54B模型和GPT-3.5-text-davinci-003模型，即使只有7B或13B参数。这种方法创立了一种新的训练 парадигма在机器翻译领域。
</details></li>
</ul>
<hr>
<h2 id="Construction-of-Paired-Knowledge-Graph-Text-Datasets-Informed-by-Cyclic-Evaluation"><a href="#Construction-of-Paired-Knowledge-Graph-Text-Datasets-Informed-by-Cyclic-Evaluation" class="headerlink" title="Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation"></a>Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11669">http://arxiv.org/abs/2309.11669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Mousavi, Xin Zhan, He Bai, Peng Shi, Theo Rekatsinas, Benjamin Han, Yunyao Li, Jeff Pound, Josh Susskind, Natalie Schluter, Ihab Ilyas, Navdeep Jaitly</li>
<li>for: 这 paper 的目的是证明使用不同噪音水平生成 Knowledge Graph (KG) 和文本对应的数据集，可以训练前向和反向神经网络模型，但是使用不同的数据集可能会导致更多的幻觉和更差的拟合率。</li>
<li>methods: 这 paper 使用了生成文本和 KG 的cyclic evaluation来评估模型的性能，并通过手动创建 WebNLG 和自动创建 TeKGen 和 T-REx 来评估模型的表现。</li>
<li>results: 这 paper 发现，使用不同噪音水平生成的数据集可以影响模型的性能，并且手动创建的 WebNLG 表现更好于自动创建的 TeKGen 和 T-REx。此外，使用大语言模型 (LLM) 构建的数据集可以训练模型在文本生成中表现出色，但是在 Knowledge Graph 生成中表现较差，可能是因为没有一个共同的 Ontology。<details>
<summary>Abstract</summary>
Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Guided by these observations, we construct a new, improved dataset called LAGRANGE using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We also construct two synthetic datasets using large language models (LLMs), and observe that these are conducive to models that perform significantly well on cyclic generation of text, but less so on cyclic generation of KGs, probably because of a lack of a consistent underlying ontology.
</details>
<details>
<summary>摘要</summary>
Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However, models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation, we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Guided by these observations, we construct a new, improved dataset called LAGRANGE using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We also construct two synthetic datasets using large language models (LLMs), and observe that these are conducive to models that perform significantly well on cyclic generation of text, but less so on cyclic generation of KGs, probably because of a lack of a consistent underlying ontology.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Towards-Effective-Disambiguation-for-Machine-Translation-with-Large-Language-Models"><a href="#Towards-Effective-Disambiguation-for-Machine-Translation-with-Large-Language-Models" class="headerlink" title="Towards Effective Disambiguation for Machine Translation with Large Language Models"></a>Towards Effective Disambiguation for Machine Translation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11668">http://arxiv.org/abs/2309.11668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Iyer, Pinzhen Chen, Alexandra Birch</li>
<li>for: 本研究旨在探讨大语言模型（LLM）在具有多义词和罕见词意义时翻译不确定性的能力。</li>
<li>methods: 我们提出了两种改进翻译不确定性处理方法，一种是在 Context 中学习，另一种是在特 curaously 编辑的不确定性数据集上进行练习和微调。</li>
<li>results: 实验结果表明，我们的方法可以与当前状态的系统如深度翻译和 NLLB 匹配或超越，在五种语言方向中四种方向中表现出色。<details>
<summary>Abstract</summary>
Resolving semantic ambiguity has long been recognised as a central challenge in the field of machine translation. Recent work on benchmarking translation performance on ambiguous sentences has exposed the limitations of conventional Neural Machine Translation (NMT) systems, which fail to capture many of these cases. Large language models (LLMs) have emerged as a promising alternative, demonstrating comparable performance to traditional NMT models while introducing new paradigms for controlling the target outputs. In this paper, we study the capabilities of LLMs to translate ambiguous sentences containing polysemous words and rare word senses. We also propose two ways to improve the handling of such ambiguity through in-context learning and fine-tuning on carefully curated ambiguous datasets. Experiments show that our methods can match or outperform state-of-the-art systems such as DeepL and NLLB in four out of five language directions. Our research provides valuable insights into effectively adapting LLMs for disambiguation during machine translation.
</details>
<details>
<summary>摘要</summary>
解决语义含义的挑战一直被认为是机器翻译领域的中心问题。最近的研究表明，使用含义ambiguous sentence进行翻译性能测试的传统神经机器翻译（NMT）系统有限，不能捕捉这些情况。大型语言模型（LLM）在这些情况下表现出了潜在的优势，并提出了新的控制目标输出的方法。在这篇论文中，我们研究了LLM在含义ambiguous sentence中翻译的能力，并提出了两种改进方法，通过在上下文学习和精心编辑的歧义数据进行训练。实验结果显示，我们的方法可以与现有的状态机DeepL和NLLB相当或超越，在五种语言方向中四种方向取得了最佳效果。我们的研究为将LLM适应到翻译中的歧义提供了有价值的视角。
</details></li>
</ul>
<hr>
<h2 id="Hate-speech-detection-in-algerian-dialect-using-deep-learning"><a href="#Hate-speech-detection-in-algerian-dialect-using-deep-learning" class="headerlink" title="Hate speech detection in algerian dialect using deep learning"></a>Hate speech detection in algerian dialect using deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11611">http://arxiv.org/abs/2309.11611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dihia Lanasri, Juan Olano, Sifal Klioui, Sin Liang Lee, Lamia Sekkai</li>
<li>for: 帮助掌握在阿拉伯语言上的仇恨言论检测问题，尤其是在阿尔жи尔语 dialect中。</li>
<li>methods: 使用深度学习架构对阿尔жи尔社交媒体上的短讯进行分类，以确定是否包含仇恨言论。</li>
<li>results: 在对13500余个阿尔жи尔社交媒体短讯的实验中，提出了一种可靠的仇恨言论检测方法，并取得了批判性的结果。<details>
<summary>Abstract</summary>
With the proliferation of hate speech on social networks under different formats, such as abusive language, cyberbullying, and violence, etc., people have experienced a significant increase in violence, putting them in uncomfortable situations and threats. Plenty of efforts have been dedicated in the last few years to overcome this phenomenon to detect hate speech in different structured languages like English, French, Arabic, and others. However, a reduced number of works deal with Arabic dialects like Tunisian, Egyptian, and Gulf, mainly the Algerian ones. To fill in the gap, we propose in this work a complete approach for detecting hate speech on online Algerian messages. Many deep learning architectures have been evaluated on the corpus we created from some Algerian social networks (Facebook, YouTube, and Twitter). This corpus contains more than 13.5K documents in Algerian dialect written in Arabic, labeled as hateful or non-hateful. Promising results are obtained, which show the efficiency of our approach.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:随着社交媒体上不同形式的仇恨言语、网络欺凌和暴力等等的普及，人们受到了不适的情况和威胁。过去几年，为了解决这种现象，各种努力已经投入了很多时间和精力，以检测不同的结构语言中的仇恨言语，如英语、法语、阿拉伯语等等。然而，对于阿拉伯 диалект，如突尼斯、埃及和 Golfo 的研究相对较少。为了填补这个空白，我们在这工作中提出了一个完整的方法，用于在在线阿尔及利亚消息中检测仇恨言语。我们在一些阿尔及利亚社交媒体（Facebook、YouTube和Twitter）上创建了一个大量的 corpus，包括13500余个文档，用阿尔及利亚 диалект的阿拉伯语书写，标注为有仇恨或无仇恨。我们评估了多种深度学习架构，并获得了良好的结果，这表明我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="SpeechAlign-a-Framework-for-Speech-Translation-Alignment-Evaluation"><a href="#SpeechAlign-a-Framework-for-Speech-Translation-Alignment-Evaluation" class="headerlink" title="SpeechAlign: a Framework for Speech Translation Alignment Evaluation"></a>SpeechAlign: a Framework for Speech Translation Alignment Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11585">http://arxiv.org/abs/2309.11585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Belen Alastruey, Aleix Sant, Gerard I. Gállego, David Dale, Marta R. Costa-jussà</li>
<li>for: 这篇论文主要为了评估speech模型中的source-target对应问题提供了一个框架。</li>
<li>methods: 这篇论文使用了两个核心组件：首先，它引入了一个英文-德语文本翻译金标对dataset，用于建立评估数据集。其次，它引入了两种新的精度指标，即Speech Alignment Error Rate (SAER)和Time-weighted Speech Alignment Error Rate (TW-SAER)，用于评估speech模型的对应质量。</li>
<li>results: 通过发布SpeechAlign框架，这篇论文为speech模型评估提供了可 accessible的评估框架，并通过使用这个框架对开源Speech Translation模型进行了比较。<details>
<summary>Abstract</summary>
Speech-to-Speech and Speech-to-Text translation are currently dynamic areas of research. To contribute to these fields, we present SpeechAlign, a framework to evaluate the underexplored field of source-target alignment in speech models. Our framework has two core components. First, to tackle the absence of suitable evaluation datasets, we introduce the Speech Gold Alignment dataset, built upon a English-German text translation gold alignment dataset. Secondly, we introduce two novel metrics, Speech Alignment Error Rate (SAER) and Time-weighted Speech Alignment Error Rate (TW-SAER), to evaluate alignment quality in speech models. By publishing SpeechAlign we provide an accessible evaluation framework for model assessment, and we employ it to benchmark open-source Speech Translation models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>现在演示的 Speech-to-Speech 和 Speech-to-Text 翻译是研究领域的动态领域。为了贡献这些领域，我们提出 SpeechAlign 框架，用于评估speech模型中source-target对齐的领域。我们的框架有两个核心组成部分。首先，由于缺乏适合的评估数据集，我们引入 Speech Gold Alignment 数据集，基于英语-德语文本翻译金标Alignment数据集。其次，我们引入两种新的指标，Speech Alignment Error Rate (SAER) 和 Time-weighted Speech Alignment Error Rate (TW-SAER)，用于评估对齐质量在speech模型中。通过发布 SpeechAlign，我们提供了一个可访问的评估框架，并使用它来对开源 Speech Translation 模型进行比较。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Singletons-and-Mention-based-Features-in-Coreference-Resolution-via-Multi-task-Learning-for-Better-Generalization"><a href="#Incorporating-Singletons-and-Mention-based-Features-in-Coreference-Resolution-via-Multi-task-Learning-for-Better-Generalization" class="headerlink" title="Incorporating Singletons and Mention-based Features in Coreference Resolution via Multi-task Learning for Better Generalization"></a>Incorporating Singletons and Mention-based Features in Coreference Resolution via Multi-task Learning for Better Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11582">http://arxiv.org/abs/2309.11582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yilunzhu/coref-mtl">https://github.com/yilunzhu/coref-mtl</a></li>
<li>paper_authors: Yilun Zhu, Siyao Peng, Sameer Pradhan, Amir Zeldes</li>
<li>for: 本研究旨在提高英语核心共referencing解决方法中的提及检测步骤，以提高核心共referencing的准确率和Robustness。</li>
<li>methods: 本研究使用多任务学习的方法，学习单个提及span的特征以及实体类型和信息状态的特征，以提高核心共referencing的准确率和Robustness。</li>
<li>results: 本研究在OntoGUMbenchmark上 achieve新的状态机制得分 (+2.7点)，并在多个out-of-domain数据集上提高了Robustness (+2.3点的平均提高值)，这些提高可能是由于更好的提及检测和更多的数据来自单个提及span的使用所致。<details>
<summary>Abstract</summary>
Previous attempts to incorporate a mention detection step into end-to-end neural coreference resolution for English have been hampered by the lack of singleton mention span data as well as other entity information. This paper presents a coreference model that learns singletons as well as features such as entity type and information status via a multi-task learning-based approach. This approach achieves new state-of-the-art scores on the OntoGUM benchmark (+2.7 points) and increases robustness on multiple out-of-domain datasets (+2.3 points on average), likely due to greater generalizability for mention detection and utilization of more data from singletons when compared to only coreferent mention pair matching.
</details>
<details>
<summary>摘要</summary>
先前的尝试将提及检测步骤包含在英语的端到端神经核心referencing中，受到缺乏单个提及跨度数据以及其他实体信息的限制。这篇论文提出了一种核心模型，可以学习单个提及以及实体类型和信息状态等特征，使用多任务学习的方式。这种方法在OntoGUM benchmark上达到了新的状态态标准分（+2.7分），并在多个 OUT-OF-DOMAIN 数据集上提高了鲁棒性（平均+2.3分），可能是因为更好的提及检测和更多的数据来自单个提及 span 的利用。
</details></li>
</ul>
<hr>
<h2 id="Examining-the-Limitations-of-Computational-Rumor-Detection-Models-Trained-on-Static-Datasets"><a href="#Examining-the-Limitations-of-Computational-Rumor-Detection-Models-Trained-on-Static-Datasets" class="headerlink" title="Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets"></a>Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11576">http://arxiv.org/abs/2309.11576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yida Mu, Xingyi Song, Kalina Bontcheva, Nikolaos Aletras</li>
<li>for: 本研究旨在评估内容基于和Context基于的谣言探测模型在探测新、未知谣言方面的表现差异。</li>
<li>methods: 本研究使用了实验方法来评估内容基于和Context基于的谣言探测模型在探测新、未知谣言方面的表现差异。</li>
<li>results: 研究结果表明，Context基于的模型仍然受到来源帖子信息的限制，并且忽略了上下文信息的重要作用。此外，研究还探讨了数据分割策略对分类器性能的影响，并提供了实践的建议来降低静态数据集中的时间概念漂移的影响。<details>
<summary>Abstract</summary>
A crucial aspect of a rumor detection model is its ability to generalize, particularly its ability to detect emerging, previously unknown rumors. Past research has indicated that content-based (i.e., using solely source posts as input) rumor detection models tend to perform less effectively on unseen rumors. At the same time, the potential of context-based models remains largely untapped. The main contribution of this paper is in the in-depth evaluation of the performance gap between content and context-based models specifically on detecting new, unseen rumors. Our empirical findings demonstrate that context-based models are still overly dependent on the information derived from the rumors' source post and tend to overlook the significant role that contextual information can play. We also study the effect of data split strategies on classifier performance. Based on our experimental results, the paper also offers practical suggestions on how to minimize the effects of temporal concept drift in static datasets during the training of rumor detection methods.
</details>
<details>
<summary>摘要</summary>
一个重要的噱头检测模型特点是其能够总结，特别是检测出现在未知噱头。过去的研究表明，含有媒体文章仅作输入的内容基于噱头检测模型在未看过的噱头上表现较差。同时，叙述基于模型的潜力仍然未得到充分利用。本文的主要贡献在于对内容和叙述基于模型的性能差异进行深入评估，特别是检测新的、未知噱头。我们的实验结果表明，叙述基于模型仍然过分依赖源媒体文章提供的信息，而忽视了Contextual信息的重要作用。我们还研究了数据分裂策略对分类器性能的影响。根据我们的实验结果，文章还提供了实践的建议，以降低在训练噱头检测方法时的时间概念退变的影响。
</details></li>
</ul>
<hr>
<h2 id="SignBank-Multilingual-Sign-Language-Translation-Dataset"><a href="#SignBank-Multilingual-Sign-Language-Translation-Dataset" class="headerlink" title="SignBank+: Multilingual Sign Language Translation Dataset"></a>SignBank+: Multilingual Sign Language Translation Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11566">http://arxiv.org/abs/2309.11566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Moryossef, Zifan Jiang</li>
<li>for: 提高手语机器翻译领域的研究，强调数据质量和翻译系统简化。</li>
<li>methods: 介绍SignBank+数据集，是Optimized for machine translation的纯净版SignBank数据集，并使用简单的文本到文本翻译方法。</li>
<li>results: 评估结果显示，使用SignBank+数据集训练的模型超过原始数据集训练的模型，创造新的benchmark和提供开放资源 для未来研究。<details>
<summary>Abstract</summary>
This work advances the field of sign language machine translation by focusing on dataset quality and simplification of the translation system. We introduce SignBank+, a clean version of the SignBank dataset, optimized for machine translation. Contrary to previous works that employ complex factorization techniques for translation, we advocate for a simplified text-to-text translation approach. Our evaluation shows that models trained on SignBank+ surpass those on the original dataset, establishing a new benchmark and providing an open resource for future research.
</details>
<details>
<summary>摘要</summary>
这个研究提高了手语机器翻译的领域，关注数据集质量和翻译系统简化。我们介绍了SignBank+，一个优化的手语数据集，适用于机器翻译。与前期工作不同，我们主张使用简单的文本到文本翻译方法。我们的评估表明，基于SignBank+的模型比原始数据集模型更高效，创造了新的标准和提供了未来研究的开放资源。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-reinforcement-learning-with-natural-language-subgoals"><a href="#Hierarchical-reinforcement-learning-with-natural-language-subgoals" class="headerlink" title="Hierarchical reinforcement learning with natural language subgoals"></a>Hierarchical reinforcement learning with natural language subgoals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11564">http://arxiv.org/abs/2309.11564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arun Ahuja, Kavya Kopparapu, Rob Fergus, Ishita Dasgupta</li>
<li>for: 这个论文的目的是实现长期行为的目标导向行为，但实现在现实环境中具有挑战。</li>
<li>methods: 这个论文使用了人类数据来软着 Parametrize Goal Space，使用无结构的自然语言来表示这个空间。</li>
<li>results: 该方法比专家复制行为和没有这种监督目标空间的HRL better表现，表明该方法可以结合人类专家监督和奖励学习的优点。<details>
<summary>Abstract</summary>
Hierarchical reinforcement learning has been a compelling approach for achieving goal directed behavior over long sequences of actions. However, it has been challenging to implement in realistic or open-ended environments. A main challenge has been to find the right space of sub-goals over which to instantiate a hierarchy. We present a novel approach where we use data from humans solving these tasks to softly supervise the goal space for a set of long range tasks in a 3D embodied environment. In particular, we use unconstrained natural language to parameterize this space. This has two advantages: first, it is easy to generate this data from naive human participants; second, it is flexible enough to represent a vast range of sub-goals in human-relevant tasks. Our approach outperforms agents that clone expert behavior on these tasks, as well as HRL from scratch without this supervised sub-goal space. Our work presents a novel approach to combining human expert supervision with the benefits and flexibility of reinforcement learning.
</details>
<details>
<summary>摘要</summary>
hierarchical reinforcement learning 是一种吸引人的方法，可以实现长序列动作的目标行为。然而，在真实或开放的环境中实现具有挑战。一个主要挑战是找到适当的下一级目标空间，以实现层次结构。我们提出了一种新的方法，使用人类解决这些任务的数据来软着册这个空间。具体来说，我们使用无结构的自然语言来 parameterize这个空间。这有两个优点：首先，可以轻松地从不熟悉的人参与者中获得这些数据；其次，它够灵活，可以表示人类相关任务中的广泛下一级目标。我们的方法比不同扩展学习的代理人和不带有此协助下一级目标空间的 HRL 表现更好。我们的工作提出了一种结合人类专家指导和强化学习的新方法。
</details></li>
</ul>
<hr>
<h2 id="DreamLLM-Synergistic-Multimodal-Comprehension-and-Creation"><a href="#DreamLLM-Synergistic-Multimodal-Comprehension-and-Creation" class="headerlink" title="DreamLLM: Synergistic Multimodal Comprehension and Creation"></a>DreamLLM: Synergistic Multimodal Comprehension and Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11499">http://arxiv.org/abs/2309.11499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RunpeiDong/DreamLLM">https://github.com/RunpeiDong/DreamLLM</a></li>
<li>paper_authors: Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, Li Yi</li>
<li>for: 本研究旨在开发一种能够生成多modal language model（MLLM），具有较少注意的多modal理解和创造之间的共识。</li>
<li>methods:  DreamLLM 使用两个基本原则：首先，通过直接抽样在原始多modal空间进行语言和图像 posterior 的生成模型化，以避免 CLIP 等外部特征提取器的局限性和信息损失，从而获得更全面的多modal理解。其次， DreamLLM 可以生成 raw 的、混合的文档，包括文本和图像内容，以及无结构的布局，从而学习所有的 conditional、marginal 和 joint 多modal分布。</li>
<li>results: DreamLLM 能够生成免 Training 的多modal通用专家，在多modal总体 экспериментах中表现出色，受益于提高的学习共识。<details>
<summary>Abstract</summary>
This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Generative modeling of both language and image posteriors through direct sampling in the raw multimodal space. This approach bypasses the limitations of external feature extractors like CLIP and enables a more comprehensive understanding of multimodal information.2. Generation of raw, interleaved documents that model both text and image contents, as well as unstructured layouts. This allows DreamLLM to effectively learn all conditional, marginal, and joint multimodal distributions.As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content, demonstrating superior performance as a zero-shot multimodal generalist. Comprehensive experiments highlight the enhanced learning synergy achieved by DreamLLM.</details></li>
</ol>
<hr>
<h2 id="Controlled-Generation-with-Prompt-Insertion-for-Natural-Language-Explanations-in-Grammatical-Error-Correction"><a href="#Controlled-Generation-with-Prompt-Insertion-for-Natural-Language-Explanations-in-Grammatical-Error-Correction" class="headerlink" title="Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction"></a>Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11439">http://arxiv.org/abs/2309.11439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Kaneko, Naoaki Okazaki</li>
<li>for: 这个论文的目的是提出一种名为控制生成（Prompt Insertion，PI）的方法，用于使大型自然语言模型（Large Language Models，LLMs）可以在自然语言中提供对 grammar 和语法错误 corrections 的直接解释。</li>
<li>methods: 这个论文使用了 Large Language Models (LLMs) 和 Prompt Insertion (PI) 方法来生成对 grammar 和语法错误 corrections 的直接解释。</li>
<li>results: 这个研究发现，使用 PI 方法可以使 LLMs 能够直接在自然语言中提供对 grammar 和语法错误 corrections 的解释，并且可以提高对 correction reasons 的生成性能。<details>
<summary>Abstract</summary>
In Grammatical Error Correction (GEC), it is crucial to ensure the user's comprehension of a reason for correction. Existing studies present tokens, examples, and hints as to the basis for correction but do not directly explain the reasons for corrections. Although methods that use Large Language Models (LLMs) to provide direct explanations in natural language have been proposed for various tasks, no such method exists for GEC. Generating explanations for GEC corrections involves aligning input and output tokens, identifying correction points, and presenting corresponding explanations consistently. However, it is not straightforward to specify a complex format to generate explanations, because explicit control of generation is difficult with prompts. This study introduces a method called controlled generation with Prompt Insertion (PI) so that LLMs can explain the reasons for corrections in natural language. In PI, LLMs first correct the input text, and then we automatically extract the correction points based on the rules. The extracted correction points are sequentially inserted into the LLM's explanation output as prompts, guiding the LLMs to generate explanations for the correction points. We also create an Explainable GEC (XGEC) dataset of correction reasons by annotating NUCLE, CoNLL2013, and CoNLL2014. Although generations from GPT-3 and ChatGPT using original prompts miss some correction points, the generation control using PI can explicitly guide to describe explanations for all correction points, contributing to improved performance in generating correction reasons.
</details>
<details>
<summary>摘要</summary>
在语法错误 corrections (GEC) 中，确保用户理解 correction 的理由是关键。现有的研究提供了 tokens、例子和提示，但没有直接解释 correction 的理由。虽然使用 Large Language Models (LLMs) 提供直接解释的自然语言方法已经被提出 для多个任务，但对 GEC 的方法不存在。生成 GEC  corrections 的解释 involves 对输入和输出 tokens 进行对应、确定 correction 点并提供相应的解释。然而，不是 straightforward  specify 复杂的生成格式，因为Explicit 控制生成是 difficult 的。这种研究引入一种名为 controlled generation with Prompt Insertion (PI) 的方法，使得 LLMs 可以通过自然语言来解释 correction 的理由。在 PI 中，LLMs 首先 corrections 输入文本，然后我们自动提取 correction 点基于规则。提取的 correction 点被自动插入 LLMs 的解释输出中作为提示，导引 LLMs 生成对 correction 点的解释。我们还创建了一个 Explainable GEC (XGEC) 数据集，其中包含 correction 理由的注释。虽然 GPT-3 和 ChatGPT 使用原始提示生成的 Generation 缺少一些 correction 点，但使用 PI 的生成控制可以明确指导 LLMs 生成对 correction 点的解释，从而提高生成 correction 理由的性能。
</details></li>
</ul>
<hr>
<h2 id="Kosmos-2-5-A-Multimodal-Literate-Model"><a href="#Kosmos-2-5-A-Multimodal-Literate-Model" class="headerlink" title="Kosmos-2.5: A Multimodal Literate Model"></a>Kosmos-2.5: A Multimodal Literate Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11419">http://arxiv.org/abs/2309.11419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/Kosmos2.5">https://github.com/kyegomez/Kosmos2.5</a></li>
<li>paper_authors: Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, Furu Wei</li>
<li>for: 这个论文是为了开发一种可以读取文本充满图像的机器学习模型，即 Kosmos-2.5。</li>
<li>methods: 该模型采用了多modal文本模型，通过共享转换器架构、任务特定的提示和灵活的文本表示来实现文本识别和文本生成任务。</li>
<li>results: 模型在终到级文档级文本识别和图像到markdown文本生成任务中表现出色，可以适应各种不同的任务，并且可以通过精度微调来适应不同的应用场景。<details>
<summary>Abstract</summary>
We present Kosmos-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. This work also paves the way for the future scaling of multimodal large language models.
</details>
<details>
<summary>摘要</summary>
我们介绍Kosmos-2.5，一种多Modal literate模型，用于机器阅读图像中的文本内容。Kosmos-2.5在两个不同 yet 相互协作的译写任务中表现出色：（1）生成具有空间坐标的文本块，每个文本块在图像中被分配特定的空间坐标；（2）生成符合markdown格式的结构化文本输出。这种多Modal literate能力通过共享Transformer架构、任务特定的提示和灵活文本表示方式实现。我们对Kosmos-2.5进行了端到端文档级文本识别和图像到markdown文本生成的评估。此外，通过精心微调，可以将模型适应不同的提示任务，使其成为实际应用中文本强度图像理解任务的通用工具。此项工作也为未来扩大多Modal大语言模型的前景铺平了路。
</details></li>
</ul>
<hr>
<h2 id="Safurai-001-New-Qualitative-Approach-for-Code-LLM-Evaluation"><a href="#Safurai-001-New-Qualitative-Approach-for-Code-LLM-Evaluation" class="headerlink" title="Safurai 001: New Qualitative Approach for Code LLM Evaluation"></a>Safurai 001: New Qualitative Approach for Code LLM Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11385">http://arxiv.org/abs/2309.11385</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openai/human-eval">https://github.com/openai/human-eval</a></li>
<li>paper_authors: Davide Cifarelli, Leonardo Boiardi, Alessandro Puppo</li>
<li>for: 这个研究旨在开发一种新的大型自然语言模型（LLM），用于编程协助领域。</li>
<li>methods: 这个模型驱动了最新的编程LLM的进步，并且通过数据工程（包括最新的数据转换技术和提示工程）和指令调整来提高性能。</li>
<li>results: 研究表明，Safurai-001可以超越GPT-3.5和WizardCoder在代码可读性方面，提高1.58%和18.78%。<details>
<summary>Abstract</summary>
This paper presents Safurai-001, a new Large Language Model (LLM) with significant potential in the domain of coding assistance. Driven by recent advancements in coding LLMs, Safurai-001 competes in performance with the latest models like WizardCoder [Xu et al., 2023], PanguCoder [Shen et al., 2023] and Phi-1 [Gunasekar et al., 2023] but aims to deliver a more conversational interaction. By capitalizing on the progress in data engineering (including latest techniques of data transformation and prompt engineering) and instruction tuning, this new model promises to stand toe-to-toe with recent closed and open source developments. Recognizing the need for an efficacious evaluation metric for coding LLMs, this paper also introduces GPT4-based MultiParameters, an evaluation benchmark that harnesses varied parameters to present a comprehensive insight into the models functioning and performance. Our assessment shows that Safurai-001 can outperform GPT-3.5 by 1.58% and WizardCoder by 18.78% in the Code Readability parameter and more.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Studying-Lobby-Influence-in-the-European-Parliament"><a href="#Studying-Lobby-Influence-in-the-European-Parliament" class="headerlink" title="Studying Lobby Influence in the European Parliament"></a>Studying Lobby Influence in the European Parliament</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11381">http://arxiv.org/abs/2309.11381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aswin Suresh, Lazar Radojevic, Francesco Salvi, Antoine Magron, Victor Kristof, Matthias Grossglauser</li>
<li>for: 这个研究是为了研究欧洲议会（EP）的法制 процесса中利益集团（ Lobby）的影响。</li>
<li>methods: 这个研究使用自然语言处理（NLP）技术，收集和分析了欧洲议会成员（MEP）的言论和利益集团的 pozition 纸。通过比较这些文本的语义相似性和推论，发现MEP和利益集团之间的可解释的连接。在缺乏ground truth数据的情况下，我们进行了间接验证，比较发现的连接与我们自己 curaated的Retweet链接和公开的MEP会议记录。我们的best方法得到了0.77的AUC分数，与多个基线相比显著性更高。</li>
<li>results: 我们的结果表明，在欧洲议会的法制 процесса中，利益集团对MEP的影响存在可解释的连接。我们对 relate  Lobby 和政治分组的MEP进行了汇总分析，发现与政治分组的意识相符（例如，中间左派组织与社会问题相关）。我们认为这项研究、方法、数据和结果，是为了提高民主机构内复杂决策过程的透明度做出了一步前进。<details>
<summary>Abstract</summary>
We present a method based on natural language processing (NLP), for studying the influence of interest groups (lobbies) in the law-making process in the European Parliament (EP). We collect and analyze novel datasets of lobbies' position papers and speeches made by members of the EP (MEPs). By comparing these texts on the basis of semantic similarity and entailment, we are able to discover interpretable links between MEPs and lobbies. In the absence of a ground-truth dataset of such links, we perform an indirect validation by comparing the discovered links with a dataset, which we curate, of retweet links between MEPs and lobbies, and with the publicly disclosed meetings of MEPs. Our best method achieves an AUC score of 0.77 and performs significantly better than several baselines. Moreover, an aggregate analysis of the discovered links, between groups of related lobbies and political groups of MEPs, correspond to the expectations from the ideology of the groups (e.g., center-left groups are associated with social causes). We believe that this work, which encompasses the methodology, datasets, and results, is a step towards enhancing the transparency of the intricate decision-making processes within democratic institutions.
</details>
<details>
<summary>摘要</summary>
我们提出了基于自然语言处理（NLP）的方法，用于研究欧洲议会（EP）中利益集团（游说者）的影响力。我们收集了和分析了游说者的位置纸和EP议员（MEP）的演讲文本。通过比较这些文本的含义相似性和推导关系，我们能够发现MEP和游说者之间的可读取连接。在没有ground truth datasets的情况下，我们进行了间接验证，比较发现的连接与我们自己curate的推特链接和MEP公开的会议记录。我们的最佳方法在AUC分数0.77达到了，并与多个基eline相比表现出色。此外，我们对发现的连接进行了聚合分析，发现与相关的游说者和政治组织相对应。这种结果与政治组织的意识相符，例如中间左派组织与社会问题相关。我们认为这种方法、数据和结果是推进民主机构内复杂决策过程的一步。
</details></li>
</ul>
<hr>
<h2 id="GECTurk-Grammatical-Error-Correction-and-Detection-Dataset-for-Turkish"><a href="#GECTurk-Grammatical-Error-Correction-and-Detection-Dataset-for-Turkish" class="headerlink" title="GECTurk: Grammatical Error Correction and Detection Dataset for Turkish"></a>GECTurk: Grammatical Error Correction and Detection Dataset for Turkish</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11346">http://arxiv.org/abs/2309.11346</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GGLAB-KU/gecturk">https://github.com/GGLAB-KU/gecturk</a></li>
<li>paper_authors: Atakan Kara, Farrin Marouf Sofian, Andrew Bond, Gözde Gül Şahin</li>
<li>for: 这个论文的目的是提出一种可以生成高质量的同步数据的Synthetic Data Generation Pipeline，用于解决土耳其语自然语言处理 tasks 中的数据缺乏问题。</li>
<li>methods: 这个论文使用了多种复杂的变换函数来实现更 than 20 个专家修改后的语法和拼写规则，并从专业编辑的文章中 derivation 了130,000个高质量的同步句子。</li>
<li>results: 这个论文通过三种基线模型（neural machine translation, sequence tagging, prefix tuning）实现了强大的结果，并通过对各种尘肤数据进行详细的实验来证明了该论文的可重复性和稳定性。<details>
<summary>Abstract</summary>
Grammatical Error Detection and Correction (GEC) tools have proven useful for native speakers and second language learners. Developing such tools requires a large amount of parallel, annotated data, which is unavailable for most languages. Synthetic data generation is a common practice to overcome the scarcity of such data. However, it is not straightforward for morphologically rich languages like Turkish due to complex writing rules that require phonological, morphological, and syntactic information. In this work, we present a flexible and extensible synthetic data generation pipeline for Turkish covering more than 20 expert-curated grammar and spelling rules (a.k.a., writing rules) implemented through complex transformation functions. Using this pipeline, we derive 130,000 high-quality parallel sentences from professionally edited articles. Additionally, we create a more realistic test set by manually annotating a set of movie reviews. We implement three baselines formulating the task as i) neural machine translation, ii) sequence tagging, and iii) prefix tuning with a pretrained decoder-only model, achieving strong results. Furthermore, we perform exhaustive experiments on out-of-domain datasets to gain insights on the transferability and robustness of the proposed approaches. Our results suggest that our corpus, GECTurk, is high-quality and allows knowledge transfer for the out-of-domain setting. To encourage further research on Turkish GEC, we release our datasets, baseline models, and the synthetic data generation pipeline at https://github.com/GGLAB-KU/gecturk.
</details>
<details>
<summary>摘要</summary>
grammatical error detection和修正工具（GEC）对本地语言和第二语言学习者都有用。开发这些工具需要大量并行、注释的数据，但这些数据对大多数语言而言罕见。Synthetic data生成是一种常见的办法来解决这个问题。然而，对于 morphologically rich的语言如土耳其来说，Synthetic data生成并不简单，因为它们的写作规则需要 fonological、morphological和 sintactic信息。在这种情况下，我们提出了一种灵活可扩展的Synthetic data生成管道，可以覆盖More than 20个专家精心编辑的语法和拼写规则（即写作规则），通过复杂的转换函数来实现。通过这种管道，我们得到了130,000个高质量的并行句子，并创建了一个更真实的测试集，通过手动注释一些电影评论。我们实现了三种基线，即 neural machine translation、sequence tagging 和 prefix tuning with a pretrained decoder-only model，取得了出色的结果。此外，我们进行了详细的对out-of-domain数据集的实验，以了解提案方法的传输性和稳定性。我们的结果表明，我们的句子库，GECTurk，具有高质量，并允许知识传输到out-of-domain Setting。为了促进土耳其GEC的研究，我们在https://github.com/GGLAB-KU/gecturk上发布了我们的数据集、基线模型和Synthetic data生成管道。
</details></li>
</ul>
<hr>
<h2 id="Improving-Article-Classification-with-Edge-Heterogeneous-Graph-Neural-Networks"><a href="#Improving-Article-Classification-with-Edge-Heterogeneous-Graph-Neural-Networks" class="headerlink" title="Improving Article Classification with Edge-Heterogeneous Graph Neural Networks"></a>Improving Article Classification with Edge-Heterogeneous Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11341">http://arxiv.org/abs/2309.11341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lyvykhang/edgehetero-nodeproppred">https://github.com/lyvykhang/edgehetero-nodeproppred</a></li>
<li>paper_authors: Khang Ly, Yury Kashnitsky, Savvas Chamezopoulos, Valeria Krzhizhanovskaya</li>
<li>for: 这个研究旨在提高文章分类的性能，使用简单的图 neural network (GNN) 拓扑，并将文章中的文本metadata 转化为高级 semantics。</li>
<li>methods: 该研究使用 SciBERT 生成节点特征，以捕捉文章中的高级 semantics。然后，使用完全supervised 推学的node classification 进行实验，使用 Open Graph Benchmark (OGB) ogbn-arxiv 数据集和 PubMed 肥瘤数据集。</li>
<li>results: 结果表明，使用edge-heterogeneous graphs 可以提高 GNN 模型的性能，而且可以使用简单和浅的 GNN 拓扑来达到与更复杂的结构相同的性能。在 OGB 竞赛中，我们获得了第15名的成绩（准确率 74.61%），并在 PubMed 数据集上与 state-of-the-art GNN 结构相当（准确率 89.88%）。<details>
<summary>Abstract</summary>
Classifying research output into context-specific label taxonomies is a challenging and relevant downstream task, given the volume of existing and newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Networks (GNN) pipelines with edge-heterogeneous graph representations. SciBERT is used for node feature generation to capture higher-order semantics within the articles' textual metadata. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate that edge-heterogeneous graphs consistently improve the performance of all GNN models compared to the edge-homogeneous graphs. The transformed data enable simple and shallow GNN pipelines to achieve results on par with more complex architectures. On ogbn-arxiv, we achieve a top-15 result in the OGB competition with a 2-layer GCN (accuracy 74.61%), being the highest-scoring solution with sub-1 million parameters. On PubMed, we closely trail SOTA GNN architectures using a 2-layer GraphSAGE by including additional co-authorship edges in the graph (accuracy 89.88%). The implementation is available at: $\href{https://github.com/lyvykhang/edgehetero-nodeproppred}{\text{https://github.com/lyvykhang/edgehetero-nodeproppred}$.
</details>
<details>
<summary>摘要</summary>
classe research output into context-specific label taxonomies 是一个复杂且有 relevance 的下游任务， giventhe volume of existing 和 newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Networks (GNN) pipelines with edge-heterogeneous graph representations. SciBERT is used for node feature generation to capture higher-order semantics within the articles' textual metadata. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate that edge-heterogeneous graphs consistently improve the performance of all GNN models compared to the edge-homogeneous graphs. The transformed data enable simple and shallow GNN pipelines to achieve results on par with more complex architectures. On ogbn-arxiv, we achieve a top-15 result in the OGB competition with a 2-layer GCN (accuracy 74.61%), being the highest-scoring solution with sub-1 million parameters. On PubMed, we closely trail SOTA GNN architectures using a 2-layer GraphSAGE by including additional co-authorship edges in the graph (accuracy 89.88%). The implementation is available at: $\href{https://github.com/lyvykhang/edgehetero-nodeproppred}{\text{https://github.com/lyvykhang/edgehetero-nodeproppred}$.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Data-Collection-and-Unsupervised-Learning-for-Code-switched-Tunisian-Arabic-Automatic-Speech-Recognition"><a href="#Leveraging-Data-Collection-and-Unsupervised-Learning-for-Code-switched-Tunisian-Arabic-Automatic-Speech-Recognition" class="headerlink" title="Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition"></a>Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11327">http://arxiv.org/abs/2309.11327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Amine Ben Abdallah, Ata Kabboudi, Amir Kanoun, Salah Zaiem</li>
<li>for: This paper is written for the purpose of developing an effective Automatic Speech Recognition (ASR) solution for dialects, specifically focusing on the Tunisian dialect.</li>
<li>methods: The paper explores self-supervision, semi-supervision, and few-shot code-switching approaches to improve the state-of-the-art in ASR for Tunisian Arabic, English, and French.</li>
<li>results: The paper produces human evaluations of transcripts to avoid the noise coming from spelling inadequacies in testing references, and the models are able to transcribe audio samples in a linguistic mix involving Tunisian Arabic, English, and French. The data used during training and testing are released for public use and further improvements.<details>
<summary>Abstract</summary>
Crafting an effective Automatic Speech Recognition (ASR) solution for dialects demands innovative approaches that not only address the data scarcity issue but also navigate the intricacies of linguistic diversity. In this paper, we address the aforementioned ASR challenge, focusing on the Tunisian dialect. First, textual and audio data is collected and in some cases annotated. Second, we explore self-supervision, semi-supervision and few-shot code-switching approaches to push the state-of-the-art on different Tunisian test sets; covering different acoustic, linguistic and prosodic conditions. Finally, and given the absence of conventional spelling, we produce a human evaluation of our transcripts to avoid the noise coming from spelling inadequacies in our testing references. Our models, allowing to transcribe audio samples in a linguistic mix involving Tunisian Arabic, English and French, and all the data used during training and testing are released for public use and further improvements.
</details>
<details>
<summary>摘要</summary>
制定一个有效的自动语音识别（ASR）解决方案 для方言需要创新的方法，不仅解决数据缺乏问题，还能够探索方言语言多样性的细节。在这篇论文中，我们关注了前述的ASR挑战，将着眼点在突尼斯方言。首先，我们收集了文本和音频数据，并在某些情况下进行了标注。其次，我们探索了无监督、半监督和少量代码交换的方法，以在不同的突尼斯测试集上提高状态。这些测试集涵盖了不同的听音、语言和语调条件。最后，由于没有传统的拼写法，我们进行了人工评估我们的讲文，以避免测试参考中的杂音。我们的模型可以将突尼斯阿拉伯语、英语和法语混合的语音样本转录为文本，并在训练和测试中使用的所有数据都公开发布，以便进一步的改进。
</details></li>
</ul>
<hr>
<h2 id="DISC-LawLLM-Fine-tuning-Large-Language-Models-for-Intelligent-Legal-Services"><a href="#DISC-LawLLM-Fine-tuning-Large-Language-Models-for-Intelligent-Legal-Services" class="headerlink" title="DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services"></a>DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11325">http://arxiv.org/abs/2309.11325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Wei Lin, Xuanjing Huang, Zhongyu Wei</li>
<li>for: 这个论文是为了提供一种智能法律系统，使得法律服务可以更加全面和智能化。</li>
<li>methods: 论文使用大量语言模型（LLMs），并采用法律逻辑提示策略来构建监督精度训练集。同时，文章还增强了模型的知识访问和利用能力。</li>
<li>results: 论文通过对DISC-Law-Eval测试集进行量化和资深评价， demonstarted了其在不同的法律场景中的效果。详细的资源可以在<a target="_blank" rel="noopener" href="https://github.com/FudanDISC/DISC-LawLLM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/FudanDISC/DISC-LawLLM上找到。</a><details>
<summary>Abstract</summary>
We propose DISC-LawLLM, an intelligent legal system utilizing large language models (LLMs) to provide a wide range of legal services. We adopt legal syllogism prompting strategies to construct supervised fine-tuning datasets in the Chinese Judicial domain and fine-tune LLMs with legal reasoning capability. We augment LLMs with a retrieval module to enhance models' ability to access and utilize external legal knowledge. A comprehensive legal benchmark, DISC-Law-Eval, is presented to evaluate intelligent legal systems from both objective and subjective dimensions. Quantitative and qualitative results on DISC-Law-Eval demonstrate the effectiveness of our system in serving various users across diverse legal scenarios. The detailed resources are available at https://github.com/FudanDISC/DISC-LawLLM.
</details>
<details>
<summary>摘要</summary>
我们提出了DISC-LawLLM，一种智能法律系统，使用大型自然语言模型（LLM）提供广泛的法律服务。我们采用法律逻辑提示策略构建监督精度训练集，在中国司法领域进行超参数 fine-tuning，以提高模型的法律推理能力。我们将LLM加载一个检索模块，以提高模型对外部法律知识的访问和利用能力。我们提供了一个全面的法律评价指标，DISC-Law-Eval，以评估智能法律系统的效果从客观和主观两个角度。我们对DISC-Law-Eval进行了量化和质量的测试，结果表明我们的系统在多种法律场景下可以为用户提供有效的服务。详细的资源可以在https://github.com/FudanDISC/DISC-LawLLM上找到。
</details></li>
</ul>
<hr>
<h2 id="The-Wizard-of-Curiosities-Enriching-Dialogues-with-Fun-Facts"><a href="#The-Wizard-of-Curiosities-Enriching-Dialogues-with-Fun-Facts" class="headerlink" title="The Wizard of Curiosities: Enriching Dialogues with Fun Facts"></a>The Wizard of Curiosities: Enriching Dialogues with Fun Facts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11283">http://arxiv.org/abs/2309.11283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederico Vicente, Rafael Ferreira, David Semedo, João Magalhães</li>
<li>for: 本研究旨在增加对对话系统的用户体验，通过引入吸引人的启示。</li>
<li>methods: 本研究使用了来自Amazon Alexa TaskBot挑战的真实人机对话，并对这些对话进行了精心准备和编辑，以创造一组具有Contextualized curiosities的对话。</li>
<li>results: 根据对Over 1000对话的A&#x2F;B测试表明，启示可以不 только增加用户参与度，还提高用户的平均相对评价值9.7%。<details>
<summary>Abstract</summary>
Introducing curiosities in a conversation is a way to teach something new to the person in a pleasant and enjoyable way. Enriching dialogues with contextualized curiosities can improve the users' perception of a dialog system and their overall user experience. In this paper, we introduce a set of curated curiosities, targeting dialogues in the cooking and DIY domains. In particular, we use real human-agent conversations collected in the context of the Amazon Alexa TaskBot challenge, a multimodal and multi-turn conversational setting. According to an A/B test with over 1000 conversations, curiosities not only increase user engagement, but provide an average relative rating improvement of 9.7%.
</details>
<details>
<summary>摘要</summary>
在对话中引入curiosities是一种教育用户新知识的有趣和愉悦的方式。在对话中添加上下文化curiosities可以提高对对话系统的评估和用户总体体验。在这篇论文中，我们介绍了一个 curae的curiosities集合，targeting cooking和DIY对话。特别是，我们使用了来自Amazon Alexa TaskBot挑战的真实人机对话收集，一种多媒体和多turn对话Setting。据A/B测试，curiosities不仅提高了用户参与度，还提供了9.7%的相对评分提升。
</details></li>
</ul>
<hr>
<h2 id="The-Scenario-Refiner-Grounding-subjects-in-images-at-the-morphological-level"><a href="#The-Scenario-Refiner-Grounding-subjects-in-images-at-the-morphological-level" class="headerlink" title="The Scenario Refiner: Grounding subjects in images at the morphological level"></a>The Scenario Refiner: Grounding subjects in images at the morphological level</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11252">http://arxiv.org/abs/2309.11252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claudia Tagliaferri, Sofia Axioti, Albert Gatt, Denis Paperno</li>
<li>for: 这个论文是用来检验语言模型是否能够捕捉语言中的微妙特征的。</li>
<li>methods: 该论文使用了一种新的方法和数据集来测试语言模型是否能够在 morphological 水平上捕捉语言中的差异。</li>
<li>results: 研究发现，语言模型的预测与人类参与者的判断存在差异，尤其是在 grammatical 方面存在偏向。<details>
<summary>Abstract</summary>
Derivationally related words, such as "runner" and "running", exhibit semantic differences which also elicit different visual scenarios. In this paper, we ask whether Vision and Language (V\&L) models capture such distinctions at the morphological level, using a a new methodology and dataset. We compare the results from V\&L models to human judgements and find that models' predictions differ from those of human participants, in particular displaying a grammatical bias. We further investigate whether the human-model misalignment is related to model architecture. Our methodology, developed on one specific morphological contrast, can be further extended for testing models on capturing other nuanced language features.
</details>
<details>
<summary>摘要</summary>
derivationally related words, such as "runner" and "running", exhibit semantic differences which also elicit different visual scenarios. In this paper, we ask whether Vision and Language (V&L) models capture such distinctions at the morphological level, using a new methodology and dataset. We compare the results from V&L models to human judgements and find that models' predictions differ from those of human participants, in particular displaying a grammatical bias. We further investigate whether the human-model misalignment is related to model architecture. Our methodology, developed on one specific morphological contrast, can be further extended for testing models on capturing other nuanced language features.Here's the translation in Traditional Chinese as well: derivationally related words, such as "runner" and "running", exhibit semantic differences which also elicit different visual scenarios. In this paper, we ask whether Vision and Language (V&L) models capture such distinctions at the morphological level, using a new methodology and dataset. We compare the results from V&L models to human judgements and find that models' predictions differ from those of human participants, in particular displaying a grammatical bias. We further investigate whether the human-model misalignment is related to model architecture. Our methodology, developed on one specific morphological contrast, can be further extended for testing models on capturing other nuanced language features.
</details></li>
</ul>
<hr>
<h2 id="OpenChat-Advancing-Open-source-Language-Models-with-Mixed-Quality-Data"><a href="#OpenChat-Advancing-Open-source-Language-Models-with-Mixed-Quality-Data" class="headerlink" title="OpenChat: Advancing Open-source Language Models with Mixed-Quality Data"></a>OpenChat: Advancing Open-source Language Models with Mixed-Quality Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11235">http://arxiv.org/abs/2309.11235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imoneoi/openchat">https://github.com/imoneoi/openchat</a></li>
<li>paper_authors: Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu</li>
<li>for: 这个论文是为了提高开源语言模型的性能而写的，特别是使用混合质量数据进行超参数调整。</li>
<li>methods: 该论文提出了一种名为 OpenChat 的框架，用于使用混合质量数据进行超参数调整，并使用 Conditioned-Reinforcement Learning Fine-Tuning (C-RLFT) 方法。</li>
<li>results: 该论文的实验表明，使用 OpenChat 框架和 C-RLFT 方法可以提高开源语言模型的性能，并且在三个标准的 bencmark 上 achieved the highest average performance 中。<details>
<summary>Abstract</summary>
Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat.
</details>
<details>
<summary>摘要</summary>
现在，开源大语言模型如LLaMA已经出现。最近的发展包括监督精细调教（SFT）和奖励学习调教（RLFT），以使模型与人类目标 better alignment。然而，SFT方法将所有训练数据视为一样的质量，而RLFT方法需要高质量的对数据进行对比或排名。在这种研究中，我们提出了一种新的框架，名为OpenChat，以提高开源语言模型的质量。 Specifically，我们考虑了通用的SFT训练数据，包括一小量的专家数据和大量的不优化数据，无需任何偏好标签。我们提议了C（条件）-RLFT，它将不同的数据来源视为粗粒化奖励标签，并学习一个类别 Conditioned 策略，以利用不同数据质量信息。有趣的是，C-RLFT 的优化策略可以通过单阶段、RL-free 监督学习，以轻量级和避免高昂的人类偏好标签。经过广泛的实验，我们的 openchat-13b 通过 C-RLFT 进行微调，在三个标准 bench mark 上 achieve 所有 13b 开源语言模型的最高平均性能。此外，我们使用 AGIEval 验证模型的通用性能，只有 openchat-13b 在基础模型之上超越。最后，我们进行了一系列的分析，以证明 OpenChat 的效果和可靠性。我们的代码、数据和模型都可以在 https://github.com/imoneoi/openchat 上获取。
</details></li>
</ul>
<hr>
<h2 id="Speak-While-You-Think-Streaming-Speech-Synthesis-During-Text-Generation"><a href="#Speak-While-You-Think-Streaming-Speech-Synthesis-During-Text-Generation" class="headerlink" title="Speak While You Think: Streaming Speech Synthesis During Text Generation"></a>Speak While You Think: Streaming Speech Synthesis During Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11210">http://arxiv.org/abs/2309.11210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avihu Dekel, Slava Shechtman, Raul Fernandez, David Haws, Zvi Kons, Ron Hoory</li>
<li>for: 这篇论文是为了解决大语言模型（LLM）的交互问题，使得LLM可以更加流畅地进行语音交互。</li>
<li>methods: 该论文提出了一种名为LM2Speech的架构，它可以在文本生成过程中同时生成语音，从而减少了对话延迟。LM2Speech模仿了一个非流动教师模型的预测，同时限制了未来上下文的暴露，以便实现流动。它利用了LLM的隐藏嵌入，这是文本生成过程中的一个侧产品，它含有有用的语义上下文。</li>
<li>results: 实验结果表明，LM2Speech可以保持教师模型的质量，同时减少对话延迟，以便实现自然的语音交互。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) demonstrate impressive capabilities, yet interaction with these models is mostly facilitated through text. Using Text-To-Speech to synthesize LLM outputs typically results in notable latency, which is impractical for fluent voice conversations. We propose LLM2Speech, an architecture to synthesize speech while text is being generated by an LLM which yields significant latency reduction. LLM2Speech mimics the predictions of a non-streaming teacher model while limiting the exposure to future context in order to enable streaming. It exploits the hidden embeddings of the LLM, a by-product of the text generation that contains informative semantic context. Experimental results show that LLM2Speech maintains the teacher's quality while reducing the latency to enable natural conversations.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）显示出很强的能力，然而与这些模型交互通常是通过文本进行的。使用文本到语音synthesize LLM输出通常会导致很长的延迟，这对于流畅的语音对话不实用。我们提议LLM2Speech，一种架构可以在文本生成过程中同时synthesize语音，从而减少延迟。LLM2Speech模仿教师模型的预测，限制未来上下文的暴露，以便实现流动。它利用LLM的隐藏嵌入，这是文本生成过程的产物，含有有用的semanticContext。实验结果表明，LLM2Speech可以保持教师的质量，同时减少延迟，以便实现自然的对话。
</details></li>
</ul>
<hr>
<h2 id="The-Languini-Kitchen-Enabling-Language-Modelling-Research-at-Different-Scales-of-Compute"><a href="#The-Languini-Kitchen-Enabling-Language-Modelling-Research-at-Different-Scales-of-Compute" class="headerlink" title="The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute"></a>The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11197">http://arxiv.org/abs/2309.11197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aleksandar Stanić, Dylan Ashley, Oleg Serikov, Louis Kirsch, Francesco Faccio, Jürgen Schmidhuber, Thomas Hofmann, Imanol Schlag</li>
<li>for:  This paper aims to provide a fair comparison of language modeling methods based on their empirical scaling trends, and to serve as a foundation for meaningful and reproducible research in the field.</li>
<li>methods: The paper introduces an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours, and uses a pre-processed dataset of books to evaluate the methods.</li>
<li>results: The paper shows that the LSTM baseline exhibits a predictable and more favourable scaling law than the GPT baseline, and that the two models intersect at roughly 50,000 accelerator hours.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文的目的是为语言模型比较提供公平的比较基础，并为语言模型研究提供可重复的基础。</li>
<li>methods: 论文提出了一种实验协议，使得模型比较基于等效计算时间（ measured in accelerator hours）进行。为了评价方法，文章使用了一个已经处理过的大型、多样化、高质量的书籍数据集。</li>
<li>results: 论文显示，LSTM基eline在计算时间上采取了一种可预测的和更有利的整体增长规律，而GPT基eline在所有等效计算时间水平上都保持了更好的折衣率。两个基eline在约50,000个加速器小时上交叉。<details>
<summary>Abstract</summary>
The Languini Kitchen serves as both a research collective and codebase designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modelling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and a recurrent model in the form of a novel LSTM with ten-fold throughput. While the GPT baseline achieves better perplexity throughout all our levels of compute, our LSTM baseline exhibits a predictable and more favourable scaling law. This is due to the improved throughput and the need for fewer training tokens to achieve the same decrease in test perplexity. Extrapolating the scaling laws leads of both models results in an intersection at roughly 50,000 accelerator hours. We hope this work can serve as the foundation for meaningful and reproducible language modelling research.
</details>
<details>
<summary>摘要</summary>
蓝夷面厨房 serves as both a research collective and codebase, designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modeling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and a recurrent model in the form of a novel LSTM with ten-fold throughput. While the GPT baseline achieves better perplexity throughout all our levels of compute, our LSTM baseline exhibits a predictable and more favourable scaling law. This is due to the improved throughput and the need for fewer training tokens to achieve the same decrease in test perplexity. Extrapolating the scaling laws leads of both models results in an intersection at roughly 50,000 accelerator hours. We hope this work can serve as the foundation for meaningful and reproducible language modeling research.
</details></li>
</ul>
<hr>
<h2 id="Assessment-of-Pre-Trained-Models-Across-Languages-and-Grammars"><a href="#Assessment-of-Pre-Trained-Models-Across-Languages-and-Grammars" class="headerlink" title="Assessment of Pre-Trained Models Across Languages and Grammars"></a>Assessment of Pre-Trained Models Across Languages and Grammars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11165">http://arxiv.org/abs/2309.11165</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amunozo/multilingual-assessment">https://github.com/amunozo/multilingual-assessment</a></li>
<li>paper_authors: Alberto Muñoz-Ortiz, David Vilares, Carlos Gómez-Rodríguez</li>
<li>for: 这个研究是为了评估多语言大型自然语言处理器（LLMs）如何学习语法结构。</li>
<li>methods: 该研究使用了抽象到多形式语法结构的方法，包括将解析视为序列标签。</li>
<li>results: 研究发现：（一）框架在不同编码下具有一致性，（二）预训练词词 vectors 不会偏好语法树表示于dependency表示，（三）使用字符串分词是需要表示语法结构的，与字符串模型不同，（四）语言出现在预训练数据中的频率比任务数据更重要于从词词 vectors 中恢复语法结构。<details>
<summary>Abstract</summary>
We present an approach for assessing how multilingual large language models (LLMs) learn syntax in terms of multi-formalism syntactic structures. We aim to recover constituent and dependency structures by casting parsing as sequence labeling. To do so, we select a few LLMs and study them on 13 diverse UD treebanks for dependency parsing and 10 treebanks for constituent parsing. Our results show that: (i) the framework is consistent across encodings, (ii) pre-trained word vectors do not favor constituency representations of syntax over dependencies, (iii) sub-word tokenization is needed to represent syntax, in contrast to character-based models, and (iv) occurrence of a language in the pretraining data is more important than the amount of task data when recovering syntax from the word vectors.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于评估多语言大型自然语言处理器（LLM）在多形式语法结构中学习语法的方式。我们希望通过将分析转换为序列标签来恢复句子和依赖结构。为此，我们选择了一些LLM并对13种UD treebanks进行了依赖分析和10种treebanks进行了句子分析。我们的结果表明：（i）框架在不同编码中具有一致性，（ii）预训练词词 vec 不倾向于 syntax 中的句子表示，（iii）字符串分词是必要的，而不是字符串模型，以表示语法，（iv）预training数据中语言的出现次数高于任务数据时，可以更好地从词 vectors 中恢复语法。
</details></li>
</ul>
<hr>
<h2 id="Prototype-of-a-robotic-system-to-assist-the-learning-process-of-English-language-with-text-generation-through-DNN"><a href="#Prototype-of-a-robotic-system-to-assist-the-learning-process-of-English-language-with-text-generation-through-DNN" class="headerlink" title="Prototype of a robotic system to assist the learning process of English language with text-generation through DNN"></a>Prototype of a robotic system to assist the learning process of English language with text-generation through DNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11142">http://arxiv.org/abs/2309.11142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Morales-Torres, Mario Campos-Soberanis, Diego Campos-Sobrino</li>
<li>for: 这个论文是为了帮助英语自学者提高英语水平的。</li>
<li>methods: 这个论文使用了Long Short Term Memory（LSTM）神经网络来生成文本，learners通过图形用户界面与系统互动，系统根据学生的英语水平生成文本。</li>
<li>results: 实验结果显示，learners与系统互动后，他们的 grammatical Range 有所提高。<details>
<summary>Abstract</summary>
In the last ongoing years, there has been a significant ascending on the field of Natural Language Processing (NLP) for performing multiple tasks including English Language Teaching (ELT). An effective strategy to favor the learning process uses interactive devices to engage learners in their self-learning process. In this work, we present a working prototype of a humanoid robotic system to assist English language self-learners through text generation using Long Short Term Memory (LSTM) Neural Networks. The learners interact with the system using a Graphic User Interface that generates text according to the English level of the user. The experimentation was conducted using English learners and the results were measured accordingly to International English Language Testing System (IELTS) rubric. Preliminary results show an increment in the Grammatical Range of learners who interacted with the system.
</details>
<details>
<summary>摘要</summary>
最近几年来，自然语言处理（NLP）领域内，有许多进展，以帮助执行多种任务，包括英语教学（ELT）。一种有效的策略是使用互动设备，以吸引学生参与自学习过程。在这个工作中，我们展示了一个人工智能机器人系统，用于帮助英语自学者通过文本生成来提高英语水平。学生通过图形用户界面与系统进行交互，系统根据用户的英语水平生成文本。实验中使用了英语学习者，并根据国际英语语言考试系统（IELTS）标准进行评估结果。初步结果表明，与系统交互的学生的 grammatical range 有所增加。
</details></li>
</ul>
<hr>
<h2 id="K-pop-Lyric-Translation-Dataset-Analysis-and-Neural-Modelling"><a href="#K-pop-Lyric-Translation-Dataset-Analysis-and-Neural-Modelling" class="headerlink" title="K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling"></a>K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11093">http://arxiv.org/abs/2309.11093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haven Kim, Jongmin Jung, Dasaem Jeong, Juhan Nam</li>
<li>for: 本研究的目的是为了推广歌曲翻译研究的范围和语言，并对K-pop歌曲翻译进行系统性的研究。</li>
<li>methods: 本研究使用了一个新的歌词翻译 dataset，该 dataset包含了大约89%的K-pop歌曲歌词，并将韩语和英语歌词进行了行内和段内的对应。</li>
<li>results: 本研究发现了K-pop歌曲翻译的独特特征，与其他已经广泛研究的类型不同，同时还构建了一个基于神经网络的歌词翻译模型，从而证明了专门为歌曲翻译而设计的 dataset 的重要性。<details>
<summary>Abstract</summary>
Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for singable lyric translations.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable "Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for singable lyric translations."中文翻译：学术界对歌词翻译一百年来进行研究，现在吸引了计算语言学研究者。我们认为前一代研究存在两个限制：首先，歌词翻译研究主要集中在西方类型和语言上，尚未对K-pop进行过研究，尽管其受欢迎程度极高。其次，歌词翻译领域缺乏公共可用数据集，到我们所知，没有这样的数据集存在。为了扩大歌词翻译研究的类型和语言范围，我们介绍了一个新的可唱歌词翻译数据集，其中大约89%是K-pop歌曲 lyrics。这个数据集将韩语和英语歌词一行一行、段段对齐。我们利用了这个数据集，揭示了K-pop歌词翻译的独特特征，与其他广泛研究的类型区分开来，并构建了神经网络歌词翻译模型，从而强调了专门为可唱歌词翻译而设置的数据集的重要性。
</details></li>
</ul>
<hr>
<h2 id="Dual-Modal-Attention-Enhanced-Text-Video-Retrieval-with-Triplet-Partial-Margin-Contrastive-Learning"><a href="#Dual-Modal-Attention-Enhanced-Text-Video-Retrieval-with-Triplet-Partial-Margin-Contrastive-Learning" class="headerlink" title="Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning"></a>Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11082">http://arxiv.org/abs/2309.11082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Jiang, Hong Liu, Xuzheng Yu, Qing Wang, Yuan Cheng, Jia Xu, Zhongyi Liu, Qingpei Guo, Wei Chu, Ming Yang, Yuan Qi</li>
<li>for: This paper focuses on improving text-video retrieval, which is essential for video filtering, recommendation, and search, due to the increasing amount of web videos.</li>
<li>methods: The paper proposes two novel techniques to improve contrastive learning for text-video retrieval: 1) Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs, and 2) Triplet Partial Margin Contrastive Learning (TPM-CL) module to construct partial order triplet samples.</li>
<li>results: The proposed approach outperforms existing methods on four widely-used text-video retrieval datasets, including MSR-VTT, MSVD, DiDeMo, and ActivityNet.Here’s the simplified Chinese text in the format you requested:</li>
<li>for: 这篇论文关注提高文本视频相似性 retrieval，用于视频过滤、推荐和搜索，由于网络视频的快速增长。</li>
<li>methods: 该论文提出了两种新的技术来提高对文本视频相似性的探索：1）双Modal Attention-Enhanced Module (DMAE) 来挖掘困难的负例，2）Triplet Partial Margin Contrastive Learning (TPM-CL) 模块来构建partial order triplet samples。</li>
<li>results: 该提出的方法在四个常用的文本视频相似性数据集上（MSR-VTT、MSVD、DiDeMo、ActivityNet）得到了较高的性能，比如 existed 方法。<details>
<summary>Abstract</summary>
In recent years, the explosion of web videos makes text-video retrieval increasingly essential and popular for video filtering, recommendation, and search. Text-video retrieval aims to rank relevant text/video higher than irrelevant ones. The core of this task is to precisely measure the cross-modal similarity between texts and videos. Recently, contrastive learning methods have shown promising results for text-video retrieval, most of which focus on the construction of positive and negative pairs to learn text and video representations. Nevertheless, they do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity. To address these two issues, this paper improves contrastive learning using two novel techniques. First, to exploit hard examples for robust discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs from textual and visual clues. By further introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively identify all these hard negatives and explicitly highlight their impacts in the training loss. Second, our work argues that triplet samples can better model fine-grained semantic similarity compared to pairwise samples. We thereby present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to construct partial order triplet samples by automatically generating fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL designs an adaptive token masking strategy with cross-modal interaction to model subtle semantic differences. Extensive experiments demonstrate that the proposed approach outperforms existing methods on four widely-used text-video retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.
</details>
<details>
<summary>摘要</summary>
近年来，Web视频的爆炸式增长使得文本视频检索变得越来越重要和受欢迎，用于视频筛选、推荐和搜索。文本视频检索的目标是将相关的文本和视频排名在不相关的文本和视频之前。核心任务是准确度量文本和视频之间的跨Modal相似性。在这个任务中，对照学习方法已经取得了显著成果，大多数方法都是通过建立正例和反例来学习文本和视频表示。然而，这些方法往往忽略硬例和不同水平的 semantic similarity。为了解决这两个问题，本文提出了两种新的技术：首先，我们提出了一种双Modal注意力增强模块（DMAE），以挖掘文本和视频中的硬例。其次，我们引入了一种Negative-aware InfoNCE（NegNCE）损失函数，以适应性地标识和特别强调硬例的影响。其次，我们 argue that triplet samples可以更好地模型细致的 semantic similarity，而不是pairwise samples。我们因此提出了一种新的Triplet Partial Margin Contrastive Learning（TPM-CL）模块，通过自动生成匹配的文本视频对的硬例来建立 partial order triplet samples。TPM-CL模块还设计了一种自适应的token掩码策略，以模型文本和视频之间的跨Modal差异。经过广泛的实验，我们发现，提出的方法在四个常用的文本视频检索数据集上都能够达到更高的性能。
</details></li>
</ul>
<hr>
<h2 id="UniPCM-Universal-Pre-trained-Conversation-Model-with-Task-aware-Automatic-Prompt"><a href="#UniPCM-Universal-Pre-trained-Conversation-Model-with-Task-aware-Automatic-Prompt" class="headerlink" title="UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt"></a>UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11065">http://arxiv.org/abs/2309.11065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cycrab/unipcm">https://github.com/cycrab/unipcm</a></li>
<li>paper_authors: Yucheng Cai, Wentao Ma, Yuchuan Wu, Shuzheng Si, Yuan Shao, Zhijian Ou, Yongbin Li</li>
<li>for: 本研究旨在提高对话系统模型的Robustness和传输能力，通过多任务预训练。</li>
<li>methods: 本研究使用Task-based Automatic Prompt generation（TAP）自动生成高质量的提示。</li>
<li>results: 通过使用高质量的提示，我们扩展了对话系统模型的训练数据集至122个任务，并实现了对多种对话任务和不同的对话系统的优秀表现。<details>
<summary>Abstract</summary>
Recent research has shown that multi-task pre-training greatly improves the model's robustness and transfer ability, which is crucial for building a high-quality dialog system. However, most previous works on multi-task pre-training rely heavily on human-defined input format or prompt, which is not optimal in quality and quantity. In this work, we propose to use Task-based Automatic Prompt generation (TAP) to automatically generate high-quality prompts. Using the high-quality prompts generated, we scale the corpus of the pre-trained conversation model to 122 datasets from 15 dialog-related tasks, resulting in Universal Pre-trained Conversation Model (UniPCM), a powerful foundation model for various conversational tasks and different dialog systems. Extensive experiments have shown that UniPCM is robust to input prompts and capable of various dialog-related tasks. Moreover, UniPCM has strong transfer ability and excels at low resource scenarios, achieving SOTA results on 9 different datasets ranging from task-oriented dialog to open-domain conversation. Furthermore, we are amazed to find that TAP can generate prompts on par with those collected with crowdsourcing. The code is released with the paper.
</details>
<details>
<summary>摘要</summary>
近期研究表明，多任务预训练可以大幅提高模型的Robustness和传递能力，这是建立高质量对话系统的关键。然而，大多数前一些工作中的多任务预训练都依赖于人类定义的输入格式或提示，这并不是最佳的质量和量。在这项工作中，我们提议使用任务基本Prompt生成（TAP）自动生成高质量提示。使用生成的高质量提示，我们扩展了预训练对话模型的训练数据集，达到了122个对话相关任务的规模，并命名为Universal Pre-trained Conversation Model（UniPCM）。广泛的实验表明，UniPCM具有输入提示的Robustness和多种对话任务的能力。此外，UniPCM在资源不足的情况下表现出色，在9个不同任务上达到了SOTA的结果，从任务型对话到开放领域对话。此外，我们发现TAP可以生成与人类收集的提示相当的提示。代码随着论文一起发布。
</details></li>
</ul>
<hr>
<h2 id="XATU-A-Fine-grained-Instruction-based-Benchmark-for-Explainable-Text-Updates"><a href="#XATU-A-Fine-grained-Instruction-based-Benchmark-for-Explainable-Text-Updates" class="headerlink" title="XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates"></a>XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11063">http://arxiv.org/abs/2309.11063</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/megagonlabs/xatu">https://github.com/megagonlabs/xatu</a></li>
<li>paper_authors: Haopeng Zhang, Hayate Iso, Sairam Gurajada, Nikita Bhutani</li>
<li>for: 这个论文旨在检验大语言模型的文本编辑能力，并提供一个新的、细化的指令基于的文本编辑benchmark。</li>
<li>methods: 本论文使用了新的benchmark，叫做XATU，它包括了各种话题和文本类型，并且包含了lexical、syntactic、semantic和knowledge-intensive的编辑指令。以提高可读性，这个benchmark使用了高质量的数据源和人工标注，以获得细化的编辑指令和金标准的编辑解释。</li>
<li>results: 通过对现有的开放和关闭大语言模型进行评估，本论文示出了 instrucion tuning 的效果和不同架构下的编辑任务的影响。此外，广泛的实验还表明了对文本编辑任务的细化解释的重要性。<details>
<summary>Abstract</summary>
Text editing is a crucial task that involves modifying text to better align with user intents. However, existing text editing benchmark datasets have limitations in providing only coarse-grained instructions. Consequently, although the edited output may seem reasonable, it often deviates from the intended changes outlined in the gold reference, resulting in low evaluation scores. To comprehensively investigate the text editing capabilities of large language models, this paper introduces XATU, the first benchmark specifically designed for fine-grained instruction-based explainable text editing. XATU covers a wide range of topics and text types, incorporating lexical, syntactic, semantic, and knowledge-intensive edits. To enhance interpretability, we leverage high-quality data sources and human annotation, resulting in a benchmark that includes fine-grained instructions and gold-standard edit explanations. By evaluating existing open and closed large language models against our benchmark, we demonstrate the effectiveness of instruction tuning and the impact of underlying architecture across various editing tasks. Furthermore, extensive experimentation reveals the significant role of explanations in fine-tuning language models for text editing tasks. The benchmark will be open-sourced to support reproduction and facilitate future research.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:文本编辑是一项重要的任务，它涉及修改文本，使其更加符合用户的意图。然而，现有的文本编辑标准数据集有限制，只提供粗略的指令。因此，编辑后的输出可能看起来合理，但它经常与金标准 refer 中的修改细则不符，导致评价分数低下。为了全面调查大语言模型的文本编辑能力，这篇论文引入 XATU，首个专门为精细指令基于的可解释文本编辑标准。XATU 覆盖了各种话题和文本类型，包括语法、语义和知识等编辑。为了增强可读性，我们利用高质量的数据源和人工标注，从而创建了一个包含精细指令和金标准编辑解释的标准。通过评价现有的开源和关闭式大语言模型，我们示出了指令调整和模型的底层结构对于不同的编辑任务的影响。此外，广泛的实验表明，解释在调整语言模型进行文本编辑任务时发挥了重要的作用。这个标准将被开源，以支持重现和未来研究。
</details></li>
</ul>
<hr>
<h2 id="fakenewsbr-A-Fake-News-Detection-Platform-for-Brazilian-Portuguese"><a href="#fakenewsbr-A-Fake-News-Detection-Platform-for-Brazilian-Portuguese" class="headerlink" title="fakenewsbr: A Fake News Detection Platform for Brazilian Portuguese"></a>fakenewsbr: A Fake News Detection Platform for Brazilian Portuguese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11052">http://arxiv.org/abs/2309.11052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luiz Giordani, Gilsiley Darú, Rhenan Queiroz, Vitor Buzinaro, Davi Keglevich Neiva, Daniel Camilo Fuentes Guzmán, Marcos Jardel Henriques, Oilson Alberto Gonzatto Junior, Francisco Louzada</li>
<li>for: 本研究旨在探讨假新闻的检测，尤其是在巴西葡萄牙语新闻文章中。</li>
<li>methods: 本研究使用自然语言处理技术，包括TF-IDF和Word2Vec，提取文本数据中的特征。并评估了多种分类算法，如逻辑回归、支持向量机、Random Forest、AdaBoost和LightGBM在一个包含真实和假新闻文章的数据集上的性能。</li>
<li>results: 提出的方法在评估中实现了高精度和F1-Score，证明其在检测假新闻中的有效性。此外，我们还开发了一个User-friendly的网页平台，fakenewsbr.com，以便用户对新闻文章的真实性进行实时分析。<details>
<summary>Abstract</summary>
The proliferation of fake news has become a significant concern in recent times due to its potential to spread misinformation and manipulate public opinion. This paper presents a comprehensive study on detecting fake news in Brazilian Portuguese, focusing on journalistic-type news. We propose a machine learning-based approach that leverages natural language processing techniques, including TF-IDF and Word2Vec, to extract features from textual data. We evaluate the performance of various classification algorithms, such as logistic regression, support vector machine, random forest, AdaBoost, and LightGBM, on a dataset containing both true and fake news articles. The proposed approach achieves high accuracy and F1-Score, demonstrating its effectiveness in identifying fake news. Additionally, we developed a user-friendly web platform, fakenewsbr.com, to facilitate the verification of news articles' veracity. Our platform provides real-time analysis, allowing users to assess the likelihood of fake news articles. Through empirical analysis and comparative studies, we demonstrate the potential of our approach to contribute to the fight against the spread of fake news and promote more informed media consumption.
</details>
<details>
<summary>摘要</summary>
“假新闻的扩散已成为当前的一大问题，因为它可能导致谣言的传播和公众意识的扭曲。这篇论文介绍了检测巴西葡萄牙语假新闻的完整研究，专注于新闻类文章。我们提议一种基于机器学习的方法，利用自然语言处理技术，包括TF-IDF和Word2Vec，提取文本数据中的特征。我们评估了多种分类算法，如逻辑回归、支持向量机和Random Forest等，在一个包含真实和假新闻文章的数据集上进行了评估。我们的方法实现了高精度和F1分数，证明了它的效iveness在识别假新闻。此外，我们还开发了一个用户友好的网站，fakenewsbr.com，以便评估新闻文章的真实性。我们的平台提供了实时分析，让用户在实时基础上评估假新闻文章的可能性。通过实验分析和比较研究，我们表明了我们的方法在抗击假新闻的扩散方面的潜在作用，并促进更有知识的媒体消费。”
</details></li>
</ul>
<hr>
<h2 id="Localize-Retrieve-and-Fuse-A-Generalized-Framework-for-Free-Form-Question-Answering-over-Tables"><a href="#Localize-Retrieve-and-Fuse-A-Generalized-Framework-for-Free-Form-Question-Answering-over-Tables" class="headerlink" title="Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables"></a>Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11049">http://arxiv.org/abs/2309.11049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wentinghome/TAGQA">https://github.com/wentinghome/TAGQA</a></li>
<li>paper_authors: Wenting Zhao, Ye Liu, Yao Wan, Yibo Wang, Zhongfen Deng, Philip S. Yu</li>
<li>for: 提高表格问答系统的能力，以生成基于表格数据和自然语言信息的详细答案。</li>
<li>methods: 提出一种三 stage 方法，包括表格转换为图形和细化筛选、外部知识 retrieval 和表格和文本融合（TAG-QA），以解决基于表格数据的自由形式问答的挑战。</li>
<li>results: 实验表明，TAG-QA 能够生成比基eline 更加准确、完整的答案，特别是与 pipeline-based 基eline TAPAS 和 end-to-end 模型 T5 相比。TAG-QA 在 BLEU-4 和 PARENT F-score 上比 TAPAS 高出 17% 和 14%，并高于 T5 的 BLEU-4 和 PARENT F-score 上的提高为 16% 和 12%。<details>
<summary>Abstract</summary>
Question answering on tabular data (a.k.a TableQA), which aims at generating answers to questions grounded on a provided table, has gained significant attention recently. Prior work primarily produces concise factual responses through information extraction from individual or limited table cells, lacking the ability to reason across diverse table cells. Yet, the realm of free-form TableQA, which demands intricate strategies for selecting relevant table cells and the sophisticated integration and inference of discrete data fragments, remains mostly unexplored. To this end, this paper proposes a generalized three-stage approach: Table-to- Graph conversion and cell localizing, external knowledge retrieval, and the fusion of table and text (called TAG-QA), to address the challenge of inferring long free-form answers in generative TableQA. In particular, TAG-QA (1) locates relevant table cells using a graph neural network to gather intersecting cells between relevant rows and columns, (2) leverages external knowledge from Wikipedia, and (3) generates answers by integrating both tabular data and natural linguistic information. Experiments showcase the superior capabilities of TAG-QA in generating sentences that are both faithful and coherent, particularly when compared to several state-of-the-art baselines. Notably, TAG-QA surpasses the robust pipeline-based baseline TAPAS by 17% and 14% in terms of BLEU-4 and PARENT F-score, respectively. Furthermore, TAG-QA outperforms the end-to-end model T5 by 16% and 12% on BLEU-4 and PARENT F-score, respectively.
</details>
<details>
<summary>摘要</summary>
问答基于表格数据（即 TableQA）在最近几年内获得了广泛关注，目的是生成基于提供的表格数据的问题的回答。然而，现有的工作主要通过提取表格单元中的信息进行信息抽取，缺乏能够跨单元进行推理的能力。为了解决这个问题，本文提出了一种通用的三stageapproach：表格转 graf并Cell Localization（TAG-QA），以生成具有推理能力的表格问答系统。具体来说，TAG-QA包括以下三个阶段：1. 使用图 neural network 来找到相关的表格单元，并将其作为交叉单元进行汇聚。2. 利用外部知识来提高表格问答的能力。3. 将表格数据和自然语言信息 integrate 起来，以生成具有 faithful 和 coherent 性的回答。实验表明，TAG-QA 在生成长度不受限制的自由形表格问答方面具有显著的优势，特别是与一些状态之际的基准值进行比较。在 BLEU-4 和 PARENT F-score 等指标上，TAG-QA 与 TAPAS 和 T5 模型相比，净提高了17%和14%。此外，TAG-QA 还在 BLEU-4 和 PARENT F-score 上出现16%和12%的提升。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Entity-Matching-with-Complex-Attribute-Associations-using-BERT-and-Neural-Networks"><a href="#Heterogeneous-Entity-Matching-with-Complex-Attribute-Associations-using-BERT-and-Neural-Networks" class="headerlink" title="Heterogeneous Entity Matching with Complex Attribute Associations using BERT and Neural Networks"></a>Heterogeneous Entity Matching with Complex Attribute Associations using BERT and Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11046">http://arxiv.org/abs/2309.11046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shitao Wang, Jiamin Lu</li>
<li>for: Addressing the challenges of entity matching in heterogeneous data with complex attribute relationships.</li>
<li>methods: Utilizing a novel entity matching model, EMM-CCAR, built upon pre-trained models, with attention mechanisms to capture complex relationships between attributes.</li>
<li>results: Achieving improvements of approximately 4% and 1% in F1 scores compared to prevalent DER-SSM and Ditto approaches, respectively, demonstrating the effectiveness of the proposed model in handling complex attribute relationships.<details>
<summary>Abstract</summary>
Across various domains, data from different sources such as Baidu Baike and Wikipedia often manifest in distinct forms. Current entity matching methodologies predominantly focus on homogeneous data, characterized by attributes that share the same structure and concise attribute values. However, this orientation poses challenges in handling data with diverse formats. Moreover, prevailing approaches aggregate the similarity of attribute values between corresponding attributes to ascertain entity similarity. Yet, they often overlook the intricate interrelationships between attributes, where one attribute may have multiple associations. The simplistic approach of pairwise attribute comparison fails to harness the wealth of information encapsulated within entities.To address these challenges, we introduce a novel entity matching model, dubbed Entity Matching Model for Capturing Complex Attribute Relationships(EMM-CCAR),built upon pre-trained models. Specifically, this model transforms the matching task into a sequence matching problem to mitigate the impact of varying data formats. Moreover, by introducing attention mechanisms, it identifies complex relationships between attributes, emphasizing the degree of matching among multiple attributes rather than one-to-one correspondences. Through the integration of the EMM-CCAR model, we adeptly surmount the challenges posed by data heterogeneity and intricate attribute interdependencies. In comparison with the prevalent DER-SSM and Ditto approaches, our model achieves improvements of approximately 4% and 1% in F1 scores, respectively. This furnishes a robust solution for addressing the intricacies of attribute complexity in entity matching.
</details>
<details>
<summary>摘要</summary>
across various domains, data from different sources such as Baidu Baike and Wikipedia often manifest in distinct forms. Current entity matching methodologies predominantly focus on homogeneous data, characterized by attributes that share the same structure and concise attribute values. However, this orientation poses challenges in handling data with diverse formats. Moreover, prevailing approaches aggregate the similarity of attribute values between corresponding attributes to ascertain entity similarity. Yet, they often overlook the intricate interrelationships between attributes, where one attribute may have multiple associations. The simplistic approach of pairwise attribute comparison fails to harness the wealth of information encapsulated within entities.To address these challenges, we introduce a novel entity matching model, dubbed Entity Matching Model for Capturing Complex Attribute Relationships(EMM-CCAR),built upon pre-trained models. Specifically, this model transforms the matching task into a sequence matching problem to mitigate the impact of varying data formats. Moreover, by introducing attention mechanisms, it identifies complex relationships between attributes, emphasizing the degree of matching among multiple attributes rather than one-to-one correspondences. Through the integration of the EMM-CCAR model, we adeptly surmount the challenges posed by data heterogeneity and intricate attribute interdependencies. In comparison with the prevalent DER-SSM and Ditto approaches, our model achieves improvements of approximately 4% and 1% in F1 scores, respectively. This furnishes a robust solution for addressing the intricacies of attribute complexity in entity matching.
</details></li>
</ul>
<hr>
<h2 id="Named-Entity-Recognition-via-Machine-Reading-Comprehension-A-Multi-Task-Learning-Approach"><a href="#Named-Entity-Recognition-via-Machine-Reading-Comprehension-A-Multi-Task-Learning-Approach" class="headerlink" title="Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach"></a>Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11027">http://arxiv.org/abs/2309.11027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yibo Wang, Wenting Zhao, Yao Wan, Zhongfen Deng, Philip S. Yu</li>
<li>for: 本研究旨在提高Machine Reading Comprehension（MRC）基于Named Entity Recognition（NER）的性能，特别是在忽略实体类别 Label 相互关系的情况下。</li>
<li>methods: 本文提出了一种基于多任务学习的 Multi-NER 模型，通过自注意机制 capture 实体类别 Label 相互关系。</li>
<li>results: 对于嵌入式 NER 和平面 NER 数据集，实验结果表明 Multi-NER 可以在所有数据集上提高性能。<details>
<summary>Abstract</summary>
Named Entity Recognition (NER) aims to extract and classify entity mentions in the text into pre-defined types (e.g., organization or person name). Recently, many works have been proposed to shape the NER as a machine reading comprehension problem (also termed MRC-based NER), in which entity recognition is achieved by answering the formulated questions related to pre-defined entity types through MRC, based on the contexts. However, these works ignore the label dependencies among entity types, which are critical for precisely recognizing named entities. In this paper, we propose to incorporate the label dependencies among entity types into a multi-task learning framework for better MRC-based NER. We decompose MRC-based NER into multiple tasks and use a self-attention module to capture label dependencies. Comprehensive experiments on both nested NER and flat NER datasets are conducted to validate the effectiveness of the proposed Multi-NER. Experimental results show that Multi-NER can achieve better performance on all datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Joint-Modeling-of-Dialogue-Response-and-Speech-Synthesis-based-on-Large-Language-Model"><a href="#Towards-Joint-Modeling-of-Dialogue-Response-and-Speech-Synthesis-based-on-Large-Language-Model" class="headerlink" title="Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large Language Model"></a>Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11000">http://arxiv.org/abs/2309.11000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XinyuZhou2000/Spoken_Dialogue">https://github.com/XinyuZhou2000/Spoken_Dialogue</a></li>
<li>paper_authors: Xinyu Zhou, Delong Chen, Yudong Chen</li>
<li>for: 构建一个基于大语言模型的对话系统，更加准确地模拟人类语言生成过程。</li>
<li>methods: 使用大语言模型来同时模型对话响应和语言特征，并在语音结构预测和对话响应 integrate 多种语言特征。</li>
<li>results: 实验结果表明，基于大语言模型的方法是建立一个紧凑的对话系统的可能性的。<details>
<summary>Abstract</summary>
This paper explores the potential of constructing an AI spoken dialogue system that "thinks how to respond" and "thinks how to speak" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules. We hypothesize that Large Language Models (LLMs) with billions of parameters possess significant speech understanding capabilities and can jointly model dialogue responses and linguistic features. We conduct two sets of experiments: 1) Prosodic structure prediction, a typical front-end task in TTS, demonstrating the speech understanding ability of LLMs, and 2) Further integrating dialogue response and a wide array of linguistic features using a unified encoding format. Our results indicate that the LLM-based approach is a promising direction for building unified spoken dialogue systems.
</details>
<details>
<summary>摘要</summary>
这个论文探讨了构建一个基于人工智能的对话系统，该系统可以同时“思考如何回答”和“思考如何说”，这更接近于人类语言生产过程。我们假设大语言模型（LLM）拥有数十亿个参数，具有强大的语音理解能力，可以同时模型对话回答和语言特征。我们进行了两组实验：1）语调结构预测，这是常见的前端任务在文本识别中，以示LLM的语音理解能力；2）将对话回答和广泛的语言特征集成使用统一编码格式。我们的结果表明，基于LLM的方法是构建统一的对话系统的可能之道。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/cs.CL_2023_09_20/" data-id="clnsn0veo008vgf880yudai73" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/55/">55</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">78</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">78</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">22</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">150</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
