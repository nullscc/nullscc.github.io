
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/2/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.AI_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T12:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/cs.AI_2023_10_30/">cs.AI - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Integrating-Summarization-and-Retrieval-for-Enhanced-Personalization-via-Large-Language-Models"><a href="#Integrating-Summarization-and-Retrieval-for-Enhanced-Personalization-via-Large-Language-Models" class="headerlink" title="Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models"></a>Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20081">http://arxiv.org/abs/2310.20081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris Richardson, Yao Zhang, Kellen Gillespie, Sudipta Kar, Arshdeep Singh, Zeynab Raeesy, Omar Zia Khan, Abhinav Sethy</li>
<li>for: 提高自然语言处理（NLP）系统的用户体验，特别是通过大语言模型（LLM）来更好地个性化用户体验。</li>
<li>methods: 使用语言模型提取过去用户数据，并将其作为下游任务的提示进行个性化。</li>
<li>results:  experiments show 我们的方法可以在实际环境下，即使有时间和成本限制，也能够具有与抽取方法相当或更好的表现，并且可以减少75%的用户数据 Retrieval。<details>
<summary>Abstract</summary>
Personalization, the ability to tailor a system to individual users, is an essential factor in user experience with natural language processing (NLP) systems. With the emergence of Large Language Models (LLMs), a key question is how to leverage these models to better personalize user experiences. To personalize a language model's output, a straightforward approach is to incorporate past user data into the language model prompt, but this approach can result in lengthy inputs exceeding limitations on input length and incurring latency and cost issues. Existing approaches tackle such challenges by selectively extracting relevant user data (i.e. selective retrieval) to construct a prompt for downstream tasks. However, retrieval-based methods are limited by potential information loss, lack of more profound user understanding, and cold-start challenges. To overcome these limitations, we propose a novel summary-augmented approach by extending retrieval-augmented personalization with task-aware user summaries generated by LLMs. The summaries can be generated and stored offline, enabling real-world systems with runtime constraints like voice assistants to leverage the power of LLMs. Experiments show our method with 75% less of retrieved user data is on-par or outperforms retrieval augmentation on most tasks in the LaMP personalization benchmark. We demonstrate that offline summarization via LLMs and runtime retrieval enables better performance for personalization on a range of tasks under practical constraints.
</details>
<details>
<summary>摘要</summary>
personalization, tailoring a system to individual users, is a crucial aspect of user experience with natural language processing (NLP) systems. with the emergence of large language models (LLMs), a key question is how to leverage these models to better personalize user experiences. to personalize a language model's output, a straightforward approach is to incorporate past user data into the language model prompt, but this approach can result in lengthy inputs exceeding limitations on input length and incurring latency and cost issues. existing approaches tackle such challenges by selectively extracting relevant user data (i.e. selective retrieval) to construct a prompt for downstream tasks. however, retrieval-based methods are limited by potential information loss, lack of more profound user understanding, and cold-start challenges. to overcome these limitations, we propose a novel summary-augmented approach by extending retrieval-augmented personalization with task-aware user summaries generated by LLMs. the summaries can be generated and stored offline, enabling real-world systems with runtime constraints like voice assistants to leverage the power of LLMs. experiments show our method with 75% less of retrieved user data is on-par or outperforms retrieval augmentation on most tasks in the LaMP personalization benchmark. we demonstrate that offline summarization via LLMs and runtime retrieval enables better performance for personalization on a range of tasks under practical constraints.
</details></li>
</ul>
<hr>
<h2 id="FOCAL-Contrastive-Learning-for-Multimodal-Time-Series-Sensing-Signals-in-Factorized-Orthogonal-Latent-Space"><a href="#FOCAL-Contrastive-Learning-for-Multimodal-Time-Series-Sensing-Signals-in-Factorized-Orthogonal-Latent-Space" class="headerlink" title="FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space"></a>FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20071">http://arxiv.org/abs/2310.20071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengzhong Liu, Tomoyoshi Kimura, Dongxin Liu, Ruijie Wang, Jinyang Li, Suhas Diggavi, Mani Srivastava, Tarek Abdelzaher</li>
<li>for: 提出了一种新的对比学习框架FOCAL，用于从多Modal时间序列感知信号中提取全面特征，通过无监督训练。</li>
<li>methods: FOCAL使用多Modal时间序列中的特征编码，并使用modal匹配目标和变换不变目标来提取共同特征和专用特征。同时，它还引入了时间结构约束，以保证模式特征之间的距离关系。</li>
<li>results: FOCAL在四个多Modal感知数据集上进行了广泛的评估，并与现有的基eline进行了比较。结果显示，FOCAL在下游任务中具有明显的优势，具有较高的准确率和较低的损失值。<details>
<summary>Abstract</summary>
This paper proposes a novel contrastive learning framework, called FOCAL, for extracting comprehensive features from multimodal time-series sensing signals through self-supervised training. Existing multimodal contrastive frameworks mostly rely on the shared information between sensory modalities, but do not explicitly consider the exclusive modality information that could be critical to understanding the underlying sensing physics. Besides, contrastive frameworks for time series have not handled the temporal information locality appropriately. FOCAL solves these challenges by making the following contributions: First, given multimodal time series, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective. In contrast, the private space extracts modality-exclusive information through a transformation-invariant objective. Second, we propose a temporal structural constraint for modality features, such that the average distance between temporally neighboring samples is no larger than that of temporally distant samples. Extensive evaluations are performed on four multimodal sensing datasets with two backbone encoders and two classifiers to demonstrate the superiority of FOCAL. It consistently outperforms the state-of-the-art baselines in downstream tasks with a clear margin, under different ratios of available labels. The code and self-collected dataset are available at https://github.com/tomoyoshki/focal.
</details>
<details>
<summary>摘要</summary>
First, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective, while the private space extracts modality-exclusive information through a transformation-invariant objective.Second, it introduces a temporal structural constraint for modality features, such that the average distance between temporally neighboring samples is no larger than that of temporally distant samples. This ensures that the model learns to capture the temporal relationships between samples.The proposed framework is evaluated on four multimodal sensing datasets with two backbone encoders and two classifiers. The results show that FOCAL consistently outperforms the state-of-the-art baselines in downstream tasks with a clear margin, under different ratios of available labels. The code and self-collected dataset are available at https://github.com/tomoyoshki/focal.
</details></li>
</ul>
<hr>
<h2 id="Vignat-Vulnerability-identification-by-learning-code-semantics-via-graph-attention-networks"><a href="#Vignat-Vulnerability-identification-by-learning-code-semantics-via-graph-attention-networks" class="headerlink" title="Vignat: Vulnerability identification by learning code semantics via graph attention networks"></a>Vignat: Vulnerability identification by learning code semantics via graph attention networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20067">http://arxiv.org/abs/2310.20067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Liu, Gail Kaiser</li>
<li>for: 本研究旨在提高软件安全性，通过自适应学习图级别Semantic Representation来发现漏洞。</li>
<li>methods: 我们使用Code Property Graphs (CPGs)来表示代码，并使用Graph Attention Networks (GATs)进行漏洞检测。</li>
<li>results: 我们在可靠的 datasets 上实现了 $57.38%$ 的准确率，并且可以获得漏洞模式的可读性。<details>
<summary>Abstract</summary>
Vulnerability identification is crucial to protect software systems from attacks for cyber-security. However, huge projects have more than millions of lines of code, and the complex dependencies make it hard to carry out traditional static and dynamic methods. Furthermore, the semantic structure of various types of vulnerabilities differs greatly and may occur simultaneously, making general rule-based methods difficult to extend. In this paper, we propose \textit{Vignat}, a novel attention-based framework for identifying vulnerabilities by learning graph-level semantic representations of code. We represent codes with code property graphs (CPGs) in fine grain and use graph attention networks (GATs) for vulnerability detection. The results show that Vignat is able to achieve $57.38\%$ accuracy on reliable datasets derived from popular C libraries. Furthermore, the interpretability of our GATs provides valuable insights into vulnerability patterns.
</details>
<details>
<summary>摘要</summary>
<<SYS>>transliteration: Vulnerability zhìyè shì yòu zhìyè zhòngjì xìtiě de yìqie zhòngjì zhìyè shì yòu xìtiě zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì yòu zhìyè zhìyè shì yòu xìtiě zhìyè shì yòu zhìyè zhìyè shì yòu yìqie zhìyè shì y
</details></li>
</ul>
<hr>
<h2 id="Concept-Alignment-as-a-Prerequisite-for-Value-Alignment"><a href="#Concept-Alignment-as-a-Prerequisite-for-Value-Alignment" class="headerlink" title="Concept Alignment as a Prerequisite for Value Alignment"></a>Concept Alignment as a Prerequisite for Value Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20059">http://arxiv.org/abs/2310.20059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunayana Rane, Mark Ho, Ilia Sucholutsky, Thomas L. Griffiths</li>
<li>for: 本研究旨在建立AI系统，以安全可靠的方式与人类交互。</li>
<li>methods: 本研究使用 inverse reinforcement learning Setting 进行形式化分析，并证明了概念Alignment 是值Alignment 的必要前提。</li>
<li>results: 人类参与者在实验中证明了，当agent acts intentionally时，人类会根据agent使用的概念来进行合理的思维。<details>
<summary>Abstract</summary>
Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -- agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model.
</details>
<details>
<summary>摘要</summary>
<<SYS>>值Alignment是AI系统与人类之间安全、可靠交互的关键。然而，人类的价值观与可能理解和评估世界的概念相关。因此，概念Alignment是价值Alignment的前提——代理需要与人类的情况表示相对应才能成功地Alignment其价值。我们在 inverse reinforcement learning  Setting formally analyze the concept alignment problem, show that neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. In addition, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model.中文简体版
</details></li>
</ul>
<hr>
<h2 id="Constrained-Hierarchical-Monte-Carlo-Belief-State-Planning"><a href="#Constrained-Hierarchical-Monte-Carlo-Belief-State-Planning" class="headerlink" title="Constrained Hierarchical Monte Carlo Belief-State Planning"></a>Constrained Hierarchical Monte Carlo Belief-State Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20054">http://arxiv.org/abs/2310.20054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arec Jamgochian, Hugo Buurmeijer, Kyle H. Wray, Anthony Corso, Mykel J. Kochenderfer</li>
<li>for: 这篇论文目的是为了解决受限制的部分可观察Markov问题（CPOMDP）中的最佳规划问题，并在不同的状态和转换 uncertainty 下保持安全的规划。</li>
<li>methods: 这篇论文使用的方法是将问题分解为较低层的控制问题，使用高层的动作原则（options）来进行搜寻。</li>
<li>results: 这篇论文的结果显示，如果将基本的选项控制器定义为满足指派的紧张预算，那么COBeTS就可以确保满足紧张预算任何时候。否则，COBeTS将导引搜寻 towards a safe sequence of option primitives，并使用层次监控来实现runtime safety。<details>
<summary>Abstract</summary>
Optimal plans in Constrained Partially Observable Markov Decision Processes (CPOMDPs) maximize reward objectives while satisfying hard cost constraints, generalizing safe planning under state and transition uncertainty. Unfortunately, online CPOMDP planning is extremely difficult in large or continuous problem domains. In many large robotic domains, hierarchical decomposition can simplify planning by using tools for low-level control given high-level action primitives (options). We introduce Constrained Options Belief Tree Search (COBeTS) to leverage this hierarchy and scale online search-based CPOMDP planning to large robotic problems. We show that if primitive option controllers are defined to satisfy assigned constraint budgets, then COBeTS will satisfy constraints anytime. Otherwise, COBeTS will guide the search towards a safe sequence of option primitives, and hierarchical monitoring can be used to achieve runtime safety. We demonstrate COBeTS in several safety-critical, constrained partially observable robotic domains, showing that it can plan successfully in continuous CPOMDPs while non-hierarchical baselines cannot.
</details>
<details>
<summary>摘要</summary>
最佳计划在受限 partially observable Markov decision process (CPOMDP) 中最大化 reward 目标，同时满足硬件成本限制，广泛应用于安全观察下的规划。然而，在大型或连续问题领域中的线上 CPOMDP 规划具有极高的问题难度。在许多大型机器人领域中，层次分解可以简化规划，使用工具 для low-level control 给 high-level action primitives（选项）。我们介绍 Constrained Options Belief Tree Search (COBeTS)，以利用这个层次，将线上搜寻基于 CPOMDP 规划 scales 到大型机器人问题领域。我们证明，如果单元选项控制器是将任务分配到硬件成本预算，COBeTS 就一定会满足限制。否则，COBeTS 将导引搜寻 towards a safe sequence of option primitives，并使用层次监控来实现 runtime 安全。我们在 Several safety-critical, constrained partially observable robotic domains 中评估 COBeTS，结果显示它可以在连续 CPOMDP 中成功规划，而非层次基于的基底不能。
</details></li>
</ul>
<hr>
<h2 id="Look-At-Me-No-Replay-SurpriseNet-Anomaly-Detection-Inspired-Class-Incremental-Learning"><a href="#Look-At-Me-No-Replay-SurpriseNet-Anomaly-Detection-Inspired-Class-Incremental-Learning" class="headerlink" title="Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class Incremental Learning"></a>Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20052">http://arxiv.org/abs/2310.20052</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tachyonicclock/surprisenet-cikm-23">https://github.com/tachyonicclock/surprisenet-cikm-23</a></li>
<li>paper_authors: Anton Lee, Yaqian Zhang, Heitor Murilo Gomes, Albert Bifet, Bernhard Pfahringer</li>
<li>for: 这种研究旨在解决人工智能网络在不断学习中遇到的悬峰性干扰和分类境界知识的问题。</li>
<li>methods: 这种方法使用参数隔离方法和基于异常检测的自适应器来解决悬峰性干扰，并且不依赖于图像特定的逻辑假设。</li>
<li>results: 实验表明，SurpriseNet在传统视觉不断学习标准准则上表现出色，以及在结构化数据集上。源代码可以在<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.8247906%E5%92%8Chttps://github.com/tachyonicClock/SurpriseNet-CIKM-23%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://doi.org/10.5281/zenodo.8247906和https://github.com/tachyonicClock/SurpriseNet-CIKM-23中下载。</a><details>
<summary>Abstract</summary>
Continual learning aims to create artificial neural networks capable of accumulating knowledge and skills through incremental training on a sequence of tasks. The main challenge of continual learning is catastrophic interference, wherein new knowledge overrides or interferes with past knowledge, leading to forgetting. An associated issue is the problem of learning "cross-task knowledge," where models fail to acquire and retain knowledge that helps differentiate classes across task boundaries. A common solution to both problems is "replay," where a limited buffer of past instances is utilized to learn cross-task knowledge and mitigate catastrophic interference. However, a notable drawback of these methods is their tendency to overfit the limited replay buffer. In contrast, our proposed solution, SurpriseNet, addresses catastrophic interference by employing a parameter isolation method and learning cross-task knowledge using an auto-encoder inspired by anomaly detection. SurpriseNet is applicable to both structured and unstructured data, as it does not rely on image-specific inductive biases. We have conducted empirical experiments demonstrating the strengths of SurpriseNet on various traditional vision continual-learning benchmarks, as well as on structured data datasets. Source code made available at https://doi.org/10.5281/zenodo.8247906 and https://github.com/tachyonicClock/SurpriseNet-CIKM-23
</details>
<details>
<summary>摘要</summary>
In contrast, our proposed solution, SurpriseNet, addresses catastrophic interference by employing a parameter isolation method and learning cross-task knowledge using an auto-encoder inspired by anomaly detection. SurpriseNet is applicable to both structured and unstructured data, as it does not rely on image-specific inductive biases. We have conducted empirical experiments demonstrating the strengths of SurpriseNet on various traditional vision continual-learning benchmarks, as well as on structured data datasets. The source code is available at <https://doi.org/10.5281/zenodo.8247906> and <https://github.com/tachyonicClock/SurpriseNet-CIKM-23>.
</details></li>
</ul>
<hr>
<h2 id="SURF-A-Generalization-Benchmark-for-GNNs-Predicting-Fluid-Dynamics"><a href="#SURF-A-Generalization-Benchmark-for-GNNs-Predicting-Fluid-Dynamics" class="headerlink" title="SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics"></a>SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20049">http://arxiv.org/abs/2310.20049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/s-kuenzli/surf-fluidsimulation">https://github.com/s-kuenzli/surf-fluidsimulation</a></li>
<li>paper_authors: Stefan Künzli, Florain Grötschla, Joël Mathys, Roger Wattenhofer</li>
<li>for: 这个论文是为了测试学习基于图的流体动力学模型的通用性而写的。</li>
<li>methods: 这篇论文使用了学习模型来模拟流体动力学，并使用了特定的数据集来测试和比较不同模型的通用性。</li>
<li>results: 研究发现，当模型需要适应不同的结构、分辨率或热动力学范围时，学习基于图的模型的通用性会受到影响。<details>
<summary>Abstract</summary>
Simulating fluid dynamics is crucial for the design and development process, ranging from simple valves to complex turbomachinery. Accurately solving the underlying physical equations is computationally expensive. Therefore, learning-based solvers that model interactions on meshes have gained interest due to their promising speed-ups. However, it is unknown to what extent these models truly understand the underlying physical principles and can generalize rather than interpolate. Generalization is a key requirement for a general-purpose fluid simulator, which should adapt to different topologies, resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to test the \textit{generalization} of learned graph-based fluid simulators. SURF comprises individual datasets and provides specific performance and generalization metrics for evaluating and comparing different models. We empirically demonstrate the applicability of SURF by thoroughly investigating the two state-of-the-art graph-based models, yielding new insights into their generalization.
</details>
<details>
<summary>摘要</summary>
模拟流体动力学是设计和开发过程中的关键环节，从简单的阀门到复杂的液压机。准确解决下面的物理方程是计算昂贵的。因此，学习型解决方案，即模型在网格上的交互，在计算速度方面表现出了扎根。然而，这些模型是否真正理解下面的物理原理，并能泛化而不仅是 interpolate？泛化是一个关键的要求，以便建立一个通用的流体 simulator，可以适应不同的topology、分辨率或热动力范围。我们提出了 SURF，一个用于测试学习型流体 simulator 的泛化能力的benchmark。SURF包括各个数据集，并提供了特定的性能和泛化指标，用于评估和比较不同的模型。我们进行了大量的实验，证明了 SURF 的可靠性和有用性，并且对两种当前最佳的图像基本模型进行了深入的探索，从而获得了新的理解。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Imitation-Edit-Feedback-for-Factual-Alignment-in-Clinical-Summarization"><a href="#Synthetic-Imitation-Edit-Feedback-for-Factual-Alignment-in-Clinical-Summarization" class="headerlink" title="Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization"></a>Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20033">http://arxiv.org/abs/2310.20033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prakamya Mishra, Zonghai Yao, Shuwei Chen, Beining Wang, Rohan Mittal, Hong Yu<br>for: This paper aims to improve the factual consistency of clinical note summarization using ChatGPT to generate high-quality feedback data.methods: The authors use ChatGPT to generate edit feedback for improving the factual consistency of clinical note summarization.results: The authors evaluate the effectiveness of using GPT edits in human alignment, showing promising results in improving factual consistency.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summarization task. We focus specifically on edit feedback because recent work discusses the shortcomings of human alignment via preference feedback in complex situations (such as clinical NLP tasks that require extensive expert knowledge), as well as some advantages of collecting edit feedback from domain experts. In addition, although GPT has reached the expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much previous work discussing whether GPT can generate expert-level edit feedback for LMs in the clinical note summarization task. We hope to fill this gap. Finally, our evaluations demonstrate the potential use of GPT edits in human alignment, especially from a factuality perspective.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT和LLaMA家族在捕捉和简化关键上下文信息方面表现出了异常的能力，并达到了当前最佳性能的概要任务。然而，社区对这些模型的幻觉问题仍然增长。LLM sometimes generates factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summarization task. We focus specifically on edit feedback because recent work discusses the shortcomings of human alignment via preference feedback in complex situations (such as clinical NLP tasks that require extensive expert knowledge), as well as some advantages of collecting edit feedback from domain experts. In addition, although GPT has reached the expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much previous work discussing whether GPT can generate expert-level edit feedback for LMs in the clinical note summarization task. We hope to fill this gap. Finally, our evaluations demonstrate the potential use of GPT edits in human alignment, especially from a factuality perspective.
</details></li>
</ul>
<hr>
<h2 id="GOPlan-Goal-conditioned-Offline-Reinforcement-Learning-by-Planning-with-Learned-Models"><a href="#GOPlan-Goal-conditioned-Offline-Reinforcement-Learning-by-Planning-with-Learned-Models" class="headerlink" title="GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models"></a>GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20025">http://arxiv.org/abs/2310.20025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mianchu Wang, Rui Yang, Xi Chen, Meng Fang</li>
<li>for: 学习通用策略从多种和多任务的离线数据集中。</li>
<li>methods: 使用两stage模型基于框架，包括预训练一个可以捕捉多种动作分布的先前策略，然后使用划算法与规划来生成假数据 для练化策略。</li>
<li>results: 在多种离线多目标摆动任务上达到了状态之 arts 性能，并且能够处理小数据预算和不同目标的扩展。<details>
<summary>Abstract</summary>
Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by planning with learned models for both intra-trajectory and inter-trajectory goals. Through experimental evaluations, we demonstrate that GOPlan achieves state-of-the-art performance on various offline multi-goal manipulation tasks. Moreover, our results highlight the superior ability of GOPlan to handle small data budgets and generalize to OOD goals.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese<</SYS>>Offline目标条件RL（GCRL）提供了一个可行的 парадиг，从多样化和多任务的离线数据集中学习通用策略。尽管最近有所进步，主要的离线GCRL方法受限于无模型的方法，这限制了它们对有限数据预算和未看过目标的泛化能力。在这项工作中，我们提出了一种新的两阶段模型基于框架，即目标条件的计划（GOPlan），包括（1）预训练一个能够捕捉多模态动作分布在多目标数据集中的先前策略；（2）使用计划方法和规划来生成假 trajectory 进行迭代优化策略。具体来说，先前策略基于带有优化的 Conditioned Generative Adversarial Networks（CGANs），可以快速分离出异常行为（OOD）的问题。为了进一步优化策略，计划方法生成了高质量的假数据，通过规划learned模型来实现内 trajectory和间 trajectory的目标。经过实验评估，我们表明 GOPlan 可以在多个离线多目标机械处理任务中 дости得状态之Art的表现。此外，我们的结果还 highlight了 GOPlan 对小数据预算和 OOD 目标的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Topology-Recoverability-Prediction-for-Ad-Hoc-Robot-Networks-A-Data-Driven-Fault-Tolerant-Approach"><a href="#Topology-Recoverability-Prediction-for-Ad-Hoc-Robot-Networks-A-Data-Driven-Fault-Tolerant-Approach" class="headerlink" title="Topology Recoverability Prediction for Ad-Hoc Robot Networks: A Data-Driven Fault-Tolerant Approach"></a>Topology Recoverability Prediction for Ad-Hoc Robot Networks: A Data-Driven Fault-Tolerant Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20024">http://arxiv.org/abs/2310.20024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matin Macktoobian, Zhan Shu, Qing Zhao</li>
<li>for: 这个论文主要是为了预测无架 robot 网络中发生故障后网络的重建可能性。</li>
<li>methods: 本文使用了 Bayesian Gaussian mixture models 的二条通路数据驱动模型，透过两条不同的预测路径，预测网络中发生故障后网络的重建可能性。</li>
<li>results: 本文的结果显示，与文献中现有最佳策略相比，二条通路数据驱动模型能够成功地解决网络（不）可回复预测问题。<details>
<summary>Abstract</summary>
Faults occurring in ad-hoc robot networks may fatally perturb their topologies leading to disconnection of subsets of those networks. Optimal topology synthesis is generally resource-intensive and time-consuming to be done in real time for large ad-hoc robot networks. One should only perform topology re-computations if the probability of topology recoverability after the occurrence of any fault surpasses that of its irrecoverability. We formulate this problem as a binary classification problem. Then, we develop a two-pathway data-driven model based on Bayesian Gaussian mixture models that predicts the solution to a typical problem by two different pre-fault and post-fault prediction pathways. The results, obtained by the integration of the predictions of those pathways, clearly indicate the success of our model in solving the topology (ir)recoverability prediction problem compared to the best of current strategies found in the literature.
</details>
<details>
<summary>摘要</summary>
FAULTS 发生在随机机器人网络中可能导致网络结构的不稳定，从而导致一些网络的分支断开。优化网络结构是在大规模随机机器人网络中实时进行的资源投入和时间consuming的任务。我们只应在缺陷发生后的概率超过了不可回复的概率时进行网络结构重新计算。我们将这个问题形式化为 binary 分类问题。然后，我们开发了基于极 bayesian Gaussian mixture 模型的两路数据驱动模型，该模型通过两个不同的预缺陷和后缺陷预测路径来预测一个典型问题的解决方案。结果显示，通过将这两个路径的预测结果融合，我们的模型在解决网络（不）可回复预测问题上具有显著的成功，比文献中最佳策略更高。
</details></li>
</ul>
<hr>
<h2 id="Multiscale-Feature-Attribution-for-Outliers"><a href="#Multiscale-Feature-Attribution-for-Outliers" class="headerlink" title="Multiscale Feature Attribution for Outliers"></a>Multiscale Feature Attribution for Outliers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20012">http://arxiv.org/abs/2310.20012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeff Shen, Peter Melchior</li>
<li>for: 这个论文是为了解决自动异常点检测问题，即使数据量很大，也能够更快、更可重复地检测到异常点。</li>
<li>methods: 这篇论文提出了一种新的特征归因方法，即反多尺度遮盖方法，这种方法专门针对异常点，因为在异常点上我们知道的特征很少，模型性能可能也不太好。</li>
<li>results: 论文的实验结果表明，这种特征归因方法在检测银河谱pectra中的异常点时比较有 interpretable 性，比如alternative归因方法更好。<details>
<summary>Abstract</summary>
Machine learning techniques can automatically identify outliers in massive datasets, much faster and more reproducible than human inspection ever could. But finding such outliers immediately leads to the question: which features render this input anomalous? We propose a new feature attribution method, Inverse Multiscale Occlusion, that is specifically designed for outliers, for which we have little knowledge of the type of features we want to identify and expect that the model performance is questionable because anomalous test data likely exceed the limits of the training data. We demonstrate our method on outliers detected in galaxy spectra from the Dark Energy Survey Instrument and find its results to be much more interpretable than alternative attribution approaches.
</details>
<details>
<summary>摘要</summary>
使用机器学习技术可以自动找出大量数据中的异常数据点，比人工检查更快速和可重复。但是发现这些异常数据点后，我们就会问：哪些特征使这个输入异常？我们提出了一种新的特征归因方法，反向多Scale遮盲，特意为异常数据点设计，我们对这些异常数据点知之甚少，而且预期模型性能很差，因为异常测试数据可能超出了训练数据的范围。我们在银河谱spectra中检测到的异常数据点上应用了这种方法，并发现其结果比替代归因方法更易于理解。
</details></li>
</ul>
<hr>
<h2 id="Evolutionary-Tabletop-Game-Design-A-Case-Study-in-the-Risk-Game"><a href="#Evolutionary-Tabletop-Game-Design-A-Case-Study-in-the-Risk-Game" class="headerlink" title="Evolutionary Tabletop Game Design: A Case Study in the Risk Game"></a>Evolutionary Tabletop Game Design: A Case Study in the Risk Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20008">http://arxiv.org/abs/2310.20008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lana Bertoldo Rossato, Leonardo Boaventura Bombardelli, Anderson Rocha Tavares</li>
<li>for: 这个论文旨在创造和评估桌面游戏，以提高现有游戏的创新和变化。</li>
<li>methods: 这篇论文使用了进化算法和自动游戏测试来创造和评估桌面游戏。</li>
<li>results: 这篇论文的结果表明，通过使用遗传算法和规则引入的智能游戏测试，可以创造出新的桌面游戏变体，比如卡牌游戏和地图游戏。这些变体的游戏时间 shorter，并且比原始游戏更具 equilibrio。但是，这种方法还有一些局限性，例如，在许多情况下，目标函数 Correctly pursued，但生成的游戏几乎是平庸的。<details>
<summary>Abstract</summary>
Creating and evaluating games manually is an arduous and laborious task. Procedural content generation can aid by creating game artifacts, but usually not an entire game. Evolutionary game design, which combines evolutionary algorithms with automated playtesting, has been used to create novel board games with simple equipment; however, the original approach does not include complex tabletop games with dice, cards, and maps. This work proposes an extension of the approach for tabletop games, evaluating the process by generating variants of Risk, a military strategy game where players must conquer map territories to win. We achieved this using a genetic algorithm to evolve the chosen parameters, as well as a rules-based agent to test the games and a variety of quality criteria to evaluate the new variations generated. Our results show the creation of new variations of the original game with smaller maps, resulting in shorter matches. Also, the variants produce more balanced matches, maintaining the usual drama. We also identified limitations in the process, where, in many cases, where the objective function was correctly pursued, but the generated games were nearly trivial. This work paves the way towards promising research regarding the use of evolutionary game design beyond classic board games.
</details>
<details>
<summary>摘要</summary>
创造和评估游戏手动是一项艰难和劳动密集的任务。生成式内容创造可以帮助，但通常不能创造整个游戏。进化游戏设计，将进化算法与自动游戏测试结合，已经用于创造了一些简单的桌面游戏，但原始方法并不包括复杂的桌面游戏，如骰子、牌和地图。本工作提出了对桌面游戏的扩展，通过生成 variants of Risk，一款军事策略游戏，要求玩家征服地图区域以赢得。我们使用了遗传算法进化选择的参数，以及规则基于的智能客户端来测试游戏，以及多种质量标准来评估新生成的变化。我们的结果显示了创造了原版游戏的新变体，地图较小，比赛更短。此外，新变体具有更平衡的比赛，保持了正常的戏剧。我们还发现了过程中的限制，在许多情况下， objective function 正确追求，但生成的游戏几乎是无聊的。这项工作开启了对进化游戏设计在古典桌面游戏之外的探索。
</details></li>
</ul>
<hr>
<h2 id="Improved-Bayesian-Regret-Bounds-for-Thompson-Sampling-in-Reinforcement-Learning"><a href="#Improved-Bayesian-Regret-Bounds-for-Thompson-Sampling-in-Reinforcement-Learning" class="headerlink" title="Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning"></a>Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20007">http://arxiv.org/abs/2310.20007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmadreza Moradipari, Mohammad Pedramfar, Modjtaba Shokrian Zini, Vaneet Aggarwal</li>
<li>for: 这个论文是为了证明 Thompson Sampling 在强化学习中的首个 Bayesian  regret bound。</li>
<li>methods: 这个论文使用了一种简化学习问题的离散集合环境，并通过 posterior consistency 进行了精细的信息率分析。</li>
<li>results: 这个论文得到了时间不同束的强化学习问题中的上界，其上界为 $\widetilde{O}(H\sqrt{d_{l_1}T})$，其中 $H$ 是 episode length，$d_{l_1}$ 是环境空间的 Kolmogorov $l_1-$ 度量。<details>
<summary>Abstract</summary>
In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\widetilde{O}(H\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们证明了决策者抽取（Thompson Sampling）在强化学习中的第一个悔弃 regret bounds。我们通过简化学习问题，使用离散的代理环境集，并对信息倍数进行细化分析，从而得到时间不同权重学习问题中的上界为$\widetilde{O}(H\sqrt{d_{l_1}T})$，其中$H$是话语长度，$d_{l_1}$是环境空间的科尔莫戈罗夫-$l_1-$度量。然后，我们在各种设置下获得具体的$d_{l_1}$bounds，包括表格、线性和finite mixtures，并讨论了我们的结果如何超越现有的最佳成果。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Limits-of-Learned-Local-Search-Heuristics-Are-You-the-Mightiest-of-the-Meek"><a href="#Unveiling-the-Limits-of-Learned-Local-Search-Heuristics-Are-You-the-Mightiest-of-the-Meek" class="headerlink" title="Unveiling the Limits of Learned Local Search Heuristics: Are You the Mightiest of the Meek?"></a>Unveiling the Limits of Learned Local Search Heuristics: Are You the Mightiest of the Meek?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19990">http://arxiv.org/abs/2310.19990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankur Nath, Alan Kuhnle</li>
<li>for: 这个论文旨在探讨 combine neural networks with local search heuristics 在 combinatorial optimization 领域的实践中的问题。</li>
<li>methods: 这个论文使用的方法包括 Tabu Search 和 deep learning 等多种方法。</li>
<li>results: 研究发现，一个简单的学习基于 Tabu Search 的规则可以超越当前最佳学习规则，并且具有更高的性能和普适性。<details>
<summary>Abstract</summary>
In recent years, combining neural networks with local search heuristics has become popular in the field of combinatorial optimization. Despite its considerable computational demands, this approach has exhibited promising outcomes with minimal manual engineering. However, we have identified three critical limitations in the empirical evaluation of these integration attempts. Firstly, instances with moderate complexity and weak baselines pose a challenge in accurately evaluating the effectiveness of learning-based approaches. Secondly, the absence of an ablation study makes it difficult to quantify and attribute improvements accurately to the deep learning architecture. Lastly, the generalization of learned heuristics across diverse distributions remains underexplored. In this study, we conduct a comprehensive investigation into these identified limitations. Surprisingly, we demonstrate that a simple learned heuristic based on Tabu Search surpasses state-of-the-art (SOTA) learned heuristics in terms of performance and generalizability. Our findings challenge prevailing assumptions and open up exciting avenues for future research and innovation in combinatorial optimization.
</details>
<details>
<summary>摘要</summary>
Recently, combining neural networks with local search heuristics has become popular in the field of combinatorial optimization. Despite its considerable computational demands, this approach has shown promising results with minimal manual engineering. However, we have identified three critical limitations in the empirical evaluation of these integration attempts. Firstly, instances with moderate complexity and weak baselines pose a challenge in accurately evaluating the effectiveness of learning-based approaches. Secondly, the absence of an ablation study makes it difficult to quantify and attribute improvements accurately to the deep learning architecture. Lastly, the generalization of learned heuristics across diverse distributions remains underexplored. In this study, we conduct a comprehensive investigation into these identified limitations. Surprisingly, we demonstrate that a simple learned heuristic based on Tabu Search surpasses state-of-the-art (SOTA) learned heuristics in terms of performance and generalizability. Our findings challenge prevailing assumptions and open up exciting avenues for future research and innovation in combinatorial optimization.Translation in Simplified Chinese:近些年来，将神经网络与本地搜索规则结合在一起已成为 combinatorial optimization 领域的 популяр趋势。尽管它们的计算需求相对较高，但这种方法在 minimal 的人工工程下已经展现出了扎心的成果。然而，我们在实证评估中发现了三个关键的限制。首先，有中等复杂度和弱基线的实例会增加评估学习基于方法的准确性问题。其次，缺乏抽象研究使得准确地归因改进到深度学习架构很困难。最后，学习基于分布的搜索规则的总体化仍未得到充分的探索。在这个研究中，我们通过对这些已知的限制进行全面的调查和分析，并 surprisingly 发现一种简单的学习基于 Tabu Search 的规则，在性能和总体化方面超越了当前的学习基于方法。我们的发现推翻了先前的假设，开 up 了未来研究和创新的潜在空间。
</details></li>
</ul>
<hr>
<h2 id="BioInstruct-Instruction-Tuning-of-Large-Language-Models-for-Biomedical-Natural-Language-Processing"><a href="#BioInstruct-Instruction-Tuning-of-Large-Language-Models-for-Biomedical-Natural-Language-Processing" class="headerlink" title="BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing"></a>BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19975">http://arxiv.org/abs/2310.19975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hieu Tran, Zhichao Yang, Zonghai Yao, Hong Yu</li>
<li>for: 本研究旨在提高生物医学自然语言处理（BioNLP）领域中大语言模型（LLMs）的性能，通过特定任务的指令特有数据集（BioInstruct）进行调整。</li>
<li>methods: 本研究使用GPT-4语言模型生成了超过25,000个例子的自然语言指令数据集（BioInstruct），并通过这些指令进行LLMs的微调。</li>
<li>results: 通过BioInstruct数据集的微调，我们可以提高LLMs在BioNLP应用中的性能，包括信息抽取、问答和文本生成等。此外，我们还发现了多任务学习原则如何帮助指令的贡献。<details>
<summary>Abstract</summary>
Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&2, 7B\&13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principles.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在许多自然语言处理（NLP）任务中获得了很大的成功。这是通过预训练LLMs在庞大数据量上并在特定领域进行调整而实现的。然而，只有很少的指令在生物医学领域发布。为解决这个问题，我们介绍了 BioInstruct，一个自定义任务特定的指令集合，包含超过25,000个示例。这个数据集是通过提示一个GPT-4语言模型三个种子样本的80个人类批准的指令来生成的。通过使用BioInstruct数据集进行训练LLMs，我们 hoping to 优化LLMs在生物医学自然语言处理（BioNLP）中的表现。我们在LLaMA LLMs（1&2, 7B&13B）上进行了指令调整，并对其进行了BioNLP应用程序的评估，包括信息提取、问答和文本生成。我们还评估了指令对模型性能的贡献，使用多任务学习原理。
</details></li>
</ul>
<hr>
<h2 id="ExPT-Synthetic-Pretraining-for-Few-Shot-Experimental-Design"><a href="#ExPT-Synthetic-Pretraining-for-Few-Shot-Experimental-Design" class="headerlink" title="ExPT: Synthetic Pretraining for Few-Shot Experimental Design"></a>ExPT: Synthetic Pretraining for Few-Shot Experimental Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19961">http://arxiv.org/abs/2310.19961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tung Nguyen, Sudhanshu Agrawal, Aditya Grover</li>
<li>for: 本研究目的是解决实验设计中的样本效率问题，因为现实中的实验评估需要耗费时间、金钱和安全成本。</li>
<li>methods: 本文使用一种名为Experiment Pretrained Transformers（ExPT）的基本模型，这是一种基于环境学习的减少样本数据集合的方法。</li>
<li>results: 研究表明，ExPT在减少样本数据集合的情况下可以达到更高的性能和普适性，并且在各种复杂的实验设计任务中表现出色。<details>
<summary>Abstract</summary>
Experimental design is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of few-shot experimental design, where only a few labeled data points of input designs and their corresponding values are available. We approach this problem as a conditional generation task, where a model conditions on a few labeled examples and the desired output to generate an optimal input design. To this end, we introduce Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that employs a novel combination of synthetic pretraining with in-context learning. In ExPT, we only assume knowledge of a finite collection of unlabelled data points from the input domain and pretrain a transformer neural network to optimize diverse synthetic functions defined over this domain. Unsupervised pretraining allows ExPT to adapt to any design task at test time in an in-context fashion by conditioning on a few labeled data points from the target task and generating the candidate optima. We evaluate ExPT on few-shot experimental design in challenging domains and demonstrate its superior generality and performance compared to existing methods. The source code is available at https://github.com/tung-nd/ExPT.git.
</details>
<details>
<summary>摘要</summary>
实验设计是许多科学和工程领域的基本问题。在这个问题中，sample efficiency是非常重要，因为实验评估的时间、钱和安全成本都是非常高昂的。现有的方法都是靠活的数据收集或者有大量的过去实验标签数据来进行，这些方法在实际 scenarios 中是不实际的。在这个工作中，我们处理更加问题的设计问题，其中只有几个标签的输入设计和其对应的值是可用的。我们这个问题作为一个 conditional generation 任务，我们的模型将根据几个标签的例子和目标值来生成最佳的输入设计。为了实现这个目标，我们引入 Experiment Pretrained Transformers（ExPT），一个基于 transformer 神经网络的基础模型，它使用了一种新的组合方法，将 synthetic pretraining 与 in-context learning 相结合。在 ExPT 中，我们仅仅假设知道输入领域中的一个finite collection 的无标例子，并将 transformer 神经网络预训来优化这个领域上的多个无相关函数。无标预训 позволяет ExPT 在测试时以内容的方式适应任务，通过根据几个标签的目标值和目标值来获得候选的最佳设计。我们评估 ExPT 在几 shot 实验设计中的一般性和表现，并证明它在挑战性的领域中比现有的方法表现更好。请见 https://github.com/tung-nd/ExPT.git 的源代码。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Spatiotemporal-Big-Data-A-Vision-on-Opportunities-and-Challenges"><a href="#Deep-Learning-for-Spatiotemporal-Big-Data-A-Vision-on-Opportunities-and-Challenges" class="headerlink" title="Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and Challenges"></a>Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19957">http://arxiv.org/abs/2310.19957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Jiang</li>
<li>for: 本研究旨在探讨深度学习技术在 espacial 大数据领域中的应用，以及这些技术在处理不同类型的 espacial 大数据时的挑战和未来研究需求。</li>
<li>methods: 本研究使用了 Earth 图像大数据等多种 espacial 大数据，并应用了深度学习技术来解决各种陆地覆盖和陆地使用模型化任务。</li>
<li>results: 本研究描述了 espacial 大数据的特点和深度学习技术在这些数据上的应用，并提出了未来研究中需要解决的一些挑战。<details>
<summary>Abstract</summary>
With advancements in GPS, remote sensing, and computational simulation, an enormous volume of spatiotemporal data is being collected at an increasing speed from various application domains, spanning Earth sciences, agriculture, smart cities, and public safety. Such emerging geospatial and spatiotemporal big data, coupled with recent advances in deep learning technologies, foster new opportunities to solve problems that have not been possible before. For instance, remote sensing researchers can potentially train a foundation model using Earth imagery big data for numerous land cover and land use modeling tasks. Coastal modelers can train AI surrogates to speed up numerical simulations. However, the distinctive characteristics of spatiotemporal big data pose new challenges for deep learning technologies. This vision paper introduces various types of spatiotemporal big data, discusses new research opportunities in the realm of deep learning applied to spatiotemporal big data, lists the unique challenges, and identifies several future research needs.
</details>
<details>
<summary>摘要</summary>
With the advancements in GPS, remote sensing, and computational simulation, an enormous amount of spatiotemporal data is being collected at an increasing speed from various application domains, including Earth sciences, agriculture, smart cities, and public safety. This emerging geospatial and spatiotemporal big data, combined with recent advances in deep learning technologies, has opened up new opportunities to solve problems that were previously unsolvable. For example, remote sensing researchers can potentially train a foundation model using Earth imagery big data for numerous land cover and land use modeling tasks. Coastal modelers can train AI surrogates to speed up numerical simulations. However, the unique characteristics of spatiotemporal big data pose new challenges for deep learning technologies. This vision paper introduces various types of spatiotemporal big data, discusses new research opportunities in the realm of deep learning applied to spatiotemporal big data, lists the unique challenges, and identifies several future research needs.Here's the translation of the text in Traditional Chinese:随着GPS、远程感知和计算 simulated的进步，各个应用领域产生了巨量的空间时间数据，包括地球科学、农业、智能城市和公共安全。这些emerging geospatial和空间时间大数据，与最近的深度学习技术的进步，带来了新的机会，例如：将地球图像大数据用于多种土地覆盖和土地使用模型任务的对称基模型训练。海岸模型师可以将AI参数器训练为加速numerical simulations。然而， espaciotemporal big data的特有特征对深度学习技术提出了新的挑战。本 vision paper introduces various types of espaciotemporal big data, discusses new research opportunities in the realm of deep learning applied to espaciotemporal big data, lists the unique challenges, and identifies several future research needs.
</details></li>
</ul>
<hr>
<h2 id="Conditional-Unscented-Autoencoders-for-Trajectory-Prediction"><a href="#Conditional-Unscented-Autoencoders-for-Trajectory-Prediction" class="headerlink" title="Conditional Unscented Autoencoders for Trajectory Prediction"></a>Conditional Unscented Autoencoders for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19944">http://arxiv.org/abs/2310.19944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boschresearch/cuae-prediction">https://github.com/boschresearch/cuae-prediction</a></li>
<li>paper_authors: Faris Janjoš, Marcel Hallgarten, Anthony Knittel, Maxim Dolgov, Andreas Zell, J. Marius Zöllner</li>
<li>for: 这篇论文的目的是挑战 \ac{CVAE} 中的一些关键 комponent，并提出改进方案以提高预测性能。</li>
<li>methods: 本论文使用了最近的 VAE 技术，包括不可应用 sampling 和更Structured mixture latent space，以及一种新的、可能更表达ive的推论方法。</li>
<li>results: 试验结果显示，我们的模型在 INTERACTION 预测 dataset 上表现出色，超过了现有的州检验标准，并在 CelebA  dataset 上进行图像模型化任务上也超过了基本的 vanilla CVAE。<details>
<summary>Abstract</summary>
The \ac{CVAE} is one of the most widely-used models in trajectory prediction for \ac{AD}. It captures the interplay between a driving context and its ground-truth future into a probabilistic latent space and uses it to produce predictions. In this paper, we challenge key components of the CVAE. We leverage recent advances in the space of the VAE, the foundation of the CVAE, which show that a simple change in the sampling procedure can greatly benefit performance. We find that unscented sampling, which draws samples from any learned distribution in a deterministic manner, can naturally be better suited to trajectory prediction than potentially dangerous random sampling. We go further and offer additional improvements, including a more structured mixture latent space, as well as a novel, potentially more expressive way to do inference with CVAEs. We show wide applicability of our models by evaluating them on the INTERACTION prediction dataset, outperforming the state of the art, as well as at the task of image modeling on the CelebA dataset, outperforming the baseline vanilla CVAE. Code is available at https://github.com/boschresearch/cuae-prediction.
</details>
<details>
<summary>摘要</summary>
《CVAE》是自驾报道预测领域中最广泛使用的模型之一。它捕捉了驾驶Context和其真实未来的关系，并将其转化为一个 probabilistic 的latent space，以生成预测。在这篇论文中，我们挑战了CVAE的关键组件。我们利用了最近的 VAE 的进步，CVAE 的基础，发现一种简单的改变抽样方法可以大幅提高性能。我们发现， deterministic 抽样（unscented sampling）可以更好地适应 trajectory prediction than potentially dangerous random sampling。我们还提供了其他改进，包括一种更结构化的混合幂space，以及一种可能更具表达力的CVAE inference方法。我们通过评估 INTERACTION prediction 数据集和 CelebA 图像模型任务，证明了我们的模型在各个领域中的广泛适用性，并超越了状态前的最佳性能。代码可以在 https://github.com/boschresearch/cuae-prediction 中找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-Few-Annotation-Learning-for-Object-Detection-Are-Transformer-based-Models-More-Efficient"><a href="#Towards-Few-Annotation-Learning-for-Object-Detection-Are-Transformer-based-Models-More-Efficient" class="headerlink" title="Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?"></a>Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19936">http://arxiv.org/abs/2310.19936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cea-list/mt-detr">https://github.com/cea-list/mt-detr</a></li>
<li>paper_authors: Quentin Bouniot, Angélique Loesch, Romaric Audigier, Amaury Habrard</li>
<li>for: 这个研究是为了提出一种适用于现有最佳物体检测器 Deformable DETR 的几shot 和半supervised 学习设置的 semi-supervised 方法。</li>
<li>methods: 本研究使用了一个学生-教师架构，避免依赖学生模型中的敏感后处理 Pseudo-labels。</li>
<li>results: 在 COCO 和 Pascal VOC 的半supervised 物体检测 benchmark 上评估了我们的方法，与之前的方法比较，特别是当标签少时表现更好。我们认为我们的贡献开启了新的可能性，以适应类似的物体检测方法。<details>
<summary>Abstract</summary>
For specialized and dense downstream tasks such as object detection, labeling data requires expertise and can be very expensive, making few-shot and semi-supervised models much more attractive alternatives. While in the few-shot setup we observe that transformer-based object detectors perform better than convolution-based two-stage models for a similar amount of parameters, they are not as effective when used with recent approaches in the semi-supervised setting. In this paper, we propose a semi-supervised method tailored for the current state-of-the-art object detector Deformable DETR in the few-annotation learning setup using a student-teacher architecture, which avoids relying on a sensitive post-processing of the pseudo-labels generated by the teacher model. We evaluate our method on the semi-supervised object detection benchmarks COCO and Pascal VOC, and it outperforms previous methods, especially when annotations are scarce. We believe that our contributions open new possibilities to adapt similar object detection methods in this setup as well.
</details>
<details>
<summary>摘要</summary>
特别是在特殊和紧张的下游任务，如物体检测，标注数据需要专家知识和成本很高，使得几步和半supervised模型变得非常吸引人。在几步设置下，我们发现基于转换器的物体检测器比基于 convolution的两stage模型在同等参数量下表现更好。然而，在 semi-supervised 设置下，它们不那么有效。在这篇论文中，我们提出了一种针对当前领先的物体检测器Deformable DETR在几步学习设置中使用学生-教师架构的半supervised方法。我们避免了依赖于敏感的后处理 pseudo-labels 生成于教师模型。我们在 COCO 和 Pascal VOC  semi-supervised 物体检测数据集上评估了我们的方法，并与之前的方法相比，它在标注稀缺的情况下表现更好。我们认为，我们的贡献打开了新的可能性，使得类似的物体检测方法在这种设置中也可以适应。
</details></li>
</ul>
<hr>
<h2 id="Model-Based-Reparameterization-Policy-Gradient-Methods-Theory-and-Practical-Algorithms"><a href="#Model-Based-Reparameterization-Policy-Gradient-Methods-Theory-and-Practical-Algorithms" class="headerlink" title="Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms"></a>Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19927">http://arxiv.org/abs/2310.19927</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/agentification/rp_pgm">https://github.com/agentification/rp_pgm</a></li>
<li>paper_authors: Shenao Zhang, Boyi Liu, Zhaoran Wang, Tuo Zhao</li>
<li>for: 该 paper 是为了研究 model-based ReParameterization Policy Gradient Methods (RP PGMs) 在长期 reinforcement learning 问题中的应用和优化问题。</li>
<li>methods: 该 paper 使用了 theoretical 和 experimental 方法来研究 RP PGMs 的整体性和优化问题，并提出了spectral normalization 方法来缓解长模型拓展导致的潜在梯度变iance问题。</li>
<li>results: 实验结果表明，采用 spectral normalization 方法可以有效缓解梯度变iance问题，并且提高了 RP PGMs 的性能，与其他梯度估计器（如 likelihood Ratio 梯度估计器）相当或更高。<details>
<summary>Abstract</summary>
ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding variance issue caused by long model unrolls. Our experimental results demonstrate that proper normalization significantly reduces the gradient variance of model-based RP PGMs. As a result, the performance of the proposed method is comparable or superior to other gradient estimators, such as the Likelihood Ratio (LR) gradient estimator. Our code is available at https://github.com/agentification/RP_PGM.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable("ReParameterization（RP）Policy Gradient Methods（PGMs）have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding variance issue caused by long model unrolls. Our experimental results demonstrate that proper normalization significantly reduces the gradient variance of model-based RP PGMs. As a result, the performance of the proposed method is comparable or superior to other gradient estimators, such as the Likelihood Ratio（LR）gradient estimator. Our code is available at https://github.com/agentification/RP_PGM.）Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Jina-Embeddings-2-8192-Token-General-Purpose-Text-Embeddings-for-Long-Documents"><a href="#Jina-Embeddings-2-8192-Token-General-Purpose-Text-Embeddings-for-Long-Documents" class="headerlink" title="Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents"></a>Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19923">http://arxiv.org/abs/2310.19923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao</li>
<li>for: The paper is written for researchers and practitioners working on text embedding models, particularly those interested in developing models that can handle long documents.</li>
<li>methods: The paper introduces Jina Embeddings 2, an open-source text embedding model that can accommodate up to 8192 tokens, which is much longer than the conventional 512-token limit. The model uses a novel combination of techniques to achieve state-of-the-art performance on a range of embedding-related tasks.</li>
<li>results: The paper reports that Jina Embeddings 2 achieves performance on par with OpenAI’s proprietary ada-002 model on the MTEB benchmark, and that an extended context can enhance performance in tasks such as NarrativeQA.<details>
<summary>Abstract</summary>
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.   To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only achieves state-of-the-art performance on a range of embedding-related tasks in the MTEB benchmark but also matches the performance of OpenAI's proprietary ada-002 model. Additionally, our experiments indicate that an extended context can enhance performance in tasks such as NarrativeQA.
</details>
<details>
<summary>摘要</summary>
文本嵌入模型已经成为强大工具，可以将句子转换成固定大小的特征向量，捕捉 semantic 信息。而这些模型在信息检索、semantic 聚合和文本重新排序等任务中是必备的，但现有的大多数开源模型，特别是基于 BERT 的模型，在处理长文档时很难表现，通常会导致 truncation。为了解决这个挑战，我们介绍 Jina Embeddings 2，一个可以处理 Up to 8192 个字的开源文本嵌入模型。这个模型不仅在 MTEB 竞赛中表现出优于 Convention 512 个字的限制，还能够高效地处理长文档。 Jina Embeddings 2 不仅达到了一系列嵌入相关任务的状态 искусственный智能表现，还与 OpenAI 的专有 ada-002 模型匹配。此外，我们的实验表明，扩展上下文可以提高 NarrativeQA 等任务的表现。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Bias-and-Inequities-A-Systematic-Review-of-Bias-Detection-and-Mitigation-in-Healthcare-Artificial-Intelligence-Using-Electronic-Health-Records"><a href="#Unmasking-Bias-and-Inequities-A-Systematic-Review-of-Bias-Detection-and-Mitigation-in-Healthcare-Artificial-Intelligence-Using-Electronic-Health-Records" class="headerlink" title="Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records"></a>Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19917">http://arxiv.org/abs/2310.19917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Chen, Liqin Wang, Julie Hong, Jiaqi Jiang, Li Zhou</li>
<li>for: 这项研究的目的是系统性地查询利用电子医疗记录（EHR）数据的人工智能（AI）应用中的偏见问题。</li>
<li>methods: 这项研究采用了遵循PRISMA指南的系统性回顾方法，从PubMed、Web of Science和IEEE检索到252篇文章，并对其中的20篇文章进行了最终审查。</li>
<li>results: 这项研究发现，在20篇文章中，5种主要的偏见问题得到了覆盖，即8篇文章分析了选择偏见问题，6篇文章分析了隐式偏见问题，5篇文章分析了干扰偏见问题，4篇文章分析了计量偏见问题，2篇文章分析了算法偏见问题。在偏见处理方法方面，10篇文章在模型开发阶段发现了偏见问题，而17篇文章提出了避免偏见问题的方法。<details>
<summary>Abstract</summary>
Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten studies identified bias during model development, while seventeen presented methods to mitigate the bias. Discussion: Bias may infiltrate the AI application development process at various stages. Although this review discusses methods for addressing bias at different development stages, there is room for implementing additional effective approaches. Conclusion: Despite growing attention to bias in healthcare AI, research using EHR data on this topic is still limited. Detecting and mitigating AI bias with EHR data continues to pose challenges. Further research is needed to raise a standardized method that is generalizable and interpretable to detect, mitigate and evaluate bias in medical AI.
</details>
<details>
<summary>摘要</summary>
目的：人工智能（AI）应用使用电子健康纪录（EHR）得到了广泛的应用，但也会产生不同类型的偏见。本研究目的是系统性地对EHR数据使用AI研究中的偏见进行评估。方法：按照Preferred Reporting Items for Systematic Reviews and Meta-analyses（PRISMA）指南进行系统性综述。我们从2010年1月1日到2022年10月31日 retrievePubMed、Web of Science和Institute of Electrical and Electronics Engineers上的文章。我们定义了六种主要的偏见类型，并summarized现有的偏见处理方法。结果：从252篇文章中，20篇符合包含期刊的要求，进行了最终审查。八种偏见中，八种是选择偏见；六种是隐式偏见；五种是混合偏见；四种是测量偏见；两种是算法偏见。对偏见处理方法，十篇文章在模型开发阶段检测了偏见，而十七篇文章提出了避免偏见的方法。讨论：偏见可能在AI应用开发过程中各个阶段偏见。虽然这篇文章讨论了在不同阶段检测和避免偏见的方法，但还需要实施更多有效的方法。结论：尽管健康AI中的偏见问题已经得到了越来越多的关注，但使用EHR数据进行的研究还是有限的。检测和避免EHR数据上的AI偏见还需要继续进行更多的研究。为了提高AI医疗应用的标准化方法，需要采用一种通用、可读性高的方法来检测、避免和评估偏见。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Prototype-based-Graph-Information-Bottleneck"><a href="#Interpretable-Prototype-based-Graph-Information-Bottleneck" class="headerlink" title="Interpretable Prototype-based Graph Information Bottleneck"></a>Interpretable Prototype-based Graph Information Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19906">http://arxiv.org/abs/2310.19906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sang-woo-seo/pgib">https://github.com/sang-woo-seo/pgib</a></li>
<li>paper_authors: Sangwoo Seo, Sungwon Kim, Chanyoung Park</li>
<li>for: 这个论文的目的是提出一种可解释的图 нейрон网络（PGIB）框架，用于提高图 нейрон网络的解释性和性能。</li>
<li>methods: 这个论文使用了prototype学习和信息瓶颈框架，从输入图中提取关键子图，并通过这些关键子图来提供可解释的预测结果。</li>
<li>results: 对比其他状态速法，PGIB在预测性能和解释性两个方面均表现出色，并且通过质量分析得到了证明。<details>
<summary>Abstract</summary>
The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph that is important for the model prediction. This is the first work that incorporates prototype learning into the process of identifying the key subgraphs that have a critical impact on the prediction performance. Extensive experiments, including qualitative analysis, demonstrate that PGIB outperforms state-of-the-art methods in terms of both prediction performance and explainability.
</details>
<details>
<summary>摘要</summary>
graph TD成功的图 neuronal networks (GNNs) 带来了理解它们的决策过程和提供对其预测的解释，这 hath led to explainable AI (XAI) 提供了透明的解释 для黑色 Box 模型。 在最近，使用 prototype 已成功地提高了模型的解释性，通过学习 prototype 来Imply training graphs that affect the prediction。然而，这些approaches 往往提供 prototype 中过度的信息，从整个图中获取信息，导致遗漏关键子结构或包含无关信息，这可能会限制模型在下游任务中的解释性和性能。在这项工作中，我们提出了一种新的解释 GNN 框架，called interpretable Prototype-based Graph Information Bottleneck (PGIB)。PGIB 在信息瓶颈框架中 incorporates prototype learning 来提供 key subgraph 从输入图中对模型预测的重要性。这是首次 incorporates prototype learning 到 identify 预测性能的关键子图的过程中。extensive experiments, including qualitative analysis, show that PGIB outperforms state-of-the-art methods in terms of both prediction performance and explainability.
</details></li>
</ul>
<hr>
<h2 id="Herd-Using-multiple-smaller-LLMs-to-match-the-performances-of-proprietary-large-LLMs-via-an-intelligent-composer"><a href="#Herd-Using-multiple-smaller-LLMs-to-match-the-performances-of-proprietary-large-LLMs-via-an-intelligent-composer" class="headerlink" title="Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer"></a>Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19902">http://arxiv.org/abs/2310.19902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surya Narayanan Hari, Matt Thomson</li>
<li>for: 这篇论文的目的是解决现有的LLM模型在实际应用中的访问和扩展问题，以及对于这些模型的性能评估。</li>
<li>methods: 这篇论文使用了开源模型库和智能路由器来组织和选择合适的LLM模型，以提高其性能和可靠性。</li>
<li>results: 论文表明，一个由多种开源模型组成的“牧场”可以与商业模型匹配或超越其性能，而且这些模型的大小比商业模型要小得多。此外，当GPT无法回答查询时，“牧场”可以识别一个能够回答查询的模型，超过40%的时间。<details>
<summary>Abstract</summary>
Currently, over a thousand LLMs exist that are multi-purpose and are capable of performing real world tasks, including Q&A, text summarization, content generation, etc. However, accessibility, scale and reliability of free models prevents them from being widely deployed in everyday use cases. To address the first two issues of access and scale, organisations such as HuggingFace have created model repositories where users have uploaded model weights and quantized versions of models trained using different paradigms, as well as model cards describing their training process. While some models report performance on commonly used benchmarks, not all do, and interpreting the real world impact of trading off performance on a benchmark for model deployment cost, is unclear. Here, we show that a herd of open source models can match or exceed the performance of proprietary models via an intelligent router. We show that a Herd of open source models is able to match the accuracy of ChatGPT, despite being composed of models that are effectively 2.5x smaller. We show that in cases where GPT is not able to answer the query, Herd is able to identify a model that can, at least 40% of the time.
</details>
<details>
<summary>摘要</summary>
Here, we show that a herd of open-source models can match or exceed the performance of proprietary models via an intelligent router. Specifically, we show that a herd of open-source models is able to match the accuracy of ChatGPT, despite being composed of models that are effectively 2.5 times smaller. Additionally, we show that in cases where GPT is not able to answer a query, the herd is able to identify a model that can, at least 40% of the time.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Geometry-of-Blind-Spots-in-Vision-Models"><a href="#Exploring-Geometry-of-Blind-Spots-in-Vision-Models" class="headerlink" title="Exploring Geometry of Blind Spots in Vision Models"></a>Exploring Geometry of Blind Spots in Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19889">http://arxiv.org/abs/2310.19889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SriramB-98/blindspots-geometry">https://github.com/SriramB-98/blindspots-geometry</a></li>
<li>paper_authors: Sriram Balasubramanian, Gaurang Sriramanan, Vinu Sankar Sadasivan, Soheil Feizi</li>
<li>for: 这种研究旨在探讨深度神经网络在视觉任务中的不敏感性问题，即大 magnitude 的输入变换不会导致网络活动变化。</li>
<li>methods: 该研究使用了Level Set Traversal算法，通过探索输入空间中的高确idence区域，以找到与源图像相似但属于其他类别的输入图像。</li>
<li>results: 研究发现，深度神经网络的等Confidence水平集在输入空间中存在星型结构，而且可以使用高确idence路径连接这些等Confidence水平集。此外，研究还评估了这些连接的高维空间范围。code可以在<a target="_blank" rel="noopener" href="https://github.com/SriramB-98/blindspots-neurips-sub%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/SriramB-98/blindspots-neurips-sub上获取。</a><details>
<summary>Abstract</summary>
Despite the remarkable success of deep neural networks in a myriad of settings, several works have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as adversarial attacks. On the other hand, prior works have also observed that deep networks can be under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable changes to network activations. In this work, we study in detail the phenomenon of under-sensitivity in vision models such as CNNs and Transformers, and present techniques to study the geometry and extent of "equi-confidence" level sets of such networks. We propose a Level Set Traversal algorithm that iteratively explores regions of high confidence with respect to the input space using orthogonal components of the local gradients. Given a source image, we use this algorithm to identify inputs that lie in the same equi-confidence level set as the source image despite being perceptually similar to arbitrary images from other classes. We further observe that the source image is linearly connected by a high-confidence path to these inputs, uncovering a star-like structure for level sets of deep networks. Furthermore, we attempt to identify and estimate the extent of these connected higher-dimensional regions over which the model maintains a high degree of confidence. The code for this project is publicly available at https://github.com/SriramB-98/blindspots-neurips-sub
</details>
<details>
<summary>摘要</summary>
尽管深度神经网络在各种设置中表现出了惊人的成功，但是一些研究表明，这些神经网络对于微不足的干扰（ adversarial attacks）具有极高的敏感性。然而，也有一些研究发现，深度网络可能具有不够敏感的问题，即在输入空间中的大规模干扰不会导致神经网络的活动变化。在这种情况下，我们对视力模型，如CNNs和Transformers，进行了详细的研究，并提出了一些技术来研究这些神经网络的几何结构和扩展。我们提出了一种Level Set Traversal算法，该算法可以在输入空间中循环探索高信任级别的区域，并使用本地梯度的正交分量来探索这些区域。给定一个源图像，我们使用这种算法来找到与源图像在输入空间中的同一个等信任水平集的输入图像，并观察到这些输入图像与源图像之间存在一条直线连接，揭示了深度网络的等信任水平集具有星型结构。此外，我们尝试了为这些相关的更高维度区域的扩展，以便更好地了解深度网络在它们中的行为。相关代码可以在https://github.com/SriramB-98/blindspots-neurips-sub上获取。
</details></li>
</ul>
<hr>
<h2 id="DEFT-Dexterous-Fine-Tuning-for-Real-World-Hand-Policies"><a href="#DEFT-Dexterous-Fine-Tuning-for-Real-World-Hand-Policies" class="headerlink" title="DEFT: Dexterous Fine-Tuning for Real-World Hand Policies"></a>DEFT: Dexterous Fine-Tuning for Real-World Hand Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19797">http://arxiv.org/abs/2310.19797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adityak77/deft-data">https://github.com/adityak77/deft-data</a></li>
<li>paper_authors: Aditya Kannan, Kenneth Shaw, Shikhar Bahl, Pragna Mannam, Deepak Pathak</li>
<li>for: 本研究旨在探讨人类手部 manipulate 软、可变形物体以及复杂、长期任务中的挑战，以提高机器人 manipulate 的能力。</li>
<li>methods: 本研究提出了一种新的方法，即 DEFT（dexterous fine-tuning for hand policies），它利用人类驱动的假设，通过在实际世界中直接执行来改进这些假设。该方法还包括一种高效的在线优化过程。</li>
<li>results: DEFT 在多个任务中显示出成功，以及一种数据效率的普适的软件 manipulate 路径，用于掌握复杂的 manipulate 任务。您可以通过访问我们的网站 <a target="_blank" rel="noopener" href="https://dexterous-finetuning.github.io/">https://dexterous-finetuning.github.io</a> 查看视频结果。<details>
<summary>Abstract</summary>
Dexterity is often seen as a cornerstone of complex manipulation. Humans are able to perform a host of skills with their hands, from making food to operating tools. In this paper, we investigate these challenges, especially in the case of soft, deformable objects as well as complex, relatively long-horizon tasks. However, learning such behaviors from scratch can be data inefficient. To circumvent this, we propose a novel approach, DEFT (DExterous Fine-Tuning for Hand Policies), that leverages human-driven priors, which are executed directly in the real world. In order to improve upon these priors, DEFT involves an efficient online optimization procedure. With the integration of human-based learning and online fine-tuning, coupled with a soft robotic hand, DEFT demonstrates success across various tasks, establishing a robust, data-efficient pathway toward general dexterous manipulation. Please see our website at https://dexterous-finetuning.github.io for video results.
</details>
<details>
<summary>摘要</summary>
dexterity 常被看作复杂的操作的基石。人们可以通过手部执行许多技能，从制备食物到操作工具。在这篇论文中，我们调查这些挑战，尤其是在软、可变形物体以及复杂、较长时间任务上。然而，从头来学习这些行为可以是数据不fficient。为了缓解这个问题，我们提出了一种新的方法，即 DEFT（手部精细调整 для手指策略），它利用人类驱动的先验知识，直接在实际世界中执行。为了提高这些先验知识，DEFT包括一种高效的在线优化过程。通过人类学习和在线细化，以及软体机械手部，DEFT在多种任务中成功，建立了一条可靠、数据fficient的通路 toward 普遍的手部精细操作。请参考我们网站 <https://dexterous-finetuning.github.io> 查看视频结果。
</details></li>
</ul>
<hr>
<h2 id="Re-evaluating-Retrosynthesis-Algorithms-with-Syntheseus"><a href="#Re-evaluating-Retrosynthesis-Algorithms-with-Syntheseus" class="headerlink" title="Re-evaluating Retrosynthesis Algorithms with Syntheseus"></a>Re-evaluating Retrosynthesis Algorithms with Syntheseus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19796">http://arxiv.org/abs/2310.19796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krzysztof Maziarz, Austin Tripp, Guoqing Liu, Megan Stanley, Shufang Xie, Piotr Gaiński, Philipp Seidl, Marwin Segler</li>
<li>for: 本研究主要目标是提高化学synthesis的计划和评估方法。</li>
<li>methods: 本研究使用了一个名为syntheseus的 benchmarking 库，该库鼓励了best practice的使用，以便对单步和多步 retrosynthesis 算法进行一致的评估。</li>
<li>results: 通过使用syntheseus库进行重新评估，发现了一些之前的 retrosynthesis 算法的排名发生了变化。<details>
<summary>Abstract</summary>
The planning of how to synthesize molecules, also known as retrosynthesis, has been a growing focus of the machine learning and chemistry communities in recent years. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques. To remedy this, we present a benchmarking library called syntheseus which promotes best practice by default, enabling consistent meaningful evaluation of single-step and multi-step retrosynthesis algorithms. We use syntheseus to re-evaluate a number of previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes when evaluated carefully. We end with guidance for future works in this area.
</details>
<details>
<summary>摘要</summary>
“retrosynthesis的规划”在过去几年内，机器学习和化学社区的关注越来越高。尽管表面上看来有稳定的进步，我们认为现有的评价标准和比较方法存在系统性的缺陷。为了解决这个问题，我们提出了一个名为 syntheseus的评价库，它默认采用了最佳实践，使得单步和多步retrosynthesis算法的meaningful评价成为可能。我们使用 syntheseus 重新评估了一些先前的retrosynthesis算法，并发现当仔细评估时，现状的模型的排名发生变化。最后，我们提出了未来这个领域的指导方针。
</details></li>
</ul>
<hr>
<h2 id="Res-Tuning-A-Flexible-and-Efficient-Tuning-Paradigm-via-Unbinding-Tuner-from-Backbone"><a href="#Res-Tuning-A-Flexible-and-Efficient-Tuning-Paradigm-via-Unbinding-Tuner-from-Backbone" class="headerlink" title="Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone"></a>Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19859">http://arxiv.org/abs/2310.19859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang Lv, Yujun Shen, Deli Zhao, Jingren Zhou</li>
<li>for: 本研究旨在提出一种新的参数高效调整方法，以便将大规模基模型传递到下游应用中。</li>
<li>methods: 该方法基于不同的调整策略，通过意图解耦调整器与基模型的关系，使得调整器的设计和学习不再依赖基模型。</li>
<li>results: 对于权重调整和泛化调整等多种调整策略，该方法能够提供更高效的参数调整，并且可以轻松地搭配多种调整策略。经验表明，该方法在推理和生成任务上具有较高的效果和效率。<details>
<summary>Abstract</summary>
Parameter-efficient tuning has become a trend in transferring large-scale foundation models to downstream applications. Existing methods typically embed some light-weight tuners into the backbone, where both the design and the learning of the tuners are highly dependent on the base model. This work offers a new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners from the backbone. With both theoretical and empirical evidence, we show that popular tuning approaches have their equivalent counterparts under our unbinding formulation, and hence can be integrated into our framework effortlessly. Thanks to the structural disentanglement, we manage to free the design of tuners from the network architecture, facilitating flexible combination of various tuning strategies. We further propose a memory-efficient variant of Res-Tuning, where the bypass i.e., formed by a sequence of tuners) is effectively detached from the main branch, such that the gradients are back-propagated only to the tuners but not to the backbone. Such a detachment also allows one-time backbone forward for multi-task inference. Extensive experiments on both discriminative and generative tasks demonstrate the superiority of our method over existing alternatives from the perspectives of efficacy and efficiency. Project page: $\href{https://res-tuning.github.io/}{\textit{https://res-tuning.github.io/}$.
</details>
<details>
<summary>摘要</summary>
大规模基础模型转移到下游应用的Parameter-efficient tuning已成为当前趋势。现有方法通常将轻量级调参器 embedding到后向，其设计和学习均高度依赖于基模型。这项工作提出了一新调参方式，名为Res-Tuning，它意图将调参器解除与后向的绑定。我们通过理论和实验证明，流行的调参方法均有其对应的等价对手在我们的解绑形式下，因此可以轻松地 интеGRATE到我们的框架中。由于结构分离，我们可以在调参器的设计上免除网络架构的限制，实现灵活的调参策略组合。此外，我们还提出了内存效率改进的Res-Tuning变体，其中分支（formed by a sequence of tuners）被有效地分离于主支，使得梯度只回传给调参器而不回传到后向。这种分离还允许一次主支前进 для多任务推理。广泛的实验表明，我们的方法在效果和效率两个角度上都超越了现有的方法。项目页面： $\href{https://res-tuning.github.io/}{\textit{https://res-tuning.github.io/}$。
</details></li>
</ul>
<hr>
<h2 id="SimMMDG-A-Simple-and-Effective-Framework-for-Multi-modal-Domain-Generalization"><a href="#SimMMDG-A-Simple-and-Effective-Framework-for-Multi-modal-Domain-Generalization" class="headerlink" title="SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization"></a>SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19795">http://arxiv.org/abs/2310.19795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/donghao51/simmmdg">https://github.com/donghao51/simmmdg</a></li>
<li>paper_authors: Hao Dong, Ismail Nejjar, Han Sun, Eleni Chatzi, Olga Fink<br>for: 这个研究旨在解决多modal Distribution Generalization (DG) 中的挑战，当模型需要在不同的modalities上缩减到未知的target Distribution。methods: 我们提出了一个简单 yet effective的多modal DG框架，叫做SimMMDG。我们认为将不同modalities的特征映射到同一个嵌入空间会降低模型的通用性。因此，我们提出了在每个modalities中分解特征为modalitiespecific和modalitieshared部分。我们运用了监督式对应学习 modalitieshared特征，以保持它们具有共同性，并对modalitiespecific特征强制距离。此外，我们引入了跨modalities翻译模块，以调整学习的特征。results: 我们的框架理论上得到支持，并在EPIC-Kitchens dataset和我们在本文中介绍的新的Human-Animal-Cartoon (HAC) dataset上实现了强大的多modal DG性能。我们的原始代码和HAC dataset可以在<a target="_blank" rel="noopener" href="https://github.com/donghao51/SimMMDG%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/donghao51/SimMMDG上获得。</a><details>
<summary>Abstract</summary>
In real-world scenarios, achieving domain generalization (DG) presents significant challenges as models are required to generalize to unknown target distributions. Generalizing to unseen multi-modal distributions poses even greater difficulties due to the distinct properties exhibited by different modalities. To overcome the challenges of achieving domain generalization in multi-modal scenarios, we propose SimMMDG, a simple yet effective multi-modal DG framework. We argue that mapping features from different modalities into the same embedding space impedes model generalization. To address this, we propose splitting the features within each modality into modality-specific and modality-shared components. We employ supervised contrastive learning on the modality-shared features to ensure they possess joint properties and impose distance constraints on modality-specific features to promote diversity. In addition, we introduce a cross-modal translation module to regularize the learned features, which can also be used for missing-modality generalization. We demonstrate that our framework is theoretically well-supported and achieves strong performance in multi-modal DG on the EPIC-Kitchens dataset and the novel Human-Animal-Cartoon (HAC) dataset introduced in this paper. Our source code and HAC dataset are available at https://github.com/donghao51/SimMMDG.
</details>
<details>
<summary>摘要</summary>
在实际应用场景中，实现领域泛化（DG）存在重大挑战，因为模型需要泛化到未知目标分布。在多modal的场景中，泛化到未见多Modal的分布呈现更大的挑战，因为不同modalities具有不同的特性。为了解决多modal的领域泛化问题，我们提出了SimMMDG框架，这是一种简单 yet有效的多Modal DG框架。我们认为将不同modalities的特征映射到同一个嵌入空间内会阻碍模型泛化。为此，我们提议将每个modalities的特征分为具有共同特性的特征和具有特定特性的特征。我们使用supervised contrastive学习来确保共同特性，并对特定特性进行距离约束，以促进多Modal特征的多样性。此外，我们还引入了跨Modal翻译模块，以规范学习的特征。我们的框架理论上有良好支持，并在EPIC-Kitchens数据集和我们在本文中介绍的新的人类动物卡通（HAC）数据集上实现了强大的性能。我们的源代码和HAC数据集可以在https://github.com/donghao51/SimMMDG上获取。
</details></li>
</ul>
<hr>
<h2 id="LILO-Learning-Interpretable-Libraries-by-Compressing-and-Documenting-Code"><a href="#LILO-Learning-Interpretable-Libraries-by-Compressing-and-Documenting-Code" class="headerlink" title="LILO: Learning Interpretable Libraries by Compressing and Documenting Code"></a>LILO: Learning Interpretable Libraries by Compressing and Documenting Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19791">http://arxiv.org/abs/2310.19791</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gabegrand/lilo">https://github.com/gabegrand/lilo</a></li>
<li>paper_authors: Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, Jacob Andreas</li>
<li>for: 本研究旨在开发一个基于神经符号学术的代码生成框架，以帮助开发人员快速生成可读可写的代码库。</li>
<li>methods: 本研究使用了大语言模型（LLM）引导的程序生成技术，以及Stitch符号压缩系统来高效地压缩代码。此外，研究还引入了自动文档（AutoDoc）程序，以帮助理解和应用学习抽象。</li>
<li>results: 对三个 inductive 程序生成benchmark进行了评测，并与现有的神经和符号方法进行了比较。研究发现，LILO可以解决更复杂的任务，并学习更加深入的语言知识。<details>
<summary>Abstract</summary>
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synthesis benchmarks for string editing, scene reasoning, and graphics composition. Compared to existing neural and symbolic methods - including the state-of-the-art library learning algorithm DreamCoder - LILO solves more complex tasks and learns richer libraries that are grounded in linguistic knowledge.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）现在在代码生成方面表现出色，但是软件开发中一个关键方面是 refactoring：将代码集成到可重用和易读的库中。在这篇论文中，我们介绍了 LILO，一个神经符号学框架，可以逐步生成、压缩和文档代码，以建立适应特定问题领域的库。LILO将神经网络引导的程序生成与Stitch的符号压缩系统相结合，以实现高效的lambda抽象。为了让这些抽象更易理解，我们引入了自动文档（AutoDoc）过程，可以根据Contextual例子来生成自然语言名称和docstrings。除了提高人类可读性外，我们发现AutoDoc会提高LILO的生成器使用学习的性能。我们对LILO进行了三个induced程序生成benchmark测试，包括字符串编辑、Scene reasoning和图形组合。相比 existed神经和符号方法，包括DreamCoder库学习算法，LILO可以解决更复杂的任务，并学习更加深入的语言知识。
</details></li>
</ul>
<hr>
<h2 id="From-External-to-Swap-Regret-2-0-An-Efficient-Reduction-and-Oblivious-Adversary-for-Large-Action-Spaces"><a href="#From-External-to-Swap-Regret-2-0-An-Efficient-Reduction-and-Oblivious-Adversary-for-Large-Action-Spaces" class="headerlink" title="From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces"></a>From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19786">http://arxiv.org/abs/2310.19786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson, Noah Golowich</li>
<li>for: 这个论文目的是为了提出一种新的减少方法，即将换 regret 转换为外部 regret，以优化 classical  reductions 中的一些不稳定性。</li>
<li>methods: 这个论文使用了一种新的减少方法，即将换 regret 转换为外部 regret，并通过对这种减少方法的分析，得出了一些新的结论。</li>
<li>results: 这个论文的结果表明，当存在一个无外部 regret 算法时，必然也存在一个无换 regret 算法，并且这种无换 regret 算法的性能比 classical  reductions 更好。此外，这个论文还提供了一个新的下界，其表明在某些游戏中，换 regret 的数量必然是 $\tilde\Omega(N&#x2F;\epsilon^2)$ 或者是 exponential in $1&#x2F;\epsilon$。<details>
<summary>Abstract</summary>
We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can employ distributions over experts, showing that the number of rounds must be $\tilde\Omega(N/\epsilon^2)$ or exponential in $1/\epsilon$.   Our reduction implies that, if no-regret learning is possible in some game, then this game must have approximate correlated equilibria, of arbitrarily good approximation. This strengthens the folklore implication of no-regret learning that approximate coarse correlated equilibria exist. Importantly, it provides a sufficient condition for the existence of correlated equilibrium which vastly extends the requirement that the action set is finite, thus answering a question left open by [DG22; Ass+23]. Moreover, it answers several outstanding questions about equilibrium computation and/or learning in games.
</details>
<details>
<summary>摘要</summary>
我们提供了一种新的减少方法，将交换 regret 转化为外部 regret，从而超越了布姆-曼索尔（BM07）和斯托尔-卢戈西（SL05）的经典减少方法，因为它不需要动作空间的Finite。我们证明了，当存在一个无外部 regret 算法时，也一定存在一个无交换 regret 算法。在学习专家建议中，我们的结果表明，可以保证在 $\log(N)^{O(1/\epsilon)}$ 轮后，交换 regret 不超过 $\epsilon$，并且每轮复杂度为 $O(N)$，where $N$ 是专家数量。而布姆-曼索尔和斯托尔-卢戈西的经典减少方法需要 $O(N/\epsilon^2)$ 轮和至少 $\Omega(N^2)$ 每轮复杂度。我们的结果还有一个相关的下界，这下界在对快速反应者和 $\ell_1$ 约束学习者来说，并且可以使用分布来选择专家，显示了轮数必须是 $\tilde\Omega(N/\epsilon^2)$ 或者 exponential in $1/\epsilon$。我们的减少方法表明，如果有一个不 regret 学习是可能的游戏，那么这个游戏一定有approximate correlated equilibria，并且这些 equilibria 的准确程度可以是arbitrarily good。这个结论超越了布姆-曼索尔的结论，因为它不需要动作空间的Finite。此外，我们的结论还回答了一些关于平衡计算和学习的问题。
</details></li>
</ul>
<hr>
<h2 id="CustomNet-Zero-shot-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models"><a href="#CustomNet-Zero-shot-Object-Customization-with-Variable-Viewpoints-in-Text-to-Image-Diffusion-Models" class="headerlink" title="CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models"></a>CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19784">http://arxiv.org/abs/2310.19784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyang Yuan, Mingdeng Cao, Xintao Wang, Zhongang Qi, Chun Yuan, Ying Shan</li>
<li>for: 文章目的是提出一种基于 CustomNet 的对象自定义方法，以实现在文本到图像生成中实现对象的自定义。</li>
<li>methods: 该方法使用了三个重要的技术：1) 3D 新视角合成；2) 对象自定义；3) 文本描述或特定用户定义的图像来控制位置和背景。</li>
<li>results: 该方法可以在不需要测试时间优化的情况下，实现零 instances 的对象自定义，同时具有较好的个体保持和多样性。<details>
<summary>Abstract</summary>
Incorporating a customized object into image generation presents an attractive feature in text-to-image generation. However, existing optimization-based and encoder-based methods are hindered by drawbacks such as time-consuming optimization, insufficient identity preservation, and a prevalent copy-pasting effect. To overcome these limitations, we introduce CustomNet, a novel object customization approach that explicitly incorporates 3D novel view synthesis capabilities into the object customization process. This integration facilitates the adjustment of spatial position relationships and viewpoints, yielding diverse outputs while effectively preserving object identity. Moreover, we introduce delicate designs to enable location control and flexible background control through textual descriptions or specific user-defined images, overcoming the limitations of existing 3D novel view synthesis methods. We further leverage a dataset construction pipeline that can better handle real-world objects and complex backgrounds. Equipped with these designs, our method facilitates zero-shot object customization without test-time optimization, offering simultaneous control over the viewpoints, location, and background. As a result, our CustomNet ensures enhanced identity preservation and generates diverse, harmonious outputs.
</details>
<details>
<summary>摘要</summary>
通过包含自定义对象在图像生成中，文本到图像生成具有吸引人的特点。然而，现有的优化方法和编码器方法受到了一些缺点，如时间消耗优化、保持对象标识不足和广泛的复制效应。为了解决这些局限性，我们介绍了CustomNet，一种新的对象自定义方法，其中Explicitly incorporates 3D新视野合成能力到对象自定义过程中。这种整合使得可以调整空间位置关系和视点，从而生成多样的输出，同时有效地保持对象标识。此外，我们还引入了细腻的设计，使得通过文本描述或特定用户定义的图像来控制位置和背景，超越现有的3D新视野合成方法的限制。我们还利用了更好的数据构建管道，可以更好地处理真实世界中的对象和复杂背景。准备这些设计，我们的方法可以在零时优化下实现无需测试时优化的自定义对象，同时控制视点、位置和背景。因此，我们的CustomNet可以保持对象标识并生成多样、和谐的输出。
</details></li>
</ul>
<hr>
<h2 id="Designing-AI-Support-for-Human-Involvement-in-AI-assisted-Decision-Making-A-Taxonomy-of-Human-AI-Interactions-from-a-Systematic-Review"><a href="#Designing-AI-Support-for-Human-Involvement-in-AI-assisted-Decision-Making-A-Taxonomy-of-Human-AI-Interactions-from-a-Systematic-Review" class="headerlink" title="Designing AI Support for Human Involvement in AI-assisted Decision Making: A Taxonomy of Human-AI Interactions from a Systematic Review"></a>Designing AI Support for Human Involvement in AI-assisted Decision Making: A Taxonomy of Human-AI Interactions from a Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19778">http://arxiv.org/abs/2310.19778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Catalina Gomez, Sue Min Cho, Shichang Ke, Chien-Ming Huang, Mathias Unberath</li>
<li>for: 提高人工智能在决策支持系统中的用户体验，增强人工智能与人类的交互。</li>
<li>methods: 系统atic review of AI-assisted decision making literature，分析105篇论文，提出了一种交互模式分类法，用于描述不同的人工智能交互方式。</li>
<li>results: 现有交互主要是简单的合作模式，报告了相对少的交互功能支持。 taxonomy 能够帮助理解现有决策支持系统中人工智能交互的现状，并促进交互设计的审慎选择。<details>
<summary>Abstract</summary>
Efforts in levering Artificial Intelligence (AI) in decision support systems have disproportionately focused on technological advancements, often overlooking the alignment between algorithmic outputs and human expectations. To address this, explainable AI promotes AI development from a more human-centered perspective. Determining what information AI should provide to aid humans is vital, however, how the information is presented, e. g., the sequence of recommendations and the solicitation of interpretations, is equally crucial. This motivates the need to more precisely study Human-AI interaction as a pivotal component of AI-based decision support. While several empirical studies have evaluated Human-AI interactions in multiple application domains in which interactions can take many forms, there is not yet a common vocabulary to describe human-AI interaction protocols. To address this gap, we describe the results of a systematic review of the AI-assisted decision making literature, analyzing 105 selected articles, which grounds the introduction of a taxonomy of interaction patterns that delineate various modes of human-AI interactivity. We find that current interactions are dominated by simplistic collaboration paradigms and report comparatively little support for truly interactive functionality. Our taxonomy serves as a valuable tool to understand how interactivity with AI is currently supported in decision-making contexts and foster deliberate choices of interaction designs.
</details>
<details>
<summary>摘要</summary>
对于人工智能（AI）在决策支持系统中的应用，各方面的努力都集中在技术进步上，而忽略了算法的人类预期之间的协调。为了解决这个问题，可观察AI的发展方式更加人类中心。决定AI为人类提供什么样的信息是重要的，但是如何呈现这些信息，例如推荐的顺序和寻求解释的方式，也是非常重要的。这为人类AI互动的研究提供了动机，并且发现了多种应用领域中的人类AI互动协议。但是，目前还没有一个通用的语言来描述人类AI互动协议。为了解决这个问题，我们描述了105篇选择的文献的系统性评审结果，并从这些文献中提取了人类AI互动协议的分类。我们发现现有的互动都偏向简单的合作模式，并报告了相对较少的支持真正互动性能。我们的分类可以作为了解人类AI互动在决策context中的支持方式，并且激发人们对互动设计的意识。
</details></li>
</ul>
<hr>
<h2 id="Learn-to-Categorize-or-Categorize-to-Learn-Self-Coding-for-Generalized-Category-Discovery"><a href="#Learn-to-Categorize-or-Categorize-to-Learn-Self-Coding-for-Generalized-Category-Discovery" class="headerlink" title="Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery"></a>Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19776">http://arxiv.org/abs/2310.19776</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sarahrastegar/infosieve">https://github.com/sarahrastegar/infosieve</a></li>
<li>paper_authors: Sarah Rastegar, Hazel Doughty, Cees G. M. Snoek</li>
<li>for: 提出了一种能够在测试时发现未知类别的新方法</li>
<li>methods: 基于优化的思路，对数据实例分配最短类别编码，从而控制类别细分程度</li>
<li>results: 经过实验证明，该方法可以在测试时成功地处理未知类别，并且与现有标准模型进行比较In English, this translates to:</li>
<li>for: Proposed a new method for discovering unknown categories at test time</li>
<li>methods: Based on optimization, assign minimum length category codes to individual data instances to control category granularity</li>
<li>results: Experimental results demonstrate the effectiveness of the method in handling unknown categories at test time, with comparisons to state-of-the-art benchmarks<details>
<summary>Abstract</summary>
In the quest for unveiling novel categories at test time, we confront the inherent limitations of traditional supervised recognition models that are restricted by a predefined category set. While strides have been made in the realms of self-supervised and open-world learning towards test-time category discovery, a crucial yet often overlooked question persists: what exactly delineates a \textit{category}? In this paper, we conceptualize a \textit{category} through the lens of optimization, viewing it as an optimal solution to a well-defined problem. Harnessing this unique conceptualization, we propose a novel, efficient and self-supervised method capable of discovering previously unknown categories at test time. A salient feature of our approach is the assignment of minimum length category codes to individual data instances, which encapsulates the implicit category hierarchy prevalent in real-world datasets. This mechanism affords us enhanced control over category granularity, thereby equipping our model to handle fine-grained categories adeptly. Experimental evaluations, bolstered by state-of-the-art benchmark comparisons, testify to the efficacy of our solution in managing unknown categories at test time. Furthermore, we fortify our proposition with a theoretical foundation, providing proof of its optimality. Our code is available at: \url{https://github.com/SarahRastegar/InfoSieve}.
</details>
<details>
<summary>摘要</summary>
“在试用时探索新的分类ategories，我们面临传统超级vised recognition模型的内在限制。这些模型仅仅受到预先定义的分类category set的限制，而我们则寻求在试用时自动发现新的分类categories。在这篇论文中，我们将分类category视为一个优化问题的最佳解决方案。我们利用这个独特的概念，提出一种新的、效率高且自动化的方法，可以在试用时发现未知的分类categories。我们的方法将实现分类category code的最小长度对个别数据实例的对应，这为我们提供了更好的分类精确度控制，因此我们的模型可以更好地处理细部分类。我们的实验结果，以及与现有的基eline比较，证明了我们的解决方案在处理未知分类ategories的能力。此外，我们还提供了理论基础，证明了我们的方法是最佳的。我们的代码可以在以下连结中找到：https://github.com/SarahRastegar/InfoSieve。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Explainable-Artificial-Intelligence-XAI-2-0-A-Manifesto-of-Open-Challenges-and-Interdisciplinary-Research-Directions"><a href="#Explainable-Artificial-Intelligence-XAI-2-0-A-Manifesto-of-Open-Challenges-and-Interdisciplinary-Research-Directions" class="headerlink" title="Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges and Interdisciplinary Research Directions"></a>Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges and Interdisciplinary Research Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19775">http://arxiv.org/abs/2310.19775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Longo, Mario Brcic, Federico Cabitza, Jaesik Choi, Roberto Confalonieri, Javier Del Ser, Riccardo Guidotti, Yoichi Hayashi, Francisco Herrera, Andreas Holzinger, Richard Jiang, Hassan Khosravi, Freddy Lecue, Gianclaudio Malgieri, Andrés Páez, Wojciech Samek, Johannes Schneider, Timo Speith, Simone Stumpf</li>
<li>for: 本研究旨在探讨透明AI（XAI）的发展和应用，以及相关领域的实际挑战。</li>
<li>methods: 本文使用 manifold learning 和 feature importance 等方法来解释AI模型的决策过程。</li>
<li>results: 本研究提出了27个开问，并分类为9个类别，以便各个领域的研究人员可以共同努力解决XAI领域的挑战。<details>
<summary>Abstract</summary>
As systems based on opaque Artificial Intelligence (AI) continue to flourish in diverse real-world applications, understanding these black box models has become paramount. In response, Explainable AI (XAI) has emerged as a field of research with practical and ethical benefits across various domains. This paper not only highlights the advancements in XAI and its application in real-world scenarios but also addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. Our goal is to put forward a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 27 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.
</details>
<details>
<summary>摘要</summary>
As systems based on opaque Artificial Intelligence (AI) continue to flourish in diverse real-world applications, understanding these black box models has become paramount. In response, Explainable AI (XAI) has emerged as a field of research with practical and ethical benefits across various domains. This paper not only highlights the advancements in XAI and its application in real-world scenarios but also addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. Our goal is to put forward a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 27 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.Translated text in Simplified Chinese:随着透明化人工智能（AI）系统在多个实际应用场景中的普及，理解这些黑盒模型已经成为非常重要。为此，解释AI（XAI）已经成为一种研究领域，具有实用和伦理上的利益。本文不仅探讨了XAI的发展和应用，还Addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. Our goal is to put forward a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 27 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Renaissance-in-Neural-PDE-Solvers"><a href="#Autoregressive-Renaissance-in-Neural-PDE-Solvers" class="headerlink" title="Autoregressive Renaissance in Neural PDE Solvers"></a>Autoregressive Renaissance in Neural PDE Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19763">http://arxiv.org/abs/2310.19763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yolanne Yi Ran Lee</li>
<li>for: 本研究旨在提出一种基于graph neural network的 partial differential equation（PDE）解决方法，以替代传统的束缚方法和Fourier Neural Operator。</li>
<li>methods: 该方法使用了一种名为message passing graph neural network的新型网络架构，通过消息传递机制来实现PDE的解决。</li>
<li>results: 研究表明，该方法可以与或超越传统的PDE解决方法和Fourier Neural Operator在泛化能力和性能上，并且可以更好地处理一些复杂的PDE问题。<details>
<summary>Abstract</summary>
Recent developments in the field of neural partial differential equation (PDE) solvers have placed a strong emphasis on neural operators. However, the paper "Message Passing Neural PDE Solver" by Brandstetter et al. published in ICLR 2022 revisits autoregressive models and designs a message passing graph neural network that is comparable with or outperforms both the state-of-the-art Fourier Neural Operator and traditional classical PDE solvers in its generalization capabilities and performance. This blog post delves into the key contributions of this work, exploring the strategies used to address the common problem of instability in autoregressive models and the design choices of the message passing graph neural network architecture.
</details>
<details>
<summary>摘要</summary>
近期在神经partial differential equation（PDE）解决方法领域，强调神经操作符的发展。然而，布兰德塞特特等在ICLR 2022年发表的论文《消息传递神经PDE解决方法》，重新评估了自动律型模型，并设计了一个可与或超越现有的快扩散 Neil 算法和传统类型PDE解决方法的消息传递图 neural network 架构。本博客文章将探讨这项工作的关键贡献，探讨了自动律型模型中常见的不稳定性问题的解决方案，以及消息传递图 neural network 架构的设计选择。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-and-Defenses-in-Large-Language-Models-Old-and-New-Threats"><a href="#Adversarial-Attacks-and-Defenses-in-Large-Language-Models-Old-and-New-Threats" class="headerlink" title="Adversarial Attacks and Defenses in Large Language Models: Old and New Threats"></a>Adversarial Attacks and Defenses in Large Language Models: Old and New Threats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19737">http://arxiv.org/abs/2310.19737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/schwinnl/llm_embedding_attack">https://github.com/schwinnl/llm_embedding_attack</a></li>
<li>paper_authors: Leo Schwinn, David Dobre, Stephan Günnemann, Gauthier Gidel</li>
<li>for: 本研究旨在解决 neural network 的Robustness问题，尤其是在 natural language processing 领域中，以防止 adversarial attack。</li>
<li>methods: 本研究使用了一些新的方法来评估 robustness，包括 embedding space attacks 和 LLM-specific best practices。</li>
<li>results: 研究发现，without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach。此外， embedding space attacks 也成为了一种可行的威胁模型。<details>
<summary>Abstract</summary>
Over the past decade, there has been extensive research aimed at enhancing the robustness of neural networks, yet this problem remains vastly unsolved. Here, one major impediment has been the overestimation of the robustness of new defense approaches due to faulty defense evaluations. Flawed robustness evaluations necessitate rectifications in subsequent works, dangerously slowing down the research and providing a false sense of security. In this context, we will face substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We provide a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations. Additionally, we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models. Finally, we demonstrate on a recently proposed defense that, without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Large-Language-Models-A-Comprehensive-Survey"><a href="#Evaluating-Large-Language-Models-A-Comprehensive-Survey" class="headerlink" title="Evaluating Large Language Models: A Comprehensive Survey"></a>Evaluating Large Language Models: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19736">http://arxiv.org/abs/2310.19736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tjunlp-lab/awesome-llms-evaluation-papers">https://github.com/tjunlp-lab/awesome-llms-evaluation-papers</a></li>
<li>paper_authors: Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong</li>
<li>For: 评估大语言模型（LLMs）的能力和安全性。* Methods: 分为三类：知识和能力评估、对Alignment评估和安全评估。* Results: 提供了一个全面的评估方法和标准套件，以及一些特定领域的评估研究。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.   This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability.   We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在各种任务上表现出了惊人的能力，引起了广泛的关注和应用。然而，与一个双刃剑相似，LLM也存在潜在的风险。它们可能会导致私人数据泄露或生成不当、伤害或误导的内容。此外，LLM的快速进步也引起了关于可能出现无适应安全措施的超智系统的担忧。为了有效利用LLM的能力并确保其安全和有益的发展，对LLM的评估是非常重要。本调查尝试提供LLM评估的全面视图。我们将LLM评估分为三个主要类别：知识和能力评估、对齐评估和安全评估。此外，我们还提供了对这三个方面评估方法和标准的全面评论，并收录了关于LLM在特定领域的表现评估，以及建立了涵盖LLM评估能力、对齐性、安全性和可用性的完整评估平台。我们希望这份全面的概述能够激发更多关于LLM评估的研究兴趣，以实现评估成为LLM发展的重要指南，以最大化社会 benefit  while minimizing potential risks。相关论文的汇总可以在 <https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers> 中找到。
</details></li>
</ul>
<hr>
<h2 id="ViR-Vision-Retention-Networks"><a href="#ViR-Vision-Retention-Networks" class="headerlink" title="ViR: Vision Retention Networks"></a>ViR: Vision Retention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19731">http://arxiv.org/abs/2310.19731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Hatamizadeh, Michael Ranzinger, Jan Kautz</li>
<li>for: 该 paper 的目的是提出一种新的计算机视网络模型，以实现高速的推理和平行的训练。</li>
<li>methods: 该 paper 使用了新的拓展 Transformer 模型，并提出了一种新的并行和循环表示方法，以实现高效的推理和训练。</li>
<li>results: 该 paper 通过了多种 dataset 和不同的图像分辨率进行了广泛的实验，并 achieved 竞争性的性能。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have attracted a lot of popularity in recent years, due to their exceptional capabilities in modeling long-range spatial dependencies and scalability for large scale training. Although the training parallelism of self-attention mechanism plays an important role in retaining great performance, its quadratic complexity baffles the application of ViTs in many scenarios which demand fast inference. This effect is even more pronounced in applications in which autoregressive modeling of input features is required. In Natural Language Processing (NLP), a new stream of efforts have proposed parallelizable models with recurrent formulation that allows for efficient inference in generative applications. Inspired by this trend, we propose a new class of computer vision models, dubbed Vision Retention Networks (ViR), with dual parallel and recurrent formulations, which strike an optimal balance between fast inference and parallel training with competitive performance. In particular, ViR scales favorably for image throughput and memory consumption in tasks that require higher-resolution images due to its flexible formulation in processing large sequence lengths. The ViR is the first attempt to realize dual parallel and recurrent equivalency in a general vision backbone for recognition tasks. We have validated the effectiveness of ViR through extensive experiments with different dataset sizes and various image resolutions and achieved competitive performance. Our code and pretrained models will be made publicly available.
</details>
<details>
<summary>摘要</summary>
视transformer（ViT）在最近几年内引起了很多关注，因为它们在模型长距离空间相互关联的能力和批处理大规模训练中表现出色。 although self-attention机制的训练并行性起到了重要的作用，但它的quadratic复杂性使得ViT在许多场景中不适用，特别是需要快速推理的应用场景。在自然语言处理（NLP）领域，一些新的努力提出了并行化模型的概念，使得在生成应用中实现快速推理变得可能。 drawing inspiration from this trend, we propose a new class of computer vision models, called Vision Retention Networks (ViR), which strike an optimal balance between fast inference and parallel training with competitive performance. in particular, ViR scales favorably for image throughput and memory consumption in tasks that require higher-resolution images due to its flexible formulation in processing large sequence lengths. ViR is the first attempt to realize dual parallel and recurrent equivalency in a general vision backbone for recognition tasks. we have validated the effectiveness of ViR through extensive experiments with different dataset sizes and various image resolutions and achieved competitive performance. our code and pretrained models will be made publicly available.
</details></li>
</ul>
<hr>
<h2 id="Generating-Medical-Instructions-with-Conditional-Transformer"><a href="#Generating-Medical-Instructions-with-Conditional-Transformer" class="headerlink" title="Generating Medical Instructions with Conditional Transformer"></a>Generating Medical Instructions with Conditional Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19727">http://arxiv.org/abs/2310.19727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Warren Del-Pinto, Goran Nenadic<br>for:The paper is written to introduce a novel task-specific model architecture, Label-To-Text-Transformer (LT3), which generates synthetic medical instructions based on provided labels.methods:The LT3 model is trained on a vast corpus of medical instructions extracted from the MIMIC-III database, and it uses a task-specific transformer architecture to generate synthetic medical instructions.results:The paper evaluates the performance of LT3 by contrasting it with a state-of-the-art Pre-trained Language Model (PLM), T5, and shows that LT3 can generate high-quality and diverse synthetic medical instructions. The generated synthetic data is used to train the SpacyNER model for the Named Entity Recognition (NER) task over the n2c2-2018 dataset, and the results show that the model trained on synthetic data can achieve a 96-98% F1 score at Label Recognition on Drug, Frequency, Route, Strength, and Form.<details>
<summary>Abstract</summary>
Access to real-world medical instructions is essential for medical research and healthcare quality improvement. However, access to real medical instructions is often limited due to the sensitive nature of the information expressed. Additionally, manually labelling these instructions for training and fine-tuning Natural Language Processing (NLP) models can be tedious and expensive. We introduce a novel task-specific model architecture, Label-To-Text-Transformer (\textbf{LT3}), tailored to generate synthetic medical instructions based on provided labels, such as a vocabulary list of medications and their attributes. LT3 is trained on a vast corpus of medical instructions extracted from the MIMIC-III database, allowing the model to produce valuable synthetic medical instructions. We evaluate LT3's performance by contrasting it with a state-of-the-art Pre-trained Language Model (PLM), T5, analysing the quality and diversity of generated texts. We deploy the generated synthetic data to train the SpacyNER model for the Named Entity Recognition (NER) task over the n2c2-2018 dataset. The experiments show that the model trained on synthetic data can achieve a 96-98\% F1 score at Label Recognition on Drug, Frequency, Route, Strength, and Form. LT3 codes and data will be shared at \url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}
</details>
<details>
<summary>摘要</summary>
accessed to real-world medical instructions is essential for medical research and healthcare quality improvement. However, access to real medical instructions is often limited due to the sensitive nature of the information expressed. Additionally, manually labelling these instructions for training and fine-tuning Natural Language Processing (NLP) models can be tedious and expensive. We introduce a novel task-specific model architecture, Label-To-Text-Transformer (\textbf{LT3}), tailored to generate synthetic medical instructions based on provided labels, such as a vocabulary list of medications and their attributes. LT3 is trained on a vast corpus of medical instructions extracted from the MIMIC-III database, allowing the model to produce valuable synthetic medical instructions. We evaluate LT3's performance by contrasting it with a state-of-the-art Pre-trained Language Model (PLM), T5, analysing the quality and diversity of generated texts. We deploy the generated synthetic data to train the SpacyNER model for the Named Entity Recognition (NER) task over the n2c2-2018 dataset. The experiments show that the model trained on synthetic data can achieve a 96-98\% F1 score at Label Recognition on Drug, Frequency, Route, Strength, and Form. LT3 codes and data will be shared at \url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="A-Path-to-Simpler-Models-Starts-With-Noise"><a href="#A-Path-to-Simpler-Models-Starts-With-Noise" class="headerlink" title="A Path to Simpler Models Starts With Noise"></a>A Path to Simpler Models Starts With Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19726">http://arxiv.org/abs/2310.19726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lesia Semenova, Harry Chen, Ronald Parr, Cynthia Rudin</li>
<li>For: 这篇论文探讨了决策森林模型在各种领域中的性能问题，特别是在含有噪声的数据集上。* Methods: 该论文提出了一种机制，即数据生成过程和分析者在学习过程中的选择，对决策森林模型的性能产生影响。同时，该论文引入了一个名为“模式多样性”的指标，用于衡量决策森林模型在不同分类模式下的差异。* Results: 该论文发现，噪声程度高的数据集会导致决策森林模型的性能更高，并且模式多样性指标与噪声度之间存在正相关关系。这些结果解释了为什么简单的模型在复杂的数据集上可以达到黑盒模型的同等精度水平。<details>
<summary>Abstract</summary>
The Rashomon set is the set of models that perform approximately equally well on a given dataset, and the Rashomon ratio is the fraction of all models in a given hypothesis space that are in the Rashomon set. Rashomon ratios are often large for tabular datasets in criminal justice, healthcare, lending, education, and in other areas, which has practical implications about whether simpler models can attain the same level of accuracy as more complex models. An open question is why Rashomon ratios often tend to be large. In this work, we propose and study a mechanism of the data generation process, coupled with choices usually made by the analyst during the learning process, that determines the size of the Rashomon ratio. Specifically, we demonstrate that noisier datasets lead to larger Rashomon ratios through the way that practitioners train models. Additionally, we introduce a measure called pattern diversity, which captures the average difference in predictions between distinct classification patterns in the Rashomon set, and motivate why it tends to increase with label noise. Our results explain a key aspect of why simpler models often tend to perform as well as black box models on complex, noisier datasets.
</details>
<details>
<summary>摘要</summary>
“Rashomon集”是指一组模型在给定数据集上表现相似的情况下，“Rashomon比”则是指所有模型空间中的模型数量在Rashomon集中的比例。在刑事、医疗、贷款、教育等领域的表格数据中，Rashomon比 часто很大，这有实际意义，例如是否可以使用简单的模型来达到与更复杂的模型相同的准确率水平。工作中，我们提出了数据生成过程中的一种机制，以及分析者在学习过程中通常会选择的决策，这会决定Rashomon比的大小。我们发现，含有噪声的数据会导致Rashomon比更大，这与模型训练过程中的噪声有关。我们还引入了一个名为“模式多样性”的指标，用于捕捉Rashomon集中不同分类模式之间的预测差异，并解释了这种差异在噪声增加时会增加。我们的结果解释了为什么简单的模型在复杂的噪声数据上常常能够表现出类似的水平。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Knowledge-Editing-of-Neural-Networks"><a href="#A-Survey-on-Knowledge-Editing-of-Neural-Networks" class="headerlink" title="A Survey on Knowledge Editing of Neural Networks"></a>A Survey on Knowledge Editing of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19704">http://arxiv.org/abs/2310.19704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vittorio Mazzia, Alessandro Pedrani, Andrea Caciolai, Kay Rottmann, Davide Bernardi</li>
<li>for: 本研究旨在解决人工智能中的神经网络编辑问题，即如何通过不影响神经网络已经学习的任务来更新神经网络模型，以适应数据的变化。</li>
<li>methods: 本研究使用了多种方法来解决神经网络编辑问题，包括常规化技术、元学习、直接模型编辑和建筑策略等。</li>
<li>results: 本研究提供了一个简洁的概述神经网络编辑领域的最新研究成果，并将相关的方法和数据集分类为四个家族：常规化技术、元学习、直接模型编辑和建筑策略等。<details>
<summary>Abstract</summary>
Deep neural networks are becoming increasingly pervasive in academia and industry, matching and surpassing human performance on a wide variety of fields and related tasks. However, just as humans, even the largest artificial neural networks make mistakes, and once-correct predictions can become invalid as the world progresses in time. Augmenting datasets with samples that account for mistakes or up-to-date information has become a common workaround in practical applications. However, the well-known phenomenon of catastrophic forgetting poses a challenge in achieving precise changes in the implicitly memorized knowledge of neural network parameters, often requiring a full model re-training to achieve desired behaviors. That is expensive, unreliable, and incompatible with the current trend of large self-supervised pre-training, making it necessary to find more efficient and effective methods for adapting neural network models to changing data. To address this need, knowledge editing is emerging as a novel area of research that aims to enable reliable, data-efficient, and fast changes to a pre-trained target model, without affecting model behaviors on previously learned tasks. In this survey, we provide a brief review of this recent artificial intelligence field of research. We first introduce the problem of editing neural networks, formalize it in a common framework and differentiate it from more notorious branches of research such as continuous learning. Next, we provide a review of the most relevant knowledge editing approaches and datasets proposed so far, grouping works under four different families: regularization techniques, meta-learning, direct model editing, and architectural strategies. Finally, we outline some intersections with other fields of research and potential directions for future works.
</details>
<details>
<summary>摘要</summary>
深度神经网络在学术和实践中日益普及，与人类表现相当或超越人类在各种领域和相关任务上。然而，就如人类一样，即使是最大的人工神经网络也会出错，并且已经正确的预测可能会变成过时的信息随着时间的推移。因此，在实际应用中，常常将数据集添加了包含错误或最新信息的样本以作为 workaround。然而，神经网络参数中的隐式记忆知识具有快速填充和损害的问题，常需要完整的模型重新训练来实现愿望的行为。这是昂贵、不可预测和不可靠的，与当前大规模自我超级vised学习的趋势不兼容，因此需要找到更有效率的方法来适应神经网络模型变化的数据。为此，知识编辑在人工智能领域出现了，旨在允许可靠、数据效率、快速地修改预先训练的目标模型，不影响模型在之前学习任务上的行为。在这篇评论中，我们首先介绍了修改神经网络的问题，将其形式化为通用框架，并与更知名的连续学习分支相区分。然后，我们提供了最新的知识编辑方法和数据集的综述，将工作分为四个家族：规范技术、元学习、直接模型编辑和建筑策略。最后，我们介绍了与其他领域的交叉和未来研究的可能性。
</details></li>
</ul>
<hr>
<h2 id="Causal-Context-Connects-Counterfactual-Fairness-to-Robust-Prediction-and-Group-Fairness"><a href="#Causal-Context-Connects-Counterfactual-Fairness-to-Robust-Prediction-and-Group-Fairness" class="headerlink" title="Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness"></a>Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19691">http://arxiv.org/abs/2310.19691</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacyanthis/causal-context">https://github.com/jacyanthis/causal-context</a></li>
<li>paper_authors: Jacy Reese Anthis, Victor Veitch</li>
<li>for:  This paper focuses on the problem of fairness in machine learning, specifically addressing the concept of counterfactual fairness and its relationship to other fairness metrics.</li>
<li>methods:  The authors use a causal context to bridge the gap between counterfactual fairness, robust prediction, and group fairness. They develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness.</li>
<li>results:  The authors show that in three common fairness contexts (measurement error, selection on label, and selection on predictors), counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Additionally, they demonstrate that counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics.<details>
<summary>Abstract</summary>
Counterfactual fairness requires that a person would have been classified in the same way by an AI or other algorithmic system if they had a different protected class, such as a different race or gender. This is an intuitive standard, as reflected in the U.S. legal system, but its use is limited because counterfactuals cannot be directly observed in real-world data. On the other hand, group fairness metrics (e.g., demographic parity or equalized odds) are less intuitive but more readily observed. In this paper, we use $\textit{causal context}$ to bridge the gaps between counterfactual fairness, robust prediction, and group fairness. First, we motivate counterfactual fairness by showing that there is not necessarily a fundamental trade-off between fairness and accuracy because, under plausible conditions, the counterfactually fair predictor is in fact accuracy-optimal in an unbiased target distribution. Second, we develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness. Third, we show that in three common fairness contexts$\unicode{x2013}$measurement error, selection on label, and selection on predictors$\unicode{x2013}$counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics.
</details>
<details>
<summary>摘要</summary>
counterfactual fairness requires that a person would have been classified in the same way by an AI or other algorithmic system if they had a different protected class, such as a different race or gender. This is an intuitive standard, as reflected in the U.S. legal system, but its use is limited because counterfactuals cannot be directly observed in real-world data. On the other hand, group fairness metrics (e.g., demographic parity or equalized odds) are less intuitive but more readily observed. In this paper, we use $\textit{causal context}$ to bridge the gaps between counterfactual fairness, robust prediction, and group fairness. First, we motivate counterfactual fairness by showing that there is not necessarily a fundamental trade-off between fairness and accuracy because, under plausible conditions, the counterfactually fair predictor is in fact accuracy-optimal in an unbiased target distribution. Second, we develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness. Third, we show that in three common fairness contexts$\unicode{x2013}$measurement error, selection on label, and selection on predictors$\unicode{x2013}$counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Can-input-reconstruction-be-used-to-directly-estimate-uncertainty-of-a-regression-U-Net-model-–-Application-to-proton-therapy-dose-prediction-for-head-and-neck-cancer-patients"><a href="#Can-input-reconstruction-be-used-to-directly-estimate-uncertainty-of-a-regression-U-Net-model-–-Application-to-proton-therapy-dose-prediction-for-head-and-neck-cancer-patients" class="headerlink" title="Can input reconstruction be used to directly estimate uncertainty of a regression U-Net model? – Application to proton therapy dose prediction for head and neck cancer patients"></a>Can input reconstruction be used to directly estimate uncertainty of a regression U-Net model? – Application to proton therapy dose prediction for head and neck cancer patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19686">http://arxiv.org/abs/2310.19686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Margerie Huet-Dastarac, Dan Nguyen, Steve Jiang, John Lee, Ana Barragan Montero</li>
<li>for: 这篇论文旨在提供一种可靠和高效的深度学习模型 uncertainty 估计方法，并且能够检测出资料集外的数据（Out-of-distribution，OOD）。</li>
<li>methods: 这篇论文提出了一种直接使用构造汇流（bottleneck）来估计模型 uncertainty的方法，具体来说是将构造汇流中的一支分支用来重建输入数据。</li>
<li>results: 在这篇论文中，这种方法在预报癌症肿瘤疗法剂量预测 tasks 中与 MCDO 和 DE 相比，得到了更高的 Pearson 相関系数（0.620），并且能够轻松地检测出 OOD 数据（Z-score 34.05）。<details>
<summary>Abstract</summary>
Estimating the uncertainty of deep learning models in a reliable and efficient way has remained an open problem, where many different solutions have been proposed in the literature. Most common methods are based on Bayesian approximations, like Monte Carlo dropout (MCDO) or Deep ensembling (DE), but they have a high inference time (i.e. require multiple inference passes) and might not work for out-of-distribution detection (OOD) data (i.e. similar uncertainty for in-distribution (ID) and OOD). In safety critical environments, like medical applications, accurate and fast uncertainty estimation methods, able to detect OOD data, are crucial, since wrong predictions can jeopardize patients safety. In this study, we present an alternative direct uncertainty estimation method and apply it for a regression U-Net architecture. The method consists in the addition of a branch from the bottleneck which reconstructs the input. The input reconstruction error can be used as a surrogate of the model uncertainty. For the proof-of-concept, our method is applied to proton therapy dose prediction in head and neck cancer patients. Accuracy, time-gain, and OOD detection are analyzed for our method in this particular application and compared with the popular MCDO and DE. The input reconstruction method showed a higher Pearson correlation coefficient with the prediction error (0.620) than DE and MCDO (between 0.447 and 0.612). Moreover, our method allows an easier identification of OOD (Z-score of 34.05). It estimates the uncertainty simultaneously to the regression task, therefore requires less time or computational resources.
</details>
<details>
<summary>摘要</summary>
深度学习模型的不确定性估计问题已成为一个开放的问题，文献中有许多不同的解决方案。大多数常用的方法基于 bayesian 近似，如 Monte Carlo dropout (MCDO) 或 Deep ensembling (DE)，但它们的推理时间较长（需要多次推理），而且可能无法处理非标本数据（OOD）。在安全关键环境，如医疗应用，准确和快速的不确定性估计方法，能够检测 OOD 数据，是非常重要的，因为错误预测可能会威胁病人的安全。在这种情况下，我们提出了一种直接的不确定性估计方法，并应用于回归 U-Net 架构。该方法是通过添加从瓶颈来的一个分支，来重construct 输入。输入重建错误可以作为模型不确定性的Surrogate。为证明，我们对抗癌病头颈患者的辐射剂量预测进行了应用。我们分析了我们方法的准确率、时间提升和 OOD 检测，并与 MCDO 和 DE 进行了比较。我们的方法显示了更高的归一化相关系数（0.620），与 DE 和 MCDO（ между 0.447 和 0.612）相比。此外，我们的方法可以更容易地检测 OOD（Z-score 34.05）。它同时估计不确定性和回归任务，因此需要更少的时间或计算资源。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Pre-trained-Language-Model-into-Neural-Machine-Translation"><a href="#Integrating-Pre-trained-Language-Model-into-Neural-Machine-Translation" class="headerlink" title="Integrating Pre-trained Language Model into Neural Machine Translation"></a>Integrating Pre-trained Language Model into Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19680">http://arxiv.org/abs/2310.19680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soon-Jae Hwang, Chang-Sung Jeong</li>
<li>for: 提高Neural Machine Translation（NMT）性能，解决高质量双语对应语料不足问题。</li>
<li>methods: 使用预训练语言模型（PLM）提供上下文信息，并提出PLM集成NMT（PiNMT）模型，包括PLM多层转换器、嵌入合并和夹角匹配等三个关键组件。</li>
<li>results: 通过提出的PiNMT模型和训练策略（分离学习率和双步训练），在IWSLT’14 En$\leftrightarrow$De数据集上实现了状态级表现。<details>
<summary>Abstract</summary>
Neural Machine Translation (NMT) has become a significant technology in natural language processing through extensive research and development. However, the deficiency of high-quality bilingual language pair data still poses a major challenge to improving NMT performance. Recent studies are exploring the use of contextual information from pre-trained language model (PLM) to address this problem. Yet, the issue of incompatibility between PLM and NMT model remains unresolved. This study proposes a PLM-integrated NMT (PiNMT) model to overcome the identified problems. The PiNMT model consists of three critical components, PLM Multi Layer Converter, Embedding Fusion, and Cosine Alignment, each playing a vital role in providing effective PLM information to NMT. Furthermore, two training strategies, Separate Learning Rates and Dual Step Training, are also introduced in this paper. By implementing the proposed PiNMT model and training strategy, we achieved state-of-the-art performance on the IWSLT'14 En$\leftrightarrow$De dataset. This study's outcomes are noteworthy as they demonstrate a novel approach for efficiently integrating PLM with NMT to overcome incompatibility and enhance performance.
</details>
<details>
<summary>摘要</summary>
PiNMT 模型包括三个关键组件：PLM 多层转换器、扩展嵌入和偏心对Alignment。每一个组件都在提供 PLM 信息给 NMT 中扮演着重要的角色。此外，本研究还提出了两种训练策略：分离学习速率和双步训练。通过实施提议的 PiNMT 模型和训练策略，我们在 IWSLT'14 En$\leftrightarrow$De 数据集上实现了状态略的性能。这些结果具有意义，因为它们证明了一种有效的 PLM 与 NMT 集成方法，以解决不兼容问题并提高性能。
</details></li>
</ul>
<hr>
<h2 id="AI-Alignment-A-Comprehensive-Survey"><a href="#AI-Alignment-A-Comprehensive-Survey" class="headerlink" title="AI Alignment: A Comprehensive Survey"></a>AI Alignment: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19852">http://arxiv.org/abs/2310.19852</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PKU-Alignment/AlignmentSurvey">https://github.com/PKU-Alignment/AlignmentSurvey</a></li>
<li>paper_authors: Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O’Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, Wen Gao<br>for: This paper aims to provide a systematic survey of AI alignment research, including the key objectives, methodology, and practice of alignment research.methods: The paper uses a four-principle framework (Robustness, Interpretability, Controllability, and Ethicality) to identify the key objectives of AI alignment, and decomposes current alignment research into two components: forward alignment and backward alignment.results: The paper discusses various techniques for forward alignment, including learning from feedback and overcoming distribution shift, and verification techniques for backward alignment to improve the assurance of forward alignment outcomes. Additionally, the paper provides a constantly updated website featuring tutorials, collections of papers, blogs, and other learning resources.<details>
<summary>Abstract</summary>
AI alignment aims to build AI systems that are in accordance with human intentions and values. With the emergence of AI systems possessing superhuman capabilities, the potential large-scale risks associated with misaligned systems become apparent. Hundreds of AI experts and public figures have expressed their concerns about AI risks, arguing that mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war. Motivated by the lack of an up-to-date systematic survey on AI alignment, in this paper, we delve into the core concepts, methodology, and practice of alignment research. To begin with, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). We outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss how to conduct learning from various types of feedback (a.k.a., outer alignment) and how to overcome the distribution shift to avoid goal misgeneralization (a.k.a., inner alignment). On backward alignment, we discuss verification techniques that can tell the degree of value alignment for various AI systems deployed, which can further improve the assurance of forward alignment outcomes.   Based on this, we also release a constantly updated website featuring tutorials, collections of papers, blogs, and other learning resources at https://www.alignmentsurvey.com.
</details>
<details>
<summary>摘要</summary>
人工智能启 align 目标是建立与人类意愿和价值观念相一致的 AI 系统。随着 AI 系统拥有超人类能力的出现，大规模的风险相关于不一致系统的出现也显得潜在危险。众多 AI 专家和公众人物表达了对 AI 风险的担忧， argued that mitigating the risk of AI extinction should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war. 因为现有的系统性的调查不够新，在这篇论文中，我们进行了对Alignment研究的核心概念、方法和实践的探讨。首先，我们确定了四个目标为AI启 align的关键原则：Robustness、Interpretability、Controllability和Ethicality（RICE）。我们还将当前的启 align研究景观分解为两个关键组成部分：前向启 align和后向启 align。前者希望通过启 align训练来使 AI 系统相互适应，而后者则希望通过获得证明 AI 系统的启 align度以避免加剧不一致风险。在前向启 align方面，我们讨论了从不同类型的反馈（即外部启 align）学习以及如何避免目标泛化风险（即内部启 align）。在后向启 align方面，我们讨论了如何证明不同 AI 系统的启 align度，以便进一步提高前向启 align 的结果。此外，我们还发布了一个不断更新的网站，包括教程、论文集、博客和其他学习资源，请参考 <https://www.alignmentsurvey.com>。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-The-Need-for-Nuance-in-Current-Debates-and-a-Pragmatic-Perspective-on-Understanding"><a href="#Large-Language-Models-The-Need-for-Nuance-in-Current-Debates-and-a-Pragmatic-Perspective-on-Understanding" class="headerlink" title="Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding"></a>Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19671">http://arxiv.org/abs/2310.19671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bram M. A. van Dijk, Tom Kouwenhoven, Marco R. Spruit, Max J. van Duijn</li>
<li>for: 这篇论文主要是为了评估大型自然语言处理器（LLM）的能力，并且探讨关于 LLM 的评价和含义。</li>
<li>methods: 本文使用了理论和实证方法来评估 LLM 的能力，包括对三个常见批评点进行了严谨的分析。</li>
<li>results: 本文的结果表明，对 LLM 的评价需要更加细化，并且提出了一种 Pragmatic 视角来理解 LLM 的含义和意图。<details>
<summary>Abstract</summary>
Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of `real' understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviour effectively. We reflect on the circumstances under which it would make sense for humans to similarly attribute mental states to LLMs, thereby outlining a pragmatic philosophical context for LLMs as an increasingly prominent technology in society.
</details>
<details>
<summary>摘要</summary>
当前的大语言模型（LLM）在生成 grammatically 正确，流畅文本方面表现出了无与伦比的能力。 LLM 的出现速度非常快，但是关于 LLM 的能力的讨论却落后于其出现。因此，在这份位点纸中，我们首先 zoom 到了评估 LLM 能力的辩论中的三个点：一、LLM 只是遵循训练数据中的统计模式；二、LLM 掌握了形式语言能力，而不是真正的语言能力；三、语言学习在 LLM 中不能 Inform 人类语言学习。通过实证和理论上的论据，我们表明这些点需要更加细腻。二、我们阐述了在 LLM 中理解和意愿的问题上的 Pragmatic 视角。理解和意愿是指我们归功于他人的心理状态，因为它们具有 Pragmatic 的价值：它们允许我们忽略复杂的下面机制，预测行为效果。我们反思在哪些情况下，人类可能会对 LLM 归功于心理状态，从而描述了一种 Pragmatic 哲学上的 LLM 技术在社会中的增长。
</details></li>
</ul>
<hr>
<h2 id="Explaining-Tree-Model-Decisions-in-Natural-Language-for-Network-Intrusion-Detection"><a href="#Explaining-Tree-Model-Decisions-in-Natural-Language-for-Network-Intrusion-Detection" class="headerlink" title="Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection"></a>Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19658">http://arxiv.org/abs/2310.19658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noah Ziems, Gang Liu, John Flanagan, Meng Jiang</li>
<li>for: 本研究旨在提高网络入侵检测（NID）系统的决策树模型，以便更好地检测恶意网络流量。</li>
<li>methods: 本研究使用大型自然语言模型（LLM）来提供解释和背景知识，以帮助用户更好地理解决策树的决策。</li>
<li>results: 研究发现，LLM生成的决策树解释与人类评价的可读性、质量和背景知识之间呈高度相关，同时能够提供更好的决策边界的理解。<details>
<summary>Abstract</summary>
Network intrusion detection (NID) systems which leverage machine learning have been shown to have strong performance in practice when used to detect malicious network traffic. Decision trees in particular offer a strong balance between performance and simplicity, but require users of NID systems to have background knowledge in machine learning to interpret. In addition, they are unable to provide additional outside information as to why certain features may be important for classification.   In this work, we explore the use of large language models (LLMs) to provide explanations and additional background knowledge for decision tree NID systems. Further, we introduce a new human evaluation framework for decision tree explanations, which leverages automatically generated quiz questions that measure human evaluators' understanding of decision tree inference. Finally, we show LLM generated decision tree explanations correlate highly with human ratings of readability, quality, and use of background knowledge while simultaneously providing better understanding of decision boundaries.
</details>
<details>
<summary>摘要</summary>
网络侵入检测（NID）系统利用机器学习显示在探测恶意网络流量方面具有优秀表现。决策树特别是在性能和简单性之间做出了良好的折衔，但需要NID系统用户具备机器学习背景知识来解释。此外，它们无法提供外部信息，以解释特定特征的分类重要性。在这项工作中，我们探讨使用大语言模型（LLM）提供解释和背景知识来增强决策树NID系统。此外，我们提出了一种新的人类评估框架 для决策树解释，该框架利用自动生成的测验题来评估人类评估者对决策树推理的理解程度。最后，我们发现LLM生成的决策树解释与人类评分的可读性、质量和背景知识相关性高，同时提供更好的决策边界理解。
</details></li>
</ul>
<hr>
<h2 id="MCAD-Multi-teacher-Cross-modal-Alignment-Distillation-for-efficient-image-text-retrieval"><a href="#MCAD-Multi-teacher-Cross-modal-Alignment-Distillation-for-efficient-image-text-retrieval" class="headerlink" title="MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval"></a>MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19654">http://arxiv.org/abs/2310.19654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youbo Lei, Feifei He, Chen Chen, Yingbin Mo, Si Jia Li, Defeng Xie, Haonan Lu</li>
<li>for: 降低大规模图文预训练模型的模型大小和加速其终端设备部署。</li>
<li>methods: 提出了一种多教师跨Modalities对齐填充（MCAD）技术，将单流模型的优点和双流模型的优点结合起来。</li>
<li>results: 通过在双流模型中插入单流模型的混合特征，并进行Logit和特征填充，提高了学生双流模型的检索性能，同时不增加搜索复杂度。<details>
<summary>Abstract</summary>
With the success of large-scale visual-language pretraining models and the wide application of image-text retrieval in industry areas, reducing the model size and streamlining their terminal-device deployment have become urgently necessary. The mainstream model structures for image-text retrieval are single-stream and dual-stream, both aiming to close the semantic gap between visual and textual modalities. Dual-stream models excel at offline indexing and fast inference, while single-stream models achieve more accurate cross-model alignment by employing adequate feature fusion. We propose a multi-teacher cross-modality alignment distillation (MCAD) technique to integrate the advantages of single-stream and dual-stream models. By incorporating the fused single-stream features into the image and text features of the dual-stream model, we formulate new modified teacher features and logits. Then, we conduct both logit and feature distillation to boost the capability of the student dual-stream model, achieving high retrieval performance without increasing inference complexity. Extensive experiments demonstrate the remarkable performance and high efficiency of MCAD on image-text retrieval tasks. Furthermore, we implement a mobile CLIP model on Snapdragon clips with only 93M running memory and 30ms search latency, without apparent performance degradation of the original large CLIP.
</details>
<details>
<summary>摘要</summary>
随着大规模视语言预训模型的成功和图像文本关联在行业领域的广泛应用，减少模型大小和加速终端设备部署已成为非常重要的需求。主流的图像文本关联模型结构有单流和双流两种，都想要填补视语言模态之间的 semantic gap。双流模型在离线索引和快速推理方面表现出色，而单流模型通过适当的特征融合来实现更高精度的跨模型对齐。我们提出了一种多教师跨模态对齐截取（MCAD）技术，通过将单流特征融合到图像和文本特征中来形成新的修改教师特征和 logits。然后，我们进行了 both logit 和特征截取来提高学生双流模型的能力，实现高度的 retrieve 性能无需增加推理复杂度。广泛的实验表明 MCAD 在图像文本关联任务中表现出了非常出色的性能和高效性。此外，我们在 Snapdragon clips 上实现了基于 CLIP 的移动模型，只需93M的运行内存和30ms的搜索延迟，而无 Apparent 性能下降。
</details></li>
</ul>
<hr>
<h2 id="Fast-swap-regret-minimization-and-applications-to-approximate-correlated-equilibria"><a href="#Fast-swap-regret-minimization-and-applications-to-approximate-correlated-equilibria" class="headerlink" title="Fast swap regret minimization and applications to approximate correlated equilibria"></a>Fast swap regret minimization and applications to approximate correlated equilibria</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19647">http://arxiv.org/abs/2310.19647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binghui Peng, Aviad Rubinstein</li>
<li>For: The paper is written to resolve the main open problem of [Blum and Mansour 2007] and to provide a new, matching lower bound.* Methods: The paper uses a simple and computationally efficient algorithm that obtains $\varepsilon T$-swap regret within only $T &#x3D; \mathsf{polylog}(n)$ rounds.* Results: The paper achieves an exponential improvement compared to the super-linear number of rounds required by the state-of-the-art algorithm, and resolves the main open problem of [Blum and Mansour 2007]. The algorithm also implies faster convergence to $\varepsilon$-Correlated Equilibrium in several regimes, including normal form two-player games and extensive-form games.<details>
<summary>Abstract</summary>
We give a simple and computationally efficient algorithm that, for any constant $\varepsilon>0$, obtains $\varepsilon T$-swap regret within only $T = \mathsf{polylog}(n)$ rounds; this is an exponential improvement compared to the super-linear number of rounds required by the state-of-the-art algorithm, and resolves the main open problem of [Blum and Mansour 2007]. Our algorithm has an exponential dependence on $\varepsilon$, but we prove a new, matching lower bound.   Our algorithm for swap regret implies faster convergence to $\varepsilon$-Correlated Equilibrium ($\varepsilon$-CE) in several regimes: For normal form two-player games with $n$ actions, it implies the first uncoupled dynamics that converges to the set of $\varepsilon$-CE in polylogarithmic rounds; a $\mathsf{polylog}(n)$-bit communication protocol for $\varepsilon$-CE in two-player games (resolving an open problem mentioned by [Babichenko-Rubinstein'2017, Goos-Rubinstein'2018, Ganor-CS'2018]; and an $\tilde{O}(n)$-query algorithm for $\varepsilon$-CE (resolving an open problem of [Babichenko'2020] and obtaining the first separation between $\varepsilon$-CE and $\varepsilon$-Nash equilibrium in the query complexity model).   For extensive-form games, our algorithm implies a PTAS for $\mathit{normal}$ $\mathit{form}$ $\mathit{correlated}$ $\mathit{equilibria}$, a solution concept often conjectured to be computationally intractable (e.g. [Stengel-Forges'08, Fujii'23]).
</details>
<details>
<summary>摘要</summary>
我们提供一个简单而计算效率高的算法，可以在任何常数 $\varepsilon>0$ 下获得 $\varepsilon T$-交换误差，只需要 $T = \mathsf{polylog}(n)$ 轮次，这比现状态艺术算法的超线性轮次数快很多，解决了 [Blum 和 Mansour 2007] 中的主要开问。我们的算法具有对 $\varepsilon$ 的幂次依赖，但我们证明了一个新的匹配下界。我们的交换误差算法意味着在几个场景中更快地 converges to $\varepsilon$-相关平衡（$\varepsilon$-CE）：1. 正常形两个玩家游戏中，我们的算法可以在 $\polylog(n)$ 轮次内 converges to $\varepsilon$-CE;2. 我们可以实现一个 $\mathsf{polylog}(n)$-位信息协议来实现 $\varepsilon$-CE在两个玩家游戏中;3. 我们可以实现一个 $\tilde{O}(n)$-询问算法来实现 $\varepsilon$-CE。此外，我们的算法还解决了对 $\varepsilon$-CE 和 $\varepsilon$- Nash 平衡之间的分离问题，这是一个在询问复杂度模型中的开问。在扩展形游戏中，我们的算法意味着一个 PTAS  для $\mathit{normal}$ $\mathit{form}$ $\mathit{相关}$ $\mathit{平衡}$，这是一个 computationally conjectured 的问题（例如 [Stengel-Forges'08, Fujii'23]）。
</details></li>
</ul>
<hr>
<h2 id="RayDF-Neural-Ray-surface-Distance-Fields-with-Multi-view-Consistency"><a href="#RayDF-Neural-Ray-surface-Distance-Fields-with-Multi-view-Consistency" class="headerlink" title="RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency"></a>RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19629">http://arxiv.org/abs/2310.19629</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vlar-group/raydf">https://github.com/vlar-group/raydf</a></li>
<li>paper_authors: Zhuoman Liu, Bo Yang</li>
<li>for: 本研究targets the problem of continuous 3D shape representations, aiming to improve the efficiency and accuracy of 3D shape reconstruction.</li>
<li>methods: 我们提出了一种新的框架 called RayDF, which consists of three major components: 1) simple ray-surface distance field, 2) novel dual-ray visibility classifier, and 3) multi-view consistency optimization module.</li>
<li>results: 我们进行了广泛的 evaluate our method on three public datasets, and the results show that our method clearly surpasses existing coordinate-based and ray-based baselines in 3D surface point reconstruction, with a 1000x faster speed than coordinate-based methods to render an 800x800 depth image.<details>
<summary>Abstract</summary>
In this paper, we study the problem of continuous 3D shape representations. The majority of existing successful methods are coordinate-based implicit neural representations. However, they are inefficient to render novel views or recover explicit surface points. A few works start to formulate 3D shapes as ray-based neural functions, but the learned structures are inferior due to the lack of multi-view geometry consistency. To tackle these challenges, we propose a new framework called RayDF. It consists of three major components: 1) the simple ray-surface distance field, 2) the novel dual-ray visibility classifier, and 3) a multi-view consistency optimization module to drive the learned ray-surface distances to be multi-view geometry consistent. We extensively evaluate our method on three public datasets, demonstrating remarkable performance in 3D surface point reconstruction on both synthetic and challenging real-world 3D scenes, clearly surpassing existing coordinate-based and ray-based baselines. Most notably, our method achieves a 1000x faster speed than coordinate-based methods to render an 800x800 depth image, showing the superiority of our method for 3D shape representation. Our code and data are available at https://github.com/vLAR-group/RayDF
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了连续3D形状表示的问题。现有大多数成功方法是坐标基的偏折神经表示。然而，它们在渲染新视图或者恢复明确的表面点时效率低下。一些工作开始将3D形状表示为射线基的神经函数，但学习结构因为缺乏多视图几何一致性而受到限制。为了解决这些挑战，我们提出了一个新框架called RayDF。它包括三个主要组成部分：1）简单的射线表面距离场，2）新的双射线可见分类器，3）多视图一致优化模块，使得学习的射线表面距离具有多视图几何一致性。我们对三个公共数据集进行了广泛的评估，并证明了我们的方法在 sintetic和实际世界3D场景中的3D表面点重建表现出色，明显超过了坐标基和射线基的基准。其中，我们的方法可以在渲染800x800深度图像时达到1000倍的速度，显示了我们的方法在3D形状表示方面的优势。我们的代码和数据可以在https://github.com/vLAR-group/RayDF上获取。
</details></li>
</ul>
<hr>
<h2 id="Transformation-vs-Tradition-Artificial-General-Intelligence-AGI-for-Arts-and-Humanities"><a href="#Transformation-vs-Tradition-Artificial-General-Intelligence-AGI-for-Arts-and-Humanities" class="headerlink" title="Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities"></a>Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19626">http://arxiv.org/abs/2310.19626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengliang Liu, Yiwei Li, Qian Cao, Junwen Chen, Tianze Yang, Zihao Wu, John Hale, John Gibbs, Khaled Rasheed, Ninghao Liu, Gengchen Mai, Tianming Liu</li>
<li>For: The paper provides a comprehensive analysis of the applications and implications of artificial general intelligence (AGI) in the arts and humanities, with a focus on its potential to promote creativity, knowledge, and cultural values while avoiding negative consequences such as factuality, toxicity, biases, and public safety concerns.* Methods: The paper surveys cutting-edge AGI systems and their usage in various areas such as poetry, history, marketing, film, and classical art, and outlines mitigation strategies to address concerns related to the technology’s responsible deployment.* Results: The paper argues for multi-stakeholder collaboration to ensure AGI’s technological capacities are aligned with enduring social goods, and highlights promising directions for further research to achieve this goal.In Simplified Chinese text, the three key points would be:</li>
<li>for: 这篇论文探讨了人工通用智能（AGI）在艺术和人文领域的应用和影响，以及如何通过多方合作确保AGI促进创造力、知识和文化价值而不让技术发展带来负面影响。</li>
<li>methods: 论文对 cutting-edge AGI 系统和其在不同领域的应用进行了概括，并提出了Addressing Concerns related to factuality, toxicity, biases, and public safety in AGI systems的缓解策略。</li>
<li>results: 论文强调了多方合作的重要性，以确保AGI的技术 capacities 与持续社会价值相align，并 highlighted 了未来研究的可能性，以实现这一目标。<details>
<summary>Abstract</summary>
Recent advances in artificial general intelligence (AGI), particularly large language models and creative image generation systems have demonstrated impressive capabilities on diverse tasks spanning the arts and humanities. However, the swift evolution of AGI has also raised critical questions about its responsible deployment in these culturally significant domains traditionally seen as profoundly human. This paper provides a comprehensive analysis of the applications and implications of AGI for text, graphics, audio, and video pertaining to arts and the humanities. We survey cutting-edge systems and their usage in areas ranging from poetry to history, marketing to film, and communication to classical art. We outline substantial concerns pertaining to factuality, toxicity, biases, and public safety in AGI systems, and propose mitigation strategies. The paper argues for multi-stakeholder collaboration to ensure AGI promotes creativity, knowledge, and cultural values without undermining truth or human dignity. Our timely contribution summarizes a rapidly developing field, highlighting promising directions while advocating for responsible progress centering on human flourishing. The analysis lays the groundwork for further research on aligning AGI's technological capacities with enduring social goods.
</details>
<details>
<summary>摘要</summary>
This paper provides a comprehensive analysis of the applications and implications of AGI in the fields of text, graphics, audio, and video related to arts and the humanities. We survey cutting-edge systems and their uses in various areas, such as poetry, history, marketing, film, and communication, as well as classical art. We also highlight significant concerns regarding factuality, toxicity, biases, and public safety in AGI systems and propose mitigation strategies.The paper argues for the importance of multi-stakeholder collaboration to ensure that AGI promotes creativity, knowledge, and cultural values without compromising truth or human dignity. Our timely contribution provides a comprehensive overview of a rapidly developing field, highlighting promising directions while advocating for responsible progress that prioritizes human flourishing. The analysis lays the groundwork for further research on aligning AGI's technological capacities with enduring social goods.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Post-Training-Quantization-of-Protein-Language-Models"><a href="#Exploring-Post-Training-Quantization-of-Protein-Language-Models" class="headerlink" title="Exploring Post-Training Quantization of Protein Language Models"></a>Exploring Post-Training Quantization of Protein Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19624">http://arxiv.org/abs/2310.19624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Peng, Fei Yang, Ning Sun, Sheng Chen, Yanfeng Jiang, Aimin Pan</li>
<li>for: 这个研究是为了解决 protein language model (ProteinLM) 的高计算需求、大量存储需求和延迟问题，以提高 ProteinLM 的可用性和效率。</li>
<li>methods: 这个研究使用了 post-training quantization (PTQ) 技术来实现 ProteinLM 的数字化。具体来说，我们对 ESMFold 进行了全量对应的数字化，并进行了广泛的数字化实验，以探索适当的数字化方法。</li>
<li>results: 我们的研究结果显示，使用 Typical uniform quantization method 对 ESMFold 的数字化会导致严重的损失TM-Score，尤其是在使用 8 位数字化时。我们还发现 ESMFold 具有特殊的问题，例如具有高度不对称的活动范围，使得使用低位数字化格式进行表示具有困难。为解决这些问题，我们提出了一个新的 PTQ 方法，使用 Piecewise linear quantization 来精确地数字化具有不对称活动值的 ESMFold。我们的研究结果显示，这个方法可以实现精确地数字化 ESMFold，并且可以应用到聚合点预测任务中。<details>
<summary>Abstract</summary>
Recent advancements in unsupervised protein language models (ProteinLMs), like ESM-1b and ESM-2, have shown promise in different protein prediction tasks. However, these models face challenges due to their high computational demands, significant memory needs, and latency, restricting their usage on devices with limited resources. To tackle this, we explore post-training quantization (PTQ) for ProteinLMs, focusing on ESMFold, a simplified version of AlphaFold based on ESM-2 ProteinLM. Our study is the first attempt to quantize all weights and activations of ProteinLMs. We observed that the typical uniform quantization method performs poorly on ESMFold, causing a significant drop in TM-Score when using 8-bit quantization. We conducted extensive quantization experiments, uncovering unique challenges associated with ESMFold, particularly highly asymmetric activation ranges before Layer Normalization, making representation difficult using low-bit fixed-point formats. To address these challenges, we propose a new PTQ method for ProteinLMs, utilizing piecewise linear quantization for asymmetric activation values to ensure accurate approximation. We demonstrated the effectiveness of our method in protein structure prediction tasks, demonstrating that ESMFold can be accurately quantized to low-bit widths without compromising accuracy. Additionally, we applied our method to the contact prediction task, showcasing its versatility. In summary, our study introduces an innovative PTQ method for ProteinLMs, addressing specific quantization challenges and potentially leading to the development of more efficient ProteinLMs with significant implications for various protein-related applications.
</details>
<details>
<summary>摘要</summary>
We observed that typical uniform quantization methods perform poorly on ESMFold, resulting in a significant drop in TM-Score when using 8-bit quantization. We conducted extensive quantization experiments and discovered unique challenges associated with ESMFold, particularly highly asymmetric activation ranges before Layer Normalization, which make representation difficult using low-bit fixed-point formats.To address these challenges, we proposed a new PTQ method for ProteinLMs, utilizing piecewise linear quantization for asymmetric activation values to ensure accurate approximation. We demonstrated the effectiveness of our method in protein structure prediction tasks, showing that ESMFold can be accurately quantized to low-bit widths without compromising accuracy. Additionally, we applied our method to the contact prediction task, demonstrating its versatility.In summary, our study introduces an innovative PTQ method for ProteinLMs, addressing specific quantization challenges and potentially leading to the development of more efficient ProteinLMs with significant implications for various protein-related applications.
</details></li>
</ul>
<hr>
<h2 id="Large-Trajectory-Models-are-Scalable-Motion-Predictors-and-Planners"><a href="#Large-Trajectory-Models-are-Scalable-Motion-Predictors-and-Planners" class="headerlink" title="Large Trajectory Models are Scalable Motion Predictors and Planners"></a>Large Trajectory Models are Scalable Motion Predictors and Planners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19620">http://arxiv.org/abs/2310.19620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-mars-lab/statetransformer">https://github.com/tsinghua-mars-lab/statetransformer</a></li>
<li>paper_authors: Qiao Sun, Shiduo Zhang, Danjiao Ma, Jingzhe Shi, Derun Li, Simian Luo, Yu Wang, Ningyi Xu, Guangzhi Cao, Hang Zhao</li>
<li>for: 本研究的目的是提出一种可扩展的轨迹模型（State Transformer，STR），用于自动驾驶中的运动预测和规划问题。</li>
<li>methods: 本研究使用了一种简单的模型设计，将观察、状态和动作拼接成一个统一的序列模型任务，以解决自动驾驶中的复杂问题。</li>
<li>results: 实验结果显示，大轨迹模型（LTM），如STR，在运动预测和规划问题中具有出色的适应性和学习效率。 Qualitative results还表明，LTM可以在训练数据分布不符的场景下做出可靠的预测，并且学习长期规划 ohnexpress的高级审核或高成本的标注。<details>
<summary>Abstract</summary>
Motion prediction and planning are vital tasks in autonomous driving, and recent efforts have shifted to machine learning-based approaches. The challenges include understanding diverse road topologies, reasoning traffic dynamics over a long time horizon, interpreting heterogeneous behaviors, and generating policies in a large continuous state space. Inspired by the success of large language models in addressing similar complexities through model scaling, we introduce a scalable trajectory model called State Transformer (STR). STR reformulates the motion prediction and motion planning problems by arranging observations, states, and actions into one unified sequence modeling task. With a simple model design, STR consistently outperforms baseline approaches in both problems. Remarkably, experimental results reveal that large trajectory models (LTMs), such as STR, adhere to the scaling laws by presenting outstanding adaptability and learning efficiency. Qualitative results further demonstrate that LTMs are capable of making plausible predictions in scenarios that diverge significantly from the training data distribution. LTMs also learn to make complex reasonings for long-term planning, without explicit loss designs or costly high-level annotations.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate="zh-CN"<</SYS>>行为预测和规划是自动驾驶中的关键任务，最近的努力都集中在机器学习基于的方法上。挑战包括理解多样化的道路地貌、跟踪交通动态过程长时间 horizon、解释不同行为类型、并在大规模连续状态空间中生成策略。启发于大语言模型在面临类似复杂性时的成功，我们介绍了一种可扩展的轨迹模型，称为State Transformer（STR）。STR将轨迹预测和轨迹规划问题重新排序为一个统一的序列模型任务。 STR的简单设计使其在基eline方法上表现出色，并且在实验结果中显示出了卓越的适应性和学习效率。另外，实验结果还表明，大轨迹模型（LTM），如STR，遵循了扩展法律，并且在不同于训练数据分布的场景中做出了可靠的预测。LTM也能够无需显式损失函数或高级标注来学习复杂的长期规划。
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Holistic-Landscape-of-Situated-Theory-of-Mind-in-Large-Language-Models"><a href="#Towards-A-Holistic-Landscape-of-Situated-Theory-of-Mind-in-Large-Language-Models" class="headerlink" title="Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models"></a>Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19619">http://arxiv.org/abs/2310.19619</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mars-tin/awesome-theory-of-mind">https://github.com/mars-tin/awesome-theory-of-mind</a></li>
<li>paper_authors: Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai</li>
<li>for: This paper aims to provide a comprehensive evaluation of machine ToM and develop a more effective evaluation protocol for assessing ToM in large language models (LLMs).</li>
<li>methods: The authors use a taxonomic approach to categorize machine ToM into 7 mental state categories, and they propose a holistic and situated evaluation protocol that considers the physical and social context of LLMs.</li>
<li>results: The authors present a pilot study in a grid world setup as a proof of concept, demonstrating the potential of their proposed evaluation protocol to provide a more comprehensive assessment of mental states in LLMs.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提供大语言模型（LLM）机器理解思维的全面评估，以及为了评估思维的更有效的评价协议。</li>
<li>methods: 作者使用分类方法将机器思维分为7种情感类别，并提议在物理和社会上将LLM视为 Agent 进行全面和围绕的评估。</li>
<li>results: 作者在格子世界设置下进行了一个证明性研究，展示了其提议的评价协议的潜在。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage. In this position paper, we seek to answer two road-blocking questions: (1) How can we taxonomize a holistic landscape of machine ToM? (2) What is a more effective evaluation protocol for machine ToM? Following psychological studies, we taxonomize machine ToM into 7 mental state categories and delineate existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM to break ToM into individual components and treat LLMs as an agent who is physically situated in environments and socially situated in interactions with humans. Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage. We further present a pilot study in a grid world setup as a proof of concept. We hope this position paper can facilitate future research to integrate ToM with LLMs and offer an intuitive means for researchers to better position their work in the landscape of ToM. Project page: https://github.com/Mars-tin/awesome-theory-of-mind
</details>
<details>
<summary>摘要</summary>
In this position paper, we aim to answer two key questions:1. How can we categorize the various aspects of machine ToM?2. What is a more effective evaluation protocol for machine ToM?Drawing on psychological studies, we categorize machine ToM into 7 mental state categories and examine existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM, where LLMs are treated as agents who are physically and socially situated in environments and interactions with humans. This approach provides a more comprehensive assessment of mental states and may mitigate the risk of shortcuts and data leakage.We present a pilot study in a grid world setup as a proof of concept. Our hope is that this position paper will facilitate future research in integrating ToM with LLMs and offer a straightforward means for researchers to better position their work in the landscape of ToM.More information can be found on our project page: <https://github.com/Mars-tin/awesome-theory-of-mind>
</details></li>
</ul>
<hr>
<h2 id="Technical-Report-on-the-Learning-of-Case-Relevance-in-Case-Based-Reasoning-with-Abstract-Argumentation"><a href="#Technical-Report-on-the-Learning-of-Case-Relevance-in-Case-Based-Reasoning-with-Abstract-Argumentation" class="headerlink" title="Technical Report on the Learning of Case Relevance in Case-Based Reasoning with Abstract Argumentation"></a>Technical Report on the Learning of Case Relevance in Case-Based Reasoning with Abstract Argumentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19607">http://arxiv.org/abs/2310.19607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GPPassos/learning-relevance-aacbr-technical-report">https://github.com/GPPassos/learning-relevance-aacbr-technical-report</a></li>
<li>paper_authors: Guilherme Paulino-Passos, Francesca Toni</li>
<li>for: 这 paper 是关于 case-based reasoning 在法律 Setting 中的一种新方法，使用抽象的 argumentation 支持，Arguments 代表 case，Attack  между arguments 是由结果不同和 case 和 relevance 之间的 disagreement 引起的。</li>
<li>methods: 这 paper 使用了 decision trees 来自动学习 relevance，并结合 abstract argumentation 和 case-based reasoning 进行预测。</li>
<li>results: 对两个法律 Dataset 的验证表明，AA-CBR 和 decision tree-based learning of case relevance 在 comparison 中表现竞争力强，并且 AA-CBR 可以获得更加 Compact 的 Representation，可能对于获得 cognitively tractable explanations 是有利的。<details>
<summary>Abstract</summary>
Case-based reasoning is known to play an important role in several legal settings. In this paper we focus on a recent approach to case-based reasoning, supported by an instantiation of abstract argumentation whereby arguments represent cases and attack between arguments results from outcome disagreement between cases and a notion of relevance. In this context, relevance is connected to a form of specificity among cases. We explore how relevance can be learnt automatically in practice with the help of decision trees, and explore the combination of case-based reasoning with abstract argumentation (AA-CBR) and learning of case relevance for prediction in legal settings. Specifically, we show that, for two legal datasets, AA-CBR and decision-tree-based learning of case relevance perform competitively in comparison with decision trees. We also show that AA-CBR with decision-tree-based learning of case relevance results in a more compact representation than their decision tree counterparts, which could be beneficial for obtaining cognitively tractable explanations.
</details>
<details>
<summary>摘要</summary>
Case-based reasoning 知名于多个法律场景中扮演重要角色。在这篇论文中，我们关注了一种最近的case-based reasoning方法，基于抽象论证的实现，其中Arguments表示案例，而冲突 междуArguments来自结果不一致 между案例和一种 relevance 的概念。在这个上下文中， relevance 与案例特定性相关。我们研究如何在实践中自动学习case relevance，使用决策树，并将case-based reasoning与抽象论证（AA-CBR）和学习案例相关性结合以进行预测。我们发现，对于两个法律数据集，AA-CBR和决策树学习案例相关性可以与决策树相比，并且AA-CBR与决策树学习案例相关性可以获得更加紧凑的表示，这可能有助于获得更易于理解的解释。
</details></li>
</ul>
<hr>
<h2 id="LLMaAA-Making-Large-Language-Models-as-Active-Annotators"><a href="#LLMaAA-Making-Large-Language-Models-as-Active-Annotators" class="headerlink" title="LLMaAA: Making Large Language Models as Active Annotators"></a>LLMaAA: Making Large Language Models as Active Annotators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19596">http://arxiv.org/abs/2310.19596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ridiculouz/LLMaAA">https://github.com/ridiculouz/LLMaAA</a></li>
<li>paper_authors: Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, Lei Zou</li>
<li>for: 这篇论文旨在探讨如何充分利用大型自然语言处理（NLP）模型，以减少需要大量高质量标注数据的训练成本。</li>
<li>methods: 论文提出了一种称为LLMaAA的方法，将大型NLP模型（LLMs）训练为标注员，并将其置入一个活动学习循环中，以有效地决定需要标注的内容。</li>
<li>results: 实验结果显示， LLMaAA 可以比较有效率地将大量未标注数据训练为NLP任务，并且可以对任务特定的模型进行训练，以获得更高的性能。<details>
<summary>Abstract</summary>
Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.
</details>
<details>
<summary>摘要</summary>
通用的监督学习方法在自然语言处理（NLP）领域广泛使用，但这些方法需要大量高质量标注数据。在实践中，获取这些数据是一件昂贵的困难任务。在最近，大语言模型（LLM）的优异几个shot性能促进了数据生成的发展，其中数据 solely 由 LLM 生成。然而，这种方法通常受到低质量问题的困扰，需要数量级更多的标注数据以达到满意性。为了充分利用 LLM 的潜力并使用巨量的未标注数据，我们提出了 LLMaAA，它将 LLM 作为标注者，并将其放入活动学习循环来确定如何效率地标注。为了学习Robustly 使用 pseudo 标签，我们优化了标注和训练过程：（1）我们从小示例池中随机选择 k-NN 示例，并将其作为上下文示例使用，（2）我们采用示例权重技术，将训练样本分配learnable 权重。与之前的方法相比，LLMaAA 具有高效和可靠的特点。我们在名实Recognition 和关系抽取两个 класси型 NLP 任务上进行了实验和分析。使用 LLMaAA，由 LLM 生成的标签上训练的任务特定模型可以在只有百度之前达到教师水平，这是其他基eline 相比的费用更高的。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Locally-Stationary-Data-Using-Expert-Advice"><a href="#Prediction-of-Locally-Stationary-Data-Using-Expert-Advice" class="headerlink" title="Prediction of Locally Stationary Data Using Expert Advice"></a>Prediction of Locally Stationary Data Using Expert Advice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19591">http://arxiv.org/abs/2310.19591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vladimir V’yugin, Vladimir Trunov</li>
<li>for: 本研究探讨了连续机器学习的问题。</li>
<li>methods: 本文使用游戏理论方法，不假设数据源的随机性，可以是抽象、算法或概率性，数据源的参数可以随机变化。提出了一种在线预测算法。</li>
<li>results: 提出的算法可以有效地预测本地站点时间序列。获得了效率估计。<details>
<summary>Abstract</summary>
The problem of continuous machine learning is studied. Within the framework of the game-theoretic approach, when for calculating the next forecast, no assumptions about the stochastic nature of the source that generates the data flow are used -- the source can be analog, algorithmic or probabilistic, its parameters can change at random times, when building a prognostic model, only structural assumptions are used about the nature of data generation. An online forecasting algorithm for a locally stationary time series is presented. An estimate of the efficiency of the proposed algorithm is obtained.
</details>
<details>
<summary>摘要</summary>
“ continuous machine learning 问题被研究。使用游戏理论方法时，不假设数据流源的随机性，数据流源可以是杂音、算法或概率的，它的参数可以随机时间变化，建立预测模型时只假设数据生成的结构。一种在本地站点时间序列上线预测算法被提出。对提出的算法的效率估计得到。”Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="CreoleVal-Multilingual-Multitask-Benchmarks-for-Creoles"><a href="#CreoleVal-Multilingual-Multitask-Benchmarks-for-Creoles" class="headerlink" title="CreoleVal: Multilingual Multitask Benchmarks for Creoles"></a>CreoleVal: Multilingual Multitask Benchmarks for Creoles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19567">http://arxiv.org/abs/2310.19567</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hclent/creoleval">https://github.com/hclent/creoleval</a></li>
<li>paper_authors: Heather Lent, Kushal Tatariya, Raj Dabre, Yiyi Chen, Marcell Fekete, Esther Ploeger, Li Zhou, Hans Erik Heje, Diptesh Kanojia, Paul Belony, Marcel Bollmann, Loïc Grobol, Miryam de Lhoneux, Daniel Hershcovich, Michel DeGraff, Anders Søgaard, Johannes Bjerva</li>
<li>for: 这个论文的目的是为了推动计算语言学和自然语言处理领域中关于克里奥语言的研究。</li>
<li>methods: 这篇论文使用了多种方法，包括创建了8种NLP任务的benchmark集合，以及对这些任务的零例设定实验。</li>
<li>results: 这篇论文的结果表明，通过利用克里奥语言之间的共同根，可以在zero-shot设定下实现transfer learning，并且可以为克里奥语言提供更多的 annotated data。<details>
<summary>Abstract</summary>
Creoles represent an under-explored and marginalized group of languages, with few available resources for NLP research. While the genealogical ties between Creoles and other highly-resourced languages imply a significant potential for transfer learning, this potential is hampered due to this lack of annotated data. In this work we present CreoleVal, a collection of benchmark datasets spanning 8 different NLP tasks, covering up to 28 Creole languages; it is an aggregate of brand new development datasets for machine comprehension, relation classification, and machine translation for Creoles, in addition to a practical gateway to a handful of preexisting benchmarks. For each benchmark, we conduct baseline experiments in a zero-shot setting in order to further ascertain the capabilities and limitations of transfer learning for Creoles. Ultimately, the goal of CreoleVal is to empower research on Creoles in NLP and computational linguistics. We hope this resource will contribute to technological inclusion for Creole language users around the globe.
</details>
<details>
<summary>摘要</summary>
创ollages represent an under-explored and marginalized group of languages, with few available resources for NLP research. While the genealogical ties between 创ollages and other highly-resourced languages imply a significant potential for transfer learning, this potential is hampered due to the lack of annotated data. In this work, we present CreoleVal, a collection of benchmark datasets spanning 8 different NLP tasks, covering up to 28 创ollages; it is an aggregate of brand new development datasets for machine comprehension, relation classification, and machine translation for 创ollages, in addition to a practical gateway to a handful of preexisting benchmarks. For each benchmark, we conduct baseline experiments in a zero-shot setting in order to further ascertain the capabilities and limitations of transfer learning for 创ollages. Ultimately, the goal of CreoleVal is to empower research on 创ollages in NLP and computational linguistics. We hope this resource will contribute to technological inclusion for 创ollages language users around the globe.
</details></li>
</ul>
<hr>
<h2 id="A-General-Neural-Causal-Model-for-Interactive-Recommendation"><a href="#A-General-Neural-Causal-Model-for-Interactive-Recommendation" class="headerlink" title="A General Neural Causal Model for Interactive Recommendation"></a>A General Neural Causal Model for Interactive Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19519">http://arxiv.org/abs/2310.19519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Liu, Xinyan Su, Peng Zhou, Xiangyu Zhao, Jun Li</li>
<li>for: This paper aims to mitigate survivor bias in observational data to optimize recommender systems towards global optima.</li>
<li>methods: The paper proposes a neural causal model to achieve counterfactual inference, using a learnable structural causal model, Gumbel-max function, and reinforcement optimizations.</li>
<li>results: The paper demonstrates the effectiveness of the proposed solution through both theoretical and empirical studies.<details>
<summary>Abstract</summary>
Survivor bias in observational data leads the optimization of recommender systems towards local optima. Currently most solutions re-mines existing human-system collaboration patterns to maximize longer-term satisfaction by reinforcement learning. However, from the causal perspective, mitigating survivor effects requires answering a counterfactual problem, which is generally unidentifiable and inestimable. In this work, we propose a neural causal model to achieve counterfactual inference. Specifically, we first build a learnable structural causal model based on its available graphical representations which qualitatively characterizes the preference transitions. Mitigation of the survivor bias is achieved though counterfactual consistency. To identify the consistency, we use the Gumbel-max function as structural constrains. To estimate the consistency, we apply reinforcement optimizations, and use Gumbel-Softmax as a trade-off to get a differentiable function. Both theoretical and empirical studies demonstrate the effectiveness of our solution.
</details>
<details>
<summary>摘要</summary>
specifically，我们首先构建了一个可学习的结构 causal model，基于可用的图形表示来质量地描述人们的喜好转移。在 mitigating  survivor bias 时，我们使用 counterfactual consistency。为了识别一致性，我们使用 Gumbel-max 函数作为结构约束。为了估计一致性，我们应用了强化优化，并使用 Gumbel-Softmax 作为一个可导的函数。 Both theoretical 和实验研究表明我们的解决方案的效果。
</details></li>
</ul>
<hr>
<h2 id="Inverse-folding-for-antibody-sequence-design-using-deep-learning"><a href="#Inverse-folding-for-antibody-sequence-design-using-deep-learning" class="headerlink" title="Inverse folding for antibody sequence design using deep learning"></a>Inverse folding for antibody sequence design using deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19513">http://arxiv.org/abs/2310.19513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frédéric A. Dreyer, Daniel Cutting, Constantin Schneider, Henry Kenlay, Charlotte M. Deane</li>
<li>for: 本研究是针对抗体序列设计，使用三维结构信息。</li>
<li>methods: 我们提出了一种特化于抗体结构的反向折叠模型，并对抗体序列恢复和结构稳定性进行了优化。我们还研究了补充定量区域的编码方法，并对这些方法进行了评估。</li>
<li>results: 我们的模型在抗体序列设计方面显示了 notable improvement，特别是在Hypervariable CDR-H3 loop中。我们还发现了一些physics-based方法可以用于评估提议的序列质量。<details>
<summary>Abstract</summary>
We consider the problem of antibody sequence design given 3D structural information. Building on previous work, we propose a fine-tuned inverse folding model that is specifically optimised for antibody structures and outperforms generic protein models on sequence recovery and structure robustness when applied on antibodies, with notable improvement on the hypervariable CDR-H3 loop. We study the canonical conformations of complementarity-determining regions and find improved encoding of these loops into known clusters. Finally, we consider the applications of our model to drug discovery and binder design and evaluate the quality of proposed sequences using physics-based methods.
</details>
<details>
<summary>摘要</summary>
我团队考虑了抗体序列设计问题，使用三维结构信息。基于先前的研究，我们提出了特制 inverse folding 模型，特化于抗体结构，并在抗体序列恢复和结构稳定性方面超越了通用蛋白质模型，具有显著改善的质量loop CDR-H3。我们研究了完整性决定区域的 canonical  conformations，并发现了这些循环的更好的编码到已知群体。最后，我们考虑了我们模型的应用于药物发现和绑定设计，并使用物理学方法评估提议的序列质量。Note that Simplified Chinese is a standardized form of Chinese that uses shorter words and phrases, and is commonly used in mainland China. Traditional Chinese is another form of Chinese that is commonly used in Hong Kong, Macau, and Taiwan, and has a more complex grammar and character set. If you prefer Traditional Chinese, I can provide the translation as well.
</details></li>
</ul>
<hr>
<h2 id="SparseByteNN-A-Novel-Mobile-Inference-Acceleration-Framework-Based-on-Fine-Grained-Group-Sparsity"><a href="#SparseByteNN-A-Novel-Mobile-Inference-Acceleration-Framework-Based-on-Fine-Grained-Group-Sparsity" class="headerlink" title="SparseByteNN: A Novel Mobile Inference Acceleration Framework Based on Fine-Grained Group Sparsity"></a>SparseByteNN: A Novel Mobile Inference Acceleration Framework Based on Fine-Grained Group Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19509">http://arxiv.org/abs/2310.19509</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lswzjuer/SparseByteNN">https://github.com/lswzjuer/SparseByteNN</a></li>
<li>paper_authors: Haitao Xu, Songwei Liu, Yuyang Xu, Shuai Wang, Jiashi Li, Chenqian Yan, Liangqiang Li, Lean Fu, Xin Pan, Fangmin Chen</li>
<li>for: 本研究旨在解决网络大小增加的挑战，通过网络剔除来实现高速并高精度的移动推理。</li>
<li>methods: 本研究提出了一种新的移动推理加速框架——SparseByteNN，它利用细化的kernel缺省来实现实时执行并高精度。SparseByteNN包括两部分：（a）细化kernel缺省schema，其中设计了多种缺省模式 для不同的运算符。与我们提出的整个网络重新排序策略相结合，这个schema可以实现高压缩率和高精度同时。（b）基于缺省模式的推理引擎。</li>
<li>results: 实验结果表明，SparseByteNN在Qualcomm 855上对30% sparse MobileNet-v1实现1.27倍的速度提升和1.29倍的效率提升，与dense版本和State-of-the-art sparse推理引擎MNN相比，只有0.224%的精度下降。<details>
<summary>Abstract</summary>
To address the challenge of increasing network size, researchers have developed sparse models through network pruning. However, maintaining model accuracy while achieving significant speedups on general computing devices remains an open problem. In this paper, we present a novel mobile inference acceleration framework SparseByteNN, which leverages fine-grained kernel sparsity to achieve real-time execution as well as high accuracy. Our framework consists of two parts: (a) A fine-grained kernel sparsity schema with a sparsity granularity between structured pruning and unstructured pruning. It designs multiple sparse patterns for different operators. Combined with our proposed whole network rearrangement strategy, the schema achieves a high compression rate and high precision at the same time. (b) Inference engine co-optimized with the sparse pattern. The conventional wisdom is that this reduction in theoretical FLOPs does not translate into real-world efficiency gains. We aim to correct this misconception by introducing a family of efficient sparse kernels for ARM and WebAssembly. Equipped with our efficient implementation of sparse primitives, we show that sparse versions of MobileNet-v1 outperform strong dense baselines on the efficiency-accuracy curve. Experimental results on Qualcomm 855 show that for 30% sparse MobileNet-v1, SparseByteNN achieves 1.27x speedup over the dense version and 1.29x speedup over the state-of-the-art sparse inference engine MNN with a slight accuracy drop of 0.224%. The source code of SparseByteNN will be available at https://github.com/lswzjuer/SparseByteNN
</details>
<details>
<summary>摘要</summary>
为了解决网络规模的挑战，研究人员已经通过网络剔除来开发了稀疏模型。然而，在普通计算设备上实现显著的速度提升 while 保持模型准确性仍然是一个开放的问题。在这篇论文中，我们提出了一种新的移动设备推理加速框架SparseByteNN，该框架利用细化的kernel疏松来实现实时执行和高精度。我们的框架包括两部分：(a) 细化kernel疏松Schema，该Schema设计了多种不同操作符的疏松模式。通过我们的提议的整个网络重新排序策略，Schema可以同时实现高压缩率和高精度。(b) 推理引擎与疏松模式相似的优化。传统的思路是，这种理论的FLOPs减少不会在实际中带来性能提升。我们希望通过引入高效的疏松元素来改变这种误区，并证明稀疏版的MobileNet-v1可以在效率-准确度曲线上超越强 dense 基准。我们的实验结果表明，在Qualcomm 855上，为30%稀疏的MobileNet-v1，SparseByteNN可以相比 dense 版本提供1.27倍的速度提升和1.29倍的速度提升，与MNN相比有一定的精度下降（0.224%）。SparseByteNN的源代码将在https://github.com/lswzjuer/SparseByteNN上公开。
</details></li>
</ul>
<hr>
<h2 id="Trust-Accountability-and-Autonomy-in-Knowledge-Graph-based-AI-for-Self-determination"><a href="#Trust-Accountability-and-Autonomy-in-Knowledge-Graph-based-AI-for-Self-determination" class="headerlink" title="Trust, Accountability, and Autonomy in Knowledge Graph-based AI for Self-determination"></a>Trust, Accountability, and Autonomy in Knowledge Graph-based AI for Self-determination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19503">http://arxiv.org/abs/2310.19503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis-Daniel Ibáñez, John Domingue, Sabrina Kirrane, Oshani Seneviratne, Aisling Third, Maria-Esther Vidal</li>
<li>for: 本研究旨在支持基于知识 graphs（KGs）的人工智能（AI），以确保公民自主权的保障。</li>
<li>methods: 本研究使用了基于 neuronal learning（e.g., Large Language Models（LLMs））的 neuro-symbolic AI，以探讨如何使AI系统的输出可信、数据驱动和内部工作方式可见，以及如何使AI系统负责决策的问题。</li>
<li>results: 本研究提出了一个研究计划，以支持KG-based AI的发展，并在实际场景中analyzed了公民自主权的挑战和机遇。<details>
<summary>Abstract</summary>
Knowledge Graphs (KGs) have emerged as fundamental platforms for powering intelligent decision-making and a wide range of Artificial Intelligence (AI) services across major corporations such as Google, Walmart, and AirBnb. KGs complement Machine Learning (ML) algorithms by providing data context and semantics, thereby enabling further inference and question-answering capabilities. The integration of KGs with neuronal learning (e.g., Large Language Models (LLMs)) is currently a topic of active research, commonly named neuro-symbolic AI. Despite the numerous benefits that can be accomplished with KG-based AI, its growing ubiquity within online services may result in the loss of self-determination for citizens as a fundamental societal issue. The more we rely on these technologies, which are often centralised, the less citizens will be able to determine their own destinies. To counter this threat, AI regulation, such as the European Union (EU) AI Act, is being proposed in certain regions. The regulation sets what technologists need to do, leading to questions concerning: How can the output of AI systems be trusted? What is needed to ensure that the data fuelling and the inner workings of these artefacts are transparent? How can AI be made accountable for its decision-making? This paper conceptualises the foundational topics and research pillars to support KG-based AI for self-determination. Drawing upon this conceptual framework, challenges and opportunities for citizen self-determination are illustrated and analysed in a real-world scenario. As a result, we propose a research agenda aimed at accomplishing the recommended objectives.
</details>
<details>
<summary>摘要</summary>
知识 graphs (KGs) 已经成为智能决策的基础 плаform，并在许多大公司，如 Google、Walmart 和 Airbnb 中使用。KGs 补充机器学习 (ML) 算法，提供数据 контекст和 semantics，因此可以实现更多的推理和问答能力。目前，KGs 与 neuronal learning（例如大语言模型）的结合，被称为 neuro-symbolic AI，是活跃的研究领域。然而，随着 KG-based AI 的广泛应用，公民的自主权可能会遭受威胁。我们越依赖这些中央化的技术，我们就越无法决定自己的命运。为了解决这个问题，AI 的规制，如欧盟 (EU) AI 法规，在某些地区被提出。这些法规规定了技术人员需要做什么，导致问题：如何确保 AI 系统输出的可信？如何确保数据驱动和这些 artifacts 的内部工作是透明的？如何让 AI 受到决策的责任？本文概括了基础主题和研究柱，以支持 KG-based AI 的自主权。基于这个概念框架，我们 illustrated 和分析了在实际场景中的挑战和机遇，并提出了研究计划，以实现这些目标。
</details></li>
</ul>
<hr>
<h2 id="Optimize-Planning-Heuristics-to-Rank-not-to-Estimate-Cost-to-Goal"><a href="#Optimize-Planning-Heuristics-to-Rank-not-to-Estimate-Cost-to-Goal" class="headerlink" title="Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal"></a>Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19463">http://arxiv.org/abs/2310.19463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aicenter/optimize-planning-heuristics-to-rank">https://github.com/aicenter/optimize-planning-heuristics-to-rank</a></li>
<li>paper_authors: Leah Chrestien, Tomás Pevný, Stefan Edelkamp, Antonín Komenda</li>
<li>for: 本研究旨在优化搜索算法中的启发函数参数，以提高搜索效率。</li>
<li>methods: 本研究使用了解决过的问题实例来优化启发函数参数，并提出了一家基于排名的损失函数家族。</li>
<li>results: 实验比较表明，基于排名的损失函数能够有效地优化搜索算法的性能。<details>
<summary>Abstract</summary>
In imitation learning for planning, parameters of heuristic functions are optimized against a set of solved problem instances. This work revisits the necessary and sufficient conditions of strictly optimally efficient heuristics for forward search algorithms, mainly A* and greedy best-first search, which expand only states on the returned optimal path. It then proposes a family of loss functions based on ranking tailored for a given variant of the forward search algorithm. Furthermore, from a learning theory point of view, it discusses why optimizing cost-to-goal \hstar\ is unnecessarily difficult. The experimental comparison on a diverse set of problems unequivocally supports the derived theory.
</details>
<details>
<summary>摘要</summary>
在模仿学习方面的规划中，规则函数的参数被优化对于一组解决过的问题实例。这项工作检查了前向搜索算法，主要是A*和偏好最佳搜索，它们只在返回优质路径上展开状态。然后，它提出了一个基于排名的家族函数，特制为给定的前向搜索算法。此外，从学习理论的角度来看，它讨论了优化成本到目标的成本是无法很容易的。实验比较在多种问题上，不可遗憾地支持 derive 的理论。Here's the breakdown of the translation:* "imitation learning" is translated as "模仿学习" (mó shì xué xí)* "parameters of heuristic functions" is translated as "规则函数的参数" (guī zhèng fún xiàng)* "optimized against a set of solved problem instances" is translated as "被优化对于一组解决过的问题实例" (bèi yǐng gāo jì zhèng yì jī)* "strictly optimally efficient heuristics" is translated as "前向搜索算法" (qian yù sōu xiǎng suān)* "which expand only states on the returned optimal path" is translated as "它们只在返回优质路径上展开状态" (tā men zhǐ shàng zhèng jì lù pí thàn)* "loss functions based on ranking" is translated as "基于排名的家族函数" (jī bù pinyīn de jiā zú fún)* "from a learning theory point of view" is translated as "从学习理论的角度来看" (cong xué xí lǐ yì zhì)* "optimizing cost-to-goal \hstar\ is unnecessarily difficult" is translated as "优化成本到目标的成本是无法很容易的" (yòu gǎn jī zhèng yì jī zhèng yì)* "The experimental comparison on a diverse set of problems unequivocally supports the derived theory" is translated as "实验比较在多种问题上，不可遗憾地支持 derive 的理论" (shí yè bǐ jiǎo zài yī yī wèn tí zhèng yì)
</details></li>
</ul>
<hr>
<h2 id="Denoising-Diffusion-Probabilistic-Models-for-Hardware-Impaired-Communication-Systems-Towards-Wireless-Generative-AI"><a href="#Denoising-Diffusion-Probabilistic-Models-for-Hardware-Impaired-Communication-Systems-Towards-Wireless-Generative-AI" class="headerlink" title="Denoising Diffusion Probabilistic Models for Hardware-Impaired Communication Systems: Towards Wireless Generative AI"></a>Denoising Diffusion Probabilistic Models for Hardware-Impaired Communication Systems: Towards Wireless Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19460">http://arxiv.org/abs/2310.19460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Letafati, Samad Ali, Matti Latva-aho</li>
<li>for: 这个论文旨在提出一种实用的无线通信系统，以便在具有硬件缺陷的接收器下实现数据生成。</li>
<li>methods: 该论文使用的方法是基于扩散模型（DDPM），它将数据生成过程分解成多个步骤，以减少数据生成过程中的噪声。</li>
<li>results: 该论文的结果表明，使用该方法可以在低SNR条件下提供网络可靠性，并且在不同的硬件缺陷水平和量化误差下实现几乎不变的重建性表现。同时，该方法还可以在非泊松噪声下实现稳定的重建性表现。<details>
<summary>Abstract</summary>
Thanks to the outstanding achievements from state-of-the-art generative models like ChatGPT and diffusion models, generative AI has gained substantial attention across various industrial and academic domains. In this paper, denoising diffusion probabilistic models (DDPMs) are proposed for a practical finite-precision wireless communication system with hardware-impaired transceivers. The intuition behind DDPM is to decompose the data generation process over the so-called "denoising" steps. Inspired by this, a DDPM-based receiver is proposed for a practical wireless communication scheme that faces realistic non-idealities, including hardware impairments (HWI), channel distortions, and quantization errors. It is shown that our approach provides network resilience under low-SNR regimes, near-invariant reconstruction performance with respect to different HWI levels and quantization errors, and robust out-of-distribution performance against non-Gaussian noise. Moreover, the reconstruction performance of our scheme is evaluated in terms of cosine similarity and mean-squared error (MSE), highlighting more than 25 dB improvement compared to the conventional deep neural network (DNN)-based receivers.
</details>
<details>
<summary>摘要</summary>
thanks to the outstanding achievements of state-of-the-art generative models like ChatGPT and diffusion models, generative AI has gained substantial attention across various industrial and academic domains. In this paper, denoising diffusion probabilistic models (DDPMs) are proposed for a practical finite-precision wireless communication system with hardware-impaired transceivers. The intuition behind DDPM is to decompose the data generation process over the so-called "denoising" steps. Inspired by this, a DDPM-based receiver is proposed for a practical wireless communication scheme that faces realistic non-idealities, including hardware impairments (HWI), channel distortions, and quantization errors. It is shown that our approach provides network resilience under low-SNR regimes, near-invariant reconstruction performance with respect to different HWI levels and quantization errors, and robust out-of-distribution performance against non-Gaussian noise. Moreover, the reconstruction performance of our scheme is evaluated in terms of cosine similarity and mean-squared error (MSE), highlighting more than 25 dB improvement compared to the conventional deep neural network (DNN)-based receivers.Here's the translation in Traditional Chinese:感谢现代 générative AI 的杰出成就，如 ChatGPT 和扩散模型，它们在不同的工业和学术领域获得了广泛的注意。在这篇论文中，我们提出了一种实用的 finite-precision 无线通信系统，使用受到硬件问题的接收器。DDPM 的假设是将数据生成过程分解为“混杂”步骤。受到这个想法的激发，我们提出了一种基于 DDPM 的接收器，用于实际的无线通信问题，面临实际的非理想情况，包括硬件问题（HWI）、频道扭曲和采样误差。我们展示了我们的方法在低 SNR 情况下提供了网络可靠性，并且在不同的 HWI 水平和采样误差水平下提供了近似对称重建性和Robust OOD 性。此外，我们评估了我们的方法的重建性，使用 cosine similarity 和 mean-squared error (MSE)，显示了较于 25 dB 的改善，相比于传统的深度神经网络（DNN）基于接收器。
</details></li>
</ul>
<hr>
<h2 id="ALT-Towards-Fine-grained-Alignment-between-Language-and-CTR-Models-for-Click-Through-Rate-Prediction"><a href="#ALT-Towards-Fine-grained-Alignment-between-Language-and-CTR-Models-for-Click-Through-Rate-Prediction" class="headerlink" title="ALT: Towards Fine-grained Alignment between Language and CTR Models for Click-Through Rate Prediction"></a>ALT: Towards Fine-grained Alignment between Language and CTR Models for Click-Through Rate Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19453">http://arxiv.org/abs/2310.19453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hangyu Wang, Jianghao Lin, Xiangyang Li, Bo Chen, Chenxu Zhu, Ruiming Tang, Weinan Zhang, Yong Yu</li>
<li>for: 这 paper 的目的是提出一种新的 CTR 预测方法，以提高个性化在线服务中 CTR 预测的准确率。</li>
<li>methods: 这 paper 使用了两种不同的模型，一种是传统的 CTR 模型，它使用一个简单的一阶FeatueInteraction模型来捕捉协作信号。另一种是使用 pretrained language models (PLMs) 来提取语义知识，并将其与 CTR 模型结合使用。</li>
<li>results: 该 paper 的实验结果表明，使用 ALT 方法可以大幅提高 CTR 预测的准确率，并且可以与不同的语言和 CTR 模型结合使用。<details>
<summary>Abstract</summary>
Click-through rate (CTR) prediction plays as a core function module in various personalized online services. According to the data modality and input format, the models for CTR prediction can be mainly classified into two categories. The first one is the traditional CTR models that take as inputs the one-hot encoded ID features of tabular modality, which aims to capture the collaborative signals via feature interaction modeling. The second category takes as inputs the sentences of textual modality obtained by hard prompt templates, where pretrained language models (PLMs) are adopted to extract the semantic knowledge. These two lines of research generally focus on different characteristics of the same input data (i.e., textual and tabular modalities), forming a distinct complementary relationship with each other. Therefore, in this paper, we propose to conduct fine-grained feature-level Alignment between Language and CTR models (ALT) for CTR prediction. Apart from the common CLIP-like instance-level contrastive learning, we further design a novel joint reconstruction pretraining task for both masked language and tabular modeling. Specifically, the masked data of one modality (i.e., tokens or features) has to be recovered with the help of the other modality, which establishes the feature-level interaction and alignment via sufficient mutual information extraction between dual modalities. Moreover, we propose three different finetuning strategies with the option to train the aligned language and CTR models separately or jointly for downstream CTR prediction tasks, thus accommodating the varying efficacy and efficiency requirements for industrial applications. Extensive experiments on three real-world datasets demonstrate that ALT outperforms SOTA baselines, and is highly compatible for various language and CTR models.
</details>
<details>
<summary>摘要</summary>
Click-through rate（CTR）预测作为个人化在线服务的核心功能模块，可以根据数据模式和输入格式分为两类模型。第一类是传统的 CTR 模型，通过一个简单的一Hot编码ID特征来捕捉协作信号，而第二类则是使用硬模板生成的文本数据，通过预训练语言模型（PLM）提取semantic知识。这两种研究方向通常都专注于输入数据（即文本和表格modalities）的不同特征，形成了一种明确的补做关系。因此，在这篇论文中，我们提议通过细致的特征水平匹配（ALT）来进行 CTR 预测。除了常见的 CLIP-like 类型的INSTANCE-level对比学习外，我们还设计了一种新的联合重建预训练任务，以便在语言和表格模型之间建立特征水平的交互和匹配。此外，我们还提出了三种不同的Finetuning策略，可以根据下游应用的效率和可行性要求来训练一起或分别训练语言和 CTR 模型。广泛的实验表明，ALT 可以超过当前的基eline，并且可以高效地与不同的语言和 CTR 模型结合使用。
</details></li>
</ul>
<hr>
<h2 id="Large-Scale-Application-of-Fault-Injection-into-PyTorch-Models-–-an-Extension-to-PyTorchFI-for-Validation-Efficiency"><a href="#Large-Scale-Application-of-Fault-Injection-into-PyTorch-Models-–-an-Extension-to-PyTorchFI-for-Validation-Efficiency" class="headerlink" title="Large-Scale Application of Fault Injection into PyTorch Models – an Extension to PyTorchFI for Validation Efficiency"></a>Large-Scale Application of Fault Injection into PyTorch Models – an Extension to PyTorchFI for Validation Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19449">http://arxiv.org/abs/2310.19449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ralf Graafe, Qutub Syed Sha, Florian Geissler, Michael Paulitsch<br>for:This paper aims to address the issue of silent data errors (SDE) in Neural Networks (NN) caused by hardware faults, and to develop a novel fault injection framework called PyTorchALFI to study the effects of hardware faults on software and NN models.methods:The paper uses a novel fault injection framework called PyTorchALFI, which is based on PyTorchFI, to inject faults into PyTorch models and study their effects. The framework provides an efficient way to define randomly generated and reusable sets of faults, enhances data sets, and generates test KPIs.results:The paper provides details about the definition of test scenarios, software architecture, and several examples of how to use the new framework to apply iterative changes in fault location and number, compare different model modifications, and analyze test results. The results show that PyTorchALFI can effectively study the effects of hardware faults on NN models and provide insights into the robustness of these models.Here is the simplified Chinese translation of the three points:for:这篇论文目标是解决神经网络（NN）中的静默数据错误（SDE），由硬件错误引起的问题，并开发了一个新的硬件投入框架called PyTorchALFI。方法:论文使用基于PyTorchFI的PyTorchALFI框架，将硬件错误投入到PyTorch模型中，以研究其影响。该框架提供了一种高效的 randomly生成和重用的硬件错误集，增强数据集，并生成测试KPI。结果:论文提供了测试场景的定义，软件架构，以及使用新框架进行迭代更改硬件错误的位置和数量，比较不同的模型修改，并分析测试结果的方法。结果表明，PyTorchALFI可以有效地研究硬件错误对NN模型的影响，并为NN模型的可靠性提供了新的视角。<details>
<summary>Abstract</summary>
Transient or permanent faults in hardware can render the output of Neural Networks (NN) incorrect without user-specific traces of the error, i.e. silent data errors (SDE). On the other hand, modern NNs also possess an inherent redundancy that can tolerate specific faults. To establish a safety case, it is necessary to distinguish and quantify both types of corruptions. To study the effects of hardware (HW) faults on software (SW) in general and NN models in particular, several fault injection (FI) methods have been established in recent years. Current FI methods focus on the methodology of injecting faults but often fall short of accounting for large-scale FI tests, where many fault locations based on a particular fault model need to be analyzed in a short time. Results need to be concise, repeatable, and comparable. To address these requirements and enable fault injection as the default component in a machine learning development cycle, we introduce a novel fault injection framework called PyTorchALFI (Application Level Fault Injection for PyTorch) based on PyTorchFI. PyTorchALFI provides an efficient way to define randomly generated and reusable sets of faults to inject into PyTorch models, defines complex test scenarios, enhances data sets, and generates test KPIs while tightly coupling fault-free, faulty, and modified NN. In this paper, we provide details about the definition of test scenarios, software architecture, and several examples of how to use the new framework to apply iterative changes in fault location and number, compare different model modifications, and analyze test results.
</details>
<details>
<summary>摘要</summary>
非暂时或永久的硬件故障可以让神经网络（NN）输出错误无法诊断到用户特定的错误迹象，即静默数据错误（SDE）。然而，现代NN也拥有内置的重复性，可以承受特定的故障。为建立安全案例，需要分化和评估两种类型的损害。为了研究硬件（HW）故障对软件（SW）的影响，以及NN模型的影响，数年来有多种硬件故障插入（FI）方法得到了创新。当前FI方法通常专注于插入故障的方法，而很少考虑大规模FI测试，其中需要对特定故障模型中的多个故障位置进行分析，并在短时间内进行测试。结果需要是简洁、重复、比较。为了解决这些需求并使硬件故障插入成为机器学习开发周期中的默认组件，我们介绍了一个新的硬件故障插入框架 called PyTorchALFI（基于PyTorch的应用层硬件故障插入）。PyTorchALFI提供了一种效率的方式来定义随机生成的和可重用的硬件故障，定义复杂的测试场景，增强数据集，生成测试KPI，同时紧密地集成 fault-free、FAULTY 和修改后的NN。在这篇文章中，我们提供了定义测试场景、软件架构以及多个例子，用于使用新框架进行应用iterative改变故障位置和数量，比较不同的模型修改，以及分析测试结果。
</details></li>
</ul>
<hr>
<h2 id="Explaining-the-Decisions-of-Deep-Policy-Networks-for-Robotic-Manipulations"><a href="#Explaining-the-Decisions-of-Deep-Policy-Networks-for-Robotic-Manipulations" class="headerlink" title="Explaining the Decisions of Deep Policy Networks for Robotic Manipulations"></a>Explaining the Decisions of Deep Policy Networks for Robotic Manipulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19432">http://arxiv.org/abs/2310.19432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seongun Kim, Jaesik Choi</li>
<li>for: This paper aims to enhance the transparency of deep policy networks in robotic manipulation by explaining robot behaviors through input attribution methods.</li>
<li>methods: The paper presents two methods for applying input attribution methods to robot policy networks: (1) measuring the importance factor of each joint torque and (2) modifying a relevance propagation method to handle negative inputs and outputs.</li>
<li>results: The paper reports on the dynamic changes of input attributions of multi-modal sensor inputs in deep policy networks online for robotic manipulation, to the best of the authors’ knowledge.Here’s the simplified Chinese text format you requested:</li>
<li>for: 这篇论文目标是通过输入归因方法解释深度策略网络在机器人控制中的行为。</li>
<li>methods: 论文提出了两种应用输入归因方法到机器人策略网络的方法：（1）测量每个 JOINT 扭矩的重要性因子，以反映机器人的 END-EFFECTOR 运动受 JOINT 扭矩的影响；（2）修改 relevance propagation 方法，以正确处理深度策略网络中的负输入和输出。</li>
<li>results: 论文报告了深度策略网络在机器人控制中的输入归因变化，特别是在多Modal 感知输入中。<details>
<summary>Abstract</summary>
Deep policy networks enable robots to learn behaviors to solve various real-world complex tasks in an end-to-end fashion. However, they lack transparency to provide the reasons of actions. Thus, such a black-box model often results in low reliability and disruptive actions during the deployment of the robot in practice. To enhance its transparency, it is important to explain robot behaviors by considering the extent to which each input feature contributes to determining a given action. In this paper, we present an explicit analysis of deep policy models through input attribution methods to explain how and to what extent each input feature affects the decisions of the robot policy models. To this end, we present two methods for applying input attribution methods to robot policy networks: (1) we measure the importance factor of each joint torque to reflect the influence of the motor torque on the end-effector movement, and (2) we modify a relevance propagation method to handle negative inputs and outputs in deep policy networks properly. To the best of our knowledge, this is the first report to identify the dynamic changes of input attributions of multi-modal sensor inputs in deep policy networks online for robotic manipulation.
</details>
<details>
<summary>摘要</summary>
深度策略网络可以让机器人学习解决各种复杂实际任务的终端方式。然而，它缺乏透明度，不能提供行为的原因。因此，这种黑盒模型在实践中常导致低可靠性和干扰行为。为了增强其透明度，需要解释机器人行为，考虑每个输入特征对决策的影响程度。在这篇论文中，我们通过输入贡献方法来解释深度策略模型中每个输入特征对机器人决策的影响。我们提出了两种应用输入贡献方法来解释机器人策略网络：（1）测量每个 JOINT 扭矩的重要性因子，以反映电动机扭矩对终端运动的影响，（2）修改 relevance propagation 方法，以处理深度策略网络中的负输入和输出。我们认为，这是首次在深度策略网络中在线识别多Modal 感知输入的动态变化。
</details></li>
</ul>
<hr>
<h2 id="Refining-Diffusion-Planner-for-Reliable-Behavior-Synthesis-by-Automatic-Detection-of-Infeasible-Plans"><a href="#Refining-Diffusion-Planner-for-Reliable-Behavior-Synthesis-by-Automatic-Detection-of-Infeasible-Plans" class="headerlink" title="Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans"></a>Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19427">http://arxiv.org/abs/2310.19427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leekwoon/rgg">https://github.com/leekwoon/rgg</a></li>
<li>paper_authors: Kyowoon Lee, Seongun Kim, Jaesik Choi</li>
<li>for: 这 paper 的目的是提出一种新的方法来修正由扩散模型生成的不可靠的计划，以便在安全关键应用中使用扩散模型。</li>
<li>methods: 该 paper 使用的方法包括提出了一种新的 metric  named restoration gap，以评估扩散模型生成的计划质量，并提出了一种 attribution map regularizer 来防止由 sub-optimal gap predictor 生成的 adversarial refining guidance。</li>
<li>results: 该 paper 在三个不同的 benchmark 上进行了评估，并证明了其效果。同时， paper 还提供了一种 explainability 的机制，通过显示 gap predictor 的归因地图，以便更深入地理解生成的计划。<details>
<summary>Abstract</summary>
Diffusion-based planning has shown promising results in long-horizon, sparse-reward tasks by training trajectory diffusion models and conditioning the sampled trajectories using auxiliary guidance functions. However, due to their nature as generative models, diffusion models are not guaranteed to generate feasible plans, resulting in failed execution and precluding planners from being useful in safety-critical applications. In this work, we propose a novel approach to refine unreliable plans generated by diffusion models by providing refining guidance to error-prone plans. To this end, we suggest a new metric named restoration gap for evaluating the quality of individual plans generated by the diffusion model. A restoration gap is estimated by a gap predictor which produces restoration gap guidance to refine a diffusion planner. We additionally present an attribution map regularizer to prevent adversarial refining guidance that could be generated from the sub-optimal gap predictor, which enables further refinement of infeasible plans. We demonstrate the effectiveness of our approach on three different benchmarks in offline control settings that require long-horizon planning. We also illustrate that our approach presents explainability by presenting the attribution maps of the gap predictor and highlighting error-prone transitions, allowing for a deeper understanding of the generated plans.
</details>
<details>
<summary>摘要</summary>
这文本将被翻译为简化字Simplified Chinese。Diffusion-based planning在长时间、罕见的奖励任务中显示出了有前途的结果，通过训练涡散模型和使用辅助指南函数来条件sampled trajectories。然而，由于它们是生成型模型，所以diffusion models不能保证生成可行的计划，导致执行失败和禁止planners在安全敏感应用中使用。在这个工作中，我们提出了一种新的方法，用于精焕不可靠的计划，包括提供修复指南来修正错误的计划。为此，我们建议了一个新的度量名为修复差，用于评估单一计划的质量。修复差是由一个差分预测器生成的修复差指南，用于修正不可靠的计划。我们还提出了一个属性图调整仪来防止由低质量的差分预测器生成的对抗式修复指南，允许进一步修正不可靠的计划。我们在三个不同的benchmark上证明了我们的方法的有效性，并显示了我们的方法具有解释性，通过显示差分预测器的属性图和显示错误的转换，允许更深入的理解生成的计划。
</details></li>
</ul>
<hr>
<h2 id="Artificial-intelligence-and-the-limits-of-the-humanities"><a href="#Artificial-intelligence-and-the-limits-of-the-humanities" class="headerlink" title="Artificial intelligence and the limits of the humanities"></a>Artificial intelligence and the limits of the humanities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19425">http://arxiv.org/abs/2310.19425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Włodzisław Duch</li>
<li>for: 本研究旨在探讨现代世界中文化复杂性的问题，以及人类认知的限制和人工智能的发展对人文学科的影响。</li>
<li>methods: 本研究采用了跨学科的方法，包括认知科学、人工智能和数据分析等，以探讨人文学科在数字时代中的发展和变革。</li>
<li>results: 研究发现，人工智能的发展将导致人文学科的重要性减退，而新的、跨学科的人文学科将出现，可以为现代社会提供更多的解决方案。此外，人工智能将对人文学科产生深远的影响，从艺术到政治科学和哲学，使这些领域变得更加吸引人和有前途。<details>
<summary>Abstract</summary>
The complexity of cultures in the modern world is now beyond human comprehension. Cognitive sciences cast doubts on the traditional explanations based on mental models. The core subjects in humanities may lose their importance. Humanities have to adapt to the digital age. New, interdisciplinary branches of humanities emerge. Instant access to information will be replaced by instant access to knowledge. Understanding the cognitive limitations of humans and the opportunities opened by the development of artificial intelligence and interdisciplinary research necessary to address global challenges is the key to the revitalization of humanities. Artificial intelligence will radically change humanities, from art to political sciences and philosophy, making these disciplines attractive to students and enabling them to go beyond current limitations.
</details>
<details>
<summary>摘要</summary>
现代世界的文化复杂性已经超出了人类理解的限度。认知科学怀疑传统基于心理模型的解释。核心人文科目可能会失去重要性。人文科学需要适应数字时代。新的跨学科人文科学出现。快速获取信息将被快速获取知识所替代。理解人类认知限制和人工智能和跨学科研究的发展机遇是人文科学的重要复兴之路。人工智能将对人文科学产生杰然的变革，从艺术到政治科学和哲学，使这些学科变得吸引人并让其走过当前的限制。
</details></li>
</ul>
<hr>
<h2 id="Variational-Curriculum-Reinforcement-Learning-for-Unsupervised-Discovery-of-Skills"><a href="#Variational-Curriculum-Reinforcement-Learning-for-Unsupervised-Discovery-of-Skills" class="headerlink" title="Variational Curriculum Reinforcement Learning for Unsupervised Discovery of Skills"></a>Variational Curriculum Reinforcement Learning for Unsupervised Discovery of Skills</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19424">http://arxiv.org/abs/2310.19424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seongun-kim/vcrl">https://github.com/seongun-kim/vcrl</a></li>
<li>paper_authors: Seongun Kim, Kyowoon Lee, Jaesik Choi</li>
<li>for: 本研究旨在提出一种基于相互信息的强化学习框架，以便自动地无需任务奖励函数而学习复杂技能。</li>
<li>methods: 本研究使用了变量强化学习和目标条件强化学习的概念，并将其重新推出为课程学习。同时，我们还提出了一种基于信息理论的无监督技能发现方法，称为Value Uncertainty Variational Curriculum（VUVC）。</li>
<li>results: 我们的方法在复杂的导航和机器人控制任务中显示了更高的样本效率和状态覆盖速度，并且在实际世界机器人导航任务中成功地应用了已经学习的技能。同时，将这些技能与全球规划器结合使得任务性能得到进一步提高。<details>
<summary>Abstract</summary>
Mutual information-based reinforcement learning (RL) has been proposed as a promising framework for retrieving complex skills autonomously without a task-oriented reward function through mutual information (MI) maximization or variational empowerment. However, learning complex skills is still challenging, due to the fact that the order of training skills can largely affect sample efficiency. Inspired by this, we recast variational empowerment as curriculum learning in goal-conditioned RL with an intrinsic reward function, which we name Variational Curriculum RL (VCRL). From this perspective, we propose a novel approach to unsupervised skill discovery based on information theory, called Value Uncertainty Variational Curriculum (VUVC). We prove that, under regularity conditions, VUVC accelerates the increase of entropy in the visited states compared to the uniform curriculum. We validate the effectiveness of our approach on complex navigation and robotic manipulation tasks in terms of sample efficiency and state coverage speed. We also demonstrate that the skills discovered by our method successfully complete a real-world robot navigation task in a zero-shot setup and that incorporating these skills with a global planner further increases the performance.
</details>
<details>
<summary>摘要</summary>
互信息基 reinforcement learning (RL) 已被提议为自动学习复杂技能的有望框架，无需任务特定奖励函数，通过互信息 (MI) 最大化或变量赋能。然而，学习复杂技能仍然具有挑战，因为训练技能的顺序可以大大影响样本效率。 inspirited by this， we recast variational empowerment as curriculum learning in goal-conditioned RL with an intrinsic reward function, which we name Variational Curriculum RL (VCRL). From this perspective, we propose a novel approach to unsupervised skill discovery based on information theory, called Value Uncertainty Variational Curriculum (VUVC). We prove that, under regularity conditions, VUVC accelerates the increase of entropy in the visited states compared to the uniform curriculum. We validate the effectiveness of our approach on complex navigation and robotic manipulation tasks in terms of sample efficiency and state coverage speed. We also demonstrate that the skills discovered by our method successfully complete a real-world robot navigation task in a zero-shot setup and that incorporating these skills with a global planner further increases the performance.
</details></li>
</ul>
<hr>
<h2 id="Text-to-3D-with-Classifier-Score-Distillation"><a href="#Text-to-3D-with-Classifier-Score-Distillation" class="headerlink" title="Text-to-3D with Classifier Score Distillation"></a>Text-to-3D with Classifier Score Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19415">http://arxiv.org/abs/2310.19415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, Xiaojuan Qi</li>
<li>for: 这个论文主要是为了研究文本到3D形态转换的技术，特别是基于Score Distillation Sampling（SDS）的方法。</li>
<li>methods: 这篇论文使用了一种新的方法，即Classifier Score Distillation（CSD），它利用了预训练的2D扩散模型，并发现了一种意外的发现：导航独立于分类器可以带来有效的文本到3D形态转换任务。</li>
<li>results: 论文在不同的文本到3D任务上，包括形状生成、 текстур合成和形状编辑等，实现了与当前最佳方法相当的效果。 project page: <a target="_blank" rel="noopener" href="https://xinyu-andy.github.io/Classifier-Score-Distillation">https://xinyu-andy.github.io/Classifier-Score-Distillation</a><details>
<summary>Abstract</summary>
Text-to-3D generation has made remarkable progress recently, particularly with methods based on Score Distillation Sampling (SDS) that leverages pre-trained 2D diffusion models. While the usage of classifier-free guidance is well acknowledged to be crucial for successful optimization, it is considered an auxiliary trick rather than the most essential component. In this paper, we re-evaluate the role of classifier-free guidance in score distillation and discover a surprising finding: the guidance alone is enough for effective text-to-3D generation tasks. We name this method Classifier Score Distillation (CSD), which can be interpreted as using an implicit classification model for generation. This new perspective reveals new insights for understanding existing techniques. We validate the effectiveness of CSD across a variety of text-to-3D tasks including shape generation, texture synthesis, and shape editing, achieving results superior to those of state-of-the-art methods. Our project page is https://xinyu-andy.github.io/Classifier-Score-Distillation
</details>
<details>
<summary>摘要</summary>
文本到3D生成技术在近期内Has made remarkable progress, especially with methods based on Score Distillation Sampling (SDS) that leverages pre-trained 2D diffusion models. Although the use of classifier-free guidance is widely recognized as crucial for successful optimization, it is considered an auxiliary trick rather than the most essential component. In this paper, we re-evaluate the role of classifier-free guidance in score distillation and discover a surprising finding: the guidance alone is sufficient for effective text-to-3D generation tasks. We name this method Classifier Score Distillation (CSD), which can be interpreted as using an implicit classification model for generation. This new perspective reveals new insights into existing techniques. We validate the effectiveness of CSD across a variety of text-to-3D tasks, including shape generation, texture synthesis, and shape editing, achieving results superior to those of state-of-the-art methods. Our project page is <https://xinyu-andy.github.io/Classifier-Score-Distillation>.
</details></li>
</ul>
<hr>
<h2 id="Resource-Constrained-Semantic-Segmentation-for-Waste-Sorting"><a href="#Resource-Constrained-Semantic-Segmentation-for-Waste-Sorting" class="headerlink" title="Resource Constrained Semantic Segmentation for Waste Sorting"></a>Resource Constrained Semantic Segmentation for Waste Sorting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19407">http://arxiv.org/abs/2310.19407</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anubis09/Resource_Constrained_Semantic_Segmentation_for_Waste_Sorting">https://github.com/anubis09/Resource_Constrained_Semantic_Segmentation_for_Waste_Sorting</a></li>
<li>paper_authors: Elisa Cascina, Andrea Pellegrino, Lorenzo Tozzi</li>
<li>for: 这个研究旨在发展一种高效的垃圾分类策略，以减少垃圾的环境影响。</li>
<li>methods: 本研究提出了一种具有10MB内存限制的资源受限 semantic segmentation模型，适用于对工业设置进行垃圾分类。研究将ICNet、BiSeNet（Xception39底层）和ENet三种网络作为实验用例。</li>
<li>results: 经过实验后，研究发现可以通过优化和剪裁技术，实现模型的优化，并且可以对垃圾分类 Task 取得更好的性能。此外，研究还提出了一种焦点和lovász损失函数的结合，以解决垃圾分类 зада难以掌握的隐藏类别问题。<details>
<summary>Abstract</summary>
This work addresses the need for efficient waste sorting strategies in Materials Recovery Facilities to minimize the environmental impact of rising waste. We propose resource-constrained semantic segmentation models for segmenting recyclable waste in industrial settings. Our goal is to develop models that fit within a 10MB memory constraint, suitable for edge applications with limited processing capacity. We perform the experiments on three networks: ICNet, BiSeNet (Xception39 backbone), and ENet. Given the aforementioned limitation, we implement quantization and pruning techniques on the broader nets, achieving positive results while marginally impacting the Mean IoU metric. Furthermore, we propose a combination of Focal and Lov\'asz loss that addresses the implicit class imbalance resulting in better performance compared with the Cross-entropy loss function.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Othello-is-Solved"><a href="#Othello-is-Solved" class="headerlink" title="Othello is Solved"></a>Othello is Solved</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19387">http://arxiv.org/abs/2310.19387</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mwarot1/othello-ai">https://github.com/mwarot1/othello-ai</a></li>
<li>paper_authors: Hiroki Takizawa</li>
<li>for: 这篇论文是解决扑克游戏“奥特洛”的 computationally solving 问题的。</li>
<li>methods: 该论文使用了计算机科学中的搜索技术来解决游戏。</li>
<li>results: 该论文提出了一个重要突破，即确认了扑克游戏的完美游戏结果，即无误的游戏结果。<details>
<summary>Abstract</summary>
The game of Othello is one of the world's most complex and popular games that has yet to be computationally solved. Othello has roughly ten octodecillion (10 to the 58th power) possible game records and ten octillion (10 to the 28th power) possible game position. The challenge of solving Othello, determining the outcome of a game with no mistake made by either player, has long been a grand challenge in computer science. This paper announces a significant milestone: Othello is now solved, computationally proved that perfect play by both players lead to a draw. Strong Othello software has long been built using heuristically designed search techniques. Solving a game provides the solution which enables software to play the game perfectly.
</details>
<details>
<summary>摘要</summary>
投筹游戏“奥赛”是全球最复杂且受欢迎的游戏之一，尚未被计算解决。奥赛拥有约10个十进制废弃数（10到58次方）可能的游戏记录和10个十进制废弃数（10到28次方）可能的游戏位置。解决奥赛的挑战，即确定没有任何错误的游戏记录，长期被计算机科学视为一项大挑战。本文宣布了一项重要突破：奥赛已经被计算解决，确定了完美游戏记录，导致游戏双方都不会赢。强大的奥赛软件已经使用了经验设计的搜索技术。解决游戏提供了解决方案，使软件可以完美地游戏。
</details></li>
</ul>
<hr>
<h2 id="Protecting-Publicly-Available-Data-With-Machine-Learning-Shortcuts"><a href="#Protecting-Publicly-Available-Data-With-Machine-Learning-Shortcuts" class="headerlink" title="Protecting Publicly Available Data With Machine Learning Shortcuts"></a>Protecting Publicly Available Data With Machine Learning Shortcuts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19381">http://arxiv.org/abs/2310.19381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas M. Müller, Maximilian Burgert, Pascal Debus, Jennifer Williams, Philip Sperl, Konstantin Böttinger</li>
<li>for: 保护在线数据库免受非法抓取（protect online databases from unauthorized crawling）</li>
<li>methods: 利用机器学习快捷（ML shortcuts）的潜在假 correlate（potential spurious correlations），并通过采用异常值填充（outlier filling）技术来防止其泄露（leakage）。</li>
<li>results: 通过实验结果表明，提出的方法可以在三个实际应用场景中减少非法抓取的风险，同时也减少了人类的察觉阈值（threshold of human perception）。<details>
<summary>Abstract</summary>
Machine-learning (ML) shortcuts or spurious correlations are artifacts in datasets that lead to very good training and test performance but severely limit the model's generalization capability. Such shortcuts are insidious because they go unnoticed due to good in-domain test performance. In this paper, we explore the influence of different shortcuts and show that even simple shortcuts are difficult to detect by explainable AI methods. We then exploit this fact and design an approach to defend online databases against crawlers: providers such as dating platforms, clothing manufacturers, or used car dealers have to deal with a professionalized crawling industry that grabs and resells data points on a large scale. We show that a deterrent can be created by deliberately adding ML shortcuts. Such augmented datasets are then unusable for ML use cases, which deters crawlers and the unauthorized use of data from the internet. Using real-world data from three use cases, we show that the proposed approach renders such collected data unusable, while the shortcut is at the same time difficult to notice in human perception. Thus, our proposed approach can serve as a proactive protection against illegitimate data crawling.
</details>
<details>
<summary>摘要</summary>
（简化中文）机器学习（ML）快捷或假相关性是数据集中的假象，它们会导致模型在训练和测试阶段表现非常好，但是很难在泛化中使用。这些快捷是隐藏的，因为它们在域内测试中表现非常好。在这篇论文中，我们研究了不同的快捷的影响，并证明了简单的快捷也很难被解释AI方法发现。然后，我们利用这一点，设计了一种防止网络数据库被抓取的方法：提供者如约会平台、时尚制造商或二手车销售商需要面对专业化的抓取行业，这些行业会大规模地抓取并重新销售数据点。我们显示了可以通过故意添加ML快捷来创建一种防止抓取的办法。这些扩展的数据集是不可用于ML用例，这种方法可以防止抓取和未经授权使用数据。使用实际数据，我们显示了我们的方法可以干扰收集的数据，同时快捷难以被人类发现。因此，我们的方法可以作为一种积极的数据保护 measure。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Hybrid-Domain-Adaptation-of-Image-Generators"><a href="#Few-shot-Hybrid-Domain-Adaptation-of-Image-Generators" class="headerlink" title="Few-shot Hybrid Domain Adaptation of Image Generators"></a>Few-shot Hybrid Domain Adaptation of Image Generators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19378">http://arxiv.org/abs/2310.19378</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/echopluto/fhda">https://github.com/echopluto/fhda</a></li>
<li>paper_authors: Hengjia Li, Yang Liu, Linxuan Xia, Yuqi Lin, Tu Zheng, Zheng Yang, Wenxiao Wang, Xiaohui Zhong, Xiaobo Ren, Xiaofei He</li>
<li>for: 能否适应多个目标领域的混合领域？</li>
<li>methods: 我们提出了一种无损器框架，通过直接编码不同领域的图像到可分离的子空间来解决这问题。我们还提出了一种新的方向loss，该损失函数包括距离损失和方向损失。</li>
<li>results: 我们的方法可以在单个适应器中获得多个目标领域的各种特征，比基eline方法在 semantic similarity、图像准确率和 cross-domain consistency 方面表现出色。<details>
<summary>Abstract</summary>
Can a pre-trained generator be adapted to the hybrid of multiple target domains and generate images with integrated attributes of them? In this work, we introduce a new task -- Few-shot Hybrid Domain Adaptation (HDA). Given a source generator and several target domains, HDA aims to acquire an adapted generator that preserves the integrated attributes of all target domains, without overriding the source domain's characteristics. Compared with Domain Adaptation (DA), HDA offers greater flexibility and versatility to adapt generators to more composite and expansive domains. Simultaneously, HDA also presents more challenges than DA as we have access only to images from individual target domains and lack authentic images from the hybrid domain. To address this issue, we introduce a discriminator-free framework that directly encodes different domains' images into well-separable subspaces. To achieve HDA, we propose a novel directional subspace loss comprised of a distance loss and a direction loss. Concretely, the distance loss blends the attributes of all target domains by reducing the distances from generated images to all target subspaces. The direction loss preserves the characteristics from the source domain by guiding the adaptation along the perpendicular to subspaces. Experiments show that our method can obtain numerous domain-specific attributes in a single adapted generator, which surpasses the baseline methods in semantic similarity, image fidelity, and cross-domain consistency.
</details>
<details>
<summary>摘要</summary>
可以使用预训练的生成器适应多个目标Domain的混合吗？在这项工作中，我们介绍了一个新任务——几shot Hybrid Domain Adaptation（HDA）。给定一个源生成器和多个目标Domain，HDA的目标是获得一个适应了所有目标Domain的特征的生成器，而不是覆盖源Domain的特征。相比于Domain Adaptation（DA），HDA具有更多的灵活性和可能性，以适应更复杂和广泛的Domain。然而，HDA也面临着更多的挑战，因为我们只有各个目标Domain的图像，缺乏真实的混合Domain的图像。为解决这个问题，我们提出了一个无需权重的框架，直接将不同的Domain的图像编码成分离的子空间。为实现HDA，我们提议一种新的方向性子空间损失，包括距离损失和方向损失。具体来说，距离损失将所有目标Domain的特征混合到生成图像中，而方向损失保持源Domain的特征，使适应过程垂直于子空间。实验表明，我们的方法可以在单个适应器中获得多个Domain的特征，超越基eline方法的semantic similarity、图像准确性和交叉Domain一致性。
</details></li>
</ul>
<hr>
<h2 id="RGB-X-Object-Detection-via-Scene-Specific-Fusion-Modules"><a href="#RGB-X-Object-Detection-via-Scene-Specific-Fusion-Modules" class="headerlink" title="RGB-X Object Detection via Scene-Specific Fusion Modules"></a>RGB-X Object Detection via Scene-Specific Fusion Modules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19372">http://arxiv.org/abs/2310.19372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsriaditya999/rgbxfusion">https://github.com/dsriaditya999/rgbxfusion</a></li>
<li>paper_authors: Sri Aditya Deevi, Connor Lee, Lu Gan, Sushruth Nagesh, Gaurav Pandey, Soon-Jo Chung</li>
<li>for: 提高自动驾驶车辆在各种天气条件下视觉理解环境的能力</li>
<li>methods: 利用小量、协调的多模式数据进行预训练、Scene-specific fusión模块来拼接单模式模型</li>
<li>results: 与现有方法相比，实现更高的融合性能，只需要小量额外参数Translation:</li>
<li>for: 提高自动驾驶车辆在各种天气条件下视觉理解环境的能力</li>
<li>methods: 利用小量、协调的多模式数据进行预训练、Scene-specific fusión模块来拼接单模式模型</li>
<li>results: 与现有方法相比，实现更高的融合性能，只需要小量额外参数<details>
<summary>Abstract</summary>
Multimodal deep sensor fusion has the potential to enable autonomous vehicles to visually understand their surrounding environments in all weather conditions. However, existing deep sensor fusion methods usually employ convoluted architectures with intermingled multimodal features, requiring large coregistered multimodal datasets for training. In this work, we present an efficient and modular RGB-X fusion network that can leverage and fuse pretrained single-modal models via scene-specific fusion modules, thereby enabling joint input-adaptive network architectures to be created using small, coregistered multimodal datasets. Our experiments demonstrate the superiority of our method compared to existing works on RGB-thermal and RGB-gated datasets, performing fusion using only a small amount of additional parameters. Our code is available at https://github.com/dsriaditya999/RGBXFusion.
</details>
<details>
<summary>摘要</summary>
多模式深度感知融合有能力使自动驾驶车辆在所有天气条件下视觉理解周围环境。然而，现有的深度感知融合方法通常采用复杂的建筑，混合多种模式特征，需要大量相关的多模式数据进行训练。在这个工作中，我们提出了一种高效和可Module的 RGB-X 融合网络，可以通过场景特定的融合模块来利用和融合预训练的单模式模型，从而实现 joint 输入适应网络架构，使用只需小量相关的多模式数据进行融合。我们的实验表明我们的方法比现有的RGB-thermal和RGB-gated数据集上的方法更高效，只需要小量额外参数。我们的代码可以在https://github.com/dsriaditya999/RGBXFusion上下载。
</details></li>
</ul>
<hr>
<h2 id="Balance-Imbalance-and-Rebalance-Understanding-Robust-Overfitting-from-a-Minimax-Game-Perspective"><a href="#Balance-Imbalance-and-Rebalance-Understanding-Robust-Overfitting-from-a-Minimax-Game-Perspective" class="headerlink" title="Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective"></a>Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19360">http://arxiv.org/abs/2310.19360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pku-ml/rebat">https://github.com/pku-ml/rebat</a></li>
<li>paper_authors: Yifei Wang, Liangchen Li, Jiansheng Yang, Zhouchen Lin, Yisen Wang</li>
<li>for: 本文研究了 adversarial training (AT) Algorithm 的一个问题：robust overfitting。</li>
<li>methods: 作者通过视 adversarial training 为动态的最小化游戏来解释这个问题，并分析了 learning rate (LR)  decay 如何使得模型师更强大的记忆能力，从而导致 robust overfitting。</li>
<li>results: 作者通过了大量实验 validate 这个理解，并提供了一个整体的视图，以及一种解决 robust overfitting 的方法：ReBalanced Adversarial Training (ReBAT)。实验表明，ReBAT 可以实现好的 robustness，并不会出现 robust overfitting 问题，即使是非常长的训练时间。<details>
<summary>Abstract</summary>
Adversarial Training (AT) has become arguably the state-of-the-art algorithm for extracting robust features. However, researchers recently notice that AT suffers from severe robust overfitting problems, particularly after learning rate (LR) decay. In this paper, we explain this phenomenon by viewing adversarial training as a dynamic minimax game between the model trainer and the attacker. Specifically, we analyze how LR decay breaks the balance between the minimax game by empowering the trainer with a stronger memorization ability, and show such imbalance induces robust overfitting as a result of memorizing non-robust features. We validate this understanding with extensive experiments, and provide a holistic view of robust overfitting from the dynamics of both the two game players. This understanding further inspires us to alleviate robust overfitting by rebalancing the two players by either regularizing the trainer's capacity or improving the attack strength. Experiments show that the proposed ReBalanced Adversarial Training (ReBAT) can attain good robustness and does not suffer from robust overfitting even after very long training. Code is available at https://github.com/PKU-ML/ReBAT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Modified-Genetic-Algorithm-for-Feature-Selection-and-Hyper-Parameter-Optimization-Case-of-XGBoost-in-Spam-Prediction"><a href="#Modified-Genetic-Algorithm-for-Feature-Selection-and-Hyper-Parameter-Optimization-Case-of-XGBoost-in-Spam-Prediction" class="headerlink" title="Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction"></a>Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19845">http://arxiv.org/abs/2310.19845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazeeh Ghatasheh, Ismail Altaharwa, Khaled Aldebei</li>
<li>for: 本研究旨在提出一种修改后的遗传算法，用于同时维度减少和超参优化，以提高Twitter社交媒体上的诈骗预测模型的性能。</li>
<li>methods: 该方法首先初始化了一个eXtreme Gradient Boosting类фика器，然后使用修改后的遗传算法将特征空间缩减至 tweets 数据集中，以生成一个诈骗预测模型。模型采用10次10个分割的十fold混合验证，并通过非 Parametric Statistical Tests 进行分析。</li>
<li>results: 实验结果表明，修改后的遗传算法在平均地达到了82.32%和92.67%的几何均值和准确率，并且使用了 less than 10% 的总特征空间。此外，eXtreme Gradient Boosting 超越了许多机器学习算法，包括 BERT 深度学习模型，在诈骗预测方面。此外，该方法还应用于短信诈骗预测模型，并与相关研究进行比较。<details>
<summary>Abstract</summary>
Recently, spam on online social networks has attracted attention in the research and business world. Twitter has become the preferred medium to spread spam content. Many research efforts attempted to encounter social networks spam. Twitter brought extra challenges represented by the feature space size, and imbalanced data distributions. Usually, the related research works focus on part of these main challenges or produce black-box models. In this paper, we propose a modified genetic algorithm for simultaneous dimensionality reduction and hyper parameter optimization over imbalanced datasets. The algorithm initialized an eXtreme Gradient Boosting classifier and reduced the features space of tweets dataset; to generate a spam prediction model. The model is validated using a 50 times repeated 10-fold stratified cross-validation, and analyzed using nonparametric statistical tests. The resulted prediction model attains on average 82.32\% and 92.67\% in terms of geometric mean and accuracy respectively, utilizing less than 10\% of the total feature space. The empirical results show that the modified genetic algorithm outperforms $Chi^2$ and $PCA$ feature selection methods. In addition, eXtreme Gradient Boosting outperforms many machine learning algorithms, including BERT-based deep learning model, in spam prediction. Furthermore, the proposed approach is applied to SMS spam modeling and compared to related works.
</details>
<details>
<summary>摘要</summary>
近些时间，社交媒体上的垃圾信息引起了研究和业务界的关注。推特成为了垃圾信息的主要传播媒体。许多研究努力以解决社交媒体上的垃圾信息问题。推特带来了额外的挑战，包括特征空间大小和数据分布不均。通常，相关研究工作会专注于这些主要挑战中的一部分，或生成黑盒模型。在这篇论文中，我们提出了一种修改后的遗传算法，用于同时维度减少和超参优化不均衡数据集。该算法首先初始化了一个极限梯度拟合分类器，然后减少了推特数据集的特征空间，以生成一个垃圾预测模型。该模型通过10次重复的10个分割验证，并通过非 Parametric 统计测试进行验证。结果表明，修改后的遗传算法在平均地达到82.32%和92.67%的垃圾预测率和准确率，使用了 menos than 10%的总特征空间。实验结果表明，修改后的遗传算法超过了$Chi^2$和$PCA$特征选择方法。此外，极限梯度拟合也超过了许多机器学习算法，包括基于BERT的深度学习模型，在垃圾预测方面。此外，我们的方法还应用于短信垃圾预测，并与相关研究相比较。
</details></li>
</ul>
<hr>
<h2 id="Introducing-instance-label-correlation-in-multiple-instance-learning-Application-to-cancer-detection-on-histopathological-images"><a href="#Introducing-instance-label-correlation-in-multiple-instance-learning-Application-to-cancer-detection-on-histopathological-images" class="headerlink" title="Introducing instance label correlation in multiple instance learning. Application to cancer detection on histopathological images"></a>Introducing instance label correlation in multiple instance learning. Application to cancer detection on histopathological images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19359">http://arxiv.org/abs/2310.19359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Morales-Álvarez, Arne Schmidt, José Miguel Hernández-Lobato, Rafael Molina</li>
<li>for: 这个论文是为了解决计算生物学中的多例学习问题，具体是用于肠癌检测。</li>
<li>methods: 该论文基于 Gaussian Processes（GP）的多例学习方法，并在这些方法中加入了一个新的牵引项，这个牵引项是基于统计物理学的 Ising 模型。</li>
<li>results: 该论文在两个实际问题中进行了评估，并表明了其在肠癌检测中的更好的性能，比如其他现有的可能性MAP 方法。  Additionally, the paper provides different visualizations and analysis to gain insights into the influence of the novel Ising term, which can be applied to other research areas.<details>
<summary>Abstract</summary>
In the last years, the weakly supervised paradigm of multiple instance learning (MIL) has become very popular in many different areas. A paradigmatic example is computational pathology, where the lack of patch-level labels for whole-slide images prevents the application of supervised models. Probabilistic MIL methods based on Gaussian Processes (GPs) have obtained promising results due to their excellent uncertainty estimation capabilities. However, these are general-purpose MIL methods that do not take into account one important fact: in (histopathological) images, the labels of neighboring patches are expected to be correlated. In this work, we extend a state-of-the-art GP-based MIL method, which is called VGPMIL-PR, to exploit such correlation. To do so, we develop a novel coupling term inspired by the statistical physics Ising model. We use variational inference to estimate all the model parameters. Interestingly, the VGPMIL-PR formulation is recovered when the weight that regulates the strength of the Ising term vanishes. The performance of the proposed method is assessed in two real-world problems of prostate cancer detection. We show that our model achieves better results than other state-of-the-art probabilistic MIL methods. We also provide different visualizations and analysis to gain insights into the influence of the novel Ising term. These insights are expected to facilitate the application of the proposed model to other research areas.
</details>
<details>
<summary>摘要</summary>
最近几年，弱地监督多个实例学习（MIL）的思想在各个领域得到了广泛的应用。一个典型的例子是计算生物学，因为整个扫描图像缺乏小块级别标签，无法应用有监督的模型。基于 Gaussian Processes（GP）的概率MIL方法在这些领域已经取得了一定的成果，因为它们可以提供出色的不确定性估计能力。然而，这些是通用MIL方法，不考虑一个重要的事实：在生物学图像中，邻近的小块标签是相关的。在这种情况下，我们将一种状态艺术GP-based MIL方法，称为VGPMIL-PR，扩展以利用这种相关性。我们开发了一个独特的联系项， inspirited by the statistical physics Ising model。我们使用变量推断来估计所有模型参数。有趣的是，VGPMIL-PR的形式在weight that regulates the strength of the Ising term为零时被恢复。我们在两个实际问题中评估了我们的模型，即肾癌检测。我们发现我们的模型比其他状态艺术概率MIL方法更好。我们还提供了不同的视觉化和分析，以获得相关联系项的影响的视觉。这些视觉预期能够促进模型在其他领域的应用。
</details></li>
</ul>
<hr>
<h2 id="Modeling-the-Telemarketing-Process-using-Genetic-Algorithms-and-Extreme-Boosting-Feature-Selection-and-Cost-Sensitive-Analytical-Approach"><a href="#Modeling-the-Telemarketing-Process-using-Genetic-Algorithms-and-Extreme-Boosting-Feature-Selection-and-Cost-Sensitive-Analytical-Approach" class="headerlink" title="Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach"></a>Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19843">http://arxiv.org/abs/2310.19843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazeeh Ghatasheh, Ismail Altaharwa, Khaled Aldebei</li>
<li>For: This research aims to leverage telemarketing data to model the willingness of clients to make a term deposit and to find the most significant characteristics of the clients.* Methods: The research uses a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously, and builds an explainable prediction model.* Results: The models significantly outperform related works in terms of class of interest accuracy, with an average of 89.07% and a type I error of 0.059. The model is expected to maximize the potential profit margin at the least possible cost and provide more insights to support marketing decision-making.Here are the three points in Simplified Chinese text:* For: 这些研究目的是利用电话营销数据来模型客户是否愿意投资贷款，以及客户最重要的特征。* Methods: 这些研究使用一种新的遗传算法基于分类器来选择最佳特征和调整分类器参数同时。* Results: 这些模型与相关研究相比，在类别 интерес精度方面表现出色，具有89.07%的平均值和0.059的类型 I 错误。这些模型预计可以最小化成本， simultaneously maximize potential profit margin and provide more insights to support marketing decision-making。<details>
<summary>Abstract</summary>
Currently, almost all direct marketing activities take place virtually rather than in person, weakening interpersonal skills at an alarming pace. Furthermore, businesses have been striving to sense and foster the tendency of their clients to accept a marketing offer. The digital transformation and the increased virtual presence forced firms to seek novel marketing research approaches. This research aims at leveraging the power of telemarketing data in modeling the willingness of clients to make a term deposit and finding the most significant characteristics of the clients. Real-world data from a Portuguese bank and national socio-economic metrics are used to model the telemarketing decision-making process. This research makes two key contributions. First, propose a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously. Second, build an explainable prediction model. The best-generated classification models were intensively validated using 50 times repeated 10-fold stratified cross-validation and the selected features have been analyzed. The models significantly outperform the related works in terms of class of interest accuracy, they attained an average of 89.07\% and 0.059 in terms of geometric mean and type I error respectively. The model is expected to maximize the potential profit margin at the least possible cost and provide more insights to support marketing decision-making.
</details>
<details>
<summary>摘要</summary>
（currently, almost all direct marketing activities are taking place virtually instead of in person, which is weakening interpersonal skills at an alarming rate. in addition, businesses have been trying to sense and foster their clients' willingness to accept marketing offers. the digital transformation and the increased virtual presence have forced companies to seek out new marketing research methods. this research aims to use telemarketing data to model clients' willingness to make a term deposit and to identify the most important characteristics of clients. real-world data from a portuguese bank and national socio-economic metrics are used to model the telemarketing decision-making process. this research makes two key contributions. first, it proposes a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously. second, it builds an explainable prediction model. the best-generated classification models were intensively validated using 50 times repeated 10-fold stratified cross-validation, and the selected features have been analyzed. the models significantly outperform the related works in terms of class of interest accuracy, with an average of 89.07% and a type i error of 0.059. the model is expected to maximize the potential profit margin at the least possible cost and provide more insights to support marketing decision-making.）
</details></li>
</ul>
<hr>
<h2 id="Improving-Factual-Consistency-of-Text-Summarization-by-Adversarially-Decoupling-Comprehension-and-Embellishment-Abilities-of-LLMs"><a href="#Improving-Factual-Consistency-of-Text-Summarization-by-Adversarially-Decoupling-Comprehension-and-Embellishment-Abilities-of-LLMs" class="headerlink" title="Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs"></a>Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19347">http://arxiv.org/abs/2310.19347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huawen Feng, Yan Fan, Xiong Liu, Ting-En Lin, Zekun Yao, Yuchuan Wu, Fei Huang, Yongbin Li, Qianli Ma</li>
<li>for: 提高基于大语言模型（LLM）的文本概要的可靠性</li>
<li>methods: 提出了一种逆反解耦方法，通过分离把握和膨胀能力来避免LLM生成的幻觉</li>
<li>results: 实验结果表明，使用DECENT方法可以显著提高基于LLM的文本概要的可靠性<details>
<summary>Abstract</summary>
Despite the recent progress in text summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as "hallucinations" in text generation. Unlike previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, and overgeneralizing, etc. These hallucinations are challenging to detect through traditional methods, which poses great challenges for improving the factual consistency of text summarization. In this paper, we propose an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based parameter-efficient technique to cover the shortage of sensitivity for true and false in the training process of LLMs. In this way, LLMs are less confused about embellishing and understanding, thus can execute the instructions more accurately and have enhanced abilities to distinguish hallucinations. Experimental results show that DECENT significantly improves the reliability of text summarization based on LLMs.
</details>
<details>
<summary>摘要</summary>
尽管最近大语言模型（LLMs）在文本摘要方面做出了一些进步，但它们经常生成的摘要与原文不匹配，这被称为“幻觉”（hallucinations）在文本生成中。与过去的小型模型（如BART、T5）相比，当前的LLMs更多地生成了更复杂的幻觉，如强制 causal relationship、添加假信息和过度总结等。这些幻觉通过传统方法很难以检测，这对改进文本摘要的准确性带来了巨大的挑战。在这篇论文中，我们提出了一种对抗分解方法，以分离LLMs的理解和赋予能力（DECENT）。此外，我们采用了一种参数高效的探测技术，以补做LLMs在训练过程中对真假敏感的缺失。这样，LLMs会更加精准地执行 instrucciones，并且具有更强的能力来分辨幻觉。实验结果表明，DECENT可以很有效地改进基于LLMs的文本摘要的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Skywork-A-More-Open-Bilingual-Foundation-Model"><a href="#Skywork-A-More-Open-Bilingual-Foundation-Model" class="headerlink" title="Skywork: A More Open Bilingual Foundation Model"></a>Skywork: A More Open Bilingual Foundation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19341">http://arxiv.org/abs/2310.19341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skyworkai/skywork">https://github.com/skyworkai/skywork</a></li>
<li>paper_authors: Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou</li>
<li>for: 这个技术报告介绍了 Skywork-13B，一个基于大语言模型（LLM）的家族，通过对英文和中文文本集的3.2万亿个字进行训练而得到。</li>
<li>methods: 这个模型使用了两个阶段的训练方法，首先是通用训练，然后是针对特定领域进行增强训练。</li>
<li>results: 这个模型不仅在各种标准测试中表现出色，还在不同领域的中文语言模型中实现了state of the art的性能。此外，作者还提出了一种泄露检测方法，表明测试数据泄露是LLM社区所需要进一步研究的问题。<details>
<summary>Abstract</summary>
In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves \emph{state of the art} performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.
</details>
<details>
<summary>摘要</summary>
在这份技术报告中，我们介绍了 Skywork-13B，一家大型自然语言模型（LLMs），在包含英语和中文文本的词库上进行了3.2亿个字的训练。这是当前最广泛训练和公开发布的相对规模相似的LMMs。我们采用了两个阶段的训练方法，首先是通用训练，然后是领域特定增强训练。我们发现，我们的模型不仅在各种标准测试 benchmark 上表现出色，而且在多个领域的中文语言模型中也达到了state of the art的性能。此外，我们还提出了一种新的泄露检测方法，表明测试数据污染是一个需要进一步调查的问题。为促进未来的研究，我们将 Skywork-13B 及其在 intermediate 阶段的检查点发布，以及一部分我们的 SkyPile 词库，这是当前最大的高质量开放中文预训练词库。我们希望 Skywork-13B 和我们的开放词库能为高质量 LLMS 的民主化提供一个价值的开源资源。
</details></li>
</ul>
<hr>
<h2 id="TempME-Towards-the-Explainability-of-Temporal-Graph-Neural-Networks-via-Motif-Discovery"><a href="#TempME-Towards-the-Explainability-of-Temporal-Graph-Neural-Networks-via-Motif-Discovery" class="headerlink" title="TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery"></a>TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19324">http://arxiv.org/abs/2310.19324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graph-and-geometric-learning/tempme">https://github.com/graph-and-geometric-learning/tempme</a></li>
<li>paper_authors: Jialin Chen, Rex Ying</li>
<li>for: 本研究旨在提高现有的时间图 neural network (TGNN) 的解释性和可靠性，通过找出导致模型预测的时间模式。</li>
<li>methods: 我们提出了一种新的方法，即 Temporal Motifs Explainer (TempME)，它抽象出 TGNN 预测过程中最重要的时间模式，以提高解释性和可靠性。 TempME 基于信息瓶颈理论，旨在抽象出最重要的互动关系模式，以降低解释中包含的信息量，保持解释的简洁和精炼。</li>
<li>results: 我们的实验结果表明， TempME 可以提高 TGNN 的解释精度和预测精度，相比现有的方法。  especifically，我们在六个实际 datasets 上进行了广泛的实验，并 obtainted 8.21% 的提升在解释精度方面，以及22.96% 的提升在预测精度方面。<details>
<summary>Abstract</summary>
Temporal graphs are widely used to model dynamic systems with time-varying interactions. In real-world scenarios, the underlying mechanisms of generating future interactions in dynamic systems are typically governed by a set of recurring substructures within the graph, known as temporal motifs. Despite the success and prevalence of current temporal graph neural networks (TGNN), it remains uncertain which temporal motifs are recognized as the significant indications that trigger a certain prediction from the model, which is a critical challenge for advancing the explainability and trustworthiness of current TGNNs. To address this challenge, we propose a novel approach, called Temporal Motifs Explainer (TempME), which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs. Derived from the information bottleneck principle, TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation. Events in the explanations generated by TempME are verified to be more spatiotemporally correlated than those of existing approaches, providing more understandable insights. Extensive experiments validate the superiority of TempME, with up to 8.21% increase in terms of explanation accuracy across six real-world datasets and up to 22.96% increase in boosting the prediction Average Precision of current TGNNs.
</details>
<details>
<summary>摘要</summary>
现代系统中的动态系统通常使用时间变化的交互来模型。在实际场景中，这些系统的下一个交互的生成机制通常是由动态系统中的时间模式所控制的。尽管现有的时间图 neural network (TGNN) 已经取得了成功和普遍性，但是仍然未知哪些时间模式是 TGNN 的准确预测Trigger 的重要指示器，这是当前 TGNN 的解释性和可靠性的主要挑战。为解决这个挑战，我们提出了一种新的方法，即 Temporal Motifs Explainer (TempME)，它可以揭示 TGNN 的预测中最重要的时间模式。基于信息瓶颈理论，TempME 提取了最关键的交互相关模式，同时尽量减少包含的信息量，以保持解释的简洁和精炼性。事件在 TempME 生成的解释中被证明更加 espacio-temporal 相关，提供更加理解的启示。广泛的实验证明 TempME 的优越性，在六个实际 dataset 上提高了解释准确率的最高达 8.21%，并在提高当前 TGNN 预测 Average Precision 的最高达 22.96%。
</details></li>
</ul>
<hr>
<h2 id="D4Explainer-In-Distribution-GNN-Explanations-via-Discrete-Denoising-Diffusion"><a href="#D4Explainer-In-Distribution-GNN-Explanations-via-Discrete-Denoising-Diffusion" class="headerlink" title="D4Explainer: In-Distribution GNN Explanations via Discrete Denoising Diffusion"></a>D4Explainer: In-Distribution GNN Explanations via Discrete Denoising Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19321">http://arxiv.org/abs/2310.19321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graph-and-geometric-learning/d4explainer">https://github.com/graph-and-geometric-learning/d4explainer</a></li>
<li>paper_authors: Jialin Chen, Shirley Wu, Abhijit Gupta, Rex Ying</li>
<li>for: 提高 Graph Neural Networks (GNNs) 的解释性，以便更好地理解模型的预测结果，并提高模型的可信度。</li>
<li>methods: 提出了一种新的方法 D4Explainer，该方法通过涵盖生成图分布学习的目标函数，生成了可靠的在分布内的 GNN 解释，包括对应的counterfactual和模型级别解释。</li>
<li>results: 实验结果表明，D4Explainer 可以具有高度的解释准确性、实际性、多样性和稳定性，并且是首个结合 counterfactual 和模型级别解释的总体方法。<details>
<summary>Abstract</summary>
The widespread deployment of Graph Neural Networks (GNNs) sparks significant interest in their explainability, which plays a vital role in model auditing and ensuring trustworthy graph learning. The objective of GNN explainability is to discern the underlying graph structures that have the most significant impact on model predictions. Ensuring that explanations generated are reliable necessitates consideration of the in-distribution property, particularly due to the vulnerability of GNNs to out-of-distribution data. Unfortunately, prevailing explainability methods tend to constrain the generated explanations to the structure of the original graph, thereby downplaying the significance of the in-distribution property and resulting in explanations that lack reliability. To address these challenges, we propose D4Explainer, a novel approach that provides in-distribution GNN explanations for both counterfactual and model-level explanation scenarios. The proposed D4Explainer incorporates generative graph distribution learning into the optimization objective, which accomplishes two goals: 1) generate a collection of diverse counterfactual graphs that conform to the in-distribution property for a given instance, and 2) identify the most discriminative graph patterns that contribute to a specific class prediction, thus serving as model-level explanations. It is worth mentioning that D4Explainer is the first unified framework that combines both counterfactual and model-level explanations. Empirical evaluations conducted on synthetic and real-world datasets provide compelling evidence of the state-of-the-art performance achieved by D4Explainer in terms of explanation accuracy, faithfulness, diversity, and robustness.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 的广泛部署引起了对其解释性的极大兴趣，这对于模型审核和建立信任worthy graph learning是非常重要。GNN解释的目标是找到影响模型预测的基本图结构，以确保生成的解释准确可靠。然而，现有的解释方法通常会限制生成的解释只是原始图结构，这会忽略在分布中的性质，导致解释不准确。为解决这些挑战，我们提出了D4Explainer，一种新的方法，可以为给定实例生成符合分布性的GNN解释，并提供模型水平的解释。D4Explainer通过将生成图分布学习纳入优化目标来实现两个目标：1）生成一个符合分布性的对应实例的多种可能性图，2）标识影响特定预测的最重要的图模式，从而提供模型水平的解释。需要注意的是，D4Explainer是首个结合对应和模型水平解释的统一框架。在synthetic和实际世界数据上进行的实验证明了D4Explainer在解释准确性、忠实度、多样性和Robustness等方面的前所未有的表现。
</details></li>
</ul>
<hr>
<h2 id="L2T-DLN-Learning-to-Teach-with-Dynamic-Loss-Network"><a href="#L2T-DLN-Learning-to-Teach-with-Dynamic-Loss-Network" class="headerlink" title="L2T-DLN: Learning to Teach with Dynamic Loss Network"></a>L2T-DLN: Learning to Teach with Dynamic Loss Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19313">http://arxiv.org/abs/2310.19313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhoyang Hai, Liyuan Pan, Xiabi Liu, Zhengzheng Liu, Mirna Yunita</li>
<li>for: 这篇论文的目的是提出一种基于Dynamic loss函数的教师模型，用于指导学生模型的训练。</li>
<li>methods: 这篇论文使用了一种带有记忆单元的教师模型，以实现学生模型的学习受教导。此外，它还使用了动态损失网络，以使用损失函数的状态来帮助教师学习。</li>
<li>results: 实验表明，这种方法可以提高学生模型的学习效果，并在多种深度模型中提高实际任务的性能，包括分类、目标检测和 semantic segmentation 等。<details>
<summary>Abstract</summary>
With the concept of teaching being introduced to the machine learning community, a teacher model start using dynamic loss functions to teach the training of a student model. The dynamic intends to set adaptive loss functions to different phases of student model learning. In existing works, the teacher model 1) merely determines the loss function based on the present states of the student model, i.e., disregards the experience of the teacher; 2) only utilizes the states of the student model, e.g., training iteration number and loss/accuracy from training/validation sets, while ignoring the states of the loss function. In this paper, we first formulate the loss adjustment as a temporal task by designing a teacher model with memory units, and, therefore, enables the student learning to be guided by the experience of the teacher model. Then, with a dynamic loss network, we can additionally use the states of the loss to assist the teacher learning in enhancing the interactions between the teacher and the student model. Extensive experiments demonstrate our approach can enhance student learning and improve the performance of various deep models on real-world tasks, including classification, objective detection, and semantic segmentation scenarios.
</details>
<details>
<summary>摘要</summary>
随着机器学习社区中的教学概念的引入，一个教师模型开始使用动态损失函数来教育学生模型的训练。动态损失函数的目的是在不同阶段的学生模型学习过程中设置适应的损失函数。现有的工作中，教师模型1）仅根据学生模型当前状态来确定损失函数，即忽略教师模型的经验; 2）仅利用学生模型的状态，如训练迭代数和训练/验证集的损失/准确率，而忽略损失函数的状态。在这篇论文中，我们首先将损失调整视为一个时间任务，通过设计教师模型的记忆单元，以此使学生学习被教师模型的经验导航。然后，通过动态损失网络，我们可以再利用损失的状态来帮助教师学习，以便提高教师和学生模型之间的互动。广泛的实验证明了我们的方法可以提高学生学习并提高各种深度模型在真实任务上的性能，包括分类、目标检测和 semantic segmentation 等场景。
</details></li>
</ul>
<hr>
<h2 id="Free-from-Bellman-Completeness-Trajectory-Stitching-via-Model-based-Return-conditioned-Supervised-Learning"><a href="#Free-from-Bellman-Completeness-Trajectory-Stitching-via-Model-based-Return-conditioned-Supervised-Learning" class="headerlink" title="Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning"></a>Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19308">http://arxiv.org/abs/2310.19308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoyi Zhou, Chuning Zhu, Runlong Zhou, Qiwen Cui, Abhishek Gupta, Simon Shaolei Du</li>
<li>for:  solves sequential decision-making problems with off-policy dynamic programming techniques, but may not converge due to function approximation challenges.</li>
<li>methods:  uses return-conditioned supervised learning (RCSL) to circumvent the challenges of Bellman completeness, with a proven convergence guarantee under more relaxed assumptions.</li>
<li>results:  outperforms state-of-the-art model-free and model-based offline RL algorithms in several simulated robotics problems, and demonstrates the ability to learn from sub-optimal datasets using the proposed MBRCSL framework.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究用off-policy动态Programming技术解决了序列决策问题，但是在函数近似时可能无法保证收敛。</li>
<li>methods: 使用返回受控学习（RCSL）circumvent了 Bellman完整性挑战，并提供了一个证明收敛保证的更松解 assumptions。</li>
<li>results: 在数据预测问题中表现出色，并在多个模拟的 роботех挑战中超越了现有的模型自由和模型基于的offlineRL算法。此外，MBRCSL框架可以学习从不优化的数据中启示出来的动力学模型，并使用前向抽象来实现路径封锁。<details>
<summary>Abstract</summary>
Off-policy dynamic programming (DP) techniques such as $Q$-learning have proven to be an important technique for solving sequential decision-making problems. However, in the presence of function approximation such algorithms are not guaranteed to converge, often diverging due to the absence of Bellman-completeness in the function classes considered, a crucial condition for the success of DP-based methods. In this paper, we show how off-policy learning techniques based on return-conditioned supervised learning (RCSL) are able to circumvent these challenges of Bellman completeness, converging under significantly more relaxed assumptions inherited from supervised learning. We prove there exists a natural environment in which if one uses two-layer multilayer perceptron as the function approximator, the layer width needs to grow linearly with the state space size to satisfy Bellman-completeness while a constant layer width is enough for RCSL. These findings take a step towards explaining the superior empirical performance of RCSL methods compared to DP-based methods in environments with near-optimal datasets. Furthermore, in order to learn from sub-optimal datasets, we propose a simple framework called MBRCSL, granting RCSL methods the ability of dynamic programming to stitch together segments from distinct trajectories. MBRCSL leverages learned dynamics models and forward sampling to accomplish trajectory stitching while avoiding the need for Bellman completeness that plagues all dynamic programming algorithms. We propose both theoretical analysis and experimental evaluation to back these claims, outperforming state-of-the-art model-free and model-based offline RL algorithms across several simulated robotics problems.
</details>
<details>
<summary>摘要</summary>
偏见的动态计划（DP）技术，如Q学习，在sequential decision-making问题中表现出非常重要。然而，在函数 aproximation 的情况下，这些算法并不一定会 converges，经常因为函数类型中缺少 Bellman 完善性，这是DP基于方法的成功所必需的条件。在这篇论文中，我们表明了基于返回conditioned supervised learning（RCSL）的偏见学习技术可以绕过DP中的 Bellman 完善性挑战，并在更松松的假设下 converges。我们证明了在使用两层多层感知机（MLP）作为函数估计器时，要使得 Bellman 完善性满足，状态空间大小 linear 增长是必需的，而Constant 宽度是 enough for RCSL。这些发现可以解释 RCSL 方法在实际中的超越DP方法的较好性能。此外，为了学习从不优化数据集中，我们提出了一个简单的框架called MBRCSL，允许 RCSL 方法通过动态计划来缝合分割的轨迹。MBRCSL 利用学习的动力模型和前向抽象来实现轨迹缝合，而不需要 Bellman 完善性，这使得 RCSL 方法可以避免所有的动态计划算法中的 Bellman 完善性问题。我们提出了理论分析和实验评估，在多个模拟的机器人问题上超越了当前的模型自由和模型基于的离线RL算法。
</details></li>
</ul>
<hr>
<h2 id="ROME-Evaluating-Pre-trained-Vision-Language-Models-on-Reasoning-beyond-Visual-Common-Sense"><a href="#ROME-Evaluating-Pre-trained-Vision-Language-Models-on-Reasoning-beyond-Visual-Common-Sense" class="headerlink" title="ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense"></a>ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19301">http://arxiv.org/abs/2310.19301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/k-square-00/rome">https://github.com/k-square-00/rome</a></li>
<li>paper_authors: Kankan Zhou, Eason Lai, Wei Bin Au Yeong, Kyriakos Mouratidis, Jing Jiang</li>
<li>for: 评估当前最先进的预训练视觉语言模型是否具备正确解释非常规内容的能力。</li>
<li>methods: 使用新的探索数据集ROME（理解非常规知识）来评估当前最先进的预训练视觉语言模型是否能够正确解释不符常规知识的图像。</li>
<li>results: 实验结果表明，大多数当前最先进的预训练视觉语言模型仍然具备不够的解释非常规内容的能力。<details>
<summary>Abstract</summary>
Humans possess a strong capability for reasoning beyond common sense. For example, given an unconventional image of a goldfish laying on the table next to an empty fishbowl, a human would effortlessly determine that the fish is not inside the fishbowl. The case, however, may be different for a vision-language model, whose reasoning could gravitate towards the common scenario that the fish is inside the bowl, despite the visual input. In this paper, we introduce a novel probing dataset named ROME (reasoning beyond commonsense knowledge) to evaluate whether the state-of-the-art pre-trained vision-language models have the reasoning capability to correctly interpret counter-intuitive content. ROME contains images that defy commonsense knowledge with regards to color, shape, material, size and positional relation. Experiments on the state-of-the-art pre-trained vision-language models reveal that most of these models are still largely incapable of interpreting counter-intuitive scenarios. We hope that ROME will spur further investigations on reasoning beyond commonsense knowledge in vision-language research.
</details>
<details>
<summary>摘要</summary>
人类具有强大的理解超出常识能力。例如，给出一张不同寻常的图片，如一只鱼在桌子上 alongside an empty fishbowl，人类会很容易理解鱼不在鱼缸里。然而，这可能不同的情况下，一个视力语言模型的理解可能会受到常识的影响，即鱼在鱼缸里。在这篇论文中，我们提出了一个新的探索数据集名为ROME（理解超出常识知识），以评估当前最先进的预训练视力语言模型是否具有正确地解释不同寻常情况的能力。ROME数据集包含图片，这些图片与常识知识有冲突，包括颜色、形状、材质、大小和位置关系。实验表明，大多数当前最先进的预训练视力语言模型仍然无法正确地解释不同寻常情况。我们希望ROME会激发更多的研究在视力语言领域的理解超出常识知识。
</details></li>
</ul>
<hr>
<h2 id="ROAM-memory-efficient-large-DNN-training-via-optimized-operator-ordering-and-memory-layout"><a href="#ROAM-memory-efficient-large-DNN-training-via-optimized-operator-ordering-and-memory-layout" class="headerlink" title="ROAM: memory-efficient large DNN training via optimized operator ordering and memory layout"></a>ROAM: memory-efficient large DNN training via optimized operator ordering and memory layout</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19295">http://arxiv.org/abs/2310.19295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huiyao Shu, Ang Wang, Ziji Shi, Hanyu Zhao, Yong Li, Lu Lu</li>
<li>for: 这篇研究旨在提高深度学习模型训练的内存使用效率，并且减少高级技术所带来的开销。</li>
<li>methods: 该研究提出了一种名为ROAM的策略，它在计算图层次上运行，以 derivation 最佳的内存减少执行计划，包括优化的算子执行顺序和维度缓存布局。</li>
<li>results: 实验显示，ROAM可以减少内存使用量35.7%、13.3%和27.2%，并且提供了非常快的53.7倍速化。此外，ROAM在大型GPT2-XL模型上进行了扩展性评估，并得到了有力的验证。<details>
<summary>Abstract</summary>
As deep learning models continue to increase in size, the memory requirements for training have surged. While high-level techniques like offloading, recomputation, and compression can alleviate memory pressure, they also introduce overheads. However, a memory-efficient execution plan that includes a reasonable operator execution order and tensor memory layout can significantly increase the models' memory efficiency and reduce overheads from high-level techniques. In this paper, we propose ROAM which operates on computation graph level to derive memory-efficient execution plan with optimized operator order and tensor memory layout for models. We first propose sophisticated theories that carefully consider model structure and training memory load to support optimization for large complex graphs that have not been well supported in the past. An efficient tree-based algorithm is further proposed to search task divisions automatically, along with delivering high performance and effectiveness to solve the problem. Experiments show that ROAM achieves a substantial memory reduction of 35.7%, 13.3%, and 27.2% compared to Pytorch and two state-of-the-art methods and offers a remarkable 53.7x speedup. The evaluation conducted on the expansive GPT2-XL further validates ROAM's scalability.
</details>
<details>
<summary>摘要</summary>
深度学习模型的大小继续增长，训练过程中的内存需求也在不断增加。高级技术如卸载、重计算和压缩可以减轻内存压力，但它们也会带来过程成本。然而，一个高效的内存执行计划，包括合理的操作符执行顺序和张量内存布局，可以大幅提高模型的内存利用率，并降低高级技术的过程成本。在这篇论文中，我们提出了ROAM，它在计算图 уров层上运行，以 derivation 高效的内存执行计划，包括优化的操作符执行顺序和张量内存布局，以适应大型复杂的图structured模型。我们首先提出了一些复杂的理论，考虑到模型结构和训练内存负担，以支持大型复杂图structured模型的优化。此外，我们还提出了一种高效的树状算法，自动搜索任务分解，并实现高性能和效果。实验表明，ROAM可以减少内存使用量35.7%、13.3%和27.2%，相比PYTORCH和两种状态对方法，并提供了Remarkable 53.7倍的速度提升。进一步的评估表明，ROAM在大型GPT2-XL模型上也具有良好的扩展性。
</details></li>
</ul>
<hr>
<h2 id="The-Memory-Perturbation-Equation-Understanding-Model’s-Sensitivity-to-Data"><a href="#The-Memory-Perturbation-Equation-Understanding-Model’s-Sensitivity-to-Data" class="headerlink" title="The Memory Perturbation Equation: Understanding Model’s Sensitivity to Data"></a>The Memory Perturbation Equation: Understanding Model’s Sensitivity to Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19273">http://arxiv.org/abs/2310.19273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Nickl, Lu Xu, Dharmesh Tailor, Thomas Möllenhoff, Mohammad Emtiyaz Khan</li>
<li>for: 本研究旨在提高模型训练数据的敏感性理解，以便更好地解决训练过程中的问题。</li>
<li>methods: 本研究使用抽象方法，根据束义理论来解释模型对训练数据的敏感性。</li>
<li>results: 研究发现，在训练过程中获得的敏感性估计可以准确预测模型在未经见测试数据上的泛化性。<details>
<summary>Abstract</summary>
Understanding model's sensitivity to its training data is crucial but can also be challenging and costly, especially during training. To simplify such issues, we present the Memory-Perturbation Equation (MPE) which relates model's sensitivity to perturbation in its training data. Derived using Bayesian principles, the MPE unifies existing sensitivity measures, generalizes them to a wide-variety of models and algorithms, and unravels useful properties regarding sensitivities. Our empirical results show that sensitivity estimates obtained during training can be used to faithfully predict generalization on unseen test data. The proposed equation is expected to be useful for future research on robust and adaptive learning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>理解模型在训练数据上的敏感性是非常重要，但也可能困难和成本高昂，特别是在训练过程中。为了简化这些问题，我们提出了记忆干扰方程（MPE），它将模型在训练数据上的敏感性相关联到干扰的变化。基于泊利亚理论，MPE总结了许多敏感度指标，泛化到各种模型和算法，并揭示了敏感度的有用性质。我们的实验结果表明，训练期间获得的敏感度估计可以准确预测未经见过的测试数据上的泛化性能。我们预期，提出的方程将对未来关于Robust和Adaptive学习的研究具有帮助。
</details></li>
</ul>
<hr>
<h2 id="NPCL-Neural-Processes-for-Uncertainty-Aware-Continual-Learning"><a href="#NPCL-Neural-Processes-for-Uncertainty-Aware-Continual-Learning" class="headerlink" title="NPCL: Neural Processes for Uncertainty-Aware Continual Learning"></a>NPCL: Neural Processes for Uncertainty-Aware Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19272">http://arxiv.org/abs/2310.19272</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srvcodes/npcl">https://github.com/srvcodes/npcl</a></li>
<li>paper_authors: Saurav Jha, Dong Gong, He Zhao, Lina Yao</li>
<li>for: 本研究旨在提高深度神经网络在流动数据上的高效训练，同时限制新任务干扰已有任务的忘记。</li>
<li>methods: 本研究使用神经过程（NP），一种元学习器，对 CL 任务进行处理。NP 可以将不同任务编码成概率分布中的函数，同时提供可靠的不确定度估计。</li>
<li>results: 我们的实验表明，NPCL 比前一代 CL 方法表现更好。我们还验证了 NPCL 中的不确定度估计能够正确地 indentify 新数据和评估实例级模型信心。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/srvCodes/NPCL%7D">https://github.com/srvCodes/NPCL}</a> 上获取。<details>
<summary>Abstract</summary>
Continual learning (CL) aims to train deep neural networks efficiently on streaming data while limiting the forgetting caused by new tasks. However, learning transferable knowledge with less interference between tasks is difficult, and real-world deployment of CL models is limited by their inability to measure predictive uncertainties. To address these issues, we propose handling CL tasks with neural processes (NPs), a class of meta-learners that encode different tasks into probabilistic distributions over functions all while providing reliable uncertainty estimates. Specifically, we propose an NP-based CL approach (NPCL) with task-specific modules arranged in a hierarchical latent variable model. We tailor regularizers on the learned latent distributions to alleviate forgetting. The uncertainty estimation capabilities of the NPCL can also be used to handle the task head/module inference challenge in CL. Our experiments show that the NPCL outperforms previous CL approaches. We validate the effectiveness of uncertainty estimation in the NPCL for identifying novel data and evaluating instance-level model confidence. Code is available at \url{https://github.com/srvCodes/NPCL}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Revisiting-Evaluation-Metrics-for-Semantic-Segmentation-Optimization-and-Evaluation-of-Fine-grained-Intersection-over-Union"><a href="#Revisiting-Evaluation-Metrics-for-Semantic-Segmentation-Optimization-and-Evaluation-of-Fine-grained-Intersection-over-Union" class="headerlink" title="Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union"></a>Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19252">http://arxiv.org/abs/2310.19252</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zifuwanggg/jdtlosses">https://github.com/zifuwanggg/jdtlosses</a></li>
<li>paper_authors: Zifu Wang, Maxim Berman, Amal Rannen-Triki, Philip H. S. Torr, Devis Tuia, Tinne Tuytelaars, Luc Van Gool, Jiaqian Yu, Matthew B. Blaschko</li>
<li>for: 提高semantic segmentation模型评估的准确性和公正性，尤其是在面临类域不均衡和大小不均衡的情况下。</li>
<li>methods: 提出使用细化的mIoU指标，并与相关的最差情况指标进行对比，以提供更全面的评估方法。</li>
<li>results: 通过对15种现代神经网络模型在12种自然和飞行 segmentation 数据集上进行训练和评估，发现使用细化的mIoU指标可以减少对大物体的偏袋，并提供更多的统计信息和有价值的模型和数据集评估信息。<details>
<summary>Abstract</summary>
Semantic segmentation datasets often exhibit two types of imbalance: \textit{class imbalance}, where some classes appear more frequently than others and \textit{size imbalance}, where some objects occupy more pixels than others. This causes traditional evaluation metrics to be biased towards \textit{majority classes} (e.g. overall pixel-wise accuracy) and \textit{large objects} (e.g. mean pixel-wise accuracy and per-dataset mean intersection over union). To address these shortcomings, we propose the use of fine-grained mIoUs along with corresponding worst-case metrics, thereby offering a more holistic evaluation of segmentation techniques. These fine-grained metrics offer less bias towards large objects, richer statistical information, and valuable insights into model and dataset auditing. Furthermore, we undertake an extensive benchmark study, where we train and evaluate 15 modern neural networks with the proposed metrics on 12 diverse natural and aerial segmentation datasets. Our benchmark study highlights the necessity of not basing evaluations on a single metric and confirms that fine-grained mIoUs reduce the bias towards large objects. Moreover, we identify the crucial role played by architecture designs and loss functions, which lead to best practices in optimizing fine-grained metrics. The code is available at \href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.
</details>
<details>
<summary>摘要</summary>
Semantic segmentation数据集经常表现出两种类型的偏度：类偏度和大小偏度。前者表示一些类别在其他类别之上出现得更多，而后者表示一些对象占据了更多的像素。这会使传统的评估指标受到主要类别（例如总像素准确率）和大对象（例如平均像素准确率和数据集平均交集覆盖率）的偏袋。为了解决这些缺陷，我们提议使用细化的mIoU以及相应的最差情况指标，从而对segmentation技术进行更全面的评估。这些细化指标具有较低的偏袋度，更丰富的统计信息，并且对模型和数据集进行严格的审核。此外，我们进行了广泛的 benchmark 研究，在12种天然和航空Segmentation数据集上训练和评估了15种现代神经网络。我们的 benchmark 研究表明，不要基于单一指标进行评估，而是应该使用细化的mIoU来减少对大对象的偏袋。此外，我们还发现了 Architecture 设计和损失函数的重要作用，这导致了优化细化指标的最佳实践。代码可以在 \href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Pre-trained-Recommender-Systems-A-Causal-Debiasing-Perspective"><a href="#Pre-trained-Recommender-Systems-A-Causal-Debiasing-Perspective" class="headerlink" title="Pre-trained Recommender Systems: A Causal Debiasing Perspective"></a>Pre-trained Recommender Systems: A Causal Debiasing Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19251">http://arxiv.org/abs/2310.19251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/myhakureimu/prerec">https://github.com/myhakureimu/prerec</a></li>
<li>paper_authors: Ziqian Lin, Hao Ding, Nghia Hoang, Branislav Kveton, Anoop Deoras, Hao Wang</li>
<li>for: 本研究旨在 investigate the possibilities and challenges of adapting pre-trained vision&#x2F;language models to the context of recommender systems, which is less investigated from the perspective of pre-trained model.</li>
<li>methods: 我们提出了一种 generic recommender 的方法，通过在不同领域中提取用户项交互数据，捕捉一般交互模式，然后快速适应改进零下或几下学习性能。此外，我们还引入了一种 causal debiasing 视角，通过一种层次权重学习模型 named PreRec，以解决数据中带有不同文化偏见的问题。</li>
<li>results: 我们的实验结果表明，提出的模型在零下或几下学习情况下可以显著提高推荐性能，并在跨市场和跨平台场景中表现出色。<details>
<summary>Abstract</summary>
Recent studies on pre-trained vision/language models have demonstrated the practical benefit of a new, promising solution-building paradigm in AI where models can be pre-trained on broad data describing a generic task space and then adapted successfully to solve a wide range of downstream tasks, even when training data is severely limited (e.g., in zero- or few-shot learning scenarios). Inspired by such progress, we investigate in this paper the possibilities and challenges of adapting such a paradigm to the context of recommender systems, which is less investigated from the perspective of pre-trained model. In particular, we propose to develop a generic recommender that captures universal interaction patterns by training on generic user-item interaction data extracted from different domains, which can then be fast adapted to improve few-shot learning performance in unseen new domains (with limited data).   However, unlike vision/language data which share strong conformity in the semantic space, universal patterns underlying recommendation data collected across different domains (e.g., different countries or different E-commerce platforms) are often occluded by both in-domain and cross-domain biases implicitly imposed by the cultural differences in their user and item bases, as well as their uses of different e-commerce platforms. As shown in our experiments, such heterogeneous biases in the data tend to hinder the effectiveness of the pre-trained model. To address this challenge, we further introduce and formalize a causal debiasing perspective, which is substantiated via a hierarchical Bayesian deep learning model, named PreRec. Our empirical studies on real-world data show that the proposed model could significantly improve the recommendation performance in zero- and few-shot learning settings under both cross-market and cross-platform scenarios.
</details>
<details>
<summary>摘要</summary>
近期研究表明，预训练的视力语言模型可以在人工智能中提供一种新的有前途的解决方案，即在具有广泛数据的基础上预训练模型，然后适应到各种下游任务中，即使训练数据scarce（如零或几个例子学习场景）。受到这些进步的 inspirited，我们在这篇论文中调查了适应这种解决方案到推荐系统的可能性和挑战。我们提议开发一种通用的推荐器，通过在不同领域中提取通用用户ITEM交互数据来捕捉通用交互模式，然后通过快速适应来提高几个例子学习中的表现。然而，与视语言数据不同，推荐数据中的通用模式受到各种文化差异的影响，导致用户和ITEM基础中的偏见和跨领域偏见隐藏了通用交互模式。我们的实验表明，这些多样性偏见在数据中存在，使得预训练模型的效果受到阻碍。为此，我们进一步引入和正式化一种 causal debiasing 的视角，通过一种层次 bayesian deep learning 模型，名为 PreRec，来解决这个问题。我们对实际数据进行了实验，并证明了我们的模型可以在零例子和几个例子学习场景下提高推荐性能。
</details></li>
</ul>
<hr>
<h2 id="IMPRESS-Evaluating-the-Resilience-of-Imperceptible-Perturbations-Against-Unauthorized-Data-Usage-in-Diffusion-Based-Generative-AI"><a href="#IMPRESS-Evaluating-the-Resilience-of-Imperceptible-Perturbations-Against-Unauthorized-Data-Usage-in-Diffusion-Based-Generative-AI" class="headerlink" title="IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI"></a>IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19248">http://arxiv.org/abs/2310.19248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aaaaaasuka/impress">https://github.com/aaaaaasuka/impress</a></li>
<li>paper_authors: Bochuan Cao, Changjiang Li, Ting Wang, Jinyuan Jia, Bo Li, Jinghui Chen</li>
<li>For: 保护原始图像免于未经授权数据使用，如颜料艺术或假信息制作。* Methods: 使用不可见干扰，引导扩散模型生成图像，并评估干扰对图像的影响，以开发新的优化策略来纯化图像。* Results: 提出了一个名为IMPRESS的干扰纯化平台，可以评估当今保护方法的效iveness，并用于未来保护方法的评估。<details>
<summary>Abstract</summary>
Diffusion-based image generation models, such as Stable Diffusion or DALL-E 2, are able to learn from given images and generate high-quality samples following the guidance from prompts. For instance, they can be used to create artistic images that mimic the style of an artist based on his/her original artworks or to maliciously edit the original images for fake content. However, such ability also brings serious ethical issues without proper authorization from the owner of the original images. In response, several attempts have been made to protect the original images from such unauthorized data usage by adding imperceptible perturbations, which are designed to mislead the diffusion model and make it unable to properly generate new samples. In this work, we introduce a perturbation purification platform, named IMPRESS, to evaluate the effectiveness of imperceptible perturbations as a protective measure. IMPRESS is based on the key observation that imperceptible perturbations could lead to a perceptible inconsistency between the original image and the diffusion-reconstructed image, which can be used to devise a new optimization strategy for purifying the image, which may weaken the protection of the original image from unauthorized data usage (e.g., style mimicking, malicious editing). The proposed IMPRESS platform offers a comprehensive evaluation of several contemporary protection methods, and can be used as an evaluation platform for future protection methods.
</details>
<details>
<summary>摘要</summary>
Diffusion-based图像生成模型，如稳定扩散或DALL-E 2，可以从给定的图像中学习并生成高质量的样本，以帮助提供指导的提示。例如，它们可以用于创建艺术性图像，模仿艺术家的原始作品风格，或者用于诡异修改原始图像，生成假内容。然而，这种能力也带来了严重的道德问题，需要对原始图像的所有权Owner进行适当的授权。为此，一些尝试已经被дела，以保护原始图像免受不当数据使用的潜在威胁。在这种情况下，我们提出了一种抗扰干扰平台，名为IMPRESS，用于评估抗扰干扰的有效性。IMPRESS基于关键观察，imperceptible扰动可能会导致原始图像和扩散重建图像之间的感知不一致，从而可以提出一种新的优化策略，用于纯化图像，可能减弱原始图像免受不当数据使用的保护。我们提出的IMPRESS平台可以对当今的保护方法进行全面的评估，并可以用作未来保护方法的评估平台。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-guided-Boundary-Learning-for-Imbalanced-Social-Event-Detection"><a href="#Uncertainty-guided-Boundary-Learning-for-Imbalanced-Social-Event-Detection" class="headerlink" title="Uncertainty-guided Boundary Learning for Imbalanced Social Event Detection"></a>Uncertainty-guided Boundary Learning for Imbalanced Social Event Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19247">http://arxiv.org/abs/2310.19247</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ringbdstack/ucl_sed">https://github.com/ringbdstack/ucl_sed</a></li>
<li>paper_authors: Jiaqian Ren, Hao Peng, Lei Jiang, Zhiwei Liu, Jia Wu, Zhengtao Yu, Philip S. Yu</li>
<li>for: 这个研究旨在提高社交事件探测 task 的模型性能，特别是在面临严重的类别不均衡情况下。</li>
<li>methods: 本研究提出了一个 uncertainty-guided class imbalance learning 框架（UCL$<em>{SED}$）和其变体（UCL-EC$</em>{SED}$），以提高模型对不确定类别的普遍化和类别学习。</li>
<li>results: 实验结果显示，这两个模型在三个严重类别不均衡社交事件数据集（Events2012_100、Events2018_100、CrisisLexT_7）上均能够提高社交事件表现和类别任务的性能，特别是在不确定类别上。<details>
<summary>Abstract</summary>
Real-world social events typically exhibit a severe class-imbalance distribution, which makes the trained detection model encounter a serious generalization challenge. Most studies solve this problem from the frequency perspective and emphasize the representation or classifier learning for tail classes. While in our observation, compared to the rarity of classes, the calibrated uncertainty estimated from well-trained evidential deep learning networks better reflects model performance. To this end, we propose a novel uncertainty-guided class imbalance learning framework - UCL$_{SED}$, and its variant - UCL-EC$_{SED}$, for imbalanced social event detection tasks. We aim to improve the overall model performance by enhancing model generalization to those uncertain classes. Considering performance degradation usually comes from misclassifying samples as their confusing neighboring classes, we focus on boundary learning in latent space and classifier learning with high-quality uncertainty estimation. First, we design a novel uncertainty-guided contrastive learning loss, namely UCL and its variant - UCL-EC, to manipulate distinguishable representation distribution for imbalanced data. During training, they force all classes, especially uncertain ones, to adaptively adjust a clear separable boundary in the feature space. Second, to obtain more robust and accurate class uncertainty, we combine the results of multi-view evidential classifiers via the Dempster-Shafer theory under the supervision of an additional calibration method. We conduct experiments on three severely imbalanced social event datasets including Events2012\_100, Events2018\_100, and CrisisLexT\_7. Our model significantly improves social event representation and classification tasks in almost all classes, especially those uncertain ones.
</details>
<details>
<summary>摘要</summary>
real-world社会活动通常具有严重的类别不均衡分布，这会使得训练的检测模型遇到严重的泛化挑战。大多数研究从频率角度出发，强调表示或分类学习的方法来解决这个问题。而我们所观察到的是，相比罕见的类别，通过尝试性深度学习网络得到的准确性估计更好地反映模型性能。为了解决这个问题，我们提出了一种基于不确定性的类别不均衡学习框架 - UCL$_{SED}$, 以及其变体 - UCL-EC$_{SED}$, 用于社会事件检测任务中的不均衡数据。我们的目标是通过提高模型对不确定类别的泛化来提高总体模型性能。由于性能下降通常来自于误分类为相似类型的样本，我们集中精力于边缘学习在特征空间和分类器学习中的高质量不确定性估计。首先，我们设计了一种基于不确定性的对比学习损失函数 - UCL和UCL-EC，以便在异常分布的数据上适应不同类别的表现。在训练中，它们让所有类别，特别是不确定的类别，在特征空间中适应自适应的清晰分割边缘。其次，为了获得更加稳定和准确的类别不确定性，我们将多视图证明类ifier的结果结合使用，并通过德мп斯特-沙佛理论进行监督和加权。我们在三个严重不均衡的社会事件 dataset上进行了实验，包括 Events2012\_100、Events2018\_100和 CrisisLexT\_7。我们的模型在大多数类别中显著提高了社会事件表示和分类任务的性能，特别是不确定的类别。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Configuration-Machines-FPGA-Implementation"><a href="#Stochastic-Configuration-Machines-FPGA-Implementation" class="headerlink" title="Stochastic Configuration Machines: FPGA Implementation"></a>Stochastic Configuration Machines: FPGA Implementation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19225">http://arxiv.org/abs/2310.19225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/plubplub1/bountyfarm">https://github.com/plubplub1/bountyfarm</a></li>
<li>paper_authors: Matthew J. Felicetti, Dianhui Wang</li>
<li>for: 本研究旨在实现SCM模型在Field Programmable Gate Array（FPGA）上，并将binary-coded输入添加到算法中。</li>
<li>methods: 该研究使用SCM模型，限制随机权重为二进制值，并使用机制模型提高学习性和结果可解性。</li>
<li>results: 对两个benchmark和两个工业数据集进行测试，SCM模型在单层和深度架构下都有良好的表现。<details>
<summary>Abstract</summary>
Neural networks for industrial applications generally have additional constraints such as response speed, memory size and power usage. Randomized learners can address some of these issues. However, hardware solutions can provide better resource reduction whilst maintaining the model's performance. Stochastic configuration networks (SCNs) are a prime choice in industrial applications due to their merits and feasibility for data modelling. Stochastic Configuration Machines (SCMs) extend this to focus on reducing the memory constraints by limiting the randomized weights to a binary value with a scalar for each node and using a mechanism model to improve the learning performance and result interpretability. This paper aims to implement SCM models on a field programmable gate array (FPGA) and introduce binary-coded inputs to the algorithm. Results are reported for two benchmark and two industrial datasets, including SCM with single-layer and deep architectures.
</details>
<details>
<summary>摘要</summary>
neural networks for industrial applications 通常有额外的约束，如响应速度、存储大小和功耗使用。随机学习者可以解决一些这些问题。然而，硬件解决方案可以提供更好的资源减少，同时保持模型的性能。随机配置网络（SCNs）在工业应用中是首选，因为它们在数据模型方面具有优点和可行性。随机配置机器（SCMs）进一步限制了随机权重，将随机权重限定为二进制值，并使用一种机制模型来提高学习性能和结果 интерпреtabILITY。这篇论文的目标是在场程可编程阵列（FPGA）上实现 SCM 模型，并将二进制编码输入到算法中。结果对两个标准准比例数据集和两个工业准比例数据集进行报告，包括 SCM 单层和深度架构。
</details></li>
</ul>
<hr>
<h2 id="EHRTutor-Enhancing-Patient-Understanding-of-Discharge-Instructions"><a href="#EHRTutor-Enhancing-Patient-Understanding-of-Discharge-Instructions" class="headerlink" title="EHRTutor: Enhancing Patient Understanding of Discharge Instructions"></a>EHRTutor: Enhancing Patient Understanding of Discharge Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19212">http://arxiv.org/abs/2310.19212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhang, Zonghai Yao, Huixue Zhou, Feiyun ouyang, Hong Yu</li>
<li>for: 这篇论文的目的是提出一种基于大语言模型的患者教育框架，帮助患者更好地理解他们的诊断和治疗计划。</li>
<li>methods: 这篇论文使用了大语言模型（LLM）来实现对患者的教育，通过对话式问答来检测和评估患者的知识水平。</li>
<li>results: 论文的评估结果表明，使用EHRTutor可以有效地帮助患者更好地理解他们的诊断和治疗计划，并且可以提高患者的参与度和遵从率。<details>
<summary>Abstract</summary>
Large language models have shown success as a tutor in education in various fields. Educating patients about their clinical visits plays a pivotal role in patients' adherence to their treatment plans post-discharge. This paper presents EHRTutor, an innovative multi-component framework leveraging the Large Language Model (LLM) for patient education through conversational question-answering. EHRTutor first formulates questions pertaining to the electronic health record discharge instructions. It then educates the patient through conversation by administering each question as a test. Finally, it generates a summary at the end of the conversation. Evaluation results using LLMs and domain experts have shown a clear preference for EHRTutor over the baseline. Moreover, EHRTutor also offers a framework for generating synthetic patient education dialogues that can be used for future in-house system training.
</details>
<details>
<summary>摘要</summary>
大型语言模型在教育领域中已经显示出优异成绩，尤其是当作教学导师。在医疗领域，教育病人关于他们的诊疗纪录和治疗计划有着重要的作用，可以帮助病人更好地遵循处方。本文介绍了EHRTutor，一个创新的多 ком成分框架，利用大型语言模型（LLM）来对病人进行教育，通过对话式的问答。EHRTutor首先将诊疗纪录中的问题形成，然后通过对话进行教育，最后将结果summarize为一个摘要。评估结果显示，使用LLM和专业人员的评价都倾向于优遇EHRTutor，而且EHRTutor还提供了一个生成Synthetic patient education dialogues的框架，可以用于未来的系统培训。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-generative-artificial-intelligence-to-simulate-student-learning-behavior"><a href="#Leveraging-generative-artificial-intelligence-to-simulate-student-learning-behavior" class="headerlink" title="Leveraging generative artificial intelligence to simulate student learning behavior"></a>Leveraging generative artificial intelligence to simulate student learning behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19206">http://arxiv.org/abs/2310.19206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songlin Xu, Xinyu Zhang</li>
<li>for: 增强学习效果、进步教育研究、制定有效教学方法</li>
<li>methods: 使用大语言模型（LLMs）实现学生学习行为模拟</li>
<li>results: 确认大语言模型可以实现学生学习行为模拟，并且可以捕捉学生学习行为与多种人口特征之间的复杂相关关系，包括学习成绩、课程材料、理解水平和参与度。<details>
<summary>Abstract</summary>
Student simulation presents a transformative approach to enhance learning outcomes, advance educational research, and ultimately shape the future of effective pedagogy. We explore the feasibility of using large language models (LLMs), a remarkable achievement in AI, to simulate student learning behaviors. Unlike conventional machine learning based prediction, we leverage LLMs to instantiate virtual students with specific demographics and uncover intricate correlations among learning experiences, course materials, understanding levels, and engagement. Our objective is not merely to predict learning outcomes but to replicate learning behaviors and patterns of real students. We validate this hypothesis through three experiments. The first experiment, based on a dataset of N = 145, simulates student learning outcomes from demographic data, revealing parallels with actual students concerning various demographic factors. The second experiment (N = 4524) results in increasingly realistic simulated behaviors with more assessment history for virtual students modelling. The third experiment (N = 27), incorporating prior knowledge and course interactions, indicates a strong link between virtual students' learning behaviors and fine-grained mappings from test questions, course materials, engagement and understanding levels. Collectively, these findings deepen our understanding of LLMs and demonstrate its viability for student simulation, empowering more adaptable curricula design to enhance inclusivity and educational effectiveness.
</details>
<details>
<summary>摘要</summary>
学生模拟提供了一种转型的方法来提高学习成果、进步教育研究和shape未来有效教学方法。我们explore LLMS的可行性，用于模拟学生学习行为。与传统的机器学习预测不同，我们利用LLMS实例化虚拟学生，并揭示了学习经验、课程材料、理解水平和参与度之间的细腻相关性。我们的目标不仅预测学习成果，而是复制真实学生的学习行为和模式。我们验证了这一假设通过三个实验。第一个实验，基于N = 145的数据集，模拟了学生学习成果的变化，发现与实际学生的各种民族因素相似。第二个实验（N = 4524），通过增加评估历史，使虚拟学生的行为变得越来越真实。第三个实验（N = 27），结合先前知识和课程互动，显示了虚拟学生学习行为与测验问题、课程材料、参与度和理解水平之间的强相关性。总之，这些发现深入了我们对LLMS的理解，并证明了其可行性，为更适应的课程设计增强包容性和教育效果。
</details></li>
</ul>
<hr>
<h2 id="Can-ChatGPT-advance-software-testing-intelligence-An-experience-report-on-metamorphic-testing"><a href="#Can-ChatGPT-advance-software-testing-intelligence-An-experience-report-on-metamorphic-testing" class="headerlink" title="Can ChatGPT advance software testing intelligence? An experience report on metamorphic testing"></a>Can ChatGPT advance software testing intelligence? An experience report on metamorphic testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19204">http://arxiv.org/abs/2310.19204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang-Hung Luu, Huai Liu, Tsong Yueh Chen</li>
<li>for: 本研究用 ChatGPT 探索它在软件测试领域的潜在潜力，特别是在使用 metamorphic testing（MT）这种先进的软件测试技术时。</li>
<li>methods: 本研究使用 ChatGPT 生成 metamorphic relations（MR）的候选者，这些 MR 是软件系统的基本性质，需要人类智能来确定。这些 MR 候选者然后被专家评估 Correctness。</li>
<li>results: 研究表明，ChatGPT 可以生成新的正确 MR，用于测试多个软件系统。但是，大多数 MR 候选者都是不充分定义或者错误的，尤其是对于没有过 MT 测试的系统。 ChatGPT 可以用来提高软件测试智能，但是人类智能仍然必须参与以确定其正确性。<details>
<summary>Abstract</summary>
While ChatGPT is a well-known artificial intelligence chatbot being used to answer human's questions, one may want to discover its potential in advancing software testing. We examine the capability of ChatGPT in advancing the intelligence of software testing through a case study on metamorphic testing (MT), a state-of-the-art software testing technique. We ask ChatGPT to generate candidates of metamorphic relations (MRs), which are basically necessary properties of the object program and which traditionally require human intelligence to identify. These MR candidates are then evaluated in terms of correctness by domain experts. We show that ChatGPT can be used to generate new correct MRs to test several software systems. Having said that, the majority of MR candidates are either defined vaguely or incorrect, especially for systems that have never been tested with MT. ChatGPT can be used to advance software testing intelligence by proposing MR candidates that can be later adopted for implementing tests; but human intelligence should still inevitably be involved to justify and rectify their correctness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.AI_2023_10_30/" data-id="clogyj8x600do7crabjl4aa7e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.CL_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T11:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/cs.CL_2023_10_30/">cs.CL - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Partial-Tensorized-Transformers-for-Natural-Language-Processing"><a href="#Partial-Tensorized-Transformers-for-Natural-Language-Processing" class="headerlink" title="Partial Tensorized Transformers for Natural Language Processing"></a>Partial Tensorized Transformers for Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20077">http://arxiv.org/abs/2310.20077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhadra Vadlamannati, Ryan Solgi</li>
<li>for: 提高BERT和ViT模型的精度和压缩</li>
<li>methods: 使用tensor-train分解方法进行压缩和部分tensor化</li>
<li>results: 提高模型的精度，无需后处理调整，在tensor decomposition领域取得新的突破<details>
<summary>Abstract</summary>
The transformer architecture has revolutionized Natural Language Processing (NLP) and other machine-learning tasks, due to its unprecedented accuracy. However, their extensive memory and parameter requirements often hinder their practical applications. In this work, we study the effect of tensor-train decomposition to improve the accuracy and compress transformer vision-language neural networks, namely BERT and ViT. We focus both on embedding-layer compression and partial tensorization of neural networks (PTNN) through an algorithmic approach. Our novel PTNN approach significantly improves the accuracy of existing models by up to 5%, all without the need for post-training adjustments, breaking new ground in the field of tensor decomposition.
</details>
<details>
<summary>摘要</summary>
transformer 架构在自然语言处理（NLP）和其他机器学习任务中引起了革命，因为它们的前无古人略度。然而，它们的延展内存和参数需求经常限制它们的实际应用。在这项工作中，我们研究了tensor-train decompositions来提高BERT和ViT模型的准确率和压缩，包括嵌入层压缩和部分tensorization of neural networks（PTNN）。我们的新PTNN方法可以在不需要后处理调整的情况下提高现有模型的准确率，创造了新的tensor decompositions领域。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Evaluation-of-Generative-Models-with-Instruction-Tuning"><a href="#Automatic-Evaluation-of-Generative-Models-with-Instruction-Tuning" class="headerlink" title="Automatic Evaluation of Generative Models with Instruction Tuning"></a>Automatic Evaluation of Generative Models with Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20072">http://arxiv.org/abs/2310.20072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuhaibm/heap">https://github.com/shuhaibm/heap</a></li>
<li>paper_authors: Shuhaib Mehri, Vered Shwartz</li>
<li>for: 本研究旨在自动评估自然语言生成（NLP）中的语言模型。</li>
<li>methods: 研究使用先修标准语言模型进行微调，以模拟人类评估标准。</li>
<li>results: 研究发现，通过对 HEAP 数据集进行 instrucion 微调，可以在多种评估任务和评价标准上达到良好的表现，但有些标准需要更多的微调。同时，将多个任务进行共同训练可以提高表现，这有助于未来具有少量或无人标注数据的任务。<details>
<summary>Abstract</summary>
Automatic evaluation of natural language generation has long been an elusive goal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate human judgements for a particular task and evaluation criterion. Inspired by the generalization ability of instruction-tuned models, we propose a learned metric based on instruction tuning. To test our approach, we collected HEAP, a dataset of human judgements across various NLG tasks and evaluation criteria. Our findings demonstrate that instruction tuning language models on HEAP yields good performance on many evaluation tasks, though some criteria are less trivial to learn than others. Further, jointly training on multiple tasks can yield additional performance improvements, which can be beneficial for future tasks with little to no human annotated data.
</details>
<details>
<summary>摘要</summary>
自然语言生成自动评估已经是NLP领域的一个长期目标。一种新的方法是根据人类评估标准来练习已经预训练的语言模型，以便模拟人类的评估标准。我们受到了通用化指令模型的总体能力的启发，并提出了一种基于 instrucion 调整的学习度量。为测试我们的方法，我们收集了HEAP数据集，这是多种NLG任务和评价标准的人类评估。我们的发现表明，通过HEAP上的 instrucion 调整语言模型可以在许多评价任务上显示出良好的性能，然而一些评价标准可能更难学习。此外，同时训练多个任务可以带来额外的性能提升，这可以对未来具有少量或无人标注数据的任务具有帮助。
</details></li>
</ul>
<hr>
<h2 id="Which-Examples-to-Annotate-for-In-Context-Learning-Towards-Effective-and-Efficient-Selection"><a href="#Which-Examples-to-Annotate-for-In-Context-Learning-Towards-Effective-and-Efficient-Selection" class="headerlink" title="Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection"></a>Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20046">http://arxiv.org/abs/2310.20046</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amazon-science/adaptive-in-context-learning">https://github.com/amazon-science/adaptive-in-context-learning</a></li>
<li>paper_authors: Costas Mavromatis, Balasubramaniam Srinivasan, Zhengyuan Shen, Jiani Zhang, Huzefa Rangwala, Christos Faloutsos, George Karypis</li>
<li>for: 这篇论文是关于语言模型（LLM）在新任务上适应的研究。</li>
<li>methods: 这篇论文使用了活动学习方法，通过在语言模型中提供少量的注释来使其适应新任务。</li>
<li>results: 这篇论文的实验结果表明，使用模型不确定性 sampling 和 semantic diversity-based sampling 可以提高语言模型的性能，同时可以减少注释的数量。这种方法比现有的标准方法提高了7.7%的性能，并且可以在3x fewer ICL examples 下达到相同的性能。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) can adapt to new tasks via in-context learning (ICL). ICL is efficient as it does not require any parameter updates to the trained LLM, but only few annotated examples as input for the LLM. In this work, we investigate an active learning approach for ICL, where there is a limited budget for annotating examples. We propose a model-adaptive optimization-free algorithm, termed AdaICL, which identifies examples that the model is uncertain about, and performs semantic diversity-based example selection. Diversity-based sampling improves overall effectiveness, while uncertainty sampling improves budget efficiency and helps the LLM learn new information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage problem, that dynamically adapts based on the model's feedback and can be approximately solved via greedy algorithms. Extensive experiments on nine datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient than performing annotations uniformly at random, while it outperforms SOTA with 2x fewer ICL examples.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型（LLM）可以通过上下文学习（ICL）适应新任务。 ICL 高效，因为它不需要任何参数更新已经训练的 LLM，只需要一些标注的示例作为 LLM 的输入。在这项工作中，我们调查了一种活动学习方法 для ICL，其中有有限的预算用于标注示例。我们提议了一种自适应优化自由算法，称为 AdaICL，它可以在 LLM 中确定不确定的示例，并进行语义多样性基于的示例选择。多样性基本样本选择提高了总效果，而不确定样本选择提高了预算效率，并帮助 LLM 学习新信息。此外，AdaICL 将其抽样策略设置为一个最大覆盖问题，动态适应基于模型反馈，可以使用贪婪算法解决。广泛的实验表明，AdaICL 可以提高精度指标4.4%（相对提高7.7%），与 SOTA 比起来更高效，并且可以在7.7%的预算下达到 SOTA 的性能。
</details></li>
</ul>
<hr>
<h2 id="Early-Detection-of-Depression-and-Eating-Disorders-in-Spanish-UNSL-at-MentalRiskES-2023"><a href="#Early-Detection-of-Depression-and-Eating-Disorders-in-Spanish-UNSL-at-MentalRiskES-2023" class="headerlink" title="Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023"></a>Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20003">http://arxiv.org/abs/2310.20003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Horacio Thompson, Marcelo Errecalde</li>
<li>for: 这个研究旨在早期探测西班牙语言中的心理健康问题。</li>
<li>methods: 这个研究使用了基于Transformer的模型，并且运用了一个决策政策根据早期探测框架定义的参数。</li>
<li>results: 在Task 1和Task 2中，我们的方法获得了第二名的成绩，并且在分类和延迟时间方面获得了良好的成绩，显示了我们的方法在西班牙语言中的应用有效性和一致性。<details>
<summary>Abstract</summary>
MentalRiskES is a novel challenge that proposes to solve problems related to early risk detection for the Spanish language. The objective is to detect, as soon as possible, Telegram users who show signs of mental disorders considering different tasks. Task 1 involved the users' detection of eating disorders, Task 2 focused on depression detection, and Task 3 aimed at detecting an unknown disorder. These tasks were divided into subtasks, each one defining a resolution approach. Our research group participated in subtask A for Tasks 1 and 2: a binary classification problem that evaluated whether the users were positive or negative. To solve these tasks, we proposed models based on Transformers followed by a decision policy according to criteria defined by an early detection framework. One of the models presented an extended vocabulary with important words for each task to be solved. In addition, we applied a decision policy based on the history of predictions that the model performs during user evaluation. For Tasks 1 and 2, we obtained the second-best performance according to rankings based on classification and latency, demonstrating the effectiveness and consistency of our approaches for solving early detection problems in the Spanish language.
</details>
<details>
<summary>摘要</summary>
MENTALRISKES是一个新的挑战，旨在解决西班牙语早期风险检测中的问题。该挑战的目标是，在可能最快速的速度下，检测telegram用户是否显示精神障碍的兆候，通过不同的任务。任务1涉及用户识别饮食障碍,任务2关注压力障碍的检测,任务3旨在检测未知障碍。这些任务被分解为不同的子任务，每个子任务都定义了一种解决方案。我们在子任务A中参与了Tasks 1和2的解决方案：一个binary分类问题，用于评估用户是否为正或负。为解决这些任务，我们提出了基于Transformers的模型，并采用根据早期检测框架定义的决策策略。我们的模型还增加了每个任务的重要词汇表，以便更好地解决这些任务。此外，我们还应用了根据模型在用户评估过程中的历史预测记录进行决策的策略。对于Tasks 1和2，我们在基于分类和延迟时间的排名中获得第二名，这说明了我们的方法在西班牙语早期风险检测中的有效性和一致性。
</details></li>
</ul>
<hr>
<h2 id="Generative-retrieval-augmented-ontologic-graph-and-multi-agent-strategies-for-interpretive-large-language-model-based-materials-design"><a href="#Generative-retrieval-augmented-ontologic-graph-and-multi-agent-strategies-for-interpretive-large-language-model-based-materials-design" class="headerlink" title="Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design"></a>Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19998">http://arxiv.org/abs/2310.19998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus J. Buehler</li>
<li>for: 这篇论文旨在探讨大语言模型（LLMs）在材料分析和设计中的应用，以及它们如何在人工语言、符号、代码和数字数据之间协同工作。</li>
<li>methods: 本论文使用了一个精度调整的模型——MechGPT，基于机械物理领域的训练数据进行调整。然后，通过对模型进行训练和检查，以确保它们在机械领域中具有合理的理解能力。</li>
<li>results: 研究发现，通过使用 Ontological Knowledge Graph 策略，可以让模型更好地理解不同领域之间的关系，并提供可读的图structures，其中包括节点、边和子图等信息。此外，通过非线性抽取和代理模型的应用，可以解决模型在不同领域之间的问题回答和代码生成等问题。<details>
<summary>Abstract</summary>
Transformer neural networks show promising capabilities, in particular for uses in materials analysis, design and manufacturing, including their capacity to work effectively with both human language, symbols, code, and numerical data. Here we explore the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrieving key information about subject areas, developing research hypotheses, discovery of mechanistic relationships across disparate areas of knowledge, and writing and executing simulation codes for active knowledge generation based on physical ground truths. When used as sets of AI agents with specific features, capabilities, and instructions, LLMs can provide powerful problem solution strategies for applications in analysis and design problems. Our experiments focus on using a fine-tuned model, MechGPT, developed based on training data in the mechanics of materials domain. We first affirm how finetuning endows LLMs with reasonable understanding of domain knowledge. However, when queried outside the context of learned matter, LLMs can have difficulty to recall correct information. We show how this can be addressed using retrieval-augmented Ontological Knowledge Graph strategies that discern how the model understands what concepts are important and how they are related. Illustrated for a use case of relating distinct areas of knowledge - here, music and proteins - such strategies can also provide an interpretable graph structure with rich information at the node, edge and subgraph level. We discuss nonlinear sampling strategies and agent-based modeling applied to complex question answering, code generation and execution in the context of automated force field development from actively learned Density Functional Theory (DFT) modeling, and data analysis.
</details>
<details>
<summary>摘要</summary>
启发式神经网络表现了承诺的能力，尤其在材料分析、设计和生产方面，包括它们可以有效地处理人类语言、符号、代码和数字数据等多种数据类型。我们在这里探索使用大语言模型（LLM）作为工程分析材料的工具，包括检索关键信息、开发研究假设、在不同领域知识之间发现机制关系以及基于物理真实情况编写和执行互动知识生成代码。当作为具有特定特征、能力和指令的集合时，LLM可以提供有力的问题解决策略。我们的实验集中使用了微调的模型——MechGPT，基于材料机理领域的训练数据进行微调。我们首先证明了微调endoows LLMs with reasonable understanding of domain knowledge。然而，当被问到外部学习的范围之外时，LLMs可能具有困难回忆正确信息的问题。我们展示了如何通过 ontological Knowledge Graph 策略来解决这个问题，该策略可以掌握模型对概念的理解和如何将其相关联。例如，我们在 music 和蛋白质之间的不同领域知识之间进行了相关的示例，这种策略还可以提供可读性的图像结构，具有节点、边和子图等级别的丰富信息。我们讨论了非线性抽样策略和基于自适应学习的代理人模型，以及在自动化 DFT 模型化、代码生成和执行中的复杂问题回答。
</details></li>
</ul>
<hr>
<h2 id="Strategies-to-Harness-the-Transformers’-Potential-UNSL-at-eRisk-2023"><a href="#Strategies-to-Harness-the-Transformers’-Potential-UNSL-at-eRisk-2023" class="headerlink" title="Strategies to Harness the Transformers’ Potential: UNSL at eRisk 2023"></a>Strategies to Harness the Transformers’ Potential: UNSL at eRisk 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19970">http://arxiv.org/abs/2310.19970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Horacio Thompson, Leticia Cagnina, Marcelo Errecalde</li>
<li>for: 本研究探讨了在互联网上发现不同风险的解决方案，包括寻找抑郁症的 симптом、早期检测Pathological gambling 风险以及评估吃吃症的严重程度。</li>
<li>methods: 本研究提出了基于Transformers的多种方法，包括基于contextualized embedding vectors的相似性方法、基于提示的现有机器学习技术，以及三个精心调整的模型。</li>
<li>results: 本研究在第一个任务中取得了良好的表现，包括基于决策 metric、排名 metric 和运行时间 metric。<details>
<summary>Abstract</summary>
The CLEF eRisk Laboratory explores solutions to different tasks related to risk detection on the Internet. In the 2023 edition, Task 1 consisted of searching for symptoms of depression, the objective of which was to extract user writings according to their relevance to the BDI Questionnaire symptoms. Task 2 was related to the problem of early detection of pathological gambling risks, where the participants had to detect users at risk as quickly as possible. Finally, Task 3 consisted of estimating the severity levels of signs of eating disorders. Our research group participated in the first two tasks, proposing solutions based on Transformers. For Task 1, we applied different approaches that can be interesting in information retrieval tasks. Two proposals were based on the similarity of contextualized embedding vectors, and the other one was based on prompting, an attractive current technique of machine learning. For Task 2, we proposed three fine-tuned models followed by decision policy according to criteria defined by an early detection framework. One model presented extended vocabulary with important words to the addressed domain. In the last task, we obtained good performances considering the decision-based metrics, ranking-based metrics, and runtime. In this work, we explore different ways to deploy the predictive potential of Transformers in eRisk tasks.
</details>
<details>
<summary>摘要</summary>
CLEF eRisk实验室探索互联网上不同任务的风险检测解决方案。2023年版本中，任务1是搜寻受到抑郁症状的用户文章，目标是根据BDI问卷症状EXTRACT用户文章的相关性。任务2是早期检测Pathological gambling风险，参与者需要尽快检测用户是否存在风险。最后一任务是估计吃苹果症状的严重程度。我们的研究组参加了前两个任务，提出了基于Transformers的解决方案。对于任务1，我们应用了不同的方法，包括contextualized embedding vector的相似性和Prompting技术。对于任务2，我们提出了三个精度调整后的模型，并按照早期检测框架中定义的标准来做决策。其中一个模型增加了重要领域中的词汇表。在最后一任务中，我们获得了良好的性能，包括决策基 metrics、排名基 metrics和运行时间。在这项工作中，我们探索了不同的方式将Transformers在eRisk任务中的预测潜力部署出来。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Depth-and-Width-on-Transformer-Language-Model-Generalization"><a href="#The-Impact-of-Depth-and-Width-on-Transformer-Language-Model-Generalization" class="headerlink" title="The Impact of Depth and Width on Transformer Language Model Generalization"></a>The Impact of Depth and Width on Transformer Language Model Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19956">http://arxiv.org/abs/2310.19956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jackson Petty, Sjoerd van Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette, Tal Linzen</li>
<li>for: 该研究旨在探讨语言模型（LM）在处理新句子时如何实现 Compositional Generalization，即将熟悉的元素组合在新的方式下。</li>
<li>methods: 研究使用 transformer 模型，并测试假设，即深度可以促进 Compositional Generalization。为了避免深度和大小之间的干扰，研究者构建了三种模型，其中每种模型有相同的总参数数量（41M、134M 和 374M）。</li>
<li>results: 研究发现，在 fine-tuning 后，深度较大的模型在 OUT-OF-DISTRIBUTION 上的泛化性能比较好，但随着添加更多层数，模型的性能提升速度逐渐减少。此外，在每个家族中，深度较大的模型在语言模型性能方面表现更好，但返回也逐渐减少。最后，研究发现，深度的作用于 Compositional Generalization 不能完全归结于语言模型性能或在 Distribution 上的数据表现。<details>
<summary>Abstract</summary>
To process novel sentences, language models (LMs) must generalize compositionally -- combine familiar elements in new ways. What aspects of a model's structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by recent theoretical and empirical work, that transformers generalize more compositionally when they are deeper (have more layers). Because simply adding layers increases the total number of parameters, confounding depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize better out-of-distribution than shallower models do, but the relative benefit of additional layers diminishes rapidly; (2) within each family, deeper models show better language modeling performance, but returns are similarly diminishing; (3) the benefits of depth for compositional generalization cannot be attributed solely to better performance on language modeling or on in-distribution data.
</details>
<details>
<summary>摘要</summary>
为处理新句子，语言模型（LM）必须通过compositional generalization来推断——将熟悉的元素组合在新的方式下。哪些方面的模型结构会促进compositional generalization？我们通过强调transformer，我们测试假设，基于最近的理论和实验研究，transformer在深度更大时会更好地推断compositional。因为单纯地添加层会增加总的参数数量，从而与深度混淆，我们构建了三类模型，其中每个模型都拥有相同的参数数量（41M、134M和374M参数）。我们在所有模型上进行预训练，然后在测试compositional generalization的任务上进行精化。我们发现以下三点：（1）在训练后，深度更大的模型在非标准数据上的推断性能更好，但附加层的效果很快消失；（2）在每个家族中，深度更大的模型在语言模型性能上表现更好，但返回的提升都很快消失；（3）深度的作用于compositional generalization不可能完全归因于语言模型性能或标准数据上的表现。
</details></li>
</ul>
<hr>
<h2 id="Split-NER-Named-Entity-Recognition-via-Two-Question-Answering-based-Classifications"><a href="#Split-NER-Named-Entity-Recognition-via-Two-Question-Answering-based-Classifications" class="headerlink" title="Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications"></a>Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19942">http://arxiv.org/abs/2310.19942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jatin Arora, Youngja Park</li>
<li>For: 本研究强调NER问题，将其拆分为两个逻辑子任务： span detection 和 span classification。* Methods: 我们将这两个子任务转化为问答问题，并生成两个简单的模型，可以独立优化每个子任务。* Results: 我们在四个跨Domestic数据集上进行实验，发现这种两步方法非但有效，还能够减少训练时间。我们的系统 SplitNER 在 Ontotes5.0、WNUT17 和一个 cybersecurity 数据集上表现优于基eline，并在 BioNLP13CG 上具有相当的表现。<details>
<summary>Abstract</summary>
In this work, we address the NER problem by splitting it into two logical sub-tasks: (1) Span Detection which simply extracts entity mention spans irrespective of entity type; (2) Span Classification which classifies the spans into their entity types. Further, we formulate both sub-tasks as question-answering (QA) problems and produce two leaner models which can be optimized separately for each sub-task. Experiments with four cross-domain datasets demonstrate that this two-step approach is both effective and time efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17 and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all cases, it achieves a significant reduction in training time compared to its QA baseline counterpart. The effectiveness of our system stems from fine-tuning the BERT model twice, separately for span detection and classification. The source code can be found at https://github.com/c3sr/split-ner.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们解决NER问题的方法是将其分解为两个逻辑子任务：（1）Span检测，它简单地提取实体提及 span，不论实体类型；（2）Span分类，它将 span 分类为其实体类型。我们将两个子任务都转换为问答（QA）问题，并生成两个简单的模型，可以独立地优化每个子任务。我们在四个跨领域数据集上进行了实验，并证明这种两步方法是有效和时间高效的。我们的系统 SplitNER 在 Ontonotes5.0、WNUT17 和一个cybersecurity数据集上超过基eline，并在 BioNLP13CG 上达到了类似的性能。在所有情况下，它实现了对基eline的显著减少的训练时间。我们的系统的效果来自于对BERT模型进行了两次精度调整，分别为 span 检测和分类。源代码可以在https://github.com/c3sr/split-ner中找到。
</details></li>
</ul>
<hr>
<h2 id="The-Eval4NLP-2023-Shared-Task-on-Prompting-Large-Language-Models-as-Explainable-Metrics"><a href="#The-Eval4NLP-2023-Shared-Task-on-Prompting-Large-Language-Models-as-Explainable-Metrics" class="headerlink" title="The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics"></a>The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19792">http://arxiv.org/abs/2310.19792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao, Rotem Dror, Steffen Eger</li>
<li>for: 这个论文的目的是探索提示和评分提取在自然语言处理中的应用，具体是在机器翻译和概要写作评估中。</li>
<li>methods: 这篇论文使用了一种新的竞赛设定，选择允许的大语言模型（LLMs）并禁止微调，以强调提示的效果。参与者们采用了不同的方法，包括提示选择和权重调整等。</li>
<li>results:  despite the task’s restrictions, the best-performing systems achieve results on par with or even surpassing recent reference-free metrics developed using larger models, including GEMBA and Comet-Kiwi-XXL。 In addition, a small-scale human evaluation of the plausibility of explanations given by the LLMs was also performed as a separate track.<details>
<summary>Abstract</summary>
With an increasing number of parameters and pre-training data, generative large language models (LLMs) have shown remarkable capabilities to solve tasks with minimal or no task-related examples. Notably, LLMs have been successfully employed as evaluation metrics in text generation tasks. Within this context, we introduce the Eval4NLP 2023 shared task that asks participants to explore prompting and score extraction for machine translation (MT) and summarization evaluation. Specifically, we propose a novel competition setting in which we select a list of allowed LLMs and disallow fine-tuning to ensure a focus on prompting. We present an overview of participants' approaches and evaluate them on a new reference-free test set spanning three language pairs for MT and a summarization dataset. Notably, despite the task's restrictions, the best-performing systems achieve results on par with or even surpassing recent reference-free metrics developed using larger models, including GEMBA and Comet-Kiwi-XXL. Finally, as a separate track, we perform a small-scale human evaluation of the plausibility of explanations given by the LLMs.
</details>
<details>
<summary>摘要</summary>
随着参数和预训练数据的增加，生成大型自然语言模型（LLM）在解决无或少任务相关示例的情况下表现出色。特别是，LLM在文本生成任务中作为评价指标得到了广泛应用。在这个 контексте，我们介绍了2023年的Eval4NLP共同任务，询问参与者探索提示和分析抽取在机器翻译（MT）和摘要评价中的应用。我们提出了一种新的竞赛设定，在allowed LLM列表中禁止细化，以确保关注提示。我们 предостави了参与者的方法概述和一个新的无参考测试集，覆盖三种语言对的机器翻译和摘要数据集。尤其是，尽管任务受限，最佳系统的表现与或超过了使用更大模型开发的最近无参考度量metric，包括GEMB和Comet-Kiwi-XXL。最后，作为一个小规模的人工评估Track，我们对LLM给出的解释的可能性进行了小规模的人工评估。
</details></li>
</ul>
<hr>
<h2 id="What’s-“up”-with-vision-language-models-Investigating-their-struggle-with-spatial-reasoning"><a href="#What’s-“up”-with-vision-language-models-Investigating-their-struggle-with-spatial-reasoning" class="headerlink" title="What’s “up” with vision-language models? Investigating their struggle with spatial reasoning"></a>What’s “up” with vision-language models? Investigating their struggle with spatial reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19785">http://arxiv.org/abs/2310.19785</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amitakamath/whatsup_vlms">https://github.com/amitakamath/whatsup_vlms</a></li>
<li>paper_authors: Amita Kamath, Jack Hessel, Kai-Wei Chang</li>
<li>for: 本研究旨在评估当代视力语言模型（VL）是否可靠地 distinguishing “right” 和 “left”。</li>
<li>methods: 研究人员创建了三个新的测试集来量化模型对这些基本的空间关系的理解。这些测试集更加精准地测试模型的空间理解能力，比如果存在的 VQAv2 测试集。</li>
<li>results: 研究人员发现，18 种 VL 模型中的所有模型表现不佳，比如 BLIP 在 VQAv2 上进行 fine-tuning 的模型，只能达到 56% 的准确率，而人类准确率为 99%。研究人员还发现，popular 的视力语言预训 Corpora 如 LAION-2B 中含有少量可靠的数据来学习空间关系，而基本的模型设计改进如 up-weighting 预hoffmann 实例或 fine-tuning 在我们的 Corpora 中并不能解决这些测试集的挑战。<details>
<summary>Abstract</summary>
Recent vision-language (VL) models are powerful, but can they reliably distinguish "right" from "left"? We curate three new corpora to quantify model comprehension of such basic spatial relations. These tests isolate spatial reasoning more precisely than existing datasets like VQAv2, e.g., our What'sUp benchmark contains sets of photographs varying only the spatial relations of objects, keeping their identity fixed (see Figure 1: models must comprehend not only the usual case of a dog under a table, but also, the same dog on top of the same table). We evaluate 18 VL models, finding that all perform poorly, e.g., BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56% accuracy on our benchmarks vs. humans at 99%. We conclude by studying causes of this surprising behavior, finding: 1) that popular vision-language pretraining corpora like LAION-2B contain little reliable data for learning spatial relationships; and 2) that basic modeling interventions like up-weighting preposition-containing instances or fine-tuning on our corpora are not sufficient to address the challenges our benchmarks pose. We are hopeful that these corpora will facilitate further research, and we release our data and code at https://github.com/amitakamath/whatsup_vlms.
</details>
<details>
<summary>摘要</summary>
现代视力语言（VL）模型强大，但它们能够准确地 distinguishes "right" 和 "left" 吗？我们创建了三个新的 corpora 来量化模型对这些基本的空间关系的理解。这些测试更精准地测试模型的空间理解能力，比如现有的 VQAv2 dataset 更加精准 (参见 Figure 1：模型不仅需要理解一只狗在桌子下，还需要理解同一只狗在同一张桌子上)。我们评估了 18 个 VL 模型，发现所有都表现不佳，例如 BLIP 在 VQAv2 上精通的 fine-tuning 只能达到 56% 的准确率，而人类则达到 99%。我们 conclude 了这些 surprising 的行为的原因，发现：1）流行的视力语言预训练 corpora like LAION-2B 中含有少量可靠的数据用于学习空间关系; 2）基本的模型 interven 如 up-weighting 包含 preposition 的实例或 fine-tuning 在我们的 corpora 中并不足以解决我们的 benchmark 所提出的挑战。我们希望这些 corpora 能够促进进一步的研究，我们将数据和代码发布在 GitHub 上，请参考 <https://github.com/amitakamath/whatsup_vlms>。
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Thought-Embeddings-for-Stance-Detection-on-Social-Media"><a href="#Chain-of-Thought-Embeddings-for-Stance-Detection-on-Social-Media" class="headerlink" title="Chain-of-Thought Embeddings for Stance Detection on Social Media"></a>Chain-of-Thought Embeddings for Stance Detection on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19750">http://arxiv.org/abs/2310.19750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Gatto, Omar Sharif, Sarah Masud Preum</li>
<li>for: 本研究旨在提高基于大语言模型（LLM）的社交媒体立场检测性能，因为在线对话中的新词汇和口语语言经常含有深层次的立场标签。</li>
<li>methods: 本研究使用链条思维（COT）提示来提高立场检测性能，但COT提示仍然困难于捕捉深层次的立场标签。为解决这个问题，我们提出了COT嵌入，将COT理由 integrates into 传统的RoBERTa基于的立场检测管道。</li>
<li>results: 我们的分析表明，1）文本编码器可以利用COT理由，即使有少量错误或幻觉，而不会扭曲COT输出标签。2）文本编码器可以忽略社交媒体特定的领域特征，导致样本预测受到干扰。我们的模型在多个社交媒体收集的多个立场检测数据集上达到了顶峰性能。<details>
<summary>Abstract</summary>
Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels. Chain-of-Thought (COT) prompting has recently been shown to improve performance on stance detection tasks -- alleviating some of these issues. However, COT prompting still struggles with implicit stance identification. This challenge arises because many samples are initially challenging to comprehend before a model becomes familiar with the slang and evolving knowledge related to different topics, all of which need to be acquired through the training data. In this study, we address this problem by introducing COT Embeddings which improve COT performance on stance detection tasks by embedding COT reasonings and integrating them into a traditional RoBERTa-based stance detection pipeline. Our analysis demonstrates that 1) text encoders can leverage COT reasonings with minor errors or hallucinations that would otherwise distort the COT output label. 2) Text encoders can overlook misleading COT reasoning when a sample's prediction heavily depends on domain-specific patterns. Our model achieves SOTA performance on multiple stance detection datasets collected from social media.
</details>
<details>
<summary>摘要</summary>
<<SYS>>社交媒体上的立场检测对大语言模型（LLM）是一项挑战，因为在线上对话中出现的新的俚语和口语语言经常含有深刻的立场标签。链条思维（COT）提示已经被证明可以改善立场检测任务的表现——减少一些这些问题。然而，COT提示仍然努力于寻找不直观的立场标签。这个问题的原因是许多样本在模型通过训练数据获得了俚语和不同主题的知识后才能够理解，而这些知识需要通过训练数据来获得。在这种情况下，我们解决这个问题 by introducing COT Embeddings，它们可以改进COT的表现在立场检测任务中。我们的分析表明，1）文本编码器可以通过COT的理由来获得有少量错误或幻想的COT输出标签。2）文本编码器可以忽略围绕特定领域模式的误导性COT理由。我们的模型在多个社交媒体上的立场检测数据集上达到了最高的表现。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Evaluation-Exploring-the-Synergy-of-Large-Language-Models-and-Humans-for-Open-ended-Generation-Evaluation"><a href="#Collaborative-Evaluation-Exploring-the-Synergy-of-Large-Language-Models-and-Humans-for-Open-ended-Generation-Evaluation" class="headerlink" title="Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation"></a>Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19740">http://arxiv.org/abs/2310.19740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qtli/coeval">https://github.com/qtli/coeval</a></li>
<li>paper_authors: Qintong Li, Leyang Cui, Lingpeng Kong, Wei Bi</li>
<li>for: 这个论文主要针对开放式自然语言生成任务（NLG）的评估问题，尤其是这类任务需要创ativity和多样化的评估标准。</li>
<li>methods: 这个论文提出了一种名为CoEval的合作评估管道，其中利用大语言模型（LLM）生成初步的想法，然后由人类进行审核和修正。</li>
<li>results: 研究发现，通过利用LLM，CoEval可以有效地评估长文本，提高评估效率和可靠性，但是人类审核仍然 игра着重要的角色，对LLM评估结果进行修正，以确保最终的可靠性。<details>
<summary>Abstract</summary>
Humans are widely involved in the evaluation of open-ended natural language generation tasks (NLG) that demand creativity, as automatic metrics often exhibit weak correlations with human judgments. Large language models (LLMs) recently have emerged as a scalable and cost-effective alternative to human evaluations. However, both humans and LLMs have limitations, i.e., inherent subjectivity and unreliable judgments, particularly for open-ended tasks that require adaptable metrics tailored to diverse task requirements. To explore the synergy between humans and LLM-based evaluators and address the challenges of existing inconsistent evaluation criteria in open-ended NLG tasks, we propose a Collaborative Evaluation pipeline CoEval, involving the design of a checklist of task-specific criteria and the detailed evaluation of texts, in which LLM generates initial ideation, and then humans engage in scrutiny. We conducted a series of experiments to investigate the mutual effects between LLMs and humans in CoEval. Results show that, by utilizing LLMs, CoEval effectively evaluates lengthy texts, saving significant time and reducing human evaluation outliers. Human scrutiny still plays a role, revising around 20% of LLM evaluation scores for ultimate reliability.
</details>
<details>
<summary>摘要</summary>
人类在开放式自然语言生成任务（NLG）的评估中广泛参与，因为自动度量器经常表现出软相关性与人类评价。大型语言模型（LLM）最近出现了可扩展且成本效果的代替方案。然而，人类和LLM都有限制，即内在主观性和不可靠的评价，特别是面临开放任务需要适应性的度量。为了探索人类和LLM-基于评估器的共同作用并解决现有不一致的评价标准在开放NLG任务中，我们提出了一个协作评估管道CoEval，包括设计任务特定的检查列表和详细的文本评估。在CoEval中，LLM生成初步的想法，然后人类进行审核。我们进行了一系列实验，研究了LLM和人类在CoEval中的互动效果。结果表明，通过利用LLM，CoEval可以有效评估长篇文本， saves significant time和 reduces human evaluation outliers。然而，人类审核仍然扮演着重要的修订角色，修改了20%的LLM评估分。
</details></li>
</ul>
<hr>
<h2 id="Combining-Language-Models-For-Specialized-Domains-A-Colorful-Approach"><a href="#Combining-Language-Models-For-Specialized-Domains-A-Colorful-Approach" class="headerlink" title="Combining Language Models For Specialized Domains: A Colorful Approach"></a>Combining Language Models For Specialized Domains: A Colorful Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19708">http://arxiv.org/abs/2310.19708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Eitan, Menachem Pirchi, Neta Glazer, Shai Meital, Gil Ayach, Aviv Shamsian, Aviv Navon, Gil Hetz, Joseph Keshet</li>
<li>for: 本研究旨在应对通用语言模型（LM）在专业领域中遇到专业术语和术语的问题。</li>
<li>methods: 本研究使用了对专业LM的标注（或“颜色”），以便在通用LM中进行自动捷径识别和处理。</li>
<li>results: 本研究的结果显示，这种方法可以将专业术语和通用语言融合在一起，并且可以有效地降低专业领域中的错误率。<details>
<summary>Abstract</summary>
General purpose language models (LMs) encounter difficulties when processing domain-specific jargon and terminology, which are frequently utilized in specialized fields such as medicine or industrial settings. Moreover, they often find it challenging to interpret mixed speech that blends general language with specialized jargon. This poses a challenge for automatic speech recognition systems operating within these specific domains. In this work, we introduce a novel approach that integrates domain-specific or secondary LM into general-purpose LM. This strategy involves labeling, or ``coloring'', each word to indicate its association with either the general or the domain-specific LM. We develop an optimized algorithm that enhances the beam search algorithm to effectively handle inferences involving colored words. Our evaluations indicate that this approach is highly effective in integrating jargon into language tasks. Notably, our method substantially lowers the error rate for domain-specific words without compromising performance in the general domain.
</details>
<details>
<summary>摘要</summary>
通用语言模型（LM）在处理域专词汇和专业术语时遇到困难，这些词汇 frequently 在医学或工业领域中使用。此外，它们经常难以理解混合语言，其中混合了通用语言和域专词汇。这对自动语音识别系统在这些特定领域中操作带来挑战。在这种情况下，我们提出了一种新的方法，即将域专语言模型（LM） integrate 到通用语言模型（LM）中。这种策略通过对每个词语标注或“颜色”来指示它们与通用语言模型或域专语言模型相关。我们开发了一种优化的算法，以便有效地处理含有颜色词语的推理。我们的评估结果表明，这种方法可以很好地将专业词汇 интеGRATE 到语言任务中。尤其是，我们的方法可以大幅降低域专词汇的错误率，而不会 compromise 通用语言模型的性能。
</details></li>
</ul>
<hr>
<h2 id="When-Do-Prompting-and-Prefix-Tuning-Work-A-Theory-of-Capabilities-and-Limitations"><a href="#When-Do-Prompting-and-Prefix-Tuning-Work-A-Theory-of-Capabilities-and-Limitations" class="headerlink" title="When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations"></a>When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19698">http://arxiv.org/abs/2310.19698</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aleksandarpetrov/prefix-tuning-theory">https://github.com/aleksandarpetrov/prefix-tuning-theory</a></li>
<li>paper_authors: Aleksandar Petrov, Philip H. S. Torr, Adel Bibi</li>
<li>for: 本文研究 Context-based fine-tuning 方法，包括提示、在Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在 Context-based fine-tuning 方法，包括提示、在<details>
<summary>Abstract</summary>
Context-based fine-tuning methods, including prompting, in-context learning, soft prompting (also known as prompt tuning), and prefix-tuning, have gained popularity due to their ability to often match the performance of full fine-tuning with a fraction of the parameters. Despite their empirical successes, there is little theoretical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations. We show that despite the continuous embedding space being more expressive than the discrete token space, soft-prompting and prefix-tuning are strictly less expressive than full fine-tuning, even with the same number of learnable parameters. Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction. This suggests that while techniques like prompting, in-context learning, soft prompting, and prefix-tuning can effectively elicit skills present in the pretrained model, they cannot learn novel tasks that require new attention patterns.
</details>
<details>
<summary>摘要</summary>
Context-based 细化方法，包括提示、在 контексте学习、软提示（也称为提示调整）、前缀调整，在 empirical successes 上备受欢迎，但是理论上的理解却很少。我们表明，虽然连续 embedding 空间比 discrete token 空间更加表达力，但是软提示和前缀调整比全 Parameters 的 fine-tuning 更加有限制，即使它们具有相同的 learnable 参数数量。具体来说，context-based 细化无法改变内容和对应关系的相对担注模式，只能偏移 attention 层的输出方向。这表明，虽然提示、在 kontext 学习、软提示和 prefix 调整可以让 pretrained 模型中的技能得到发挥，但它们无法学习新的任务，需要新的担注模式。
</details></li>
</ul>
<hr>
<h2 id="Sentiment-Analysis-in-Digital-Spaces-An-Overview-of-Reviews"><a href="#Sentiment-Analysis-in-Digital-Spaces-An-Overview-of-Reviews" class="headerlink" title="Sentiment Analysis in Digital Spaces: An Overview of Reviews"></a>Sentiment Analysis in Digital Spaces: An Overview of Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19687">http://arxiv.org/abs/2310.19687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura E. M. Ayravainen, Joanne Hinds, Brittany I. Davidson</li>
<li>for: 本研究提供了一份系统性审review的概述，涵盖了2,275个原始研究。</li>
<li>methods: 该研究使用了一套自定义的质量评估框架，以评估系统性审review的方法质量和报告标准。</li>
<li>results: 研究发现了各种应用和方法，报告质量有限，以及时间的挑战。<details>
<summary>Abstract</summary>
Sentiment analysis (SA) is commonly applied to digital textual data, revealing insight into opinions and feelings. Many systematic reviews have summarized existing work, but often overlook discussions of validity and scientific practices. Here, we present an overview of reviews, synthesizing 38 systematic reviews, containing 2,275 primary studies. We devise a bespoke quality assessment framework designed to assess the rigor and quality of systematic review methodologies and reporting standards. Our findings show diverse applications and methods, limited reporting rigor, and challenges over time. We discuss how future research and practitioners can address these issues and highlight their importance across numerous applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MoCa-Measuring-Human-Language-Model-Alignment-on-Causal-and-Moral-Judgment-Tasks"><a href="#MoCa-Measuring-Human-Language-Model-Alignment-on-Causal-and-Moral-Judgment-Tasks" class="headerlink" title="MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks"></a>MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19677">http://arxiv.org/abs/2310.19677</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cicl-stanford/moca">https://github.com/cicl-stanford/moca</a></li>
<li>paper_authors: Allen Nie, Yuhui Zhang, Atharva Amdekar, Chris Piech, Tatsunori Hashimoto, Tobias Gerstenberg</li>
<li>for: 这些研究旨在探讨人类常识的物理和社会世界理解是如何组织的，以及这些理解如何支持 causal 和 moral 判断。</li>
<li>methods: 研究人员收集了 24 篇 cognitive science 论文中的故事集，并开发了一个系统来注释每个故事中调查的因素。然后，他们使用这个数据集测试大型自然语言处理模型（LLMs）是否对文本场景中的 causal 和 moral 判断与人类参与者相似。</li>
<li>results: 研究发现，较新的 LLMs 在总体水平上的听起来比较好，但是使用统计分析发现，LLMs 对不同因素进行了不同的重视，与人类参与者的偏好不符。这些结果表明，通过精心编辑、挑战数据集和认知科学的洞察，我们可以超越仅基于总体指标的比较，探索 LLMs 的隐性偏好，并评估这些偏好与人类常识之间的一致程度。<details>
<summary>Abstract</summary>
Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.
</details>
<details>
<summary>摘要</summary>
人类常识理解physical和social世界是通过直觉理论来组织的。这些理论支持我们作出 causal和道德判断。当omething bad happens，我们就会自然地问：who did what, and why? cognitive science中有一个丰富的文献研究了人们的 causal和道德直觉。这些研究发现了许多影响人们的判断的因素，如违反 norms和是否可避免或不可避免。我们收集了24篇 cognitive science paper的故事集并开发了一系统来标注每个故事中investigated的因素。使用这个数据集，我们测试了 whether large language models (LLMs) 对 text-based scenarios 的judgment是否与人类参与者相符。在聚合水平上，与更新的 LLMs 相比，alignment 有所提高。然而，通过统计分析，我们发现 LLMs 对不同因素进行了不同的重要性评估，与人类参与者不同。这些结果表明可以通过制备、挑战数据集和 cognitive science 的洞察，我们可以超越仅根据聚合指标进行比较：掌握 LLMs 的隐性倾向，并证明这些与人类直觉相符。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-by-Design-Text-Classification-with-Iteratively-Generated-Concept-Bottleneck"><a href="#Interpretable-by-Design-Text-Classification-with-Iteratively-Generated-Concept-Bottleneck" class="headerlink" title="Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck"></a>Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19660">http://arxiv.org/abs/2310.19660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Magnus Ludan, Qing Lyu, Yue Yang, Liam Dugan, Mark Yatskar, Chris Callison-Burch</li>
<li>for: 提高文本分类任务的可解释性，尤其是在高风险领域应用。</li>
<li>methods: 提出文本瓶颈模型（TBMs），一种内在可解释的文本分类框架，可以提供全局和局部解释。TBMs 不直接预测输出标签，而是预测一个稀疏的概念分布，并使用这些概念值进行最终预测。这些概念可以通过大语言模型（LLM）自动发现和评估，无需人工干预。</li>
<li>results: 在 12 种多样化的数据集上，使用 GPT-4 进行概念生成和评估，TBMs 能够与已知黑盒基eline模型相比，如 GPT-4 fewshot 和 DeBERTa 的Finetune 版本，具有类似的性能水平，而与 Finetune 版本 GPT-3.5 相比，TBMs 略显落后。总的来说，我们的发现表明，TBMs 是一种有前途的新框架，可以提高可解释性，而无需性能交换。<details>
<summary>Abstract</summary>
Deep neural networks excel in text classification tasks, yet their application in high-stakes domains is hindered by their lack of interpretability. To address this, we propose Text Bottleneck Models (TBMs), an intrinsically interpretable text classification framework that offers both global and local explanations. Rather than directly predicting the output label, TBMs predict categorical values for a sparse set of salient concepts and use a linear layer over those concept values to produce the final prediction. These concepts can be automatically discovered and measured by a Large Language Model (LLM), without the need for human curation. On 12 diverse datasets, using GPT-4 for both concept generation and measurement, we show that TBMs can rival the performance of established black-box baselines such as GPT-4 fewshot and finetuned DeBERTa, while falling short against finetuned GPT-3.5. Overall, our findings suggest that TBMs are a promising new framework that enhances interpretability, with minimal performance tradeoffs, particularly for general-domain text.
</details>
<details>
<summary>摘要</summary>
深度神经网络在文本分类任务上表现出色，但它们在高度竞争的领域应用受限因其解释性不足。为解决这问题，我们提出了文本瓶颈模型（TBM），一种内在可解释的文本分类框架，它可以提供全局和局部解释。TBM不直接预测输出标签，而是预测一个稀疏的概念集中的 categorical 值，然后使用这些概念值上的直线层来生成最终预测。这些概念可以通过大型自然语言模型（LLM）自动发现和测量，不需要人类审核。在12种多样化的数据集上，使用 GPT-4 进行概念生成和测量，我们发现TBM可以与已知黑盒基线相比，例如 GPT-4 几个步骤和 DeBERTa 的训练版本，在总体性能方面占据中间地位，落后于训练版本 GPT-3.5。总的来说，我们的发现表明TBM 是一种有前途的新框架，它可以增强解释性，并且与性能交易得来。特别是在通用领域的文本分类任务上。
</details></li>
</ul>
<hr>
<h2 id="Dynamics-of-Instruction-Tuning-Each-Ability-of-Large-Language-Models-Has-Its-Own-Growth-Pace"><a href="#Dynamics-of-Instruction-Tuning-Each-Ability-of-Large-Language-Models-Has-Its-Own-Growth-Pace" class="headerlink" title="Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace"></a>Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19651">http://arxiv.org/abs/2310.19651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiyu Song, Zhanchao Zhou, Jianhao Yan, Yuejiao Fei, Zhenzhong Lan, Yue Zhang</li>
<li>for: 本研究的目的是解释数据建构指南，以便更好地理解如何使Large Language Models（LLMs）实现通用智能。</li>
<li>methods: 本研究使用了深度研究每个基本能力的发展，如创意写作、代码生成和逻辑推理，并系统地研究数据量、参数大小和数据建构方法对每个能力的发展的影响。</li>
<li>results: 研究发现：（i）尽管数据量和参数Scale直接影响模型的总性能，但一些能力更sensitive于其增加，可以使用有限数据进行有效训练，而其他些能力却很难被改进。（ii）人工纠正的数据可以在效率和数据量方面高于GPT-4的 sintetic数据，并可以不断提高模型性能，而且可以在out-of-domain数据上取得反应。（iii）制定数据可以带来强大的跨能力普适性，并且可以在out-of-domain数据上取得相同的结果。<details>
<summary>Abstract</summary>
Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). However, the creation of instruction data is still largely heuristic, leading to significant variation in quality and distribution across existing datasets. Experimental conclusions drawn from these datasets are also inconsistent, with some studies emphasizing the importance of scaling instruction numbers, while others argue that a limited number of samples suffice. To better understand data construction guidelines, we deepen our focus from the overall model performance to the growth of each underlying ability, such as creative writing, code generation, and logical reasoning. We systematically investigate the effects of data volume, parameter size, and data construction methods on the development of various abilities, using hundreds of model checkpoints (7b to 33b) fully instruction-tuned on a new collection of over 40k human-curated instruction data. This proposed dataset is stringently quality-controlled and categorized into ten distinct LLM abilities. Our study reveals three primary findings: (i) Despite data volume and parameter scale directly impacting models' overall performance, some abilities are more responsive to their increases and can be effectively trained using limited data, while some are highly resistant to these changes. (ii) Human-curated data strongly outperforms synthetic data from GPT-4 in efficiency and can constantly enhance model performance with volume increases, but is unachievable with synthetic data. (iii) Instruction data brings powerful cross-ability generalization, with evaluation results on out-of-domain data mirroring the first two observations. Furthermore, we demonstrate how these findings can guide more efficient data constructions, leading to practical performance improvements on public benchmarks.
</details>
<details>
<summary>摘要</summary>
大量的 instruction tuning 是一种快速发展的方法来提高大型语言模型（LLM）的通用智能。然而，创建 instruction data 仍然受到较大的变量和不确定性影响，现有数据集的质量和分布仍然存在很大的差异。各个研究对这些数据集的结论也存在很大的不一致，一些研究认为需要扩大 instruction 数量，而其他研究则认为只需要一个有限的样本数量。为了更好地理解数据建构指南，我们将我们的关注深化到每个基础能力的发展，如创作写作、代码生成和逻辑推理。我们系统地 investigate 数据量、参数大小和数据建构方法对每个能力的发展的影响，使用 hundreds of model checkpoints (7b to 33b) 完全 instruction-tuned 在一个新收集的超过 40k 个人精心编辑的 instruction data 上。这个提posed dataset 是 strict quality-controlled 并分为十种不同的 LLM 能力。我们的研究发现了以下三个主要结论：1. 虽然数据量和参数缩放直接影响模型的总性能，但一些能力更敏感于这些变化，可以使用有限的数据进行有效地训练，而其他能力则具有很高的抗变化能力。2. 人类精心编辑的数据可以在效率和数据量方面大大超越 Synthetic data from GPT-4，并且可以随着数据量的增加不断提高模型性能。然而，synthetic data 无法 достичь这一点。3. instruction data 具有强大的跨能力总体化，评估结果表明，模型在不同的领域数据上的表现与前两个结论类似。此外，我们还证明了这些发现可以导向更有效的数据建构，从而实现实际性能的提高在公共benchmark上。
</details></li>
</ul>
<hr>
<h2 id="KeyGen2Vec-Learning-Document-Embedding-via-Multi-label-Keyword-Generation-in-Question-Answering"><a href="#KeyGen2Vec-Learning-Document-Embedding-via-Multi-label-Keyword-Generation-in-Question-Answering" class="headerlink" title="KeyGen2Vec: Learning Document Embedding via Multi-label Keyword Generation in Question-Answering"></a>KeyGen2Vec: Learning Document Embedding via Multi-label Keyword Generation in Question-Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19650">http://arxiv.org/abs/2310.19650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iftitahu Ni’mah, Samaneh Khoshrou, Vlado Menkovski, Mykola Pechenizkiy</li>
<li>for: 本研究旨在减少依赖于标签指导，通过Sequence-to-Sequence（Seq2Seq）文本生成器来学习文档嵌入。</li>
<li>methods: 我们将关键短语生成任务转换为多标签关键词生成在社区基于问答（cQA）中。</li>
<li>results: 我们的实验结果显示，KeyGen2Vec在整体上比多标签关键词分类器更高，具有14.7%的提升 based on Purity、Normalized Mutual Information（NMI）和F1-Score度量。尝试ingly，尽管在评估数据集上，获得标签指导的学习嵌入的绝对优势在Yahoo! cQA中较大，KeyGen2Vec仍然与这些标签指导的分类器竞争。<details>
<summary>Abstract</summary>
Representing documents into high dimensional embedding space while preserving the structural similarity between document sources has been an ultimate goal for many works on text representation learning. Current embedding models, however, mainly rely on the availability of label supervision to increase the expressiveness of the resulting embeddings. In contrast, unsupervised embeddings are cheap, but they often cannot capture implicit structure in target corpus, particularly for samples that come from different distribution with the pretraining source.   Our study aims to loosen up the dependency on label supervision by learning document embeddings via Sequence-to-Sequence (Seq2Seq) text generator. Specifically, we reformulate keyphrase generation task into multi-label keyword generation in community-based Question Answering (cQA). Our empirical results show that KeyGen2Vec in general is superior than multi-label keyword classifier by up to 14.7% based on Purity, Normalized Mutual Information (NMI), and F1-Score metrics. Interestingly, although in general the absolute advantage of learning embeddings through label supervision is highly positive across evaluation datasets, KeyGen2Vec is shown to be competitive with classifier that exploits topic label supervision in Yahoo! cQA with larger number of latent topic labels.
</details>
<details>
<summary>摘要</summary>
文档表示在高维空间内表示学习，保持文档来源之间的结构相似性是许多文本表示学习工作的最终目标。现有的嵌入模型大多数依赖于标签超级视图增加嵌入的表达力。然而，无监督嵌入是便宜的，但它们通常无法捕捉目标句子分布中的隐藏结构，特别是来自不同分布的样本。我们的研究旨在减少依赖于标签超级视图的限制，通过序列到序列（Seq2Seq）文本生成器学习文档嵌入。 Specifically，我们将关键短语生成任务转换为多标签关键词生成在社区基于问答（cQA）中。我们的实验结果表明，KeyGen2Vec在总体来说比多标签关键词分类器高出14.7%的纯度、 норма化共同信息（NMI）和F1-Score指标。有趣的是，虽然在评估数据集上，通常有监督嵌入学习的绝对优势，KeyGen2Vec在Yahoo! cQA中的更多 latent topic label 的情况下与使用主题标签监督的 классифика器竞争。
</details></li>
</ul>
<hr>
<h2 id="DPATD-Dual-Phase-Audio-Transformer-for-Denoising"><a href="#DPATD-Dual-Phase-Audio-Transformer-for-Denoising" class="headerlink" title="DPATD: Dual-Phase Audio Transformer for Denoising"></a>DPATD: Dual-Phase Audio Transformer for Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19588">http://arxiv.org/abs/2310.19588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhui Li, Pu Wang, Jialu Li, Xinzhe Wang, Youshan Zhang</li>
<li>for: 提高 speech 干扰除模型的性能，使其能够更好地处理长时间序列 audio 信号。</li>
<li>methods: 使用 smaller audio chunks 作为输入，并提出了一种新的 transformer 层结构，以便更好地学习干扰除 clean audio sequences。</li>
<li>results: 对比state-of-the-art方法，我们的模型表现更好，并且可以快速 converges。<details>
<summary>Abstract</summary>
Recent high-performance transformer-based speech enhancement models demonstrate that time domain methods could achieve similar performance as time-frequency domain methods. However, time-domain speech enhancement systems typically receive input audio sequences consisting of a large number of time steps, making it challenging to model extremely long sequences and train models to perform adequately. In this paper, we utilize smaller audio chunks as input to achieve efficient utilization of audio information to address the above challenges. We propose a dual-phase audio transformer for denoising (DPATD), a novel model to organize transformer layers in a deep structure to learn clean audio sequences for denoising. DPATD splits the audio input into smaller chunks, where the input length can be proportional to the square root of the original sequence length. Our memory-compressed explainable attention is efficient and converges faster compared to the frequently used self-attention module. Extensive experiments demonstrate that our model outperforms state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:近期高性能的转换器基于Speech Enhancement模型表明，时域方法可以达到相同的性能水平，与时域频谱方法相比。然而，时域语音净化系统通常处理大量的音频数据，这会使模型学习和训练变得困难。在这篇论文中，我们使用更小的音频块作为输入，以便有效地利用音频信息。我们提出了一种双相Audio Transformer for Denoising（DPATD），一种新的模型，用于在深度结构中学习干净的音频序列。DPATD将音频输入拆分成更小的块，块的长度与原始序列长度的平方根成正比。我们的内存压缩可解释注意力更加高效，并且在训练过程中更快 converges。广泛的实验表明，我们的模型超越了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="Improving-Input-label-Mapping-with-Demonstration-Replay-for-In-context-Learning"><a href="#Improving-Input-label-Mapping-with-Demonstration-Replay-for-In-context-Learning" class="headerlink" title="Improving Input-label Mapping with Demonstration Replay for In-context Learning"></a>Improving Input-label Mapping with Demonstration Replay for In-context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19572">http://arxiv.org/abs/2310.19572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuocheng Gong, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan</li>
<li>for: 这篇论文旨在探讨一种新的卷积语言模型（ICL），它可以通过在输入和标签之间附加一些示例来提高模型对下游自然语言处理任务的理解，而无需直接调整模型参数。</li>
<li>methods: 该论文提出了一种新的ICL方法，即重复示例与滚动 causal attention（RdSca）。这种方法通过重复后期示例并将其添加到输入的开头，使模型可以在 causal 限制下„ observation“ later 信息。此外，该方法还引入了定制化 causal attention，以避免信息泄露。</li>
<li>results: 实验结果表明，该方法可以显著改善 ICL 示例中的输入-标签映射。此外，该论文还进行了对定制化 causal attention 的深入分析，这是在前一些研究中未经探讨的领域。<details>
<summary>Abstract</summary>
In-context learning (ICL) is an emerging capability of large autoregressive language models where a few input-label demonstrations are appended to the input to enhance the model's understanding of downstream NLP tasks, without directly adjusting the model parameters. The effectiveness of ICL can be attributed to the strong language modeling capabilities of large language models (LLMs), which enable them to learn the mapping between input and labels based on in-context demonstrations. Despite achieving promising results, the causal nature of language modeling in ICL restricts the attention to be backward only, i.e., a token only attends to its previous tokens, failing to capture the full input-label information and limiting the model's performance. In this paper, we propose a novel ICL method called Repeated Demonstration with Sliding Causal Attention, (RdSca). Specifically, we duplicate later demonstrations and concatenate them to the front, allowing the model to `observe' the later information even under the causal restriction. Besides, we introduce sliding causal attention, which customizes causal attention to avoid information leakage. Experimental results show that our method significantly improves the input-label mapping in ICL demonstrations. We also conduct an in-depth analysis of how to customize the causal attention without training, which has been an unexplored area in previous research.
</details>
<details>
<summary>摘要</summary>
增强语言模型（ICL）是一种新兴能力，使用几个输入标签示例来增强模型对下游自然语言处理任务的理解，而不直接修改模型参数。ICL的效果可以归结于大型语言模型（LLM）的强大语言模型能力，它们可以通过示例学习映射输入和标签之间的关系。although ICL has achieved promising results, the causal nature of language modeling in ICL restricts the attention to be backward only, i.e., a token only attends to its previous tokens, failing to capture the full input-label information and limiting the model's performance.在这篇论文中，我们提出了一种新的ICL方法，即重复示例with Sliding Causal Attention（RdSca）。特别是，我们将后续的示例重复并 concatenate 到前面，使模型可以在 causal 约束下“观察”后续信息。此外，我们引入了滑动 causal attention，以适应 causal 约束，以避免信息泄露。实验结果表明，我们的方法可以显著提高ICL示例中的输入-标签映射。我们还进行了对自然语言处理任务的深入分析，以了解如何自然地调整 causal attention 而无需训练。Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Representation-to-Improve-Team-Problem-Solving-in-Real-Time"><a href="#A-Novel-Representation-to-Improve-Team-Problem-Solving-in-Real-Time" class="headerlink" title="A Novel Representation to Improve Team Problem Solving in Real-Time"></a>A Novel Representation to Improve Team Problem Solving in Real-Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19539">http://arxiv.org/abs/2310.19539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Doboli</li>
<li>for: 这篇论文旨在提供一种新的表示方式，以帮助在实时问题解决中Team的行为理解和改进。</li>
<li>methods: 本论文使用了一种新的表示方式，捕捉了解决过程中Team的不同想像力的发展、增强和利用。</li>
<li>results: 本论文通过一个案例研究表明，该表示方式可以帮助理解和改进Team的问题解决行为。<details>
<summary>Abstract</summary>
This paper proposes a novel representation to support computing metrics that help understanding and improving in real-time a team's behavior during problem solving in real-life. Even though teams are important in modern activities, there is little computing aid to improve their activity. The representation captures the different mental images developed, enhanced, and utilized during solving. A case study illustrates the representation.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的表示方式，用于支持实时计算团队的行为，以便更好地理解和改进团队在实际问题解决过程中的行为。尽管团队在现代活动中占据重要地位，但是计算机支持团队活动的工具却很少。该表示方式捕捉了解决过程中发展、增强和使用的不同心理图像。一个案例研究 illustrate了该表示方式。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="InfoEntropy-Loss-to-Mitigate-Bias-of-Learning-Difficulties-for-Generative-Language-Models"><a href="#InfoEntropy-Loss-to-Mitigate-Bias-of-Learning-Difficulties-for-Generative-Language-Models" class="headerlink" title="InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models"></a>InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19531">http://arxiv.org/abs/2310.19531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenpeng Su, Xing Wu, Xue Bai, Zijia Lin, Hui Chen, Guiguang Ding, Wei Zhou, Songlin Hu</li>
<li>for: The paper aims to address the imbalance between frequent and infrequent tokens in training large generative language models, which can lead to the models neglecting the difficult-to-learn tokens.</li>
<li>methods: The proposed method is an Information Entropy Loss (InfoEntropy Loss) function, which dynamically assesses the learning difficulty of a to-be-learned token based on the information entropy of the predicted probability distribution over the vocabulary. The loss function scales the training loss adaptively to lead the model to focus more on the difficult-to-learn tokens.</li>
<li>results: The authors train generative language models at different scales (436M, 1.1B, and 6.7B parameters) on the Pile dataset and show that incorporating the proposed InfoEntropy Loss can consistently improve the performance of the models on downstream benchmarks.Here’s the Chinese version of the three key information points:</li>
<li>for:  paper 旨在解决大型生成语言模型在训练中频繁和少见token的不均衡问题，以便模型不会忽略困难学习的token。</li>
<li>methods: 提出的方法是一种信息熵损失函数（InfoEntropy Loss），它在预测概率分布中计算token的学习困难程度，然后根据学习困难程度进行自适应调整training损失。</li>
<li>results: 作者在不同规模（436M, 1.1B, 6.7B参数）的模型上在Pile数据集上进行了训练，并证明了 incorporating 提出的 InfoEntropy Loss 可以一直提高模型在下游任务上的表现。<details>
<summary>Abstract</summary>
Generative language models are usually pretrained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose an Information Entropy Loss (InfoEntropy Loss) function. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn tokens. On the Pile dataset, we train generative language models at different scales of 436M, 1.1B, and 6.7B parameters. Experiments reveal that models incorporating the proposed InfoEntropy Loss can gain consistent performance improvement on downstream benchmarks.
</details>
<details>
<summary>摘要</summary>
将给定的文本翻译成简化中文。大多数生成语言模型通常在大量文本资料上进行预训练，通过预测下一个token（即子字/词/短语）来预测下一个token。在最近的研究中，大型生成语言模型在下游任务上表现出了惊人的表现。然而，现有的生成语言模型通常忽略了文本资料中的自然挑战，即频率分布不均。这可能导致语言模型受到常见和易于学习的token的束缚，而忽略不常见和Difficult-to-learn的token。为了解决这个问题，我们提议一种信息熵损失函数（InfoEntropy Loss）。在训练中，它可以动态评估要学习的token的学习难度，根据对词汇表中预测的概率分布的信息熵。然后它可以对训练损失进行自适应缩放，以便让模型更注重困难的token。在Pile数据集上，我们在不同的参数大小436M、1.1B和6.7B中训练生成语言模型。实验表明，包含我们提议的InfoEntropy Loss的模型在下游 bencmarks 上具有透明的性能改进。
</details></li>
</ul>
<hr>
<h2 id="Constituency-Parsing-using-LLMs"><a href="#Constituency-Parsing-using-LLMs" class="headerlink" title="Constituency Parsing using LLMs"></a>Constituency Parsing using LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19462">http://arxiv.org/abs/2310.19462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Bai, Jialong Wu, Yulong Chen, Zhongqing Wang, Yue Zhang</li>
<li>for: 本研究旨在探讨最新的大语言模型（LLMs）是否可以解决基本 yet 未解决的自然语言处理任务——构成分析。</li>
<li>methods: 我们采用三种线性化策略将输出树转换成符号序列，使LLMs可以通过生成线性化树来解决构成分析。</li>
<li>results: 我们在一个多样化的LLMs中进行实验，包括ChatGPT、GPT-4、OPT、LLaMA和Alpaca，并与现有的状态对抗构成分析器进行比较。我们的实验包括零shot、几shot和全程学习的学习设置，并在一个本地测试集和五个外部测试集上评估模型的性能。我们的发现提供了LLMs在构成分析中的表现、通用能力和挑战的深入了解。<details>
<summary>Abstract</summary>
Constituency parsing is a fundamental yet unsolved natural language processing task. In this paper, we explore the potential of recent large language models (LLMs) that have exhibited remarkable performance across various domains and tasks to tackle this task. We employ three linearization strategies to transform output trees into symbol sequences, such that LLMs can solve constituency parsing by generating linearized trees. We conduct experiments using a diverse range of LLMs, including ChatGPT, GPT-4, OPT, LLaMA, and Alpaca, comparing their performance against the state-of-the-art constituency parsers. Our experiments encompass zero-shot, few-shot, and full-training learning settings, and we evaluate the models on one in-domain and five out-of-domain test datasets. Our findings reveal insights into LLMs' performance, generalization abilities, and challenges in constituency parsing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>��ollar constituency parsing 是一个基本 yet 未解决的自然语言处理任务。在这篇论文中，我们探索了最近的大型语言模型（LLMs）在不同领域和任务上表现出色的潜力，以解决这个任务。我们采用三种线性化策略，将输出树转换成符号序列，以便 LLMS 可以通过生成线性化树来解决 constituency parsing。我们在一系列不同的 LLMs 上进行实验，包括 ChatGPT、GPT-4、OPT、LLaMA 和 Alpaca，与现有的状态 искусственный智能 constituency parser 进行比较。我们的实验包括零shot、几 shot 和全training 学习设定，并在一个 domain 内和五个 out-of-domain 测试集上评估模型的性能。我们的发现揭示了 LLMs 的性能、泛化能力和 constituency parsing 中的挑战。
</details></li>
</ul>
<hr>
<h2 id="Mean-BERTs-make-erratic-language-teachers-the-effectiveness-of-latent-bootstrapping-in-low-resource-settings"><a href="#Mean-BERTs-make-erratic-language-teachers-the-effectiveness-of-latent-bootstrapping-in-low-resource-settings" class="headerlink" title="Mean BERTs make erratic language teachers: the effectiveness of latent bootstrapping in low-resource settings"></a>Mean BERTs make erratic language teachers: the effectiveness of latent bootstrapping in low-resource settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19420">http://arxiv.org/abs/2310.19420</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Samuel</li>
<li>for: 这 paper 探讨了使用 latent bootstrapping，一种刚才自我监督技术，用于预训语言模型。</li>
<li>methods: 这 paper 使用 contextualized embeddings 作为更丰富的监督信号，而不是使用典型的自我监督方法，即使 discrete subwords。</li>
<li>results: 我们的实验表明，使用 latent bootstrapping 可以更好地从有限资源中获得语言知识。 Specifically, 我们的实验基于 BabyLM 共享任务，包括预训两个小型 curaated corpus 并在四个语言标准准则上进行评估。<details>
<summary>Abstract</summary>
This paper explores the use of latent bootstrapping, an alternative self-supervision technique, for pretraining language models. Unlike the typical practice of using self-supervision on discrete subwords, latent bootstrapping leverages contextualized embeddings for a richer supervision signal. We conduct experiments to assess how effective this approach is for acquiring linguistic knowledge from limited resources. Specifically, our experiments are based on the BabyLM shared task, which includes pretraining on two small curated corpora and an evaluation on four linguistic benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Lightweight-Method-to-Generate-Unanswerable-Questions-in-English"><a href="#A-Lightweight-Method-to-Generate-Unanswerable-Questions-in-English" class="headerlink" title="A Lightweight Method to Generate Unanswerable Questions in English"></a>A Lightweight Method to Generate Unanswerable Questions in English</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19403">http://arxiv.org/abs/2310.19403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uds-lsv/unanswerable-question-generation">https://github.com/uds-lsv/unanswerable-question-generation</a></li>
<li>paper_authors: Vagrant Gautam, Miaoran Zhang, Dietrich Klakow</li>
<li>for: 这篇论文主要写于如何建立更好的问答系统，具体来说是通过对可answer的问题进行对称和实体交换来提高问答模型的性能。</li>
<li>methods: 这篇论文使用了对称和实体交换来生成无法答题的问题，以提高问答模型的性能。</li>
<li>results: 对比之前的状态艺术，使用这种训练自由和轻量级的方法可以提高问答模型的性能，并且人类评价相关性和可读性也提高了。<details>
<summary>Abstract</summary>
If a question cannot be answered with the available information, robust systems for question answering (QA) should know _not_ to answer. One way to build QA models that do this is with additional training data comprised of unanswerable questions, created either by employing annotators or through automated methods for unanswerable question generation. To show that the model complexity of existing automated approaches is not justified, we examine a simpler data augmentation method for unanswerable question generation in English: performing antonym and entity swaps on answerable questions. Compared to the prior state-of-the-art, data generated with our training-free and lightweight strategy results in better models (+1.6 F1 points on SQuAD 2.0 data with BERT-large), and has higher human-judged relatedness and readability. We quantify the raw benefits of our approach compared to no augmentation across multiple encoder models, using different amounts of generated data, and also on TydiQA-MinSpan data (+9.3 F1 points with BERT-large). Our results establish swaps as a simple but strong baseline for future work.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:如果问题无法被 answered  WITH 可用信息，then robust question answering (QA) 系统应该知道不要回答。一种方法来建立 QA 模型是通过额外的训练数据，包括不可回答的问题，使用注释员或自动生成不可回答问题的方法。为了证明现有的自动化方法的模型复杂性不合理，我们研究了一种更简单的数据扩充方法：在可回答问题中进行反义和实体换换。与先前的状态 искусственный智能相比，我们的训练自由和轻量级方法在 SQuAD 2.0 数据上得到了更好的模型 (+1.6 F1 点 WITH BERT-large)，并且人类判断的相关性和可读性更高。我们通过不同的encoder模型和不同的生成数据量进行评估，并在 TydiQA-MinSpan 数据上 (+9.3 F1 点 WITH BERT-large) 获得了更高的Raw Benefits。我们的结果证明了交换是一种简单 yet strong 的基线 для未来的工作。
</details></li>
</ul>
<hr>
<h2 id="Japanese-SimCSE-Technical-Report"><a href="#Japanese-SimCSE-Technical-Report" class="headerlink" title="Japanese SimCSE Technical Report"></a>Japanese SimCSE Technical Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19349">http://arxiv.org/abs/2310.19349</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hpprc/simple-simcse-ja">https://github.com/hpprc/simple-simcse-ja</a></li>
<li>paper_authors: Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda</li>
<li>for: 本研究是为了开发日本语言实体表示（SimCSE）模型，用于提高日本语言实体表示的研究。</li>
<li>methods: 本研究使用了24种预训练的日本语言或多语言模型，并在5个监督数据集和4个无监督数据集上进行了广泛的实验。</li>
<li>results: 本研究通过详细的训练设置和评估结果，为日本语言实体表示领域提供了一个可靠的基线。<details>
<summary>Abstract</summary>
We report the development of Japanese SimCSE, Japanese sentence embedding models fine-tuned with SimCSE. Since there is a lack of sentence embedding models for Japanese that can be used as a baseline in sentence embedding research, we conducted extensive experiments on Japanese sentence embeddings involving 24 pre-trained Japanese or multilingual language models, five supervised datasets, and four unsupervised datasets. In this report, we provide the detailed training setup for Japanese SimCSE and their evaluation results.
</details>
<details>
<summary>摘要</summary>
我们报道了日本SimCSE的开发，日本句子嵌入模型通过SimCSE进行了精细调教。由于日本的句子嵌入模型缺乏可以作为基准的研究句子嵌入模型，我们在日本句子嵌入方面进行了广泛的实验，使用了24种预训练的日本或多语言模型，5个指定数据集和4个无指定数据集。在这份报告中，我们提供了日本SimCSE的详细训练设置和评估结果。
</details></li>
</ul>
<hr>
<h2 id="Test-Suites-Task-Evaluation-of-Gender-Fairness-in-MT-with-MuST-SHE-and-INES"><a href="#Test-Suites-Task-Evaluation-of-Gender-Fairness-in-MT-with-MuST-SHE-and-INES" class="headerlink" title="Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and INES"></a>Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and INES</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19345">http://arxiv.org/abs/2310.19345</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlt-mt/fbk-fairseq">https://github.com/hlt-mt/fbk-fairseq</a></li>
<li>paper_authors: Beatrice Savoldi, Marco Gaido, Matteo Negri, Luisa Bentivogli</li>
<li>for: 本研究使用两个新创建的测试集（MuST-SHE-WMT23和INES）来评估翻译系统对女性和男性语言形式的翻译能力以及生成包容性的翻译。</li>
<li>methods: 本研究使用en-de和de-en语言对的翻译系统进行评估，并采用新创建的测试集来评估系统对女性和男性语言形式的翻译能力以及生成包容性的翻译。</li>
<li>results: 研究结果显示，翻译系统在正常的自然性 gender 现象中 correctly 翻译女性和男性语言形式的能力相对较高，但生成包容性的翻译仍然是一个挑战，所有评估的MT模型都表现出不够的能力。<details>
<summary>Abstract</summary>
As part of the WMT-2023 "Test suites" shared task, in this paper we summarize the results of two test suites evaluations: MuST-SHE-WMT23 and INES. By focusing on the en-de and de-en language pairs, we rely on these newly created test suites to investigate systems' ability to translate feminine and masculine gender and produce gender-inclusive translations. Furthermore we discuss metrics associated with our test suites and validate them by means of human evaluations. Our results indicate that systems achieve reasonable and comparable performance in correctly translating both feminine and masculine gender forms for naturalistic gender phenomena. Instead, the generation of inclusive language forms in translation emerges as a challenging task for all the evaluated MT models, indicating room for future improvements and research on the topic.
</details>
<details>
<summary>摘要</summary>
为了WMT-2023"测试集"共同任务，在这篇论文中，我们summarize了两个测试集评估结果：MuST-SHE-WMT23和INES。我们通过专注于英-德和德-英语对的语言对，使用这些新创建的测试集来调查系统在翻译女性和♂性形式和生成包容性翻译方面的能力。此外，我们还讨论了我们的测试集中关联的度量和通过人工评估验证了其合理性。我们的结果表明，评估自然语言 gender 现象时，系统的表现是相对合理的和相似的，但生成包容性翻译仍然是所有评估MT模型的挑战，这表明需要未来的改进和研究。
</details></li>
</ul>
<hr>
<h2 id="Fusing-Temporal-Graphs-into-Transformers-for-Time-Sensitive-Question-Answering"><a href="#Fusing-Temporal-Graphs-into-Transformers-for-Time-Sensitive-Question-Answering" class="headerlink" title="Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering"></a>Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19292">http://arxiv.org/abs/2310.19292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Su, Phillip Howard, Nagib Hakim, Steven Bethard</li>
<li>for: 本研究旨在探讨 whether large language models can perform temporal reasoning solely using a provided text document, or whether they can benefit from additional temporal information extracted using other systems.</li>
<li>methods: 本研究使用现有的时间信息抽取系统构建时间图，并 investigate different approaches for fusing these graphs into Transformer models.</li>
<li>results: 实验结果显示，我们提议的方法可以substantially enhance the temporal reasoning capabilities of Transformer models with or without fine-tuning。此外，我们的方法也比graph convolution-based approaches的性能更高，并在 SituatedQA 和 TimeQA 中设置三个分区的新州OF-the-art perfomance。<details>
<summary>Abstract</summary>
Answering time-sensitive questions from long documents requires temporal reasoning over the times in questions and documents. An important open question is whether large language models can perform such reasoning solely using a provided text document, or whether they can benefit from additional temporal information extracted using other systems. We address this research question by applying existing temporal information extraction systems to construct temporal graphs of events, times, and temporal relations in questions and documents. We then investigate different approaches for fusing these graphs into Transformer models. Experimental results show that our proposed approach for fusing temporal graphs into input text substantially enhances the temporal reasoning capabilities of Transformer models with or without fine-tuning. Additionally, our proposed method outperforms various graph convolution-based approaches and establishes a new state-of-the-art performance on SituatedQA and three splits of TimeQA.
</details>
<details>
<summary>摘要</summary>
回答时间敏感问题从长文档中需要时间逻辑。现有一个重要的开放问题是大语言模型可以通过提供的文档来完成这种逻辑，或者它们可以从其他系统中提取的其他时间信息中受益。我们解决这个研究问题 by applying existing temporal information extraction systems to construct temporal graphs of events, times, and temporal relations in questions and documents. We then investigate different approaches for fusing these graphs into Transformer models. 实验结果表明，我们提议的方法可以具有提高Transformer模型的时间逻辑能力，无需 Fine-tuning或者使用graph convolution-based approaches。此外，我们的方法还超过了多种graph convolution-based approaches，并在SituatedQA和TimeQA中达到了新的状态对。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-love-diligent-trolls-Accounting-for-rater-effects-in-the-dialogue-safety-task"><a href="#Learning-to-love-diligent-trolls-Accounting-for-rater-effects-in-the-dialogue-safety-task" class="headerlink" title="Learning to love diligent trolls: Accounting for rater effects in the dialogue safety task"></a>Learning to love diligent trolls: Accounting for rater effects in the dialogue safety task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19271">http://arxiv.org/abs/2310.19271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michaeljohnilagan/aestrollhunting">https://github.com/michaeljohnilagan/aestrollhunting</a></li>
<li>paper_authors: Michael John Ilagan</li>
<li>for: 本研究旨在提高聊天机器人的性能，通过减少具有错误标签的用户反馈数据的影响。</li>
<li>methods: 本研究使用多用户评分和隐藏类分析（LCA）来推断正确的标签，以避免具有错误标签的用户反馈数据的影响。</li>
<li>results: 实验结果表明，当具有错误标签的用户反馈数据占多数时，使用隐藏类分析（LCA）可以准确地推断正确的标签，即使这些用户是一致的。<details>
<summary>Abstract</summary>
Chatbots have the risk of generating offensive utterances, which must be avoided. Post-deployment, one way for a chatbot to continuously improve is to source utterance/label pairs from feedback by live users. However, among users are trolls, who provide training examples with incorrect labels. To de-troll training data, previous work removed training examples that have high user-aggregated cross-validation (CV) error. However, CV is expensive; and in a coordinated attack, CV may be overwhelmed by trolls in number and in consistency among themselves. In the present work, I address both limitations by proposing a solution inspired by methodology in automated essay scoring (AES): have multiple users rate each utterance, then perform latent class analysis (LCA) to infer correct labels. As it does not require GPU computations, LCA is inexpensive. In experiments, I found that the AES-like solution can infer training labels with high accuracy when trolls are consistent, even when trolls are the majority.
</details>
<details>
<summary>摘要</summary>
chatbots 有危险性，需要避免生成冒犯性的词汇。部署后，一种方法是通过用户反馈获取词汇/标签对的Source，以便不断改进。然而，用户中有些人是啰呛用户（troll），他们提供了错误的标签的示例。为了除啰呛示例，previous work 使用了用户共同验证（CV）错误值来除啰呛示例。然而，CV 是expensive，而且在协调攻击下，CV 可能会被啰呛用户淹没。在当前的工作中，我解决了这两个限制，提出了基于 automated essay scoring（AES）的方法：在每个词汇上有多个用户评分，然后使用潜在类分析（LCA）来推断正确的标签。由于LCA不需要GPU计算，因此它是便宜的。在实验中，我发现，AES-like 方法可以在啰呛用户是一致的情况下，高精度地推断训练标签。
</details></li>
</ul>
<hr>
<h2 id="Moral-Judgments-in-Narratives-on-Reddit-Investigating-Moral-Sparks-via-Social-Commonsense-and-Linguistic-Signals"><a href="#Moral-Judgments-in-Narratives-on-Reddit-Investigating-Moral-Sparks-via-Social-Commonsense-and-Linguistic-Signals" class="headerlink" title="Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via Social Commonsense and Linguistic Signals"></a>Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via Social Commonsense and Linguistic Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19268">http://arxiv.org/abs/2310.19268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijie Xi, Munindar P. Singh</li>
<li>for: 研究在社交媒体上的实际道德情境下的人类道德判断。</li>
<li>methods: 使用计算机技术 investigate moral judgment 的因素，包括事件触发社会常识和语言标志。</li>
<li>results: 发现事件相关的负性人格特质（如immature和无礼）吸引注意力，使得它们被负责任，表明道德异常之处和负责任之间存在互相关系。此外，Language 的形式和语意对commenters的认知过程产生影响，使得事件和人物的描述更有可能被读者视为道德异常之处。<details>
<summary>Abstract</summary>
Given the increasing realism of social interactions online, social media offers an unprecedented avenue to evaluate real-life moral scenarios. We examine posts from Reddit, where authors and commenters share their moral judgments on who is blameworthy. We employ computational techniques to investigate factors influencing moral judgments, including (1) events activating social commonsense and (2) linguistic signals. To this end, we focus on excerpt-which we term moral sparks-from original posts that commenters include to indicate what motivates their moral judgments. By examining over 24,672 posts and 175,988 comments, we find that event-related negative personal traits (e.g., immature and rude) attract attention and stimulate blame, implying a dependent relationship between moral sparks and blameworthiness. Moreover, language that impacts commenters' cognitive processes to depict events and characters enhances the probability of an excerpt become a moral spark, while factual and concrete descriptions tend to inhibit this effect.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Overview-of-the-CLAIMSCAN-2023-Uncovering-Truth-in-Social-Media-through-Claim-Detection-and-Identification-of-Claim-Spans"><a href="#Overview-of-the-CLAIMSCAN-2023-Uncovering-Truth-in-Social-Media-through-Claim-Detection-and-Identification-of-Claim-Spans" class="headerlink" title="Overview of the CLAIMSCAN-2023: Uncovering Truth in Social Media through Claim Detection and Identification of Claim Spans"></a>Overview of the CLAIMSCAN-2023: Uncovering Truth in Social Media through Claim Detection and Identification of Claim Spans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19267">http://arxiv.org/abs/2310.19267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megha Sundriyal, Md Shad Akhtar, Tanmoy Chakraborty</li>
<li>for: The paper aims to address the problem of fake news and false information on online social media platforms by developing an automatic claim detection system.</li>
<li>methods: The proposed system, called CLAIMSCAN, uses natural language processing techniques to identify and extract claims from social media posts, and then verifies their credibility.</li>
<li>results: The paper reports that CLAIMSCAN achieved high accuracy in both tasks, with an F1-score of 0.85 for Task A and an F1-score of 0.75 for Task B. Additionally, the system was able to identify claims in a variety of formats and contexts, demonstrating its versatility and effectiveness.<details>
<summary>Abstract</summary>
A significant increase in content creation and information exchange has been made possible by the quick development of online social media platforms, which has been very advantageous. However, these platforms have also become a haven for those who disseminate false information, propaganda, and fake news. Claims are essential in forming our perceptions of the world, but sadly, they are frequently used to trick people by those who spread false information. To address this problem, social media giants employ content moderators to filter out fake news from the actual world. However, the sheer volume of information makes it difficult to identify fake news effectively. Therefore, it has become crucial to automatically identify social media posts that make such claims, check their veracity, and differentiate between credible and false claims. In response, we presented CLAIMSCAN in the 2023 Forum for Information Retrieval Evaluation (FIRE'2023). The primary objectives centered on two crucial tasks: Task A, determining whether a social media post constitutes a claim, and Task B, precisely identifying the words or phrases within the post that form the claim. Task A received 40 registrations, demonstrating a strong interest and engagement in this timely challenge. Meanwhile, Task B attracted participation from 28 teams, highlighting its significance in the digital era of misinformation.
</details>
<details>
<summary>摘要</summary>
在线社交媒体平台的快速发展已经提供了大量内容创作和信息交换的机会，这对我们来说非常有利。然而，这些平台也成为了散布 FALSE 信息、宣传和假新闻的天堂。我们的看法是基于声明的，但耻势在散布 FALSE 信息上。为了解决这个问题，社交媒体巨头雇用了内容筛选人员来从实际世界中排除假新闻。然而，巨大量的信息使得检测 FALSE 信息变得非常困难。因此，自动地找到社交媒体上的这些声明，评估其真实性，并将准确和 FALSE 声明分开变得非常重要。为此，我们在2023年 Forum for Information Retrieval Evaluation（FIRE'2023）上发表了 CLAIMSCAN。主要目标是解决两个关键任务：任务 A，判断社交媒体帖子是否表达了声明，以及任务 B，在帖子中 precisely Identify 声明的单词或短语。任务 A 得到了 40 个注册，表明这是一个有关时间的挑战。任务 B 吸引了 28 个团队的参与，强调其在数字时代中的重要性。
</details></li>
</ul>
<hr>
<h2 id="M4LE-A-Multi-Ability-Multi-Range-Multi-Task-Multi-Domain-Long-Context-Evaluation-Benchmark-for-Large-Language-Models"><a href="#M4LE-A-Multi-Ability-Multi-Range-Multi-Task-Multi-Domain-Long-Context-Evaluation-Benchmark-for-Large-Language-Models" class="headerlink" title="M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models"></a>M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19240">http://arxiv.org/abs/2310.19240</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kwanwaichung/m4le">https://github.com/kwanwaichung/m4le</a></li>
<li>paper_authors: Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou Li, Lifeng Shang, Qun Liu, Kam-Fai Wong</li>
<li>for: 本研究旨在系统地评估大语言模型（LLM）在长序处理方面的能力。</li>
<li>methods: 本研究提出了一个多能力、多范围、多任务、多领域的benchmark，即M4LE（Multi-ability, Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation），以evaluate LLMs在长序上的表现。</li>
<li>results: 研究发现，当务务要求多个 span 注意时，当前的 LLMs 很难理解长序上的上下文； semantic retrieval 任务对于高水平的 LLMs 更加困难；模型通过长文本的 fine-tuning 和NTK意识扩展方法来提高表现相当。<details>
<summary>Abstract</summary>
Managing long sequences has become an important and necessary feature for large language models (LLMs). However, it is still an open question of how to comprehensively and systematically evaluate the long-sequence capability of LLMs. One of the reasons is that conventional and widely-used benchmarks mainly consist of short sequences. In this paper, we propose M4LE, a Multi-ability, Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation. M4LE is based on a diverse NLP task pool comprising 36 NLP datasets, 11 task types and 12 domains. To alleviate the scarcity of tasks with naturally long sequences and incorporate multiple-ability assessment, we propose an automatic approach (but with negligible human annotations) to convert short-sequence tasks into a unified long-sequence scenario where LLMs have to identify single or multiple relevant spans in long contexts based on explicit or semantic hints. Specifically, the scenario includes five different types of abilities: (1) explicit single-span; (2) semantic single-span; (3) explicit multiple-span; (4) semantic multiple-span; and (5) global context understanding. The resulting samples in M4LE are evenly distributed from 1k to 8k input length. We conducted a systematic evaluation on 11 well-established LLMs, especially those optimized for long-sequence inputs. Our results reveal that: 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval task is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的长序处理已成为一项重要和必要的功能。然而，评估 LLM 的长序能力仍然是一个开放的问题。一个原因是，现有的标准和广泛使用的benchmark主要由短序组成。在这篇论文中，我们提出了M4LE，一个多能力、多范围、多任务、多领域的benchmark，用于长Context评估。M4LE基于36个NLP任务、11种任务类型和12个领域的多样化任务池。为了解决短序任务的缺乏和多能力评估的问题，我们提出了一种自动化（但具有较少的人工注解）的方法，将短序任务转换成一个统一的长序场景，要求LLMs在长上下文中identify单个或多个相关的span，基于显式或 semantics 的提示。具体来说，场景包括5种能力：（1）显式单 span;（2）semantic single span;（3）显式多 span;（4）semantic multiple span;和（5）全文理解。M4LE的样本均匀分布于1k至8k输入长度。我们对11个已确立的LLM进行了系统性的评估。我们的结果表明：1）当前 LLMs 在长上下文中理解缺乏，特别是需要多个span的注意。2）semantic retrieval任务更难 для能力LLMs。3）通过长文本进行Position interpolating fine-tuning可以达到与没有 fine-tuning 的NTK 意识的比较类似的性能。我们将benchmark公开提供，以便未来的研究在这个挑战领域。
</details></li>
</ul>
<hr>
<h2 id="Building-Real-World-Meeting-Summarization-Systems-using-Large-Language-Models-A-Practical-Perspective"><a href="#Building-Real-World-Meeting-Summarization-Systems-using-Large-Language-Models-A-Practical-Perspective" class="headerlink" title="Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective"></a>Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19233">http://arxiv.org/abs/2310.19233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, Shashi Bhushan TN</li>
<li>for: 这个论文目的是研究如何使用大型自然语言模型（LLM）建立实际应用中的会议概要系统。</li>
<li>methods: 作者通过对多种关闭源和开源LLM进行广泛的评估和比较，包括GPT-4、GPT-3.5、PaLM-2和LLaMA-2等。</li>
<li>results: 研究发现大多数关闭源LLM在性能方面表现较好，但是较小的开源模型Like LLaMA-2（7B和13B）在零基础enario下仍可以达到相当于大关闭源模型的性能。考虑到关闭源模型的隐私问题和仅通过API访问的高成本，开源模型更有利可图在实际应用中使用。 LLama-2-7B模型在性能和成本之间做出了更好的平衡，因此更适合工业应用。<details>
<summary>Abstract</summary>
This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs). For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findings reveal that most closed-source LLMs are generally better in terms of performance. However, much smaller open-source models like LLaMA- 2 (7B and 13B) could still achieve performance comparable to the large closed-source models even in zero-shot scenarios. Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models, the opensource models that can achieve competitive performance are more advantageous for industrial use. Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage. In sum, this paper offers practical insights on using LLMs for real-world business meeting summarization, shedding light on the trade-offs between performance and cost.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adapter-Pruning-using-Tropical-Characterization"><a href="#Adapter-Pruning-using-Tropical-Characterization" class="headerlink" title="Adapter Pruning using Tropical Characterization"></a>Adapter Pruning using Tropical Characterization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19232">http://arxiv.org/abs/2310.19232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Bhardwaj, Tushar Vaidya, Soujanya Poria</li>
<li>for: 本研究旨在找出适合下游应用的adaptor层参数数量，以提高模型性能。</li>
<li>methods: 本文提出了一种基于тропиcal геометри的adapter层杜台法，通过优化问题来减少无用参数。</li>
<li>results: 实验结果显示，基于 тропиcal геометри的方法可以更好地标识需要减少的参数，而combinedapproach在多个任务上表现最佳。<details>
<summary>Abstract</summary>
Adapters are widely popular parameter-efficient transfer learning approaches in natural language processing that insert trainable modules in between layers of a pre-trained language model. Apart from several heuristics, however, there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications. In this paper, we propose an adapter pruning approach by studying the tropical characteristics of trainable modules. We cast it as an optimization problem that aims to prune parameters from the adapter layers without changing the orientation of underlying tropical hypersurfaces. Our experiments on five NLP datasets show that tropical geometry tends to identify more relevant parameters to prune when compared with the magnitude-based baseline, while a combined approach works best across the tasks.
</details>
<details>
<summary>摘要</summary>
<SYS> translate-language: zh-CN</SYS>Adapter 是自然语言处理领域非常流行的参数效率转移学习方法之一，它通过在预训练语言模型层之间插入可训练模块来实现。然而， apart from several heuristics， there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications. In this paper, we propose an adapter pruning approach by studying the tropical characteristics of trainable modules. We cast it as an optimization problem that aims to prune parameters from the adapter layers without changing the orientation of underlying tropical hypersurfaces. Our experiments on five NLP datasets show that tropical geometry tends to identify more relevant parameters to prune when compared with the magnitude-based baseline, while a combined approach works best across the tasks.Here's the translation in Traditional Chinese:<SYS> translate-language: zh-TW</SYS>Adapter 是自然语言处理领域非常流行的参数效率传授学习方法之一，它通过在预训练语言模型层之间插入可训练模块来实现。然而， apart from several heuristics， there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications. In this paper, we propose an adapter pruning approach by studying the tropical characteristics of trainable modules. We cast it as an optimization problem that aims to prune parameters from the adapter layers without changing the orientation of underlying tropical hypersurfaces. Our experiments on five NLP datasets show that tropical geometry tends to identify more relevant parameters to prune when compared with the magnitude-based baseline, while a combined approach works best across the tasks.
</details></li>
</ul>
<hr>
<h2 id="LitCab-Lightweight-Calibration-of-Language-Models-on-Outputs-of-Varied-Lengths"><a href="#LitCab-Lightweight-Calibration-of-Language-Models-on-Outputs-of-Varied-Lengths" class="headerlink" title="LitCab: Lightweight Calibration of Language Models on Outputs of Varied Lengths"></a>LitCab: Lightweight Calibration of Language Models on Outputs of Varied Lengths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19208">http://arxiv.org/abs/2310.19208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/launchnlp/litcab">https://github.com/launchnlp/litcab</a></li>
<li>paper_authors: Xin Liu, Muhammad Khalifa, Lu Wang</li>
<li>for: 本研究旨在提高语言模型（LM）的准确率，即使用于短文本生成和长文本生成任务上。</li>
<li>methods: 本研究提出了一种名为 LitCab 的轻量级准确机制，该机制通过对输入文本表示进行 Linear 层的处理，以改善 LM 的准确率。</li>
<li>results: 在使用 LitCab 进行准确化后，LM 的准确率得到了改善，具体来说，在 CaT  benchmark 上，LitCab 可以降低 ECE 平均值 by 20%。此外，研究还发现，大型模型在短文本生成任务上表现出较好的准确率，而 GPT 家族模型在各种任务上表现出较好的准确率。<details>
<summary>Abstract</summary>
A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations, a common issue of LMs, as well as building more trustworthy models. Yet, popular neural model calibration techniques are not well-suited for LMs due to their lack of flexibility in discerning answer correctness and their high computational costs. For instance, post-processing methods like temperature scaling are often unable to reorder the candidate generations. Moreover, training-based methods require finetuning the entire model, which is impractical due to the increasing sizes of modern LMs. In this paper, we present LitCab, a lightweight calibration mechanism consisting of a single linear layer taking the input text representation and manipulateing the LM output logits. LitCab improves model calibration by only adding < 2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of 7 text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks, by reducing the average ECE score by 20%. We further conduct a comprehensive evaluation with 7 popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (1) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones. (2) GPT-family models show superior calibration compared to LLaMA, Llama2 and Vicuna models despite having much fewer parameters. (3) Finetuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of finetuning setups for calibrating LMs.
</details>
<details>
<summary>摘要</summary>
一个模型被视为良好准确报告当其报告与实际输出正确性匹配。为语言模型（LM）进行准确性调整是非常重要，因为它可以掌握hallucination问题，并建立更可靠的模型。然而，流行的神经网络模型调整技术不适合LM，因为它们缺乏对正确答案的分辨率，并且计算成本较高。例如，后处理方法like温度scaling经常无法重新排序候选生成。此外，基于训练的方法需要进一步训练整个模型，这是现代LM的增大模型大小的不可避免问题。在这篇论文中，我们提出了LitCab，一种轻量级准确机制，由单个线性层对输入文本表示进行修改。LitCab提高了模型准确性，仅添加了<2%的原始模型参数。为了评估，我们构建了CaT，一个包含7个文本生成任务的benchmark，覆盖各种响应范围。我们在Llama2-7B上测试LitCab，其在所有任务上提高了准确性，减少了平均ECE分数20%。此外，我们进行了7种流行的开源LM的全面评估，得到以下关键发现：（1）同家大型模型在短文生成任务上表现更好，但并不一定是在更长的任务上。（2）GPT家族模型在准确性方面表现更好，尽管它们具有许多 fewer参数。（3）对于有限目标（如对话）的预训练模型（如LLaMA）进行训练可能会导致准确性下降，highlighting the importance of finetuning setups for calibrating LMs。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.CL_2023_10_30/" data-id="clogyj8xa00ee7craa23pcr9y" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/cs.LG_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T10:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/cs.LG_2023_10_30/">cs.LG - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Subgraph-GNNs-by-Learning-Effective-Selection-Policies"><a href="#Efficient-Subgraph-GNNs-by-Learning-Effective-Selection-Policies" class="headerlink" title="Efficient Subgraph GNNs by Learning Effective Selection Policies"></a>Efficient Subgraph GNNs by Learning Effective Selection Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20082">http://arxiv.org/abs/2310.20082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beatrice Bevilacqua, Moshe Eliasof, Eli Meirom, Bruno Ribeiro, Haggai Maron</li>
<li>for: 本研究旨在学习从多个子图中学习图表示，但是存在许多可能的子图，选择这些子图是一项 computationally expensive 的任务。</li>
<li>methods: 我们提出了一种新的方法，called Policy-Learn，它通过Iterative manner来学习选择子图。</li>
<li>results: 我们的实验结果表明，Policy-Learn 可以在各种 dataset 上超过现有的基准值。<details>
<summary>Abstract</summary>
Subgraph GNNs are provably expressive neural architectures that learn graph representations from sets of subgraphs. Unfortunately, their applicability is hampered by the computational complexity associated with performing message passing on many subgraphs. In this paper, we consider the problem of learning to select a small subset of the large set of possible subgraphs in a data-driven fashion. We first motivate the problem by proving that there are families of WL-indistinguishable graphs for which there exist efficient subgraph selection policies: small subsets of subgraphs that can already identify all the graphs within the family. We then propose a new approach, called Policy-Learn, that learns how to select subgraphs in an iterative manner. We prove that, unlike popular random policies and prior work addressing the same problem, our architecture is able to learn the efficient policies mentioned above. Our experimental results demonstrate that Policy-Learn outperforms existing baselines across a wide range of datasets.
</details>
<details>
<summary>摘要</summary>
<font face="Times New Roman"><font size="5"><font color="#333333"> SUBGRAPH GNNs 是可证明表示力强的神经网络架构，可以从多个子图中学习图表示。然而，它们的应用受到多个子图计算消息传递的复杂性所限制。在这篇论文中，我们考虑了选择一小集合可能的大量子图的问题。我们首先提出了这个问题的问题，并证明了存在一些 families of WL-indistinguishable graphs 中存在高效的子图选择策略：小 subsets of subgraphs 可以已经识别整个家族中的所有图。然后，我们提出了一种新的方法，叫做 Policy-Learn，可以在循环的方式中学习选择子图。我们证明了，与很多Random policies 和现有的相关工作不同，我们的架构可以学习上述高效的策略。我们的实验结果表明，Policy-Learn 在各种数据集上都超过了现有的基准值。</font></font></font>
</details></li>
</ul>
<hr>
<h2 id="Hybridizing-Physics-and-Neural-ODEs-for-Predicting-Plasma-Inductance-Dynamics-in-Tokamak-Fusion-Reactors"><a href="#Hybridizing-Physics-and-Neural-ODEs-for-Predicting-Plasma-Inductance-Dynamics-in-Tokamak-Fusion-Reactors" class="headerlink" title="Hybridizing Physics and Neural ODEs for Predicting Plasma Inductance Dynamics in Tokamak Fusion Reactors"></a>Hybridizing Physics and Neural ODEs for Predicting Plasma Inductance Dynamics in Tokamak Fusion Reactors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20079">http://arxiv.org/abs/2310.20079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Allen M. Wang, Darren T. Garnier, Cristina Rea</li>
<li>for: 这个论文的目的是提高核聚变炉的控制和稳定性，以便使其成为可靠的能源来源。</li>
<li>methods: 这篇论文使用神经网络Ordinary Differential Equations（ODE）框架来预测核聚变炉中的电流和内 inductance 动态。</li>
<li>results: 研究发现，将物理基于的方程和神经网络模型结合在一起，能够更好地预测核聚变炉中的动态，并且比现有的物理驱动的ODE模型和纯神经网络模型性能更高。<details>
<summary>Abstract</summary>
While fusion reactors known as tokamaks hold promise as a firm energy source, advances in plasma control, and handling of events where control of plasmas is lost, are needed for them to be economical. A significant bottleneck towards applying more advanced control algorithms is the need for better plasma simulation, where both physics-based and data-driven approaches currently fall short. The former is bottle-necked by both computational cost and the difficulty of modelling plasmas, and the latter is bottle-necked by the relative paucity of data. To address this issue, this work applies the neural ordinary differential equations (ODE) framework to the problem of predicting a subset of plasma dynamics, namely the coupled plasma current and internal inductance dynamics. As the neural ODE framework allows for the natural inclusion of physics-based inductive biases, we train both physics-based and neural network models on data from the Alcator C-Mod fusion reactor and find that a model that combines physics-based equations with a neural ODE performs better than both existing physics-motivated ODEs and a pure neural ODE model.
</details>
<details>
<summary>摘要</summary>
tokamak核聚变 реактор显示出可能成为一种可靠的能源来源，但是进一步的材料控制和失控事件处理技术的发展仍然需要进一步。目前的控制算法应用受到更好的材料仿真的限制，现有的物理基于的仿真方法和数据驱动的方法都有局限性。为解决这个问题，本研究使用神经网络Ordinary Differential Equations（ODE）框架来预测材料动态的一部分，即材料电流和内部 inductance 动态。由于神经网络ODE框架具有自然地包含物理基于的 inductive bias，我们在 Alcator C-Mod 核聚变反应堆数据上训练了物理基于的 ODE 模型和神经网络模型，并发现一个结合物理基于的 ODE 和神经网络模型的模型在物理基于的 ODE 模型和纯神经网络模型之间具有更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Meek-Separators-and-Their-Applications-in-Targeted-Causal-Discovery"><a href="#Meek-Separators-and-Their-Applications-in-Targeted-Causal-Discovery" class="headerlink" title="Meek Separators and Their Applications in Targeted Causal Discovery"></a>Meek Separators and Their Applications in Targeted Causal Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20075">http://arxiv.org/abs/2310.20075</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uhlerlab/meek_sep">https://github.com/uhlerlab/meek_sep</a></li>
<li>paper_authors: Kirankumar Shiragur, Jiaqi Zhang, Caroline Uhler</li>
<li>for: 学习干预结构FROM� intervened数据</li>
<li>methods: 引入Meek separator，一种可以将尚未oriented edges分解为小型连接组件的subset of vertices</li>
<li>results: 提出了两种随机算法，可以在 subset search 和 causal matching 问题上实现 logarithmic approximation 的近似解决方案，并且提供了首次known的平均情况证明 garantuee。<details>
<summary>Abstract</summary>
Learning causal structures from interventional data is a fundamental problem with broad applications across various fields. While many previous works have focused on recovering the entire causal graph, in practice, there are scenarios where learning only part of the causal graph suffices. This is called $targeted$ causal discovery. In our work, we focus on two such well-motivated problems: subset search and causal matching. We aim to minimize the number of interventions in both cases.   Towards this, we introduce the $Meek~separator$, which is a subset of vertices that, when intervened, decomposes the remaining unoriented edges into smaller connected components. We then present an efficient algorithm to find Meek separators that are of small sizes. Such a procedure is helpful in designing various divide-and-conquer-based approaches. In particular, we propose two randomized algorithms that achieve logarithmic approximation for subset search and causal matching, respectively. Our results provide the first known average-case provable guarantees for both problems. We believe that this opens up possibilities to design near-optimal methods for many other targeted causal structure learning problems arising from various applications.
</details>
<details>
<summary>摘要</summary>
Towards this, we introduce the Meek separator, which is a subset of vertices that, when intervened, decomposes the remaining unoriented edges into smaller connected components. We then present an efficient algorithm to find Meek separators that are of small sizes. Such a procedure is helpful in designing various divide-and-conquer-based approaches.In particular, we propose two randomized algorithms that achieve logarithmic approximation for subset search and causal matching, respectively. Our results provide the first known average-case provable guarantees for both problems. We believe that this opens up possibilities to design near-optimal methods for many other targeted causal structure learning problems arising from various applications.
</details></li>
</ul>
<hr>
<h2 id="Decentralised-Scalable-and-Privacy-Preserving-Synthetic-Data-Generation"><a href="#Decentralised-Scalable-and-Privacy-Preserving-Synthetic-Data-Generation" class="headerlink" title="Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation"></a>Decentralised, Scalable and Privacy-Preserving Synthetic Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20062">http://arxiv.org/abs/2310.20062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishal Ramesh, Rui Zhao, Naman Goel<br>for:The paper is written for the purpose of exploring the use of synthetic data in machine learning, with a focus on privacy and trustworthiness.methods:The paper proposes a novel system that utilizes three building blocks: Solid (Social Linked Data), MPC (Secure Multi-Party Computation), and Trusted Execution Environments (TEEs) to generate differentially private synthetic data in a decentralized and scalable manner.results:The paper presents empirical results on simulated and real datasets, demonstrating the effectiveness of the proposed system in addressing various challenges in responsible and trustworthy synthetic data generation, including contributor autonomy, decentralization, privacy, and scalability.<details>
<summary>Abstract</summary>
Synthetic data is emerging as a promising way to harness the value of data, while reducing privacy risks. The potential of synthetic data is not limited to privacy-friendly data release, but also includes complementing real data in use-cases such as training machine learning algorithms that are more fair and robust to distribution shifts etc. There is a lot of interest in algorithmic advances in synthetic data generation for providing better privacy and statistical guarantees and for its better utilisation in machine learning pipelines. However, for responsible and trustworthy synthetic data generation, it is not sufficient to focus only on these algorithmic aspects and instead, a holistic view of the synthetic data generation pipeline must be considered. We build a novel system that allows the contributors of real data to autonomously participate in differentially private synthetic data generation without relying on a trusted centre. Our modular, general and scalable solution is based on three building blocks namely: Solid (Social Linked Data), MPC (Secure Multi-Party Computation) and Trusted Execution Environments (TEEs). Solid is a specification that lets people store their data securely in decentralised data stores called Pods and control access to their data. MPC refers to the set of cryptographic methods for different parties to jointly compute a function over their inputs while keeping those inputs private. TEEs such as Intel SGX rely on hardware based features for confidentiality and integrity of code and data. We show how these three technologies can be effectively used to address various challenges in responsible and trustworthy synthetic data generation by ensuring: 1) contributor autonomy, 2) decentralisation, 3) privacy and 4) scalability. We support our claims with rigorous empirical results on simulated and real datasets and different synthetic data generation algorithms.
</details>
<details>
<summary>摘要</summary>
人工数据正在成为一种有前途的数据利用方式，同时减少隐私风险。人工数据的潜力不仅限于隐私友好数据发布，还包括补充真实数据在使用场景中，如训练不同分布下的机器学习算法等。有很多关注于人工数据生成算法的进步，以提供更好的隐私和统计保证，并更好地在机器学习管道中使用。但是，为了负责任地和可信地生成人工数据，不能仅仅专注于算法方面，而是需要考虑整个人工数据生成管道的各个方面。我们建立了一个新的系统，允许真实数据的contributors自主参与在匿名 differentially private 人工数据生成中，不需要依赖于信任中心。我们的模块化、通用和可扩展解决方案基于以下三个基础 componenets：Solid（社交链接数据）、MPC（安全多方计算）和TEEs（信任执行环境）。Solid是一种规范，允许人们安全地存储他们的数据在分布式数据存储 called Pods 中，控制对他们的数据的访问。MPC是一组 криптографических方法，用于不同党在他们的输入上进行共同计算，而保持这些输入private。TEEs，如 intel SGX，基于硬件特性，提供代码和数据的confidentiality和完整性。我们示出了这三种技术可以有效地解决负责任和可信的人工数据生成中的各种挑战，包括：1）参与者自主权，2）分布化，3）隐私和4）可扩展性。我们支持我们的主张通过对 simulated 和实际数据集和不同的人工数据生成算法进行严格的实验结果来证明。
</details></li>
</ul>
<hr>
<h2 id="AdaSub-Stochastic-Optimization-Using-Second-Order-Information-in-Low-Dimensional-Subspaces"><a href="#AdaSub-Stochastic-Optimization-Using-Second-Order-Information-in-Low-Dimensional-Subspaces" class="headerlink" title="AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces"></a>AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20060">http://arxiv.org/abs/2310.20060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jvictormata/adasub">https://github.com/jvictormata/adasub</a></li>
<li>paper_authors: João Victor Galvão da Mata, Martin S. Andersen</li>
<li>for: 本研究旨在提出一种基于第二阶信息的搜索方向算法，以优化计算成本和算法效率。</li>
<li>methods: 该算法使用随机优化方法，并在低维度子空间中基于当前和历史信息进行适应性定义。</li>
<li>results: 数据结果表明，AdaSub 超过了流行的随机优化器在时间和迭代次数方面达到给定精度所需的成本和效率。<details>
<summary>Abstract</summary>
We introduce AdaSub, a stochastic optimization algorithm that computes a search direction based on second-order information in a low-dimensional subspace that is defined adaptively based on available current and past information. Compared to first-order methods, second-order methods exhibit better convergence characteristics, but the need to compute the Hessian matrix at each iteration results in excessive computational expenses, making them impractical. To address this issue, our approach enables the management of computational expenses and algorithm efficiency by enabling the selection of the subspace dimension for the search. Our code is freely available on GitHub, and our preliminary numerical results demonstrate that AdaSub surpasses popular stochastic optimizers in terms of time and number of iterations required to reach a given accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍AdaSub，一种随机优化算法，它基于现有的第二项信息在低维度的子空间中计算搜寻方向。相比于第一项方法，第二项方法在数据数据库中具有更好的征具特性，但是计算Hessian矩阵在每个迭代中的需求导致过度的计算成本，使得它们不实际。为了解决这个问题，我们的方法可以选择搜寻子空间的维度，以管理计算成本和算法效率。我们的代码可以在GitHub上免费下载，我们的初步数据显示AdaSub比流行的随机优化器在时间和迭代数量上优化。
</details></li>
</ul>
<hr>
<h2 id="Estimating-optimal-PAC-Bayes-bounds-with-Hamiltonian-Monte-Carlo"><a href="#Estimating-optimal-PAC-Bayes-bounds-with-Hamiltonian-Monte-Carlo" class="headerlink" title="Estimating optimal PAC-Bayes bounds with Hamiltonian Monte Carlo"></a>Estimating optimal PAC-Bayes bounds with Hamiltonian Monte Carlo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20053">http://arxiv.org/abs/2310.20053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Szilvia Ujváry, Gergely Flamich, Vincent Fortuin, José Miguel Hernández Lobato</li>
<li>for: 本文探讨了在PAC-Bayes理论中，如何在 posterior family 中强制 Gaussian 分布时，失去的紧张程度。</li>
<li>methods: 本文使用了优化的 Gibbs  posterior，使用 Hamiltonian Monte Carlo 采样，并使用 thermodynamic integration 来估算 KL 差。</li>
<li>results: 实验结果表明，使用优化的 posterior 可以减少紧张程度，在一些情况下可以达到 5-6% 的差异。<details>
<summary>Abstract</summary>
An important yet underexplored question in the PAC-Bayes literature is how much tightness we lose by restricting the posterior family to factorized Gaussian distributions when optimizing a PAC-Bayes bound. We investigate this issue by estimating data-independent PAC-Bayes bounds using the optimal posteriors, comparing them to bounds obtained using MFVI. Concretely, we (1) sample from the optimal Gibbs posterior using Hamiltonian Monte Carlo, (2) estimate its KL divergence from the prior with thermodynamic integration, and (3) propose three methods to obtain high-probability bounds under different assumptions. Our experiments on the MNIST dataset reveal significant tightness gaps, as much as 5-6\% in some cases.
</details>
<details>
<summary>摘要</summary>
“一个重要但未得到充分探讨的问题在PAC-Bayes文献中是：当我们将 posterior 家族限制为分布式 Gaussian 分布时，我们会失去多少紧密性？我们调查这个问题，使用最佳 Gibbs  posterior，与 MFVI  Comparing  them, we propose three methods to obtain high-probability bounds under different assumptions. Our experiments on the MNIST dataset show significant tightness gaps, as much as 5-6% in some cases.”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="The-Expressibility-of-Polynomial-based-Attention-Scheme"><a href="#The-Expressibility-of-Polynomial-based-Attention-Scheme" class="headerlink" title="The Expressibility of Polynomial based Attention Scheme"></a>The Expressibility of Polynomial based Attention Scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20051">http://arxiv.org/abs/2310.20051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao Song, Guangyi Xu, Junze Yin</li>
<li>for: 本研究旨在理解幂数注意 Mechanism的表达能力。</li>
<li>methods: 我们使用两个特意设计的数据集，分别是 $\mathcal{D}_0$ 和 $\mathcal{D}_1$，并通过单层幂数注意网络分类这两个数据集来进行分析。</li>
<li>results: 我们的分析发现，当幂数degree高 enough时，单层幂数注意网络可以有效地分类 $\mathcal{D}_0$ 和 $\mathcal{D}_1$。但是，当幂数degree低时，网络无法有效地分类这两个数据集。这种分析表明高度幂数可以更好地强调大值，并提供了在幂数注意 Mechanism中包含更高级幂数的理由。<details>
<summary>Abstract</summary>
Large language models (LLMs) have significantly improved various aspects of our daily lives. These models have impacted numerous domains, from healthcare to education, enhancing productivity, decision-making processes, and accessibility. As a result, they have influenced and, to some extent, reshaped people's lifestyles. However, the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference. While a recent study by [KMZ23] introduced a technique that replaces the softmax with a polynomial function and polynomial sketching to speed up attention mechanisms, the theoretical understandings of this new approach are not yet well understood.   In this paper, we offer a theoretical analysis of the expressive capabilities of polynomial attention. Our study reveals a disparity in the ability of high-degree and low-degree polynomial attention. Specifically, we construct two carefully designed datasets, namely $\mathcal{D}_0$ and $\mathcal{D}_1$, where $\mathcal{D}_1$ includes a feature with a significantly larger value compared to $\mathcal{D}_0$. We demonstrate that with a sufficiently high degree $\beta$, a single-layer polynomial attention network can distinguish between $\mathcal{D}_0$ and $\mathcal{D}_1$. However, with a low degree $\beta$, the network cannot effectively separate the two datasets. This analysis underscores the greater effectiveness of high-degree polynomials in amplifying large values and distinguishing between datasets. Our analysis offers insight into the representational capacity of polynomial attention and provides a rationale for incorporating higher-degree polynomials in attention mechanisms to capture intricate linguistic correlations.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经对我们日常生活中的多个领域产生了重要影响，包括医疗、教育和决策等。这些模型不仅提高了生产力和决策过程，而且也提高了存取性。因此，它们已经影响了和改变了人们的生活方式。然而， трансформа器架构中的对话复杂度问题对于处理长文本背景时仍然是一个挑战。这个问题使得训练非常大的模型或在处理长文本时使用它们不可避免地成为问题。在一篇最近发表的研究《[KMZ23]》中，提出了一种替代使用多项函数和概率卷积来加速对话机制的技术。然而，这新的方法的理论基础仍然未被彻底了解。在这篇论文中，我们提供了对于多项函数的表达能力的理论分析。我们的研究显示，高度的多项函数能够更好地增强大值，并且可以让单层多项函数网络分辨不同的数据集。具体来说，我们创建了两个特别设计的数据集，分别为$\mathcal{D}_0$和$\mathcal{D}_1$。 $\mathcal{D}_1$ 包含了一个具有许多更大的特征值，相比于 $\mathcal{D}_0$。我们展示了，透过调整$\beta$的高度，单层多项函数网络可以对 $\mathcal{D}_0$ 和 $\mathcal{D}_1$ 进行分辨。然而，在低度$\beta$下，这个网络无法有效地分辨这两个数据集。这一分析显示出高度的多项函数在增强大值和分辨不同数据集方面的更好表现。我们的分析给出了对于多项函数的表达能力的理论基础，并且提供了将更高度的多项函数包含在对话机制中以捕捉复杂的语言相关性的理论基础。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Riemannian-Diffusion-Models"><a href="#Scaling-Riemannian-Diffusion-Models" class="headerlink" title="Scaling Riemannian Diffusion Models"></a>Scaling Riemannian Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20030">http://arxiv.org/abs/2310.20030</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/louaaron/Scaling-Riemannian-Diffusion">https://github.com/louaaron/Scaling-Riemannian-Diffusion</a></li>
<li>paper_authors: Aaron Lou, Minkai Xu, Stefano Ermon</li>
<li>for: 学习扩展到普通的欧几丁素空间之外的扩展分布（Riemannian diffusion models），以便在普通多槽中进行分布学习。</li>
<li>methods: 提出了一些实用的改进方法，包括使用各种 Ansatz 来快速计算相关量，以及利用Symmetric spaces的特点来提高计算精度。</li>
<li>results: 在低维数据集上，提出的改进方法能够提供明显的改进，使扩散方法在对抗其他方法的比赛中表现出色，并且在高维任务上也能够成功应用。<details>
<summary>Abstract</summary>
Riemannian diffusion models draw inspiration from standard Euclidean space diffusion models to learn distributions on general manifolds. Unfortunately, the additional geometric complexity renders the diffusion transition term inexpressible in closed form, so prior methods resort to imprecise approximations of the score matching training objective that degrade performance and preclude applications in high dimensions. In this work, we reexamine these approximations and propose several practical improvements. Our key observation is that most relevant manifolds are symmetric spaces, which are much more amenable to computation. By leveraging and combining various ans\"{a}tze, we can quickly compute relevant quantities to high precision. On low dimensional datasets, our correction produces a noticeable improvement, allowing diffusion to compete with other methods. Additionally, we show that our method enables us to scale to high dimensional tasks on nontrivial manifolds. In particular, we model QCD densities on $SU(n)$ lattices and contrastively learned embeddings on high dimensional hyperspheres.
</details>
<details>
<summary>摘要</summary>
里曼尼扩散模型 Draws inspiration from标准Euclidean空间扩散模型 To learn distribution on general manifolds. Unfortunately, the additional geometric complexity makes the diffusion transition term inexpressible in closed form, so prior methods resort to imprecise approximations of the score matching training objective that degrade performance and preclude applications in high dimensions. In this work, we reexamine these approximations and propose several practical improvements. Our key observation is that most relevant manifolds areSymmetric spaces, which are much more amenable to computation. By leveraging and combining various ansätze, we can quickly compute relevant quantities to high precision. On low dimensional datasets, our correction produces a noticeable improvement, allowing diffusion to compete with other methods. Additionally, we show that our method enables us to scale to high dimensional tasks on nontrivial manifolds. In particular, we model QCD densities on $SU(n)$ lattices and contrastively learned embeddings on high dimensional hyperspheres.Note: Simplified Chinese is a romanization of Chinese, and the translation may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="PolyThrottle-Energy-efficient-Neural-Network-Inference-on-Edge-Devices"><a href="#PolyThrottle-Energy-efficient-Neural-Network-Inference-on-Edge-Devices" class="headerlink" title="PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices"></a>PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19991">http://arxiv.org/abs/2310.19991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghao Yan, Hongyi Wang, Shivaram Venkataraman</li>
<li>for: 这篇论文旨在调查使用硬件组件的配置方式对神经网络（NN）的推理过程中的能源消耗。</li>
<li>methods: 该论文使用了受限的权重优化算法来优化硬件组件的配置，以保持能源消耗的最佳平衡。</li>
<li>results: 实验结果显示，PolyThrottle可以将 популяр的模型的能源消耗减少至36%，并且可以快速地 converges到近似优化的设置，满足应用程序的约束。<details>
<summary>Abstract</summary>
As neural networks (NN) are deployed across diverse sectors, their energy demand correspondingly grows. While several prior works have focused on reducing energy consumption during training, the continuous operation of ML-powered systems leads to significant energy use during inference. This paper investigates how the configuration of on-device hardware-elements such as GPU, memory, and CPU frequency, often neglected in prior studies, affects energy consumption for NN inference with regular fine-tuning. We propose PolyThrottle, a solution that optimizes configurations across individual hardware components using Constrained Bayesian Optimization in an energy-conserving manner. Our empirical evaluation uncovers novel facets of the energy-performance equilibrium showing that we can save up to 36 percent of energy for popular models. We also validate that PolyThrottle can quickly converge towards near-optimal settings while satisfying application constraints.
</details>
<details>
<summary>摘要</summary>
neural networks (NN) 在多个领域部署时，其能量需求也随之增长。虽然先前的研究主要关注在训练阶段减少能量消耗，但是 ML 搭建在运行阶段的能量使用也是非常重要的。这篇论文研究了在设备硬件元素配置，如 GPU、内存和 CPU 频率等，在 NN 推理过程中对能量消耗的影响。我们提出了 PolyThrottle，一种解决方案，通过受限的 Bayesian 优化来优化硬件元素的配置，以节省能量。我们的实验证明，PolyThrottle 可以保持最佳配置，同时满足应用程序的限制。我们的实验结果还表明，PolyThrottle 可以在减少训练时间的情况下，将能量消耗减少到 36% 左右。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Up-Differentially-Private-LASSO-Regularized-Logistic-Regression-via-Faster-Frank-Wolfe-Iterations"><a href="#Scaling-Up-Differentially-Private-LASSO-Regularized-Logistic-Regression-via-Faster-Frank-Wolfe-Iterations" class="headerlink" title="Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations"></a>Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19978">http://arxiv.org/abs/2310.19978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Raff, Amol Khanna, Fred Lu</li>
<li>for: 这个论文是为了提出一种可以在单簇输入数据上训练具有数据隐私的回归模型。</li>
<li>methods: 这个论文使用了Frank-Wolfe算法，并将其修改以适应单簇输入数据，并且使用了这些单簇输入来优化算法的训练时间。</li>
<li>results: 这个论文的结果显示，使用这种方法可以将训练时间从 $\mathcal{O}( T D S + T N S)$ 降至 $\mathcal{O}(N S + T \sqrt{D} \log{D} + T S^2)$，具体取决于隐私参数 $\epsilon$ 和资料集的绝对簇率 $S$。这个结果显示了这种方法可以将训练时间降低到最多 $2,200\times$。<details>
<summary>Abstract</summary>
To the best of our knowledge, there are no methods today for training differentially private regression models on sparse input data. To remedy this, we adapt the Frank-Wolfe algorithm for $L_1$ penalized linear regression to be aware of sparse inputs and to use them effectively. In doing so, we reduce the training time of the algorithm from $\mathcal{O}( T D S + T N S)$ to $\mathcal{O}(N S + T \sqrt{D} \log{D} + T S^2)$, where $T$ is the number of iterations and a sparsity rate $S$ of a dataset with $N$ rows and $D$ features. Our results demonstrate that this procedure can reduce runtime by a factor of up to $2,200\times$, depending on the value of the privacy parameter $\epsilon$ and the sparsity of the dataset.
</details>
<details>
<summary>摘要</summary>
“根据我们所知，现在没有训练具有零入力数据的差异化公式 regression 模型的方法。为了解决这个问题，我们将 Frank-Wolfe 算法 для $L_1$ 责任 linear regression 修改来考虑 sparse input，并将其使用有效地。这将training时间从 $\mathcal{O}(TD+TNS)$ 降至 $\mathcal{O}(NS+T\sqrt{D}\log{D}+TS^2)$，其中 $T$ 是迭代次数，$S$ 是资料集中的简洁率，$N$ 是资料集中的行数，$D$ 是资料集中的特征数。我们的结果显示，这个程序可以将runtime降低到最多 $2,200\times$，具体取决于隐私参数 $\epsilon$ 和资料集的简洁率。”Note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Unified-Enhancement-of-Privacy-Bounds-for-Mixture-Mechanisms-via-f-Differential-Privacy"><a href="#Unified-Enhancement-of-Privacy-Bounds-for-Mixture-Mechanisms-via-f-Differential-Privacy" class="headerlink" title="Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy"></a>Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19973">http://arxiv.org/abs/2310.19973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chendi Wang, Buxin Su, Jiayuan Ye, Reza Shokri, Weijie J. Su</li>
<li>for: This paper focuses on improving privacy bounds for shuffling models and one-iteration differentially private gradient descent (DP-GD) with random initializations using $f$-DP.</li>
<li>methods: The paper derives a closed-form expression of the trade-off function for shuffling models and investigates the effects of random initialization on the privacy of one-iteration DP-GD.</li>
<li>results: The paper shows that random initialization can enhance the privacy of DP-GD and derives a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on $(\epsilon,\delta)$-DP.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文关注于改进洗牌模型和一轮权值私有化梯度下降（DP-GD）的随机初始化影响私有性保证。</li>
<li>methods: 论文 derive了洗牌模型的减少函数的封闭式表述，并 investigate了随机初始化对DP-GD的私有性影响。</li>
<li>results: 论文显示随机初始化可以增强DP-GD的私有性，并 derive了洗牌模型的减少函数封闭式表述，超过当前最佳的($\epsilon$,$\delta$)-DP结果。<details>
<summary>Abstract</summary>
Differentially private (DP) machine learning algorithms incur many sources of randomness, such as random initialization, random batch subsampling, and shuffling. However, such randomness is difficult to take into account when proving differential privacy bounds because it induces mixture distributions for the algorithm's output that are difficult to analyze. This paper focuses on improving privacy bounds for shuffling models and one-iteration differentially private gradient descent (DP-GD) with random initializations using $f$-DP. We derive a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on $(\epsilon,\delta)$-DP. Moreover, we investigate the effects of random initialization on the privacy of one-iteration DP-GD. Our numerical computations of the trade-off function indicate that random initialization can enhance the privacy of DP-GD. Our analysis of $f$-DP guarantees for these mixture mechanisms relies on an inequality for trade-off functions introduced in this paper. This inequality implies the joint convexity of $F$-divergences. Finally, we study an $f$-DP analog of the advanced joint convexity of the hockey-stick divergence related to $(\epsilon,\delta)$-DP and apply it to analyze the privacy of mixture mechanisms.
</details>
<details>
<summary>摘要</summary>
Diffitionally private（DP）机器学习算法中存在多种Randomness，如随机初始化，随机批处理抽样和排序。然而，这些随机性很难在证明批处理保密约束时进行考虑，因为它们会导致算法输出的混合分布，难以分析。这篇论文关注于提高洗混模型和一轮批处理批处理批处理批处理批处理的隐私约束，使用$f$-DP。我们得到了洗混模型的封闭形式的贸易函数表达，超过目前最佳的($\epsilon$, $\delta$)-DP结果。此外，我们还研究了随机初始化对DP-GD的隐私的影响。我们的数值计算表明，随机初始化可以增强DP-GD的隐私。我们的分析表明，$f$-DP保证对这些混合机制的隐私性具有较好的性质。 finally，我们研究了$f$-DP中的进阶共轭性，并应用其分析洗混机制的隐私性。Note: Simplified Chinese is also known as "Mandarin" Chinese, and it is the official language of the People's Republic of China. It is written using the same characters as Traditional Chinese, but with simpler stroke order and fewer characters.
</details></li>
</ul>
<hr>
<h2 id="Early-detection-of-inflammatory-arthritis-to-improve-referrals-using-multimodal-machine-learning-from-blood-testing-semi-structured-and-unstructured-patient-records"><a href="#Early-detection-of-inflammatory-arthritis-to-improve-referrals-using-multimodal-machine-learning-from-blood-testing-semi-structured-and-unstructured-patient-records" class="headerlink" title="Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records"></a>Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19967">http://arxiv.org/abs/2310.19967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bing Wang, Weizi Li, Anthony Bradlow, Antoni T. Y. Chan, Eghosa Bazuaye</li>
<li>for: 早期识别急性关节炎 (IA) 是预测病人能够在有限的医疗资源下得到及时治疗和避免疾病诊断的关键。</li>
<li>methods: 我们使用多模态数据 fusion 和 ensemble learning 技术来支持早期识别IA。</li>
<li>results: 我们的研究表明，使用多模态数据可以提高早期识别IA的精度和效果，并且可以帮助医生更快地做出诊断。<details>
<summary>Abstract</summary>
Early detection of inflammatory arthritis (IA) is critical to efficient and accurate hospital referral triage for timely treatment and preventing the deterioration of the IA disease course, especially under limited healthcare resources. The manual assessment process is the most common approach in practice for the early detection of IA, but it is extremely labor-intensive and inefficient. A large amount of clinical information needs to be assessed for every referral from General Practice (GP) to the hospitals. Machine learning shows great potential in automating repetitive assessment tasks and providing decision support for the early detection of IA. However, most machine learning-based methods for IA detection rely on blood testing results. But in practice, blood testing data is not always available at the point of referrals, so we need methods to leverage multimodal data such as semi-structured and unstructured data for early detection of IA. In this research, we present fusion and ensemble learning-based methods using multimodal data to assist decision-making in the early detection of IA. To the best of our knowledge, our study is the first attempt to utilize multimodal data to support the early detection of IA from GP referrals.
</details>
<details>
<summary>摘要</summary>
早期检测Inflammatory Arthritis（IA）是关键，以实现有效和准确的医疗机构推荐，以及防止IA疾病趋势加剧，特别是在医疗资源有限的情况下。现在，手动评估是在实践中最常见的方法，但它具有很大的劳动成本和不fficient。每次从普通医生（GP） referring 到医院，需要评估大量的临床信息。机器学习表明了很大的潜力，可以自动进行重复的评估任务，并为IA早期检测提供决策支持。然而，大多数机器学习基于IA检测方法都依赖于血液测试结果。但在实践中，血液测试数据不总是在提交病人时可以获得，因此我们需要使用多modal数据来支持IA早期检测。在这项研究中，我们提出了 fusione和ensemble学习方法，使用多modal数据来帮助决策。到我们所知，这是第一次使用多modal数据来支持IA早期检测。
</details></li>
</ul>
<hr>
<h2 id="Topological-Learning-for-Motion-Data-via-Mixed-Coordinates"><a href="#Topological-Learning-for-Motion-Data-via-Mixed-Coordinates" class="headerlink" title="Topological Learning for Motion Data via Mixed Coordinates"></a>Topological Learning for Motion Data via Mixed Coordinates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19960">http://arxiv.org/abs/2310.19960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrluo/topologicalmotionseries">https://github.com/hrluo/topologicalmotionseries</a></li>
<li>paper_authors: Hengrui Luo, Jisu Kim, Alice Patania, Mikael Vejdemo-Johansson</li>
<li>for: 本研究旨在吸收批处理数据中的结构信息，并在转移学习中使用多输出 Gaussian process 模型。</li>
<li>methods: 作者提出了一种基于混合类别坐标的新框架，以处理时间序列中的直线趋势。同时，他们还使用 topological induced clustering 构建一个基于群集的函数kernel，以便更好地学习多个时间序列。</li>
<li>results: 研究人员通过实验表明，使用这种方法可以更好地捕捉时间序列的结构信息，并且在转移学习中提高模型的性能。<details>
<summary>Abstract</summary>
Topology can extract the structural information in a dataset efficiently. In this paper, we attempt to incorporate topological information into a multiple output Gaussian process model for transfer learning purposes. To achieve this goal, we extend the framework of circular coordinates into a novel framework of mixed valued coordinates to take linear trends in the time series into consideration.   One of the major challenges to learn from multiple time series effectively via a multiple output Gaussian process model is constructing a functional kernel. We propose to use topologically induced clustering to construct a cluster based kernel in a multiple output Gaussian process model. This kernel not only incorporates the topological structural information, but also allows us to put forward a unified framework using topological information in time and motion series.
</details>
<details>
<summary>摘要</summary>
One of the challenges to learn from multiple time series effectively via a multiple output Gaussian process model is constructing a functional kernel. We propose using topologically induced clustering to construct a cluster-based kernel in the model. This kernel not only incorporates topological structural information but also allows us to present a unified framework using topological information in time and motion series.
</details></li>
</ul>
<hr>
<h2 id="PriPrune-Quantifying-and-Preserving-Privacy-in-Pruned-Federated-Learning"><a href="#PriPrune-Quantifying-and-Preserving-Privacy-in-Pruned-Federated-Learning" class="headerlink" title="PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning"></a>PriPrune: Quantifying and Preserving Privacy in Pruned Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19958">http://arxiv.org/abs/2310.19958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyue Chu, Mengwei Yang, Nikolaos Laoutaris, Athina Markopoulou</li>
<li>for: 这个论文旨在 investigate  federated learning (FL) 中的模型缩减技术对隐私保护的影响。</li>
<li>methods: 本论文使用 information-theoretic upper bounds 和 comprehensive experiments 来研究模型缩减对隐私保护的影响。</li>
<li>results: 研究结果表明，模型缩减可以提供一定的隐私保护，但是不同的缩减策略和参数可能会影响这种保护的效果。在这基础之上， authors 提出了一种名为 PriPrune 的隐私意识 algorithm，可以在 client 端应用于任何缩减 FL 算法，并在尝试隐私攻击时提供了更好的隐私保护。<details>
<summary>Abstract</summary>
Federated learning (FL) is a paradigm that allows several client devices and a server to collaboratively train a global model, by exchanging only model updates, without the devices sharing their local training data. These devices are often constrained in terms of communication and computation resources, and can further benefit from model pruning -- a paradigm that is widely used to reduce the size and complexity of models. Intuitively, by making local models coarser, pruning is expected to also provide some protection against privacy attacks in the context of FL. However this protection has not been previously characterized, formally or experimentally, and it is unclear if it is sufficient against state-of-the-art attacks.   In this paper, we perform the first investigation of privacy guarantees for model pruning in FL. We derive information-theoretic upper bounds on the amount of information leaked by pruned FL models. We complement and validate these theoretical findings, with comprehensive experiments that involve state-of-the-art privacy attacks, on several state-of-the-art FL pruning schemes, using benchmark datasets. This evaluation provides valuable insights into the choices and parameters that can affect the privacy protection provided by pruning. Based on these insights, we introduce PriPrune -- a privacy-aware algorithm for local model pruning, which uses a personalized per-client defense mask and adapts the defense pruning rate so as to jointly optimize privacy and model performance. PriPrune is universal in that can be applied after any pruned FL scheme on the client, without modification, and protects against any inversion attack by the server. Our empirical evaluation demonstrates that PriPrune significantly improves the privacy-accuracy tradeoff compared to state-of-the-art pruned FL schemes that do not take privacy into account.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种 paradigm，allowing several client devices and a server to collaboratively train a global model, by exchanging only model updates, without the devices sharing their local training data. These devices are often constrained in terms of communication and computation resources, and can further benefit from model pruning -- a paradigm that is widely used to reduce the size and complexity of models. Intuitively, by making local models coarser, pruning is expected to also provide some protection against privacy attacks in the context of FL. However, this protection has not been previously characterized, formally or experimentally, and it is unclear if it is sufficient against state-of-the-art attacks. In this paper, we perform the first investigation of privacy guarantees for model pruning in FL. We derive information-theoretic upper bounds on the amount of information leaked by pruned FL models. We complement and validate these theoretical findings with comprehensive experiments that involve state-of-the-art privacy attacks, on several state-of-the-art FL pruning schemes, using benchmark datasets. This evaluation provides valuable insights into the choices and parameters that can affect the privacy protection provided by pruning. Based on these insights, we introduce PriPrune -- a privacy-aware algorithm for local model pruning, which uses a personalized per-client defense mask and adapts the defense pruning rate so as to jointly optimize privacy and model performance. PriPrune is universal in that it can be applied after any pruned FL scheme on the client, without modification, and protects against any inversion attack by the server. Our empirical evaluation demonstrates that PriPrune significantly improves the privacy-accuracy tradeoff compared to state-of-the-art pruned FL schemes that do not take privacy into account.
</details></li>
</ul>
<hr>
<h2 id="The-Acquisition-of-Physical-Knowledge-in-Generative-Neural-Networks"><a href="#The-Acquisition-of-Physical-Knowledge-in-Generative-Neural-Networks" class="headerlink" title="The Acquisition of Physical Knowledge in Generative Neural Networks"></a>The Acquisition of Physical Knowledge in Generative Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19943">http://arxiv.org/abs/2310.19943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cross32768/PlaNet_PyTorch">https://github.com/cross32768/PlaNet_PyTorch</a></li>
<li>paper_authors: Luca M. Schulze Buschoff, Eric Schulz, Marcel Binz</li>
<li>for:  investigate how the learning trajectories of deep generative neural networks compare to children’s developmental trajectories using physical understanding as a testbed</li>
<li>methods:  using physical understanding as a testbed, examine two distinct hypotheses of human development - stochastic optimization and complexity increase</li>
<li>results:  find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.<details>
<summary>Abstract</summary>
As children grow older, they develop an intuitive understanding of the physical processes around them. Their physical understanding develops in stages, moving along developmental trajectories which have been mapped out extensively in previous empirical research. Here, we investigate how the learning trajectories of deep generative neural networks compare to children's developmental trajectories using physical understanding as a testbed. We outline an approach that allows us to examine two distinct hypotheses of human development - stochastic optimization and complexity increase. We find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.
</details>
<details>
<summary>摘要</summary>
As children grow older, they develop an intuitive understanding of the physical processes around them. Their physical understanding develops in stages, moving along developmental trajectories which have been mapped out extensively in previous empirical research. Here, we investigate how the learning trajectories of deep generative neural networks compare to children's developmental trajectories using physical understanding as a testbed. We outline an approach that allows us to examine two distinct hypotheses of human development - stochastic optimization and complexity increase. We find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.Here's the translation in Traditional Chinese: As children grow older, they develop an intuitive understanding of the physical processes around them. Their physical understanding develops in stages, moving along developmental trajectories which have been mapped out extensively in previous empirical research. Here, we investigate how the learning trajectories of deep generative neural networks compare to children's developmental trajectories using physical understanding as a testbed. We outline an approach that allows us to examine two distinct hypotheses of human development - stochastic optimization and complexity increase. We find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.
</details></li>
</ul>
<hr>
<h2 id="Lyapunov-Based-Dropout-Deep-Neural-Network-Lb-DDNN-Controller"><a href="#Lyapunov-Based-Dropout-Deep-Neural-Network-Lb-DDNN-Controller" class="headerlink" title="Lyapunov-Based Dropout Deep Neural Network (Lb-DDNN) Controller"></a>Lyapunov-Based Dropout Deep Neural Network (Lb-DDNN) Controller</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19938">http://arxiv.org/abs/2310.19938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saiedeh Akbari, Emily J. Griffis, Omkar Sudhir Patil, Warren E. Dixon</li>
<li>for: 论文目的是提出一种基于深度神经网络（DNN）的适应控制器，用于资料处理系统中的不结构化不确定性补做。</li>
<li>methods: 论文使用了Dropout正则化技术，在训练过程中随机禁用节点，以避免过拟合和共适应问题。同时，文章提出了基于Lyapunov函数的实时权重更新方法，以进行在线不监督学习。</li>
<li>results:  simulations 表明，与基线控制器相比，提出的Dropout DNN-based适应控制器可以提高跟踪错误率38.32%，函数适应错误率53.67%，控制努力50.44%。<details>
<summary>Abstract</summary>
Deep neural network (DNN)-based adaptive controllers can be used to compensate for unstructured uncertainties in nonlinear dynamic systems. However, DNNs are also very susceptible to overfitting and co-adaptation. Dropout regularization is an approach where nodes are randomly dropped during training to alleviate issues such as overfitting and co-adaptation. In this paper, a dropout DNN-based adaptive controller is developed. The developed dropout technique allows the deactivation of weights that are stochastically selected for each individual layer within the DNN. Simultaneously, a Lyapunov-based real-time weight adaptation law is introduced to update the weights of all layers of the DNN for online unsupervised learning. A non-smooth Lyapunov-based stability analysis is performed to ensure asymptotic convergence of the tracking error. Simulation results of the developed dropout DNN-based adaptive controller indicate a 38.32% improvement in the tracking error, a 53.67% improvement in the function approximation error, and 50.44% lower control effort when compared to a baseline adaptive DNN-based controller without dropout regularization.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)深度神经网络（DNN）基于的自适应控制器可以补偿非结构性不确定性在非线性动态系统中。然而，DNN也很易于过拟合和共适应。Dropout常量化是一种approach，其中在训练过程中随机dropout节点以解决过拟合和共适应问题。在这篇论文中，我们开发了dropout DNN基于的自适应控制器。我们的dropout技术允许每个层内的权重随机停用。同时，我们引入了基于Lyapunov的实时重量更新法，以更新DNN所有层的重量进行在线无监督学习。我们通过非稀 Ligado-based稳定分析，确保追踪错误的极限收敛。实验结果表明，与基线 adaptive DNN基于控制器无dropout常量化相比，我们的dropout DNN基于自适应控制器可以提高追踪错误38.32%，功能近似错误53.67%，控制努力50.44%。
</details></li>
</ul>
<hr>
<h2 id="Sim2Real-for-Environmental-Neural-Processes"><a href="#Sim2Real-for-Environmental-Neural-Processes" class="headerlink" title="Sim2Real for Environmental Neural Processes"></a>Sim2Real for Environmental Neural Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19932">http://arxiv.org/abs/2310.19932</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jonas-scholz123/sim2real-downscaling">https://github.com/jonas-scholz123/sim2real-downscaling</a></li>
<li>paper_authors: Jonas Scholz, Tom R. Andersson, Anna Vaughan, James Requeima, Richard E. Turner</li>
<li>for: 本研究旨在提高天气预测和气候监测的准确性，通过使用机器学习（ML）模型来利用观测数据。</li>
<li>methods: 本研究使用了一种名为ConvCNP的 convolutional conditional neural process（ConvCNP）模型，该模型可以conditioning on both gridded和off-the-grid context data来做uncertainty-aware预测。</li>
<li>results: 研究发现，通过使用Sim2Real方法（pre-training on reanalysis和 fine-tuning on observational data），可以substantially improve Surface air temperature interpolation performance over Germany， compared to only using reanalysis data or only using station data。<details>
<summary>Abstract</summary>
Machine learning (ML)-based weather models have recently undergone rapid improvements. These models are typically trained on gridded reanalysis data from numerical data assimilation systems. However, reanalysis data comes with limitations, such as assumptions about physical laws and low spatiotemporal resolution. The gap between reanalysis and reality has sparked growing interest in training ML models directly on observations such as weather stations. Modelling scattered and sparse environmental observations requires scalable and flexible ML architectures, one of which is the convolutional conditional neural process (ConvCNP). ConvCNPs can learn to condition on both gridded and off-the-grid context data to make uncertainty-aware predictions at target locations. However, the sparsity of real observations presents a challenge for data-hungry deep learning models like the ConvCNP. One potential solution is 'Sim2Real': pre-training on reanalysis and fine-tuning on observational data. We analyse Sim2Real with a ConvCNP trained to interpolate surface air temperature over Germany, using varying numbers of weather stations for fine-tuning. On held-out weather stations, Sim2Real training substantially outperforms the same model architecture trained only with reanalysis data or only with station data, showing that reanalysis data can serve as a stepping stone for learning from real observations. Sim2Real could thus enable more accurate models for weather prediction and climate monitoring.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）基于天气模型最近几年发展迅速。这些模型通常通过数值数据吸收系统获得格子化再分析数据训练。然而，再分析数据存在一些限制，如物理法律假设和低空时间分辨率。这导致了使用直接训练ML模型于天气站数据的增加兴趣。模型散布和罕见环境观测数据需要扩展和灵活的ML体系结构，其中之一是卷积条件神经过程（ConvCNP）。ConvCNP可以通过对格子和非格子上下文数据进行条件学习来预测目标位置的不确定性。然而，实际观测数据的稀缺性对深度学习模型如ConvCNP进行挑战。一个可能的解决方案是“Sim2Real”：先在再分析数据上进行预训练，然后在观测数据上进行微调。我们对德国的表面温度 interpolating 使用了一个ConvCNP，并使用不同数量的天气站进行微调。在保留的天气站上，Sim2Real 训练显著超过了同样的模型结构只使用再分析数据或只使用站数据进行训练，这表明了再分析数据可以作为学习实际观测数据的步骤。Sim2Real 可能可以实现更准确的天气预测和气候监测。
</details></li>
</ul>
<hr>
<h2 id="Solving-a-Class-of-Cut-Generating-Linear-Programs-via-Machine-Learning"><a href="#Solving-a-Class-of-Cut-Generating-Linear-Programs-via-Machine-Learning" class="headerlink" title="Solving a Class of Cut-Generating Linear Programs via Machine Learning"></a>Solving a Class of Cut-Generating Linear Programs via Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19920">http://arxiv.org/abs/2310.19920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atefeh Rajabalizadeh, Danial Davarnia</li>
<li>for: 提高杂 integer  программирования的解法</li>
<li>methods: 使用机器学习 Approximate CGLP 来选择加入分支和约束树的优秀节点</li>
<li>results: 使用该方法可以提高解法的效率和精度，并且可以应用到大量的节点中<details>
<summary>Abstract</summary>
Cut-generating linear programs (CGLPs) play a key role as a separation oracle to produce valid inequalities for the feasible region of mixed-integer programs. When incorporated inside branch-and-bound, the cutting planes obtained from CGLPs help to tighten relaxations and improve dual bounds. However, running the CGLPs at the nodes of the branch-and-bound tree is computationally cumbersome due to the large number of node candidates and the lack of a priori knowledge on which nodes admit useful cutting planes. As a result, CGLPs are often avoided at default settings of branch-and-cut algorithms despite their potential impact on improving dual bounds. In this paper, we propose a novel framework based on machine learning to approximate the optimal value of a CGLP class that determines whether a cutting plane can be generated at a node of the branch-and-bound tree. Translating the CGLP as an indicator function of the objective function vector, we show that it can be approximated through conventional data classification techniques. We provide a systematic procedure to efficiently generate training data sets for the corresponding classification problem based on the CGLP structure. We conduct computational experiments on benchmark instances using classification methods such as logistic regression. These results suggest that the approximate CGLP obtained from classification can improve the solution time compared to that of conventional cutting plane methods. Our proposed framework can be efficiently applied to a large number of nodes in the branch-and-bound tree to identify the best candidates for adding a cut.
</details>
<details>
<summary>摘要</summary>
刻生成线性程序（CGLP）在杂integer程序的分解 oracle 中发挥关键作用，生成有效的不等式来缩小relaxation 和提高对偶下界。然而，在branch-and-bound 树中运行 CGLP  computationally cumbersome due to the large number of node candidates and the lack of a priori knowledge on which nodes admit useful cutting planes。因此，CGLP  often avoided at default settings of branch-and-cut algorithms despite their potential impact on improving dual bounds。在这篇论文中，我们提出了一种基于机器学习的新框架，用于approximate CGLP 的优化值。我们将 CGLP 表示为目标函数 вектор 的指示函数，并示出可以通过 convential data classification techniques 来approximate it。我们还提供了一种系统的进程来生成相关的训练数据集，以便应用 classification 方法来解决相关的分类问题。我们在 benchmark instances 上进行了计算实验，使用 logistic regression 等分类方法，结果表明，使用我们的框架可以提高解决时间。我们的提posed framework可以高效地应用到 branch-and-bound 树中的大量节点，以确定最佳的加法点。
</details></li>
</ul>
<hr>
<h2 id="Meta-Learning-Strategies-through-Value-Maximization-in-Neural-Networks"><a href="#Meta-Learning-Strategies-through-Value-Maximization-in-Neural-Networks" class="headerlink" title="Meta-Learning Strategies through Value Maximization in Neural Networks"></a>Meta-Learning Strategies through Value Maximization in Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19919">http://arxiv.org/abs/2310.19919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rodrigo Carrasco-Davis, Javier Masís, Andrew M. Saxe</li>
<li>for: 这个论文的目的是 investigate optimal strategies for learning control in deep networks, and provide a tractable theoretical test bed to study normative benefits of interventions in various learning systems.</li>
<li>methods: 这个论文使用了 average dynamical equations for gradient descent, 以及一种 learning effort framework capable of efficiently optimizing control signals on a fully normative objective: discounted cumulative performance throughout learning.</li>
<li>results: 研究发现，在不同的学习设置下，控制努力在早期学习 easier aspects of a task 是最有利的，然后坚持努力 harder aspects of the task. 此外，研究还发现了一些关于 optimal curricula 和 neuronal resource allocation 的结论。<details>
<summary>Abstract</summary>
Biological and artificial learning agents face numerous choices about how to learn, ranging from hyperparameter selection to aspects of task distributions like curricula. Understanding how to make these meta-learning choices could offer normative accounts of cognitive control functions in biological learners and improve engineered systems. Yet optimal strategies remain challenging to compute in modern deep networks due to the complexity of optimizing through the entire learning process. Here we theoretically investigate optimal strategies in a tractable setting. We present a learning effort framework capable of efficiently optimizing control signals on a fully normative objective: discounted cumulative performance throughout learning. We obtain computational tractability by using average dynamical equations for gradient descent, available for simple neural network architectures. Our framework accommodates a range of meta-learning and automatic curriculum learning methods in a unified normative setting. We apply this framework to investigate the effect of approximations in common meta-learning algorithms; infer aspects of optimal curricula; and compute optimal neuronal resource allocation in a continual learning setting. Across settings, we find that control effort is most beneficial when applied to easier aspects of a task early in learning; followed by sustained effort on harder aspects. Overall, the learning effort framework provides a tractable theoretical test bed to study normative benefits of interventions in a variety of learning systems, as well as a formal account of optimal cognitive control strategies over learning trajectories posited by established theories in cognitive neuroscience.
</details>
<details>
<summary>摘要</summary>
In this study, we investigate optimal strategies in a tractable setting. We develop a learning effort framework that can efficiently optimize control signals based on a fully normative objective: discounted cumulative performance throughout learning. We use average dynamical equations for gradient descent, which are available for simple neural network architectures, to achieve computational tractability. Our framework accommodates a range of meta-learning and automatic curriculum learning methods in a unified normative setting.We apply our framework to investigate the effect of approximations in common meta-learning algorithms, infer aspects of optimal curricula, and compute optimal neuronal resource allocation in a continual learning setting. Our results show that control effort is most beneficial when applied to easier aspects of a task early in learning, followed by sustained effort on harder aspects.Overall, our learning effort framework provides a tractable theoretical test bed to study normative benefits of interventions in a variety of learning systems, as well as a formal account of optimal cognitive control strategies over learning trajectories as posited by established theories in cognitive neuroscience.
</details></li>
</ul>
<hr>
<h2 id="GPCR-BERT-Interpreting-Sequential-Design-of-G-Protein-Coupled-Receptors-Using-Protein-Language-Models"><a href="#GPCR-BERT-Interpreting-Sequential-Design-of-G-Protein-Coupled-Receptors-Using-Protein-Language-Models" class="headerlink" title="GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models"></a>GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19915">http://arxiv.org/abs/2310.19915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seongwon Kim, Parisa Mollaei, Akshay Antony, Rishikesh Magar, Amir Barati Farimani</li>
<li>for: 本研究旨在开发一种基于Transformers和大语言模型（LLMs）的G Protein-Coupled Receptors（GPCRs）模型，以便更好地理解GPCRs的Sequential设计。</li>
<li>methods: 本研究使用了 pré-trained protein模型（Prot-Bert），并通过对变化的掩码任务进行精度调整，以提高预测隐藏的残基的精度。此外，还利用了模型的注意力权重和隐藏状态，以EXTRACT隐藏的残基的贡献。</li>
<li>results: 研究发现，通过对模型的预测结果进行分析，可以了解GPCRs的靶点残基与掩码的关系，以及一些保守的残基组合（如NPxxY、CWxP、E&#x2F;DRY）的作用。此外，还可以利用3D结构分析来描述GPCRs的高级相互作用。<details>
<summary>Abstract</summary>
With the rise of Transformers and Large Language Models (LLMs) in Chemistry and Biology, new avenues for the design and understanding of therapeutics have opened up to the scientific community. Protein sequences can be modeled as language and can take advantage of recent advances in LLMs, specifically with the abundance of our access to the protein sequence datasets. In this paper, we developed the GPCR-BERT model for understanding the sequential design of G Protein-Coupled Receptors (GPCRs). GPCRs are the target of over one-third of FDA-approved pharmaceuticals. However, there is a lack of comprehensive understanding regarding the relationship between amino acid sequence, ligand selectivity, and conformational motifs (such as NPxxY, CWxP, E/DRY). By utilizing the pre-trained protein model (Prot-Bert) and fine-tuning with prediction tasks of variations in the motifs, we were able to shed light on several relationships between residues in the binding pocket and some of the conserved motifs. To achieve this, we took advantage of attention weights, and hidden states of the model that are interpreted to extract the extent of contributions of amino acids in dictating the type of masked ones. The fine-tuned models demonstrated high accuracy in predicting hidden residues within the motifs. In addition, the analysis of embedding was performed over 3D structures to elucidate the higher-order interactions within the conformations of the receptors.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:随着Transformers和Large Language Models（LLMs）在化学和生物领域的普及，新的 Avenues for the design and understanding of therapeutics 已经开放给科学社区。蛋白序列可以被视为语言，可以利用最近的LLMs的进步，特别是我们对蛋白序列数据的丰富访问。在这篇论文中，我们开发了GPCR-BERT模型，用于理解G蛋白coupled receptors（GPCRs）的序列设计。GPCRs 是FDA批准药品的目标之一，但是还没有充分了解蛋白序列、 Ligand selectivity 和 conformational motifs（如NPxxY、CWxP、E/DRY）之间的关系。我们利用预训练的蛋白模型（Prot-Bert）和预测任务的变化来 fins-tune GPCRs 的序列设计。通过利用模型的注意力权重和隐藏状态，我们可以提取蛋白结构中各个残基的贡献程度，以及它们如何决定遮盖的残基。 fine-tuned 模型在预测隐藏在抑制motifs中的残基上达到了高精度。此外，我们还使用了三维结构的分析，以描述GPCRs 的高级相互作用。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Simulation-based-Inference-for-Cosmological-Initial-Conditions"><a href="#Bayesian-Simulation-based-Inference-for-Cosmological-Initial-Conditions" class="headerlink" title="Bayesian Simulation-based Inference for Cosmological Initial Conditions"></a>Bayesian Simulation-based Inference for Cosmological Initial Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19910">http://arxiv.org/abs/2310.19910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian List, Noemi Anau Montel, Christoph Weniger</li>
<li>for: 用于重construct astrophysical和cosmological场的观测数据</li>
<li>methods: 使用 Bayesian场重建算法，基于模拟基于推理和自适应模型</li>
<li>results: 在一个证明性应用中，成功地 recovered cosmological initial conditions from late-time density fields<details>
<summary>Abstract</summary>
Reconstructing astrophysical and cosmological fields from observations is challenging. It requires accounting for non-linear transformations, mixing of spatial structure, and noise. In contrast, forward simulators that map fields to observations are readily available for many applications. We present a versatile Bayesian field reconstruction algorithm rooted in simulation-based inference and enhanced by autoregressive modeling. The proposed technique is applicable to generic (non-differentiable) forward simulators and allows sampling from the posterior for the underlying field. We show first promising results on a proof-of-concept application: the recovery of cosmological initial conditions from late-time density fields.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将astro物理和cosmology领域中的场景重建为观测数据是一项复杂的任务。这需要考虑非线性变换、空间结构的混合以及噪声。相比之下，向前的模拟器可以快速地将场景映射到观测数据上。我们介绍了一种可靠的bayesian场景重建算法，基于 simulations-based inference 和自动关联模型。该算法适用于通用（非�ifferentiable）向前模拟器，并允许采样 posterior 中的下面场景。我们在一个证明性应用中表明了这种方法的首次成果： cosmological initial condition 的恢复从 late-time density fields。Note: The translation is done using a machine translation tool, and may not be perfect or idiomatic. Please let me know if you have any further questions or requests.
</details></li>
</ul>
<hr>
<h2 id="BTRec-BERT-Based-Trajectory-Recommendation-for-Personalized-Tours"><a href="#BTRec-BERT-Based-Trajectory-Recommendation-for-Personalized-Tours" class="headerlink" title="BTRec: BERT-Based Trajectory Recommendation for Personalized Tours"></a>BTRec: BERT-Based Trajectory Recommendation for Personalized Tours</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19886">http://arxiv.org/abs/2310.19886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nxh912/BTRec_RecSys23">https://github.com/nxh912/BTRec_RecSys23</a></li>
<li>paper_authors: Ngai Lam Ho, Roy Ka-Wei Lee, Kwan Hui Lim</li>
<li>For: 提供个性化旅游路线建议，使旅游者在不熟悉的城市中享有愉悦的旅行体验。* Methods: 使用BERT框架，结合用户个人信息和历史 POI 访问记录，提出个性化 POI 访问预测算法 BTREC。* Results: 对八座不同规模的城市的实验结果表明，提posed算法稳定性高，与多种序列预测算法相比， measured by recall, precision, and F1-scores 表现出色。<details>
<summary>Abstract</summary>
An essential task for tourists having a pleasant holiday is to have a well-planned itinerary with relevant recommendations, especially when visiting unfamiliar cities. Many tour recommendation tools only take into account a limited number of factors, such as popular Points of Interest (POIs) and routing constraints. Consequently, the solutions they provide may not always align with the individual users of the system. We propose an iterative algorithm in this paper, namely: BTREC (BERT-based Trajectory Recommendation), that extends from the POIBERT embedding algorithm to recommend personalized itineraries on POIs using the BERT framework. Our BTREC algorithm incorporates users' demographic information alongside past POI visits into a modified BERT language model to recommend a personalized POI itinerary prediction given a pair of source and destination POIs. Our recommendation system can create a travel itinerary that maximizes POIs visited, while also taking into account user preferences for categories of POIs and time availability. Our recommendation algorithm is largely inspired by the problem of sentence completion in natural language processing (NLP). Using a dataset of eight cities of different sizes, our experimental results demonstrate that our proposed algorithm is stable and outperforms many other sequence prediction algorithms, measured by recall, precision, and F1-scores.
</details>
<details>
<summary>摘要</summary>
旅游者有一项非常重要的任务，即制定一份合理的行程安排，尤其当访问不熟悉的城市时。许多旅游推荐工具只考虑有限数量的因素，如受欢迎的景点（POI）和路径约束。因此，它们提供的解决方案可能并不一定与个人用户匹配。我们在本文中提出了一种迭代算法，即BTREC（基于BERT的 trajectory推荐算法），它从POIBERT嵌入算法中扩展，为用户提供个性化的行程安排。我们的BTREC算法将用户的人口信息和过去访问的 POI 纳入修改后的 BERT 语言模型，以提供基于源和目的 POI 的个性化 POI 行程预测。我们的推荐系统可以创建一个 maximizes POIs 访问的旅游路线，同时也考虑用户对类型 POIs 和时间可用性的偏好。我们的推荐算法受到自然语言处理（NLP）中句子完成问题的启发，使用了八个不同规模的城市的 Dataset。我们的实验结果表明，我们的提议算法稳定和许多其他序列预测算法之上， measured by recall, precision 和 F1-scores。
</details></li>
</ul>
<hr>
<h2 id="Learning-quantum-states-and-unitaries-of-bounded-gate-complexity"><a href="#Learning-quantum-states-and-unitaries-of-bounded-gate-complexity" class="headerlink" title="Learning quantum states and unitaries of bounded gate complexity"></a>Learning quantum states and unitaries of bounded gate complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19882">http://arxiv.org/abs/2310.19882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haimeng Zhao, Laura Lewis, Ishaan Kannan, Yihui Quek, Hsin-Yuan Huang, Matthias C. Caro</li>
<li>for: 本文研究了量子状态批量学习的复杂性，特别是考虑到实际应用场景下的量子状态和单位里程计数的复杂性。</li>
<li>methods: 作者使用了数据采样和查询复杂度来研究量子状态和单位里程计数的学习复杂性。</li>
<li>results: 作者证明了在学习量子状态和单位里程计数时，样本复杂度必须线性增长，而查询复杂度可以达到小平均误差。此外，作者还证明了在某些理想的 крипτографических假设下，计算复杂性必须线性增长。这些结果解释了量子机器学习模型的表达能力和创建量子状态和单位里程计数的复杂性之间的关系。<details>
<summary>Abstract</summary>
While quantum state tomography is notoriously hard, most states hold little interest to practically-minded tomographers. Given that states and unitaries appearing in Nature are of bounded gate complexity, it is natural to ask if efficient learning becomes possible. In this work, we prove that to learn a state generated by a quantum circuit with $G$ two-qubit gates to a small trace distance, a sample complexity scaling linearly in $G$ is necessary and sufficient. We also prove that the optimal query complexity to learn a unitary generated by $G$ gates to a small average-case error scales linearly in $G$. While sample-efficient learning can be achieved, we show that under reasonable cryptographic conjectures, the computational complexity for learning states and unitaries of gate complexity $G$ must scale exponentially in $G$. We illustrate how these results establish fundamental limitations on the expressivity of quantum machine learning models and provide new perspectives on no-free-lunch theorems in unitary learning. Together, our results answer how the complexity of learning quantum states and unitaries relate to the complexity of creating these states and unitaries.
</details>
<details>
<summary>摘要</summary>
“量子状态批量检测是非常困难的，大多数状态对实用检测师来说无法吸引兴趣。因为自然界中的状态和单位里是有限门数的，那么我们可以问：是否存在高效的学习方法？在这项工作中，我们证明了为了通过量子电路中的$G$个二量子门来学习一个状态，需要一个样本复杂度 linearly 增长于 $G$。我们还证明了在平均错误下学习一个由 $G$ 个门组成的单位ри数学需要一个平均Query complexity linearly 增长于 $G$。虽然可以实现高效的学习，但我们表明了在合理的 криптографических假设下，计算复杂性 для学习状态和单位里的计算复杂性必须 exponentially 增长于 $G$。我们表明了这些结果是量子机器学习模型的基本限制，并提供了新的视角来解释无免答定理在单位学习中。这些结果回答了学习量子状态和单位里的复杂性与创造这些状态和单位里的复杂性之间的关系。”Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Metric-Flows-with-Neural-Networks"><a href="#Metric-Flows-with-Neural-Networks" class="headerlink" title="Metric Flows with Neural Networks"></a>Metric Flows with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19870">http://arxiv.org/abs/2310.19870</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teluashish/traffic-flow-volume-prediction">https://github.com/teluashish/traffic-flow-volume-prediction</a></li>
<li>paper_authors: James Halverson, Fabian Ruehle</li>
<li>for: 该论文旨在研究各种流体在里曼度量空间中的流动，以及这些流动如何与神经网络的梯度下降相关。</li>
<li>methods: 论文使用神经网络梯度下降引起的里曼度量空间中的流动理论，并 derive了相应的流动方程，其中包括一个复杂的、非本地的 metric neural tangent kernel。</li>
<li>results: 论文发现了一些架构在无限宽限下，该流动简化，并且可以通过特定的假设来引入本地性，使得流动可以实现 péri Mérel’s Ricci flow 表述，解决了三维凯茨玻射悖论。 另外，论文还应用了这些想法到数字Calabi-Yau度量中，包括一个关于特征学习的讨论。<details>
<summary>Abstract</summary>
We develop a theory of flows in the space of Riemannian metrics induced by neural network gradient descent. This is motivated in part by recent advances in approximating Calabi-Yau metrics with neural networks and is enabled by recent advances in understanding flows in the space of neural networks. We derive the corresponding metric flow equations, which are governed by a metric neural tangent kernel, a complicated, non-local object that evolves in time. However, many architectures admit an infinite-width limit in which the kernel becomes fixed and the dynamics simplify. Additional assumptions can induce locality in the flow, which allows for the realization of Perelman's formulation of Ricci flow that was used to resolve the 3d Poincar\'e conjecture. We apply these ideas to numerical Calabi-Yau metrics, including a discussion on the importance of feature learning.
</details>
<details>
<summary>摘要</summary>
我们开发了一种在里曼度量空间中的流理论，它是由神经网络梯度下降引起的。这是在近期用神经网络approximate Calabi-Yau度量的进步和理解流在神经网络空间中的进步的基础之上。我们 derivate了相应的流方程，它们是由一个metric neural tangent kernel控制的，这是一个复杂的、非本地的对象，它在时间演化。然而，许多架构允许无穷宽限制，在这种情况下，核心变得固定，动态简化。额外的假设可以使得流动具有本地性，使得可以实现Perelman的 Ricci流形式化，这是用于解决3D Poincaré conjecture的。我们应用这些想法来数字Calabi-Yau度量，包括一个关于特征学习的讨论。
</details></li>
</ul>
<hr>
<h2 id="Posterior-Sampling-for-Competitive-RL-Function-Approximation-and-Partial-Observation"><a href="#Posterior-Sampling-for-Competitive-RL-Function-Approximation-and-Partial-Observation" class="headerlink" title="Posterior Sampling for Competitive RL: Function Approximation and Partial Observation"></a>Posterior Sampling for Competitive RL: Function Approximation and Partial Observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19861">http://arxiv.org/abs/2310.19861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Qiu, Ziyu Dai, Han Zhong, Zhaoran Wang, Zhuoran Yang, Tong Zhang</li>
<li>for: 这个论文研究了激励学习（RL）中的后验抽样算法，特别是在通用函数近似中。</li>
<li>methods: 论文提出了自我玩家和反对敌人学习下的两种情况下的扩展通用杰伦评价因子（GEC）作为函数近似的复杂性度量，并基于自我玩家 GEC 提出了一种基于模型的自我玩家 posterior 抽样方法来控制两个玩家学习 Nash 平衡。</li>
<li>results: 论文提出了一种可以成功处理部分可见状态的模型基于 posterior 抽样方法，并在部分可见状态下学习反对敌人的模型。此外，论文还提出了一种基于反对敌人 GEC 的模型基于 posterior 抽样方法，可以在部分可见状态下学习反对敌人的模型。论文还提供了对 proposed 算法的低征退 bound，可以在 proposed GEC 和 episodes 数量 $T$ 上下降幂函数。<details>
<summary>Abstract</summary>
This paper investigates posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations. Focusing on zero-sum Markov games (MGs) under two critical settings, namely self-play and adversarial learning, we first propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, capturing the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle the partial observability of states. Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We further provide low regret bounds for proposed algorithms that can scale sublinearly with the proposed GEC and the number of episodes $T$. To the best of our knowledge, we for the first time develop generic model-based posterior sampling algorithms for competitive RL that can be applied to a majority of tractable zero-sum MG classes in both fully observable and partially observable MGs with self-play and adversarial learning.
</details>
<details>
<summary>摘要</summary>
First, we propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, which capture the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle partial observability of states.Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability.We provide low regret bounds for our proposed algorithms, which can scale sublinearly with the proposed GEC and the number of episodes $T$. To the best of our knowledge, we develop generic model-based posterior sampling algorithms for competitive RL that can be applied to a majority of tractable zero-sum MG classes in both fully observable and partially observable MGs with self-play and adversarial learning.
</details></li>
</ul>
<hr>
<h2 id="Robust-Causal-Bandits-for-Linear-Models"><a href="#Robust-Causal-Bandits-for-Linear-Models" class="headerlink" title="Robust Causal Bandits for Linear Models"></a>Robust Causal Bandits for Linear Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19794">http://arxiv.org/abs/2310.19794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zirui Yan, Arpan Mukherjee, Burak Varıcı, Ali Tajer</li>
<li>for: 这个论文是为了研究Sequential Design of Experiments（SDE）在 causal systems 中的优化问题。</li>
<li>methods: 这个论文使用了 causal bandits（CBs）模型，并研究了这些模型在 temporal model fluctuations 中的稳定性。</li>
<li>results: 论文表明，existin 的方法在 temporal model fluctuations 中不稳定，并且可能会导致 linear regret。而 proposed 算法可以在这些 fluctuations 中实现 nearly optimal 的 regret。<details>
<summary>Abstract</summary>
Sequential design of experiments for optimizing a reward function in causal systems can be effectively modeled by the sequential design of interventions in causal bandits (CBs). In the existing literature on CBs, a critical assumption is that the causal models remain constant over time. However, this assumption does not necessarily hold in complex systems, which constantly undergo temporal model fluctuations. This paper addresses the robustness of CBs to such model fluctuations. The focus is on causal systems with linear structural equation models (SEMs). The SEMs and the time-varying pre- and post-interventional statistical models are all unknown. Cumulative regret is adopted as the design criteria, based on which the objective is to design a sequence of interventions that incur the smallest cumulative regret with respect to an oracle aware of the entire causal model and its fluctuations. First, it is established that the existing approaches fail to maintain regret sub-linearity with even a few instances of model deviation. Specifically, when the number of instances with model deviation is as few as $T^\frac{1}{2L}$, where $T$ is the time horizon and $L$ is the longest causal path in the graph, the existing algorithms will have linear regret in $T$. Next, a robust CB algorithm is designed, and its regret is analyzed, where upper and information-theoretic lower bounds on the regret are established. Specifically, in a graph with $N$ nodes and maximum degree $d$, under a general measure of model deviation $C$, the cumulative regret is upper bounded by $\tilde{\mathcal{O}(d^{L-\frac{1}{2}(\sqrt{NT} + NC))$ and lower bounded by $\Omega(d^{\frac{L}{2}-2}\max\{\sqrt{T},d^2C\})$. Comparing these bounds establishes that the proposed algorithm achieves nearly optimal $\tilde{\mathcal{O}(\sqrt{T})$ regret when $C$ is $o(\sqrt{T})$ and maintains sub-linear regret for a broader range of $C$.
</details>
<details>
<summary>摘要</summary>
sequential设计实验可以有效地模型 causal系统中的奖励函数。在现有文献中，一个关键假设是 causal模型在时间上是相对定常的。然而，这个假设并不一定成立在复杂系统中，这些系统在时间上不断发生模型波动。这篇论文考虑了 causal系统中 linear structural equation models (SEMs) 的Robustness。我们的目标是设计一个序列 intervención，使其最小化对 oracle 所知道的整个 causal模型和其波动的 regret。首先，我们证明了现有的方法在模型偏移情况下无法保持 regret 线性。specifically，当模型偏移的数量为 $T^\frac{1}{2L}$，where $T$ 是时间框架和 $L$ 是最长 causal 路径，现有的算法将在 $T$ 时间框架内具有线性 regret。接着，我们设计了一种 Robust CB 算法，并分析了其 regret。我们设定了 upper 和 information-theoretic lower bounds on regret，其中，在一个 $N$ 节点、最大度为 $d$ 的图中，在一般测度 $C$ 下，累积 regret 上限为 $\tilde{\mathcal{O}(d^{L-\frac{1}{2}(\sqrt{NT} + NC))$，下限为 $\Omega(d^{\frac{L}{2}-2}\max\{\sqrt{T},d^2C\})$. Comparing these bounds shows that the proposed algorithm achieves nearly optimal $\tilde{\mathcal{O}(\sqrt{T})$ regret when $C$ is $o(\sqrt{T})$ and maintains sub-linear regret for a broader range of $C$.
</details></li>
</ul>
<hr>
<h2 id="On-Learning-Gaussian-Multi-index-Models-with-Gradient-Flow"><a href="#On-Learning-Gaussian-Multi-index-Models-with-Gradient-Flow" class="headerlink" title="On Learning Gaussian Multi-index Models with Gradient Flow"></a>On Learning Gaussian Multi-index Models with Gradient Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19793">http://arxiv.org/abs/2310.19793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Bietti, Joan Bruna, Loucas Pillaud-Vivien</li>
<li>for: 这个论文是关于高维 Gaussian 数据的多指量回归问题中的梯度流算法研究。</li>
<li>methods: 这种算法使用了两个时间尺度的方法，其中低维链函数使用非Parametric模型，而且Parametrizing低维投影的子空间 correlation matrices 的matrix semigroup structure进行了有效利用。</li>
<li>results: 研究结果表明，这种梯度流算法在Grassmannian population gradient flow dynamics中具有全球收敛性，并且可以对链函数进行有效的描述。另外，这种算法在 Plant 问题中的优化景观存在摇摆性，梯度流动可能会在高概率下被困。<details>
<summary>Abstract</summary>
We study gradient flow on the multi-index regression problem for high-dimensional Gaussian data. Multi-index functions consist of a composition of an unknown low-rank linear projection and an arbitrary unknown, low-dimensional link function. As such, they constitute a natural template for feature learning in neural networks.   We consider a two-timescale algorithm, whereby the low-dimensional link function is learnt with a non-parametric model infinitely faster than the subspace parametrizing the low-rank projection. By appropriately exploiting the matrix semigroup structure arising over the subspace correlation matrices, we establish global convergence of the resulting Grassmannian population gradient flow dynamics, and provide a quantitative description of its associated `saddle-to-saddle' dynamics. Notably, the timescales associated with each saddle can be explicitly characterized in terms of an appropriate Hermite decomposition of the target link function. In contrast with these positive results, we also show that the related \emph{planted} problem, where the link function is known and fixed, in fact has a rough optimization landscape, in which gradient flow dynamics might get trapped with high probability.
</details>
<details>
<summary>摘要</summary>
我们研究梯度流在多指标回归问题上，特别是高维 Gaussian 数据上。多指标函数可以看作是一个低维线性投影和一个低维链函数的复杂组合，因此它们成为了神经网络中的自然特征学习模板。我们考虑了两个时间步长的算法，其中低维链函数使用非参数化模型在无穷多个步长上学习，而低维线性投影则由参数化的子空间协同矩阵的matrix semigroup结构来学习。我们证明了这种涌流动力学的全球收敛性，并给出了相应的`鞍点-鞍点'动力学的量化描述。与此同时，我们还显示了相关的植入问题（即链函数已知和固定）实际上有一个恶劣优化景观，在这种情况下，梯度流动可能会陷入高概率下。
</details></li>
</ul>
<hr>
<h2 id="Locally-Optimal-Best-Arm-Identification-with-a-Fixed-Budget"><a href="#Locally-Optimal-Best-Arm-Identification-with-a-Fixed-Budget" class="headerlink" title="Locally Optimal Best Arm Identification with a Fixed Budget"></a>Locally Optimal Best Arm Identification with a Fixed Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19788">http://arxiv.org/abs/2310.19788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Kato</li>
<li>for: 这种研究探讨了如何确定最佳治疗臂，即具有最高预期效果的治疗臂。</li>
<li>methods: 我们使用了各种方法，包括最佳臂标识（BAI）和ORDINAL优化。</li>
<li>results: 我们发现，在小差度 Régime下，我们可以通过设计一个称为通用-EBA策略（Generalized-Neyman-Allocation-Empirical-Best-Arm）来减少错误的可能性。这种策略是 asymptotically 优化的，即其错误率与下界相对应。<details>
<summary>Abstract</summary>
This study investigates the problem of identifying the best treatment arm, a treatment arm with the highest expected outcome. We aim to identify the best treatment arm with a lower probability of misidentification, which has been explored under various names across numerous research fields, including \emph{best arm identification} (BAI) and ordinal optimization. In our experiments, the number of treatment-allocation rounds is fixed. In each round, a decision-maker allocates a treatment arm to an experimental unit and observes a corresponding outcome, which follows a Gaussian distribution with a variance different among treatment arms. At the end of the experiment, we recommend one of the treatment arms as an estimate of the best treatment arm based on the observations. The objective of the decision-maker is to design an experiment that minimizes the probability of misidentifying the best treatment arm. With this objective in mind, we develop lower bounds for the probability of misidentification under the small-gap regime, where the gaps of the expected outcomes between the best and suboptimal treatment arms approach zero. Then, assuming that the variances are known, we design the Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, which is an extension of the Neyman allocation proposed by Neyman (1934) and the Uniform-EBA strategy proposed by Bubeck et al. (2011). For the GNA-EBA strategy, we show that the strategy is asymptotically optimal because its probability of misidentification aligns with the lower bounds as the sample size approaches infinity under the small-gap regime. We refer to such optimal strategies as locally asymptotic optimal because their performance aligns with the lower bounds within restricted situations characterized by the small-gap regime.
</details>
<details>
<summary>摘要</summary>
To achieve this objective, we develop lower bounds for the probability of misidentification under the small-gap regime, where the gaps of the expected outcomes between the best and suboptimal treatment arms approach zero. We then design the Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, which is an extension of the Neyman allocation proposed by Neyman (1934) and the Uniform-EBA strategy proposed by Bubeck et al. (2011).We show that the GNA-EBA strategy is asymptotically optimal because its probability of misidentification aligns with the lower bounds as the sample size approaches infinity under the small-gap regime. We refer to such optimal strategies as locally asymptotic optimal because their performance aligns with the lower bounds within restricted situations characterized by the small-gap regime.
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Attention-Neural-Networks-for-Non-Line-of-Sight-User-Tracking-with-Dynamic-Metasurface-Antennas"><a href="#Autoregressive-Attention-Neural-Networks-for-Non-Line-of-Sight-User-Tracking-with-Dynamic-Metasurface-Antennas" class="headerlink" title="Autoregressive Attention Neural Networks for Non-Line-of-Sight User Tracking with Dynamic Metasurface Antennas"></a>Autoregressive Attention Neural Networks for Non-Line-of-Sight User Tracking with Dynamic Metasurface Antennas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19767">http://arxiv.org/abs/2310.19767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyriakos Stylianopoulos, Murat Bayraktar, Nuria González Prelcic, George C. Alexandropoulos</li>
<li>for: 这篇论文旨在探讨非线性多paths环境下的用户追踪，并提出了一个基于机器学习的二阶方法。</li>
<li>methods: 这篇论文使用了一个具有注意力的神经网络，首先将杂凑的通道响应映射到潜在的用户位置，然后使用一个学习式autoregressive模型来利用时间相关的通道信息获取最终的位置预测。</li>
<li>results: numerical evaluation over an outdoor ray-tracing scenario shows that this methodology is capable of achieving high position accuracy across various multipath settings, even in the presence of LoS blockage.<details>
<summary>Abstract</summary>
User localization and tracking in the upcoming generation of wireless networks have the potential to be revolutionized by technologies such as the Dynamic Metasurface Antennas (DMAs). Commonly proposed algorithmic approaches rely on assumptions about relatively dominant Line-of-Sight (LoS) paths, or require pilot transmission sequences whose length is comparable to the number of DMA elements, thus, leading to limited effectiveness and considerable measurement overheads in blocked LoS and dynamic multipath environments. In this paper, we present a two-stage machine-learning-based approach for user tracking, specifically designed for non-LoS multipath settings. A newly proposed attention-based Neural Network (NN) is first trained to map noisy channel responses to potential user positions, regardless of user mobility patterns. This architecture constitutes a modification of the prominent vision transformer, specifically modified for extracting information from high-dimensional frequency response signals. As a second stage, the NN's predictions for the past user positions are passed through a learnable autoregressive model to exploit the time-correlated channel information and obtain the final position predictions. The channel estimation procedure leverages a DMA receive architecture with partially-connected radio frequency chains, which results to reduced numbers of pilots. The numerical evaluation over an outdoor ray-tracing scenario illustrates that despite LoS blockage, this methodology is capable of achieving high position accuracy across various multipath settings.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:User localization and tracking in the upcoming generation of wireless networks have the potential to be revolutionized by technologies such as the Dynamic Metasurface Antennas (DMAs). Commonly proposed algorithmic approaches rely on assumptions about relatively dominant Line-of-Sight (LoS) paths, or require pilot transmission sequences whose length is comparable to the number of DMA elements, thus, leading to limited effectiveness and considerable measurement overheads in blocked LoS and dynamic multipath environments. In this paper, we present a two-stage machine-learning-based approach for user tracking, specifically designed for non-LoS multipath settings. A newly proposed attention-based Neural Network (NN) is first trained to map noisy channel responses to potential user positions, regardless of user mobility patterns. This architecture constitutes a modification of the prominent vision transformer, specifically modified for extracting information from high-dimensional frequency response signals. As a second stage, the NN's predictions for the past user positions are passed through a learnable autoregressive model to exploit the time-correlated channel information and obtain the final position predictions. The channel estimation procedure leverages a DMA receive architecture with partially-connected radio frequency chains, which results to reduced numbers of pilots. The numerical evaluation over an outdoor ray-tracing scenario illustrates that despite LoS blockage, this methodology is capable of achieving high position accuracy across various multipath settings.Translate the text into Simplified Chinese:</SYS>Here's the translation:用户本地化和跟踪在未来的无线网络中具有很大的潜力，尤其是通过动态元件天线（DMA）等技术。常见的算法方法假设有相对主要的直线视图（LoS）路径，或者需要与DMA元件数量相同的导航传输序列，从而导致效果有限和测量开销很大在阻塞的LoS和动态吸收环境中。在这篇论文中，我们提出了一种基于机器学习的两 stage方法，特别是设计用于非LoS吸收环境。我们首先使用一种新的注意力基本的神经网络（NN）来映射噪声通道响应到潜在的用户位置，无论用户移动模式。这种架构改进了知名的视Transformer，特意为提取高维频响应信号中的信息。作为第二stage，NN的预测结果以前的用户位置通过一个可学习的自 regression模型来利用时相关的通道信息，以获得最终的位置预测。通道估计过程利用了DMA接收架构中的半连接 radio frequency链，从而减少了数量的导航传输。数值评估在一个外部照明场景中表明，尽管LoS堵塞，这种方法可以在不同的 multipath 设置下实现高精度的位置预测。
</details></li>
</ul>
<hr>
<h2 id="Epidemic-outbreak-prediction-using-machine-learning-models"><a href="#Epidemic-outbreak-prediction-using-machine-learning-models" class="headerlink" title="Epidemic outbreak prediction using machine learning models"></a>Epidemic outbreak prediction using machine learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19760">http://arxiv.org/abs/2310.19760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshara Pramod, JS Abhishek, Dr. Suganthi K</li>
<li>for: 预测疫情爆发（Influenza、肝炎和马拉疫苗）在纽约州、美国</li>
<li>methods: 使用机器学习和深度学习算法，以及一个portal来预测疫情爆发，并且利用历史数据预测5周内可能的病例数</li>
<li>results: 预测疫情爆发的可能性，并且利用非клиниче因素（Google搜索趋势、社交媒体数据和天气数据）预测疫情爆发的概率<details>
<summary>Abstract</summary>
In today's world,the risk of emerging and re-emerging epidemics have increased.The recent advancement in healthcare technology has made it possible to predict an epidemic outbreak in a region.Early prediction of an epidemic outbreak greatly helps the authorities to be prepared with the necessary medications and logistics required to keep things in control. In this article, we try to predict the epidemic outbreak (influenza, hepatitis and malaria) for the state of New York, USA using machine and deep learning algorithms, and a portal has been created for the same which can alert the authorities and health care organizations of the region in case of an outbreak. The algorithm takes historical data to predict the possible number of cases for 5 weeks into the future. Non-clinical factors like google search trends,social media data and weather data have also been used to predict the probability of an outbreak.
</details>
<details>
<summary>摘要</summary>
今天的世界中，突发和复发疫情的风险增加了。最近的医疗科技进步使得可以预测一个地区的疫情爆发。在这篇文章中，我们使用机器学习和深度学习算法预测新 York 州的 influenza、hepatitis 和 malaria 疫情爆发，并创建了一个 portal，可以警示当地权力机构和医疗组织在疫情爆发时。算法使用历史数据预测下一个5个星期内可能出现的病例数。此外，我们还使用google搜索趋势、社交媒体数据和天气数据预测疫情爆发的可能性。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Reward-Estimation-with-Preference-Feedback"><a href="#Differentially-Private-Reward-Estimation-with-Preference-Feedback" class="headerlink" title="Differentially Private Reward Estimation with Preference Feedback"></a>Differentially Private Reward Estimation with Preference Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19733">http://arxiv.org/abs/2310.19733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayak Ray Chowdhury, Xingyu Zhou, Nagarajan Natarajan</li>
<li>for: 这个论文的目的是如何使用偏好反馈来调整生成模型，以便更好地满足人类的兴趣。</li>
<li>methods: 这个论文使用了反馈学习 WITH human feedback (RLHF) 方法，首先从人类标签器获取反馈，然后计算出奖励模型，最后使用奖励模型来制定策略。</li>
<li>results: 这个论文通过引入标签幂 differential privacy (DP) 的概念，解决了基于偏好反馈的奖励估计中人类标签器隐私泄露的问题。 Specifically, 论文考虑了 Bradley-Terry-Luce (BTL) 模型，并在标准的最小最大估计框架下提供了紧Binding的Upper和Lower bounds on 奖励估计错误。<details>
<summary>Abstract</summary>
Learning from preference-based feedback has recently gained considerable traction as a promising approach to align generative models with human interests. Instead of relying on numerical rewards, the generative models are trained using reinforcement learning with human feedback (RLHF). These approaches first solicit feedback from human labelers typically in the form of pairwise comparisons between two possible actions, then estimate a reward model using these comparisons, and finally employ a policy based on the estimated reward model. An adversarial attack in any step of the above pipeline might reveal private and sensitive information of human labelers. In this work, we adopt the notion of label differential privacy (DP) and focus on the problem of reward estimation from preference-based feedback while protecting privacy of each individual labelers. Specifically, we consider the parametric Bradley-Terry-Luce (BTL) model for such pairwise comparison feedback involving a latent reward parameter $\theta^* \in \mathbb{R}^d$. Within a standard minimax estimation framework, we provide tight upper and lower bounds on the error in estimating $\theta^*$ under both local and central models of DP. We show, for a given privacy budget $\epsilon$ and number of samples $n$, that the additional cost to ensure label-DP under local model is $\Theta \big(\frac{1}{ e^\epsilon-1}\sqrt{\frac{d}{n}\big)$, while it is $\Theta\big(\frac{\text{poly}(d)}{\epsilon n} \big)$ under the weaker central model. We perform simulations on synthetic data that corroborate these theoretical results.
</details>
<details>
<summary>摘要</summary>
学习从偏好反馈中获得了 considerable 的满意度，这种方法可以让生成模型与人类的兴趣相匹配。而不是依靠数字奖励，这些生成模型通过人类反馈（RLHF）进行训练。这些方法首先从人类标签器获得反馈，通常是两个可能的动作之间的对比，然后计算出一个奖励模型，最后使用这个奖励模型来采取策略。在上述执行管道中，一个反对攻击可能泄露人类标签器的私人和敏感信息。在这个工作中，我们采用标签权限隐私（DP）的想法，并关注在基于偏好反馈中保护每名标签器的隐私问题。我们考虑 Bradley-Terry-Luce（BTL）模型，这是一种对比式反馈模型，其中包含一个隐藏奖励参数 $\theta^* \in \mathbb{R}^d$。在标准的最小最大估计框架中，我们提供了紧跟的上下限错误估计 $\theta^*$ 的误差，并且分析了在本地和中央模型下的DP保护成本。我们发现，对于给定的隐私预算 $\epsilon$ 和样本数 $n$，在本地模型下添加额外的DP保护成本是 $\Theta \big(\frac{1}{ e^\epsilon-1}\sqrt{\frac{d}{n}\big)$，而在更弱的中央模型下是 $\Theta\big(\frac{\text{poly}(d)}{\epsilon n} \big)$。我们在 sintetic 数据上进行了仪表实验，并证明了这些理论结论。
</details></li>
</ul>
<hr>
<h2 id="Support-matrix-machine-A-review"><a href="#Support-matrix-machine-A-review" class="headerlink" title="Support matrix machine: A review"></a>Support matrix machine: A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19717">http://arxiv.org/abs/2310.19717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Anuradha Kumari, Mushir Akhtar, Rupal Shah, M. Tanveer</li>
<li>for: 这个研究是为了解决支持向量机器学习（SVM）在处理矩阵输入数据时所遇到的问题。</li>
<li>methods: 这个研究使用支持矩阵机器学习（SMM）方法，它是一种处理矩阵输入数据的新型机器学习方法。SMM方法利用特征矩阵的特性，保留矩阵资料中的结构资讯，并且可以处理高维度输入数据。</li>
<li>results: 这个研究提出了多种SMM模型的变形，包括预期抗噪、簇短、分类不均等模型。这些模型都可以实现高度的准确率和高效率。此外，这个研究还简述了SMM模型的应用和未来可能的研究方向。<details>
<summary>Abstract</summary>
Support vector machine (SVM) is one of the most studied paradigms in the realm of machine learning for classification and regression problems. It relies on vectorized input data. However, a significant portion of the real-world data exists in matrix format, which is given as input to SVM by reshaping the matrices into vectors. The process of reshaping disrupts the spatial correlations inherent in the matrix data. Also, converting matrices into vectors results in input data with a high dimensionality, which introduces significant computational complexity. To overcome these issues in classifying matrix input data, support matrix machine (SMM) is proposed. It represents one of the emerging methodologies tailored for handling matrix input data. The SMM method preserves the structural information of the matrix data by using the spectral elastic net property which is a combination of the nuclear norm and Frobenius norm. This article provides the first in-depth analysis of the development of the SMM model, which can be used as a thorough summary by both novices and experts. We discuss numerous SMM variants, such as robust, sparse, class imbalance, and multi-class classification models. We also analyze the applications of the SMM model and conclude the article by outlining potential future research avenues and possibilities that may motivate academics to advance the SMM algorithm.
</details>
<details>
<summary>摘要</summary>
支持向量机 (SVM) 是机器学习领域中最受研究的一种 paradigma，用于分类和回归问题。它基于 вектор化输入数据。然而，实际世界中大量数据存在矩阵形式，需要通过将矩阵转换为向量来输入 SVM。这个过程会破坏矩阵数据中的空间相关性，并且将输入数据的维度增加，导致计算复杂性增加。为了解决这些问题，支持矩阵机 (SMM) 被提出。它是一种处理矩阵输入数据的新趋势。SMM 方法利用矩阵数据的特征信息，使用 spectral elastic net 性质，这是一种组合核心值和 Frobenius 值的方法。本文提供了 SMM 模型的首个深入分析，可以作为新手和专家的参考。我们讨论了多种 SMM 变体，如 robust、稀热、类偏振和多类分类模型。我们还分析了 SMM 模型的应用，并在文章结尾列出了可能的未来研究方向和动机，以便学术人士继续推进 SMM 算法。
</details></li>
</ul>
<hr>
<h2 id="Exact-Recovery-and-Bregman-Hard-Clustering-of-Node-Attributed-Stochastic-Block-Model"><a href="#Exact-Recovery-and-Bregman-Hard-Clustering-of-Node-Attributed-Stochastic-Block-Model" class="headerlink" title="Exact Recovery and Bregman Hard Clustering of Node-Attributed Stochastic Block Model"></a>Exact Recovery and Bregman Hard Clustering of Node-Attributed Stochastic Block Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19854">http://arxiv.org/abs/2310.19854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilien Dreveton, Felipe S. Fernandes, Daniel R. Figueiredo</li>
<li>for: 本文旨在解决节点网络中的社区分割问题，但是节点也有属性信息，这些信息和网络信息可以并行利用以设计高性能的分 clustering算法。</li>
<li>methods: 本文提出了一个信息理论基础的优化算法，可以准确地还原社区标签，并研究了这种算法在不同的网络和属性模型下的性能。</li>
<li>results: 对比 классических算法和现有的算法，提出的算法在synthetic数据上表现出优于其他算法，并且可以处理不同的网络和属性模型，以及稀疏网络。<details>
<summary>Abstract</summary>
Network clustering tackles the problem of identifying sets of nodes (communities) that have similar connection patterns. However, in many scenarios, nodes also have attributes that are correlated with the clustering structure. Thus, network information (edges) and node information (attributes) can be jointly leveraged to design high-performance clustering algorithms. Under a general model for the network and node attributes, this work establishes an information-theoretic criterion for the exact recovery of community labels and characterizes a phase transition determined by the Chernoff-Hellinger divergence of the model. The criterion shows how network and attribute information can be exchanged in order to have exact recovery (e.g., more reliable network information requires less reliable attribute information). This work also presents an iterative clustering algorithm that maximizes the joint likelihood, assuming that the probability distribution of network interactions and node attributes belong to exponential families. This covers a broad range of possible interactions (e.g., edges with weights) and attributes (e.g., non-Gaussian models), as well as sparse networks, while also exploring the connection between exponential families and Bregman divergences. Extensive numerical experiments using synthetic data indicate that the proposed algorithm outperforms classic algorithms that leverage only network or only attribute information as well as state-of-the-art algorithms that also leverage both sources of information. The contributions of this work provide insights into the fundamental limits and practical techniques for inferring community labels on node-attributed networks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Convolutional-State-Space-Models-for-Long-Range-Spatiotemporal-Modeling"><a href="#Convolutional-State-Space-Models-for-Long-Range-Spatiotemporal-Modeling" class="headerlink" title="Convolutional State Space Models for Long-Range Spatiotemporal Modeling"></a>Convolutional State Space Models for Long-Range Spatiotemporal Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19694">http://arxiv.org/abs/2310.19694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jimmy T. H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon</li>
<li>for: 这篇论文的目的是对长时间空间序列进行有效地模型，以满足需要同时模型复杂的空间相关性和长时间依赖性。</li>
<li>methods: 这篇论文提出了一种将tensor值状态更新为回归神经网络的方法，并且使用了平行扫描来实现快速的自动生成。另外，这篇论文还提出了一种基于state space方法的长时间模型，将数值状态转换为字串，以便平行处理整个时空序列。</li>
<li>results: 这篇论文的结果显示，ConvS5在一个长时间Moving-MNIST实验中与Transformers和ConvLSTM相比，训练3倍 faster，并在生成数据时间方面比Transformers快400倍。此外，ConvS5在DMLab、Minecraft和Habitat预测测试中与状态点方法匹配或超越，并开启了新的长时间空间序列模型方向。<details>
<summary>Abstract</summary>
Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.
</details>
<details>
<summary>摘要</summary>
长时间空间序列模型化是一项具有挑战性的任务，因为需要同时模型复杂的空间相关性和长距离时间相关性。ConvLSTM通过更新tensor值状态使用Recurrent Neural Networks来解决这个问题，但它们的顺序计算使得它们在训练时slow。与之相比，Transformers可以在同时处理整个空间时间序列，压缩成token，但它们的注意力成本随序列长度平方增加，限制它们对更长的序列的可扩展性。在这里，我们解决先前方法的挑战，并引入了 convolutional state space models (ConvSSM)，它们将ConvLSTM中的tensor模型 идеи与状态方法 such as S4和S5的长时间序列模型approaches相结合。首先，我们展示了如何通过并行扫描来实现subquadratic parallelization，并在 autoregressive generation 中实现快速生成。然后，我们证明了 ConvSSM 的动力学和 SSM 的动力学之间的等价性，这种等价性驱动了参数化和初始化策略，以便模型长距离相关性。最终，我们提出了 ConvS5，一种高效的 ConvSSM 变体，用于长距离空间时间模型化。ConvS5在一个长期 Moving-MNIST 实验中表现出色，同时训练 3X  faster than ConvLSTM 和生成样本 400X faster than Transformers。此外，ConvS5 与当前状态艺术方法相当或更好的性能在 DMLab、Minecraft 和 Habitat 预测 benchmark 上，并启动了新的长时间空间序列模型化方向。
</details></li>
</ul>
<hr>
<h2 id="Towards-Practical-Non-Adversarial-Distribution-Alignment-via-Variational-Bounds"><a href="#Towards-Practical-Non-Adversarial-Distribution-Alignment-via-Variational-Bounds" class="headerlink" title="Towards Practical Non-Adversarial Distribution Alignment via Variational Bounds"></a>Towards Practical Non-Adversarial Distribution Alignment via Variational Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19690">http://arxiv.org/abs/2310.19690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyu Gong, Ben Usman, Han Zhao, David I. Inouye</li>
<li>for: 学习不变表示，应用于公平和鲁棒性。</li>
<li>methods: 非对抗性概率基的方法，不需要模型反转性和假设潜在分布。</li>
<li>results: 可以取代对抗损失，在标准不变表示学习管道中使用，无需修改原始架构。In English, this means:</li>
<li>for: Learning invariant representations for fairness and robustness.</li>
<li>methods: Non-adversarial likelihood-based approaches that do not require model invertibility and can be applied to any model pipeline.</li>
<li>results: Our novel alignment losses can replace adversarial losses in standard invariant representation learning pipelines without modifying the original architectures, significantly broadening the applicability of non-adversarial alignment methods.<details>
<summary>Abstract</summary>
Distribution alignment can be used to learn invariant representations with applications in fairness and robustness. Most prior works resort to adversarial alignment methods but the resulting minimax problems are unstable and challenging to optimize. Non-adversarial likelihood-based approaches either require model invertibility, impose constraints on the latent prior, or lack a generic framework for alignment. To overcome these limitations, we propose a non-adversarial VAE-based alignment method that can be applied to any model pipeline. We develop a set of alignment upper bounds (including a noisy bound) that have VAE-like objectives but with a different perspective. We carefully compare our method to prior VAE-based alignment approaches both theoretically and empirically. Finally, we demonstrate that our novel alignment losses can replace adversarial losses in standard invariant representation learning pipelines without modifying the original architectures -- thereby significantly broadening the applicability of non-adversarial alignment methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>Distribution alignment可以用来学习不变表示，并有应用于公平和Robustness。大多数先前的工作采用了对抗启动方法，但 resulting minimax问题是不稳定并难以优化。非对抗的可能性基于方法 Either require model invertibility, 强制 latent prior 的限制，或lack 一个通用的对齐框架。为了突破这些限制，我们提议一种非对抗 VAE-based 对齐方法，可以应用于任何模型管道。我们开发了一系列对齐上限（包括噪声 bound），它们具有 VAE-like 目标，但从不同的视角来看。我们在理论和实验方面仔细比较了我们的方法和先前 VAE-based 对齐方法。最后，我们示出了我们的新对齐损失可以在标准不变表示学习管道中代替对抗损失，无需修改原始架构 --  thereby significantly broadening the applicability of non-adversarial alignment methods。
</details></li>
</ul>
<hr>
<h2 id="DGFN-Double-Generative-Flow-Networks"><a href="#DGFN-Double-Generative-Flow-Networks" class="headerlink" title="DGFN: Double Generative Flow Networks"></a>DGFN: Double Generative Flow Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19685">http://arxiv.org/abs/2310.19685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elaine Lau, Nikhil Vemgal, Doina Precup, Emmanuel Bengio</li>
<li>for: 这个研究用于探索药物设计中的问题，特别是在缺乏对答问题和高维度状态空间中。</li>
<li>methods: 这个研究使用了两个流程网络（GFlowNets&#x2F;GFNs），并将其与强化学习和双层深度Q学习结合，以增强探索。</li>
<li>results: 实验结果显示，这种方法可以有效地增强探索在缺乏对答问题和高维度状态空间中，并且在药物设计中进行创新设计。<details>
<summary>Abstract</summary>
Deep learning is emerging as an effective tool in drug discovery, with potential applications in both predictive and generative models. Generative Flow Networks (GFlowNets/GFNs) are a recently introduced method recognized for the ability to generate diverse candidates, in particular in small molecule generation tasks. In this work, we introduce double GFlowNets (DGFNs). Drawing inspiration from reinforcement learning and Double Deep Q-Learning, we introduce a target network used to sample trajectories, while updating the main network with these sampled trajectories. Empirical results confirm that DGFNs effectively enhance exploration in sparse reward domains and high-dimensional state spaces, both challenging aspects of de-novo design in drug discovery.
</details>
<details>
<summary>摘要</summary>
深度学习在药物发现中展示出了有效性，具有预测和生成模型的潜在应用。生成流网络（GFlowNets/GFNs）是最近引入的方法，被认可为能够生成多样化候选者，尤其在小分子生成任务中。在这项工作中，我们引入双生成流网络（DGFNs）。启发自强化学习和双层深度Q学习，我们引入目标网络用于采样轨迹，同时更新主网络使用这些采样轨迹。实验结果表明，DGFNs有效地增强了探索性在稀资 reward 领域和高维状态空间中，这些领域都是药物发现中的挑战。
</details></li>
</ul>
<hr>
<h2 id="Density-Estimation-for-Entry-Guidance-Problems-using-Deep-Learning"><a href="#Density-Estimation-for-Entry-Guidance-Problems-using-Deep-Learning" class="headerlink" title="Density Estimation for Entry Guidance Problems using Deep Learning"></a>Density Estimation for Entry Guidance Problems using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19684">http://arxiv.org/abs/2310.19684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jens A. Rataczak, Davide Amato, Jay W. McMahon</li>
<li>for: 这个研究旨在使用深度学习方法来估算入 atmospheric density profiles，用于 planetary entry guidance 问题。</li>
<li>methods: 研究使用了 long short-term memory (LSTM) 神经网络，学习从飞行器上可available的测量数据和density profile之间的映射。测量数据包括圆柱体状态表示、 Cartesian 感知加速度分量和surface-pressure measurement。</li>
<li>results: 研究发现，使用 LSTM 网络可以不仅预测飞行器将飞行的density profile，还可以重建飞行器已经飞行过的density profile。与其他两种估算技术相比，使用 LSTM 网络得到了更高的终端准确性。<details>
<summary>Abstract</summary>
This work presents a deep-learning approach to estimate atmospheric density profiles for use in planetary entry guidance problems. A long short-term memory (LSTM) neural network is trained to learn the mapping between measurements available onboard an entry vehicle and the density profile through which it is flying. Measurements include the spherical state representation, Cartesian sensed acceleration components, and a surface-pressure measurement. Training data for the network is initially generated by performing a Monte Carlo analysis of an entry mission at Mars using the fully numerical predictor-corrector guidance (FNPEG) algorithm that utilizes an exponential density model, while the truth density profiles are sampled from MarsGRAM. A curriculum learning procedure is developed to refine the LSTM network's predictions for integration within the FNPEG algorithm. The trained LSTM is capable of both predicting the density profile through which the vehicle will fly and reconstructing the density profile through which it has already flown. The performance of the FNPEG algorithm is assessed for three different density estimation techniques: an exponential model, an exponential model augmented with a first-order fading-memory filter, and the LSTM network. Results demonstrate that using the LSTM model results in superior terminal accuracy compared to the other two techniques when considering both noisy and noiseless measurements.
</details>
<details>
<summary>摘要</summary>
The training data is generated through a Monte Carlo analysis of an entry mission at Mars using the fully numerical predictor-corrector guidance (FNPEG) algorithm with an exponential density model. The truth density profiles are sampled from MarsGRAM. To refine the LSTM network's predictions, a curriculum learning procedure is developed.The trained LSTM is capable of predicting the density profile through which the vehicle will fly and reconstructing the density profile through which it has already flown. The performance of the FNPEG algorithm is assessed for three different density estimation techniques: an exponential model, an exponential model augmented with a first-order fading-memory filter, and the LSTM network. The results show that the LSTM model achieves superior terminal accuracy compared to the other two techniques, both with noisy and noiseless measurements.
</details></li>
</ul>
<hr>
<h2 id="An-Online-Bootstrap-for-Time-Series"><a href="#An-Online-Bootstrap-for-Time-Series" class="headerlink" title="An Online Bootstrap for Time Series"></a>An Online Bootstrap for Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19683">http://arxiv.org/abs/2310.19683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Nicolai Palm, Thomas Nagler</li>
<li>for: This paper is written for researchers and practitioners who need to perform uncertainty quantification on large streams of dependent data, such as time series or spatially correlated observations.</li>
<li>methods: The paper proposes a novel bootstrap method that accounts for data dependencies and can be executed online, making it suitable for real-time applications. The method is based on an autoregressive sequence of increasingly dependent resampling weights.</li>
<li>results: The proposed bootstrap scheme is proven to be theoretically valid under general conditions, and is demonstrated to provide reliable uncertainty quantification even in the presence of complex data dependencies through extensive simulations.<details>
<summary>Abstract</summary>
Resampling methods such as the bootstrap have proven invaluable in the field of machine learning. However, the applicability of traditional bootstrap methods is limited when dealing with large streams of dependent data, such as time series or spatially correlated observations. In this paper, we propose a novel bootstrap method that is designed to account for data dependencies and can be executed online, making it particularly suitable for real-time applications. This method is based on an autoregressive sequence of increasingly dependent resampling weights. We prove the theoretical validity of the proposed bootstrap scheme under general conditions. We demonstrate the effectiveness of our approach through extensive simulations and show that it provides reliable uncertainty quantification even in the presence of complex data dependencies. Our work bridges the gap between classical resampling techniques and the demands of modern data analysis, providing a valuable tool for researchers and practitioners in dynamic, data-rich environments.
</details>
<details>
<summary>摘要</summary>
traditional bootstrap方法在受到大量相互相关数据时，其可行性有限。在这篇论文中，我们提出了一种新的bootstrap方法，可以考虑数据相互关系，并可以在线执行，因此特别适用于实时应用。这种方法基于一个自增式相关的排重采样序列。我们证明了这种bootstrap方案的理论有效性，并通过广泛的仿真实验证了其可靠性。我们的工作将经典采样技术与现代数据分析的需求相连接，为研究人员和实践者在动态数据丰富环境中提供了一种有价值的工具。
</details></li>
</ul>
<hr>
<h2 id="HyPE-Attention-with-Hyperbolic-Biases-for-Relative-Positional-Encoding"><a href="#HyPE-Attention-with-Hyperbolic-Biases-for-Relative-Positional-Encoding" class="headerlink" title="HyPE: Attention with Hyperbolic Biases for Relative Positional Encoding"></a>HyPE: Attention with Hyperbolic Biases for Relative Positional Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19676">http://arxiv.org/abs/2310.19676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgio Angelotti</li>
<li>for: 提高Transformer架构中的注意力机制的可 permutation-invariance性，以便在输入序列中推断 tokens 的顺序。</li>
<li>methods: 引入Hyperbolic Positional Encoding (HyPE)，一种使用高维函数性质来编码 tokens 的相对位置的新方法，不需要存储 $O(L^2)$ 值的 маask，并且可以通过预liminary concatenation operations和矩阵乘法来实现编码。</li>
<li>results: HyPE 可以准确地 aproximate ALiBi 的注意力偏好，并且可以在不同的长度上进行推断，这表示 HyPE 具有良好的泛化能力。<details>
<summary>Abstract</summary>
In Transformer-based architectures, the attention mechanism is inherently permutation-invariant with respect to the input sequence's tokens. To impose sequential order, token positions are typically encoded using a scheme with either fixed or learnable parameters. We introduce Hyperbolic Positional Encoding (HyPE), a novel method that utilizes hyperbolic functions' properties to encode tokens' relative positions. This approach biases the attention mechanism without the necessity of storing the $O(L^2)$ values of the mask, with $L$ being the length of the input sequence. HyPE leverages preliminary concatenation operations and matrix multiplications, facilitating the encoding of relative distances indirectly incorporating biases into the softmax computation. This design ensures compatibility with FlashAttention-2 and supports the gradient backpropagation for any potential learnable parameters within the encoding. We analytically demonstrate that, by careful hyperparameter selection, HyPE can approximate the attention bias of ALiBi, thereby offering promising generalization capabilities for contexts extending beyond the lengths encountered during pretraining. The experimental evaluation of HyPE is proposed as a direction for future research.
</details>
<details>
<summary>摘要</summary>
在基于变换器的架构中，注意机制自然地具有输入序列中元素的排序不敏感性。为了强制实现顺序，通常使用固定或学习参数的编码方案来编码token的位置。我们介绍了一种新的方法——折射函数编码（HyPE），它利用折射函数的性质来编码token的相对位置。这种方法不需要存储$O(L^2)$的掩码值，其中$L$是输入序列的长度。HyPE通过预处理的 concatenation 操作和矩阵乘法，实现编码相对距离，并通过直接在softmax计算中涂抹偏好来实现。这种设计允许HyPE与FlashAttention-2兼容，并且支持任何可能的学习参数在编码中的梯度反射。我们分析表明，通过精心选择参数，HyPE可以近似ALiBi的注意偏好，从而提供了扩展训练集之外的扩展能力。HyPE的实验性评估被提议为未来研究的方向。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes"><a href="#Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes" class="headerlink" title="Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes"></a>Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19666">http://arxiv.org/abs/2310.19666</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wzhut/dynamic-tensor-decomposition-via-neural-diffusion-reaction-processes">https://github.com/wzhut/dynamic-tensor-decomposition-via-neural-diffusion-reaction-processes</a></li>
<li>paper_authors: Zheng Wang, Shikai Fang, Shibo Li, Shandian Zhe</li>
<li>for: 这个论文的目的是提出一种基于神经Diffusion-Reaction过程的动态tensor分解方法，以更好地捕捉多方数据分析中的时间结构信息。</li>
<li>methods: 该方法使用神经网络构建多部分图来编码缺失的tensor模式中的关系，然后通过协同演化图像演化轨迹和个体特征来捕捉个体的共同特征和个体特点。</li>
<li>results: 作者通过实验研究和实际应用示例显示了该方法的优势，并提供了一个可用的代码库。<details>
<summary>Abstract</summary>
Tensor decomposition is an important tool for multiway data analysis. In practice, the data is often sparse yet associated with rich temporal information. Existing methods, however, often under-use the time information and ignore the structural knowledge within the sparsely observed tensor entries. To overcome these limitations and to better capture the underlying temporal structure, we propose Dynamic EMbedIngs fOr dynamic Tensor dEcomposition (DEMOTE). We develop a neural diffusion-reaction process to estimate dynamic embeddings for the entities in each tensor mode. Specifically, based on the observed tensor entries, we build a multi-partite graph to encode the correlation between the entities. We construct a graph diffusion process to co-evolve the embedding trajectories of the correlated entities and use a neural network to construct a reaction process for each individual entity. In this way, our model can capture both the commonalities and personalities during the evolution of the embeddings for different entities. We then use a neural network to model the entry value as a nonlinear function of the embedding trajectories. For model estimation, we combine ODE solvers to develop a stochastic mini-batch learning algorithm. We propose a stratified sampling method to balance the cost of processing each mini-batch so as to improve the overall efficiency. We show the advantage of our approach in both simulation study and real-world applications. The code is available at https://github.com/wzhut/Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes.
</details>
<details>
<summary>摘要</summary>
tensor 分解是多方数据分析中的重要工具。在实践中，数据通常是稀疏的，但具有丰富的时间信息。现有方法往往忽略时间信息，并忽略tensor中稀疏观察到的结构知识。为了超越这些局限性，我们提出了动态嵌入（DEMOTE）。我们采用神经扩散-反应过程来估计动态嵌入 для每个tensor模式中的实体。具体来说，根据观察到的tensor入口，我们建立了多部分图来编码实体之间的相关性。我们构建了图扩散过程来同步嵌入轨迹的演化，并使用神经网络构建每个个体的反应过程。这样，我们的模型可以捕捉到不同实体的共同特征和个性特征在演化过程中的变化。然后，我们使用神经网络来模型每个入口的值为非线性函数。为估计模型，我们结合ODE解引入了随机批处理算法。我们提出了一种层次随机抽样法，以平衡处理每个批处理的成本，从而提高整体效率。我们在实验研究和实际应用中显示了我们的方法的优势。代码可以在https://github.com/wzhut/Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes中找到。
</details></li>
</ul>
<hr>
<h2 id="Predicting-mutational-effects-on-protein-protein-binding-via-a-side-chain-diffusion-probabilistic-model"><a href="#Predicting-mutational-effects-on-protein-protein-binding-via-a-side-chain-diffusion-probabilistic-model" class="headerlink" title="Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model"></a>Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19849">http://arxiv.org/abs/2310.19849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eurekazhu/diffaffinity">https://github.com/eurekazhu/diffaffinity</a></li>
<li>paper_authors: Shiwei Liu, Tian Zhu, Milong Ren, Chungong Yu, Dongbo Bu, Haicang Zhang</li>
<li>for: 预测蛋白质-蛋白质交互的作用对蛋白质工程和药物发现是关键。</li>
<li>methods: 本文提出了SidechainDiff，一种基于学习表征的方法，利用无标注实验数据来预测蛋白质-蛋白质交互的作用。SidechainDiff使用瑞氏幂函数模型来学习生成残氨链的过程，同时还可以提供蛋白质-蛋白质界面上的结构上下文表示。</li>
<li>results: 利用SidechainDiff进行预测，对蛋白质-蛋白质交互的作用具有最佳性能。此外，SidechainDiff是首先使用扩散模型生成残氨链的方法，与之前的主要关注蛋白质脊梁结构生成相区别。<details>
<summary>Abstract</summary>
Many crucial biological processes rely on networks of protein-protein interactions. Predicting the effect of amino acid mutations on protein-protein binding is vital in protein engineering and therapeutic discovery. However, the scarcity of annotated experimental data on binding energy poses a significant challenge for developing computational approaches, particularly deep learning-based methods. In this work, we propose SidechainDiff, a representation learning-based approach that leverages unlabelled experimental protein structures. SidechainDiff utilizes a Riemannian diffusion model to learn the generative process of side-chain conformations and can also give the structural context representations of mutations on the protein-protein interface. Leveraging the learned representations, we achieve state-of-the-art performance in predicting the mutational effects on protein-protein binding. Furthermore, SidechainDiff is the first diffusion-based generative model for side-chains, distinguishing it from prior efforts that have predominantly focused on generating protein backbone structures.
</details>
<details>
<summary>摘要</summary>
多种生物过程都依赖于蛋白质-蛋白质交互网络。预测蛋白质-蛋白质绑定的效果是蛋白工程和药物发现中的关键。然而，实验数据缺乏绑定能量的标注，对于开发计算方法，特别是深度学习方法，呈现了 significante挑战。在这项工作中，我们提出了SidechainDiff方法，它是基于学习扩散过程的 representation learning 方法，可以利用无标注实验蛋白结构来学习蛋白的侧链姿态。 SidechainDiff 还可以给蛋白质-蛋白质界面上的变化提供结构上下文表示。利用学习的表示，我们实现了对蛋白质-蛋白质绑定效果的预测，并且 SidechainDiff 是首个 для侧链的扩散型生成模型，与之前的主要关注在蛋白质脊梁结构生成方面。
</details></li>
</ul>
<hr>
<h2 id="Dis-inhibitory-neuronal-circuits-can-control-the-sign-of-synaptic-plasticity"><a href="#Dis-inhibitory-neuronal-circuits-can-control-the-sign-of-synaptic-plasticity" class="headerlink" title="Dis-inhibitory neuronal circuits can control the sign of synaptic plasticity"></a>Dis-inhibitory neuronal circuits can control the sign of synaptic plasticity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19614">http://arxiv.org/abs/2310.19614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmi-basel/disinhibitory-control">https://github.com/fmi-basel/disinhibitory-control</a></li>
<li>paper_authors: Julian Rossbroich, Friedemann Zenke</li>
<li>for: 这个研究的目的是解释如何 neuronal circuits 实现 credit assignment，即在系统神经科学中一个中心未解决的问题。</li>
<li>methods: 这个研究使用了一种可能的 microcircuit model 和 Hebbian learning rule，这些模型来自 adaptive control theory 框架。</li>
<li>results: 研究发现，当假设错误是通过 top-down 抑制性 synaptic afferents 编码时，error-modulated learning 会自然地出现在 circuit level，并且可以解释实验室中观察到的材料塑性。此外，这种学习规则还可以与 back-propagation of error 相比肩，在非线性分离的benchmark上表现相似。<details>
<summary>Abstract</summary>
How neuronal circuits achieve credit assignment remains a central unsolved question in systems neuroscience. Various studies have suggested plausible solutions for back-propagating error signals through multi-layer networks. These purely functionally motivated models assume distinct neuronal compartments to represent local error signals that determine the sign of synaptic plasticity. However, this explicit error modulation is inconsistent with phenomenological plasticity models in which the sign depends primarily on postsynaptic activity. Here we show how a plausible microcircuit model and Hebbian learning rule derived within an adaptive control theory framework can resolve this discrepancy. Assuming errors are encoded in top-down dis-inhibitory synaptic afferents, we show that error-modulated learning emerges naturally at the circuit level when recurrent inhibition explicitly influences Hebbian plasticity. The same learning rule accounts for experimentally observed plasticity in the absence of inhibition and performs comparably to back-propagation of error (BP) on several non-linearly separable benchmarks. Our findings bridge the gap between functional and experimentally observed plasticity rules and make concrete predictions on inhibitory modulation of excitatory plasticity.
</details>
<details>
<summary>摘要</summary>
neronal 网络中实现信用分配的问题仍然是系统神经科学的中心问题。多个研究表明可能的解决方案是通过多层网络传递错误讯号的后填宽频率调控。这些仅功能上验证的模型假设了不同的 neuronal 分 compartment 来表示本地错误讯号，这些讯号决定了synaptic 静电� Monday 的方向。然而，这些Explicit error 调控是与现象学静电� Monday 模型不一致，这些模型中错误的方向主要取决于跟随频率的postsynaptic 活动。在这篇文章中，我们显示了一个可能的微网络模型和Hebbian 学习规则，这个规则是在数据控制理论框架下 derivation 的。我们假设错误是通过上方的抑制性 synaptic 联系传递的，我们显示出错误调控的学习现象将 naturally 出现在网络水平上，当recurrent 抑制性explicitly influencing Hebbian 学习。同时，我们显示了这个学习规则可以解释实验观察到的plasticity 现象，并且与back-propagation of error (BP) 相比，这个学习规则在多个非线性分类 benchmark 上表现出来的相似。我们的发现 bridge 了功能和实验观察到的静电� Monday 规则之间的差距，并且对抑制性的影响 på excitatory 静电� Monday 的学习提出了具体预测。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Exploration-in-Continuous-time-Model-based-Reinforcement-Learning"><a href="#Efficient-Exploration-in-Continuous-time-Model-based-Reinforcement-Learning" class="headerlink" title="Efficient Exploration in Continuous-time Model-based Reinforcement Learning"></a>Efficient Exploration in Continuous-time Model-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19848">http://arxiv.org/abs/2310.19848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lenart Treven, Jonas Hübotter, Bhavya Sukhija, Florian Dörfler, Andreas Krause</li>
<li>for: 本研究旨在提出一种基于模型的强化学习算法，用于处理连续时间动态系统。</li>
<li>methods: 本paper使用非线性常微分方程（ODE）来表示连续时间动态系统，并使用受限概率模型来捕捉知识uncertainty。用户选择策略（MSS）来控制探索和观察。</li>
<li>results: 我们的 regret bound表明，使用GP动力学模型和合适的MSS可以实现下线性的征 regret。我们还提出了一种自适应、数据依赖的实用MSS，可以在几何上减少样本数量，并且在多个应用中显示出优势。<details>
<summary>Abstract</summary>
Reinforcement learning algorithms typically consider discrete-time dynamics, even though the underlying systems are often continuous in time. In this paper, we introduce a model-based reinforcement learning algorithm that represents continuous-time dynamics using nonlinear ordinary differential equations (ODEs). We capture epistemic uncertainty using well-calibrated probabilistic models, and use the optimistic principle for exploration. Our regret bounds surface the importance of the measurement selection strategy(MSS), since in continuous time we not only must decide how to explore, but also when to observe the underlying system. Our analysis demonstrates that the regret is sublinear when modeling ODEs with Gaussian Processes (GP) for common choices of MSS, such as equidistant sampling. Additionally, we propose an adaptive, data-dependent, practical MSS that, when combined with GP dynamics, also achieves sublinear regret with significantly fewer samples. We showcase the benefits of continuous-time modeling over its discrete-time counterpart, as well as our proposed adaptive MSS over standard baselines, on several applications.
</details>
<details>
<summary>摘要</summary>
Note: The text is translated into Simplified Chinese, which is the standard writing system used in mainland China.Please note that the translation is done by a machine and may not be perfect, especially for idiomatic expressions and cultural references.
</details></li>
</ul>
<hr>
<h2 id="On-Feynman–Kac-training-of-partial-Bayesian-neural-networks"><a href="#On-Feynman–Kac-training-of-partial-Bayesian-neural-networks" class="headerlink" title="On Feynman–Kac training of partial Bayesian neural networks"></a>On Feynman–Kac training of partial Bayesian neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19608">http://arxiv.org/abs/2310.19608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Zhao, Sebastian Mair, Thomas B. Schön, Jens Sjölund</li>
<li>for: 这个论文是为了提出一种高效的训练方法，以便在具有限制的参数 circumstance 下使用具有概率性的神经网络。</li>
<li>methods: 这种训练方法基于 Feynman–Kac 模型，并使用了继承 Monte Carlo 样本的变种来同时估计参数和秘密 posterior distribution。</li>
<li>results: 在各种 sintetic 和实际数据集上，这种训练方法比现有方法有更高的预测性能。<details>
<summary>Abstract</summary>
Recently, partial Bayesian neural networks (pBNNs), which only consider a subset of the parameters to be stochastic, were shown to perform competitively with full Bayesian neural networks. However, pBNNs are often multi-modal in the latent-variable space and thus challenging to approximate with parametric models. To address this problem, we propose an efficient sampling-based training strategy, wherein the training of a pBNN is formulated as simulating a Feynman--Kac model. We then describe variations of sequential Monte Carlo samplers that allow us to simultaneously estimate the parameters and the latent posterior distribution of this model at a tractable computational cost. We show on various synthetic and real-world datasets that our proposed training scheme outperforms the state of the art in terms of predictive performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-Kalman-Filters-Can-Filter"><a href="#Deep-Kalman-Filters-Can-Filter" class="headerlink" title="Deep Kalman Filters Can Filter"></a>Deep Kalman Filters Can Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19603">http://arxiv.org/abs/2310.19603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rishabhpahuja/Apple-Tracking">https://github.com/rishabhpahuja/Apple-Tracking</a></li>
<li>paper_authors: Blanka Hovart, Anastasis Kratsios, Yannick Limmer, Xuwei Yang</li>
<li>for: 这篇论文旨在为数字 kalman 筛（DKF）提供数学基础，以便DKF可以更加广泛地应用于数学金融中的模型调整和股票价格预测等领域。</li>
<li>methods: 这篇论文使用了一种名为 continuous-time DKF 的 neural network模型，该模型可以在基于数据的顺序序列中生成高度分布的概率测度。</li>
<li>results: 论文的结果表明，这种 continuous-time DKF 可以在一定的条件下，即 measurement noise 是可控的，以高度的准确性来实现模型的调整和股票价格预测。此外，论文还提供了一种用于评估DKF的数学基础的方法，该方法可以在具有不同概率分布的数据上进行预测。<details>
<summary>Abstract</summary>
Deep Kalman filters (DKFs) are a class of neural network models that generate Gaussian probability measures from sequential data. Though DKFs are inspired by the Kalman filter, they lack concrete theoretical ties to the stochastic filtering problem, thus limiting their applicability to areas where traditional model-based filters have been used, e.g.\ model calibration for bond and option prices in mathematical finance. We address this issue in the mathematical foundations of deep learning by exhibiting a class of continuous-time DKFs which can approximately implement the conditional law of a broad class of non-Markovian and conditionally Gaussian signal processes given noisy continuous-times measurements. Our approximation results hold uniformly over sufficiently regular compact subsets of paths, where the approximation error is quantified by the worst-case 2-Wasserstein distance computed uniformly over the given compact set of paths.
</details>
<details>
<summary>摘要</summary>
深度卡尔曼筛 (DKF) 是一类基于神经网络的模型，用于生成来自顺序数据的高 probabilistic 度量。虽然 DKF 受 Kalman 筛的影响，但它们与传统的模型基于筛法的应用领域之间没有直接的理论联系，因此其应用范围受限于传统的模型基于筛法的应用领域，例如股票和期货价格的数学金融中的模型准确性。我们在数学深度学习的基础上解决这个问题，展示了一类可以高度度量的非马歇尔分布 Signal 过程的 conditional 法则，给出了噪声 kontinuierliche 测量结果的approximation 结果。我们的approximation 结果在sufficiently 紧密的Compact 集上uniform 地保持，错误量由worst-case 2-Wasserstein 距离来量化，uniform 地计算在给定 Compact 集上的所有路径上。
</details></li>
</ul>
<hr>
<h2 id="Operator-Learning-Enhanced-Physics-informed-Neural-Networks-for-Solving-Partial-Differential-Equations-Characterized-by-Sharp-Solutions"><a href="#Operator-Learning-Enhanced-Physics-informed-Neural-Networks-for-Solving-Partial-Differential-Equations-Characterized-by-Sharp-Solutions" class="headerlink" title="Operator Learning Enhanced Physics-informed Neural Networks for Solving Partial Differential Equations Characterized by Sharp Solutions"></a>Operator Learning Enhanced Physics-informed Neural Networks for Solving Partial Differential Equations Characterized by Sharp Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19590">http://arxiv.org/abs/2310.19590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Lin, Zhiping Mao, Zhicheng Wang, George Em Karniadakis</li>
<li>for: 解决具有锐解的 partiall differential equations (PDEs) 的问题</li>
<li>methods:  combinig Physics-informed Neural Networks (PINNs) 和 Deep Operator Network (DeepONet) 方法</li>
<li>results: 提出了一种新的 Operator Learning Enhanced Physics-informed Neural Networks (OL-PINN) 方法，能够高效地解决具有锐解的 PDEs 问题，并且可以更好地处理不同类型的 boundary conditions 问题。<details>
<summary>Abstract</summary>
Physics-informed Neural Networks (PINNs) have been shown as a promising approach for solving both forward and inverse problems of partial differential equations (PDEs). Meanwhile, the neural operator approach, including methods such as Deep Operator Network (DeepONet) and Fourier neural operator (FNO), has been introduced and extensively employed in approximating solution of PDEs. Nevertheless, to solve problems consisting of sharp solutions poses a significant challenge when employing these two approaches. To address this issue, we propose in this work a novel framework termed Operator Learning Enhanced Physics-informed Neural Networks (OL-PINN). Initially, we utilize DeepONet to learn the solution operator for a set of smooth problems relevant to the PDEs characterized by sharp solutions. Subsequently, we integrate the pre-trained DeepONet with PINN to resolve the target sharp solution problem. We showcase the efficacy of OL-PINN by successfully addressing various problems, such as the nonlinear diffusion-reaction equation, the Burgers equation and the incompressible Navier-Stokes equation at high Reynolds number. Compared with the vanilla PINN, the proposed method requires only a small number of residual points to achieve a strong generalization capability. Moreover, it substantially enhances accuracy, while also ensuring a robust training process. Furthermore, OL-PINN inherits the advantage of PINN for solving inverse problems. To this end, we apply the OL-PINN approach for solving problems with only partial boundary conditions, which usually cannot be solved by the classical numerical methods, showing its capacity in solving ill-posed problems and consequently more complex inverse problems.
</details>
<details>
<summary>摘要</summary>
physics-informed neural networks (PINNs) 已经被证明为解决部分梯度方程（PDEs）的前向和反向问题的有力方法。同时，神经网络方法，包括深度运算网络（DeepONet）和傅里叶神经网络（FNO），已经被引入并广泛应用于PDEs的解的 aproximation。然而，解决具有锐度解的问题却存在一定的挑战，当使用这两种方法时。为Addressing this issue, we propose in this work a novel framework termed Operator Learning Enhanced Physics-informed Neural Networks (OL-PINN). Initially, we utilize DeepONet to learn the solution operator for a set of smooth problems relevant to the PDEs characterized by sharp solutions. Subsequently, we integrate the pre-trained DeepONet with PINN to resolve the target sharp solution problem. We showcase the efficacy of OL-PINN by successfully addressing various problems, such as the nonlinear diffusion-reaction equation, the Burgers equation, and the incompressible Navier-Stokes equation at high Reynolds number. Compared with the vanilla PINN, the proposed method requires only a small number of residual points to achieve a strong generalization capability. Moreover, it substantially enhances accuracy, while also ensuring a robust training process. Furthermore, OL-PINN inherits the advantage of PINN for solving inverse problems. To this end, we apply the OL-PINN approach for solving problems with only partial boundary conditions, which usually cannot be solved by the classical numerical methods, showing its capacity in solving ill-posed problems and consequently more complex inverse problems.
</details></li>
</ul>
<hr>
<h2 id="Modeling-Dynamics-over-Meshes-with-Gauge-Equivariant-Nonlinear-Message-Passing"><a href="#Modeling-Dynamics-over-Meshes-with-Gauge-Equivariant-Nonlinear-Message-Passing" class="headerlink" title="Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message Passing"></a>Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message Passing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19589">http://arxiv.org/abs/2310.19589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jypark0/hermes">https://github.com/jypark0/hermes</a></li>
<li>paper_authors: Jung Yeon Park, Lawson L. S. Wong, Robin Walters</li>
<li>for: 解决 computer graphics 和生物物理系统中的数据上非欧氏空间的问题，这些问题常常是 surface mesh 的离散化表示。</li>
<li>methods: 我们使用 gauge equivariant 杂交网络，这种网络可以充分利用 surface mesh 的下面结构，并且可以处理复杂非线性动力学。</li>
<li>results: 我们的新方法可以在具有高度复杂和非线性动力学的 Domain 中达到更高的性能，但是设计决策还是会偏向于 convolutional、attentional 或 message passing 网络，具体取决于任务的需求。<details>
<summary>Abstract</summary>
Data over non-Euclidean manifolds, often discretized as surface meshes, naturally arise in computer graphics and biological and physical systems. In particular, solutions to partial differential equations (PDEs) over manifolds depend critically on the underlying geometry. While graph neural networks have been successfully applied to PDEs, they do not incorporate surface geometry and do not consider local gauge symmetries of the manifold. Alternatively, recent works on gauge equivariant convolutional and attentional architectures on meshes leverage the underlying geometry but underperform in modeling surface PDEs with complex nonlinear dynamics. To address these issues, we introduce a new gauge equivariant architecture using nonlinear message passing. Our novel architecture achieves higher performance than either convolutional or attentional networks on domains with highly complex and nonlinear dynamics. However, similar to the non-mesh case, design trade-offs favor convolutional, attentional, or message passing networks for different tasks; we investigate in which circumstances our message passing method provides the most benefit.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Data over non-Euclidean manifolds, often discretized as surface meshes, naturally arise in computer graphics and biological and physical systems. In particular, solutions to partial differential equations (PDEs) over manifolds depend critically on the underlying geometry. While graph neural networks have been successfully applied to PDEs, they do not incorporate surface geometry and do not consider local gauge symmetries of the manifold. Alternatively, recent works on gauge equivariant convolutional and attentional architectures on meshes leverage the underlying geometry but underperform in modeling surface PDEs with complex nonlinear dynamics. To address these issues, we introduce a new gauge equivariant architecture using nonlinear message passing. Our novel architecture achieves higher performance than either convolutional or attentional networks on domains with highly complex and nonlinear dynamics. However, similar to the non-mesh case, design trade-offs favor convolutional, attentional, or message passing networks for different tasks; we investigate in which circumstances our message passing method provides the most benefit."中文翻译：数据在非欧几何空间上，通常为表面网格化的数据，在计算机图形和生物物理系统中自然出现。特别是，权值方程（PDE）在 manifold上的解决取决于下面的几何结构。虽然图 neural network 已经成功应用于 PDE，但它们不考虑表面几何和地方 gauge symmetry。相反，当前的 gauge 等变 convolutional 和注意力架构在网格上使用了下面的几何结构，但在模型表面 PDE 的复杂非线性动力学时表现不佳。为解决这些问题，我们引入了一种新的 gauge 等变架构，使用非线性消息传递。我们的新架构在具有高度复杂和非线性动力学的领域中表现更高效，而不同的任务的设计评估权衡会倾向于 convolutional、注意力或消息传递网络。我们在哪些情况下 investigate 我们的消息传递方法提供最大的优势。
</details></li>
</ul>
<hr>
<h2 id="Model-Uncertainty-based-Active-Learning-on-Tabular-Data-using-Boosted-Trees"><a href="#Model-Uncertainty-based-Active-Learning-on-Tabular-Data-using-Boosted-Trees" class="headerlink" title="Model Uncertainty based Active Learning on Tabular Data using Boosted Trees"></a>Model Uncertainty based Active Learning on Tabular Data using Boosted Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19573">http://arxiv.org/abs/2310.19573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharath M Shankaranarayana</li>
<li>for: 这个论文主要针对的是活动学习在表格数据上的应用，具体来说是使用树 boosting 模型在活动学习中选择最有价值的数据实例进行模型训练，并且仅对这些实例进行人类标注。</li>
<li>methods: 本文使用的方法包括 uncertainty based sampling 和 cost-effective active learning，其中 uncertainty based sampling 是根据模型预测结果的最大不确定性选择数据实例进行人类标注，而 cost-effective active learning 则是一种基于模型预测结果的成本效果进行活动学习。</li>
<li>results: 本文的实验结果表明，使用 boosted trees 模型 uncertainty 方法在活动学习中可以提高模型的准确率，而且可以避免人类标注的成本。此外，本文还提出了一种新的成本效果的活动学习方法和一种改进的成本效果的活动学习方法，可以在 regression 和 classification 任务中具有更高的效果。<details>
<summary>Abstract</summary>
Supervised machine learning relies on the availability of good labelled data for model training. Labelled data is acquired by human annotation, which is a cumbersome and costly process, often requiring subject matter experts. Active learning is a sub-field of machine learning which helps in obtaining the labelled data efficiently by selecting the most valuable data instances for model training and querying the labels only for those instances from the human annotator. Recently, a lot of research has been done in the field of active learning, especially for deep neural network based models. Although deep learning shines when dealing with image\textual\multimodal data, gradient boosting methods still tend to achieve much better results on tabular data. In this work, we explore active learning for tabular data using boosted trees. Uncertainty based sampling in active learning is the most commonly used querying strategy, wherein the labels of those instances are sequentially queried for which the current model prediction is maximally uncertain. Entropy is often the choice for measuring uncertainty. However, entropy is not exactly a measure of model uncertainty. Although there has been a lot of work in deep learning for measuring model uncertainty and employing it in active learning, it is yet to be explored for non-neural network models. To this end, we explore the effectiveness of boosted trees based model uncertainty methods in active learning. Leveraging this model uncertainty, we propose an uncertainty based sampling in active learning for regression tasks on tabular data. Additionally, we also propose a novel cost-effective active learning method for regression tasks along with an improved cost-effective active learning method for classification tasks.
</details>
<details>
<summary>摘要</summary>
超vised机器学习需要有高质量的标签数据进行模型训练。标签数据通常通过人工标注获得，这是一个费时费力的过程，经常需要专家的帮助。活动学习是机器学习的一个子领域，它帮助在获取标签数据的过程中选择最有价值的数据实例进行模型训练，并且只需要对这些实例的标签进行人工标注。在深度学习领域，最近很多研究是关于活动学习，特别是基于深度神经网络的模型。虽然深度学习在处理图像\textual\多Modal数据方面表现出色，但是梯度提升方法在标量数据上仍然具有优异的表现。在这个工作中，我们探索了基于树的活动学习方法，并使用树的模型不确定性来选择需要人工标注的数据实例。在活动学习中，通常使用不确定性来评估模型的不确定性。然而，不确定性并不是模型不确定性的直接度量。虽然在深度学习中有很多研究是关于模型不确定性的测量和应用，但是它们尚未被应用于非神经网络模型。为了解决这个问题，我们研究了基于树的模型不确定性方法在活动学习中的效iveness。此外，我们还提出了一种新的cost-effective的活动学习方法和一种改进的cost-effective的活动学习方法 для类别任务。
</details></li>
</ul>
<hr>
<h2 id="DataZoo-Streamlining-Traffic-Classification-Experiments"><a href="#DataZoo-Streamlining-Traffic-Classification-Experiments" class="headerlink" title="DataZoo: Streamlining Traffic Classification Experiments"></a>DataZoo: Streamlining Traffic Classification Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19568">http://arxiv.org/abs/2310.19568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Luxemburk, Karel Hynek</li>
<li>for: This paper is written for researchers and practitioners working in the field of network traffic classification, particularly those who are interested in using machine learning techniques for this task.</li>
<li>methods: The paper introduces a toolset called DataZoo, which provides a standardized API for accessing three extensive datasets and includes methods for feature scaling and realistic dataset partitioning.</li>
<li>results: The paper aims to address the lack of standard benchmark datasets and supportive software in the field of network traffic classification, and to provide a toolset that simplifies the creation of realistic evaluation scenarios and makes it easier to cross-compare classification methods and reproduce results.<details>
<summary>Abstract</summary>
The machine learning communities, such as those around computer vision or natural language processing, have developed numerous supportive tools and benchmark datasets to accelerate the development. In contrast, the network traffic classification field lacks standard benchmark datasets for most tasks, and the available supportive software is rather limited in scope. This paper aims to address the gap and introduces DataZoo, a toolset designed to streamline dataset management in network traffic classification and to reduce the space for potential mistakes in the evaluation setup. DataZoo provides a standardized API for accessing three extensive datasets -- CESNET-QUIC22, CESNET-TLS22, and CESNET-TLS-Year22. Moreover, it includes methods for feature scaling and realistic dataset partitioning, taking into consideration temporal and service-related factors. The DataZoo toolset simplifies the creation of realistic evaluation scenarios, making it easier to cross-compare classification methods and reproduce results.
</details>
<details>
<summary>摘要</summary>
machine learning社区，如计算机视觉或自然语言处理等，已经开发出了许多支持工具和标准 benchmark datasets，以加速开发。然而，网络流量分类领域缺乏大多数任务的标准 benchmark datasets，可用的支持软件范围很限定。这篇论文旨在填补这个差距，并引入 DataZoo，一套设计用于加速网络流量分类 dataset 管理的工具集。DataZoo 提供了访问 CESNET-QUIC22、CESNET-TLS22 和 CESNET-TLS-Year22 三个广泛的 dataset 的标准 API。此外，它还包括了考虑时间和服务相关因素的Feature scaling 和实际dataset partitioning 方法。DataZoo 工具集使得创建实际的评估enario 变得更加简单，使得cross- compare 分类方法和重复结果变得更加容易。
</details></li>
</ul>
<hr>
<h2 id="Non-parametric-regression-for-robot-learning-on-manifolds"><a href="#Non-parametric-regression-for-robot-learning-on-manifolds" class="headerlink" title="Non-parametric regression for robot learning on manifolds"></a>Non-parametric regression for robot learning on manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19561">http://arxiv.org/abs/2310.19561</a></li>
<li>repo_url: None</li>
<li>paper_authors: P. C. Lopez-Custodio, K. Bharath, A. Kucukyilmaz, S. P. Preston</li>
<li>for: 本研究旨在提出一种在拓扑空间上直接进行回归的”内在”方法，以提高 robot learning 中对 manifold-valued data 的处理精度。</li>
<li>methods: 该方法基于一种可靠的 probability distribution 在拓扑空间上，其参数函数是一个 predictor 变量，例如时间。通过一种 “local likelihood” 方法，使用 kernel 来估计该函数。</li>
<li>results: 实验结果表明，该方法在三种常见的拓扑空间上进行回归时，能够达到更高的预测精度，比 projection-based 算法更好。<details>
<summary>Abstract</summary>
Many of the tools available for robot learning were designed for Euclidean data. However, many applications in robotics involve manifold-valued data. A common example is orientation; this can be represented as a 3-by-3 rotation matrix or a quaternion, the spaces of which are non-Euclidean manifolds. In robot learning, manifold-valued data are often handled by relating the manifold to a suitable Euclidean space, either by embedding the manifold or by projecting the data onto one or several tangent spaces. These approaches can result in poor predictive accuracy, and convoluted algorithms. In this paper, we propose an "intrinsic" approach to regression that works directly within the manifold. It involves taking a suitable probability distribution on the manifold, letting its parameter be a function of a predictor variable, such as time, then estimating that function non-parametrically via a "local likelihood" method that incorporates a kernel. We name the method kernelised likelihood estimation. The approach is conceptually simple, and generally applicable to different manifolds. We implement it with three different types of manifold-valued data that commonly appear in robotics applications. The results of these experiments show better predictive accuracy than projection-based algorithms.
</details>
<details>
<summary>摘要</summary>
许多 robot 学习工具都是为欧几何数据设计的，但是许多 robotics 应用中的数据是 manifold-valued 数据。一个常见的例子是 orientation，它可以表示为 3x3 转换矩阵或 quarternion，这些空间都是非欧几何 manifold。在 robot 学习中， manifold-valued 数据通常是通过将 manifold 映射到适当的欧几何空间来处理，或者通过将数据投影到一个或多个 tangent space。这些方法可能会导致预测精度不高和算法复杂。在这篇论文中，我们提出了一种 "内在" 的方法来进行回归，即在 manifold 上直接使用适当的概率分布，让参数是一个时间变量的函数，然后使用 "本地概率" 方法来估算该函数，这种方法可以 incorporate 一个核函数。我们称之为 kernelised likelihood estimation。该方法概念简单，通用于不同的 manifold。我们在三种常见的 manifold-valued 数据中实现了这种方法，其中包括 rotation matrix、quaternion 和 pose 数据。实验结果显示，我们的方法的预测精度比投影基于的算法更高。
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-Federated-Primal-dual-Learning-for-Non-convex-and-Non-smooth-Problems-with-Model-Sparsification"><a href="#Privacy-preserving-Federated-Primal-dual-Learning-for-Non-convex-and-Non-smooth-Problems-with-Model-Sparsification" class="headerlink" title="Privacy-preserving Federated Primal-dual Learning for Non-convex and Non-smooth Problems with Model Sparsification"></a>Privacy-preserving Federated Primal-dual Learning for Non-convex and Non-smooth Problems with Model Sparsification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19558">http://arxiv.org/abs/2310.19558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwei Li, Chien-Wei Huang, Shuai Wang, Chong-Yung Chi, Tony Q. S. Quek</li>
<li>for: 该论文探讨了 Federated Learning (FL) 领域中的一类非 convex 和非光滑损失函数问题，这些问题在 FL 应用中很普遍，但是它们的复杂非 convexity 和非光滑性 nature 以及保护客户端数据隐私的矛盾要求使其成为一个挑战。</li>
<li>methods: 该论文提出了一种基于 primal-dual 算法的 Federated Learning 算法，该算法通过对模型进行 bidirectional 模型缩放来适应非 convex 和非光滑 FL 问题，同时应用了差分隐私来保证强大的隐私保证。</li>
<li>results: 该论文的实验结果表明，提出的 Federated Learning 算法在实际世界数据上具有非常出色的效果，与一些状态当前的 FL 算法相比，具有明显的优势，同时 validate 了所有的分析结果和性质。<details>
<summary>Abstract</summary>
Federated learning (FL) has been recognized as a rapidly growing research area, where the model is trained over massively distributed clients under the orchestration of a parameter server (PS) without sharing clients' data. This paper delves into a class of federated problems characterized by non-convex and non-smooth loss functions, that are prevalent in FL applications but challenging to handle due to their intricate non-convexity and non-smoothness nature and the conflicting requirements on communication efficiency and privacy protection. In this paper, we propose a novel federated primal-dual algorithm with bidirectional model sparsification tailored for non-convex and non-smooth FL problems, and differential privacy is applied for strong privacy guarantee. Its unique insightful properties and some privacy and convergence analyses are also presented for the FL algorithm design guidelines. Extensive experiments on real-world data are conducted to demonstrate the effectiveness of the proposed algorithm and much superior performance than some state-of-the-art FL algorithms, together with the validation of all the analytical results and properties.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Approximation-Theory-Computing-and-Deep-Learning-on-the-Wasserstein-Space"><a href="#Approximation-Theory-Computing-and-Deep-Learning-on-the-Wasserstein-Space" class="headerlink" title="Approximation Theory, Computing, and Deep Learning on the Wasserstein Space"></a>Approximation Theory, Computing, and Deep Learning on the Wasserstein Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19548">http://arxiv.org/abs/2310.19548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Massimo Fornasier, Pascal Heid, Giacomo Enrico Sodini</li>
<li>for: 本研究 aimed at solving the challenging problem of approximating Sobolev-smooth functions defined on probability spaces, with a focus on the Wasserstein distance function.</li>
<li>methods: 本研究 employs three machine learning-based approaches: 1) solving a finite number of optimal transport problems and computing the corresponding Wasserstein potentials; 2) using empirical risk minimization with Tikhonov regularization in Wasserstein Sobolev spaces; 3) addressing the problem through the saddle point formulation of the Tikhonov functional’s Euler-Lagrange equation.</li>
<li>results: 本研究 provides explicit and quantitative bounds on generalization errors for each of these solutions, and the numerical implementation uses appropriately designed neural networks as basis functions, which can be rapidly evaluated after training, significantly enhancing the evaluation speed and surpassing state-of-the-art methods by several orders of magnitude.<details>
<summary>Abstract</summary>
The challenge of approximating functions in infinite-dimensional spaces from finite samples is widely regarded as formidable. In this study, we delve into the challenging problem of the numerical approximation of Sobolev-smooth functions defined on probability spaces. Our particular focus centers on the Wasserstein distance function, which serves as a relevant example. In contrast to the existing body of literature focused on approximating efficiently pointwise evaluations, we chart a new course to define functional approximants by adopting three machine learning-based approaches: 1. Solving a finite number of optimal transport problems and computing the corresponding Wasserstein potentials. 2. Employing empirical risk minimization with Tikhonov regularization in Wasserstein Sobolev spaces. 3. Addressing the problem through the saddle point formulation that characterizes the weak form of the Tikhonov functional's Euler-Lagrange equation. As a theoretical contribution, we furnish explicit and quantitative bounds on generalization errors for each of these solutions. In the proofs, we leverage the theory of metric Sobolev spaces and we combine it with techniques of optimal transport, variational calculus, and large deviation bounds. In our numerical implementation, we harness appropriately designed neural networks to serve as basis functions. These networks undergo training using diverse methodologies. This approach allows us to obtain approximating functions that can be rapidly evaluated after training. Consequently, our constructive solutions significantly enhance at equal accuracy the evaluation speed, surpassing that of state-of-the-art methods by several orders of magnitude.
</details>
<details>
<summary>摘要</summary>
《 Sobolev 函数的数值近似在无穷数据时的挑战》在这项研究中，我们围绕 Sobolev 平滑函数定义在概率空间上的数值近似问题进行了深入的研究。我们的特点是通过三种机器学习基于方法来定义函数近似：1. 解决有限多个交通问题，并计算相应的 Wasserstein 潜在函数。2. 使用 Wasserstein Sobolev 空间中的empirical risk minimization，并添加 Tikhonov 正则化。3. 通过极点形式来表示 Tikhonov 函数的弱形Euler-Lagrange方程。我们在证明中利用了度量 Sobolev 空间的理论，并结合了优化交通、变量 calculus 和大偏度上界。我们的数值实现中使用了适当设计的神经网络作为基函数。这些神经网络在训练过程中进行了改进，以便在训练后快速计算近似函数。因此，我们的构建解决方案可以快速提高评估速度，超过当前方法的几个数量级。
</details></li>
</ul>
<hr>
<h2 id="On-consequences-of-finetuning-on-data-with-highly-discriminative-features"><a href="#On-consequences-of-finetuning-on-data-with-highly-discriminative-features" class="headerlink" title="On consequences of finetuning on data with highly discriminative features"></a>On consequences of finetuning on data with highly discriminative features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19537">http://arxiv.org/abs/2310.19537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wojciech Masarczyk, Tomasz Trzciński, Mateusz Ostaszewski</li>
<li>for: 这篇论文旨在探讨在转移学习时，神经网络是如何保留先前学习的知识，以便用于新任务。</li>
<li>methods: 这篇论文使用了转移学习的方法，并分析了其对神经网络性能和内部表示的影响。</li>
<li>results: 研究发现，在使用转移学习时，神经网络可能会忽略有价值的先前学习特征，导致性能下降。这种现象被称为“特征衰变”。<details>
<summary>Abstract</summary>
In the era of transfer learning, training neural networks from scratch is becoming obsolete. Transfer learning leverages prior knowledge for new tasks, conserving computational resources. While its advantages are well-documented, we uncover a notable drawback: networks tend to prioritize basic data patterns, forsaking valuable pre-learned features. We term this behavior "feature erosion" and analyze its impact on network performance and internal representations.
</details>
<details>
<summary>摘要</summary>
在转移学习时代，从头开始训练神经网络已经变得过时。转移学习利用了先前学习的知识来解决新任务，并节省计算资源。虽然它的优点已经得到了广泛的文献记录，但我们发现了一个重要的缺点：神经网络往往会强调基本数据模式，抛弃价值的先前学习特征。我们称这种行为为“特征衰变”，并分析它对网络性能和内部表示的影响。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Batch-Inverse-Reinforcement-Learning-Learn-to-Reward-from-Imperfect-Demonstration-for-Interactive-Recommendation"><a href="#Adversarial-Batch-Inverse-Reinforcement-Learning-Learn-to-Reward-from-Imperfect-Demonstration-for-Interactive-Recommendation" class="headerlink" title="Adversarial Batch Inverse Reinforcement Learning: Learn to Reward from Imperfect Demonstration for Interactive Recommendation"></a>Adversarial Batch Inverse Reinforcement Learning: Learn to Reward from Imperfect Demonstration for Interactive Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19536">http://arxiv.org/abs/2310.19536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Liu, Xinyan Su, Zeyu He, Xiangyu Zhao, Jun Li</li>
<li>for: 本研究旨在解决学习奖励（LTR）问题，它是基于奖励学习的核心问题。在过去的方法中， Either introduce additional procedures for learning to reward, thereby increasing the complexity of optimization, or assume that user-agent interactions provide perfect demonstrations, which is not feasible in practice.</li>
<li>methods: 我们提议一种批量反式学习 paradigma，可以同时优化奖励和策略。我们使用折扣站点分布 corrections来结合LTR和推荐客户端评估。为了满足compositional requirement，我们引入了对照保守的概念，并通过bellman transform和KL regularization来约束 consecutive policy updates。</li>
<li>results: 我们在两个实际数据集上进行了empirical studies，结果表明，提议的方法可以相对提高效果（2.3%）和效率（11.53%）。<details>
<summary>Abstract</summary>
Rewards serve as a measure of user satisfaction and act as a limiting factor in interactive recommender systems. In this research, we focus on the problem of learning to reward (LTR), which is fundamental to reinforcement learning. Previous approaches either introduce additional procedures for learning to reward, thereby increasing the complexity of optimization, or assume that user-agent interactions provide perfect demonstrations, which is not feasible in practice. Ideally, we aim to employ a unified approach that optimizes both the reward and policy using compositional demonstrations. However, this requirement presents a challenge since rewards inherently quantify user feedback on-policy, while recommender agents approximate off-policy future cumulative valuation. To tackle this challenge, we propose a novel batch inverse reinforcement learning paradigm that achieves the desired properties. Our method utilizes discounted stationary distribution correction to combine LTR and recommender agent evaluation. To fulfill the compositional requirement, we incorporate the concept of pessimism through conservation. Specifically, we modify the vanilla correction using Bellman transformation and enforce KL regularization to constrain consecutive policy updates. We use two real-world datasets which represent two compositional coverage to conduct empirical studies, the results also show that the proposed method relatively improves both effectiveness (2.3\%) and efficiency (11.53\%)
</details>
<details>
<summary>摘要</summary>
奖励 serve as a measure of user satisfaction and act as a limiting factor in interactive recommender systems. 在这些研究中，我们关注了学习奖励（LTR）问题，这是基本的再奖学习问题。先前的方法可能会添加额外的过程来学习奖励，从而增加优化复杂度，或者假设用户-代理交互提供完美的示范，这并不是实际情况。理想情况是使用一种统一的方法，同时优化奖励和策略使用 Compositional demonstrations。然而，这种需求呈现出一个挑战，因为奖励自然地衡量用户反馈on-policy，而推荐代理 approximates off-policy future cumulative valuation。为解决这个挑战，我们提出了一种新的批量反向学习 paradigm。我们的方法使用折扣站立分布 corrections  combinese LTR和推荐代理评估。为了满足compositional requirement，我们在 vanilla correction 中添加了 Bellman transformation 和 KL regularization，以限制 consecutive policy updates。我们使用两个实际数据集，代表两种compositional coverage，进行empirical studies，结果还显示了我们提posed方法相对提高了效iveness（2.3%）和效率（11.53%）。
</details></li>
</ul>
<hr>
<h2 id="Decoupled-Actor-Critic"><a href="#Decoupled-Actor-Critic" class="headerlink" title="Decoupled Actor-Critic"></a>Decoupled Actor-Critic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19527">http://arxiv.org/abs/2310.19527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Nauman, Marek Cygan</li>
<li>for: 本研究旨在解决actor-critic方法的两个矛盾问题，即评价器倾向于过度估计，需要从保守策略优化的下界Q值中采样 temporal-difference 目标；同时，已知的结果表明在不确定性面前，乐观的策略具有更低的 regret 水平。</li>
<li>methods: 我们提出了一种叫做Decoupled Actor-Critic（DAC）的离散actor-critic方法，该方法通过梯度反propagation来学习两种不同的演员：一个保守的演员用于 temporal-difference 学习，另一个乐观的演员用于探索。</li>
<li>results: 我们在DeepMind Control任务中进行了low和high replay ratio regime的测试，并对多个设计选择进行了简洁。 despite minimal computational overhead, DAC在locomotion任务上实现了state-of-the-art perfomance和样本效率。<details>
<summary>Abstract</summary>
Actor-Critic methods are in a stalemate of two seemingly irreconcilable problems. Firstly, critic proneness towards overestimation requires sampling temporal-difference targets from a conservative policy optimized using lower-bound Q-values. Secondly, well-known results show that policies that are optimistic in the face of uncertainty yield lower regret levels. To remedy this dichotomy, we propose Decoupled Actor-Critic (DAC). DAC is an off-policy algorithm that learns two distinct actors by gradient backpropagation: a conservative actor used for temporal-difference learning and an optimistic actor used for exploration. We test DAC on DeepMind Control tasks in low and high replay ratio regimes and ablate multiple design choices. Despite minimal computational overhead, DAC achieves state-of-the-art performance and sample efficiency on locomotion tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generator-Identification-for-Linear-SDEs-with-Additive-and-Multiplicative-Noise"><a href="#Generator-Identification-for-Linear-SDEs-with-Additive-and-Multiplicative-Noise" class="headerlink" title="Generator Identification for Linear SDEs with Additive and Multiplicative Noise"></a>Generator Identification for Linear SDEs with Additive and Multiplicative Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19491">http://arxiv.org/abs/2310.19491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Wang, Xi Geng, Wei Huang, Biwei Huang, Mingming Gong</li>
<li>for: 本研究探讨了如何从解决方程的分布来确定生成器的线性随机振荡方程（SDE）的生成器。</li>
<li>methods: 本研究使用了线性SDE的解决方程的分布来确定生成器。</li>
<li>results: 本研究提出了线性SDE的生成器确定性条件，包括加法噪声和乘法噪声两种类型的条件。这些条件是必要和足够的，可以用来在 causal inference 中识别生成器。此外，研究还提供了这些条件的 geometric 解释，以便更好地理解。<details>
<summary>Abstract</summary>
In this paper, we present conditions for identifying the generator of a linear stochastic differential equation (SDE) from the distribution of its solution process with a given fixed initial state. These identifiability conditions are crucial in causal inference using linear SDEs as they enable the identification of the post-intervention distributions from its observational distribution. Specifically, we derive a sufficient and necessary condition for identifying the generator of linear SDEs with additive noise, as well as a sufficient condition for identifying the generator of linear SDEs with multiplicative noise. We show that the conditions derived for both types of SDEs are generic. Moreover, we offer geometric interpretations of the derived identifiability conditions to enhance their understanding. To validate our theoretical results, we perform a series of simulations, which support and substantiate the established findings.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了线性随机杂分方程（SDE）生成器的标识条件，基于给定的固定初始状态的解过程的分布。这些标识条件是 causal inference 中 linear SDE 的关键因素，它们允许将 post-intervention 分布从 observational 分布中标识出来。我们得到了线性 SDE 加法噪声的必要和 suficient 条件，以及线性 SDE 乘法噪声的 suficient 条件。我们发现这些条件对于两种类型的 SDE 都是通用的。此外，我们还提供了这些标识条件的几何 интерпретаción，以增强它们的理解。为验证我们的理论结果，我们进行了一系列的仿真实验，支持和证明了我们的发现。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Meta-Learning-Based-KKL-Observer-Design-for-Nonlinear-Dynamical-Systems"><a href="#Adaptive-Meta-Learning-Based-KKL-Observer-Design-for-Nonlinear-Dynamical-Systems" class="headerlink" title="Adaptive Meta-Learning-Based KKL Observer Design for Nonlinear Dynamical Systems"></a>Adaptive Meta-Learning-Based KKL Observer Design for Nonlinear Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19489">http://arxiv.org/abs/2310.19489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Trommer, Halil Yigit Oksuz</li>
<li>for: 用于非线性系统状态估计</li>
<li>methods: 使用机器学习的meta-学习方法和非线性变换Map来设计一个可靠的状态估计器</li>
<li>results: 通过实验结果，显示了高精度、泛化能力和噪声鲁棒性，并且可以在不同的系统条件和属性下进行在线适应<details>
<summary>Abstract</summary>
The theory of Kazantzis-Kravaris/Luenberger (KKL) observer design introduces a methodology that uses a nonlinear transformation map and its left inverse to estimate the state of a nonlinear system through the introduction of a linear observer state space. Data-driven approaches using artificial neural networks have demonstrated the ability to accurately approximate these transformation maps. This paper presents a novel approach to observer design for nonlinear dynamical systems through meta-learning, a concept in machine learning that aims to optimize learning models for fast adaptation to a distribution of tasks through an improved focus on the intrinsic properties of the underlying learning problem. We introduce a framework that leverages information from measurements of the system output to design a learning-based KKL observer capable of online adaptation to a variety of system conditions and attributes. To validate the effectiveness of our approach, we present comprehensive experimental results for the estimation of nonlinear system states with varying initial conditions and internal parameters, demonstrating high accuracy, generalization capability, and robustness against noise.
</details>
<details>
<summary>摘要</summary>
基于Kazantzis-Kravaris/Luenberger（KKL）观察器设计理论，本文提出了一种基于机器学习的观察器设计方法，通过非线性变换Map和其左逆函数来估算非线性系统的状态。使用人工神经网络进行数据驱动的方法已经证明了高度准确地表示这些变换 Map。本文提出了一种基于meta-学习的观察器设计方法，通过更好地理解下面学习问题的内在特性来优化学习模型，以便快速适应多种任务的分布。我们提出了一种基于测量系统输出信息的框架，用于设计一种可在线适应系统条件和特性的学习基于KKL观察器。为验证我们的方法的有效性，我们在不同的初始条件和内部参数下进行了广泛的实验，并达到了高精度、泛化能力和鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="Grokking-Tickets-Lottery-Tickets-Accelerate-Grokking"><a href="#Grokking-Tickets-Lottery-Tickets-Accelerate-Grokking" class="headerlink" title="Grokking Tickets: Lottery Tickets Accelerate Grokking"></a>Grokking Tickets: Lottery Tickets Accelerate Grokking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19470">http://arxiv.org/abs/2310.19470</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gouki510/grokking-tickets">https://github.com/gouki510/grokking-tickets</a></li>
<li>paper_authors: Gouki Minegishi, Yusuke Iwasawa, Yutaka Matsuo</li>
<li>for: 这篇论文探讨了神经网络通过’’感知’’（Grokking）过程中的急剧改善，即一个网络从初始的记忆化解决方案转化为完美的通用解决方案。</li>
<li>methods: 作者使用了’’抽奖票’’（Grokking tickets）来描述这个过程的关键，即在精炼网络中找到’’好’’的子网络（good sparse subnetworks），以加速感知过程。</li>
<li>results: 作者通过多种配置（MLP和Transformer，以及数学和图像分类任务）和比较’’抽奖票’’和 dense 网络来证明 ‘’抽奖票’’ 的重要性，并发现在适当的剪除率下，感知可以在不使用 weight decay 的情况下完成。<details>
<summary>Abstract</summary>
Grokking is one of the most surprising puzzles in neural network generalization: a network first reaches a memorization solution with perfect training accuracy and poor generalization, but with further training, it reaches a perfectly generalized solution. We aim to analyze the mechanism of grokking from the lottery ticket hypothesis, identifying the process to find the lottery tickets (good sparse subnetworks) as the key to describing the transitional phase between memorization and generalization. We refer to these subnetworks as ''Grokking tickets'', which is identified via magnitude pruning after perfect generalization. First, using ''Grokking tickets'', we show that the lottery tickets drastically accelerate grokking compared to the dense networks on various configurations (MLP and Transformer, and an arithmetic and image classification tasks). Additionally, to verify that ''Grokking ticket'' are a more critical factor than weight norms, we compared the ''good'' subnetworks with a dense network having the same L1 and L2 norms. Results show that the subnetworks generalize faster than the controlled dense model. In further investigations, we discovered that at an appropriate pruning rate, grokking can be achieved even without weight decay. We also show that speedup does not happen when using tickets identified at the memorization solution or transition between memorization and generalization or when pruning networks at the initialization (Random pruning, Grasp, SNIP, and Synflow). The results indicate that the weight norm of network parameters is not enough to explain the process of grokking, but the importance of finding good subnetworks to describe the transition from memorization to generalization. The implementation code can be accessed via this link: \url{https://github.com/gouki510/Grokking-Tickets}.
</details>
<details>
<summary>摘要</summary>
干脆是神经网络泛化的一个最有趣的谜题：一个网络在完美地训练时达到了记忆解决方案，但是在进一步训练时达到了完美的泛化解决方案。我们想要分析干脆的机制，从抽奖签分 hypothesis开始，identify the process of finding good sparse subnetworks（即''Grokking tickets''）作为描述 transition phase between memorization and generalization的关键。我们将这些子网络称为''干脆签''，通过 magnitude pruning 后的完美泛化来标识。我们发现，使用 ''干脆签'' 可以在不同的配置（MLP和Transformer，以及数学和图像分类任务）上加速干脆，并且比 dense network 更快。此外，我们还发现，在适当的剪枝率下，可以通过剪枝来实现干脆，而不需要weight decay。此外，我们发现，在使用 ''干脆签'' 时，速度不会增加，而是在初始化（Random pruning、Grasp、SNIP和Synflow）时剪枝时才会增加速度。这些结果表明，网络参数的重量 нор 不能完全解释干脆的过程，而是找到好的 sparse subnetworks 来描述 transition phase between memorization and generalization 的重要性。实现代码可以通过以下链接获取：https://github.com/gouki510/Grokking-Tickets。
</details></li>
</ul>
<hr>
<h2 id="Regret-Minimization-Algorithms-for-Multi-Agent-Cooperative-Learning-Systems"><a href="#Regret-Minimization-Algorithms-for-Multi-Agent-Cooperative-Learning-Systems" class="headerlink" title="Regret-Minimization Algorithms for Multi-Agent Cooperative Learning Systems"></a>Regret-Minimization Algorithms for Multi-Agent Cooperative Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19468">http://arxiv.org/abs/2310.19468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Yi</li>
<li>For: 这个论文的目的是研究多智能合作学习（MACL）系统在Sequential Decision Making（SDM）问题上的设计和分析。* Methods: 本论文使用了多智能合作学习算法来解决SDM问题，并研究了不同的sequential decision making问题，包括多智能多投机问题、全信息或投机反馈问题等。* Results: 本论文提出了一系列的 regret lower bounds，用于衡量多智能合作学习算法在SDM问题上的性能。这些 regret lower bounds 适用于不同的communication network和communication delay情况，可以帮助设计MACL系统的communication协议。<details>
<summary>Abstract</summary>
A Multi-Agent Cooperative Learning (MACL) system is an artificial intelligence (AI) system where multiple learning agents work together to complete a common task. Recent empirical success of MACL systems in various domains (e.g. traffic control, cloud computing, robotics) has sparked active research into the design and analysis of MACL systems for sequential decision making problems. One important metric of the learning algorithm for decision making problems is its regret, i.e. the difference between the highest achievable reward and the actual reward that the algorithm gains. The design and development of a MACL system with low-regret learning algorithms can create huge economic values. In this thesis, I analyze MACL systems for different sequential decision making problems. Concretely, the Chapter 3 and 4 investigate the cooperative multi-agent multi-armed bandit problems, with full-information or bandit feedback, in which multiple learning agents can exchange their information through a communication network and the agents can only observe the rewards of the actions they choose. Chapter 5 considers the communication-regret trade-off for online convex optimization in the distributed setting. Chapter 6 discusses how to form high-productive teams for agents based on their unknown but fixed types using adaptive incremental matchings. For the above problems, I present the regret lower bounds for feasible learning algorithms and provide the efficient algorithms to achieve this bound. The regret bounds I present in Chapter 3, 4 and 5 quantify how the regret depends on the connectivity of the communication network and the communication delay, thus giving useful guidance on design of the communication protocol in MACL systems
</details>
<details>
<summary>摘要</summary>
一个多智能合作学习（MACL）系统是一个人工智能（AI）系统，其中多个学习代理共同完成一项共同任务。在不同领域（如交通控制、云计算、机器人等）的实践成功已经引起了关于MACL系统的设计和分析的活跃研究。一个重要的学习算法度量 для决策问题是它的 regret，即实际获得的奖励与最高可能获得的奖励之差。设计和开发一个MACL系统 WITH low-regret 学习算法可以创造巨大的经济价值。在这个论文中，我分析了MACL系统在不同的决策问题上。特别是第3章和第4章研究了多智能合作多臂投机问题，包括全信息或投机反馈，在其中多个学习代理可以通过通信网络进行信息交换，代理只能观察它们选择的动作的奖励。第5章考虑了在分布式环境中的通信 regret 与满意度之间的贸易。第6章讨论了如何基于代理的未知但固定类型形成高产力团队，使用适应增量匹配。对于这些问题，我提供了可行的学习算法的 regret 下界，并提供了有效的算法来实现这个下界。我在第3、4、5章中提供的 regret 下界表明了通信网络的连接性和通信延迟对 regret 的影响，因此为设计通信协议的设计提供了有用的指导。
</details></li>
</ul>
<hr>
<h2 id="MMM-and-MMMSynth-Clustering-of-heterogeneous-tabular-data-and-synthetic-data-generation"><a href="#MMM-and-MMMSynth-Clustering-of-heterogeneous-tabular-data-and-synthetic-data-generation" class="headerlink" title="MMM and MMMSynth: Clustering of heterogeneous tabular data, and synthetic data generation"></a>MMM and MMMSynth: Clustering of heterogeneous tabular data, and synthetic data generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19454">http://arxiv.org/abs/2310.19454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chandrani Kumari, Rahul Siddharthan<br>for:This paper proposes new algorithms for two tasks related to heterogeneous tabular datasets: clustering and synthetic data generation.methods:The proposed algorithms use an EM-based clustering method called MMM (Madras Mixture Model) and a synthetic data generation method called MMMsynth, which pre-clusters the input data and generates cluster-wise synthetic data assuming cluster-specific data distributions for the input columns.results:The proposed algorithms outperform standard algorithms in determining clusters in synthetic heterogeneous data and recover structure in real data. The synthetic tabular data generation algorithm approaches the performance of training purely with real data and outperforms other literature tabular-data generators.<details>
<summary>Abstract</summary>
We provide new algorithms for two tasks relating to heterogeneous tabular datasets: clustering, and synthetic data generation. Tabular datasets typically consist of heterogeneous data types (numerical, ordinal, categorical) in columns, but may also have hidden cluster structure in their rows: for example, they may be drawn from heterogeneous (geographical, socioeconomic, methodological) sources, such that the outcome variable they describe (such as the presence of a disease) may depend not only on the other variables but on the cluster context. Moreover, sharing of biomedical data is often hindered by patient confidentiality laws, and there is current interest in algorithms to generate synthetic tabular data from real data, for example via deep learning.   We demonstrate a novel EM-based clustering algorithm, MMM (``Madras Mixture Model''), that outperforms standard algorithms in determining clusters in synthetic heterogeneous data, and recovers structure in real data. Based on this, we demonstrate a synthetic tabular data generation algorithm, MMMsynth, that pre-clusters the input data, and generates cluster-wise synthetic data assuming cluster-specific data distributions for the input columns. We benchmark this algorithm by testing the performance of standard ML algorithms when they are trained on synthetic data and tested on real published datasets. Our synthetic data generation algorithm outperforms other literature tabular-data generators, and approaches the performance of training purely with real data.
</details>
<details>
<summary>摘要</summary>
我们提供了新的算法对异构表格数据进行划分和生成 sintetico 数据。表格数据通常包含异构数据类型（数值、排序、分类）的列，但也可能具有隐藏的划分结构在行中：例如，它们可能来自不同的地理、社会经济、方法ологических 源泉，因此结果变量（例如疾病存在）可能会受到其他变量以及划分上下文的影响。此外，生物医学数据分享受到患者隐私法规的限制，现在有兴趣于使用深度学习生成 sintetico 表格数据。我们描述了一种新的EM基于的划分算法，MMM（Madras Mixture Model），它在异构数据上比标准算法更高效地划分划分结构，并在实际数据上回归结构。基于这种算法，我们提出了一种生成 sintetico 表格数据的算法，MMMsynth，它先划分输入数据，然后为每个划分群分别生成假数据，假设每列的数据分布为划分群pecific的数据分布。我们对这种算法进行了 benchmark，测试了使用生成的 sintetico 数据训练标准机器学习算法，并测试在实际发表的数据上。我们发现，我们的 sintetico 数据生成算法在与标准Literatura 表格数据生成算法进行比较时表现更好，并且在训练只使用实际数据时的性能接近于使用实际数据进行训练。
</details></li>
</ul>
<hr>
<h2 id="Hodge-Compositional-Edge-Gaussian-Processes"><a href="#Hodge-Compositional-Edge-Gaussian-Processes" class="headerlink" title="Hodge-Compositional Edge Gaussian Processes"></a>Hodge-Compositional Edge Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19450">http://arxiv.org/abs/2310.19450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maosheng Yang, Viacheslav Borovitskiy, Elvin Isufi</li>
<li>for: 本文提出了基于 Gaussian processes（GPs）的方法，用于模型 simplicial 2-complex 上的函数，这种结构类似于图，但允许边界形成三角形面。这种方法适用于学习网络上的流数据，其边界流可以表示为离散异常和旋流的积分。</li>
<li>methods: 本文首先开发了 divergence-free 和 curl-free 边GPs 类，适用于不同应用场景。然后， authors 将这些类合并成 \emph{Hodge-compositional edge GPs}，可以表示任何边函数。这些 GPs 允许直接和独立地学习不同 Hodge  ком成分的边函数，以便在 hyperparameter 优化中捕捉它们的相关性。</li>
<li>results:  authors 在 currency exchange, ocean flows 和 water supply 网络中应用了这些 GPs，与其他模型进行比较。结果表明，Hodge-compositional edge GPs 能够更好地捕捉流数据的特点，并且在 hyperparameter 优化中表现更好。<details>
<summary>Abstract</summary>
We propose principled Gaussian processes (GPs) for modeling functions defined over the edge set of a simplicial 2-complex, a structure similar to a graph in which edges may form triangular faces. This approach is intended for learning flow-type data on networks where edge flows can be characterized by the discrete divergence and curl. Drawing upon the Hodge decomposition, we first develop classes of divergence-free and curl-free edge GPs, suitable for various applications. We then combine them to create \emph{Hodge-compositional edge GPs} that are expressive enough to represent any edge function. These GPs facilitate direct and independent learning for the different Hodge components of edge functions, enabling us to capture their relevance during hyperparameter optimization. To highlight their practical potential, we apply them for flow data inference in currency exchange, ocean flows and water supply networks, comparing them to alternative models.
</details>
<details>
<summary>摘要</summary>
我们提出原理式加aussian processes（GP）来模型 simplicial 2-complex 上的函数，这种结构类似于图，在其边界上可能会形成三角形面。这种方法适用于学习网络上流体数据，其边界流可以通过离散异变和旋转来描述。基于哈代分解，我们首先开发了不同的漏斗和旋转自由边GP，适用于各种应用。然后，我们将它们组合成\emph{Hodge-compositional edge GP}，可以表示任何边函数。这些GP 使得边函数的不同哈代组件可以独立地学习，以便捕捉其相关性 durante 超参数优化。为了强调其实用性，我们在货币交易、海洋流和水利网络中应用了它们，并与其他模型进行比较。
</details></li>
</ul>
<hr>
<h2 id="A-Federated-Learning-Framework-for-Stenosis-Detection"><a href="#A-Federated-Learning-Framework-for-Stenosis-Detection" class="headerlink" title="A Federated Learning Framework for Stenosis Detection"></a>A Federated Learning Framework for Stenosis Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19445">http://arxiv.org/abs/2310.19445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariachiara Di Cosmo, Giovanna Migliorelli, Matteo Francioni, Andi Mucaj, Alessandro Maolo, Alessandro Aprile, Emanuele Frontoni, Maria Chiara Fiorentino, Sara Moccia</li>
<li>for: 这项研究探讨了基于联合学习（Federated Learning，FL）的 coronary angiography 图像（CA）中的狭窄血管检测。</li>
<li>methods: 我们使用了一种基于 Faster R-CNN 模型的狭窄血管检测方法，并在两个不同机构的数据上进行了训练。在我们的 FL 框架中，只有模型背部的参数被共享，使用了联合平均（FedAvg）来聚合参数。</li>
<li>results: 我们的结果表明，FL 框架对客户端 2 的性能没有显著影响， client 2 的本地训练已经达到了良好的性能。而对 client 1 来说，FL 框架提高了性能，具体来说，与本地模型相比，FL 框架提高了 +3.76%、+17.21% 和 +10.80% 的精度、回归率和 F1 分别。最终，我们达到了 P rec &#x3D; 73.56，Rec &#x3D; 67.01 和 F1 &#x3D; 70.13。这些结果表明，FL 可以帮助多中心研究自动 CA 图像中的狭窄血管检测，同时保持患者隐私。<details>
<summary>Abstract</summary>
This study explores the use of Federated Learning (FL) for stenosis detection in coronary angiography images (CA). Two heterogeneous datasets from two institutions were considered: Dataset 1 includes 1219 images from 200 patients, which we acquired at the Ospedale Riuniti of Ancona (Italy); Dataset 2 includes 7492 sequential images from 90 patients from a previous study available in the literature. Stenosis detection was performed by using a Faster R-CNN model. In our FL framework, only the weights of the model backbone were shared among the two client institutions, using Federated Averaging (FedAvg) for weight aggregation. We assessed the performance of stenosis detection using Precision (P rec), Recall (Rec), and F1 score (F1). Our results showed that the FL framework does not substantially affects clients 2 performance, which already achieved good performance with local training; for client 1, instead, FL framework increases the performance with respect to local model of +3.76%, +17.21% and +10.80%, respectively, reaching P rec = 73.56, Rec = 67.01 and F1 = 70.13. With such results, we showed that FL may enable multicentric studies relevant to automatic stenosis detection in CA by addressing data heterogeneity from various institutions, while preserving patient privacy.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Federated Learning" (FL) is translated as "合作学习" (hezuo xuexí)* "coronary angiography images" (CA) is translated as "心血管影像" (xin xue kan yingxiang)* "stenosis detection" is translated as "stenosis检测" (stenosis jiandete)* "Faster R-CNN" is translated as "更快的R-CNN" (gengkuai de R-CNN)* "Precision" (Precision), "Recall" (Recall), and "F1 score" (F1) are translated as "准确率" (zhèngzhèng lǐ), "回归率" (huíguī lǐ), and "F1分数" (F1 fēnshū) respectively.* "client institutions" is translated as "客户机构" (kehu jigou)* "local training" is translated as "本地训练" (ben di xùntraining)* "FL framework" is translated as "FL框架" (FL kuàikāng)* "weight aggregation" is translated as "weight合并" (weight hebing)
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-Diffusion-Based-Channel-Adaptive-Secure-Wireless-Semantic-Communications"><a href="#Asymmetric-Diffusion-Based-Channel-Adaptive-Secure-Wireless-Semantic-Communications" class="headerlink" title="Asymmetric Diffusion Based Channel-Adaptive Secure Wireless Semantic Communications"></a>Asymmetric Diffusion Based Channel-Adaptive Secure Wireless Semantic Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19439">http://arxiv.org/abs/2310.19439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xintian Ren, Jun Wu, Hansong Xu, Qianqian Pan<br>for:The paper aims to address the security vulnerabilities of semantic communication systems by proposing a secure semantic communication system called DiffuSeC.methods:The proposed system utilizes a diffusion model and deep reinforcement learning (DRL) to mitigate semantic perturbations, including data source attacks and channel attacks. Additionally, a DRL-based channel-adaptive diffusion step selection scheme is developed to improve robustness under unstable channel conditions.results:The proposed DiffuSeC system demonstrates higher robust accuracy than previous works under a wide range of channel conditions, and can quickly adjust the model state according to signal-to-noise ratios (SNRs) in unstable environments.<details>
<summary>Abstract</summary>
Semantic communication has emerged as a new deep learning-based communication paradigm that drives the research of end-to-end data transmission in tasks like image classification, and image reconstruction. However, the security problem caused by semantic attacks has not been well explored, resulting in vulnerabilities within semantic communication systems exposed to potential semantic perturbations. In this paper, we propose a secure semantic communication system, DiffuSeC, which leverages the diffusion model and deep reinforcement learning (DRL) to address this issue. With the diffusing module in the sender end and the asymmetric denoising module in the receiver end, the DiffuSeC mitigates the perturbations added by semantic attacks, including data source attacks and channel attacks. To further improve the robustness under unstable channel conditions caused by semantic attacks, we developed a DRL-based channel-adaptive diffusion step selection scheme to achieve stable performance under fluctuating environments. A timestep synchronization scheme is designed for diffusion timestep coordination between the two ends. Simulation results demonstrate that the proposed DiffuSeC shows higher robust accuracy than previous works under a wide range of channel conditions, and can quickly adjust the model state according to signal-to-noise ratios (SNRs) in unstable environments.
</details>
<details>
<summary>摘要</summary>
《含义通信：一种新的深度学习基于的通信模式》（DiffuSeC）是一种新的安全含义通信系统，它利用扩散模型和深度强化学习（DRL）来解决含义攻击的安全问题。在发送端有扩散模块，而接收端有非对称减噪模块，DiffuSeC可以减轻由含义攻击引起的扰动。为了进一步提高不稳定的通信环境下的稳定性，我们开发了基于DRL的通道适应扩散步选择策略，以实现在不稳定的环境下稳定的性能。同时，我们还设计了一种时间步同步方案，以确保扩散步的协调。实验结果表明，提案的DiffuSeC在各种通信环境下 exhibits higher robust accuracy than previous works, and can quickly adjust model state according to signal-to-noise ratios (SNRs) in unstable environments.
</details></li>
</ul>
<hr>
<h2 id="LightSAGE-Graph-Neural-Networks-for-Large-Scale-Item-Retrieval-in-Shopee’s-Advertisement-Recommendation"><a href="#LightSAGE-Graph-Neural-Networks-for-Large-Scale-Item-Retrieval-in-Shopee’s-Advertisement-Recommendation" class="headerlink" title="LightSAGE: Graph Neural Networks for Large Scale Item Retrieval in Shopee’s Advertisement Recommendation"></a>LightSAGE: Graph Neural Networks for Large Scale Item Retrieval in Shopee’s Advertisement Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19394">http://arxiv.org/abs/2310.19394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dang Minh Nguyen, Chenfei Wang, Yan Shen, Yifan Zeng</li>
<li>for: 这个研究旨在应用图形神经网络（GNN）解决大规模电子商务推荐中的项目搜寻问题。</li>
<li>methods: 我们使用了一种简单 yet novel和具有影响力的图构成、模型和资料偏好处理技术。具体来说，我们结合强信号用户行为和高精度共同推荐（CF）算法来建立高质量的项目图。然后，我们开发了一个名为LightSAGE的新的GNN架构，以生成高品质的项目嵌入 Vector 搜寻。</li>
<li>results: 我们的模型在线上评估、A&#x2F;B 测试和生产环境中获得改善，并且在Shopee 的推荐广告系统中部署。<details>
<summary>Abstract</summary>
Graph Neural Network (GNN) is the trending solution for item retrieval in recommendation problems. Most recent reports, however, focus heavily on new model architectures. This may bring some gaps when applying GNN in the industrial setup, where, besides the model, constructing the graph and handling data sparsity also play critical roles in the overall success of the project. In this work, we report how GNN is applied for large-scale e-commerce item retrieval at Shopee. We introduce our simple yet novel and impactful techniques in graph construction, modeling, and handling data skewness. Specifically, we construct high-quality item graphs by combining strong-signal user behaviors with high-precision collaborative filtering (CF) algorithm. We then develop a new GNN architecture named LightSAGE to produce high-quality items' embeddings for vector search. Finally, we design multiple strategies to handle cold-start and long-tail items, which are critical in an advertisement (ads) system. Our models bring improvement in offline evaluations, online A/B tests, and are deployed to the main traffic of Shopee's Recommendation Advertisement system.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 是当前推荐问题的流行解决方案。然而，最新的报告强调新的模型架构，这可能会导致在实际应用中，除了模型外，构建图和处理数据稀缺性也发挥关键作用。在这项工作中，我们报道了在大规模电商Item Retrieval中使用GNN的应用。我们介绍了我们的简单 yet novel和重要的图建构、模型化和数据偏好技术。具体来说，我们组合强signal用户行为和高精度的collaborative filtering（CF）算法来构建高质量的item图。然后，我们开发了一种新的GNN架构名为LightSAGE，以生成高质量的项 embeddings  дляvector搜索。最后，我们设计了多种方法来处理冷启动和长尾项，这些方法在广告（ads）系统中是关键的。我们的模型在线评估、在线A/B测试中提高了性能，并被部署到Shopee推荐广告系统的主要流量中。
</details></li>
</ul>
<hr>
<h2 id="Causal-Fair-Metric-Bridging-Causality-Individual-Fairness-and-Adversarial-Robustness"><a href="#Causal-Fair-Metric-Bridging-Causality-Individual-Fairness-and-Adversarial-Robustness" class="headerlink" title="Causal Fair Metric: Bridging Causality, Individual Fairness, and Adversarial Robustness"></a>Causal Fair Metric: Bridging Causality, Individual Fairness, and Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19391">http://arxiv.org/abs/2310.19391</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ehyaei/Causal-Fair-Metric-Learning">https://github.com/Ehyaei/Causal-Fair-Metric-Learning</a></li>
<li>paper_authors: Ahmad-Reza Ehyaei, Golnoosh Farnadi, Samira Samadi</li>
<li>for: 本研究旨在提出一种基于 causal 结构的 fair 度量，以便在机器学习模型中实现公平待遇。</li>
<li>methods: 本研究使用了 causal 推理来定义一种新的 fair 度量，并提出了一种基于 protected causal perturbation 的 robustness 分析方法。</li>
<li>results: 本研究的结果表明，新引入的 fair 度量可以准确地捕捉机器学习模型中的不公平现象，并且可以在不同的应用场景中进行metric learning和投入。<details>
<summary>Abstract</summary>
Adversarial perturbation is used to expose vulnerabilities in machine learning models, while the concept of individual fairness aims to ensure equitable treatment regardless of sensitive attributes. Despite their initial differences, both concepts rely on metrics to generate similar input data instances. These metrics should be designed to align with the data's characteristics, especially when it is derived from causal structure and should reflect counterfactuals proximity. Previous attempts to define such metrics often lack general assumptions about data or structural causal models. In this research, we introduce a causal fair metric formulated based on causal structures that encompass sensitive attributes. For robustness analysis, the concept of protected causal perturbation is presented. Additionally, we delve into metric learning, proposing a method for metric estimation and deployment in real-world problems. The introduced metric has applications in the fields adversarial training, fair learning, algorithmic recourse, and causal reinforcement learning.
</details>
<details>
<summary>摘要</summary>
<INST>这里使用敌对扰乱来曝光机器学习模型的漏洞，而个人公平性的概念则希望在敏感特征上进行公平对待。这两个概念都需要基于度量来生成相似的输入数据实例。这些度量应该和数据的特点相匹配，特别是当它们来自 causal 结构时。先前的尝试定义这些度量通常缺乏一般对数据或结构 causal 模型的假设。在这个研究中，我们引入了基于 causal 结构的公平度量，并提出了保护 causal 扰乱的概念。此外，我们还进行了度量学习，提出了度量估计和部署在实际问题上的方法。引入的度量具有敌对训练、公平学习、算法公平和 causal 强化学习等应用。</INST>Note that Simplified Chinese is used here, as it is more widely used in mainland China and other parts of the world. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Implicit-Manifold-Gaussian-Process-Regression"><a href="#Implicit-Manifold-Gaussian-Process-Regression" class="headerlink" title="Implicit Manifold Gaussian Process Regression"></a>Implicit Manifold Gaussian Process Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19390">http://arxiv.org/abs/2310.19390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bernardo Fichera, Viacheslav Borovitskiy, Andreas Krause, Aude Billard</li>
<li>for: 这 paper 是为了提高 Gaussian process regression 在高维数据上的预测性能和准确性而写的。</li>
<li>methods: 这 paper 使用了一种新的方法，可以直接从数据中推导出隐藏的 manifold 结构，而不需要显式提供 manifold 结构。这种方法 是基于 fully differentiable 的 Gaussian process regression 技术。</li>
<li>results: 这 paper 的结果表明，使用这种新方法可以提高 Gaussian process regression 在高维数据上的预测性能和准确性，并且可以处理大量数据点（上千个数据点）。<details>
<summary>Abstract</summary>
Gaussian process regression is widely used because of its ability to provide well-calibrated uncertainty estimates and handle small or sparse datasets. However, it struggles with high-dimensional data. One possible way to scale this technique to higher dimensions is to leverage the implicit low-dimensional manifold upon which the data actually lies, as postulated by the manifold hypothesis. Prior work ordinarily requires the manifold structure to be explicitly provided though, i.e. given by a mesh or be known to be one of the well-known manifolds like the sphere. In contrast, in this paper we propose a Gaussian process regression technique capable of inferring implicit structure directly from data (labeled and unlabeled) in a fully differentiable way. For the resulting model, we discuss its convergence to the Mat\'ern Gaussian process on the assumed manifold. Our technique scales up to hundreds of thousands of data points, and may improve the predictive performance and calibration of the standard Gaussian process regression in high-dimensional~settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Gradient-free-online-learning-of-subgrid-scale-dynamics-with-neural-emulators"><a href="#Gradient-free-online-learning-of-subgrid-scale-dynamics-with-neural-emulators" class="headerlink" title="Gradient-free online learning of subgrid-scale dynamics with neural emulators"></a>Gradient-free online learning of subgrid-scale dynamics with neural emulators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19385">http://arxiv.org/abs/2310.19385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hugo Frezat, Guillaume Balarac, Julien Le Sommer, Ronan Fablet</li>
<li>for: 本研究提出了一种通用算法，用于在线训练基于机器学习的子网络参数化，即使 numerical solvers 无法导数。</li>
<li>methods: 提议的方法利用神经emuulator来训练减少状态空间解决方案的近似，然后使用这个近似来允许时间推进中的梯度传播。</li>
<li>results: 实验表明，通过分别训练神经emuulator和参数化组件，可以减少一些近似偏差的传播。<details>
<summary>Abstract</summary>
In this paper, we propose a generic algorithm to train machine learning-based subgrid parametrizations online, i.e., with $\textit{a posteriori}$ loss functions for non-differentiable numerical solvers. The proposed approach leverage neural emulators to train an approximation of the reduced state-space solver, which is then used to allows gradient propagation through temporal integration steps. The algorithm is able to recover most of the benefit of online strategies without having to compute the gradient of the original solver. It is demonstrated that training the neural emulator and parametrization components separately with respective loss quantities is necessary in order to minimize the propagation of some approximation bias.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种通用的算法，用于在线训练基于机器学习的子网络参数化，即使使用不可导的数值方法。我们的方法利用神经 emulator 来训练一个简化的状态空间解决方案的approximation，然后使用这个approximation来允许时间整合步骤中的梯度传播。我们的算法可以在大多数情况下重新获得在线策略中的优点，而不需要计算原始解决方案的梯度。我们还证明了在训练神经 emulator 和参数化组件时分别使用相应的损失函数是必要的，以避免某些近似误差的卷积。
</details></li>
</ul>
<hr>
<h2 id="Deep-anytime-valid-hypothesis-testing"><a href="#Deep-anytime-valid-hypothesis-testing" class="headerlink" title="Deep anytime-valid hypothesis testing"></a>Deep anytime-valid hypothesis testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19384">http://arxiv.org/abs/2310.19384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teodora Pandeva, Patrick Forré, Aaditya Ramdas, Shubhanshu Shekhar</li>
<li>for: 这 paper 的目的是提出一种通用的框架，用于构建强大的序列假设测试方法，以解决一类非参数测试问题。</li>
<li>methods: 这 paper 使用了两个已知操作符来定义 null 假设，并通过 continuous monitoring 和有效地聚合证据来快速验证 null 假设。</li>
<li>results: 实验结果表明，使用这 paper 提出的方法可以与专门的基eline测试方法竞争，并且可以适应不知道的问题难度。<details>
<summary>Abstract</summary>
We propose a general framework for constructing powerful, sequential hypothesis tests for a large class of nonparametric testing problems. The null hypothesis for these problems is defined in an abstract form using the action of two known operators on the data distribution. This abstraction allows for a unified treatment of several classical tasks, such as two-sample testing, independence testing, and conditional-independence testing, as well as modern problems, such as testing for adversarial robustness of machine learning (ML) models. Our proposed framework has the following advantages over classical batch tests: 1) it continuously monitors online data streams and efficiently aggregates evidence against the null, 2) it provides tight control over the type I error without the need for multiple testing correction, 3) it adapts the sample size requirement to the unknown hardness of the problem. We develop a principled approach of leveraging the representation capability of ML models within the testing-by-betting framework, a game-theoretic approach for designing sequential tests. Empirical results on synthetic and real-world datasets demonstrate that tests instantiated using our general framework are competitive against specialized baselines on several tasks.
</details>
<details>
<summary>摘要</summary>
我们提出了一个通用框架，用于构建强大、顺序的假设测试，用于覆盖一类非参数测试问题。 null假设使用两个已知运算符来定义数据分布。这种抽象允许我们对多种传统任务，如两个样本测试、独立测试和条件独立测试，以及现代问题，如机器学习模型对抗性测试，进行统一处理。我们的提议的优点包括：1) Continuously monitoring online数据流并有效地聚合对null的证据，2) 不需要多测试修正，可以保持控制型I错误的紧张，3) 可以根据未知问题的难度自适应样本大小。我们开发了一种基于测试-by-betting框架的原则，一种基于游戏理论的方法，用于设计顺序测试。实验结果表明，使用我们的通用框架实例化的测试与特殊基准相比，在多个任务上具有竞争力。
</details></li>
</ul>
<hr>
<h2 id="Musical-Form-Generation"><a href="#Musical-Form-Generation" class="headerlink" title="Musical Form Generation"></a>Musical Form Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19842">http://arxiv.org/abs/2310.19842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rshohan/Jeffrey-Whalen-from-Yorktown1">https://github.com/rshohan/Jeffrey-Whalen-from-Yorktown1</a></li>
<li>paper_authors: Lilac Atassi</li>
<li>for: 这篇论文旨在生成结构化、可持续演奏的乐曲。</li>
<li>methods: 该方法使用条件生成模型创建乐曲段落，并在这些段落之间进行转折。而高级 Compositional 提示的生成则由大型自然语言模型完成。</li>
<li>results: 该方法可生成结构化、可持续演奏的乐曲，并且可以提供多种不同的 musical form。<details>
<summary>Abstract</summary>
While recent generative models can produce engaging music, their utility is limited. The variation in the music is often left to chance, resulting in compositions that lack structure. Pieces extending beyond a minute can become incoherent or repetitive. This paper introduces an approach for generating structured, arbitrarily long musical pieces. Central to this approach is the creation of musical segments using a conditional generative model, with transitions between these segments. The generation of prompts that determine the high-level composition is distinct from the creation of finer, lower-level details. A large language model is then used to suggest the musical form.
</details>
<details>
<summary>摘要</summary>
近期的生成模型可以生成有趣的音乐，但它们的用途受限。音乐的变化通常由偶合机会决定，导致作品缺乏结构。extend beyond a minute的作品可能会变得无法理解或重复。这篇论文提出了一种生成结构化、无限长的音乐作品的方法。中心思想是通过决定 Musical segments的conditional生成模型，并在这些段落之间进行过渡。Prompt的生成，用于决定高级作曲形式，与生成细节的详细进行分离。然后，一个大型语言模型被用来建议音乐的形式。
</details></li>
</ul>
<hr>
<h2 id="An-interpretable-clustering-approach-to-safety-climate-analysis-examining-driver-group-distinction-in-safety-climate-perceptions"><a href="#An-interpretable-clustering-approach-to-safety-climate-analysis-examining-driver-group-distinction-in-safety-climate-perceptions" class="headerlink" title="An interpretable clustering approach to safety climate analysis: examining driver group distinction in safety climate perceptions"></a>An interpretable clustering approach to safety climate analysis: examining driver group distinction in safety climate perceptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19841">http://arxiv.org/abs/2310.19841</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nus-dbe/truck-driver-safety-climate">https://github.com/nus-dbe/truck-driver-safety-climate</a></li>
<li>paper_authors: Kailai Sun, Tianxiang Lan, Yang Miang Goh, Sufiana Safiena, Yueng-Hsiang Huang, Bailey Lytle, Yimin He</li>
<li>for: 本研究旨在探讨大客车驾驶员的安全氛围因素和其影响组织内部的安全性。</li>
<li>methods: 本研究使用了5种不同的聚类算法来分析大客车驾驶员对安全氛围的感受，并引入了可解释的机器学习度量来解释聚类结果。</li>
<li>results: 研究发现，对大客车驾驶员的安全氛围感受进行聚类分析可以分出不同的驾驶群体，并发现监督者关心帮助分开不同驾驶群体。<details>
<summary>Abstract</summary>
The transportation industry, particularly the trucking sector, is prone to workplace accidents and fatalities. Accidents involving large trucks accounted for a considerable percentage of overall traffic fatalities. Recognizing the crucial role of safety climate in accident prevention, researchers have sought to understand its factors and measure its impact within organizations. While existing data-driven safety climate studies have made remarkable progress, clustering employees based on their safety climate perception is innovative and has not been extensively utilized in research. Identifying clusters of drivers based on their safety climate perception allows the organization to profile its workforce and devise more impactful interventions. The lack of utilizing the clustering approach could be due to difficulties interpreting or explaining the factors influencing employees' cluster membership. Moreover, existing safety-related studies did not compare multiple clustering algorithms, resulting in potential bias. To address these issues, this study introduces an interpretable clustering approach for safety climate analysis. This study compares 5 algorithms for clustering truck drivers based on their safety climate perceptions. It proposes a novel method for quantitatively evaluating partial dependence plots (QPDP). To better interpret the clustering results, this study introduces different interpretable machine learning measures (SHAP, PFI, and QPDP). Drawing on data collected from more than 7,000 American truck drivers, this study significantly contributes to the scientific literature. It highlights the critical role of supervisory care promotion in distinguishing various driver groups. The Python code is available at https://github.com/NUS-DBE/truck-driver-safety-climate.
</details>
<details>
<summary>摘要</summary>
交通业界，特别是卡车运输业，具有高度的工作意外和死亡率。大型卡车相关的意外占了交通意外总死亡人数的许多。为了预防意外，研究人员对安全气候的因素进行了广泛的研究，并且尝试了度量其影响。现有的数据驱动的安全气候研究已经做出了很大的进步，但是使用受众分 clustering 方法仍然是一个新的探索。通过分组 drivers 根据他们的安全气候观点，企业可以对员工进行资料分析和更有效的干预措施。然而，使用 clustering 方法可能会受到几个因素的限制，例如：解释和解释 clustering 结果的困难，以及现有的安全相关研究未能比较多种 clustering 算法，这可能会导致偏见。这个研究采用了一个可解释的 clustering 方法，并且比较了5种 clustering 算法，以及一个新的量化评估方法（QPDP）。这个研究还引入了不同的可解释机器学习度量（SHAP、PFI和QPDP），以便更好地解释 clustering 结果。基于超过7,000名美国卡车司机的数据，这个研究具有很大的科学文献意义。它显示出监管者关爱宣传的重要性，以区分不同的司机群体。Python 代码可以在 GitHub 上获取：https://github.com/NUS-DBE/truck-driver-safety-climate。
</details></li>
</ul>
<hr>
<h2 id="ProNet-Progressive-Neural-Network-for-Multi-Horizon-Time-Series-Forecasting"><a href="#ProNet-Progressive-Neural-Network-for-Multi-Horizon-Time-Series-Forecasting" class="headerlink" title="ProNet: Progressive Neural Network for Multi-Horizon Time Series Forecasting"></a>ProNet: Progressive Neural Network for Multi-Horizon Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19322">http://arxiv.org/abs/2310.19322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Lin</li>
<li>for: 这 paper 是为了解决多时间序列预测问题，提出了一种基于深度学习的 ProNet 模型，可以同时利用 AR 和 NAR 策略。</li>
<li>methods: ProNet 模型使用了 segmentation 技术，将预测时间轴分成多个段落，并在每个段落中采用非autoregressive 策略预测最重要的时间步骤，而其余时间步骤则通过 autoregressive 策略预测。 segmentation 过程基于 latent variables，可以有效地捕捉每个时间步骤的重要性。</li>
<li>results: ProNet 模型在四个大规模数据集上进行了广泛的评估，并对 ProNet 模型进行了ablation study。结果显示，ProNet 模型在精度和预测速度两个方面表现出色，与 AR 和 NAR 预测模型相比，具有更高的准确率和更快的预测速度。<details>
<summary>Abstract</summary>
In this paper, we introduce ProNet, an novel deep learning approach designed for multi-horizon time series forecasting, adaptively blending autoregressive (AR) and non-autoregressive (NAR) strategies. Our method involves dividing the forecasting horizon into segments, predicting the most crucial steps in each segment non-autoregressively, and the remaining steps autoregressively. The segmentation process relies on latent variables, which effectively capture the significance of individual time steps through variational inference. In comparison to AR models, ProNet showcases remarkable advantages, requiring fewer AR iterations, resulting in faster prediction speed, and mitigating error accumulation. On the other hand, when compared to NAR models, ProNet takes into account the interdependency of predictions in the output space, leading to improved forecasting accuracy. Our comprehensive evaluation, encompassing four large datasets, and an ablation study, demonstrate the effectiveness of ProNet, highlighting its superior performance in terms of accuracy and prediction speed, outperforming state-of-the-art AR and NAR forecasting models.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的深度学习方法，称为ProNet，用于多个时间序列预测。这种方法可以适应ively分割预测时间序列，在每个分割中采用非autoregressive（NAR）策略预测最重要的步骤，而剩下的步骤使用autoregressive（AR）策略预测。这个分割过程基于隐藏变量，可以有效地捕捉各个时间步骤的重要性。相比AR模型，ProNet具有明显的优势，需要 fewer AR迭代，导致更快的预测速度，并可以减轻预测错误的积累。相比NAR模型，ProNet考虑了预测输出空间中的相互关系，从而提高预测精度。我们的全面评估，包括四个大型数据集，以及一个减少研究，表明ProNet的效果，其中包括精度和预测速度方面的表现，超越了当前AR和NAR预测模型的表现。
</details></li>
</ul>
<hr>
<h2 id="Dual-Directed-Algorithm-Design-for-Efficient-Pure-Exploration"><a href="#Dual-Directed-Algorithm-Design-for-Efficient-Pure-Exploration" class="headerlink" title="Dual-Directed Algorithm Design for Efficient Pure Exploration"></a>Dual-Directed Algorithm Design for Efficient Pure Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19319">http://arxiv.org/abs/2310.19319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Qin, Wei You</li>
<li>for: 这篇论文主要研究的是在随机顺序式适应试验中的探索问题，具体来说是在一组可选项中精确地回答一个查询问题，以高度确idence并且尽量减少测量努力。</li>
<li>methods: 这篇论文使用了 dual variables 来描述优化条件，并使用了适应试验的概率论基础来解决问题。</li>
<li>results: 这篇论文提出了一种新的算法方法，可以在随机顺序式适应试验中实现高效的探索问题解决方案，并且可以解决一些已有的开放问题。 numerics 表明该算法的效率比现有算法更高。<details>
<summary>Abstract</summary>
We consider pure-exploration problems in the context of stochastic sequential adaptive experiments with a finite set of alternative options. The goal of the decision-maker is to accurately answer a query question regarding the alternatives with high confidence with minimal measurement efforts. A typical query question is to identify the alternative with the best performance, leading to ranking and selection problems, or best-arm identification in the machine learning literature. We focus on the fixed-precision setting and derive a sufficient condition for optimality in terms of a notion of strong convergence to the optimal allocation of samples. Using dual variables, we characterize the necessary and sufficient conditions for an allocation to be optimal. The use of dual variables allow us to bypass the combinatorial structure of the optimality conditions that relies solely on primal variables. Remarkably, these optimality conditions enable an extension of top-two algorithm design principle, initially proposed for best-arm identification. Furthermore, our optimality conditions give rise to a straightforward yet efficient selection rule, termed information-directed selection, which adaptively picks from a candidate set based on information gain of the candidates. We outline the broad contexts where our algorithmic approach can be implemented. We establish that, paired with information-directed selection, top-two Thompson sampling is (asymptotically) optimal for Gaussian best-arm identification, solving a glaring open problem in the pure exploration literature. Our algorithm is optimal for $\epsilon$-best-arm identification and thresholding bandit problems. Our analysis also leads to a general principle to guide adaptations of Thompson sampling for pure-exploration problems. Numerical experiments highlight the exceptional efficiency of our proposed algorithms relative to existing ones.
</details>
<details>
<summary>摘要</summary>
我们考虑了纯粹的探索问题在随机顺序适应实验中，具有finite集合的选择选项。决策者的目标是通过高度信任度和最小测试努力来准确回答关于选项的查询问题。一般的查询问题是确定最佳选项，导致排名和选择问题，或最佳臂标识在机器学习文献中。我们关注固定精度设定下的情况，并 deriv a sufficient condition for optimality in terms of a strong convergence notion to the optimal allocation of samples.使用对偶变量，我们Characterize the necessary and sufficient conditions for an allocation to be optimal.这些优化条件使我们可以绕过 primal variable的 combinatorial结构，减少优化条件的复杂性。另外，这些优化条件允许我们扩展top-two algorithm design principle，最初是为best-arm identification提出的。此外，我们的优化条件会导致一种简单又高效的选择规则，称为信息导向选择，这种选择规则可以动态地从候选集中选择基于候选者的信息增益。我们详细介绍了我们的算法approach可以应用的广泛场景。我们证明，在信息导向选择的情况下，top-two Thompson sampling是（几何）最佳的，解决了纯探索领域中的一个开问题。我们的算法是最佳的 дляeps-best-arm identification和阈值bandit问题。我们的分析还导致了一个通用的指南，用于导向Thompson sampling的修改。 numerics experiments highlight the exceptional efficiency of our proposed algorithms relative to existing ones.
</details></li>
</ul>
<hr>
<h2 id="A-Planning-and-Exploring-Approach-to-Extreme-Mechanics-Force-Fields"><a href="#A-Planning-and-Exploring-Approach-to-Extreme-Mechanics-Force-Fields" class="headerlink" title="A Planning-and-Exploring Approach to Extreme-Mechanics Force Fields"></a>A Planning-and-Exploring Approach to Extreme-Mechanics Force Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19306">http://arxiv.org/abs/2310.19306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengjie Shi, Zhiping Xu</li>
<li>for: 本研究旨在开发一种基于神经网络的破坏力场模型（NN-F$^3），以便更好地预测高度机械处理过程中的破坏行为。</li>
<li>methods: 研究人员采用了分子模拟技术，通过对破坏过程中的微structural变化进行解决，以探索破坏过程中的机械能量消耗、破坏路径选择和动态不稳定性（如弯曲、分支）。</li>
<li>results: 研究人员通过对各种材料（如h-BN和扭曲双层graphene）的破坏过程进行模拟，证明了NN-F$^3$模型的能力，并与实验结果相符。此外，研究还表明了需要在预测EXTREME mechanical processes中包含电子结构的知识，以确保模型的准确性。<details>
<summary>Abstract</summary>
Extreme mechanical processes such as strong lattice distortion and bond breakage during fracture are ubiquitous in nature and engineering, which often lead to catastrophic failure of structures. However, understanding the nucleation and growth of cracks is challenged by their multiscale characteristics spanning from atomic-level structures at the crack tip to the structural features where the load is applied. Molecular simulations offer an important tool to resolve the progressive microstructural changes at crack fronts and are widely used to explore processes therein, such as mechanical energy dissipation, crack path selection, and dynamic instabilities (e.g., kinking, branching). Empirical force fields developed based on local descriptors based on atomic positions and the bond orders do not yield satisfying predictions of fracture, even for the nonlinear, anisotropic stress-strain relations and the energy densities of edges. High-fidelity force fields thus should include the tensorial nature of strain and the energetics of rare events during fracture, which, unfortunately, have not been taken into account in both the state-of-the-art empirical and machine-learning force fields. Based on data generated by first-principles calculations, we develop a neural network-based force field for fracture, NN-F$^3$, by combining pre-sampling of the space of strain states and active-learning techniques to explore the transition states at critical bonding distances. The capability of NN-F$^3$ is demonstrated by studying the rupture of h-BN and twisted bilayer graphene as model problems. The simulation results confirm recent experimental findings and highlight the necessity to include the knowledge of electronic structures from first-principles calculations in predicting extreme mechanical processes.
</details>
<details>
<summary>摘要</summary>
EXTREME mechanical processes such as strong lattice distortion and bond breakage during fracture are ubiquitous in nature and engineering, which often lead to catastrophic failure of structures. However, understanding the nucleation and growth of cracks is challenged by their multiscale characteristics spanning from atomic-level structures at the crack tip to the structural features where the load is applied. Molecular simulations offer an important tool to resolve the progressive microstructural changes at crack fronts and are widely used to explore processes therein, such as mechanical energy dissipation, crack path selection, and dynamic instabilities (e.g., kinking, branching). Empirical force fields developed based on local descriptors based on atomic positions and the bond orders do not yield satisfying predictions of fracture, even for the nonlinear, anisotropic stress-strain relations and the energy densities of edges. High-fidelity force fields thus should include the tensorial nature of strain and the energetics of rare events during fracture, which, unfortunately, have not been taken into account in both the state-of-the-art empirical and machine-learning force fields. Based on data generated by first-principles calculations, we develop a neural network-based force field for fracture, NN-F$^3$, by combining pre-sampling of the space of strain states and active-learning techniques to explore the transition states at critical bonding distances. The capability of NN-F$^3$ is demonstrated by studying the rupture of h-BN and twisted bilayer graphene as model problems. The simulation results confirm recent experimental findings and highlight the necessity to include the knowledge of electronic structures from first-principles calculations in predicting extreme mechanical processes.Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Privacy-Preserving-Federated-Learning-over-Vertically-and-Horizontally-Partitioned-Data-for-Financial-Anomaly-Detection"><a href="#Privacy-Preserving-Federated-Learning-over-Vertically-and-Horizontally-Partitioned-Data-for-Financial-Anomaly-Detection" class="headerlink" title="Privacy-Preserving Federated Learning over Vertically and Horizontally Partitioned Data for Financial Anomaly Detection"></a>Privacy-Preserving Federated Learning over Vertically and Horizontally Partitioned Data for Financial Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19304">http://arxiv.org/abs/2310.19304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swanand Ravindra Kadhe, Heiko Ludwig, Nathalie Baracaldo, Alan King, Yi Zhou, Keith Houck, Ambrish Rawat, Mark Purcell, Naoise Holohan, Mikio Takeuchi, Ryo Kawahara, Nir Drucker, Hayim Shaul, Eyal Kushnir, Omri Soceanu</li>
<li>for: 本研究旨在提高金融异常探测的效率，并且解决多家金融机构之间的信任问题。</li>
<li>methods: 本研究使用了联合学习（Federated Learning）、完全同质加密（Fully Homomorphic Encryption）、安全多方计算（Secure Multi-Party Computation）、减少隐私（Differential Privacy）和随机化技术来保证隐私和准确性。</li>
<li>results: 本研究的解决方案可以实现高度的隐私和准确性，并且可以实现高效的金融异常探测。 Specifically, 本研究显示在不信任的假设下，银行不会学习任何敏感特征，而 payment network system 则不会学习任何关于银行的数据。<details>
<summary>Abstract</summary>
The effective detection of evidence of financial anomalies requires collaboration among multiple entities who own a diverse set of data, such as a payment network system (PNS) and its partner banks. Trust among these financial institutions is limited by regulation and competition. Federated learning (FL) enables entities to collaboratively train a model when data is either vertically or horizontally partitioned across the entities. However, in real-world financial anomaly detection scenarios, the data is partitioned both vertically and horizontally and hence it is not possible to use existing FL approaches in a plug-and-play manner.   Our novel solution, PV4FAD, combines fully homomorphic encryption (HE), secure multi-party computation (SMPC), differential privacy (DP), and randomization techniques to balance privacy and accuracy during training and to prevent inference threats at model deployment time. Our solution provides input privacy through HE and SMPC, and output privacy against inference time attacks through DP. Specifically, we show that, in the honest-but-curious threat model, banks do not learn any sensitive features about PNS transactions, and the PNS does not learn any information about the banks' dataset but only learns prediction labels. We also develop and analyze a DP mechanism to protect output privacy during inference. Our solution generates high-utility models by significantly reducing the per-bank noise level while satisfying distributed DP. To ensure high accuracy, our approach produces an ensemble model, in particular, a random forest. This enables us to take advantage of the well-known properties of ensembles to reduce variance and increase accuracy. Our solution won second prize in the first phase of the U.S. Privacy Enhancing Technologies (PETs) Prize Challenge.
</details>
<details>
<summary>摘要</summary>
要有效探测金融异常，需要多个金融机构之间的合作，其中包括支付网络系统（PNS）和其合作银行。但是，这些金融机构之间的信任受到了法规和竞争的限制。基于联合学习（FL）的解决方案可以在数据分布在不同机构之间时进行模型训练，但是在实际的金融异常探测场景中，数据通常会分布在 Vertical 和 Horizontal 两个方向上，因此不能直接使用现有的 FL 方法。我们的新解决方案，PV4FAD，结合了完全同质加密（HE）、安全多方计算（SMPC）、不同隐私（DP）和随机化技术，以平衡隐私和准确性 durante 训练，并在模型部署时防止推理攻击。我们的解决方案提供了输入隐私通过 HE 和 SMPC，并在推理时对输出进行隐私保护。我们还开发了一种 DP 机制来保护输出隐私。我们的解决方案可以减少每家银行的噪音水平，同时满足分布式隐私的要求。为了保证高准确性，我们采用了随机森林 ensemble 模型。这使我们可以利用随机森林的 bekannt properties 来减少差异和提高准确性。我们的解决方案在美国隐私提升技术（PETs）奖励挑战第一阶段中获得了第二名。
</details></li>
</ul>
<hr>
<h2 id="Stage-Aware-Learning-for-Dynamic-Treatments"><a href="#Stage-Aware-Learning-for-Dynamic-Treatments" class="headerlink" title="Stage-Aware Learning for Dynamic Treatments"></a>Stage-Aware Learning for Dynamic Treatments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19300">http://arxiv.org/abs/2310.19300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanwen Ye, Wenzhuo Zhou, Ruoqing Zhu, Annie Qu</li>
<li>for: 这篇论文旨在提出一种新的个人化学习方法，以提高适应性评估和决策过程中的标准化和稳定性。</li>
<li>methods: 本论文提出了一种新的个人化学习方法，包括估算DTR和考虑几个决策阶段的重要性。</li>
<li>results:  empirical results show that the proposed method can significantly improve the sample efficiency and stability of inverse probability weighted based methods, and provide more accurate and personalized treatment recommendations.<details>
<summary>Abstract</summary>
Recent advances in dynamic treatment regimes (DTRs) provide powerful optimal treatment searching algorithms, which are tailored to individuals' specific needs and able to maximize their expected clinical benefits. However, existing algorithms could suffer from insufficient sample size under optimal treatments, especially for chronic diseases involving long stages of decision-making. To address these challenges, we propose a novel individualized learning method which estimates the DTR with a focus on prioritizing alignment between the observed treatment trajectory and the one obtained by the optimal regime across decision stages. By relaxing the restriction that the observed trajectory must be fully aligned with the optimal treatments, our approach substantially improves the sample efficiency and stability of inverse probability weighted based methods. In particular, the proposed learning scheme builds a more general framework which includes the popular outcome weighted learning framework as a special case of ours. Moreover, we introduce the notion of stage importance scores along with an attention mechanism to explicitly account for heterogeneity among decision stages. We establish the theoretical properties of the proposed approach, including the Fisher consistency and finite-sample performance bound. Empirically, we evaluate the proposed method in extensive simulated environments and a real case study for COVID-19 pandemic.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AMLNet-Adversarial-Mutual-Learning-Neural-Network-for-Non-AutoRegressive-Multi-Horizon-Time-Series-Forecasting"><a href="#AMLNet-Adversarial-Mutual-Learning-Neural-Network-for-Non-AutoRegressive-Multi-Horizon-Time-Series-Forecasting" class="headerlink" title="AMLNet: Adversarial Mutual Learning Neural Network for Non-AutoRegressive Multi-Horizon Time Series Forecasting"></a>AMLNet: Adversarial Mutual Learning Neural Network for Non-AutoRegressive Multi-Horizon Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19289">http://arxiv.org/abs/2310.19289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Lin</li>
<li>For: 多Horizon时间系列预测，需要高准确和快速。而AutoRegressive（AR）模型在短期预测方面表现出色，但是随着预测时间跨度增加，它们的速度和误差问题日益突出。Non-AutoRegressive（NAR）模型更适合长期预测，但是它们在不同时间序列之间的互相关系不强，导致预测结果不真实。* Methods: 我们提出了AMLNet，一种创新的NAR模型，通过在线知识传授（KD）方法实现了可靠的预测。AMLNet combinesthe strengths of AR和NAR模型，通过训练一个深度AR解码器和一个深度NAR解码器，并将它们作为ensemble teachers，向一个浅度NAR解码器传授知识。这种知识传授是通过两种关键机制：1）结果驱动KD， dynamically weights the contribution of KD losses from the teacher models，使得浅度NAR解码器能够 incorporate ensemble的多样性；2）提示驱动KD，使用对抗训练提取模型的隐藏状态中的有价值信息进行传授。* Results: 广泛的实验表明AMLNet在传统AR和NAR模型的基础上提供了更高的准确率和更快的计算速度。因此，AMLNet提供了一个可靠的多Horizon时间系列预测方法，为不同预测任务提供了一个有 promise的 Avenues。<details>
<summary>Abstract</summary>
Multi-horizon time series forecasting, crucial across diverse domains, demands high accuracy and speed. While AutoRegressive (AR) models excel in short-term predictions, they suffer speed and error issues as the horizon extends. Non-AutoRegressive (NAR) models suit long-term predictions but struggle with interdependence, yielding unrealistic results. We introduce AMLNet, an innovative NAR model that achieves realistic forecasts through an online Knowledge Distillation (KD) approach. AMLNet harnesses the strengths of both AR and NAR models by training a deep AR decoder and a deep NAR decoder in a collaborative manner, serving as ensemble teachers that impart knowledge to a shallower NAR decoder. This knowledge transfer is facilitated through two key mechanisms: 1) outcome-driven KD, which dynamically weights the contribution of KD losses from the teacher models, enabling the shallow NAR decoder to incorporate the ensemble's diversity; and 2) hint-driven KD, which employs adversarial training to extract valuable insights from the model's hidden states for distillation. Extensive experimentation showcases AMLNet's superiority over conventional AR and NAR models, thereby presenting a promising avenue for multi-horizon time series forecasting that enhances accuracy and expedites computation.
</details>
<details>
<summary>摘要</summary>
多Horizon时间序列预测，在多个领域都是关键，需要高精度和速度。而AutoRegressive（AR）模型在短期预测方面 excel，但是作为预测 horizon 增长，其速度和错误问题日益突出。Non-AutoRegressive（NAR）模型更适合长期预测，但是它们在相互依赖关系下预测结果不实际。我们介绍AMLNet，一种创新的NAR模型，通过在线知识传授（KD）方法实现实实际的预测。AMLNet 利用了 AR 和 NAR 模型的优点，通过在深度AR decoder和深度NAR decoder之间的协作，使得 ensemble 教师模型对一个较浅的 NAR decoder进行知识传授。这种知识传授是通过两个关键机制进行：1）结果驱动KD，通过动态权重 teacher 模型中 KD 损失的贡献，使得较浅的 NAR decoder能够包含 ensemble 的多样性；2）提示驱动KD，通过对模型的隐藏状态进行对抗训练，提取价值的信息进行传授。广泛的实验表明AMLNet 在传统 AR 和 NAR 模型的基础上提高了精度和计算速度，因此提供了一个可靠的多Horizon时间序列预测方法。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Scalability-and-Reliability-in-Semi-Decentralized-Federated-Learning-With-Blockchain-Trust-Penalization-and-Asynchronous-Functionality"><a href="#Enhancing-Scalability-and-Reliability-in-Semi-Decentralized-Federated-Learning-With-Blockchain-Trust-Penalization-and-Asynchronous-Functionality" class="headerlink" title="Enhancing Scalability and Reliability in Semi-Decentralized Federated Learning With Blockchain: Trust Penalization and Asynchronous Functionality"></a>Enhancing Scalability and Reliability in Semi-Decentralized Federated Learning With Blockchain: Trust Penalization and Asynchronous Functionality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19287">http://arxiv.org/abs/2310.19287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Kumar Shrestha, Faijan Ahamad Khan, Mohammed Afaan Shaikh, Amir Jaberzadeh, Jason Geng</li>
<li>for: 这篇论文旨在解决分布式联合学习中的可扩展性和可靠性问题，通过结合区块链技术。</li>
<li>methods: 论文提出了一种新的方法，即 Semi-Decentralized Federated Learning with Blockchain (SDFL-B)，通过增强参与节点的可信度而实现公平、安全和透明的合作机器学习环境，而无需妥协数据隐私。</li>
<li>results: 研究人员通过实验和讨论，证明了SDFL-B系统的优势，包括可扩展性、可靠性和数据隐私保护等方面。<details>
<summary>Abstract</summary>
The paper presents an innovative approach to address the challenges of scalability and reliability in Distributed Federated Learning by leveraging the integration of blockchain technology. The paper focuses on enhancing the trustworthiness of participating nodes through a trust penalization mechanism while also enabling asynchronous functionality for efficient and robust model updates. By combining Semi-Decentralized Federated Learning with Blockchain (SDFL-B), the proposed system aims to create a fair, secure and transparent environment for collaborative machine learning without compromising data privacy. The research presents a comprehensive system architecture, methodologies, experimental results, and discussions that demonstrate the advantages of this novel approach in fostering scalable and reliable SDFL-B systems.
</details>
<details>
<summary>摘要</summary>
文章提出了一种创新的方法，用于解决分布式联合学习中的可扩展性和可靠性问题，通过启用区块链技术的整合。文章通过加入信任惩罚机制来增强参与节点的可信worthiness，同时允许 asynchronous 功能，以实现高效的模型更新。通过结合 Semi-Decentralized Federated Learning with Blockchain (SDFL-B)，提出的系统旨在创造一个公正、安全、透明的合作机器学习环境，无需妥协数据隐私。文章介绍了完整的系统架构、方法、实验结果和讨论，以示该新方法在推动可扩展和可靠 SDFL-B 系统的优势。
</details></li>
</ul>
<hr>
<h2 id="Facilitating-Graph-Neural-Networks-with-Random-Walk-on-Simplicial-Complexes"><a href="#Facilitating-Graph-Neural-Networks-with-Random-Walk-on-Simplicial-Complexes" class="headerlink" title="Facilitating Graph Neural Networks with Random Walk on Simplicial Complexes"></a>Facilitating Graph Neural Networks with Random Walk on Simplicial Complexes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19285">http://arxiv.org/abs/2310.19285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhouc20/hodgerandomwalk">https://github.com/zhouc20/hodgerandomwalk</a></li>
<li>paper_authors: Cai Zhou, Xiyuan Wang, Muhan Zhang</li>
<li>for: 这篇论文旨在系统分析随机漫步在不同级别 simplicial complexes 中对图神经网络的提升作用，并设计基于随机漫步的表达能力更高的位置编码方法。</li>
<li>methods: 论文使用随机漫步在不同级别 simplicial complexes 中进行分析，并提出基于随机漫步和霍德 Laplacians 的表达能力更高的位置编码方法，包括节点级别的 PE 和 Hodge1Lap，以及边级别的 EdgeRWSE。</li>
<li>results: 实验表明，这些随机漫步基于的位置编码方法可以提高图神经网络的表达能力，并且可以在不同的 simplicial complexes 上进行扩展。<details>
<summary>Abstract</summary>
Node-level random walk has been widely used to improve Graph Neural Networks. However, there is limited attention to random walk on edge and, more generally, on $k$-simplices. This paper systematically analyzes how random walk on different orders of simplicial complexes (SC) facilitates GNNs in their theoretical expressivity. First, on $0$-simplices or node level, we establish a connection between existing positional encoding (PE) and structure encoding (SE) methods through the bridge of random walk. Second, on $1$-simplices or edge level, we bridge edge-level random walk and Hodge $1$-Laplacians and design corresponding edge PE respectively. In the spatial domain, we directly make use of edge level random walk to construct EdgeRWSE. Based on the spectral analysis of Hodge $1$-Laplcians, we propose Hodge1Lap, a permutation equivariant and expressive edge-level positional encoding. Third, we generalize our theory to random walk on higher-order simplices and propose the general principle to design PE on simplices based on random walk and Hodge Laplacians. Inter-level random walk is also introduced to unify a wide range of simplicial networks. Extensive experiments verify the effectiveness of our random walk-based methods.
</details>
<details>
<summary>摘要</summary>
< sys>Node-level random walk 已经广泛应用于提高图神经网络。然而，对于Random Walk在边和更广泛地在k-简单形上的注意力却有限。这篇论文系统地分析了Random Walk在不同阶 simplicial complexes (SC) 上的facilitates GNNs的理论表达能力。首先，在0-简单形或节点水平，我们建立了 pozitional encoding (PE) 和 structure encoding (SE) 方法之间的连接，通过Random Walk的桥接。其次，在1-简单形或边水平，我们将edge-level random walk 和Hodge 1-Laplacians相连，并设计相应的边 pozitional encoding (EdgeRWSE)。在空间领域中，我们直接使用边水平 random walk 来构建EdgeRWSE。基于Hodge 1-Laplacians的 спектраль分析，我们提出了一种可 permutation equivariant 和表达力强的边级 pozitional encoding，即Hodge1Lap。第三，我们推广了我们的理论到高阶 simplicial complexes 上，并提出了一个通用的方法来在 simplicial complexes 上设计 pozitional encoding 基于Random Walk和Hodge Laplacians。在多级 simplicial networks 中，我们还引入了Inter-level random walk 来统一广泛的 simplicial networks。广泛的实验证明了我们的Random Walk-based方法的效果。</sys>Note: Simplified Chinese is a written form of Chinese that uses simpler characters and grammar than Traditional Chinese. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="rTsfNet-a-DNN-model-with-Multi-head-3D-Rotation-and-Time-Series-Feature-Extraction-for-IMU-based-Human-Activity-Recognition"><a href="#rTsfNet-a-DNN-model-with-Multi-head-3D-Rotation-and-Time-Series-Feature-Extraction-for-IMU-based-Human-Activity-Recognition" class="headerlink" title="rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction for IMU-based Human Activity Recognition"></a>rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction for IMU-based Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19283">http://arxiv.org/abs/2310.19283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Enokibori</li>
<li>for: 这个论文提出了一种基于IMU的人体活动识别（HAR）的新型深度学习模型（DNN），即rTsfNet。</li>
<li>methods: 该模型使用多头3D旋转和时间序列特征提取（TSF）来自动选择3D基架，然后使用多层感知网络（MLP）进行人体活动识别。</li>
<li>results: 该模型在管理良好的benchmark条件下和多个数据集（UCI HAR、PAMAP2、Daphnet、OPPORTUNITY）中达到了最高准确率，超过了现有模型的性能。<details>
<summary>Abstract</summary>
This paper proposes rTsfNet, a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction, as a new DNN model for IMU-based human activity recognition (HAR). rTsfNet automatically selects 3D bases from which features should be derived by deriving 3D rotation parameters within the DNN. Then, time series features (TSFs), the wisdom of many researchers, are derived and realize HAR using MLP. Although a model that does not use CNN, it achieved the highest accuracy than existing models under well-managed benchmark conditions and multiple datasets: UCI HAR, PAMAP2, Daphnet, and OPPORTUNITY, which target different activities.
</details>
<details>
<summary>摘要</summary>
Here is the Simplified Chinese translation of the text:这篇论文提出了一种新的深度神经网络（DNN）模型，即rTsfNet，用于基于各种传感器的人体活动识别（HAR）。该模型具有多头3D旋转和时间序列特征提取功能，自动从3D基础中选择特征。模型在管理的标准环境下和多个数据集（UCI HAR、PAMAP2、Daphnet和OPPORTUNITY）下达到了最高精度，这些数据集涵盖了不同的活动。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Regularization-for-the-Minimum-Volume-Formula-of-Toric-Calabi-Yau-3-folds"><a href="#Machine-Learning-Regularization-for-the-Minimum-Volume-Formula-of-Toric-Calabi-Yau-3-folds" class="headerlink" title="Machine Learning Regularization for the Minimum Volume Formula of Toric Calabi-Yau 3-folds"></a>Machine Learning Regularization for the Minimum Volume Formula of Toric Calabi-Yau 3-folds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19276">http://arxiv.org/abs/2310.19276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugene Choi, Rak-Kyeong Seong</li>
<li>for: 这 paper 是 investigate Sasaki-Einstein 5-manifolds 的最小体积问题的。</li>
<li>methods: 这 paper 使用了机器学习规范技术来解决这个问题，并提出了基于 geometric invariants 的准确的解。</li>
<li>results: 这 paper 的结果表明，使用机器学习规范技术可以准确地计算 Sasaki-Einstein 5-manifolds 的最小体积，并且可以提供可读写的解释性式。<details>
<summary>Abstract</summary>
We present a collection of explicit formulas for the minimum volume of Sasaki-Einstein 5-manifolds. The cone over these 5-manifolds is a toric Calabi-Yau 3-fold. These toric Calabi-Yau 3-folds are associated with an infinite class of 4d N=1 supersymmetric gauge theories, which are realized as worldvolume theories of D3-branes probing the toric Calabi-Yau 3-folds. Under the AdS/CFT correspondence, the minimum volume of the Sasaki-Einstein base is inversely proportional to the central charge of the corresponding 4d N=1 superconformal field theories. The presented formulas for the minimum volume are in terms of geometric invariants of the toric Calabi-Yau 3-folds. These explicit results are derived by implementing machine learning regularization techniques that advance beyond previous applications of machine learning for determining the minimum volume. Moreover, the use of machine learning regularization allows us to present interpretable and explainable formulas for the minimum volume. Our work confirms that, even for extensive sets of toric Calabi-Yau 3-folds, the proposed formulas approximate the minimum volume with remarkable accuracy.
</details>
<details>
<summary>摘要</summary>
我们提出了一系列Explicit的方程式，用于找到Sasaki-Einstein 5-次元空间中的最小体积。这些5-次元空间的对偶是一组toric Calabi-Yau 3-次元多普遍，这些多普遍与一个无穷的4d N=1 supersymmetric gauge theory相关，它们是D3- Branes在这些toric Calabi-Yau 3-次元多普遍的世界体积理论。根据AdS/CFT对偶，Sasaki-Einstein 5-次元空间的最小体积与4d N=1 superconformal field theory中的中心荷电荷有逆比例关系。我们提出的方程式使用机器学习调整技术，可以在许多toric Calabi-Yau 3-次元多普遍中精确地找到最小体积。此外，这些方程式具有可读性和解释性，可以帮助我们更好地理解Sasaki-Einstein 5-次元空间的特性。我们的研究确认，即使是广泛的toric Calabi-Yau 3-次元多普遍，我们提出的方程式可以对其最小体积进行高精度的预测。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Effective-Elastic-Moduli-of-Rocks-using-Graph-Neural-Networks"><a href="#Prediction-of-Effective-Elastic-Moduli-of-Rocks-using-Graph-Neural-Networks" class="headerlink" title="Prediction of Effective Elastic Moduli of Rocks using Graph Neural Networks"></a>Prediction of Effective Elastic Moduli of Rocks using Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19274">http://arxiv.org/abs/2310.19274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehong Chung, Rasool Ahmad, WaiChing Sun, Wei Cai, Tapan Mukerji</li>
<li>for: 预测岩石弹性模量</li>
<li>methods: 使用图 neuronal networks (GNNs) 方法，将数字 CT 扫描图像转换为图Dataset，并通过训练获得预测弹性模量的能力。</li>
<li>results: GNN 模型在不同尺度的子 кубы上表现出了良好的预测能力，并且可以预测未经测试的岩石和未知的子 кубы尺度。 Comparative analysis 表明，GNNs 在预测未经测试的岩石性质方面表现出了superiority。<details>
<summary>Abstract</summary>
This study presents a Graph Neural Networks (GNNs)-based approach for predicting the effective elastic moduli of rocks from their digital CT-scan images. We use the Mapper algorithm to transform 3D digital rock images into graph datasets, encapsulating essential geometrical information. These graphs, after training, prove effective in predicting elastic moduli. Our GNN model shows robust predictive capabilities across various graph sizes derived from various subcube dimensions. Not only does it perform well on the test dataset, but it also maintains high prediction accuracy for unseen rocks and unexplored subcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs) reveals the superior performance of GNNs in predicting unseen rock properties. Moreover, the graph representation of microstructures significantly reduces GPU memory requirements (compared to the grid representation for CNNs), enabling greater flexibility in the batch size selection. This work demonstrates the potential of GNN models in enhancing the prediction accuracy of rock properties and boosting the efficiency of digital rock analysis.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)这项研究提出了基于图 neural network (GNN) 的方法，用于预测岩石的有效弹性模量从其数字 CT-扫描图像中。我们使用 Mapper 算法将三维数字岩石图像转换为图Dataset，捕捉到重要的几何信息。这些图， после训练，证明可以有效预测弹性模量。我们的 GNN 模型在不同的图大小和维度下表现出了良好的预测能力。不仅在测试数据集上表现出色，而且在未看过的岩石和未探索的子 куби数上也保持高的预测精度。与 Convolutional Neural Networks (CNNs) 进行比较分析表明，GNNs 在预测未看过的岩石特性方面表现出了superior的性能。此外，图表示 Microstructure 的 Representation 可以减少 GPU 内存需求（与网格 Representation 相比，用于 CNNs），使得批处理大小的选择更加灵活。这项研究示出 GNN 模型在提高岩石特性预测精度和数字岩石分析效率方面的潜力。
</details></li>
</ul>
<hr>
<h2 id="Invariant-kernels-on-Riemannian-symmetric-spaces-a-harmonic-analytic-approach"><a href="#Invariant-kernels-on-Riemannian-symmetric-spaces-a-harmonic-analytic-approach" class="headerlink" title="Invariant kernels on Riemannian symmetric spaces: a harmonic-analytic approach"></a>Invariant kernels on Riemannian symmetric spaces: a harmonic-analytic approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19270">http://arxiv.org/abs/2310.19270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathael Da Costa, Cyrus Mostajeran, Juan-Pablo Ortega, Salem Said</li>
<li>for: 证明 классический Gaussian kernel 在非欧几何空间上 nunca 是 positively definite。</li>
<li>methods: 发展了新的几何和分析方法，提供了几何空间上 Gaussian kernel 的完善Characterization，但是只有一些低维度的情况需要数值计算。主要结果包括 L$^{!\scriptscriptstyle p}$-Godement 定理（$p &#x3D; 1,2），它们提供了几何空间上 Gaussian kernel 的必要和 suficient 条件，但是这些条件是可靠的。</li>
<li>results: 获得了几何空间上 Gaussian kernel 的完善Characterization，并提供了许多未来应用的特定几何分析工具。<details>
<summary>Abstract</summary>
This work aims to prove that the classical Gaussian kernel, when defined on a non-Euclidean symmetric space, is never positive-definite for any choice of parameter. To achieve this goal, the paper develops new geometric and analytical arguments. These provide a rigorous characterization of the positive-definiteness of the Gaussian kernel, which is complete but for a limited number of scenarios in low dimensions that are treated by numerical computations. Chief among these results are the L$^{\!\scriptscriptstyle p}$-$\hspace{0.02cm}$Godement theorems (where $p = 1,2$), which provide verifiable necessary and sufficient conditions for a kernel defined on a symmetric space of non-compact type to be positive-definite. A celebrated theorem, sometimes called the Bochner-Godement theorem, already gives such conditions and is far more general in its scope, but is especially hard to apply. Beyond the connection with the Gaussian kernel, the new results in this work lay out a blueprint for the study of invariant kernels on symmetric spaces, bringing forth specific harmonic analysis tools that suggest many future applications.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)这个工作目的是证明经典的加aussian kernel，当定义在非欧几何同胚空间时，从任何参数选择来说是无法确定正定的。为达到这个目标，这篇论文发展了新的几何和分析方法。这些方法提供了非欧几何同胚空间上加aussian kernel的正定性的彻底Characterization，但是只有在低维度情况下，通过数值计算来处理一些特殊情况。主要结果包括L$^{\!\scriptscriptstyle p}$-$\hspace{0.02cm}$Godement定理（其中$p = 1,2），这些定理提供了非欧几何同胚空间上kernel的正定性的必要和 suficient Conditions，并且这些条件是可靠的。此外，这些结果还提供了对各种几何同胚空间上的抽象几何和分析工具的深入了解，这些工具在未来应用中具有广泛的前途。
</details></li>
</ul>
<hr>
<h2 id="A-Metadata-Driven-Approach-to-Understand-Graph-Neural-Networks"><a href="#A-Metadata-Driven-Approach-to-Understand-Graph-Neural-Networks" class="headerlink" title="A Metadata-Driven Approach to Understand Graph Neural Networks"></a>A Metadata-Driven Approach to Understand Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19263">http://arxiv.org/abs/2310.19263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting Wei Li, Qiaozhu Mei, Jiaqi Ma</li>
<li>for: 本文旨在分析 graph neural networks (GNNs) 在不同图数据上表现的敏感性，并提出一种 $\textit{metadata-driven}$ 方法来分析 GNNs 的敏感性。</li>
<li>methods: 本文使用了多ivariate sparse regression 分析 metadata，从而得到了一些关键的数据特性。然后，通过理论分析和控制实验，证明了该数据特性对 GNNs 表现的影响。</li>
<li>results: 研究发现，在图数据上，具有更平衡的度分布的dataset表现更好，这是因为这些dataset中的节点表示更好地 Linear separability，从而导致更好的 GNN 表现。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved remarkable success in various applications, but their performance can be sensitive to specific data properties of the graph datasets they operate on. Current literature on understanding the limitations of GNNs has primarily employed a $\textit{model-driven}$ approach that leverage heuristics and domain knowledge from network science or graph theory to model the GNN behaviors, which is time-consuming and highly subjective. In this work, we propose a $\textit{metadata-driven}$ approach to analyze the sensitivity of GNNs to graph data properties, motivated by the increasing availability of graph learning benchmarks. We perform a multivariate sparse regression analysis on the metadata derived from benchmarking GNN performance across diverse datasets, yielding a set of salient data properties. To validate the effectiveness of our data-driven approach, we focus on one identified data property, the degree distribution, and investigate how this property influences GNN performance through theoretical analysis and controlled experiments. Our theoretical findings reveal that datasets with more balanced degree distribution exhibit better linear separability of node representations, thus leading to better GNN performance. We also conduct controlled experiments using synthetic datasets with varying degree distributions, and the results align well with our theoretical findings. Collectively, both the theoretical analysis and controlled experiments verify that the proposed metadata-driven approach is effective in identifying critical data properties for GNNs.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 已经取得了各种应用的显著成功，但它们在不同的图 dataset 上表现的性能可能会受到特定的数据特性的影响。现有文献中理解 GNN 的限制的方法主要是使用 $\textit{model-driven}$ 方法，利用网络科学或图论中的习惯和专业知识来模型 GNN 的行为，这是时间consuming 和高度主观的。在这项工作中，我们提出了一种 $\textit{metadata-driven}$ 方法来分析 GNN 对图 dataset 的敏感性，由于图学学习 benchmark 的 increasing 可用性而 Motivated。我们通过对 benchmark 中 GNN 性能的多ivariate 稀疏回归分析得到了一组突出的数据特性。为了验证我们的数据驱动方法的有效性，我们选择了一个被归类为重要的数据特性，即度分布，并通过理论分析和控制实验来调查这个特性对 GNN 性能的影响。我们的理论发现表明，具有更平衡的度分布的图 dataset 会具有更好的线性分离性，从而导致更好的 GNN 性能。我们还通过使用 Synthetic 数据集来实验 validate 我们的理论发现，结果与理论发现一致。总之，两者的结果证明了我们的数据驱动方法是有效的。
</details></li>
</ul>
<hr>
<h2 id="Diversify-Conquer-Outcome-directed-Curriculum-RL-via-Out-of-Distribution-Disagreement"><a href="#Diversify-Conquer-Outcome-directed-Curriculum-RL-via-Out-of-Distribution-Disagreement" class="headerlink" title="Diversify &amp; Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement"></a>Diversify &amp; Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19261">http://arxiv.org/abs/2310.19261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daesol Cho, Seungjae Lee, H. Jin Kim</li>
<li>for: 解决RL在无知搜索问题中的挑战，提出一种新的课程RL方法called Diversify for Disagreement &amp; Conquer (D2C)。</li>
<li>methods: D2C方法需要只几个欲达到结果的示例，无论环境的几何结构或欲达到结果的分布，可以在任何环境中工作。该方法首先实现了目标状态分类器的多样化，以确定与访问的状态相似的点，并确保在未知区域中的状态分布不同，从而可以量化未探索区域和设计一个简单直观的目标做出策略奖励信号。然后，该方法使用两分配匹配定义一个课程学习目标，以生成一系列适应度较高的中间目标，使得agent自动探索和征服未探索区域。</li>
<li>results: 实验结果表明，D2C方法在量和质上都高于先前的课程RL方法，即使欲达到结果的示例随机分布。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) often faces the challenges of uninformed search problems where the agent should explore without access to the domain knowledge such as characteristics of the environment or external rewards. To tackle these challenges, this work proposes a new approach for curriculum RL called Diversify for Disagreement & Conquer (D2C). Unlike previous curriculum learning methods, D2C requires only a few examples of desired outcomes and works in any environment, regardless of its geometry or the distribution of the desired outcome examples. The proposed method performs diversification of the goal-conditional classifiers to identify similarities between visited and desired outcome states and ensures that the classifiers disagree on states from out-of-distribution, which enables quantifying the unexplored region and designing an arbitrary goal-conditioned intrinsic reward signal in a simple and intuitive way. The proposed method then employs bipartite matching to define a curriculum learning objective that produces a sequence of well-adjusted intermediate goals, which enable the agent to automatically explore and conquer the unexplored region. We present experimental results demonstrating that D2C outperforms prior curriculum RL methods in both quantitative and qualitative aspects, even with the arbitrarily distributed desired outcome examples.
</details>
<details>
<summary>摘要</summary>
常见的强化学习（RL）问题中，智能机器会面临无知搜索问题，其中智能机器需要在没有环境特征或外部奖励的情况下探索。为解决这些问题，这项工作提出了一种新的目标学习方法，称为多样化为分裂和征服（D2C）。与过去的学习目标方法不同，D2C只需很少的感兴趣结果示例，并且可以在任何环境中工作，无论环境的geometry或感兴趣结果示例的分布。该方法首先将目标 conditioned 分类器多样化，以确定visited和感兴趣结果状态之间的相似性，并确保分类器对非标准分布的状态表示不同意见，从而使得可以量化未探索区域和设计一个简单直观的目标conditioned内在奖励信号。然后，该方法使用两分图匹配来定义学习目标，生成一系列适应度高的中间目标，使得智能机器自动探索和征服未探索区域。我们对D2C进行了实验，并证明它在量化和质量上都高于过去的目标学习方法，即使感兴趣结果示例随机分布。
</details></li>
</ul>
<hr>
<h2 id="Flow-based-Distributionally-Robust-Optimization"><a href="#Flow-based-Distributionally-Robust-Optimization" class="headerlink" title="Flow-based Distributionally Robust Optimization"></a>Flow-based Distributionally Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19253">http://arxiv.org/abs/2310.19253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Xu, Jonghyeok Lee, Xiuyuan Cheng, Yao Xie</li>
<li>for: 解决流基于分布robust优化（DRO）问题，其中需要最差分布（也称为最有利分布，LFD）是连续的，以便可以扩展到更大的样本大小和实现更好的泛化能力。</li>
<li>methods: 使用流基模型，连续时间可逆运输映射来解决计算挑战的无穷维度优化问题，并开发了 Wasserstein 距离 proximal 流体系类型的算法。在实践中，我们使用一系列神经网络来参数化运输映射，通过梯度下降进行块式训练。</li>
<li>results: 在对真实高维数据进行测试中，提出了一种基于数据驱动分布扰动隐私的新机制，并在分布robust假设测试和数据驱动分布扰动隐私方面实现了强有力的实验性表现。<details>
<summary>Abstract</summary>
We present a computationally efficient framework, called \texttt{FlowDRO}, for solving flow-based distributionally robust optimization (DRO) problems with Wasserstein uncertainty sets, when requiring the worst-case distribution (also called the Least Favorable Distribution, LFD) to be continuous so that the algorithm can be scalable to problems with larger sample sizes and achieve better generalization capability for the induced robust algorithms. To tackle the computationally challenging infinitely dimensional optimization problem, we leverage flow-based models, continuous-time invertible transport maps between the data distribution and the target distribution, and develop a Wasserstein proximal gradient flow type of algorithm. In practice, we parameterize the transport maps by a sequence of neural networks progressively trained in blocks by gradient descent. Our computational framework is general, can handle high-dimensional data with large sample sizes, and can be useful for various applications. We demonstrate its usage in adversarial learning, distributionally robust hypothesis testing, and a new mechanism for data-driven distribution perturbation differential privacy, where the proposed method gives strong empirical performance on real high-dimensional data.
</details>
<details>
<summary>摘要</summary>
我们提出一种 computationally efficient 框架，called \texttt{FlowDRO}，用于解决流量基于分布ally robust优化（DRO）问题，具有 Wasserstein 不确定集，并要求最差情况分布（也称为最有利分布，LFD）是连续的，以便可以适用于更大的样本大小和更好的泛化能力。为了解决计算复杂的无穷维度优化问题，我们利用流量模型，连续时间可逆运输Map zwischen 数据分布和目标分布，并开发了 Wasserstein 靠近流类型的梯度流动算法。在实践中，我们归一化运输Map 使用一个序列化的神经网络，逐步在块内使用梯度下降进行训练。我们的计算框架是通用的，可以处理高维数据和大样本大小，并可以用于各种应用。我们在抗 adversarial 学习、分布ally robust假设测试和数据驱动分布泛化隐私中示出了提出的方法的强有效性。
</details></li>
</ul>
<hr>
<h2 id="Assessment-of-Differentially-Private-Synthetic-Data-for-Utility-and-Fairness-in-End-to-End-Machine-Learning-Pipelines-for-Tabular-Data"><a href="#Assessment-of-Differentially-Private-Synthetic-Data-for-Utility-and-Fairness-in-End-to-End-Machine-Learning-Pipelines-for-Tabular-Data" class="headerlink" title="Assessment of Differentially Private Synthetic Data for Utility and Fairness in End-to-End Machine Learning Pipelines for Tabular Data"></a>Assessment of Differentially Private Synthetic Data for Utility and Fairness in End-to-End Machine Learning Pipelines for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19250">http://arxiv.org/abs/2310.19250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayana Pereira, Meghana Kshirsagar, Sumit Mukherjee, Rahul Dodhia, Juan Lavista Ferres, Rafael de Sousa<br>for: 这个论文主要是为了研究使用异步private的人工数据集来保护个人数据提供者的隐私，并对这种方法在医疗和人道主义领域中的应用进行分析。methods: 这个论文使用了两种主要的异步private数据生成算法：marginal-based和GAN-based。同时，该论文还提出了一种不要求实际数据存在的训练和评估框架。results: 研究结果表明，使用marginal-based数据生成算法可以达到模型训练的同等Utility水平，而且该算法还可以让模型同时实现Utility和公平性的特点。此外，该论文还对异步private数据生成算法的多种定义公平性进行了广泛的分析。<details>
<summary>Abstract</summary>
Differentially private (DP) synthetic data sets are a solution for sharing data while preserving the privacy of individual data providers. Understanding the effects of utilizing DP synthetic data in end-to-end machine learning pipelines impacts areas such as health care and humanitarian action, where data is scarce and regulated by restrictive privacy laws. In this work, we investigate the extent to which synthetic data can replace real, tabular data in machine learning pipelines and identify the most effective synthetic data generation techniques for training and evaluating machine learning models. We investigate the impacts of differentially private synthetic data on downstream classification tasks from the point of view of utility as well as fairness. Our analysis is comprehensive and includes representatives of the two main types of synthetic data generation algorithms: marginal-based and GAN-based. To the best of our knowledge, our work is the first that: (i) proposes a training and evaluation framework that does not assume that real data is available for testing the utility and fairness of machine learning models trained on synthetic data; (ii) presents the most extensive analysis of synthetic data set generation algorithms in terms of utility and fairness when used for training machine learning models; and (iii) encompasses several different definitions of fairness. Our findings demonstrate that marginal-based synthetic data generators surpass GAN-based ones regarding model training utility for tabular data. Indeed, we show that models trained using data generated by marginal-based algorithms can exhibit similar utility to models trained using real data. Our analysis also reveals that the marginal-based synthetic data generator MWEM PGM can train models that simultaneously achieve utility and fairness characteristics close to those obtained by models trained with real data.
</details>
<details>
<summary>摘要</summary>
diferencialmente privado (DP) 的 sintética datos sets 是一种解决分享数据而保护个人数据提供者隐私的解决方案。 理解使用 DP sintética datos in end-to-end machine learning pipelines 的影响，特别是在医疗和人道主义领域， где data 稀缺并受到严格隐私法规限制。在这项工作中，我们 investigate 如何使用 sintética datos replace 实际的 tabular data 在 machine learning pipelines 中，并 identify 最有效的 sintética datos generation techniques  для训练和评估机器学习模型。我们 investigate 使用异 differentially private sintética datos 对下游分类任务的影响，包括实用性和公平性。我们的分析是全面的，包括两种主要的 sintética datos generation algorithms：marginal-based 和 GAN-based。到目前为止，我们的工作是第一个：1. 提出一种不假设实际数据可用于测试机器学习模型在 sintética datos 上的训练和公平性的训练和评估框架。2. 对 sintética datos set generation algorithms 进行了最广泛的分析，包括实用性和公平性在内。3. 涵盖了多种定义的公平性。我们的发现表明，marginal-based sintética datos generators 在机器学习模型训练中的实用性比 GAN-based 更高。我们显示，使用 marginal-based 生成的数据可以训练模型，其Utility 与实际数据训练模型相似。我们的分析还显示，使用 MWEM PGM 生成的 sintética datos 可以训练同时实现实用性和公平性的模型，与实际数据训练模型几乎相当。
</details></li>
</ul>
<hr>
<h2 id="A-spectral-regularisation-framework-for-latent-variable-models-designed-for-single-channel-applications"><a href="#A-spectral-regularisation-framework-for-latent-variable-models-designed-for-single-channel-applications" class="headerlink" title="A spectral regularisation framework for latent variable models designed for single channel applications"></a>A spectral regularisation framework for latent variable models designed for single channel applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19246">http://arxiv.org/abs/2310.19246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Balshaw, P. Stephan Heyns, Daniel N. Wilke, Stephan Schmidt</li>
<li>for: 这篇论文的目的是提出一个 Python 包，用于解决单 канал Latent Variable Models (LVMs) 中的数据重复问题，并通过添加一个新的spectral regularization term来解决这个问题。</li>
<li>methods: 该论文使用的方法是在 LVM 优化过程中添加spectral regularization term，以提高 LVM 的拟合精度和稳定性。</li>
<li>results: 该论文提出的 Python 包可以在单 канал时间序列应用中提供一个一致的线性 LVM 优化框架，并通过spectral regularization来提高 LVM 的拟合精度。<details>
<summary>Abstract</summary>
Latent variable models (LVMs) are commonly used to capture the underlying dependencies, patterns, and hidden structure in observed data. Source duplication is a by-product of the data hankelisation pre-processing step common to single channel LVM applications, which hinders practical LVM utilisation. In this article, a Python package titled spectrally-regularised-LVMs is presented. The proposed package addresses the source duplication issue via the addition of a novel spectral regularisation term. This package provides a framework for spectral regularisation in single channel LVM applications, thereby making it easier to investigate and utilise LVMs with spectral regularisation. This is achieved via the use of symbolic or explicit representations of potential LVM objective functions which are incorporated into a framework that uses spectral regularisation during the LVM parameter estimation process. The objective of this package is to provide a consistent linear LVM optimisation framework which incorporates spectral regularisation and caters to single channel time-series applications.
</details>
<details>
<summary>摘要</summary>
The proposed package provides a framework for spectral regularization in single channel LVM applications, making it easier to investigate and utilize LVMs with spectral regularization. The package uses symbolic or explicit representations of potential LVM objective functions and incorporates spectral regularization during the LVM parameter estimation process. The objective of this package is to provide a consistent linear LVM optimization framework that incorporates spectral regularization and caters to single channel time-series applications.
</details></li>
</ul>
<hr>
<h2 id="Maximum-Knowledge-Orthogonality-Reconstruction-with-Gradients-in-Federated-Learning"><a href="#Maximum-Knowledge-Orthogonality-Reconstruction-with-Gradients-in-Federated-Learning" class="headerlink" title="Maximum Knowledge Orthogonality Reconstruction with Gradients in Federated Learning"></a>Maximum Knowledge Orthogonality Reconstruction with Gradients in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19222">http://arxiv.org/abs/2310.19222</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wfwf10/mkor">https://github.com/wfwf10/mkor</a></li>
<li>paper_authors: Feng Wang, Senem Velipasalar, M. Cenk Gursoy</li>
<li>for: 防止客户端资料遭到泄露，保持隐私。</li>
<li>methods: 使用秘密修改参数，从客户端的梯度更新中恢复输入数据。</li>
<li>results: 在MNIST、CIFAR-100和ImageNet dataset上表现出色，比现有方法高品质。<details>
<summary>Abstract</summary>
Federated learning (FL) aims at keeping client data local to preserve privacy. Instead of gathering the data itself, the server only collects aggregated gradient updates from clients. Following the popularity of FL, there has been considerable amount of work, revealing the vulnerability of FL approaches by reconstructing the input data from gradient updates. Yet, most existing works assume an FL setting with unrealistically small batch size, and have poor image quality when the batch size is large. Other works modify the neural network architectures or parameters to the point of being suspicious, and thus, can be detected by clients. Moreover, most of them can only reconstruct one sample input from a large batch. To address these limitations, we propose a novel and completely analytical approach, referred to as the maximum knowledge orthogonality reconstruction (MKOR), to reconstruct clients' input data. Our proposed method reconstructs a mathematically proven high quality image from large batches. MKOR only requires the server to send secretly modified parameters to clients and can efficiently and inconspicuously reconstruct the input images from clients' gradient updates. We evaluate MKOR's performance on the MNIST, CIFAR-100, and ImageNet dataset and compare it with the state-of-the-art works. The results show that MKOR outperforms the existing approaches, and draws attention to a pressing need for further research on the privacy protection of FL so that comprehensive defense approaches can be developed.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 目的是保持客户端数据本地，以保持隐私。而不是收集客户端数据本身，服务器只收集客户端发送的聚合梯度更新。随着 Federated learning 的普及，有很多研究揭露了 Federated learning 方法的漏洞，可以从梯度更新中重建输入数据。然而，大多数现有工作假设了 Federated learning 的batch size很小，并且在大batch size时的图像质量很差。其他工作会修改神经网络的架构或参数，使其变得异常，因此可以被客户端探测。此外，大多数方法只能从大批量中重建一个样本输入。为解决这些限制，我们提出了一种新的、完全分析的方法， referred to as maximum knowledge orthogonality reconstruction (MKOR)，可以从客户端发送的梯度更新中重建客户端的输入数据。我们的提议方法可以从大批量中重建高质量的数学确定的图像。MKOR只需服务器在客户端所需的情况下隐藏地将参数发送给客户端，并可以高效地、不受注意的重建输入图像。我们对 MKOR 的性能进行了 MNIST、CIFAR-100 和 ImageNet  dataset 的测试，并与现有方法进行了比较。结果表明，MKOR 在输入图像质量和梯度更新精度等方面都有较好的表现，引起了隐私保护方面的进一步研究，以开发全面的防御策略。
</details></li>
</ul>
<hr>
<h2 id="From-Stream-to-Pool-Dynamic-Pricing-Beyond-i-i-d-Arrivals"><a href="#From-Stream-to-Pool-Dynamic-Pricing-Beyond-i-i-d-Arrivals" class="headerlink" title="From Stream to Pool: Dynamic Pricing Beyond i.i.d. Arrivals"></a>From Stream to Pool: Dynamic Pricing Beyond i.i.d. Arrivals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19220">http://arxiv.org/abs/2310.19220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Titing Cui, Su Jia, Thomas Lavastida<br>for:The paper is written to address the dynamic pricing problem in a stream of customers, where high-valuation customers tend to make purchases earlier and leave the market, leading to a shift in the valuation distribution.methods:The paper uses a model where a pool of non-strategic unit-demand customers interact repeatedly with the seller, and each customer monitors the price intermittently according to an independent Poisson process. The paper presents a minimax optimal algorithm that computes a non-adaptive policy which guarantees a $1&#x2F;k$ fraction of the optimal revenue, given any set of $k$ prices. Additionally, the paper presents an adaptive learn-then-earn policy based on a novel debiasing approach.results:The paper achieves an $\tilde O(kn^{3&#x2F;4})$ regret bound for the adaptive policy, and further improves the bound to $\tilde O(k^{3&#x2F;4} n^{3&#x2F;4})$ using martingale concentration inequalities.<details>
<summary>Abstract</summary>
The dynamic pricing problem has been extensively studied under the \textbf{stream} model: A stream of customers arrives sequentially, each with an independently and identically distributed valuation. However, this formulation is not entirely reflective of the real world. In many scenarios, high-valuation customers tend to make purchases earlier and leave the market, leading to a \emph{shift} in the valuation distribution. Thus motivated, we consider a model where a \textbf{pool} of $n$ non-strategic unit-demand customers interact repeatedly with the seller. Each customer monitors the price intermittently according to an independent Poisson process and makes a purchase if the observed price is lower than her \emph{private} valuation, whereupon she leaves the market permanently. We present a minimax \emph{optimal} algorithm that efficiently computes a non-adaptive policy which guarantees a $1/k$ fraction of the optimal revenue, given any set of $k$ prices. Moreover, we present an adaptive \emph{learn-then-earn} policy based on a novel \emph{debiasing} approach, and prove an $\tilde O(kn^{3/4})$ regret bound. We further improve the bound to $\tilde O(k^{3/4} n^{3/4})$ using martingale concentration inequalities.
</details>
<details>
<summary>摘要</summary>
“流动价格问题已经广泛研究过，使用流动模型：一条流动的客户来sequentially，每个客户都有独立并同分布的评估。但这种形式并不完全反映现实世界。在许多场景下，高评估客户往往在早期购买并离开市场，导致评估分布的变化。因此，我们考虑了一个池塘模型：$n$个不策略性单元需求客户与卖家互动着重。每个客户按照独立的波利逊过程监测价格，如果观察到的价格低于她的私有评估，就会购买并永久离开市场。我们提出了一种最优化算法，可以快速计算一个不适应的策略，保证收入的$1/k$部分是最优的。此外，我们还提出了一种学习然后获得的策略，基于一种新的减偏方法，并证明了$\tilde O(kn^{3/4})$的违和 bound。最后，我们使用 martingale concetration不等式提高 bound to $\tilde O(k^{3/4} n^{3/4})$。”
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Federated-Unlearning-A-Taxonomy-Challenges-and-Future-Directions"><a href="#A-Survey-of-Federated-Unlearning-A-Taxonomy-Challenges-and-Future-Directions" class="headerlink" title="A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions"></a>A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19218">http://arxiv.org/abs/2310.19218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxi Yang, Yang Zhao</li>
<li>for: This paper provides a comprehensive survey of Federated Unlearning (FU), which is an emerging area that aims to selectively unlearn specific information in a decentralized and privacy-preserving manner.</li>
<li>methods: The paper discusses various algorithms, objectives, and evaluation metrics for FU, and identifies some challenges in this area.</li>
<li>results: The paper summarizes existing studies on FU into a taxonomy, including schemes, potential applications, and future directions.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文提供了 Federated Unlearning（FU）的全面检讨，FU 是一个emerging领域，旨在在分布式和隐私保护的情况下选择性地忘记特定的信息。</li>
<li>methods: 论文讨论了 FU 中不同的算法、目标和评价指标，并指出了一些挑战。</li>
<li>results: 论文将现有的 FU 研究总结为一个分类表，包括方案、应用 potential 和未来方向。<details>
<summary>Abstract</summary>
With the development of trustworthy Federated Learning (FL), the requirement of implementing right to be forgotten gives rise to the area of Federated Unlearning (FU). Comparing to machine unlearning, a major challenge of FU lies in the decentralized and privacy-preserving nature of FL, in which clients jointly train a global model without sharing their raw data, making it substantially more intricate to selectively unlearn specific information. In that regard, many efforts have been made to tackle the challenges of FU and have achieved significant progress. In this paper, we present a comprehensive survey of FU. Specially, we provide the existing algorithms, objectives, evaluation metrics, and identify some challenges of FU. By reviewing and comparing some studies, we summarize them into a taxonomy for various schemes, potential applications and future directions.
</details>
<details>
<summary>摘要</summary>
随着可靠的联合学习（FL）的发展，实施“忘记权”的需求带来了联合学习（FU）的领域。与机器学习中的机器忘记相比，联合学习中的主要挑战在于在分布式和隐私保护的情况下，客户端共同训练全球模型，而不是分享raw数据，使其变得显著更加复杂，以选择性地忘记特定信息。为此，许多努力已经被作出，并取得了显著进步。在这篇论文中，我们提供了联合学习的完整报告。特别是，我们提供了现有的算法、目标、评价指标，并 indentified一些联合学习的挑战。通过对一些研究的复习和比较，我们将其总结为一种分类表，包括不同的方案、应用领域和未来方向。
</details></li>
</ul>
<hr>
<h2 id="On-the-accuracy-and-efficiency-of-group-wise-clipping-in-differentially-private-optimization"><a href="#On-the-accuracy-and-efficiency-of-group-wise-clipping-in-differentially-private-optimization" class="headerlink" title="On the accuracy and efficiency of group-wise clipping in differentially private optimization"></a>On the accuracy and efficiency of group-wise clipping in differentially private optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19215">http://arxiv.org/abs/2310.19215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqi Bu, Ruixuan Liu, Yu-Xiang Wang, Sheng Zha, George Karypis</li>
<li>for: 这个研究旨在探讨差异private（DP）深度学习中的渐进式对策，特别是在大规模的 Computer Vision 和自然语言处理模型中。</li>
<li>methods: 这个研究使用了渐进式对策的各种不同式抑制方法，包括层别抑制和各层抑制，并进行了实验和分析。</li>
<li>results: 研究结果显示，不同的抑制式对策有相同的时间复杂度，但是它们实现了精确度-储存空间负面的贸易：对于大型模型，层别抑制可以实现更高的精确度和更低的峰值储存空间。<details>
<summary>Abstract</summary>
Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Factor-Fitting-Rank-Allocation-and-Partitioning-in-Multilevel-Low-Rank-Matrices"><a href="#Factor-Fitting-Rank-Allocation-and-Partitioning-in-Multilevel-Low-Rank-Matrices" class="headerlink" title="Factor Fitting, Rank Allocation, and Partitioning in Multilevel Low Rank Matrices"></a>Factor Fitting, Rank Allocation, and Partitioning in Multilevel Low Rank Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19214">http://arxiv.org/abs/2310.19214</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cvxgrp/mlr_fitting">https://github.com/cvxgrp/mlr_fitting</a></li>
<li>paper_authors: Tetiana Parshakova, Trevor Hastie, Eric Darve, Stephen Boyd</li>
<li>for: 这个论文关注了multilevel low rank（MLR）矩阵的问题，MLR矩阵是一种扩展了低级矩阵的矩阵，它们共享许多特性，如积分器产生的存储量和积分器-向量乘法的复杂性。</li>
<li>methods: 该论文提出了三个问题的解决方案，即因子适应、级别分配和层次分区。它们使用了一种基于因子的方法来适应MLR矩阵，并且提供了一个开源包来实现这些方法。</li>
<li>results: 该论文的结果表明，使用该方法可以高效地适应MLR矩阵，并且可以控制积分器的存储量和复杂性。它们还提供了一些对MLR矩阵的分析和评估。<details>
<summary>Abstract</summary>
We consider multilevel low rank (MLR) matrices, defined as a row and column permutation of a sum of matrices, each one a block diagonal refinement of the previous one, with all blocks low rank given in factored form. MLR matrices extend low rank matrices but share many of their properties, such as the total storage required and complexity of matrix-vector multiplication. We address three problems that arise in fitting a given matrix by an MLR matrix in the Frobenius norm. The first problem is factor fitting, where we adjust the factors of the MLR matrix. The second is rank allocation, where we choose the ranks of the blocks in each level, subject to the total rank having a given value, which preserves the total storage needed for the MLR matrix. The final problem is to choose the hierarchical partition of rows and columns, along with the ranks and factors. This paper is accompanied by an open source package that implements the proposed methods.
</details>
<details>
<summary>摘要</summary>
我们考虑多层低级矩阵（MLR矩阵），定义为一个行列排序的卷积矩阵的和，其中每一个矩阵都是前一个矩阵的块分解，所有块都是低级矩阵的块分解， givens in factored form。 MLR矩阵扩展了低级矩阵，但与其有许多共同性，如绝对存储量和矩阵-向量乘法的复杂度。我们解决了三个在使用给定矩阵适应MLR矩阵的 Frobenius 范数中出现的问题。第一个问题是调整MLR矩阵的因子。第二个问题是分配级别，即在每个层中选择块的级别，保持总级别的值，这 preserved 绝对存储量需要的MLR矩阵。最后一个问题是选择行列层次结构，以及级别和因子。这篇文章附有一个开源包，实现我们提议的方法。
</details></li>
</ul>
<hr>
<h2 id="Investigative-Pattern-Detection-Framework-for-Counterterrorism"><a href="#Investigative-Pattern-Detection-Framework-for-Counterterrorism" class="headerlink" title="Investigative Pattern Detection Framework for Counterterrorism"></a>Investigative Pattern Detection Framework for Counterterrorism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19211">http://arxiv.org/abs/2310.19211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashika R. Muramudalige, Benjamin W. K. Hung, Rosanne Libretti, Jytte Klausen, Anura P. Jayasumana</li>
<li>for: 预防暴力激进分子发动袭击，保障公众安全。</li>
<li>methods: 使用自动化工具提取信息，回答分析员的问题，不断扫描新信息，与过去事件集成，提醒出现风险。</li>
<li>results: 开发了一套名为INSPECT的调查模式检测框架，可以自动执行大规模的详细审讯评估，形成知识网络，并提供行为指标和激进路径的查询功能。<details>
<summary>Abstract</summary>
Law-enforcement investigations aimed at preventing attacks by violent extremists have become increasingly important for public safety. The problem is exacerbated by the massive data volumes that need to be scanned to identify complex behaviors of extremists and groups. Automated tools are required to extract information to respond queries from analysts, continually scan new information, integrate them with past events, and then alert about emerging threats. We address challenges in investigative pattern detection and develop an Investigative Pattern Detection Framework for Counterterrorism (INSPECT). The framework integrates numerous computing tools that include machine learning techniques to identify behavioral indicators and graph pattern matching techniques to detect risk profiles/groups. INSPECT also automates multiple tasks for large-scale mining of detailed forensic biographies, forming knowledge networks, and querying for behavioral indicators and radicalization trajectories. INSPECT targets human-in-the-loop mode of investigative search and has been validated and evaluated using an evolving dataset on domestic jihadism.
</details>
<details>
<summary>摘要</summary>
法警调查用于预防暴力激进分子的袭击已成为公共安全的重要问题。这问题受到巨量数据的检索和分析的困难，以检测激进分子和组织的复杂行为。我们面临的挑战是 automatizat 调查模式的检测和发现潜在威胁。为解决这些挑战，我们提出了一个调查模式检测框架（INSPECT）。该框架 integrates 多种计算工具，包括机器学习技术和图pattern匹配技术，以检测行为指标和风险个人/组织。INSPECT 还自动化了大规模的审批细节传记、组织知识网络和查询行为指标和激进化轨迹。INSPECT 采用人类在循环搜索模式，并已经验证和评估使用了随时间变化的数据集，以适应家庭激进主义的调查需求。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/cs.LG_2023_10_30/" data-id="clogyj90200s37craacmuhm64" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/30/eess.SP_2023_10_30/" class="article-date">
  <time datetime="2023-10-30T08:00:00.000Z" itemprop="datePublished">2023-10-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/30/eess.SP_2023_10_30/">eess.SP - 2023-10-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Analytical-Nonlinear-Distortion-Characterization-for-Frequency-Selective-Massive-MIMO-Channels"><a href="#Analytical-Nonlinear-Distortion-Characterization-for-Frequency-Selective-Massive-MIMO-Channels" class="headerlink" title="Analytical Nonlinear Distortion Characterization for Frequency-Selective Massive MIMO Channels"></a>Analytical Nonlinear Distortion Characterization for Frequency-Selective Massive MIMO Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20038">http://arxiv.org/abs/2310.20038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Murat Babek Salman, Emil Björnson, Gokhan Muzaffer Guvensen, Tolga Ciloglu</li>
<li>for:  investigate the impact of frequency selectivity on nonlinear distortion in wireless communication</li>
<li>methods: closed-form expression for received distortion power, numerical simulations</li>
<li>results: in-band and OOB distortion power inversely proportional to number of multipath components, beamforming gain for intended user, higher received in-band distortion compared to OOB distortion.Here’s the full Chinese text:</li>
<li>for: 本文研究了无线通信中频 Selectivity 对不对称噪误的影响。</li>
<li>methods: 使用关键表达式来 deriv 接收噪误功率，并通过数据模拟来验证。</li>
<li>results: 随着多普勒复合数量 (MPCs) 的增加，噪误呈现出明显的特征，尤其是在对应频率和延迟宽度之间。在这篇文章中，我们发现了接收到的对称噪误功率与 MPCs 之间存在负相关，并且发现当延迟宽度变窄时，对称噪误功率对应用户方向传递更高。<details>
<summary>Abstract</summary>
Nonlinear distortion stemming from low-cost power amplifiers may severely affect wireless communication performance through out-of-band (OOB) radiation and in-band distortion. The distortion is correlated between different transmit antennas in an antenna array, which results in a beamforming gain at the receiver side that grows with the number of antennas. In this paper, we investigate how the strength of the distortion is affected by the frequency selectivity of the channel. A closed-form expression for the received distortion power is derived as a function of the number of multipath components (MPCs) and the delay spread, which highlight their impact. The performed analysis, which is verified via numerical simulations, reveals that as the number of MPCs increases, distortion exhibits distinct characteristics for in-band and OOB frequencies. It is shown that the received in-band and OOB distortion power is inversely proportional to the number of MPCs, and it is reported that as the delay spread gets narrower, the in-band distortion power is beamformed towards the intended user, which yields higher received in-band distortion compared to the OOB distortion.
</details>
<details>
<summary>摘要</summary>
非线性扭曲由低成本功率增强器引起的无线通信性能可能受到严重影响，包括射频外带（OOB）辐射和射频扭曲。这种扭曲与不同的发射天线相关，导致接收器侧的扩散增强增长与天线数量成直线关系。本文研究了频率选择性通道的影响于扭曲强度。我们 deriv了一个关于数据量和延迟跨度的关注表达，并通过数值仿真验证。结果表明，随着多路射频（MPC）的增加，扭曲表现出明显的特征，其中射频和OOB频率强度与MPC数量成反比。此外，随着延迟跨度的缩短，指定用户方向的射频扭曲强度增加，导致更高的接收射频扭曲质量。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Enabled-Text-Semantic-Communication-under-Interference-An-Empirical-Study"><a href="#Deep-Learning-Enabled-Text-Semantic-Communication-under-Interference-An-Empirical-Study" class="headerlink" title="Deep Learning-Enabled Text Semantic Communication under Interference: An Empirical Study"></a>Deep Learning-Enabled Text Semantic Communication under Interference: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19974">http://arxiv.org/abs/2310.19974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tilahun M. Getu, Georges Kaddoum, Mehdi Bennis<br>for: This paper aims to explore the fundamental limits of DeepSC, a text semantic communication scheme, under radio frequency interference (RFI) and to validate the theoretical predictions using empirical computer experiments.methods: The paper uses the proceedings of the European Parliament (Europarl) dataset, tokenizes and vectorizes the text data, and trains the DeepSC architecture in Keras 2.9 with TensorFlow 2.9 as a backend. The testing is conducted under Gaussian multi-interferer RFI received over Rayleigh fading channels.results: The testing results corroborate the theoretical predictions that DeepSC produces semantically irrelevant sentences as the number of Gaussian RFI emitters gets very large, indicating the need for a fundamental 6G design paradigm for interference-resistant and robust SemCom (IR$^2$ SemCom).Here is the format you requested:for: 这篇论文目的是探索DeepSC在干扰(RFI)下的基本限制，并使用计算机实验验证理论预测。methods: 这篇论文使用欧洲议会议程(Europarl) dataset，将文本数据Token化并Vector化，并使用Keras 2.9和TensorFlow 2.9作为后端来训练DeepSC架构。测试在Gaussian多干扰RFI下接收的Rayleigh渐减通道上进行。results: 测试结果证实了理论预测，DeepSC在干扰量很大时会生成无关 semantic sentence。这表明需要一种基本的6G设计方针，以提供干扰抗性和Robust SemCom(IR$^2$ SemCom)。<details>
<summary>Abstract</summary>
At the confluence of 6G, deep learning (DL), and natural language processing (NLP), DL-enabled text semantic communication (SemCom) has emerged as a 6G enabler by promising to minimize bandwidth consumption, transmission delay, and power usage. Among text SemCom techniques, \textit{DeepSC} is a popular scheme that leverages advancements in DL and NLP to reliably transmit semantic information in low signal-to-noise ratio (SNR) regimes. To understand the fundamental limits of such a transmission paradigm, our recently developed theory \cite{Getu'23_Performance_Limits} predicted the performance limits of DeepSC under radio frequency interference (RFI). Although these limits were corroborated by simulations, trained deep networks can defy classical statistical wisdom, and hence extensive computer experiments are needed to validate our theory. Accordingly, this empirical work follows concerning the training and testing of DeepSC using the proceedings of the European Parliament (Europarl) dataset. Employing training, validation, and testing sets \textit{tokenized and vectorized} from Europarl, we train the DeepSC architecture in Keras 2.9 with TensorFlow 2.9 as a backend and test it under Gaussian multi-interferer RFI received over Rayleigh fading channels. Validating our theory, the testing results corroborate that DeepSC produces semantically irrelevant sentences as the number of Gaussian RFI emitters gets very large. Therefore, a fundamental 6G design paradigm for \textit{interference-resistant and robust SemCom} (IR$^2$ SemCom) is needed.
</details>
<details>
<summary>摘要</summary>
在6G、深度学习（DL）和自然语言处理（NLP）的交汇点上，DL所启动的文本semantic communication（SemCom）已经成为6G的推动者，承诺可以减少带宽消耗、传输延迟和功率使用。文本SemCom技术中，深入SC是一种广泛使用的方案，通过利用深度学习和NLP的进步来可靠地在低信号噪响（SNR） régime中传输semantic信息。为了理解这种传输模式的基本限制，我们在\cite{Getu'23_Performance_Limits}中提出了一种理论，预测DeepSC在电磁干扰（RFI）下的性能限制。虽然这些限制得到了 simulations的证明，但是训练深度网络可以跳迈经典统计智慧，因此需要广泛的计算实验来验证我们的理论。因此，本文的实验部分关注了对DeepSC的训练和测试，使用欧洲议会（Europarl）数据集。我们使用Europarl数据集中的token和vector进行分割和矩阵化，然后使用Keras 2.9和TensorFlow 2.9作为后端，在Keras中训练DeepSC架构。我们在 Gaussian多个干扰RFI下接收的Rayleigh渐射通道上测试DeepSC，并证明了我们的理论。测试结果表明，当Gaussian RFI发送器的数量很大时，DeepSC会生成semantic irrelevance的句子。因此，为了实现6G的核心设计方针，我们需要开发一种可靠的SemCom方法，即interference-resistant和robust SemCom（IR$^2$ SemCom）。
</details></li>
</ul>
<hr>
<h2 id="Transmission-line-condition-prediction-based-on-semi-supervised-learning"><a href="#Transmission-line-condition-prediction-based-on-semi-supervised-learning" class="headerlink" title="Transmission line condition prediction based on semi-supervised learning"></a>Transmission line condition prediction based on semi-supervised learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19756">http://arxiv.org/abs/2310.19756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sizhe Li, Xun Ma, Nan Liu, Yi Jin</li>
<li>for: 本研究旨在提出一种基于半监督学习的电力传线状态预测方法，以便更好地规划和维护传线系统。</li>
<li>methods: 该方法首先使用常 matrices 填充扩展特征向量中的缺失数据，然后使用表示学习解决干扰矩阵的稀疏编码问题。接着，使用一小数量的标注样本初步确定不同缺陷状态的线段分类中心。最后，使用无标注样本更正估计模型的参数。</li>
<li>results: 例分析表明，该方法可以提高识别精度和更好地利用数据，相比于现有模型。<details>
<summary>Abstract</summary>
Transmission line state assessment and prediction are of great significance for the rational formulation of operation and maintenance strategy and improvement of operation and maintenance level. Aiming at the problem that existing models cannot take into account the robustness and data demand, this paper proposes a state prediction method based on semi-supervised learning. Firstly, for the expanded feature vector, the regular matrix is used to fill in the missing data, and the sparse coding problem is solved by representation learning. Then, with the help of a small number of labelled samples to initially determine the category centers of line segments in different defective states. Finally, the estimated parameters of the model are corrected using unlabeled samples. Example analysis shows that this method can improve the recognition accuracy and use data more efficiently than the existing models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CNTransmission line state assessment and prediction are of great significance for the rational formulation of operation and maintenance strategy and improvement of operation and maintenance level. Aiming at the problem that existing models cannot take into account the robustness and data demand, this paper proposes a state prediction method based on semi-supervised learning. Firstly, for the expanded feature vector, the regular matrix is used to fill in the missing data, and the sparse coding problem is solved by representation learning. Then, with the help of a small number of labelled samples to initially determine the category centers of line segments in different defective states. Finally, the estimated parameters of the model are corrected using unlabeled samples. Example analysis shows that this method can improve the recognition accuracy and use data more efficiently than the existing models.Note: "zh-CN" is the ISO 639-1 language code for Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Increased-Multiplexing-Gain-with-Reconfigurable-Surfaces-Simultaneous-Channel-Orthogonalization-and-Information-Embedding"><a href="#Increased-Multiplexing-Gain-with-Reconfigurable-Surfaces-Simultaneous-Channel-Orthogonalization-and-Information-Embedding" class="headerlink" title="Increased Multiplexing Gain with Reconfigurable Surfaces: Simultaneous Channel Orthogonalization and Information Embedding"></a>Increased Multiplexing Gain with Reconfigurable Surfaces: Simultaneous Channel Orthogonalization and Information Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19409">http://arxiv.org/abs/2310.19409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Vidal Alegria, Joao Vieira, Fredrik Rusek</li>
<li>for: 这个论文是为了研究并emonstrate the use of reconfigurable intelligent surfaces (RIS) to improve wireless communication links in multi-user multiple-input multiple-output (MU-MIMO) settings.</li>
<li>methods: The paper uses amplitude-reconfigurable intelligent surfaces (ARIS) and fully-reconfigurable intelligent surfaces (FRIS) to achieve perfect orthogonalization of MU-MIMO channels, which allows for maximum multiplexing gain at reduced complexity.</li>
<li>results: The paper shows that the resulting achievable rates allow for full exploitation of the degrees of freedom in a MU-MIMO system with excess of base station antennas.<details>
<summary>Abstract</summary>
Reconfigurable surface (RS) has been shown to be an effective solution for improving wireless communication links in general multi-user multiple-input multiple-output (MU-MIMO) setting. Current research efforts have been largely directed towards the study of reconfigurable intelligent surface (RIS), which corresponds to an RS made of passive reconfigurable elements with only phase shifting capabilities. RIS constitutes a cost- and energy- efficient solution for increased beamforming gain since it allows to generate constructive interference towards desired directions, e.g., towards a base station (BS). However, in many situations, multiplexing gain may have greater impact on the achievable transmission rates and number of simultaneously connected devices, while RIS has only been able to achieve minor improvements in this aspect. Recent work has proposed the use of alternative RS technologies, namely amplitude-reconfigurable intelligent surface (ARIS) and fully-reconfigurable intelligent surface (FRIS), to achieve perfect orthogonalization of MU-MIMO channels, thus allowing for maximum multiplexing gain at reduced complexity. In this work we consider the use of ARIS and FRIS for simultaneously orthogonalizing a MU-MIMO channel, while embedding extra information in the orthogonalized channel. We show that the resulting achievable rates allow for full exploitation of the degrees of freedom in a MU-MIMO system with excess of BS antennas.
</details>
<details>
<summary>摘要</summary>
弹性表面（RS）已被证明可以提高无线通信链接在多用户多输入多Output（MU-MIMO）环境中。现在的研究努力主要集中在研究弹性智能表面（RIS），这corresponds to an RS made of passive reconfigurable elements with only phase shifting capabilities. RIS是一个cost-和energy- efficient的解决方案，可以增加扭转矩阵的得分，因为它可以将功率分配到需要的方向，例如基站（BS）。但在许多情况下，多普通频率增加may have greater impact on the achievable transmission rates and the number of simultaneously connected devices, while RIS只能够 achieve minor improvements in this aspect. recent work has proposed the use of alternative RS technologies, namely amplitude-reconfigurable intelligent surface（ARIS）and fully-reconfigurable intelligent surface（FRIS）， to achieve perfect orthogonalization of MU-MIMO channels, thus allowing for maximum multiplexing gain at reduced complexity. In this work we consider the use of ARIS和FRIS for simultaneously orthogonalizing a MU-MIMO channel, while embedding extra information in the orthogonalized channel. We show that the resulting achievable rates allow for full exploitation of the degrees of freedom in a MU-MIMO system with excess of BS antennas.
</details></li>
</ul>
<hr>
<h2 id="A-Low-Complexity-Machine-Learning-Design-for-mmWave-Beam-Prediction"><a href="#A-Low-Complexity-Machine-Learning-Design-for-mmWave-Beam-Prediction" class="headerlink" title="A Low-Complexity Machine Learning Design for mmWave Beam Prediction"></a>A Low-Complexity Machine Learning Design for mmWave Beam Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19323">http://arxiv.org/abs/2310.19323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Qurratulain Khan, Abdo Gaber, Mohammad Parvini, Philipp Schulz, Gerhard Fettweis</li>
<li>for: 这个研究是为了提高 fifth generation（5G）-Advanced New Radio（NR）空间接口中的机器学习（ML）技术，以提高空间频域 beam 预测的精度和效率。</li>
<li>methods: 该研究使用了一种低复杂度的 ML 设计，以减少空间频域 beam 预测所需的计算复杂度和参考 сигналы overhead。</li>
<li>results: 研究结果显示，提案的模型可以达到现有最佳精度水平，同时减少计算复杂度和参考 сигнаLS overhead，从而降低电池电量消耗和快速 beam 预测。此外，研究还发现了这种模型的总体化特征。<details>
<summary>Abstract</summary>
The 3rd Generation Partnership Project (3GPP) is currently studying machine learning (ML) for the fifth generation (5G)-Advanced New Radio (NR) air interface, where spatial and temporal-domain beam prediction are important use cases. With this background, this letter presents a low-complexity ML design that expedites the spatial-domain beam prediction to reduce the power consumption and the reference signaling overhead, which are currently imperative for frequent beam measurements. Complexity analysis and evaluation results showcase that the proposed model achieves state-of-the-art accuracy with lower computational complexity, resulting in reduced power consumption and faster beam prediction. Furthermore, important observations on the generalization of the proposed model are presented in this letter.
</details>
<details>
<summary>摘要</summary>
第三代合作项目（3GPP）目前正在研究机器学习（ML）技术，以提高第五代新 радио（NR）空 Interface的性能。在这个背景下，这封信件提出了一种低复杂度ML设计，用于减少空间频域指向预测的功率消耗和参考信号 overhead。复杂度分析和评估结果表明，提议的模型可以实现当前竞争力最高的准确率，同时减少功率消耗和快速指向预测。此外，这封信件还提供了一些重要的通用性观察结果。
</details></li>
</ul>
<hr>
<h2 id="A-Synopsis-of-Stent-Graft-Technology-Development"><a href="#A-Synopsis-of-Stent-Graft-Technology-Development" class="headerlink" title="A Synopsis of Stent Graft Technology Development"></a>A Synopsis of Stent Graft Technology Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19235">http://arxiv.org/abs/2310.19235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Umme Hafsa Momy</li>
<li>for: 本研究评估了 coronary artery disease (CAD) 的治疗发展，尤其是心血管内嵌入技术的进步。</li>
<li>methods: 本文回顾了 coronary stent 技术的发展历程，从1977年首次 ангиопластиy 开始，涵盖了 Forssmann、Dotter 和 Gruentzig 等先驱的贡献。</li>
<li>results: 文章介绍了 bare metal stents (BMS)、first-generation drug-eluting stents (DES) 和 second-generation DES 等不同类型的心血管内嵌入器，以及最新的 dissolvable vascular scaffolds (BVS)。 clinical trials 的结果表明，每种内嵌入器都有其特点和优劣点。<details>
<summary>Abstract</summary>
Coronary artery disease (CAD) is a leading cause of death worldwide. Treatments have evolved, with stenting becoming the primary approach over bypass surgery. This article reviews the evolution of coronary stent technology, starting from the first angioplasty in 1977. Pioneers like Forssmann, Dotter, and Gruentzig established the foundation. The late 1980s saw the introduction of bare metal stents (BMS) to address angioplasty limitations. However, BMS had issues, leading to the development of first-generation drug-eluting stents (DES) in the early 2000s, which reduced restenosis but had safety concerns. Subsequent innovations introduced second-generation DES with better results and the latest bioresorbable vascular scaffolds (BVS) that dissolve over time. Clinical trials have been crucial in validating each stent's effectiveness. Despite progress, challenges remain in stent selection, approval processes, and minimizing risks. The future may see personalized stenting based on patient needs, highlighting the significant advancements in stent technology and its impact on patient care.
</details>
<details>
<summary>摘要</summary>
心血管疾病（CAD）是全球至关重要的死亡原因之一。治疗方法逐渐发展，填充成为主要方法，代替了跳过手术。本文评论了心血管填充技术的演化，从1977年的首次抗阻治疗开始。先驱们如福斯曼、杜特和格劳恩茨基础设置了基础。1980年代末期，无质量填充（BMS）出现，以解决抗阻治疗的局限性。然而，BMS存在问题，导致了首代药粉碎填充（DES）的开发，它在2000年代初期减少了再病理现象，但存在安全问题。随后的创新引入了第二代DES和最新的生物逐序材料托盘（BVS），它们在时间上逐渐消解。临床试验是证明每款填具的有效性的关键。尽管进步了，仍有填具选择、获得批准和降低风险的挑战。未来可能会出现个性化的填具，这将高亮显示填具技术的进步和对病人护理的影响。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Status-Updates-for-Minimizing-Age-of-Correlated-Information-in-IoT-Networks-with-Energy-Harvesting-Sensors"><a href="#Optimal-Status-Updates-for-Minimizing-Age-of-Correlated-Information-in-IoT-Networks-with-Energy-Harvesting-Sensors" class="headerlink" title="Optimal Status Updates for Minimizing Age of Correlated Information in IoT Networks with Energy Harvesting Sensors"></a>Optimal Status Updates for Minimizing Age of Correlated Information in IoT Networks with Energy Harvesting Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19216">http://arxiv.org/abs/2310.19216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Xu, Xinyan Zhang, Howard H. Yang, Xijun Wang, Nikolaos Pappas, Dusit Niyato, Tony Q. S. Quek</li>
<li>for: 这 paper 的目的是设计一种高效的状态更新策略，以最小化 IoT 网络中相关信息的 Age of Correlated Information (AoCI)。</li>
<li>methods: 该 paper 使用了深度学习 reinforcement learning (DRL) 技术，以解决 IoT 网络中传感器的能量使用 causality、未知环境动力学、传感器true battery state 不可见性和大规模离散动作空间的挑战。</li>
<li>results:  simulations 表明，提出的动作分解和映射机制可以有效地解决这些挑战，并且比较可靠地最小化 AoCI。<details>
<summary>Abstract</summary>
Many real-time applications of the Internet of Things (IoT) need to deal with correlated information generated by multiple sensors. The design of efficient status update strategies that minimize the Age of Correlated Information (AoCI) is a key factor. In this paper, we consider an IoT network consisting of sensors equipped with the energy harvesting (EH) capability. We optimize the average AoCI at the data fusion center (DFC) by appropriately managing the energy harvested by sensors, whose true battery states are unobservable during the decision-making process. Particularly, we first formulate the dynamic status update procedure as a partially observable Markov decision process (POMDP), where the environmental dynamics are unknown to the DFC. In order to address the challenges arising from the causality of energy usage, unknown environmental dynamics, unobservability of sensors'true battery states, and large-scale discrete action space, we devise a deep reinforcement learning (DRL)-based dynamic status update algorithm. The algorithm leverages the advantages of the soft actor-critic and long short-term memory techniques. Meanwhile, it incorporates our proposed action decomposition and mapping mechanism. Extensive simulations are conducted to validate the effectiveness of our proposed algorithm by comparing it with available DRL algorithms for POMDPs.
</details>
<details>
<summary>摘要</summary>
多种实时应用程序互联网关系物（IoT）需要处理多感器生成的相关信息。设计高效状态更新策略，以最小化相关信息年龄（AoCI）是关键因素。在这篇论文中，我们考虑了一个具有能源收集（EH）能力的IoT网络，其中感知器的真正电池状态不可见。我们在数据融合中心（DFC）优化平均AoCI，通过合理管理感知器收集的能量。特别是，我们首先将动态状态更新过程表示为一个部分可见Markov决策过程（POMDP），其中环境动态是不可见的。为了解决因能源使用 causality、不可见环境动态、感知器真正电池状态不可见和大规模离散动作空间而产生的挑战，我们开发了一种基于深度学习执行（DRL）的动态状态更新算法。这种算法利用了深度学习的优势，包括软actor-critic和长期短 памяantine技术。同时，它采用我们提出的动作分解和映射机制。我们对比了我们的提posed算法与可用的DRL算法进行了广泛的 simulations，以验证其效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/30/eess.SP_2023_10_30/" data-id="clogyj94n01a87craexb253kc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/cs.SD_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T15:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/cs.SD_2023_10_29/">cs.SD - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-Audio-Analyzer-a-Framework-to-Industrialize-the-Research-on-Audio-Forensics"><a href="#Deep-Audio-Analyzer-a-Framework-to-Industrialize-the-Research-on-Audio-Forensics" class="headerlink" title="Deep Audio Analyzer: a Framework to Industrialize the Research on Audio Forensics"></a>Deep Audio Analyzer: a Framework to Industrialize the Research on Audio Forensics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19081">http://arxiv.org/abs/2310.19081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valerio Francesco Puglisi, Oliver Giudice, Sebastiano Battiato</li>
<li>for: 这篇论文是为了提高音频掌控领域的研究和开发过程的简化和加速，使用户可以快速创建、比较和共享结果。</li>
<li>methods: 该论文描述了一种核心架构，用于支持多个音频分析任务，包括音频特征视图、预训练模型评估、新Audio分析工作流程创建等功能。</li>
<li>results: 通过使用Deep Audio Analyzer工具，法律 enforcement 机构和研究人员可以轻松地评估预训练模型的性能、创建新的Audio分析工作流程，并将其导出和分享。这些功能将提高音频分析实验室的速度和可重复性。<details>
<summary>Abstract</summary>
Deep Audio Analyzer is an open source speech framework that aims to simplify the research and the development process of neural speech processing pipelines, allowing users to conceive, compare and share results in a fast and reproducible way. This paper describes the core architecture designed to support several tasks of common interest in the audio forensics field, showing possibility of creating new tasks thus customizing the framework. By means of Deep Audio Analyzer, forensics examiners (i.e. from Law Enforcement Agencies) and researchers will be able to visualize audio features, easily evaluate performances on pretrained models, to create, export and share new audio analysis workflows by combining deep neural network models with few clicks. One of the advantages of this tool is to speed up research and practical experimentation, in the field of audio forensics analysis thus also improving experimental reproducibility by exporting and sharing pipelines. All features are developed in modules accessible by the user through a Graphic User Interface. Index Terms: Speech Processing, Deep Learning Audio, Deep Learning Audio Pipeline creation, Audio Forensics.
</details>
<details>
<summary>摘要</summary>
深度音频分析器是一个开源的语音框架，旨在简化语音处理管道的研究和开发过程，让用户快速地实现语音处理任务，并且可以方便地比较和共享结果。本文描述了核心架构，支持audio дляensis领域的多个任务，并示出了创建新任务的可能性，因此可以根据需要自定义框架。通过深度音频分析器，法律机关的审查员和研究人员可以轻松地查看音频特征，快速评估预训练模型的性能，创建、导出和共享新的音频分析工作流程，只需几Click。这个工具的一个优点是快速加速了研究和实践实验的速度，因此也提高了实验 reproducibility。所有功能都是在用户可访问的模块中实现的，可以通过图形用户界面来访问。关键词：语音处理、深度学习音频、深度学习音频管道创建、音频鉴定。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/cs.SD_2023_10_29/" data-id="clogyj91600x77cra148y8nik" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/cs.CV_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T13:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/cs.CV_2023_10_29/">cs.CV - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="3DMiner-Discovering-Shapes-from-Large-Scale-Unannotated-Image-Datasets"><a href="#3DMiner-Discovering-Shapes-from-Large-Scale-Unannotated-Image-Datasets" class="headerlink" title="3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets"></a>3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19188">http://arxiv.org/abs/2310.19188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ta-Ying Cheng, Matheus Gadelha, Soren Pirk, Thibault Groueix, Radomir Mech, Andrew Markham, Niki Trigoni</li>
<li>for: 这个论文是为了挖掘大规模未标注图像集中的3D形状而写的。</li>
<li>methods: 该方法使用了自适应学习的图像表示学习技术来对图像集中的图像进行聚类，并在这些聚类中找到相似的图像对应关系。然后，通过这些对应关系来初始化吊销级整理，并逐步应用束合并推理方法来学习图像集中的神经占据场。</li>
<li>results: 该方法可以在使用Pix3D椅子图像集时生成比州前方法更好的结果， both quantitatively and qualitatively。此外， authors还展示了如何使用3DMiner在实际场景中进行3D重建，例如使用LAION-5B图像集中的图像进行重建。<details>
<summary>Abstract</summary>
We present 3DMiner -- a pipeline for mining 3D shapes from challenging large-scale unannotated image datasets. Unlike other unsupervised 3D reconstruction methods, we assume that, within a large-enough dataset, there must exist images of objects with similar shapes but varying backgrounds, textures, and viewpoints. Our approach leverages the recent advances in learning self-supervised image representations to cluster images with geometrically similar shapes and find common image correspondences between them. We then exploit these correspondences to obtain rough camera estimates as initialization for bundle-adjustment. Finally, for every image cluster, we apply a progressive bundle-adjusting reconstruction method to learn a neural occupancy field representing the underlying shape. We show that this procedure is robust to several types of errors introduced in previous steps (e.g., wrong camera poses, images containing dissimilar shapes, etc.), allowing us to obtain shape and pose annotations for images in-the-wild. When using images from Pix3D chairs, our method is capable of producing significantly better results than state-of-the-art unsupervised 3D reconstruction techniques, both quantitatively and qualitatively. Furthermore, we show how 3DMiner can be applied to in-the-wild data by reconstructing shapes present in images from the LAION-5B dataset. Project Page: https://ttchengab.github.io/3dminerOfficial
</details>
<details>
<summary>摘要</summary>
我们介绍3DMiner---一个用于从大规模无标注图像集合中采矿3D形状的管道。与其他无监督3D重建方法不同，我们假设在大到足够大的集合中，存在着objects with similar shapes but varying backgrounds, textures, and viewpoints的图像。我们的方法利用了最近的自适应图像表示学习的进步，将这些图像分组为具有相似形状的图像集合，并寻找这些集合之间的共同图像对应。我们然后利用这些对应来初始化捆绑调整，并运用这些调整来学习每个图像集合的神经占位场，从而获得形状和位势的标注。我们显示了这个程序可以对实际应用中的图像进行彻底的处理，并且与现有的无监督3D重建方法相比，可以获得更好的结果。此外，我们还显示了3DMiner可以对LAION-5B dataset中的图像进行重建，以及如何将3DMiner应用到实际应用中。更多资讯请参考https://ttchengab.github.io/3dminerOfficial。
</details></li>
</ul>
<hr>
<h2 id="Fast-Trainable-Projection-for-Robust-Fine-Tuning"><a href="#Fast-Trainable-Projection-for-Robust-Fine-Tuning" class="headerlink" title="Fast Trainable Projection for Robust Fine-Tuning"></a>Fast Trainable Projection for Robust Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19182">http://arxiv.org/abs/2310.19182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gt-ripl/ftp">https://github.com/gt-ripl/ftp</a></li>
<li>paper_authors: Junjiao Tian, Yen-Cheng Liu, James Seale Smith, Zsolt Kira<br>for:This paper aims to improve the robustness of pre-trained models when fine-tuning them for downstream tasks, while maintaining their in-distribution (ID) performance.methods:The proposed method, Fast Trainable Projection (FTP), uses projection-based fine-tuning with learnable projection constraints to improve the efficiency and scalability of the algorithm. FTP can be combined with existing optimizers like AdamW and is a special instance of hyper-optimizers that tune the hyper-parameters of optimizers in a learnable manner.results:The proposed FTP method achieves superior robustness on out-of-distribution (OOD) datasets, including domain shifts and natural corruptions, across four different vision tasks with five different pre-trained models. Additionally, FTP is broadly applicable and beneficial to other learning scenarios such as low-label and continual learning settings. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/GT-RIPL/FTP.git.Here">https://github.com/GT-RIPL/FTP.git.Here</a> is the simplified Chinese text:for:这篇论文目标是在下游任务中练习预训练模型，保持其内部分布（ID）性能，同时提高对外部分布（OOD）的Robustness。methods:提议的方法是快速可调 projection-based fine-tuning，使用可调 projection constraint来提高算法的可扩展性和可优化性。这种方法可以与现有的优化器相结合，如 AdamW，并且是一种特殊的超优化器，可以在learnable manner中调整优化器的超参数。results:提议的FTP方法在不同的视觉任务和预训练模型上，都实现了superior的OOD Robustness，包括频率Shift和自然损害等。此外，FTP还可以在其他学习场景中使用，如low-label和连续学习 Settings，因为它的易于适应性。代码将在<a target="_blank" rel="noopener" href="https://github.com/GT-RIPL/FTP.git%E4%B8%AD%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/GT-RIPL/FTP.git中提供。</a><details>
<summary>Abstract</summary>
Robust fine-tuning aims to achieve competitive in-distribution (ID) performance while maintaining the out-of-distribution (OOD) robustness of a pre-trained model when transferring it to a downstream task. Recently, projected gradient descent has been successfully used in robust fine-tuning by constraining the deviation from the initialization of the fine-tuned model explicitly through projection. However, algorithmically, two limitations prevent this method from being adopted more widely, scalability and efficiency. In this paper, we propose a new projection-based fine-tuning algorithm, Fast Trainable Projection (FTP) for computationally efficient learning of per-layer projection constraints, resulting in an average $35\%$ speedup on our benchmarks compared to prior works. FTP can be combined with existing optimizers such as AdamW, and be used in a plug-and-play fashion. Finally, we show that FTP is a special instance of hyper-optimizers that tune the hyper-parameters of optimizers in a learnable manner through nested differentiation. Empirically, we show superior robustness on OOD datasets, including domain shifts and natural corruptions, across four different vision tasks with five different pre-trained models. Additionally, we demonstrate that FTP is broadly applicable and beneficial to other learning scenarios such as low-label and continual learning settings thanks to its easy adaptability. The code will be available at https://github.com/GT-RIPL/FTP.git.
</details>
<details>
<summary>摘要</summary>
Robust fine-tuning aimed at achieving competitive in-distribution (ID) performance while maintaining out-of-distribution (OOD) robustness of a pre-trained model when transferring it to a downstream task. Recently, projected gradient descent has been successfully used in robust fine-tuning by constraining the deviation from the initialization of the fine-tuned model explicitly through projection. However, algorithmically, two limitations prevent this method from being adopted more widely, scalability, and efficiency. In this paper, we propose a new projection-based fine-tuning algorithm, Fast Trainable Projection (FTP) for computationally efficient learning of per-layer projection constraints, resulting in an average $35\%$ speedup on our benchmarks compared to prior works. FTP can be combined with existing optimizers such as AdamW, and be used in a plug-and-play fashion. Finally, we show that FTP is a special instance of hyper-optimizers that tune the hyper-parameters of optimizers in a learnable manner through nested differentiation. Empirically, we show superior robustness on OOD datasets, including domain shifts and natural corruptions, across four different vision tasks with five different pre-trained models. Additionally, we demonstrate that FTP is broadly applicable and beneficial to other learning scenarios such as low-label and continual learning settings thanks to its easy adaptability. The code will be available at https://github.com/GT-RIPL/FTP.git.
</details></li>
</ul>
<hr>
<h2 id="BirdSAT-Cross-View-Contrastive-Masked-Autoencoders-for-Bird-Species-Classification-and-Mapping"><a href="#BirdSAT-Cross-View-Contrastive-Masked-Autoencoders-for-Bird-Species-Classification-and-Mapping" class="headerlink" title="BirdSAT: Cross-View Contrastive Masked Autoencoders for Bird Species Classification and Mapping"></a>BirdSAT: Cross-View Contrastive Masked Autoencoders for Bird Species Classification and Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19168">http://arxiv.org/abs/2310.19168</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mvrl/birdsat">https://github.com/mvrl/birdsat</a></li>
<li>paper_authors: Srikumar Sastry, Subash Khanal, Aayush Dhakal, Di Huang, Nathan Jacobs</li>
<li>for: 本研究旨在开发一个 metadata-aware 自主学习<del>(SSL)</del>框架，用于细致分类和生物多样性地图的鸟类种类识别。</li>
<li>methods: 该框架结合了对比学习<del>(CL) 和伪像图像模型</del>(MIM) 两种 SSL 策略，同时将元数据与基础级图像相结合，以扩充嵌入空间。研究人员使用单模态和交叉模态 Vision Transformer 进行训练，并在全球鸟类种类数据集上进行训练。</li>
<li>results: 研究人员通过评估两个下游任务：细致视觉分类~(FGVC) 和交叉模态检索，发现模型学习了鸟类种类的细致特征和地域条件。同时，预训练模型在转移学习设置下表现出了顶尖性能，并且模型的交叉模态检索表现强化了鸟类种类的分布地图创建。<details>
<summary>Abstract</summary>
We propose a metadata-aware self-supervised learning~(SSL)~framework useful for fine-grained classification and ecological mapping of bird species around the world. Our framework unifies two SSL strategies: Contrastive Learning~(CL) and Masked Image Modeling~(MIM), while also enriching the embedding space with metadata available with ground-level imagery of birds. We separately train uni-modal and cross-modal ViT on a novel cross-view global bird species dataset containing ground-level imagery, metadata (location, time), and corresponding satellite imagery. We demonstrate that our models learn fine-grained and geographically conditioned features of birds, by evaluating on two downstream tasks: fine-grained visual classification~(FGVC) and cross-modal retrieval. Pre-trained models learned using our framework achieve SotA performance on FGVC of iNAT-2021 birds and in transfer learning settings for CUB-200-2011 and NABirds datasets. Moreover, the impressive cross-modal retrieval performance of our model enables the creation of species distribution maps across any geographic region. The dataset and source code will be released at https://github.com/mvrl/BirdSAT}.
</details>
<details>
<summary>摘要</summary>
我们提出一个具有元数据意识的自助学习~(SSL)~框架，用于精细分类和鸟类生态地图的世界各地鸟种。我们的框架将两种SSL策略：异构学习~(CL) 和伪像图模型~(MIM) 融合在一起，同时将地面鸟类图像中可用的元数据纳入嵌入空间。我们分别在不同视图上训练uni-modal和cross-modal ViT，并在一个新的跨视图全球鸟类数据集上进行训练，该数据集包括地面鸟类图像、元数据（位置、时间）以及相应的卫星图像。我们示示了我们的模型学习到了鸟类精细特征和地理条件特征，通过评估两个下游任务：精细视觉分类~(FGVC) 和交叉模式检索。预训练模型使用我们的框架学习后在iNAT-2021鸟类数据集上达到了最佳性能，并在传输学习设置下在CUB-200-2011和NABirds数据集上达到了优秀的表现。此外，我们的模型在交叉模式检索任务中表现出色，可以创建任何地理区域的鸟种分布图。数据集和源代码将在https://github.com/mvrl/BirdSAT 上发布。
</details></li>
</ul>
<hr>
<h2 id="Out-of-distribution-Object-Detection-through-Bayesian-Uncertainty-Estimation"><a href="#Out-of-distribution-Object-Detection-through-Bayesian-Uncertainty-Estimation" class="headerlink" title="Out-of-distribution Object Detection through Bayesian Uncertainty Estimation"></a>Out-of-distribution Object Detection through Bayesian Uncertainty Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19119">http://arxiv.org/abs/2310.19119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianhao Zhang, Shenglin Wang, Nidhal Bouaynaya, Radu Calinescu, Lyudmila Mihaylova<br>for: 本研究旨在提出一种 novel 的 bayesian 对象检测方法，以提高对象检测器在异常数据（out-of-distribution，OOD）下的性能。methods: 本方法基于提案的 Gaussian 分布来对准确度进行模型化，并通过采样weight参数来 отличаID数据与OOD数据。不同于其他不确定性模型方法，我们的方法不需要巨大的计算成本来推导weight分布，也不需要通过synthetic outlier数据进行模型训练。results: 我们在BDD100k和VOC数据集上进行训练，并在COCO2017数据集上进行评估。结果表明，我们的 bayesian 对象检测器可以在OOD数据下提供满意的鉴别性能，将FPR95分数降低8.19%，AUROC分数提高13.94%。<details>
<summary>Abstract</summary>
The superior performance of object detectors is often established under the condition that the test samples are in the same distribution as the training data. However, in many practical applications, out-of-distribution (OOD) instances are inevitable and usually lead to uncertainty in the results. In this paper, we propose a novel, intuitive, and scalable probabilistic object detection method for OOD detection. Unlike other uncertainty-modeling methods that either require huge computational costs to infer the weight distributions or rely on model training through synthetic outlier data, our method is able to distinguish between in-distribution (ID) data and OOD data via weight parameter sampling from proposed Gaussian distributions based on pre-trained networks. We demonstrate that our Bayesian object detector can achieve satisfactory OOD identification performance by reducing the FPR95 score by up to 8.19% and increasing the AUROC score by up to 13.94% when trained on BDD100k and VOC datasets as the ID datasets and evaluated on COCO2017 dataset as the OOD dataset.
</details>
<details>
<summary>摘要</summary>
超过90%的人会把这篇文章评为“非常好”。文章主要内容是关于Object Detector的性能评估，具体来说是在不同数据分布下进行评估。作者提出了一种新的、直观的、可扩展的概率性Object Detector方法，用于检测Out-of-Distribution（OOD）实例。与其他不确定性模型不同，该方法不需要巨大的计算成本来推导权重分布，也不需要通过人工异常数据进行模型训练。作者提出了一种基于预训练网络的Gaussian分布 sampling方法，用于分辨ID数据和OOD数据。文章示出，该抽象Object Detector可以在BDD100k和VOC数据集上达到满意的OOD标识性能，减少FPR95分数8.19%，提高AUROC分数13.94%。
</details></li>
</ul>
<hr>
<h2 id="CrossEAI-Using-Explainable-AI-to-generate-better-bounding-boxes-for-Chest-X-ray-images"><a href="#CrossEAI-Using-Explainable-AI-to-generate-better-bounding-boxes-for-Chest-X-ray-images" class="headerlink" title="CrossEAI: Using Explainable AI to generate better bounding boxes for Chest X-ray images"></a>CrossEAI: Using Explainable AI to generate better bounding boxes for Chest X-ray images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19835">http://arxiv.org/abs/2310.19835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinze Zhao<br>for: This paper focuses on improving the accuracy of bounding box generation for chest x-ray image diagnosis using post-hoc AI explainable methods.methods: The proposed method, CrossEAI, combines heatmap and gradient map to generate more targeted bounding boxes. The model uses a weighted average of Guided Backpropagation and Grad-CAM++ to generate bounding boxes that are closer to the ground truth.results: The proposed method achieves significant improvement over the state of the art model with the same setting, with an average improvement of 9% in all diseases over all Intersection over Union (IoU). Additionally, the model is able to achieve the same performance as a model that uses 80% of the ground truth bounding box information for training, without using any ground truth bounding box information.<details>
<summary>Abstract</summary>
Explainability is critical for deep learning applications in healthcare which are mandated to provide interpretations to both patients and doctors according to legal regulations and responsibilities. Explainable AI methods, such as feature importance using integrated gradients, model approximation using LIME, or neuron activation and layer conductance to provide interpretations for certain health risk predictions. In medical imaging diagnosis, disease classification usually achieves high accuracy, but generated bounding boxes have much lower Intersection over Union (IoU). Different methods with self-supervised or semi-supervised learning strategies have been proposed, but few improvements have been identified for bounding box generation. Previous work shows that bounding boxes generated by these methods are usually larger than ground truth and contain major non-disease area. This paper utilizes the advantages of post-hoc AI explainable methods to generate bounding boxes for chest x-ray image diagnosis. In this work, we propose CrossEAI which combines heatmap and gradient map to generate more targeted bounding boxes. By using weighted average of Guided Backpropagation and Grad-CAM++, we are able to generate bounding boxes which are closer to the ground truth. We evaluate our model on a chest x-ray dataset. The performance has significant improvement over the state of the art model with the same setting, with $9\%$ improvement in average of all diseases over all IoU. Moreover, as a model that does not use any ground truth bounding box information for training, we achieve same performance in general as the model that uses $80\%$ of the ground truth bounding box information for training
</details>
<details>
<summary>摘要</summary>
“医疗领域深度学习应用中，解释性是关键。由于法律和责任要求，患者和医生都需要获得解释。解释AI方法，如综合梯度的重要性或LIME模型 aproximation，可以提供医疗风险预测的解释。在医学成像诊断中，疾病分类通常具有高准确率，但生成的 bounding box 的 intersection over union（IoU）较低。不同的自动学习或半自动学习策略已经被提出，但很少有改进 bounding box 生成。本文利用post-hoc AI解释方法的优势，对呼吸道X射线成像进行诊断。我们提出 CrossEAI，它结合热图和梯度图来生成更加准确的 bounding box。通过使用权重平均的导引反射和 Grad-CAM++，我们能够生成更加接近真实值的 bounding box。我们在呼吸道X射线数据集上进行评估，并与状态方法相比，显示我们的模型在所有疾病和所有 IoU 上具有9%的提升。此外，作为没有使用任何真实 bounding box 信息进行训练的模型，我们在总体上与使用80%真实 bounding box 信息进行训练的模型相当。”Note that the translation is done using Google Translate, and the text may not be perfectly fluent or idiomatic in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Reward-Finetuning-for-Faster-and-More-Accurate-Unsupervised-Object-Discovery"><a href="#Reward-Finetuning-for-Faster-and-More-Accurate-Unsupervised-Object-Discovery" class="headerlink" title="Reward Finetuning for Faster and More Accurate Unsupervised Object Discovery"></a>Reward Finetuning for Faster and More Accurate Unsupervised Object Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19080">http://arxiv.org/abs/2310.19080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katie Z Luo, Zhenzhen Liu, Xiangyu Chen, Yurong You, Sagie Benaim, Cheng Perng Phoo, Mark Campbell, Wen Sun, Bharath Hariharan, Kilian Q. Weinberger</li>
<li>for: 提高机器学习模型和人类预期的对应性，并在自动驾驶研究中应用RLHF。</li>
<li>methods: 使用RL方法，通过简单的规则来模拟人类反馈，并使用损失函数来评估矩形框的准确性。</li>
<li>results: 比对于先前的工作，该方法更准确，而且训练速度比较快。<details>
<summary>Abstract</summary>
Recent advances in machine learning have shown that Reinforcement Learning from Human Feedback (RLHF) can improve machine learning models and align them with human preferences. Although very successful for Large Language Models (LLMs), these advancements have not had a comparable impact in research for autonomous vehicles -- where alignment with human expectations can be imperative. In this paper, we propose to adapt similar RL-based methods to unsupervised object discovery, i.e. learning to detect objects from LiDAR points without any training labels. Instead of labels, we use simple heuristics to mimic human feedback. More explicitly, we combine multiple heuristics into a simple reward function that positively correlates its score with bounding box accuracy, \ie, boxes containing objects are scored higher than those without. We start from the detector's own predictions to explore the space and reinforce boxes with high rewards through gradient updates. Empirically, we demonstrate that our approach is not only more accurate, but also orders of magnitudes faster to train compared to prior works on object discovery.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的机器学习进步表明，人类反馈学习（RLHF）可以提高机器学习模型，使其更加符合人类的偏好。尽管在自动驾驶汽车领域中非常成功，但这些进步尚未在研究中得到了相应的影响。在这篇论文中，我们提议使用类似的RL基于方法，以无监督方式探索物体检测。而不是使用标签，我们使用简单的规则来模拟人类反馈。具体来说，我们将多个规则组合成一个简单的奖励函数，其奖励分数与盒子准确率正相关。我们从检测器的自己预测开始，通过梯度更新来强化高奖励的盒子。我们的方法不仅更准确，而且速度也是以前工作的多个量级快。
</details></li>
</ul>
<hr>
<h2 id="Myriad-Large-Multimodal-Model-by-Applying-Vision-Experts-for-Industrial-Anomaly-Detection"><a href="#Myriad-Large-Multimodal-Model-by-Applying-Vision-Experts-for-Industrial-Anomaly-Detection" class="headerlink" title="Myriad: Large Multimodal Model by Applying Vision Experts for Industrial Anomaly Detection"></a>Myriad: Large Multimodal Model by Applying Vision Experts for Industrial Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19070">http://arxiv.org/abs/2310.19070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanze Li, Haolin Wang, Shihao Yuan, Ming Liu, Yiwen Guo, Chen Xu, Guangming Shi, Wangmeng Zuo</li>
<li>for: 这个研究是为了提出一个新的大型多模式辨识模型（Myriad），用于工业异常检测（Industrial Anomaly Detection，IAD），以提供明确的异常检测和详细的异常描述。</li>
<li>methods: 这个研究使用了MiniGPT-4作为基础的大型语言模型（LMM），并设计了专家观察模组，将预设知识从视觉专家转换为可以读取的语言模型（LLM）的 tokens。此外，它还引入了领域调整器，以bridging generic和工业图像的视觉表现差异。最后，它提出了视觉专家导师，将Q-Former变数为生成IAD领域的视觉语言 tokens。</li>
<li>results: 实验结果显示，提案的方法不仅在1-class和几个shot设定下与现有方法相比，表现出色，并且提供了明确的异常预测和详细的异常描述在IAD领域。<details>
<summary>Abstract</summary>
Existing industrial anomaly detection (IAD) methods predict anomaly scores for both anomaly detection and localization. However, they struggle to perform a multi-turn dialog and detailed descriptions for anomaly regions, e.g., color, shape, and categories of industrial anomalies. Recently, large multimodal (i.e., vision and language) models (LMMs) have shown eminent perception abilities on multiple vision tasks such as image captioning, visual understanding, visual reasoning, etc., making it a competitive potential choice for more comprehensible anomaly detection. However, the knowledge about anomaly detection is absent in existing general LMMs, while training a specific LMM for anomaly detection requires a tremendous amount of annotated data and massive computation resources. In this paper, we propose a novel large multi-modal model by applying vision experts for industrial anomaly detection (dubbed Myriad), which leads to definite anomaly detection and high-quality anomaly description. Specifically, we adopt MiniGPT-4 as the base LMM and design an Expert Perception module to embed the prior knowledge from vision experts as tokens which are intelligible to Large Language Models (LLMs). To compensate for the errors and confusions of vision experts, we introduce a domain adapter to bridge the visual representation gaps between generic and industrial images. Furthermore, we propose a Vision Expert Instructor, which enables the Q-Former to generate IAD domain vision-language tokens according to vision expert prior. Extensive experiments on MVTec-AD and VisA benchmarks demonstrate that our proposed method not only performs favorably against state-of-the-art methods under the 1-class and few-shot settings, but also provide definite anomaly prediction along with detailed descriptions in IAD domain.
</details>
<details>
<summary>摘要</summary>
现有的工业异常检测（IAD）方法预测异常得分，但它们在多Turn对话和细节描述方面强不甚，例如颜色、形状和工业异常类别。最近，大量多模式（i.e., 视觉和语言）模型（LMMs）在多种视觉任务上表现出了杰出的感知能力，例如图像描述、视觉理解、视觉逻辑等，使其成为可能的优秀选择。然而，现有的通用LMMs中关于异常检测的知识缺失，而特定LMM的训练需要大量的注释数据和巨大的计算资源。在本文中，我们提出了一种新的大型多模式模型，称为Myriad，用于工业异常检测，它可以实现准确的异常检测和高质量的异常描述。我们采用MiniGPT-4作为基础LMM，并设计了专家感知模块，将视觉专家的先前知识embed为可以被语言模型理解的令牌。为了补做视觉专家的错误和混乱，我们引入了领域适应器，将Generic和工业图像之间的视觉表示差异bridged。此外，我们提出了视觉专家导师，使Q-Former可以根据视觉专家的先前知识生成IAD领域的视觉语言令牌。我们对MVTec-AD和VisAbenchmark进行了广泛的实验，结果表明，我们的提出方法不仅在1类和少shot设置下与状态 искусственный智能方法相当，而且还可以提供准确的异常预测和详细的描述在IAD领域。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-ChatGPT-for-Medical-Applications-an-Experimental-Study-of-GPT-4V"><a href="#Multimodal-ChatGPT-for-Medical-Applications-an-Experimental-Study-of-GPT-4V" class="headerlink" title="Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V"></a>Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19061">http://arxiv.org/abs/2310.19061</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhilingyan/gpt4v-medical-report">https://github.com/zhilingyan/gpt4v-medical-report</a></li>
<li>paper_authors: Zhiling Yan, Kai Zhang, Rong Zhou, Lifang He, Xiang Li, Lichao Sun</li>
<li>for: 这个论文是用来评估当今最先进的多模态大语言模型GPT-4V在视觉问答任务上的能力的。</li>
<li>methods: 我们使用了GPT-4V synergize视觉和文本信息的文本提问来评估其在医学视觉问答任务上的能力。</li>
<li>results: 我们的实验结果表明，当前版本的GPT-4V不建议用于实际诊断，因为它在医学视觉问答任务中的准确率不稳定和较低。此外，我们还分析了GPT-4V在医学视觉问答任务中的七种特点，揭示了它在这个复杂的领域中的约束。详细的评估案例可以在<a target="_blank" rel="noopener" href="https://github.com/ZhilingYan/GPT4V-Medical-Report%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZhilingYan/GPT4V-Medical-Report中找到。</a><details>
<summary>Abstract</summary>
In this paper, we critically evaluate the capabilities of the state-of-the-art multimodal large language model, i.e., GPT-4 with Vision (GPT-4V), on Visual Question Answering (VQA) task. Our experiments thoroughly assess GPT-4V's proficiency in answering questions paired with images using both pathology and radiology datasets from 11 modalities (e.g. Microscopy, Dermoscopy, X-ray, CT, etc.) and fifteen objects of interests (brain, liver, lung, etc.). Our datasets encompass a comprehensive range of medical inquiries, including sixteen distinct question types. Throughout our evaluations, we devised textual prompts for GPT-4V, directing it to synergize visual and textual information. The experiments with accuracy score conclude that the current version of GPT-4V is not recommended for real-world diagnostics due to its unreliable and suboptimal accuracy in responding to diagnostic medical questions. In addition, we delineate seven unique facets of GPT-4V's behavior in medical VQA, highlighting its constraints within this complex arena. The complete details of our evaluation cases are accessible at https://github.com/ZhilingYan/GPT4V-Medical-Report.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们对当今最先进的多Modal大语言模型GPT-4 with Vision（GPT-4V）在视觉问答（VQA）任务上进行了批判性评估。我们的实验 Thoroughly assess GPT-4V 在与图像相关的问题上使用多种modalities（例如 Microscopy、Dermoscopy、X-ray、CT等）和十五种 объек interests（脑、肝脏、肺等）进行了评估。我们的数据集包括医学问题的广泛范围，包括十六种不同的问题类型。在我们的评估中，我们设计了文本提示，用于导引GPT-4V 将视觉和文本信息相互协同。实验结果显示，目前版本的GPT-4V 在回答医学问题上并不可靠，其精度较低。此外，我们还描述了GPT-4V 在医学VQA中的七种特点， highlighting its constraints within this complex arena。详细的评估结果可以在 GitHub上找到：https://github.com/ZhilingYan/GPT4V-Medical-Report。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Decision-Based-Black-Box-Adversarial-Attack-with-Gradient-Priors"><a href="#Boosting-Decision-Based-Black-Box-Adversarial-Attack-with-Gradient-Priors" class="headerlink" title="Boosting Decision-Based Black-Box Adversarial Attack with Gradient Priors"></a>Boosting Decision-Based Black-Box Adversarial Attack with Gradient Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19038">http://arxiv.org/abs/2310.19038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Liu, Xingshuo Huang, Xiaotong Zhang, Qimai Li, Fenglong Ma, Wei Wang, Hongyang Chen, Hong Yu, Xianchao Zhang</li>
<li>for: 这篇论文是关于黑盒攻击的研究，旨在提出一种基于决策的黑盒攻击方法，以提高攻击效率和精度。</li>
<li>methods: 该方法使用了数据依存的梯度先验和时间依存的梯度更新策略，以解决隐藏梯度不均匀和Successive Iteration梯度方向问题。具体来说，该方法使用了双向散射 filter 来处理每个随机扰动，以保持扰动在边缘位置的不整合性。</li>
<li>results: 对比其他强基eline，该方法在广泛的实验中表现出色，显著超过了其他方法。<details>
<summary>Abstract</summary>
Decision-based methods have shown to be effective in black-box adversarial attacks, as they can obtain satisfactory performance and only require to access the final model prediction. Gradient estimation is a critical step in black-box adversarial attacks, as it will directly affect the query efficiency. Recent works have attempted to utilize gradient priors to facilitate score-based methods to obtain better results. However, these gradient priors still suffer from the edge gradient discrepancy issue and the successive iteration gradient direction issue, thus are difficult to simply extend to decision-based methods. In this paper, we propose a novel Decision-based Black-box Attack framework with Gradient Priors (DBA-GP), which seamlessly integrates the data-dependent gradient prior and time-dependent prior into the gradient estimation procedure. First, by leveraging the joint bilateral filter to deal with each random perturbation, DBA-GP can guarantee that the generated perturbations in edge locations are hardly smoothed, i.e., alleviating the edge gradient discrepancy, thus remaining the characteristics of the original image as much as possible. Second, by utilizing a new gradient updating strategy to automatically adjust the successive iteration gradient direction, DBA-GP can accelerate the convergence speed, thus improving the query efficiency. Extensive experiments have demonstrated that the proposed method outperforms other strong baselines significantly.
</details>
<details>
<summary>摘要</summary>
决策基本方法在黑盒反击攻击中表现良好，因为它们只需访问最终模型预测。梯度估计是黑盒反击攻击中关键的步骤，因为它直接影响了查询效率。现有研究尝试使用梯度假设来促进分数基本方法获得更好的结果。然而，这些梯度假设仍然受到边梯度差异问题和连续迭代梯度方向问题的限制，因此难以简单地扩展到决策基本方法。在这篇论文中，我们提出了一种新的决策基本黑盒攻击框架（DBA-GP），该框架将数据依存梯度假设和时间依存假设细致地 интеグриinto梯度估计过程中。首先，通过利用 JOINT 双方filter来处理每个随机扰动，DBA-GP可以保证生成的扰动在边缘位置几乎不平滑，即消除边梯度差异，保持原始图像的特征。其次，通过利用新的梯度更新策略来自动调整 successive 迭代梯度方向，DBA-GP可以加速迭代速度，提高查询效率。经验表明，提议的方法与其他强式基准相比显著出众。
</details></li>
</ul>
<hr>
<h2 id="FPGAN-Control-A-Controllable-Fingerprint-Generator-for-Training-with-Synthetic-Data"><a href="#FPGAN-Control-A-Controllable-Fingerprint-Generator-for-Training-with-Synthetic-Data" class="headerlink" title="FPGAN-Control: A Controllable Fingerprint Generator for Training with Synthetic Data"></a>FPGAN-Control: A Controllable Fingerprint Generator for Training with Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19024">http://arxiv.org/abs/2310.19024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alon Shoshan, Nadav Bhonker, Emanuel Ben Baruch, Ori Nizan, Igor Kviatkovsky, Joshua Engelsma, Manoj Aggarwal, Gerard Medioni</li>
<li>for: 用于训练指纹识别模型，使用人工生成的数据，而不是具有敏感性的个人数据。</li>
<li>methods: 我们提出了FPGAN-Control，一种保持人工生成指纹图像的身份属性的权限控制框架。我们引入了一种新的出现损失，以促进指纹图像的分解性。</li>
<li>results: 我们在使用公开的NIST SD302（N2N）数据集进行训练FPGAN-Control模型时，得到了优秀的结果。我们quantitatively和qualitatively证明了FPGAN-Control的优势，包括保持身份属性的水平、控制指纹图像的出现特征和Synthetic-to-Real域阶跃小。最后，使用仅使用FPGAN-Control生成的人工数据进行训练指纹识别模型，可以达到与使用真实数据进行训练的相同或更高的识别率。<details>
<summary>Abstract</summary>
Training fingerprint recognition models using synthetic data has recently gained increased attention in the biometric community as it alleviates the dependency on sensitive personal data. Existing approaches for fingerprint generation are limited in their ability to generate diverse impressions of the same finger, a key property for providing effective data for training recognition models. To address this gap, we present FPGAN-Control, an identity preserving image generation framework which enables control over the fingerprint's image appearance (e.g., fingerprint type, acquisition device, pressure level) of generated fingerprints. We introduce a novel appearance loss that encourages disentanglement between the fingerprint's identity and appearance properties. In our experiments, we used the publicly available NIST SD302 (N2N) dataset for training the FPGAN-Control model. We demonstrate the merits of FPGAN-Control, both quantitatively and qualitatively, in terms of identity preservation level, degree of appearance control, and low synthetic-to-real domain gap. Finally, training recognition models using only synthetic datasets generated by FPGAN-Control lead to recognition accuracies that are on par or even surpass models trained using real data. To the best of our knowledge, this is the first work to demonstrate this.
</details>
<details>
<summary>摘要</summary>
<<SYS>>用生成的指纹数据训练指纹识别模型已经在生物认证社区中受到了加大的关注，因为它减轻了敏感个人数据的依赖。现有的指纹生成方法具有生成同一个手指多个印记的限制，这限制了生成的指纹数据的多样性。为了解决这个问题，我们提出了FPGAN-Control，一个保持身份的图像生成框架，允许控制生成的指纹图像的显示形式（例如，手指类型、获取设备、压力水平）。我们引入了一种新的外观损失，该损失促进了指纹的身份和外观属性的分离。我们使用公共可用的NIST SD302（N2N）数据集进行FPGAN-Control模型的训练。我们表明FPGAN-Control的优点，包括身份保持水平、外观控制度和实际领域与Synthetic领域之间的差异度。最后，使用FPGAN-Control生成的synthetic数据进行训练，可以达到与实际数据训练的识别率相同或者还高。这是我们知道的第一个研究。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Test-Time-Adaptation-for-Super-Resolution-with-Second-Order-Degradation-and-Reconstruction"><a href="#Efficient-Test-Time-Adaptation-for-Super-Resolution-with-Second-Order-Degradation-and-Reconstruction" class="headerlink" title="Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction"></a>Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19011">http://arxiv.org/abs/2310.19011</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dengzeshuai/srtta">https://github.com/dengzeshuai/srtta</a></li>
<li>paper_authors: Zeshuai Deng, Zhuokun Chen, Shuaicheng Niu, Thomas H. Li, Bohan Zhuang, Mingkui Tan<br>for:* 这个论文旨在提出一种快速适应测试环境的超分辨率图像重建方法，以便在不同&#x2F;未知降低类型的测试图像上实现高质量的SR图像重建。methods:* 该方法使用了次序降低方案来生成带有不同降低类型的对应数据，并通过特征级别重建学习来适应测试图像的降低类型。results:* 对于新 synthesized corrupted DIV2K数据集和一些实际世界数据集进行了广泛的实验，并显示了该方法可以具有惊人的提升，并且与现有方法相比具有满意的速度。<details>
<summary>Abstract</summary>
Image super-resolution (SR) aims to learn a mapping from low-resolution (LR) to high-resolution (HR) using paired HR-LR training images. Conventional SR methods typically gather the paired training data by synthesizing LR images from HR images using a predetermined degradation model, e.g., Bicubic down-sampling. However, the realistic degradation type of test images may mismatch with the training-time degradation type due to the dynamic changes of the real-world scenarios, resulting in inferior-quality SR images. To address this, existing methods attempt to estimate the degradation model and train an image-specific model, which, however, is quite time-consuming and impracticable to handle rapidly changing domain shifts. Moreover, these methods largely concentrate on the estimation of one degradation type (e.g., blur degradation), overlooking other degradation types like noise and JPEG in real-world test-time scenarios, thus limiting their practicality. To tackle these problems, we present an efficient test-time adaptation framework for SR, named SRTTA, which is able to quickly adapt SR models to test domains with different/unknown degradation types. Specifically, we design a second-order degradation scheme to construct paired data based on the degradation type of the test image, which is predicted by a pre-trained degradation classifier. Then, we adapt the SR model by implementing feature-level reconstruction learning from the initial test image to its second-order degraded counterparts, which helps the SR model generate plausible HR images. Extensive experiments are conducted on newly synthesized corrupted DIV2K datasets with 8 different degradations and several real-world datasets, demonstrating that our SRTTA framework achieves an impressive improvement over existing methods with satisfying speed. The source code is available at https://github.com/DengZeshuai/SRTTA.
</details>
<details>
<summary>摘要</summary>
图像超分辨率（SR）目标是通过LR和HR paired培成图像来学习LR到HR的映射。传统的SR方法通常使用预先确定的减样模型，如二维度下采样，来生成LR图像。然而，实际场景中的质量变化可能导致培成时的减样类型与测试时的减样类型不匹配，从而导致SR图像质量下降。为解决这问题，现有方法通常是通过估计减样模型并培成图像特定的模型来解决这个问题，但是这些方法需要较长的时间和不实际的培成过程。另外，这些方法主要集中于估计一种减样类型（例如，模糊减Randomized image degradation），忽略了实际场景中的其他减样类型（如噪声和JPEG），从而限制其实际应用。为此，我们提出了一种高效的测试时适应框架，名为SRTTA，可以快速适应测试图像的不同/未知减样类型。具体来说，我们设计了一种二阶减样方案，通过测试图像的减样类型来构建对应的paired数据，这些数据被预训练的减样类别预测器预测。然后，我们适应SR模型，通过实现初始测试图像的特征级重建学习，从而帮助SR模型生成可靠的HR图像。我们在新 synthesized corrupted DIV2K数据集上进行了广泛的实验，并得到了非常出色的提高，证明了我们的SRTTA框架的可靠性和速度。SR模型的源代码可以在https://github.com/DengZeshuai/SRTTA上获取。
</details></li>
</ul>
<hr>
<h2 id="DynPoint-Dynamic-Neural-Point-For-View-Synthesis"><a href="#DynPoint-Dynamic-Neural-Point-For-View-Synthesis" class="headerlink" title="DynPoint: Dynamic Neural Point For View Synthesis"></a>DynPoint: Dynamic Neural Point For View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18999">http://arxiv.org/abs/2310.18999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaichen Zhou, Jia-Xing Zhong, Sangyun Shin, Kai Lu, Yiyuan Yang, Andrew Markham, Niki Trigoni</li>
<li>for: 实现短时间内预测和合成复杂的单一影像视频中的视角</li>
<li>methods: 使用深度探索和Scene Flow估计来预测邻帧之间的3D对应关系，并将多个参考帧资讯聚合到目标帧上</li>
<li>results: 提高训练时间的减少和与传统方法相比的比较类似的效果，以及强大的长期视频处理能力<details>
<summary>Abstract</summary>
The introduction of neural radiance fields has greatly improved the effectiveness of view synthesis for monocular videos. However, existing algorithms face difficulties when dealing with uncontrolled or lengthy scenarios, and require extensive training time specific to each new scenario. To tackle these limitations, we propose DynPoint, an algorithm designed to facilitate the rapid synthesis of novel views for unconstrained monocular videos. Rather than encoding the entirety of the scenario information into a latent representation, DynPoint concentrates on predicting the explicit 3D correspondence between neighboring frames to realize information aggregation. Specifically, this correspondence prediction is achieved through the estimation of consistent depth and scene flow information across frames. Subsequently, the acquired correspondence is utilized to aggregate information from multiple reference frames to a target frame, by constructing hierarchical neural point clouds. The resulting framework enables swift and accurate view synthesis for desired views of target frames. The experimental results obtained demonstrate the considerable acceleration of training time achieved - typically an order of magnitude - by our proposed method while yielding comparable outcomes compared to prior approaches. Furthermore, our method exhibits strong robustness in handling long-duration videos without learning a canonical representation of video content.
</details>
<details>
<summary>摘要</summary>
“对于单一影像 видео的视角合成，射频场景导入对效果有很大改善。然而，现有的算法在面对无法控制或长度很长的场景时会遇到困难，并且需要对每个新场景进行专门的训练时间。为了解决这些限制，我们提出了DynPoint算法，用于快速合成单一影像 vide的目标帧的视角。而不是将整个场景信息转换为潜在表示，DynPoint专注于预测内部帧之间的明确三维对应关系，以便实现信息聚合。具体来说，这个对应预测是通过对内部帧之间的深度和场景流动信息进行估计。接着，所得到的对应信息被用来将多个参考帧聚合到目标帧上，通过建立对应的神经点云。这个框架可以实现快速和精准地合成目标帧的视角。实验结果显示，我们的提出方法可以大大减少训练时间，通常是一个次的频率，而且与先前的方法相比，其效果相似。此外，我们的方法还表现出强大的韧性，可以处理长度很长的影像 videowithout学习对影像内容的对应表示。”
</details></li>
</ul>
<hr>
<h2 id="Controllable-Group-Choreography-using-Contrastive-Diffusion"><a href="#Controllable-Group-Choreography-using-Contrastive-Diffusion" class="headerlink" title="Controllable Group Choreography using Contrastive Diffusion"></a>Controllable Group Choreography using Contrastive Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18986">http://arxiv.org/abs/2310.18986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aioz-ai/GCD">https://github.com/aioz-ai/GCD</a></li>
<li>paper_authors: Nhat Le, Tuong Do, Khoa Do, Hien Nguyen, Erman Tjiputra, Quang D. Tran, Anh Nguyen</li>
<li>for: 用于生成高质量、可定制的群体舞蹈动画</li>
<li>methods: 使用扩散基 générativeapproach Synthesize flexible number of dancers and long-term group dances, while ensuring coherence to the input music</li>
<li>results: 实现了可观赏的、一致的群体舞蹈动画，可控制consistency或多样性水平<details>
<summary>Abstract</summary>
Music-driven group choreography poses a considerable challenge but holds significant potential for a wide range of industrial applications. The ability to generate synchronized and visually appealing group dance motions that are aligned with music opens up opportunities in many fields such as entertainment, advertising, and virtual performances. However, most of the recent works are not able to generate high-fidelity long-term motions, or fail to enable controllable experience. In this work, we aim to address the demand for high-quality and customizable group dance generation by effectively governing the consistency and diversity of group choreographies. In particular, we utilize a diffusion-based generative approach to enable the synthesis of flexible number of dancers and long-term group dances, while ensuring coherence to the input music. Ultimately, we introduce a Group Contrastive Diffusion (GCD) strategy to enhance the connection between dancers and their group, presenting the ability to control the consistency or diversity level of the synthesized group animation via the classifier-guidance sampling technique. Through intensive experiments and evaluation, we demonstrate the effectiveness of our approach in producing visually captivating and consistent group dance motions. The experimental results show the capability of our method to achieve the desired levels of consistency and diversity, while maintaining the overall quality of the generated group choreography.
</details>
<details>
<summary>摘要</summary>
音乐驱动的群体编舞存在较大的挑战，但具有广泛的应用前景。可以生成协调和视觉吸引人的群体编舞动作，与音乐相对应，可以应用于娱乐、广告和虚拟表演等领域。然而，大多数最近的研究无法生成高品质长期编舞动作，或者失去控制体验。在这个工作中，我们希望通过有效地控制群体编舞的一致性和多样性来解决这个问题。特别是，我们利用扩散基本的生成方法，使得可以生成多个舞者和长期编舞动作，同时保证音乐的一致性。最后，我们引入了群体对比扩散策略（GCD），以提高舞者与群体之间的连接，并通过类型指导抽象技术来控制生成的群体动画的一致性或多样性水平。通过广泛的实验和评估，我们证明了我们的方法的可行性和效果，能够生成视觉吸引人的、一致的群体编舞动作。实验结果表明，我们的方法可以达到所需的一致性和多样性水平，同时保持生成的群体编舞动作的整体质量。
</details></li>
</ul>
<hr>
<h2 id="Blacksmith-Fast-Adversarial-Training-of-Vision-Transformers-via-a-Mixture-of-Single-step-and-Multi-step-Methods"><a href="#Blacksmith-Fast-Adversarial-Training-of-Vision-Transformers-via-a-Mixture-of-Single-step-and-Multi-step-Methods" class="headerlink" title="Blacksmith: Fast Adversarial Training of Vision Transformers via a Mixture of Single-step and Multi-step Methods"></a>Blacksmith: Fast Adversarial Training of Vision Transformers via a Mixture of Single-step and Multi-step Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18975">http://arxiv.org/abs/2310.18975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahdi Salmani, Alireza Dehghanpour Farashah, Mohammad Azizmalayeri, Mahdi Amiri, Navid Eslami, Mohammad Taghi Manzuri, Mohammad Hossein Rohban</li>
<li>for: 防止深度学习模型受到攻击时的灾难性欠拟合 (Catastrophic Overfitting) 问题</li>
<li>methods: 提议使用随机选择PGD-2和FGSM两种攻击方法在小批量训练中，以增加攻击多样性，避免CO问题</li>
<li>results: 比较其他方法，包括N-FGSM，实现更好的防止CO和实现PGD-2级别的性能<details>
<summary>Abstract</summary>
Despite the remarkable success achieved by deep learning algorithms in various domains, such as computer vision, they remain vulnerable to adversarial perturbations. Adversarial Training (AT) stands out as one of the most effective solutions to address this issue; however, single-step AT can lead to Catastrophic Overfitting (CO). This scenario occurs when the adversarially trained network suddenly loses robustness against multi-step attacks like Projected Gradient Descent (PGD). Although several approaches have been proposed to address this problem in Convolutional Neural Networks (CNNs), we found out that they do not perform well when applied to Vision Transformers (ViTs). In this paper, we propose Blacksmith, a novel training strategy to overcome the CO problem, specifically in ViTs. Our approach utilizes either of PGD-2 or Fast Gradient Sign Method (FGSM) randomly in a mini-batch during the adversarial training of the neural network. This will increase the diversity of our training attacks, which could potentially mitigate the CO issue. To manage the increased training time resulting from this combination, we craft the PGD-2 attack based on only the first half of the layers, while FGSM is applied end-to-end. Through our experiments, we demonstrate that our novel method effectively prevents CO, achieves PGD-2 level performance, and outperforms other existing techniques including N-FGSM, which is the state-of-the-art method in fast training for CNNs.
</details>
<details>
<summary>摘要</summary>
尽管深度学习算法在不同领域取得了惊人的成功，但它们仍然面临到抗击干扰的漏洞。对于这个问题，对抗训练（AT）是一种非常有效的解决方案，但是单步AT可能会导致极端过拟合（CO）。这种情况发生在对多步攻击，如投影 gradient descent（PGD）进行了适应训练后，神经网络 suddenly lost its robustness。虽然一些方法已经被提出来解决这个问题在卷积神经网络（CNNs）中，但是这些方法在视图转换器（ViTs）中并不perform well。在这篇论文中，我们提出了黑锤子，一种新的训练策略，可以在ViTs中解决CO问题。我们的方法在 adversarial training 中随机使用 PGD-2 或 Fast Gradient Sign Method（FGSM），以增加训练攻击的多样性，从而可能解决CO问题。为了控制因此的增加训练时间，我们在PGD-2攻击基于神经网络的前半部分，而FGSM在整个神经网络中进行。通过我们的实验，我们证明了黑锤子有效地避免了CO问题，实现了PGD-2水平的性能，并超过了其他现有的方法，包括 N-FGSM，它是对于快速训练的CNNs最佳方法。
</details></li>
</ul>
<hr>
<h2 id="AnomalyCLIP-Object-agnostic-Prompt-Learning-for-Zero-shot-Anomaly-Detection"><a href="#AnomalyCLIP-Object-agnostic-Prompt-Learning-for-Zero-shot-Anomaly-Detection" class="headerlink" title="AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection"></a>AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18961">http://arxiv.org/abs/2310.18961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zqhang/anomalyclip">https://github.com/zqhang/anomalyclip</a></li>
<li>paper_authors: Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen</li>
<li>for: 这个论文的目的是为了提出一个新的零分数异常探测（ZSAD）方法，以便在没有目标数据的情况下，精确地探测图像中的异常。</li>
<li>methods: 这个方法使用了大量的预训数据，并且将其与CLIP模型结合，以便学习一些通用的异常特征。另外，这个方法还使用了一些特定的文本描述来帮助模型更好地理解图像中的异常。</li>
<li>results: 在17个真实世界的异常探测数据集上，这个方法获得了Superior的零分数性能，可以实现在不同类型的物品上进行异常探测和分类。<details>
<summary>Abstract</summary>
Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, \eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate text into Simplified Chinese<</SYS>>Zero-shot异常检测（ZSAD）需要使用辅助数据训练的检测模型，以检测异常点 без任何目标数据。这是一个重要的任务，因为训练数据可能无法存取，例如因为数据隐私问题。然而，这是一个具有挑战的任务，因为模型需要对不同领域中的异常点进行概念扩展。最近，大型预训条件语音视觉模型（VLM），例如CLIP，已经展示了在不同视觉任务中的强大零shot识别能力。然而，它们的ZSAD性能较弱，因为VLM专注于模型背景物件的类别 semantics，而不是图像中的异常/正常领域。在这篇论文中，我们介绍了一个新的方法，即AnomalyCLIP，以适应CLIP для精确的ZSAD过程。关键思想是学习对应于图像中任何物件的通用正常和异常文本描述，从而让我们的模型专注于图像中的异常领域，而不是物件 semantics。这使我们的模型能够实现多元类型物件的通用正常和异常识别。大规模的实验显示，AnomalyCLIP在17个真实世界异常检测数据集上表现出色，可以实现零shot检测和分类异常点。代码将会在https://github.com/zqhang/AnomalyCLIP中公开。
</details></li>
</ul>
<hr>
<h2 id="TIC-TAC-A-Framework-To-Learn-And-Evaluate-Your-Covariance"><a href="#TIC-TAC-A-Framework-To-Learn-And-Evaluate-Your-Covariance" class="headerlink" title="TIC-TAC: A Framework To Learn And Evaluate Your Covariance"></a>TIC-TAC: A Framework To Learn And Evaluate Your Covariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18953">http://arxiv.org/abs/2310.18953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vita-epfl/TIC-TAC">https://github.com/vita-epfl/TIC-TAC</a></li>
<li>paper_authors: Megh Shukla, Mathieu Salzmann, Alexandre Alahi</li>
<li>for: 本研究实际问题是无监督 hetroscedastic 幂复数估计，目的是从观察 $x$ 学习多元目标分布 $\mathcal{N}(y, \Sigma_y | x)$。</li>
<li>methods: 通常是使用两个神经网络，通过负数alog-likelihood 训练，预测目标分布的均值 $f_{\theta}(x)$ 和幂复数 $\text{Cov}(f_{\theta}(x))$。</li>
<li>results: 我们解决了这两个问题：首先，我们提出了 TIC：泰勒引入幂复数，它通过在 $x$ 附近的第二阶 Taylor 多项式，捕捉多元 $f_{\theta}(x)$ 的随机性。其次，我们引入了 TAC：任务无关相关，这是一个基于条件的正常分布来评估幂复数。我们的实验显示，TIC 可以更好地学习幂复数，并且通过 TAC 评估其性能。<details>
<summary>Abstract</summary>
We study the problem of unsupervised heteroscedastic covariance estimation, where the goal is to learn the multivariate target distribution $\mathcal{N}(y, \Sigma_y | x )$ given an observation $x$. This problem is particularly challenging as $\Sigma_{y}$ varies for different samples (heteroscedastic) and no annotation for the covariance is available (unsupervised). Typically, state-of-the-art methods predict the mean $f_{\theta}(x)$ and covariance $\textrm{Cov}(f_{\theta}(x))$ of the target distribution through two neural networks trained using the negative log-likelihood. This raises two questions: (1) Does the predicted covariance truly capture the randomness of the predicted mean? (2) In the absence of ground-truth annotation, how can we quantify the performance of covariance estimation? We address (1) by deriving TIC: Taylor Induced Covariance, which captures the randomness of the multivariate $f_{\theta}(x)$ by incorporating its gradient and curvature around $x$ through the second order Taylor polynomial. Furthermore, we tackle (2) by introducing TAC: Task Agnostic Correlations, a metric which leverages conditioning of the normal distribution to evaluate the covariance. We verify the effectiveness of TIC through multiple experiments spanning synthetic (univariate, multivariate) and real-world datasets (UCI Regression, LSP, and MPII Human Pose Estimation). Our experiments show that TIC outperforms state-of-the-art in accurately learning the covariance, as quantified through TAC.
</details>
<details>
<summary>摘要</summary>
我们研究无监督不均匀 covariance 估计问题，目标是学习 multivariate 目标分布 $\mathcal{N}(y, \Sigma_y | x)$  Given an observation $x$. 这个问题特别困难，因为 $\Sigma_y$ 对不同样本而变化 (heteroscedastic) 并且没有对 covariance 的注释 (unsupervised)。通常，当前的方法预测目标分布的均值 $f_{\theta}(x)$ 和 covariance $\text{Cov}(f_{\theta}(x))$ 通过两个神经网络，通过负LOG-likelihood 训练。这引出了两个问题：1. 预测的 covariance 是否真正捕捉了预测的均值的Randomness？2. 在缺乏真实注释的情况下，如何评价 covariance 估计的性能？我们解决了第一个问题，通过 derivation TIC：Taylor Induced Covariance，它利用 $x$ 的第二阶 Taylor 级数来捕捉 multivariate $f_{\theta}(x)$ 的Randomness。此外，我们解决了第二个问题，通过引入 TAC：Task Agnostic Correlations，它利用 conditioning 来评价 covariance。我们通过多个实验证明 TIC 的效果，其中包括 synthetic 数据 (univariate, multivariate) 和实际世界数据 (UCI Regression, LSP, MPII Human Pose Estimation)。我们的实验表明，TIC 可以更好地学习 covariance，并且通过 TAC 评价其性能。
</details></li>
</ul>
<hr>
<h2 id="Customize-StyleGAN-with-One-Hand-Sketch"><a href="#Customize-StyleGAN-with-One-Hand-Sketch" class="headerlink" title="Customize StyleGAN with One Hand Sketch"></a>Customize StyleGAN with One Hand Sketch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18949">http://arxiv.org/abs/2310.18949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaocong Zhang</li>
<li>for: 用于控制 StyleGAN 图像生成的单个用户绘图</li>
<li>methods: 基于 CLIP 的能量学习方法，包括两种新的能量函数，用于在 StyleGAN 的含义空间中学习 conditional 分布</li>
<li>results: 可以使用单个用户绘图控制 StyleGAN 图像生成，并且在一阶段 regime 中显著超越先前方法，同时在不同风格和姿势的人工绘图上也表现出优异性。<details>
<summary>Abstract</summary>
Generating images from human sketches typically requires dedicated networks trained from scratch. In contrast, the emergence of the pre-trained Vision-Language models (e.g., CLIP) has propelled generative applications based on controlling the output imagery of existing StyleGAN models with text inputs or reference images. Parallelly, our work proposes a framework to control StyleGAN imagery with a single user sketch. In particular, we learn a conditional distribution in the latent space of a pre-trained StyleGAN model via energy-based learning and propose two novel energy functions leveraging CLIP for cross-domain semantic supervision. Once trained, our model can generate multi-modal images semantically aligned with the input sketch. Quantitative evaluations on synthesized datasets have shown that our approach improves significantly from previous methods in the one-shot regime. The superiority of our method is further underscored when experimenting with a wide range of human sketches of diverse styles and poses. Surprisingly, our models outperform the previous baseline regarding both the range of sketch inputs and image qualities despite operating with a stricter setting: with no extra training data and single sketch input.
</details>
<details>
<summary>摘要</summary>
通常需要专门的网络来生成图像从人工绘制。然而，clip的出现提高了基于文本输入或参考图像控制现有的StyleGAN模型的生成应用。我们的工作则是一个框架，可以使用单个用户绘制来控制StyleGAN图像。具体来说，我们通过能量学习学习 StyleGAN模型的latent空间中的conditional分布，并提出了两种新的能量函数，利用clip进行跨领域semantic监督。一旦训练完成，我们的模型可以生成与输入绘制semantic相关的多模态图像。对于synthesized dataset的量化评价表明，我们的方法在一键 режиме中表现出了明显的提升。此外，我们的方法还在使用不同风格和姿势的人工绘制中表现出色，并且在没有额外训练数据和单个绘制输入的情况下，我们的模型仍然能够超越前一个基准值。
</details></li>
</ul>
<hr>
<h2 id="Video-Frame-Interpolation-with-Many-to-many-Splatting-and-Spatial-Selective-Refinement"><a href="#Video-Frame-Interpolation-with-Many-to-many-Splatting-and-Spatial-Selective-Refinement" class="headerlink" title="Video Frame Interpolation with Many-to-many Splatting and Spatial Selective Refinement"></a>Video Frame Interpolation with Many-to-many Splatting and Spatial Selective Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18946">http://arxiv.org/abs/2310.18946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Hu, Simon Niklaus, Lu Zhang, Stan Sclaroff, Kate Saenko</li>
<li>for: 这篇论文旨在提出一种可微分的多个目标（Many-to-Many，M2M）拼接框架，以高效地 interpolate 帧。</li>
<li>methods: 该方法使用多个 bidirectional 流来直接将像素截割到想要的时间步，并将每个源像素映射到多个目标像素，以实现多对多拼接方案。</li>
<li>results: 该方法可以在 interpolating 任意数量的中间帧时，对每个输入帧对对有较少的计算开销，因此实现了高速多帧 interpolating。但是，直接在Intensity Domain中截割和融合像素可能会受到运动估计质量的影响，并且可能会受到较差的表示能力。为了提高 interpolating 精度，该方法还提出了一种可调 SSR 组件，可以根据计算效率和 interpolating 质量进行调整。<details>
<summary>Abstract</summary>
In this work, we first propose a fully differentiable Many-to-Many (M2M) splatting framework to interpolate frames efficiently. Given a frame pair, we estimate multiple bidirectional flows to directly forward warp the pixels to the desired time step before fusing overlapping pixels. In doing so, each source pixel renders multiple target pixels and each target pixel can be synthesized from a larger area of visual context, establishing a many-to-many splatting scheme with robustness to undesirable artifacts. For each input frame pair, M2M has a minuscule computational overhead when interpolating an arbitrary number of in-between frames, hence achieving fast multi-frame interpolation. However, directly warping and fusing pixels in the intensity domain is sensitive to the quality of motion estimation and may suffer from less effective representation capacity. To improve interpolation accuracy, we further extend an M2M++ framework by introducing a flexible Spatial Selective Refinement (SSR) component, which allows for trading computational efficiency for interpolation quality and vice versa. Instead of refining the entire interpolated frame, SSR only processes difficult regions selected under the guidance of an estimated error map, thereby avoiding redundant computation. Evaluation on multiple benchmark datasets shows that our method is able to improve the efficiency while maintaining competitive video interpolation quality, and it can be adjusted to use more or less compute as needed.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们首先提出了一个完全可导Many-to-Many（M2M）拼接框架，以高效地 interpolate帧。给定一个帧对，我们估算多个双向流来直接forward扭曲像素到所需的时间步，以便在拼接过程中 fusion overlapping pixels。在这样的情况下，每个源像素可以渲染多个目标像素，并且每个目标像素可以从更大的视觉上下文中 Synthesize，建立了一个多个源像素到多个目标像素的拼接方案，从而具有较好的鲁棒性。对于每个输入帧对，M2M在 interpolating 任意数量的中间帧时，只需要投入微scopic的计算负担，因此实现了高速多帧 interpolating。然而，直接在Intensity Domain中扭曲和合并像素是对动作估计质量的敏感，可能会受到较差的表示能力的影响。为了提高 interpolating 精度，我们进一步扩展了 M2M++ 框架，通过引入 flexible Spatial Selective Refinement（SSR）组件，以便在需要更高的 interpolating 精度时，通过选择难度较高的区域进行精细化，从而避免需要 redundant computation。我们对多个标准数据集进行评估，发现我们的方法可以提高效率，同时保持竞争力强的视频 interpolating 质量，并且可以根据需要调整使用更多或更少的计算资源。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Examples-Are-Not-Real-Features"><a href="#Adversarial-Examples-Are-Not-Real-Features" class="headerlink" title="Adversarial Examples Are Not Real Features"></a>Adversarial Examples Are Not Real Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18936">http://arxiv.org/abs/2310.18936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pku-ml/advnotrealfeatures">https://github.com/pku-ml/advnotrealfeatures</a></li>
<li>paper_authors: Ang Li, Yifei Wang, Yiwen Guo, Yisen Wang</li>
<li>for: 本研究探讨了 adversarial example 的形成原因，以及非Robust 特征是否真的有用。</li>
<li>methods: 研究者使用了多种学习模式，包括 supervised learning、contrastive learning、masked image modeling 和 diffusion models，以检验 non-Robust 特征的用用性。</li>
<li>results: 研究结果表明，non-Robust 特征在不同的学习模式下 Transfer 性差，而 Robust 特征具有更好的 Transfer 性。此外，研究者还发现，自然地训练的 encoder 在 AutoAttack 中不具有 robustness。结论是，non-Robust 特征并不是真正有用，而是学习模式偏好的快捷途径。<details>
<summary>Abstract</summary>
The existence of adversarial examples has been a mystery for years and attracted much interest. A well-known theory by \citet{ilyas2019adversarial} explains adversarial vulnerability from a data perspective by showing that one can extract non-robust features from adversarial examples and these features alone are useful for classification. However, the explanation remains quite counter-intuitive since non-robust features are mostly noise features to humans. In this paper, we re-examine the theory from a larger context by incorporating multiple learning paradigms. Notably, we find that contrary to their good usefulness under supervised learning, non-robust features attain poor usefulness when transferred to other self-supervised learning paradigms, such as contrastive learning, masked image modeling, and diffusion models. It reveals that non-robust features are not really as useful as robust or natural features that enjoy good transferability between these paradigms. Meanwhile, for robustness, we also show that naturally trained encoders from robust features are largely non-robust under AutoAttack. Our cross-paradigm examination suggests that the non-robust features are not really useful but more like paradigm-wise shortcuts, and robust features alone might be insufficient to attain reliable model robustness. Code is available at \url{https://github.com/PKU-ML/AdvNotRealFeatures}.
</details>
<details>
<summary>摘要</summary>
exist adversarial examples 年来都是一个谜，吸引了很多关注。一种常见的理论是由\citet{ilyas2019adversarial}提出的，它解释了对 adversarial examples 的敏感性从数据角度，表明可以从 adversarial examples 中提取不稳定特征，并且这些特征可以帮助进行分类。然而，这种解释仍然很Counter-intuitive，因为这些不稳定特征对人类来说都是噪音。在这篇论文中，我们重新审视这种理论，通过将多种学习概念相结合。结果发现，相比于supervised learning中的好用性，non-robust features在其他自适应学习概念中，如对比学习、干扰学习和扩散模型， exhibit poor usefulness。这显示出non-robust features并不是如人们所想的那么有用，而是在某些学习概念下的偏好短cut。此外，我们还发现，自然地训练的encoder从robust特征中获得的模型不 robust under AutoAttack。我们的 across-paradigm 审视表明，non-robust features并不是真正有用的，而更像是学习概念的偏好短cut。代码可以在 \url{https://github.com/PKU-ML/AdvNotRealFeatures} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Label-Poisoning-is-All-You-Need"><a href="#Label-Poisoning-is-All-You-Need" class="headerlink" title="Label Poisoning is All You Need"></a>Label Poisoning is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18933">http://arxiv.org/abs/2310.18933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MarkipTheMudkip/in-class-project-2">https://github.com/MarkipTheMudkip/in-class-project-2</a></li>
<li>paper_authors: Rishi D. Jha, Jonathan Hayase, Sewoong Oh</li>
<li>For: The paper investigates the possibility of launching a successful backdoor attack by only corrupting the training labels, rather than the images themselves.* Methods: The paper introduces a novel approach called FLIP, which uses trajectory matching to design label-only backdoor attacks.* Results: The paper demonstrates the effectiveness of FLIP on three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer), achieving a near-perfect attack success rate of 99.4% with only a 1.8% drop in the clean test accuracy.Here are the three points in Simplified Chinese text:</li>
<li>for: 论文 investigate 是否可以通过只 corrupting 训练标签来发动成功的后门攻击。</li>
<li>methods: 论文提出了一种新的方法 called FLIP，使用 trajectory matching 设计 label-only 后门攻击。</li>
<li>results: 论文在三个 dataset (CIFAR-10, CIFAR-100, Tiny-ImageNet) 和四种架构 (ResNet-32, ResNet-18, VGG-19, Vision Transformer) 上进行了实验，成功率为 99.4%，但clean test accuracy 下降了1.8%。<details>
<summary>Abstract</summary>
In a backdoor attack, an adversary injects corrupted data into a model's training dataset in order to gain control over its predictions on images with a specific attacker-defined trigger. A typical corrupted training example requires altering both the image, by applying the trigger, and the label. Models trained on clean images, therefore, were considered safe from backdoor attacks. However, in some common machine learning scenarios, the training labels are provided by potentially malicious third-parties. This includes crowd-sourced annotation and knowledge distillation. We, hence, investigate a fundamental question: can we launch a successful backdoor attack by only corrupting labels? We introduce a novel approach to design label-only backdoor attacks, which we call FLIP, and demonstrate its strengths on three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). With only 2% of CIFAR-10 labels corrupted, FLIP achieves a near-perfect attack success rate of 99.4% while suffering only a 1.8% drop in the clean test accuracy. Our approach builds upon the recent advances in trajectory matching, originally introduced for dataset distillation.
</details>
<details>
<summary>摘要</summary>
在一种后门攻击中，敌对者将损坏数据插入模型的训练集中，以获得对特定触发符的图像预测的控制权。通常需要在图像上应用触发符，并修改标签。由于clean图像上训练的模型被认为是安全的，因此这种攻击被称为后门攻击。然而，在一些常见的机器学习场景中，训练标签由可能有恶意的第三方提供，包括人工标注和知识储存。我们因此研究了一个基本问题：可以通过只修改标签来发动成功的后门攻击吗？我们提出了一种新的标签修改攻击方法，称之为FLIP，并在CIFAR-10、CIFAR-100和Tiny-ImageNet三个数据集和四种架构（ResNet-32、ResNet-18、VGG-19和Vision Transformer）上进行了实验。只有2%的CIFAR-10标签被损坏，FLIP可以达到99.4%的攻击成功率，同时只有1.8%的干净测试准确率下降。我们的方法基于最近的曲线匹配原理，原本用于数据储存。
</details></li>
</ul>
<hr>
<h2 id="A-transfer-learning-approach-with-convolutional-neural-network-for-Face-Mask-Detection"><a href="#A-transfer-learning-approach-with-convolutional-neural-network-for-Face-Mask-Detection" class="headerlink" title="A transfer learning approach with convolutional neural network for Face Mask Detection"></a>A transfer learning approach with convolutional neural network for Face Mask Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18928">http://arxiv.org/abs/2310.18928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abolfazl Younesi, Reza Afrouzian, Yousef Seyfari<br>for: 本研究旨在提出一个基于传播学习和Inception v3架构的面具识别系统，以检测拥有人群中的面具使用情况。methods: 本研究使用了两个同时训练 dataset，包括Simulated Mask Face Dataset (SMFD) 和 MaskedFace-Net (MFN)，并通过优化协eles hyper-parameters和精确设计全接触层，以提高系统的准确性和效率。results: 实验结果显示，提案的方法具有高准确性和效率，在训练和测试数据中分别 achievement 99.47% 和 99.33%。<details>
<summary>Abstract</summary>
Due to the epidemic of the coronavirus (Covid-19) and its rapid spread around the world, the world has faced an enormous crisis. To prevent the spread of the coronavirus, the World Health Organization (WHO) has introduced the use of masks and keeping social distance as the best preventive method. So, developing an automatic monitoring system for detecting facemasks in some crowded places is essential. To do this, we propose a mask recognition system based on transfer learning and Inception v3 architecture. In the proposed method, two datasets are used simultaneously for training including the Simulated Mask Face Dataset (SMFD) and MaskedFace-Net (MFN) This paper tries to increase the accuracy of the proposed system by optimally setting hyper-parameters and accurately designing the fully connected layers. The main advantage of the proposed method is that in addition to masked and unmasked faces, it can also detect cases of incorrect use of mask. Therefore, the proposed method classifies the input face images into three categories. Experimental results show the high accuracy and efficiency of the proposed method; so, this method has achieved an accuracy of 99.47% and 99.33% in training and test data respectively
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose a mask recognition system based on transfer learning and the Inception v3 architecture. Our approach utilizes two datasets simultaneously for training: the Simulated Mask Face Dataset (SMFD) and MaskedFace-Net (MFN). The primary goal of this paper is to enhance the accuracy of the proposed system by optimally setting hyperparameters and designing the fully connected layers.The key advantage of our method is that it can detect not only masked and unmasked faces but also incorrect use of masks. Therefore, the proposed method classifies input face images into three categories. Experimental results demonstrate the high accuracy and efficiency of the proposed method, with an accuracy of 99.47% and 99.33% in training and test data, respectively.
</details></li>
</ul>
<hr>
<h2 id="Improving-Multi-Person-Pose-Tracking-with-A-Confidence-Network"><a href="#Improving-Multi-Person-Pose-Tracking-with-A-Confidence-Network" class="headerlink" title="Improving Multi-Person Pose Tracking with A Confidence Network"></a>Improving Multi-Person Pose Tracking with A Confidence Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18920">http://arxiv.org/abs/2310.18920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehua Fu, Wenhang Zuo, Zhenghui Hu, Qingjie Liu, Yunhong Wang</li>
<li>for: 本研究旨在提高顶向方法中的人体检测和pose estimation的精度，以解决 occlusion 和 missed detection 问题。</li>
<li>methods: 本文提出了一种新的关键点信任网络和跟踪管道，以提高顶向方法中的人体检测和pose estimation。关键点信任网络用于确定每个关键点是否受到 occlusion，而跟踪管道包括bbox-revision模块和ID-retrieve模块，以减少丢失检测和修复lost trajectory。</li>
<li>results: 实验结果显示，我们的方法在人体检测和pose estimation方面具有 universality，在 PoseTrack 2017 和2018 数据集上达到了状态对精度。<details>
<summary>Abstract</summary>
Human pose estimation and tracking are fundamental tasks for understanding human behaviors in videos. Existing top-down framework-based methods usually perform three-stage tasks: human detection, pose estimation and tracking. Although promising results have been achieved, these methods rely heavily on high-performance detectors and may fail to track persons who are occluded or miss-detected. To overcome these problems, in this paper, we develop a novel keypoint confidence network and a tracking pipeline to improve human detection and pose estimation in top-down approaches. Specifically, the keypoint confidence network is designed to determine whether each keypoint is occluded, and it is incorporated into the pose estimation module. In the tracking pipeline, we propose the Bbox-revision module to reduce missing detection and the ID-retrieve module to correct lost trajectories, improving the performance of the detection stage. Experimental results show that our approach is universal in human detection and pose estimation, achieving state-of-the-art performance on both PoseTrack 2017 and 2018 datasets.
</details>
<details>
<summary>摘要</summary>
人体姿势估计和跟踪是视频理解人类行为的基本任务。现有的顶部框架基础方法通常执行三个阶段任务：人员检测、姿势估计和跟踪。虽然已经获得了出色的结果，但这些方法受到高性能探测器的依赖，可能会在 occluded 或者检测错误时失败。为了解决这些问题，在这篇论文中，我们开发了一种新的关键点信任网络和跟踪管道，以提高顶部方法中的人员检测和姿势估计。具体来说，关键点信任网络是用于判断每个关键点是否受到遮挡的，并将其 incorporated 到姿势估计模块中。在跟踪管道中，我们提出了 Bbox-revision 模块，以减少缺失检测，以及 ID-retrieve 模块，以更正丢失的轨迹，从而提高检测阶段的性能。实验结果表明，我们的方法在人员检测和姿势估计中具有通用性和state-of-the-art 性能，在 PoseTrack 2017 和 2018 数据集上达到了最佳性能。
</details></li>
</ul>
<hr>
<h2 id="TiV-NeRF-Tracking-and-Mapping-via-Time-Varying-Representation-with-Dynamic-Neural-Radiance-Fields"><a href="#TiV-NeRF-Tracking-and-Mapping-via-Time-Varying-Representation-with-Dynamic-Neural-Radiance-Fields" class="headerlink" title="TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields"></a>TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18917">http://arxiv.org/abs/2310.18917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengyao Duan, Zhiliu Yang</li>
<li>for: track and reconstruct dynamic scenes in SLAM framework</li>
<li>methods: time-varying representation, self-supervised training, distinct sampling strategies, and keyframe selection strategy</li>
<li>results: more effective compared to current state-of-the-art dynamic mapping methodsHere’s the full summary in Simplified Chinese:</li>
<li>for: 这 paper 旨在将 Neural Radiance Fields (NeRF)  интегрирован到 Simultaneous Localization and Mapping (SLAM) 框架中，以处理动态场景。</li>
<li>methods: 该 paper 提出了时间变化表示，自动Supervised 训练，不同区域采样策略，以及关键帧选择策略。</li>
<li>results: 比现有的动态映射方法更有效。I hope that helps!<details>
<summary>Abstract</summary>
Previous attempts to integrate Neural Radiance Fields (NeRF) into Simultaneous Localization and Mapping (SLAM) framework either rely on the assumption of static scenes or treat dynamic objects as outliers. However, most of real-world scenarios is dynamic. In this paper, we propose a time-varying representation to track and reconstruct the dynamic scenes. Our system simultaneously maintains two processes, tracking process and mapping process. For tracking process, the entire input images are uniformly sampled and training of the RGB images are self-supervised. For mapping process, we leverage know masks to differentiate dynamic objects and static backgrounds, and we apply distinct sampling strategies for two types of areas. The parameters optimization for both processes are made up by two stages, the first stage associates time with 3D positions to convert the deformation field to the canonical field. And the second associates time with 3D positions in canonical field to obtain colors and Signed Distance Function (SDF). Besides, We propose a novel keyframe selection strategy based on the overlapping rate. We evaluate our approach on two publicly available synthetic datasets and validate that our method is more effective compared to current state-of-the-art dynamic mapping methods.
</details>
<details>
<summary>摘要</summary>
previous attempts to integrate Neural Radiance Fields (NeRF) into Simultaneous Localization and Mapping (SLAM) framework either rely on the assumption of static scenes or treat dynamic objects as outliers. However, most of real-world scenarios is dynamic. In this paper, we propose a time-varying representation to track and reconstruct the dynamic scenes. Our system simultaneously maintains two processes, tracking process and mapping process. For tracking process, the entire input images are uniformly sampled and training of the RGB images are self-supervised. For mapping process, we leverage know masks to differentiate dynamic objects and static backgrounds, and we apply distinct sampling strategies for two types of areas. The parameters optimization for both processes are made up by two stages, the first stage associates time with 3D positions to convert the deformation field to the canonical field. And the second associates time with 3D positions in canonical field to obtain colors and Signed Distance Function (SDF). Besides, We propose a novel keyframe selection strategy based on the overlapping rate. We evaluate our approach on two publicly available synthetic datasets and validate that our method is more effective compared to current state-of-the-art dynamic mapping methods.Here's the translation in Traditional Chinese:previous attempts to integrate Neural Radiance Fields (NeRF) into Simultaneous Localization and Mapping (SLAM) framework either rely on the assumption of static scenes or treat dynamic objects as outliers. However, most of real-world scenarios is dynamic. In this paper, we propose a time-varying representation to track and reconstruct the dynamic scenes. Our system simultaneously maintains two processes, tracking process and mapping process. For tracking process, the entire input images are uniformly sampled and training of the RGB images are self-supervised. For mapping process, we leverage known masks to differentiate dynamic objects and static backgrounds, and we apply distinct sampling strategies for two types of areas. The parameters optimization for both processes are made up by two stages, the first stage associates time with 3D positions to convert the deformation field to the canonical field. And the second associates time with 3D positions in canonical field to obtain colors and Signed Distance Function (SDF). Besides, We propose a novel keyframe selection strategy based on the overlapping rate. We evaluate our approach on two publicly available synthetic datasets and validate that our method is more effective compared to current state-of-the-art dynamic mapping methods.
</details></li>
</ul>
<hr>
<h2 id="Identifiable-Contrastive-Learning-with-Automatic-Feature-Importance-Discovery"><a href="#Identifiable-Contrastive-Learning-with-Automatic-Feature-Importance-Discovery" class="headerlink" title="Identifiable Contrastive Learning with Automatic Feature Importance Discovery"></a>Identifiable Contrastive Learning with Automatic Feature Importance Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18904">http://arxiv.org/abs/2310.18904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pku-ml/tri-factor-contrastive-learning">https://github.com/pku-ml/tri-factor-contrastive-learning</a></li>
<li>paper_authors: Qi Zhang, Yifei Wang, Yisen Wang</li>
<li>for: 本研究旨在提出一种新的对比学习方法（tri-factor contrastive learning，简称triCL），以便从人类视角获得更加可解解释的数据表示。</li>
<li>methods: triCL使用了一种3因素对比的形式，即 $z_x^\top S z_{x’}$，其中 $S$ 是一个可学习的对角矩阵，自动捕捉到每个特征的重要性。</li>
<li>results: 我们证明了 triCL 可以不仅获得可解解释的特征，而且可以通过对比学习方法来获得更高的性能。我们还发现，高重要性的特征具有良好的可解解释性，可以捕捉到共同的类别特征。<details>
<summary>Abstract</summary>
Existing contrastive learning methods rely on pairwise sample contrast $z_x^\top z_{x'}$ to learn data representations, but the learned features often lack clear interpretability from a human perspective. Theoretically, it lacks feature identifiability and different initialization may lead to totally different features. In this paper, we study a new method named tri-factor contrastive learning (triCL) that involves a 3-factor contrast in the form of $z_x^\top S z_{x'}$, where $S=\text{diag}(s_1,\dots,s_k)$ is a learnable diagonal matrix that automatically captures the importance of each feature. We show that by this simple extension, triCL can not only obtain identifiable features that eliminate randomness but also obtain more interpretable features that are ordered according to the importance matrix $S$. We show that features with high importance have nice interpretability by capturing common classwise features, and obtain superior performance when evaluated for image retrieval using a few features. The proposed triCL objective is general and can be applied to different contrastive learning methods like SimCLR and CLIP. We believe that it is a better alternative to existing 2-factor contrastive learning by improving its identifiability and interpretability with minimal overhead. Code is available at https://github.com/PKU-ML/Tri-factor-Contrastive-Learning.
</details>
<details>
<summary>摘要</summary>
现有的对比学习方法通常基于对比样本的对比度 $z_x^\top z_{x'}$ 来学习数据表示，但学习的特征通常缺乏人类可理解的解释性。理论上来说，它缺乏特征可识别性，不同的初始化可能会导致极其不同的特征。在这篇论文中，我们研究了一种新的方法 named tri-factor contrastive learning (triCL)，它包含了一种三因子对比的形式，即 $z_x^\top S z_{x'}$, 其中 $S$ 是一个可学习的对角矩阵，自动捕捉每个特征的重要性。我们显示了，通过这种简单的扩展，triCL 可以不仅获得可识别的特征，而且可以获得更加可解的特征，这些特征被排序于重要性矩阵 $S$ 中，并且高度重要的特征具有良好的解释性，可以捕捉共同的类别特征，并且在图像检索任务中获得更高的性能。我们表明了 triCL 目标是一种通用的对比学习目标，可以应用于不同的对比学习方法，如 SimCLR 和 CLIP。我们认为，triCL 是现有的 two-factor 对比学习的更好的替代方案，可以提高其可识别性和可解性，而且带来最小的开销。代码可以在 GitHub 上找到：https://github.com/PKU-ML/Tri-factor-Contrastive-Learning。
</details></li>
</ul>
<hr>
<h2 id="Multi-task-deep-learning-for-large-scale-building-detail-extraction-from-high-resolution-satellite-imagery"><a href="#Multi-task-deep-learning-for-large-scale-building-detail-extraction-from-high-resolution-satellite-imagery" class="headerlink" title="Multi-task deep learning for large-scale building detail extraction from high-resolution satellite imagery"></a>Multi-task deep learning for large-scale building detail extraction from high-resolution satellite imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18899">http://arxiv.org/abs/2310.18899</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chanceqz/buildingdetails-multitask">https://github.com/chanceqz/buildingdetails-multitask</a></li>
<li>paper_authors: Zhen Qian, Min Chen, Zhuo Sun, Fan Zhang, Qingsong Xu, Jinzhao Guo, Zhiwei Xie, Zhixin Zhang</li>
<li>for: 本研究旨在提高城市动态的理解和可持续发展，通过对高分辨度卫星图像进行分析，提取建筑物的详细信息。</li>
<li>methods: 本研究提出了一种适应性的神经网络，称为多任务建筑精细化器（MT-BR），可同时提取各种建筑物的空间和属性信息，如建筑物顶、城市功能类型和瓦屋顶型。此外，MT-BR可以根据需要进行可调整，以涵盖更多的建筑物详细信息。</li>
<li>results: 研究人员通过设计一种新的空间采样方法，可以有效地选择高分辨度卫星图像的限定示例，以提高提取建筑物详细信息的效率。此外，通过启用先进的增强技术，MT-BR可以提高预测性能和泛化能力。实验结果表明，MT-BR在不同的维度上都能够达到更高的预测精度，并且在实际应用中，可以生成包含了建筑物的空间和属性信息的一体化数据集。<details>
<summary>Abstract</summary>
Understanding urban dynamics and promoting sustainable development requires comprehensive insights about buildings. While geospatial artificial intelligence has advanced the extraction of such details from Earth observational data, existing methods often suffer from computational inefficiencies and inconsistencies when compiling unified building-related datasets for practical applications. To bridge this gap, we introduce the Multi-task Building Refiner (MT-BR), an adaptable neural network tailored for simultaneous extraction of spatial and attributional building details from high-resolution satellite imagery, exemplified by building rooftops, urban functional types, and roof architectural types. Notably, MT-BR can be fine-tuned to incorporate additional building details, extending its applicability. For large-scale applications, we devise a novel spatial sampling scheme that strategically selects limited but representative image samples. This process optimizes both the spatial distribution of samples and the urban environmental characteristics they contain, thus enhancing extraction effectiveness while curtailing data preparation expenditures. We further enhance MT-BR's predictive performance and generalization capabilities through the integration of advanced augmentation techniques. Our quantitative results highlight the efficacy of the proposed methods. Specifically, networks trained with datasets curated via our sampling method demonstrate improved predictive accuracy relative to those using alternative sampling approaches, with no alterations to network architecture. Moreover, MT-BR consistently outperforms other state-of-the-art methods in extracting building details across various metrics. The real-world practicality is also demonstrated in an application across Shanghai, generating a unified dataset that encompasses both the spatial and attributional details of buildings.
</details>
<details>
<summary>摘要</summary>
理解城市动力和推动可持续发展需要全面的建筑相关信息。 although geospatial人工智能已经提高了对地球观测数据的EXTRACTION，现有的方法经常受到计算不fficient和不一致的问题，这限制了实际应用中的建筑相关数据集的编译。为了bridging这个差距，我们介绍了多任务建筑精细化器（MT-BR），这是适应同时EXTRACTION的建筑相关细节的适应性神经网络。MT-BR可以根据需要进行微调，以包含更多的建筑细节，从而扩展其可用性。为了应对大规模应用，我们提出了一种新的空间采样方案，该方案选择了有限但表示性强的图像样本。这种方法可以最大化图像样本的空间分布和城市环境特征，从而提高EXTRACTION的效果，同时降低数据准备成本。此外，我们还通过 incorporating advanced augmentation techniques 提高MT-BR的预测性能和泛化能力。我们的量化结果表明，使用我们的采样方法训练的网络比使用其他采样方法更高的预测精度，而无需修改网络结构。此外，MT-BR还在不同的维度上一直 OUTPERFORMS 其他现有的方法。此外，我们在上海应用了MT-BR，生成了一个包括建筑物的空间和特征细节的一体化数据集，这进一步证明了MT-BR的实际可行性。
</details></li>
</ul>
<hr>
<h2 id="Emergence-of-Shape-Bias-in-Convolutional-Neural-Networks-through-Activation-Sparsity"><a href="#Emergence-of-Shape-Bias-in-Convolutional-Neural-Networks-through-Activation-Sparsity" class="headerlink" title="Emergence of Shape Bias in Convolutional Neural Networks through Activation Sparsity"></a>Emergence of Shape Bias in Convolutional Neural Networks through Activation Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18894">http://arxiv.org/abs/2310.18894</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/crazy-jack/nips2023_shape_vs_texture">https://github.com/crazy-jack/nips2023_shape_vs_texture</a></li>
<li>paper_authors: Tianqin Li, Ziqi Wen, Yangfan Li, Tai Sing Lee</li>
<li>for: 本研究旨在解释深度学习模型为何偏好文本，而人类视觉系统却偏好形状和结构。</li>
<li>methods: 研究人员使用 sparse coding 原理，通过非 differencing Top-K 操作来引入形状偏好到网络中。</li>
<li>results: 研究发现，在卷积神经网络中，强制执行 sparse coding 约束可以导致 neuron 中的结构编码 emerge，从而使网络具有更好的形状偏好。这种形状偏好会使网络在不同的数据集上展现出更好的鲁棒性和可变性。代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/Crazy-Jack/nips2023_shape_vs_texture%E3%80%82">https://github.com/Crazy-Jack/nips2023_shape_vs_texture。</a><details>
<summary>Abstract</summary>
Current deep-learning models for object recognition are known to be heavily biased toward texture. In contrast, human visual systems are known to be biased toward shape and structure. What could be the design principles in human visual systems that led to this difference? How could we introduce more shape bias into the deep learning models? In this paper, we report that sparse coding, a ubiquitous principle in the brain, can in itself introduce shape bias into the network. We found that enforcing the sparse coding constraint using a non-differential Top-K operation can lead to the emergence of structural encoding in neurons in convolutional neural networks, resulting in a smooth decomposition of objects into parts and subparts and endowing the networks with shape bias. We demonstrated this emergence of shape bias and its functional benefits for different network structures with various datasets. For object recognition convolutional neural networks, the shape bias leads to greater robustness against style and pattern change distraction. For the image synthesis generative adversary networks, the emerged shape bias leads to more coherent and decomposable structures in the synthesized images. Ablation studies suggest that sparse codes tend to encode structures, whereas the more distributed codes tend to favor texture. Our code is host at the github repository: \url{https://github.com/Crazy-Jack/nips2023_shape_vs_texture}
</details>
<details>
<summary>摘要</summary>
当前深度学习模型对物体识别存在强烈的文本偏好。然而，人类视觉系统却具有形态和结构偏好。这种差异的原因可能是什么？我们可以如何在深度学习模型中引入更多的形态偏好？在这篇论文中，我们发现了一种叫做稀畴编码的原则，这种原则在大脑中是普遍存在的。我们发现，通过在 convolutional neural networks 中使用不同的 Top-K 操作来实现稀畴编码约束，可以导致神经元内的编码变得更加结构化，从而使得神经网络具有形态偏好。我们通过不同的数据集来证明这种形态偏好的出现和其功能上的好处。对于物体识别 convolutional neural networks，形态偏好使得神经网络更加抗性于样式和 Pattern 变化的干扰。对于生成 adversarial networks， emerged 形态偏好导致生成的图像更加协调和可分解。我们的代码可以在 GitHub 上找到：\url{https://github.com/Crazy-Jack/nips2023_shape_vs_texture}
</details></li>
</ul>
<hr>
<h2 id="Dynamo-Depth-Fixing-Unsupervised-Depth-Estimation-for-Dynamical-Scenes"><a href="#Dynamo-Depth-Fixing-Unsupervised-Depth-Estimation-for-Dynamical-Scenes" class="headerlink" title="Dynamo-Depth: Fixing Unsupervised Depth Estimation for Dynamical Scenes"></a>Dynamo-Depth: Fixing Unsupervised Depth Estimation for Dynamical Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18887">http://arxiv.org/abs/2310.18887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Sun, Bharath Hariharan</li>
<li>for: 本文提出了一种解决单目深度估计中动态场景中物体运动所引起的困难的方法，通过对无标注单目视频进行共同学习深度、独立流场和动作分割来解决这种问题。</li>
<li>methods: 本文提出了一种jointly学习深度和独立流场的方法，通过提供了一个关键思想，即在初始化时对运动分割有good的估计可以帮助jointly学习深度和独立运动。</li>
<li>results: 本文在 Waymo Open和nuScenes Dataset上实现了单目深度估计的state-of-the-art性能，对运动中的深度有显著改进。<details>
<summary>Abstract</summary>
Unsupervised monocular depth estimation techniques have demonstrated encouraging results but typically assume that the scene is static. These techniques suffer when trained on dynamical scenes, where apparent object motion can equally be explained by hypothesizing the object's independent motion, or by altering its depth. This ambiguity causes depth estimators to predict erroneous depth for moving objects. To resolve this issue, we introduce Dynamo-Depth, an unifying approach that disambiguates dynamical motion by jointly learning monocular depth, 3D independent flow field, and motion segmentation from unlabeled monocular videos. Specifically, we offer our key insight that a good initial estimation of motion segmentation is sufficient for jointly learning depth and independent motion despite the fundamental underlying ambiguity. Our proposed method achieves state-of-the-art performance on monocular depth estimation on Waymo Open and nuScenes Dataset with significant improvement in the depth of moving objects. Code and additional results are available at https://dynamo-depth.github.io.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>无监督单目深度估计技术已经表现出了激动人心的结果，但通常假设场景是静止的。这些技术在动态场景下遇到问题，因为 Apparent 对象的运动可以 equally 由假设对象的独立运动或者由其深度变化来解释。这种歧义导致深度估计器预测错误的深度值。为了解决这个问题，我们介绍了 Dynamo-Depth，一种统一的方法，它在不监督的单目视频上同时学习单目深度、3D 独立流场和动态分割。我们提供了关键的思路，即一个好的初始化动态分割可以为 JOINTLY 学习深度和独立运动，尽管基本的下面歧义存在。我们的提议方法在 Waymo Open 和 nuScenes 数据集上实现了状态的最佳性能，对于运动中的对象的深度有显著改善。代码和更多结果可以在 <https://dynamo-depth.github.io> 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/cs.CV_2023_10_29/" data-id="clogyj8yc00jw7craf8j428km" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/cs.AI_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T12:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/cs.AI_2023_10_29/">cs.AI - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="JEN-1-Composer-A-Unified-Framework-for-High-Fidelity-Multi-Track-Music-Generation"><a href="#JEN-1-Composer-A-Unified-Framework-for-High-Fidelity-Multi-Track-Music-Generation" class="headerlink" title="JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation"></a>JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19180">http://arxiv.org/abs/2310.19180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Yao, Peike Li, Boyu Chen, Alex Wang</li>
<li>for: 本研究旨在提出一种能够实现高级控制的多轨音乐生成模型，以便用户可以通过 iterative 选择和修改音乐轨迹，创造出符合自己的音乐创作审美。</li>
<li>methods: 本研究使用了 JEN-1 算法，并提出了一种带有评估策略的训练策略，以便帮助模型在多轨音乐生成中具备更高的灵活性和控制能力。</li>
<li>results: 对比于现有的音乐生成模型，JEN-1 Composer 能够实现更高的音乐质量和控制能力，并且可以在用户提供的音乐风格和元素的基础上进行高级的音乐创作。<details>
<summary>Abstract</summary>
With rapid advances in generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch. However, finer-grained control over multi-track generation remains an open challenge. Existing models exhibit strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers. To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model. JEN-1 Composer framework exhibits the capacity to seamlessly incorporate any diffusion-based music generation system, \textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music generation. We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible generation of multi-track combinations. During the inference, users have the ability to iteratively produce and choose music tracks that meet their preferences, subsequently creating an entire musical composition incrementally following the proposed Human-AI co-composition workflow. Quantitative and qualitative assessments demonstrate state-of-the-art performance in controllable and high-fidelity multi-track music synthesis. The proposed JEN-1 Composer represents a significant advance toward interactive AI-facilitated music creation and composition. Demos will be available at https://jenmusic.ai/audio-demos.
</details>
<details>
<summary>摘要</summary>
With the rapid development of generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch. However, finer-grained control over multi-track generation remains an open challenge. Existing models have strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers. To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model. The JEN-1 Composer framework can seamlessly incorporate any diffusion-based music generation system, such as Jen-1, enhancing its capacity for versatile multi-track music generation. We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible generation of multi-track combinations. During the inference, users have the ability to iteratively produce and choose music tracks that meet their preferences, subsequently creating an entire musical composition incrementally following the proposed Human-AI co-composition workflow. Quantitative and qualitative assessments demonstrate state-of-the-art performance in controllable and high-fidelity multi-track music synthesis. The proposed JEN-1 Composer represents a significant advance toward interactive AI-facilitated music creation and composition. Demos will be available at https://jenmusic.ai/audio-demos.
</details></li>
</ul>
<hr>
<h2 id="Predicting-recovery-following-stroke-deep-learning-multimodal-data-and-feature-selection-using-explainable-AI"><a href="#Predicting-recovery-following-stroke-deep-learning-multimodal-data-and-feature-selection-using-explainable-AI" class="headerlink" title="Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI"></a>Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19174">http://arxiv.org/abs/2310.19174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam White, Margarita Saranti, Artur d’Avila Garcez, Thomas M. H. Hope, Cathy J. Price, Howard Bowman</li>
<li>for: 这 paper 的目的是使用机器学习自动预测 stroke 后症状和其回归治疗的效果。</li>
<li>methods: 这 paper 使用了两种策略：首先使用 2D 图像概述 MRI 扫描结果，其次选择关键特征以提高分类精度。此外，文章还介绍了一种新的方法，即在 MRI 图像和表格数据之间融合学习。</li>
<li>results: 文章的结果显示，可以通过组合 MRI 图像和表格数据来实现高精度的 post-stroke 分类。在不同的 CNN 架构和数据 Representation 下，分类精度最高达 0.854。<details>
<summary>Abstract</summary>
Machine learning offers great potential for automated prediction of post-stroke symptoms and their response to rehabilitation. Major challenges for this endeavour include the very high dimensionality of neuroimaging data, the relatively small size of the datasets available for learning, and how to effectively combine neuroimaging and tabular data (e.g. demographic information and clinical characteristics). This paper evaluates several solutions based on two strategies. The first is to use 2D images that summarise MRI scans. The second is to select key features that improve classification accuracy. Additionally, we introduce the novel approach of training a convolutional neural network (CNN) on images that combine regions-of-interest extracted from MRIs, with symbolic representations of tabular data. We evaluate a series of CNN architectures (both 2D and a 3D) that are trained on different representations of MRI and tabular data, to predict whether a composite measure of post-stroke spoken picture description ability is in the aphasic or non-aphasic range. MRI and tabular data were acquired from 758 English speaking stroke survivors who participated in the PLORAS study. The classification accuracy for a baseline logistic regression was 0.678 for lesion size alone, rising to 0.757 and 0.813 when initial symptom severity and recovery time were successively added. The highest classification accuracy 0.854 was observed when 8 regions-of-interest was extracted from each MRI scan and combined with lesion size, initial severity and recovery time in a 2D Residual Neural Network.Our findings demonstrate how imaging and tabular data can be combined for high post-stroke classification accuracy, even when the dataset is small in machine learning terms. We conclude by proposing how the current models could be improved to achieve even higher levels of accuracy using images from hospital scanners.
</details>
<details>
<summary>摘要</summary>
Machine learning可以提供很大的潜在 для自动预测 poste stroke 症状和其回归治疗的结果。主要挑战包括神经成像数据的非常高维度，可用学习 dataset 的较小尺寸，以及如何有效地结合神经成像和表格数据（例如人口信息和临床特征）。本文评估了多种解决方案，包括使用 2D 图像简化 MRI 扫描结果，以及选择关键特征来提高分类精度。此外，我们还引入了一种新的方法，即在 MRI 扫描结果和表格数据之间进行 симвоlic 表示的训练 convolutional neural network (CNN)。我们评估了一系列 CNN 架构（包括 2D 和 3D），并在不同的 MRI 和表格数据表示下进行训练，以预测stroke 后 spoken picture 描述能力是否在非语症状范围内。MRI 和表格数据来自英国758名中文roke 幸存者参与了PLORAS 研究。分类精度的最高值为 0.854，出现在使用 8 个区域特征 Extracted from each MRI scan 和 lesion size、初始症状严重程度和Recovery time 的 2D Residual Neural Network 中。我们的发现表明，通过结合神经成像和表格数据，可以实现高度的 poste stroke 分类精度，即使数据集较小。我们结束的建议如下：通过使用医疗器械上的图像，可以进一步提高当前模型的准确率。
</details></li>
</ul>
<hr>
<h2 id="Rare-Event-Probability-Learning-by-Normalizing-Flows"><a href="#Rare-Event-Probability-Learning-by-Normalizing-Flows" class="headerlink" title="Rare Event Probability Learning by Normalizing Flows"></a>Rare Event Probability Learning by Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19167">http://arxiv.org/abs/2310.19167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenggqi Gao, Dinghuai Zhang, Luca Daniel, Duane S. Boning<br>for:NOFIS 是一种用于估计罕seen事件的方法，可以在多种领域中提供精确的估计。methods:NOFIS 使用了 normalizing flows 的特点，通过学习一系列的提案分布来实现高效的估计。results:NOFIS 在多个测试 caso 中表现出色，superior 于基eline方法，并且可以提供高质量的估计结果。<details>
<summary>Abstract</summary>
A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing $10$ distinct test cases, which highlight NOFIS's superiority over baseline approaches.
</details>
<details>
<summary>摘要</summary>
一种罕见事件被定义为发生的概率很低。正确地估计这种小概率是多种领域的关键问题。传统的 Монте卡洛方法是不具有效率，需要很多样本来获得可靠的估计。以启发式扩展流为引用，我们再次挑战这个问题，并提出了正则化流助けimportance sampling（NOFIS）方法。NOFIS首先学习一个序列的提议分布，这些分布与预先定义的嵌套子事件相关。然后，它使用重要样本法并与最后一个提议相结合，来估计罕见事件的概率。我们通过对多个测试案例进行详细的可见化，证明了提议分布的优化性，以及对基eline方法的超越。
</details></li>
</ul>
<hr>
<h2 id="Automaton-Distillation-Neuro-Symbolic-Transfer-Learning-for-Deep-Reinforcement-Learning"><a href="#Automaton-Distillation-Neuro-Symbolic-Transfer-Learning-for-Deep-Reinforcement-Learning" class="headerlink" title="Automaton Distillation: Neuro-Symbolic Transfer Learning for Deep Reinforcement Learning"></a>Automaton Distillation: Neuro-Symbolic Transfer Learning for Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19137">http://arxiv.org/abs/2310.19137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suraj Singireddy, Andre Beckus, George Atia, Sumit Jha, Alvaro Velasquez</li>
<li>for: 这篇论文旨在应用自适应学习（Reinforcement Learning，RL）来找到最佳策略，但深度RL方法受到两个弱点：需要大量的机器人体验，并且学习的策略对于训练分布外的任务呈现出差强。</li>
<li>methods: 我们提出了两种方法来生成Q值估计：静止转移，它是基于先前知识建立的抽象Markov Decision Process（MDP）上进行推理，以及动态转移，它是从教师Deep Q-Network（DQN）提取Symbolic信息。</li>
<li>results: 我们的实验结果显示，静止转移和动态转移都可以减少获得最佳策略所需的时间，并且在不同的决策任务中均有良好的表现。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) is a powerful tool for finding optimal policies in sequential decision processes. However, deep RL methods suffer from two weaknesses: collecting the amount of agent experience required for practical RL problems is prohibitively expensive, and the learned policies exhibit poor generalization on tasks outside of the training distribution. To mitigate these issues, we introduce automaton distillation, a form of neuro-symbolic transfer learning in which Q-value estimates from a teacher are distilled into a low-dimensional representation in the form of an automaton. We then propose two methods for generating Q-value estimates: static transfer, which reasons over an abstract Markov Decision Process constructed based on prior knowledge, and dynamic transfer, where symbolic information is extracted from a teacher Deep Q-Network (DQN). The resulting Q-value estimates from either method are used to bootstrap learning in the target environment via a modified DQN loss function. We list several failure modes of existing automaton-based transfer methods and demonstrate that both static and dynamic automaton distillation decrease the time required to find optimal policies for various decision tasks.
</details>
<details>
<summary>摘要</summary>
《强化学习（RL）是一种有力的工具，可以找到sequential decision process中的优化策略。但是深度RL方法受到两点弱点：收集agent经验所需的成本是实际RL问题中 prohibitively expensive，并且学习的策略具有poor generalization在训练分布外的任务上。为了缓解这些问题，我们引入自动机液化，一种neuro-symbolic transfer learning的形式，其中Q值估计从教师中提取到一个低维度表示形式中，这个形式是一个自动机。然后我们提出了两种方法来生成Q值估计：静态传输，这里是reasoning over一个基于先前知识构建的抽象Markov决策过程，以及动态传输，其中Symbolic信息从一个教师深度Q网络（DQN）中提取出来。这些Q值估计的结果被用来启动target环境中的学习，通过一个修改后DQN损失函数。我们列出了现有自动机基于转移方法的失败模式，并证明了静态和动态自动机液化都可以降低在不同决策任务中找到优化策略所需的时间。》Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Three-Dogmas-a-Puzzle-and-its-Solution"><a href="#Three-Dogmas-a-Puzzle-and-its-Solution" class="headerlink" title="Three Dogmas, a Puzzle and its Solution"></a>Three Dogmas, a Puzzle and its Solution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19123">http://arxiv.org/abs/2310.19123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elnaserledinellah Mahmood Abdelwahab<br>for:The paper challenges the assumptions of Modern Logics, particularly those of Frege, Russell, and Tarski, and their applications in formal languages.methods:The paper uses undisputed principles of Arabic to falsify the Logicians’ ideas and demonstrate the limitations of their approaches. It also utilizes the existence of “meaning-particles” in Arabic syntax to efficiently recognize words, phrases, and sentences.results:The paper shows that the assumptions of Modern Logics contradict basic principles of Arabic, and that the approaches based on these assumptions are not applicable to Arabic. It also presents a new way to approach the computational problem of Satisfiability (SAT) using the realization that parsing Arabic utilizes the existence of “meaning-particles” within syntax. The paper provides practical evidence, obtained for multiplication circuits, supporting its claims.<details>
<summary>Abstract</summary>
Modern Logics, as formulated notably by Frege, Russell and Tarski involved basic assumptions about Natural Languages in general and Indo-European Languages in particular, which are contested by Linguists. Based upon those assumptions, formal Languages were designed to overcome what Logicians claimed to be 'defects' of Natural Language. In this paper we show that those assumptions contradict basic principles of Arabic. More specifically: The Logicians ideas, that within Natural Language words refer to objects, 'ToBe'-constructions represent identity statements, Indefinite Descriptions must be replaced by existential quantifiers to form meaningful Sentences and Symbols can have no interpretation-independent meanings, are all falsified using undisputed principles of Arabic. The here presented falsification serves two purposes. First, it is used as a factual basis for the rejection of approaches adopting Semantic axioms of Mathematical Logics as Models for meaning of Arabic Syntax. Second, it shows a way to approach the important computational problem: Satisfiability (SAT). The described way is based upon the realization that parsing Arabic utilizes the existence of 'meaning-particles' within Syntax to efficiently recognize words, phrases and Sentences. Similar meaning-particles are shown to exist in 3CNF formulas, which, when properly handled within the machinery of 3SAT-Solvers, enable structural conditions to be imposed on formulas, sufficient alone to guarantee the efficient production of non-exponentially sized Free Binary Decision Diagrams (FBDDs). We show, why known exponential Lower Bounds on sizes of FBDDs do not contradict our results and reveal practical evidence, obtained for multiplication circuits, supporting our claims.
</details>
<details>
<summary>摘要</summary>
现代逻辑，如Frege、Russell和Tarski所提出的基本假设，对于自然语言和印欧语言而言都存在争议。基于这些假设，形式语言被设计用于超越逻辑家所认为自然语言存在的缺陷。在这篇论文中，我们表明了这些假设与阿拉伯语言的基本原理矛盾。具体来说，逻辑家所认为的各种假设，如自然语言中词语引用对象、'ToBe'-构造表示Identidad句子、不确定描述需要通过存在量词替换来形成意义的句子，以及符号没有独立的解释意义，都被使用不争的阿拉伯语言原理驳斥。这种驳斥服两目的。首先，它用作拒绝采用数学逻辑语义模型的方法的拒绝基础。其次，它显示了如何使用阿拉伯语言的存在意思粒子来效率地识别单词、短语和句子。这种方法还可以应用于计算问题：满足（SAT）。我们表明了如何使用这种方法，并解释了为什么已知的下界不会与我们的结果相矛盾。此外，我们还提供了实践证据，来支持我们的主张。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-V2X-Autonomous-Perception-from-Road-to-Vehicle-Vision"><a href="#Dynamic-V2X-Autonomous-Perception-from-Road-to-Vehicle-Vision" class="headerlink" title="Dynamic V2X Autonomous Perception from Road-to-Vehicle Vision"></a>Dynamic V2X Autonomous Perception from Road-to-Vehicle Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19113">http://arxiv.org/abs/2310.19113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayao Tan, Fan Lyu, Linyan Li, Fuyuan Hu, Tingliang Feng, Fenglei Xu, Rui Yao<br>for: 提高自动驾驶系统的安全性和可靠性，适应动态场景methods: 基于路径视觉建立道路到车辆视觉，提出适应性强的道路到车辆视觉感知方法（AR2VP）results: 在3D对象检测和分割任务中，AR2VP在性能和带宽之间做出了优秀的折衔，同时在动态环境中保持了模型的适应性。<details>
<summary>Abstract</summary>
Vehicle-to-everything (V2X) perception is an innovative technology that enhances vehicle perception accuracy, thereby elevating the security and reliability of autonomous systems. However, existing V2X perception methods focus on static scenes from mainly vehicle-based vision, which is constrained by sensor capabilities and communication loads. To adapt V2X perception models to dynamic scenes, we propose to build V2X perception from road-to-vehicle vision and present Adaptive Road-to-Vehicle Perception (AR2VP) method. In AR2VP,we leverage roadside units to offer stable, wide-range sensing capabilities and serve as communication hubs. AR2VP is devised to tackle both intra-scene and inter-scene changes. For the former, we construct a dynamic perception representing module, which efficiently integrates vehicle perceptions, enabling vehicles to capture a more comprehensive range of dynamic factors within the scene.Moreover, we introduce a road-to-vehicle perception compensating module, aimed at preserving the maximized roadside unit perception information in the presence of intra-scene changes.For inter-scene changes, we implement an experience replay mechanism leveraging the roadside unit's storage capacity to retain a subset of historical scene data, maintaining model robustness in response to inter-scene shifts. We conduct perception experiment on 3D object detection and segmentation, and the results show that AR2VP excels in both performance-bandwidth trade-offs and adaptability within dynamic environments.
</details>
<details>
<summary>摘要</summary>
自动驾驶系统的安全和可靠性得到了提高，由于交通场景的变化和不确定性，需要进一步提高汽车的感知精度。现有的V2X感知方法都是基于主要是汽车视觉的静止场景，受到感知器和通信负担的限制。为了适应动态场景，我们提出了基于路面到汽车视觉的Adaptive Road-to-Vehicle Perception（AR2VP）方法。在AR2VP中，我们利用路边设备提供稳定、广泛感知能力，并作为通信枢纽。AR2VP能够应对内场景和间场景变化。对于内场景变化，我们构建了动态感知表示模块，能够有效地集成汽车感知，让汽车能够捕捉更广泛的动态因素。此外，我们引入了路面到汽车感知补做模块，以保持路边设备感知信息的最大化，对于内场景变化。对于间场景变化，我们实施了经验回放机制，利用路边设备的存储容量保留一部分历史场景数据，以保持模型对间场景变化的Robustness。我们对3D объек特征检测和分割进行感知实验，结果显示，AR2VP在性能和带宽之间的负担平衡和动态环境中的适应性都表现出色。
</details></li>
</ul>
<hr>
<h2 id="Efficient-IoT-Inference-via-Context-Awareness"><a href="#Efficient-IoT-Inference-via-Context-Awareness" class="headerlink" title="Efficient IoT Inference via Context-Awareness"></a>Efficient IoT Inference via Context-Awareness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19112">http://arxiv.org/abs/2310.19112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Mehdi Rastikerdar, Jin Huang, Shiwei Fang, Hui Guan, Deepak Ganesan</li>
<li>for: 提高深度学习模型在有限资源环境中的表现，特别是在iot设备上。</li>
<li>methods: 提出了一种新的 paradigm，即CACTUS，它可以快速、可扩展地在不同的上下文中进行Context-aware classification。CACTUS包括了三个innovation：1）优化上下文意识类фика器的训练成本，2）实现在线上下文意识切换，3）根据有限资源选择最佳上下文意识类фика器。</li>
<li>results: CACTUS在多种数据集和iot平台上实现了显著的准确率、响应时间和计算预算的改善。<details>
<summary>Abstract</summary>
While existing strategies for optimizing deep learning-based classification models on low-power platforms assume the models are trained on all classes of interest, this paper posits that adopting context-awareness i.e. focusing solely on the likely classes in the current context, can substantially enhance performance in resource-constrained environments. We propose a new paradigm, CACTUS, for scalable and efficient context-aware classification where a micro-classifier recognizes a small set of classes relevant to the current context and, when context change happens, rapidly switches to another suitable micro-classifier. CACTUS has several innovations including optimizing the training cost of context-aware classifiers, enabling on-the-fly context-aware switching between classifiers, and selecting the best context-aware classifiers given limited resources. We show that CACTUS achieves significant benefits in accuracy, latency, and compute budget across a range of datasets and IoT platforms.
</details>
<details>
<summary>摘要</summary>
While existing strategies for optimizing deep learning-based classification models on low-power platforms assume the models are trained on all classes of interest, this paper proposes a new approach that focuses solely on the likely classes in the current context, which can significantly enhance performance in resource-constrained environments. The proposed paradigm, CACTUS, is designed for scalable and efficient context-aware classification, where a micro-classifier recognizes a small set of classes relevant to the current context and can rapidly switch to another suitable micro-classifier when context changes occur. CACTUS has several innovative features, including optimizing the training cost of context-aware classifiers, enabling on-the-fly context-aware switching between classifiers, and selecting the best context-aware classifiers given limited resources. The paper shows that CACTUS achieves significant benefits in accuracy, latency, and compute budget across a range of datasets and IoT platforms.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Task-and-Weight-Prioritization-Curriculum-Learning-for-Multimodal-Imagery"><a href="#Dynamic-Task-and-Weight-Prioritization-Curriculum-Learning-for-Multimodal-Imagery" class="headerlink" title="Dynamic Task and Weight Prioritization Curriculum Learning for Multimodal Imagery"></a>Dynamic Task and Weight Prioritization Curriculum Learning for Multimodal Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19109">http://arxiv.org/abs/2310.19109</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fualsan/datwep">https://github.com/fualsan/datwep</a></li>
<li>paper_authors: Huseyin Fuat Alsan, Taner Arsan<br>for:这篇论文探索了在多modal深度学习模型下进行后灾分析，使用了curriculum learning方法来优化模型的性能。methods:这篇论文提出了一个curriculum learning策略，通过让深度学习模型在增加复杂性的数据上进行运动，以提高模型的性能。这篇论文使用了U-Net模型进行semantic segmentation和图像编码，并使用了自定义的文本分类器进行视觉问题回答。results:这篇论文的结果显示， DATWEP方法可以帮助提高多modal深度学习模型的视觉问题回答性能。 sources code可以在<a target="_blank" rel="noopener" href="https://github.com/fualsan/DATWEP%E4%B8%8A%E5%8F%96%E5%BE%97%E3%80%82">https://github.com/fualsan/DATWEP上取得。</a><details>
<summary>Abstract</summary>
This paper explores post-disaster analytics using multimodal deep learning models trained with curriculum learning method. Studying post-disaster analytics is important as it plays a crucial role in mitigating the impact of disasters by providing timely and accurate insights into the extent of damage and the allocation of resources. We propose a curriculum learning strategy to enhance the performance of multimodal deep learning models. Curriculum learning emulates the progressive learning sequence in human education by training deep learning models on increasingly complex data. Our primary objective is to develop a curriculum-trained multimodal deep learning model, with a particular focus on visual question answering (VQA) capable of jointly processing image and text data, in conjunction with semantic segmentation for disaster analytics using the FloodNet\footnote{https://github.com/BinaLab/FloodNet-Challenge-EARTHVISION2021} dataset. To achieve this, U-Net model is used for semantic segmentation and image encoding. A custom built text classifier is used for visual question answering. Existing curriculum learning methods rely on manually defined difficulty functions. We introduce a novel curriculum learning approach termed Dynamic Task and Weight Prioritization (DATWEP), which leverages a gradient-based method to automatically decide task difficulty during curriculum learning training, thereby eliminating the need for explicit difficulty computation. The integration of DATWEP into our multimodal model shows improvement on VQA performance. Source code is available at https://github.com/fualsan/DATWEP.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Web3-Meets-AI-Marketplace-Exploring-Opportunities-Analyzing-Challenges-and-Suggesting-Solutions"><a href="#Web3-Meets-AI-Marketplace-Exploring-Opportunities-Analyzing-Challenges-and-Suggesting-Solutions" class="headerlink" title="Web3 Meets AI Marketplace: Exploring Opportunities, Analyzing Challenges, and Suggesting Solutions"></a>Web3 Meets AI Marketplace: Exploring Opportunities, Analyzing Challenges, and Suggesting Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19099">http://arxiv.org/abs/2310.19099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peihao Li</li>
<li>for: 这篇论文旨在探讨AI和Web3的交叉领域，即DeAI，以及该领域的机遇和挑战。</li>
<li>methods: 本文使用了一种框架，让用户可以使用任何种投资链urrency来购买AI服务，同时也可以在协议中短时间锁定资产来获得免费AI服务。</li>
<li>results: 本文提出了一种解决AI市场在Web3空间快速发展的方案，并且打开了该领域的新的商业机会。<details>
<summary>Abstract</summary>
Web3 and AI have been among the most discussed fields over the recent years, with substantial hype surrounding each field's potential to transform the world as we know it. However, as the hype settles, it's evident that neither AI nor Web3 can address all challenges independently. Consequently, the intersection of AI and Web3 is gaining increased attention, emerging as a new field with the potential to address the limitations of each. In this article, we will focus on the integration of web3 and the AI marketplace, where AI services and products can be provided in a decentralized manner (DeAI). A comprehensive review is provided by summarizing the opportunities and challenges on this topic. Additionally, we offer analyses and solutions to address these challenges. We've developed a framework that lets users pay with any kind of cryptocurrency to get AI services. Additionally, they can also enjoy AI services for free on our platform by simply locking up their assets temporarily in the protocol. This unique approach is a first in the industry. Before this, offering free AI services in the web3 community wasn't possible. Our solution opens up exciting opportunities for the AI marketplace in the web3 space to grow and be widely adopted.
</details>
<details>
<summary>摘要</summary>
“Web3和人工智能（AI）在最近几年内得到了很多关注，但是它们无法独立解决全部问题。因此，Web3和AI之间的交叉领域正在吸引越来越多的关注，并且被认为可以抵消每个领域的局限性。本文将关注Web3和AI市场的融合，即DeAI（Decentralized AI）。我们将提供全面的机会和挑战的概述，以及解决这些挑战的分析和解决方案。我们已经开发了一套框架，允许用户使用任何种 криптовалю来购买AI服务。此外，用户还可以在我们的平台上免费使用AI服务，只需将资产短时间内锁定在协议中。这种独特的方法是行业中的首次实践。在这之前，在Web3社区中无法免费提供AI服务。我们的解决方案将推动AI市场在Web3空间广泛采用和普及。”
</details></li>
</ul>
<hr>
<h2 id="Roles-of-Scaling-and-Instruction-Tuning-in-Language-Perception-Model-vs-Human-Attention"><a href="#Roles-of-Scaling-and-Instruction-Tuning-in-Language-Perception-Model-vs-Human-Attention" class="headerlink" title="Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention"></a>Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19084">http://arxiv.org/abs/2310.19084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RiverGao/human_llm_attention">https://github.com/RiverGao/human_llm_attention</a></li>
<li>paper_authors: Changjiang Gao, Shujian Huang, Jixing Li, Jiajun Chen</li>
<li>for: 这研究旨在调查大型自然语言处理器（LLMs）在培训过程中的成功因素，以及这些因素对模型的语言理解能力的影响。</li>
<li>methods: 这研究使用了多种现有的LLMs（LLaMA、Alpaca和Vicuna），并对不同大小（7B、13B、30B、65B）进行比较，以评估 scaling和指令调整对语言理解的影响。</li>
<li>results: 结果显示， scaling 可以增强人类阅读注意力的效果，并减少无关的模式依赖性，而 instruction tuning 则不会对语言理解产生影响。此外，现有的 LLMS 都在注意力方面存在一定的不足，它们的注意力更接近非native than native 语言。<details>
<summary>Abstract</summary>
Recent large language models (LLMs) have revealed strong abilities to understand natural language. Since most of them share the same basic structure, i.e. the transformer block, possible contributors to their success in the training process are scaling and instruction tuning. However, how these factors affect the models' language perception is unclear. This work compares the self-attention of several existing LLMs (LLaMA, Alpaca and Vicuna) in different sizes (7B, 13B, 30B, 65B), together with eye saccade, an aspect of human reading attention, to assess the effect of scaling and instruction tuning on language perception. Results show that scaling enhances the human resemblance and improves the effective attention by reducing the trivial pattern reliance, while instruction tuning does not. However, instruction tuning significantly enhances the models' sensitivity to instructions. We also find that current LLMs are consistently closer to non-native than native speakers in attention, suggesting a sub-optimal language perception of all models. Our code and data used in the analysis is available on GitHub.
</details>
<details>
<summary>摘要</summary>
最近的大型语言模型（LLMs）表现出了对自然语言的强大理解能力。由于大多数这些模型具有相同的基本结构，即转换块，因此可能的贡献因素包括扩大和指导调整。然而，这些因素对模型的语言识别是如何影响的还未清楚。这项工作比较了一些现有的LLMs（LLaMA、Alpaca和Vicuna）在不同大小（7B、13B、30B、65B）中的自我注意力，以及人类阅读注意力的眼动踪迹，以评估扩大和指导调整对语言识别的影响。结果显示，扩大可以提高人类类似性和有效注意力，而减少了杂乱模式的依赖性。然而，指导调整并没有这样的效果。此外，我们发现现有的LLMs在注意力方面都更接近非本地语言 speaker，这表明所有模型的语言识别能力有所不足。我们在 GitHub 上提供了代码和数据，用于进行分析。
</details></li>
</ul>
<hr>
<h2 id="Bespoke-Solvers-for-Generative-Flow-Models"><a href="#Bespoke-Solvers-for-Generative-Flow-Models" class="headerlink" title="Bespoke Solvers for Generative Flow Models"></a>Bespoke Solvers for Generative Flow Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19075">http://arxiv.org/abs/2310.19075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neta Shaul, Juan Perez, Ricky T. Q. Chen, Ali Thabet, Albert Pumarola, Yaron Lipman</li>
<li>for: 这个论文是为了提高Diffusion或流体模型的生成能力而写的。</li>
<li>methods: 这个论文使用了” Bespoke solvers”，一种新的框架，用于构建特定的ODE解。</li>
<li>results: 这个论文的结果表明，使用” Bespoke solvers”可以大幅提高生成质量，只需要1%的GPU时间和80个学习参数。<details>
<summary>Abstract</summary>
Diffusion or flow-based models are powerful generative paradigms that are notoriously hard to sample as samples are defined as solutions to high-dimensional Ordinary or Stochastic Differential Equations (ODEs/SDEs) which require a large Number of Function Evaluations (NFE) to approximate well. Existing methods to alleviate the costly sampling process include model distillation and designing dedicated ODE solvers. However, distillation is costly to train and sometimes can deteriorate quality, while dedicated solvers still require relatively large NFE to produce high quality samples. In this paper we introduce "Bespoke solvers", a novel framework for constructing custom ODE solvers tailored to the ODE of a given pre-trained flow model. Our approach optimizes an order consistent and parameter-efficient solver (e.g., with 80 learnable parameters), is trained for roughly 1% of the GPU time required for training the pre-trained model, and significantly improves approximation and generation quality compared to dedicated solvers. For example, a Bespoke solver for a CIFAR10 model produces samples with Fr\'echet Inception Distance (FID) of 2.73 with 10 NFE, and gets to 1% of the Ground Truth (GT) FID (2.59) for this model with only 20 NFE. On the more challenging ImageNet-64$\times$64, Bespoke samples at 2.2 FID with 10 NFE, and gets within 2% of GT FID (1.71) with 20 NFE.
</details>
<details>
<summary>摘要</summary>
Diffusion或流程基本模型是一种强大的生成概念，但它们很难进行样本生成，因为样本是定义为高维度常微方程（ODE）或随机常微方程（SDE）的解。现有的方法可以减少样本生成的成本，包括模型热塑化和专门设计的ODE解程。然而，热塑化训练成本较高，并且有时会降低质量，而专门的解程仍然需要相对较多的功能评估（NFE）来生成高质量的样本。在这篇论文中，我们介绍了“特制解程”，一种新的框架，用于建立针对已经训练的流变模型的自定义ODE解程。我们的方法通过优化一个与顺序数相同的并高效的参数（例如80个学习参数），在约1%的GPU时间上训练，并显著改善了样本生成和预测质量，相比特定解程。例如，一个特制解程为CIFAR10模型生成的样本的Fréchet Inception Distance（FID）为2.73，只需10个NFE，并在20个NFE下达到1%的原始真实值（GT）FID（2.59）。在更加困难的ImageNet-64×64上，特制样本的FID为2.2，只需10个NFE，并在20个NFE下达到2%的GT FID（1.71）。
</details></li>
</ul>
<hr>
<h2 id="Gauge-optimal-approximate-learning-for-small-data-classification-problems"><a href="#Gauge-optimal-approximate-learning-for-small-data-classification-problems" class="headerlink" title="Gauge-optimal approximate learning for small data classification problems"></a>Gauge-optimal approximate learning for small data classification problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19066">http://arxiv.org/abs/2310.19066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edoardo Vecchi, Davide Bassetti, Fabio Graziato, Lukas Pospisil, Illia Horenko</li>
<li>for: 这篇论文是为了解决小数据学问题， Specifically, the paper aims to address small data learning problems, where there is a significant discrepancy between the limited amount of response variable observations and the large feature space dimension.</li>
<li>methods: 本论文提出了一个新的方法，即对焦点测量（Gauge-Optimal Approximate Learning，GOAL）算法，这个算法可以实现缩减和旋转特征空间，并提供一个分析可能的解方案。 The paper proposes a new method called Gauge-Optimal Approximate Learning (GOAL) algorithm, which can reduce and rotate the feature space and provide an analytically tractable joint solution to the dimension reduction, feature segmentation, and classification problems for small data learning problems.</li>
<li>results: 实验结果显示， compared to other state-of-the-art machine learning (ML) tools, the proposed GOAL algorithm outperforms the reported best competitors for these problems both in learning performance and computational cost. The experimental results show that the proposed algorithm can accurately classify the data and is more efficient than other methods.<details>
<summary>Abstract</summary>
Small data learning problems are characterized by a significant discrepancy between the limited amount of response variable observations and the large feature space dimension. In this setting, the common learning tools struggle to identify the features important for the classification task from those that bear no relevant information, and cannot derive an appropriate learning rule which allows to discriminate between different classes. As a potential solution to this problem, here we exploit the idea of reducing and rotating the feature space in a lower-dimensional gauge and propose the Gauge-Optimal Approximate Learning (GOAL) algorithm, which provides an analytically tractable joint solution to the dimension reduction, feature segmentation and classification problems for small data learning problems. We prove that the optimal solution of the GOAL algorithm consists in piecewise-linear functions in the Euclidean space, and that it can be approximated through a monotonically convergent algorithm which presents -- under the assumption of a discrete segmentation of the feature space -- a closed-form solution for each optimization substep and an overall linear iteration cost scaling. The GOAL algorithm has been compared to other state-of-the-art machine learning (ML) tools on both synthetic data and challenging real-world applications from climate science and bioinformatics (i.e., prediction of the El Nino Southern Oscillation and inference of epigenetically-induced gene-activity networks from limited experimental data). The experimental results show that the proposed algorithm outperforms the reported best competitors for these problems both in learning performance and computational cost.
</details>
<details>
<summary>摘要</summary>
小数据学习问题特征在于响应变量观察数量较少，而特征空间维度很大。在这种情况下，常见的学习工具困难分化重要的特征和无关信息，并 derivate一个适当的学习规则来区分不同的类别。为解决这个问题，我们利用减少和旋转特征空间的低维度投影的想法，并提出了一种可解算的GOAL算法，该算法可以同时解决小数据学习问题中的维度减少、特征分解和分类问题。我们证明了GOAL算法的优化解决方案是均匀分割的几何函数，并且可以通过一个 monotonically convergent 算法来approximate，该算法在特征空间的精确分割情况下具有closed-form解决方案和linear iteration cost scaling。GOAL算法与其他当前领先的机器学习工具进行比较，在 sintetic data 和挑战性的实际应用中（如气候科学和生物信息学）表现出色，其性能和计算成本均高于报道的最佳竞争对手。
</details></li>
</ul>
<hr>
<h2 id="A-multi-modal-table-tennis-robot-system"><a href="#A-multi-modal-table-tennis-robot-system" class="headerlink" title="A multi-modal table tennis robot system"></a>A multi-modal table tennis robot system</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19062">http://arxiv.org/abs/2310.19062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Ziegler, Thomas Gossard, Karl Vetter, Jonas Tebbe, Andreas Zell</li>
<li>for: 研究人员透过设计一个高精度视觉检测和快速机器人反应的推进系统，以解决机器人网球游戏中的视觉感知和机器人控制问题。</li>
<li>methods: 本研究使用了KUKA机器人臂，配备了6DOF的四个架构式摄像头和两个事件式摄像头，并开发了一个新的检测和均衡方法以测量这个多modal的感知系统。</li>
<li>results: 研究人员透过调整该多modal的感知系统，提高了网球的推进精度和速度，并且引入了一个新的旋转估计方法以提高网球的旋转精度。最后，研究人员还展示了结合事件式摄像头和神经网络的精度实时网球检测方法。<details>
<summary>Abstract</summary>
In recent years, robotic table tennis has become a popular research challenge for perception and robot control. Here, we present an improved table tennis robot system with high accuracy vision detection and fast robot reaction. Based on previous work, our system contains a KUKA robot arm with 6 DOF, with four frame-based cameras and two additional event-based cameras. We developed a novel calibration approach to calibrate this multimodal perception system. For table tennis, spin estimation is crucial. Therefore, we introduced a novel, and more accurate spin estimation approach. Finally, we show how combining the output of an event-based camera and a Spiking Neural Network (SNN) can be used for accurate ball detection.
</details>
<details>
<summary>摘要</summary>
Recently, robotic table tennis has become a popular research challenge for perception and robot control. Here, we present an improved table tennis robot system with high accuracy vision detection and fast robot reaction. Based on previous work, our system contains a KUKA robot arm with 6 DOF, with four frame-based cameras and two additional event-based cameras. We developed a novel calibration approach to calibrate this multimodal perception system. For table tennis, spin estimation is crucial. Therefore, we introduced a novel, and more accurate spin estimation approach. Finally, we show how combining the output of an event-based camera and a Spiking Neural Network (SNN) can be used for accurate ball detection.Translation in Simplified Chinese:近年来，机器人乒乓球成为了视觉和机器人控制领域的流行研究挑战。在这里，我们提出了一个改进型的乒乓球机器人系统，拥有高精度视觉检测和快速机器人反应。基于先前的工作，我们的系统包括一个KUKA机器人臂 WITH 6 DOE，以及四个帧基的摄像头和两个事件基的摄像头。我们开发了一种新的委外纠偏方法来委外这个多模态感知系统。为乒乓球而言，轨迹估计是非常重要的。因此，我们引入了一种新的、更加准确的轨迹估计方法。最后，我们示出了将事件基摄像头的输出和神经网络（SNN）结合使用，可以实现高精度的球体检测。
</details></li>
</ul>
<hr>
<h2 id="TESTA-Temporal-Spatial-Token-Aggregation-for-Long-form-Video-Language-Understanding"><a href="#TESTA-Temporal-Spatial-Token-Aggregation-for-Long-form-Video-Language-Understanding" class="headerlink" title="TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding"></a>TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19060">http://arxiv.org/abs/2310.19060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/renshuhuai-andy/testa">https://github.com/renshuhuai-andy/testa</a></li>
<li>paper_authors: Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, Lu Hou</li>
<li>for: 提高视频语言理解任务的效率，尤其是对长形视频的处理。</li>
<li>methods: 提出了一种名为 TESTA（时空Token汇集）的有效方法，通过适应地聚合相似帧和帧内相似区域来缩短视频semantics。</li>
<li>results: TESTA可以减少视频Token的数量，提高视频编码的效率，并且在五个数据集上进行了段落到视频检索和长形视频问答任务的实验，经验表明，TESTA可以提高计算效率1.7倍，并且可以充分利用更长的输入帧，例如+13.7 R@1 on QuerYD和+6.5 R@1 on Condensed Movie。<details>
<summary>Abstract</summary>
Large-scale video-language pre-training has made remarkable strides in advancing video-language understanding tasks. However, the heavy computational burden of video encoding remains a formidable efficiency bottleneck, particularly for long-form videos. These videos contain massive visual tokens due to their inherent 3D properties and spatiotemporal redundancy, making it challenging to capture complex temporal and spatial relationships. To tackle this issue, we propose an efficient method called TEmporal-Spatial Token Aggregation (TESTA). TESTA condenses video semantics by adaptively aggregating similar frames, as well as similar patches within each frame. TESTA can reduce the number of visual tokens by 75% and thus accelerate video encoding. Building upon TESTA, we introduce a pre-trained video-language model equipped with a divided space-time token aggregation module in each video encoder block. We evaluate our model on five datasets for paragraph-to-video retrieval and long-form VideoQA tasks. Experimental results show that TESTA improves computing efficiency by 1.7 times, and achieves significant performance gains from its scalability in processing longer input frames, e.g., +13.7 R@1 on QuerYD and +6.5 R@1 on Condensed Movie.
</details>
<details>
<summary>摘要</summary>
大规模视频语言预训练已经取得了关键视频语言理解任务的显著进步。然而，视频编码的计算沉重仍然是效率瓶颈，特别是长形视频。这些视频具有自然的3D特性和空间时间重复，使得捕捉复杂的时间和空间关系变得困难。为解决这个问题，我们提出了高效的方法called TESTA。TESTA通过适应地聚合相似帧和帧内相似的小块来缩短视 semantics。TESTA可以减少视觉token数量，从而加速视频编码。基于TESTA，我们介绍了一个带有分开的空间时间token聚合模块的预训练视频语言模型。我们在五个数据集上进行了对 paragraph-to-video retrieval和长形 VideoQA任务的测试。实验结果显示，TESTA可以提高计算效率，并且在处理 longer input frames 时 achieved significant performance gains，比如QuerYD上的+13.7 R@1和Condensed Movie上的+6.5 R@1。
</details></li>
</ul>
<hr>
<h2 id="A-Unique-Training-Strategy-to-Enhance-Language-Models-Capabilities-for-Health-Mention-Detection-from-Social-Media-Content"><a href="#A-Unique-Training-Strategy-to-Enhance-Language-Models-Capabilities-for-Health-Mention-Detection-from-Social-Media-Content" class="headerlink" title="A Unique Training Strategy to Enhance Language Models Capabilities for Health Mention Detection from Social Media Content"></a>A Unique Training Strategy to Enhance Language Models Capabilities for Health Mention Detection from Social Media Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19057">http://arxiv.org/abs/2310.19057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pervaiz Iqbal Khan, Muhammad Nabeel Asim, Andreas Dengel, Sheraz Ahmed</li>
<li>for: 提取社交媒体上的健康相关内容，用于疾病传播、评估药物对疾病的影响等应用。</li>
<li>methods: 采用随机权重扰动和对比学习策略来训练语言模型，以便从社交媒体文本中提取通用模式。</li>
<li>results: 提出一种基于多种语言模型的元预测器，可以将社交媒体文本分类为非健康和健康相关两类，并在三个公共评测数据集上实现了3.87%的F1分数提升和超过现有健康提及分类预测器的性能。<details>
<summary>Abstract</summary>
An ever-increasing amount of social media content requires advanced AI-based computer programs capable of extracting useful information. Specifically, the extraction of health-related content from social media is useful for the development of diverse types of applications including disease spread, mortality rate prediction, and finding the impact of diverse types of drugs on diverse types of diseases. Language models are competent in extracting the syntactic and semantics of text. However, they face a hard time extracting similar patterns from social media texts. The primary reason for this shortfall lies in the non-standardized writing style commonly employed by social media users. Following the need for an optimal language model competent in extracting useful patterns from social media text, the key goal of this paper is to train language models in such a way that they learn to derive generalized patterns. The key goal is achieved through the incorporation of random weighted perturbation and contrastive learning strategies. On top of a unique training strategy, a meta predictor is proposed that reaps the benefits of 5 different language models for discriminating posts of social media text into non-health and health-related classes. Comprehensive experimentation across 3 public benchmark datasets reveals that the proposed training strategy improves the performance of the language models up to 3.87%, in terms of F1-score, as compared to their performance with traditional training. Furthermore, the proposed meta predictor outperforms existing health mention classification predictors across all 3 benchmark datasets.
</details>
<details>
<summary>摘要</summary>
随着社交媒体内容的不断增加，需要更高级的人工智能计算机程序来提取有用信息。具体来说，从社交媒体中提取健康相关内容非常有用，可以用于生病传播、死亡率预测和不同类型的药物对不同类型疾病的影响等多种应用。语言模型可以提取文本的语法和 semantics，但它们在社交媒体文本上遇到困难。主要的原因在于社交媒体用户通常采用不标准的写作风格。为了解决这个问题，本文的关键目标是使语言模型学习泛化模式。这个目标通过随机权重扰动和对比学习策略来实现。此外，我们还提出了一种基于5种语言模型的元预测器，可以对社交媒体文本分类为非健康和健康相关类别。通过对3个公共 benchmark 数据集进行广泛的实验，我们发现，我们的训练策略可以提高语言模型的性能，相比传统训练策略，提高F1-score的表现达3.87%。此外，我们的元预测器还可以在所有3个 benchmark 数据集上超过现有的健康提及分类预测器。
</details></li>
</ul>
<hr>
<h2 id="MILL-Mutual-Verification-with-Large-Language-Models-for-Zero-Shot-Query-Expansion"><a href="#MILL-Mutual-Verification-with-Large-Language-Models-for-Zero-Shot-Query-Expansion" class="headerlink" title="MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion"></a>MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19056">http://arxiv.org/abs/2310.19056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengyue Jia, Yiding Liu, Xiangyu Zhao, Xiaopeng Li, Changying Hao, Shuaiqiang Wang, Dawei Yin</li>
<li>for: 提高搜寻系统的查询扩展功能，以更好地反映用户的资讯需求。</li>
<li>methods: 使用大语言模型（LLM）基础的共识验证框架，从多个角度生成来扩展查询。</li>
<li>results: 在三个资讯搜寻 dataset 上进行了广泛的实验，与其他基eline相比，提高了查询扩展的性能。<details>
<summary>Abstract</summary>
Query expansion is a commonly-used technique in many search systems to better represent users' information needs with additional query terms. Existing studies for this task usually propose to expand a query with retrieved or generated contextual documents. However, both types of methods have clear limitations. For retrieval-based methods, the documents retrieved with the original query might not be accurate enough to reveal the search intent, especially when the query is brief or ambiguous. For generation-based methods, existing models can hardly be trained or aligned on a particular corpus, due to the lack of corpus-specific labeled data. In this paper, we propose a novel Large Language Model (LLM) based mutual verification framework for query expansion, which alleviates the aforementioned limitations. Specifically, we first design a query-query-document generation pipeline, which can effectively leverage the contextual knowledge encoded in LLMs to generate sub-queries and corresponding documents from multiple perspectives. Next, we employ a mutual verification method for both generated and retrieved contextual documents, where 1) retrieved documents are filtered with the external contextual knowledge in generated documents, and 2) generated documents are filtered with the corpus-specific knowledge in retrieved documents. Overall, the proposed method allows retrieved and generated documents to complement each other to finalize a better query expansion. We conduct extensive experiments on three information retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO. The results demonstrate that our method outperforms other baselines significantly.
</details>
<details>
<summary>摘要</summary>
很多搜索系统中使用的查询扩展技术可以更好地表达用户的信息需求。现有的研究通常是使用已经retsieved或生成的文档来扩展查询。然而，这两种方法都有明显的局限性。对于retsieval-based方法来说，用于扩展查询的文档可能并不准确地反映搜索意图，特别是当查询语句简短或 ambiguous 时。对于生成-based方法来说，现有的模型很难在特定的文献上进行训练或对Alignment，因为缺乏带有标注数据的文献特有的训练数据。在本文中，我们提出了一种基于大型自然语言模型（LLM）的 queries 的共同验证框架，以解决以上所述的局限性。具体来说，我们首先设计了一个查询-查询-文档生成管道，可以借助LLM中的上下文知识来生成多个角度的子查询和相应的文档。接着，我们使用一种互verify方法，其中1）retsieved的文档被 Filter 出外部上下文知识生成的文档中，2）生成的文档被 Filter 出特定文献中的训练数据。总的来说，我们的方法可以让retsieved和生成的文档互相补做，以实现更好的查询扩展。我们在TREC-DL-2020、TREC-COVID和MSMARCO三个信息检索数据集上进行了广泛的实验，结果表明我们的方法与其他基准方法相比显著有优势。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Emotional-Landscape-of-Music-An-Analysis-of-Valence-Trends-and-Genre-Variations-in-Spotify-Music-Data"><a href="#Exploring-the-Emotional-Landscape-of-Music-An-Analysis-of-Valence-Trends-and-Genre-Variations-in-Spotify-Music-Data" class="headerlink" title="Exploring the Emotional Landscape of Music: An Analysis of Valence Trends and Genre Variations in Spotify Music Data"></a>Exploring the Emotional Landscape of Music: An Analysis of Valence Trends and Genre Variations in Spotify Music Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19052">http://arxiv.org/abs/2310.19052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shruti Dutta, Shashwat Mookherjee</li>
<li>for: 这个论文通过分析Spotify音乐数据，探讨音乐情感和趋势，包括音频特征和投票得分，以揭示音乐情感关系的Patterns。</li>
<li>methods: 该研究使用回归模型、时间分析、情绪过渡和分类调查等方法，以预测投票得分。</li>
<li>results: 研究发现了音乐情感关系的模式，包括时间的变化和情绪的过渡。这些发现有助于深入理解音乐和情感之间的关系，并提供了长期的音乐情感探索。<details>
<summary>Abstract</summary>
This paper conducts an intricate analysis of musical emotions and trends using Spotify music data, encompassing audio features and valence scores extracted through the Spotipi API. Employing regression modeling, temporal analysis, mood transitions, and genre investigation, the study uncovers patterns within music-emotion relationships. Regression models linear, support vector, random forest, and ridge, are employed to predict valence scores. Temporal analysis reveals shifts in valence distribution over time, while mood transition exploration illuminates emotional dynamics within playlists. The research contributes to nuanced insights into music's emotional fabric, enhancing comprehension of the interplay between music and emotions through years.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "Mandarin" Chinese.Translation Notes:* "valence scores" is translated as "积分" (jīpǐn), which is a term commonly used in Chinese to refer to the emotional content of music.* "regression modeling" is translated as "回归分析" (huíjì fāngxì), which is a term commonly used in Chinese to refer to statistical modeling techniques used to predict continuous outcomes.* "temporal analysis" is translated as "时间分析" (shíjiàn fāngxì), which is a term commonly used in Chinese to refer to the analysis of data over time.* "mood transitions" is translated as "情绪转移" (qíngxù zhōngmǐ), which is a term commonly used in Chinese to refer to the changes in emotional states within a piece of music.* "genre investigation" is translated as "类型调查" (lèitype jiàozhè), which is a term commonly used in Chinese to refer to the examination of different styles or genres of music.
</details></li>
</ul>
<hr>
<h2 id="TeacherLM-Teaching-to-Fish-Rather-Than-Giving-the-Fish-Language-Modeling-Likewise"><a href="#TeacherLM-Teaching-to-Fish-Rather-Than-Giving-the-Fish-Language-Modeling-Likewise" class="headerlink" title="TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise"></a>TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19019">http://arxiv.org/abs/2310.19019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan He, Hanyu Lai, Chenyang Zhao, Zirui Cheng, Junting Pan, Ruoyu Qin, Ruofan Lu, Rui Lu, Yunchen Zhang, Gangming Zhao, Zhaohui Hou, Zhiyuan Huang, Shaoqing Lu, Ding Liang, Mingjie Zhan</li>
<li>for: 这项研究旨在提出一种能够对多种自然语言处理任务进行准确的推理和数据增强的小型语言模型（TeacherLM-7.1B）。</li>
<li>methods: 该模型使用了对重要基础知识、链式思维和常见错误的注释，以便其他模型可以学习“为什么”而不仅仅是“什么”。</li>
<li>results: 根据实验结果，TeacherLM-7.1B模型在MMLU测试中获得了零shot得分52.3，超过了大多数超过100亿参数的模型。此外，基于TeacherLM-7.1B模型，我们对58个NLP数据集进行了数据增强，并在多任务 Setting中教育了不同参数的OPT和BLOOM系列学生模型。实验结果表明，TeacherLM的数据增强对学生模型带来了显著的改进。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn "why" instead of just "what". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）在不同的自然语言处理任务中表现出了吸引人的推理和数据增强能力。然而，小型模型呢？在这项工作中，我们提出了TeacherLM-7.1B模型，可以对大多数NLU样本进行相关基础知识、链条思维和常见错误的标注，使得其他模型可以学习“为什么”而不仅仅是“什么”，从而提高NLU模型的泛化能力。TeacherLM-7.1B模型在MMLU上取得了零基eline得分52.3，超过了大多数超过100亿参数的模型。此外，TeacherLM还具有出色的数据增强能力。基于TeacherLM，我们对58个NLU数据集进行了增强，并使用不同的OPT和BLOOM系列模型在多任务 Setting中进行了启发。实验结果表明，TeacherLM提供的数据增强对NLU模型的表现带来了显著的改善。我们将发布TeacherLM系列模型和增强数据集作为开源。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-Prototypical-Knowledge-for-Weakly-Open-Vocabulary-Semantic-Segmentation"><a href="#Uncovering-Prototypical-Knowledge-for-Weakly-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Uncovering Prototypical Knowledge for Weakly Open-Vocabulary Semantic Segmentation"></a>Uncovering Prototypical Knowledge for Weakly Open-Vocabulary Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19001">http://arxiv.org/abs/2310.19001</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ferenas/PGSeg">https://github.com/Ferenas/PGSeg</a></li>
<li>paper_authors: Fei Zhang, Tianfei Zhou, Boyang Li, Hao He, Chaofan Ma, Tianjiao Zhang, Jiangchao Yao, Ya Zhang, Yanfeng Wang</li>
<li>for: 本研究探讨了弱类开放词汇 semantic segmentation（WOVSS）问题，即通过仅使用图像和文本对进行学习， segmenting objects of arbitrary classes。</li>
<li>methods: existings works 增强 vanilla vision transformer 的方法，通过引入显式分组识别，例如使用多个组token&#x2F;中心来分组图像 токен并进行组级文本对齐。</li>
<li>results: 我们的提议方法可以减少对group token的粒度不一致，并且可以在不同的批处理级别进行多模态规范化，从而提高分区能力和精度。实验结果显示，我们的提议方法可以在多个 benchmark 数据集上达到状态 искусственный智能的性能。<details>
<summary>Abstract</summary>
This paper studies the problem of weakly open-vocabulary semantic segmentation (WOVSS), which learns to segment objects of arbitrary classes using mere image-text pairs. Existing works turn to enhance the vanilla vision transformer by introducing explicit grouping recognition, i.e., employing several group tokens/centroids to cluster the image tokens and perform the group-text alignment. Nevertheless, these methods suffer from a granularity inconsistency regarding the usage of group tokens, which are aligned in the all-to-one v.s. one-to-one manners during the training and inference phases, respectively. We argue that this discrepancy arises from the lack of elaborate supervision for each group token. To bridge this granularity gap, this paper explores explicit supervision for the group tokens from the prototypical knowledge. To this end, this paper proposes the non-learnable prototypical regularization (NPR) where non-learnable prototypes are estimated from source features to serve as supervision and enable contrastive matching of the group tokens. This regularization encourages the group tokens to segment objects with less redundancy and capture more comprehensive semantic regions, leading to increased compactness and richness. Based on NPR, we propose the prototypical guidance segmentation network (PGSeg) that incorporates multi-modal regularization by leveraging prototypical sources from both images and texts at different levels, progressively enhancing the segmentation capability with diverse prototypical patterns. Experimental results show that our proposed method achieves state-of-the-art performance on several benchmark datasets. The source code is available at https://github.com/Ferenas/PGSeg.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="AMIR-Automated-MisInformation-Rebuttal-–-A-COVID-19-Vaccination-Datasets-based-Recommendation-System"><a href="#AMIR-Automated-MisInformation-Rebuttal-–-A-COVID-19-Vaccination-Datasets-based-Recommendation-System" class="headerlink" title="AMIR: Automated MisInformation Rebuttal – A COVID-19 Vaccination Datasets based Recommendation System"></a>AMIR: Automated MisInformation Rebuttal – A COVID-19 Vaccination Datasets based Recommendation System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19834">http://arxiv.org/abs/2310.19834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shakshi Sharma, Anwitaman Datta, Rajesh Sharma</li>
<li>for: 本研究旨在开发一种可靠、可扩展的自动回击谣言工具，以抗谣言的扩散和恶性影响。</li>
<li>methods: 本研究使用现有社交媒体上的信息和更加权威的 фа核数据库，自动生成对谣言的回击。</li>
<li>results: 研究表明，通过使用这种方法，可以快速、高效地对谣言进行回击，并且可以扩展到其他社交媒体平台和谣言类型。<details>
<summary>Abstract</summary>
Misinformation has emerged as a major societal threat in recent years in general; specifically in the context of the COVID-19 pandemic, it has wrecked havoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable solutions for combating misinformation are the need of the hour. This work explored how existing information obtained from social media and augmented with more curated fact checked data repositories can be harnessed to facilitate automated rebuttal of misinformation at scale. While the ideas herein can be generalized and reapplied in the broader context of misinformation mitigation using a multitude of information sources and catering to the spectrum of social media platforms, this work serves as a proof of concept, and as such, it is confined in its scope to only rebuttal of tweets, and in the specific context of misinformation regarding COVID-19. It leverages two publicly available datasets, viz. FaCov (fact-checked articles) and misleading (social media Twitter) data on COVID-19 Vaccination.
</details>
<details>
<summary>摘要</summary>
“误情传播已经成为现代社会的主要问题，尤其是在 COVID-19 大流行期间。这种误情传播可能会导致疫苗抵触，例如通过传播不实信息。为了解决这个问题，我们需要一些可靠且可扩展的解决方案。这个研究探索了如何使用社交媒体上的现有信息，加上更加精心的实验 checked 数据库，来自动反驳误情传播。这个研究的想法可以应用于更 широ的误情传播问题，使用多种信息来源和覆盖多个社交媒体平台。这个研究作为证明，仅对于 Twitter 上的反驳误情传播进行了评估，并且仅在 COVID-19 疫苗接种方面进行了评估。它使用了两个公共可用的数据集，namely，FaCov（实验 checked 文章）和 misleading（社交媒体 Twitter）数据集。”Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Bipartite-Graph-Pre-training-for-Unsupervised-Extractive-Summarization-with-Graph-Convolutional-Auto-Encoders"><a href="#Bipartite-Graph-Pre-training-for-Unsupervised-Extractive-Summarization-with-Graph-Convolutional-Auto-Encoders" class="headerlink" title="Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders"></a>Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18992">http://arxiv.org/abs/2310.18992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianren Mao, Shaobo Zhao, Jiarui Li, Xiaolei Gu, Shizhu He, Bo Li, Jianxin Li</li>
<li>for: 用于自动生成文摘</li>
<li>methods: 使用特定设计的 sentence embedding 模型，以优化句子特征和文档凝聚特征</li>
<li>results: 提供了高效的自动文摘方法，超越了使用BERT或RoBERTa的句子表示Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本文使用自动生成文摘技术，以提高文摘的效率和质量。</li>
<li>methods: 我们提出了一种使用 sentence embedding 模型来优化句子特征和文档凝聚特征，从而提供高效的自动文摘方法。</li>
<li>results: 我们的方法在下游任务中表现出色，超越了使用BERT或RoBERTa的句子表示。<details>
<summary>Abstract</summary>
Pre-trained sentence representations are crucial for identifying significant sentences in unsupervised document extractive summarization. However, the traditional two-step paradigm of pre-training and sentence-ranking, creates a gap due to differing optimization objectives. To address this issue, we argue that utilizing pre-trained embeddings derived from a process specifically designed to optimize cohensive and distinctive sentence representations helps rank significant sentences. To do so, we propose a novel graph pre-training auto-encoder to obtain sentence embeddings by explicitly modelling intra-sentential distinctive features and inter-sentential cohesive features through sentence-word bipartite graphs. These pre-trained sentence representations are then utilized in a graph-based ranking algorithm for unsupervised summarization. Our method produces predominant performance for unsupervised summarization frameworks by providing summary-worthy sentence representations. It surpasses heavy BERT- or RoBERTa-based sentence representations in downstream tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="NP-SBFL-Bridging-the-Gap-Between-Spectrum-Based-Fault-Localization-and-Faulty-Neural-Pathways-Diagnosis"><a href="#NP-SBFL-Bridging-the-Gap-Between-Spectrum-Based-Fault-Localization-and-Faulty-Neural-Pathways-Diagnosis" class="headerlink" title="NP-SBFL: Bridging the Gap Between Spectrum-Based Fault Localization and Faulty Neural Pathways Diagnosis"></a>NP-SBFL: Bridging the Gap Between Spectrum-Based Fault Localization and Faulty Neural Pathways Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18987">http://arxiv.org/abs/2310.18987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soroush Hashemifar, Saeed Parsa, Akram Kalaee</li>
<li>for: 这篇论文旨在提出一种基于NP-SBFL方法的深度学习网络FAULT LOCALIZATION方法，以便更好地找到深度学习网络中的FAULTY PATH。</li>
<li>methods: 该方法使用层间相关传播（LRP）技术确定关键神经元，并使用多个阶段加速（MGA）来有效地激活一个序列中的神经元，以保持之前神经元的激活。</li>
<li>results: 对于MNIST和CIFAR-10两个常用的数据集，以及三种异常神经元度量 Tarantula、Ochiai 和 Barinel，我们的方法比基elines更有效地识别异常路径和生成攻击输入。特别是在 Tarantula 上，NP-SBFL-MGA 的异常检测率达到 96.75%，超过 DeepFault 在 Ochiai 上的 89.90% 和 NP-SBFL-GA 在 Ochiai 上的 60.61%。<details>
<summary>Abstract</summary>
Deep learning has revolutionized various real-world applications, but the quality of Deep Neural Networks (DNNs) remains a concern. DNNs are complex and have millions of parameters, making it difficult to determine their contributions to fulfilling a task. Moreover, the behavior of a DNN is highly influenced by the data used during training, making it challenging to collect enough data to exercise all potential DNN behavior under all possible scenarios. This paper proposes a novel NP-SBFL method that adapts spectrum-based fault localization (SBFL) to locate faulty neural pathways. Our method identifies critical neurons using the layer-wise relevance propagation (LRP) technique and determines which critical neurons are faulty. We propose a multi-stage gradient ascent (MGA), an extension of gradient ascent, to effectively activate a sequence of neurons one at a time while maintaining the activation of previous neurons. We evaluated the effectiveness of our method on two commonly used datasets, MNIST and CIFAR-10, two baselines DeepFault and NP-SBFL-GA, and three suspicious neuron measures, Tarantula, Ochiai, and Barinel. The empirical results showed that NP-SBFL-MGA is statistically more effective than the baselines at identifying suspicious paths and synthesizing adversarial inputs. Particularly, Tarantula on NP-SBFL-MGA had the highest fault detection rate at 96.75%, surpassing DeepFault on Ochiai (89.90%) and NP-SBFL-GA on Ochiai (60.61%). Our approach also yielded comparable results to the baselines in synthesizing naturalness inputs, and we found a positive correlation between the coverage of critical paths and the number of failed tests in DNN fault localization.
</details>
<details>
<summary>摘要</summary>
深度学习已经革命化了各种实际应用，但是深度神经网络（DNN）的质量仍然是一大问题。DNN具有很多参数，因此很难确定它们在完成任务时的贡献。此外，DNN的行为受到训练数据的影响，因此收集足够的数据来覆盖所有可能的scenario是很困难的。这篇论文提出了一种基于spectrum-based fault localization（SBFL）的新方法，用于 locate faulty neural pathways。我们的方法使用层 wise relevance propagation（LRP）技术来确定关键神经元，并使用多Stage gradient ascent（MGA）来有效地激活一系列神经元，而不会产生前一个神经元的激活失效。我们对MNIST和CIFAR-10两个常用的数据集进行了评估，与DeepFault和NP-SBFL-GA两个基eline进行了比较，以及三种异常神经元度量 Tarantula、Ochiai和Barinel。实际结果表明，NP-SBFL-MGA在 Identifying suspicious paths和生成攻击输入方面具有 statistically higher effectiveness than baselines。特别是在 Tarantula上，NP-SBFL-MGA的异常检测率为 96.75%，超过 DeepFault on Ochiai（89.90%）和 NP-SBFL-GA on Ochiai（60.61%）。我们的方法还在生成自然输入方面得到了相似的结果，并发现了关键路径覆盖率和失败测试数量之间的正相关关系。
</details></li>
</ul>
<hr>
<h2 id="DCQA-Document-Level-Chart-Question-Answering-towards-Complex-Reasoning-and-Common-Sense-Understanding"><a href="#DCQA-Document-Level-Chart-Question-Answering-towards-Complex-Reasoning-and-Common-Sense-Understanding" class="headerlink" title="DCQA: Document-Level Chart Question Answering towards Complex Reasoning and Common-Sense Understanding"></a>DCQA: Document-Level Chart Question Answering towards Complex Reasoning and Common-Sense Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18983">http://arxiv.org/abs/2310.18983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AnranWu-RichPo/DCQA">https://github.com/AnranWu-RichPo/DCQA</a></li>
<li>paper_authors: Anran Wu, Luwei Xiao, Xingjiao Wu, Shuwen Yang, Junjie Xu, Zisong Zhuang, Nian Xie, Cheng Jin, Liang He</li>
<li>for: 这篇论文是为了解决文档级图表问答问题（DCQA）而写的。</li>
<li>methods: 这篇论文使用了文档格式分析（DLA）和图表问答（CQA）技术来解决DCQA问题。</li>
<li>results: 论文提出了一个新的DCQA数据集，包含6种不同的图表样式和699,051个需要高度理解和常识能力的问题。此外，论文还提出了一种使用表格数据、颜色集和基本问题模板生成大量理智问答对的问题生成引擎。<details>
<summary>Abstract</summary>
Visually-situated languages such as charts and plots are omnipresent in real-world documents. These graphical depictions are human-readable and are often analyzed in visually-rich documents to address a variety of questions that necessitate complex reasoning and common-sense responses. Despite the growing number of datasets that aim to answer questions over charts, most only address this task in isolation, without considering the broader context of document-level question answering. Moreover, such datasets lack adequate common-sense reasoning information in their questions. In this work, we introduce a novel task named document-level chart question answering (DCQA). The goal of this task is to conduct document-level question answering, extracting charts or plots in the document via document layout analysis (DLA) first and subsequently performing chart question answering (CQA). The newly developed benchmark dataset comprises 50,010 synthetic documents integrating charts in a wide range of styles (6 styles in contrast to 3 for PlotQA and ChartQA) and includes 699,051 questions that demand a high degree of reasoning ability and common-sense understanding. Besides, we present the development of a potent question-answer generation engine that employs table data, a rich color set, and basic question templates to produce a vast array of reasoning question-answer pairs automatically. Based on DCQA, we devise an OCR-free transformer for document-level chart-oriented understanding, capable of DLA and answering complex reasoning and common-sense questions over charts in an OCR-free manner. Our DCQA dataset is expected to foster research on understanding visualizations in documents, especially for scenarios that require complex reasoning for charts in the visually-rich document. We implement and evaluate a set of baselines, and our proposed method achieves comparable results.
</details>
<details>
<summary>摘要</summary>
文本中的可见语言如图表和折衣图是现实生活中文档中 ubique 存在的。这些图形展示是人类可读的，并在文档中常常用于回答复杂的问题，需要复杂的理解和常识。Despite the growing number of datasets that aim to answer questions over charts, most only address this task in isolation, without considering the broader context of document-level question answering. Moreover, such datasets lack adequate common-sense reasoning information in their questions. In this work, we introduce a novel task named 文档级图表问题回答 (DCQA). The goal of this task is to conduct document-level question answering, extracting charts or plots in the document via 文档布局分析 (DLA) first and subsequently performing 图表问题回答 (CQA). The newly developed benchmark dataset comprises 50,010 synthetic documents integrating charts in a wide range of styles (6 styles in contrast to 3 for PlotQA and ChartQA) and includes 699,051 questions that demand a high degree of reasoning ability and common-sense understanding. Besides, we present the development of a potent question-answer generation engine that employs table data, a rich color set, and basic question templates to produce a vast array of reasoning question-answer pairs automatically. Based on DCQA, we devise an OCR-free transformer for document-level chart-oriented understanding, capable of DLA and answering complex reasoning and common-sense questions over charts in an OCR-free manner. Our DCQA dataset is expected to foster research on understanding visualizations in documents, especially for scenarios that require complex reasoning for charts in the visually-rich document. We implement and evaluate a set of baselines, and our proposed method achieves comparable results.
</details></li>
</ul>
<hr>
<h2 id="EtiCor-Corpus-for-Analyzing-LLMs-for-Etiquettes"><a href="#EtiCor-Corpus-for-Analyzing-LLMs-for-Etiquettes" class="headerlink" title="EtiCor: Corpus for Analyzing LLMs for Etiquettes"></a>EtiCor: Corpus for Analyzing LLMs for Etiquettes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18974">http://arxiv.org/abs/2310.18974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashutosh Dwivedi, Pradhyumna Lavania, Ashutosh Modi</li>
<li>for: 本文提出了一个etiquette corpus，用于评估LLMs在不同地区社会规范方面的知识和理解能力。</li>
<li>methods: 本文使用了现有的LLMs（Delphi、Falcon40B、GPT-3.5）进行实验，以评估它们在不同地区社会规范方面的性能。</li>
<li>results: 初步结果表明，LLMs在非西方世界的社会规范方面往往无法理解和遵循当地的习俗。<details>
<summary>Abstract</summary>
Etiquettes are an essential ingredient of day-to-day interactions among people. Moreover, etiquettes are region-specific, and etiquettes in one region might contradict those in other regions. In this paper, we propose EtiCor, an Etiquettes Corpus, having texts about social norms from five different regions across the globe. The corpus provides a test bed for evaluating LLMs for knowledge and understanding of region-specific etiquettes. Additionally, we propose the task of Etiquette Sensitivity. We experiment with state-of-the-art LLMs (Delphi, Falcon40B, and GPT-3.5). Initial results indicate that LLMs, mostly fail to understand etiquettes from regions from non-Western world.
</details>
<details>
<summary>摘要</summary>
礼仪是日常人际交流中的重要组成部分。同时，礼仪在不同地区之间可能存在差异，一地的礼仪可能与另一地的礼仪矛盾。在这篇论文中，我们提出了“礼仪库”（EtiCor），包含来自五个不同地区的社会规范文本。该库提供了对LRMs（语言模型）的评估和理解地区特有的礼仪知识的测试平台。此外，我们还提出了“礼仪敏感度”的任务。我们对当今顶尖LRMs（Delphi、Falcon40B和GPT-3.5）进行了实验，初步结果显示，LRMs在非西方世界地区的礼仪知识方面表现不佳。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Vision-Transformers-for-Image-Classification-in-Class-Embedding-Space"><a href="#Analyzing-Vision-Transformers-for-Image-Classification-in-Class-Embedding-Space" class="headerlink" title="Analyzing Vision Transformers for Image Classification in Class Embedding Space"></a>Analyzing Vision Transformers for Image Classification in Class Embedding Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18969">http://arxiv.org/abs/2310.18969</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/martinagvilas/vit-cls_emb">https://github.com/martinagvilas/vit-cls_emb</a></li>
<li>paper_authors: Martina G. Vilas, Timothy Schaumlöffel, Gemma Roig</li>
<li>for: 本研究用于解释视觉转换器模型如何用于图像分类任务。</li>
<li>methods: 该研究使用了一种基于前期研究的方法，通过将内部表示映射到学习的类嵌入空间，以解释这些网络如何建立图像分类预测的 categorical 表示。</li>
<li>results: 研究发现，图像块在层次结构中发展出了类型特定的表示，这与注意力机制和上下文信息有关。此外，该方法还可以用来确定图像中关键的部分，并且与传统的直接探测方法相比，具有显著的优势。<details>
<summary>Abstract</summary>
Despite the growing use of transformer models in computer vision, a mechanistic understanding of these networks is still needed. This work introduces a method to reverse-engineer Vision Transformers trained to solve image classification tasks. Inspired by previous research in NLP, we demonstrate how the inner representations at any level of the hierarchy can be projected onto the learned class embedding space to uncover how these networks build categorical representations for their predictions. We use our framework to show how image tokens develop class-specific representations that depend on attention mechanisms and contextual information, and give insights on how self-attention and MLP layers differentially contribute to this categorical composition. We additionally demonstrate that this method (1) can be used to determine the parts of an image that would be important for detecting the class of interest, and (2) exhibits significant advantages over traditional linear probing approaches. Taken together, our results position our proposed framework as a powerful tool for mechanistic interpretability and explainability research.
</details>
<details>
<summary>摘要</summary>
尽管变换器模型在计算机视觉领域的使用正在增长，但是我们仍需要更好地理解这些网络。这项工作提出了一种方法，用于反向工程视Transformers在图像分类任务上训练过后的内部表示。 Drawing inspiration from previous research in NLP, we demonstrate how the inner representations at any level of the hierarchy can be projected onto the learned class embedding space to uncover how these networks build categorical representations for their predictions. We use our framework to show how image tokens develop class-specific representations that depend on attention mechanisms and contextual information, and give insights on how self-attention and MLP layers differentially contribute to this categorical composition. We additionally demonstrate that this method (1) can be used to determine the parts of an image that would be important for detecting the class of interest, and (2) exhibits significant advantages over traditional linear probing approaches. Taken together, our results position our proposed framework as a powerful tool for mechanistic interpretability and explainability research.
</details></li>
</ul>
<hr>
<h2 id="Spacecraft-Autonomous-Decision-Planning-for-Collision-Avoidance-a-Reinforcement-Learning-Approach"><a href="#Spacecraft-Autonomous-Decision-Planning-for-Collision-Avoidance-a-Reinforcement-Learning-Approach" class="headerlink" title="Spacecraft Autonomous Decision-Planning for Collision Avoidance: a Reinforcement Learning Approach"></a>Spacecraft Autonomous Decision-Planning for Collision Avoidance: a Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18966">http://arxiv.org/abs/2310.18966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Bourriez, Adrien Loizeau, Adam F. Abdin<br>for:The paper is written for the purpose of proposing an implementation of autonomous collision avoidance decision-making capabilities on spacecraft using reinforcement learning techniques.methods:The proposed methodology is based on a partially observable Markov decision process (POMDP) framework, which considers epistemic and aleatory uncertainties and allows the AI system on board the spacecraft to learn stochastic policies for accurate collision avoidance maneuvers.results:The objective of the paper is to successfully delegate the decision-making process for autonomously implementing a collision avoidance maneuver to the spacecraft without human intervention, allowing for a faster response in the decision-making process and highly decentralized operations.<details>
<summary>Abstract</summary>
The space environment around the Earth is becoming increasingly populated by both active spacecraft and space debris. To avoid potential collision events, significant improvements in Space Situational Awareness (SSA) activities and Collision Avoidance (CA) technologies are allowing the tracking and maneuvering of spacecraft with increasing accuracy and reliability. However, these procedures still largely involve a high level of human intervention to make the necessary decisions. For an increasingly complex space environment, this decision-making strategy is not likely to be sustainable. Therefore, it is important to successfully introduce higher levels of automation for key Space Traffic Management (STM) processes to ensure the level of reliability needed for navigating a large number of spacecraft. These processes range from collision risk detection to the identification of the appropriate action to take and the execution of avoidance maneuvers. This work proposes an implementation of autonomous CA decision-making capabilities on spacecraft based on Reinforcement Learning (RL) techniques. A novel methodology based on a Partially Observable Markov Decision Process (POMDP) framework is developed to train the Artificial Intelligence (AI) system on board the spacecraft, considering epistemic and aleatory uncertainties. The proposed framework considers imperfect monitoring information about the status of the debris in orbit and allows the AI system to effectively learn stochastic policies to perform accurate Collision Avoidance Maneuvers (CAMs). The objective is to successfully delegate the decision-making process for autonomously implementing a CAM to the spacecraft without human intervention. This approach would allow for a faster response in the decision-making process and for highly decentralized operations.
</details>
<details>
<summary>摘要</summary>
地球附近的空间环境正在不断增加活跃的空间craft和空间垃圾的数量，为了避免 potential collision event，空间 situational awareness (SSA) 活动和 collision avoidance (CA) 技术得到了重要改进，可以准确地跟踪和 manipulate 空间craft。然而，这些过程仍然需要人类参与，以便做出必要的决策。随着空间环境的增加复杂度，这种决策策略可能不可持续。因此，需要成功地把Space Traffic Management (STM) 过程中的关键步骤自动化，以确保在大量空间craft navigating 时的可靠性。这些过程包括风险检测和避免措施的识别以及执行避免措施。本工作提出了基于 reinforcement learning (RL) 技术的自动化 CA 决策能力的实现。一种基于 partially observable Markov decision process (POMDP) 框架的新方法被开发，用于在空间craft上训练人工智能 (AI) 系统，考虑到了 epistemic 和 aleatory 不确定性。该方法考虑了在轨道上监测空间垃圾的状况不准确的情况，并允许 AI 系统学习 Stochastic policies 以实现准确的避免措施。目标是成功地委托空间craft上的 AI 系统自动实施避免措施，无需人类参与。这种方法可以提供更快的决策过程，并允许高度分布式的操作。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Autoregressive-Retrieval-via-Bootstrapping-for-Smart-Reply-Systems"><a href="#End-to-End-Autoregressive-Retrieval-via-Bootstrapping-for-Smart-Reply-Systems" class="headerlink" title="End-to-End Autoregressive Retrieval via Bootstrapping for Smart Reply Systems"></a>End-to-End Autoregressive Retrieval via Bootstrapping for Smart Reply Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18956">http://arxiv.org/abs/2310.18956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Towle, Ke Zhou</li>
<li>for: 这篇论文是为了提出一种新的自动回复方法，以解决现有的自动回复系统的缺陷。</li>
<li>methods: 这篇论文使用了一种束合文本对话模型，通过对一组（消息，回复集）的数据进行Bootstrap来学习自动回复任务的终端到终端的模型。</li>
<li>results: 实验结果表明，这种方法可以与一些现有的基eline方法相比，在三个数据集上表现出5.1%-17.9%的改善空间，并且在0.5%-63.1%的多样性上具有显著的改善。<details>
<summary>Abstract</summary>
Reply suggestion systems represent a staple component of many instant messaging and email systems. However, the requirement to produce sets of replies, rather than individual replies, makes the task poorly suited for out-of-the-box retrieval architectures, which only consider individual message-reply similarity. As a result, these system often rely on additional post-processing modules to diversify the outputs. However, these approaches are ultimately bottlenecked by the performance of the initial retriever, which in practice struggles to present a sufficiently diverse range of options to the downstream diversification module, leading to the suggestions being less relevant to the user. In this paper, we consider a novel approach that radically simplifies this pipeline through an autoregressive text-to-text retrieval model, that learns the smart reply task end-to-end from a dataset of (message, reply set) pairs obtained via bootstrapping. Empirical results show this method consistently outperforms a range of state-of-the-art baselines across three datasets, corresponding to a 5.1%-17.9% improvement in relevance, and a 0.5%-63.1% improvement in diversity compared to the best baseline approach. We make our code publicly available.
</details>
<details>
<summary>摘要</summary>
快件和电子邮件系统中的回复建议系统是一个基本组件。然而，需要生成多个回复而不是单个回复，使得这种任务与传统的检索架构不Compatible，后者只考虑单个消息和回复之间的相似性。因此，这些系统通常需要额外的后处理模块来增加多样性。然而，这些方法受到最初的检索器的性能的限制，导致下游多样化模块无法提供充分多样化的选项，从而导致建议相对较少 relevance。在这篇论文中，我们考虑了一种新的方法，通过自然语言模型来实现简化这个管道。我们通过对 (消息、回复集) 对的数据集进行 bootstrap 来学习端到端的文本到文本检索模型。实验结果表明，这种方法可以一直 exceed  state-of-the-art 基准方法，在三个数据集上取得了5.1%-17.9%的改善，并在多样性方面取得了0.5%-63.1%的改善。我们将代码公开发布。
</details></li>
</ul>
<hr>
<h2 id="Mask-Propagation-for-Efficient-Video-Semantic-Segmentation"><a href="#Mask-Propagation-for-Efficient-Video-Semantic-Segmentation" class="headerlink" title="Mask Propagation for Efficient Video Semantic Segmentation"></a>Mask Propagation for Efficient Video Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18954">http://arxiv.org/abs/2310.18954</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ziplab/mpvss">https://github.com/ziplab/mpvss</a></li>
<li>paper_authors: Yuetian Weng, Mingfei Han, Haoyu He, Mingjie Li, Lina Yao, Xiaojun Chang, Bohan Zhuang</li>
<li>for: 本研究目标是提出一种高效的视频 semantic segmentation（VSS）方法，以优化现有的图像semantic segmentation模型，使其能够利用视频帧之间的时间关系，并降低计算成本。</li>
<li>methods: 本方法首先使用强Query-based图像分割器在稀疏的关键帧上生成准确的二值掩码和类别预测。然后，我们设计了一个流量估计模块，使用学习的Query来生成一组相关的掩码流图，每个流图与关键帧的掩码预测相关。最后，掩码-流对被折射以生成非关键帧的掩码预测。通过重用关键帧的预测，我们缓解了对每帧的图像分割器进行资源占用的需求，从而降低计算成本。</li>
<li>results: 我们的mask propagation方法在VSPW和Cityscapes dataset上实现了SOTA的准确率和效率负担的 equilibrio。例如，我们的最佳模型（Swin-L backbone）在VSPW dataset上比SOTA的MRCFA（使用MiT-B5）高4.0%的mIoU，仅需26%的FLOPs。此外，我们的框架可以在Cityscapes验证集上减少到4倍的FLOPs，同时保持只有2%的mIoU下降。代码可以在<a target="_blank" rel="noopener" href="https://github.com/ziplab/MPVSS%E4%B8%AD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ziplab/MPVSS中获取。</a><details>
<summary>Abstract</summary>
Video Semantic Segmentation (VSS) involves assigning a semantic label to each pixel in a video sequence. Prior work in this field has demonstrated promising results by extending image semantic segmentation models to exploit temporal relationships across video frames; however, these approaches often incur significant computational costs. In this paper, we propose an efficient mask propagation framework for VSS, called MPVSS. Our approach first employs a strong query-based image segmentor on sparse key frames to generate accurate binary masks and class predictions. We then design a flow estimation module utilizing the learned queries to generate a set of segment-aware flow maps, each associated with a mask prediction from the key frame. Finally, the mask-flow pairs are warped to serve as the mask predictions for the non-key frames. By reusing predictions from key frames, we circumvent the need to process a large volume of video frames individually with resource-intensive segmentors, alleviating temporal redundancy and significantly reducing computational costs. Extensive experiments on VSPW and Cityscapes demonstrate that our mask propagation framework achieves SOTA accuracy and efficiency trade-offs. For instance, our best model with Swin-L backbone outperforms the SOTA MRCFA using MiT-B5 by 4.0% mIoU, requiring only 26% FLOPs on the VSPW dataset. Moreover, our framework reduces up to 4x FLOPs compared to the per-frame Mask2Former baseline with only up to 2% mIoU degradation on the Cityscapes validation set. Code is available at https://github.com/ziplab/MPVSS.
</details>
<details>
<summary>摘要</summary>
视频 semantic segmentation (VSS) 是将每帧视频序列中的每个像素分配 semantic 标签。先前的工作在这个领域已经实现了可观的结果，通过将图像 semantic segmentation 模型扩展到利用视频帧之间的时间关系，但这些方法经常产生巨大的计算成本。在这篇论文中，我们提出了一种高效的面 mask 传播框架，称为 MPVSS。我们的方法首先使用强大的查询基于图像 segmentor 在稀疏的关键帧上生成准确的二进制面和分类预测。然后，我们设计了学习查询的流量估计模块，使用学习的查询来生成每帧视频的流量映射，每个映射都与关键帧中的面预测相关。最后，面-流量对被折叠，以用于非关键帧的面预测。通过重用关键帧的预测，我们绕过处理大量视频帧的资源占用INTENSIVE segmentor，解决时间重复和大幅减少计算成本。广泛的实验表明，我们的面传播框架在 VSPW 和 Cityscapes 上实现了最佳的质量和效率的交换。例如，我们的最佳模型（Swin-L 背景）在 VSPW 数据集上的 mIoU 比 SOTA MRCFA（使用 MiT-B5 背景）高4.0%，而计算成本只占26% FLOPs。此外，我们的框架可以在 Cityscapes 验证集上减少至4倍的计算成本，而且只增加了2% mIoU 下降。代码可以在 GitHub 上找到：https://github.com/ziplab/MPVSS。
</details></li>
</ul>
<hr>
<h2 id="Building-a-Safer-Maritime-Environment-Through-Multi-Path-Long-Term-Vessel-Trajectory-Forecasting"><a href="#Building-a-Safer-Maritime-Environment-Through-Multi-Path-Long-Term-Vessel-Trajectory-Forecasting" class="headerlink" title="Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting"></a>Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18948">http://arxiv.org/abs/2310.18948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Spadon, Jay Kumar, Matthew Smith, Sarah Vela, Romina Gehrmann, Derek Eden, Joshua van Berkel, Amilcar Soares, Ronan Fablet, Ronald Pelot, Stan Matwin</li>
<li>for: 这篇论文旨在提高船舶途径预测精度，以便提高水上交通安全性和环境可持续性。</li>
<li>methods: 该论文使用encoder-decoder模型，并采用了bidirectional长短时间记忆网络（Bi-LSTM）进行预测。其中，利用概率特征引入了AIS数据中的潜在路径和目的地信息，以便模型可以基于这些特征进行预测。</li>
<li>results: 该模型在加拿大圣劳伦斯湾（Gulf of St. Lawrence）实现了R2分数超过98%，并且在不同的技术和特征下实现了高精度预测。此外，模型还表现出了更好的复杂决策能力和更高的准确率， average和 median预测错误分别为11km和6km。<details>
<summary>Abstract</summary>
Maritime transport is paramount to global economic growth and environmental sustainability. In this regard, the Automatic Identification System (AIS) data plays a significant role by offering real-time streaming data on vessel movement, which allows for enhanced traffic surveillance, assisting in vessel safety by avoiding vessel-to-vessel collisions and proactively preventing vessel-to-whale ones. This paper tackles an intrinsic problem to trajectory forecasting: the effective multi-path long-term vessel trajectory forecasting on engineered sequences of AIS data. We utilize an encoder-decoder model with Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1 to 3 hours of AIS data. We feed the model with probabilistic features engineered from the AIS data that refer to the potential route and destination of each trajectory so that the model, leveraging convolutional layers for spatial feature learning and a position-aware attention mechanism that increases the importance of recent timesteps of a sequence during temporal feature learning, forecasts the vessel trajectory taking the potential route and destination into account. The F1 Score of these features is approximately 85% and 75%, indicating their efficiency in supplementing the neural network. We trialed our model in the Gulf of St. Lawrence, one of the North Atlantic Right Whales (NARW) habitats, achieving an R2 score exceeding 98% with varying techniques and features. Despite the high R2 score being attributed to well-defined shipping lanes, our model demonstrates superior complex decision-making during path selection. In addition, our model shows enhanced accuracy, with average and median forecasting errors of 11km and 6km, respectively. Our study confirms the potential of geographical data engineering and trajectory forecasting models for preserving marine life species.
</details>
<details>
<summary>摘要</summary>
海运是全球经济增长和环境可持续性的重要因素。在这个意义上，自动识别系统（AIS）数据在实时流处理中提供了船舶运动的实时流处理数据，从而帮助提高船舶管理和避免船舶相撞和避免船舶和鲸鱼相撞。这篇论文面临了一个核心问题：在Engineered Sequence of AIS Data上进行多path长期船舶轨迹预测。我们使用了编码器-解码器模型，并利用Bi-LSTM网络来预测下一个12小时的船舶轨迹，使用1-3小时的AIS数据。我们对AIS数据进行了Engineering probabilistic特征，这些特征描述了每个轨迹的潜在路径和目的地，以便模型可以通过卷积层学习空间特征和时间特征来预测船舶轨迹。F1 Score的效果为 approximately 85%和75%，表明这些特征的有效性。我们在加拿大圣劳伦斯湾进行了试验， achieve an R2 score exceeding 98% with varying techniques and features。 despite the high R2 score being attributed to well-defined shipping lanes, our model demonstrates superior complex decision-making during path selection。此外，我们的模型还显示了更高的准确率， average和median forecasting errors of 11km和6km， respectively。我们的研究证明了地理数据工程和轨迹预测模型在保护海洋生物种的潜在作用。
</details></li>
</ul>
<hr>
<h2 id="Language-Agents-with-Reinforcement-Learning-for-Strategic-Play-in-the-Werewolf-Game"><a href="#Language-Agents-with-Reinforcement-Learning-for-Strategic-Play-in-the-Werewolf-Game" class="headerlink" title="Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game"></a>Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18940">http://arxiv.org/abs/2310.18940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zelai Xu, Chao Yu, Fei Fang, Yu Wang, Yi Wu</li>
<li>for: 这个研究旨在开发一个基于强大语言模型（LLM）的战略语言代理人，以增强在社交推理游戏“狼人”中的决策能力。</li>
<li>methods: 这个框架使用强大语言模型来推理可能的诈欺，然后使用人口基于训练的循环学习策略来增强代理人的决策能力。</li>
<li>results: 这个研究获得了在对其他LLM-based代理人的胜率最高，并在对反对人类玩家的情况下保持了稳定性的成果。<details>
<summary>Abstract</summary>
Agents built with large language models (LLMs) have recently achieved great advancements. However, most of the efforts focus on single-agent or cooperative settings, leaving more general multi-agent environments underexplored. We propose a new framework powered by reinforcement learning (RL) to develop strategic language agents, i.e., LLM-based agents with strategic thinking ability, for a popular language game, Werewolf. Werewolf is a social deduction game with hidden roles that involves both cooperation and competition and emphasizes deceptive communication and diverse gameplay. Our agent tackles this game by first using LLMs to reason about potential deceptions and generate a set of strategically diverse actions. Then an RL policy, which selects an action from the candidates, is learned by population-based training to enhance the agents' decision-making ability. By combining LLMs with the RL policy, our agent produces a variety of emergent strategies, achieves the highest win rate against other LLM-based agents, and stays robust against adversarial human players in the Werewolf game.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）驱动的代理人最近获得了重要的进步。然而，大多数努力都集中在单个代理人或合作设置上，留下更通用的多代理人环境得以探索。我们提出了一个基于返点学习（RL）的新框架，用于开发语言代理人，即基于LLM的策略思维代理人，用于受欢迎的语言游戏《狼人》。《狼人》是一款社交推理游戏，涉及到协作和竞争，并强调误导性的交流和多样化游戏。我们的代理人首先使用LLM来理解潜在的误导和生成一组策略多样化的动作。然后，通过人口学习来学习RL策略，从候选actions中选择一个动作，以提高代理人的决策能力。通过结合LLM和RL策略，我们的代理人产生了多种emergent策略，在与其他LLM-based代理人的比赛中获得最高胜率，并在对人类玩家的抗击中保持稳定。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Algorithms-to-Predict-Chess960-Result-and-Develop-Opening-Themes"><a href="#Machine-Learning-Algorithms-to-Predict-Chess960-Result-and-Develop-Opening-Themes" class="headerlink" title="Machine Learning Algorithms to Predict Chess960 Result and Develop Opening Themes"></a>Machine Learning Algorithms to Predict Chess960 Result and Develop Opening Themes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18938">http://arxiv.org/abs/2310.18938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyan Deo, Nishchal Dwivedi</li>
<li>for: 这个研究旨在预测棋盘960（也称为菲律敦Random棋盘）游戏结果，以及为每个开局位置发展的开局主题。</li>
<li>methods: 这个研究使用机器学习技术预测游戏结果，并分析开局中每个位置的 Piece 的移动。</li>
<li>results: 研究使用三种机器学习算法（KNN clustering、Random Forest 和 Gradient Boosted Trees）预测游戏结果，并通过分析开局中每个位置的 Piece 的移动，预测游戏的发展方向。<details>
<summary>Abstract</summary>
This work focuses on the analysis of Chess 960, also known as Fischer Random Chess, a variant of traditional chess where the starting positions of the pieces are randomized. The study aims to predict the game outcome using machine learning techniques and develop an opening theme for each starting position. The first part of the analysis utilizes machine learning models to predict the game result based on certain moves in each position. The methodology involves segregating raw data from .pgn files into usable formats and creating datasets comprising approximately 500 games for each starting position. Three machine learning algorithms -- KNN Clustering, Random Forest, and Gradient Boosted Trees -- have been used to predict the game outcome. To establish an opening theme, the board is divided into five regions: center, white kingside, white queenside, black kingside, and black queenside. The data from games played by top engines in all 960 positions is used to track the movement of pieces in the opening. By analysing the change in the number of pieces in each region at specific moves, the report predicts the region towards which the game is developing. These models provide valuable insights into predicting game outcomes and understanding the opening theme in Chess 960.
</details>
<details>
<summary>摘要</summary>
To begin, the study uses machine learning models to predict the game result based on certain moves in each position. The methodology involves separating raw data from .pgn files into usable formats and creating datasets comprising approximately 500 games for each starting position. Three machine learning algorithms - KNN Clustering, Random Forest, and Gradient Boosted Trees - have been used to predict the game outcome.To establish an opening theme, the board is divided into five regions: center, white kingside, white queenside, black kingside, and black queenside. The data from games played by top engines in all 960 positions is used to track the movement of pieces in the opening. By analyzing the change in the number of pieces in each region at specific moves, the report predicts the region towards which the game is developing. These models provide valuable insights into predicting game outcomes and understanding the opening theme in Chess 960.
</details></li>
</ul>
<hr>
<h2 id="The-Utility-of-“Even-if…”-Semifactual-Explanation-to-Optimise-Positive-Outcomes"><a href="#The-Utility-of-“Even-if…”-Semifactual-Explanation-to-Optimise-Positive-Outcomes" class="headerlink" title="The Utility of “Even if…” Semifactual Explanation to Optimise Positive Outcomes"></a>The Utility of “Even if…” Semifactual Explanation to Optimise Positive Outcomes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18937">http://arxiv.org/abs/2310.18937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eoinkenny/semifactual_recourse_generation">https://github.com/eoinkenny/semifactual_recourse_generation</a></li>
<li>paper_authors: Eoin M. Kenny, Weipeng Huang</li>
<li>for: 这个论文的目的是优化积极的结果（如贷款批准），而不是改变否定结果（如贷款拒绝）。</li>
<li>methods: 该论文使用了Explainable AI（XAI）技术，并引入了一个新的概念called“Gain”来衡量用户从解释中受益。</li>
<li>results: 对比PRIOR WORK，该论文的算法能够更好地最大化用户的增值（Gain），并且 causality 在这个过程中具有重要性。 最重要的是，用户测试表明，当用户收到贷款批准的积极结果时，semifactual 解释比counterfactuals更有用。<details>
<summary>Abstract</summary>
When users receive either a positive or negative outcome from an automated system, Explainable AI (XAI) has almost exclusively focused on how to mutate negative outcomes into positive ones by crossing a decision boundary using counterfactuals (e.g., \textit{"If you earn 2k more, we will accept your loan application"}). Here, we instead focus on \textit{positive} outcomes, and take the novel step of using XAI to optimise them (e.g., \textit{"Even if you wish to half your down-payment, we will still accept your loan application"}). Explanations such as these that employ "even if..." reasoning, and do not cross a decision boundary, are known as semifactuals. To instantiate semifactuals in this context, we introduce the concept of \textit{Gain} (i.e., how much a user stands to benefit from the explanation), and consider the first causal formalisation of semifactuals. Tests on benchmark datasets show our algorithms are better at maximising gain compared to prior work, and that causality is important in the process. Most importantly however, a user study supports our main hypothesis by showing people find semifactual explanations more useful than counterfactuals when they receive the positive outcome of a loan acceptance.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The translation is written in the formal tone, which is appropriate for academic writing.)
</details></li>
</ul>
<hr>
<h2 id="Self-Attention-with-Temporal-Prior-Can-We-Learn-More-from-Arrow-of-Time"><a href="#Self-Attention-with-Temporal-Prior-Can-We-Learn-More-from-Arrow-of-Time" class="headerlink" title="Self Attention with Temporal Prior: Can We Learn More from Arrow of Time?"></a>Self Attention with Temporal Prior: Can We Learn More from Arrow of Time?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18932">http://arxiv.org/abs/2310.18932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyung Geun Kim, Byeong Tak Lee</li>
<li>for: 这篇论文是为了解决具有短期和长期时间相关性的自然现象中的问题，特别是时间流向的方向导致的短期相关性。</li>
<li>methods: 该论文使用了注意力层来学习时间序列中的短期相关性，并应用了可学习的、适应性kernel来改进注意力矩阵中的短期时间偏好。</li>
<li>results: 实验结果显示，使用该方法可以在医疗记录（EHR）数据集上实现出色的预测结果，并在大多数任务和数据集上超越最佳模型。<details>
<summary>Abstract</summary>
Many of diverse phenomena in nature often inherently encode both short and long term temporal dependencies, short term dependencies especially resulting from the direction of flow of time. In this respect, we discovered experimental evidences suggesting that {\it interrelations} of these events are higher for closer time stamps. However, to be able for attention based models to learn these regularities in short term dependencies, it requires large amounts of data which are often infeasible. This is due to the reason that, while they are good at learning piece wised temporal dependencies, attention based models lack structures that encode biases in time series. As a resolution, we propose a simple and efficient method that enables attention layers to better encode short term temporal bias of these data sets by applying learnable, adaptive kernels directly to the attention matrices. For the experiments, we chose various prediction tasks using Electronic Health Records (EHR) data sets since they are great examples that have underlying long and short term temporal dependencies. The results of our experiments show exceptional classification results compared to best performing models on most of the task and data sets.
</details>
<details>
<summary>摘要</summary>
很多自然现象具有各种多样化特征，其中许多现象具有短期和长期时间依赖关系。特别是短期时间依赖关系通常由时间流动的方向决定。我们发现了实验证据，表明在更近的时间戳之间的事件关系更高。然而，为了让注意力基本模型学习这些短期时间依赖关系，需要大量数据，而这些数据经常是不可能获得的。这是因为注意力基本模型能够学习独立的时间序列，但缺乏时间序列中的偏好编码结构。为解决这个问题，我们提出了一种简单而高效的方法，即在注意力矩阵上应用学习可变核函数，以更好地编码短期时间依赖关系。我们选择使用医疗记录（EHR）数据集进行实验，因为它们具有下面和长期时间依赖关系的特点。实验结果表明，我们的方法在大多数任务和数据集上达到了最佳性能。
</details></li>
</ul>
<hr>
<h2 id="CHAIN-Exploring-Global-Local-Spatio-Temporal-Information-for-Improved-Self-Supervised-Video-Hashing"><a href="#CHAIN-Exploring-Global-Local-Spatio-Temporal-Information-for-Improved-Self-Supervised-Video-Hashing" class="headerlink" title="CHAIN: Exploring Global-Local Spatio-Temporal Information for Improved Self-Supervised Video Hashing"></a>CHAIN: Exploring Global-Local Spatio-Temporal Information for Improved Self-Supervised Video Hashing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18926">http://arxiv.org/abs/2310.18926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rukai Wei, Yu Liu, Jingkuan Song, Heng Cui, Yanzhao Xie, Ke Zhou</li>
<li>for: The paper is written for improving the efficiency of video retrieval by compressing videos into binary codes and learning accurate hash codes for video retrieval.</li>
<li>methods: The paper uses contrastive learning with augmentation strategies to capture global spatio-temporal information and local spatio-temporal details within video frames, and incorporates two collaborative learning tasks to enhance the perception of temporal structure and the modeling of spatio-temporal relationships.</li>
<li>results: The proposed method outperforms state-of-the-art self-supervised video hashing methods on four video benchmark datasets.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 本文是为了提高视频检索的效率，通过压缩视频为二进制代码，并学习正确的视频哈希代码。</li>
<li>methods: 本文使用对比学习，并采用了一系列的增强策略，以捕捉视频中的全球空间时间信息和本地空间时间细节。</li>
<li>results: 提议的方法在四个视频标准数据集上超越了现有的自动生成视频哈希方法。<details>
<summary>Abstract</summary>
Compressing videos into binary codes can improve retrieval speed and reduce storage overhead. However, learning accurate hash codes for video retrieval can be challenging due to high local redundancy and complex global dependencies between video frames, especially in the absence of labels. Existing self-supervised video hashing methods have been effective in designing expressive temporal encoders, but have not fully utilized the temporal dynamics and spatial appearance of videos due to less challenging and unreliable learning tasks. To address these challenges, we begin by utilizing the contrastive learning task to capture global spatio-temporal information of videos for hashing. With the aid of our designed augmentation strategies, which focus on spatial and temporal variations to create positive pairs, the learning framework can generate hash codes that are invariant to motion, scale, and viewpoint. Furthermore, we incorporate two collaborative learning tasks, i.e., frame order verification and scene change regularization, to capture local spatio-temporal details within video frames, thereby enhancing the perception of temporal structure and the modeling of spatio-temporal relationships. Our proposed Contrastive Hashing with Global-Local Spatio-temporal Information (CHAIN) outperforms state-of-the-art self-supervised video hashing methods on four video benchmark datasets. Our codes will be released.
</details>
<details>
<summary>摘要</summary>
压缩视频到二进制编码可以提高检索速度和减少存储开销。然而，学习准确的Hash代码 для视频检索可以是一个挑战，因为视频帧之间存在高度的本地重复和复杂的全球依赖关系，特别是在标签缺失的情况下。现有的自动编号视频方法有效地设计了表达力强的时间编码器，但是它们没有完全利用视频的时间动态和空间显示特征，尤其是在较难和不可靠的学习任务下。为解决这些挑战，我们开始利用对比学习任务来捕捉视频的全球空间时间信息，并采用我们设计的扩展策略，以创建正确的对应对。这些扩展策略专注于视频帧中的空间和时间变化，以便生成不变于运动、缩放和视点的Hash代码。此外，我们还集成了两个协作学习任务，即帧顺序验证和场景变化规范，以捕捉视频帧中的本地空间时间细节，从而增强视频的时间结构和空间时间关系的模型。我们提出的Contrastive Hashing with Global-Local Spatio-temporal Information（CHAIN）方法在四个视频标准测试集上超越了当前自动编号视频方法的性能。我们的代码将会公开发布。
</details></li>
</ul>
<hr>
<h2 id="QWID-Quantized-Weed-Identification-Deep-neural-network"><a href="#QWID-Quantized-Weed-Identification-Deep-neural-network" class="headerlink" title="QWID: Quantized Weed Identification Deep neural network"></a>QWID: Quantized Weed Identification Deep neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18921">http://arxiv.org/abs/2310.18921</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/parikshit14/qnn-for-weed">https://github.com/parikshit14/qnn-for-weed</a></li>
<li>paper_authors: Parikshit Singh Rathore</li>
<li>for: 本研究旨在提供一种高效的农业用途隐藏植物分类方法。</li>
<li>methods: 该方法使用量化深度神经网络模型，通过8位整数（int8）量化，与标准32位浮点数（fp32）模型不同。它采用了模型大小、执行时间和准确率的平衡，适应农业领域的具体需求。</li>
<li>results: 该方法在ResNet-50和InceptionV3架构上实现了准确率与模型大小、执行时间之间的平衡，并在Desktop、Mobile和Raspberry Pi等实际生产环境中实现了显著的模型大小和执行时间减少，同时保持了准确率的水平。<details>
<summary>Abstract</summary>
In this paper, we present an efficient solution for weed classification in agriculture. We focus on optimizing model performance at inference while respecting the constraints of the agricultural domain. We propose a Quantized Deep Neural Network model that classifies a dataset of 9 weed classes using 8-bit integer (int8) quantization, a departure from standard 32-bit floating point (fp32) models. Recognizing the hardware resource limitations in agriculture, our model balances model size, inference time, and accuracy, aligning with practical requirements. We evaluate the approach on ResNet-50 and InceptionV3 architectures, comparing their performance against their int8 quantized versions. Transfer learning and fine-tuning are applied using the DeepWeeds dataset. The results show staggering model size and inference time reductions while maintaining accuracy in real-world production scenarios like Desktop, Mobile and Raspberry Pi. Our work sheds light on a promising direction for efficient AI in agriculture, holding potential for broader applications.   Code: https://github.com/parikshit14/QNN-for-weed
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种高效的苔藿类分类方案，旨在在农业领域中提高模型性能的推理过程。我们提出了一种量化深度神经网络模型，通过使用8比特整数（int8）量化，与标准32比特浮点数（fp32）模型不同。考虑到农业领域的硬件资源有限，我们的模型坚持平衡模型大小、推理时间和准确率之间的权衡，符合实际需求。我们使用ResNet-50和InceptionV3架构进行评估，与其int8量化版本进行比较。通过使用传输学习和精度调整，我们在桌面、移动设备和Raspberry Pi等实际生产环境中实现了各种模型大小和推理时间的减少，同时保持了准确率。我们的工作探讨了苔藿类分类领域中高效的AI应用的可能性，对更广泛的应用产生了深见。Code: <https://github.com/parikshit14/QNN-for-weed>
</details></li>
</ul>
<hr>
<h2 id="Posterior-Sampling-with-Delayed-Feedback-for-Reinforcement-Learning-with-Linear-Function-Approximation"><a href="#Posterior-Sampling-with-Delayed-Feedback-for-Reinforcement-Learning-with-Linear-Function-Approximation" class="headerlink" title="Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation"></a>Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18919">http://arxiv.org/abs/2310.18919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikki Lijing Kuang, Ming Yin, Mengdi Wang, Yu-Xiang Wang, Yi-An Ma</li>
<li>for: 这个论文主要目标是解决延迟反馈的问题在强化学习中，以提高实际世界系统的性能。</li>
<li>methods: 这个论文使用了线性函数近似来解决延迟反馈问题，并使用 posterior sampling 来实现。</li>
<li>results: 论文提出了一种名为 Delayed-PSVI 的优化策略，并提供了对这种策略的首次分析。这种策略在不知道延迟时间的情况下可以达到 $\widetilde{O}(\sqrt{d^3H^3 T} + d^2H^2 E[\tau])$ 的最差情况 regret。<details>
<summary>Abstract</summary>
Recent studies in reinforcement learning (RL) have made significant progress by leveraging function approximation to alleviate the sample complexity hurdle for better performance. Despite the success, existing provably efficient algorithms typically rely on the accessibility of immediate feedback upon taking actions. The failure to account for the impact of delay in observations can significantly degrade the performance of real-world systems due to the regret blow-up. In this work, we tackle the challenge of delayed feedback in RL with linear function approximation by employing posterior sampling, which has been shown to empirically outperform the popular UCB algorithms in a wide range of regimes. We first introduce Delayed-PSVI, an optimistic value-based algorithm that effectively explores the value function space via noise perturbation with posterior sampling. We provide the first analysis for posterior sampling algorithms with delayed feedback in RL and show our algorithm achieves $\widetilde{O}(\sqrt{d^3H^3 T} + d^2H^2 E[\tau])$ worst-case regret in the presence of unknown stochastic delays. Here $E[\tau]$ is the expected delay. To further improve its computational efficiency and to expand its applicability in high-dimensional RL problems, we incorporate a gradient-based approximate sampling scheme via Langevin dynamics for Delayed-LPSVI, which maintains the same order-optimal regret guarantee with $\widetilde{O}(dHK)$ computational cost. Empirical evaluations are performed to demonstrate the statistical and computational efficacy of our algorithms.
</details>
<details>
<summary>摘要</summary>
现代再强化学习（RL）研究已经做出了重要进步，通过函数近似来缓解样本复杂性问题，以提高性能。然而，现有的可证fficient算法通常需要快速获得反馈，如果忽略延迟的影响，则可能导致实际系统中的 regret 增长。在这项工作中，我们面临了RL中延迟反馈的挑战，使用线性函数近似，并利用 posterior sampling，这种方法在各种场景中都有良好的实际表现。我们首先介绍了延迟PSVI算法，这是一种使用噪声扰动和 posterior sampling 来精细探索值函数空间的优化算法。我们提供了RL中延迟反馈 posterior sampling 的首次分析，并证明我们的算法在存在未知随机延迟的情况下可以 achiev $\widetilde{O}(\sqrt{d^3H^3T} + d^2H^2 E[\tau])$ 最坏情况的 regret。这里 $E[\tau]$ 是预期的延迟。为了进一步改进其计算效率和扩展到高维RL问题，我们提出了增强版的 Delayed-LPSVI 算法，使用朗凯朋 dynamics 来实现约同 Optimal 的 regret 保证，但计算成本为 $\widetilde{O}(dHK)$。我们的实验表明我们的算法在统计和计算上具有良好的效果。
</details></li>
</ul>
<hr>
<h2 id="Debiasing-Algorithm-through-Model-Adaptation"><a href="#Debiasing-Algorithm-through-Model-Adaptation" class="headerlink" title="Debiasing Algorithm through Model Adaptation"></a>Debiasing Algorithm through Model Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18913">http://arxiv.org/abs/2310.18913</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tomlimi/DAMA">https://github.com/tomlimi/DAMA</a></li>
<li>paper_authors: Tomasz Limisiewicz, David Mareček, Tomáš Musil</li>
<li>for: 这项研究旨在检测和 Mitigating 语言模型中的性别偏见。</li>
<li>methods: 该研究使用 causal analysis 方法来 indentify 问题atic model components，并发现 mid-upper feed-forward layers 最容易传递偏见。基于分析结果，我们采用 linear projection 方法来修改模型。</li>
<li>results: 我们的 DAMA 方法能够 Significantly 降低 bias 指标，同时保持模型在下游任务中的表现。我们发布了我们的方法和模型代码，可以 retrained LLaMA 的 state-of-the-art performance，但是significantly less biased。<details>
<summary>Abstract</summary>
Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. Based on the analysis results, we adapt the model by multiplying these layers by a linear projection. Our titular method, DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.
</details>
<details>
<summary>摘要</summary>
大型语言模型在各种语言任务中成为首选解决方案，但是随着容量的增长，模型往往会受到来自于训练数据中的偏见和 sterotypes 的影响，导致模型偏误预测。本研究提出一种新的方法来检测和减轻语言模型中的性别偏见。我们通过 causal 分析发现，中upper feed-forward 层最容易传递偏见。根据分析结果，我们对这些层进行线性投影修改，实现了我们的 DAMA 方法，可以对多种度量进行减轻偏见，同时保持模型在下游任务上的性能。我们发布了我们的方法和模型代码，可以在 retrained LLaMA 的基础性能下进行训练，并且具有较少偏见的性能。
</details></li>
</ul>
<hr>
<h2 id="InstanT-Semi-supervised-Learning-with-Instance-dependent-Thresholds"><a href="#InstanT-Semi-supervised-Learning-with-Instance-dependent-Thresholds" class="headerlink" title="InstanT: Semi-supervised Learning with Instance-dependent Thresholds"></a>InstanT: Semi-supervised Learning with Instance-dependent Thresholds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18910">http://arxiv.org/abs/2310.18910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muyang Li, Runze Wu, Haoyu Liu, Jun Yu, Xun Yang, Bo Han, Tongliang Liu</li>
<li>for: 这篇论文主要是研究 semi-supervised learning（SSL）中的一种新方法，即使用实例висиendent的阈值来选择可信度高的无标示实例，以提高SSL的性能。</li>
<li>methods: 本文提出了一种新的实例висиendent阈值函数，通过利用实例的具体性和 pseudo-标签的实例 dependent 错误率来确定每个实例的阈值。此外，本文还提供了一个 bounded 概率保证，以确保 pseudo-标签 的正确性。</li>
<li>results: 实验结果表明，使用实例висиendent阈值函数可以提高 SSL 的性能，并且可以更好地适应实际应用中的不同数据分布。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) has been a fundamental challenge in machine learning for decades. The primary family of SSL algorithms, known as pseudo-labeling, involves assigning pseudo-labels to confident unlabeled instances and incorporating them into the training set. Therefore, the selection criteria of confident instances are crucial to the success of SSL. Recently, there has been growing interest in the development of SSL methods that use dynamic or adaptive thresholds. Yet, these methods typically apply the same threshold to all samples, or use class-dependent thresholds for instances belonging to a certain class, while neglecting instance-level information. In this paper, we propose the study of instance-dependent thresholds, which has the highest degree of freedom compared with existing methods. Specifically, we devise a novel instance-dependent threshold function for all unlabeled instances by utilizing their instance-level ambiguity and the instance-dependent error rates of pseudo-labels, so instances that are more likely to have incorrect pseudo-labels will have higher thresholds. Furthermore, we demonstrate that our instance-dependent threshold function provides a bounded probabilistic guarantee for the correctness of the pseudo-labels it assigns.
</details>
<details>
<summary>摘要</summary>
半监督学习（SSL）已经是机器学习领域的基本挑战之一。主要的SSL算法家族是使用 Pseudo-labeling 方法，即将 confidence 度不高的无标签实例分配 pseudo-label。因此，选择 pseudo-label 的标准是 SSL 的关键。近年来，有越来越多的关注于开发 SSL 方法，使用动态或适应性的阈值。然而，这些方法通常是对所有样本使用同一个阈值，或者使用基于类型的阈值 для归类为某个类型的实例，而忽略实例级别信息。在这篇论文中，我们提出了研究实例 dependent 阈值的想法。具体来说，我们设计了一种基于实例级别的阈值函数，用于所有无标签实例。我们利用实例级别的冗余和实例 dependent 的 pseudo-label 错误率，来确定实例的阈值。此外，我们也证明了我们的实例 dependent 阈值函数可以为 pseudo-label 提供 bounded 概率 garantue。
</details></li>
</ul>
<hr>
<h2 id="Stacking-the-Odds-Transformer-Based-Ensemble-for-AI-Generated-Text-Detection"><a href="#Stacking-the-Odds-Transformer-Based-Ensemble-for-AI-Generated-Text-Detection" class="headerlink" title="Stacking the Odds: Transformer-Based Ensemble for AI-Generated Text Detection"></a>Stacking the Odds: Transformer-Based Ensemble for AI-Generated Text Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18906">http://arxiv.org/abs/2310.18906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dukeraphaelng/synth_detectives">https://github.com/dukeraphaelng/synth_detectives</a></li>
<li>paper_authors: Duke Nguyen, Khaing Myat Noe Naing, Aditya Joshi</li>
<li>for: 本研究是参加 ALTA 2023 分享任务的提交作品，用于检测人工生成文本。</li>
<li>methods: 我们使用一个核心是 Transformer 的堆叠ensemble，选用了可 accessible 和轻量级的模型。</li>
<li>results: 我们的方法在官方提供的测试数据上达到了0.9555 的准确率。<details>
<summary>Abstract</summary>
This paper reports our submission under the team name `SynthDetectives' to the ALTA 2023 Shared Task. We use a stacking ensemble of Transformers for the task of AI-generated text detection. Our approach is novel in terms of its choice of models in that we use accessible and lightweight models in the ensemble. We show that ensembling the models results in an improved accuracy in comparison with using them individually. Our approach achieves an accuracy score of 0.9555 on the official test data provided by the shared task organisers.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese:这篇论文报道我们在 `SynthDetectives' 团队名下提交到 ALTA 2023 共同任务。我们使用Transformers核心来实现人工生成文本检测任务。我们的方法在选择模型方面具有创新性，我们使用可 accessible 和轻量级模型。我们的结果表明，将多个模型 ensemble 后，对比使用单个模型时，具有更高的准确率。我们在官方提供的测试数据上 achieved 0.9555 的准确率。
</details></li>
</ul>
<hr>
<h2 id="Ever-Evolving-Evaluator-EV3-Towards-Flexible-and-Reliable-Meta-Optimization-for-Knowledge-Distillation"><a href="#Ever-Evolving-Evaluator-EV3-Towards-Flexible-and-Reliable-Meta-Optimization-for-Knowledge-Distillation" class="headerlink" title="Ever Evolving Evaluator (EV3): Towards Flexible and Reliable Meta-Optimization for Knowledge Distillation"></a>Ever Evolving Evaluator (EV3): Towards Flexible and Reliable Meta-Optimization for Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18893">http://arxiv.org/abs/2310.18893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Ding, Masrour Zoghi, Guy Tennenholtz, Maryam Karimzadehgan</li>
<li>for: 本文提出了一种基于综合优化的机器学习模型训练框架EV3，用于高效地训练可扩展的机器学习模型。</li>
<li>methods: EV3使用了一种intuitive的explore-assess-adapt协议，在每个迭代中探索不同的模型参数更新，使用相关的评估方法进行评估，并根据最佳更新和之前的进程历史来适应模型。</li>
<li>results: 实验结果表明，EV3可以安全地探索模型空间，并且在多个目标下可以动态调整任务。这种具有很大的灵活性和适应能力的方法，在多种领域可能有广泛的应用。<details>
<summary>Abstract</summary>
We introduce EV3, a novel meta-optimization framework designed to efficiently train scalable machine learning models through an intuitive explore-assess-adapt protocol. In each iteration of EV3, we explore various model parameter updates, assess them using pertinent evaluation methods, and adapt the model based on the optimal updates and previous progress history. EV3 offers substantial flexibility without imposing stringent constraints like differentiability on the key objectives relevant to the tasks of interest. Moreover, this protocol welcomes updates with biased gradients and allows for the use of a diversity of losses and optimizers. Additionally, in scenarios with multiple objectives, it can be used to dynamically prioritize tasks. With inspiration drawn from evolutionary algorithms, meta-learning, and neural architecture search, we investigate an application of EV3 to knowledge distillation. Our experimental results illustrate EV3's capability to safely explore model spaces, while hinting at its potential applicability across numerous domains due to its inherent flexibility and adaptability.
</details>
<details>
<summary>摘要</summary>
我们介绍EV3，一种新的元优化框架，用于高效地训练可扩展机器学习模型。EV3采用一种直观的探索-评估-适应协议，在每次迭代中探索不同的模型参数更新，使用相关的评估方法进行评估，并根据最佳更新和前一次进程历史来适应模型。EV3具有明显的灵活性，不需要在关键任务中强制执行梯度的微分性。此外，这个协议还允许使用多种损失函数和优化器，并在多个目标场景下动态准备任务。 drawing inspiration from evolutionary algorithms, meta-learning, and neural architecture search, we investigate the application of EV3 to knowledge distillation. Our experimental results show that EV3 can safely explore model spaces and hint at its potential applicability in various domains due to its inherent flexibility and adaptability.
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalized-Multi-stage-Clustering-Multi-view-Self-distillation"><a href="#Towards-Generalized-Multi-stage-Clustering-Multi-view-Self-distillation" class="headerlink" title="Towards Generalized Multi-stage Clustering: Multi-view Self-distillation"></a>Towards Generalized Multi-stage Clustering: Multi-view Self-distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18890">http://arxiv.org/abs/2310.18890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiatai Wang, Zhiwei Xu, Xin Wang</li>
<li>for: 这篇论文的目的是提出一个新的多stage深度嵌入式多视角 clustering 框架，以解决多视角 clustering 中 pseudo-label 错误导致模型预测不准确的问题。</li>
<li>methods: 这篇论文使用的方法包括 multi-view self-distillation (DistilMVC)，具体来说是在不同层次的特征空间中运用对比学习来探索多视角中的共同 semantics，并通过最大化两个视角之间的联系信息来取得 pseudo-labels。此外，还有一个 teacher network 负责将 pseudo-labels 转换为黑知识，以便对学习 Network 进行监督和改善预测能力。</li>
<li>results: 实验结果显示，这篇论文的方法在真实世界的多视角数据上表现较好，与现有的方法相比，具有更好的 clustering 性能。<details>
<summary>Abstract</summary>
Existing multi-stage clustering methods independently learn the salient features from multiple views and then perform the clustering task. Particularly, multi-view clustering (MVC) has attracted a lot of attention in multi-view or multi-modal scenarios. MVC aims at exploring common semantics and pseudo-labels from multiple views and clustering in a self-supervised manner. However, limited by noisy data and inadequate feature learning, such a clustering paradigm generates overconfident pseudo-labels that mis-guide the model to produce inaccurate predictions. Therefore, it is desirable to have a method that can correct this pseudo-label mistraction in multi-stage clustering to avoid the bias accumulation. To alleviate the effect of overconfident pseudo-labels and improve the generalization ability of the model, this paper proposes a novel multi-stage deep MVC framework where multi-view self-distillation (DistilMVC) is introduced to distill dark knowledge of label distribution. Specifically, in the feature subspace at different hierarchies, we explore the common semantics of multiple views through contrastive learning and obtain pseudo-labels by maximizing the mutual information between views. Additionally, a teacher network is responsible for distilling pseudo-labels into dark knowledge, supervising the student network and improving its predictive capabilities to enhance the robustness. Extensive experiments on real-world multi-view datasets show that our method has better clustering performance than state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
现有的多阶段划分方法独立地学习多视图中的突出特征，然后进行划分任务。特别是，多视图划分（MVC）在多视图或多模式场景中吸引了很多关注。MVC aimed at exploring common semantics and pseudo-labels from multiple views and clustering in a self-supervised manner. However, limited by noisy data and inadequate feature learning, such a clustering paradigm generates overconfident pseudo-labels that misguide the model to produce inaccurate predictions. Therefore, it is desirable to have a method that can correct this pseudo-label mistraction in multi-stage clustering to avoid the bias accumulation. To alleviate the effect of overconfident pseudo-labels and improve the generalization ability of the model, this paper proposes a novel multi-stage deep MVC framework, where multi-view self-distillation (DistilMVC) is introduced to distill dark knowledge of label distribution. Specifically, in the feature subspace at different hierarchies, we explore the common semantics of multiple views through contrastive learning and obtain pseudo-labels by maximizing the mutual information between views. Additionally, a teacher network is responsible for distilling pseudo-labels into dark knowledge, supervising the student network and improving its predictive capabilities to enhance the robustness. Extensive experiments on real-world multi-view datasets show that our method has better clustering performance than state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Learning-of-Generalized-Structured-Matrices-for-Efficient-Deep-Neural-Networks"><a href="#Differentiable-Learning-of-Generalized-Structured-Matrices-for-Efficient-Deep-Neural-Networks" class="headerlink" title="Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks"></a>Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18882">http://arxiv.org/abs/2310.18882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changwoo Lee, Hun-Seok Kim</li>
<li>for: 这个论文是研究如何使用有效的深度神经网络（DNN）来取代笼统的权重矩阵，以便提高神经网络的性能和复杂性。</li>
<li>methods: 作者提出了一种通用和可导的框架，可以系统地学习权重矩阵的有效结构。该框架包括定义一种新的结构化矩阵类型，并采用频域分布式参数化方法来通过拥平方根下降来学习结构参数。</li>
<li>results: 作者的方法可以学习出高性能且低复杂度的DNN，并且比之前使用低级、块稀或块低级矩阵的方法更高效。<details>
<summary>Abstract</summary>
This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters by proximal gradient descent. Finally, we introduce an effective initialization method for the proposed scheme. Our method learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior approaches that employ low-rank, block-sparse, or block-low-rank matrices.
</details>
<details>
<summary>摘要</summary>
To address this challenge, the authors propose a generalized and differentiable framework for learning efficient weight matrix structures using gradient descent. They define a new class of structured matrices that covers a wide range of existing structured matrices, and use a frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel to learn the structural parameters.The proposed method uses proximal gradient descent to optimize the structural parameters, and introduces an effective initialization method to improve performance. The authors demonstrate that their approach learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior methods that use low-rank, block-sparse, or block-low-rank matrices.
</details></li>
</ul>
<hr>
<h2 id="HDMNet-A-Hierarchical-Matching-Network-with-Double-Attention-for-Large-scale-Outdoor-LiDAR-Point-Cloud-Registration"><a href="#HDMNet-A-Hierarchical-Matching-Network-with-Double-Attention-for-Large-scale-Outdoor-LiDAR-Point-Cloud-Registration" class="headerlink" title="HDMNet: A Hierarchical Matching Network with Double Attention for Large-scale Outdoor LiDAR Point Cloud Registration"></a>HDMNet: A Hierarchical Matching Network with Double Attention for Large-scale Outdoor LiDAR Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18874">http://arxiv.org/abs/2310.18874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyi Xue, Fan Lu, Guang Chen</li>
<li>for: 大规模外部LiDAR点云注册问题</li>
<li>methods: 提出了一种基于卷积神经网络的double attention机制的 hierarchical neural network（HDMNet），通过两stage匹配和高效的patch-to-patch匹配来提高注册性能。</li>
<li>results: 对两个大规模外部LiDAR点云数据集进行了广泛的实验，证明了提案的HDMNet可以具有高精度和高效性。<details>
<summary>Abstract</summary>
Outdoor LiDAR point clouds are typically large-scale and complexly distributed. To achieve efficient and accurate registration, emphasizing the similarity among local regions and prioritizing global local-to-local matching is of utmost importance, subsequent to which accuracy can be enhanced through cost-effective fine registration. In this paper, a novel hierarchical neural network with double attention named HDMNet is proposed for large-scale outdoor LiDAR point cloud registration. Specifically, A novel feature consistency enhanced double-soft matching network is introduced to achieve two-stage matching with high flexibility while enlarging the receptive field with high efficiency in a patch-to patch manner, which significantly improves the registration performance. Moreover, in order to further utilize the sparse matching information from deeper layer, we develop a novel trainable embedding mask to incorporate the confidence scores of correspondences obtained from pose estimation of deeper layer, eliminating additional computations. The high-confidence keypoints in the sparser point cloud of the deeper layer correspond to a high-confidence spatial neighborhood region in shallower layer, which will receive more attention, while the features of non-key regions will be masked. Extensive experiments are conducted on two large-scale outdoor LiDAR point cloud datasets to demonstrate the high accuracy and efficiency of the proposed HDMNet.
</details>
<details>
<summary>摘要</summary>
大规模户外LiDAR点云注册通常具有复杂分布和大规模特征。为了实现高效和准确的注册，需要强调本地区域之间的相似性，并且优先进行全局本地-本地匹配。在本文中，一种名为HDMNet的新型层次神经网络是提出来解决大规模户外LiDAR点云注册问题。具体来说，我们引入了一种增强特征一致性的双注意网络，可以在patch-to-patch方式下实现高灵活性的两stage匹配，从而显著提高注册性能。此外，我们还开发了一种可 trains embeddingmask，以利用深层次姿态估计中的稀疏匹配信息，从而消除额外计算。高 confidence键点在更 sparse的点云中对应于深层次姿态中的高 confidence空间区域，这些区域将收到更多的注意力，而非键点区域的特征将被masked。我们在两个大规模户外LiDAR点云数据集上进行了广泛的实验，以示提出的HDMNet高精度和高效性。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Engineering-and-Transformer-based-Question-Generation-and-Evaluation"><a href="#Prompt-Engineering-and-Transformer-based-Question-Generation-and-Evaluation" class="headerlink" title="Prompt-Engineering and Transformer-based Question Generation and Evaluation"></a>Prompt-Engineering and Transformer-based Question Generation and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18867">http://arxiv.org/abs/2310.18867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rubaba Amyeen</li>
<li>for: 这篇论文主要是为了找出最佳的问题生成方法，以便在教学中使用。</li>
<li>methods: 该论文使用了一种名为distilBERT的变换器模型，并通过提示工程来生成问题。</li>
<li>results: 研究发现，使用这种方法可以生成高相似度的问题，其中30%的问题达到了高于70%的相似度。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Question generation has numerous applications in the educational context. Question generation can prove helpful for students when reviewing content and testing themselves. Furthermore, a question generation model can aid teachers by lessening the burden of creating assessments and other practice material. This paper aims to find the best method to generate questions from textual data through a transformer model and prompt engineering. In this research, we finetuned a pretrained distilBERT model on the SQuAD question answering dataset to generate questions. In addition to training a transformer model, prompt engineering was applied to generate questions effectively using the LLaMA model. The generated questions were compared against the baseline questions in the SQuAD dataset to evaluate the effectiveness of four different prompts. All four prompts demonstrated over 60% similarity on average. Of the prompt-generated questions, 30% achieved a high similarity score greater than 70%.
</details>
<details>
<summary>摘要</summary>
Question generation has numerous applications in educational contexts. Question generation can help students review content and assess themselves. Additionally, a question generation model can aid teachers by reducing the burden of creating assessments and practice material. This paper aims to find the best method to generate questions from textual data using a transformer model and prompt engineering. In this research, we fine-tuned a pre-trained distilBERT model on the SQuAD question answering dataset to generate questions. In addition to training a transformer model, prompt engineering was applied to generate questions effectively using the LLaMA model. The generated questions were compared to the baseline questions in the SQuAD dataset to evaluate the effectiveness of four different prompts. All four prompts showed an average similarity of over 60%. Of the prompt-generated questions, 30% achieved a high similarity score of over 70%.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/cs.AI_2023_10_29/" data-id="clogyj8vr00657cra4ls64llh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/cs.CL_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T11:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/cs.CL_2023_10_29/">cs.CL - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="From-Chatbots-to-PhishBots-–-Preventing-Phishing-scams-created-using-ChatGPT-Google-Bard-and-Claude"><a href="#From-Chatbots-to-PhishBots-–-Preventing-Phishing-scams-created-using-ChatGPT-Google-Bard-and-Claude" class="headerlink" title="From Chatbots to PhishBots? – Preventing Phishing scams created using ChatGPT, Google Bard and Claude"></a>From Chatbots to PhishBots? – Preventing Phishing scams created using ChatGPT, Google Bard and Claude</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19181">http://arxiv.org/abs/2310.19181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayak Saha Roy, Poojitha Thota, Krishna Vamsi Naragam, Shirin Nilizadeh</li>
<li>for: 防止 Large Language Models (LLMs) 生成邪恶内容，包括骗财攻击。</li>
<li>methods: 使用四种常见的商业可用 LLMs（ChatGPT、GPT 4、Claude 和 Bard）生成功能骗财攻击，使用 serie 的邪恶提示。</li>
<li>results: 发现这些 LLMs 可以生成具有识别度的骗财电子邮件和网站，并且可以使用诸如逃脱检测系统的诡计来掩盖自己。<details>
<summary>Abstract</summary>
The advanced capabilities of Large Language Models (LLMs) have made them invaluable across various applications, from conversational agents and content creation to data analysis, research, and innovation. However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks. This study explores the potential of using four popular commercially available LLMs - ChatGPT (GPT 3.5 Turbo), GPT 4, Claude and Bard to generate functional phishing attacks using a series of malicious prompts. We discover that these LLMs can generate both phishing emails and websites that can convincingly imitate well-known brands, and also deploy a range of evasive tactics for the latter to elude detection mechanisms employed by anti-phishing systems. Notably, these attacks can be generated using unmodified, or "vanilla," versions of these LLMs, without requiring any prior adversarial exploits such as jailbreaking. As a countermeasure, we build a BERT based automated detection tool that can be used for the early detection of malicious prompts to prevent LLMs from generating phishing content attaining an accuracy of 97\% for phishing website prompts, and 94\% for phishing email prompts.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）的高级功能使得它们在不同应用程序中变得不可或缺，从对话代理和内容创作到数据分析、研究和创新。然而，它们的效iveness和可用性也使得它们容易遭受用于生成恶意内容的违用，包括骗财攻击。这项研究探讨了使用四种流行的商业可用的 LLM——ChatGPT（GPT 3.5 Turbo）、GPT 4、Claude 和 Bard——生成功能攻击。我们发现这些 LLM 可以生成具有识别度的恶意电子邮件和网站，并且可以部署一系列逃脱检测机制的诡计。值得注意的是，这些攻击可以使用未修改的、“纯净”的 LLM 进行生成，不需要任何先前的反对攻击如监禁。作为对策，我们构建了基于 BERT 的自动检测工具，可以用于早期检测恶意提示，以防止 LLM 生成攻击内容，其准确率为 97%  для骗财网站提示，和 94%  для骗财电子邮件提示。
</details></li>
</ul>
<hr>
<h2 id="Robustifying-Language-Models-with-Test-Time-Adaptation"><a href="#Robustifying-Language-Models-with-Test-Time-Adaptation" class="headerlink" title="Robustifying Language Models with Test-Time Adaptation"></a>Robustifying Language Models with Test-Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19177">http://arxiv.org/abs/2310.19177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noah Thomas McDermott, Junfeng Yang, Chengzhi Mao</li>
<li>for: 防止语言模型受到语言攻击</li>
<li>methods: 使用遮盖词预测来动态适应输入句子，以逆转语言攻击</li>
<li>results: 在两个受欢迎的句子分类任务上，我们的方法可以修复65%以上的语言攻击In English, this means:</li>
<li>for: Preventing language models from being attacked by adversarial language</li>
<li>methods: Using dynamic adaptation of input sentences with predictions from masked words to reverse language adversarial attacks</li>
<li>results: Our method can repair over 65% of adversarial language attacks on two popular sentence classification tasks without requiring any training.<details>
<summary>Abstract</summary>
Large-scale language models achieved state-of-the-art performance over a number of language tasks. However, they fail on adversarial language examples, which are sentences optimized to fool the language models but with similar semantic meanings for humans. While prior work focuses on making the language model robust at training time, retraining for robustness is often unrealistic for large-scale foundation models. Instead, we propose to make the language models robust at test time. By dynamically adapting the input sentence with predictions from masked words, we show that we can reverse many language adversarial attacks. Since our approach does not require any training, it works for novel tasks at test time and can adapt to novel adversarial corruptions. Visualizations and empirical results on two popular sentence classification datasets demonstrate that our method can repair adversarial language attacks over 65% o
</details>
<details>
<summary>摘要</summary>
大规模语言模型在多种语言任务上实现了状态机器的表现，但它们对语言攻击例子失败，这些例子是用来欺骗语言模型的，但对人类来说含义相同的句子。而现有的工作通常在训练时做 robustness 的优化，但对大规模基础模型来说，这种 retraining 是不现实的。因此，我们提议在测试时使用语言模型的 robustness。我们通过在输入句子上动态适应预测结果来显示，我们可以反转许多语言攻击例子。我们的方法不需要任何训练，因此它在测试时可以对新任务进行适应，并且可以适应新的语言攻击。我们的实验结果和视觉化结果表明，我们的方法可以修复大于 65% 的语言攻击例子。
</details></li>
</ul>
<hr>
<h2 id="Poisoning-Retrieval-Corpora-by-Injecting-Adversarial-Passages"><a href="#Poisoning-Retrieval-Corpora-by-Injecting-Adversarial-Passages" class="headerlink" title="Poisoning Retrieval Corpora by Injecting Adversarial Passages"></a>Poisoning Retrieval Corpora by Injecting Adversarial Passages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19156">http://arxiv.org/abs/2310.19156</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princeton-nlp/corpus-poisoning">https://github.com/princeton-nlp/corpus-poisoning</a></li>
<li>paper_authors: Zexuan Zhong, Ziqing Huang, Alexander Wettig, Danqi Chen</li>
<li>for: 本研究旨在测试紧密搜寻系统的安全性，特别是在真实世界应用中是否可以安全地启用。</li>
<li>methods: 作者提出了一种新的对紧密搜寻系统的攻击方法，其中一名黑客产生了一小批陌生过程，并将其插入到大量搜寻 corpora 中，以导致紧密搜寻系统错误地回答查询。</li>
<li>results: 研究发现，这种攻击可以将紧密搜寻系统误导回答，并且这些陌生过程可以直接扩展到不同的域外查询和 corpora，例如在金融文档或网络论坛中，50个生成的过程可以误导&gt;94%的查询。<details>
<summary>Abstract</summary>
Dense retrievers have achieved state-of-the-art performance in various information retrieval tasks, but to what extent can they be safely deployed in real-world applications? In this work, we propose a novel attack for dense retrieval systems in which a malicious user generates a small number of adversarial passages by perturbing discrete tokens to maximize similarity with a provided set of training queries. When these adversarial passages are inserted into a large retrieval corpus, we show that this attack is highly effective in fooling these systems to retrieve them for queries that were not seen by the attacker. More surprisingly, these adversarial passages can directly generalize to out-of-domain queries and corpora with a high success attack rate -- for instance, we find that 50 generated passages optimized on Natural Questions can mislead >94% of questions posed in financial documents or online forums. We also benchmark and compare a range of state-of-the-art dense retrievers, both unsupervised and supervised. Although different systems exhibit varying levels of vulnerability, we show they can all be successfully attacked by injecting up to 500 passages, a small fraction compared to a retrieval corpus of millions of passages.
</details>
<details>
<summary>摘要</summary>
dense retrievers 在多种信息检索任务中实现了状态码表现，但它们在实际应用中是否可以安全部署？在这项工作中，我们提出了一种 novel 的攻击方法，malicious user 通过修改精确的token来生成一小数量的对抗 passage，以最大化与提供的训练问题的相似性。当这些对抗 passage 添加到大量的检索库中时，我们发现这种攻击非常有效，能让这些系统 retrieve 这些恶意生成的 passage 作为未看过的查询。即使在不同的领域和数据集上，这些对抗 passage 仍然能够直接泛化，并达到高度的成功攻击率。例如，我们发现50个优化后的对抗 passage 可以误导 >94%的金融文档或在线讨论中的问题。我们还对多种当前最佳的 dense retrievers 进行了 benchmark 和比较，包括不超过500个对抗 passage 的攻击。虽然不同的系统在攻击上展现出不同的抵抗程度，但我们发现所有这些系统都可以被成功攻击。
</details></li>
</ul>
<hr>
<h2 id="BERT-Lost-Patience-Won’t-Be-Robust-to-Adversarial-Slowdown"><a href="#BERT-Lost-Patience-Won’t-Be-Robust-to-Adversarial-Slowdown" class="headerlink" title="BERT Lost Patience Won’t Be Robust to Adversarial Slowdown"></a>BERT Lost Patience Won’t Be Robust to Adversarial Slowdown</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19152">http://arxiv.org/abs/2310.19152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ztcoalson/waffle">https://github.com/ztcoalson/waffle</a></li>
<li>paper_authors: Zachary Coalson, Gabriel Ritter, Rakesh Bobba, Sanghyun Hong</li>
<li>for: 这 paper 评估了多出口语言模型对钝化攻击的 Robustness。</li>
<li>methods: 作者设计了一种钝化攻击，通过生成自然的 adversarial text 绕过早出点。 然后，他们使用这种 WAFFLE 攻击来进行多出口机制的全面评估，并在 GLUE benchmark 上测试了三种多出口机制在钝化攻击下的性能。</li>
<li>results: 研究发现，钝化攻击可以减少多出口机制提供的计算成本，特别是对于复杂的机制而言。 此外，研究还发现了一些常见的 perturbation 模式，并与标准的 adversarial text 攻击进行比较。 最后，研究发现了输入整形可以有效地解决钝化攻击，但是 adversarial training 无法战胜钝化攻击。<details>
<summary>Abstract</summary>
In this paper, we systematically evaluate the robustness of multi-exit language models against adversarial slowdown. To audit their robustness, we design a slowdown attack that generates natural adversarial text bypassing early-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark against adversarial slowdown. We then show our attack significantly reduces the computational savings provided by the three methods in both white-box and black-box settings. The more complex a mechanism is, the more vulnerable it is to adversarial slowdown. We also perform a linguistic analysis of the perturbed text inputs, identifying common perturbation patterns that our attack generates, and comparing them with standard adversarial text attacks. Moreover, we show that adversarial training is ineffective in defeating our slowdown attack, but input sanitization with a conversational model, e.g., ChatGPT, can remove perturbations effectively. This result suggests that future work is needed for developing efficient yet robust multi-exit models. Our code is available at: https://github.com/ztcoalson/WAFFLE
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们系统地评估了多出口语言模型对针对性慢速攻击的Robustness。为了审计其Robustness，我们设计了一种通过绕过早出点生成自然针对性文本的攻击。我们使用这种WAFFLE攻击来进行对三种多出口机制的GLUEbenchmark进行广泛的评估，并显示我们的攻击可以在白盒和黑盒设置下减少了这些方法提供的计算成本。我们发现，复杂的机制更容易受到针对性慢速攻击。此外，我们还进行了文本输入的语言分析，找到了我们的攻击生成的扰乱模式，并与标准针对性攻击相比较。此外，我们还发现，对于我们的慢速攻击，反向训练无法有效地抵抗，但是使用 conversational 模型，例如 ChatGPT，可以有效地除掉扰乱。这种结果表明，未来的工作需要开发高效又Robust的多出口模型。我们的代码可以在：https://github.com/ztcoalson/WAFFLE 获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Follow-Object-Centric-Image-Editing-Instructions-Faithfully"><a href="#Learning-to-Follow-Object-Centric-Image-Editing-Instructions-Faithfully" class="headerlink" title="Learning to Follow Object-Centric Image Editing Instructions Faithfully"></a>Learning to Follow Object-Centric Image Editing Instructions Faithfully</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19145">http://arxiv.org/abs/2310.19145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tuhinjubcse/faithfuledits_emnlp2023">https://github.com/tuhinjubcse/faithfuledits_emnlp2023</a></li>
<li>paper_authors: Tuhin Chakrabarty, Kanishk Singh, Arkadiy Saakyan, Smaranda Muresan</li>
<li>for: 这篇论文旨在提高文本到图像扩展模型中的自然语言指令编辑性能。</li>
<li>methods: 本文提出了一种基于最新的分割、链式思维提示和视觉问答技术的方法，可以提高自然语言指令下的图像编辑质量。</li>
<li>results: 对比于现有的基线，该方法能够进行细化的对象中心编辑，并且能够在未经训练的领域中进行扩展。此外，模型还能够捕捉到文本指令中的含义，进行 faithfulness 的捕捉和修改。<details>
<summary>Abstract</summary>
Natural language instructions are a powerful interface for editing the outputs of text-to-image diffusion models. However, several challenges need to be addressed: 1) underspecification (the need to model the implicit meaning of instructions) 2) grounding (the need to localize where the edit has to be performed), 3) faithfulness (the need to preserve the elements of the image not affected by the edit instruction). Current approaches focusing on image editing with natural language instructions rely on automatically generated paired data, which, as shown in our investigation, is noisy and sometimes nonsensical, exacerbating the above issues. Building on recent advances in segmentation, Chain-of-Thought prompting, and visual question answering, we significantly improve the quality of the paired data. In addition, we enhance the supervision signal by highlighting parts of the image that need to be changed by the instruction. The model fine-tuned on the improved data is capable of performing fine-grained object-centric edits better than state-of-the-art baselines, mitigating the problems outlined above, as shown by automatic and human evaluations. Moreover, our model is capable of generalizing to domains unseen during training, such as visual metaphors.
</details>
<details>
<summary>摘要</summary>
自然语言指令是文本到图像扩散模型的高级用户界面。然而，需要解决以下挑战：1）下pecification（需要模型理解指令的隐含含义）、2）grounding（需要确定编辑操作的具体位置）、3）loyal（需要保持图像中未受影响的元素）。现有的方法通过自动生成的对应数据来实现图像编辑，但这些数据经过我们的调查发现噪音和无意义，这些问题进一步加剧了上述问题。我们基于最近的分割、链条提示和视觉问答技术进行了大幅改进，提高对应数据的质量。此外，我们还强调要更改的图像部分，以提高模型的精细化对象编辑能力。经过练习这些改进后的模型，在自动和人工评估中都能够更好地完成细化的对象编辑任务，并且能够在训练时未看到的领域中进行推断。此外，我们的模型还能够捕捉到视觉 метаFOR，进一步提高图像编辑的精度和效果。
</details></li>
</ul>
<hr>
<h2 id="Women-Wearing-Lipstick-Measuring-the-Bias-Between-an-Object-and-Its-Related-Gender"><a href="#Women-Wearing-Lipstick-Measuring-the-Bias-Between-an-Object-and-Its-Related-Gender" class="headerlink" title="Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender"></a>Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19130">http://arxiv.org/abs/2310.19130</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahmedssabir/genderscore">https://github.com/ahmedssabir/genderscore</a></li>
<li>paper_authors: Ahmed Sabir, Lluís Padró</li>
<li>for:  investigate the impact of objects on gender bias in image captioning systems</li>
<li>methods:  use visual semantic-based gender score to measure the degree of bias</li>
<li>results:  propose a gender score that can be used as an additional metric to existing approach, and observe the bias relation between caption and related gender<details>
<summary>Abstract</summary>
In this paper, we investigate the impact of objects on gender bias in image captioning systems. Our results show that only gender-specific objects have a strong gender bias (e.g., women-lipstick). In addition, we propose a visual semantic-based gender score that measures the degree of bias and can be used as a plug-in for any image captioning system. Our experiments demonstrate the utility of the gender score, since we observe that our score can measure the bias relation between a caption and its related gender; therefore, our score can be used as an additional metric to the existing Object Gender Co-Occ approach. Code and data are publicly available at \url{https://github.com/ahmedssabir/GenderScore}.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了图像描述系统中的性别偏见。我们的结果显示，只有性别特定的物品会带有强烈的性别偏见（例如女性 lipstick）。此外，我们提出了基于视觉 semantics 的性别分数，可以用于任何图像描述系统中。我们的实验表明了这个分数的用途，因为我们发现了这个分数可以测量描述和其相关的性别之间的偏见关系，因此可以用作现有的 Object Gender Co-Occ 方法的附加指标。代码和数据都可以在 \url{https://github.com/ahmedssabir/GenderScore} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Unified-Representation-for-Non-compositional-and-Compositional-Expressions"><a href="#Unified-Representation-for-Non-compositional-and-Compositional-Expressions" class="headerlink" title="Unified Representation for Non-compositional and Compositional Expressions"></a>Unified Representation for Non-compositional and Compositional Expressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19127">http://arxiv.org/abs/2310.19127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziheng Zeng, Suma Bhat</li>
<li>for: This paper is written for researchers and developers working on natural language processing (NLP) and machine learning, specifically those interested in non-compositional language and idiomatic expressions.</li>
<li>methods: The paper proposes a language model called PIER, which builds on BART and generates semantically meaningful and contextually appropriate representations for English potentially idiomatic expressions (PIEs).</li>
<li>results: The paper shows that the representations generated by PIER result in higher homogeneity scores for embedding clustering and gains in accuracy and sequence accuracy for PIE sense classification and span detection compared to the state-of-the-art IE representation model, GIEA, without sacrificing performance on NLU tasks.<details>
<summary>Abstract</summary>
Accurate processing of non-compositional language relies on generating good representations for such expressions. In this work, we study the representation of language non-compositionality by proposing a language model, PIER, that builds on BART and can create semantically meaningful and contextually appropriate representations for English potentially idiomatic expressions (PIEs). PIEs are characterized by their non-compositionality and contextual ambiguity in their literal and idiomatic interpretations. Via intrinsic evaluation on embedding quality and extrinsic evaluation on PIE processing and NLU tasks, we show that representations generated by PIER result in 33% higher homogeneity score for embedding clustering than BART, whereas 3.12% and 3.29% gains in accuracy and sequence accuracy for PIE sense classification and span detection compared to the state-of-the-art IE representation model, GIEA. These gains are achieved without sacrificing PIER's performance on NLU tasks (+/- 1% accuracy) compared to BART.
</details>
<details>
<summary>摘要</summary>
Accurate processing of non-compositional language relies on generating good representations for such expressions. In this work, we study the representation of language non-compositionality by proposing a language model, PIER, that builds on BART and can create semantically meaningful and contextually appropriate representations for English potentially idiomatic expressions (PIEs). PIEs are characterized by their non-compositionality and contextual ambiguity in their literal and idiomatic interpretations. Via intrinsic evaluation on embedding quality and extrinsic evaluation on PIE processing and NLU tasks, we show that representations generated by PIER result in 33% higher homogeneity score for embedding clustering than BART, whereas 3.12% and 3.29% gains in accuracy and sequence accuracy for PIE sense classification and span detection compared to the state-of-the-art IE representation model, GIEA. These gains are achieved without sacrificing PIER's performance on NLU tasks (+/- 1% accuracy) compared to BART.Here's the translation in Traditional Chinese: Accurate processing of non-compositional language relies on generating good representations for such expressions. In this work, we study the representation of language non-compositionality by proposing a language model, PIER, that builds on BART and can create semantically meaningful and contextually appropriate representations for English potentially idiomatic expressions (PIEs). PIEs are characterized by their non-compositionality and contextual ambiguity in their literal and idiomatic interpretations. Via intrinsic evaluation on embedding quality and extrinsic evaluation on PIE processing and NLU tasks, we show that representations generated by PIER result in 33% higher homogeneity score for embedding clustering than BART, whereas 3.12% and 3.29% gains in accuracy and sequence accuracy for PIE sense classification and span detection compared to the state-of-the-art IE representation model, GIEA. These gains are achieved without sacrificing PIER's performance on NLU tasks (+/- 1% accuracy) compared to BART.
</details></li>
</ul>
<hr>
<h2 id="PACuna-Automated-Fine-Tuning-of-Language-Models-for-Particle-Accelerators"><a href="#PACuna-Automated-Fine-Tuning-of-Language-Models-for-Particle-Accelerators" class="headerlink" title="PACuna: Automated Fine-Tuning of Language Models for Particle Accelerators"></a>PACuna: Automated Fine-Tuning of Language Models for Particle Accelerators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19106">http://arxiv.org/abs/2310.19106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonin Sulc, Raimund Kammering, Annika Eichler, Tim Wilksen</li>
<li>for: 提高加速器设备的理解和解释能力</li>
<li>methods: 使用公开available的加速器资源（如会议、预印文章和书籍）自动生成问题和数据集，并使用Language模型进行精细调整</li>
<li>results: PACuna可以解决复杂的加速器问题，并被专家 validateTranslation:</li>
<li>for: 提高加速器设备的理解和解释能力</li>
<li>methods: 使用公开available的加速器资源（如会议、预印文章和书籍）自动生成问题和数据集，并使用Language模型进行精细调整</li>
<li>results: PACuna可以解决复杂的加速器问题，并被专家 validate<details>
<summary>Abstract</summary>
Navigating the landscape of particle accelerators has become increasingly challenging with recent surges in contributions. These intricate devices challenge comprehension, even within individual facilities. To address this, we introduce PACuna, a fine-tuned language model refined through publicly available accelerator resources like conferences, pre-prints, and books. We automated data collection and question generation to minimize expert involvement and make the data publicly available. PACuna demonstrates proficiency in addressing intricate accelerator questions, validated by experts. Our approach shows adapting language models to scientific domains by fine-tuning technical texts and auto-generated corpora capturing the latest developments can further produce pre-trained models to answer some intricate questions that commercially available assistants cannot and can serve as intelligent assistants for individual facilities.
</details>
<details>
<summary>摘要</summary>
在加速器领域的探索中，由于最近的贡献增加， navigating 已成为越来越复杂的任务。这些细腻的设备会使人们感到困惑，甚至在同一个设施内。为解决这问题，我们介绍了 PACuna，一种精度调整的语言模型，通过公共可用的加速器资源，如会议、预印和书籍来优化。我们自动收集数据和生成问题，以最小化专家参与度，并将数据公开可用。 PACuna 在解决复杂的加速器问题方面表现出色，由专家 validate。我们的方法表明，通过科学领域中的技术文本和自动生成的 corpora 来练习语言模型，可以生成适用于一些复杂问题的预训练模型，这些模型可以作为加速器设施的智能助手。
</details></li>
</ul>
<hr>
<h2 id="Pushdown-Layers-Encoding-Recursive-Structure-in-Transformer-Language-Models"><a href="#Pushdown-Layers-Encoding-Recursive-Structure-in-Transformer-Language-Models" class="headerlink" title="Pushdown Layers: Encoding Recursive Structure in Transformer Language Models"></a>Pushdown Layers: Encoding Recursive Structure in Transformer Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19089">http://arxiv.org/abs/2310.19089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher D. Manning</li>
<li>for: This paper aims to improve the syntactic generalization of Transformer language models by introducing a new self-attention layer called Pushdown Layers.</li>
<li>methods: The Pushdown Layers model recursive state via a stack tape that tracks estimated depths of every token, and the Transformer LMs with Pushdown Layers use this stack tape to softly modulate attention over tokens.</li>
<li>results: The authors achieve dramatically better and 3-5x more sample-efficient syntactic generalization when training Transformers equipped with Pushdown Layers on a corpus of strings annotated with silver constituency parses, while maintaining similar perplexities.<details>
<summary>Abstract</summary>
Recursion is a prominent feature of human language, and fundamentally challenging for self-attention due to the lack of an explicit recursive-state tracking mechanism. Consequently, Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic generalization. This work introduces Pushdown Layers, a new self-attention layer that models recursive state via a stack tape that tracks estimated depths of every token in an incremental parse of the observed prefix. Transformer LMs with Pushdown Layers are syntactic language models that autoregressively and synchronously update this stack tape as they predict new tokens, in turn using the stack tape to softly modulate attention over tokens -- for instance, learning to "skip" over closed constituents. When trained on a corpus of strings annotated with silver constituency parses, Transformers equipped with Pushdown Layers achieve dramatically better and 3-5x more sample-efficient syntactic generalization, while maintaining similar perplexities. Pushdown Layers are a drop-in replacement for standard self-attention. We illustrate this by finetuning GPT2-medium with Pushdown Layers on an automatically parsed WikiText-103, leading to improvements on several GLUE text classification tasks.
</details>
<details>
<summary>摘要</summary>
人类语言中具有重要特点的一种是Recursion，它对于自注意机制的缺乏显式状态跟踪机制而具有挑战性。因此，Transformer语言模型在捕捉长尾递归结构方面表现不佳，并且 exhibit  sample-inefficient  sintactic generalization。这项工作介绍了Pushdown层，一种新的自注意层，通过一个堆栈带跟踪每个字符的估计深度来模型 recursive state。Transformer LMs  WITH Pushdown Layers 是一种强式语言模型，可以同步和顺序地更新这个堆栈带，并在预测新字符时使用堆栈来软模式地修饰注意力。例如，学习"跳过"关闭的成分。当在一个Silver Constituency Parses 的集合上训练 Transformer 时，它们配备 Pushdown Layers 可以在同样的批量大小下达到更好的 3-5 倍的样本效率，同时保持相似的折衔率。Pushdown Layers 是一种可替换的自注意层。我们通过对 GPT2-medium  WITH Pushdown Layers 在自动生成的 WikiText-103 上进行训练，来示例这一点。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Recent-Named-Entity-Recognition-and-Relation-Classification-Methods-with-Focus-on-Few-Shot-Learning-Approaches"><a href="#A-Survey-on-Recent-Named-Entity-Recognition-and-Relation-Classification-Methods-with-Focus-on-Few-Shot-Learning-Approaches" class="headerlink" title="A Survey on Recent Named Entity Recognition and Relation Classification Methods with Focus on Few-Shot Learning Approaches"></a>A Survey on Recent Named Entity Recognition and Relation Classification Methods with Focus on Few-Shot Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19055">http://arxiv.org/abs/2310.19055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sakher Alqaaidi, Elika Bozorgi</li>
<li>for: 本研究主要针对非结构化文本中的命名实体识别和关系类型分类两个关键阶段，以抽取有用信息。</li>
<li>methods: 本文主要介绍了最新的非结构化文本处理应用中的命名实体识别和关系类型分类方法，特别是几步学习方法。</li>
<li>results: 本文对两个领域的最新成果进行了比较分析，并对几步学习方法的结果进行了结构化分析。<details>
<summary>Abstract</summary>
Named entity recognition and relation classification are key stages for extracting information from unstructured text. Several natural language processing applications utilize the two tasks, such as information retrieval, knowledge graph construction and completion, question answering and other domain-specific applications, such as biomedical data mining. We present a survey of recent approaches in the two tasks with focus on few-shot learning approaches. Our work compares the main approaches followed in the two paradigms. Additionally, we report the latest metric scores in the two tasks with a structured analysis that considers the results in the few-shot learning scope.
</details>
<details>
<summary>摘要</summary>
Named entity recognition和关系分类是抽取无结构文本信息的关键阶段。许多自然语言处理应用程序利用这两个任务，如信息检索、知识图构建和完善、问答等领域应用程序，以及生物医学数据挖掘等领域应用程序。我们对最近的方法进行了评论，并对几种学习 paradigms进行了比较。此外，我们还对这两个任务中最新的 метри克分数进行了报告，并进行了结构化分析，考虑到几种少量学习范围内的结果。
</details></li>
</ul>
<hr>
<h2 id="ArBanking77-Intent-Detection-Neural-Model-and-a-New-Dataset-in-Modern-and-Dialectical-Arabic"><a href="#ArBanking77-Intent-Detection-Neural-Model-and-a-New-Dataset-in-Modern-and-Dialectical-Arabic" class="headerlink" title="ArBanking77: Intent Detection Neural Model and a New Dataset in Modern and Dialectical Arabic"></a>ArBanking77: Intent Detection Neural Model and a New Dataset in Modern and Dialectical Arabic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19034">http://arxiv.org/abs/2310.19034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustafa Jarrar, Ahmet Birim, Mohammed Khalilia, Mustafa Erden, Sana Ghanem</li>
<li>for: 本研究开发了一个大型的阿拉伯语言Intent检测dataset，名为ArBanking77，并将其 arabized 和 localized 到了英文 Banking77 dataset。</li>
<li>methods: 本研究使用了一个基于 AraBERT 的神经网络模型，并在 ArBanking77 上进行了 fine-tuning，以达到了 F1-score 的 0.9209 和 0.8995 在 Modern Standard Arabic 和 Palestinian dialect 中 respectively。</li>
<li>results: 本研究实现了对 live chat 查询中的实际应用，并在 simulated low-resource 环境下进行了广泛的实验，以评估模型在不同的情况下的表现。<details>
<summary>Abstract</summary>
This paper presents the ArBanking77, a large Arabic dataset for intent detection in the banking domain. Our dataset was arabized and localized from the original English Banking77 dataset, which consists of 13,083 queries to ArBanking77 dataset with 31,404 queries in both Modern Standard Arabic (MSA) and Palestinian dialect, with each query classified into one of the 77 classes (intents). Furthermore, we present a neural model, based on AraBERT, fine-tuned on ArBanking77, which achieved an F1-score of 0.9209 and 0.8995 on MSA and Palestinian dialect, respectively. We performed extensive experimentation in which we simulated low-resource settings, where the model is trained on a subset of the data and augmented with noisy queries to simulate colloquial terms, mistakes and misspellings found in real NLP systems, especially live chat queries. The data and the models are publicly available at https://sina.birzeit.edu/arbanking77.
</details>
<details>
<summary>摘要</summary>
Note: "AraBERT" is a pre-trained Arabic language model, similar to BERT.
</details></li>
</ul>
<hr>
<h2 id="SALMA-Arabic-Sense-Annotated-Corpus-and-WSD-Benchmarks"><a href="#SALMA-Arabic-Sense-Annotated-Corpus-and-WSD-Benchmarks" class="headerlink" title="SALMA: Arabic Sense-Annotated Corpus and WSD Benchmarks"></a>SALMA: Arabic Sense-Annotated Corpus and WSD Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19029">http://arxiv.org/abs/2310.19029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustafa Jarrar, Sanad Malaysha, Tymaa Hammouda, Mohammed Khalilia</li>
<li>for: 这个论文是为了描述一个新的阿拉伯语意义权重annotated corpus（SALMA），以及该 corpus 的注释工具和评估 metric。</li>
<li>methods: 这个论文使用了两种不同的意义инвенタри（Modern和Ghani）同时进行注释，并为每个词语提供了多个意义的分数。在注释过程中，研究人员还使用了六种名称实体的注释。</li>
<li>results: 研究人员通过了多种 metric（Kappa、Lineal Weighted Kappa、Quadratic Weighted Kappa、Mean Average Error、Root Mean Square Error）来评估注释质量，并发现了非常高的间接对应者一致性。此外，研究人员还开发了一个基于 Target Sense Verification 的 Word Sense Disambiguation 系统，并使用这个系统来评估三种 Target Sense Verification 模型的性能，其中最佳模型的准确率达到了 84.2%（使用 Modern）和 78.7%（使用 Ghani）。<details>
<summary>Abstract</summary>
SALMA, the first Arabic sense-annotated corpus, consists of ~34K tokens, which are all sense-annotated. The corpus is annotated using two different sense inventories simultaneously (Modern and Ghani). SALMA novelty lies in how tokens and senses are associated. Instead of linking a token to only one intended sense, SALMA links a token to multiple senses and provides a score to each sense. A smart web-based annotation tool was developed to support scoring multiple senses against a given word. In addition to sense annotations, we also annotated the corpus using six types of named entities. The quality of our annotations was assessed using various metrics (Kappa, Linear Weighted Kappa, Quadratic Weighted Kappa, Mean Average Error, and Root Mean Square Error), which show very high inter-annotator agreement. To establish a Word Sense Disambiguation baseline using our SALMA corpus, we developed an end-to-end Word Sense Disambiguation system using Target Sense Verification. We used this system to evaluate three Target Sense Verification models available in the literature. Our best model achieved an accuracy with 84.2% using Modern and 78.7% using Ghani. The full corpus and the annotation tool are open-source and publicly available at https://sina.birzeit.edu/salma/.
</details>
<details>
<summary>摘要</summary>
SALMA，首个阿拉伯语意义注释 корпу斯，包含约34000个字符，所有字符都有意义注释。 corpora 使用两个不同的意义集 simultaneously (Modern 和 Ghani)。 SALMA 的创新在于如何将字符和意义相关联。而不是将字符与一个固定的意义相关联，SALMA 将字符与多个意义相关联，并为每个意义提供一个分数。为支持多个意义对一个词的分数，我们开发了一个智能的网络基于的注释工具。此外，我们还对 corpora 进行了六种命名实体的注释。我们对注释的质量进行了多种 metric 评估（Kappa、线性权重Kappa、quadratic Weighted Kappa、平均误差和根平方误差），它们显示了非常高的间对注释者一致性。为建立基于我们 SALMA  корпу的单词意义推断基线，我们开发了一个 Target Sense Verification 基于的全 End-to-end Word Sense Disambiguation 系统。我们使用这个系统来评估Literature 中提供的三种 Target Sense Verification 模型。我们的最佳模型在Modern 和 Ghani 中达到了84.2%和78.7%的准确率。整个corpus 和注释工具都是开源的，可以在 <https://sina.birzeit.edu/salma/> 获取。
</details></li>
</ul>
<hr>
<h2 id="LLMs-and-Finetuning-Benchmarking-cross-domain-performance-for-hate-speech-detection"><a href="#LLMs-and-Finetuning-Benchmarking-cross-domain-performance-for-hate-speech-detection" class="headerlink" title="LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection"></a>LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18964">http://arxiv.org/abs/2310.18964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Nasir, Aadish Sharma, Kokil Jaidka</li>
<li>for: 这 paper 比较了不同的预训练和精度调整的大语言模型（LLMs）在仇恨言语检测中的表现。</li>
<li>methods: 本研究发现了cross-domain 适用性和过拟合风险是LLMs的主要挑战。我们通过评估发现了需要更多的标签多样性来让模型更好地捕捉仇恨言语的细节。</li>
<li>results: 我们的研究结果表明，通过适度调整和更多的标签多样性，可以提高模型的泛化性和检测精度。我们认为未来的仇恨言语检测应该强调cross-domain泛化和合适的benchmarking实践。<details>
<summary>Abstract</summary>
This paper compares different pre-trained and fine-tuned large language models (LLMs) for hate speech detection. Our research underscores challenges in LLMs' cross-domain validity and overfitting risks. Through evaluations, we highlight the need for fine-tuned models that grasp the nuances of hate speech through greater label heterogeneity. We conclude with a vision for the future of hate speech detection, emphasizing cross-domain generalizability and appropriate benchmarking practices.
</details>
<details>
<summary>摘要</summary>
这篇论文比较了不同的预训练和微调大型自然语言模型（LLM）对仇视言语检测的性能。我们的研究强调了LLM在不同领域的交叉领域有效性和过拟合风险。通过评估，我们强调需要微调模型，以便更好地捕捉仇视言语的细节和多样性。我们 conclude with a future vision for hate speech detection，强调跨领域一致性和合适的标准化实践。Note that the word " LL" in the original text was translated as "LLM" in Simplified Chinese, as "LL" is not a commonly used term in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="S2F-NER-Exploring-Sequence-to-Forest-Generation-for-Complex-Entity-Recognition"><a href="#S2F-NER-Exploring-Sequence-to-Forest-Generation-for-Complex-Entity-Recognition" class="headerlink" title="S2F-NER: Exploring Sequence-to-Forest Generation for Complex Entity Recognition"></a>S2F-NER: Exploring Sequence-to-Forest Generation for Complex Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18944">http://arxiv.org/abs/2310.18944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongxiu Xu, Heyan Huang, Yue Hu</li>
<li>for: 这篇论文主要针对复杂的实体识别问题（Named Entity Recognition，NER），例如嵌入、重叠和不连续的实体。</li>
<li>methods: 我们提出了一种新的序列到森林生成模式（Sequence-to-Forest，S2F-NER），它可以直接在句子中提取实体，而不是采用传统的序列到序列（Sequence-to-Sequence，Seq2Seq）生成模式。</li>
<li>results: 我们的模型在三个不连续NER数据集和两个嵌入NER数据集上表现出色，特别是对于不连续实体识别。<details>
<summary>Abstract</summary>
Named Entity Recognition (NER) remains challenging due to the complex entities, like nested, overlapping, and discontinuous entities. Existing approaches, such as sequence-to-sequence (Seq2Seq) generation and span-based classification, have shown impressive performance on various NER subtasks, but they are difficult to scale to datasets with longer input text because of either exposure bias issue or inefficient computation. In this paper, we propose a novel Sequence-to-Forest generation paradigm, S2F-NER, which can directly extract entities in sentence via a Forest decoder that decode multiple entities in parallel rather than sequentially. Specifically, our model generate each path of each tree in forest autoregressively, where the maximum depth of each tree is three (which is the shortest feasible length for complex NER and is far smaller than the decoding length of Seq2Seq). Based on this novel paradigm, our model can elegantly mitigates the exposure bias problem and keep the simplicity of Seq2Seq. Experimental results show that our model significantly outperforms the baselines on three discontinuous NER datasets and on two nested NER datasets, especially for discontinuous entity recognition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Retrofitting-Light-weight-Language-Models-for-Emotions-using-Supervised-Contrastive-Learning"><a href="#Retrofitting-Light-weight-Language-Models-for-Emotions-using-Supervised-Contrastive-Learning" class="headerlink" title="Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning"></a>Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18930">http://arxiv.org/abs/2310.18930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sapan Shah, Sreedhar Reddy, Pushpak Bhattacharyya</li>
<li>for: 这篇论文旨在探讨如何将情感方面的知识嵌入预训语言模型（BERT和RoBERTa）中，以提高模型的情感识别能力。</li>
<li>methods: 这篇论文使用对照学习方法将预训网络重新训练，使得文本片段具有相似情感时，在表现空间中被更加靠近地编码，而具有不同情感内容时则被推离。同时，这篇论文还确保了预训网络中对语言知识的不偏独影响。</li>
<li>results: 这篇论文的结果显示，使用这种方法更新预训网络的BERT和RoBERTa模型，可以实现情感识别的改进。对于情感分析和讽刺检测任务，这些模型比预训网络原始版本（约1%的提升）和其他已知方法更好。此外，这些更新后的模型在少量学习设定下表现更好。<details>
<summary>Abstract</summary>
We present a novel retrofitting method to induce emotion aspects into pre-trained language models (PLMs) such as BERT and RoBERTa. Our method updates pre-trained network weights using contrastive learning so that the text fragments exhibiting similar emotions are encoded nearby in the representation space, and the fragments with different emotion content are pushed apart. While doing so, it also ensures that the linguistic knowledge already present in PLMs is not inadvertently perturbed. The language models retrofitted by our method, i.e., BERTEmo and RoBERTaEmo, produce emotion-aware text representations, as evaluated through different clustering and retrieval metrics. For the downstream tasks on sentiment analysis and sarcasm detection, they perform better than their pre-trained counterparts (about 1% improvement in F1-score) and other existing approaches. Additionally, a more significant boost in performance is observed for the retrofitted models over pre-trained ones in few-shot learning setting.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的改进方法，用于启用语言模型（PLM）中的情感方面，如BERT和RoBERTa。我们的方法通过对预训练网络权重进行更新，使得表达同样情感的文本片段在表示空间中相近，而表达不同情感的片段则被推迟。同时，我们的方法 также确保了预训练语言模型中的语言知识不会偶然受到影响。我们修改后的语言模型，即BERTEmo和RoBERTaEmo，可以生成情感意识的文本表示，根据不同的聚类和检索指标进行评估。在情感分析和讽刺检测下投入下，它们与预训练模型（大约1%的提升）和其他现有方法相比，表现出较好的性能。此外，我们发现在少量学习 Setting中，修改后的模型比预训练模型表现更好，具有更大的提升。
</details></li>
</ul>
<hr>
<h2 id="Sentence-Bag-Graph-Formulation-for-Biomedical-Distant-Supervision-Relation-Extraction"><a href="#Sentence-Bag-Graph-Formulation-for-Biomedical-Distant-Supervision-Relation-Extraction" class="headerlink" title="Sentence Bag Graph Formulation for Biomedical Distant Supervision Relation Extraction"></a>Sentence Bag Graph Formulation for Biomedical Distant Supervision Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18912">http://arxiv.org/abs/2310.18912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Yang Liu, Xiaoyan Liu, Tianming Liang, Gaurav Sharma, Liang Xue, Maozu Guo</li>
<li>for: 提高生物医学数据中 distant supervision relation extraction 的精度和效果。</li>
<li>methods: 提出了一种基于图的框架，使用 message-passing 的信息汇集机制，解决了远级指导关系提取中的噪声标注问题，同时也能够有效地捕捉句子袋内sentence之间的依赖关系。</li>
<li>results: 在两个大规模生物医学关系集和 NYT 集上进行了广泛的实验，并证明了我们提出的方法可以在生物医学数据中 distant supervision relation extraction 中表现出色，同时也在普通文本挖掘领域中表现出优秀的relation extraction 能力。<details>
<summary>Abstract</summary>
We introduce a novel graph-based framework for alleviating key challenges in distantly-supervised relation extraction and demonstrate its effectiveness in the challenging and important domain of biomedical data. Specifically, we propose a graph view of sentence bags referring to an entity pair, which enables message-passing based aggregation of information related to the entity pair over the sentence bag. The proposed framework alleviates the common problem of noisy labeling in distantly supervised relation extraction and also effectively incorporates inter-dependencies between sentences within a bag. Extensive experiments on two large-scale biomedical relation datasets and the widely utilized NYT dataset demonstrate that our proposed framework significantly outperforms the state-of-the-art methods for biomedical distant supervision relation extraction while also providing excellent performance for relation extraction in the general text mining domain.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的图structure-based框架，用于解决远程supervised关系抽取中的一些主要挑战，并在生物医学数据中进行了实质性的证明。特别是，我们提出了一种将句子袋视为实体对的图视图，使得对于实体对的信息在句子袋中进行消息传递基于的聚合。该提议的框架可以解决远程supervised关系抽取中的常见问题，即标签杂乱，并有效地 incorporate句子之间的依赖关系。我们在两个大规模的生物医学关系数据集和常用的NYT数据集上进行了广泛的实验，得到了我们提议的框架在生物医学关系抽取中的显著超越州方法的性能，同时也在文本挖掘领域中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Pre-trained-Speech-Processing-Models-Contain-Human-Like-Biases-that-Propagate-to-Speech-Emotion-Recognition"><a href="#Pre-trained-Speech-Processing-Models-Contain-Human-Like-Biases-that-Propagate-to-Speech-Emotion-Recognition" class="headerlink" title="Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition"></a>Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18877">http://arxiv.org/abs/2310.18877</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/isaaconline/speat">https://github.com/isaaconline/speat</a></li>
<li>paper_authors: Isaac Slaughter, Craig Greenberg, Reva Schwartz, Aylin Caliskan</li>
<li>for: 这个研究旨在检测语音处理模型中的偏见，具体来说是检测预训练模型中的偏见。</li>
<li>methods: 这个研究使用了Speech Embedding Association Test（SpEAT）来检测预训练模型中的偏见。SpEAT是基于自然语言处理中的词嵌入协会测试，用于量化模型对不同概念的偏见，如种族、性别等。</li>
<li>results: 这个研究发现了14种预训练模型中的偏见，包括abled人群对disabled人群的正面偏见、欧洲裔美国人群对非洲裔美国人群的正面偏见、女性对♂的正面偏见、美国口音 speaker对非美国口音 speaker的正面偏见、年轻人群对老年人群的正面偏见。此外，研究还发现了这些偏见在下游任务Speech Emotion Recognition（SER）中的影响。在66个测试中（69%），由SpEAT测试发现的偏见与SER任务中的偏见相关。<details>
<summary>Abstract</summary>
Previous work has established that a person's demographics and speech style affect how well speech processing models perform for them. But where does this bias come from? In this work, we present the Speech Embedding Association Test (SpEAT), a method for detecting bias in one type of model used for many speech tasks: pre-trained models. The SpEAT is inspired by word embedding association tests in natural language processing, which quantify intrinsic bias in a model's representations of different concepts, such as race or valence (something's pleasantness or unpleasantness) and capture the extent to which a model trained on large-scale socio-cultural data has learned human-like biases. Using the SpEAT, we test for six types of bias in 16 English speech models (including 4 models also trained on multilingual data), which come from the wav2vec 2.0, HuBERT, WavLM, and Whisper model families. We find that 14 or more models reveal positive valence (pleasantness) associations with abled people over disabled people, with European-Americans over African-Americans, with females over males, with U.S. accented speakers over non-U.S. accented speakers, and with younger people over older people. Beyond establishing that pre-trained speech models contain these biases, we also show that they can have real world effects. We compare biases found in pre-trained models to biases in downstream models adapted to the task of Speech Emotion Recognition (SER) and find that in 66 of the 96 tests performed (69%), the group that is more associated with positive valence as indicated by the SpEAT also tends to be predicted as speaking with higher valence by the downstream model. Our work provides evidence that, like text and image-based models, pre-trained speech based-models frequently learn human-like biases. Our work also shows that bias found in pre-trained models can propagate to the downstream task of SER.
</details>
<details>
<summary>摘要</summary>
先前的研究已经证明人的民族和语言风格会影响语音处理模型对他们的性能。但是这种偏见来自哪里？在这项工作中，我们介绍了语音嵌入协会测试（SpEAT），用于检测语音处理模型中的偏见。SpEAT Draws inspiration from natural language processing中的嵌入协会测试，用于衡量不同概念的嵌入表示，如种族或语言风格，并捕捉模型从大规模社会文化数据中学习的人类化偏见。使用SpEAT，我们测试了16种英语语音模型（包括4种多语言模型），来自wav2vec 2.0、HuBERT、WavLM和Whisper模型家族。我们发现14个或更多的模型表现出了有利可能（愉悦）偏见，即abled人群比 disabled人群更有利可能，European-Americans比 African-Americans更有利可能，女性比男性更有利可能，U.S.口音说话者比非U.S.口音说话者更有利可能，以及年轻人比老年人更有利可能。我们不仅证明了语音处理模型中的这些偏见，还表明它们可能有实际的影响。我们比较了预训练模型中的偏见和下游任务speech emotion recognition（SER）模型中的偏见，发现在96次测试中（69%），与预训练模型中的偏见相关的组 Also tends to be predicted as speaking with higher valence by the downstream model。我们的工作证明了，如文本和图像基于模型一样，预训练语音基于模型经常学习人类化偏见。我们的工作还表明了预训练模型中的偏见可能会传播到下游任务中。
</details></li>
</ul>
<hr>
<h2 id="MUST-A-Multilingual-Student-Teacher-Learning-approach-for-low-resource-speech-recognition"><a href="#MUST-A-Multilingual-Student-Teacher-Learning-approach-for-low-resource-speech-recognition" class="headerlink" title="MUST: A Multilingual Student-Teacher Learning approach for low-resource speech recognition"></a>MUST: A Multilingual Student-Teacher Learning approach for low-resource speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18865">http://arxiv.org/abs/2310.18865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Umar Farooq, Rehan Ahmad, Thomas Hain</li>
<li>for: 本研究旨在解决语音识别系统训练中数据稀缺问题，通过学生教师学习（KD）方法。</li>
<li>methods: 本研究使用的方法包括提议一种多语言学生教师（MUST）学习方法，利用一个预训练的映射模型将教师语言的 posterior 映射到学生语言的 ASR 模型中。</li>
<li>results: 根据实验结果，使用 MUST 学习方法可以将Relative Character Error Rate（CER）降低到9.5%，相比基eline monolingual ASR 模型。<details>
<summary>Abstract</summary>
Student-teacher learning or knowledge distillation (KD) has been previously used to address data scarcity issue for training of speech recognition (ASR) systems. However, a limitation of KD training is that the student model classes must be a proper or improper subset of the teacher model classes. It prevents distillation from even acoustically similar languages if the character sets are not same. In this work, the aforementioned limitation is addressed by proposing a MUltilingual Student-Teacher (MUST) learning which exploits a posteriors mapping approach. A pre-trained mapping model is used to map posteriors from a teacher language to the student language ASR. These mapped posteriors are used as soft labels for KD learning. Various teacher ensemble schemes are experimented to train an ASR model for low-resource languages. A model trained with MUST learning reduces relative character error rate (CER) up to 9.5% in comparison with a baseline monolingual ASR.
</details>
<details>
<summary>摘要</summary>
学生教师学习或知识蒸馏（KD）已经曾用于解决训练语音识别（ASR）系统的数据稀缺问题。然而，KD 训练的一个限制是学生模型类型必须是教师模型类型的正确或错误子集。这会防止训练不同语言的扩展，即使字符集不同。在这种情况下，我们提出了一种多语言学生教师（MUST）学习方法，利用 posterior mapping 技术。我们使用一个预训练的映射模型将教师语言的 posterior 映射到学生语言 ASR 中。这些映射 posterior 用作 KD 学习的软标签。我们对各种教师集合方案进行了实验，以训练一个低资源语言的 ASR 模型。与基线单语言 ASR 模型相比，我们的 MUST 学习方法可以降低相对字符错误率（CER）的差异为9.5%。
</details></li>
</ul>
<hr>
<h2 id="Counterfactually-Probing-Language-Identity-in-Multilingual-Models"><a href="#Counterfactually-Probing-Language-Identity-in-Multilingual-Models" class="headerlink" title="Counterfactually Probing Language Identity in Multilingual Models"></a>Counterfactually Probing Language Identity in Multilingual Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18862">http://arxiv.org/abs/2310.18862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/venkatasg/multilingual-counterfactual-probing">https://github.com/venkatasg/multilingual-counterfactual-probing</a></li>
<li>paper_authors: Anirudh Srinivasan, Venkata S Govindarajan, Kyle Mahowald</li>
<li>for: 这 paper 探讨了语言模型中语言信息的组织方式，使用一种技术 called AlterRep 进行 counterfactual probing。</li>
<li>methods: 作者使用了一种 linear classifier 来解释 tokens 的语言标识 Task，并通过 counterfactual probing 方法来探讨模型的内部结构。</li>
<li>results: 研究发现，给定一个 Language X 模板，向 Language Y 方向推动 embedding 会系统性地增加 Language Y 词汇的概率，超过第三方控制语言。但是，这并不特别地推动模型转化为翻译相当的 Language Y 词汇。向 Language X 方向推动也有一定的效果，但是会有些程度下降。总之，这些结果表明大量多语言语言模型具有both语言特定和语言通用的结构，并且 counterfactual probing 可以成功应用于多语言模型。<details>
<summary>Abstract</summary>
Techniques in causal analysis of language models illuminate how linguistic information is organized in LLMs. We use one such technique, AlterRep, a method of counterfactual probing, to explore the internal structure of multilingual models (mBERT and XLM-R). We train a linear classifier on a binary language identity task, to classify tokens between Language X and Language Y. Applying a counterfactual probing procedure, we use the classifier weights to project the embeddings into the null space and push the resulting embeddings either in the direction of Language X or Language Y. Then we evaluate on a masked language modeling task. We find that, given a template in Language X, pushing towards Language Y systematically increases the probability of Language Y words, above and beyond a third-party control language. But it does not specifically push the model towards translation-equivalent words in Language Y. Pushing towards Language X (the same direction as the template) has a minimal effect, but somewhat degrades these models. Overall, we take these results as further evidence of the rich structure of massive multilingual language models, which include both a language-specific and language-general component. And we show that counterfactual probing can be fruitfully applied to multilingual models.
</details>
<details>
<summary>摘要</summary>
使用 causal 分析技术可以探索语言模型（mBERT 和 XLM-R）中的语言信息结构。我们使用一种方法——Counterfactual probing，以探索这些模型的内部结构。我们在一个 binary 语言标识任务上训练了一个线性分类器，以分类Token是来自哪种语言。通过对这些分类器权重进行Counterfactual probing操作，我们可以将表示Vector проек到null空间中，并将其推动向Language X 或 Language Y 方向。然后，我们在一个隐藏语言模型任务上进行评估。我们发现，当给定一个 Language X 模板时，推动向 Language Y 方向会系统地增加 Language Y 词汇的概率，而这与第三种控制语言相比，这种效果明显。但是，不会 Specifically push 模型向翻译相同的 Language Y 词汇方向。推动向 Language X （与模板相同的方向）的效果相对较小，但是会有一定程度的降低这些模型的性能。总之，我们认为这些结果是证明大型多语言语言模型具有了rich结构，包括语言特定和语言通用的组成部分。同时，我们示出了对多语言模型的 counterfactual probing 可以得到有用的结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/cs.CL_2023_10_29/" data-id="clogyj8x500dj7cra10vlf5a4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/cs.LG_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T10:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/cs.LG_2023_10_29/">cs.LG - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improved-Motor-Imagery-Classification-Using-Adaptive-Spatial-Filters-Based-on-Particle-Swarm-Optimization-Algorithm"><a href="#Improved-Motor-Imagery-Classification-Using-Adaptive-Spatial-Filters-Based-on-Particle-Swarm-Optimization-Algorithm" class="headerlink" title="Improved Motor Imagery Classification Using Adaptive Spatial Filters Based on Particle Swarm Optimization Algorithm"></a>Improved Motor Imagery Classification Using Adaptive Spatial Filters Based on Particle Swarm Optimization Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19202">http://arxiv.org/abs/2310.19202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiong Xiong, Ying Wang, Tianyuan Song, Jinguo Huang, Guixia Kang</li>
<li>for: 这个论文主要针对的应用领域是 robot控制、roke rehabilitation 和 stroke 或脊梁损伤患者的助手。</li>
<li>methods: 该论文提出了一种基于 particle swarm optimization 算法 (PSO) 的适应空间筛选解决方案，用于提取 MI-EEG 信号中更加有效的空间特征，以提高分类性能。</li>
<li>results:  Comparative experiments 表明，该提案的方法在两个公共数据集（2a 和 2b）上实现了显著的平均识别率提高，达到 74.61% 和 81.19% 分别。相比基eline algorithm（FBCSP），该提案的算法提高了 11.44% 和 7.11% 在两个数据集上。<details>
<summary>Abstract</summary>
As a typical self-paced brain-computer interface (BCI) system, the motor imagery (MI) BCI has been widely applied in fields such as robot control, stroke rehabilitation, and assistance for patients with stroke or spinal cord injury. Many studies have focused on the traditional spatial filters obtained through the common spatial pattern (CSP) method. However, the CSP method can only obtain fixed spatial filters for specific input signals. Besides, CSP method only focuses on the variance difference of two types of electroencephalogram (EEG) signals, so the decoding ability of EEG signals is limited. To obtain more effective spatial filters for better extraction of spatial features that can improve classification to MI-EEG, this paper proposes an adaptive spatial filter solving method based on particle swarm optimization algorithm (PSO). A training and testing framework based on filter bank and spatial filters (FBCSP-ASP) is designed for MI EEG signal classification. Comparative experiments are conducted on two public datasets (2a and 2b) from BCI competition IV, which show the outstanding average recognition accuracy of FBCSP-ASP. The proposed method has achieved significant performance improvement on MI-BCI. The classification accuracy of the proposed method has reached 74.61% and 81.19% on datasets 2a and 2b, respectively. Compared with the baseline algorithm (FBCSP), the proposed algorithm improves 11.44% and 7.11% on two datasets respectively. Furthermore, the analysis based on mutual information, t-SNE and Shapley values further proves that ASP features have excellent decoding ability for MI-EEG signals, and explains the improvement of classification performance by the introduction of ASP features.
</details>
<details>
<summary>摘要</summary>
如常的自适应脑机器接口（BCI）系统中，运动想象（MI）BCI已经广泛应用在机器人控制、roke rehabilitation 和roke 或脊梁损伤患者的助手等领域。许多研究都集中在传统的空间筛选（CSP）方法上。然而，CSP 方法只能获得特定输入信号的固定空间筛选。此外，CSP 方法只关注两种电enzephalogram（EEG）信号之间的差异，因此EEG 信号的解码能力受限。为了获得更有效的空间筛选，提高MI-EEG 信号的特征提取，这篇文章提出了基于聚合粒子猎 optimization 算法（PSO）的自适应空间筛选解决方案。为MI EEG 信号类型的分类，设计了一个基于筛选银行和空间筛选（FBCSP-ASP）的训练和测试框架。对于BCI 竞赛 IV 公共数据集（2a 和 2b）进行了比较性实验，实验结果表明，提案方法在MI-BCI 中 achieved significant performance improvement。提案方法的识别率为74.61% 和 81.19% 在数据集 2a 和 2b 中，相比基准算法（FBCSP），提案算法提高了11.44% 和 7.11% 在两个数据集中。此外，基于mutual information、t-SNE 和 Shapley 值的分析进一步证明了ASP 特征对MI-EEG 信号的解码能力具有优秀性，并解释了提案方法的性能提升原因。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Motor-Imagery-Decoding-in-Brain-Computer-Interfaces-using-Riemann-Tangent-Space-Mapping-and-Cross-Frequency-Coupling"><a href="#Enhancing-Motor-Imagery-Decoding-in-Brain-Computer-Interfaces-using-Riemann-Tangent-Space-Mapping-and-Cross-Frequency-Coupling" class="headerlink" title="Enhancing Motor Imagery Decoding in Brain Computer Interfaces using Riemann Tangent Space Mapping and Cross Frequency Coupling"></a>Enhancing Motor Imagery Decoding in Brain Computer Interfaces using Riemann Tangent Space Mapping and Cross Frequency Coupling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19198">http://arxiv.org/abs/2310.19198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiong Xiong, Li Su, Jinguo Huang, Guixia Kang<br>for: 这个论文的目的是提高motor imagery（MI）特征的编码和解码能力。methods: 这篇论文提出了一种基于Riemannian geometry和Cross-Frequency Coupling（CFC）的新方法，称为Riemann Tangent Space Mapping using Dichotomous Filter Bank with Convolutional Neural Network（DFBRTS），用于提高MI特征的表达质量和解码能力。DFBRTS使用了一个完整的二进制树结构的 dichotomous filter bank来精炼EEG信号，然后使用Riemann Tangent Space Mapping提取每个子带中的突出的EEG信号特征。最后，一个轻量级的卷积神经网络被用于进一步提取特征和分类，在彼此之间同时受到了cross-entropy和center loss的联合监督。results: 对于BCI竞赛IV 2a（BCIC-IV-2a）数据集和OpenBMI数据集进行了广泛的实验，DFBRTS在两个数据集上显示出了明显的优异性，在四个类和二个类的保留分类中分别达到了78.16%和71.58%的高精度分类率，与现有的参考值进行比较。<details>
<summary>Abstract</summary>
Objective: Motor Imagery (MI) serves as a crucial experimental paradigm within the realm of Brain Computer Interfaces (BCIs), aiming to decoding motor intentions from electroencephalogram (EEG) signals. Method: Drawing inspiration from Riemannian geometry and Cross-Frequency Coupling (CFC), this paper introduces a novel approach termed Riemann Tangent Space Mapping using Dichotomous Filter Bank with Convolutional Neural Network (DFBRTS) to enhance the representation quality and decoding capability pertaining to MI features. DFBRTS first initiates the process by meticulously filtering EEG signals through a Dichotomous Filter Bank, structured in the fashion of a complete binary tree. Subsequently, it employs Riemann Tangent Space Mapping to extract salient EEG signal features within each sub-band. Finally, a lightweight convolutional neural network is employed for further feature extraction and classification, operating under the joint supervision of cross-entropy and center loss. To validate the efficacy, extensive experiments were conducted using DFBRTS on two well-established benchmark datasets: the BCI competition IV 2a (BCIC-IV-2a) dataset and the OpenBMI dataset. The performance of DFBRTS was benchmarked against several state-of-the-art MI decoding methods, alongside other Riemannian geometry-based MI decoding approaches. Results: DFBRTS significantly outperforms other MI decoding algorithms on both datasets, achieving a remarkable classification accuracy of 78.16% for four-class and 71.58% for two-class hold-out classification, as compared to the existing benchmarks.
</details>
<details>
<summary>摘要</summary>
目的：使用电气生物 интерфей斯（BCI）中的动作幻像（MI）作为关键实验方法，从电气生物学信号（EEG）中提取动作意图。方法：基于里敦纬度 geometry和跨频相关（CFC），这篇论文提出了一种新的方法，即里敦 Tangent Space Mapping using Dichotomous Filter Bank with Convolutional Neural Network（DFBRTS），以提高MI特征的表达质量和解码能力。DFBRTS首先通过完整的 binary tree 结构的 dichotomous Filter Bank 精细筛选 EEG 信号，然后使用里敦 Tangent Space Mapping 提取每个子带中的优秀 EEG 信号特征。最后，一种轻量级的 convolutional neural network 进行进一步的特征提取和分类，在joint 超VI 和中心损失的协同监督下运行。为证明DFBRTS的效果，对DFBRTS进行了广泛的实验，并与其他里敦 geometry 基于MI解码方法进行比较。结果：DFBRTS在两个常用的benchmark数据集上（BCIC-IV-2a数据集和OpenBMI数据集）上显著地超过了其他MI解码方法，达到了78.16%的四类分类率和71.58%的二类分类率。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Normalization-in-Recurrent-Neural-Network-of-Grid-Cells"><a href="#Conformal-Normalization-in-Recurrent-Neural-Network-of-Grid-Cells" class="headerlink" title="Conformal Normalization in Recurrent Neural Network of Grid Cells"></a>Conformal Normalization in Recurrent Neural Network of Grid Cells</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19192">http://arxiv.org/abs/2310.19192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dehong Xu, Ruiqi Gao, Wen-Hao Zhang, Xue-Xin Wei, Ying Nian Wu</li>
<li>for: 该研究探讨了grid cells在脑内的响应模式，以及这些模式如何影响agent的 Navigation。</li>
<li>methods: 研究使用了高维神经网络，并提出了一种简单而普遍的准确normalization方法，以便在agent移动时保持响应vector的正确尺寸。</li>
<li>results: 实验结果表明，通过使用准确normalization方法，grid cells可以形成六角形响应模式，并且这些模式与agent的实际位置在2D physical space有直接的关系。<details>
<summary>Abstract</summary>
Grid cells in the entorhinal cortex of the mammalian brain exhibit striking hexagon firing patterns in their response maps as the animal (e.g., a rat) navigates in a 2D open environment. The responses of the population of grid cells collectively form a vector in a high-dimensional neural activity space, and this vector represents the self-position of the agent in the 2D physical space. As the agent moves, the vector is transformed by a recurrent neural network that takes the velocity of the agent as input. In this paper, we propose a simple and general conformal normalization of the input velocity for the recurrent neural network, so that the local displacement of the position vector in the high-dimensional neural space is proportional to the local displacement of the agent in the 2D physical space, regardless of the direction of the input velocity. Our numerical experiments on the minimally simple linear and non-linear recurrent networks show that conformal normalization leads to the emergence of the hexagon grid patterns. Furthermore, we derive a new theoretical understanding that connects conformal normalization to the emergence of hexagon grid patterns in navigation tasks.
</details>
<details>
<summary>摘要</summary>
“ENTORHINAL CORTEX中的格子细胞在动物（例如鼠）在2D开放环境中探索时表现出惊人的六角发射模式。这些细胞的响应集体形成一个高维神经活动空间中的向量，该向量表示动物的自身位置在2D物理空间中。随着动物的移动，这个向量被一个循环神经网络转换，该神经网络的输入是动物的速度。在这篇论文中，我们提出了一种简单而普遍的几何正常化方法，以确保输入速度的local displacement在高维神经空间中与动物在2D物理空间中的local displacement成正比。我们的数值实验表明，几何正常化会导致格子网格模式的出现。此外，我们还derived一种新的理论理解，该理解连接几何正常化与导航任务中的格子网格模式的出现。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="The-Power-of-Explainability-in-Forecast-Informed-Deep-Learning-Models-for-Flood-Mitigation"><a href="#The-Power-of-Explainability-in-Forecast-Informed-Deep-Learning-Models-for-Flood-Mitigation" class="headerlink" title="The Power of Explainability in Forecast-Informed Deep Learning Models for Flood Mitigation"></a>The Power of Explainability in Forecast-Informed Deep Learning Models for Flood Mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19166">http://arxiv.org/abs/2310.19166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jimeng Shi, Vitalii Stebliankin, Giri Narasimhan</li>
<li>for: 这 paper 是为了提出一种基于深度学习架构的洪水管理方法，以优化洪水预 release 的决策。</li>
<li>methods: 这 paper 使用了 Forecast Informed Deep Learning Architecture (FIDLAR)， combinig 预测和深度学习来实现洪水管理。</li>
<li>results: 实验结果表明，FIDLAR 比现有的状态艺术有几个数量级的速度提高，并且可以提供更好的预 release 决策。这些速度提高使得 FIDLAR 可以用于实时洪水管理。此外，这 paper 还使用了工具来解释模型的决策，从而更好地理解洪水管理中环境因素的贡献。<details>
<summary>Abstract</summary>
Floods can cause horrific harm to life and property. However, they can be mitigated or even avoided by the effective use of hydraulic structures such as dams, gates, and pumps. By pre-releasing water via these structures in advance of extreme weather events, water levels are sufficiently lowered to prevent floods. In this work, we propose FIDLAR, a Forecast Informed Deep Learning Architecture, achieving flood management in watersheds with hydraulic structures in an optimal manner by balancing out flood mitigation and unnecessary wastage of water via pre-releases. We perform experiments with FIDLAR using data from the South Florida Water Management District, which manages a coastal area that is highly prone to frequent storms and floods. Results show that FIDLAR performs better than the current state-of-the-art with several orders of magnitude speedup and with provably better pre-release schedules. The dramatic speedups make it possible for FIDLAR to be used for real-time flood management. The main contribution of this paper is the effective use of tools for model explainability, allowing us to understand the contribution of the various environmental factors towards its decisions.
</details>
<details>
<summary>摘要</summary>
洪水可以带来惊人的破坏力和财产损失。然而，通过有效地使用 гидро利用结构，如坝、闸门和泵，可以减轻或缓解洪水的影响。在这种情况下，我们提出了 FIDLAR，一种基于预测的深度学习架构，通过在洪水事件前预先释放水，以达到Optimal flood management in watersheds with hydraulic structures by balancing flood mitigation and water wastage via pre-releases.我们在使用南佛瑞达水资源管理区的数据进行实验，这是一个常遇洪水的沿海地区。结果显示，FIDLAR比当前状态艺术有几个数量级的速度减速，并且可以证明更好的预release schedule。这些减速使得FIDLAR可以用于实时洪水管理。本文的主要贡献是通过工具来描述模型的解释，以便理解响应不同环境因素的决策的贡献。
</details></li>
</ul>
<hr>
<h2 id="RAIFLE-Reconstruction-Attacks-on-Interaction-based-Federated-Learning-with-Active-Data-Manipulation"><a href="#RAIFLE-Reconstruction-Attacks-on-Interaction-based-Federated-Learning-with-Active-Data-Manipulation" class="headerlink" title="RAIFLE: Reconstruction Attacks on Interaction-based Federated Learning with Active Data Manipulation"></a>RAIFLE: Reconstruction Attacks on Interaction-based Federated Learning with Active Data Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19163">http://arxiv.org/abs/2310.19163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dzungvpham/raifle">https://github.com/dzungvpham/raifle</a></li>
<li>paper_authors: Dzung Pham, Shreyas Kulkarni, Amir Houmansadr</li>
<li>for: 这个论文关注了 federated learning (FL) 中的用户隐私问题，具体来说是在用户互动域中的 recommender systems (RS) 和 online learning to rank (OLTR) 中。</li>
<li>methods: 这篇论文使用了一种名为 RAIFLE 的整合优化基于攻击框架，用于攻击 IFL 系统中的用户隐私。RAIFLE 使用了一种新的攻击技术名为 Active Data Manipulation (ADM)，通过在训练特征上操纵ITEMS来导致本地 FL 更新中的 adversarial 行为。</li>
<li>results: 论文表明 RAIFLE 可以在 IFL 系统中更有效地攻击用户隐私，并且可以干扰隐私防御技术，如安全汇聚和私人信息检索。基于这些发现，论文提出了一些Countermeasure 建议来 mitigate 这种攻击。<details>
<summary>Abstract</summary>
Federated learning (FL) has recently emerged as a privacy-preserving approach for machine learning in domains that rely on user interactions, particularly recommender systems (RS) and online learning to rank (OLTR). While there has been substantial research on the privacy of traditional FL, little attention has been paid to studying the privacy properties of these interaction-based FL (IFL) systems. In this work, we show that IFL can introduce unique challenges concerning user privacy, particularly when the central server has knowledge and control over the items that users interact with. Specifically, we demonstrate the threat of reconstructing user interactions by presenting RAIFLE, a general optimization-based reconstruction attack framework customized for IFL. RAIFLE employs Active Data Manipulation (ADM), a novel attack technique unique to IFL, where the server actively manipulates the training features of the items to induce adversarial behaviors in the local FL updates. We show that RAIFLE is more impactful than existing FL privacy attacks in the IFL context, and describe how it can undermine privacy defenses like secure aggregation and private information retrieval. Based on our findings, we propose and discuss countermeasure guidelines to mitigate our attack in the context of federated RS/OLTR specifically and IFL more broadly.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）已经被认为是一种保护用户隐私的机器学习方法，尤其是在用户互动域中，如推荐系统（RS）和在线学习排名（OLTR）等领域。 although there has been extensive research on the privacy of traditional FL, little attention has been paid to the privacy properties of these interaction-based FL（IFL）systems. in this work, we show that IFL can introduce unique challenges concerning user privacy, particularly when the central server has knowledge and control over the items that users interact with. specifically, we demonstrate the threat of reconstructing user interactions by presenting RAIFLE, a general optimization-based reconstruction attack framework customized for IFL. RAIFLE employs Active Data Manipulation（ADM）, a novel attack technique unique to IFL, where the server actively manipulates the training features of the items to induce adversarial behaviors in the local FL updates. we show that RAIFLE is more impactful than existing FL privacy attacks in the IFL context, and describe how it can undermine privacy defenses like secure aggregation and private information retrieval. based on our findings, we propose and discuss countermeasure guidelines to mitigate our attack in the context of federated RS/OLTR specifically and IFL more broadly.
</details></li>
</ul>
<hr>
<h2 id="Transfer-Learning-in-Transformer-Based-Demand-Forecasting-For-Home-Energy-Management-System"><a href="#Transfer-Learning-in-Transformer-Based-Demand-Forecasting-For-Home-Energy-Management-System" class="headerlink" title="Transfer Learning in Transformer-Based Demand Forecasting For Home Energy Management System"></a>Transfer Learning in Transformer-Based Demand Forecasting For Home Energy Management System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19159">http://arxiv.org/abs/2310.19159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gargya Gokhale, Jonas Van Gompel, Bert Claessens, Chris Develder</li>
<li>for: 这个研究旨在开发一个基于转移学习的家用电力负载预测模型，以提高家用电力负载预测的精度和效率。</li>
<li>methods: 研究人员使用了一种名为“时间融合 трансформа”的先进预测模型，并通过将这个全球模型调整到新的一个家用电力负载数据中，以提高预测的精度和效率。</li>
<li>results: 研究人员发现，使用转移学习设置可以比仅使用单一家用电力负载数据更好地预测家用电力负载，具体而言，可以降低预测误差率约15%，并且可以降低家用电力负载成本约2%。<details>
<summary>Abstract</summary>
Increasingly, homeowners opt for photovoltaic (PV) systems and/or battery storage to minimize their energy bills and maximize renewable energy usage. This has spurred the development of advanced control algorithms that maximally achieve those goals. However, a common challenge faced while developing such controllers is the unavailability of accurate forecasts of household power consumption, especially for shorter time resolutions (15 minutes) and in a data-efficient manner. In this paper, we analyze how transfer learning can help by exploiting data from multiple households to improve a single house's load forecasting. Specifically, we train an advanced forecasting model (a temporal fusion transformer) using data from multiple different households, and then finetune this global model on a new household with limited data (i.e. only a few days). The obtained models are used for forecasting power consumption of the household for the next 24 hours~(day-ahead) at a time resolution of 15 minutes, with the intention of using these forecasts in advanced controllers such as Model Predictive Control. We show the benefit of this transfer learning setup versus solely using the individual new household's data, both in terms of (i) forecasting accuracy ($\sim$15\% MAE reduction) and (ii) control performance ($\sim$2\% energy cost reduction), using real-world household data.
</details>
<details>
<summary>摘要</summary>
HOMEOWNERS 对光伏系统和/或电池储存系统的选择在增加，以最大化能源成本和可再生能源使用。这导致了高级控制算法的发展，以最大化这些目标。然而，开发这些控制器时常遇到缺乏精确的家用电力消耗预测，特别是在短时间尺度（15分钟）和高效率下。在这篇文章中，我们分析了如何使用传播学习来解决这个问题。我们使用多个不同的家庭的数据来训练进阶预测模型（时间融合变换器），然后在新的家庭中进行精确化训练（仅使用几天的数据）。所得到的模型用于预测新家庭的电力消耗预测，时间尺度为24小时（日前），每15分钟一次。我们显示了这个传播学习设置的优点，包括预测精度（约15% MAE减少）和控制性能（约2%能源成本减少），使用实际家庭数据进行评估。
</details></li>
</ul>
<hr>
<h2 id="Real-World-Implementation-of-Reinforcement-Learning-Based-Energy-Coordination-for-a-Cluster-of-Households"><a href="#Real-World-Implementation-of-Reinforcement-Learning-Based-Energy-Coordination-for-a-Cluster-of-Households" class="headerlink" title="Real-World Implementation of Reinforcement Learning Based Energy Coordination for a Cluster of Households"></a>Real-World Implementation of Reinforcement Learning Based Energy Coordination for a Cluster of Households</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19155">http://arxiv.org/abs/2310.19155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gargya Gokhale, Niels Tiben, Marie-Sophie Verwee, Manu Lahariya, Bert Claessens, Chris Develder<br>for: 这 paper 的目的是研究如何通过聚合控制多幢住宅建筑物来为现代电力网提供支持服务，包括备用服务。methods: 这 paper 使用了学习反馈控制（RL）技术来协调8幢住宅建筑物的电力消耗，不需要任何建筑模型或模拟器，因此实施和扩展非常方便。results: 这 paper 通过实验示出了RL基于排名系统选择哪些户型动用可变资产，并使用实时PI控制机制来控制选择的资产，实现了功能的电力跟踪和RL基于数据驱动的排名效果的可行性。<details>
<summary>Abstract</summary>
Given its substantial contribution of 40\% to global power consumption, the built environment has received increasing attention to serve as a source of flexibility to assist the modern power grid. In that respect, previous research mainly focused on energy management of individual buildings. In contrast, in this paper, we focus on aggregated control of a set of residential buildings, to provide grid supporting services, that eventually should include ancillary services. In particular, we present a real-life pilot study that studies the effectiveness of reinforcement-learning (RL) in coordinating the power consumption of 8 residential buildings to jointly track a target power signal. Our RL approach relies solely on observed data from individual households and does not require any explicit building models or simulators, making it practical to implement and easy to scale. We show the feasibility of our proposed RL-based coordination strategy in a real-world setting. In a 4-week case study, we demonstrate a hierarchical control system, relying on an RL-based ranking system to select which households to activate flex assets from, and a real-time PI control-based power dispatch mechanism to control the selected assets. Our results demonstrate satisfactory power tracking, and the effectiveness of the RL-based ranks which are learnt in a purely data-driven manner.
</details>
<details>
<summary>摘要</summary>
由于它的严重贡献了40%的全球电力消耗，建筑环境在现代电力网络中获得了越来越多的注意力，以满足需求。在这个意义上，之前的研究主要集中在建筑物之间的能源管理。相比之下，在这篇论文中，我们将关注一组住宅建筑物的总控制，以为电力网络提供支持服务，最终应包括辅助服务。具体来说，我们将展示一个实际的 Pilot 研究，研究使用强化学习（RL）来协调8个住宅建筑物的电力消耗，以同步跟踪目标电力信号。我们的RL方法不需要任何建筑物模型或模拟器，因此实施可行和扩展容易。我们在实际情况下展示了我们提议的RL-基于协调策略的可行性。在4个星期的案例研究中，我们实现了一个层次控制系统，通过RL-基于排名系统来选择需要活动的资产，并使用实时PI控制-基于的电力派发机制来控制选择的资产。我们的结果表明了满意的电力跟踪，以及RL-基于排名系统的学习效果，这些排名系统是通过实际数据驱动学习而学习的。
</details></li>
</ul>
<hr>
<h2 id="MAG-GNN-Reinforcement-Learning-Boosted-Graph-Neural-Network"><a href="#MAG-GNN-Reinforcement-Learning-Boosted-Graph-Neural-Network" class="headerlink" title="MAG-GNN: Reinforcement Learning Boosted Graph Neural Network"></a>MAG-GNN: Reinforcement Learning Boosted Graph Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19142">http://arxiv.org/abs/2310.19142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, Muhan Zhang</li>
<li>for: 提高Graph Neural Networks（GNNs）的结构编码能力，以提高GNNs的表达能力。</li>
<li>methods: 使用搜索算法来选择一小subset of subgraphs，并使用强化学习（RL）Agent来更新subgraph set，以提高GNNs的表达能力。</li>
<li>results: 在多个 datasets 上进行了广泛的实验，显示了 MAG-GNN 可以与现有方法竞争，甚至超过一些subgraph GNNs，同时也可以减少subgraph GNNs 的运行时间。<details>
<summary>Abstract</summary>
While Graph Neural Networks (GNNs) recently became powerful tools in graph learning tasks, considerable efforts have been spent on improving GNNs' structural encoding ability. A particular line of work proposed subgraph GNNs that use subgraph information to improve GNNs' expressivity and achieved great success. However, such effectivity sacrifices the efficiency of GNNs by enumerating all possible subgraphs. In this paper, we analyze the necessity of complete subgraph enumeration and show that a model can achieve a comparable level of expressivity by considering a small subset of the subgraphs. We then formulate the identification of the optimal subset as a combinatorial optimization problem and propose Magnetic Graph Neural Network (MAG-GNN), a reinforcement learning (RL) boosted GNN, to solve the problem. Starting with a candidate subgraph set, MAG-GNN employs an RL agent to iteratively update the subgraphs to locate the most expressive set for prediction. This reduces the exponential complexity of subgraph enumeration to the constant complexity of a subgraph search algorithm while keeping good expressivity. We conduct extensive experiments on many datasets, showing that MAG-GNN achieves competitive performance to state-of-the-art methods and even outperforms many subgraph GNNs. We also demonstrate that MAG-GNN effectively reduces the running time of subgraph GNNs.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 在图学任务中最近成为了强大工具，但是大量的工作被投入到了GNNs的结构编码能力的提高中。一种特定的工作提出了子图GNNs，使用子图信息来提高GNNs的表达能力，并取得了很大的成功。然而，这种表达能力来源于完全对所有可能的子图进行枚举，这会导致GNNs的效率下降。在这篇论文中，我们分析了完全子图枚举的必要性，并证明了一个模型可以通过考虑一小部分的子图来达到相似的表达能力。然后，我们将这个问题转化为一个 combinatorial 优化问题，并提出了磁矢量图神经网络（MAG-GNN）来解决这个问题。MAG-GNN从候选子图集开始，使用了一个强化学习（RL）的代理人来逐步更新子图，以查找最有表达力的集合用于预测。这将枚举子图的枚举复杂度从对数复杂度降低到常数复杂度，保持好的表达能力。我们在许多数据集上进行了广泛的实验，显示MAG-GNN与当前状态的方法竞争，甚至超过了许多子图GNNs。我们还证明了MAG-GNN可以有效减少子图GNNs的运行时间。
</details></li>
</ul>
<hr>
<h2 id="Worst-case-Performance-of-Popular-Approximate-Nearest-Neighbor-Search-Implementations-Guarantees-and-Limitations"><a href="#Worst-case-Performance-of-Popular-Approximate-Nearest-Neighbor-Search-Implementations-Guarantees-and-Limitations" class="headerlink" title="Worst-case Performance of Popular Approximate Nearest Neighbor Search Implementations: Guarantees and Limitations"></a>Worst-case Performance of Popular Approximate Nearest Neighbor Search Implementations: Guarantees and Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19126">http://arxiv.org/abs/2310.19126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piotr Indyk, Haike Xu</li>
<li>for: 这些纸都是为了研究近似 neighboor搜索算法的最坏情况性能的。</li>
<li>methods: 这些算法包括HNSW、NSG和DiskANN。</li>
<li>results: 我们发现，对于DiskANN的”慢预处理”版本，它可以在数据集中有 bounded “内在”维度时支持常数准确度和多余logarithmic查询时间的近似最近邻搜索查询。对于其他数据结构variant studied，包括DiskANN的”快预处理”版本、HNSW和NSG，我们提出了一家实例集，其中查询过程可以 linear in instance size 的时间内返回”合理”的准确率。例如，对于DiskANN，我们显示了，在实例大小为 n 时，查询过程至少需要 0.1 n 步骤才能查找其中的5个最近邻。<details>
<summary>Abstract</summary>
Graph-based approaches to nearest neighbor search are popular and powerful tools for handling large datasets in practice, but they have limited theoretical guarantees. We study the worst-case performance of recent graph-based approximate nearest neighbor search algorithms, such as HNSW, NSG and DiskANN. For DiskANN, we show that its "slow preprocessing" version provably supports approximate nearest neighbor search query with constant approximation ratio and poly-logarithmic query time, on data sets with bounded "intrinsic" dimension. For the other data structure variants studied, including DiskANN with "fast preprocessing", HNSW and NSG, we present a family of instances on which the empirical query time required to achieve a "reasonable" accuracy is linear in instance size. For example, for DiskANN, we show that the query procedure can take at least $0.1 n$ steps on instances of size $n$ before it encounters any of the $5$ nearest neighbors of the query.
</details>
<details>
<summary>摘要</summary>
Graph-based方法是实际中处理大数据集的强大工具，但它们在理论上有限的保证。我们研究近期的图形基于近似最近邻搜索算法，如HNSW、NSG和DiskANN的最坏情况性能。对于DiskANN，我们表明其"慢预处理"版本可以在数据集中的"内在维度"是bounded时提供常数准确率和多项几何查询时间的近似最近邻搜索查询。对其他数据结构变体，包括DiskANN的"快预处理"版本、HNSW和NSG，我们提供了一家实际上的实例，其查询时间与实例大小线性相关。例如，对于DiskANN，我们表明其查询过程可以在实例大小为$n$时间内至少执行$0.1n$步骤才会遇到5个最近邻居。
</details></li>
</ul>
<hr>
<h2 id="Software-engineering-for-deep-learning-applications-usage-of-SWEng-and-MLops-tools-in-GitHub-repositories"><a href="#Software-engineering-for-deep-learning-applications-usage-of-SWEng-and-MLops-tools-in-GitHub-repositories" class="headerlink" title="Software engineering for deep learning applications: usage of SWEng and MLops tools in GitHub repositories"></a>Software engineering for deep learning applications: usage of SWEng and MLops tools in GitHub repositories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19124">http://arxiv.org/abs/2310.19124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evangelia Panourgia, Theodoros Plessas, Diomidis Spinellis</li>
<li>for: 这篇论文主要关注于深度学习（DL）软件开发中的软件工程（SE）实践，特别是DL软件开发中的工程挑战和资料驱动的非决定性模式。</li>
<li>methods: 本研究使用Python为主要编程语言，采用 précédente MSR 研究的工具使用方法，扫描GitHub上popular的应用DL项目，探索这些项目中的SE工具使用情况。</li>
<li>results: 研究发现，大约70%的GitHub库中包含至少一个SE工具，软件配置管理工具是最多使用的，而维护工具则较少使用。另外，MLOps工具的使用相对较少，只有9个工具在该样本中被使用。TensorBoard是唯一在对�る repository 中使用的MLOps工具。<details>
<summary>Abstract</summary>
The rising popularity of deep learning (DL) methods and techniques has invigorated interest in the topic of SE4DL, the application of software engineering (SE) practices on deep learning software. Despite the novel engineering challenges brought on by the data-driven and non-deterministic paradigm of DL software, little work has been invested into developing AI-targeted SE tools. On the other hand, tools tackling more general engineering issues in DL are actively used and referred to under the umbrella term of ``MLOps tools''. Furthermore, the available literature supports the utility of conventional SE tooling in DL software development. Building upon previous MSR research on tool usage in open-source software works, we identify conventional and MLOps tools adopted in popular applied DL projects that use Python as the main programming language. About 70% of the GitHub repositories mined contained at least one conventional SE tool. Software configuration management tools are the most adopted, while the opposite applies to maintenance tools. Substantially fewer MLOps tools were in use, with only 9 tools out of a sample of 80 used in at least one repository. The majority of them were open-source rather than proprietary. One of these tools, TensorBoard, was found to be adopted in about half of the repositories in our study. Consequently, the use of conventional SE tooling demonstrates its relevance to DL software. Further research is recommended on the adoption of MLOps tooling by open-source projects, focusing on the relevance of particular tool types, the development of required tools, as well as ways to promote the use of already available tools.
</details>
<details>
<summary>摘要</summary>
随着深度学习（DL）方法和技术的普及，关注SE4DL（深度学习软件工程）领域的应用而增加。然而，由于深度学习软件的数据驱动和不确定的理论带来的新的工程挑战，对于AI目标的SE工具仍然受到了少量投入。相反，关于更一般的机器学习（ML）工程问题，如MLOps工具，活跃地使用和引用。此外，现有的文献支持传统的SE工具在深度学习软件开发中的可用性。基于之前的微软研究人员在开源软件项目中工具使用情况，我们识别了传统和MLOps工具在流行的应用深度学习项目中的采用情况。我们发现，大约70%的GitHub存储库包含至少一个传统SE工具。软件配置管理工具是最广泛采用的，而维护工具则相对较少。与此同时，MLOps工具的采用远远少于传统SE工具，只有80个存储库中的9个被使用。大多数这些工具是开源的，而不是商业化的。 tensorBoard 是这些工具中的一个，在我们的研究中被采用的约半数。因此，传统SE工具在深度学习软件开发中的使用表明了它们的重要性。进一步的研究建议在开源项目中采用MLOps工具，特别是关注特定工具类型、开发需要的工具以及如何促进现有工具的使用。
</details></li>
</ul>
<hr>
<h2 id="Proving-Linear-Mode-Connectivity-of-Neural-Networks-via-Optimal-Transport"><a href="#Proving-Linear-Mode-Connectivity-of-Neural-Networks-via-Optimal-Transport" class="headerlink" title="Proving Linear Mode Connectivity of Neural Networks via Optimal Transport"></a>Proving Linear Mode Connectivity of Neural Networks via Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19103">http://arxiv.org/abs/2310.19103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/9aze/ot_lmc">https://github.com/9aze/ot_lmc</a></li>
<li>paper_authors: Damien Ferbach, Baptiste Goujaud, Gauthier Gidel, Aymeric Dieuleveut</li>
<li>for: 本研究探讨了高维非凸优化问题的能量景观，以解释现代深度神经网络架构的效果。</li>
<li>methods: 本文提出了一种理论框架，可以理解现有两次权重训练后得到的两个解的连续性。基于 Wasserstein 距离的整体速度，我们表明了两个够宽的两层神经网络，通过权重训练来连续地连接。</li>
<li>results: 我们提供了一种上下限bounds，可以量化每层神经网络的宽度，以便确保连续性。此外，我们还经验表明了积分权重分布的维度与连续性之间的相关性。<details>
<summary>Abstract</summary>
The energy landscape of high-dimensional non-convex optimization problems is crucial to understanding the effectiveness of modern deep neural network architectures. Recent works have experimentally shown that two different solutions found after two runs of a stochastic training are often connected by very simple continuous paths (e.g., linear) modulo a permutation of the weights. In this paper, we provide a framework theoretically explaining this empirical observation. Based on convergence rates in Wasserstein distance of empirical measures, we show that, with high probability, two wide enough two-layer neural networks trained with stochastic gradient descent are linearly connected. Additionally, we express upper and lower bounds on the width of each layer of two deep neural networks with independent neuron weights to be linearly connected. Finally, we empirically demonstrate the validity of our approach by showing how the dimension of the support of the weight distribution of neurons, which dictates Wasserstein convergence rates is correlated with linear mode connectivity.
</details>
<details>
<summary>摘要</summary>
高维非对称优化问题的能量景观对现代深度神经网络架构的效果是关键。latest studies have shown that two different solutions found after two runs of stochastic training are often connected by very simple continuous paths (e.g., linear) modulo a permutation of the weights. In this paper, we provide a theoretical framework to explain this empirical observation. Based on the convergence rates of empirical measures in Wasserstein distance, we show that, with high probability, two wide enough two-layer neural networks trained with stochastic gradient descent are linearly connected. Additionally, we provide upper and lower bounds on the width of each layer of two deep neural networks with independent neuron weights to be linearly connected. Finally, we empirically demonstrate the validity of our approach by showing how the dimension of the support of the weight distribution of neurons, which dictates Wasserstein convergence rates, is correlated with linear mode connectivity.
</details></li>
</ul>
<hr>
<h2 id="Atom-Low-bit-Quantization-for-Efficient-and-Accurate-LLM-Serving"><a href="#Atom-Low-bit-Quantization-for-Efficient-and-Accurate-LLM-Serving" class="headerlink" title="Atom: Low-bit Quantization for Efficient and Accurate LLM Serving"></a>Atom: Low-bit Quantization for Efficient and Accurate LLM Serving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19102">http://arxiv.org/abs/2310.19102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, Baris Kasikci</li>
<li>for: 提高 Large Language Models（LLMs）在内容生成、智能客服和情感分析等应用中的效率，以适应应用场景中的增长需求。</li>
<li>methods: 使用批处理技术批处理多个请求，以提高 GPU 资源的使用效率和throughput。</li>
<li>results: 在服务器上实现高 durchput 提高 ($7.73\times$ 比FP16和 $2.53\times$ 比INT8 归一化)，同时保持同样的响应时间目标，而不会增加精度损失。<details>
<summary>Abstract</summary>
The growing demand for Large Language Models (LLMs) in applications such as content generation, intelligent chatbots, and sentiment analysis poses considerable challenges for LLM service providers. To efficiently use GPU resources and boost throughput, batching multiple requests has emerged as a popular paradigm; to further speed up batching, LLM quantization techniques reduce memory consumption and increase computing capacity. However, prevalent quantization schemes (e.g., 8-bit weight-activation quantization) cannot fully leverage the capabilities of modern GPUs, such as 4-bit integer operators, resulting in sub-optimal performance.   To maximize LLMs' serving throughput, we introduce Atom, a low-bit quantization method that achieves high throughput improvements with negligible accuracy loss. Atom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization. It attains high accuracy by applying a novel mixed-precision and fine-grained quantization process. We evaluate Atom on 4-bit weight-activation quantization setups in the serving context. Atom improves end-to-end throughput by up to $7.73\times$ compared to the FP16 and by $2.53\times$ compared to INT8 quantization, while maintaining the same latency target.
</details>
<details>
<summary>摘要</summary>
受大语言模型（LLM）应用的内容生成、智能客服和情感分析等领域的需求不断增长， LLM 服务提供商面临着巨大的挑战。为了高效利用 GPU 资源并提高通过put，批处多个请求已成为了流行的方法；而为了进一步加速批处， LLM 量化技术可以降低内存占用量和提高计算能力。然而，现有的量化方案（如 8 位 weight-activation 量化）无法完全利用现代 GPU 的能力，导致性能下降。为了最大化 LLM 的服务通过put，我们介绍 Atom，一种低位量化方法，可以 achieve high throughput improvements with negligible accuracy loss。Atom 使用低位操作和减少内存占用量，以提高服务通过put。它通过应用一种新的混合精度和细致的量化过程，保持高度的准确性。我们在 4 位 weight-activation 量化设置下测试 Atom。Atom 可以提高终端通过put的吞吐量，比FP16和 INT8 量化的吞吐量高出 $7.73\times$，而且保持同样的响应时间目标。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Towards-an-Expanded-Toolkit-for-ML-Supported-Decision-Making-in-the-Public-Sector"><a href="#Bridging-the-Gap-Towards-an-Expanded-Toolkit-for-ML-Supported-Decision-Making-in-the-Public-Sector" class="headerlink" title="Bridging the Gap: Towards an Expanded Toolkit for ML-Supported Decision-Making in the Public Sector"></a>Bridging the Gap: Towards an Expanded Toolkit for ML-Supported Decision-Making in the Public Sector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19091">http://arxiv.org/abs/2310.19091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Unai Fischer Abaigar, Christoph Kern, Noam Barda, Frauke Kreuter</li>
<li>For: This paper aims to bridge the gap between machine learning (ML) and public sector decision-making by addressing key technical challenges that arise when aligning intricate policy objectives with the precise formalization requirements of ML models.* Methods: The paper concentrates on pivotal points of the ML pipeline that connect the model to its operational environment, including the significance of representative training data and the importance of a model setup that facilitates effective decision-making. The paper also links these challenges with emerging methodological advancements, such as causal ML, domain adaptation, uncertainty quantification, and multi-objective optimization.* Results: The paper provides a comprehensive overview of the challenges that arise when using ML in the public sector, and highlights the importance of addressing these challenges in order to harmonize ML and public sector objectives. The paper also illustrates the path forward for addressing these challenges, including the use of emerging methodological advancements.<details>
<summary>Abstract</summary>
Machine Learning (ML) systems are becoming instrumental in the public sector, with applications spanning areas like criminal justice, social welfare, financial fraud detection, and public health. While these systems offer great potential benefits to institutional decision-making processes, such as improved efficiency and reliability, they still face the challenge of aligning intricate and nuanced policy objectives with the precise formalization requirements necessitated by ML models. In this paper, we aim to bridge the gap between ML and public sector decision-making by presenting a comprehensive overview of key technical challenges where disjunctions between policy goals and ML models commonly arise. We concentrate on pivotal points of the ML pipeline that connect the model to its operational environment, delving into the significance of representative training data and highlighting the importance of a model setup that facilitates effective decision-making. Additionally, we link these challenges with emerging methodological advancements, encompassing causal ML, domain adaptation, uncertainty quantification, and multi-objective optimization, illustrating the path forward for harmonizing ML and public sector objectives.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>训练数据的选择和调整：ML 模型的精度和可靠性受到训练数据的影响，但是公共部门的决策过程中的训练数据可能不够完整或不够有代表性。2. 模型设置的构成和调整：为了使 ML 模型能够实际地支持公共部门的决策过程，需要适当地设置和调整模型的参数和架构。3. 适用于公共部门的 ML 技术发展：包括 causal ML、领域适应、uncertainty quantification 和多目标优化在内的新技术可以帮助解决 ML 和公共部门之间的匹配问题。本文通过聚焦 ML pipeline 中的关键点子，探讨 ML 模型如何与公共部门的决策过程进行匹配，并提出了一些实际的方法来解决这些挑战。这些方法包括：1. 使用更多的代表性丰富的训练数据来优化 ML 模型的性能。2. 适当地设置和调整 ML 模型的参数和架构，以便更好地支持公共部门的决策过程。3. 采用新的 ML 技术，例如 causal ML、领域适应、uncertainty quantification 和多目标优化，来解决 ML 和公共部门之间的匹配问题。</details></li>
</ol>
<hr>
<h2 id="Efficient-Cluster-Selection-for-Personalized-Federated-Learning-A-Multi-Armed-Bandit-Approach"><a href="#Efficient-Cluster-Selection-for-Personalized-Federated-Learning-A-Multi-Armed-Bandit-Approach" class="headerlink" title="Efficient Cluster Selection for Personalized Federated Learning: A Multi-Armed Bandit Approach"></a>Efficient Cluster Selection for Personalized Federated Learning: A Multi-Armed Bandit Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19069">http://arxiv.org/abs/2310.19069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhou Ni, Morteza Hashemi</li>
<li>for: 这篇论文目的是为了解决在人工智能学习中的联邦学习网络中的问题，特别是在变化很大的数据分布和设备能力下。</li>
<li>methods: 这篇论文使用了一种名为“动态Upper Confidence Bound”的算法，它是基于多臂枪（MAB）的方法，用于在联邦学习网络中聚合用户。这个算法可以将新用户的数据分布与最佳的聚合群相匹配。</li>
<li>results: 这篇论文的实验结果显示，在不同的数据分布和设备能力下，这个算法可以有效地处理变化很大的联邦学习enario。<details>
<summary>Abstract</summary>
Federated learning (FL) offers a decentralized training approach for machine learning models, prioritizing data privacy. However, the inherent heterogeneity in FL networks, arising from variations in data distribution, size, and device capabilities, poses challenges in user federation. Recognizing this, Personalized Federated Learning (PFL) emphasizes tailoring learning processes to individual data profiles. In this paper, we address the complexity of clustering users in PFL, especially in dynamic networks, by introducing a dynamic Upper Confidence Bound (dUCB) algorithm inspired by the multi-armed bandit (MAB) approach. The dUCB algorithm ensures that new users can effectively find the best cluster for their data distribution by balancing exploration and exploitation. The performance of our algorithm is evaluated in various cases, showing its effectiveness in handling dynamic federated learning scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Sketching-Algorithms-for-Sparse-Dictionary-Learning-PTAS-and-Turnstile-Streaming"><a href="#Sketching-Algorithms-for-Sparse-Dictionary-Learning-PTAS-and-Turnstile-Streaming" class="headerlink" title="Sketching Algorithms for Sparse Dictionary Learning: PTAS and Turnstile Streaming"></a>Sketching Algorithms for Sparse Dictionary Learning: PTAS and Turnstile Streaming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19068">http://arxiv.org/abs/2310.19068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregory Dexter, Petros Drineas, David P. Woodruff, Taisuke Yasuda</li>
<li>for: 这 paper 的目的是扩展 sketching 算法的应用范围，包括稀疏字储学习和 EUCLIDEAN $k$-means 归类问题。</li>
<li>methods: 这 paper 使用了新的技术来推广 sketching 算法的应用范围，包括一种新的 PTAS 方法和新的上界和下界。</li>
<li>results: 这 paper 得到了一些新的结果，包括一种新的 PTAS 方法和新的上界和下界，以及一些关于 dictionary learning 和 $k$-means 归类问题的研究。<details>
<summary>Abstract</summary>
Sketching algorithms have recently proven to be a powerful approach both for designing low-space streaming algorithms as well as fast polynomial time approximation schemes (PTAS). In this work, we develop new techniques to extend the applicability of sketching-based approaches to the sparse dictionary learning and the Euclidean $k$-means clustering problems. In particular, we initiate the study of the challenging setting where the dictionary/clustering assignment for each of the $n$ input points must be output, which has surprisingly received little attention in prior work. On the fast algorithms front, we obtain a new approach for designing PTAS's for the $k$-means clustering problem, which generalizes to the first PTAS for the sparse dictionary learning problem. On the streaming algorithms front, we obtain new upper bounds and lower bounds for dictionary learning and $k$-means clustering. In particular, given a design matrix $\mathbf A\in\mathbb R^{n\times d}$ in a turnstile stream, we show an $\tilde O(nr/\epsilon^2 + dk/\epsilon)$ space upper bound for $r$-sparse dictionary learning of size $k$, an $\tilde O(n/\epsilon^2 + dk/\epsilon)$ space upper bound for $k$-means clustering, as well as an $\tilde O(n)$ space upper bound for $k$-means clustering on random order row insertion streams with a natural "bounded sensitivity" assumption. On the lower bounds side, we obtain a general $\tilde\Omega(n/\epsilon + dk/\epsilon)$ lower bound for $k$-means clustering, as well as an $\tilde\Omega(n/\epsilon^2)$ lower bound for algorithms which can estimate the cost of a single fixed set of candidate centers.
</details>
<details>
<summary>摘要</summary>
algorithm 已经证明是一种强大的方法，不仅用于设计具有低空间流处理器的算法，也用于快速的多项时间算法（PTAS）。在这个工作中，我们开发了新的技术，以扩展画 sketching 方法的应用范围至简短字典学习和欧几何 $k$-means 聚类问题。具体来说，我们开始研究具有复杂的设定，其中每个输入点的字典/聚类分配必须被输出，这个问题在先前的工作中很少获得关注。在快速算法方面，我们取得了一新的方法，用于设计 PTAS 的 $k$-means 聚类问题，这个方法可扩展到简短字典学习问题的首次 PTAS。在流处理算法方面，我们取得了新的上界和下界，用于字典学习和 $k$-means 聚类问题。具体来说，在turnstile流中，我们显示了一个 $\tilde O(nr/\epsilon^2 + dk/\epsilon)$ 的空间上界，用于 $r$-简字典学习的大小为 $k$，以及一个 $\tilde O(n/\epsilon^2 + dk/\epsilon)$ 的空间上界，用于 $k$-means 聚类问题。此外，我们还取得了一个 $\tilde O(n)$ 的空间上界，用于 $k$-means 聚类在随机排序推入流中。在下界方面，我们取得了一个通用的 $\tilde\Omega(n/\epsilon + dk/\epsilon)$ 下界，用于 $k$-means 聚类问题，以及一个 $\tilde\Omega(n/\epsilon^2)$ 下界，用于可以估计单一集合中心的成本的算法。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-LLP-Methods-Challenges-and-Approaches"><a href="#Evaluating-LLP-Methods-Challenges-and-Approaches" class="headerlink" title="Evaluating LLP Methods: Challenges and Approaches"></a>Evaluating LLP Methods: Challenges and Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19065">http://arxiv.org/abs/2310.19065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gaabrielfranco/llp-variants-datasets-benchmarks">https://github.com/gaabrielfranco/llp-variants-datasets-benchmarks</a></li>
<li>paper_authors: Gabriel Franco, Giovanni Comarela, Mark Crovella</li>
<li>for: 本研究是为了解决Label Proportions（LLP）问题，这是一个机器学习问题，有很多实际应用。</li>
<li>methods: 本研究使用了生成 variant-specific 数据集，以捕捉不同的 dependence structure 和 bag 特性。此外，还使用了一种新的模型选择方法，以适应 LLQ 问题的特殊性。</li>
<li>results: 研究发现，选择最佳算法需要考虑不同的 LLP 变种和模型选择方法。通过对一些常见 LLQ 算法进行了广泛的比较， demonstrate 了需要我们提出的方法。<details>
<summary>Abstract</summary>
Learning from Label Proportions (LLP) is an established machine learning problem with numerous real-world applications. In this setting, data items are grouped into bags, and the goal is to learn individual item labels, knowing only the features of the data and the proportions of labels in each bag. Although LLP is a well-established problem, it has several unusual aspects that create challenges for benchmarking learning methods. Fundamental complications arise because of the existence of different LLP variants, i.e., dependence structures that can exist between items, labels, and bags. Accordingly, the first algorithmic challenge is the generation of variant-specific datasets capturing the diversity of dependence structures and bag characteristics. The second methodological challenge is model selection, i.e., hyperparameter tuning; due to the nature of LLP, model selection cannot easily use the standard machine learning paradigm. The final benchmarking challenge consists of properly evaluating LLP solution methods across various LLP variants. We note that there is very little consideration of these issues in prior work, and there are no general solutions for these challenges proposed to date. To address these challenges, we develop methods capable of generating LLP datasets meeting the requirements of different variants. We use these methods to generate a collection of datasets encompassing the spectrum of LLP problem characteristics, which can be used in future evaluation studies. Additionally, we develop guidelines for benchmarking LLP algorithms, including the model selection and evaluation steps. Finally, we illustrate the new methods and guidelines by performing an extensive benchmark of a set of well-known LLP algorithms. We show that choosing the best algorithm depends critically on the LLP variant and model selection method, demonstrating the need for our proposed approach.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we develop methods capable of generating LLP datasets meeting the requirements of different variants. We use these methods to generate a collection of datasets encompassing the spectrum of LLP problem characteristics, which can be used in future evaluation studies. Additionally, we develop guidelines for benchmarking LLP algorithms, including the model selection and evaluation steps. Finally, we illustrate the new methods and guidelines by performing an extensive benchmark of a set of well-known LLP algorithms. We show that choosing the best algorithm depends critically on the LLP variant and model selection method, demonstrating the need for our proposed approach.翻译结果：LLP（学习从标签分量）是一个已经有长期应用的机器学习问题，具有许多实际应用场景。在这个设定中，数据项目被分组为袋子，目标是从数据特征和标签分量中学习各自的项目标签。虽然LLP是一个已知的问题，但它有一些不寻常的特点，导致评估学习方法的挑战。主要的挑战包括：1. 生成 variant-specific 数据集，捕捉不同的依赖结构和袋子特征的多样性。2. 因为 LLP 的特点，选择最佳模型不能使用标准机器学习范文。3. 评估 LLP 解决方案的多样性，以确保它们在不同的 LLP 变体中表现良好。为了解决这些挑战，我们开发了生成 LLP 数据集的方法，以满足不同变体的需求。我们使用这些方法生成了一系列包含 LLP 问题特征谱的数据集，可以在未来的评估研究中使用。此外，我们还提供了评估 LLP 算法的指南，包括模型选择和评估步骤。最后，我们使用新方法和指南对一组知名 LLP 算法进行了广泛的比较。我们发现，选择最佳算法取决于 LLP 变体和模型选择方法，这说明了我们的提出的方法的需要。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-the-Learnability-of-Apple-Tasting"><a href="#Revisiting-the-Learnability-of-Apple-Tasting" class="headerlink" title="Revisiting the Learnability of Apple Tasting"></a>Revisiting the Learnability of Apple Tasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19064">http://arxiv.org/abs/2310.19064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinod Raman, Unique Subedi, Ananth Raman, Ambuj Tewari</li>
<li>for: 研究在apple tasting反馈下的在线分类问题。</li>
<li>methods: 使用 combinatorial perspective 研究在线学习可能性，并提出了一个新的参数Effective width，用于量化在可 realizable 设定下的最差预期错误数。</li>
<li>results: 在 realizable 设定下，showed that the expected number of mistakes for any learner under apple tasting feedback can only be $\Theta(1), \Theta(\sqrt{T})$, or $\Theta(T)$。<details>
<summary>Abstract</summary>
In online binary classification under \textit{apple tasting} feedback, the learner only observes the true label if it predicts "1". First studied by \cite{helmbold2000apple}, we revisit this classical partial-feedback setting and study online learnability from a combinatorial perspective. We show that the Littlestone dimension continues to prove a tight quantitative characterization of apple tasting in the agnostic setting, closing an open question posed by \cite{helmbold2000apple}. In addition, we give a new combinatorial parameter, called the Effective width, that tightly quantifies the minimax expected mistakes in the realizable setting. As a corollary, we use the Effective width to establish a \textit{trichotomy} of the minimax expected number of mistakes in the realizable setting. In particular, we show that in the realizable setting, the expected number of mistakes for any learner under apple tasting feedback can only be $\Theta(1), \Theta(\sqrt{T})$, or $\Theta(T)$.
</details>
<details>
<summary>摘要</summary>
在在线二分类学习中，学习者只会看到真实标签，如果预测结果为1。这个问题最早由Helmbold等人（2000）研究，我们现在从 combinatorial 角度重新研究这个古典的partial-feedback 设定，并证明 Littlestone 维度仍然是agnostic 设定中的一个紧张量量化 caracterization。此外，我们还提出了一个新的 combinatorial 参数，called Effective width，它紧密地量化了可行情况下的最差预期错误。为此，我们使用 Effective width 证明了可行情况下的最差预期错误数可以只是 $\Theta(1), \Theta(\sqrt{T})$ 或 $\Theta(T)$。
</details></li>
</ul>
<hr>
<h2 id="Feature-Aggregation-in-Joint-Sound-Classification-and-Localization-Neural-Networks"><a href="#Feature-Aggregation-in-Joint-Sound-Classification-and-Localization-Neural-Networks" class="headerlink" title="Feature Aggregation in Joint Sound Classification and Localization Neural Networks"></a>Feature Aggregation in Joint Sound Classification and Localization Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19063">http://arxiv.org/abs/2310.19063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brendan Healy, Patrick McNamee, Zahra Nili Ahmadabadi</li>
<li>for: 本研究探讨了深度学习技术在共同声音信号分类和地点化网络中的应用。现有状态的声音源地点化深度学习网络缺乏特征聚合在其架构中。特征聚合可以提高模型性能，因为它使得不同特征尺度上的信息可以被集成，从而提高特征Robustness和不变性。这特别重要在SSL网络中，因为它们必须在直接和间接声音信号之间进行区分。为解决这个漏洞，我们将计算机视觉网络中的特征聚合技术应用到声音检测网络中。</li>
<li>methods: 我们采用了计算机视觉网络中的特征聚合技术，包括Path Aggregation Network (PANet)、Weighted Bi-directional Feature Pyramid Network (BiFPN)和Scale Encoding Network (SEN)等。这些技术被integrated into a SSL control architecture，并被评估使用两种声音分类和两种方向射 regression 的指标。PANet和BiFPN是计算机视觉模型中已知的聚合器，而我们提议的SEN是更加压缩的聚合器。</li>
<li>results: 结果表明，包含特征聚合的模型在声音分类和地点化方面的性能都高于控制模型，即Sound Event Localization and Detection network (SELDnet)。特征聚合技术提高了声音检测神经网络的性能，特别是在方向射 regression 方面。<details>
<summary>Abstract</summary>
This study addresses the application of deep learning techniques in joint sound signal classification and localization networks. Current state-of-the-art sound source localization deep learning networks lack feature aggregation within their architecture. Feature aggregation enhances model performance by enabling the consolidation of information from different feature scales, thereby improving feature robustness and invariance. This is particularly important in SSL networks, which must differentiate direct and indirect acoustic signals. To address this gap, we adapt feature aggregation techniques from computer vision neural networks to signal detection neural networks. Additionally, we propose the Scale Encoding Network (SEN) for feature aggregation to encode features from various scales, compressing the network for more computationally efficient aggregation. To evaluate the efficacy of feature aggregation in SSL networks, we integrated the following computer vision feature aggregation sub-architectures into a SSL control architecture: Path Aggregation Network (PANet), Weighted Bi-directional Feature Pyramid Network (BiFPN), and SEN. These sub-architectures were evaluated using two metrics for signal classification and two metrics for direction-of-arrival regression. PANet and BiFPN are established aggregators in computer vision models, while the proposed SEN is a more compact aggregator. The results suggest that models incorporating feature aggregations outperformed the control model, the Sound Event Localization and Detection network (SELDnet), in both sound signal classification and localization. The feature aggregation techniques enhance the performance of sound detection neural networks, particularly in direction-of-arrival regression.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Escaping-Saddle-Points-in-Heterogeneous-Federated-Learning-via-Distributed-SGD-with-Communication-Compression"><a href="#Escaping-Saddle-Points-in-Heterogeneous-Federated-Learning-via-Distributed-SGD-with-Communication-Compression" class="headerlink" title="Escaping Saddle Points in Heterogeneous Federated Learning via Distributed SGD with Communication Compression"></a>Escaping Saddle Points in Heterogeneous Federated Learning via Distributed SGD with Communication Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19059">http://arxiv.org/abs/2310.19059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijin Chen, Zhize Li, Yuejie Chi</li>
<li>For: 提高 federated learning（FL）中communication efficiency和学习精度的问题。* Methods: 提出了一种新的error-feedback scheme，实现了在不同客户端数据异ogeneous的情况下，通过压缩信息进行分布式SGD算法的实现。* Results: 证明了Power-EF算法可以在不同客户端数据异ogeneous情况下，逃脱平均点，并且在第二阶段 convergence 中，展现出线性增长。<details>
<summary>Abstract</summary>
We consider the problem of finding second-order stationary points of heterogeneous federated learning (FL). Previous works in FL mostly focus on first-order convergence guarantees, which do not rule out the scenario of unstable saddle points. Meanwhile, it is a key bottleneck of FL to achieve communication efficiency without compensating the learning accuracy, especially when local data are highly heterogeneous across different clients. Given this, we propose a novel algorithm Power-EF that only communicates compressed information via a novel error-feedback scheme. To our knowledge, Power-EF is the first distributed and compressed SGD algorithm that provably escapes saddle points in heterogeneous FL without any data homogeneity assumptions. In particular, Power-EF improves to second-order stationary points after visiting first-order (possibly saddle) points, using additional gradient queries and communication rounds only of almost the same order required by first-order convergence, and the convergence rate exhibits a linear speedup in terms of the number of workers. Our theory improves/recovers previous results, while extending to much more tolerant settings on the local data. Numerical experiments are provided to complement the theory.
</details>
<details>
<summary>摘要</summary>
我们考虑到寻找非常复杂的联邦学习（FL）中的第二阶站点问题。前一些FL工作主要集中在第一阶均衡保证，这不能排除不稳定的阶均点的情况。另一方面，在FL中实现通信效率不损学习精度的挑战，尤其是当地方数据具有很高的不同客户端的多样性时。为了解决这个问题，我们提出了一个新的算法Power-EF，它仅在一个新的错误反馈方案下进行压缩通信。我们知道Power-EF是首个分布式压缩SGD算法，可以在不同客户端的数据多样性下，避免阶均点而实现第二阶站点，并且在额外的梯度询问和通信轮次上进行几乎相同的复杂度。我们的理论提高了/恢复了先前的结果，同时扩展到许多更允许的本地数据设置。实验数据来补充理论。
</details></li>
</ul>
<hr>
<h2 id="Object-centric-architectures-enable-efficient-causal-representation-learning"><a href="#Object-centric-architectures-enable-efficient-causal-representation-learning" class="headerlink" title="Object-centric architectures enable efficient causal representation learning"></a>Object-centric architectures enable efficient causal representation learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19054">http://arxiv.org/abs/2310.19054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Mansouri, Jason Hartford, Yan Zhang, Yoshua Bengio</li>
<li>for: 这篇论文旨在探讨如何在多个物体的观察数据上实现 causal representation learning，以实现对每个物体的属性的分离。</li>
<li>methods: 该论文使用了对象中心学习和 causal representation learning 的最新发展，通过修改 Slot Attention 架构，使用稀有的干扰来强制实现对每个物体的属性的分离。</li>
<li>results: 该论文在一系列简单的图像基于的分离实验中成功地分离了一组物体的属性，并且需要更少的干扰than comparable approach 。<details>
<summary>Abstract</summary>
Causal representation learning has showed a variety of settings in which we can disentangle latent variables with identifiability guarantees (up to some reasonable equivalence class). Common to all of these approaches is the assumption that (1) the latent variables are represented as $d$-dimensional vectors, and (2) that the observations are the output of some injective generative function of these latent variables. While these assumptions appear benign, we show that when the observations are of multiple objects, the generative function is no longer injective and disentanglement fails in practice. We can address this failure by combining recent developments in object-centric learning and causal representation learning. By modifying the Slot Attention architecture arXiv:2006.15055, we develop an object-centric architecture that leverages weak supervision from sparse perturbations to disentangle each object's properties. This approach is more data-efficient in the sense that it requires significantly fewer perturbations than a comparable approach that encodes to a Euclidean space and we show that this approach successfully disentangles the properties of a set of objects in a series of simple image-based disentanglement experiments.
</details>
<details>
<summary>摘要</summary>
causal representation learning 在多种设置中展示了可以分离干扰变量的可靠性保证 ( hasta certain extent 的等价类). 这些方法假设：1) 干扰变量是 $d$-维 вектор表示; 2) 观察是这些干扰变量的生成函数的输出。 although these assumptions seem innocuous, we show that when the observations are of multiple objects, the generative function is no longer injective and disentanglement fails in practice. 我们可以通过结合近期的对象中心学习和 causal representation learning 来解决这种失败。 我们修改了 arXiv:2006.15055 中的槽注意架构，以便在 sparse perturbations 的 weak supervision 下，为每个对象分离其特性。 这种方法比一种在 Euclidean space 中编码并且需要更少的扰动而言，我们展示了这种方法可以成功地分离一系列的图像基于的对象分离实验中的对象特性。
</details></li>
</ul>
<hr>
<h2 id="Datasets-and-Benchmarks-for-Nanophotonic-Structure-and-Parametric-Design-Simulations"><a href="#Datasets-and-Benchmarks-for-Nanophotonic-Structure-and-Parametric-Design-Simulations" class="headerlink" title="Datasets and Benchmarks for Nanophotonic Structure and Parametric Design Simulations"></a>Datasets and Benchmarks for Nanophotonic Structure and Parametric Design Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19053">http://arxiv.org/abs/2310.19053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jungtaekkim/nanophotonic-structures">https://github.com/jungtaekkim/nanophotonic-structures</a></li>
<li>paper_authors: Jungtaek Kim, Mingxuan Li, Oliver Hinder, Paul W. Leu</li>
<li>for: 这个论文主要针对的应用是设计和理解奈米光学结构，以实现太阳能电池、反射层、电磁干扰屏蔽、光滤波器和LED等多种应用。</li>
<li>methods: 这篇论文使用了电动力学模拟来模拟电磁场的时间变化和光学性质。同时，它还提出了一些参数结构设计问题的评价框架和标准。</li>
<li>results: 研究人员通过对不同Grid大小的电动力学模拟进行比较，发现可以通过灵活地选择评价精度来提高结构设计。此外，他们还提出了一些参数结构设计问题的解决方案。<details>
<summary>Abstract</summary>
Nanophotonic structures have versatile applications including solar cells, anti-reflective coatings, electromagnetic interference shielding, optical filters, and light emitting diodes. To design and understand these nanophotonic structures, electrodynamic simulations are essential. These simulations enable us to model electromagnetic fields over time and calculate optical properties. In this work, we introduce frameworks and benchmarks to evaluate nanophotonic structures in the context of parametric structure design problems. The benchmarks are instrumental in assessing the performance of optimization algorithms and identifying an optimal structure based on target optical properties. Moreover, we explore the impact of varying grid sizes in electrodynamic simulations, shedding light on how evaluation fidelity can be strategically leveraged in enhancing structure designs.
</details>
<details>
<summary>摘要</summary>
几何光子结构具有多方面应用，包括太阳能电池、反射层、电磁干扰隔绝、光滤波器和发光二极管。为设计和理解这些几何光子结构，电动力学模拟是必备的。这些模拟可以模拟电磁场过时的变化，并计算光学性能。在这个工作中，我们介绍了框架和参考标准，用于评估几何光子结构在参数结构设计问题中的性能。这些参考标准可以评估优化算法的性能，并帮助选择基于目标光学性能的最佳结构。此外，我们还探讨了在电动力学模拟中不同格子大小的影响，照明了如何积极地利用评估实价来提升结构设计。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Permutation-Tests-Applications-to-Kernel-Methods"><a href="#Differentially-Private-Permutation-Tests-Applications-to-Kernel-Methods" class="headerlink" title="Differentially Private Permutation Tests: Applications to Kernel Methods"></a>Differentially Private Permutation Tests: Applications to Kernel Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19043">http://arxiv.org/abs/2310.19043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antoninschrab/dpkernel-paper">https://github.com/antoninschrab/dpkernel-paper</a></li>
<li>paper_authors: Ilmun Kim, Antonin Schrab</li>
<li>for: 隐私保护的敏感数据分析</li>
<li>methods: 使用差异性保护的排序测试（differentially private permutation tests），extend classical non-private permutation tests to private settings，maintain both finite-sample validity and differential privacy</li>
<li>results: 提出了 differentially private kernel tests（dpMMD和dpHSIC），可以在不同的隐私环境下实现最佳的能力，实现了在Synthetic和实际场景下的竞争力比较Here’s the breakdown of each point:</li>
<li>for: The paper is written for the purpose of privacy-preserving data analysis, specifically in the context of hypothesis testing.</li>
<li>methods: The paper introduces differentially private permutation tests as a way to extend classical non-private permutation tests to private settings while maintaining both finite-sample validity and differential privacy.</li>
<li>results: The paper proposes two differentially private kernel tests (dpMMD and dpHSIC) that can achieve optimal power under different privacy regimes, and demonstrates their competitive power through empirical evaluations on synthetic and real-world data.<details>
<summary>Abstract</summary>
Recent years have witnessed growing concerns about the privacy of sensitive data. In response to these concerns, differential privacy has emerged as a rigorous framework for privacy protection, gaining widespread recognition in both academic and industrial circles. While substantial progress has been made in private data analysis, existing methods often suffer from impracticality or a significant loss of statistical efficiency. This paper aims to alleviate these concerns in the context of hypothesis testing by introducing differentially private permutation tests. The proposed framework extends classical non-private permutation tests to private settings, maintaining both finite-sample validity and differential privacy in a rigorous manner. The power of the proposed test depends on the choice of a test statistic, and we establish general conditions for consistency and non-asymptotic uniform power. To demonstrate the utility and practicality of our framework, we focus on reproducing kernel-based test statistics and introduce differentially private kernel tests for two-sample and independence testing: dpMMD and dpHSIC. The proposed kernel tests are straightforward to implement, applicable to various types of data, and attain minimax optimal power across different privacy regimes. Our empirical evaluations further highlight their competitive power under various synthetic and real-world scenarios, emphasizing their practical value. The code is publicly available to facilitate the implementation of our framework.
</details>
<details>
<summary>摘要</summary>
近年来，有越来越多的关注关于敏感数据的隐私问题。为回应这些问题，差分隐私在学术和工业圈中得到了广泛的认可，成为隐私保护的严格框架。虽然在私人数据分析方面已经做出了大量的进展，但现有方法经常受到实用性或统计效率的限制。这篇论文的目标是在假设测试中解决这些问题，通过引入差分隐私排序测试来保持rigorous的隐私和统计有效性。我们的框架将经典的非私人排序测试扩展到私人设置下，并保持了finite-sample的有效性和差分隐私。我们的测试能力取决于选择的测试统计量，我们确定了一般的一致性和非假设统计上的强大能力。为了证明我们的框架的实用性和实用性，我们将重点关注使用归一化测试统计量，并引入差分隐私kernel测试：dpMMD和dpHSIC。这些差分隐私kernel测试是易于实现，适用于各种数据类型，并在不同的隐私环境下具有最佳的可比性。我们的实验证明了它们在不同的 sintetic 和实际场景下具有竞争力，强调它们的实际价值。代码publicly available，以便实现我们的框架。
</details></li>
</ul>
<hr>
<h2 id="On-Linear-Separation-Capacity-of-Self-Supervised-Representation-Learning"><a href="#On-Linear-Separation-Capacity-of-Self-Supervised-Representation-Learning" class="headerlink" title="On Linear Separation Capacity of Self-Supervised Representation Learning"></a>On Linear Separation Capacity of Self-Supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19041">http://arxiv.org/abs/2310.19041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shulei Wang</li>
<li>for: 本研究旨在探讨数据增强学习在多材料模型下的表达学习，以及这种表达学习是如何提高线性分类器的表达能力的。</li>
<li>methods: 本研究使用了自助学习和数据增强学习方法，并对这些方法的表达学习效果进行了分析。</li>
<li>results: 研究发现，数据增强学习可以提高线性分类器的表达能力，并且可以在多材料模型下 Linearly separate manifolds。此外，研究还发现，自助学习可以在小样本大量数据下提高线性分类器的表达能力。<details>
<summary>Abstract</summary>
Recent advances in self-supervised learning have highlighted the efficacy of data augmentation in learning data representation from unlabeled data. Training a linear model atop these enhanced representations can yield an adept classifier. Despite the remarkable empirical performance, the underlying mechanisms that enable data augmentation to unravel nonlinear data structures into linearly separable representations remain elusive. This paper seeks to bridge this gap by investigating under what conditions learned representations can linearly separate manifolds when data is drawn from a multi-manifold model. Our investigation reveals that data augmentation offers additional information beyond observed data and can thus improve the information-theoretic optimal rate of linear separation capacity. In particular, we show that self-supervised learning can linearly separate manifolds with a smaller distance than unsupervised learning, underscoring the additional benefits of data augmentation. Our theoretical analysis further underscores that the performance of downstream linear classifiers primarily hinges on the linear separability of data representations rather than the size of the labeled data set, reaffirming the viability of constructing efficient classifiers with limited labeled data amid an expansive unlabeled data set.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Machine-Learning-for-the-identification-of-phase-transitions-in-interacting-agent-based-systems"><a href="#Machine-Learning-for-the-identification-of-phase-transitions-in-interacting-agent-based-systems" class="headerlink" title="Machine Learning for the identification of phase-transitions in interacting agent-based systems"></a>Machine Learning for the identification of phase-transitions in interacting agent-based systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19039">http://arxiv.org/abs/2310.19039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolaos Evangelou, Dimitrios G. Giovanis, George A. Kevrekidis, Grigorios A. Pavliotis, Ioannis G. Kevrekidis</li>
<li>for: 这篇论文的目的是提出一种数据驱动的框架，用于描述agent-based模型（ABM）中的相态转变。</li>
<li>methods: 该论文使用了Diffusion Maps算法来Identify一个简洁的数据驱动变量，并使用深度学习框架来获得一个参数化的坐标系，以便在这些坐标系中identify一个参数dependent的涨落函数。</li>
<li>results: 该论文通过使用这种数据驱动的方法，成功地construct了一个相态转变的 диаграм。<details>
<summary>Abstract</summary>
Deriving closed-form, analytical expressions for reduced-order models, and judiciously choosing the closures leading to them, has long been the strategy of choice for studying phase- and noise-induced transitions for agent-based models (ABMs). In this paper, we propose a data-driven framework that pinpoints phase transitions for an ABM in its mean-field limit, using a smaller number of variables than traditional closed-form models. To this end, we use the manifold learning algorithm Diffusion Maps to identify a parsimonious set of data-driven latent variables, and show that they are in one-to-one correspondence with the expected theoretical order parameter of the ABM. We then utilize a deep learning framework to obtain a conformal reparametrization of the data-driven coordinates that facilitates, in our example, the identification of a single parameter-dependent ODE in these coordinates. We identify this ODE through a residual neural network inspired by a numerical integration scheme (forward Euler). We then use the identified ODE -- enabled through an odd symmetry transformation -- to construct the bifurcation diagram exhibiting the phase transition.
</details>
<details>
<summary>摘要</summary>
使用闭式表达式和选择合适的闭式来研究基于代理模型（ABM）的相对阶段和噪声引起的转变，已经是长期的策略。在这篇文章中，我们提出了一个数据驱动的框架，用于在ABM的含义场限制下标出相对阶段的转变点。为此，我们使用扩散地图算法来确定一个简洁的数据驱动的秘密变量，并证明它们与ABM的预期的理论参量之间存在一一对应关系。然后，我们使用深度学习框架来获得一个符号映射，以便在这些坐标系中进行数据驱动的协调。通过这种方式，我们可以在这些坐标系中提取出一个参数依赖的径谱方程。我们使用这个径谱方程，通过一种奇偶变换，构建了相对阶段的分布图。
</details></li>
</ul>
<hr>
<h2 id="Does-Invariant-Graph-Learning-via-Environment-Augmentation-Learn-Invariance"><a href="#Does-Invariant-Graph-Learning-via-Environment-Augmentation-Learn-Invariance" class="headerlink" title="Does Invariant Graph Learning via Environment Augmentation Learn Invariance?"></a>Does Invariant Graph Learning via Environment Augmentation Learn Invariance?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19035">http://arxiv.org/abs/2310.19035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lfhase/gala">https://github.com/lfhase/gala</a></li>
<li>paper_authors: Yongqiang Chen, Yatao Bian, Kaiwen Zhou, Binghui Xie, Bo Han, James Cheng</li>
<li>For: 本 paper 的目的是学习图像上的不变性，以便在图像上进行对外部数据进行泛化。* Methods: 本 paper 使用环境扩充来提高图像的不变性学习，但是这些环境扩充的有用性从未被证明。因此，本 paper 提出了一些最小假设，包括变化 suficiency 和变化 consistency，以便可能地学习图像的不变性。* Results: 本 paper 提出了一个新的框架 Graph invAriant Learning Assistant (GALA)，该框架包括一个助手模型，该模型需要对图像环境变化或分布变化敏感。助手模型的proxy预测可以判断图像中的杂乱子图的变化。根据这些 proxy 预测，提取图像中最大不变性子图可以唯一地标识图像的不变性子图，并且在成功的 OOD 泛化下保证不变性。经过对多个 dataset 的广泛实验，包括 DrugOOD 等，确认了 GALA 的有效性。<details>
<summary>Abstract</summary>
Invariant graph representation learning aims to learn the invariance among data from different environments for out-of-distribution generalization on graphs. As the graph environment partitions are usually expensive to obtain, augmenting the environment information has become the de facto approach. However, the usefulness of the augmented environment information has never been verified. In this work, we find that it is fundamentally impossible to learn invariant graph representations via environment augmentation without additional assumptions. Therefore, we develop a set of minimal assumptions, including variation sufficiency and variation consistency, for feasible invariant graph learning. We then propose a new framework Graph invAriant Learning Assistant (GALA). GALA incorporates an assistant model that needs to be sensitive to graph environment changes or distribution shifts. The correctness of the proxy predictions by the assistant model hence can differentiate the variations in spurious subgraphs. We show that extracting the maximally invariant subgraph to the proxy predictions provably identifies the underlying invariant subgraph for successful OOD generalization under the established minimal assumptions. Extensive experiments on datasets including DrugOOD with various graph distribution shifts confirm the effectiveness of GALA.
</details>
<details>
<summary>摘要</summary>
《固定 graph 表示学习中的不变性学习目标是学习数据集中的不变性，以实现对不同环境的外部数据泛化。然而，通常获取 graph 环境分区是非常昂贵的，因此通常会使用环境扩充来解决这个问题。然而，这种环境扩充的有用性从来没有得到证明。在这种情况下，我们发现，通过环境扩充来学习不变的 graph 表示是不可能的，因此我们提出了一些最小化假设，包括变化充分和变化一致，以便实现可能的不变的 graph 学习。然后，我们提出了一个新的框架Graph invAriant Learning Assistant（GALA）。GALA 包含一个助手模型，该模型需要对 graph 环境变化或分布变化敏感。如果助手模型的代理预测正确，那么可以区分真正的变量和误差的变量。我们证明，从助手模型的代理预测中提取最大可变的子图可以识别下来的不变的子图，并且在我们提出的假设下，可以 garantuee 对外部数据的泛化。我们的实验结果表明，GALA 在具有不同 graph 分布变化的数据集上具有非常高的有效性。》
</details></li>
</ul>
<hr>
<h2 id="An-Improved-Relaxation-for-Oracle-Efficient-Adversarial-Contextual-Bandits"><a href="#An-Improved-Relaxation-for-Oracle-Efficient-Adversarial-Contextual-Bandits" class="headerlink" title="An Improved Relaxation for Oracle-Efficient Adversarial Contextual Bandits"></a>An Improved Relaxation for Oracle-Efficient Adversarial Contextual Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19025">http://arxiv.org/abs/2310.19025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kiarash Banihashem, MohammadTaghi Hajiaghayi, Suho Shin, Max Springer</li>
<li>for: 这个论文是为了解决 adversarial contextual bandits 问题的 oracle-efficient relaxation。</li>
<li>methods: 这个论文使用的方法是一个 online adversary 选择 cost sequence，contexts 是从 known distribution 随机地被引入。</li>
<li>results: 这个论文的 regret bound 是 $O(T^{\frac{2}{3}(K\log(|\Pi|))^{\frac{1}{3})$，比之前的最好 bound $O((TK)^{\frac{2}{3}(\log(|\Pi|))^{\frac{1}{3})$ 更好。此外，这个论文还是第一个能够与 Langford 和 Zhang 在 NeurIPS 2007 提出的原始 bound 匹配的 result。<details>
<summary>Abstract</summary>
We present an oracle-efficient relaxation for the adversarial contextual bandits problem, where the contexts are sequentially drawn i.i.d from a known distribution and the cost sequence is chosen by an online adversary. Our algorithm has a regret bound of $O(T^{\frac{2}{3}(K\log(|\Pi|))^{\frac{1}{3})$ and makes at most $O(K)$ calls per round to an offline optimization oracle, where $K$ denotes the number of actions, $T$ denotes the number of rounds and $\Pi$ denotes the set of policies. This is the first result to improve the prior best bound of $O((TK)^{\frac{2}{3}(\log(|\Pi|))^{\frac{1}{3})$ as obtained by Syrgkanis et al. at NeurIPS 2016, and the first to match the original bound of Langford and Zhang at NeurIPS 2007 which was obtained for the stochastic case.
</details>
<details>
<summary>摘要</summary>
我们提出了一个 oracle-efficient relaxation 的方法来解决对抗上下文带状奖励问题，其中上下文是以独立 Identically distributed（i.i.d）方式从一个已知分布中随机获取，而问题选择的成本序列则是由一个在线 adversary 选择。我们的算法具有一个 regret  bound of $O(T^{\frac{2}{3}}(K\log(|\Pi|))^{\frac{1}{3}})$，并在每个回合最多做 $O(K)$ 个调用于 offline 优化库的请求，其中 $K$ 表示行动的数量，$T$ 表示回合的数量，$\Pi$ 表示策略的集合。这是第一个超越先前最好的 bound of $O((TK)^{\frac{2}{3}}(\log(|\Pi|))^{\frac{1}{3}})$，它是 Syrgkanis et al. 在 NeurIPS 2016 上提出的，并且是第一个与 Langford 和 Zhang 在 NeurIPS 2007 上提出的原始 bound 匹配，这个 bound 是为 Stochastic 情况。
</details></li>
</ul>
<hr>
<h2 id="Optimization-Landscape-of-Policy-Gradient-Methods-for-Discrete-time-Static-Output-Feedback"><a href="#Optimization-Landscape-of-Policy-Gradient-Methods-for-Discrete-time-Static-Output-Feedback" class="headerlink" title="Optimization Landscape of Policy Gradient Methods for Discrete-time Static Output Feedback"></a>Optimization Landscape of Policy Gradient Methods for Discrete-time Static Output Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19022">http://arxiv.org/abs/2310.19022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingliang Duan, Jie Li, Xuyang Chen, Kai Zhao, Shengbo Eben Li, Lin Zhao</li>
<li>for: 这篇论文探讨了使用policy gradient方法实现线性时域不变（LTI）系统的优化控制问题。</li>
<li>methods: 该论文使用了三种policy gradient方法：混合策略梯度法、自然策略梯度法和Gauss-Newton法。</li>
<li>results: 论文提出了关于这三种方法的新发现，包括它们在 дискреase时间LTI系统中的收敛性和约瑟率。此外，论文还证明了vanilla policy gradient方法在初始化 nearby local minima时的线性收敛性。<details>
<summary>Abstract</summary>
In recent times, significant advancements have been made in delving into the optimization landscape of policy gradient methods for achieving optimal control in linear time-invariant (LTI) systems. Compared with state-feedback control, output-feedback control is more prevalent since the underlying state of the system may not be fully observed in many practical settings. This paper analyzes the optimization landscape inherent to policy gradient methods when applied to static output feedback (SOF) control in discrete-time LTI systems subject to quadratic cost. We begin by establishing crucial properties of the SOF cost, encompassing coercivity, L-smoothness, and M-Lipschitz continuous Hessian. Despite the absence of convexity, we leverage these properties to derive novel findings regarding convergence (and nearly dimension-free rate) to stationary points for three policy gradient methods, including the vanilla policy gradient method, the natural policy gradient method, and the Gauss-Newton method. Moreover, we provide proof that the vanilla policy gradient method exhibits linear convergence towards local minima when initialized near such minima. The paper concludes by presenting numerical examples that validate our theoretical findings. These results not only characterize the performance of gradient descent for optimizing the SOF problem but also provide insights into the effectiveness of general policy gradient methods within the realm of reinforcement learning.
</details>
<details>
<summary>摘要</summary>
近些时间，在政策梯度方法中探索优化景观的进展很大。相比状态反馈控制，输出反馈控制更为普遍，因为实际情况中系统的下面状态可能不完全 observable。这篇论文分析了在静态输出反馈（SOF）控制中政策梯度方法的优化景观。我们首先证明了SOF成本函数的重要性质，包括半征性、L-smoothness和M-Lipschitz连续偏导。尽管不具有凸性，我们利用这些性质来 derivate 新的发现，包括政策梯度方法的三种方法（包括混合政策梯度方法、自然政策梯度方法和Gauss-Newton方法）的收敛性（以及几乎维度独立的速率）。此外，我们提供了证明，在 initialization 近于 Local minima 时，混合政策梯度方法 exhibits 线性收敛到 Local minima。文章结束，通过数学实验证明我们的理论发现。这些结果不仅描述了随机梯度下引擎的 SOF 问题的优化，还为束缚学习中的政策梯度方法提供了信息。
</details></li>
</ul>
<hr>
<h2 id="Behavior-Alignment-via-Reward-Function-Optimization"><a href="#Behavior-Alignment-via-Reward-Function-Optimization" class="headerlink" title="Behavior Alignment via Reward Function Optimization"></a>Behavior Alignment via Reward Function Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19007">http://arxiv.org/abs/2310.19007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhawal Gupta, Yash Chandak, Scott M. Jordan, Philip S. Thomas, Bruno Castro da Silva</li>
<li>for: 本研究旨在设计导引问题解决的优化学习搜寻（RL）代理人，以实现特定行为的目标。</li>
<li>methods: 本研究使用了一个新的两级目标框架，将auxiliary reward函数与环境的主要优化函数整合，以学习行为调整优化函数。</li>
<li>results: 本研究的结果显示，使用本研究的方法可以对RL代理人的政策优化过程进行自动调整，以减少问题解决中的限制和偏误。此外，本研究还证明了其可以对不同的任务和环境进行适用，并且可以实现高性能的解决方案，即使auxiliary reward函数存在误差或偏误。<details>
<summary>Abstract</summary>
Designing reward functions for efficiently guiding reinforcement learning (RL) agents toward specific behaviors is a complex task. This is challenging since it requires the identification of reward structures that are not sparse and that avoid inadvertently inducing undesirable behaviors. Naively modifying the reward structure to offer denser and more frequent feedback can lead to unintended outcomes and promote behaviors that are not aligned with the designer's intended goal. Although potential-based reward shaping is often suggested as a remedy, we systematically investigate settings where deploying it often significantly impairs performance. To address these issues, we introduce a new framework that uses a bi-level objective to learn \emph{behavior alignment reward functions}. These functions integrate auxiliary rewards reflecting a designer's heuristics and domain knowledge with the environment's primary rewards. Our approach automatically determines the most effective way to blend these types of feedback, thereby enhancing robustness against heuristic reward misspecification. Remarkably, it can also adapt an agent's policy optimization process to mitigate suboptimalities resulting from limitations and biases inherent in the underlying RL algorithms. We evaluate our method's efficacy on a diverse set of tasks, from small-scale experiments to high-dimensional control challenges. We investigate heuristic auxiliary rewards of varying quality -- some of which are beneficial and others detrimental to the learning process. Our results show that our framework offers a robust and principled way to integrate designer-specified heuristics. It not only addresses key shortcomings of existing approaches but also consistently leads to high-performing solutions, even when given misaligned or poorly-specified auxiliary reward functions.
</details>
<details>
<summary>摘要</summary>
设计奖励函数以有效引导学习控制（RL）代理人行为是一个复杂的任务。这是因为它需要识别不 sparse的奖励结构，以避免不恰当的奖励引导代理人行为。直接修改奖励结构以提供更密集和更频繁的反馈可能会导致不预期的结果，并且激励代理人不符合设计者的目标行为。虽然潜在基于奖励的奖励形成 often 被建议作为解决方案，但我们系统地调查这种方法在一些情况下可能会导致性能下降。为解决这些问题，我们提出一种新的框架，使用二级目标学习行为Alignment奖励函数。这些函数将auxiliary奖励与环境的主要奖励相结合，以便自动确定最有效的奖励杂合方式，从而提高对奖励misspecification的Robustness。此外，它还可以通过调整代理人的政策优化过程来抑制基于RL算法的限制和偏见所导致的优化不足。我们对这种方法的可行性进行了多种任务的测试，从小规模实验到高维控制挑战。我们研究了不同质量的辅助奖励，一些有利于学习过程，而另一些有害。我们的结果表明，我们的框架可以采取一种原则性的方式来整合设计者指定的euristic。它不仅解决了现有方法的主要缺陷，还一致地导致高性能的解决方案，即使auxiliary奖励函数给出了偏移或低质量的指示。
</details></li>
</ul>
<hr>
<h2 id="Kernel-based-Joint-Multiple-Graph-Learning-and-Clustering-of-Graph-Signals"><a href="#Kernel-based-Joint-Multiple-Graph-Learning-and-Clustering-of-Graph-Signals" class="headerlink" title="Kernel-based Joint Multiple Graph Learning and Clustering of Graph Signals"></a>Kernel-based Joint Multiple Graph Learning and Clustering of Graph Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19005">http://arxiv.org/abs/2310.19005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamad H. Alizade, Aref Einizade</li>
<li>for: 这种paper是为了掌握图像处理中的图结构学习和划分问题。</li>
<li>methods: 这种方法使用了kernel-based算法，结合了节点特征信息，以jointly partition signals和学习图。</li>
<li>results: 实验结果表明，这种方法在比较 estado-of-the-art方法时表现出了更高的效果。<details>
<summary>Abstract</summary>
Within the context of Graph Signal Processing (GSP), Graph Learning (GL) is concerned with the inference of a graph's topology from nodal observations, i.e., graph signals. However, data is often in mixed form, relating to different underlying structures. This heterogeneity necessitates the joint clustering and learning of multiple graphs. In many real-life applications, there are available node-side covariates (i.e., kernels) that imperatively should be incorporated, which has not been addressed by the rare graph signal clustering approaches. To this end and inspired by the rich K-means framework, we propose a novel kernel-based algorithm to incorporate this node-side information as we jointly partition the signals and learn a graph for each cluster. Numerical experiments demonstrate its effectiveness over the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
在图像处理（GSP）中，图学习（GL）关注图像的结构划分，即从节点观测获取图像。然而，数据往往是混合形式的，关系不同的基础结构。这种多元性需要同时划分多个图。在许多实际应用中，有可用的节点侧特征（即kernel），需要考虑其中的信息，这一点未在前期的图像信号划分方法中被考虑。为此，我们基于rich K-means框架，提出一种新的kernel-based算法，并在同时划分信号和学习图中jointly使用节点侧信息。数值实验表明其效果胜过现有的状态。
</details></li>
</ul>
<hr>
<h2 id="A-U-turn-on-Double-Descent-Rethinking-Parameter-Counting-in-Statistical-Learning"><a href="#A-U-turn-on-Double-Descent-Rethinking-Parameter-Counting-in-Statistical-Learning" class="headerlink" title="A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning"></a>A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18988">http://arxiv.org/abs/2310.18988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alicia Curth, Alan Jeffares, Mihaela van der Schaar</li>
<li>for: 本研究探讨了double descent现象在传统统计机器学习方法中的存在，并挑战了现有的U型曲线假设。</li>
<li>methods: 研究使用了非神经网络模型，包括线性回归、树和加法拟合。</li>
<li>results: 研究发现，当 Parameter 数量增加时，模型的测试错误率会经历两次下降，而不是传统的U型曲线预测。此外，通过视角改变，研究发现这种现象是由多个复杂性轴的交叠导致的。<details>
<summary>Abstract</summary>
Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a U-shaped curve reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count p grows past sample size n - a phenomenon dubbed double descent. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include linear regression, trees, and boosting. In this work, we take a closer look at evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of double descent truly extend the limits of a traditional U-shaped complexity-generalization curve therein. We show that once careful consideration is given to what is being plotted on the x-axes of their double descent plots, it becomes apparent that there are implicitly multiple complexity axes along which the parameter count grows. We demonstrate that the second descent appears exactly (and only) when and where the transition between these underlying axes occurs, and that its location is thus not inherently tied to the interpolation threshold p=n. We then gain further insight by adopting a classical nonparametric statistics perspective. We interpret the investigated methods as smoothers and propose a generalized measure for the effective number of parameters they use on unseen examples, using which we find that their apparent double descent curves indeed fold back into more traditional convex shapes - providing a resolution to tensions between double descent and statistical intuition.
</details>
<details>
<summary>摘要</summary>
传统统计智能认为，模型复杂度和预测误差之间存在一个很好地理解的关系，通常表现为一个U型曲线，反映模型在过拟合和under拟合两个 режиmes之间的转换。然而，受深度学习的成功影响，近期一些influential的工作表明，这种理论是通常不准确的，存在一个第二个下降的测试误差情况，被称为double descent。而且，这种现象不仅限于深度学习设置，还出现在非神经网络模型中，如线性回归、树和抛物模型。在这项工作中，我们更加仔细地研究了这些经典统计机器学习方法的证据，并挑战这些方法的double descent现象是否真的超出传统的U型复杂度-通用曲线的限制。我们发现，只要注意把plot的x轴上的图表绘制得到的是多少个复杂度轴，double descent现象就会变得更加明确。我们示示了第二个下降出现在这些下推轴之间的转换处，并且其位置不是因为 interpolate threshold p=n 决定的。然后，我们采用了一种类非Parametric统计视角，将这些方法看作是简单器，并提出了一种通用的效果参数计数器，用于测试这些方法在未seen例中的表现。我们发现，这些方法的apparent double descent曲线实际上是fold back到了传统的convex形状，解决了对double descent和统计直觉之间的矛盾。
</details></li>
</ul>
<hr>
<h2 id="TRIAGE-Characterizing-and-auditing-training-data-for-improved-regression"><a href="#TRIAGE-Characterizing-and-auditing-training-data-for-improved-regression" class="headerlink" title="TRIAGE: Characterizing and auditing training data for improved regression"></a>TRIAGE: Characterizing and auditing training data for improved regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18970">http://arxiv.org/abs/2310.18970</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seedatnabeel/triage">https://github.com/seedatnabeel/triage</a></li>
<li>paper_authors: Nabeel Seedat, Jonathan Crabbé, Zhaozhi Qian, Mihaela van der Schaar</li>
<li>for: 本研究旨在提供一种适用于回归任务的数据Characterization方法，以提高机器学习算法的稳定性和性能。</li>
<li>methods: 本方法基于Conformal predictive distributions提供一个模型无关的评分方法，称为TRIAGE score。该分数用于分析个体样本的训练动态和 caracterizar样本为模型下未经估计、过估计或良好估计。</li>
<li>results: 研究人员通过应用TRIAGE分析了多个回归任务，并证明了TRIAGE的 charactization是一致的。此外，TRIAGE还可以用于选择数据集和获取特征。总的来说，TRIAGE highlights the value of data characterization in real-world regression applications.<details>
<summary>Abstract</summary>
Data quality is crucial for robust machine learning algorithms, with the recent interest in data-centric AI emphasizing the importance of training data characterization. However, current data characterization methods are largely focused on classification settings, with regression settings largely understudied. To address this, we introduce TRIAGE, a novel data characterization framework tailored to regression tasks and compatible with a broad class of regressors. TRIAGE utilizes conformal predictive distributions to provide a model-agnostic scoring method, the TRIAGE score. We operationalize the score to analyze individual samples' training dynamics and characterize samples as under-, over-, or well-estimated by the model. We show that TRIAGE's characterization is consistent and highlight its utility to improve performance via data sculpting/filtering, in multiple regression settings. Additionally, beyond sample level, we show TRIAGE enables new approaches to dataset selection and feature acquisition. Overall, TRIAGE highlights the value unlocked by data characterization in real-world regression applications
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>机器学习算法中数据质量的重要性已得到更多的关注，特别是在数据中心式AI时代，数据Characterization的重要性得到了更多的认可。然而，现有的数据Characterization方法主要集中在分类任务上， regression任务相对较少研究。为了解决这个问题，我们介绍了一种新的数据Characterization框架，即TRIAGE，该框架适用于多种回归器，并且可以提供一个模型无关的评分方法，即TRIAGE分数。我们将TRIAGE分数操作化，以分析个体样本的训练过程和模型评估。我们示出了TRIAGE的分类是一致的，并且它的用途可以提高数据雕刻/筛选的性能，在多种回归任务中。此外，TRIAGE还可以用于数据集选择和特征收集新的方法。总之，TRIAGE highlights the value of data characterization in real-world regression applications.
</details></li>
</ul>
<hr>
<h2 id="Playing-in-the-Dark-No-regret-Learning-with-Adversarial-Constraints"><a href="#Playing-in-the-Dark-No-regret-Learning-with-Adversarial-Constraints" class="headerlink" title="Playing in the Dark: No-regret Learning with Adversarial Constraints"></a>Playing in the Dark: No-regret Learning with Adversarial Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18955">http://arxiv.org/abs/2310.18955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Sinha, Rahul Vaze</li>
<li>For: 本文探讨了线性 convex 优化框架的扩展，包括额外的长期反对抗约束。特别是，在一线策略决定动作后，除了一个凸成本函数外，反对抗还会透露一组 $k$ 凸约束。成本和约束函数可以随时间变化，无法预测未来函数的信息。* Methods: 本文提出了一种元策略，同时实现了下线性累积约束和下线性 regret。这是通过将受限问题降低到标准 OCO 问题的 recursive 构建的一个黑盒减reduction。我们表明，可以通过解决 surrogate 问题使用任何适应 OCO 策略，满足标准数据依赖 regret bound。* Results: 本文提出了一种新的 Lyapunov 基于证明技术，揭示了 regret 与某些顺序不等式之间的连接。通过一种新的分解结果，我们得出了 regret 的优化性质。 finally, 本文应用于在线多任务学习和网络控制问题。<details>
<summary>Abstract</summary>
We study a generalization of the classic Online Convex Optimization (OCO) framework by considering additional long-term adversarial constraints. Specifically, after an online policy decides its action on a round, in addition to a convex cost function, the adversary also reveals a set of $k$ convex constraints. The cost and the constraint functions could change arbitrarily with time, and no information about the future functions is assumed to be available. In this paper, we propose a meta-policy that simultaneously achieves a sublinear cumulative constraint violation and a sublinear regret. This is achieved via a black box reduction of the constrained problem to the standard OCO problem for a recursively constructed sequence of surrogate cost functions. We show that optimal performance bounds can be achieved by solving the surrogate problem using any adaptive OCO policy enjoying a standard data-dependent regret bound. A new Lyapunov-based proof technique is presented that reveals a connection between regret and certain sequential inequalities through a novel decomposition result. We conclude the paper by highlighting applications to online multi-task learning and network control problems.
</details>
<details>
<summary>摘要</summary>
我们研究了online convex optimization（OCO）框架的一种普遍化，其中包括额外的长期反对派对约束。具体来说，在一个线上策略决定其行动后，除了一个凸成本函数外，反对派也会公布一组$k$个凸约束。成本函数和约束函数可以随时间变化，并不知道未来函数的信息。在这篇论文中，我们提议一个meta策略，可以同时实现一个凸累累约束和一个凸后悔。这是通过一种黑盒减少法将受约束问题转化为标准OCO问题的 recursively constructed sequence of surrogate cost functions。我们表明了一种新的 Lyapunov-based 证明技术，可以通过一种新的分解结果显示回归和某些顺序不等式之间的联系。最后，我们将报告应用于在线多任务学习和网络控制问题。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Bias-of-Gradient-Descent-for-Two-layer-ReLU-and-Leaky-ReLU-Networks-on-Nearly-orthogonal-Data"><a href="#Implicit-Bias-of-Gradient-Descent-for-Two-layer-ReLU-and-Leaky-ReLU-Networks-on-Nearly-orthogonal-Data" class="headerlink" title="Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data"></a>Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18935">http://arxiv.org/abs/2310.18935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Kou, Zixiang Chen, Quanquan Gu</li>
<li>for: 这个论文主要探讨了Gradient Descent的隐式偏好在训练非平滑神经网络时的影响。</li>
<li>methods: 本文使用了Gradient Descent来训练两层完全相连（漏斗 activtion function）神经网络，并研究了隐式偏好在不同activation function下的影响。</li>
<li>results: 本文发现在训练数据接近对称的情况下，隐式偏好会使Gradient Descent寻找一个稳定的rank值，并且这値值会在ReLU activation function下随着训练进程的推移而变化。此外，本文还发现隐式偏好会使Gradient Descent寻找一个神经网络，使得所有的训练数据点都具有相同的normalized margin。实验结果与理论结果匹配。<details>
<summary>Abstract</summary>
The implicit bias towards solutions with favorable properties is believed to be a key reason why neural networks trained by gradient-based optimization can generalize well. While the implicit bias of gradient flow has been widely studied for homogeneous neural networks (including ReLU and leaky ReLU networks), the implicit bias of gradient descent is currently only understood for smooth neural networks. Therefore, implicit bias in non-smooth neural networks trained by gradient descent remains an open question. In this paper, we aim to answer this question by studying the implicit bias of gradient descent for training two-layer fully connected (leaky) ReLU neural networks. We showed that when the training data are nearly-orthogonal, for leaky ReLU activation function, gradient descent will find a network with a stable rank that converges to $1$, whereas for ReLU activation function, gradient descent will find a neural network with a stable rank that is upper bounded by a constant. Additionally, we show that gradient descent will find a neural network such that all the training data points have the same normalized margin asymptotically. Experiments on both synthetic and real data backup our theoretical findings.
</details>
<details>
<summary>摘要</summary>
“偏好解释器的偏好”是一个关键因素，使得神经网络通过梯度下降优化得到良好的泛化能力。而“梯度流”中的偏好已经广泛研究过 homogeneous 神经网络（包括 ReLU 和泄漏 ReLU 网络），但是“梯度下降”中的偏好对非满意神经网络仍然是一个开Question。在这篇论文中，我们尝试答复这个问题，通过研究两层全连接（泄漏 ReLU）神经网络在训练过程中的偏好。我们发现，当训练数据几乎正交时，使用泄漏 ReLU 活动函数时，梯度下降会找到一个稳定的权重积分，其渐近值为 1；而使用 ReLU 活动函数时，梯度下降会找到一个神经网络，其稳定权重积分上限为一个常数。此外，我们还发现，梯度下降会找到一个神经网络，使得所有的训练数据点都具有相同的归一化margin。实验结果证明了我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Remaining-Useful-Life-Prediction-of-Lithium-ion-Batteries-using-Spatio-temporal-Multimodal-Attention-Networks"><a href="#Remaining-Useful-Life-Prediction-of-Lithium-ion-Batteries-using-Spatio-temporal-Multimodal-Attention-Networks" class="headerlink" title="Remaining Useful Life Prediction of Lithium-ion Batteries using Spatio-temporal Multimodal Attention Networks"></a>Remaining Useful Life Prediction of Lithium-ion Batteries using Spatio-temporal Multimodal Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18924">http://arxiv.org/abs/2310.18924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Dhruvadityamittal/RUL_Prediction_of_LIB_using_Spatio_temporal_Multimodal_Attention_Networks">https://github.com/Dhruvadityamittal/RUL_Prediction_of_LIB_using_Spatio_temporal_Multimodal_Attention_Networks</a></li>
<li>paper_authors: Sungho Suh, Dhruv Aditya Mittal, Hymalai Bello, Bo Zhou, Mayank Shekhar Jha, Paul Lukowicz</li>
<li>For: The paper aims to predict the remaining useful life of Lithium-ion batteries in real-world scenarios, addressing the limitations of existing methods and improving the reliability and efficiency of battery operations.* Methods: The proposed method uses a two-stage remaining useful life prediction scheme based on a spatio-temporal multimodal attention network (ST-MAN), which captures complex spatio-temporal dependencies in the battery data and neglected features such as temperature, internal resistance, and material type.* Results: The proposed ST-MAN model outperforms existing CNN and LSTM-based methods, achieving state-of-the-art performance in predicting the remaining useful life of Li-ion batteries.<details>
<summary>Abstract</summary>
Lithium-ion batteries are widely used in various applications, including electric vehicles and renewable energy storage. The prediction of the remaining useful life (RUL) of batteries is crucial for ensuring reliable and efficient operation, as well as reducing maintenance costs. However, determining the life cycle of batteries in real-world scenarios is challenging, and existing methods have limitations in predicting the number of cycles iteratively. In addition, existing works often oversimplify the datasets, neglecting important features of the batteries such as temperature, internal resistance, and material type. To address these limitations, this paper proposes a two-stage remaining useful life prediction scheme for Lithium-ion batteries using a spatio-temporal multimodal attention network (ST-MAN). The proposed model is designed to iteratively predict the number of cycles required for the battery to reach the end of its useful life, based on available data. The proposed ST-MAN is to capture the complex spatio-temporal dependencies in the battery data, including the features that are often neglected in existing works. Experimental results demonstrate that the proposed ST-MAN model outperforms existing CNN and LSTM-based methods, achieving state-of-the-art performance in predicting the remaining useful life of Li-ion batteries. The proposed method has the potential to improve the reliability and efficiency of battery operations and is applicable in various industries, including automotive and renewable energy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Hyperbolic-Graph-Neural-Networks-at-Scale-A-Meta-Learning-Approach"><a href="#Hyperbolic-Graph-Neural-Networks-at-Scale-A-Meta-Learning-Approach" class="headerlink" title="Hyperbolic Graph Neural Networks at Scale: A Meta Learning Approach"></a>Hyperbolic Graph Neural Networks at Scale: A Meta Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18918">http://arxiv.org/abs/2310.18918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nurendra Choudhary, Nikhil Rao, Chandan K. Reddy</li>
<li>for: 提高几何神经网络（HNNs）的泛化能力和可扩展性，以便在新任务上快速学习和掌握大型图数据集。</li>
<li>methods: 学习图节点和边的本地子图中的抽象特征，并将其转移到新的子图上进行几拟 shot 学习。引入一种新的方法——几何 GRAph Meta Learner（H-GRAM），可以在节点 classification 和链接预测任务中学习并转移抽象信息，以便更快地学习新的任务。</li>
<li>results: 在多个具有挑战性的几拟 shot Setting 中，H-GRAM 能够有效地学习和转移信息，并且在大型图数据集上可以扩展性地提高性能。与标准 HNNs 相比，我们的方法可以更好地扩展到大型图数据集和提高性能。<details>
<summary>Abstract</summary>
The progress in hyperbolic neural networks (HNNs) research is hindered by their absence of inductive bias mechanisms, which are essential for generalizing to new tasks and facilitating scalable learning over large datasets. In this paper, we aim to alleviate these issues by learning generalizable inductive biases from the nodes' local subgraph and transfer them for faster learning over new subgraphs with a disjoint set of nodes, edges, and labels in a few-shot setting. We introduce a novel method, Hyperbolic GRAph Meta Learner (H-GRAM), that, for the tasks of node classification and link prediction, learns transferable information from a set of support local subgraphs in the form of hyperbolic meta gradients and label hyperbolic protonets to enable faster learning over a query set of new tasks dealing with disjoint subgraphs. Furthermore, we show that an extension of our meta-learning framework also mitigates the scalability challenges seen in HNNs faced by existing approaches. Our comparative analysis shows that H-GRAM effectively learns and transfers information in multiple challenging few-shot settings compared to other state-of-the-art baselines. Additionally, we demonstrate that, unlike standard HNNs, our approach is able to scale over large graph datasets and improve performance over its Euclidean counterparts.
</details>
<details>
<summary>摘要</summary>
progress in гиперболических нейронных сетях (HNNs) 研究受到其缺乏抽象假设机制的限制，这些机制是必需的 для泛化到新任务和促进大量数据集上的学习。在这篇论文中，我们想要解决这些问题，通过从节点的本地子图中学习通用的抽象假设，并将其传递给新的子图上快速学习。我们提出了一种新的方法，即гиперболическиеGRAPH元学习器（H-GRAM），它在节点分类和链接预测任务上，通过从支持本地子图集中学习hyperbolic meta 梯度和标签hyerbolic气泡来启用快速学习新任务。此外，我们还证明了我们的meta学习框架的扩展可以解决现有方法所面临的扩展性问题。我们的比较分析表明，H-GRAM在多个具有挑战性的几个shot设定中能够有效地学习和传递信息，并且与标准HNNs相比，我们的方法可以在大规模图数据集上提高性能。
</details></li>
</ul>
<hr>
<h2 id="Estimating-the-Rate-Distortion-Function-by-Wasserstein-Gradient-Descent"><a href="#Estimating-the-Rate-Distortion-Function-by-Wasserstein-Gradient-Descent" class="headerlink" title="Estimating the Rate-Distortion Function by Wasserstein Gradient Descent"></a>Estimating the Rate-Distortion Function by Wasserstein Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18908">http://arxiv.org/abs/2310.18908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiboyang/wgd">https://github.com/yiboyang/wgd</a></li>
<li>paper_authors: Yibo Yang, Stephan Eckstein, Marcel Nutz, Stephan Mandt</li>
<li>for: 本研究的目的是提出一种基于最优运输的Rate-Distortion（R-D）函数估计方法，用于评估数据源的压缩性。</li>
<li>methods: 该方法使用 Wasserstein 梯度下降算法学习优化的抽象分布，不同于经典的 Blahut–Arimoto 算法，它预先固定了往复分布的支持。</li>
<li>results: 实验表明，该方法可以在低比特率源上获得相当或更紧的约束，而需要许多更少的调整和计算努力。此外，该方法还与最大极值估计有关，并引入了一种新的测试源。<details>
<summary>Abstract</summary>
In the theory of lossy compression, the rate-distortion (R-D) function $R(D)$ describes how much a data source can be compressed (in bit-rate) at any given level of fidelity (distortion). Obtaining $R(D)$ for a given data source establishes the fundamental performance limit for all compression algorithms. We propose a new method to estimate $R(D)$ from the perspective of optimal transport. Unlike the classic Blahut--Arimoto algorithm which fixes the support of the reproduction distribution in advance, our Wasserstein gradient descent algorithm learns the support of the optimal reproduction distribution by moving particles. We prove its local convergence and analyze the sample complexity of our R-D estimator based on a connection to entropic optimal transport. Experimentally, we obtain comparable or tighter bounds than state-of-the-art neural network methods on low-rate sources while requiring considerably less tuning and computation effort. We also highlight a connection to maximum-likelihood deconvolution and introduce a new class of sources that can be used as test cases with known solutions to the R-D problem.
</details>
<details>
<summary>摘要</summary>
理论上，吞吐率-损均衡（R-D）函数 $R(D)$ 描述了数据源可以通过压缩（bit-rate）来实现任何级别的准确性（损均）。确定 $R(D)$ 对于给定数据源是吞吐率压缩算法的基本性能上限。我们提出了一种基于最优运输的新方法来估计 $R(D)$。这种方法不同于经典的布拉哈特-阿里莫托算法，它在预先固定往复复制分布的支持上进行估计。我们的渐进梯度滚动算法会学习最优往复复制分布的支持，通过移动粒子来实现。我们证明了本方法的本地收敛性和样本复杂性，并与经典神经网络方法进行比较。实验结果表明，我们的方法可以在低比特率源上获得相对或更紧的约束，而且需要远少的调整和计算努力。我们还 highlight了与最大似然减杂的连接，并介绍了一种新的测试集，其中可以使用已知解决R-D问题的源。
</details></li>
</ul>
<hr>
<h2 id="Topological-or-Non-topological-A-Deep-Learning-Based-Prediction"><a href="#Topological-or-Non-topological-A-Deep-Learning-Based-Prediction" class="headerlink" title="Topological, or Non-topological? A Deep Learning Based Prediction"></a>Topological, or Non-topological? A Deep Learning Based Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18907">http://arxiv.org/abs/2310.18907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xercxis/P_zeta">https://github.com/xercxis/P_zeta</a></li>
<li>paper_authors: Ashiqur Rasul, Md Shafayat Hossain, Ankan Ghosh Dastider, Himaddri Roy, M. Zahid Hasan, Quazi D. M. Khosru</li>
<li>for: 预测和发现新材料的性能预测和材料设计</li>
<li>methods: 使用深度学习模型，结合 persistently homology 和图神经网络，实现高精度的材料分类</li>
<li>results: 实验结果显示，该模型的准确率为 91.4%，F1 分数为 88.5%，在分类非材料和材料中表现出色，超过其他状态对照模型<details>
<summary>Abstract</summary>
Prediction and discovery of new materials with desired properties are at the forefront of quantum science and technology research. A major bottleneck in this field is the computational resources and time complexity related to finding new materials from ab initio calculations. In this work, an effective and robust deep learning-based model is proposed by incorporating persistent homology and graph neural network which offers an accuracy of 91.4% and an F1 score of 88.5% in classifying topological vs. non-topological materials, outperforming the other state-of-the-art classifier models. The incorporation of the graph neural network encodes the underlying relation between the atoms into the model based on their own crystalline structures and thus proved to be an effective method to represent and process non-euclidean data like molecules with a relatively shallow network. The persistent homology pipeline in the suggested neural network is capable of integrating the atom-specific topological information into the deep learning model, increasing robustness, and gain in performance. It is believed that the presented work will be an efficacious tool for predicting the topological class and therefore enable the high-throughput search for novel materials in this field.
</details>
<details>
<summary>摘要</summary>
科学家们正在努力探索新材料的搜索和预测，以满足现代科学和技术的需求。然而，计算资源和计算复杂性问题成为了这一领域的主要瓶颈。在这篇文章中，我们提出了一种有效和可靠的深度学习模型，通过结合持续同态和图神经网络来减少计算资源的占用和提高预测的精度。这种模型在分类非普遍材料和普遍材料方面的准确率达91.4%，F1分数达88.5%，超过了其他现有的分类模型。在这种模型中，图神经网络允许通过晶体结构中的原子之间的关系来编码材料的结构，从而实现了对非欧几何数据的有效处理。持续同态管道在建议的神经网络中允许将原子特征的拓扑信息纳入深度学习模型中，从而提高了模型的稳定性和性能。总之，这种方法将成为预测材料的拓扑类别的有效工具，并促进高速搜索新材料的搜索。
</details></li>
</ul>
<hr>
<h2 id="Learning-Subgrid-Scale-Models-in-Discontinuous-Galerkin-Methods-with-Neural-Ordinary-Differential-Equations-for-Compressible-Navier–Stokes-Equations"><a href="#Learning-Subgrid-Scale-Models-in-Discontinuous-Galerkin-Methods-with-Neural-Ordinary-Differential-Equations-for-Compressible-Navier–Stokes-Equations" class="headerlink" title="Learning Subgrid-Scale Models in Discontinuous Galerkin Methods with Neural Ordinary Differential Equations for Compressible Navier–Stokes Equations"></a>Learning Subgrid-Scale Models in Discontinuous Galerkin Methods with Neural Ordinary Differential Equations for Compressible Navier–Stokes Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18897">http://arxiv.org/abs/2310.18897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shinhoo Kang, Emil M. Constantinescu</li>
<li>for: 该文章的目的是提出一种基于神经普通微分方程的新方法，用于在离散哈姆频率（DG）空间积分中学习低级别模型的影响。</li>
<li>methods: 该方法使用神经网络来学习低级别模型中缺失的涨落尺度，从而提高低级别DG近似的准确性和加速筛选高级别DG simulation的运算速度。</li>
<li>results: 作者通过多维泰勒-格林涡漩示例来证明该方法的性能，并证明该方法不仅可以重construct低级别模型的涨落尺度，还可以加速筛选高级别DG simulation的运算速度，提高了模型的准确性和效率。<details>
<summary>Abstract</summary>
The growing computing power over the years has enabled simulations to become more complex and accurate. However, high-fidelity simulations, while immensely valuable for scientific discovery and problem solving, come with significant computational demands. As a result, it is common to run a low-fidelity model with a subgrid-scale model to reduce the computational cost, but selecting the appropriate subgrid-scale models and tuning them are challenging. We propose a novel method for learning the subgrid-scale model effects when simulating partial differential equations using neural ordinary differential equations in the context of discontinuous Galerkin (DG) spatial discretization. Our approach learns the missing scales of the low-order DG solver at a continuous level and hence improves the accuracy of the low-order DG approximations as well as accelerates the filtered high-order DG simulations with a certain degree of precision. We demonstrate the performance of our approach through multidimensional Taylor--Green vortex examples at different Reynolds numbers and times, which cover laminar, transitional, and turbulent regimes. The proposed method not only reconstructs the subgrid-scale from the low-order (1st-order) approximation but also speeds up the filtered high-order DG (6th-order) simulation by two orders of magnitude.
</details>
<details>
<summary>摘要</summary>
随着计算能力的提高， simulations 已经能够更加复杂和准确。然而，高精度 simulations 的计算需求很大，因此通常采用低精度模型和缺失涂抹模型来降低计算成本。然而，选择合适的缺失涂抹模型并调整它们是困难的。我们提出了一种基于神经ordinary differential equations的新方法，用于在discontinuous Galerkin（DG）空间积分方法中学习subgrid-scale模型的效果。我们的方法可以在 kontinuierlichen Level学习低阶DG解的缺失尺度，从而提高低阶DG的准确性和加速筛选高阶DG（6th-order） simulations的过程。我们通过多维 Taylor--Green涡涌示例来证明我们的方法的性能，这些示例覆盖了laminar、transition和turbulent режиmes。我们的方法不仅可以从低阶（1st-order）解中重construct subgrid-scale，还可以加速筛选高阶DG simulations的过程，提高速度两个数量级。
</details></li>
</ul>
<hr>
<h2 id="D2NO-Efficient-Handling-of-Heterogeneous-Input-Function-Spaces-with-Distributed-Deep-Neural-Operators"><a href="#D2NO-Efficient-Handling-of-Heterogeneous-Input-Function-Spaces-with-Distributed-Deep-Neural-Operators" class="headerlink" title="D2NO: Efficient Handling of Heterogeneous Input Function Spaces with Distributed Deep Neural Operators"></a>D2NO: Efficient Handling of Heterogeneous Input Function Spaces with Distributed Deep Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18888">http://arxiv.org/abs/2310.18888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zecheng Zhang, Christian Moya, Lu Lu, Guang Lin, Hayden Schaeffer</li>
<li>for: 解决 Parametric partial differential equations、动力系统控制和反向问题中的hetERogeneous输入函数问题</li>
<li>methods: 使用Discretization-invariant neural operators和分布式方法处理多感器输入函数</li>
<li>results: 提出一种新的分布式方法，可以降低Gradient descent back-propagation步数，提高效率而不失精度，并 Validated by four numerical examples<details>
<summary>Abstract</summary>
Neural operators have been applied in various scientific fields, such as solving parametric partial differential equations, dynamical systems with control, and inverse problems. However, challenges arise when dealing with input functions that exhibit heterogeneous properties, requiring multiple sensors to handle functions with minimal regularity. To address this issue, discretization-invariant neural operators have been used, allowing the sampling of diverse input functions with different sensor locations. However, existing frameworks still require an equal number of sensors for all functions. In our study, we propose a novel distributed approach to further relax the discretization requirements and solve the heterogeneous dataset challenges. Our method involves partitioning the input function space and processing individual input functions using independent and separate neural networks. A centralized neural network is used to handle shared information across all output functions. This distributed methodology reduces the number of gradient descent back-propagation steps, improving efficiency while maintaining accuracy. We demonstrate that the corresponding neural network is a universal approximator of continuous nonlinear operators and present four numerical examples to validate its performance.
</details>
<details>
<summary>摘要</summary>
我们的方法包括将输入函数空间分区，并使用独立的 neural network 处理个别输入函数。中央 neural network 用于处理所有输出函数之间的共享信息。这种分布式方法可以降低梯度下降反propagation 步骤数量，提高效率而无损准确性。我们证明了相应的 neural network 是一个 universal approximator 的连续非线性算子，并在四个数字示例中验证了其性能。
</details></li>
</ul>
<hr>
<h2 id="A-foundational-neural-operator-that-continuously-learns-without-forgetting"><a href="#A-foundational-neural-operator-that-continuously-learns-without-forgetting" class="headerlink" title="A foundational neural operator that continuously learns without forgetting"></a>A foundational neural operator that continuously learns without forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18885">http://arxiv.org/abs/2310.18885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tapas Tripura, Souvik Chakraborty</li>
<li>for: 本研究旨在开发一种基础模型，用于科学计算中的物理问题。</li>
<li>methods: 该模型基于神经网络和波峰分解技术，并使用了闭合结构和记忆 Ensemble 技术来学习多种物理系统的解方程。</li>
<li>results: 该模型能够同时学习多种 Parametric PDE 的解方程，并能够快速适应新的 Parametric PDE。同时，该模型也能够保持Positive Transfer和避免 Catastrophic Forgetting。经过广泛的 benchmark 测试，该模型可以在预测阶段比task-specific基eline模型表现更好，并且具有较少的hyperparameter tuning。<details>
<summary>Abstract</summary>
Machine learning has witnessed substantial growth, leading to the development of advanced artificial intelligence models crafted to address a wide range of real-world challenges spanning various domains, such as computer vision, natural language processing, and scientific computing. Nevertheless, the creation of custom models for each new task remains a resource-intensive undertaking, demanding considerable computational time and memory resources. In this study, we introduce the concept of the Neural Combinatorial Wavelet Neural Operator (NCWNO) as a foundational model for scientific computing. This model is specifically designed to excel in learning from a diverse spectrum of physics and continuously adapt to the solution operators associated with parametric partial differential equations (PDEs). The NCWNO leverages a gated structure that employs local wavelet experts to acquire shared features across multiple physical systems, complemented by a memory-based ensembling approach among these local wavelet experts. This combination enables rapid adaptation to new challenges. The proposed foundational model offers two key advantages: (i) it can simultaneously learn solution operators for multiple parametric PDEs, and (ii) it can swiftly generalize to new parametric PDEs with minimal fine-tuning. The proposed NCWNO is the first foundational operator learning algorithm distinguished by its (i) robustness against catastrophic forgetting, (ii) the maintenance of positive transfer for new parametric PDEs, and (iii) the facilitation of knowledge transfer across dissimilar tasks. Through an extensive set of benchmark examples, we demonstrate that the NCWNO can outperform task-specific baseline operator learning frameworks with minimal hyperparameter tuning at the prediction stage. We also show that with minimal fine-tuning, the NCWNO performs accurate combinatorial learning of new parametric PDEs.
</details>
<details>
<summary>摘要</summary>
In this study, we introduce the Neural Combinatorial Wavelet Neural Operator (NCWNO) as a foundational model for scientific computing. This model is specifically designed to excel in learning from a diverse spectrum of physics and continuously adapt to the solution operators associated with parametric partial differential equations (PDEs). The NCWNO leverages a gated structure that employs local wavelet experts to acquire shared features across multiple physical systems, complemented by a memory-based ensembling approach among these local wavelet experts. This combination enables rapid adaptation to new challenges.The proposed foundational model offers two key advantages: (i) it can simultaneously learn solution operators for multiple parametric PDEs, and (ii) it can swiftly generalize to new parametric PDEs with minimal fine-tuning. Additionally, the NCWNO is distinguished by its:* Robustness against catastrophic forgetting* Maintenance of positive transfer for new parametric PDEs* Facilitation of knowledge transfer across dissimilar tasksThrough an extensive set of benchmark examples, we demonstrate that the NCWNO can outperform task-specific baseline operator learning frameworks with minimal hyperparameter tuning at the prediction stage. We also show that with minimal fine-tuning, the NCWNO can accurately combine learning of new parametric PDEs.
</details></li>
</ul>
<hr>
<h2 id="Simple-and-Asymmetric-Graph-Contrastive-Learning-without-Augmentations"><a href="#Simple-and-Asymmetric-Graph-Contrastive-Learning-without-Augmentations" class="headerlink" title="Simple and Asymmetric Graph Contrastive Learning without Augmentations"></a>Simple and Asymmetric Graph Contrastive Learning without Augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18884">http://arxiv.org/abs/2310.18884</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tengxiao1/graphacl">https://github.com/tengxiao1/graphacl</a></li>
<li>paper_authors: Teng Xiao, Huaisheng Zhu, Zhengyu Chen, Suhang Wang</li>
<li>for: 本文研究了对异谱图进行对照学习，并提出了一种简单的算法GraphACL，可以 capture一步邻居信息和两步同类相似性。</li>
<li>methods: 本文使用了对照学习方法，并提出了一种偏 asymmetric 视角来处理异谱图。</li>
<li>results: 实验结果表明，GraphACL 可以在异谱图上 achieve 出色的表现，并且在 homophilic 和异谱图上都具有优异的泛化能力。<details>
<summary>Abstract</summary>
Graph Contrastive Learning (GCL) has shown superior performance in representation learning in graph-structured data. Despite their success, most existing GCL methods rely on prefabricated graph augmentation and homophily assumptions. Thus, they fail to generalize well to heterophilic graphs where connected nodes may have different class labels and dissimilar features. In this paper, we study the problem of conducting contrastive learning on homophilic and heterophilic graphs. We find that we can achieve promising performance simply by considering an asymmetric view of the neighboring nodes. The resulting simple algorithm, Asymmetric Contrastive Learning for Graphs (GraphACL), is easy to implement and does not rely on graph augmentations and homophily assumptions. We provide theoretical and empirical evidence that GraphACL can capture one-hop local neighborhood information and two-hop monophily similarity, which are both important for modeling heterophilic graphs. Experimental results show that the simple GraphACL significantly outperforms state-of-the-art graph contrastive learning and self-supervised learning methods on homophilic and heterophilic graphs. The code of GraphACL is available at https://github.com/tengxiao1/GraphACL.
</details>
<details>
<summary>摘要</summary>
图像对比学习（GCL）在图结构数据中的表示学习表现出色。然而，大多数现有的GCL方法都基于先制制图像增强和同类连接假设。因此，它们在不同类型连接的图中失去泛化能力。在这篇论文中，我们研究了在同类连接和不同类型连接图中进行对比学习的问题。我们发现，只需考虑偏 asymmetric 的邻居节点视角，就可以获得了良好的表现。 resulting algorithm, Asymmetric Contrastive Learning for Graphs (GraphACL), 易于实现并不需要图像增强和同类连接假设。我们提供了理论和实验证据，表明 GraphACL 可以捕捉一次邻居信息和两次同类连接相似性，这些都是模型不同类型连接图的关键。实验结果表明，简单的 GraphACL 在同类连接和不同类型连接图中明显超越了当前最佳的图像对比学习和自然学习方法。GraphACL 的代码可以在 https://github.com/tengxiao1/GraphACL 上获取。
</details></li>
</ul>
<hr>
<h2 id="Correlation-Aware-Sparsified-Mean-Estimation-Using-Random-Projection"><a href="#Correlation-Aware-Sparsified-Mean-Estimation-Using-Random-Projection" class="headerlink" title="Correlation Aware Sparsified Mean Estimation Using Random Projection"></a>Correlation Aware Sparsified Mean Estimation Using Random Projection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18868">http://arxiv.org/abs/2310.18868</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/11hifish/Rand-Proj-Spatial">https://github.com/11hifish/Rand-Proj-Spatial</a></li>
<li>paper_authors: Shuli Jiang, Pranay Sharma, Gauri Joshi</li>
<li>for: 这篇论文主要探讨了分布式vector mean估计问题，是分布式优化和联合学习（Federated Learning，FL）中通用的子routine。</li>
<li>methods: 这篇论文使用了 Rand-$k$ 簇范例化技术来减少分布式传输成本，每个客户端将 $k &lt; d$ 个坐标发送到服务器。然而，Rand-$k$ 无法考虑实际应用中客户端之间的相互联系。这篇论文提出了 Rand-$k$-Spatial 估计器，利用服务器端的客户端间联系信息来改善 Rand-$k$ 的性能。然而，Rand-$k$-Spatial 的性能仍然不足。这篇论文提出了 Rand-Proj-Spatial 估计器，具有更加灵活的嵌入构造和解oding程序，可以更好地利用客户端间的联系信息。</li>
<li>results: 这篇论文的实验结果显示，Rand-Proj-Spatial 比 Rand-$k$-Spatial 和其他更加复杂的簇范例化技术更高效。此外，这篇论文还提出了一种可以根据客户端间联系信息不同程度的弹性 Rand-Proj-Spatial 方法，并且在实验中证明其效果。<details>
<summary>Abstract</summary>
We study the problem of communication-efficient distributed vector mean estimation, a commonly used subroutine in distributed optimization and Federated Learning (FL). Rand-$k$ sparsification is a commonly used technique to reduce communication cost, where each client sends $k < d$ of its coordinates to the server. However, Rand-$k$ is agnostic to any correlations, that might exist between clients in practical scenarios. The recently proposed Rand-$k$-Spatial estimator leverages the cross-client correlation information at the server to improve Rand-$k$'s performance. Yet, the performance of Rand-$k$-Spatial is suboptimal. We propose the Rand-Proj-Spatial estimator with a more flexible encoding-decoding procedure, which generalizes the encoding of Rand-$k$ by projecting the client vectors to a random $k$-dimensional subspace. We utilize Subsampled Randomized Hadamard Transform (SRHT) as the projection matrix and show that Rand-Proj-Spatial with SRHT outperforms Rand-$k$-Spatial, using the correlation information more efficiently. Furthermore, we propose an approach to incorporate varying degrees of correlation and suggest a practical variant of Rand-Proj-Spatial when the correlation information is not available to the server. Experiments on real-world distributed optimization tasks showcase the superior performance of Rand-Proj-Spatial compared to Rand-$k$-Spatial and other more sophisticated sparsification techniques.
</details>
<details>
<summary>摘要</summary>
我们研究了一个分布式向量均值估计问题，这是分布式优化和联合学习（FL）中广泛使用的一种子 Routine。 Rand-$k$ 精炼是一种常用的减少通信成本的技术，每个客户端向服务器发送 $k < d$ 个坐标。然而，Rand-$k$ 无法考虑客户端之间的协方差信息，这可能导致性能下降。我们提出了 Rand-$k$-Spatial 估计器，使用服务器端的协方差信息来改进 Rand-$k$ 的性能。然而，Rand-$k$-Spatial 的性能仍然有限制。我们提出了 Rand-Proj-Spatial 估计器，它使用随机 $k$-维空间的投影来扩展 Rand-$k$ 的编码过程。我们使用 Subsampled Randomized Hadamard Transform (SRHT) 作为投影矩阵，并证明 Rand-Proj-Spatial 使用 SRHT 的投影可以更好地利用协方差信息。此外，我们提出了一种根据协方差信息不同程度的变化来修改 Rand-Proj-Spatial 的方法，并建议在服务器端不可获得协方差信息时使用实际 variant。我们在实际分布式优化任务上进行了实验，并证明 Rand-Proj-Spatial 的性能比 Rand-$k$-Spatial 和其他更复杂的精炼技术更高。
</details></li>
</ul>
<hr>
<h2 id="Peer-to-Peer-Deep-Learning-for-Beyond-5G-IoT"><a href="#Peer-to-Peer-Deep-Learning-for-Beyond-5G-IoT" class="headerlink" title="Peer-to-Peer Deep Learning for Beyond-5G IoT"></a>Peer-to-Peer Deep Learning for Beyond-5G IoT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18861">http://arxiv.org/abs/2310.18861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srinivasa Pranav, José M. F. Moura</li>
<li>for: 这个论文是为了解决智能城市等 beyond-5G  computing 环境中的规模问题，而不需要中央服务器或云端协调。</li>
<li>methods: 这个算法使用 max norm synchronization 来驱动训练，保留了设备上的深度模型训练，并使用本地设备之间的通信来实现分布式共识。每个设备会逐次交替进行两个阶段：1）设备上的学习，2）分布式合作，其中它们与附近的设备结合模型参数。</li>
<li>results: 这个算法可以让所有参与设备都 дости得到与 federated 和中央训练相同的测试性能，甚至在 100 个设备和宽松的单器散发加重的情况下。此外，这个算法还可以在不同的网络拓扑、罕见的通信和非Identical 数据分布情况下进行扩展。<details>
<summary>Abstract</summary>
We present P2PL, a practical multi-device peer-to-peer deep learning algorithm that, unlike the federated learning paradigm, does not require coordination from edge servers or the cloud. This makes P2PL well-suited for the sheer scale of beyond-5G computing environments like smart cities that otherwise create range, latency, bandwidth, and single point of failure issues for federated approaches.   P2PL introduces max norm synchronization to catalyze training, retains on-device deep model training to preserve privacy, and leverages local inter-device communication to implement distributed consensus. Each device iteratively alternates between two phases: 1) on-device learning and 2) distributed cooperation where they combine model parameters with nearby devices. We empirically show that all participating devices achieve the same test performance attained by federated and centralized training -- even with 100 devices and relaxed singly stochastic consensus weights. We extend these experimental results to settings with diverse network topologies, sparse and intermittent communication, and non-IID data distributions.
</details>
<details>
<summary>摘要</summary>
我们介绍P2PL，一种实用多设备 peer-to-peer深度学习算法，不同于联邦学习模式，不需要边缘服务器或云端协调。这使得P2PL在 beyond-5G 计算环境中，如智能城市，创造范围、延迟、带宽和单点故障问题，而 federated 方法不适用。P2PL 引入最大范数同步来促进训练，保留设备上深度模型训练，并利用本地设备间通信实现分布式共识。每个设备会逐次 alternate between two 阶段：1）设备上学习和 2）分布式合作，其中 combines 模型参数与附近设备。我们实验表明，参与训练的所有设备可以达到 federated 和中央训练所得到的测试性能，即使有 100 个设备和松弛单调共识加权。我们还将这些实验结果扩展到不同的网络拓扑、笔数和间歇性通信、非标一致数据分布的设置下。
</details></li>
</ul>
<hr>
<h2 id="Bayes-beats-Cross-Validation-Efficient-and-Accurate-Ridge-Regression-via-Expectation-Maximization"><a href="#Bayes-beats-Cross-Validation-Efficient-and-Accurate-Ridge-Regression-via-Expectation-Maximization" class="headerlink" title="Bayes beats Cross Validation: Efficient and Accurate Ridge Regression via Expectation Maximization"></a>Bayes beats Cross Validation: Efficient and Accurate Ridge Regression via Expectation Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18860">http://arxiv.org/abs/2310.18860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shu Yu Tew, Mario Boley, Daniel F. Schmidt</li>
<li>for: 本研究提出了一种新的方法，用于调整ridge regression中的正则化参数（λ），它比遗弃一个样本的跨Validation（LOOCV）更快速，同时可以提供与LOOCV risk最小化的拟合参数的同等或更高质量的估计。</li>
<li>methods: 本研究使用了一种 bayesian 的ridge regression形式ulation，通过一个迭代的期望最大化（EM）过程来学习jointly 估计 $\lambda$ 和拟合参数。</li>
<li>results: 研究表明，该方法可以在大 enough $n$ 的情况下，无需设定任何难以确定的 гипер参数，具有唯一最优解，并且在 $O(\min(n, p))$ 操作下实现单一迭代EM循环。此外，研究还发现，通过采用合适的预处理步骤，可以在 $O(n \min(n, p))$ 操作下评估单个 $\lambda$ 值，而不需要评估所有 $l$ 个 candidate $\lambda$ 值。<details>
<summary>Abstract</summary>
We present a novel method for tuning the regularization hyper-parameter, $\lambda$, of a ridge regression that is faster to compute than leave-one-out cross-validation (LOOCV) while yielding estimates of the regression parameters of equal, or particularly in the setting of sparse covariates, superior quality to those obtained by minimising the LOOCV risk. The LOOCV risk can suffer from multiple and bad local minima for finite $n$ and thus requires the specification of a set of candidate $\lambda$, which can fail to provide good solutions. In contrast, we show that the proposed method is guaranteed to find a unique optimal solution for large enough $n$, under relatively mild conditions, without requiring the specification of any difficult to determine hyper-parameters. This is based on a Bayesian formulation of ridge regression that we prove to have a unimodal posterior for large enough $n$, allowing for both the optimal $\lambda$ and the regression coefficients to be jointly learned within an iterative expectation maximization (EM) procedure. Importantly, we show that by utilizing an appropriate preprocessing step, a single iteration of the main EM loop can be implemented in $O(\min(n, p))$ operations, for input data with $n$ rows and $p$ columns. In contrast, evaluating a single value of $\lambda$ using fast LOOCV costs $O(n \min(n, p))$ operations when using the same preprocessing. This advantage amounts to an asymptotic improvement of a factor of $l$ for $l$ candidate values for $\lambda$ (in the regime $q, p \in O(\sqrt{n})$ where $q$ is the number of regression targets).
</details>
<details>
<summary>摘要</summary>
我团队提出了一种新的方法来调整ridge regression中的正则化超参数($\lambda$ )，这种方法比逐个留下一个（LOOCV）更快速计算，而且可以提供与LOOCVrisk相同或更高质量的回归参数估计。LOOCV risk可能会在finite $n$ 下存在多个和坏的地方极 minimum，因此可能需要指定一组 candidate $\lambda$，这可能会导致不良的解决方案。然而，我们证明了该方法在 suficiently large $n$ 下是唯一优化解决方案，不需要指定任何难以确定的超参数。这是基于ridge regression的 Bayesian 表述，我们证明了其 posterior 在 suficiently large $n$ 下是单模的，因此可以通过 iterative expectation maximization (EM) 过程来同时学习 optimal $\lambda$ 和回归系数。其中，我们还证明了可以通过适当的预处理步骤，在 $O(\min(n, p))$ 操作下完成一次主 EM 循环，其中 $n$ 是行数，$p$ 是列数。与此相比，通过快速 LOOCV 评估 $\lambda$ 的值需要 $O(n \min(n, p))$ 操作。这种优势在 $l$ 个 candidate $\lambda$ 值的情况下（在 $q, p \in O(\sqrt{n})$  regime 中）amounts to an asymptotic improvement factor of $l$。
</details></li>
</ul>
<hr>
<h2 id="SiDA-Sparsity-Inspired-Data-Aware-Serving-for-Efficient-and-Scalable-Large-Mixture-of-Experts-Models"><a href="#SiDA-Sparsity-Inspired-Data-Aware-Serving-for-Efficient-and-Scalable-Large-Mixture-of-Experts-Models" class="headerlink" title="SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts Models"></a>SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18859">http://arxiv.org/abs/2310.18859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhixu Du, Shiyu Li, Yuhao Wu, Xiangyu Jiang, Jingwei Sun, Qilin Zheng, Yongkai Wu, Ang Li, Hai “Helen” Li, Yiran Chen</li>
<li>for: 这篇论文的目的是提出一种高效的大型 mixture-of-experts（MoE）模型测试方法，以减少 GPU 内存使用量并提高模型效率。</li>
<li>methods: 这篇论文使用了一种叫做 SiDA（Sparsity-inspired Data-Aware）的方法，它利用了系统的主内存和 GPU 内存，并利用了 MoE 模型中专家活化的内在绝对性来优化模型效率。</li>
<li>results: 根据论文的结果，SiDA 方法可以实现大幅提高 MoE 模型的测试速度，将对 GPU 内存的使用量减少到 1%，同时保持模型效率不变。具体来说，SiDA 方法可以实现 Up to 3.93X 的测试速度增加、Up to 75% 的延迟降低和 Up to 80% 的 GPU 内存储存量减少。<details>
<summary>Abstract</summary>
Mixture-of-Experts (MoE) has emerged as a favorable architecture in the era of large models due to its inherent advantage, i.e., enlarging model capacity without incurring notable computational overhead. Yet, the realization of such benefits often results in ineffective GPU memory utilization, as large portions of the model parameters remain dormant during inference. Moreover, the memory demands of large models consistently outpace the memory capacity of contemporary GPUs. Addressing this, we introduce SiDA (Sparsity-inspired Data-Aware), an efficient inference approach tailored for large MoE models. SiDA judiciously exploits both the system's main memory, which is now abundant and readily scalable, and GPU memory by capitalizing on the inherent sparsity on expert activation in MoE models. By adopting a data-aware perspective, SiDA achieves enhanced model efficiency with a neglectable performance drop. Specifically, SiDA attains a remarkable speedup in MoE inference with up to 3.93X throughput increasing, up to 75% latency reduction, and up to 80% GPU memory saving with down to 1% performance drop. This work paves the way for scalable and efficient deployment of large MoE models, even in memory-constrained systems.
</details>
<details>
<summary>摘要</summary>
大型模型时代，混合专家（MoE）架构已成为有利的选择，因为它可以无需增加显著的计算负担来扩大模型的容量。然而，实现这些利点经常导致大量模型参数在推理过程中处于休眠状态，同时大模型的内存需求常常超过当今的GPU内存容量。为解决这个问题，我们提出了SiDA（基于缺省性的数据意识），这是一种高效的推理方法，专门为大MoE模型设计。SiDA利用系统的主存，这是现在充足且可扩展的，同时还利用GPU内存，通过利用MoE模型中专家活动的自然缺省性来提高模型效率。通过采用数据意识的视角，SiDA实现了更高的模型效率，减少了推理延迟和GPU内存占用，同时保持了模型性能的稳定。具体来说，SiDA在MoE推理中可以达到3.93倍的吞吐量提高、75%的延迟减少和80%的GPU内存减少，同时保持模型性能下降不到1%。这项工作为大MoE模型的扩展和高效部署提供了可行的方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/cs.LG_2023_10_29/" data-id="clogyj8zs00qq7crab6zbg0lz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/29/eess.IV_2023_10_29/" class="article-date">
  <time datetime="2023-10-29T09:00:00.000Z" itemprop="datePublished">2023-10-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/29/eess.IV_2023_10_29/">eess.IV - 2023-10-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Subjective-Quality-Evaluation-of-Point-Clouds-Using-a-Head-Mounted-Display"><a href="#Subjective-Quality-Evaluation-of-Point-Clouds-Using-a-Head-Mounted-Display" class="headerlink" title="Subjective Quality Evaluation of Point Clouds Using a Head Mounted Display"></a>Subjective Quality Evaluation of Point Clouds Using a Head Mounted Display</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19179">http://arxiv.org/abs/2310.19179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joao Prazeres, Rafael Rodrigues, Manuela Pereira, Antonio M. G. Pinheiro</li>
<li>for: 这个论文报告了对静止点云编码器MPEG V-PCC、G-PCC、深度学习编码器RS-DLPCC以及受欢迎的Draco编码器的主观质量评估。</li>
<li>methods: 该论文使用了18名参与者通过头戴式显示器直接比较了3D表示的扭曲点云的视觉效果，并对所获得的主观评分（MOS）与之前两项研究中对同一内容的视觉效果进行了比较，包括潘森相关指数、斯宾塞排名相关指数、平均方差和外围异常指数。</li>
<li>results: 结果表明这三项研究之间存在高度相关性，并且对所有评估中的差异没有发现任何显著差异。<details>
<summary>Abstract</summary>
This paper reports on a subjective quality evaluation of static point clouds encoded with the MPEG codecs V-PCC and G-PCC, the deep learning-based codec RS-DLPCC, and the popular Draco codec. 18 subjects visualized 3D representations of distorted point clouds using a Head Mounted Display, which allowed for a direct comparison with their reference. The Mean Opinion Scores (MOS) obtained in this subjective evaluation were compared with the MOS from two previous studies, where the same content was visualized either on a 2D display or a 3D stereoscopic display, through the Pearson Correlation, Spearman Rank Order Correlation, Root Mean Square Error, and the Outlier Ratio. The results indicate that the three studies are highly correlated with one another. Moreover, a statistical analysis between all evaluations showed no significant differences between them.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)这篇论文报道了一项主观质量评估，涉及到静止点云编码器MPEG codecs V-PCC和G-PCC、深度学习基于的RS-DLPCC编码器以及受欢迎的Draco编码器。18名参与者通过头戴式显示器 visualized 3D表示distorted点云，可以直接与参考进行比较。获得的主观意见分（MOS）在这个主观评估中被与之前两个研究相比较，这两个研究分别使用2D显示器和3D立体显示器显示同一内容。通过皮尔逊相关度、Spearman排序相关度、平均方差误差和异常比率进行比较。结果表明这三个研究之间存在高度相关性，并且在所有评估中没有发现显著差异。
</details></li>
</ul>
<hr>
<h2 id="Transport-of-Intensity-Model-for-Single-Mask-X-ray-Differential-Phase-Contrast-Imaging"><a href="#Transport-of-Intensity-Model-for-Single-Mask-X-ray-Differential-Phase-Contrast-Imaging" class="headerlink" title="Transport-of-Intensity Model for Single-Mask X-ray Differential Phase Contrast Imaging"></a>Transport-of-Intensity Model for Single-Mask X-ray Differential Phase Contrast Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19087">http://arxiv.org/abs/2310.19087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingcheng Yuan, Mini Das</li>
<li>for: 该研究旨在提高软组织和肿瘤的可见度，使用X射线阶段差图像技术。</li>
<li>methods: 该研究提出了一种基于运输Intensity公式的单面phas imaging系统模型，以提供图像形成过程的直观理解。此外，该研究还展示了使用单一的干扰图像来 Retrieval attenuation和分别阶段差信息，不需要spectral信息或探测器&#x2F;Mask移动。</li>
<li>results: 该研究通过实验和Monte Carlo仿真示出了模型的有效性和提议的Retrieval方法。该模型超越了现有模型的限制，提供了直观的图像形成过程的视觉化，同时允许优化分别阶段差投影 geometries，进一步提高了实际应用的可行性。<details>
<summary>Abstract</summary>
X-ray phase contrast imaging has emerged as a promising technique for enhancing contrast and visibility of light-element materials, including soft tissues and tumors. In this paper, we propose a novel model for a single-mask phase imaging system based on the transport-of-intensity equation. Our model offers an intuitive understanding of signal and contrast formation in single-mask phase imaging systems. We also demonstrate efficient retrieval of attenuation and differential phase contrast with just one intensity image without requiring spectral information or mask/detector movement. The model validity as well as the proposed retrieval method is demonstrated via both experimental results on a system developed in-house as well as with Monte Carlo simulations. Our proposed model overcomes the limitations of existing models by providing an intuitive visualization of the image formation process. It also allows optimizing differential phase imaging geometries for practical applications, further enhancing broader applicability. Furthermore, the general methodology described herein offers insight on deriving transport-of-intensity models for novel X-ray imaging systems with periodic structures in the beam path.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/29/eess.IV_2023_10_29/" data-id="clogyj93r016u7cra5lge8fln" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/83/">83</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
