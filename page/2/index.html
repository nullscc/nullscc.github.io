
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="http://nullscc.github.io/page/2/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="Fun Paper" type="application/atom+xml">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
      <a class="main-nav-link st-search-show-outputs">Search</a>
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/cs.CL_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/24/cs.CL_2023_08_24/">cs.CL - 2023-08-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Text-Similarity-from-Image-Contents-using-Statistical-and-Semantic-Analysis-Techniques"><a href="#Text-Similarity-from-Image-Contents-using-Statistical-and-Semantic-Analysis-Techniques" class="headerlink" title="Text Similarity from Image Contents using Statistical and Semantic Analysis Techniques"></a>Text Similarity from Image Contents using Statistical and Semantic Analysis Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12842">http://arxiv.org/abs/2308.12842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagar Kulkarni, Sharvari Govilkar, Dhiraj Amin</li>
</ul>
<p>Abstract:<br>Plagiarism detection is one of the most researched areas among the Natural Language Processing(NLP) community. A good plagiarism detection covers all the NLP methods including semantics, named entities, paraphrases etc. and produces detailed plagiarism reports. Detection of Cross Lingual Plagiarism requires deep knowledge of various advanced methods and algorithms to perform effective text similarity checking. Nowadays the plagiarists are also advancing themselves from hiding the identity from being catch in such offense. The plagiarists are bypassed from being detected with techniques like paraphrasing, synonym replacement, mismatching citations, translating one language to another. Image Content Plagiarism Detection (ICPD) has gained importance, utilizing advanced image content processing to identify instances of plagiarism to ensure the integrity of image content. The issue of plagiarism extends beyond textual content, as images such as figures, graphs, and tables also have the potential to be plagiarized. However, image content plagiarism detection remains an unaddressed challenge. Therefore, there is a critical need to develop methods and systems for detecting plagiarism in image content. In this paper, the system has been implemented to detect plagiarism form contents of Images such as Figures, Graphs, Tables etc. Along with statistical algorithms such as Jaccard and Cosine, introducing semantic algorithms such as LSA, BERT, WordNet outperformed in detecting efficient and accurate plagiarism.</p>
<hr>
<h2 id="Use-of-LLMs-for-Illicit-Purposes-Threats-Prevention-Measures-and-Vulnerabilities"><a href="#Use-of-LLMs-for-Illicit-Purposes-Threats-Prevention-Measures-and-Vulnerabilities" class="headerlink" title="Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities"></a>Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12833">http://arxiv.org/abs/2308.12833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Mozes, Xuanli He, Bennett Kleinberg, Lewis D. Griffin</li>
</ul>
<p>Abstract:<br>Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures. With our work, we hope to raise awareness of the limitations of LLMs in light of such security concerns, among both experienced developers and novel users of such technologies.</p>
<hr>
<h2 id="WavMark-Watermarking-for-Audio-Generation"><a href="#WavMark-Watermarking-for-Audio-Generation" class="headerlink" title="WavMark: Watermarking for Audio Generation"></a>WavMark: Watermarking for Audio Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12770">http://arxiv.org/abs/2308.12770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, Xiaoyong Du, Furu Wei</li>
</ul>
<p>Abstract:<br>Recent breakthroughs in zero-shot voice synthesis have enabled imitating a speaker’s voice using just a few seconds of recording while maintaining a high level of realism. Alongside its potential benefits, this powerful technology introduces notable risks, including voice fraud and speaker impersonation. Unlike the conventional approach of solely relying on passive methods for detecting synthetic data, watermarking presents a proactive and robust defence mechanism against these looming risks. This paper introduces an innovative audio watermarking framework that encodes up to 32 bits of watermark within a mere 1-second audio snippet. The watermark is imperceptible to human senses and exhibits strong resilience against various attacks. It can serve as an effective identifier for synthesized voices and holds potential for broader applications in audio copyright protection. Moreover, this framework boasts high flexibility, allowing for the combination of multiple watermark segments to achieve heightened robustness and expanded capacity. Utilizing 10 to 20-second audio as the host, our approach demonstrates an average Bit Error Rate (BER) of 0.48% across ten common attacks, a remarkable reduction of over 2800% in BER compared to the state-of-the-art watermarking tool. See <a target="_blank" rel="noopener" href="https://aka.ms/wavmark">https://aka.ms/wavmark</a> for demos of our work.</p>
<hr>
<h2 id="Real-time-Detection-of-AI-Generated-Speech-for-DeepFake-Voice-Conversion"><a href="#Real-time-Detection-of-AI-Generated-Speech-for-DeepFake-Voice-Conversion" class="headerlink" title="Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion"></a>Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12734">http://arxiv.org/abs/2308.12734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan J. Bird, Ahmad Lotfi</li>
</ul>
<p>Abstract:<br>There are growing implications surrounding generative AI in the speech domain that enable voice cloning and real-time voice conversion from one individual to another. This technology poses a significant ethical threat and could lead to breaches of privacy and misrepresentation, thus there is an urgent need for real-time detection of AI-generated speech for DeepFake Voice Conversion. To address the above emerging issues, the DEEP-VOICE dataset is generated in this study, comprised of real human speech from eight well-known figures and their speech converted to one another using Retrieval-based Voice Conversion. Presenting as a binary classification problem of whether the speech is real or AI-generated, statistical analysis of temporal audio features through t-testing reveals that there are significantly different distributions. Hyperparameter optimisation is implemented for machine learning models to identify the source of speech. Following the training of 208 individual machine learning models over 10-fold cross validation, it is found that the Extreme Gradient Boosting model can achieve an average classification accuracy of 99.3% and can classify speech in real-time, at around 0.004 milliseconds given one second of speech. All data generated for this study is released publicly for future research on AI speech detection.</p>
<hr>
<h2 id="Harnessing-the-Power-of-David-against-Goliath-Exploring-Instruction-Data-Generation-without-Using-Closed-Source-Models"><a href="#Harnessing-the-Power-of-David-against-Goliath-Exploring-Instruction-Data-Generation-without-Using-Closed-Source-Models" class="headerlink" title="Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models"></a>Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12711">http://arxiv.org/abs/2308.12711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Wang, Xinrui Wang, Juntao Li, Jinxiong Chang, Qishen Zhang, Zhongyi Liu, Guannan Zhang, Min Zhang</li>
</ul>
<p>Abstract:<br>Instruction tuning is instrumental in enabling Large Language Models~(LLMs) to follow user instructions to complete various open-domain tasks. The success of instruction tuning depends on the availability of high-quality instruction data. Owing to the exorbitant cost and substandard quality of human annotation, recent works have been deeply engaged in the exploration of the utilization of powerful closed-source models to generate instruction data automatically. However, these methods carry potential risks arising from the usage requirements of powerful closed-source models, which strictly forbid the utilization of their outputs to develop machine learning models. To deal with this problem, in this work, we explore alternative approaches to generate high-quality instruction data that do not rely on closed-source models. Our exploration includes an investigation of various existing instruction generation methods, culminating in the integration of the most efficient variant with two novel strategies to enhance the quality further. Evaluation results from two benchmarks and the GPT-4 model demonstrate the effectiveness of our generated instruction data, which can outperform Alpaca, a method reliant on closed-source models. We hope that more progress can be achieved in generating high-quality instruction data without using closed-source models.</p>
<hr>
<h2 id="Improving-Translation-Faithfulness-of-Large-Language-Models-via-Augmenting-Instructions"><a href="#Improving-Translation-Faithfulness-of-Large-Language-Models-via-Augmenting-Instructions" class="headerlink" title="Improving Translation Faithfulness of Large Language Models via Augmenting Instructions"></a>Improving Translation Faithfulness of Large Language Models via Augmenting Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12674">http://arxiv.org/abs/2308.12674</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pppa2019/swie_overmiss_llm4mt">https://github.com/pppa2019/swie_overmiss_llm4mt</a></li>
<li>paper_authors: Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou</li>
</ul>
<p>Abstract:<br>Large Language Models (LLMs) present strong general capabilities, and a current compelling challenge is stimulating their specialized capabilities, such as machine translation, through low-cost instruction tuning. The standard instruction-following data is sequentially organized as the concatenation of an instruction, an input, and a response. As the attention mechanism of LLMs has limitations on local focus, LLMs tend to focus more on the words or sentences nearby at each position. This leads to a high risk of instruction forgetting during decoding. To alleviate the above issues, We propose SWIE (Segment-Weighted Instruction Embedding) and an instruction-following dataset OVERMISS. SWIE improves the model instruction understanding by adding a global instruction representation on the following input and response representations. OVERMISS improves model faithfulness by comparing over-translation and miss-translation results with the correct translation. We apply our methods to two main-stream open-source LLMs, BLOOM and LLaMA. The experimental results demonstrate significant improvements in translation performance with SWIE based on BLOOMZ-3b, particularly in zero-shot and long text translations due to reduced instruction forgetting risk. Additionally, OVERMISS outperforms the baseline in translation performance (e.g. an increase in BLEU scores from 0.69 to 3.12 and an average improvement of 0.48 percentage comet scores for LLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE (e.g. the BLUE scores increase up to 0.56 from English to German across three different backbones), and both exhibit improvements in the faithfulness metric based on word alignment.</p>
<hr>
<h2 id="From-Chatter-to-Matter-Addressing-Critical-Steps-of-Emotion-Recognition-Learning-in-Task-oriented-Dialogue"><a href="#From-Chatter-to-Matter-Addressing-Critical-Steps-of-Emotion-Recognition-Learning-in-Task-oriented-Dialogue" class="headerlink" title="From Chatter to Matter: Addressing Critical Steps of Emotion Recognition Learning in Task-oriented Dialogue"></a>From Chatter to Matter: Addressing Critical Steps of Emotion Recognition Learning in Task-oriented Dialogue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12648">http://arxiv.org/abs/2308.12648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shutong Feng, Nurul Lubis, Benjamin Ruppik, Christian Geishauser, Michael Heck, Hsien-chin Lin, Carel van Niekerk, Renato Vukovic, Milica Gašić</li>
</ul>
<p>Abstract:<br>Emotion recognition in conversations (ERC) is a crucial task for building human-like conversational agents. While substantial efforts have been devoted to ERC for chit-chat dialogues, the task-oriented counterpart is largely left unattended. Directly applying chit-chat ERC models to task-oriented dialogues (ToDs) results in suboptimal performance as these models overlook key features such as the correlation between emotions and task completion in ToDs. In this paper, we propose a framework that turns a chit-chat ERC model into a task-oriented one, addressing three critical aspects: data, features and objective. First, we devise two ways of augmenting rare emotions to improve ERC performance. Second, we use dialogue states as auxiliary features to incorporate key information from the goal of the user. Lastly, we leverage a multi-aspect emotion definition in ToDs to devise a multi-task learning objective and a novel emotion-distance weighted loss function. Our framework yields significant improvements for a range of chit-chat ERC models on EmoWOZ, a large-scale dataset for user emotion in ToDs. We further investigate the generalisability of the best resulting model to predict user satisfaction in different ToD datasets. A comparison with supervised baselines shows a strong zero-shot capability, highlighting the potential usage of our framework in wider scenarios.</p>
<hr>
<h2 id="Probabilistic-Method-of-Measuring-Linguistic-Productivity"><a href="#Probabilistic-Method-of-Measuring-Linguistic-Productivity" class="headerlink" title="Probabilistic Method of Measuring Linguistic Productivity"></a>Probabilistic Method of Measuring Linguistic Productivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12643">http://arxiv.org/abs/2308.12643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergei Monakhov</li>
</ul>
<p>Abstract:<br>In this paper I propose a new way of measuring linguistic productivity that objectively assesses the ability of an affix to be used to coin new complex words and, unlike other popular measures, is not directly dependent upon token frequency. Specifically, I suggest that linguistic productivity may be viewed as the probability of an affix to combine with a random base. The advantages of this approach include the following. First, token frequency does not dominate the productivity measure but naturally influences the sampling of bases. Second, we are not just counting attested word types with an affix but rather simulating the construction of these types and then checking whether they are attested in the corpus. Third, a corpus-based approach and randomised design assure that true neologisms and words coined long ago have equal chances to be selected. The proposed algorithm is evaluated both on English and Russian data. The obtained results provide some valuable insights into the relation of linguistic productivity to the number of types and tokens. It looks like burgeoning linguistic productivity manifests itself in an increasing number of types. However, this process unfolds in two stages: first comes the increase in high-frequency items, and only then follows the increase in low-frequency items.</p>
<hr>
<h2 id="Advancing-Hungarian-Text-Processing-with-HuSpaCy-Efficient-and-Accurate-NLP-Pipelines"><a href="#Advancing-Hungarian-Text-Processing-with-HuSpaCy-Efficient-and-Accurate-NLP-Pipelines" class="headerlink" title="Advancing Hungarian Text Processing with HuSpaCy: Efficient and Accurate NLP Pipelines"></a>Advancing Hungarian Text Processing with HuSpaCy: Efficient and Accurate NLP Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12635">http://arxiv.org/abs/2308.12635</a></li>
<li>repo_url: None</li>
<li>paper_authors: György Orosz, Gergő Szabó, Péter Berkecz, Zsolt Szántó, Richárd Farkas</li>
</ul>
<p>Abstract:<br>This paper presents a set of industrial-grade text processing models for Hungarian that achieve near state-of-the-art performance while balancing resource efficiency and accuracy. Models have been implemented in the spaCy framework, extending the HuSpaCy toolkit with several improvements to its architecture. Compared to existing NLP tools for Hungarian, all of our pipelines feature all basic text processing steps including tokenization, sentence-boundary detection, part-of-speech tagging, morphological feature tagging, lemmatization, dependency parsing and named entity recognition with high accuracy and throughput. We thoroughly evaluated the proposed enhancements, compared the pipelines with state-of-the-art tools and demonstrated the competitive performance of the new models in all text preprocessing steps. All experiments are reproducible and the pipelines are freely available under a permissive license.</p>
<hr>
<h2 id="PromptMRG-Diagnosis-Driven-Prompts-for-Medical-Report-Generation"><a href="#PromptMRG-Diagnosis-Driven-Prompts-for-Medical-Report-Generation" class="headerlink" title="PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation"></a>PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12604">http://arxiv.org/abs/2308.12604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haibo Jin, Haoxuan Che, Yi Lin, Hao Chen</li>
</ul>
<p>Abstract:<br>Automatic medical report generation (MRG) is of great research value as it has the potential to relieve radiologists from the heavy burden of report writing. Despite recent advancements, accurate MRG remains challenging due to the need for precise clinical understanding and the identification of clinical findings. Moreover, the imbalanced distribution of diseases makes the challenge even more pronounced, as rare diseases are underrepresented in training data, making their diagnostic performance unreliable. To address these challenges, we propose diagnosis-driven prompts for medical report generation (PromptMRG), a novel framework that aims to improve the diagnostic accuracy of MRG with the guidance of diagnosis-aware prompts. Specifically, PromptMRG is based on encoder-decoder architecture with an extra disease classification branch. When generating reports, the diagnostic results from the classification branch are converted into token prompts to explicitly guide the generation process. To further improve the diagnostic accuracy, we design cross-modal feature enhancement, which retrieves similar reports from the database to assist the diagnosis of a query image by leveraging the knowledge from a pre-trained CLIP. Moreover, the disease imbalanced issue is addressed by applying an adaptive logit-adjusted loss to the classification branch based on the individual learning status of each disease, which overcomes the barrier of text decoder’s inability to manipulate disease distributions. Experiments on two MRG benchmarks show the effectiveness of the proposed method, where it obtains state-of-the-art clinical efficacy performance on both datasets.</p>
<hr>
<h2 id="Mind-vs-Mouth-On-Measuring-Re-judge-Inconsistency-of-Social-Bias-in-Large-Language-Models"><a href="#Mind-vs-Mouth-On-Measuring-Re-judge-Inconsistency-of-Social-Bias-in-Large-Language-Models" class="headerlink" title="Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models"></a>Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12578">http://arxiv.org/abs/2308.12578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yachao Zhao, Bo Wang, Dongming Zhao, Kun Huang, Yan Wang, Ruifang He, Yuexian Hou</li>
</ul>
<p>Abstract:<br>Recent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs. This paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology. It posits that individuals’ explicit social bias, which is their conscious expression of bias in the statements, may differ from their implicit social bias, which represents their unconscious bias. We propose a two-stage approach and discover a parallel phenomenon in LLMs known as “re-judge inconsistency” in social bias. In the initial stage, the LLM is tasked with automatically completing statements, potentially incorporating implicit social bias. However, in the subsequent stage, the same LLM re-judges the biased statement generated by itself but contradicts it. We propose that this re-judge inconsistency can be similar to the inconsistency between human’s unaware implicit social bias and their aware explicit social bias. Experimental investigations on ChatGPT and GPT-4 concerning common gender biases examined in psychology corroborate the highly stable nature of the re-judge inconsistency. This finding may suggest that diverse cognitive constructs emerge as LLMs’ capabilities strengthen. Consequently, leveraging psychological theories can provide enhanced insights into the underlying mechanisms governing the expressions of explicit and implicit constructs in LLMs.</p>
<hr>
<h2 id="A-Small-and-Fast-BERT-for-Chinese-Medical-Punctuation-Restoration"><a href="#A-Small-and-Fast-BERT-for-Chinese-Medical-Punctuation-Restoration" class="headerlink" title="A Small and Fast BERT for Chinese Medical Punctuation Restoration"></a>A Small and Fast BERT for Chinese Medical Punctuation Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12568">http://arxiv.org/abs/2308.12568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongtao Ling, Chen Liao, Zhipeng Yu, Lei Chen, Shilei Huang, Yi Liu</li>
</ul>
<p>Abstract:<br>In clinical dictation, utterances after automatic speech recognition (ASR) without explicit punctuation marks may lead to the misunderstanding of dictated reports. To give a precise and understandable clinical report with ASR, automatic punctuation restoration is required. Considering a practical scenario, we propose a fast and light pre-trained model for Chinese medical punctuation restoration based on ‘pretraining and fine-tuning’ paradigm. In this work, we distill pre-trained models by incorporating supervised contrastive learning and a novel auxiliary pre-training task (Punctuation Mark Prediction) to make it well-suited for punctuation restoration. Our experiments on various distilled models reveal that our model can achieve 95% performance while 10% model size relative to state-of-the-art Chinese RoBERTa.</p>
<hr>
<h2 id="CALM-A-Multi-task-Benchmark-for-Comprehensive-Assessment-of-Language-Model-Bias"><a href="#CALM-A-Multi-task-Benchmark-for-Comprehensive-Assessment-of-Language-Model-Bias" class="headerlink" title="CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias"></a>CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12539">http://arxiv.org/abs/2308.12539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vipulgupta1011/calm">https://github.com/vipulgupta1011/calm</a></li>
<li>paper_authors: Vipul Gupta, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, Rebecca J. Passonneau</li>
</ul>
<p>Abstract:<br>As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language models including six prominent families of LMs such as Llama-2. In two LM series, OPT and Bloom, we found that larger parameter models are more biased than lower parameter models. We found the T0 series of models to be the least biased. Furthermore, we noticed a tradeoff between gender and racial bias with increasing model size in some model series. The code is available at <a target="_blank" rel="noopener" href="https://github.com/vipulgupta1011/CALM">https://github.com/vipulgupta1011/CALM</a>.</p>
<hr>
<h2 id="CARE-Co-Attention-Network-for-Joint-Entity-and-Relation-Extraction"><a href="#CARE-Co-Attention-Network-for-Joint-Entity-and-Relation-Extraction" class="headerlink" title="CARE: Co-Attention Network for Joint Entity and Relation Extraction"></a>CARE: Co-Attention Network for Joint Entity and Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12531">http://arxiv.org/abs/2308.12531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjun Kong, Yamei Xia</li>
</ul>
<p>Abstract:<br>Joint entity and relation extraction is the fundamental task of information extraction, consisting of two subtasks: named entity recognition and relation extraction. Most existing joint extraction methods suffer from issues of feature confusion or inadequate interaction between two subtasks. In this work, we propose a Co-Attention network for joint entity and Relation Extraction (CARE). Our approach involves learning separate representations for each subtask, aiming to avoid feature overlap. At the core of our approach is the co-attention module that captures two-way interaction between two subtasks, allowing the model to leverage entity information for relation prediction and vice versa, thus promoting mutual enhancement. Extensive experiments on three joint entity-relation extraction benchmark datasets (NYT, WebNLG and SciERC) show that our proposed model achieves superior performance, surpassing existing baseline models.</p>
<hr>
<h2 id="Large-Language-Model-as-Autonomous-Decision-Maker"><a href="#Large-Language-Model-as-Autonomous-Decision-Maker" class="headerlink" title="Large Language Model as Autonomous Decision Maker"></a>Large Language Model as Autonomous Decision Maker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12519">http://arxiv.org/abs/2308.12519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yining Ye, Xin Cong, Yujia Qin, Yankai Lin, Zhiyuan Liu, Maosong Sun</li>
</ul>
<p>Abstract:<br>While large language models (LLMs) exhibit impressive language understanding and in-context learning abilities, their decision-making ability still heavily relies on the guidance of task-specific expert knowledge when solving real-world tasks. To unleash the potential of LLMs as autonomous decision makers, this paper presents an approach JuDec to endow LLMs with the self-judgment ability, enabling LLMs to achieve autonomous judgment and exploration for decision making. Specifically, in JuDec, Elo-based Self-Judgment Mechanism is designed to assign Elo scores to decision steps to judge their values and utilities via pairwise comparisons between two solutions and then guide the decision-searching process toward the optimal solution accordingly. Experimental results on the ToolBench dataset demonstrate JuDec’s superiority over baselines, achieving over 10% improvement in Pass Rate on diverse tasks. It offers higher-quality solutions and reduces costs (ChatGPT API calls), highlighting its effectiveness and efficiency.</p>
<hr>
<h2 id="MultiPA-a-multi-task-speech-pronunciation-assessment-system-for-a-closed-and-open-response-scenario"><a href="#MultiPA-a-multi-task-speech-pronunciation-assessment-system-for-a-closed-and-open-response-scenario" class="headerlink" title="MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario"></a>MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12490">http://arxiv.org/abs/2308.12490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Wen Chen, Zhou Yu, Julia Hirschberg</li>
</ul>
<p>Abstract:<br>The design of automatic speech pronunciation assessment can be categorized into closed and open response scenarios, each with strengths and limitations. A system with the ability to function in both scenarios can cater to diverse learning needs and provide a more precise and holistic assessment of pronunciation skills. In this study, we propose a Multi-task Pronunciation Assessment model called MultiPA. MultiPA provides an alternative to Kaldi-based systems in that it has simpler format requirements and better compatibility with other neural network models. Compared with previous open response systems, MultiPA provides a wider range of evaluations, encompassing assessments at both the sentence and word-level. Our experimental results show that MultiPA achieves comparable performance when working in closed response scenarios and maintains more robust performance when directly used for open responses.</p>
<hr>
<h2 id="GPTEval-A-Survey-on-Assessments-of-ChatGPT-and-GPT-4"><a href="#GPTEval-A-Survey-on-Assessments-of-ChatGPT-and-GPT-4" class="headerlink" title="GPTEval: A Survey on Assessments of ChatGPT and GPT-4"></a>GPTEval: A Survey on Assessments of ChatGPT and GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12488">http://arxiv.org/abs/2308.12488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, Erik Cambria</li>
</ul>
<p>Abstract:<br>The emergence of ChatGPT has generated much speculation in the press about its potential to disrupt social and economic systems. Its astonishing language ability has aroused strong curiosity among scholars about its performance in different domains. There have been many studies evaluating the ability of ChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive review summarizing the collective assessment findings is lacking. The objective of this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4, focusing on its language and reasoning abilities, scientific knowledge, and ethical considerations. Furthermore, an examination of the existing evaluation methods is conducted, offering several recommendations for future research in evaluating large language models.</p>
<hr>
<h2 id="American-Stories-A-Large-Scale-Structured-Text-Dataset-of-Historical-U-S-Newspapers"><a href="#American-Stories-A-Large-Scale-Structured-Text-Dataset-of-Historical-U-S-Newspapers" class="headerlink" title="American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers"></a>American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12477">http://arxiv.org/abs/2308.12477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Melissa Dell, Jacob Carlson, Tom Bryan, Emily Silcock, Abhishek Arora, Zejiang Shen, Luca D’Amico-Wong, Quan Le, Pablo Querubin, Leander Heldring</li>
</ul>
<p>Abstract:<br>Existing full text datasets of U.S. public domain newspapers do not recognize the often complex layouts of newspaper scans, and as a result the digitized content scrambles texts from articles, headlines, captions, advertisements, and other layout regions. OCR quality can also be low. This study develops a novel, deep learning pipeline for extracting full article texts from newspaper images and applies it to the nearly 20 million scans in Library of Congress’s public domain Chronicling America collection. The pipeline includes layout detection, legibility classification, custom OCR, and association of article texts spanning multiple bounding boxes. To achieve high scalability, it is built with efficient architectures designed for mobile phones. The resulting American Stories dataset provides high quality data that could be used for pre-training a large language model to achieve better understanding of historical English and historical world knowledge. The dataset could also be added to the external database of a retrieval-augmented language model to make historical information - ranging from interpretations of political events to minutiae about the lives of people’s ancestors - more widely accessible. Furthermore, structured article texts facilitate using transformer-based methods for popular social science applications like topic classification, detection of reproduced content, and news story clustering. Finally, American Stories provides a massive silver quality dataset for innovating multimodal layout analysis models and other multimodal applications.</p>
<hr>
<h2 id="Are-ChatGPT-and-GPT-4-Good-Poker-Players-–-A-Pre-Flop-Analysis"><a href="#Are-ChatGPT-and-GPT-4-Good-Poker-Players-–-A-Pre-Flop-Analysis" class="headerlink" title="Are ChatGPT and GPT-4 Good Poker Players? – A Pre-Flop Analysis"></a>Are ChatGPT and GPT-4 Good Poker Players? – A Pre-Flop Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12466">http://arxiv.org/abs/2308.12466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshat Gupta</li>
</ul>
<p>Abstract:<br>Since the introduction of ChatGPT and GPT-4, these models have been tested across a large number of tasks. Their adeptness across domains is evident, but their aptitude in playing games and specifically their aptitude in the realm of poker has remained unexplored. Poker is a game that requires decision making under uncertainty and incomplete information. In this paper, we put ChatGPT and GPT-4 through the poker test and evaluate their poker skills. Our findings reveal that while both models display an advanced understanding of poker, encompassing concepts like the valuation of starting hands, playing positions and other intricacies of game theory optimal (GTO) poker, both ChatGPT and GPT-4 are NOT game theory optimal poker players.   Through a series of experiments, we first discover the characteristics of optimal prompts and model parameters for playing poker with these models. Our observations then unveil the distinct playing personas of the two models. We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This exploration then sheds light on the divergent poker tactics of the two models: ChatGPT’s conservativeness juxtaposed against GPT-4’s aggression. In poker vernacular, when tasked to play GTO poker, ChatGPT plays like a Nit, which means that it has a propensity to only engage with premium hands and folds a majority of hands. When subjected to the same directive, GPT-4 plays like a maniac, showcasing a loose and aggressive style of play. Both strategies, although relatively advanced, are not game theory optimal.</p>
<hr>
<h2 id="Evolution-of-ESG-focused-DLT-Research-An-NLP-Analysis-of-the-Literature"><a href="#Evolution-of-ESG-focused-DLT-Research-An-NLP-Analysis-of-the-Literature" class="headerlink" title="Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature"></a>Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12420">http://arxiv.org/abs/2308.12420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Walter Hernandez, Kamil Tylinski, Alastair Moore, Niall Roche, Nikhil Vadgama, Horst Treiblmaier, Jiangbo Shangguan, Paolo Tasca, Jiahua Xu</li>
</ul>
<p>Abstract:<br>Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT’s ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our contributions are a methodology to conduct a machine learning-driven systematic literature review in the DLT field, placing a special emphasis on ESG aspects. Furthermore, we present a first-of-its-kind NER dataset, composed of 54,808 named entities, designed for DLT and ESG-related explorations.</p>
<hr>
<h2 id="Toward-American-Sign-Language-Processing-in-the-Real-World-Data-Tasks-and-Methods"><a href="#Toward-American-Sign-Language-Processing-in-the-Real-World-Data-Tasks-and-Methods" class="headerlink" title="Toward American Sign Language Processing in the Real World: Data, Tasks, and Methods"></a>Toward American Sign Language Processing in the Real World: Data, Tasks, and Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12419">http://arxiv.org/abs/2308.12419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Shi</li>
</ul>
<p>Abstract:<br>Sign language, which conveys meaning through gestures, is the chief means of communication among deaf people. Recognizing sign language in natural settings presents significant challenges due to factors such as lighting, background clutter, and variations in signer characteristics. In this thesis, I study automatic sign language processing in the wild, using signing videos collected from the Internet. This thesis contributes new datasets, tasks, and methods. Most chapters of this thesis address tasks related to fingerspelling, an important component of sign language and yet has not been studied widely by prior work. I present three new large-scale ASL datasets in the wild: ChicagoFSWild, ChicagoFSWild+, and OpenASL. Using ChicagoFSWild and ChicagoFSWild+, I address fingerspelling recognition, which consists of transcribing fingerspelling sequences into text. I propose an end-to-end approach based on iterative attention that allows recognition from a raw video without explicit hand detection. I further show that using a Conformer-based network jointly modeling handshape and mouthing can bring performance close to that of humans. Next, I propose two tasks for building real-world fingerspelling-based applications: fingerspelling detection and search. For fingerspelling detection, I introduce a suite of evaluation metrics and a new detection model via multi-task training. To address the problem of searching for fingerspelled keywords in raw sign language videos, we propose a novel method that jointly localizes and matches fingerspelling segments to text. Finally, I will describe a benchmark for large-vocabulary open-domain sign language translation based on OpenASL. To address the challenges of sign language translation in realistic settings, we propose a set of techniques including sign search as a pretext task for pre-training and fusion of mouthing and handshape features.</p>
<hr>
<h2 id="With-a-Little-Help-from-your-own-Past-Prototypical-Memory-Networks-for-Image-Captioning"><a href="#With-a-Little-Help-from-your-own-Past-Prototypical-Memory-Networks-for-Image-Captioning" class="headerlink" title="With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning"></a>With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12383">http://arxiv.org/abs/2308.12383</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aimagelab/pma-net">https://github.com/aimagelab/pma-net</a></li>
<li>paper_authors: Manuele Barraco, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</li>
</ul>
<p>Abstract:<br>Image captioning, like many tasks involving vision and language, currently relies on Transformer-based architectures for extracting the semantics in an image and translating it into linguistically coherent descriptions. Although successful, the attention operator only considers a weighted summation of projections of the current input sample, therefore ignoring the relevant semantic information which can come from the joint observation of other samples. In this paper, we devise a network which can perform attention over activations obtained while processing other training samples, through a prototypical memory model. Our memory models the distribution of past keys and values through the definition of prototype vectors which are both discriminative and compact. Experimentally, we assess the performance of the proposed model on the COCO dataset, in comparison with carefully designed baselines and state-of-the-art approaches, and by investigating the role of each of the proposed components. We demonstrate that our proposal can increase the performance of an encoder-decoder Transformer by 3.7 CIDEr points both when training in cross-entropy only and when fine-tuning with self-critical sequence training. Source code and trained models are available at: <a target="_blank" rel="noopener" href="https://github.com/aimagelab/PMA-Net">https://github.com/aimagelab/PMA-Net</a>.</p>
<hr>
<h2 id="Vision-Transformer-Adapters-for-Generalizable-Multitask-Learning"><a href="#Vision-Transformer-Adapters-for-Generalizable-Multitask-Learning" class="headerlink" title="Vision Transformer Adapters for Generalizable Multitask Learning"></a>Vision Transformer Adapters for Generalizable Multitask Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12372">http://arxiv.org/abs/2308.12372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deblina Bhattacharjee, Sabine Süsstrunk, Mathieu Salzmann</li>
</ul>
<p>Abstract:<br>We introduce the first multitasking vision transformer adapters that learn generalizable task affinities which can be applied to novel tasks and domains. Integrated into an off-the-shelf vision transformer backbone, our adapters can simultaneously solve multiple dense vision tasks in a parameter-efficient manner, unlike existing multitasking transformers that are parametrically expensive. In contrast to concurrent methods, we do not require retraining or fine-tuning whenever a new task or domain is added. We introduce a task-adapted attention mechanism within our adapter framework that combines gradient-based task similarities with attention-based ones. The learned task affinities generalize to the following settings: zero-shot task transfer, unsupervised domain adaptation, and generalization without fine-tuning to novel domains. We demonstrate that our approach outperforms not only the existing convolutional neural network-based multitasking methods but also the vision transformer-based ones. Our project page is at \url{<a target="_blank" rel="noopener" href="https://ivrl.github.io/VTAGML%7D">https://ivrl.github.io/VTAGML}</a>.</p>
<hr>
<h2 id="D4-Improving-LLM-Pretraining-via-Document-De-Duplication-and-Diversification"><a href="#D4-Improving-LLM-Pretraining-via-Document-De-Duplication-and-Diversification" class="headerlink" title="D4: Improving LLM Pretraining via Document De-Duplication and Diversification"></a>D4: Improving LLM Pretraining via Document De-Duplication and Diversification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12284">http://arxiv.org/abs/2308.12284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kushal Tirumala, Daniel Simig, Armen Aghajanyan, Ari S. Morcos</li>
</ul>
<p>Abstract:<br>Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever data selection can significantly improve LLM pre-training, calls into question the common practice of training for a single epoch on as much data as possible, and demonstrates a path to keep improving our models past the limits of randomly sampling web data.</p>
<hr>
<h2 id="Simple-is-Better-and-Large-is-Not-Enough-Towards-Ensembling-of-Foundational-Language-Models"><a href="#Simple-is-Better-and-Large-is-Not-Enough-Towards-Ensembling-of-Foundational-Language-Models" class="headerlink" title="Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models"></a>Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12272">http://arxiv.org/abs/2308.12272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nancy Tyagi, Aidin Shiri, Surjodeep Sarkar, Abhishek Kumar Umrawal, Manas Gaur</li>
</ul>
<p>Abstract:<br>Foundational Language Models (FLMs) have advanced natural language processing (NLP) research. Current researchers are developing larger FLMs (e.g., XLNet, T5) to enable contextualized language representation, classification, and generation. While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty. Fundamentally, larger FLMs are built on the same foundations as smaller FLMs (e.g., BERT); hence, one must recognize the potential of smaller FLMs which can be realized through an ensemble. In the current research, we perform a reality check on FLMs and their ensemble on benchmark and real-world datasets. We hypothesize that the ensembling of FLMs can influence the individualistic attention of FLMs and unravel the strength of coordination and cooperation of different FLMs. We utilize BERT and define three other ensemble techniques: {Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a knowledge-guided reinforcement learning approach. We discovered that the suggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by a factor of many times using datasets that show the usefulness of NLP in sensitive fields, such as mental health.</p>
<hr>
<h2 id="Prompt2Model-Generating-Deployable-Models-from-Natural-Language-Instructions"><a href="#Prompt2Model-Generating-Deployable-Models-from-Natural-Language-Instructions" class="headerlink" title="Prompt2Model: Generating Deployable Models from Natural Language Instructions"></a>Prompt2Model: Generating Deployable Models from Natural Language Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12261">http://arxiv.org/abs/2308.12261</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neulab/prompt2model">https://github.com/neulab/prompt2model</a></li>
<li>paper_authors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, Graham Neubig</li>
</ul>
<p>Abstract:<br>Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at <a target="_blank" rel="noopener" href="https://github.com/neulab/prompt2model">https://github.com/neulab/prompt2model</a>.</p>
<hr>
<h2 id="How-to-Protect-Copyright-Data-in-Optimization-of-Large-Language-Models"><a href="#How-to-Protect-Copyright-Data-in-Optimization-of-Large-Language-Models" class="headerlink" title="How to Protect Copyright Data in Optimization of Large Language Models?"></a>How to Protect Copyright Data in Optimization of Large Language Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12247">http://arxiv.org/abs/2308.12247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timothy Chu, Zhao Song, Chiwun Yang</li>
</ul>
<p>Abstract:<br>Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.   In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.</p>
<hr>
<h2 id="Diffusion-Language-Models-Can-Perform-Many-Tasks-with-Scaling-and-Instruction-Finetuning"><a href="#Diffusion-Language-Models-Can-Perform-Many-Tasks-with-Scaling-and-Instruction-Finetuning" class="headerlink" title="Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning"></a>Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12219">http://arxiv.org/abs/2308.12219</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yegcjs/diffusionllm">https://github.com/yegcjs/diffusionllm</a></li>
<li>paper_authors: Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Quanquan Gu</li>
</ul>
<p>Abstract:<br>The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/24/cs.CL_2023_08_24/" data-id="cllqyyxas001gbpr8cl3850zd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.AI_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.AI_2023_08_23/">cs.AI - 2023-08-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CLIPN-for-Zero-Shot-OOD-Detection-Teaching-CLIP-to-Say-No"><a href="#CLIPN-for-Zero-Shot-OOD-Detection-Teaching-CLIP-to-Say-No" class="headerlink" title="CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No"></a>CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12213">http://arxiv.org/abs/2308.12213</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/clipn">https://github.com/xmed-lab/clipn</a></li>
<li>paper_authors: Hualiang Wang, Yi Li, Huifeng Yao, Xiaomeng Li</li>
</ul>
<p>Abstract:<br>Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes. Considerable effort has been invested in designing various OOD detection methods based on either convolutional neural networks or transformers. However, zero-shot OOD detection methods driven by CLIP, which only require class names for ID, have received less attention. This paper presents a novel method, namely CLIP saying no (CLIPN), which empowers the logic of saying no within CLIP. Our key motivation is to equip CLIP with the capability of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts. Specifically, we design a novel learnable no prompt and a no text encoder to capture negation semantics within images. Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to associate images with no prompts, thereby enabling it to identify unknown samples. Furthermore, we propose two threshold-free inference algorithms to perform OOD detection by utilizing negation semantics from no prompts and the text encoder. Experimental results on 9 benchmark datasets (3 ID datasets and 6 OOD datasets) for the OOD detection task demonstrate that CLIPN, based on ViT-B-16, outperforms 7 well-used algorithms by at least 2.34% and 11.64% in terms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K. Our CLIPN can serve as a solid foundation for effectively leveraging CLIP in downstream OOD tasks. The code is available on <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/CLIPN">https://github.com/xmed-lab/CLIPN</a>.</p>
<hr>
<h2 id="Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies"><a href="#Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies" class="headerlink" title="Learning to Learn Financial Networks for Optimising Momentum Strategies"></a>Learning to Learn Financial Networks for Optimising Momentum Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12212">http://arxiv.org/abs/2308.12212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyue, Pu, Stefan Zohren, Stephen Roberts, Xiaowen Dong</li>
</ul>
<p>Abstract:<br>Network momentum provides a novel type of risk premium, which exploits the interconnections among assets in a financial network to predict future returns. However, the current process of constructing financial networks relies heavily on expensive databases and financial expertise, limiting accessibility for small-sized and academic institutions. Furthermore, the traditional approach treats network construction and portfolio optimisation as separate tasks, potentially hindering optimal portfolio performance. To address these challenges, we propose L2GMOM, an end-to-end machine learning framework that simultaneously learns financial networks and optimises trading signals for network momentum strategies. The model of L2GMOM is a neural network with a highly interpretable forward propagation architecture, which is derived from algorithm unrolling. The L2GMOM is flexible and can be trained with diverse loss functions for portfolio performance, e.g. the negative Sharpe ratio. Backtesting on 64 continuous future contracts demonstrates a significant improvement in portfolio profitability and risk control, with a Sharpe ratio of 1.74 across a 20-year period.</p>
<hr>
<h2 id="Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques"><a href="#Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques" class="headerlink" title="Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques"></a>Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12192">http://arxiv.org/abs/2308.12192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophie A. Neubauer, Radu Grosu</li>
</ul>
<p>Abstract:<br>This paper presents, in a unified fashion, deterministic as well as statistical Lagrangian-verification techniques. They formally quantify the behavioral robustness of any time-continuous process, formulated as a continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube, algorithms for constructing a tight reachtube, that is, an over-approximation of the set of states reachable within a given time-horizon, and provide guarantees for the reachtube bounds. We compare the usage of the variational equations, associated to the system equations, the mean value theorem, and the Lipschitz constants, in achieving deterministic and statistical guarantees. In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation, to compute the radius of an ellipsoid in an optimal metric, which over-approximates the set of reachable states. In SLR and GoTube, we get statistical guarantees, by using the Lipschitz constants to compute local balls around samples. These are needed to calculate the probability of having found an upper bound, of the true maximum perturbation at every timestep. Our experiments demonstrate the superior performance of Lagrangian techniques, when compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness analysis of various continuous-depth models.</p>
<hr>
<h2 id="Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning"><a href="#Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning" class="headerlink" title="Unsupervised anomalies detection in IIoT edge devices networks using federated learning"></a>Unsupervised anomalies detection in IIoT edge devices networks using federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12175">http://arxiv.org/abs/2308.12175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niyomukiza Thamar, Hossam Samy Elsaid Sharara</li>
</ul>
<p>Abstract:<br>In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules. However, some owners are reluctant of availing their data out of the company due to data security concerns. Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself. In this scenario, data is not share over the network for training purpose. Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session. The devices could be chosen at random, and a device can be aborted. The resulting models are sent to the coordinating server and then average models from the devices that finished training. The process is repeated until a desired model accuracy is achieved. By doing this, FL approach solves the privacy problem for IoT&#x2F; IIoT devices that held sensitive data for the owners. In this paper, we leverage the benefits of FL and implemented Fedavg algorithm on a recent dataset that represent the modern IoT&#x2F; IIoT device networks. The results were almost the same as the centralized machine learning approach. We also evaluated some shortcomings of Fedavg such as unfairness that happens during the training when struggling devices do not participate for every stage of training. This inefficient training of local or global model could lead in a high number of false alarms in intrusion detection systems for IoT&#x2F;IIoT gadgets developed using Fedavg. Hence, after evaluating the FedAv deep auto encoder with centralized deep auto encoder ML, we further proposed and designed a Fair Fedavg algorithm that will be evaluated in the future work.</p>
<hr>
<h2 id="Evaluation-of-Faithfulness-Using-the-Longest-Supported-Subsequence"><a href="#Evaluation-of-Faithfulness-Using-the-Longest-Supported-Subsequence" class="headerlink" title="Evaluation of Faithfulness Using the Longest Supported Subsequence"></a>Evaluation of Faithfulness Using the Longest Supported Subsequence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12157">http://arxiv.org/abs/2308.12157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Mittal, Timo Schick, Mikel Artetxe, Jane Dwivedi-Yu</li>
</ul>
<p>Abstract:<br>As increasingly sophisticated language models emerge, their trustworthiness becomes a pivotal issue, especially in tasks such as summarization and question-answering. Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers. In this paper, we introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, which we refer to as the Longest Supported Subsequence (LSS). Using a new human-annotated dataset, we finetune a model to generate LSS. We introduce a new method of evaluation and demonstrate that these metrics correlate better with human ratings when LSS is employed, as opposed to when it is not. Our proposed metric demonstrates an 18% enhancement over the prevailing state-of-the-art metric for faithfulness on our dataset. Our metric consistently outperforms other metrics on a summarization dataset across six different models. Finally, we compare several popular Large Language Models (LLMs) for faithfulness using this metric. We release the human-annotated dataset built for predicting LSS and our fine-tuned model for evaluating faithfulness.</p>
<hr>
<h2 id="Multimodal-Latent-Emotion-Recognition-from-Micro-expression-and-Physiological-Signals"><a href="#Multimodal-Latent-Emotion-Recognition-from-Micro-expression-and-Physiological-Signals" class="headerlink" title="Multimodal Latent Emotion Recognition from Micro-expression and Physiological Signals"></a>Multimodal Latent Emotion Recognition from Micro-expression and Physiological Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12156">http://arxiv.org/abs/2308.12156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangfei Zhang, Yifei Qian, Ognjen Arandjelovic, Anthony Zhu</li>
</ul>
<p>Abstract:<br>This paper discusses the benefits of incorporating multimodal data for improving latent emotion recognition accuracy, focusing on micro-expression (ME) and physiological signals (PS). The proposed approach presents a novel multimodal learning framework that combines ME and PS, including a 1D separable and mixable depthwise inception network, a standardised normal distribution weighted feature fusion method, and depth&#x2F;physiology guided attention modules for multimodal learning. Experimental results show that the proposed approach outperforms the benchmark method, with the weighted fusion method and guided attention modules both contributing to enhanced performance.</p>
<hr>
<h2 id="A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models"><a href="#A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models" class="headerlink" title="A Probabilistic Fluctuation based Membership Inference Attack for Generative Models"></a>A Probabilistic Fluctuation based Membership Inference Attack for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12143">http://arxiv.org/abs/2308.12143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang</li>
</ul>
<p>Abstract:<br>Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model’s training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting these trends via analyzing the overall probabilistic fluctuations around given records. We conduct extensive experiments across multiple generative models and datasets, which demonstrate PFAMI can improve the attack success rate (ASR) by about 27.9% when compared with the best baseline.</p>
<hr>
<h2 id="Semantic-Change-Detection-for-the-Romanian-Language"><a href="#Semantic-Change-Detection-for-the-Romanian-Language" class="headerlink" title="Semantic Change Detection for the Romanian Language"></a>Semantic Change Detection for the Romanian Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12131">http://arxiv.org/abs/2308.12131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ds4ai-upb/semanticchange-ro">https://github.com/ds4ai-upb/semanticchange-ro</a></li>
<li>paper_authors: Ciprian-Octavian Truică, Victor Tudose, Elena-Simona Apostol</li>
</ul>
<p>Abstract:<br>Automatic semantic change methods try to identify the changes that appear over time in the meaning of words by analyzing their usage in diachronic corpora. In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and ELMo, on real-world English and Romanian datasets. To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an English dataset (SEMEVAL-CCOHA). Afterward, we focus our experiments on a Romanian dataset, and we underline different aspects of semantic changes in this low-resource language, such as meaning acquisition and loss. The experimental results show that, depending on the corpus, the most important factors to consider are the choice of model and the distance to calculate a score for detecting semantic change.</p>
<hr>
<h2 id="Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models"><a href="#Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models" class="headerlink" title="Masking Strategies for Background Bias Removal in Computer Vision Models"></a>Masking Strategies for Background Bias Removal in Computer Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12127">http://arxiv.org/abs/2308.12127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ananthu-aniraj/masking_strategies_bias_removal">https://github.com/ananthu-aniraj/masking_strategies_bias_removal</a></li>
<li>paper_authors: Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos</li>
</ul>
<p>Abstract:<br>Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.</p>
<hr>
<h2 id="Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient"><a href="#Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient" class="headerlink" title="Quantifying degeneracy in singular models via the learning coefficient"></a>Quantifying degeneracy in singular models via the learning coefficient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12108">http://arxiv.org/abs/2308.12108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edmundlth/scalable_learning_coefficient_with_sgld">https://github.com/edmundlth/scalable_learning_coefficient_with_sgld</a></li>
<li>paper_authors: Edmund Lau, Daniel Murfet, Susan Wei</li>
</ul>
<p>Abstract:<br>Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of “flat” directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.</p>
<hr>
<h2 id="Out-of-the-Cage-How-Stochastic-Parrots-Win-in-Cyber-Security-Environments"><a href="#Out-of-the-Cage-How-Stochastic-Parrots-Win-in-Cyber-Security-Environments" class="headerlink" title="Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments"></a>Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12086">http://arxiv.org/abs/2308.12086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Rigaki, Ondřej Lukáš, Carlos A. Catania, Sebastian Garcia</li>
</ul>
<p>Abstract:<br>Large Language Models (LLMs) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks. Despite their inherent limitations, LLM-based designs have shown promising capabilities in planning and navigating open-world scenarios. This paper introduces a novel application of pre-trained LLMs as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes.   We present an approach wherein pre-trained LLMs are leveraged as attacking agents in two reinforcement learning environments. Our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations. In addition, the best LLM agents perform similarly to human testers of the environment without any additional training process. This design highlights the potential of LLMs to efficiently address complex decision-making tasks within cybersecurity.   Furthermore, we introduce a new network security environment named NetSecGame. The environment is designed to eventually support complex multi-agent scenarios within the network security domain. The proposed environment mimics real network attacks and is designed to be highly modular and adaptable for various scenarios.</p>
<hr>
<h2 id="Stabilizing-RNN-Gradients-through-Pre-training"><a href="#Stabilizing-RNN-Gradients-through-Pre-training" class="headerlink" title="Stabilizing RNN Gradients through Pre-training"></a>Stabilizing RNN Gradients through Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12075">http://arxiv.org/abs/2308.12075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Herranz-Celotti, Jean Rouat</li>
</ul>
<p>Abstract:<br>Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponential explosion that emerges from counting gradient paths in a rectangular grid in depth and time. We propose a new approach to mitigate this issue, that consists on giving a weight of a half to the time and depth contributions to the gradient, instead of the classical weight of one. Our empirical results confirm that pre-training both feed-forward and recurrent networks to fulfill the LSC often results in improved final performance across models. This study contributes to the field by providing a means to stabilize networks of any complexity. Our approach can be implemented as an additional step before pre-training on large augmented datasets, and as an alternative to finding stable initializations analytically.</p>
<hr>
<h2 id="Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning"><a href="#Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning" class="headerlink" title="Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning"></a>Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12069">http://arxiv.org/abs/2308.12069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ni Dang, Tao Shi, Zengjie Zhang, Wanxin Jin, Marion Leibold, Martin Buss</li>
</ul>
<p>Abstract:<br>The driving style of an Autonomous Vehicle (AV) refers to how it behaves and interacts with other AVs. In a multi-vehicle autonomous driving system, an AV capable of identifying the driving styles of its nearby AVs can reliably evaluate the risk of collisions and make more reasonable driving decisions. However, there has not been a consistent definition of driving styles for an AV in the literature, although it is considered that the driving style is encoded in the AV’s trajectories and can be identified using Maximum Entropy Inverse Reinforcement Learning (ME-IRL) methods as a cost function. Nevertheless, an important indicator of the driving style, i.e., how an AV reacts to its nearby AVs, is not fully incorporated in the feature design of previous ME-IRL methods. In this paper, we describe the driving style as a cost function of a series of weighted features. We design additional novel features to capture the AV’s reaction-aware characteristics. Then, we identify the driving styles from the demonstration trajectories generated by the Stochastic Model Predictive Control (SMPC) using a modified ME-IRL method with our newly proposed features. The proposed method is validated using MATLAB simulation and an off-the-shelf experiment.</p>
<hr>
<h2 id="RemovalNet-DNN-Fingerprint-Removal-Attacks"><a href="#RemovalNet-DNN-Fingerprint-Removal-Attacks" class="headerlink" title="RemovalNet: DNN Fingerprint Removal Attacks"></a>RemovalNet: DNN Fingerprint Removal Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12319">http://arxiv.org/abs/2308.12319</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grasses/removalnet">https://github.com/grasses/removalnet</a></li>
<li>paper_authors: Hongwei Yao, Zheng Li, Kunzhe Huang, Jian Lou, Zhan Qin, Kui Ren</li>
</ul>
<p>Abstract:<br>With the performance of deep neural networks (DNNs) remarkably improving, DNNs have been widely used in many areas. Consequently, the DNN model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., DNN fingerprinting). However, the feasibility of the DNN fingerprint removal attack and its potential influence remains an open problem. In this paper, we perform the first comprehensive investigation of DNN fingerprint removal attacks. Generally, the knowledge contained in a DNN model can be categorized into general semantic and fingerprint-specific knowledge. To this end, we propose a min-max bilevel optimization-based DNN fingerprint removal attack named RemovalNet, to evade model ownership verification. The lower-level optimization is designed to remove fingerprint-specific knowledge. While in the upper-level optimization, we distill the victim model’s general semantic knowledge to maintain the surrogate model’s performance. We conduct extensive experiments to evaluate the fidelity, effectiveness, and efficiency of the RemovalNet against four advanced defense methods on six metrics. The empirical results demonstrate that (1) the RemovalNet is effective. After our DNN fingerprint removal attack, the model distance between the target and surrogate models is x100 times higher than that of the baseline attacks, (2) the RemovalNet is efficient. It uses only 0.2% (400 samples) of the substitute dataset and 1,000 iterations to conduct our attack. Besides, compared with advanced model stealing attacks, the RemovalNet saves nearly 85% of computational resources at most, (3) the RemovalNet achieves high fidelity that the created surrogate model maintains high accuracy after the DNN fingerprint removal process. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/grasses/RemovalNet">https://github.com/grasses/RemovalNet</a>.</p>
<hr>
<h2 id="InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4"><a href="#InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4" class="headerlink" title="InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"></a>InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12067">http://arxiv.org/abs/2308.12067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun</li>
</ul>
<p>Abstract:<br>Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient to enable multimodal large language models to generate better output.</p>
<hr>
<h2 id="Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference"><a href="#Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference" class="headerlink" title="Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference"></a>Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12066">http://arxiv.org/abs/2308.12066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ranggi Hwang, Jianyu Wei, Shijie Cao, Changho Hwang, Xiaohu Tang, Ting Cao, Mao Yang, Minsoo Rhu</li>
</ul>
<p>Abstract:<br>Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE’s high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE’s memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs our novel pre-gating function which alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE is able to improve performance, reduce GPU memory consumption, while also maintaining the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.</p>
<hr>
<h2 id="Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers"><a href="#Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers" class="headerlink" title="Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers"></a>Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12065">http://arxiv.org/abs/2308.12065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Zoppi, Andrea Ceccarelli, Andrea Bondavalli</li>
</ul>
<p>Abstract:<br>Machine Learning (ML) algorithms that perform classification may predict the wrong class, experiencing misclassifications. It is well-known that misclassifications may have cascading effects on the encompassing system, possibly resulting in critical failures. This paper proposes SPROUT, a Safety wraPper thROugh ensembles of UncertainTy measures, which suspects misclassifications by computing uncertainty measures on the inputs and outputs of a black-box classifier. If a misclassification is detected, SPROUT blocks the propagation of the output of the classifier to the encompassing system. The resulting impact on safety is that SPROUT transforms erratic outputs (misclassifications) into data omission failures, which can be easily managed at the system level. SPROUT has a broad range of applications as it fits binary and multi-class classification, comprising image and tabular datasets. We experimentally show that SPROUT always identifies a huge fraction of the misclassifications of supervised classifiers, and it is able to detect all misclassifications in specific cases. SPROUT implementation contains pre-trained wrappers, it is publicly available and ready to be deployed with minimal effort.</p>
<hr>
<h2 id="FlexKBQA-A-Flexible-LLM-Powered-Framework-for-Few-Shot-Knowledge-Base-Question-Answering"><a href="#FlexKBQA-A-Flexible-LLM-Powered-Framework-for-Few-Shot-Knowledge-Base-Question-Answering" class="headerlink" title="FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering"></a>FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12060">http://arxiv.org/abs/2308.12060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leezythu/flexkbqa">https://github.com/leezythu/flexkbqa</a></li>
<li>paper_authors: Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, Zhichao Duan, Bowen Dong, Ning Liu, Jianyong Wang</li>
</ul>
<p>Abstract:<br>Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executionguided self-training method to iterative leverage unlabeled user questions. Furthermore, we explore harnessing the inherent reasoning capability of LLMs to enhance the entire framework. Consequently, FlexKBQA delivers substantial flexibility, encompassing data annotation, deployment, and being domain agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we observe that under the few-shot even the more challenging zero-shot scenarios, FlexKBQA achieves impressive results with a few annotations, surpassing all previous baselines and even approaching the performance of supervised models, achieving a remarkable 93% performance relative to the fully-supervised models. We posit that FlexKBQA represents a significant advancement towards exploring better integration of large and lightweight models. The code is open-sourced.</p>
<hr>
<h2 id="Layer-wise-Feedback-Propagation"><a href="#Layer-wise-Feedback-Propagation" class="headerlink" title="Layer-wise Feedback Propagation"></a>Layer-wise Feedback Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12053">http://arxiv.org/abs/2308.12053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leander Weber, Jim Berend, Alexander Binder, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</li>
</ul>
<p>Abstract:<br>In this paper, we present Layer-wise Feedback Propagation (LFP), a novel training approach for neural-network-like predictors that utilizes explainability, specifically Layer-wise Relevance Propagation(LRP), to assign rewards to individual connections based on their respective contributions to solving a given task. This differs from traditional gradient descent, which updates parameters towards anestimated loss minimum. LFP distributes a reward signal throughout the model without the need for gradient computations. It then strengthens structures that receive positive feedback while reducingthe influence of structures that receive negative feedback. We establish the convergence of LFP theoretically and empirically, and demonstrate its effectiveness in achieving comparable performance to gradient descent on various models and datasets. Notably, LFP overcomes certain limitations associated with gradient-based methods, such as reliance on meaningful derivatives. We further investigate how the different LRP-rules can be extended to LFP, what their effects are on training, as well as potential applications, such as training models with no meaningful derivatives, e.g., step-function activated Spiking Neural Networks (SNNs), or for transfer learning, to efficiently utilize existing knowledge.</p>
<hr>
<h2 id="Aligning-Language-Models-with-Offline-Reinforcement-Learning-from-Human-Feedback"><a href="#Aligning-Language-Models-with-Offline-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Aligning Language Models with Offline Reinforcement Learning from Human Feedback"></a>Aligning Language Models with Offline Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12050">http://arxiv.org/abs/2308.12050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Hu, Li Tao, June Yang, Chandler Zhou</li>
</ul>
<p>Abstract:<br>Learning from human preferences is crucial for language models (LMs) to effectively cater to human needs and societal values. Previous research has made notable progress by leveraging human feedback to follow instructions. However, these approaches rely primarily on online reinforcement learning (RL) techniques like Proximal Policy Optimization (PPO), which have been proven unstable and challenging to tune for language models. Moreover, PPO requires complex distributed system implementation, hindering the efficiency of large-scale distributed training. In this study, we propose an offline reinforcement learning from human feedback (RLHF) framework to align LMs using pre-generated samples without interacting with RL environments. Specifically, we explore maximum likelihood estimation (MLE) with filtering, reward-weighted regression (RWR), and Decision Transformer (DT) to align language models to human preferences. By employing a loss function similar to supervised fine-tuning, our methods ensure more stable model training than PPO with a simple machine learning system~(MLSys) and much fewer (around 12.3%) computing resources. Experimental results demonstrate the DT alignment outperforms other Offline RLHF methods and is better than PPO.</p>
<hr>
<h2 id="Towards-Privacy-Supporting-Fall-Detection-via-Deep-Unsupervised-RGB2Depth-Adaptation"><a href="#Towards-Privacy-Supporting-Fall-Detection-via-Deep-Unsupervised-RGB2Depth-Adaptation" class="headerlink" title="Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation"></a>Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12049">http://arxiv.org/abs/2308.12049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/1015206533/privacy_supporting_fall_detection">https://github.com/1015206533/privacy_supporting_fall_detection</a></li>
<li>paper_authors: Hejun Xiao, Kunyu Peng, Xiangsheng Huang, Alina Roitberg1, Hao Li, Zhaohui Wang, Rainer Stiefelhagen</li>
</ul>
<p>Abstract:<br>Fall detection is a vital task in health monitoring, as it allows the system to trigger an alert and therefore enabling faster interventions when a person experiences a fall. Although most previous approaches rely on standard RGB video data, such detailed appearance-aware monitoring poses significant privacy concerns. Depth sensors, on the other hand, are better at preserving privacy as they merely capture the distance of objects from the sensor or camera, omitting color and texture information. In this paper, we introduce a privacy-supporting solution that makes the RGB-trained model applicable in depth domain and utilizes depth data at test time for fall detection. To achieve cross-modal fall detection, we present an unsupervised RGB to Depth (RGB2Depth) cross-modal domain adaptation approach that leverages labelled RGB data and unlabelled depth data during training. Our proposed pipeline incorporates an intermediate domain module for feature bridging, modality adversarial loss for modality discrimination, classification loss for pseudo-labeled depth data and labeled source data, triplet loss that considers both source and target domains, and a novel adaptive loss weight adjustment method for improved coordination among various losses. Our approach achieves state-of-the-art results in the unsupervised RGB2Depth domain adaptation task for fall detection. Code is available at <a target="_blank" rel="noopener" href="https://github.com/1015206533/privacy_supporting_fall_detection">https://github.com/1015206533/privacy_supporting_fall_detection</a>.</p>
<hr>
<h2 id="CgT-GAN-CLIP-guided-Text-GAN-for-Image-Captioning"><a href="#CgT-GAN-CLIP-guided-Text-GAN-for-Image-Captioning" class="headerlink" title="CgT-GAN: CLIP-guided Text GAN for Image Captioning"></a>CgT-GAN: CLIP-guided Text GAN for Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12045">http://arxiv.org/abs/2308.12045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lihr747/cgtgan">https://github.com/lihr747/cgtgan</a></li>
<li>paper_authors: Jiarui Yu, Haoran Li, Yanbin Hao, Bin Zhu, Tong Xu, Xiangnan He</li>
</ul>
<p>Abstract:<br>The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training&#x2F;inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to “see” real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN’s discriminator and the semantic guidance reward computed by the CLIP-based reward module. In addition to the cosine similarity as the semantic guidance reward (i.e., CLIP-cos), we further introduce a novel semantic guidance reward called CLIP-agg, which aligns the generated caption with a weighted text embedding by attentively aggregating the entire corpus. Experimental results on three subtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms state-of-the-art methods significantly across all metrics. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Lihr747/CgtGAN">https://github.com/Lihr747/CgtGAN</a>.</p>
<hr>
<h2 id="A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks"><a href="#A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks" class="headerlink" title="A multiobjective continuation method to compute the regularization path of deep neural networks"></a>A multiobjective continuation method to compute the regularization path of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12044">http://arxiv.org/abs/2308.12044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aamakor/continuation-method">https://github.com/aamakor/continuation-method</a></li>
<li>paper_authors: Augustina C. Amakor, Konstantin Sonntag, Sebastian Peitz</li>
</ul>
<p>Abstract:<br>Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto front for the above-mentioned objectives in a very efficient manner. We present numerical examples using both deterministic and stochastic gradients. We furthermore demonstrate that knowledge of the regularization path allows for a well-generalizing network parametrization.</p>
<hr>
<h2 id="IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning"><a href="#IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning" class="headerlink" title="IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning"></a>IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12043">http://arxiv.org/abs/2308.12043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feiyuzhang98/increlora">https://github.com/feiyuzhang98/increlora</a></li>
<li>paper_authors: Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, Yiming Qian</li>
</ul>
<p>Abstract:<br>With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it is not limited by the initial number of training parameters, and each parameter matrix has a higher rank upper bound for the same training overhead. We conduct extensive experiments on GLUE to demonstrate the effectiveness of IncreLoRA. The results show that our method owns higher parameter efficiency, especially when under the low-resource settings where our method significantly outperforms the baselines. Our code is publicly available.</p>
<hr>
<h2 id="PREFER-Prompt-Ensemble-Learning-via-Feedback-Reflect-Refine"><a href="#PREFER-Prompt-Ensemble-Learning-via-Feedback-Reflect-Refine" class="headerlink" title="PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine"></a>PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12033">http://arxiv.org/abs/2308.12033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zcrwind/prefer">https://github.com/zcrwind/prefer</a></li>
<li>paper_authors: Chenrui Zhang, Lin Liu, Jinpeng Wang, Chuyuan Wang, Xiao Sun, Hongyu Wang, Mingchen Cai</li>
</ul>
<p>Abstract:<br>As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Pompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement. Moreover, to enhance stability of the prompt effect evaluation, we propose a novel prompt bagging method involving forward and backward thinking, which is superior to majority voting and is beneficial for both feedback and weight calculation in boosting. Extensive experiments demonstrate that our PREFER achieves state-of-the-art performance in multiple types of tasks by a significant margin. We have made our code publicly available.</p>
<hr>
<h2 id="CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures"><a href="#CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures" class="headerlink" title="CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures"></a>CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12031">http://arxiv.org/abs/2308.12031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Gherardini, Varun Ravi Varma, Karol Capala, Roger Woods, Jose Sousa</li>
</ul>
<p>Abstract:<br>The availability of large data sets is providing an impetus for driving current artificial intelligent developments. There are, however, challenges for developing solutions with small data sets due to practical and cost-effective deployment and the opacity of deep learning models. The Comprehensive Abstraction and Classification Tool for Uncovering Structures called CACTUS is presented for improved secure analytics by effectively employing explainable artificial intelligence. It provides additional support for categorical attributes, preserving their original meaning, optimising memory usage, and speeding up the computation through parallelisation. It shows to the user the frequency of the attributes in each class and ranks them by their discriminative power. Its performance is assessed by application to the Wisconsin diagnostic breast cancer and Thyroid0387 data sets.</p>
<hr>
<h2 id="Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning"><a href="#Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning" class="headerlink" title="Prompt-Based Length Controlled Generation with Reinforcement Learning"></a>Prompt-Based Length Controlled Generation with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12030">http://arxiv.org/abs/2308.12030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, Qun Liu</li>
</ul>
<p>Abstract:<br>Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. We believe this length-controllable ability can provide more potentials towards the era of LLMs.</p>
<hr>
<h2 id="A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning"><a href="#A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning" class="headerlink" title="A Scale-Invariant Task Balancing Approach for Multi-Task Learning"></a>A Scale-Invariant Task Balancing Approach for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12029">http://arxiv.org/abs/2308.12029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baijiong Lin, Weisen Jiang, Feiyang Ye, Yu Zhang, Pengguang Chen, Ying-Cong Chen, Shu Liu</li>
</ul>
<p>Abstract:<br>Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss&#x2F;gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.</p>
<hr>
<h2 id="LKPNR-LLM-and-KG-for-Personalized-News-Recommendation-Framework"><a href="#LKPNR-LLM-and-KG-for-Personalized-News-Recommendation-Framework" class="headerlink" title="LKPNR: LLM and KG for Personalized News Recommendation Framework"></a>LKPNR: LLM and KG for Personalized News Recommendation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12028">http://arxiv.org/abs/2308.12028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuan-zw/lkpnr">https://github.com/xuan-zw/lkpnr</a></li>
<li>paper_authors: Chen hao, Xie Runfeng, Cui Xiangyang, Yan Zhou, Wang Xin, Xuan Zhanwei, Zhang Kai</li>
</ul>
<p>Abstract:<br>Accurately recommending candidate news articles to users is a basic challenge faced by personalized news recommendation systems. Traditional methods are usually difficult to grasp the complex semantic information in news texts, resulting in unsatisfactory recommendation results. Besides, these traditional methods are more friendly to active users with rich historical behaviors. However, they can not effectively solve the “long tail problem” of inactive users. To address these issues, this research presents a novel general framework that combines Large Language Models (LLM) and Knowledge Graphs (KG) into semantic representations of traditional methods. In order to improve semantic understanding in complex news texts, we use LLMs’ powerful text understanding ability to generate news representations containing rich semantic information. In addition, our method combines the information about news entities and mines high-order structural information through multiple hops in KG, thus alleviating the challenge of long tail distribution. Experimental results demonstrate that compared with various traditional models, the framework significantly improves the recommendation effect. The successful integration of LLM and KG in our framework has established a feasible path for achieving more accurate personalized recommendations in the news field. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Xuan-ZW/LKPNR">https://github.com/Xuan-ZW/LKPNR</a>.</p>
<hr>
<h2 id="From-Instructions-to-Intrinsic-Human-Values-–-A-Survey-of-Alignment-Goals-for-Big-Models"><a href="#From-Instructions-to-Intrinsic-Human-Values-–-A-Survey-of-Alignment-Goals-for-Big-Models" class="headerlink" title="From Instructions to Intrinsic Human Values – A Survey of Alignment Goals for Big Models"></a>From Instructions to Intrinsic Human Values – A Survey of Alignment Goals for Big Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12014">http://arxiv.org/abs/2308.12014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, Xing Xie</li>
</ul>
<p>Abstract:<br>Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, &#96;what to align with’ has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced LLMs. Based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.</p>
<hr>
<h2 id="Quantum-Noise-driven-Generative-Diffusion-Models"><a href="#Quantum-Noise-driven-Generative-Diffusion-Models" class="headerlink" title="Quantum-Noise-driven Generative Diffusion Models"></a>Quantum-Noise-driven Generative Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12013">http://arxiv.org/abs/2308.12013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Parigi, Stefano Martina, Filippo Caruso</li>
</ul>
<p>Abstract:<br>Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a very remarkably beneficial key ingredient to generate much more complex probability distributions that would be difficult or even impossible to express classically, and from which a quantum processor might sample more efficiently than a classical one. Therefore, our results are expected to pave the way for new quantum-inspired or quantum-based generative diffusion algorithms addressing more powerfully classical tasks as data generation&#x2F;prediction with widespread real-world applications ranging from climate forecasting to neuroscience, from traffic flow analysis to financial forecasting.</p>
<hr>
<h2 id="Trustworthy-Representation-Learning-Across-Domains"><a href="#Trustworthy-Representation-Learning-Across-Domains" class="headerlink" title="Trustworthy Representation Learning Across Domains"></a>Trustworthy Representation Learning Across Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12315">http://arxiv.org/abs/2308.12315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronghang Zhu, Dongliang Guo, Daiqing Qi, Zhixuan Chu, Xiang Yu, Sheng Li</li>
</ul>
<p>Abstract:<br>As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.</p>
<hr>
<h2 id="Topical-Chat-Towards-Knowledge-Grounded-Open-Domain-Conversations"><a href="#Topical-Chat-Towards-Knowledge-Grounded-Open-Domain-Conversations" class="headerlink" title="Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations"></a>Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11995">http://arxiv.org/abs/2308.11995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexa/Topical-Chat">https://github.com/alexa/Topical-Chat</a></li>
<li>paper_authors: Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-Tur</li>
</ul>
<p>Abstract:<br>Building socialbots that can have deep, engaging open-domain conversations with humans is one of the grand challenges of artificial intelligence (AI). To this end, bots need to be able to leverage world knowledge spanning several domains effectively when conversing with humans who have their own world knowledge. Existing knowledge-grounded conversation datasets are primarily stylized with explicit roles for conversation partners. These datasets also do not explore depth or breadth of topical coverage with transitions in conversations. We introduce Topical-Chat, a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don’t have explicitly defined roles, to help further research in open-domain conversational AI. We also train several state-of-the-art encoder-decoder conversational models on Topical-Chat and perform automated and human evaluation for benchmarking.</p>
<hr>
<h2 id="Critical-Evaluation-of-Artificial-Intelligence-as-Digital-Twin-of-Pathologist-for-Prostate-Cancer-Pathology"><a href="#Critical-Evaluation-of-Artificial-Intelligence-as-Digital-Twin-of-Pathologist-for-Prostate-Cancer-Pathology" class="headerlink" title="Critical Evaluation of Artificial Intelligence as Digital Twin of Pathologist for Prostate Cancer Pathology"></a>Critical Evaluation of Artificial Intelligence as Digital Twin of Pathologist for Prostate Cancer Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11992">http://arxiv.org/abs/2308.11992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Okyaz Eminaga, Mahmoud Abbas, Christian Kunder, Yuri Tolkach, Ryan Han, James D. Brooks, Rosalie Nolley, Axel Semjonow, Martin Boegemann, Robert West, Jin Long, Richard Fan, Olaf Bettendorf</li>
</ul>
<p>Abstract:<br>Prostate cancer pathology plays a crucial role in clinical management but is time-consuming. Artificial intelligence (AI) shows promise in detecting prostate cancer and grading patterns. We tested an AI-based digital twin of a pathologist, vPatho, on 2,603 histology images of prostate tissue stained with hematoxylin and eosin. We analyzed various factors influencing tumor-grade disagreement between vPatho and six human pathologists. Our results demonstrated that vPatho achieved comparable performance in prostate cancer detection and tumor volume estimation, as reported in the literature. Concordance levels between vPatho and human pathologists were examined. Notably, moderate to substantial agreement was observed in identifying complementary histological features such as ductal, cribriform, nerve, blood vessels, and lymph cell infiltrations. However, concordance in tumor grading showed a decline when applied to prostatectomy specimens (kappa &#x3D; 0.44) compared to biopsy cores (kappa &#x3D; 0.70). Adjusting the decision threshold for the secondary Gleason pattern from 5% to 10% improved the concordance level between pathologists and vPatho for tumor grading on prostatectomy specimens (kappa from 0.44 to 0.64). Potential causes of grade discordance included the vertical extent of tumors toward the prostate boundary and the proportions of slides with prostate cancer. Gleason pattern 4 was particularly associated with discordance. Notably, grade discordance with vPatho was not specific to any of the six pathologists involved in routine clinical grading. In conclusion, our study highlights the potential utility of AI in developing a digital twin of a pathologist. This approach can help uncover limitations in AI adoption and the current grading system for prostate cancer pathology.</p>
<hr>
<h2 id="Relational-Concept-Based-Models"><a href="#Relational-Concept-Based-Models" class="headerlink" title="Relational Concept Based Models"></a>Relational Concept Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11991">http://arxiv.org/abs/2308.11991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pietro Barbiero, Francesco Giannini, Gabriele Ciravegna, Michelangelo Diligenti, Giuseppe Marra</li>
</ul>
<p>Abstract:<br>The design of interpretable deep learning models working in relational domains poses an open challenge: interpretable deep learning methods, such as Concept-Based Models (CBMs), are not designed to solve relational problems, while relational models are not as interpretable as CBMs. To address this problem, we propose Relational Concept-Based Models, a family of relational deep learning methods providing interpretable task predictions. Our experiments, ranging from image classification to link prediction in knowledge graphs, show that relational CBMs (i) match generalization performance of existing relational black-boxes (as opposed to non-relational CBMs), (ii) support the generation of quantified concept-based explanations, (iii) effectively respond to test-time interventions, and (iv) withstand demanding settings including out-of-distribution scenarios, limited training data regimes, and scarce concept supervisions.</p>
<hr>
<h2 id="Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks"><a href="#Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks" class="headerlink" title="Will More Expressive Graph Neural Networks do Better on Generative Tasks?"></a>Will More Expressive Graph Neural Networks do Better on Generative Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11978">http://arxiv.org/abs/2308.11978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiandong Zou, Xiangyu Zhao, Pietro Liò, Yiren Zhao</li>
</ul>
<p>Abstract:<br>Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZINC-250k dataset. Through our extensive experiments, we demonstrate that advanced GNNs can indeed improve the performance of GCPN and GraphAF on molecular generation tasks, but GNN expressiveness is not a necessary condition for a good GNN-based generative model. Moreover, we show that GCPN and GraphAF with advanced GNNs can achieve state-of-the-art results across 17 other non-GNN-based graph generative approaches, such as variational autoencoders and Bayesian optimisation models, on the proposed molecular generative objectives (DRD2, Median1, Median2), which are important metrics for de-novo molecular design.</p>
<hr>
<h2 id="Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression"><a href="#Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression" class="headerlink" title="Approximating Score-based Explanation Techniques Using Conformal Regression"></a>Approximating Score-based Explanation Techniques Using Conformal Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11975">http://arxiv.org/abs/2308.11975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Alkhatib, Henrik Boström, Sofiane Ennadir, Ulf Johansson</li>
</ul>
<p>Abstract:<br>Score-based explainable machine-learning techniques are often used to understand the logic behind black-box models. However, such explanation techniques are often computationally expensive, which limits their application in time-critical contexts. Therefore, we propose and investigate the use of computationally less costly regression models for approximating the output of score-based explanation techniques, such as SHAP. Moreover, validity guarantees for the approximated values are provided by the employed inductive conformal prediction framework. We propose several non-conformity measures designed to take the difficulty of approximating the explanations into account while keeping the computational cost low. We present results from a large-scale empirical investigation, in which the approximate explanations generated by our proposed models are evaluated with respect to efficiency (interval size). The results indicate that the proposed method can significantly improve execution time compared to the fast version of SHAP, TreeSHAP. The results also suggest that the proposed method can produce tight intervals, while providing validity guarantees. Moreover, the proposed approach allows for comparing explanations of different approximation methods and selecting a method based on how informative (tight) are the predicted intervals.</p>
<hr>
<h2 id="Blending-NeRF-Text-Driven-Localized-Editing-in-Neural-Radiance-Fields"><a href="#Blending-NeRF-Text-Driven-Localized-Editing-in-Neural-Radiance-Fields" class="headerlink" title="Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields"></a>Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11974">http://arxiv.org/abs/2308.11974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeonseop Song, Seokhun Choi, Hoseok Do, Chul Lee, Taehyeong Kim</li>
</ul>
<p>Abstract:<br>Text-driven localized editing of 3D objects is particularly difficult as locally mixing the original 3D object with the intended new object and style effects without distorting the object’s form is not a straightforward process. To address this issue, we propose a novel NeRF-based model, Blending-NeRF, which consists of two NeRF networks: pretrained NeRF and editable NeRF. Additionally, we introduce new blending operations that allow Blending-NeRF to properly edit target regions which are localized by text. By using a pretrained vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects with varying colors and densities, modify textures, and remove parts of the original object. Our extensive experiments demonstrate that Blending-NeRF produces naturally and locally edited 3D objects from various text prompts.</p>
<hr>
<h2 id="Value-of-Assistance-for-Mobile-Agents"><a href="#Value-of-Assistance-for-Mobile-Agents" class="headerlink" title="Value of Assistance for Mobile Agents"></a>Value of Assistance for Mobile Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11961">http://arxiv.org/abs/2308.11961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clair-lab-technion/voa">https://github.com/clair-lab-technion/voa</a></li>
<li>paper_authors: Adi Amuzig, David Dovrat, Sarah Keren</li>
</ul>
<p>Abstract:<br>Mobile robotic agents often suffer from localization uncertainty which grows with time and with the agents’ movement. This can hinder their ability to accomplish their task. In some settings, it may be possible to perform assistive actions that reduce uncertainty about a robot’s location. For example, in a collaborative multi-robot system, a wheeled robot can request assistance from a drone that can fly to its estimated location and reveal its exact location on the map or accompany it to its intended location. Since assistance may be costly and limited, and may be requested by different members of a team, there is a need for principled ways to support the decision of which assistance to provide to an agent and when, as well as to decide which agent to help within a team. For this purpose, we propose Value of Assistance (VOA) to represent the expected cost reduction that assistance will yield at a given point of execution. We offer ways to compute VOA based on estimations of the robot’s future uncertainty, modeled as a Gaussian process. We specify conditions under which our VOA measures are valid and empirically demonstrate the ability of our measures to predict the agent’s average cost reduction when receiving assistance in both simulated and real-world robotic settings.</p>
<hr>
<h2 id="Physics-informed-Neural-Networks-applied-to-the-description-of-wave-particle-resonance-in-kinetic-simulations-of-fusion-plasmas"><a href="#Physics-informed-Neural-Networks-applied-to-the-description-of-wave-particle-resonance-in-kinetic-simulations-of-fusion-plasmas" class="headerlink" title="Physics informed Neural Networks applied to the description of wave-particle resonance in kinetic simulations of fusion plasmas"></a>Physics informed Neural Networks applied to the description of wave-particle resonance in kinetic simulations of fusion plasmas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12312">http://arxiv.org/abs/2308.12312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jai Kumar, David Zarzoso, Virginie Grandgirard, Jan Ebert, Stefan Kesselheim</li>
</ul>
<p>Abstract:<br>The Vlasov-Poisson system is employed in its reduced form version (1D1V) as a test bed for the applicability of Physics Informed Neural Network (PINN) to the wave-particle resonance. Two examples are explored: the Landau damping and the bump-on-tail instability. PINN is first tested as a compression method for the solution of the Vlasov-Poisson system and compared to the standard neural networks. Second, the application of PINN to solving the Vlasov-Poisson system is also presented with the special emphasis on the integral part, which motivates the implementation of a PINN variant, called Integrable PINN (I-PINN), based on the automatic-differentiation to solve the partial differential equation and on the automatic-integration to solve the integral equation.</p>
<hr>
<h2 id="Maintaining-Plasticity-via-Regenerative-Regularization"><a href="#Maintaining-Plasticity-via-Regenerative-Regularization" class="headerlink" title="Maintaining Plasticity via Regenerative Regularization"></a>Maintaining Plasticity via Regenerative Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11958">http://arxiv.org/abs/2308.11958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabh Kumar, Henrik Marklund, Benjamin Van Roy</li>
</ul>
<p>Abstract:<br>In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates plasticity loss. We additionally find that our regularization term reduces parameter magnitudes and maintains a high effective feature rank.</p>
<hr>
<h2 id="When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation"><a href="#When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation" class="headerlink" title="When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation"></a>When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11953">http://arxiv.org/abs/2308.11953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Huang, Geng Tian, Ming Tang</li>
</ul>
<p>Abstract:<br>Federated learning (FL) enables collaborative model training across distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can be computationally expensive as the clients need to train the entire model multiple times. SplitFed learning (SFL) is a recent distributed approach that alleviates computation workload at the client device by splitting the model at a cut layer into two parts, where clients only need to train part of the model. However, SFL still suffers from the \textit{client drift} problem when clients’ data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This algorithm incorporates MiniBatch SGD into SFL, where the clients train the client-side model in an FL fashion while the server trains the server-side model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and show that the bound of the expected loss can be obtained by analyzing the expected server-side and client-side model updates, respectively. The server-side updates do not depend on the non-IID degree of the clients’ datasets and can potentially mitigate client drift. However, the client-side model relies on the non-IID degree and can be optimized by properly choosing the cut layer. Perhaps counter-intuitive, our empirical result shows that a latter position of the cut layer leads to a smaller average gradient divergence and a better algorithm performance. Moreover, numerical results show that MiniBatch-SFL achieves higher accuracy than conventional SFL and FL. The accuracy improvement can be up to 24.1% and 17.1% with highly non-IID data, respectively.</p>
<hr>
<h2 id="Pose-Modulated-Avatars-from-Video"><a href="#Pose-Modulated-Avatars-from-Video" class="headerlink" title="Pose Modulated Avatars from Video"></a>Pose Modulated Avatars from Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11951">http://arxiv.org/abs/2308.11951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunjin Song, Bastian Wandt, Helge Rhodin</li>
</ul>
<p>Abstract:<br>It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state-of-the-art methods in terms of preserving details and generalization capabilities.</p>
<hr>
<h2 id="High-quality-Image-Dehazing-with-Diffusion-Model"><a href="#High-quality-Image-Dehazing-with-Diffusion-Model" class="headerlink" title="High-quality Image Dehazing with Diffusion Model"></a>High-quality Image Dehazing with Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11949">http://arxiv.org/abs/2308.11949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hu Yu, Jie Huang, Kaiwen Zheng, Man Zhou, Feng Zhao</li>
</ul>
<p>Abstract:<br>Image dehazing is quite challenging in dense-haze scenarios, where quite less original information remains in the hazy image. Though previous methods have made marvelous progress, they still suffer from information loss in content and color in dense-haze scenarios. The recently emerged Denoising Diffusion Probabilistic Model (DDPM) exhibits strong generation ability, showing potential for solving this problem. However, DDPM fails to consider the physics property of dehazing task, limiting its information completion capacity. In this work, we propose DehazeDDPM: A DDPM-based and physics-aware image dehazing framework that applies to complex hazy scenarios. Specifically, DehazeDDPM works in two stages. The former stage physically models the dehazing task with the Atmospheric Scattering Model (ASM), pulling the distribution closer to the clear data and endowing DehazeDDPM with fog-aware ability. The latter stage exploits the strong generation ability of DDPM to compensate for the haze-induced huge information loss, by working in conjunction with the physical modelling. Extensive experiments demonstrate that our method attains state-of-the-art performance on both synthetic and real-world hazy datasets.</p>
<hr>
<h2 id="LongDanceDiff-Long-term-Dance-Generation-with-Conditional-Diffusion-Model"><a href="#LongDanceDiff-Long-term-Dance-Generation-with-Conditional-Diffusion-Model" class="headerlink" title="LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model"></a>LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11945">http://arxiv.org/abs/2308.11945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siqi Yang, Zejun Yang, Zhisheng Wang</li>
</ul>
<p>Abstract:<br>Dancing with music is always an essential human art form to express emotion. Due to the high temporal-spacial complexity, long-term 3D realist dance generation synchronized with music is challenging. Existing methods suffer from the freezing problem when generating long-term dances due to error accumulation and training-inference discrepancy. To address this, we design a conditional diffusion model, LongDanceDiff, for this sequence-to-sequence long-term dance generation, addressing the challenges of temporal coherency and spatial constraint. LongDanceDiff contains a transformer-based diffusion model, where the input is a concatenation of music, past motions, and noised future motions. This partial noising strategy leverages the full-attention mechanism and learns the dependencies among music and past motions. To enhance the diversity of generated dance motions and mitigate the freezing problem, we introduce a mutual information minimization objective that regularizes the dependency between past and future motions. We also address common visual quality issues in dance generation, such as foot sliding and unsmooth motion, by incorporating spatial constraints through a Global-Trajectory Modulation (GTM) layer and motion perceptual losses, thereby improving the smoothness and naturalness of motion generation. Extensive experiments demonstrate a significant improvement in our approach over the existing state-of-the-art methods. We plan to release our codes and models soon.</p>
<hr>
<h2 id="RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching"><a href="#RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching" class="headerlink" title="RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching"></a>RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11943">http://arxiv.org/abs/2308.11943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steve Vott, Adam M. Lehavi</li>
</ul>
<p>Abstract:<br>The Ramsey number is the minimum number of nodes, $n &#x3D; R(s, t)$, such that all undirected simple graphs of order $n$, contain a clique of order $s$, or an independent set of order $t$. This paper explores the application of a best first search algorithm and reinforcement learning (RL) techniques to find counterexamples to specific Ramsey numbers. We incrementally improve over prior search methods such as random search by introducing a graph vectorization and deep neural network (DNN)-based heuristic, which gauge the likelihood of a graph being a counterexample. The paper also proposes algorithmic optimizations to confine a polynomial search runtime. This paper does not aim to present new counterexamples but rather introduces and evaluates a framework supporting Ramsey counterexample exploration using other heuristics. Code and methods are made available through a PyPI package and GitHub repository.</p>
<hr>
<h2 id="Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series"><a href="#Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series" class="headerlink" title="Retail Demand Forecasting: A Comparative Study for Multivariate Time Series"></a>Retail Demand Forecasting: A Comparative Study for Multivariate Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11939">http://arxiv.org/abs/2308.11939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sabbirul Haque, Md Shahedul Amin, Jonayet Miah</li>
</ul>
<p>Abstract:<br>Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.</p>
<hr>
<h2 id="Learning-Bottleneck-Transformer-for-Event-Image-Voxel-Feature-Fusion-based-Classification"><a href="#Learning-Bottleneck-Transformer-for-Event-Image-Voxel-Feature-Fusion-based-Classification" class="headerlink" title="Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification"></a>Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11937">http://arxiv.org/abs/2308.11937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/event-ahu/efv_event_classification">https://github.com/event-ahu/efv_event_classification</a></li>
<li>paper_authors: Chengguo Yuan, Yu Jin, Zongzhen Wu, Fanting Wei, Yangzirui Wang, Lan Chen, Xiao Wang</li>
</ul>
<p>Abstract:<br>Recognizing target objects using an event-based camera draws more and more attention in recent years. Existing works usually represent the event streams into point-cloud, voxel, image, etc, and learn the feature representations using various deep neural networks. Their final results may be limited by the following factors: monotonous modal expressions and the design of the network structure. To address the aforementioned challenges, this paper proposes a novel dual-stream framework for event representation, extraction, and fusion. This framework simultaneously models two common representations: event images and event voxels. By utilizing Transformer and Structured Graph Neural Network (GNN) architectures, spatial information and three-dimensional stereo information can be learned separately. Additionally, a bottleneck Transformer is introduced to facilitate the fusion of the dual-stream information. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance on two widely used event-based classification datasets. The source code of this work is available at: \url{<a target="_blank" rel="noopener" href="https://github.com/Event-AHU/EFV_event_classification%7D">https://github.com/Event-AHU/EFV_event_classification}</a></p>
<hr>
<h2 id="Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe"><a href="#Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe" class="headerlink" title="Diverse Policies Converge in Reward-free Markov Decision Processe"></a>Diverse Policies Converge in Reward-free Markov Decision Processe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11924">http://arxiv.org/abs/2308.11924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openrl-lab/diversepolicies">https://github.com/openrl-lab/diversepolicies</a></li>
<li>paper_authors: Fanqi Lin, Shiyu Huang, Weiwei Tu</li>
</ul>
<p>Abstract:<br>Reinforcement learning has achieved great success in many decision-making tasks, and traditional reinforcement learning algorithms are mainly designed for obtaining a single optimal solution. However, recent works show the importance of developing diverse policies, which makes it an emerging research topic. Despite the variety of diversity reinforcement learning algorithms that have emerged, none of them theoretically answer the question of how the algorithm converges and how efficient the algorithm is. In this paper, we provide a unified diversity reinforcement learning framework and investigate the convergence of training diverse policies. Under such a framework, we also propose a provably efficient diversity reinforcement learning algorithm. Finally, we verify the effectiveness of our method through numerical experiments.</p>
<hr>
<h2 id="Concept-Bottleneck-with-Visual-Concept-Filtering-for-Explainable-Medical-Image-Classification"><a href="#Concept-Bottleneck-with-Visual-Concept-Filtering-for-Explainable-Medical-Image-Classification" class="headerlink" title="Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification"></a>Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11920">http://arxiv.org/abs/2308.11920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Injae Kim, Jongha Kim, Joonmyung Choi, Hyunwoo J. Kim</li>
</ul>
<p>Abstract:<br>Interpretability is a crucial factor in building reliable models for various medical applications. Concept Bottleneck Models (CBMs) enable interpretable image classification by utilizing human-understandable concepts as intermediate targets. Unlike conventional methods that require extensive human labor to construct the concept set, recent works leveraging Large Language Models (LLMs) for generating concepts made automatic concept generation possible. However, those methods do not consider whether a concept is visually relevant or not, which is an important factor in computing meaningful concept scores. Therefore, we propose a visual activation score that measures whether the concept contains visual cues or not, which can be easily computed with unlabeled image data. Computed visual activation scores are then used to filter out the less visible concepts, thus resulting in a final concept set with visually meaningful concepts. Our experimental results show that adopting the proposed visual activation score for concept filtering consistently boosts performance compared to the baseline. Moreover, qualitative analyses also validate that visually relevant concepts are successfully selected with the visual activation score.</p>
<hr>
<h2 id="LFS-GAN-Lifelong-Few-Shot-Image-Generation"><a href="#LFS-GAN-Lifelong-Few-Shot-Image-Generation" class="headerlink" title="LFS-GAN: Lifelong Few-Shot Image Generation"></a>LFS-GAN: Lifelong Few-Shot Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11917">http://arxiv.org/abs/2308.11917</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jjuon/lfs-gan">https://github.com/jjuon/lfs-gan</a></li>
<li>paper_authors: Juwon Seo, Ji-Su Kang, Gyeong-Moon Park</li>
</ul>
<p>Abstract:<br>We address a challenging lifelong few-shot image generation task for the first time. In this situation, a generative model learns a sequence of tasks using only a few samples per task. Consequently, the learned model encounters both catastrophic forgetting and overfitting problems at a time. Existing studies on lifelong GANs have proposed modulation-based methods to prevent catastrophic forgetting. However, they require considerable additional parameters and cannot generate high-fidelity and diverse images from limited data. On the other hand, the existing few-shot GANs suffer from severe catastrophic forgetting when learning multiple tasks. To alleviate these issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can generate high-quality and diverse images in lifelong few-shot image generation task. Our proposed framework learns each task using an efficient task-specific modulator - Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and has a rich representation ability due to its unique reconstruction technique. Furthermore, we propose a novel mode seeking loss to improve the diversity of our model in low-data circumstances. Extensive experiments demonstrate that the proposed LFS-GAN can generate high-fidelity and diverse images without any forgetting and mode collapse in various domains, achieving state-of-the-art in lifelong few-shot image generation task. Surprisingly, we find that our LFS-GAN even outperforms the existing few-shot GANs in the few-shot image generation task. The code is available at Github.</p>
<hr>
<h2 id="Towards-CausalGPT-A-Multi-Agent-Approach-for-Faithful-Knowledge-Reasoning-via-Promoting-Causal-Consistency-in-LLMs"><a href="#Towards-CausalGPT-A-Multi-Agent-Approach-for-Faithful-Knowledge-Reasoning-via-Promoting-Causal-Consistency-in-LLMs" class="headerlink" title="Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs"></a>Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11914">http://arxiv.org/abs/2308.11914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Tang, Ruilin Wang, Weixing Chen, Keze Wang, Yang Liu, Tianshui Chen, Liang Lin</li>
</ul>
<p>Abstract:<br>Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference. Existing methods primarily encourage LLMs to autonomously plan and solve problems or to extensively sample reasoning chains without addressing the conceptual and inferential fallacies. Attempting to alleviate inferential fallacies and drawing inspiration from multi-agent collaboration, we present a framework to increase faithfulness and causality for knowledge-based reasoning. Specifically, we propose to employ multiple intelligent agents (i.e., reasoner and causal evaluator) to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness. The reasoners focus on providing solutions with human-like causality to solve open-domain problems. On the other hand, the causal evaluator agent scrutinizes if the answer in a solution is causally deducible from the question and vice versa, with a counterfactual answer replacing the original. According to the extensive and comprehensive evaluations on a variety of knowledge reasoning tasks (e.g., science question answering and commonsense reasoning), our framework outperforms all compared state-of-the-art approaches by large margins.</p>
<hr>
<h2 id="Utilizing-Admissible-Bounds-for-Heuristic-Learning"><a href="#Utilizing-Admissible-Bounds-for-Heuristic-Learning" class="headerlink" title="Utilizing Admissible Bounds for Heuristic Learning"></a>Utilizing Admissible Bounds for Heuristic Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11905">http://arxiv.org/abs/2308.11905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Núñez-Molina, Masataro Asai</li>
</ul>
<p>Abstract:<br>While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and empirically show that, as a result, it yields more accurate heuristics and converges faster during training.</p>
<hr>
<h2 id="Exploring-the-Optimization-Objective-of-One-Class-Classification-for-Anomaly-Detection"><a href="#Exploring-the-Optimization-Objective-of-One-Class-Classification-for-Anomaly-Detection" class="headerlink" title="Exploring the Optimization Objective of One-Class Classification for Anomaly Detection"></a>Exploring the Optimization Objective of One-Class Classification for Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11898">http://arxiv.org/abs/2308.11898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Gao, Huiyuan Luo, Fei Shen, Zhengtao Zhang</li>
</ul>
<p>Abstract:<br>One-class classification (OCC) is a longstanding method for anomaly detection. With the powerful representation capability of the pre-trained backbone, OCC methods have witnessed significant performance improvements. Typically, most of these OCC methods employ transfer learning to enhance the discriminative nature of the pre-trained backbone’s features, thus achieving remarkable efficacy. While most current approaches emphasize feature transfer strategies, we argue that the optimization objective space within OCC methods could also be an underlying critical factor influencing performance. In this work, we conducted a thorough investigation into the optimization objective of OCC. Through rigorous theoretical analysis and derivation, we unveil a key insights: any space with the suitable norm can serve as an equivalent substitute for the hypersphere center, without relying on the distribution assumption of training samples. Further, we provide guidelines for determining the feasible domain of norms for the OCC optimization objective. This novel insight sparks a simple and data-agnostic deep one-class classification method. Our method is straightforward, with a single 1x1 convolutional layer as a trainable projector and any space with suitable norm as the optimization objective. Extensive experiments validate the reliability and efficacy of our findings and the corresponding methodology, resulting in state-of-the-art performance in both one-class classification and industrial vision anomaly detection and segmentation tasks.</p>
<hr>
<h2 id="Bridging-the-Gap-Deciphering-Tabular-Data-Using-Large-Language-Model"><a href="#Bridging-the-Gap-Deciphering-Tabular-Data-Using-Large-Language-Model" class="headerlink" title="Bridging the Gap: Deciphering Tabular Data Using Large Language Model"></a>Bridging the Gap: Deciphering Tabular Data Using Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11891">http://arxiv.org/abs/2308.11891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengyuan Zhang, Peng Chang, Zongcheng Ji</li>
</ul>
<p>Abstract:<br>In the realm of natural language processing, the understanding of tabular data has perpetually stood as a focal point of scholarly inquiry. The emergence of expansive language models, exemplified by the likes of ChatGPT, has ushered in a wave of endeavors wherein researchers aim to harness these models for tasks related to table-based question answering. Central to our investigative pursuits is the elucidation of methodologies that amplify the aptitude of such large language models in discerning both the structural intricacies and inherent content of tables, ultimately facilitating their capacity to provide informed responses to pertinent queries. To this end, we have architected a distinctive module dedicated to the serialization of tables for seamless integration with expansive language models. Additionally, we’ve instituted a corrective mechanism within the model to rectify potential inaccuracies. Experimental results indicate that, although our proposed method trails the SOTA by approximately 11.7% in overall metrics, it surpasses the SOTA by about 1.2% in tests on specific datasets. This research marks the first application of large language models to table-based question answering tasks, enhancing the model’s comprehension of both table structures and content.</p>
<hr>
<h2 id="Integrating-the-Wikidata-Taxonomy-into-YAGO"><a href="#Integrating-the-Wikidata-Taxonomy-into-YAGO" class="headerlink" title="Integrating the Wikidata Taxonomy into YAGO"></a>Integrating the Wikidata Taxonomy into YAGO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11884">http://arxiv.org/abs/2308.11884</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yago-naga/yago-4.5">https://github.com/yago-naga/yago-4.5</a></li>
<li>paper_authors: Fabian Suchanek, Mehwish Alam, Thomas Bonald, Pierre-Henri Paris, Jules Soria</li>
</ul>
<p>Abstract:<br>Wikidata is one of the largest public general-purpose Knowledge Bases (KBs). Yet, due to its collaborative nature, its schema and taxonomy have become convoluted. For the YAGO 4 KB, we combined Wikidata with the ontology from Schema.org, which reduced and cleaned up the taxonomy and constraints and made it possible to run automated reasoners on the data. However, it also cut away large parts of the Wikidata taxonomy. In this paper, we present our effort to merge the entire Wikidata taxonomy into the YAGO KB as much as possible. We pay particular attention to logical constraints and a careful distinction of classes and instances. Our work creates YAGO 4.5, which adds a rich layer of informative classes to YAGO, while at the same time keeping the KB logically consistent.</p>
<hr>
<h2 id="Cabrita-closing-the-gap-for-foreign-languages"><a href="#Cabrita-closing-the-gap-for-foreign-languages" class="headerlink" title="Cabrita: closing the gap for foreign languages"></a>Cabrita: closing the gap for foreign languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11878">http://arxiv.org/abs/2308.11878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celio Larcher, Marcos Piau, Paulo Finardi, Pedro Gengo, Piero Esposito, Vinicius Caridá</li>
</ul>
<p>Abstract:<br>The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.   The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.   To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We believe that this methodology can be applied to any transformer-like architecture model. To validate the study, we conducted continuous pre-training exclusively using Portuguese text on a 3-billion-parameter model known as OpenLLaMA, resulting in a model named openCabrita 3B. The openCabrita 3B also features a new tokenizer that results in a significant reduction in the number of tokens required to represent the text. In our assessment, for few-shot learning tasks, we achieved similar results with this 3B model compared to a traditional continuous pre-training approach as well as to 7B models English pre-trained models.</p>
<hr>
<h2 id="Integrated-Image-and-Location-Analysis-for-Wound-Classification-A-Deep-Learning-Approach"><a href="#Integrated-Image-and-Location-Analysis-for-Wound-Classification-A-Deep-Learning-Approach" class="headerlink" title="Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach"></a>Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11877">http://arxiv.org/abs/2308.11877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Patel, Tirth Shah, Mrinal Kanti Dhar, Taiyu Zhang, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Zeyun Yu</li>
</ul>
<p>Abstract:<br>The global burden of acute and chronic wounds presents a compelling case for enhancing wound classification methods, a vital step in diagnosing and determining optimal treatments. Recognizing this need, we introduce an innovative multi-modal network based on a deep convolutional neural network for categorizing wounds into four categories: diabetic, pressure, surgical, and venous ulcers. Our multi-modal network uses wound images and their corresponding body locations for more precise classification. A unique aspect of our methodology is incorporating a body map system that facilitates accurate wound location tagging, improving upon traditional wound image classification techniques. A distinctive feature of our approach is the integration of models such as VGG16, ResNet152, and EfficientNet within a novel architecture. This architecture includes elements like spatial and channel-wise Squeeze-and-Excitation modules, Axial Attention, and an Adaptive Gated Multi-Layer Perceptron, providing a robust foundation for classification. Our multi-modal network was trained and evaluated on two distinct datasets comprising relevant images and corresponding location information. Notably, our proposed network outperformed traditional methods, reaching an accuracy range of 74.79% to 100% for Region of Interest (ROI) without location classifications, 73.98% to 100% for ROI with location classifications, and 78.10% to 100% for whole image classifications. This marks a significant enhancement over previously reported performance metrics in the literature. Our results indicate the potential of our multi-modal network as an effective decision-support tool for wound image classification, paving the way for its application in various clinical contexts.</p>
<hr>
<h2 id="Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0"><a href="#Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0" class="headerlink" title="Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0"></a>Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11854">http://arxiv.org/abs/2308.11854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anmol Chaure, Ashok Kumar Behera, Sudip Bhattacharya</li>
</ul>
<p>Abstract:<br>Climate projections using data driven machine learning models acting as emulators, is one of the prevailing areas of research to enable policy makers make informed decisions. Use of machine learning emulators as surrogates for computationally heavy GCM simulators reduces time and carbon footprints. In this direction, ClimateBench [1] is a recently curated benchmarking dataset for evaluating the performance of machine learning emulators designed for climate data. Recent studies have reported that despite being considered fundamental, regression models offer several advantages pertaining to climate emulations. In particular, by leveraging the kernel trick, regression models can capture complex relationships and improve their predictive capabilities. This study focuses on evaluating non-linear regression models using the aforementioned dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the best-in-class performance against standard evaluation metrics used for climate field emulation studies. However, Gaussian Process Regression suffers from being computational resource hungry in terms of space and time complexity. Alternatively, Support Vector and Kernel Ridge models also deliver competitive results and but there are certain trade-offs to be addressed. Additionally, we are actively investigating the performance of composite kernels and techniques such as variational inference to further enhance the performance of the regression models and effectively model complex non-linear patterns, including phenomena like precipitation.</p>
<hr>
<h2 id="A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data"><a href="#A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data" class="headerlink" title="A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data"></a>A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11849">http://arxiv.org/abs/2308.11849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Liu, Zhiyuan Lin, Judith Y. T. Wang, Hong Chen</li>
</ul>
<p>Abstract:<br>Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accumulation of overcrowded passengers at this station, to prevent additional accidents arising from overcrowding. This research addresses the challenges associated with this scenario, including the dynamics of arriving and leaving of passengers, station overcrowding, rolling stock shortage, open-ended disruption duration, integrated rescheduling on multiple routes, and delays due to detours. A deep reinforcement learning (DRL) framework is proposed to determine the optimal rescheduled timetable, route stops, and rolling stock allocation, while considering real-time demand satisfaction, station overcrowding, train capacity utilization, and headway safety.</p>
<hr>
<h2 id="rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning"></a>${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11842">http://arxiv.org/abs/2308.11842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dchen48/e3ac">https://github.com/dchen48/e3ac</a></li>
<li>paper_authors: Dingyang Chen, Qi Zhang</li>
</ul>
<p>Abstract:<br>Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/dchen48/E3AC">https://github.com/dchen48/E3AC</a>.</p>
<hr>
<h2 id="A-Benchmark-Study-on-Calibration"><a href="#A-Benchmark-Study-on-Calibration" class="headerlink" title="A Benchmark Study on Calibration"></a>A Benchmark Study on Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11838">http://arxiv.org/abs/2308.11838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, Chang Xu</li>
</ul>
<p>Abstract:<br>Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can robustness be used as a calibration measurement? (iii) How reliable are calibration metrics? (iv) Does a post-hoc calibration method affect all models uniformly? (v) How does calibration interact with accuracy? (vi) What is the impact of bin size on calibration measurement? (vii) Which architectural designs are beneficial for calibration? Additionally, our study bridges an existing gap by exploring calibration within NAS. By providing this dataset, we enable further research into NAS calibration. As far as we are aware, our research represents the first large-scale investigation into calibration properties and the premier study of calibration issues within NAS.</p>
<hr>
<h2 id="Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity"><a href="#Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity" class="headerlink" title="Characterizing normal perinatal development of the human brain structural connectivity"></a>Characterizing normal perinatal development of the human brain structural connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11836">http://arxiv.org/abs/2308.11836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Wu, Lana Vasung, Camilo Calixto, Ali Gholipour, Davood Karimi</li>
</ul>
<p>Abstract:<br>Early brain development is characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain’s cognitive abilities and influences its response to diseases and environmental factors. Hence, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determining such baselines. We used this framework to analyze the structural connectivity between 33 and 44 postmenstrual weeks using data from 166 subjects. Our results unveiled clear and strong trends in the development of structural connectivity in perinatal stage. Connection weighting based on fractional anisotropy and neurite density produced the most consistent results. We observed increases in global and local efficiency, a decrease in characteristic path length, and widespread strengthening of the connections within and across brain lobes and hemispheres. We also observed asymmetry patterns that were consistent between different connection weighting approaches. The new computational method and results are useful for assessing normal and abnormal development of the structural connectome early in life.</p>
<hr>
<h2 id="Algorithm-assisted-discovery-of-an-intrinsic-order-among-mathematical-constants"><a href="#Algorithm-assisted-discovery-of-an-intrinsic-order-among-mathematical-constants" class="headerlink" title="Algorithm-assisted discovery of an intrinsic order among mathematical constants"></a>Algorithm-assisted discovery of an intrinsic order among mathematical constants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11829">http://arxiv.org/abs/2308.11829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rotem Elimelech, Ofir David, Carlos De la Cruz Mengual, Rotem Kalisch, Wolfgang Berndt, Michael Shalyt, Mark Silberstein, Yaron Hadad, Ido Kaminer</li>
</ul>
<p>Abstract:<br>In recent decades, a growing number of discoveries in fields of mathematics have been assisted by computer algorithms, primarily for exploring large parameter spaces that humans would take too long to investigate. As computers and algorithms become more powerful, an intriguing possibility arises - the interplay between human intuition and computer algorithms can lead to discoveries of novel mathematical concepts that would otherwise remain elusive. To realize this perspective, we have developed a massively parallel computer algorithm that discovers an unprecedented number of continued fraction formulas for fundamental mathematical constants. The sheer number of formulas discovered by the algorithm unveils a novel mathematical structure that we call the conservative matrix field. Such matrix fields (1) unify thousands of existing formulas, (2) generate infinitely many new formulas, and most importantly, (3) lead to unexpected relations between different mathematical constants, including multiple integer values of the Riemann zeta function. Conservative matrix fields also enable new mathematical proofs of irrationality. In particular, we can use them to generalize the celebrated proof by Ap&#39;ery for the irrationality of $\zeta(3)$. Utilizing thousands of personal computers worldwide, our computer-supported research strategy demonstrates the power of experimental mathematics, highlighting the prospects of large-scale computational approaches to tackle longstanding open problems and discover unexpected connections across diverse fields of science.</p>
<hr>
<h2 id="Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test"><a href="#Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test" class="headerlink" title="Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test"></a>Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11827">http://arxiv.org/abs/2308.11827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saba Rahimi, Tucker Balch, Manuela Veloso</li>
</ul>
<p>Abstract:<br>Large language models such as Open AI’s Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver’s Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model’s passing score fell to 82%. However, the model still fails to answer some questions correctly even with providing library of context, highlighting room for improvement. The research also examined the impact of prompt length and context format, on the model’s performance. Overall, the study provides insights into the limitations and potential improvements for GPT models in question-answering tasks.</p>
<hr>
<h2 id="Expressive-probabilistic-sampling-in-recurrent-neural-networks"><a href="#Expressive-probabilistic-sampling-in-recurrent-neural-networks" class="headerlink" title="Expressive probabilistic sampling in recurrent neural networks"></a>Expressive probabilistic sampling in recurrent neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11809">http://arxiv.org/abs/2308.11809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shirui Chen, Linxin Preston Jiang, Rajesh P. N. Rao, Eric Shea-Brown</li>
</ul>
<p>Abstract:<br>In sampling-based Bayesian models of brain function, neural activities are assumed to be samples from probability distributions that the brain uses for probabilistic computation. However, a comprehensive understanding of how mechanistic models of neural dynamics can sample from arbitrary distributions is still lacking. We use tools from functional analysis and stochastic differential equations to explore the minimum architectural requirements for $\textit{recurrent}$ neural circuits to sample from complex distributions. We first consider the traditional sampling model consisting of a network of neurons whose outputs directly represent the samples (sampler-only network). We argue that synaptic current and firing-rate dynamics in the traditional model have limited capacity to sample from a complex probability distribution. We show that the firing rate dynamics of a recurrent neural circuit with a separate set of output units can sample from an arbitrary probability distribution. We call such circuits reservoir-sampler networks (RSNs). We propose an efficient training procedure based on denoising score matching that finds recurrent and output weights such that the RSN implements Langevin sampling. We empirically demonstrate our model’s ability to sample from several complex data distributions using the proposed neural dynamics and discuss its applicability to developing the next generation of sampling-based brain models.</p>
<hr>
<h2 id="Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings"><a href="#Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings" class="headerlink" title="Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings"></a>Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11804">http://arxiv.org/abs/2308.11804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugene Bagdasaryan, Vitaly Shmatikov</li>
</ul>
<p>Abstract:<br>Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call “adversarial illusions.” Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.   Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.</p>
<hr>
<h2 id="WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters"><a href="#WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters" class="headerlink" title="WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters"></a>WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11776">http://arxiv.org/abs/2308.11776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ange Lou, Jack Noble</li>
</ul>
<p>Abstract:<br>Depth estimation in surgical video plays a crucial role in many image-guided surgery procedures. However, it is difficult and time consuming to create depth map ground truth datasets in surgical videos due in part to inconsistent brightness and noise in the surgical scene. Therefore, building an accurate and robust self-supervised depth and camera ego-motion estimation system is gaining more attention from the computer vision community. Although several self-supervision methods alleviate the need for ground truth depth maps and poses, they still need known camera intrinsic parameters, which are often missing or not recorded. Moreover, the camera intrinsic prediction methods in existing works depend heavily on the quality of datasets. In this work, we aimed to build a self-supervised depth and ego-motion estimation system which can predict not only accurate depth maps and camera pose, but also camera intrinsic parameters. We proposed a cost-volume-based supervision manner to give the system auxiliary supervision for camera parameters prediction. The experimental results showed that the proposed method improved the accuracy of estimated camera parameters, ego-motion, and depth estimation.</p>
<hr>
<h2 id="3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network"><a href="#3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network" class="headerlink" title="3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network"></a>3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11771">http://arxiv.org/abs/2308.11771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinyu Chen, Zuowen Wang, Shih-Chii Liu, Chang Gao</li>
</ul>
<p>Abstract:<br>This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR&#x2F;VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\times$ without losing accuracy when tested on a \texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \url{<a target="_blank" rel="noopener" href="https://github.com/qinche106/cb-convlstm-eyetracking%7D">https://github.com/qinche106/cb-convlstm-eyetracking}</a>.</p>
<hr>
<h2 id="Halo-Estimation-and-Reduction-of-Hallucinations-in-Open-Source-Weak-Large-Language-Models"><a href="#Halo-Estimation-and-Reduction-of-Hallucinations-in-Open-Source-Weak-Large-Language-Models" class="headerlink" title="Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models"></a>Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11764">http://arxiv.org/abs/2308.11764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/engsalem/halo">https://github.com/engsalem/halo</a></li>
<li>paper_authors: Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu</li>
</ul>
<p>Abstract:<br>Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.</p>
<hr>
<h2 id="VBMO-Voting-Based-Multi-Objective-Path-Planning"><a href="#VBMO-Voting-Based-Multi-Objective-Path-Planning" class="headerlink" title="VBMO: Voting-Based Multi-Objective Path Planning"></a>VBMO: Voting-Based Multi-Objective Path Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11755">http://arxiv.org/abs/2308.11755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raj Korpan</li>
</ul>
<p>Abstract:<br>This paper presents VBMO, the Voting-Based Multi-Objective path planning algorithm, that generates optimal single-objective plans, evaluates each of them with respect to the other objectives, and selects one with a voting mechanism. VBMO does not use hand-tuned weights, consider the multiple objectives at every step of search, or use an evolutionary algorithm. Instead, it considers how a plan that is optimal in one objective may perform well with respect to others. VBMO incorporates three voting mechanisms: range, Borda, and combined approval. Extensive evaluation in diverse and complex environments demonstrates the algorithm’s ability to efficiently produce plans that satisfy multiple objectives.</p>
<hr>
<h2 id="Multi-Instance-Adversarial-Attack-on-GNN-Based-Malicious-Domain-Detection"><a href="#Multi-Instance-Adversarial-Attack-on-GNN-Based-Malicious-Domain-Detection" class="headerlink" title="Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection"></a>Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11754">http://arxiv.org/abs/2308.11754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Nazzal, Issa Khalil, Abdallah Khreishah, NhatHai Phan, Yao Ma</li>
</ul>
<p>Abstract:<br>Malicious domain detection (MDD) is an open security challenge that aims to detect if an Internet domain is associated with cyber-attacks. Among many approaches to this problem, graph neural networks (GNNs) are deemed highly effective. GNN-based MDD uses DNS logs to represent Internet domains as nodes in a maliciousness graph (DMG) and trains a GNN to infer their maliciousness by leveraging identified malicious domains. Since this method relies on accessible DNS logs to construct DMGs, it exposes a vulnerability for adversaries to manipulate their domain nodes’ features and connections within DMGs. Existing research mainly concentrates on threat models that manipulate individual attacker nodes. However, adversaries commonly generate multiple domains to achieve their goals economically and avoid detection. Their objective is to evade discovery across as many domains as feasible. In this work, we call the attack that manipulates several nodes in the DMG concurrently a multi-instance evasion attack. We present theoretical and empirical evidence that the existing single-instance evasion techniques for are inadequate to launch multi-instance evasion attacks against GNN-based MDDs. Therefore, we introduce MintA, an inference-time multi-instance adversarial attack on GNN-based MDDs. MintA enhances node and neighborhood evasiveness through optimized perturbations and operates successfully with only black-box access to the target model, eliminating the need for knowledge about the model’s specifics or non-adversary nodes. We formulate an optimization challenge for MintA, achieving an approximate solution. Evaluating MintA on a leading GNN-based MDD technique with real-world data showcases an attack success rate exceeding 80%. These findings act as a warning for security experts, underscoring GNN-based MDDs’ susceptibility to practical attacks that can undermine their effectiveness and benefits.</p>
<hr>
<h2 id="Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data"><a href="#Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data" class="headerlink" title="Patient Clustering via Integrated Profiling of Clinical and Digital Data"></a>Patient Clustering via Integrated Profiling of Clinical and Digital Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11748">http://arxiv.org/abs/2308.11748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjin Choi, Andy Xiang, Ozgur Ozturk, Deep Shrestha, Barry Drake, Hamid Haidarian, Faizan Javed, Haesun Park</li>
</ul>
<p>Abstract:<br>We introduce a novel profile-based patient clustering model designed for clinical data in healthcare. By utilizing a method grounded on constrained low-rank approximation, our model takes advantage of patients’ clinical data and digital interaction data, including browsing and search, to construct patient profiles. As a result of the method, nonnegative embedding vectors are generated, serving as a low-dimensional representation of the patients. Our model was assessed using real-world patient data from a healthcare web portal, with a comprehensive evaluation approach which considered clustering and recommendation capabilities. In comparison to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.</p>
<hr>
<h2 id="Lifted-Inference-beyond-First-Order-Logic"><a href="#Lifted-Inference-beyond-First-Order-Logic" class="headerlink" title="Lifted Inference beyond First-Order Logic"></a>Lifted Inference beyond First-Order Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11738">http://arxiv.org/abs/2308.11738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagar Malhotra, Davide Bizzaro, Luciano Serafini</li>
</ul>
<p>Abstract:<br>Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic inference in statistical relational learning models. As WFOMC is known to be intractable in general ($#$P-complete), logical fragments that admit polynomial time WFOMC are of significant interest. Such fragments are called domain liftable. Recent works have shown that the two-variable fragment of first order logic extended with counting quantifiers ($\mathrm{C^2}$) is domain-liftable. However, many properties of real-world data, like acyclicity in citation networks and connectivity in social networks, cannot be modeled in $\mathrm{C^2}$, or first order logic in general. In this work, we expand the domain liftability of $\mathrm{C^2}$ with multiple such properties. We show that any $\mathrm{C^2}$ sentence remains domain liftable when one of its relations is restricted to represent a directed acyclic graph, a connected graph, a tree (resp. a directed tree) or a forest (resp. a directed forest). All our results rely on a novel and general methodology of “counting by splitting”. Besides their application to probabilistic inference, our results provide a general framework for counting combinatorial structures. We expand a vast array of previous results in discrete mathematics literature on directed acyclic graphs, phylogenetic networks, etc.</p>
<hr>
<h2 id="Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering"><a href="#Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering" class="headerlink" title="Knowledge Graph Prompting for Multi-Document Question Answering"></a>Knowledge Graph Prompting for Multi-Document Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11730">http://arxiv.org/abs/2308.11730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa Siu, Ruiyi Zhang, Tyler Derr</li>
</ul>
<p>Abstract:<br>The ‘pre-train, prompt, predict’ paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages&#x2F;tables), and edges denoting the semantic&#x2F;lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the LM-guided traverser acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code is at <a target="_blank" rel="noopener" href="https://github.com/YuWVandy/KG-LLM-MDQA">https://github.com/YuWVandy/KG-LLM-MDQA</a>.</p>
<hr>
<h2 id="Efficient-Benchmarking-of-Language-Models"><a href="#Efficient-Benchmarking-of-Language-Models" class="headerlink" title="Efficient Benchmarking (of Language Models)"></a>Efficient Benchmarking (of Language Models)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11696">http://arxiv.org/abs/2308.11696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sumankrsh/Sentiment-Analysis.ipynb">https://github.com/sumankrsh/Sentiment-Analysis.ipynb</a></li>
<li>paper_authors: Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen</li>
</ul>
<p>Abstract:<br>The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark ranking. Conversely a slightly different choice of HELM scenarios varies ranking widely. Based on our findings we outline a set of concrete recommendations for more efficient benchmark design and utilization practices leading to dramatic cost savings with minimal loss of benchmark reliability often reducing computation by x100 or more.</p>
<hr>
<h2 id="Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models"><a href="#Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models" class="headerlink" title="Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models"></a>Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11601">http://arxiv.org/abs/2308.11601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surya Narayanan Hari, Matt Thomson</li>
</ul>
<p>Abstract:<br>The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.</p>
<hr>
<h2 id="Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge"><a href="#Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge" class="headerlink" title="Practical Insights on Incremental Learning of New Human Physical Activity on the Edge"></a>Practical Insights on Incremental Learning of New Human Physical Activity on the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11691">http://arxiv.org/abs/2308.11691</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Arvanitakis, Jingwei Zuo, Mthandazo Ndhlovu, Hakim Hacid</li>
</ul>
<p>Abstract:<br>Edge Machine Learning (Edge ML), which shifts computational intelligence from cloud-based systems to edge devices, is attracting significant interest due to its evident benefits including reduced latency, enhanced data privacy, and decreased connectivity reliance. While these advantages are compelling, they introduce unique challenges absent in traditional cloud-based approaches. In this paper, we delve into the intricacies of Edge-based learning, examining the interdependencies among: (i) constrained data storage on Edge devices, (ii) limited computational power for training, and (iii) the number of learning classes. Through experiments conducted using our MAGNETO system, that focused on learning human activities via data collected from mobile sensors, we highlight these challenges and offer valuable perspectives on Edge ML.</p>
<hr>
<h2 id="Handling-the-inconsistency-of-systems-of-min-rightarrow-fuzzy-relational-equations"><a href="#Handling-the-inconsistency-of-systems-of-min-rightarrow-fuzzy-relational-equations" class="headerlink" title="Handling the inconsistency of systems of $\min\rightarrow$ fuzzy relational equations"></a>Handling the inconsistency of systems of $\min\rightarrow$ fuzzy relational equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12385">http://arxiv.org/abs/2308.12385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ismaïl Baaj</li>
</ul>
<p>Abstract:<br>In this article, we study the inconsistency of systems of $\min-\rightarrow$ fuzzy relational equations. We give analytical formulas for computing the Chebyshev distances $\nabla &#x3D; \inf_{d \in \mathcal{D}} \Vert \beta - d \Vert$ associated to systems of $\min-\rightarrow$ fuzzy relational equations of the form $\Gamma \Box_{\rightarrow}^{\min} x &#x3D; \beta$, where $\rightarrow$ is a residual implicator among the G&quot;odel implication $\rightarrow_G$, the Goguen implication $\rightarrow_{GG}$ or Lukasiewicz’s implication $\rightarrow_L$ and $\mathcal{D}$ is the set of second members of consistent systems defined with the same matrix $\Gamma$. The main preliminary result that allows us to obtain these formulas is that the Chebyshev distance $\nabla$ is the lower bound of the solutions of a vector inequality, whatever the residual implicator used. Finally, we show that, in the case of the $\min-\rightarrow_{G}$ system, the Chebyshev distance $\nabla$ may be an infimum, while it is always a minimum for $\min-\rightarrow_{GG}$ and $\min-\rightarrow_{L}$ systems.</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/23/cs.AI_2023_08_23/" data-id="cllqyyxa80006bpr83kc5g2nl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.CV_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.CV_2023_08_23/">cs.CV - 2023-08-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CIParsing-Unifying-Causality-Properties-into-Multiple-Human-Parsing"><a href="#CIParsing-Unifying-Causality-Properties-into-Multiple-Human-Parsing" class="headerlink" title="CIParsing: Unifying Causality Properties into Multiple Human Parsing"></a>CIParsing: Unifying Causality Properties into Multiple Human Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12218">http://arxiv.org/abs/2308.12218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaojia Chen, Xuanhan Wang, Lianli Gao, Beitao Chen, Jingkuan Song, HenTao Shen</li>
</ul>
<p>Abstract:<br>Existing methods of multiple human parsing (MHP) apply statistical models to acquire underlying associations between images and labeled body parts. However, acquired associations often contain many spurious correlations that degrade model generalization, leading statistical models to be vulnerable to visually contextual variations in images (e.g., unseen image styles&#x2F;external interventions). To tackle this, we present a causality inspired parsing paradigm termed CIParsing, which follows fundamental causal principles involving two causal properties for human parsing (i.e., the causal diversity and the causal invariance). Specifically, we assume that an input image is constructed by a mix of causal factors (the characteristics of body parts) and non-causal factors (external contexts), where only the former ones cause the generation process of human parsing.Since causal&#x2F;non-causal factors are unobservable, a human parser in proposed CIParsing is required to construct latent representations of causal factors and learns to enforce representations to satisfy the causal properties. In this way, the human parser is able to rely on causal factors w.r.t relevant evidence rather than non-causal factors w.r.t spurious correlations, thus alleviating model degradation and yielding improved parsing ability. Notably, the CIParsing is designed in a plug-and-play fashion and can be integrated into any existing MHP models. Extensive experiments conducted on two widely used benchmarks demonstrate the effectiveness and generalizability of our method.</p>
<hr>
<h2 id="SG-Former-Self-guided-Transformer-with-Evolving-Token-Reallocation"><a href="#SG-Former-Self-guided-Transformer-with-Evolving-Token-Reallocation" class="headerlink" title="SG-Former: Self-guided Transformer with Evolving Token Reallocation"></a>SG-Former: Self-guided Transformer with Evolving Token Reallocation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12216">http://arxiv.org/abs/2308.12216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sucheng Ren, Xingyi Yang, Songhua Liu, Xinchao Wang</li>
</ul>
<p>Abstract:<br>Vision Transformer has demonstrated impressive success across various vision tasks. However, its heavy computation cost, which grows quadratically with respect to the token sequence length, largely limits its power in handling large feature maps. To alleviate the computation cost, previous works rely on either fine-grained self-attentions restricted to local small regions, or global self-attentions but to shorten the sequence length resulting in coarse granularity. In this paper, we propose a novel model, termed as Self-guided Transformer~(SG-Former), towards effective global self-attention with adaptive fine granularity. At the heart of our approach is to utilize a significance map, which is estimated through hybrid-scale self-attention and evolves itself during training, to reallocate tokens based on the significance of each region. Intuitively, we assign more tokens to the salient regions for achieving fine-grained attention, while allocating fewer tokens to the minor regions in exchange for efficiency and global receptive fields. The proposed SG-Former achieves performance superior to state of the art: our base size model achieves \textbf{84.7%} Top-1 accuracy on ImageNet-1K, \textbf{51.2mAP} bbAP on CoCo, \textbf{52.7mIoU} on ADE20K surpassing the Swin Transformer by \textbf{+1.3% &#x2F; +2.7 mAP&#x2F; +3 mIoU}, with lower computation costs and fewer parameters. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/OliverRensu/SG-Former%7D%7Bhttps://github.com/OliverRensu/SG-Former%7D">https://github.com/OliverRensu/SG-Former}{https://github.com/OliverRensu/SG-Former}</a></p>
<hr>
<h2 id="CLIPN-for-Zero-Shot-OOD-Detection-Teaching-CLIP-to-Say-No"><a href="#CLIPN-for-Zero-Shot-OOD-Detection-Teaching-CLIP-to-Say-No" class="headerlink" title="CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No"></a>CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12213">http://arxiv.org/abs/2308.12213</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/clipn">https://github.com/xmed-lab/clipn</a></li>
<li>paper_authors: Hualiang Wang, Yi Li, Huifeng Yao, Xiaomeng Li</li>
</ul>
<p>Abstract:<br>Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes. Considerable effort has been invested in designing various OOD detection methods based on either convolutional neural networks or transformers. However, zero-shot OOD detection methods driven by CLIP, which only require class names for ID, have received less attention. This paper presents a novel method, namely CLIP saying no (CLIPN), which empowers the logic of saying no within CLIP. Our key motivation is to equip CLIP with the capability of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts. Specifically, we design a novel learnable no prompt and a no text encoder to capture negation semantics within images. Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to associate images with no prompts, thereby enabling it to identify unknown samples. Furthermore, we propose two threshold-free inference algorithms to perform OOD detection by utilizing negation semantics from no prompts and the text encoder. Experimental results on 9 benchmark datasets (3 ID datasets and 6 OOD datasets) for the OOD detection task demonstrate that CLIPN, based on ViT-B-16, outperforms 7 well-used algorithms by at least 2.34% and 11.64% in terms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K. Our CLIPN can serve as a solid foundation for effectively leveraging CLIP in downstream OOD tasks. The code is available on <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/CLIPN">https://github.com/xmed-lab/CLIPN</a>.</p>
<hr>
<h2 id="Towards-Real-Time-Analysis-of-Broadcast-Badminton-Videos"><a href="#Towards-Real-Time-Analysis-of-Broadcast-Badminton-Videos" class="headerlink" title="Towards Real-Time Analysis of Broadcast Badminton Videos"></a>Towards Real-Time Analysis of Broadcast Badminton Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12199">http://arxiv.org/abs/2308.12199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nitin Nilesh, Tushar Sharma, Anurag Ghosh, C. V. Jawahar</li>
</ul>
<p>Abstract:<br>Analysis of player movements is a crucial subset of sports analysis. Existing player movement analysis methods use recorded videos after the match is over. In this work, we propose an end-to-end framework for player movement analysis for badminton matches on live broadcast match videos. We only use the visual inputs from the match and, unlike other approaches which use multi-modal sensor data, our approach uses only visual cues. We propose a method to calculate the on-court distance covered by both the players from the video feed of a live broadcast badminton match. To perform this analysis, we focus on the gameplay by removing replays and other redundant parts of the broadcast match. We then perform player tracking to identify and track the movements of both players in each frame. Finally, we calculate the distance covered by each player and the average speed with which they move on the court. We further show a heatmap of the areas covered by the player on the court which is useful for analyzing the gameplay of the player. Our proposed framework was successfully used to analyze live broadcast matches in real-time during the Premier Badminton League 2019 (PBL 2019), with commentators and broadcasters appreciating the utility.</p>
<hr>
<h2 id="Sign-Language-Translation-with-Iterative-Prototype"><a href="#Sign-Language-Translation-with-Iterative-Prototype" class="headerlink" title="Sign Language Translation with Iterative Prototype"></a>Sign Language Translation with Iterative Prototype</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12191">http://arxiv.org/abs/2308.12191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huijie Yao, Wengang Zhou, Hao Feng, Hezhen Hu, Hao Zhou, Houqiang Li</li>
</ul>
<p>Abstract:<br>This paper presents IP-SLT, a simple yet effective framework for sign language translation (SLT). Our IP-SLT adopts a recurrent structure and enhances the semantic representation (prototype) of the input sign language video via an iterative refinement manner. Our idea mimics the behavior of human reading, where a sentence can be digested repeatedly, till reaching accurate understanding. Technically, IP-SLT consists of feature extraction, prototype initialization, and iterative prototype refinement. The initialization module generates the initial prototype based on the visual feature extracted by the feature extraction module. Then, the iterative refinement module leverages the cross-attention mechanism to polish the previous prototype by aggregating it with the original video feature. Through repeated refinement, the prototype finally converges to a more stable and accurate state, leading to a fluent and appropriate translation. In addition, to leverage the sequential dependence of prototypes, we further propose an iterative distillation loss to compress the knowledge of the final iteration into previous ones. As the autoregressive decoding process is executed only once in inference, our IP-SLT is ready to improve various SLT systems with acceptable overhead. Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the IP-SLT.</p>
<hr>
<h2 id="Tumor-Centered-Patching-for-Enhanced-Medical-Image-Segmentation"><a href="#Tumor-Centered-Patching-for-Enhanced-Medical-Image-Segmentation" class="headerlink" title="Tumor-Centered Patching for Enhanced Medical Image Segmentation"></a>Tumor-Centered Patching for Enhanced Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12168">http://arxiv.org/abs/2308.12168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutyyba Asghar, Ahmad Raza Shahid, Akhtar Jamil, Kiran Aftab, Syed Ather Enam</li>
</ul>
<p>Abstract:<br>The realm of medical image diagnosis has advanced significantly with the integration of computer-aided diagnosis and surgical systems. However, challenges persist, particularly in achieving precise image segmentation. While deep learning techniques show potential, obstacles like limited resources, slow convergence, and class imbalance impede their effectiveness. Traditional patch-based methods, though common, struggle to capture intricate tumor boundaries and often lead to redundant samples, compromising computational efficiency and feature quality. To tackle these issues, this research introduces an innovative approach centered on the tumor itself for patch-based image analysis. This novel tumor-centered patching method aims to address the class imbalance and boundary deficiencies, enabling focused and accurate tumor segmentation. By aligning patches with the tumor’s anatomical context, this technique enhances feature extraction accuracy and reduces computational load. Experimental results demonstrate improved class imbalance, with segmentation scores of 0.78, 0.76, and 0.71 for whole, core, and enhancing tumors, respectively using a lightweight simple U-Net. This approach shows potential for enhancing medical image segmentation and improving computer-aided diagnosis systems.</p>
<hr>
<h2 id="NPF-200-A-Multi-Modal-Eye-Fixation-Dataset-and-Method-for-Non-Photorealistic-Videos"><a href="#NPF-200-A-Multi-Modal-Eye-Fixation-Dataset-and-Method-for-Non-Photorealistic-Videos" class="headerlink" title="NPF-200: A Multi-Modal Eye Fixation Dataset and Method for Non-Photorealistic Videos"></a>NPF-200: A Multi-Modal Eye Fixation Dataset and Method for Non-Photorealistic Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12163">http://arxiv.org/abs/2308.12163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyu Yang, Sucheng Ren, Zongwei Wu, Nanxuan Zhao, Junle Wang, Jing Qin, Shengfeng He</li>
</ul>
<p>Abstract:<br>Non-photorealistic videos are in demand with the wave of the metaverse, but lack of sufficient research studies. This work aims to take a step forward to understand how humans perceive non-photorealistic videos with eye fixation (\ie, saliency detection), which is critical for enhancing media production, artistic design, and game user experience. To fill in the gap of missing a suitable dataset for this research line, we present NPF-200, the first large-scale multi-modal dataset of purely non-photorealistic videos with eye fixations. Our dataset has three characteristics: 1) it contains soundtracks that are essential according to vision and psychological studies; 2) it includes diverse semantic content and videos are of high-quality; 3) it has rich motions across and within videos. We conduct a series of analyses to gain deeper insights into this task and compare several state-of-the-art methods to explore the gap between natural images and non-photorealistic data. Additionally, as the human attention system tends to extract visual and audio features with different frequencies, we propose a universal frequency-aware multi-modal non-photorealistic saliency detection model called NPSNet, demonstrating the state-of-the-art performance of our task. The results uncover strengths and weaknesses of multi-modal network design and multi-domain training, opening up promising directions for future works. {Our dataset and code can be found at \url{<a target="_blank" rel="noopener" href="https://github.com/Yangziyu/NPF200%7D%7D">https://github.com/Yangziyu/NPF200}}</a>.</p>
<hr>
<h2 id="Multimodal-Latent-Emotion-Recognition-from-Micro-expression-and-Physiological-Signals"><a href="#Multimodal-Latent-Emotion-Recognition-from-Micro-expression-and-Physiological-Signals" class="headerlink" title="Multimodal Latent Emotion Recognition from Micro-expression and Physiological Signals"></a>Multimodal Latent Emotion Recognition from Micro-expression and Physiological Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12156">http://arxiv.org/abs/2308.12156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangfei Zhang, Yifei Qian, Ognjen Arandjelovic, Anthony Zhu</li>
</ul>
<p>Abstract:<br>This paper discusses the benefits of incorporating multimodal data for improving latent emotion recognition accuracy, focusing on micro-expression (ME) and physiological signals (PS). The proposed approach presents a novel multimodal learning framework that combines ME and PS, including a 1D separable and mixable depthwise inception network, a standardised normal distribution weighted feature fusion method, and depth&#x2F;physiology guided attention modules for multimodal learning. Experimental results show that the proposed approach outperforms the benchmark method, with the weighted fusion method and guided attention modules both contributing to enhanced performance.</p>
<hr>
<h2 id="A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models"><a href="#A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models" class="headerlink" title="A Probabilistic Fluctuation based Membership Inference Attack for Generative Models"></a>A Probabilistic Fluctuation based Membership Inference Attack for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12143">http://arxiv.org/abs/2308.12143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang</li>
</ul>
<p>Abstract:<br>Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model’s training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting these trends via analyzing the overall probabilistic fluctuations around given records. We conduct extensive experiments across multiple generative models and datasets, which demonstrate PFAMI can improve the attack success rate (ASR) by about 27.9% when compared with the best baseline.</p>
<hr>
<h2 id="Mesh-Conflation-of-Oblique-Photogrammetric-Models-using-Virtual-Cameras-and-Truncated-Signed-Distance-Field"><a href="#Mesh-Conflation-of-Oblique-Photogrammetric-Models-using-Virtual-Cameras-and-Truncated-Signed-Distance-Field" class="headerlink" title="Mesh Conflation of Oblique Photogrammetric Models using Virtual Cameras and Truncated Signed Distance Field"></a>Mesh Conflation of Oblique Photogrammetric Models using Virtual Cameras and Truncated Signed Distance Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12139">http://arxiv.org/abs/2308.12139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Song, Rongjun Qin</li>
</ul>
<p>Abstract:<br>Conflating&#x2F;stitching 2.5D raster digital surface models (DSM) into a large one has been a running practice in geoscience applications, however, conflating full-3D mesh models, such as those from oblique photogrammetry, is extremely challenging. In this letter, we propose a novel approach to address this challenge by conflating multiple full-3D oblique photogrammetric models into a single, and seamless mesh for high-resolution site modeling. Given two or more individually collected and created photogrammetric meshes, we first propose to create a virtual camera field (with a panoramic field of view) to incubate virtual spaces represented by Truncated Signed Distance Field (TSDF), an implicit volumetric field friendly for linear 3D fusion; then we adaptively leverage the truncated bound of meshes in TSDF to conflate them into a single and accurate full 3D site model. With drone-based 3D meshes, we show that our approach significantly improves upon traditional methods for model conflations, to drive new potentials to create excessively large and accurate full 3D mesh models in support of geoscience and environmental applications.</p>
<hr>
<h2 id="Select-and-Combine-SAC-A-Novel-Multi-Stereo-Depth-Fusion-Algorithm-for-Point-Cloud-Generation-via-Efficient-Local-Markov-Netlets"><a href="#Select-and-Combine-SAC-A-Novel-Multi-Stereo-Depth-Fusion-Algorithm-for-Point-Cloud-Generation-via-Efficient-Local-Markov-Netlets" class="headerlink" title="Select-and-Combine (SAC): A Novel Multi-Stereo Depth Fusion Algorithm for Point Cloud Generation via Efficient Local Markov Netlets"></a>Select-and-Combine (SAC): A Novel Multi-Stereo Depth Fusion Algorithm for Point Cloud Generation via Efficient Local Markov Netlets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12138">http://arxiv.org/abs/2308.12138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Elhashash, Rongjun Qin</li>
</ul>
<p>Abstract:<br>Many practical systems for image-based surface reconstruction employ a stereo&#x2F;multi-stereo paradigm, due to its ability to scale for large scenes and its ease of implementation for out-of-core operations. In this process, multiple and abundant depth maps from stereo matching must be combined and fused into a single, consistent, and clean point cloud. However, the noises and outliers caused by stereo matching and the heterogenous geometric errors of the poses present a challenge for existing fusion algorithms, since they mostly assume Gaussian errors and predict fused results based on data from local spatial neighborhoods, which may inherit uncertainties from multiple depths resulting in lowered accuracy. In this paper, we propose a novel depth fusion paradigm, that instead of numerically fusing points from multiple depth maps, selects the best depth map per point, and combines them into a single and clean point cloud. This paradigm, called select-and-combine (SAC), is achieved through modeling the point level fusion using local Markov Netlets, a micro-network over point across neighboring views for depth&#x2F;view selection, followed by a Netlets collapse process for point combination. The Markov Netlets are optimized such that they can inherently leverage spatial consistencies among depth maps of neighboring views, thus they can address errors beyond Gaussian ones. Our experiment results show that our approach outperforms existing depth fusion approaches by increasing the F1 score that considers both accuracy and completeness by 2.07% compared to the best existing method. Finally, our approach generates clearer point clouds that are 18% less redundant while with a higher accuracy before fusion</p>
<hr>
<h2 id="Lite-HRNet-Plus-Fast-and-Accurate-Facial-Landmark-Detection"><a href="#Lite-HRNet-Plus-Fast-and-Accurate-Facial-Landmark-Detection" class="headerlink" title="Lite-HRNet Plus: Fast and Accurate Facial Landmark Detection"></a>Lite-HRNet Plus: Fast and Accurate Facial Landmark Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12133">http://arxiv.org/abs/2308.12133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sota Kato, Kazuhiro Hotta, Yuhki Hatakeyama, Yoshinori Konishi</li>
</ul>
<p>Abstract:<br>Facial landmark detection is an essential technology for driver status tracking and has been in demand for real-time estimations. As a landmark coordinate prediction, heatmap-based methods are known to achieve a high accuracy, and Lite-HRNet can achieve a fast estimation. However, with Lite-HRNet, the problem of a heavy computational cost of the fusion block, which connects feature maps with different resolutions, has yet to be solved. In addition, the strong output module used in HRNetV2 is not applied to Lite-HRNet. Given these problems, we propose a novel architecture called Lite-HRNet Plus. Lite-HRNet Plus achieves two improvements: a novel fusion block based on a channel attention and a novel output module with less computational intensity using multi-resolution feature maps. Through experiments conducted on two facial landmark datasets, we confirmed that Lite-HRNet Plus further improved the accuracy in comparison with conventional methods, and achieved a state-of-the-art accuracy with a computational complexity with the range of 10M FLOPs.</p>
<hr>
<h2 id="Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models"><a href="#Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models" class="headerlink" title="Masking Strategies for Background Bias Removal in Computer Vision Models"></a>Masking Strategies for Background Bias Removal in Computer Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12127">http://arxiv.org/abs/2308.12127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ananthu-aniraj/masking_strategies_bias_removal">https://github.com/ananthu-aniraj/masking_strategies_bias_removal</a></li>
<li>paper_authors: Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos</li>
</ul>
<p>Abstract:<br>Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.</p>
<hr>
<h2 id="The-TYC-Dataset-for-Understanding-Instance-Level-Semantics-and-Motions-of-Cells-in-Microstructures"><a href="#The-TYC-Dataset-for-Understanding-Instance-Level-Semantics-and-Motions-of-Cells-in-Microstructures" class="headerlink" title="The TYC Dataset for Understanding Instance-Level Semantics and Motions of Cells in Microstructures"></a>The TYC Dataset for Understanding Instance-Level Semantics and Motions of Cells in Microstructures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12116">http://arxiv.org/abs/2308.12116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoph Reich, Tim Prangemeier, Heinz Koeppl</li>
</ul>
<p>Abstract:<br>Segmenting cells and tracking their motion over time is a common task in biomedical applications. However, predicting accurate instance-wise segmentation and cell motions from microscopy imagery remains a challenging task. Using microstructured environments for analyzing single cells in a constant flow of media adds additional complexity. While large-scale labeled microscopy datasets are available, we are not aware of any large-scale dataset, including both cells and microstructures. In this paper, we introduce the trapped yeast cell (TYC) dataset, a novel dataset for understanding instance-level semantics and motions of cells in microstructures. We release $105$ dense annotated high-resolution brightfield microscopy images, including about $19$k instance masks. We also release $261$ curated video clips composed of $1293$ high-resolution microscopy images to facilitate unsupervised understanding of cell motions and morphology. TYC offers ten times more instance annotations than the previously largest dataset, including cells and microstructures. Our effort also exceeds previous attempts in terms of microstructure variability, resolution, complexity, and capturing device (microscopy) variability. We facilitate a unified comparison on our novel dataset by introducing a standardized evaluation strategy. TYC and evaluation code are publicly available under CC BY 4.0 license.</p>
<hr>
<h2 id="Less-is-More-–-Towards-parsimonious-multi-task-models-using-structured-sparsity"><a href="#Less-is-More-–-Towards-parsimonious-multi-task-models-using-structured-sparsity" class="headerlink" title="Less is More – Towards parsimonious multi-task models using structured sparsity"></a>Less is More – Towards parsimonious multi-task models using structured sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12114">http://arxiv.org/abs/2308.12114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richa Upadhyay, Ronald Phlypo, Rajkumar Saini, Marcus Liwicki</li>
</ul>
<p>Abstract:<br>Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups. This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model. Sparsifying the model during training helps decrease the model’s memory footprint, computation requirements, and prediction time during inference. We use channel-wise l1&#x2F;l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN). This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks. We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ. We also investigate how changing the sparsification degree impacts both the performance of the model and the sparsity of groups.</p>
<hr>
<h2 id="Advancements-in-Point-Cloud-Data-Augmentation-for-Deep-Learning-A-Survey"><a href="#Advancements-in-Point-Cloud-Data-Augmentation-for-Deep-Learning-A-Survey" class="headerlink" title="Advancements in Point Cloud Data Augmentation for Deep Learning: A Survey"></a>Advancements in Point Cloud Data Augmentation for Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12113">http://arxiv.org/abs/2308.12113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinfeng Zhu, Lei Fan, Ningxin Weng</li>
</ul>
<p>Abstract:<br>Point cloud has a wide range of applications in areas such as autonomous driving, mapping, navigation, scene reconstruction, and medical imaging. Due to its great potentials in these applications, point cloud processing has gained great attention in the field of computer vision. Among various point cloud processing techniques, deep learning (DL) has become one of the mainstream and effective methods for tasks such as detection, segmentation and classification. To reduce overfitting during training DL models and improve model performance especially when the amount and&#x2F;or diversity of training data are limited, augmentation is often crucial. Although various point cloud data augmentation methods have been widely used in different point cloud processing tasks, there are currently no published systematic surveys or reviews of these methods. Therefore, this article surveys and discusses these methods and categorizes them into a taxonomy framework. Through the comprehensive evaluation and comparison of the augmentation methods, this article identifies their potentials and limitations and suggests possible future research directions. This work helps researchers gain a holistic understanding of the current status of point cloud data augmentation and promotes its wider application and development.</p>
<hr>
<h2 id="Generalized-Continual-Category-Discovery"><a href="#Generalized-Continual-Category-Discovery" class="headerlink" title="Generalized Continual Category Discovery"></a>Generalized Continual Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12112">http://arxiv.org/abs/2308.12112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Marczak, Grzegorz Rypeść, Sebastian Cygert, Tomasz Trzciński, Bartłomiej Twardowski</li>
</ul>
<p>Abstract:<br>Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of novel classes are present. In light of these limitations, we propose a method that incorporates both supervised and unsupervised signals and mitigates the forgetting through the use of centroid adaptation. Our method surpasses strong CL methods adopted for GCD techniques and presents a superior representation learning performance.</p>
<hr>
<h2 id="Cross-Modality-Proposal-guided-Feature-Mining-for-Unregistered-RGB-Thermal-Pedestrian-Detection"><a href="#Cross-Modality-Proposal-guided-Feature-Mining-for-Unregistered-RGB-Thermal-Pedestrian-Detection" class="headerlink" title="Cross-Modality Proposal-guided Feature Mining for Unregistered RGB-Thermal Pedestrian Detection"></a>Cross-Modality Proposal-guided Feature Mining for Unregistered RGB-Thermal Pedestrian Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12111">http://arxiv.org/abs/2308.12111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Tian, Zikun Zhou, Yuqing Huang, Gaojun Li, Zhenyu He</li>
</ul>
<p>Abstract:<br>RGB-Thermal (RGB-T) pedestrian detection aims to locate the pedestrians in RGB-T image pairs to exploit the complementation between the two modalities for improving detection robustness in extreme conditions. Most existing algorithms assume that the RGB-T image pairs are well registered, while in the real world they are not aligned ideally due to parallax or different field-of-view of the cameras. The pedestrians in misaligned image pairs may locate at different positions in two images, which results in two challenges: 1) how to achieve inter-modality complementation using spatially misaligned RGB-T pedestrian patches, and 2) how to recognize the unpaired pedestrians at the boundary. To deal with these issues, we propose a new paradigm for unregistered RGB-T pedestrian detection, which predicts two separate pedestrian locations in the RGB and thermal images, respectively. Specifically, we propose a cross-modality proposal-guided feature mining (CPFM) mechanism to extract the two precise fusion features for representing the pedestrian in the two modalities, even if the RGB-T image pair is unaligned. It enables us to effectively exploit the complementation between the two modalities. With the CPFM mechanism, we build a two-stream dense detector; it predicts the two pedestrian locations in the two modalities based on the corresponding fusion feature mined by the CPFM mechanism. Besides, we design a data augmentation method, named Homography, to simulate the discrepancy in scales and views between images. We also investigate two non-maximum suppression (NMS) methods for post-processing. Favorable experimental results demonstrate the effectiveness and robustness of our method in dealing with unregistered pedestrians with different shifts.</p>
<hr>
<h2 id="DISGAN-Wavelet-informed-Discriminator-Guides-GAN-to-MRI-Super-resolution-with-Noise-Cleaning"><a href="#DISGAN-Wavelet-informed-Discriminator-Guides-GAN-to-MRI-Super-resolution-with-Noise-Cleaning" class="headerlink" title="DISGAN: Wavelet-informed Discriminator Guides GAN to MRI Super-resolution with Noise Cleaning"></a>DISGAN: Wavelet-informed Discriminator Guides GAN to MRI Super-resolution with Noise Cleaning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12084">http://arxiv.org/abs/2308.12084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Wang, Lucas Mahler, Julius Steiglechner, Florian Birk, Klaus Scheffler, Gabriele Lohmann</li>
</ul>
<p>Abstract:<br>MRI super-resolution (SR) and denoising tasks are fundamental challenges in the field of deep learning, which have traditionally been treated as distinct tasks with separate paired training data. In this paper, we propose an innovative method that addresses both tasks simultaneously using a single deep learning model, eliminating the need for explicitly paired noisy and clean images during training. Our proposed model is primarily trained for SR, but also exhibits remarkable noise-cleaning capabilities in the super-resolved images. Instead of conventional approaches that introduce frequency-related operations into the generative process, our novel approach involves the use of a GAN model guided by a frequency-informed discriminator. To achieve this, we harness the power of the 3D Discrete Wavelet Transform (DWT) operation as a frequency constraint within the GAN framework for the SR task on magnetic resonance imaging (MRI) data. Specifically, our contributions include: 1) a 3D generator based on residual-in-residual connected blocks; 2) the integration of the 3D DWT with $1\times 1$ convolution into a DWT+conv unit within a 3D Unet for the discriminator; 3) the use of the trained model for high-quality image SR, accompanied by an intrinsic denoising process. We dub the model “Denoising Induced Super-resolution GAN (DISGAN)” due to its dual effects of SR image generation and simultaneous denoising. Departing from the traditional approach of training SR and denoising tasks as separate models, our proposed DISGAN is trained only on the SR task, but also achieves exceptional performance in denoising. The model is trained on 3D MRI data from dozens of subjects from the Human Connectome Project (HCP) and further evaluated on previously unseen MRI data from subjects with brain tumours and epilepsy to assess its denoising and SR performance.</p>
<hr>
<h2 id="Understanding-Dark-Scenes-by-Contrasting-Multi-Modal-Observations"><a href="#Understanding-Dark-Scenes-by-Contrasting-Multi-Modal-Observations" class="headerlink" title="Understanding Dark Scenes by Contrasting Multi-Modal Observations"></a>Understanding Dark Scenes by Contrasting Multi-Modal Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12320">http://arxiv.org/abs/2308.12320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/palmdong/smmcl">https://github.com/palmdong/smmcl</a></li>
<li>paper_authors: Xiaoyu Dong, Naoto Yokoya</li>
</ul>
<p>Abstract:<br>Understanding dark scenes based on multi-modal image data is challenging, as both the visible and auxiliary modalities provide limited semantic information for the task. Previous methods focus on fusing the two modalities but neglect the correlations among semantic classes when minimizing losses to align pixels with labels, resulting in inaccurate class predictions. To address these issues, we introduce a supervised multi-modal contrastive learning approach to increase the semantic discriminability of the learned multi-modal feature spaces by jointly performing cross-modal and intra-modal contrast under the supervision of the class correlations. The cross-modal contrast encourages same-class embeddings from across the two modalities to be closer and pushes different-class ones apart. The intra-modal contrast forces same-class or different-class embeddings within each modality to be together or apart. We validate our approach on a variety of tasks that cover diverse light conditions and image modalities. Experiments show that our approach can effectively enhance dark scene understanding based on multi-modal images with limited semantics by shaping semantic-discriminative feature spaces. Comparisons with previous methods demonstrate our state-of-the-art performance. Code and pretrained models are available at <a target="_blank" rel="noopener" href="https://github.com/palmdong/SMMCL">https://github.com/palmdong/SMMCL</a>.</p>
<hr>
<h2 id="RemovalNet-DNN-Fingerprint-Removal-Attacks"><a href="#RemovalNet-DNN-Fingerprint-Removal-Attacks" class="headerlink" title="RemovalNet: DNN Fingerprint Removal Attacks"></a>RemovalNet: DNN Fingerprint Removal Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12319">http://arxiv.org/abs/2308.12319</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grasses/removalnet">https://github.com/grasses/removalnet</a></li>
<li>paper_authors: Hongwei Yao, Zheng Li, Kunzhe Huang, Jian Lou, Zhan Qin, Kui Ren</li>
</ul>
<p>Abstract:<br>With the performance of deep neural networks (DNNs) remarkably improving, DNNs have been widely used in many areas. Consequently, the DNN model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., DNN fingerprinting). However, the feasibility of the DNN fingerprint removal attack and its potential influence remains an open problem. In this paper, we perform the first comprehensive investigation of DNN fingerprint removal attacks. Generally, the knowledge contained in a DNN model can be categorized into general semantic and fingerprint-specific knowledge. To this end, we propose a min-max bilevel optimization-based DNN fingerprint removal attack named RemovalNet, to evade model ownership verification. The lower-level optimization is designed to remove fingerprint-specific knowledge. While in the upper-level optimization, we distill the victim model’s general semantic knowledge to maintain the surrogate model’s performance. We conduct extensive experiments to evaluate the fidelity, effectiveness, and efficiency of the RemovalNet against four advanced defense methods on six metrics. The empirical results demonstrate that (1) the RemovalNet is effective. After our DNN fingerprint removal attack, the model distance between the target and surrogate models is x100 times higher than that of the baseline attacks, (2) the RemovalNet is efficient. It uses only 0.2% (400 samples) of the substitute dataset and 1,000 iterations to conduct our attack. Besides, compared with advanced model stealing attacks, the RemovalNet saves nearly 85% of computational resources at most, (3) the RemovalNet achieves high fidelity that the created surrogate model maintains high accuracy after the DNN fingerprint removal process. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/grasses/RemovalNet">https://github.com/grasses/RemovalNet</a>.</p>
<hr>
<h2 id="InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4"><a href="#InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4" class="headerlink" title="InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"></a>InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12067">http://arxiv.org/abs/2308.12067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun</li>
</ul>
<p>Abstract:<br>Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient to enable multimodal large language models to generate better output.</p>
<hr>
<h2 id="SILT-Shadow-aware-Iterative-Label-Tuning-for-Learning-to-Detect-Shadows-from-Noisy-Labels"><a href="#SILT-Shadow-aware-Iterative-Label-Tuning-for-Learning-to-Detect-Shadows-from-Noisy-Labels" class="headerlink" title="SILT: Shadow-aware Iterative Label Tuning for Learning to Detect Shadows from Noisy Labels"></a>SILT: Shadow-aware Iterative Label Tuning for Learning to Detect Shadows from Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12064">http://arxiv.org/abs/2308.12064</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cralence/silt">https://github.com/cralence/silt</a></li>
<li>paper_authors: Han Yang, Tianyu Wang, Xiaowei Hu, Chi-Wing Fu</li>
</ul>
<p>Abstract:<br>Existing shadow detection datasets often contain missing or mislabeled shadows, which can hinder the performance of deep learning models trained directly on such data. To address this issue, we propose SILT, the Shadow-aware Iterative Label Tuning framework, which explicitly considers noise in shadow labels and trains the deep model in a self-training manner. Specifically, we incorporate strong data augmentations with shadow counterfeiting to help the network better recognize non-shadow regions and alleviate overfitting. We also devise a simple yet effective label tuning strategy with global-local fusion and shadow-aware filtering to encourage the network to make significant refinements on the noisy labels. We evaluate the performance of SILT by relabeling the test set of the SBU dataset and conducting various experiments. Our results show that even a simple U-Net trained with SILT can outperform all state-of-the-art methods by a large margin. When trained on SBU &#x2F; UCF &#x2F; ISTD, our network can successfully reduce the Balanced Error Rate by 25.2% &#x2F; 36.9% &#x2F; 21.3% over the best state-of-the-art method.</p>
<hr>
<h2 id="HarvestNet-A-Dataset-for-Detecting-Smallholder-Farming-Activity-Using-Harvest-Piles-and-Remote-Sensing"><a href="#HarvestNet-A-Dataset-for-Detecting-Smallholder-Farming-Activity-Using-Harvest-Piles-and-Remote-Sensing" class="headerlink" title="HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing"></a>HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12061">http://arxiv.org/abs/2308.12061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Xu, Amna Elmustafa, Liya Weldegebriel, Emnet Negash, Richard Lee, Chenlin Meng, Stefano Ermon, David Lobell</li>
</ul>
<p>Abstract:<br>Small farms contribute to a large share of the productive land in developing countries. In regions such as sub-Saharan Africa, where 80% of farms are small (under 2 ha in size), the task of mapping smallholder cropland is an important part of tracking sustainability measures such as crop productivity. However, the visually diverse and nuanced appearance of small farms has limited the effectiveness of traditional approaches to cropland mapping. Here we introduce a new approach based on the detection of harvest piles characteristic of many smallholder systems throughout the world. We present HarvestNet, a dataset for mapping the presence of farms in the Ethiopian regions of Tigray and Amhara during 2020-2023, collected using expert knowledge and satellite images, totaling 7k hand-labeled images and 2k ground collected labels. We also benchmark a set of baselines including SOTA models in remote sensing with our best models having around 80% classification performance on hand labelled data and 90%, 98% accuracy on ground truth data for Tigray, Amhara respectively. We also perform a visual comparison with a widely used pre-existing coverage map and show that our model detects an extra 56,621 hectares of cropland in Tigray. We conclude that remote sensing of harvest piles can contribute to more timely and accurate cropland assessments in food insecure region.</p>
<hr>
<h2 id="Manipulating-Embeddings-of-Stable-Diffusion-Prompts"><a href="#Manipulating-Embeddings-of-Stable-Diffusion-Prompts" class="headerlink" title="Manipulating Embeddings of Stable Diffusion Prompts"></a>Manipulating Embeddings of Stable Diffusion Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12059">http://arxiv.org/abs/2308.12059</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/webis-de/arxiv23-prompt-embedding-manipulation">https://github.com/webis-de/arxiv23-prompt-embedding-manipulation</a></li>
<li>paper_authors: Niklas Deckers, Julia Peters, Martin Potthast</li>
</ul>
<p>Abstract:<br>Generative text-to-image models such as Stable Diffusion allow users to generate images based on a textual description, the prompt. Changing the prompt is still the primary means for the user to change a generated image as desired. However, changing the image by reformulating the prompt remains a difficult process of trial and error, which has led to the emergence of prompt engineering as a new field of research. We propose and analyze methods to change the embedding of a prompt directly instead of the prompt text. It allows for more fine-grained and targeted control that takes into account user intentions. Our approach treats the generative text-to-image model as a continuous function and passes gradients between the image space and the prompt embedding space. By addressing different user interaction problems, we can apply this idea in three scenarios: (1) Optimization of a metric defined in image space that could measure, for example, image style. (2) Assistance of users in creative tasks by enabling them to navigate the image space along a selection of directions of “near” prompt embeddings. (3) Changing the embedding of the prompt to include information that the user has seen in a particular seed but finds difficult to describe in the prompt. Our experiments demonstrate the feasibility of the described methods.</p>
<hr>
<h2 id="DR-Tune-Improving-Fine-tuning-of-Pretrained-Visual-Models-by-Distribution-Regularization-with-Semantic-Calibration"><a href="#DR-Tune-Improving-Fine-tuning-of-Pretrained-Visual-Models-by-Distribution-Regularization-with-Semantic-Calibration" class="headerlink" title="DR-Tune: Improving Fine-tuning of Pretrained Visual Models by Distribution Regularization with Semantic Calibration"></a>DR-Tune: Improving Fine-tuning of Pretrained Visual Models by Distribution Regularization with Semantic Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12058">http://arxiv.org/abs/2308.12058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weeknan/dr-tune">https://github.com/weeknan/dr-tune</a></li>
<li>paper_authors: Nan Zhou, Jiaxin Chen, Di Huang</li>
</ul>
<p>Abstract:<br>The visual models pretrained on large-scale benchmarks encode general knowledge and prove effective in building more powerful representations for downstream tasks. Most existing approaches follow the fine-tuning paradigm, either by initializing or regularizing the downstream model based on the pretrained one. The former fails to retain the knowledge in the successive fine-tuning phase, thereby prone to be over-fitting, and the latter imposes strong constraints to the weights or feature maps of the downstream model without considering semantic drift, often incurring insufficient optimization. To deal with these issues, we propose a novel fine-tuning framework, namely distribution regularization with semantic calibration (DR-Tune). It employs distribution regularization by enforcing the downstream task head to decrease its classification error on the pretrained feature distribution, which prevents it from over-fitting while enabling sufficient training of downstream encoders. Furthermore, to alleviate the interference by semantic drift, we develop the semantic calibration (SC) module to align the global shape and class centers of the pretrained and downstream feature distributions. Extensive experiments on widely used image classification datasets show that DR-Tune consistently improves the performance when combing with various backbones under different pretraining strategies. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/weeknan/DR-Tune">https://github.com/weeknan/DR-Tune</a>.</p>
<hr>
<h2 id="Towards-Privacy-Supporting-Fall-Detection-via-Deep-Unsupervised-RGB2Depth-Adaptation"><a href="#Towards-Privacy-Supporting-Fall-Detection-via-Deep-Unsupervised-RGB2Depth-Adaptation" class="headerlink" title="Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation"></a>Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12049">http://arxiv.org/abs/2308.12049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/1015206533/privacy_supporting_fall_detection">https://github.com/1015206533/privacy_supporting_fall_detection</a></li>
<li>paper_authors: Hejun Xiao, Kunyu Peng, Xiangsheng Huang, Alina Roitberg1, Hao Li, Zhaohui Wang, Rainer Stiefelhagen</li>
</ul>
<p>Abstract:<br>Fall detection is a vital task in health monitoring, as it allows the system to trigger an alert and therefore enabling faster interventions when a person experiences a fall. Although most previous approaches rely on standard RGB video data, such detailed appearance-aware monitoring poses significant privacy concerns. Depth sensors, on the other hand, are better at preserving privacy as they merely capture the distance of objects from the sensor or camera, omitting color and texture information. In this paper, we introduce a privacy-supporting solution that makes the RGB-trained model applicable in depth domain and utilizes depth data at test time for fall detection. To achieve cross-modal fall detection, we present an unsupervised RGB to Depth (RGB2Depth) cross-modal domain adaptation approach that leverages labelled RGB data and unlabelled depth data during training. Our proposed pipeline incorporates an intermediate domain module for feature bridging, modality adversarial loss for modality discrimination, classification loss for pseudo-labeled depth data and labeled source data, triplet loss that considers both source and target domains, and a novel adaptive loss weight adjustment method for improved coordination among various losses. Our approach achieves state-of-the-art results in the unsupervised RGB2Depth domain adaptation task for fall detection. Code is available at <a target="_blank" rel="noopener" href="https://github.com/1015206533/privacy_supporting_fall_detection">https://github.com/1015206533/privacy_supporting_fall_detection</a>.</p>
<hr>
<h2 id="Head-Tail-Cooperative-Learning-Network-for-Unbiased-Scene-Graph-Generation"><a href="#Head-Tail-Cooperative-Learning-Network-for-Unbiased-Scene-Graph-Generation" class="headerlink" title="Head-Tail Cooperative Learning Network for Unbiased Scene Graph Generation"></a>Head-Tail Cooperative Learning Network for Unbiased Scene Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12048">http://arxiv.org/abs/2308.12048</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wanglei0618/htcl">https://github.com/wanglei0618/htcl</a></li>
<li>paper_authors: Lei Wang, Zejian Yuan, Yao Lu, Badong Chen</li>
</ul>
<p>Abstract:<br>Scene Graph Generation (SGG) as a critical task in image understanding, facing the challenge of head-biased prediction caused by the long-tail distribution of predicates. However, current unbiased SGG methods can easily prioritize improving the prediction of tail predicates while ignoring the substantial sacrifice in the prediction of head predicates, leading to a shift from head bias to tail bias. To address this issue, we propose a model-agnostic Head-Tail Collaborative Learning (HTCL) network that includes head-prefer and tail-prefer feature representation branches that collaborate to achieve accurate recognition of both head and tail predicates. We also propose a self-supervised learning approach to enhance the prediction ability of the tail-prefer feature representation branch by constraining tail-prefer predicate features. Specifically, self-supervised learning converges head predicate features to their class centers while dispersing tail predicate features as much as possible through contrast learning and head center loss. We demonstrate the effectiveness of our HTCL by applying it to various SGG models on VG150, Open Images V6 and GQA200 datasets. The results show that our method achieves higher mean Recall with a minimal sacrifice in Recall and achieves a new state-of-the-art overall performance. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/wanglei0618/HTCL">https://github.com/wanglei0618/HTCL</a>.</p>
<hr>
<h2 id="CgT-GAN-CLIP-guided-Text-GAN-for-Image-Captioning"><a href="#CgT-GAN-CLIP-guided-Text-GAN-for-Image-Captioning" class="headerlink" title="CgT-GAN: CLIP-guided Text GAN for Image Captioning"></a>CgT-GAN: CLIP-guided Text GAN for Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12045">http://arxiv.org/abs/2308.12045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lihr747/cgtgan">https://github.com/lihr747/cgtgan</a></li>
<li>paper_authors: Jiarui Yu, Haoran Li, Yanbin Hao, Bin Zhu, Tong Xu, Xiangnan He</li>
</ul>
<p>Abstract:<br>The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training&#x2F;inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to “see” real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN’s discriminator and the semantic guidance reward computed by the CLIP-based reward module. In addition to the cosine similarity as the semantic guidance reward (i.e., CLIP-cos), we further introduce a novel semantic guidance reward called CLIP-agg, which aligns the generated caption with a weighted text embedding by attentively aggregating the entire corpus. Experimental results on three subtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms state-of-the-art methods significantly across all metrics. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Lihr747/CgtGAN">https://github.com/Lihr747/CgtGAN</a>.</p>
<hr>
<h2 id="Large-Multilingual-Models-Pivot-Zero-Shot-Multimodal-Learning-across-Languages"><a href="#Large-Multilingual-Models-Pivot-Zero-Shot-Multimodal-Learning-across-Languages" class="headerlink" title="Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages"></a>Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12038">http://arxiv.org/abs/2308.12038</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openbmb/viscpm">https://github.com/openbmb/viscpm</a></li>
<li>paper_authors: Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, Maosong Sun</li>
</ul>
<p>Abstract:<br>Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a zero-shot manner for both image-to-text and text-to-image generation, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in image-to-text and text-to-image generation, which achieve state-of-the-art (open-source) performance in Chinese. To facilitate future research, we open-source codes and model weights at <a target="_blank" rel="noopener" href="https://github.com/OpenBMB/VisCPM.git">https://github.com/OpenBMB/VisCPM.git</a>.</p>
<hr>
<h2 id="RefEgo-Referring-Expression-Comprehension-Dataset-from-First-Person-Perception-of-Ego4D"><a href="#RefEgo-Referring-Expression-Comprehension-Dataset-from-First-Person-Perception-of-Ego4D" class="headerlink" title="RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D"></a>RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12035">http://arxiv.org/abs/2308.12035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuhei Kurita, Naoki Katsura, Eri Onami</li>
</ul>
<p>Abstract:<br>Grounding textual expressions on scene objects from first-person views is a truly demanding capability in developing agents that are aware of their surroundings and behave following intuitive text instructions. Such capability is of necessity for glass-devices or autonomous robots to localize referred objects in the real-world. In the conventional referring expression comprehension tasks of images, however, datasets are mostly constructed based on the web-crawled data and don’t reflect diverse real-world structures on the task of grounding textual expressions in diverse objects in the real world. Recently, a massive-scale egocentric video dataset of Ego4D was proposed. Ego4D covers around the world diverse real-world scenes including numerous indoor and outdoor situations such as shopping, cooking, walking, talking, manufacturing, etc. Based on egocentric videos of Ego4D, we constructed a broad coverage of the video-based referring expression comprehension dataset: RefEgo. Our dataset includes more than 12k video clips and 41 hours for video-based referring expression comprehension annotation. In experiments, we combine the state-of-the-art 2D referring expression comprehension models with the object tracking algorithm, achieving the video-wise referred object tracking even in difficult conditions: the referred object becomes out-of-frame in the middle of the video or multiple similar objects are presented in the video.</p>
<hr>
<h2 id="Distribution-Aware-Calibration-for-Object-Detection-with-Noisy-Bounding-Boxes"><a href="#Distribution-Aware-Calibration-for-Object-Detection-with-Noisy-Bounding-Boxes" class="headerlink" title="Distribution-Aware Calibration for Object Detection with Noisy Bounding Boxes"></a>Distribution-Aware Calibration for Object Detection with Noisy Bounding Boxes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12017">http://arxiv.org/abs/2308.12017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donghao Zhou, Jialin Li, Jinpeng Li, Jiancheng Huang, Qiang Nie, Yong Liu, Bin-Bin Gao, Qiong Wang, Pheng-Ann Heng, Guangyong Chen</li>
</ul>
<p>Abstract:<br>Large-scale well-annotated datasets are of great importance for training an effective object detector. However, obtaining accurate bounding box annotations is laborious and demanding. Unfortunately, the resultant noisy bounding boxes could cause corrupt supervision signals and thus diminish detection performance. Motivated by the observation that the real ground-truth is usually situated in the aggregation region of the proposals assigned to a noisy ground-truth, we propose DIStribution-aware CalibratiOn (DISCO) to model the spatial distribution of proposals for calibrating supervision signals. In DISCO, spatial distribution modeling is performed to statistically extract the potential locations of objects. Based on the modeled distribution, three distribution-aware techniques, i.e., distribution-aware proposal augmentation (DA-Aug), distribution-aware box refinement (DA-Ref), and distribution-aware confidence estimation (DA-Est), are developed to improve classification, localization, and interpretability, respectively. Extensive experiments on large-scale noisy image datasets (i.e., Pascal VOC and MS-COCO) demonstrate that DISCO can achieve state-of-the-art detection performance, especially at high noise levels.</p>
<hr>
<h2 id="StofNet-Super-resolution-Time-of-Flight-Network"><a href="#StofNet-Super-resolution-Time-of-Flight-Network" class="headerlink" title="StofNet: Super-resolution Time of Flight Network"></a>StofNet: Super-resolution Time of Flight Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12009">http://arxiv.org/abs/2308.12009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hahnec/stofnet">https://github.com/hahnec/stofnet</a></li>
<li>paper_authors: Christopher Hahne, Michel Hayoz, Raphael Sznitman</li>
</ul>
<p>Abstract:<br>Time of Flight (ToF) is a prevalent depth sensing technology in the fields of robotics, medical imaging, and non-destructive testing. Yet, ToF sensing faces challenges from complex ambient conditions making an inverse modelling from the sparse temporal information intractable. This paper highlights the potential of modern super-resolution techniques to learn varying surroundings for a reliable and accurate ToF detection. Unlike existing models, we tailor an architecture for sub-sample precise semi-global signal localization by combining super-resolution with an efficient residual contraction block to balance between fine signal details and large scale contextual information. We consolidate research on ToF by conducting a benchmark comparison against six state-of-the-art methods for which we employ two publicly available datasets. This includes the release of our SToF-Chirp dataset captured by an airborne ultrasound transducer. Results showcase the superior performance of our proposed StofNet in terms of precision, reliability and model complexity. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/hahnec/stofnet">https://github.com/hahnec/stofnet</a>.</p>
<hr>
<h2 id="Multi-stage-Factorized-Spatio-Temporal-Representation-for-RGB-D-Action-and-Gesture-Recognition"><a href="#Multi-stage-Factorized-Spatio-Temporal-Representation-for-RGB-D-Action-and-Gesture-Recognition" class="headerlink" title="Multi-stage Factorized Spatio-Temporal Representation for RGB-D Action and Gesture Recognition"></a>Multi-stage Factorized Spatio-Temporal Representation for RGB-D Action and Gesture Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12006">http://arxiv.org/abs/2308.12006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujun Ma, Benjia Zhou, Ruili Wang, Pichao Wang</li>
</ul>
<p>Abstract:<br>RGB-D action and gesture recognition remain an interesting topic in human-centered scene understanding, primarily due to the multiple granularities and large variation in human motion. Although many RGB-D based action and gesture recognition approaches have demonstrated remarkable results by utilizing highly integrated spatio-temporal representations across multiple modalities (i.e., RGB and depth data), they still encounter several challenges. Firstly, vanilla 3D convolution makes it hard to capture fine-grained motion differences between local clips under different modalities. Secondly, the intricate nature of highly integrated spatio-temporal modeling can lead to optimization difficulties. Thirdly, duplicate and unnecessary information can add complexity and complicate entangled spatio-temporal modeling. To address the above issues, we propose an innovative heuristic architecture called Multi-stage Factorized Spatio-Temporal (MFST) for RGB-D action and gesture recognition. The proposed MFST model comprises a 3D Central Difference Convolution Stem (CDC-Stem) module and multiple factorized spatio-temporal stages. The CDC-Stem enriches fine-grained temporal perception, and the multiple hierarchical spatio-temporal stages construct dimension-independent higher-order semantic primitives. Specifically, the CDC-Stem module captures bottom-level spatio-temporal features and passes them successively to the following spatio-temporal factored stages to capture the hierarchical spatial and temporal features through the Multi- Scale Convolution and Transformer (MSC-Trans) hybrid block and Weight-shared Multi-Scale Transformer (WMS-Trans) block. The seamless integration of these innovative designs results in a robust spatio-temporal representation that outperforms state-of-the-art approaches on RGB-D action and gesture recognition datasets.</p>
<hr>
<h2 id="Local-Distortion-Aware-Efficient-Transformer-Adaptation-for-Image-Quality-Assessment"><a href="#Local-Distortion-Aware-Efficient-Transformer-Adaptation-for-Image-Quality-Assessment" class="headerlink" title="Local Distortion Aware Efficient Transformer Adaptation for Image Quality Assessment"></a>Local Distortion Aware Efficient Transformer Adaptation for Image Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12001">http://arxiv.org/abs/2308.12001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kangmin Xu, Liang Liao, Jing Xiao, Chaofeng Chen, Haoning Wu, Qiong Yan, Weisi Lin</li>
</ul>
<p>Abstract:<br>Image Quality Assessment (IQA) constitutes a fundamental task within the field of computer vision, yet it remains an unresolved challenge, owing to the intricate distortion conditions, diverse image contents, and limited availability of data. Recently, the community has witnessed the emergence of numerous large-scale pretrained foundation models, which greatly benefit from dramatically increased data and parameter capacities. However, it remains an open problem whether the scaling law in high-level tasks is also applicable to IQA task which is closely related to low-level clues. In this paper, we demonstrate that with proper injection of local distortion features, a larger pretrained and fixed foundation model performs better in IQA tasks. Specifically, for the lack of local distortion structure and inductive bias of vision transformer (ViT), alongside the large-scale pretrained ViT, we use another pretrained convolution neural network (CNN), which is well known for capturing the local structure, to extract multi-scale image features. Further, we propose a local distortion extractor to obtain local distortion features from the pretrained CNN and a local distortion injector to inject the local distortion features into ViT. By only training the extractor and injector, our method can benefit from the rich knowledge in the powerful foundation models and achieve state-of-the-art performance on popular IQA datasets, indicating that IQA is not only a low-level problem but also benefits from stronger high-level features drawn from large-scale pretrained models.</p>
<hr>
<h2 id="Progressive-Feature-Mining-and-External-Knowledge-Assisted-Text-Pedestrian-Image-Retrieval"><a href="#Progressive-Feature-Mining-and-External-Knowledge-Assisted-Text-Pedestrian-Image-Retrieval" class="headerlink" title="Progressive Feature Mining and External Knowledge-Assisted Text-Pedestrian Image Retrieval"></a>Progressive Feature Mining and External Knowledge-Assisted Text-Pedestrian Image Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11994">http://arxiv.org/abs/2308.11994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huafeng Li, Shedan Yang, Yafei Zhang, Dapeng Tao, Zhengtao Yu</li>
</ul>
<p>Abstract:<br>Text-Pedestrian Image Retrieval aims to use the text describing pedestrian appearance to retrieve the corresponding pedestrian image. This task involves not only modality discrepancy, but also the challenge of the textual diversity of pedestrians with the same identity. At present, although existing research progress has been made in text-pedestrian image retrieval, these methods do not comprehensively consider the above-mentioned problems. Considering these, this paper proposes a progressive feature mining and external knowledge-assisted feature purification method. Specifically, we use a progressive mining mode to enable the model to mine discriminative features from neglected information, thereby avoiding the loss of discriminative information and improving the expression ability of features. In addition, to further reduce the negative impact of modal discrepancy and text diversity on cross-modal matching, we propose to use other sample knowledge of the same modality, i.e., external knowledge to enhance identity-consistent features and weaken identity-inconsistent features. This process purifies features and alleviates the interference caused by textual diversity and negative sample correlation features of the same modal. Extensive experiments on three challenging datasets demonstrate the effectiveness and superiority of the proposed method, and the retrieval performance even surpasses that of the large-scale model-based method on large-scale datasets.</p>
<hr>
<h2 id="RankMixup-Ranking-Based-Mixup-Training-for-Network-Calibration"><a href="#RankMixup-Ranking-Based-Mixup-Training-for-Network-Calibration" class="headerlink" title="RankMixup: Ranking-Based Mixup Training for Network Calibration"></a>RankMixup: Ranking-Based Mixup Training for Network Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11990">http://arxiv.org/abs/2308.11990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jongyoun Noh, Hyekang Park, Junghyup Lee, Bumsub Ham</li>
</ul>
<p>Abstract:<br>Network calibration aims to accurately estimate the level of confidences, which is particularly important for employing deep neural networks in real-world systems. Recent approaches leverage mixup to calibrate the network’s predictions during training. However, they do not consider the problem that mixtures of labels in mixup may not accurately represent the actual distribution of augmented samples. In this paper, we present RankMixup, a novel mixup-based framework alleviating the problem of the mixture of labels for network calibration. To this end, we propose to use an ordinal ranking relationship between raw and mixup-augmented samples as an alternative supervisory signal to the label mixtures for network calibration. We hypothesize that the network should estimate a higher level of confidence for the raw samples than the augmented ones (Fig.1). To implement this idea, we introduce a mixup-based ranking loss (MRL) that encourages lower confidences for augmented samples compared to raw ones, maintaining the ranking relationship. We also propose to leverage the ranking relationship among multiple mixup-augmented samples to further improve the calibration capability. Augmented samples with larger mixing coefficients are expected to have higher confidences and vice versa (Fig.1). That is, the order of confidences should be aligned with that of mixing coefficients. To this end, we introduce a novel loss, M-NDCG, in order to reduce the number of misaligned pairs of the coefficients and confidences. Extensive experimental results on standard benchmarks for network calibration demonstrate the effectiveness of RankMixup.</p>
<hr>
<h2 id="Multi-Modal-Multi-Task-3MT-Road-Segmentation"><a href="#Multi-Modal-Multi-Task-3MT-Road-Segmentation" class="headerlink" title="Multi-Modal Multi-Task (3MT) Road Segmentation"></a>Multi-Modal Multi-Task (3MT) Road Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11983">http://arxiv.org/abs/2308.11983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/erkanmilli/3mt-roadseg">https://github.com/erkanmilli/3mt-roadseg</a></li>
<li>paper_authors: Erkan Milli, Özgür Erkent, Asım Egemen Yılmaz</li>
</ul>
<p>Abstract:<br>Multi-modal systems have the capacity of producing more reliable results than systems with a single modality in road detection due to perceiving different aspects of the scene. We focus on using raw sensor inputs instead of, as it is typically done in many SOTA works, leveraging architectures that require high pre-processing costs such as surface normals or dense depth predictions. By using raw sensor inputs, we aim to utilize a low-cost model thatminimizes both the pre-processing andmodel computation costs. This study presents a cost-effective and highly accurate solution for road segmentation by integrating data from multiple sensorswithin a multi-task learning architecture.Afusion architecture is proposed in which RGB and LiDAR depth images constitute the inputs of the network. Another contribution of this study is to use IMU&#x2F;GNSS (inertial measurement unit&#x2F;global navigation satellite system) inertial navigation system whose data is collected synchronously and calibrated with a LiDAR-camera to compute aggregated dense LiDAR depth images. It has been demonstrated by experiments on the KITTI dataset that the proposed method offers fast and high-performance solutions. We have also shown the performance of our method on Cityscapes where raw LiDAR data is not available. The segmentation results obtained for both full and half resolution images are competitive with existing methods. Therefore, we conclude that our method is not dependent only on raw LiDAR data; rather, it can be used with different sensor modalities. The inference times obtained in all experiments are very promising for real-time experiments.</p>
<hr>
<h2 id="Rotation-Invariant-Completion-Network"><a href="#Rotation-Invariant-Completion-Network" class="headerlink" title="Rotation-Invariant Completion Network"></a>Rotation-Invariant Completion Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11979">http://arxiv.org/abs/2308.11979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Chen, Pengcheng Shi</li>
</ul>
<p>Abstract:<br>Real-world point clouds usually suffer from incompleteness and display different poses. While current point cloud completion methods excel in reproducing complete point clouds with consistent poses as seen in the training set, their performance tends to be unsatisfactory when handling point clouds with diverse poses. We propose a network named Rotation-Invariant Completion Network (RICNet), which consists of two parts: a Dual Pipeline Completion Network (DPCNet) and an enhancing module. Firstly, DPCNet generates a coarse complete point cloud. The feature extraction module of DPCNet can extract consistent features, no matter if the input point cloud has undergone rotation or translation. Subsequently, the enhancing module refines the fine-grained details of the final generated point cloud. RICNet achieves better rotation invariance in feature extraction and incorporates structural relationships in man-made objects. To assess the performance of RICNet and existing methods on point clouds with various poses, we applied random transformations to the point clouds in the MVP dataset and conducted experiments on them. Our experiments demonstrate that RICNet exhibits superior completion performance compared to existing methods.</p>
<hr>
<h2 id="Blending-NeRF-Text-Driven-Localized-Editing-in-Neural-Radiance-Fields"><a href="#Blending-NeRF-Text-Driven-Localized-Editing-in-Neural-Radiance-Fields" class="headerlink" title="Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields"></a>Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11974">http://arxiv.org/abs/2308.11974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeonseop Song, Seokhun Choi, Hoseok Do, Chul Lee, Taehyeong Kim</li>
</ul>
<p>Abstract:<br>Text-driven localized editing of 3D objects is particularly difficult as locally mixing the original 3D object with the intended new object and style effects without distorting the object’s form is not a straightforward process. To address this issue, we propose a novel NeRF-based model, Blending-NeRF, which consists of two NeRF networks: pretrained NeRF and editable NeRF. Additionally, we introduce new blending operations that allow Blending-NeRF to properly edit target regions which are localized by text. By using a pretrained vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects with varying colors and densities, modify textures, and remove parts of the original object. Our extensive experiments demonstrate that Blending-NeRF produces naturally and locally edited 3D objects from various text prompts.</p>
<hr>
<h2 id="EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE"><a href="#EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE" class="headerlink" title="EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE"></a>EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11971">http://arxiv.org/abs/2308.11971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Chen, Longteng Guo, Jia Sun, Shuai Shao, Zehuan Yuan, Liang Lin, Dongyu Zhang</li>
</ul>
<p>Abstract:<br>Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy to scale up, enabling better downstream performance with fewer resources and faster training speed. Despite its simplicity, EVE achieves state-of-the-art performance on various vision-language downstream tasks, including visual question answering, visual reasoning, and image-text retrieval.</p>
<hr>
<h2 id="Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification"><a href="#Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification" class="headerlink" title="Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification"></a>Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11969">http://arxiv.org/abs/2308.11969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Lambert, Pauline Roca, Florence Forbes, Senan Doyle, Michel Dojat</li>
</ul>
<p>Abstract:<br>The burden of liver tumors is important, ranking as the fourth leading cause of cancer mortality. In case of hepatocellular carcinoma (HCC), the delineation of liver and tumor on contrast-enhanced magnetic resonance imaging (CE-MRI) is performed to guide the treatment strategy. As this task is time-consuming, needs high expertise and could be subject to inter-observer variability there is a strong need for automatic tools. However, challenges arise from the lack of available training data, as well as the high variability in terms of image resolution and MRI sequence. In this work we propose to compare two different pipelines based on anisotropic models to obtain the segmentation of the liver and tumors. The first pipeline corresponds to a baseline multi-class model that performs the simultaneous segmentation of the liver and tumor classes. In the second approach, we train two distinct binary models, one segmenting the liver only and the other the tumors. Our results show that both pipelines exhibit different strengths and weaknesses. Moreover we propose an uncertainty quantification strategy allowing the identification of potential false positive tumor lesions. Both solutions were submitted to the MICCAI 2023 Atlas challenge regarding liver and tumor segmentation.</p>
<hr>
<h2 id="Gaze-Estimation-on-Spresense"><a href="#Gaze-Estimation-on-Spresense" class="headerlink" title="Gaze Estimation on Spresense"></a>Gaze Estimation on Spresense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12313">http://arxiv.org/abs/2308.12313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Ruegg, Pietro Bonazzi, Andrea Ronco</li>
</ul>
<p>Abstract:<br>Gaze estimation is a valuable technology with numerous applications in fields such as human-computer interaction, virtual reality, and medicine. This report presents the implementation of a gaze estimation system using the Sony Spresense microcontroller board and explores its performance in latency, MAC&#x2F;cycle, and power consumption. The report also provides insights into the system’s architecture, including the gaze estimation model used. Additionally, a demonstration of the system is presented, showcasing its functionality and performance. Our lightweight model TinyTrackerS is a mere 169Kb in size, using 85.8k parameters and runs on the Spresense platform at 3 FPS.</p>
<hr>
<h2 id="Pose-Modulated-Avatars-from-Video"><a href="#Pose-Modulated-Avatars-from-Video" class="headerlink" title="Pose Modulated Avatars from Video"></a>Pose Modulated Avatars from Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11951">http://arxiv.org/abs/2308.11951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunjin Song, Bastian Wandt, Helge Rhodin</li>
</ul>
<p>Abstract:<br>It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state-of-the-art methods in terms of preserving details and generalization capabilities.</p>
<hr>
<h2 id="High-quality-Image-Dehazing-with-Diffusion-Model"><a href="#High-quality-Image-Dehazing-with-Diffusion-Model" class="headerlink" title="High-quality Image Dehazing with Diffusion Model"></a>High-quality Image Dehazing with Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11949">http://arxiv.org/abs/2308.11949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hu Yu, Jie Huang, Kaiwen Zheng, Man Zhou, Feng Zhao</li>
</ul>
<p>Abstract:<br>Image dehazing is quite challenging in dense-haze scenarios, where quite less original information remains in the hazy image. Though previous methods have made marvelous progress, they still suffer from information loss in content and color in dense-haze scenarios. The recently emerged Denoising Diffusion Probabilistic Model (DDPM) exhibits strong generation ability, showing potential for solving this problem. However, DDPM fails to consider the physics property of dehazing task, limiting its information completion capacity. In this work, we propose DehazeDDPM: A DDPM-based and physics-aware image dehazing framework that applies to complex hazy scenarios. Specifically, DehazeDDPM works in two stages. The former stage physically models the dehazing task with the Atmospheric Scattering Model (ASM), pulling the distribution closer to the clear data and endowing DehazeDDPM with fog-aware ability. The latter stage exploits the strong generation ability of DDPM to compensate for the haze-induced huge information loss, by working in conjunction with the physical modelling. Extensive experiments demonstrate that our method attains state-of-the-art performance on both synthetic and real-world hazy datasets.</p>
<hr>
<h2 id="Efficient-Transfer-Learning-in-Diffusion-Models-via-Adversarial-Noise"><a href="#Efficient-Transfer-Learning-in-Diffusion-Models-via-Adversarial-Noise" class="headerlink" title="Efficient Transfer Learning in Diffusion Models via Adversarial Noise"></a>Efficient Transfer Learning in Diffusion Models via Adversarial Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11948">http://arxiv.org/abs/2308.11948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiyu Wang, Baijiong Lin, Daochang Liu, Chang Xu</li>
</ul>
<p>Abstract:<br>Diffusion Probabilistic Models (DPMs) have demonstrated substantial promise in image generation tasks but heavily rely on the availability of large amounts of training data. Previous works, like GANs, have tackled the limited data problem by transferring pre-trained models learned with sufficient data. However, those methods are hard to be utilized in DPMs since the distinct differences between DPM-based and GAN-based methods, showing in the unique iterative denoising process integral and the need for many timesteps with no-targeted noise in DPMs. In this paper, we propose a novel DPMs-based transfer learning method, TAN, to address the limited data problem. It includes two strategies: similarity-guided training, which boosts transfer with a classifier, and adversarial noise selection which adaptive chooses targeted noise based on the input image. Extensive experiments in the context of few-shot image generation tasks demonstrate that our method is not only efficient but also excels in terms of image quality and diversity when compared to existing GAN-based and DDPM-based methods.</p>
<hr>
<h2 id="LongDanceDiff-Long-term-Dance-Generation-with-Conditional-Diffusion-Model"><a href="#LongDanceDiff-Long-term-Dance-Generation-with-Conditional-Diffusion-Model" class="headerlink" title="LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model"></a>LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11945">http://arxiv.org/abs/2308.11945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siqi Yang, Zejun Yang, Zhisheng Wang</li>
</ul>
<p>Abstract:<br>Dancing with music is always an essential human art form to express emotion. Due to the high temporal-spacial complexity, long-term 3D realist dance generation synchronized with music is challenging. Existing methods suffer from the freezing problem when generating long-term dances due to error accumulation and training-inference discrepancy. To address this, we design a conditional diffusion model, LongDanceDiff, for this sequence-to-sequence long-term dance generation, addressing the challenges of temporal coherency and spatial constraint. LongDanceDiff contains a transformer-based diffusion model, where the input is a concatenation of music, past motions, and noised future motions. This partial noising strategy leverages the full-attention mechanism and learns the dependencies among music and past motions. To enhance the diversity of generated dance motions and mitigate the freezing problem, we introduce a mutual information minimization objective that regularizes the dependency between past and future motions. We also address common visual quality issues in dance generation, such as foot sliding and unsmooth motion, by incorporating spatial constraints through a Global-Trajectory Modulation (GTM) layer and motion perceptual losses, thereby improving the smoothness and naturalness of motion generation. Extensive experiments demonstrate a significant improvement in our approach over the existing state-of-the-art methods. We plan to release our codes and models soon.</p>
<hr>
<h2 id="Boosting-Diffusion-Models-with-an-Adaptive-Momentum-Sampler"><a href="#Boosting-Diffusion-Models-with-an-Adaptive-Momentum-Sampler" class="headerlink" title="Boosting Diffusion Models with an Adaptive Momentum Sampler"></a>Boosting Diffusion Models with an Adaptive Momentum Sampler</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11941">http://arxiv.org/abs/2308.11941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiyu Wang, Anh-Dung Dinh, Daochang Liu, Chang Xu</li>
</ul>
<p>Abstract:<br>Diffusion probabilistic models (DPMs) have been shown to generate high-quality images without the need for delicate adversarial training. However, the current sampling process in DPMs is prone to violent shaking. In this paper, we present a novel reverse sampler for DPMs inspired by the widely-used Adam optimizer. Our proposed sampler can be readily applied to a pre-trained diffusion model, utilizing momentum mechanisms and adaptive updating to smooth the reverse sampling process and ensure stable generation, resulting in outputs of enhanced quality. By implicitly reusing update directions from early steps, our proposed sampler achieves a better balance between high-level semantics and low-level details. Additionally, this sampler is flexible and can be easily integrated into pre-trained DPMs regardless of the sampler used during training. Our experimental results on multiple benchmarks demonstrate that our proposed reverse sampler yields remarkable improvements over different baselines. We will make the source code available.</p>
<hr>
<h2 id="Learning-Bottleneck-Transformer-for-Event-Image-Voxel-Feature-Fusion-based-Classification"><a href="#Learning-Bottleneck-Transformer-for-Event-Image-Voxel-Feature-Fusion-based-Classification" class="headerlink" title="Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification"></a>Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11937">http://arxiv.org/abs/2308.11937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/event-ahu/efv_event_classification">https://github.com/event-ahu/efv_event_classification</a></li>
<li>paper_authors: Chengguo Yuan, Yu Jin, Zongzhen Wu, Fanting Wei, Yangzirui Wang, Lan Chen, Xiao Wang</li>
</ul>
<p>Abstract:<br>Recognizing target objects using an event-based camera draws more and more attention in recent years. Existing works usually represent the event streams into point-cloud, voxel, image, etc, and learn the feature representations using various deep neural networks. Their final results may be limited by the following factors: monotonous modal expressions and the design of the network structure. To address the aforementioned challenges, this paper proposes a novel dual-stream framework for event representation, extraction, and fusion. This framework simultaneously models two common representations: event images and event voxels. By utilizing Transformer and Structured Graph Neural Network (GNN) architectures, spatial information and three-dimensional stereo information can be learned separately. Additionally, a bottleneck Transformer is introduced to facilitate the fusion of the dual-stream information. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance on two widely used event-based classification datasets. The source code of this work is available at: \url{<a target="_blank" rel="noopener" href="https://github.com/Event-AHU/EFV_event_classification%7D">https://github.com/Event-AHU/EFV_event_classification}</a></p>
<hr>
<h2 id="Synergistic-Multiscale-Detail-Refinement-via-Intrinsic-Supervision-for-Underwater-Image-Enhancement"><a href="#Synergistic-Multiscale-Detail-Refinement-via-Intrinsic-Supervision-for-Underwater-Image-Enhancement" class="headerlink" title="Synergistic Multiscale Detail Refinement via Intrinsic Supervision for Underwater Image Enhancement"></a>Synergistic Multiscale Detail Refinement via Intrinsic Supervision for Underwater Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11932">http://arxiv.org/abs/2308.11932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dehuan Zhang, Jingchun Zhou, Weishi Zhang, ChunLe Guo, Chongyi Li</li>
</ul>
<p>Abstract:<br>Visual restoration of underwater scenes is crucial for visual tasks, and avoiding interference from underwater media has become a prominent concern. In this work, we present a synergistic multiscale detail refinement via intrinsic supervision (SMDR-IS) to recover underwater scene details. The low-degradation stage provides multiscale detail for original stage, which achieves synergistic multiscale detail refinement through feature propagation via the adaptive selective intrinsic supervised feature module (ASISF), which achieves synergistic multiscale detail refinement. ASISF is developed using intrinsic supervision to precisely control and guide feature transmission in the multi-degradation stages. ASISF improves the multiscale detail refinement while reducing interference from irrelevant scene information from the low-degradation stage. Additionally, within the multi-degradation encoder-decoder of SMDR-IS, we introduce a bifocal intrinsic-context attention module (BICA). This module is designed to effectively leverage multi-scale scene information found in images, using intrinsic supervision principles as its foundation. BICA facilitates the guidance of higher-resolution spaces by leveraging lower-resolution spaces, considering the significant dependency of underwater image restoration on spatial contextual relationships. During the training process, the network gains advantages from the integration of a multi-degradation loss function. This function serves as a constraint, enabling the network to effectively exploit information across various scales. When compared with state-of-the-art methods, SMDR-IS demonstrates its outstanding performance. Code will be made publicly available.</p>
<hr>
<h2 id="OFVL-MS-Once-for-Visual-Localization-across-Multiple-Indoor-Scenes"><a href="#OFVL-MS-Once-for-Visual-Localization-across-Multiple-Indoor-Scenes" class="headerlink" title="OFVL-MS: Once for Visual Localization across Multiple Indoor Scenes"></a>OFVL-MS: Once for Visual Localization across Multiple Indoor Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11928">http://arxiv.org/abs/2308.11928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mooncake199809/ufvl-net">https://github.com/mooncake199809/ufvl-net</a></li>
<li>paper_authors: Tao Xie, Kun Dai, Siyi Lu, Ke Wang, Zhiqiang Jiang, Jinghan Gao, Dedong Liu, Jie Xu, Lijun Zhao, Ruifeng Li</li>
</ul>
<p>Abstract:<br>In this work, we seek to predict camera poses across scenes with a multi-task learning manner, where we view the localization of each scene as a new task. We propose OFVL-MS, a unified framework that dispenses with the traditional practice of training a model for each individual scene and relieves gradient conflict induced by optimizing multiple scenes collectively, enabling efficient storage yet precise visual localization for all scenes. Technically, in the forward pass of OFVL-MS, we design a layer-adaptive sharing policy with a learnable score for each layer to automatically determine whether the layer is shared or not. Such sharing policy empowers us to acquire task-shared parameters for a reduction of storage cost and task-specific parameters for learning scene-related features to alleviate gradient conflict. In the backward pass of OFVL-MS, we introduce a gradient normalization algorithm that homogenizes the gradient magnitude of the task-shared parameters so that all tasks converge at the same pace. Furthermore, a sparse penalty loss is applied on the learnable scores to facilitate parameter sharing for all tasks without performance degradation. We conduct comprehensive experiments on multiple benchmarks and our new released indoor dataset LIVL, showing that OFVL-MS families significantly outperform the state-of-the-arts with fewer parameters. We also verify that OFVL-MS can generalize to a new scene with much few parameters while gaining superior localization performance.</p>
<hr>
<h2 id="Recovering-a-Molecule’s-3D-Dynamics-from-Liquid-phase-Electron-Microscopy-Movies"><a href="#Recovering-a-Molecule’s-3D-Dynamics-from-Liquid-phase-Electron-Microscopy-Movies" class="headerlink" title="Recovering a Molecule’s 3D Dynamics from Liquid-phase Electron Microscopy Movies"></a>Recovering a Molecule’s 3D Dynamics from Liquid-phase Electron Microscopy Movies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11927">http://arxiv.org/abs/2308.11927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Ye, Yuhang Wang, Hong Zhang, Yiqin Gao, Huan Wang, He Sun</li>
</ul>
<p>Abstract:<br>The dynamics of biomolecules are crucial for our understanding of their functioning in living systems. However, current 3D imaging techniques, such as cryogenic electron microscopy (cryo-EM), require freezing the sample, which limits the observation of their conformational changes in real time. The innovative liquid-phase electron microscopy (liquid-phase EM) technique allows molecules to be placed in the native liquid environment, providing a unique opportunity to observe their dynamics. In this paper, we propose TEMPOR, a Temporal Electron MicroscoPy Object Reconstruction algorithm for liquid-phase EM that leverages an implicit neural representation (INR) and a dynamical variational auto-encoder (DVAE) to recover time series of molecular structures. We demonstrate its advantages in recovering different motion dynamics from two simulated datasets, 7bcq and Cas9. To our knowledge, our work is the first attempt to directly recover 3D structures of a temporally-varying particle from liquid-phase EM movies. It provides a promising new approach for studying molecules’ 3D dynamics in structural biology.</p>
<hr>
<h2 id="MixNet-Toward-Accurate-Detection-of-Challenging-Scene-Text-in-the-Wild"><a href="#MixNet-Toward-Accurate-Detection-of-Challenging-Scene-Text-in-the-Wild" class="headerlink" title="MixNet: Toward Accurate Detection of Challenging Scene Text in the Wild"></a>MixNet: Toward Accurate Detection of Challenging Scene Text in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12817">http://arxiv.org/abs/2308.12817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Xiang Zeng, Jun-Wei Hsieh, Xin Li, Ming-Ching Chang</li>
</ul>
<p>Abstract:<br>Detecting small scene text instances in the wild is particularly challenging, where the influence of irregular positions and nonideal lighting often leads to detection errors. We present MixNet, a hybrid architecture that combines the strengths of CNNs and Transformers, capable of accurately detecting small text from challenging natural scenes, regardless of the orientations, styles, and lighting conditions. MixNet incorporates two key modules: (1) the Feature Shuffle Network (FSNet) to serve as the backbone and (2) the Central Transformer Block (CTBlock) to exploit the 1D manifold constraint of the scene text. We first introduce a novel feature shuffling strategy in FSNet to facilitate the exchange of features across multiple scales, generating high-resolution features superior to popular ResNet and HRNet. The FSNet backbone has achieved significant improvements over many existing text detection methods, including PAN, DB, and FAST. Then we design a complementary CTBlock to leverage center line based features similar to the medial axis of text regions and show that it can outperform contour-based approaches in challenging cases when small scene texts appear closely. Extensive experimental results show that MixNet, which mixes FSNet with CTBlock, achieves state-of-the-art results on multiple scene text detection datasets.</p>
<hr>
<h2 id="Concept-Bottleneck-with-Visual-Concept-Filtering-for-Explainable-Medical-Image-Classification"><a href="#Concept-Bottleneck-with-Visual-Concept-Filtering-for-Explainable-Medical-Image-Classification" class="headerlink" title="Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification"></a>Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11920">http://arxiv.org/abs/2308.11920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Injae Kim, Jongha Kim, Joonmyung Choi, Hyunwoo J. Kim</li>
</ul>
<p>Abstract:<br>Interpretability is a crucial factor in building reliable models for various medical applications. Concept Bottleneck Models (CBMs) enable interpretable image classification by utilizing human-understandable concepts as intermediate targets. Unlike conventional methods that require extensive human labor to construct the concept set, recent works leveraging Large Language Models (LLMs) for generating concepts made automatic concept generation possible. However, those methods do not consider whether a concept is visually relevant or not, which is an important factor in computing meaningful concept scores. Therefore, we propose a visual activation score that measures whether the concept contains visual cues or not, which can be easily computed with unlabeled image data. Computed visual activation scores are then used to filter out the less visible concepts, thus resulting in a final concept set with visually meaningful concepts. Our experimental results show that adopting the proposed visual activation score for concept filtering consistently boosts performance compared to the baseline. Moreover, qualitative analyses also validate that visually relevant concepts are successfully selected with the visual activation score.</p>
<hr>
<h2 id="AMSP-UOD-When-Vortex-Convolution-and-Stochastic-Perturbation-Meet-Underwater-Object-Detection"><a href="#AMSP-UOD-When-Vortex-Convolution-and-Stochastic-Perturbation-Meet-Underwater-Object-Detection" class="headerlink" title="AMSP-UOD: When Vortex Convolution and Stochastic Perturbation Meet Underwater Object Detection"></a>AMSP-UOD: When Vortex Convolution and Stochastic Perturbation Meet Underwater Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11918">http://arxiv.org/abs/2308.11918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingchun Zhou, Zongxin He, Kin-Man Lam, Yudong Wang, Weishi Zhang, ChunLe Guo, Chongyi Li</li>
</ul>
<p>Abstract:<br>In this paper, we present a novel Amplitude-Modulated Stochastic Perturbation and Vortex Convolutional Network, AMSP-UOD, designed for underwater object detection. AMSP-UOD specifically addresses the impact of non-ideal imaging factors on detection accuracy in complex underwater environments. To mitigate the influence of noise on object detection performance, we propose AMSP Vortex Convolution (AMSP-VConv) to disrupt the noise distribution, enhance feature extraction capabilities, effectively reduce parameters, and improve network robustness. We design the Feature Association Decoupling Cross Stage Partial (FAD-CSP) module, which strengthens the association of long and short-range features, improving the network performance in complex underwater environments. Additionally, our sophisticated post-processing method, based on non-maximum suppression with aspect-ratio similarity thresholds, optimizes detection in dense scenes, such as waterweed and schools of fish, improving object detection accuracy. Extensive experiments on the URPC and RUOD datasets demonstrate that our method outperforms existing state-of-the-art methods in terms of accuracy and noise immunity. AMSP-UOD proposes an innovative solution with the potential for real-world applications. Code will be made publicly available.</p>
<hr>
<h2 id="LFS-GAN-Lifelong-Few-Shot-Image-Generation"><a href="#LFS-GAN-Lifelong-Few-Shot-Image-Generation" class="headerlink" title="LFS-GAN: Lifelong Few-Shot Image Generation"></a>LFS-GAN: Lifelong Few-Shot Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11917">http://arxiv.org/abs/2308.11917</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jjuon/lfs-gan">https://github.com/jjuon/lfs-gan</a></li>
<li>paper_authors: Juwon Seo, Ji-Su Kang, Gyeong-Moon Park</li>
</ul>
<p>Abstract:<br>We address a challenging lifelong few-shot image generation task for the first time. In this situation, a generative model learns a sequence of tasks using only a few samples per task. Consequently, the learned model encounters both catastrophic forgetting and overfitting problems at a time. Existing studies on lifelong GANs have proposed modulation-based methods to prevent catastrophic forgetting. However, they require considerable additional parameters and cannot generate high-fidelity and diverse images from limited data. On the other hand, the existing few-shot GANs suffer from severe catastrophic forgetting when learning multiple tasks. To alleviate these issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can generate high-quality and diverse images in lifelong few-shot image generation task. Our proposed framework learns each task using an efficient task-specific modulator - Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and has a rich representation ability due to its unique reconstruction technique. Furthermore, we propose a novel mode seeking loss to improve the diversity of our model in low-data circumstances. Extensive experiments demonstrate that the proposed LFS-GAN can generate high-fidelity and diverse images without any forgetting and mode collapse in various domains, achieving state-of-the-art in lifelong few-shot image generation task. Surprisingly, we find that our LFS-GAN even outperforms the existing few-shot GANs in the few-shot image generation task. The code is available at Github.</p>
<hr>
<h2 id="Semantic-Aware-Implicit-Template-Learning-via-Part-Deformation-Consistency"><a href="#Semantic-Aware-Implicit-Template-Learning-via-Part-Deformation-Consistency" class="headerlink" title="Semantic-Aware Implicit Template Learning via Part Deformation Consistency"></a>Semantic-Aware Implicit Template Learning via Part Deformation Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11916">http://arxiv.org/abs/2308.11916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sihyeon Kim, Minseok Joo, Jaewon Lee, Juyeon Ko, Juhan Cha, Hyunwoo J. Kim</li>
</ul>
<p>Abstract:<br>Learning implicit templates as neural fields has recently shown impressive performance in unsupervised shape correspondence. Despite the success, we observe current approaches, which solely rely on geometric information, often learn suboptimal deformation across generic object shapes, which have high structural variability. In this paper, we highlight the importance of part deformation consistency and propose a semantic-aware implicit template learning framework to enable semantically plausible deformation. By leveraging semantic prior from a self-supervised feature extractor, we suggest local conditioning with novel semantic-aware deformation code and deformation consistency regularizations regarding part deformation, global deformation, and global scaling. Our extensive experiments demonstrate the superiority of the proposed method over baselines in various tasks: keypoint transfer, part label transfer, and texture transfer. More interestingly, our framework shows a larger performance gain under more challenging settings. We also provide qualitative analyses to validate the effectiveness of semantic-aware deformation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/mlvlab/PDC">https://github.com/mlvlab/PDC</a>.</p>
<hr>
<h2 id="ACLS-Adaptive-and-Conditional-Label-Smoothing-for-Network-Calibration"><a href="#ACLS-Adaptive-and-Conditional-Label-Smoothing-for-Network-Calibration" class="headerlink" title="ACLS: Adaptive and Conditional Label Smoothing for Network Calibration"></a>ACLS: Adaptive and Conditional Label Smoothing for Network Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11911">http://arxiv.org/abs/2308.11911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyekang Park, Jongyoun Noh, Youngmin Oh, Donghyeon Baek, Bumsub Ham</li>
</ul>
<p>Abstract:<br>We address the problem of network calibration adjusting miscalibrated confidences of deep neural networks. Many approaches to network calibration adopt a regularization-based method that exploits a regularization term to smooth the miscalibrated confidences. Although these approaches have shown the effectiveness on calibrating the networks, there is still a lack of understanding on the underlying principles of regularization in terms of network calibration. We present in this paper an in-depth analysis of existing regularization-based methods, providing a better understanding on how they affect to network calibration. Specifically, we have observed that 1) the regularization-based methods can be interpreted as variants of label smoothing, and 2) they do not always behave desirably. Based on the analysis, we introduce a novel loss function, dubbed ACLS, that unifies the merits of existing regularization methods, while avoiding the limitations. We show extensive experimental results for image classification and semantic segmentation on standard benchmarks, including CIFAR10, Tiny-ImageNet, ImageNet, and PASCAL VOC, demonstrating the effectiveness of our loss function.</p>
<hr>
<h2 id="Edge-aware-Hard-Clustering-Graph-Pooling-for-Brain-Imaging-Data"><a href="#Edge-aware-Hard-Clustering-Graph-Pooling-for-Brain-Imaging-Data" class="headerlink" title="Edge-aware Hard Clustering Graph Pooling for Brain Imaging Data"></a>Edge-aware Hard Clustering Graph Pooling for Brain Imaging Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11909">http://arxiv.org/abs/2308.11909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Zhu, Jiayi Zhu, Lijuan Zhang, Xi Wu, Shuqi Yang, Ping Liang, Honghan Chen, Ying Tan</li>
</ul>
<p>Abstract:<br>Graph Convolutional Networks (GCNs) can capture non-Euclidean spatial dependence between different brain regions, and the graph pooling operator in GCNs is key to enhancing the representation learning capability and acquiring abnormal brain maps. However, the majority of existing research designs graph pooling operators only from the perspective of nodes while disregarding the original edge features, in a way that not only confines graph pooling application scenarios, but also diminishes its ability to capture critical substructures. In this study, a clustering graph pooling method that first supports multidimensional edge features, called Edge-aware hard clustering graph pooling (EHCPool), is developed. EHCPool proposes the first ‘Edge-to-node’ score evaluation criterion based on edge features to assess node feature significance. To more effectively capture the critical subgraphs, a novel Iteration n-top strategy is further designed to adaptively learn sparse hard clustering assignments for graphs. Subsequently, an innovative N-E Aggregation strategy is presented to aggregate node and edge feature information in each independent subgraph. The proposed model was evaluated on multi-site brain imaging public datasets and yielded state-of-the-art performance. We believe this method is the first deep learning tool with the potential to probe different types of abnormal functional brain networks from data-driven perspective.</p>
<hr>
<h2 id="Rethinking-Data-Perturbation-and-Model-Stabilization-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Rethinking-Data-Perturbation-and-Model-Stabilization-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation"></a>Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11903">http://arxiv.org/abs/2308.11903</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenzhao/dpms">https://github.com/zhenzhao/dpms</a></li>
<li>paper_authors: Zhen Zhao, Ye Liu, Meng Zhao, Di Yin, Yixuan Yuan, Luping Zhou</li>
</ul>
<p>Abstract:<br>Studies on semi-supervised medical image segmentation (SSMIS) have seen fast progress recently. Due to the limited labelled data, SSMIS methods mainly focus on effectively leveraging unlabeled data to enhance the segmentation performance. However, despite their promising performance, current state-of-the-art methods often prioritize integrating complex techniques and loss terms rather than addressing the core challenges of semi-supervised scenarios directly. We argue that the key to SSMIS lies in generating substantial and appropriate prediction disagreement on unlabeled data. To this end, we emphasize the crutiality of data perturbation and model stabilization in semi-supervised segmentation, and propose a simple yet effective approach to boost SSMIS performance significantly, dubbed DPMS. Specifically, we first revisit SSMIS from three distinct perspectives: the data, the model, and the loss, and conduct a comprehensive study of corresponding strategies to examine their effectiveness. Based on these examinations, we then propose DPMS, which adopts a plain teacher-student framework with a standard supervised loss and unsupervised consistency loss. To produce appropriate prediction disagreements, DPMS perturbs the unlabeled data via strong augmentations to enlarge prediction disagreements considerably. On the other hand, using EMA teacher when strong augmentation is applied does not necessarily improve performance. DPMS further utilizes a forwarding-twice and momentum updating strategies for normalization statistics to stabilize the training on unlabeled data effectively. Despite its simplicity, DPMS can obtain new state-of-the-art performance on the public 2D ACDC and 3D LA datasets across various semi-supervised settings, e.g. obtaining a remarkable 22.62% improvement against previous SOTA on ACDC with 5% labels.</p>
<hr>
<h2 id="Camera-Driven-Representation-Learning-for-Unsupervised-Domain-Adaptive-Person-Re-identification"><a href="#Camera-Driven-Representation-Learning-for-Unsupervised-Domain-Adaptive-Person-Re-identification" class="headerlink" title="Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-identification"></a>Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11901">http://arxiv.org/abs/2308.11901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geon Lee, Sanghoon Lee, Dohyung Kim, Younghoon Shin, Yongsang Yoon, Bumsub Ham</li>
</ul>
<p>Abstract:<br>We present a novel unsupervised domain adaption method for person re-identification (reID) that generalizes a model trained on a labeled source domain to an unlabeled target domain. We introduce a camera-driven curriculum learning (CaCL) framework that leverages camera labels of person images to transfer knowledge from source to target domains progressively. To this end, we divide target domain dataset into multiple subsets based on the camera labels, and initially train our model with a single subset (i.e., images captured by a single camera). We then gradually exploit more subsets for training, according to a curriculum sequence obtained with a camera-driven scheduling rule. The scheduler considers maximum mean discrepancies (MMD) between each subset and the source domain dataset, such that the subset closer to the source domain is exploited earlier within the curriculum. For each curriculum sequence, we generate pseudo labels of person images in a target domain to train a reID model in a supervised way. We have observed that the pseudo labels are highly biased toward cameras, suggesting that person images obtained from the same camera are likely to have the same pseudo labels, even for different IDs. To address the camera bias problem, we also introduce a camera-diversity (CD) loss encouraging person images of the same pseudo label, but captured across various cameras, to involve more for discriminative feature learning, providing person representations robust to inter-camera variations. Experimental results on standard benchmarks, including real-to-real and synthetic-to-real scenarios, demonstrate the effectiveness of our framework.</p>
<hr>
<h2 id="HashReID-Dynamic-Network-with-Binary-Codes-for-Efficient-Person-Re-identification"><a href="#HashReID-Dynamic-Network-with-Binary-Codes-for-Efficient-Person-Re-identification" class="headerlink" title="HashReID: Dynamic Network with Binary Codes for Efficient Person Re-identification"></a>HashReID: Dynamic Network with Binary Codes for Efficient Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11900">http://arxiv.org/abs/2308.11900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshitij Nikhal, Yujunrong Ma, Shuvra S. Bhattacharyya, Benjamin S. Riggan</li>
</ul>
<p>Abstract:<br>Biometric applications, such as person re-identification (ReID), are often deployed on energy constrained devices. While recent ReID methods prioritize high retrieval performance, they often come with large computational costs and high search time, rendering them less practical in real-world settings. In this work, we propose an input-adaptive network with multiple exit blocks, that can terminate computation early if the retrieval is straightforward or noisy, saving a lot of computation. To assess the complexity of the input, we introduce a temporal-based classifier driven by a new training strategy. Furthermore, we adopt a binary hash code generation approach instead of relying on continuous-valued features, which significantly improves the search process by a factor of 20. To ensure similarity preservation, we utilize a new ranking regularizer that bridges the gap between continuous and binary features. Extensive analysis of our proposed method is conducted on three datasets: Market1501, MSMT17 (Multi-Scene Multi-Time), and the BGC1 (BRIAR Government Collection). Using our approach, more than 70% of the samples with compact hash codes exit early on the Market1501 dataset, saving 80% of the networks computational cost and improving over other hash-based methods by 60%. These results demonstrate a significant improvement over dynamic networks and showcase comparable accuracy performance to conventional ReID methods. Code will be made available.</p>
<hr>
<h2 id="Exploring-the-Optimization-Objective-of-One-Class-Classification-for-Anomaly-Detection"><a href="#Exploring-the-Optimization-Objective-of-One-Class-Classification-for-Anomaly-Detection" class="headerlink" title="Exploring the Optimization Objective of One-Class Classification for Anomaly Detection"></a>Exploring the Optimization Objective of One-Class Classification for Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11898">http://arxiv.org/abs/2308.11898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Gao, Huiyuan Luo, Fei Shen, Zhengtao Zhang</li>
</ul>
<p>Abstract:<br>One-class classification (OCC) is a longstanding method for anomaly detection. With the powerful representation capability of the pre-trained backbone, OCC methods have witnessed significant performance improvements. Typically, most of these OCC methods employ transfer learning to enhance the discriminative nature of the pre-trained backbone’s features, thus achieving remarkable efficacy. While most current approaches emphasize feature transfer strategies, we argue that the optimization objective space within OCC methods could also be an underlying critical factor influencing performance. In this work, we conducted a thorough investigation into the optimization objective of OCC. Through rigorous theoretical analysis and derivation, we unveil a key insights: any space with the suitable norm can serve as an equivalent substitute for the hypersphere center, without relying on the distribution assumption of training samples. Further, we provide guidelines for determining the feasible domain of norms for the OCC optimization objective. This novel insight sparks a simple and data-agnostic deep one-class classification method. Our method is straightforward, with a single 1x1 convolutional layer as a trainable projector and any space with suitable norm as the optimization objective. Extensive experiments validate the reliability and efficacy of our findings and the corresponding methodology, resulting in state-of-the-art performance in both one-class classification and industrial vision anomaly detection and segmentation tasks.</p>
<hr>
<h2 id="Age-Prediction-From-Face-Images-Via-Contrastive-Learning"><a href="#Age-Prediction-From-Face-Images-Via-Contrastive-Learning" class="headerlink" title="Age Prediction From Face Images Via Contrastive Learning"></a>Age Prediction From Face Images Via Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11896">http://arxiv.org/abs/2308.11896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeongnam Chae, Poulami Raha, Mijung Kim, Bjorn Stenger</li>
</ul>
<p>Abstract:<br>This paper presents a novel approach for accurately estimating age from face images, which overcomes the challenge of collecting a large dataset of individuals with the same identity at different ages. Instead, we leverage readily available face datasets of different people at different ages and aim to extract age-related features using contrastive learning. Our method emphasizes these relevant features while suppressing identity-related features using a combination of cosine similarity and triplet margin losses. We demonstrate the effectiveness of our proposed approach by achieving state-of-the-art performance on two public datasets, FG-NET and MORPH-II.</p>
<hr>
<h2 id="Does-Physical-Adversarial-Example-Really-Matter-to-Autonomous-Driving-Towards-System-Level-Effect-of-Adversarial-Object-Evasion-Attack"><a href="#Does-Physical-Adversarial-Example-Really-Matter-to-Autonomous-Driving-Towards-System-Level-Effect-of-Adversarial-Object-Evasion-Attack" class="headerlink" title="Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack"></a>Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11894">http://arxiv.org/abs/2308.11894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ningfei Wang, Yunpeng Luo, Takami Sato, Kaidi Xu, Qi Alfred Chen</li>
</ul>
<p>Abstract:<br>In autonomous driving (AD), accurate perception is indispensable to achieving safe and secure driving. Due to its safety-criticality, the security of AD perception has been widely studied. Among different attacks on AD perception, the physical adversarial object evasion attacks are especially severe. However, we find that all existing literature only evaluates their attack effect at the targeted AI component level but not at the system level, i.e., with the entire system semantics and context such as the full AD pipeline. Thereby, this raises a critical research question: can these existing researches effectively achieve system-level attack effects (e.g., traffic rule violations) in the real-world AD context? In this work, we conduct the first measurement study on whether and how effectively the existing designs can lead to system-level effects, especially for the STOP sign-evasion attacks due to their popularity and severity. Our evaluation results show that all the representative prior works cannot achieve any system-level effects. We observe two design limitations in the prior works: 1) physical model-inconsistent object size distribution in pixel sampling and 2) lack of vehicle plant model and AD system model consideration. Then, we propose SysAdv, a novel system-driven attack design in the AD context and our evaluation results show that the system-level effects can be significantly improved, i.e., the violation rate increases by around 70%.</p>
<hr>
<h2 id="A-Unified-Framework-for-3D-Point-Cloud-Visual-Grounding"><a href="#A-Unified-Framework-for-3D-Point-Cloud-Visual-Grounding" class="headerlink" title="A Unified Framework for 3D Point Cloud Visual Grounding"></a>A Unified Framework for 3D Point Cloud Visual Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11887">http://arxiv.org/abs/2308.11887</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leon1207/3dreftr">https://github.com/leon1207/3dreftr</a></li>
<li>paper_authors: Haojia Lin, Yongdong Luo, Xiawu Zheng, Lijiang Li, Fei Chao, Taisong Jin, Donghao Luo, Chengjie Wang, Yan Wang, Liujuan Cao</li>
</ul>
<p>Abstract:<br>3D point cloud visual grounding plays a critical role in 3D scene comprehension, encompassing 3D referring expression comprehension (3DREC) and segmentation (3DRES). We argue that 3DREC and 3DRES should be unified in one framework, which is also a natural progression in the community. To explain, 3DREC can help 3DRES locate the referent, while 3DRES can also facilitate 3DREC via more finegrained language-visual alignment. To achieve this, this paper takes the initiative step to integrate 3DREC and 3DRES into a unified framework, termed 3D Referring Transformer (3DRefTR). Its key idea is to build upon a mature 3DREC model and leverage ready query embeddings and visual tokens from the 3DREC model to construct a dedicated mask branch. Specially, we propose Superpoint Mask Branch, which serves a dual purpose: i) By leveraging the heterogeneous CPU-GPU parallelism, while the GPU is occupied generating visual tokens, the CPU concurrently produces superpoints, equivalently accomplishing the upsampling computation; ii) By harnessing on the inherent association between the superpoints and point cloud, it eliminates the heavy computational overhead on the high-resolution visual features for upsampling. This elegant design enables 3DRefTR to achieve both well-performing 3DRES and 3DREC capacities with only a 6% additional latency compared to the original 3DREC model. Empirical evaluations affirm the superiority of 3DRefTR. Specifically, on the ScanRefer dataset, 3DRefTR surpasses the state-of-the-art 3DRES method by 12.43% in mIoU and improves upon the SOTA 3DREC method by 0.6% <a href="mailto:&#x41;&#99;&#99;&#64;&#x30;&#46;&#x32;&#53;&#73;&#111;&#x55;">&#x41;&#99;&#99;&#64;&#x30;&#46;&#x32;&#53;&#73;&#111;&#x55;</a>.</p>
<hr>
<h2 id="SUMMIT-Source-Free-Adaptation-of-Uni-Modal-Models-to-Multi-Modal-Targets"><a href="#SUMMIT-Source-Free-Adaptation-of-Uni-Modal-Models-to-Multi-Modal-Targets" class="headerlink" title="SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets"></a>SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11880">http://arxiv.org/abs/2308.11880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csimo005/summit">https://github.com/csimo005/summit</a></li>
<li>paper_authors: Cody Simons, Dripta S. Raychaudhuri, Sk Miraj Ahmed, Suya You, Konstantinos Karydis, Amit K. Roy-Chowdhury</li>
</ul>
<p>Abstract:<br>Scene understanding using multi-modal data is necessary in many applications, e.g., autonomous navigation. To achieve this in a variety of situations, existing models must be able to adapt to shifting data distributions without arduous data annotation. Current approaches assume that the source data is available during adaptation and that the source consists of paired multi-modal data. Both these assumptions may be problematic for many applications. Source data may not be available due to privacy, security, or economic concerns. Assuming the existence of paired multi-modal data for training also entails significant data collection costs and fails to take advantage of widely available freely distributed pre-trained uni-modal models. In this work, we relax both of these assumptions by addressing the problem of adapting a set of models trained independently on uni-modal data to a target domain consisting of unlabeled multi-modal data, without having access to the original source dataset. Our proposed approach solves this problem through a switching framework which automatically chooses between two complementary methods of cross-modal pseudo-label fusion – agreement filtering and entropy weighting – based on the estimated domain gap. We demonstrate our work on the semantic segmentation problem. Experiments across seven challenging adaptation scenarios verify the efficacy of our approach, achieving results comparable to, and in some cases outperforming, methods which assume access to source data. Our method achieves an improvement in mIoU of up to 12% over competing baselines. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/csimo005/SUMMIT">https://github.com/csimo005/SUMMIT</a>.</p>
<hr>
<h2 id="Integrated-Image-and-Location-Analysis-for-Wound-Classification-A-Deep-Learning-Approach"><a href="#Integrated-Image-and-Location-Analysis-for-Wound-Classification-A-Deep-Learning-Approach" class="headerlink" title="Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach"></a>Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11877">http://arxiv.org/abs/2308.11877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Patel, Tirth Shah, Mrinal Kanti Dhar, Taiyu Zhang, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Zeyun Yu</li>
</ul>
<p>Abstract:<br>The global burden of acute and chronic wounds presents a compelling case for enhancing wound classification methods, a vital step in diagnosing and determining optimal treatments. Recognizing this need, we introduce an innovative multi-modal network based on a deep convolutional neural network for categorizing wounds into four categories: diabetic, pressure, surgical, and venous ulcers. Our multi-modal network uses wound images and their corresponding body locations for more precise classification. A unique aspect of our methodology is incorporating a body map system that facilitates accurate wound location tagging, improving upon traditional wound image classification techniques. A distinctive feature of our approach is the integration of models such as VGG16, ResNet152, and EfficientNet within a novel architecture. This architecture includes elements like spatial and channel-wise Squeeze-and-Excitation modules, Axial Attention, and an Adaptive Gated Multi-Layer Perceptron, providing a robust foundation for classification. Our multi-modal network was trained and evaluated on two distinct datasets comprising relevant images and corresponding location information. Notably, our proposed network outperformed traditional methods, reaching an accuracy range of 74.79% to 100% for Region of Interest (ROI) without location classifications, 73.98% to 100% for ROI with location classifications, and 78.10% to 100% for whole image classifications. This marks a significant enhancement over previously reported performance metrics in the literature. Our results indicate the potential of our multi-modal network as an effective decision-support tool for wound image classification, paving the way for its application in various clinical contexts.</p>
<hr>
<h2 id="Motion-to-Matching-A-Mixed-Paradigm-for-3D-Single-Object-Tracking"><a href="#Motion-to-Matching-A-Mixed-Paradigm-for-3D-Single-Object-Tracking" class="headerlink" title="Motion-to-Matching: A Mixed Paradigm for 3D Single Object Tracking"></a>Motion-to-Matching: A Mixed Paradigm for 3D Single Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11875">http://arxiv.org/abs/2308.11875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leozhiheng/mtm-tracker">https://github.com/leozhiheng/mtm-tracker</a></li>
<li>paper_authors: Zhiheng Li, Yu Lin, Yubo Cui, Shuo Li, Zheng Fang</li>
</ul>
<p>Abstract:<br>3D single object tracking with LiDAR points is an important task in the computer vision field. Previous methods usually adopt the matching-based or motion-centric paradigms to estimate the current target status. However, the former is sensitive to the similar distractors and the sparseness of point cloud due to relying on appearance matching, while the latter usually focuses on short-term motion clues (eg. two frames) and ignores the long-term motion pattern of target. To address these issues, we propose a mixed paradigm with two stages, named MTM-Tracker, which combines motion modeling with feature matching into a single network. Specifically, in the first stage, we exploit the continuous historical boxes as motion prior and propose an encoder-decoder structure to locate target coarsely. Then, in the second stage, we introduce a feature interaction module to extract motion-aware features from consecutive point clouds and match them to refine target movement as well as regress other target states. Extensive experiments validate that our paradigm achieves competitive performance on large-scale datasets (70.9% in KITTI and 51.70% in NuScenes). The code will be open soon at <a target="_blank" rel="noopener" href="https://github.com/LeoZhiheng/MTM-Tracker.git">https://github.com/LeoZhiheng/MTM-Tracker.git</a>.</p>
<hr>
<h2 id="Semi-Supervised-Learning-via-Weight-aware-Distillation-under-Class-Distribution-Mismatch"><a href="#Semi-Supervised-Learning-via-Weight-aware-Distillation-under-Class-Distribution-Mismatch" class="headerlink" title="Semi-Supervised Learning via Weight-aware Distillation under Class Distribution Mismatch"></a>Semi-Supervised Learning via Weight-aware Distillation under Class Distribution Mismatch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11874">http://arxiv.org/abs/2308.11874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pan Du, Suyun Zhao, Zisen Sheng, Cuiping Li, Hong Chen</li>
</ul>
<p>Abstract:<br>Semi-Supervised Learning (SSL) under class distribution mismatch aims to tackle a challenging problem wherein unlabeled data contain lots of unknown categories unseen in the labeled ones. In such mismatch scenarios, traditional SSL suffers severe performance damage due to the harmful invasion of the instances with unknown categories into the target classifier. In this study, by strict mathematical reasoning, we reveal that the SSL error under class distribution mismatch is composed of pseudo-labeling error and invasion error, both of which jointly bound the SSL population risk. To alleviate the SSL error, we propose a robust SSL framework called Weight-Aware Distillation (WAD) that, by weights, selectively transfers knowledge beneficial to the target task from unsupervised contrastive representation to the target classifier. Specifically, WAD captures adaptive weights and high-quality pseudo labels to target instances by exploring point mutual information (PMI) in representation space to maximize the role of unlabeled data and filter unknown categories. Theoretically, we prove that WAD has a tight upper bound of population risk under class distribution mismatch. Experimentally, extensive results demonstrate that WAD outperforms five state-of-the-art SSL approaches and one standard baseline on two benchmark datasets, CIFAR10 and CIFAR100, and an artificial cross-dataset. The code is available at <a target="_blank" rel="noopener" href="https://github.com/RUC-DWBI-ML/research/tree/main/WAD-master">https://github.com/RUC-DWBI-ML/research/tree/main/WAD-master</a>.</p>
<hr>
<h2 id="CoC-GAN-Employing-Context-Cluster-for-Unveiling-a-New-Pathway-in-Image-Generation"><a href="#CoC-GAN-Employing-Context-Cluster-for-Unveiling-a-New-Pathway-in-Image-Generation" class="headerlink" title="CoC-GAN: Employing Context Cluster for Unveiling a New Pathway in Image Generation"></a>CoC-GAN: Employing Context Cluster for Unveiling a New Pathway in Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11857">http://arxiv.org/abs/2308.11857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Wang, Yiming Huang, Ziyu Zhou</li>
</ul>
<p>Abstract:<br>Image generation tasks are traditionally undertaken using Convolutional Neural Networks (CNN) or Transformer architectures for feature aggregating and dispatching. Despite the frequent application of convolution and attention structures, these structures are not fundamentally required to solve the problem of instability and the lack of interpretability in image generation. In this paper, we propose a unique image generation process premised on the perspective of converting images into a set of point clouds. In other words, we interpret an image as a set of points. As such, our methodology leverages simple clustering methods named Context Clustering (CoC) to generate images from unordered point sets, which defies the convention of using convolution or attention mechanisms. Hence, we exclusively depend on this clustering technique, combined with the multi-layer perceptron (MLP) in a generative model. Furthermore, we implement the integration of a module termed the ‘Point Increaser’ for the model. This module is just an MLP tasked with generating additional points for clustering, which are subsequently integrated within the paradigm of the Generative Adversarial Network (GAN). We introduce this model with the novel structure as the Context Clustering Generative Adversarial Network (CoC-GAN), which offers a distinctive viewpoint in the domain of feature aggregating and dispatching. Empirical evaluations affirm that our CoC-GAN, devoid of convolution and attention mechanisms, exhibits outstanding performance. Its interpretability, endowed by the CoC module, also allows for visualization in our experiments. The promising results underscore the feasibility of our method and thus warrant future investigations of applying Context Clustering to more novel and interpretable image generation.</p>
<hr>
<h2 id="Compressed-Models-Decompress-Race-Biases-What-Quantized-Models-Forget-for-Fair-Face-Recognition"><a href="#Compressed-Models-Decompress-Race-Biases-What-Quantized-Models-Forget-for-Fair-Face-Recognition" class="headerlink" title="Compressed Models Decompress Race Biases: What Quantized Models Forget for Fair Face Recognition"></a>Compressed Models Decompress Race Biases: What Quantized Models Forget for Fair Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11840">http://arxiv.org/abs/2308.11840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro C. Neto, Eduarda Caldeira, Jaime S. Cardoso, Ana F. Sequeira</li>
</ul>
<p>Abstract:<br>With the ever-growing complexity of deep learning models for face recognition, it becomes hard to deploy these systems in real life. Researchers have two options: 1) use smaller models; 2) compress their current models. Since the usage of smaller models might lead to concerning biases, compression gains relevance. However, compressing might be also responsible for an increase in the bias of the final model. We investigate the overall performance, the performance on each ethnicity subgroup and the racial bias of a State-of-the-Art quantization approach when used with synthetic and real data. This analysis provides a few more details on potential benefits of performing quantization with synthetic data, for instance, the reduction of biases on the majority of test scenarios. We tested five distinct architectures and three different training datasets. The models were evaluated on a fourth dataset which was collected to infer and compare the performance of face recognition models on different ethnicity.</p>
<hr>
<h2 id="PatchBackdoor-Backdoor-Attack-against-Deep-Neural-Networks-without-Model-Modification"><a href="#PatchBackdoor-Backdoor-Attack-against-Deep-Neural-Networks-without-Model-Modification" class="headerlink" title="PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification"></a>PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11822">http://arxiv.org/abs/2308.11822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xaiveryuan/patchbackdoor">https://github.com/xaiveryuan/patchbackdoor</a></li>
<li>paper_authors: Yizhen Yuan, Rui Kong, Shenghao Xie, Yuanchun Li, Yunxin Liu</li>
</ul>
<p>Abstract:<br>Backdoor attack is a major threat to deep learning systems in safety-critical scenarios, which aims to trigger misbehavior of neural network models under attacker-controlled conditions. However, most backdoor attacks have to modify the neural network models through training with poisoned data and&#x2F;or direct model editing, which leads to a common but false belief that backdoor attack can be easily avoided by properly protecting the model. In this paper, we show that backdoor attacks can be achieved without any model modification. Instead of injecting backdoor logic into the training data or the model, we propose to place a carefully-designed patch (namely backdoor patch) in front of the camera, which is fed into the model together with the input images. The patch can be trained to behave normally at most of the time, while producing wrong prediction when the input image contains an attacker-controlled trigger object. Our main techniques include an effective training method to generate the backdoor patch and a digital-physical transformation modeling method to enhance the feasibility of the patch in real deployments. Extensive experiments show that PatchBackdoor can be applied to common deep learning models (VGG, MobileNet, ResNet) with an attack success rate of 93% to 99% on classification tasks. Moreover, we implement PatchBackdoor in real-world scenarios and show that the attack is still threatening.</p>
<hr>
<h2 id="CLIP-Multi-modal-Hashing-A-new-baseline-CLIPMH"><a href="#CLIP-Multi-modal-Hashing-A-new-baseline-CLIPMH" class="headerlink" title="CLIP Multi-modal Hashing: A new baseline CLIPMH"></a>CLIP Multi-modal Hashing: A new baseline CLIPMH</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11797">http://arxiv.org/abs/2308.11797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Zhu, Mingkai Sheng, Mingda Ke, Zhangmin Huang, Jingfei Chang</li>
</ul>
<p>Abstract:<br>The multi-modal hashing method is widely used in multimedia retrieval. It can fuse multi-source data to generate binary hash code. However, the current multi-modal methods have the problem of low retrieval accuracy. The reason is that the individual backbone networks have limited feature expression capabilities and are not jointly pre-trained on large-scale unsupervised multi-modal data. To solve this problem, we propose a new baseline CLIP Multi-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and image features, and then fuse to generate hash code. CLIP improves the expressiveness of each modal feature. In this way, it can greatly improve the retrieval performance of multi-modal hashing methods. In comparison to state-of-the-art unsupervised and supervised multi-modal hashing methods, experiments reveal that the proposed CLIPMH can significantly enhance performance (Maximum increase of 8.38%). CLIP also has great advantages over the text and visual backbone networks commonly used before.</p>
<hr>
<h2 id="Time-Does-Tell-Self-Supervised-Time-Tuning-of-Dense-Image-Representations"><a href="#Time-Does-Tell-Self-Supervised-Time-Tuning-of-Dense-Image-Representations" class="headerlink" title="Time Does Tell: Self-Supervised Time-Tuning of Dense Image Representations"></a>Time Does Tell: Self-Supervised Time-Tuning of Dense Image Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11796">http://arxiv.org/abs/2308.11796</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smsd75/timetuning">https://github.com/smsd75/timetuning</a></li>
<li>paper_authors: Mohammadreza Salehi, Efstratios Gavves, Cees G. M. Snoek, Yuki M. Asano</li>
</ul>
<p>Abstract:<br>Spatially dense self-supervised learning is a rapidly growing problem domain with promising applications for unsupervised segmentation and pretraining for dense downstream tasks. Despite the abundance of temporal data in the form of videos, this information-rich source has been largely overlooked. Our paper aims to address this gap by proposing a novel approach that incorporates temporal consistency in dense self-supervised learning. While methods designed solely for images face difficulties in achieving even the same performance on videos, our method improves not only the representation quality for videos-but also images. Our approach, which we call time-tuning, starts from image-pretrained models and fine-tunes them with a novel self-supervised temporal-alignment clustering loss on unlabeled videos. This effectively facilitates the transfer of high-level information from videos to image representations. Time-tuning improves the state-of-the-art by 8-10% for unsupervised semantic segmentation on videos and matches it for images. We believe this method paves the way for further self-supervised scaling by leveraging the abundant availability of videos. The implementation can be found here : <a target="_blank" rel="noopener" href="https://github.com/SMSD75/Timetuning">https://github.com/SMSD75/Timetuning</a></p>
<hr>
<h2 id="Enhancing-NeRF-akin-to-Enhancing-LLMs-Generalizable-NeRF-Transformer-with-Mixture-of-View-Experts"><a href="#Enhancing-NeRF-akin-to-Enhancing-LLMs-Generalizable-NeRF-Transformer-with-Mixture-of-View-Experts" class="headerlink" title="Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts"></a>Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11793">http://arxiv.org/abs/2308.11793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/VITA-Group/GNT-MOVE">https://github.com/VITA-Group/GNT-MOVE</a></li>
<li>paper_authors: Wenyan Cong, Hanxue Liang, Peihao Wang, Zhiwen Fan, Tianlong Chen, Mukund Varma, Yi Wang, Zhangyang Wang</li>
</ul>
<p>Abstract:<br>Cross-scene generalizable NeRF models, which can directly synthesize novel views of unseen scenes, have become a new spotlight of the NeRF field. Several existing attempts rely on increasingly end-to-end “neuralized” architectures, i.e., replacing scene representation and&#x2F;or rendering modules with performant neural networks such as transformers, and turning novel view synthesis into a feed-forward inference pipeline. While those feedforward “neuralized” architectures still do not fit diverse scenes well out of the box, we propose to bridge them with the powerful Mixture-of-Experts (MoE) idea from large language models (LLMs), which has demonstrated superior generalization ability by balancing between larger overall model capacity and flexible per-instance specialization. Starting from a recent generalizable NeRF architecture called GNT, we first demonstrate that MoE can be neatly plugged in to enhance the model. We further customize a shared permanent expert and a geometry-aware consistency loss to enforce cross-scene consistency and spatial smoothness respectively, which are essential for generalizable view synthesis. Our proposed model, dubbed GNT with Mixture-of-View-Experts (GNT-MOVE), has experimentally shown state-of-the-art results when transferring to unseen scenes, indicating remarkably better cross-scene generalization in both zero-shot and few-shot settings. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/VITA-Group/GNT-MOVE">https://github.com/VITA-Group/GNT-MOVE</a>.</p>
<hr>
<h2 id="An-extensible-point-based-method-for-data-chart-value-detection"><a href="#An-extensible-point-based-method-for-data-chart-value-detection" class="headerlink" title="An extensible point-based method for data chart value detection"></a>An extensible point-based method for data chart value detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11788">http://arxiv.org/abs/2308.11788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bnlnlp/ppn_model">https://github.com/bnlnlp/ppn_model</a></li>
<li>paper_authors: Carlos Soto, Shinjae Yoo</li>
</ul>
<p>Abstract:<br>We present an extensible method for identifying semantic points to reverse engineer (i.e. extract the values of) data charts, particularly those in scientific articles. Our method uses a point proposal network (akin to region proposal networks for object detection) to directly predict the position of points of interest in a chart, and it is readily extensible to multiple chart types and chart elements. We focus on complex bar charts in the scientific literature, on which our model is able to detect salient points with an accuracy of 0.8705 F1 (@1.5-cell max deviation); it achieves 0.9810 F1 on synthetically-generated charts similar to those used in prior works. We also explore training exclusively on synthetic data with novel augmentations, reaching surprisingly competent performance in this way (0.6621 F1) on real charts with widely varying appearance, and we further demonstrate our unchanged method applied directly to synthetic pie charts (0.8343 F1). Datasets, trained models, and evaluation code are available at <a target="_blank" rel="noopener" href="https://github.com/BNLNLP/PPN_model">https://github.com/BNLNLP/PPN_model</a>.</p>
<hr>
<h2 id="Coarse-to-Fine-Multi-Scene-Pose-Regression-with-Transformers"><a href="#Coarse-to-Fine-Multi-Scene-Pose-Regression-with-Transformers" class="headerlink" title="Coarse-to-Fine Multi-Scene Pose Regression with Transformers"></a>Coarse-to-Fine Multi-Scene Pose Regression with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11783">http://arxiv.org/abs/2308.11783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoli Shavit, Ron Ferens, Yosi Keller</li>
</ul>
<p>Abstract:<br>Absolute camera pose regressors estimate the position and orientation of a camera given the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron (MLP) head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended to learn multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into pose predictions. This allows our model to focus on general features that are informative for localization, while embedding multiple scenes in parallel. We extend our previous MS-Transformer approach \cite{shavit2021learning} by introducing a mixed classification-regression architecture that improves the localization accuracy. Our method is evaluated on commonly benchmark indoor and outdoor datasets and has been shown to exceed both multi-scene and state-of-the-art single-scene absolute pose regressors.</p>
<hr>
<h2 id="Understanding-Hessian-Alignment-for-Domain-Generalization"><a href="#Understanding-Hessian-Alignment-for-Domain-Generalization" class="headerlink" title="Understanding Hessian Alignment for Domain Generalization"></a>Understanding Hessian Alignment for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11778">http://arxiv.org/abs/2308.11778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/federated-learning">https://github.com/huawei-noah/federated-learning</a></li>
<li>paper_authors: Sobhan Hemati, Guojun Zhang, Amir Estiri, Xi Chen</li>
</ul>
<p>Abstract:<br>Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenarios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regularizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier’s head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spectral norm between the classifier’s head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Furthermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients. Our analysis explains the success of many regularizers like CORAL, IRM, V-REx, Fish, IGA, and Fishr as they regularize part of the classifier’s head Hessian and&#x2F;or gradient. Finally, we propose two simple yet effective methods to match the classifier’s head Hessians and gradients in an efficient way, based on the Hessian Gradient Product (HGP) and Hutchinson’s method (Hutchinson), and without directly calculating Hessians. We validate the OOD generalization ability of proposed methods in different scenarios, including transferability, severe correlation shift, label shift and diversity shift. Our results show that Hessian alignment methods achieve promising performance on various OOD benchmarks. The code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment%7D">https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment}</a>.</p>
<hr>
<h2 id="WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters"><a href="#WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters" class="headerlink" title="WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters"></a>WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11776">http://arxiv.org/abs/2308.11776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ange Lou, Jack Noble</li>
</ul>
<p>Abstract:<br>Depth estimation in surgical video plays a crucial role in many image-guided surgery procedures. However, it is difficult and time consuming to create depth map ground truth datasets in surgical videos due in part to inconsistent brightness and noise in the surgical scene. Therefore, building an accurate and robust self-supervised depth and camera ego-motion estimation system is gaining more attention from the computer vision community. Although several self-supervision methods alleviate the need for ground truth depth maps and poses, they still need known camera intrinsic parameters, which are often missing or not recorded. Moreover, the camera intrinsic prediction methods in existing works depend heavily on the quality of datasets. In this work, we aimed to build a self-supervised depth and ego-motion estimation system which can predict not only accurate depth maps and camera pose, but also camera intrinsic parameters. We proposed a cost-volume-based supervision manner to give the system auxiliary supervision for camera parameters prediction. The experimental results showed that the proposed method improved the accuracy of estimated camera parameters, ego-motion, and depth estimation.</p>
<hr>
<h2 id="SAMSNeRF-Segment-Anything-Model-SAM-Guides-Dynamic-Surgical-Scene-Reconstruction-by-Neural-Radiance-Field-NeRF"><a href="#SAMSNeRF-Segment-Anything-Model-SAM-Guides-Dynamic-Surgical-Scene-Reconstruction-by-Neural-Radiance-Field-NeRF" class="headerlink" title="SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene Reconstruction by Neural Radiance Field (NeRF)"></a>SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene Reconstruction by Neural Radiance Field (NeRF)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11774">http://arxiv.org/abs/2308.11774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ange Lou, Yamin Li, Xing Yao, Yike Zhang, Jack Noble</li>
</ul>
<p>Abstract:<br>The accurate reconstruction of surgical scenes from surgical videos is critical for various applications, including intraoperative navigation and image-guided robotic surgery automation. However, previous approaches, mainly relying on depth estimation, have limited effectiveness in reconstructing surgical scenes with moving surgical tools. To address this limitation and provide accurate 3D position prediction for surgical tools in all frames, we propose a novel approach called SAMSNeRF that combines Segment Anything Model (SAM) and Neural Radiance Field (NeRF) techniques. Our approach generates accurate segmentation masks of surgical tools using SAM, which guides the refinement of the dynamic surgical scene reconstruction by NeRF. Our experimental results on public endoscopy surgical videos demonstrate that our approach successfully reconstructs high-fidelity dynamic surgical scenes and accurately reflects the spatial information of surgical tools. Our proposed approach can significantly enhance surgical navigation and automation by providing surgeons with accurate 3D position information of surgical tools during surgery.The source code will be released soon.</p>
<hr>
<h2 id="3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network"><a href="#3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network" class="headerlink" title="3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network"></a>3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11771">http://arxiv.org/abs/2308.11771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinyu Chen, Zuowen Wang, Shih-Chii Liu, Chang Gao</li>
</ul>
<p>Abstract:<br>This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR&#x2F;VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\times$ without losing accuracy when tested on a \texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \url{<a target="_blank" rel="noopener" href="https://github.com/qinche106/cb-convlstm-eyetracking%7D">https://github.com/qinche106/cb-convlstm-eyetracking}</a>.</p>
<hr>
<h2 id="Weakly-Supervised-Face-and-Whole-Body-Recognition-in-Turbulent-Environments"><a href="#Weakly-Supervised-Face-and-Whole-Body-Recognition-in-Turbulent-Environments" class="headerlink" title="Weakly Supervised Face and Whole Body Recognition in Turbulent Environments"></a>Weakly Supervised Face and Whole Body Recognition in Turbulent Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11757">http://arxiv.org/abs/2308.11757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshitij Nikhal, Benjamin S. Riggan</li>
</ul>
<p>Abstract:<br>Face and person recognition have recently achieved remarkable success under challenging scenarios, such as off-pose and cross-spectrum matching. However, long-range recognition systems are often hindered by atmospheric turbulence, leading to spatially and temporally varying distortions in the image. Current solutions rely on generative models to reconstruct a turbulent-free image, but often preserve photo-realism instead of discriminative features that are essential for recognition. This can be attributed to the lack of large-scale datasets of turbulent and pristine paired images, necessary for optimal reconstruction. To address this issue, we propose a new weakly supervised framework that employs a parameter-efficient self-attention module to generate domain agnostic representations, aligning turbulent and pristine images into a common subspace. Additionally, we introduce a new tilt map estimator that predicts geometric distortions observed in turbulent images. This estimate is used to re-rank gallery matches, resulting in up to 13.86% improvement in rank-1 accuracy. Our method does not require synthesizing turbulent-free images or ground-truth paired images, and requires significantly fewer annotated samples, enabling more practical and rapid utility of increasingly large datasets. We analyze our framework using two datasets – Long-Range Face Identification Dataset (LRFID) and BRIAR Government Collection 1 (BGC1) – achieving enhanced discriminability under varying turbulence and standoff distance.</p>
<hr>
<h2 id="Efficient-Controllable-Multi-Task-Architectures"><a href="#Efficient-Controllable-Multi-Task-Architectures" class="headerlink" title="Efficient Controllable Multi-Task Architectures"></a>Efficient Controllable Multi-Task Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11744">http://arxiv.org/abs/2308.11744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Aich, Samuel Schulter, Amit K. Roy-Chowdhury, Manmohan Chandraker, Yumin Suh</li>
</ul>
<p>Abstract:<br>We aim to train a multi-task model such that users can adjust the desired compute budget and relative importance of task performances after deployment, without retraining. This enables optimizing performance for dynamically varying user needs, without heavy computational overhead to train and save models for various scenarios. To this end, we propose a multi-task model consisting of a shared encoder and task-specific decoders where both encoder and decoder channel widths are slimmable. Our key idea is to control the task importance by varying the capacities of task-specific decoders, while controlling the total computational cost by jointly adjusting the encoder capacity. This improves overall accuracy by allowing a stronger encoder for a given budget, increases control over computational cost, and delivers high-quality slimmed sub-architectures based on user’s constraints. Our training strategy involves a novel ‘Configuration-Invariant Knowledge Distillation’ loss that enforces backbone representations to be invariant under different runtime width configurations to enhance accuracy. Further, we present a simple but effective search algorithm that translates user constraints to runtime width configurations of both the shared encoder and task decoders, for sampling the sub-architectures. The key rule for the search algorithm is to provide a larger computational budget to the higher preferred task decoder, while searching a shared encoder configuration that enhances the overall MTL performance. Various experiments on three multi-task benchmarks (PASCALContext, NYUDv2, and CIFAR100-MTL) with diverse backbone architectures demonstrate the advantage of our approach. For example, our method shows a higher controllability by ~33.5% in the NYUD-v2 dataset over prior methods, while incurring much less compute cost.</p>
<hr>
<h2 id="Animal3D-A-Comprehensive-Dataset-of-3D-Animal-Pose-and-Shape"><a href="#Animal3D-A-Comprehensive-Dataset-of-3D-Animal-Pose-and-Shape" class="headerlink" title="Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape"></a>Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11737">http://arxiv.org/abs/2308.11737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, Wei Ji, Chen Wang, Xiaoding Yuan, Prakhar Kaushik, Guofeng Zhang, Jie Liu, Yushan Xie, Yawen Cui, Alan Yuille, Adam Kortylewski</li>
</ul>
<p>Abstract:<br>Accurately estimating the 3D pose and shape is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. However, research in this area is held back by the lack of a comprehensive and diverse dataset with high-quality 3D pose and shape annotations. In this paper, we propose Animal3D, the first comprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D consists of 3379 images collected from 40 mammal species, high-quality annotations of 26 keypoints, and importantly the pose and shape parameters of the SMAL model. All annotations were labeled and checked manually in a multi-stage process to ensure highest quality results. Based on the Animal3D dataset, we benchmark representative shape and pose estimation models at: (1) supervised learning from only the Animal3D data, (2) synthetic to real transfer from synthetically generated images, and (3) fine-tuning human pose and shape estimation models. Our experimental results demonstrate that predicting the 3D shape and pose of animals across species remains a very challenging task, despite significant advances in human pose estimation. Our results further demonstrate that synthetic pre-training is a viable strategy to boost the model performance. Overall, Animal3D opens new directions for facilitating future research in animal 3D pose and shape estimation, and is publicly available.</p>
<hr>
<h2 id="Un-fair-Exposure-in-Deep-Face-Rankings-at-a-Distance"><a href="#Un-fair-Exposure-in-Deep-Face-Rankings-at-a-Distance" class="headerlink" title="(Un)fair Exposure in Deep Face Rankings at a Distance"></a>(Un)fair Exposure in Deep Face Rankings at a Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11732">http://arxiv.org/abs/2308.11732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Atzori, Gianni Fenu, Mirko Marras</li>
</ul>
<p>Abstract:<br>Law enforcement regularly faces the challenge of ranking suspects from their facial images. Deep face models aid this process but frequently introduce biases that disproportionately affect certain demographic segments. While bias investigation is common in domains like job candidate ranking, the field of forensic face rankings remains underexplored. In this paper, we propose a novel experimental framework, encompassing six state-of-the-art face encoders and two public data sets, designed to scrutinize the extent to which demographic groups suffer from biases in exposure in the context of forensic face rankings. Through comprehensive experiments that cover both re-identification and identification tasks, we show that exposure biases within this domain are far from being countered, demanding attention towards establishing ad-hoc policies and corrective measures. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/atzoriandrea/ijcb2023-unfair-face-rankings">https://github.com/atzoriandrea/ijcb2023-unfair-face-rankings</a></p>
<hr>
<h2 id="GRIP-Generating-Interaction-Poses-Using-Latent-Consistency-and-Spatial-Cues"><a href="#GRIP-Generating-Interaction-Poses-Using-Latent-Consistency-and-Spatial-Cues" class="headerlink" title="GRIP: Generating Interaction Poses Using Latent Consistency and Spatial Cues"></a>GRIP: Generating Interaction Poses Using Latent Consistency and Spatial Cues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11617">http://arxiv.org/abs/2308.11617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omid Taheri, Yi Zhou, Dimitrios Tzionas, Yang Zhou, Duygu Ceylan, Soren Pirk, Michael J. Black</li>
</ul>
<p>Abstract:<br>Hands are dexterous and highly versatile manipulators that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand-object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality. Prior work on capturing and modeling humans interacting with objects in 3D focuses on the body and object motion, often ignoring hand pose. In contrast, we introduce GRIP, a learning-based method that takes, as input, the 3D motion of the body and the object, and synthesizes realistic motion for both hands before, during, and after object interaction. As a preliminary step before synthesizing the hand motion, we first use a network, ANet, to denoise the arm motion. Then, we leverage the spatio-temporal relationship between the body and the object to extract two types of novel temporal interaction cues, and use them in a two-stage inference pipeline to generate the hand motion. In the first stage, we introduce a new approach to enforce motion temporal consistency in the latent space (LTC), and generate consistent interaction motions. In the second stage, GRIP generates refined hand poses to avoid hand-object penetrations. Given sequences of noisy body and object motion, GRIP upgrades them to include hand-object interaction. Quantitative experiments and perceptual studies demonstrate that GRIP outperforms baseline methods and generalizes to unseen objects and motions from different motion-capture datasets.</p>
<hr>
<h2 id="Efficient-Benchmarking-of-Language-Models"><a href="#Efficient-Benchmarking-of-Language-Models" class="headerlink" title="Efficient Benchmarking (of Language Models)"></a>Efficient Benchmarking (of Language Models)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11696">http://arxiv.org/abs/2308.11696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen</li>
</ul>
<p>Abstract:<br>The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark ranking. Conversely a slightly different choice of HELM scenarios varies ranking widely. Based on our findings we outline a set of concrete recommendations for more efficient benchmark design and utilization practices leading to dramatic cost savings with minimal loss of benchmark reliability often reducing computation by x100 or more.</p>
<hr>
<h2 id="Delving-into-Motion-Aware-Matching-for-Monocular-3D-Object-Tracking"><a href="#Delving-into-Motion-Aware-Matching-for-Monocular-3D-Object-Tracking" class="headerlink" title="Delving into Motion-Aware Matching for Monocular 3D Object Tracking"></a>Delving into Motion-Aware Matching for Monocular 3D Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11607">http://arxiv.org/abs/2308.11607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kuanchihhuang/moma-m3t">https://github.com/kuanchihhuang/moma-m3t</a></li>
<li>paper_authors: Kuan-Chih Huang, Ming-Hsuan Yang, Yi-Hsuan Tsai</li>
</ul>
<p>Abstract:<br>Recent advances of monocular 3D object detection facilitate the 3D multi-object tracking task based on low-cost camera sensors. In this paper, we find that the motion cue of objects along different time frames is critical in 3D multi-object tracking, which is less explored in existing monocular-based approaches. In this paper, we propose a motion-aware framework for monocular 3D MOT. To this end, we propose MoMA-M3T, a framework that mainly consists of three motion-aware components. First, we represent the possible movement of an object related to all object tracklets in the feature space as its motion features. Then, we further model the historical object tracklet along the time frame in a spatial-temporal perspective via a motion transformer. Finally, we propose a motion-aware matching module to associate historical object tracklets and current observations as final tracking results. We conduct extensive experiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3T achieves competitive performance against state-of-the-art methods. Moreover, the proposed tracker is flexible and can be easily plugged into existing image-based 3D object detectors without re-training. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/kuanchihhuang/MoMA-M3T">https://github.com/kuanchihhuang/MoMA-M3T</a>.</p>
<hr>
<h2 id="StoryBench-A-Multifaceted-Benchmark-for-Continuous-Story-Visualization"><a href="#StoryBench-A-Multifaceted-Benchmark-for-Continuous-Story-Visualization" class="headerlink" title="StoryBench: A Multifaceted Benchmark for Continuous Story Visualization"></a>StoryBench: A Multifaceted Benchmark for Continuous Story Visualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11606">http://arxiv.org/abs/2308.11606</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google/storybench">https://github.com/google/storybench</a></li>
<li>paper_authors: Emanuele Bugliarello, Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan, Vittorio Ferrari, Pieter-Jan Kindermans, Paul Voigtlaender</li>
</ul>
<p>Abstract:<br>Generating video stories from text prompts is a complex task. In addition to having high visual quality, videos need to realistically adhere to a sequence of text prompts whilst being consistent throughout the frames. Creating a benchmark for video generation requires data annotated over time, which contrasts with the single caption used often in video datasets. To fill this gap, we collect comprehensive human annotations on three existing datasets, and introduce StoryBench: a new, challenging multi-task benchmark to reliably evaluate forthcoming text-to-video models. Our benchmark includes three video generation tasks of increasing difficulty: action execution, where the next action must be generated starting from a conditioning video; story continuation, where a sequence of actions must be executed starting from a conditioning video; and story generation, where a video must be generated from only text prompts. We evaluate small yet strong text-to-video baselines, and show the benefits of training on story-like data algorithmically generated from existing video captions. Finally, we establish guidelines for human evaluation of video stories, and reaffirm the need of better automatic metrics for video generation. StoryBench aims at encouraging future research efforts in this exciting new area.</p>
<hr>
<h2 id="GOPro-Generate-and-Optimize-Prompts-in-CLIP-using-Self-Supervised-Learning"><a href="#GOPro-Generate-and-Optimize-Prompts-in-CLIP-using-Self-Supervised-Learning" class="headerlink" title="GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised Learning"></a>GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11605">http://arxiv.org/abs/2308.11605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mainak Singha, Ankit Jha, Biplab Banerjee</li>
</ul>
<p>Abstract:<br>Large-scale foundation models, such as CLIP, have demonstrated remarkable success in visual recognition tasks by embedding images in a semantically rich space. Self-supervised learning (SSL) has also shown promise in improving visual recognition by learning invariant features. However, the combination of CLIP with SSL is found to face challenges due to the multi-task framework that blends CLIP’s contrastive loss and SSL’s loss, including difficulties with loss weighting and inconsistency among different views of images in CLIP’s output space. To overcome these challenges, we propose a prompt learning-based model called GOPro, which is a unified framework that ensures similarity between various augmented views of input images in a shared image-text embedding space, using a pair of learnable image and text projectors atop CLIP, to promote invariance and generalizability. To automatically learn such prompts, we leverage the visual content and style primitives extracted from pre-trained CLIP and adapt them to the target task. In addition to CLIP’s cross-domain contrastive loss, we introduce a visual contrastive loss and a novel prompt consistency loss, considering the different views of the images. GOPro is trained end-to-end on all three loss objectives, combining the strengths of CLIP and SSL in a principled manner. Empirical evaluations demonstrate that GOPro outperforms the state-of-the-art prompting techniques on three challenging domain generalization tasks across multiple benchmarks by a significant margin. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/mainaksingha01/GOPro">https://github.com/mainaksingha01/GOPro</a>.</p>
<hr>
<h2 id="G3Reg-Pyramid-Graph-based-Global-Registration-using-Gaussian-Ellipsoid-Model"><a href="#G3Reg-Pyramid-Graph-based-Global-Registration-using-Gaussian-Ellipsoid-Model" class="headerlink" title="G3Reg: Pyramid Graph-based Global Registration using Gaussian Ellipsoid Model"></a>G3Reg: Pyramid Graph-based Global Registration using Gaussian Ellipsoid Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11573">http://arxiv.org/abs/2308.11573</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-aerial-robotics/lidar-registration-benchmark">https://github.com/hkust-aerial-robotics/lidar-registration-benchmark</a></li>
<li>paper_authors: Zhijian Qiao, Zehuan Yu, Binqian Jiang, Huan Yin, Shaojie Shen</li>
</ul>
<p>Abstract:<br>This study introduces a novel framework, G3Reg, for fast and robust global registration of LiDAR point clouds. In contrast to conventional complex keypoints and descriptors, we extract fundamental geometric primitives including planes, clusters, and lines (PCL) from the raw point cloud to obtain low-level semantic segments. Each segment is formulated as a unified Gaussian Ellipsoid Model (GEM) by employing a probability ellipsoid to ensure the ground truth centers are encompassed with a certain degree of probability. Utilizing these GEMs, we then present a distrust-and-verify scheme based on a Pyramid Compatibility Graph for Global Registration (PAGOR). Specifically, we establish an upper bound, which can be traversed based on the confidence level for compatibility testing to construct the pyramid graph. Gradually, we solve multiple maximum cliques (MAC) for each level of the graph, generating numerous transformation candidates. In the verification phase, we adopt a precise and efficient metric for point cloud alignment quality, founded on geometric primitives, to identify the optimal candidate. The performance of the algorithm is extensively validated on three publicly available datasets and a self-collected multi-session dataset, without changing any parameter settings in the experimental evaluation. The results exhibit superior robustness and real-time performance of the G3Reg framework compared to state-of-the-art methods. Furthermore, we demonstrate the potential for integrating individual GEM and PAGOR components into other algorithmic frameworks to enhance their efficacy. To advance further research and promote community understanding, we have publicly shared the source code.</p>
<hr>
<h2 id="SPANet-Frequency-balancing-Token-Mixer-using-Spectral-Pooling-Aggregation-Modulation"><a href="#SPANet-Frequency-balancing-Token-Mixer-using-Spectral-Pooling-Aggregation-Modulation" class="headerlink" title="SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation"></a>SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11568">http://arxiv.org/abs/2308.11568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guhnoo Yun, Juhan Yoo, Kijung Kim, Jeongho Lee, Dong Hwan Kim</li>
</ul>
<p>Abstract:<br>Recent studies show that self-attentions behave like low-pass filters (as opposed to convolutions) and enhancing their high-pass filtering capability improves model performance. Contrary to this idea, we investigate existing convolution-based models with spectral analysis and observe that improving the low-pass filtering in convolution operations also leads to performance improvement. To account for this observation, we hypothesize that utilizing optimal token mixers that capture balanced representations of both high- and low-frequency components can enhance the performance of models. We verify this by decomposing visual features into the frequency domain and combining them in a balanced manner. To handle this, we replace the balancing problem with a mask filtering problem in the frequency domain. Then, we introduce a novel token-mixer named SPAM and leverage it to derive a MetaFormer model termed as SPANet. Experimental results show that the proposed method provides a way to achieve this balance, and the balanced representations of both high- and low-frequency components can improve the performance of models on multiple computer vision tasks. Our code is available at $\href{<a target="_blank" rel="noopener" href="https://doranlyong.github.io/projects/spanet/%7D%7B/text%7Bhttps://doranlyong.github.io/projects/spanet/%7D%7D$">https://doranlyong.github.io/projects/spanet/}{\text{https://doranlyong.github.io/projects/spanet/}}$</a>.</p>
<hr>
<h2 id="EndoNet-model-for-automatic-calculation-of-H-score-on-histological-slides"><a href="#EndoNet-model-for-automatic-calculation-of-H-score-on-histological-slides" class="headerlink" title="EndoNet: model for automatic calculation of H-score on histological slides"></a>EndoNet: model for automatic calculation of H-score on histological slides</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11562">http://arxiv.org/abs/2308.11562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Egor Ushakov, Anton Naumov, Vladislav Fomberg, Polina Vishnyakova, Aleksandra Asaturova, Alina Badlaeva, Anna Tregubova, Evgeny Karpulevich, Gennady Sukhikh, Timur Fatkhudinov</li>
</ul>
<p>Abstract:<br>H-score is a semi-quantitative method used to assess the presence and distribution of proteins in tissue samples by combining the intensity of staining and percentage of stained nuclei. It is widely used but time-consuming and can be limited in accuracy and precision. Computer-aided methods may help overcome these limitations and improve the efficiency of pathologists’ workflows. In this work, we developed a model EndoNet for automatic calculation of H-score on histological slides. Our proposed method uses neural networks and consists of two main parts. The first is a detection model which predicts keypoints of centers of nuclei. The second is a H-score module which calculates the value of the H-score using mean pixel values of predicted keypoints. Our model was trained and validated on 1780 annotated tiles with a shape of 100x100 $\mu m$ and performed 0.77 mAP on a test dataset. Moreover, the model can be adjusted to a specific specialist or whole laboratory to reproduce the manner of calculating the H-score. Thus, EndoNet is effective and robust in the analysis of histology slides, which can improve and significantly accelerate the work of pathologists.</p>
<hr>
<h2 id="Target-Grounded-Graph-Aware-Transformer-for-Aerial-Vision-and-Dialog-Navigation"><a href="#Target-Grounded-Graph-Aware-Transformer-for-Aerial-Vision-and-Dialog-Navigation" class="headerlink" title="Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation"></a>Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11561">http://arxiv.org/abs/2308.11561</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yifeisu/avdn-challenge">https://github.com/yifeisu/avdn-challenge</a></li>
<li>paper_authors: Yifei Su, Dong An, Yuan Xu, Kehan Chen, Yan Huang</li>
</ul>
<p>Abstract:<br>This report details the method of the winning entry of the AVDN Challenge in ICCV 2023. The competition addresses the Aerial Navigation from Dialog History (ANDH) task, which requires a drone agent to associate dialog history with aerial observations to reach the destination. For better cross-modal grounding abilities of the drone agent, we propose a Target-Grounded Graph-Aware Transformer (TG-GAT) framework. Concretely, TG-GAT first leverages a graph-aware transformer to capture spatiotemporal dependency, which benefits navigation state tracking and robust action planning. In addition, an auxiliary visual grounding task is devised to boost the agent’s awareness of referred landmarks. Moreover, a hybrid augmentation strategy based on large language models is utilized to mitigate data scarcity limitations. Our TG-GAT framework won the AVDN Challenge 2023, with 2.2% and 3.0% absolute improvements over the baseline on SPL and SR metrics, respectively. The code is available at <a target="_blank" rel="noopener" href="https://github.com/yifeisu/avdn-challenge">https://github.com/yifeisu/avdn-challenge</a>.</p>
<hr>
<h2 id="Open-Set-Synthetic-Image-Source-Attribution"><a href="#Open-Set-Synthetic-Image-Source-Attribution" class="headerlink" title="Open Set Synthetic Image Source Attribution"></a>Open Set Synthetic Image Source Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11557">http://arxiv.org/abs/2308.11557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbang Fang, Tai D. Nguyen, Matthew C. Stamm</li>
</ul>
<p>Abstract:<br>AI-generated images have become increasingly realistic and have garnered significant public attention. While synthetic images are intriguing due to their realism, they also pose an important misinformation threat. To address this new threat, researchers have developed multiple algorithms to detect synthetic images and identify their source generators. However, most existing source attribution techniques are designed to operate in a closed-set scenario, i.e. they can only be used to discriminate between known image generators. By contrast, new image-generation techniques are rapidly emerging. To contend with this, there is a great need for open-set source attribution techniques that can identify when synthetic images have originated from new, unseen generators. To address this problem, we propose a new metric learning-based approach. Our technique works by learning transferrable embeddings capable of discriminating between generators, even when they are not seen during training. An image is first assigned to a candidate generator, then is accepted or rejected based on its distance in the embedding space from known generators’ learned reference points. Importantly, we identify that initializing our source attribution embedding network by pretraining it on image camera identification can improve our embeddings’ transferability. Through a series of experiments, we demonstrate our approach’s ability to attribute the source of synthetic images in open-set scenarios.</p>
<hr>
<h2 id="Multi-event-Video-Text-Retrieval"><a href="#Multi-event-Video-Text-Retrieval" class="headerlink" title="Multi-event Video-Text Retrieval"></a>Multi-event Video-Text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11551">http://arxiv.org/abs/2308.11551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gengyuanmax/mevtr">https://github.com/gengyuanmax/mevtr</a></li>
<li>paper_authors: Gengyuan Zhang, Jisen Ren, Jindong Gu, Volker Tresp</li>
</ul>
<p>Abstract:<br>Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We present a simple model, Me-Retriever, which incorporates key event video representation and a new MeVTR loss for the MeVTR task. Comprehensive experiments show that this straightforward framework outperforms other models in the Video-to-Text and Text-to-Video tasks, effectively establishing a robust baseline for the MeVTR task. We believe this work serves as a strong foundation for future studies. Code is available at <a target="_blank" rel="noopener" href="https://github.com/gengyuanmax/MeVTR">https://github.com/gengyuanmax/MeVTR</a>.</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/23/cs.CV_2023_08_23/" data-id="cllqyyxae000gbpr89v7p0zse" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.LG_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.LG_2023_08_23/">cs.LG - 2023-08-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Challenges-of-Machine-Learning-for-Trust-and-Safety-A-Case-Study-on-Misinformation-Detection"><a href="#The-Challenges-of-Machine-Learning-for-Trust-and-Safety-A-Case-Study-on-Misinformation-Detection" class="headerlink" title="The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection"></a>The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12215">http://arxiv.org/abs/2308.12215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ramybaly/News-Media-Reliability">https://github.com/ramybaly/News-Media-Reliability</a></li>
<li>paper_authors: Madelyne Xiao, Jonathan Mayer</li>
</ul>
<p>Abstract:<br>We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is for future work to avoid the pitfalls that we identify.</p>
<hr>
<h2 id="Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies"><a href="#Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies" class="headerlink" title="Learning to Learn Financial Networks for Optimising Momentum Strategies"></a>Learning to Learn Financial Networks for Optimising Momentum Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12212">http://arxiv.org/abs/2308.12212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyue, Pu, Stefan Zohren, Stephen Roberts, Xiaowen Dong</li>
</ul>
<p>Abstract:<br>Network momentum provides a novel type of risk premium, which exploits the interconnections among assets in a financial network to predict future returns. However, the current process of constructing financial networks relies heavily on expensive databases and financial expertise, limiting accessibility for small-sized and academic institutions. Furthermore, the traditional approach treats network construction and portfolio optimisation as separate tasks, potentially hindering optimal portfolio performance. To address these challenges, we propose L2GMOM, an end-to-end machine learning framework that simultaneously learns financial networks and optimises trading signals for network momentum strategies. The model of L2GMOM is a neural network with a highly interpretable forward propagation architecture, which is derived from algorithm unrolling. The L2GMOM is flexible and can be trained with diverse loss functions for portfolio performance, e.g. the negative Sharpe ratio. Backtesting on 64 continuous future contracts demonstrates a significant improvement in portfolio profitability and risk control, with a Sharpe ratio of 1.74 across a 20-year period.</p>
<hr>
<h2 id="ULDP-FL-Federated-Learning-with-Across-Silo-User-Level-Differential-Privacy"><a href="#ULDP-FL-Federated-Learning-with-Across-Silo-User-Level-Differential-Privacy" class="headerlink" title="ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy"></a>ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12210">http://arxiv.org/abs/2308.12210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fumiyukikato/uldp-fl">https://github.com/fumiyukikato/uldp-fl</a></li>
<li>paper_authors: Fumiyuki Kato, Li Xiong, Shun Takagi, Yang Cao, Masatoshi Yoshikawa</li>
</ul>
<p>Abstract:<br>Differentially Private Federated Learning (DP-FL) has garnered attention as a collaborative machine learning approach that ensures formal privacy. Most DP-FL approaches ensure DP at the record-level within each silo for cross-silo FL. However, a single user’s data may extend across multiple silos, and the desired user-level DP guarantee for such a setting remains unknown. In this study, we present ULDP-FL, a novel FL framework designed to guarantee user-level DP in cross-silo FL where a single user’s data may belong to multiple silos. Our proposed algorithm directly ensures user-level DP through per-user weighted clipping, departing from group-privacy approaches. We provide a theoretical analysis of the algorithm’s privacy and utility. Additionally, we enhance the algorithm’s utility and showcase its private implementation using cryptographic building blocks. Empirical experiments on real-world datasets show substantial improvements in our methods in privacy-utility trade-offs under user-level DP compared to baseline methods. To the best of our knowledge, our work is the first FL framework that effectively provides user-level DP in the general cross-silo FL setting.</p>
<hr>
<h2 id="Curriculum-Learning-with-Adam-The-Devil-Is-in-the-Wrong-Details"><a href="#Curriculum-Learning-with-Adam-The-Devil-Is-in-the-Wrong-Details" class="headerlink" title="Curriculum Learning with Adam: The Devil Is in the Wrong Details"></a>Curriculum Learning with Adam: The Devil Is in the Wrong Details</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12202">http://arxiv.org/abs/2308.12202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Weber, Jaap Jumelet, Paul Michel, Elia Bruni, Dieuwke Hupkes</li>
</ul>
<p>Abstract:<br>Curriculum learning (CL) posits that machine learning models – similar to humans – may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chosen hyperparameters. As such, our results contribute to understanding why CL methods work, but at the same time urge caution when claiming positive results.</p>
<hr>
<h2 id="Predicting-Drug-Solubility-Using-Different-Machine-Learning-Methods-–-Linear-Regression-Model-with-Extracted-Chemical-Features-vs-Graph-Convolutional-Neural-Network"><a href="#Predicting-Drug-Solubility-Using-Different-Machine-Learning-Methods-–-Linear-Regression-Model-with-Extracted-Chemical-Features-vs-Graph-Convolutional-Neural-Network" class="headerlink" title="Predicting Drug Solubility Using Different Machine Learning Methods – Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network"></a>Predicting Drug Solubility Using Different Machine Learning Methods – Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12325">http://arxiv.org/abs/2308.12325</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Ho, Zhao-Heng Yin, Colin Zhang, Henry Overhauser, Kyle Swanson, Yang Ha</li>
</ul>
<p>Abstract:<br>Predicting the solubility of given molecules is an important task in the pharmaceutical industry, and consequently this is a well-studied topic. In this research, we revisited this problem with the advantage of modern computing resources. We applied two machine learning models, a linear regression model and a graph convolutional neural network model, on multiple experimental datasets. Both methods can make reasonable predictions while the GCNN model had the best performance. However, the current GCNN model is a black box, while feature importance analysis from the linear regression model offers more insights into the underlying chemical influences. Using the linear regression model, we show how each functional group affects the overall solubility. Ultimately, knowing how chemical structure influences chemical properties is crucial when designing new drugs. Future work should aim to combine the high performance of GCNNs with the interpretability of linear regression, unlocking new advances in next generation high throughput screening.</p>
<hr>
<h2 id="Self-Supervised-Knowledge-Driven-Deep-Learning-for-3D-Magnetic-Inversion"><a href="#Self-Supervised-Knowledge-Driven-Deep-Learning-for-3D-Magnetic-Inversion" class="headerlink" title="Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion"></a>Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12193">http://arxiv.org/abs/2308.12193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinshuo Li, Zhuo Jia, Wenkai Lu, Cao Song</li>
</ul>
<p>Abstract:<br>The magnetic inversion method is one of the non-destructive geophysical methods, which aims to estimate the subsurface susceptibility distribution from surface magnetic anomaly data. Recently, supervised deep learning methods have been widely utilized in lots of geophysical fields including magnetic inversion. However, these methods rely heavily on synthetic training data, whose performance is limited since the synthetic data is not independently and identically distributed with the field data. Thus, we proposed to realize magnetic inversion by self-supervised deep learning. The proposed self-supervised knowledge-driven 3D magnetic inversion method (SSKMI) learns on the target field data by a closed loop of the inversion and forward models. Given that the parameters of the forward model are preset, SSKMI can optimize the inversion model by minimizing the mean absolute error between observed and re-estimated surface magnetic anomalies. Besides, there is a knowledge-driven module in the proposed inversion model, which makes the deep learning method more explicable. Meanwhile, comparative experiments demonstrate that the knowledge-driven module can accelerate the training of the proposed method and achieve better results. Since magnetic inversion is an ill-pose task, SSKMI proposed to constrain the inversion model by a guideline in the auxiliary loop. The experimental results demonstrate that the proposed method is a reliable magnetic inversion method with outstanding performance.</p>
<hr>
<h2 id="Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques"><a href="#Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques" class="headerlink" title="Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques"></a>Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12192">http://arxiv.org/abs/2308.12192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophie A. Neubauer, Radu Grosu</li>
</ul>
<p>Abstract:<br>This paper presents, in a unified fashion, deterministic as well as statistical Lagrangian-verification techniques. They formally quantify the behavioral robustness of any time-continuous process, formulated as a continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube, algorithms for constructing a tight reachtube, that is, an over-approximation of the set of states reachable within a given time-horizon, and provide guarantees for the reachtube bounds. We compare the usage of the variational equations, associated to the system equations, the mean value theorem, and the Lipschitz constants, in achieving deterministic and statistical guarantees. In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation, to compute the radius of an ellipsoid in an optimal metric, which over-approximates the set of reachable states. In SLR and GoTube, we get statistical guarantees, by using the Lipschitz constants to compute local balls around samples. These are needed to calculate the probability of having found an upper bound, of the true maximum perturbation at every timestep. Our experiments demonstrate the superior performance of Lagrangian techniques, when compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness analysis of various continuous-depth models.</p>
<hr>
<h2 id="Development-and-external-validation-of-a-lung-cancer-risk-estimation-tool-using-gradient-boosting"><a href="#Development-and-external-validation-of-a-lung-cancer-risk-estimation-tool-using-gradient-boosting" class="headerlink" title="Development and external validation of a lung cancer risk estimation tool using gradient-boosting"></a>Development and external validation of a lung cancer risk estimation tool using gradient-boosting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12188">http://arxiv.org/abs/2308.12188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/plbenveniste/lungcancerrisk">https://github.com/plbenveniste/lungcancerrisk</a></li>
<li>paper_authors: Pierre-Louis Benveniste, Julie Alberge, Lei Xing, Jean-Emmanuel Bibault</li>
</ul>
<p>Abstract:<br>Lung cancer is a significant cause of mortality worldwide, emphasizing the importance of early detection for improved survival rates. In this study, we propose a machine learning (ML) tool trained on data from the PLCO Cancer Screening Trial and validated on the NLST to estimate the likelihood of lung cancer occurrence within five years. The study utilized two datasets, the PLCO (n&#x3D;55,161) and NLST (n&#x3D;48,595), consisting of comprehensive information on risk factors, clinical measurements, and outcomes related to lung cancer. Data preprocessing involved removing patients who were not current or former smokers and those who had died of causes unrelated to lung cancer. Additionally, a focus was placed on mitigating bias caused by censored data. Feature selection, hyper-parameter optimization, and model calibration were performed using XGBoost, an ensemble learning algorithm that combines gradient boosting and decision trees. The ML model was trained on the pre-processed PLCO dataset and tested on the NLST dataset. The model incorporated features such as age, gender, smoking history, medical diagnoses, and family history of lung cancer. The model was well-calibrated (Brier score&#x3D;0.044). ROC-AUC was 82% on the PLCO dataset and 70% on the NLST dataset. PR-AUC was 29% and 11% respectively. When compared to the USPSTF guidelines for lung cancer screening, our model provided the same recall with a precision of 13.1% vs. 9.3% on the PLCO dataset and 3.2% vs. 3.1% on the NLST dataset. The developed ML tool provides a freely available web application for estimating the likelihood of developing lung cancer within five years. By utilizing risk factors and clinical data, individuals can assess their risk and make informed decisions regarding lung cancer screening. This research contributes to the efforts in early detection and prevention strategies, aiming to reduce lung cancer-related mortality rates.</p>
<hr>
<h2 id="Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning"><a href="#Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning" class="headerlink" title="Unsupervised anomalies detection in IIoT edge devices networks using federated learning"></a>Unsupervised anomalies detection in IIoT edge devices networks using federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12175">http://arxiv.org/abs/2308.12175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niyomukiza Thamar, Hossam Samy Elsaid Sharara</li>
</ul>
<p>Abstract:<br>In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules. However, some owners are reluctant of availing their data out of the company due to data security concerns. Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself. In this scenario, data is not share over the network for training purpose. Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session. The devices could be chosen at random, and a device can be aborted. The resulting models are sent to the coordinating server and then average models from the devices that finished training. The process is repeated until a desired model accuracy is achieved. By doing this, FL approach solves the privacy problem for IoT&#x2F; IIoT devices that held sensitive data for the owners. In this paper, we leverage the benefits of FL and implemented Fedavg algorithm on a recent dataset that represent the modern IoT&#x2F; IIoT device networks. The results were almost the same as the centralized machine learning approach. We also evaluated some shortcomings of Fedavg such as unfairness that happens during the training when struggling devices do not participate for every stage of training. This inefficient training of local or global model could lead in a high number of false alarms in intrusion detection systems for IoT&#x2F;IIoT gadgets developed using Fedavg. Hence, after evaluating the FedAv deep auto encoder with centralized deep auto encoder ML, we further proposed and designed a Fair Fedavg algorithm that will be evaluated in the future work.</p>
<hr>
<h2 id="Data-driven-decision-focused-surrogate-modeling"><a href="#Data-driven-decision-focused-surrogate-modeling" class="headerlink" title="Data-driven decision-focused surrogate modeling"></a>Data-driven decision-focused surrogate modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12161">http://arxiv.org/abs/2308.12161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ddolab/decfocsurrmod">https://github.com/ddolab/decfocsurrmod</a></li>
<li>paper_authors: Rishabh Gupta, Qi Zhang</li>
</ul>
<p>Abstract:<br>We introduce the concept of decision-focused surrogate modeling for solving computationally challenging nonlinear optimization problems in real-time settings. The proposed data-driven framework seeks to learn a simpler, e.g. convex, surrogate optimization model that is trained to minimize the decision prediction error, which is defined as the difference between the optimal solutions of the original and the surrogate optimization models. The learning problem, formulated as a bilevel program, can be viewed as a data-driven inverse optimization problem to which we apply a decomposition-based solution algorithm from previous work. We validate our framework through numerical experiments involving the optimization of common nonlinear chemical processes such as chemical reactors, heat exchanger networks, and material blending systems. We also present a detailed comparison of decision-focused surrogate modeling with standard data-driven surrogate modeling methods and demonstrate that our approach is significantly more data-efficient while producing simple surrogate models with high decision prediction accuracy.</p>
<hr>
<h2 id="A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models"><a href="#A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models" class="headerlink" title="A Probabilistic Fluctuation based Membership Inference Attack for Generative Models"></a>A Probabilistic Fluctuation based Membership Inference Attack for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12143">http://arxiv.org/abs/2308.12143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang</li>
</ul>
<p>Abstract:<br>Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model’s training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting these trends via analyzing the overall probabilistic fluctuations around given records. We conduct extensive experiments across multiple generative models and datasets, which demonstrate PFAMI can improve the attack success rate (ASR) by about 27.9% when compared with the best baseline.</p>
<hr>
<h2 id="Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models"><a href="#Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models" class="headerlink" title="Masking Strategies for Background Bias Removal in Computer Vision Models"></a>Masking Strategies for Background Bias Removal in Computer Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12127">http://arxiv.org/abs/2308.12127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ananthu-aniraj/masking_strategies_bias_removal">https://github.com/ananthu-aniraj/masking_strategies_bias_removal</a></li>
<li>paper_authors: Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos</li>
</ul>
<p>Abstract:<br>Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.</p>
<hr>
<h2 id="An-Accelerated-Block-Proximal-Framework-with-Adaptive-Momentum-for-Nonconvex-and-Nonsmooth-Optimization"><a href="#An-Accelerated-Block-Proximal-Framework-with-Adaptive-Momentum-for-Nonconvex-and-Nonsmooth-Optimization" class="headerlink" title="An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization"></a>An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12126">http://arxiv.org/abs/2308.12126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weifeng Yang, Wenwen Min</li>
</ul>
<p>Abstract:<br>We propose an accelerated block proximal linear framework with adaptive momentum (ABPL$^+$) for nonconvex and nonsmooth optimization. We analyze the potential causes of the extrapolation step failing in some algorithms, and resolve this issue by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm. Furthermore, we extends our algorithm to any scenario involving updating block variables with positive integers, allowing each cycle to randomly shuffle the update order of the variable blocks. Additionally, under mild assumptions, we prove that ABPL$^+$ can monotonically decrease the function value without strictly restricting the extrapolation parameters and step size, demonstrates the viability and effectiveness of updating these blocks in a random order, and we also more obviously and intuitively demonstrate that the derivative set of the sequence generated by our algorithm is a critical point set. Moreover, we demonstrate the global convergence as well as the linear and sublinear convergence rates of our algorithm by utilizing the Kurdyka-Lojasiewicz (K{\L}) condition. To enhance the effectiveness and flexibility of our algorithm, we also expand the study to the imprecise version of our algorithm and construct an adaptive extrapolation parameter strategy, which improving its overall performance. We apply our algorithm to multiple non-negative matrix factorization with the $\ell_0$ norm, nonnegative tensor decomposition with the $\ell_0$ norm, and perform extensive numerical experiments to validate its effectiveness and efficiency.</p>
<hr>
<h2 id="An-Open-Source-ML-Based-Full-Stack-Optimization-Framework-for-Machine-Learning-Accelerators"><a href="#An-Open-Source-ML-Based-Full-Stack-Optimization-Framework-for-Machine-Learning-Accelerators" class="headerlink" title="An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators"></a>An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12120">http://arxiv.org/abs/2308.12120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Esmaeilzadeh, Soroush Ghodrati, Andrew B. Kahng, Joon Kyung Kim, Sean Kinzer, Sayak Kundu, Rohan Mahapatra, Susmita Dey Manasi, Sachin Sapatnekar, Zhiang Wang, Ziqing Zeng</li>
</ul>
<p>Abstract:<br>Parameterizable machine learning (ML) accelerators are the product of recent breakthroughs in ML. To fully enable their design space exploration (DSE), we propose a physical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN ML algorithms. It adopts a unified approach that combines backend power, performance, and area (PPA) analysis with frontend performance simulation, thereby achieving a realistic estimation of both backend PPA and system metrics such as runtime and energy. In addition, our framework includes a fully automated DSE technique, which optimizes backend and system metrics through an automated search of architectural and backend parameters. Experimental studies show that our approach consistently predicts backend PPA and system metrics with an average 7% or less prediction error for the ASIC implementation of two deep learning accelerator platforms, VTA and VeriGOOD-ML, in both a commercial 12 nm process and a research-oriented 45 nm process.</p>
<hr>
<h2 id="Less-is-More-–-Towards-parsimonious-multi-task-models-using-structured-sparsity"><a href="#Less-is-More-–-Towards-parsimonious-multi-task-models-using-structured-sparsity" class="headerlink" title="Less is More – Towards parsimonious multi-task models using structured sparsity"></a>Less is More – Towards parsimonious multi-task models using structured sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12114">http://arxiv.org/abs/2308.12114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richa Upadhyay, Ronald Phlypo, Rajkumar Saini, Marcus Liwicki</li>
</ul>
<p>Abstract:<br>Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups. This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model. Sparsifying the model during training helps decrease the model’s memory footprint, computation requirements, and prediction time during inference. We use channel-wise l1&#x2F;l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN). This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks. We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ. We also investigate how changing the sparsification degree impacts both the performance of the model and the sparsity of groups.</p>
<hr>
<h2 id="Generalized-Continual-Category-Discovery"><a href="#Generalized-Continual-Category-Discovery" class="headerlink" title="Generalized Continual Category Discovery"></a>Generalized Continual Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12112">http://arxiv.org/abs/2308.12112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Marczak, Grzegorz Rypeść, Sebastian Cygert, Tomasz Trzciński, Bartłomiej Twardowski</li>
</ul>
<p>Abstract:<br>Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of novel classes are present. In light of these limitations, we propose a method that incorporates both supervised and unsupervised signals and mitigates the forgetting through the use of centroid adaptation. Our method surpasses strong CL methods adopted for GCD techniques and presents a superior representation learning performance.</p>
<hr>
<h2 id="Constrained-Stein-Variational-Trajectory-Optimization"><a href="#Constrained-Stein-Variational-Trajectory-Optimization" class="headerlink" title="Constrained Stein Variational Trajectory Optimization"></a>Constrained Stein Variational Trajectory Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12110">http://arxiv.org/abs/2308.12110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Power, Dmitry Berenson</li>
</ul>
<p>Abstract:<br>We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with arbitrary equality and inequality constraints and includes a novel particle resampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constrained tasks, such as a 7DoF wrench manipulation task, where CSVTO succeeds in 20&#x2F;20 trials vs 13&#x2F;20 for the closest baseline. Our results demonstrate that generating diverse constraint-satisfying trajectories improves robustness to disturbances and initialization over baselines.</p>
<hr>
<h2 id="Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient"><a href="#Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient" class="headerlink" title="Quantifying degeneracy in singular models via the learning coefficient"></a>Quantifying degeneracy in singular models via the learning coefficient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12108">http://arxiv.org/abs/2308.12108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edmundlth/scalable_learning_coefficient_with_sgld">https://github.com/edmundlth/scalable_learning_coefficient_with_sgld</a></li>
<li>paper_authors: Edmund Lau, Daniel Murfet, Susan Wei</li>
</ul>
<p>Abstract:<br>Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of “flat” directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.</p>
<hr>
<h2 id="Cached-Operator-Reordering-A-Unified-View-for-Fast-GNN-Training"><a href="#Cached-Operator-Reordering-A-Unified-View-for-Fast-GNN-Training" class="headerlink" title="Cached Operator Reordering: A Unified View for Fast GNN Training"></a>Cached Operator Reordering: A Unified View for Fast GNN Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12093">http://arxiv.org/abs/2308.12093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Bazinska, Andrei Ivanov, Tal Ben-Nun, Nikoli Dryden, Maciej Besta, Siyuan Shen, Torsten Hoefler</li>
</ul>
<p>Abstract:<br>Graph Neural Networks (GNNs) are a powerful tool for handling structured graph data and addressing tasks such as node classification, graph classification, and clustering. However, the sparse nature of GNN computation poses new challenges for performance optimization compared to traditional deep neural networks. We address these challenges by providing a unified view of GNN computation, I&#x2F;O, and memory. By analyzing the computational graphs of the Graph Convolutional Network (GCN) and Graph Attention (GAT) layers – two widely used GNN layers – we propose alternative computation strategies. We present adaptive operator reordering with caching, which achieves a speedup of up to 2.43x for GCN compared to the current state-of-the-art. Furthermore, an exploration of different caching schemes for GAT yields a speedup of up to 1.94x. The proposed optimizations save memory, are easily implemented across various hardware platforms, and have the potential to alleviate performance bottlenecks in training large-scale GNN models.</p>
<hr>
<h2 id="Stabilizing-RNN-Gradients-through-Pre-training"><a href="#Stabilizing-RNN-Gradients-through-Pre-training" class="headerlink" title="Stabilizing RNN Gradients through Pre-training"></a>Stabilizing RNN Gradients through Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12075">http://arxiv.org/abs/2308.12075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Herranz-Celotti, Jean Rouat</li>
</ul>
<p>Abstract:<br>Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponential explosion that emerges from counting gradient paths in a rectangular grid in depth and time. We propose a new approach to mitigate this issue, that consists on giving a weight of a half to the time and depth contributions to the gradient, instead of the classical weight of one. Our empirical results confirm that pre-training both feed-forward and recurrent networks to fulfill the LSC often results in improved final performance across models. This study contributes to the field by providing a means to stabilize networks of any complexity. Our approach can be implemented as an additional step before pre-training on large augmented datasets, and as an alternative to finding stable initializations analytically.</p>
<hr>
<h2 id="Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning"><a href="#Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning" class="headerlink" title="Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning"></a>Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12069">http://arxiv.org/abs/2308.12069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ni Dang, Tao Shi, Zengjie Zhang, Wanxin Jin, Marion Leibold, Martin Buss</li>
</ul>
<p>Abstract:<br>The driving style of an Autonomous Vehicle (AV) refers to how it behaves and interacts with other AVs. In a multi-vehicle autonomous driving system, an AV capable of identifying the driving styles of its nearby AVs can reliably evaluate the risk of collisions and make more reasonable driving decisions. However, there has not been a consistent definition of driving styles for an AV in the literature, although it is considered that the driving style is encoded in the AV’s trajectories and can be identified using Maximum Entropy Inverse Reinforcement Learning (ME-IRL) methods as a cost function. Nevertheless, an important indicator of the driving style, i.e., how an AV reacts to its nearby AVs, is not fully incorporated in the feature design of previous ME-IRL methods. In this paper, we describe the driving style as a cost function of a series of weighted features. We design additional novel features to capture the AV’s reaction-aware characteristics. Then, we identify the driving styles from the demonstration trajectories generated by the Stochastic Model Predictive Control (SMPC) using a modified ME-IRL method with our newly proposed features. The proposed method is validated using MATLAB simulation and an off-the-shelf experiment.</p>
<hr>
<h2 id="InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4"><a href="#InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4" class="headerlink" title="InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"></a>InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12067">http://arxiv.org/abs/2308.12067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun</li>
</ul>
<p>Abstract:<br>Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient to enable multimodal large language models to generate better output.</p>
<hr>
<h2 id="Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference"><a href="#Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference" class="headerlink" title="Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference"></a>Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12066">http://arxiv.org/abs/2308.12066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ranggi Hwang, Jianyu Wei, Shijie Cao, Changho Hwang, Xiaohu Tang, Ting Cao, Mao Yang, Minsoo Rhu</li>
</ul>
<p>Abstract:<br>Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE’s high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE’s memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs our novel pre-gating function which alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE is able to improve performance, reduce GPU memory consumption, while also maintaining the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.</p>
<hr>
<h2 id="Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers"><a href="#Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers" class="headerlink" title="Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers"></a>Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12065">http://arxiv.org/abs/2308.12065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Zoppi, Andrea Ceccarelli, Andrea Bondavalli</li>
</ul>
<p>Abstract:<br>Machine Learning (ML) algorithms that perform classification may predict the wrong class, experiencing misclassifications. It is well-known that misclassifications may have cascading effects on the encompassing system, possibly resulting in critical failures. This paper proposes SPROUT, a Safety wraPper thROugh ensembles of UncertainTy measures, which suspects misclassifications by computing uncertainty measures on the inputs and outputs of a black-box classifier. If a misclassification is detected, SPROUT blocks the propagation of the output of the classifier to the encompassing system. The resulting impact on safety is that SPROUT transforms erratic outputs (misclassifications) into data omission failures, which can be easily managed at the system level. SPROUT has a broad range of applications as it fits binary and multi-class classification, comprising image and tabular datasets. We experimentally show that SPROUT always identifies a huge fraction of the misclassifications of supervised classifiers, and it is able to detect all misclassifications in specific cases. SPROUT implementation contains pre-trained wrappers, it is publicly available and ready to be deployed with minimal effort.</p>
<hr>
<h2 id="HarvestNet-A-Dataset-for-Detecting-Smallholder-Farming-Activity-Using-Harvest-Piles-and-Remote-Sensing"><a href="#HarvestNet-A-Dataset-for-Detecting-Smallholder-Farming-Activity-Using-Harvest-Piles-and-Remote-Sensing" class="headerlink" title="HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing"></a>HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12061">http://arxiv.org/abs/2308.12061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Xu, Amna Elmustafa, Liya Weldegebriel, Emnet Negash, Richard Lee, Chenlin Meng, Stefano Ermon, David Lobell</li>
</ul>
<p>Abstract:<br>Small farms contribute to a large share of the productive land in developing countries. In regions such as sub-Saharan Africa, where 80% of farms are small (under 2 ha in size), the task of mapping smallholder cropland is an important part of tracking sustainability measures such as crop productivity. However, the visually diverse and nuanced appearance of small farms has limited the effectiveness of traditional approaches to cropland mapping. Here we introduce a new approach based on the detection of harvest piles characteristic of many smallholder systems throughout the world. We present HarvestNet, a dataset for mapping the presence of farms in the Ethiopian regions of Tigray and Amhara during 2020-2023, collected using expert knowledge and satellite images, totaling 7k hand-labeled images and 2k ground collected labels. We also benchmark a set of baselines including SOTA models in remote sensing with our best models having around 80% classification performance on hand labelled data and 90%, 98% accuracy on ground truth data for Tigray, Amhara respectively. We also perform a visual comparison with a widely used pre-existing coverage map and show that our model detects an extra 56,621 hectares of cropland in Tigray. We conclude that remote sensing of harvest piles can contribute to more timely and accurate cropland assessments in food insecure region.</p>
<hr>
<h2 id="Manipulating-Embeddings-of-Stable-Diffusion-Prompts"><a href="#Manipulating-Embeddings-of-Stable-Diffusion-Prompts" class="headerlink" title="Manipulating Embeddings of Stable Diffusion Prompts"></a>Manipulating Embeddings of Stable Diffusion Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12059">http://arxiv.org/abs/2308.12059</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/webis-de/arxiv23-prompt-embedding-manipulation">https://github.com/webis-de/arxiv23-prompt-embedding-manipulation</a></li>
<li>paper_authors: Niklas Deckers, Julia Peters, Martin Potthast</li>
</ul>
<p>Abstract:<br>Generative text-to-image models such as Stable Diffusion allow users to generate images based on a textual description, the prompt. Changing the prompt is still the primary means for the user to change a generated image as desired. However, changing the image by reformulating the prompt remains a difficult process of trial and error, which has led to the emergence of prompt engineering as a new field of research. We propose and analyze methods to change the embedding of a prompt directly instead of the prompt text. It allows for more fine-grained and targeted control that takes into account user intentions. Our approach treats the generative text-to-image model as a continuous function and passes gradients between the image space and the prompt embedding space. By addressing different user interaction problems, we can apply this idea in three scenarios: (1) Optimization of a metric defined in image space that could measure, for example, image style. (2) Assistance of users in creative tasks by enabling them to navigate the image space along a selection of directions of “near” prompt embeddings. (3) Changing the embedding of the prompt to include information that the user has seen in a particular seed but finds difficult to describe in the prompt. Our experiments demonstrate the feasibility of the described methods.</p>
<hr>
<h2 id="Sample-Complexity-of-Robust-Learning-against-Evasion-Attacks"><a href="#Sample-Complexity-of-Robust-Learning-against-Evasion-Attacks" class="headerlink" title="Sample Complexity of Robust Learning against Evasion Attacks"></a>Sample Complexity of Robust Learning against Evasion Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12054">http://arxiv.org/abs/2308.12054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascale Gourdeau</li>
</ul>
<p>Abstract:<br>It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. One of the fundamental problems in adversarial machine learning is to quantify how much training data is needed in the presence of evasion attacks, where data is corrupted at test time. In this thesis, we work with the exact-in-the-ball notion of robustness and study the feasibility of adversarially robust learning from the perspective of learning theory, considering sample complexity.   We first explore the setting where the learner has access to random examples only, and show that distributional assumptions are essential. We then focus on learning problems with distributions on the input data that satisfy a Lipschitz condition and show that robustly learning monotone conjunctions has sample complexity at least exponential in the adversary’s budget (the maximum number of bits it can perturb on each input). However, if the adversary is restricted to perturbing $O(\log n)$ bits, then one can robustly learn conjunctions and decision lists w.r.t. log-Lipschitz distributions.   We then study learning models where the learner is given more power. We first consider local membership queries, where the learner can query the label of points near the training sample. We show that, under the uniform distribution, the exponential dependence on the adversary’s budget to robustly learn conjunctions remains inevitable. We then introduce a local equivalence query oracle, which returns whether the hypothesis and target concept agree in a given region around a point in the training sample, and a counterexample if it exists. We show that if the query radius is equal to the adversary’s budget, we can develop robust empirical risk minimization algorithms in the distribution-free setting. We give general query complexity upper and lower bounds, as well as for concrete concept classes.</p>
<hr>
<h2 id="Layer-wise-Feedback-Propagation"><a href="#Layer-wise-Feedback-Propagation" class="headerlink" title="Layer-wise Feedback Propagation"></a>Layer-wise Feedback Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12053">http://arxiv.org/abs/2308.12053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leander Weber, Jim Berend, Alexander Binder, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</li>
</ul>
<p>Abstract:<br>In this paper, we present Layer-wise Feedback Propagation (LFP), a novel training approach for neural-network-like predictors that utilizes explainability, specifically Layer-wise Relevance Propagation(LRP), to assign rewards to individual connections based on their respective contributions to solving a given task. This differs from traditional gradient descent, which updates parameters towards anestimated loss minimum. LFP distributes a reward signal throughout the model without the need for gradient computations. It then strengthens structures that receive positive feedback while reducingthe influence of structures that receive negative feedback. We establish the convergence of LFP theoretically and empirically, and demonstrate its effectiveness in achieving comparable performance to gradient descent on various models and datasets. Notably, LFP overcomes certain limitations associated with gradient-based methods, such as reliance on meaningful derivatives. We further investigate how the different LRP-rules can be extended to LFP, what their effects are on training, as well as potential applications, such as training models with no meaningful derivatives, e.g., step-function activated Spiking Neural Networks (SNNs), or for transfer learning, to efficiently utilize existing knowledge.</p>
<hr>
<h2 id="A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks"><a href="#A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks" class="headerlink" title="A multiobjective continuation method to compute the regularization path of deep neural networks"></a>A multiobjective continuation method to compute the regularization path of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12044">http://arxiv.org/abs/2308.12044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aamakor/continuation-method">https://github.com/aamakor/continuation-method</a></li>
<li>paper_authors: Augustina C. Amakor, Konstantin Sonntag, Sebastian Peitz</li>
</ul>
<p>Abstract:<br>Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto front for the above-mentioned objectives in a very efficient manner. We present numerical examples using both deterministic and stochastic gradients. We furthermore demonstrate that knowledge of the regularization path allows for a well-generalizing network parametrization.</p>
<hr>
<h2 id="IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning"><a href="#IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning" class="headerlink" title="IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning"></a>IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12043">http://arxiv.org/abs/2308.12043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feiyuzhang98/increlora">https://github.com/feiyuzhang98/increlora</a></li>
<li>paper_authors: Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, Yiming Qian</li>
</ul>
<p>Abstract:<br>With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it is not limited by the initial number of training parameters, and each parameter matrix has a higher rank upper bound for the same training overhead. We conduct extensive experiments on GLUE to demonstrate the effectiveness of IncreLoRA. The results show that our method owns higher parameter efficiency, especially when under the low-resource settings where our method significantly outperforms the baselines. Our code is publicly available.</p>
<hr>
<h2 id="CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures"><a href="#CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures" class="headerlink" title="CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures"></a>CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12031">http://arxiv.org/abs/2308.12031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Gherardini, Varun Ravi Varma, Karol Capala, Roger Woods, Jose Sousa</li>
</ul>
<p>Abstract:<br>The availability of large data sets is providing an impetus for driving current artificial intelligent developments. There are, however, challenges for developing solutions with small data sets due to practical and cost-effective deployment and the opacity of deep learning models. The Comprehensive Abstraction and Classification Tool for Uncovering Structures called CACTUS is presented for improved secure analytics by effectively employing explainable artificial intelligence. It provides additional support for categorical attributes, preserving their original meaning, optimising memory usage, and speeding up the computation through parallelisation. It shows to the user the frequency of the attributes in each class and ranks them by their discriminative power. Its performance is assessed by application to the Wisconsin diagnostic breast cancer and Thyroid0387 data sets.</p>
<hr>
<h2 id="Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning"><a href="#Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning" class="headerlink" title="Prompt-Based Length Controlled Generation with Reinforcement Learning"></a>Prompt-Based Length Controlled Generation with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12030">http://arxiv.org/abs/2308.12030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, Qun Liu</li>
</ul>
<p>Abstract:<br>Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. We believe this length-controllable ability can provide more potentials towards the era of LLMs.</p>
<hr>
<h2 id="A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning"><a href="#A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning" class="headerlink" title="A Scale-Invariant Task Balancing Approach for Multi-Task Learning"></a>A Scale-Invariant Task Balancing Approach for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12029">http://arxiv.org/abs/2308.12029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baijiong Lin, Weisen Jiang, Feiyang Ye, Yu Zhang, Pengguang Chen, Ying-Cong Chen, Shu Liu</li>
</ul>
<p>Abstract:<br>Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss&#x2F;gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.</p>
<hr>
<h2 id="Bias-Aware-Minimisation-Understanding-and-Mitigating-Estimator-Bias-in-Private-SGD"><a href="#Bias-Aware-Minimisation-Understanding-and-Mitigating-Estimator-Bias-in-Private-SGD" class="headerlink" title="Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD"></a>Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12018">http://arxiv.org/abs/2308.12018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Knolle, Robert Dorfman, Alexander Ziller, Daniel Rueckert, Georgios Kaissis</li>
</ul>
<p>Abstract:<br>Differentially private SGD (DP-SGD) holds the promise of enabling the safe and responsible application of machine learning to sensitive datasets. However, DP-SGD only provides a biased, noisy estimate of a mini-batch gradient. This renders optimisation steps less effective and limits model utility as a result. With this work, we show a connection between per-sample gradient norms and the estimation bias of the private gradient oracle used in DP-SGD. Here, we propose Bias-Aware Minimisation (BAM) that allows for the provable reduction of private gradient estimator bias. We show how to efficiently compute quantities needed for BAM to scale to large neural networks and highlight similarities to closely related methods such as Sharpness-Aware Minimisation. Finally, we provide empirical evidence that BAM not only reduces bias but also substantially improves privacy-utility trade-offs on the CIFAR-10, CIFAR-100, and ImageNet-32 datasets.</p>
<hr>
<h2 id="Graph-Neural-Stochastic-Differential-Equations"><a href="#Graph-Neural-Stochastic-Differential-Equations" class="headerlink" title="Graph Neural Stochastic Differential Equations"></a>Graph Neural Stochastic Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12316">http://arxiv.org/abs/2308.12316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard Bergna, Felix Opolka, Pietro Liò, Jose Miguel Hernandez-Lobato</li>
</ul>
<p>Abstract:<br>We present a novel model Graph Neural Stochastic Differential Equations (Graph Neural SDEs). This technique enhances the Graph Neural Ordinary Differential Equations (Graph Neural ODEs) by embedding randomness into data representation using Brownian motion. This inclusion allows for the assessment of prediction uncertainty, a crucial aspect frequently missed in current models. In our framework, we spotlight the \textit{Latent Graph Neural SDE} variant, demonstrating its effectiveness. Through empirical studies, we find that Latent Graph Neural SDEs surpass conventional models like Graph Convolutional Networks and Graph Neural ODEs, especially in confidence prediction, making them superior in handling out-of-distribution detection across both static and spatio-temporal contexts.</p>
<hr>
<h2 id="MKL-L-0-1-SVM"><a href="#MKL-L-0-1-SVM" class="headerlink" title="MKL-$L_{0&#x2F;1}$-SVM"></a>MKL-$L_{0&#x2F;1}$-SVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12016">http://arxiv.org/abs/2308.12016</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maxis1718/simplemkl">https://github.com/maxis1718/simplemkl</a></li>
<li>paper_authors: Bin Zhu, Yijie Shi</li>
</ul>
<p>Abstract:<br>This paper presents a Multiple Kernel Learning (abbreviated as MKL) framework for the Support Vector Machine (SVM) with the $(0, 1)$ loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver to deal with the nonconvex and nonsmooth optimization problem. Extensive numerical experiments on synthetic and real datasets show that the performance of our MKL-$L_{0&#x2F;1}$-SVM is comparable with the one of the leading approaches called SimpleMKL developed by Rakotomamonjy, Bach, Canu, and Grandvalet [Journal of Machine Learning Research, vol. 9, pp. 2491-2521, 2008].</p>
<hr>
<h2 id="Quantum-Noise-driven-Generative-Diffusion-Models"><a href="#Quantum-Noise-driven-Generative-Diffusion-Models" class="headerlink" title="Quantum-Noise-driven Generative Diffusion Models"></a>Quantum-Noise-driven Generative Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12013">http://arxiv.org/abs/2308.12013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Parigi, Stefano Martina, Filippo Caruso</li>
</ul>
<p>Abstract:<br>Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a very remarkably beneficial key ingredient to generate much more complex probability distributions that would be difficult or even impossible to express classically, and from which a quantum processor might sample more efficiently than a classical one. Therefore, our results are expected to pave the way for new quantum-inspired or quantum-based generative diffusion algorithms addressing more powerfully classical tasks as data generation&#x2F;prediction with widespread real-world applications ranging from climate forecasting to neuroscience, from traffic flow analysis to financial forecasting.</p>
<hr>
<h2 id="Neural-oscillators-for-magnetic-hysteresis-modeling"><a href="#Neural-oscillators-for-magnetic-hysteresis-modeling" class="headerlink" title="Neural oscillators for magnetic hysteresis modeling"></a>Neural oscillators for magnetic hysteresis modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12002">http://arxiv.org/abs/2308.12002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Chandra, Taniya Kapoor, Bram Daniels, Mitrofan Curti, Koen Tiels, Daniel M. Tartakovsky, Elena A. Lomonova</li>
</ul>
<p>Abstract:<br>Hysteresis is a ubiquitous phenomenon in science and engineering; its modeling and identification are crucial for understanding and optimizing the behavior of various systems. We develop an ordinary differential equation-based recurrent neural network (RNN) approach to model and quantify the hysteresis, which manifests itself in sequentiality and history-dependence. Our neural oscillator, HystRNN, draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states. The performance of HystRNN is evaluated to predict generalized scenarios, involving first-order reversal curves and minor loops. The findings show the ability of HystRNN to generalize its behavior to previously untrained regions, an essential feature that hysteresis models must have. This research highlights the advantage of neural oscillators over the traditional RNN-based methods in capturing complex hysteresis patterns in magnetic materials, where traditional rate-dependent methods are inadequate to capture intrinsic nonlinearity.</p>
<hr>
<h2 id="Trustworthy-Representation-Learning-Across-Domains"><a href="#Trustworthy-Representation-Learning-Across-Domains" class="headerlink" title="Trustworthy Representation Learning Across Domains"></a>Trustworthy Representation Learning Across Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12315">http://arxiv.org/abs/2308.12315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronghang Zhu, Dongliang Guo, Daiqing Qi, Zhixuan Chu, Xiang Yu, Sheng Li</li>
</ul>
<p>Abstract:<br>As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.</p>
<hr>
<h2 id="On-Uniformly-Optimal-Algorithms-for-Best-Arm-Identification-in-Two-Armed-Bandits-with-Fixed-Budget"><a href="#On-Uniformly-Optimal-Algorithms-for-Best-Arm-Identification-in-Two-Armed-Bandits-with-Fixed-Budget" class="headerlink" title="On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget"></a>On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12000">http://arxiv.org/abs/2308.12000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-An Wang, Kaito Ariu, Alexandre Proutiere</li>
</ul>
<p>Abstract:<br>We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.</p>
<hr>
<h2 id="Relational-Concept-Based-Models"><a href="#Relational-Concept-Based-Models" class="headerlink" title="Relational Concept Based Models"></a>Relational Concept Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11991">http://arxiv.org/abs/2308.11991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pietro Barbiero, Francesco Giannini, Gabriele Ciravegna, Michelangelo Diligenti, Giuseppe Marra</li>
</ul>
<p>Abstract:<br>The design of interpretable deep learning models working in relational domains poses an open challenge: interpretable deep learning methods, such as Concept-Based Models (CBMs), are not designed to solve relational problems, while relational models are not as interpretable as CBMs. To address this problem, we propose Relational Concept-Based Models, a family of relational deep learning methods providing interpretable task predictions. Our experiments, ranging from image classification to link prediction in knowledge graphs, show that relational CBMs (i) match generalization performance of existing relational black-boxes (as opposed to non-relational CBMs), (ii) support the generation of quantified concept-based explanations, (iii) effectively respond to test-time interventions, and (iv) withstand demanding settings including out-of-distribution scenarios, limited training data regimes, and scarce concept supervisions.</p>
<hr>
<h2 id="Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks"><a href="#Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks" class="headerlink" title="Will More Expressive Graph Neural Networks do Better on Generative Tasks?"></a>Will More Expressive Graph Neural Networks do Better on Generative Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11978">http://arxiv.org/abs/2308.11978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiandong Zou, Xiangyu Zhao, Pietro Liò, Yiren Zhao</li>
</ul>
<p>Abstract:<br>Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZINC-250k dataset. Through our extensive experiments, we demonstrate that advanced GNNs can indeed improve the performance of GCPN and GraphAF on molecular generation tasks, but GNN expressiveness is not a necessary condition for a good GNN-based generative model. Moreover, we show that GCPN and GraphAF with advanced GNNs can achieve state-of-the-art results across 17 other non-GNN-based graph generative approaches, such as variational autoencoders and Bayesian optimisation models, on the proposed molecular generative objectives (DRD2, Median1, Median2), which are important metrics for de-novo molecular design.</p>
<hr>
<h2 id="Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression"><a href="#Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression" class="headerlink" title="Approximating Score-based Explanation Techniques Using Conformal Regression"></a>Approximating Score-based Explanation Techniques Using Conformal Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11975">http://arxiv.org/abs/2308.11975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Alkhatib, Henrik Boström, Sofiane Ennadir, Ulf Johansson</li>
</ul>
<p>Abstract:<br>Score-based explainable machine-learning techniques are often used to understand the logic behind black-box models. However, such explanation techniques are often computationally expensive, which limits their application in time-critical contexts. Therefore, we propose and investigate the use of computationally less costly regression models for approximating the output of score-based explanation techniques, such as SHAP. Moreover, validity guarantees for the approximated values are provided by the employed inductive conformal prediction framework. We propose several non-conformity measures designed to take the difficulty of approximating the explanations into account while keeping the computational cost low. We present results from a large-scale empirical investigation, in which the approximate explanations generated by our proposed models are evaluated with respect to efficiency (interval size). The results indicate that the proposed method can significantly improve execution time compared to the fast version of SHAP, TreeSHAP. The results also suggest that the proposed method can produce tight intervals, while providing validity guarantees. Moreover, the proposed approach allows for comparing explanations of different approximation methods and selecting a method based on how informative (tight) are the predicted intervals.</p>
<hr>
<h2 id="EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE"><a href="#EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE" class="headerlink" title="EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE"></a>EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11971">http://arxiv.org/abs/2308.11971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Chen, Longteng Guo, Jia Sun, Shuai Shao, Zehuan Yuan, Liang Lin, Dongyu Zhang</li>
</ul>
<p>Abstract:<br>Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy to scale up, enabling better downstream performance with fewer resources and faster training speed. Despite its simplicity, EVE achieves state-of-the-art performance on various vision-language downstream tasks, including visual question answering, visual reasoning, and image-text retrieval.</p>
<hr>
<h2 id="Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification"><a href="#Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification" class="headerlink" title="Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification"></a>Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11969">http://arxiv.org/abs/2308.11969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Lambert, Pauline Roca, Florence Forbes, Senan Doyle, Michel Dojat</li>
</ul>
<p>Abstract:<br>The burden of liver tumors is important, ranking as the fourth leading cause of cancer mortality. In case of hepatocellular carcinoma (HCC), the delineation of liver and tumor on contrast-enhanced magnetic resonance imaging (CE-MRI) is performed to guide the treatment strategy. As this task is time-consuming, needs high expertise and could be subject to inter-observer variability there is a strong need for automatic tools. However, challenges arise from the lack of available training data, as well as the high variability in terms of image resolution and MRI sequence. In this work we propose to compare two different pipelines based on anisotropic models to obtain the segmentation of the liver and tumors. The first pipeline corresponds to a baseline multi-class model that performs the simultaneous segmentation of the liver and tumor classes. In the second approach, we train two distinct binary models, one segmenting the liver only and the other the tumors. Our results show that both pipelines exhibit different strengths and weaknesses. Moreover we propose an uncertainty quantification strategy allowing the identification of potential false positive tumor lesions. Both solutions were submitted to the MICCAI 2023 Atlas challenge regarding liver and tumor segmentation.</p>
<hr>
<h2 id="Maintaining-Plasticity-via-Regenerative-Regularization"><a href="#Maintaining-Plasticity-via-Regenerative-Regularization" class="headerlink" title="Maintaining Plasticity via Regenerative Regularization"></a>Maintaining Plasticity via Regenerative Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11958">http://arxiv.org/abs/2308.11958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabh Kumar, Henrik Marklund, Benjamin Van Roy</li>
</ul>
<p>Abstract:<br>In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates plasticity loss. We additionally find that our regularization term reduces parameter magnitudes and maintains a high effective feature rank.</p>
<hr>
<h2 id="When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation"><a href="#When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation" class="headerlink" title="When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation"></a>When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11953">http://arxiv.org/abs/2308.11953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Huang, Geng Tian, Ming Tang</li>
</ul>
<p>Abstract:<br>Federated learning (FL) enables collaborative model training across distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can be computationally expensive as the clients need to train the entire model multiple times. SplitFed learning (SFL) is a recent distributed approach that alleviates computation workload at the client device by splitting the model at a cut layer into two parts, where clients only need to train part of the model. However, SFL still suffers from the \textit{client drift} problem when clients’ data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This algorithm incorporates MiniBatch SGD into SFL, where the clients train the client-side model in an FL fashion while the server trains the server-side model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and show that the bound of the expected loss can be obtained by analyzing the expected server-side and client-side model updates, respectively. The server-side updates do not depend on the non-IID degree of the clients’ datasets and can potentially mitigate client drift. However, the client-side model relies on the non-IID degree and can be optimized by properly choosing the cut layer. Perhaps counter-intuitive, our empirical result shows that a latter position of the cut layer leads to a smaller average gradient divergence and a better algorithm performance. Moreover, numerical results show that MiniBatch-SFL achieves higher accuracy than conventional SFL and FL. The accuracy improvement can be up to 24.1% and 17.1% with highly non-IID data, respectively.</p>
<hr>
<h2 id="Multi-scale-Transformer-Pyramid-Networks-for-Multivariate-Time-Series-Forecasting"><a href="#Multi-scale-Transformer-Pyramid-Networks-for-Multivariate-Time-Series-Forecasting" class="headerlink" title="Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting"></a>Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11946">http://arxiv.org/abs/2308.11946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhang, Rui Wu, Sergiu M. Dascalu, Frederick C. Harris Jr</li>
</ul>
<p>Abstract:<br>Multivariate Time Series (MTS) forecasting involves modeling temporal dependencies within historical records. Transformers have demonstrated remarkable performance in MTS forecasting due to their capability to capture long-term dependencies. However, prior work has been confined to modeling temporal dependencies at either a fixed scale or multiple scales that exponentially increase (most with base 2). This limitation hinders their effectiveness in capturing diverse seasonalities, such as hourly and daily patterns. In this paper, we introduce a dimension invariant embedding technique that captures short-term temporal dependencies and projects MTS data into a higher-dimensional space, while preserving the dimensions of time steps and variables in MTS data. Furthermore, we present a novel Multi-scale Transformer Pyramid Network (MTPNet), specifically designed to effectively capture temporal dependencies at multiple unconstrained scales. The predictions are inferred from multi-scale latent representations obtained from transformers at various scales. Extensive experiments on nine benchmark datasets demonstrate that the proposed MTPNet outperforms recent state-of-the-art methods.</p>
<hr>
<h2 id="RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching"><a href="#RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching" class="headerlink" title="RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching"></a>RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11943">http://arxiv.org/abs/2308.11943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steve Vott, Adam M. Lehavi</li>
</ul>
<p>Abstract:<br>The Ramsey number is the minimum number of nodes, $n &#x3D; R(s, t)$, such that all undirected simple graphs of order $n$, contain a clique of order $s$, or an independent set of order $t$. This paper explores the application of a best first search algorithm and reinforcement learning (RL) techniques to find counterexamples to specific Ramsey numbers. We incrementally improve over prior search methods such as random search by introducing a graph vectorization and deep neural network (DNN)-based heuristic, which gauge the likelihood of a graph being a counterexample. The paper also proposes algorithmic optimizations to confine a polynomial search runtime. This paper does not aim to present new counterexamples but rather introduces and evaluates a framework supporting Ramsey counterexample exploration using other heuristics. Code and methods are made available through a PyPI package and GitHub repository.</p>
<hr>
<h2 id="Audio-Generation-with-Multiple-Conditional-Diffusion-Model"><a href="#Audio-Generation-with-Multiple-Conditional-Diffusion-Model" class="headerlink" title="Audio Generation with Multiple Conditional Diffusion Model"></a>Audio Generation with Multiple Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11940">http://arxiv.org/abs/2308.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifang Guo, Jianguo Mao, Rui Tao, Long Yan, Kazushige Ouchi, Hong Liu, Xiangdong Wang</li>
</ul>
<p>Abstract:<br>Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a series of evaluation metrics to evaluate the controllability performance. Experimental results demonstrate that our model successfully achieves fine-grained control to accomplish controllable audio generation. Audio samples and our dataset are publicly available at <a target="_blank" rel="noopener" href="https://conditionaudiogen.github.io/conditionaudiogen/">https://conditionaudiogen.github.io/conditionaudiogen/</a></p>
<hr>
<h2 id="Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series"><a href="#Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series" class="headerlink" title="Retail Demand Forecasting: A Comparative Study for Multivariate Time Series"></a>Retail Demand Forecasting: A Comparative Study for Multivariate Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11939">http://arxiv.org/abs/2308.11939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sabbirul Haque, Md Shahedul Amin, Jonayet Miah</li>
</ul>
<p>Abstract:<br>Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.</p>
<hr>
<h2 id="System-Identification-for-Continuous-time-Linear-Dynamical-Systems"><a href="#System-Identification-for-Continuous-time-Linear-Dynamical-Systems" class="headerlink" title="System Identification for Continuous-time Linear Dynamical Systems"></a>System Identification for Continuous-time Linear Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11933">http://arxiv.org/abs/2308.11933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Halmos, Jonathan Pillow, David A. Knowles</li>
</ul>
<p>Abstract:<br>The problem of system identification for the Kalman filter, relying on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has largely been studied assuming that observations are sampled at equally-spaced time points. However, in many applications this is a restrictive and unrealistic assumption. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates which do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure which estimates the parameters of the SDE, naturally incorporating irregularly sampled measurements. Generalizing the learning of latent linear dynamical systems (LDS) to continuous-time may extend the use of the hybrid Kalman filter to data which is not regularly sampled or has intermittent missing values, and can extend the power of non-linear system identification methods such as switching LDS (SLDS), which rely on EM for the linear discrete-time Kalman filter as a sub-unit for learning locally linearized behavior of a non-linear system. We apply the method by learning the parameters of a latent, multivariate Fokker-Planck SDE representing a toggle-switch genetic circuit using biologically realistic parameters, and compare the efficacy of learning relative to the discrete-time Kalman filter as the step-size irregularity and spectral-radius of the dynamics-matrix increases.</p>
<hr>
<h2 id="Dynamic-landslide-susceptibility-mapping-over-recent-three-decades-to-uncover-variations-in-landslide-causes-in-subtropical-urban-mountainous-areas"><a href="#Dynamic-landslide-susceptibility-mapping-over-recent-three-decades-to-uncover-variations-in-landslide-causes-in-subtropical-urban-mountainous-areas" class="headerlink" title="Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas"></a>Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11929">http://arxiv.org/abs/2308.11929</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cli-de/d_lsm">https://github.com/cli-de/d_lsm</a></li>
<li>paper_authors: Peifeng Ma, Li Chen, Chang Yu, Qing Zhu, Yulin Ding</li>
</ul>
<p>Abstract:<br>Landslide susceptibility assessment (LSA) is of paramount importance in mitigating landslide risks. Recently, there has been a surge in the utilization of data-driven methods for predicting landslide susceptibility due to the growing availability of aerial and satellite data. Nonetheless, the rapid oscillations within the landslide-inducing environment (LIE), primarily due to significant changes in external triggers such as rainfall, pose difficulties for contemporary data-driven LSA methodologies to accommodate LIEs over diverse timespans. This study presents dynamic landslide susceptibility mapping that simply employs multiple predictive models for annual LSA. In practice, this will inevitably encounter small sample problems due to the limited number of landslide samples in certain years. Another concern arises owing to the majority of the existing LSA approaches train black-box models to fit distinct datasets, yet often failing in generalization and providing comprehensive explanations concerning the interactions between input features and predictions. Accordingly, we proposed to meta-learn representations with fast adaptation ability using a few samples and gradient updates; and apply SHAP for each model interpretation and landslide feature permutation. Additionally, we applied MT-InSAR for LSA result enhancement and validation. The chosen study area is Lantau Island, Hong Kong, where we conducted a comprehensive dynamic LSA spanning from 1992 to 2019. The model interpretation results demonstrate that the primary factors responsible for triggering landslides in Lantau Island are terrain slope and extreme rainfall. The results also indicate that the variation in landslide causes can be primarily attributed to extreme rainfall events, which result from global climate change, and the implementation of the Landslip Prevention and Mitigation Programme (LPMitP) by the Hong Kong government.</p>
<hr>
<h2 id="Solving-Elliptic-Optimal-Control-Problems-using-Physics-Informed-Neural-Networks"><a href="#Solving-Elliptic-Optimal-Control-Problems-using-Physics-Informed-Neural-Networks" class="headerlink" title="Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks"></a>Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11925">http://arxiv.org/abs/2308.11925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bangti Jin, Ramesh Sau, Luowei Yin, Zhi Zhou</li>
</ul>
<p>Abstract:<br>In this work, we present and analyze a numerical solver for optimal control problems (without &#x2F; with box constraint) for linear and semilinear second-order elliptic problems. The approach is based on a coupled system derived from the first-order optimality system of the optimal control problem, and applies physics informed neural networks (PINNs) to solve the coupled system. We present an error analysis of the numerical scheme, and provide $L^2(\Omega)$ error bounds on the state, control and adjoint state in terms of deep neural network parameters (e.g., depth, width, and parameter bounds) and the number of sampling points in the domain and on the boundary. The main tools in the analysis include offset Rademacher complexity and boundedness and Lipschitz continuity of neural network functions. We present several numerical examples to illustrate the approach and compare it with three existing approaches.</p>
<hr>
<h2 id="Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe"><a href="#Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe" class="headerlink" title="Diverse Policies Converge in Reward-free Markov Decision Processe"></a>Diverse Policies Converge in Reward-free Markov Decision Processe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11924">http://arxiv.org/abs/2308.11924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openrl-lab/diversepolicies">https://github.com/openrl-lab/diversepolicies</a></li>
<li>paper_authors: Fanqi Lin, Shiyu Huang, Weiwei Tu</li>
</ul>
<p>Abstract:<br>Reinforcement learning has achieved great success in many decision-making tasks, and traditional reinforcement learning algorithms are mainly designed for obtaining a single optimal solution. However, recent works show the importance of developing diverse policies, which makes it an emerging research topic. Despite the variety of diversity reinforcement learning algorithms that have emerged, none of them theoretically answer the question of how the algorithm converges and how efficient the algorithm is. In this paper, we provide a unified diversity reinforcement learning framework and investigate the convergence of training diverse policies. Under such a framework, we also propose a provably efficient diversity reinforcement learning algorithm. Finally, we verify the effectiveness of our method through numerical experiments.</p>
<hr>
<h2 id="Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement"><a href="#Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement" class="headerlink" title="Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement"></a>Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11923">http://arxiv.org/abs/2308.11923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino</li>
</ul>
<p>Abstract:<br>We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.</p>
<hr>
<h2 id="Addressing-Selection-Bias-in-Computerized-Adaptive-Testing-A-User-Wise-Aggregate-Influence-Function-Approach"><a href="#Addressing-Selection-Bias-in-Computerized-Adaptive-Testing-A-User-Wise-Aggregate-Influence-Function-Approach" class="headerlink" title="Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach"></a>Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11912">http://arxiv.org/abs/2308.11912</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/riiid/useraif">https://github.com/riiid/useraif</a></li>
<li>paper_authors: Soonwoo Kwon, Sojung Kim, Seunghyun Lee, Jin-Young Kim, Suyeong An, Kyuseok Kim</li>
</ul>
<p>Abstract:<br>Computerized Adaptive Testing (CAT) is a widely used, efficient test mode that adapts to the examinee’s proficiency level in the test domain. CAT requires pre-trained item profiles, for CAT iteratively assesses the student real-time based on the registered items’ profiles, and selects the next item to administer using candidate items’ profiles. However, obtaining such item profiles is a costly process that involves gathering a large, dense item-response data, then training a diagnostic model on the collected data. In this paper, we explore the possibility of leveraging response data collected in the CAT service. We first show that this poses a unique challenge due to the inherent selection bias introduced by CAT, i.e., more proficient students will receive harder questions. Indeed, when naively training the diagnostic model using CAT response data, we observe that item profiles deviate significantly from the ground-truth. To tackle the selection bias issue, we propose the user-wise aggregate influence function method. Our intuition is to filter out users whose response data is heavily biased in an aggregate manner, as judged by how much perturbation the added data will introduce during parameter estimation. This way, we may enhance the performance of CAT while introducing minimal bias to the item profiles. We provide extensive experiments to demonstrate the superiority of our proposed method based on the three public datasets and one dataset that contains real-world CAT response data.</p>
<hr>
<h2 id="Diagnosing-Infeasible-Optimization-Problems-Using-Large-Language-Models"><a href="#Diagnosing-Infeasible-Optimization-Problems-Using-Large-Language-Models" class="headerlink" title="Diagnosing Infeasible Optimization Problems Using Large Language Models"></a>Diagnosing Infeasible Optimization Problems Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12923">http://arxiv.org/abs/2308.12923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Gonzalo E. Constante-Flores, Can Li</li>
</ul>
<p>Abstract:<br>Decision-making problems can be represented as mathematical optimization models, finding wide applications in fields such as economics, engineering and manufacturing, transportation, and health care. Optimization models are mathematical abstractions of the problem of making the best decision while satisfying a set of requirements or constraints. One of the primary barriers to deploying these models in practice is the challenge of helping practitioners understand and interpret such models, particularly when they are infeasible, meaning no decision satisfies all the constraints. Existing methods for diagnosing infeasible optimization models often rely on expert systems, necessitating significant background knowledge in optimization. In this paper, we introduce OptiChat, a first-of-its-kind natural language-based system equipped with a chatbot GUI for engaging in interactive conversations about infeasible optimization models. OptiChat can provide natural language descriptions of the optimization model itself, identify potential sources of infeasibility, and offer suggestions to make the model feasible. The implementation of OptiChat is built on GPT-4, which interfaces with an optimization solver to identify the minimal subset of constraints that render the entire optimization problem infeasible, also known as the Irreducible Infeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought, key-retrieve, and sentiment prompts to enhance OptiChat’s reliability. Our experiments demonstrate that OptiChat assists both expert and non-expert users in improving their understanding of the optimization models, enabling them to quickly identify the sources of infeasibility.</p>
<hr>
<h2 id="Utilizing-Admissible-Bounds-for-Heuristic-Learning"><a href="#Utilizing-Admissible-Bounds-for-Heuristic-Learning" class="headerlink" title="Utilizing Admissible Bounds for Heuristic Learning"></a>Utilizing Admissible Bounds for Heuristic Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11905">http://arxiv.org/abs/2308.11905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Núñez-Molina, Masataro Asai</li>
</ul>
<p>Abstract:<br>While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and empirically show that, as a result, it yields more accurate heuristics and converges faster during training.</p>
<hr>
<h2 id="Rethinking-Data-Perturbation-and-Model-Stabilization-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Rethinking-Data-Perturbation-and-Model-Stabilization-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation"></a>Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11903">http://arxiv.org/abs/2308.11903</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenzhao/dpms">https://github.com/zhenzhao/dpms</a></li>
<li>paper_authors: Zhen Zhao, Ye Liu, Meng Zhao, Di Yin, Yixuan Yuan, Luping Zhou</li>
</ul>
<p>Abstract:<br>Studies on semi-supervised medical image segmentation (SSMIS) have seen fast progress recently. Due to the limited labelled data, SSMIS methods mainly focus on effectively leveraging unlabeled data to enhance the segmentation performance. However, despite their promising performance, current state-of-the-art methods often prioritize integrating complex techniques and loss terms rather than addressing the core challenges of semi-supervised scenarios directly. We argue that the key to SSMIS lies in generating substantial and appropriate prediction disagreement on unlabeled data. To this end, we emphasize the crutiality of data perturbation and model stabilization in semi-supervised segmentation, and propose a simple yet effective approach to boost SSMIS performance significantly, dubbed DPMS. Specifically, we first revisit SSMIS from three distinct perspectives: the data, the model, and the loss, and conduct a comprehensive study of corresponding strategies to examine their effectiveness. Based on these examinations, we then propose DPMS, which adopts a plain teacher-student framework with a standard supervised loss and unsupervised consistency loss. To produce appropriate prediction disagreements, DPMS perturbs the unlabeled data via strong augmentations to enlarge prediction disagreements considerably. On the other hand, using EMA teacher when strong augmentation is applied does not necessarily improve performance. DPMS further utilizes a forwarding-twice and momentum updating strategies for normalization statistics to stabilize the training on unlabeled data effectively. Despite its simplicity, DPMS can obtain new state-of-the-art performance on the public 2D ACDC and 3D LA datasets across various semi-supervised settings, e.g. obtaining a remarkable 22.62% improvement against previous SOTA on ACDC with 5% labels.</p>
<hr>
<h2 id="Shape-conditioned-3D-Molecule-Generation-via-Equivariant-Diffusion-Models"><a href="#Shape-conditioned-3D-Molecule-Generation-via-Equivariant-Diffusion-Models" class="headerlink" title="Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models"></a>Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11890">http://arxiv.org/abs/2308.11890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Chen, Bo Peng, Srinivasan Parthasarathy, Xia Ning</li>
</ul>
<p>Abstract:<br>Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.</p>
<hr>
<h2 id="Adversarial-Training-Using-Feedback-Loops"><a href="#Adversarial-Training-Using-Feedback-Loops" class="headerlink" title="Adversarial Training Using Feedback Loops"></a>Adversarial Training Using Feedback Loops</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11881">http://arxiv.org/abs/2308.11881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Haisam Muhammad Rafid, Adrian Sandu</li>
</ul>
<p>Abstract:<br>Deep neural networks (DNN) have found wide applicability in numerous fields due to their ability to accurately learn very complex input-output relations. Despite their accuracy and extensive use, DNNs are highly susceptible to adversarial attacks due to limited generalizability. For future progress in the field, it is essential to build DNNs that are robust to any kind of perturbations to the data points. In the past, many techniques have been proposed to robustify DNNs using first-order derivative information of the network.   This paper proposes a new robustification approach based on control theory. A neural network architecture that incorporates feedback control, named Feedback Neural Networks, is proposed. The controller is itself a neural network, which is trained using regular and adversarial data such as to stabilize the system outputs. The novel adversarial training approach based on the feedback control architecture is called Feedback Looped Adversarial Training (FLAT). Numerical results on standard test problems empirically show that our FLAT method is more effective than the state-of-the-art to guard against adversarial attacks.</p>
<hr>
<h2 id="SUMMIT-Source-Free-Adaptation-of-Uni-Modal-Models-to-Multi-Modal-Targets"><a href="#SUMMIT-Source-Free-Adaptation-of-Uni-Modal-Models-to-Multi-Modal-Targets" class="headerlink" title="SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets"></a>SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11880">http://arxiv.org/abs/2308.11880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csimo005/summit">https://github.com/csimo005/summit</a></li>
<li>paper_authors: Cody Simons, Dripta S. Raychaudhuri, Sk Miraj Ahmed, Suya You, Konstantinos Karydis, Amit K. Roy-Chowdhury</li>
</ul>
<p>Abstract:<br>Scene understanding using multi-modal data is necessary in many applications, e.g., autonomous navigation. To achieve this in a variety of situations, existing models must be able to adapt to shifting data distributions without arduous data annotation. Current approaches assume that the source data is available during adaptation and that the source consists of paired multi-modal data. Both these assumptions may be problematic for many applications. Source data may not be available due to privacy, security, or economic concerns. Assuming the existence of paired multi-modal data for training also entails significant data collection costs and fails to take advantage of widely available freely distributed pre-trained uni-modal models. In this work, we relax both of these assumptions by addressing the problem of adapting a set of models trained independently on uni-modal data to a target domain consisting of unlabeled multi-modal data, without having access to the original source dataset. Our proposed approach solves this problem through a switching framework which automatically chooses between two complementary methods of cross-modal pseudo-label fusion – agreement filtering and entropy weighting – based on the estimated domain gap. We demonstrate our work on the semantic segmentation problem. Experiments across seven challenging adaptation scenarios verify the efficacy of our approach, achieving results comparable to, and in some cases outperforming, methods which assume access to source data. Our method achieves an improvement in mIoU of up to 12% over competing baselines. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/csimo005/SUMMIT">https://github.com/csimo005/SUMMIT</a>.</p>
<hr>
<h2 id="Cabrita-closing-the-gap-for-foreign-languages"><a href="#Cabrita-closing-the-gap-for-foreign-languages" class="headerlink" title="Cabrita: closing the gap for foreign languages"></a>Cabrita: closing the gap for foreign languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11878">http://arxiv.org/abs/2308.11878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celio Larcher, Marcos Piau, Paulo Finardi, Pedro Gengo, Piero Esposito, Vinicius Caridá</li>
</ul>
<p>Abstract:<br>The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.   The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.   To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We believe that this methodology can be applied to any transformer-like architecture model. To validate the study, we conducted continuous pre-training exclusively using Portuguese text on a 3-billion-parameter model known as OpenLLaMA, resulting in a model named openCabrita 3B. The openCabrita 3B also features a new tokenizer that results in a significant reduction in the number of tokens required to represent the text. In our assessment, for few-shot learning tasks, we achieved similar results with this 3B model compared to a traditional continuous pre-training approach as well as to 7B models English pre-trained models.</p>
<hr>
<h2 id="Integrating-Large-Language-Models-into-the-Debugging-C-Compiler-for-generating-contextual-error-explanations"><a href="#Integrating-Large-Language-Models-into-the-Debugging-C-Compiler-for-generating-contextual-error-explanations" class="headerlink" title="Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations"></a>Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11873">http://arxiv.org/abs/2308.11873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comp1511unsw/dcc">https://github.com/comp1511unsw/dcc</a></li>
<li>paper_authors: Andrew Taylor, Alexandra Vassar, Jake Renzella, Hammond Pearce</li>
</ul>
<p>Abstract:<br>This paper introduces a method for Large Language Models (LLM) to produce enhanced compiler error explanations, in simple language, within our Debugging C Compiler (DCC). It is well documented that compiler error messages have been known to present a barrier for novices learning how to program. Although our initial use of DCC in introductory programming (CS1) has been instrumental in teaching C to novice programmers by providing safeguards to commonly occurring errors and translating the usually cryptic compiler error messages at both compile- and run-time, we proposed that incorporating LLM-generated explanations would further enhance the learning experience for novice programmers. Through an expert evaluation, we observed that LLM-generated explanations for compiler errors were conceptually accurate in 90% of compile-time errors, and 75% of run-time errors. Additionally, the new DCC-help tool has been increasingly adopted by students, with an average of 1047 unique runs per week, demonstrating a promising initial assessment of using LLMs to complement compiler output to enhance programming education for beginners. We release our tool as open-source to the community.</p>
<hr>
<h2 id="Fast-Exact-NPN-Classification-with-Influence-aided-Canonical-Form"><a href="#Fast-Exact-NPN-Classification-with-Influence-aided-Canonical-Form" class="headerlink" title="Fast Exact NPN Classification with Influence-aided Canonical Form"></a>Fast Exact NPN Classification with Influence-aided Canonical Form</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12311">http://arxiv.org/abs/2308.12311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhang, Liwei Ni, Jiaxi Zhang, Guojie Luo, Huawei Li, Shenggen Zheng</li>
</ul>
<p>Abstract:<br>NPN classification has many applications in the synthesis and verification of digital circuits. The canonical-form-based method is the most common approach, designing a canonical form as representative for the NPN equivalence class first and then computing the transformation function according to the canonical form. Most works use variable symmetries and several signatures, mainly based on the cofactor, to simplify the canonical form construction and computation. This paper describes a novel canonical form and its computation algorithm by introducing Boolean influence to NPN classification, which is a basic concept in analysis of Boolean functions. We show that influence is input-negation-independent, input-permutation-dependent, and has other structural information than previous signatures for NPN classification. Therefore, it is a significant ingredient in speeding up NPN classification. Experimental results prove that influence plays an important role in reducing the transformation enumeration in computing the canonical form. Compared with the state-of-the-art algorithm implemented in ABC, our influence-aided canonical form for exact NPN classification gains up to 5.5x speedup.</p>
<hr>
<h2 id="KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods"><a href="#KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods" class="headerlink" title="KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods"></a>KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11863">http://arxiv.org/abs/2308.11863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Nzeyimana</li>
</ul>
<p>Abstract:<br>Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art to the best of our knowledge. Our experiments also indicate that using syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda.</p>
<hr>
<h2 id="Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0"><a href="#Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0" class="headerlink" title="Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0"></a>Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11854">http://arxiv.org/abs/2308.11854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anmol Chaure, Ashok Kumar Behera, Sudip Bhattacharya</li>
</ul>
<p>Abstract:<br>Climate projections using data driven machine learning models acting as emulators, is one of the prevailing areas of research to enable policy makers make informed decisions. Use of machine learning emulators as surrogates for computationally heavy GCM simulators reduces time and carbon footprints. In this direction, ClimateBench [1] is a recently curated benchmarking dataset for evaluating the performance of machine learning emulators designed for climate data. Recent studies have reported that despite being considered fundamental, regression models offer several advantages pertaining to climate emulations. In particular, by leveraging the kernel trick, regression models can capture complex relationships and improve their predictive capabilities. This study focuses on evaluating non-linear regression models using the aforementioned dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the best-in-class performance against standard evaluation metrics used for climate field emulation studies. However, Gaussian Process Regression suffers from being computational resource hungry in terms of space and time complexity. Alternatively, Support Vector and Kernel Ridge models also deliver competitive results and but there are certain trade-offs to be addressed. Additionally, we are actively investigating the performance of composite kernels and techniques such as variational inference to further enhance the performance of the regression models and effectively model complex non-linear patterns, including phenomena like precipitation.</p>
<hr>
<h2 id="A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data"><a href="#A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data" class="headerlink" title="A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data"></a>A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11849">http://arxiv.org/abs/2308.11849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Liu, Zhiyuan Lin, Judith Y. T. Wang, Hong Chen</li>
</ul>
<p>Abstract:<br>Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accumulation of overcrowded passengers at this station, to prevent additional accidents arising from overcrowding. This research addresses the challenges associated with this scenario, including the dynamics of arriving and leaving of passengers, station overcrowding, rolling stock shortage, open-ended disruption duration, integrated rescheduling on multiple routes, and delays due to detours. A deep reinforcement learning (DRL) framework is proposed to determine the optimal rescheduled timetable, route stops, and rolling stock allocation, while considering real-time demand satisfaction, station overcrowding, train capacity utilization, and headway safety.</p>
<hr>
<h2 id="SEA-Shareable-and-Explainable-Attribution-for-Query-based-Black-box-Attacks"><a href="#SEA-Shareable-and-Explainable-Attribution-for-Query-based-Black-box-Attacks" class="headerlink" title="SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks"></a>SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11845">http://arxiv.org/abs/2308.11845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Gao, Ilia Shumailov, Kassem Fawaz</li>
</ul>
<p>Abstract:<br>Machine Learning (ML) systems are vulnerable to adversarial examples, particularly those from query-based black-box attacks. Despite various efforts to detect and prevent such attacks, there is a need for a more comprehensive approach to logging, analyzing, and sharing evidence of attacks. While classic security benefits from well-established forensics and intelligence sharing, Machine Learning is yet to find a way to profile its attackers and share information about them. In response, this paper introduces SEA, a novel ML security system to characterize black-box attacks on ML systems for forensic purposes and to facilitate human-explainable intelligence sharing. SEA leverages the Hidden Markov Models framework to attribute the observed query sequence to known attacks. It thus understands the attack’s progression rather than just focusing on the final adversarial examples. Our evaluations reveal that SEA is effective at attack attribution, even on their second occurrence, and is robust to adaptive strategies designed to evade forensics analysis. Interestingly, SEA’s explanations of the attack behavior allow us even to fingerprint specific minor implementation bugs in attack libraries. For example, we discover that the SignOPT and Square attacks implementation in ART v1.14 sends over 50% specific zero difference queries. We thoroughly evaluate SEA on a variety of settings and demonstrate that it can recognize the same attack’s second occurrence with 90+% Top-1 and 95+% Top-3 accuracy.</p>
<hr>
<h2 id="rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning"></a>${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11842">http://arxiv.org/abs/2308.11842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dchen48/e3ac">https://github.com/dchen48/e3ac</a></li>
<li>paper_authors: Dingyang Chen, Qi Zhang</li>
</ul>
<p>Abstract:<br>Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/dchen48/E3AC">https://github.com/dchen48/E3AC</a>.</p>
<hr>
<h2 id="A-Survey-for-Federated-Learning-Evaluations-Goals-and-Measures"><a href="#A-Survey-for-Federated-Learning-Evaluations-Goals-and-Measures" class="headerlink" title="A Survey for Federated Learning Evaluations: Goals and Measures"></a>A Survey for Federated Learning Evaluations: Goals and Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11841">http://arxiv.org/abs/2308.11841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Chai, Leye Wang, Liu Yang, Junxue Zhang, Kai Chen, Qiang Yang</li>
</ul>
<p>Abstract:<br>Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval, an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.</p>
<hr>
<h2 id="A-Benchmark-Study-on-Calibration"><a href="#A-Benchmark-Study-on-Calibration" class="headerlink" title="A Benchmark Study on Calibration"></a>A Benchmark Study on Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11838">http://arxiv.org/abs/2308.11838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, Chang Xu</li>
</ul>
<p>Abstract:<br>Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can robustness be used as a calibration measurement? (iii) How reliable are calibration metrics? (iv) Does a post-hoc calibration method affect all models uniformly? (v) How does calibration interact with accuracy? (vi) What is the impact of bin size on calibration measurement? (vii) Which architectural designs are beneficial for calibration? Additionally, our study bridges an existing gap by exploring calibration within NAS. By providing this dataset, we enable further research into NAS calibration. As far as we are aware, our research represents the first large-scale investigation into calibration properties and the premier study of calibration issues within NAS.</p>
<hr>
<h2 id="Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity"><a href="#Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity" class="headerlink" title="Characterizing normal perinatal development of the human brain structural connectivity"></a>Characterizing normal perinatal development of the human brain structural connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11836">http://arxiv.org/abs/2308.11836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Wu, Lana Vasung, Camilo Calixto, Ali Gholipour, Davood Karimi</li>
</ul>
<p>Abstract:<br>Early brain development is characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain’s cognitive abilities and influences its response to diseases and environmental factors. Hence, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determining such baselines. We used this framework to analyze the structural connectivity between 33 and 44 postmenstrual weeks using data from 166 subjects. Our results unveiled clear and strong trends in the development of structural connectivity in perinatal stage. Connection weighting based on fractional anisotropy and neurite density produced the most consistent results. We observed increases in global and local efficiency, a decrease in characteristic path length, and widespread strengthening of the connections within and across brain lobes and hemispheres. We also observed asymmetry patterns that were consistent between different connection weighting approaches. The new computational method and results are useful for assessing normal and abnormal development of the structural connectome early in life.</p>
<hr>
<h2 id="Performance-Comparison-and-Implementation-of-Bayesian-Variants-for-Network-Intrusion-Detection"><a href="#Performance-Comparison-and-Implementation-of-Bayesian-Variants-for-Network-Intrusion-Detection" class="headerlink" title="Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection"></a>Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11834">http://arxiv.org/abs/2308.11834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tosin Ige, Christopher Kiekintveld</li>
</ul>
<p>Abstract:<br>Bayesian classifiers perform well when each of the features is completely independent of the other which is not always valid in real world application. The aim of this study is to implement and compare the performances of each variant of Bayesian classifier (Multinomial, Bernoulli, and Gaussian) on anomaly detection in network intrusion, and to investigate whether there is any association between each variant assumption and their performance. Our investigation showed that each variant of Bayesian algorithm blindly follows its assumption regardless of feature property, and that the assumption is the single most important factor that influences their accuracy. Experimental results show that Bernoulli has accuracy of 69.9% test (71% train), Multinomial has accuracy of 31.2% test (31.2% train), while Gaussian has accuracy of 81.69% test (82.84% train). Going deeper, we investigated and found that each Naive Bayes variants performances and accuracy is largely due to each classifier assumption, Gaussian classifier performed best on anomaly detection due to its assumption that features follow normal distributions which are continuous, while multinomial classifier have a dismal performance as it simply assumes discreet and multinomial distribution.</p>
<hr>
<h2 id="Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test"><a href="#Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test" class="headerlink" title="Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test"></a>Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11827">http://arxiv.org/abs/2308.11827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saba Rahimi, Tucker Balch, Manuela Veloso</li>
</ul>
<p>Abstract:<br>Large language models such as Open AI’s Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver’s Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model’s passing score fell to 82%. However, the model still fails to answer some questions correctly even with providing library of context, highlighting room for improvement. The research also examined the impact of prompt length and context format, on the model’s performance. Overall, the study provides insights into the limitations and potential improvements for GPT models in question-answering tasks.</p>
<hr>
<h2 id="Accel-GCN-High-Performance-GPU-Accelerator-Design-for-Graph-Convolution-Networks"><a href="#Accel-GCN-High-Performance-GPU-Accelerator-Design-for-Graph-Convolution-Networks" class="headerlink" title="Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks"></a>Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11825">http://arxiv.org/abs/2308.11825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiexi1990/iccad-accel-gnn">https://github.com/xiexi1990/iccad-accel-gnn</a></li>
<li>paper_authors: Xi Xie, Hongwu Peng, Amit Hasan, Shaoyi Huang, Jiahui Zhao, Haowen Fang, Wei Zhang, Tong Geng, Omer Khan, Caiwen Ding</li>
</ul>
<p>Abstract:<br>Graph Convolutional Networks (GCNs) are pivotal in extracting latent information from graph data across various domains, yet their acceleration on mainstream GPUs is challenged by workload imbalance and memory access irregularity. To address these challenges, we present Accel-GCN, a GPU accelerator architecture for GCNs. The design of Accel-GCN encompasses: (i) a lightweight degree sorting stage to group nodes with similar degree; (ii) a block-level partition strategy that dynamically adjusts warp workload sizes, enhancing shared memory locality and workload balance, and reducing metadata overhead compared to designs like GNNAdvisor; (iii) a combined warp strategy that improves memory coalescing and computational parallelism in the column dimension of dense matrices.   Utilizing these principles, we formulated a kernel for sparse matrix multiplication (SpMM) in GCNs that employs block-level partitioning and combined warp strategy. This approach augments performance and multi-level memory efficiency and optimizes memory bandwidth by exploiting memory coalescing and alignment. Evaluation of Accel-GCN across 18 benchmark graphs reveals that it outperforms cuSPARSE, GNNAdvisor, and graph-BLAST by factors of 1.17 times, 1.86 times, and 2.94 times respectively. The results underscore Accel-GCN as an effective solution for enhancing GCN computational efficiency.</p>
<hr>
<h2 id="PatchBackdoor-Backdoor-Attack-against-Deep-Neural-Networks-without-Model-Modification"><a href="#PatchBackdoor-Backdoor-Attack-against-Deep-Neural-Networks-without-Model-Modification" class="headerlink" title="PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification"></a>PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11822">http://arxiv.org/abs/2308.11822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xaiveryuan/patchbackdoor">https://github.com/xaiveryuan/patchbackdoor</a></li>
<li>paper_authors: Yizhen Yuan, Rui Kong, Shenghao Xie, Yuanchun Li, Yunxin Liu</li>
</ul>
<p>Abstract:<br>Backdoor attack is a major threat to deep learning systems in safety-critical scenarios, which aims to trigger misbehavior of neural network models under attacker-controlled conditions. However, most backdoor attacks have to modify the neural network models through training with poisoned data and&#x2F;or direct model editing, which leads to a common but false belief that backdoor attack can be easily avoided by properly protecting the model. In this paper, we show that backdoor attacks can be achieved without any model modification. Instead of injecting backdoor logic into the training data or the model, we propose to place a carefully-designed patch (namely backdoor patch) in front of the camera, which is fed into the model together with the input images. The patch can be trained to behave normally at most of the time, while producing wrong prediction when the input image contains an attacker-controlled trigger object. Our main techniques include an effective training method to generate the backdoor patch and a digital-physical transformation modeling method to enhance the feasibility of the patch in real deployments. Extensive experiments show that PatchBackdoor can be applied to common deep learning models (VGG, MobileNet, ResNet) with an attack success rate of 93% to 99% on classification tasks. Moreover, we implement PatchBackdoor in real-world scenarios and show that the attack is still threatening.</p>
<hr>
<h2 id="Mitigating-Health-Disparity-on-Biased-Electronic-Health-Records-via-Deconfounder"><a href="#Mitigating-Health-Disparity-on-Biased-Electronic-Health-Records-via-Deconfounder" class="headerlink" title="Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder"></a>Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11819">http://arxiv.org/abs/2308.11819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Liu, Xiaohan Li, Philip Yu</li>
</ul>
<p>Abstract:<br>The fairness issue of clinical data modeling, especially on Electronic Health Records (EHRs), is of utmost importance due to EHR’s complex latent structure and potential selection bias. It is frequently necessary to mitigate health disparity while keeping the model’s overall accuracy in practice. However, traditional methods often encounter the trade-off between accuracy and fairness, as they fail to capture the underlying factors beyond observed data. To tackle this challenge, we propose a novel model called Fair Longitudinal Medical Deconfounder (FLMD) that aims to achieve both fairness and accuracy in longitudinal Electronic Health Records (EHR) modeling. Drawing inspiration from the deconfounder theory, FLMD employs a two-stage training process. In the first stage, FLMD captures unobserved confounders for each encounter, which effectively represents underlying medical factors beyond observed EHR, such as patient genotypes and lifestyle habits. This unobserved confounder is crucial for addressing the accuracy&#x2F;fairness dilemma. In the second stage, FLMD combines the learned latent representation with other relevant features to make predictions. By incorporating appropriate fairness criteria, such as counterfactual fairness, FLMD ensures that it maintains high prediction accuracy while simultaneously minimizing health disparities. We conducted comprehensive experiments on two real-world EHR datasets to demonstrate the effectiveness of FLMD. Apart from the comparison of baseline methods and FLMD variants in terms of fairness and accuracy, we assessed the performance of all models on disturbed&#x2F;imbalanced and synthetic datasets to showcase the superiority of FLMD across different settings and provide valuable insights into its capabilities.</p>
<hr>
<h2 id="Incorporating-Nonlocal-Traffic-Flow-Model-in-Physics-informed-Neural-Networks"><a href="#Incorporating-Nonlocal-Traffic-Flow-Model-in-Physics-informed-Neural-Networks" class="headerlink" title="Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks"></a>Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11818">http://arxiv.org/abs/2308.11818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Archie J. Huang, Animesh Biswas, Shaurya Agarwal</li>
</ul>
<p>Abstract:<br>This research contributes to the advancement of traffic state estimation methods by leveraging the benefits of the nonlocal LWR model within a physics-informed deep learning framework. The classical LWR model, while useful, falls short of accurately representing real-world traffic flows. The nonlocal LWR model addresses this limitation by considering the speed as a weighted mean of the downstream traffic density. In this paper, we propose a novel PIDL framework that incorporates the nonlocal LWR model. We introduce both fixed-length and variable-length kernels and develop the required mathematics. The proposed PIDL framework undergoes a comprehensive evaluation, including various convolutional kernels and look-ahead windows, using data from the NGSIM and CitySim datasets. The results demonstrate improvements over the baseline PIDL approach using the local LWR model. The findings highlight the potential of the proposed approach to enhance the accuracy and reliability of traffic state estimation, enabling more effective traffic management strategies.</p>
<hr>
<h2 id="Evaluation-of-Deep-Neural-Operator-Models-toward-Ocean-Forecasting"><a href="#Evaluation-of-Deep-Neural-Operator-Models-toward-Ocean-Forecasting" class="headerlink" title="Evaluation of Deep Neural Operator Models toward Ocean Forecasting"></a>Evaluation of Deep Neural Operator Models toward Ocean Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11814">http://arxiv.org/abs/2308.11814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ellery Rajagopal, Anantha N. S. Babu, Tony Ryu, Patrick J. Haley Jr., Chris Mirabito, Pierre F. J. Lermusiaux</li>
</ul>
<p>Abstract:<br>Data-driven, deep-learning modeling frameworks have been recently developed for forecasting time series data. Such machine learning models may be useful in multiple domains including the atmospheric and oceanic ones, and in general, the larger fluids community. The present work investigates the possible effectiveness of such deep neural operator models for reproducing and predicting classic fluid flows and simulations of realistic ocean dynamics. We first briefly evaluate the capabilities of such deep neural operator models when trained on a simulated two-dimensional fluid flow past a cylinder. We then investigate their application to forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay, learning from high-resolution data-assimilative simulations employed for real sea experiments. We confirm that trained deep neural operator models are capable of predicting idealized periodic eddy shedding. For realistic ocean surface flows and our preliminary study, they can predict several of the features and show some skill, providing potential for future research and applications.</p>
<hr>
<h2 id="Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings"><a href="#Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings" class="headerlink" title="Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings"></a>Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11804">http://arxiv.org/abs/2308.11804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugene Bagdasaryan, Vitaly Shmatikov</li>
</ul>
<p>Abstract:<br>Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call “adversarial illusions.” Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.   Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.</p>
<hr>
<h2 id="Variational-Density-Propagation-Continual-Learning"><a href="#Variational-Density-Propagation-Continual-Learning" class="headerlink" title="Variational Density Propagation Continual Learning"></a>Variational Density Propagation Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11801">http://arxiv.org/abs/2308.11801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Angelini, Nidhal Bouaynaya, Ghulam Rasool</li>
</ul>
<p>Abstract:<br>Deep Neural Networks (DNNs) deployed to the real world are regularly subject to out-of-distribution (OoD) data, various types of noise, and shifting conceptual objectives. This paper proposes a framework for adapting to data distribution drift modeled by benchmark Continual Learning datasets. We develop and evaluate a method of Continual Learning that leverages uncertainty quantification from Bayesian Inference to mitigate catastrophic forgetting. We expand on previous approaches by removing the need for Monte Carlo sampling of the model weights to sample the predictive distribution. We optimize a closed-form Evidence Lower Bound (ELBO) objective approximating the predictive distribution by propagating the first two moments of a distribution, i.e. mean and covariance, through all network layers. Catastrophic forgetting is mitigated by using the closed-form ELBO to approximate the Minimum Description Length (MDL) Principle, inherently penalizing changes in the model likelihood by minimizing the KL Divergence between the variational posterior for the current task and the previous task’s variational posterior acting as the prior. Leveraging the approximation of the MDL principle, we aim to initially learn a sparse variational posterior and then minimize additional model complexity learned for subsequent tasks. Our approach is evaluated for the task incremental learning scenario using density propagated versions of fully-connected and convolutional neural networks across multiple sequential benchmark datasets with varying task sequence lengths. Ultimately, this procedure produces a minimally complex network over a series of tasks mitigating catastrophic forgetting.</p>
<hr>
<h2 id="Complex-valued-neural-networks-for-voice-anti-spoofing"><a href="#Complex-valued-neural-networks-for-voice-anti-spoofing" class="headerlink" title="Complex-valued neural networks for voice anti-spoofing"></a>Complex-valued neural networks for voice anti-spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11800">http://arxiv.org/abs/2308.11800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas M. Müller, Philip Sperl, Konstantin Böttinger</li>
</ul>
<p>Abstract:<br>Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the “In-the-Wild” anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing.</p>
<hr>
<h2 id="Karasu-A-Collaborative-Approach-to-Efficient-Cluster-Configuration-for-Big-Data-Analytics"><a href="#Karasu-A-Collaborative-Approach-to-Efficient-Cluster-Configuration-for-Big-Data-Analytics" class="headerlink" title="Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics"></a>Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11792">http://arxiv.org/abs/2308.11792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Scheinert, Philipp Wiesner, Thorsten Wittkopp, Lauritz Thamsen, Jonathan Will, Odej Kao</li>
</ul>
<p>Abstract:<br>Selecting the right resources for big data analytics jobs is hard because of the wide variety of configuration options like machine type and cluster size. As poor choices can have a significant impact on resource efficiency, cost, and energy usage, automated approaches are gaining popularity. Most existing methods rely on profiling recurring workloads to find near-optimal solutions over time. Due to the cold-start problem, this often leads to lengthy and costly profiling phases. However, big data analytics jobs across users can share many common properties: they often operate on similar infrastructure, using similar algorithms implemented in similar frameworks. The potential in sharing aggregated profiling runs to collaboratively address the cold start problem is largely unexplored.   We present Karasu, an approach to more efficient resource configuration profiling that promotes data sharing among users working with similar infrastructures, frameworks, algorithms, or datasets. Karasu trains lightweight performance models using aggregated runtime information of collaborators and combines them into an ensemble method to exploit inherent knowledge of the configuration search space. Moreover, Karasu allows the optimization of multiple objectives simultaneously. Our evaluation is based on performance data from diverse workload executions in a public cloud environment. We show that Karasu is able to significantly boost existing methods in terms of performance, search time, and cost, even when few comparable profiling runs are available that share only partial common characteristics with the target job.</p>
<hr>
<h2 id="HypBO-Expert-Guided-Chemist-in-the-Loop-Bayesian-Search-for-New-Materials"><a href="#HypBO-Expert-Guided-Chemist-in-the-Loop-Bayesian-Search-for-New-Materials" class="headerlink" title="HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials"></a>HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11787">http://arxiv.org/abs/2308.11787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdoulatif Cisse, Xenophon Evangelopoulos, Sam Carruthers, Vladimir V. Gusev, Andrew I. Cooper</li>
</ul>
<p>Abstract:<br>Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function&#x2F;property is known. Here we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate an improved seed of samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampling. This process continues in a global versus local search fashion, organized in a bilevel optimization framework. We validate the performance of our method on a range of synthetic functions and demonstrate its practical utility on a real chemical design task where the use of expert hypotheses accelerates the search performance significantly.</p>
<hr>
<h2 id="Coarse-to-Fine-Multi-Scene-Pose-Regression-with-Transformers"><a href="#Coarse-to-Fine-Multi-Scene-Pose-Regression-with-Transformers" class="headerlink" title="Coarse-to-Fine Multi-Scene Pose Regression with Transformers"></a>Coarse-to-Fine Multi-Scene Pose Regression with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11783">http://arxiv.org/abs/2308.11783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoli Shavit, Ron Ferens, Yosi Keller</li>
</ul>
<p>Abstract:<br>Absolute camera pose regressors estimate the position and orientation of a camera given the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron (MLP) head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended to learn multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into pose predictions. This allows our model to focus on general features that are informative for localization, while embedding multiple scenes in parallel. We extend our previous MS-Transformer approach \cite{shavit2021learning} by introducing a mixed classification-regression architecture that improves the localization accuracy. Our method is evaluated on commonly benchmark indoor and outdoor datasets and has been shown to exceed both multi-scene and state-of-the-art single-scene absolute pose regressors.</p>
<hr>
<h2 id="Addressing-Dynamic-and-Sparse-Qualitative-Data-A-Hilbert-Space-Embedding-of-Categorical-Variables"><a href="#Addressing-Dynamic-and-Sparse-Qualitative-Data-A-Hilbert-Space-Embedding-of-Categorical-Variables" class="headerlink" title="Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables"></a>Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11781">http://arxiv.org/abs/2308.11781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirban Mukherjee, Hannah H. Chang</li>
</ul>
<p>Abstract:<br>We propose a novel framework for incorporating qualitative data into quantitative models for causal estimation. Previous methods use categorical variables derived from qualitative data to build quantitative models. However, this approach can lead to data-sparse categories and yield inconsistent (asymptotically biased) and imprecise (finite sample biased) estimates if the qualitative information is dynamic and intricate. We use functional analysis to create a more nuanced and flexible framework. We embed the observed categories into a latent Baire space and introduce a continuous linear map – a Hilbert space embedding – from the Baire space of categories to a Reproducing Kernel Hilbert Space (RKHS) of representation functions. Through the Riesz representation theorem, we establish that the canonical treatment of categorical variables in causal models can be transformed into an identified structure in the RKHS. Transfer learning acts as a catalyst to streamline estimation – embeddings from traditional models are paired with the kernel trick to form the Hilbert space embedding. We validate our model through comprehensive simulation evidence and demonstrate its relevance in a real-world study that contrasts theoretical predictions from economics and psychology in an e-commerce marketplace. The results confirm the superior performance of our model, particularly in scenarios where qualitative information is nuanced and complex.</p>
<hr>
<h2 id="Few-shot-Anomaly-Detection-in-Text-with-Deviation-Learning"><a href="#Few-shot-Anomaly-Detection-in-Text-with-Deviation-Learning" class="headerlink" title="Few-shot Anomaly Detection in Text with Deviation Learning"></a>Few-shot Anomaly Detection in Text with Deviation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11780">http://arxiv.org/abs/2308.11780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anindya Sundar Das, Aravind Ajay, Sriparna Saha, Monowar Bhuyan</li>
</ul>
<p>Abstract:<br>Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably deviate from the reference score in the upper tail of the prior. Additionally, our model is optimized to learn the distinct behavior of anomalies by utilizing a multi-head self-attention layer and multiple instance learning approaches. Comprehensive experiments on several benchmark datasets demonstrate that our proposed approach attains a new level of state-of-the-art performance.</p>
<hr>
<h2 id="Understanding-Hessian-Alignment-for-Domain-Generalization"><a href="#Understanding-Hessian-Alignment-for-Domain-Generalization" class="headerlink" title="Understanding Hessian Alignment for Domain Generalization"></a>Understanding Hessian Alignment for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11778">http://arxiv.org/abs/2308.11778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/federated-learning">https://github.com/huawei-noah/federated-learning</a></li>
<li>paper_authors: Sobhan Hemati, Guojun Zhang, Amir Estiri, Xi Chen</li>
</ul>
<p>Abstract:<br>Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenarios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regularizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier’s head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spectral norm between the classifier’s head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Furthermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients. Our analysis explains the success of many regularizers like CORAL, IRM, V-REx, Fish, IGA, and Fishr as they regularize part of the classifier’s head Hessian and&#x2F;or gradient. Finally, we propose two simple yet effective methods to match the classifier’s head Hessians and gradients in an efficient way, based on the Hessian Gradient Product (HGP) and Hutchinson’s method (Hutchinson), and without directly calculating Hessians. We validate the OOD generalization ability of proposed methods in different scenarios, including transferability, severe correlation shift, label shift and diversity shift. Our results show that Hessian alignment methods achieve promising performance on various OOD benchmarks. The code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment%7D">https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment}</a>.</p>
<hr>
<h2 id="3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network"><a href="#3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network" class="headerlink" title="3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network"></a>3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11771">http://arxiv.org/abs/2308.11771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinyu Chen, Zuowen Wang, Shih-Chii Liu, Chang Gao</li>
</ul>
<p>Abstract:<br>This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR&#x2F;VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\times$ without losing accuracy when tested on a \texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \url{<a target="_blank" rel="noopener" href="https://github.com/qinche106/cb-convlstm-eyetracking%7D">https://github.com/qinche106/cb-convlstm-eyetracking}</a>.</p>
<hr>
<h2 id="Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data"><a href="#Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data" class="headerlink" title="Patient Clustering via Integrated Profiling of Clinical and Digital Data"></a>Patient Clustering via Integrated Profiling of Clinical and Digital Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11748">http://arxiv.org/abs/2308.11748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjin Choi, Andy Xiang, Ozgur Ozturk, Deep Shrestha, Barry Drake, Hamid Haidarian, Faizan Javed, Haesun Park</li>
</ul>
<p>Abstract:<br>We introduce a novel profile-based patient clustering model designed for clinical data in healthcare. By utilizing a method grounded on constrained low-rank approximation, our model takes advantage of patients’ clinical data and digital interaction data, including browsing and search, to construct patient profiles. As a result of the method, nonnegative embedding vectors are generated, serving as a low-dimensional representation of the patients. Our model was assessed using real-world patient data from a healthcare web portal, with a comprehensive evaluation approach which considered clustering and recommendation capabilities. In comparison to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.</p>
<hr>
<h2 id="Animal3D-A-Comprehensive-Dataset-of-3D-Animal-Pose-and-Shape"><a href="#Animal3D-A-Comprehensive-Dataset-of-3D-Animal-Pose-and-Shape" class="headerlink" title="Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape"></a>Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11737">http://arxiv.org/abs/2308.11737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, Wei Ji, Chen Wang, Xiaoding Yuan, Prakhar Kaushik, Guofeng Zhang, Jie Liu, Yushan Xie, Yawen Cui, Alan Yuille, Adam Kortylewski</li>
</ul>
<p>Abstract:<br>Accurately estimating the 3D pose and shape is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. However, research in this area is held back by the lack of a comprehensive and diverse dataset with high-quality 3D pose and shape annotations. In this paper, we propose Animal3D, the first comprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D consists of 3379 images collected from 40 mammal species, high-quality annotations of 26 keypoints, and importantly the pose and shape parameters of the SMAL model. All annotations were labeled and checked manually in a multi-stage process to ensure highest quality results. Based on the Animal3D dataset, we benchmark representative shape and pose estimation models at: (1) supervised learning from only the Animal3D data, (2) synthetic to real transfer from synthetically generated images, and (3) fine-tuning human pose and shape estimation models. Our experimental results demonstrate that predicting the 3D shape and pose of animals across species remains a very challenging task, despite significant advances in human pose estimation. Our results further demonstrate that synthetic pre-training is a viable strategy to boost the model performance. Overall, Animal3D opens new directions for facilitating future research in animal 3D pose and shape estimation, and is publicly available.</p>
<hr>
<h2 id="Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering"><a href="#Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering" class="headerlink" title="Knowledge Graph Prompting for Multi-Document Question Answering"></a>Knowledge Graph Prompting for Multi-Document Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11730">http://arxiv.org/abs/2308.11730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa Siu, Ruiyi Zhang, Tyler Derr</li>
</ul>
<p>Abstract:<br>The ‘pre-train, prompt, predict’ paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages&#x2F;tables), and edges denoting the semantic&#x2F;lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the LM-guided traverser acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code is at <a target="_blank" rel="noopener" href="https://github.com/YuWVandy/KG-LLM-MDQA">https://github.com/YuWVandy/KG-LLM-MDQA</a>.</p>
<hr>
<h2 id="When-Are-Two-Lists-Better-than-One-Benefits-and-Harms-in-Joint-Decision-making"><a href="#When-Are-Two-Lists-Better-than-One-Benefits-and-Harms-in-Joint-Decision-making" class="headerlink" title="When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making"></a>When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11721">http://arxiv.org/abs/2308.11721</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kpdonahue/benefits_harms_joint_decision_making">https://github.com/kpdonahue/benefits_harms_joint_decision_making</a></li>
<li>paper_authors: Kate Donahue, Kostas Kollias, Sreenivas Gollapudi</li>
</ul>
<p>Abstract:<br>Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k&#x3D;1$, performance is optimized by the algorithm acting alone, and for $k&#x3D;n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to collaborating, even when the human and algorithm have equal accuracy separately. We demonstrate this theoretically for the Mallows model and experimentally for the Random Utilities models of noisy permutations. However, we show this pattern is reversed when the human is anchored on the algorithm’s presented ordering - the joint system always has strictly worse performance. We extend these results to the case where the human and algorithm differ in their accuracy levels, showing that there always exist regimes where a more accurate agent would strictly benefit from collaborating with a less accurate one, but these regimes are asymmetric between the human and the algorithm’s accuracy.</p>
<hr>
<h2 id="Collect-Measure-Repeat-Reliability-Factors-for-Responsible-AI-Data-Collection"><a href="#Collect-Measure-Repeat-Reliability-Factors-for-Responsible-AI-Data-Collection" class="headerlink" title="Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection"></a>Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12885">http://arxiv.org/abs/2308.12885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oana Inel, Tim Draws, Lora Aroyo</li>
</ul>
<p>Abstract:<br>The rapid entry of machine learning approaches in our daily activities and high-stakes domains demands transparency and scrutiny of their fairness and reliability. To help gauge machine learning models’ robustness, research typically focuses on the massive datasets used for their deployment, e.g., creating and maintaining documentation for understanding their origin, process of development, and ethical considerations. However, data collection for AI is still typically a one-off practice, and oftentimes datasets collected for a certain purpose or application are reused for a different problem. Additionally, dataset annotations may not be representative over time, contain ambiguous or erroneous annotations, or be unable to generalize across issues or domains. Recent research has shown these practices might lead to unfair, biased, or inaccurate outcomes. We argue that data collection for AI should be performed in a responsible manner where the quality of the data is thoroughly scrutinized and measured through a systematic set of appropriate metrics. In this paper, we propose a Responsible AI (RAI) methodology designed to guide the data collection with a set of metrics for an iterative in-depth analysis of the factors influencing the quality and reliability} of the generated data. We propose a granular set of measurements to inform on the internal reliability of a dataset and its external stability over time. We validate our approach across nine existing datasets and annotation tasks and four content modalities. This approach impacts the assessment of data robustness used for AI applied in the real world, where diversity of users and content is eminent. Furthermore, it deals with fairness and accountability aspects in data collection by providing systematic and transparent quality analysis for data collections.</p>
<hr>
<h2 id="SuperCalo-Calorimeter-shower-super-resolution"><a href="#SuperCalo-Calorimeter-shower-super-resolution" class="headerlink" title="SuperCalo: Calorimeter shower super-resolution"></a>SuperCalo: Calorimeter shower super-resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11700">http://arxiv.org/abs/2308.11700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ian-pang/supercalo">https://github.com/ian-pang/supercalo</a></li>
<li>paper_authors: Ian Pang, John Andrew Raine, David Shih</li>
</ul>
<p>Abstract:<br>Calorimeter shower simulation is a major bottleneck in the Large Hadron Collider computational pipeline. There have been recent efforts to employ deep-generative surrogate models to overcome this challenge. However, many of best performing models have training and generation times that do not scale well to high-dimensional calorimeter showers. In this work, we introduce SuperCalo, a flow-based super-resolution model, and demonstrate that high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers. This novel approach presents a way to reduce computational cost, memory requirements and generation time associated with fast calorimeter simulation models. Additionally, we show that the showers upsampled by SuperCalo possess a high degree of variation. This allows a large number of high-dimensional calorimeter showers to be upsampled from much fewer coarse showers with high-fidelity, which results in additional reduction in generation time.</p>
<hr>
<h2 id="Efficient-Benchmarking-of-Language-Models"><a href="#Efficient-Benchmarking-of-Language-Models" class="headerlink" title="Efficient Benchmarking (of Language Models)"></a>Efficient Benchmarking (of Language Models)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11696">http://arxiv.org/abs/2308.11696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen</li>
</ul>
<p>Abstract:<br>The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark ranking. Conversely a slightly different choice of HELM scenarios varies ranking widely. Based on our findings we outline a set of concrete recommendations for more efficient benchmark design and utilization practices leading to dramatic cost savings with minimal loss of benchmark reliability often reducing computation by x100 or more.</p>
<hr>
<h2 id="Semantic-Multi-Resolution-Communications"><a href="#Semantic-Multi-Resolution-Communications" class="headerlink" title="Semantic Multi-Resolution Communications"></a>Semantic Multi-Resolution Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11604">http://arxiv.org/abs/2308.11604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matin Mortaheb, Mohammad A. Amir Khojastepour, Srimat T. Chakradhar, Sennur Ulukus</li>
</ul>
<p>Abstract:<br>Deep learning based joint source-channel coding (JSCC) has demonstrated significant advancements in data reconstruction compared to separate source-channel coding (SSCC). This superiority arises from the suboptimality of SSCC when dealing with finite block-length data. Moreover, SSCC falls short in reconstructing data in a multi-user and&#x2F;or multi-resolution fashion, as it only tries to satisfy the worst channel and&#x2F;or the highest quality data. To overcome these limitations, we propose a novel deep learning multi-resolution JSCC framework inspired by the concept of multi-task learning (MTL). This proposed framework excels at encoding data for different resolutions through hierarchical layers and effectively decodes it by leveraging both current and past layers of encoded data. Moreover, this framework holds great potential for semantic communication, where the objective extends beyond data reconstruction to preserving specific semantic attributes throughout the communication process. These semantic features could be crucial elements such as class labels, essential for classification tasks, or other key attributes that require preservation. Within this framework, each level of encoded data can be carefully designed to retain specific data semantics. As a result, the precision of a semantic classifier can be progressively enhanced across successive layers, emphasizing the preservation of targeted semantics throughout the encoding and decoding stages. We conduct experiments on MNIST and CIFAR10 dataset. The experiment with both datasets illustrates that our proposed method is capable of surpassing the SSCC method in reconstructing data with different resolutions, enabling the extraction of semantic features with heightened confidence in successive layers. This capability is particularly advantageous for prioritizing and preserving more crucial semantic features within the datasets.</p>
<hr>
<h2 id="Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models"><a href="#Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models" class="headerlink" title="Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models"></a>Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11601">http://arxiv.org/abs/2308.11601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surya Narayanan Hari, Matt Thomson</li>
</ul>
<p>Abstract:<br>The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.</p>
<hr>
<h2 id="Low-Tensor-Rank-Learning-of-Neural-Dynamics"><a href="#Low-Tensor-Rank-Learning-of-Neural-Dynamics" class="headerlink" title="Low Tensor Rank Learning of Neural Dynamics"></a>Low Tensor Rank Learning of Neural Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11567">http://arxiv.org/abs/2308.11567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur Pellegrino, N Alex Cayco-Gajic, Angus Chadwick</li>
</ul>
<p>Abstract:<br>Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task by performing a low-tensor-rank decomposition directly on the ground truth weights, and by showing that the method we applied to the data faithfully recovers this low rank structure. Finally, we present a set of mathematical results bounding the matrix and tensor ranks of gradient descent learning dynamics which show that low-tensor-rank weights emerge naturally in RNNs trained to solve low-dimensional tasks. Taken together, our findings provide novel constraints on the evolution of population connectivity over learning in both biological and artificial neural networks, and enable reverse engineering of learning-induced changes in recurrent network dynamics from large-scale neural recordings.</p>
<hr>
<h2 id="Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge"><a href="#Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge" class="headerlink" title="Practical Insights on Incremental Learning of New Human Physical Activity on the Edge"></a>Practical Insights on Incremental Learning of New Human Physical Activity on the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11691">http://arxiv.org/abs/2308.11691</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Arvanitakis, Jingwei Zuo, Mthandazo Ndhlovu, Hakim Hacid</li>
</ul>
<p>Abstract:<br>Edge Machine Learning (Edge ML), which shifts computational intelligence from cloud-based systems to edge devices, is attracting significant interest due to its evident benefits including reduced latency, enhanced data privacy, and decreased connectivity reliance. While these advantages are compelling, they introduce unique challenges absent in traditional cloud-based approaches. In this paper, we delve into the intricacies of Edge-based learning, examining the interdependencies among: (i) constrained data storage on Edge devices, (ii) limited computational power for training, and (iii) the number of learning classes. Through experiments conducted using our MAGNETO system, that focused on learning human activities via data collected from mobile sensors, we highlight these challenges and offer valuable perspectives on Edge ML.</p>
<hr>
<h2 id="Multi-event-Video-Text-Retrieval"><a href="#Multi-event-Video-Text-Retrieval" class="headerlink" title="Multi-event Video-Text Retrieval"></a>Multi-event Video-Text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11551">http://arxiv.org/abs/2308.11551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gengyuanmax/mevtr">https://github.com/gengyuanmax/mevtr</a></li>
<li>paper_authors: Gengyuan Zhang, Jisen Ren, Jindong Gu, Volker Tresp</li>
</ul>
<p>Abstract:<br>Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We present a simple model, Me-Retriever, which incorporates key event video representation and a new MeVTR loss for the MeVTR task. Comprehensive experiments show that this straightforward framework outperforms other models in the Video-to-Text and Text-to-Video tasks, effectively establishing a robust baseline for the MeVTR task. We believe this work serves as a strong foundation for future studies. Code is available at <a target="_blank" rel="noopener" href="https://github.com/gengyuanmax/MeVTR">https://github.com/gengyuanmax/MeVTR</a>.</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/23/cs.LG_2023_08_23/" data-id="cllqyyxal000lbpr8e4sf0dvg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.SD_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.SD_2023_08_23/">cs.SD - 2023-08-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Analysis-of-XLS-R-for-Speech-Quality-Assessment"><a href="#Analysis-of-XLS-R-for-Speech-Quality-Assessment" class="headerlink" title="Analysis of XLS-R for Speech Quality Assessment"></a>Analysis of XLS-R for Speech Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12077">http://arxiv.org/abs/2308.12077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bastiaan Tamm, Rik Vandenberghe, Hugo Van hamme</li>
</ul>
<p>Abstract:<br>In online conferencing applications, estimating the perceived quality of an audio signal is crucial to ensure high quality of experience for the end user. The most reliable way to assess the quality of a speech signal is through human judgments in the form of the mean opinion score (MOS) metric. However, such an approach is labor intensive and not feasible for large-scale applications. The focus has therefore shifted towards automated speech quality assessment through end-to-end training of deep neural networks. Recently, it was shown that leveraging pre-trained wav2vec-based XLS-R embeddings leads to state-of-the-art performance for the task of speech quality prediction. In this paper, we perform an in-depth analysis of the pre-trained model. First, we analyze the performance of embeddings extracted from each layer of XLS-R and also for each size of the model (300M, 1B, 2B parameters). Surprisingly, we find two optimal regions for feature extraction: one in the lower-level features and one in the high-level features. Next, we investigate the reason for the two distinct optima. We hypothesize that the lower-level features capture characteristics of noise and room acoustics, whereas the high-level features focus on speech content and intelligibility. To investigate this, we analyze the sensitivity of the MOS predictions with respect to different levels of corruption in each category. Afterwards, we try fusing the two optimal feature depths to determine if they contain complementary information for MOS prediction. Finally, we compare the performance of the proposed models and assess the generalizability of the models on unseen datasets.</p>
<hr>
<h2 id="Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning"><a href="#Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning" class="headerlink" title="Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning"></a>Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11980">http://arxiv.org/abs/2308.11980</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanbo2020/hgrl">https://github.com/yuanbo2020/hgrl</a></li>
<li>paper_authors: Yuanbo Hou, Siyang Song, Cheng Luo, Andrew Mitchell, Qiaoqiao Ren, Weicheng Xie, Jian Kang, Wenwu Wang, Dick Botteldooren</li>
</ul>
<p>Abstract:<br>Sound events in daily life carry rich information about the objective world. The composition of these sounds affects the mood of people in a soundscape. Most previous approaches only focus on classifying and detecting audio events and scenes, but may ignore their perceptual quality that may impact humans’ listening mood for the environment, e.g. annoyance. To this end, this paper proposes a novel hierarchical graph representation learning (HGRL) approach which links objective audio events (AE) with subjective annoyance ratings (AR) of the soundscape perceived by humans. The hierarchical graph consists of fine-grained event (fAE) embeddings with single-class event semantics, coarse-grained event (cAE) embeddings with multi-class event semantics, and AR embeddings. Experiments show the proposed HGRL successfully integrates AE with AR for AEC and ARP tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR.</p>
<hr>
<h2 id="CED-Consistent-ensemble-distillation-for-audio-tagging"><a href="#CED-Consistent-ensemble-distillation-for-audio-tagging" class="headerlink" title="CED: Consistent ensemble distillation for audio tagging"></a>CED: Consistent ensemble distillation for audio tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11957">http://arxiv.org/abs/2308.11957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/richermans/ced">https://github.com/richermans/ced</a></li>
<li>paper_authors: Heinrich Dinkel, Yongqing Wang, Zhiyong Yan, Junbo Zhang, Yujun Wang</li>
</ul>
<p>Abstract:<br>Augmentation and knowledge distillation (KD) are well-established techniques employed in the realm of audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn’t been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED’s efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at <a target="_blank" rel="noopener" href="https://github.com/RicherMans/CED">https://github.com/RicherMans/CED</a>.</p>
<hr>
<h2 id="Audio-Generation-with-Multiple-Conditional-Diffusion-Model"><a href="#Audio-Generation-with-Multiple-Conditional-Diffusion-Model" class="headerlink" title="Audio Generation with Multiple Conditional Diffusion Model"></a>Audio Generation with Multiple Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11940">http://arxiv.org/abs/2308.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifang Guo, Jianguo Mao, Rui Tao, Long Yan, Kazushige Ouchi, Hong Liu, Xiangdong Wang</li>
</ul>
<p>Abstract:<br>Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a series of evaluation metrics to evaluate the controllability performance. Experimental results demonstrate that our model successfully achieves fine-grained control to accomplish controllable audio generation. Audio samples and our dataset are publicly available at <a target="_blank" rel="noopener" href="https://conditionaudiogen.github.io/conditionaudiogen/">https://conditionaudiogen.github.io/conditionaudiogen/</a></p>
<hr>
<h2 id="Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement"><a href="#Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement" class="headerlink" title="Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement"></a>Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11923">http://arxiv.org/abs/2308.11923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino</li>
</ul>
<p>Abstract:<br>We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.</p>
<hr>
<h2 id="KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods"><a href="#KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods" class="headerlink" title="KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods"></a>KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11863">http://arxiv.org/abs/2308.11863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Nzeyimana</li>
</ul>
<p>Abstract:<br>Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art to the best of our knowledge. Our experiments also indicate that using syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda.</p>
<hr>
<h2 id="Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation"><a href="#Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation" class="headerlink" title="Example-Based Framework for Perceptually Guided Audio Texture Generation"></a>Example-Based Framework for Perceptually Guided Audio Texture Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11859">http://arxiv.org/abs/2308.11859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Purnima Kamath, Chitralekha Gupta, Lonce Wyse, Suranga Nanayakkara</li>
</ul>
<p>Abstract:<br>Generative models for synthesizing audio textures explicitly encode controllability by conditioning the model with labelled data. While datasets for audio textures can be easily recorded in-the-wild, semantically labeling them is expensive, time-consuming, and prone to errors due to human annotator subjectivity. Thus, to control generation, there is a need to automatically infer user-defined perceptual factors of variation in the latent space of a generative model while modelling unlabeled textures. In this paper, we propose an example-based framework to determine vectors to guide texture generation based on user-defined semantic attributes. By synthesizing a few synthetic examples to indicate the presence or absence of a semantic attribute, we can infer the guidance vectors in the latent space of a generative model to control that attribute during generation. Our results show that our method is capable of finding perceptually relevant and deterministic guidance vectors for controllable generation for both discrete as well as continuous textures. Furthermore, we demonstrate the application of this method to other tasks such as selective semantic attribute transfer.</p>
<hr>
<h2 id="Complex-valued-neural-networks-for-voice-anti-spoofing"><a href="#Complex-valued-neural-networks-for-voice-anti-spoofing" class="headerlink" title="Complex-valued neural networks for voice anti-spoofing"></a>Complex-valued neural networks for voice anti-spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11800">http://arxiv.org/abs/2308.11800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas M. Müller, Philip Sperl, Konstantin Böttinger</li>
</ul>
<p>Abstract:<br>Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the “In-the-Wild” anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing.</p>
<hr>
<h2 id="Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model"><a href="#Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model" class="headerlink" title="Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model"></a>Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11773">http://arxiv.org/abs/2308.11773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuezhou Zhang, Amos A Folarin, Judith Dineley, Pauline Conde, Valeria de Angel, Shaoxiong Sun, Yatharth Ranjan, Zulqarnain Rashid, Callum Stewart, Petroula Laiou, Heet Sankesara, Linglong Qian, Faith Matcham, Katie M White, Carolin Oetzmann, Femke Lamers, Sara Siddi, Sara Simblett, Björn W. Schuller, Srinivasan Vairavan, Til Wykes, Josep Maria Haro, Brenda WJH Penninx, Vaibhav A Narayan, Matthew Hotopf, Richard JB Dobson, Nicholas Cummins, RADAR-CNS consortium</li>
</ul>
<p>Abstract:<br>Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested the BERTopic model on a similar smaller dataset (356 speech recordings from 57 participants), obtaining some consistent results. In summary, our findings demonstrate specific speech topics may indicate depression severity. The presented data-driven workflow provides a practical approach to collecting and analyzing large-scale speech data from real-world settings for digital health research.</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/23/cs.SD_2023_08_23/" data-id="cllqyyxan000vbpr82ajy2sl1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/eess.AS_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/eess.AS_2023_08_23/">eess.AS - 2023-08-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Analysis-of-XLS-R-for-Speech-Quality-Assessment"><a href="#Analysis-of-XLS-R-for-Speech-Quality-Assessment" class="headerlink" title="Analysis of XLS-R for Speech Quality Assessment"></a>Analysis of XLS-R for Speech Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12077">http://arxiv.org/abs/2308.12077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bastiaan Tamm, Rik Vandenberghe, Hugo Van hamme</li>
</ul>
<p>Abstract:<br>In online conferencing applications, estimating the perceived quality of an audio signal is crucial to ensure high quality of experience for the end user. The most reliable way to assess the quality of a speech signal is through human judgments in the form of the mean opinion score (MOS) metric. However, such an approach is labor intensive and not feasible for large-scale applications. The focus has therefore shifted towards automated speech quality assessment through end-to-end training of deep neural networks. Recently, it was shown that leveraging pre-trained wav2vec-based XLS-R embeddings leads to state-of-the-art performance for the task of speech quality prediction. In this paper, we perform an in-depth analysis of the pre-trained model. First, we analyze the performance of embeddings extracted from each layer of XLS-R and also for each size of the model (300M, 1B, 2B parameters). Surprisingly, we find two optimal regions for feature extraction: one in the lower-level features and one in the high-level features. Next, we investigate the reason for the two distinct optima. We hypothesize that the lower-level features capture characteristics of noise and room acoustics, whereas the high-level features focus on speech content and intelligibility. To investigate this, we analyze the sensitivity of the MOS predictions with respect to different levels of corruption in each category. Afterwards, we try fusing the two optimal feature depths to determine if they contain complementary information for MOS prediction. Finally, we compare the performance of the proposed models and assess the generalizability of the models on unseen datasets.</p>
<hr>
<h2 id="Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning"><a href="#Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning" class="headerlink" title="Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning"></a>Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11980">http://arxiv.org/abs/2308.11980</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanbo2020/hgrl">https://github.com/yuanbo2020/hgrl</a></li>
<li>paper_authors: Yuanbo Hou, Siyang Song, Cheng Luo, Andrew Mitchell, Qiaoqiao Ren, Weicheng Xie, Jian Kang, Wenwu Wang, Dick Botteldooren</li>
</ul>
<p>Abstract:<br>Sound events in daily life carry rich information about the objective world. The composition of these sounds affects the mood of people in a soundscape. Most previous approaches only focus on classifying and detecting audio events and scenes, but may ignore their perceptual quality that may impact humans’ listening mood for the environment, e.g. annoyance. To this end, this paper proposes a novel hierarchical graph representation learning (HGRL) approach which links objective audio events (AE) with subjective annoyance ratings (AR) of the soundscape perceived by humans. The hierarchical graph consists of fine-grained event (fAE) embeddings with single-class event semantics, coarse-grained event (cAE) embeddings with multi-class event semantics, and AR embeddings. Experiments show the proposed HGRL successfully integrates AE with AR for AEC and ARP tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR.</p>
<hr>
<h2 id="CED-Consistent-ensemble-distillation-for-audio-tagging"><a href="#CED-Consistent-ensemble-distillation-for-audio-tagging" class="headerlink" title="CED: Consistent ensemble distillation for audio tagging"></a>CED: Consistent ensemble distillation for audio tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11957">http://arxiv.org/abs/2308.11957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/richermans/ced">https://github.com/richermans/ced</a></li>
<li>paper_authors: Heinrich Dinkel, Yongqing Wang, Zhiyong Yan, Junbo Zhang, Yujun Wang</li>
</ul>
<p>Abstract:<br>Augmentation and knowledge distillation (KD) are well-established techniques employed in the realm of audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn’t been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED’s efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at <a target="_blank" rel="noopener" href="https://github.com/RicherMans/CED">https://github.com/RicherMans/CED</a>.</p>
<hr>
<h2 id="Audio-Generation-with-Multiple-Conditional-Diffusion-Model"><a href="#Audio-Generation-with-Multiple-Conditional-Diffusion-Model" class="headerlink" title="Audio Generation with Multiple Conditional Diffusion Model"></a>Audio Generation with Multiple Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11940">http://arxiv.org/abs/2308.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifang Guo, Jianguo Mao, Rui Tao, Long Yan, Kazushige Ouchi, Hong Liu, Xiangdong Wang</li>
</ul>
<p>Abstract:<br>Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a series of evaluation metrics to evaluate the controllability performance. Experimental results demonstrate that our model successfully achieves fine-grained control to accomplish controllable audio generation. Audio samples and our dataset are publicly available at <a target="_blank" rel="noopener" href="https://conditionaudiogen.github.io/conditionaudiogen/">https://conditionaudiogen.github.io/conditionaudiogen/</a></p>
<hr>
<h2 id="Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement"><a href="#Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement" class="headerlink" title="Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement"></a>Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11923">http://arxiv.org/abs/2308.11923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino</li>
</ul>
<p>Abstract:<br>We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.</p>
<hr>
<h2 id="KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods"><a href="#KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods" class="headerlink" title="KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods"></a>KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11863">http://arxiv.org/abs/2308.11863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Nzeyimana</li>
</ul>
<p>Abstract:<br>Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art to the best of our knowledge. Our experiments also indicate that using syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda.</p>
<hr>
<h2 id="Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation"><a href="#Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation" class="headerlink" title="Example-Based Framework for Perceptually Guided Audio Texture Generation"></a>Example-Based Framework for Perceptually Guided Audio Texture Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11859">http://arxiv.org/abs/2308.11859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Purnima Kamath, Chitralekha Gupta, Lonce Wyse, Suranga Nanayakkara</li>
</ul>
<p>Abstract:<br>Generative models for synthesizing audio textures explicitly encode controllability by conditioning the model with labelled data. While datasets for audio textures can be easily recorded in-the-wild, semantically labeling them is expensive, time-consuming, and prone to errors due to human annotator subjectivity. Thus, to control generation, there is a need to automatically infer user-defined perceptual factors of variation in the latent space of a generative model while modelling unlabeled textures. In this paper, we propose an example-based framework to determine vectors to guide texture generation based on user-defined semantic attributes. By synthesizing a few synthetic examples to indicate the presence or absence of a semantic attribute, we can infer the guidance vectors in the latent space of a generative model to control that attribute during generation. Our results show that our method is capable of finding perceptually relevant and deterministic guidance vectors for controllable generation for both discrete as well as continuous textures. Furthermore, we demonstrate the application of this method to other tasks such as selective semantic attribute transfer.</p>
<hr>
<h2 id="Complex-valued-neural-networks-for-voice-anti-spoofing"><a href="#Complex-valued-neural-networks-for-voice-anti-spoofing" class="headerlink" title="Complex-valued neural networks for voice anti-spoofing"></a>Complex-valued neural networks for voice anti-spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11800">http://arxiv.org/abs/2308.11800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas M. Müller, Philip Sperl, Konstantin Böttinger</li>
</ul>
<p>Abstract:<br>Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the “In-the-Wild” anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing.</p>
<hr>
<h2 id="Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model"><a href="#Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model" class="headerlink" title="Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model"></a>Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11773">http://arxiv.org/abs/2308.11773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuezhou Zhang, Amos A Folarin, Judith Dineley, Pauline Conde, Valeria de Angel, Shaoxiong Sun, Yatharth Ranjan, Zulqarnain Rashid, Callum Stewart, Petroula Laiou, Heet Sankesara, Linglong Qian, Faith Matcham, Katie M White, Carolin Oetzmann, Femke Lamers, Sara Siddi, Sara Simblett, Björn W. Schuller, Srinivasan Vairavan, Til Wykes, Josep Maria Haro, Brenda WJH Penninx, Vaibhav A Narayan, Matthew Hotopf, Richard JB Dobson, Nicholas Cummins, RADAR-CNS consortium</li>
</ul>
<p>Abstract:<br>Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested the BERTopic model on a similar smaller dataset (356 speech recordings from 57 participants), obtaining some consistent results. In summary, our findings demonstrate specific speech topics may indicate depression severity. The presented data-driven workflow provides a practical approach to collecting and analyzing large-scale speech data from real-world settings for digital health research.</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/23/eess.AS_2023_08_23/" data-id="cllqyyxap0012bpr8ff09gaf6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/eess.IV_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/eess.IV_2023_08_23/">eess.IV - 2023-08-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Tumor-Centered-Patching-for-Enhanced-Medical-Image-Segmentation"><a href="#Tumor-Centered-Patching-for-Enhanced-Medical-Image-Segmentation" class="headerlink" title="Tumor-Centered Patching for Enhanced Medical Image Segmentation"></a>Tumor-Centered Patching for Enhanced Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12168">http://arxiv.org/abs/2308.12168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutyyba Asghar, Ahmad Raza Shahid, Akhtar Jamil, Kiran Aftab, Syed Ather Enam</li>
</ul>
<p>Abstract:<br>The realm of medical image diagnosis has advanced significantly with the integration of computer-aided diagnosis and surgical systems. However, challenges persist, particularly in achieving precise image segmentation. While deep learning techniques show potential, obstacles like limited resources, slow convergence, and class imbalance impede their effectiveness. Traditional patch-based methods, though common, struggle to capture intricate tumor boundaries and often lead to redundant samples, compromising computational efficiency and feature quality. To tackle these issues, this research introduces an innovative approach centered on the tumor itself for patch-based image analysis. This novel tumor-centered patching method aims to address the class imbalance and boundary deficiencies, enabling focused and accurate tumor segmentation. By aligning patches with the tumor’s anatomical context, this technique enhances feature extraction accuracy and reduces computational load. Experimental results demonstrate improved class imbalance, with segmentation scores of 0.78, 0.76, and 0.71 for whole, core, and enhancing tumors, respectively using a lightweight simple U-Net. This approach shows potential for enhancing medical image segmentation and improving computer-aided diagnosis systems.</p>
<hr>
<h2 id="DISGAN-Wavelet-informed-Discriminator-Guides-GAN-to-MRI-Super-resolution-with-Noise-Cleaning"><a href="#DISGAN-Wavelet-informed-Discriminator-Guides-GAN-to-MRI-Super-resolution-with-Noise-Cleaning" class="headerlink" title="DISGAN: Wavelet-informed Discriminator Guides GAN to MRI Super-resolution with Noise Cleaning"></a>DISGAN: Wavelet-informed Discriminator Guides GAN to MRI Super-resolution with Noise Cleaning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12084">http://arxiv.org/abs/2308.12084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Wang, Lucas Mahler, Julius Steiglechner, Florian Birk, Klaus Scheffler, Gabriele Lohmann</li>
</ul>
<p>Abstract:<br>MRI super-resolution (SR) and denoising tasks are fundamental challenges in the field of deep learning, which have traditionally been treated as distinct tasks with separate paired training data. In this paper, we propose an innovative method that addresses both tasks simultaneously using a single deep learning model, eliminating the need for explicitly paired noisy and clean images during training. Our proposed model is primarily trained for SR, but also exhibits remarkable noise-cleaning capabilities in the super-resolved images. Instead of conventional approaches that introduce frequency-related operations into the generative process, our novel approach involves the use of a GAN model guided by a frequency-informed discriminator. To achieve this, we harness the power of the 3D Discrete Wavelet Transform (DWT) operation as a frequency constraint within the GAN framework for the SR task on magnetic resonance imaging (MRI) data. Specifically, our contributions include: 1) a 3D generator based on residual-in-residual connected blocks; 2) the integration of the 3D DWT with $1\times 1$ convolution into a DWT+conv unit within a 3D Unet for the discriminator; 3) the use of the trained model for high-quality image SR, accompanied by an intrinsic denoising process. We dub the model “Denoising Induced Super-resolution GAN (DISGAN)” due to its dual effects of SR image generation and simultaneous denoising. Departing from the traditional approach of training SR and denoising tasks as separate models, our proposed DISGAN is trained only on the SR task, but also achieves exceptional performance in denoising. The model is trained on 3D MRI data from dozens of subjects from the Human Connectome Project (HCP) and further evaluated on previously unseen MRI data from subjects with brain tumours and epilepsy to assess its denoising and SR performance.</p>
<hr>
<h2 id="StofNet-Super-resolution-Time-of-Flight-Network"><a href="#StofNet-Super-resolution-Time-of-Flight-Network" class="headerlink" title="StofNet: Super-resolution Time of Flight Network"></a>StofNet: Super-resolution Time of Flight Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12009">http://arxiv.org/abs/2308.12009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Hahne, Michel Hayoz, Raphael Sznitman</li>
</ul>
<p>Abstract:<br>Time of Flight (ToF) is a prevalent depth sensing technology in the fields of robotics, medical imaging, and non-destructive testing. Yet, ToF sensing faces challenges from complex ambient conditions making an inverse modelling from the sparse temporal information intractable. This paper highlights the potential of modern super-resolution techniques to learn varying surroundings for a reliable and accurate ToF detection. Unlike existing models, we tailor an architecture for sub-sample precise semi-global signal localization by combining super-resolution with an efficient residual contraction block to balance between fine signal details and large scale contextual information. We consolidate research on ToF by conducting a benchmark comparison against six state-of-the-art methods for which we employ two publicly available datasets. This includes the release of our SToF-Chirp dataset captured by an airborne ultrasound transducer. Results showcase the superior performance of our proposed StofNet in terms of precision, reliability and model complexity. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/hahnec/stofnet">https://github.com/hahnec/stofnet</a>.</p>
<hr>
<h2 id="Comparing-Autoencoder-to-Geometrical-Features-for-Vascular-Bifurcations-Identification"><a href="#Comparing-Autoencoder-to-Geometrical-Features-for-Vascular-Bifurcations-Identification" class="headerlink" title="Comparing Autoencoder to Geometrical Features for Vascular Bifurcations Identification"></a>Comparing Autoencoder to Geometrical Features for Vascular Bifurcations Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12314">http://arxiv.org/abs/2308.12314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibtissam Essadik, Anass Nouri, Raja Touahni, Florent Autrusseau</li>
</ul>
<p>Abstract:<br>The cerebrovascular tree is a complex anatomical structure that plays a crucial role in the brain irrigation. A precise identification of the bifurcations in the vascular network is essential for understanding various cerebral pathologies. Traditional methods often require manual intervention and are sensitive to variations in data quality. In recent years, deep learning techniques, and particularly autoencoders, have shown promising performances for feature extraction and pattern recognition in a variety of domains. In this paper, we propose two novel approaches for vascular bifurcation identification based respectiveley on Autoencoder and geometrical features. The performance and effectiveness of each method in terms of classification of vascular bifurcations using medical imaging data is presented. The evaluation was performed on a sample database composed of 91 TOF-MRA, using various evaluation measures, including accuracy, F1 score and confusion matrix.</p>
<hr>
<h2 id="Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification"><a href="#Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification" class="headerlink" title="Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification"></a>Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11969">http://arxiv.org/abs/2308.11969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Lambert, Pauline Roca, Florence Forbes, Senan Doyle, Michel Dojat</li>
</ul>
<p>Abstract:<br>The burden of liver tumors is important, ranking as the fourth leading cause of cancer mortality. In case of hepatocellular carcinoma (HCC), the delineation of liver and tumor on contrast-enhanced magnetic resonance imaging (CE-MRI) is performed to guide the treatment strategy. As this task is time-consuming, needs high expertise and could be subject to inter-observer variability there is a strong need for automatic tools. However, challenges arise from the lack of available training data, as well as the high variability in terms of image resolution and MRI sequence. In this work we propose to compare two different pipelines based on anisotropic models to obtain the segmentation of the liver and tumors. The first pipeline corresponds to a baseline multi-class model that performs the simultaneous segmentation of the liver and tumor classes. In the second approach, we train two distinct binary models, one segmenting the liver only and the other the tumors. Our results show that both pipelines exhibit different strengths and weaknesses. Moreover we propose an uncertainty quantification strategy allowing the identification of potential false positive tumor lesions. Both solutions were submitted to the MICCAI 2023 Atlas challenge regarding liver and tumor segmentation.</p>
<hr>
<h2 id="Recovering-a-Molecule’s-3D-Dynamics-from-Liquid-phase-Electron-Microscopy-Movies"><a href="#Recovering-a-Molecule’s-3D-Dynamics-from-Liquid-phase-Electron-Microscopy-Movies" class="headerlink" title="Recovering a Molecule’s 3D Dynamics from Liquid-phase Electron Microscopy Movies"></a>Recovering a Molecule’s 3D Dynamics from Liquid-phase Electron Microscopy Movies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11927">http://arxiv.org/abs/2308.11927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Ye, Yuhang Wang, Hong Zhang, Yiqin Gao, Huan Wang, He Sun</li>
</ul>
<p>Abstract:<br>The dynamics of biomolecules are crucial for our understanding of their functioning in living systems. However, current 3D imaging techniques, such as cryogenic electron microscopy (cryo-EM), require freezing the sample, which limits the observation of their conformational changes in real time. The innovative liquid-phase electron microscopy (liquid-phase EM) technique allows molecules to be placed in the native liquid environment, providing a unique opportunity to observe their dynamics. In this paper, we propose TEMPOR, a Temporal Electron MicroscoPy Object Reconstruction algorithm for liquid-phase EM that leverages an implicit neural representation (INR) and a dynamical variational auto-encoder (DVAE) to recover time series of molecular structures. We demonstrate its advantages in recovering different motion dynamics from two simulated datasets, 7bcq and Cas9. To our knowledge, our work is the first attempt to directly recover 3D structures of a temporally-varying particle from liquid-phase EM movies. It provides a promising new approach for studying molecules’ 3D dynamics in structural biology.</p>
<hr>
<h2 id="Studying-the-Impact-of-Augmentations-on-Medical-Confidence-Calibration"><a href="#Studying-the-Impact-of-Augmentations-on-Medical-Confidence-Calibration" class="headerlink" title="Studying the Impact of Augmentations on Medical Confidence Calibration"></a>Studying the Impact of Augmentations on Medical Confidence Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11902">http://arxiv.org/abs/2308.11902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrit Rao, Joon-Young Lee, Oliver Aalami</li>
</ul>
<p>Abstract:<br>The clinical explainability of convolutional neural networks (CNN) heavily relies on the joint interpretation of a model’s predicted diagnostic label and associated confidence. A highly certain or uncertain model can significantly impact clinical decision-making. Thus, ensuring that confidence estimates reflect the true correctness likelihood for a prediction is essential. CNNs are often poorly calibrated and prone to overconfidence leading to improper measures of uncertainty. This creates the need for confidence calibration. However, accuracy and performance-based evaluations of CNNs are commonly used as the sole benchmark for medical tasks. Taking into consideration the risks associated with miscalibration is of high importance. In recent years, modern augmentation techniques, which cut, mix, and combine images, have been introduced. Such augmentations have benefited CNNs through regularization, robustness to adversarial samples, and calibration. Standard augmentations based on image scaling, rotating, and zooming, are widely leveraged in the medical domain to combat the scarcity of data. In this paper, we evaluate the effects of three modern augmentation techniques, CutMix, MixUp, and CutOut on the calibration and performance of CNNs for medical tasks. CutMix improved calibration the most while CutOut often lowered the level of calibration.</p>
<hr>
<h2 id="Enhanced-Residual-SwinV2-Transformer-for-Learned-Image-Compression"><a href="#Enhanced-Residual-SwinV2-Transformer-for-Learned-Image-Compression" class="headerlink" title="Enhanced Residual SwinV2 Transformer for Learned Image Compression"></a>Enhanced Residual SwinV2 Transformer for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11864">http://arxiv.org/abs/2308.11864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqiang Wang, Feng Liang, Haisheng Fu, Jie Liang, Haipeng Qin, Junzhe Liang</li>
</ul>
<p>Abstract:<br>Recently, the deep learning technology has been successfully applied in the field of image compression, leading to superior rate-distortion performance. However, a challenge of many learning-based approaches is that they often achieve better performance via sacrificing complexity, which making practical deployment difficult. To alleviate this issue, in this paper, we propose an effective and efficient learned image compression framework based on an enhanced residual Swinv2 transformer. To enhance the nonlinear representation of images in our framework, we use a feature enhancement module that consists of three consecutive convolutional layers. In the subsequent coding and hyper coding steps, we utilize a SwinV2 transformer-based attention mechanism to process the input image. The SwinV2 model can help to reduce model complexity while maintaining high performance. Experimental results show that the proposed method achieves comparable performance compared to some recent learned image compression methods on Kodak and Tecnick datasets, and outperforms some traditional codecs including VVC. In particular, our method achieves comparable results while reducing model complexity by 56% compared to these recent methods.</p>
<hr>
<h2 id="Robust-RF-Data-Normalization-for-Deep-Learning"><a href="#Robust-RF-Data-Normalization-for-Deep-Learning" class="headerlink" title="Robust RF Data Normalization for Deep Learning"></a>Robust RF Data Normalization for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11833">http://arxiv.org/abs/2308.11833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Sharifzadeh, Habib Benali, Hassan Rivaz</li>
</ul>
<p>Abstract:<br>Radio frequency (RF) data contain richer information compared to other data types, such as envelope or B-mode, and employing RF data for training deep neural networks has attracted growing interest in ultrasound image processing. However, RF data is highly fluctuating and additionally has a high dynamic range. Most previous studies in the literature have relied on conventional data normalization, which has been adopted within the computer vision community. We demonstrate the inadequacy of those techniques for normalizing RF data and propose that individual standardization of each image substantially enhances the performance of deep neural networks by utilizing the data more efficiently. We compare conventional and proposed normalizations in a phase aberration correction task and illustrate how the former enhances the generality of trained models.</p>
<hr>
<h2 id="Frequency-Space-Prediction-Filtering-for-Phase-Aberration-Correction-in-Plane-Wave-Ultrasound"><a href="#Frequency-Space-Prediction-Filtering-for-Phase-Aberration-Correction-in-Plane-Wave-Ultrasound" class="headerlink" title="Frequency-Space Prediction Filtering for Phase Aberration Correction in Plane-Wave Ultrasound"></a>Frequency-Space Prediction Filtering for Phase Aberration Correction in Plane-Wave Ultrasound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11830">http://arxiv.org/abs/2308.11830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Sharifzadeh, Habib Benali, Hassan Rivaz</li>
</ul>
<p>Abstract:<br>Ultrasound imaging often suffers from image degradation stemming from phase aberration, which represents a significant contributing factor to the overall image degradation in ultrasound imaging. Frequency-space prediction filtering or FXPF is a technique that has been applied within focused ultrasound imaging to alleviate the phase aberration effect. It presupposes the existence of an autoregressive (AR) model across the signals received at the transducer elements and removes any components that do not conform to the established model. In this study, we illustrate the challenge of applying this technique to plane-wave imaging, where, at shallower depths, signals from more distant elements lose relevance, and a fewer number of elements contribute to image reconstruction. While the number of contributing signals varies, adopting a fixed-order AR model across all depths, results in suboptimal performance. To address this challenge, we propose an AR model with an adaptive order and quantify its effectiveness using contrast and generalized contrast-to-noise ratio metrics.</p>
<hr>
<h2 id="WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters"><a href="#WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters" class="headerlink" title="WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters"></a>WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11776">http://arxiv.org/abs/2308.11776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ange Lou, Jack Noble</li>
</ul>
<p>Abstract:<br>Depth estimation in surgical video plays a crucial role in many image-guided surgery procedures. However, it is difficult and time consuming to create depth map ground truth datasets in surgical videos due in part to inconsistent brightness and noise in the surgical scene. Therefore, building an accurate and robust self-supervised depth and camera ego-motion estimation system is gaining more attention from the computer vision community. Although several self-supervision methods alleviate the need for ground truth depth maps and poses, they still need known camera intrinsic parameters, which are often missing or not recorded. Moreover, the camera intrinsic prediction methods in existing works depend heavily on the quality of datasets. In this work, we aimed to build a self-supervised depth and ego-motion estimation system which can predict not only accurate depth maps and camera pose, but also camera intrinsic parameters. We proposed a cost-volume-based supervision manner to give the system auxiliary supervision for camera parameters prediction. The experimental results showed that the proposed method improved the accuracy of estimated camera parameters, ego-motion, and depth estimation.</p>
<hr>
<h2 id="EndoNet-model-for-automatic-calculation-of-H-score-on-histological-slides"><a href="#EndoNet-model-for-automatic-calculation-of-H-score-on-histological-slides" class="headerlink" title="EndoNet: model for automatic calculation of H-score on histological slides"></a>EndoNet: model for automatic calculation of H-score on histological slides</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11562">http://arxiv.org/abs/2308.11562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Egor Ushakov, Anton Naumov, Vladislav Fomberg, Polina Vishnyakova, Aleksandra Asaturova, Alina Badlaeva, Anna Tregubova, Evgeny Karpulevich, Gennady Sukhikh, Timur Fatkhudinov</li>
</ul>
<p>Abstract:<br>H-score is a semi-quantitative method used to assess the presence and distribution of proteins in tissue samples by combining the intensity of staining and percentage of stained nuclei. It is widely used but time-consuming and can be limited in accuracy and precision. Computer-aided methods may help overcome these limitations and improve the efficiency of pathologists’ workflows. In this work, we developed a model EndoNet for automatic calculation of H-score on histological slides. Our proposed method uses neural networks and consists of two main parts. The first is a detection model which predicts keypoints of centers of nuclei. The second is a H-score module which calculates the value of the H-score using mean pixel values of predicted keypoints. Our model was trained and validated on 1780 annotated tiles with a shape of 100x100 $\mu m$ and performed 0.77 mAP on a test dataset. Moreover, the model can be adjusted to a specific specialist or whole laboratory to reproduce the manner of calculating the H-score. Thus, EndoNet is effective and robust in the analysis of histology slides, which can improve and significantly accelerate the work of pathologists.</p>
<hr>
<h2 id="Open-Set-Synthetic-Image-Source-Attribution"><a href="#Open-Set-Synthetic-Image-Source-Attribution" class="headerlink" title="Open Set Synthetic Image Source Attribution"></a>Open Set Synthetic Image Source Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11557">http://arxiv.org/abs/2308.11557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbang Fang, Tai D. Nguyen, Matthew C. Stamm</li>
</ul>
<p>Abstract:<br>AI-generated images have become increasingly realistic and have garnered significant public attention. While synthetic images are intriguing due to their realism, they also pose an important misinformation threat. To address this new threat, researchers have developed multiple algorithms to detect synthetic images and identify their source generators. However, most existing source attribution techniques are designed to operate in a closed-set scenario, i.e. they can only be used to discriminate between known image generators. By contrast, new image-generation techniques are rapidly emerging. To contend with this, there is a great need for open-set source attribution techniques that can identify when synthetic images have originated from new, unseen generators. To address this problem, we propose a new metric learning-based approach. Our technique works by learning transferrable embeddings capable of discriminating between generators, even when they are not seen during training. An image is first assigned to a candidate generator, then is accepted or rejected based on its distance in the embedding space from known generators’ learned reference points. Importantly, we identify that initializing our source attribution embedding network by pretraining it on image camera identification can improve our embeddings’ transferability. Through a series of experiments, we demonstrate our approach’s ability to attribute the source of synthetic images in open-set scenarios.</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/23/eess.IV_2023_08_23/" data-id="cllqyyxaq0018bpr89sv5cup1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eessp.SP_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/eessp.SP_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/eessp.SP_2023_08_23/">eessp.SP - 2023-08-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        
      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/23/eessp.SP_2023_08_23/" data-id="cllqyyxaq001abpr8ceno33m7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.CL_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.CL_2023_08_23/">cs.CL - 2023-08-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Challenges-of-Machine-Learning-for-Trust-and-Safety-A-Case-Study-on-Misinformation-Detection"><a href="#The-Challenges-of-Machine-Learning-for-Trust-and-Safety-A-Case-Study-on-Misinformation-Detection" class="headerlink" title="The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection"></a>The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12215">http://arxiv.org/abs/2308.12215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ramybaly/News-Media-Reliability">https://github.com/ramybaly/News-Media-Reliability</a></li>
<li>paper_authors: Madelyne Xiao, Jonathan Mayer</li>
</ul>
<p>Abstract:<br>We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is for future work to avoid the pitfalls that we identify.</p>
<hr>
<h2 id="Curriculum-Learning-with-Adam-The-Devil-Is-in-the-Wrong-Details"><a href="#Curriculum-Learning-with-Adam-The-Devil-Is-in-the-Wrong-Details" class="headerlink" title="Curriculum Learning with Adam: The Devil Is in the Wrong Details"></a>Curriculum Learning with Adam: The Devil Is in the Wrong Details</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12202">http://arxiv.org/abs/2308.12202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Weber, Jaap Jumelet, Paul Michel, Elia Bruni, Dieuwke Hupkes</li>
</ul>
<p>Abstract:<br>Curriculum learning (CL) posits that machine learning models – similar to humans – may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chosen hyperparameters. As such, our results contribute to understanding why CL methods work, but at the same time urge caution when claiming positive results.</p>
<hr>
<h2 id="Evaluation-of-Faithfulness-Using-the-Longest-Supported-Subsequence"><a href="#Evaluation-of-Faithfulness-Using-the-Longest-Supported-Subsequence" class="headerlink" title="Evaluation of Faithfulness Using the Longest Supported Subsequence"></a>Evaluation of Faithfulness Using the Longest Supported Subsequence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12157">http://arxiv.org/abs/2308.12157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Mittal, Timo Schick, Mikel Artetxe, Jane Dwivedi-Yu</li>
</ul>
<p>Abstract:<br>As increasingly sophisticated language models emerge, their trustworthiness becomes a pivotal issue, especially in tasks such as summarization and question-answering. Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers. In this paper, we introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, which we refer to as the Longest Supported Subsequence (LSS). Using a new human-annotated dataset, we finetune a model to generate LSS. We introduce a new method of evaluation and demonstrate that these metrics correlate better with human ratings when LSS is employed, as opposed to when it is not. Our proposed metric demonstrates an 18% enhancement over the prevailing state-of-the-art metric for faithfulness on our dataset. Our metric consistently outperforms other metrics on a summarization dataset across six different models. Finally, we compare several popular Large Language Models (LLMs) for faithfulness using this metric. We release the human-annotated dataset built for predicting LSS and our fine-tuned model for evaluating faithfulness.</p>
<hr>
<h2 id="Semantic-Change-Detection-for-the-Romanian-Language"><a href="#Semantic-Change-Detection-for-the-Romanian-Language" class="headerlink" title="Semantic Change Detection for the Romanian Language"></a>Semantic Change Detection for the Romanian Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12131">http://arxiv.org/abs/2308.12131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ds4ai-upb/semanticchange-ro">https://github.com/ds4ai-upb/semanticchange-ro</a></li>
<li>paper_authors: Ciprian-Octavian Truică, Victor Tudose, Elena-Simona Apostol</li>
</ul>
<p>Abstract:<br>Automatic semantic change methods try to identify the changes that appear over time in the meaning of words by analyzing their usage in diachronic corpora. In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and ELMo, on real-world English and Romanian datasets. To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an English dataset (SEMEVAL-CCOHA). Afterward, we focus our experiments on a Romanian dataset, and we underline different aspects of semantic changes in this low-resource language, such as meaning acquisition and loss. The experimental results show that, depending on the corpus, the most important factors to consider are the choice of model and the distance to calculate a score for detecting semantic change.</p>
<hr>
<h2 id="Instruction-Position-Matters-in-Sequence-Generation-with-Large-Language-Models"><a href="#Instruction-Position-Matters-in-Sequence-Generation-with-Large-Language-Models" class="headerlink" title="Instruction Position Matters in Sequence Generation with Large Language Models"></a>Instruction Position Matters in Sequence Generation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12097">http://arxiv.org/abs/2308.12097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adaxry/post-instruction">https://github.com/adaxry/post-instruction</a></li>
<li>paper_authors: Yijin Liu, Xianfeng Zeng, Fandong Meng, Jie Zhou</li>
</ul>
<p>Abstract:<br>Large language models (LLMs) are capable of performing conditional sequence generation tasks, such as translation or summarization, through instruction fine-tuning. The fine-tuning data is generally sequentially concatenated from a specific task instruction, an input sentence, and the corresponding response. Considering the locality modeled by the self-attention mechanism of LLMs, these models face the risk of instruction forgetting when generating responses for long input sentences. To mitigate this issue, we propose enhancing the instruction-following capability of LLMs by shifting the position of task instructions after the input sentences. Theoretical analysis suggests that our straightforward method can alter the model’s learning focus, thereby emphasizing the training of instruction-following capabilities. Concurrently, experimental results demonstrate that our approach consistently outperforms traditional settings across various model scales (1B &#x2F; 7B &#x2F; 13B) and different sequence generation tasks (translation and summarization), without any additional data or annotation costs. Notably, our method significantly improves the zero-shot performance on conditional sequence generation, e.g., up to 9.7 BLEU points on WMT zero-shot translation tasks.</p>
<hr>
<h2 id="Out-of-the-Cage-How-Stochastic-Parrots-Win-in-Cyber-Security-Environments"><a href="#Out-of-the-Cage-How-Stochastic-Parrots-Win-in-Cyber-Security-Environments" class="headerlink" title="Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments"></a>Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12086">http://arxiv.org/abs/2308.12086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Rigaki, Ondřej Lukáš, Carlos A. Catania, Sebastian Garcia</li>
</ul>
<p>Abstract:<br>Large Language Models (LLMs) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks. Despite their inherent limitations, LLM-based designs have shown promising capabilities in planning and navigating open-world scenarios. This paper introduces a novel application of pre-trained LLMs as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes.   We present an approach wherein pre-trained LLMs are leveraged as attacking agents in two reinforcement learning environments. Our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations. In addition, the best LLM agents perform similarly to human testers of the environment without any additional training process. This design highlights the potential of LLMs to efficiently address complex decision-making tasks within cybersecurity.   Furthermore, we introduce a new network security environment named NetSecGame. The environment is designed to eventually support complex multi-agent scenarios within the network security domain. The proposed environment mimics real network attacks and is designed to be highly modular and adaptable for various scenarios.</p>
<hr>
<h2 id="InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4"><a href="#InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4" class="headerlink" title="InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"></a>InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12067">http://arxiv.org/abs/2308.12067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun</li>
</ul>
<p>Abstract:<br>Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient to enable multimodal large language models to generate better output.</p>
<hr>
<h2 id="FlexKBQA-A-Flexible-LLM-Powered-Framework-for-Few-Shot-Knowledge-Base-Question-Answering"><a href="#FlexKBQA-A-Flexible-LLM-Powered-Framework-for-Few-Shot-Knowledge-Base-Question-Answering" class="headerlink" title="FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering"></a>FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12060">http://arxiv.org/abs/2308.12060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leezythu/flexkbqa">https://github.com/leezythu/flexkbqa</a></li>
<li>paper_authors: Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, Zhichao Duan, Bowen Dong, Ning Liu, Jianyong Wang</li>
</ul>
<p>Abstract:<br>Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executionguided self-training method to iterative leverage unlabeled user questions. Furthermore, we explore harnessing the inherent reasoning capability of LLMs to enhance the entire framework. Consequently, FlexKBQA delivers substantial flexibility, encompassing data annotation, deployment, and being domain agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we observe that under the few-shot even the more challenging zero-shot scenarios, FlexKBQA achieves impressive results with a few annotations, surpassing all previous baselines and even approaching the performance of supervised models, achieving a remarkable 93% performance relative to the fully-supervised models. We posit that FlexKBQA represents a significant advancement towards exploring better integration of large and lightweight models. The code is open-sourced.</p>
<hr>
<h2 id="Aligning-Language-Models-with-Offline-Reinforcement-Learning-from-Human-Feedback"><a href="#Aligning-Language-Models-with-Offline-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Aligning Language Models with Offline Reinforcement Learning from Human Feedback"></a>Aligning Language Models with Offline Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12050">http://arxiv.org/abs/2308.12050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Hu, Li Tao, June Yang, Chandler Zhou</li>
</ul>
<p>Abstract:<br>Learning from human preferences is crucial for language models (LMs) to effectively cater to human needs and societal values. Previous research has made notable progress by leveraging human feedback to follow instructions. However, these approaches rely primarily on online reinforcement learning (RL) techniques like Proximal Policy Optimization (PPO), which have been proven unstable and challenging to tune for language models. Moreover, PPO requires complex distributed system implementation, hindering the efficiency of large-scale distributed training. In this study, we propose an offline reinforcement learning from human feedback (RLHF) framework to align LMs using pre-generated samples without interacting with RL environments. Specifically, we explore maximum likelihood estimation (MLE) with filtering, reward-weighted regression (RWR), and Decision Transformer (DT) to align language models to human preferences. By employing a loss function similar to supervised fine-tuning, our methods ensure more stable model training than PPO with a simple machine learning system~(MLSys) and much fewer (around 12.3%) computing resources. Experimental results demonstrate the DT alignment outperforms other Offline RLHF methods and is better than PPO.</p>
<hr>
<h2 id="CgT-GAN-CLIP-guided-Text-GAN-for-Image-Captioning"><a href="#CgT-GAN-CLIP-guided-Text-GAN-for-Image-Captioning" class="headerlink" title="CgT-GAN: CLIP-guided Text GAN for Image Captioning"></a>CgT-GAN: CLIP-guided Text GAN for Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12045">http://arxiv.org/abs/2308.12045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lihr747/cgtgan">https://github.com/lihr747/cgtgan</a></li>
<li>paper_authors: Jiarui Yu, Haoran Li, Yanbin Hao, Bin Zhu, Tong Xu, Xiangnan He</li>
</ul>
<p>Abstract:<br>The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training&#x2F;inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to “see” real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN’s discriminator and the semantic guidance reward computed by the CLIP-based reward module. In addition to the cosine similarity as the semantic guidance reward (i.e., CLIP-cos), we further introduce a novel semantic guidance reward called CLIP-agg, which aligns the generated caption with a weighted text embedding by attentively aggregating the entire corpus. Experimental results on three subtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms state-of-the-art methods significantly across all metrics. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Lihr747/CgtGAN">https://github.com/Lihr747/CgtGAN</a>.</p>
<hr>
<h2 id="IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning"><a href="#IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning" class="headerlink" title="IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning"></a>IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12043">http://arxiv.org/abs/2308.12043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feiyuzhang98/increlora">https://github.com/feiyuzhang98/increlora</a></li>
<li>paper_authors: Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, Yiming Qian</li>
</ul>
<p>Abstract:<br>With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it is not limited by the initial number of training parameters, and each parameter matrix has a higher rank upper bound for the same training overhead. We conduct extensive experiments on GLUE to demonstrate the effectiveness of IncreLoRA. The results show that our method owns higher parameter efficiency, especially when under the low-resource settings where our method significantly outperforms the baselines. Our code is publicly available.</p>
<hr>
<h2 id="Hybrid-Retrieval-and-Multi-stage-Text-Ranking-Solution-at-TREC-2022-Deep-Learning-Track"><a href="#Hybrid-Retrieval-and-Multi-stage-Text-Ranking-Solution-at-TREC-2022-Deep-Learning-Track" class="headerlink" title="Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track"></a>Hybrid Retrieval and Multi-stage Text Ranking Solution at TREC 2022 Deep Learning Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12039">http://arxiv.org/abs/2308.12039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangwei Xu, Yangzhao Zhang, Longhui Zhang, Dingkun Long, Pengjun Xie, Ruijie Guo</li>
</ul>
<p>Abstract:<br>Large-scale text retrieval technology has been widely used in various practical business scenarios. This paper presents our systems for the TREC 2022 Deep Learning Track. We explain the hybrid text retrieval and multi-stage text ranking method adopted in our solution. The retrieval stage combined the two structures of traditional sparse retrieval and neural dense retrieval. In the ranking stage, in addition to the full interaction-based ranking model built on large pre-trained language model, we also proposes a lightweight sub-ranking module to further enhance the final text ranking performance. Evaluation results demonstrate the effectiveness of our proposed approach. Our models achieve the 1st and 4th rank on the test set of passage ranking and document ranking respectively.</p>
<hr>
<h2 id="Large-Multilingual-Models-Pivot-Zero-Shot-Multimodal-Learning-across-Languages"><a href="#Large-Multilingual-Models-Pivot-Zero-Shot-Multimodal-Learning-across-Languages" class="headerlink" title="Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages"></a>Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12038">http://arxiv.org/abs/2308.12038</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openbmb/viscpm">https://github.com/openbmb/viscpm</a></li>
<li>paper_authors: Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, Maosong Sun</li>
</ul>
<p>Abstract:<br>Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a zero-shot manner for both image-to-text and text-to-image generation, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in image-to-text and text-to-image generation, which achieve state-of-the-art (open-source) performance in Chinese. To facilitate future research, we open-source codes and model weights at <a target="_blank" rel="noopener" href="https://github.com/OpenBMB/VisCPM.git">https://github.com/OpenBMB/VisCPM.git</a>.</p>
<hr>
<h2 id="PREFER-Prompt-Ensemble-Learning-via-Feedback-Reflect-Refine"><a href="#PREFER-Prompt-Ensemble-Learning-via-Feedback-Reflect-Refine" class="headerlink" title="PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine"></a>PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12033">http://arxiv.org/abs/2308.12033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zcrwind/prefer">https://github.com/zcrwind/prefer</a></li>
<li>paper_authors: Chenrui Zhang, Lin Liu, Jinpeng Wang, Chuyuan Wang, Xiao Sun, Hongyu Wang, Mingchen Cai</li>
</ul>
<p>Abstract:<br>As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Pompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement. Moreover, to enhance stability of the prompt effect evaluation, we propose a novel prompt bagging method involving forward and backward thinking, which is superior to majority voting and is beneficial for both feedback and weight calculation in boosting. Extensive experiments demonstrate that our PREFER achieves state-of-the-art performance in multiple types of tasks by a significant margin. We have made our code publicly available.</p>
<hr>
<h2 id="From-Quantity-to-Quality-Boosting-LLM-Performance-with-Self-Guided-Data-Selection-for-Instruction-Tuning"><a href="#From-Quantity-to-Quality-Boosting-LLM-Performance-with-Self-Guided-Data-Selection-for-Instruction-Tuning" class="headerlink" title="From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning"></a>From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12032">http://arxiv.org/abs/2308.12032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, Jing Xiao</li>
</ul>
<p>Abstract:<br>In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model’s expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both efficiency and resource-conscious advancements.</p>
<hr>
<h2 id="Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning"><a href="#Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning" class="headerlink" title="Prompt-Based Length Controlled Generation with Reinforcement Learning"></a>Prompt-Based Length Controlled Generation with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12030">http://arxiv.org/abs/2308.12030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, Qun Liu</li>
</ul>
<p>Abstract:<br>Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. We believe this length-controllable ability can provide more potentials towards the era of LLMs.</p>
<hr>
<h2 id="Knowledge-injected-Prompt-Learning-for-Chinese-Biomedical-Entity-Normalization"><a href="#Knowledge-injected-Prompt-Learning-for-Chinese-Biomedical-Entity-Normalization" class="headerlink" title="Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization"></a>Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12025">http://arxiv.org/abs/2308.12025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songhua Yang, Chenghao Zhang, Hongfei Xu, Yuxiang Jia</li>
</ul>
<p>Abstract:<br>The Biomedical Entity Normalization (BEN) task aims to align raw, unstructured medical entities to standard entities, thus promoting data coherence and facilitating better downstream medical applications. Recently, prompt learning methods have shown promising results in this task. However, existing research falls short in tackling the more complex Chinese BEN task, especially in the few-shot scenario with limited medical data, and the vast potential of the external medical knowledge base has yet to be fully harnessed. To address these challenges, we propose a novel Knowledge-injected Prompt Learning (PL-Knowledge) method. Specifically, our approach consists of five stages: candidate entity matching, knowledge extraction, knowledge encoding, knowledge injection, and prediction output. By effectively encoding the knowledge items contained in medical entities and incorporating them into our tailor-made knowledge-injected templates, the additional knowledge enhances the model’s ability to capture latent relationships between medical entities, thus achieving a better match with the standard entities. We extensively evaluate our model on a benchmark dataset in both few-shot and full-scale scenarios. Our method outperforms existing baselines, with an average accuracy boost of 12.96% in few-shot and 0.94% in full-data cases, showcasing its excellence in the BEN task.</p>
<hr>
<h2 id="Reranking-Passages-with-Coarse-to-Fine-Neural-Retriever-using-List-Context-Information"><a href="#Reranking-Passages-with-Coarse-to-Fine-Neural-Retriever-using-List-Context-Information" class="headerlink" title="Reranking Passages with Coarse-to-Fine Neural Retriever using List-Context Information"></a>Reranking Passages with Coarse-to-Fine Neural Retriever using List-Context Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12022">http://arxiv.org/abs/2308.12022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyin Zhu</li>
</ul>
<p>Abstract:<br>Passage reranking is a crucial task in many applications, particularly when dealing with large-scale documents. Traditional neural architectures are limited in retrieving the best passage for a question because they usually match the question to each passage separately, seldom considering contextual information in other passages that can provide comparison and reference information. This paper presents a list-context attention mechanism to augment the passage representation by incorporating the list-context information from other candidates. The proposed coarse-to-fine (C2F) neural retriever addresses the out-of-memory limitation of the passage attention mechanism by dividing the list-context modeling process into two sub-processes, allowing for efficient encoding of context information from a large number of candidate answers. This method can be generally used to encode context information from any number of candidate answers in one pass. Different from most multi-stage information retrieval architectures, this model integrates the coarse and fine rankers into the joint optimization process, allowing for feedback between the two layers to update the model simultaneously. Experiments demonstrate the effectiveness of the proposed approach.</p>
<hr>
<h2 id="From-Instructions-to-Intrinsic-Human-Values-–-A-Survey-of-Alignment-Goals-for-Big-Models"><a href="#From-Instructions-to-Intrinsic-Human-Values-–-A-Survey-of-Alignment-Goals-for-Big-Models" class="headerlink" title="From Instructions to Intrinsic Human Values – A Survey of Alignment Goals for Big Models"></a>From Instructions to Intrinsic Human Values – A Survey of Alignment Goals for Big Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12014">http://arxiv.org/abs/2308.12014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, Xing Xie</li>
</ul>
<p>Abstract:<br>Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, &#96;what to align with’ has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced LLMs. Based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.</p>
<hr>
<h2 id="Graecia-capta-ferum-victorem-cepit-Detecting-Latin-Allusions-to-Ancient-Greek-Literature"><a href="#Graecia-capta-ferum-victorem-cepit-Detecting-Latin-Allusions-to-Ancient-Greek-Literature" class="headerlink" title="Graecia capta ferum victorem cepit. Detecting Latin Allusions to Ancient Greek Literature"></a>Graecia capta ferum victorem cepit. Detecting Latin Allusions to Ancient Greek Literature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12008">http://arxiv.org/abs/2308.12008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederick Riemenschneider, Anette Frank</li>
</ul>
<p>Abstract:<br>Intertextual allusions hold a pivotal role in Classical Philology, with Latin authors frequently referencing Ancient Greek texts. Until now, the automatic identification of these intertextual references has been constrained to monolingual approaches, seeking parallels solely within Latin or Greek texts. In this study, we introduce SPhilBERTa, a trilingual Sentence-RoBERTa model tailored for Classical Philology, which excels at cross-lingual semantic comprehension and identification of identical sentences across Ancient Greek, Latin, and English. We generate new training data by automatically translating English texts into Ancient Greek. Further, we present a case study, demonstrating SPhilBERTa’s capability to facilitate automated detection of intertextual parallels. Our models and resources are available at <a target="_blank" rel="noopener" href="https://github.com/Heidelberg-NLP/ancient-language-models">https://github.com/Heidelberg-NLP/ancient-language-models</a>.</p>
<hr>
<h2 id="Topical-Chat-Towards-Knowledge-Grounded-Open-Domain-Conversations"><a href="#Topical-Chat-Towards-Knowledge-Grounded-Open-Domain-Conversations" class="headerlink" title="Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations"></a>Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11995">http://arxiv.org/abs/2308.11995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexa/Topical-Chat">https://github.com/alexa/Topical-Chat</a></li>
<li>paper_authors: Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-Tur</li>
</ul>
<p>Abstract:<br>Building socialbots that can have deep, engaging open-domain conversations with humans is one of the grand challenges of artificial intelligence (AI). To this end, bots need to be able to leverage world knowledge spanning several domains effectively when conversing with humans who have their own world knowledge. Existing knowledge-grounded conversation datasets are primarily stylized with explicit roles for conversation partners. These datasets also do not explore depth or breadth of topical coverage with transitions in conversations. We introduce Topical-Chat, a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don’t have explicitly defined roles, to help further research in open-domain conversational AI. We also train several state-of-the-art encoder-decoder conversational models on Topical-Chat and perform automated and human evaluation for benchmarking.</p>
<hr>
<h2 id="EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE"><a href="#EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE" class="headerlink" title="EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE"></a>EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11971">http://arxiv.org/abs/2308.11971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Chen, Longteng Guo, Jia Sun, Shuai Shao, Zehuan Yuan, Liang Lin, Dongyu Zhang</li>
</ul>
<p>Abstract:<br>Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy to scale up, enabling better downstream performance with fewer resources and faster training speed. Despite its simplicity, EVE achieves state-of-the-art performance on various vision-language downstream tasks, including visual question answering, visual reasoning, and image-text retrieval.</p>
<hr>
<h2 id="Audio-Generation-with-Multiple-Conditional-Diffusion-Model"><a href="#Audio-Generation-with-Multiple-Conditional-Diffusion-Model" class="headerlink" title="Audio Generation with Multiple Conditional Diffusion Model"></a>Audio Generation with Multiple Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11940">http://arxiv.org/abs/2308.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifang Guo, Jianguo Mao, Rui Tao, Long Yan, Kazushige Ouchi, Hong Liu, Xiangdong Wang</li>
</ul>
<p>Abstract:<br>Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a series of evaluation metrics to evaluate the controllability performance. Experimental results demonstrate that our model successfully achieves fine-grained control to accomplish controllable audio generation. Audio samples and our dataset are publicly available at <a target="_blank" rel="noopener" href="https://conditionaudiogen.github.io/conditionaudiogen/">https://conditionaudiogen.github.io/conditionaudiogen/</a></p>
<hr>
<h2 id="Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement"><a href="#Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement" class="headerlink" title="Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement"></a>Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11923">http://arxiv.org/abs/2308.11923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino</li>
</ul>
<p>Abstract:<br>We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.</p>
<hr>
<h2 id="Diagnosing-Infeasible-Optimization-Problems-Using-Large-Language-Models"><a href="#Diagnosing-Infeasible-Optimization-Problems-Using-Large-Language-Models" class="headerlink" title="Diagnosing Infeasible Optimization Problems Using Large Language Models"></a>Diagnosing Infeasible Optimization Problems Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12923">http://arxiv.org/abs/2308.12923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Gonzalo E. Constante-Flores, Can Li</li>
</ul>
<p>Abstract:<br>Decision-making problems can be represented as mathematical optimization models, finding wide applications in fields such as economics, engineering and manufacturing, transportation, and health care. Optimization models are mathematical abstractions of the problem of making the best decision while satisfying a set of requirements or constraints. One of the primary barriers to deploying these models in practice is the challenge of helping practitioners understand and interpret such models, particularly when they are infeasible, meaning no decision satisfies all the constraints. Existing methods for diagnosing infeasible optimization models often rely on expert systems, necessitating significant background knowledge in optimization. In this paper, we introduce OptiChat, a first-of-its-kind natural language-based system equipped with a chatbot GUI for engaging in interactive conversations about infeasible optimization models. OptiChat can provide natural language descriptions of the optimization model itself, identify potential sources of infeasibility, and offer suggestions to make the model feasible. The implementation of OptiChat is built on GPT-4, which interfaces with an optimization solver to identify the minimal subset of constraints that render the entire optimization problem infeasible, also known as the Irreducible Infeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought, key-retrieve, and sentiment prompts to enhance OptiChat’s reliability. Our experiments demonstrate that OptiChat assists both expert and non-expert users in improving their understanding of the optimization models, enabling them to quickly identify the sources of infeasibility.</p>
<hr>
<h2 id="Bridging-the-Gap-Deciphering-Tabular-Data-Using-Large-Language-Model"><a href="#Bridging-the-Gap-Deciphering-Tabular-Data-Using-Large-Language-Model" class="headerlink" title="Bridging the Gap: Deciphering Tabular Data Using Large Language Model"></a>Bridging the Gap: Deciphering Tabular Data Using Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11891">http://arxiv.org/abs/2308.11891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengyuan Zhang, Peng Chang, Zongcheng Ji</li>
</ul>
<p>Abstract:<br>In the realm of natural language processing, the understanding of tabular data has perpetually stood as a focal point of scholarly inquiry. The emergence of expansive language models, exemplified by the likes of ChatGPT, has ushered in a wave of endeavors wherein researchers aim to harness these models for tasks related to table-based question answering. Central to our investigative pursuits is the elucidation of methodologies that amplify the aptitude of such large language models in discerning both the structural intricacies and inherent content of tables, ultimately facilitating their capacity to provide informed responses to pertinent queries. To this end, we have architected a distinctive module dedicated to the serialization of tables for seamless integration with expansive language models. Additionally, we’ve instituted a corrective mechanism within the model to rectify potential inaccuracies. Experimental results indicate that, although our proposed method trails the SOTA by approximately 11.7% in overall metrics, it surpasses the SOTA by about 1.2% in tests on specific datasets. This research marks the first application of large language models to table-based question answering tasks, enhancing the model’s comprehension of both table structures and content.</p>
<hr>
<h2 id="Cabrita-closing-the-gap-for-foreign-languages"><a href="#Cabrita-closing-the-gap-for-foreign-languages" class="headerlink" title="Cabrita: closing the gap for foreign languages"></a>Cabrita: closing the gap for foreign languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11878">http://arxiv.org/abs/2308.11878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celio Larcher, Marcos Piau, Paulo Finardi, Pedro Gengo, Piero Esposito, Vinicius Caridá</li>
</ul>
<p>Abstract:<br>The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.   The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.   To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We believe that this methodology can be applied to any transformer-like architecture model. To validate the study, we conducted continuous pre-training exclusively using Portuguese text on a 3-billion-parameter model known as OpenLLaMA, resulting in a model named openCabrita 3B. The openCabrita 3B also features a new tokenizer that results in a significant reduction in the number of tokens required to represent the text. In our assessment, for few-shot learning tasks, we achieved similar results with this 3B model compared to a traditional continuous pre-training approach as well as to 7B models English pre-trained models.</p>
<hr>
<h2 id="Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test"><a href="#Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test" class="headerlink" title="Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test"></a>Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11827">http://arxiv.org/abs/2308.11827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saba Rahimi, Tucker Balch, Manuela Veloso</li>
</ul>
<p>Abstract:<br>Large language models such as Open AI’s Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver’s Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model’s passing score fell to 82%. However, the model still fails to answer some questions correctly even with providing library of context, highlighting room for improvement. The research also examined the impact of prompt length and context format, on the model’s performance. Overall, the study provides insights into the limitations and potential improvements for GPT models in question-answering tasks.</p>
<hr>
<h2 id="Towards-an-On-device-Agent-for-Text-Rewriting"><a href="#Towards-an-On-device-Agent-for-Text-Rewriting" class="headerlink" title="Towards an On-device Agent for Text Rewriting"></a>Towards an On-device Agent for Text Rewriting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11807">http://arxiv.org/abs/2308.11807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Zhu, Yinxiao Liu, Felix Stahlberg, Shankar Kumar, Yu-hui Chen, Liangchen Luo, Lei Shu, Renjie Liu, Jindong Chen, Lei Meng</li>
</ul>
<p>Abstract:<br>Large Language Models (LLMs) have demonstrated impressive capabilities for text rewriting. Nonetheless, the large sizes of these models make them impractical for on-device inference, which would otherwise allow for enhanced privacy and economical inference. Creating a smaller yet potent language model for text rewriting presents a formidable challenge because it requires balancing the need for a small size with the need to retain the emergent capabilities of the LLM, that requires costly data collection. To address the above challenge, we introduce a new instruction tuning approach for building a mobile-centric text rewriting model. Our strategies enable the generation of high quality training data without any human labeling. In addition, we propose a heuristic reinforcement learning framework which substantially enhances performance without requiring preference data. To further bridge the performance gap with the larger server-side model, we propose an effective approach that combines the mobile rewrite agent with the server model using a cascade. To tailor the text rewriting tasks to mobile scenarios, we introduce MessageRewriteEval, a benchmark that focuses on text rewriting for messages through natural language instructions. Through empirical experiments, we demonstrate that our on-device model surpasses the current state-of-the-art LLMs in text rewriting while maintaining a significantly reduced model size. Notably, we show that our proposed cascading approach improves model performance.</p>
<hr>
<h2 id="Few-shot-Anomaly-Detection-in-Text-with-Deviation-Learning"><a href="#Few-shot-Anomaly-Detection-in-Text-with-Deviation-Learning" class="headerlink" title="Few-shot Anomaly Detection in Text with Deviation Learning"></a>Few-shot Anomaly Detection in Text with Deviation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11780">http://arxiv.org/abs/2308.11780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anindya Sundar Das, Aravind Ajay, Sriparna Saha, Monowar Bhuyan</li>
</ul>
<p>Abstract:<br>Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably deviate from the reference score in the upper tail of the prior. Additionally, our model is optimized to learn the distinct behavior of anomalies by utilizing a multi-head self-attention layer and multiple instance learning approaches. Comprehensive experiments on several benchmark datasets demonstrate that our proposed approach attains a new level of state-of-the-art performance.</p>
<hr>
<h2 id="Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model"><a href="#Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model" class="headerlink" title="Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model"></a>Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11773">http://arxiv.org/abs/2308.11773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuezhou Zhang, Amos A Folarin, Judith Dineley, Pauline Conde, Valeria de Angel, Shaoxiong Sun, Yatharth Ranjan, Zulqarnain Rashid, Callum Stewart, Petroula Laiou, Heet Sankesara, Linglong Qian, Faith Matcham, Katie M White, Carolin Oetzmann, Femke Lamers, Sara Siddi, Sara Simblett, Björn W. Schuller, Srinivasan Vairavan, Til Wykes, Josep Maria Haro, Brenda WJH Penninx, Vaibhav A Narayan, Matthew Hotopf, Richard JB Dobson, Nicholas Cummins, RADAR-CNS consortium</li>
</ul>
<p>Abstract:<br>Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested the BERTopic model on a similar smaller dataset (356 speech recordings from 57 participants), obtaining some consistent results. In summary, our findings demonstrate specific speech topics may indicate depression severity. The presented data-driven workflow provides a practical approach to collecting and analyzing large-scale speech data from real-world settings for digital health research.</p>
<hr>
<h2 id="Halo-Estimation-and-Reduction-of-Hallucinations-in-Open-Source-Weak-Large-Language-Models"><a href="#Halo-Estimation-and-Reduction-of-Hallucinations-in-Open-Source-Weak-Large-Language-Models" class="headerlink" title="Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models"></a>Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11764">http://arxiv.org/abs/2308.11764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/engsalem/halo">https://github.com/engsalem/halo</a></li>
<li>paper_authors: Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu</li>
</ul>
<p>Abstract:<br>Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.</p>
<hr>
<h2 id="Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering"><a href="#Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering" class="headerlink" title="Knowledge Graph Prompting for Multi-Document Question Answering"></a>Knowledge Graph Prompting for Multi-Document Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11730">http://arxiv.org/abs/2308.11730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa Siu, Ruiyi Zhang, Tyler Derr</li>
</ul>
<p>Abstract:<br>The ‘pre-train, prompt, predict’ paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages&#x2F;tables), and edges denoting the semantic&#x2F;lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the LM-guided traverser acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code is at <a target="_blank" rel="noopener" href="https://github.com/YuWVandy/KG-LLM-MDQA">https://github.com/YuWVandy/KG-LLM-MDQA</a>.</p>
<hr>
<h2 id="Efficient-Benchmarking-of-Language-Models"><a href="#Efficient-Benchmarking-of-Language-Models" class="headerlink" title="Efficient Benchmarking (of Language Models)"></a>Efficient Benchmarking (of Language Models)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11696">http://arxiv.org/abs/2308.11696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen</li>
</ul>
<p>Abstract:<br>The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark ranking. Conversely a slightly different choice of HELM scenarios varies ranking widely. Based on our findings we outline a set of concrete recommendations for more efficient benchmark design and utilization practices leading to dramatic cost savings with minimal loss of benchmark reliability often reducing computation by x100 or more.</p>
<hr>
<h2 id="StoryBench-A-Multifaceted-Benchmark-for-Continuous-Story-Visualization"><a href="#StoryBench-A-Multifaceted-Benchmark-for-Continuous-Story-Visualization" class="headerlink" title="StoryBench: A Multifaceted Benchmark for Continuous Story Visualization"></a>StoryBench: A Multifaceted Benchmark for Continuous Story Visualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11606">http://arxiv.org/abs/2308.11606</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google/storybench">https://github.com/google/storybench</a></li>
<li>paper_authors: Emanuele Bugliarello, Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan, Vittorio Ferrari, Pieter-Jan Kindermans, Paul Voigtlaender</li>
</ul>
<p>Abstract:<br>Generating video stories from text prompts is a complex task. In addition to having high visual quality, videos need to realistically adhere to a sequence of text prompts whilst being consistent throughout the frames. Creating a benchmark for video generation requires data annotated over time, which contrasts with the single caption used often in video datasets. To fill this gap, we collect comprehensive human annotations on three existing datasets, and introduce StoryBench: a new, challenging multi-task benchmark to reliably evaluate forthcoming text-to-video models. Our benchmark includes three video generation tasks of increasing difficulty: action execution, where the next action must be generated starting from a conditioning video; story continuation, where a sequence of actions must be executed starting from a conditioning video; and story generation, where a video must be generated from only text prompts. We evaluate small yet strong text-to-video baselines, and show the benefits of training on story-like data algorithmically generated from existing video captions. Finally, we establish guidelines for human evaluation of video stories, and reaffirm the need of better automatic metrics for video generation. StoryBench aims at encouraging future research efforts in this exciting new area.</p>
<hr>
<h2 id="Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models"><a href="#Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models" class="headerlink" title="Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models"></a>Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11601">http://arxiv.org/abs/2308.11601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surya Narayanan Hari, Matt Thomson</li>
</ul>
<p>Abstract:<br>The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.</p>
<hr>
<h2 id="SeamlessM4T-Massively-Multilingual-Multimodal-Machine-Translation"><a href="#SeamlessM4T-Massively-Multilingual-Multimodal-Machine-Translation" class="headerlink" title="SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation"></a>SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11596">http://arxiv.org/abs/2308.11596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/seamless_communication">https://github.com/facebookresearch/seamless_communication</a></li>
<li>paper_authors: Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-jussà, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzmán, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang</li>
</ul>
<p>Abstract:<br>What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages? While recent breakthroughs in text-based models have pushed machine translation coverage beyond 200 languages, unified speech-to-speech translation models have yet to achieve similar strides. More specifically, conventional speech-to-speech translation systems rely on cascaded systems that perform translation progressively, putting high-performing unified systems out of reach. To address these gaps, we introduce SeamlessM4T, a single model that supports speech-to-speech translation, speech-to-text translation, text-to-speech translation, text-to-text translation, and automatic speech recognition for up to 100 languages. To build this, we used 1 million hours of open speech audio data to learn self-supervised speech representations with w2v-BERT 2.0. Subsequently, we created a multimodal corpus of automatically aligned speech translations. Filtered and combined with human-labeled and pseudo-labeled data, we developed the first multilingual system capable of translating from and into English for both speech and text. On FLEURS, SeamlessM4T sets a new standard for translations into multiple target languages, achieving an improvement of 20% BLEU over the previous SOTA in direct speech-to-text translation. Compared to strong cascaded models, SeamlessM4T improves the quality of into-English translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in speech-to-speech. Tested for robustness, our system performs better against background noises and speaker variations in speech-to-text tasks compared to the current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and added toxicity to assess translation safety. Finally, all contributions in this work are open-sourced and accessible at <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/seamless_communication">https://github.com/facebookresearch/seamless_communication</a></p>
<hr>
<h2 id="Using-ChatGPT-as-a-CAT-tool-in-Easy-Language-translation"><a href="#Using-ChatGPT-as-a-CAT-tool-in-Easy-Language-translation" class="headerlink" title="Using ChatGPT as a CAT tool in Easy Language translation"></a>Using ChatGPT as a CAT tool in Easy Language translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11563">http://arxiv.org/abs/2308.11563</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/katjakaterina/chatgpt4easylang">https://github.com/katjakaterina/chatgpt4easylang</a></li>
<li>paper_authors: Silvana Deilen, Sergio Hernández Garrido, Ekaterina Lapshinova-Koltunski, Christiane Maaß</li>
</ul>
<p>Abstract:<br>This study sets out to investigate the feasibility of using ChatGPT to translate citizen-oriented administrative texts into German Easy Language, a simplified, controlled language variety that is adapted to the needs of people with reading impairments. We use ChatGPT to translate selected texts from websites of German public authorities using two strategies, i.e. linguistic and holistic. We analyse the quality of the generated texts based on different criteria, such as correctness, readability, and syntactic complexity. The results indicated that the generated texts are easier than the standard texts, but that they still do not fully meet the established Easy Language standards. Additionally, the content is not always rendered correctly.</p>
<hr>
<h2 id="BELB-a-Biomedical-Entity-Linking-Benchmark"><a href="#BELB-a-Biomedical-Entity-Linking-Benchmark" class="headerlink" title="BELB: a Biomedical Entity Linking Benchmark"></a>BELB: a Biomedical Entity Linking Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11537">http://arxiv.org/abs/2308.11537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sg-wbi/belb-exp">https://github.com/sg-wbi/belb-exp</a></li>
<li>paper_authors: Samuele Garda, Leon Weber-Genzel, Robert Martin, Ulf Leser</li>
</ul>
<p>Abstract:<br>Biomedical entity linking (BEL) is the task of grounding entity mentions to a knowledge base. It plays a vital role in information extraction pipelines for the life sciences literature. We review recent work in the field and find that, as the task is absent from existing benchmarks for biomedical text mining, different studies adopt different experimental setups making comparisons based on published numbers problematic. Furthermore, neural systems are tested primarily on instances linked to the broad coverage knowledge base UMLS, leaving their performance to more specialized ones, e.g. genes or variants, understudied. We therefore developed BELB, a Biomedical Entity Linking Benchmark, providing access in a unified format to 11 corpora linked to 7 knowledge bases and spanning six entity types: gene, disease, chemical, species, cell line and variant. BELB greatly reduces preprocessing overhead in testing BEL systems on multiple corpora offering a standardized testbed for reproducible experiments. Using BELB we perform an extensive evaluation of six rule-based entity-specific systems and three recent neural approaches leveraging pre-trained language models. Our results reveal a mixed picture showing that neural approaches fail to perform consistently across entity types, highlighting the need of further studies towards entity-agnostic models.</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/23/cs.CL_2023_08_23/" data-id="cllqyyxar001ebpr86q1n8w79" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/22/cs.AI_2023_08_22/" class="article-date">
  <time datetime="2023-08-21T16:00:00.000Z" itemprop="datePublished">2023-08-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CR/">cs.CR</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/22/cs.AI_2023_08_22/">cs.AI - 2023-08-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Furnishing-Sound-Event-Detection-with-Language-Model-Abilities"><a href="#Furnishing-Sound-Event-Detection-with-Language-Model-Abilities" class="headerlink" title="Furnishing Sound Event Detection with Language Model Abilities"></a>Furnishing Sound Event Detection with Language Model Abilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11530">http://arxiv.org/abs/2308.11530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hualei Wang, Jianguo Mao, Zhifang Guo, Jiarui Wan, Hong Liu, Xiangdong Wang</li>
</ul>
<p>Abstract:<br>Recently, the ability of language models (LMs) has attracted increasing attention in visual cross-modality. In this paper, we further explore the generation capacity of LMs for sound event detection (SED), beyond the visual domain. Specifically, we propose an elegant method that aligns audio features and text features to accomplish sound event classification and temporal location. The framework consists of an acoustic encoder, a contrastive module that align the corresponding representations of the text and audio, and a decoupled language decoder that generates temporal and event sequences from the audio characteristic. Compared with conventional works that require complicated processing and barely utilize limited audio features, our model is more concise and comprehensive since language model directly leverage its semantic capabilities to generate the sequences. We investigate different decoupling modules to demonstrate the effectiveness for timestamps capture and event classification. Evaluation results show that the proposed method achieves accurate sequences of sound event detection.</p>
<hr>
<h2 id="TrackFlow-Multi-Object-Tracking-with-Normalizing-Flows"><a href="#TrackFlow-Multi-Object-Tracking-with-Normalizing-Flows" class="headerlink" title="TrackFlow: Multi-Object Tracking with Normalizing Flows"></a>TrackFlow: Multi-Object Tracking with Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11513">http://arxiv.org/abs/2308.11513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianluca Mancusi, Aniello Panariello, Angelo Porrello, Matteo Fabbri, Simone Calderara, Rita Cucchiara</li>
</ul>
<p>Abstract:<br>The field of multi-object tracking has recently seen a renewed interest in the good old schema of tracking-by-detection, as its simplicity and strong priors spare it from the complex design and painful babysitting of tracking-by-attention approaches. In view of this, we aim at extending tracking-by-detection to multi-modal settings, where a comprehensive cost has to be computed from heterogeneous information e.g., 2D motion cues, visual appearance, and pose estimates. More precisely, we follow a case study where a rough estimate of 3D information is also available and must be merged with other traditional metrics (e.g., the IoU). To achieve that, recent approaches resort to either simple rules or complex heuristics to balance the contribution of each cost. However, i) they require careful tuning of tailored hyperparameters on a hold-out set, and ii) they imply these costs to be independent, which does not hold in reality. We address these issues by building upon an elegant probabilistic formulation, which considers the cost of a candidate association as the negative log-likelihood yielded by a deep density estimator, trained to model the conditional joint probability distribution of correct associations. Our experiments, conducted on both simulated and real benchmarks, show that our approach consistently enhances the performance of several tracking-by-detection algorithms.</p>
<hr>
<h2 id="User-Identity-Linkage-in-Social-Media-Using-Linguistic-and-Social-Interaction-Features"><a href="#User-Identity-Linkage-in-Social-Media-Using-Linguistic-and-Social-Interaction-Features" class="headerlink" title="User Identity Linkage in Social Media Using Linguistic and Social Interaction Features"></a>User Identity Linkage in Social Media Using Linguistic and Social Interaction Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11684">http://arxiv.org/abs/2308.11684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Despoina Chatzakou, Juan Soler-Company, Theodora Tsikrika, Leo Wanner, Stefanos Vrochidis, Ioannis Kompatsiaris</li>
</ul>
<p>Abstract:<br>Social media users often hold several accounts in their effort to multiply the spread of their thoughts, ideas, and viewpoints. In the particular case of objectionable content, users tend to create multiple accounts to bypass the combating measures enforced by social media platforms and thus retain their online identity even if some of their accounts are suspended. User identity linkage aims to reveal social media accounts likely to belong to the same natural person so as to prevent the spread of abusive&#x2F;illegal activities. To this end, this work proposes a machine learning-based detection model, which uses multiple attributes of users’ online activity in order to identify whether two or more virtual identities belong to the same real natural person. The models efficacy is demonstrated on two cases on abusive and terrorism-related Twitter content.</p>
<hr>
<h2 id="Large-Language-Models-Sensitivity-to-The-Order-of-Options-in-Multiple-Choice-Questions"><a href="#Large-Language-Models-Sensitivity-to-The-Order-of-Options-in-Multiple-Choice-Questions" class="headerlink" title="Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions"></a>Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11483">http://arxiv.org/abs/2308.11483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pouya Pezeshkpour, Estevam Hruschka</li>
</ul>
<p>Abstract:<br>Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions – commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2&#x2F;3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model’s bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs’ predictions, leading to up to 8 percentage points improvement across different models and benchmarks.</p>
<hr>
<h2 id="Expecting-The-Unexpected-Towards-Broad-Out-Of-Distribution-Detection"><a href="#Expecting-The-Unexpected-Towards-Broad-Out-Of-Distribution-Detection" class="headerlink" title="Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection"></a>Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11480">http://arxiv.org/abs/2308.11480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/servicenow/broad-openood">https://github.com/servicenow/broad-openood</a></li>
<li>paper_authors: Charles Guille-Escuret, Pierre-André Noël, Ioannis Mitliagkas, David Vazquez, Joao Monteiro</li>
</ul>
<p>Abstract:<br>Improving the reliability of deployed machine learning systems often involves developing methods to detect out-of-distribution (OOD) inputs. However, existing research often narrowly focuses on samples from classes that are absent from the training set, neglecting other types of plausible distribution shifts. This limitation reduces the applicability of these methods in real-world scenarios, where systems encounter a wide variety of anomalous inputs. In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expect. As a first step toward broad OOD detection, we learn a generative model of existing detection scores with a Gaussian mixture. By doing so, we present an ensemble approach that offers a more consistent and comprehensive solution for broad OOD detection, demonstrating superior performance compared to existing methods. Our code to download BROAD and reproduce our experiments is publicly available.</p>
<hr>
<h2 id="Revisiting-column-generation-based-matheuristic-for-learning-classification-trees"><a href="#Revisiting-column-generation-based-matheuristic-for-learning-classification-trees" class="headerlink" title="Revisiting column-generation-based matheuristic for learning classification trees"></a>Revisiting column-generation-based matheuristic for learning classification trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11477">http://arxiv.org/abs/2308.11477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krooonal/col_gen_estimator">https://github.com/krooonal/col_gen_estimator</a></li>
<li>paper_authors: Krunal Kishor Patel, Guy Desaulniers, Andrea Lodi</li>
</ul>
<p>Abstract:<br>Decision trees are highly interpretable models for solving classification problems in machine learning (ML). The standard ML algorithms for training decision trees are fast but generate suboptimal trees in terms of accuracy. Other discrete optimization models in the literature address the optimality problem but only work well on relatively small datasets. \cite{firat2020column} proposed a column-generation-based heuristic approach for learning decision trees. This approach improves scalability and can work with large datasets. In this paper, we describe improvements to this column generation approach. First, we modify the subproblem model to significantly reduce the number of subproblems in multiclass classification instances. Next, we show that the data-dependent constraints in the master problem are implied, and use them as cutting planes. Furthermore, we describe a separation model to generate data points for which the linear programming relaxation solution violates their corresponding constraints. We conclude by presenting computational results that show that these modifications result in better scalability.</p>
<hr>
<h2 id="IT3D-Improved-Text-to-3D-Generation-with-Explicit-View-Synthesis"><a href="#IT3D-Improved-Text-to-3D-Generation-with-Explicit-View-Synthesis" class="headerlink" title="IT3D: Improved Text-to-3D Generation with Explicit View Synthesis"></a>IT3D: Improved Text-to-3D Generation with Explicit View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11473">http://arxiv.org/abs/2308.11473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/buaacyw/it3d-text-to-3d">https://github.com/buaacyw/it3d-text-to-3d</a></li>
<li>paper_authors: Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, Guosheng Lin</li>
</ul>
<p>Abstract:<br>Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs). Nonetheless, existing Text-to-3D approaches often grapple with challenges such as over-saturation, inadequate detailing, and unrealistic outputs. This study presents a novel strategy that leverages explicitly synthesized multi-view images to address these issues. Our approach involves the utilization of image-to-image pipelines, empowered by LDMs, to generate posed high-quality images based on the renderings of coarse 3D models. Although the generated images mostly alleviate the aforementioned issues, challenges such as view inconsistency and significant content variance persist due to the inherent generative nature of large diffusion models, posing extensive difficulties in leveraging these images effectively. To overcome this hurdle, we advocate integrating a discriminator alongside a novel Diffusion-GAN dual training strategy to guide the training of 3D models. For the incorporated discriminator, the synthesized multi-view images are considered real data, while the renderings of the optimized 3D models function as fake data. We conduct a comprehensive set of experiments that demonstrate the effectiveness of our method over baseline approaches.</p>
<hr>
<h2 id="Dynamic-Open-Vocabulary-Enhanced-Safe-landing-with-Intelligence-DOVESEI"><a href="#Dynamic-Open-Vocabulary-Enhanced-Safe-landing-with-Intelligence-DOVESEI" class="headerlink" title="Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI)"></a>Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11471">http://arxiv.org/abs/2308.11471</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mistlab/dovesei">https://github.com/mistlab/dovesei</a></li>
<li>paper_authors: Haechan Mark Bong, Rongge Zhang, Ricardo de Azambuja, Giovanni Beltrame</li>
</ul>
<p>Abstract:<br>This work targets what we consider to be the foundational step for urban airborne robots, a safe landing. Our attention is directed toward what we deem the most crucial aspect of the safe landing perception stack: segmentation. We present a streamlined reactive UAV system that employs visual servoing by harnessing the capabilities of open vocabulary image segmentation. This approach can adapt to various scenarios with minimal adjustments, bypassing the necessity for extensive data accumulation for refining internal models, thanks to its open vocabulary methodology. Given the limitations imposed by local authorities, our primary focus centers on operations originating from altitudes of 100 meters. This choice is deliberate, as numerous preceding works have dealt with altitudes up to 30 meters, aligning with the capabilities of small stereo cameras. Consequently, we leave the remaining 20m to be navigated using conventional 3D path planning methods. Utilizing monocular cameras and image segmentation, our findings demonstrate the system’s capability to successfully execute landing maneuvers at altitudes as low as 20 meters. However, this approach is vulnerable to intermittent and occasionally abrupt fluctuations in the segmentation between frames in a video stream. To address this challenge, we enhance the image segmentation output by introducing what we call a dynamic focus: a masking mechanism that self adjusts according to the current landing stage. This dynamic focus guides the control system to avoid regions beyond the drone’s safety radius projected onto the ground, thus mitigating the problems with fluctuations. Through the implementation of this supplementary layer, our experiments have reached improvements in the landing success rate of almost tenfold when compared to global segmentation. All the source code is open source and available online (github.com&#x2F;MISTLab&#x2F;DOVESEI).</p>
<hr>
<h2 id="Internal-Cross-layer-Gradients-for-Extending-Homogeneity-to-Heterogeneity-in-Federated-Learning"><a href="#Internal-Cross-layer-Gradients-for-Extending-Homogeneity-to-Heterogeneity-in-Federated-Learning" class="headerlink" title="Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning"></a>Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11464">http://arxiv.org/abs/2308.11464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Hin Chan, Rui Zhou, Running Zhao, Zhihan Jiang, Edith C. -H. Ngai</li>
</ul>
<p>Abstract:<br>Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverags internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods can be tailored to accommodate model-homogeneous FL methods such as FedAvg, FedProx, FedNova, Scaffold, and MOON, to expand their capabilities to handle the system heterogeneity. Copious experimental results validate the effectiveness of InCo Aggregation, spotlighting internal cross-layer gradients as a promising avenue to enhance the performance in heterogenous FL.</p>
<hr>
<h2 id="A-Survey-on-Self-Supervised-Representation-Learning"><a href="#A-Survey-on-Self-Supervised-Representation-Learning" class="headerlink" title="A Survey on Self-Supervised Representation Learning"></a>A Survey on Self-Supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11455">http://arxiv.org/abs/2308.11455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/esvit">https://github.com/microsoft/esvit</a></li>
<li>paper_authors: Tobias Uelwer, Jan Robine, Stefan Sylvius Wagner, Marc Höftmann, Eric Upschulte, Sebastian Konietzny, Maike Behrendt, Stefan Harmeling</li>
</ul>
<p>Abstract:<br>Learning meaningful representations is at the heart of many tasks in the field of modern machine learning. Recently, a lot of methods were introduced that allow learning of image representations without supervision. These representations can then be used in downstream tasks like classification or object detection. The quality of these representations is close to supervised learning, while no labeled images are needed. This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other. Furthermore, our survey summarizes the most-recent experimental results reported in the literature in form of a meta-study. Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning.</p>
<hr>
<h2 id="Convergence-guarantee-for-consistency-models"><a href="#Convergence-guarantee-for-consistency-models" class="headerlink" title="Convergence guarantee for consistency models"></a>Convergence guarantee for consistency models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11449">http://arxiv.org/abs/2308.11449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junlong Lyu, Zhitang Chen, Shoubo Feng</li>
</ul>
<p>Abstract:<br>We provide the first convergence guarantees for the Consistency Models (CMs), a newly emerging type of one-step generative models that can generate comparable samples to those generated by Diffusion Models. Our main result is that, under the basic assumptions on score-matching errors, consistency errors and smoothness of the data distribution, CMs can efficiently sample from any realistic data distribution in one step with small $W_2$ error. Our results (1) hold for $L^2$-accurate score and consistency assumption (rather than $L^\infty$-accurate); (2) do note require strong assumptions on the data distribution such as log-Sobelev inequality; (3) scale polynomially in all parameters; and (4) match the state-of-the-art convergence guarantee for score-based generative models (SGMs). We also provide the result that the Multistep Consistency Sampling procedure can further reduce the error comparing to one step sampling, which support the original statement of “Consistency Models, Yang Song 2023”. Our result further imply a TV error guarantee when take some Langevin-based modifications to the output distributions.</p>
<hr>
<h2 id="Aspect-oriented-Opinion-Alignment-Network-for-Aspect-Based-Sentiment-Classification"><a href="#Aspect-oriented-Opinion-Alignment-Network-for-Aspect-Based-Sentiment-Classification" class="headerlink" title="Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification"></a>Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11447">http://arxiv.org/abs/2308.11447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aone-nlp/absa-aoan">https://github.com/aone-nlp/absa-aoan</a></li>
<li>paper_authors: Xueyi Liu, Rui Hou, Yanglei Gan, Da Luo, Changlin Li, Xiaojun Shi, Qiao Liu</li>
</ul>
<p>Abstract:<br>Aspect-based sentiment classification is a crucial problem in fine-grained sentiment analysis, which aims to predict the sentiment polarity of the given aspect according to its context. Previous works have made remarkable progress in leveraging attention mechanism to extract opinion words for different aspects. However, a persistent challenge is the effective management of semantic mismatches, which stem from attention mechanisms that fall short in adequately aligning opinions words with their corresponding aspect in multi-aspect sentences. To address this issue, we propose a novel Aspect-oriented Opinion Alignment Network (AOAN) to capture the contextual association between opinion words and the corresponding aspect. Specifically, we first introduce a neighboring span enhanced module which highlights various compositions of neighboring words and given aspects. In addition, we design a multi-perspective attention mechanism that align relevant opinion information with respect to the given aspect. Extensive experiments on three benchmark datasets demonstrate that our model achieves state-of-the-art results. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/AONE-NLP/ABSA-AOAN">https://github.com/AONE-NLP/ABSA-AOAN</a>.</p>
<hr>
<h2 id="Exploration-of-Rashomon-Set-Assists-Explanations-for-Medical-Data"><a href="#Exploration-of-Rashomon-Set-Assists-Explanations-for-Medical-Data" class="headerlink" title="Exploration of Rashomon Set Assists Explanations for Medical Data"></a>Exploration of Rashomon Set Assists Explanations for Medical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11446">http://arxiv.org/abs/2308.11446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katarzyna Kobylińska, Mateusz Krzyziński, Rafał Machowicz, Mariusz Adamek, Przemysław Biecek</li>
</ul>
<p>Abstract:<br>The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric. However, this approach leads to abandoning a more profound analysis of slightly inferior models. Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions. This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis. This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach. The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\texttt{Rashomon_DETECT}$ algorithm. This algorithm compares profiles illustrating prediction dependencies on variable values generated by eXplainable Artificial Intelligence (XAI) techniques. To quantify differences in variable effects among models, we introduce the Profile Disparity Index (PDI) based on measures from functional data analysis. To illustrate the effectiveness of our approach, we showcase its application in predicting survival among hemophagocytic lymphohistiocytosis (HLH) patients - a foundational case study. Additionally, we benchmark our approach on other medical data sets, demonstrating its versatility and utility in various contexts.</p>
<hr>
<h2 id="Inferring-gender-from-name-a-large-scale-performance-evaluation-study"><a href="#Inferring-gender-from-name-a-large-scale-performance-evaluation-study" class="headerlink" title="Inferring gender from name: a large scale performance evaluation study"></a>Inferring gender from name: a large scale performance evaluation study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12381">http://arxiv.org/abs/2308.12381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kriste Krstovski, Yao Lu, Ye Xu</li>
</ul>
<p>Abstract:<br>A person’s gender is a crucial piece of information when performing research across a wide range of scientific disciplines, such as medicine, sociology, political science, and economics, to name a few. However, in increasing instances, especially given the proliferation of big data, gender information is not readily available. In such cases researchers need to infer gender from readily available information, primarily from persons’ names. While inferring gender from name may raise some ethical questions, the lack of viable alternatives means that researchers have to resort to such approaches when the goal justifies the means - in the majority of such studies the goal is to examine patterns and determinants of gender disparities. The necessity of name-to-gender inference has generated an ever-growing domain of algorithmic approaches and software products. These approaches have been used throughout the world in academia, industry, governmental and non-governmental organizations. Nevertheless, the existing approaches have yet to be systematically evaluated and compared, making it challenging to determine the optimal approach for future research. In this work, we conducted a large scale performance evaluation of existing approaches for name-to-gender inference. Analysis are performed using a variety of large annotated datasets of names. We further propose two new hybrid approaches that achieve better performance than any single existing approach.</p>
<hr>
<h2 id="A-Survey-on-Large-Language-Model-based-Autonomous-Agents"><a href="#A-Survey-on-Large-Language-Model-based-Autonomous-Agents" class="headerlink" title="A Survey on Large Language Model based Autonomous Agents"></a>A Survey on Large Language Model based Autonomous Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11432">http://arxiv.org/abs/2308.11432</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paitesanshi/llm-agent-survey">https://github.com/paitesanshi/llm-agent-survey</a></li>
<li>paper_authors: Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen</li>
</ul>
<p>Abstract:<br>Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework that encompasses a majority of the previous work. Additionally, we provide a summary of the various applications of LLM-based AI agents in the domains of social science, natural science, and engineering. Lastly, we discuss the commonly employed evaluation strategies for LLM-based AI agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository for the related references at <a target="_blank" rel="noopener" href="https://github.com/Paitesanshi/LLM-Agent-Survey">https://github.com/Paitesanshi/LLM-Agent-Survey</a>.</p>
<hr>
<h2 id="A-Study-on-the-Impact-of-Non-confounding-Covariates-on-the-Inferential-Performance-of-Methods-based-on-the-Potential-Outcome-Framework"><a href="#A-Study-on-the-Impact-of-Non-confounding-Covariates-on-the-Inferential-Performance-of-Methods-based-on-the-Potential-Outcome-Framework" class="headerlink" title="A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework"></a>A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11676">http://arxiv.org/abs/2308.11676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhao, Shuai Fu, Huiyan Sun</li>
</ul>
<p>Abstract:<br>The Potential Outcome Framework (POF) plays a prominent role in the field of causal inference. Most causal inference models based on the POF (CIMs-B-POF) are designed for eliminating confounding bias and default to an underlying assumption of Confounding Covariates. This assumption posits that the covariates consist solely of confounders. However, the assumption of Confounding Covariates is challenging to maintain in practice, particularly when dealing with high-dimensional covariates. While certain methods have been proposed to differentiate the distinct components of covariates prior to conducting causal inference, the consequences of treating non-confounding covariates as confounders remain unclear. This ambiguity poses a potential risk when applying the CIMs-B-POF in practical scenarios. In this paper, we present a unified graphical framework for the CIMs-B-POF, which greatly enhances the comprehension of these models’ underlying principles. Using this graphical framework, we quantitatively analyze the extent to which the inference performance of CIMs-B-POF is influenced when incorporating various types of non-confounding covariates, such as instrumental variables, mediators, colliders, and adjustment variables. The key findings are: in the task of eliminating confounding bias, the optimal scenario is for the covariates to exclusively encompass confounders; in the subsequent task of inferring counterfactual outcomes, the adjustment variables contribute to more accurate inferences. Furthermore, extensive experiments conducted on synthetic datasets consistently validate these theoretical conclusions.</p>
<hr>
<h2 id="AIxArtist-A-First-Person-Tale-of-Interacting-with-Artificial-Intelligence-to-Escape-Creative-Block"><a href="#AIxArtist-A-First-Person-Tale-of-Interacting-with-Artificial-Intelligence-to-Escape-Creative-Block" class="headerlink" title="AIxArtist: A First-Person Tale of Interacting with Artificial Intelligence to Escape Creative Block"></a>AIxArtist: A First-Person Tale of Interacting with Artificial Intelligence to Escape Creative Block</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11424">http://arxiv.org/abs/2308.11424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Makayla Lewis</li>
</ul>
<p>Abstract:<br>The future of the arts and artificial intelligence (AI) is promising as technology advances. As the use of AI in design becomes more widespread, art practice may not be a human-only art form and could instead become a digitally integrated experience. With enhanced creativity and collaboration, arts and AI could work together towards creating artistic outputs that are visually appealing and meet the needs of the artist and viewer. While it is uncertain how far the integration will go, arts and AI will likely influence one another. This workshop pictorial puts forward first-person research that shares interactions between an HCI researcher and AI as they try to escape the creative block. The pictorial paper explores two questions: How can AI support artists’ creativity, and what does it mean to be explainable in this context? HIs, ChatGPT and Midjourney were engaged; the result was a series of reflections that require further discussion and explorations in the XAIxArts community: Transparency of attribution, the creation process, ethics of asking, and inspiration vs copying.</p>
<hr>
<h2 id="TurboViT-Generating-Fast-Vision-Transformers-via-Generative-Architecture-Search"><a href="#TurboViT-Generating-Fast-Vision-Transformers-via-Generative-Architecture-Search" class="headerlink" title="TurboViT: Generating Fast Vision Transformers via Generative Architecture Search"></a>TurboViT: Generating Fast Vision Transformers via Generative Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11421">http://arxiv.org/abs/2308.11421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Wong, Saad Abbasi, Saeejith Nair</li>
</ul>
<p>Abstract:<br>Vision transformers have shown unprecedented levels of performance in tackling various visual perception tasks in recent years. However, the architectural and computational complexity of such network architectures have made them challenging to deploy in real-world applications with high-throughput, low-memory requirements. As such, there has been significant research recently on the design of efficient vision transformer architectures. In this study, we explore the generation of fast vision transformer architecture designs via generative architecture search (GAS) to achieve a strong balance between accuracy and architectural and computational efficiency. Through this generative architecture search process, we create TurboViT, a highly efficient hierarchical vision transformer architecture design that is generated around mask unit attention and Q-pooling design patterns. The resulting TurboViT architecture design achieves significantly lower architectural computational complexity (&gt;2.47$\times$ smaller than FasterViT-0 while achieving same accuracy) and computational complexity (&gt;3.4$\times$ fewer FLOPs and 0.9% higher accuracy than MobileViT2-2.0) when compared to 10 other state-of-the-art efficient vision transformer network architecture designs within a similar range of accuracy on the ImageNet-1K dataset. Furthermore, TurboViT demonstrated strong inference latency and throughput in both low-latency and batch processing scenarios (&gt;3.21$\times$ lower latency and &gt;3.18$\times$ higher throughput compared to FasterViT-0 for low-latency scenario). These promising results demonstrate the efficacy of leveraging generative architecture search for generating efficient transformer architecture designs for high-throughput scenarios.</p>
<hr>
<h2 id="Tensor-Regression"><a href="#Tensor-Regression" class="headerlink" title="Tensor Regression"></a>Tensor Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11419">http://arxiv.org/abs/2308.11419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tensorly/torch">https://github.com/tensorly/torch</a></li>
<li>paper_authors: Jiani Liu, Ce Zhu, Zhen Long, Yipeng Liu</li>
</ul>
<p>Abstract:<br>Regression analysis is a key area of interest in the field of data analysis and machine learning which is devoted to exploring the dependencies between variables, often using vectors. The emergence of high dimensional data in technologies such as neuroimaging, computer vision, climatology and social networks, has brought challenges to traditional data representation methods. Tensors, as high dimensional extensions of vectors, are considered as natural representations of high dimensional data. In this book, the authors provide a systematic study and analysis of tensor-based regression models and their applications in recent years. It groups and illustrates the existing tensor-based regression methods and covers the basics, core ideas, and theoretical characteristics of most tensor-based regression methods. In addition, readers can learn how to use existing tensor-based regression methods to solve specific regression tasks with multiway data, what datasets can be selected, and what software packages are available to start related work as soon as possible. Tensor Regression is the first thorough overview of the fundamentals, motivations, popular algorithms, strategies for efficient implementation, related applications, available datasets, and software resources for tensor-based regression analysis. It is essential reading for all students, researchers and practitioners of working on high dimensional data.</p>
<hr>
<h2 id="Interpretable-Distribution-Invariant-Fairness-Measures-for-Continuous-Scores"><a href="#Interpretable-Distribution-Invariant-Fairness-Measures-for-Continuous-Scores" class="headerlink" title="Interpretable Distribution-Invariant Fairness Measures for Continuous Scores"></a>Interpretable Distribution-Invariant Fairness Measures for Continuous Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11375">http://arxiv.org/abs/2308.11375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ann-Kristin Becker, Oana Dumitrasc, Klaus Broelemann</li>
</ul>
<p>Abstract:<br>Measures of algorithmic fairness are usually discussed in the context of binary decisions. We extend the approach to continuous scores. So far, ROC-based measures have mainly been suggested for this purpose. Other existing methods depend heavily on the distribution of scores, are unsuitable for ranking tasks, or their effect sizes are not interpretable. Here, we propose a distributionally invariant version of fairness measures for continuous scores with a reasonable interpretation based on the Wasserstein distance. Our measures are easily computable and well suited for quantifying and interpreting the strength of group disparities as well as for comparing biases across different models, datasets, or time points. We derive a link between the different families of existing fairness measures for scores and show that the proposed distributionally invariant fairness measures outperform ROC-based fairness measures because they are more explicit and can quantify significant biases that ROC-based fairness measures miss. Finally, we demonstrate their effectiveness through experiments on the most commonly used fairness benchmark datasets.</p>
<hr>
<h2 id="How-Much-Temporal-Long-Term-Context-is-Needed-for-Action-Segmentation"><a href="#How-Much-Temporal-Long-Term-Context-is-Needed-for-Action-Segmentation" class="headerlink" title="How Much Temporal Long-Term Context is Needed for Action Segmentation?"></a>How Much Temporal Long-Term Context is Needed for Action Segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11358">http://arxiv.org/abs/2308.11358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ltcontext/ltcontext">https://github.com/ltcontext/ltcontext</a></li>
<li>paper_authors: Emad Bahrami, Gianpiero Francesca, Juergen Gall</li>
</ul>
<p>Abstract:<br>Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation. An interesting question that is still open is how much long-term temporal context is needed for optimal performance. While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window. While these approaches show good results, their performance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Breakfast, and Assembly101. Our experiments show that modeling the full context of a video is necessary to obtain the best performance for temporal action segmentation.</p>
<hr>
<h2 id="Semantic-RGB-D-Image-Synthesis"><a href="#Semantic-RGB-D-Image-Synthesis" class="headerlink" title="Semantic RGB-D Image Synthesis"></a>Semantic RGB-D Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11356">http://arxiv.org/abs/2308.11356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Li, Rong Li, Juergen Gall</li>
</ul>
<p>Abstract:<br>Collecting diverse sets of training images for RGB-D semantic image segmentation is not always possible. In particular, when robots need to operate in privacy-sensitive areas like homes, the collection is often limited to a small set of locations. As a consequence, the annotated images lack diversity in appearance and approaches for RGB-D semantic image segmentation tend to overfit the training data. In this paper, we thus introduce semantic RGB-D image synthesis to address this problem. It requires synthesising a realistic-looking RGB-D image for a given semantic label map. Current approaches, however, are uni-modal and cannot cope with multi-modal data. Indeed, we show that extending uni-modal approaches to multi-modal data does not perform well. In this paper, we therefore propose a generator for multi-modal data that separates modal-independent information of the semantic layout from the modal-dependent information that is needed to generate an RGB and a depth image, respectively. Furthermore, we propose a discriminator that ensures semantic consistency between the label maps and the generated images and perceptual similarity between the real and generated images. Our comprehensive experiments demonstrate that the proposed method outperforms previous uni-modal methods by a large margin and that the accuracy of an approach for RGB-D semantic segmentation can be significantly improved by mixing real and generated images during training.</p>
<hr>
<h2 id="ProAgent-Building-Proactive-Cooperative-AI-with-Large-Language-Models"><a href="#ProAgent-Building-Proactive-Cooperative-AI-with-Large-Language-Models" class="headerlink" title="ProAgent: Building Proactive Cooperative AI with Large Language Models"></a>ProAgent: Building Proactive Cooperative AI with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11339">http://arxiv.org/abs/2308.11339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, Yitao Liang, Yaodong Yang</li>
</ul>
<p>Abstract:<br>Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent’s capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates’ forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations conducted within the framework of \textit{Overcook-AI} unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training in cooperation with AI agents. Further, when cooperating with human proxy models, its performance exhibits an average improvement exceeding 10% compared to the current state-of-the-art, COLE. The advancement was consistently observed across diverse scenarios involving interactions with both AI agents of varying characteristics and human counterparts. These findings inspire future research for human-robot collaborations. For a hands-on demonstration, please visit \url{<a target="_blank" rel="noopener" href="https://pku-proagent.github.io}/">https://pku-proagent.github.io}</a>.</p>
<hr>
<h2 id="On-the-Opportunities-and-Challenges-of-Offline-Reinforcement-Learning-for-Recommender-Systems"><a href="#On-the-Opportunities-and-Challenges-of-Offline-Reinforcement-Learning-for-Recommender-Systems" class="headerlink" title="On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems"></a>On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11336">http://arxiv.org/abs/2308.11336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaocong Chen, Siyu Wang, Julian McAuley, Dietmar Jannach, Lina Yao</li>
</ul>
<p>Abstract:<br>Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain limited. This survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domain. Furthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field.</p>
<hr>
<h2 id="GrowCLIP-Data-aware-Automatic-Model-Growing-for-Large-scale-Contrastive-Language-Image-Pre-training"><a href="#GrowCLIP-Data-aware-Automatic-Model-Growing-for-Large-scale-Contrastive-Language-Image-Pre-training" class="headerlink" title="GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training"></a>GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11331">http://arxiv.org/abs/2308.11331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinchi Deng, Han Shi, Runhui Huang, Changlin Li, Hang Xu, Jianhua Han, James Kwok, Shen Zhao, Wei Zhang, Xiaodan Liang</li>
</ul>
<p>Abstract:<br>Cross-modal pre-training has shown impressive performance on a wide range of downstream tasks, benefiting from massive image-text pairs collected from the Internet. In practice, online data are growing constantly, highlighting the importance of the ability of pre-trained model to learn from data that is continuously growing. Existing works on cross-modal pre-training mainly focus on training a network with fixed architecture. However, it is impractical to limit the model capacity when considering the continuously growing nature of pre-training data in real-world applications. On the other hand, it is important to utilize the knowledge in the current model to obtain efficient training and better performance. To address the above issues, in this paper, we propose GrowCLIP, a data-driven automatic model growing algorithm for contrastive language-image pre-training with continuous image-text pairs as input. Specially, we adopt a dynamic growth space and seek out the optimal architecture at each growth step to adapt to online learning scenarios. And the shared encoder is proposed in our growth space to enhance the degree of cross-modal fusion. Besides, we explore the effect of growth in different dimensions, which could provide future references for the design of cross-modal model architecture. Finally, we employ parameter inheriting with momentum (PIM) to maintain the previous knowledge and address the issue of the local minimum dilemma. Compared with the existing methods, GrowCLIP improves 2.3% average top-1 accuracy on zero-shot image classification of 9 downstream tasks. As for zero-shot image retrieval, GrowCLIP can improve 1.2% for top-1 image-to-text recall on Flickr30K dataset.</p>
<hr>
<h2 id="From-Mundane-to-Meaningful-AI’s-Influence-on-Work-Dynamics-–-evidence-from-ChatGPT-and-Stack-Overflow"><a href="#From-Mundane-to-Meaningful-AI’s-Influence-on-Work-Dynamics-–-evidence-from-ChatGPT-and-Stack-Overflow" class="headerlink" title="From Mundane to Meaningful: AI’s Influence on Work Dynamics – evidence from ChatGPT and Stack Overflow"></a>From Mundane to Meaningful: AI’s Influence on Work Dynamics – evidence from ChatGPT and Stack Overflow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11302">http://arxiv.org/abs/2308.11302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Gallea</li>
</ul>
<p>Abstract:<br>This paper illustrates how generative AI could give opportunities for big productivity gains but also opens up questions about the impact of these new powerful technologies on the way we work and share knowledge. More specifically, we explore how ChatGPT changed a fundamental aspect of coding: problem-solving. To do so, we exploit the effect of the sudden release of ChatGPT on the 30th of November 2022 on the usage of the largest online community for coders: Stack Overflow. Using quasi-experimental methods (Difference-in-Difference), we find a significant drop in the number of questions. In addition, the questions are better documented after the release of ChatGPT. Finally, we find evidence that the remaining questions are more complex. These findings suggest not only productivity gains but also a fundamental change in the way we work where routine inquiries are solved by AI allowing humans to focus on more complex tasks.</p>
<hr>
<h2 id="Improving-Knot-Prediction-in-Wood-Logs-with-Longitudinal-Feature-Propagation"><a href="#Improving-Knot-Prediction-in-Wood-Logs-with-Longitudinal-Feature-Propagation" class="headerlink" title="Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation"></a>Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11291">http://arxiv.org/abs/2308.11291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeremyfix/icvs2023">https://github.com/jeremyfix/icvs2023</a></li>
<li>paper_authors: Salim Khazem, Jeremy Fix, Cédric Pradalier</li>
</ul>
<p>Abstract:<br>The quality of a wood log in the wood industry depends heavily on the presence of both outer and inner defects, including inner knots that are a result of the growth of tree branches. Today, locating the inner knots require the use of expensive equipment such as X-ray scanners. In this paper, we address the task of predicting the location of inner defects from the outer shape of the logs. The dataset is built by extracting both the contours and the knots with X-ray measurements. We propose to solve this binary segmentation task by leveraging convolutional recurrent neural networks. Once the neural network is trained, inference can be performed from the outer shape measured with cheap devices such as laser profilers. We demonstrate the effectiveness of our approach on fir and spruce tree species and perform ablation on the recurrence to demonstrate its importance.</p>
<hr>
<h2 id="ShadowNet-for-Data-Centric-Quantum-System-Learning"><a href="#ShadowNet-for-Data-Centric-Quantum-System-Learning" class="headerlink" title="ShadowNet for Data-Centric Quantum System Learning"></a>ShadowNet for Data-Centric Quantum System Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11290">http://arxiv.org/abs/2308.11290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Du, Yibo Yang, Tongliang Liu, Zhouchen Lin, Bernard Ghanem, Dacheng Tao</li>
</ul>
<p>Abstract:<br>Understanding the dynamics of large quantum systems is hindered by the curse of dimensionality. Statistical learning offers new possibilities in this regime by neural-network protocols and classical shadows, while both methods have limitations: the former is plagued by the predictive uncertainty and the latter lacks the generalization ability. Here we propose a data-centric learning paradigm combining the strength of these two approaches to facilitate diverse quantum system learning (QSL) tasks. Particularly, our paradigm utilizes classical shadows along with other easily obtainable information of quantum systems to create the training dataset, which is then learnt by neural networks to unveil the underlying mapping rule of the explored QSL problem. Capitalizing on the generalization power of neural networks, this paradigm can be trained offline and excel at predicting previously unseen systems at the inference stage, even with few state copies. Besides, it inherits the characteristic of classical shadows, enabling memory-efficient storage and faithful prediction. These features underscore the immense potential of the proposed data-centric approach in discovering novel and large-scale quantum systems. For concreteness, we present the instantiation of our paradigm in quantum state tomography and direct fidelity estimation tasks and conduct numerical analysis up to 60 qubits. Our work showcases the profound prospects of data-centric artificial intelligence to advance QSL in a faithful and generalizable manner.</p>
<hr>
<h2 id="Recording-of-50-Business-Assignments"><a href="#Recording-of-50-Business-Assignments" class="headerlink" title="Recording of 50 Business Assignments"></a>Recording of 50 Business Assignments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12211">http://arxiv.org/abs/2308.12211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Sroka, Mohammadreza Fani Sani</li>
</ul>
<p>Abstract:<br>One of the main use cases of process mining is to discover and analyze how users follow business assignments, providing valuable insights into process efficiency and optimization. In this paper, we present a comprehensive dataset consisting of 50 real business processes. The dataset holds significant potential for research in various applications, including task mining and process automation which is a valuable resource for researchers and practitioners.</p>
<hr>
<h2 id="CNN-based-Cuneiform-Sign-Detection-Learned-from-Annotated-3D-Renderings-and-Mapped-Photographs-with-Illumination-Augmentation"><a href="#CNN-based-Cuneiform-Sign-Detection-Learned-from-Annotated-3D-Renderings-and-Mapped-Photographs-with-Illumination-Augmentation" class="headerlink" title="CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation"></a>CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11277">http://arxiv.org/abs/2308.11277</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernst Stötzner, Timo Homburg, Hubert Mara</li>
</ul>
<p>Abstract:<br>Motivated by the challenges of the Digital Ancient Near Eastern Studies (DANES) community, we develop digital tools for processing cuneiform script being a 3D script imprinted into clay tablets used for more than three millennia and at least eight major languages. It consists of thousands of characters that have changed over time and space. Photographs are the most common representations usable for machine learning, while ink drawings are prone to interpretation. Best suited 3D datasets that are becoming available. We created and used the HeiCuBeDa and MaiCuBeDa datasets, which consist of around 500 annotated tablets. For our novel OCR-like approach to mixed image data, we provide an additional mapping tool for transferring annotations between 3D renderings and photographs. Our sign localization uses a RepPoints detector to predict the locations of characters as bounding boxes. We use image data from GigaMesh’s MSII (curvature, see <a target="_blank" rel="noopener" href="https://gigamesh.eu/">https://gigamesh.eu</a>) based rendering, Phong-shaded 3D models, and photographs as well as illumination augmentation. The results show that using rendered 3D images for sign detection performs better than other work on photographs. In addition, our approach gives reasonably good results for photographs only, while it is best used for mixed datasets. More importantly, the Phong renderings, and especially the MSII renderings, improve the results on photographs, which is the largest dataset on a global scale.</p>
<hr>
<h2 id="Music-Understanding-LLaMA-Advancing-Text-to-Music-Generation-with-Question-Answering-and-Captioning"><a href="#Music-Understanding-LLaMA-Advancing-Text-to-Music-Generation-with-Question-Answering-and-Captioning" class="headerlink" title="Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning"></a>Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11276">http://arxiv.org/abs/2308.11276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, Ying Shan</li>
</ul>
<p>Abstract:<br>Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.</p>
<hr>
<h2 id="Robust-Lagrangian-and-Adversarial-Policy-Gradient-for-Robust-Constrained-Markov-Decision-Processes"><a href="#Robust-Lagrangian-and-Adversarial-Policy-Gradient-for-Robust-Constrained-Markov-Decision-Processes" class="headerlink" title="Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes"></a>Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11267">http://arxiv.org/abs/2308.11267</a></li>
<li>repo_url: None</li>
<li>paper_authors: David M. Bossens</li>
</ul>
<p>Abstract:<br>The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy through gradient descent rather than indirectly and abruptly through constrained optimisation on a sorted value list. A theoretical analysis first derives the Lagrangian policy gradient for the policy optimisation of both proposed algorithms and then the adversarial policy gradient to learn the adversary for Adversarial RCPG. Empirical experiments injecting perturbations in inventory management and safe navigation tasks demonstrate the competitive performance of both algorithms compared to traditional RCPG variants as well as non-robust and non-constrained ablations. In particular, Adversarial RCPG ranks among the top two performing algorithms on all tests.</p>
<hr>
<h2 id="Efficient-Last-iterate-Convergence-Algorithms-in-Solving-Games"><a href="#Efficient-Last-iterate-Convergence-Algorithms-in-Solving-Games" class="headerlink" title="Efficient Last-iterate Convergence Algorithms in Solving Games"></a>Efficient Last-iterate Convergence Algorithms in Solving Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11256">http://arxiv.org/abs/2308.11256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linjian Meng, Zhenxing Ge, Wenbin Li, Bo An, Yang Gao</li>
</ul>
<p>Abstract:<br>No-regret algorithms are popular for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs). Many recent works consider the last-iterate convergence no-regret algorithms. Among them, the two most famous algorithms are Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weight Update (OMWU). However, OGDA has high per-iteration complexity. OMWU exhibits a lower per-iteration complexity but poorer empirical performance, and its convergence holds only when NE is unique. Recent works propose a Reward Transformation (RT) framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU. Unfortunately, RT-based algorithms perform worse than OGDA under the same number of iterations, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios. To address these issues, we provide a closer analysis of the RT framework, which holds for both continuous and discrete-time feedback. We demonstrate that the essence of the RT framework is to transform the problem of learning NE in the original game into a series of strongly convex-concave optimization problems (SCCPs). We show that the bottleneck of RT-based algorithms is the speed of solving SCCPs. To improve the their empirical performance, we design a novel transformation method to enable the SCCPs can be solved by Regret Matching+ (RM+), a no-regret algorithm with better empirical performance, resulting in Reward Transformation RM+ (RTRM+). RTRM+ enjoys last-iterate convergence under the discrete-time feedback setting. Using the counterfactual regret decomposition framework, we propose Reward Transformation CFR+ (RTCFR+) to extend RTRM+ to EFGs. Experimental results show that our algorithms significantly outperform existing last-iterate convergence algorithms and RM+ (CFR+).</p>
<hr>
<h2 id="A-survey-on-bias-in-machine-learning-research"><a href="#A-survey-on-bias-in-machine-learning-research" class="headerlink" title="A survey on bias in machine learning research"></a>A survey on bias in machine learning research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11254">http://arxiv.org/abs/2308.11254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnieszka Mikołajczyk-Bareła, Michał Grochowski</li>
</ul>
<p>Abstract:<br>Current research on bias in machine learning often focuses on fairness, while overlooking the roots or causes of bias. However, bias was originally defined as a “systematic error,” often caused by humans at different stages of the research process. This article aims to bridge the gap between past literature on bias in research by providing taxonomy for potential sources of bias and errors in data and models. The paper focus on bias in machine learning pipelines. Survey analyses over forty potential sources of bias in the machine learning (ML) pipeline, providing clear examples for each. By understanding the sources and consequences of bias in machine learning, better methods can be developed for its detecting and mitigating, leading to fairer, more transparent, and more accurate ML models.</p>
<hr>
<h2 id="Modeling-Bends-in-Popular-Music-Guitar-Tablatures"><a href="#Modeling-Bends-in-Popular-Music-Guitar-Tablatures" class="headerlink" title="Modeling Bends in Popular Music Guitar Tablatures"></a>Modeling Bends in Popular Music Guitar Tablatures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12307">http://arxiv.org/abs/2308.12307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/adhooge1/bend-prediction">https://gitlab.com/adhooge1/bend-prediction</a></li>
<li>paper_authors: Alexandre D’Hooge, Louis Bigo, Ken Déguernel</li>
</ul>
<p>Abstract:<br>Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on&#x2F;pull-off or bends.This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard. In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 anda limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of non-guitar music into guitar tablatures.</p>
<hr>
<h2 id="Multi-Source-Domain-Adaptation-for-Cross-Domain-Fault-Diagnosis-of-Chemical-Processes"><a href="#Multi-Source-Domain-Adaptation-for-Cross-Domain-Fault-Diagnosis-of-Chemical-Processes" class="headerlink" title="Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes"></a>Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11247">http://arxiv.org/abs/2308.11247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo Fernandes Montesuma, Michela Mulas, Fred Ngolè Mboula, Francesco Corona, Antoine Souloumiac</li>
</ul>
<p>Abstract:<br>Fault diagnosis is an essential component in process supervision. Indeed, it determines which kind of fault has occurred, given that it has been previously detected, allowing for appropriate intervention. Automatic fault diagnosis systems use machine learning for predicting the fault type from sensor readings. Nonetheless, these models are sensible to changes in the data distributions, which may be caused by changes in the monitored process, such as changes in the mode of operation. This scenario is known as Cross-Domain Fault Diagnosis (CDFD). We provide an extensive comparison of single and multi-source unsupervised domain adaptation (SSDA and MSDA respectively) algorithms for CDFD. We study these methods in the context of the Tennessee-Eastmann Process, a widely used benchmark in the chemical industry. We show that using multiple domains during training has a positive effect, even when no adaptation is employed. As such, the MSDA baseline improves over the SSDA baseline classification accuracy by 23% on average. In addition, under the multiple-sources scenario, we improve classification accuracy of the no adaptation setting by 8.4% on average.</p>
<hr>
<h2 id="Faster-Optimization-in-S-Graphs-Exploiting-Hierarchy"><a href="#Faster-Optimization-in-S-Graphs-Exploiting-Hierarchy" class="headerlink" title="Faster Optimization in S-Graphs Exploiting Hierarchy"></a>Faster Optimization in S-Graphs Exploiting Hierarchy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11242">http://arxiv.org/abs/2308.11242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hriday Bavle, Jose Luis Sanchez-Lopez, Javier Civera, Holger Voos</li>
</ul>
<p>Abstract:<br>3D scene graphs hierarchically represent the environment appropriately organizing different environmental entities in various layers. Our previous work on situational graphs extends the concept of 3D scene graph to SLAM by tightly coupling the robot poses with the scene graph entities, achieving state-of-the-art results. Though, one of the limitations of S-Graphs is scalability in really large environments due to the increased graph size over time, increasing the computational complexity.   To overcome this limitation in this work we present an initial research of an improved version of S-Graphs exploiting the hierarchy to reduce the graph size by marginalizing redundant robot poses and their connections to the observations of the same structural entities. Firstly, we propose the generation and optimization of room-local graphs encompassing all graph entities within a room-like structure. These room-local graphs are used to compress the S-Graphs marginalizing the redundant robot keyframes within the given room. We then perform windowed local optimization of the compressed graph at regular time-distance intervals. A global optimization of the compressed graph is performed every time a loop closure is detected. We show similar accuracy compared to the baseline while showing a 39.81% reduction in the computation time with respect to the baseline.</p>
<hr>
<h2 id="An-Effective-Transformer-based-Contextual-Model-and-Temporal-Gate-Pooling-for-Speaker-Identification"><a href="#An-Effective-Transformer-based-Contextual-Model-and-Temporal-Gate-Pooling-for-Speaker-Identification" class="headerlink" title="An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification"></a>An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11241">http://arxiv.org/abs/2308.11241</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harunorikawano/speaker-identification-with-tgp">https://github.com/harunorikawano/speaker-identification-with-tgp</a></li>
<li>paper_authors: Harunori Kawano, Sota Shimizu</li>
</ul>
<p>Abstract:<br>Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HarunoriKawano/speaker-identification-with-tgp">https://github.com/HarunoriKawano/speaker-identification-with-tgp</a>.</p>
<hr>
<h2 id="ROSGPT-Vision-Commanding-Robots-Using-Only-Language-Models’-Prompts"><a href="#ROSGPT-Vision-Commanding-Robots-Using-Only-Language-Models’-Prompts" class="headerlink" title="ROSGPT_Vision: Commanding Robots Using Only Language Models’ Prompts"></a>ROSGPT_Vision: Commanding Robots Using Only Language Models’ Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11236">http://arxiv.org/abs/2308.11236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bilel-bj/rosgpt_vision">https://github.com/bilel-bj/rosgpt_vision</a></li>
<li>paper_authors: Bilel Benjdira, Anis Koubaa, Anas M. Ali</li>
</ul>
<p>Abstract:<br>In this paper, we argue that the next generation of robots can be commanded using only Language Models’ prompts. Every prompt interrogates separately a specific Robotic Modality via its Modality Language Model (MLM). A central Task Modality mediates the whole communication to execute the robotic mission via a Large Language Model (LLM). This paper gives this new robotic design pattern the name of: Prompting Robotic Modalities (PRM). Moreover, this paper applies this PRM design pattern in building a new robotic framework named ROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using only two prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in natural language, the visual semantic features related to the task under consideration (Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the robotic reaction to the visual description (Task Modality). The framework automates all the mechanisms behind these two prompts. The framework enables the robot to address complex real-world scenarios by processing visual data, making informed decisions, and carrying out actions automatically. The framework comprises one generic vision module and two independent ROS nodes. As a test application, we used ROSGPT_Vision to develop CarMate, which monitors the driver’s distraction on the roads and makes real-time vocal notifications to the driver. We showed how ROSGPT_Vision significantly reduced the development cost compared to traditional methods. We demonstrated how to improve the quality of the application by optimizing the prompting strategies, without delving into technical details. ROSGPT_Vision is shared with the community (link: <a target="_blank" rel="noopener" href="https://github.com/bilel-bj/ROSGPT_Vision">https://github.com/bilel-bj/ROSGPT_Vision</a>) to advance robotic research in this direction and to build more robotic frameworks that implement the PRM design pattern and enables controlling robots using only prompts.</p>
<hr>
<h2 id="Adaptive-White-Box-Watermarking-with-Self-Mutual-Check-Parameters-in-Deep-Neural-Networks"><a href="#Adaptive-White-Box-Watermarking-with-Self-Mutual-Check-Parameters-in-Deep-Neural-Networks" class="headerlink" title="Adaptive White-Box Watermarking with Self-Mutual Check Parameters in Deep Neural Networks"></a>Adaptive White-Box Watermarking with Self-Mutual Check Parameters in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11235">http://arxiv.org/abs/2308.11235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenzhe Gao, Zhaoxia Yin, Hongjian Zhan, Heng Yin, Yue Lu</li>
</ul>
<p>Abstract:<br>Artificial Intelligence (AI) has found wide application, but also poses risks due to unintentional or malicious tampering during deployment. Regular checks are therefore necessary to detect and prevent such risks. Fragile watermarking is a technique used to identify tampering in AI models. However, previous methods have faced challenges including risks of omission, additional information transmission, and inability to locate tampering precisely. In this paper, we propose a method for detecting tampered parameters and bits, which can be used to detect, locate, and restore parameters that have been tampered with. We also propose an adaptive embedding method that maximizes information capacity while maintaining model accuracy. Our approach was tested on multiple neural networks subjected to attacks that modified weight parameters, and our results demonstrate that our method achieved great recovery performance when the modification rate was below 20%. Furthermore, for models where watermarking significantly affected accuracy, we utilized an adaptive bit technique to recover more than 15% of the accuracy loss of the model.</p>
<hr>
<h2 id="Traffic-Flow-Optimisation-for-Lifelong-Multi-Agent-Path-Finding"><a href="#Traffic-Flow-Optimisation-for-Lifelong-Multi-Agent-Path-Finding" class="headerlink" title="Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding"></a>Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11234">http://arxiv.org/abs/2308.11234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Chen, Daniel Harabor, Jiaoyang Li, Peter J. Stuckey</li>
</ul>
<p>Abstract:<br>Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that asks us to compute collision-free paths for a team of agents, all moving across a shared map. Although many works appear on this topic, all current algorithms struggle as the number of agents grows. The principal reason is that existing approaches typically plan free-flow optimal paths, which creates congestion. To tackle this issue we propose a new approach for MAPF where agents are guided to their destination by following congestion-avoiding paths. We evaluate the idea in two large-scale settings: one-shot MAPF, where each agent has a single destination, and lifelong MAPF, where agents are continuously assigned new tasks. For one-shot MAPF we show that our approach substantially improves solution quality. For Lifelong MAPF we report large improvements in overall throughput.</p>
<hr>
<h2 id="On-Premise-AIOps-Infrastructure-for-a-Software-Editor-SME-An-Experience-Report"><a href="#On-Premise-AIOps-Infrastructure-for-a-Software-Editor-SME-An-Experience-Report" class="headerlink" title="On-Premise AIOps Infrastructure for a Software Editor SME: An Experience Report"></a>On-Premise AIOps Infrastructure for a Software Editor SME: An Experience Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11225">http://arxiv.org/abs/2308.11225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anes Bendimerad, Youcef Remil, Romain Mathonat, Mehdi Kaytoue</li>
</ul>
<p>Abstract:<br>Information Technology has become a critical component in various industries, leading to an increased focus on software maintenance and monitoring. With the complexities of modern software systems, traditional maintenance approaches have become insufficient. The concept of AIOps has emerged to enhance predictive maintenance using Big Data and Machine Learning capabilities. However, exploiting AIOps requires addressing several challenges related to the complexity of data and incident management. Commercial solutions exist, but they may not be suitable for certain companies due to high costs, data governance issues, and limitations in covering private software. This paper investigates the feasibility of implementing on-premise AIOps solutions by leveraging open-source tools. We introduce a comprehensive AIOps infrastructure that we have successfully deployed in our company, and we provide the rationale behind different choices that we made to build its various components. Particularly, we provide insights into our approach and criteria for selecting a data management system and we explain its integration. Our experience can be beneficial for companies seeking to internally manage their software maintenance processes with a modern AIOps approach.</p>
<hr>
<h2 id="Evaluating-Large-Language-Models-on-Graphs-Performance-Insights-and-Comparative-Analysis"><a href="#Evaluating-Large-Language-Models-on-Graphs-Performance-Insights-and-Comparative-Analysis" class="headerlink" title="Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis"></a>Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11224">http://arxiv.org/abs/2308.11224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ayame1006/llmtograph">https://github.com/ayame1006/llmtograph</a></li>
<li>paper_authors: Chang Liu, Bo Wu</li>
</ul>
<p>Abstract:<br>Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has demonstrated the capacity to rectify responses from GPT-3.5-turbo and its own previous iterations. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/Ayame1006/LLMtoGraph">https://github.com/Ayame1006/LLMtoGraph</a>.</p>
<hr>
<h2 id="Federated-Learning-on-Patient-Data-for-Privacy-Protecting-Polycystic-Ovary-Syndrome-Treatment"><a href="#Federated-Learning-on-Patient-Data-for-Privacy-Protecting-Polycystic-Ovary-Syndrome-Treatment" class="headerlink" title="Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment"></a>Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11220">http://arxiv.org/abs/2308.11220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/toriqiu/fl-pcos">https://github.com/toriqiu/fl-pcos</a></li>
<li>paper_authors: Lucia Morris, Tori Qiu, Nikhil Raghuraman</li>
</ul>
<p>Abstract:<br>The field of women’s endocrinology has trailed behind data-driven medical solutions, largely due to concerns over the privacy of patient data. Valuable datapoints about hormone levels or menstrual cycling could expose patients who suffer from comorbidities or terminate a pregnancy, violating their privacy. We explore the application of Federated Learning (FL) to predict the optimal drug for patients with polycystic ovary syndrome (PCOS). PCOS is a serious hormonal disorder impacting millions of women worldwide, yet it’s poorly understood and its research is stunted by a lack of patient data. We demonstrate that a variety of FL approaches succeed on a synthetic PCOS patient dataset. Our proposed FL models are a tool to access massive quantities of diverse data and identify the most effective treatment option while providing PCOS patients with privacy guarantees.</p>
<hr>
<h2 id="Federated-Learning-in-Big-Model-Era-Domain-Specific-Multimodal-Large-Models"><a href="#Federated-Learning-in-Big-Model-Era-Domain-Specific-Multimodal-Large-Models" class="headerlink" title="Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models"></a>Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11217">http://arxiv.org/abs/2308.11217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zengxiang Li, Zhaoxiang Hou, Hui Liu, Ying Wang, Tongzhi Li, Longfei Xie, Chao Shi, Chengyi Yang, Weishan Zhang, Zelei Liu, Liang Xu</li>
</ul>
<p>Abstract:<br>Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient coordination of the federated learning platform, technical innovations on data quality improvement based on large model capabilities and efficient joint fine-tuning approaches. Preliminary experiments show that enterprises can enhance and accumulate intelligent capabilities through multimodal model federated learning, thereby jointly creating an smart city model that provides high-quality intelligent services covering energy infrastructure safety, residential community security, and urban operation management. The established federated learning cooperation ecosystem is expected to further aggregate industry, academia, and research resources, realize large models in multiple vertical domains, and promote the large-scale industrial application of artificial intelligence and cutting-edge research on multimodal federated learning.</p>
<hr>
<h2 id="ConcatPlexer-Additional-Dim1-Batching-for-Faster-ViTs"><a href="#ConcatPlexer-Additional-Dim1-Batching-for-Faster-ViTs" class="headerlink" title="ConcatPlexer: Additional Dim1 Batching for Faster ViTs"></a>ConcatPlexer: Additional Dim1 Batching for Faster ViTs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11199">http://arxiv.org/abs/2308.11199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donghoon Han, Seunghyeon Seo, Donghyeon Jeon, Jiho Jang, Chaerin Kong, Nojun Kwak</li>
</ul>
<p>Abstract:<br>Transformers have demonstrated tremendous success not only in the natural language processing (NLP) domain but also the field of computer vision, igniting various creative approaches and applications. Yet, the superior performance and modeling flexibility of transformers came with a severe increase in computation costs, and hence several works have proposed methods to reduce this burden. Inspired by a cost-cutting method originally proposed for language models, Data Multiplexing (DataMUX), we propose a novel approach for efficient visual recognition that employs additional dim1 batching (i.e., concatenation) that greatly improves the throughput with little compromise in the accuracy. We first introduce a naive adaptation of DataMux for vision models, Image Multiplexer, and devise novel components to overcome its weaknesses, rendering our final model, ConcatPlexer, at the sweet spot between inference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and CIFAR100 dataset and it achieved 23.5% less GFLOPs than ViT-B&#x2F;16 with 69.5% and 83.4% validation accuracy, respectively.</p>
<hr>
<h2 id="ViLLA-Fine-Grained-Vision-Language-Representation-Learning-from-Real-World-Data"><a href="#ViLLA-Fine-Grained-Vision-Language-Representation-Learning-from-Real-World-Data" class="headerlink" title="ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data"></a>ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11194">http://arxiv.org/abs/2308.11194</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanfordmimi/villa">https://github.com/stanfordmimi/villa</a></li>
<li>paper_authors: Maya Varma, Jean-Benoit Delbrouck, Sarah Hooper, Akshay Chaudhari, Curtis Langlotz</li>
</ul>
<p>Abstract:<br>Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained on datasets consisting of image-caption pairs obtained from the web. However, real-world multimodal datasets, such as healthcare data, are significantly more complex: each image (e.g. X-ray) is often paired with text (e.g. physician report) that describes many distinct attributes occurring in fine-grained regions of the image. We refer to these samples as exhibiting high pairwise complexity, since each image-text pair can be decomposed into a large number of region-attribute pairings. The extent to which VLMs can capture fine-grained relationships between image regions and textual attributes when trained on such data has not been previously evaluated. The first key contribution of this work is to demonstrate through systematic evaluations that as the pairwise complexity of the training dataset increases, standard VLMs struggle to learn region-attribute relationships, exhibiting performance degradations of up to 37% on retrieval tasks. In order to address this issue, we introduce ViLLA as our second key contribution. ViLLA, which is trained to capture fine-grained region-attribute relationships from complex datasets, involves two components: (a) a lightweight, self-supervised mapping model to decompose image-text samples into region-attribute pairs, and (b) a contrastive VLM to learn representations from generated region-attribute pairs. We demonstrate with experiments across four domains (synthetic, product, medical, and natural images) that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks, such as zero-shot object detection (up to 3.6 AP50 points on COCO and 0.6 mAP points on LVIS) and retrieval (up to 14.2 R-Precision points).</p>
<hr>
<h2 id="Diversity-Measures-Domain-Independent-Proxies-for-Failure-in-Language-Model-Queries"><a href="#Diversity-Measures-Domain-Independent-Proxies-for-Failure-in-Language-Model-Queries" class="headerlink" title="Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries"></a>Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11189">http://arxiv.org/abs/2308.11189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lab-v2/diversity_measures">https://github.com/lab-v2/diversity_measures</a></li>
<li>paper_authors: Noel Ngu, Nathaniel Lee, Paulo Shakarian</li>
</ul>
<p>Abstract:<br>Error prediction in large language models often relies on domain-specific information. In this paper, we present measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt - hence independent of the underlying application. We describe how three such measures - based on entropy, Gini impurity, and centroid distance - can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.</p>
<hr>
<h2 id="MISSRec-Pre-training-and-Transferring-Multi-modal-Interest-aware-Sequence-Representation-for-Recommendation"><a href="#MISSRec-Pre-training-and-Transferring-Multi-modal-Interest-aware-Sequence-Representation-for-Recommendation" class="headerlink" title="MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation"></a>MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11175">http://arxiv.org/abs/2308.11175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinpeng Wang, Ziyun Zeng, Yunxiao Wang, Yuting Wang, Xingyu Lu, Tianxiang Li, Jun Yuan, Rui Zhang, Hai-Tao Zheng, Shu-Tao Xia</li>
</ul>
<p>Abstract:<br>The goal of sequential recommendation (SR) is to predict a user’s potential interested items based on her&#x2F;his historical interaction sequences. Most existing sequential recommenders are developed based on ID features, which, despite their widespread use, often underperform with sparse IDs and struggle with the cold-start problem. Besides, inconsistent ID mappings hinder the model’s transferability, isolating similar recommendation domains that could have been co-optimized. This paper aims to address these issues by exploring the potential of multi-modal information in learning robust and generalizable sequence representations. We propose MISSRec, a multi-modal pre-training and transfer learning framework for SR. On the user side, we design a Transformer-based encoder-decoder model, where the contextual encoder learns to capture the sequence-level multi-modal synergy while a novel interest-aware decoder is developed to grasp item-modality-interest relations for better sequence representation. On the candidate item side, we adopt a dynamic fusion module to produce user-adaptive item representation, providing more precise matching between users and items. We pre-train the model with contrastive learning objectives and fine-tune it in an efficient manner. Extensive experiments demonstrate the effectiveness and flexibility of MISSRec, promising an practical solution for real-world recommendation scenarios.</p>
<hr>
<h2 id="Hierarchical-Point-based-Active-Learning-for-Semi-supervised-Point-Cloud-Semantic-Segmentation"><a href="#Hierarchical-Point-based-Active-Learning-for-Semi-supervised-Point-Cloud-Semantic-Segmentation" class="headerlink" title="Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation"></a>Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11166">http://arxiv.org/abs/2308.11166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongyi Xu, Bo Yuan, Shanshan Zhao, Qianni Zhang, Xinbo Gao</li>
</ul>
<p>Abstract:<br>Impressive performance on point cloud semantic segmentation has been achieved by fully-supervised methods with large amounts of labelled data. As it is labour-intensive to acquire large-scale point cloud data with point-wise labels, many attempts have been made to explore learning 3D point cloud segmentation with limited annotations. Active learning is one of the effective strategies to achieve this purpose but is still under-explored. The most recent methods of this kind measure the uncertainty of each pre-divided region for manual labelling but they suffer from redundant information and require additional efforts for region division. This paper aims at addressing this issue by developing a hierarchical point-based active learning strategy. Specifically, we measure the uncertainty for each point by a hierarchical minimum margin uncertainty module which considers the contextual information at multiple levels. Then, a feature-distance suppression strategy is designed to select important and representative points for manual labelling. Besides, to better exploit the unlabelled data, we build a semi-supervised segmentation framework based on our active strategy. Extensive experiments on the S3DIS and ScanNetV2 datasets demonstrate that the proposed framework achieves 96.5% and 100% performance of fully-supervised baseline with only 0.07% and 0.1% training data, respectively, outperforming the state-of-the-art weakly-supervised and active learning methods. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/SmiletoE/HPAL">https://github.com/SmiletoE/HPAL</a>.</p>
<hr>
<h2 id="xxMD-Benchmarking-Neural-Force-Fields-Using-Extended-Dynamics-beyond-Equilibrium"><a href="#xxMD-Benchmarking-Neural-Force-Fields-Using-Extended-Dynamics-beyond-Equilibrium" class="headerlink" title="xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium"></a>xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11155">http://arxiv.org/abs/2308.11155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zpengmei/xxmd">https://github.com/zpengmei/xxmd</a></li>
<li>paper_authors: Zihan Pengmei, Junyu Liu, Yinan Shu</li>
</ul>
<p>Abstract:<br>Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional theory. Furthermore, its nuclear configuration spaces authentically depict chemical reactions, making xxMD a more chemically relevant dataset. Our re-assessment of equivariant models on the xxMD datasets reveals notably higher mean absolute errors than those reported for MD17 and its variants. This observation underscores the challenges faced in crafting a generalizable NFF model with extrapolation capability. Our proposed xxMD-CASSCF and xxMD-DFT datasets are available at \url{<a target="_blank" rel="noopener" href="https://github.com/zpengmei/xxMD%7D">https://github.com/zpengmei/xxMD}</a>.</p>
<hr>
<h2 id="Exploring-Unsupervised-Cell-Recognition-with-Prior-Self-activation-Maps"><a href="#Exploring-Unsupervised-Cell-Recognition-with-Prior-Self-activation-Maps" class="headerlink" title="Exploring Unsupervised Cell Recognition with Prior Self-activation Maps"></a>Exploring Unsupervised Cell Recognition with Prior Self-activation Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11144">http://arxiv.org/abs/2308.11144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cpystan/psm">https://github.com/cpystan/psm</a></li>
<li>paper_authors: Pingyi Chen, Chenglu Zhu, Zhongyi Shui, Jiatong Cai, Sunyi Zheng, Shichuan Zhang, Lin Yang</li>
</ul>
<p>Abstract:<br>The success of supervised deep learning models on cell recognition tasks relies on detailed annotations. Many previous works have managed to reduce the dependency on labels. However, considering the large number of cells contained in a patch, costly and inefficient labeling is still inevitable. To this end, we explored label-free methods for cell recognition. Prior self-activation maps (PSM) are proposed to generate pseudo masks as training targets. To be specific, an activation network is trained with self-supervised learning. The gradient information in the shallow layers of the network is aggregated to generate prior self-activation maps. Afterward, a semantic clustering module is then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo masks for downstream tasks. We evaluated our method on two histological datasets: MoNuSeg (cell segmentation) and BCData (multi-class cell detection). Compared with other fully-supervised and weakly-supervised methods, our method can achieve competitive performance without any manual annotations. Our simple but effective framework can also achieve multi-class cell detection which can not be done by existing unsupervised methods. The results show the potential of PSMs that might inspire other research to deal with the hunger for labels in medical area.</p>
<hr>
<h2 id="Is-There-Any-Social-Principle-for-LLM-Based-Agents"><a href="#Is-There-Any-Social-Principle-for-LLM-Based-Agents" class="headerlink" title="Is There Any Social Principle for LLM-Based Agents?"></a>Is There Any Social Principle for LLM-Based Agents?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11136">http://arxiv.org/abs/2308.11136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jitao Bai, Simiao Zhang, Zhonghao Chen</li>
</ul>
<p>Abstract:<br>Focus on Large Language Model based agents should involve more than “human-centered” alignment or application. We argue that more attention should be paid to the agent itself and discuss the potential of social sciences for agents.</p>
<hr>
<h2 id="ReLLa-Retrieval-enhanced-Large-Language-Models-for-Lifelong-Sequential-Behavior-Comprehension-in-Recommendation"><a href="#ReLLa-Retrieval-enhanced-Large-Language-Models-for-Lifelong-Sequential-Behavior-Comprehension-in-Recommendation" class="headerlink" title="ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation"></a>ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11131">http://arxiv.org/abs/2308.11131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, Weinan Zhang</li>
</ul>
<p>Abstract:<br>With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data quality of testing samples, which greatly reduces the difficulty for LLMs to extract the essential knowledge from user behavior sequences. As for few-shot recommendation, we further design retrieval-enhanced instruction tuning (ReiT) by adopting SUBR as a data augmentation technique for training samples. Specifically, we develop a mixed training dataset consisting of both the original data samples and their retrieval-enhanced counterparts. We conduct extensive experiments on a real-world public dataset (i.e., MovieLens-1M) to demonstrate the superiority of ReLLa compared with existing baseline models, as well as its capability for lifelong sequential behavior comprehension.</p>
<hr>
<h2 id="Transformers-for-Capturing-Multi-level-Graph-Structure-using-Hierarchical-Distances"><a href="#Transformers-for-Capturing-Multi-level-Graph-Structure-using-Hierarchical-Distances" class="headerlink" title="Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances"></a>Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11129">http://arxiv.org/abs/2308.11129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuankai Luo</li>
</ul>
<p>Abstract:<br>Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.</p>
<hr>
<h2 id="CAME-Contrastive-Automated-Model-Evaluation"><a href="#CAME-Contrastive-Automated-Model-Evaluation" class="headerlink" title="CAME: Contrastive Automated Model Evaluation"></a>CAME: Contrastive Automated Model Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11111">http://arxiv.org/abs/2308.11111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pengr/contrastive_autoeval">https://github.com/pengr/contrastive_autoeval</a></li>
<li>paper_authors: Ru Peng, Qiuyang Duan, Haobo Wang, Jiachen Ma, Yanbo Jiang, Yongjun Tu, Xiu Jiang, Junbo Zhao</li>
</ul>
<p>Abstract:<br>The Automated Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set. Despite the promise and some decent results, the existing AutoEval methods heavily rely on computing distribution shifts between the unlabelled testing set and the training set. We believe this reliance on the training set becomes another obstacle in shipping this technology to real-world ML development. In this work, we propose Contrastive Automatic Model Evaluation (CAME), a novel AutoEval framework that is rid of involving training set in the loop. The core idea of CAME bases on a theoretical analysis which bonds the model performance with a contrastive loss. Further, with extensive empirical validation, we manage to set up a predictable relationship between the two, simply by deducing on the unlabeled&#x2F;unseen testing set. The resulting framework CAME establishes a new SOTA results for AutoEval by surpassing prior work significantly.</p>
<hr>
<h2 id="Anonymity-at-Risk-Assessing-Re-Identification-Capabilities-of-Large-Language-Models"><a href="#Anonymity-at-Risk-Assessing-Re-Identification-Capabilities-of-Large-Language-Models" class="headerlink" title="Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models"></a>Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11103">http://arxiv.org/abs/2308.11103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skatinger/anonymity-at-risk-assessing-re-identification-capabilities-of-large-language-models">https://github.com/skatinger/anonymity-at-risk-assessing-re-identification-capabilities-of-large-language-models</a></li>
<li>paper_authors: Alex Nyffenegger, Matthias Stürmer, Joel Niklaus</li>
</ul>
<p>Abstract:<br>Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions. The complexity is attributed to the lack of test datasets, the necessity for substantial training resources, and data sparsity in the information used for re-identification. In conclusion, this study demonstrates that re-identification using LLMs may not be feasible for now, but as the proof-of-concept on Wikipedia showed, it might become possible in the future. We hope that our system can help enhance the confidence in the security of anonymized decisions, thus leading to the courts being more confident to publish decisions.</p>
<hr>
<h2 id="Using-Early-Exits-for-Fast-Inference-in-Automatic-Modulation-Classification"><a href="#Using-Early-Exits-for-Fast-Inference-in-Automatic-Modulation-Classification" class="headerlink" title="Using Early Exits for Fast Inference in Automatic Modulation Classification"></a>Using Early Exits for Fast Inference in Automatic Modulation Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11100">http://arxiv.org/abs/2308.11100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elsayed Mohammed, Omar Mashaal, Hatem Abou-Zeid</li>
</ul>
<p>Abstract:<br>Automatic modulation classification (AMC) plays a critical role in wireless communications by autonomously classifying signals transmitted over the radio spectrum. Deep learning (DL) techniques are increasingly being used for AMC due to their ability to extract complex wireless signal features. However, DL models are computationally intensive and incur high inference latencies. This paper proposes the application of early exiting (EE) techniques for DL models used for AMC to accelerate inference. We present and analyze four early exiting architectures and a customized multi-branch training algorithm for this problem. Through extensive experimentation, we show that signals with moderate to high signal-to-noise ratios (SNRs) are easier to classify, do not require deep architectures, and can therefore leverage the proposed EE architectures. Our experimental results demonstrate that EE techniques can significantly reduce the inference speed of deep neural networks without sacrificing classification accuracy. We also thoroughly study the trade-off between classification accuracy and inference time when using these architectures. To the best of our knowledge, this work represents the first attempt to apply early exiting methods to AMC, providing a foundation for future research in this area.</p>
<hr>
<h2 id="Video-OWL-ViT-Temporally-consistent-open-world-localization-in-video"><a href="#Video-OWL-ViT-Temporally-consistent-open-world-localization-in-video" class="headerlink" title="Video OWL-ViT: Temporally-consistent open-world localization in video"></a>Video OWL-ViT: Temporally-consistent open-world localization in video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11093">http://arxiv.org/abs/2308.11093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Heigold, Matthias Minderer, Alexey Gritsenko, Alex Bewley, Daniel Keysers, Mario Lučić, Fisher Yu, Thomas Kipf</li>
</ul>
<p>Abstract:<br>We present an architecture and a training recipe that adapts pre-trained open-world image models to localization in videos. Understanding the open visual world (without being constrained by fixed label spaces) is crucial for many real-world vision tasks. Contrastive pre-training on large image-text datasets has recently led to significant improvements for image-level tasks. For more structured tasks involving object localization applying pre-trained models is more challenging. This is particularly true for video tasks, where task-specific data is limited. We show successful transfer of open-world models by building on the OWL-ViT open-vocabulary detection model and adapting it to video by adding a transformer decoder. The decoder propagates object representations recurrently through time by using the output tokens for one frame as the object queries for the next. Our model is end-to-end trainable on video data and enjoys improved temporal consistency compared to tracking-by-detection baselines, while retaining the open-world capabilities of the backbone detector. We evaluate our model on the challenging TAO-OW benchmark and demonstrate that open-world capabilities, learned from large-scale image-text pre-training, can be transferred successfully to open-world localization across diverse videos.</p>
<hr>
<h2 id="Collaborative-Route-Planning-of-UAVs-Workers-and-Cars-for-Crowdsensing-in-Disaster-Response"><a href="#Collaborative-Route-Planning-of-UAVs-Workers-and-Cars-for-Crowdsensing-in-Disaster-Response" class="headerlink" title="Collaborative Route Planning of UAVs, Workers and Cars for Crowdsensing in Disaster Response"></a>Collaborative Route Planning of UAVs, Workers and Cars for Crowdsensing in Disaster Response</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11088">http://arxiv.org/abs/2308.11088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Han, Chunyu Tu, Zhiwen Yu, Zhiyong Yu, Weihua Shan, Liang Wang, Bin Guo</li>
</ul>
<p>Abstract:<br>Efficiently obtaining the up-to-date information in the disaster-stricken area is the key to successful disaster response. Unmanned aerial vehicles (UAVs), workers and cars can collaborate to accomplish sensing tasks, such as data collection, in disaster-stricken areas. In this paper, we explicitly address the route planning for a group of agents, including UAVs, workers, and cars, with the goal of maximizing the task completion rate. We propose MANF-RL-RP, a heterogeneous multi-agent route planning algorithm that incorporates several efficient designs, including global-local dual information processing and a tailored model structure for heterogeneous multi-agent systems. Global-local dual information processing encompasses the extraction and dissemination of spatial features from global information, as well as the partitioning and filtering of local information from individual agents. Regarding the construction of the model structure for heterogeneous multi-agent, we perform the following work. We design the same data structure to represent the states of different agents, prove the Markovian property of the decision-making process of agents to simplify the model structure, and also design a reasonable reward function to train the model. Finally, we conducted detailed experiments based on the rich simulation data. In comparison to the baseline algorithms, namely Greedy-SC-RP and MANF-DNN-RP, MANF-RL-RP has exhibited a significant improvement in terms of task completion rate.</p>
<hr>
<h2 id="Neural-Amortized-Inference-for-Nested-Multi-agent-Reasoning"><a href="#Neural-Amortized-Inference-for-Nested-Multi-agent-Reasoning" class="headerlink" title="Neural Amortized Inference for Nested Multi-agent Reasoning"></a>Neural Amortized Inference for Nested Multi-agent Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11071">http://arxiv.org/abs/2308.11071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunal Jha, Tuan Anh Le, Chuanyang Jin, Yen-Ling Kuo, Joshua B. Tenenbaum, Tianmin Shu</li>
</ul>
<p>Abstract:<br>Multi-agent interactions, such as communication, teaching, and bluffing, often rely on higher-order social inference, i.e., understanding how others infer oneself. Such intricate reasoning can be effectively modeled through nested multi-agent reasoning. Nonetheless, the computational complexity escalates exponentially with each level of reasoning, posing a significant challenge. However, humans effortlessly perform complex social inferences as part of their daily lives. To bridge the gap between human-like inference capabilities and computational limitations, we propose a novel approach: leveraging neural networks to amortize high-order social inference, thereby expediting nested multi-agent reasoning. We evaluate our method in two challenging multi-agent interaction domains. The experimental results demonstrate that our method is computationally efficient while exhibiting minimal degradation in accuracy.</p>
<hr>
<h2 id="Temporal-Distributed-Backdoor-Attack-Against-Video-Based-Action-Recognition"><a href="#Temporal-Distributed-Backdoor-Attack-Against-Video-Based-Action-Recognition" class="headerlink" title="Temporal-Distributed Backdoor Attack Against Video Based Action Recognition"></a>Temporal-Distributed Backdoor Attack Against Video Based Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11070">http://arxiv.org/abs/2308.11070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis</li>
</ul>
<p>Abstract:<br>Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibility of video-based systems under backdoor attacks remains largely unexplored. Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are \textbf{independently} embedded within the frames, which tend to be detectable by existing defenses. In this paper, we introduce a \textit{simple} yet \textit{effective} backdoor attack against video data. Our proposed attack, adding perturbations in a transformed domain, plants an \textbf{imperceptible, temporally distributed} trigger across the video frames, and is shown to be resilient to existing defensive strategies. The effectiveness of the proposed attack is demonstrated by extensive experiments with various well-known models on two video recognition benchmarks, UCF101 and HMDB51, and a sign language recognition benchmark, Greek Sign Language (GSL) dataset. We delve into the impact of several influential factors on our proposed attack and identify an intriguing effect termed “collateral damage” through extensive studies.</p>
<hr>
<h2 id="Topological-Graph-Signal-Compression"><a href="#Topological-Graph-Signal-Compression" class="headerlink" title="Topological Graph Signal Compression"></a>Topological Graph Signal Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11068">http://arxiv.org/abs/2308.11068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Bernárdez, Lev Telyatnikov, Eduard Alarcón, Albert Cabellos-Aparicio, Pere Barlet-Ros, Pietro Liò</li>
</ul>
<p>Abstract:<br>Recently emerged Topological Deep Learning (TDL) methods aim to extend current Graph Neural Networks (GNN) by naturally processing higher-order interactions, going beyond the pairwise relations and local neighborhoods defined by graph representations. In this paper we propose a novel TDL-based method for compressing signals over graphs, consisting in two main steps: first, disjoint sets of higher-order structures are inferred based on the original signal –by clustering $N$ datapoints into $K\ll N$ collections; then, a topological-inspired message passing gets a compressed representation of the signal within those multi-element sets. Our results show that our framework improves both standard GNN and feed-forward architectures in compressing temporal link-based signals from two real-word Internet Service Provider Networks’ datasets –from $30%$ up to $90%$ better reconstruction errors across all evaluation scenarios–, suggesting that it better captures and exploits spatial and temporal correlations over the whole graph-based network structure.</p>
<hr>
<h2 id="CSM-H-R-An-Automatic-Context-Reasoning-Framework-for-Interoperable-Intelligent-Systems-and-Privacy-Protection"><a href="#CSM-H-R-An-Automatic-Context-Reasoning-Framework-for-Interoperable-Intelligent-Systems-and-Privacy-Protection" class="headerlink" title="CSM-H-R: An Automatic Context Reasoning Framework for Interoperable Intelligent Systems and Privacy Protection"></a>CSM-H-R: An Automatic Context Reasoning Framework for Interoperable Intelligent Systems and Privacy Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11066">http://arxiv.org/abs/2308.11066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songhui01/csm-h-r">https://github.com/songhui01/csm-h-r</a></li>
<li>paper_authors: Songhui Yue, Xiaoyan Hong, Randy K. Smith</li>
</ul>
<p>Abstract:<br>Automation of High-Level Context (HLC) reasoning for intelligent systems at scale is imperative due to the unceasing accumulation of contextual data in the IoT era, the trend of the fusion of data from multi-sources, and the intrinsic complexity and dynamism of the context-based decision-making process. To mitigate this issue, we propose an automatic context reasoning framework CSM-H-R, which programmatically combines ontologies and states at runtime and the model-storage phase for attaining the ability to recognize meaningful HLC, and the resulting data representation can be applied to different reasoning techniques. Case studies are developed based on an intelligent elevator system in a smart campus setting. An implementation of the framework - a CSM Engine, and the experiments of translating the HLC reasoning into vector and matrix computing especially take care of the dynamic aspects of context and present the potentiality of using advanced mathematical and probabilistic models to achieve the next level of automation in integrating intelligent systems; meanwhile, privacy protection support is achieved by anonymization through label embedding and reducing information correlation. The code of this study is available at: <a target="_blank" rel="noopener" href="https://github.com/songhui01/CSM-H-R">https://github.com/songhui01/CSM-H-R</a>.</p>
<hr>
<h2 id="FedDAT-An-Approach-for-Foundation-Model-Finetuning-in-Multi-Modal-Heterogeneous-Federated-Learning"><a href="#FedDAT-An-Approach-for-Foundation-Model-Finetuning-in-Multi-Modal-Heterogeneous-Federated-Learning" class="headerlink" title="FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning"></a>FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12305">http://arxiv.org/abs/2308.12305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haokun Chen, Yao Zhang, Denis Krompass, Jindong Gu, Volker Tresp</li>
</ul>
<p>Abstract:<br>Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning framework tailored to heterogeneous multi-modal FL, called Federated Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the client local updates and applying Mutual Knowledge Distillation (MKD) for an efficient knowledge transfer. FedDAT is the first approach that enables an efficient distributed finetuning of foundation models for a variety of heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we conduct extensive experiments on four multi-modality FL benchmarks with different types of data heterogeneity, where FedDAT substantially outperforms the existing centralized PEFT methods adapted for FL.</p>
<hr>
<h2 id="Beyond-Discriminative-Regions-Saliency-Maps-as-Alternatives-to-CAMs-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Beyond-Discriminative-Regions-Saliency-Maps-as-Alternatives-to-CAMs-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Beyond Discriminative Regions: Saliency Maps as Alternatives to CAMs for Weakly Supervised Semantic Segmentation"></a>Beyond Discriminative Regions: Saliency Maps as Alternatives to CAMs for Weakly Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11052">http://arxiv.org/abs/2308.11052</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Maruf, Arka Daw, Amartya Dutta, Jie Bu, Anuj Karpatne</li>
</ul>
<p>Abstract:<br>In recent years, several Weakly Supervised Semantic Segmentation (WS3) methods have been proposed that use class activation maps (CAMs) generated by a classifier to produce pseudo-ground truths for training segmentation models. While CAMs are good at highlighting discriminative regions (DR) of an image, they are known to disregard regions of the object that do not contribute to the classifier’s prediction, termed non-discriminative regions (NDR). In contrast, attribution methods such as saliency maps provide an alternative approach for assigning a score to every pixel based on its contribution to the classification prediction. This paper provides a comprehensive comparison between saliencies and CAMs for WS3. Our study includes multiple perspectives on understanding their similarities and dissimilarities. Moreover, we provide new evaluation metrics that perform a comprehensive assessment of WS3 performance of alternative methods w.r.t. CAMs. We demonstrate the effectiveness of saliencies in addressing the limitation of CAMs through our empirical studies on benchmark datasets. Furthermore, we propose random cropping as a stochastic aggregation technique that improves the performance of saliency, making it a strong alternative to CAM for WS3.</p>
<hr>
<h2 id="Personalized-Event-Prediction-for-Electronic-Health-Records"><a href="#Personalized-Event-Prediction-for-Electronic-Health-Records" class="headerlink" title="Personalized Event Prediction for Electronic Health Records"></a>Personalized Event Prediction for Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11013">http://arxiv.org/abs/2308.11013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeong Min Lee, Milos Hauskrecht</li>
</ul>
<p>Abstract:<br>Clinical event sequences consist of hundreds of clinical events that represent records of patient care in time. Developing accurate predictive models of such sequences is of a great importance for supporting a variety of models for interpreting&#x2F;classifying the current patient condition, or predicting adverse clinical events and outcomes, all aimed to improve patient care. One important challenge of learning predictive models of clinical sequences is their patient-specific variability. Based on underlying clinical conditions, each patient’s sequence may consist of different sets of clinical events (observations, lab results, medications, procedures). Hence, simple population-wide models learned from event sequences for many different patients may not accurately predict patient-specific dynamics of event sequences and their differences. To address the problem, we propose and investigate multiple new event sequence prediction models and methods that let us better adjust the prediction for individual patients and their specific conditions. The methods developed in this work pursue refinement of population-wide models to subpopulations, self-adaptation, and a meta-level model switching that is able to adaptively select the model with the best chance to support the immediate prediction. We analyze and test the performance of these models on clinical event sequences of patients in MIMIC-III database.</p>
<hr>
<h2 id="“Guinea-Pig-Trials”-Utilizing-GPT-A-Novel-Smart-Agent-Based-Modeling-Approach-for-Studying-Firm-Competition-and-Collusion"><a href="#“Guinea-Pig-Trials”-Utilizing-GPT-A-Novel-Smart-Agent-Based-Modeling-Approach-for-Studying-Firm-Competition-and-Collusion" class="headerlink" title="“Guinea Pig Trials” Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion"></a>“Guinea Pig Trials” Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10974">http://arxiv.org/abs/2308.10974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Han, Zengqing Wu, Chuan Xiao</li>
</ul>
<p>Abstract:<br>Firm competition and collusion involve complex dynamics, particularly when considering communication among firms. Such issues can be modeled as problems of complex systems, traditionally approached through experiments involving human subjects or agent-based modeling methods. We propose an innovative framework called Smart Agent-Based Modeling (SABM), wherein smart agents, supported by GPT-4 technologies, represent firms, and interact with one another. We conducted a controlled experiment to study firm price competition and collusion behaviors under various conditions. SABM is more cost-effective and flexible compared to conducting experiments with human subjects. Smart agents possess an extensive knowledge base for decision-making and exhibit human-like strategic abilities, surpassing traditional ABM agents. Furthermore, smart agents can simulate human conversation and be personalized, making them ideal for studying complex situations involving communication. Our results demonstrate that, in the absence of communication, smart agents consistently reach tacit collusion, leading to prices converging at levels higher than the Bertrand equilibrium price but lower than monopoly or cartel prices. When communication is allowed, smart agents achieve a higher-level collusion with prices close to cartel prices. Collusion forms more quickly with communication, while price convergence is smoother without it. These results indicate that communication enhances trust between firms, encouraging frequent small price deviations to explore opportunities for a higher-level win-win situation and reducing the likelihood of triggering a price war. We also assigned different personas to firms to analyze behavioral differences and tested variant models under diverse market structures. The findings showcase the effectiveness and robustness of SABM and provide intriguing insights into competition and collusion.</p>
<hr>
<h2 id="DocPrompt-Large-scale-continue-pretrain-for-zero-shot-and-few-shot-document-question-answering"><a href="#DocPrompt-Large-scale-continue-pretrain-for-zero-shot-and-few-shot-document-question-answering" class="headerlink" title="DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering"></a>DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10959">http://arxiv.org/abs/2308.10959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijin Wu, Dan Zhang, Teng Hu, Shikun Feng</li>
</ul>
<p>Abstract:<br>In this paper, we propose Docprompt for document question answering tasks with powerful zero-shot and few-shot performance. We proposed a novel weakly supervised data generation method, a novel multl-stage training method and a novel understanding model &amp; generation model ensemble method. Experiment results show that the Docprompt model after continue pretrain significantly outperforms the existing strong baseline models on document question answering tasks. This method greatly improves the delivery efficiency and model performance of document question answering customer projects, reducing annotation costs and labor costs. Our demo can be found at <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout">https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout</a>.</p>
<hr>
<h2 id="Structured-World-Models-from-Human-Videos"><a href="#Structured-World-Models-from-Human-Videos" class="headerlink" title="Structured World Models from Human Videos"></a>Structured World Models from Human Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10901">http://arxiv.org/abs/2308.10901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Russell Mendonca, Shikhar Bahl, Deepak Pathak</li>
</ul>
<p>Abstract:<br>We tackle the problem of learning complex, general behaviors directly in the real world. We propose an approach for robots to efficiently learn manipulation skills using only a handful of real-world interaction trajectories from many different settings. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, our belief is that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. Our approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. Further, we train a world model on human videos and fine-tune on a small amount of robot interaction data without any task supervision. We show that this approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction. Videos can be found at <a target="_blank" rel="noopener" href="https://human-world-model.github.io/">https://human-world-model.github.io</a></p>
<hr>
<h2 id="TADA-Text-to-Animatable-Digital-Avatars"><a href="#TADA-Text-to-Animatable-Digital-Avatars" class="headerlink" title="TADA! Text to Animatable Digital Avatars"></a>TADA! Text to Animatable Digital Avatars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10899">http://arxiv.org/abs/2308.10899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang, Yangyi Huang, Justus Thies, Michael J. Black</li>
</ul>
<p>Abstract:<br>We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines. Existing text-based character generation methods are limited in terms of geometry and texture quality, and cannot be realistically animated due to inconsistent alignment between the geometry and the texture, particularly in the face region. To overcome these limitations, TADA leverages the synergy of a 2D diffusion model and an animatable parametric body model. Specifically, we derive an optimizable high-resolution body model from SMPL-X with 3D displacements and a texture map, and use hierarchical rendering with score distillation sampling (SDS) to create high-quality, detailed, holistic 3D avatars from text. To ensure alignment between the geometry and texture, we render normals and RGB images of the generated character and exploit their latent embeddings in the SDS training process. We further introduce various expression parameters to deform the generated character during training, ensuring that the semantics of our generated character remain consistent with the original SMPL-X model, resulting in an animatable character. Comprehensive evaluations demonstrate that TADA significantly surpasses existing approaches on both qualitative and quantitative measures. TADA enables creation of large-scale digital character assets that are ready for animation and rendering, while also being easily editable through natural language. The code will be public for research purposes.</p>
<hr>
<h2 id="Giraffe-Adventures-in-Expanding-Context-Lengths-in-LLMs"><a href="#Giraffe-Adventures-in-Expanding-Context-Lengths-in-LLMs" class="headerlink" title="Giraffe: Adventures in Expanding Context Lengths in LLMs"></a>Giraffe: Adventures in Expanding Context Lengths in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10882">http://arxiv.org/abs/2308.10882</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abacusai/long-context">https://github.com/abacusai/long-context</a></li>
<li>paper_authors: Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, Siddartha Naidu</li>
</ul>
<p>Abstract:<br>Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time. To use these models on sequences longer than the train-time context length, one might employ techniques from the growing family of context length extrapolation methods – most of which focus on modifying the system of positional encodings used in the attention mechanism to indicate where tokens or activations are located in the input sequence. We conduct a wide survey of existing methods of context length extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own design as well – in particular, a new truncation strategy for modifying the basis for the position encoding.   We test these methods using three new evaluation tasks (FreeFormQA, AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to be less fine-grained as a measure of long context performance of LLMs. We release the three tasks publicly as datasets on HuggingFace. We discover that linear scaling is the best method for extending context length, and show that further gains can be achieved by using longer scales at evaluation time. We also discover promising extrapolation capabilities in the truncated basis. To support further research in this area, we release three new 13B parameter long-context models which we call Giraffe: 4k and 16k context models trained from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We also release the code to replicate our results.</p>
<hr>
<h2 id="Analyzing-Transformer-Dynamics-as-Movement-through-Embedding-Space"><a href="#Analyzing-Transformer-Dynamics-as-Movement-through-Embedding-Space" class="headerlink" title="Analyzing Transformer Dynamics as Movement through Embedding Space"></a>Analyzing Transformer Dynamics as Movement through Embedding Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10874">http://arxiv.org/abs/2308.10874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumeet S. Singh</li>
</ul>
<p>Abstract:<br>Transformer language models exhibit intelligent behaviors such as understanding natural language, recognizing patterns, acquiring knowledge, reasoning, planning, reflecting and using tools. This paper explores how their underlying mechanics give rise to intelligent behaviors. We adopt a systems approach to analyze Transformers in detail and develop a mathematical framework that frames their dynamics as movement through embedding space. This novel perspective provides a principled way of thinking about the problem and reveals important insights related to the emergence of intelligence:   1. At its core the Transformer is a Embedding Space walker, mapping intelligent behavior to trajectories in this vector space.   2. At each step of the walk, it composes context into a single composite vector whose location in Embedding Space defines the next step.   3. No learning actually occurs during decoding; in-context learning and generalization are simply the result of different contexts composing into different vectors.   4. Ultimately the knowledge, intelligence and skills exhibited by the model are embodied in the organization of vectors in Embedding Space rather than in specific neurons or layers. These abilities are properties of this organization.   5. Attention’s contribution boils down to the association-bias it lends to vector composition and which influences the aforementioned organization. However, more investigation is needed to ascertain its significance.   6. The entire model is composed from two principal operations: data independent filtering and data dependent aggregation. This generalization unifies Transformers with other sequence models and across modalities.   Building upon this foundation we formalize and test a semantic space theory which posits that embedding vectors represent semantic concepts and find some evidence of its validity.</p>
<hr>
<h2 id="Real-World-Time-Series-Benchmark-Datasets-with-Distribution-Shifts-Global-Crude-Oil-Price-and-Volatility"><a href="#Real-World-Time-Series-Benchmark-Datasets-with-Distribution-Shifts-Global-Crude-Oil-Price-and-Volatility" class="headerlink" title="Real World Time Series Benchmark Datasets with Distribution Shifts: Global Crude Oil Price and Volatility"></a>Real World Time Series Benchmark Datasets with Distribution Shifts: Global Crude Oil Price and Volatility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10846">http://arxiv.org/abs/2308.10846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranay Pasula</li>
</ul>
<p>Abstract:<br>The scarcity of task-labeled time-series benchmarks in the financial domain hinders progress in continual learning. Addressing this deficit would foster innovation in this area. Therefore, we present COB, Crude Oil Benchmark datasets. COB includes 30 years of asset prices that exhibit significant distribution shifts and optimally generates corresponding task (i.e., regime) labels based on these distribution shifts for the three most important crude oils in the world. Our contributions include creating real-world benchmark datasets by transforming asset price data into volatility proxies, fitting models using expectation-maximization (EM), generating contextual task labels that align with real-world events, and providing these labels as well as the general algorithm to the public. We show that the inclusion of these task labels universally improves performance on four continual learning algorithms, some state-of-the-art, over multiple forecasting horizons. We hope these benchmarks accelerate research in handling distribution shifts in real-world data, especially due to the global importance of the assets considered. We’ve made the (1) raw price data, (2) task labels generated by our approach, (3) and code for our algorithm available at <a target="_blank" rel="noopener" href="https://oilpricebenchmarks.github.io/">https://oilpricebenchmarks.github.io</a>.</p>
<hr>
<h2 id="Neural-Networks-Optimizations-Against-Concept-and-Data-Drift-in-Malware-Detection"><a href="#Neural-Networks-Optimizations-Against-Concept-and-Data-Drift-in-Malware-Detection" class="headerlink" title="Neural Networks Optimizations Against Concept and Data Drift in Malware Detection"></a>Neural Networks Optimizations Against Concept and Data Drift in Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10821">http://arxiv.org/abs/2308.10821</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Maillet, Benjamin Marais</li>
</ul>
<p>Abstract:<br>Despite the promising results of machine learning models in malware detection, they face the problem of concept drift due to malware constant evolution. This leads to a decline in performance over time, as the data distribution of the new files differs from the training one, requiring regular model update. In this work, we propose a model-agnostic protocol to improve a baseline neural network to handle with the drift problem. We show the importance of feature reduction and training with the most recent validation set possible, and propose a loss function named Drift-Resilient Binary Cross-Entropy, an improvement to the classical Binary Cross-Entropy more effective against drift. We train our model on the EMBER dataset (2018) and evaluate it on a dataset of recent malicious files, collected between 2020 and 2023. Our improved model shows promising results, detecting 15.2% more malware than a baseline model.</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/22/cs.AI_2023_08_22/" data-id="cllqyyx9w0000bpr87v8h5hfx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">Next</a>
    </nav>
  

</section>
      
      <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">26</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">4</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
  <a href="#search" class="mobile-nav-link st-search-show-outputs">Search</a>
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


</div>
</body>
</html>
